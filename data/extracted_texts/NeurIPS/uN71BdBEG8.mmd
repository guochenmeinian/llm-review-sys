# The Graph Pencil Method:

Mapping Subgraph Densities to

Stochastic Block Models

 Lee M. Gunderson

Gatsby Unit

University College London &Gecia Bravo-Hermsdorff

Department of Statistics

University College London &Peter Orbanz

Gatsby Unit

University College London

###### Abstract

In this work, we describe a method that determines an exact map from a finite set of subgraph densities to the parameters of a stochastic block model (SBM) matching these densities. Given a number \(K\) of blocks, the subgraph densities of a finite number of stars and bistars uniquely determines a single element of the class of all degree-separated stochastic block models with \(K\) blocks. Our method makes it possible to translate estimates of these subgraph densities into model parameters, and hence to use subgraph densities directly for inference. The computational overhead is negligible; computing the translation map is polynomial in \(K\), but independent of the graph size once the subgraph densities are given.

## 1 Introduction

The class of infinitely exchangeable random graphs is a large class of network models that contains all graphon models  and stochastic block models (SBMs) . The key statistics of such random graphs are subgraph densities: each fixed, finite graph defines a subgraph density, and it is well known that every exchangeable graph distribution is completely determined by the collection of all its densities of finite subgraphs .

In general, an infinite number of densities is required to uniquely determine an infinitely exchangeable graph distribution. For a subclass of models, a finite number of densities suffice. In combinatorics, these random graph distributions are called finitely forcible . In particular, every stochastic block model is completely determined by a finite number of subgraph densities . Such a finite number of subgraph densities can be consistently estimated at a guaranteed asymptotic rate from a single graph-data as this graph grows large .

It has long been recognized that the role subgraph densities play for exchangeable network models is analogous to that played by moments for estimation problems on Euclidean sample spaces . On Euclidean space, the method of moments  is a powerful inference tool, in particular for mixture models . This suggests that subgraph densities should be well-suited for inference in stochastic block models in particular, since these models are a network analogue of mixture models and only a finite number of densities must be estimated.

Nonetheless, most inference algorithms used in practice (e.g., ) do no rely on subgraph densities, but rather on some combination of maximum likelihood estimation, MCMC sampling, variational inference, clustering of the nodes, or other heuristics. The reason is that estimating subgraph densities is only a first step in the inference process -- once the densities are known, they must be translated into an estimate of a graphon or stochastic block model.

This paper introduces a new method that does so by generalizing a classical strategy for inferring latent sources (sometimes known as Prony's method ) to graph data. The classical Prony's

[MISSING_PAGE_FAIL:2]

Mapping Star Densities to Block Degrees

As the first step of our method, we obtain the normalized degrees of the blocks \(\) (equation (4)), as well as their relative sizes \(\). This can be seen as an application of the classical matrix pencil method to the degree distribution, for which the density of the star subgraphs serve as sufficient statistics.

### Classical Coin Collecting

Before we explain our graph pencil method, let us consider the simpler case of a mixture model for infinitely exchangeable sequences of binary variables. Also known as the Bernoulli mixture model , such a distribution can be thought of as the outcomes of (some number of) flips of (some number of) biased coins, where each coin is sampled i.i.d. from a (possibly unequal) mixture of \(K\) different biases.

To recover the parameters of this model, one must infer the \(K\) (unobserved) latent biases \(b_{k}\), as well as the fraction \(_{k}\) of coins with each bias. Note that one does not know which of the coins have the same latent bias (otherwise the inference problem would be trivial).

The moments of this distribution are the expectation of powers of these latent biases, which are indexed by the exponents \(r\):

\[ b^{r}=_{k}_{k}b^{r}_{k}\] (2)

Note that these moments can be consistently estimated from the observed data, as we are averaging over the latent variables. From these moments, one can systematically infer the mixture proportion \(\) and the biases \(\) using the matrix pencil method.

In short, construct two matrices \(\) and \(^{}\), with entries \(C_{ij}= b^{i+j}\) and \(C^{}_{ij}= b^{i+j+1}\):

\[=[ b^{0}& b^{1} && b^{K-1}\\  b^{1}& b^{2}&& b^{K}\\ &&&\\  b^{K-1}& b^{K}&& b^{2K-2} ]^{}=[  b^{1}& b^{2}&& b^{K}\\  b^{2}& b^{3}&& b^{K+1}\\ &&&\\  b^{K}& b^{K+1}&& b^{2K-1} ]\] (3)

While not immediately obvious, it is easy to show that the eigenvalues of \(^{}^{-1}\) are the entries of \(\). From these, it is straightforward to obtain the associated entries of \(\). In the next section, we show why this is the case, while superficially replacing the biases \(\) of the latent coin types with the normalized average degrees \(\) of the latent node blocks.

### Distilling the Degree Distribution

The first step of our graph pencil method is to apply the standard matrix pencil method to the obtain the average normalized degree of each of the \(K\) latent blocks (that is, the likelihood that a random node shares an edge with a node in block \(k\)). Denoted by \(\), the entries of this vector are:

\[d_{k}=_{j}_{j}B_{jk}\] (4)

These \(d_{k}\) are latent parameters of the blocks, so we can solve for them in exactly the same way as we did for \(b_{k}\) in the previous coin example. To this end, we consider the analogous moments:

\[ d^{r}=_{k}_{k}d^{r}_{k}\] (5)

As before, while \(_{k}\) and \(d_{k}\) are unobserved, the resulting moments can be consistently estimated from observations. In particular, they are precisely the homomorphism densities of the star subgraphs:

\[ d^{0}=} =1 d^{1}=} d^{2}=} d^{3}= }\] (6)We then define the two matrices \(}\) and \(}\), with entries \(C^{}_{\ }= d^{i+j}\) and \(C^{}_{\ }= d^{i+j+1}\), and the eigenvalues of \(}(})^{-1}\) are the normalized degrees of the \(K\) blocks. To see why, notice that \(}\) can be written as a sum of \(K\) rank-1 matrices (weighted by \(\)):

\[} =_{k}_{k}[d_{k}^{0}&d_{k}^{1}& &d_{k}^{K-1}\\ d_{k}^{1}&d_{k}^{2}&&d_{k}^{K}\\ &&&\\ d_{k}^{K-1}&d_{k}^{K}&&d_{k}^{K-2}]=_{k}_{k} [d_{k}^{0}\\ d_{k}^{1}\\ \\ d_{k}^{K-1}][d_{k}^{0}&d_{k}^{1}& &d_{k}^{K-1}\\ ]\] \[=()\;^{}\] (7)

where we have defined \(\) as the matrix with entries \(V_{jk}=d_{k}^{j-1}\), i.e.:

\[=[d_{1}^{0}&d_{2}^{0}&&d_{K}^{0}\\ d_{1}^{1}&d_{2}^{1}&&d_{K}^{1}\\ &&&\\ d_{1}^{K-1}&d_{2}^{K-1}&&d_{K}^{K-1}]\] (8)

By decomposing \(}\) in the same manner, we get:

\[}=()\;^{}\] (9)

where \(()\) is the diagonal matrix with entries \(_{k}d_{k}\).

We now see that \(}(})^{-1}\) can be diagonalized as follows:

\[}(})^{-1} =()\;^{} ()\;^{}^{-1}\] \[=()\;^{ }^{}^{-1}(^{-1})\; ^{-1}\] \[=()\;( ^{-1})\;^{-1}\] \[=()\;^{-1}\] (10)

Hence, the eigenvalues of \(}(})^{-1}\) are indeed the normalized degrees \(\) of the \(K\) latent blocks:

\[}(})^{-1}=d_ {k}}_{k[K]}\] (11)

**Remark 1** (Degree separated assumption).: This is where the assumption of degree-separated blocks is used; if the normalized degrees of the blocks are not unique, then \(\) will not be invertible.

From the entries of \(\), we know the entries of \(\), and can solve a linear system of equations for the corresponding entries of \(\):

\[_{k}V_{jk}_{k}= d^{j-1}\] (12)

## 4 From Bistar Densities to a Stochastic Block Model

This second step of our graph pencil method is the main insight of our paper. It uses the results from the previous step to reconstruct the connection probabilities \(B_{k,k^{}}\).

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

As before, \(}\) is obtained by unrooting the entries, while for the construction of \(}\), we glue the rooted subgraph \(\;}}\;\) corresponding to entries of \(\).

\[} =[[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.0pt]{fig/C^ {}}&[height=1.0pt]{fig/C^{}}&[height= 1.0pt]{fig/C^{}}\\ [height=1.0pt]{fig/C^{}}&[height=1.

We can use this to add more columns to \(}\) and \(}\):

\[} =[[height=142.26378pt]{ 142.26378pt}&[height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}&[ height=142.26378pt]{214.26378pt}& [ height=142.

matrix of node degrees, and \(\) is the (traceless) "two-hop adjacency" matrix. From these, we can recursively count all the relevant subgraphs using only entrywise multiplication.

We index our subgraphs with a tuple of non-negative integers \((,c,r)\), corresponding to: the number of edges incident to only the "left" node, the number of two-hop paths between the "left" and "right" nodes, and the number of edges incident to only the "right" node \(j\). First, we add the two-hop paths:

\[^{(0,0,0)} =-\] (28) \[^{(0,c+1,0)} =^{(0,c,0)}(-c)\] (29)

Define the matrix of "left" degrees to be \(=(-)-\) (i.e., the degree of node \(i\) if node \(j i\) were deleted, and zero when \(i=j\)), and the matrix of "right" degrees to be its transpose \(=^{}\). Next, we add single edges to node \(i\), then to node \(j\):

\[^{(+1,c,0)} =^{(,c,0)}(-(+c))\] (30) \[^{(,c,r+1)} =^{(,c,r)}(-(r+c) )-^{(-1,c+1,r-1)}\] (31)

Finally, if an edge connecting the left and right nodes is to be included, we put a mark on the middle integer:

\[^{(,c^{},r)}=^{(,c,r)} \] (32)

To obtain the counts of a subgraph \(g\) in the entire graph, simply sum the entries of the corresponding matrix \(^{(,c,r)}\). This is analogous to the "unrooting" operation from before.

\[[\![^{(,c,r)}]\!]=_{i}_{j}^{(,c,r)}\] (33)

For example, \([\![]\!]\) is twice the number of edges, as the injective homomorphisms count both orientations. The injective homomorphism densities are obtained by dividing this count by the number of injective maps

\[g^{(,c,r)}=^{(,c,r)} ]\!]}{NN-1N-|V(g)|+1}\] (34)

## 6 Discussion

The map from a stochastic block model to the density of a given subgraph can be computed analytically (e.g., ), or approximated by simulation (sample a graph from the model, count the number of occurrences of the given subgraph, and normalize appropriately). In contrast, the map from the subgraph densities to the stochastic block model is rather nontrivial. This paper provides a way to compute this map.

Since asymptotic rates are known for subgraph densities (see, e.g.,  and ), and the mapping we construct here is differentiable in the interior of its domain, we should expect similar asymptotic rates for recovery of the model parameters. We do not currently know how the rate behaves at the boundaries. At the boundary, the method may in principle output invalid probabilities, as there is at present no explicit constraint enforcing the connection probabilities to be between \(0\) and \(1\). Hence, as many other methods for inferring SBMs, our method is well-behaved in the interior.

Finally, we note that the method presented here can be extended in a variety of ways, for example to directed edges, weighted edges, directed weighted edges, etc.

## Code

Code implementing our method is available at https://github.com/TheGravLab/TheGraphPencilMethod.