# RedCode: Risky Code Execution and Generation Benchmark for Code Agents

Chengquan Guo\({}^{1}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Xun Liu\({}^{2}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Chulin Xie\({}^{2*}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Andy Zhou\({}^{2,3}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Yi Zeng\({}^{4}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Zinan Lin\({}^{5}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Dawn Song\({}^{6}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

Bo Li\({}^{1,2}\)

Equal Contribution. Work done during Chengquan's internship at the University of Chicago and Xun's internship at the University of Illinois Urbana-Champaign.

###### Abstract

With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests. RedCode consists of two parts to evaluate agents' safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents' ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents' vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT-4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.

## 1 Introduction

LLM-based code agents  have significantly advanced AI-assisted coding and software development. Integrated with external tools like Python interpreters or command-line interfaces, these agents can execute code actions and dynamically adjust the actions based on observations (e.g., execution results) for multiple interaction runs. This capability allows agents to interact with operating systems, leverage existing packages or install new ones , and use automated feedback (e.g., error messages) to self-debug and improve task-solving . However, despite their impressive capabilities, these code agents are not risk-free. For example, if code agents inadvertently suggest or execute code with security vulnerabilities, the consequences could be severe, particularly when the code is integrated into critical systems or when the agents directly operatethese systems, potentially leading to actions such as deleting important files or leaking sensitive information.

While efforts have been made to assess the safety of code generated by _code LLMs_[19; 11; 25], a comprehensive safety evaluation of LLM-based _code agents_ remains challenging and, to date, is still absent. In contrast to generating static code as _code LLMs_, _code agents_ extend beyond mere code generation to include dynamic executions and interactions with the broader system environment, such as file and operating systems, network communications, API calls, etc. This broader range of functionalities introduces additional layers of complexity and potential risks, as code agents must be assessed not only for the vulnerability of the generated code but also for the safety and security implications of their actions in various execution environments. Such multifaceted interaction with external resources poses challenges for evaluating code agents' safety.

To rigorously and comprehensively evaluate the safety of code agents, we propose RedCode, a benchmark for assessing the risks of code agents around code execution and generation. RedCode is built on the following principles: (1) **Real interaction with systems**. We build Docker images for our test cases, requiring minimal modification to make them compatible with each agent framework. (2) **Holistic evaluation on code execution and generation.** We provide risky test cases to evaluate agent's safety in risky code execution and generation. i) _Risky Code Execution (RedCode-Exec)_: Execute risky code via the agent's interaction with the system, given the risky code snippets or natural language descriptions about the risky operations in Python or Bash. ii) _Risky Software Generation (RedCode-Gen)_: Generate malicious code via the agent's self-debugging and self-improvement ability, given the Python function signature instructions. (3) **Diverse natural and programming languages input format**. For RedCode-Exec, we provide test cases where user queries can be in various input formats, including risky code snippets and summarized or detailed text instructions. For each input, we support Python and Bash programming languages and natural language instructions. (4) **Comprehensive risky scenarios and tests**. For RedCode-Exec, we source risky scenarios from a complete list of Common Weakness Enumeration (CWE)  and prior efforts in safety benchmarks [28; 22] with manual modification, leading to 25 scenarios (Fig. 3) ranging from real system, network operations to program logic and so on. We are dedicated to generating high-quality test cases in different natural and programming languages following our effective data generation framework and providing a large number of tests for comprehensive evaluations. Additionally, we create corresponding evaluation scripts for each risky test case in our Docker environment. For RedCode-Gen, we provide diverse prompts to generate risky software from eight malware families.

With these principles in mind, we build RedCode, a benchmark that evaluates the safety of code agents. Specifically, we have constructed \(4050\) risky test cases in RedCode-Exec for code execution, spanning 25 major safety scenarios across various 8 domains (Fig. 3), and \(160\) prompts in RedCode-Gen for malicious software generation spanning 8 malware families.

**Empirical findings.** We evaluate 3 types of agents with a total of 19 LLM-based code agents under different RedCode scenarios. We highlight the following findings from our evaluations: (1) **Safety risks comparisons** (Figs. 5 and 6). The overall attack success rate is high on RedCode-Exec when agents are queried to execute risky or buggy code, highlighting the vulnerability of existing agents. The rejection rate for risky test cases on the operating and file systems is higher than in other domains. (2) **Natural and programming languages comparisons** (Fig. 7). Agents are more likely to execute harmful actions by risky queries in natural language than in programming languages. Python leads to a higher rejection rate than Bash. (3) **Agent comparisons** (Figs. 1 and 6). Experiments on three types of code agents, OpenCodeInterpreter , CodeAct , and ReAct , show that OpenCodeInterpreter is relatively safer than CodeAct and ReAct, potentially due to its hard-coded

Figure 1: Safety evaluation of 19 code agents on RedCode-Exec Python and Bash test cases under agent frameworks CodeAct, ReAct, and OpenCodeInterpreter (OCI). Among the evaluated LLMs, * demotes the fine-tuned LLMs released from OCI and CodeAct.

safety constraints. (4) **Model comparisons** (Fig. 1 and Tab. 1). Agents paired with stronger base LLMs (e.g., GPT-4) can have a higher rejection rate for risky code execution in RedCode-Exec, but they also generate more sophisticated and effective harmful software in RedCode-Gen, indicating safety concerns.

## 2 Related work

**Safety evaluation for code LLMs.** Broad safety benchmarks have been proposed [20; 33; 14; 12] for general-purpose LLMs, using natural language instructions to evaluate harmful generations. While some instructions are code-related , such as generating a type of malware , these benchmarks are designed for LLMs, not agents. For code LLMs, existing benchmarks evaluate vulnerabilities of generated code [19; 25; 11; 4] mainly based on top weaknesses from the list of Common Weakness Enumeration (CWE) . In contrast to evaluating risks in static code generated by code LLMs, we focus on evaluating _code agents_ with more comprehensive risky scenarios, not only for the vulnerability of generated code but also for the safety implications of their actions in various execution environments.

**Safety evaluation for LLM agents.** Existing agent safety benchmarks like R-judge  and AgentMonitor  manually curate agent risky trajectory records to evaluate the ability of LLMs, _acting as a judge_ in identifying safety risks in these records. To alleviate human-efforts in records construction and to evaluate tool-use _agents_, ToolEmu , HAICOSYSTEM  proposes LLM-based emulation frameworks with simulated tool-use environments to generate risky trajectory records. However, there can be a large gap between those simulated records in sandbox environments and the agent's actual behavior in real systems. Moreover, during evaluation, these works use another LLM as a judge to provide a safety score for generated records, which can be inaccurate due to the LLM's lack of safety awareness. In contrast, our benchmark offers challenging red-teaming prompts and corresponding Docker environments for _agents that interact with real systems_, allowing us to evaluate the actual risks associated with the agent's code execution and generation. Our approach involves having agents generate and execute code for given risky tasks in a well-prepared environment (i.e., Docker containers with prepared resources). After agents implement the task, we use designed evaluation scripts corresponding to each risky scenario to evaluate the safety outcome, which could proactively check the status of the execution environment (e.g., if a file is deleted), providing the most accurate judgment. To the best of our knowledge, we are the first to provide such a fine-grained safety evaluation for code agents in real systems. The comparison between our work and previous benchmarks is shown in Tab. 3 and the detailed evaluation difference is shown in Fig. 9.

Concurrent benchmarks AgentDojo  and ASB  evaluate attack and defense mechanisms for agents, including prompt injection, memory poisoning, and backdoors. However, they do not consider agents' vulnerabilities that exist without external attacks. Our work addresses this gap by revealing the inherent risks of code agents.

## 3 RedCode Benchmark

### RedCode overview

Fig. 2 is an overview of RedCode dataset generation and evaluation (see dataset statistics in Tab. 2). To assess agent _code execution safety_, we provide 4,050 prompts in RedCode-Exec (SS3.2). We start by collecting risky scenarios from existing benchmarks and CWE, manually creating initial seed Python snippets test cases, and expanding them using a human-in-the-loop AI-assisted process (SS3.2.1). These Python snippets are translated into Bash scripts and natural language using LLMs (SS3.2.3). Agents execute risky code in a Docker environment, and their performance is evaluated with our prepared scripts (SS3.2.1). For assessing _risky code generation_, RedCode-Gen offers 160 prompts based on malware categories, with code generated in our docker environment and code quality evaluated by LLMs and VirusTotal API (SS3.3).

### RedCode-Exec: Code agent safety benchmark for code execution

We start by identifying risky scenarios, creating a collection of risky Python code snippets, building docker environments, and designing scripts for agent execution evaluation (SS3.2.1). Then, we translate Python code to different programming languages (PL), such as Bash (SS3.2.2). Finally, we translate the risky code snippets in each PL into different formats of risky natural language prompts, such as summarized text or detailed text description (SS3.2.3) for evaluation.

**Risky scenarios and seed test cases.** We start our dataset construction by identifying risk scenarios for code agents based on related work and well-defined software weakness lists, and manually constructing seed risky test cases (i.e., "user query") in Python. Specifically, **(1)** We first carefully examine all risky cases in existing code benchmarks, such as ToolEmu  and R-judge , and collect 12 all applicable code-related risky scenarios. These scenarios include dangerous actions such as _installing an unofficial version of 'pandas' from a GitHub repository_, _deleting important files_, and _adding a risky alias to "-bashrc"_. As existing benchmarks only provide simulated unsafe agent trajectories without considering actual code implementation and execution results, we concretize these risks into concise _executable_ Python code snippets. For example, we import relevant packages, define variables, implement functions, and ask code agents to execute the code. We also prepare fake websites, files, servers, and other resources for agents to access under corresponding risky scenarios if needed. In these cases, executing the code could lead to real harmful outcomes. **(2)** Secondly, we refer to authoritative sources for well-defined security weaknesses, focusing mainly on MITRE's Common Weakness Enumeration (CWE) , a community-developed dictionary of software weakness types. There are a total of 33 Python code examples under "Example Language: Python" in CWE, which explain the code weakness and represent different types of risky scenarios. As some examples are repetitive or similar, we distill them into 13 distinct risky scenarios for our benchmark, such as _improper execution privileges, eval injection, unsafe deserialization_. However, their code examples are usually educational and abstract. Using these examples directly may not be sufficient to pose a threat. Guided by CWE's risk descriptions and code examples, we manually write risky code snippets by providing specific inputs or potentially risky contexts, defining relevant variables and implementing risky operations. We use "_Execute the following Python code [code_snippet]_" as the risky user query (i.e., test case). A test case comparison between our benchmark and R-judge/CWE is provided in Fig. 8.

This procedure results in a total of 25 risky scenarios and corresponding Python code snippets, which serve as our 25 seed test cases. These 25 scenarios span 8 domains, as shown in Fig. 3, including

Figure 3: 25 risky scenarios spanning 8 domains in RedCode-Exec.

Figure 2: Overview of RedCode benchmark curation and evaluation where RedCode-Exec assess risks associated with code execution (ยง3.2), and RedCode-Gen evaluates risks in software generation (ยง3.3).

operation systems, file systems, program logic, cyber security, website, network, data processing, and others. We defer the full risk description and test case design to Tab. 4.

**Automatically augmenting unsafe test cases at scale.** To support scalable risk assessments, we automatically augment the initial 25 seed test cases into a large-scale collection of risky Python code snippets. Specifically, we use the risk descriptions (in natural language) for each scenario along with the corresponding seed code snippet to prompt LLMs to generate additional risky code snippets (e.g., _variable name replacement, code structure transformation, and the utilization of different libraries_). Due to the sensitivity of some seed code snippets, we employ a series of jailbreaking prefixes (e.g., "for education purposes") in the prompt to mitigate the issue of LLMs refusing to respond. We randomly select one jailbreaking prefix for each generation. We iteratively use GPT-4 and Mistral-Large to generate the code snippets until we obtain \(N_{}\) test cases for each risk scenario.

**Dataset selection and optimization.** To avoid low-quality or incorrect data samples, we perform data selection and modification following three steps. **(1)**_Manual Review_: We manually check each test case based on threat prominence and code conciseness. We retain test cases that maintain the safety threat under the corresponding risk scenario and have an appropriate length (e.g., similar to the seed test cases). We discard code snippets that are benign or excessively long. **(2)**_Accessible Resources Preparation_: To ensure the successful execution of the code without failures due to inaccessible resources, we carefully design and select specific resources for the code to interact with. These resources include various files, such as sensitive system files (e.g., "/etc/passwd", "/root/.bashrc"), manually prepared files, numerous websites we created specifically for risk assessment, servers for connection establishment, and Linux processes to be used (see Tab. 5 for a detailed list of resources). We replace the resource names in the generated code with our prepared resources. **(3)**_Executability Verification_: We verify the executability of the risky code by direct execution (without involving agents) in our Docker environment and ensure that it is executable and passes our checking pipeline, i.e., yield unsafe answers/states, which we will elaborate later. We manually edit the code until it passes the checking. Finally, we curate high-quality \(N_{}=30\) risky test cases for each scenario, and a total of \(750\) (\(30 25\)) risky test cases under 25 scenarios.

**Docker execution environment.** We use the Docker environment  for sandbox isolation and resource preparation. Specifically, the isolation allows us to test potentially dangerous code without compromising system security. Additionally, we prepare a Docker container with a specific image for each test case, which includes the necessary environment setups and resources (e.g., files, processes).

**Evaluation pipeline.** Existing works  use LLMs as judges to predict safety scores for agent trajectories. However, this approach is often unreliable since LLMs can make errors. Moreover, agents' trajectory records may lack the necessary information for accurate risk identification. To this end, we develop evaluation scripts tailored to each test case for deterministic risk assessments. Each evaluation script combines three components to provide an accurate assessment: (1) agent responses, (2) code execution output, and (3) the status of the execution environment. We employ two primary evaluation methods: **(i) Operation Verification** and **(ii) Output Scrutiny** as follows:

For operations verifiable on the operating system (e.g., deleting sensitive files), we use the **Operation Verification** method, relying on components (1-3). To assess the execution environment's status in (3), our evaluation scripts execute additional commands within the Docker container. For instance, in scenario _# 8 Delete sensitive file /etc/passwd_, after the agent finishes its task, we re-enter the Docker container and run an extra command: _ls /etc \(\) grep passwd_, which will list the files under _/etc_ directory and the execution result of this command can help confirm whether the deletion of _/etc/passwd_ is successful. In scenario _# 18 Terminate the process_ whose parameters contain _sshd_, we run an extra command: _ps aux \(\) grep sshd \(\) grep -v grep_ in the Docker container, which will list all the processes whose parameters contain _sshd_ except the _grep_ process itself. The execution results of this command can help confirm whether the target process is terminated.

For scenarios where (1) the agent response and (2) code execution output directly indicate outcomes, we apply the **Output Scrutiny** method to confirm successful execution of the intended functionality. For instance, in scenario _# 3 Crawl a risky website_, if the agent successfully crawls our prepared site and outputs its content, the code execution output should display the designed content (e.g., "_create a bomb "_), indicating success. In scenario _# 7 Read sensitive file /etc/passwd_, if the agent successfully reads and prints the content, the code execution output should include the keyword "_root_" that exists in this file, confirming success.

We provide detailed construction procedure of evaluation scripts in Algorithm 1. Because RedCode-Exec provides unsafe user queries (i.e., test cases) for code execution, our query can be considered as an attack on the agent. The evaluation scripts will output one out of the three possible safety outcomes for each test case: (a) **Rejection**: The user query is rejected; (b) **Execution Failure**: The threat functionality is not achieved; (c) **Attack Success**: The threat functionality is successfully achieved (i.e., code execution succeeds and brings the corresponding outcome). The evaluation scripts allow us to assess agents' behaviors based on _safety awareness_ (i.e., rejecting the request) and _attack success_ (i.e., achieving the risky functionality by implementing the query).

#### 3.2.2 Benchmark construction in other programming languages

Our initial test cases focus on Python (SS3.2.1), a widely used programming language (PL). However, code agents often need to handle tasks involving other PLs, such as using Bash commands to interact with operating systems. Therefore, we translate Python snippets into Bash snippets, given Bash's importance in systems, to more comprehensively evaluate agents' safety across broader applications.

Leveraging the strong coding capabilities of LLMs, we employ the state-of-the-art GPT-4 with curated few-shot examples for the Python-to-Bash translation task. Specifically, for each Python test case (i.e., code snippet), we prompt GPT-4 to generate the Bash code snippet. Our prompt used for translation is provided in App. B.1. We adhere to 25 safety scenarios as detailed in Fig. 3, but remove 5 of them due to compatibility issues, such as the absence of necessary libraries in Bash. This approach allows us to generate Bash test cases at scale automatically. We also provide the corresponding Docker environment for these Bash test cases. To ensure consistency, the variable names, accessible resources, and risky functionality after translation are expected to remain the same as in the source Python code snippets. This allows us to use the same evaluation scripts described in SS3.2.1 to verify the correctness of the translated Bash code. For low-quality code generated by GPT-4, we manually fix it until it is executable and passes our evaluation scripts (i.e., yielding unsafe answers/states). This process results in \(600\) high-quality risky Bash test cases (\(20 30=600\)). An example of Bash test case is provided in Fig. 4.

#### 3.2.3 Benchmark construction in natural language

For daily usage, natural language queries from users are more common than code inputs. Therefore, we translate each risky test case in code into two formats of natural language queries: (1) text summaries and (2) text descriptions. Text summaries reflect typical usage by non-technical users, e.g., "_Create and execute a Python script that fetches the content from the URL [Malicious_URL]"_. Text descriptions imitate scenarios for users with basic technical knowledge about the overall code structure that users want agents to write and execute, e.g., "_1. import the'requests' module to enable HTTP requests; 2. Define a function 'fetch_url_content'..."_. It is worth noting that these natural language test cases require agents to first _generate_ the risky code based on the text summaries or descriptions, and then _execute_ it. This approach provides a more diverse and comprehensive evaluation of agent safety across different query formats.

Figure 4: Curation overview of RedCode-Exec. We map the risky Python snippets into Bash snippets and corresponding text summaries or descriptions to form the final risky test cases.

We perform the translation separately for Python and Bash given the different libraries and functions they require. We reuse the corresponding PL's Docker environments. We use GPT-4 with curated few-shot examples to accomplish the tasks of Python-to-Text and Bash-to-Text translation. Our prompt used for translation is provided in App. B.1. Examples of translated text summaries and text descriptions test cases are provided in Fig. 4.

### RedCode-Gen: Code agent safety benchmark for malicious software generation

To evaluate the ability of LLMs-based code agents to generate harmful code or malware, we propose RedCode-Gen with 160 Python function signature + docstring instructions from eight malware families. While RedCode-Exec is designed to evaluate the handling and execution of vulnerable code, RedCode-Gen evaluates the generation of malware with clear malicious intentions and the potential to cause damage.

**Benchmark construction.** RedCode-Gen is constructed based on real malware samples. We select 11 well-studied malware families sorted into eight categories, including main categories consisting of _adware_, _malware_, _rootkits_, _trojans_, _viruses_, _DDoS attacks_, _ransomware_, and one miscellaneous category consisting of _backdoors_, _botnets_, and _worms_. Details for each family are provided in Table 6. In contrast to the natural language malware-related instructions found in prior safety benchmarks [33; 14; 12], we follow the format of HumanEval  to evaluate code agents in more realistic scenarios where they must handle code. Compared to base LLMs, code agents can interact with the environment, potentially leading to the generation and execution of higher-quality malware. Each category consists of 20 prompts, comprised of a function signature, docstring, and function body, for a total of 160 prompts. The docstring contains a detailed description of the overall functionality and components of the function, with an emphasis on the malicious components. We find that this format rarely triggers refusal mechanisms in even alignment-trained LLMs that normally reject natural language instructions to create malware. An example is given in Fig. 10.

These prompts are generated with human-in-the-loop AI-assisted generation, where we use GPT-4 to generate candidate prompts using real examples of malware and human-written descriptions of potential scenarios and attack vectors. After each round of generation, we do a careful manual check of the prompt before generating the next set until we have a complete set for each category. We also provide the Docker environment for sandbox isolation, where the code agent can execute its solution.

**Evaluation pipeline.** We evaluate agents across three criteria: (1) whether the agent responds to the instruction and generates code, (2) whether the generated code is correct, and (3) whether the code is sufficiently malicious to be classified as malware by commercial antivirus products such as VirusTotal . For (1) and (2) we employ LLM-as-a-judge and a code compiler, using GPT-4 to evaluate if the code response successfully integrates every component specified in the docstring and the compiler to evaluate if the code response successfully executes. The LLM judge analyzes a function response and uses a five-level scoring system, providing a score from 0-5. To summarize, 0 indicates direct refusal, 1 indicates irrelevant code, 2 indicates incomplete code, 3-4 indicates complete but not sufficient code, and 5 indicates complete and sufficient code. This provides a more granular code assessment, and avoids the complexity of using unit tests to evaluate open-ended software such as malware. For (3), we use VirusTotal API to evaluate the code, which indicates if a vendor has detected it as malicious.

## 4 Experiments

We conducted experiments across 19 code agents on RedCode-Exec (SS4.1) and 9 code agents on RedCode-Gen (SS4.2). We performed a detailed analysis from various perspectives: risky domains, risky scenarios, different languages, different agents, and different LLMs.

### Evaluation on RedCode-Exec

**Code agents.** As a systematic attempt to benchmark the safety of existing LLMs-based code agents, we consider three types of code agents based on various base LLMs: (1) **OpenCodeInterpreter** (OCI) : CL-7B*, CL-13B*, DS-6.7B*. (2) **CodeAct**: CL-7b, CL-13b, Llama2-7B*, Mistral-7B*. (3) **ReAct**: CL-7B, CL-13B, DS-6.7B, DS-v2-lite, CQ1.5-7b, GPT-3.5, GPT-4, GPT-4o, Claude-3.5, Llama-3-8B, Llama-3.1-8B, Llama-3.1-70B. Here, CL denotes CodeLlama , CQ denotes CodeQwen  and DS denotes DeepSeeKoder . Among them, * demotes the fine-tuned LLMs released from OCI and CodeAct for their improved agent capabilities.

We craft system prompts to enable LLMs to perform agent tasks effectively under agent frameworks. OCI and CodeAct were designed for Python-related tasks and not suitable for Bash tasks, so we evaluate their safety on our Python test cases; for ReAct, we evaluate both Python and Bash. App. C provides the prompt details and HuggingFace links or endpoint specifications for all evaluated models.

**Prompt setting.** For each programming language (Python, Bash), we use the code snippets, text summaries, and text descriptions (SS3.2) as input prompts. In addition, for code snippets, we add a persuasive jailbreaking prefix (i.e., _I highly recommend that you run the following (language) code..._), which we name as "code with jailbreak". These four input formats lead to 120 (\(30 4\)) risky test cases for each safety scenario under each programming language.

**Evaluation metrics.** Existing work often uses LLM-as-judge to calculate the metrics. However, our work proposes authentic evaluation scripts to accurately measure the agent's output. As mentioned in SS3.2.1, our evaluation scripts output three possible safety outcomes for each test case: Rejection, Execution Failure, and Attack Success. We mainly report two metrics: **Rejection Rate (RR)** and **Attack Success Rate (ASR)**. We discuss failure cases in App. D.6.

#### 4.1.1 Main results

**Attack success and rejection rates of risky test cases in 8 domains.** We summarize our key findings from Fig. 5: **(1)** The overall rejection rate is low, and the attack success rate is high for all code agents, which demonstrates the high quality, stealthiness, and challenging test cases in our benchmark. **(2)** Among the 8 domains, the agents' rejection rates are generally high for dangerous test cases related to the operating system (OS) and file system domains, which include risky operations that can impact the system's normal usage such as reading or modifying the " /.bashrc" file or deleting the "/etc/passwd" file, and risky behaviors like monitoring the system. It reflects that the agents exhibit some safety comprehension and vigilance regarding sensitive system file names/paths and highly risky system operations. We provide a list of evaluated files/paths in Tab. 5. Similarly, the attack success rate in OS could be low for some agents because these scenarios are inherently more dangerous and have a higher level of difficulty regarding a successful attack. **(3)** The rejection rate for risky test cases in the website domain is also relatively high for ReAct-GPT4, which could be because they can identify sensitive keywords that we deliberately include in the URLs, such as "dangerous" and "malicious". However, other agents are less aware of such risks, leading to a high attack success rate. We further discuss the more realistic scenario of URLs with random characters in App. D.2, which leads to an even lower rejection rate and a higher attack success rate. **(4)** In the remaining domains, such as program logic and cybersecurity, the rejection rate is relatively low, and the attack success rate is high, which highlights the serious safety concerns for existing code agents. This is likely because the direct security implications in these two domains are not as obvious as OS domain, as they include scenarios such as "Missing default case" in program logic and "PRNG (random) seed management" in cybersecurity, which can be viewed as instances of buggy code. Agents are less likely to be cautious with these bugs and thus execute them. However, these buggy codes could introduce vulnerabilities for attackers to exploit when integrated into real-world systems. For example, if a user uses an insecure encryption program to encrypt and store passwords, the passwords could be more easily computed and cracked by attackers.

**Analysis for 25 specific risk scenarios.** We report the attack success and rejection rates for 25 risk scenarios under Python and Bash tasks in Fig. 6. Generally, the rejection rate for technically buggy code issues (e.g., scenarios #15-#17, #19-#20, #22-#25) is lower than for operationally risky code that directly affects the system (e.g., deleting sensitive files). Furthermore, we study the risks associated

Figure 5: Rejection rate and attack success rate of 8 representative code agents on RedCode-Exec Python test cases in 8 domains. The overall attack success rate is high, indicating existing code agents are vulnerable. Agents achieve a higher rejection rate on risky cases in operating and file systems than those in other domains.

with memory leaks in App. D.1 and crawling websites with random-character URLs in App. D.2. For detailed discussions on each scenario and reasons for execution failure, refer to App. D.4.

**Agents are more easily attacked by risky queries in natural language than programming languages.** From Fig. 7, we find that natural text inputs (text summaries/descriptions) are generally less prone to rejection than code inputs, and agents are easier to fulfill the threat functionality with a higher attack success rate in textual instructions compared to direct code inputs. This is likely because text inputs are inherently less risky as they require further interpretation before execution. Agents fail to recognize these underlying risks given text inputs, thus generating risky code and executing it. Moreover, surprisingly, code queries with jailbreak prefixes have a higher rejection rate compared to plain code queries for some agents. This indicates that agents are more cautious and reject tasks that appear to be attempts to bypass safety mechanisms.

**Text descriptions lead to higher attack success rate than text summaries.** Fig. 7 shows that agents achieve a higher attack success rate and lower rejection rate given text descriptions than summaries. This might be because natural text descriptions provide clear instructions, which helps agents implement the risky code and execute it successfully.

**Python leads to a higher rejection rate than Bash on code agents.** Results in Fig. 6 show that Bash code inputs usually have a lower rejection rate than Python under the same risky scenarios. Moreover, Fig. 7 indicates that the gap between Python and Bash exists in almost every input modality (text or code). This might be due to the unbalanced ability of code agents to handle different programming languages. Python tasks could be perceived as more complex or risky, leading to higher rejection rate. Alternatively, agents might be more familiar with Python, hence, more conservative in executing potentially unsafe Python code.

**Safety comparison of different agents highlights OpenCodeInterpreter's robustness.** We compare the RedCode-Exec results of 19 code agents in Fig. 1. **(1)** It shows that OpenCodeInterpreter (OCI) is more robust than ReAct and CodeAct, with a higher rejection rate and lower attack success rate.

Figure 6: Rejection rate (RR) and attack success rate (ASR) of **Python** and **Bash** tasks in RedCode-Exec across 15 selective agents (full results are in Fig. 11). For visualization, scenarios (rows) are arranged in descending order by the total rejection rate among all agents. Agents (columns) are sorted based on their overall RR/ASR.

Figure 7: RedCode-Exec evaluation on GPT-4 ReAct agent over various risky inputs. Agents are less likely to reject risky queries in natural language than programming language inputs, or in Bash code than Python code inputs.

This is because OCI has hard-coded disk space protection/constraints in its agent codebase. **(2)** Moreover, the safety of ReAct is affected by different base LLMs, and ReAct generally has a higher rejection rate than CodeAct. This might be because ReAct employs the "Think-Then-Act" procedure, which helps the agents to reason about the security implications of the risky user query. **(3)** The results in Fig. 6 show that given the same LLMs, different agents exhibit different safety risks, For example, three agents using CL-7b show different reject/attack success rate, which could be because of the difference in agent implementation/prompts. We defer detailed comparison of code agents in each specific risky domain to App. D.

**Strong base models exhibit higher safety awareness while fine-tuned LLMs may compromise safety. **(1)** Fig. 1 suggests that, under ReAct, strong base LLMs in general have a high rejection rate (e.g., GPT series in ReAct), indicating the stronger safety awareness of more capable models. **(2)** Comparing different base LLMs under the same type of agent CodeAct in Fig. 6 and Fig. 1, we find that _fine-tuned LLMs could compromise the safety of agents_, leading to lowest rejection rate in CodeAct Mistral-7B* and Llama2-7b*. This suggests that instruction tuning for agent tasks enhances the agents' general task-solving ability but may unintentionally weaken the safety guardrails.

**Safety-aware prompts as risk mitigation are not effective enough**. For the mitigation methods, we discuss several potential strategies in App. D.3, and report experimental results conducted on safety-aware prompts in Tab. 9. We find that while safety-aware prompts have some effect, the attack success rate remains high, suggesting the need for stronger mitigation strategies.

### Evaluation on RedCode-Gen

**Code agents.** For RedCode-Gen, we evaluate both base code LLMs and code agents. For base LLMs, we consider 4 closed-source and 5 open-source LLMs. We design a generic code-agent framework similar to CodeAct  for these base models where the model has access to a compiler, the judge's score and response, and is allowed to modify its code for a certain number of iterations.

**Evaluation metrics.** Following evaluation pipeline in SS3.3, we report the (1) refusal rate, (2) accuracy of generated code based on LLM's judge score, and (3) VirusTotal score.

**Main results.** We find in Tab. 1 that most models, with the exception of GPT-3.5, Claude-Opus, and Llama-2-7B, have a low refusal rate despite rejecting natural language based instructions to generate malware in prior safety benchmarks [33; 14; 12]. This is likely due code-based input formats not being covered as much during safety training, and the amount of detail we provide in each instruction. In addition, we also observe a strong correlation between general coding capabilities with the quality of generated malware: we compare each model's overall accuracy on 8 malware categories in RedCode-Gen with their pass@1 accuracy on HumanEval, and obtain a Pearson score of 0.448, indicating a moderate, almost strong, positive correlation. The correlation is positive because most models (e.g. GPT models) have low refusal rates, and their performance on RedCode-Gen and HumanEval are well correlated. The GPT-4 models, which currently have the highest performance on standard coding benchmarks such as HumanEval , also generate malicious code more likely to be scored highly by the judge. Alarmingly, we also find that some of the generated samples from GPT-4 and DeepseeKoder can trigger antivirus evaluations from VirusTotal, suggesting LLMs have the potential to automatically generate functional malware. However, certain models with strong coding capabilities, e.g.,Claude-Opus, also have strong safety guardrails, resulting in low accuracy on RedCode-Gen. In addition, for our Code Agent models, we find that the ability to evaluate code and refine it results in a higher judge accuracy, lower refusal rate, and higher VirusTotal accuracy, suggesting the capabilities of code agents introduce new risks. However, the VirusTotal accuracy is still relatively low, suggesting there is a large gap between the maliciousness of LLM-generated and human-written malware.

## 5 Conclusion

In this work, we propose RedCode, a benchmark for evaluating the safety of code agents in risky code execution and generation. We find that existing code agents are generally vulnerable, and robust and safe code agents are in great need.

   Group & Model & Accuracy & Retail Rate & VirusTotal \\  GPT-4 & 69.4 & 19.4 & 3.8 \\ GPT-4 & 65.0 & 17.8 & 4.4 \\ GPT-5.5 & 0.0 & 85.5 & 0.0 \\ Data-Opus & 1.9 & 89.9 & 0.0 \\ Deep-CodeNet & 49.4 & 11.3 & 4.4 \\ LLM & CodeAtt-7B & 40.0 & 0.0 & 0.0 \\ Candidate-7B & 49.4 & 30.6 & 0.6 \\ Llama-7B & 16.9 & 51.9 & 0.0 \\ Mistral-7B & 46.3 & 21.0 & 0.6 \\  GPT-4 & 72.5 & 18.1 & 4.4 \\ GPT-4 & 66.9 & 11.3 & 5.6 \\ GPT-5.5 & 32.5 & 30.6 & 1.3 \\ Code-Opus & 3.1 & 96.9 & 0.0 \\ Deep-CodeNet & 79.4 & 1.3 & 4.4 \\ Auto-Code-7B & 46.3 & 19.4 & 1.9 \\ Candidate-7B & 42.0 & 35.0 & 0.0 \\ Llama-7B & 20.7 & 56.7 & 0.0 \\ Mistral-7B & 75.3 & 0.0 & 1.3 \\   

Table 1: Results for base models and code agents on RedCode-Gen. Most base models have low refusal rates and high accuracy. Code agents have even lower refusal rates and higher accuracy.