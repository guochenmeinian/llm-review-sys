# Mind the Gap Between Prototypes and Images in Cross-domain Finetuning

Hongduan Tian1, Feng Liu2, Zhanke Zhou1, Tongliang Liu3, Chengqi Zhang4, Bo Han1

Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).

###### Abstract

In _cross-domain few-shot classification_ (CFC), recent works mainly focus on adapting a simple transformation head on top of a frozen pre-trained backbone with few labeled data to project embeddings into a task-specific metric space where classification can be performed by measuring similarities between image instance and prototype representations. Technically, an _assumption_ implicitly adopted in such a framework is that the prototype and image instance embeddings share the same representation transformation. However, in this paper, we find that there naturally exists a gap, which resembles the modality gap, between the prototype and image instance embeddings extracted from the frozen pre-trained backbone, and simply applying the same transformation during the adaptation phase constrains exploring the optimal representations and shrinks the gap between prototype and image representations. To solve this problem, we propose a simple yet effective method, _contrastive prototype-image adaptation_ (CoPA), to adapt different transformations respectively for prototypes and images similarly to CLIP by treating prototypes as text prompts. Extensive experiments on Meta-Dataset demonstrate that CoPA achieves the _state-of-the-art_ performance more efficiently. Meanwhile, further analyses also indicate that CoPA can learn better representation clusters, enlarge the gap, and achieve minimal validation loss at the enlarged gap. The project repository of CoPA is available at:[https://github.com/tmlr-group/CoPA](https://github.com/tmlr-group/CoPA).

## 1 Introduction

Cross-domain few-shot classification  (a.k.a. CFC) aims at learning to perform classification on tasks sampled from previously unseen domains with only a few labeled data. Recent works , which aims at fast adapting a light model on top of the frozen pre-trained backbones for feature selection or transformation, has shown impressive generalization capability on CFC tasks.

Typically, URL , a representative work of this genre, proposes to fast fine-tune a linear transformation head on top of a frozen pre-trained backbone with the nearest centroid classifier loss . The ultimate goal of URL is to project embeddings into a space where classification can be performed via measuring the similarity between representations of prototypes and image instances. Technically, an _assumption_ implicitly adopted in URL is that the prototype and image instance embeddings share the same representation transformation (see the upper subfigure of Fig. 2) during the adaptation phase. Intuitively, however, prototype and image instance embeddings depict different levels of information. Specifically, the embeddings of an image instance encode the _instance-level_ information from the given image, while the embeddings of a prototype contain some abstract and _higher-level_ information that is commonly shared among image instances within the corresponding class yet discriminative from other classes. Thus, applying the same representation transformation as URL might constrain models from exploring the optimal representations respectively for prototypes and image instances.

In this paper, we first conduct an analysis to validate the intuition that prototypes and image instances describe different levels of information. Specifically, following Liang et al. , we visualize the distributions of prototype and instance embeddings extracted from a frozen pre-trained backbone and measure the Euclidean distance between the centroids of the normalized prototype and image instance embeddings. According to Fig. 1(a), the UMAP  visualization indicates that there _naturally_ exists a gap, which resembles the modality gap demonstrated by Liang et al.  in visual language models (VLMs, e.g. CLIP ), between prototype and image instance embeddings. However, after applying the same representation transformation to both prototype and image instance embeddings as done in URL , we observe that the gap is numerically shrunken (see Fig. 1(b)). Moreover, by visualizing the cluster distribution of data samples (see Figs. 3(b) and 3(c)), we further find that applying the same representation transformation to both prototype and image instance embeddings may fail to learn compact and clear representation clusters. With all these empirical results taken into consideration, two aspects can be summarized here: (1) there does exist a gap between the prototype and image instance embeddings extracted from the frozen backbone; (2) the shared representation transformation tends to shrink the gap between the learned prototype and image instance representations and may potentially result in the failure of learning clear and compact representation clusters for classes in the task. Our further investigation into the URL framework in Section 3.3 indicates that applying the same transformation, on the one hand, removes the discriminative information in gradients. On the other hand, our empirical results further reveal that applying the shared transformation constrains learning representations where the gap between prototypes and image instances is preserved.

Previous work  has demonstrated that contrastive loss helps preserve the modality gap and further contributes to improving the generalization performance of downstream tasks. Thus, in this paper, we propose a simple yet effective method, _contrastive prototype-image adaptation_ (CoPA), to adapt different transformations for prototype and image embeddings similarly to CLIP  by treating prototypes as text prompts. In this way, the discriminative information in gradients can be preserved in different parameters, and the optimal transformations, where the gap between prototype and image representations is held, can be explored in the function space constructed by the two transformations.

Extensive experiments on the representative Meta-Dataset benchmark  under several task settings demonstrate that CoPA can achieve the _state-of-the-art_ performance with less time consumption and learn more clear and compact representation clusters. In addition, further analysis reveals that the global minimum of the validation loss is achieved at the enlarged gap, which implies the enlargement of the gap is a process of exploring representation distributions for better generalization performance.

**Our Contribution.** In this paper, our contributions can be summarized in the following 4 parts:

* We validate there _naturally_ exists a gap, which resembles the modality gap, between prototype and image instance embeddings extracted from a frozen backbone in Section 3.2.
* Our analyses in Section 3.2 reveal that the shared representation transformation shrinks the gap between prototype and image instance representations. Our investigation into the URL in Section 3.3 indicates that applying the same transformation removes the discriminative information in gradients and constrains learning representations where the gap is preserved.
* To solve this problem, in Section 4, we propose a simple yet effective method, _contrastive prototype-image adapation_ (CoPA), to adapt two different transformations for prototypes and image instances as done in CLIP via substituting text prompts with prototypes.
* Extensive results reveal that CoPA can achieve the _state-of-the-art_ performance on Meta-Dataset  (Section 5.1), enlarge the gap to achieve the global minimum of the validation loss (Section 5.2), and learn more compact image representation clusters (Section 5.2).

Figure 1: **There _naturally_ exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap.** Fig.(a) shows that there naturally exists a gap, which resembles the “modality” gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.

Preliminary

Few-shot Task Generation.Consider a _meta_ dataset \(=\{_{i}\}_{i=1}^{n}\), where \(n\) is the number of sub-datasets in \(\). The sub-datasets in meta dataset satisfy that \(_{i}_{j}=\), for \(_{i},_{j}\). Each sub-dataset \(_{i}\) is split into three _disjoint_ subsets, which are training set \(_{i}^{}\), validation set \(_{i}^{}\) and test set \(_{i}^{}\). Consistent with typical supervised learning paradigm, the meta dataset \(\) is divided into two disjoint parts respectively for model training and evaluation. The datasets, in which training sets are used for pre-training, are called _seen_ domains while the remaining datasets, which are only available for evaluation, are called _unseen_ domains. Thus, the meta dataset can be alternatively expressed as \(=^{}^{}\), \(^{}^{}=\), where the notations \(^{}\) and \(^{}\) respectively denote the seen and the unseen domains.

Typically, a cross-domain few-shot classification task with _vary-way vary-shot_ is randomly sampled in an episodic way. To be specific, at the beginning of each learning episode, a new task \(=\{_{},_{}\}\) is sampled from a specific sub-dataset of the meta dataset \(_{i}\), where \(_{}=\{(_{i}^{},y_{i}^{})\}_{i= 1}^{|_{}|}\) denotes the _support set_ of the sampled task, which is used for model training and \(_{}=\{(_{i}^{},y_{i}^{})\}_{i= 1}^{|_{}|}\) denotes the _query set_ of the task that is used for model evaluation.

Problem Formulation.In this paper, we follow the pre-training framework adopted in previous works [12; 34; 29; 30] and build our method upon a frozen pre-trained backbone.

Consider a _frozen_ pre-trained backbone \(f_{^{*}}:^{d_{}}^{d_{}}\) that is parameterized with a set of optimal parameters \(^{*}\) and a trainable fine-tuning module \(h_{}:^{d_{}}^{d_{}}\) that is parameterized with \(\). Given a set of support data \(_{}=\{(_{i},y_{i})\}_{i=1}^{|_{}|}\), we then can obtain a set of representations \(=\{(_{i},y_{i})\}_{i=1}^{|_{}|}\), where \(_{i}=h_{}(f_{^{*}}(_{i}))\), from the frozen pre-trained backbone and the fine-tuning module. The ultimate goal of the pre-training framework is learning a set of optimal task-specific parameters \(^{*}\) from the given support data so that better generalization performance can be achieved on the query data in the task. To be specific, following previous works [34; 29], the learning problem is formulated as minimizing a _nearest centroid classifier_ (NCC) loss :

\[()=-_{}|}_{i=1}^{| _{}|}(p(y_{i}=c|_{i};)). \]

where \(p(y=c|;)=,_{c})}}{_{^{}}e^{ d(,^{})}}\) denotes the likelihood of a sample \(\) belonging to its class \(c\), \(d(,)\) denotes a measure, such as negative Euclidean distance function or cosine similarity function, to describe the similarity between samples and prototypes. In this paper, we follow URL  and adopt cosine similarity as the measure. Moreover, \(_{c}\) denotes the prototype of class \(c\), which is generated with all available samples in the class \(c\) and formulated as: \(_{c}=_{c}|}_{_{c}}, _{c}=\{_{i}|y_{i}=c\}\).

## 3 Revisit the Previous Adaptation Strategy

In this section, we first revisit the adaptation strategy applied in the previous work and uncover an implicitly adopted assumption that the prototype and image instance representations are transformed with the same (linear) transformation. However, prototypes intuitively depict different levels of information from image instances. Then, we empirically demonstrate that there naturally exists a gap between prototype and image instance embeddings. Our further investigation into URL reveals that applying the same transformation to prototypes and images removes the discriminative information in gradients and constrains learning compact representation clusters where the gap is preserved.

Figure 2: The **upper** subfigure shows the URL pipeline which applies the same transformation to both prototype and image instance embeddings. The **bottom** subfigure shows the pipeline of CoPA, which tries to adapt two different representation transformation heads respectively for prototypes and image instances in the way of CLIP via substituting text prompts with prototype embeddings.

### An Assumption of Previous Adaptation Strategy

In previous works [29; 30] that are based on prototypes, cross-domain few-shot classification is typically formulated as a maximum likelihood estimation problem as mentioned in Eq. (1). The ultimate goal of this problem is to learn a set of representations such that classification can be performed by measuring the similarity between representations of image instances and prototypes.

Typically, the prototype of each class is calculated with all instances within the class (see Section 2 for concrete details). Assume that the transformation is defined as a linear transformation parameterized with \(^{d_{} d_{}}\), then the calculation of the prototype \(\) of the class set \(\) can be formulated as:

\[=|}_{}}())}_{}=|}_{}f_{^{*}}())}_{}. \]

Since the representation transformation is usually linear, the average of image instance representations (the second part) is equivalent to the representation transformation of prototype embeddings (the third part). Thus, it is easy to observe that an assumption is implicitly adopted in the URL framework:

_Instance-level and prototype-level embeddings share the same representation transformation._

Intuitively, compared with image instance representations, the prototype representations contain higher-level information which is commonly shared among the instances within the class yet discriminative from that of other classes. Thus, there seems to exist a gap between prototypes and images. Such an intuition motivates us to take a further step to explore whether such a gap exists and what effect the shared representation transformation imposes on such a gap. To this end, we perform a series of analyses on the prototype and image instance embeddings and their corresponding transformed representations. In addition, we also investigate the mechanism of the adaptation strategy.

### Empirical Analysis on Prototype and Image Instance

In this section, we conduct an analysis on the prototype and image instance embeddings and their corresponding representations obtained from the same representation transformation. Specifically, the embeddings are extracted from a frozen pre-trained ResNet-18  backbone, and the representations are transformed from the embeddings with a shared linear transformation model as done in URL .

**"Modality" Gap between Prototype and Image Instance.** According to Liang et al. , there exist modality gaps between different modal embeddings, such as text and image embeddings extracted from visual language models (e.g. CLIP), and preserving such modality gaps facilitates improving the downstream performance. We notice that the prototypes, to some extent, play the same role as text prompts. Specifically, both text prompts and prototypes depict the common concepts shared among all images within the corresponding class yet discriminative from other classes. This observation motivates us to validate whether such a "modality" gap exists between prototypes and image instances.

In this section, an empirical analysis is performed on the validation set of ImageNet . Following Liang et al. , we define the gap as the difference between the centroids of _normalized_ prototype and image instance representations (embeddings) \(:=_{}|}_{i=1}^{|_{ }|}_{i}-}_{j=1}^{N_{C}}_{j}\), where \(|_{}|\) denotes the size of support data representation set \(=\{_{i}\}_{i=1}^{|_{}|}\) and \(N_{C}\) denotes the number of classes. More detailed validation results on other datasets are available in Appendix B.

We visualize the distributions of prototype and image instance embeddings with UMAP  in Fig. 1(a). As shown in the figure, it is easy to observe that the prototype and image instance embeddings are located in two completely separate regions of the feature space, and there naturally exists a gap between prototype and image instance embeddings. Moreover, to verify the general existence of such a gap, we conduct an analysis with 600 randomly sampled tasks on each of all 8 seen domain datasets in Meta-Dataset. The results are reported in Table 3 in Appendix B. The results reveal that the gap also generally exists between prototype and instance embeddings in other datasets.

**Larger Gap Facilitates Generalization.** To further figure out the property of the prototype-image gap, we conduct the same embedding shift experiment as done in previous work . Specifically,we manually shift prototype and image instance embeddings by scaling the gap between two sets of embeddings/representations to narrow or enlarge the gap and observe the change of validation loss. The results are reported in Fig. 3(a). The X-axis depicts the size of the gap where the negative value means the position of prototype and image embeddings are mutually replaced. From the figure, it is easy to observe that the validation loss fails to achieve its global minimum at the original gap between prototype and image instance embeddings. Interestingly, we can further observe that the global minimum can be achieved by slightly enlarging the gap. This is consistent with the phenomenon observed by Liang et al. . Thus, an intuitive insight is that appropriately enlarging the gap between the prototypes and image instances contributes to achieving better generalization performance.

From our perspective, the phenomenon that the larger gap facilitates improving the generalization performance can be attributed to two aspects. On the one hand, each prototype is generated with all instances within the class. Thus, data samples are naturally closer/more similar to the prototypes of their own class. This potentially results in slight overfitting. Thus, when the gap is slightly enlarged, it is equivalent to decreasing the similarity between prototypes and images. Such an enlargement, in turn, helps alleviate the overfitting and further improve the generalization performance on downstream tasks. On the other hand, as images and prototypes describe different levels of information, good generalization performance can only be achieved when the embeddings are well aligned. Thus, enlarging the gap can also be seen as exploring the optimal distributions of embeddings for alignment.

**Drawbacks of Shared Transformation.** In order to determine the effect of the shared representation transformation on representation learning, we further analyze the prototype and image instance representations obtained from the existing SOTA baseline, URL . The visualization results are respectively presented in Figs. 1(b) and 3(c). According to Fig. 1(b), we can observe that the gap between prototype and image instance representations is numerically shrunken after applying the same transformation to both prototype and image instance embeddings. Meanwhile, we can also observe from Fig. 3(c) that URL fails to learn well-clustered representations for classes in the task. All these results, to some extent, demonstrate the drawbacks of applying the shared transformation.

### Further Exploration on Shared Transformation

In this section, we conduct a further investigation to explore the effect of the shared transformation.

**Theorem 3.1**.: _Let the measure \(d(,)\) be the cosine similarity function. Given a set of normalized finite support data representation \(=\{(_{i},y_{i})\}_{i=1}^{n}\), where \(||||_{2}=1\) for \(\) and \(N_{C}\) classes are included, then we have a lower bound of the NCC-based loss in Eq. (1):_

\[()-_{i=1}^{n}_{i}^{}_{c}+ _{i=1}^{n}_{^{}}_{i}^{ }^{},\]

_where \(^{}\) is an independent copy of samples in \(\), \(_{c}\) denotes sets of sample representations \(_{c}=\{_{i}|y_{i}=c\}\), and \(\) is a constant that satisfies \(0<1/(N_{C}|_{j}|)\) for \( j\)._

Figure 3: **(a). The global minimum validation loss is achieved when the “modality” gap is enlarged.** Fig. (a) depicts the validation loss landscape w.r.t the changes of the “modality” gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. **(b)-(c). The shared representation transformation fails to learn compact instance representation clusters.** According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).

The complete proof is available in Appendix G.1. Theorem 3.1 provides a lower bound of Eq. (1). According to the bound, if we adopt the right side as the surrogate loss, solving Problem (1) is then equivalent to simultaneously maximizing the similarities between instances and their corresponding prototypes and minimizing the similarities among all samples. Moreover, since \(_{i}^{}_{j}<_{i}^{}_{i}\) for \( j i\), the second term can then be approximated as \(_{}^{}\) by omitting the similarity term between different image instance representations. Then, we can reformulate Problem (1) as:

\[(_{},_{})= -_{}|}(f_{^{*}}( )_{}(YY^{}f_{^{*}}()_{})^{} )+_{}|}(f_{^{*} }()_{}_{}^{}f_{^{*}}()^{ }),\]

where \(^{|_{}| d_{}}\) and \(Y^{|_{}| N_{C}}\) respectively denote the support image instances and the corresponding one-hot labels, \(_{}^{d_{} d_{}}\) and \(_{}^{d_{} d_{}}\) denote the model parameters of linear transformation heads respectively for prototype and image instance embeddings, \(()\) denotes the matrix trace operation. \(YY^{}f_{^{*}}()^{|_{}| d _{}}\) denotes the prototypes which are expanded to the same size of instance embeddings. In this way, the gradients w.r.t. \(_{}\) and \(_{}\) are:

\[_{_{}}(_{},_{}) =-_{}|}_{}^{}f_ {^{*}}()^{}YY^{}f_{^{*}}(),\] \[_{_{}}(_{}, _{}) =-_{}|}_{}^{}f_ {^{*}}()^{}YY^{}f_{^{*}}()+_{}|}_{}^{}f_{^{*}}()^{ }f_{^{*}}().\]

According to the gradients above, we can observe that the gradients of \(_{}\) depict the similarity between instance representations and prototype embeddings while the gradients of \(_{}\) depict the similarity between prototype representations and instance embeddings. However, in the case of the shared transformation (i.e., \(_{}=_{}=\)), such differences are removed. Thus, the shared transformation may potentially drop the discriminative information in gradients during the adaptation phase. Meanwhile, the shared transformation also constrains the complexity of the function space, and in turn, prevents exploring the optimal representation distributions for both prototypes and images.

**Theorem 3.2** (The shared transformation).: _Consider a support data set \(_{}=\{(_{i},y_{i})\}_{i=1}^{|_{}|}\) composed of \(N_{C}\) classes and a frozen pretrained backbone \(f_{^{*}}:^{d_{}}^{d}\) parameterized with the optimal parameters \(^{*}\). Let \(^{d d}\) be a shared linear transformation across the prototype and image instance embeddings. Then, we can obtain the image instance representations \(=\{_{i}\}_{i=1}^{|_{}|}=\{f_{^{*}} (_{i})_{}^{_{}|}\), and the prototype representations \(=\{_{i}\}_{i=1}^{N_{C}}\), where \(_{i}=_{i}|}_{^{}_{i}} ^{}=_{i}|}_{^{}_{ i}}f_{^{*}}(^{})\). Then we can obtain the bounds of the representation gap:_

\[m\|\|_{F}^{2}\|_{} \|_{2}^{2}\|_{}|}_{ }-}_{}\|_{ 2}^{2} M\|\|_{F}^{2}\|_{} \|_{2}^{2},\]

_where \(_{}=_{}|}_{ _{}}f_{^{*}}()-}_{b=1}^{ N_{C}}(_{b}|}_{^{}_{b}}f_{^{*}}( ^{}))\) denotes the gap between prototype and image embeddings, \(m=_{1 i d}^{2}(_{},^{i})\) denotes the minimum value of \(^{2}(_{},^{i})\), and \(M=_{1 j d}^{2}(_{},^{j})\) denotes the maximum of \(^{2}(_{},^{j})\)._

The proof is provided in Appendix G.2.1. In Theorem 3.2, we derive the bounds of the gap between prototype and image representations learned from a shared linear transformation. For simplicity, we only consider the unnormalized representations. However, in practice, the gap is calculated with normalized representations. According to the theorem, we find that the bounds are closely related to the Frobenius norm of the transformation matrix and the angle between the embedding gap vector and the column vectors of the transformation matrix.

Theoretically, it is intractable to directly analyze the upper bound of the gap between prototype and image representations. Thus, to determine the reason for the gap shrinkage, we empirically track the change of the scale \(\|\|_{F}\) on 600 randomly sampled tasks. The insight here is that the representation gap will not be larger than the embedding gap if the scale is smaller than 1.0. According to curves in Fig. 4, we observe that the scale is consistently smaller than 1.0 in all cases. Thus, a shared transformation cannot preserve the original gap between prototype and image instance embeddings.

Figure 4: The change of the scale of the upper bound of URL representation gaps during the adaptation.

[MISSING_PAGE_FAIL:7]

notes the support data samples, while \(Y^{|_{}| N_{C}}\) denotes the corresponding one-hot labels, where \(N_{C}\) denotes the number of classes, we can respectively obtain the prototype representations \(_{}=h_{_{}}(YY^{}f_{^{*}}())\) and image instance representations \(_{}=h_{_{}}(f_{^{*}}())\). Then, CoPA learns the representations by minimizing the following symmetry cross entropy objective:

\[_{_{},_{}}(_{ },_{}):= _{}(_{}_{ }^{},Y_{})+_{}(_{}_{}^{},Y_{}), \]

where \(\) is a temperature coefficient, \(Y_{}=[0,1,...,|_{}|-1]\) denotes the pseudo labels, and \(_{}\) denotes the cross-entropy loss. The complete CoPA algorithm is presented in Algorithm 1.

**Discussion.** Two aspects are worth noticing in our proposed CoPA. On the one hand, two different transformations are adopted in our proposed CoPA method. In this way, the discriminative information in gradients is preserved in different sets of parameters. Besides, introducing different transformations expands the function space, which further facilitates learning better prototype and image instance representations. On the other hand, the contrastive learning objective, which has been demonstrated to facilitate preserving the modality gap, is adopted as the learning objective. In particular, to match this objective, we modify the prototype embeddings from \(Y^{}f_{^{*}}()\) to \(YY^{}f_{^{*}}()\) so that the size of the prototype embeddings is consistent with that of image instance embeddings. Such a modification further indicates the cluster structure of the given support data set. Thus, by minimizing the symmetric cross entropy loss, the two transformations are encouraged to align the prototype and image instance representations and learn more class-specific and discriminative representations.

## 5 Experiments

In this section, we evaluate CoPA on Meta-Dataset  under both "train on all datasets" and "train on ImageNet only" settings with a series of tasks to answer the following questions: (1) Does CoPA achieve better generalization performance? (2) Does CoPA benefit from extra learning modules? (3) Why does CoPA perform better? Detailed experimental settings are available in Appendix E.

### Main Results & Analysis

In this section, we evaluate the generalization performance of our proposed CoPA method on Meta-Dataset under both "train on all datasets" and "train on ImageNet only" settings. In order to demonstrate the effectiveness of CoPA, we compare CoPA against several currently _state-of-the-art_ baselines, including fo-Proto-MAML (FP-MAML) , ALFA+fo-Proto-MAML (AFP-MAML) , CNAPs , Simple-CNAPs , SUR , URT , Tri-M , FLUTE , URL .

**Train on All Datasets.** We report the evaluation results under the "train on all datasets" setting in Table 1. As shown in the table, it is easy to observe that CoPA achieves the best performance on 9 out of 13 datasets among all approaches and outperforms URL in all cases. Specifically, compared with URL, our proposed CoPA method achieves \(0.7\%\), \(1.5\%\), and \(1.0\%\) improvements on average respectively on seen, unseen, and all domains. It is also worthwhile to notice that CoPA performs better on unseen domains. Specifically, our proposed CoPA method achieves \(3.3\%\), \(2.1\%\), and \(1.1\%\) improvements respectively on Traffic Sign, MSCOCO, and CIFAR 10 datasets. Meanwhile, we also

Figure 5: **(a). The gap between prototype and image instance representations is enlarged from 0.22 to 1.38 by CoPA. Such a phenomenon is consistent with that demonstrated by Liang et al. . (b). The clusters of image instance representations learned from CoPA. The more compact clusters reveal that CoPA learns better instance representations. (c). The validation loss achieves its global minimum at the gap learned by CoPA, which indicates that CoPA can improve the generalization performance.**

notice that CoPA achieves comparable or even better results on seen domains compared with TSA  which plugs extra learning modules into the frozen backbone for more task-specific parameters.

**Train on ImageNet Only.** The results under the "train on ImageNet only" setting are reported in Table 2. Briefly, CoPA achieves the best performance on 11 out of 13 datasets and consistently outperforms URL in all cases. On average, CoPA achieves \(0.4\%\), \(2.1\%\), and \(1.8\%\) improvements respectively on seen, unseen, and all domains. Moreover, the results also reveal that CoPA consistently performs better on unseen domains. Specifically, CoPA achieve impressive improvements on Omniglot (\(1.5\%\)), Aircraft (\(4\%\)), Birds (\(1.3\%\)), Textures (\(1.8\%\)), Quick Draw (\(3.4\%\)), Fungi (\(1.8\%\)), VGG_Flower (\(2\%\)), Traffic Sign (\(2.6\%\)), MSCOCO (\(3.1\%\)), MNIST (\(1.1\%\)), CIFAR (\(2.9\%\) & \(0.7\%\)).

**Extra Learning Modules.** We follow TSA  to plug extra trainable modules into the frozen backbone.The results are reported respectively in Table 1 and 2. According to the table, we can observe that CoPA benefits from these extra modules and achieves better performance than previous state-of-the-art approaches (e.g., TSA and TA\({}^{2}\)-Net). CoPA + TSA outperforms 11 out of 13 datasets under the "train on all datasets" setting. Similarly, CoPA + TSA consistently achieves better generalization performance on unseen domains.Specifically, CoPA + TSA achieves \(5\%\), \(2.6\%\), and \(0.8\%\) improvements respectively on Traffic Sign, MSCOCO, and MNIST datasets. Under the "train on ImageNet only" setting, CoPA + TSA achieves the best performance on 7 out of 13 datasets. Compared with TSA, CoPA + TSA achieves better performance on Birds (\(0.7\%\)), Textures (\(0.8\%\)), Fungi (\(1.5\%\)), VGG_Flower (\(0.8\%\)), Traffic Sign (\(4.5\%\)), MSCOCO (\(1.6\%\)) and MNIST (\(1.5\%\)).

### Discussion: Why CoPA Performs Better?

In previous experiments, we have demonstrated that CoPA can achieve better generalization performance. In this section, we aim to validate whether CoPA preserves the gap and learns better representations. Further, we would like to explore why CoPA performs better than previous works.

**More Results.** To verify the effectiveness of CoPA, we conduct more analyses. Firstly, we investigate the representation shift experiment to observe the gap between prototype and image representations. As shown in 5(a), in contrast to URL, the gap between prototype and image representations learned via CoPA is enlarged. Such a phenomenon demonstrates that CoPA can _hold_ the gap between prototypes and image instances. In addition, we further check the image instance representations learned with CoPA. According to Fig. 5(b), the image representations are more clearly and compactly clustered than Fig. 3(c). This case reveals that CoPA can facilitate learning more class-specific representations.

#### 5.2.1 Ablation Study

The results reported above indicate that CoPA can enlarge the representation gap and learn well-clustered image representations simultaneously. In order to further figure out why CoPA is able to improve performance, in this section, we propose to conduct ablation studies on the two important components of CoPA: the different transformations and SCE loss. The results are reported in Table 10.

**Ablation: Different Transformation Modules.** As we have discussed in Section 3.3, the shared transformation may drop the discriminative information in gradients. Thus, we propose to substitute the shared transformation with the different transformations adopted in CoPA. As shown in the table, although URL+2Heads achieves slightly better results in some cases, its performance drops in the remaining cases. By plotting the test loss curves (cf. Fig. 6), we can observe that overfitting takes place.

**Ablation: SCE Loss.** From the table, we can observe that URL with SCE loss achieves comparable performance to the URL baseline in most datasets. Meanwhile, URL with SCE achieves significantly better results on some datasets, such as Fungi and MSCOCO, in which URL baseline tends to overfit (cf. Figs. 27(g) and 27(j)). This indicates that SCE loss can improve generalization performance. In addition, we also replace the SCE loss with the NCC loss on CoPA. According to the results of "CoPA + NCC" shown in the table, it is easy to find that the performance in all cases drops drastically. For example, the performance on Fungi and MSCOCO respectively decreases by \(11.6\%\) and \(18.7\%\). All these empirical results demonstrate that SCE loss facilitates improving generalization performance.

Figure 6: Comparison of test losses of URL and URL+2Heads on ImageNet and CIFAR100.

#### 5.2.2 Further Discussion

**Effectiveness of Different Transformations.** As aforementioned, an advantage of CoPA is applying two different transformations respectively to prototype and image instance embeddings. The goal of applying two different transformations in CoPA is to preserve the discriminative information of gradients in different sets of parameters. Meanwhile, applying two different transformations also increases the complexity of the hypothesis space, which contributes to reducing the approximation error and learning representations in a more flexible way. However, simply increasing hypothesis space complexity may also result in overfitting (cf. Fig. 6). According to Fig. 27, CoPA, which is equipped with SCE loss, benefits from the increased complexity and achieves better performance.

**Essence of Gap Enlargement.** In Fig. 3(a), we observe that the global minimum of the validation loss can be achieved by slightly enlarging the gap between prototype and images. Meanwhile, according to Fig. 5(c), the validation loss achieves its global minimum at the enlarged gap learned with CoPA. As aforementioned, we conjecture that the reasons for the phenomenon that gap enlargement facilitates generalization mainly include two aspects: representation alignment and overfitting alleviation.

We first start with representation alignment. Typically, representation alignment is performed with SCE loss. Specifically, we omit the temperature coefficient \(\) and simply rewrite Eq. (3) as:

\[_{}=-_{}|}_{i=1}^{ |_{}|}_{i}^{}_{i})}{ _{j=1}^{|_{}|}(_{i}^{}_{j})}- {|_{}|}_{i=1}^{|_{}|} {(_{i}^{}_{i})}{_{j=1}^{|_{}|} (_{i}^{}_{j})}. \]

**Theorem 5.1**.: _Given a set of normalized finite support data representation \(=\{(_{i},y_{i})\}_{i=1}^{n}\) and a set of normalized prototype representations \(=\{_{i}\}_{i=1}^{n}\), where \(||||_{2}=1\) for \(\) and \(||||_{2}=1\) for \(\), then we are able to obtain a lower bound of SCE loss in Eq. (4):_

\[_{}-_{i=1}^{n}_{i}^{}_{i}+_{i=1}^{n}_{k=1}^{N_{C}}_{k}|}{n} _{i}^{}_{k},\]

_where \(_{k}\) denotes the set of support data of the class \(k\) and \(N_{C}\) denotes the number of classes._

The proof is in Appendix G.3. According to Theorem 5.1, we find that the lower bound of SCE loss plays a similar role to that of NCC loss. In detail, the lower bound aims at maximizing the similarity between each sample and its corresponding prototype while minimizing the similarities between the sample and all prototypes. When minimizing the lower bound as the surrogate loss, the second term, which measures the similarities between image and prototype representations, is minimized. Thus, the Euclidean distances between representations of prototypes and images (i.e., the gap) are enlarged.

An interesting point is that the similarities between images and prototypes are minimized with the weights calculated based on the size of \(_{k}\). Differently, in NCC loss, the weights are fixed to \(}\) for all cases. Thus, the similarities between samples and the prototypes involving more samples will be significantly reduced. Consequently, the gap between images and prototypes tends to be enlarged.

For the overfitting, on the one hand, the potential overfitting discussed in Section 3.2 is mitigated by applying different transformations. On the other hand, the high similarities between samples and prototypes may also result in overfitting. For example, in Fig. 3(a), the validation loss increases when the gap is narrowed. However, as we have discussed above, minimizing SCE loss tends to reduce such similarities. Thus, the gap enlargement also functions as a regularization to alleviate overfitting.

Therefore, with all aspects taken into consideration, the essence of CoPA is a representation alignment of prototypes and images, in which a balance between learning discriminative representations and achieving better generalization performance, is explored in a more flexible hypothesis space.

## 6 Conclusion

In this paper, we validate that there naturally exists a gap, which resembles the modality gap in visual language models, between prototype and image instance embeddings, and uncover that the shared transformation shrinks such a gap and constrains the learning of well-clustered representations. In order to solve these problems, we propose CoPA to adapt different transformations respectively for prototype and image instances via optimizing SCE loss. Empirical results on Meta-Dataset reveal that CoPA can achieve _state-of-the-art_ performance. Our further analyses indicate that the essence of CoPA is a representation alignment, where a balance between learning discriminative representations and achieving better generalization performance is explored in a more flexible hypothesis space.