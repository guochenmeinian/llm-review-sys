# Efficiently Parameterized Neural Metriplectic Systems

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Metriplectic systems are learned from data in a way that scales quadratically in both the size of the state and the rank of the metriplectic data. Besides being provably energy conserving and entropy stable, the proposed approach comes with approximation results demonstrating its ability to accurately learn metriplectic dynamics from data as well as an error estimate indicating its potential for generalization to unseen timescales when approximation error is low. Examples are provided which illustrate performance in the presence of both full state information as well as when entropic variables are unknown, confirming that the proposed approach exhibits superior accuracy and scalability without compromising on model expressivity.

## 1 Introduction

The theory of metriplectic, also called GENERIC, systems [1; 2] provides a principled formalism for encoding dissipative dynamics in terms of complete thermodynamical systems that conserve energy and produce entropy. By formally expressing the reversible and irreversible parts of state evolution with separate algebraic brackets, the metriplectic formalism provides a general mechanism for maintaining essential conservation laws while simultaneously respecting dissipative effects. Thermodynamic completeness implies that any dissipation is caught within a metriplectic system through the generation of entropy, allowing for a holistic treatment which has already found use in modeling fluids [3; 4], plasmas [5; 6], and kinetic theories [7; 8].

From a machine learning point of view, the formal separation of conservative and dissipative effects makes metriplectic systems highly appealing for the development of phenomenological models. Given data which is physics-constrained or exhibits some believed properties, a metriplectic system can be learned to exhibit the same properties with clearly identifiable conservative and dissipative parts. This allows for a more nuanced understanding of the governing dynamics via an evolution equation which reduces to an idealized Hamiltonian system as the dissipation is taken to zero. Moreover, elements in the kernel of the learned conservative part are immediately understood as Casimir invariants, which are special conservation laws inherent to the phase space of solutions, and are often useful for understanding and exerting control on low-dimensional structure in the system. On the other hand, the same benefit of metriplectic structure as a "direct sum" of reversible and irreversible parts makes it challenging to parameterize in an efficient way, since delicate degeneracy conditions must be enforced in the system for all time. In fact, there are no methods at present for learning general metriplectic systems which scale optimally with both dimension and the rank of metriplectic data--an issue which this work directly addresses.

Precisely, metriplectic dynamics on the finite or infinite dimensional phase space \(\) are generated by a free energy function(al) \(F:\), \(F=E+S\) defined in terms of a pair \(E,S:\) representing energy and entropy, respectively, along with two algebraic brackets \(\{,\},[,]:C^{}() C^{}()()}\) which are bilinear derivations on \(C^{}()\) with prescribed symmetries and degeneracies \(\{S,\}=[E,]=0\). Here \(\{,\}\) is an antisymmetric Poisson bracket, which is a Lie algebra realization on functions, and \([,]\) is a degenerate metric bracket which is symmetric and positivesemi-definite. When \(^{n}\) for some \(n>0\), these brackets can be identified with symmetric matrix fields \(:_{n}(),: _{n}()\) satisfying \(\{F,G\}= F G\) and \([F,G]= F M G\) for all functions \(F,G C^{}()\) and all states \(\). Using the degeneracy conditions along with \(=\) and abusing notation slightly then leads the standard equations for metriplectic dynamics,

\[}=\{,F\}+[,F]=\{,E\}+[,S]= E+ {M} S,\]

which are provably energy conserving and entropy producing. To see why this is the case, recall that \(^{}=-\). It follows that the infinitesimal change in energy satisfies

\[=} E= E E+ S  E=- E E+ S E=0,\]

and therefore energy is conserved along the trajectory of \(\). Similarly, the fact that \(^{}=\) is positive semi-definite implies that

\[=} S=L E S+ S  S=- E S+ S S=| S |_{}^{2} 0,\]

so that entropy is nondecreasing along \(\) as well. Geometrically, this means that the motion of a trajectory \(\) is everywhere tangent to the level sets of energy and transverse to those of entropy, reflecting the fact that metriplectic dynamics are a combination of noncanonical Hamiltonian (\(=\)) and generalized gradient (\(=\)) motions. Note that these considerations also imply the Lyapunov stability of metriplectic trajectories, as can be seen by taking \(E\) as a Lyapunov function. Importantly, this also implies that metriplectic trajectories which start in the (often compact) set \(K=\{\,|\,E() E(_{0})\}\) remain there for all time.

In phenomenological modeling, the entropy \(S\) is typically chosen from Casimirs of the Poisson bracket generated by \(\), i.e. those quantities \(C C^{}()\) such that \( C=\). On the other hand, the method which will be presented here, termed neural metriplectic systems (NMS), allows for all of the metriplectic data \(,,E,S\) to be approximated simultaneously, removing the need for Casimir invariants to be known or assumed ahead of time. The only restriction inherent to NMS is that the metriplectic system being approximated is nondegenerate (c.f. Definition 3.1), a mild condition meaning that the gradients of energy and entropy cannot vanish at any point \(\) in the phase space. It will be shown that NMS alleviates known issues with previous methods for metriplectic learning, leading to easier training, superior parametric efficiency, and better generalization performance.

Contributions.The proposed NMS method for learning metriplectic models offers the following advantages over previous state-of-the-art: **(1)** It approximates arbitrary nondegenerate metriplectic dynamics with optimal quadratic scaling in both the problem dimension \(n\) and the rank \(r\) of the irreversible dynamics. **(2)** It produces realistic, thermodynamically consistent entropic dynamics from unobserved entropy data. **(3)** It admits universal approximation and error accumulation results given in Proposition 3.7 and Theorem 3.9. **(4)** It yields exact energy conservation and entropy stability by construction, allowing for effective generalization to unseen timescales.

## 2 Previous and Related Work

Previous attempts to learn metriplectic systems from data separate into "hard" and "soft" constrained methods. Hard constrained methods enforce metriplectic structure by construction, so that the defining properties of metriplecticity cannot be violated. Conversely, methods with soft constraints relax some aspects of metriplectic structure in order to produce a wider model class which is easier to parameterize. While hard constraints are the only way to truly guarantee appropriate generalization in the learned surrogate, the hope of soft constrained methods is that the resulting model is "close enough" to metriplectic that it will exhibit some of the favorable characteristics of metriplectic systems, such as energy and entropy stability. Some properties of the methods compared in this work are summarized in Table 1.

Soft constrained methods.Attempts to learn metriplectic systems using soft constraints rely on relaxing the degeneracy conditions \( S= E=\). This is the approach taken in , termed SPNN, which learns an almost-metriplectic model parameterized with generic neural networks through a simple \(L^{2}\) penalty term in the training loss, \(_{}=| E|^{2}+| S|^{2}\). This widens the space of allowable network parameterizations for the approximate operators \(,\). Whilethe resulting model violates the first and second laws of thermodynamics, the authors show that reasonable trajectories are still obtained, at least when applied within the range of timescales used for training. A similar approach is taken in , which targets larger problems and develops an almost-metriplectic model reduction strategy based on the same core idea.

Hard constrained methods.Perhaps the first example of learning metriplectic systems from data was given in  in the context of system identification. Here, training data is assumed to come from a finite element simulation, so that the discrete gradients of energy and entropy can be approximated as \( E()=, S()=\). Assuming a fixed form for \(\) produces a constrained learning problem for the constant matrices \(,,\) which is solved to yield a provably metriplectic surrogate model. Similarly, the work  learns \(,E\) given \(,S\) by considering a fixed block-wise decoupled form which trivializes the degeneracy conditions, i.e. \(=[;\;]\) and \(=[\;;\;]\). This line of thought is continued in  and , both of which learn metriplectic systems with neural network parameterizations by assuming this decoupled block structure. A somewhat broader class of metriplectic systems are considered in  using tools from exterior calculus, with the goal of learning metriplectic dynamics on graph data. This leads to a structure-preserving network surrogate which scales linearly in the size of the graph domain, but also cannot express arbitrary metriplectic dynamics due to the specific choices of model form for \(,\).

A particularly inspirational method for learning general metriplectic systems was given in  and termed GNODE, building on parameterizations of metriplectic operators developed in . GNODE parameterizes learnable reversible and irreversible bracket generating matrices \(,\) in terms of state-independent tensors \((^{n})^{ 3}\) and \((^{n})^{ 4}\): for \(1,,,, n\), the authors choose \(L_{}()=_{}_{}^{}S\) and \(M_{}()=_{,}_{,}^{} E^{}E,\) where \(^{}F= F/ x_{}\), \(\) is totally antisymmetric, and \(\) is symmetric between the pairs \((,)\) and \((,)\) but antisymmetric within each of these pairs. The key idea here is to exchange the problem of enforcing degeneracy conditions \( E= S=\) in matrix fields \(,\) with the problem of enforcing symmetry conditions in tensor fields \(,\), which is comparatively easier but comes at the expense of underdetermining the problem. In GNODE, enforcement of these symmetries is handled by a generic learnable 3-tensor \(}(^{n})^{ 3}\) along with learnable matrices \(_{r}()\) and \(^{s}_{n}()\) for \(1 s r n\), leading to the final parameterizations \(_{}=(_{}- _{}+_{}-_{ }+_{}-_{ })\) and \(_{,}=_{s,t}^{s}_{}D_{st}^{t}_ {}\). Along with learnable energy and entropy functions \(E,S\) parameterized by multi-layer perceptrons (MLPs), the data \(,\) learned by GNODE guarantees metriplectic structure in the surrogate model and leads to successful learning of metriplectic systems in some simple cases of interest. However, note that this is a highly redundant parameterization requiring \(+r++2\) learnable scalar functions, which exhibits \((n^{3}+rn^{2})\) scaling in the problem size because of the necessity to compute and store \(\) entries of \(\) and \(r\) entries of \(\). Additionally, the assumption of state-independence in the bracket generating tensors \(,\) is somewhat restrictive, limiting the class of problems to which GNODE can be applied.

A related approach to learning metriplectic dynamics with hard constraints was seen in , which proposed a series of GFINN architectures depending on how much of the information \(,,E,S\) is assumed to be known. In the case that \(,\) are known, the GFINN energy and entropy are parameterized with scalar-valued functions \(f_{}\) where \(f:^{n}\) (\(E\) or \(S\)) is learnable and \(_{}:^{n}^{n}\) is orthogonal projection onto the kernel of the (known) operator \(\) (\(\) or \(\)). It follows that the gradient \((f_{})=_{} f( _{})\) lies in the kernel of \(\), so that the degeneracy conditions are guaranteed at the expense of constraining the model class of potential energies/entropies. Alternatively, in the case that all of \(,,E,S\) are unknown, GFINNs use learnable scalar functions \(f\) for \(E,S\) parameterized by MLPs as well as two matrix fields \(^{E},^{S}^{r n}\) with rows given by \(^{f}_{s}=(^{f}_{s} f)^{}\) for learnable skew-symmetric matrices \(^{f}_{s}\), \(1 s r\), \(f=E,S\). Along with two triangular \((r r)\) matrix fields \(_{},_{}\) this yields the parameterizations \(()=^{S}()^{}(_{}()^{ }-_{}())^{S}()\) and \(()=^{E}()^{}(_{}()^{ }_{}())^{E}()\), which necessarily satisfy the symmetries and degeneracy conditions required for metriplectic structure. GFINNs are shown to both increase expressivity over the GNODE method as well as decrease redundancy, since the need for an explicit order-3 tensor field is removed and the reversible and irreversible brackets are allowed to depend explicitly on the state \(\). However, GFINNs still exhibit cubic scaling through the requirement of \(rn(n-1)+r^{2}+2=rn^{2}\) learnable functions, which is well above the theoretical minimum required to express a general metriplectic system and leads to difficulties in training the resulting models.

Model reduction.Finally, it is worth mentioning the closely related line of work involving model reduction for metriplectic systems, which began with . As remarked there, preserving metriplecticity in reduced-order models (ROMs) exhibits many challenges due to its delicate requirements on the kernels of the involved operators. There are also hard and soft constrained approaches: the already mentioned  aims to learn a close-to-metriplectic data-driven ROM by enforcing degeneracies by penalty, while  directly enforces metriplectic structure in projection-based ROMs using exterior algebraic factorizations. The parameterizations of metriplectic data presented here are related to those presented in , although NMS does not require access to nonzero components of \( E, S\).

## 3 Formulation and Analysis

The proposed formulation of the metriplectic bracket-generating operators \(,\) used by NMS is based on the idea of exploiting structure in the tensor fields \(,\) to reduce the necessary number of degrees of freedom. In particular, it will be shown that the degeneracy conditions \( S= E=\) imply more than just symmetry constraints on these fields, and that taking these additional constraints into account allows for a more compact representation of metriplectic data. Following this, results are presented which show that the proposed formulation is universally approximating on nondegenerate systems (c.f. Definition 3.1) and admits a generalization error bound in time.

### Exterior algebra

Developing these metriplectic expressions will require some basic facts from exterior algebra, of which more details can be found in, e.g., [21, Chapter 19]. The basic objects in the exterior algebra \( V\) over the vector space \(V\) are multivectors, which are formal linear combinations of totally antisymmetric tensors on \(V\). More precisely, if \(I(V)\) denotes the two-sided ideal of the free tensor algebra \(T(V)\) generated by elements of the form \(\) (\( V\)), then the exterior algebra is the quotient space \( V T(V)/I(V)\) equipped with the antisymmetric wedge product operation \(\). This graded algebra is equipped with natural projection operators \(P^{k}: V^{k}V\) which map between the full exterior algebra and the \(k^{}\) exterior power of \(V\), denoted \(^{k}V\), whose elements are homogeneous \(k\)-vectors. More generally, given an \(n\)-manifold \(M\) with tangent bundle \(TM\), the exterior algebra \((TM)\) is the algebra of multivector fields whose fiber over \(x M\) is given by \( T_{x}M\).

For the present purposes, it will be useful to develop a correspondence between bivectors \(^{2}(^{n})\) and skew-symmetric matrices \(_{n}()\), which follows directly from Riesz representation in terms of the usual Euclidean dot product. More precisely, supposing that \(_{1},...,_{n}\) are the standard basis vectors for \(^{n}\), any bivector \(^{2}T^{n}\) can be represented as \(=_{i<j}B^{ij}_{i}_{j}\) where \(B^{ij}\) denote the components of \(\). Define a grade-lowering action of bivectors on vectors through right contraction (see e.g. Section 3.4 of ), expressed for any vector \(\) and basis bivector \(_{i}_{j}\) as \((_{i}_{j})=(_{j})_{i}-( _{i})_{j}\). It follows that the action of \(\) is equivalent to

\[=_{i<j}B^{ij}((_{j})_{i}-(_{i})_{j})=_{i<j}B^{ij}v_{j}_{i}-_{j<i}B^{ji}v_{j} _{i}=_{i,j}B^{ij}v_{j}_{i}=,\]

where \(^{}=-^{n n}\) is a skew-symmetric matrix representing \(\), and we have re-indexed under the second sum and applied that \(B^{ij}=-B^{ji}\) for all \(i,j\). Since the kernel of this action is the zero bivector, it is straightforward to check that this string of equalities defines an isomorphism \(:^{2}^{n}_{n}()\) from the \(2^{}\) exterior power of \(^{n}\) to the space of skew-symmetric \((n n)\)-matrices over \(\): in what follows, we will write \(\) rather than \(=()\) for notational convenience. Note that a correspondence in the more general case of bivector/matrix fields follows in the usual way via the fiber-wise extension of \(\).

### Learnable metriplectic operators

It is now possible to explain the proposed NMS formulation. First, note the following key definition which prevents the consideration of unphysical examples.

**Definition 3.1**.: A metriplectic system on \(K^{n}\) generated by the data \(,,E,S\) will be called _nondegenerate_ provided \( E, S\) for all \( K\).

With this, the NMS parameterizations for metriplectic operators are predicated on an algebraic result proven in Appendix A.

**Lemma 3.2**.: _Let \(K^{n}\). For all \( K\), the operator \(:K^{n n}\) satisfies \(^{}=-\) and \(L S=\) for some \(S:K\), \( S\), provided there exists a non-unique bivector field \(:U^{2}^{n}\) and equivalent matrix field \(\) such that_

\[(} ) S=-} S  S.\]

_Similarly, for all \( K\) a positive semi-definite operator \(:K^{n n}\) satisfies \(^{}=\) and \( E=\) for some \(E:K\), \( E\), provided there exists a non-unique matrix-valued \(:K^{n r}\) and symmetric matrix-valued \(:K^{r r}\) such that \(r n\) and_

\[ =_{s,t}D_{st}^{s}} E\,\,^{t} } E\] \[=_{s,t}D_{st}^{s}-^{s} E} {| E|^{2}} E^{t}-^{t}  E}{| E|^{2}} E^{},\]

_where \(^{s}\) denotes the \(s^{}\) column of \(\). Moreover, using \(_{f}^{}=(-})\) to denote the orthogonal projector onto \(( f)^{}\), these parameterizations of \(,\) are equivalent to the matricized expressions \(=_{S}^{}_{S}^{}\) and \(=_{E}^{}^{}_{E}^{}\)._

_Remark 3.3_.: Observe that the projections appearing in these expressions are the minimum necessary for guaranteeing the symmetries and degeneracy conditions necessary for metriplectic structure. In particular, conjugation by \(_{f}^{}\) respects symmetry and ensures that both the input and output to the conjugated matrix field lie in \(( f)^{}\).

Lemma 3.2 demonstrates specific parameterizations for \(,\) that hold for any nondegenerate metriplectic data and are core to the NMS method for learning metriplectic dynamics. While generally underdetermined, these expressions are in a sense maximally specific given no additional information, since there is nothing available in the general metriplectic formalism to determine the matrix fields \(,^{}\) on \(( S),( E)\), respectively. The following Theorem, also proven in Appendix A, provides a rigorous correspondence between metriplectic systems and these particular parameterizations.

**Theorem 3.4**.: _The data \(,,E,S\) form a nondegenerate metriplectic system in the state variable \( K\) if and only if there exist a skew-symmetric \(:K_{n}()\), symmetric postive semidefinite \(:K_{r}()\), and generic \(:K^{n r}\) such that_

\[}= E+ S=_{S}^{}_{S} ^{} E+_{E}^{}^{}_{E}^ {} S.\]

_Remark 3.5_.: Note that the proposed parameterizations for \(,\) are not one-to-one but properly contain the set of valid nondegenerate metriplectic systems in \(E,S\), since the Jacobi identity on \(\) necessary for a true Poisson manifold structure is not strictly enforced. For \(1 i,j,k n\), the Jacobi identity is given in components as \(_{}L_{i}^{L}L_{jk}+L_{j}^{L}L_{ki}+L_{k} ^{L}L_{ij}=0\). However, this requirement is not often enforced in algorithms for learning general metriplectic (or even symplectic) systems, since it is considered subordinate to energy conservation and it is well known that both qualities cannot hold simultaneously in general .

### Specific parameterizations

Now that Theorem 3.4 has provided a model class which is rich enough to express any desired metriplectic system, it remains to discuss what NMS actually learns. First, note that it is unlikely to be the case that \(E,S\) are known _a priori_, so it is beneficial to allow these functions to be learnable alongside the governing operators \(,\). For simplicity, energy and entropy \(E,S\) are parameterized as scalar-valued MLPs with \(\) activation, although any desired architecture could be chosen for this task. The skew-symmetric matrix field \(=_{}-_{}^{}\) used to build \(\) is parameterized through its strictly lower-triangular part \(_{}\) using a vector-valued MLP with output dimension \(\)which guarantees that the mapping \(_{}\) above is bijective. Similarly, the symmetric matrix field \(=_{}_{}^{}\) is parameterized through its lower-triangular Cholesky factor \(_{}\), which is a vector-valued MLP with output dimension \(\). While this choice does not yield a bijective mapping \(_{}\) unless, e.g., \(\) is assumed to be positive definite with diagonal entries of fixed sign, this does not hinder the method in practice. In fact, \(\) should not be positive definite in general, as this is equivalent to claiming that \(\) is positive definite on vectors tangent to the level sets of \(E\). Finally, the generic matrix field \(\) is parameterized as a vector-valued MLP with output dimension \(nr\). Remarkably, the exterior algebraic expressions in Lemma 3.2 require less redundant operations than the corresponding matricized expressions from Theorem 3.4, and therefore the expressions from Lemma 3.2 are used when implementing NMS. Figure 1 summarizes this information.

_Remark 3.6_.: With these choices, the NMS parameterization of metriplectic systems requires only \((1/2)(n+r)^{2}-(n-r)+2\) learnable scalar functions, in contrast to \(+r++2\) for the GNODE approach in  and \(rn(n-1)+r^{2}+2\) for the GFINN approach in . In particular, NMS is quadratic in both \(n,r\) with no decrease in model expressivity, in contrast to the cubic scaling of previous methods.

### Approximation and error

Besides offering a compact parameterization of metriplectic dynamics, the expressions used in NMS also exhibit desirable approximation properties which guarantee a reasonable bound on state error over time. To state this precisely, first note the following universal approximation result proven in Appendix A.

**Proposition 3.7**.: _Let \(K^{n}\) be compact and \(E,S:K\) be continuous such that \( S= E=\) and \( E, S\) for all \( K\). For any \(>0\), there exist two-layer neural network functions \(,:K,}:K_{n}( )\) and \(}:K_{n}()\) such that \(,\) on \(K\), \(}\) is positive semi-definite, \(}=}=\) for all \( K\), and each approximate function is \(\)-close to its given counterpart on \(K\). Moreover, if \(,\) have \(k 0\) continuous derivatives on \(K\) then so do \(},}\)._

_Remark 3.8_.: The assumption \( K\) of the state remaining in a compact set \(V\) is not restrictive when at least one of \(E,-S:^{n}\), say \(E\), has bounded sublevel sets. In this case, letting \(_{0}=(0)\) it follows from \( 0\) that \(E((t))=E(_{0})+_{0}^{t}(())\,d E( _{0}),\) so that the entire trajectory \((t)\) lies in the (closed and bounded) compact set \(K=\{\,|\,E() E(_{0})\}^{n}\).

Leaning on Proposition 3.7 and classical universal approximation results in , it is further possible to establish the following error estimate also proven in Appendix A which gives an idea of the error accumulation rate that can be expected from this method.

**Theorem 3.9**.: _Suppose \(,,E,S\) are nondegenerate metriplectic data such that \(,\) have at least one continuous derivative, \(E,S\) have Lipschitz continuous gradients, and at least one of \(E,-S\) have bounded sublevel sets. For any \(>0\), there exist nondegenerate metriplectic data \(},},,\) defined by two-layer neural networks such that, for all \(T>0\),_

\[(_{0}^{T}-}^{2}\,dt)^{}-2e^{aT}+ T+1},\]

    &  &  &  &  \\  NODE & None & No & Linear \\ SPNN & Soft & No & Quadratic \\ GNODE & Hard & Yes & Cubic \\ GFINN & Hard & No & Cubic \\ NMS & Hard & No & Quadratic \\   

Table 1: Properties of the metriplectic architectures compared.

Figure 1: A visual depiction of the NMS architecture.

_where \(a,b\) are constants depending on both sets of metriplectic data and \(}=}()()+}( })()\)._

_Remark 3.10_.: Theorem 3.9 provides a bound on state error over time under the assumption that the approximation error in the metriplectic networks can be controlled. On the other hand, notice that Theorem 3.9 can also be understood as a generic error bound on any trained metriplectic networks \(},},,\) provided universal approximation results are not invoked in the estimation leading to \( b\).

This result confirms that the error in the state \(\) for a fixed final time \(T\) tends to zero with the approximation error in the networks \(},},,\), as one would hope based on the approximation capabilities of neural networks. More importantly, Theorem 3.9 also bounds the generalization error of any trained metriplectic network for an appropriate (and possibly large) \(\) equal to the maximum approximation error on \(K\), where the learned metriplectic trajectories are confined for all time. With this theoretical guidance, the remaining goal of this work is to demonstrate that NMS is also practically effective at learning metriplectic systems from data and exhibits reasonable generalization to unseen timescales.

## 4 Algorithm

Similar to previous approaches in  and , the learnable parameters in NMS are calibrated using data along solution trajectories to a given dynamical system. This brings up an important question regarding how much information about the system in question is realistically present in the training data. While many systems have a known metriplectic form, it is not always the case that one will know metriplectic governing equations for a given set of training data. Therefore, two approaches are considered in the experiments below corresponding to whether full or partial state information is assumed available during NMS training. More precisely, the state \(=(^{o},^{u})\) will be partitioned into "observable" and "unobservable" variables, where \(^{u}\) may be empty in the case that full state information is available. In a partially observable system \(^{o}\) typically contains positions and momenta while \(^{u}\) contains entropy or other configuration variables which are more difficult to observe during physical experiments. In both cases, NMS will learn a metriplectic system in \(\) according to the procedure described in Algorithm 1.

```
1:Input: snapshot data \(^{n n_{s}}\), each column \(_{s}=(t_{s},_{s})\), target rank \(r 1\), batch size \(n_{b} 1\).
2:Initialize networks \(_{},,_{},E,S,\) and loss \(L=0\)
3:for step in \(N_{}\)do
4: Randomly draw batch \(P=\{(t_{s_{k}},_{s_{k}})\}_{k=1}^{n_{b}}\)
5:for\((t,)\) in \(P\)do
6: Evaluate \(_{}(),(),_{}( ),E(),S()\)
7: Automatically differentiate \(E,S\) to obtain \( E(), S()\)
8: Form \(()=_{}()-_{}()^{}\) and \(()=_{}()_{}()^{}\)
9: Build \((),()\) according to Lemma 3.2
10: Evaluate \(}=() E()+() S()\)
11: Randomly draw \(n_{1},...,n_{l}\) and form \(t_{j}=t+n_{j} t\) for all \(j\)
12:\(_{1},...,_{l}=(,t_{1},...,t_{l})\)
13:\(L+=l^{-1}_{j}(_{j},_{j})\)
14:endfor
15: Rescale \(L=|P|^{-1}L\)
16: Update \(_{},,_{},E,S\) through gradient descent on \(L\).
17:endfor
```

**Algorithm 1** Training neural metriplectic systems

Note that the batch-wise training strategy in Algorithm 1 requires initial conditions for \(^{u}\) in the partially observed case. There are several options for this, and two specific strategies will be considered here. Suppose the data are drawn from the training interval \([0,T]\) with initial state \(_{0}\) and final state \(_{T}\). The first strategy sets \(^{u}_{0}=\), \(^{u}_{T}=\) (where \(\) is the all ones vector), and \(^{u}_{s}=/T\), \(0<s<T\), so that the unobserved states are initially assumed to lie on a straight line. The second strategy is more sophisticated, and involves training a diffusion model to predict the distribution of \(^{u}\) given \(^{o}\). Specific details of this procedure are given in Appendix E.

Examples

The goal of the following experiments is to show that NMS is effective even when entropic information cannot be observed during training, yielding superior performance when compared to previous methods including GNODE, GFINN, and SPNN discussed in Section 2. The metrics considered for this purpose will be mean absolute error (MAE) and mean squared error (MSE) defined in the usual way as the average \(^{1}\) (resp. squared \(^{2}\)) error between the discrete states \(,}^{n n_{s}}\). For brevity, many implementation details have been omitted here and can be found in Appendix B. An additional experiment showing the effectiveness of NMS in the presence of both full and partial state information can be found in Appendix C.

_Remark 5.1_.: To facilitate a more equal parameter count between the compared metriplectic methods, the results of the experiments below were generated using the alternative parameterization \(=^{}\) where \(:K^{r r^{}}\) is full and \(r^{} r\). Of course, this change does not affect metriplecticity since \(\) is still positive semi-definite for each \( K\).

### Two gas containers

The first test of NMS involves two ideal gas containers separated by movable wall which is removed at time \(t_{0}\), allowing for the exchange of heat and volume. In this example, the motion of the state \(=(q p S_{1} S_{2})^{}\) is governed by the metriplectic equations:

\[ =,  =()}{q}-()} {2L-q},\] \[_{1} =k_{B}^{2}}{4E_{1}()}()}-()}, _{2} =-k_{B}^{2}}{4E_{1}()}()}-()},\]

where \((q,p)\) are the position and momentum of the separating wall, \(S_{1},S_{2}\) are the entropies of the two subsystems, and the internal energies \(E_{1},E_{2}\) are determined from the Sackur-Tetrode equation for ideal gases, \(S_{i}/Nk_{B}=V_{i}E_{i}^{3/2},1 i 2\). Here, \(m\) denotes the mass of the wall, \(2L\) is the total length of the system, and \(V_{i}\) is the volume of the \(i^{}\) container. As in [16; 25]\(Nk_{B}=1\) and \(=0.5\) fix the characteristic macroscopic unit of entropy while \(=102.25\) ensures the argument of the logarithm defining \(E_{i}\) is dimensionless. This leads to the total entropy \(S()=S_{1}+S_{2}\) and the total energy \(E()=(1/2m)p^{2}+E_{1}()+E_{2}()\), which are guaranteed to be nondecreasing and constant, respectively.

The primary goal here is to verify that NMS can accurately and stably predict gas container dynamics without the need to observe the entropic variables \(S_{1},S_{2}\). To that end, NMS has been compared to GNODE, SPNN, and GFINN on the task of predicting the trajectories of this metriplectic system over time, with results displayed in Table 2. More precisely, given an intial condition \(_{0}\) and an interval \(0<t_{}<t_{}<t_{}\), each method is trained on partial state information (in the case of NMS) or full state information (in the case of the others) from the interval \([0,t_{}]\) and validated on \((t_{},t_{}]\) before state errors in \(q,p\) only are calculated on the whole interval \([0,t_{}]\). As can be seen from Table 2 and Figure 2, NMS is remarkably accurate over unseen timescales even in this unfair comparison, avoiding the unphysical behavior which often hinders soft-constrained methods like SPNN. The energy and instantaneous entropy plots in Figure 2 further confirm that the strong enforcement of metriplectic structure guaranteed by NMS leads to correct energetic and entropic dynamics for all time.

### Thermoelastic double pendulum

Next, consider the thermoelastic double pendulum from  with 10-dimensional state variable \(=(_{1}_{2}_{1}_{2} S_{1} S _{2})^{}\), which represents a highly challenging benchmark for metriplectic methods. The equations of motion in this case are given for \(1 i 2\) as

\[}_{i}=_{i}}{m_{i}},}_{i}=-_{ {q}_{i}}(E_{1}()+E_{2}()),_{1}=T_{1}^{-1 }T_{2}-1,_{2}=T_{1}T_{2}^{-1}-1,\]

where \(>0\) is a thermal conductivity constant (set to 1), \(m_{i}\) is the mass of the \(i^{}\) spring (also set to 1) and \(T_{i}=_{S_{i}}E_{i}\) is its absolute temperature. In this case, \(_{i},_{i}^{2}\) represent the position and momentum of the \(i^{ th}\) mass, while \(S_{i}\) represents the entropy of the \(i^{ th}\) pendulum. As before, the total entropy \(S()=S_{1}+S_{2}\) is the sum of the entropies of the two springs, while defining the internal energies \(E_{i}()=(1/2)(_{i})^{2}+_{i}+e^{S_{i}-_{i}}- 1,_{1}=|_{i}|,_{2}=|_{2}-_{1}|\), leads to the total energy \(E()=(1/2m_{1}){|_{1}|}^{2}+(1/2m_{2}){|_{2}|}^{2}+E_{1}()+E_{2}()\).

The task in this case is prediction across initial conditions. As in , 100 trajectories are drawn from the ranges in Appendix B and integrated over the interval \(\) with \( t=0.1\), with an 80/10/10 split for training/validation/testing. Here all compared models are trained using full state information. As seen in Table 2, NMS is again the most performant, although all models struggle to approximate the dynamics over the entire training interval. It is also notable that the training time of NMS is greatly decreased relative to GNODE and GFINN due to its improved quadratic scaling; a representative study to this effect is given in Appendix D.

## 6 Conclusion

Neural metriplectic systems (NMS) have been considered for learning finite-dimensional metriplectic dynamics from data. Making use of novel non-redundant parameterizations for metriplectic operators, NMS provably approximates arbitrary nondegenerate metriplectic systems with generalization error bounded in terms of the operator approximation quality. Benchmark examples have shown that NMS is both more scalable and more accurate than previous methods, including when only partial state information is observed. Future work will consider extensions of NMS to infinite-dimensional metriplectic systems with the aim of addressing its main limitation: the difficulty of scaling NMS (among all present methods for metriplectic learning) to realistic, 3-D problems of the size that would be considered in practice. A promising direction is to consider the use of NMS in model reduction, where sparse, large-scale systems are converted to small, dense systems through a clever choice of encoding/decoding.

    & NODE & SPNN & GNODE & GFINN & NMS \\  MSE &.12 \(\).04 &.13 \(\).10 &.16 \(\).10 &.07 \(\).03 & **.01 \(\).02** \\ MAE &.25 \(\).10 &.26 \(\).14 &.25 \(\).13 &.13 \(\).03 & **.08 \(\).06** \\    
    & NODE & SPNN & GNODE & GFINN & NMS \\  MSE &.41 \(\).01 &.42 \(\).01 &.42 \(\).01 &.40 \(\).03 & **.38 \(\).03** \\ MAE &.48 \(\).04 &.47 \(\).03 &.46 \(\).04 &.43 \(\).07 & **.42 \(\).07** \\   

Table 2: Prediction errors for \(^{o}\) measured in MSE and MAE on the interval \([0,t_{ test}]\) in the two gas containers example (left) and on the test set in the thermoelastic double pendulum example (right).

Figure 2: The ground-truth and predicted position, momentum, instantaneous entropy, and energies for the two gas containers example in the training (white), validation (yellow), and testing (red) regimes.