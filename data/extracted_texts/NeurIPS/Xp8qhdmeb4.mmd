# E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation

Boqian Wu\({}^{1,2,}\), Qiao Xiao\({}^{3,}\), Shiwei Liu\({}^{4}\), Lu Yin\({}^{5}\), Mykola Pechenizkiy\({}^{3}\),

**Decebal Constantin Hocanu\({}^{2,3}\), Maurice van Keulen\({}^{1}\), Elena Mocanu\({}^{1}\)**

\({}^{1}\) University of Twente, \({}^{2}\) University of Luxembourg, \({}^{3}\) Eindhoven University of Technology,

\({}^{4}\) University of Oxford, \({}^{5}\) University of Surrey

{b.wu, m.vankeulen,e.mocanu}@utwente.nl

{q.xiao,m.pechenizkiy}@tue.nl, shiwei.liu@maths.ox.ac.uk,

l.yin@surrey.ac.uk, decebal.mocanu@uni.lu

Equal contribution.

###### Abstract

Deep neural networks have evolved as the leading approach in 3D medical image segmentation due to their outstanding performance. However, the ever-increasing model size and computational cost of deep neural networks have become the primary barriers to deploying them on real-world, resource-limited hardware. To achieve both segmentation accuracy and efficiency, we propose a 3D medical image segmentation model called Efficient to Efficient Network (E2ENet), which incorporates two parametrically and computationally efficient designs. i. Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse informative multi-scale features while reducing redundancy. ii. Restricted depth-shift in 3D convolution: it leverages the 3D spatial information while keeping the model and computational complexity as 2D-based methods. We conduct extensive experiments on AMOS, Brain Tumor Segmentation and BTCV Challenge, demonstrating that E2ENet consistently achieves a superior trade-off between accuracy and efficiency than prior arts across various resource constraints. E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while saving over \(69\%\) parameter count and \(27\%\) FLOPs in the inference phase, compared with the previous best-performing method. Our code has been made available at: https://github.com/boqian333/E2ENet-Medical.

## 1 Introduction

3D medical image segmentation plays an essential role in numerous clinical applications, including computer-aided diagnosis (Yu et al., 2020) and image-guided surgery systems (Ronneberger et al., 2015). Over the past decade, the rapid development of deep neural networks has achieved tremendous breakthroughs and significantly boosted the progress in this area (Zhou et al., 2018; Huang et al., 2020; Isensee et al., 2021). However, the model sizes and computational costs are also exploding, which deter their deployment in many real-world applications, especially for 3D models where resource consumption scales cubically (Hu et al., 2021; Valanarasu and Patel, 2022). This naturally raises a research question: _Can we design a 3D medical image segmentation method that trades off accuracy and efficiency better, subjected to different resource availability?_

Accurately segmenting organs is a challenging task in 3D medical image segmentation due to the variability in size and shape even among the same type of organ, caused by factors such as patient anatomy and disease stage. This diversity amplifies the difficulty to accurately identify and distinguishthe boundaries of different organs, leading to potential errors in segmentation. One of the main solutions for accurate medical image segmentation is to favorably leverage the multi-scale features extracted by the backbone network, but this remains a long-standing problem. Pioneering work UNet (Ronneberger et al., 2015) utilizes skip connection to propagate detailed information to high-level features. More recently, UNet++ (Zhou et al., 2018), CoTr (Xie et al., 2021), and DiNTS (He et al., 2021) have introduced more complex neural network architectures (e.g., dense skip connection and attention mechanism) and optimization techniques (e.g., neural architecture search (NAS) (Elsken et al., 2019)) for cross-scale feature fusion. NAS-based methods search for network topology and operators (e.g., 3x3 Convolution and Maxpool) and subsequently optimize model weights. However, these methods typically require substantial computational resources to explore network topologies and evaluate numerous candidate architectures, making them time-consuming. For instance, C2FNAS (Yu et al., 2020) requires nearly one GPU-year to discover a single 3D segmentation architecture, while DiNTS (He et al., 2021) improves search efficiency but still needs 5.8 GPU days to find a single architecture. In contrast to NAS approaches, our method does not require costly architecture search time and instead directly optimizes and searches for sparse topologies within the predefined architecture.

In this paper, we propose the **Efficient to Efficient Network (E2ENet)**, a model that efficiently incorporates both bottom-up and top-down features from the backbone network in a dynamically sparse pattern, achieving an improved accuracy-efficiency trade-off. As shown in Figure 1 (e), E2ENet incorporates multi-scale features from the backbone into the final output by gradually fusing adjacent features, allowing the network to fully utilize information across various scales. To prevent unnecessary information aggregation, we propose a **dynamic sparse feature fusion (DSSF)** mechanism and embed it in each fusion node. The DSSF mechanism adaptively integrates informative multi-scale features and filters out unnecessary ones during the course of the training process, significantly reducing the computational overhead without sacrificing performance. Additionally, to further improve efficiency, our E2ENet employs a **restricted depth-shift** strategy within 3D convolutions, derived from the temporal shift (Lin et al., 2019) and 3D-shift (Fan et al., 2020) strategies used in efficient video action recognition. This allows the 3D convolution operation with a kernel size of \((1,3,3)\) to capture 3D spatial relationships while maintaining the parameter complexity of 2D convolutions and reducing computational cost. To evaluate the performance of our proposed E2ENet, we conducted extensive experiments on the AMOS-CT (Ji et al., 2022), Brain Tumor Segmentation in the Medical Segmentation Decathlon (MSD) (Antonelli et al., 2022), and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) (Landman et al., 2015) challenges. We found that E2ENet can effectively trade-off between segmentation accuracy and efficiency compared to both convolution-based and transformer-based architectures. In particular, on the AMOS-CT challenge, E2ENet achieves competitive accuracy with a mDice of 90.0%, while being 69% smaller and using 27% fewer FLOPs during inference. With the DSSF mechanism filtering out 90% of feature connections, E2ENet further reduces resource costs without significantly sacrificing accuracy.

## 2 Methodology

In this section, we first present the overall architecture of E2ENet, which allows for the fusion of multi-scale features from three directions. Next, we describe the proposed DSSF mechanism, which adaptively selects informative features and filters out unnecessary ones during training. Lastly, to further increase efficiency, we introduce the use of restricted depth-shift in 3D convolution.

Figure 1: A comparison of feature fusion schemes. The purple nodes depict features extracted from the backbone, while the green nodes depict the fused features. In particular, in DiNTS (d), red lines indicate information flow paths determined through neural architecture search techniques. In E2ENet (e), the red lines with different widths represent sparse information flows determined by the DSSF mechanism, allowing for efficient feature fusion. E2ENet is capable of dynamically learning how many of the features to fuse are derived from the backbone.

### The Architecture

Figure 2 provides an overview of the proposed architecture. Given the input 3D image \(I_{in}^{D H W}\), the CNN backbone extracts feature maps at multiple scales, represented by \(^{0}=(^{0,1},^{0,2},,^{0,L})\), where \(L\) is the total number of feature scales. The feature at level \(i\), \(^{0,i}\) is a tensor with dimensions \(d_{i} h_{i} w_{i} c_{i}\), where \(d_{i},h_{i},w_{i}\) and \(c_{i}\) represent the depth, height, width, and number of channels of the feature maps at level \(i\), respectively. It is worth mentioning that the spatial resolution of the feature maps decreases as the level increases, while the number of channels increases up to a maximum of \(320\). To fully exploit the hierarchical features extracted by the CNN backbone, these features are aggregated across multiple stages, as depicted in Figure 2 (Dynamic Sparse Feature Fusion section). At stage \(j\) (\(0<j L-1\)), the features at level \(i\) are obtained by fusing the adjacent features from the previous stage along three directions:

1. "_Downward flow_" (in yellow): The high-resolution feature \(^{j-1,i-1}\), which provides richer visual details, is passed downward; 2. "_Upward flow_" (in blue): The low-resolution feature \(^{j-1,i+1}\), which captures more global context, is passed upward; 3. "_Forward flow_" (in red): The features \(x^{j-1,i}\), which maintain their spatial resolution, are passed forward for further information integration. The fused cross-scale feature maps at the \(i\)-th level of the \(j\)-th stage can be formulated as:

\[^{j,i}=\{^{j,i}([^{j-1,1},(^{j-1,2})]),&i=1;\\ ^{j,i}([(^{j-1,i-1}),^{j-1,i}, (^{j-1,i+1})]),&others,.\] (1)

where \(^{j,i}(.)\) is a fusion operation, consisting of a convolution operation followed by Instance Normalization (IN)  and a Leaky Rectified Linear Unit (LeakyReLU) . \((.)\) and \((.)\) denote the up-sampling and down-sampling, respectively. \([.]\) denotes the concatenation operation. In the convolution operation, the input feature maps, which have a channel number of \(C_{in}^{j,i}\) (\(C_{in}^{j,i}=c_{i-1}+c_{i}+c_{i+1}\) or \(c_{i}+c_{i+1}\)), are fused and processed to produce output feature maps with a channel number of \(C_{out}^{j,i}\) (\(C_{out}^{j,i}=c_{i}\)). Unlike the UNet++ model, which only considers bottom-up information flows for image segmentation, our proposed E2ENet architecture incorporates both bottom-up and top-down information flows. This allows E2ENet to **make use of both high-level contextual information and fine-grained details** in order to produce more accurate segmentation maps (experimental results can be found in Table 2).

### Dynamic Sparse Feature Fusion

Such multi-stage cross-level feature propagation provides a more comprehensive understanding of the images but can also introduce redundant information, necessitating careful handling in feature fusion to ensure efficient interaction between scales or stages. Our proposed Dynamic Sparse Feature Fusion

Figure 2: The overall architecture of the proposed E2ENet consists of a CNN backbone that extracts multiple levels of features. These features are then gradually aggregated through several stages, during which the multi-scale features are fused using a fusion operation.

(DSFF) mechanism addresses the issue of multi-scale feature fusion in an intuitive and effective way. It optimizes the process by enabling **selective and adaptive use of features from different levels during training**. This results in a more efficient feature fusion process with lower computational and memory overhead.

The DSEF mechanism is applied in each fusion operation, allowing the fusion operation \(^{j,i}()\) to select the informative feature maps from the input fused features. The feature map selection process is controlled by a binary mask \(^{j,i}()\{0,1\}^{C_{in}^{j,i} C_{out}^{j,i}}\), which are trained to filter out \(S\) unnecessary feature map connections by zeroing out the corresponding kernels in \(^{j,i}()\). With the DSFF mechanism, the output of the fusion operation is then computed as:

\[^{j,i}_{c_{out},:,:,:}=((_{c=0}^{C_{in}^{j,i}}( }^{j,i}_{c,:,:,:}*(^{j,i}_{c,c_{out},:,:,:})))),\] (2)

where \(}^{j,i}_{c,:,:,:}\) is the input fused feature map at the \(c\)-th channel. \(^{j,i}_{c,c_{out},:,:,:}\) is the kernel (feature map connection, as in Figure 3) that connects the \(c\)-th input feature map to \(c_{out}\)-th output feature map, \(*\) is a convolution operation, \(\) is the product of a scalar (i.e., \(^{j,i}_{c,c_{out}}\)) and a matrix (i.e., \(^{j,i}_{c,c_{out},:,:,:}\)) 2.

\(^{j,i}_{c,c_{out}}\) denotes the existence or absence of a connection between the \(c\)-th input feature map and the \(c_{out}\)-th output feature map. \((.)\) and \((.)\) denote the Instance Normalization and LeakyReLU, respectively. Thus, the core of DSEF mechanism is the learning of binary masks during the training. At initialization, each binary mask is initialized randomly, with the number of non-zero entries \(\|^{j,i}\|_{0}\) equals \((1-S) C_{in}^{j,i} C_{out}^{j,i}\). Here, \(S\) (\(0<S<1\)) is a hyperparameter called the feature sparsity level, which determines the percentage of feature map connections that are inactivated. The activated connections can be updated throughout the course of training, while the feature sparsity level \(S\) remains constant. This enables efficiency in both testing and training, as the exploitation of connections remains sparse throughout the training process.

**Every \( T\) training epoch, the activated connections with lower importance are removed, while the same number of deactivated connections are randomly reactivated**, as shown in Figure 3. The importance of activated connection is determined by the \(L_{1}\) norm of the corresponding kernel. That is, for \(^{j,i}\), the importance score of connection between \(c_{in}\)-th input feature and \(c_{out}\)-th output is \(\|^{j,i}_{c_{in},c_{out},:,:,:}\|_{1}\). Our intuition is simple: if a feature map connection is more important (i.e., has a larger effect on output accuracy) than others, the \(L_{1}\) norm of the corresponding kernel should be larger. However, the importance score is suboptimal during training, as the kernels are gradually optimized. Randomly reactivating previous "obsolete" connections with re-initialization can avoid un-recoverable feature map abandonment and thoroughly explore the representative feature maps that contribute to the final performance.

The DSEF mechanism allows for the exploration and exploitation of multi-scaled sparse features through a sparse-to-sparse training method. Our method selects features at multiple levels, as opposed to relying solely on feature selection at the input layer [Sokar et al., 2022, Atashgahi et al., 2022]. Additionally, it is a plastic approach to fuse features that can adapt to changing conditions

Figure 3: Illustration of our Dynamic Sparse Feature Fusion (DSFF) mechanism. The fusion operation starts from sparse feature connections and allows the connectivity to be evolved after training for \( T\) epochs. During each evolution stage, a fraction of kernels with smaller \(L_{1}\) norms will be zeroed out (red dotted line), while the same fraction of other inactivated connections will be re-activated randomly, keeping the feature sparsity \(S\) constant during training (blue solid line).

during training, as opposed to static methods that rely on one-shot feature selection, as shown in the comparison of dynamic and static sparsity in Table 2 of . More details of the training process are elaborated in the Algorithm 1 in Appendix A.3.

### Restricted Depth-Shift in 3D Convolution

In 3D medical image segmentation, the 2D-based methods (such as the 2D nnUNet ), which apply the 2D convolution to each slice of the 3D image, are computationally efficient but can not fully capture the relationships between slices. To overcome this limitation, we take inspiration from the temporal-shift  and 3D-shift  in efficient video action recognition, and the axial-shift  in efficient MLP architecture for vision. Our proposed E2ENet incorporates a depth-shift strategy in 3D convolution operations, which facilitates inter-slice information exchange and **captures 3D spatial relationships while retaining the simplicity and computational efficiency of 2D-based method**. Temporal-shift  requires selecting a Shift Proportion (the proportion of channels to conduct temporal shift). Axial-shift  and 3D-Shift optimize the learnable 3D spatiotemporal shift. We have made refinements to the channel shifting technique by shifting along the depth dimension and incorporating constraints on the shift size. This adaptation is thoughtfully designed to align with the distinctive needs of sparse models employed in medical image segmentation.

In our method, we employ a simple depth shift technique by shifting all channels while constraining the shift size to be either +1, 0 or -1, as shown in Figure 4. This choice is motivated using dynamic sparse feature fusion, where the feature maps contain sparse information. If the shift magnitude is too large, it can result in an insufficient representation of channels or an excessive representation of depth information, which can have a negative impact on the effectiveness of the shift operation (experimental results can be found in Table 3).

## 3 Experiments

In this section, we compare the performance of our E2ENet model to baseline methods and report results in terms of both segmentation quality and efficiency. In addition, we will perform ablation studies to investigate the behavior of each component in the E2ENet model. To further analyze the performance of our model, we will examine its generalizability and the effect of model capacity, and present qualitative results by visualizing the predicted segmentations on sample images. We will also visualize the feature fusion ratios to gain insights into which features play an important role in the segmentation process. The description of the dataset and experimental setup can be found in Appendix A.4 and A.5.

    & mDisc & Params & FLOPs.1  & IT score & mNNSD \\  & (\(_{0}\) \(\)) & (M\(\)) & (G\(\)) & \(\) & (\(\%\) \(\)) \\   CoTr & 77.1 & 41.87 & 1510.53 & 1.07 & 64.2 \\ mFomer & 85.6 & 150.14 & 1343.65 & 1.12 & 74.2 \\  UNET & 78.3 & 93.02 & **391.83** & 1.41 & 61.5 \\ Swin UNET & 86.4 & 62.83 & 1562.99 & 1.14 & 75.3 \\  VNet & 82.0 & 45.65 & 1737.57 & 1.10 & 67.9 \\ mUNet & 90.0 & 30.76 & 1067.89 & 1.30 & 82.1 \\ natNet & **90.5** & 30.76 & 1067.89 & 1.31 & **83.0** \\  E2ENet (as-0.7) & 90.3 & 112.39 & 969.32 & 1.54 & 82.7 \\ E2ENet (as-0.8) & 90.3 & 9.44 & 778.74 & 1.65 & 82.5 \\ E2ENet (as-0.9) & 89.9 & **76.4** & 492.29 & **1.89** & 81.8 \\  E2ENet (as-0.7) & 90.1 & 112.39 & 969.29 & 1.54 & 82.3 \\ E2ENet (as-0.8) & 90.0 & 9.44 & 778.74 & 1.65 & 82.3 \\ E2ENet (as-0.9) & 89.6 & **7.64** & 492.29 & 1.88 & 81.4 \\   

Table 1: Quantitative comparisons of segmentation performance on the validation set of AMOS-CT dataset. * denotes the results with postprocessing.

Figure 4: Illustration of restricted depth-shift in 3D Convolution of our E2ENet. The input features (left) are firstly split into 3 parts along the channel dimension, and then shifted by \(\{-1,0,1\}\) units along the depth dimension respectively (middle). After that, 3D CNNs with kernel size 1\(\)3\(\)3 are performed on the feature maps (middle) to generate the output features (right).

### Comparison with SOTA methods

**AMOS-CT Challenge.** To comprehensively validate our method, we compare it to several state-of-the-art CNN-based models (e.g. nnUNet , and Vnet ) and transformer-based models (e.g. CoTr , nnFormer , UNETR , and Swin UNETR ). We record the mDice (class-wise Dice can be found in Table 10 of Appendix), Params, inference FLOPs, and PT score 3 on the validation set 4 of the AMOS-CT challenge in Table 1. E2ENet with a feature sparsity (s) of \(0.8\) achieves comparable performance, with a mDice of \(90.3\%\), while being 3\(\) smaller in model size and requiring less computational cost during the inference phase compared to the top-performing lightweight model, nnUNet. As the feature sparsity of E2ENet increases, the number of model parameters and inference FLOPs can be further reduced without significantly compromising segmentation performance. This indicates that there is potential to trade-off performance and efficiency by adjusting the feature sparsity of E2ENet. We also report the results of mean normalized surface dice (mNSD), the official segmentation metric for the AMOS challenge, to provide supplementary information on boundary segmentation quality.

**BraTS Challenge in MSD.** E2ENet demonstrates superior performance in terms of mDice compared to other state-of-the-art methods (DiNTS , UNet+++  and nnUNet ). Additionally, it is a resource-efficient network that is competitive with the baselines, as evidenced by its small model size and low inference FLOPs. Specifically, E2ENet model a feature sparsity level of \(90\%\) has only \(7.63\) M parameters, which is significantly smaller than other models yet still outperforms them in terms of mDice. The results on the BraTS dataset, which are MRI images, further validate the effectiveness and efficiency of E2ENet across both CT and MRI medical image analysis.

Due to the limited space and the similarity between the BTCV and AMOS-CT challenges, both of which involve multi-organ segmentation in CT images, the results of the **BTCV challenge** are provided in Appendix A.8.2.

### Ablation Studies

In this section, we investigate the impact of two factors on the performance of E2ENet: (i) the DSEF mechanism and (ii) restricted depth-shift in 3D convolution. Specifically, we consider the following scenarios: (#1) w/ DSEF: DSEF mechanism is used to dynamically activate feature map, otherwise, all feature maps are activated during training; (#2) w/ shift: restricted depth/height/width shifting is applied on feature maps before the convolution operation, otherwise, the convolution is performed directly as a standard 3D convolution without shifting operation.

    &  &  &  & FLOPs 1  & PT score \\  & ED & ET & NET & (\%) \(\) & (M) \(\) & (G) \(\) & \(\) \\   DiNTS & 80.2 & 61.1 & 77.6 & 73.0 & / & / & / \\ UNet++ & 80.5 & 62.5 & 79.2 & 74.1 & 58.38 & 3938.25 & 1.12 \\ nnUNet & 81.0 & 62.0 & 79.3 & 74.1 & 31.20 & 1076.62 & 1.35 \\  E2ENet (\(S=0.7\)) & **81.2** & **62.7** & **79.5** & **74.5** & 11.24 & 1067.06 & 1.57 \\ E2ENet (\(S=0.8\)) & 81.0 & 62.5 & 79.0 & 74.2 & 9.44 & 780.97 & 1.72 \\ E2ENet (\(S=0.9\)) & 80.9 & 62.5 & 79.4 & 74.3 & **76.3** & **494.52** & **2.00** \\   

Table 2: 5-fold cross-validation of segmentation performance on the BraTS Challenge training set in the MSD. Note: ED, ET, and NET denote edema, enhancing tumor, and non-enhancing tumor, respectively.

   w/ & \( T\) & w/ & shift size & kernel size & mDice & Params & FLOPs & mNSD \\ DSFF (\#) & shrink & & & (\%) \(\) & (M) \(\) & (G) \(\) & (\%) \(\) \\    \(\) \\ \(\) \\ \(\) \\ \(\) \\  } & / & / & \((-1.0,1)\) & \((1 3 3)\) & **90.2** & 23.90 & 3069.55 & 82.6 \\  & / & / & \((1 3 3)\) & 88.6 & 23.90 & 3069.55 & 78.6 \\  & 1200 & \(\) & \((-1.0,1)\) & \((1 3 3)\) & 90.1 & 11.23 & 669.32 & 82.3 \\  & 1200 & \(\) & \((1 3 3)\) & 88.2 & 11.23 & 696.32 & 79.4 \\   \(\) \\ \(\) \\  } & 1200 & \(\) & \((-2.0,2)\) & \((1 3 3)\) & 90.8 & 11.23 & 696.32 & 82.0 \\  & 1200 & \(\) & \((-2.0,3)\) & \((1 3 3)\) & 89.7 & 11.23 & 696.392 & 81.6 \\  & 1200 & \(\) & \((-7.0,7)\) & \((1 3 3)\) & 87.6 & 11.23 & 696.32 & 77.5 \\   \(\) \\ \(\) \\ \(\) \\  } & 1200 & \(\) & / & \((3 3 3)\) & 90.2 & 52.54 & 5411.57 & 82.5 \\  & 1200 & \(\) & \((3 3 3)\) & 90.1 & 27.97 & 7178.58 & 82.1 \\   \(\) \\ \(\) \\  } & 600 & \(\) & \((-1.0,1)\) & \((1 3 3)\) & 90.0 & 11.23 & 969.32 & 82.2 \\  & 1800 & \(\) & \((-1.0,1)\) & \((1 3 3)\) & 89.9 & 11.23 & 696.32 & 82.0 \\   

Table 3: Ablation study on the effects of the DSEF mechanism and restricted depth-shift in 3D convolution on the validation set of the AMOS-CT Challenge.

**Effect of DSEF Mechanism.** Table 3 (rows 1 and 3) demonstrates that the E2ENet with the DSEF mechanism, achieved comparable performance while using three times fewer parameters and inference FLOPs. Table 4 shows that removing the DSEF mechanism caused the mDice score to drop from 74.5 % (row 4) to 74.1 % (row 2) on the BraTS challenge, while also increasing the number of parameters by more than 2\(\) and the inference FLOPs by nearly 3\(\). These ablation study results highlight that dynamic sparse feature fusion (DSEF) helps reduce resource costs while maintaining segmentation performance.

To further investigate the impact of the DSEF mechanism, we compared E2ENet with other multi-scale medical image segmentation methods on the AMOS-CT validation dataset, as shown in Table 5. These methods include DeepLabv3 [Chen et al., 2017], CoTr [Xie et al., 2021], and MedFormer [Gao et al., 2022]. DeepLabv3 uses atrous convolution to capture multi-scale context, CoTr integrates multi-scale features using attention, and MedFormer employs all-to-all attention for comprehensive multi-scale fusion, addressing both semantic and spatial aspects.

We observed that with a sparsity ratio of 0.9, E2ENet outperforms DeepLabv3 in terms of mDice while significantly reducing computational and memory costs by more than 3\(\) and nearly 10\(\), respectively. Additionally, E2ENet with a sparsity ratio of 0.8 matches MedFormer's performance while significantly reducing computational and memory costs by 3\(\) and 4\(\), respectively. This demonstrates the benefits of multi-scale feature aggregation for medical image segmentation, with E2ENet's DSEF mechanism being much more efficient than other multi-scale methods.

Moreover, as shown in Table 2, E2ENet with the DSEF mechanism outperforms other feature fusion architectures, such as UNet++ and DiNTS, on the BraTS challenge. Overall, the results from the BraTS and AMOS-CT challenges demonstrate that the DSEF mechanism provides an effective and efficient approach for feature fusion.

Finally, we also studied the impact of topology update frequency (\( T\)) in the DSEF mechanism and observed that our algorithm is not sensitive to the hyperparameter \( T\).

**Effect of Restricted Depth-Shift in 3D Convolution.** We evaluated the effectiveness of our restricted shift strategy by comparing the performance of E2ENet and its variants. Without restricted depth-shift, the mDice decreased from 74.5% (row 4) to 73.9% (row 3) for the BraTS challenge as shown in Table 4, and decreased from 90.1% (row 4) to 88.2% (row 3) for AMOS-CT (as shown in Table 3). In Table 3, when comparing E2ENet with kernel sizes of 3x3x3 (rows 8 and 9) to E2ENet with kernel sizes of 1x3x3 combined with a depth shift (rows 1 and 3), their segmentation accuracy remains the same, whether DSEF is used or not. In Table 4, we find that E2ENet with kernel sizes of 1x3x3 combined with a depth shift (rows 2 and 4) even improves segmentation accuracy, whether without DSEF or with DSEF, when compared to kernel sizes of 3x3x3 (rows 5 and 6). This further demonstrates that our proposed efficient Restricted Depth-Shift 3D Convolutional layer, which utilizes a 1x3x3 kernel with restricted depth shift, is equivalent to a 3x3x3 kernel in terms of segmentation accuracy. Moreover, it offers significant savings in computational and memory resources. This demonstrates that the use of a 1x3x3 kernel with restricted depth shift is functionally equivalent to a 3x3x3 kernel in terms of segmentation accuracy, all the while offering savings in computational and memory resources.

   w/ & w/ & kernel size &  & mDice & Params & FLOPs \\ DSEF & shift & ED & ET & NET & (\%) \(\) & (M) \(\) & (G) \(\) \\   ✗ & ✗ & \((1 3 3)\) & 80.4 & 62.4 & 79.0 & 73.9 & 23.89 & 3071.78 \\ ✗ & ✗ & \((1 3 3)\) & 81.0 & 62.3 & 79.0 & 74.1 & 23.89 & 3071.78 \\ ✗ & ✗ & \((1 3 3)\) & 80.3 & 62.5 & 79.0 & 73.9 & 11.24 & 1067.06 \\ ✗ & ✗ & \((1 3 3)\) & **81.2** & **62.7** & **79.5** & **74.5** & 11.24 & 1067.06 \\  ✗ & ✗ & \((3 3 3)\) & 80.9 & 61.6 & 79.1 & 74.0 & 52.55 & 4519.26 \\ ✗ & ✗ & \((3 3 3)\) & 81.0 & 62.2 & 79.1 & 74.1 & 28.02 & 2023.52 \\  ✗ & ✗ & \((3 1 3)\) & 80.3 & 61.7 & 78.7 & 73.6 & 11.24 & 1067.06 \\ ✗ & ✗ & \((3 3 1)\) & 80.5 & 61.9 & 78.7 & 73.7 & 11.24 & 1067.06 \\ ✗ & ✗ & \((3 1 3)\) & 81.0 & 62.4 & 78.9 & 74.1 & 11.24 & 1067.06 \\ ✗ & ✗ & \((3 3 1)\) & 81.0 & 62.3 & 79.4 & 74.2 & 11.24 & 1067.06 \\   

Table 4: Ablation study on the effects of the DSEF mechanism and restricted depth-shift in 3D convolution, evaluated through 5-fold cross-validation of segmentation performance on the BraTS Challenge training set in the MSD.

   method & mDice (\%) \(\) & Params (M) \(\) & FLOPs (G) \(\) \\  DeepLabv3 & 89.0 & 74.68 & 1546.25 \\ CoTr & 77.1 & 41.87 & 1510.53 \\ MedFormer & **90.1** & 39.59 & 2332.75 \\ E2ENet (S=0.8) & 90.0 & 9.44 & 778.74 \\ E2ENet (S=0.9) & 89.6 & **76.4** & **492.29** \\   

Table 5: Comparison with other multi-scale medical image segmentation methods on the AMOS-CT validation dataset. The mDice score for MedFormer is sourced from [Gao et al., 2022].

Rows 9 and 10 of Table 4 demonstrate that when restricted shift is applied to the height or width dimensions, the model's performance decreases compared to E2ENet (4th row) with restricted shift on the depth dimension, called as restricted depth-shift. We also observed that the performance of 3D convolution with kernel sizes of \(1 3 3\) (row 3), \(3 1 3\) (row 7), and \(3 3 1\) (row 8) decreased significantly without restricted shift compared to their counterparts with restricted shift on the corresponding dimensions (rows 4, 9, and 10, respectively). These phenomena demonstrate the effectiveness of restricted shift, especially restricted depth-shift, for medical image segmentation.

Furthermore, we evaluated the impact of different shift sizes on the model's performance. As shown in Table 3, we compared the performance of E2ENet with shift sizes of (-1, 0, 1) to (-2, 0, 2), (-3, 0, 3) and (-7, 0, 7), and observed a decrease in performance from \(90.1\%\) to \(89.8\%\), \(89.7\%\) and \(87.6\%\), respectively, as the shift size increased. Increasing the shift size means considering more depth-wise information at the expense of channel-wise information, leading to an insufficient representation of channels, as discussed in Section 2.3. Additionally, a large shift size (equivalent to a large kernel size) leads to a loss of local spatial relationships, which are crucial for segmentation. This results in a blurring effect that reduces the precision of boundary alignment, particularly affecting metrics like mNSD, which rely heavily on accurate boundary information. Therefore, we use a shift size of (-1, 0, 1) in our restricted depth-shift strategy as the default setting in our experiments. Finally, we plot a critical distance diagram to show the statistical significance of our modules in Appendix A.8.3.

### Generalizability Analysis

To evaluate the generalizability of the resulting architectures, we compared our model, pre-trained on AMOS-CT and fine-tuned on AMOS-MRI, with nnUNet, which was pre-trained on AMOS-CT and fine-tuned on AMOS-MRI, and both models are fine-tuned for 250 epochs. It is worth noting that the topology (weight connections) is determined through AMOS-CT pre-training and remains fixed during the fine-tuning phase. From Table 6, we discovered that the E2ENet architecture, initially designed for AMOS-CT, can be effectively applied to AMOS-MRI as well. This adaptation resulted in improved performance compared to nnUNet and other baselines. nnUNet and these baselines were either transferred from CT, trained solely on MRI data, or jointly on MRI and CT datasets.

In the AMOS-CT dataset, there is a domain shift between the training and test datasets due to variations in the image acquisition scanners (Ji et al., 2022). Thus, we also assess the generalizability of our approach by evaluating its performance on the AMOS-CT test dataset. As demonstrated in Table 7, E2ENet exhibits comparable performance even in the presence of domain shift when compared to nnUNet. This is achieved while maintaining lower computational and memory costs.

   method &  &  \\  & CT \(\) MRI & MRI & CT \(\) MRI & MRI & CT + MRI \\   E2ENet & **87.8** & 7 & 7 & **83.5** & 7 & 7 \\ nnUNet & 87.4 & 87.1 & 87.7 & 83.3 & 83.1 & 82.7 \\ vnet & \(\) & 83.9 & 75.4 & / & 65.8 & 65.6 \\ nnFNer & \(\) & 80.6 & 75.48 & / & 74.0 & 66.3 \\ CoTr & \(\) & 77.5 & 73.46 & / & 78.0 & 65.35 \\ Swin UNETR & \(\) & 75.7 & 77.52 & / & 65.3 & 69.10 \\ UNTER & \(\) & 75.3 & / & / & 70.1 & / \\   

Table 6: Evaluating the generalizability of E2ENet on the AMOS-MRI dataset. CT \(\) MRI: pretrained on the CT dataset and fine-tuned on the MRI dataset; MRI: trained solely on the MRI dataset; CT+MRI: trained on both the CT and MRI datasets. Results for all models, except E2ENet and nnUNet (CT \(\) MRI), are sourced from the AMOS website.

   Methods &  &  & FLOPs & PT score & mNSD \\  & (\%) \(\) & (M) \(\) & (G) \(\) & \(\) & (\%) \(\) \\   CoTr & \(80.9\) & 41.87 & 1510.53 & 1.11 & 66.3 \\ nnFormer & \(85.6\) & 150.14 & 1343.65 & 1.11 & 72.5 \\ UNETR & \(79.4\) & 93.02 & **391.03** & 1.41 & 60.8 \\ Swin-UNETR & \(86.3\) & 62.83 & 1562.99 & 1.13 & 73.8 \\  VNet & \(82.9\) & 45.65 & 1737.57 & 1.11 & 67.6 \\ nnUNet\({}^{}\) & 90.6 & 30.76 & 1067.89 & 1.31 & 82.0 \\ nnUNet\({}^{}\) & **91.0** & 30.76 & 1067.89 & 1.31 & 82.6 \\  E2ENet\({}^{*}\) (s=0.7) & 90.7 & 1.123 & 969.32 & 1.54 & 82.2 \\ E2ENet\({}^{*}\) (s=0.8) & 90.7 & 9.44 & 778.74 & 1.65 & 82.1 \\ E2ENet\({}^{*}\) (s=0.9) & 90.4 & **7.64** & 492.29 & **1.89** & 81.4 \\  E2ENet (s=0.7) & 90.6 & 11.23 & 969.32 & 1.54 & 82.0 \\ E2ENet (s=0.8) & 90.4 & 9.44 & 778.74 & 1.65 & 81.8 \\ E2ENet (s=0.9) & 90.1 & **7.64** & 492.29 & **1.89** & 80.7 \\   

Table 7: Quantitative comparisons of segmentation performance on AMOS-CT test dataset. \({}^{}\) and \({}^{}\) denote the results with and without postprocessing that are reproduced by us. \({}^{*}\) indicates the results with postprocessing.

### Model Capacity Analysis

To ensure a fair comparison, we scale down nnUNet by reducing the number of channels at level 1 from 32 to 27 and decreasing the total number of feature scales from 5 to 4, resulting in a modified version referred to as nnUNet (-). Additionally, we scale up E2ENet by increasing the number of channels at level 1 from 48 to 58, creating a modified version referred to as E2ENet (+). We evaluated the performance of these models, and the results can be found in Table 8. It is worth noting that scaling down nnUNet resulted in decreased performance in terms of mDice and mNSD, while scaling up E2ENet led to an increase in mDice and comparable performance in terms of mNSD. This indicates that the comparable performance of the memory and computation-efficient E2ENet is not attributed to the dataset's requirement for a small number of parameters and computations.

### Qualitative Results

In this section, we compare the proposed E2ENet and nnUNet qualitatively on three challenges. To make the comparison easier, we highlight detailed segmentation results with red dashed boxes.

Figure 5 presents a qualitative comparison of our proposed E2ENet with nnUNet on the AMOS-CT challenge. As a widely used baseline, nnUNet has been evaluated on multiple medical image segmentation challenges. Our results demonstrate that, on certain samples, E2ENet can improve segmentation quality and overcome some of the challenges faced on the AMOS-CT challenge. For example, as shown in the first column, E2ENet more accurately distinguishes between the stomach and the esophagus, which is challenging due to their close proximity. In the second column, E2ENet better differentiates the duodenum from the background, while in the third column, E2ENet accurately identifies the precise boundaries of the liver, a structure that is prone to over-segmentation. These examples demonstrate the potential of our proposed method to improve the accuracy of medical image segmentation to some extent, especially in challenging cases such as distinguishing between closely located organs or accurately segmenting complex shapes. The Qualitative Results of BTCV and BraTS Challenge can be found in Appendix A.9.1.

### Feature Fusion Visualization

In this section, we explore how the DSFF mechanism fuses features from three directions (upward, forward, and downward) during training. To shed light on this mechanism, we specifically analyze the proportions 5 of feature map connections from different directions to a specific fused feature node, considering a feature sparsity level of 0.8. After training, we observed in Figure 6 (d) that the DSFF module learned to assign greater importance to features from the "forward" directions for most fused feature nodes. For example, in the first feature level, the flow and processing of features can be observed as if they were passing through a fully convolutional neural network (FCN), which preserves the spatial dimensions of the input image. At the second feature level, the original image is downsampled by a factor of \(1/2\) and flows through another FCN. Therefore, from this perspective, E2ENet can take advantage of FCN to effectively incorporate and preserve multi-scale information.

Figure 5: Qualitative comparison of E2ENet and nnUNet on the AMOS-CT challenges.

   Method & mDice & Params & FLOPs & mNSD \\  & (\%) \(\) & (M) \(\) & (G) \(\) & (\%) \(\) \\   E2ENet (S=0.8) & 90.0 & **9.43** & 778.74 & **82.3** \\ nnUNet & 90.0 & 31.20 & 1067.89 & 82.1 \\  nnUNet (-) & 89.7 & 12.96 & **755.79** & 81.9 \\  E2ENet (+) & **90.4** & 10.37 & 1119.17 & 82.2 \\   

Table 8: A comparison under a similar FLOPs budget on the AMOS-CT challenge in two cases: when nnUNet is scaled down and when E2ENet is scaled up.

Simultaneously, the complementary feature flows from the 'upward' and 'downward' directions provide richer information. At the later fusion step of the lower feature levels, the 'upward' information becomes more dominant than the 'downward' information. This prioritization of upward feature flow is similar to the design of the decoders in UNet and UNet++. While E2ENet alleviates the semantic gap more effectively, the proportion of feature map connections in the 'downward' direction always has a certain ratio. This also allows the network to better preserve low-level information and integrate it with high-level information, leading to improved performance in capturing fine details. Interestingly, this trend becomes increasingly apparent during training, as illustrated in Figure 6 (a), (c) and (d).

## 4 Conclusion

In this work, our aim is to address the challenge of designing a 3D medical image segmentation method that is both accurate and efficient. By proposing a dynamic sparse feature fusion mechanism and incorporating restricted depth-shift in 3D convolution, our E2ENet improves performance on 3D medical image segmentation tasks while significantly reducing computational and memory overhead. The dynamic sparse feature fusion mechanism demonstrates its ability to adaptively learn the importance of each feature map, zeroing out the less important ones. This results in a more efficient feature representation without compromising performance. Additionally, the experiments demonstrate that restricted depth-shift in 3D convolution enables the model to capture spatial information more effectively and efficiently. Extensive experiments on three benchmarks demonstrate that E2ENet consistently achieves a superior trade-off between accuracy and efficiency compared to previous state-of-the-art baselines. While E2ENet provides a promising solution for balancing accuracy and computational cost, future work could explore the potential of a learnable shift offset, which may further improve performance. Additionally, as plug-and-play components, the DSFF mechanism and restricted depth-shift could be interesting to apply to other 3D segmentation models to explore their potential in the future. Furthermore, future advancements in hardware support for sparse neural networks could fully unlock the potential of sparse training methods.

## 5 Acknowledgements

Qiao Xiao is supported by the research program 'MegaMind - Measuring, Gathering, Mining and Integrating Data for Self-management in the Edge of the Electricity System', (partly) financed by the Dutch Research Council (NWO) through the Perspective program under number P19-25. Elena Mocanu is partly supported by the Modular Integrated Sustainable Datacenter (MISD) project funded by the Dutch Ministry of Economic Affairs and Climate under the European Important Projects of Common European Interest - Cloud Infrastructure and Services (IPCEI-CIS) program for 2024-2029. This research used the Dutch national e-infrastructure with the support of the SURF Cooperative, using grant no. EINF-7499.

Figure 6: The proportions of feature connections during training with the DSFF mechanism at a feature sparsity level of 0.8 on the AMOS-CT challenge.