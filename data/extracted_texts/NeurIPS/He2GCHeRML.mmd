# Empowering Active Learning for 3D Molecular Graphs with Geometric Graph Isomorphism

Ronast Subedi

Florida State University

rs22ce@fsu.edu

&Lu Wei

Stony Brook University

lu.wei.1@stonybrook.edu

&Wenhan Gao1

Stony Brook University

wenhan.gao@stonybrook.edu

&Shayok Chakraborty2

Florida State University

shayok@cs.fsu.edu

&Yi Liu2

Stony Brook University

yi.liu.4@stonybrook.edu

Equal contribution.

###### Abstract

Molecular learning is pivotal in many real-world applications, such as drug discovery. Supervised learning requires heavy human annotation, which is particularly challenging for molecular data; _e.g._, the commonly used density functional theory (DFT) is highly computationally expensive. Active learning (AL) automatically queries labels for the most informative samples, thereby remarkably alleviating the annotation hurdle. In this paper, we present a principled AL paradigm for molecular learning, where we treat molecules as 3D molecular graphs. Specifically, we propose a new diversity sampling method to eliminate mutual redundancy built on distributions of 3D geometries. We first propose a set of new 3D graph isometries for 3D graph isomorphism analysis. Our method is provably at least as expressive as the Geometric Weisfeiler-Lehman (GWL) test. The moments of the distributions of the associated geometries are then extracted for efficient diversity computing. To ensure our AL paradigm selects samples with maximal uncertainties, we carefully design a Bayesian geometric graph neural network to compute uncertainties specifically for 3D molecular graphs. We pose active sampling as a quadratic programming (QP) problem using the proposed components. Experimental results demonstrate the effectiveness of our AL paradigm, as well as the proposed diversity and uncertainty methods. The code is publicly available at https://github.com/sronast/al_3dgraph.

## 1 Introduction

Molecular representation learning is essential for various real-world applications, such as molecular design, drug discovery, material design, _etc._. In recent studies, molecules have been formulated as 3D graphs, based on the evidence that 3D spatial information is crucial to determine the properties of molecules (Liu et al., 2019; Townshend et al., 2019; Axelrod and Gomez-Bombarelli, 2020). Generally, in a 3D graph, atoms are represented as nodes, each associated with Cartesian coordinates in 3D space. A predefined cut-off distance can be used as a threshold to determine if there is an edge between two nodes in the 3D graph. With the advance of deep learning, 3D graph neural networks (GNNs) have been developed to learn from 3D molecular graph data (Thomas et al., 2018; Schutt et al., 2017; Satorras et al., 2021; Gasteiger et al., 2020; Liu et al., 2021, 2022; Wang et al., 2022; Liao and Smidt, 2022; Zhou et al., 2022; Yan et al., 2022; Wang et al., 2023; Lin et al., 2023; Zhang et al., 2023). These models are data-hungry and necessitate a large amount of annotated training datato attain good performance. However, annotation usually consumes excessive manpower, which is particularly challenging for molecules, _e.g._, the commonly used density functional theory (DFT) for molecular energy computing (Hohenberg and Kohn, 1964) is very expensive, inducing a complexity of \(O(n_{e}^{3})\), where \(n_{e}\) is the number of electrons. As a concrete example, DFT can be hundreds of thousands of times slower than a reasonably good GNN for inference (Gilmer et al., 2017).

_Active Learning (AL)_ algorithms automatically identify the salient and exemplar samples from large amounts of unlabeled data (Settles, 2009; Ren et al., 2021). This tremendously reduces the human annotation effort, as only the few samples identified by the algorithm need to be labeled manually. Further, since the deep network gets trained on the representative samples from the underlying data population, it typically depicts better generalization capability than a passive learner, where the training data are selected at random. Deep AL has been used with remarkable success in various applications, such as computer vision (Yoo and Kweon, 2019; Sinha et al., 2019), natural language processing (Zhang et al., 2022), medical diagnosis (Blanch et al., 2017), chemistry (Smith et al., 2018), and anomaly detection (Pimentel et al., 2020) among others. There are a few AL applications for 3D GNNs (Smith et al., 2021; van der Oord et al., 2023); however, these works do not specifically account for 3D geometric information. The 3D geometry of molecules is crucial for determining molecular properties, but it introduces unique challenges in designing effective AL schemes. Currently, a principled AL algorithm for 3D molecular graphs is still lacking.

In this paper, we propose a principled AL paradigm for 3D molecular graphs. We formulate a criterion based on uncertainty and diversity, which ensures that the queried molecules are those where the graph learning model has maximal uncertainty about the labels, and that are also mutually diverse to avoid duplicate sample queries. In particular, diversity computing for 3D graphs is challenging and the _difficulties are twofold_. Firstly, the AL pipeline requires computing the difference between any two 3D molecular graphs, which could have different planar (2D) molecules (entangling different atom numbers, _etc._), in most cases. Secondly, the 3D shape (geometry) of a 3D graph should be captured completely for expressive geometric representations and accurate diversity computation.

To tackle these challenges, we propose a novel diversity sampling method for 3D molecular graphs based on distributions of important 3D geometries. We propose a set of new 3D graph isometries for geometric modeling, which produces geometric representations that are at least as powerful as the Geometric Weisfeiler-Leman (GWL) test (Joshi et al., 2023) in distinguishing 3D graph geometries. This indicates our approach sets an upper bound on the expressive power of any existing 3D GNN models. Hence, the geometries derived from our geometric modeling method (_e.g._, reference distances, triangles) can be used for accurate diversity computing. To compare any two 3D molecules (with different planar graphs), the moments of the distributions of the derived geometries are extracted for final diversity computing of 3D graphs. In addition, to ensure our AL paradigm selects samples with maximal uncertainties, we carefully design a Bayesian geometric GNN specifically for 3D graph uncertainty computing. Our method is shown to be effective and efficient based on a set of ground approximations. With our novel components, we pose the sample selection as a quadratic programming (QP) problem and implement a fast QP solver to identify exemplar molecules to be annotated. Our method is easy to implement and can be applied in conjunction with any 3D GNN architecture.

Overall, our proposed AL paradigm incorporates both diversity and uncertainty for 3D molecular graphs. The diversity component, driven by proposed geometric isometries, captures diverse chemical properties from geometries. The uncertainty component leverages chemical contexts, such as atom types, as node features, enhancing the model's ability to identify and learn from uncertain chemical interactions. By considering both, our method represents a powerful AL paradigm for 3D molecular graphs. We conduct extensive experiments, and the results demonstrate the effectiveness of the proposed diversity and uncertainty methods as well as the overall AL paradigm.

**Our contributions are summarized below.** (i) We propose a principled AL paradigm to alleviate the annotation hurdle of 3D molecular graphs. We employ diversity and uncertainty measures to select the most informative subset for AL. (ii) We introduce a novel diversity component for 3D molecular graphs. Investigating geometric graph isomorphism, we introduce a _model-agnostic_ geometric modeling method, which is provably at least as expressive as the GWL test. Our method can significantly enhance the accuracy of diversity computing for 3D molecular graphs. (iii) Our proposed graph isometries set the theoretical upper bound to the expressive power of all existing 3D GNNs, and thus can serve as the new gold standard to test the expressiveness of various 3DGNNs. (iv) Rooted in Bayesian inference, we develop an effective and efficient pipeline to compute uncertainties for 3D molecular graphs. (v) Our framework significantly outperforms mainstream AL baselines, achieving remarkable efficiency owing to the cheap complexity of \(O(N^{2})\) as well as the implementation of a fast QP solver.

## 2 Methods

### Diversity Computing for 3D Molecular Graphs

In molecular AL tasks, diversity sampling is important for eliminating redundancy, thereby wisely leveraging the annotation budget. The model's capability of capturing the 3D shape diversity among molecules is crucial for informed sampling. A particular challenge is that a diversity measure for two 3D molecules with different planar graphs is indispensable. Methods for diversity measures for 3D molecules with the same planar graph have been developed (Kumar and Zhang, 2018; Kearnes et al., 2016; Gfeller et al., 2013), but a diversity method for two 3D molecules with different planar graphs (entailing different atoms, _etc_) is demanding. Inspired by the USR method (Ballester and Richards, 2007), we propose a novel solution to achieve the goal from the distribution perspective. Generally, we develop a set of new _isometries_ for expressive representations of 3D molecular graphs, after which the distributions of geometries associated with the isometries are obtained for diversity computing.

#### 2.1.1 Isometries of 3D Molecular Graphs

As the first step, we introduce a set of new _isometries_ as a basis, aiming at expressive representations of 3D graphs. As we focus on 3D geometry of molecules in this section, for simplicity, we use 3D point clouds to illustrate our ideas. Let \(A=\{a_{1},a_{2},...,a_{n}\}\) and \(B=\{f(a_{1}),f(a_{2}),...,f(a_{n})\}\) be two sets representing 3D point clouds. Here, each \(a_{i}\) in \(A\) is associated with a positional vector \(}=(x_{a_{i}},y_{a_{i}},z_{a_{i}})\) in 3D space. \(f\) denotes a bijective mapping between \(A\) and \(B\). Then, similarly, each point \(f(a_{i})\) in \(B\) is associated with a positional vector \()}=(x_{f(a_{i})},y_{f(a_{i})},z_{f(a_{i})})\).

Two 3D point clouds, \(A\) and \(B\), are said to be \(E(3)\)-isomorphic, if there exists \( E(3)\) such that \(A= B\). We further choose or compute a consistent reference point (_e.g._, centroid) for each point cloud, denoted as \(r_{1}\) and \(r_{2}\), respectively. Without loss of generality, we use \(a_{}\) to denote the farthest point from the reference point in point cloud \(A\). Below, we will define three levels of isometries, each of which fulfills an isometric mapping between \(A\) and \(B\). To satisfy _any_ isometry, there needs to exist a bijective function \(f:A B\), such that \(}=}\) for any node \(a A\). Here, \(}\) and \(}\) denote the node feature vectors for \(f(a)\) and \(a\), respectively.

**Reference Distance Isometry:** If there exists a collection of global group elements \(_{i} E(3)\), such that \((r_{2},f(a_{i}))=(_{i}r_{1},_{i}a_{i})\) for each point \(a_{i} A\), \(A\) is reference distance isometric to \(B\).

Reference distance isometry involves the Euclidean distance between any atom in the molecule and the predefined reference point.

**Triangular Isometry:** If there exists a collection of global group elements \(_{i} E(3)\), such that \((r_{2},f(a_{}),f(a_{i}))=(_{i}r_{1},_{i}a_{ },_{i}a_{i})\) for each point \(a_{i} A\), \(A\) is triangular isometric to \(B\).

With reference point \(r\), we define the reference vector \(_{0}\) as \(r\) pointing to the farthest point \(a_{}\) in a 3D molecule. Based on reference distance isometry, triangular isometry further involves the angle between \(_{0}\) and other vectors pointing from \(r\) to any other point in the molecule, computed as \(_{k}=^{-1}(_{0}_{k}}{\|_{0}\|\| _{k}\|}),\) where \(_{k}\) denotes vectors originating from \(r\) and directed towards \(k^{}\) atoms in the molecule. The process is illustrated in part A of Fig. 1. For a molecule with \(N\) nodes, we compute \(N-1\) angles. Essentially, such angles provide insights into the spatial arrangement of atoms with respect to the pre-assigned reference vector.

**Cross-angle Isometry:** If there exists a collection of global group elements \(_{ij} E(3)\), such that \((r_{2},f(a_{j}),f(a_{i}))=(_{ij}r_{1},_{ij}a_{j}, _{ij}a_{i}),\)\( a_{i},a_{j} A\)\((i j)\), \(A\) is cross-angle isometric to \(B\).

Beyond the angles in triangular isometry as well as based on reference distance isometry, cross-angular isometry further considers angles formed by any two atoms in the molecule with respect to

Figure 1: The illustrations of encoding the molecular triangular and cross-angular isometries

the reference vector as above. Specifically, for every pair of atoms \(i\) and \(j\), a vector \(_{ij}\) is formed from \(i\) to \(j\). With the reference vector \(_{0}\), the cross angle is computed as \(_{ij}=^{-1}(_{0}_{ij}}{\|_{0}\|\|_{ij}\|})\). This approach, as depicted in part B of Fig. 1, essentially reflects cross-angle information globally. For a molecule with \(N\) nodes, we compute \(N(N-1)/2\) cross angles with the complexity of \(O(N^{2})\).

Next, we propose **Theorem 1** to indicate the relationship between these three isometries as below.

**Theorem 1**.: _If \(A\) and \(B\) are triangular isometric, then \(A\) and \(B\) are reference distance isometric; If \(A\) and \(B\) are cross-angle isometric, then \(A\) and \(B\) are triangular isometric._

The proof of **Theorem** 1 can be found in Appendix A.1. Generally, we define three levels of isometries for graph iso-morphism. _Reference distance isometry_ ensures that the Euclidean distance between each point and a predefined reference point is consistent in two different point clouds. _Triangular isometry_ further manifests the spatial arrangement of atoms referring to the pre-assigned pivot. Built on _triangular isometry_, _cross-angular isometry_ then reflects the pair-wise global information. An illustrative example for _triangular isometry_ and _cross-angular isometry_ is also given in Fig. 2. Clearly, cross-angular isometry represents the strictest isometry among the three. In the following Sec. 2.1.2, we show that a designed geometric representation based on _cross-angular isometry_ can exhibit great expressive power.

#### 2.1.2 Expressive Power of Our Geometric Representations

In this section, we aim to formally elucidate the expressive power of a geometric representation (GR) based on our developed isometries in Sec. 2.1.1. Naturally, we formulate \(GR_{}\) as a set containing all reference distances, triangles, and cross angles in a 3D graph.

We explore the Geometric Weisfeiler-Leman (GWL) test (Joshi et al., 2023), and then leverage GWL to illustrate the expressiveness power of our model. GWL test is an extension of the classic WL Test, enhancing its capabilities by incorporating both the topological structure of the graph and the geometric attributes of its vertices. Such an integration allows the GWL test especially apt for evaluating all 3D graph representation methods. Similar to the regular WL test, GWL test imposes an upper bound to the expressive power of 3D GNNs, _i.e._, if GWL test fails to distinguish two 3D graphs, then all existing 3D GNNs would also fail. See details of the GWL test in Appendix A.2.

**Proposition 1**.: \(GR_{}\) _is at least as expressive as the GWL test. In other words, \(GR_{}\) suffices to distinguish any non-isomorphic molecular structures that are distinguishable by any 3D GNN._

The proof of **Proposition** 1 can be found in Appendix A.1. In conclusion, the molecular geometric representation \(GR_{}\) developed in this work has the greater expressive power than the GWL test, which indicates our diversity sampling method is accurate enough to capture the 3D shape diversity among different molecules. Notably, as mentioned before, GWL test sets the upper bound to the expressiveness of any existing 3D GNNs. _Apparently, our geometric representation \(GR_{}\) is provably at least as powerful as any existing 3D GNN for learning geometric features._ Essentially, the three isometries associated with \(GR_{}\) define expressiveness at different levels. For example, as only considering distance information, a well pretrained SchNet is upper bounded by reference distance isometry (but not triangular isometry or cross-angular isometry); as a more powerful model than SchNet, a well pretrained DimNet is upper bounded by triangular isometry (but not cross-angular isometry). Additionally, learning accurate geometric representations requires a perfectly pretrained 3D GNN model, which is hard to guarantee in practice. _Our isomorphy study provides a deterministic and model-agnostic diversity component for 3D graphs, avoiding the need of a 'perfectly' pretrained 3D GNN model, as well as achieving a theoretically guaranteed upper bound of the expressiveness of all existing 3D GNN models._

#### 2.1.3 Final Distributional Representations

Based on the isomorphy study in Sec. 2.1.1, we obtain our geometric representation method \(GR_{}\) and prove \(GR_{}\) possesses greater expressive power than any existing 3D GNN models in Sec.

Figure 2: \(A\) and \(B\) are triangular isometric but not cross-angular isometric. The angles \( br_{1}a_{far}\), \( cr_{1}a_{far}\), and \( dr_{1}a_{far}\) in structure \(A\) are equal to the angles \( f(b)r_{2}f(a_{far})\), \( f(c)r_{2}f(a_{far})\), and \( f(d)r_{2}f(a_{far})\) in structure \(B\), respectively. However, the cross angle \( dr_{1}c\) is not equal to the cross angle \( f(d)r_{2}f(c)\).

2.1.2. In this section, we aim to extract the distributions of the _entangled three geometries in \(GR_{}}\), including reference distances, triangles, and cross angles_, for diversity computing. Fortunately, we have the theorem (Hall, 1983) implying that the sequence of translated moments can be used to determine the original distribution. Following the USR work (Ballester and Richards, 2007), for each of the three aforementioned geometries, we also use four reference points to reflect the "translated" geometries; those are, the centroid (denoted as ctd) computed by the mean position of all the atoms in the 3D molecule, the point closest to the centroid (denoted as cst), the point farthest from the centroid (denoted as fct), and the point farthest from fct (denoted as ftt). For each reference point, we use a set of moments, including mean, variance, skewness, and kurtosis, which describe a distribution from different angles, _e.g._, skewness indicates the asymmetry and kurtosis describes the tailedness of a distribution. Detailed formulae for these moments can be found in the Appendix A.3. Notably, we compute these translated moments for all three entangled geometries as above. Eventually, we obtain summarized representations of distributions over geometries of 3D graphs, capturing essential characteristics of a molecule's shape.

We use cross angles as an example to describe the final distributional vector. For a molecule with \(N\) atoms, as shown in Fig. 1, we can obtain a set of cross angles \([_{ij}^{}]_{i j,0<i,j<N}\) for a reference point (_e.g._, ctd). After applying statistical moments as an approximation, we can obtain a 4-dimensional vector \(}^{}}=[m_{}^{},v_{}^{},s_{}^{},k_{}^{}]\), where the four elements denote the mean, variance, skewness, and kurtosis for this reference point, respectively. We perform a similar process for all four reference points mentioned above. By doing this, we can obtain four 4-dimensional vectors including \(}^{}}\), \(}^{}}\), \(}^{}}\), and \(}^{}}\), which are then concatenated together, resulting in the final 16-dimensional vector to represent the distribution of cross angles. We repeat the similar process for reference distances and triangles, and then all three corresponding 16-dimensional vectors are further concatenated as a 48-dimensional distributional vector to represent the geometric information of the input molecule. The 48-dimensional distributional vectors are then used to compute the diversity matrix. For any two molecules \(n_{1}\) and \(n_{2}\) in the dataset with \(N\) molecules, we perform the inner product on their distributional vectors to achieve the similarity, and then use \(1-\) similarity to obtain the final value \(D_{n_{1}n_{2}}\) as the diversity measure between them. Finally, a matrix \(D^{N N}\) is obtained, which contains the diversity between every pair of molecules.

**Comparing Our Method to Traditional Structural Descriptors.** Our method generates a 48-dimensional vector that encodes the geometric structure of a molecule. This representation is both equivariant to roto-translations and invariant to atomic permutations as the statistical quantities remain unchanged under such transformations. In contrast, Smooth Overlap of Atomic Positions (SOAP) (Bartok et al., 2013; De et al., 2016; Jager et al., 2018) generates atom-wise vectors that capture local atomic environments by employing spherical harmonics and radial basis functions. While SOAP is also equivariant to roto-translations, it is not invariant to atomic permutations. On the other hand, Atomic Cluster Expansion (ACE) (Drautz, 2019) uses a systematic expansion to describe interactions of varying orders (_e.g._, two-body, three-body interactions). However, ACE is less of a traditional descriptor compared to our method and SOAP; it is designed to provide a complete and systematic representation of atomic interactions by focusing on higher-order expansions (e.g., two-body, three-body interactions). This makes ACE more comprehensive in capturing the physical interactions within a system, but less suited for producing a fixed-dimensional, flexible descriptor. Unlike our method and SOAP, which generate more compact and adaptable descriptors, ACE emphasizes thorough expansions, making it less ideal for tasks requiring flexible, low-dimensional representations that can adapt easily to the active learning scheme. An empirical comparison between our method and the approach that uses SOAP will be provided, highlighting the effectiveness of our method in capturing molecular geometries.

### Uncertainty Computing for 3D Molecular Graphs

In Sec. 2.1, we develop an effective method for diversity computing among different 3D molecular graphs. In addition to selecting diverse molecules, it is important to select molecules where the model has maximal prediction uncertainty about the labels, so as to append maximal information to the model. Uncertainty qualification is well-studied in planar graph analysis (Hirschfeld et al., 2020), but an effective paradigm for 3D molecular graphs is currently lacking. Additionally, existing methods, such as Bayesian neural networks (BNNs) (Lampinen and Vethari, 2001; Titterington, 2004; Goan and Fookes, 2020) and deep model ensemble methods (Lakshminarayanan et al., 2017; Huang et al., 2017), are excessively computationally expensive, limiting their capacity in 3D graph analyses. In a concurrent work (Thaler et al., 2024) on active learning for partial charge prediction of metal-organic frameworks, a dropout Monte Carlo scheme has been proposed to lessen these issues.

In this work, we develop an effective and efficient method, known as Bayesian geometric graph neural network (BGGNN), that takes a 3D graph as input and produces the demanding properties as well as uncertainty values, _e.g._, mean and variance. Formally, a 3D graph is represented as \(=(V,E,P)\), where \(V\) denotes the set of vertices (atoms), \(E\) denotes the set of edges (bonds), and \(P\) denotes the set of Cartesian coordinates for all atoms. A 3D molecular graph is associated with a set of properties, denoted as \(\). Recently, researchers have developed 3D GNNs, such as SchNet (Schutt et al., 2017), DimeNet (Gasteiger et al., 2020), SphereNet (Liu et al., 2022), and GemNet (Gasteiger et al., 2021), for 3D graph representation learning. The likelihood of a 3D GNN can be represented as \(p_{}(,)\), where 3DGNN indicates any existing 3D GNN and \(\) denotes the set of parameters of the used 3D GNN. We also use \(p_{}()\) to represent the prior distribution for the parameters. Assume we collect a new input and output pair, denoted as \(^{*}\) and \(^{*}\). Then based on the conventional Bayesian theorem, Bayesian inference for this new output \(^{*}\) is given by

\[p_{}(^{*}^{*},, )=_{^{n}}p_{}(^{*}^{*},)p_{}(,)d ,\] (1)

where \(^{n}\) is the whole space of \(n\) parameters in 3DGNN. It's infeasible to perform the above integration on \(^{n}\) due to prohibitive computational cost. To tackle this, the variational inference method is introduced to approximate \(p_{}(,)\) with the parameterized \(q_{}()\) through minimizing the Kullback-Leibler (KL) divergence between these two distributions. After applying Bayesian theorem once more, the minimization objective becomes

\[_{}()=-_{^{n}}q_{}()  p_{}(,)d+(q_{}()\|p_{}()),\] (2)

To completely avoid the integration over the whole parameter space, the MC-dropout method (Gal and Ghahramani, 2016; Srivastava et al., 2014) is further used in our BGGNN. Specifically, it employes the Monte-Carlo estimator (Gal et al., 2016; Gal and Ghahramani, 2016) to approximate the integration by performing summation over the sampled models. In practice, researchers implement an MC-dropout network by using dropout as the network's regularization(Gal and Ghahramani, 2016). Following this, we propose to insert dropout layers after the linear layers in our used 3DGNN as an effective yet efficient estimation of Bayesian inference.

Now as we have obtained the variational predictive distribution of a new output with \(q_{}()\), we can easily compute the predictive mean and variance of this distribution. For the molecular property prediction tasks, after we sample \(N\) outputs from the same input, the heteroscedastic predictive uncertainty is then given by

\[}(^{*}^{*})=_{n=1}^{N}(}_{n}^{*})^{2}-( _{n=1}^{N}}_{n}^{*})^{2}+_{n=1}^{N} _{n}^{2},\] (3)

where \(}_{n}^{*}\) is the \(n^{th}\) sampled output and \(_{n}^{2}\) is the variance that is the same among all the data samples. By doing this, we can obtain an uncertainty value (variance) for each molecule. Additionally, built on a 3D GNN, our BGGNN can faithfully produce a set of molecular properties \(\).

Practically, any of the existing 3D GNN can be used as the backbone network for property prediction and uncertainty computing. In this study, we employ SphereNet (Liu et al., 2022) as our 3DGNN, owing to its great power in incorporating 3D geometric information. We apply dropout layers onto the linear layers of SphereNet for Bayesian inference in our BGGNN. To allow more accurate AL selections, we particularly employ the concrete dropout with a learnable dropout rate (Gal et al., 2017) in our BGGNN. Overall, our method is shown to be an effective and efficient paradigm for 3D graph uncertainty computing, as further empirically demonstrated in Sec. 4.

### Active Sampling

A schematic diagram of our active sampling framework is depicted in Fig. 6 and described in A.4 in Appendix. Specifically, in Sec. 2.1, we obtain the matrix \(D^{N N}\) containing the mutual diversity between every pair of unlabeled molecules, where \(N\) is the number of unlabeled molecules. In Sec. 2.2, we employ our designed BGGNN to achieve the vector \(r^{N 1}\) quantifying the prediction uncertainty score of each unlabeled molecule. In the AL setting, our objective is to select a batch of \(k\) unlabeled molecules (\(k\) is a pre-defined query batch size) with high prediction uncertainty and high mutual diversity among them. Let \(z\{0,1\}^{N 1}\) be a binary vector with \(N\) entries which denotes whether the unlabeled molecule \(x_{i}\) will be included in the batch (\(z_{i}=1\)) or not (\(z_{i}=0\)). The molecule selection can thus be posed as the following optimization problem as in Eq. (4), where \(\) is a weight parameter governing the relative importance of the two terms. This is a standard quadratic programming (QP) problem; we relax the integer constraints into continuous constraints and solve the problem using an off-the-shelf QP solver. In this work, we employ the widely used Operator Splitting Quadratic Program (OSQP) (Stellato et al., 2020) to solve the QP problem in Eq. (4). We then apply a greedy approach to project the continuous solution back to the binary space, where the \(k\) highest entries of the continuous solution vector are set to \(1\) and the remaining to \(0\). Such an approach is commonly used to convert continuous solutions obtained from a QP solver to binary solutions in AL (Chattopadhyay et al., 2013; Wang and Ye, 2013). To accelerate the optimization, we implement a solution to execute the problem in the GPU (instead of the CPU) using the parallel implementation of the alternating direction method of multipliers, as detailed in Schubiger et al. (2020). Notably, the predictions in the main tasks (_e.g._, molecular properties) are produced by our BGGNN built on SphereNet as in Sec. 2.2.

## 3 Related Work

### Active Learning

AL is a well-researched problem in the machine learning community (Settles, 2009). There exist two commonly used strategies for AL sampling. Uncertainty based sampling queries unlabeled samples with the highest prediction uncertainties for annotation. Diversity/representativeness based sampling aims to select the subset that can well represent the entire data distribution. A full review of the two AL sampling methods is provided in Appendix A.5.

### Molecular Shape Similarity

Molecular shape similarity plays a pivotal role in drug discovery and virtual screening of compounds (Kumar and Zhang, 2018; Murgueitio et al., 2012; Shang et al., 2017). Methods predominantly fall into several categories (Kumar and Zhang, 2018), including descriptor-based methods (Schreyer and Blundell, 2012; Cannon et al., 2008; Li et al., 2016; Armstrong et al., 2009; Zhou et al., 2010), atom-centered Gaussian-based methods (Haque and Pande, 2010; de Lima and Nascimento, 2013; Yan et al., 2013), surface-based methods (Hobauer et al., 2004; Mavridis et al., 2007; Cai et al., 2012; Karaboga et al., 2013; Venkatraman et al., 2009; Sael et al., 2008), _etc_. Descriptor-based methods are notably represented by the Ultrafast Shape Recognition (USR) algorithm (Ballester and Richards, 2007), which uses statistical moments of the distance distribution to characterize molecular shapes. Gaussian overlay-based methods, with ROCS (Rush et al., 2005; Hawkins et al., 2007) being the most commonly used one, evaluate the maximum volume overlap between two molecules. Surface-based methods typically employ shape signatures (Zauhar et al., 2013) or shape histograms to delineate molecular surfaces for shape similarity assessment. Despite the progress, a principled and theoretically ground similarity method for 3D molecular graphs is currently lacking.

## 4 Experiments

### Experimental Setup

**Implementation Details**: We use two mainstream 3D GNNs SphereNet (Liu et al., 2022) and \(^{++}\)(Gasteiger et al., 2020a) as the backbone models of our BGGNN. We directly use the optimal network configurations from the original papers for both backbone models. We train the network for \(200\) epochs, unless otherwise specified. We use the _Adam Optimizer_ with an initial learning rate \(5 10^{-4}\) and scale it by a factor of \(0.5\) every \(15\) epochs.

**Data and Active Learning Setup**: We first perform experiments on the QM9 benchmark dataset. Since SphereNet is more stable and incorporates more 3D information, we conduct experiments on _mu, alpha, homo, and lumo_ for SphereNet, and _mu and lumo_ for \(^{++}\). These properties have continuous values, making the prediction problem a regression task. We randomly divide the training set of \(110,000\) molecules into three splits of size \(25,000\) each. From each split, we randomly select \(5,000\) molecules as the initial labeled set and the remaining \(20,000\) molecules as the unlabeled set. In each AL iteration, we query \(1,500\) molecules from the unlabeled set, which are labeled and appended to the labeled set. The model's performance is evaluated on a held-out validation set containing \(10,000\) molecules. We save the best-performing model on the validation set and report its performance on the test set containing \(10,831\) molecules. The process is repeated for \(7\) AL iterations, which is taken as the stopping criterion. The final results are averaged over the three splits to rule out the effects of randomness. \(\) in Eq. 4 is taken as \(1\). The Mean Absolute Error (MAE) is used as the evaluation metric. In addition, to study the generalizability of our framework to more geometric data, we also conduct experiments to predict atomic forces for _Aspirin_ in MD17 using our framework.

**Comparison Baselines**: We use four classic AL methods as baselines: _Random Sampling_, _Coreset_[Sener and Savarese, 2018], _Learning Loss_[Yoo and Kweon, 2019], and _Evidential Uncertainty_[Beluch et al., 2018, Amini et al., 2020]. _Random Sampling_ is the default comparison baseline in AL research. _Coreset_ and _Learning Loss_ are two extensively used deep active learning algorithms for regression applications. _Evidential Uncertainty_ is also a commonly used technique to quantify uncertainty for molecular property prediction and was hence included as a comparison baseline. Note some existing studies [Kulichenko et al., 2023, Gusev et al., 2023, Craig and Garcia-Melchor, 2021] have applied AL to molecule research and chemistry. However, these works focus on 2D molecules without considering 3D geometry, which is the focus of our work. Additionally, the techniques used in existing studies can arguably fall into the aforementioned AL categories. Hence, we think comparing with these classic AL methods is sufficient to demonstrate the superiority of our pipeline.

### Active Learning Performance

The active learning performance with SphereNet is depicted in Fig. 3. In each graph, the \(x\)-axis denotes the iteration number and the \(y\)-axis denotes the MAE on the test set. Our analysis revealed that _Evidential Uncertainty_ depicted the worst performance and furnished significantly high error values for all four properties, which obscured the difference in performance among the other methods in the plots. For better interpretation and understanding, we exclude the _Evidential Uncertainty_ method from the plots here and present the results with this baseline in Sec. A.6 of the Appendix. The other baseline methods depict more or less similar performance, with _Coreset_ marginally outperforming the other baselines. Our method comprehensively outperforms all the baselines.

At any given AL iteration, it consistently attains a lower MAE compared to all the baselines.

We also conducted statistical tests of significance using paired t-test to assess whether the improvement in performance achieved by our method is statistically significant. For this purpose, we compared the average MAE achieved by our method against each of the baselines individually. The results are reported in Table 1; each entry in the table denotes the p-value of the paired t-test between our method and the corresponding baseline (denoted in the columns) for the property studied (denoted in the rows). From the table, we note that the improvement in performance achieved by our method is statistically significant (\(p<0.05\)) compared to all the baselines, consistently for all the four properties studied. These results unanimously corroborate the promise and potential of the proposed active sampling method to tremendously reduce the annotation cost in inducing a robust 3D graph neural network for molecular property prediction.

In addition, to study the robustness of our framework to the underlying network architecture and generalizability to the underlying geometric graph data, we have the following results: _1. To study the

    &  \\   & Random & L. Loss & Coreset & Evidential \\  _muk_ & \(7.54 10^{-6}\) & \(5.09 10^{-5}\) & \(1.51 10^{-4}\) & \(2.19 10^{-7}\) \\ _alpha_ & \(1.06 10^{-5}\) & \(8.14 10^{-4}\) & \(4.27 10^{-5}\) & \(2.72 10^{-4}\) \\ _homo_ & \(2.26 10^{-5}\) & \(8.36 10^{-7}\) & \(4.23 10^{-6}\) & \(1.71 10^{-8}\) \\ _lmmo_ & \(4.48 10^{-5}\) & \(1.25 10^{-5}\) & \(3.12 10^{-4}\) & \(2.39 10^{-6}\) \\   

Table 1: The table shows the p-values obtained using paired t-test between the results our method against each of the baselines for all the properties studied. _Here, L. Loss refers to Learning Loss_.

Figure 3: Active learning performance results with SphereNet. The graphs show the mean (averaged over \(3\) runs) and the errorbars for all the methods. We plot the MAE values from the first iteration onwards, to focus on the comparative performance of the methods after they start selecting samples using AL. Best viewed in color.

_robustness of our framework to the underlying network architecture_, results on the _mu and lumo_ properties of the QM9 dataset using DimeNet\({}^{++}\)(Gasteiger et al., 2020) as the backbone model are presented in Section A.7 of the Appendix due to space constraints. The results depict a similar pattern as Figure 3, with the proposed method consistently outperforming all the baselines for both the properties. A paired t-test, presented in Table 3 revealed that the performance improvement achieved by our framework is statistically significant. _2. To study the generalizability of our framework to the underlying geometric graph data_, results on predicting atomic forces for _Aspirin_ molecules in the MD17 benchmark dataset (Chmiela et al., 2017) using our framework are depicted in Section A.8 of the Appendix and further corroborate the potential of our framework.

### Study of Query Budget

The goal of this experiment is to study the effect of query budget (batch size) on the AL performance. The results on the _mu_ property with SphereNet for query budgets \(1,000\), \(1,500\) and \(2,000\) are depicted in Fig. 4. Since _Evidential Uncertainty_ depicted much worse performance than all the methods, it was excluded from this comparison. Our framework once again outperforms all the baselines consistently for all the query budgets. As before, we conducted a paired t-test and the results are presented in Appendix A.9. From the p-values, we conclude that the error values furnished by our method are statistically significantly better (\(p<0.05\)) than all the baselines, consistently for all the query budgets. These results are particularly significant from a practical standpoint as the available query budget in a real-world application is dependent on time, resources, and other constraints.

### Ablation Studies

We conduct ablation studies to examine the power of our diversity computing method, as it is our primary contribution in this research. We perform experiments on the _mu_ and _lumo_ properties with SphereNet from two aspects. Firstly, we compare our framework with only the diversity term in Eq. 4 against _Coreset_, the state-of-the-art diversity-based AL technique. The results are reported in Fig. 5, from which we note that the diversity component of our framework consistently furnishes much lower MAE values than _Coreset_ over all the AL iterations, for both properties. Secondly, we also conducted experiments where we compared the performance of our overall framework (using both uncertainty and diversity) against the baseline where only the uncertainty term in Eq. (4) was used for active sampling. The results revealed that removing the diversity term adversely affected the performance of our framework. A paired t-test revealed that the improvement in performance achieved by our diversity component is statistically significant (\(p<0.05\)) for both these properties (\(p=0.0001\) for _mu_ and \(p=0.04\) for _lumo_). These results show the effectiveness of the proposed diversity metric for AL framework to train a 3D GNN for molecular property prediction. Additionally, we examine the individual impact of diversity and uncertainty components in Appendix A.10. We also compare our proposed diversity component with the SOAP-based diversity, and test our method against BatchBALD (Kirsch et al., 2019), a greedy clustering-based Bayesian uncertainty approach in Appendix A.10.

Figure 4: Study of query budget on the active learning performance. The graphs show the mean (averaged over \(3\) runs) and the errorbars for all the methods. The results with budget \(1500\) are the same as the those presented in Figure 3 and are included here for comparison. Best viewed in color.

### Computation Time Analysis

In this experiment, we analyze the computation time of all the methods studied in this paper. The average time taken to query a batch of unlabeled samples and update the SphereNet model (one active learning iteration) are shown in Table 2. For fair comparison, all the methods were run on the same NVIDIA RTX A4500 20GB GPU.

The computation time of our framework is much less than _Coreset_, which needs to solve a mixed integer programming (MIP) problem. The other three methods have similar computation time, as they don't involve iterative algorithms. Our method takes **only slightly more** time than them, owing to the implementation of a faster QP solver as mentioned in Sec. 2.3, as well as our vectorized implementation to enable the use of GPUs to perform diversity matrix computation. The performance studies in Sec. 4.2 show that our framework is much more accurate than these baselines, and the ablation studies in Sec. 4.4 indicate both the diversity and uncertainty components are necessary to form a QP problem. Given the large margin of performance improvement, we think the efficiency of our method is acceptable.

## 5 Conclusion, Limitations, Future Work, and Broader Impacts

We present a principled active learning framework with the goal of reducing the annotation cost for learning from 3D molecules represented as 3D graphs. The sample selection is posed as a QP problem, which selects samples with high mutual diversity and high uncertainty. Novel diversity and uncertainty components are proposed for 3D graphs, with strong empirical results presented.

We present a model-agnostic diversity component for 3D graphs, and our method is provably at least as powerful as any existing 3D GNN for learning geometric information. Even though our method can set the upper bound of the accuracy of diversity sampling for 3D molecules, it remains unexplored if such an advantage can be incorporated into 3D GNN models for diversity sampling. For example, molecular similarity might be incorporated into 3D GNNs to achieve comparable AL performance. Moreover, our experimental studies focus on small molecules in this work.

As part of future work, we plan to apply our methods to problems where much more accurate but expensive annotation is required, such as computing molecular systems' ground states using the Schrodinger equation. DFT calculations are widely used but still involve approximations, as Schrodinger equation is prohibitively expensive and its use is limited in very small molecules. Our AL pipeline is anticipated to unleash greater potential in such extreme-scale applications. Additionally, given AL needs several interactions with each requiring the model is well-trained, we test our methods on the commonly used but medium-scale QM9 and MD17 datasets in this work. Even though we think the empirical studies are sufficient to support our theory, we still plan to test the scalability of our methods on large-scale molecule datasets, such as OC20 (Chanussoot et al., 2021), in the future.

This work facilitates a new avenue in graph analysis by effective and efficient representation of 3D geometric information, thereby dramatically advancing graph learning and mining. Our methods can reduce the annotation cost for molecular data and also have the potential in a broad set of scientific data types, such as materials and proteins, facilitating various disciplines including basic biology, material science, and quantum chemistry. This work is anticipated to have strong impacts on drug discovery and material design by enabling low-cost representation learning. Any positive and negative societal impact associated with those applications and domains can be applied to our methods.

## 6 Acknowledgment

This work of Y. Liu has used the computational equipment supported by the US Army Research Office under the award W911NF-20-10159. Y. Liu and W. Gao would like to thank Xiaolin Li at Stony Brook University for providing this computational equipment. The work of S. Chakraborty is partially supported by the National Science Foundation under the award IIS-2143424 (NSF CAREER Award).

   Random & L. Loss & Coreset & Evidential & Ours \\  \(53 4.5\) & \(56 2.1\) & \(127 3.5\) & \(56 2.3\) & \(64.9 7.5\) \\   

Table 2: Average (\(\) std) time (minutes) taken by each method for sample selection and training the SphereNet model (one iteration of AL). _Here, L. Loss refers to Learning Loss._