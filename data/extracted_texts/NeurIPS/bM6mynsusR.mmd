# Function Space Bayesian Pseudocoreset

for Bayesian Neural Networks

Balhae Kim\({}^{1}\), Hyungi Lee\({}^{1}\), Juho Lee\({}^{1,2}\)

KAIST AI\({}^{1}\), AITRICS\({}^{2}\)

{balhaekim, lhk2708, juholee}@kaist.ac.kr

###### Abstract

A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass several challenges that may arise when working on a weight space, including limited scalability and multi-modality issue. Through various experiments, we demonstrate that the Bayesian pseudocoresets constructed from our method enjoys enhanced uncertainty quantification and better robustness across various model architectures.

## 1 Introduction

Deep learning has achieved tremendous success, but its requirement for large amounts of data makes it often inefficient or infeasible in terms of resources and computation. To enable continuous learning like humans, it is necessary to learn from a large number of data points in a continuous manner, which requires the ability to discern and retain important information. This motivates the learning of a coreset, a small dataset that is informative enough to represent a large dataset.

On the other hand, the ability to incorporate uncertainties into predictions is essential for real-world applications, as it contributes to the safety and reliability of a model. One approach to achieving this is by adopting a Bayesian framework, where a prior distribution is established to represent our initial belief about the models. This belief is then updated through inference of posterior distributions based on the acquired knowledge. Although this approach shows promise, scalability becomes a concern when working with large-scale datasets during Bayesian inference. To address this issue, a potential solution is to employ a Bayesian coreset. A Bayesian coreset is a small subset of the original dataset where the posterior conditioning on it closely approximates the original posterior conditioning on the full dataset. Once the Bayesian coreset is trained, it can be utilized as a lightweight proxy dataset for subsequent Bayesian inference or as a replay buffer for continual learning or transfer learning.

A Bayesian coreset is constructed by selecting a subset from a large dataset. However, recent research suggests that this approach may not be effective, especially in high-dimensional settings . Instead, an alternative method of synthesizing a coreset, wherein the coreset is learned as trainable parameters, has been found to significantly enhance the quality of the approximation. This synthesized coreset isreferred to as a Bayesian pseudocoreset. The process of learning a Bayesian pseudocoreset involves minimizing a divergence measure between the posterior of the full dataset and the posterior of the pseudocoreset. However, learning Bayesian pseudocoresets is generally challenging due to the intractability of constructing both the full dataset posterior and the pseudocoreset posterior, as well as the computation of the divergence between them, which necessitates approximation. Consequently, existing works on Bayesian pseudocoresets have primarily focused on small-scale problems [19; 8; 20; 21]. Recently, Kim et al.  introduced a scalable method for constructing Bayesian pseudocoresets using variational Gaussian approximation for the posteriors and minimizing forward KL divergence. Although their method shows promise, it still demands substantial computational resources for high-dimensional models like deep neural networks.

In this paper, we present a novel approach to enhance the scalability of Bayesian pseudocoreset construction, particularly for Bayesian neural networks (BNNs) with a large number of parameters. Our proposed method operates in _function space_. When working with BNNs, it is common to define a prior distribution on the weight space and infer the corresponding weight posterior distribution, which also applies to Bayesian pseudocoreset construction. However, previous studies [31; 28] have highlighted the challenge of interpreting weights in high-dimensional neural networks, making it difficult to elicit meaningful prior distributions. Additionally, in high-dimensional networks, the loss surfaces often exhibit a complex multimodal structure, which means that proximity in the weight space does not necessarily imply proximity in the desired prediction variable [24; 30]. This same argument can be applied to Bayesian pseudocoreset construction, as matching the full data and pseudocoreset posteriors in the weight space may not result in an optimal pseudocoreset in terms of representation power and computational scalability.

To be more specific, our method constructs a variational approximation to the pseudocoreset posteriors in function space by linearization and variational approximation to the true posterior. Then we learn Bayesian pseudocoreset by minimizing a divergence measure between the full data posterior and the pseudocoreset posterior in the function space. Compared to the previous weight space approaches, our method readily scales to the large models for which the weight space approaches were not able to compute. Another benefit of function space matching is that it does not constrain the architectures of the neural networks to be matched, provided that their inherited function space posteriors are likely to be similar. So for instance, we can train with multiple neural network architectures simultaneously with varying numbers of neurons or types of normalization layers, and we empirically observe that this improves the architectural robustness of the learned pseudocoresets. Moreover, it has another advantage that the posteriors learned from the Bayesian pseudocoreset in function space have better out-of-distribution (OOD) robustness, similar to the previous reports showing the benefit of function space approaches in OOD robustness .

In summary, this paper presents a novel approach to creating a scalable and effective Bayesian pseudocoreset using function space variational inference. The resulting Bayesian pseudocoreset is capable of being generated in high-dimensional image and deep neural network settings and has better uncertainty quantification abilities compared to weight space variational inference. Additionally, it has better architectural robustness. We demonstrate the efficiency of the function space Bayesian pseudocoreset through the various experiments.

## 2 Background

### Bayesian pseudocoresets

In this paper, we focus on probabilistic models for supervised learning problem. Let \(\) be a parameter, and let \(p(y\,|\,x,)\) be a probabilistic model indexed by the parameter \(\). Given a set of observations \(:=(x_{i})_{i=1}^{n}\) and the set of labels \(:=(y_{i})_{i=1}^{n}\) with each \(x_{i}\) and \(y_{i}\), we are interested in updating our prior belief \(_{0}()\) about the parameter to the posterior,

\[_{}():=()}{Z(\,|\, )}_{i=1}^{n}p(y_{i}\,|\,x_{i},), Z(\,|\, ):=_{}_{i=1}^{n}p(y_{i}\,|\,x_{i},)_{0}( ).\] (1)

However, when the size of the dataset \(n\) is large, the computation of the posterior distribution can be computationally expensive and infeasible. To overcome this issue, Bayesian pseudocoresets are constructed as a synthetic dataset \(=(u_{j})_{j=1}^{m}\) with \(m n\) with the set of labelswhere the posterior conditioning on it approximates the original posterior \(_{}()\).

\[_{}()=()}{Z(}\,|\,)}_{j=1}^{m}p(_{j}\,|\,u_{j},), Z(}\,|\, ):=_{}_{j=1}^{m}p(_{j}\,|\,u_{j},)_{ 0}().\] (2)

This approximation is made possible by solving an optimization problem that minimizes a divergence measure \(D\) between the two posterior distributions 1

\[^{*}=*{arg\,min}_{}\ D(_{},_ {}).\] (3)

In a recent paper , three variants of Bayesian pseudocoresets were proposed using different divergence measures, namely reverse Kullback-Leibler divergence, Wasserstein distance, and forward Kullback-Leibler divergence. However, performing both the approximation and the optimization in the parameter space can be computationally challenging, particularly for high-dimensional models such as deep neural networks.

### Bayesian pseudocoresets in weight-space

Kim et al.  advocates using forward KL divergence as the divergence measure when constructing Bayesian pseudocoresets, with the aim of achieving a more even exploration of the posterior distribution of the full dataset when performing uncertainty quantification with the learned pseudocoreset. The forward KL objective is computed as,

\[ D_{}[_{}\|_{} ]&= Z(}\,|\,)- Z(\,| \,)\\ &+_{_{}}_{i=1}^{n} p( y_{i}\,|\,x_{i},)-_{_{}}_{j=1}^{m}  p(_{j}\,|\,u_{j},).\] (4)

The derivative of the divergence with respect to the pseudocoreset \(\) is computed as

\[_{}D_{}[_{}\|_{}]= _{_{}}_{}_{j=1}^{m} p (_{j}\,|\,u_{j},)-_{}_{_{ }}_{j=1}^{m} p(_{j}|u_{j},))\] (5)

For the gradient, we need the expected gradients of the log posteriors that require sampling from the posteriors \(_{}\) and \(_{}\). Most of the probabilistic models do not admit simple closed-form expressions for these posteriors, and it is not easy to simulate those posteriors for high-dimensional models. To address this, Kim et al.  proposes to use a Gaussian variational distributions \(q_{}()\) and \(q_{}()\) to approximate \(_{}\) and \(_{}\) whose means are set to the parameters obtained from the SGD trajectories,

\[q_{}()=(;_{},_{ }), q_{}()=(;_{},_{ }),\] (6)

where \(_{}\) and \(_{}\) are the maximum a posteriori (MAP) solutions computed for the dataset \(\) and \(\), respectively. \(_{}\) and \(_{}\) are covariances. The gradient, with the stop gradient applied to \(_{}\), is approximated as,

\[}}{S}_{s=1}^{S}(_{j=1}^{m} p _{j}\,|\,u_{j},(_{})+_{}^{1/2} _{}^{(s)}-_{j=1}^{m} p_{j} \,|\,u_{j},_{}+_{}^{1/2}_{}^ {(s)}).\] (7)

Here, \(_{}^{(s)}\) and \(_{}^{(s)}\) are i.i.d. standard Gaussian noises and \(S\) is the number of Monte-Carlo samples.

Expert trajectoriesApproximating the full data and coreset posteriors with variational distributions as specified above requires \(_{}\) and \(_{}\) as consequences of running optimization algorithms untill convergence. While this may be feasible for small datasets, for large-scale setting of our interest, obtaining \(_{}\) and \(_{}\) from scratch at each iteration for updating \(\) can be time-consuming. To alleviate this, in the dataset distillation literature, Cazenavette et al.  proposed to use the _expert trajectories_, the set of pretrained optimization trajectories constructed in advance to the coreset learning. Kim et al.  brought this idea to Bayesian pseudocoresets, where a pool of pretrained trajectories are assume to be given before pseudocoreset learning. At each step of pseudocoreset update, a checkpoint \(_{0}\) from an expert trajectory is randomly drawn from the pool, and then \(_{}\) and \(_{}\) are quickly constructed by taking few optimization steps from \(_{0}\).

Function space Bayesian pseudocoreset

### Function space Bayesian neural networks

We follow the framework presented in Rudner et al. (2017); Rudner et al. (2017) to define a Function-space Bayesian Neural Network (FBNN). Let \(_{0}()\) be a prior distribution on the parameter and \(g_{}:^{d}\) be a neural network index by \(\). Let \(h:(^{d})\) be a deterministic mapping from a parameter \(\) to a neural network \(g_{}\). Then a function-space prior is simply defined as a pushforward \(_{0}(f)=h_{*}_{0}(f):=_{0}(h^{-1}(f))\). The corresponding posterior is also defined as a pushforward \(_{}(f)=h_{*}_{}(f)\) and so is the pseudocoreset posterior \(_{}(f)=h_{*}_{}(f)\).

### Learning function space Bayesian pseudocoresets

Given the function space priors and posteriors, a Function space Bayesian PseudoCoreset (FBPC) is obtained by minimizing a divergence measure between the function space posteriors,

\[^{*}=*{arg\,min}_{}\;D(_{},_{ }).\] (8)

We follow Kim et al. (2017) suggesting to use the forward KL divergence, so our goal is to solve

\[^{*}=*{arg\,min}_{}\;D_{}[_{ }||_{}].\] (9)

The following proposition provides an expression for the gradient to minimize the divergence, whose proof is given A.

**Proposition 3.1**.: _The gradient of the forward KL divergence with respect to the coreset \(\) is_

\[_{}D_{}[_{}||_{}]=- _{}_{[_{}|_{}]}[ p(}_{})]+_{[_{}]_{ }}[_{} p(}_{ })],\] (10)

_where \([_{}]_{}\) and \([_{}]_{}\) are finite-dimensional distributions of the stochastic processes \(_{}\) and \(_{}\), respectively, \(_{}:=(f(u_{j}))_{j=1}^{m}\), and \(p(}_{})=_{j=1}^{m}p(_ {j} f(u_{j}))\)._

To evaluate the gradient Eq. 10, we should identify the finite-dimensional functional posterior distributions \([_{}]_{}\) and \([_{}]_{}\). While this is generally intractable, as proposed in Rudner et al. (2017); Rudner et al. (2017), we can instead consider a linearized approximation of the neural network \(g_{}\),

\[_{}()=g_{_{}}()+_{_{ }}()(-_{}),\] (11)

where \(_{}=_{_{}}[]\) and \(_{_{}}()\) is the Jacobian of \(g_{}\) evaluated at \(_{}\). Then we approximate the function space posterior \(_{}\) with \(_{}:=_{*}_{}\) where \(()=_{}\), and as shown in Rudner et al. (2017); Rudner et al. (2017), the finite dimensional distribution \([_{}]_{}\) is a multivariate Gaussian distribution,

\[[_{}]_{}(_{})= _{} g_{_{}}(),_{_{}}()_{}_{_{}}( )^{},\] (12)

with \(_{}=_{_{}}()\). Similarly, we obtain

\[[_{}]_{}(_{})=_{} g_{_{}}(),_{_{}}()_{}_{_{}}( )^{},\] (13)

with \(_{}:=_{_{}}[]\) and \(_{}:=_{_{}}()\). Using these linearized finite-dimensional distribution, we can approximate

\[_{}D_{}[_{}||_{}]=- _{}_{[_{}|_{}]}[  p(}_{})]+_{[_{}]_{}}[_{} p(} _{})],\] (14)

### Tractable approximation to the gradient

Even with the linearization, evaluating Eq. 14 is still challenging because it requires obtaining \(_{}\) and \(_{}\) which are the statistics of the weight-space posterior \(_{}\). Rudner et al. (2017) proposes to learn a variational approximation \(q_{}()\) in the _weight-space_, and use the linearized pushforward of the variational distribution \(_{*}q_{}\) as a proxy to the function space posterior. Still, this approach requires computing the heavy Jacobian matrix \(_{_{q_{}}[]}()\), so may not be feasible for our scenario where we have to compute such variational approximations _at each_ update of the pseudocoreset \(\).

Instead, we choose to directly construct a variational approximations to the finite-dimensional distributions of the function space posteriors, that is,

\[&[_{}]_{}(_{ }) q_{}(_{})=( _{}\,|\,g_{_{}}(),_ {}),\\ &[_{}]_{}(_{} ) q_{}(_{})=(_{ }\,|\,g_{_{}}(),_{ }),\] (15)

where \((_{},_{})\) and \((_{},_{})\) are variational parameters for the full data and coreset posteriors. Inspired by Kim et al. , we construct the variational parameters using expert trajectories. Unlike , we simply let the MAP solution computed for \(\), \(_{}\), by sampling a checkpoint from the later part of the expert trajectories, and obtain the MAP solution of \(\), \(_{}\), by directly optimizing an initial random parameter. Then we obtain \(_{}\) and \(_{}\) using \(\). For the covariance matrices \(_{}\) and \(_{}\), while Kim et al.  proposed to use spherical Gaussian noises, we instead set them as an empirical covariance matrices of the samples collected from the optimization trajectory. Specifically, we take additional \(K\) steps from each MAP solution to compute the empirical covariance.

\[_{}^{(0)}&=_{ },_{}^{(t)}=(_{}^{ (t-1)},(,)),_{}=g_{( _{}^{(0)})}(),\\ _{}&:=_{k=1}^{K}g_{_{}^{(k)}}^{2}( )-_{k=1}^{K}g_{_{}^{(k)}}( )^{2},\] (16)

where \((,)\) is a step of SGD optimization applied to \(\) with data \(\) and the squares in the \(()\) are applied in element-wise manner. Note also that we are applying the \(gradient}\) operations for to block the gradient flow that might lead to complication in the backpropagation procedure. The variational parameters \((_{},_{})\) are constructed in a similar fashion, but using the psedocoreset \((,})\) instead of the original data \((,)\). It is noteworthy that our approach is similar to one of the methods in Bayesian learning, SWAG . However, while SWAG focuses on collecting statistics on weight space trajectories, our method constructs statistics in function spaces. This distinction makes our approach more suitable and scalable for pseudocoreset construction. The overview of proposed method is provided in Fig. 1.

With the variational approximations constructed as described, we obtain a Monte-Carlo estimator of Eq. 14,

\[&_{}D_{}[_{}| _{}]-_{}_{q_{}( _{})}[ p(}\,|\,_{})]+_{q_{}(_{})}_{ } p(}\,|\,_{})\\ &=-_{}_{p(_{})}[  p(}\,|\,_{}+_{}^ {1/2}_{})]+_{p(_{})} _{} p(}\,|\,_{}+_{}^{1/2}_{})\\ &_{s=1}^{S}-_{}  p}\,|\,_{}+_{ }^{1/2}_{}^{(s)}+_{} p }\,|\,_{}+_{} ^{1/2}_{}^{(s)},\] (17)

where \(p(_{})\) and \(p(_{})\) are standard Gaussians, \((_{}^{(s)})_{s=1}^{S}\) and \((_{}^{(s)})_{s=1}^{S}\) are i.i.d. samples from them.

Figure 1: The conceptual overview of our proposed training procedure.

### Multiple architectures FBPC training

One significant advantage of function space posterior matching is that the function is typically of much lower dimension compared to the weight. This makes it more likely for function spaces to exhibit similar posterior shapes in the vicinity of the MAP solutions. This characteristic of function space encourages the exploration of function space pseudocoreset training in the context of multiple architectures. Because, the task of training a coreset that matches the highly complex weight space posterior across multiple architectures is indeed challenging, while the situation becomes relatively easier when dealing with architectures that exhibit similar function posteriors.

Therefore we propose a novel multi-architecture FBPC algorithm in Algorithm 1. The training procedure involves calculating the FBPC losses for each individual architecture separately and then summing them together to update. This approach allows us to efficiently update the pseudocoreset by considering the contributions of each architecture simultaneously. We will empirically demonstrate that this methodology significantly enhances the architecture generalization ability of pseudocoresets in Section 5.

```
0: Set of architectures \(\), expert trajectories \(\{^{(a)}:a\}\), prior distributions of parameters \(\{_{0}^{(a)}:a\}\), an optimizer opt.  Initialize \(\) with random minibatch of coreset size \(m\). for\(i=1,,N\)do  Initialize the gradient of pseudocoreset, \( 0\). for\(a\)do  Sample the MAP solution computed for \(\), \(_{}^{(a)}\).  Sample an initial random parameter \(_{0}_{0}^{(a)}()\). repeat \(_{t}(_{t-1},(,}))\) until converges to obtain the MAP solution computed for \(\), \(_{}\).  Obtain \(_{}\), \(_{}\), \(_{}\), \(_{}\) by Eq. 16.  Compute the pseudocoreset gradient \(^{(a)}\) using Eq. 17. \(+^{(a)}\). endfor  Update the pseudocoreset \(\) by using the gradient \(\). endfor ```

**Algorithm 1** Multi-architecture Function space Bayesian Pseudocoreset

### Compare to weight space Bayesian pseudocoresets

By working directly on the function space, our method could bypass several challenges that may arise when working on a weight space. Indeed, a legitimate concern arises regarding multi-modality, as the posterior distributions of deep neural networks are highly complex. It makes the optimization of pseudocoresets on weight space difficult. Moreover, minimization of weight space divergence does not necessarily guarantee proximity in the function space. Consequently, although we try to minimize the weight space divergence, there is a possibility that the obtained function posterior may significantly deviate from the true posterior. However, if we directly minimize the divergence between the function distributions, we can effectively address this issue.

On the other hand, there is an additional concern related to memory limitations. While it has been demonstrated in Kim et al.  that the memory usage of Bayesian pseudocoresets employing forward KL divergence is not excessively high, we can see that Eq. 7 requires Monte-Carlo samples of weights, which requires \((Sp)\) where \(S\) and \(p\) represent the number of Monte-Carlo samples and the dimensionality of the weights, respectively. This dependence on Monte-Carlo sampling poses a limitation for large-scale networks when memory resources are constrained. In contrast, our proposed method requires significantly less memory, \((Sd)\) where \(d\) represents the dimensionality of the functions. Indeed, all the results presented in this paper were obtained using a single NVIDIA RTX-3090 GPU with 24GB VRAM.

## 4 Related work

Bayesian coresetsBayesian Coreset [3; 4; 5; 14] is a field of research aimed at addressing the computational challenges of MCMC and VI on large datasets in terms of time and space complexity [9; 1; 26]. It aims to approximate the energy function of the entire dataset using a weighted sum of a small subset. However for high-dimensional data, Manousakas et al.  demonstrates that considering only subsets as Bayesian coreset is not sufficient, as the KL divergence between the approximated coreset posterior and the true posterior increases with the data dimension, and they proposed Bayesian pseudocoresets. There are recent works on constructing pseudocoreset variational posterior to be more flexible  or how to effectively optimize the divergences between posteriors [15; 8; 20; 21]. However, there is still a limitation in constructing high-dimensional Bayesian pseudocoresets specifically for deep neural networks.

Dataset distillationDataset distillation also aims to synthesize the compact datasets that capture the essence of the original dataset. However, the dataset distillation places particulary on optimizing the test performance of the distilled dataset. Consequently, the primary objective in dataset distillation is to maximize the performance of models trained using the distilled dataset, and researchers provides how to effectively solve this bi-level optimization [33; 22; 40; 36; 39; 7]. In recent work, Kim et al.  established a link between specific dataset distillation methods and optimizing certain divergence measures associated with Bayesian pseudocoresets.

Function space variational inferenceAlthough Bayesian neural networks exhibit strong capabilities in performing variational inference, defining meaningful priors or efficiently inferring the posterior on weight space is still challenging due to their over-parametrization. To overcome this issue, researchers have increasingly focused on function space variational inference [6; 2; 16; 30; 25; 32; 27; 17]. For instance, Sun et al.  introduced a framework that formulates the KL divergence between functions as the supremum of marginal KL divergences over finite sets of inputs. Wang et al.  utilizes particle-based optimization directly in the function space. Furthermore, Rudner et al.  recently proposed a scalable method for function space variational inference on deep neural networks.

## 5 Experiments

### Experimental Setup

In our study, we employed the CIFAR10, CIFAR100 and Tiny-ImageNet datasets to create Bayesian pseudocoresets of coreset size \(m\{1,10,50\}\) images per class (ipc). These pseudocoresets were then evaluated by conducting the Stochastic Gradient Hamiltonian Monte Carlo (SGHMC)  algorithm on those pseudocoresets. We measured the top-1 accuracy and negative log-likelihood of the SGHMC algorithm on the respective test datasets. Following the experimental setup of previous works [15; 7; 39], we use the differentiable siamese augmentation . For the network architectures, we use 3-layer ConvNet for CIFAR10 and CIFAR100, and 4-layer ConvNet for Tiny-ImageNet.

We employed three baseline methods to compare the effectiveness of function space Bayesian pseudocoresets. The first baseline is the random coresets, which involves selecting a random mini-batch of the coreset size. The others two baseline methods, BPC-rKL [15; 19] and BPC-fKL , are Bayesian pseudocoresets on weight space. BPC-rKL and BPC-fKL employ reverse KL divergence and forward KL divergence as the divergence measures for their training, respectively.

### Main Results

Table 1 and Table 2 show the results of each baseline and our method for each dataset. For BPC-rKL and BPC-fKL, we used the official code from  for training the pseudocoresets, and only difference is that we used our own SGHMC hyperparameters during evaluation. For detailed experiment setting, please refer to Appendix C.

As discussed earlier, we utilized the empirical covariance to variational posterior instead of using naive isotropic Gaussian for the function space variational posterior. To assess the effectiveness of using sample covariance, we compare these two in Table 1, as we also presented the results forFBPC-isotropic, which represents the FBPC trained with a unit covariance Gaussian posteriors. The results clearly demonstrate that using sample covariance captures valuable information from the posterior distribution, resulting in improved performance. Overall, the results presented in Table 1 and Table 2 demonstrate that our method also outperforms the baseline approaches, including random coresets, BPC-rKL and BPC-fKL, in terms of both accuracy and negative log-likelihood, especially on the large-scale datasets in Table 2.

Furthermore, the Bayesian pseudocoreset can be leveraged to enhance robustness against distributional shifts when combined with Bayesian model averaging. To assess the robustness of our function space Bayesian pseudocoresets on out-of-distribution inputs, we also conducted experiments using the CIFAR10-C dataset . This dataset involves the insertion of image corruptions into the CIFAR10 images. By evaluating the performance of the pseudocoresets on CIFAR10-C, we can see the model's ability to handle input data that deviates from the original distribution. In Table 3, we provide the results for top-1 accuracy and degradation scores, which indicate the extent to which accuracy is reduced compared to the in-distribution's test accuracy. The result demonstrates that our FBPC consistently outperforms the weight space Bayesian pseudocoreset, BPC-fKL.

### Architecture generalization

In this section, we aim to demonstrate the architecture generalizability of FBPC and emphasize the utility of multi-architecture training as we discussed in the previous section. We specifically focus on investigating the impact of varying normalization layers on the generalizability of the pseudocoreset, since it is widely recognized that a pseudocoreset trained using one architecture may struggle to generalize effectively to a model that employs different normalization layers. We have also included the results of cross-architecture experiments that involve changing the architecture itself in Appendix D.1.

To assess this, we compare a single architecture trained pseudocoreset and a multiple architecture trained pseudocoreset. For single architecture training, we initially train a pseudocoreset using one architecture with a specific normalization layer, for instance we use instance normalization. Subsequently, we evaluate the performance of this pseudocoreset on four different types of normalization layers: instance normalization, group normalization, layer normalization, and batch normalization. For multiple architecture training, we aggregate four losses for single architecture training of each architecture, and train the pseudocoreset with the sum of all losses, as mentioned in previous section.

    & SGHMC & Random & BPC-rKL [15; 19] & BPC-fKL  & FBPC-isotropic (Ours) & FBPC (Ours) \\   & Acc (\(\)) & 16.30\({}_{ 0.74}\) & 20.44\({}_{ 1.06}\) & 34.50\({}_{ 1.62}\) & 32.00\({}_{ 0.75}\) & **35.45\({}_{ 0.31}\)** \\  & NLL (\(\)) & 4.66\({}_{ 0.03}\) & 4.51\({}_{ 0.10}\) & 3.86\({}_{ 0.13}\) & **3.40\({}_{ 0.27}\)** & 3.79\({}_{ 0.04}\) \\   & Acc (\(\)) & 32.48\({}_{ 0.34}\) & 37.92\({}_{ 0.66}\) & 56.19\({}_{ 0.61}\) & 61.43\({}_{ 0.35}\) & **62.33\({}_{ 0.34}\)** \\  & NLL (\(\)) & 2.98\({}_{ 0.03}\) & 2.47\({}_{ 0.04}\) & 1.48\({}_{ 0.02}\) & **1.35\({}_{ 0.02}\)** & **1.31\({}_{ 0.02}\)** \\   & Acc (\(\)) & 49.68\({}_{ 0.46}\) & 51.86\({}_{ 0.38}\) & 64.74\({}_{ 0.32}\) & **71.33\({}_{ 0.19}\)** & 71.23\({}_{ 0.17}\) \\  & NLL (\(\)) & 2.06\({}_{ 0.02}\) & 1.95\({}_{ 0.02}\) & 1.26\({}_{ 0.01}\) & **1.03\({}_{ 0.01}\)** & **1.03\({}_{ 0.05}\)** \\   

Table 1: Averaged test accuracy and negative log-likelihoods of models trained on each Bayesian pseudocoreset from scratch using SGHMC on the CIFAR10 dataset. Bold is the best and underline is the second best. These values are averaged over 5 random seeds.

    & &  &  \\  & ipc & 1 & 10 & 50 & 1 & 10 & 50 \\   & Acc (\(\)) & 4.82\({}_{ 0.47}\) & 18.0\({}_{ 0.31}\) & 35.1\({}_{ 0.23}\) & 1.90\({}_{ 0.08}\) & 7.21\({}_{ 0.04}\) & 19.15\({}_{ 0.12}\) \\  & NLL (\(\)) & 5.55\({}_{ 0.07}\) & 4.57\({}_{ 0.01}\) & 3.35\({}_{ 0.01}\) & 6.18\({}_{ 0.04}\) & 5.77\({}_{ 0.02}\) & 4.88\({}_{ 0.01}\) \\  & Acc (\(\)) & 14.7\({}_{ 0.16}\) & 28.1\({}_{ 0.60}\) & 37.1\({}_{ 0.33}\) & 3.98\({}_{ 0.13}\) & 11.4\({}_{ 0.45}\) & 22.18\({}_{ 0.32}\) \\  & NLL (\(\)) & 4.17\({}_{ 0.05}\) & 3.53\({}_{ 0.05}\) & 3.28\({}_{ 0.24}\) & 5.63\({}_{ 0.03}\) & 5.08\({}_{ 0.05}\) & 4.65\({}_{ 0.02}\) \\   & Acc (\(\)) & **21.0\({}_{ 0.76}\)** & **39.7\({}_{ 0.31}\)** & **44.47\({}_{ 0.35}\)** & **10.14\({}_{ 0.68}\)** & **19.42\({}_{ 0.51}\)** & **26.43\({}_{ 0.31}\)** \\  & NLL (\(\)) & **3.76\({}_{ 0.11}\)** & **2.67\({}_{ 0.02}\)** & **2.63\({}_{ 0.01}\)** & **4.69\({}_{ 0.05}\)** & **4.14\({}_{ 0.02}\)** & **4.30\({}_{ 0.05}\)** \\   

Table 2: Averaged test accuracy and negative log-likelihoods of models trained on each Bayesian pseudocoreset from scratch using SGHMC on the CIFAR100 and Tiny-ImageNet datasets. These values are averaged over 3 random seeds.

As depicted in Fig. 1(a), we observe that both WBPC (Weight space Bayesian pseudocoresets) and FBPC-single (Function space Bayesian pseudocoresets trained on a single architecture) exhibit a notable trend, that they tend to not perform well when evaluated on the architecture that incorporates different normalizations, regardless of whether it is trained on weight space or function space. On the other hand, when trained with multiple architectures, both WBPC-multi and FBPC-multi perform well across the all architectures, while notably FBPC-multi significantly outperforms WBPC-multi.

As mentioned in the previous section, we hypothesize that the superior performance of FBPC compared to WBPC can be attributed to the likelihood of having similar function space posterior across architectures. To validate this, we conduct an examination of the logit values for each sample across different architectures. As an illustration, we provide an example pseudocoreset image belonging to the class label "dog" along with its corresponding logits for all four architectures. As Fig. 1(b) shows, it can be observed that the logits display a high degree of similarity, indicating a strong likelihood of matching function posterior distributions. Our analysis confirms that, despite architectural disparities, the function spaces generated by these architectures exhibit significant similarity and it contributes to superiority of FBPC in terms of architecture generalizability.

## 6 Conclusion

In this paper, we explored the function space Bayesian pseudocoreset. We constructed it by minimizing forward KL divergence between the function posteriors of pseudocoreset and the entire dataset. To optimize the divergence, we proposed a novel method to effectively approximate the function posteriors with an efficient training procedure. Finally, we empirically demonstrated the superiority of our function space Bayesian pseudocoresets compared to weight space Bayesian pseudocoresets, in terms of test performance, uncertainty quantification, OOD robustness, and architectural robustness.

LimitationDespite showing promising results on function space Bayesian pseudocoresets, there still exist a few limitations in the training procedure. Our posterior approximation strategy requires

    & corruption & BN & DB & ET & FT & GB & JPEG & MB & PLX & SN & SP & Avg. \\   & Acc (\(\)) & 33.5 & 34 & 35.9 & 25.1 & 33.7 & 39.1 & 32.7 & 38.3 & 28.9 & 41.2 & 34.3 \\  & Degradation (\(\)) & 40.3 & 39.4 & 36 & 55.2 & 39.9 & 30.3 & 41.6 & 31.6 & 48.4 & 26.5 & 38.9 \\   & Acc (\(\)) & 48.9 & 46.4 & 47.6 & 41.7 & 44.0 & 52.0 & 44.3 & 51.0 & 47.1 & 52.3 & **47.5** \\  & Degradation (\(\)) & 21.5 & 25.7 & 23.7 & 33.0 & 29.3 & 16.4 & 28.8 & 18.1 & 24.4 & 16.1 & **23.7** \\   

Table 3: Test accuracy and degradation scores of models trained on each Bayesian pseudocoreset from scratch using SGHMC on the CIFAR10-C. Degradation refers to the extent by which a modelâ€™s accuracy decreases when evaluated on the CIFAR10-C dataset compared to the CIFAR10 test dataset.

Figure 2: Results for multiple architecture FBPC training.

the MAP solutions, which necessitates training them prior to each update step or preparing expert trajectories. This can be time-consuming and requires additional memory to store the expert trajectories.

Societal ImpactsOur work is hardly likely to bring any negative societal impacts.