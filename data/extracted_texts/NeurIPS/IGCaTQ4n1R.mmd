# OpenDign: Open-World Point Cloud Understanding with Depth-Aligned Images

Ye Mao  Junpeng Jing\({}^{*}\) Krystian Mikolajczyk

Imperial College London

https://yebulabula.github.io/OpenDign/

{ye.mao21, j.jing23, k.mikolajczyk}@imperial.ac.uk

Corresponding Author

###### Abstract

Recent open-world 3D representation learning methods using Vision-Language Models (VLMs) to align 3D point clouds with image-text information have shown superior 3D zero-shot performance. However, CAD-rendered images for this alignment often lack realism and texture variation, compromising alignment robustness. Moreover, the volume discrepancy between 3D and 2D pretraining datasets highlights the need for effective strategies to transfer the representational abilities of VLMs to 3D learning. In this paper, we present OpenDign, a novel open-world 3D model using depth-aligned images generated from a diffusion model for robust multimodal alignment. These images exhibit greater texture diversity than CAD renderings due to the stochastic nature of the diffusion model. By refining the depth map projection pipeline and designing depth-specific prompts, OpenDign leverages rich knowledge in pre-trained VLM for 3D representation learning with streamlined fine-tuning. Our experiments show that OpenDign achieves high zero-shot and few-shot performance on diverse 3D tasks, despite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In zero-shot classification, OpenDign surpasses previous models by 8.0% on ModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images for multimodal alignment consistently enhances the performance of other state-of-the-art models.

## 1 Introduction

3D understanding, including tasks like 3D object classification [1; 2], 3D scene segmentation , and 3D reconstruction [4; 5], is becoming increasingly pivotal in real-world applications like augmented/virtual reality [6; 7], autonomous vehicles . Traditional 3D models [1; 2; 9; 10; 11] are closed-world, recognizing only pre-defined categories and struggling with 'unseen' ones. Recent studies aim to leverage Vision-Language Models (e.g., CLIP ) to develop open-world 3D models that generalize beyond'seen' 3D data, enabling zero-shot handling of various 3D tasks.

Existing open-world 3D learning methods can be classified into depth-based and point-based approaches. Depth-based methods [13; 14; 15] convert point clouds into multi-view depth maps and use the pre-trained CLIP image encoder for 3D representations. A significant challenge is the domain gap since CLIP is pre-trained on RGB images, not depth maps. To mitigate this gap, methods like  introduce an additional depth encoder to align depth features with the image and text features from the pre-trained CLIP encoders, as shown in Fig. 1(a). The images used for feature alignment are produced by rendering synthetic CAD models from various camera viewpoints. Point-based methods [16; 17; 18; 19; 20] directly extract 3D representations from point clouds, thereby bypassing the need for depth map projection. However, they also require an extra point encoder for feature alignment to address format disparities between images and point clouds, as shown in Fig. 1(b). Thus, employingan extra 3D encoder for multimodal feature alignment of 3D data, CAD-rendered images, and text modalities is a standard practice in modern open-world 3D learning methods.

Despite widespread usage, we argue that CAD-rendered images fall short of providing the visual diversity and realism necessary for robust multimodal alignment. This limitation arises because CAD models in current open-source datasets [21; 22] often feature simplistic or entirely absent textures. These models also struggle to realistically simulate environmental effects like lighting, shadows, and reflections. Moreover, most CAD models maintain visual coherence across viewpoints, leading to overly consistent textures and colors from all angles. To achieve generalizable 3D representations, each image view for alignment is expected to display significant visual variations (See Fig. 1).

Additionally, pretraining an extra 3D encoder for alignment may not fully leverage the rich knowledge in CLIP for 3D understanding due to the significant volume discrepancy between 2D and 3D pre-training datasets. Mainstream 3D datasets like ShapeNet  and Objavverse  contain fewer than _1 million_ synthetic 3D objects, significantly less than the vast image datasets such as DFN5B  and LAION-5B , which include around _5 billion_ images used to train cutting-edge CLIP models. While direct fine-tuning of CLIP's encoders facilitates more straightforward knowledge transfer, it restricts inputs to depth maps. Yet, developing 3D representations from depth maps is currently less effective than from point clouds for two primary reasons: (1) The current widely used CLIP text prompt templates are tailored for matching with RGB images, not depth maps, and (2) the lack of a robust projection method for creating dense depth maps with smooth contours from point clouds.

In this paper, we present _OpenDlign_, the first 3D open-world framework that develops 3D representation by aligning with multi-view diffusion model-generated images, termed _depth-aligned images_. These images benefit from the stochastic nature of the diffusion model, offering greater texture diversity compared to CAD renderings while maintaining the original 3D data's geometric and semantic integrity (See Fig. 1). Remarkably, OpenDlign demonstrates competitive open-world capability by fine-tuning only 6 million parameters of the CLIP image encoder on ShapeNet , unleashing CLIP's vast potential in 3D learning (See Fig. 1(c)). The success of this fine-tuning stems from refining the depth map projection pipeline, developing depth-specific text prompts, and introducing a logit aggregation strategy to merge multi-view prediction results. Experimental results reveal that OpenDlign significantly outperforms previous state-of-the-art (SOTA) models on a variety

Figure 1: _Top_: Comparison of OpenDlign with traditional open-world 3D learning models. Depth-based (a) and point-based (b) methods employ additional depth or point encoders for pre-training to align with CAD-rendered images. Conversely, OpenDlign (c) fine-tunes only the image encoder, aligning with vividly colored and textured depth-aligned images for enhanced 3D representation. Both rendered and depth-aligned images are utilized solely during training. _Bottom_: Visual comparison between multi-view CAD-rendered and corresponding depth-aligned images in OpenDlign.

of 3D tasks, including zero-shot/few-shot classification, object detection, and cross-modal retrieval. In zero-shot classification, OpenDign achieves accuracy improvements of 8.0% on ModelNet40 and 16.4% on OmniObject3D, the largest real-world 3D shape dataset. Additionally, depth-aligned images markedly enhance the performance of SOTA models, consistently improving results across diverse benchmarks and highlighting their transformative impact on open-world 3D learning pipelines.

The main contributions of this paper are outlined as follows:

* We introduce _depth-aligned images_ as a robust alternative to CAD-rendered images for open-world 3D learning. These images, generated from point cloud-projected depth maps using a diffusion model, offer rich and realistic texture diversity across viewpoints.
* We propose a multimodal alignment framework that robustly aligns depth maps, depth-aligned images, and text through streamlined fine-tuning of the CLIP image encoder.
* We develop a contour-aware projection pipeline to produce dense and contour-preserving multi-view depth maps from point clouds.
* We present depth-specific text prompts and a logit aggregation strategy to boost OpenDign's zero-shot capabilities and mitigate catastrophic forgetting during alignment fine-tuning.

## 2 Related Work

### Open-World 3D Representation Learning

Vision-Language Models (VLMs) such as CLIP  have revolutionized 2D representation learning in open-world settings through contrastive learning with large-scale image-text pairs [26; 27; 28; 29]. Building on this, recent studies have adapted CLIP for 3D representation learning, achieving impressive performance in diverse 3D zero-shot tasks [18; 19].

PointCLIP , as a pioneering study, utilizes the CLIP image encoder for extracting 3D representations from depth maps of point clouds, achieving zero-shot recognition by aligning with text embeddings of semantic categories. To address CLIP's training bias towards RGB images, Zhu _et al._ introduced GPT-generated 3D-specific prompts and a denser depth map projection, while CLIP2Point  pre-trains a depth encoder for closer alignment with CLIP's encoders. These methods derive representations from depth maps with noisy contours, causing a loss of key shape features needed for precise recognition. Moreover, their reliance on either natural image text prompts or depth-specific prompts generated by GPT-3  for certain categories highlights a lack of versatility in handling diverse 3D contexts. Alternative methods [17; 18; 19; 20] avoid depth map projection by directly aligning point clouds, images, and text using specialized 3D encoders. By scaling up the dataset and encoder sizes, these methods show promise in diverse 3D tasks. However, these methods are limited by their reliance on CAD-rendered images, which have limited texture diversity across views, leading to less generalizable representations. Additionally, the smaller volume of 3D datasets compared to CLIP's training data hinders effective knowledge transfer to point cloud encoders.

In this paper, we substitute rendered images with depth-aligned images generated from a diffusion model to enhance texture diversity. We also fine-tune the CLIP image encoder for 3D representation learning instead of training a new 3D encoder from scratch, reducing the reliance on large 3D datasets.

### Continual Learning in CLIP Fine-Tuning

Continual Learning (CL) in CLIP aims to mitigate catastrophic forgetting , ensuring retention of zero-shot capabilities across varied data distributions while fine-tuning to new tasks. CL methods fall into three categories: adaptive-plasticity methods [32; 33], replay methods [34; 35], and architecture-based methods [36; 37]. Adaptive-plasticity methods limit the plasticity of the essential model parameters for past tasks during fine-tuning. For instance, the IMM-Mean  method achieves CL by simply averaging parameters of pre-trained and fine-tuned models for inference, although its efficacy might be limited for complex tasks . Replay methods leverage stored exemplars to enable CLIP to recall previously learned knowledge, while they encounter scalability challenges. Without relying on exemplars, architecture-based CL methods dynamically adjust the model's architecture to accommodate new information without losing existing knowledge . In this study, we align the depth map with the RGB image by freezing the pre-trained CLIP encoder weights and incorporating a trainable transformer-based branch for encoding depth maps, adhering to architecture-based principles. Inspired by IMM-Mean , we use pre-trained and fine-tuned model weights to compute classification logits from multi-view depth maps for inference.

## 3 Methodology

The OpenDlign framework, depicted in Fig. 2, starts with a contour-aware projection method that transforms point clouds into multi-view depth maps with preserved contours. These maps guide a diffusion model to produce depth-aligned images with varied colors and textures, maintaining consistent geometry with the depth maps. OpenDlign then aligns features from depth maps and depth-aligned images by fine-tuning a transformer block connected to the pre-trained image encoder. The goal is to align feature embeddings from depth maps and corresponding depth-aligned images using contrastive learning. At inference, 3D representations are composed of embeddings from multi-view depth maps, derived using both fine-tuned and pre-trained encoder states. These embeddings are matched with depth-specific text embeddings, which capture the semantic and visual properties of the depth maps, to generate multi-view logits. These logits are then aggregated to facilitate label prediction in a zero-shot setting. In few-shot scenarios, these embeddings can be further refined with a logistic regressor. Detailed training and model implementation are provided in Appendix A.2.

### Contour-Aware Depth Map Projection

The contour-aware projection method transforms the input point cloud into multi-view depth maps with clear contours. Inspired by the pipeline in , this method involves four main steps: Quantize, Densify, Smooth, and Squeeze.

In the **Quantize** step, for the \(i^{}\) view of point cloud \(P_{i}\), the 3D coordinates \((x,y,z) P_{i}\) are normalized to \(\) and mapped onto a discrete grid \(G^{H W B}\), where \(H\) and \(W\) correspond to the dimensions required by the CLIP image encoder, and \(B\) is a pre-defined depth dimension. Next, the **Density** step enhances \(G\) by updating each voxel to the maximum value within its \(7 7 7\) neighborhood, yielding a denser map \(G^{}\). Subsequently, the **Smooth** step applies bilateral filtering to each voxel \(v_{i}\) in \(G^{}\), adjusting its intensity \(I_{v_{i}}\) to \(I^{}_{v_{i}}\) using:

\[I^{}_{v_{i}}=}_{v_{j} S}G_{_{1}}(\|v_{i}-v_{j} \|)G_{_{2}}(|I_{v_{i}}-I_{v_{j}}|)I_{v_{j}}\] (1)

where \(W_{v}=_{v_{j} S}G_{_{1}}(\|v_{i}-v_{j}\|)G_{_{2}}(|I_{v_{i} }-I_{v_{j}}|)\) is the normalization factor that ensures voxel weights sum to 1.0. The Gaussian functions \(G_{_{1}}\) and \(G_{_{2}}\) adjust the influence of each neighboring voxel \(v_{j}\) within the \(5 5 5\) kernel from set \(S\) around \(v_{i}\), based on spatial and intensity differences, enhancing contour sharpness and reducing jagged edges in \(G^{}\). Finally, the **Squeeze** step applies the minimal pooling on the depth channel of the smoothed \(G^{}\), then triples the output to mimic RGB intensity, producing the final depth map \(D^{H W 3}\).

### Depth-Aligned Image Generation

A total of **524,700** depth-aligned images are generated from ShapeNet , a leading public 3D CAD dataset containing approximately 52,470 models, each annotated with semantic metadata. To generate these images, a point cloud of 10,000 points is sampled from each CAD model, aligning with prior experimental protocols [18; 17]. For each point cloud, 10 views of depth maps are projected using the proposed contour-aware projection method. Subsequently, the ControlNet v1.1  depth model produces depth-aligned images for each contour-aware depth map view, using the CAD model's metadata as text input and the inverse of the depth map \(\) as image generation control. This approach ensures that the generated images remain consistent with the depth maps both geometrically and semantically, while also adding texture diversity across different views. ControlNet utilizes \(\) instead of \(D\) for controlling image outputs because it is predominantly pre-trained on depth images where brighter regions indicate closer proximity. Please refer to Appendix A.2 for the positive and negative prompts used in ControlNet to achieve high-fidelity depth-aligned image generation.

### Multimodal Representation Alignment

OpenDlign aligns depth maps and depth-aligned images by fine-tuning a transformer block residually linked to the last block of the CLIP image encoder, using contrastive learning. With CLIP pre-trained to align images and text, OpenDlign implicitly aligns depth maps with the shared image-text space.

**Multimodal Feature Extraction.** Given a 3D point cloud input, let \(D=\{D_{i}\}_{i=1}^{N}\) represent the set of its \(N\) projected depth map views, and \(R=\{R_{i}\}_{i=1}^{N}\) the corresponding set of depth-aligned images. Each image \(R_{i}\) is encoded through \(L\) layers of a pre-trained CLIP image encoder, \(\{_{l}()\}_{l=1}^{L}\), to obtain feature representations \(I_{i}^{R}=_{1 L}(R_{i})\). Each depth map \(D_{i}\) is processed up to layer \(_{L-1}\), yielding preliminary features \(_{1 L-1}(D_{i})\). Subsequently, these depth features are passed through the frozen layer \(_{L}\) and its trainable counterpart \(_{L}^{t}\), where only the attention layers for spatial interaction in \(_{L}^{t}\) are trainable, as inspired by . This process produces the feature for the \(i_{th}\) depth map view

Figure 2: Overview of OpenDlign. In (a), OpenDlign converts point clouds into multi-view depth maps using a contour-aware projection, which then helps generate depth-aligned RGB images with diverse textures, geometrically and semantically aligned with the maps. A transformer block, residually connected to the CLIP image encoder, is fine-tuned to align depth maps with depth-aligned images for robust 3D representation. For zero-shot classification (b), OpenDlign aggregates multi-view logits from both pre-trained and fine-tuned encoders for label prediction. For few-shot classification (c), it employs a logistic regressor trained on multi-view features from the encoders.

\(I_{i}^{D}=_{1 L}(D_{i})+_{L}^{t}(_{1 L-1 }(D_{i}))\). The final feature vectors for multi-view depth maps \(D\) and depth-aligned images \(R\) are \(^{D}=_{i=1}^{N}\|I_{i}^{D}\|\) and \(^{R}=_{i=1}^{N}\|I_{i}^{R}\|\), respectively.

**Loss Functions.** The alignment of \(^{D}\) and \(^{R}\) is achieved by minimizing a composite loss function, comprising the contrastive loss \(_{}\) and the feature distance loss \(_{}\), defined as:

\[_{}=-_{i}^{D}_{j}^{R}/)}{_{k}( _{i}^{D}_{k}^{R}/)}-_{i}^{D}_{j}^{R}/)}{_{k}( _{k}^{D}_{j}^{R}/)}}_{_{}}+ \|_{i}^{D}-_{j}^{R}\|_{2}}_{ _{}}\] (2)

In each training batch, \((_{i}^{D},_{j}^{R})\) is a positive pair, while \((_{i}^{D},_{k}^{R})\) and \((_{k}^{D},_{j}^{R})\) are negative pairs where \(k i\), \(j\). \(\) is a learnable temperature parameter. The contrastive loss enables learning robust representations by maximizing similarity in positive pairs and minimizing it in negative pairs [42; 43].

### 3D Zero-Shot Transfer

The alignment between depth maps and depth-aligned images facilitates 3D zero-shot classification by aggregating multi-view classification logits. Each logit represents the similarity between single-view depth features and text features of category candidates, as illustrated in Fig. 2(b).

**Depth-Specific Text Generation.** Depth-specific prompt templates are developed by augmenting a base set of 80 text prompts, initially designed for ImageNet classification2, with depth-related keywords such as "depth map", "raytraced image", and "silhouette of [CLASS]". These keywords direct OpenDlign's attention to geometric details and contours rather than color or texture. The CLIP Interrogator  is a prompt engineering tool that combines CLIP and BLIP  to select optimal text prompts for specific images. To identify these keywords, this tool identifies the top 10 prompts that match depth maps from the ShapeNet dataset , chosen as targeted keywords. For zero-shot inference, we employ our depth-specific templates to generate 80 text descriptions for each label \(l\). These descriptions \(\{t_{i}\}_{i=1}^{80}\) are encoded by a texture encoder \(F()\), normalized, and then merged into a unified text feature \(F_{l}\) via average pooling, calculated as \(_{i=1}^{80}\|F(t_{i})\|\).

**Multi-View Logits Aggregation.** To calculate classification logits, we first gather visual features from multi-view depth maps \(\{V_{i}\}_{i=1}^{N}\), aiming to align with depth-specific text features of \(M\) candidate labels \(=\{F_{i}\}_{i=1}^{M}\). The feature extraction utilizes a dual-encoder strategy: the first half of the views \(\{V_{i}\}_{i=1}^{N/2}\) utilize a pre-trained CLIP image encoder, while the second half of the views \(\{V_{i}\}_{i=N/2+1}^{N}\) employs a fine-tuned encoder. The strategy ensures that OpenDlign maintains its capability to recognize previously identifiable depth maps after learning multimodal alignment via fine-tuning. As shown in Fig. 2(b), the logit for a single depth map view is the product of \(V_{i}\) and \(\), with the overall classification logit being the sum of logits across all views, calculated as \(_{i=1}^{N}V_{i}^{T}\).

## 4 Experiments

### Zero-Shot 3D Classification

We first evaluated OpenDlign under the zero-shot shape classification task on four benchmark datasets: ModelNet40 , ScanObjectNN , OmniObject3D , and Objaveres-LVIS . ModelNet40 offers synthetic 3D CAD models in 40 categories. ScanObjectNN provides real-scanned objects in 15 categories from OBJ_ONLY version. OmniObject3D, the largest, includes 5,911 real-scanned objects in 216 categories, well-suited for fine-grained, real-world classification evaluation. Objaveres-LVIS contains 1,156 categories for evaluating long-tail classification. Point cloud sizes are 10,000 points for ModelNet40 and Objaveres-LVIS, 2,048 for ScanObjectNN, and 4,096 for OmniObject3D. OpenDlign was compared against existing methods, including three depth-based methods: PointCLIP , PointCLIP V2 , and CLIP2Point , and three point-based methods: ULIP , OpenShape , and TAMM . To investigate if depth-aligned images consistently enhance the representational abilities of other 3D open-world methods, we retrained all OpenShape and TAMM variants using their original CAD-rendered images and some depth-aligned images. Note

[MISSING_PAGE_FAIL:7]

inference, the regressor aggregates logits from 10 views to predict the final label. We compared OpenDlign with ULIP , OpenShape , and TAMM , which extract features for training regressor from their point encoders pre-trained on ShapeNet. Table 2 shows OpenDlign outperforms all baselines across varied few-shot scenarios with 1 to 16 training samples per class. OpenDlign significantly outperforms the leading baseline on the OmniObject3D dataset, exceeding it by 8.8% and 11.8% in the 4-shot and 8-shot classification, respectively. See Appendix A.5 for more results.

### Zero-Shot 3D Object Detection

OpenDlign's capability in zero-shot 3D object detection was evaluated on the ScanNet V2 dataset , which contains richly annotated 3D indoor scenes in 18 object categories. Following the PointCLIP V2 methodology , we started with the pre-trained 3DETR-m  model to identify 3D regions of interest, delineate 3D bounding boxes, and extract points within each box. Finally, we applied OpenDlign to these points to generate our predictions. Table 3 illustrates OpenDlign's zero-shot detection prowess using mean Average Precision (mAP) at IoU thresholds of 0.25 and 0.5, achieving scores of 50.72% and 37.97%, respectively. It significantly outperforms PointCLIP V2 by more than 31.75% and 26.44%. Remarkably, OpenDlign can detect the 'Sofa' shape with an AP\({}_{50}\) of \(54.96\%\), whereas PointCLIP and V2 score below \(10\%\), demonstrating OpenDlign's superior ability to extract robust 3D representations from sparse and noisy point clouds in real-world indoor scenes.

### Cross-Modal Retrieval

3D shapes were retrieved by computing the cosine similarity between the embeddings of a query and those generated by OpenDlign, followed by a k-nearest neighbors (kNN) analysis to find the most similar shapes. Fig. 3 showcases OpenDlign's ability to match 3D shapes to image and text queries. Column (a) shows its precision in distinguishing sub-categories like grand versus upright pianos from image queries. Column (b) demonstrates successful shape retrieval using distinctive text descriptions like "Batmobile armored". Notably, averaging image and text query embeddings allows OpenDlign to find shapes that combine elements of both queries. For instance, merging a running horse image with the text "man" retrieves both a centaur and a running man, as shown in Fig. 3(c). Similarly, combining a house image with "tree" retrieves a treehouse. See Appendix A.6 for more results.

Figure 3: 3D shape retrieval results. (a) Two most similar shapes for each image query. (b) Most similar shapes for each text query. (c) Two most similar shapes for combined image and text queries.

    & Method & Mean & Cabinet & Bed & Chair & Sofa & Table & Dover & Window & Counter & Desk & Sink & Bathtub \\   _{25}\)} & PointCLIP  & 6.00 & 3.99 & 4.82 & 45.16 & 4.82 & 7.36 & 4.62 & 2.19 & 1.02 & 4.00 & 13.40 & 6.46 \\  & PointCLIP V2  & 18.97 & 19.32 & 09.08 & 61.89 & 15.55 & 27.38 & 13.22 & 17.42 & 12.43 & 14.54 & 16.77 \\  & **OpenDlign (ours)** & **50.72** & **38.91** & **67.27** & **86.33** & **72.01** & **58.72** & **44.58** & **32.07** & **50.49** & **62.04** & **51.98** & **64.29** \\  _{50}\)} & PointCLIP  & 4.76 & 1.67 & 4.33 & 39.53 & 3.65 & 5.97 & 2.61 & 0.52 & 0.42 & 2.45 & 5.27 & 1.31 \\  & PointCLIP V2  & 11.53 & 10.43 & 13.54 & 41.23 & 6.60 & 15.21 & 6.23 & 11.35 & 6.23 & 10.84 & 11.43 & 10.14 \\   & **OpenDlign (ours)** & **37.97** & **17.04** & **66.68** & **73.92** & **54.96** & **50.03** & **24.73** & **12.84** & **20.44** & **41.64** & **34.17** & **64.29** \\   

Table 3: Zero-shot 3D object detection results on ScanNet V2 .

### Ablation Study

Ablation studies were conducted on zero-shot classification benchmarks to assess the contribution of each component in OpenDlign. Consistently, all OpenDlign variants used in these studies employed OpenCLIP-ViT-H-14 as their backbone. ShapeNet was the default training dataset for all models.

**Contour-Aware Projection.** Replacing PointCLIP V2's projection pipeline  with our contour-aware version enables a pre-trained CLIP to achieve 68.8% zero-shot accuracy on ModelNet40, outperforming several baselines that require extra training (See Table 4). This indicates that CLIP can understand RGB images and depth maps when shape features are highlighted.

**Effect of Alignment with Depth-Aligned Images.** Table 4 shows that aligning depth maps with depth-aligned images (i.e., depth-dlign) significantly boosts performance, improving Top1 accuracy by around 10% on ScanObjectNN, with or without depth-specific prompts. This indicates that depth-d alignment effectively transfers CLIP's rich knowledge to interpret depth maps.

Further analysis compared depth-dlign alignment against three alternatives: depth-rendCAD (aligning depth maps with CAD-rendered RGB images), dign-text & depth (aligning depth-aligned images with text before depth-dlign alignment), and depth-text & dlign (simultaneous alignment of depth maps with text and depth-aligned images). Table 5 shows depth-dlign outperforming depth-rendCAD by 6.8% on the ScanObjectNN dataset, confirming concerns that alignment with rendered images may lead to overfitting on specific 3D shapes. Moreover, dign-text & depth performs worst, suggesting that pre-aligning depth-aligned images with text compromises CLIP's ability to generate robust image representations, thus affecting subsequent depth-dlign alignment efficacy. The superior performance of depth-dlign on ModelNet40 and OmniObject3D compared to depth-text & dlign shows that aligning depth maps with depth-aligned images indirectly aligns with text, making additional text alignment unnecessary and potentially limiting OpenDlign's generalization.

**Depth-Specific Texts.** Table 4 shows that depth-specific prompts enhance OpenDlign's performance, regardless of using multimodal alignment or logit aggregation. This indicates that some recognition inaccuracies arise from processing input data as typical RGB images instead of depth maps.

**Logits Aggregation.** Results in Table 4 show that multi-view logit aggregation improves zero-shot classification on all datasets by combining logits from pre-trained and fine-tuned encoders. This approach effectively mitigates the catastrophic forgetting problem in OpenDlign's multimodal alignment, enabling it to recognize 3D objects identifiable by both pre-trained CLIP and OpenDlign.

   Contour-Aware \\ Projection \\  } &  Multimodal \\ Alignment \\  } &  Depth-Specific \\ Texts \\  } &  Logits \\ Aggregation \\  } &  &  \\   & & & & Top1 & Top3 & Top5 & Top1 & Top3 & Top5 \\   ✗ & ✗ & ✗ & ✗ & 59.7 & 79.6 & 86.3 & 42.8 & 66.7 & 78.4 \\ ✓ & ✗ & ✗ & ✗ & 68.8 (+9.1) & 85.8 (+6.2) & 91.6 (+5.3) & 44.6 (+1.8) & 68.3 (+1.6) & 78.9 (+0.5) \\ ✓ & ✓ & ✗ & ✗ & 79.2 (+19.5) & 94.4 (+14.8) & 97.6 (+11.3) & 56.9 (+14.1) & 75.5 (+8.8) & 83.8 (+5.4) \\ ✓ & ✗ & ✓ & ✗ & 75.9 (+16.2) & 91.0 (+11.4) & 95.4 (+9.1) & 49.3 (+6.5) & 69.8 (+3.1) & 79.2 (+0.8) \\ ✓ & ✓ & ✓ & ✗ & 80.2 (+20.5) & 95.3 (+15.7) & 97.7 (+11.4) & 88.1 (+5.3) & 75.2 (+8.5) & **84.2** (+5.8) \\ ✓ & ✓ & ✗ & ✓ & 81.0 (+21.3) & 95.2 (+15.6) & 97.6 (+11.3) & 56.8 (+14.0) & 74.6 (+7.9) & 81.6 (+3.2) \\ ✓ & ✓ & ✓ & ✓ & **82.6** (+22.9) & **96.2** (+16.6) & **98.4** (+12.1) & **59.5** (+16.7) & **76.8** (+10.1) & 83.7 (+5.3) \\  

Table 4: Ablation study for OpenDlign on ModelNet40  and ScanObjectNN . Accuracy improvements over the baseline (first-row) are highlighted in \(\).

Figure 4: Effect of the number of views on OpenDlign’s zero-shot performance.

   Alignment \\ Strategy \\  } &  MNIST0 \\ Top1 \\  } &  ScanNN  \\ Top1 \\  } &  Omni3D  \\ Top1 \\  } \\   & & & Top5 & Top1 & Top5 & Top1 & Top5 \\   depth-rendCAD & 78.8 & 96.8 & 52.7 & 82.5 & 29.4 & 51.8 \\ dign-text \& depth & 78.6 & 96.4 & 51.1 & 79.6 & 29.1 & 51.6 \\ depth-text \& dlign & 79.4 & 98.0 & **60.7** & **86.0** & 29.5 & 52.7 \\
**depth-dlign (ours)** & **82.6** & **98.4** & 59.5 & 83.7 & **31.3** & **53.2** \\  

Table 5: Ablation study on various alignment strategies. Aligning with text modality was achieved by fine-tuning the image encoder.

**Varying Number of Views.** OpenDlign, like other depth-based methods, requires extracting multiple embeddings from multi-view depth maps for zero-shot inference. Fig. 7 shows that OpenDlign's zero-shot accuracy on ModelNet40 and OmniObject3D improves with more depth map views. Notably, OpenDlign achieves top performance, comparable to TAMM-PointBERT, with just two views, balancing latency and effective zero-shot classification.

## 5 Conclusion

In this study, we introduce OpenDlign, an open-world framework that learns robust 3D representations from multi-view depth maps by efficiently fine-tuning with depth-aligned images, which are more visually diverse than CAD-rendered images. The effectiveness of OpenDlign is validated on various 3D zero-shot and few-shot tasks. We also show that depth-aligned images consistently enhance the performance of existing 3D open-world methods. Future work will explore the application of depth-aligned images in designing open-world models for 3D scenes (See Appendix A.1).

**Limitations.** Due to limited computational resources, we cannot generate depth-aligned images from the largest 3D dataset , containing around 1 million 3D objects. Retraining 3D open-world models with billions of parameters using these images is also too expensive. Moreover, a data filtering strategy is needed to remove low-quality depth-aligned images (See details in Appendix A.6).