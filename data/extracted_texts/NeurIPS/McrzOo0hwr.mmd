# Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning

Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning

Yiqin Lv Qi Wang Dong Liang Zheng Xie

College of Science, National University of Defense Technology

Changsha, China

Email to: {lvyiqin98,wangqi15,dongliangnudt,xiezheng81}@nudt.edu.cn

Correspondence Authors.

###### Abstract

Meta learning is a promising paradigm in the era of large models and task distributional robustness has become an indispensable consideration in real-world scenarios. Recent advances have examined the effectiveness of tail task risk minimization in fast adaptation robustness improvement . This work contributes to more theoretical investigations and practical enhancements in the field. Specifically, we reduce the distributionally robust strategy to a max-min optimization problem, constitute the Stackelberg equilibrium as the solution concept, and estimate the convergence rate. In the presence of tail risk, we further derive the generalization bound, establish connections with estimated quantiles, and practically improve the studied strategy. Accordingly, extensive evaluations demonstrate the significance of our proposal and its scalability to multimodal large models in boosting robustness.

## 1 Introduction

The past few years have witnessed a surge of research interest in meta learning due to its great potential in the academia and industry . By leveraging previous experience, such a learning paradigm can extract knowledge as priors and empower learning models with adaptability to unseen tasks from a few examples .

Nevertheless, the investigation of the robustness needs to be more comprehensive from the task distribution perspective. In particular, the recently developed large models heavily rely on the few-shot learning capability and demand robustness of prediction in risk-sensitive scenarios . For example, when the GPT-like dialogue generation system  comes into medical consultancy domains, imprecise answers can cause catastrophic consequences to patients, families, and even societies in real-world scenarios. In light of these considerations, it is desirable to watch adaptation differences across tasks when deploying meta learning models and promote task robustness study for meeting substantial practical demands.

Recently, Wang et al.  proposes to increase task distributional robustness via employing the tail risk minimization principle  for meta learning. In circumventing the optimization intractability in the presence of nonconvex risk functions, a two-stage optimization strategy is adopted as the heuristic to solve the problem. In brief, the strategy consists of two phases in iteration, respectively: (i) estimating the risk quantile \(_{}\) with the crude Monte Carlo method  in the task space; (ii) updating the meta learning model parameters from the screened subset of tasks. Such a strategy is simple in implementation, with an improvement guarantee under certain conditions, and empirically shows improved robustness when faced with task distributional shifts. Despite these advances, there remain several unresolved theoretical or practical issues in the field.

**Existing limitations.** This paper also works on the robustness of fast adaptation in the task space and tries to fill gaps in . Theoretically, we notice that in  (i) there constitutes no notion of solutions, (ii) it lacks an algorithmic understanding of the two-stage optimization strategy, (iii) the analysis on generalization capability is ignored in the tail risk of tasks. Empirically, the use of the crude Monte Carlo might be less efficient in quantile estimates and suffers from a higher approximation error of the VaR\({}_{}\), degrading the adaptation robustness. These bottlenecks may weaken the versatility of the two-stage optimization strategy's use in practice and require more understanding before deployment.

**Primary contributions.** In response to the above-mentioned concerns, we propose translating the two-stage optimization strategy for distributionally robust meta learning  into a max-min optimization problem . Intrinsically, this work models the optimization steps as a Stackelberg game, and task selection and the sub-gradient optimizer work as the leader and follower players in decision-making, respectively. The theoretical understanding is from two aspects:

1. We constitute the local Stackelberg equilibrium as a solution concept, estimate the convergence rate, and characterize the asymptotic behavior in learning dynamics.
2. We derive the generalization bound in the presence of the tail task risk, which connects quantile estimates with fast adaptation capability in unseen tasks.

Meanwhile, the empirical influence of VaR\({}_{}\) estimators is examined, and we advance meta learners' robustness by comprising more accurate quantile estimators.

## 2 Literature Review

### Meta Learning

Meta learning, or _learning to learn_, is an increasingly popular paradigm to distill knowledge from prior experience to unseen scenarios with a few examples . Various meta learning methods have emerged in the past decade, and this section overviews some dominant families.

The context-based methods mainly use the encoder-decoder structure and represent tasks by latent variables. Typical ones are in the form of the conditional exchangeable stochastic processes and learn function distributions, such as neural processes , conditional neural processes  and their extensions [16; 17; 18; 19; 20; 21; 22; 23; 24]. The optimization-based approaches seek the optimal meta initialization of model parameters and update models from a few examples. Widely known are model agnostic meta learning  and related variants [26; 27; 28; 29], such as MetaCurvature , which learns curvature information and transforms gradients in the inner-loop optimization. The metrics-based methods represent tasks in geometry and perform well in few-shot image classification [31; 32; 33]. For example, MetaOptNet  proposes to learn embeddings under a linear classifier and achieve SOTA few-shot classification performance. There also exist other methods, e.g., hyper-networks [35; 36], memory-augmented networks  and recurrent models .

### Robustness & Generalization

The robustness concept in meta learning attracts recent attention, particularly when deploying large models in real-world scenarios. Admittedly, previous literature works have investigated the scenarios where the meta dataset's input is corrupted [39; 40] or the model parameter is perturbed . Studies regarding the fast adaptation robustness in task distribution remain limited. Wang et al.  explicitly generates task distribution for robust adaptation. Collins et al.  employs the worst-case optimization for promoting MAML's robustness to extreme worst cases. With the help of tail risk minimization, Wang et al.  proposes two-stage optimization strategies to robustify the fast adaptation. This work centers around  but stresses more theoretical understandings and performance improvement points.

As for generalization capability, there are a couple of works in meta learning. Chen et al.  exploits the information theory to derive the bound for MAML's like methods. From the data splitting perspective, Bai et al.  formulates the theoretical foundation and connects it to optimality. In , an average risk bound is constructed with the bias for improving performance. Importantly, prior work  ignores the generalization analysis, and meta learner's generalization in tail risk cases has not been studied in the literature.

Preliminaries

**General notations.** Let \(p()\) be the task distribution in meta learning. We respectively express the task space and the model parameter space as \(_{}\) and \(\). We denote the complete task set by \(\) and refer to \(_{}\) as the meta dataset.

For instance, \(_{}\) comprises a collection of data points \(\{(x_{i},y_{i})\}_{i=1}^{n+m}\) in regression. \(_{}\) is usually prepared into the support set \(_{}^{S}\) for skill transfer and the query set \(_{}^{Q}\) to assess adaptation performance. Take the conditional neural process  as an example, \(_{}^{S}=\{(x_{i},y_{i})\}_{i=1}^{n}\) works for task representation with \(_{}^{Q}=\{(x_{i},y_{i})\}_{i=1}^{n+m}\) the all data points to fit in regression.

The meta risk function corresponds to a map \(:_{}^{+}\), evaluating fast adaptation performance. Given \(p()\) and meta learning model parameters \(\), we can induce the cumulative distribution of the meta risk function value in the real space as \(F_{}(l;):=(\{(_{}^{Q},_{ }^{S};) l;,l^{+}\})\), but there is no explicit parameterized form for \(F_{}\) in practice as \(F_{}\) is \(\)-dependent.

When it comes to the tail risk minimization, we commonly use the conditional value-at-risk (CVaR\({}_{}\)) with the probability threshold \([0,1)\). The quantile of our interest is called the value-at-risk (VaR\({}_{}\))  with the definition: \(_{}[(,)]=_{l^ {+}}\{l|F_{}(l;),\}\). The resulting normalized cumulative distribution \(F_{}^{}(l;)\) is defined as:

\[F_{}^{}(l;)=0,&l<_{}[( ,)]\\ (l;)-}{1-},&l_{}[( ,)].\]

\(\), the meta learning operator \(_{}\) defines: \(_{}:(_{}^{Q},_{ }^{S};)\). Accordingly, the tail risk task subspace \(_{,}:=_{_{}[(, )]}[_{}^{-1}()]\), with the task distribution constrained in \(_{,}\) by \(p_{}(;)\). Please refer to Fig. 7 for illustrations of risk concepts.

**Assumption 1**.: _To proceed, we retain most assumptions from  for theoretical analysis, including:_

1. _The meta risk function_ \((_{}^{Q},_{}^{S};)\) _is_ \(_{}\)_-Lipschitz continuous w.r.t._ \(\)_;_
2. _The cumulative distribution_ \(F_{}(l;)\) _is_ \(_{}\)_-Lipschitz continuous w.r.t._ \(l\)_, and the normalized density function_ \(p_{}(;)\) _is_ \(_{}\)_-Lipschitz continuous w.r.t._ \(\)_;_
3. _For arbitrary valid_ \(\) _and corresponding_ \(p_{}(;)\)_,_ \((_{}^{Q},_{}^{S};)\) _is bounded:_ \(_{_{,}}(_{}^{Q},_{ }^{S};)_{}\)_._

### Risk Minimization Principles

This subsection revisits commonly used risk minimization principles in the meta learning field.

**Expected risk minimization.** The standard principle is the expected/empirical risk minimization originated from statistical learning theory . It minimizes meta risk based on the sampling chance of tasks from the original task distribution:

\[_{}():=_{p()}( _{}^{Q},_{}^{S};).\] (1)

**Worst-case risk minimization.** Noticing that the worst fast adaptation can be disastrous in some risk sensitive scenarios, Collins et al.  proposes to conduct the worst-case optimization in meta learning:

\[_{}_{}_{}( ):=(_{}^{Q},_{}^{S};).\] (2)

However, as observed from experiments in , such a principle inevitably sacrifices too much average performance for gains of worst-case robustness. Meanwhile, it requires a couple of implementation tricks and specialized algorithms in stabilizing optimization.

**Expected tail risk minimization (CVaR\({}_{}\)).** To balance the average performance and the worst-case performance, Wang et al.  minimizes the expected tail risk, or equivalently CVaR\({}_{}\) risk measure:

\[_{}_{}():=_{p_{}( ;)}(_{}^{Q},_{}^{S}; ).\] (3)Due to no closed form of \(p_{}(;)\), Wang et al.  introduces a slack variable \(\) and reformulates the objective as follows:

\[_{,}_{}(,):=_{}^{1}v_{}d=+_{p ()}[[(_{}^{Q},_{}^{S};) -]^{+}],\] (4)

where \(v_{}:=F_{}^{-1}()\) denotes the quantile statistics and \([(_{}^{Q},_{}^{S};)-]^ {+}:=\{(_{}^{Q},_{}^{S};)-,0\}\) is the hinge risk.

The optimization objective involves the integral of quantiles in a continuous interval \((,1]\), which is intractable to precisely parameterize with neural networks. The form in Eq. (4) utilizes the duality trick , enabling tractable sampling from the complete task space.

### Examples & Two-stage Heuristic Strategies

Before delving deeper into the theoretical issues, we first present DR-MAML  as an instantiation to explain the expected tail risk minimization.

**Example 1** (DR-MAML ).: _Given \(p()\) and vanilla MAML , the distributionally robust MAML within \(_{}\) can be written as a bi-level optimization problem:_

\[_{\\ }+_{p()} [[(_{}^{Q};-_{}( _{}^{S};))-]^{+}],\] (5)

_where the gradient update w.r.t. the support set \(_{}(_{}^{S};)\) indicates the inner loop with a learning rate \(\). The outer loop executes the gradient updates w.r.t. Eq. (5) and seeks the robust meta initialization in the parameter space._

**Two-stage optimization strategies.** Without loss of generality, we further detail the computational pipelines of Example 1 with two-stage optimization strategies. Note that MAML  is an optimization-based meta learning method, and the implementation is to execute the sub-gradient descent over a batch of tasks when updating the meta initialization \(^{}\):

\[_{t}^{_{i}}=_{t}^{}-_{1} _{}(_{_{i}}^{S};),\;i=1,,\] (6a) \[=_{}^{}(),\] (6b) \[(_{i})=1[(_{_{i}}^{Q};_{t}^{ _{i}})],\;i=1,,\] (6c) \[_{t+1}^{}_{t}^{}- _{2}_{i=1}^{}_{}[(_{i}) (_{_{i}}^{Q};_{t}^{_{i}})].\] (6d)

Here, \(_{1}\) and \(_{2}\) are the inner loop and the outer loop learning rates, and the subscript \(t\) records the iteration number, with \((_{i})\) the indicator variable. \(_{}\) is the empirical distribution with \(\) Monte Carlo task samples. \((_{i})=1\) indicates the meta risk \((_{_{i}}^{Q};_{t}^{_{i}})\) after fast adaptation falls into the defined tail risk region, otherwise \((_{i})=0\).

Throughout optimizing DR-MAML, **Stage-I** includes the fast adaptation _w.r.t._ individual task in Eq. (6a), and the quantile estimate in Eq. (6b). **Stage-II** applies the sub-gradient updates to the model parameters in Eq. (6c)/(6d). These two stages repeat until convergence is achieved.

## 4 Theoretical Investigations

This section presents theoretical insights into two-stage optimization strategies. We perform analysis from the algorithmic convergence, the asymptotic tail risk robustness, and the cross-task generalization capability in meta learning.

Figure 1: **Illustration of optimization stages in distributionally robust meta learning from a Stackelberg game. Given the DR-MAML example, the pipeline can be interpreted as bi-level optimization: the leader’s move for characterizing tail task risk and the follower’s move for robust fast adaptation.**

### Distributionally Robust Meta Learning as a Stackelberg Game

Implementing the two-stage optimization strategy in meta learning requires first specifying the stages' order. The default is the minimization of the risk measure _w.r.t._ the parameter space after the maximization of the risk measure _w.r.t._ the task subspace. Hence, we propose to connect it to max-min optimization  and the Stackelberg game .

**Max-min optimization.** With the pre-assigned decision-making orders, the studied problem can be characterized as:

\[_{q()_{}}_{}(q, ):=_{q()}(_{}^{Q}, _{}^{S};),\] (7)

where \(_{}:=\{q()|_{q},_{ _{q}}p()d=1-\}\) constitutes a collection of uncertainty sets  over task subspace \(_{q}\), and \(q()\) is the normalized probability density over the task subspace. Note that in the expected tail risk minimization principle, there is no closed form of optimization objective Eq. (4) as the tail risk is \(\)-dependent. It is approximately interpreted as the max-min optimization when applied to the distribution over the uncertainty set \(_{}\).

**Proposition 1**.: _The uncertainty set \(_{}\) is convex and compact in terms of probability measures._

Practical optimization is achieved via mini-batch gradient estimates and sub-gradient updates with the task size \(\) in ; the feasible subsets correspond to all combinations of size \(*(1-)\). Also, Eq. (7) is non-differentiable _w.r.t._\(q()\), leaving previous approaches  unavailable in practice.

**Stackelberg game & best responses.** The example computational pipelines in Eq. (6) can be understood as approximately solving a stochastic two-player zero-sum Stackelberg game. Mathematically, such a game referred to as \(\) can be depicted as \(:=_{L},_{F};\{q_{ }\},\{\};(q,)\).

Moreover, we translate the two-stage optimization as decisions made by two competitors, which are illustrated in Fig. 1. The maximization operator executes in the task space, corresponding to the leader \(_{L}\) in \(\) with the utility function \((q,)\). The follower \(_{F}\) attempts to execute sub-gradient updates over the meta learners' parameters via maximizing \(-(q,)\).

The two players compete to maximize separate utility functions in \(\), which can be characterized as:

\[:\;q_{t}=_{}}_{q}(_{}^{Q},_{}^{S};_{t}) }_{},_{t+1}=_{q_{t}}(_{}^{Q}, _{}^{S};)}_{},\] (8)

where the leader player \(_{L}\) specifies the worst case combinations from the uncertainty set \(_{}\), and the follower \(_{F}\) reacts to the resulting normalized tail risk for increasing fast adaptation robustness.

It is worth noting that the update rules in Eq. (8) are also called _best responses_ of players in game theory. The above procedures can be deemed the bi-level optimization  since the update of the meta learner implicitly depends on the leader's last time decision.

Figure 2: **The sketch of theoretical and empirical contributions in two-stage robust strategies.** On the left side is the two-stage distributionally robust strategy . The contributed theoretical understanding is right-down, with the right-up the empirical improvement. Arrows show connections between components.

### Solution Concept & Properties

The improvement guarantee has been demonstrated when employing two-stage optimization strategies for minimizing the tail risk in . Furthermore, we claim that under certain conditions, there converges to a solution for the proposed Stackelberg game \(\). The sufficient evidence is:

1. The two-stage optimization  results in a monotonic sequence: \[}:\{q_{t-1},_{t}\} \{q_{t},_{t+1}\}\] (9a) \[}:(q_{t-1}, _{t})(q_{t},_{t+1});\] (9b)
2. As \(_{}\), the objective \(_{q}(_{}^{Q},_{}^{S}; )_{}\) naturally holds \( q_{}\) and \(\).

Built on the boundness of risk functions and the theorem of improvement guarantee, such an optimization process can finally converge . Then, a crucial question arises concerning the obtained solution: _What is the notion of the convergence point in the game?_

To answer this question, we need to formulate the corresponding solution concept in \(\). Here, the global Stackelberg equilibrium is introduced as follows.

**Definition 1** (Global Stackelberg Equilibrium).: Let \((q_{*},_{*})_{}\) be the solution. With the leader \(q_{*}_{}\) and the follower \(_{*}\), \((q_{*},_{*})\) is called a _global Stackelberg equilibrium_ if the following inequalities are satisfied, \( q_{}\) and \(\),

\[_{^{}}(q,^{})( q_{*},_{*})(q_{*},).\]

**Proposition 2** (Existence of Equilibrium).: _Given the Assumption 1, there always exists the global Stackelberg equilibrium as the Definition 1 for the studied \(\)._

Nevertheless, the existence of the global Stackelberg equilibrium can be guaranteed; it is NP-hard to obtain the equilibrium with existing optimization techniques. The same as that in , we turn to the local Stackelberg equilibrium as the Definition 2, where the notion of the local Stackelberg game is restricted in a neighborhood \(_{}^{}^{}\) in strategies.

**Definition 2** (Local Stackelberg Equilibrium).: Let \((q_{*},_{*})_{}\) be the solution. With the leader \(q_{*}_{}\) and the follower \(_{*}\), \((q_{*},_{*})\) is called a _local Stackelberg equilibrium_ for the leader if the following inequalities hold, \( q_{}^{}\),

\[_{_{^{}}(q_{*})}(q_{*},) _{_{^{}}(q)}(q,), _{^{}}(q):=\{^{}| (q,)(q,),^{ }\}.\]

The nature of nonconvex programming comprises the above local optimum, and we introduce concepts below for further analysis. It can be validated that \((q,)\) is a quasi-concave function _w.r.t._\(q\), meaning that for any positive number \(l_{+}\), the set \(\{q|q_{},(q,)>l\}\) is convex in \(_{}\). As a result, we deduce that there exists an implicit function \(h():_{}\) such that the condition holds \(h()=q\) with \(q=_{q_{}}(,)\). For the implicit function \(h\), along with \(_{}(q,)\), we make the Assumption below.

**Assumption 2**.: _The implicit function \(h()\) is \(_{h}\)-Lipschitz continuous w.r.t. \(\), and \(_{}(q,)\) is \(_{q}\)-Lipschitz continuous w.r.t. \(q_{}\)._

### Convergence Rate & Generalization Bound

_Learning to learn_ scales with the number of tasks, but the optimization process is computationally expensive [56; 57; 58; 59; 60], particularly when large language models are meta learners [3; 61; 62]. In training distributionally robust meta learners, estimating the convergence rate allows monitoring of the convergence and designing early stopping criteria to reach a desirable performance, reducing computational burdens . Consequently, we turn to another question regarding the solution concept: _What is the convergence rate of the two-stage optimization algorithm?_

The runtime complexity for the leader's move can be easily estimated from subset selection, while the analysis for the follower is non-trivial. Under certain conditions, we can derive the following convergence rate theorem, where \(\) is the learning rate in gradient descent _w.r.t. \(\).

**Theorem 4.1** (Convergence Rate for the Second Player).: _Let the iteration sequence in optimization be: \(\{q_{t-1},_{t}\}\{q_{t},_{t+1}\} \{q_{*},_{*}\}\), with the converged equilibrium \((q_{*},_{*})\). Under the Assumption 2 and suppose that \(||I-^{2}_{}(q_{*},_{*})||_{2}<1- _{q}_{h}\), we can have \(_{t}-_{t}||_{2}}{||_{t}-_ {t}||_{2}} 1\), and the iteration converges with the rate \(||I-^{2}_{}(q_{*},_{*})||_{2} +_{q}_{h}\) when \(t\) approaches infinity._

Moreover, after executing the two-stage algorithm \(T\) time steps and given learned \(^{}_{T}\), we can establish a bound on the asymptotic performance gap _w.r.t._\(}}_{}\) in Theorem 4.2. For expositional clarity, we simplify \((^{Q}_{},^{S}_{};_{*})\), \((^{Q}_{},^{S}_{};^{}_{T})\), \(_{}[(,_{*})]\), and \(_{}[(,_{}^{})]\) as \(^{*}\), \(^{}\), \(^{*}_{}\), and \(^{*}_{}\), respectively.

**Theorem 4.2** (Asymptotic Performance Gap in Tail Task Risk).: _Under the Assumption 1 and given a batch of tasks \(\{_{i}\}_{i=1}^{}\), we can have_

\[}}_{}(^{}_{T})-}}_{}(_{*})_{}\|^{}_{T}-_{*}\|+^{*}_{}}{1-}( _{1})-(_{2}),\] (10)

_where \(_{1}=\{:^{*}<^{*}_{},^{} ^{}_{}\},_{2}=\{:^{*} ^{*}_{},^{}<^{ }_{}\}\)._

For sufficiently large \(T\), the first term can be bounded by a small number due to the convergence, and the second term vanishes since \(_{T}^{}=^{*}\) and \(_{T}^{}_{}=^{*}_{}\), respectively.

Another crucial issue regarding meta learning lies in the fast adaptation capability in unseen cases. This drives us to answer the following question: _How does the resulting meta learner generalize in the presence of tail task risk?_

To this end, we first define \(R(_{*})=_{p_{}()}[^{*}]\), \((_{*})=_{i=1}^{}(_{i}) (^{Q}_{_{i}},^{S}_{_{i}};_{*}),\) and \(_{w}(_{*})=_{i=1}^{} (_{i})}{p(_{i})}(^{Q}_{_{i}},^{S}_{ _{i}};_{*})\), where \(_{i} p()\). Also note that the support of \(p_{}(;_{*})\) is within that of \(p()\), namely \((p_{}(;_{*}))(p())\). Then we can induce Theorem 4.3_w.r.t._ the tail risk generalization.

**Theorem 4.3** (Generalization Bound in the Tail Risk Cases).: _Given a collection of task samples \(\{_{i}\}_{i=1}^{}\) and corresponding meta datasets, we can derive the following generalization bound in the presence of tail risk:_

\[ R(_{*})(_{*})+^{2}_{}+_{ _{i} p_{}()}(^{Q}_{_{i}},^ {S}_{_{i}};_{*})} {}}\\ +_{}}{} (2+3),\] (11)

_where the inequality holds with probability at least \(1-\) and \((0,1)\), \([]\) denotes the variance operation, and \(_{}\) is from the Assumption 1._

In conjunction with the confidence \(\) and a task batch \(\) of significant size, Theorem 4.3 reveals the generalization bound given the meta-trained parameter \(_{*}\). It is also associated with the variance \(_{_{i} p_{}()}[(^{Q}_{_{i}}, ^{S}_{_{i}};_{*})]\). Besides, we also derive a specific bound in the case of MAML, and details are attached in Appendix Theorem C.1.

### Practical Enhancements & Implementations

Theorem 4.3 reveals that an accurate estimate of \(_{}\) yields a precise variance (i.e., \(_{_{i} p_{}()}[(^{Q}_{_{i}}, ^{S}_{_{i}};_{*})]\)), leading to more reliable bounds. Accordingly, this section offers improvements over  via utilizing kernel density estimators (KDE)  for \(_{}\)'s estimates. Compared to crude Monte Carlo (MC) methods, KDE can handle arbitrary complex distributions, capture local statistics well, and smoothen the cumulative function in a non-parametric way.

Specifically, we can construct KDE with a batch of task risk values \(\{(^{Q}_{_{i}},^{S}_{_{i}};)\}_{i=1}^{ }\):

\[F_{}(l;)=_{-}^{l}h_{}}_{ i=1}^{}K^{Q}_{_{i}},^{S}_{ _{i}};)}{h_{}}dt,\] (12)where \(K:^{d}\) is a kernel function, e.g., the Gaussian kernel, \(K(x)=/2)}{(-||x||^{2}/2)d}\), and \(h_{}\) is the smoothing bandwidth. Once the KDE is built, it enables access to the quantile from the cumulative distribution functions or numeric integrals. The following Theorem 4.4 shows that KDE serves as a reliable approximation for \(_{}\).

**Theorem 4.4**.: _Let \(F_{}^{-1}(;)=_{}^{}[( ,)]\) and \(F_{}^{-1}(;)=_{}[(,)]\). Suppose that \(K(x)\) is lower bounded by a constant, \( x\). For any \(>0\), with probability at least \(1-\), we can have the following bound:_

\[_{}\,(F_{}^{-1}(;)-F_{ }^{-1}(;))(}{*}}).\] (13)

As implied, one can close the distribution approximation gap by adopting a smaller, more flexible bandwidth. Additionally, KDE models offer a smooth estimate of the cumulative distribution function and require no prior assumptions.

**Remark 1**.: In addition to smoothness, flexibility, and distribution agnostic traits, KDE in adoption can enhance the studied method's generalization capability. The crude Monte Carlo used in  typically incurs an error of approximately \((}})\) in estimating quantiles . In contrast, that of KDE is no more than \((}{*}})\) from Theorem 4.4.

## 5 Empirical Findings

Prior sections mainly focus on the theoretical understanding of two-stage distributionally robust strategies. This section conducts extensive experiments on a broader range of benchmarks and examines the improvement tricks, e.g., the use of KDE for quantile estimates, from empirical results.

**Benchmarks & baselines.** We perform experiments on the few-shot regression, system identification, image classification, and meta reinforcement learning, where most of them keep setups the same as prior work [1; 42]. We evaluate the methods from risk minimization principles and corresponding indicators, including expected/empirical risk minimization (Average), worst-case risk minimization (Worst), and tail risk minimization (CVaR\({}_{}\)).

MAML mainly works as the base meta learner, and we term the KDE-augmented DR-MAML as DR-MAML+. Then we compare DR-MAML+ with several baselines, including vanilla MAML , TR-MAML , DRO-MAML  and DR-MAML .

### Sinusoid Regression

The goal of the sinusoid regression  is to quickly fit an underlying function \(f(x)=A(x-B)\) from \(K\) randomly sampled data points, and tasks are specified by \((A,B)\). The meta-training and testing setups are the same as that in [1; 42], where many easy functions with a tiny fraction of difficult ones are included in the training.

**Result & analysis.** As illustrated in Fig. 4, we can observe that DR-MAML+ consistently outperforms all baselines across average and CVaR\({}_{}\) indicators in the 5-shot case. Though the average performance slightly lags behind DR-MAML in the 10-shot case, DR-MAML+ surpasses other baselines in both the Worst and CVaR\({}_{}\) indicators. This implies that DR-MAML+ exhibits more

Figure 4: **Meta testing performance in Pendulum 10-shot and 20-shot problems (5 runs). Reported are testing MSEs over 529 unseen tasks with \(=0.5\), where black vertical lines indicate standard error bars.**

Figure 3: **Meta testing performance in sinusoid regression problems (5 runs). The charts report testing mean square errors (MSEs) over 490 unseen tasks  with \(=0.7\), where black vertical lines indicate standard error bars.**

robustness in challenging task distributions, e.g., 5-shot case. Furthermore, the standard error associated with our method is significantly smaller than others, underscoring the stability of DR-MAML+.

### System Identification

The system identification corresponds to learning a dynamics model from a few collected transitions in physics systems. Here, we consider the Pendulum system and create diverse dynamical systems by varying its mass \(m\) and length \(l\), with \((m,l)([0.4,1.6],[0.4,1.6])\). A random policy collects transitions for meta training, and 10 random transitions work as a support dataset.

**Result & analysis.** Fig. 4 shows no significant difference between 10-shot and 20-shot cases. DR-MAML+ dominates the performance across all indicators in both cases. Due to the min-max optimization, TR-MAML behaves well in the worst-case but sacrifices too much average performance. Within the studied strategies, DR-MAML+ exhibits an advantage over DR-MAML regarding CVaR\({}_{}\).

### Few-shot Image Classification

We perform few-shot image classification on the _mini_-ImageNet dataset , with the same setup in . The task is a 5-way 1-shot classification problem. And 64 classes are selected for constructing meta-training tasks, with the remaining 32 classes for meta-testing.

**Result & analysis.** In Table 1, methods within a two-stage distributionally robust strategy, namely DR-MAML and DR-MAML+, show superiority to others across all indicators in both training and testing scenarios, which is similar to empirical findings in . Interstingly, DR-MAML+ and DR-MAML are comparable in most scenarios, and we attribute this to the small batch size in training, which weakens KDE's quantile approximation advantage.

### Meta Reinforcement Learning

Here, we take 2-D point robot navigation as the meta reinforcement learning benchmark in evaluation. The goal is to reach the target destination with the help of a few exploration transitions for fast adaptation, and we retain the setup in MAML . In meta testing, we randomly sample 80 navigation goals and examine methods' navigation performance.

**Result & analysis.** As reinforcement learning methods fluctuate fiercely in worst-case indicators, we only report Average and CVaR\({}_{}\) returns in Table 2. We observe that using studied strategies in DR-MAML enhances the returns. DR-MAML+ benefits from a more reliable quantile estimate and achieves superior performance. The application of distributional robustness to reinforcement learning yields improvements in returns.

    &  &  \\  Method & Average & Worst & CVaR\({}_{}\) & Average & Worst & CVaR\({}_{}\) \\  MAML  & 70.1\(\)2.2 & 48.0\(\)4.5 & 63.2\(\)2.6 & 46.6\(\)0.4 & 44.7\(\)0.7 & 44.6\(\)0.7 \\ TR-MAML  & 63.2\(\)1.3 & 60.7\(\)1.6 & 62.1\(\)1.2 & 48.5\(\)0.6 & 45.9\(\)0.8 & 46.6\(\)0.5 \\ DRO-MAML  & 67.0\(\)0.2 & 56.6\(\)0.4 & 61.6\(\)0.2 & 49.1\(\)0.2 & 46.6\(\)0.1 & 47.2\(\)0.2 \\ DR-MAML  & 70.2\(\)0.2 & 63.4\(\)0.2 & 67.2\(\)0.1 & 49.4\(\)0.1 & 47.1\(\)0.1 & 47.5\(\)0.1 \\ DR-MAML+(Ours) & **70.4\(\)0.1** & **63.8\(\)0.2** & **67.5\(\)0.1** & **49.9\(\)0.1** & **47.2\(\)0.1** & **48.1\(\)0.1** \\   

Table 1: **Average 5-way 1-shot classification accuracies in _mini_-ImageNet with reported standard deviations (3 runs).** With \(=0.5\), the best results are in bold.

   Method & Average & CVaR\({}_{}\) \\  MAML  & -21.1 \(\) 0.69 & -29.2 \(\) 1.37 \\ DRO-MAML  & -20.9 \(\) 0.41 & -29.0 \(\) 0.66 \\ DR-MAML  & -19.6 \(\) 0.49 & -28.9 \(\) 1.20 \\ DR-MAML+(Ours) & **-19.2\(\)0.44** & **-28.4\(\)0.86** \\   

Table 2: **Meta testing returns in point robot navigation (4 runs).** The chart reports average return and CVaR\({}_{}\) return with \(=0.5\).

### Assessment of Quantile Estimators

With the meta trained model, e.g., DR-MAML+ in sinusoid regression, we collect the testing task risk values with different task batch sizes to estimate the VaR\({}_{}\) from respectively the crude MC and KDE. As observed from Fig. 5, the VaR\({}_{}\) approximation error decreases with more tasks, and the KDE produces more accurate estimates with a sharper decreasing trend. The above well verifies the conclusion in Theorem 4.3.

### Empricial Result Summarization

Here, we summarize two points from the above empirical results and associated theorems. (i) From Theorem 4.2/4.3 and Fig. 3/4/5: the VaR\({}_{}\) estimate relates to the reliable generalization bound, and cumulated tiny approximation errors along iterations potentially result in worse equilibrium. (ii) From Theorem 4.3/4.4, Remark 1, Fig. 3, and Table 1/2: with the studied strategy, the KDE is a better choice of task risk distribution modelling than the crude MC in tougher benchmarks, e.g., 5-shot sinusoid regression, meta-testing _mini_-ImageNet classification, and point robot navigation.

### Compatibility with Large Models

**Improved Robustness in Evaluation:** As illustrated in Fig. 6, DR-MaPLe and DR-MaPLe+ consistently outperform baselines across both average and indicators in cases, demonstrating the advantage of the two-stage strategy in enhancing the robustness of few-shot learning. DR-MaPLe+ achieves better results as KDE quantiles are more accurate with large batch sizes. These results confirm the scalability and compatibility of our method on large models.

**Learning Efficiency as Limitations:** In terms of implementation time and memory cost, we retain the setup the same as that in : use the same maximum number of meta gradient updates for all baselines in training processes, which means given \(=0.5\), the tail risk minimization principle requires double task batches to evaluate and screen sub-batches. It can be seen that both DR-MaPLe and DR-MaPLe+ consume more memories, and the extra training time over MaPLe arises from the evaluation and sub-batch screening in the forward pass. Such additional computations and memory costs raise computational and memory efficiency issues for exchanging extra significant robustness improvement in fast adaptation.

## 6 Conclusion

To conclude, this paper proposes to understand the two-stage distributionally robust strategy from optimization processes, define the convergence solution, and derive the generalization bound in the presence of tail task risk. Extensive experiments validate the studied improvement tricks and reveal more empirical properties of the studied strategy. We leave computational overhead reduction as a promising topic for future exploration in robust fast adaptation.

Figure 6: **Meta testing results on 5-way 1-shot classification accuracies with reported standard deviations (3 runs). The charts respectively report classification accuracies over 150 unseen tasks. We further conduct few-shot image classification experiments in the presence of large model. Note that CLIP  exhibits strong zero-shot adaptation capability; hence, we employ “ViT-B/16’-based CLIP as the backbone to enable few-shot learning in the same way as MaPLe with training setup N_CTX \(=2\) and MAX_EPOCH \(=30\), scaling to large neural networks in evaluation (See Appendix Section D for details).**