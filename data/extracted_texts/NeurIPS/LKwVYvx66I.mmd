# Almost Free: Self-concordance in Natural Exponential Families and an Application to Bandits

Shuai Liu

University of Alberta

shuailiu7250@gmail.com &Alex Ayoub

University of Alberta

aayoub@ualberta.ca &Flore Sentenac

HEC Paris

sentenac@hec.fr &Xiaoqi Tan

University of Alberta

xiaoqi.tan@ualberta.ca &Csaba Szepesvari

University of Alberta

csaba.szepesvari@gmail.com

###### Abstract

We prove that single-parameter natural exponential families with subexponential tails are self-concordant with polynomial-sized parameters. For subgaussian natural exponential families we establish an exact characterization of the growth rate of the self-concordance parameter. Applying these findings to bandits allows us to fill gaps in the literature: We show that optimistic algorithms for generalized linear bandits enjoy regret bounds that are both second-order (scale with the variance of the optimal arm's reward distribution) and free of an exponential dependence on the bound of the problem parameter in the leading term. To the best of our knowledge, ours is the first regret bound for generalized linear bandits with subexponential tails, broadening the class of problems to include Poisson, exponential and gamma bandits.

## 1 Introduction

Single-parameter natural exponential families (NEFs)  abound in statistical applications . In this paper we study properties of NEFs and in doing so we make two main contributions:

1. We study how tail properties of the base distribution of a NEF impose limits on the NEF: if the base distribution is subexponential (subgaussian), we show that the NEF is _self-concordant_ with a stretch factor that grows inverse quadratically (respectively, linearly)
2. In generalized linear bandits whose reward distributions follow a NEF with subexponential base distribution, we show how this new result can be utilized to derive a novel second order regret bound whose leading term is free of exponential dependencies on the problem parameter -- the first such result for this setting.

The class of distributions our results extend to includes: normal, Poisson, exponential, gamma and negative binomial distributions. Our findings _partially_ address conjectures on whether generalized linear models with unbounded targets are self-concordant . This significantly generalizes the case when the targets are assumed to be bounded1 and thus extends the applicability of the results therein.

Self-concordance in NEFs turns out to be useful for both optimization and statistical estimation. The self-concordance property controls the remainder term, or approximation error, of a NEF's second-order Taylor expansion. This is useful in designing and analyzing both estimation and optimization methods. Historically the self-concordance property was first found to be useful for optimization [18; 19; 20] and later for statistical estimation [1; 21; 22]. In this paper, we will employ the self-concordance property in bandit problems  where it helps with controlling the error terms related to estimation.

Generalized linear bandits (GLBs)  has emerged as a standard framework for studying the role that nonlinear function approximation plays in decision making problems. Earlier works on GLBs (or its special case of of logistic bandits) [23; 24; 25] naively approximates the nonlinear function with a linear first order Taylor expansion. This approach ends up paying a price in the leading term of the regret bound that is exponential in the size of the true underlying parameter. In logistic bandits,  were the first to exploit the self-concordance property of the Bernoulli distribution in order to get regret bounds free of an exponential dependence on the size of the problem parameter in the leading term.  use self-concordance to get a tighter second order Taylor expansion that better captures the curvature of the sigmoid function. Employing improved self-concordant analysis,  get second-order regret bounds for logistic bandits and  extend these results to GLBs, under the assumption that the underlying reward distributions are self-concordant. We build upon this line of research and fill a gap in the bandit literature by designing and analyzing algorithms for GLBs with subexponential reward distributions. To the best of the authors' knowledge, our work considers the most general setting in the sense that all the previous works on GLBs considered subgaussian or bounded reward distributions while ours consider subexponential ones. For subgaussian rewards, these works [23; 19; 24; 25] still depend on \(\). Note that , also employs self-concordance in GLBs but they assume bounded reward and focuses on addressing non-stationarity of the environment. In addition, their bounds also scale with \(\) in the leading term, which can be exponentially large for logistic bandits.  consider a similar setting to ours. They assume the moment generating function of the base distribution \(Q\) is defined over the entire real line. This implies that \(Q\) does not have a tail as heavy as an exponential distribution hence less general than our setting. A concurrent work  develops novel confidence sets for self-concordant GLBs to improve the theoretical bounds while we prove that all GLBs with light-tailed base distribution are self-concordant. An interesting future direction would be applying their techniques to improve the bandit result of our work.  considers GLBs with bounded reward, which is known to be self-concordant, in a regime that only a limited number of decision policy updates is available.

The paper is organized as follows: In Section 2, we introduce single-parameter NEFs and review key properties relevant to our analysis. Section 3 demonstrates the self-concordance property of subexponential (subgaussian) NEFs, with a quadratic (respectively, linear) growth rate of the stretch factor. Additionally, we establish the tightness of the linear growth rate for subgaussian NEFs. In Section 4 we apply these findings to subexponential GLBs and derive novel second-order regret bounds devoid of exponential dependencies on the problem parameter in the leading term. Proofs omitted from the main text are provided in the appendix, except for those of well-known results, which are referenced accordingly.

## 2 Preliminaries

In this section we first introduce the notation we will use. We then introduce natural exponential families, review some of their basic properties and illustrate the concepts introduced by means of an example.

### Notation

For a real-valued differentiable function \(f\) defined over an open interval, we use \(,\) and \(\) to denote the first, second and third derivative of \(f\). The set of reals is denoted by \(\), the set of nonnegative reals by \(_{+}\). For a set \(\), we denote its interior by \(^{}\). For real numbers \(a,b\), we use \(a b\) and \(a b\) to denote the \(\{a,b\}\) and \(\{a,b\}\), respectively. With \(\) a logical expression, \(\{\}=1\) if \(\) evaluates to true and \(\{\}=0\), otherwise. We use \(f g\) to indicate that \(g\) dominates \(f\) up to a constant factor over their common domain. For \(S\), \(x\), we let \(S x\) denote the set \(\{s x\,:\,s S\}\). A distribution over the reals is centered if it has zero mean. We use \(()\) to denote the probability measure over the probability space that holds our random variables and we let \(\) to denote the expectation corresponding to this measure. By \(Y Q\) we mean that the distribution of \(Y\), a random variable, is \(Q\).

### Single-parameter NEFs

In this section we give our definitions for the NEF. We only consider single-parameter natural exponential families when the base distribution is defined over the reals. We follow the approach of the beautifully written monograph of  that the reader is also referred to for any statements made about NEFs with no proofs.

Given a probability distribution \(Q\) over \(\) let \(M_{Q}:_{+}\{+\}\) denote its _moment generating function_ (MGF):

\[M_{Q}(u)=(uy)Q(dy)\,, u\,.\]

We will find it convenient to also use the logarithm of the moment generating function, which is known as its _cumulant generating function_ (CGF). We denote this by \(_{Q}:\{+\}\). Thus, \(_{Q}(u)= M_{Q}(u)\).

Let \(_{Q}=\{u:M_{Q}(u)<\}\) denote the domain of \(M_{Q}\) (and, thus the domain of \(_{Q}\)). As it is well-known, \(_{Q}\) is convex and hence \(_{Q}\) is always an interval (which, trivially, always contains \(0\)). For a subset of \(_{Q}\), denoted by \(_{Q}\), we call \(=(Q_{u})_{u}\) a natural exponential family (NEF) generated by \(Q\) where for any \(u\) we have

\[Q_{u}(dy)=(u)}(uy)Q(dy)\,.\]

It follows that \(Q_{u}\) is also a probability distribution over the reals for any \(u\) by definition. An equivalent, useful form for \(Q_{u}\) is \(Q_{u}(dy)=(uy-_{Q}(u))Q(dy)\). In applications, \(u\) denotes an unknown parameter that is to be estimated based on observations from \(Q_{u}\). Thus, \(\) allows one to express extra restrictions on the admissible parameters beyond the limits imposed by \(Q\). We call \(\) the _parameter space_, and \(_{Q}\) the _natural parameter space_.

The distributions \(Q\), \(Q_{u}\) and parameter \(u\) are referred to as the _base distribution_, the (exponentially) _tilted (base) distribution_ and the _tilting parameter_. An NEF is said to be _regular_ when \(_{Q}\) is open. It is easy to see that for any \(u,u_{0}_{Q}\), \(Q_{u}=(Q_{u_{0}})_{u-u_{0}}\), where the distribution on the right-hand side stands for the tilt of \(Q_{u_{0}}\) with parameter \(u-u_{0}\). As such, up to a constant shift of the parameter space, in a regular NEF, one can always assume that \(0_{Q}^{}\). In fact, the same can be assumed for the parameter set, as long as \(^{}\) is nonempty. If \(\) is an interval then \(^{}=\) means that \(\) is a singleton: An uninteresting case if we want to model a host of _non-identical_ distributions.

In a regular family, an equivalent way to parameterize a NEF is using the mean function (cf. Theorem 3.6, page 74, of ), \(_{Q}:_{Q}\), which is defined as

\[_{Q}(u)= y\ Q_{u}(dy)=(u)}.\]

Since \(Q_{0}=Q\), clearly, \(_{Q}(0)\) is just the mean of \(Q\). To minimize clutter, when \(Q\) is clear from the context, we will write \(\) instead of \(_{Q}\). To illustrate the developments so far, we consider the example when the base distribution is an exponential distribution.

**Example 1** (Exponential distributions).: _For \(>0\), let \(Q\) be an exponential distribution with parameter \(\): \(Q(dy)=\{y 0\} e^{- y}dy\). As is well known, the MGF of \(Q\) takes the form \(M_{Q}(u)=\) when \(u<\) and \(M_{Q}(u)=\) otherwise. Thus, \(_{Q}=\{u:M_{Q}(u)<\}=(-,)\). The mean function takes the form of_

\[(u)=^{} y(- y)(uy)dy}{M_{Q}(u)}= , u<\,.\]

In what follows, we will need the following proposition to relate the central moments of \(Q_{u}\) to the derivatives of \(_{q}\), the CGF.

**Proposition 2**.: _Let \(^{}_{Q}\) be non-empty. Then, \(_{Q}\) is infinitely differentiable on \(^{}_{Q}\). Furthermore, the first three derivatives of \(_{Q}\) at \(u_{Q}\) give the first moment, second and third central moments of \(Q_{u}\)._

In the context of Example 1, Proposition 2 gives that \(_{Q}(u)=()-(-u)\) when \(u<\). Then, \(_{Q}(u)=\), agreeing with our earlier computation.

## 3 Self-concordance of NEFs

This section contains the first set of main results of this paper. We start by giving our definition of self-concordance of NEFs, followed by a study of when this property is satisfied. We include a result that shows how self-concordance allows one to derive tail properties of members of the family from that of the base distribution.

In general, if the magnitude of a higher order derivative of a real function can be bounded in terms of a lower order derivative of the function, the function is said to be self-concordant . This property is useful for studying how fast the function changes with its argument, as well as for deriving useful bounds on how well the function can be approximated by low order polynomials . In the context of single-parameter natural exponential families, we propose the following natural definition:

**Definition 1** (Self-concordant NEF).: _Let \(=(Q_{u})_{u}\) be a NEF with parameter set \(^{}_{Q}\) and some base distribution \(Q\). We say that \(\) is self-concordant if there exists a nonnegative valued function \(:_{+}\) such that_

\[|(u)|(u)(u) u \,.\] (1)

_Any function \(:_{+}\) that satisfies Eq. (1) is called a stretch function of the NEF._

This definition takes inspiration from the works of  and  who define an analogous property (cf. Assumption 2 of ). By Proposition 2, we know that \((u)\) is the variance of \(Q_{u}\), while \((u)\) is the third central moment. Hence, \((u)\) is nonnegative, which explains why there is no absolute value on the right-hand side of Eq. (1).

According to our definition, we require that the (absolute value) of the second derivative of \(\) is bounded by the first derivative, up to the "stretch factor" \((u)\). Clearly, provided that \(\) is positive over \(\), self-concordance is equivalent to stating that \(_{Q}(u)(u)|}{(u)}\) is finite valued over \(\). When \((u)=0\) for some \(u\), one can show that \(Q_{u}\) must be a Dirac distribution and hence so does \(Q\) (\(Q\) and \(Q_{u}\) share their support). In this case, we also have \( 0\) and hence any nonnegative valued function is a valid stretch-function. In particular, \(_{Q} 0\) is also a valid stretch function.

It turns out that studying self-concordance property of distributions in detail can turn out much finer bounds than naively bounding \(\) and \(\) separately.

**Example 3** (Avoiding exponential dependencies).: _Consider a NEF \(\) with base distribution \(Q\), a Bernoulli distribution with parameter \(1/2\), we have \(_{Q}=\), \((u)=}\) and thus \(=(1-)\), \(=(1-2)\). Hence \(\) is \(_{Q}\)-self-concordant with \(_{Q}(u) 1\) for all \(u\). This is an example where naively bounding \(_{Q}\), by bounding the numerator and denominator separately over \([-s,s]\) for \(s>0\) gives a quantity of size \(e^{s}\), which lags far behind the constant we obtained with a direct calculation, or what we can get from the result in Section 3.2, which show a scaling of order \(O(s)\)._

As opposed to earlier literature where self-concordance is used , we allow a non-constant stretch-factor \(\). As it turns out, this is necessary if \(=^{}_{Q}\) is to be allowed:

**Example 4** (Non-constant stretch factor).: _Consider a NEF \(\) with base distribution \(Q\). For \(Q\) an exponential distribution with parameter \(>0\), \(\) is self-concordant over \(=_{Q}=(-,)\) with \(_{Q}(u)=\), \(u<\). Indeed, a simple calculation gives that for \(u<\), \((u)=(-u)^{-2}\) and \((u)=2(-u)^{-3}\), and so \(|(u)|/(u)=2/(-u)\)._

As was shown above, for the NEF built on the exponential distribution with parameter \(\), there is no constant stretch factor that makes the NEF self-concordant over the entirety of the natural parameter space. The main result of the next section shows that a non-constant stretch factor with growth similar

[MISSING_PAGE_FAIL:5]

In words, \(_{}\) is the class of distributions over the reals whose right-tail displays an exponential decay governed with the rate parameter \(c_{1}>0\) and scaling constant \(C_{1}>0\). Similarly, we let \(_{}(c_{1},C_{1})\) be the class of distributions \(Q\) over the reals such that for \(X Q\), \(Y=X-X\) satisfies Eq. (3). With this notation, the first part of the previous proposition is equivalent to that if \(Q_{}(c_{1},C_{1})\) is centered then \(M_{Q}\) stays below the function \( 1+^{2}}{c_{1}(c_{1}-)}\) over the interval \([0,c_{1})\). In particular, this means that \(_{Q}\) contains \([0,c_{1})\).

With this, we are ready to present our theorem that establishes the self-concordance property of NEFs with subexponential tails.

**Theorem 7**.: _Let \(Q_{}(c_{1},C_{1})_{}(c_{ 2},C_{2})\) for some positive constants \(c_{i},C_{i}\), \(i=1,2\). Then, the NEF \(=(Q_{u})_{u}\) is self-concordant. Moreover, the function \(:_{+}\) defined by_

\[(u)=[2eC_{1}c_{1}(-u} )^{2}+-u}]+G_{Q}(C_{1},C_{2},c_{1},c_{2})&0 u<c_{1}\\ [2ec_{2}C_{2}(+u})^{2}++u}]+G_{Q}(C_{2},C_{1},c_{2},c_{1})&-c_{2}<u<0,\]

_is a stretch function for the NEF \(\), where \(G_{Q}(M_{1},M_{2},m_{1},m_{2})\) is a polynomial whose coefficients depend on \(Q\)._

The exact expression of \(G_{Q}\) can be found in Eq. (8). Let \(\) be a NEF with base distribution \(Q\) when \(Q\) is a zero-mean Laplace distribution with variance \(2^{2}\). In this case, we can choose \(c_{1}=c_{2}=1/\), \(C_{1}=C_{2}=1/2\). The actual stretch function is \(_{Q}(u)=|u|}{(1/-u)(1/+u)(3^{4}u^ {2}+^{2})}\). Thus, the above theorem gives the correct behavior in that both the stretch function from the theorem and the actual stretch function \(_{Q}\) blow up with the inverse of the distance to the boundaries of \(_{Q}\), except that the actual growth scales linearly with the inverse distance, while the theorem gives a quadratic scaling. It remains an open problem to see whether this quadratic order can be improved.

The following corollary is an immediate result of Lemma 5 and Theorem 7 (Appendix C.3), but can also be proved directly from the definitions (and we include a direct proof as part of the proof of Theorem 7).

**Corollary 8** (Distributions in regular NEFs are subexponential).: _Let \(u_{Q}^{}\). Then \(Q_{u}\) is subexponential both on left and right._

Because of this result, there is essentially no loss in generality in only considering the subexponential case when working with NEFs. In particular, self-concordance in NEFs is "almost free".

Proof sketch for Theorem 7We sketch here the result for \((0)=0\), \((0)>0\) and \(u 0\). The arguments used to extend the result to all cases can be found in Appendix C. By Proposition 2, bounding the stretch function of a NEF amounts to showing

\[_{Q}(u):=Q_{u}(dy)}{(y-(u))^{2}Q_{u}(dy )}(u),\]

where the division by \((Q_{u})=(y-(u))^{2}Q(dy)\) is justified by Lemma 12. We split the proof of that upper bound into two steps: controlling the variance and the absolute third moment.

Step 1: Controlling the varianceSince \((0)=(Q)>0\), there exists a \(Q\)-dependent constant \(>0\) and an interval \([-b,-a]_{<0}\) s.t. \(Q([-b,-a])\) (Lemma 16). With this observation, we can show (Lemma 17):

\[(u) a^{2}}{M_{Q}(u)}.\] (4)

Thus, the second moment decreases at most exponentially with the parameter \(u\).

Step 2: Controlling the absolute third central momentFirst, we use a classical result on moments of random variables (see the proof of \(i ii\) for prop.2.5.2 in ):

\[|y-(u)|^{3}Q_{u}(dy) =_{0}^{B}3t^{2}(|Y-(u)| t)dt+_{B}^{ }3t^{2}(|Y-(u)| t)dt\] \[B(u)+_{B}^{}3t^{2}(| Y-(u)| t)dt,\] (5)where \(Y Q_{u}\) and \(B\) is a constant to be optimized. It should remain small enough for the first term not to blow up. On the other hand, it should be large enough for the second term divided by the lower bound obtained on \((u)\) to remain controlled.

To upper bound the second term in Eq. (5), we start by showing that the right tail of the tilted distribution \(Q_{u}\) is also subexponential (Lemma 18):

\[Q_{u}((t,))(u)}e^{-(c_{1}-u)t}-u}  0 u<c_{1}\,.\] (6)

Following this lemma, we get an upper bound on \((u)\) (Lemma 20):

\[0(u)(-u})^{2} 0  u<c_{1}\,.\] (7)

In the proof, we bound separately the positive and negative values in the second term of Eq. (5):

\[_{B}^{}3t^{2}(|Y-(u)| t)dt=^{ }3t^{2}(Y(u)+t)dt}_{}+^{}3t^{2}(Y(u)-t)dt}_{}\,.\]

We give here the sketch of proof on the bound of \(\) as the proof for bounding \(\) is nearly identical. From plugging in Eq. (6), we get:

\[(u)}-u}_{B}^{}3t^{2}e^{ -(c_{1}-u)(t+(u))}dt.\]

By choosing \(B-u}+(-u})^{2}\), some algebra gives:

\[}{M_{Q}(u)}(b^{2}}{c_{1}^{3}C_ {1}^{3}}).\]

Setting \(B(-u})^{2}++u}\) gives a similar bound on \(\). Chaining the bounds on \(\) and \(\) with Eq. (5) and Lemma 17 finishes the proof. 

### Self-concordance with a subgaussian base

In this section we refine the previous result for NEFs by considering the case when the base distribution is subgaussian. Let \(>0\). Recall that a centered distribution \(Q\) is \(\)-subgaussian if for all \(u\), \(M_{Q}(u) e^{^{2}u^{2}/2}\) (or, equivalently, \(_{Q}(u)^{2}u^{2}/2\) for any \(u\)). A non-centered distribution is \(\)-subgaussian, if it is subgaussian after centering. Similarly to the subexponential case, one can show that a centered distribution \(Q\) is subgaussian if and only if for some \(,C>0\), it holds that for any \(t 0\), \((|X| t) C(-t^{2}/(2^{2}))\) where \(X Q\) (cf. Proposition 2.5.2 of ).3

Our promised result is as follows:

**Theorem 9**.: _Let \(Q\) be subgaussian. Then, the NEF \(=(Q_{u})_{u}\) is self-concordant and \(_{Q}(u)=O(|u|)\), \(u\)._

As it turns out, the linear growth exhibited in the previous result is tight for NEFs with a subgaussian base distribution:

**Theorem 10**.: _There exists a distribution \(Q\) that is subgaussian such that \(_{u}_{Q}(u)/u>0\)._

Again, this shows that even if we stay with subgaussian distributions, it would be limiting to only consider NEFs that are self-concordant with a bounded (or constant) stretch function over their natural parameter space.

Generalized Linear Bandits

In this section we apply the self-concordance property of subexponential NEFs in order to derive novel confidence sets and regret bounds for subexponential generalized linear bandits. To our knowledge, these are the first such results for parametric bandits with subexponential rewards.

### Bandit model

Following Filippi, Cappe, Garivier, and Szepesvari , we consider stochastic generalized linear bandit (GLB) models \(\) specified by a tuple \((,,)\), where \(^{d}\) is a non-empty arm set, \(^{d}\) is a non-empty set of potential parameters, both closed for convenience, \(=(Q_{u})_{u_{Q}}\) is a NEF with base distribution \(Q\). Without the loss of generality we assume that \(\) is a compact subset of the Euclidean unit ball of \(^{d}\).

In each round \(t^{+}\), the learner selects and plays an arm \(X_{t}\). As a response, they receive a reward \(Y_{t}\) sampled from the distribution \(Q_{X_{t}^{}_{}}\), where \(_{}^{d}\) is a parameter of the bandit environment, which is initially unknown to the learner. The learner's goal is to maximize its total expected reward. The GLB is _well-posed_ when

\[\{x^{}\,:\,x,\} _{Q}^{}\]

holds, which we assume from now on. The condition that \(_{Q}\) simply ensures that the reward distributions \(Q_{x^{}}\) are defined regardless of the value of \((x,)\). We require the stronger condition \(_{Q}^{}\) to exclude the boundaries of the interval \(_{Q}\). This way we avoid pathologies that arise when a parameter reaches the boundary of \(_{Q}\) (e.g., when \(Q\) is the exponential distribution with parameter \(\), the mean and variance of \(Q_{u}\) grow unbounded as \(u\) approaches \(\) from below).

The expected reward in round \(t\) given that the learner plays \(X_{t}\) is \([Y_{t}|X_{t}]=(X_{t}^{}_{})\). The performance of the learner will be assessed by their pseudo regret \(R(T)\), which is the total cumulative shortfall of the mean reward of the arms the learner chose relative to optimal choice:

\[R(T)=_{t=1}^{T}(x_{}^{}_{})-(X_{t}^{}_ {})\,.\]

Here, \(x_{}_{x}(x^{}_{})\) is the arm that results in the best possible expected reward in a round. For simplicity, we assume that such an arm exists. We establish guarantees of our algorithm for a subclass of GLBs, captured by the following assumption:

**Assumption 1** (Subexponential base).: _The base distribution \(Q\) is subexponential both on the left and the right. In particular, \(Q_{}(c_{1},C_{1})_{}(c_{2},C_{2})\) for some \(c_{i},C_{i}\) (\(i=1,2\)) positive numbers. Furthermore, \(-c_{2}<<c_{1}\) and \(c_{1},c_{2}\) are known to the learner._

Note that in a well-posed GLB, \(_{Q}<<_{Q}\). In light of this, the assumption just stated boils down to whether \(0_{Q}^{}\), which, as discussed, is free when \(_{Q}^{}\). Indeed, when \(0_{Q}^{}\) holds, \(_{Q}<0<_{Q}\) and one can always find positive values \(c_{1},c_{2}\) such that \(_{Q}<-c_{2}<<c_{1}< _{Q}\). Then, from Proposition 6 and some extra calculation one can conclude that the \(Q_{}(c_{1},e^{-c_{1}X}M_{Q}(c_{1})) _{}(c_{2},e^{-c_{2}X}M_{Q}(-c_{2}))\). Since it is assumed that the learner has access to \(Q\), we see that Assumption 1 can be satisfied whenever \(0_{Q}^{}\), which we think is a rather mild assumption. We will also for the sake of simplicity assume that the learner has access to \(S_{0}=\{\|\|\,:\,\}\), \(S_{2}=\), \(S_{1}=\). These values will be used in setting the parameters of the algorithm. Note that it is not critical that the learner knows these exact values; appropriate bounds suffice. We also assume that the learner is given access to an upper bound on the worst-case variance over the parameter space \(\):

**Assumption 2** (Bounded Variance).: _The learner is given \(L 1\) such that \(_{u}(u) L\)._

Note that since the GLB is well-posed, \(_{u}(u)<\) is automatically satisfied. Also, there is no loss of generality in assuming \(L 1\). A crude upper bound on \(_{u}(u)\) is \(C_{1}e/(c_{1}-S_{1})^{3} C_{2}e/(c_{2}+S_{2})^{3}\), so this assumption is implied by the previous one.

### The OFU-GLB algorithm and its regret

Just like the previous works [11; 12; 13] which considered special cases of the generalized linear bandit problem, our algorithm follows the "optimism in the face of uncertainty" principle. In each time step, the algorithm constructs a confidence set \(_{t}\), based on past information, that contains the unknown parameter \(_{}\) with a controlled probability. Next, the algorithm chooses a parameter \(_{t}\), in the confidence set \(_{t}\), and an underlying action \(X_{t}\) such that the mean reward underlying \(X_{t}\) and \(_{t}\) is as large as plausibly possible. Since \(=_{Q}\) is guaranteed to be an increasing function (recall that \((u)\) is the variance of \(Q_{u}\) and is hence nonnegative), it suffices to find the maximizer of \(x^{}\) where \((x,)\). We call our algorithm, shown in Algorithm 1, OFU-GLB (Optimism in the Face of Uncertainty in Generalized Linear Bandits). The main novelty here is that our bandit model makes minimal assumptions.

```
0: GLB instance \(=(,,)\) for\(t=1,2,\)do  Construct \(_{t}\) based on \(((X_{s},Y_{s}))_{s<t}\) and \(\)  Compute \((X_{t},_{t})*{arg\,max}_{(x,) _{t}}x^{}\)  Select arm \(X_{t}\) and receive reward \(Y_{t} Q_{X_{t}^{}_{}}\) endfor ```

**Algorithm 1** The OFU-GLB Algorithm

The confidence set is based on ideas from the work of Janz, Liu, Ayoub, and Szepesvari . Note that this paper analyzed a randomized method for those GLBs where \(_{Q}=\) and \(_{u_{t}}_{Q}(u)<\). The assumption that \(_{Q}=\) is restrictive, as it does not allow many common distributions (e.g., the exponential distribution). Thanks to Theorem 7, under our assumptions, \(_{u}_{Q}(u)<\) follows. Then, an appropriate confidence set can be constructed based on Lemma 5, which also extended the corresponding result of Janz, Liu, Ayoub, and Szepesvari . While the confidence set construction is based on the ideas of Janz, Liu, Ayoub, and Szepesvari , the main steps of the analysis are taken from  who analyzed logistic bandits, which are \(1\)-self-concordant. Our result follows by carefully modifying the proof of  and carefully propagating both the effect of replacing their confidence set with a different one, and the effect of \(_{u}_{Q}(u)>1\). This leads to the main result on GLBs:

**Theorem 11** (Regret upper bound of OFU-GLB).: _Let \((0,1]\) and \(T\) a positive integer and consider a well-posed GLB model \(=(,,)\) and assume that Assumptions 1 and 2 hold. For \(_{}\), let \((_{})=(x^{}_{})}\) and let \((T,_{})\) stand for the \(T\)-round regret of OFU-GLB when it interacts with a GLB specified by \(_{}\). Then, with an appropriate construction of \(_{t}\), for any \(_{}\), it holds that with probability at least \(1-\),_

\[(T)=}(d(x_{}^{ }_{})T}+d(_{}))\,,\]

_where \(()\) hides polylogarithmic factors in \(T,d,L,1/\) and constants that depend on the base distribution \(Q\)._

This result extends the class of distributions for which OFU algorithms with parametric models achieve sublinear regret. Previous results in the literature [11; 12; 13; 14; 15] all assume that the base distribution is a natural exponential family with subgaussian tail and prove that the OFU algorithm enjoys sublinear regret. Thus, Theorem 11 extends the class of distributions for which optimistic algorithms enjoy sublinear regret to any natural exponential family with subexponential base distribution.

An essential quality of the result is that it makes the dependence of the regret on the instance \(_{}\) explicit. Recalling that in a NEF, \((u)\) is the variance of the tilted distribution \(Q_{u}\), we see that the leading term (shown as the first term on the right-hand side of the last display) scales with the variance of the optimal arm's reward distribution. In queuing theory, the service times of agents (actions) in an environment are often modeled as exponentially distributed random variables . When aiming to minimize service times (maximize negative reward) with an exponential bandit model (with mean function \((x)=-1/x\)), the variance of the optimal arm's service time lower bounds that of the other arms. Furthermore, the dependence on \(\), a term that is inversely proportional to the optimal variance,is pushed to a second order term. In logistic bandits, \(\) can be exponentially large in the size of the parameter set \(S_{0}\) and thus much attention has been focused on mitigating its effect . Our regret bound also matches the lower bound in logistic bandits given by , thus our analysis is tight for this special case.

## 5 Conclusions and Future Work

The main contribution of this work establishes that all subexponential NEFs are self-concordant with a polynomial-sized stretch factor. We then applied this finding and derived regret bounds for subexponential GLBs that scale with the variance of the optimal arm's reward, which is the smallest variance amongst all arm's rewards in problems such as: minimizing service times  or minimizing insurance claim severity (dollars lost per claim) .

Our findings also have implications when performing maximum likelihood estimation with subexponential NEFs, which includes a rich family of generalized linear models (GLMs). Since the log loss in a NEF is the sum of a linear function and the NEF's CGF, the GLM's loss is self-concordant in the sense of (say)  whenever the NEF is self-concordant. While this is outside of the scope of our paper, it follows that this family of GLMs enjoy: \((i)\) fast rates of convergence to the minimizer for regularized empirical risk minimization , \((ii)\) fast rates for averaged stochastic gradient descent  and \((iii)\) fast rates for constrained optimization with first-order methods , without restrictive conditions on bounded responses, which previous works had to assume to achieve these results.

One interesting direction for future work would be either deriving a matching lower bound on the stretch function for subexponential NEFs or tighter analysis that matches the lower bound for subgaussian NEFs. Another potential avenue for future work would be in extending our results to other exponential families, beyond NEFs.