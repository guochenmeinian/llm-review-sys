# Breaking the curse of dimensionality in

structured density estimation

Robert A. Vandermeulen

Much of this work was conducted at the Berlin Institute for the Foundations of Learning and Data (BIFOLD), Technische Universitat Berlin. Contact: robert.anton.vandermeulen@gmail.com

Wai Ming Tai

The work was done when the author was at Nanyang Technological University. Contact: taivaiming2003@gmail.com

Bryon Aragam

University of Chicago. Contact: bryon@chicagobooth.edu

###### Abstract

We consider the problem of estimating a structured multivariate density, subject to Markov conditions implied by an undirected graph. In the worst case, without Markovian assumptions, this problem suffers from the curse of dimensionality. Our main result shows how the curse of dimensionality can be avoided or greatly alleviated under the Markov property, and applies to arbitrary graphs. While existing results along these lines focus on sparsity or manifold assumptions, we introduce a new graphical quantity called "graph resilience" and show how it controls the sample complexity. Surprisingly, although one might expect the sample complexity of this problem to scale with local graph parameters such as the degree, this turns out not to be the case. Through explicit examples, we compute uniform deviation bounds and illustrate how the curse of dimensionality in density estimation can thus be circumvented. Notable examples where the rate improves substantially include sequential, hierarchical, and spatial data.

## 1 Introduction

Density estimation is a classical problem in statistical machine learning, and provides the backbone of modern generative models such as diffusion models, which are now state-of-the-art density estimators for a variety of applications, as well as normalizing flows, energy-based models, and implicit generative models. At the same time, density estimation is a notoriously difficult problem in high-dimensions, known to suffer from the so-called curse of dimensionality. When the density depends on only a few variables or, more generally, is supported on a low-dimensional manifold, it is known that the curse of dimensionality can be circumvented by substituting the ambient dimension \(d\) with the effective dimension \(s\)(e.g. Lafferty and Wasserman, 2008; Yang and Tokdar, 2015). But what happens when the distribution is spread over the entire space in a structured manner--is it still possible to circumvent the curse of dimensionality?

Three representative examples are given in Figure 1, corresponding to sequential, hierarchical, and spatial (or convolutional) data. In these examples, although both manifold and sparsity assumptions are violated, there are structured dependencies that one might hope to leverage when estimating the underlying density. These kinds of structures are pervasive in machine learning applications. For one example, consider computer vision and imaging. Images have long been modeled as a grid graph where adjacent pixels correspond to adjacent vertices (see Keener (2010), for example). Such an assumption is very natural: pixels tend to be strongly dependent on nearby pixels and independent of far away pixels. See Figure 2 for an example of this.

These examples are particularly appealing in applications, however, we emphasize that our problem setting is considerably more general, and applies to general dependence structures given by anyMarkov random field. Since real data is expected to (approximately) exhibit these types of structures, a reasonable question to ask is whether or not the curse of dimensionality persists in such structured settings.

Structured density estimationTo formalize the notion of structured dependencies in a high-dimensional, multivariate distribution, we adopt the framework of undirected graphical models, also known as Markov random fields (MRFs). In this setting, we are given samples from an unknown distribution \(P\), with density \(p\), over the random vector \(X=(X_{1},,X_{d})\), and it is assumed that \(P\) is Markov to some undirected graph \(G\) (see Section 3.1 for definitions). We treat both cases where \(G\) is _a priori_ known, and where it is unknown but is in some known subset of all graphs. The graph \(G\) encodes the underlying dependence structure between the variables, which we hope simplifies the estimation problem. For example, when \(P\) is a Gaussian, this gives rise to the well-known Gaussian graphical model (Speed and Kiiveri, 1986), and various extensions of this idea to nonparametric settings are known, including trees (Liu et al., 2011; Gyorfi et al., 2022) and nonparanormal models (Liu et al., 2009). While this line of work also discusses the problem of _structure learning_, our focus is on the problem of nonparametric _density estimation_, which is comparatively understudied in graphical models. This may come as a surprise given the outsized literature on the general density estimation problem; see Section 2 for an overview of related work. In contrast to most existing work on density estimation, in lieu of imposing parametric or functional restrictions on \(P\), the only assumption we make in addition to the Markov assumption is Lipschitz continuity.

OverviewOur main result establishes the sample complexity of estimating such a density--for arbitrary graphs \(G\)--in total variation (TV) distance: It is approximately \((1/)^{r+2}\), where \(r d\) is a novel graphical parameter we call _graph resilience_ that depends only on \(G\). Roughly, \(r\) is a measure of how connected the graph \(G\) is; the easier it is to disconnect \(G\), the smaller \(r\) will be. The examples in Figure 1 illustrate a range of values from constant \(r=O(1)\) to sublinear \(r=O()=o(d)\). In the former case, this leads to an exponential improvement in the sample complexity over the usual nonparametric sample complexity of \((1/)^{d}\).

While it is not surprising that graphical structure (i.e. sparsity in the form of conditional independence) can make estimation easier, what is surprising is the quantity involved: It is not, as one might guess, one of the "usual" suspects such as sparsity, degree, or width. To capture the effective dimension of the problem and its resulting sample complexity, we introduce the concept of _graph resilience_. In particular, the usual suspects are insufficient to break the curse of dimensionality, whereas graph resilience does. Moreover, it is easy to construct examples where these are not only insufficient, but wholly misleading: Graph resilience can be controlled in graphs with unbounded degree, sparsity, and/or diameter.

Our work also marks a substantial departure from the existing literature that focuses primarily on functional restrictions (e.g. compositional structure), sparsity (e.g. density regression), or low-dimensional embeddings (e.g. manifold hypothesis). Instead, we impose no _explicit_ restrictions on the functional form (although certain restrictions are implicit through the Markov assumption). In practice, empirical evidence points to a combination of these properties prevailing in real-world data,

Figure 1: Examples of common structures that yield improvements in density estimation. As indicated by the path example on the left, which is also a tree, the worst-case resilience of any tree is \(r=O( d)\), but for bounded-depth trees, \(r=O(1)\).

and thus our approach hopefully serves to provide another practical assumption under which density estimation is feasible, and in particular, the curse of dimensionality can be avoided.

ContributionsMore precisely, we make the following contributions.

1. (Section 3.2) We introduce the graphical property of _resilience_ (Definition 3.2), which approximately quantifies the connectivity of an undirected graph. Resilience is defined through the process of disintegration (Definition 3.1), which is described in detail.
2. (Section 3) We prove that the sample complexity of estimating a density \(p\) that is Markov to any undirected graph \(G\) scales with the resilience \(r=r(G)\), as opposed to the dimension \(d\) (Theorem 3.1). We also provide examples to show that other metrics such as degree, diameter, and sparsity cannot properly capture the sample complexity (Section 4.3). We also show that efficient estimation is still possible even if \(G\) is unknown (Theorem 3.2).
3. (Section 4) We demonstrate numerous concrete examples where the resilience (and hence the sample complexity) can either be exactly calculated or bounded. These examples include familiar graphs such as trees (including paths for sequential data), cliques, and grids (also known as lattice graphs), and represent a broad continuum of possible complexities (Section 4.2).

All told, the potential savings implied by our results can be substantial, and are not isolated or pathological in any way: If there is a _single_ independence relation satisfied by \(P\), the effective dimension will be strictly less than the ambient dimension \(d\), and in practical applications such as spatial or imaging data, there is an exponential savings in the sample complexity (see Figure 1). As we show, the graph resilience reveals a continuum of complexities ranging from dimension-independent (i.e. \(r=O(1)\)), in which case the curse of dimensionality is circumvented completely, to dimension-dependent with nontrivial savings (e.g. \(r=o(d)\)).

## 2 Related work

We begin by recalling classical rates and results on density estimation. The standard nonparametric rate for estimating an \(L\)-Lipschitz density \(p\) in \(d\) dimensions in TV distance is \(n^{-1/(d+2)}\); see Devroye and Gyorfi (1985); Tsybakov (2009); Gine and Nickl (2015) for more details. This rate ignores dimension-dependent constants that affect finite-sample rates, and a more refined bound on the sample

Figure 2: Heatmaps of the magnitude of the correlation between red pixel and every other pixel, using the CIFAR-10 training set. The left image shows the correlation without conditioning, the right image shows correlation conditioned on the green pixels. We see that the modeling the image as a Markov random grid is valid.

complexity (ignoring log-factors) is given in McDonald (2017):

\[n}{^{d+2}}.\] (1)

See also Ghorbani et al. (2020); Jiao et al. (2023). For comparison, our main result is that only

\[n}{^{r+2}}\] (2)

samples are needed (again up to log-factors) when \(p\) is Markov to an undirected graph \(G\) with resilience \(r=r(G)\), and this rate cannot be improved among graphs whose resilience is at most \(r\). The improvement over (1) is clear: Not only the exponent, which dominates the rate, but also the constant is improved by replacing \(d\) with \(r\), which can be much smaller than \(d\) (see examples in Section 4.2).

Ignoring the dimension-dependent constant factor (as most papers do), the basic idea behind most results on circumventing the curse of dimensionality is to replace the \(d\)-dependence in the exponent of (1) with some \(s<d\), where \(s\) is the _effective dimension_ of the problem. Examples include sparsity, low-dimensional embeddings (e.g. manifold assumptions), and hierarchical and/or compositional structure. Viewed from this perspective, our main contribution is to propose a new measure of effective dimension in structured data, where \(s=r\) is the graph resilience.

Recently, there has been a renewed interest in this problem along two broad axes: 1) Generative models as density estimators, and 2) Breaking the curse of dimensionality. In the remainder of this section, we review this related work.

### Density estimation

As noted in the introduction, density estimation is a classical problem with a literature dating back more than 50 years. Notably, Stone (1980, 1982) established minimax rates for nonparametric estimation problems including density estimation and regression. These papers derived the now-classical nonparametric rate \(n^{-/(2+d)}\) for \(\)-smooth densities, which implies the curse of dimensionality, i.e. unless \(p\) is very smooth, then the sample complexity of estimating \(p\) is exponential in the dimension. Yet, at the same time, the stark practical success of generative models suggest that high-dimensional density estimation may not be quite as intractable as this slow rate suggests. Motivated by these empirical observations, a growing line of work has established minimax optimality for a range of generative models, including GANs (Liang, 2017; Singh et al., 2018; Singh and Poczos, 2018; Uppal et al., 2019, 2020), diffusion models (Oko et al., 2023; Zhang et al., 2024; Cole and Lu, 2024; Tang and Yang, 2024), and variational autoencoders (Tang and Yang, 2021; Kwon and Chae, 2024).

Other theoretical developments have focused on efficient algorithms (Acharya et al., 2021, 2017; Chan et al., 2014) in the univariate case.

There has also been recent interest in developing density estimators that exploit graphical structure (Germain et al., 2015; Johnson et al., 2016; Khemakhem et al., 2021; Wehenkel and Louppe, 2021; Chen et al., 2024). Of course, there is an enormous literature on algorithms and methods for general density estimation that we cannot cover here.

Most closely related to our work are the papers Liu et al. (2007, 2011); Gyorfi et al. (2022). Liu et al. (2007) use the RODEO estimator on a model that satisfies a sparsity assumption, i.e. only \(s d\) variables are involved in the nonparametric component. Liu et al. (2011) use forests to approximate the underlying density under certain regularity conditions; Gyorfi et al. (2022) relax these conditions and replace forests with trees. Their main result is a pointwise \(O(n^{-1/4})\) rate of convergence for estimating a tree-structured density, which is notably dimension-independent. The main difference between our results and these previous results is that our results apply to general graphs \(G\) that may not be trees or forests, in addition to being _uniform_ in \(p\) and unimprovable (cf. Remark 1 in Gyorfi et al., 2022, in particular). Most importantly, moving beyond tree-based models requires new ideas, and motivates our introduction of the graph resilience to measure the effective dimension of the estimation problem.

After the initial posting of our paper, we were made aware of the related work by Bos and Schmidt-Hieber (2023) which proposes a supervised approach to density estimation that also leverages the Markov assumption to break the curse of dimensionality in density estimation.

### Curse of dimensionality

There is a long line of literature on understanding how and when the curse of dimensionality can be avoided. Common assumptions include the manifold assumption (Pelletier, 2005; Ozakin and Gray, 2009; Jiang, 2017; Schmidt-Hieber, 2019; Nakada and Imaizumi, 2020; Berenfeld et al., 2022; Jiao et al., 2023), additive structure (Stone, 1985; Raskutti et al., 2012), compositional structure (Horowitz and Mammen, 2007; Juditsky et al., 2009; Kohler and Krzyzak, 2017; Schmidt-Hieber, 2017; Bauer and Kohler, 2019; Kohler and Langer, 2021; Shen et al., 2021), low-rank structure (Hall and Zhou, 2003; Hall et al., 2005; Song and Dai, 2013; Amirdi et al., 2022, 20; Vandermeulen and Ledent, 2021; Vandermeulen, 2023) and sparsity (Liu et al., 2007; Lafferty and Wasserman, 2008; Yang and Tokdar, 2015). Bach (2017) showed that neural networks are adaptive to many of these underlying structures.

This line of work is particularly relevant as it pertains to breaking the curse of dimensionality via structural assumptions on the unknown parameter. Notably, it seems that the advantages of (in)dependence via the Markov property have not been thoroughly investigated. Our work aims to fill this gap for a wide range of structured models that _do not fit into_ any of the classes above. Indeed, it is easy to construct densities that are non-sparse (i.e. every variable is active), non-additive and non-compositional (we consider arbitrary continuous densities), and are not supported on any lower-dimensional manifold, but that are Markov to a given graph \(G\).

## 3 Main Results

Before presenting the main results of this paper we must present some basic background.

### Background Definitions and Notation

Throughout the paper, we use undirected graphs to model the dependencies in \(P\). We adopt the usual terminology and conventions from graphical models: \(G=(V,E)\) is an undirected graph with \(V=X=[d]\) and \(d=(X)\). To avoid technical complications, we assume compact support with \(X=(X_{1},,X_{d})^{d}\). Two disjoint subsets \(A,B V\) are said to be separated by \(C\) if all paths connecting \(A\) to \(B\) intersect \(C\); equivalently, the subgraph over \((A B)-C\) is disconnected. The distribution \(P\) is called Markov with respect to \(G\) if

\[ABC A\!\!\!_{P}B\,|\,C,\] (3)

where \(\!\!\!_{P}\) denotes conditional independence in \(P\). In other words, graph separation implies conditional independence, but not necessarily vice versa. See Lauritzen (1996) for a review of graphical modeling terminology.

We term a subgraph of \(G\) to be a _component of \(G\)_ (sometimes called a _connected component_) if it is a maximal connected subgraph of \(G\). A _path_ in \(G\) is a sequence of vertices \((v_{0},v_{1},,v_{k})\) such that \(v_{i}-v_{i+1}\) in \(G\) (i.e. \((i,i+1) E\)). A _simple_ path is a path without repeated vertices. The _length_ of a path is the number of edges in the path; e.g. the length of \((v_{0},v_{1},,v_{k})\) is \(k\). A _geodesic_ path between two vertices is any path of shortest length, and the distance between two vertices is the length of any geodesic between them. For graphs \(G,G^{}\) the notation \(G G^{}\) denotes a disjoint union of graphs, i.e., the graph whose vertex set is the disjoint union of the vertices in \(G\) and \(G^{}\) and inherits edges from the edges in \(G\) and \(G^{}\). For a graph \(G=(V,E)\), for \(V^{} V\), the graph \(G V^{}\) denotes the graph with vertices \(V V^{}\) and the edges \(\{i,j\} E\) where \(\{i,j\} V V^{}\).

For any \(d\) and \(L 0\), let \(_{d}\) be the set of densities on \(^{d}\) and \(_{d,L}_{d}\) be those densities which are \(L\)-Lipschitz continuous. For any graph \(G\) with \(d\) vertices, we define \((G)_{d}\), such that, for \(p(G)\) and \((X_{1},,X_{d}) p\), the entries of the random variable, \(X_{1},,X_{d}\), satisfy the global Markov property with respect to the graph \(G\). Finally let \(_{L}(G)(G)_{d,L}\). When estimating these densities we will sometimes use the term _sample complexity_. This refers to the number of samples necessary to estimate a density to within \(\) error in total variation distance. For functions, \(f\) on \(^{d}\) we define \(\|f\|_{1}=|f(x)|dx\); this is the total variation distance.

### Graph Resilience

The key concept in this work, which characterizes the difficulty of estimating densities in \((G)\), is what we term the _graph resilience of \(G\)_. Graph resilience is based on a process we call a _disintegration_.

**Definition 3.1**.: For a graph \(G=(V,E)\), an \(r\)-tuple \((V_{1},,V_{r})\) with \(V_{i} V\) is called a _disintegration of \(G\)_ if:

1. \(\{V_{i}\}_{i}\) is a partition of \(V\);
2. The elements of \(V_{1}\) all lie in different components of \(G\);
3. \(|V_{1}|\) is equal to the number of components in \(G\);
4. For all \(i[r-1]\) the elements of \(V_{i+1}\) lie in different components of \(G_{j=1}^{i}V_{j}\);
5. \(|V_{i+1}|\) is equal to the number of components in \(G_{j=1}^{i}V_{j}\).

The _length_ of a disintegration is the value \(r\) in the above definition. A disintegration of length \(r\) will be called an \(r\)-disintegration.

The first disintegration step, \(V_{1}\) above, can be thought of as the process of picking one vertex from each component in \(G\) and removing it. The next step of a disintegration, \(V_{2}\) above, then selects and removes one vertex from each component of the resultant graph. An \(r\)-disintegration is a sequence of \(r\) such steps until one is left with the null graph. It is possible for a graph to admit many different disintegrations of different lengths. A visual representation of the steps of a graph disintegration can be found in Figure 3.

The _resilience_ of a graph \(G\) describes the length of the shortest possible disintegration of \(G\).

**Definition 3.2**.: For a graph \(G\), the _resilience of \(G\)_, denoted \(r(G)\), is the smallest \(r\) such that there exists a \(r\)-disintegration of \(G\).

We elaborate more on some properties of graph resilience in Section 4, including upper bounds for the graph resilience of several graphs corresponding to image or sequence data. First, we need to establish that graph resilience is the right metric for quantifying the sample complexity of density estimation.

### Sample complexity

The following theorem characterizes the difficulty of estimating a density which is known to satisfy the Markov property in terms of its graph. All proofs are deferred the appendix.

**Theorem 3.1**.: _Let \(G\) be a (known) graph whose number of vertices is \(d\) and resilience is \(r\). Let \(L 0\). For any \(0<<1\), there exists an algorithm that takes \(n=(}}}} }{}}}}}}}}}}()+})\) i.i.d. samples drawn from any \(p_{L}(G)\) and returns a distribution \(q\) such that_

\[\|p-q\|_{1}1-.\]

This corresponds to a convergence rate of \((n^{-1/(r+2)})\) and is _uniform_ over \(_{L}(G)\). For comparison, the rate \(O(n^{-1/(d+2)})\) is known to be optimal for estimating densities in \(_{d,L}\) in the total variation norm (e.g. Beirlant and Gyorfi, 1998). Consequently the rate \((n^{-1/(r+2)})\) indicates that the rate of convergence when estimating densities in \(_{L}(G)\) is akin to estimating densities in \(_{r,L}\). We will see in Section 4.2 that, for graphs typically used to represent audio or image data, the effective dimension for estimating densities can be drastically, even exponentially, smaller than the ambient dimension.

Figure 3: Visual representation of the steps of the \(3\)-disintegration \((\{1,6\},\{3,5,7\},\{2,4\})\). In each step of the disintegration, one vertex is removed from every graph component. The total number of steps to the null graph is \(3\).

While it may be reasonable to assume a known graph in some situations, one often encounters the situation where the graph is unknown. If one assumes that the graph \(G\) is unknown, but that \(G\) lies in some subset of graphs whose maximum graph resilience is bounded above by some value \(r\) then it is still possible to achieve a rate of \((n^{-1/(r+2)})\).

**Theorem 3.2**.: _Let \(\) be the set of all graphs whose number of vertices is \(d\) and resilience is \(r\). Let \(L 0\). For any \(0<<1\), there exists an algorithm that takes \(n=(L^{r}}{^{r+2}}()+})\) i.i.d. samples drawn from any \(p_{G}_{L}(G)\) and returns a distribution \(q\) such that_

\[\|p-q\|_{1}1-.\]

We show in Appendix D that these rates are optimal (up to a polylogarithmic factor) for all dimensions \(d\) and resiliences \(r\). However, it's worth noting that these rates are not optimal for all graphs \(G\). For instance, for the special case of trees see Gyorfi et al. (2022), where better rates can be achieved. The case of general graphs \(G\) is an open problem.

### Proof Outline and Practical Consequences

We will outline our proof techniques and demonstrate their relation to practically implementable estimators. Our theorem proofs employ disintegrations to construct a class of densities that closely approximate those with the given MRF. This class contains densities that take the form of histograms, i.e., piecewise constant densities on a grid. The remainder of the argument is relatively standard: A finite collection of the aforementioned class is found to cover the space, from which our estimate is chosen using a method akin to Scheffe tournaments (Scheffe, 1947; Yatracos, 1985; Devroye and Lugosi, 2001; Ashtiani et al., 2018). Disintegrations are the core novel aspect of this work, characterizing a class of histograms much smaller than the full set, thereby reducing its metric entropy and enhancing the sample efficiency of our selection.

At a high level, a disintegration outlines a method to estimate a density by inductively conditioning out the entries of a random vector. For instance, consider a random vector \([X_{1},X_{2}]\). The disintegration in Figure 4 corresponds to first estimating a histogram for \(X_{2}\), and then, for each bin \(b\) of the \(X_{2}\) histogram, estimating a histogram for \(X_{1}|X_{2} b\). The resilience of a graph simply characterizes the shortest disintegration, i.e. the most efficient decomposition of a distribution into factors. Removing one vertex from each component of a graph captures the idea that, after sufficient conditioning, these components become independent. This allows us to avoid estimating the high-dimensional joint density of all vertices in the graph. Instead, we can estimate the low-dimensional components individually and take their product, which is effective due to the following inequality

\[\|_{i=1}^{d}p_{i}-_{j=1}^{d}q_{j}\|_{1}_{i=1}^{d} \|p_{i}-q_{i}\|_{1}p_{1},,p_{d},q_{1},,q_{d}.\]

Thus a disintegration can be thought of as a "meta-algorithm" providing an ordering for estimating conditional densities. In practice, it would be up to the implementation to determine how to handle the one-dimensional and conditional density estimation.

## 4 Graph Resilience Examples

Theorems 3.1 and 3.2 demonstrate that the graph resilience acts as the effective dimension of a nonparametric estimation problem, however, graph resilience is still somewhat of an opaque property. In this section we will go over basic properties of graph resilience and describe graph resiliences for some graphs that reflect real-world settings.

### Graph Resilience Properties

Here we present some basic results regarding graph resilience. We first introduce two very basic lemmas outlining the most fundamental properties of graph resilience. The first lemma shows how graph resilience behaves with disjoint graph union.

Figure 4: A simple disintegration example.

**Lemma 4.1**.: _Let \(G_{1},,G_{m}\) be graphs, then \(r(_{i=1}^{m}G_{i})=_{i}r(G_{i})\)._

This lemma encapsulates the notion that estimating joint density for a collection of independent random vectors, e.g., \((Y_{1},,Y_{m})_{i=1}^{m}_{i}\), from a collection of samples is no more difficult than estimating each independent vector individually, \(_{i}_{i}\), and taking the product measure, \(_{i=1}^{m}_{i}_{i=1}^{m}_{i}\).

The second lemma demonstrates the behavior of graph resilience upon vertex removal.

**Lemma 4.2**.: _Let \(G=(V,E)\) be a graph and let \(V^{} V\), then \(r(G) r(G V^{})+|V^{}|\)._

This corresponds to conditioning the random variables in \(V V^{}\) on the \(|V^{}|\)-dimensional random vector corresponding to \(V^{}\).

Simpler graphs, in terms of number of edges and vertices, tend to have smaller graph resilience. The following lemma shows that adding edges to a graph will, at most, increase its resilience by the number of edges added.

**Lemma 4.3**.: _Let \(G=(V,E)\) be a graph, let \(E^{}\) be a set of edges for vertices \(V\), and let \(G^{}=(V,E E^{})\), then \(r(G^{}) r(G)+|E^{}|\)._

For a pair of graphs \(G,G^{}\), let the relation \(G^{} G\) denote that \(G^{}\) is isomorphic to some subgraph of \(G\). The following lemma demonstrates that removing random variables and dependencies between random variables can only reduce graph resilience.

**Lemma 4.4**.: _For a pair of graphs \(G\) and \(G^{}\), if \(G^{} G\), then \(r(G^{}) r(G)\)._

The following corollary follows directly from the previous Lemma 4.4 and Lemma 4.6, which we present later.

**Corollary 4.5**.: _Let \(G\) be a graph whose largest clique contains \(k\) vertices, then \(r(G) k\)._

### Example Graph Resiliences

From these properties of graph resilience we can derive the graph resilience of some example graphs. Some results from this section are summarized in Table 1. We begin with a couple of simple cases.

The following lemma demonstrates the surprising fact that, if even a single edge is missing from the graph, then the effective dimension of the estimation problem is strictly smaller than the ambient dimension \(d\).

**Lemma 4.6**.: _For a graph \(G=(V,E)\), \(r(G)=|V|\) if and only if \(G\) is a complete graph._

Analogously, a graph can only have resilience 1 if it is the empty graph.

**Lemma 4.7**.: _For a graph \(G=(V,E)\), \(r(G)=1\) if and only if \(E=\)._

The star graph is an example of a connected graph whose resilience is very small. Recall that a star graph \(S_{d}\) is a graph with a single central vertex connected to every other node, i.e. \(E=\{(i,j):j i\}\). Equivalently, it is (a) a complete bipartite graph with a single vertex in one partition or (b) a tree of depth one with a single root.

**Lemma 4.8**.: \(r(S_{d}) 2\)

   Graph \(G\) & Resilience \(r=r(G)\) & Sample complexity \(n\) \\  Trees (depth \(k\)) & \( k\) & \(^{-(k+2)}\) \\ Trees (general) & \(O((d))\) & \(^{-((d)+2)}\) \\ Paths & \(O((d))\) & \(^{-((d)+2)}\) \\ Grid & \(O()\) & \(^{-(+2)}\) \\ Complete graph & \(d\) & \(^{-(d+2)}\) \\   

Table 1: Example graph resiliences and corresponding sample complexitiesIn the remainder of this section, we describe additional examples of graphs whose resilience can be bounded. We focus on four broad classes of graphs, categorized by the application of interest: Hierarchical data (e.g. language, biology, phylogeny), sequential data (e.g. audio, video, language), spatial data (e.g. images, vision, signal processing), and clustered data (e.g. genetics, ecology, medicine). See Figure 1 for a guide to these classes of graphs.

The following definition will be helpful for analyzing graphs that have a linear or grid shape.

**Definition 4.1**.: For a graph \(G\) and \(n\), the power graph \(G^{n}\), is the graph which inherits its vertices from \(G\) and a pair of vertices in \(G^{n}\) are adjacent when their distance in \(G\) is at most \(n\).

#### 4.2.1 Hierarchical Data

Hierarchical data arises from distributions that have a tree-like structure, i.e. \(G\) is tree. In the worst-case, the resilience of a tree can scale at most logarithmically with \(d\), however, in practical applications where the tree has bounded depth the resilience is also bounded.

**Lemma 4.9**.: _Let \(G\) be a \(k\)-ary tree with depth \(m\), then \(r(G) m\)._

**Lemma 4.10**.: _Let \(G\) be a tree with \(d\) vertices, then \(r(G)_{2}(d)+1\)._

#### 4.2.2 Sequential Data

A path graph naturally represents sequential data. For a more realistic model one may assume that a vertex in a path graph is connected to all vertices within a given distance, so that nearby vertices are highly dependent, but far away indices are less dependent. Even with this assumption we see that, like tree graphs, the path graph's resilience only grows at rate \(O( d)\).

**Definition 4.2**.: The _path graph of length \(d\)_, denoted \(L_{d}\), is the graph \((V,E)\) with \(V=[d]\) and \(E=\{\{i,j\}|i-j|=1\}\).

**Proposition 4.11**.: _Recall that \(L_{d}\) is the path graph of length \(d\) as defined in Definition 4.2. For any \(s,t\), we have_

\[r(L_{t(2^{s}-1)}^{t}) st.\]

_Here, \(L_{t(2^{s}-1)}^{t}\) is the power graph of \(L_{t(2^{s}-1)}\) as defined in Definition 4.1._

This, along with Lemma 4.4, yields the following rate on graph resilience for path graphs.

**Corollary 4.12**.: _For any \(d\) and any constant \(t\), we have_

\[r(L_{d}^{t})=O(t d).\]

#### 4.2.3 Spatial Data

Image data is naturally represented via a grid graph. The following graph describes a \(k k^{}\) grid of vertices with every vertex connected connected to its vertical, horizontal, and diagonal, neighbors.

**Definition 4.3**.: The _grid graph of shape \(k k^{}\)_, denoted \(L_{k k^{}}\), is the graph \((V,E)\) with \(V=[k][k^{}]\) and \(E=\{\{(i,j),(i^{},j^{})(i,j)(i^{},j^{}),|i-i^{ }| 1,|j-j^{}| 1\}\).

**Proposition 4.13**.: _Recall that \(L_{k k}\) be the grid graph of shape \(k k\) as defined in Definition 4.3. For any \(s,t\), we have_

\[r(L_{t(2^{s}-1) t(2^{s}-1)}^{t}) 4t^{2}2^{s}.\]

_Here, \(L_{t(2^{s}-1) t(2^{s}-1)}^{t}\) is the power graph of \(L_{t(2^{s}-1) t(2^{s}-1)}\) as defined in Definition 4.1._

Examples of a grid graph and its power can be found in Figure 5 in the appendix. The previous proposition gives us a general rate of \(\) for grid graphs.

**Corollary 4.14**.: _For any \(k\) and any constant \(t\), we have_

\[r(L_{k k}^{t})=O(t^{2})d=k^{2}.\]

_Note that the graph \(L_{k k}^{t}\) has \(d\) vertices._

#### 4.2.4 Clustered Data

Another common type of structured data exhibits clusters: Variables in the same cluster are dependent, while variables in different clusters are independent. This type of structure can be modeled with disjoint cliques, where each clique represents a cluster or community. Let \(V_{i} V\) denote each cluster, with \(V_{i} V_{j}=\) and \(V=V_{1} V_{m}\) (i.e. \(\{V_{i}\}\) is a partition of \(V\)). Let \(d_{i}|V_{i}|\), \(G_{i} K_{d_{i}}\) be a clique (complete graph) on \(V_{i}\), and \(G=_{i}G_{i}\). Then it follows from Lemmas 4.6 and 4.1 that \(r(G)=_{i}d_{i}\). In particular, if \(d_{i}=O(1)\), then \(r(G)=O(1)\). Of course, this simply recovers the well-known result that a density that factorizes into a product of densities can be estimated at a rate that depends only on the largest factor. Using our graph resilience analysis we have that, for a stochastic block model, the graph resilience is bounded by the size of the blocks times the graph that encapsulates dependencies between the blocks.

**Lemma 4.15**.: _Let \(G^{}=([k],E^{})\) be a graph. Let \(G=(V,E)\) be a graph, such that there is a partition of \(V=_{i=1}^{k}V_{i}\), where, if \(\{v,v^{}\} E\) it follows that \(v V_{i}\) and \(v^{} V_{j}\) where \(i=j\) or \(i\) and \(j\) are adjacent in \(G^{}\). Then \(r(G) r(G^{})_{i[k]}|V_{i}|\)._

This has the useful interpretation that the effect of additional dependencies _between_ clusters does not interact with the clusters themselves. For example, if we allow for noisy interactions between clusters (e.g. as in a stochastic block model), the complexity scales separately with the noise and the size of the largest cluster.

### Comparison to other graphical properties

The examples in the previous section can be used to show that the classical graphical properties that one might expect to govern the sample complexity surprisingly fail to capture the sample complexity.

DegreeThe first and most obvious is the maximum degree of \(G\). The star graph \(S_{d}\) is an example where the resilience is \(O(1)\), and hence the sample complexity is \((1/)^{O(1)}\), but the maximum degree is \((d)\). Thus, the degree cannot properly capture the sample complexity. Moreover, the path graph shows the reverse: The maximum degree can be \(O(1)\) while the resilience is \(( d)\).

DiameterThe diameter \((G)\) of a graph \(G\) is the length of its longest geodesic path. A clique \(K_{d}\) thus has \((K_{d})=1\) (since every node is connected to every other node), but \(r(K_{d})=d\). It is clear even from classical results that a clique cannot be estimated any faster than \((1/)^{d}\). Thus, the diameter also cannot capture the sample complexity.

SparsityCall a density \(p\) on \(d\) variables \(s\)-sparse if \(p(x)=q(x_{S})\), where \(S[d]\) with \(|S|=s\). Now suppose \(p\) is \(s\)-sparse and \(q\) is Markov to a star graph \(S_{s}\) on \(s\) vertices. Then Theorem 3.1 implies that \(p\) can be estimated with \((1/)^{O(1)}\) samples (since \(r(G)=O(1)\)). Thus, taking \(s\), it follows that the sparsity \(s\) cannot properly capture the sample complexity.

In other words, not only does the graph resilience properly capture the sample complexity of structured density estimation, it is also not simply a proxy for commonly used graphical properties.

## 5 Conclusion

This work has introduced a new concept, _graph resilience_, and demonstrated that it controls the complexity of estimating densities satisfying the Markov property. This characterization has shown that estimating such densities can be significantly easier than previous works have indicated. This finding contributes to the broader understanding of graph theoretical properties in statistical estimation and, more generally, to nonparametric estimation. Although our approach sheds light on the intrinsic possibilities and barriers to breaking the curse of dimensionality, the development of practical methods remains an important open problem. For example, can neural networks achieve these rates? Recent concurrent work has provided insight into the use of neural density estimators, see e.g. Bos and Schmidt-Hieber (2023); Cole and Lu (2024); Vandermeulen et al. (2024). Finally, the concept of resilience is a new and unexplored property with significant potential for discovering graphs that yield good estimation properties.