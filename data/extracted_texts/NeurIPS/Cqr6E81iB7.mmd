# The Limits of Differential Privacy in Online Learning

Bo Li Wei Wang Peng Ye

Department of Computer Science and Engineering

The Hong Kong University of Science and Technology

Hong Kong SAR, China

bli@ust.hk, weiwa@cse.ust.hk, pyeac@connect.ust.hk

###### Abstract

Differential privacy (DP) is a formal notion that restricts the privacy leakage of an algorithm when running on sensitive data, in which privacy-utility trade-off is one of the central problems in private data analysis. In this work, we investigate the fundamental limits of differential privacy in online learning algorithms and present evidence that separates three types of constraints: no DP, pure DP, and approximate DP. We first describe a hypothesis class that is online learnable under approximate DP but not online learnable under pure DP under the adaptive adversarial setting. This indicates that approximate DP must be adopted when dealing with adaptive adversaries. We then prove that any private online learner must make an infinite number of mistakes for almost all hypothesis classes. This essentially generalizes previous results and shows a strong separation between private and non-private settings since a finite mistake bound is always attainable (as long as the class is online learnable) when there is no privacy requirement.

## 1 Introduction

Machine learning has demonstrated extraordinary capabilities in various industries, from healthcare to finance. Yet, it could raise serious privacy concerns as it may require access to a vast amount of personal data. The data used to train machine learning models may also contain sensitive information such as medical records or financial transactions. Therefore, it is crucial to ensure that the private data is well-protected during the training process.

Differential privacy (DP)  is a rigorous mathematical definition that quantifies the level of personal data leakage. In a nutshell, an algorithm is said to be _differentially private_ if the change of any individual's data won't make the output significantly different. DP has become the standard notion of privacy and has been broadly employed .

However, privacy is not a free lunch and usually comes at a cost. Simple tasks may become much harder or even intractable when privacy constraints are imposed. It is crucial to understand the cost to pay for privacy. For probably approximately correct (PAC) learning , which is the standard theoretical model of machine learning, there have been many works investigating the cost associated with privacy, which demonstrate a huge discrepancy in terms of the cost under non-private, pure private, and approximate private constraints.

One central requirement in PAC learning is that the data need to be i.i.d. generated and given in advance. Such assumptions fail to capture many scenarios in practice. For example, fraud detection in financial transactions often needs to be handled in real time, which prohibits access to the entire dataset. Moreover, fraudulent patterns can change over time, and new types of fraud can emerge. In such a scenario, the data are clearly not i.i.d. and can even be adaptive to the algorithm's prior predictions, in which the online learning model should be adopted.

Compared with private PAC learning, the limits of private online learning are less understood. For approximate DP, algorithms for Littlestone classes were proposed . But for pure DP, the only known result is the one for point functions given by Dmitriev et al. . They also suggested that one could leverage existing tools of DP continual observation [26; 34] to design algorithms for finite hypothesis classes and asked if generic learners can be constructed for infinite hypothesis classes.

Going beyond qualitative learnability, it is worth quantitatively exploring the number of mistakes made by an online learner. Without privacy, it is possible to achieve a mistake bound of at most the Littlestone dimension of the hypothesis class , which is independent of the total rounds \(T\). Therefore, the number of mistakes is always bounded as \(T\). However, all existing private online learning algorithms suffer from an error count that grows at least logarithmically with \(T\). It was asked by Sanyal and Ramponi  whether such a cost is inevitable for DP online learning. In a recent work of Cohen et al. , they showed that any private online learning algorithm for the class of point functions over \([T]\) must incur \(( T)\) mistakes. However, it remains open whether such cost is unavoidable for generic hypothesis classes, especially for those with a smaller cardinality.

### Main Results

We obtain results that separate three types of constraints: no DP, pure DP, and approximate DP.

Separation between pure and approximate DP.We first perform a systematic study of online learning under pure DP. We prove that every pure privately PAC learnable class is also pure privately online learnable against oblivious adversaries, answering a question raised by Dmitriev et al. . For the stronger adaptive adversaries, we obtain an impossibility result that the class of point functions over \(\), which can be pure privately learned in the offline model, is not online learnable under pure DP. According to the result of Golowich and Livni , it is online learnable under approximate DP. Thus, our conclusion reveals a strong separation between these two privacy definitions.

Separation between private and non-private settings.We next quantitatively investigate the dependence on \(T\) in the mistake bound. We show that for any hypothesis class \(\), any private online learning algorithm must make \(( T)\) mistakes unless \(\) contains only one single hypothesis or exactly two complementary hypotheses (see Section 4 for the definition). This largely generalizes previous results and indicates that such a separation indeed exists universally. We further improve the lower bound to \((() T)\), where \(()\) represents the Littlestone dimension of \(\).

To better demonstrate our results, we consider the task of online learning point functions over \(\) in the oblivious setting and summarize in Table 1 the finiteness of mistakes and learnability under the three types of constraints. Note that for this hypothesis class, the impossibility of making finite mistakes in private online learning can also be derived from the result in . However, our conclusion (Theorem 4.3) is more general - it applies to a much broader family of hypothesis classes. We choose this hypothesis class for illustration because it separates the learnability against adaptive adversaries under pure DP and approximate DP.

    &  &  \\   & Finite mistakes? & Learnable? & Learnable? \\   & ✓ & ✓ & ✓ \\  & () & () & () \\   & ✗ & ✓ & ✓ \\  & (Theorem 4.3) & () & () \\   & ✗ & ✓ & ✗ \\  & (Theorem 4.3) & (Theorem 3.3) & (Theorem 3.5) \\   

Table 1: Separation between three types of constraints

### Related Work

The work of  initialized the study of PAC learning with differential privacy. A series of subsequent works then showed that privacy constraints have a distinctive impact on the learners. The most remarkable result is the equivalence between approximate private learning and (non-private) online learning [4; 15; 5; 29]. For pure DP, the learnability is characterized by the so-called representation dimension . Both results suggest that private learning is strictly harder than non-private learning. For some specific hypothesis class such as the one-dimensional threshold over finite domain, it was shown that learning with approximate DP enjoys a much lower sample complexity than with pure DP [9; 19; 28; 13; 14; 37; 20], separating these two types of privacy. Another separation result is the huge gap between properly and improperly learning point functions with pure DP , which does not exist in non-private and approximate private settings.

The problem of private online learning Littlestone classes was first studied by Golowich and Livni . They proposed private online learning algorithms for hypothesis classes of finite Littlestone dimension in the realizable setting against oblivious and adaptive adversaries, further strengthening the connection between online learning and differential privacy. In contrast with the non-private setting where the mistake bound is always finite, their algorithms exhibit a cost of \( T\) for data streams of length \(T\). Recently, it was shown by Cohen et al.  that this extra cost is unavoidable for point functions over \([T]\). Dmitriev et al.  also obtained similar results, but only for algorithms that satisfy certain properties. It was questioned by Sanyal and Ramponi  whether an unbounded number of mistakes is necessary for \(\) (see Section 2.2 for the definition of \(\)).

There were also a great number of results on private parametric online learning tasks such as online predictions from experts (OPE) and online convex optimization (OCO) [36; 42; 35; 3; 8]. Most of them focus on the agnostic setting. Asi et al.  developed algorithms for both problems in the realizable regime with oblivious adversaries, again with a \( T\) overhead. Asi et al.  obtained some hardness results for DP-OPE against adaptive adversaries, but they require the number of experts to be larger than \(T\).

Another related field is differential privacy under continual observation (see, e.g., [26; 18; 34]). While the techniques can be used to design online learning algorithms, it is unclear whether lower bounds for DP continual observation can be transformed to any hardness results for private online learning (see  for a detailed discussion).

## 2 Preliminaries

Notation.Throughout this paper, we use \(S=\{z_{1},,z_{t}\}\) to denote a data stream of length \(T\). We write \(S[t]\) to denote the data point comes at time-step \(t\), i.e., \(z_{t}\). For an algorithm \(\) that runs on \(S\), we use \((S)_{t}\) to denote the output of \(\) at time-step \(t\).

### Online Learning

We start by defining online learning as a sequential game played between a learner and an adversary. Let \(0,1}^{}\) be a hypothesis class over domain \(\) and \(T\) be a positive integer indicating the total number of rounds. In the \(t\)-th round, the learner outputs a hypothesis \(h_{t}\{0,1\}^{}\) (not required to be in \(\)) while the adversary presents a pair \((x_{t},y_{t})\). The performance of the learner is measured by the expected _regret_, which is the expected number of additive mistakes made by the learner compared to the best (in hindsight) hypothesis in \(\):

\[[_{t=1}^{T}[h_{t}(x_{t}) y_{t}]-_{h^{ *}}_{t=1}^{T}[h^{*}(x_{t}) y_{t}]].\]

The above setting is usually referred to as the _agnostic_ setting, where we do not make any assumptions on the data. In the _realizable_ setting, it is guaranteed that there is some \(h^{*}\) so that \(y_{t}=h^{*}(x_{t})\) for all \(t[T]\). In this setting, the performance is directly measured by the expected number of mistakes made by the learner, which is called the _mistake bound_, defined as

\[[_{t=1}^{T}[h_{t}(x_{t}) y_{t}]].\]An online learning algorithm is considered successful if it attains a sublinear regret, i.e., the regret is \(o(T)\). In this paper, we mainly focus on the realizable setting. We say a hypothesis class \(\) is online learnable if there is an online learning algorithm for \(\) that makes \(o(T)\) mistakes in expectation.

We consider two types of adversaries: an _oblivious_ adversary chooses the examples in advance (could depend on the learner's strategy, but not on its internal randomness), and \((x_{t},y_{t})\) is revealed to the learner in the \(t\)-th round. An _adaptive_ adversary instead, can choose \((x_{t},y_{t})\) based on past history, i.e., \(h_{1},,h_{t-1}\) and \((x_{1},y_{1}),,(x_{t-1},y_{t-1})\).

Without privacy, the mistake bound is exactly characterized by the Littlestone dimension  even with stronger adversaries that can choose \((x_{t},y_{t})\) after seeing \(h_{t}\). To define the Littlestone dimension, we first introduce the notion of a shattered tree.

**Definition 2.1** (Shattered Tree).: Consider a full binary tree of depth \(d\) such that each node is labeled by some \(x\). Every \(\{y_{1},,y_{d}\}\{0,1\}^{d}\) defines a root-to-leaf path \(x_{1},,x_{d}\) obtained by starting at the root, then for each \(i=2,,d\) choosing \(x_{i}\) to be the left child of \(x_{i-1}\) if \(y_{i-1}=0\) and to be the right child otherwise. The tree is said to be shattered by \(\) if for every root-to-leaf path defined in this way, there exists \(h\) such that \(y_{i}=h(x_{i})\) for all \(i[d]\).

**Definition 2.2** (Littlestone Dimension).: The Littlestone dimension of \(\), denoted by \(()\), is the maximal \(d\) such that there exists a full binary tree of depth \(d\) that is shattered by \(\).

The problem of online prediction from experts can be viewed as a parametric version of online learning. Let \(d\) be the total number of experts. In the \(t\)-th round, the algorithm chooses an expert \(i_{t}[d]\) while the adversary selects a loss function \(_{t}:[d]\). Then \(_{t}\) is revealed and a cost of \(_{t}(i_{t})\) is incurred. The goal is to minimize the expected regret

\[[_{t=1}^{T}_{t}(i_{t})-_{i[d]}_{t=1}^{T} _{t}(i)].\]

Similar to online learning, an oblivious adversary chooses all \(_{t}\) in advance, while an adaptive adversary determines \(_{t}\) based on \(i_{1},,i_{t-1}\) and \(_{1},,_{t-1}\). In the realizable setting, it is guaranteed that there exists \(i^{}[d]\) such that \(_{t}(i^{})=0\) for all \(t[T]\).

### Differential Privacy

We first recall the standard definition of differential privacy.

**Definition 2.3** (Differential Privacy).: An algorithm \(\) is said to be \((,)\)-differentially private if for any two sequences \(S_{1}\) and \(S_{2}\) that differ in only one entry and any event \(O\), we have

\[[(S_{1}) O] e^{}[(S_{2}) O]+.\]

When \(=0\), we also say \(\) is \(\)-differentially private.

Our proofs use the packing argument , which heavily relies on the following property of DP.

**Fact 2.4** (Group Privacy).: _Let \(\) be an \((,)\)-differentially private algorithm. Then for any two sequences \(S_{1}\) and \(S_{2}\) that differ in \(k\) entries and any event \(O\), we have_

\[[(S_{1}) O] e^{k}[(S_{2}) O ]+-1}{e^{}-1}.\]

Privacy with _adaptive_ adversaries.When interacting with adaptive adversaries, the notion of differential privacy becomes a bit trickier.1 We adopt the definition of adaptive differential privacy from . Let \(\) be an online algorithm, \(\) be an adversary2 who generates two sequences \(S_{1}\) and \(S_{2}\) adaptively such that \(S_{1}\) and \(S_{2}\) differ in only one entry, and \(b\{1,2\}\) be a global parameter that is unknown to \(\) and \(\). The interactive process \((b)\) works as follows: in each time-step \(t\), \(\) generates two data points \(S_{1}[t],S_{2}[t]\) based on the past history and \(\) gets \(S_{b}[t]\). The output of \((b)\) is defined to be the entire output of \(\). We say \(\) satisfies \((,)\)-adaptive differential privacy if for any such adversary \(\) and any event \(O\), we have

\[[(1) O] e^{}[ (2) O]+.\]Choosing the privacy parameters.It is a commonly agreed principle that for the definition of differential privacy to be meaningful, the parameter \(\) should be much less than the inverse of the dataset size . In this paper, when we say an algorithm \(\) is private without specifying the privacy parameters, we typically refer to the set-up that \(\) is a small constant (say \(0.01\)) and \(=o(1/T)\).

## 3 Learning with Pure Differential Privacy

In this section, we study online learning under pure DP constraint. We first propose algorithms for privately offline learnable hypothesis classes against oblivious adversaries via a reduction to OPE using the tool of probabilistic representation. Then we turn to adaptive adversaries and present a hypothesis class that is privately offline learnable but not privately online learnable. Note that according to the results of , this class is online learnable under approximate DP with adaptive adversaries. Hence, we manifest a strong separation between pure and approximate DP.

### Learning Against Oblivious Adversaries

In this section, we consider an oblivious adversary. We first recall the notion of representation dimension, which was introduced by Beimel et al.  to characterize pure DP offline learnability. Let \(\) be a distribution over \(\{0,1\}\) and \(h\{0,1\}^{}\) be a hypothesis. The error of \(h\) with respect to \(\) is defined as \(_{}(h)=_{(x,y)}[h(x) y]\).

**Definition 3.1** (Representation Dimension).: A probability distribution \(\) over hypothesis classes is said to be an \((,)\)-probabilistic representation for \(\) if for any \(h^{}\) and any distribution \(\) over \(\{0,1\}\) that is labeled by \(h^{}\), we have

\[_{V}[ v V\ s.t.\ _{ }(v)] 1-.\]

Let \(()=_{V()} |V|\). The representation dimension of \(\), denoted by \(()\), is defined as

\[()=_{\ \ (1/4,1/4)\ }().\]

The following lemma from  shows that a constant probabilistic representation can be boosted to an \((,)\) one with logarithmic cost in \(1/\) and \(1/\).

**Lemma 3.2**.: _There exists an \((,)\)-probabilistic representation for \(\) with_

\[()=O((1/)(( )+(1/)+(1/))).\]

We first consider the realizable setting. Let \(S=\{(x_{1},y_{1}),,(x_{T},y_{T})\}\) be the sequence chosen by the adversary and \(_{S}\) be the empirical distribution of \(S\) (i.e., \(_{(x,y)_{S}}[(x,y)=(x_{t},y_{t})]=1/T\) for all \(t[T]\)). By sampling a hypothesis class \(V\) from an \((,)\)-probabilistic representation with \(1/<1/T\), we know that it holds with probability at least \(1-\) that \(_{_{S}}(v) 1/<1/T\) for some \(v V\). This further implies that \(v\) is consistent with all examples in \(S\). By Lemma 3.2, \(V\) is finite as long as \(\) has a finite representation dimension. Thus, it suffices to run the DP-OPE algorithm in  with every \(v V\) as an expert.

**Theorem 3.3**.: _Let \(\) be a hypothesis class with \(()<\). In the realizable setting, there exists an online learning algorithm that is \(\)-differentially private and has an expected mistake bound of \(O(T(()+ T)^{2}}{ })\) with an oblivious adversary._

The above conclusion directly extends to the agnostic setting by replacing the DP-OPE algorithm with an agnostic one .

**Theorem 3.4**.: _Let \(\) be a hypothesis class with \(()<\). In the agnostic setting, there exists an online learning algorithm that is \(\)-differentially private and achieves an expected regret of \(O( T(()+ T)}{ })\) with an oblivious adversary._

Note that every online learning algorithm can be transformed to a PAC learner by the online-to-batch conversion . Our result reveals that pure private online learnability against oblivious adversaries is equivalent to pure private PAC learnability in both realizable and agnostic settings.

### Learning Against Adaptive Adversaries

We now turn to adaptive adversaries. For finite hypothesis classes, it is still feasible to employ techniques from DP-OPE  or DP continual observation  to devise online learning algorithms (in Appendix F, we give an algorithm with a better mistake bound in the realizable setting). One may hope that this can be extended to hypothesis class with finite representation dimension, as we did in the oblivious setting. However, it turns out that our method for oblivious adversaries is not applicable here. Since the examples are not fixed in advance, we cannot guarantee that the sampled hypothesis class \(V\) contains a consistent hypothesis. Moreover, the famous oblivious-to-adaptive transformation (see, e.g., ), which was used by Golowich and Livni  to construct online learners under approximate DP, also fails to give a sublinear mistake bound. This is because pure DP only has the basic composition property, which yields a mistake bound that scales linearly with \(T\) (for approximate DP, this can be improved to \(\) by advanced composition). Therefore, it is not clear if every offline learnable hypothesis class can also be made online learnable against adaptive adversaries under pure DP.

We will show that this is an impossible mission. Let \(_{d}\) be the set of point functions over \([d]\) and \(_{}\) be the set of point functions over \(\), where a point function \(f_{x}:\{0,1\}\) is a function that maps \(x\) to \(1\) and all other elements to \(0\). Both \(_{d}\) and \(_{}\) have a constant representation dimension and thus are offline learnable under pure DP . In the rest of this section, we will prove that for any pure DP online learning algorithm for \(_{d}\), an adaptive adversary can force it to make \((( d,T))\) errors. As a direct corollary, \(_{}\) is not pure privately online learnable against adaptive adversaries.

We now illustrate the idea of our proof. Let us start by considering a simplified version, where the algorithm is constrained to be proper, i.e., \(h_{t}=_{d}\) for every \(t[T]\). Then one can construct a series of data streams \(S_{i}=\{(i,1),,(i,1)\}\) for every \(i[d]\). An accurate proper learner must output \(f_{i}\) for most of the rounds. This allows us to use the packing argument to derive an \(( d)\) lower bound for \(T=( d)\).

However, the above argument does not apply to the general case where the learner may be improper since a learner can simply output an all-one function that makes \(0\) errors on each \(S_{i}\). Therefore, we have to insert to \(S_{i}\) some examples of the form \((j,0)\) where \(j i\). This prevents \(h_{t}\) from taking \(1\) on elements other than \(i\). But when should we insert \((j,0)\)? And how do we determine the value of \(j\)?

Note that till now, we have not used the adversary's adaptivity. It is necessary to exploit this power since any oblivious construction can be solved by our algorithm in Theorem 3.3. When the adversary acts adaptively, the construction becomes a dual online learning game: in each round, the learner outputs \(h_{t}\) as a "data point" and the adversary chooses \((i,1)\) or some \((j,0)\) as the "hypothesis". This inspires us to leverage tools from online learning to construct the adversary.

We now sketch our idea. In each round, we choose \((i,1)\) as the data point with probability \(1/2\), and otherwise sample a \((j,0)\) from some probability distribution. We maintain the distribution by the multiplicative update rule, which is a widely used method in online decision making. The weight of \(j\) is increased by a multiplicative factor whenever \(h_{t}(j)=1\), and the probability of selecting \(j\) is proportional to its weight. We provide a detailed implementation in Algorithm 1.

Using the standard argument of multiplicative update, we can show that an accurate learner must predict \(h_{t}(i)=1\) for most rounds and \(h_{t}(j)=1\) for very few rounds. This allows us to apply the packing argument to obtain the following hardness result.

**Theorem 3.5**.: _Let \( O(1)\) and \(d 2\). Any \(\)-differentially private online learning algorithm for \(_{d}\) must incur a mistake bound of \((( d/,T))\) in the adaptive adversarial setting._

Since \(_{d}\) is a subset of \(_{}\) for any \(d\), the above result directly implies that \(_{}\) is not online learnable with adaptive adversaries under pure DP. This shows a strong separation between pure DP and approximate DP.

**Corollary 3.6**.: _Let \( O(1)\). In the adaptive adversarial setting, any \(\)-differentially private online learning algorithm for \(_{}\) must make \((T)\) mistakes._``` Input: the number of rounds \(T\); online learning algorithm \(\); input data stream \(S\) Output: hypotheses \(h_{1},,h_{T}\) outputted by \(\)
1\(w_{0}(j) 1\) for all \(j[d]\)
2for\(t=1,,T\)do
3\((x_{t},y_{t}) S[t]\)
4 Set \(p(j)(j)}{_{i[d](x_{t})}w_{t-1}(k)}\) for \(j[d]\{x_{t}\}\)
5 With probability 1/2, sample \(j p\) and set \((x_{t},y_{t})(j,0)\)
6 Present \((x_{t},y_{t})\) to \(\) and receive \(h_{t}\) from \(\)
7 Update \(w_{t}(j)=w_{t-1}(j) e^{h_{t}(j)}\) for all \(j[d]\)
8
9 end for return\(h_{1},,h_{T}\) ```

**Algorithm 1**Adaptive adversary for POINT\({}_{d}\)

## 4 A General Lower Bound on the Number of Mistakes

In this section, we prove an \((() T)\) lower bound on the number of mistakes made by any private learner for every hypothesis class \(\) that contains a pair of non-complementary hypotheses.3 This implies that as \(T\), any private algorithm will make an infinite number of mistakes. Note that without privacy, the Standard Optimal Algorithm always makes at most \(()\) mistakes . Thus, our lower bound reveals a universal separation between non-private and private models.

Our proof proceeds in two steps. We first show an \(( T)\) lower bound in Section 4.1. Then based on this result, we prove the \((() T)\) lower bound in Section 4.2.

### A Lower Bound for Non-complementary Hypotheses

We first define the notion of complementary hypotheses.

**Definition 4.1**.: We say two different hypotheses \(f_{1}\) and \(f_{2}\) over \(\) are complementary if \(f_{1}(x)=1-f_{2}(x)\) for all \(x\). Otherwise we say they are non-complementary.

It is worth noticing the following important fact about non-complementary hypotheses, where the first item directly comes from the above definition and the second is because \(f_{1}\) and \(f_{2}\) are different.

**Fact 4.2**.: _Let \(f_{1}\) and \(f_{2}\) be two different hypotheses over \(\) that are non-complementary. Then:_

1. _There exists some_ \(u_{0}\) _such that_ \(f_{1}(u_{0})=f_{2}(u_{0})\)_;_
2. _There exists some_ \(u_{1}\) _such that_ \(f_{1}(u_{1}) f_{2}(u_{1})\)_._

We remark that this fact is also used by Dmitriev et al.  to prove a lower bound (in their work, they call it a "distinguishing tuple"). However, they make a strong assumption that when running on a data stream containing \((u_{0},f_{1}(u_{0}))\) only, with high probability, the algorithm predicts \(h_{t}(u_{1})=f_{1}(u_{1})\) simultaneously for all \(t[T]\). This largely weakens their bound since most DP algorithms clearly do not have such property.

To see how to use Fact 4.2, consider a hypothesis class that contains a pair of non-complementary hypotheses. We will focus on \(u_{0},u_{1}\) and \(f_{1},f_{2}\) only and ignore all other elements and hypotheses. In our proof, we will use \((u_{0},f_{1}(u_{0}))=(u_{0},f_{2}(u_{0}))\) as a dummy input that provides no information about which hypothesis is correct. Let \(S_{0}\) be a sequence that contains the dummy input only and \(\) be an online learning algorithm. Without loss of generality, we can assume that \([(S_{0})_{t}(u_{1})=f_{1}(u_{1})] 1/2\) for all \(t[T]\) (we can make this hold for half of the rounds by swapping \(f_{1}\) and \(f_{2}\), and ignore the rounds that it does not hold). We will insert \((u_{1},f_{2}(u_{1}))\)'s to make algorithm error.

Our proof relies on the classical packing argument. For ease of presentation, we only consider pure DP here, but the proof strategy easily extends to approximate DP via group privacy under approximate DP. In the framework of packing argument, we will construct a series of input sequences \(S_{1},,S_{m}\)from \(S_{0}\) and disjoint subsets of output \(O_{1},,O_{m}\) such that \(S_{0}\) and \(S_{i}\) differ by at most \(k\) elements for every \(i[m]\), and any algorithm will make \((k)\) mistakes on \(S_{i}\). Then by group privacy, for any \(\)-differentially private algorithm \(\) we have

\[1_{i=1}^{m}[(S_{0}) O_{i}] e^{-k} _{i=1}^{m}[(S_{i}) O_{i}].\]

Thus, a lower bound on \([(S_{i}) O_{i}]\) implies a lower bound on \(k\) by the above inequality.

The first challenge here is the construction of \(S_{i}\). By our assumption, we can insert a \((u_{1},f_{2}(u_{1}))\) at any position of \(S_{0}\) to cause a loss of \(1/2\). However, when inserting the second one, the loss may decrease by a multiplicative factor of \(e^{}\). Following this argument, no matter how many \((u_{1},f_{2}(u_{1}))\)'s are inserted, we can only bound the expected number of mistakes by

\[(1+e^{-}+e^{-2}+)=constant,\]

failing to give an \((k)\) bound for \(k= T\).

We overcome this by constructing them according to the given algorithm \(\) instead of arbitrary algorithms. We will assume \(\) has a mistake bound of \(O( T)\) and seek to derive a contradiction. We now depict our construction. For \(S_{1}\), we let \(S=S_{0}\) be the initial data stream. We then go through every \(t[T]\) in an increasing order, insert a \((u_{1},f_{2}(u_{1}))\) at time-step \(t\) whenever \([(S)_{t}(u_{1})=f_{1}(u_{1})] 1/3\), and let \(S_{1}=S\) at the end. By our assumption, the number of \((u_{1},f_{2}(u_{1}))\)'s should not exceed \(k=3 O( T)\). Hence, \(S_{1}\) and \(S_{0}\) differ by at most \(k=O( T)\) points. Moreover, by our construction, for each \(t[T]\) such that \(S_{1}[t]=(u_{0},f_{1}(u_{0}))\), we must have \([(S_{1})_{t}(u_{1})=f_{1}(u_{1})]<1/3\).

Now let us construct \(S_{2}\). We find the earliest round \(t_{1}\) such that \([(S_{1})_{t_{1}}(u_{1})=f_{1}(u_{1})]<1/3\). The property we mentioned above ensures the existence of such \(t_{1}\) as long as \(k<T\). We then perform a similar procedure as in the construction of \(S_{1}\), but instead of starting from \(t=1\) and going over the entire time span \([T]\), we start from \(t=t_{1}\). The online nature of \(\) allows us to use \(t_{1}\) to distinguish \(S_{1}\) and \(S_{2}\) (as well as \(S_{3},,S_{m}\), which we will construct later) since

\[[(S_{1})_{t_{1}}(u_{1})=f_{1}(u_{1})]<1/3<1/2[ (S_{2})_{t_{1}}(u_{1})=f_{1}(u_{1})].\]

In other words, \(\) is more likely to predict \(h_{t_{1}}(u_{1})=f_{1}(u_{1})\) on \(S_{2}\) but is less likely to do so on \(S_{1}\).

We repeat the construction for \(i=3,,m\). For each \(i\), we first identify the minimal \(t_{i-1}\) such that \([(S_{j})_{t_{i-1}}]<1/3\) for every \(j<i\). Then we insert \((u_{1},f_{2}(u_{1}))\)'s starting from \(t=t_{i-1}\). By the same argument, \(t_{i-1}\) can be used to distinguish \(S_{1},,S_{i-1}\) and \(S_{i},,S_{m}\). We formally describe the construction procedure in Algorithm 2.

At the end, we will have \(m\) sequences \(S_{1},,S_{m}\) and \(m-1\) time-steps \(t_{1},,t_{m-1}\) such that \([(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})]<1/3\) for any \(j i\) and \([(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})] 1/2\) for any \(j<i\). It can be proved that \(m=(T/k)\), which is sufficiently large for \(k=O( T)\). Now we run \(\) on some \(S=S_{i}\). Suppose we can figure out the index \(i\), we can apply the packing argument to derive an \(( m)=( T)\) lower bound.

Here comes the second challenge. Though we can use the output of \(\) to estimate \([(S)_{t_{j}}(u_{1})=f_{1}(u_{1})]\) for a given \(j\), we only have a constant success probability. To make the estimate accurate for every \(j[m-1]\), one has to achieve a success probability of \(1-1/m\) for each \(j\). This requires running \(\) for \(O( m)=O( T)\) times and taking the average, which is prohibited since the resulting algorithm would be \(O( T)\)-DP, yielding a meaningless \((1)\) lower bound.

We address this issue by using binary search. We start with \(\{t_{1},,t_{m-1}\}\) and select the middle point \(t_{mid}\) in each iteration. By averaging over multiple copies of \((S)\), we can figure out whether we should go left or go right. This can be done in \(O( m)=O( T)\) iterations, and we only require the decision made on each middle point to be correct. Thus, the number of independent copies can be reduced to \(O( T)\), which leads to a lower bound of \(( T/ T)\).

The above approach is already sufficient to show an unbounded number of mistakes, but we can further refine our method to achieve an \(( T)\) bound. The key observation here is that we do not need the probability of outputting \(i\) on \(S_{i}\) to be a constant. In fact, a success probability of \(1/m^{1-(1)}\) is enough to obtain \(k((m/m^{1-(1)}))=( T)\).

We thus "smooth" our binary search. In each iteration, instead of going left or right deterministically, we go to the side that is more likely to be correct with some probability \(p>1/2\). We show that, by choosing \(p\) appropriately, this approach will output \(i\) on \(S_{i}\) with probability \(1/m^{1-(1)}\). Moreover, it only requires running the online learning algorithm \(O(1)\) times, avoiding the \( T\) blow-up of privacy parameters. The \(( T)\) lower bound then follows by applying the packing argument. We illustrate this approach in Algorithm 3.

``` Input: the number of rounds \(T\); online learning algorithm \(\); threshold \(k\); \(f_{1},f_{2}\) and \(u_{0},u_{1}\) Output: a single data stream \(S_{i}\), or a collection of \(m\) data streams \(S_{1},,S_{m}\) along with \(m-1\) time-steps \(t_{1},,t_{m-1}\)
1\(S_{0}\{(u_{0},f_{1}(u_{0})),,(u_{0},f_{1}(u_{0}))\}\)
2\(m T/k\)
3for\(i=1,,m\)do
4\(S_{i} S_{0}\)
5 Find the smallest \(t_{i-1}\) such that \( j[i-1],[(S_{j})_{t_{i-1}}(u_{1})=f_{1}(u_{1})]<1/3\)
6for\(t=t_{i-1},,T\)do
7if\([(S_{i})_{t}(u_{1})=f_{1}(u_{1})] 1/3\)then
8\(S_{i}[t](u_{1},f_{2}(u_{1}))\)
9 end for
10
11 end for
12if\(|\{t[T]:S_{i}[t]=(u_{1},f_{2}(u_{1}))\}|>k\)then
13return\(S_{i}\)
14 end if
15
16 end for return\(S_{1},,S_{m}\) and \(t_{1},,t_{m-1}\) ```

**Algorithm 2**Constructing \(S_{0},S_{1},,S_{m}\) and \(t_{1},,t_{m-1}\)

**Theorem 4.3**.: _Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \(( T/)\) even in the oblivious adversarial setting._

One may ask whether the existence of a non-complementary pair is a necessary condition for the number of mistakes to be unbounded. Note that there are only two cases that \(\) contains no non-complementary pairs: either \(||=1\) or \(=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). The former is definitely online learnable with zero mistakes. For the latter one, we give an algorithm with a finite expected mistake bound in Appendix F, showing that the condition is indeed necessary and sufficient.

``` Input: the number of rounds \(T\); online learning algorithm \(\); time-steps \(t_{1},,t_{m-1}\); input data stream \(S\{S_{1},,S_{m}\}\); \(f_{1},f_{2}\) and \(u_{0},u_{1}\) used in Algorithm 2 Output: an index \(i[m]\)
1 Run \(\) on \(S\) for \(360\) times, obtain \(360\) copies of output \(\{h_{1}^{(w)},,h_{T}^{(w)}\}\) for \(w\)
2\(l 1\), \(r m\)
3while\(l<r\)do
4\(mid\)
5if\(|\{h_{mid}^{(w)}(u_{1})=f_{1}(u_{1}):w\}|<150\)then
6 Let \(r mid\) with probability \(3/4\), and \(l mid+1\) otherwise
7else
8 Let \(l mid+1\) with probability \(3/4\), and \(r mid\) otherwise
9
10 end if
11return\(l\) ```

**Algorithm 3**Distinguishing \(S_{1},,S_{m}\)

**Theorem 4.4**.: _Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \(( T/)\) even in the oblivious adversarial setting._

Proof.: Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \(( T/)\) even in the oblivious adversarial setting.

One may ask whether the existence of a non-complementary pair is a necessary condition for the number of mistakes to be unbounded. Note that there are only two cases that \(\) contains no non-complementary pairs: either \(||=1\) or \(=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). The former is definitely online learnable with zero mistakes. For the latter one, we give an algorithm with a finite expected mistake bound in Appendix F, showing that the condition is indeed necessary and sufficient.

**Theorem 4.5**.: _Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \(( T/)\) even in the oblivious adversarial setting._

Proof.: Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \(( T/)\) even in the oblivious adversarial setting.

One may ask whether the existence of a non-complementary pair is a necessary condition for the number of mistakes to be unbounded. Note that there are only two cases that \(\) contains no non-complementary pairs: either \(||=1\) or \(=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). The former is definitely online learnable with zero mistakes. For the latter one, we give an algorithm with a finite expected mistake bound in Appendix F, showing that the condition is indeed necessary and sufficient.

**Theorem 4.6**.: _Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \(( T/)\) even in the oblivious adversarial setting._

Proof.: Let \(c(0,1)\) be some constant. Suppose \( T/T^{1-c}\) and \(/T\).

### Incorporating the Littlestone Dimension

Building upon the \(( T)\) lower bound, we are now ready to show an \((() T)\) lower bound for general hypothesis classes. Let \(\) be a private online learning algorithm for \(\). Consider a shattered tree of depth \(() 2\). Let \(u_{0}\) denote its root and \(u_{1}\) be its left child. By the definition of shattered tree, there exists \(f_{1},f_{2}\) such that \(f_{1}(u_{0})=f_{2}(u_{0})=0\) and \(0=f_{1}(u_{1}) f_{2}(u_{1})=1\). Note that \(f_{1},f_{2}\) and \(u_{0},u_{1}\) satisfy the property mentioned in Fact 4.2. We can thus apply Theorem 4.3 to find a sequence \(S_{1}\) of length \(T^{}\) on which \(\) makes \(( T^{})\) mistakes.

Till now, only the true labels of \(u_{0}\) and \(u_{1}\) are revealed to the learner. Therefore, we can go into the corresponding subtree of \(u_{1}\) and reiterate the above operation. After repeating it \(()/2\) times, we obtain a series of completely non-overlapping sequences \(S_{1},,S_{()/2}\) and on any one of them \(\) makes \(( T^{})\) mistakes. By concatenating them together and setting \(T^{}=T/()\), we arrive at the \((() T)\) lower bound assuming \(T>()^{1+c}\).

**Theorem 4.4**.: _Let \(c_{1}(0,1)\) and \(c_{2}>0\) be two constants. Suppose \( T/T^{(1-c_{1})c_{2}/(1+c_{2})}\) and \(/T\). If \(\) is a hypothesis class that contains two non-complementary hypotheses, then any \((,)\)-differentially private online learning algorithm for \(\) must incur a mistake bound of \((() T/)\) even in the oblivious adversarial setting given that \(T>()^{1+c_{2}}\)._

Note that the class of all hypotheses over \([d]\) has a Littlestone dimension of \(_{2}d\). The above theorem directly implies the following lower bound for the OPE problem. This improves the lower bound in  by a \( T\) factor.

**Corollary 4.5**.: _Let \(c_{1}(0,1)\) and \(c_{2}>0\) be two constants. Suppose \( T/T^{(1-c_{1})c_{2}/(1+c_{2})}\) and \(/T\). In the realizable setting, any \((,)\)-differentially private algorithm for OPE has a regret of \(( d T/)\) even against oblivious adversaries given that \(T>_{2}d^{1+c_{2}}\)._

### Comparing to the Upper Bounds

We have shown an \(_{}( T)\) lower bound on the number of mistakes made by any private online learning algorithm. We now compare it to existing upper bounds.

For pure DP, we provide an upper bound of \(O_{}(^{2}T( T)^{2})\). This is larger than our lower bound by a factor of \( T( T)^{2}\). In Appendix F, we show that \(O_{}( T)\) is achievable for some specific hypothesis classes. Whether \(O_{}( T)\) is attainable for generic hypothesis classes remains open.

For approximate DP, Golowich and Livni  proposed an algorithm with \(O_{}( T)\) mistakes against oblivious adversaries. Thus, our lower bound is tight assuming a constant Littlestone dimension. However, their algorithm exhibits an \(O_{}()\) upper bound against upper bound against adaptive adversaries. Whether this can also be improved to \(O_{}( T)\) is an interesting open question.