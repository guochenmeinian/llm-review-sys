# MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction

Jiahe Chen\({}^{1,2*}\)  Jinkun Cao\({}^{3*{}}\)  Dahua Lin\({}^{4,2,5}\)  Kris Kitani\({}^{3}\)  Jiangmiao Pang\({}^{2{}}\)

\({}^{1}\) Zhejiang University \({}^{2}\)Shanghai AI Laboratory \({}^{3}\)Carnegie Mellon University

\({}^{4}\)The Chinese University of Hong Kong \({}^{5}\)CPII under InnoHK

*: co-first authors \({}\): co-corresponding authors

###### Abstract

To predict future trajectories, the normalizing flow with a standard Gaussian prior suffers from weak diversity. The ineffectiveness comes from the conflict between the fact of asymmetric and multi-modal distribution of likely outcomes and symmetric and single-modal original distribution and supervision losses. Instead, we propose constructing a mixed Gaussian prior for a normalizing flow model for trajectory prediction. The prior is constructed by analyzing the trajectory patterns in the training samples without requiring extra annotations while showing better expressiveness and being multi-modal and asymmetric. Besides diversity, it also provides better controllability for probabilistic trajectory generation. We name our method Mixed Gaussian Flow (MGF). It achieves state-of-the-art performance in the evaluation of both trajectory alignment and diversity on the popular UCY/ETH and SDD datasets. Code is available at https://github.com/mulplue/MGF.

## 1 Introduction

In this work, we aim to improve the diversity for probabilistic trajectory prediction. In trajectory prediction, diversity describes the fact that agents (pedestrians) can move in different directions, speeds, and interact with other agents. Because the motion intentions of agents can not be determined by their historical positions, there is typically no global-optimal strategy to predict a single outcome of future trajectories. Therefore, recent works have focused on probabilistic methods to generate multiple likely outcomes. However, existing solutions are argued to lack good diversity and they often fail to generate the under-represented future trajectory patterns in the training data.

Different motion patterns are usually imbalanced in a dataset. For example, agents are more likely to move straight than turn around in most datasets. Thus, many motion patterns are highly under-represented though discoverable. Therefore, intuitively, an ideal distribution to represent the possible future trajectories should be asymmetric, multi-modal, and expressive to represent long-tailed patterns.

However, most existing generative models solve the problem of trajectory prediction by modeling it as a single-modal and symmetric distribution, i.e., standard Gaussian. This is because the standard Gaussian is tractable and there is a belief that it can be transformed into any desired distribution of the same or a lower dimension. However, deriving a desired complex distribution from a simple and symmetric prior distribution is challenging, especially with limited and imbalanced data. Moreover, when we derive the target distribution by transforming from the tractable original distribution as Normalizing Flows, GANs, and VAEs do, a dilemma arises: an over-smoothing transformation model can neglect under-represented samples while an over-decorated transformation model will overfit. Especially for normalizing flow, some studies discussed the difficulty of training normalizing flow in practice to represent a complex target distribution.

To solve this dilemma, we propose a prior distribution with more expressiveness and data-driven statistics. It is asymmetric, multi-modal, and adaptive to the training data in the form of a mixed set of Gaussians. Compared to the standard Gaussian, the mixture of Gaussians can better summarize the under-represented samples scattered far away in the representation space. This relieves the sparsity issue of rare cases and thus enhances the diversity of the generated outcomes. Besides diversity, asthe mixed Gaussian prior is parametric and transparent during construction, we could control the generation by manipulating this prior, such as adjusting the weights of different sub-Gaussians or manipulating the mean value of them. All these manipulations change generation results statistically without requiring fine-tuning or other re-training. Upon the prior distribution, we choose to construct the generative model by normalizing flow with the unique advantage of being invertible. We thus could estimate the likelihood of each single generated outcome. By combining the designs, we propose a normalizing flow-based trajectory prediction model named Mixed Gaussian Flow (MGF). It enjoys better diversity, controllability, interruptibility, and invertibility for trajectory prediction. During our study, we find that though several evaluation tools have been proposed for measuring diversity[49; 62; 33; 37], they employ varying calculation method and have not gained widespread adoption within the research community. The most popular evaluation metrics (APD/FDE scores) focus on how similar a generated trajectory is to the single ground truth. It is calculated in a "best-of-\(M\)" fashion where only one candidate in a batch of \(M\) predictions is taken into the measurement. This protocol encourages the methods to generate outcomes approaching the mean (most likelihood) of the learned distribution and provides no sense of the diversity of generation outcomes. Therefore, building upon previous research in the field of human motion prediction, we formulate a metric set of Average Pairwise Displacement (APD) and Final Pairwise Displacement (FPD), which measure the diversity of a batch of \(M\) generated samples. This helps us to have a concrete study about generation diversity and avoid bias from the "best-of-\(M\)" evaluation protocol. With the proposed metrics, we demonstrate that the proposed architecture design improves the diversity of generated trajectories. Still, we estimate the "best-of-\(M\)" candidate's alignment with the ground truth under widely adopted APD/FDE metrics. Surprisingly, MGF also achieves state-of-the-art performance.

To conclude, In this work, we focus on enhancing the diversity of trajectory prediction. We propose Mixed Gaussian Flow (MGF) by reforming the prior distribution of normalizing flows as a novel design of mixed Gaussians. It achieves state-of-the-art performance with respect to both the "best-of-\(M\)" alignment metrics and diversity metrics. We demonstrate that the proposed MGF model is capable of diverse and controllable trajectory predictions.

## 2 Related Works

**Generative Models for Trajectory Prediction.** Trajectory prediction aims to predict the positions in a future horizon given historical position observations of multiple participants (agents). Early studies solve the problem by deterministic trajectory prediction  where Social forces , RNNs [1; 35; 52], and the Gaussian Process  are proposed to model the agent motion intentions. Recent works mostly seek multi-modal and probabilistic solutions for trajectory prediction instead, which is a more challenging but faithful setting. Though some of them leverage reinforcement learning [25; 4], the mainstream uses generative models to solve the problem as generating likely future trajectories. Auto-encoder  and its variants, such as CVAE [23; 63], are widely adopted. GANs make another line of work . More recently, the diffusion  model is also used in this area . However, they are typically not capable of estimating outcome probability as the generation process is not invertible. Normalizing flow  is preferred in many cases for being invertible.

**Normalizing Flow for Trajectory Prediction.** In this work, we would like the predicted trajectories diverse and controllable. We prefer the generation process invertible to allow tractable generation likelihood. We thus choose normalizing flow  generative models. Normalizing flow  constructs complex distributions by transforming a probability density through invertible mappings

Figure 1: Non-invertible generative models (a), e.g., CVAE, GAN, and diffusions, lack the invertibility for probability density estimation. Flow-based methods (b) are invertible while, sampling from the symmetric standard Gaussian, undermines the diversity and controllability of generation. Our proposed Mixed Gaussian flow (c) maps from a mixed Gaussian prior instead. Summarizing distributions from data and controllable edits, it achieves better **diversity** and **controllability** for trajectory prediction.

from tractable distribution. Normalizing flow has been studied for trajectory prediction in some previous works [40; 41; 12]. In the commonly adopted evaluation protocol of "best-of-\(M\)" trajectory candidates, normalizing flow-based methods are once considered not capable of achieving state-of-the-art performance. However, we will show in this paper that with proper design of architecture, normalizing flow can be state-of-the-art. And much more importantly, its invertibility allows more controllable and explainable trajectory prediction.

Gaussian Mixture models as prior.Though the standard Gaussian is chosen by mainstream generative models as the original sampling distribution, some previous works explored how Gaussian mixture models (GMM) can be an alternative to help with generation or classification tasks.  uses a GMM prior in VAE models to enhance the clustering and classification performance.  adopts GMM to enhance the conditional generation of GAN networks. FlowGMM  uses GMM as the prior for flow-based models to deal with the classification task in a semi-supervised way. A recent work PluGen  proposes to mix two Gaussians to model the conditional presence of two binary labels to control generation tasks. Existing methods mostly use GMM to describe the presence of multiple auxiliary labels and they typically require additional annotations to construct the GMM. In this work, we use GMM as the distribution prior for normalizing flows without requiring any label annotations. It is designed to enhance the diversity of the generation and relieve the difficulty of learning transforming the tractable prior distribution to the desired complex and multi-modal target distribution for future trajectory generation.

## 3 Method

Our proposed method is based on the normalizing flow paradigm for invertible trajectory generation while we construct a mixed Gaussian prior as the original distribution instead of the naive standard Gaussian to allow more diverse and controllable outcomes. In this section, we first provide the formal problem formulation in Section 3.1. Then we introduce normalizing flow in Section 3.2 and the proposed Mixed Gaussian Flow (MGF) model in Section 3.3. We detail the training/inference implementations in Section 3.4. At last, we introduce the proposed metrics set to measure the diversity of generated trajectories in Section 3.5. The overall illustration of MGF is shown in Figure 2.

### Problem Formulation

We focus on 2D agent trajectory forecasting and represent the agent positions by 2D coordinates. Given a set of multiple agents, i.e., pedestrians in our case, we denote the 2D position of an agent \(a\) at time \(t\) as \(_{t}^{a}\) and a trajectory from \(t_{i}\) to \(t_{j}(t_{i}<t_{j})\) as \(_{t_{i}:t_{j}}^{a}\). Given a fixed scene with map \(\) and a period \(\): \(t_{0},t_{1},t_{2},...,t_{c},...,t_{T}\), there are \(N\) agents that have appeared during the period \(\), denoted as \(A_{t_{0}:t_{T}}=\{a_{0},a_{1},...,a_{N-1}\}\). Without loss of generality, given a current time step \(t_{c}(t_{0},t_{T})\), the task of trajectory prediction aims to obtain a set of likely trajectories \(_{t_{c}:t_{T}}^{a}\) with the past trajectories of all observed agents \(_{t_{0}:t_{c}}^{A_{t_{0}:t_{c}}}=\{_{t_{0}:t_{c}}^{a},a  A_{t_{0}:t_{c}}\}\) as input, where \(a\) is an arbitrary agent that has shown up during \(t:t_{0} t_{c}\). For each agent \(a A_{t_{0}:t_{c}}\) we seek to sample plausible and likely trajectories of it over the remaining time steps \(t_{c} t_{T}\) by a generative model \(\), i.e.,

\[}_{t_{c}:t_{T}}^{a}=(_{t_{0}:t_{c}}^{A_{t_{0}:t _{c}}}),\] (1)

at the same time, when there are other variables such as the observations of the maps are provided, we can use them as additional input information. By denoting the observations until \(t\) as \(_{t_{0}:t_{c}}\) we have

\[}_{t_{c}:t_{T}}^{a}=(_{t_{0}:t_{c}}^{A_{t_{0}:t _{c}}};_{t_{0}:t_{c}}).\] (2)

If the generation process is probabilistic instead of deterministic, the outcome of the solution is a set of trajectories instead of a single one. The formulation thus turns to

\[\{^{(i)}}_{t_{c}:t_{T}}^{a}\}=(_{t_{0}:t_{c}}^{A _{t_{0}:t_{c}}};_{t_{0}:t_{c}}),\] (3)

where \(i\) is the index of one candidate in the predicted batch.

For some generative models relying on transforming from a sample point in a known distribution \(_{0}\) to the target distribution, e.g., GANs and normalizing flows, the set is generated by mapping from different sample points, i.e., \(p_{0}\). Therefore, the full formulation becomes

\[\{^{(i)}}_{t_{c}:t_{T}}^{a}\}=(_{t_{0}:t_{c}}^{A _{t_{0}:t_{c}}};_{t_{0}:t_{c}},),\] (4)

where \(=\{p_{0},...,p_{K}\}\) is a set of sampled points from \(_{0}\).

Implicitly, the model \(\) is required to construct a transformation (Jacobians) between the two distributions. Usually, \(_{0}\) is chosen as a symmetric and tractable distribution, such as a standard Gaussian. However, the distribution of the target distribution can be shaped by many data-biased asymmetries thus posing a challenge to learning the transformation effectively and inclusively. This often causes failure of generating under-represented trajectory patterns for trajectory forecasting and hurts the diversity of the outcomes. This observation motivates us to propose a probabilistic generative model for more diverse outcomes by representing the original distribution with more expressiveness.

### Normalizing Flow

Normalizing flow  is a genre of generative model that constructs complex distributions by transforming a simple distribution through a series of invertible mappings. We choose normalizing flow over other probabilistic generative models as it can provide per-sample likelihood estimates thanks to being invertible. This property is critical to more comprehensively understand the distribution of different future trajectory patterns, especially when typically only sampling dozens of outcomes and considering the existence of long-tailed trajectory patterns. We denote a normalizing flow as a bijective mapping \(f\) which transforms a simple distribution \(p()\) to a complex distribution \(p()\). The transformation is often conditioned on context information \(\). With the change-of-variables formula, we can derive the transformation connecting two smooth distributions as follows:

\[ =f(;),\] (5) \[p() =p()|(_{}f^{-1}(; ))|,\] \[-(p()) =-(p())-(|(_{}f^{-1}( ;))|).\]

Given the formulations, with a known distribution \(_{0}\), we can calculate the density of \(p()\) following the transformations and vice versa. However, the equations require the Jacobian determinant of the function \(f\) to obtain the distribution density \(p()\). The calculation of it in the high-dimensional space is not trivial. Recent works propose to use deep neural networks to approximate the Jacobians. To maintain the inevitability of the normalizing flows, some carefully designed layers are inserted into the deep models and the coupling layers  are one of the most widely adopted ones.

More recently, FlowChain  is proposed to enhance the standard normalizing flow models by using a series of Conditional Continuously-indexed Flows (CIFs)  to estimate the density of outcomes. CIFs are obtained by replacing the single bijection \(f\) in normalizing flows with an indexed family \(F(;u)_{u U}\), where \(U R\) is the index set and each \(F(;u):\) is a bijection. Then, the transformation is changed to

\[ p(), U p_{U|}(|), :=F(;U).\] (6)

Please refer to  for more details about CIFs and their connection with variational inference. In this work, we follow the idea of using a stack of CIFs from  to achieve fast inference and the updates of trajectory density estimates.

Normalizing flow based model samples from a standard Gaussian, \((0,1)\), usually results in overfitting to the most-likelihood for trajectory prediction. This is because each data sample from the

Figure 2: The illustration of our proposed Mixed Gaussian Flow (MGF). During training, we construct a mixed Gaussian prior by statistics from the training set. During sampling, the initial noise samples are from the constructed mixed Gaussian prior. MGF keeps a tractable prior distribution and an invertible inference process while the novel mixed Gaussian prior provides more diversity and controllability to the generation outcomes.

training sample is considered extracted as the mode of a standard Gaussian. Only the mode value (the ground truth) is directly supervised and the underlying target distribution is assumed to be perfectly symmetric, which is not aligned with the usual real-world facts. Related discussion can be found in many previous literatures. This typically results in degraded expressiveness of the model to fail to capture under-represented motion patterns from the data and thus hurts the outcome diversity.

### Mixed Gaussian Flow (MGF)

We propose Mixed Gaussian Flow (MGF) to enhance the diversity and controllability in trajectory prediction. MGF consists of two stages as summarized in Figure 2. First, we construct the mixed Gaussian prior by fitting the parametric model of a combination of \(K\) Gaussians, \(\{(_{k},_{k}^{2})\},(1 k K)\). The parametric model is obtained with the data samples from training sets. Then, during inference, we sample points from the mixture of Gaussian and map them into a trajectory latent in the target distribution by a stack of CIF layers with the historical trajectories of all involved agents as the condition. We will introduce the two stages in detail below.

MGF maps from a mixture of Gaussians instead of a single Gaussian to the target distribution. To maintain the inevitability of the model, the mixed Gaussian prior can not be arbitrary. We obtain the parametric construction of the mixed Gaussian by fitting it with training data. In this fashion, we can derive multiple Gaussians to represent different motion patterns in the dataset, such as going straight or turning left and right. In a simplified perspective, we regard the mixture as combining multiple clusters, each of which represents a certain sub-distribution. By sampling from the mixture of Gaussians instead of a standard Gaussian, our constructed model has more powerful expressiveness than the standard normalizing flow model. This results in more diverse trajectory predictions. Also, by manipulating the mixed Gaussian prior, we can achieve controllable trajectory prediction.

**Mixed Gaussian Prior Construction.** For the data pre-processing, we transfer motion directions into relative directions with respect to a zero-degree direction. All position footage is represented in meters. Given the trajectory between \(t_{0} t_{c}\) to predict the trajectory between \(t_{c} t_{T}\), we would put the position pivot at \(t_{c}\), i.e., \(_{t_{c}}\), as the origin and convert the position on all other time steps to be the offset from \(_{t_{c}}\). Then, we cluster the preprocessed future trajectories into \(K\) clusters, which is a hyper-parameter. We note the mean of the clusters as \(=\{_{i}\}_{i=1,,K}\).

These cluster centers reveal the mean value of \(K\) representative patterns of pedestrians' motion, e.g. go straight, turn left. They will be the means of the Gaussians. The variances of the Gaussian, i.e., \(_{k}^{2}\), can be pre-determined or learned. The final mixture of Gaussians is denoted as

\[^{}=_{k=1}^{K}_{k}(_{k},_{k}^{2 }),\] (7)

where \(_{k}\) are the weights assigned to each cluster following the k-means clustering of the training data. By default, we perform clustering by K-means with \(K=8\).

**Flow Prediction.** Once the mixed Gaussian prior is built, we can do trajectory prediction by mapping samples from the distribution to future trajectories conditioned on historical information(e.g. social interaction features extracted by a trajectron++ encoder). Here, we ignore the intermediate transformation by CIFs as Equation (6) shows while following the original formulations of normalizing flows as Equation (5) for simplicity. We distribute the samples from different Gaussians by their weights. Given the \(i\)-th sample from \((_{k},_{k}^{2})\), we can transform it to the \(i\)-th predicted trajectories

\[_{i}^{},}$}_{t_{c}: t_{T}}^{a}=(_{t_{0}:t_{c}}^{A_{t_{0}:t_{c}}};_{t_{0}:t_{ c}},_{i}).\] (8)

For a sample \(_{i}}{_{k}}(_{k},_{k}^{2})\), we have the probability estimate

\[p(_{i})=_{k}}e^{-- _{k})^{2}}{2_{k}^{2}}},\] (9)

and the transformation is converted to

\[p(^{(i)}}_{t_{c}:t_{T}}^{a})=(--_{k})^{2}}{2 _{k}^{2}}+}{_{k}})|(_ {f(_{i};_{t_{0}:t_{c}})}_{i})|,\] (10)

which can be also invested back for the density estimate by the normalizing flow law

\[(_{i})=_{k}}(-(^{(i)}_{t_{c}:t_{T}}^{a};O_{t_{0}:t_{c}})-_{k}]^{2}}{2_{ k}^{2}}).\] (11)

### Training and Inference

The training loss of MGF comes from two directions: the forward process to get mixed flow loss and the inverse process to get minimum \(_{2}\) loss.

**Forward process.** Given a ground truth trajectory sample \(^{a}_{t_{c}:t_{T}}\), we need to assign it to a cluster in the mixed Gaussian prior by measuring its distance to the centroids

\[=*{arg\,min}_{i}(^{a}_{t_{c}:t_{T}}-_{i})^{2},^{}:=_{}(_{},_{ ^{2}}),\] (12)

with a tractable probability density function \(p_{}()\). Through the inverse process \(f^{-1}\) of flow model, we transform \(^{a}_{t_{c}:t_{T}}\) into its corresponding latent representation, here denoted as

\[}=f^{-1}(^{a}_{t_{c}:t_{T}};_{t_{0}:t_{c}}).\] (13)

Then we can compute the forward mixed flow loss:

\[L_{forward}=-(p(^{a}_{t_{c}:t_{T}}))=-(p_{}(}))-(|(_{^{a}_{t_{c}:t_{T}}}})|).\] (14)

Instead of computing negative-log-likelihood(NLL) loss of \(}\) in the mixed distribution \(_{k=1}^{K}_{k}(_{k},_{k}^{2})\), we compute NLL loss in the sub-Gaussian with the nearest centroid \(_{}(_{},_{^{2}})\) because each centroid is independent to others in the mixed distribution and we encourage the model to learn specified motion patterns to avoid overwhelming by the major data patterns. Calculating NLL loss over the mixed distribution may fuse other centroids and damage the diversity of model outputs. By our design, the mixed Gaussian prior can maintain more capacity for expressing complicated multi-modal distribution than the traditional single Gaussian prior, which typically constrains the target distribution to be single-modal and symmetric.

**Inverse Process.** This process repeats the flow prediction process to get generated trajectories. To predict \(M\) candidates, we sample \(_{i}_{k=1}^{K}_{k}(_{k},_{k}^{2}), i=1,2,...,M\) and transform them into \(M\) trajectories

\[\{^{i}}^{a}_{t_{c}:t_{T}}\}=\{f(_{i};_{t_{0 }:t_{c}})\},i=1,2,...,M.\] (15)

We compute the minimum \(_{2}\) loss between \(M\) predictions and ground truth trajectory as  does:

\[L_{inverse}=_{i=1}^{M}}^{a}_{t_{c}:t_{T}}- ^{a}_{t_{c}:t_{T}})^{2}}{t_{T}-t_{c}}.\] (16)

We sample \(_{i}\) from sub Gaussians by their weight. This is approximately equal to sampling from the original mixed Gaussians but makes the reparameterization trick doable.

Although approximated differential backpropagation techniques, such as the Gumbel-Softmax trick, can be employed to make the sampling process of mixed Gaussians differentiable, computing the Negative Log-Likelihood (NLL) loss between a sample point and the mixed Gaussian distribution remains challenging because

\[-(p_{^{}}(}))=-(_{k=1}^{K}}{_{k}} e^{--_{k})^{2}}{2_{k}^{2}} })+C,\] (17)

contains exponential operations on matrices, which can be simplified through logarithmic operations in single Gaussian condition. Computing this term requires iterative optimization methods, such as the Expectation-Maximization algorithm for approximation[60; 47], which makes the computing process much more complex. Therefore, in practice, sampling from individual Gaussian components is preferred for computing efficiency. Furthermore, applying the Gumble-softmax to learn a mixture of Gaussians in generative models has been reported difficult in practice in some cases due to gradient vanishing problem.

Figure 3: During training, the model is trained at both forward and inverse process of the normalizing flow.

The forward and inverse losses encourage the model to predict a well-aligned sample in a sub-space from the prior without hurting the flexibility and expressiveness of other sub-spaces. We combine the forward and inverse losses by a ratio \(\) to be a Symmetric Cross-Entropy loss , which was proved beneficial for better balancing the "diversity" and "precision" of predicted trajectories:

\[L=L_{forward}+ L_{inverse}.\] (18)

### Diversity Metrics

The widely adopted average/final displacement error (ADE/FDE) scores measure the alignment (precision) between the ground truth future trajectory and one predicted trajectory. Under the common "best-of-\(M\)" evaluation protocol, ADE/FDE scores encourage nothing but finding a single "aligned" trajectory with the ground truth. ADE encourages the position on all time steps to be aligned with the single ground truth and FDE chooses the trajectory with the closest endpoint while all other trajectories are neglected in score calculating. Such an evaluation protocol overwhelmingly encourages the methods to fit the most likelihood from a certain distribution and all generated candidates race to be the most similar one as the distribution mean. Under the single-mode and symmetric assumption, this usually tends to fit into a Gaussian with a smaller variance. However, this tendency hurts the diversity of predicted trajectory hypotheses.

To provide a tool for quantitative trajectory diversity evaluation, we formulate a set of metrics. Following the idea of average displacement error (ADE) and final displacement error (FDE), we measure the diversity of trajectories by their pairwise displacement along the whole generated trajectories and the final step. Follow Dlow, we name that average pairwise displacement (APD) and final pairwise displacement (FPD). We note that the diversity metrics are measured in the complete set of generated trajectory candidates instead of between a single candidate and the ground truth. The formulation of APD and FPD are as below

\[=^{M}_{j=1}^{M}}^{t_{T}}((i) }_{t}^{a}-(j)}_{t}^{a})^{2}}}{M^{2}(t_{T}- t_{c})},=^{M}_{j=1}^{M}}_{t_{T}}^{a}-(j)}_{t_{T}}^{a})^{2}}}{M^{2}},\] (19)

where APD measures the average displacement along the whole predicted trajectories and FPD measures the displacement of trajectory endpoints. We would mainly follow the widely adopted ADE/FDE for benchmarking purposes while using APD/FPD as a secondary metric set to better understand the diversity of the generated future trajectories.

## 4 Experiments

In this section, we provide experiments to demonstrate the effectiveness of our method. We first introduce experiment setup in Section 4.1 and benchmark with related works to evaluate the trajectory prediction alignment and diversity in Section 4.2. Then, we showcase the diversity and controllability of MGF in Section 4.3 and Section 4.4. Finally, we ablate key implementation components in Section 4.5.

### Setup

**Datasets.** We evaluate on two major benchmarks, i.e., ETH/UCY [24; 38] and SDD . ETH/UCY consists of five subsets. We follow the widely used Social-GAN  benchmark. SDD dataset consists of 20 scenes captured in bird's eye view. We follow the TrajNet  benchmark. We note that in the community of trajectory prediction, previous works have inconsistent evaluation protocol details and thus have made unfair comparisons. Please refer to the appendix in supplementary materials for details.

**Metrics.** We use the widely used average displacement error (ADE) and final displacement error (FDE) to measure the alignment of the predicted trajectories and the ground truth. ADE is the average L2 distance between the ground truth and the predicted trajectory. FDE is the L2 distance between the ground truth endpoints and predictions. Most previous works choose the "Best-of-\(M\)" evaluation protocol and we follow it to choose \(M=20\) as default.

Here, we note that, under different assumptions of distribution spreading and variance, the evaluation is ideally done with different values of \(M\). However, most existing methods only provide results with \(M=20\) and many of them do not open-source the code of the models so we can not rebenchmark with other value choices of \(M\). Besides the metrics for trajectory alignment, we also use the proposed metrics set APD and FPD to measure the diversity of the predicted trajectory candidates.

**Implementation Details.** We enhance our model using a similar technique as "intension clustering"  and we name it "prediction clustering". The key difference is that we directly cluster the entire trajectory instead of the endpoints. To make a fair comparison, we followed the data processing from FlowChain  and Trajectron++ . We also follow FlowChain's implementations of CIFs that each layer consists of a ReallNVP  with a 3-layer MLP and 128 hidden units. We use a Trajectron++  encoder to encode historical trajectories. All models were trained on a single NVIDIA V100 GPU for 100 epochs(approximately 4 to 8 hours).

### Benchmark Results

We benchmark MGF with a line of recent related works on ETH/UCY dataset in Table 1. The results of Trajectron++ and MID are updated according to a reported implementation issue 1. MGF achieves on-par state-of-the-art performance with Eomotion . Specifically, Our method achieves the best ADE and FDE in 3 out of 5 subsets and the best ADE and FDE score by averaging all splits. Here we note that we build MGF as a normalizing flow-based method as its invertibility is key property we desire, though normalizing flow is usually considered inferior regarding the alignment evaluation. Therefore, such a good performance on the alignment is surprising to us. To compare with other normalizing flow-based methods, our method significantly improves the performance compared to FlowChain, achieving **27.6%** improvement by ADE and **34.6%** improvement by FDE.

On the SDD dataset, where the motion pattern is considered more diverse than UCY/ETH, the benchmark results are shown in Table 2. Our method outperforms all baselines measured by ADE/FDE for trajectory alignment. Specifically, Our method reduces ADE from 8.56 to 7.74 compared to the current state-of-the-art method MemoNet, achieving **9.6%** improvement. Our method also significantly improves the performance of FlowChain for **22.1%** by ADE and **29.7%** by FDE. According to the benchmarking on the two popular datasets, we demonstrate the state-of-the-art alignment (precision) of our proposed method. Here we note again that the alignment with the deterministic ground truth is not the highest priority when we design our method, we will discuss the main advantages of MGF, diversity, and controllability, in the next paragraphs.

    &  &  &  &  &  &  \\   & **ADE** & **FDE** & **ADE** & **FDE** & **ADE** & **FDE** & **ADE** & **FDE** & **ADE** & **FDE** & **ADE** & **FDE** \\  Social-GAN  & 0.87 & 1.62 & 0.67 & 1.37 & 0.76 & 1.52 & 0.35 & 0.68 & 0.42 & 0.84 & 0.61 & 1.21 \\ STGAT  & 0.65 & 1.12 & 0.35 & 0.66 & 0.52 & 1.10 & 0.34 & 0.69 & 0.29 & 0.60 & 0.43 & 0.83 \\ Social-STGCNN  & 0.64 & 1.11 & 0.49 & 0.85 & 0.44 & 0.79 & 0.34 & 0.53 & 0.30 & 0.48 & 0.44 & 0.75 \\ Trajectron++  & 0.61 & 1.03 & 0.20 & 0.28 & 0.30 & 0.55 & 0.24 & 0.41 & 0.18 & 0.32 & 0.31 & 0.52 \\ MD  & 0.55 & 0.88 & 0.20 & 0.35 & 0.30 & 0.55 & 0.29 & 0.51 & 0.20 & 0.38 & 0.31 & 0.53 \\ PECNet  & 0.54 & 0.87 & 0.18 & 0.24 & 0.35 & 0.60 & 0.22 & 0.39 & 0.17 & 0.30 & 0.29 & 0.48 \\ GroupNet  & 0.46 & 0.73 & 0.15 & 0.25 & 0.26 & 0.49 & 0.21 & 0.39 & 0.17 & 0.33 & 0.25 & 0.44 \\ AgentFormer  & 0.45 & 0.75 & 0.14 & 0.22 & 0.25 & 0.45 & 0.18 & 0.30 & 0.14 & 0.24 & 0.23 & 0.39 \\ EqMotion  & 0.40 & 0.61 & **0.12** & **0.18** & 0.23 & 0.43 & 0.18 & 0.32 & **0.13** & **0.23** & 0.21 & 0.35 \\ FlowChain  & 0.55 & 0.99 & 0.20 & 0.35 & 0.29 & 0.54 & 0.22 & 0.40 & 0.20 & 0.34 & 0.29 & 0.52 \\  MGF(Ours) & **0.39** & **0.59** & 0.13 & 0.20 & **0.21** & **0.39** & **0.17** & **0.29** & 0.14 & 0.24 & **0.21** & **0.34** \\   

Table 1: **Results on _ETH/UCY_ dataset with Best-of-20 metrics. Scores are in meters, lower is better. bold and underlined scores denote the best and the second-best scores.**

    &  &  &  &  &  &  \\   & **APD** & **FFD** & **APD** & **FFD** & **APD** & **FFD** & **APD** & **FFD** & **APD** & **FFD** & **APD** & **FFD** \\  Social-GAN  & 0.680 & 1.331 & 0.566 & 1.259 & 0.657 & 1.502 & 0.617 & 1.360 & 0.515 & 1.119 & 0.607 & 1.314 \\ Social-STGCNN  & 0.404 & 0.633 & 0.591 & 0.923 & 0.333 & 0.497 & 0.490 & 0.762 & 0.417 & 0.657 & 0.447 & 0.694 \\ Trajectron++  & 0.704 & 1.532 & 0.568 & 1.240 & 0.648 & 1.404 & 0.697 & 1.528 & 0.532 & 1.161 & 0.630 & 1.373 \\ AgentFormer  & **1.998** & **4.560** & 0.995 & 2.333 & 1.049 & **2.445** & 0.774 & 1.772 & 0.849 & 1.982 & 1.133 & **2.618** \\ MemoNet  & 1.232 & 2.870 & 0.950 & 2.030 & 0.847 & 1.822 & 0.844 & 1.919 & 0.880 & 2.120 & 0.951 & 2.152 \\ FlowChain  & 0.814 & 1.481 & 0.484 & 0.833 & 0.636 & 1.094 & 0.505 & 0.890 & 0.492 & 0.859 & 0.586 & 1.031 \\  MGF(Ours) & 1.624 & 3.555 & **1.138** & **2.387** & **1.115** & 2.163 & **1.029** & **2.119** & **1.065** & **2.182** & **1.194** & 2.481 \\   

Table 2: **Evaluation results on _SDD_ (in pixels).**

    &  &  &  &  &  &  \\   & **APD** & **FFD** & **APD** & **FPD** & **APD** & **FFD** & **APD** & **FPD** & **APD** & **FPD** & **APD** & **FPD** \\  Social-GAN  & 0.680 & 1.331 & 0.566 & 1.259 & 0.657 & 1.502 & 0.617 & 1.360 & 0.515 & 1.119 & 0.607 & 1.314 \\ Social-STGCNN  & 0.404 & 0.633 & 0.591 & 0.923 & 0.333 & 0.497 & 0.490 & 0.762 & 0.417 & 0.657 & 0.447 & 0.694 \\ Trajectron++  & 0.704 & 1.532 & 0.568 & 1.240 & 0.648

### Diverse Generation

By leveraging the mixed Gaussian prior, our model can generate trajectories from the corresponding clusters, resulting in a more diverse set of trajectories than sampling from a Gaussian. This is intuitively due to less difficulty in learning the Jacobians for distribution transformation. We present examples in Figure 5. Given a past trajectory, there is a single ground truth future trajectory possibility from the dataset. We select four samples with different ground truth intentions, i.e., going straight, U-turn, left-turn, and right-turn. By sampling noise from the clustered distributions, we could generate future trajectories with diverse intentions. From the visualizations, we could notice, of course, that we generate outcomes that are very similar to the ground truth with close intentions while we also generate outcomes that have very diverse intentions. The well-aligned single trajectory accounts for the high ADE and FDE score our method achieves. And the impressive diversity demonstrates the effectiveness of our design, especially considering they are well controlled by the clusters where they are sampled from.

Quantitatively, we evaluate the generation diversity according to our proposed metrics on ETH/UCY dataset since most existing methods did not either make experiments on SDD or open-source training code/checkpoint on SDD. The results are presented in Table 3. We can observe that MGF achieves the best or second-best APD and FPD score on all splits among sota methods. Besides, our method significantly improves the performance compared to FlowChain, achieving **103.7%** improvement by APD and **140.6%** improvement by FPD. The only method that can achieve close diversity with our method is Agentformer , which designs sampling from a set of conditional VAE to improve the diversity. However, compared to MGF, Agentformer is more computation-intensive and shows significantly lower alignment according to ADE/FDE scores in Table 3. Also, Agentformer is not fully invertible, which is considered a key property we desire for trajectory forecasting. The superior quantitative performance according to the alignment (precision) and diversity metrics suggests the effectiveness of our method by balancing these two adversarial features.

We also find that APD/FPD metrics are not sensitive to \(M\), which is a natural result, see appendix B.

### Controllable Generation

The generated sample from MGF is highly correlated with the original sample drawn from the mixed Gaussian prior. If the prior distribution is a standard Gaussian as in the canonical normalizing flow method, we can have almost no control over the generated sample. The only controllability is to sample near the mode to generate a sample similar to the learned most-likelihood outcome or far from the mode to make them more different. However, as we discussed, after sufficient training and supervision by the forward loss, the variance of the latent Gaussian distribution of the outcome is usually very small, which further hurts the controllability. However, as we chose a transparent mixed Gaussian prior for the sampling, we can control the generation flexibility. First, by adjusting sub-Gaussians in the mixture prior, we can manipulate the generation process statistically. Figure 5 shows that by editing cluster compositions, we can control the predictions of MGF with good interpretability. By editing the weights of sub-Gaussians, we can control the ratio of splatting into directions. By editing the directions of the cluster means, we can control the intentions of samples statistically. Besides cluster centers, we can also edit the variance of Gaussian to control the density of generated trajectories or combine a set of operations to get expected predictions. We provide more discussions and examples in the appendix in the supplement.

### Ablation Study

We ablate some key components of our implementation for both ADE/FDE and APD/FPD metrics, see Table 4 and Table 5. (1)**Prediction clustering** is a common post-processing method, which improves the ADE/FDE as expected. However, it hurts the diversity for normalizing flow model with single Gaussian prior. This is reasonable as the single Gaussian prior tends to generate trajectories densely close to the most likelihood and prediction clustering can't cluster them into well-separated clusters for different motion intentions. (2)**Mixed Gaussian prior** help the model generates more diverse outputs and achieves higher APD/FPD scores and this improvement can be further enhanced by prediction clustering. It also increases ADE/FDE scores a lot, we believe this is because mixed Gaussian prior relieves the difficulty of learning the Jacobians for distribution transformation. Thus more under-explored patterns, which may be selected as the "best-of-\(M\)" samples in rare but plausible scenarios, have the chance to be expressed. (3)**Learnable variance** improve ADE/FDE while bring down APD/FPD a bit. We find that the learnable variance usually converges to a smaller value than the fixed situation. This is encouraged by the supervision from the ground truth (most likelihood) to a desired steeper Gaussian, thus hurting the diversity. However, its substantial improvement in ADE/FDE indicates that it remains a valuable component of the model architecture. (4)**Inverse loss** provides a straightforward supervision of the trajectory in the coordinate space, which is also proved beneficial for ADE/FDE and APD/FPD scores.

## 5 Conclusion

We focus on improving the diversity while keeping the estimated probability tractable for trajectory forecasting in this work. We noticed the poor expressiveness of Gaussian distribution as the original sampling distribution for normalizing flow-based methods to generate complicated and clustered outcome patterns. We thus propose to construct a mixed Gaussian prior to help learn Jacobians for distribution transformation with less difficulty and higher flexibility. Based on this main innovation, we propose Mixed Gaussian Flow (MGF) model for the diverse and controllable trajectory generation. The cooperating strategy of constructing the prior distribution and training the model is also designed. According to the evaluation of popular benchmarks, we demonstrate that MGF achieves state-of-the-art prediction alignment and diversity. It also has other good properties such as controllability and being invertible for probability estimates.