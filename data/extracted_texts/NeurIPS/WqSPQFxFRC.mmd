# LegalBench: A Collaboratively Built Benchmark

for Measuring Legal Reasoning in Large Language Models

Neel Guha

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1South Texas College of Law Houston, 7University of Toronto, 8St. Thomas University Benjamin L. Crump College of Law, 9Harvard Law School, 10Stanford Center for Legal Informatics - CodeX,

1University of Southern California, 12Georgetown University Law Center, 13Stanford Law School,

1University of Virginia, 15Telecom Paris, Institut Polytechnique de Paris, 16Osgoode Hall Law

1University of Virginia, 17Harvard Kennedy School, 18Golden Gate University School of Law,

1

Julian Nyarko

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1South Texas College of Law Houston, 7University of Toronto, 8St. Thomas University Benjamin L. Crump College of Law, 9Harvard Law School, 10Stanford Center for Legal Informatics - CodeX,

1

Daniel E. Ho

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1South Texas College of Law Houston, 7University of Toronto, 8St. Thomas University Benjamin L. Crump College of Law, 9Harvard Law School, 10Stanford Center for Legal Informatics - CodeX,

1

Christopher Re*

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1South Texas College of Law Houston, 7University of Toronto, 8St. Thomas University Benjamin L. Crump College of Law, 9Harvard Law School, 10Stanford Center for Legal Informatics - CodeX,

1

Adam Chilton

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Aditya Narayana

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Alex Chohlas-Wood

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Austin Peters

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Brandon Waldon

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Daniel N. Rockmore

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Diego Zambrano

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Dmitry Talisman

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Enam Hoque

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Faiz Surani

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Frank Fagan

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Galit Sarfaty

7University of Washington, 1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Gregory M. Dickinson

8University of Virginia, 1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Haggai Porat

9Harvard Law School, 1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Jason Hegland

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Jessica Wu

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Joe Nudell

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Joel Niklaus

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

John Nay
10Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Jonathan H. Choi

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Kevin Tobia

12Georgetown University Law Center, 13Stanford Law School,

1

Margaret Hagan

13Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Megan Ma

10Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Michael Livermore

14University of Virginia, 15Telecom Paris, Institut Polytechnique de Paris, 16Osgoode Hall Law

1

Nikon Rasumov-Rahe

35Nila Holzenberger

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Noam Kolt

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Peter Henderson

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Sean Rehaag

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Shaad Goel

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Shang Gao

20Stanford University, 2University of Toronto, 8St. Thomas University Benjamin L. Crump College of Law, 9Harvard Law School, 10Stanford Center for Legal Informatics - CodeX,

1

Spencer Williams

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Sunny Gandhi

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Tom Zur

9Harvard Iyer

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Varun Iyer

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

Zehua Li

1Stanford University, 2University of Chicago, 3Maxime Tools, 4Dartmouth College, 5LawBeta,

1

###### Abstract

The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning--which distinguish between its many forms--correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.

## 1 Introduction

Advances in large language models (LLMs) are leading American legal professionals to reexamine the practice of law .2 Proponents have argued that LLMs could alter how lawyers approach tasks ranging from brief writing to corporate compliance . By making legal services more accessible, they could eventually help alleviate the United States' long standing access-to-justice crisis . This perspective is informed by the observation that LLMs possess special propertieswhich, it is argued, make them more suited for legal tasks. The models' capacity to learn new tasks from limited labeled data would reduce the manual data annotation costs that ordinarily burden the development of legal language models . Their apparent proficiency at sophisticated reasoning tasks would also make them ideal for the rigor of law, which requires parsing obtuse texts with heavy jargon, and inferential processes which combine different modalities of reasoning .

This excitement, however, is tempered by the fact that legal applications often involve significant risk . Existing work has shown that LLMs are capable of generating content that is offensive, misleading, and factually incorrect . Such behaviors--if replicated in legal applications --could result in substantial harms , with much of the potential burden imposed on traditionally marginalized and under-resourced populations . The safety implications thus create a pressing need to develop infrastructure and processes for benchmarking LLMs in legal contexts.

However, significant challenges face practitioners seeking to assess whether LLMs can perform legal reasoning. The first challenge is the limited ecosystem of legal benchmarks . The majority of existing benchmarks, for example, focus on tasks which models learn by finetuning or training on task-specific data . These benchmarks do not measure the aspects of LLMs which generate excitement for law--namely, their ability to perform many different tasks using only few-shot prompts. Relatedly, benchmarking efforts have focused on professional certification exams like the Uniform Bar Exam , but these are not always representative of the actual use-cases for LLMs. The second challenge is the incongruity between the ways in which existing benchmarks and lawyers frame "legal reasoning." Existing benchmarks coarsely generalize all tasks involving legal data or laws as measuring "legal reasoning." In contrast, lawyers recognize that legal reasoning is a broad umbrella term encompassing many distinct types of reasoning . Different legal tasks require different skills and bodies of knowledge. Because existing legal benchmarks fail to draw these distinctions, it is difficult for legal professionals to contextualize the performance of modern LLMs within their own understanding of legal competency. In short: legal benchmarks do not use the same vocabulary or conceptual frameworks as the legal profession.

In light of these limitations, we believe that rigorously evaluating the legal reasoning capabilities of LLMs will require the legal community to take a more proactive role in the process of benchmarking. To that end, we present LegalBench: the first steps towards constructing an interdisciplinary collaborative legal reasoning benchmark for the English language.3 Over the past year, the authors of this paper--drawing from their diverse legal and computer science backgrounds--came together to assemble 162 tasks (from 36 different data sources), each of which measures a specific type of legal reasoning. LegalBench is thus, to the best of our knowledge, the first _open-source legal benchmarking effort_. We believe that this style of benchmark construction--where domain experts take an active and participatory role in the crafting of evaluation tasks--illustrates one approach to interdisciplinary collaboration in LLM research. Importantly, we believe it also shows that legal professionals have an essential role to play in the assessment and development of LLMs for law.

As a research project, we highlight three components of LegalBench:

1. LegalBench was constructed from a mix of existing legal datasets (restructured for the few-shot LLM paradigm), and hand-crafted datasets created and contributed by legal professionals (included as authors on this work). The legal professionals involved in this collaboration were asked to contribute datasets that they believed to either measure an interesting legal reasoning skill, or to capture a practically useful application for LLMs in the law. High performance on LegalBench tasks thus provides useful information, allowing lawyers to validate their assessment of an LLM's legal competency, or identify an LLM that could be used in their workflow.
2. LegalBench tasks are organized into an extensive typology which describes the types of legal reasoning required to perform the task. Because this typology is drawn from frameworks familiar to the legal community, it enables legal professionals to meaningfully engage in discussions of LLM performance, using a terminology and conceptual framework familiar to them .
3. Finally, LegalBench is intended as a platform to support further research. For AI researchers who lack legal expertise, LegalBench comes with significant support for understanding how to prompt and evaluate different tasks. And as more of the legal community begins to engage with the potential impact and role of LLMs, we hope to grow LegalBench by continuing to solict and incorporate tasks from legal professionals.4

In this paper, we make several contributions. First, we present a typology for organizing and describing legal tasks in terms of the types of reasoning they require. This typology is drawn from frameworks lawyers use to describe legal reasoning . Second, we provide an overview of the tasks in LegalBench, describing the process by which they were constructed, important dimensions of heterogeneity, and limitations. A full description of each task is provided in the Appendix. Finally, we use LegalBench to evaluate 20 LLMs from 11 different families, across a range of size points. We make observations regarding the performance of different models and present an initial study into different prompt-engineering strategies. Ultimately, these results are intended to highlight different directions of future work that LegalBench may enable.

We hope that this benchmark will be interesting to a diverse set of communities. Practitioners may use these tasks to determine whether and where LLMs can be integrated into existing workflows to improve outcomes for clients. Legal academics may benefit from observing the types of annotation that LLMs are capable of , and different forms of empirical scholarly work they may enable. Computer scientists may benefit from studying the performance of these models in a domain like law, where distinct lexical properties and unique tasks may surface new insights.

Before we progress further, we note that the purpose of this work isn't to evaluate whether computational systems _should_ replace lawyers and legal officers, or to understand the positive and negative impacts of that replacement . Rather, our goal is to construct artifacts that enable the relevant stakeholders and affected communities to better understand, _empirically_, the capacity for LLMs to perform different types of legal tasks. Given the proliferation of computational legal tools, we believe that answering this question is vital for ensuring their safe and ethical usage.

## 2 Related work

Benchmarking legal reasoningUnderstanding the extent to which NLP models can perform tasks or skills traditionally associated with lawyers--or be useful in legal analysis--has been the focus of significant work . Prior work has identified manually arduous tasks currently performed by lawyers (e.g., document review, case summarization) and developed corresponding benchmarks . Researchers have focused on the technically challenging aspects of legal tasks, like document length, jargon, or inferential reasoning . Other work has focused on creating datasets for pretraining models , non-English/multilingual tasks , legal judgement prediction , legal role labeling , and different forms of retrieval .

Importantly, the majority of previous benchmarking efforts have focused on language models which learn by supervised training or finetuning (e.g., BERT variants ), and researchers have consequently studied questions related to the role of domain specific datasets . More recently, researchers have begun to ask whether _large_ language models (LLMs) like GPT-3/4 can perform legal reasoning , citing to evidence of these models' capacity to perform sophisticated reasoning tasks in domains like math or programming . Unlike BERT-based models, LLMs are evaluated on their ability to learn tasks _in-context_, primarily through prompting. While a few works have experimented with LLMs on existing benchmarks , most evaluations focus on standardized tests or other exam equivalents . Studies have explored the role of prompt-engineering , potential applications , questions regarding human-LLM interaction , and comparisons to older finetuned-models .

Connections to other LLM benchmarking effortsWe highlight connections to two broader research efforts. First, we draw inspiration from existing efforts within NLP and machine learning to define fine-grained measures of performance, which allow researchers to discuss model capabilities with precision and specificity. Examples include the diagnostic set of the GLUE Benchmark , the "reasoning patterns" studied in , the task organization used in HELM , and the BigBench effort . We believe this paradigm of expert-driven evaluation is essential for specialized domains like law.

The LegalBench typology

LegalBench identifies six types of legal reasoning that LLMs can be evaluated for: (1) issue-spotting, (2) rule-recall, (3) rule-application, (4) rule-conclusion, (5) interpretation, and (6) rhetorical-understanding. We first justify the selection of these types by providing background on how the legal profession frames "legal reasoning," and the connections to our typology. We then illustrate how task datasets may be used to evaluate LLMs for each type, using examples from LegalBench.

Though this framework draws heavily on American legal thought, we believe it can be extended to characterize LegalBench tasks that implicate non-American bodies of law. We also note that our types are non-exhaustive, and in future work hope to consider extensions.

### Frameworks for legal reasoning

IracAmerican legal scholars often describe "legal reasoning" as the process of determining the legal conditions that arise from a set of events or occurrences, with reference to both prior cases and codified laws . A common framework for executing this type of legal reasoning is the **I**ssue, **R**ule, **A**pplication and **C**onclusion (**IRAC**) framework . In this framework, legal reasoning decomposes into four sequential steps.

First, lawyers identify the legal issue in a given set of facts (**issue-spotting**). An issue is either (1) a specific unanswered legal question posed by the facts, or (2) an area of law implicated in the facts. Depending on the setting, a lawyer may be told the issue, or be required to _infer_ a possible issue.

Second, lawyers identify the relevant legal rules for this issue (**rule-recall**). A rule is a statement of law which dictates the conditions that are necessary (or sufficient) for some legal outcome to be achieved. In the United States, rules come from different sources: the Constitution, federal and state statutes, regulations, and court opinions. Importantly, rules often differ between jurisdictions. Hence, the relevant rule in California might be different than the relevant rule in New York.

Third, lawyers apply these rules to the facts at hand (**rule-application**). Application, or the analysis of rule applicability, consists of identifying those facts which are most relevant to the rule, and determining how those facts influence the outcome under the rule. Application can also involve referencing prior cases involving similar rules (i.e. _precedent_), and using the similarities or differences to those cases to determine the outcome of the current dispute.

Finally, lawyers reach a conclusion with regards to their application of law to facts, and determine what the legal outcome of those facts are (**rule-conclusion**).

ExampleSuppose that BusinessMart--a large manufacturing corporation--is being sued by Amy in federal court on diversity jurisdiction.5 BusinessMart sells the majority of its goods in Texas, has its headquarters (where its CEO and board members sit and work) in California, and maintains a factory in Florida. A court is trying to determine--for the purposes of diversity jurisdiction--where BusinessMart's "principal place of business is."

* Issue-spotting: Here, a narrow issue is offered--where is BusinessMart's principal place of business?
* Rule-recall: A lawyer would recognize that the most relevant rule here comes from the case _Hertz Corp. v. Friend_,6 in which the Supreme Court determined "that the phrase 'principal place of business' refers to the place where the corporation's high level officers direct, control, and coordinate the corporation's activities." * Rule-application: Applying this rule to the facts above yields two observations. First, a corporation's CEO and board members are examples of high level officers referred to in _Hertz_ that control and conduct a company. Second, the place where BusinessMart's high level officers control the company is California, as that is where the CEO and board sit and work.
* Rule-conclusion: Based on the chain of inference spelled out in the application stage, a lawyer would thus conclude that California is BusinessMart's principal place of business.

The extent to which the outcome of the application and conclusion steps follow each other is dictated by the level of ambiguity in the fact patterns. When the law on a particular question is clear and there is little ambiguity in the facts (as the case in the above example), then the application and conclusion steps point towards the same outcome. Sometimes however, the facts may be unclear or contested, and reasonable minds may differ as the conclusion step. For now, LegalBench focuses entirely on the former setting (unambiguous answers), and all tasks are considered to have objectively "correct" answers.

Other types of reasoningThough IRAC is the most formal framework for legal reasoning, lawyers recognize a variety of skills which are useful to practice of law . For instance, lawyers are often required to exercise interpretive skills, in order to identify the rights, obligations, or limitations of certain legal language (e.g., what a contractual clause may or may not enable). They must also exhibit rhetorical skills, and understand the types of arguments that are made. Though these tasks require the knowledge base and skill set of lawyers, they, arguably, do not always fit neatly within the IRAC framework. Hence, we consider these to be distinct from the examples offered in the previous section.

### Evaluating legal reasoning in large language models

LegalBench identifies six categories of legal reasoning. For each category, we describe how a LLM task may evaluate the typified legal reasoning, using examples from LegalBench.

Issue-spottingLegalBench evaluates issue-spotting through tasks in which an LLM must determine if a set of facts raise a particular set of legal questions, implicate an area of the law, or are relevant to a specific party. Issue tasks evaluate a LLM's ability to reason over the legal implications of different activities, events, and occurrences. An example of an issue-spotting task is the learned_hands_benefits task, which requires an LLM to determine (Yes/No) whether a post on a public legal aid forum raises issues related to welfare law (i.e., public benefits or social services). The box below shows how a LLM might be prompted for this task.

Does the post discuss public benefits and social services that people can get from the government, like for food, disability, old age, housing, medical help, unemployment, child care, or other social needs?

Post: "I am currently receiving support from social services, idk why, this is just how my life turned out. They have asked for all of my bank information for the past 12 months. I don't know what this means. Why would they want that?"

Answer: Yes

Rule-recallLegalBench evaluates rule-recall through tasks which require the LLM to generate the correct legal rule on an issue in a jurisdiction (e.g., the rule for hearsay in US federal court). A rule task can be an open-ended generation task--in which the LLM must generate the text of the rule for a jurisdiction--or a classification task--in which the LLM must determine whether the rule exists in that jurisdiction. Anchoring to jurisdiction is important, as legal rules differ across different jurisdictions. Rule tasks are particularly useful for measuring _hallications_. An example of a rule-recall task is rule_qa, a question-answer task where questions include asking the model to state the formulations for different legal rules, identify where laws are codified, and general questions about doctrine.

Rule-recall example-safe

Question: What are the four requirements for class certification under the Federal Rules of Civil Procedure?"

Answer: Numerosity, commonality, typicality, adequacy

Rule-conclusionLegalBench evaluates rule-conclusion through tasks which require an LLM to determine the legal outcome of a set of facts under a specified rule. LLMs are evaluated purely on whether their predicted outcome is correct. For example, the occ_v_common_law task asks a LLM to determine whether a contract is governed by the Uniform Commercial Code (UCC) or the common law of contracts. The LLM is always provided with the relevant rule, via the prompt (see below).

[MISSING_PAGE_EMPTY:6]

function or has a certain property. An example is the definition_classification task, in which an LLM must determine if a sentence from a judicial opinion provides a definition of a term.

[title=RationalUnderstanding example definition, description] Does the sentence define a term?

Sentence:"To animadvert carried the broader implication of "turn[ing] the attention officially or judicially, tak[ing] legal cognizance of anything deserving of chastisement or censure; hence, to proceed by

way of punishment or censure." 1 Oxford English Dictionary 474 (2d ed.1989)."

Answer: Yes

We emphasize one aspect of LegalBench: IRAC in this work is used as an organizing principle for grouping tasks. On a law exam, a student would be expected to generate an answer which structurally resembles IRAC, where each step builds on the inferences of the previous step . LegalBench tasks, in contrast, each evaluate a single type of legal reasoning. Hence, a task like learned_hands_benefits can only be used to evaluate issue-spotting, and not rule-recall. In future work we hope to add tasks which evaluate multiple steps jointly.

## 4 LegalBench tasks

Appendix G discusses each task in detail, providing a description of the reasoning that each task evaluates, how task data was constructed, task examples, and evaluation protocols. This section provides an overview of LegalBench.

### Construction process

Task sourcesLegalBench tasks are drawn from three sources. The first source of tasks are existing available datasets and corpora. Most of these were originally released for non-LLM evaluation settings. In creating tasks for LegalBench from these sources, we often significantly reformatted data and restructured the prediction objective. For instance, the original CUAD dataset  contains annotations on long-documents and is intended for evaluating extraction with span-prediction models. We restructure this corpora to generate a binary classification task for each type of contractual clause. While the original corpus emphasized the long-document aspects of contracts, our restructured tasks emphasize whether LLMs can identify the distinguishing features of different types of clauses. The second source of tasks are datasets that were previously constructed by legal professionals but never released. This primarily includes datasets hand-coded by legal scholars as part of prior empirical legal projects (e.g., ). The last category of tasks are those that were developed specifically for LegalBench, by the authors of this paper. Overall, tasks are drawn from 36 distinct corpora.

Collaborative componentIn August 2022, we published a call for tasks, describing the goals of the project and its structure . We publicized the project through mailing lists and legal computational conferences. Submitted tasks were vetted for legal correctness and task validity. Task contributors are drawn from diverse professional backgrounds within the law (e.g., academics, practitioners, computational legal researchers) and constitute the authors of this paper.

InfrastructureLegalBench comes with support designed to enable non-law AI researchers to use and study LegalBench tasks. First, each LegalBench task is accompanied by extensive documentation describing how the task is performed, its legal significance, and the construction procedure. The objective of this documentation is to provide AI researchers with a working understanding of the mechanical processes behind each task, for the purposes of better understanding LLM performance. Second, each task is accompanied by a "base" prompt, which contains task instructions and demonstrations. The base prompt is provided to promote replicability and standardization. We anticipate that future research efforts building off of LegalBench will identify higher performing prompts/prompt formats. We intended to update the LegalBench GitHub repository with these prompts as they are discovered.

LimitationsWe note several limitations of the current LegalBench tasks (additional limitations are noted in Appendix C). First, when this project began, most LLM context-windows were constrained to a few pages of text. As a result, the initial round of LegalBench tasks does not involve longer documents. We hope to include such tasks in future work, particularly as recent technical developments have resulted in significantly longer context windows . Second, LegalBench's tasks focus on legal reasoning questions with objectively correct answers. LegalBench is thus not helpful for evaluating legal reasoning involving degrees of correctness or tasks where "reasonable minds differ." Third, LegalBench only considers English language tasks, is skewed towards certain jurisdictions (American law), and certain areas of the law (contracts). Thus, the current iteration of the benchmark limits inferences regarding how LLMs may generalize to legal tasks involving other jurisdictions. As we continue to solicit and incorporate contributions to LegalBench, we hope to add tasks addressing these limitations. Finally, LegalBench evaluates IRAC abilities independently, while law exams and other legal work requires lawyers to generate outputs which follow IRAC in a multi-hop matter (i.e., each aspect is applied to the same fact pattern).

### Dimensions of variation

Task structureAll LegalBench tasks contain at least 50 samples, with an average task size of 563 samples (Appendix E.4). These tasks are comparable in size to those used in benchmarking efforts like BigBench , HELM  or RAFT . LegalBench tasks also span different formats: multiple-choice questions (35 tasks), open-generation (7 tasks), binary classification (112 tasks), and multi-class/multi-label classification (8 tasks).

Reasoning types and legal domainsLegalBench provides tasks for each of the reasoning categories discussed above: rule-recall (5 tasks), issue-spotting (16 tasks), rule-application (16 tasks), rule-conclusion (16 tasks), interpretation (119 tasks), and rhetorical-understanding (10 tasks). Tasks are predominantly drawn from areas of law implicating civil matters, including contracts (58 tasks), civil procedure (8 tasks), evidence law (1 task), and corporate law (58 tasks). The skew towards interpretation tasks and tasks from contract law can be explained by the ubiquity of legal documents from these areas (e.g., contracts, terms-of-service agreements, disclosures, and etc.) and their immediate commercial implications [60; 74].

Language variationLegal language is highly heterogeneous, varying in sentence structure, vocabulary, and rhetorical style across different legal areas and document types . This poses a distinct challenge for LLMs, which are extremely sensitive to structure of input text and the vocabulary used . LegalBench tasks are drawn from a diverse set of legal language types, thus enabling researchers to study performance variation across different categories of legal text. Specifically, LegalBench encompasses tasks with language drawn from plain English (32 tasks), legal opinions (11 tasks), merger agreements (34 tasks), contracts (55 tasks), statutory text (3), and other sources.

## 5 Results

We use LegalBench to conduct a three-part study. In the first part (Section 5.2), we evaluate 20 LLMs from 11 different families, at four different size points. In the second part (Section 5.3), we show how LegalBench can be used to conduct in-depth evaluations of models. To illustrate, we use LegalBench to highlight similarities and differences in the performance of three popular commercial models: GPT-4, GPT-3.5, and Claude-1. In the final part (Section 5.4), we show how LegalBench can support the development of law-specific LLM methods. We focus on prompting, and conduct experiments illustrating tradeoffs and challenges with regards to guiding LLMs towards certain tasks. Ultimately, our study here serves to illustrate the types of analyses that LegalBench enables, and highlight potential directions for future work. We summarize the findings of our study here, and provide more details in the Appendix.

### Setup

ModelsWe study 20 LLMs from 11 different families. These are: GPT-3.5  (text-davinci-003) and GPT-4 from OpenAI ; Claude-1 (v1.3) from Anthropic ; Incite-Instruct-7B, Incite-Base-7B, and Incite-Instruct-3B from Together [34; 133]; OPT-2.7B, OPT-6.7B, and OPT-13B from Meta ; Falcon-7B-Instruct from TII [2; 103]; MPT-7B-8k-Instruct from MosaicML ; Vicuna-7B-16k and Vicuna-13B-16k from LMSYS  ; Flan-T5-XL (3B parameters) and Flan-T5-XXL (11B parameters) from Google ; LLaMA-2-7B, and LLaMA-2-13B from Meta ; WizardLM-13B ; and BLOOM-3b and BLOOM-7B from BigScience . All inference was performed on two-GPU GCP 40GB A100s, using the Manifest library . HuggingFace links for each model are provided in the Appendix.

PromptsWe designed a prompt for each task by manually writing instructions for the task, and selecting between zero and eight samples from the available train split to use as in-context demonstration. The number of samples selected depended on the availability of data and the sequence length of samples. For application evaluation, we augmented the prompt with an instruction for the LLM to explain its reasoning. We used the same prompt for all models with one exception (Claude-1). LLM outputs were generated using next-token generation at a temperature of 0.0. For classification/extraction tasks, we terminated at a new-line 353 token. For rule_qa and all application tasks except diversity_jurisdiction_6 we generated 150 tokens. For diversity_jurisdiction_6 we generated 300 tokens.

EvaluationClassification tasks are evaluated using "exact-match" (following HELM ). Because some tasks contain significant label imbalances, we use balanced-accuracy as a metric. For extraction tasks, we perform normalization on generated outputs to account for differences in tense/casing/punctuation. A few tasks (e.g., successor_liability and ssla_individual_defendants) requires the LLM to produce multiple classes or extracted terms per instance. For these, we evaluate using F1. Rule-application tasks were evaluated manually by a law-trained individual, who analyzed LLM responses for both correctness and analysis. The Appendix provides more details on our approach to manual grading. All manual evaluation was performed with reference to a grading guide, which we additionally make available.

### Performance trends

Table 2 provides the average task performance for all 20 models in five reasoning categories. The first block of rows corresponds to large commercial models, the second block corresponds to models in the 11B-13B range, the third block corresponds to models in the 6B-7B range, and the final block corresponds to models in the 2B-3B range. Table 3 provides the average task performance for the three large models on rule-application. The Appendix provides full results for each model on each task.

We highlight several observations. First, within LLM families, larger models usually outperform smaller models. For instance, Flan-T5-XXL (11B parameters) outperforms 372 Flan-T5-XL (3B parameters) on average across all five reasoning categories, and LLaMA-2-13B outperforms 373 LLaMA-2-7B on average across four reasoning categories. Second, even for LLMs of the same size, we find considerable differences in performance. For instance, we observe significant gaps in performance between Flan-T5-XXL (11B parameters) and Vicuna-13B-16k (13B parameters), across all reasoning categories. This suggests, unsurprisingly, that the choice of pretraining data, regime of instruction-tuning, and architecture play an important role in determining performance, and that certain configurations may be better aligned for LegalBench tasks. Interestingly, such choices may affect which types of reasoning categories LLMs appear to perform well at. For instance, WizardLM-13B performs worse than all peers on issue-spotting tasks, best on rule-recall tasks, and nearly matches the performance of the best-performing peer on rule-conclusion tasks. Third, we find evidence that open-source models are

  
**LLM** & **Issue** & **Rule** & **Conclusion** & **Interpretation** & **Rhetorical** \\  GPT-4 & 82.9 & 59.2 & 89.9 & 75.2 & 79.4 \\ GPT-3.5 & 60.9 & 46.3 & 78.0 & 72.6 & 66.7 \\ Claude-1 & 58.1 & 57.7 & 79.5 & 67.4 & 68.9 \\  Flan-T5-XXL & 66.0 & 36.0 & 63.3 & 64.4 & 70.7 \\ LLaMA-2-13B & 50.2 & 37.7 & 59.3 & 50.9 & 54.9 \\ OPT-13B & 52.9 & 28.4 & 45.0 & 45.1 & 43.2 \\ Vicuna-13B-16k & 34.3 & 29.4 & 34.9 & 40.0 & 30.1 \\ WizardLM-13B & 24.1 & 38.0 & 62.6 & 50.9 & 59.8 \\  BLOOM-7B & 50.6 & 24.1 & 47.2 & 42.8 & 40.7 \\ Falcon-7B-Instruct & 51.3 & 25.0 & 52.9 & 46.3 & 44.2 \\ Incite-7B-Base & 50.1 & 36.2 & 47.0 & 46.6 & 40.9 \\ Incite-7B-Instruct & 54.9 & 35.6 & 52.9 & 54.5 & 45.1 \\ LLaMA-2-7B & 50.2 & 33.7 & 55.9 & 47.7 & 47.7 \\ MPT-7B-sk-Instruct & 54.3 & 25.9 & 48.9 & 42.1 & 44.3 \\ OPT-6.7B & 52.4 & 23.1 & 46.3 & 48.9 & 42.2 \\ Vicuna-7B-16k & 3.9 & 14.0 & 35.6 & 28.1 & 14.0 \\  BLOOM-3B & 47.4 & 20.6 & 45.0 & 45.0 & 36.4 \\ Flan-T5-XL & 56.8 & 31.7 & 52.1 & 51.4 & 67.4 \\ Incite-3B-Instruct & 51.1 & 26.9 & 47.4 & 49.6 & 40.2 \\ OPT-2.7B & 53.7 & 22.2 & 46.0 & 44.4 & 39.8 \\   

Table 2: Average performance for each LLM over the different LegalBench categories. The first block of rows corresponds to large commercial models, the second block corresponds to models in the 11B-13B range, the third block corresponds to models in the 6B-7B range, and the final block corresponds to models in the 2B-3B range. The columns correspond to (in order): issue-spotting, rule-recall, rule-conclusion, interpretation, and rhetorical-understanding. For each class of models (large, 13B, 7B, and 3B), the best performing model in each category of reasoning is underlined.

  
**LLM** & **Correctness** & **Analysis** \\  GPT-4 & 82.2 & 79.7 \\ GPT-3.5 & 58.5 & 44.2 \\ Claude-v1 & 61.4 & 59.0 \\   

Table 3: Average performance for the large LLMs on rule-application tasks.

capable of performance that matches or exceeds certain commercial models. For instance, Flan-T5-XXL outperforms GPT-3.5 and Claude-1 383 on two categories (issue-spotting and rhetorical-understanding), despite the relative gap in parameter count. Notably, the gap between closed and open-source models is largest for the rule-conclusion category. Amongst LegalBench tasks, rule-conclusion tasks most like the other types of multi-step/common-sense reasoning tasks where commercial LLMs have been found to perform well.

### Comparing commercial models

Next, we conduct a more in-depth study of performance, focusing on the three commercial models (GPT-4, GPT-3.5, and Claude-1). In particular we highlight how LegalBench can provide more rigorous empirical support for anecdotal observations arising out of the legal community's use of these models, and explain performance differences between models. The Appendix provides a more in-depth analysis.

We highlight two major findings. First, evaluation on LegalBench reveals that the largest performance difference between GPT-4 and GPT-3.5/Claude-1 occurs (on average) for rule-application tasks. On rule application tasks, we observe that GPT-4 outperforms both GPT-3.5 (\(p<0.01\)) and Claude-1 (\(p<0.01\)) on both correctness and analysis. Across LLMs, we find that variation in performance across tasks is consistent with subjective impressions of task difficulty. For instance, performance on diversity_jurisdiction_1 (an easy task requiring a model to determine if an amount is greater than $75k and if the plaintiff and defendant are from different states) is much higher than performance on successor_liability (a harder task requiring a model to identify multiple successor liability exceptions in a fact pattern describing a complex transaction). Second, on the interpretation tasks, we find that on average GPT-4 outperforms GPT-3.5 (\(p<0.01\)), and GPT-3.5 outperforms Claude-1 (\(p<0.01\)). Here, the larger API-models are highly performant on tasks which involve binary classification over short clauses. Averaged across the 38 CUD tasks (contract clauses), for instance, GPT-4, GPT-3.5, and Claude-1 all have a balanced-accuracy \( 88\%\). And on proa (statutory clauses), both GPT-4 and GPT-3.5 have a balanced-accuracy \( 90\%\). Notably, performance degrades on tasks which contain longer text sequences or involve multi-class classification. On the Supply Chain Disclosure tasks for instance--in which LLMs must classify disclosures which are 1-2 pages in length--the average balanced-accuracy of the large commercial models ranges between \(74-75\%\). And on the MAUD tasks--which require answering multiple choice questions about merger deals--the average balanced-accuracy of GPT-4 drops to \(47.8\%\) accuracy.

### Prompt engineering strategies

Finally, we illustrate--through a series of micro-studies--how LegalBench can be used to explore different aspects of prompt-engineering for LLMs in legal settings. We focus on three questions: (1) Can LLMs rely on their latent knowledge of a rule for rule-conclusion tasks? (2) Does simplifying task descriptions to plain language affect performance? (3) Are LLMs sensitive to the choice of in-context demonstrations?

We highlight our findings (full results can be found in the Appendix). With regards to (1), we find considerable variation over studied tasks--some rules appear to be sufficiently well known that explaining the rule to the LLM in the prompt has a negligible impact on performance. With regards to (2), we find that a plain-language prompt significantly outperforms the technical language prompt, by up to 21 points (balanced-accuracy) on the subset of studied tasks. And with regards to (3), we find that LLMs are highly sensitive to the choice of in-context samples, with performance differences of up to 20pts (balanced-accuracy). These results illustrate that further work on understanding how to effectively and efficiently prompt LLMs for legal tasks is needed.

## 6 Conclusion

Our work here describes LegalBench: a collaboratively constructed benchmark of 162 tasks for measuring the legal reasoning capabilities of LLMs. In future work, we hope to expand this project, by continuing to solicit and collect interesting and useful tasks from the legal community