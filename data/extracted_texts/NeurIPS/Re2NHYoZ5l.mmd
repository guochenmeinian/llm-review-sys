# Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions

Ruihai Wu\({}^{1,4}\) Kai Cheng\({}^{2}\)

**Yan Shen\({}^{1,4}\) Chuanruo Ning \({}^{2}\) Guanqi Zhan \({}^{3}\) Hao Dong \({}^{1,4}\)\({}^{}\)**

\({}^{1}\)CFCS, School of CS, PKU \({}^{2}\)School of EECS, PKU \({}^{3}\)University of Oxford

\({}^{4}\)National Key Laboratory for Multimedia Information Processing, School of CS, PKU

Equal contribution. Author ordering determined by coin flip.Corresponding author.

###### Abstract

Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, _e.g._, occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containing a single occluder and generalizing to scenes with complex occluder combinations. Experiments demonstrate the effectiveness of our proposed approach in learning affordance considering environment constraints.

## 1 Introduction

Articulated objects, such as doors and drawers, exist everywhere in our daily life. Perceiving and manipulating these objects present crucial yet challenging tasks in computer vision and robotics. Unlike rigid objects, articulated objects exhibit diverse articulation types and functionally important articulated parts crucial for human and robot interactions. Numerous research endeavors have been investigating articulated objects broadly, encompassing joint parameters estimation , part pose estimation , kinematic structure estimation , digital twins generalization , articulated part robotic manipulation  and few-shot policy adaptation .

However, most existing works for manipulating articulated objects primarily focus on single-object scenarios with homogeneous agents, such as flying grippers  or fixed-position robot arms . Consequently, these approaches tend to develop **object-centric** representations and policies, neglecting the realistic constraints imposed by both the environment and the agent's morphology. These constraints are commonplace in real-world scenarios and their oversight limits the applicability and performance of the manipulation tasks. For example, successfully opening a cabinet door that is obstructed by occluders not only depends on the properties of the target door but also heavily relies on the robot's position and the way it interacts (_e.g._, colliding or bypassing) with the occluders.

We take a significant step towards manipulating articulated objects in a more realistic setting, _i.e._, considering constraints imposed by the environment and robot. Such a task encounters the combinatorial explosion challenge in complexity. To be specific, the substantial variability of occluders, characterized by their numbers, geometries, positions, and poses, leads to exponential complexity in scene data distribution [49; 18]. This introduces a significant obstacle to training a model that can comprehensively understand the diversity and intricacies of different scenes within the limits of available data. Moreover, sampling-based motion planning [19; 35], a kind of commonly used approach for planning and manipulation, also faces performance degradation with increasing occluders  as the probability of valid sampling rapidly diminishes.

We propose the use of **point-level** representations to tackle the challenge outlined above. Specifically, we exploit a notable property: even in situations where the combinations of occluders are complex, given the target manipulation point and the robot, the occluder parts that significantly affect the manipulation are typically confined to a limited area. This is clearly illustrated in Figure 1 (Row 1, Column 2), where, in the case of pushing a drawer obstructed by multiple occluders, the portions of the occluders that could potentially collide with the robot are confined to a specific area (shown as areas in the two boxes with bright colors in the heatmap). This confinement results from the predetermined position of the robot and the target manipulation point, which limits the range of possible trajectories for the manipulation. Hence, when we contemplate the manipulation of the target object at the **point level**, the complexity of these significant scene components remains manageable, despite the possible addition of unseen occluders.

Further, we take inspiration from **affordance** for robotic manipulation, which provides actionable priors of the target at the point level and thus guides manipulation policies, and has exhibited remarkable efficacy in 3D articulated object manipulation [25; 44; 42; 6] and other manipulation tasks [54; 26; 43]. Unlike these works that focus solely on single objects, we introduce the concept of **environment-aware affordance** for articulated object manipulation, which integrates **object-centric actionable priors** with **environment constraints** at the point level. Specifically, in a scene featuring a target articulated object, diverse occlusion objects, and a robot at different positions, we aim to learn the per-point actionable information of the target object, taking into account the constraints imposed by the environment and the agent. Furthermore, in light of Gibson's theory of affordance

Figure 1: **Learning Environment-Aware Affordance for Articulated Object Manipulation. As the complexity of occluder combinations grows exponentially, we leverage the property that target manipulation point (**Red Point**) conditioned _significant parts of occluders_ that impact the manipulation usually have limited local areas (**Red Pointâ€™s** corresponding **Red Box**) even in complex scenes. Aware of such significant parts, our model can be trained on one-ocluder scenes (**Train**) and generalize to multiple occluder combinations (**Test**, **Affordance**). The learned affordance provides actionable information for articulated object manipulation (**Manipulation**). Our model predicts reasonable affordance on real-world scanned point clouds (**Real-world Scan**).**

(the originator of affordance theory), where affordance is defined as the different possibilities of action that the **environment** offers to an **agent**, our proposed task is not only more realistic but also more closely aligned with this foundational theory compared to preceding works.

The point-level representation we propose above shows the potential to train a _significant-part-aware_ model in simple scenes while capable of generalizing to more complex scenes. This approach leverages the aforementioned point-level significant parts, whose complexity remains manageable across a spectrum from simple to complicated scenes. To facilitate learning of significant-part-aware scene representations, we design a robot-target conditioned occlusion field.

Further, we introduce a contrastive learning method that refines our learned representations to selectively disregard insignificant elements while maintaining sensitivity to similar, significant parts across varied scenes. Consequently, our proposed learning framework can be trained on scenes with a single occluder, yet generalizable to scenes with numerous novel occluder combinations. This strategy effectively mitigates the issue of combinatorial explosion in a data-efficient manner.

We conduct experiments using SAPIEN physics simulator  equipped with large-scale PartNet-Mobility  and ShapeNet  datasets. To assess our framework's performance and data efficiency, we initially train it on scenes containing only one occluder, with additional augmented contrastive scenes that incorporate an extra occluder. We then test our framework on significantly more complex scenes, featuring diverse combinations of novel occluders. Experimental results yield convincing evidence of the effectiveness and data-efficiency of our proposed framework.

In summary, we make the following contributions:

* We explore the task of manipulating articulated objects within environment constraints and formulate the task of **environment-aware** affordance learning for manipulating 3D articulated objects, incorporating **object-centric** per-point priors and environment constraints.
* To tackle the **combinatorial explosion** problem in scene complexity, we propose a data-efficient framework capable of training on scenes featuring a single occluder and generalizing to scenes with complex occluder combinations, leveraging contrastive learning and point-level local significant part representations.
* We establish benchmarking multi-object full-robot (as opposed to flying grippers) environments in SAPIEN simulator. Results show our framework learns environment-aware affordance generalizable well to novel scenes with complex novel occluder combinations.

## 2 Related Work

### Visual Affordance for Robotic Manipulation

Visual affordance  is a kind of representation that indicates possible ways for robots to interact with the target and complete tasks. Many works study affordance for the classic grasping task in robotics [24; 28; 3; 17; 16; 50], while there exist many current works on point-level affordance indicating object geometrics for articulated object manipulation [25; 44; 42; 5; 6; 29], dual-gripper collaboration  and object to object interaction . Tracing back to Gibson , the proposer of affordance, however, affordance notates the opportunities of interactions that involve consideration of the environment constraints and the robot, such as the occluders and the robot morphology, and thus ought to be aware of the robot and environment. Therefore, we propose the environment-aware affordance learning task, incorporating both the object actionable priors and environment constraints.

### Occlusion Handling

Occlusion is a significant challenge for current computer vision and robotics systems. In computer vision, there have been previous works trying to handle occlusion in different tasks, including object detection [39; 14; 52], instance segmentation [48; 14; 53] and tracking . Besides, to intrinsically handle the occlusion, another series of work aim to recover the entire shape of occluded objects, _i.e._, amodal segmentation [55; 11; 52].

In robotics, the occlusion problem mainly exists in object retrieval [20; 12], grasping [40; 51; 36] or rearrangement [37; 2; 21] in clutters. In our paper, we study the task of learning the manipulation of 3D articulated objects with cluttered occlusions in front of the target object, which involves consideration of not only the observations of occlusions and the target, but also the agent's relationship with them.

Problem Formulation

We formulate a new challenging task, **Environment-Aware Affordance**, aiming to infer point-level affordance to indicate the actionable information for a full robot arm in different positions to manipulate 3D articulated objects under diverse occlusions. While previous point-level affordance learning works only consider flying grippers [25; 42; 44; 54] or fixed arms  to manipulate the target object without occluders, we further consider the robot location as well as scene occlusions.

Specifically, in our formulated task, given as input a 3D scene point cloud \(S\) with \(n\) points \(p_{1},p_{2},...,p_{n}\), containing a target articulated object \(T\) with \(m\) points \(T_{p_{1}},T_{p_{2}},...,T_{p_{n}}\), \(k\) occluder object point cloud \(O_{1},O_{2},...,O_{k}\), and the robot position \(R^{3}\), the model is required to predict the point-level affordance score \(a_{T_{p_{i}}|R,S}\) on each target point \(T_{p_{i}}\) of \(T\), indicating how likely the point is interactable. The point-level affordance is able to guide 3D articulated object manipulation tasks.

As the complexity of occluder combinations grows exponentially, making it difficult and time-consuming to collect enough occluder combinations for training, we propose a data-efficient method that trains on only a few scenes with a single occluder, which can generalize to scenes with multiple occluder combinations.

## 4 Method

### Overview

Our framework is composed of four networks: Robot Encoder \(_{R}\), Target Encoder \(_{T_{p}|S}\), Scene Encoder \(_{S|T_{p},R}\) and Affordance Predictor \(A_{T_{p}|R,S}\). While \(_{R}\) and \(_{T_{p}}\) extracts the robot and target representations \(f_{R}\) and \(f_{T_{p}|S}\) (Sec. 4.2), the most important component of our framework is learning the robot-target conditioned scene representations that are sensitive to the significant local parts in the scene for manipulation (Sec. 4.3), which uses \(_{S|T_{p},R}\) to extract the scene representations \(f_{S|T_{p},R}\) from a designed robot-target conditioned occlusion field. This component makes the model sensitive to robot-target point conditioned significant parts and thus empowers the model with the generalization in novel complicated scenes. Finally, \(A_{T_{p}|R,S}\) takes \(f_{R}\), \(f_{T_{p}|S}\) and \(f_{S|T_{p},R}\), and predicts the point-level environment-aware affordance score \(a_{T_{p}|R,S}\) on \(T_{p}\) (Sec. 4.4).

Figure 2: **Our Proposed Data-efficient Framework for Learning Environment-Aware Affordance. Our model takes an occluded scene point cloud and robot position as input. Our framework generates per-point occlusion fields indicating most significant local parts of occluders. Then, it predicts per-point affordance using extracted features of each target point, its corresponding occlusion field, and robot position. The trained model can generalize to novel multi-occluder scenes. Red points denote manipulation points over the target object.**

### Robot and Target Representations

Both the robot position and the target manipulation point (including its position and local geometry) will affect the environment-aware affordance. For example, when the robot position changes, the affordance will change according to the robot's new reach area and interaction modes with occluders. Similarly, when the target manipulation point changes, the originally interactable point will become non-interactive for various reasons, such as robot being unable to reach the new point, the local geometry of the target point not supporting the desired action (_e.g._, a smooth surface does not support pulling), or the robot colliding with occluders while attempting to move to the new point. So we design a Robot Encoder \(_{R}\) and a Target Encoder \(_{T_{p}|S}\) to extract relevant information.

As shown in Figure 2, we employ a MLP network as \(_{R}\), which encodes the robot position \(R\) into a latent representation \(f_{R}^{128}\) and a Segmentation-version of PointNet++  that encodes the scene point cloud into per-point features, where \(f_{T_{p}|S}^{128}\) represents the feature of \(T_{p}\).

### Robot-Target Conditioned Scene Representations

In scenes with multiple occluders, the quantity, geometry and arrangement of occluders demonstrate rich diversity. However, given the robot position and the target manipulation point, most points in the scene are unlikely to significantly affect the manipulation, and the area of the significant parts that do have an impact (_e.g._, the area around occluders where potential collisions may occur during robot movement) is usually not large. By focusing on these significant parts of the scene, the change of occluder number and geometry will not matter, and a model can effectively learn to generalize to more complex scenes with multiple novel occlusion combinations.

To this end, we design the robot-target conditioned occlusion field to facilitate learning significant scene representations conditioned on the robot and the target point (Sec. 4.3.1), and propose the robot-target conditioned contrastive learning to further enhance learned scene representations (Sec. 4.3.2).

#### 4.3.1 Robot-Target Conditioned Occlusion Field

Although a scene may contain a large number of points (of occlusions and the target object), when we know the target manipulation point and the robot position, the point number of the significant part is not large, and the distance between these points and either the robot or the target manipulation point is not far. Therefore, we design the robot-target conditioned occlusion field that maps the scene into a vector field conditioned on both the target point and the robot, facilitating learning scene representations highlighting significant parts for representations.

We define the _Occlusion Field_ as a conditional continuous vector field \(\) on an open and connected set \(=^{3}^{3}\), which can be represented by a value function \(_{R,T_{p}}:^{3}\),

\[_{R,T_{p}}(x,y,z)&=}}}\\ &= x-x_{R},y-y_{R},z-z_{R}  x-x_{T_{p}},y-y_{T_{p}},z-z_{T_{p}}\\ &= F_{1},F_{2},F_{3}.(x,y,z) .\] (1)

The field factors \(}\) and \(}}\) are seperately conditional on \(R\) and \(T_{p}\). For any point \((x,y,z)\) in \(\), its value vector \(_{R,T_{p}}(x,y,z)\) is calculated as the cross product (\(\)) of the Euclidean vectors from \((x,y,z)\) to robot \(R\) and target \(T_{p}\), two fixed points of \(_{R,T_{p}}\). Since the field is continuous, we have:

\[_{p T_{p}}_{R,T_{p}}(x,y,z)=_{p R}_{R,T_{p }}(x,y,z)=0.\] (2)

So the model can filter the unimportant points whose field values are too large.

We employ a PointNet  network as the Scene Encoder \(_{S|T_{p},R}\). It takes as input the conditional vector field \(_{R,T_{p}}\) mapped from the scene point cloud \(S\) selected with small field values, and then outputs the robot-target conditioned scene representations \(f_{S|T_{p},R}\).

#### 4.3.2 Robot-Target Conditioned Contrastive Learning

The above-learned robot-target conditioned scene representations \(f_{S|T_{p},R}\) should be sensitive to the meaningful local parts of the occluders for manipulation, while agnostic to unimportant occluders and their local parts, despite the number, geometry and combinations of them. Specifically, the learnedscene representations should have the following properties: (1) given the same target point, when a new occluder occurs in the scene while not affecting the manipulation, the learned representations will keep the same; (2) given the same scene, when the target point changes, the meaningful local parts of scene as well as the learned representations will correspondingly change.

To better empower scene representations with such properties, which can further boost the performance and data-efficiency in affordance prediction, we propose the robot-target conditioned contrastive learning method for learning scene representations.

As shown in Figure 3, for predicting the point affordance \(_{T_{p}|R,S}\) for \(T_{R,S}(p)_{,}\) task in single-occluder scene \(S\), the target point \(T_{p}\) and the robot \(R\), we correspondingly generate a positive and a negative task. Specifically, we generate the task of predicting the point affordance \(_{T_{p^{}}|R,S}\) with the same scene \(S\) drawn from augmentation distribution \(A( T_{R,S}(p))\), robot \(R\) and a different target point \(T_{p^{}}\) drawn from marginal distribution \(A()=_{T_{R,S}(p)}A( T_{R,S}(p)),\) for the reason that although the scene keeps unchanged, the target point changes and thus the significant part in occludes will correspondingly change, and thus the scene representations should change. Also, we generate the task of predicting the point affordance \(_{T_{p}|R,S^{}}\) with the same robot \(R\), target point \(T_{p}\), and a new scene\(S^{}\) which adds to the original scene \(S\) with an occluder that is uninfluential to the manipulation. Even though the scene changes from \(S\) to \(S^{}\), the significant part in the occluders keeps unchanged, and thus the target-robot conditioned scene representations should keep the same. We leverage this prior to conduct our contrastive learning, which performs better representation learning for the occlusion field.

The original task and its positive and negative paired tasks constitute triplets, so we use triplet loss  to learn the scene representations of them contrastively (\(\) is the boundary constant):

\[^{CL}_{T_{p}|R,S}=\|f_{S|T_{p},R}-f_{S^{}|T_{p},R} \|_{2}^{2}-\|f_{S|T_{p},R}-f_{S|T_{p^{}},R}\|_{2}^{2}+\] (3)

### Affordance Prediction

We propose the Affordance Predictor \(A_{T_{p}|R,S}\) that aggregates the information of the target, the robot and the scene to predict the point-level environment-aware affordance.

We employ a MLP as the Affordance Predictor \(A_{T_{p}|R,S}\) that takes \(f_{R}\), \(f_{T_{p}|S}\) and \(f_{S|T_{p},R}\) as input, and predicts the affordance score \(_{T_{p}|R,S} R\) on each target point \(T_{p}\) conditioned on \(R\) and \(S\).

We apply \(_{1}\) loss to measure the error between the affordance prediction \(_{T_{p}|R,S}\) and the ground-truth affordance score \(a_{T_{p}|R,S}\) on a certain target point \(T_{p}\):

\[^{AFF}_{T_{p}|R,S}=_{1}(a_{T_{p}|R,S},_{T_{p}|R,S }).\] (4)

The total loss for the whole framework is then defined as:

\[^{total}_{T_{p}|R,S}=^{AFF}_{T_{p}|R,S}+_{CL} ^{CL}_{T_{p}|R,S},\] (5)

where \(_{CL}\) is a balancing coefficient.

Figure 3: **Robot-Target Conditioned Contrastive Learning.** For each point affordance prediction task (Middle, \(_{T_{p}|R,S}\)), we respectively generate its corresponding positive (Up, same target point with an extra occluder \(_{T_{p}|R,S^{}}\)) and negative (Down, different target point \(_{T_{p^{}}|R,S}\)) paired tasks, and use triplet loss to learn the scene representations.

Experiments

### Settings

For **tasks**, we follow Where2Act  and use the point-level affordance predictions of pushing and pulling articulated parts (doors and drawers) as our tasks. In a scene with occluders, a target point and a robot, the ground-truth affordance score of a target point is set to be 0 / 1 when the robot fails / succeeds in pushing or pulling the target point without colliding occluders.

For **simulation and dataset**, we use SAPIEN  as our simulation environment, equipped with large-scale Partnet-Mobility dataset  and ShapeNet  dataset, with occluder data statistics as shown in Table 4. Besides, we use cabinets and tables as target objects, for the reason that in the real world there often exist many occluders in front of them.

For **training**, we collect interactions in one-ocluder scenes. Specifically, we collect 900 successful and 900 failure interactions for pushing, and 300 successful and 2500 failure interactions for pulling. Pulling needs more failure interactions as most points are not pullable. For each data, we additionally collect a positive and a negative paired interaction for contrastive learning. For **testing**, we use multi-ocluder scenes respectively in training category test shapes and novel categories.

For **evaluation**, to evaluate the accuracy of affordance prediction, we follow Where2Act and use **F-Score** and **Average Precision**. To evaluate predicted affordance's capability in providing actionable priors for manipulation in scenes, we introduce **Sample Manipulation Accuracy** metric \(sma\):

\[sma=}{}\] (6)

The total interactions are proposed based on affordance predictions over the whole scene. We randomly adopt proposals with a confidence threshold and compute the manipulation accuracy.

### Baselines and Ablations

We compare our method with the following baselines that learn affordance for manipulation:

* **Where2Act (W2A)** that abstracts the robot arm into a flying gripper and learns point-level actionable affordance for manipulating articulated objects.
* **W2A-R** that uses a robot arm instead of a flying gripper in Where2Act's setting, and adds a robot encoder in Where2Act's networks.
* **O2O-Afford (O2O)** that learns point-level affordance for object-to-object interactions. We use the robot and the scene as two input objects.
* **O2O-M** that trains O2O-Afford using scenes containing multiple occluders.

To ensure fair comparison, we train the baselines using both originally generated data and their corresponding positive and negative paired data collected for contrastive learning.

Moreover, we compare our method with Collision Avoidance RTT Planner (**RTT-CA**) [19; 35; 8], a commonly used sampling-based planner for robotic manipulation, to demonstrate the capability of point-level affordance in guiding manipulation in complicated scenes.

In addition, we compare our method with two versions that respectively ablate one core component:

* **Ours w/o OF** that learns environment-aware affordance without the occlusion field.
* **Ours w/o CL** that learns environment-aware affordance without contrastive learning.

   Train-Cats. & All & Basket & Bottle & Bowl & Box \\   & & & & & \\  Train-Data & 367 & 77 & 16 & 128 & 17 \\ Test-Data & 128 & 31 & 4 & 44 & 5 \\   & & Bucket & Chair & Pot & TrashCan \\  & & & & & \\   & & 27 & 61 & 16 & 25 \\  & & 9 & 20 & 5 & 10 \\  Novel-Cats. & All & Dispenser & Jar & Kettle & FoldChair \\  & & & & & \\  Test-Data & 589 & 9 & 528 & 26 & 26 \\   

Table 1: **Ocluder Dataset Statistics.** We use 1,084 different shapes in ShapeNet  and PartNet-Mobility , covering 12 commonly seen indoor occluder categories. We use 8 training categories (split into 367 training shapes and 128 test shapes), and 4 novel categories with 589 shapes.

[MISSING_PAGE_FAIL:8]

Additionally, Figure 6 shows our method generates different environment-aware affordance conditioned on different robot positions in the same scene. Figure 7 demonstrates our framework generates promising results by testing on different real-world scenes. It is worth mentioning, as denoted in the red circles, our model not only learns constraints from occluders, but also learns to avoid manipulating points that may lead to collisions with other parts of the object (_i.e._, self occlusion).

## 6 Conclusion

We introduce environment-aware affordance for manipulating 3D articulated objects within environment constraints, leveraging point-level representations to address the combinatorial explosion challenge in scene complexity. Using an interactive environment built upon SAPIEN and the PartNet-Mobility and ShapeNet datasets, we train neural networks that predict per-point actionable information for manipulating articulated 3D objects under occlusions. We present extensive quantitative evaluations and qualitative analyses of the proposed method. Results show that the learned priors are highly localized and thus generalizable to novel scenes with unseen occluder combinations.

Ethics Statement.Our work has the potential to enable robots on articulated object manipulation in complicated scenes. The learned environment-aware affordance avoids collisions in manipulation, and thus reduces the risk of accident. We do not see our work has any particular harm or issue.

   Method & pushing & pushing (novel) & pulling & pulling (novel) \\   W2A & 64.70 / 52.97 & 62.80 / 47.07 & 66.42 / 37.59 & 60.37 / 40.73 \\  W2A-R & 68.66 / 71.31 & 64.86 / 69.55 & 34.06 / 72.56 & 26.79 / 73.79 \\  O2O & 65.18 / 77.07 & 59.04 / 72.18 & 46.03 / 74.06 & 43.42 / 73.56 \\  O2O-M & 61.06 / 67.82 & 67.43 / 68.54 & 39.76 / 66.42 & 33.29 / 61.47 \\  Ours w/o OF & 64.28 / 69.02 & 58.31 / 66.94 & 36.36 / 67.19 & 33.33 / 64.30 \\  Ours w/o CL & 68.59 / 74.02 & 64.70 / 71.62 & 65.74 / 75.40 & 55.25 / 71.92 \\  Ours & **77.30 / 86.11** & **76.11 / 83.58** & **71.28 / 75.96** & **61.32 / 75.18** \\   

Table 3: **Quantitative Evaluations and Comparisons with Baselines and Ablated Versions. In each entry, we report F-Score(%) and Average Precision(%) before and after slash.**

Figure 6: **Predicted Affordance Changes Conditioned on Different Robot Rositions.**

Figure 7: **Predicted Affordance on Real-World Scans.**

Acknowledgment

This work was supported by National Natural Science Foundation of China - General Program (62376006) and The National Youth Talent Support Program (8200800081).