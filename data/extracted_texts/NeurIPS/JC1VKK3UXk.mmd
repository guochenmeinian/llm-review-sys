# Poseidon: Efficient Foundation Models for PDEs

Maximilian Herde\({}^{1,}\)1

Bogdan Raonic\({}^{1,2,}\)1

Tobias Rohner\({}^{1}\)

Roger Kappeli\({}^{1}\)

Roberto Molinaro\({}^{1}\)

Emmanuel de Bezenac\({}^{1}\)

Siddhartha Mishra\({}^{1,2}\)

\({}^{1}\)Seminar for Applied Mathematics, ETH Zurich, Switzerland

\({}^{2}\)ETH AI Center, Zurich, Switzerland

Correspondence to herdem@ethz.ch

###### Abstract

We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.

## 1 Introduction

Partial Differential Equations (PDEs)  are referred to as the _language_ of physics as they mathematically model a very wide variety of physical phenomena across a vast range of spatio-temporal scales. _Numerical methods_ such as finite difference, finite element, spectral methods etc.  are commonly used to approximate or _simulate_ PDEs. However, their (prohibitive) computational cost, particularly for the so-called many-query problems , has prompted the design of various _data-driven_ machine learning (ML) methods for simulating PDEs,  and references therein. Among them, _operator learning_ algorithms have gained increasing traction in recent years.

These methods aim to learn the underlying PDE solution operator, which maps function space inputs (initial and boundary conditions, coefficients, sources) to the PDE solution. They include algorithms which approximate a _discretization_, on a fixed grid, of the underlying solution operator. These can be based on convolutions , graph neural networks  or transformers . Other operator learning algorithms are _neural operators_ which can directlyprocess function space inputs and outputs, possibly sampled on multiple grid resolutions [27; 3]. These include DeepONets [13; 42], Fourier Neural Operator , SFNO , Geo-FNO , Low-rank NO  and Convolutional Neural Operator , among many others.

However, existing operator learning methods are not _sample efficient_ as they can require a very large number of training examples to learn the target solution operator with desired accuracy (see Figure 1 or Figure 3 of ). This impedes their widespread use as _task-specific_ training data is very expensive to generate either with numerical simulations or measurements of the underlying physical system.

_How can the number of training samples for PDE learning be significantly reduced?_ In this context, can we learn from language modeling and computer vision where a similar question often arises and the current paradigm is to build _foundation models_. These _generalist_ models are _pretrained_, at-scale, on large datasets drawn from a diverse set of data distributions. They leverage the intrinsic ability of neural networks to learn _effective representations_ from pretraining and are then successfully deployed on a variety of _downstream_ tasks by _finetuning_ them on a few task-specific samples. Examples of such models include highly successful large language models [10; 72], large multi-modal models[17; 52] and foundation models for robotics , chemistry , biology , medicine  and climate .

Despite very recent preliminary attempts [67; 74; 1; 49; 19; 68; 66], the challenge of designing such foundation models for PDEs is _formidable_, given the sheer variety of PDEs (linear and nonlinear, steady and evolutionary, elliptic, parabolic, hyperbolic and mixed etc.), the immense diversity of data distributions, wide range of underlying spatio-temporal scales and the paucity of publicly available high-quality datasets. In particular, the very feasibility of designing PDE foundation models rests on the fundamental and unanswered science question of _why pretraining a model on a (very) small set of PDEs and underlying data-distributions can allow it to learn effective representations and generalize to unseen and unrelated PDEs and data-distributions via finetuning?_

The investigation of this open question motivates us here to present the Poseidon family of PDE foundation models. Poseidon, see Figures 1 and 2, is based on i) scalable Operator Transformer or scOT, a _multiscale vision transformer_ with (shifted) windowed or Swin attention [38; 37], adapted for operator learning, ii) a novel all2all training strategy for efficiently leveraging _trajectories_ of solutions of time-dependent PDEs to scale up the volume of training data and iii) an open source large-scale pretraining dataset, containing a set of novel solution operators of the compressible Euler and incompressible Navier-Stokes equations of fluid dynamics. We evaluate Poseidon on a challenging suite of 15 downstream tasks, comprising of well-established benchmarks in computational physics that encompass linear and nonlinear, time-dependent and independent and elliptic, parabolic, hyperbolic and mixed type PDEs. All of these tasks are _out-of-distribution_ with respect to the pretraining data. Moreover, nine out of the 15 tasks even involve PDEs (and underlying physical processes) which are not encountered during pretraining.

Figure 1: As opposed to PDE-specific operator learning, our pretrained model Poseidon is up to multiple orders of magnitude more sample efficient than a task-specific neural operator while also being able to transfer to unseen physics during finetuning.

Through extensive experiments, we find that i) Poseidon shows impressive performance across the board and outperforms baselines on the downstream tasks, with significant gains in accuracy and order of magnitude gains in sample efficiency. For instance, on an average (median) over the downstream tasks, Poseidon requires a mere 20 samples to attain the same error level as the widely-used FNO does with 1024 samples. ii) These gains in accuracy and sample efficiency are also displayed on tasks which involve PDEs not encountered during pretraining, allowing us to conclude that Poseidon can _generalize to unseen and a priori unrelated physical processes and phenomena_ with a few task-specific training examples and iii) Poseidon scales with model and dataset size, both for the pretraining as well as for downstream tasks and iv) through case studies, we elucidate possible mechanisms via which Poseidon is able to learn _effective representations_ during pretraining, which are then leveraged to generalize to unrelated PDEs downstream. Taken together, these results provide the first positive answers to the afore-mentioned fundamental question of the very feasibility of PDE foundation models and pave the way for the further development and deployment of Poseidon as an _efficient general purpose PDE foundation model._ Finally, we also open source the Poseidon model and the entire pretraining and downstream task datasets within the PDEgym database.

## 2 Approach

**Problem Formulation.** We denote a generic time-dependent PDE as,

\[&_{t}u(x,t)+(u,_{x}u, _{x}^{2}u,)=0, x D^{d},t( 0,T),\\ &(u)=0,(x,t) D(0,T), u (0,x)=a(x), x D\] (1)

Here, with a function space \( L^{p}(D;^{n})\) for some \(1 p<\), \(u C([0,T];)\) is the solution of (1), \(a\) the initial datum and \(,\) are the underlying differential and boundary operators, respectively. Note that (1) accommodates both PDEs with high-order time-derivatives as well as PDEs with (time-independent) coefficients and sources by including the underlying functions within the solution vector and augmenting \(\) accordingly (see **SM** B.2 for examples).

Even _time-independent_ PDEs can be recovered from (1) by taking the _long-time limit_, i.e., \(_{t}u=\), which will be the solution of the (generic) time-independent PDE,

\[((x),_{x},_{x}^{2} ,)=0, x D,()=0, x D.\] (2)

Solutions of the PDE (1) are given in terms of the underlying _solution operator_\(:[0,T]\) such that \(u(t)=(t,a)\) is the solution of (1) at any time \(t(0,T)\). Given a data distribution \(()\), the _underlying operator learning task (OLT)_ is,

**OLT**: _Given any initial datum \(a\), find an approximation \(^{*}\) to the solution operator \(\) of (1), in order to generate the entire solution trajectory \(\{^{*}(t,a)\}\) for all \(t[0,T]\)._

It is essential to emphasize here that the learned operator \(^{*}\) has to generate the _entire solution trajectory for (1), given only the initial datum (and boundary conditions)_, as this is what the underlying solution operator \(\) (and any numerical approximation to it) does.

**Model Architecture.** The backbone for the Poseidon foundation model is provided by scOT or _scalable Operator Transformer_, see Figure 2 (a-c) for an illustrated summary. scOT is a _hierarchical multiscale vision transformer with lead-time conditioning_ that processes lead time \(t\) and function space valued initial data input \(a\) to approximate the solution operator \((t,a)\) of the PDE (1).

For simplicity of exposition, we set \(d=2\) and \(D=^{2}\) as the underlying domain. As in a vision transformer , any underlying input is first _partitioned into patches and (linearly) embedded into a latent space_. At the level of function inputs \(a C(D;^{n})\), this amounts to the action of the _patch partitioning and embedding_ operator \(=}(a)\), with \(}\) defined in **SM** (12). This operator transforms the input function into a piecewise constant function, which is constant within patches (subdivisions of the domain \(D\)), by taking weighted averages and then transforming these piecewise constant values into a \(C\)-dimensional latent space resulting in output \( C(D;^{C})\). In practice, a discrete version of this operator is used and is described in **SM** A.2.

As shown in Figure 2 (a), this patch embedded output is then processed through a sequence of _SwinV2 transformer_ blocks , each of which has the structure of \(SW_{}:C(D;^{C}) C(D;^{C})\),

\[_{}=SW_{}(_{-1})& =_{}^{}+LN_{_{2}^{},_{2}^{}}(MLP (_{}^{})),\\ _{}^{}=_{-1}+LN_{_{1}^{ },_{1}^{}}(W-MSA(_{-1})).\] (3)

for layer index \(=1,...,L\). The main building block of a SwinV2 transformer block (3) (see Figure 2 (b)) is the _windowed multi-head self attention_ operator defined in **SM** (14) (see **SM** A.2 for its discrete version). In particular, the attention operator acts only inside each window, which is defined by another (coarser) sub-division of \(D\) (see Figure 2 (c)), making it more computationally efficient than a standard vision transformer . Moreover, the windows are shifted across layers, as depicted in Figure 2 (c), so that all the points in the domain can be attended to, by iteratively shifting windows across multiple layers, see **SM** A.2 for a detailed description of the SwinV2 block.

The MLP in (3) is defined by **SM** (15). We follow  to propose a _time-conditioning_ strategy by introducing a _lead-time conditioned_ layer norm in (3),

\[ LN_{(t),(t)}()(x)& =(t)(x)-_{}(x)}{_{ }(x)}+(t),\\ _{}(x)&=_{j=1}^{C} _{j}(x),\;_{}^{2}(x)=_{j=1}^{C}( _{j}(x)-_{}(x))^{2},\] (4)

Here, \((t)= t+\) and \((t)= t+\), with learnable \(,,,\) although more general (small) MLPs can also be considered. This choice of time embedding enables _continuous-in-time evaluations_.

Finally, as depicted in Figure 2 (a), the SwinV2 transformer blocks (3) are arranged in a hierarchical, multiscale manner, within a U-Net style _encoder-decoder_ architecture , by employing patch merging (downscaling) and patch expansion (upscaling) operations, (see **SM** A.2 for a detailed description). Moreover, layers at the same scale, but within the encoder and decoder stages of scOT, respectively, are connected through _ConvNeXt_ convolutional layers , specified in **SM** A.2.

**Training and Inference.** We denote scOT by \(_{}^{*}:[0,T]\), with trainable parameters \(^{p}\). For scOT to approximate the solution operator \(\) of (1), the parameters \(\) need to be determined by minimizing the mismatch between the predictions of scOT and ground truth training data, given in the form of trajectories \(\{(t_{k},a_{i})\}\), for \(0 k K\) and \(1 i M\), with \(a_{i}\) and \(0=t_{0}<t_{1}< t_{k}<<t_{K}=T\), being the time points at which the data is sampled. We assume that the data is sampled at the same timepoints for each sample \(a_{i}\) for simplicity. For training, it is natural to consider the loss function,

\[}():=_{i=1}^{M} _{k=0}^{K}\|_{}^{*}(t_{k},a_{i})-(t_{k},a_{i} )\|_{L^{p}(D)}^{p},\] (5)

Figure 2: (a) scOT, the model underlying Poseidon; (b) SwinV2 Transformer block; (c) Shifting Window over patch-based tokens with window (patch) boundaries with black (white); (d) all2all Training for time-dependent PDEs.

with the (spatial) integral in (5) being replaced by a quadrature at some underlying sampling points and \(p=1\) in our paper. Thus, we use \(K+1\) samples per trajectory in order to train our model.

Given the fact that scaling up available training data is necessary for successful foundation models , we propose a _novel training strategy_ that further leverages the _structure_ of the time-dependent PDE (1) to increase the amount of training data. To this end, we consider the modified loss function,

\[}():=}_{i=1}^{M}_{k, =0,k}^{K}\|(t_{}-t_{k},u_{i}(t_{k}))- ^{*}_{}(t_{}-t_{k},u_{i}(t_{k}))\|_{L^{p}(D)}^{p},\] (6)

with \(u_{i}(t_{k})=(t_{k},a_{i})\) (approximately) solving (1) and \(=\). In other words, we leverage the fact that the solution operator of (1) possesses a _semi-group property_ and one can realize,

\[u(t^{*})=(t^{*},a)=(t^{*}-t,u(t))=(t^{*}-t, (t,a)),\;\;0 t t^{*} T,\] (7)

and any initial condition \(a\). We term this use of all possible data pairs \((u(t_{k}),u(t_{}))\) with \(k\), see Figure 2 (d) for a visual representation, within a trajectory as _all2all training_ and observe that it allows us to utilize _quadratic_\((K^{2})\) samples per trajectory, when compared to the linear \(K\) samples used for training corresponding to the _vanilla_ loss function (5). In practice, we consider a relative form of Equation 6 to balance out different scales of different operator outputs, see **SM** C for details.

Once scOT has been trained with (stochastic) gradient descent to find a (local) minimum \(^{*}\) of the all2all loss function (6), the trained model, denoted as \(^{*}_{^{*}}\) can be deployed for inference for any initial condition \(a\) and for any \(t_{+}\) by directly applying \(^{*}_{^{*}}(t,a)\) to provide continuous-in-time evaluation of the entire trajectory. However, it might be advantageous to infer using _autoregressive rollouts_. To this end, we consider a sequence \(0=t_{0}^{*}<t_{1}^{*}<<t_{}^{*}=t\). Then, the rollout,

\[(t,a)^{*}_{^{*}}(t_{}^{*}-t_{ -1}^{*},^{*}_{^{*}}(^{*}_{ ^{*}}(t_{2}^{*}-t_{1}^{*},^{*}_{^{*}}(t_{1}^{*},a))),\] (8)

of \(\) successive applications of the trained scOT approximates the solution operator at any time \(t\).

**Pretraining.** The key point in the development of any foundation model is the _pretraining_ step, in which the model is trained on a diverse set of data distributions, rather than just on data drawn from one specific operator. To formulate pretraining and subsequent steps precisely, we introduce index sets \(,\) and let \(\) and \(\) correspond to indexing the PDE type and the data-distribution, respectively. To see this, we fix any \(,\) and tag the differential and boundary operators \(,\) in the PDE (1) by \(^{}\) and \(^{}\). Similarly the initial distribution \(\) in (1) is tagged by \(^{}\) and the resulting solution operator for PDE (1) with \(^{},^{}\) and initial datum \(a^{}\) is denoted by \(^{,}\). In other words, \(,\) indexes the entire set of PDEs and data distributions that we consider.

Next, we fix index sets, \(\) and \(\) and consider a set of PDEs (1), indexed by \(\) and with data distributions \(^{}\), indexed by \(\) as the _pretraining dataset_, which consists of the corresponding trajectories, \(\{^{,}(t,)\}\), for all \(t\) and all \((,)(,)\).

Let \(n^{}\) be the maximum number of components of the solution vectors for all the operators in the pretraining dataset. By including additional (constant \(0\) over space and time) components, we augment the relevant solution operators (for which the number of components is below \(n^{}\) ) such that for each \(,\), all the input functions have the same number of \(n^{}\) components (channels). These inputs are fed into a scOT model \(^{,}_{}:[0,] L^{p}(D; ^{n^{}}) C([0,];L^{p}(D;^{ n^{}}))\), with \(\) being the supremum over all the final times in the pretraining dataset. The trainable parameters \(\) of this pretrained model are then determined by _minimizing the mismatch between model predictions and ground truth over all PDEs and data distributions in the pretraining dataset_ resulting in,

\[^{,}_{*}=^{,}_{^{*}},\;_{*}=_{ }|||}_{} _{}}^{,}()\;,\] (9)

with \(}^{,}\) obtained by replacing \(\) and \(}^{*}_{}\) in (6) with \(^{,}\) and \(^{,}_{}\), respectively.

**Finetuning.** To _finetune_ the pretrained foundation model \(^{,}_{*}\) for any downstream task, corresponding any specific solution operator \(^{,}\) for any \(,\), we decompose the vector of learnable parameters \(^{p}\) as \(=[,,^{}]\), with \(^{}\), \(^{}\), and \(^{}^{_{}}\) and \(++_{}=p\), with \(,_{}\). A gradient descent step for finetuning is then written as,

\[ r 1,&[_{r+1}, _{r+1},_{r+1}^{}]=[_{r},_{r},_{r}^{}]-[ _{r},_{r},_{r}^{}] _{}}^{,}(_{r}),\\ &_{0}=_{*},_{0}^{}=_{*}^{}, {}_{0},(^ {}).\] (10)

Hence, during finetuning, a subset of parameters \(\) of the foundation model are trained from scratch with random initializations, whereas the complementary, much larger subset of \(\) and \(_{}\) is initialized by _transferring_ the corresponding parameters from the pretrained model. When \(\), \(\) consists of the _embedding/recovery_ parameters. On the other hand, if \(\), then all trainable parameters, including the patch embeddings/recovery, are initialized with the corresponding parameters of the pretrained model. However, the corresponding learning rate \(_{r}_{r}\) in (10) is much higher. Similarly, the time embeddings \(^{}\), i.e., the trainable parameters in the layer-norm operators (4) are always initialized from the corresponding time embeddings in the pretrained model but finetuned with a higher learning rate \(^{}\).

## 3 Experiments

**Pretraining Dataset.** We pretrain Poseidon on a dataset containing 6 operators, defined on the space-time domain \(^{2}\). 4 of these operators (CE-RP, CE-KH, CE-CRP, CE-Gauss) pertain to the compressible Euler equations (**SM** (37)) of gas dynamics and 2 (NS-Sines, NS-Gauss) to the incompressible Navier-Stokes equations (**SM** (31)) of fluid dynamics, see **SM** Table 3 for abbreviations and **SM** B.1 for a detailed description of these datasets. These datasets have been selected to highlight different aspects of the PDEs governing fluid flows (shocks and shear layers, global and local turbulent features, and mixing layers etc.). The pretraining dataset contains 9640 and 19640 trajectories for the Euler and Navier-Stokes operators, respectively, leading to a total of 77840 trajectories. Each trajectory is uniformly sampled at 11 time snapshots. Within the all2all training procedure (Section 2), this implies a total of \(66\) input-output pairs per trajectory, leading to approx 5.11M training examples in the pretraining dataset.

**Downstream Tasks.** To evaluate Poseidon (and the baselines), we select a suite of 15 challenging downstream tasks, see **SM** Table 4 for abbreviations and **SM** B.2 for detailed description. Each of these tasks is a (variant of) well-known benchmarks for PDEs in the numerical analysis and computational physics literature and corresponds to a distinct PDE solution operator. They have also been selected for their diversity in terms of the PDE types as they contain linear (4) and nonlinear (11), time-dependent (12) and time-independent (3), elliptic (2), parabolic (1), hyperbolic (4) and mixed-type (8). The tasks also cover a wide gamut of physical processes across a range of spatio-temporal scales. Moreover, we emphasize that each of the downstream tasks is _out-of-distribution_ with respect to the pretraining data. While 6 of them do pertain to the Euler and Navier-Stokes equations seen in the pretraining dataset but with very different data distributions, the remaining 9 involve PDEs not seen during pretraining. These include 3 (NS-Tracer-PwC, FNS-KF, GCE-RT) which add new physical processes (tracer transport, forcing, gravity) to the Navier-Stokes and Euler equations. 3 more (Wave-Gauss, Wave-Layer, ACE) involve completely new time-dependent PDEs (Wave Eqn., Allen-Cahn Eqn.) and the final 3 (SE-AF, Poisson-Gauss, Helmholtz) _even consider time-independent PDEs_, which is in stark contrast to the pretraining dataset where only 2 time-dependent PDEs are covered. For these steady state PDEs, we finetune them by using the interpretation of the PDE (2) as a _long-time limit_ of the time-dependent PDE (1) with a normalized lead time of \(1\). Finally, the tasks have also been selected to probe the ability of the foundation model to handle different _task or operator types_. To this end, we point out that all the operators in the pretraining dataset simply map the initial conditions to the solution at later times in time-dependent fluid flows on the two-dimensional unit square with _periodic boundary conditions_. While some of the downstream tasks (8 out of 15) do pertain to this type of operators, the remaining (7 out of 15) tasks involve different types of operators which include operators mapping the coefficients or PDE parameters to the PDE solution (5 out of 15), forcing term to the PDE solution (2) and domain shape to the PDE solution. Moreover, many of the downstream tasks are with non-periodic boundary conditions while one of them is even on a non-Cartesian domain. Thus, these downstream tasks deviate from the setup of the pretraining operators and provide a hierarchy of challenges for any foundation model.

Models and Baselines.We consider three different Poseidon models: i) Poseidon-T with \( 21\)M parameters, ii) Poseidon-B with \( 158\)M parameters, and iii) Poseidon-L with \( 629\)M parameters. The detailed specifications of each of these models is provided in **SM** C.1. As baselines, in addition to the standalone scOT, we use trained from scratch neural operators in the form of the widely used FNO  and recently proposed CNO , each augmented with time-conditioned instance normalizations. Foundation model baselines are provided by MPP-aVIT (MPP)  and we also pretrain a CNO  model (see details in **SM** C.5) on our pretraining dataset, resulting in an additional foundation model baseline termed CNO-FM, see **SM** C for details on baselines.

Evaluation Metrics.All the models and baselines are evaluated on each task in terms of the relative \(L^{1}\) error at the underlying final time. This choice is motivated by the fact that errors tend to grow over time, making final time prediction harder than any time-averaged quantities, see **SM** D.6.3. This also corresponds well to the interpretation of time-independent PDEs as long-time limits of (1). Following  that advocates this approach for LLMs, we evaluate all models in terms of _scaling_ curves which plot the test error for each task vs. the number of task-specific training examples, see **SM** D.1. To extract further information from scaling plots, we introduce two evaluation metrics,

\[_{}():=_{S}() }{_{S}()},_{}():=,\;\;_{s}()=_ {S}(),\] (11)

with \(_{S}()\) being the relative error (at final time) for the model with \(S\) trajectories. Thus, _Accuracy Gain_\(_{S}\) measures how accurate the model is w.r.t. FNO for a given number (\(S\)) of samples while _Efficiency Gain_\(_{S}\) measures how much fewer (greater) number of samples the model needs to attain the same error level as FNO trained on \(S\) samples. **AG** is the relevant metric for the _limited compute_ regime whereas **EG** is relevant for the _limited data_ regime.

Poseidon performs very well on all downstream tasks.From the scaling plots **SM** Figures 7 to 21, we observe that Poseidon readily outperforms FNO on _all the 15 downstream tasks_. This point is further buttressed by Table 1, where the **EG** and **AG** (11) metrics are presented (see also **SM** Table 8 for these metrics for the Poseidon-B and -T models). We observe from this table that Poseidon requires _far fewer_ task specific samples to attain the same error level as FNO does with \(S=1024\) samples for time-dependent PDEs (\(S=4096\) for time-independent PDEs). In fact, there are 4 tasks for which a mere 3 task-specific samples suffice for Poseidon to attain the same error as FNO with 1024 samples. From **SM** Table 9, we observe that, on an average (median), only 20 samples are needed for Poseidon-L to reach the errors of FNO with 1024 samples and in 13 (of the 15) tasks, Poseidon-L needs an order of magnitude fewer samples than FNO. Similarly from Table 1 and **SM** Table 9, we see that for the same number (\(S=128\) for time-dependent, and \(S=512\) for time-independent PDEs) of samples, Poseidon-L has significantly lower error than FNO, with gains ranging from anywhere between \(10\%\) to a factor of 25, with the mean gain of accuracy being an _entire order of magnitude_.

Among the trained-from-scratch neural operator baselines, CNO and scOT are comparable in performance to each other, while both outperform FNO significantly on almost all tasks (see Table 1 and **SM** Table 9). However, Poseidon is much superior to both of them, in terms of gains in sample efficiency (median gain of an order of magnitude) as well as accuracy (average gain of a factor of \(4\)).

Poseidon generalizes well to unseen physics.This impressive performance of Poseidon is particularly noteworthy as all the downstream tasks are _out-of-distribution_ with respect to the pre-training dataset. This performance is also consistent across the 9 tasks which involve PDEs not seen during pretraining. Poseidon is the best performing model on 8 of these tasks, including all the time-dependent PDEs. It is only for 1 of the time-indepedent PDEs, which constitute the hardest generalization challenge, that Poseidon is outperformed by CNO, but only marginally. These results underscore the ability of Poseidon to learn completely new physical processes and contexts from a few downstream task-specific samples.

Architecture of the foundation model matters.We observe from **SM** D.1 and Table 1 (see also **SM** Table 9) that Poseidon outperforms CNO-FM clearly on 14 out of 15 downstream tasks. On average (median over all tasks), CNO-FM requires approximately 100 task-specific examples to attain the error levels of FNO with 1024 samples, whereas Poseidon only requires approximately 20. As CNO-FM and Poseidon have been pretrained on exactly the same dataset, this difference in performance can be largely attributed to architectural differences as CNO-FM is based on multiscale CNNs, in contrast to the multiscale vision transformer which is the backbone of Poseidon.

The second baseline foundation model, MPP-B of , is based on a transformer with axial attention and is pretrained on the PDEBench dataset . However, it has been trained to predict the next time step, given a context window of \(\) previous time steps, with \(=16\) as the default. We emphasize that this next step prediction, given a context window, _does not solve the underlying operator learning task_ **OLT** directly as **OLT** requires that the entire trajectory needs to be generated, given the initial data. Hence, we had to finetune the pretrained MPP model with varying context windows (starting with window size of 1), see **SM** C.6 for details. We see from Table 1 and **SM** Table 9 that the finetuned MPP modestly outperformed FNO on some (8 out of 15) of the downstream tasks but it failed on the rest of them, where MPP simply could not attain the error levels of FNO, as it did not converge or even blew up with increasing number of downstream samples (see scaling plots in **SM** D.1).

In this context, it can be argued that the Poseidon-L model is larger in size than both CNO-FM and MPP-B and perhaps, it is this size difference which explains the differential in performance. However, this is far from the case. As shown in all the scaling plots of **SM** D.1 and **SM** Tables 8 and 9, both CNO-FM and MPP-B are significantly inferior to the Poseidon-B model, which is comparable in size. In fact, we can see from these tables that even the Poseidon-T model, which is an order of magnitude smaller in size, outperforms CNO-FM and MPP-B handily. It also readily outperforms all the trained-from-scratch neural operators (CNO, FNO and scOT) which are of comparable size to it, leading us to conclude that it is the combination of the pretraining dataset as well as the underlying architecture, rather than just model size, that underpins the superior performance of Poseidon.

**Poseidon scales with model size.** Nevertheless, the model size of Poseidon does matter. As seen from **SM** Figure 22, both the training as well as evaluation (validation) errors on the pretraining dataset clearly decrease with increasing model size of Poseidon. However, does this scaling with model size lead to any impact on the performance of these models, when finetuned on downstream tasks? We see from the scaling plots in **SM** D.1 that Poseidon-L consistently outperforms the smaller Poseidon-B on most downstream tasks. This trend is reinforced by **SM** Tables 8 and 9, where we find that, on an average, increasing model size correlates with a consistent decrease in test error as well as an increase in sample efficiency of the pretrained model on downstream tasks.

    &  &  \\   &  &  &  &  &  &  \\   & EG & _AG_ & EG & _AG_ & EG & _AG_ & EG & _AG_ & EG & _AG_ & EG & _AG_ \\   NS-PwC & **890.6** & _24.7_ & 16.6 & _3.3_ & _7.4_ & _2.3_ & _3.7_ & _1.5_ & _5.4_ & _2.0_ & \(1\) & \(1\) \\  NS-SVS & **502.9** & _7.3_ & 59.6 & _3.1_ & 34.8 & _2.2_ & 73.2 & _3.4_ & 10.2 & _1.2_ & \(1\) & \(1\) \\  NS-BB & **552.5** & _29.3_ & 10.6 & _3.9_ & 4.6 & _2.6_ & 2.7 & _1.7_ & 3.4 & _2.1_ & \(1\) & \(1\) \\  NS-SL & **21.9** & _5.5_ & 0.4 & _0.8_ & 0.3 & _0.8_ & 0.8 & _1.2_ & 0.3 & _0.8_ & 1 & \(1\) \\  NS-Tracer-PwC & **49.8** & _8.7_ & 17.8 & _3.6_ & 8.5 & _2.7_ & 4.6 & _1.9_ & 4.6 & _1.9_ & \(1\) & \(1\) \\  FNS-KF & **62.5** & _7.4_ & 13.2 & _2.7_ & 2.0 & _1.6_ & 3.1 & _1.5_ & 3.3 & _0.9_ & \(1\) & \(1\) \\  CE-RPUI & **352.2** & _6.5_ & 33.2 & _2.3_ & 0.0 & _1.2_ & 12.5 & _1.8_ & 15.6 & _2.1_ & \(1\) & \(1\) \\  CE-RM & **4.6** & _1.2_ & 0.6 & _1.0_ & 0.0 & _0.2_ & 1.7 & _1.1_ & 0.4 & _1.0_ & \(1\) & \(1\) \\  SE-AF & 3.4 & _1.2_ & 4.8 & _1.3_ & _2.2_ & _1.1_ & **5.5** & _1.5_ & _1.2_ & _1.0_ & \(1\) & \(1\) \\  GCE-RT & **5.3** & _2.0_ & 1.2 & _1.0_ & 0.0 & _0.3_ & 1.2 & _1.4_ & 1.1 & _1.1_ & \(1\) & \(1\) \\  Wave-Layer & **46.5** & _6.1_ & 5.6 & _2.2_ & 0.0 & _0.9_ & 11.4 & _3.0_ & 13.0 & _2.9_ & \(1\) & \(1\) \\  Wave-Gauss & **62.1** & _5.6_ & 6.0 & _1.8_ & 0.0 & _0.8_ & 14.0 & _2.6_ & 9.2 & _2.1_ & \(1\) & \(1\) \\  ACE & **17.0** & _11.6_ & 1.7 & _2.0_ & 0.0 & _0.3_ & 4.5 & _4.6_ & 6.5 & _5.2_ & \(1\) & \(1\) \\  Poisson-Gauss & **42.5** & _20.5_ & 25.0 & _9.2_ & 17.0 & _7.3_ & 21.1 & _7.0_ & 9.8 & _5.3_ & \(1\) & \(1\) \\  Helmholtz & **78.3** & _6.1_ & 54.0 & _5.1_ & 22.4 & _3.0_ & 68.9 & _7.3_ & 60.4 & _9.0_ & \(1\) & \(1\) \\   

Table 1: Efficiency gain EG ((11) with \(S=1024\) for time-dependent and \(S=4096\) for time-independent PDEs) and Accuracy Gain (_AG_) ((11) with \(S=128\) for time-dependent and \(S=512\) for time-independent PDEs) for all models and downstream tasks.

**Poseidon scales with dataset size.** In **SM** Figure 23, we show how by increasing the size of the pretraining dataset, in terms of the number of trajectories, the training and validation losses for the pretrained Poseidon-B model decrease. Moreover, from **SM** Figures 24 to 38, where we plot the test error versus number of downstream task-specific samples for 2 different models, Poseidon-B trained on the full pretraining dataset and on one-eighth of the pretraining dataset, we find that for most (9 of the 15) of the downstream tasks, increasing the number of samples in the pretraining dataset, by an order of magnitude, _does lead to significantly greater accuracy_ even at the downstream task level. For the remaining tasks, the models trained with less data are either on par or marginally inferior to the model trained with the full dataset.

**The quality/diversity of the pretraining dataset matters.** To demonstrate this point, we consider two different datasets: one in which half the trajectories of the pretraining dataset for Poseidon-B are randomly dropped (from every operator), and the other where less diversity of the pretraining dataset is imposed by dropping all the trajectories corresponding to 3 out of 6 operators, namely CE-CRP, CE-Gauss and NS-Sines. Thus, the total size of both datasets is the same but one is clearly less diverse than the other. The respective Poseidon-B models are then evaluated on all the downstream tasks. As shown **SM** Figures 24 to 38, the model trained on less diverse data performs worse than its counterpart on 10 out of the 15 tasks and is on par on 4 of them. Thus, we demonstrate that in a large majority of downstream tasks, the quality/diversity of the pretraining dataset matters.

**How does Poseidon generalize to unseen physics?** In order to understand the _surprising_ ability of Poseidon to generalize so well to unseen and _a priori_ unrelated PDEs and physical processes downstream, we present three case studies in **SM** D.4 to uncover some of the inner workings of this foundation model. In particular, we first consider the CE-RPUI downstream task. This task pertains to the compressible Euler equations, which are included in the pretraining dataset. However, the underlying initial data distribution is not seen during pretraining, making the task _out-of-distribution_. We show in **SM** D.4.1, how Poseidon leverages different features of different operators from the pretraining dataset to learn this task accurately with very few samples (see **SM** Figure 39). In particular, the diversity of the pretraining dataset is more instrumental in ensuring better generalization to this unseen initial condition than the size of the dataset.

In **SM** D.4.3, we study the Poisson-Gauss task to understand arguably the most surprising finding about the Poseidon foundation models, i.e., their ability to generalize well to PDEs that are completely unrelated to the Euler and Navier-Stokes equations of fluid dynamics. This task pertains to the Poisson equation (68) with a forcing term, which is a superposition of Gaussians. The task is very different from those seen during pretraining in multiple ways, namely the underlying PDE is not only time-independent (in contrast to the time-dependent PDEs of pretraining) but also elliptic (whereas the PDEs during pretraining are either hyperbolic or convection-dominated) and the boundary conditions are Dirichlet (instead of Periodic) leading to very different physics, that of diffusion and smoothing, being manifested for this task, when contrasted with the physics seen during pretraining which is dominated by transport, shock wave propagation and fluid mixing. Given this context, one would not expect Poseidon to perform well on this task. Yet, from **SM** Figures 20 and 74, we know that Poseidon performs exceptionally well, learning the solution operator accurately with a few samples. As we elaborate in **SM** D.4.3, Poseidon does not use the first few training examples to _forget_ the physics that it has learned during pretraining and learn the new physics for this task after that. Rather surprisingly, as illustrated in **SM** Figure 43, already with _one_ task specific training example, Poseidon outputs an (approximation of the) input, rather than the expected dynamic evolution of fluids with Gaussian inputs (see **SM** Figures 56 and 60) seen during pretraining. Then, with very few (16) examples, it is able to learn the rudiments of diffusion and smoothing of features (**SM** Figure 43), which are characteristics of elliptic equations. To further test how the foundation model leverages physics representations learned during pretraining, we _froze_ the latent space by only finetuning the embeddings and freezing the latent space parameters by setting \(_{r}=_{*}\) for all \(r\), in (10) for finetuning. As shown in (**SM** Figure 44), even this _frozen latent_ version of Poseidon is very effective at learning the underlying solution operator, demonstrating that very rich physical representations were learned during pretraining.

Further results on the robustness of Poseidon for different factors and ablations as well as comparisons with other foundation models is provided in **SM** D and details of computational resources are described in **SM** E.

## 4 Discussion

**Summary.** In this paper, we have presented Poseidon, a family of foundation models for learning PDEs. The backbone of Poseidon is scOT, a multiscale vision transformer with shifted-windowed (SwinV2) attention that maps input functions (initial data, coefficients, sources) etc. to the solution (trajectory) of a PDE. Lead-time conditioning through a time-modulated layer norm allows for continuous-in-time evaluation and a novel all2all training strategy enables the scaling up of training data by leveraging the semi-group structure of solutions of time-dependent PDEs. Poseidon is pretrained on a diverse large-scale dataset of operators for the compressible Euler and incompressible Navier-Stokes PDEs. Its performance is evaluated on a challenging suite of 15 _out-of-distribution_ downstream tasks covering a wide variety of PDEs and data distributions. Poseidon displays excellent downstream performance and is the best performing model on 14 of the 15 tasks. In particular, it requires orders of magnitude (median of \(50\)) fewer task-specific samples to attain the same error as the widely used FNO. This large gain in sample efficiency as well as order of magnitude gains in accuracy also holds for PDEs that are not seen during pretraining, making us conclude that Poseidon generalizes well to _new physics_. Poseidon also scales with model and dataset size, with respect to pretraining and even downstream task performance. To the best of our knowledge, this is the first time that it has been clearly demonstrated that by pretraining on a very small set of PDEs, a foundation model can generalize to a wider variety of unseen and unrelated PDEs and data distributions downstream. Thus, we provide an affirmative answer to the very fundamental question of whether foundation models for PDEs are even feasible. Moreover, we investigate possible mechanisms via which Poseidon can effectively leverage representations, learnt during pretraining, to accurately learn downstream tasks by finetuning on a few task-specific examples. Our case studies suggest hitherto undiscovered relationships between different PDEs that enable this transfer to materialize. Finally, all the models are made publicly available, as well as the pretraining and downstream datasets are open sourced in the PDEGym collection.

**Related Work.** Foundation models for PDEs are of very recent vintage. The foundation model of  is limited to very specific elliptic Poisson and Helmholtz PDEs with a FNO backbone whereas ICON  considers a very small 1-D dataset. Neither of these models are comparable in scope to Poseidon. Universal physics transformers  employs transformers but its focus is on incompressible fluid flows and the ability to generalize across Eulerian and Lagrangian data. Thus, a direct comparison with Poseidon is not possible. On the other hand, MPP  and DPOT  are designed to be general purpose foundation models for PDEs that can be compared to Poseidon. We have already extensively compared MPP with Poseidon in Section 3 to demonstrate the very large superiority of Poseidon across various metrics. Although DPOT has a different architecture (Adaptive FNO) and was trained on more datasets than MPP, it follows a similar training and evaluation strategy of next time-step prediction, given a context window of previous time-steps. As argued before, this does not solve the operator learning task of generating the entire trajectory, given initial data. At the time of writing this paper, DPOT was not publicly available but it was released by the time this paper has been revised, enabling us to modify the fine-tuning procedure of DPOT and to perform comparisons between it and Poseidon. While directing the interested reader to **SM** D.5 for details, we summarize our findings by observing that Poseidon models are significantly better performing than DPOT foundation models, both in terms of accuracy and sample efficiency.

**Limitations.** The range of PDEs and underlying data distributions is huge and Poseidon was only trained and evaluated on a few of them. Although the results here clearly demonstrate its ability to learn unseen physics from a few task-specific training examples, we anticipate that given that it is scaling with respect to both data quantity and quality, Poseidon's performance as a general purpose PDE foundation model will significantly improve when it is pretrained with even more diverse PDE datasets in the future. In particular, pretraining with time-independent PDEs (particularly elliptic PDEs) as well as a larger range of time-scales in time-dependent PDEs will greatly enhance Poseidon. The focus here was on Cartesian geometries although Poseidon displayed the ability to generalize to non-Cartesian geometries, via masking, on the SE-AF task. We plan to add several non-Cartesian examples in the pretraining dataset to augment Poseidon's performance on general geometries/boundary conditions. Moreover, given the fact that Poseidon serves as a fast and accurate neural PDE surrogate, its extension to qualitatively different downstream tasks such as uncertainty quantification , inverse problems  and PDE-constrained optimization  is fairly straightforward and will be considered in future work.