# Block Broyden's Methods for Solving Nonlinear Equations

Chengchang Liu

Department of Computer Science \(\&\) Engineering

The Chinese University of Hong Kong

7liuchengchang@gmail.com

&Cheng Chen\({}^{*}\)

Shanghai Key Laboratory of Trustworthy Computing

East China Normal University

chchen@sei.ecnu.edu.cn

&Luo Luo

School of Data Science

Fudan University

luoluo@fudan.edu.cn

&John C.S. Lui

Department of Computer Science \(\&\) Engineering

The Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

The corresponding author

###### Abstract

This paper studies quasi-Newton methods for solving nonlinear equations. We propose block variants of both good and bad Broyden's methods, which enjoy explicit local superlinear convergence rates. Our block good Broyden's method has a faster condition-number-free convergence rate than existing Broyden's methods because it takes the advantage of multiple rank modification on Jacobian estimator. On the other hand, our block bad Broyden's method directly estimates the inverse of the Jacobian provably, which reduces the computational cost of the iteration. Our theoretical results provide some new insights on why good Broyden's method outperforms bad Broyden's method in most of the cases. The empirical results also demonstrate the superiority of our methods and validate our theoretical analysis.

## 1 Introduction

In this paper, we consider solving the following nonlinear equation systems:

\[()=,\] (1)

where \(^{d}\), \(()}}{{=}}[F_{1}( ),,F_{d}()]^{}:^{d}^{d}\) and each \(F_{i}()\) is differentiable. Solving nonlinear equations is one of the most important problems in scientific computing . It has various applications including machine learning , game theory , economics  and control systems .

Newton's method and its variants  such as the Gauss-Newton method , the Levenberg-Marquart method  and the trust region method  are widely adopted to solve the systems of nonlinear equations. These methods usually enjoy fast local superlinear rates.

Newton's method takes iterates of form

\[_{t+1}=_{t}-((_{t}))^{-1}( _{t}),\]

where \(()^{d d}\) is the Jacobian at \(\). Since computing the inverse of the exact Jacobian matrix requires \((d^{3})\) running time, Newton's method suffers from expensive computation especially when solving the large-scale nonlinear equations .

Quasi-Newton methods have been proposed for avoiding the heavy computational cost of Newton-type methods while preserving good local convergence behaviour . Among these quasi-Newton methods, the Broyden's methods , including the good and the bad schemes , are considered to be the most effective methods for solving nonlinear equations. The Broyden's good method2 approximates the Jacobian \((_{t})\) by an estimator \(_{t}\) and updates the Jacobian estimator in each round as \(_{t+1}=_{t}+_{t}\). Here \(_{t}\) is a rank-\(1\) updating matrix constructed by the curvature information. Broyden et al. , Kelley and Sachs  proved that the good Broyden's method can achieve asymptotic local superlinear rates.

The bad Broyden's method approximates the inverse of the Jacobian by \(_{t}\) and updates the approximate matrix directly. Although the bad Broyden's method enjoys less computational cost than good Broyden's method in each iteration, it does not perform as well as the good method in most cases . Lin et al.  show that both the good and bad Broyden's methods have superlinear rates of \(((1/)^{t})\) and provide some insights on the difference between their empirical performance.

Ye et al.  proposed a new variant of good Broyden's method by conducting \(_{t}\) with a greedy or random strategy. Their method achieves a better explicit convergence rate of \(((1-1/d)^{t(t-1)/4})\). However, it remains unknown whether this convergence rate can be further improved by leveraging block updates which increase the reuse rate of the data in cache and take advantage of parallel computing . Gower and Richtarik  studied several random quasi-Newton updates including the Broyden's updates for approximating the inverse of matrices, but they only provide implicit linear rates for their methods. Liu et al.  established explicit convergence rates for several block quasi-Newton updates, but they focus on approximating positive definite matrices.

In this paper, we propose two random block Broyden's methods for solving nonlinear equations and provide their explicit superlinear convergence rates. We compare the theoretical results of proposed methods with existing Broyden's methods in Table 1 and summarize our contribution as follows:

* We provide explicit convergence rates for the block good Broyden's udpate and the block bad Broyden's update proposed by Gower and Richtarik . Our results show that the block good Broyden's update can approximate a nonsingular matrix \(\) with a linear rate of \((1-k/d)^{t}\) which improves the previous rate of \((1-1/d)^{t}\) where \(k}}{{=}}(_{t})\). We also show that the "bad" update can approximate the inverse matrix \(^{-1}\) with an linear rate of \((1-k/(d^{2}))^{t}\) where \(\) is the condition number of \(\). To the best of our knowledge, this is the first explicit convergence rate for the block bad Broyden's update.
* We propose the block good Broyden's method with convergence rate \(((1-k/d)^{t(t-1)/4})\) where \(k\) is the rank of the updating matrix \(_{t}\). This rate reveals the advantage of block update and improves previous results. Our method also relaxes the initial conditions stated in Ye et al. .
* We propose the block bad Broyden's method with convergence rate \(((1-k/(4dk^{2}))^{t(t-1)/4})\). We also study the initial conditions of two proposed block variants. Our analysis shows that bad Broyden's method is only suitable for the cases where the condition number of the Jacobian is small, while good Broyden's method performs well in most cases.

Paper OrganizationIn Section 2, we introduce the notation and assumptions as the preliminaries of this paper. In Section 3, we introduce the block good or bad Broyden's updates for approximating the general matrix. In Section 4, we propose the block good or bad Broyden's methods with explicit local superlinear rates. In Section 5, we discuss the behavior difference of the good and bad methods. We validate our methods by numerical experiments in Section 6. Finally, we conclude our results in Section 7. All proofs are deferred to appendix.

## 2 Preliminaries

We let \([d]}}{{=}}\{1,2,d\}\). We use \(\|\|_{F}\) to denote the Frobenius norm of a given matrix, \(\|\|_{2}\) to denote the spectral norm of a vector and Euclidean norm of a matrix respectively. The standard basis for \(^{d}\) is presented by \(\{_{1},,_{d}\}\) and \(_{d}\) is the identity matrix. We denote the trace, the largest singular value, and the smallest singular value of a matrix by \(()\), \(_{}()\), and \(_{}()\) respectively.

We use \(_{*}\) to denote the solution of the nonlinear equation (1) and \(_{*}\) to denote the Jacobian matrix at \(_{*}\), i.e., \(_{*}}}{{=}}(_ {*})\). We let \(}}{{=}}_{}(( _{*}))\), \(L}}{{=}}_{}((_ {*}))\) and then define the condition number of \(_{*}\) as \(}}{{=}}L/\). We also use \(}}{{=}}_{}( )/_{}()\) to present the condition number of given matrix \(\).

Then we present two standard assumptions on the nonlinear equations (1), which is widely used in previous works [16; 29; 50].

**Assumption 2.1**.: The solution \(_{*}\) of the nonlinear equation (1) is unique and nondegenerate, i.e.,

\[}}{{=}}_{}(_{*})>0.\]

**Assumption 2.2**.: The Jacobian \(()\) satisfies

\[\|()-_{*}\|_{2} M\|-_{*} \|_{2}\ \ \ \ \ ^{d}.\] (2)

The following proposition shows that if \(\) is in some local region of \(_{*}\), the Jacobian matrix \(()\) has a bounded condition number.

**Proposition 2.3**.: _Suppose Assumptions 2.1 and 2.2 hold. For all \(\) satisfies \(\|-_{*}\|_{2}^{2}/(6LM)\), we have_

\[_{}(())} _{}(())L.\]

We present two notations for the block Broyden's Update.

**Definition 2.4** (Block Good Broyden's Update).: Let \(\), \(^{d d}\). For any full column rank matrix \(^{d k}\), we define

\[(,,)+( -)(^{})^{-1} ^{}.\] (3)

**Definition 2.5** (Block Bad Broyden's Update).: Let \(\), \(^{d d}\). For any full column rank matrix \(^{d k}\), we define

\[(,,)+( _{d}-)(^{}^{ })^{-1}^{}^{}.\] (4)

## 3 The Block Broyden's Updates for Approximating Matrices

In this section, we provide the linear convergence rates of the block good and bad Broyden's updates for approximating matrices. The theoretical results is summarized in Table 2.

  
**Methods** & \((_{t})\) & **Convergence Rate** \\  Good/Bad Broyden’s Method [1; 6; 29] & \(1\) & \((1/t^{t/2})\) \\  Greedy/Randomized Good Broyden’s Method  & \(1\) & \((1-1/d)^{t(t-1)/4}\) \\  Block Good Broyden’s Method & \(k[d-1]\) & \((1-k/d)^{t(t-1)/4}\) \\ Algorithm 1 & \(k[d]\) & \((1-k/(4^{2}d))^{t(t-1)/4}\) \\  Block Bad Broyden’s Method & \(k[d]\) & \((1-k/(4^{2}d))^{t(t-1)/4}\) \\ Algorithm 2 & \(k[d]\) & \((1-k/(4^{2}d))^{t(t-1)/4}\) \\  

Table 1: We summarize the properties of Broyden’s methods for solving the Nonlinear equationsThe block good Broyden's update, which aims to compute an approximation of matrix \(\), can be written as:

\[_{t+1}=(_{t},,_{t}).\]

The following theorem presents a linear convergence rate of \((1-k/d)^{t}\) which is better than the rate \((1-1/d)^{t}\) provided by Gower and Richtarik , Ye et al. .

**Theorem 3.1**.: _Assume that \(^{d d}\) and \(_{0}^{d d}\). If we select \(_{t}=[_{i_{1}},_{i_{2}},,_{i_{ k}}]^{d k}\), where \(\{i_{1},,i_{k}\}\) are uniformly chosen from \(\{1,2,,d\}\) without replacement at each round, then for any nonsingular matrix \(^{d d}\), the block good Broyden's update satisfies_

\[\|(_{t+1}-)\|_{F}^{2}\| (_{t}-)\|_{F}^{2},\] (5)

_and_

\[\|(_{t}-)\|_{F}^ {2}(1-)^{t}\|(_{0}- )\|_{F}^{2}.\] (6)

On the other hand, the bad Broyden's update which targets to approximate \(^{-1}\) can be written as:

\[_{t+1}=(_{t},, _{t}).\]

Gower and Richtarik  provide an implicit rate of \((1-)^{t}\) for the above scheme with \([0,k/d]\), but their analysis cannot guarantee an explicit \(\). In the following theorem, we show that the block bad Broyden's update can approximate \(_{t}\) to \(^{-1}\) with an explicit linear rate of \((1-k/(^{2}d))^{t}\).

**Theorem 3.2**.: _Assume that \(^{d d}\) and \(_{0}^{d d}\). If we select \(_{t}=[_{i_{1}},_{i_{2}},,_{i_{ k}}]^{d k}\) where \(\{i_{1},,i_{k}\}\) are uniformly chosen from \(\{1,2,,d\}\) without replacement at each round, then for any nonsingular matrix \(^{d d}\), the block bad Broyden's update satisfies_

\[\|(_{t+1}-^{-1})\|_{F}^{2} \|(_{t}-^{-1})\|_{F}^{2},\] (7)

_and_

\[\|(_{t}-^{-1})\|_ {F}^{2}(1-^{2}})^{t}\|( _{0}-^{-1})\|_{F}^{2}.\] (8)

_Remark 3.3_.: If we choose \(=_{d}\) in Theorem 3.1 and Theorem 3.2, then the measures in these two theorems are exactly the same as the one in Section 8.5 and Section 8.3 of . Besides, the rate of Theorem 3.1 recovers the convergent rates of Section 8.5 in  and Lemma 4.1 in  when we take \(k=1\).

## 4 The Block Broyden's Methods

In this section, we propose two block Broyden's methods for solving the nonlinear equation (1). We present our algorithms in section 4.1 and the corresponding convergence results in Section 4.2.

  
**Updates** & **Previous Results** & 
 **Improved Results** \\ Theorem 3.1/3.2 \\  & **Measure** \\  Block Good & \((1-)^{t}\)\({}^{}\) & \((1-)^{t}\) & \([\|(_{t}-)\|_{F}^{2}]\) \\  Block Bad & \((1-)^{t}\)\({}^{}\) & \((1-^{2}})^{t}\) & \([\|(_{t}-^{-1})\|_{F}^{2}]\) \\   \\ ]\), but do not provide the explicit value of \(\).} \\ 

Table 2: We summarize the properties of Broyden’s updates for approximating a given nonsingular matrix \(\) or \(^{-1}\).

### Algorithms

By using the block Broyden's updates in Section 3, we propose two novel algorithms called Block Good Broyden's Method (BGB) and Block Bad Broyden's Method (BBB) for solving nonlinear equations.

We present the BGB algorithm in Algorithm 1 which updates the Jacobian estimator \(_{t}\) by the block good Broyden's update in each iteration. Notice that the inverse of \(_{t}\) can be computed efficiently by adopting Sherman-Morrison-Woodbury formula . On the other hand, the BBB algorithm, which is presented in Algorithm 2, approximates the inverse of the Jacobian directly by using the block bad Broyden's update. It usually has a lower computational cost than the BGB algorithm in each round because the BBB algorithm does not need to compute the inverse of the estimator \(_{t}\).

```
1:Input: Initial estimator \(_{0}\), initial point \(_{0}\) and block size \(k\).
2:for\(t=0,1\)
3:\(_{t+1}=_{t}-_{t}(_{t})\).
4: Choose \(\{i_{1},,i_{k}\}\) by uniformly select \(k\) items from \(\{1,,d\}\) without replacement.
5:\(_{t}=[_{i_{1}},,_{i_{k}}]^{d  k}\).
6:\(_{t+1}=(_{t},(_{t +1}),_{t})\).
7:endfor ```

**Algorithm 2** Block Bad Broyden's Method (BBB)

_Remark 4.1_.: Algorithms 1 and 2 do not require full information of the Jacobian. We construct \(}\) by subsampling the columns of the identity matrix. When updating the Jacobian estimator by the block updates, we need to calculate \(_{t+1}_{t}\) which is only the partial information of \(_{t+1}\) (columns of \(_{t+1}\)). Since we have \(k d\), it is not expensive to access the partial information of the Jacobian.

### Convergence Analysis for the Block Broyden's Methods

We provide the convergence analysis for Algorithm 1 and Algorithm 2 in Section 4.2.1 and Section 4.2.2 respectively. We denote the Jabocbian matrix at \(_{t}\) as \(_{t}\). As previous works [16; 29; 50], we make an assumption on the estimator matrices in Algorithm 1 and Algorithm 2 as follows:

**Assumption 4.2**.: We assume the sequence \(\{_{t}\}_{t=0}^{}\) generated by Algorithm 1 (and \(\{_{t}\}_{t=0}^{}\) generated by Algorithm 2) are well-defined and nonsingular.

#### 4.2.1 Analysis for Block Good Broyden's Methods

In this subsection, we use the following measures for our convergence analysis,

\[r_{t}}}{{=}}\|_{t}-_{*} \|_{2}_{t}}}{{=}}\| _{*}^{-1}(_{t}-_{*})\|_{F}.\]

The \(r_{t}\) measures the distance between \(_{t}\) and the solution \(_{*}\) and \(_{t}\) measures how well does the estimator matrix \(_{t}\) approximate the Jacobian at \(_{*}\).

The following lemma provides upper bound of \(_{t}\) after one block Broyden's update.

**Lemma 4.3**.: _Performing Algorithm 1 under Assumptions 2.1, 2.2 and 4.2, we have_

\[_{t+1}_{t}+}{}r_{t+1}[_{t+1}]}_{t}+}{} r_{t+1}.\] (9)

Based on Lemma 4.3, we present the superlinear convergence rate for Algorithm 1.

**Theorem 4.4**.: _Suppose Assumptions 2.1, 2.2 and 4.2 hold and the initial condition of Algorithm 1 satisfies_

\[r_{0}}{}\{,\}_{0}\] (10)

_for arbitary \(q(0,1)\). Then for any \(k[d-1]\), the output of Algorithm 1 satisfies_

\[[\|_{*}^{-1}(_{t}-_{*})\|_{F} ] 2(1-)^{t/2},\]

_and_

\[[_{t+1}-_{*}\|_{2}}{\|_ {t}-_{*}\|_{2}}] 4(1-)^{t/2}.\]

Theorem 4.4 implies the following high probability bound for Algorithm 1.

**Corollary 4.5**.: _Performing Algorithm 1 under the same assumption and initial condition as Theorem 4.4, with probability at least \(1-\), we have_

\[\|_{*}^{-1}(_{t}-_{*})\|_{F}d^{2}}{k^{2}}(1-)^{t/2},\] (11)

_and_

\[\|_{t}-_{*}\|_{2}(d^{2}}{k^{2} })^{t}(1-)^{t(t-1)/4}\|_{0}- _{*}\|_{2}.\] (12)

Comparison with Compare Theorem 4.4 with Theorem 4.3 of , we can find that the convergence rate of our BGB algorithm is better than greedy and randomized good Broyden's methods  if we choose \(k>1\).

On the other hand, the initial condition of greedy and randomized good Broyden's methods  is

\[\|_{0}-_{*}\|_{2}=(} )\ \ \ \ \|_{0}-_{0}\|_{F}=(),\] (13)

while the condition of Theorem 4.4 can be reformulated as

\[\|_{0}-_{*}\|_{2}=(} )\ \ \ \ \|_{*}^{-1}(_{0}-_{*})\|_{F}= (1).\] (14)

Since

\[\|_{*}^{-1}(_{0}-_{*})\|_{F} \|_{*}^{-1}(_{*}-_{0})\|_{F}+ \|_{*}^{-1}(_{0}-_{0})\|_{F}\] \[}{}\|_{0}-_{*}\|_{2}+ \|_{0}-_{0}\|_{F}=(1),\]

condition (13) can implies condition (14). However, the reverse is not always true. For example, we can choose \(_{0}=1.5_{*}\) and suppose

\[_{0}=_{*}=3&0\\ 0&10^{-10}.\]

Then we have \(\|_{*}^{-1}(_{0}-_{*})\|_{F}=\| _{2}\|_{F}=(1)\) while \(\|_{0}-_{0}\|_{F}=\|_{*}\|_{F} 10 ^{-10}=\).

Overall, compared with the greedy or randomized good Broyden's method , Theorem 4.4 not only gives a faster convergence superlinear rate by leveraging the idea of block update, but also weakens the initial condition by using different measures in the analysis.

#### 4.2.2 Analysis for Block Bad Broyden's Methods

This subsection gives the convergence analysis for Algorithm 2. We use the following measures to describe the convergent behavior

\[R_{t}}}{{=}}\|_{*}( _{t}-_{*})\|_{2}_{t}}}{{=}}\|_{*}(_{t}-_{*}^{-1})\|_{F}.\]

The \(R_{t}\) measures the distance between \(_{t}\) and the solution \(x_{*}\) and \(_{t}\) measures how well does the estimator \(_{t}\) approximate the matrix \(_{*}^{-1}\).

Using the convergence results for the block bad Broyden's update in Theorem 3.2, we are able to tackle the difference between the estimator \(_{t}\) and the matrix \(_{*}^{-1}\) after one block update in Algorithm 2.

**Lemma 4.6**.: _Performing Algorithm 2 under Assumptions 2.1, 2.2 and 4.2 and suppose the sequence \(\{_{t}\}_{t=0}^{}\) generated by Algorithm 2 satisfies that \(\|_{t}-_{*}\|_{2}^{2}/(6LM)\), we have_

\[_{t+1}_{t}+}{^{2}} R_{t+1}^{2 }[_{t+1}]d} _{t}+}{^{2}}} R_{t+1}.\] (15)

We can establish the superlinear convergence of the block bad Broyden's method based on Lemma 4.6.

**Theorem 4.7**.: _Suppose Assumptions 2.1, 2.2 and 4.2 hold and the initial condition of Algorithm 2 satisfies_

\[R_{0}}{^{2}}\{, {q}{2},}{3}\}_{0}\] (16)

_for arbitrary \(q(0,1)\). Then for \(k[d]\), the output of Algorithm 2 satisfies_

\[[\|_{*}(_{t}-_{*}^{- 1})\|_{F}](1-})^{t/2},\]

_and_

\[[_{*}(_{t+1}-_{*})\|_{2}}{\|_{*}(_{t}-_{*})\|_{2}}]  2(1-})^{t/2}.\]

Similar to Corollary 4.5, we can also obtain the high probability bound for Algorithm 2.

**Corollary 4.8**.: _Performing Algorithm 2 under the same assumption and initial condition as Theorem 4.7, with probability at least \(1-\), we have_

\[\|_{*}(_{t}-_{*}^{-1})\|_{F} d^{2}^{4}}{ k^{2}}(1-+k })^{t/2},\] (17)

_and_

\[\|_{*}(_{t}-_{*})\|_{2}( d^{2}^{4}}{k^{2}})^{t}(1-+k})^{t(t-1)/4}\|_{*}(_{0}-_{*}) \|_{2}.\] (18)

## 5 Discussion

In this section, we discuss the performance difference between the good and bad Broyden's methods which is considered as an important open problem in the field of nonlinear equations .

We first discuss the different performance of the block Broyden's methods (Algorithm 1 and 2). Notice that the "good" method enjoys a condition-number-free superlinear rate of \(((1-k/d)^{t(t-1)/4})\) and the initial conditions of \(_{0}\) and \(_{0}\) are \(\|_{*}^{-1}(_{0}-_{*})\|_{F}=(1) \|_{0}-_{*}\|_{2}=( })\) respectively. On the other hand, both the superlinear rate \(((1-k/(4d^{2}))^{t(t-1)/4})\) and initial conditions \(\|_{*}(_{0}-_{*}^{-1})\|_{F}=(\{ 1,/\})\), \(\|_{*}(_{0}-_{*})\|_{2}=(^{2}/(M ))\) for \(_{0}\), \(_{0}\) of the "bad" method depend on \(\) heavily. Thus we think these two block Broyden's methods are suitable for different scenarios:* The "good" method is more suitable for the cases of large condition number (\( 1\)) because its convergence rate is condition-number-free and its initial condition has weaker dependency on \(\) than the "bad" method.
* The 'bad" method may have better performance when \(=(1)\) because under this case the convergence rates do not differ much between the "good" and "bad" method while the latter one usually has a cheaper computational cost per iteration.

The condition number is very large in most of the cases which means the "good" method generally outperforms the "bad" one. We summarize the different convergence rates, initial conditions and suitable scenes of the block good and bad Broyden's methods in Table 3.

The similar phenomenon also holds for the classical good and bad Broyden's methods , whose iterations can be reformulated as

\[_{t+1}=_{t}-_{t}^{-1} (_{t}),\\ _{t+1}=(_{t},}_{ t+1},_{t})=_{t}+_{t}-_{t} _{t})_{t}^{}}{_{t}^{}_{t}} ,\] (19)

and

\[_{t+1}=_{t}-_{t} (_{t}),\\ _{t+1}=(_{t},}_{t+1},_{t})=_{t}+_{t}- _{t}_{t})_{t}^{}}{_{t}^{} _{t}}\] (20)

respectively, where \(_{t}=_{t+1}-_{t}\), \(}_{t+1}=_{0}^{1}(_{t}+s_{t}) s\) and \(_{t}=(_{t+1})-(_{t})\). The different convergent behavior of the block Broyden's updates helps us understand the performance difference between the classical good and bad Broyden's methods for the similarity of their frameworks.

## 6 Experiments

We validate our methods on the Chandrasekhar H-equation which is well studied in the previous literature [25; 29; 50] as follows

\[F_{i}()=x_{i}-(1-_{j=1}^{N} {_{i}x_{j}}{_{i}+_{j}})^{-1},\] (21)

where \(=[x_{1},,x_{N}]^{}^{N}\) and \(()=[F_{1}(),,F_{N}()]^{} ^{N}\). We denote GB-Cl and BB-Cl as the classical good and bad Broyden's methods respectively [1; 29]. We denote GB-Gr and GB-Ra as the greedy and randomized Broyden's methods  respectively. Our experiments are conducted on a PC with Apple M1 and all algorithms are implemented in Python 3.8.12.

Our first experiment considers three cases: \(N=200\), \(N=300\), \(N=400\). We set \(c=1-10^{-12}\) for the H-equation and choose the block size \(k=N/10\) for the proposed methods. In all cases, we use the same inputs \(_{0}=0.1_{N}\) (\(_{0}=10_{N}\)) for all algorithms. We use classical Newton method as the warm-up algorithm to obtain \(_{0}\) which satisfies the local condition and take it as the initial point for all methods. We compare the proposed BGB and BBB algorithm with baselines and present the results of iteration number against \(\|()\|_{2}\) and running time against \(\|()\|_{2}\) in Figure 1. We observe that the proposed block good Broyden's method (BGB) outperforms the baselines in all cases, but the block bad Broyden's method (BBB) does not perform very well. This is mainly because \(\) is very large in this setting (\( 10^{6}\)). We also note that the classical Broyden's methods (GB-Cl and BB-Cl) are numerical unstable. Specifically, they do not guarantee the descent of \(\|(_{t})\|_{2}\) and encounter nan value during the iterations. The BB-Cl algorithm even fails to converge after some iterations. Such instability of the classical Broyden's methods is also observed in the previous literature .

Our second experiment explores the performance of the proposed block Broyden's methods with different block size. We also study whether BBB algorithm has good performance for the nonlinear equation which Jacobian of the solution small condition number. By fixing \(N=400\) and setting \(c=\{1-10^{-1},1-10^{-3},1-10^{-5}\}\), we obtain different condition numbers of (21) as \(=2,31,327\). We present the results in Figure 2. For each \(\), we also vary the block size \(k=\{1,10,100\}\) for BBB and BBB algorithms. We observe that when \(=(1)\), BBB outperforms BGB in terms of the CPU time (Figure 2 (d), (e)). which matches our analysis in section 5. We also find that larger block size \(k\) will lead to faster convergence in terms of the iterations ((a), (b), (c) of Figure 2), which verifies our theoretical results in section 4.2.

## 7 Conclusion

In this paper, we have proposed the block Broyden's methods for solving nonlinear equations. The proposed block good Broyden's method enjoys a faster superlinear rate than all of the existing Broyden's methods. We have also shown that the block bad Broyden's update approximates the inverse of the object matrix with an explicit linear rate and proposed the block bad Broyden's method accordingly. The established convergence results for the block good and bad methods bring us new understanding on the performance difference between the good and bad Broyden's methods. Especially, they can explain why good Broyden's method generally outperforms the "bad" one.

For the future work, it is possible to incorporate the safeguard mechanism in Wang et al.  to remove the assumption on the Jacobian estimator (Assumption 4.2). It will also be interesting to study the global behavior based on the recent advance in Jiang et al.  and design efficient stochastic or sketched algorithms  for solving nonlinear equations.

Figure 1: We demonstrate iteration numbers vs. \(\|()\|_{2}\) and CPU time (second) vs. \(\|()\|_{2}\) for H-equation with different equation numbers \(N\).