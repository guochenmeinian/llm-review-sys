# Rank-1 Matrix Completion with

Gradient Descent and Small Random Initialization

 Daesung Kim

Samsung Electronics

dskim95phd@gmail.com

&Hye Won Chung

School of Electrical Engineering

KAIST

hwchung@kaist.ac.kr

###### Abstract

The nonconvex formulation of the matrix completion problem has received significant attention in recent years due to its affordable complexity compared to the convex formulation. Gradient Descent (GD) is a simple yet efficient baseline algorithm for solving nonconvex optimization problems. The success of GD has been witnessed in many different problems in both theory and practice when it is combined with random initialization. However, previous works on matrix completion require either careful initialization or regularizers to prove the convergence of GD. In this paper, we study the rank-1 symmetric matrix completion and prove that GD converges to the ground truth when small random initialization is used. We show that in a logarithmic number of iterations, the trajectory enters the region where local convergence occurs. We provide an upper bound on the initialization size that is sufficient to guarantee the convergence, and show that a larger initialization can be used as more samples are available. We observe that the implicit regularization effect of GD plays a critical role in the analysis, and for the entire trajectory, it prevents each entry from becoming much larger than the others.

## 1 Introduction

Recovering a low-rank matrix from a set of linear measurements is at the heart of many statistical learning problems. Depending on the structure of the matrix and the linear measurements, it reduces to various problems such as phase retrieval , blind deconvolution , and matrix sensing . Matrix completion  is also one such type of problem where each measurement provides an entry of the matrix, and the goal is to recover the low-rank matrix from a partial, usually very sparse, observation of the entries. One of the most notable applications of matrix completion is collaborative filtering , which aims to predict user preferences for items based on a highly incomplete observation of user-item ratings. There are also a number of different applications, such as principal component analysis  and image reconstruction , just to name a few.

Extensive amount of work has been dedicated to provide an efficient recovery algorithm for matrix completion with theoretical guarantees . The convex relaxation based nuclear norm minimization [4; 9] was the first algorithm proven to recover the matrix with near optimal sample complexity. Despite its theoretical success, the convex algorithm was found hard to be used in practical scenarios due to its unaffordable computational complexity and memory size. Therefore, the nonconvex formulation of matrix completion with quadratic loss has received significant attention in recent years. Many different algorithms have been proposed for the nonconvex problem, and their convergence toward the ground truth has been analyzed. Examples include optimization on Grassmann manifolds , alternating minimization , projected gradient descent , gradient descent with regularizer , and (vanilla) gradient descent [14; 15].

Gradient descent (GD) has served as a baseline algorithm for solving nonconvex optimization problems. However, the convergence of GD to global minimizers is not guaranteed, and it can take exponential time to escape saddle points . Nevertheless, GD with random initialization has been shown to successfully recover the global minimum in many different problems such as phase retrieval , matrix sensing , matrix factorization , and neural network training . Previous work on matrix completion  proved the convergence of GD under the spectral initialization, which locates the initial point in the local region of the minima. However, the role of random initialization in solving matrix completion with GD is not fully understood yet, although its success is observed in practice. Therefore, we aim to answer the following question:

_Can GD with random initialization solve the nonconvex matrix completion problem?_

We answer this question affirmatively and show that GD with small random initialization successfully converges to the ground truth for rank-1 symmetric matrix completion. In the analysis, we use vanilla GD, which does not incorporate any modifications, such as regularization or truncation, into the GD algorithm. We also characterize the entire trajectory that GD follows by showing that the trajectory is well approximated by the fully observed case. The small initialization plays a critical role in analyzing the trajectory of the early stages, where the randomly initialized vector is nearly orthogonal to the first eigenvector of the ground truth matrix. We provide a bound on the required initialization size for the algorithm to converge, and our bound suggests that one can use a larger initialization to improve the convergence speed as more samples are provided. However, in any case, GD with a small random initialization takes only logarithmic amount of time (with respect to the matrix dimension) to reach the point where local convergence can begin. To the best of our knowledge, this is the first result on matrix completion that proves the convergence of vanilla GD without a carefully designed initialization.

Although our result is restricted to the rank-1 case, we believe that this work provides an important evidence for understanding the more general rank-\(r\) case. At the end of this paper, we will discuss some technical difficulties that the rank-\(r\) case naturally has, and provide some empirical results related to them. However, studying the rank-1 matrix completion problem is not only motivated by theoretical interest, but the problem itself also appears in some practical problems such as crowdsourcing .

**Related Works** This work is motivated by the recent success of small initialization in matrix factorization and matrix sensing. It was first conjectured in  that sufficiently small step sizes and initialization lead GD to converge to the minimum nuclear norm solution of a full-dimensional matrix sensing problem. The conjecture was proved in  for the fully overparameterized matrix sensing under the standard restricted isometry property (RIP). A recent study by  provided more general results by showing that the early iterations of GD with small initialization have spectral bias. Many other works such as  have also studied how GD or gradient flow with small initialization implicitly forces the recovered matrix to be low-rank. However, the recovery guarantee for matrix completion has not been provided by any work.

For the matrix sensing where RIP holds, the loss function has global benign geometry in that it does not contain any spurious local minima or non-strict saddle points . In the case of matrix completion, a similar result was obtained but with a regularizer that penalizes the matrices with large rows . Controlling the norm of each row (absolute value of each entry in the case of rank-1) is the biggest hurdle in the analysis of matrix completion. In the local convergence analysis of , it was proved that GD implicitly regularizes the largest \(_{2}\)-norm of the rows of error matrices, showing that explicit regularization is unnecessary. In this paper, we also prove that such an implicit regularization is induced by GD when it starts from a point of small size. We show that the trajectory is close to the fully observed case in both \(_{2}\) and \(_{}\) norms. Thus, the trajectory is confined to the region where it has benign geometry, and GD can converge without an explicit regularizer.

**Notations** We denote vectors with lowercase bold letters and matrices with uppercase bold letters. The components or entries of them are written without bold. We use \(_{2}\) and \(_{}\) to denote \(_{2}\) and \(_{}\)-norm of vectors, respectively, and \(_{}\) is used for Frobenius norm of matrices. For any norm \(\) and two vectors \(,\), we let \(=\{+,-\}\). Asymptotic dependencies with respect to the matrix dimension are denoted with the standard big \(O\) notations, or with the symbols, \(,\) and \(\).

Problem Formulation

The matrix completion problem aims to reconstruct a low-rank matrix from partially observed entries. In this paper, we focus on the case where the ground truth matrix, denoted by \(^{}^{n n}\), is a rank-1 positive semidefinite matrix. Thus, the ground truth matrix is decomposed as \(^{}=^{}^{}{^{}}^{}\) with \(^{}>0\) and a unit vector \(^{}\). We define \(^{}=}^{}\) so that \(^{}=^{}{^{}}^{}\). To follow the standard incoherence assumption, we let \(\|^{}\|_{}=}\) and allow \(\) to be as large as \(( n)\). We consider a random sampling model that is also symmetric as \(^{}\). Each entry in the diagonal and the upper (or lower) triangular part of \(^{}\) is independently revealed with probability \(0<p 1\). We consider the noisy case where Gaussian noise is added to each observation. Formally, we get as an observation the matrix \(^{}\) whose \((i,j)\)th entry is \(_{ij}(M^{}_{ij}+E_{ij})\), where \([_{ij}]_{1 i j n}\) are independent Bernoulli random variables with expectation \(p\) and \([E_{ij}]_{1 i j n}\) are independent Gaussian random variables with the distribution \((0,^{2})\). They are both symmetric in the sense that \(_{ij}=_{ji}\) and \(E_{ij}=E_{ji}\) for all \(1 i j n\). We use \(\) to denote the symmetric matrix whose entries are \(E_{ij}\). We denote the set of observed entries as \(:=\{(i,j)_{ij}=1\}\), and define an operator \(_{}\) on matrices that sets the entries not contained in \(\) to zero. (e.g. \(^{}=_{}(^{}+)\))

To recover the matrix \(^{}\), we find \(^{n}\) that minimizes the nonconvex loss function \(f()\), which is the sum of the squared differences on the observed entries. It is explicitly written as \(f():=_{(i,j)}(x_{i}x_{j}-x_{i}^{}x_{j}^{ }-E_{ij})^{2}\). We apply vanilla GD to solve the optimization problem starting from a small randomly initialized vector \(^{(0)}\). Each entry of \(^{(0)}\) is sampled independently from the Gaussian distribution \((0,_{0}^{2})\), so that the squared norm of \(^{(0)}\) is expected to be \(_{0}^{2}\). The update rule of GD is written as

\[^{(t+1)}=^{(t)}- f(^{(t)})=^{(t)} -_{}(^{(t)}^{(t)}) ^{(t)}+^{}^{(t)},\] (1)

where \(>0\) is the step size.

We define \(F\) as the loss function \(f\) when all entries of \(^{}\) are observed without noise, i.e., \(F():=^{}-^{}_{ }^{2}\). We also define \(}^{(t)}\) as the trajectory of GD when it is applied to \(F\) with the same initial point \(^{(0)}\), i.e., \(}^{(t)}\) is the trajectory of the fully observed case. Specificially, it evolves with

\[}^{(t+1)}=}^{(t)}- F(}^{(t)})=}^{(t)}-}^{(t)} _{2}^{2}}^{(t)}+^{}}^ {(t)}\] (2)

from the same starting point \(}^{(0)}=^{(0)}\).

Lastly, we introduce the so-called _leave-one-out_ sequences. These were the main ingredient in controlling the \(_{}\)-norm of trajectory in . We use them for a similar purpose. For each \(l[n]\), we define an operator \(_{}^{(l)}\) such that \(_{}^{(l)}()\) is equal to \(\) on the \(l\)th row and column, and equal to \(_{}()\) otherwise. The \(l\)th leave-one-out sequence, \(^{(t,l)}\), evolves with

\[^{(t+1,l)}=^{(t,l)}-_{}^{(l)}^{(t, l)}^{(t,l)}^{(t,l)}+^{(l)}^{(t,l)},\] (3)

for \(^{(0,l)}=^{(0)}\), where \(^{(l)}=_{}^{(l)}(^{})+^{(l)}\), and \(^{(l)}\) is obtained by zeroing out the \(l\)th row and column of \(_{}()\).

## 3 Main Results

In this section, we present our main results. The first main result concerns the global convergence of GD with small random initialization.

**Theorem 3.1**.: _Let us consider a rank-1 matrix completion problem that recovers the matrix \(^{}=^{}{^{}}^{}^{n n}\) such that \(\|^{}\|_{2}=}\) and \(\|^{}\|_{}=}\|^{ }\|_{2}\), where \(=O(( n))\). Let the initial point \(^{(0)}^{n}\) be sampled from the Gaussian distribution \((,_{0}^{2})\) and \(^{(t)}\) be updated with (1). Suppose that a small step size with \(^{}<0.1\) is used and the sample complexity satisfies \(n^{2}p^{5}n^{22}n\). Then, there exists \(T^{}=(1+o(1))}n}} {_{0}}\) such that_\[\|^{(t)}^{}\|_{2} }\|^{}\|_{2},\] (4) \[\|^{(t)}^{}\|_{} }\|^{}\|_{},\] (5) \[_{1 l n}\|(^{(t,l)}-^{})_{l}\| }\|^{}\|_{}\] (7)

_hold at \(t=T^{}\) with probability at least \(1-o(1/)\), if a sufficiently small initialization with_

\[}n^{-10}_{0}} ^{26}n}}}\] (8)

_is used and the noise satisfies \(}{n}\)._

Theorem 3.1 proves that, starting from a small random initialization, the trajectory of GD eventually enters the local region of the global minimizers \(^{}\) in terms of both \(_{2}\) and \(_{}\) norms. Combined with the result of , GD starts to converge linearly to either \(^{}\) or \(-^{}\) after \(t=T^{}\), as stated in the corollary below.

**Corollary 3.2**.: _Suppose that the conditions in Theorem 3.1 are satisfied, and let \(\) be a constant such that \(1-<1\). Then, with probability at least \(1-o(1/)\), we have_

\[\|^{(t)}^{}\|_{2} (}^{t-T^{}}+}})\|^{}\| _{2},\] (9) \[\|^{(t)}^{}\|_{} (}^{t-T^{}}+}})\|^{}\| _{},\] (10)

_for all \(T^{} t T=O(n^{5})\)._

The desired global convergence result is provided by Corollary 3.2. Several remarks about Theorem 3.1 and Corollary 3.2 are in order.

Matrix RecoverySuppose \(^{(t)}\) converges to a global minimum \(^{}\) of the function \(f\), which is different from \(^{}\). In such a case, despite achieving global convergence, the reconstructed matrix \(^{}^{}\) deviates from the ground truth matrix \(^{}\). However, Theorem 3.1 establishes that \(^{(t)}\) converges exclusively to the correct global minima \(^{}\), so that the matrix \(^{}\) is recovered with high probability.

Leave-one-out SequenceTo apply the local convergence result of , in addition to (4) and (5), the existence of leave-one-out sequences \(\{^{(t,l)}\}_{l[n]}\) satisfying (6) and (7) is required. Leave-one-out sequences also play a critical role and appear naturally in the proof of Theorem 3.1.

Sample ComplexityThe required sample complexity for Theorem 3.1 to hold is optimal up to a logarithmic factor compared to the statistical lower bound of \((n n)\). We have not done our best to optimize the \(\) factors, and about half of them can be reduced with more delicate analysis. We will discuss this briefly in Section 6.

Convergence TimeConsidering that \(_{0}^{-1}\) is at most polynomial in \(n\) (due to the lower bound of (8)), only \(O( n)\) iterations are required for GD to enter the local region. It takes \(O(())\) more iterations to achieve \(\)-accuracy in the local region, so the total iteration complexity is given by \(O( n)+O(())\).

Initialization SizeAlthough small initialization provides a good geometry to GD, a larger initialization is preferred because the convergence time, \(T^{}\), is inversely proportional to \(_{0}\). When the sample complexity is optimal, i.e., \(n^{2}p n( n)\), an upper bound on the initialization size given by Theorem 3.1 is \(n^{-}\), ignoring the log factors. However, as more samples are provided, we are allowed to use a larger initialization to reduce the convergence time. When the sample complexity satisfies \(n^{2}p n^{1+a}\), the bound is \(n^{-(1-a)}\) ignoring the log factors. The bound becomes nearly constant as \(a\) approaches \(1\), namely the fully observed case, and this is consistent with the previous result that small initialization is unnecessary for the fully observed case . We also note that the lower bound of (8) is necessary in the proof of Theorem 3.1, since we derive probabilistic bounds for all iterations, and the lower bound limits the maximum number of iterations. However, we can further reduce the lower bound \(n^{-10}\) to \(n^{-c}\) for any constant \(c>10\) by tuning some constant factors during the proof.

**Noise Size** From the incoherence assumption, the maximum absolute value of entries of \(^{}\) is bounded by \(}{n}\). The condition \(}{n}\) in Theorem 3.1 allows the standard deviation of the Gaussian noise to be much larger than the maximum entry. It also implies \(}}}\), so that the upper bounds in Corollary 3.2 are dominated by the first terms at \(t=T^{}\) and they eventually converge to the second terms as \(t\) increases.

**Estimation Error** The current estimation bounds (4) to (7) are all proportional to \(}\) times the norms of \(^{}\). However, if we do not allow the initialization size to grow with the sample complexity, we are able to obtain tighter bounds; if we use the fixed initialization size \(n^{-}\) regardless of the sample complexity, in Theorem 3.1, the factor \(}\) is improved to \(}+}}\), and the upper bound on noise size is also improved to \(}{n}\) (not being precise on the factors of \(\) and \( n\) here). Then, the estimation error in Corollary 3.2 is improved to \(}^{t}+}}\) to match the result of  which uses spectral initialization. Thus, we have a tradeoff between estimation error and initialization size.

The next main result concerns the trajectory of GD before it enters the local region. The theorem states that for all \(t T^{}\), \(^{(t)}\) stays close to the fully observed case \(}^{(t)}\) in both \(_{2}\) and \(_{}\)-norm.

**Theorem 3.3**.: _Suppose that the conditions of Theorem 3.1 hold, and \(T^{}\) is defined as in Theorem 3.1. Then, for all \(t T^{}\), we have_

\[\|^{(t)}-}^{(t)}\|_{2}}}^{(t)}}_{2},\] (11)

_with probability at least \(1-o(1/)\)._

**Trajectory of GD** The sequence \(}^{(t)}\) is a linear combination of \(^{(0)}\) and \(^{}\) (see (C.1) in the appendix), and it is easy to analyze how \(}^{(t)}\) evolves. By showing that \(^{(t)}\) stays close to \(}^{(t)}\) for all iterations, we not only show the convergence of GD with small initialization as in Theorem 3.1, but also characterize the exact trajectory that GD follows by Theorem 3.3.

**Implicit Regularization** One can prove that \(}^{(t)}\) is incoherent up to some log factors over all iterations, and from (11) and (12), the incoherence of \(^{(t)}\) is bounded by that of \(}^{(t)}\). Thus, Theorem 3.3 shows that the incoherence of \(^{(t)}\) is _implicitly_ controlled by GD without any regularizer. This is an improvement over the previous result on the global convergence of GD for matrix completion , where an explicit regularizer was used to control the \(_{}\)-norm of \(^{(t)}\), although no small initialization was used in that work.

## 4 Fully Observed Case and Proof Sketch

Before we explain the proof of Theorems 3.1 and 3.3, we describe the trajectory of the fully observed case. We characterize \(}^{(t)}\) with three variables: \(_{t}=^{}}^{(t)} \), \(_{t}=\|}^{(t)}\|_{2}\), and \(_{t}=\|}^{(t)}_{}\|_{2}\), where \(}^{(t)}_{}=}^{(t)}-^{}^ {}}^{(t)}\). According to (2), the three variables are updated with

\[_{t+1}=(1-_{t}^{2}+ ^{})_{t};_{t+1}=(1- _{t}^{2})_{t};\] \[_{t}^{2}=_{t}^{2}+_{t}^{2}.\]

At \(t=0\), due to random initialization, the initial vector is nearly orthogonal to \(^{}\), and we have \(_{0}}_{0}\) and \(_{0}_{0}=_{0}\). Also, due to the small initialization, the term \(_{t}^{2}\) is ignorable until \(_{t}\) becomes sufficiently large, so \(_{t}\) grows exponentially at the rate of \(1+^{}\), while \(_{t}\) remains still. Thus, in the early iterations where \((1+^{})^{t}\) is still much less than \(\), \(_{t}\) is kept close to its initialvalue \(_{0}\) while the trajectory becomes more parallel to \(^{}\) as \(_{t}\) increases. When \((1+^{})^{t}\) becomes much larger than \(\), the trajectory becomes almost parallel to \(^{}\) in that \(_{t}_{t}_{t}\). Until \(_{t}\) (asymptotically) reaches \(}}{}\), we can consider \(_{t}\) as increasing at a rate of \((1+^{})\), and it takes about \()}n}}{_ {0}}\) steps to reach this point. After that, we can no longer ignore the term \(_{t}^{2}\), and \(_{t}\) increases at a slower rate as \(_{t}\) increases. We can show that \(_{t}^{2}\) becomes sufficiently close to \(^{}\) within \(O( n)\) additional iterations, as stated in the following lemma.

**Lemma 4.1**.: _Let \(T_{2}^{}\) be the largest \(t\) such that \(_{t}^{2}}{64 n}\). At \(t=T_{2}^{}+)}\), we have \(_{t}^{2}^{}(1-)\)._

Finally, local convergence to \(^{}\) occurs in that \(_{t}\) approaches \(^{}\) and \(_{t}\) decreases exponentially with the rate \((1-^{})\). The actual behavior of quantities \(_{t}\), \(_{t}\), \(_{t}\) are plotted in Figure 1.

We define the iterates before \((1+^{})^{t}\) reaches \(}\), within some logarithmic factors, as Phase I, and the next iterates before \(_{t}^{2}\) reaches \(^{}(1-)\) as Phase II. Different techniques are used for each phase to prove that \(^{(t)}\) stays close to \(}^{(t)}\). At the end of Phase I, \(_{t}\) is increased to \(}_{0}\) from its initial scale \(}_{0}\), but it is still not dominant over \(_{0}\). Therefore, the magnitudes of both \(^{(t)}\) and \(}^{(t)}\) are kept close to \(_{0}\) throughout Phase I, and we take advantage of the small random initialization to show that the deviation of \(^{(t)}\) from \(}^{(t)}\) does not increase much, and is kept at \(}\) times the norms of \(^{(t)}\). In Phase II, we show that \(^{(t)}-}^{(t)}\) expands at a rate of at most \((1+^{})\). Since the norms of \(^{(t)}\) also grows at a rate of \((1+^{})\) during most of Phase II, the norms of \(^{(t)}-}^{(t)}\) remain negligible compared to those of \(^{(t)}\). The next two sections give the main lemmas of Phase I and II, respectively, which are used to prove Theorems 3.1 and 3.3. For a visual representation of the results in the following two sections, please refer to Figure 2.

## 5 Phase I: Finding Direction

We provide detailed results and proof ideas for Phase I. Our main goal is to analyze the deviation of \(^{(t)}\) from \(}^{(t)}\). First, if we look at the update equations (1) and (2), the second term is proportional to the third power of \(\|^{(t)}\|_{2}\), while the other terms depend linearly on \(\|^{(t)}\|_{2}\). Thus, the second term is almost negligible due to the small initialization. Without the second terms, the difference between \(^{(t)}\) and \(}^{(t)}\) at \(t=1\) is \((^{}-^{})^{(0)}\). From concentration inequalities, one can see that the \(_{2}\) and \(_{}\) norms of \((^{}-^{})^{(0)}\) are about \(}\) times smaller than those of \(}^{(1)}\).

Due to the third terms of (1) and (2), the norms of \(^{(t)}-}^{(t)}\) can grow exponentially at a rate of \((1+^{})\) in the worst case where \(^{(t)}-}^{(t)}\) is parallel to \(^{}\). In such a case, the norms of \(^{(t)}-}^{(t)}\) would be larger than those of \(}^{(t)}\) at the end of Phase I, since those of \(}^{(t)}\) remain still in Phase I. However, we overcome this problem by proving that the bounds grow at most _polynomially_ with respect to \(t\), and since \(t\) is at most \(O( n)\), the bounds remain \(}\) times smaller than the norms of \(}^{(t)}\) up to logarithmic factors throughout Phase I.

**Lemma 5.1**.: _Let \(T_{1}\) be the largest \(t\) such that \((1+^{})^{t}^{21}n}{np}}\). Under the conditions of Theorem 3.1, with probability at least \(1-o(1/)\), for all \(t T_{1}\), we have_

\[\|^{(t)}-}^{(t)}\|_{2}}_{0}t,\|^{(t)}- }^{(t)}\|_{}^{2}n}{ np}}}{}t^{2}.\] (14)

\(T_{1}\) is defined to be the end of Phase I. Lemma 5.1 proves Theorem 3.3 for Phase I.

**Proof of (13)**  We will first demonstrate how to obtain the \(_{2}\)-norm bound of Lemma 5.1. Let us define a sequence \(}^{(t)}\) that is updated as

\[}^{(t+1)}=}^{(t)}-\|}^{( t)}\|_{2}^{2}}^{(t)}+^{}}^{(t)}; }^{(0)}=^{(0)}.\] (15)Note that the norm of \(}^{(t)}\) is used in the second term of (15). The update equation of \(}^{(t)}\) differs from \(}^{(t)}\) in the third term and from \(^{(t)}\) in the second term. We use \(}^{(t)}\) as a proxy for bounding \(\|^{(t)}-}^{(t)}\|_{2}\). We first show that \(\|}^{(t)}-}^{(t)}\|_{2}\) grows at most linearly with respect to \(t\).

**Lemma 5.2**.: _With probability at least \(1-o(1/)\), for all \(t T_{1}\), we have_

\[\|}^{(t)}-}^{(t)}\|_{2} }_{0}t.\] (16)

The proof of this lemma is based on the fact that \(}^{(t)}\) is a product of \(^{(0)}\) and a matrix polynomial of \(\) and \(^{}\), while \(}^{(t)}\) is a product between \(^{(0)}\) and a matrix polynomial of \(\) and \(^{}\). We prove the lemma by comparing the two matrix polynomials. We remark that Lemma 5.2 holds regardless of the small initialization, but it relies on the randomness of \(^{(0)}\).

Since \(^{(t)}\) and \(}^{(t)}\) differ only in the second term, their initial difference is proportional to \(_{0}^{3}\). More precisely, it is \(}_{0}^{3}\). We show that the difference grows exponentially at a rate of \((1+^{})\).

**Lemma 5.3**.: _If (14) holds for all \(t T_{1}\), we have_

\[\|^{(t)}-}^{(t)}\|_{2}}^{3}n}{np}}(1+^{})^{t} _{0}^{3}\] (17)

_for all \(t T_{1}\) with probability at least \(1-o(1/)\)._

The upper bound in (17) becomes smaller than that of (16) if \((1+^{})^{t}_{0}^{2}^{}n}}\). One can check that this condition is satisfied from the definition of \(T_{1}\) given in Lemma 5.1 and the bound on the initialization size (8). Thus, (13) is proved by (16) and (17).

Proof of (14) We control the \(l\)th component of \(^{(t)}-}^{(t)}\) using the \(l\)th leave-one-out sequence. Leave-one-out sequences have two important properties. First, because they are defined without only one row/column, they are extremely close to \(^{(t)}\), and at \(t=1\), \(\|^{(t)}-^{(t,l)}\|_{2}\) is about \(}}{}\). Second, the \(l\)th component of the \(l\)th leave-one-out sequence evolves similarly to that of \(}^{(t)}\) and is easy to analyze. With these two properties, we bound the \(l\)th component of \(^{(t)}-}^{(t)}\) as

\[|(^{(t)}-}^{(t)})_{l}|\| ^{(t)}-^{(t,l)}\|_{2}+|(^{(t,l)}- {}^{(t)})_{l}|\] (18)

Figure 2: An illustrative description of trajectory of various quantities compared to the norms of \(^{(t)}\) on logarithmic scales. Arrows between lines represent the ratio between them. The quantities depicted are not precise, and only the key factors are shown for simplicity. (a) In Phase I, \(\|}^{(t)}-}^{(t)}\|_{2}\) increases linearly and \(\|^{(t)}-}^{(t)}\|_{2}\) increases exponentially with the rate of \((1+^{})\). They have the same scale at the end of Phase I. (b) In Phase I, even if the \(^{}\) component of \(^{(t)}-^{(t,l)}\) grows exponentially, it remains almost orthogonal to \(^{}\) throughout the phase. (c) Phase II is divided into three parts according to the growth speed of \(^{(t)}\) and \(^{(t)}-}^{(t)}\). The ratio between them at the start and the end of each part is described, and it is at most \(}\) in Phase II.

We claim that both \(\|^{(t)}-^{(t,l)}\|_{2}\) and \(|(^{(t,l)}-}^{(t)})_{l}|\) increase at most polynomially with respect to \(t\) from the initial scale \(}{}}{}\).

**Lemma 5.4**.: _With probability at least \(1-o(1/)\), for all \(t T_{1}\), we have_

\[\|^{(t)}-^{(t,l)}\|_{2}n}{np}}{}}t,\] (19)

As explained for \(^{(t)}-}^{(t)}\), due to the third terms of (1) and (3), \(^{(t)}-^{(t,l)}\) can also grow exponentially at the rate of \((1+^{})\) in the worst case where \(^{(t)}-^{(t,l)}\) is parallel to \(^{}\). This contradicts our result (19) that \(\|^{(t)}-^{(t,l)}\|_{2}\) grows only linearly. We show that \(^{(t)}-^{(t,l)}\) remains nearly orthogonal to \(^{}\) in Phase I, and thus the worst case does not occur.

**Lemma 5.5**.: _For all \(l[n]\) and \(t T_{1}\), we have_

\[|^{(t)}(^{(t)}-^{(t,l)})| ^{2}n}{np}}(1+^{})^{t}} {n}\]

_with probability at least \(1-o(1/)\), where \(^{(l)}\) is the first eigenvector of \(^{(l)}\)._

Note that \(^{(l)}\) is almost parallel to \(^{}\) (see Lemma A.5 in the appendix). The \(^{(l)}\) component of \(^{(t)}-^{(t,l)}\) is initialized to the order of \(}}{n}\), which is \(}\) times smaller than \(\|^{(t)}-^{(t,l)}\|_{2}\). Although it is increased exponentially, from the definition of \(T_{1}\), the \(^{(l)}\) component remains much smaller than \(\|^{(t)}-^{(t,l)}\|_{2}\) in Phase I.

One can see that \(|(^{(t,l)}-}^{(t)})_{l}|\) increases by \(\|^{(t)}-}^{(t)}\|_{2}\|^{ }\|_{}\) at each step, and summing the bound (13) up to \(t\) gives (20). Finally, (14) is obtained by putting (19) and (20) into (18).

## 6 Phase II: Expansion

In the next phase, we show that the bounds obtained in Phase I are increased at a rate of \((1+^{})\).

**Lemma 6.1**.: _Let \(T_{2}\) be the largest \(t\) such that \(_{t}^{2}^{}(1-)\). Then, for all \(T_{1}<t T_{2}\), we have_

\[\|^{(t)}-}^{(t)}\|_{2} n}{np}}_{0}(1+^{ })^{t-T_{1}},\] (21) \[\|^{(t)}-^{(t,l)}\|_{2} n}{np}}}{}( 1+^{})^{t-T_{1}},\] (22) \[\|^{(t)}-}^{(t)}\|_{} ^{8}n}{np}}}{}( 1+^{})^{t-T_{1}},\] (23) \[|(^{(t,l)}-}^{(t)})_{l} |^{8}n}{np}}}{}( 1+^{})^{t-T_{1}},\] (24)

_with probability at least \(1-o(1/)\)._

\(T_{2}\) is defined as the end of Phase II. We will explain how Lemma 6.1 leads to Theorem 3.3 in Phase II. Let us first focus on (21) and (11). We can divide Phase II into three parts according to the behavior of \(\|}^{(t)}\|_{2}\). First, \(\|}^{(t)}\|_{2}\) is kept close to \(_{0}\) until \((1+^{})^{t}\) becomes \(\), or \((1+^{})^{t-T_{1}}\) becomes \(\). In this part, although the bounds increase exponentially with the rate of \((1+^{})\), the factor \(}\), which was already present in (13) of Phase I, compensates for this increase. At the end of the first part, \(\|^{(t)}-}^{(t)}\|_{2}\) is smaller than \(\|}^{(t)}\|_{2}\) by some log factors. Next, \(\|}^{(t)}\|_{2}\) grows at the rate of \((1+^{})\) until it reaches \(}}{8}\). Since both \(\|^{(t)}-}^{(t)}\|_{2}\) and \(\|}^{(t)}\|_{2}\) increase with \((1+^{})\)the ratio between them is maintained in the second part. Finally, in the remaining iterations, \(\|}^{(t)}\|_{2}\) increases with \((1-_{t}^{2}+^{})\) at each step, and the increment becomes smaller as it converges to \(}\). Thus, as in the first part, \(\|^{(t)}-}^{(t)}\|_{2}\) increases faster than \(\|}^{(t)}\|_{2}\). However, from Lemma 4.1, the length of this part is \(O( n)\), and the ratio between \(\|^{(t)}-}^{(t)}\|_{2}\) and \(\|}^{(t)}\|_{2}\) increases only by \(^{6}n\). We prove that the log factors already present at the end of the second part compensate this, and finally (11) holds for all \(t\) in Phase II. A more delicate analysis may prove that \(\|^{(t)}-}^{(t)}\|_{2}\) grows at the same rate as \(\|}^{(t)}\|_{2}\) in the third part, and this will reduce the required sample complexity by at most \(^{12}n\). A similar argument can be used to prove that the bounds for \(\|^{(t)}-}^{(t)}\|_{}\), \(\|^{(t)}-^{(t,l)}\|_{2}\), and \(|(^{(t,l)}-}^{(t)})_{l}|\) are smaller than \(\|}^{(t)}\|_{}\) by some log factors throughout Phase II.

At the end of Phase II, \(}^{(t)}\) is very close to \(^{}\) in both \(_{2}\) and \(_{}\) norms (see Corollary C.3 in the appendix), so one can replace \(}^{(t)}\) of Lemma 6.1 with \(^{}\) to prove (4) to (7) of Theorem 3.1. Hence, we can let \(T^{}=T_{2}\), and as explained in Section 4, \(T_{2}\) is approximately given by \()}n}}{ _{0}}+O( n)\).

## 7 Simulation

In this section, we present some simulation results that support our theoretical findings.

**Trajectory of GD** With the dimension \(n=5000\), we constructed the ground truth vector \(^{}\) by sampling it from the Gaussian distribution \((,)\) and normalizing it to have unit norm. We let \(^{}=1\) so that the matrix \(^{}\) is given by \(^{}^{}\), and we randomly sampled the matrix symmetrically with a sampling rate of \(p=0.1\) and Gaussian noise of \(=\). The initialization size was set to \(_{0}=\) and a step size of \(0.1\) was used for GD. Figure 3 (a) and (b) represent one trial of the experiment, but similar graphs were obtained in each repetition of the experiment. The evolution of some important quantities such as \(\|^{(t)}\|_{2}\) and \(|^{}^{(t)}|\) is shown in Figure 3(a). As in the fully observed case, the signal component \(|^{}^{(t)}|\) increases at the the rate of \((1+^{})\) until it approaches \(}\), and a local convergence to \(^{}\) occurs, where \(\|^{(t)}-^{}\|_{2}\) decreases exponentially and saturates at the level determined by the noise size \(\). In Figure 3(b), we describe the deviation of \(^{(t)}\) from \(}^{(t)}\) in both \(_{2}\) and \(_{}\) norms. The solid lines represent the norms of \(^{(t)}\) and the dotted lines represent those of \(^{(t)}-}^{(t)}\). We can see that there is a gap between the solid and the dotted lines during the whole iterations. Thus, \(^{(t)}\) stays close to the trajectory of the fully observed case, as we proved in Theorem 3.3.

**Small Initialization** In the next experiment, we investigated the importance of a small initialization for the convergence of GD. We used the same conditions as in the previous experiment except \(n=500\). We measured \(\|^{(t)}^{}\|_{2}\) at \(t=)}n}}{_{ 0}}+100\) and averaged it over \(1000\) trials. We repeated the experiment while changing the initialization size from \(10^{0}\) to \(10^{-9}\) and the sampling probability from \(0.01\) to \(0.04\). The result is summarized in Figure 3(c). For all sampling probabilities, the small initialization improves the convergence of GD. Also, the performance starts to saturate at much larger initialization sizes as the sampling probability increases, and this is consistent with our finding (8) that a larger initialization is possible as more samples are available.

## 8 Discussion

In this paper, we showed that for rank-1 symmetric matrix completion with \(_{2}\) loss, GD can converge to the ground truth starting from a small random initialization. Ignoring log factors, the bound on the initialization size is \(n^{-}\) when the optimal \(n( n)\) samples are provided, and the bound becomes larger as more samples are provided. The result is interesting because the loss function does not have global benign geometry if no regularizer is applied. Our result does not use any explicit regularizer and relies only on the implicit regularizing effect of GD.

The most important future work is an extension to the rank-\(r\) case. Suppose that \(^{}\) is a rank-\(r\) matrix and its eigendecomposition is given by \(^{}^{}^{}=^{}^{}\), where \(^{}=(_{1}^{},,_{ r}^{})\) and \(^{}=^{}^{}\). Then, the trajectory of GD becomes an \(n r\) matrix \(^{(t)}\), which is updated as

\[^{(t+1)}=^{(t)}-_{}^{( t)}^{(t)}^{(t)}+^{}^{(t)}.\]

Each entry of \(^{(0)}\) is sampled independently from the Gaussian distribution \((0,_{0}^{2})\) as in the rank-1 case.

An instance of \(^{(t)}\) is shown in Figure 4. The same conditions as in Figure 3 are used, except that the ground truth matrix is a rank-3 matrix with non-zero eigenvalues \(1,0.75,0.5\). The singular values of \(^{(t)}\) behave similarly to \(\|^{(t)}\|_{2}\) in the rank-1 case. In the early iterations, where the orthogonal components dominate, the singular values stay close to their initial scale \(_{0}\). After that, each singular value \(_{i}(^{(t)})\) increases at a rate of \((1+_{i}^{})\) and saturates at \(^{}}\). We use \(\|-\|_{}\) to denote the Frobenius norm between \(\) and \(\) under best rotational alignment. \(\|^{(t)}-^{}\|_{}\) decreases exponentially and saturates at the level determined by the noise size \(\), after all singular values have saturated, as local convergence begins.

To extend the results of the rank-1 case, we need to show that \(\|^{(t)}-}^{(t)}\|_{}\) remains much smaller than \(\|^{(t)}\|_{}\) throughout the iterations, where \(}^{(t)}\) is the trajectory of the fully observed case. Before \(_{1}(^{(t)})\) saturates around \(^{}}\), it behaves similarly to \(\|^{(t)}-}^{(t)}\|_{2}\) of the rank-1 case, i.e., it expands at a rate of \((1+_{1}^{})\) along with \(\|^{(t)}\|_{}\) after the early iterations. However, because each singular value grows at a different rate, a different phenomenon is observed for the rank-\(r\) case. During the iterations before \(_{i+1}(^{(t)})\) saturates after \(_{i}(^{(t)})\) does, both \(\|^{(t)}-}^{(t)}\|_{}\) and \(\|^{(t)}\|_{}\) do not increase much. Our current theory can only show that \(\|^{(t)}-}^{(t)}\|_{}\) increases at a rate less than \((1+_{i+1}^{})\), and in order for \(\|^{(t)}-}^{(t)}\|_{}\) to remain much smaller than \(\|^{(t)}\|_{}\), additional sample complexity is required to compensate for the exponential increases. Therefore, we expect that the convergence of GD for the case of rank-\(r\) can be proved with the techniques developed in this paper if \(n^{1+(-1)}(,r, n)\) samples are provided, where \(=^{}}{_{r}^{}}\) is the condition number. Nevertheless, whether GD can converge with the optimal \(n(,r, n)\) samples for the rank-\(r\) matrix completion problem remains an open problem.

Figure 4: Trajectory of GD obtained for a rank-3 matrix with non-zero eigenvalues \(1,0.75,0.5\). The same parameters were used as in Figure 3.