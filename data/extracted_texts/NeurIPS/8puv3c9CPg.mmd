# Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects

Michael A. Lepori\({}^{1}\)1

Alexa R. Tartaglini\({}^{2,\,3}\)1

Wai Keen Vong\({}^{2}\)   Thomas Serre\({}^{1}\)   Brenden M. Lake\({}^{2}\)   Ellie Pavlick\({}^{1}\)

\({}^{1}\)Brown University  \({}^{2}\)New York University  \({}^{3}\)Stanford University

{michael_lepori,thomas_serre,ellie_pavlick}@brown.edu

alexart@stanford.edu  {waikeen.vong,brenden}@nyu.edu

###### Abstract

Though vision transformers (ViTs) have achieved state-of-the-art performance in a variety of settings, they exhibit surprising failures when performing tasks involving visual relations. This begs the question: how do ViTs attempt to perform tasks that require computing visual relations between objects? Prior efforts to interpret ViTs tend to focus on characterizing relevant low-level visual _features_. In contrast, we adopt methods from mechanistic interpretability to study the higher-level visual _algorithms_ that ViTs use to perform abstract visual reasoning. We present a case study of a fundamental, yet surprisingly difficult, relational reasoning task: judging whether two visual entities are the same or different. We find that pretrained ViTs fine-tuned on this task often exhibit two qualitatively different stages of processing despite having no obvious inductive biases to do so: 1) a _perceptual_ stage wherein local object features are extracted and stored in a disentangled representation, and 2) a _relational_ stage wherein object representations are compared. In the second stage, we find evidence that ViTs can sometimes learn to represent abstract visual relations, a capability that has long been considered out of reach for artificial neural networks. Finally, we demonstrate that failures at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks. By understanding ViTs in terms of discrete processing stages, one can more precisely diagnose and rectify shortcomings of existing and future models.

## 1 Introduction

Despite the well-established successes of transformer models (Vaswani et al., 2017) for a variety of vision applications (ViTs; Dosovitskiy et al. (2020)) - notably image generation and classification - there has been comparatively little breakthrough progress on complex tasks involving _relations_ between visual entities, such as visual question answering (Schwenk et al., 2022) and image-text matching (Thrush et al., 2022; Liu et al., 2023; Yuksekgoni et al., 2022). One fundamental difference between these tasks is that the former is largely _semantic_ - relying on pixel-level image features that correlate with learned class labels - whereas the latter often involves _syntactic_ operations - those which are independent of pixel-level features (Ricci et al., 2021; Hochmann et al., 2021). Though the ability to compute over abstract visual relations is thought to be fundamental to human visual intelligence (Ullman, 1987; Hespos et al., 2021), the ability of neural networks to perform such syntactic operations has been the subject of intense debate (Fodor and Pylyshyn, 1988; Marcus, 2003; Chalmers, 1992; Quilty-Dunn et al., 2023; Lake et al., 2017; Davidson et al., 2024).

Much prior work has attempted to empirically resolve whether or not vision networks can implement an abstract, relational operation, typically by behaviorally assessing the model's ability to generalizeto held-out stimuli (Fleuret et al., 2011; Zerroug et al., 2022; Kim et al., 2018; Puebla & Bowers, 2022; Tartaglini et al., 2023). However, strikingly different algorithms might produce the same model behavior, rendering it difficult to characterize whether models do or do not possess an abstract operation (Lepori et al., 2023a). This problem is exacerbated when analyzing pretrained models, whose opaque training data renders it difficult to distinguish true generalization from memorization (McCoy et al., 2023). In this work, we employ newly-developed techniques from mechanistic interpretability to characterize the _algorithms_ learned by ViTs. Analyzing the internal mechanisms of models enables us to more precisely understand how they attempt to implement relational operations, allowing us to more clearly diagnose problems in current and future models when applied to complex visual tasks.

One of the most fundamental of these abstract operations is identifying whether two objects are the same or different. This operation undergirds human visual and analogical reasoning (Forbus & Lovett, 2021; Cook & Wasserman, 2007), and is crucial for answering a wide variety of common visual questions, such as "How many plates are on the table?" (as each plate must be identified as an instance of the same object) or "Are Mary and Bob reading the same book?" (Ricci et al., 2021). Indeed, same-different judgments can be found across the animal kingdom, being successfully captured by bees (Giurfa et al., 2001), ducklings (Martinho III & Kacelnik, 2016), primates (Thompson & Oden, 2000), and crows (Cook & Wasserman, 2007).

We analyze ViTs trained on two same-different tasks: an identity discrimination task, which constitutes the most basic instances of the abstract concept of same-different (and is most commonly studied in artificial systems) (Newport, 2021), and a relational match-to-sample task, which requires explicitly representing and manipulating an abstract concept of "same" or "different" (Cook & Wasserman, 2007; Geiger et al., 2023). See Figure 1 for examples of each. We relate the algorithms that models adopt to their downstream behavior, including compositional and OOD generalization2.

Our main contributions are the following:

1. Inspired by the infant and animal abstract concept learning literature, we introduce a synthetic visual relation match-to-sample (RMTs) task, which assesses a model's ability to represent and compute over the abstract concept of "same" or "different".
2. We identify a processing pipeline within the layers of several - but not all - pretrained ViTs, consisting of a "perceptual" stage followed by a more abstract "relational" stage. We characterize each stage individually, demonstrating that the perceptual stage produces disentangled object representations (Higgins et al., 2018), while the relational stage implements fairly abstract (i.e. invariant to perceptual properties of input images) relational computations - an ability that has been intensely debated since the advent of neural networks (Fodor & Pylyshyn, 1988).
3. We demonstrate that deficiencies in either the perceptual or relational stage can completely prevent models from learning abstract relational operations. By rectifying either stage, models can solve simple relational operations. However, both stages must be intact in order to learn more complex operations.

## 2 Methods

**Discrimination Task**. The discrimination task tests the most basic instance of the same-different relation. This task is well studied in machine learning (Kim et al., 2018; Puebla & Bowers, 2022;

Figure 1: **Two same-different tasks. (a) Discrimination: “same” images contain two objects with the same color and shape. Objects in “different” images differ in at least one of those properties—in this case, both color and shape. (b) RMTs: “same” images contain a pair of objects that exhibit the same relation as a display pair of objects in the top left corner. In the image on the left, both pairs demonstrate a “different” relation, so the classification is “same” (relation). “Different” images contain pairs exhibiting different relations.**

Tartaglini et al., 2023) and simply requires a single comparison between two objects. Stimuli in our discrimination task consist of images containing two simple objects (see Figure 0(a)). Each object may take on one of 16 different shapes and one of 16 different colors (see Appendix A for dataset details). Models are trained to classify whether these two objects are the "same" along both color and shape dimensions or "different" along at least one dimension. Crucially, our stimuli are _patch-aligned_. ViTs tokenize images into patches of \(N N\) pixels (\(N=\{14,16,32\}\)). We generate datasets such that individual objects reside completely within the bounds of a single patch (for \(N=32\) models) or within exactly 4 patches (for \(N=16\) and \(N=14\) models). Patch alignment allows us to adopt techniques from NLP mechanistic interpretability, which typically assume meaningful discrete inputs (e.g. words). To increase the difficulty of the task, objects are randomly placed within patch boundaries, and Gaussian noise is sampled and added to the tokens (see Appendix A). Models are trained on \(6,400\) images.

We evaluate these models on out-of-distribution synthetic stimuli to ensure that the learned relations generalize. Finally, we evaluate a rather extreme form of generalization by generating a "realistic" dataset of discrimination examples using Blender, which include various visual attributes such as lighting conditions, backgrounds, and depth of field (see Appendix A.3).

Relational Match-to-Sample (RMTS) Task.Due to the simplicity of the discrimination task, we also analyze a more abstract (and thus more difficult) iteration of the same-different relation using a relational match-to-sample (RMTS) design. In this task, the model must generate explicit representations of "sameness" and "difference", and then operate over these representations (Martinho III & Kacelnik, 2016; Hochmann et al., 2017). Although many species can solve the discrimination task, animals (Penn et al., 2008) and children younger than 6 (Hochmann et al., 2017; Holyoak & Lu, 2021) struggle to solve the RMTS task. Stimuli in the RMTS task contain 4 objects grouped in two pairs (Figure 0(b)). The "display" pair always occupies patches in the top left of the image. The "sample" pair can occupy any other position. The task is defined as follows: for each pair, produce a same-different judgment (as in the discrimination task). Then, compare these judgments--if both pairs exhibit the same intermediate judgment (i.e., both pairs exhibit "same" _or_ both pairs exhibit "different"), then the label is "same". Otherwise, the label is "different." Objects are identical to those in the discrimination task, and they are similarly patch-aligned.

Models.Tartaglini et al. (2023) demonstrated that CLIP-pretrained ViTs (Radford et al., 2021) can achieve high performance in generalizing the same-different relation to out-of-distribution stimuli when fine-tuned on a same-different discrimination task. Thus, we primarily focus our analysis on this model3. In later sections, we compare CLIP to additional ViT models pretrained using DINO (Caron et al., 2021), DINOv2 (Oquab et al.), masked auto encoding (MAE; He et al. (2022)) and ImageNet classification (Russakovsky et al., 2015; Dosovitskiy et al., 2020) objectives. We also train a randomly-initialized ViT model on each task (From Scratch). All models are fine-tuned on either a discrimination or RMTS task for 200 epochs using the AdamW optimizer with a learning rate of 1e-6. We perform a sweep over learning rate schedulers (Exponential with a decay rate of \(0.95\) and ReduceLRonPlateau with a patience of \(40\)). Models are selected by validation accuracy, and test accuracy is reported. Our results affirm and extend those presented in Tartaglini et al. (2023)--CLIP and DINOv2-pretrained ViTs perform extremely well on both tasks, achieving \(\)97% accuracy on a held-out test set. Appendix B presents results for all models.

## 3 Two-Stage Processing in ViTs

We now begin to characterize the internal mechanisms underlying the success of the CLIP and DINOv2 ViT models. In the following section, we cover how processing occurs in two distinct stages within the model: a _perceptual_ stage, where the object tokens strongly attend to other tokens within the same object, and a _relational_ stage, where tokens in one object attend to tokens in another object (or pair of objects).

Methods -- Attention Pattern Analysis.We explore the operations performed by the model's attention heads, which "read" from particular patches and "write" that information to other patches (Elhage et al., 2021). In particular, we are interested in the flow of information _within_ individual objects, _between_ the two objects, and (in the case of RMTS) _between two pairs_ of objects. We refer to attention heads that consistently exhibit within-object patterns across images as _local_ attention heads and heads that attend to other tokens _global_ attention heads. To classify an attention head as local or global, we score the head from \(0\) to \(1\), where values closer to \(0\) indicate local operations and values closer to \(1\) indicate global operations. To compute the score for an individual head, we collect its attention patterns on \(500\) randomly selected "same" and "different" images (\(1,000\) images total). Then, for each object in a given image, we compute the proportion of attention from the object's patches to any other patches that do not belong to the same object (excluding the CLS token)--this includes patches containing the other object(s) and non-object background tokens.4 This procedure yields two proportions, one for each object in the image. The attention head's score for the image is the maximum of these two proportions. Finally, these scores are averaged across the images to produce the final score.

**Results**. Attention head scores for CLIP ViT-B/16 fine-tuned on the discrimination and RMTS tasks are displayed in the heatmaps of Figure 1(a) and 1(c), respectively. The first half of these models is dominated by attention heads that most often perform local operations (blue cells). See Appendix F for examples of attention patterns. In the intermediate layers, attention heads begin to perform global operations reliably, and the deeper layers of the model are dominated by global heads. The prevalence of these two types of operations clearly demarcates two processing stages in CLIP ViTs: a _perceptual_ stage where within-object processing occurs, followed by a _relational_ stage where

Figure 2: **Attention Pattern Analysis. (a) CLIP Discrimination: The heatmap _(top)_ shows the distribution of “local” (blue) vs. “global” (red) attention heads throughout a CLIP ViT-B/16 model fine-tuned on discrimination (Figure 0(a)). The \(x\)-axis is the model layer, while the \(y\)-axis is the head index. Local heads tend to cluster in early layers and transition to global heads around layer 6. For each layer, the line graph _(bottom)_ plots the maximum proportion of attention across all \(12\) heads from object patches to image patches that are 1) within the same object (within-object=**WO**), 2) within the other object (within-pair\(=\)**WP**), or 3) in the background (**BG**). The stars mark the peak of each. WO attention peaks in early layers, followed by WP, and finally BG. **(b) From Scratch Discrimination**: We repeat the analysis in (a). The model contains nearly zero local heads. **(c) CLIP RMTS**: We repeat the analysis for a CLIP model fine-tuned on RMTS (Figure 0(b)). _Top_: Our results largely hold from (a). _Bottom_: We track a fourth attention pattern—attention _between_ pairs of objects (between pair\(=\)**BP**). We find that WO peaks first, then WP, then BP, and finally BG. This accords with the hierarchical computations implied by the RMTS task. **(d) DINO RMTS**: We repeat the analysis in (c) for a DINO model and find no such hierarchical pattern.

between-object processing occurs. These stages are explored in further depth in Sections 4 and 5 respectively.5 We also find similar two-stage processing when evaluating on a discrimination dataset that employs realistic stimuli, suggesting that the patterns observed on our synthetic stimuli are robust and transferable (see Appendix A.3).

The line charts in Figure 2 show maximal scores of each attention head type (local and global) in each layer. Since values closer to \(0\) indicate local (i.e., _within-object_; WO in Figure 2) heads by construction, we plot these values subtracted from \(1\). The global attention heads are further broken down into two subcategories for the discrimination task: _within-pair_ attention heads, whereby the tokens of one object attend to tokens associated with the object it is being compared to (WP in Figure 2), and _background_ attention heads, whereby object tokens attend to background tokens (BG in Figure 2). We add a third subcategory for RMTS: _between-pair_ attention heads, which attend to tokens in the other pair of objects (e.g., a display object attending to a sample object; BP in Figure 2). For both tasks, objects strongly attend to themselves throughout the first six layers, with a peak at layer 3. Throughout this perceptual stage, _within-pair_ heads steadily increase in prominence and peak in layer 6 (discrimination) or 5 (RMTS). In RMTS models, this _within-pair_ peak is followed by a _between-pair_ peak, recapitulating the expected sequence of steps that one might use to solve RMTS. Notably, the _within-pair_ (and _between-pair_) peaks occur precisely where an abrupt transition from perceptual operations to relational operations occurs. Around layer 4, object attention to a set of background tokens begins to increase; after layer 6, object-to-background attention accounts for nearly all outgoing attention from object tokens. This suggests that processing may have moved into a set of register tokens (Darcet et al., 2023).

Notably, this two-stage processing pipeline is not trivial to learn--several models, including a randomly initialized model trained on the discrimination task and a DINO model trained on RMTS (Figure 2b and d) fail to exhibit any obvious transition from local to global operations (See Appendix E for results from other models). However, we do find this pipeline in DINOv2 and ImageNet pretrained models (See Appendix D and E). We note that this two-stage processing pipeline loosely recapitulates the processing sequence found in biological vision systems: image representations are first formed during a feedforward sweep of the visual cortex, then feedback connections enable relational reasoning over these representations Kreiman and Serre (2020).

Figure 3: **(a) Interchange interventions**: The base image exhibits the “different” relation, as the two objects differ in either shape _(top)_ or color _(bottom)_. An interchange intervention extracts {shape, color}information from the intermediate representations generated by the same model run on a different image (source), then patches this information from the source image into the model’s intermediate representations of the base image. If successful, the intervened model will now return “same” when run on the base image. DAS is optimized to succeed at interchange interventions. **(b) Disentanglement Results**: We report the success of interchange interventions on shape and color across layers for CLIP ViT-B/16 fine-tuned on either the discrimination or RMTS task. We find that these properties are disentangled early in the model—one property can be manipulated without interfering with the other. The background is colored according to the heatmap in Figure 2a, where blue denotes local heads and red denotes global heads.

## 4 The Perceptual Stage

Attention between tokens is largely restricted to other tokens within the same object in the perceptual stage, but to what end? In the following section, we demonstrate that these layers produce disentangled local object representations which encode shape and color. These properties are represented in separate linear subspaces within the intermediate representations of CLIP and DINOv2-pretrained ViTs.

Methods -- DAS.Distributed Alignment Search (DAS) (Geiger et al., 2024; Wu et al., 2024c) is used to identify whether particular variables are causally implicated in a model's computation6. Given a neural network \(M\), hypothesized high-level causal model \(C\), and high-level variable \(v\), DAS attempts to isolate a linear subspace \(s\) of the residual stream states generated by \(M\) that represents \(v\) (i.e. \(s\) takes on a value \(s_{1}\) to represent \(v_{1}\), \(s_{2}\) to represent \(v_{2}\), and so on). The success of DAS is measured by the success of _counterfactual interventions_. If \(C(v_{1})=y_{1}\) and \(C(v_{2})=y_{2}\), and \(M(x)=y_{1}\) for some input \(x\), does replacing \(s_{1}\) with \(s_{2}\) change the model's decision to \(y_{2}\)?

Concretely, \(M\) corresponds to our pretrained ViT, and a high-level causal model for the discrimination task can be summarized as follows: 1) Extract shape\({}_{1}\) and color\({}_{1}\) from object\({}_{1}\), repeat for object\({}_{2}\); 2) Compare shape\({}_{1}\) and shape\({}_{2}\), compare color\({}_{1}\) and color\({}_{2}\); 3) Return same if both comparisons return same, otherwise return different. Similarly, we can define a slightly more complex causal model for the RMTS task. We use this method to understand better the object representations generated by the perceptual stage. In particular, we try to identify whether shape and color are disentangled (Higgins et al., 2018) such that we could edit shape\({}_{1}^{}_{1}\) without interfering with either color property (See Figure 2(a)). For this work, we use a version of DAS where the subspace \(s\) is found by optimizing a differentiable binary mask and a rotation matrix over model representations (Wu et al., 2024a). See Appendix G for technical details.

Results.We identify independent linear subspaces for color and shape in the intermediate representations produced in the early layers of CLIP-pretrained ViTs (Figure 2(b)). In other words, we

Figure 4: **(a) Novel Representations Analysis**: Using trained DAS interventions, we can inject any vector into a model’s shape or color subspaces, allowing us to test whether the same-different operation can be computed over arbitrary vectors. We intervene on a “different” image—differing only in its color property—by patching a novel color (an interpolation of red and black) into _both_ objects in order to flip the decision to “same”. **(b) Discrimination Results**: We perform novel representations analysis using four methods for generating novel representations: 1) _adding_ observed representations, 2) _interpolating_ observed representations, 3) per-dimension _sampling_ using a distribution derived from observed representations, and 4) sampling _randomly_ from a normal distribution \((0,1)\). The model’s same-different operation generalizes well to vectors generated by adding (and generalizes somewhat to interpolated vectors) in early layers but not to sampled or random vectors. The background is colored according to the heatmap in Figure 1(a) (blue\(=\)local heads; red\(=\)global heads).

can extract either color or shape information from one object and inject it into another object. This holds for both discrimination and RMTS tasks. One can conclude that at least one function of the perceptual stage is to form disentangled local object representations, which are then used to solve same-different tasks. Notably, these local object representations are formed in the first few layers and become increasingly irrelevant in deeper layers; intervening on the intermediate representations of object tokens at layers 5 and beyond results in chance intervention performance or worse. DINOv2-pretrained ViTs provide similar results (Appendix D), whereas other models exhibit these patterns less strongly (Appendix I). We present a control experiment in Appendix H, which further confirms our interpretation of these results.

## 5 The Relational Stage

We now characterize the relational stage, where tokens within one object largely attend to tokens in the other object. We hypothesize that this stage takes in the object representations formed in the perceptual stage and computes relational same-different operations over them. We find that the operations implemented by these relational layers are somewhat abstract in that 1) they do not rely on memorizing individual objects and 2) one can identify abstract _same_ and _different_ representations in the RMTS task, which are constant even as the perceptual qualities of the object pairs vary.

Methods -- Patching Novel Representations.In Section 4, we identify independent linear subspaces encoding shape and color. Does the _content_ of these subspaces matter to the same-different computation? One can imagine an ideal same-different relation that is completely abstracted away from the particular properties of the objects being compared. In this setting, a model could accurately judge "same" vs. "different" for object representations where colors and shapes are represented by arbitrary vectors. To study this, we intervene on the linear subspaces for either shape or color for both objects in a pair, replacing the content found therein with novel representations (see Figure 3(a)). To create a "different" example, we start with a "same" image and replace the shape (or color) representation of both objects with two different novel representations; to create a "same" example, we start with a "different" image and replace them with two identical novel representations. We then assess whether the model's decision changes accordingly. We generate novel representations using four methods: 1) we _add_ the representations found within the linear subspaces corresponding to two randomly sampled objects in an IID validation set, 2) we _interpolate_ between these representations, 3) we _sample_ each dimension randomly from a distribution of embeddings, and 4) we sample each dimension from an OOD _random_ distribution (a \(=0\) normal). See Appendix J for technical details.

Results.The results of patching novel representations into a CLIP-pretrained ViT are presented in Figure 3(b). Overall, we find the greatest success when patching in _added_ representations, followed by _interpolated_ representations. We observe limited success when patching _sampled_ representations, and no success patching _random_ vectors. All interventions perform best in layers 2 and 3, towards

Figure 5: **Linear probing and intervention results**. We probe for the intermediate same-different judgments required to perform the RMTS task (blue). Probe performance reaches ceiling at around layer 5 and maintains throughout the rest of the model. We use the directions defined by the linear probe to intervene on model representations and flip an intermediate judgment (green). This intervention succeeds reliably at layer 5 but not deeper. We add a vector that is consistent with a pair’s exhibited same-different relation as a control (yellow). This has little effect. The background is colored according to the heatmap in Figure 3(c) (blue\(=\)local heads; red\(=\)global heads).

the end of the perceptual stage. Overall, this points to a limited form of abstraction in the relational stage. The same-different operation is somewhat abstract--while it cannot operate over completely arbitrary vectors, it _can_ generalize to additions and interpolations of shape & color representations, indicating that it does not rely on rote memorization of specific objects. Results for CLIP-pretrained ViTs on RITS and other models on both tasks are found in Appendix K. Results for DINOv2 are found in Appendix D. Other models largely produce similar results to CLIP in this analysis.

Methods -- Linear Interventions.The RITS task allows us to further characterize the relational stage, as it requires first forming then comparing intermediate representations of _same_ and _different_. Are these intermediate representations abstract (i.e. invariant to the perceptual qualities of the object pairs that underlie them)? We linearly probe for intermediate same or different judgments from the collection of tokens corresponding to object pairs. The probe consists of a linear transformation mapping the residual stream to two dimensions representing _same_ and _different_. Each row of this transformation can be viewed as a direction \(d\) in the residual stream corresponding to the value being probed for (e.g. \(d_{}\) is the linear direction representing _same_). We train one probe for each layer on images from the model's train set and test on images from a test set. To understand whether the directions discovered by the probe are causally implicated in model behavior, we create a counterfactual intervention (Nanda et al., 2023). In order to change an intermediate judgment from _same_ to _different_, we add the direction \(d_{}\) to the intermediate representations of objects that exhibit the _same_ relation. We then observe whether the model behaves as if this pair now exhibits the _different_ relation.7 We run this intervention on images from the model's test set. We also run a control intervention where we add the incorrect direction (e.g., we add \(d_{}\) when the object pair is already "same"). This control intervention should not reliably flip the model's downstream decisions.

Results.Probing and linear intervention results for a CLIP-pretrained ViT are shown in Figure 5. We observe that linear probe performance peaks in the middle layers of the model (layer 5) and then remains high. However, our linear intervention accuracy peaks at layer 5 and then drops precipitously. Notably, layer 5 also corresponds to the peak of within-pair attention (see Figure 1(c)). This indicates that--at least in layer 5--there exists a single direction representing _same_ and a single direction representing _different_. One can flip the intermediate same-different judgment by adding a vector in one of these directions to the residual streams of any pair of objects. Finally, the control intervention completely fails throughout all layers, as expected. Thus, CLIP ViT does in fact generate and operate over abstract representations of same and different in the RMITS task. We find similar results for a DINOv2 pretrained model (see Appendix D), but not for others (see Appendix K).

Figure 6: We average the best counterfactual intervention accuracy for shape and color and plot it against IID, OOD, and Compositional Test set performance for CLIP, DINO, DINOv2, ImageNet, MAE, and from-scratch B/16 models. We observe that increased disentanglement (i.e. higher counterfactual accuracy) correlates with downstream performance. The from-scratch model achieved only chance IID performance in RMITS, so we omitted it from the analysis.

## 6 Disentanglement Correlates with Generalization Performance

Object representations that disentangle perceptual properties may enable a model to generalize to out-of-distribution stimuli. Specifically, disentangled visual representations may enable compositional generalization to unseen combinations of perceptual properties (Higgins et al. (2018); Bengio et al. (2013), cf. Locatello et al. (2019)8. To investigate the relationship between disentanglement and generalization, we fine-tune CLIP, ImageNet, DINO, DINOv2, MAE, and randomly-initialized ViTs on a new dataset where each shape is only ever paired with two distinct colors. We then repeat our analyses in Section 4 to identify independent linear subspaces for shape and color.9 We evaluate models in 3 settings: 1) on an IID test set consisting of observed shape-color combinations, 2) on a compositional generalization test set consisting of unobserved shape-color combinations (where each shape and each color have been individually observed), and 3) on an OOD test set consisting of completely novel shapes and colors. We plot the relationship between disentanglement (i.e. counterfactual intervention accuracy) and overall performance on these model evaluations. We find a consistent trend: more disentangled representations correlates with downstream model performance in all cases (See Figure 6).

## 7 Failure Modes

Previous sections have argued that pretrained ViTs that achieve high performance when finetuned on same-different tasks implement a two-stage processing pipeline. In this section, we argue that both perceptual and relational stages can serve as failure points for models, impeding their ability to solve same-different tasks. In practice, tasks that rely on relations between objects likely have perceptual and relational stages that are orders of magnitude more complex than those we study here. The results presented herein indicate that solutions targeting _either_ the perceptual (Zeng et al., 2022) or relational (Bugliarello et al., 2023) stages may be insufficient for producing the robust, abstract computations that we desire.

Perceptual and Relational Regularizers.We introduce two loss functions, designed to induce disentangled object representations and multi-stage relational processing, respectively. When employing the _disentanglement loss_, we introduce token-level probes that are optimized to predict shape information from one linear subspace (e.g., the first 384 dimensions) of the representations generated at an intermediate layer of the model and color information from the complementary linear subspace at that same layer (layer 3, in our experiments). These probes are optimized during training, and the probe loss is backpropagated through the model. This approach is motivated by classic work on disentangled representations (Eastwood and Williams, 2018). The _pipeline loss_ is designed to encourage discrete, specific stages of processing by regularizing the attention maps to maximize the attention pattern scores defined in Section 3. Specifically, early layers are encouraged to maximize attention within-object, then within-pair, and finally (in the case of RMTS stimuli) between-pair. See Appendix L for technical details. Note that the disentanglement loss targets the perceptual stage of processing, whereas the pipeline loss targets both perceptual and relational stages.

Results.First, we note that models trained from scratch on the discrimination task do not clearly distinguish between perceptual and relational stages (Figure 1(b)). Thus, we might expect that a model trained on a limited number of shape-color combinations would not learn a robust representation of the same-different relation. Indeed, Table 1 confirms this. However, we see that either the disentanglement loss or the pipeline loss is sufficient for learning a generalizable representation of this relation.

Similarly, we find that models trained from scratch on the RMTS task only achieve chance performance. However, in this case we must include _both_ disentanglement and pipeline losses in order to induce a fairly general (though still far from perfect) hierarchical representation of same-different. This provides evidence that models may fail at either the perceptual _or_ relational stages: they mightfail to produce the correct types of object representations, and/or they might fail to execute relational operations over them. See Appendix M for further analysis.

## 8 Discussion

**Related Work**. This work takes inspiration from the field of mechanistic interpretability, which seeks to characterize the algorithms that neural networks implement Olah (2022). Though many of these ideas originated in the domain of NLP Wang et al. (2022); Hanna et al. (2024); Feng and Steinhardt (2023); Wu et al. (2024); Merullo et al. (2023); Geva et al. (2022); Meng et al. (2022) and in toy settings Nanda et al. (2022); Elhage et al. (2022); Li et al. (2022), they are beginning to find applications in computer vision Feel et al. (2023); Vilas et al. (2024); Palit et al. (2023). These techniques augment an already-robust suite of tools that visualize the features (rather than algorithms) that vision models use Olah et al. (2017); Selvaraju et al. (2017); Simonyan et al. (2014). Finally, this study contributes to a growing literature employing mechanistic interpretability to address debates within cognitive science Milliere and Buckner (2024); Lepori et al. (2023); Kallini et al. (2024); Traylor et al. (2024).

**Conclusion**. The ability to compute abstract visual relations is a fundamental aspect of biological visual intelligence and a crucial stepping stone toward useful and robust artificial vision systems. In this work, we demonstrate that some fine-tuned ViTs adopt a two-stage processing pipeline to solve same-different tasks--despite having no obvious inductive biases towards this algorithm. First, models produce disentangled object representations in a perceptual stage; models then compute a somewhat abstract version of the same-different computation in a relational stage. Finally, we observe a correlation between disentanglement and generalization and note that models might fail to learn _either_ the perceptual or relational operations necessary to solve a task.

Why do CLIP and DINOv2-pretrained ViTs perform favorably and adopt this two-stage algorithm so cleanly relative to other pretrained models? Raghu et al. (2021) find that models pretrained on more data tend to learn local attention patterns in early layers, followed by global patterns in later layers. Thus, pretraining scale (rather than training objective) might enable these models to first form local object representations, which are then used in global relational operations. Future work might focus on pinning down the precise relationship between data scale and relational reasoning ability, potentially by studying the training dynamics of these models. Additionally, future work might focus on characterizing the precise mechanisms (e.g. the attention heads and MLPs) used to implement the perceptual and relational stages, or generalize our findings to more complex relational tasks.