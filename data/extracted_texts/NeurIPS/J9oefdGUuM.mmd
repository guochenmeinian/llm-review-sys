# RagChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation

Dongyu Ru\({}^{1}\) Lin Qiu\({}^{1}\) Xiangkun Hu\({}^{1}\) Tianhang Zhang\({}^{1}\) Peng Shi\({}^{1}\)

Shareh Chang\({}^{1}\)1 Cheng Jiayang\({}^{1}\)1 Cunxiang Wang\({}^{1}\)

Shichao Sun\({}^{1}\)

**Huanyu Li\({}^{2}\) Zizhao Zhang\({}^{1}\)1 Binjie Wang\({}^{1}\)1 Jiarong Jiang\({}^{1}\) Tong He\({}^{1}\) Zhiguo Wang\({}^{1}\) Pengfei Liu\({}^{2}\) Yue Zhang\({}^{3}\) Zheng Zhang\({}^{1}\)**

\({}^{1}\)Amazon AWS AI Shanghai Jiaotong University Westlake University

###### Abstract

Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RagChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RagChecker has significantly better correlations with human judgments than other evaluation metrics. Using RagChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RagChecker can guide researchers and practitioners in developing more effective RAG systems3.

## 1 Introduction

Retrieval-Augmented Generation (RAG) systems  enhance Large Language Models (LLMs) by incorporating external knowledge bases, enabling more precise and contextually relevant responses . As these systems become integral to a variety of applications , it's imperative to develop robust and comprehensive evaluation frameworks to assess their performance and identify areas for improvement. Evaluating RAG systems, however, presents several challenges:

(1) _modular complexity_: The modular nature of RAG systems, comprising both a retriever and a generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics that can holistically assess the entire system as well as evaluate the individual modules and their interplay , allowing for fully understanding the sources of the errors and misses and how they are generated. (2) _metric limitation_: Existing metrics for evaluating RAG systems, which are often rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically, traditional metrics like recall@k and MRR  for retrievers depend on annotated chunks and a rigid chunking approach, missing out on the full semantic scope of the knowledge base. For generators, typical measures such as n-gram-based (e.g., BLEU , ROUGE ), embedding-based (e.g., BERTScore ), and LLM-based methods  perform well with concise answers but fail to detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed, semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both the retrieval and generation components in RAG systems. (3) _metric reliability_: the reliabilityof existing metrics for RAG remains under-explored. Effective evaluation metrics must not only accurately reflect system performance but also align with human judgments to ensure their utility in real-world scenarios.

To overcome these challenges, we introduce RagChecker, an innovative evaluation framework designed for detailed analysis of both retrieval and generation processes. RagChecker is based on claim-level entailment checking which involves operations of extracting claims from the response and ground truth answer and checking them against other texts. This approach enables fine-grained evaluation instead of response-level assessment. RagChecker processes the user query, retrieved context, response, and ground truth answer, producing a suite of metrics:

1. **Overall Metrics** to provide a holistic view of the system performance, assessing the overall quality of the generated responses.
2. **Diagnostic Retriever Metrics** to evaluate the effectiveness of the retriever, identifying its strengths and weaknesses in finding relevant information from the knowledge base.
3. **Diagnostic Generator Metrics** to assess the performance of the generator, diagnosing how well the generator utilizes the retrieved context, handles noisy information, and generates accurate and faithful responses.

Compared to existing evaluation frameworks, RagChecker provides a more comprehensive assessment of RAG systems. While some frameworks offer fine-grained evaluation only on certain metrics (e.g., RAGAS , TruLens , ARES ) or evaluate specific aspects of RAG (e.g., RGB , RECALL , NoMIRACL ), RagChecker's metrics are all based on fine-grained claim-level checking and are designed to provide actionable insights into the sources of errors.

To ensure the reliability of RagChecker, we annotate a human judgment dataset to assess the correlations between the proposed metrics and human judgments. This meta-evaluation validates the effectiveness of RagChecker in capturing the quality and reliability of RAG systems from a human perspective. We demonstrate the effectiveness of RagChecker through comprehensive experiments evaluating 8 state-of-the-art RAG systems on a benchmark repurposed from public datasets across 10 domains. In-depth analysis of the evaluation results reveals that RagChecker provides insightful diagnostic signals (Sec. 4.3) pointing the directions for improvements of RAG systems (Sec. 4.4).

The main contributions of this paper are as follows:

* We propose RagChecker, a novel RAG evaluation framework that offers fine-grained evaluation for both the retriever and generator components, introducing new diagnostic metrics to provide actionable insights into the sources of errors.
* We conduct meta evaluation and verified RagChecker has significantly better correlations with human judgements than other evaluation metrics.
* We perform extensive experiments evaluating 8 RAG systems on our curated benchmark across 10 domains, and uncover valuable insights, such as the trade-off between retrieval improvement and noise introduction, and the tendency of faithful open-source models to blind trust on context.

## 2 Related Work

### Retrieval Augmented Generation

Large Language Models (LLMs) demonstrate strong capabilities in generating text, but there are also obstacles such as outdated information and the potential to hallucinate [42; 46; 12]. To address these issues, RAG retrieves external knowledge to generate responses with improved accuracy and factuality [7; 53; 13]. Integrating external knowledge is especially crucial in fields like legal, medical and finance, where precision and reliability are essential [24; 50; 55].

RAG systems have shown impressive performance across a range of tasks, including open-domain question answering [27; 10; 18], code generation [32; 57; 38] and dialogue [37; 16; 41]. Additionally, real world products like Bing Search4 and Langchain  have integrated applications based on RAG.

### Evaluation of RAG

Existing evaluation practices for RAG systems can be categorized into two main approaches: evaluating essential capabilities of generators only and assessing end-to-end performance of RAG systems.

Within the two components of a RAG system, the retriever has been well studied in recent years, thus a line of recent work focused on evaluating essential generator capabilities. RGB  evaluated 4 fundamental abilities required for generators including Noise Robustness, Negative Rejection, Information Integration and Counterfactual Robustness by manually constructed test sets. RECALL  introduced manually edited counterfactual contexts into QA and text generation datasets to evaluate the counterfactual robustness of LLMs. NoMIRACL  evaluated LLMs' robustness against first-stage retrieval errors of RAG systems with manually judged relevant and non-relevant datasets. Wu et al.  quantified the tug-of-war between LLMs' faithfulness and internal prior by introducing varying levels of perturbations on the provided contexts. FaaF  introduced a fine-grained fact verification formulation to improve previous prompting-based approaches in evaluating factuality of generators. However, we argue that above generator-only evaluation approaches with manually constructed datasets cannot serve as a general RAG evaluation framework to reveal the entanglement of between generation results and different retrieval behaviors, as shown in the analysis of Sec. 4.3.

Another line of work focused on assessing end-to-end quality scores of RAG systems. TruLens  introduced the concept of RAG Triad, which decompose the quality scores into three aspects: context relevance, groundedness and answer relevance, then predicted the score by prompting LLMs or using NLI models. RAGAS  and ARES  followed the RAG Triad concept and improved the score prediction approaches on different datasets. CRUD-RAG  refered to the CRUD (Create, Read, Update and Delete) actions between users and knowledge bases to develop corresponding datasets and evaluation metrics for RAG systems. We compare the above four evaluation frameworks with RagChecker in the meta evaluation of Sec. 4.2.

Besides, the following work also provided good insight or high quality datasets for end-to-end RAG evaluation. Liu et al.  conducted human evaluation to audit four popular generative search engines in terms of fluency, perceived utility, and verifiability. MEDRAG  constructed a medical RAG benchmark from medical QA datasets and evaluated medical RAG systems with QA accuracy. MultiHop-RAG  generated multi-hop queries from news articles and evaluated RAG systems with QA accuracy. CDQA  proposed a novel approach to generate dynamic QA questions which requires latest information to answer. However, the evaluation metrics used in the work mentioned above rely either on human evaluation or simple textual accuracy, making them incapable of complex RAG scenarios that require long answer evaluation. Therefore, we do not include them in the meta evaluation.

## 3 RagChecker Framework

FormulationDefine a modular RAG system as \(=\{,\}\), where R is the retriever and G is the generator. Given a query \(q\) and documents \(D\), it first retrieves top-\(k\) relevant context \(\{_{j}\}=(q,D,k)\), and then generates a model response \(=(\{_{j}\},q)\). For simplicity, we can also represent the overall RAG generation process as \(=(q,D)\).

Design PrincipleGiven the compositional nature of RAG, we observe there are two major personae using a RAG evaluation framework. The first persona is a user that cares about the overall performance of RAGs and might choose a system with the best performance. Such a persona prefers a single value metric to compare and rank among RAG systems against a benchmark. The second persona is a developer that focuses on improving a RAG system with the need to identify causes of mistakes and potential rooms for improvements. Causes of errors in response can be classified into 1) retrieval errors, where the retriever fails to return complete and relevant context, and 2) generator errors, where the generator struggles to identify and leverage relevant information from context.

Consequently, metrics that reveal error causes should be different from those for overall performance, in the sense that error causes are module-specific or even reflected only by a certain behavior of a module. To help both personae to assess RAG performance, we design RagChecker, a evaluation framework of RAG systems that consists of a benchmark with rich annotations and a set of diversely-purposed fine-grained metrics.

### Inputs to RagChecker

We prepare each sample in our benchmark dataset in the format of a tuple \( q,D,gt\) representing query, documents, and ground-truth answer, where query is the input question to a RAG system, documents form the database providing possible context and are processed into chunks with the same number of tokens, and ground-truth answer is a complete and correct answer for the input question. Further information is provided in Sec. 4.1.

### Fine-grained Evaluation with Claim Entailment

As illustrated in Fig. 1, a response generated by a RAG system might be a mixture of correct ( \(\) ) and incorrect claims ( \(\) ), while also missing some in-ground-truth claims ( \(\) ). In this sense, evaluating responses at a finer granularity is crucial to comprehensively assess the quality of an answer. For this purpose, we introduce two components: 1) a text-to-claim extractor that decomposes a given text \(T\) into a set of claims \(\{c_{i}\}\), and 2) a claim-entailment checker to determine whether a given claim \(c\) is entailed (\(\)) in a reference text \(Ref\) or not (\(\)).

### RagChecker Metrics

With the annotation and claim-level entailment functions specified, we next define the metrics. For a RAG user, we design metrics to compare the performance among RAG systems, including a single-value F1 score as an overall metric. For a RAG developer, on the other hand, we propose two sets of modular metrics for the retriever and the generator in a RAG system respectively, that aim to decompose the system and diagnose the source of errors. In the rest of this section, we will first introduce the overall metrics and then go over modular metrics for retriever and generator separately. The formulas for each metric are summarized in Appendix B.

Figure 1: Illustration of the proposed metrics in RagChecker. The upper Venn diagram depicts the comparison between a model response and the ground truth answer, showing possible correct( \(\) ), incorrect( \(\) ), and missing claims( \(\) ). The retrieved chunks are classified into two categories based on the type of claims they contain. Below, we define the overall, retriever, and generator metrics, illustrating how each component of the RAG system is evaluated for its performance.

#### 3.3.1 Overall Metrics

To assess the overall response quality of a RAG system from a user's perspective, we can compute the precision and recall at claim level for each model generated response against its paired ground-truth answer. Specifically, we first extract claims from a model response \(m\) and a ground-truth answer \(gt\) as \(\{c_{i}^{(m)}\}\) and \(\{c_{i}^{(gt)}\}\) respectively. Then, we define correct claims in the response as \(\{c_{i}^{(m)}|c_{i}^{(m)} gt\}\), and correct claims in the ground-truth answer as \(\{c_{i}^{(gt)}|c_{i}^{(gt)} m\}\). Two metrics can be computed directly: **precision** is the proportion of correct claims in all response claims, and **recall** is the proportion of correct claims in all ground-truth answer claims. Further, the harmonic average of precision and recall gives the **F1** score, as the overall performance metric.

#### 3.3.2 Retriever Metrics

Ideally, a perfect retriever returns precisely all claims needed to generate the ground-truth answer. Completeness-wise, we can measure how many claims made in the ground-truth answer are covered by retrieved chunks. With retrieved chunks as the reference text, we compute **claim recall** as the proportion of \(\{c_{i}^{(gt)}|c_{i}^{(gt)}\{_{j}\}\}\).

Differently, we define the retriever precision at chunk-level instead of claim-level. A retrieved chunk is called _relevant chunk_ (r-chunk), if any ground-truth claim is entailed in it. In other words, \(_{j}\) is a relevant chunk if \( i,\ s.t.\ c_{i}^{(gt)}_{j}\). The rest retrieved chunks are called _irrelevant chunk_ (irr-chunk). The retriever's **context precision** is defined as \(|\{_{j}\}|/k\), where \(k\) is the number of all retrieved chunks.

Note that a chunk-level precision provides better interpretability than a claim-level one, because in practice RAG systems usually work with documents processed to be text chunks in a fixed size. That being said, it is likely that a chunk may contain relevant claims and irrelevant or misleading information at the same time. As a result, the best possible retriever can only achieve a claim-level precision score lower than 100%, and such an upper-bound varies depending on the actual text distribution in \(D\) and chunking strategy.

#### 3.3.3 Generator Metrics

Given \(k\) retrieved chunks (possibly mixing relevant and irrelevant information), a perfect generator would identify and include all ground-truth-relevant claims and ignore any that are not. Because the generator's results have dependency on retrieved chunks, we provide in total six metrics characterizing different aspects of its performance.

Given a model response \(m\) and its claims \(\{c_{i}^{(m)}\}\), we first compute the proportion of \(c_{i}^{(m)}\) that are entailed in retrieved chunks. This metric is **faithfulness**, as it describes how faithful the generator is to the provided context, thus the higher the better.

Next, we examine three types of incorrect response claims, i.e. \(\{c_{i}^{(m)}|c_{i}^{(m)} gt\}\).

1. The first type includes incorrect claim that are entailed in a relevant chunk, then it indicates the generator is sensitive to noise coupled with useful information. The proportion of this type of claims to all \(\{c_{i}^{(m)}\}\) is **relevant noise sensitivity**.
2. The second type includes incorrect claim that are entailed in an irrelevant chunk, then it indicates the generator is also sensitive to noise even in an irrelevant context. The proportion of these incorrect claims is **irrelevant noise sensitivity**.
3. Finally, the third type includes incorrect claims that are not entailed in any retrieved chunk, meaning all such claims are generated by the generator itself. Its proportion is **hallucination**.

Note that for simplicity we group the two noise sensitivities in Fig. 1, but later in Sec. 4.3 we can see that generators generally has different sensitivity to relevant and irrelevant noise.

Finally, we characterize how a generator uses information sources to produce correct claims. A correct claim not entailed by any chunk can only be based on generator's **self-knowledge**, thus the proportion of these claims reflects how many correct claims are generated on its own. A lower self-knowledge score is better, when the generator is expected to fully depend on retrieved context only in a RAG system. On the other hand, we also check how much retrieved relevant information isused by the generator. Retrieved relevant information is measured by the number of ground-truth answer claims entailed in retrieved chunks, while the evidence of being used by generator is reflected by entailment in model response. Therefore, the **context utilization** is computed as the ratio between \(|\{c_{i}^{(gt)}|c_{i}^{(gt)}\{_{j}\}c_{i}^{(gt)} m\}|\) and \(|\{c_{i}^{(gt)}|c_{i}^{(gt)}\{_{j}\}|\). Generally a higher context utilization is preferred.

## 4 Experiments

### Experimental Setup

Baseline RAG SystemsWe apply RagChecker to 8 customized RAG systems to demonstrate how these metrics reflect the properties and differences among them, and how they guide the refinement of these systems. The 8 RAG systems are combinations with 2 retrievers and 4 generators. For retrievers, we choose BM25 , a representative classic sparse retrieval framework, and E5-Mistral , the SOTA open-source dense retriever. Our four generators are GPT-4 , Mistral-8x7B , Llama3-8B, and Llama3-70B , covering open-source and proprietary LLMs in various sizes. Further details are deferred to Appendix D. We employ Llama3-70B as both the claim extractor and checker models implemented by an open-sourced framework RefChecker5. As a validation of its performance on the RefChecker's hallucination detection benchmark, this setup outperforms the best purely open-sourced combinations reported in RefChecker's paper (see Appendix G).

Benchmark DatasetsFor comprehensive evaluations, we curate a benchmark containing 4,162 queries across 10 domains. This benchmark is repurposed from public datasets of open domain question answering, spanning domains of Wikipedia, AI science, novel, biomedical, finance, lifestyle, recreation, science, technology and writing. We convert the short answers to long-form answers in the datasets to align with the current LLM-based RAG systems. Please refer to Appendix A for the details of the benchmark curation process. The statistics of the benchmark are shown in Tab. 1.

### Meta Evaluation

We first conduct the meta evaluation to verify the soundness of RagChecker and compare with existing baseline RAG evaluation frameworks.

**Baseline RAG Evaluation Frameworks** We include a total of 10 metrics from Trulens , RAGAS , ARES  and CRUD-RAG  in the meta evaluation, as they are capable to evaluate end-to-end performance with long answers. Metrics selected for comparison along with their descriptions are summarized in Tab. 4 of Appendix C. To ensure a fair comparison, we use Llama3-70B-Instrt as the LLM backbone when applicable. Since models in the Llama3 family don't provide an embedding model, baseline metrics requiring embedding capability still use their corresponding default LLM backbones. In addition to the 10 metrics detailed in the table, we also incorporate BLEU , ROUGE-L , and BERTScore  to assess the correlation between the generated responses and the ground truth answers.

**Meta Evaluation Dataset** All baseline metrics are designed with different aspects and functionalities to a certain degree, thus making an exact comparison over metric scores inapplicable. However, we argue that a good metric should reflect the relative human preference over different RAG systems. In this spirit, we construct the meta evaluation dataset with sampled instances from the generated responses of 8 baseline RAG systems introduced in Sec. 4.1 on our benchmark. Each meta evaluation instance is a pair of responses from two baseline RAG systems given the same query. By considering all combinations over 10 domains and 28 baseline pairs, we end up with 280 instances for pairwise human preference labeling. For each instance, annotators compare a pair of responses based on correctness, completeness, and overall assessment. For each aspect, annotators measure their preferences as one out of five relative choices, including significantly better (2), slightly better (1), tie (0), slightly worse (-1) and significantly worse (-2). For quality control, each instance is annotated by two annotators, and their overall agreement and correlation are measured. To conclude, we build a meta evaluation dataset with 280 instances, each instance is labeled by two annotators with their preference in terms of correctness, completeness and overall assessment.

**Meta Evaluation Process and Results** Based on the meta evaluation dataset, we perform the following evaluation process. Since the human preference labels can be seen as the score difference of a response pair: \(h_{i}\{-2,-1,0,1,2\}\), with a baseline RAG evaluation model \(E\), we compute a normalized score difference as \(e_{i}=f(E(r_{i}^{2})-E(r_{i}^{1}))[-2,2]\), where \(f\) is a linear normalization function. Our meta evaluation is the correlation between \(h_{i}\) and \(e_{i}\) overall 280 instances as reported in Tab. 2, together with the correlation between \(h_{i}\) and \(h_{i}^{}\) from two annotators as the upper-bound. In addition, we further compute human agreement rate as the proportion of instances satisfying \((h_{i}-h_{i}^{}) 1\), and the result is 90.95%.

From the table, we can observe that RagChecker has the strongest correlation with human preference in terms of three aspects. Among other baseline metrics, Answer Similarity of RAGAS, which is based on the stronger backbone model text-embedding-ada-002 , shows the best performance. We also provide a detailed comparison between RagChecker and this strongest baseline in Fig. 4

  

  
**Baseline** & **Metric** &  &  &  \\   & & **Pearson** & **Spearman** & **Pearson** & **Spearman** & **Pearson** & **Spearman** \\  BLEU & BLEU-avg & 38.89 & 35.32 & 32.13 & 21.85 & 35.14 & 29.42 \\ ROUGE & ROUGE-L & 31.75 & 31.72 & 47.88 & 45.67 & 43.10 & 43.21 \\ BERTScore & BERTScore & 30.34 & 27.05 & 37.93 & 40.05 & 33.51 & 35.57 \\ TruLens & Answer Relevance & 35.01 & 27.37 & 37.24 & 37.91 & 35.15 & 33.59 \\ ARES & Answer Relevance & 18.63 & 16.84 & 20.13 & 18.13 & 17.81 & 16.26 \\ RAGAS & Answer Similarity & 41.07 & 43.21 & 53.16 & **61.35** & 48.31 & 57.23 \\ CRUD-RAG & Recall & 30.93 & 27.13 & 45.11 & 43.76 & 41.25 & 39.71 \\ RAGChecker & Same metric as human & **49.66** & **46.95** & **60.67** & 58.11 & **61.93** & **60.90** \\  Human & Annotator correlation & 63.67 & 59.19 & 71.91 & 68.36 & 70.09 & 68.89 \\   

Table 2: Correlation results with Human Evaluation of Correctness, Completeness, and Overall Assessment. We only show the metric with the best correlation for each baseline framework. Full results can be found in Tab. 5 of Appendix C.

of Appendix C. As an upper bound, the human correlations at the bottom show that there is still a clear gap between model predictions and human annotators.

### Main Results

We present the averaged evaluation results for 8 RAG systems across 10 diverse domain datasets in Tab. 3. Additional results for all datasets are provided in Appendix E. The RAG system that exhibited the best performance in our experiments is E5-Mistral_GPT-4, owing to the strong retrieval capability of E5-Mistral coupled with the adept comprehension abilities of GPT-4. Next, we provide a list of insights induced from Tab. 3, along with their interpretation and possible directions for improvements.

**Retriever Matters Consistently.** The quality of retrieval is crucial, as evidenced by the notable differences in overall Precision, Recall and F1 scores when comparing BM25 with E5-Mistral with the generator fixed. This improvement is agnostic to the specific choice of generator, suggesting a consistent benefit from employing a better retriever.

**Generator Model Size Brings All-Round Improvement.** Paired to the same retriever, Llama3-70B consistently achieves better overall performance than Llama3-8B. More concretely, this superiority is supported by a better performance over every generator metric, such as improved context utilization, reduced noise sensitivity, and less hallucination.

**Stable and Performant Context Utilization is Key.** Among all generator metrics, we observe that context utilization strongly correlates to the overall F1 score, while such correlation is relatively weaker for other generator metrics. Also, generators' context utilization are relatively stable between the two retrievers, meaning their overall recall can be improved with a better retriever. These observations indicate that the capability to fully utilize retrieved context is key, which is intuitive because the generator in a RAG system is expected to leverage context to surpass its self-knowledge.

**Informative Context Improves Faithfulness and Reduces Hallucination.** As E5-Mistral achieves better claim recall, we observe generators paired to it achieves better faithfulness, indicating generators are all capable to identify and leverage information in context. Similarly, hallucination and self-knowledge are both reduced as well.

**Retriever Recall Trades-off with Generator Noise Sensitivity.** Claim recall for a retriever characterizes the coverage of all information necessary to produce ground-truth answer. In practice, however, because of the fixed-size chunking strategy, retrieved relevant chunks may inevitably also carry over noise as part of the context. As retriever claim recall increases, all generators become more sensitive to such noise, which can be explained as their faithfulness to certain context is not discriminative enough. This observation shows that generators' capability to precisely leverage relevant context is still a challenge.

**Relevant Noise Sensitivity is More Challenging.** For every baseline RAG system, there's an apparent gap between its relevant and irrelevant noise sensitivity. In correlation to the last paragraph, it further enhance the point that generators demonstrate a chunk-level faithfulness. It means a relevant

    &  &  &  \\   & Prec.\(\) & Rec.\(\) & F1\(\) & CR\(\) & CP\(\) & CU & NS(I)\(\) & NS(II)\(\) & Hallu.\(\) & SK & Faith.\(\) \\  BM25\_GPT-4 & 61.0 & 49.7 & 50.3 & 74.0 & 52.3 & 61.4 & 26.2 & 4.1 & 8.7 & 3.4 & 87.9 & 12 \\ BM25\_Llama3-8b & 52.1 & 43.9 & 42.1 & 74.0 & 52.3 & 54.9 & 31.3 & 6.1 & 9.8 & 1.8 & 88.4 & 11 \\ BM25\_Llama3-70b & 59.1 & 44.9 & 46.3 & 74.0 & 52.3 & 56.2 & 30.4 & 5.3 & 5.1 & 1.7 & 93.2 & 9 \\ BM25\_Mistral-87b & 52.5 & 44.3 & 42.9 & 74.0 & 52.3 & 54.9 & 34.3 & 5.8 & 6.2 & 1.8 & 92.0 & 9 \\ E5-Mistral\_GPT-4 & 62.0 & 53.0 & 52.7 & 83.5 & 61.8 & 60.4 & 28.9 & 3.5 & 5.7 & 1.4 & 92.3 & 12 \\ E5-Mistral\_Llama3-8b & 53.8 & 48.3 & 45.0 & 83.5 & 61.8 & 55.0 & 33.5 & 5.5 & 6.6 & 0.8 & 92.7 & 11 \\ E5-Mistral\_Llama3-70b & 60.6 & 50.4 & 50.2 & 83.5 & 61.8 & 57.6 & 31.7 & 4.3 & 3.3 & 0.8 & 95.9 & 10 \\ E5-Mistral\_Mistral-8x7b & 53.1 & 48.6 & 45.7 & 83.5 & 61.8 & 55.2 & 36.5 & 5.1 & 4.0 & 0.8 & 95.2 & 10 \\   

Table 3: The averaged evaluation results for different RAG systems across 10 datasets. The overall performance of the RAG system is quantified using precision (Prec.), recall (Rec.), and F1 scores. The retriever component is evaluated based on claim recall (CR) and context precision (CP), while the generator component is diagnosed through context utilization (CU), relevant noise sensitivity (NS(I)), irrelevant noise sensitivity (NS(II)), hallucination (Hallu.), self-knowledge (SK), and faithfulness (Faith.). Additionally, the average number of response claims for each RAG system is provided.

chunk is trusted as a whole, while an irrelevant one only has minimal impact. This subtle yet significant distinction supports and explains the importance of the quality and specification of the database for a RAG system.

**Open-Source Models are Worse at Distinguishing Accurate Information from Noise.** GPT-4 has both higher context utilization and lower noise sensitivity than the other three open source models. Open source models are faithful but tend to trust the context blindly especially when retrieval gets better. This observation raises the need for improving open source models' reasoning ability.

### Diagnosis on RAG Settings for Improvements

Guided by observations in Sec. 4.3, we modify settings commonly tuned in RAG systems that may lead to improvements, diagnose their working mechanisms with RagChecker metrics, and provide suggestions for improvements on certain aspects. We experiment with different numbers of chunks, chunk sizes, chunk overlap ratios, and generation prompts. We highlight our main findings and suggestions as below, please refer to Appendix F for detailed analysis and results.

**More Context Enhances Faithfulness.** Increasing the number (\(k\)) and size of chunks improves the recall of more useful information (_claim recall_ 61.5\(\)77.6 with \(k\) 5\(\)20, 70.3\(\)77.6 with size 150\(\)300). Consequently, this provides more context for the generators to be more faithful to (_faithfulness_ 88.1\(\)92.2 with \(k\) 5\(\)20, 91.2\(\)92.2 with size 150\(\)300), though at the same time they also become more sensitive to additional noise (_noise sensitivity_ 34.0\(\)35.4 with \(k\) 5\(\)20, 34.5\(\)35.4 with size 150\(\)300). Improvements in the overall performance (_F1_ 51.7\(\)53.4 with \(k\) 5\(\)20, 52.6\(\)53.4 with size 150\(\)300) indicates benefits from more context.

**Explicit Requirements in Prompts Affect Generation Preferences.** When prompts introduces explicit requirements for better _faithfulness_, _context utilization_, and lower _noise sensitivity_, generators show improvements in _faithfulness_ (92.2\(\)93.6), but struggle with the subtle tension between _context utilization_ (59.2\(\)63.7) and _noise sensitivity_ (35.4\(\)38.1).

**Chunk Overlap Does Not Matter a Lot.** The chunk overlap ratio is usually set to be non-zero to help generators better utilize surrounding information and identify chunks with coherent logic. However, it minimally affects generation performance, as retrieving more chunks sharing similar useful information (increased _context precision_ 69.3\(\)71.1) does not necessarily increase the total amount of retrieved useful information (comparable _claim recall_ 77.8\(\)78.1).

**Suggestions to RAG Builders** Improving the retriever is an effective way to enhance overall performance. While a better embedding model leads to improvements in both _precision_ and _recall_, moderately increasing the number and size of chunks improves _recall_ and thus _F1_ with minimal efforts in practice. Note that the effect saturates as the total amount of relevant information is fixed, so they need not be too large for a balanced cost-performance. On the other hand, given a limited number of context, larger chunk sizes with fewer chunks are preferred for better _context precision_. However, when targeting better _context utilization_ or reduced _noise sensitivity_, opposite adjustments should be made to alleviate the influence of noise.

When tuning the generator, the trilemma of _context utilization_, _noise sensitivity_, and _faithfulness_ makes it difficult to improve all aspects simultaneously. RAG builders should prioritize certain aspects in the prompt based on their targets, user preferences and the generator's capability.

## 5 Conclusion

This paper presents RagChecker, a novel evaluation framework designed for RAG systems. We validate our comprehensive suite of metrics, both overall and modular, through rigorous human assessments, demonstrating a strong correlation with evaluations conducted by human annotators. We have undertaken a detailed evaluation of eight distinct RAG systems using these metrics, yielding pivotal insights into the behaviors of the retriever and generator components and the trade-offs inherent in RAG system designs. These findings not only deepen our understanding of RAG system architectures but also furnish critical guidance for future advancements in RAG applications.