# A Theory of Link Prediction via

Relational Weisfeiler-Leman on Knowledge Graphs

 Xingyue Huang

Department of Computer Science

University of Oxford

Oxford, UK.

xingyue.huang@cs.ox.ac.uk

&Miguel Romero

Department of Computer Science

Universidad Catolica de Chile

& CENIA Chile

mgromero@uc.cl

Ismail Ilkan Ceylan

Department of Computer Science

University of Oxford

Oxford, UK.

ismail.ceylan@cs.ox.ac.uk

&Pablo Barcelo

Inst. for Math. and Comp. Eng.

Universidad Catolica de Chile

& IMFD Chile & CENIA Chile

pbarcelo@uc.cl

###### Abstract

Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.

## 1 Introduction

Graph neural networks (GNNs) [27; 12] are prominent models for representation learning over graph-structured data, where the idea is to iteratively compute vector representations of nodes of an input graph through a series of invariant (resp., equivariant) transformations. While the landscape of GNNs is overwhelmingly rich, the vast majority of such models are instances of _message passing neural networks_ which are well-studied, leading to a theoretical understanding of their capabilities and limitations [34; 21]. In turn, our understanding is rather limited for GNN models dedicated to learning over _knowledge graphs_, which are applied in a wide range of domains.

To make our context precise, we first consider an extension of message passing neural networks with relation-specific message functions, which we call _relational_ message passing neural networks. Two prominent examples of this framework are RGCN  and CompGCN , and their expressive power has recently been characterized through a dedicated relational Weisfeiler-Leman test .

While offering principled means for learning over knowledge graphs, the standard relational message passing framework is tailored for computing _unary_ node representations and therefore models of this class are better suited for node-level tasks (e.g., node/entity classification). Actually, it is well-known that even a good node-level representation might not necessarily induce a good edge representation,hindering the applicability of such models for the crucial task of link prediction . This has led to the design of GNN architectures specifically tailored for link prediction over knowledge graphs [30; 40; 33; 18], for which our understanding remains limited.

The goal of this paper is to offer a theory of the capabilities and limitations of a class of relational GNN architectures which compute _pairwise_ node representations to be utilized for link prediction. Although most such architectures can be seen to be subsumed by _higher-order_ message passing neural networks that compute pairwise representations on nodes, the inherently quadratic behavior of the latter justifies local approximations which align better with models used in practice. Of particular interest to us is Neural Bellman-Ford Networks (NBFNets) , which define a message passing approach inspired by the Bellman-Ford algorithm. We argue in salient detail that the crucial insight of this approach is in leveraging the idea of computing _conditional_ pairwise-node representations, which leads to more expressive models at a more reasonable computational cost, given its local nature.

Building on this fundamental aspect, we define _conditional_ message passing neural networks, which extend traditional ones by a conditional message passing paradigm: every node representation is conditional on a source node and a query relation, which allows for computing pairwise node representations. This framework strictly contains NBFNets and allows for a systematic treatment of various other models. Through a careful study of this framework, we can explain the conceptual differences between different models along with their respective expressive power.

Our contributions can be summarized as follows:

* We introduce _conditional_ message passing neural networks which encode representations of nodes \(v\) conditioned on a (source) node \(u\) and a query relation \(q\), yielding pairwise node representations. We discuss the model design space, including a discussion on different initialization regimes and the presence of global readout functions in each layer of the network.
* We define a relational Weisfeiler-Leman algorithm (building on similar works such as Barcelo et al. ), and prove that _conditional_ message passing neural networks can match the expressive power of this algorithm. This study reveals interesting insights about NBFNets, suggesting that their strong empirical performance is precisely due to the expressive power, which can be matched by other instances of this framework.
* Viewing _conditional_ message passing neural networks (with or without global readouts) as classifiers over pairs of nodes, we give logical characterizations of their expressive power based on formulas definable in some _binary_ variants of _graded modal logic_[8; 19]. This provides us with a declarative and well-studied formal counterpart of C-MPNNs.
* We conduct an experimental analysis to verify the impact of various model choices, particularly pertaining to initialization, history, message computation, and global readout functions. We also conduct both inductive and transductive experiments on various real-world datasets, empirically validating our theoretical findings.

## 2 Related work and motivation

Early GNNs for knowledge graphs are relational variations of message passing neural networks. A prototypical example is the RGCN architecture , which extends graph convolutional networks (GCNs)  with relation-specific message functions. CompGCN  and several other architectures  follow this line of work with differences in their aggregate, update, and message functions. These architectures encode _unary_ node representations and typically rely on a _pairwise decoder_ function to predict the likelihood of a link which is known to be suboptimal for link prediction . There is a good understanding of the expressive power of these architectures : we generalize these results in Section 3, since they form the basis for the rest of our results.

A different approach is given for single-relational graphs by SEAL , where the idea is to encode (labeled) subgraphs (instead of nodes). GraIL  extends this idea to knowledge graphs, and one important virtue of these models is that they are _inductive_ even if there are no node features in the input graph. The idea is to use a form of labeling trick  based on pairwise shortest path distances in sampled subgraphs, but these architectures suffer from scalability issues. More recent inductive architectures integrate ideas from earlier path-based link prediction approaches [24; 14] into modern GNN architectures, resulting in proposals such as PathCon , Geodesic GNNs , and NBFNets . Our study is very closely related to NBFNets which is inspired by the generalized version of the Bellman-Ford algorithm for finding shortest paths. These architectures aggregate over relational paths by keeping track of conditional pairwise-node representations. While NBFNets can be intuitively seen as the neural counterpart of the _generalized_ Bellman-Ford algorithm, they do _not_ provably align with this algorithm since the "semiring assumption" is invalidated through the use of non-linearities (which is explicit in Zhu et al. ). This leaves open many questions regarding the capabilities and limitations of these architectures.

In this paper, we argue that the key insight behind architectures such as NBFNets is in locally computing _pairwise representations through conditioning_ on a source node, and this has roots in earlier works, such as ID-GNNs . To formally study this, we introduce conditional message passing neural networks as a strict generalization of NBFNets  and related models such as NeuralLP , or DRUM . Through this abstraction, we theoretically study the properties of a large class of models in relation to local variants of relational Weisfeiler-Leman algorithms. Broadly, our study can be seen as the relational counterpart of the expressiveness studies conducted for GNNs [34; 21; 3], particularly related to higher-order GNNs , which align with higher-order dimensional variants of the WL test. Our characterization relies on _local_ versions of higher-order WL tests , albeit not in a relational context. This can be seen as a continuation and generalization of the results given for relational message passing neural networks  to a broader class of models.

## 3 Background

### Knowledge graphs and invariants

**Knowledge graphs.** A _knowledge graph_ is a tuple \(G=(V,E,R,c)\), where \(V\) is a set of nodes, \(E R V V\) is a set of labeled edges, or facts, \(R\) is the set of relation types and \(c:V D\) is a node coloring. When \(D=^{d}\), we also say that \(c\) is a \(d\)-dimensional _feature map_, and typically use \(\) instead of \(c\). We write \(r(u,v)\) to denote a labeled edge, or a fact, where \(r R\) and \(u,v V\). The _neighborhood_ of a node \(v V\) relative to a relation \(r R\) is defined as \(_{r}(v):=\{u r(u,v) E\}\).

**Graph invariants.** We define \(k\)_-ary graph invariants_ following the terminology of Grobe , for which we first define isomorphism over knowledge graphs. An _isomorphism_ from a knowledge graph \(G=(V,E,R,c)\) to a knowledge graph \(G^{}=(V^{},E^{},R,c^{})\) is a bijection \(f:V V^{}\) such that \(c(v)=c^{}(f(v))\) for all \(v V\), and \(r(u,v) E\) if and only if \(r(f(u),f(v)) E^{}\), for all \(r R\) and \(u,v V\). A \(0\)_-ary graph invariant_ is a function \(\) defined on knowledge graphs such that \((G)=(G^{})\) for all isomorphic knowledge graphs \(G\) and \(G^{}\). For \(k 1\), a \(k\)_-ary graph invariant_ is a function \(\) that associates with each knowledge graph \(G=(V,E,R,c)\) a function \((G)\) defined on \(V^{k}\) such that for all knowledge graphs \(G\) and \(G^{}\), all isomorphisms \(f\) from \(G\) to \(G^{}\), and all \(k\)-tuples of nodes \( V^{k}\), it holds that \((G)()=(G^{})(f())\). If \(k=1\), this defines a _node invariant_, or _unary invariant_, and if \(k=2\), this defines a _binary invariant_, which is central to our study.

**Refinements.** A function \((G):V^{k} D\)_refines_ a function \(^{}(G):V^{k} D\), denoted as \((G)^{}(G)\), if for all \(,^{} V^{k}\), \((G)()=(G)(^{})\) implies \(^{}(G)()=^{}(G)(^{})\). We call such functions _equivalent_, denoted as \((G)^{}(G)\), if \((G)^{}(G)\) and \(^{}(G)(G)\). A \(k\)-ary graph invariant \(\)_refines_ a \(k\)-ary graph invariant \(^{}\), if \((G)\) refines \(^{}(G)\) for all knowledge graphs \(G\).

### Relational message passing neural networks

We introduce _relational message passing neural networks (R-MPNNs)_, which encompass several known models such as RGCN  and CompGCN . The idea is to iteratively update the feature of a node \(v\) based on the different relation types \(r R\) and the features of the corresponding neighbors in \(_{r}(v)\). In our most general model we also allow readout functions, that allow further updates to the feature of \(v\) by aggregating over the features of all nodes in the graph.

Let \(G=(V,E,R,)\) be a knowledge graph, where \(\) is a feature map. An _R-MPNN_ computes a sequence of feature maps \(^{(t)}:V^{d(t)}\), for \(t 0\). For simplicity, we write \(^{(t)}_{v}\) instead of \(^{(t)}(v)\). For each node \(v V\), the representations \(^{(t)}_{v}\) are iteratively computed as:

\[^{(0)}_{v} =_{v}\] \[^{(t+1)}_{v} =}(^{(f(t))}_{v},( \{\!\!\{_{r}(^{(t)}_{w})|\;w_{r}(v),r R\!\!\} \!\},}(\{\!\!\{^{(t)}_{w} w V\!\!\}})),\]where Upd, Agg, Read, and Msg\({}_{r}\) are differentiable _update_, _aggregation_, _global readout_, and relation-specific _message_ functions, respectively, and \(f:\) is a _history_ function1, which is always non-decreasing and satisfies \(f(t) t\). An R-MPNN has a fixed number of layers \(T 0\), and then, the final node representations are given by the map \(^{(T)}:V^{d(T)}\).

The use of a readout component in message passing is well-known  but its effect is not well-explored in a relational context. It is of interest to us since it has been shown that standard GNNs can capture a larger class of functions with global readout .

An R-MPNN can be viewed as an encoder function \(\) that associates with each knowledge graph \(G\) a function \((G):V^{d(T)}\), which defines a node invariant corresponding to \(^{(T)}\). The final representations can be used for node-level predictions. For link-level tasks, we use a binary decoder \(_{q}:^{d(T)}^{d(T)}\), which produces a score for the likelihood of the fact \(q(u,v)\), for \(q R\).

In Appendix A.1 we provide a useful characterization of the expressive power of R-MPNNs in terms of a relational variant of the Weisfeiler-Leman test . This characterization is essential for the rest of the results that we present in the paper.

## 4 Conditional message passing neural networks

R-MPNNs have serious limitations for the task of link prediction , which has led to several proposals that compute pairwise representations directly. In contrast to the case of R-MPNNs, our understanding of these architectures is limited. In this section, we introduce the framework of conditional MPNNs that offers a natural framework for the systematic study of these architectures.

Let \(G=(V,E,R,)\) be a knowledge graph, where \(\) is a feature map. A _conditional message passing neural network_ (C-MPNN) iteratively computes pairwise representations, relative to a fixed query \(q R\) and a fixed node \(u V\), as follows:

\[^{(0)}_{v|u,q} =(u,v,q)\] \[^{(t+1)}_{v|u,q} =(^{f(t)}_{v|u,q},(\{\!\!\! \{_{r}(^{(t)}_{w|u,q},_{q})|\;w_{r}(v),r  R\!\!\!\}),(\{\!\!\{^{(t)}_{w|u,q}\;|\;w V\!\!\!\})\! \}),}\]

where \(\), Upd, Agg, Read, and Msg\({}_{r}\) are differentiable _initialization_, _update_, _aggregation_, _global readout_, and relation-specific _message_ functions, respectively, and \(f\) is the history function. We denote by \(^{(t)}_{q}:V V^{d(t)}\) the function \(^{(t)}_{q}(u,v):=^{(t)}_{v|u,q}\), and denote \(_{q}\) to be a learnable vector representing the query \(q R\). A C-MPNN has a fixed number of layers \(T 0\), and the final pair representations are given by \(^{(T)}_{q}\). We sometimes write C-MPNNs _without global readout_ to refer to the class of models which do not use a readout component.

Intuitively, C-MPNNs condition on a source node \(u\) in order to compute representations of \((u,v)\) for all target nodes \(v\). To further explain, we show a visualization of C-MPNNs and R-MPNNs in Figure 1 to demonstrate the differences in the forward pass. Contrary to the R-MPNN model where we carry out relational message passing first and rely on the binary decoder to compute a query fact \(q(u,v)\), the C-MPNN model first initializes all node representations with the zero vector except the representation of the node \(u\) which is assigned a vector with non-zero entry. Following the initialization, we carry out relational message passing and decode the hidden state of the target node \(v\) to obtain the output, which yields the representation of \(v\) conditioned on \(u\).

Observe that C-MPNNs compute binary invariants, provided that the initialization \(\) is a binary invariant. To ensure that the resulting model computes pairwise representations, we require \((u,v,q)\) to be a nontrivial function in the sense that it needs to satisfy _target node distinguishability_: for all \(q R\) and \(v u V\), it holds that \((u,u,q)(u,v,q)\).

This is very closely related to the _Labeling Trick_ proposed by Zhang et al. , which is an initialization method aiming to differentiate a set \(\{u,v\}\) of target nodes from the remaining nodes in a graph. However, the _Labeling Trick_ only applies when both the source \(u\) and target nodes \(v\) are labeled. Recent state-of-the-art models such as ID-GNN  and NBFNet  utilize a similar method, but only with the source node \(u\) labeled differently in initialization. Our definition captures precisely this, and we offer a theoretical analysis of the capabilities of the aforementioned architectures accordingly.

One alternative is to directly learn pairwise representations following similar ideas to those of higher-order GNNs, but these algorithms are not scalable. Architectures such as NBFNets represent a trade-off between computational complexity and expressivity. The advantage of learning conditional representations \(_{v|u,q}\) is to be able to learn such representations in parallel for all \(v V\), amortizing the computational overhead; see Zhu et al.  for a discussion. We have also carried out a runtime analysis comparison among different classes of models in Appendix B.

### Design space and basic model architectures

To specify a C-MPNN architecture, we need to specify the functions \(\), \(\), \(_{r}\), \(f\), and Read. In the following, we consider three initialization functions \(\{^{1},^{2},^{3}\}\), two aggregation functions \(\{,\}\), three message functions \(\{^{1}_{r},^{2}_{r},^{3}_{r}\}\), two history functions \(\{f(t)=t,f(t)=0\}\), and either _sum global readout_ or no readout term.

**Initialization.** We consider the following natural variants for initialization:

\[^{1}(u,v,q)=_{u=v}*,^{2}(u,v, q)=_{u=v}*_{q},^{3}(u,v,q)=_{u=v}*( _{q}+_{u}),\]

where \(*\) represents element-wise multiplication, the function \(_{u=v}(v)\) is the indicator function which returns the all-ones vector \(\) if \(u=v\) and the all-zeros vector \(\) otherwise with corresponding size.

Clearly, both \(^{1}\) and \(^{2}\) satisfy _target node distinguishability_ assumption if we assume \(_{q}\) has no zero entry, where \(^{2}\), in addition, allows query-specific initialization to be considered. Suppose we further relax the condition to be _target node distinguishability in expectation_. Then, \(^{3}\) can also distinguish between each conditioned node \(u\) given the same query vector \(_{q}\) by adding an error vector \(_{u}\) sampled from \((0,1)\) to the conditioned node's initialization.

**Aggregation.** We consider sum aggregation and Principal Neighborhood Aggregation (PNA) .

**Message.** We consider the following variations of _message_ functions:

\[^{1}_{r}(^{(t)}_{w|u,q},_{q}) =^{(t)}_{w|u,q}*^{(t)}_{r}_{q},\] \[^{2}_{r}(^{(t)}_{w|u,q},_{q}) =^{(t)}_{w|u,q}*^{(t)}_{r},\] \[^{3}_{r}(^{(t)}_{w|u,q},_{q}) =^{(t)}_{r}^{(t)}_{w|u,q},\]

where \(^{(t)}_{r}\) are relation-specific transformations. \(^{1}_{r}\) computes a query-dependent message, whereas \(^{2}_{r}\) and \(^{3}_{r}\) are analogous to message functions of CompGCN and RGCN, respectively.

Figure 1: Visualization of R-MPNN and C-MPNN. The dashed arrow is the target query \(q(u,v)\). Arrow colors indicate distinct relation types, while node colors indicate varying hidden states. R-MPNN considers a unary encoder and relies on a binary decoder, while C-MPNN first initializes binary representation based on the target query \(q(u,v)\), and then uses a unary decoder.

**History.** In addition, we can set \(f\), which intuitively is the function that determines the history of node embeddings to be considered. By setting \(f(t)=t\), we obtain a standard message-passing algorithm where the update function considers the representation of the node in the previous iteration. We can alternatively set \(f(t)=0\), in which case we obtain (a generalization of) NBFNets.

**Readout.** We consider a standard readout which sums the representations and applies a linear transformation on the resulting representations. Alternatively, we consider the special case, where we omit this component (we discuss a dedicated readout operation in our empirical analysis later).

**A simple architecture.** Consider a _basic_ C-MPNN architecture with global readout, which, for a query relation \(q R\) and a fixed node \(u\), updates the representations as:

\[^{(0)}_{v|u,q} =_{u=v}*_{q}\] \[^{(t+1)}_{v|u,q} =^{(t)}_{0}^{(t)}_{v|u,q}+_{ r R}_{w_{r}(v)}^{1}_{r}(^{(t)}_{w|u,q}, _{q})+^{(t)}_{1}_{w V}^{(t)}_{w|u,q},\]

where \(^{(t)}_{0},^{(t)}_{1}\) are linear transformations followed by a non-linearity \(\).

## 5 Characterizing the expressive power

### A relational Weisfeiler-Leman characterization

To analyze the expressive power of C-MPNNs, we introduce the _relational asymmetric local \(2\)-WL_, denoted by \(_{2}\). In this case, we work with knowledge graphs of the form \(G=(V,E,R,c,)\), where \(:V V D\) is a _pairwise coloring_. We say that \(\) satisfies _target node distinguishability_ if \((u,u)(u,v)\) for all \(u v V\). The notions of isomorphism and invariants extend to this context in a natural way. For each \(t 0\), we update the coloring as:

\[_{2}^{(0)}(u,v) =(u,v),\] \[_{2}^{(t+1)}(u,v) =_{2}^{(t)}(u,v),\{\!\{\!(_{ 2}^{(t)}(u,w),r) w_{r}(v),r R\}\!\},\]

where \(\) injectively maps the above pair to a unique color, which has not been used in previous iterations. Observe that \(_{2}^{(t)}\) defines a binary invariant, for all \(t 0\).

The test is asymmetric: given a pair \((u,v)\), we only look at neighbors of \((u,v)\) obtained by changing the second coordinate of the pair. In contrast, usual versions of (local) \(k\)-WL are symmetric as neighbors may change any coordinate. Interestingly, this test characterizes the power of C-MPNNs in terms of distinguishing pairs of nodes.

**Theorem 5.1**.: _Let \(G=(V,E,R,,)\) be a knowledge graph, where \(\) is a feature map and \(\) is a pairwise coloring satisfying target node distinguishability. Let \(q R\) be any query relation. Then:_

1. _For all C-MPNNs with_ \(T\) _layers and initializations_ \(\) _with_ \(\)_, and_ \(0 t T\)_, we have_ \(_{2}^{(t)}_{q}^{(t)}\)_._
2. _For all_ \(T 0\) _and history function_ \(f\)_, there is a C-MPNN without global readout with_ \(T\) _layers and history function_ \(f\) _such that for all_ \(0 t T\)_, we have_ \(_{2}^{(t)}_{q}^{(t)}\)_._

The idea of the proof is as follows: we first show a correspondent characterization of the expressive power of R-MPNNs in terms of a relational variant of the WL test (Theorem A.1). This result generalizes results from Barcelo et al. . We then apply a reduction from C-MPNNs to R-MPNNs, that is, we carefully build an auxiliary knowledge graph (encoding the pairs of nodes of the original knowledge graph) to transfer our R-MPNN characterization to our sought C-MPNN characterization.

Note that the lower bound (item (2)) holds even for the _basic_ model of C-MPNNs (without global readout) and the three proposed message functions from Section 4.1. The expressive power of C-MPNNs is independent of the history function as in any case it is matched by \(_{2}\). This suggests that the difference between traditional message passing models using functions \(f(t)=t\) and path-based models (such as NBFNets ) using \(f(t)=0\) is not relevant from a theoretical point of view.

### Logical characterization

We now turn to the problem of which _binary classifiers_ can be expressed as C-MPNNs. That is, we look at C-MPNNs that classify each pair of nodes in a knowledge graph as true or false. Following Barcelo et al. , we study _logical_ binary classifiers, i.e., those that can be defined in the formalism of first-order logic (FO). Briefly, a first-order formula \((x,y)\) with two free variables \(x,y\) defines a logical binary classifier that assigns value true to the pair \((u,v)\) of nodes in knowledge graph \(G\) whenever \(G(u,v)\), i.e., \(\) holds in \(G\) when \(x\) is interpreted as \(u\) and \(y\) as \(v\). A logical classifier \((x,y)\) is _captured_ by a C-MPNN \(\) if over every knowledge graph \(G\) the pairs \((u,v)\) of nodes that are classified as true by \(\) and \(\) are the same.

A natural problem then is to understand what are the logical classifiers captured by C-MPNNs. Fix a set of relation types \(R\) and a set of pair colors \(\). We consider knowledge graphs of the form \(G=(V,E,R,)\) where \(\) is a mapping assigning colors from \(\) to pairs of nodes from \(V\). In this context, FO formulas can refer to the different relation types in \(R\) and the different pair colors in \(\). Our first characterization is established in terms of a simple fragment of FO, which we call \(^{3}_{}\), and is inductively defined as follows: First, \(a(x,y)\) for \(a\), is in \(^{3}_{}\). Second, if \((x,y)\) and \((x,y)\) are in \(^{3}_{}\), \(N 1\) is a positive integer, and \(r R\), then the formulas

\[(x,y),(x,y)(x,y),^{ N}z\,( (x,z) r(z,y))\]

are also in \(^{3}_{}\). Intuitively, \(a(u,v)\) holds in \(G=(V,E,R,)\) if \((u,v)=a\), and \(^{ N}z\,((u,z) r(z,v))\) holds in \(G\) if \(v\) has at least \(N\) incoming edges labeled \(r R\) from nodes \(w\) for which \((u,w)\) holds in \(G\). We use the acronym \(^{3}_{}\) as this logic corresponds to a restriction of FO with three variables and counting. We can show the following result which is the first of its kind in the context of knowledge graphs:

**Theorem 5.2**.: _A logical binary classifier is captured by C-MPNNs without global readout if and only if it can be expressed in \(^{3}_{}\)._

The idea of the proof is to show a logical characterization for R-MPNNs (without global readout) in terms of a variant of graded modal logic called \(^{2}_{}\) (Theorem A.11), which generalizes results from Barcelo et al.  to the case of multiple relations. Then, as in the case of Theorem 5.1, we apply a reduction from C-MPNNs to R-MPNNs (without global readout) using an auxiliary knowledge graph and a useful translation between the logics \(^{2}_{}\) and \(^{3}_{}\).

Interestingly, arbitrary C-MPNNs (with global readout) are strictly more powerful than C-MPNNs without global readout in capturing logical binary classifiers: they can at least capture a strict extension of \(^{3}_{}\), denoted by \(^{3}_{}\).

**Theorem 5.3**.: _Each logical binary classifier expressible in \(^{3}_{}\) can be captured by a C-MPNN._

Intuitively speaking, our logic \(^{3}_{}\) from Theorem 5.2 only allows us to navigate the graph by moving to neighbors of the "current node". The logic \(^{3}_{}\) is a simple extension that allows us to move also to non-neighbors (Proposition A.15 shows that this logic actually gives us more power). Adapting the translation from logic to GNNs (from Theorem A.11), we can easily show that C-MPNNs with global readout can capture this extended logic.

The precise definition of \(^{3}_{}\) together with the proofs of Theorems 5.2 and 5.3 can be found in Appendices A.3 and A.4, respectively. Let us stress that \(^{3}_{}\) and \(^{3}_{}\) correspond to some binary variants of _graded modal logic_. As in Barcelo et al. , these connections are exploited in our proofs.

### Locating \(_{2}\) in the relational WL landscape

Let us note that \(_{2}\) strictly contains \(_{1}\), since intuitively, we can degrade the \(_{2}\) test to compute unary invariants such that it coincides with \(_{1}\). As a result, this allows us to conclude that R-MPNNs are less powerful than C-MPNNs. The \(_{2}\) test itself is upper bounded by a known relational variant of \(2\)-WL, namely, the _relational (symmetric) local 2-WL_ test, denoted by \(_{2}\). Given a knowledgegraph \(G=(V,E,R,c,)\), this test assigns pairwise colors via the following update rule:

\[_{2}^{(t+1)}(u,v)= _{2}^{(t)}(u,v),\! _{2}^{(t)}(w,v),r w_{r}(u),r R}},\] \[\!_{2}^{(t)}(u,w),r w _{r}(v),r R}}\]

This test and a corresponding neural architecture, for arbitrary order \(k 2\), have been recently studied in Barcelo et al.  under the name of _multi-relational local \(k\)-WL_.

This helps us to locate the test \(_{2}\) within the broader WL hierarchy, but it does not align perfectly with the practical setup: one common practice in link prediction is to extend knowledge graphs with inverse relations [40; 35; 25; 32] which empirically yields stronger results and hence is used in most practical setups. However, the effect of this choice has never been quantified formally. We formally explain the benefits of this design choice, showing that it leads to provably more powerful models. The idea is to consider tests (and architectures) which are augmented with the inverse edges: we write \(_{2}^{+}\) and \(_{2}^{+}\) to denote the corresponding augmented tests and prove that it results in more expressive tests (and hence architectures) in each case, respectively. The precise tests and propositions, along with their proofs can be found in Appendix A.5. We present in Figure 2 the resulting expressiveness hierarchy for all these tests.

## 6 Experimental evaluation

We experiment on knowledge graph benchmarks and aim to answer the following questions: **Q1.** What is the impact of the history function on the model performance? In particular, do models with \(f(t)=t\) perform comparably to those with \(f(t)=0\) as our theory suggests? **Q2.** How do the specific choices for aggregation and message functions affect the performance? **Q3.** What is the impact of the initialization function on the performance? What happens when the target identifiability property does not hold? **Q4.** Do C-MPNNs outperform R-MPNNs empirically? **Q5.** Does the use of a global readout, or a relational version affect the performance?

### Experimental setup

**Datasets.** We use the datasets WN18RR  and FB15k-237 , for inductive relation prediction tasks, following a standardized train-test split given in four versions . We augment each fact \(r(u,v)\) with an inverse fact \(r^{-1}(v,u)\). There are no node features for either of the datasets, and the initialization is given by the respective initialization function \(\). This allows all the proposed GNN models to be applied in the inductive setup and to better align with the corresponding relational Weisfeiler-Leman algorithms. The statistics of the datasets are reported in Table 5 of Appendix C.1. The code for experiments is reported in https://github.com/HxyScothuang/CMPNN.

**Implementation.** All models use 6 layers, each with 32 hidden dimensions. The decoder function parameterizes the probability of a fact \(q(u,v)\) as \(p(v u,q)=(f(_{v|u,q}^{(T)}))\), where \(\) is the sigmoid function, and \(f\) is a 2-layer MLP with 64 hidden dimensions. We adopted layer-normalization  and short-cut connection after each aggregation and before applying ReLU. For the experiments concerning the message function \(_{r}^{3}\), we follow the basis decomposition for the FB15k-237 dataset with 30 basis functions for sum aggregation, and 15 for PNA aggregation. We ran the experiments for 20 epochs on 1 Tesla T4 GPU. with mild modifications to accommodate all architectures studied in this paper. We discard the edges that directly connect query node pairs to prevent overfitting. The best checkpoint for each model instance is selected based on its performance on the validation set. All hyperparameter details are reported in Table 6 of Appendix C.1.

**Evaluation.** We consider _filtered ranking protocol_: for each test fact \(r(u,v)\), we construct 50 negative samples \(r(u^{},v^{})\), randomly replacing either the head entity or the tail entity, and we report Hits@10, the rate of correctly predicted entities appearing in the top 10 entries for each instance list prediction. We report averaged results of _five_ independent runs for all experiments.

Figure 2: Expressiveness hierarchy: \(A B\) iff \(A B\). By Proposition A.20, \(_{2}\) and \(_{2}^{+}\) are incomparable. The case of \(_{2}^{+}_{2}^{+}\) is analogous to Proposition A.18.

### Results for inductive link prediction with C-MPNN architectures

We report inductive link prediction results for different C-MPNN architectures in Table 1, all initialized with \(^{2}\). Each row of Table 1 corresponds to a specific architecture, which allows us to compare the model components. Note that while NBFNets  use different message functions for different datasets, we separately report for each model architecture to specifically pinpoint the impact of each model component.

**History functions (Q1).** First, we note that there is no significant difference between the models with different history functions. Specifically, for any choice of aggregate and message functions, the model which sets \(f(t)=t\) performs comparably to the one which sets \(f(t)=0\). This supports our theoretical findings, which state that path-based message passing and traditional message passing have the same expressive power. This may appear as a subtle point, but it is important for informing future work: the strength of these architectures is fundamentally due to their ability to compute more expressive _binary invariants_, which holds regardless of the choice of the history function.

**Message functions (Q2).** We highlight that there is no significant difference between different message functions on WN18RR, which is unsurprising: WN18RR splits contain at most 11 relation types, which undermines the impact of the differences in message functions. In contrast, the results on FB15k-237 are informative in this respect: \(_{r}^{2}\) clearly leads to worse performance than all the other choices, which can be explained by the fact that \(_{r}^{2}\) utilizes fewer relation-specific parameters. Importantly, \(_{r}^{3}\) appears strong and robust across models. This is essentially the message function of RGCN and uses basis decomposition to regularize the parameter matrices. Architectures using \(_{r}^{3}\) with fewer parameters (see the appendix) can match or substantially exceed the performance of the models using \(_{r}^{1}\), where the latter is the primary message function used in NBFNets. This may appear counter-intuitive since \(_{r}^{3}\) does not have a learnable query vector \(_{q}\), but this vector is nonetheless part of the model via the initialization function \(^{2}\).

**Aggregation functions (Q2).** We experimented with aggregation functions \(\) and \(\). We do not observe significant trends on WN18RR, but \(\) tends to result in slightly better-performing architectures. On FB15k-237, there seems to be an intricate interplay between aggregation and message functions. For \(_{r}^{1}\), \(\) appears to be a better choice than \(\). On the other hand, for both \(_{r}^{2}\) and \(_{r}^{3}\), sum aggregation is substantially better. This suggests that a sophisticated aggregation, such as \(\), may not always be necessary since it can be matched (and even outperformed) with a sum aggregation. In fact, the model with \(\) aggregation and \(_{r}^{3}\) is very closely related to RGCN and appears to be one of the best-performing models across the board. This supports our theory since, intuitively, this model can be seen as an adaptation of RGCN to compute binary invariants while keeping the choices for model components the same as RGCN.

    &  &  \\ Agg & \(_{r}\) & \(f(t)\) & **v1** & **v2** & **v3** & **v4** & **v1** & **v2** & **v3** & **v4** \\  sum & \(_{r}^{1}\) & \(0\) & \(0.934\) & \(0.896\) & \(0.894\) & \(0.881\) & \(0.784\) & \(0.900\) & \(0.940\) & \(0.923\) \\ sum & \(_{r}^{1}\) & \(t\) & \(0.932\) & \(0.896\) & \(\) & \(0.881\) & \(0.794\) & \(0.906\) & \(\) & \(0.933\) \\  sum & \(_{r}^{2}\) & \(0\) & \(0.939\) & \(\) & \(0.881\) & \(0.881\) & \(0.734\) & \(0.899\) & \(0.911\) & \(0.941\) \\ sum & \(_{r}^{2}\) & \(t\) & \(0.937\) & \(\) & \(0.865\) & \(\) & \(0.728\) & \(0.883\) & \(0.929\) & \(0.931\) \\  sum & \(_{r}^{3}\) & \(0\) & \(\) & \(0.898\) & \(0.888\) & \(0.877\) & \(\) & \(0.934\) & \(0.919\) & \(0.941\) \\ sum & \(_{r}^{3}\) & \(t\) & \(0.934\) & \(0.896\) & \(0.892\) & \(0.880\) & \(0.844\) & \(\) & \(0.926\) & \(\) \\   PNA & \(_{r}^{1}\) & \(0\) & \(0.943\) & \(0.897\) & \(0.898\) & \(0.886\) & \(0.801\) & \(0.945\) & \(0.934\) & \(\) \\ PNA & \(_{r}^{1}\) & \(t\) & \(0.941\) & \(0.895\) & \(\) & \(0.886\) & \(\) & \(\) & \(\) & \(0.954\) \\  PNA & \(_{r}^{2}\) & \(0\) & \(0.946\) & \(0.900\) & \(0.896\) & \(0.887\) & \(0.715\) & \(0.896\) & \(0.887\) & \(0.886\) \\ PNA & \(_{r}^{2}\) & \(t\) & \(\) & \(\) & \(0.901\) & \(\) & \(0.709\) & \(0.899\) & \(0.875\) & \(0.894\) \\  PNA & \(_{r}^{3}\) & \(0\) & \(\) & \(0.898\) & \(0.899\) & \(0.884\) & \(0.788\) & \(0.908\) & \(0.906\) & \(0.927\) \\ PNA & \(_{r}^{3}\) & \(t\) & \(0.944\) & \(0.897\) & \(0.894\) & \(0.882\) & \(0.795\) & \(0.916\) & \(0.908\) & \(0.926\) \\   

Table 1: Inductive relation prediction with C-MPNNs using \(^{2}\) initialization and _no_ readout. The best results for each category are shown in **bold** and the second best results are underlined.

[MISSING_PAGE_FAIL:10]

Acknowledgement

The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work (http://dx.doi.org/10.5281/zenodo.22558). We would also like to thank Google Cloud for kindly providing computational resources. Barcelo is funded by ANID-Millennium Science Initiative Program - CodeICN17002. Romero is funded by Fondecyt grant 11200956. Barcelo and Romero are funded by the National Center for Artificial Intelligence CENIA FB210017, BasalANID.