# Birder: Communication-Efficient 1-bit Adaptive Optimizer for Practical Distributed DNN Training

Hanyang Peng\({}^{1}\)1, Shuang Qin\({}^{1}\)1, Yue Yu\({}^{1}\)2, Jin Wang\({}^{1}\), Hui Wang\({}^{1}\), Ge Li\({}^{2}\)2

\({}^{1}\)Peng Cheng Laboratory, Shenzhen, China

\({}^{2}\)School of Electronic and Computer Engineering,

Shenzhen Graduate School, Peking University, Shenzhen, China

penghy@pcl.ac.cn, qinsh@pcl.ac.cn, yuy@pcl.ac.cn

wangj05@pcl.ac.cn, wangh06@pcl.ac.cn, geli@ece.pku.edu.cn

Equal ContributionCorresponding Author

###### Abstract

Various gradient compression algorithms have been proposed to alleviate the communication bottleneck in distributed learning, and they have demonstrated effectiveness in terms of high compression ratios and theoretical low communication complexity. However, when it comes to practically training modern deep neural networks (DNNs), these algorithms have yet to match the inference performance of uncompressed SGD-momentum (SGDM) and adaptive optimizers (_e.g._, Adam). More importantly, recent studies suggest that these algorithms actually offer no speed advantages over SGDM/Adam when used with common distributed DNN training frameworks ( _e.g._, _DistributedDataParallel (DDP)_) in the typical settings, due to heavy compression/decompression computation or incompatibility with the efficient _All-Reduce_ or the requirement of uncompressed warmup at the early stage. For these reasons, we propose a novel 1-bit adaptive optimizer, dubbed **B**inary **r**andomization **a**d**aptive **o**tim**er (**B**irder**). The quantization of **B**irder can be easily and lightly computed, and it does not require warmup with its uncompressed version in the beginning. Also, we devise _Hierarchical-1-bit-All-Reduce_ to further lower the communication volume. We theoretically prove that it promises the same convergence rate as the Adam. Extensive experiments, conducted on 8 to 64 GPUs (1 to 8 nodes) using _DDP_, demonstrate that **B**irder achieves comparable inference performance to uncompressed SGDM/Adam, with up to \(\) speedup for training ResNet-50 and \(\) speedup for training BERT-Base. Code is publicly available at https://openi.pcl.ac.cn/c2net_optim/Birder.

## 1 Introduction

With the rapid development of computational power, "bigger" and "bigger" deep neural network (DNN) models are proposed for expect better performance, from the early classical models, such as AlexNet(61 million parameters) , and ResNet (ResNet-50: 20.5 million parameters)  to the current large language models (LLMs), such as BERT (BERT-Lagre: 340 million parameters), and GPT (GPT-3: 176 billion parameters). Scalable parallelism across distributed computing workers for training these large-scale models becomes a necessity. During training, millions to billions of parameters need to be communicated among workers at each iteration, so distributed large-scale DNN training almost invariably suffers from the communication bottleneck.

To address the communication bottleneck, a wide variety of lossy gradient compression algorithms have been proposed to lower the communication overhead. The algorithms can be broadly dividedinto three categories based on the compression techniques, including low-precision approximation (_e.g._, SignSGD, TernGrad, and QSGD, 1-bit Adam), low-rank simplification (_e.g._, ATOMO, PowerSGD, and GradZip), and sparsification (_e.g._, Random-\(k\), Top-\(k\), and MSRTop-\(k\)). In addition to the specific compression techniques, some other works, such as Error Feedback(EF), EF21, DIANA and MARINA, focus on changing the compression objects from the gradient to the gradient and delayed error summation, or the gradient differences to mitigate compressing errors and/or accelerate the convergence rate.

Gradient compression algorithms have demonstrated promising results with a high compression ratio and low oracle/communication complexity in theory. However, when practically training DNNs, they are still **inferior to** uncompressed SGDM/Adam in terms of inference performance. This is because, these gradient compression algorithms are SGD-type optimizers, which will be commonly reduced to vanilla SGD without momentum if compression is not employed. The performance for a compressed optimizer is commonly upper bounded by its uncompressed counterpart, while vanilla SGD is typically less effective than SGDM for training DNNs. Particularly, SGD-type optimizers are known to be substantially inferior to adaptive optimizers (_e.g._, Adam) for training Transformer-based networks, which have become predominant in the DNN community. _Supporting empirical evidences for this phenomenon can be found in Section B in the Appendix._ Furthermore, if we apply the techniques of gradient compression algorithms to compress and communicate the gradients, and subsequently utilize the compressed gradients to construct adaptive optimizers in the local nodes, the final performance will be degraded. Therefore, designing native communication-compression adaptive optimizers is an underexplored problem that requires further research.

As for the system-level speed, recent studies (,) pointed out, when distributedly training typical DNN models (_e.g._, ResNet-50 and BERT-Base) with off-the-shelf _DistributedDataParallel (DDP)_ at typical bandwidths (_e.g._, 10Gbps), existing gradient compression algorithms are still **slower** than uncompressed SGDM/Adam. This is because, the compressor for these algorithms are either quantization or sparsification or low-rank simplification, which exhibit one or more weaknesses below. (\(i\)) Vector-wise quantization and low-rank simplification compressors are commonly computationally heavy, and their time cost, in some cases, is close to and even larger than the savings from the reduces communications, as empirical evidence has shown in  ; (\(ii\)) Sparsification compressors and bias quantization compressor are not naively combatable with the efficient communication primitive_All-Reduce_ due to their inherent structures, and they have to utilize _All-Gather_ for aggregation in stead, which will significantly slow down the communication speed, as empirically shown in  ; (\(iii\)) Some low-rank simplification compressors and quantization compressors need to harness their uncompressed counterparts for warm-up at the early stage to stabilize the convergence, and the warm-up time is commonly nontrivial which to some extent renders their high compression ratios vacuous. Therefore, from a system-level perspective, the design ethos of a system-efficient communication-compression algorithm is that we should guarantee that the compression/decompression of the algorithm is computationally light and takes less time, and it should also be friendly to efficient collective communication primitives. Additionally, there is no need to resort to an uncompressed optimizer for warm-up.

To this end, we propose a 1-bit adaptive optimizer, called **B**inary **r**andomization **a**d**aptive **o**m**(**B**irder), which use the following updating rule is \(x_{t+1}=x_{t}-_{t}(}{b_{t}})\) where \(m_{t}= m_{t-1}+(1-)g_{t}\), \(b_{t}= b_{t-1}+(1-)|g_{t}|\) and \(g_{t}\) is the gradient, and \(()\) is a element-wise binary quantization operator. The main difference between **B**irder and existing gradient-quantization algorithms is that we directly quantize the entire adaptive update \(}{b_{t}}\) rather than quantize the gradient \(g_{t}\) or the momentum \(m_{t}\). Because \(-1)_{j}}{(b_{t})_{j}} 1\), where \((m_{t})_{j}\), \((b_{t})_{j}\) are the \(j^{th}\) element of \(m_{t}\), \(b_{t}\) respectively, each element of \(}{b_{t}}\) is easy to be randomly quantized to \(1\) or \(-1\) in probability, making the quantization computationally light. Another advantage of **B**irder is that it does not require a full-precision optimizer to warm up at the early stage to ensure stable convergence. We also demonstrate **B**irder's convergence rate can match that of Adam. Moreover, taking into accost the nature of **B**irder, we devise an efficient hierarchical communication scheme to further speed up communication, which sufficiently leverages the ultra-high intra-bandwidth among GPUs within the same node.

In particular, we make the following key **contributions**:

* We propose a novel 1-bit optimizer, dubbed **B**irder, **which is a native communication-compression _adaptive_ **algorithm that _element-wise_ **quantizes the entire model update and does not need to leverage its uncompressed counterpart for warm-up**, making compression/decompression computationally light and the extreme quantization ratio exert its best function (Section 2).
* We theoretically prove that despite employing extreme 1-bit quantization is employed, **Birder still promise the same convergence speed as the full-precision Adam** (Section 3).
* We develop a new communication scheme for 1-bit communication, called _Hierarchical-1-bit-All-Reduce_, **which sufficiently harnesses the ultra-fast intra-connects to accelerate the local communication, and utilize more efficient commutation primitives to further reduce the communication overhead** (Section 4).
* We perform extensive distributed training experiments to demonstrate the effectiveness of the proposed algorithm. **As far as we know, running with _DDP, our algorithm is the first work to consistently trump SGDM/Adam in terms of entire running time at little/no inference performance cost**, reaching up to **2.47\(\)** speedup for ResNet-50 and **6.26\(\)** speedup for BERT-Base on \(64\) GPUs. (Section 5).

## 2 One-Bit Adaptive Optimizer Birder

In this section, we focus on solving the following problem for distributed training :

\[_{x^{d}}f(x)=_{i=1}^{n}f_{i}(x;^{(i)})\] (1)

where \(x\) is the \(d\)-dimensional model parameter, \(n\) is the number of distributed workers. \(^{(i)}\) is the sampled min-batch data on the \(i\)-the worker. The sampled min-batch data on all the workers is independent and identically distributed (_i.i.d._). \(f_{i}(x;^{(i)})\) is the loss function. Note that \(f_{i}(x;_{i})\) is commonly abbreviated as \(f_{i}(x)\) in the following.

When distributedly training large-scale DNN models, using vanilla full-precision optimizers can cause communication bottleneck issues in gradient communication among workers at each iteration. To alleviate this problem, elegant SignSGD was proposed, which merely takes the sign of each coordinate of the gradients. While this algorithm can substantially reduce the communication overhead, its practical performance is still inferior to popular adaptive optimizers, such as Adam. Fortunately, we observe that the mathematical formulations of SignSGD and Adam have close connections, providing an opportunity to propose a new optimizer that can combine their merits. This new optimizer can considerably reducing the communication volume with light computation, while maintaining fast convergence speed and high inference performance.

The mathematical updating rule of SignSGD can be formulated as:

\[x_{t+1} x_{t}-_{t}(g_{t})=x_{t}-_{t}}{|g_{t}|}\] (2)

where \(_{t}\) is the learning rate, \(g_{t}\) denotes the estimated unbias noisy gradient of \(f(x_{t})\) with random samples, \(()\) is a element-wise signum, and \(||\) is an element-wise absolute operator.

Whereas the updating rule of vanilla Adam can be expressed as:

\[& m_{t}_{1}m_{t-1}+(1-_{1})g_{t}, \\ & v_{t}_{2}v_{t-1}+(1-_{2})g_{t}^{2},\\ & x_{t+1} x_{t}-_{t}}{}}, \] (3)

where \(_{1}\) and \(_{2}\) represents the exponential moving average factors 3.

If taking \(_{1}\) and \(_{2}\) to zero, \(_{1},_{2} 0\) in Eq. (3), Adam will be reduced to SignSGD.

Given the observations above, we propose a new optimizer that is an intermediate between SignSGD and Adam, referred to as Binder, _i.e._,

\[& m_{t} m_{t-1}+(1-)g_{t},\\ & b_{t} b_{t-1}+(1-)|g_{t}|,\\ & x_{t+1} x_{t}-_{t}( }{b_{t}}),\] (4)

where the \(j\)-th elements of \(m_{t},b_{t}\) rigorously satisfies \(-1)_{j}}{(b_{t})_{j}} 1\), \(()\) is an element-wise quantization operator, and it quantizes the \(j\)-th element of \(}{b_{t}}\) as follows:

\[()_{j}}{(b_{t})_{j}})=\{[] {ll}1,&p=()_{j}}{(b_{t})_{j}}+1)\\ -1,&1-p.,\] (5)

where \(((}{b_{t}}))=}{b_{t}}\), so \(()\) is unbiased, and proof is provided in Section A of the appendix. The detailed implementation of Binder in a parameter-server model is illustrated in Algorithm 1.

```
1:Input: all workers’s model parameter \(x_{0},x_{1}\), the \(i^{th}\) worker’s momentum \(m_{0}^{(i)}=0\), \(b_{0}^{(i)}=0\), the \(i^{th}\) worker’s local error \(e_{0}^{(i)}=0\), server’s global error \(_{0}=0\), exponential moving average factor \(\), the threshold \(T_{0}\), and the learning rate sequence \(\{_{t}\}\).
2:for\(t=1,...,T\)do
3: (On the \(i^{th}\) worker)
4: Randomly sample \(_{t}^{(i)}\) and compute local gradient: \(g_{t}^{(i)}= f_{i}(x_{t};_{t}^{(i)})\)
5: Update the local \(m_{t}^{(i)}\): \(m_{t}^{(i)}= m_{t-1}^{(i)}+(1-)g_{t}^{(i)}\)
6: Update the local \(_{t}^{(i)}\): \(_{t}^{(i)}=_{t-1}^{(i)}+(1-)|g_{t}^{(i)}|\)
7: Update the local \(b_{t}^{(i)}\): if\(t>T_{0}\) {\(b_{t}^{(i)}=(b_{t}^{(i)},_{t}^{(i)})\)} else {\(b_{t}^{(i)}=_{t}^{(i)}\)} *
8: Quantize the local update: \(u_{t}^{(i)}=(^{(i)}}{b_{t}^{(i)}}+_{t-1}^{(i)})\)
9: Update the local error feedback \(e_{t}^{(i)}:e_{t}^{(i)}=e_{t-1}^{(i)}+^{(i)}}{b_{t}^{(i)}}-u_{t}^{(i)}\)
10: Send \(u_{t}^{(i)}\) to the server
11: (On server)
12: Average all received \(q_{t}\) and quantize it: \(_{t}=(_{i=1}^{n}u_{t}^{(i)}+_{t-1})\)
13: Update the global error feedback \(_{t}:_{t}=_{t-1}+_{i=1}^{n}u_{t}^{(i)}- _{t}\)
14: Send back \(_{t}\) to all workers
15: (On the \(i^{th}\) worker)
16: Update the local model parameter \(x_{t+1}\): \(x_{t+1}=x_{t}-_{t}_{t}\)
17:endfor ```

**Algorithm 1**Binder

The appealing characters of Binder are summarized in the following:

* Compared to SGD-type optimizers, Adam provides a fast convergence rate in practice by adaptively preconditioning the gradients with \(v_{t}\). **Binder inherits this feature to accelerate convergence speed.** On the other hand, unlike Adam, Binder employs the same exponential moving average factor \(\) for both \(m_{t}\) and \(b_{t}\). **This eliminates the need for bias correction and reduces the amount of tuning work required.**
* Existing quantization optimizers are built upon _vector-wise quantization_4

\[((g_{t})_{j})=\{\|g_{t}\|_{p} ((g_{t})_{j}),&p_{i}=)_{j}|}{\|g_{t}\|_{p}}-r\\ \|g_{t}\|_{p}((g_{t})_{j}),&1-p_{i} .\]

 where \(\|g_{t}\|_{p}=(_{j}|(g_{t})_{j}|^{p})^{}(p 1)\), \(0 r<s\) (\(r,l\)) and \()_{j}|}{\|g_{t}\|_{2}}[,]\). * E and estimate the sign of each element, which will renders the saved communication time cost somewhat meaningless. In contrast, Binder _element-wise_ quantizes the update, and the subtle design for \(m_{t}\) and \(b_{t}\) ensures the unquantized update is strictly bounded in the range \([-1,1]\), **allowing quantization is computed easily and lightly**. Further, unlike most quantization optimizers that only compress the gradients , **Binder performs the quantization for the entire adaptive update**, which further streamlines the optimization process.

**Remark.** We have noticed that the prior works 1-bit Adam () and its variants (, ) are also categorized as 1-bit adaptive optimizers. However, the design ethos of 1-bit Adam and Binder differ significantly. 1-bit Adam is still built on gradient quantization and and essentially functions as a preconditioned SGDM. 1-bit Adam runs full-precision Adam in the beginning (warm-up phase) and utilizes it as a fixed precondition for SGDM during the rest of training (compression phase). There are three aspects that influence 1-bit Adam to indeed accelerate communication. _First_, the warm-up stage constitutes approximately 15%-25% of the total steps, which to some extent discounts the high quantization ratio. _Second_, the vector-wise quantization employed by 1-bit Adam necessitates extra computations, including the calculation of vector norms and estimation of element signs, which are then transmitted as additional data. These factors diminish the time savings achieved through reduced communication bits. _Third_, the vector-wise quantization technique employed by 1-bit Adam is not compatible with the common-used distributed framework _DDP_ (system-level engineered distributed framework). In _DDP_, communication data is uniformly divided into buckets of equal size on the sender's side to enhance communication efficiency. Consequently, when vector-wise quantization is used, communication data from a single layer may be divided into different buckets, resulting in substantial errors during restoration on the receiver's end.

## 3 Theoretical Analysis

In this section, we present the theoretical convergence guarantee for Binder (Algorithm 1). We first introduce some necessary assumptions.

**Assumption 1.[Bounded infimum]** _For any \(x\) and a constant \(f^{*}\), we have the objective value \(f(x) f^{*}\)._

**Assumption 2.** [Lipschitz continuous gradient] _The gradient \( f()\) is \(L\)-Lipschitz continuous, i.e.,, \(\| f(x)- f(y)\| L\|x-y\|_{2}, x,y^{d}\)._

**Assumption 3.** [Unbias and independent noisy gradient] _The gradient with respect to the random samples on each worker and at a different time is independent identically distributed (i.i.d.), i.e., \([g_{t}^{(i)}]= f(x_{t}), t 1\), \(g_{t}^{(i)}\) is independent of \(g_{t}^{(j)}\) for \(i j\), and \(g_{t_{1}}^{(i)}\) is independent of \(g_{t_{2}}^{(j)}\) for \(t_{1} t_{2}\)._

**Assumption 4.** [Bounded gradient] _The noisy gradient and the full-set gradient are bounded i.e., \(\|g_{t}^{(i)}\| G,\| f_{t}(x)\| G, t 1\)._

Under the assumptions above, we then present the theoretical convergence for Binder in Algorithm 1.

**Theorem 1.** _For Binder in Algorithm 1, under Assumption 1-4, assuming \((b_{t}^{(i)})_{j}>0\), \( j[1,2,...,d]\)5, choosing \(_{t}=}\), \( t[1,2,...,T]\) and \(_{0}=_{1}\), and defining \(z_{1}=x_{1}+_{1}(_{1}-e_{1})\) where \(_{1}{=}_{i=1}^{n}^{(i)}}{b_{1}^{(i)}}-^{n}m_{i}^{(i)}}{_{i=1}^{n}b_{1}^{(i)}}\) and \(e_{1}{=}_{i=1}^{n}e_{1}^{(i)}+_{1}\), we then have the following_

\[[_{t=1}^{T}\| f(x_{t})\|^{2}]^{2} }{}+(1+ T)}{},\] (6)

_where_

\[C_{1} =cG([f(z_{1})-f^{*}]+dL}{16}+}{(1-)}+}{}+^{2}LG^{2 }d}{^{2}(1-)^{2}}),\] \[C_{2} =c^{3}G(+10+5)L^{2}d}{(1-)^{2}}+ (1+L)}{2^{2}}+2dL).\]The theoretical results suggested that 1-bit Binder essentially achieve the same convergence rate (\(O(})\)) as the uncompressed Adam.

## 4 Hierarchical-1-bit-All-Reduce

The data communication for Binder is one-bit, which cannot be directly aggregated using the efficient _All-Reduce_. Additionally, there is a significant imbalance between the intra-node and inter-node bandwidths. If we attempt to aggregate the data uniformly from both intra-nodes and inter-nodes, the communication process will be hindered by the inter-node data exchanges, resulting in slower overall communication.

In light of the problems above, we propose a hierarchical communication scheme called _Hierarchical-1-bit-All-Reduce_. This scheme efficiently aggregates our 1-bit data by leveraging the ultra-high intra-node bandwidth and reducing the inter-node communication overhead. Assuming we have \(n\) nodes, each containing \(m\) GPUs, and the overall volume for each GPU needs to be communicated is \(P\),as visually depicted in Figure 1, the steps of _Hierarchical-1-bit-All-Reduce_ are as follows: (\(i\)) Each GPU performs _Reduce-Scatter_ to locally aggregate and scatter the data within its node. The communication volume for each GPU in this step is \(\). (\(ii\))Each GPU then applies Binder to quantize the data, resulting in a reduced volume of \(\) on each GPU. (\(iii\)) The GPU proceeds with _1-bit-All-Reduce_ to inter-aggregate the data. This step consists of two sub-steps: 1) Each GPU performs _All-to-All_ to collect the corresponding data from GPUs in other nodes, with a communication volume of \(\). 2) Each GPU averages and re-quantizes the data, followed by _All-Gather_ operation to gather the data. The communication volume in this sub-step is also \(\). (iv)Finally, each GPU performs _All-Gather_ to intra-aggregate the data, with a communication volume of \(\).

Compared to the time cost of inter-node communication, the time cost of intra-node communication is relatively insignificant. Thus, when utilizing _Hierarchical-1-bit-All-Reduce_, the majority of the communication cost arises from the _1-bit All-Reduce_ step in Step (\(iii\)). The communication volume across nodes for all GPUs in this scheme is approximately \(\). In contrast, if we were to simply employ the original _All-Gather_ to aggregate data, the communication volume across nodes for all GPUs would be approximately \(n(n-1)P}{32}\). Consequently, _Hierarchical-1-bit-All-Reduce_ proves significantly more efficient than the original _All-Gather_.

Notably, the work  also introduces a 1-bit data communication scheme among nodes, but it can only guarantee the expected value of the gathered 1-bit data equals to the average of the original 1-bit data among nodes, thereby it will bring performance deterioration. In contrast, _All-to-All_ in _Hierarchical-1-bit-All-Reduce_ ensures the final data exactly equals to the average of the original data.

Figure 1: Paradigm of _Hierarchical-1-bit-All-Reduce_

## 5 Experiments

Recently, several works , have shown that when utilizing the system-level engineered distributed data-parallel framework _DDP_, the existing communication-compression optimizers (excluding 1-bit Adam) still perform slower than the uncompressed SGDM/Adam. Therefore, in our evaluation, we focus on assessing the performance of Birder, the uncompressed SGDM/Adam, and the closely related algorithm 1-bit Adam through distributed training experiments using the benchmark models ResNet-50 (CNN) and BERT-Base (Transformer). **More extensive experiments can be founded in Section B of the appendix.**

   &  &  &  \\  & &  &  &  &  \\  & & (samples/s) & & & (samples/s) & \\  SGDM &  & **3693 (1.00\(\))** & 76.19 & **5272 (1.00\(\))** & 75.05 \\
1-bit Adam & & 3243 (0.83\(\)) & 75.55 & 5229 (0.99\(\)) & 75.42 \\ Birder & & 3462 (0.94\(\)) & 75.98 & 5251 (0.99\(\)) & 75.45 \\  SGDM &  & 2959 (1.00\(\)) & 75.96 & 6189 (1.00\(\)) & 74.61 \\
1-bit Adam & & 4745 (1.60\(\)) & 75.33 & 8836 (1.42\(\)) & 75.05 \\ Birder & & **6015 (2.03\(\))** & 75.53 & **9633 (1.56\(\))** & 75.09 \\  SGDM &  & 4270 (1.00\(\)) & 75.47 & 9909 (1.00\(\)) & 74.54 \\
1-bit Adam & & 7268 (1.70\(\)) & 75.18 & 13827 (1.40\(\)) & 74.62 \\ Birder & & **9416 (2.21\(\))** & 75.27 & **15950 (1.61\(\))** & 74.82 \\  SGDM &  & 6189 (1.00\(\)) & 75.37 & 16640 (1.00\(\)) & 74.22 \\
1-bit Adam & & 5546 (0.89\(\)) & 75.54 & 16426 (0.99\(\)) & 74.14 \\ Birder & & **15253 (2.47\(\))** & 75.30 & **23727 (1.43\(\))** & 74.24 \\  

Table 1: System throughput and Test Accuracy of SGDM, 1-bit Adam and Birder for training ResNet-50 on ILSVRC2012 from scratch with \(8,16,32,64\) GPUs.

Figure 2: Epoch-wise and time-wise convergence speed for training ResNet-50 with \(32\) samples per GPU, ResNet-50 with \(128\) samples per GPU, and fine tuning BERT-Base with \(3\) samples per GPU with \(64\) GPUs.

### Experimental Settings

Our experiments were conducted on a testbed consisting of 1, 2, 4, 8 nodes interconnected via 10Gbps Ethernet. Each node was equipped with 8 Nvidia Tesla A100-80GB GPUs. The hardware and software configurations were identical across all instances, with Ubuntu 20.04.4 LTS serving as the operating system. PyTorch 1.11.0 was used as the primary framework, accompanied by CUDA-11.6, cuDNN-8.2, NCCL-2.10.3, and PyTorch 1.11.0 for other relevant libraries. Notably, to ensure compatibility with PyTorch's _DDP_, certain components of Binder and our hierarchical communication scheme were implemented within the customized communication hook of _DDP_.

**Training details.** For the experiments over ResNet-50, we evaluate the convergence and performance of SGDM, 1-bit Adam and Binder on ILSVRC2012. The batch size per GPU is set to \(32\) or \(128\) with the standard input resolution \(224 224\). When employing _SGDM (baseline)_, the learning rate starts at \(0.1\) with momentum of \(0.9\) and weight decay of \(0.0001\). When employing 1-bit Adam and Binder, the learning rate starts at \(0.001\) with weight decay of \(0.0001\), and \([_{1},_{2}]\) for 1-bit Adam is set to \([0.9,0.999]\) and \(\) for Binder is set to \(0.95\). Then, the learning rate is divided by \(10\) after \(30\), \(60\) and \(90\) epochs, and training is finally terminated after \(100\) epochs. Specifically, the first 15 epochs are used as the warmup stage for 1-bit Adam. For the experiments over BERT-Base, we access the convergence and performance of BertAdam (baseline), 1-bit Adam and Binder for SQuAD 1.1 fine-tuning task using a pre-trained BERT-Base model checkpoint from HuggingFace 6. The batch size per GPU is set to \(3\). We perform fine-tuning for 2 epochs. The learning rate linearly increases to \(1 10^{-4}\) steps in the early \(500\) steps and then linearly decreases to \(0\) in the rest iteration. Specifically, the first \(0.2\) steps are used as the warmup stage for 1-bit Adam. \([_{1},_{2}]\) for BertAdam, and 1-bit Adam is set to \([0.9,0.999]\) and \(\) for Binder is set to \(0.9\).

### Experimental Results

Figure 2 shows the convergence behaviors of epoch-wise and time-wise training for SGDM / BertAdam (baseline), 1-bit Adam, and Binder using ResNet-50 and BERT-Base models running on 64 GPUs. The experimental results clearly demonstrate that Binder achieves a similar epoch-wise convergence rate compared to the baseline. However, the actual training speed of Binder surpasses both the baseline and 1-bit Adam by a significant margin.

Figure 3 illustrates the system throughput of different optimizers when running ResNet-50 and BERT-Base on 8 GPUs to 64 GPUs (1 node to 8 nodes). When training on 8 GPUs (1 node), where computation takes precedence over communication, the throughput of Binder is slightly lower than that of SGDM and BertAdam. However, as the number of GPUs increases, Binder consistently outperforms its counterparts, and this superiority becomes increasingly evident with more GPUs. Additionally, the system throughput for SGDM, BertAdam, and 1-bit Adam occasionally decreases as the number of GPUs increases, whereas the throughput of Binder steadily grows. This observation indicates that Binder offers better scalability efficiency.

  Optimizer & \#GPUs & Throughput & F1-Score (\%) & Exact Match (\%) \\  & & (samples/s) & & \\  BertAdam &  & **413 (1.00\(\))** & 88.13 & 80.59 \\
1-bit Adam & & 358 (0.87\(\)) & 88.05 & 80.06 \\ Binder & & 412 (1.00\(\)) & 88.71 & 81.18 \\  BertAdam &  & 84 (1.00\(\)) & 88.47 & 81.07 \\
1-bit Adam & & 213 (2.54\(\)) & 87.87 & 80.31 \\ Binder & & **431 (5.13\(\))** & 88.31 & 80.80 \\  BertAdam &  & 119 (1.00\(\)) & 88.38 & 80.94 \\
1-bit Adam & & 274 (2.30\(\)) & 87.78 & 80.08 \\ Binder & & **730 (6.13\(\))** & 88.08 & 80.50 \\  BertAdam &  & 158 (1.00\(\)) & 88.13 & 80.94 \\
1-bit Adam & & 252 (1.59\(\)) & 87.33 & 79.67 \\ Binder & & **990 (6.26\(\))** & 88.28 & 80.75 \\  

Table 2: System throughput and F1-Score / Exact-Match of BertAdam, 1-bit Adam and Binder for fine tuning BERT-base on SQuAD 1.1 with \(8,16,32,64\) GPUs.

In terms of inference performance for ResNet-50, we evaluate the Top-1 accuracy after training on ILSVRC2012 from scratch. For BERT-Base, we measure the F1-score and exact-match score after fine-tuning on SQuAD 1.1. Table 1 shows that when the batch size is set to 32 samples per GPU, the accuracy of Brider is slightly lower than that of SGDM. It has been suggested in some works (, ) that adaptive optimizers generally yield worse generalization compared to SGDM for CNN architectures. However, as the batch size increases (Table 2), both 1-bit Adam and Brider achieve better accuracy. This can be attributed to the beneficial effect of introducing a certain level of noise for generalization (), which biases the optimizer towards wider valleys. Table 2 demonstrates that Brider achieves similar or higher F1-score and exact-match score compared to BertAdam and 1-bit Adam, validating the effectiveness of Brider for inference tasks.

### Communication Efficiency Analysis

As shown in Figure 4, training on a single node demonstrates that the baseline SGDM and BertAdam algorithms are slightly faster compared to Brider and 1-bit Adam. In this scenario, the inter-GPU bandwidth within a node is extremely high, rendering communication time negligible. However, the newly introduced compression/decompression process by Brider and 1-bit Adam adds extra time due to its implementation. Thanks to light-computation quantization, the compression/decompression time for Brider with ResNet-50 and BERT-Base is significantly reduced to approximately \(15ms\) and \(8ms\) respectively.When conducting distributed training across two nodes, the bandwidth between them is relatively limited (10Gbps in our experiment), making communication time a critical factor. In the case of uncompressed SGDM and BertAdam, the communication time substantially exceeds the computation time for ResNet with 32 samples per GPU and BERT-Base. Consequently, the system throughput is lower compared to a single node (as depicted in Figure 2). However, the extreme 1-bit quantization implemented in Brider effectively reduces communication overhead, resulting in only a marginal increase in the total time required for Brider. As the number of nodes continues to increase, the importance of an efficient communication scheme becomes paramount. By leveraging

Figure 4: Computation time, communication time and compression/decompression time per iteration of optimizers for training (a) ResNet-50 with \(32\) samples per GPU, (b)ResNet-50 with \(128\) samples per GPU, and (c) fine tuning BERT-Base with \(3\) samples per GPU with \(8\), \(16\), \(32\), \(64\) GPUs.

Figure 3: System throughput of optimizers for training (a) ResNet-50 with \(32\) samples per GPU, (b)ResNet-50 with 128 samples per GPU, and (c) fine tuning BERT-Base with \(3\) samples per GPU with \(8\), \(16\), \(32\), \(64\) GPUs.

our proposed _Hierarchical-1-bit-All-Reduce_, the overall inter-node communication volume exchanged scales proportionally with the number of nodes. In contrast, the _Compressed-All-Reduce_ method employed by 1-bit Adam results in the overall communication volume exchanged among nodes being proportional to the number of GPUs (eight times larger than the number of nodes in our experiments). Consequently, as the number of nodes increases, the communication time for Binder exhibits a gradual rise, while the communication time for 1-bit Adam experiences a sudden surge.

## 6 Conclusion

In this study, we introduce a novel 1-bit adaptive optimizer for distributed training. Our optimizer offers the advantages of being lightweight in terms of computation while employing extreme 1-bit quantization for the communication data. Furthermore, we provide theoretical evidence demonstrating that Binder can achieve convergence rates comparable to the uncompressed Adam. To enhance communication speed, we propose a novel communication scheme tailored specifically for Binder, replacing the inefficient naive _All-Gather_ approach. Through extensive experiments on benchmark models such as ResNet-50 and BERT-Base, we validate the effectiveness and efficiency of Binder in comparison to uncompressed methods like SGDM/Adam as well as the relevant 1-bit Adam.