# Beyond Aesthetics: Cultural Competence in Text-to-Image Models

Nithish Kannen\({}^{}\), Arif Ahmad\({}^{}\), Marco Andreetto\({}^{}\), Vinodkumar Prabhakaran\({}^{}\),

Utsav Prabhu\({}^{}\), Adji Bousso Dieng\({}^{}\), Pushpak Bhattacharyya\({}^{}\), Shachi Dave\({}^{}\)

\({}^{}\)Google Research, \({}^{@sectionsign}\)Google DeepMind, \({}^{}\)IIT Bombay, \({}^{}\)Princeton

**Correspondence:** (nitkan, shachi)@google.com

Work done while Arif Ahmad was a student researcher at Google Research.

###### Abstract

Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: _cultural awareness_ and _cultural diversity_, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltrural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models.2 CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal significant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for under-specified prompts. Our methodology is extendable to other cultural regions and concepts, and can facilitate the development of T2I models that better cater to the global population.

Figure 1: Images from a SOTA T2I model demonstrating its lack of cultural diversity: (a) and (b) and cultural awareness: (c) and (d). (a) Images for "_High definition photo of a monument_" lack architectural and global diversity. (b) Images for "_Image of Nigerian dish_" lack the rich diversity in Nigerian cuisine. (c) "_Image of Jagamath Temple from India_" produces an incorrect depiction of the temple. (d) "_Image of Japanese dish Kabayaki_" produces an incorrect and cartoonized photo.

Introduction

Text-to-image (T2I) generative capabilities have advanced rapidly in recent years, exemplified by models such as Imagen 2 (Saharia et al., 2022), and DALLE-3 (Betker et al., 2023). As powerful tools for creative expression and communication, they have the potential to revolutionize numerous industries such as digital arts, advertising, and education. However, their widespread adoption across the globe raises important ethical and social considerations (Bird et al., 2023; Weidinger et al., 2023), in particular, in ensuring that these models work well for all people across the world (Qadri et al., 2023; Mim et al., 2024). While early T2I model evaluations focused on photo-realism (Saharia et al., 2022) and faithfulness(Hu et al., 2023; Cho et al., 2024; Huang et al., 2023), recent work has demonstrated various societal biases that they reflect (Cho et al., 2023; Bianchi et al., 2023; Luccioni et al., 2024). However, the predominantly mono-cultural development ecosystems of these models risks unequal representation of cultural awareness in them, potentially exacerbating existing technological inequalities (Prabhakaran et al., 2022). While the term "culture" has a myriad definitions across disciplines (Rapport & Overing, 2002), in this paper we focus on cultures formed within societies demarcated geographically through national boundaries (similar to Li et al. (2024c)), rather than cultures defined through organizational or other socio-demographic categories. This focus stems from our aim to assess global disparities in the capabilities of T2I models. Such disparities are shown to perpetuate harmful stereotypes about cultures (Jha et al., 2024; Basu et al., 2023), as well as cause the erasure and suppression of sub- and co-cultures (Qadri et al., 2023), and limit their utility across geo-cultural contexts (Mim et al., 2024). While recent work has focussed on biases and stereotypes these models propagate (Jha et al., 2024; Basu et al., 2023), not much work has looked into how competent these models are in capturing the richness and diversity of various cultures.

Gaps in cultural competence may manifest primarily along two aspects of model generations: (i) _cultural awareness_: failure to recognize or generate the breadth of concepts/artifacts associated with a culture (Figure 1(c) and 1(d)), and (ii) _cultural diversity_: the tendency to adopt an oversimplified and homogenized view of a culture that associates (and generates) a narrow set of concepts/artifacts within that culture (Figure 1(b)) or across global cultures (Figure 1(a)). While the lack of cultural awareness in text to image models has been documented before (Hutchinson et al., 2022; Ventura et al., 2023), a major challenge in effectively assessing it at scale is the lack of resources that have a broad representation of cultural artifacts. Similarly, while dataset diversity has also been identified as an important part of the data-centric AI agenda (Oala et al., 2023) and has been investigated for text (Chung et al., 2023) and image modalities (Srinivasan et al., 2024; Dunlap et al., 2023), there has been limited focus on diversity of model generations (Lahoti et al., 2023), especially for T2I models. While works studying diversity of image generations focus on visual similarity (Hall et al., 2024; Zameshina et al., 2023), we study the diversity of generated cultural artifacts (aka _cultural diversity_).

In this paper, we present **CUBE:****C**Ultrat **BE**nchmark, a first-of-its-kind benchmark designed to facilitate the evaluation of cultural competence of T2I models along two axes: cultural awareness and cultural diversity. We build this benchmark at the country level (in line with recent works (Jha et al., 2024; Li et al., 2024c)), encompassing eight countries and representing three different concepts of cultural artifacts chosen as concepts of clear visual elements, and hence of importance to T2I models. We employed a large-scale extraction strategy that leverages a Knowledge Graph (KG) augmented with a Large Language Model (LLM) to build a broad-coverage compilation of country-specific artifacts to ground our evaluation. CUBE consists of a) **CUBE-1K** - a carefully curated subset of 1000 artifacts, made into prompts that enable evaluation of cultural awareness (Nguyen et al., 2023), and b) **CUBE-C**SP**ace - a collection of \(\)300K cultural artifacts spanning the 8 countries and 3 concepts we consider, with a potential to be scaled to other concepts and countries. Furthermore, we introduce cultural diversity (CD) as a new evaluation component for T2I models, adapting the quality-weighted Vendi score (Nguyen & Dieng, 2024).We detail the CUBE curation process in Section 3, cultural awareness evaluation in Section 4 and cultural diversity evaluation in Section 5. To summarize, our main contributions are:

* A new T2I CUltural BEnchmark (**CUBE**), that assesses the cultural competence of T2I models along two key dimensions: (1) Cultural Awareness and (2) Cultural Diversity. We curate a dataset of 300K cultural artifacts spanning three concepts with a potential to be scaled to other concepts.
* An extensive human evaluation measuring the faithfulness and realism of T2I-generated cultural artifacts across eight countries and three concepts, revealing substantial gaps in cultural awareness.
* A novel T2I evaluation component leveraging the quality-weighted Vendi score that satisfies the desirable properties to assess cultural diversity in T2I models.

## 2 Related Work

In our discussion of related work, we focus on T2I evaluation and culture in large models.

T2I Evaluation.Inception Score (Salimans et al., 2016) and Frechet Inception Distance (Heusel et al., 2018) focus on similarity of generated images to real ones, also called realism. Metrics like DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure the prompt-image alignment, also called faithfulness. Other metrics such as ImageReward (Xu et al., 2023), PickScore (Kirstain et al., 2023), and HPSv2 (Wu et al., 2023) fine-tune vision-language models on human ratings to better align with human preferences. There have been recent works on bias and fairness evaluation (Feng et al., 2022; Naik & Nushi, 2023; Zhang et al., 2023; Jha et al., 2024) of T2I models. There have also been efforts to build comprehensive evaluation benchmarks aimed at tracking the progress of model capabilities over time, focusing on tasks such as realism, text faithfulness, and compositional abilities. These benchmarks, such as DrawBench (Saharia et al., 2022), CC500 (Feng et al., 2023), T2I-CompBench (Huang et al., 2023), TIFA v1.0 (Hu et al., 2023), DSG-1k (Cho et al., 2024), GenEval (Ghosh et al., 2023), and GenAIBench (Lin et al., 2024) employ diverse prompts and metrics to assess factors such as image-text coherence, perceptual quality, attribute binding, faithfulness, semantic competence, and compositionality, to list a few.

Culture in Language Technologies.NLP researchers have long argued for the need for cross-cultural awareness in language technologies (Hovy & Spruit, 2016; Hershcovich et al., 2022), and built datasets to assess cultural biases in language technologies (Jha et al., 2023; Naous et al., 2023; Seth et al., 2024). There have also been efforts to identify cultural keywords across languages (Lim et al., 2024), extract cultural commonsense knowledge (Nguyen et al., 2023), as well as to generate culture-conditioned content (Li et al., 2024). Along those lines, CultureLLM (Li et al., 2024) proposes generating training data using the World Value Survey for semantic data augmentation to integrate cultural differences into large language models.

Culture in Vision.Efforts to understand cultural competence in computer vision technologies are relatively more recent and limited. Basu et al. (2023) explored the geographical representation of under-specified prompts and found that most of them default to United States or India. Dig In (Hall et al., 2024) evaluates disparity in geographical diversities of household objects. ScoFT (Liu et al., 2024) enhances cultural fairness using the cross-cultural awareness Benchmark (CCUB). Recent work also shows that cultural and linguistic diversity in datasets enables semantic understanding and helps address cultural dimensions in text-to-image models (Ye et al., 2023; Ventura et al., 2023). Proposals for more inclusive model design and dataset development have been made to address cultural stereotypes and Western-centric biases, to better represent global cultural diversity (Bianchi et al., 2023; Liu et al., 2021). Our work contributes to this line of work, where we introduce a large benchmark dataset and associated metrics to assess cultural competence along cultural awareness and cultural diversity in T2I models.

  
**Benchmark** & **Skill** &  \\   & & **Faithfulness** & **Realism** & **Diversity** \\  DrawBench & Spatial \& Object & ✓ & ✓ & ✗ \\ CC500 & Composition (color) & ✓ & ✓ & ✗ \\ T2I-CompBench & Composition & ✓ & ✗ & ✗ \\ Tifa160 & Spatial & ✓ & ✗ & ✗ \\ DSG1k & Spatial & ✓ & ✗ & ✗ \\ GenEval & Object & ✓ & ✗ & ✗ \\ GenAIBench & Spatial & ✓ & ✗ & ✗ \\  CUBE & Cultural & ✓ & ✓ & ✓ \\   

Table 1: **Overview of text-to-image benchmarks. Existing benchmarks focus only on faithfulness and realism as evaluation aspects and overlook the cultural skill. CUBE is the first T2I benchmark that evaluates cultural competence while introducing diversity as an evaluation aspect.**Construction of CUBE

Our benchmark aspires to enable reliable, trustworthy, and tangible measurement of text-to-image generative models for two distinct yet complementary behaviors: _cultural awareness_ (i.e., the model's ability to reliably and accurately portray objects associated with a particular culture), and _cultural diversity_ (i.e., the model's ability to suppress oversimplified stereotypical depiction for an underspecified input that references a specific culture). One of the core prerequisites to meaningfully evaluate these aspects of cultural competence is a broad-coverage repository of cultural artifacts to ground such an evaluation. Inspired by previous work , we focus on _geo-cultures_ (realized through the lens of national identity) to build such a repository, potentially extendable to other ways of categorizing culture, such as regions, religions, races, etc. We select eight countries from different geo-cultural regions across continents and the Global South-North divide: Brazil, France, India, Italy, Japan, Nigeria, Turkey, and USA. While we acknowledge that this list of countries is necessarily incomplete, and may result in a biased global sampling, future iterations of this work could include a wider range of countries for a more comprehensive evaluation.

Additionally, we focus on distinctive artifacts, i.e., cultural aspects that reference singular real objects with clear visual elements which are commonly held as belonging to a specific country - as opposed to cultural manifestations that are not visualizable (e.g. speech accents) or multifarious (e.g. complex scenes, or unique inter-object relationships). The three artifact categories ("concepts") included here are _landmarks_ (prominent and recognizable structures such as monuments and buildings, located in specific countries), _art_ (clothing and regional garments or traditional regalia, performance arts, and style of painting, associated at possibly a specific time in history), and _cuisine_ (specific dishes and culinary ingredients that are commonly associated with certain countries). In practice, for the art and cuisine categories, we additionally consider "country of origin" as a strong indicator of national association, acknowledging that there may be other factors.

Finally, for each country-concept combination, we aim to construct grounding "concept spaces", which leads to a collection of \(\)300K cultural artifacts, which we call **CUBE-CSpace**. This is an extensive compilation of concept space instances, also intended to be used as grounding for diversity evaluation. From this, we create **CUBE-1K**: a much smaller, curated set of the 1000 artifacts across the 8 countries and 3 concepts - selected for relevance and popularity, intended to be used for testing cultural awareness. The country and concept wise split of CUBE-1K is presented in Table 7. In order to build CUBE, we adopt a Knowledge-Base (KB)-augmented LLM approach wherein we use graph-traversal on a pre-existing KB to extract a broad-coverage set of candidate cultural artifacts, followed by a self-critiquing LLM step to iteratively refine the repository.

### CUBE-CSpace

We use WikiData  as the KB to extract cultural artifacts, as it is the world's largest publicly available knowledge base, with each entry intended to be supported by authoritative sources of information. We use the SLING framework3 to traverse the WikiData dump of April 2024, by first manually identifying _root nodes_ (see Table 8), a small seed set of manually selected nodes that represent the concept in question. For example, the node 'dish' (WikiID: Q746549) is identified as a root node for the concept 'cuisine'. We then look for child nodes that lie along the 'instance of' (P31) and'subclass of' (P279) edges; e.g. 'Biriyani',(Q271555), a popular dish from India, is a child node of 'dish'along the 'instance of' edge. The child nodes that have the 'country-of-origin' (P495) or the 'country' (P17) are extracted at the iteration. We recursively traverse the remaining nodes along the same edge classes in search of child nodes that satisfy these properties. For example 'bread' (Q7802) is a child of 'dish'; since it is a generic food item, it doesn't have the 'country-of-origin' (P495) property. However, 'Filone' (Q5449200) is a child of 'bread' and has 'country-of-origin' (P495) as Italy, which would be extracted at the step. We outline the extraction process in Algorithm 1. In practise, we iterated for H=4 hops and have detailed considerations in Appendix D.4.

Refinement.The above KB extraction process results in \(\)500K collection of WikiData nodes, which is expected to have missing and inconsistent entries, owing to the noisy nature of WikiData (Kannen et al., 2023). We use GPT4-Turbo to filter out cultural artifacts that may not necessarily belong to a concept space, taking inspiration from existing self refinement (Madaan et al., 2023) and self critiquing (Lahoti et al., 2023) techniques. Once we filter out the erroneous artifacts, we prompt GPT-4 to fill out popular missing artifacts from the cultural concept, similar to the diversity expansion application in (Lahoti et al., 2023). This filtering and completion process brings down the count to \(\)300K entries, which forms the **CUBE-CSpace**. Table 2 presents some examples cultural artifacts extracted by this process.

### Cube-1k

As T2I models are primarily trained on English image-text pairs (Pouget et al., 2024), we expect them to struggle with visualizing artifacts from non-English-speaking cultures. To this end, CUBE-1K consists of prompts focusing on widely recognized artifacts, reflecting a model's ability to capture mainstream cultural elements. The artifacts in CUBE-1K are a carefully curated subset of CUBE-CSpace. To ensure the inclusion of popular artifacts relevant to each country, we leverage the number of Google search results as a proxy for popularity. Specifically, we employ the Google Search API, utilizing the geolocation feature ('gl' property) to tailor search results to a user located within the target country, thus capturing local popularity. We use this popularity estimate to sample artifacts for CUBE-1K. While search results serve as a useful proxy, we acknowledge they can be noisy, potentially inflated by the presence of popular keywords. Therefore, the final collection undergoes a manual verification process (detailed in Appendix D.4) to ensure relevance of selected artifacts. CUBE-1K consists of 1000 prompts, spanning 8 countries and 3 concepts described above. Table 7 presents the distribution of artifacts across different countries in CUBE-1K. We use prompt templates designed to probe the models for cultural awareness, along with a negative prompt (Hao et al., 2023) to obtain images with desired qualities. Each prompt tests the model's ability to visualize a single artifact. The prompt templates and negative prompt are provided in Table 13.

## 4 Evaluating Cultural Awareness

To assess the cultural awareness of text-to-image (T2I) models, we leverage prompts from the CUBE-1K dataset. We use traditional T2I evaluation aspects like _faithfulness_ (adherence of the generated image to the input prompt) and _realism_ (similarity of the generated image to a real photograph) to measure cultural awareness. Conventionally, these are measured using automated metrics like DSG (Cho et al., 2024) and FID (Heusel et al., 2018). However, these prove insufficient for capturing the complexities of _cultural_ representation. Existing automated metrics are primarily trained on datasets lacking diverse cultural content and struggle to adequately assess the nuances of cultural elements. Therefore, we introduce a human annotation scheme specifically tailored to measure a model's cultural awareness along the two key dimensions: a) faithfulness and b) realism.

  
**Geo-culture** & **Concept** & **Cultural Artifacts** \\  Japan & Cuisine & Ramen, Soba, Sushi, Katsu sandwich \\ France & Landmarks & Eiffel Tower, Mont Saint-Michel, Palace of Versailles \\ India & Art/Clothing & Kurta, Lehanga Choli, Dhoti, Patola Saree \\   

Table 2: Examples of cultural artifacts collected in CUBE-CSpace

### Human Annotation

In order to evaluate cultural awareness of the T2I models, we asked human annotators questions that are analogous to standard metrics used in T2I evaluation: a) _faithfulness_ and b) _realism_ also called _fidelity_. Each annotator was presented with the AI-generated image for an artifact and the corresponding description along with the country association, and was asked the following questions:

1. **Cultural Relevance:** Based solely on the image, does the item depicted belong to the annotator's country? (Yes/No/Maybe)
2. **Faithfulness:** If the image is from the annotator's country, how well does it match the item in the text description? (1-5 Likert scale)
3. **Realism:** How realistic does the image look, regardless of faithfulness? (1-5 Likert scale, with optional comment for scores \(\) 3)

We recruited diverse groups of raters from each of the countries we consider. Each rater pool underwent comprehensive training and was also given a "golden set" of examples, as reference. Once training was complete, the raters proceeded to annotate the 1K prompts spanning the three concepts and the eight countries outlined in Table 3. Raters were instructed to focus on both the image and text when evaluating cultural relevance, and solely the image for realism. Detailed guidelines for each criterion (Appendix E), the inter-annotator agreement (Appendix E.1), and the interface used for human annotation (Figure 4) can be found in Appendix.

### Results

Figure 2 presents examples of faithfulness and realism scores for images that were deemed culturally relevant. In 2(a), the model was prompted to generate _Pastel de angu_ from Brazilian cuisine and raters gave perfect score for both faithfulness and realism. In contrast, raters gave the lowest score of 1 for both aspects in 2(d), clearly identifying that it is neither faithful nor realistic. Similarly, the image of _Sushi_ (2(b)) from Japanese cuisine got an faithfulness score of 5, but realism score of 1 with an observation: _"The fish looks hard and made of glossy plastic."_. Whereas, the image of _lavalliere_ from France is realistic but not faithful, according to the raters.

Table 3 presents the average consensus scores (and standard deviations) for both faithfulness and realism, as rated for each model across regions and concepts. Both Imagen 2 and SDXL exhibit substantial room for improvement in both faithfulness and realism. Both models achieve relatively lower scores for countries regarded as the Global South (such as Brazil, Turkey, and Nigeria), with this disparity particularly pronounced for faithfulness. On average, in comparison to faithfulness, realism scores are lower across geo-cultures. While Imagen generally outperforms SDXL, exceptions exist, such as art faithfulness in the USA where SDXL scores higher. Table 12 shows the percentage of times our raters from each region deemed the images generated by each model to be culturally relevant (i.e., a yes answer to the first question in Annotation Guidelines in E) showing non-uniform disparities across models and cultures. This suggests that the cultures marginalized by any particular model may depend on factors such as training data, reiterating the need for such cross-cultural benchmarks.

Figure 2: Examples of human evaluation results on cultural awareness for T2I models with high and low scores on faithfulness and realism. More qualitative examples are in Figures 9 and 10.

## 5 Evaluating Cultural Diversity

We seek to assess the cultural diversity of T2I outputs across different seeds as a way to measure the model's intrinsic latent space cultural diversity (Xu et al., 2024). For instance, a model capable of generating a diverse array of cultural artifacts across a range of seeds demonstrates the cultural richness of its learned representations. A more detailed note on our motivation for seed variation is outlined in Appendix H. For this, we focus on _under-specified_ prompts (Hutchinson et al., 2022) - prompts that elicit the generation of diverse cultural artifacts (e.g. "Image of tourist landmarks") rather than specific objects (e.g. "Image of Eiffel Tower"). We then seek to answer: _What is the geo-cultural diversity of the generated cultural artifacts for prompts that mention just a concept?_. We further study the within-culture diversity in Appendix K and perform a correlation analysis of cultural diversity with existing metrics in Appendix C.

### Cultural Diversity (CD)

Existing works that focus on visual diversity use LPIPS (Zameshina et al., 2023) and Coverage (Hall et al., 2024), based on image embeddings. However, these metrics are not directly applicable in our case, as the similarity here may be attributed to color, texture, spatial orientation, and other visual aspects of the images. Measuring the cultural diversity of text-to-image (T2I) models requires a approach that accounts for both the variety of generated cultural artifacts and the quality of images generated from text prompts. To address this, we introduce **Cultural Diversity (CD)**, a new T2I evaluation component leveraging the quality-weighted Vendi Score (Nguyen and Dieng, 2024).

#### 5.1.1 Foundation: Vendi Scores

Vendi scores are a family of interpretable diversity metrics that satisfy the axioms of ecological diversity (Dan Friedman and Dieng, 2023; Pasarkar and Dieng, 2023). Vendi score captures the "effective number" of distinct items within a collection, considering both richness (number of unique elements) and evenness (distribution of those elements), and is defined as follows

**Definition 5.1** (Vendi Scores): _Let \(X=(x_{1},,x_{N})\) be a collection of \(N\) items. Let \(k:\) be a positive semi-definite similarity function, such that \(k(x,x)=1\) for all \(x\). Denote by \(K^{N N}\) the kernel matrix whose \(i,j\) entry \(K_{i,j}=k(x_{i},x_{j})\). Further denote by \(_{1},_{2},,_{N}\) the eigenvalues of \(K\) and their normalized counterparts by \(_{1},,_{N}\) where \(_{i}=_{i}/_{i=1}^{N}_{i}\). The Vendi score of order \(q 0\) is defined as the exponential of the Renyi entropy of the normalized eigenvalues of \(K\),_

\[_{q}(X;k)=(\,_{i=1}^{N}( _{i})^{q}), \]

  
**Concept** & **Model** & **India** & **Japan** & **Italy** & **USA** & **Brazil** & **France** & **Turkey** & **Nigeria** \\   \\   & Imagen & \(2.8 1.9\) & \(2.4 1.3\) & \(2.6 1.5\) & \(3.4 1.4\) & \(1.99 1.5\) & \(3.1 1.5\) & \(2.2 1.4\) & \(2.7 1.5\) \\  & SDXL & \(2.1 1.7\) & \(1.8 0.6\) & \(2.2 1.1\) & \(3.7 1.3\) & \(1.5 1.0\) & \(2.8 1.4\) & \(1.8 1.1\) & \(2.1 1.3\) \\   & Imagen & \(3.6 1.8\) & \(2.2 0.9\) & \(2.6 1.2\) & \(3.8 0.6\) & \(2.5 1.7\) & \(4.0 0.9\) & \(3.6 0.9\) & \(2.4 0.8\) \\  & SDXL & \(2.7 1.7\) & \(2.0 0.7\) & \(2.2 0.9\) & \(3.3 1.3\) & \(2.5 1.6\) & \(4.0 0.6\) & \(3.0 0.8\) & \(1.9 0.7\) \\   & Imagen & \(3.5 1.8\) & \(2.8 0.9\) & \(4.2 1.2\) & \(3.3 1.2\) & \(2.9 1.8\) & \(3.7 1.0\) & \(2.5 1.3\) & \(2.1 1.4\) \\  & SDXL & \(3.2 1.8\) & \(2.0 0.8\) & \(3.0 1.2\) & \(3.9 1.7\) & \(2.2 1.6\) & \(3.2 1.0\) & \(2.1 1.4\) & \(2.0 1.2\) \\   \\   & Imagen & \(4.2 0.5\) & \(2.0 0.8\) & \(3.2 0.9\) & \(2.2 0.9\) & \(4.4 0.7\) & \(3.4 0.9\) & \(2.4 0.8\) & \(3.3 0.9\) \\  & SDXL & \(3.6 1.1\) & \(1.4 0.6\) & \(2.2 1.3\) & \(1.9 0.9\) & \(2.1 1.4\) & \(2.8 1.4\) & \(2.2 0.8\) & \(3.3 0.9\) \\   & Imagen & \(3.8 1.0\) & \(1.7 0.6\) & \(2.1 1.4\) & \(2.4 1.2\) & \(2.5 1.6\) & \(3.2 1.2\) & \(2.7 1.0\) & \(2.9 0.9\) \\  & SDXL & \(3.7 1.0\) & \(1.5 0.6\) & \(2.7 1.3\) & \(2.1 0.8\) & \(3.5 0.9\) & \(3.9 0.6\) & \(2.5 0.8\) & \(3.6 0.7\) \\   & Imagen & \(3.4 1.4\) & \(2.3 0.8\) & \(2.6 1.4\) & \(1.3 0.6\) & \(2.4 1.5\) & \(1.9 1.3\) & \(1.6 0.7\) & \(2.2 1.2\) \\   & SDXL & \(2.8 1.4\) & \(1.4 0.9\) & \(1.6 1.1\) & \(1.4 0.7\) & \(1.3 0.6\) & \(2.1 1.4\) & \(1.6 0.8\) & \(3.0 1.0\) \\   

Table 3: Comparison between Imagen 2 and Stable Diffusion XL (SDXL) for Faithfulness and Realism. Reported score is the average consensus (1 to 5) and the standard deviation among 3 raters for each country. Highlighted cells indicate scores below 3 (light gray) and below 2 (dark gray).

_where we use the convention \(0* 0=0\)._

The order \(q\) determines the sensitivity allocated to feature prevalence, with values of \(q<1\) being more sensitive to rarer features and \(q>1\) putting more emphasis on more common features. When \(q=1\), we recover the original Vendi score (Dan Friedman Dieng, 2023), the exponential of the Shannon entropy of the normalized eigenvalues of \(K\).

#### 5.1.2 Incorporating Quality: Quality-Weighted Vendi Scores

While Vendi scores measure diversity, they treat all items equally without considering individual quality. In the context of T2I, however, it is crucial to account for the quality of the generated images conditional on text prompts. We therefore rely on _quality-weighted Vendi scores_ (qVS) (Nguyen Dieng, 2024) that extends VS to account for the quality of items in a given collection. qVS is defined as the product of the average quality of the items in the collection and their diversity,

\[_{q}(X;k,s)=(_{i=1}^{N}s(x_{i}))\, _{q}(X;k), \]

where \(s()\) is a function that scores the quality of the items.

In order to be able to compare different collections of images with different sizes, we normalize qVS by the size of the collection to measure cultural diversity:

\[q}_{q}(X;k,s)=(_{i=1}^{N}s(x_{i}) )\,(_{q}(X;k)}{N}). \]

We employ the HPS-v2 metric (Wu et al., 2023) as the \(s()\) function to score the quality of T2I outputs. HPS-v2, trained on 790k human preferences, provides a quality score \(s\) for an image conditioned on text prompt, making it suitable proxy to measure image quality in our case. We leave exploration of other quality measures of salience of generated artifacts, for future work.

\(q}\) is minimized to 0 when every element has a quality score of 0, and is maximized to 1, when all elements have a perfect quality (s = 1) and are all distinct from each other (\(}\) = 1)

#### 5.1.3 Desirable Properties of \(q}\)

\(q}\) has many desiderata in the context of T2I models: it accounts for similarity and inherits several desirable features of the Vendi scores such as sensitivity to richness and evenness. It also exhibits quality-awareness, duplication scaling and offers flexibility to define kernels that capture different facets of geo-cultural diversity.

Properties.Consider the same setup as in Definition 5.1.

* **Quality-awareness.** Denote by \(_{1}=(x_{1},,x_{M})\) and \(_{2}=(y_{1},,y_{L})\) two collections such that \(_{q}(_{1};k)=_{q}(_{2};k)\). Denote by \(s()\) a function that scores the quality of an item such that \(_{i=1}^{M}s(x_{i})_{j=1}^{L}s(y_{j})\). Then \[q}_{q}(_{1};k,s) q}_ {q}(_{2};k,s)\;.\]
* **Duplication scaling.** Denote by \(=(x_{1},,x_{N})\) a collection of \(N\) items. Define \(^{}\) as the collection containing all elements of \(\) each duplicated M times. Then \[q}_{q}(;k,s)=M q}_{ q}(^{};k,s)\;.\]
* **Kernel generalizability.** Let \(k_{1}(,)\) and \(k_{2}(,)\) represent two different positive semi-definite similarity functions. Then, given a collection \(=(x_{1},,x_{N})\), the quantities \(q}_{q}(;k_{1},s)\) and \(q}_{q}(;k_{2},s)\) may capture different aspects of diversity based on the properties of \(k_{1}\) and \(k_{2}\).

We state above 3 core properties of \(q}\) that makes it suitable for measuring cultural diversity in T2I models: 1) prioritizes collections with higher-quality items when other factors are equal, 2) penalizes the duplication of elements, and 3) exhibits flexibility in capturing various aspects of diversity through the selection of an appropriate similarity kernel. The proof of the quality-awareness property is immediate following the definition of \(q}\). See Appendix I for a proof of duplication scaling.

### Experimental Setup

We discuss the experimental pipeline: 1) **Prompting and Seeding:** We calculate \(q}\) for 8 images per prompt, matching the typical number of output images of image-generation APIs. To account for variances in prompt wording as well as seed selections, we report scores averaged over 50 repetitions. 2) **Mapping Generated Images to cultural artifacts:** We map each image to to its most closely resembling artifact from the concept space of the domain.4 Note that since the prompts focus on global concepts, we obtain the continent, country, and artifact name annotation for each generated image. 3) **Computing Vendi Scores:** With each generated image linked to its closest artifact, we compute the cultural diversity of the generated outputs using the metric defined in Section 5.1. We expand on the details of each of these steps in Appendix J. We details the different kernels to capture different aspects of geo-cultural diversity below.

Kernel definition.With each generated image linked to its closest cultural artifact, we now compute the _cultural diversity_ (CD) of the model's output using the definition in Section 5.1. We define a general similarity kernel that allows us to analyze different aspects of geo-cultural diversity:

\[k(x_{i},x_{j})=w_{1} k_{1}(x_{i},x_{j})+w_{2} k_{2}(x_{i},x_{j})+w_ {3} k_{3}(x_{i},x_{j}) \]

where \(k_{1}(,)\), \(k_{2}(,)\), and \(k_{3}(,)\) are three distinct kernels measuring different aspects of similarity, and \(w_{1}\), \(w_{2}\), \(w_{3}\) assign weights to each. We define \(k_{1}(x_{i},x_{j})=1\) if \(x_{i}\) and \(x_{j}\) have the same continent, and 0 otherwise. Similarly, \(k_{2}(x_{i},x_{j})=1\) if the two items share the same country, and 0 if not. Lastly, \(k_{3}(x_{i},x_{j})=1\) if the two items represent the same artifact, regardless of geographical origin, and 0 otherwise. To illustrate this flexibility, we present results under different kernel configurations:

* **Continent-level diversity**\(\): \(w_{1}=1\), \(w_{2}=0\), \(w_{3}=0\). Considers continent-level similarity.
* **Country-level diversity**\(\): \(w_{1}=0\), \(w_{2}=1\), \(w_{3}=0\). Considers country-level similarity.
* **Artifact-level diversity**\(\): \(w_{1}=0\), \(w_{2}=0\), \(w_{3}=1\). Only considers distinct artifacts.
* **Hierarchical geographical diversity**\(\): \(w_{1}=1/2\), \(w_{2}=1/2\), \(w_{3}=0\). This captures a hierarchical notion of diversity where both continent and country similarities are penalized equally, without explicitly considering individual artifacts.
* **Uniformly weighted diversity**\(\): \(w_{1}=1/3\), \(w_{2}=1/3\), \(w_{3}=1/3\).

**Models.** We evaluate 4 models across closed-source and open-source model types: 1) Imagen 2, 2) Stable-Diffusion-XL, 3) Playground, and 4) Realistic Vision. More details about the model usage and hyperparameters are provided in Appendix J.2.4.

### Results

Results in Figure 8 reveals that when prompted with under-specified prompts mentioning general concepts (Figure 3), current T2I models tend to generate artifacts that lack comprehensive geographical representation. This finding aligns with previous observations (Basu et al., 2023), suggesting a bias towards well-represented and popular countries.

Table 4 presents the results for both the average quality score (q) and the diversity component (\(q}\)) across different kernels. Playground and Imagen generally achieve the highest quality scores based on the HPS-v2 metric. As anticipated, models exhibit the lowest diversity for \(w_{1}=1\), \(w_{2}=0\), \(w_{3}=0\), which considers only continent-level similarity, due to the limited number of continents. Conversely, \(w_{1}=1\), \(w_{2}=0\), \(w_{3}=0\), focusing solely on artifact diversity, yields the highest scores, reflecting the wide array of potential cultural artifacts. In terms of overall performance, Imagen 2 consistently demonstrates the best \(q}\) scores across different kernels for the _Cuisine_ and _Art_ concepts, whereas

SDXL obtains the highest scores for _Landmarks_ concept. Table 5 shows the cultural diversity (\(q}\)). Note that the scores across the board are still low, remaining far from the maximum score of 1. Current T2I models fall short of representing the true breadth and richness of global cultural diversity. In Appendix C, we study the correlation between the 3 metrics: faithfulness, realism and diversity for cultural prompts. As reported in Table 6, we find weak correlations between diversity-faithfulness (\(\) = 0.016) and diversity-realism (\(\) = 0.156) in the cultural context. This finding resonates with the Pareto fronts reported in (Astolfi et al., 2024).

## 6 Discussion

To the best of our knowledge, CUBE is the first large-scale cultural competence benchmark to evaluate T2I models along two crucial dimensions: cultural awareness and cultural diversity. We presented a scalable methodology with a potential to be scaled beyond the 8 countries and 3 concepts considered in this work. Furthermore, we proposed a novel T2I evaluation component: cultural diversity (CD) and measured it using the quality-aware Vendi score. From our investigations so far, one clear finding stands out: there is yet significant headroom for improvement of global cultural competence in the current generation of text-to-image models -- both in terms of awareness and diversity. This seems especially true for the Global South, highlighting the urgency of the need for comprehensive and informative cultural competence testing frameworks. Our correlation analysis of the 3 metrics reveals a noteworthy trend: while faithfulness and realism exhibit a moderate positive correlation, suggesting they can be improved in tandem, cultural diversity remains weakly correlated to these metrics. By highlighting existing limitations in cultural competence of T2I models, we believe our work contributes to a critical dialogue surrounding the development of truly inclusive generative AI systems. To benefit the community, we intend to make the CUBE dataset publicly available5, and encourage its adoption and expansion by the multimodal generative AI community.

    &  &  &  \\   & **IM** & **SDXL** & **PG** & **RV** & **IM** & **SDXL** & **PG** & **RV** & **IM** & **SDXL** & **PG** & **RV** \\   q(\(\)) & 0.27 & 0.21 & **0.29** & 0.27 & **0.25** & 0.22 & 0.21 & 0.23 & 0.31 & 0.30 & **0.34** & 0.33 \\  \(}(w_{1},w_{2},w_{3})\) & & & & & & & & & & & & \\  \(}\) (1, 0, 0) & **0.32** & 0.23 & 0.24 & 0.27 & 0.17 & **0.27** & 0.23 & 0.25 & **0.23** & 0.14 & 0.18 & 0.16 \\ \(}\) (0, 1, 0) & **0.59** & 0.53 & 0.51 & 0.51 & 0.50 & **0.65** & 0.34 & 0.52 & **0.42** & 0.29 & 0.37 & 0.23 \\ \(}\) (0, 0, 1) & **0.91** & 0.71 & 0.82 & 0.74 & 0.73 & **0.84** & 0.58 & 0.81 & **0.72** & 0.60 & 0.51 & 0.44 \\ \(}\) (\(\), \(\), 0) & **0.51** & 0.44 & 0.41 & 0.38 & 0.42 & **0.53** & 0.31 & 0.45 & **0.36** & 0.24 & 0.31 & 0.22 \\ \(}\) (\(\), \(\), \(\)) & **0.72** & 0.58 & 0.66 & 0.59 & 0.55 & **0.66** & 0.45 & 0.52 & **0.52** & 0.39 & 0.41 & 0.30 \\   

Table 4: Breakdown of the mean quality component (q) and mean diversity component (\(q}\)) averaged over 50 repetitions. While all models show relatively low quality scores (as per HPS-v2), Playground (PG) has best quality for _cuisine_ and _art_ concepts and Imagen-2 (IM) for _landmarks_. Different kernels (\(w_{1},w_{2},w_{3}\)) capture different aspects of diversity.

    &  &  &  \\ 
**CD** & **IM** & **SDXL** & **PG** & **RV** & **IM** & **SDXL** & **PG** & **RV** & **IM** & **SDXL** & **PG** & **RV** \\  \(q}(1,0,0)\) & **0.08** & 0.04 & 0.07 & 0.07 & 0.04 & **0.06** & 0.04 & 0.05 & **0.07** & 0.042 & 0.06 & 0.05 \\ \(q}(0,1,0)\) & **0.15** & 0.11 & 0.14 & 0.13 & 0.12 & **0.14** & 0.07 & 0.12 & **0.13** & 0.08 & 0.12 & 0.07 \\ \(q}(0,0,1)\) & **0.24** & 0.14 & 0.23 & 0.20 & **0.18** & **0.18** & 0.12 & **0.18** & **0.22** & 0.18 &Ethical Considerations

We built a large repository of cultural artifacts with the intended use of evaluation of T2I models. Our approach to build this resource relies partly on automated tools, including LLMs, that have been shown to exhibit various societal biases. Hence, care must be taken in interpreting the results of evaluation using this benchmark. While the CUBE benchmark enables a broad-coverage and flexible evaluation of cultural competence in T2I models, their coverage is still limited by the underlying resources it is built on -- namely WikiData (the KB) and GPT-4 Turbo (the LLM). Future work should explore bridging the gaps in coverage through participatory efforts in partnership with communities of people within respective cultures. Furthermore, both CUBE-CSpace and CUBE-1K are intended to be used in evaluation pipelines, rather than training or mitigation efforts.

## 8 Acknowledgements

We thank Partha Talukdar, Kathy Meier-Hellstern, Caroline Pantofaru, Remi Denton, Susanna Ricco, Hansa Srinivasan, David Madras, Nitish Gupta and Sunipa Dev for their feedback and advice; Lucas Beyer and Xiaohua Zhai for insights and support on use of mSigLIP for auto-evals; Sagar Gubbi and Kartikeya Badola for helpful discussions on the human rating template; Shikhar Vashishth for discussions on the use of WikiData; Preetika Verma for assistance with the SLING framework and Dinesh Tewari and the annotation team for facilitating our human evaluation work. We are grateful to the anonymous reviewers for their insightful reviews and suggestions.