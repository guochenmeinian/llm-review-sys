# Towards Comprehensive Detection of Chinese

Harmful Memes

 Junyu Lu1, Bo Xu1, Xiaokun Zhang1, Hongbo Wang1,

**Haohao Zhu1, Dongyu Zhang2, Liang Yang1,3, Hongfei Lin1**

1 School of Computer Science and Technology, 2 School of Foreign Languages,

3 Key Laboratory of Social Computing and Cognitive Intelligence,

Dalian University of Technology, China

dutljy,zhuhh@mail.dlut.edu.cn, dawnknun1993,1846742523a@gmail.com

xubo,zhangdongyu,liang,hflin@dlut.edu.cn

Corresponding author

###### Abstract

Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors. To this end, we focus on the comprehensive detection of Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with fine-grained annotations for various meme types. Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), incorporating contextual information of meme content generated by the LLM to enhance the understanding of Chinese memes. During the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. The experimental results indicate that detecting Chinese harmful memes is challenging for existing models while demonstrating the effectiveness of MKE.1

_Disclaimer: The samples presented by this paper may be considered offensive._

## 1 Introduction

With the development of the Internet, harmful memes on the web have become increasingly rampant. Harmful memes are typically defined as multimodal units consisting of an image and embedded text that cause harm to an individual, an organization, a community, or a social group by specifically targeting social entities (Pramanick et al., 2021; Sharma et al., 2022). They may exacerbate social divisions, trigger discriminatory behaviors, and harm social harmony and unity (Kiela et al., 2020). Due to their negative impact on society, the widespread dissemination of harmful memes has been widely recognized as a growing concern.

In recent years, researchers have made substantial progress in detecting harmful memes. Several datasets, including HMC (Kiela et al., 2020), MMHS (Gomez et al., 2020), Harm-C, and Harm-P (Pramanick et al., 2021), have been established, and various detectors have been proposed Hee et al. (2022); Aggarwal et al. (2023); Pramanick et al. (2021); Sharma et al. (2022); Cao et al. (2022). However, most existing studies only focus on English memes. In contrast, Chinese harmful meme detection remains largely unexplored, presenting challenges in building reliable datasets and developing effective detectors.

On one hand, the types of Chinese harmful memes are diverse. In addition to those targeting specific social entities, many memes on Chinese platforms contain general offense, sexual innuendo, ordispirited culture (Liu and Xu, 2016), as shown in Figure 1. Despite lacking specific targets, they still exhibit potential toxicity, subtly propagating negative values that could lead to serious consequences like violent acts and sexual harassment (Lin and Zhang, 2019). To adapt to the online environment, it is crucial to consider this diversity when constructing the dataset.

On the other hand, understanding the semantics of Chinese harmful memes presents a significant challenge for detectors, necessitating contextual information from both textual and visual elements. For example, Exp. (a) of Figure 1 illustrates gender bias through the height difference between the man and the woman in the image. In contrast, the inline text of Exp. (b) introduces a Chinese insult, _"vegetable dog"_, to tease others, which implies incompetent people. Therefore, incorporating such information is necessary for the effective detection of Chinese harmful memes.

In this paper, we facilitate the detection of Chinese harmful memes, primarily focusing on two aspects: **dataset construction** and **detector development**. For the dataset construction, we first propose the definition of "_Chinese harmful memes_" as guidance, accurately adapting to the Chinese online environment. Based on the definition, we focus on both targeted harmful memes and those exhibiting potential toxicity without specific targets. We conduct fine-grained annotation for harmful memes collected from Chinese online platforms, analyzing their harmful types and combination features of textual and visual information. ToxiCN MM dataset is then constructed, encompassing 12,000 samples containing different harmful types. Two progressive tasks are established using the dataset: (I) Detect if a meme is harmful, and (II) If harmful, further identify its harmful type.

For the detector development, we present a Multimodal Knowledge Enhancement (MKE) detector, integrating the contextual information of meme content for effective detection. We first utilize the large language model (LLM) to capture the context of both the text and image of the meme, leveraging its extensive knowledge acquired through pre-training Zhao et al. (2023). This information is then integrated into a trainable detector as enhanced captions to improve the understanding of memes. In the experimental phase, we evaluate the detection performance of various baselines, including both traditional pre-trained language models (PLMs) and large language models (LLMs), providing a benchmark for evaluation. Experimental results show the effectiveness of our MKE. The main contributions of this paper are summarized as follows:

* We construct, to the best of our knowledge, the first Chinese harmful meme dataset ToxiCN MM, which comprises 12,000 diverse samples. We conduct a fine-grained annotation to analyze their harmful types and modality combination features.
* We present Multimodal Knowledge Enhancement (MKE) as a baseline detector, integrating contextual information of meme content generated by the LLM to improve the detector's understanding of Chinese harmful memes.
* We utilize ToxiCN MM as a benchmark to evaluate the detection performance of various baseline models. Extensive quantitative experiments and qualitative analysis illustrate the effect of MKE. We summarize the challenges of Chinese harmful memes detection.

Figure 1: Illustration of memes in Chinese. (a) is a harmless meme that humorously expresses concern about math homework. (b) is a targeted harmful meme conveying gender bias through the height differences between the man and woman. (c) contains general offense without specific targets, where “_vegetable dog_” is a Chinese insult, implying incompetent people. (d) subtly conveys sexual innuendo with an idiom ”_shy things_” (alluding to ”_sexual intercourse_”). (e) spreads dispirited culture by comparing oneself to ”_garbage_”.

## 2 Related Work

**Harmful Meme.** In recent years, researchers have noticed the importance of detecting harmful memes. Several datasets have been established Kiela et al. (2020); Gomez et al. (2020); Pramanick et al. (2021). Nevertheless, most current studies only focus on English, while research on detecting Chinese harmful memes remains unexplored. To this end, we present the first Chinese harmful meme dataset ToxiCN MM. Here we list Table 1 to compare existing datasets with ToxiCN MM.

Existing studies define "_harmful memes_" as memes that cause harm to specific social entities, based on their social attributes such as religion, race, and gender (Pramanick et al., 2021). However, this definition does not fully apply to the Chinese Internet, where harmful memes often exhibit potential toxicity without specific targets but still perpetuate negative cultural values (Liu and Xu, 2016; Zhang and Zhao, 2021). In this paper, we consider different harmful types for comprehensive detection.

Several methods have been proposed for harmful meme detection, primarily focusing on modeling based on targets of memes Pramanick et al. (2021); Sharma et al. (2022); Koutlis et al. (2023); Ji et al. (2023). However, they do not apply to Chinese harmful memes, which often lack specific targets. Despite this limitation, some methods enhance the detector's understanding of memes by integrating image descriptions to make decisions Wang et al. (2024); Cao et al. (2022). These studies still inspire us to integrate more comprehensive contextual information of meme content into detectors.

**Toxic Language.** Harmful memes are closely linked to toxic language (Kiela et al., 2020), which are rude, disrespectful, or unreasonable, and can drive people away from conversations (Dixon et al., 2018). Chinese harmful memes frequently feature toxic language in their inline text, utilizing slang and linguistic phenomena like homophony. Therefore, understanding Chinese memes necessitates the incorporation of linguistic knowledge.

Additionally, labeling both harmful memes and toxic language is often subjective. Several studies have addressed this issue by focusing on mitigating the subjective bias of annotators during the construction of toxic language datasets (Waseem and Hovy, 2016; Ross et al., 2016; Zeinert et al., 2021; Lu et al., 2023; Wang et al., 2023, 2024), enhancing the reliability of datasets. In this paper, we adopt these measures as a reference in the annotation process of our ToxiCN MM.

## 3 Dataset Construction

### Overview

In this section, we detail the construction of our ToxiCN MM dataset. We first define "Chinese harmful memes" to guide the dataset annotation. We then conduct fine-grained annotation for memes collected from Chinese platforms. In addition to the basic binary labels, we analyze harmful memes from both harmful types and combinations of inline text and image information. After the annotation, statistics of ToxiCN MM are presented. The diagram of dataset construction is shown in Figure 2.

  
**Work** & **Language** & **Size** & **Ratio** & **Pot. Tox.** \\  HarMeme (Pramanick et al., 2021) & English & 3,544 & 34.96\% & ✗ \\ Harm-C (Pramanick et al., 2021) & English & 3,013 & 35.31\% & ✗ \\ Harm-P (Pramanick et al., 2021) & English & 3,020 & 49.21\% & ✗ \\ HMC (Kiela et al., 2020) & English & 10,000 & 38.00\% & ✗ \\ MMHS150K (Gomez et al., 2020) & English & 150,000 & 24.68\% & ✗ \\ TamilMemes (Suryawanshi et al., 2020) & Indian & 2,969 & 65.71\% & ✗ \\ MUTE (Hossain et al., 2022) & Bengali & 4,158 & 37.87\% & ✗ \\ MAMI (Fersini et al., 2022) & English & 11,000 & 50.00\% & ✗ \\  ToxiCN MM (ours) & Chinese & 12,000 & 31.89\% & ✓ \\   

Table 1: Information of harmful meme datasets, in terms of _Language_, _Size_, _Ratio_ of harmful samples, and whether containing memes exhibiting potential toxicity (_Pot. Tox._) without specific targets.

### Definition Development

The recognized definition of _"harmful meme"_ typically pertains to memes that target specific social entities. However, numerous memes on the Chinese Internet diverge from this definition by only propagating negative values without specific targets, which can be equally detrimental to society. To adapt to the Chinese online environment, a refined definition is necessary. Here we introduce the definition of Chinese harmful memes:

_Chinese harmful memes are multimodal units consisting of an image and Chinese inline text that have the potential to cause harm to an individual, an organization, a community, a social group, or society as a whole. These memes can range from offense or joking that perpetuate harmful stereotypes towards specific social entities, to memes that are more subtle and general but still have the potential to cause harm. It is important to note that Chinese harmful memes can be created and spread intentionally or unintentionally. They often reflect and reinforce underlying negative values and cultural attitudes on the Chinese Internet, which are detrimental from legal or moral perspectives._

According to the definition, we further identified the most common harmful types of memes on Chinese platforms based on the consensus of social psychology (Liu and Xu, 2016; Lin and Zhang, 2019) and communication (Peng, 2019; Zheng, 2016) studies. Specifically, it mainly includes _targeted harmful_, _general offense_, _sexual innuendo_, and _dispirited culture_. The harm of these memes to individuals and society has been widely discussed. In this study, we focus on these harmful types when constructing the dataset.

### Data Collection and Filtering

Data collection is the basic work of constructing datasets, and its breadth and quality greatly affect the subsequent research. To ensure a comprehensive dataset, we collect Chinese memes from two well-known public online platforms, _Weibo_ and _Baidu Tieba_, both widely representative of local users in China with active meme communities. We first conduct a random crawl to obtain a diverse set of memes. To maximize the inclusion of harmful memes, we further focus our data crawl on sensitive topics commonly debated online (_e.g._, "_gender_" and "_region_"). Additionally, we also target memes expressing negative emotions and attitudes (_e.g._, "_crazy_" and "_Dispirited Culture_") to enrich the dataset with samples potentially exhibiting toxicity. A total of about 14k memes are collected. We then de-duplicate the data and filter out dirty samples including unreadable memes. The final dataset contains 12k refined memes.

Subsequently, we utilize Baidu-OCR to extract inline text from memes, which offers a high-precision service for Chinese text recognition. To further enhance the sample quality, we also introduce a manual review process to examine the accuracy of the extracted text. Specifically, we normalize the text by adding appropriate separators and removing additional line breaks and spaces.

Figure 2: Illustration of ToxiCN MM construction procedure. According to the definition (Section 3.2), data collection and filtering (Section 3.3) and fine-grained annotations (Section 3.4) are conducted sequentially. The final statistics of ToxiCN MM are presented in Section 3.5.

### Data Annotation

#### 3.4.1 Annotator Selection and Training

Before the formal annotation process, it is crucial to select annotators carefully and mitigate their subjective bias, as this can significantly impact the quality of the dataset (Waseem and Hovy, 2016). To this end, we adopt the following measures: The majority of active users on Chinese platforms are between 12 and 35 years old. Considering Chinese laws that restrict individuals under 18 from engaging in activities that could harm their physical or mental health, we selected annotators aged 18 to 35. We assessed the annotators' proficiency in Chinese meme culture through questionnaires and ensured diversity in terms of gender, region, and education level to enhance reproducibility. The demographics of annotators are shown in Table 2.

During the training of annotators, we provided definitions of Chinese harmful memes, their various types, and conducted case analyses with diverse examples. To evaluate the annotators' abilities, we introduced three test groups of 100 memes each. Annotators labeled the memes independently, with researchers finalizing the labels. Post-round discussions were held to reduce errors, and detailed criteria, including edge cases, were established. Annotators improved from 63% accuracy in the first round to 78% in the final round, demonstrating the effectiveness of the training.

#### 3.4.2 Label Annotation

To guarantee the consistency of label annotation, we establish a comprehensive annotation framework as a guideline. The specific process includes the following three stages.

**Whether Harmful**. The foundation of the labeling is to determine whether a meme is harmful or benign, which is a binary annotation. We strictly follow the definition of "_Chinese harmful memes_", focusing not only on targeted harmful memes but also on samples exhibiting potential toxicity without specific targets.

**Harmful Type**. In the second stage, we further refine the categorization of harmful memes, including targeted harmful, general offensive, sexual minuendo, and dispirited culture. The annotation criteria for each harmful type are provided below. **Targeted Harmful** memes express disgust, prejudice, or stereotypes towards specific individuals or social groups. In contrast, **General Offensive** memes encompass sarcastic or rude content but lack specific targets. We also adhere to psychological and sociological definitions to classify the other two types: **Sexual Innundedo** refers to memes that imply sexual intent to provoke sexual arousal (Bell, 1997). Here we label memes that contain suggestive elements but not sexism or sexual assault as such samples to distinguish them from targeted harmful memes. And **Dispirited Culture** is characterized by the integration of decadent and desperate emotions, conveying a self-negative attitude (Dong et al., 2017).

**Modality Combination**. As multimodal units, harmful memes consist of both textual and visual modality, expressing toxicity through fused or independent features (Kiela et al., 2020). To gain a more comprehensive understanding of how harmful content is propagated via memes, we classify them based on the toxic manifestation of these two modalities, exploring their individual and combined effects. Among them, **Text-Image Fusion** memes exhibit toxicity only through the combined effect of both modalities, while the text and image separately remain benign. In contrast, **Harmful Text** and **Harmful Image** categories refer to one modality (either the text or the image) that independently exhibits toxicity.

During the label annotation, each meme is labeled by at least three annotators. And we use a majority vote to assign the final label. In addition, specific targets of targeted harmful memes are provided. We then discuss the Inter-Annotator Agreement (IAA) of each granularity, as shown in Appendix B.3.

### Statistics Description

For subsequent model training and evaluation, all samples in ToxiCN MM are divided into a training set and a test set at a ratio of 8:2, as detailed in Table 3. We note that there exists a sample imbalance

  
**Characteristic** & **Demographics** \\  Gender & Male: 5, Female: 5 \\ Age & 20: 3, 20–30: 5, 30: 2 \\ Race \& Region & Asians: 7, Others: 3 \\ Education & UG: 3, PG: 4, PhD: 3 \\   

Table 2: Annotators demographics.

among different categories of harmful samples. Memes containing general offensive content (_Off._) constitute nearly 40% of harmful memes. Regarding modality combinations, over 50% of the inline text of harmful memes is harmful. Given that the data distribution accurately reflects the real state of platforms, we do not introduce supplementary sampling to address existing imbalances.

We then analyze the modality combinations across different harmful types, as shown in Table 4. Each type displays distinct patterns in its modality combinations. For example, memes containing general offense or dispirited culture (_Disp._) mainly feature inherently harmful inline text. Contrast, over 50% of targeted harmful memes (_Conv._) integrate multimodal features to express toxicity, where both text and image are individually benign. Moreover, there are 63 memes in which both text and image exhibit toxicity.

## 4 Detector Development

### Overview

To improve the detector's understanding of memes, we present a baseline detector, Multimodal Knowledge Enhancement (MKE), which integrates contextual information of meme content for more accurate predictions. We first leverage the LLM to capture the contextual context of memes and generate enhanced captions. Then, we fine-tune the detector by integrating the original inputs (i.e., text-image pairs) with the generated captions. The illustration of MKE is shown in Figure 3.

For a given meme, its inline text and image are encoded by modality-specific encoders, represented as \(S^{d_{s}}\) for text and \(V^{d_{v}}\) for the image, where \(d_{s}\) and \(d_{v}\) denote the dimensions of the textual and visual vector spaces, respectively.

### Knowledge Mining

We instruct the LLM to generate enhanced captions for the meme by designing the instruction template, which respectively captures the contextual information from both the inline text and image. To improve the understanding of the inline text that may contain slang, we enable LLM to incorporate language features unique to Chinese for semantic analysis. The template is as follows: "_Considering Chinese linguistic characteristics, please analyze the meaning of the text <Texry._" We further convert the image into textual descriptions with the multimodal large language model (MLLM), capturing

    & **T-I** & **Harm.T** & **Harm.I** & **Total** \\ 
**Tg.** & 575 & 404 & 47 & 1,016 \\
**Off.** & 198 & 1,247 & 93 & 1,498 \\
**Sex.** & 431 & 307 & 186 & 914 \\
**Disp.** & 149 & 234 & 19 & 399 \\ 
**Total** & 1,353 & 2,192 & 345 & 12,000 \\   

Table 4: Modality combination distribution of different harmful types in ToxiCN MM.

Figure 3: Overview of MKE. The translation of the inline text is ”what vegetable dog are you”.

    &  &  &  &  &  \\    & & & **Tg.** & **Off.** & **Sex.** & **Disp.** & **T-I** & **Harm.T** & **Harm.I** \\ 
**Train** & 6,538 & 3,062 & 813 & 1,198 & 731 & 320 & 1,082 & 1,754 & 276 & 9,600 \\
**Test** & 1,635 & 765 & 203 & 300 & 183 & 79 & 271 & 438 & 69 & 2,400 \\ 
**Total** & 8,173 & 3,827 & 1,016 & 1,498 & 914 & 399 & 1,353 & 2,192 & 345 & 12,000 \\   

Table 3: Basic statistics of ToxiCN MM, listing the number of non-harmful (_N-Harm._) and harmful (_Harm._) samples, containing targeted harmful memes (_Tg._), general offense (_Off._), sexual innuendo (_Sex._), and dispirited culture (_Disp._), as well as each modality combination category (including text-image fusion (_T-I_), harmful text (_Harm.T_) and harmful image (_Harm.I_)).

harmful elements in the context of Chinese culture. The template is designed as "_Considering Chinese cultural background, please describe the content of the image <Image>_."

In the process of knowledge mining, all parameters of LLMs are frozen. To facilitate knowledge integration for subsequent detectors, the captions of inline text and images are represented by the text encoder, denoted as \(K_{s}\) and \(K_{v}^{d_{s}}\).

### Knowledge Integration

To leverage contextual information, we employ a cross-attention mechanism to integrate the inline text with two types of caption information, due to the consistency of the textual vector space. The feature introducing the textual caption \(K_{s}\) is defined as \(S_{K_{s}}=(SK_{s}^{\ T}/})S\). Similarly, the feature introducing the visual caption \(K_{v}\) is obtained and denoted as \(S_{K_{v}}\). We then incorporate these features into a knowledge-enhanced representation \(S_{K}=(S,S_{K_{s}},S_{K_{v}})\), where \(S_{K}^{d_{s}}\). Next, we concatenate \(S_{K}\) with the original image feature \(V\) to obtain the final representation of a meme, denoted as \(C^{d_{c}}\), where \(d_{c}=d_{s}+d_{v}\). \(C\) is then processed by a trainable classifier, which applies a linear transformation followed by a softmax function to produce detection probabilities.

## 5 Experiments

### Tasks and Baselines

We utilize ToxiCN MM as the benchmark for Chinese harmful meme detection. Specifically, we establish two progressive tasks. (I) **Harmful Meme Detection**, a binary classification task, detects if a meme is harmful; (II) **Harmful Type Identification**, a multi-classification, further identifies its harmful type, including targeted harmful memes, general offense, sexual innuendo, or dispirited culture. In addition to MKE, we evaluate the performance of various baselines, including both unimodal and multimodal models. For unimodal models, we utilize RoBERTa (Liu et al., 2019), GPT-3.5, and GPT-4 (text input) as text-only models, while ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2021) serve as image-only models. For multimodal models, we employ CLIP (Radford et al., 2021), the fusion of RoBERTa and ViT, which concatenates representations of the text and image for classification, and GPT-4 (text and image input).

### Implementation

We adopt precision (\(P\)), recall (\(R\)), and macro \(F_{1}\)-score (\(F_{1}\)) as metrics. We also report the \(F_{1}\) of harmful memes and each harmful type. We respectively utilize CLIP and the fusion of RoBERTa and ViT as the backbones of MKE, and we use GPT-4 to generate enhanced captions. For conventional PLMs, we fine-tune their parameters and select the best-performing model based on test set outcomes. For LLMs, we evaluate their performance in a zero-shot scenario, using instruction templates in Chinese. More details are provided in Appendix B.5.

### Results and Discussions

In this section, we present our experimental results and conduct a detailed analysis. The performance of baselines is evaluated across two tasks, as shown in Table 5. From the results, we can observe that: (1) In contrast to LLMs, conventional fine-tuned pre-trained baselines (i.e., CLIP and the combination of RoBERTa and ViT) achieve better detection performance, indicating their effectiveness in specific tasks. When considering the modality of input information, RoBERTa, which solely utilizes the inline text of memes, achieves a significantly higher \(F_{1}\) score (average increase of 8.4%) than vision-based methods such as ResNet and ViT, which solely utilize images. This result supports the conclusion drawn in (Hee et al., 2022), namely that text comprehension plays a more crucial role than image understanding in the detection of harmful memes.

(2) GPT-4 and GPT-3.5 show similar performance in binary _harmful meme detection_ when only the inline text is provided, and there is a clear enhancement in the multiclass _harmful type identification_ task. After incorporating the image input, GPT-4 shows the best detection performance for sexual innuendo (_Sex._) memes, while its performance decreases for general offense (_Off._) and dispirited culture (_Disp._). Referring to Table 4, we observe that most samples of _Sex._ exhibit toxicity through image-text fusion or harmful images, whereas images of _Off._ and _Disp._ are mostly benign. This suggests that the toxicity of visual information has a significant impact on the decisions of GPT-4. We will further explain this in the following case study.

(3) Our MKE demonstrates superior performance, with an average macro-F1 -score improvement of 0.73% and 3.22% over the backbone models for both tasks. This improvement illustrates the effectiveness of introducing contextual information of meme content for detecting Chinese harmful memes. Ablation studies show that both enhanced captions for inline text and images contribute to the detector's deeper understanding of memes, leading to more precise classifications. Additionally, the degree of performance enhancement varies depending on the type of harmful meme. For instance, for targeted harmful memes (\(T_{g}\).), where toxicity often relies on the combination of image and text, image captions provide a greater boost (2.07%). In contrast, for memes expressing dispirited culture (_Disp._), where the text is typically harmful, inline text captions lead to a larger improvement (2.58%).

### Case Study

To further illustrate the rationales of MKE, we provide several case studies, as shown in Table 6. We list enhanced captions of harmful memes and the predictions of other models for reference. We also instruct GPT-4 to generate reasons for its detection decisions. We do not introduce additional templates to standardize its reasoning to reflect GPT-4's true understanding of memes more accurately.

Exp. (a) is a targeted harmful meme towards Asians. Through the caption, we observe that GPT-4 understands the meme's meaning solely through the inline text, recognizing the high standard of Asian parents on their children's academic performance. This highlights GPT-4's strong contextual understanding. After integrating this information, compared to the backbone (Fusion), our MKE model makes the correct decision, illustrating that incorporating contextual information of meme content enhances the model's understanding of memes.

For more insight into the challenges of detecting Chinese harmful memes, we manually inspected the samples misclassified by most baselines. Two main types of errors are summarized.

**Type I error**: Benign information contained in harmful memes can influence the judgment of models, resulting in incorrect detection. In Exp. (b), when presented solely with the inline text, GPT-4 accurately interprets the meme's meaning, comparing "\(I\)" with a "_little mouse_" to convey a dispirited culture. However, upon introducing the image, GPT-4 mistakenly interprets the mouse as being "_gently stroked_" and incorrectly categorizes the meme as harmless. This suggests that the model may overlook the potential toxicity of memes due to the seemingly benign nature of a certain modal. In contrast, MKE integrates original sample and caption information to make the correct judgment.

    &  &  \\   & Model & P & R & F1 & F1\({}_{}\) & P & R & F1 & F1\({}_{_{g}}\) & F1\({}_{T}\) & F1\({}_{C}\) & F1\({}_{}\) \\   & GPT3.5 & 69.46 & 66.59 & 67.46 & 53.25 & 36.93 & 23.60 & 24.35 & 27.42 & 29.01 & 10.52 & 14.63 \\  & GPT4 & 74.52 & 65.59 & 68.01 & 51.78 & 58.29 & 41.81 & 44.86 & 11.43 & 62.22 & 32.36 & 34.15 \\  & RoBERTa & 75.52 & 77.54 & 76.36 & 66.48 & 53.24 & 60.06 & 55.85 & 48.79 & 71.81 & 42.70 & 29.23 \\   & ResNet & 66.61 & 66.92 & 66.76 & 53.76 & 35.66 & 36.23 & 36.46 & 16.67 & 51.28 & 20.93 & 12.28 \\  & ViT & 68.97 & 68.61 & 68.78 & 57.24 & 43.38 & 37.86 & 39.10 & 24.17 & 54.57 & 34.32 & 9.01 \\   & GPT4 & 74.67 & 68.64 & 70.11 & 55.77 & 58.87 & 41.77 & 43.89 & 23.53 & 32.56 & 55.74 & 23.53 \\   & Fusion & 77.77 & 79.18 & 78.39 & 69.61 & 58.93 & 60.85 & 59.35 & 50.24 & 73.35 & 47.85 & 38.71 \\   & + \(K_{s}\) & 78.17 & 79.32 & 79.04 & 70.33 & 60.09 & 63.38 & 61.28 & 51.02 & 75.60 & 48.75 & **43.06** \\   & + \(K_{v}\) & 77.93 & 79.32 & 78.55 & 69.85 & 59.16 & 60.67 & 59.61 & 51.41 & 75.00 & 48.68 & 36.23 \\   & + MKE & 77.96 & 80.96 & 79.16 & 70.22 & **62.41** & 62.08 & **62.17** & 53.27 & 77.45 & 56.10 & 39.24 \\    & CLIP & 78.95 & 80.26 & 79.54 & 71.28 & 54.85 & 64.95 & 57.85 & 49.58 & 74.65 & 49.64 & 26.92 \\   & + \(K_{s}\) & 79.23 & 80.71 & 79.89 & 71.72 & 56.24 & **66.45** & 59.23 & 47.80 & 77.17 & 54.72 & 27.78 \\   & + \(K_{v}\) & 79.39 & **80.93** & 80.07 & 71.96 & 57.20 & 64.27 & 59.24 & **53.41** & 76.97 & 52.53 & 25.24 \\   & + MKE & **79.76** & 80.79 & **80.23** & **72.35** & 60.38 & 63.52 & 61.47 & 51.52 & **77.19** & **57.14** & 33.33 \\   

Table 5: Detection performance of baselines. Results show the mean of \(P\), \(R\), and macro \(F_{1}\), where the **bold** and underline scores respectively represent the optimal and suboptimal values. _Fusion_ refers to the fusion of RoBERTa and ViT, and \(K_{s}\) and \(K_{v}\) respectively denote introducing enhanced captions of inline text and image. All results are statistically significant, as determined by a \(t\)-test (\(p<0.01\)).

**Type II error**: Harmful memes containing unique cultural backgrounds are easily missed by models. Among them, the most challenging samples are the memes whose inline text contains specific expressions in Chinese, such as homophones and metaphors. Take Exp. (c) for example, where the term "_dog-banana_" is a homonym of "_bark_" in Chinese. Therefore, this meme is essentially a harmful meme containing general offense, implicitly expressing dissatisfaction with another person. Due to the lack of related knowledge, current models fail to comprehend the underlying semantics of these memes, making them difficult to detect successfully.

These case studies further illustrate that Chinese harmful memes detection is a complex multimodal semantic understanding task, which is challenging for existing models. The error analysis shows that fully integrating image and text information and introducing more comprehensive knowledge about Chinese culture are both crucial for effectively detecting harmful memes.

## 6 Conclusions and Future Work

In this paper, we focus on the comprehensive detection of Chinese harmful memes. We present the first Chinese harmful meme dataset ToxiCN MM. It has 12k samples including not only targeted harmful memes but also those only exhibiting potential toxicity without specific targets, adapting to the Chinese online environment. In addition to binary labels, ToxiCN MM provides harmful types and modality combination categories of memes. To improve the understanding of Chinese harmful memes, we present a Multimodal Knowledge Enhancement (MKE) detector, introducing the contextual information of inline text and images. In the experimental phase, we evaluate multiple baseline models for their performance in detecting Chinese harmful memes. Our case study suggests that integrating multimodal information and comprehensive background knowledge is crucial for effective detection.

In future work, we aim to design more effective methods for Chinese harmful memes detection. Meanwhile, we notice that the accuracy of LLMs in detecting Chinese harmful memes is still limited. Considering the potential harm that these memes may cause, this task can be used to evaluate the safety of LLMs. We will employ prompt engineering and instruction fine-tuning methods to explore and enhance the detection performance of LLMs. Additionally, we will continuously evaluate state-of-the-art models to ensure the effectiveness of ToxiCN MM. We expect our dataset, benchmark, and insights will assist researchers in related fields.

## 7 Limitations

In this study, we focus on several most common harmful types of memes on the Chinese online environment. Due to the filtering mechanism, some harmful memes, such as those containing _fake news_, are extremely scarce on Chinese platforms. As a result, our ToxiCN MM does not encompass all harmful types. The techniques we used to boost the percentage of harmful content during the dataset construction process may introduce problematic bias. In future work, we plan to broaden the scope and increase the number of meme cravks, focusing on more Chinese platforms to mitigate sampling bias. While we have implemented several measures to mitigate annotation bias, we acknowledge that our dataset may still contain mislabeled data due to the subjective understanding of annotators for Chinese harmful memes. Furthermore, our current study primarily focuses on predicting whether a given meme is harmful. We will further evaluate the ability of baselines to generate explanations for Chinese harmful memes with quantitative experiments.

## 8 Ethics Statement

Our study aims to facilitate the comprehensive detection of Chinese harmful memes and raise researchers' attention to non-English memes. The social psychological community has recognized the harms of the harmful types we selected in the dataset. We acknowledge the risk of malicious actors attempting to reverse-engineer memes. We strongly discourage and denounce such practices, emphasizing the necessity of human moderation to prevent them. All resources are intended solely for scientific research and are prohibited from commercial use. We believe the benefits of our proposed resources outweigh the associated risks. We strictly follow the data use agreements of each public online social platform. The opinions and findings contained in the samples of our presented dataset should not be interpreted as representing the views expressed or implied by the authors.

To mitigate the potential psychological impact on annotators evaluating harmful content, we implement the following protective measures: 1) obtain explicit consent regarding exposure to potentially abusive content, 2) limit weekly evaluations to manage exposure and ensure reasonable daily workloads, and 3) recommend discontinuing reviews if they experience distress. Additionally, we conduct regular well-being checks to monitor their mental health.