# Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning

The Viet Bui

Singapore Management University, Singapore

theviet.bui.2023@phdcs.smu.edu.sg

&Tien Mai

Singapore Management University, Singapore

atmai@smu.edu.sg

&Thanh Hong Nguyen

University of Oregon Eugene, Oregon, United States

thanhhng@cs.uoregon.edu

###### Abstract

This paper concerns imitation learning (IL) in cooperative multi-agent systems. The learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL was shown to be done efficiently via an inverse _soft-Q_ learning process. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning. In this work, we introduce a new multi-agent IL algorithm designed to address these challenges. Our approach enables the centralized learning by leveraging mixing networks to aggregate decentralized Q functions. We further establish conditions for the mixing networks under which the multi-agent IL objective function exhibits convexity within the Q function space. We present extensive experiments conducted on some challenging multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (_SMACv2_), which demonstrates the effectiveness of our algorithm.

## 1 Introduction

Imitation learning (IL) is a powerful approach for sequential decision making in complex environments in which agents can learn desirable behavior by imitating an expert. There are applications of IL in several real-world domains, including healthcare  and autonomous driving . A rich body of IL literature focuses on simple single-agent settings  while many recent works develop new multi-agent IL methods that are tailored to either cooperative or non-cooperative settings . Leading methods in  explore solution concepts for Markov games such as Nash equilibria. Findings on underlying properties of these solution concepts are integrated to extend the single-agent IL models to multi-agent settings. While these methods show promising results, they face difficulties in training, as the underlying adversarial optimization process involves biased and high variance gradient estimators, leading to a highly unstable learning process.

Our work studies IL in cooperative multi-agent settings. We leverage recent advanced findings in both single-agent IL and multi-agent reinforcement learning (MARL) to build a unified multi-agent IL algorithm, named Multi-agent Inverse Factorized Q-learning (MIFQ). MIFQ is built upon inverse soft Q-learning (IQ-Learn) , a leading single-agent IL method of which advantage is to learn a single Q-function that implicitly defines both reward and policy functions, thus avoiding adversarial training. To adapt this idea for multi-agent settings, following the well-known paradigm of centralised training with decentralised execution (CTDE) in MARL , we develop a centralized learningapproach based on the concepts of mixing and hyper-networks . This approach facilitates the integration of individual state-value and reward functions into a unified learning objective, enabling the training of decentralized imitation policies for agents in a centralized manner.

In this work, we formulate the IL problem as a multi-agent inverse soft-Q learning task, where the objective is to match the occupancy distribution of the joint learning policy with that of the expert. To make the learning process practical and facilitate the CTDE paradigm, we propose factorizing both the global Q and V functions using mixing networks with non-negative weights and convex activation functions (such as ReLU and ELU). We further examine the Individual-Global-Max (IGM) principle [29; 35], a core concept in factorization learning, which suggests that the optimal joint actions across agents are equivalent to the collection of individual optimal actions for each agent. However, we argue that this principle does not apply for our problem setting since our objective is to recover soft policies based on max-entropy reinforcement learning. Therefore, we introduce a generalized version of IGM, called Individual-Global-Consistency (IGC), which requires that the distribution of joint actions produced by the optimal joint policy matches the collection of local action distributions produced by the individual optimal policies. We then demonstrate that IGC holds in our approach.

A key advantage of single-agent inverse soft Q-learning (IQ-Learn) is that the original training problem, which is a max-min optimization, can be conveniently converted into a (non-adversarial) concave maximization problem . This conversion ensures that the optimization objective is well-behaved, contributing to the effectiveness of the original IQ-Learn. Interestingly, we provide theoretical results showing that under our factorization approach, the multi-agent training objective can also be converted into a (non-adversarial) concave maximization problem. This advantage holds with any multi-layer feed-forward mixing networks with non-negative weights and convex activations.

Finally, we conduct extensive experiments in three domains: SMACv2 , Gold Miner , and MPE (Multi Particle Environments) . We show that our MIFQ outperforms other baselines in all these environments. To the best of our knowledge, our experiments with SMACv2 mark the first time IL algorithms are employed and evaluated on such a challenging multi-agent environment.

Concretely, we make the following main contribution:

1. We introduce a new multi-agent IL method based on inverse soft-Q learning and factorization.
2. We show that our approach satisfies the IGC, a generation of the IGM principle.
3. We show that the max-min learning objective can be converted into a concave maximization problem, under any mixing networks of non-negative weights and convex activations, which helps avoid adversarial training and ensure well-behaved learning within the Q-space.
4. We empirically show state-of-art results on several challenging multi-agent game tasks, including an advanced version of the Star-Craft multi-agent challenge (_SMACv2_).

## 2 Related Work

**Single-Agent Imitation Learning.** There is a rich body of existing works focusing on generating policies that mimic an expert's behavior given data of expert demonstrations in single-agent settings. A classic approach is behavioral cloning which casts IL as a supervised learning problem, attempting to maximize the likelihood of the expert's trajectories [32; 28]. This approach, while simple, requires a large amount of data to work well due to its compounding error issue. An alternative approach is to recover the reward function (either implicitly or explicitly) for which the expert's policy is optimal [13; 18; 30; 20; 10]. Leading methods [18; 13] follow an adversarial optimization process (which is similar to GAN ) to train the imitation policy and reward function alternatively. These adversarial training-based methods, however, suffer instability. A more recent work by  overcomes this instability issue by introducing an inverse soft-Q learning process. For a comprehensive review of literature on this topic, we refer readers to the survey article in .

**Multi-Agent Imitation Learning.** Most of single-agent IL works, however, do not apply directly to multi-agent settings. Literature on multi-agent IL is rather limited. A few works study multi-agent IL either in cooperative environments [3; 22; 37; 5] or competitive environments [24; 31; 36; 42]. Recent leading methods employ equilibrium solution concepts in Markov games to extend some existing single-agent IL methods to the multi-agent settings [36; 42]. However, these methods still suffer the instability challenge during training as they still rely on adversarial training. Our work focuseson multi-agent IL in a cooperative Dec-POMDP environment. We utilize the idea of inverse soft-Q learning in single-agent IL  to avoid adversarial training. We extend the idea to the multi-agent settings under the paradigm of centralized training decentralized execution (CTDE) from MARL, allowing an efficient and stable learning process.

**MARL.** The literature of MARL encompasses a number of advanced methods; many follow the well-known CTDE paradigm. In [25; 11], they employ actor-critic architectures and train a centralized critic that utilizes global information. Value-decomposition (VD) methods represent the joint Q-function as a function of agents' local Q-functions [38; 29]. QMIX  offers a more advanced VD method for consolidating agents' individual local Q-functions through the utilization of mixing and hyper-network  concepts. Later value function factorization approaches, such as QTRAN  and QPLEX , introduce new factorization methods for MARL based on the IGM principle, which necessitates consistency between joint and local action selections. There are also policy gradient based MARL algorithms. Independent PPO (IPPO), a decentralized MARL, can achieve high success rates in several challenging SMAC maps . MAPPO, a PPO version for MARL, achieves SOTA results on several tasks . Our work utilizes MAPPO to train our expert and generate expert demonstrations. We also employ a hyper-network architecture  with mixing networks of non-negative weights and convex activations to facilitate our CTDE. While our mixing network techniques are similar to those used in QMIX , we show that this configuration offers several _unique advantages_ in the context of inverse Q-learning. Specifically, the training objective can be transformed from a max-min problem into a (non-adversarial) concave maximization problem, and it upholds the _IGC principle_. Other methods, such as QTRAN and QPLEX [35; 39], are primarily based on the IGM principle and are designed for different objective structures, making them unsuitable for our context.

## 3 Preliminaries

A multi-agent cooperative system can be described as a decentralized partially observable Markov decision process (Dec-POMDP), defined by a tuple \(\{,,,,P,R\}\), where \(\) is the set of global states, \(\) is the set of local observations of agents, and \(\) is all the agents. In addition, \(=_{i_{}}_{i}\) is the set of joint actions of all the agents, \(_{i}\) is the set of actions of an agent \(i\), and \(P\) is the transition dynamics of the multi-agent environment. Finally, in Cooperative MARL (Coop-MARL), all agents share the same reward function, \(R\), that can take inputs as global states and actions of all the agents and return the corresponding rewards.

At each step, given a global state \(S\), each ally agent \(i\) takes an action \(a_{i}_{i}\) based on his policy \(_{i}(a_{i} o_{i})\), where \(o_{i}\) is his local observation. The joint action is defined as \(A=\{a_{i} i\}\) and the joint policy is defined accordingly: \((A S)=_{i}_{i}(a_{i} o_{i})\). After all the agent actions are executed, the global state is updated to a new state \(S^{}\) with the transition probability \(P(S^{} A,S)\). The objective of Coop-MARL is to find a joint policy \(( S)=_{i}_{i}( o_{i})\) that maximizes the expected long-term joint reward, formulated as follows:

\[_{}_{}_{t=0}^{}^{t}R(A_ {t},S_{t})\]

## 4 Multi-agent Inverse Q-Learning

In IL, the objective is to recover an expert reward or expert policy from some expert demonstration data. Our new multi-agent IL algorithm is built upon an integration of recent advanced findings in single-agent IL (i.e., the new leading IQ-Learn ) and in cooperative MARL. In the following, we first present the main idea of IQ-Learn, with some direct centralized and decentralized adaptations to multi-agent settings. We then discuss key shortcomings of such simple adaptations in the context of a Dec-POMDP. Finally, we present our new algorithm which tackles all those shortcomings.

### Inverse Soft Q-Learning

#### 4.1.1 Centralized Inverse Q-Learning

In general, IQ-Learn can be directly adapted for IL in a _fully-observable_ cooperative multi-agent setting. Given expert samples \(^{E}\!=\!\{\!=\!\{(A_{t},S_{t}),\ t\!=\!0,1...\}\}\), IQ-Learn aims to solve the following maximin problem, which is also the objective of adversarial learning-based IL approaches [18; 13]:

\[_{R}_{}L(R,)=_{(S,A)^{E}}R(S,A) -_{^{}}R(S,A)-_{^{}} (S,A)}\] (1)

where \(^{}(S,A)\) is the occupancy measure of visiting state \(S\) and joint action \(A\), under the policy \(\):

\[^{}(S,A)=(1-)(A S)_{t=0}^{}^{t}P(S_{t}=S ),\]

\(^{E}\) is the occupancy measure of the expert policy \(^{E}\) and \(_{^{}}(S,A)\) is the entropy regularizer.

A typical method to solve (1) is to run an adversarial optimization process over rewards and policies, of which idea is similar to generative adversarial networks [18; 13]. However, such an approach suffers instability, a well-known challenge of adversarial training. Thus, IQ-Learn avoids adversarial training by learning a single soft Q-function, as defined in the following.

**Definition 4.1** (Soft Q-function ).: _Given a policy \(\), the soft Bellman operator \(^{}\) is defined as:_

\[(^{}Q)(S,A)=R(S,A)+_{S^{} P ( S,A)}V^{,Q}(S^{})\] \[V^{,Q}(S)=_{A( S)} Q(S,A)-(A S)\] (2)

\(^{}\) _is contractive and defines a unique soft Q-function for the reward function \(R\), i.e., \(Q=^{}Q\)._

Essentially, in , they show that (1) is equivalent to the following single minimization problem which only requires optimizing over the soft Q-function: 1

\[_{Q}J(Q)=_{(S,A)^{E}}Q(S,A)- _{S^{} P( S,A)}V^{Q}(S^{}) -(1-)_{S^{0}}V^{Q}(S^{0})}\] (3)

where \(Q:\) is the soft Q-function, \(S^{0}\) is the initial state, and \(V^{Q}(S)\) is computed as \(V^{Q}(S)=(_{A}(Q(S,A)))\). A shortcoming of centralized IQ-Learn described above is that the computation of \(V^{,Q}(S)\) or \(V^{Q}(S)\) is **not tractable** as it requires to sample joint actions, an the joint action space grows exponentially in the number of agents. Furthermore, it requires full observations for agents, which is not applicable in a Dec-POMDP with local partial observations.

#### 4.1.2 Independent Inverse Q-Learning

An alternative approach to overcome the shortcomings of centralized IQ-Learn is to consider a separate IL problem for each individual agent, considering its local observations. That is, one can set the objective to recover a local Q function \(Q_{i}(o_{i},a_{i})\), as a function of a local observation \(o_{i}\) and local action \(a_{i}\), for each agent \(i\). The local IQ-Learn loss function can be formulated as follows:

\[_{Q_{i}}_{_{i}}J_{i}(Q_{i},_{i})=_{ ^{E}}Q_{i}(o_{i},a_{i})-_{o^{}_{i}(P(  S,A)}V^{Q,}_{i}(o^{}_{i})-(1-) _{o^{}_{i}}V^{Q,}_{i}(o^{0}_{i})}\] (4)

Here, \(o^{}_{i} P( S,A)\) means \(o^{}_{i}\) is the local observation of agent \(i\) corresponding to the new state \(S^{} P( S,A)\). The value function can be computed as :

\[V^{Q,}_{i}(o_{i})=_{_{i}(a_{i}|o_{i})}[Q_{i}(o_{i},a_{i}) -_{i}(a_{i}|o_{i})]\] (5)

This approach is more tractable than the centralized method, making it suitable for a Dec-POMDP environment by enabling decentralized policies for agents. However, it has limitations in addressing the interdependence between agents and the global information available during the training process.

### Inverse Factorized Soft Q-Learning

We now present our new multi-agent IL algorithm designed to recover Q functions in the multi-agent setting, following the CTDE paradigm. A possible straightforward approach is to directly factorize the centralized objective function in 3. This factorization can be achieved by decomposing the global Q-function \(Q(S,A)\) into local Q-functions \(Q_{i}(o_{i},a_{i})\), and computing the global V-functions using the formula \(V^{Q}(S)=(_{A}(Q(S,A)))\). However, this means that the global policy has to be computed based on a softmax of the global Q-function as \((S,A)=\,(Q(S,A^{}))}}\). This requirement of a global policy computation violates the decentralized execution under CTDE. According to this analysis, we will instead start from the original _max-min_ formulation in (1), keeping in mind the necessity of having \(_{i}\) to support CTDE.

Our key ideas involve creating (i) agent local Q-value networks that output local V-values \(V_{i}^{Q,}(o_{i},a_{i})\) and (ii) mixing the networks that utilize global state information to combine local values of agents into joint values that comprise the objective of the inverse soft Q-learning; and (iii) hyper-networks that provide a rich representation for the weights of the mixing networks, allowing us to govern their value ranges. There are two important aspects driving our mixing architecture: the consistency between local and global policies, and whether we can leverage the advantages of single-agent IQ-learn , specifically non-adversarial training and convexity within the Q-space. We will first describe our factorization approach, followed by an analysis of these two aspects.

#### 4.2.1 Multi-agent IL Network Architecture

Overall, our network architecture comprises of three different types of networks, described below.

**Agent local \(Q\) networks.** Let \((S,A)=\{Q_{i}(o_{i},a_{i})\}_{i=1}^{m}\) of local soft Q-values of agents where \(m=||\) is the number of agents and \((o_{i},a_{i})(S,A),\ i\). For an abuse of notations, we denote by \(Q_{i}(o_{i},a_{i};_{i})\) as the local \(Q\)-value network of the agent \(i\) of which learnable parameter is \(_{i}\).2 Given \((Q,)\), we then use \(^{Q,}(S,A)\) to represent the corresponding state-value vector \(^{Q,}(S)=\{V_{i}^{Q,}(o_{i})\}_{i=1}^{m}\) where \(V_{i}^{Q,}(o_{i})\) is computed in (5). **Q** and \(^{Q,}\) will be passed to the corresponding mixing networks to induce the joint values \(Q^{tot}\) and \(V_{Q,}^{tot}\). These joint values will be then incorporated into computing the objective of the inverse soft Q-learning. Here, we do not assume any specific model for the policies \(_{i}\). As we show later, there exists a closed-form solution to compute optimal policies, which allows us to eliminate the variable \(_{i}\) from the training problem.

**Value mixing networks.** We create two mixing networks to combine local Q and V values into the joint values \(Q^{tot}\) and \(V^{tot}\), respectively. Let's denote these networks as \(_{_{Q}}()\) and \(_{_{V}}()\). Here \(_{Q}\) and \(_{V}\) are corresponding weights of these two networks. In particular, we have:

\[Q^{tot}(S,A)=-_{_{Q}}-(S,A) V_{Q,}^{tot}(S)=_{_{V}}^{Q,}(S) \]

Note that, instead of directly mixing \((S)\), we mix the negative of this vector to achieve the **IGC** principle and the convexity. We can now formulate the objective function of our _multi-agent inverse factorized Q-learning_ w.r.t the local \(Q\)-values and polices, and these mixing networks, as follows:

\[_{}_{}J(,,_{Q}, _{V}) =_{(S,A)^{E}}Q^{tot}(S,A)-_{S^{} P(|S,A)}V_{Q,}^{tot}(S^{}) \] \[-(1-)_{S^{Q}}V_{Q,}^{tot}(S_{0}) }\] (6)

We further assume that _the two mixing networks are multi-layer feed-forward networks, constructed with non-negative weights and convex activation functions_ (e.g., _ReLU_, _ELU_, and _Maxout_). This configuration is widely-used in the literature and, as shown in the next section, is sufficient to ensure consistency between the global and local policies (a form of the IGM property, as shown later) and, importantly, the convexity of the training objective function within the Q-space.

**Hyper-networks.** Finally, we create two hyper-networks corresponding the two mixing networks. These hyper-networks take the global state \(S\) as an input and generate the weights \(_{V}\) and \(_{Q}\) of the mixing networks accordingly. The creation of such hyper-networks allows us to have a rich representation of the weights that can be governed to ensure the convexity of the objective \(J(,,_{Q},_{V})\) as well as the IGC property (as described below ). We can write \(_{V}=_{V}(S;_{V})\) and \(_{Q}=_{Q}(S;_{R})\) where \(_{V}\) and \(_{R}\) denote trainable parameters of the state-value and reward hyper-networks. We can alternatively write the objective \(J(,,_{Q},_{V})\) as \(J(,_{R},_{V})\) when the local soft \(Q\)-values \(\) are parameterized by \(=\{_{1},_{2},,_{m}\}\) and the weights of the mixing networks \((_{V},_{Q})\) are parameterized by \(_{V}\) and \(_{R}\) respectively. The optimal policy, as shown later, can be computed as soft-max of **Q**. We note that we will use either \(J(,,_{Q},_{V})\) or \(J(,_{R},_{V})\) depending on the context of our analysis.

#### 4.2.2 Individual-Global-Consistency (IGC)

One of the key aspects of CTDE is ensuring consistency between global learning and local policy execution. In previous MARL approaches based on mixing Q-functions, this is often expressed through the IGM principle , implying that the optimal joint actions across agents (obtained by maximizing the joint Q-function) are equivalent to the collection of individual optimal actions of each agent (obtained by maximizing their local Q-functions). In our context, there are significant differences that prevent direct application of this principle. First, our recovered policy is not derived from directly maximizing the Q-function. Instead, it is obtained by solving a distribution-matching objective, which results in soft policies computed as the soft-max of Q-values. To adapt this principle in the context of inverse Q-learning, we introduce the concept of _Individual-Global-Consistency (IGC)_, which is a generalization of the IGM, defined in the following.

**Definition 4.2** (Individual-Global-Consistency (IGC)).: _A factorized learning approach is said to adhere to the IGC principle if and only if the optimal joint policy (obtained from solving the global training problem) is equivalent to the collection of individual optimal policies for each agent (obtained by solving their respective local training objectives)._

The following result states that our factorized inverse Q-learning approach satisfies the IGC principle.

**Theorem 4.3**.: _Let \(^{*}=_{}\{J(,,_{Q},_{V})\}\), then there are a set of optimal local policies \(\{_{i}^{*}\}\) such that \(_{i}^{*}=_{_{i}}J_{i}(Q_{i},_{i}),\; i\), and \(^{*}=\{_{i}^{*},\;i\}\)._

The theorem implies that the distribution of joint actions, produced by the optimal joint policy, is the same as the collection of local action distributions produced by the local optimal policies. Moreover, as an additional note, the _IGM principle_ with respect to the joint Q-function also holds under our mixing network architecture, i.e., for all \(S\):

\[_{A}\{Q^{tot}(S,A)\}=\{_{a_{i}}\{Q_{i}(o_{i},a_{i}) \},\;i\}.\]

#### 4.2.3 Non-adversarial Training and Convexity

One of the main advantages of the single-agent inverse Q-learning algorithm is that the training problem can be equivalently transformed into a concave maximization problem over the Q-space, making the training process highly stable and well-behaved. We demonstrate below that these features still hold under our mixing network architecture.

**Proposition 4.4**.: _The max-min problem in (6) is equivalent to the following maximization problem:_

\[_{}J(,_{Q},_{V}) =_{(S,A)^{k}}Q^{tot}(S,A)- _{S^{} P(|S,A)}V_{Q}^{tot}(S^{}) \] \[-(1-)_{S^{0}}V_{Q}^{tot}(S_{0}) }\] (7)

_where \(V_{Q}^{tot}(S)=_{_{V}}^{Q}(S)\), and \(^{Q}(S)=V_{i}^{Q}(o_{i})}{{=}}( _{a_{i}}(Q_{i}(o_{i},a_{i}))),\;i\). Moreover, let \(^{*}\) be optimal to (7), then the global optimal policy can be recovered as follows:_

\[^{*}=_{i}^{*}(o_{i},a_{i})=(o_{i},a_{i})))}{ _{a_{i}^{}}(Q_{i}(o_{i},a_{i}^{}))}\;i}\] (8)

Furthermore, it can be shown that the objective in (7) is concave in **Q** which is an essential property that make the Q-learning procedure well-behaved and stable .

**Theorem 4.5**.: \(J(,_{Q},_{V})\) _is (strictly) concave in **Q**. As a result, (7) always yield a unique solution within the Q-space._

Theorem (4.5) indicates that the global training objective \(J(,_{Q},_{V})\) is concave in **Q** when using _any multi-layer feed-forward mixing networks with non-negative weights and convex activation functions_. This result is highly general and notably non-trivial due to the nonlinearity and complexity of \(V_{i}^{Q}\) (as functions of \(Q_{i}\)) and the mixing networks. Prior work often relies on a two-layer mixing structure , noting that such a two-layer structure is sufficient for the mixing network to approximate any monotonic function arbitrarily closely in the limit of infinite width .

### MIFQ Algorithm

We now present our practical algorithm, MIFQ, and details of our implemented network architecture.

**Mixing and Hyper Networks.** We employ the following two-layer feed-forward network structure:

\[_{_{Q}}() =(|W_{1}^{Q}|+b_{1}^{Q})|W_{2}^{Q}| +b_{2}^{Q}\] (9) \[_{_{V}}() =(|W_{1}^{V}|+b_{1}^{V})|W_{2}^{V}| +b_{2}^{V}\] (10)

for mixing networks, where \(_{Q}=\{W_{1}^{Q},W_{2}^{Q},b_{1}^{Q},b_{2}^{Q},\}\) and \(_{V}=\{W_{1}^{V},W_{2}^{V},b_{1}^{V},b_{2}^{V}\}\) are the weight and bias vectors. The absolute operations \(||\) ensure that all the weights are non-negative. Moreover, \(_{Q}\) and \(_{V}\) are an output of a _hyper-network_ taking the global state \(S\) as input. Each hyper-network consists of two fully-connected layers with a ReLU activation. Finally, ELU is employed to mitigate the issue of gradient vanishing and to ensure that negative inputs remain negative. Indeed, ELU is convex and the two mixing networks \(_{_{Q}}\) and \(_{_{V}}\) have non-negative weights, implying that the loss function \(J(,_{Q},_{V})\) is _concave_ in \(\) and the _IGC_ holds.

**Practical Implementation.** Similar to , we use a \(^{2}\)-regularizer \((x)=x+x^{2}\) for the first terms of the loss function in (6). This convex regularizer is useful to ensure that this is lower-bounded even when \(Q_{i}\), for some \(i\), go to \(-\), which is crucial to keep the learning process stable. In addition, instead of directly estimating \(_{S_{0}}[V^{tot}(S_{0})]\), we utilize the following equation to approximate \(_{S_{0}}[V^{tot}_{Q}(S_{0})]\) which can stabilize training:

\[(1-)[V^{tot}_{Q}(S)]=_{(S,A)}V^{tot}_ {Q}(S)-_{S^{} P(|S,A)}[V^{tot}_{Q}(S^{}) ]\]

for any value function \(V^{tot}_{Q}()\) and occupancy measure \(\), we can estimate \(_{S_{0}}[V^{tot}_{Q}(S_{0})]\) by sampling \((S,A)\) from replay buffer and estimate \(_{(S,A,S^{})}[V^{tot}_{Q}(S)- V^{tot}_ {Q}(S^{})]\) instead. In summary, we will employ the following practical loss function:

\[_{,_{R},_{V}}J(,_{R}, _{V}) =_{(S,A)^{E}}Q^{tot}(S,A)-  V^{tot}_{Q}(S^{})\] \[+_{(S,A,S^{})^{replay}}V^{ tot}_{Q}(S)- V^{tot}_{Q}(S^{})}.\] (11)

Fig. 1 shows our network architecture and the details of our MIFQ can be found in the appendix.

## 5 Experiments

**Environments.** We use three environments in which enemy agents' policies are fixed and controlled by a simulator. We collect expert trajectories from well-trained ally agents and build IL to mimic them. The resulting imitating agents are evaluated by letting them play against the simulator's enemies.

_SMACv2 ._ SMAC is a well-known multi-agent environment built based on StarCraft II. We employ SMACv2 , an enhanced version of SMACv1 , that introduces a more formidable environment

Figure 1: An overview of our network architecture.

for evaluating MARL algorithms. In SMACv2, scenarios are procedurally generated, compelling agents to adapt to previously un-encountered situations. This benchmark has \(6\) sub-tasks, including _Protoss_, _Terran_, and _Zerg_ which feature \(5\) to \(10\) agents. These agents have the flexibility to engage with opponents of differing difficulty levels.

_Gold Miner ._ This game originates from a MARL competition, in which two teams, ally and enemy, navigate through a 2D terrain to find gold. A team win if they mined a larger amount of gold than the other. Winning this game is challenging since the allied agents must compete against exceptionally well-developed heuristic-based enemies. We consider three sub-tasks, each involves two ally and two enemy agents: (i) _Easy_: The enemies employ a simple shortest-path strategy to reach the gold deposits; (ii) _Medium_: One enemy agent follows a greedy approach, while the other

    &  &  &  &  &  &  &  &  &  \\  & _5vs5_ & 86.7\% & 19.5\% & 33.6\% & 39.8\% & 47.7\% & 42.6\% & 42.6\% & 64.8\% & **72.7\%** \\ Protoss & _10vs10_ & 90.3\% & 5.5\% & 16.6\% & 38.3\% & 36.8\% & 19.8\% & 28.3\% & 61.7\% & **77.3\%** \\  Terran & _5vs5_ & 81.7\% & 18.0\% & 19.7\% & 32.0\% & 24.6\% & 10.9\% & 10.9\% & 55.9\% & **71.9\%** \\  & _10vs10_ & 81.7\% & 6.2\% & 14.1\% & 35.9\% & 0.5\% & 2.3\% & 1.0\% & 53.8\% & **72.7\%** \\   & _5vs5_ & 73.5\% & 10.9\% & 18.6\% & 33.6\% & 14.8\% & 5.3\% & 18.8\% & 46.7\% & **60.9\%** \\  & _10vs10_ & 76.3\% & 9.4\% & 16.4\% & 17.2\% & 27.1\% & 1.2\% & 31.2\% & 51.0\% & **59.4\%** \\   & _easy_ & 82.4\% & 31.2\% & 21.9\% & 18.8\% & 36.7\% & 35.2\% & 34.6\% & 52.7\% & **60.2\%** \\  & _medium_ & 74.9\% & 28.1\% & 9.8\% & 14.8\% & 28.3\% & 21.1\% & 26.0\% & 43.8\% & **49.2\%** \\  & _hard_ & 69.8\% & 12.5\% & 6.6\% & 11.7\% & 19.9\% & 17.2\% & 20.9\% & 39.1\% & **39.8\%** \\   & _reference_ & -17.3 & **-18.3** & -18.6 & -19.0 & -36.6 & -40.7 & -40.0 & -23.4 & -20.5 \\  & _spread_ & -11.1 & **-20.5** & -23.6 & -21.6 & -23.0 & -26.5 & -24.3 & -23.2 & -24.2 \\  & _speaker_ & -19.4 & -27.5 & -29.1 & -29.5 & -104.3 & -125.5 & -78.7 & -30.8 & **-26.3** \\   

Table 1: Winrate and reward comparisons

Figure 3: Comparison with different numbers of demonstrations. X-axis: winning rate. Y-axis: number of demonstrations.

Figure 2: Learning curves

follows the algorithm developed by the second-ranking team in the competition; and (iii) _Hard_: the enemies consists of the first- and second-ranking teams.

_Multi Particle Environments (MPE) ._ MPE contains multiple communication-oriented deterministic multi-agent environments. We use three cooperative scenarios available in MPE for evaluating, including: (i) _Simple spread:_ three agents learn to avoid collisions while covering all of the landmarks; (ii) _Simple reference:_ two agents learn to get closer to the target landmarks. Each target landmark is known only by the other agents, so all agents have to communicate to each others; and (iii) _Simple speaker listener:_ similar to simple reference, but one agent (speaker) can speak but cannot move, and one agent (listener) can move but cannot speak.

**Expert Demonstrations.** For each task, we trained an expert policy by MAPPO with large-scale hyper-parameters (e.g., multi layers, higher dimensions, longer training steps, and using recurrent neural network, etc). In term of expert buffer collection, we test each method with different numbers of expert trajectories: up to \(128\) trajectories for MPEs and up to \(4096\) trajectories for Miner and SMAC-v2. Note that MPEs are not dynamic environments, so we do not need a large number of expert demonstrations for evaluation. For each collected trajectory, we used a different random seed. For a fair comparison, each method uses the same saved expert demonstrations for the training.

**Baselines.** We compare our MIFQ against other multi-agent IL algorithms, which either originate from the multi-agent IL literature, or be adapted from SOTA single-agent IL algorithms. These baselines include: (i) _Behavior Cloning (BC)_; (ii) _Independent IQ-Learn (IIQ)_ -- this is a simple adaption of IQ-Learn for a multi-agent setting, described in Section 4.1.2; (iii) _IQ-Learn with Value Decomposition Network (IQVDN)_ -- this is another adaptation of IQ-Learn, but instead of using mixing and hyper networks to aggregate agent Q functions, we employ Value Decomposition (VDN) ; (iv) _MASQIL_ -- we adapt SQIL for multi-agent settings . MASQIL shares some similar advantages with our MIFQ, such as being non-adversarial and enabling decentralized learning through centralized learning; (v) _MAGAIL_ -- a multi-agent IL algorithm introduced by ; and finally (vi) _MAAIRL_ -- this algorithm is proposed by . Moreover, since our MIFQ algorithm relies on soft policies while most previous factorized Q-learning algorithms [29; 35; 39] use deterministic policies, we include a deterministic-policy version of MIFQ for comparison purposes. In this version, the deterministic optimal local actions are determined as \(a_{i}^{*}=*{argmax}_{a_{i}}\{Q_{i}(o_{i},a_{i})\}\), and the local V functions are \(V_{i}^{Q}(o_{i})=_{a_{i}}Q_{i}(o_{i},a_{i})\). We denote our main MIFQ algorithm as **MIFQ (Soft)**, and the deterministic-policy version as **MIFQ (Det)**.

**Comparison Results.** We train each task with our algorithms and the different baselines, using \(128\) trajectories for MPE, and \(4096\) trajectories for SMACv2 and Miner. As a standard practice, we report winrates for SMACv2 & Gold Miner, and reward scores for MPEs. Figure 2 shows the learning curves and Table 1 reports the final winrates and average scores (the full table with std values can be found in appendix). Our methods MIFQ (_Soft_) significantly outperforms all other baselines on SMACv2 & Gold Miner tasks, and has competitive performance on MPEs, with a note that MPEs is a deterministic environment and its tasks are considerably easier than SMACv2 and Gold Miner. We also observe that MIFQ (_Det_) significantly outperforms other baselines, but remarkably worse than MIFQ (_Soft_), indicating the advantage of using soft-policy learning in our context. Moreover, to evaluate the efficiency of our method with different numbers of expert demonstrations, we compare our MIFQ with the baselines on two dynamic tasks: _SMACv2 & Miner_. Figure 3 shows box and whisker plots of the average winning rate of each method on each task for data summarising analysis. It shows that MIFQ (_Det_ and _Soft_) offer higher winrates, and it converges better with smaller standard errors. Moreover, to evaluate how the convexity help enhance our algorithm, we compares MIFQ with those that rely on non-convex mixing networks, which are obtained by replacing the convex activation ELU with non-convex ones such as _sigmoid_ or _tanh_. More details can be found in the appendix.

## 6 Conclusion and Limitations

**Conclusion.** We developed a multi-agent IL algorithm based on the inversion of soft-Q functions. By employing mixing and hyper-network architectures, our algorithm, MIFQ, is non-adversarial and enables the CTDE approach. We demonstrated that, with some commonly used two-layer mixing network structures, our IL loss function is convex within the Q-function space, making learning convenient. Extensive experiments conducted across several challenging multi-agent tasks demonstrate the superiority of our algorithm compared to existing IL approaches.

**Limitations.** Some limitations of the work includes: (i) while MIFQ achieves impressive performance across various tasks, it falls short of reaching the expertise levels (increasing the number of expert demonstrations could solve the issue, but it would introduce additional computational challenges, as the replay buffer will become too large and the training process will be very costly); (ii) MIFQ, along with other baseline methods, struggles when confronted with very large-scale tasks, such as some of the largest games in SMACv2; and (iii) our multi-agent IL (and other baselines) still requires a large amount of (clean) expert demonstrations, which would be not always available in practice. These limitations could pave the way for future work.

**Broader Impacts.** Our research focuses on imitation learning in multi-agent systems, it may have potential applications similar to areas where imitation learning has been impactful, such as autonomous driving, healthcare, and game theory. There are also potential negative impacts. For instance, imitation learning could be used for surveillance purposes, following and monitoring individuals in public spaces, or for developing autonomous weapons.