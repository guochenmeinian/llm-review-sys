# Rethinking Decoders for Transformer-based Semantic Segmentation: A Compression Perspective

Qishuai Wen and Chun-Guang Li\({}^{*}\)

School of Artificial Intelligence

Beijing University of Posts and Telecommunications, Beijing 100876, P.R. China

{wqs,lichuguang}@bupt.edu.cn

###### Abstract

State-of-the-art methods for Transformer-based semantic segmentation typically adopt Transformer decoders that are used to extract additional embeddings from image embeddings via cross-attention, refine either or both types of embeddings via self-attention, and project image embeddings onto the additional embeddings via dot-product. Despite their remarkable success, these empirical designs still lack theoretical justifications or interpretations, thus hindering potentially principled improvements. In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA). From such a perspective, we derive a white-box, fully attentional DEcoder for PrIncelied semanticC segementation (DEPICT), with the interpretations as follows: 1) the self-attention operator refines image embeddings to construct an ideal principal subspace that aligns with the supervision and retains most information; 2) the cross-attention operator seeks to find a low-rank approximation of the refined image embeddings, which is expected to be a set of orthonormal bases of the principal subspace and corresponds to the predefined classes; 3) the dot-product operation yields compact representation for image embeddings as segmentation masks. Experiments conducted on dataset ADE20K find that DEPICT consistently outperforms its black-box counterpart, Segmenter, and it is light weight and more robust. Our code and models are available at https://github.com/QishuaiWen/DEPICT.

## 1 Introduction

Semantic segmentation has been a fundamental task in computer vision for decades. In the supervised setting, the task aims to segment an image into regions corresponding to different predefined classes. The dominant approaches for semantic segmentation have experienced significant shifts, in particular, from hand-crafted features  to deep learning, from Convolutional Neural Networks (CNNs)  to Vision Transformers (ViT) , and then from per-pixel classification to mask classification . Recently, state-of-the-art methods  for Transformer-based semantic segmentation  typically adopted the Transformer decoders inspired by DETR .

Although they vary among different methods, the Transformer decoders typically consist of cross-attention operators that extract additional embeddings (known as class embeddings  or mask embeddings ) from image embeddings, self-attention operators that refine either or both the additional embeddings and image embeddings, layer normalization (LN)  and feedforward neural networks (FFN), which are the default compositions of a Transformer block , and one (or two for mask classification) dot-product operation of the two types of embeddings. Here, we illustrate in Figure 1 two representative methods, i.e., Segmenter  and MaskFormer .

Despite the empirical designs of the Transformer decoders being intuitive and having achieved remarkable success [7; 20], there still lack theoretical justifications or interpretations, thus hindering potentially principled improvement (such as identifying and addressing performance bottlenecks). We believe that the first step toward white-box models for Transformer-based semantic segmentation is to answer the following questions: 1) Why do the Transformer decoders outperform a position-wise MultiLayer Perceptron (MLP) that independently classifies each image embedding ? 2) What is the underlying mechanism of the self- and cross-attention operators adopted by the Transformer decoders? 3) More importantly, is there a principle for designing and improving the Transformer decoders?

In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA), and that the principle of compression is all we need to derive a white-box decoder akin to the Transformer decoder. To be specific, we extend the objectives of PCA in the geometric and rank minimization views  to the context of the coding rate [11; 25; 43] and, following the derivation in CRATE , we derive self- and cross-attention operators for semantic segmentation by unrolling the optimization of these objectives.

**Contributions.** The contributions of the paper are highlighted as follows.

1. We take a further step along the fundamental connections between semantic segmentation and compression, by introducing PCA for understanding the empirical designs of decoders for Transformer-based semantic segmentation.
2. We extend the objectives as well as the idea of PCA in terms of the coding rate to unrolled optimization, and thus derive a family of white-box fully attentional DEcoder for PrIncipled semantiC segmentTation (DEPICT).
3. We conduct extensive experiments to evaluate the performance of our DEPICT and find that our DEPICT consistently outperforms its black-box counterpart and shows desirable properties suggested by the derivation.

## 2 Related Work

### Interpretability of Decoders for Semantic Segmentation

We cast decoders for semantic segmentation into four categories: 1) convolutional decoders [24; 33; 5; 46]; 2) MLP-based decoders [35; 39; 6; 26]; 3) Transformer decoders [35; 8; 7; 45]; 4) clustering decoders [49; 41; 50; 23; 17]. The core operations of convolutional decoders are convolution and pooling. Despite rooting in signal processing, the interpretability of the learned convolutional filters is limited. Among these decoders, segmentation masks are also features. And, as a self-attention layer can express any convolutional layer , the self-attention operator we derive is very likely to be decomposed into convolutional filters. Due its powerful fitting ability, an MLP-based

Figure 1: **Illustration for Segmenter and MaskFormer.** a) Segmenter. b) MaskFormer. c) Transformer block adopted by Segmenter. We omit the details of the Transformer decoder adopted by MaskFormer, which refines image embeddings and mask embeddings via self-attention respectively before the cross-attention operations.

decoder is quite difficult to interpret. Nonetheless, a single linear layer which is used by pre-trained models [15; 18; 3; 28] as the classifier can be viewed as a parametric softmax projection, which learns a prototype for each class .

To our knowledge, Segmenter  proposed the first Transformer decoder, which is used to perform per-patch classification; whereas MaskFormer  formulated the task of mask classification, which performs clustering at first and then classifies the clusters. As a much broader concept, clustering decoders include all methods that are explicitly based on the idea of clustering. In particular,  reformulates cross attention as a clustering process; whereas  exploits the principal directions for segmentation, based on the relationship between PCA and \(k\)-means . In this paper, we instead take a compression perspective, which is more fundamental than clustering.

### White-Box Models based on Coding Rate

The coding rate  is an effective criterion for compression and is first introduced for segmentation by . And a solid justification for the connections between image segmentation and compression can be traced back to , which argues that the optimal segmentation of an image is the one that will give the shortest coding length for encoding the image. Then, a principled framework, termed Maximal Coding Rate Reduction (\(^{2}\)) , was proposed to learn discriminative and diverse representations, in which \(^{2}\) maximizes the difference between the coding rate of the ambient space and the sum of the coding rate for each class-specific individual subspace and is shown to promote representations to lie in a union of orthogonal subspaces.

By unrolling the gradient-based optimization procedure of \(^{2}\), a deep architecture, termed ReduNet  is derived as a white-box counterpart to the black-box of ResNets and CNNs, followed by CRATE  for ViTs  and CRATE-MAE  for masked autoencoders . Intriguingly, CRATE shows a segmentation emergence similar to DINO [3; 44], which is mainly attributed to the Multi-head Subspace Self-Attention (MSSA) operator derived by . These works have laid a solid foundation for further investigation of the connections among compression, representation learning, and vision tasks at varying granularity. Especially, the role of normalization, e.g., ensuring the effectiveness of the coding rate, has been discussed in [1; 4]. As an analog, we adopt LayerNorm to normalize the scale of image embeddings before all attention operators that we derive.

## 3 Our Methods

### Notations and Preliminaries

Given an arbitrary image for semantic segmentation, we use \(=[_{1},,_{N}]^{D N}\) to denote a set of image embeddings, where each image embedding \(_{i}^{D}\) represents one of the \(N\) regular non-overlapping patches to which the image is split. Specifically, we assume that \(\) is zero mean, i.e., \(=\), where \(^{N}\) is the vector of \(1\)'s. For a Transformer decoder, we use \(_{0}\) to denote the input, which is actually the output of the ViT backbone, and \(_{}\) is for \(_{0}\) after being updated \(\) times, where \( L_{1}\). We use \(=[_{1},,_{K}]^{D K}\) to denote the additional embeddings (or queries, or more precisely, cluster centroids), \(_{0}\) for their initialization, \(_{}\) for \(_{0}\) after being updated \(\) times, where \( L_{2}\), and \(\) for an identity matrix with proper dimension. We specially set \(K\) equal to the number of predefined classes, \(C\), thus referring to the additional embeddings as class embeddings, or classifiers instead. A generalized case will be discussed in Appendix A.3.

For PCA, what we concern about are the leading \(C\) principal directions of \(\) and the associated \(C\)-dimensional principal subspace, which is denoted as \(\). For convenience, we simply refer to them as the principal directions and the principal subspace. And we use \(=[_{1},,_{C}]^{D C}\) to denote an arbitrary set of orthonormal bases of \(\). Notably, we introduce PCA from two different perspectives here and will extend them to the context of coding rate in the following sections. From a geometric perspective, PCA minimizes the squared reconstruction error when recovering \(\), i.e.,

\[_{,}\|-\|_{F}^{2},^{}=_{C}.\] (1)

From a rank minimization perspective, it seeks a low-rank approximation of \(\), i.e.,

\[_{}\|-\|_{F}^{2},()=C.\] (2)Finally, we introduce the concept of the coding rate of \(\) subject to a certain distortion \(>0\), which is calculated as follows:

\[R()(_{D}+}^{}).\] (3)

### Bridging the Transformer Decoders and PCA

For classification tasks, such as semantic segmentation, on the scale of an entire dataset, it is desirable to have a Linear Discriminative Representation (LDR), which can be well modeled by a union of orthogonal subspaces . However, it is difficult to explicitly model such a desirable structure when \(C\) is relatively large, even at the cost of sacrificing the diversity of each class (i.e., the dimensions of each subspace). Fortunately, we notice that a Transformer decoder segments each image independently; that is, an embedding of one image would never attend to or interact with the embeddings from a different image. Additionally, despite that the intra-class variance or diversity is very rich for semantic segmentation on entire dataset, it would be limited within a single image. Therefore, we focus on one single image for now and then generalize it in Section 3.5.

As shown in Figure 1, the Transformer decoders for semantic segmentation typically project image embeddings onto additional embeddings to predict masks. In the case we focus on, it is projected onto the \(C\)-dimensional subspace spanned by class embeddings, where \(C\) is much smaller than \(D\). Therefore, we can view semantic segmentation as a process of dimension reduction, i.e., compression, where the masks \(=^{}^{C N}\) represent more compact features compared to \(\). Intuitively, the subspace is expected to contain as much information as possible, which is crucial for capturing the rich intra-class variance for semantic segmentation. Meanwhile, it is also desirable for the class embeddings to be orthonormal for more discriminative classification . In other words, we seek to find a low-dimensional subspace that best fits the image embeddings, which is exactly the idea behind PCA, and to find a set of orthonormal bases of it as classifiers.

It is not difficult to show that the problem in (1) is equivalent to:

\[_{}(^{}^{}): =_{c=1}^{C}(_{c}^{}), ^{}=_{C},\] (4)

which indicates that the principal subspace should retain most variance (i.e., information), and the variances after projection onto the principal directions are maximized. Therefore, we contend that the principal subspace and the subspace spanned by class embeddings, as well as the principal directions and class embeddings, are fundamentally related; in an ideal case, they are equivalent. In Figure 2, we visualize several images segmented using PCA and find that PCA can indeed serve as an effective method, validating the above analysis.

In contrast to the Transformer decoders, a single linear layer can be viewed as performing PCA on the entire dataset, at which scale the image embeddings definitely cannot be well fitted by a \(C\)-dimensional subspace (i.e., performing a bad compression). Meanwhile, as the intra-class variance in semantic segmentation is richer than in image classification, a single static classifier (or prototype,

Figure 2: **Image Segmentation via PCA and DEPICT.** Given an image, we segment it via PCA and our DEPICT. We perform PCA on its representations \(_{0}\) and \(_{L_{1}}\), respectively, where the first 10 principal directions are used as cluster centroids. We find that PCA can serve as an effective method for image segmentation especially on the refined features, like \(_{L_{1}}\). We also observe that performing PCA on \(_{0}\) is more likely to lead to an over-segmentation, which indicates that its principal subspace is not ideal.

or more precisely, basis) is not sufficient to represent a class well. This is why a single linear layer clearly lags behind more complex methods in fine-grained classification tasks.

### Constructing an Ideal Principal Subspace via Self-Attention

Despite PCA being an effective method, which is also observed in , we find that performing it directly on \(_{0}\) typically yields inferior segmentation results, such as oversegmentation, as shown in Figure 2. We attribute the reason for this to an less ideal principal subspace of \(_{0}\), which means that the relevant information for supervised semantic segmentation is either not contained in it or not significant enough. This hypothesis is intuitive since \(_{0}\) is expected to be generic features and requires refinement to align with specific supervision. To this end, we propose to refine it to construct an ideal principal subspace. As shown in Figure 2, the refinement can in fact alleviate the over-segmentation and improve the segmentation results.

Specifically, we assume that \(=[_{1},,_{C}]^{D C}\) is a set of orthonormal bases of an ideal subspace. Now we optimize the objective of (4) with respect to (w.r.t.) \(\) to ensure that \(\) is the principal subspace of \(\) after refinement, that is,

\[_{}_{c=1}^{C}(_{c}^{}),\] (5)

where \(\) is learned through backpropagation during training. However, instead of optimizing \(\) in (5) directly, we choose to maximize the projected coding rate onto these bases, that is,

\[_{}_{c=1}^{C}R(_{c}^{}):=_{c=1}^{C} {2}(1+}_{c}^{}^{}_{c}).\] (6)

This is because the coding rate is a more generalized metric and is effective in high-dimensional spaces. Still, it is equivalent to (5) (as \(_{c}^{}^{}_{c}\) is a non-negative scalar). According to the seminal work , optimizing problem (6) via a gradient step with a learnable step size \(>0\) derives the MSSA operator, which in our case can be written as:

\[^{+1}=(1+})^{}-}(^{}),\] (7)

Figure 3: **Illustration for DEPICT.** Given an image for semantic segmentation, we represent it as \(_{0}\) by the ViT backbone. Segmenting it by performing PCA on \(_{0}\), we find that \(\) of \(_{0}\) is not ideal. We thus adopt the MSSA operator to refine the image embeddings, iteratively constructing an ideal \(\). Performing PCA again on \(_{L_{1}}\), we find that the segmentation results are improved. Then, we adopt the MSCA operator to find a low-rank approximation of \(_{L_{1}}\) that lies in \(\) as classifiers. For example, we use the dogs and cats on the right to represent image embeddings of two different classes in the feature space. Initially, the projections of dogs and cats onto \(\) are not well linearly separable. DEPICT, however, constructs an ideal \(\) and effectively classify them.

where the MSSA operator is defined as follows:

\[() }[_{1},,_{C}] [(_{1})\\ \\ (_{C})],\] (8) \[(_{c}) (_{c}^{})((_ {c}^{})^{}(_{c}^{})),\ \ \ \ \ c\{1,,C\}.\] (9)

In (8), however, MSSA demands \(C\) attention heads, each of which calculates an attention matrix in the shape of \(N N\). As \(C\) is always much larger than the number of heads of Multi-Head Self-Attention (MHSA) , which is the black-box counterpart of the MSSA operator, thus the computation of (8) occupies an unacceptable amount of GPU memory.

To remedy this issue, we propose to maximize a lower bound of the sum of the projected coding rate onto the bases in groups, rather than maximizing them directly one by one. By our proof in Appendix D.1, the sum of the projected coding rate onto a set of bases can be bounded below by the projected coding rate onto the subspace they span, i.e.,

\[_{m=1}^{M}R(_{m}^{}) R(^{} )-,\] (10)

where \(^{}=[_{1}^{},,_{M}^{}] ^{D M}\) consists of \(M\) different bases in \(\), and \(\) is a product of \(M(M-1)\) and a constant w.r.t. \(\). We thus divide the columns of \(\) into \(H=C/M\) non-overlapping groups, denoted as \(_{1}^{},,_{H}^{}\), and maximize the lower bound for each group, respectively. We thus reformulate the MSSA operator in our case as:

\[^{+1} =(1+})^{}-}(^{}),\] (11) \[() }[_{1}^{},, {P}_{H}^{}][(_{1}^{ })\\ \\ (_{H}^{})],\] (12) \[(_{h}^{}) (_{h}^{})(( _{h}^{})^{}(_{h}^{})), \ \ \ \ \ h\{1,,H\}.\] (13)

So far, we have derived a self-attention operator (12) which takes a gradient step toward constructing an ideal principal subspace. In Appendix A.2, we discuss the differences among MHSA, the original MSSA, and our modified MSSA in (12). Additionally, our goal can also be achieved by minimizing the projected coding rate onto the bases of the orthogonal complement of the ideal principal subspace, with the only change being to set \(<0\). Therefore, we do not constrain the sign of \(\) in our implementation.

### Finding a Low-Rank Approximation via Cross-Attention

With the ideal principal subspace learned via back-propagation and constructed via self-attention, the remaining problem is to find a set of classifiers (i.e., class embeddings) for semantic segmentation. As discussed in Section 3.2, the principal directions would be a desirable choice. However, as shown in Figure 2, PCA still yields inferior segmentation results compared to parametric and learnable methods. We attribute this issue to the principal directions not being flexible to align with supervision, despite the fact that the constructed principal subspace has been. Meanwhile, being an unsupervised method, PCA requires an additional and challenging step that assigns each principal direction a predefined class. To this end, we derive an operator to extract a set of classifiers that satisfy the requirements as follows: 1) learns to align with supervision; 2) span the principal subspace; 3) effectively abstract or represent the image embeddings.

For the last requirement, we propose to optimize an objective that seeks to find a low-rank approximation of \(\) in terms of the coding rate, i.e.,

\[_{}}|R()-R(})|,\ \ \ \ \ \ (})=C,\] (14)

where \(}^{D N}\). This is inspired by the rank minimization perspective of PCA in (2), and the objective of \(k\)-means which can be written as

\[_{}\|-}\|_{F}^{2}\ \ \ \ \ \ ( })=C,\] (15)where the columns of \(}^{D N}\) consist of cluster centroids. Using the coding rate to measure the approximation, the objective (14) is more generalized than (2) and (15). In Appendix D.2, we prove that both \(k\)-means cluster centroids and the principal directions are reasonably good (or even the optimal) solutions under certain conditions, for (14).

Intuitively, since that the columns of \(}\) span a subspace with which dimension is lower than that of \(\), we have \(R() R(})\), which will be validated by the experiments in the Appendix C. Thus, we equivalently maximize the coding rate of \(}\) and derive that:

\[}^{+1}=(1+})}^ {}-(})^{2}}^{}( }^{}}^{}),\] (16)

which is similar to (12). Note that compared to using the Frobenius norm in (2) and (15), the approximation in (14) in terms of the coding rate is relatively loose due to its invariant property , thus optimizing over \(}\) turns out to be irrelevant to \(\). Therefore, we replace some \(}\) in (16) with \(\) to further encourage \(}\) to approximate \(\), and thus we have:

\[}^{+1}=(1+})} ^{}-(})^{2}^{}(^{}}^{}).\] (17)

Rather than \(}\) which is redundant, what we are indeed concerned with is \(\). Thus, we simplify (17) by updating each \(_{i}\) only once, i.e.,

\[^{+1}=(1+})^{}-(})^{2}^{}(^{}^{ }).\] (18)

For the first requirement, we set \(_{0}\) by learnable parameters; in other words, the alignment to predefined classes is learned by adjusting the starting point of gradient optimization. For the second requirement, we confine the updates of \(\) to the principal subspace of \(\) by adding projections onto \(\). Similarly to (12), we reformulate (18) as:

\[^{+1} =(1+})^{}-}(^{},),\] (19) \[(,) }[_{1}^{},, _{H}^{}][(,_{1}^{})\\ \\ (,_{H}^{})],\] (20) \[(,_{h}^{}) (_{h}^{})(( _{h}^{})^{}(_{h}^{})), \ h\{1,,H\},\] (21)

which we refer to as Multihead Subspace Cross-Attention (MSCA).

### Decoder for Principled Semantic Segmentation

From a single image to the entire dataset, we should consider multiple "principal subspaces"  and thus allow \(\) to model more bases by raising the number of columns of \(\) from \(C\) to a hyperparameter \(K\) that is larger than \(C\). We expect that the principal subspace, as well as the class embeddings, of different images vary, but the class embeddings of the same class lie in a low-dimensional subspace, and all the subspaces of class are orthogonal, thus satisfying our anticipation for LDR on the entire dataset.

By stacking and combining the two steps of Section 3.3 and 3.4, we have a fully attentional white-box decoder as shown in Figure 3, which iteratively constructs an ideal principal subspace by refining image embeddings via the self-attention operators, and then find a low-rank approximation of the refined image embeddings that lies in the principal subspace and corresponds to predefined classes via the cross-attention operators. As our derivations demonstrate that the principle of compression is all we need for designing the decoders, we refer to our approach as the DEcoder for PrIncipled semantiC segmenTation (DEPICT), and our approach described above that extracts additional embeddings via cross-attention is referred to as DEPICT-CA.

We note that the additional embeddings can also be extracted by self-attention [15; 35; 12]; that is, concatenating the two types of embeddings and updating them simultaneously via self-attention alone, as shown in Figure 1 a). In Appendix D.3, we prove that it implicitly performs cross-attention, thus it can be interpreted by our derivations in Section 3.4. We implement a simpler variant of DEPICT that extracts the additional embeddings via self-attention and refer to it as DEPICT-SA.

[MISSING_PAGE_FAIL:8]

particular, the best-performing variant, DEPICT-SA based on ViT-Large with an input resolution of 640\(\)640, surpasses Segmenter by 1.1/0.7 mIoU for single/multi-scale inference, while using only 1/14 of the parameters and 1/3 of the FLOPs. We mainly attribute the efficiency of DEPICT to its removal of the FFN block, which plays no role in our interpretations. Such a redundancy has been observed empirically in the field of NLP . but our ablation study in Appendix C shows that naively adopting MSSA while removing the FFN from Segmenter results in worse performance. Moreover,  proves that pure attention causes the rank of tokens to decrease rapidly with depth, which is desirable for us to construct an ideal principal subspace. In particular, according to , both a finer segmentation granularity and a more performant backbone can significantly boost segmentation performance, partly explaining the inferiority of DEPICT compared to MaskFormer. In Table 2, we find that PICT performs slightly worse than Segmenter. We believe that this is due to the limited size of the dataset; the training set sizes of ADE20K, Cityscapes, and Pascal Context are about 20K, 3K and 5K, respectively. As Table 3 shows, DEPICT requires a sufficient amount of data to demonstrate its superiority.

### Desirable Properties of DEPICT

**Orthogonal properties.** According to our interpretations, the parameter matrices of attention are consisted of orthonormal bases, and the extracted class embeddings are fundamentally related to the principal directions, which are also orthonormal. As shown in Figure 4, both \(\) and \(\) are very close to being orthonormal in DEPICT. Although the parameter vectors learned by Segmenter are also nearly orthogonal, their norms are too small. Despite there are semantic similarities among the predefined classes, the class embeddings of Segmenter are excessively related. Moreover, as we

Figure 4: **Investigating orthogonality in DEPICT.**_Left_: \(^{}\); _Right_: \(^{}\). All variants are based on ViT-L. Since that the MHSA operator contains three parameter matrices, unlike MSSA which has only one, we visualize the matrix responsible for transforming queries. Notably, all the \(\)’s are normalized, whereas \(\) is not.

Figure 5: **Inner product of class embeddings across images. We group the class embeddings by their classes and visualize the inner-product among them. We exemplify 30 classes across 100 images. All variants are based on ViT-L.**

expected in Section 3.5 and shown in Figure 5, the class embeddings of different classes nearly lie in a union of orthogonal subspaces, and thus the image embeddings are very likely to satisfy assumption for LDR.

**Measuring the coding rate and robustness.** On the ViT-L variant of DEPICT-SA with an input resolution of 512\(\)512, we measure the projected coding rate of image embeddings onto the subspaces spanned by \(_{h}^{}\) across layers, as shown in Figure 6. We find that the sign of the learned step size distinguishes two types of subspaces (distinguished by red and blue regions in the figure): one onto which the projected coding rate increases and the other onto which it decreases, which is consistent with our derivations. Furthermore, we measure the robustness of DEPICT under four types of parameter perturbations and find that DEPICT is surprisingly robust whereas Segmenter collapses, as shown in Table 4. We attribute the robustness of DEPICT to its awareness of and modeling for low-dimensional subspaces, which are not significantly altered under the parameter perturbations. Actually, the seminal work  has already noticed that it is the subspace, rather than individual units, that contains the semantic information in the high layers of neural networks. Our work demonstrates that this intriguing property can be strengthened by improving model designs, which yields better robustness.

## 5 Conclusion and Future Work

We proposed a compression perspective to view the Transformer decoders widely adopted in Transformer-based semantic segmentation, where we expect that class embeddings are actually the principal directions and thus they span the principal subspace and segment images via PCA. In experiments, we found that the principal subspace of the generic features extracted by the encoder is not ideal and that the principal directions are not flexible enough to align well with predefined classes. To this end, we extended the objectives of PCA to construct an ideal principal subspace and to find a low-rank approximation of image embeddings as classifiers. By unrolling the optimization procedure of these objectives, we derived a family of fully attentional white-box decoders, called DEPICT, providing theoretical interpretations for the empirical designs of the Transformer decoders. Experiments conducted on ADE20K have shown that DEPICT consistently outperforms its black-box counterpart, Segmenter, using significantly fewer parameters and FLOPs. We further validated the effectiveness of DEPICT on Cityscapes and Pascal Context datasets and investigated that DEPICT possesses desirable properties, such as orthogonality and robustness, as we expected and derived.

We believe that our work serves as a promising first step toward developing a comprehensive interpretation framework for Transformer-based semantic segmentation, and further efforts are needed to contribute to this goal. Focusing on interpretability, we use a relatively simple implementation for DEPICT from architecture designing to training tricks, compared to state-of-the-art methods. Whether the improvements for black-box models, such as hierarchical transformer encoders, pixel decoders, the mask classification formulation, and masked attention, are compatible with DEPICT remains an open question to explore.

    &  \\  & ViT-S & ViT-B & ViT-L \\  \(_{h}^{}_{M}\) & 46.7(+1.4) & 49.2(+0.7) & 52.9(+1.1) \\ \(_{K}\) & 42.4(+27.8) & 47.3(+46.1) & 52.0(+51.4) \\ \((_{h}^{})\) & 42.6(+27.6) & 45.7(+44.5) & 51.8(+51.2) \\   

Table 4: **Investigating robustness of DEPICT under parameter perturbation.** We experiment with four types of perturbations: 1) the \(\) of each attention operator undergoes a random orthogonal transformation, \(_{K}^{K K}\); 2) the \(_{h}^{}\) of each head undergoes a random orthogonal transformation, \(_{M}^{M M}\); 3) orthogonalizing \(_{h}^{}\); 4) adding random noise from a Gaussian distribution with zero mean and variance \(\) for each parameter independently. The baseline is Segmenter, and we mark the improvements in the table.