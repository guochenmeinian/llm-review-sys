# Rethinking Open-set Noise in Learning with Noisy Labels

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

To reduce reliance on labelled data, learning with noisy labels (LNL) has gained increasing attention. However, prevailing works typically assume that such datasets are primarily affected by closed-set noise (where the true/clean labels of noisy samples come from another known category), and ignore therefore the ubiquitous presence of open-set noise (where the true/clean labels of noisy samples may not belong to any known category). In this paper, we formally refine the LNL problem setting considering the presence of open-set noise. We theoretically analyze and compare the effects of open-set noise and closed-set noise, as well as the effects between different open-set noise modes. We also analyze common open-set noise detection mechanisms based on prediction entropy values. To empirically validate the theoretical results, we construct two open-set noisy datasets - CIFAR100-O/ImageNet-O and introduce a novel open-set test set for the widely used WebVision benchmark. Our work suggests that open-set noise exhibits qualitatively and quantitatively distinct characteristics, and how to fairly and comprehensively evaluate models in this condition requires more exploration.

## 1 Introduction

In recent years, the tremendous success of machine learning often relies on the assumption that data labels are accurate and free from noise. However, in real-world scenarios, label noise caused by factors such as annotation errors and label ambiguity is ubiquitous, posing a pervasive challenge to the performance and generalization of models. To address this challenge, various methods have been proposed to learn with noisy labels, including noise transition matrix [7; 23], label correction [17; 3], robust loss functions [6; 29; 19], and recently dominant sample selection-based approaches [11; 2].

Most current efforts, however, primarily focus on closed-set noise, where the true labels of noisy samples belong to another known class. This includes common noise models like symmetric noise (assuming that the labels of samples are randomly flipped with a certain probability to any other known classes) or asymmetric noise model (assuming that the probability of label confusion is influenced by the classes, such as 'cat' being more likely to be confused with 'dog' than with 'airplane'). Recent advancements have also explored instance-dependent noise models [4; 26], where label confusion depends directly on individual instances.

Unfortunately, unlike the in-depth exploration of closed-set noise, there is noticeably limited research on open-set noise, where the true labels of noisy samples may not belong to any known category. This gap becomes particularly crucial when considering one of the primary motivations for learning with noisy labels: learning with datasets obtained through web crawling. Examining one of the most commonly used benchmarks - the WebVision dataset , we validate the prevalence of open-set noise (fig. 1). In fact, the 'open-world' assumption involving open-set samples has received more attention in other weakly supervised learning problems, such as open-set recognition and outlierdetection, but lacks enough exploration in the context of LNL. To this end, we focus on a thorough theoretical analysis of open-set noise in this paper. Specifically:

* Considering the presence of open-set noise, we introduce the concept of a complete noise transition matrix and reformulate the LNL problem and label noise definition in this context.
* To enable offline analysis, we consider two pragmatic cases: _fitted case_, that the model perfectly fits the noisy distribution, and _memorized case_, that the model completely memorises the noisy labels.
* We analyze and compare the open-set noise _vs._ closed-set noise on closed-set classification accuracy and suggest that open-set noise has a less negative impact in both cases. We also analyze and compare the 'hard' open-set noise _vs._ 'easy' open-set noise, but find that these two different noise modes show opposite trends in two different cases.
* Since closed-set classification evaluation may be insufficient to fully reflect model performance, we consider introducing an additional open-set detection task and conduct preliminary experiments.
* We derive and analyze the open-set noise detection mechanism based on the entropy values of model predictions and suggest that it may be effective only for 'easy' open-set noise. We also consider two representative LNL methods and combine them with such open-set noise detection mechanism for further experiments.
* For controlled experiments, we construct two novel synthetic open-set noise datasets: CIFAR100-O and ImageNet-O. Additionally, we introduce a new open-set test set to the WebVision dataset for the open-set detection task.

## 2 Related works

Methods for learning with noisy labels can be roughly categorized into two main directions. The first direction typically focuses on estimating noise transition matrix [4; 26; 23; 7] or designing robust loss functions [29; 19; 6], aiming to achieve theoretically risk-consistent or probabilistic-consistent models. However, most of these works often assume an ideal scenario where the model can learn to fit the sampled distribution well, overlooking the over-fitting issues arising from excessive model capacity and insufficient data in practical situations. _In this paper, we introduce the concept of complete noise transition matrix considering the presence of open-set noise and conduct theoretical analyses and experimental validations for both ideal case and over-fitting case, namely fitted case and memorized case_. The second type is often based on sample selection strategies, involving also different regularization terms and off-the-shelf techniques such as semi-supervised learning and model co-training, to achieve the state-of-the-art performance. Most sample selection methods are based on the model's current predictions, such as the popular'small loss' mechanism [2; 11; 8; 28; 10; 17; 13; 27; 24; 30], or model's feature space [21; 22; 15; 5].

Especially, the investigation on open-set noise is relatively scarce. Wang et al.  utilize Local Outlier Factor algorithm to identify open-set noise in feature space, Wu et al.  propose to identify open-set noise with subgraph connectivity, while both Sachdeva et al.  and Albert et al.  try to identify open-set noise based on entropy-related dynamics. Instead, Feng et al.  do not identify open-set noise explicitly while avoid relabelling and including open-set noise in the training. More closely related to our work, Xia et al.  also investigates noise transition matrices involving open-set noise but considering all open-set noise belonging to a single meta-class. In this paper, we consider

Figure 1: Example images of class “Tench” from WebVision dataset. Clean samples are marked in extcolorgreenGreen, closed-set noise is marked in Blue and open-set noise is marked in Red. See appendix F for more details.

that open-set noise may originate from different classes, and based on this premise, we analyze two distinct open-set noise modes. Wei et al.  propose leveraging open-set noise to mitigate the impact of closed-set noise, as it helps alleviating the model's over-fitting tendency. Instead, we focus on a thorough theoretical analysis of the effects with different noise modes, including open-set noise versus closed-set noise, and different open-set noise versus each other.

## 3 Methodology

In section 3.1, we briefly introduce the traditional problem formulation of LNL. In section 3.2, we reformulate the LNL problem considering open-set noise. In section 3.3, we formalize how label noise influences model generalization, particularly, on the proposed error rate inflation metric. In section 3.4, we analyze and compare the impact of open-set _vs._ closed-set noise, as well as 'easy' open-set noise _vs._ 'hard' open-set noise. In section 3.5, we scrutinize the open-set noise detection mechanism based on model prediction entropy values.

### Traditional formulation of LNL

Supervised classification learning typically assumes that we sample a certain number of independently and identically distributed training samples \(\{_{k},y_{k}\}_{k=1}^{K}\) from a joint distribution \(P(,;^{in})\), i.e., the so-called train set. By default, here all the possible values for \(y_{k}\) in the discrete label space \(^{in}:\{1,2,...,A\}\) (referred here as _inlier classes_), are known in advance. With a certain loss function, given the train set \(\{_{k},y_{k}\}_{k=1}^{K}\) we aim to train a model \(f: y\) whose predictions can achieve the minimum error rate under the whole clean distribution \(P(,;^{in})\).

Under LNL problem setting, we believe that the joint distribution \(P(,;^{in})\) has been perturbed to \(P^{n}(,;^{in})\); especially, the conditional distribution \(P^{n}(|;^{in})\) changes -- normally we assume the sampling prior is free of the label noise (\(P(;^{in})=P^{n}(; ^{in})\)), leading to the presence of noisy labels \(y_{k}^{n}\) in the noisy train set \(\{_{k},y_{k}^{n}\}_{k=1}^{K}\) that do not conform to the clean conditional distribution \(P(|;^{in})\).

### Revisiting LNL considering open-set noise

We here formally revisit the problem formulation of learning with noisy labels considering the existence of open-set noise. Instead of assuming all the possible classes are known (\(^{in}\)), we consider samples from some unknown outlier classes may also exist in the train set. Let us denote these classes as _outlier classes_\(^{out}:\{A+1,A+2,...,A+B\}\) with \(B\) as the number of possible outlier classes. Then, we expand the support of joint distribution to contain both inlier and outlier classes, denoted as \(P(,;^{in}^{out})\) and \(P^{n}(,;^{in}^{out})\) for the clean and noisy ones, respectively. For brevity, we denote as \(^{all}^{in}^{out}\). Similarly as above, we still assume the noisy labelling will not affect the sampling prior (\(P(;^{all})=P^{n}(; ^{all})\)). For subsequent analysis, we first define below complete noise transition matrix:

**Definition 3.1** (Complete noise transition matrix).: For a specific sample \(\), we define as \(T\) (sample index omitted here for simplicity) the complete noise transition matrix1:

\[T=[T^{in}_{}}&_{ }}\\  T^{out}_{}}&_{}} ].\]

\(T^{in}\) corresponds to the confusion process between inlier classes \(^{in}:\{1,2,...,A\}\), and \(T^{out}\) corresponds to the confusion process from outlier classes \(^{out}:\{A+1,A+2,...,A+B\}\) to inlier classes \(^{in}:\{1,2,...,A\}\).

For brevity, we denote as \(T_{ij} P(^{n}=j|=i,=;^ {n},^{all})\). We have further \(_{j=1}^{A+B}T_{ij}=1\) for \(i\{1,...,A+B\}\) - noise transition from each clean class sums to 1 over all possible noisy classes. With such a complete noise transition matrix \(T\), we can connect the clean 

[MISSING_PAGE_FAIL:4]

To measure the negative impacts of noisy labels, we care about how much extra errors have been introduced, measured by the _error rate inflation_ of learned model \(f\) compared to the Bayes optimal model \(f^{*}\):

**Definition 3.4** (Error rate inflation).: With \(E^{*}_{}\) as the Bayes error rate, we define the _error rate inflation_ for sample \(\) as: \( E_{}=E_{}-E^{*}_{}\).

Two pragmatic casesHowever, \(P^{f}(|=;^{in})\), as the prediction of the final learned model \(f\), is affected by many factors (model capacity/dataset size/training hyperparameters such as training epochs, etc.), which is non-trivial to determine its specific value for an offline analysis2. Thus, we consider two specific pragmatic cases:

* _Fitted case_: the model perfectly fits the noisy distribution: \(P^{f}(|=;^{in})=P^{n}( |=;^{in})\);
* _Memorized case_: the model completely memorises the noisy labels: \(P^{f}(|=;^{in})=P^{y^{n}}( |=;^{in})\); Here \(P^{y^{n}}\) denotes the one-hot encoding of the noisy label \(y^{n}\).

Nonetheless, these two cases are very realistic and important; Empirically, it is highly possible that the _memorized case_ can correspond to scenarios such as scratch training based on a single-label dataset with a normal deep neural network - as normally such model has enough capacity to memorize all the labels, while the _fitted case_ can correspond to scenarios such as fine-tuning a linear classifier with a pre-trained model - as the pre-trained model already captures good sample representations and the capacity of a linear classifier is limited.

### Error rate inflation analysis _w.r.t_ different label noise

In this section, we focus on analyzing the error rate inflation of different label noise. Let us recall the clean conditional distribution as \(P(|;^{all})\). For ease of analysis, we contemplate a simple scenario, wherein the entire clean conditional distribution remains unchanged, except only one of the sample points, say \(\), is afflicted by label noise:

\[P^{n}(|;^{all})=P( |;^{all}),\ P^{n}( |=;^{all}) P( |=;^{all}).\] (6)

In this condition, we can simplify analyzing the impact of label noise on the whole distribution to analyzing the error rate inflation of a single sample \(\). Specifically, we consider two specific sample points \(_{1}\) and \(_{2}\), corresponding to two in our later comparative analysis. Let us denote its clean conditional probability as \(P(|=_{1};^{all})=[p^{1}_{1},...,p^{1}_{A},...,p^{1}_{A+B}]\) and \(P(|=_{2};^{all})=[p^{2}_{1},...,p^{2}_{A},...,p^{2}_{A+B}]\), and noise transition matrix as \(T^{1}\) and \(T^{2}\), respectively. We further assume:

\[O_{_{1}}+C_{_{1}}=O_{_{2}}+C_{_{2}}=.\] (7)

_We compare the error rate inflation (\( E_{_{1}}\) vs \( E_{_{2}}\)) with different label noise given same/fixed noise ratio for a strictly fair comparison_. Note we assume that \(_{1}\) and \(_{2}\) hold the same sampling prior probability: \(P(=_{1};^{all})=P(=_ {2};^{all}))\); so that, we assure that the whole noise ratio \(N\) is fixed, and more importantly, sample \(_{1}\) and \(_{2}\) can be considered as probabilistic exchangeable in the dataset collection process.

For better clarity, we depict the derivation relations for \(_{}\) in fig. 2. Specifically, for our two interested cases above, we have corresponding error rate inflation for sample \(\) (sample subscript omitted for simplicity) as:

* _Fitted case_: \[ E_{}=[p_{1},...,p_{A}]-p_{[_{i=1}^{A+B}p_{i}T_{i 1},...,_{i=1}^{A+B}p_{i}T_{i,A}]}\] (8)
* _Memorized case_: \[ E_{}=[p_{1},...,p_{A}]-_{i=1}^{A}(p_{i}_{j=1}^{A +B}p_{j}T_{ji})\] (9)

We notice that \(_{}\) in both cases are only affected by clean conditional probability \(P(|=_{1};^{all})\) and complete noise transition matrix \(T\).

#### 3.4.1 How does open-set noise compare to closed-set noise?

We first try to elucidate the difference between open-set noise and closed-set noise. Without loss of generality, we consider:

\[O_{_{1}}>O_{_{2}}\,\ C_{_{1}}<C_{_{2}}.\] (10)

Intuitively speaking, we consider sample \(_{1}\) to be more prone to open-set noise compared to sample \(_{2}\), thus corresponding to the'more open-set noise' scenario. However, without extra regularizations, there exist infinite \(T^{1}\) and \(T^{2}\) fulfilling eq. (7) and eq. (10) given specific \(P(|=_{1};^{all})\) and \(P(|=_{2};^{all})\) (see toy example below), the analysis on \( E_{_{1}}\)_vs_\( E_{_{2}}\) is thus infeasible.

**Toy example about agnostic \(T\)** Assuming a ternary classification, with two known inlier classes ("0" and "1") and one unknown outlier class "2". Say, we have sample \(_{1}\) with clean conditional probability as \([0.1,0.2,0.7]\). Assuming two different noise transition matrices for \(T^{1}\) below:

\[[0.55,0.45,0.0]=[0.1,0.2,0.7][0.5&0.5&0\\ 0.75&0.25&0\\ 0.5&0.5&0]\]

\[[0.45,0.55,0.0]=[0.1,0.2,0.7][0&1&0\\ 0.5&0.5&0\\  0.5&0.5&0]\]

We have \(O_{_{1}}=0.7,C_{_{1}}=0.2\) in both conditions but we arrive at different noisy conditional probability, similarly for sample \(_{2}\).

We thus consider a class concentration assumption -- in most classification datasets, the majority of samples belong to specific class exclusively with high probability. In this condition, we have proved:

**Theorem 3.5** (Open-set noise _vs_ closed-set noise).: _Let us consider sample \(_{1}\), \(_{2}\) fulfilling eq. (7) and eq. (10) - compared to \(_{2}\), \(_{1}\) is considered as more prone to open-set noise. Let us denote \(a=_{i}P(=i|=_{1};^{ all})\) and \(b=_{i}P(=i|=_{2};^{ all})\), we assume (with a high probability): \(p_{a}^{1} 1,\{p_{i}^{1} 0\}_{i a}\) and \(p_{b}^{2} 1,\{p_{b}^{2} 0\}_{i b}\). Then, we have:_

\[ E_{_{1}}< E_{_{2}}\]

_in both **Fitted case** and **Memorized case**._

Please refer to appendix D.1 for detailed proof. _To summarize, we validate that in most conditions, open-set noise is less harmful than closed-set noise in both **fitted case** and **memorized case**.

#### 3.4.2 How does different open-set noise compare to each other?

We further study how different open-set noise affect the model. Specifically, we consider:

\[O_{_{1}}=O_{_{2}}\,\ C_{_{1}}=C_{_{2}}=0.\] (11)

Intuitively speaking, we focus on the impacts of different open-set noise modes given the same/fixed open-set noise ratio, while excluding the effect of closed-set noise. In this section, we assume sample \(_{1}\) and sample \(_{2}\) holds the same clean conditional probability: \([p_{1}^{1},...,p_{A}^{1},...,p_{A+B}^{1}]=[p_{1}^{2},...,p_{A}^{2},...,p_{A+B} ^{2}]\), to only focus on the impact of different open-set noise modes with the same original sample. It is straightforward that \(O_{_{1}}=O_{_{2}}\) always holds since \(_{i=A+1}^{A+B}p_{i}^{1}=_{i=A+1}^{A+B}p_{i}^{2}\). To ensure \(C_{_{1}}=C_{_{2}}=0\), we simply set \(T_{in}^{1}=T_{in}^{2}=\).

Figure 2: All-in-one derivation flowchart. Full details in appendix C.

Thus, we have the flexibility to explore various forms of \(T_{out}\) -- corresponding to different open-set noise modes. Specifically, we consider two distinct open-set noise modes: 'easy' open-set noise when the transition from outlier classes to inlier classes involves completely random flipping, and 'hard' open-set noise when there exists an exclusive transition between the outlier class and specific inlier class. We denote as \(T^{easy}\) for 'easy' open-set noise and \(T^{hard}\) for 'hard' open-set noise, with intuitive explanations below:

\[T^{easy}=&...&\\...&...&...\\ &...&_{}\] (12)

and

\[T^{hard}=0&...&1\\...&...&...\\ 1&...&0_{}\] (13)

Especially, for \(T^{easy}\), we have \(T_{ij}=\) everywhere; for \(T^{hard}\), we denote as \(H_{i}:\{_{j}(T^{hard}_{ji}=1)\}_{i=1}^{A}\) the set of corresponding outlier classes \(j^{out}\) confused to inlier class \(i^{in}\). Without loss of generality, we consider \(_{1}\) with 'easy' open-set noise \(T^{easy}\) and \(_{2}\) with 'hard' open-set noise \(T^{hard}\). Please note, that we no longer require class concentration assumption here as the noise transition matrix is already known. In this condition, we have proved:

**Theorem 3.6** ('Hard' open-set noise _vs_ 'easy' open-set noise).: _Let us consider sample \(_{1}\), \(_{2}\) fulfilling eq. (7) and eq. (11). We set the corresponding noise transition matrix as \(T^{1}_{out}=T^{easy},T^{2}_{out}=T^{hard},T^{1}_{in}=T^{2}_{in}=\) and denote \([p^{1}_{1},...,p^{1}_{A},...,p^{1}_{A+B}]=[p^{2}_{1},...,p^{2}_{A},...,p^{2}_{ A+B}]=[p_{1},...,p_{A},...,p_{A+B}]\). Then, we have:_

* _Fitted case_: \[ E_{_{1}} E_{_{2}}.\]
* _Memorized case_: \[ E_{_{1}}- E_{_{2}}=_{i=1}^{A}a_{i}b_{i}.\]

_Here, \(a_{i}=p_{i},b_{i}=_{j H_{i}}p_{j}-_{i=A+1}^{A+B}p_{i}\)._

Please refer to appendix D.2 for detailed proof. Specifically, we further discuss about _memorized case_ here. Since \(_{i=1}^{A}b_{i}=0,_{i=1}^{A}a_{i}=1\), we can easily infer \(( E_{_{1}}- E_{_{2}}) 0,( E_{_{1}} - E_{_{2}}) 0\). With theorem D.3, we know when the ranking of \(\{p^{1}_{i}\}_{i=1}^{A}\) is completely in agreement with the ranking \(\{_{j H_{i}}p^{1}_{j}\}_{i=1}^{A}\) (constant term \(-_{i=A+1}^{A+B}p^{1}_{i}\) omitted here), we reach its maximum value with \( E_{_{1}}- E_{_{2}} 0\). Intuitively speaking, this implies a scenario that the 'hard' open-set noise tends to confuse a sample into the inlier class it primarily belongs to (with higher semantic similarity), as indicated by its higher probability (the higher the \(p^{1}_{i}\) the higher the \(_{j H_{i}}p^{1}_{j}\)). For example, an outlier 'tiger' image is wrongly included as a 'cat' rather than a 'dog' in 'cat vs dog' binary classification dataset. As this is more consistent with the common intuition, we default to such noise mode for 'hard' open-set noise -- assuming the ranking of \(\{p^{1}_{i}\}_{i=1}^{A}\) is of high agreement with the ranking of \(\{_{j H_{i}}p^{1}_{j}\}_{i=1}^{A}\).

_To summarize, unlike the general comparison between open-set noise and closed-set noise, the 'hard' open-set noise and the 'easy' open-set noise exhibit an opposite trend in two different cases. In the **fitted case**, 'easy' open-set noise appears to be less harmful, while in the **memorized case**, the impact of 'hard' open-set noise is comparatively smaller._

### Rethinking open-set noise detection

In this section, we try to investigate a commonly used open-set noise identification mechanism based on entropy dynamics. Within the sample selection paradigm, several methods  have proposed to further identify open-set noise, based on the empirical phenomenon that samples with relatively in-confident predictions are usually open-set samples, characterized by its high prediction entropy. Specifically, we consider original sample \(\) without noise transition, \(\) with \(T^{hard}\) and \(\) with \(T^{easy}\)as a clean sample, a 'hard' open-set noise and an 'easy' open-set noise, respectively. For simplicity, we omit the subscript.

Empirically, most sample selection method starts from the early training stages after certain epochs of warm-up training, expecting the model to learn meaningful information before over-fitting. To analyze the entropy dynamics, we thus consider the model predictions in the _fitted case_ as a pragmatic proxy. Let us denote as \(_{easy}\), \(_{hard}\) and \(_{clean}\) the prediction entropy corresponds to these three conditions, we have3:

\[_{clean} =([}{_{i=1}^{A}p_{i}},...,}{ _{i=1}^{A}p_{i}}])\] (14) \[=([p_{1}+}{_{i=1}^{A}p_{i}}_{i=A+1} ^{A+B}p_{i},...,p_{A}+}{_{i=1}^{A}p_{i}}_{i=A+1}^{A+B}p_{i}]),\] \[_{easy} =([p_{1}+_{i=A+1}^{A+B}p_{i},...,p_{A} +_{i=A+1}^{A+B}p_{i}]),\] \[_{hard} =([p_{1}+_{j H_{1}}p_{j},...,p_{A}+_{j H _{A}}p_{j}]).\]

We note \(_{easy}_{clean}\)4. However, comparing \(_{hard}\) and \(_{clean}\) is non-trivial without specific values for each entry. _Thus, we suggest open-set noise detection based on the prediction entropy may only be effective for 'easy' open-set noise._

## 4 Experiments

In this section, we try to validate our theoretical findings. In section 4.1, we validate the theoretical comparisons of different label noise. In section 4.2, we validate the entropy dynamics with different label noise. Moreover, in appendix E.1, we revisit the performance of two existing LNL methods involving open-set noise. To conduct more controllable, fair and accurate experiments, we propose two synthetic open-set noisy datasets -- CIFAR100-O and ImageNet-O, respectively based on the CIFAR100 and ImageNet datasets. We also consider closed-set noise in some experiments, particularly, the symmetric closed-set noise. Please refer to appendix A for more dataset and implementation details and also details about open-set detection protocol.

### Empirical validation on previous probabilistic findings

In this section, we conduct experiments to validate the theorem 3.5 and theorem 3.6. Since most deep models have sufficient capacity, we consider direct supervised learning from scratch on the noisy dataset and consider the final model as the _memorized case_ - as evidenced by nearly \(100\%\) train set accuracy. Conversely, obtaining a model that perfectly fits the data distribution is often challenging; here, we consider training a single-layer linear classifier upon a frozen pretrained encoder. Due to the limited capacity of the linear layer, we expect to roughly approach the _fitted case_.

We show classification accuracy on CIFAR100-O and ImageNet-O datasets under different noise ratios, as shown in fig. 3(a/b). We find that: 1) in both cases, the presence of open-set noise has a significantly smaller impact on classification accuracy compared to closed-set noise. 2) 'hard' open-set noise and 'easy' open-set noise show opposite trends in the two different scenarios. These results align perfectly with our theoretical analysis.

In addition to closed-set classification accuracy, we also report the model's open-set detection performance using the maximum prediction value as the indicator ) in fig. 3(c/d). We find that, in both cases, the presence of open-set noise leads to a degraded open-set detection performance, while conversely, the presence of closed-set noise can often even enhance open-set detection performance. In light of this contrasting trend, we propose that the open-set detection task, in addition to the default closed-set classification, may help to offer a more comprehensive evaluation of LNL methods.

### Inspecting entropy-based open-set noise detection mechanism

In section 3.5, we briefly analyze the open-set detection mechanism based on the entropy values of model predictions and find that it may be effective only for 'easy' open-set noise. Here, we again utilize the CIFAR100-O and ImageNet-O datasets for validation experiments with different open-set noise ratios and modes. Specifically, we adopt the common warm-up idea used in existing LNL methods - training with the entire dataset for a certain number of epochs. We report the model's predicted entropy values for each sample at the \(\{5th,10th,20th,30th\}\) epoch in fig. 4.

We validate that the entropy dynamics is a more effective indicator for 'easy' open-set noise compared to 'hard' open-set noise ((a) vs (b), (c) vs (d) in fig. 4). However, even for 'easy' open-set noise, we also notice that the warm-up epoch matters a lot -- too early (\(5th\) epoch in fig. 4(c)) or too late (\(30th\) epoch in fig. 4(c)) also make open-set noise difficult to distinguish. We also test with mixed noise including both open-set noise and closed-set noise, please refer to appendix B for more discussions.

## 5 Conclusions

This paper focuses on exploring how open-set label noise affects the performance of models. While the 'open world' setting involving open-set samples has been widely discussed in several other weakly supervised learning settings, its application in the context of learning with noisy labels has been understudied. In light of this, we reconsider the LNL problem, specifically focusing on the impact of open-set noise compared to closed-set noise, and different types of open-set noise compared to each other, on the evaluation performance. In light of the challenges existing testing frameworks face in handling open-set noise, we explore the open-set detection task to address the deficiencies in model evaluation for open-set noise and conducted preliminary experiments. Additionally, we look into the common mechanism for detecting open-set noise based on the model's prediction entropy. Both theoretical and empirical results highlight the urgent need for a deeper exploration of open-set noise and its complex impact on model performance.

Figure 4: Entropy dynamics _w.r.t_ different datasets/noise modes/noise ratios.

Figure 3: Direct supervised training with different noise modes/ratios.