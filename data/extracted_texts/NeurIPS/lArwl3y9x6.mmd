# Normalization Layers Are All That Sharpness-Aware Minimization Needs

Maximilian Muller

University of Tubingen

and Tubingen AI Center

maximilian.mueller@wisi.uni-tuebingen.de &Tiffany Vlaar

McGill University and

Mila - Quebec AI Institute

tiffany.vlaar@mila.quebec &David Rolnick

McGill University and

Mila - Quebec AI Institute

drolnick@cs.mcgill.ca &Matthias Hein

University of Tubingen

and Tubingen AI Center

matthias.hein@uni-tuebingen.de

###### Abstract

Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (typically comprising 0.1% of the total parameters) in the adversarial step of SAM can outperform perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness.

+
Footnote â€ : Code is provided at https://github.com/mueller-mp/SAM-ON.

## 1 Introduction

Numerous works have been dedicated to studying the potential connection between flatness of minima and generalization performance of deep neural networks . Several aspects of training are thought to affect sharpness, but how these interact with each other remains an ongoing area of research. Recently, sharpness-aware minimization (SAM) has become a popular approach to actively try to find minima with low sharpness using a min-max type algorithm . SAM was found to be remarkably effective in enhancing generalization performance for various settings .

Several variants of SAM have been proposed, focusing both on enhanced performance  and reduced computational cost . In particular, with the original aim of making SAM more efficient Mi et al.  propose a'sparse' SAM approach, which applies SAM only to a select number of parameters. Although they do not actually succeed in reducing wall-clock time, they make the surprising observation that using 50% (in some settings even up to 95%) sparse perturbations can maintain or even enhance performance compared to applying SAM to all parameters. They thus hypothesize that "complete perturbation on all parameters will result in suboptimal minima'.

Similar to the effect of SAM , normalization layers are thought to reduce sharpness . Frankle et al.  found for ResNets that the trainable affine parameters of the normalization layershave remarkable representation capacity in their own right, whereas disabling them can reduce performance. Inspired by this we focus on the interplay between SAM and normalization layers and show that for various settings perturbing exclusively the normalization layers of a network (often less than 0.1% of the total parameters) outperforms perturbing all parameters. We find that this can not be solely attributed to possible benefits of sparse perturbation approaches and highlight the unique role played by the normalization layer affine parameters. As our main contributions we show that:

* Applying SAM only to the normalization layers of a network (_SAM-ON_, short for _SAM-OnlyNorm_) enhances performance on CIFAR data compared to applying SAM to the full network (_SAM-all_) and also performs competitively on ImageNet. We corroborate the remarkable generalization performance of _SAM-ON_ for ResNet and Vision Transformer architectures, across different SAM variants, and for different batch sizes. _(Section 4)_
* Alternative sparse perturbation approaches do not result in similar performance as _SAM-ON_, especially not at the extreme sparsity levels of our method. _(Section 5.1)_
* Similar to _SAM-all_, _SAM-ON_ yields non-trivial adversarial robustness (Section 4.2). It also reduces the feature-rank, but the sharpness-reducing qualities of _SAM-all_ are not fully preserved (Section 5.2).

## 2 Related Work

**Normalization layers.** Batch Normalization (BatchNorm)  and Layer Normalization (LayerNorm)  form an essential component of most convolutional [25; 29] and Transformer [51; 19] architectures, respectively. Across various works these normalization layers were shown to accelerate and stabilize training, reducing sensitivity to initialization and learning rate [13; 5; 60; 36]. But despite their widespread adoption and illustrated effectiveness, a conclusive explanation for their success is still elusive. The original motivation for BatchNorm as reducing internal covariance shift  has been disputed . The hypothesis that normalization layers enhance smoothness is supported through both empirical and theoretical analyses [46; 10; 41], though also not completely undisputed . Unlike LayerNorm, BatchNorm is sensitive to the choice of batch size [38; 49]. Ghost BatchNorm, where BatchNorm statistics from disjoint subsets of the batch are used, is found to regularize and generally enhance generalization [28; 49] even though it reduces smoothness .

**Affine parameters.** There are relatively few papers that study the role of the trainable affine parameters of the normalization layers. Frankle et al.  were able to obtain surprisingly high performance on vision data by only training the BatchNorm layers, illustrating the expressive power of the affine parameters, which potentially achieve this by sparsifying activations. For BatchNorm in ResNets, disabling the affine parameters was shown empirically to reduce generalization performance , but for LayerNorm in Transformers to not affect or even improve performance . For few-shot transfer tasks disabling the BatchNorm affine parameters during pretraining was found to enhance performance . Further, many other aspects of training will have a non-trivial effect, e.g. applying weight decay to the BatchNorm affine parameters was found to increase performance for ResNets but harm performance in other settings .

Figure 1: **The interplay of normalization layers with SAM: Perturbing _only_ normalization layers (OnlyNorm, dashed) improves generalization performance, while omitting them in the perturbation (no-norm, dotted) can harm training. WideResNet-28-10 trained with different SAM-variants on CIFAR-100. Best seen in color.**

**Sharpness-aware minimization.** SAM was developed to try to actively seek out minima with low sharpness . Training with SAM may lead to increased sparsity of active neurons  and models which are more compressible . SAM has been shown to be effective in enhancing generalization performance in various settings, but also increases the computational overhead compared to base optimizers [23; 14; 7]. Hence there have been several approaches to try to reduce the computational cost of SAM, such as ESAM which utilizes sharpness-sensitive data selection and perturbs only a randomly selected fraction of parameters . Related work shows that "only employing 20% of the batch to compute the gradients for the ascent step,... [can] result in equivalent performance"  and that only applying SAM to part of the parameters (SSAM) using e.g. a Fisher-information mask can lead to enhanced performance . A common variant of SAM utilizes \(m\)-sharpness , which uses subbatches of size \(m\) and benefits performance [20; 2; 43] though nuances in its implementation vary . Andriushchenko and Flammarion  argue that its success is not unique to settings with BatchNorm and hence cannot be attributed to the Ghost BatchNorm effect. Andriushchenko et al.  further show that SAM leads to low-rank features. We discuss different SAM variants in Section 3.2.

## 3 Background: SAM and Normalization Layers

In this paper, we focus on the interplay between two popular aspects of neural network training, both of which we recapitulate here: In Sec. 3.1 we provide an overview of normalization layers, in particular BatchNorm and LayerNorm, and in Sec. 3.2 of Sharpness-Aware Minimization variants.

### BatchNorm and LayerNorm

Modern neural network architectures typically incorporate normalization layers. In this work we will focus on Batch Normalization (BatchNorm)  and Layer Normalization (LayerNorm) , which are an essential building block of most convolutional [25; 29] and transformer [51; 19] architectures, respectively. Normalization layers transform an input \(\) according to

\[N()=-}{}+\] (1)

where \(\) and \(^{2}\) are the mean and variance, which are computed over the batch dimension in the case of BatchNorm, or over the embedding dimension, in the case of LayerNorm. BatchNorm is therefore sensitive to the choice of batch size [38; 49]. For BatchNorm, \(\) and \(\) are computed from the current batch-statistics during training, and running estimates are used at test time. In our experiments, we focus on the trainable parameters \(\) and \(\), which perform an affine transformation of the normalized input.

### SAM and its variants

We recapitulate SAM , ASAM  and Fisher-SAM  with their respective perturbation models. To this end, we consider a neural network \(f_{}:^{d}^{k}\) which is parameterized by a vector \(\) as our model. The training dataset \(S^{train}=\{(_{1},_{1}),...(_{n},_{n})\}\) consists of input-output pairs which are drawn from the data distribution \(D\) and we write the loss function as \(l:^{k}^{k}_{+}\). The goal is to learn a model \(f_{}\) with good generalization performance, i.e. low expected loss \(L_{D}()=_{(,) D}[l(,f_{ }())]\) on the distribution \(D\). The training loss can be written as \(L()=_{i=1}^{n}l(_{i},f_{}(_{i}))\). Conventional SGD-like optimization methods minimize (a regularized version of) \(L\) by stochastic gradient descent. SAM aims at additionally minimizing the worst-case sharpness of the training loss in a neighborhood defined by an \(_{p}\) ball around \(\), i.e. \(_{||||_{p}}L(+)-L()\). This leads to the overall objective

\[_{}_{||||_{p}}L(+).\] (2)

In practice, SAM uses \(p=2\) and approximates the inner maximization by a single gradient step, yielding \(= L()/|| L()||_{2}\) and requiring an additional forward-backward pass compared to SGD. The gradient is then re-evaluated at the perturbed point \(+\), giving the actual weight update

\[- L(+)\] (3)

with learning rate \(\). Computing \(\) separately for the batch of each GPU in multi-GPU settings and then averaging the resulting perturbed gradients for the update step in Eq. (3) has been shown toincrease SAM's performance . This method is called \(m\)-sharpness, with \(m\) being the number of samples on each GPU. Since the perturbation model in Eq. (2) is not invariant with respect to a rescaling of the weights that leaves \(f_{}\) invariant , ASAM , a partly scale-invariant version of SAM, was proposed, with the objective

\[_{}_{||T_{w}^{i-1}||_{2}}L(+)\] (4)

where \(T_{w}\) is a normalization operator, making the perturbation adaptive to the scale of the network parameters. Kwon et al.  choose \(T_{w}\) to be diagonal with entries \(T_{w}^{i}=|w_{i}|+\) for weight parameters and \(T_{w}^{i}=1\) for bias parameters, called _elementwise_ normalization. \(\) is typically set to \(0.01\). As with SAM, the inner maximization is solved by a single gradient step:

\[_{2}=^{2} L()}{||T_{w} L( )||_{2}}p=2,_{}= T_{w} L()p=.\] (5)

We note that for \(T_{w}\) equal to the identity matrix and \(p=2\), this is equivalent to the original SAM formulation. Recently, Kim et al.  proposed to use a distance metric induced by the Fisher information instead of a Euclidean distance measure between parameters. The approach can also be framed as a variant of ASAM, with \(T_{w}\) being diagonal with entries \(T_{w}^{i}=1/}\) and \(f_{i}\) approximating the \(i^{th}\) diagonal entry of the Fisher-matrix by the squared average batch-gradient, \(f_{i}=(_{w_{i}}L_{Batch}())^{2}\). For our experiments, we additionally employ layerwise normalization. This is, we set the diagonal entries of \(T_{w}^{i}=||_{[i]}||_{2}\), which corresponds to a normalization with respect to the \(_{2}\)-norm of a layer, similar to .

## 4 SAM-ON: Perturbing Only the Normalization Layers

We study the effect of applying SAM (and its variants) solely to the normalization layers of a considered model. We will refer to this approach as _SAM-ON_ (SAM-OnlyNorm) throughout this paper and provide a convergence analysis for _SAM-ON_ in Appendix C. We find that _SAM-ON_ obtains enhanced generalization performance compared to conventional SAM (denoted as _SAM-all_) for ResNet architectures with BatchNorm (Section 4.1) and Vision Transformers with LayerNorm (Section 4.2) on CIFAR data and performs competitively on ImageNet. For comparison, we also study the reverse of _SAM-ON_, i.e. we exclude the affine normalization parameters from the adversarial SAM-step, which we shall refer to as _no-norm_.

**Training set-up.** We use SGD with momentum, weight decay, and cosine learning rate decay as our base optimizer for ResNet architectures and employ label smoothing to adopt similar settings as in the literature . For Vision Transformers we employ AdamW  as our base optimizer on CIFAR and for ImageNet we additionally use Lion . We use both basic augmentations (random cropping and flipping) and strong augmentations (basic+AutoAugment, denoted as +AA). We consider a range of SAM-variants which differ either in the perturbation model (\(_{2}\) or \(_{}\)) or in the definition of the normalization operator. We train models for 200 epochs, and do not employ \(m\)-sharpness unless indicated otherwise. Complete training details are described in Appendix A.

### BatchNorm and ResNet

**CIFAR.** We showcase the effect of _SAM-ON_, i.e. only applying SAM to the BatchNorm parameters, for a WideResNet-28-10 (WRN-28) on CIFAR-100 in Figure 1. We observe that _SAM-ON_ obtains higher accuracy than conventional SAM (_SAM-all_) for all SAM variants considered (more SAM-variants are shown in Figure 6 in the Appendix). In contrast, excluding the affine BatchNorm parameters from the adversarial step (_no-norm_) either significantly decreases performance (for elementwise-\(_{2}\) variants) or maintains similar performance as _SAM-all_ (for all other variants). For variants which do not experience a performance drop for _no-norm_, the ideal SAM perturbation radius \(\) shifts towards larger values, indicating that the perturbation model cannot perturb the BatchNorm parameters enough when _all_ parameters are used. To study if the benefits of only perturbing the normalization layers extends to other settings, we train more ResNet-like models on CIFAR-10 and CIFAR-100. For each SAM-variant and dataset, we probe a set of pre-defined \(\)-values (shown in Table 7 in the Appendix) with a ResNet-56 (RN-56) and fix the best-performing \(\) for the other models to compare _SAM-ON_ to _SAM-all_. We report mean accuracy and standard deviation over 3 seeds for CIFAR-100 in Table 1. On average, _SAM-ON_ outperforms _SAM-all_ for all considered SAM-variants. Of these, layerwise-\(_{2}\) achieves the highest performance for most settings. We obtain similar results for CIFAR-10 (App. Table 10) and for more network architectures (App. Table 11).

**ImageNet.** For ImageNet, we adopt the timm training script . We train a ResNet-50 for 100 epochs on eight 2080-Ti GPUs with \(m=64\), leading to an overall batch-size of 512. Apart from \(\), all hyperparameters are shared for all SAM-variants and can be found in the Appendix in Table 8. We select the most promising _SAM-ON_ variants and compare them against the established methods (SGD, SAM, ASAM elementwise \(_{2}\)). The results are shown in Table 2. We observe that for layerwise \(_{2}\), the _all_ variant achieves higher accuracy, whereas the _SAM-ON_ models outperform their _all_ counterparts for elementwise \(_{2}\) and elementwise \(_{}\). All _SAM-ON_ variants outperform the previously established methods (SGD, SAM, ASAM). For reference, we also show the values reported for ESAM  and GSAM , two SAM-variants we did not include in our study.

**Varying the batchsize.** In Figure 4 (right) we report the performance of a WRN-28 on CIFAR-100 with _SAM-ON_ and _SAM-all_ for a range of batch-sizes and values of \(m\), where \(m\) is the batch-size per accelerator, as discussed in Section 3.2. Similar to the findings in , we confirm that lower values of \(m\) lead to better performance within each batch-size. Importantly, _SAM-ON_ outperforms _SAM-all_ for all combinations of batch-size and \(m\), illustrating that Ghost BatchNorm  (see discussion in Section 2) does not play a role in the success of _SAM-ON_.

### LayerNorm and Vision Transformer

**CIFAR.** To study the effectiveness of _SAM-ON_ beyond ResNet architectures and BatchNorm, we train ViTs from scratch on CIFAR data with AdamW as the base optimizer (Figure 2). Although ResNet architectures are known to outperform Vision Transformers when trained from scratch on small-scale datasets like CIFAR, our aim here is not to outperform state-of-the-art, but rather to study if the benefits of _SAM-ON_ extend to substantially different training settings. Remarkably, we find that the same phenomena occur: The _SAM-ON_ variants outperform their conventional counterparts _SAM-all_ by a clear margin. For the elementwise-\(_{2}\) variants there is a strong drop in accuracy for _no-norm_, whereas for the other SAM variants the optimal perturbation radius \(\) shifts towards larger values. We show that this extends to a ViT-T and CIFAR-10 as well (Table 3).

   & &  &  &  \\  & variant & all & ON & all & ON & all & ON \\   & SGD & \(72.82^{ 0.3}\) & \)} & \)} \\  & SAM & \(75.07^{ 0.6}\) & **75.58\({}^{ 0.4}\)** & \(81.79^{ 0.4}\) & **82.22\({}^{ 0.2}\)** & \(83.11^{ 0.3}\) & **84.19\({}^{ 0.2}\)** \\  & el. \(_{2}\) & \(75.05^{ 0.1}\) & **76.25\({}^{ 0.0}\)** & \(81.26^{ 0.2}\) & **82.30\({}^{ 0.3}\)** & \(82.38^{ 0.2}\) & **83.67\({}^{ 0.3}\)** \\  & el. \(_{}\) & orig. & \(75.54^{ 0.7}\) & **76.07\({}^{ 0.2}\)** & **82.15\({}^{ 0.3}\)** & \(81.90^{ 0.4}\) & **83.67\({}^{ 0.1}\)** & 83.53\({}^{ 0.2}\) \\  & el. \(_{}\) & \(75.36^{ 0.1}\) & **76.10\({}^{ 0.2}\)** & \(81.02^{ 0.6}\) & **82.38\({}^{ 0.3}\)** & \(83.25^{ 0.2}\) & **84.14\({}^{ 0.2}\)** \\  & Fisher & \(75.01^{ 0.4}\) & **75.65\({}^{ 0.1}\)** & \(81.55^{ 0.2}\) & **82.21\({}^{ 0.2}\)** & \(83.37^{ 0.1}\) & **84.01\({}^{ 0.1}\)** \\  & layer. \(_{2}\) & \(74.63^{ 0.1}\) & **76.03\({}^{ 0.3}\)** & \(81.66^{ 0.2}\) & **82.52\({}^{ 0.2}\)** & \(83.23^{ 0.2}\) & **84.05\({}^{ 0.2}\)** \\   & SGD & \(75.26^{ 0.2}\) &  & \(80.31^{ 0.3}\)} &  & \(83.62^{ 0.1}\) \\  & SAM & **76.33\({}^{ 0.3}\)** & \(76.02^{ 0.3}\) & \(82.33^{ 0.5}\) & **83.19\({}^{ 0.2}\)** & \(85.30^{ 0.1}\) & **85.42\({}^{ 0.1}\)** \\  & el. \(_{2}\) & **76.51\({}^{ 0.1}\)** & \(76.04^{ 0.3}\) & \(82.00^{ 0.3}\) & **83.20\({}^{ 0.1}\)** & \(84.80^{ 0.3}\) & **85.43\({}^{ 0.3}\)** \\  & el. \(_{2}\), orig. & \(76.49^{ 0.2}\) & **76.58\({}^{ 0.4}\)** & \(82.78^{ 0.1}\) & **82.87\({}^{ 0.3}\)** & \(85.25^{ 0.4}\) & **85.41\({}^{ 0.1}\)** \\  & el. \(_{}\) & \(74.89^{ 0.4}\) & **76.19\({}^{ 0.4}\)** & \(82.33^{ 0.1}\) & **83.11\({}^{ 0.2}\)** & \(85.28^{ 0.1}\) & **85.46\({}^{ 0.1}\)** \\  & Fisher & **76.67\({}^{ 0.1}\)** & \(76.25^{ 0.2}\) & \(82.56^{ 0.3}\) & **83.28\({}^{ 0.4}\)** & \(85.09^{ 0.3}\) & **85.35\({}^{ 0.1}\)** \\  & layer. \(_{2}\) & \(76.23^{ 0.5}\) & **76.93\({}^{ 0.4}\)** & \(82.61^{ 0.3}\) & **83.32\({}^{ 0.2}\)** & \(85.32^{ 0.3}\) & **85.95\({}^{ 0.1}\)** \\  

Table 1: _SAM-ON improves over _SAM-all_ for BatchNorm and ResNets_: Test accuracy for ResNet-like models on CIFAR-100. Bold values mark the better performance between _SAM-ON_ and _SAM-all_ within a SAM-variant, and underline highlights the overall best method per model and augmentation.

  SGD & SAM & ESAMI & GSAM & elem. \(_{2}\) & elem. \(_{}\) & layer \(_{2}\) \\  & all & all & all & ON & all & ON & all & ON \\  \(77.03^{ 0.13}\) & \(77.65^{ 0.11}\) & \(77.05\) & \(77.20\) & \(77.65^{ 0.05}\) & \(^{ 0.14}\) & \(77.45^{ 0.04}\) & \(^{ 0.01}\) & \(^{ 0.05}\) & \(77.87^{ 0.07}\) \\  

Table 2: ImageNet top-1 accuracy for a ResNet-50. ESAM  and GSAM  values are taken from the respective papers.

[MISSING_PAGE_FAIL:6]

### Computational savings

In Figure 3 we report the wall-clock time of training a WRN-28 (left) and a ResNeXt (right) with batchsize 128 on a single A100 with PyTorch. Since the normalization parameters at the earlier layers of the network require a gradient, potentially a full backpropagation pass has to be computed for the ascent-step of _SAM-ON_, even though only a tiny fraction of all parameters is perturbed. However, as discussed in  for a related setting, the gradients of the intermediate (no-norm) layers do not need to be stored or used for updating. This leads to computational gains of _SAM-ON_ over _SAM-all_ as reported in Figure 3.

In contrast, although future development of hardware for sparse operation may allow for acceleration, Mi et al.  report that their sparse perturbation approach does not at present lead to reduced wall-clock time. Their approach also suffers from additional computational cost associated with selecting the mask, and hence is outshined by _SAM-ON_ both in computational cost and generalization performance (as discussed in Section 5.1).

We also show results for perturbing only the normalization layers of selected blocks (Block 1-3) of the network, and interestingly the main benefits of _SAM-ON_ seem to arise from the later normalization layers, allowing for further computational savings. When perturbing only the normalization layers from the last block (B3), the biggest computational gains can be achieved (reducing the additional cost of SAM by more than 50%), without much loss in test accuracy. When perturbing the normalization layers from Block 2 and 3 (B2+B3), the test accuracy even slightly improves over _SAM-ON_ for both models, while the runtime is still significantly lower. We have not investigated this variant of _SAM-ON_ thoroughly, i.e. in combination with other ASAM perturbation models and more network architectures, but think that this is an interesting research direction for future work.

## 5 Towards Understanding SAM-ON

To gain a better understanding of _SAM-ON_, we study different hypotheses for the method's success. First, we investigate the role of sparsity by comparing _SAM-ON_ to different sparsified perturbation approaches (Section 5.1), concluding that sparsity alone is not enough to explain its success. Then, we highlight that _SAM-ON_ might in fact find _sharper_ minima, while generalizing better than _SAM-all_ (Section 5.2). We also show that _SAM-ON_ - similar to _SAM-all_ - reduces the feature-rank compared to vanilla optimizers (Section 5.2). Further, we showcase that depending on the perturbation method, _SAM-ON_ can induce a significant shift in the distribution of the normalization parameters (Section 5.3) and relate this to the _no-norm_ results from Section 4. Unless stated otherwise, we use a WRN-28 with BatchNorm and the setting described in Section 4 for the ablation studies. Further ablation studies are presented in Appendix B.

Figure 3: **Computational gains of SAM-ON over SAM-all:** Test accuracy vs. normalized wall-clock runtime for SAM and different variations of SAM-ON for a WRN-28 (left) and a ResNeXt (right) on CIFAR-100. Only perturbing selected normalization parameters (e.g. those from block 3, or those from block 2 _and_ 3) can lead to further computational gains. Reported values are averaged over three random seeds.

### The effect of sparsified perturbations

Mi et al.  propose a sparsified SAM (_SSAM_) approach which only considers part of the parameters for the SAM perturbation step. They find that using 50% perturbation sparsity _SSAM_ can outperform _SAM-all_, and still perform competitively with up to 99% sparsity in certain settings. This raises the question whether the enhanced performance of _SAM-ON_ over _SAM-all_ is simply due to perturbing fewer parameters. We therefore compare _SAM-ON_ to other sparse perturbation approaches.

In order to determine the parameters which should be perturbed, one solution Mi et al.  propose is to compute a binary mask via an approximation of the Fisher matrix. We call this approach _SSAM-F_ to avoid confusion with Fisher-SAM introduced in Section 3.2. Mi et al.  also propose a dynamic mask sparse perturbation approach (_SSAM-D_), but do not clearly favour either method. Since they consider _SSAM-F_ "relatively more stable but a bit time-consuming, while _[SSAM-D]_ is more efficient", we will for fair comparison focus on _SSAM-F_ here and provide results for _SSAM-D_ in Appendix B.2. According to Mi et al.  neither approach improves wall-clock time in practice, while we find that _SAM-ON_ does (see Section 4.3).

We provide a comparison between _SAM-ON_, _SSAM-F_, and a random sparsity mask of the same sparsity level as _SAM-ON_ for _a)_ ResNet-18 on CIFAR-10 (main setting considered in ) in Figure 4 (left) and _b)_ WRN-28 on CIFAR-100 in Table 5. We find that although _SSAM-F_ can indeed perform on par or even outperform _SAM-all_ at the medium to high sparsity levels recommended in  it is less successful than _SAM-ON_. Moreover, for the sparsity levels of _SAM-ON_, which are above \(99.9\%\) in both settings, a random mask performs poorly. These results suggest that sparsity is not the sole cause for _SAM-ON_'s success.

### Sharpness and feature-rank of SAM-ON

The improved generalization performance of SAM-trained models is often attributed to finding flatter minima [27; 34] - indeed this was the initial motivation behind SAM in . Andriushchenko and Flammarion  however cast doubt on this explanation, and argue that the benefits of SAM might stem from a favorable implicit bias induced by the method. A recent study furthermore found that sharpness often does not correlate well with a model's generalization performance . In this section we therefore compare the sharpness of _SAM-all_ to _SAM-ON_ models (following the setup in , more details provided in Appendix B.10).

Figure 4: **Left:** _SAM-ON_ outperforms _SSAM-F_ (with different sparsity levels) and random mask _SAM-rand_ (same sparsity level 99.93% as _SAM-ON_) sparse perturbation approaches on CIFAR-10 for ResNet-18. **Right:**_SAM-ON_ improves over SAM for various batch-sizes (bs) and values of \(m\), where smaller values of \(m\) tend to improve performance. WRN-28, CIFAR-100.

  & SAM & SAM-ON & Random Mask &  \\ Sparsity & 0\% & 99.95\% & 99.95\% & 50\% & 99.95\% \\  Test Accuracy (\%) & 83.11\(\)0.3 & **84.19\(\)**0.2 & 80.97\(\)0.2 & 83.94\(\)0.1 & 83.14\(\)0.1 \\ 

Table 5: **SAM-ON outperforms other sparse perturbation approaches:** Although _SSAM-F_ with different sparsity levels can outperform _SAM-all_ on CIFAR-100 with WRN-28, it is less effective than _SAM-ON_, especially when probed at very high sparsity levels.

[MISSING_PAGE_FAIL:9]

## 6 Discussion and Conclusion

In recent years the method of sharpness-aware minimization (SAM)  has risen to prominence due to its demonstrated effectiveness and the community's long-standing interest in flat minima. In this work we show that only applying SAM to the normalization layers (_SAM-ON_) - typically less than \(0.1\%\) of the total parameters - can significantly enhance performance compared to regular SAM (_SAM-all_). We show results on CIFAR and ImageNet for ResNet and Vision Transformers with BatchNorm and Layernorm, respectively. Although the use of sparsified perturbations was recently shown to benefit generalization , we show that the success of _SAM-ON_ cannot be attributed to sparsity alone: targeting the normalization layers clearly improves over other masked sparsity approaches.

We find that while _SAM-ON_ outperforms _SAM-all_ in almost all settings, the optimal SAM variant to use varies. We do not see a consistent benefit of using reparameterization-invariant perturbation models compared to variants with fewer invariances. In particular, we find that layerwise \(_{2}\) in combination with _SAM-ON_ reaches the highest accuracy for many settings.

While _SAM-ON_ improves generalization, it loses some of SAM's sharpness-reducing qualities. Although perhaps surprising given SAM's original motivation, this finding relates naturally to the literature.  find that sharpness does not always correlate well with generalization performance. Further,  question if sharpness is the sole driving factor behind SAM's success in enhancing generalization. We lend support to this question, showing that SAM-like methods can generalize better, without significant sharpness reduction.

In summary, we demonstrate benefits of SAM beyond reducing sharpness and highlight the special role played by the normalization layers. More investigation into the interplay of SAM and other aspects of training are needed to fully understand where the methods gains come from.

**Limitations.** Similar to the main inspirations for this work [23; 24; 42] we focus on vision data. We provide a simple experiment in the language domain in Appendix B.5 indicating that the efficacy of _SAM-ON_ might be preserved for language tasks, but more extensive benchmarking, as done by  is required. Further, more work is required to try to leverage the perturbation sparsity for reduced computational cost beyond the gains obtained in this work, and to fully explore the benefits of perturbing subsets of the normalization layers building on the findings in Section 4.3.