# Absorb & Escape: Overcoming Single Model Limitations in Generating Genomic Sequences

Zehui Li1,*, Yuhao Ni1, Guoxuan Xia1, William Beardall1,

**Akashaditya Das1, Guy-Bart Stan1,*, Yiren Zhao1,* 1**Imperial College London, {zehui.li222, harry.ni21, g.xia21, william.beardall15, akashaditya.das13, g.stan, a.zhao}@imperial.ac.uk

*Correspondence: {zehui.li22, g.stan, a.zhao}@imperial.ac.uk

###### Abstract

Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb & Escape (A&E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps. To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&E outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the Github Repo1.

## 1 Introduction

DNA sequences, as the blueprint of life, encode proteins and RNAs and directly interact with these molecules to regulate biological activities within cells. The success of deep generative models in image , text , and protein design  has drawn the attention of deep learning researchers to the problem of DNA design, i.e. applying these models to genomic sequence generation . However, one rarely explored issue is how well existing methods can handle the unique property of DNA sequences: heterogeneity. DNA sequences are highly heterogeneous, consisting of multipleconnected functional regions (e.g. Promoter Regions, Exons, and Introns) in sequential order. While elements within each functional region might be homogeneous (coming from the same distribution), the overall sequence is non-homogeneous. This heterogeneity, along with the discrete nature of genomic sequences, poses challenges to popular deep generative methods.

Limitations of Existing Single-Model Approaches in Generating Genomic SequencesAutoRegressive Models (AR) [8; 17; 24] are one of the most dominant approaches for discrete sequence generation. To model the data distribution of a sequence \(x\) of length \(T\), the probability of \(x\) is factorized as:

\[p^{AR}()=_{i=1}^{T}p_{}(x_{i}|x_{1},x_{2},,x_{i-1}).\] (1)

An issue arises when modeling heterogeneous data, where the value of \(\) may vary significantly from one segment to another. Additionally, AR models assume a dependency between the current element and previous elements; this assumption may not hold true for heterogeneous sequences, potentially hindering the learning process (see Section 3 for details).

On the other hand, Diffusion Models (DMs), initially proposed by , have been dominant in image generation. In the probabilistic denoising view , DMs gradually add noise to the input data \(x_{0}\), and a reverse diffusion (generative) process is trained to gradually remove the noise from the perturbed data \(x_{t}\). DMs directly model the data distribution without AutoRegressive factorization, thereby avoiding the issues associated with AR models. However, it has been shown that DMs are less competent than AR models for discrete data generation [21; 36]. When it comes to modeling heterogeneous genomic sequences, it remains unclear how the performance of DMs compares to AR models within each homogeneous segment.

Model Composition As a SolutionBalancing the ability of generative algorithms to capture both local and global properties of the data distribution is central to the problem. An obvious solution could be to combine these two types of models and perform generation using the composed models. However, this typically requires converting these two models into an Energy-Based Model and then sampling using Markov Chain Monte Carlo (MCMC), which can be inherently slow due to the sampling nature of the algorithm , and the potential long inference time of individual models. With the goal of accurate and efficient DNA generation, we aim to investigate two key questions in this work: (i) How well does a single AR model or DM perform in DNA generation, given the heterogeneous nature of genomic sequences? (ii) Is there an efficient algorithm to combine the benefits of AR models and DMs, outperforming a single model? In answering these two questions, _our contribution is three-fold_:

1. We study the properties of AR models and DMs in heterogeneous sequence generation through theoretical and empirical analysis (Section 3).

Figure 1: (a) Generated DNA interacting with TATA-binding protein. (b) Proposed A&E framework.

2. We design the theoretical framework Absorb & Escape (A&E) to sample from the compositional distribution of an AR model and a DM (Section 4.1). Furthermore, as shown in Figure 0(b), we propose an efficient post-training sampling algorithm termed Fast A&E to sample from the composed model, requiring at most one forward pass through the pretrained DM and AR model (Section 4.2).
3. We design a comprehensive evaluation workflow for DNA generation, assessing the sequence composition, diversity, and functional properties of generated genomic sequences. Extensive experiments (15 species, 6 recent DMs, 1 AR model, and 3 types of evaluations) reveal: 1) the limitations of existing models in DNA generation (Section 5.3), and 2) that the proposed algorithm Fast A&E consistently outperforms state-of-the-art models as measured by motif distribution and functional property similarity to natural DNA sequences (Sections 4.2 and 5.3).

## 2 Preliminaries and Related Work

### Problem Formulation: DNA Generation

DNA generation aims to produce synthetic sequences that functionally approximate real DNA sequences. Formally, let \(_{4}=\{1,2,3,4\}\), where each element represents one of the four nucleotides: adenine (A), thymine (T), guanine (G), and cytosine (C). A DNA sequence of length \(L\) can be represented as \(_{4}^{L}\), with each element/nucleotide denoted by \(_{1},_{2},,_{L}\).

**Unconditional Generation:** Given a dataset of real-world DNA sequences \(=\{^{(n)}\}_{n=1}^{N}\) collected from some distribution \(p()\), where each sequence \(^{(n)}_{4}^{L}\) represents a chain of nucleotides, the objective is to develop a generative model \(p_{}()\) of the data distribution \(p()\) from which we can sample novel sequences \(} p_{}()\). These sequences should be structured arrangements of A, T, G, and C, reflecting the complex patterns found in actual DNA. Earlier works applying Generative Adversarial Networks (GANs)  to generate protein-encoding sequences  and functional elements [33; 34; 38] fall into this category.

**Conditional Generation:** In this task, the dataset of DNA sequences \(=\{^{(n)},c^{(n)}\}_{n=1}^{N}\) is sampled from the joint distribution \(p(,c)\), where \(c\) represents the condition associated with each sequence. The objective is to develop a model \(p_{}(|c)\) that generates new DNA sequences \(}\) given condition \(c\). Recently, a discrete diffusion model DDSM  and an AutoRegressive model RegML  have used expression level as the condition, while DNADiffusion  has used cell type as the condition.

### Homogeneous vs. Heterogeneous Sequences

**Homogeneous Generation Process** In the context of sequence generation, a homogeneous Markov Chain is characterized by constant probabilistic rules for generating the sequence at each time step \(t\). More generally, a process is defined as **homogeneous** if the transition probabilities are independent of time \(t\). This means there exists a constant \(P_{,j}\) such that:

\[P_{,j}=[_{t}=j_{1:t-1}=]\] (2)

holds for all times \(t\), where \(P_{,j}\) is a constant, and \(\) represents a specific sequence of past values, i.e., \(=(c_{1},c_{2},,c_{t-1})\).

**Heterogeneous Generation Process** Assuming homogeneous properties simplifies modeling but can be overly restrictive for certain modalities, leading to inaccuracies. For example, DNA sequences consist of various functionally distinct regions, such as promoters, enhancers, regulatory regions, and protein-coding regions scattered across the genome . Each region may be assumed to be homogeneous, but the overall sequence is non-homogeneous due to the differing properties of these elements.

For sequences like DNA, which consist of locally homogeneous segments, we define them as **heterogeneous sequences**. Formally, a heterogeneous sequence is defined as follows: Suppose a sequence \(\) is divided into segments \(_{1},_{2},,_{m}\), where each segment \(_{i}\) is homogeneous. For each segment \(_{i}=(_{t},_{t-1},_{t-2},, _{t-k})\), there exists a constant \(P_{_{i},j}\) such that:

\[P_{_{i},j}=[_{t}=j_{t-k:t-1}=_{i}]\] (3)

holds for all times \(t\) within segment \(_{i}\), where \(P_{_{i},j}\) is a constant, and \(_{i}=(c_{i,t-1},c_{i,t-2},,c_{i,t-k})\) represents values characterizing the history within that segment. While segment \(_{i}\) is homogeneous, the entire sequence is non-homogeneous due to the varying properties across different segments.

## 3 Single Model Limitations in Heterogeneous Sequence Generation

How powerful are AR models and DMs in modelling heterogeneous sequences? We first provide a theoretical analysis, and then perform experiments on synthetic sequences to validate our assumption.

**AutoRegressive (AR) Models** Suppose a heterogeneous sequence \(\) consist of two homogeneous segments of length \(\), then \(=\{\{x_{1},x_{2},,x_{k}\},\{x_{k+1},x_{k+2},,x_{2k}\}\}\). AR models factorize \(p()\) into conditional probability in eq.4; consider the case where the true factorisation of \(p(x)\) follows eq.5.

\[p^{AR}()=p_{}(x_{1})p_{}(x_{2}|x_{1}) p_{}( x_{k}|_{1:k-1}) p_{}(x_{k+1}|_{1:k})p_{}(x_{ k+2}|_{1:k+1}) p_{}(x_{2k}|_{1:2k-1})\] (4)

\[p^{data}()=(x_{1})p_{1}(x_{2}|x_{1})  p_{1}(x_{k}|_{1:k-1})}_{}(x_{k+1})p_{2}(x_{k+2}| _{k+1}) p_{2}(x_{2k}|_{k+1:2k-1})}_{}\] (5)

AR factorisation allows the accurate modelling of the first homogeneous segment; however, it may struggle to disassociate the elements of the second segment from the first segment. More precisely, sufficient data is needed for AR model to learn that \(p_{}(x_{k+1}),p_{}(x_{k+2}),,p_{}(x_{2k})\) should be independent to the elements \(\{x_{1},x_{2},,x_{k}\}\) in the first segments. Secondly, when the context length of the AR model is shorter than the sequence length \(2k\), it could struggle to capture the difference between \(p_{1}\) and \(p_{2}\) with a single set of parameters \(\).

**Diffusion Models (DMs)** On the other hand, DMs estimate the overall probability distribution \(p()\) without factorization. The elements of \(\) are usually generated in parallel. Thus, they do not suffer from the conditional dependence assumption. However, the removal of the conditional dependence assumption may also decrease the accuracy of generation within each homogeneous segment compared to AR models, as DMs do not explicitly consider previous elements.

### A Toy Example

To evaluate the performance of Autoregressive (AR) models and Diffusion Models (DMs) in generating heterogeneous sequences, we consider a toy example with 50,000 heterogeneous sequences \(=\{^{(n)}\}_{n=1}^{50000}\). Each sequence contains 16 segments, as illustrated in Figure2(a), and each segment comprises 16 elements, resulting in a total sequence length of 256 (\(_{4}^{256}\)). A simple Hidden Markov Model (HMM) is used to generate each segment, as shown in Figure2(b), with deterministic transition and emission probabilities that ensure homogeneity within each segment. The emitted tokens differ from one segment to another, mimicking the properties of real DNA

Figure 2: A toy example with heterogeneous sequences: (a) The overall training set consists of \(N=50,000\) heterogeneous sequences, where each sequence further consists of 16 homogeneous segments. We apply an autoregressive and a diffusion model to learn the underlying distribution. (b) Within each segment, the sequences are generated with a simple Hidden Markov Model (HMM), with deterministic transition probability and emission probability.

sequences. Whilst it is possible to use more complex distributions for each segment, doing so could complicate the evaluation of the generated sequences.

**Evaluation** Under our toy HMM setup, a generative model could make two types of mistakes within each segment: **1) Illegal Start Token**: The generated sequence starts with a token which has zero emission probability. E.g. in Figure 2(b), the starting token could only be \(\{A,T\}\). \(\{G,C\}\) at the beginning of the sequence are classified as illegal start tokens. **2) Incorrect Transition**: The generated sequence contains tokens with zero transition probability. E.g. in Figure 2(b) given the start of the sequence is \((A,T,A,T)\), the next token must be \(\), any other tokens such as \(\{T,G,C\}\) are classified as incorrect transitions. We use the number of incorrect tokens as the metric for evaluation.

**Experiment** We use HyperaDNA  as the representative autoregressive (AR) model. For the diffusion model, we develop a simple latent Discrete Diffusion model termed DiscDiff. It resembles the design of StableDiffusion , a latent diffusion model for image generation. DiscDiff consists of a CNN-based Variational Encoder-decoder, trained with cross entropy, to map the discrete DNA data into a latent space, and a standard 2-D UNIT as the denoising network (detailed in appendix A). The training dataset consists of \(=\{^{(n)}\}_{n=1}^{50,000}\). For detailed training procedures see Appendix B. We generate 4,000 sequences from each model and present the evaluation results in Table 1. As expected, the diffusion model DiscDiff makes fewer errors regarding Illegal Start (IS) tokens but tends to generate more Incorrect Transition (IT) tokens. Conversely, while the AR model HyenaDNA generates some IS token errors, it produces significantly fewer IT token errors. This motivates the question: _can we combine the strengths of both algorithms to achieve better sequence generation?_

## 4 Method

```
0: Pretrained AutoRegressive model \(p_{}^{AR}()\) and pretrained Diffusion Model \(p_{}^{DM}()\)
1: Initialize \(}^{0} p_{}^{DM}()\)
2: Set \(t=0\)
3: Assume \(=\{_{1},_{2},,_{n}\}\), where each \(_{k}=\{_{i},_{i+1},,_{j}\}\) is a segment
4:while not converged do
5: Sample a segment \(\{_{1},_{2},,_{n}\}\)
6: Set \(i=_{k}, j=_{k}\)
7:Absorb step:
8:\(}^{}_{i,j} p_{}^{AR}(_{i:j}| _{0:i-1},_{j+1:L}) p_{}^{AR}(_{i:j }|_{0:i-1})\) //Refine segment \(_{k}\) using the AR model
9:Escap step:
10:\(}^{}_{i,j}=}^{}_{i:j}\)//Update \(}^{t}\)
11: Increment \(t=t+1\)
12:endwhile
13:Output: Final sample \(}^{t}\) with improved quality ```

**Algorithm 1** Absorb & Escape Algorithm

```
0: Absorb Threshold \(T_{Absorb}\), Pretrained AutoRegressive model \(p_{}^{AR}()\) and pre-trained Diffusion Model \(p_{}^{DM}()\)
1: Initialize \(}^{0} p_{}^{DM}()\)
2:for\(i\) in \(len(})\) do
3:if\(p^{DM}<T_{Absorb}\)then
4: Absorb step:
5: j = i+1
6:\(}^{}_{j} p_{}^{AR}(_{j}|_{0:i})\)
7:while\(p^{AR}(}^{}_{j})>p^{DM}(}_{j})\)do
8: Increment \(j=j+1\)
9:\(}^{}_{j} p_{}^{AR}(_{j}|_{0:i},_{i:j-1})\) //Refine Inaccurate region of the sequence token by token
10:endwhile
11:Escap step:
12:\(}_{i:j}=}^{}_{i:j}\)//Update \(}\)
13: Increment i = i + j
14:endif
15:endfor
16:Output:\(}\) with improved quality ```

**Algorithm 2** Fast Absorb & Escape Algorithm

    &  & DiscDiff \\  \# IS Tokens \(\) & 812 & **0** \\  \# IT Tokens \(\) & **3,586** & 110,192 \\   

Table 1: **Number of Incorrect Tokens on Synthetic Dataset**. The performance metrics used are the number of Illegal Start (IS) Tokens and Incorrect Transition (IT) Tokens. Note that there are a total of \(4,000 256=1024,000\) tokens.

### The Absorb & Escape Framework

Given a pretrained AutoRegressive model \(p_{}^{AR}()\) and a Diffusion Model \(p_{}^{DM}()\), we aim to generate a higher quality example \(}\) from the composed distribution \(p_{,}^{C}()=p_{}^{AR}() p_{}^{ DM}()\). However, directly computing \(p_{,}^{C}()\) is generally intractable, as both the autoregressive factorizations from \(p^{AR}\) and score functions from \(p^{DM}\) are not directly composable [9; 10]. We propose the **A**bsorb **& Escape (A&E)** framework, as shown in Algorithm 1, to efficiently sample from \(p_{,}^{C}()\).

Absorb-StepInspired by Gibbs sampling , which iteratively refines each dimension of a single sample and moves to higher density areas, our algorithm starts with a sequence \(^{0} p^{DM}()\), generated by the diffusion model and then refines the samples through the Absorb step and Escape step. By exploiting the heterogeneous nature of the sequence, we assume that \(^{0}\) can be factorized into multiple segments \(\{_{1},_{2},,_{n}\}\). For each segment \(_{k}\), we set \(i\) and \(j\) as the start and end indices, respectively. During the Absorb step, we sample a subset of segments \(\{_{1},_{2},,_{n}\}\) and refine each segment \(_{k}\) by sampling \(}_{i:j}^{f} p(_{i:j}|_{0:i-1},_{j+1:L}) p_{}^{AR}(_{i:j}|_{0:i-1})\), using the autoregressive model to approximate the conditional probability.

Escape-StepAfter refining the segment in the Absorb step, we proceed with the Escape step where we update the refined segment \(}_{i:j}^{t}\) to \(}_{i:j}^{t}\). This iterative process continues for each selected segment \(_{k}\), with \(t\) incrementing after each update. By leveraging the ability of the diffusion model to capture the overall data distribution and the autoregressive model to refine homogeneous sequences within each segment, our algorithm efficiently improves the quality of the generated samples. The final output \(}^{t}\) is hereby closer to the true data distribution \(p()\) compared to the initial sample \(}^{0}\). A proof for the convergence in Proposition 1 is provided in Appendix C.

**Proposition 1**.: _The Absorb & Escape (A&E) algorithm converges to the target distribution \(p_{,}^{C}()=p_{}^{AR}() p_{}^{ DM}()\), under the assumptions that both models are properly trained, the segments of \(\) are homogeneous, the subset of segments is chosen randomly, and the conditional distribution \(p(_{i:j}|_{0:i-1},_{j+1:L})\) is accurately approximated by \(p_{}^{AR}(_{i:j}|_{0:i-1})\)._

### Practical Implementation: Fast A&E

While A&E offers a method to sample from a compositional distribution, two practical issues remain unresolved. Firstly, the algorithm may take a considerable amount of time to converge. Secondly, a crucial step in Line 3 of Algorithm 1 involves splitting \(\) into homogeneous segments \(\{_{1},_{2},,_{n}\}\) and then sampling a subset of these segments. Segmentation is straightforward when the boundaries of functional regions of the DNA sequence are known, such as protein-coding regions, exons, or introns, where each region naturally forms a homogeneous segment. However, this information is often unavailable in practice.

To address these challenges, we propose a practical implementation termed Fast A&E. For generating a sequence \(_{4}^{L}\), it requires at most \(L\) forward passes through the AR model. As shown in Algorithm 2 and Figure 0(b), Fast A&E adopts a heuristic-based approach to select segments for refinement. It scans the sequence from left to right, identifying low-quality segments through a thresholding mechanism. Tokens with predicted probabilities smaller than the \(T_{absorb}\) threshold trigger the absorb action, while the autoregressive process terminates once the probability of a token generated by the AR model \(p^{DM}(}_{j}^{})\) is smaller than that of the diffusion model \(p^{AR}(}_{j})\). In this manner, Fast A&E corrects errors made by the diffusion model with a maximum running time of \(O(T_{DM}+T_{AR})\), where \(T_{DM}\) and \(T_{AR}\) are the times required for generating a single sequence from the diffusion model and autoregressive model, respectively.

## 5 Experiment

### Transcription Profile (TP) conditioned Promoter Design

We first evaluate Fast A&E in the task of TP-conditioned promoter design, following the same evaluation procedures and metrics as used by DDSM  and Dirichlet Flow Matching (DFM) .

Data Format & Evaluation Metric:Each data point in this task is represented as a \((,)\) pair, where _signal_ corresponds to the CAGE values for a given DNA sequence, providing a quantitative measure of gene expression around the transcription start site (TSS). Both the DNA sequence and the signal have a length of 1024. The goal of this task is to generate DNA sequences conditioned on specified signals. During evaluation, for a given test set data point \((x,c)\), the generated sequence \(}\) and the ground truth sequence are processed through the genomic neural network Sei . The performance metric is the mean squared error (MSE) between \(Sei(x)\) and \(Sei(})\).

**Results:** As shown in Table 2, we ran Fast A&E on this task with a default threshold of \(T_{}=0.85\), using the pretrained model as both the autoregressive (AR) model and the denoising model (DM) component. The evaluation followed the same procedure as described in the DFM repository. The Fast A&E model, comprising AR and DM components (i.e., the language model and the distilled DFM checkpoints provided in the DFM repository), achieved state-of-the-art results with an MSE of 0.0262 on the test split.

### Multi-species Promoter Generation

#### 5.2.1 Experimental Setup

**Dataset Construction** Prior efforts in DNA generation have been constrained by small, single-species datasets . To better evaluate the capability of various generative algorithms in DNA generation, we construct a dataset with 15 species from the Eukaryotic Promoter Database (EPDnew). Table 3 compares our EPD dataset with those used in previous studies, including DDSM, ExpGAN , and EnhancerDesign . The key advantage of EPD is its diversity in both species types and DNA sequence types. Additionally, although the number of sequences in EPD is on a similar scale to that of DDSM, EPD offers greater uniqueness: each sequence corresponds to a unique promoter-gene combination, a guarantee not provided by the other datasets.

**Baseline Model** We evaluate the state-of-the-art diffusion models for DNA sequence generation: _DDSM_, _DNADiffusion_, _DDPM_, and a AR model _Hyena_. In addition, we implement a VAE with a CNN-based encoder-decoder architecture. Adding UNet as the denoising network to VAE results in another baseline latent diffusion model termed _DiscDiff_. For a fair evaluation, we maximally scale up the denoising networks of each diffusion model to fit into an 40GB NVIDIA A100. Additionally, we adapt four pretrained _Hyena_ models from HuggingFace for comprehensive fine-tuning. The additional details of the network architectures are shown in Appendix D.

Model TrainingAll the models are implemented in Pytorch and trained on a NVIDIA A100-PCIE-40GB with a maximum wall time of 48 GPU hours per model; most of the models converged within the given time. Adam optimizer  is used together with the CosineAnnealingLR  scheduler. The learning rate of each model are detailed in Appendix D. For the evaluation of various diffusion models in unconditional generation (see Section 5.2.2), we sample 50,000 sequences from each model. For

  
**Method** & **MSE\(\)** \\ 
**Bit Diffusion (bit-encoding)*** &.0414 \\
**Bit Diffusion (one-hot encoding)*** &.0395 \\
**D3PM-uniform*** &.0375 \\
**DDSM*** &.0334 \\
**Language Model*** &.0333 \\
**Linear FM*** &.0281 \\
**Dirichlet FM (DFM)*** &.0269 \\
**Dirichlet FM distilled (DFM distilled)*** &.0278 \\ 
**A\&E (Language Model+Dirichlet FM distilled)** & **.0262** \\   

Table 2: **Evaluation of transcription profile conditioned promoter sequence design. A&E achieves the smallest MSE with Language Model and DFM distilled being the AR and DM components.**

   Dataset & \# DNA & Multi Species & DNA Regions \\  EPD (Ours) & **160,000** & \(\) & Reg. \& Prot. \\ DDSM  & 100,000 & \(\) & Reg. \& Prot. \\ ExpGAN  & 4238 & \(\) & Reg. \\ EnhancerDesign  & 7770 & \(\) & Reg. \\   

Table 3: **A comparison of DNA generation datasets. EPD used in this work is significantly larger in size and contains fifteen species. Reg. represents the regulatory regions, and Prot. represents the protein encoding region.**the conditional generation across 15 species (see Section 5.3), we generate 4,000 sequences. In both cases, we use the DDPM sampler  with 1,000 sequential denoising steps.

#### 5.2.2 Evaluating Diffusion Models on Mammalian Model Organisms

One prerequisite of Fast A&E (Algorithm 2) is that the diffusion model \(P_{}^{DM}\) should be properly trained and provide accurate approximations of underlying data distribution. We first evaluate existing Diffusion Models on a subset of EPD datasets. This subset includes sequences from four mammalians _H. Sapiens_ (human), _Rattus Norvegicus_ (rat), _Macaca mulatta_, and _Mus musculus_ (mouse), which collectively represent 50% of the total dataset. Training on this subset allows for a more precise assessment of the generative algorithm's accuracy in a unconditional generation setting.

Metrics
1. **Motif Distribution Correlation (Cor\({}_{}\))** and **Mean Square Error (MSE\({}_{}\))**: Cor\({}_{}\) is the Pearson correlation between the motif distributions of generated and natural DNA sequences for motifs like TATA-box, GC-box, Initiator, and CCAAT-box. MSE\({}_{}\) is the average squared differences between these motif distributions.
2. **S-FID (Sei Frechet Inception Distance)**: Measures the distance between distributions of generated and natural DNA sequences in latent space similar to the FID metric  for images, replacing the encoder with the pre-trained genomic neural network, Sei .

The results are presented in Table 4, indicating that most existing models perform worse than the simple baseline DiscDiff proposed here, as measured by S-FID, Cor\({}_{}\), and MSE\({}_{}\). TATA is one of the most fundamental motifs for gene transcription - a special type of protein called transcription factors binds to TATA tokens on the DNA as shown in Figure 0(a). It changes the shape of DNA and then enables the gene transcription. The failure of existing diffusion models to capture the TATA-box distribution indicates a potential gap in existing research. In the following, we hereby use DiscDiff as the \(p_{}^{DM}\) to initialize the A&E algorithm.

### Multi-species DNA Sequences Generation

We compare our model Fast A&E with _Hyena_ and the best-performing diffusion model from Section 5.2.2, _DiscDiff_, on the task of generating species-specific DNA sequences.

Motif-centric EvaluationWe consider four types of motifs closely related to promoter activities \(\{,,,\}\). We calculate 4 types of motif distributions for 15 species across 3 models, resulting in 180 frequency distributions.

Figure 3 shows the average MSE and Correlation between generated and natural DNA distributions for each model and motif type across 15 species. Fast A&E improves upon _Hyena_ and _DiscDiff_, generating the most realistic sequences across almost all species and motif types. It achieves the lowest MSE and highest Correlation across all four motifs. This pattern is consistent across all 15 species. The motif plots for all 15 species are provided in Appendix F. As an example, Figure 4 shows the motif distributions of sequences generated by the three models versus real DNA sequences

    &  &  \\  Model & S-FID \(\) & Cor\({}_{}\) & MSE\({}_{}\) & S-FID \(\) & Cor\({}_{}\) & MSE\({}_{}\) \\  Random (Reference) & 119.0 & -0.241 & 8.21 & 106.0 & 0.030 & 1.86 \\ Sample from Training Set & 0.509 & 1.0 & 0 & 0.100 & 0.999 & 0 \\  VAE & 295.0 & -0.167 & 26.5 & 250.0 & 0.007 & 9.40 \\ BitDiffusion & 405 & 0.058 & 5.29 & 100.0 & 0.066 & 5.91 \\ D3PM (small) & 97.4 & 0.0964 & 4.97 & 94.5 & 0.363 & **1.50** \\ D3PM (large) & 161.0 & -0.208 & 4.75 & 224.0 & 0.307 & 8.49 \\ D3D (Time Dilation) & 504.0 & 0.897 & 13.4 & 1113.0 & 0.839 & 2673.7 \\ DiscDiff (Ours) & **57.4** & **0.973** & **0.669** & **45.2** & **0.858** & 1.74 \\   

Table 4: Comparison of diffusion models on unconditional generation evaluated on EPD (256 bp) and EPD (2048 bp). Metrics include S-FID, Cor\({}_{}\), and MSE\({}_{}\). The **best** and _second_-best scores are highlighted in bold and underlined, respectively.

for _macaque_. Fast A&E closely resembles the natural motif distribution, especially for the TATA and GC box motifs, while _Hyena_ and _DiscDiff_ fail to capture the values or trends accurately.

**Sequence Diversity** To assess the diversity of the generated sequences, we applied BLASTN  to check (1) the similarity between the training set (Natural DNA) and generated sequences from three models, and (2) the similarity within the generated sequences. BLASTN takes a query DNA sequence and compares it with a database of sequences, returning all the aligned sequences in the database that are similar to the query. For each alignment, an alignment score, the alignment length (AlignLen), and statistical significance (eValue) are provided to indicate the quality of the alignment, where a larger alignment score, a smaller statistical significance (eValue), and a longer alignment sequence (AlignLen) indicate a higher similarity between the query sequence and the database sequences. Ideally, when using generated sequences to query the training dataset, a good generative sequence should align better than a random sequence, but not replicate the sequences in the training set.

Table 5 shows the results of the BLASTN algorithm. From the table, _DiscDiff_, _Hyena_, and A&E all satisfied the mentioned criteria. In terms of the diversity within the groups, none of the algorithms generated repetitive sequences. Furthermore, A&E tends to lie between Hyena and DiscDiff in terms of the diversity of the generated sequences, implying that A&E may combine the properties of AR and DM models. One notable fact is that the alignment scores are very high within the natural sequences, potentially indicating that natural sequences naturally have repetitive properties (conservative motifs), while the generative sequences do not have this characteristic.

**Genome Integration with Promoter Sequences** As shown in Figure 5, to evaluate the functional properties of sequences generated by _Hyena_, _DiscDiff_, and _A&E_, we inserted the generated promoter sequences of length 128 bp upstream (5') of three commonly studied genes in oncology: **TP53**,

Figure 4: **Motif distributions in macaque DNA compared across natural DNA, FAST A&E, DiscDiff, and Hyena**. FAST A&E closely aligns with natural DNA, especially for the TATA and GC motifs.

Figure 3: **The average MSE and Correlation between generated and natural DNA distributions for each model and motif type across 15 species.** Fast A&E outperforms _Hyena_ and _DiscDiff_, generating the most realistic sequences with the lowest MSE and highest Correlation across four motif types. This pattern is consistent across all 15 species.

**Query vs. Database** & **Score\(\)** & **eValue\(\)** & **AlignLen\(\)** \\  Random vs. Natural DNA & 17.78 & 1.1769 & 24.2 \\  A\&E vs. Natural & 21.39 & 0.1695 & 35.9 \\ Hyena vs. Natural & 22.89 & 0.2895 & 40.1 \\ DiscDiff vs. Natural & 20.25 & 0.2098 & 31.4 \\  A\&E vs. A\&E & 20.14 & 0.0968 & 33.69 \\ Hyena vs. Hyena & 19.57 & 0.0843 & 28.7 \\ DiscDiff vs. DiscDiff & 20.95 & 0.1029 & 37.6 \\ Natural vs. Natural DNA & 57.06 & 0.0633 & 77.6 \\  

Table 5: **BLASTN Results**We use Enformer  to predict transcription profiles. Enformer takes a DNA sequence of 200k bps as input and outputs a matrix \(P^{896 638}\), representing a multi-cell type transcription profile. We sampled 300 promoter sequences from each source: _Natural DNA promoters_, _Hyena_, _DiscDiff_, and A&E. For each set, we calculated the average transcription profile across the sequences. The Sum of Squared Errors (SSE) between these average transcription profiles of the generated sequences and those of natural promoters are shown in Table 6. The results indicate that A&E produces the smallest SSE, suggesting it best captures the properties of natural DNA. This finding highlights the potential of generative algorithms to create promoter sequences that effectively regulate gene expression, with applications in bioproduct manufacturing and gene therapy.

#### 5.3.1 Sensitivity Analysis of \(T_{}\)

We perform a _sensitivity analysis_ of A&E algorithm over hyperparameter \(T_{}\). As shown in Figure 6, with a small \(T_{}\), the sequences generated by A&E are dominated by the diffusion model. As \(T_{}\) increases, the AR helps to correct the errors made by the DM. Finally, when \(T_{}\) is larger than 0.7, the correlation flattens and fluctuates. In conclusion, A&E is robust under different values of \(T_{}\), and it is best to use the validation dataset to choose the optimal value. However, a wide range of \(T_{}\) can still be used with improved performance.

## 6 Conclusion

This paper demonstrates that (i) both the AutoRegressive (AR) model and Diffusion Models (DMs) fail to accurately model DNA sequences due to the heterogeneous nature of DNA sequences when used separately, and (ii) this limitation can be overcome by introducing A&E, a novel sampling algorithm that combines AR models and DMs. Additionally, we developed a fast implementation of the proposed algorithm, Fast A&E, which enables efficient generation of realistic DNA sequences without the repetitive function evaluations required by conventional sampling algorithms. Experimental results across 15 species show that Fast A&E consistently outperforms single models in generating DNA sequences with functional and structural similarities to natural DNA, as evidenced by metrics such as Motif Distribution, Sequence Diversity, and Genome Integration. Regarding the future work, the generated DNA sequences still require validation through wet-lab experiments before they can be directly used in clinical settings.

  & **TP53\(\)** & **EGFR\(\)** & **AKT1\(\)** \\  Random & 278.18 & 8.09 & 65.70 \\  A\&E & **17.21** & **0.28** & **1.65** \\ Hyena & 36.25 & 0.89 & 2.88 \\ DiscDiff & 124.03 & 2.17 & 25.50 \\  

Table 6: **Sum of Squared Errors (SSE) of Transcription Profiles between Real and Generated Sequences**

Figure 5: **Evaluation of Generated Promoters for gene regulation through Genome Integration EGFR**, and **AKT1**, which are closely related to tumor activities. Our goal was to determine which model generates promoter sequences that produce gene expression levels closest to those of natural promoters when reinserted into the human genome.

Figure 6: **Sensitivity of A&E under various \(T_{}\). For each value of \(T_{}\), 3000 sequences are generated and compared with natural DNA. The correlation between the generated sequences and natural DNA increases initially as \(T_{}\) increases, and then it flattens. An optimal \(T_{}\) can be selected based on the validation set, or a default value of 0.85 can be used.**