# Unchosen Experts Can Contribute Too:

Unleashing MoE Models' Power by Self-Contrast

Chufan Shi\({}^{1}\) Cheng Yang\({}^{1}\) Xinyu Zhu\({}^{2}\) Jiahao Wang\({}^{3}\)

**Taiqiang Wu\({}^{3}\) Siheng Li\({}^{1}\) Deng Cai\({}^{4}\) Yujiu Yang\({}^{1}\) Yu Meng\({}^{2}\)\({}^{}\)**

\({}^{1}\)Tsinghua University \({}^{2}\)University of Virginia

\({}^{3}\)The University of Hong Kong \({}^{4}\)Tencent AI Lab

scf22@mails.tsinghua.edu.cn

yang.yujiu@sz.tsinghua.edu.cn

Equal Contribution. Source code is available at https://github.com/DavidFanzz/SCMoE.git Corresponding authors.

###### Abstract

Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling model size while maintaining computational efficiency. In MoE, each token in the input sequence activates a different subset of experts determined by a routing mechanism. However, the unchosen experts in MoE models do not contribute to the output, potentially leading to underutilization of the model's capacity. In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality. Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically. Motivated by these findings, we propose **S**elf-**C**ontrast **M**ixture-**of-**E**xperts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference. In SCMoE, the next-token probabilities are determined by contrasting the outputs from strong and weak activation using the same MoE model. Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's reasoning capability across various domains. For example, it improves the accuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with self-consistency yields additional gains, increasing major@20 accuracy from 75.59 to 78.31.

## 1 Introduction

Scaling up model parameters, dataset size and training time has been considered the most direct and effective approach to improving foundation models' performance . However, scaling dense models substantially increases computational costs, which poses a significant practical challenge. Mixture-of-Experts (MoE)  has emerged as a compelling solution for optimizing the balance between model capacity and computation overhead in the era of large foundation models.

MoE models achieve the goal by sparsely activating only a portion of the parameters for each specific input. Specifically, in MoE models, parameters are grouped into a bunch of experts, MoE models only activate some of them for processing a given input. This selective activation is achieved through a routing mechanism that dispatches each input token to a fixed number of experts (e.g, top-\(k\) routing ). Therefore, compared to their dense counterparts, MoE models enjoy more efficient training with significantly reduced computational costs . At the inference stage, they typically adhere to the same routing strategy as the training stage, activating only a small fractionof experts. Basically, for each input token, most of the well-trained experts do not contribute to the output prediction. As a result, the potential of utilizing more experts during the inference stage to enhance performance remains underexplored.

In this paper, we investigate the impact of unchosen experts3 on the performance of MoE models and explore their suitable usage. A direct hypothesis is that incorporating more experts improves MoE models and helps solve more difficult problems [12; 13; 14]. However, in our exploratory experiment on Mixtral 8x7B , we find simply raising the number of activated experts (blue lines in Figure 1) does not lead to stable improvements and may even hurt performance on different tasks. This indicates that unchosen experts may contribute little or even negatively to the final performance, which is contrary to the common perception of unchosen experts as candidates of positive power.

Inspired by the finding, we further dive deep into the difference between the output probability distributions of MoE models applying different routing strategies. As shown in Figure 3, we calculate the Kullback-Leibler Divergence (KLD) between the token distributions obtained from the default top-2 routing and rank-\(k\) routing, and find apparent discrepancy. The discrepancy is particularly evident in the parts that require rigorous reasoning. This suggests that different experts do not always act synergistically; instead, they may exhibit conflicting behaviors.

Therefore, we introduce **S**elf-**C**ontrast **M**ixture-**of-**E**serts (SCMoE), which can convert the negative effects brought by unchosen experts into positive ones through contrasting the output logits obtained using different routing strategies. Specifically, the probability of next token is based on the logits difference between strong and weak activation of the MoE models. For "strong activation" and "weak activation", we use the top-2 routing strategy (Figure 2 (a)) and the rank-\(k\) routing strategy (Figure 2 (b)) respectively. Thus, SCMoE enables unchosen experts to contribute to the prediction. An overview of how SCMoE works is presented in Figure 2 (c).

Experimental results on various benchmarks across different domains demonstrate that SCMoE significantly enhances Mixtral 8x7B's reasoning capability (Section 3). Specifically, compared to greedy decoding, the accuracy increases from 61.79 to 66.94 (+5.15) on GSM8K, 72.83 to 76.29 (+3.46) on StrategyQA, and the pass@1 accuracy increases from 46.20 to 48.80 (+2.60) on MBPP and 33.54 to 41.46 (+7.92) on HumanEval. Further analysis shows that SCMoE can even surpass the result of using self-consistency with major@5 (66.87) on GSM8K. What's more, combining SCMoE with self-consistency can further boost the model's performance, improving major@20 accuracy from 75.59 to 78.31 (+2.72) on GSM8K. Regarding inference efficiency, it turns out that SCMoE incurs only a minor (x1.30) delay compared to greedy decoding, which is competitive among several strong decoding baselines. To sum up, empirical results and comprehensive analyses demonstrate that SCMoE is a both effective and efficient approach to unleashing MoE models' power.

## 2 Method

In this section, we first provide a preliminary introduction of MoE models. Then, we present an analysis based on next-token distribution KLD to reveal the divergence between different routing

Figure 1: Performance comparison between increasing the value of top-\(k\) (_i.e._, ensemble routing) and SCMoE. SCMoE surpasses the performance of ensemble routing across various benchmarks.

strategies in MoE models. This analysis motivates the introduction of SCMoE, a self-contrast method to leverage the contrastive information existing between different routing strategies in MoE models.

### Preliminary

In Transformer-based MoE models, the conventional Feed-Forward Network (FFN) is substituted with the MoE layer . Typically, each MoE layer consists of a router \(R\) and a set of experts \(\{E_{i}\}_{i=1}^{N}\). For a given input sequence \(x_{<t}=(x_{1},x_{2},...,x_{t-1})\), the router allocates each token in \(x_{<t}\) to a specific subset of experts, which are subsequently activated to process the tokens. Specifically, given each token's hidden state \(\), the router first calculates an initial gate value vector \(\) across the \(N\) experts as follows:

\[=(_{r})\] (1)

where \(_{r}\) denotes the weight matrix of the router. Each element \(_{i}\) in \(\) represents the probability of activating the \(i\)-th expert.

After that, the router applies a routing strategy (_e.g._, top-2 or rank-\(k\) routing in Section 2.2) to determine the subset of experts to be activated. Then the \(_{i}\) of the unchoosen expert is set to 0 and \(\) is renormalized to \(}\) accordingly. Subsequently, the output \(\) of the MoE layer is computed as the weighted sum of outputs from the activated experts:

\[=_{i\{j|}_{j} 0\}}}_{i}  E_{i}()\] (2)

Once the input sequence \(x_{<t}\) has undergone a complete forward pass through the MoE model, the next-token distribution \(p(x_{t}|x_{<t})\) is computed based on the output of the final layer. A decoding algorithm is then applied to predict \(x_{t}\) from the vocabulary \(\) based on \(p(x_{t}|x_{<t})\).

### Divergence Between Different Routing Strategies: An Exploratory Analysis

As depicted in Figure 1, unchosen experts may contribute little or even negatively to the final performance. Based on this finding, we are inspired to study the difference of output probabilities using different routing strategies. Specifically, we conduct an analysis on Mikrtal 8x7B , with two different routing strategies, _i.e._, top-2 routing and rank-\(k\) routing, which are detailed as follows.

**Top-2 Routing**. Top-2 routing (Figure 2 (a))  is the default routing strategy of Mikrtal 8x7B, which activates the two experts with the highest values in \(\). In this setting, the renormalized gate value for the \(i\)-th expert, \(}_{i}\), is defined as follows:

\[}_{i}=_{i}}{_{j(w_{i},2)} _{j}},&i(,2)\\ 0,&i(,2)\] (3)

Figure 2: (a & b) Given an input \(\), (a) and (b) demonstrate the workflows of top-2 routing and rank-\(k\) routing (_e.g._, \(k\)=2). We use two MoE layers as a simple schematic, omitting other layers in MoE models. Note that, in the second MoE layer, rank-\(k\) routing activates the unchosen expert in top-2 routing; (c) An illustrative example of how SCMoE works, which contrasts \(z_{}(x_{t}|x_{<t})\) with \(z_{}(x_{t}|x_{<t})\). The complete question and answer for this example are shown in Figure 3.

where \((,2)\) returns the indices of the largest 2 elements in \(\).

**Rank-\(k\) Routing**. The rank-\(k\) routing strategy (Figure 2 (b)) only activates one expert, whose initial gate value is ranked at \(k\) in \(\). The renormalized gate value \(}_{i}\) is defined as follows:

\[}_{i}=1,&i=(,k)\\ 0,&i(,k)\] (4)

where \((,k)\) returns the index of the \(k\)-th largest element in \(\). For Mixtral 8x7B, \(k\) is enumerated from 1 to 8. We employ rank-\(k\) routing as a representative strategy to utilize unchosen experts of top-2 routing (Additional statistics on the utilization ratio of unchosen experts are provided in Appendix B).

Given an input sequence \(x_{<t}\), we denote the next-token distributions using top-2 routing and rank-\(k\) routing as \(p_{}(x_{t}|x_{<t})\) and \(p_{}(x_{t}|x_{<t})\), respectively. Then, we compute the Kullback-Leibler Divergence (KLD) between \(p_{}(x_{t}|x_{<t})\) and \(p_{}(x_{t}|x_{<t})\) on GSM8K dataset. A qualitative illustration is presented in Figure 3 and a more detailed quantitative study is included in Appendix A. As shown in Figure 3, the KLD between \(p_{}(x_{t}|x_{<t})\) and \(p_{}(x_{t}|x_{<t})\) is relatively minor, suggesting a high similarity in their next-token prediction capabilities. However, for \(k\) values ranging from 2 to 8, we identify three key findings:

**Finding 1:**\(p_{}(x_{t}|x_{<t})\) with different \(k\) values exhibits discernible KLD with \(p_{}(x_{t}|x_{<t})\). As \(k\) increases from 2 to 8, the KLD increases accordingly. This finding indicates the overall next-token prediction capability gap between top-2 and rank-\(k\) routing.

**Finding 2:** Apparent KLD is observed when generating reasoning sequences, such as mathematical expressions (_e.g._, "1+2+2+2=5") and the initiation of reasoning steps (_e.g._, "On Thursday"). This suggests notable differences between top-2 and rank-\(k\) routing in generating tokens for reasoning.

**Finding 3:** For the generation of function words (_e.g._, "is") and punctuations (_e.g._, ","), the KLD between \(p_{}(x_{t}|x_{<t})\) and \(p_{}(x_{t}|x_{<t})\) is relatively smaller than that for generating critical reasoning sequences. This indicates that such predictions pose fewer challenges for rank-\(k\) routing.

To sum up, it is observed that, in scenarios demanding reasoning capability for next-token prediction, MoE models with top-2 and rank-\(k\) routing strategies demonstrate distinct generation behaviors. This phenomenon suggests that different experts do not always act synergistically, and could in fact exhibit conflicting behaviors. To harness such information introduced by more experts, a feasible approach is to apply contrastive methods [17; 18] to transform the observed negative impacts into positive ones.

Figure 3: Heatmap of Kullback-Leibler Divergence between the output distribution of top-2 routing strategy ( \(p_{}(x_{t}|x_{<t})\) ) and different rank-\(k\) routing strategies ( \(p_{}(x_{t}|x_{<t})\) ). The \(k\) in rank-\(k\) routing ranges from 1 to 8. The values in the heatmap are scaled by \(10^{5}\). This example is taken from the GSM8K dataset. An additional quantitative study of the KLD is provided in Appendix A.

_Therefore, we propose to leverage the contrastive information existing between different routing strategies of the MoE model (e.g., top-2 routing and rank-\(k\) routing) during inference decoding._

### SCMoE: Self-Contrast Mixture-of-Experts

We introduce **S**elf-**C**ontrast **M**ixture-**of-**E**xperts (SCMoE), an MoE-native self-contrast decoding method. The fundamental idea behind SCMoE is to determine next-token distribution of an MoE model by leveraging the contrastive information between its strong and weak activation, thereby amplifying the desirable behaviors of the strong activation. In this context, "strong activation" and "weak activation" of an MoE model refer to the activations obtained by adopting routing strategies with inherent differences (_e.g._, top-2 routing and rank-\(k\) routing). An MoE model offers flexible combinations of routing strategies that can be applied for strong and weak activation. We consider the case of top-2 routing for strong activation and rank-\(k\) routing for weak activation.

Specifically, in SCMoE, given the output logits of strong and weak activation, we use the following equation to obtain the adjusted logits for next-token prediction:

\[z_{sc}(x_{t}=i|x_{<t})=(1+) z_{}(x_{t}=i|x _{<t})- z_{}(x_{t}=i|x_{<t})&i_{valid}\\ -&i_{valid}\] (5)

where \((0,)\) is a hyperparameter modulating the intensity of the contrastive penalty. \(z_{}(x_{t}|x_{<t})\) and \(z_{}(x_{t}|x_{<t})\) represent the output logits prior to the softmax operation. \(_{valid}\) is a subset of the vocabulary \(\) to restrict the search space:

\[_{valid}=\{i\:|\:z_{}(x_{t}=i|x_{<t})+ _{j}z_{}(x_{t}=j|x_{<t})\}\] (6)

where \((0,1]\) is a hyperparameter to control the size of \(_{valid}\) by masking out tokens that are assigned lower logits. Empirically, \(\) is set to 0.1.

Figure 2 (c) presents an example of how SCMoE works. In this figure, the output logit of "_=" is consistently high across both top-2 and rank-\(k\) routing strategies. Notably, the logit of the ground-truth token "_+" shows an apparent increase with the top-2 routing compared to rank-\(k\) routing. SCMoE capitalizes on this contrast to boost the logit of "_+", thereby generating more accurate output.

## 3 Experiments

### Datasets and Models

To measure the effectiveness of SCMoE, we consider several challenging tasks for LLMs, including mathematical reasoning, commonsense reasoning, and code generation. For mathematical reasoning and commonsense reasoning, we select GSM8K  and StrategyQA  respectively, reporting accuracy. For code generation, we use HumanEval  and MBPP , reporting pass@1 accuracy. We choose Mixtral 8x7B  as our backbone model.

### Setup

As discussed in Section 2.3, in SCMoE, we use Mixtral 8x7B's default top-2 routing as the strong activation. For the weak activation, we only consider the rank-\(k\) routing with \(k=2\). For the penalty strength \(\), we search from \([0.1,0.3,0.5,0.7,0.9]\).

We employ the representative routing-based methods (_i.e._, dynamic and ensemble routing) as the baselines of experts utilization for MoE models. Noting that SCMoE can be seen as a decoding method, we also select commonly used search-based methods (_i.e._, contrastive search, contrastive decoding and Dola) for LLMs as additional baselines. The details of each method are listed below:

Greedy.Greedy chooses the highest probability token at each step.

Dynamic Routing.Inspired by , during inference, the number of activated experts is not fixed. Instead, a threshold is set, and experts are selected in order from highest to lowest scores until the threshold is exceeded. The range of the threshold is \([0.2,0.3,0.4,0.5,0.6]\).

Ensemble Routing.Ensemble routing activates \(k\) experts for inference with greedy search, where \(k\) ranges from 1 to 8. Note that when \(k=2\), it is the same as greedy.

Contrastive Search.Su et al.  use a look-ahead mechanism and penalizes tokens compromising the isotropy of the model's latent space. We search the penalty degree from \([0.3,0.4,0.5,0.6]\).

Contrastive Decoding.Li et al.  search for tokens that maximize the probability difference between the base LLM and an amateur model. We use Mixtral 8x7B as base LLM and Mistral-7B  as the amateur. We search the strength of the amateur penalty \(\) from \([0.1,0.3,0.5,0.7,0.9]\).

DoLa.Chuan et al.  obtain the next-token distribution by contrasting the logits differences between the last layer and a premature layer. The premature layer is dynamically selected from a pre-specified set of layers. Following DoLa , we test two sets of layers: even-numbered layers from \([0,16)\) and from \([16,32)\) respectively.

### Results

Unchosen experts can contribute too.We present the results for each method in Table 1. For dynamic routing, compared with the greedy approach, dynamically selecting the number of experts to use can enhance Mixtral 8x7B's performance except for GSM8K (GSM8K -0.68, StrategyQA +1.58, MBPP +1.60, HumanEval + 4.87). This observation indicates that adopting the same top-2 routing strategy during inference as in the training stage may not be optimal for MoE models. Furthermore, for ensemble routing, incorporating additional experts into inference can also improve performance for each task except for MBPP (GSM8K + 2.05, StrategyQA +1.54, MBPP + 0, HumanEval + 3.66). This findings implies that unchosen experts can be further utilized.

SCMoE unleashes MoE models' power.SCMoE enhances mathematical reasoning by a +5.10 increase on GSM8K, commonsense reasoning by a +3.46 improvement on StrategyQA. Moreover, in code generation, SCMoE gets improvements of +2.60 and +7.92 on the MBPP and HumanEval, respectively. In contrast, traditional search-based methods do not demonstrate substantial improvements on MoE models. In particular, DoLa's performance not only fails to surpass, but actually falls below the greedy baseline, particularly due to its inability to terminate generation sequences appropriately (for specific examples, refer to Table 11 in the appendix). Meanwhile, contrastive decoding with Mistral 7B as the amateur model does not result in consistent improvements, and even a decrease in pass@1 accuracy on MBPP (-1.00). Contrastive decoding necessitates a suitable amateur model for effectiveness [18; 25], but selecting a separate amateur model with same vocabulary is not always feasible. In comparison, SCMoE capitalizes on the MoE models' inherent strong and weak activation to conduct self-contrast. Different weak activation can be viewed as different amateur models, offering higher flexibility and thus help to find the ideal one for contrast.

## 4 Analysis

### Impact of Weak Activation

In our main experiments, we use weak activation with rank-2 routing across all benchmarks. In fact, SCMoE offers the flexibility to employ various routing strategies to determine weak activation. Thus,

  
**Method** & **GSM8K** & **StrategyQA** & **MBPP** & **HumanEval** \\  Greedy & 61.79 & 72.83 & 46.20 & 33.54 \\   \\  Dynamic Routing & 61.11 & 74.41 & 47.80 & 38.41 \\ Ensemble Routing & 63.84 & 74.37 & 46.20 & 37.20 \\   \\  Contrastive Search & 60.96 & 74.85 & 46.20 & 36.59 \\ DoLa & 49.96 & 71.04 & 33.00 & 12.80 \\ Contrastive Decoding & 62.24 & 74.45 & 45.20 & 35.98 \\  SCMoE & **66.94** & **76.29** & **48.80** & **41.46** \\   

Table 1: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with Mixtral 8x7B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 7.

in this section, we further explore the effects of selecting different weak activation. Specifically, we first set rank-\(k\) routing with \(k\) ranging from 1 to 8 as different weak activation and then investigate corresponding performance changes. Besides rank-\(k\) routing, we also consider random-1 routing strategy to serve as an alternative weak activation for SCMoE. In the random-1 routing strategy, at each MoE layer, the router randomly selects one expert to process current input token.

The experimental results for each candidate weak activation are presented in Figure 4. Firstly, compared to the greedy baseline (represented by the red line), there is a noticeable enhancement in GSM8K, StrategyQA and HumanEval regardless of the chosen weak activation in SCMoE. Moreover, when using random-1 routing (represented by the green line), there is still an improvement compared to greedy, which demonstrates the advantage of SCMoE in utilizing its weak activation for self-contrast. Overall, using rank-2 routing as weak activation can provide consistently good performances, and further exploring rank-\(k\) or other routing strategies may bring additional improvements.

### Impact of Strong Activation

As revealed by Figure 1, using default top-2 routing is not optimal for all tasks. For instance, top-3 routing yields best results on GSM8K, while top-4 routing achieves the highest accuracy on HumanEval and StrategyQA. This leads us to consider whether enhancing the strong activation in SCMoE can further unlock MoE models' potential. To this end, we adjust the strong activation of Mistral 8x7B to top-3 for GSM8K, and to top-4 for StrategyQA, MBPP, and HumanEval, while keeping the weak activation with rank-2 routing as before. The experimental results, as shown in Table 2, reveal that enhancing the strong activation of SCMoE can further boost MoE models' performance. Compared to the previous best performance achieved when only utilizing top-2 routing for strong activation, this adjustment improves Mistral 8x7B's performance by 1.98 on GSM8K, 0.13 on StrategyQA, and 1.80 on MBPP.

  
**Method** & **GSM8K** & **StrategyQA** & **MBPP** & **HumanEval** \\  SCMoE & 66.94 & 76.29 & 48.80 & 41.46 \\  SCMoE w/ ideal strong activations & **68.92** & **76.42** & **50.60** & 41.46 \\   

Table 2: Experimental results of different strong activations. We set the weak activation with rank-\(2\) routing. For each benchmark, we select the top-\(k\) routing yielding the best performance in Figure 1 as the ideal strong activation. The specific hyperparameter settings can be found in Table 9.

Figure 4: Experimental results of different weak activations. We set the strong activation with top-2 routing in SCMoE. The detailed results with their hyperparameters are report in Appendix Table 8.

Figure 5: Experimental results on combining SCMoE with self-consistency on GSM8K using Mistrtal 8x7B.

### Combining SCMoE with Self-Consistency

Using self-consistency  for multiple sampling and taking a majority vote to determine the final answer is a common method to improve LLMs' performance. Therefore, we explore whether SCMoE can combined with self-consistency. For vanilla self-consistency, we use temperature sampling with temperature \(=0.7\) to reach the best baseline performance . For self-consistency with SCMoE, we simply employ \(=0.5\), rank-3 routing as weak activation, according to the best hyperparameters setting from Table 8. It is worth noting that since SCMoE already has a mask \(=0.1\) to limit the sampling range of the vocabulary, we do not perform any additional temperature processing on the final logits. As shown in Figure 5, SCMoE (67.94) yields comparable results with major@5 (66.87). Furthermore, SCMoE can enhance the major@20 accuracy from 75.59 to 78.31 (+2.72) on GSM8K.

### Latency

We further evaluate the impact of SCMoE on decoding latency and compare it with other methods on Mixrtal 8x7B. Specifically, we first input 32 tokens to each method and then force them to generate a sequence of 512 tokens to calculate the latency. The results in Table 3 show that SCMoE increases the decoding time by a factor of 1.30x compared to greedy. When compared with other methods, SCMoE does not introduce a significant amount of latency, especially when compared to contrastive search (x1.62) and contrastive decoding (x1.43). Moreover, SCMoE even surpasses the results of using self-consistency with major@5 on GSM8K, which has a 5x latency compared to greedy. Therefore, the latency of SCMoE can be considered negligible, making it both effective and efficient approach.

### Employ DeepSeekMoE

We further explore the adaptability of SCMoE to other MoE models. We conduct experiments on DeepSeekMoE-16B . DeepSeekMoE-16B employs fine-grained expert segmentation and shared expert isolation routing strategies, which is different from Mixrtal 8x7B . We detail the hyperparameters settings of experiments in Appendix C. It is worth noting that contrastive decoding needs a suitable model to serve as an amateur. However, DeepSeekMoE-16B does not have a smaller model with the same vocabulary, so DeepSeekMoE-16B does not have the contrast decoding baseline. As depicted in Table 4, SCMoE effectively unleashes the potential of DeepSeekMoE-16B. Specifically, compared to greedy baseline, SCMoE demonstrates improvements across all tasks: it enhances mathematical reasoning by 1.82 on GSM8K, commonsense reasoning by 2.58 on

  
**Method** & **GSM8K** & **StrategyQA** & **MBPP** & **HumanEval** \\  Greedy & 18.95 & 60.41 & 35.20 & 26.83 \\   \\  Dynamic Routing & 19.71 & 60.63 & 34.80 & 25.00 \\ Ensemble Routing & 19.71 & 60.41 & 35.20 & 26.83 \\   \\  Contrastive Search & 19.94 & 61.77 & 33.40 & 25.00 \\ DoLa & 18.27 & 61.72 & 36.00 & 22.56 \\  SCMoE & **20.77** & **62.99** & **37.20** & **28.05** \\   

Table 4: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with DeepSeekMoE-16B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 10.

  
**Method** & **Greedy** & **Ensemble** & **Dynamic** & **CS** & **DoLa** & **CD** & **SCMoE** \\  Latency (s / 512 tokens) & 50.32 & 59.82 & 54.85 & 81.73 & 53.30 & 72.04 & 65.47 \\ Latency Ratio & x1.00 & x1.19 & x1.09 & x1.62 & x1.06 & x1.43 & x1.30 \\   

Table 3: Averaged decoding latency for each method. CS is short for contrastive search and CD is short for contrastive decoding. We set \(k\) = 3 for ensemble routing, while for dynamic routing we set threshold = 0.5. The speeds are tested on 4 A100 40G with batch size = 1.

StrategyQA, code generation by 2.00 on MBPP, and 1.22 on HumanEval. In contrast, other methods, regardless of routing-based or search-based, struggle to outperform the greedy baseline. These results demonstrate that SCMoE can be successfully applied to other MoE models.

## 5 Related Work

Mixture-of-ExpertsThe Mixture-of-Experts (MoE) model was initially introduced by A. Jacob et al. . Previous studies have demonstrated that sparsely gated MoE models can significantly improve model capacity and efficiency, enabling superior performance compared to dense ones [4; 5; 11; 30]. In MoE models, a static number of experts are activated regardless of the varying complexity presented by input tokens. Typically, top-1 or top-2 experts are activated in these models [15; 10]. In the era of LLMs, numerous extensive open-source models based on MoE architecture have emerged. Specifically, both Mixtral 8x7B  and Grok-1  introduce an 8-expert MoE that uses a top-2 routing algorithm during inference. DeepSeekMoE  and QwenMoE , on the other hand, both employ a fine-grained expert segmentation, applying 2 shared experts with \(N\) routed experts. As a result, they use \(k\)+2 experts for inference, with 2 fixed shared experts and top-\(k\) routed experts.

While several works have attempted to examine pruning or dynamic routing algorithms for MoE models [31; 32; 14] from the perspective of reducing computational costs while maintaining performance. Our approach differs in that we investigate the utilization of unchosen experts in a self-contrast manner to boost MoE models' capability without increasing too much computation.

Contrast in Language ModelingThe idea of employing contrast to enhance language modeling has been explored through various approaches. Specifically, the contrast enables language models to discern between desirable and undesirable behaviors, a distinction that the conventional maximum log-likelihood modeling often fails to adequately capture . One line of research focuses on training-time optimization. Reinforcement learning from human feedback (RLHF) [34; 35; 36] trains reward models by contrasting the rewards associated with desirable outputs to those of undesirable ones, and then optimize the LLM to maximize rewards through reinforcement learning. RRHF , DPO , and PRO  eliminate the necessity of constructing reward models and instead directly optimize LLMs by contrasting preferred responses versus dispreferred ones. Another research avenue focuses on inference-time optimization. DExperts  fine-tunes two models with desirable and undesirable attributes separately, guiding the base model by leveraging the contrast between those models. Contrastive Decoding [18; 25] contrasts base model with an amateur model to mitigate undesirable tendencies of the amateur. Emulated fine-tuning  and proxy-tuning  achieve training-free alignment in a similar way, applying the contrast between aligned and unaligned models as a reward signal to guide the decoding process of a larger unaligned LLM. Contrastive Search  uses a look-ahead contrastive mechanism and penalizes tokens compromising the isotropy of the model's latent space. DoLa  obtains the next-token distribution by contrasting the logits differences between the last layer and a premature layer to improve factuality.

Our research focuses on inference-time optimization. Distinct from the above methods that mainly utilize contrasts between different models, our work leverages the contrastive information among strong and weak activation of MoE models to unleash their potential through self-contrast.

## 6 Conclusion

In this work, we develop Self-Contrast Mixture-of-Experts (SCMoE), a conceptually simple and computationally lightweight strategy to unleash MoE models' power via self-contrast. We find that different routing strategies within an MoE model output results with considerable divergent information. Utilizing this information in a self-contrast manner can further enhance MoE models' reasoning capabilities in next-token prediction. Experimental results show that SCMoE improves the MoE models' performance on multiple benchmarks with only minor latency increase at inference time. Due to resource constraints, our main limitation is that we cannot further explore the performance of SCMoE on larger MoE models such as Mixtral 8x22B or DeepSeek-V2. Overall, SCMoE is a critical step to leverage the inherent self-contrast features of MoE models, and offers new insights to the utilization of unchosen experts.