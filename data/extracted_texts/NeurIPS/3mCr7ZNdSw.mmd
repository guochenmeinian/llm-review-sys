# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

series, and images), scalability to high-dimensional data, and accelerated runtime using GPUs. However, fine-tuning hyper-parameters within these frameworks can be challenging . Additionally, they encounter a dilemma when determining the number of training epochs: with a fixed privacy budget, increasing the number of epochs requires injecting more noise into gradient update per iteration, while fewer epochs might not be sufficient for the optimizer to converge. In such a setting, if the training does not converge, the only recourse may be to increase the privacy budget. This motivates a fundamental question:

_How can we train generative models with DP guarantees while ensuring_

_easy fine-tuning, stable convergence, and high utility?_

In this paper, we introduce a new learning paradigm for training privacy-preserving generative models. Our approach decouples the training process into two steps: (i) computing noisy low-dimensional projections of the private data along random directions, and (ii) updating the generative models to fit these noisy projections. We establish DP guarantees for the first step and leverage the random projection to further tighten our privacy bound. This decoupling strategy offers several advantages. The post-processing property of DP ensures that any deep learning techniques can be applied in the second step. In other words, our approach is model-agnostic, allowing for smooth integration into existing training pipelines of generative models. With our method, data scientists have the flexibility to adjust generator architecture and hyper-parameters, optimize the generative model for any number of epochs, and even restart the optimization--all without worrying about additional privacy costs.

Based on this paradigm, we are motivated to introduce a new information-theoretic measure: the _smoothed-sliced \(f\)-divergence_. This divergence (randomly) projects the original and synthetic data distributions onto lower-dimensional spaces, followed by smoothing with isotropic Gaussian noise, and averaging their \(f\)-divergence over all projections. We prove that using this divergence as the loss function in generative model training is equivalent to the aforementioned two-step training process. Additionally, we present a kernel-based, differentiable estimator for this divergence. It circumvents the need for adversarial training in generative models, thereby enhancing convergence stability and robustness to different choices of hyper-parameters. Finally, we establish the statistical consistency of training generative models using this divergence.

In terms of the slicing mechanism, we build upon the work of . They introduced the smoothed-sliced Wasserstein distance and applied it to generative models and domain adaptation tasks. However, their approach is limited to 1-dimensional projection spaces (\(k=1\)), exploiting the closed-form expression of Wasserstein distances in 1D. Moreover, their privacy analysis contains a significant flaw in its derivation (see Remark 1 for detailed discussions). In contrast, we propose a generic framework for training privacy-preserving generative models, applicable to any \(k\)-dimensional projections. Empirically, we observe that setting the projection dimension to a small number (e.g., \(k=2,3\)) significantly improves the quality of synthetic data compared with \(k=1\) under the same privacy budget. Additionally, our kernel-based estimator eliminates the need for adversarial training in generative models, facilitating stable convergence. Finally, we present a completely new proof for establishing the DP guarantees of our framework. This analysis also applies to any smoothed-sliced divergence objective, for instance, we provide corrected (and improved!) DP bounds for the smoothed-sliced Wasserstein framework of .

We demonstrate the effectiveness of our method through numerical experiments. We compare generative models trained using our method with those trained by standard privacy mechanisms (DP-SGD, PATE, or smoothed-sliced Wasserstein distance) among various real-world datasets. The results indicate that our approach consistently produces synthetic data of higher quality compared with baseline methods.

In summary, our main contributions are:

* We introduce a new framework for training privacy-preserving generative models. It offers easy hyper-parameter tuning and allows for optimizing generative models over any number of training epochs without extra privacy costs.
* We propose a new information-theoretic divergence and provide a kernel-based estimator. This estimator enables the training of generative models without relying on adversarial training, enhancing convergence stability.

* For the slicing mechanism, we extend the work of  and allow for projecting data onto any \(k\)-dimensional space. Using an entirely new proof technique, we provide DP guarantees, which in the \(k=1\) setting correct (and strengthen!) the privacy analysis in .
* We validate our method through numerical experiments. The results show that our method produces synthetic data of higher quality compared with baselines.

### Additional Related Work

Recent work has proposed alternative approaches for training DP generative models without adversarial networks. For example,  considered using the Sinkhorn divergence as the loss function, but their method adds noise to gradient updates to ensure DP, which leads to the challenges discussed in the introduction. Another line of work  used the maximum mean discrepancy (MMD) to train DP generative models. They inject noise into the embedding of the private data distribution to maintain DP. However, minimizing their loss function to zero does not guarantee perfect matching between the synthetic and real data distributions, due to either not using a characteristic kernel or the approximation errors stemming from using finite-dimensional feature mappings to approximate the kernel function. In contrast, Proposition 1 proves that our smoothed-sliced \(f\)-divergence equals zero iff the synthetic and real distributions are identical; Corollary 1 establishes the statistical consistency of training generative models using our loss function. Additionally, we amplify our privacy bound by leveraging random projections in Theorem 1.

There is significant research introducing DP mechanisms tailored to generating tabular synthetic data [e.g., ZCP\({}^{+}17\), MSM19, MMS21, MMSM22, \(^{+}14\), \(^{+}20\), ABK\({}^{+}21\), ZWL\({}^{+}21\), LVW21, VAA\({}^{+}22\), DAHY24]. They select a set of workload queries (e.g., low-order marginal queries) and generates synthetic data to minimize approximation errors on these queries. These methods often maintain statistical properties of the original data with high accuracy, particularly for the selected workload queries; downstream predictive models trained on such synthetic data often achieve high performance when deployed on real data . However, they only apply to categorical features1, rely on special generative model architectures, or struggle to scale effectively to high-dimensional data. More broadly, there are several works analyzing DP synthetic data from a theoretical perspective or investigating other properties of synthetic data (e.g., missing values) .

A burgeoning line of work has explored slicing and smoothing to improve sample complexity in estimating divergence and optimal transport measures. These measures often suffer from extreme curves of dimensionality (e.g., \(n^{-1/d}\) for Wasserstein distance ). Previous studies have shown that both slicing  and smoothing  facilitate convergence at the parametric rate for both \(f\)-divergences and optimal transport distances, while preserving key properties of the original divergence (e.g., being zero iff the two distributions are identical). These modified divergences are also used as objective functions for training generative models. In our context, the use of smoothed-sliced divergence is less motivated by sample complexity (indeed using both smoothing and slicing would be unnecessary for achieving the parametric rate). Instead, it is motivated by the slicing privacy mechanism. Also, while we enjoy the sample complexity benefits of slicing, we do not benefit from the sample complexity improvements of smoothing . This is because achieving DP requires injecting a finite number (typically just one) of noise realizations per data point. Full smoothing results are still useful, however, for our consistency results in the asymptotic regime.

While the 1-dimensional sliced Wasserstein distance can be computed via a simple sorting algorithm , there is in general no closed-form sample-based estimator for \(f\)-divergence, even in one dimension. While nearest neighbor  and kernel-based  estimators do exist, they often suffer from scalability issues and are not friendly to gradient-based optimization. Hence, it is a common practice to use dual forms of \(f\)-divergence in deriving adversarial-based training procedures for generative models . In the present work, we avoid this costly adversarial training using our moment matching estimator.

The supplementary material of this paper includes: (i) omitted proofs of all theoretical results and (ii) supporting experimental results.

Preliminaries and Problem Setup

In this section, we review the concepts of differential privacy (DP), \(f\)-divergence, and a moment matching method used for estimating \(f\)-divergence.

### Differential Privacy

We denote the original dataset as a matrix \(^{n d}\), where \(n\) denotes the number of records and \(d\) represents the number of real-valued features per record.

**Definition 1** (Dataset adjacency).: Two datasets \(\) and \(^{}\) are considered adjacent if they differ in a single row, say the \(i\)-th row, such that \(\|_{i,:}-^{}_{i,:}\|_{2} 1\) where \(_{i,:}\) and \(^{}_{i,:}\) are the \(i\)-th row of \(\) and \(^{}\), respectively.

Next, we recall the definition of differential privacy (DP) .

**Definition 2** (\((,)\) differential privacy).: A randomized mechanism \(:^{n d}\) satisfies \((,)\)-differential privacy if for any adjacent datasets \(\), \(^{}\) and all possible outcomes \(\), we have:

\[(())()(( ^{}))+.\]

DP has many compelling properties. The post-processing property states that if a mechanism \(\) is \((,)\)-DP, its outcome remains \((,)\)-DP even after applying a (potentially randomized) function; the basic composition rule states that given a sequence of mechanisms \(_{1},_{k}\), if \(_{i}\) is \((_{i},_{i})\)-DP, then their composition \(()=(_{1}(),,_{k }())\) will satisfy \((_{i=1}^{k}_{i},_{i=1}^{k}_{i})\)-DP.

### f-divergence and Moment Matching

We first recall the definition of \(f\)-divergence [Chapter 7 in PW23].

**Definition 3**.: Let \(f:(0,)\) be a convex function with \(f(1)=0\) and \(f(0) f(0+)\). Let \(P_{}\) and \(Q_{}\) be two probability distributions on \(\). If \(P Q\), then the \(f\)-divergence is defined as \(_{f}(P\|Q)_{Q}[f()]\) where \(P}{dQ}\) is a Radon-Nikodym derivative. Additionally, we denote the density ratio by \(r(x)Q}(x)\).

\(f\)-divergences have many nice properties. For example, it is always non-negative; and assuming \(f\) is strictly convex at \(1\), then \(_{f}(P\|Q)=0\) if and only \(P=Q\).

There is a burgeoning field of research focusing on \(f\)-divergence estimation [see e.g., GGNWP20, WY20, SXGS20]. Here we revisit a framework based on kernel mean matching, a special instance of moment matching methods [Chapter 3 in SSK12]. Given a reproducing kernel \((,^{})\), one can solve the following optimization problem to estimate the density-ratio function:

\[_{r}\|(,)P()- (,)r()Q()\|_{}^ {2},\]

where \(\|\|_{}\) denotes the norm of a reproducing kernel Hilbert space \(\). One example of reproducing kernels is the Gaussian kernel \((,^{})=(-^{}\|_ {2}^{2}}{2_{g}^{2}})\). Given \(\{_{i}^{p}\}_{i=1}^{n_{p}}\) drawn from \(P\) and \(\{_{j}^{q}\}_{j=1}^{n_{q}}\) drawn from \(Q\), we can optimize an empirical version of the above optimization to obtain an analytical solution:

\[}_{q}=}{n_{p}}_{q,q}^{-1}_{q,p} _{n_{p}},\]

where \(}_{q}^{n_{q}}\) is the (empirically) optimal density ratio values at samples drawn from \(Q\), \(_{q,q}^{n_{q} n_{q}}\) and \(_{q,p}^{n_{q} n_{p}}\) are the kernel Gram matrices:

\[[}_{q}]_{j}=(_{j}^{q}),[_{q,q}]_{j,j^{}}=(_{j}^{q},_{j^{}}^{q} ),[_{q,p}]_{j,i}=(_{j}^{q},_{i }^{p}).\]

We extend the definition of \(f\) to a vector by applying it to each element of the vector. Then we have

\[}_{f}(P\|Q)=}_{n_{q}}^{T}f(}_{q })=}_{n_{q}}^{T}f(}{n_{p}}_{q,q}^{-1}_{q,p}_{n_{p}}).\] (1)Main Results

We introduce a new information-theoretic measure--the smoothed-sliced \(f\)-divergence. We show that this divergence can measure the difference between distributions, with only access to noised \(k\)-dimensional slices of the distributions. This finding motivates a new DP mechanism: the _\(k\)-slicing privacy mechanism_. Finally, using the non-adversarial estimator (1), we apply the smoothed-sliced \(f\)-divergence as a new loss function to train privacy-preserving generative models.

### Smoothed-sliced f-divergence

We start with giving a formal definition of the smoothed-sliced \(f\)-divergence.

**Definition 4**.: Denote the Stiefel manifold of \(d k\) matrices with orthonormal columns by \(_{k}(^{d})\). Let \((_{k}(^{d}))\) and \((,^{2}_{k})\). The smoothed-sliced \(f\)-divergence between distributions \(P_{}\) and \(Q_{}\) on \(^{d}\) is defined as

\[_{f,k,^{2}}(P_{}\|Q_{}) _{f}(P_{^{T}+|}\|Q _{^{T}+|}|P_{})\] \[=(_{k}(^{d}))}_{ _{k}(^{d})}_{f}(P_{ ^{T}+}\|Q_{^{T} {X}+}).\]

Next, we discuss some basic properties of this new divergence.

**Proposition 1**.: _The smoothed-sliced \(f\)-divergence is non-negative: \(_{f,k,^{2}}(P_{}\|Q_{}) 0\) for any \(k 1\) and \( 0\). If \(f\) is strictly convex at \(1\) and \(P_{},Q_{}\) have moment generating functions, then \(_{f,k,^{2}}(P_{}\|Q_{})=0\) if and only if \(P_{}=Q_{}\)._

Given a set of real data \(\{_{i}\}_{i=1}^{n}\) (i.e., rows of \(\)) and synthetic data \(\{_{i}^{}\}_{i=1}^{n_{}}\), let \(}_{f}\) denote any estimator of \(k\)-dimensional \(f\)-divergence (e.g., (1)). We draw random directions \(_{s}\) and additive noise \(_{s,i}\) and \(}_{s,i}\). Then we can estimate the smoothed-sliced \(f\)-divergence by

\[}_{f,k,^{2}}(P_{^{}}\|P_{ })=_{s=1}^{m}}_{f}(\{_{i}^{ }_{s}+}_{s,i}\}_{i=1}^{n_{ }}\|\{_{i}_{s}+_{s, i}\}_{i=1}^{n}).\] (2)

As \(}_{f,k,^{2}}\) is an empirical average of \(m\) estimates of \(k\)-dimensional divergences, this estimator will inherit any statistical convergence bounds that apply to the chosen \(}_{f}\).

### Slicing Privacy Mechanism

The loss function in (2) accesses the original data solely through their noisy projections along random directions. This observation motivates the following \(k\)-slicing privacy mechanism.

**Definition 5**.: Let \(k\) denote the dimension of the random projections, and \(m\) denote the number of them, yielding \(m^{}=mk\). Let \(^{n d}\) represent the original dataset where we assume \(\|_{i,}\|_{2} 1\) for all \(i\). Let \(^{d m^{}}\) and \(^{n m^{}}\) be random slicing and noise matrices with each element independently drawn from \(_{i,j}(0,d^{-1})\) and \(_{i,j}(0,^{2})\), respectively. The slicing privacy mechanism is defined as

\[()(,+).\] (3)

**Remark 1**.: Our mechanism outputs the random slicing matrix \(\), as it will be used to project the synthetic data onto the same spaces during the training of generative models. In contrast,  did not include \(\) in their privacy mechanism, leading them to give an incorrect derivation of the privacy guarantee. Similarly, our approach differs from using the Johnson-Lindenstrauss transform to preserve DP , as these methods do not reveal the random matrix.

In Definition 5, we draw random projections from a Gaussian distribution instead of uniformly from the Stiefel manifold. This choice simplifies the sampling procedure and streamlines our privacy analysis by making the the mechanism output jointly Gaussian with zero mean, conditioned on the data. Next, we establish the DP guarantees for our noisy slicing mechanism.

**Theorem 1**.: _Assume \(^{-2}(^{2}-)<d\). For all \((0,1)\) and \(>1\), the mechanism \(()=(,+)\) in Definition 5 satisfies_

\[(}{2^{2}(d-)}+,)-.\]

_where \((,)\)-DP is as defined in Definition 2 with dataset adjacency specified in Definition 1._

Our privacy bound relies on \(k\) (the dimension of projection spaces) and \(m\) (the number of slices) only through their product \(m^{}=mk\), which reveals a trade-off: with a fixed privacy budget, increasing \(k\) aids generative models in capturing higher-order information of the original data, while increasing \(m\) improves the accuracy of learning lower-order information. This trade-off resembles marginal-based mechanisms, where adjusting \(k\) to enable generative models to learn the \(k\)-way marginal queries [Definition 1 in LVW21] involves a similar effect. However, we remark that even one-dimensional slices suffice to fully characterize a distribution since \(_{f,1,^{2}}(P_{}\|P_{^{}})=0\) implies \(P_{}=P_{^{}}\) by Proposition 1. In contrast, matching all \(k\)-way marginal distributions for \(k<d\) does not necessarily ensure \(P_{}=P_{^{}}\).

We prove in Appendix Proposition 3 that the slicing privacy mechanism is \((}+,)\)-DP if the projection matrix \(\) is deterministic. Comparing it with Theorem 1, we observe that by randomly selecting the projection matrix, we can achieve a tighter privacy bound by reducing a factor of \(\) (for the first-term), even if \(\) is disclosed by the privacy mechanism. The rationale behind this lies in the fact that a deterministic projection matrix allows the model designer/adversary to target specific individual records through carefully designed projection directions.

**Remark 2** (Choosing \(\)).: The bound in Theorem 1 can be optimized in terms of \(\) for a desired fixed \(\) and this optimization can be done numerically. Alternatively, there is an ad hoc strategy for 'approximately optimizing' \(\). Let us assume \( d/2\), i.e. \(^{2}- d^{2}/2\). Then

\[}{^{2}d}+.\]

Minimizing the right hand side expression w.r.t. \(>1\) yields an optimal

\[^{*}=1+d(1/)}{m^{}}}.\]

Substituting it into the (true) expression for \(\) yields

\[^{*}=}{2^{2}(d-^{*})}+(1+})(1/)}{^{2}d }}}{^{2}d}+2(1/)}{ ^{2}d}},\]

where \(^{*}=^{-2}(^{2}-)\).

### Training Generative Models

We outline our approach for training privacy-preserving generative models using the smoothed-sliced \(f\)-divergence (see Algorithm 1 for more details). First, we transform the output of the slicing privacy mechanism \(()=(,+)\) into \(=\{(_{s},_{i,s})\}_{s[m],i[n]}\). Here \(_{s}=_{:,(s-1)k+1:sk}^{d k}\) is the random slicing directions and \(_{i,s}=(+)_{i,(s-1)k+1:sk} ^{k}\) is the (noisy) projection of the \(i\)-th data point onto the \(s\)-th slice.

Let \(G_{}\) be a generative model with trainable parameters \(\). At each iteration, we sample a mini-batch from \(\) as \(\{(_{s},_{i,s})\}_{s[m],i[b]}\), where \(b\) is the batch size.

To produce synthetic data, we draw random noise \(_{j}\) for \(j[b]\) and feed them into the generative model \(_{j}^{}=G_{}(_{j})\). Additionally, we draw random noise \(}_{j,s}(,^{2}_ {k})\) for \(j[b]\) and \(s[m]\) that will be added to the projected synthetic data.2 Given these samples, we train the generative model \(G_{}\) to generate samples close to the real data, using an estimator for the sliced\(f\)-divergence to measure the discrepancy between the noisy-sliced real data \(_{i,s}\) and the noisy-sliced synthetic data \(_{j}^{}_{s}+}_{j,s}\). The smoothed slice \(f\)-divergence (2) yields the following loss function:

\[L()=_{s=1}^{m}}_{f}(\{_{j}^{ }_{s}+}_{s,j}\}_{j=1}^{b}||\{_{i,s}\}_{ i=1}^{b}).\] (4)

The following corollary is a direct application of Proposition 1.

**Corollary 1** (Consistency).: _Consider a dataset \(^{n d}\) of \(n\) i.i.d. samples from a \(d\)-dimensional distribution \(P_{X}\) whose moment generating function exists, and the slicing privacy mechanism \(()\) as in Definition 5. Suppose the noise level \(\) is fixed white \(m\) and \(n\).3 Additionally, suppose that the chosen \(f\)-divergence estimator \(}_{f}\) is consistent and \(f\) is strictly convex at 1. Then \(_{n,m,b}L(^{*})=0\) for some \(^{*}\) if and only if \(G_{^{*}}(Z) P_{X}\)._

Inserting our kernel-based estimator (1) yields for kernel \(\)

\[L()=_{s=1}^{m}_{b}^{T}f(((_{s }+_{b})^{-1}_{s,}_{b})_{+} ),\] (5)

where we compute the kernel Gram matrices \(_{s},_{s,}^{b b}\) along each slice:

\[[_{s}]_{i,i^{}}=(_{i,s},_{i^{ },s}),[_{s,}]_{i,j}=(_{ i,s},_{j}^{}_{s}+}_{j,s}), \;s[m].\] (6)

To ensure the stability of computing matrix inversion, we include a \(_{b}\) where \(>0\) is a small constant. Given that the domain of \(f\) is \([0,)\), we clip the density ratio estimator to make it non-negative.

**Remark 3**.: Note that to compute the kernel Gram matrices \(_{s}\) and \(_{s,}\), we need to apply the Gaussian kernel, which requires a constant \(_{g}\). A heuristic of choosing this \(_{g}\) is to make it the median distance between all samples--note that this will lead to different kernels along different slices. In practice, we use an ensemble of \(_{g}\) and average their density ratio estimators.

``` Input: training data \(=\{_{i}\}_{i=1}^{n}\); slicing dimension \(k\); number of slices \(m\); batch size; max number of iterations \(T\); learning rate \(\); privacy budget \(\), \(\).  Apply the noisy slicing mechanism \(()\) with \((,)\)-DP, finding a noise variance \(^{2}_{(,)}\) and transforming the output into \(\{(_{s},_{i,s})\}_{s[m],i[n]}\). for\(t=1,,T\)do  Sample a mini-batch of data from \(\{(_{s},_{i,s})\}_{s[m],i[n]}\), choosing a batch subset of the \(i\).  Generate synthetic data \(_{}\) by feeding random noise into \(G_{}\).  Slice and noise the synthetic data as \(_{s}^{}=_{s}_{}+_{( ,)}(0,)\), redrawing the noise each time (no privacy cost).  Compute the kernel Gram matrices (6) under Gaussian kernels in each slice.  Compute the loss function \(L()\) in (5).  Run stochastic-gradient optimization \(-_{}L()\). endfor Output: generative model \(G_{}\). ```

**Algorithm 1** Training DP generative modes with the smoothed-sliced \(f\)-divergence.

## 4 Numerical Experiments

We validate our approach and compare it with baselines through numerical experiments. Additional experimental results and details of our setup are reported in Appendix C.

### Synthetic Tabular Data

Baselines.We compare our Algorithm 1 with four DP mechanisms: DP-SGD, PATE, MERF and SliceWass. Like our algorithm, these baselines can handle both numerical and categorical data types without requiring data discretization and return a generative model that can produce any number of synthetic data. The implementations of the first two are adapted from an open-source Python library , and the MERF implementation is from https://github.com/ParkLabML/DP-MERF. For SliceWass, we combine our slicing privacy mechanism with their main algorithm, which fixes the flaw in their privacy analysis and improves their privacy guarantees (and therefore quality results). Additionally, since their public Github repo does not include their synthetic data code, we implement their algorithm ourselves.

Data.We validate both our method and baselines using the US Census data derived from the American Community Survey (ACS) Public Use Microdata Sample (PUMS). Using the API of the Folktables package , we access the 2018 California data. Additionally, Folktables package provides five prediction tasks (Income, Coverage, Mobility, Employment, TravelTime) based

    & &  &  &  \\  Dataset & DP Mechanism & KSComp & TVComp & ContSim & CorrSim & LogitRegression \\   & Algorithm 1 & 0.40 \(\) 0.05 & **0.70**\(\) 0.01 & **0.35**\(\) 0.01 & **0.96**\(\) 0.04 & **0.31**\(\) 0.20 \\   & SliceWass & 0.24 \(\) 0.05 & 0.61 \(\) 0.03 & 0.27 \(\) 0.02 & 0.93 \(\) 0.07 & 0.13 \(\) 0.14 \\   & DP-SGD & 0.29 \(\) 0.06 & 0.38 \(\) 0.02 & 0.10 \(\) 0.01 & 0.75 \(\) 0.07 & 0.00 \(\) 0.00 \\   & PATE & 0.15 \(\) 0.03 & 0.40 \(\) 0.05 & 0.13 \(\) 0.03 & 0.90 \(\) 0.07 & 0.16 \(\) 0.18 \\   & MERF & **0.81**\(\) 0.01 & 0.51 \(\) 0.04 & 0.10 \(\) 0.02 & 0.56 \(\) 0.03 & 0.28 \(\) 0.001 \\   & Algorithm 1 & **0.74**\(\) 0.07 & **0.87**\(\) 0.02 & **0.63**\(\) 0.02 & **0.91**\(\) 0.05 & 0.41 \(\) 0.04 \\   & SliceWass & 0.72 \(\) 0.06 & 0.85 \(\) 0.01 & 0.60 \(\) 0.01 & **0.91**\(\) 0.05 & **0.41**\(\) 0.02 \\   & DP-SGD & 0.44 \(\) 0.06 & 0.63 \(\) 0.09 & 0.35 \(\) 0.10 & 0.73 \(\) 0.15 & 0.24 \(\) 0.28 \\   & PATE & 0.35 \(\) 0.05 & 0.51 \(\) 0.03 & 0.26 \(\) 0.02 & 0.85 \(\) 0.09 & 0.32 \(\) 0.22 \\   & MERF & 0.36 \(\) 0.18 & 0.52 \(\) 0.04 & 0.18 \(\) 0.03 & 0.69 \(\) 0.15 & 0.29 \(\) 0.19 \\   & Algorithm 1 & 0.68 \(\) 0.06 & **0.85**\(\) 0.01 & **0.50**\(\) 0.01 & **0.86**\(\) 0.01 & **0.69**\(\) 0.03 \\   & SliceWass & **0.74**\(\) 0.04 & 0.84 \(\) 0.02 & 0.50 \(\) 0.02 & 0.86 \(\) 0.03 & 0.67 \(\) 0.06 \\   & DP-SGD & 0.52 \(\) 0.05 & 0.70 \(\) 0.06 & 0.34 \(\) 0.06 & 0.74 \(\) 0.13 & 0.67 \(\) 0.19 \\   & PATE & 0.08 \(\) 0.03 & 0.54 \(\) 0.03 & 0.23 \(\) 0.02 & 0.83 \(\) 0.02 & 0.41 \(\) 0.42 \\   & MERF & 0.34 \(\) 0.05 & 0.59 \(\) 0.01 & 0.21 \(\) 0.01 & 0.68 \(\) 0.04 & 0.59 \(\) 0.34 \\   & Algorithm 1 & 0.77 \(\) 0.03 & **0.83**\(\) 0.01 & **0.64**\(\) 0.02 & - & 0.47 \(\) 0.07 \\   & SliceWass & 0.74 \(\) 0.04 & **0.83**\(\) 0.01 & 0.62 \(\) 0.00 & - & 0.50 \(\) 0.10 \\   & DP-SGD & 0.45 \(\) 0.04 & 0.52 \(\) 0.05 & 0.25 \(\) 0.05 & - & 0.46 \(\) 0.31 \\   & PATE & 0.39 \(\) 0.12 & 0.54 \(\) 0.04 & 0.30 \(\) 0.04 & - & 0.38 \(\) 0.28 \\   & MERF & **0.96**\(\) 0.01 & 0.67 \(\) 0.03 & 0.34 \(\) 0.02 & - & **0.67**\(\) 0.03 \\   & Algorithm 1 & 0.45 \(\) 0.03 & **0.75**\(\) 0.01 & **0.45**\(\) 0.01 & 0.87 \(\) 0.05 & **0.42**\(\) 0.12 \\   & SliceWass & 0.33 \(\) 0.01 & 0.62 \(\) 0.02 & 0.33 \(\) 0.01 & 0.84 \(\) 0.05 & 0.42 \(\) 0.14 \\   & DP-SGD & 0.37 \(\) 0.08 & 0.54 \(\) 0.03 & 0.22 \(\) 0.03 & **0.94**\(\) 0.07 & 0.37 \(\) 0.33 \\   & PATE & 0.37 \(\) 0.04 & 0.40 \(\) 0.06 & 0.15 \(\) 0.03 & 0.91 \(\) 0.02 & 0.34 \(\) 0.20 \\   & MERF & **0.61**\(\) 0.04 & 0.37 \(\) 0.02 & 0.07 \(\) 0.01 & 0.62 \(\) 0.02 & 0.14 \(\) 0.28on a target column and a set of mixed-type features. Details about these data, including the number of records and columns, are provided in Table 2 in the appendix.

Evaluation metrics.We follow the evaluation principles in  for assessing the quality of synthetic data and leverage the APIs from an open-source library in our implementation .

* KSComplement (numerical columns) and TVComplement (categorical columns). They measure the (average) similarity of one-way marginals (i.e., histograms of individual columns) between real and synthetic data.
* ContingencySimilarity. It measures the (average) similarity of pairs of categorical columns between real and synthetic data.
* CorrelationSimilarity. It measures the (average) correlations among numerical column pairs and computes the similarity between real and synthetic data.
* BinaryLogisticRegression. It measures the downstream classifier's F-1 score when trained on synthetic and test on real data.

Note that the benchmark datasets contain a limited number of numerical columns, with the majority being categorical (see Table 2). Hence, the applicability of CorrelationSimilarity and KSComplement is constrained as they are tailored for numerical columns.

Main results and observations.We present the experimental results for \(=5.1\) in Table 1 and show results for \(=8.1\) in Appendix C. As shown, the two methods using the slicing privacy mechanism (Algorithm 1 and SliceWass) consistently outperform the other baselines. MERF has better numbers for three instances of KSComp and one of LogitRegression, but is significantly worse in other instances and for the remaining metrics. For both Algorithm 1 and SliceWass, we used the same hyper-parameter initialization and neural network architecture across all datasets. We believe that with more extensive hyperparameter tuning, which incurs no extra privacy cost as previously discussed, their performance could be even further improved.

Algorithm 1 exhibits a more favorable performance compared with SliceWass in most settings. The rationale behind this observation lies in the fact that SliceWass is limited to 1-dimensional slices (\(k=1\)), relying on the closed-form expression of Wasserstein distances in 1D. In contrast, our Algorithm 1 is applicable to higher-dimensional slices. This capability enables the generative models to capture higher-order statistical information, leading to better performance in the pairwise ContingencySimilarity statistic and higher robustness in the BinaryLogisticRegression downstream task.

### Synthetic Image Data

We conduct an experiment to generate DP synthetic image data using the MNIST dataset . We train separate generative models for each of the 10 classes in MNIST, with 10% of the data randomly sampled for each class. We evaluate the quality of the synthetic images by measuring the downstream accuracy of a classifier trained on the synthetic data and deployed on the real MNIST test set. We compare Algorithm 1 and SliceWass (combined with our slicing privacy mechanism), with MERF, which is a state-of-the-art MMD-based method for generating synthetic images . We observe that MERF outperforms the two slicing-mechanism-based algorithms at lower privacy budgets but underperforms at higher budgets. We hypothesize that MERF achieves superior performance in the low-budget regime since it privatizes only the mean embedding of the data for each class; its generative model is designed to recover this mean rather than the full data distribution. As using only the mean involves very little privacy budget, they are able to add less noise and outperform in the low-budget regime. Our approach, which models the full data distribution, does not

Figure 1: We compare accuracy for downstream classification as a function of privacy budget (\(\)) for synthetic MNIST data created by MERF with our Algorithm 1 and SliceWass. Note that the two slicing-mechanism-based approaches outperform MERF for higher privacy budgets.

benefit from these savings. However, as the privacy budget increases, our method better captures the true data distribution, leading to improved results over MERF.

### Domain Adaptation

Following the experiments in , for completeness we also apply our smoothed-sliced \(f\)-divergence in domain adaptation tasks. We benchmark our approach with SliceWass using the implementation available on their Github repo, modified to use our DP bound. This experiment aims to privately train a classifier using labeled data from a source domain and unlabelled data from a target domain. We consider the target and source domains being MNIST and USPS, as well as the reverse. Results for varying \(\) are depicted in Figure 2. As shown, our Algorithm 1 using 100 slices improves on SliceWass.

## 5 Final Remarks and Limitations

In this paper, we consider the slicing mechanism for training privacy-preserving generative models and derive strong privacy guarantees for it. This mechanism motivates us to combine it with \(f\)-divergence to yield a smoothed-sliced \(f\)-divergence that can be estimated with a kernel-based density ratio estimator. Our approach circumvents the need for injecting noise into gradient updates and avoids adversarial training for optimizing generative models. It provides flexibility in selecting neural architectures and tuning hyper-parameters without incurring additional privacy costs. Through experiments on synthetic data generation, we validate our approach and compare it with existing baselines. Our findings suggest that the slicing privacy mechanism is a powerful tool for training generative models to create private synthetic data, and the smoothed-sliced \(f\)-divergence provides a promising avenue for advancing the field of privacy-preserving data synthesis in sensitive, high-dimensional datasets. Additionally, we hope our effort can be of particular interest to the information theory community and open up a new application frontier for classical information-theoretic tools.

There are several promising avenues worth exploring. The \(f\)-divergence has many nice properties, including (strong) data processing inequalities, inequalities among different \(f\)-divergences, and variational representation. It would be interesting to investigate whether similar properties extend to the smoothed-sliced \(f\)-divergence SD\({}_{f,k,^{2}}\). Additionally, deriving sample-complexity bounds for estimating SD\({}_{f,k,^{2}}\) from finite samples would offer valuable insights. Finally, the use of synthetic data presents both opportunities and challenges. On the one hand, synthetic data is well-suited for various tasks like early model development, educational demonstrations, simulation, and testing. On the other, synthetic data can never fully replicate all aspects of the original data, and adding DP guarantees introduces an additional layer of complexity requiring a careful balance between privacy and utility. Therefore, any machine learning models trained on synthetic data or any insights drawn from synthetic data should undergo thorough evaluation before deployment in real-world scenarios. If biases are detected, it is crucial to diagnose their source--whether stemming from the synthetic data generation algorithm, the noise added to the real data, or other factors--to ensure that the models perform robustly and reliably in practice.