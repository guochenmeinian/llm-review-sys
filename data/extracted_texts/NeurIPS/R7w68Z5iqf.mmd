# Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts

Hang Guo\({}^{1}\) Tao Dai\({}^{*}{}^{2}\) Yuanchao Bai\({}^{3}\) Bin Chen\({}^{3}\)

**Xudong Ren\({}^{1}\) Zexuan Zhu\({}^{2}\) Shu-Tao Xia\({}^{1,4}\)**

\({}^{1}\)Tsinghua University \({}^{2}\)Shenzhen University

\({}^{3}\)Harbin Institute of Technology \({}^{4}\)Peng Cheng Lardoratory

https://github.com/csguoh/AdaptIR

Corresponding author: Tao Dai (daitao.edu@gmail.com).

###### Abstract

Designing single-task image restoration models for specific degradation has seen great success in recent years. To achieve generalized image restoration, all-in-one methods have recently been proposed and shown potential for multiple restoration tasks using one single model. Despite the promising results, the existing all-in-one paradigm still suffers from high computational costs as well as limited generalization on unseen degradations. In this work, we introduce an alternative solution to improve the generalization of image restoration models. Drawing inspiration from recent advancements in Parameter Efficient Transfer Learning (PETL), we aim to tune only a small number of parameters to adapt pre-trained restoration models to various tasks. However, current PETL methods fail to generalize across varied restoration tasks due to their homogeneous representation nature. To this end, we propose AdapIR, a Mixture-of-Experts (MoE) with orthogonal multi-branch design to capture local spatial, global spatial, and channel representation bases, followed by adaptive base combination to obtain heterogeneous representation for different degradations. Extensive experiments demonstrate that our AdapIR achieves stable performance on single-degradation tasks, and excels in hybrid-degradation tasks, with fine-tuning only 0.6% parameters for 8 hours.

## 1 Introduction

Image restoration, aiming to restore high-quality images from their degraded counterparts, is a fundamental computer vision problem and has been studied for many years. Due to its ill-posed nature, early research efforts [1; 2; 3] typically focus on developing single-task models, with each model handling only one specific degradation. Consequently, these methods often exhibit limited generalization across different image restoration tasks.

To improve generalization ability, all-in-one image restoration methods [4; 5; 6] have recently been proposed and have attracted great research interest. By training one model with multiple degradation data, these methods enable the single model to handle various degradations. Despite the promising results, the existing all-in-one paradigm still faces several challenges. Firstly, the all-in-one model can only restore degradations encountered during training; once training is complete, the model cannot handle new degradations. Secondly, since the knowledge of restoring multiple degradations is learned by a single model, it incurs a significant cost to train and store these all-in-one models.

In this work, we propose an alternative solution to improve the generalization ability of restoration models in handling multiple degradations. Drawing inspiration from Parameter Efficient Transfer Learning (PETL) [7; 8; 9; 10], we aim to insert a small number of trainable modules into frozen pre-trained restoration backbones. By training only these newly added modules on downstream tasks, the pre-trained restoration backbone can be adapted to unseen restoration tasks. Since only a smallnumber of parameters need to be trained, the training cost is very small and the training process can converge quickly when new tasks are added. When the training is completed, only the newly added parameters need to be stored, thus greatly reducing the storage cost.

Despite the potential of applying PETL techniques to image restoration, our experiments reveal that existing PETL methods can work normally on specific degradation, but fail to generalize across multiple degradations, exhibiting unstable performance when adapted to different restoration tasks. As shown in Fig. 1(a), the most widely used PETL method, Adapter , performs well on the draining task. However, when applying Adapter to the low-light image enhancement task, the adapted model shows significant performance degradation. This phenomenon also occurs with other methods, such as the recent state-of-the-art PETL method FacT (Fig. 1(b)). This confusing phenomenon motivates us to discover possible reasons.

To this end, we design preliminary experiments, in which we fine-tune the pre-trained restoration model  using existing PETL schemes, and then use Fourier analysis  to observe the frequency characteristics of features from these methods. It is observed that the features from current PETL methods exhibit homogeneous representation across different restoration tasks (see Fig. 1(d)). As demonstrated in previous work , different restoration tasks prefer certain representations for optimal results, we thus hypothesize that the performance drop occurs when the representation needed to address one specific degradation does not match the homogeneous representation of existing PETL methods. To verify this hypothesis, we further test current PETL methods using the hybrid degradation task (Fig. 1(c)), which requires heterogeneous representations to handle diverse degradations, and we find all existing approaches suffer severe performance drops. Based on the above experiments, we argue that the homogeneous representation of existing PETL methods hinders stable performance on single degradation tasks and advanced performance on hybrid degradation tasks.

In order to learn heterogeneous representations across tasks, one possible solution is the multi-branch structures, where each branch is designed to learn orthogonal representation bases, and then adaptively combine these bases for specific degradation. Following this idea, we propose AdaptIR, a heterogeneous Mixture-of-Experts (MoE) to adapt pre-trained restoration models with heterogeneous representations across tasks. Our AdaptIR adopts orthogonal multi-branch design to learn local spatial, global spatial, and channel representation bases. Specifically, The Local Interaction Module (LIM) employs depth-separable convolution with kernel weight decomposition to exploit local spatial representation. We then employ the Frequency Affine Module (FAM), which performs frequency affine transformation to introduce global spatial modeling ability. Additionally, the Channel Gating Module (CGM) is adopted to capture channel interactions. Finally, we utilize the Adaptive Feature Ensemble to dynamically fuse these three representation bases for specific degradation. Thanks to the heterogeneous representation modeling, our AdaptIR achieves stable performance on single-degradation tasks and advanced performance on hybrid-degradation tasks.

The contributions of our work are as follows: **(i)** We propose a novel PETL paradigm to improve the generalization for image restoration models, and further investigate the specific challenges when applying existing PETL methods to low-level restoration. **(ii)** We introduce AdaptIR, a custom PETL method that employs a heterogeneous MoE for orthogonal representation modeling. To the best of our knowledge, this is the first work to explore parameter-efficient adaptation for image restoration. **(iii)** Experiments on various downstream tasks demonstrate that AdaptIR achieves robust performance on single degradation tasks and advanced results on hybrid degradation tasks.

Figure 1: (a)&(b) We find directly applying the current PETL methods to image restoration leads to unstable performance on single degradation. (c) The current PETL method suffers sub-optimal results on hybrid degradation which requires heterogeneous representation. (d)&(e) We use Fourier analysis to visualize Adapter and our AdaptIR and find that Adapter exhibits homogeneous frequency representations even when faced with different degradations, while our AdaptIR can adaptively learn degradation-specific heterogeneous representations. We provide more evidence in Appendix I.

Related Work

### Generalized Image Restoration

Image restoration has attracted a lot of research interest in recent years. Due to the challenging ill-posed nature, some early research paradigms typically study each sub-task in image restoration independently and have recently achieved favorable progress in their respective fields [13; 14; 15; 2; 16]. However, designing such a single-task model is cumbersome, and does not consider the similarities among different tasks. Recently, all-in-one image restoration [4; 5; 6] has offered a way to improve the generalization of image restoration models. By training one single model on multiple degradations, it allows the model to have the ability to handle multiple degradations. For example, AirNet  proposes a two-stage training scheme to first learn the degradation representation, which is then used in the following restoration stage. PromptIR  utilizes prompt learning to obtain degradation-specific prompts to train the model in an end-to-end manner. Despite the progress, the current all-in-one restoration paradigm can only deal with degradation seen during training and it is inevitable to re-train the model when needing to add new degradations. In addition, incorporating the knowledge of handling multiple degradations into one model has to increase the model size, which causes large training and storage costs.

### Parameter-Efficient Transfer Learning

Parameter efficient transfer learning, which initially came from with NLP [17; 18; 19; 8; 20; 21; 22; 23], aims to catch up with full fine-tuning by training a small number of parameters. Recently, this technique has emerged in the field of computer vision with promising results [9; 7; 24; 10; 25; 26; 27; 28; 29; 30; 31]. For example, VPT  adds learnable tokens, also called prompts, to the input sequence of one frozen transformer layer. Adapter  employs a bottleneck structure to adapt the pre-trained model. Some attempts also introduce parameterized hypercomplex multiplication layers  and re-parameterisation  to adapter-based methods. Moreover, LoRA  utilizes the low-rank nature of the incremental weight in attention and performs matrix decomposition for parameter efficiency. He et al.  go further to identify all the above three approaches from a unified perspective. In addition, NOAH  and GLoRA  introduce Neural Architecture Search (NAS) to combine different methods. SSF  performs a learnable affine transformation on features of the pre-trained model. FaCT  tensorizes ViT and decomposes the increments into lightweight factors. Although applying PETL methods to pre-trained image restoration models to improve the generalization seems promising, we find that current PETL methods suffer from homogeneous representations when facing different degradations, hindering stable performance across tasks.

## 3 Method

### Preliminary

In this work, we aim to adapt pre-trained restoration models to multiple downstream tasks by fine-tuning a small number of parameters. Following existing PETL works, we mainly focus on transformer-based restoration models since transformer has been shown to be suitable for pre-training  and there is no CNN-based pre-trained model available. As shown in Fig. 2, a typical pre-trained restoration model [11; 34] usually contains one large transformer body as well as task-specific heads and tails. Given the pre-assigned task type, the low-quality image \(I_{LQ}\) will first go through the corresponding head to get the shallow feature \(X_{head}\). After that, \(X_{head}\) is flattened into a 1D sequence on the spatial dimension and is input to the transformer body which contains several stacked transformer blocks with each block containing multiple transformer layers . Finally, a skip connection is adopted followed by the task-specific tail to reconstruct the high-quality image \(I_{HQ}\). During the pre-training stage, gradients from multiple tasks are used to update the shared body as well as the corresponding task-specific head and tail. After pre-training the restoration model, previous common practice fine-tunes all parameters of the pre-trained model for specific downstream tasks, which burdens training and storage due to the per-task model weights.

### Heterogeneous Representation Learning

To obtain stable performance across multiple restoration tasks, it is crucial to allow the learning of heterogeneous representation for different degradations. To this end, we formalize AdaptIR as a multi-branch MoE structure, where each branch learns representations orthogonal to each other to form the representation bases, and then these bases are adaptively combined to achieve degradation-specific representation. Formally, as shown in Fig. 2(a), since the transformer body flattens the \(l\)-th layer feature into a 1D token sequence, we first restore the 2D image structure to obtain \(X_{l}^{C H W}\). After that, we apply the \(1 1\) convolution with channel reduction rate \(\) to transfer \(X_{l}\) to low-dimension space for parameter efficiency and obtain the intrinsic feature \(X_{l}^{intrin}^{ H W}\). Then three parallel branches are orthogonally designed to learn local spatial, global spatial, and channel bases. Next, bases from these three branches are adaptively ensembled to obtain representation \(X_{l}^{adapt}\) for specific degradation. Finally, \(X_{l}^{adapt}\) is added to the output of the frozen MLP to adapt the pre-trained restoration models. Details of the three branches are given below.

**Local Interaction Module.** We first introduce the Local Interaction Module (LIM) to model the local spatial representation. As shown in Fig. 2(b), the proposed LIM is implemented by the depth-wise convolution with weight factorization for parameter efficiency. Specifically, given the convolution weight \(W^{C_{in}}{group} K K}\), where \(C_{in}\), \(C_{out}\) are input and output channel, \(K\) is the kernel size and \(group\) is the number of convolution groups, we first reshape \(W\) into a 2D weight matrix \(W^{}^{C_{in}}{group}K^{2}}\), and then decompose \(W^{}\) into multiplication of two low-rank weight matrices:

\[W^{}=UV^{},\] (1)

where \(U^{C_{in} r}\), \(V^{}{group}K^{2} r}\) and \(r\) is the rank to trade-off performance and efficiency. Then we reshape \(W^{}\) to the original kernel size and use it to convolve \(X_{l}^{intrin}\) to get \(X_{l}^{LIM}\):

\[X_{l}^{LIM}=(W^{}) X_{l}^{intrin},\] (2)

where \(\) denotes convolution operator, and \(()\) transforms 2D matrices into 4D convolution kernel weights.

**Frequency Affine Module.** We then consider modeling global spatial to achieve orthogonal spatial modeling to LIM. A possible solution is to introduce the attention mechanism  which has a global receptive field. However, the attention comes at the cost of high complexity, which goes against the principle of parameter efficiency. In this work, we resort to the frequency domain for a solution. Specifically, we apply the Fast Fourier Transform (FFT) on \(X_{l}^{intrin}\) to obtain the corresponding

Figure 2: An illustration of the proposed AdaptIR. Our AdaptIR is placed parallel to the frozen MLP in one transformer layer and thus can be seamlessly inserted into various transformer-based pre-trained restoration models.

frequency feature map \(X_{l}^{}^{ H(+1)}\):

\[X_{l}^{}(u,v)=_{h=0}^{H-1}_{w=0}^{W-1}X_{l}^{ intrin}(h,w)e^{-2 i(+)},\] (3)

As can be seen from Eq.3, a good property of FFT is that each position of the frequency feature map is the weighted sum of all features in the spatial domain. Therefore, performing pixel-wise projection on \(X_{l}^{}\) is equivalent to performing a global operator in the spatial domain.

Motivated by this observation, we propose the Frequency Affine Module (FAM) to take advantage of the inherent global representation in \(X_{l}^{}\) (see Fig. 2(c)). Concretely, we perform the affine transformation on amplitude map \(Mag_{l}\) and phase map \(Pha_{l}\) respectively with depth-separable \(1 1\) convolution. To ensure numerical stability during the early training stages, we initialize the transformation layers as all-one weights and zero bias. Subsequently, the inverse Fast Fourier Transform (iFFT) is applied to convert the affined feature back to the spatial domain. Finally, another depth-separable \(1 1\) convolution is used as a scale layer for subsequent feature ensemble. In short, the whole process can be formalized as:

\[[Mag_{l},Pha_{l}]=(X_{l}^{intrin}),\\ X_{l}^{FAM}=((( _{1}(Mag_{l}),_{2}(Pha_{l})))),\] (4)

where \(_{1}()\) and \(_{2}()\) are the frequency projection function and \((,)\) converts the magnitude and phase to complex numbers.

**Channel Gating Module.** The above LIM and FAM both adopt the depth-separable strategy for parameter efficiency. To allow for another orthogonal representation, we further develop the Channel Gating Module (CGM) for salient channel selection. As shown in Fig. 2(d), we first obtain the spatial weight mask \(_{l}^{1 H W}\) by employing \(1 1\) convolution which compresses the channel dimension of \(X_{l}^{intrin}\) to 1, followed by the \(\) on the spatial dimension:

\[_{l}=((X_{l}^{intrin})).\] (5)

We then apply \(_{l}\) on each channel of \(X_{l}^{intrin}\) to perform spatially weighted summation to obtain the channel vector which will go through a Feed Forward Network (FFN) to generate the channel gating factor \(X_{l}^{CGM}^{ 1 1}\):

\[X_{l}^{CGM}=(_{h,w}_{l} X_{l}^{intrin}),\] (6)

where \(\) denotes the Hadamard product.

**Adaptive Feature Ensemble.** The orthogonal modeling from local spatial, global spatial, and channel can serve as favorable representation bases, and we further introduce the Adaptive Feature Ensemble to learn their combinations to obtain degradation-specific representations. As shown in Fig. 2(e), we use convolution to compress the channel of \(X_{l}^{LIM}\) to 1 to extract its spatial details, while applying global average pooling and FFN on \(X_{l}^{FAM}\) to preserve the global information. Next, the sigmoid activation is employed to generate dynamic weights for multiplication and produces the adaptive spatial features \(X_{l}^{spatial}\). After that, we use \(X_{l}^{CGM}\) to perform channel selection on \(X_{l}^{spatial}\) to obtain the degradation-specific ensemble feature \(X_{l}^{ensen}^{ H W}\).

At last, a \(1 1\) convolution is employed to up-dimension the \(X_{l}^{ensen}\) to generate \(X_{l}^{adapt}^{C H W}\). For stability, we use zero to initialize the convolution weights. Then, the \(X_{l}^{adapt}\) is added to the output of frozen MLP as residual to adapt pre-trained models to downstream tasks.

### Parameter Efficient Training

During training, we freeze all parameters in the pre-trained model, including task-specific heads and tails as well as the transformer body except for the proposed AdaptIR. A simple \(L_{1}\) loss is employed to provide pixel-level supervision:

\[_{pix}=||I_{HQ}-I_{LQ}||_{1},\] (7)

where \(||||_{1}\) denotes \(L_{1}\) norm.

Experiment

We first employ single-degradation restoration tasks to assess the performance stability of different PETL methods, including image SR, color image denoising, image deraining, and low-light enhancement. Subsequently, we introduce hybrid degradation to further evaluate the ability to learn heterogeneous representations. In addition, we compare with recent all-in-one methods in both effectiveness and efficiency to demonstrate the advantages of applying PETL for generalized image restoration. Finally, we conduct ablation studies to reveal the working mechanism of the proposed method as well as different design choices. Since evaluating the performance stability requires experiments on multiple single-degradation tasks, due to the page limit, the related experiments can be seen in the Appendix E.1.

### Experimental Settings

**Datasets.** For image SR, we choose DIV2K  and Flickr2K  as the training set, and we evaluate on Set5 , Set14 , BSDS100 , Urban100 , and Manga109 . For color image denoising, training sets consist of DIV2K , Flickr2K , BSD400 , and WED , and we have two testing sets: CBSD68  and Urban100 . For image deraining, we evaluate using the Rain100L  and Rain100H  benchmarks, corresponding to light/heavy rain streaks. For evaluation on hybrid degradation, where one image contains multiple degradation types, we choose two representatives, consisting of low-resolution and noise as well as low-resolution and JPEG artifact compression, and we add noise or apply JEPG compression on the low-resolution images to synthesize second-order degraded images. For low-light image enhancement, we utilize the training and testing set of LOLv1 .

**Evaluation Metrics.** We use the PSNR and SSIM to evaluate the effectiveness. The PSNR/SSIM of image SR, deraining, and second-order degradation are computed on the Y channel from the YCbCr space, and we evaluate the RGB channel for denoising and low-light image enhancement. Moreover, we use trainable #param to measure efficiency.

**Baseline Setup.** This work focuses on transferring pre-trained restoration models to downstream tasks under low parameter budgets. Since there is little work studying PETL on image restoration, we reproduce existing PETL approaches and compare them with the proposed AdaptIR. Specifically, we include the following representative PETL methods: **i)** VPT , where the learnable prompts are inserted as the input token of transformer layers, and we compare VPT\({}_{}\) in experiments because of its better performance. **ii)** Adapter , which introduces bottleneck structure placed after Attention and MLP. **iii)** LoRA , which adds parallel sub-networks to learn low-rank incremental matrices of query and value. **iv)** AdaptFormer , which inserts a tunable module parallel to MLP. **v)** SSF , where learnable scale and shift factors are used to modulate the frozen features. **vi)** FacT , which tensorises a ViT and then decomposes the incremental weights. We also present results of **vii)** full fine-tuning (Full-ft), and **viii)** directly applying pre-trained models to downstream tasks (Pretrain), to provide more insights. For readers unfamiliar with PETL, we have also provided a basic background introduction in Appendix A.

**Implementation Details.** We use two pre-trained transformer-based restoration models, _i.e._, IPT  and EDT , as the base models to evaluate different PETL methods. We control tunable parameters by adjusting channel reduction rate \(\). We use AdamW  as the optimizer and train for 500 epochs. The learning rate is initialized to 1e-4 and decayed by half at {250,400,450,475} epochs. All experiments are conducted on four NVIDIA 3080Ti GPUs.

### Comparison on Hybrid Degradation Tasks

In order to obtain convincing evaluation results, it is tedious and time-consuming to observe the stability of one particular PETL method on multiple single-degradation tasks. Here, we introduce hybrid-degradation restoration. Since restoring hybrid degraded images requires a heterogeneous representation of the PETL methods, and thus the hybrid degradation is more suitable for evaluation.

In this work, we consider the second-order degradation as a representative of hybrid degradation. Specifically, we employ two different types of second-order degradations, _i.e._, the \(\)4 low-resolution and noise with \(\)=30 (denoted as LR4&Noise30) as well as the \(\)4 low-resolution and JEPG compression with quality factor \(q\)=30 (denoted as LR4&JPEG30). Moreover, we also include the classic MoE [47; 48; 49], which also employs the multi-branch structure but the design of each branch is the same, to give the impact of the multi-branch structure on the performance.

Tab. 1 gives the results. Consistent with the previous analysis in Fig. 1, existing PETL methods suffer severe performance drops on hybrid degradation tasks due to the difficulty of learning heterogeneous representations. Interestingly, even the simple MoE baseline which only uses the multi-branch structure outperforms the current state-of-the-art PETL methods, suggesting that multi-branch structures are promising for heterogeneity across tasks. However, since each branch of the classical MoE employs the same structure, it struggles to capture orthogonal representation bases from different branches. In contrast, our method achieves consistent state-of-the-art performance across all tasks and on all datasets. For example, our AdapIR outperforms the state-of-the-art PETL method FacT  by 1.78dB on Urban100 with LR4&Noise30, and 0.28dB on Manga109 with LR4&JEPG30. By orthogonally designing branches to obtain representation bases and then adaptively combining them, our AdaptIR allows for heterogeneous representations across different tasks. We also give several visual results in Fig. 3, and our AdapIR can well handle complex degradation.

### Comparison with All-in-One Methods

Recently, all-in-one image restoration methods [5; 4], which learn a single restoration model for various degradations, have shown to be a promising paradigm in achieving generalized image restoration. Here, we compare our AdaptIR with these methods on both single-task and multi-task setups in Tab. 2. For the single-task setting, our method achieves better PSNR results, _e.g._ 0.31dB higher than PromptIR on denoising \(\)=50. In addition, the performance advantage of our AdaptIR still preserves the multi-task setup. For instance, our AdaptIR outperforms PromptIR by even 4.9dB PSNR and 0.016 SSIM on light rain streak removal. This is because all-in-one methods need to learn multiple degradation restoration within one model, resulting in learning difficulties, and the problem of negative transfer among different tasks  can also lead to performance degradation. By contrast, the heterogeneous representation from the orthogonal design facilitates the stable performance of our

    &  &  &  &  &  &  &  \\  & & & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  Full-ft & LR4\&Noise30 & 119M & 27.24 & 0.7859 & 25.56 & 0.6686 & 25.02 & 0.6166 & 24.02 & 0.6967 & 26.31 & 0.8245 \\ Pretrain & LR48Noise30 & - & 19.74 & 0.3569 & 19.27 & 0.3114 & 19.09 & 0.2783 & 18.54 & 0.3254 & 19.75 & 0.3832 \\ SSF  & LR48Noise30 & 373K & 25.41 & 0.6720 & 24.02 & 0.5761 & 24.06 & 0.5411 & 21.89 & 0.5514 & 23.33 & 0.6736 \\ VPT  & LR48Noise30 & 884K & 24.11 & 0.5570 & 24.97 & 0.4722 & 22.91 & 0.4336 & 21.20 & 0.4527 & 22.61 & 0.5570 \\ Adapter  & LR48Noise30 & 691K & 25.60 & 0.6862 & 24.16 & 0.5856 & 24.17 & 0.5498 & 22.05 & 0.5640 & 23.61 & 0.6904 \\ LoRA  & LR48Noise30 & 995K & 25.19 & 0.6371 & 23.82 & 0.5405 & 23.82 & 0.5026 & 21.81 & 0.5193 & 23.30 & 0.6396 \\ Adaptor.  & LR48Noise30 & 677K & 26.10 & 0.7138 & 24.58 & 0.6095 & 24.44 & 0.5686 & 22.52 & 0.5976 & 24.38 & 0.7296 \\ Fact  & LR48Noise30 & 537K & 25.70 & 0.6963 & 24.24 & 0.5944 & 24.25 & 0.5586 & 21.10 & 0.5727 & 23.63 & 0.6993 \\ MoE & LR48Noise30 & 667K & 26.35 & 0.7335 & 24.80 & 0.6254 & 24.59 & 0.5835 & 22.77 & 0.6188 & 24.73 & 0.7517 \\ Ours & LR48Noise30 & 697K & 26.48 & 0.7441 & 24.88 & 0.6345 & 24.67 & 0.6279 & 22.88 & 0.5932 & 24.96 & 0.7625 \\  Full-ft & LR4\&JPEG30 & 119M & 27.21 & 0.7778 & 25.49 & 0.6563 & 25.08 & 0.6076 & 23.54 & 0.6687 & 25.48 & 0.7971 \\ Pretrain & LR48JPEG30 & - & 25.23 & 0.6702 & 24.1AdaptIR across different degradations. As for efficiency, our AdaptIR only trains 0.7% parameters than that of PromptIR with a fast fine-tuning process. We provide a detailed summarization and discussion about the existing multi-task restoration paradigm in Appendix C.

### Discussion

**Why does the Proposed Methods Work?** Our proposed AdaptIR adopts the heterogeneous MoE structure to allow diverse representation learning. Here, we delve deep to verify whether the model design can indeed influence the learned features. For LIM and FAM, we visualize the frequency characteristics of their outputs in Fig. 4. It can be seen that LIM's relative log amplitude at \(\) is 11.02 higher than FAM, suggesting it has learned to capture high-frequency local textures. Meanwhile, more than 95% of energy is centralized within 0.05\(\) for FAM, indicating it can well model low-frequency global structure. For CGM, we visualize the channel activation in Fig. 5, and find large activation differences across channels, with a large variance of 96.10, indicating that the CGM learns to select degradation-specific channels.

**Scaling Trainable Parameters.** We compare the performance of different PETL methods under varying parameter budgets. We use the hybrid degradation LR4&Noise30 in this setup. Fig. 6 shows the results. It can be seen that the proposed method surpasses other strong baselines across various parameter settings, demonstrating the strong scalability of the proposed method.

**How About the Performance on Other Pre-trained Models?** The above experiments employ IPT  as the base model. In order to verify the generalization of the proposed method, we further adopt another pre-trained image restoration model EDT  as the frozen base model. Tab. 4 represents the results. It can be seen that the proposed method maintains state-of-the-art performance by tuning only 1.5% parameters. More experiments with EDT can be seen in Appendix E.2.

### Ablation Study

**Parameter Efficient Designs.** In this work, we introduce several techniques to achieve orthogonal representation learning. Here, we ablate to study the impact of these choices. The results, presented in Tab. 5, indicate that (1) the adaptive feature ensemble can assemble representations according to specific degradation, without which will cause a performance drop. In addition, (2)&(3) removing the

   Method & task & dataset & \#param & training time & GPU memory & PSNR & SSIM \\  AirNet  & light domain & Rain100L & 8.75M & \(\)48h & \(\)11G & 34.90 & 0.977 \\ PromptIR  & light domain & Rain100L & 97M & \(\)84h & \(\)128G & 37.04 & 0.979 \\ Ours & light domain & Rain100L & 697K & \(\)8h & \(\)8G & 37.81 & 0.981 \\  AirNet  & denoise \(\)=50 & Urban100 & 8.75M & \(\)48h & \(\)11G & 28.88 & 0.871 \\ PromptIR  & denoise \(\)=50 & Urban100 & 97M & \(\)84h & \(\)128G & 29.39 & 0.881 \\ Ours & denoise \(\)=50 & Urban100 & 697K & \(\)8h & \(\)8G & 29.70 & 0.881 \\   

Table 2: Comparison with all-in-one image restoration methods under single-task setting. The ‘training time’ of AdaptIR refers to the downstream fine-tuning time excluding the pre-training stage.

depth-separable design in LIM or FAM will conflict with the channel modeling in CGM, and lead to sub-optimal results. Further, (4) removing the CGM branch while allowing full-channel interaction in other branches results in poor performance, which we attribute to the learning difficulty of modeling channel and spatial simultaneously.

**Ablation for Components.** In the proposed AdaptIR, three parallel branches are developed to learn orthogonal bases. We ablate to discern the roles of different branches. As shown in Tab. 6, separate utilization of one or two branches only yields sub-optimal results owing to the insufficient representation. And the combination of the three branches achieves the best results.

**Insertion Position and Form.** There are various options for both the insertion location and form of our AdapIR. The impact of these choices is shown in Tab. 7. It can be seen that inserting AdaptIR into MLP achieves better performance under both parallel and sequential forms. This is because there is a certain dependency between the well-trained MLP and attention, and insertion into the middle of them will damage this relationship. Moreover, the parallel insertion form performs better than its sequential counterpart. We argue that parallel form can preserve the knowledge of frozen features through summation, thus reducing the learning difficulty.

## 5 Conclusion

In this work, we explore for the first time the potential of parameter-efficient adaptation to improve the generalization of image restoration models. We observe that current PETL methods struggle to generalize to multiple single-degradation tasks and suffer from performance degradation on hybrid-degradation tasks. We identify that this issue arises from the misalignment between the degradation-required representation and the homogeneity in current PETL methods. Based on this observation, we propose AdaptIR, a heterogeneous Mixture-of-Experts (MoE) to learn local spatial, global spatial, and channel orthogonal bases under low parameter budgets, followed by the adaptive feature ensemble to dynamically fuse these bases for degradation-specific representation. Extensive experiments validate our AdaptIR as a versatile and powerful adaptation solution.

   LIM & FAM & CGM & \#param & Set5 & Set14 & Urban100 \\   & & \(}}\) & 680K & 23.64 & 22.66 & 20.97 \\  & \(}}\) & \(}}\) & 682K & 25.52 & 24.17 & 22.05 \\  & \(}}\) & \(}}\) & 678K & 26.11 & 24.60 & 22.52 \\  & \(}}\) & \(}}\) & 697K & 26.48 & 24.88 & 22.88 \\   

Table 6: Ablation experiments of different components on PSNR(dB).

   Method & \#param & Set5 & Set14 &  BSDS \\ 100 \\  &  Urban \\ 100 \\  & 
 Manga \\ 109 \\  \\  Full-ft & 11.6M & 27.32 & 25.60 & 25.03 & 24.10 & 26.42 \\ Pretrain & 19.29 & 18.45 & 18.27 & 17.92 & 19.25 \\ SSF  & 117K & 26.9 & 25.25 & 24.83 & 23.41 & 25.77 \\ VPT  & 311K & 24.19 & 22.91 & 22.81 & 21.12 & 22.49 \\ Adapter  & 194K & 26.92 & 25.27 & 24.81 & 23.48 & 25.84 \\ LORA  & 259K & 26.91 & 25.28 & 24.80 & 23.46 & 25.81 \\ AdaptFor.  & 162K & 26.99 & 25.31 & 24.85 & 23.59 & 25.95 \\ FactT  & 174K & 26.89 & 25.25 & 24.81 & 23.43 & 25.78 \\ Ours & 174K & 27.04 & 25.36 & 24.87 & 23.60 & 25.97 \\   

Table 4: Comparison on generalization ability with more pretrained base model.

Figure 6: Scalability comparison with different PETL methods.

   Settings & & \#param & Set5 & Set14 & Urban100 \\  (0)Baseline & 697K & 26.48 & 24.88 & 22.88 \\ (1)+w/o adaptive feature ensemble & 692K & 24.26 & 24.88 & 22.82 \\ (2)+w/o depth-separable in LIM & 718K & 25.67 & 24.22 & 22.10 \\ (3)+w/o depth-separable in FAM & 728K & 25.73 & 24.28 & 22.17 \\ (4)+w/o CGM&&w/o depth-separable & 743K & 24.26 & 23.15 & 21.36 \\   

Table 7: Ablation for different insertion positions and forms on PSNR(dB).