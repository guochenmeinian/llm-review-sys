# Aligning Model Properties via Conformal Risk Control

William Overman\({}^{1}\) Jacqueline Jil Vallon\({}^{2}\) Mohsen Bayati\({}^{1}\)

\({}^{1}\) Stanford Graduate School of Business \({}^{2}\) Management Science and Engineering

{wpo,bayati}@stanford.edu, jjvallon@alumni.stanford.edu

###### Abstract

AI model alignment is crucial due to inadvertent biases in training data and the underspecified machine learning pipeline, where models with excellent test metrics may not meet end-user requirements. While post-training alignment via human feedback shows promise, these methods are often limited to generative AI settings where humans can interpret and provide feedback on model outputs. In traditional non-generative settings with numerical or categorical outputs, detecting misalignment through single-sample outputs remains challenging, and enforcing alignment during training requires repeating costly training processes. In this paper we consider an alternative strategy. We propose interpreting model alignment through property testing, defining an aligned model \(f\) as one belonging to a subset \(\) of functions that exhibit specific desired behaviors. We focus on post-processing a pre-trained model \(f\) to better align with \(\) using conformal risk control. Specifically, we develop a general procedure for converting queries for testing a given property \(\) to a collection of loss functions suitable for use in a conformal risk control algorithm. We prove a probabilistic guarantee that the resulting conformal interval around \(f\) contains a function approximately satisfying \(\). We exhibit applications of our methodology on a collection of supervised learning datasets for (shape-constrained) properties such as monotonicity and concavity. The general procedure is flexible and can be applied to a wide range of desired properties. Finally, we prove that pre-trained models will always require alignment techniques even as model sizes or training data increase, as long as the training data contains even small biases.

## 1 Introduction

The emergence of large foundation models has increased the attention to the problem of alignment. Aligned models are artificial intelligences designed to pursue goals that align with human values, principles, and intentions (Leike et al., 2018; Ouyang et al., 2022; Hendrycks et al., 2023; Ngo et al., 2024). Although the alignment problem is predominantly examined in the context of potential artificial general intelligence (AGI), large language models (LLMs), and reinforcement learning (RL) agents, it also has roots in the modern machine learning pipeline (D'Amour et al., 2022). Motivated by this, we introduce a broader notion of alignment in this paper, extending beyond the aforementioned generative models to include even tabular regression models.

As an example, consider a regression task where property \(\) represents models that are monotonically decreasing in a given feature (covariate). For example, predicting cancer patient survival should be monotonically decreasing in cancer stage (Vallon et al., 2022, 2024). Constraining a prediction model during training to maintain monotonicity in this feature can be viewed as a form of alignment. For a pre-trained model \(f\) that was trained without such constraints, ensuring monotonically decreasing predictions in this feature can be more complex. This complexity arises particularly in non-generativesettings where the user cannot update \(f\) or obtain any outputs other than point predictions \(f(X)\) for a given input \(X\).

In this work, we propose an approach to aligning a pre-trained model \(f\) that is motivated by property testing (Ron, 2008; Goldreich, 2017) and conformal risk control (Angelopoulos et al., 2024). Property testing aims to design efficient algorithms for determining membership to the set \(\) of functions with a given property, that require fewer resources than learning algorithms for \(\)(Ron, 2008). This is particularly relevant for modern deep learning, where a user may need to determine if a pre-trained model \(f\) belongs to \(\) without the resources to train a model of comparable size.

Property testing algorithms use local queries to determine, with high probability, whether a function has a given global property or is far from having it. We map such queries for a property \(\) to a set of loss functions, which we then use in a conformal risk control procedure (Angelopoulos et al., 2024) to establish a notion of alignment for \(\). We prove that this procedure yields a conformal interval around \(f\) containing a function close to \(\).

We demonstrate our methodology on real-world datasets for the properties of monotonicity and concavity. Motivated by the potential for systematic under- or over-estimation bias in \(f\), we provide a straightforward extension of Angelopoulos et al. (2024) to obtain asymmetric conformal intervals with multi-dimensional parameters. While we examine both monotonicity and concavity constraints, the majority of our focus is on monotonicity, as these constraints have been shown to promote crucial aspects of alignment to human values, such as fairness and adherence to social norms (Wang and Gupta, 2020).

While our methodology provides a way to align pre-trained models, one may question whether such techniques will remain necessary as AI capabilities advance. Given the outstanding capabilities of modern AI models with substantially large numbers of parameters and training data, one may argue that the alignment problem may naturally disappear as such advances continue (Kaplan et al., 2020). However, another contribution of this paper is to refute this argument in a stylized setting, building on recent advances in the theory of linearized neural networks (Mei and Montanari, 2023). Specifically, we show that increasing the size of the training data or the number of parameters in a random feature model (a theoretically tractable neural network proxy where hidden layer weights are randomly initialized and fixed (Rahimi and Recht, 2007)) cannot help it satisfy a property \(\), if the pre-training data has biased labels. Our simulations show that the result holds even if only a small fraction of the training labels are impacted by the bias.

Summarizing our main contributions, we: (1) introduce an alignment perspective based on property testing, (2) use conformal risk control to post-process predictions of pre-trained models for better alignment, and (3) demonstrate that increasing training data and parameters in a random feature model does not eliminate the need for alignment. We discuss related work in Section 6, particularly our connections to Yadkori et al. (2024), who use conformal risk control to address large language model hallucinations (Ji et al., 2023).

## 2 Preliminaries

In this section we provide key definitions drawn from property testing as well as a condensed overview to conformal prediction and conformal risk control. We provide a short introduction to propery testing in the extended version of the paper Overman et al. (2024) and an extensive introduction to the field can be found in Goldreich (2017).

### Properties and Property Testing for Set-Valued Functions

Our perspective on alignment in this work is motivated by the field of property testing (Goldreich, 2017; Ron, 2008). Property testing studies algorithms that, by making a small number of local queries to a large object (such as a function or a graph), can determine whether the object has a certain property or is significantly far from having it.

Classic examples include linearity testing of Boolean functions (Blum et al., 1993), testing whether a function is a low-degree polynomial (Kaufman and Ron, 2006; Bhattacharyya et al., 2009), and testing \(k\)-juntas (Blais, 2009). These algorithms generally operate by randomly sampling and querying the object, leveraging local information to infer global properties.

In this work, we focus on _set-valued functions_, which are functions that map elements of a domain \(\) to subsets of a codomain \(\), i.e., \(F: 2^{}\). While the standard definitions of property testing are technically sufficient for our purposes--since we can consider set-valued functions as functions with range \(2^{}\)--we introduce specialized definitions to maintain clarity and to facilitate the transition between discussing \(\) and \(2^{}\).

**Definition 1** (Satisfying and Accommodating a Property).: _Let **property**\(\) denote a specific subset of all functions that map \(\) to \(\). A function \(f:\)**satisfies** the property \(\) if \(f\). A set-valued function \(F: 2^{}\)**acommodates** a property \(\) if there exists a function \(g\) such that \(g(x) F(x)\) for all \(x\)._

Intuitively, \(F\) accommodates \(\) if it contains at least one function \(g\) satisfying \(\) within its possible outputs.

We extend the notion of \(\)-farness from a property (as defined in the extended version of the paper Overman et al. (2024)) to set-valued functions. For set-valued functions, we measure the distance based on how often the outputs of any function \(g\) fall within the sets provided by \(F\).

**Definition 2** (\(\)-Faraway).: _For a set-valued function \(F: 2^{}\), a distribution \(\) over \(\), \(>0\), and a property \(\), we say \(F\) is \(\)**-Faraway** from \(\) with respect to \(\) if \(_{,}(F)>\), where_

\[_{,}(F)}}{{=}} _{g}_{}(F,g)_{ }(F,g)}}{{=}}_{ }[g(X) F(X)].\]

**Note.** Throughout this work, we assume that \(\) is the empirical distribution of a fixed and finite calibration dataset, and thus has finite support. While this assumption is not strictly necessary, most property testing results are over finite domains. Property testing over functions with Euclidean domains is a in general a difficult problem, though there have been notable recent successes (Fleming and Yoshida, 2020; Arora et al., 2023)..

With these definitions in place, we can define _testers_ for set-valued functions. We focus on _one-sided error testers_, which are algorithms that take in a set-valued function \(F\), a distribution \(\), and a distance parameter \(\) and output either Accept or Reject. These algorithms never reject a function that accommodates the property. The standard definition of one-sided error testers (provided in the extended version of the paper Overman et al. (2024)) extends naturally to set-valued functions by replacing the notion of satisfying a property with accommodating it.

**Definition 3** (One-Sided Error Tester for Set-Valued Functions).: _A one-sided error tester for a property \(\) in the context of set-valued functions is a probabilistic oracle machine \(\) that, given a distance parameter \(>0\), oracle access to a set-valued function \(F: 2^{}\), and oracle access to samples from a fixed but unknown distribution \(\) over \(\), satisfies:_

1. _If_ \(F\) _accommodates_ \(\)_, then_ \([^{F,}()=]=1\)_._
2. _If_ \(F\) _is_ \(\)_-Faraway from_ \(\) _with respect to_ \(\)_, then_ \([^{F,}()=]\)_._

_Here, \(^{F,}()\) denotes the execution of the tester \(\) when given oracle access to the function \(F\), the distribution \(\), and the parameter \(\)._

Note that \(\) itself is an abstract algorithm; \(^{F,}\) is the instantiation of this algorithm with specific oracle access to \(F\) and \(\).

In many property testing algorithms, the parameter \(\) is used only to determine the number of iterations or samples required, not the core logic of the tester. This leads to the concept of _proximity-oblivious testers_ (POTs), where the basic testing procedure is independent of \(\). The general definition of POTs (given in the extended version of the paper Overman et al. (2024)) also extends naturally to set-valued functions.

**Definition 4** (Proximity-Oblivious Tester for Set-Valued Functions).: _A proximity-oblivious tester for a property \(\) in the context of set-valued functions is a probabilistic oracle machine \(\) that satisfies:_

1. _If_ \(F\) _accommodates_ \(\)_, then_ \([^{F,}=]=1\)__
2. _There exists a non-decreasing function_ \(:(0,1](0,1]\) _(called the_ detection probability_) such that if_ \(F\) _is_ \(\)_-Faraway from_ \(\)_,_ \[[^{F,}=]().\]_Here, \(^{F,}\) denotes the execution of the tester \(\) when given oracle access to the function \(F\) and the distribution \(\)._

To obtain a one-sided error tester with parameter \(\), we can make \(()\) independent calls to the POT \(\) and accept if and only if all the calls accept (Goldreich and Ron, 2008). We denote by \(^{F,}(X)\) the output when applied to a specific sample \(X\), and note that with abuse of notation we will late consider \(\) to be the empirical distribution of calibration dataset \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) in which case we write \(^{F,}(X_{i},Y_{i})\) for the output on this specific sample from \(\).

Example.Consider functions \(f:^{d}\), and let \(\) denote the property that \(f\) is constant in the \(k\)-th dimension. This property has has connections to fairness among other applications Caton and Haas (2024). Assume \(\) is the empirical distribution of the inputs \(X\) for some fixed dataset.

Restrict to set-valued functions \(F\) that output compact and connected intervals of the form \([a,b]\) for \(a,b\). The candidate POT \(^{F,}\) for whether such a set-valued function \(F\) accommodates \(\) is then as follows: sample \(X,X^{}\), If \(F(X) F(X^{})\), then Accept; otherwise, Reject. We prove that this satisfies Definition 4 in the extended version of the paper Overman et al. (2024).

### Conformal prediction and conformal risk control

Our main tool for achieving alignment from this property perspective is built on conformal prediction and conformal risk control (Vovk et al., 2005; Bates et al., 2021; Angelopoulos et al., 2024). Conformal prediction post-processes the outputs of any model \(f\) to create prediction intervals \(C()\) that ensure certain statistical coverage guarantees. Using a calibration dataset \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) consisting of ground truth input-output pairs, conformal prediction constructs intervals around the predictions of \(f\) such that \([Y_{n+1} C(X_{n+1})]\) for a user-specified error rate \(\) on a test point \((X_{n+1},Y_{n+1})\).

This guarantee is notably distribution-free and holds for any function \(f\). The probability is over the randomness in all \(n+1\) points; both the calibration set and the test point. The construction of \(C()\) depends on both the model \(f\) and the draw of the calibration data.

The conformal risk control framework extends conformal prediction to notions of error beyond miscoverage (Angelopoulos et al., 2024). Consider a paramater set \(_{ 0}\) that is a bounded subset of the nonnegative reals. Given an exchangeable collection of non-increasing, random loss functions \(L_{i}:(-,B]\), \(i=1,,n+1\), conformal risk control uses the first \(n\) loss functions and calibration data \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) to determine \(\) such that

\[[L_{n+1}()].\]

Consider loss functions of the form \(L_{i}()=(C_{}(X_{i}),Y_{i})\), where \(C_{}(X_{i})\) is a set of outputs constructed by \(f\) and the calibration data. Larger values of \(\) generate more conservative prediction sets \(C_{}()\). Let the risk on the calibration data for a given \(\) be \(_{n}()=_{i=1}^{n}L_{i}()\). For a user-specified risk rate \(\), we let

\[=\{:_{n}()+\}.\]

This choice of \(\) guarantees the desired risk control \([L_{n+1}()]\)(Angelopoulos et al., 2024).

## 3 Conformal property alignment

Our main methodology is to use conformal risk control to create prediction intervals that align with specific properties \(\). Our approach allows us to post-process the outputs of a pre-trained model \(f\) to ensure that within the resulting conformal band, with a given probability, there exists predictions that adhere to desired properties such as monotonicity.

### Multi-lambda conformal risk control

We make particular use of the conformal risk control algorithm to allow for a \(k\)-dimensional vector of tuning parameters \(=(_{1},_{2},,_{k})\), where larger values of \(^{k}\) yield more conservative outputs, where \(_{ 0}^{k}\) is a bounded subset of \(_{ 0}^{k}\). This works by mapping \(\) to a scalar and then applying standard conformal risk control. We emphasize that this result is not new and follows essentially directly from Angelopoulos et al. (2024). The construction of the output set \(F_{}(X)\) depends on the specific application and provides flexibility in how the function \(f(X)\) and the parameters \(\) are utilized.

**Definition 5** (Construction of \(F_{}(X)\)).: _Let \(f:\) be a given function. For each \(^{k}\), define the set-valued function \(F_{}: 2^{}\) such that, for each \(X\), \(F_{}(X)\) is a set of predictions for \(X\) constructed from \(f\) and \(\). The specific construction of \(F_{}(X)\) should satisfy the following properties:_

1. _When_ \(=\)_, we have_ \(F_{}(X)=\{f(X)\}\)_._
2. _For any_ \(,^{}^{k}\)_, if_ \(^{}\) _(i.e.,_ \(_{i}_{i}^{}\; i=1,2,,k\)_), then_ \(F_{}(X) F_{^{}}(X)\)_._

This definition ensures that increasing the parameters \(\) leads to larger (more conservative) prediction sets, and that when all parameters are zero, the prediction set reduces to the point prediction given by \(f(X)\).

Following the original scalar \(\) setting, we assess \(F_{}\) using non-increasing random loss functions \(L_{i}=(F_{}(X_{i}),Y_{i})(-,B]\) for \(B<\). In particular, we consider an exchangeable collection of non-increasing random functions \(L_{i}:(-,B],i=1,...,n+1\), where \(_{ 0}^{k}\) is a bounded subset of \(_{ 0}^{k}\), with bound \(_{j}^{}\) in each dimension \(j[k]\).

As in Angelopoulos et al. (2024), we use the first \(n\) functions to determine \(}\) so that the risk on the \((n+1)\)-th function is controlled, specifically so that \([L_{n+1}(})]\).

We apply a similar algorithm. Given \((,B)\) and letting \(_{n}()=()++L_{n}() }{n}\), define

\[_{}=\{:_{n}( )+\}\]

to be the set of minimal elements (Boyd and Vandenberghe, 2004) of \(\) that satisfy the condition \(_{n}()+\). Let \(g:\) be a strictly increasing function such that \(L_{i}()\) is non-increasing with respect to the level sets defined by \(g()\). Then select \(}_{}\) to be a minimizer of \(g\) over \(_{}\).

We then deploy the resulting set-valued function \(F_{}}\) on the test point \(X_{n+1}\). For this choice of \(}\), we have a risk control guarantee that mimics the result of Angelopoulos et al. (2024), specifically:

**Proposition 1**.: _Assume that \(L_{i}()\) is non-increasing with respect to the partial ordering of \(\) inherited from \(^{k}\). Additionally, assume that \(L_{i}()\) is non-increasing with respect to \(g()\) for some strictly increasing function \(g:\). Also assume \(L_{i}\) is right-continuous in each dimension, \(L_{i}(^{})\), and \(_{}L_{i}() B<\) almost surely. Then_

\[[L_{n+1}(})].\]

The proof is similar to the proof of the guarantee for the conformal risk control algorithm in Angelopoulos et al. (2024) and is deferred to the extended version of the paper Overman et al. (2024)

To provide intuition on \(g()\), we note that for our primary use case we will take \(g()=_{i=1}^{k}_{i}\). Clearly this function is strictly increasing in \(\) and intuitively it is reasonable to consider loss functions \(L_{i}\) that are non-increasing as the sum of the components of \(\) increases.

### Conformal property alignment from proximity oblivious testers

We now demonstrate how to construct a conformal risk control problem using proximity-oblivious testers (POTs) for a given property \(\). Suppose we are given a pre-trained model \(f:\). We aim to extend the point predictions of \(f\) to prediction sets, where the size or conservativeness of the set is parameterized by a parameter \(\). Let \(F_{}: 2^{}\) denote the set-valued function that outputs, for each \(X\), the set \(F_{}(X)\) determined by \(f\), \(X\), and \(\).

Let \(^{F,}\) be a proximity-oblivious tester for whether a set-valued function \(F\) accommodates the property \(\) as given y Definition 4. We denote the random output of \(^{F,}\) evaluated at \((X,Y)\) by \(^{F,}(X,Y)\).

We now define a loss function, generated from \(^{F,}\), which will be crucial in formulating our conformal risk control problem.

**Definition 6** (Loss Function Generated from a POT).: _Let \(^{F,}\) be a proximity-oblivious tester for a property \(\). We define the loss function \(L_{i}\) as:_

\[L_{i}=0,&^{F,}(X_{i},Y_{i})=,\\ 1,&,\]

_where \((X_{i},Y_{i})\) are samples from the distribution \(\)._

Example.Consider the POT for the property \(\) of a function \(f:\) being constant, as mentioned in Section 2.1. Assume we have access to a calibration set \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) of size \(n\). We use a two-dimensional parameter \(=(^{-},^{+})\), and define the set-valued function:

\[F_{}(X)=[f(X)-^{-},\ f(X)+^{+}],\]

for each \(X\). This creates prediction intervals around the point prediction \(f(X)\), with widths controlled by \(^{-}\) and \(^{+}\).

We then apply the loss function generated by \(^{F_{},}\) as given in Definition 6, and use conformal risk control to tune \(\) such that the expected loss on the \((n+1)\)th point falls below a given target level \(\).

Note that in this case the tester and loss function does not depend on the \(Y_{i}\). This is because the property of \(f\) being constant does not depend on the \(Y_{i}\) from the calibration set and here \(\) is only used to obtain samples of the \(X_{i}\). This is not the case in general, however, and properties can be defined with respect to the whole sample \((X_{i},Y_{i})\). For example, we could consider the property \(\) that \(f\) does not over-predict, that is, for \((X,Y)\) we have \(f(X) Y\). Now we state our main theorem.

**Theorem 1**.: _Let \(\) be a proximity-oblivious tester for a property \(\) with detection probability function \(()\). Assume access to a calibration dataset \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) sampled independently from a distribution \(\). Suppose we run conformal risk control on this calibration dataset using risk parameter \(\) and loss functions \(L_{i}\) for property \(\) generated from \(\) (as in Definition 6). Then, for any \(\) such that \(()>\), the probability that \(F_{}}\) is \(\)-Faraway from \(\) satisfies:_

\[_{(X_{1},Y_{1}),,(X_{n},Y_{n})}F_{}}.\]

Proof.: Let \(\) denote the event that \(F_{}}\) is \(\)-Faraway from the property \(\). Our goal is to bound the probability \(_{(X_{1},Y_{1}),,(X_{n},Y_{n})}[]\).

The conformal risk control procedure ensures that the expected loss on a new sample \((X_{n+1},Y_{n+1})\) satisfies:

\[_{(X_{1},Y_{1}),,(X_{n},Y_{n}),(X_{n+1},Y_{n+1})}[L_{n+1}].\]

Now, we can write

\[[L_{n+1}]=()[L_{n+1}]+ (^{c})[L_{n+1}^{c}].\]

When \(\) occurs, \(F_{}}\) is \(\)-Faraway from \(\). By the properties of the proximity-oblivious tester \(\), we have:

\[_{(X,Y)}^{F_{}},}(X,Y)=().\]

Thus, the conditional expected loss satisfies:

\[[L_{n+1}]=^{F_{}},}(X_{n+1},Y_{n+1})=( ).\]

And when since \((^{c})[L_{n+1}^{c}]\) is non-negative because \(L_{n+1}\) is non-negative, we obtain

\[[L_{n+1}]()().\]Combining this result with our guarantee from the conformal risk control procedure,

\[[L_{n+1}]()().\]

This implies:

\[().\]

Therefore, the probability that \(F_{}}\) is \(\)-Faraway from \(\) satisfies:

\[_{(X_{1},Y_{1}),,(X_{n},Y_{n})}F_{}}$}.\]

Amplifying Detection Probability via Independent Calls.When the detection probability \(()\) of the proximity-oblivious tester \(\) is less than or close to the risk parameter \(\), the bound provided by Theorem 1 may not be tight or meaningful (since \(/()\) could be greater than or equal to 1). To address this issue, we can amplify the detection probability by performing multiple independent executions of \(\) and combining their results appropriately.

To increase the detection probability beyond \(\), we execute the proximity-oblivious tester \(\) independently \(k\) times on independent samples and define a new tester \(^{}\) that rejects if _any_ of the \(k\) executions reject (i.e., by applying a logical OR to the outcomes). This amplification technique yields an adjusted detection probability

\[^{}()=1-(1-())^{k},\]

representing the probability that at least one of the \(k\) independent executions rejects when the function is \(\)-Faraway from \(\).

In this approach, the calibration dataset needs to be partitioned into \(n^{}=\) disjoint batches, each containing \(k\) samples. Each batch provides the independent samples required for the \(k\) executions of \(\) per calibration point. As a result, the effective sample size available for calibrartion becomes \(n^{}\) due to this batching of samples.

## 4 Examples

### Monotonicity

Monotonic behavior is important in various applications. We focus on monotonicity in a single feature, where we expect that \(f(X)\) should have monotonically increasing or decreasing behavior with respect to a certain feature \(x^{k}\) when other features \(x^{-k}\) are held fixed. While there is a long-standing literature on using monotonic constraints for regularization (Brunk et al., 1973; Sill and Abu-Mostafa, 1996; You et al., 2017; Bonakdarpour et al., 2018) and on integrating such monotonic shape constraints into prediction models (Groeneboom and Jongbloed, 2014; Cano et al., 2018; Runje and Shankaranarayana, 2023), our aim is not to view monotonicity as a possible means to improve test accuracy, but rather as a user-desired property for safe or fair deployment of a given model. For example, Wang and Gupta (2020) highlight the importance of monotonicity in models for criminal sentencing, wages, and medical triage.

Consider a user given a pre-trained model \(f\) that was not trained with monotonic constraints. The user, however, wishes for the sake of safe or fair deployment to make predictions in a way that is as monotonic as possible. In particular, let \(\) be the property that \(f\) is monotonically decreasing in dimension \(k\). To apply our methodology we consider the proximity oblivious tester \(\) for \(\) as given in Algorithm 1.

We prove in the extended version of the paper Overman et al. (2024) that Algorithm 1 is indeed a POT for the property \(\) of being monotonically decreasing in a given dimension. Then let \(\) be the one-sided error tester for \(\) resulting from \((1/())\) calls to \(\). Now assume we have access to a calibration dataset \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) sampled from \(\) of size \(n(1/())\). We will use this calibration dataset to determine the setting of \(=(^{+},^{-})\) via conformal risk control where the loss function is generated as in Definition 6. Here the set-valued function will be constructed as \(F_{}(X)=[f(X)-^{-},f(X)+^{+}]\). Then by Theorem 1 if the tester has sufficient detection probability \(()>\) we expect to obtain a set-valued function \(F_{}}\) at most \(\) from \(\). We now investigate this empirically.

Setup.We align for monotonicity on various UCI ML repository datasets (Dua and Graff, 2023) with a 70-15-15 train-calibrate-test split, averaged over 30 random splits. We use XGBoost regression models (Chen and Guestrin, 2016). For each dataset, we select a feature for which we desire the model to be monotonic, not with the intention of improving test-set accuracy, but from the perspective of a user who desires this property.

We train two models per dataset: one unconstrained, trained on the training set, and another constrained to be monotonic, trained on both the training and calibration sets. The conformal risk control procedure is applied to the unconstrained model using the calibration data. The constrained model can be considered best possible from the user's perspective, using all available pre-test data and satisfying the monotonicity property \(\) during training.

To compare performance with respect to the training metric of accuracy, we convert conformal intervals into point predictions by taking \(k\)-quantiles of the constrained feature, linearly interpolating between adding \(^{+}\) at the lowest quantile to subtracting \(^{-}\) at the highest quantile for monotonically decreasing, or vice versa for monotonically increasing.

Results.Table 4.1 presents results on the test set for the Combined Cycle Power Plant dataset (Tfekci and Kaya, 2014). In practice, Exhaust-vacuum is known to negatively influence turbine efficiency (Tfekci and Kaya, 2014). The conformal procedure outperforms the constrained model in terms of MSE for all \(\), which is a fortuitous but unexpected outcome. The constrained model should be seen as an oracle benchmark in the sense that the model was given to the user already trained to satisfy the desired property. The risk metric closely matches the theoretical guarantee from conformal risk control and achieves optimal performance of 0 for the constrained model. Additional datasets and results are detailed in the extended version of the paper Overman et al. (2024).

### Concavity

Concavity and convexity are crucial behaviors in many applications. In this context, we focus on concavity in a single feature. A common example where users might expect concave behavior is in recommendation or preference prediction models. According to economic theory, the utility function with respect to the quantity of an item is often quasi-concave, reflecting the principle of diminishing marginal utility (Mas-Colell et al., 1995). Jenkins et al. (2021) propose a novel loss function to account for this expected concavity, which aligns the model with the concavity property \(\) during training. Here we again consider aligning a pre-trained model, not trained to satisfy \(\), using a proximity oblivious tester \(\) for \(\) as described in Algorithm 2.

   \(\) & \(\) & Metric & Unconstrained & Adjusted & Constrained \\   & \(^{+}=0.51_{( 0.24)}\) & MSE & \(10.19_{( 0.46)}\) & \(10.47_{( 0.46)}\) & \(16.21_{( 0.45)}\) \\
0.1 & \(^{-}=0.76_{( 0.24)}\) & Risk & \(0.75_{( 0.09)}\) & \(0.10_{( 0.001)}\) & \(0.00_{( 0.00)}\) \\   & \(^{+}=1.09_{( 0.51)}\) & MSE & \(10.19_{( 0.46)}\) & \(11.42_{( 0.44)}\) & \(16.21_{( 0.45)}\) \\
0.05 & \(^{-}=1.61_{( 0.50)}\) & Risk & \(0.75_{( 0.09)}\) & \(0.05_{( 0.001)}\) & \(0.00_{( 0.00)}\) \\   & \(^{+}=2.39_{( 0.82)}\) & MSE & \(10.19_{( 0.46)}\) & \(14.46_{( 0.48)}\) & \(16.21_{( 0.45)}\) \\
0.01 & \(^{-}=3.33_{( 0.79)}\) & Risk & \(0.75_{( 0.09)}\) & \(0.01_{( 0.001)}\) & \(0.00_{( 0.00)}\) \\   

Table 1: Power Plant, \(n=9568\). Monotonically decreasing on Exhaust Vacuum. \(^{}=(10,10)\).

We can use a calibration dataset to determine the setting of \(=(^{+},^{-})\) via conformal risk control where the loss function is generated as in Definition 6. The set-valued function will be constructed as \(F_{}(X)=[f(X)-^{-},f(X)+^{+}]\). We demonstrate running conformal risk control with this loss function on a real-world dataset in the extended version of the paper Overman et al. (2024)

## 5 A stylized examination of alignment persistence in AI models

Consider data generated as:

\[y=g(X)+h(X)+\,,\]

where \(\) is mean-zero noise with variance \(^{2}\) independent of \(X\). Here, \(h(X)\) is biased noise we want to ignore, aiming to learn only \(g(X)\). Consider the case in which experts expect data to follow \(g(X)+\), but biased noise \(h(X)\) can obscure this.

One potential reason for the presence of biased noise in data could be due to a measurement error of the outcome that is correlated with select features, leading to an incorrectly calculated outcome. A biased measurement error could occur if there is incomplete data and the presence of the incomplete

Figure 1: Univariate partial dependence plot of unconstrained model. Risk control band for \(=0.05\). Dashed line exemplifying Theorem 1 demonstrating existence of monotonically decreasing function falling within the conformal band on \(0.975>1-\) fraction of the domain.

data is correlated with select features in an unexpected, systematic way. Our goal is to understand how this bias affects model behavior when trying to learn \(g(X)\) alone.

Given \(n\) i.i.d. samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) from the above model, we denote this dataset by \(_{n}\). We use a random feature model:

\[f_{}(X;,\{_{j}\}_{j[N]})=} _{j[N]}a_{j}( X,_{j})\,,\]

where \(^{N}\) are learned weights, and \(\{_{j}\}_{j[N]}\) are fixed random weights. The squared loss is minimized by ridge regression:

\[}_{}=_{^{N}}_{i[n ]}(Y_{i}-f_{}(X_{i}))^{2}+\|\|_{2}^{2}\,.\]

Users expect a model to exhibit a property \(\), satisfied by \(g(X)\) but not necessarily by \(g(X)+h(X)\). We can constrain training to ensure \(\). Let \(C_{}=\{^{N}f_{}(X;)\}\), yielding a constrained model: \(}_{,}=_{ C_{}}_{i[n]}(Y_{i}-f_{}(X_{i}))^{2}+\| \|_{2}^{2}\).

Assuming \(g\) and \(h\) are polynomials with \(_{g}<_{h}\), and given specific conditions on data size and model parameters, we consider two settings: (i) _Classic:_\(d^{_{g}+}<N<d^{_{h}-}\), and (ii) _Underspecified:_\(N>d^{_{h}+}\) for a small \(>0\).

In the extended version of the paper Overman et al. (2024), we utilize results from Misiakiewicz and Montanari (2023) to derive insights into the impact of model complexity and data size on adherence to \(\). In particular, we show that under certain assumptions, including small noise bias and robustness of property \(\), the constrained and unconstrained models have zero distance in the classic setting: \(}_{,}=}_{}\). However, in the underspecified setting, the constrained and unconstrained models will differ, resulting in a non-zero distance: \(}_{,}}_{}\). This result implies that in the presence of noise bias, the overparameterized models (i.e., underspecified setting) fail to satisfy the property \(\), and this cannot be remedied as the data size increases.

## 6 Related work

Our paper draws from a broad range of areas, hence we refer the reader to textbooks and surveys in alignment (Everitt et al., 2018; Hendrycks et al., 2022; Ji et al., 2024; Hendrycks, 2024), conformal prediction (Angelopoulos and Bates, 2022), property testing (Ron, 2008; Goldreich, 2017), and linearized neural networks (Misiakiewicz and Montanari, 2023).

RLHF (Christiano et al., 2017) has been notably effective in aligning LLMs with human values and intentions, as demonstrated by (Ouyang et al., 2022). Our work considers attempts at alignment that generalizes to models without human-interpretable outputs, which has connections to the scalable oversight problem (Irving et al., 2018; Christiano et al., 2018; Wu et al., 2021). Goal misgeneralization (Langosco et al., 2022; Shah et al., 2022) has potential connections to the underspecified pipeline (D'Amour et al., 2022) considered in this paper in the sense that models with equivalent performance according to the training metric may differ in some other user-desired property during deployment. One of the main methods of assurance (Batarseh et al., 2021), which is concerned with assessing the alignment of pre-trained AI systems, is safety evaluations (Perez et al., 2022; Shevlane et al., 2023) meant to assess risk during deployment, which also has connections to our approach.

The work of Yadkori et al. (2024) closely aligns with ours in both methodology and theme, utilizing conformal risk control to reduce LLM hallucinations (Ji et al., 2023). We discuss connections to this work in the extended version of the paper Overman et al. (2024).

## 7 Discussion

We introduce a method to align pre-trained models with desired user properties using conformal risk control. By post-processing outputs using property dependent loss functions, we provide probabilistic guarantees that conformal intervals contain functions close to the desired set. This allows for alignment without retraining, effective in both generative and non-generative contexts. Future work should extend these techniques to more properties, explore sample complexity and adaptive querying, and potentially apply them to policy functions in MDP settings for RL agent safety guarantees.