# Swarm Reinforcement Learning for Adaptive Mesh Refinement

Niklas Freymuth\({}^{1}\)1 Philipp Dahlinger\({}^{1}\) Tobias Wurth\({}^{2}\) Simon Reisch\({}^{1}\)

Luise Karger\({}^{2}\) Gerhard Neumann\({}^{1}\)

\({}^{1}\)Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe

\({}^{2}\)Institute of Vehicle Systems Technology, Karlsruhe Institute of Technology, Karlsruhe

###### Abstract

Adaptive Mesh Refinement (AMR) enhances the Finite Element Method, an important technique for simulating complex problems in engineering, by dynamically refining mesh regions, enabling a favorable trade-off between computational speed and simulation accuracy. Classical methods for AMR depend on heuristics or expensive error estimators, hindering their use for complex simulations. Recent learning-based AMR methods tackle these issues, but so far scale only to simple toy examples. We formulate AMR as a novel Adaptive Swarm Markov Decision Process in which a mesh is modeled as a system of simple collaborating agents that may split into multiple new agents. This framework allows for a spatial reward formulation that simplifies the credit assignment problem, which we combine with Message Passing Networks to propagate information between neighboring mesh elements. We experimentally validate our approach, Adaptive Swarm Mesh Refinement (ASMR), on challenging refinement tasks. Our approach learns reliable and efficient refinement strategies that can robustly generalize to different domains during inference. Additionally, it achieves a speedup of up to 2 orders of magnitude compared to uniform refinements in more demanding simulations. We outperform learned baselines and heuristics, achieving a refinement quality that is on par with costly error-based oracle AMR strategies.

## 1 Introduction

The Finite Element Method (FEM) is a widely used numerical technique in engineering and applied sciences for solving complex partial differential equations [1; 2; 3; 4]. The method discretizes the continuous problem domain into smaller, finite elements, allowing for an efficient numerical solution. A key aspect of the FEM for complex systems is Adaptive Mesh Refinement (AMR), which dynamically refines regions of high solution variability, allowing for a favorable trade-off between computational speed and simulation accuracy [5; 6; 7]. As problems in engineering grow more complex, the FEM and especially Adaptive Mesh Refinement (AMR) techniques have become increasingly important tools in providing computationally tractable yet precise solutions. Applications of AMR include fluid dynamics [8; 9; 10; 11; 12; 13], structural mechanics [14; 15; 16; 17], and astrophysics [18; 19; 20]. Yet, classical approaches for AMR usually rely on either problem-dependent or more general but potentially suboptimal error indicators, or require expensive error estimates [21; 22; 23; 24; 25; 26; 12]. In either case, they can be cumbersome to use in practice.

To address this issue, we formalize AMR as a Reinforcement Learning (RL)  problem. Following previous work [28; 29; 30], each refinement step encodes the state of the current simulation as local observations that we feed to RL agents, who then determine which elements of a mesh to refine. However, previous work has issues with scalability due to an expensive inference process ,misaligned objectives and high variance in the state transitions , and noisy reward signals . To mitigate these shortcomings and scale to more complex problems, we formulate AMR as a Swarm RL [31; 32] problem. We extend the Swarm RL framework to per-agent rewards, a shared observation space, and the option of splitting agents into new agents as required in the AMR process. Additionally, we introduce a novel spatial reward formulation that provides a dense reward signal for the refinement of each mesh element, simplifying the credit assignment problem for swarm systems. Our policy is based on Message Passing Networks (MPNs) , a class of Graph Neural Networks (GNNs) [34; 35; 36; 37] that has proven to be effective for physical simulations [38; 39]. The resulting method, Adaptive Swarm Mesh Refinement (ASMR), consistently produces highly refined meshes with thousands of elements while applying to arbitrary Partial Differential Equations (PDEs). A high-level overview is given in Figure 1.

Experimentally, we show the effectiveness of our approach on a suite of PDEs that require complex and challenging refinement strategies, including a non-stationary heat diffusion problem and a linear elasticity task. We implement our tasks as OpenAI gym  environments. The experiments use static meshes with conforming triangular elements and corresponding h-adaptive refinements [41; 42] due to their importance in engineering applications [43; 44; 45; 46]. Here, accurate meshes require multiple precise refinement steps and thousands of elements. We implement and compare to current state-of-the-art RL methods for AMR [28; 29; 30] that have been shown to work well on dynamic tasks where shallow mesh refinement and coarsening is sufficient1. We observe that these methods struggle with finer static meshes, whereas ASMR yields stable and consistent refinements across tasks. To further evaluate our method's effectiveness, we compare it to the popular Zienkiewicz-Zhu Error Estimator (_ZZ Error_) estimate and a traditional AMR heuristic requiring oracle error estimates. ASMR not only outperforms both learned and traditional methods that lack oracle data but also achieves performance comparable to oracle-based heuristics. Furthermore, ASMR is \(2\) to \(100\) times faster than computing the uniform mesh on which the oracle information is based, and demonstrates robust generalization capabilities across different domains and initial conditions. We conduct a series of ablations to show which parts of the approach make it uniquely effective, finding that spatial rewards per agent are preferable to a shared global reward signal for all agents.

To summarize our contributions, we (1) propose a novel Markov Decision Process (MDP) formulation that naturally integrates local rewards for swarms where agents may split into new agents over time; (2) combine this formulation with MPNs and a novel spatial reward formulation to reliable and efficiently scale learned AMR to static meshes with thousands of elements on multiple levels of refinement; (3) showcase our approach's effectiveness on a suite of PDEs with challenging refinement

Figure 1: Given a mesh \(^{t}\), an observation graph \(_{^{t}}\) encodes the elements as graph nodes and the neighborhood between elements as edges. The graph is given to a learned policy \(\), which marks mesh elements for refinement. A remesher refines the mesh, and spatial rewards \((_{i}^{t})\) are calculated for all agents \(i\) based on the quality of the refinement. This process is iterated for several steps until the mesh is fully refined.

problems. Our method surpasses state-of-the-art RL methods and the popular _ZZ Error_ heuristic, and achieves refinement quality comparable to oracle-based AMR strategies, all without requiring costly error estimates during inference.

## 2 Related Work

**Learned Physics Simulation.** A considerable body of work deals with directly learning to simulate physical systems with neural networks. These approaches typically learn from data generated by some underlying ground-truth simulator and train the network to predict the (change in) quantities of interest during a simulation. Such learned physics simulators are fully differentiable and often orders of magnitude faster than their classical counterparts [35; 38], lending them to use cases such as Inverse Design [47; 48; 49]. Researchers have developed simulators based on simple feed-forward networks [50; 51] and Convolutional Neural Networks [52; 53; 54; 55; 56; 57; 58; 59; 60]. Closely related to our method are Graph Network Simulators (GNSs) [33; 38; 61; 62; 63; 64; 39; 65], which utilize GNNs to encode physical problems as a graph on which to compute quantities of interest per node. Here, a recent method  jointly learns a GNS and an AMR strategy on mesh edges to allow for simulation on different resolutions.

Physics-Informed Neural Networks [67; 68; 69] are mesh-free methods designed to directly train neural networks to satisfy the governing equations of a physical system. They share the goal of using deep neural networks to solve PDEs, yet differ in their approach in that they directly approximate the equations rather than providing a mesh for a classical solver. Thus, AMR strategies provide a more robust, flexible, and risk-averse approach to solving complex physics problems that demand high precision and accuracy [11; 70; 13]. As Physics-Informed Neural Networks also operate on geometric domains, they have been extended to GNN architectures [71; 72; 73]. In this work, we do not learn to solve a system of equations directly, but rather propose an efficient mesh refinement for a classical solver.

**Supervised Learning for AMR.** Applications of supervised learning for AMR include directly calculating an error per mesh element with a Multilayer Perceptron (MLP)  and predicting mesh densities from domain images . Additionally, recurrent networks have been used to find optimal marking strategies for second-order elliptical PDEs . Another body of work speeds up the computation of Dual Weighted Residual [75; 76] error estimators by substituting expensive parts of the procedure with neural networks. Here, recent methods [77; 78] consider learning a metric tensor from solution information that can then be used in existing refinement procedures . Other approaches [79; 13] employ neural networks to solve the strong form of the adjoint problem and use hand-crafted features to compute error estimates directly. We instead leverage the fact that RL can optimize non-differentiable rewards, enabling us to directly learn a refinement strategy in an iterative manner instead of learning error estimators or other specific facets of AMR.

**Reinforcement Learning for AMR.** Current research in Reinforcement Learning for AMR involves various approaches, such as optimizing the positions of mesh elements , predicting a global threshold for heuristic-based refinement using existing error estimates , and generating quadrilateral meshes by iteratively extracting elements from the problem domain .

We instead directly manipulate the mesh elements themselves. This approach presents a unique challenge in that the size of the observation and action spaces is constantly changing during refinement. Existing methods [28; 29; 30] derive their observation spaces from the mesh geometry and the solution computed on the mesh. These methods are generally designed for non-stationary PDEs and thus include mesh coarsening operations that are required for time-dependent problems. While our method can easily be extended to coarsening and time-dependent problems, this work instead considers static meshes and mostly stationary problems due to their prevalence in engineering [43; 44]. Here, the challenge lies in finding multiple levels of accurate refinements rather than shallow or local time-dependent refinement and coarsening. The earliest of these methods  treats the entire mesh as an action and observation space for a _Single Agent_, which uses a GNN-based policy to provide a categorical action that selects a single element for refinement. _Single Agent_ demands solving the system of equations after each refinement step, significantly increasing inference time while reducing the amount of information per environment sample. Another approach  iteratively selects a random element during training and uses an MLP policy to determine its marking based on local and global features. During inference, the method performs a _Sweep_ for all mesh elements in parallel.

This procedure speeds up inference but causes a misalignment in the environment transition between training and inference . Additionally, during training, the agent is randomly assigned a new element after each action, leading to high variance in the state transitions.

Most similar to our method are Value Decomposition Graph Networks (_VDGN_)  which frame AMR as a cooperative multi-agent problem by setting a maximum refinement depth and number of agents. _VDGN_ employs a Value Decomposition Network  to circumvent the posthumous credit assignment problem  of vanishing agents. Though theoretically efficient in training and inference, the method's performance depends on the quality of the value decomposition, which becomes more difficult for larger meshes. In summary, existing RL methods do not utilize the spatial nature of AMR and thus only scale to either simple or comparatively shallow refinements. We instead formulate AMR as a Swarm Reinforcement Learning problem with spatial rewards, naturally integrating changing observation and action spaces while also providing a strong feedback signal to all agents.

## 3 Adaptive Swarm Mesh Refinement

In the following, we introduce the individual components of ASMR, including our novel Adaptive Swarm Markov Decision Process (ASMDP) and spatial reward function. We consider each element \(_{i}^{t}^{t}\) of a mesh \(^{t}\) to be an agent in a swarm system. The agent's state is its position in the mesh, as well as boundary conditions and other PDE-dependent quantities. The agent's observation consists of a local view of a graph \(_{^{t}}\) where each node represents a mesh element and each edge the neighborhood of two elements. We train a simple MPN-based policy \((|_{^{t}})\) that computes a joint action vector \(^{N}\) for each mesh element by passing messages along the observation graph, as detailed in Appendix A. The action vector is used for refinement, and the process is repeated with the refined mesh for a given number of steps. Since our policy uses a GNN, it is equivariant to permutation and can handle varying numbers of agents by construction. Figure 1 provides a schematic overview of our method.

**Adaptive Swarm Markov Decision Process.** We adapt the SwarMDP framework [31; 32] to incorporate action and observation spaces of changing size as necessary for AMR, agent-wise rewards and mappings between agents over time. The resulting framework is conceptually simpler than, e.g., a decentralized partially observable MDP with dummy states , makes the permutation-equivariance of the agents explicit and naturally integrates both the agent-dependent reward and the mapping between agents. Formally, we define an Adaptive Swarm Markov Decision Process (ASMDP) as a tuple \(,,,T,,,\). Here, \(\) is the state space of the system, \(\) is the space of observations for this state space, and \(\) is the action space for the system of agents. Let \(^{N}\), \(^{N}\), \(^{N}\) denote the subsets of the state, observation, and action spaces with exactly \(N\) agents. The transition function \(T:^{N}^{N}^{K}\) maps to a new system state with a potentially different number of agents, and \(:^{N}^{N}^{N}\) is a per-agent reward function. The observation graph of the agents is calculated from their states via the observation function \(:^{N}^{N}\). To

Figure 2: Exemplary ASMR refinements. The heatmaps represent the normalized quantities of interest. ASMR provides complex and accurate refinements for different tasks. From left to right: Laplace’s equation requires a homogeneous refinement near the source at the inner boundary. For Poisson’s equation, a multi-modal load function causes multiple distinct regions of interest. The Stokes flow uses more complex shape functions and requires high precision near the inlet on the left and the rhomboid holes. For the linear elasticity task, we consider both the deformation and the resulting stress as quantities of interest. The heat diffusion task requires accurate refinements on the path of the moving heat source to predict an accurate solution of the final step.

accommodate changing numbers of agents throughout an episode, we define an agent mapping \(^{t}^{N K}\) with \(_{i}^{t}_{ij}=1\) for all \(j=1,,K\) that specifies how agents evolve at time step \(t\). Each entry \(^{t}_{ij}\) describes whether agent \(i\) at step \(t\) progresses into agent \(j\) at step \(t+1\). The influence of each agent at step \(t\) on the reward, in terms of all successor agents it is responsible for up to step \(t+k\), can then be computed via the matrix multiplication \(^{t,k}:=^{t}^{t+1}^{t+k-1}\).

The usual objective in RL is to find a policy \(:^{N}^{N}\) that maximizes the return, i.e., the expected discounted cumulative future reward \(J^{t}:=_{(|())}[_{k=0}^{} ^{k}r(^{t+k},^{t+k})]\) for a discount factor \(\) and scalar reward \(r(^{t+k},^{t+k})\) at step \(t+k\). Adapting this to varying numbers of agents within a single episode, as necessary for e.g., the refinement of mesh elements, yields

\[J^{t}_{i}:=_{(|())}[_{k=0}^{ }^{k}(^{t,k}(^{t+k},^{t+k} ))_{i}],\] (1)

for agent \(i\) at step \(t\). Intuitively, this return represents the discounted sum of rewards of all agents that agent \(i\) is responsible for. We set \(V_{i}(^{t})=(^{t},^{t})_{i}+ _{j}^{t}_{ij}V_{j}(T(^{t},^{t}))\) with \((())\) for training the value function and derive the targets for \(Q\)-functions analogously.

Agents and Observations.Given a domain \(\) and a mesh \(^{t}:=\{^{t}_{i}|\;_{i}\;^{t}_{ i}=\}\), we view each mesh element \(^{t}_{i}\) as an agent. Each element's action space comprises a binary decision to mark it for refinement. These markings are provided to a remesher, which refines all marked elements, yielding a finer mesh \(^{t+1}=\{^{t+1}_{j}\}_{j}\). Here, \(^{t+1}_{j}:=^{t}_{i}\) for no refinement and \(^{t+1}_{j}^{t}_{i}\) with \(_{j}^{t+1}_{j}=^{t}_{i}\) if \(^{t}_{i}\) is refined. The remesher may also refine unmarked elements to assert a conforming solution , i.e., to make sure that elements of the mesh align with each other at the boundaries and interfaces to ensure continuity of solution variables between adjacent elements. We define the mapping for an agent to its successor agents as the indicator function \(^{t}_{ij}:=(^{t+1}_{j}^{t}_{i})\). In other words, an agent maps to all future agents that it spawns, or equivalently, an element is responsible for all sub-elements that it refines into over time. While we focus on mesh refinement in this work, this mapping can be extended to coarsening by setting, e.g., \(^{t}_{ij}:=(^{t+1}_{j}^{t}_{i})+ ((^{t}_{i}^{t+1}_{j})/(_{k} (^{t}_{k}^{t+1}_{j})))\).

For encoding the observations, we use an observation graph \(_{^{t}}==(,,_{ },_{})\), which is a bidirectional directed graph with mesh elements as nodes \(\) and their neighborhood relation as edges \(\). Node and edge features of dimensions \(d_{}\) and \(d_{}\) are given as \(_{}:^{d_{}}\) and \(_{}:^{d_{}}\). Further details can be found in Appendix B.

**Reward.** A good refinement strategy trades off the accuracy of the solution of the mesh \(^{t}\) with its total number of elements \(^{t}_{i}^{t}\). We define an error per element as the difference in the solution of this element compared to a solution using a fine-grained reference mesh \(^{*}\). We consider \(^{*}\) to be optimal, but very slow to compute due to a large number of elements. However, we only require the reference mesh \(^{*}\) for the reward calculation, not during inference. For each element \(^{t}_{i}\) we then integrate over the evaluated differences of all midpoints \(p_{^{*}_{m}}^{*}_{m}\) of reference elements \(^{*}_{m}\) that fall into it, scaling each by the area \((^{*}_{m})\) of its respective element. This procedure results in an error estimate

\[}(^{t}_{i})_{^{*}_{m}^{* }_{i}}(^{*}_{m})|u_{^{*}}(p_{^{*}_{m}})-u_{ ^{t}}(p_{^{*}_{m}})|,\] (2)

where \(u_{^{*}}\) denotes the solution on the fine mesh and \(u_{^{t}}\) the solution on the current mesh. We note that this error estimate can be efficiently calculated using a \(k\)-d tree  and that it is generally applicable for a large range of PDEs. Problem-specific error estimates may be used instead to include domain knowledge. To get an error estimate that is consistent across different geometries, we normalize the error with the total error of the elements of the initial mesh \(^{0}\), i.e., \((^{t}_{i})=}(^{t}_{i})/_{^{0}_{ j}^{0}}}(^{0}_{j})\). We then formulate a local reward per element as

\[(^{t}_{i}):=(^{t}_{i})}((^{t}_{i})-_{j}^{t}_{ij}(^{t+1}_{j}) )-(_{j}^{t}_{ij}-1),\] (3)

where \(\) is a hyperparameter that penalizes adding new elements.

This reward function evaluates whether a refinement decreases the overall error by enough to justify the extra resources required, with a reward of \(0\) for unrefined elements. Thus, the reward maximizeserror reduction, rather than simply encouraging a refinement of areas with a high existing error regardless of the resulting mesh improvement. Further, incorporating a novel area scaling term encourages the policy to focus on smaller elements with a high potential reduction in error rather than larger elements with a low average error reduction, up to some threshold depending on the element penalty \(\). We find that the combination of a local formulation and the area scaling term allows for a simpler credit assignment for the RL agents, as it ensures that each agent gets rewarded for its own actions and that rewards of elements of different sizes are on the same scale. Similarly, the area scaling term of the reward effectively cancels out the area of the integration points in Equation 2, causing the policies optimized on this reward to implicitly minimize the maximum remaining error of the mesh, while also making sure that the mean error stays sufficiently low. We compare this to directly minimizing the maximum error in Appendix C.2.

Since the effects of mesh refinement can be non-local for elliptical PDEs, we optimize the average of the local and global returns, i.e.,

\[{J_{i}^{t}}^{}=J_{i}^{t}+J^{t},\] (4)

where \(J_{i}^{t}\) is the return of agent \(i\) at step \(t\) as shown in Equation 1, and \(J^{t}\) is the global return calculated using the average reward \(r=_{j}_{j}\). In multi-quantity systems of equations, it is important for the mesh to be suitable for all the quantities of interest. For this, we calculate individual errors \(^{d}(_{i}^{t})\) for each solution dimension and then use a norm or a convex sum of these as the overall error, depending on the application.

## 4 Experiments

**Setup.** All learned methods are trained on \(100\) PDEs and their corresponding initial and reference meshes \(^{0}\), \(^{*}\) to limit the number of required reference meshes during training. We experiment with \(10\) different target mesh resolutions per method to produce a wide range of solutions, as detailed in Appendix F.3. We repeat each experiment for \(10\) random seeds and always report the average performance on \(100\) randomly sampled but fixed evaluation PDEs. These PDEs are disjoint from the training PDEs, and both sets of PDEs consist of randomly sampled domains as well as boundary and initial conditions as detailed below. Details on the setup and the computational budget for our experiments are provided in Appendix C.1. The reference mesh \(^{*}\) is created by uniformly refining the initial mesh \(6\) times. An environment episode consists of drawing one of the \(100\) training PDE without replacement, and iteratively refining the coarse initial mesh \(^{0}\) a total of \(T=6\) times unless mentioned otherwise. Since the maximum number of elements scales exponentially with the refinement depth, we additionally evaluate a simpler task setup with \(T=4\) refinements to roughly replicate the task complexity of existing work [28; 63; 29]. We experiment with Deep Q-Network (DQN) [86; 87] as an off-policy and Proximal Policy Optimization (PPO)  with discrete actions as an on-policy RL algorithm for all RL-based methods.

We evaluate mesh quality by calculating the squared error at each point in the high-resolution reference \(^{*}\), i.e., as \(_{_{m}^{*}^{*}}(_{m}^{*})(u_{^ {*}}(p_{_{m}^{*}})-u_{^{t}}(p_{_{m}^{*}}))^{2},\) and normalize the resulting value by that of the initial mesh for comparability across PDEs. This metric captures both the maximum localized errors by punishing outliers, and the overall error across the domain. We evaluate both the mean error and an approximation of the maximum error over the mesh as additional metrics in Appendix D.6. Appendix F.1 lists all further algorithm and neural network hyperparameters.

Figure 3: Final refinements of ASMR on a Heat Diffusion problem for different values of the element penalty \(\). All refinements focus on the relevant parts of the problem, and lower element penalties lead to more fine-grained meshes.

**Graph Features.** The features \(_{v}\) of each node \(v\) consist of the environment timestep, the element area, the distance to the closest boundary, and the mean and standard deviation of the solution on the element's vertices. Edge features \(_{e}\) are defined as Euclidean distances between element midpoints. We omit absolute positions to ensure that the observations are equivariant under the Euclidean group [38; 37] to utilize the underlying symmetry of the task. We use additional task-dependent node features for some considered systems of equations, as described in Appendix B.

**Systems of Equations** We experiment on various 2D elliptical PDEs, namely the Laplace equation, the Poisson equation, a Stokes flow task, a linear elasticity example, and a non-stationary heat diffusion equation. The domains are L-shapes, rectangles with a square hole or multiple rhomboid holes, and convex polygons. Figure 2 shows exemplary ASMR refinements on all tasks and briefly explains the challenge associated with each task. The PDEs and the FEM are implemented using _scikit-fem_, and we use conforming triangular meshes and linear elements unless mentioned otherwise. The code provides OpenAI gym  environments for all tasks. We define the systems of equations and their specific features in Appendix B.

**Baselines.** We adapt several recent RL methods [28; 29; 30] that were originally designed for non-stationary AMR as baselines for our application focusing on stationary refinements. We use our error estimates as the basis of all reward calculations for comparability but otherwise calculate the rewards as described in the respective papers. _Single Agent_ predicts a categorical action over the mesh to mark the next element for refinement. _Sweep_ trains a single-agent policy by randomly sampling an element on the mesh and deciding its refinement based on local features and a global resource budget. During inference, each timestep consists of a sweep over the full mesh that may mark each element. Finally, _VDGN_ estimates a global Q-function as the sum of agent-wise local Q-functions. As the PPO version of _VDGN_ has no Q-Function, we decompose the value function as the sum of value functions of the individual elements, yielding a _VDGN_-like baseline in the case of the PPO version. We use an MPN policy for _Single Agent_ and _VDGN_, while _Sweep_ utilizes a simple MLP. Hyperparameters and further details are provided in Appendix F.2

We also compare to a traditional error-based _Oracle Error Heuristic_[90; 91; 30]. Given a refinement threshold \(\), the _Oracle Error Heuristic_ iteratively refines all elements \(_{i}^{t}\) for which \((_{i}^{t})>_{j}( _{j}^{t})\). As we are also interested in the reduction of the maximum error, we analogously define the _Maximum Oracle Error Heuristic_, which uses the maximum error per element \(_{_{m}^{*}_{i}^{t}}|u_{^{*}}(p_{_{m} ^{*}})-u_{^{*}}(p_{_{m}^{*}})|\) as a surrogate error estimate. Note that these baselines require the fine-grained reference mesh \(^{*}\), which is expensive to compute and thus usually unavailable during inference. As a substitute, we consider the commonly used _ZZ Error_, which uses the superconvergent patch recovery process to estimate an error per mesh element . Similar to the _Oracle Error Heuristic_, these estimates are combined with a refinement threshold \(\) to iteratively refine the mesh. The _ZZ Error_ generally produces smooth error estimates as the recovery process requires averaging over neighboring mesh elements, which can in some cases lead to more coherent refinements when compared to the _Oracle Error Heuristic_. The heuristics act on local element information and greedily refine elements with a high error rather than elements for which a refinement would lead to a high reduction in error. As such, they may select sub-optimal refinements for globally propagating errors, which is a well-known issue for elliptic PDEs [92; 30]. In contrast, RL methods learn to directly maximize the decrease in error, allowing them to find long-term strategies that also take the local receptive fields of the individual agents into account.

**Additional Experiments.** We conduct a series of ablation experiments to determine which parts of ASMR make it uniquely effective. We look at both the area scaling and the spatial decomposition of the reward in Equation 3 and ablate different node features and the number of training PDEs that are used. Additionally, we consider an alternate reward formulation that uses the maximum error per element instead of its average as detailed in Appendix C.2. Due to their importance for practical applications, we further experiment with both generalization to unseen and larger domains and the improvements in runtime for ASMR compared to a uniform refinement.

## 5 Results

**Quantitative Results.** We visualize the mesh quality quantitatively with a Pareto plot of the number of elements and the remaining error. We plot one point per trained policy, which represents the interquartile mean  of this policy when evaluated on \(100\) evaluation environments. We furtherprovide a log-log quadratic regression over the aggregated results of each method as a general trend-line. To enhance visibility and focus on typical behavior, we exclude sporadic outliers from the baseline methods that produce degenerate meshes with an excessively high number of elements. For all learned methods, we experiment with both PPO and DQN as the RL backbone on the Poisson equation in Appendix D.1. All learned methods, including ASMR yield better results with PPO, indicating that an on-policy algorithm is favorable when dealing with action and observation spaces of varying size. Similarly, we compare the Graph Attention Network (GAT)-like  architecture proposed by _VDGN_ to MPNs in Appendix D.2, finding that ASMR works well for both architectures, while _VDGN_ performs better with MPNs. We consequently use PPO and MPNs in all other experiments. Appendix D.3 compares the _ZZ Error_ estimator for different initial refinement levels. As the results show that a sufficiently fine initial mesh is important, we start each refinement procedure for the _ZZ Error Heuristic_ with two uniform refinements. We note that this initial tuning prevents coarse refinements and is not needed for our method.

Using these results, Figure 4 compares the different approaches on Laplace's equation. The left side of Figure 4 shows that all methods work in a simple setup on par with experiments from previous work. Here, \(4\) refinement steps are used for all methods except for _Single Agent_, which instead refines \(4\) times fewer elements. On the right side, scaling to \(6\) refinement steps and significantly more elements, only ASMR effectively handles larger instances while learned methods falter. Notably, ASMR also outperforms the _Oracle_, _Maximum Oracle_, and _ZZ Error Heuristics_. These results demonstrate the effectiveness of our Swarm RL framework for learning non-greedy refinement strategies for static meshes. Specifically, ASMR refines elements with a high potential for error reduction over the heuristics' strategy of targeting elements with high error. Figure 5 provides results on the remaining tasks. Appendix D.6 presents additional results using a mean error and a smooth version of a maximum error metric. ASMR clearly outperforms the learned baselines on all tasks and is generally competitive with or better than the _Heuristics_. Both heuristics improve over the RL methods on the Stokes flow task, likely because the task requires high precision for both the inlet and on inner boundaries near regions of high flow velocity.

**Qualitative Results.** Figure 2 shows refinements of ASMR on randomly sampled systems of equations for all considered tasks. The refinement strategy adapts to the given task, providing an efficient trade-off between simulation accuracy and the number of elements used. Figure 3 visualizes the refinements of ASMR on a randomly sampled heat diffusion problem. ASMR refines based on the element penalty \(\), yet always focuses on the heat source and its path. Appendix G.1 provides additional ASMR visualizations for all tasks, and Appendix G.2 visualizes all methods on Poisson's equation to showcase common refinement behaviors. Appendix G.3 presents the iterative marking procedure of our approach on an exemplary Poisson problem.

Figure 4: Pareto plot of normalized squared errors and number of final mesh elements for Laplace’s Equation. (Left) For only \(4\) refinement steps, all learned methods perform well and significantly improve upon a uniform refinement. (Right) Scaling to \(6\) refinement steps, the learned baselines become less stable and in some cases fail to provide refinements that are better than uniform. In contrast, ASMR consistently provides high-quality refinements and is on par with or better than the heuristics in both cases.

**Ablations.** The reward proposed in Equation 3 combines an area scaling per element with a spatial allocation of the decrease in error to the individual mesh elements. Figure 6 investigates these decisions. We find that combining both features is uniquely responsible for the effectiveness of our method, suggesting that the spatial reward's limited expressiveness for small elements is compensated by area scaling. However, the area scaling can only be leveraged if it is allocated to individual mesh elements, as it may introduce excessive reward noise on the full mesh. The maximum reward variant of Appendix C.2 explicitly minimizes the maximum error of the mesh, resulting to refinements of similar quality than those created by ASMR using the reward in Equation 3. We thus use the latter as it simplifies the error estimate in the reward function and better aligns with existing work.

We evaluate the effect of different parameters for the target mesh resolution in Appendix D.4, finding that ASMR provides meshes with considerably more consistent numbers of elements for a given target resolution than the other learned methods. Additional ablations in Appendix D.5 show that \(100\) training PDEs are sufficient and that adding absolute positions in the node features is detrimental, while providing solution information and load function evaluations improves performance.

**Generalization Capabilities and Runtime Experiments.** Table 1 compares the wall-clock time of ASMR trained on a variant of the Poisson task with that of the reference \(^{*}\), showing that our method provides a speedup of more than factor \(100\) compared to computing a uniform mesh for large domains. The evaluation uses load functions with \(16\) Gaussian modes and spiral-shaped of varying sizes for the same average initial element size. Appendix E.1 provides details for the training environments and the spiral-shaped evaluation domain, as well as results on larger domains and the associated improvement in runtime. These results includes an ASMR visualization of a refinement of a \(20 20\) spiral domain with more than \(50\,000\) elements. Appendix E.2 additionally shows the exceptional generalization capabilities of ASMR across various domains and load functions for Poisson's equation on \(1 1\) domains. Appendix E.3 presents further runtime comparisons for all tasks and shows that ASMR provides a task-dependent speedup of factor \(2\) to \(30\) over a uniform refinement.

Figure 5: Pareto plot of normalized squared errors and number of final mesh elements across different tasks. All methods generally work well for relatively small instances, but _Single Agent_, _VDGN_-like and _Sweep_ break down for larger meshes. Our method uniquely scales to meshes to thousands of elements and consistently outperforms these learned baselines on all tasks and while performing on par with or better than the _Oracle_, _Maximum Oracle_ and _ZZ Error Heuristics_ in most cases.

The generalization capabilities in combination with the fast runtime of our method offer substantial advantages in practical engineering applications. A policy can be trained on small, cost-effective environments and then deployed on much larger and dynamically changing setups during inference. These generalization traits arise from the MPN architecture and the utilized observation graphs, which both lead to refinement strategies based on local element neighborhoods rather than global meshes.

## 6 Conclusion

We present a novel Adaptive Mesh Refinement method that uses Swarm Reinforcement Learning to iteratively refine meshes for efficient solutions of Partial Differential Equations. Our approach, Adaptive Swarm Mesh Refinement (ASMR), treats each mesh element as an agent and trains all agents under a shared policy using Graph Neural Networks and a novel per-agent reward formulation. ASMR gracefully scales to meshes with thousands of elements without requiring an error estimate during inference. In our experiments focused on static meshes, ASMR demonstrates strong performance in handling complex refinements. The method significantly outperforms both existing Reinforcement Learning-based approaches and traditional refinement strategies, achieving a mesh quality comparable to expensive oracle-based error heuristics. Once trained, the ASMR policy generalizes well to different forcing functions and significantly larger problem domains. In terms of runtime, our method outperforms uniform refinements by up to \(30\) times on domains similar in scale to the training set, and by over \(100\) times in larger evaluation setups.

**Broader Impact** Our proposed Adaptive Mesh Refinement technique can positively impact various fields relying on computational modeling and simulation. By reducing simulation times while maintaining high precision, this technology enables researchers to explore a wider range of scenarios. However, like any powerful tool, there are potential negative impacts, such as the development of advanced weapon models or exploitation of resources.

**Limitations and Future Work** Our approach solves the partial differential equation after each refinement step, which requires a considerable amount of computation time. In future work, we will explore using Swarm RL for refinement strategies from the raw geometry and boundary conditions to further speed up our approach. We currently use relatively simple message passing networks for our policy, and want to optimize the network architecture to include, e.g., long-range message passing. Lastly, this work only considers \(2\)D problems, static meshes with triangular elements, and comparatively simple domains. Here, we want to extend and modify our approach to quadrilateral meshes, time-dependent refinement and coarsening operations, and \(3\)-dimensional domains.

 Domain & Elements & Time[s] & Speedup[\(\)] \\  \(2 2\) & \(4208\) & \(0.19\) & \(14.6\) \\ \(3 3\) & \(4185\) & \(0.23\) & \(29.6\) \\ \(4 4\) & \(4391\) & \(0.28\) & \(47.1\) \\ \(5 5\) & \(5120\) & \(0.35\) & \(67.3\) \\ \(6 6\) & \(5462\) & \(0.41\) & \(90.1\) \\ \(7 7\) & \(6515\) & \(0.51\) & \(114.8\) \\ \(8 8\) & \(7506\) & \(0.60\) & \(131.4\) \\ 

Table 1: Elements and speedup versus uniform refinement for achieving a normalized squared error of \(0.001\) for different domain sizes on Poisson’s Equation. The elements required for the error threshold grow slower than the domain size, indicating fewer areas of significant error in larger domains. Thus, ASMR offers increasing speedups as the size of the domain increases.

Figure 6: Pareto plot of normalized squared errors and number of final mesh elements for the Linear Elasticity task for different reward ablations. ASMR benefits from the area scaling term of Equation 3 and a spatial reward formulation. The maximum reward of Appendix C.2 performs similar to that of Equation 3.