# Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach

Yudi Zhang1 Yali Du2 Biwei Huang3 Ziyan Wang2 Jun Wang4 Meng Fang5, 1 Mykola Pechenizkiy1

1Eindhoven University of Technology 2King's College London

3University of California San Diego 4University College London

5University of Liverpool

{y.zhang5,m.pechenizkiy}@tue.nl,{yali.du,ziyan.wang}@kcl.ac.uk

bih007@ucsd.edu, jun.wang@cs.ucl.ac.uk, Meng.Fang@liverpool.ac.uk

###### Abstract

A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Reward redistribution serves as a solution to re-assign credits for each time step from observed sequences. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable reward redistribution and preserving policy invariance. In this paper, we start by studying the role of causal generative models in reward redistribution by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called _Generative Return Decomposition_ (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal generative model to form a compact representation to train policy over the most favorable subspace of the state space of the agent. Theoretically, we show that the unobservable Markovian reward function is identifiable, as well as the underlying causal structure and causal models. Experimental results show that our method outperforms state-of-the-art methods and the provided visualization further demonstrates the interpretability of our method. The project page is located at https://reedzyd.github.io/GenerativeReturnDecomposition/.

## 1 Introduction

Reinforcement Learning (RL) has achieved significant success in a variety of applications, such as autonomous driving [1; 2], robot [3; 4], games [5; 6; 7], financial trading [8; 9], and healthcare . A challenge in real-world RL is the delay of reward feedback [11; 12; 13; 14]. In such a delayed reward setting, the learning process may suffer from instability due to the lack of proper guidance from the reward sequences [15; 16; 17]. Methods such as reward shaping [18; 19], curiosity-driven intrinsic reward [20; 21; 22; 23] and hindsight relabeling [24; 25; 26; 13] have been proposed to provide proxy rewards in RL.

How to compute the contribution of each state-action pair affected by a delayed reward and explain the reasons behind such contribution are equally important, as they can provide insights into the underlying dynamics and the decision process, guiding the development of more effective RL algorithms. Recent return decomposition methods employ the return equivalent hypothesis to produce proxy rewards for the state-action pair at each time step [15; 16; 17]. These methods allow for thedecomposition of the trajectory-wise return into Markovian rewards, preserving the policy invariance. However, previous methods construct reward redistribution by hand-designed rules , or an uninterpretable model , where the explanation of how the state and action contribute to the proxy reward is unclear. On the other hand, recently, causal modeling has shown promise in environmental model estimation and follow-up policy learning for RL . Works along this direction investigate and characterize the significant causes with respect to their expected outcomes, which help improve the efficiency of exploration by making use of structural constraints , resulting in an impressive performance improvement. Therefore, causal modeling serves as a natural tool to investigate the contributions of states and actions toward the Markovian rewards.

In this paper, we propose a novel algorithm for reward redistribution with causal treatment, called Generative Return Decomposition (GRD). GRD tackles reward redistribution with causal modeling that enjoys the following advantages. First, instead of a flat representation, GRD specifies each state and action as a combination of values of several constituent variables relevant to the problem and accounts for the causal relationships among variables in the system. Such a structural factored representation, relating to Factored MDP , provides favorability to form and identify the Markovian reward function from the perspective of causality. Unlike previous approaches, GRD utilizes such a parsimonious graphical representation to discover how each dimension of state and action contributes to the Markovian reward. Moreover, within the generative process of the MDP environment, we can naturally explain and model the observed delayed return as a causal effect of the unobserved Markovian reward sequence, which provides insights into the underlying dynamics of the environment, preserves the policy invariance as well. Figure 1 shows the framework of GRD, involving the causal relationship among environmental variables. The nodes denote different variables in the MDP environment, _i.e._, all dimensions of state \(s_{,t}\) and action \(_{,t}\), Markovian rewards \(r_{t}\) for \(t[1,T]\), as well as the long-term return \(R\). For sparse reward settings in RL, the Markovian rewards \(r_{t}\) are unobservable, which are represented by nodes with blue filling. While considering the return-equivalent assumption in return decomposition, we can observe the trajectory-wise long-term return, \(R\), which equals the discounted sum of delayed reward \(o_{t}\) and evaluates the performance of the agent within the whole episode. A special case of delayed rewards is in episodic RL, where \(o_{1:T-1}=0\) and \(o_{T} 0\). Theoretically, we prove that the underlying generative process, including the unknown causal structure, the Markovian reward function, and the dynamics function, are identifiable. GRD learns the Markovian reward function and dynamics function in a component-wise way to recover the underlying causal generative process, resulting in an explainable and principled reward redistribution. Furthermore, we identify a minimal sufficient representation for policy training from the learned parameters, consisting of the dimensions of the state that have an impact on the generated Markovian rewards, both directly and indirectly. Such a compact representation has a matching causal structure with the generation of the Markovian reward, aiding in the effectiveness and stability of policy learning.

We summarize the main contributions of this paper as follows. First, we reformulate the reward redistribution by introducing a graphical representation to describe the causal relationship among the dimensions of state, action, the Markovian reward, and the long-term return. The causal structure over the environmental variables, the unknown Markovian reward function, and the dynamics function are identifiable, which is supported by theoretical proof. Second, we propose GRD to learn the underlying causal generative process. Based on the learned model, we construct interpretable reward redistribution and identify a compact representation to facilitate policy learning. Furthermore, empirical experiments on robot tasks from the MuJoCo environment demonstrate that our method outperforms the state-of-the-art methods and the provided visualization shows the interpretability of the redistributed rewards, indicating the usefulness of causal modeling for the sparse reward settings.

Figure 1: A graphical illustration of causal structure in Generative Return Decomposition. See the main text for the interpretation.

Related Work

Below we review the related work on reward redistribution and causality-facilitated RL methods.

Previous work redistributes rewards to deal with the delayed rewards in RL, such as reward shaping [18; 19] and curiosity-driven intrinsic reward [20; 21; 22; 23]. Return decomposition draws attention since RUDDER  rethinks the return-equivalent condition for reward shaping  to decompose the long-term return into proxy rewards for each time step through an LSTM-based long-term return predictor and manually designed assignment. Align-RUDDER  introduces the bioinformatical alignment method to align successful demonstration sequences, then manually scores new sequences according to the aligned demonstrations. However, in many reinforcement learning (RL) tasks, obtaining a sufficient number of successful demonstrations can be challenging.  and  explore to improve RUDDER by the replacement of expressive language models  and the continuous modern Hopfield network  to decompose delayed rewards. Apart from them, RRD  proposes an upper bound for the common return-equivalent assumption to serve as a surrogate optimization objective, bridging the return decomposition and uniform reward redistribution in IRCR . However, those methods lack the explanation of how the contributions derives [17; 35], or applying unflexible manual design [15; 16] to decompose the long-term returns into dense proxy rewards for the collected sequences. By contrast, we study the role of the causal model, investigate the relationships within the generation of the Markovian reward and exploit them to guide interpretable return decomposition and efficient policy learning.

Plenty of work explores solving diverse RL problems with _causal treatment_. Most conduct research on the transfer ability of RL agents. For instance,  learns factored representation and an individual change factor for different domains, and  extends it to cope with non-stationary changes. More recently, [28; 40] remove unnecessary dependencies between states and actions variables in the causal dynamics model to improve the generalizing capability in the unseen state. Also, causal modeling is introduced to multi-agent task [41; 42], model-based RL , imitation learning  and so on. However, most of them do not consider the cases of MDP with observed delayed and sparse rewards explicitly. As an exception,  distinguishes the impact of the actions from one-time step and future time step to the delayed feedback by a policy gradient estimator, but still suffering from very delayed sparse rewards. Compared with the previous work, we investigate the causes for the generation of sequences of Markovian rewards to address very delayed rewards, even episodic delayed ones.

## 3 Preliminaries

In this section, we review the Markov Decision Process  and return decomposition [15; 16; 17].

**Markov Decision Process** (MDP) is represented by a tuple \(,,,,P\), where \(\), \(\), \(\) denote the state space, action space, and reward function, separately. The state transition probability of the environment is denoted as \(P(s_{t+1} s_{t},a_{t})\). The goal of reinforcement learning is to find an optimal policy \(:\) to maximize the expected long-term return, _i.e_., a discounted sum of the rewards with the predefined discounted factor \(\) and episode length \(T\), \(J()=_{s_{t} P(s_{t}|s_{t-1},a_{t-1}),a_{t}(a_{t}|s_{t}) }[_{t=1}^{T}^{t-1}(s_{t},a_{t})]\).

**Return decomposition** is proposed to decompose the long-term return feedback into a sequence of Markovian rewards in RL with delayed rewards [15; 16] while maintaining the invariance of optimal policy . In the case of delayed reward setting, the agent can only observe some sparse delayed rewards. An extreme case is that the agent can only get an episodic non-zero reward \(o_{T}\) at the end of each episode, _i.e_., \(o_{1:T-1}=0\) and \(o_{T} 0\), called episodic reward . In general, the observed rewards \(o_{t}\) are delayed and usually harm policy learning, since the contributions of the state-action pairs are not clear. Therefore, return decomposition is proposed to generate proxy rewards \(_{t}\) for each time step, which is expected to be non-delayed, dense, Markovian, and able to clarify the contributions of the state-action pairs to the delayed feedback. The work along this line shares a common assumption,

\[R=_{t=1}^{T}^{t-1}o_{t}=_{t=1}^{T}^{t-1}_{t},\] (1)

where \(R\) is the long-term return and \(o_{t}\), \(_{t}\) denote the observed delayed reward and the decomposed proxy reward for each time step, separately.

Causal Reformulation of Reward Redistribution

As a foundation, below we introduce a Dynamic Bayesian Network (DBN)  to reformulate reward redistribution and characterize the underlying generative process, leading to a natural interpretation of the explicit contribution of each dimension of state and action towards the Markovian rewards.

### Causal Modeling

We describe the MDP process with a trajectory-wise long-term return, \(=,,g,P_{f},,\). \(\), \(\) represent the sets of states \(\) and actions \(\), respectively. \(\) denotes a DBN that describes the causal relationship within the MDP environment, constructed over a finite number of random variables, \(\{_{1,t},,_{||,t},_{1,t},,_{||,t},r_{t}\}_{t=1}^{T} R\) where \(||\) and \(||\) are the dimension of \(\) and \(\). \(r_{t}\) is the unobservable Markovian reward that serves as the objective of return decomposition. The state transition \(P_{f}\) can be represented as, \(P_{f}(_{t+1}_{t},_{t})=_{i=1}^{||}P(_{ i,t+1}(_{i,t+1}))\), where \(i\) is the index of dimension of \(_{t}\). Here \((_{i,t+1})\) denotes the causal parents of \(_{i,t+1}\) and usually is a subset of the dimensions of \(_{t}\) and \(_{t}\). We assume a given initial state distribution \(P(_{1})\). Similarly, we define the functions, \(g\), which generates unobservable Markovian reward, \(r_{t}=g((r_{t}))\), where \((r_{t})\) is a subset of the dimensions of \(_{t}\) and \(_{t}\). The trajectory-wise long-term return \(R\) is the causal effect of all the Markovian rewards. An example of the causal structure denoted by \(\) is given in Figure 1. For simplicity, we reformulate the generative process as,

\[s_{i,t+1}=f(_{,i}^{}_{t}, {C}_{,i}^{}_{t},_{s,i,t})\\ r_{t}=g(^{}_{t},^{} _{t},_{r,t})\\ R=_{t=1}^{T}^{t-1}r_{t}\] (2)

for \(i=(1,2,,||)\). \(\) is the element-wise product. \(f\) and \(g\) stand for the dynamics function and reward function, separately. \(^{}\) are categorical masks organizing the causal structure in \(\). \(_{s,i,t}\) and \(_{r,t}\) are i.i.d random noises. Specially, \(^{ r}\{0,1\}^{||}\) and \(^{ r}\{0,1\}^{||}\) control if a specific dimension of state \(_{t}\) and action \(_{t}\) impact the Markovian reward \(r_{t}\), separately. For example, if there is an edge from \(_{i,t}\) to \(r_{t}\), then the \(_{i}^{ r}=1\). Similarly, \(^{}\{0,1\}^{||||}\) and \(^{}\{0,1\}^{||||}\) indicate the causal relationship between \(_{t}\), \(_{t}\) and \(_{t+1}\), separately. In particular, we assume that \(\) is time-invariant, _i.e._, \(f\), \(g\), \(^{}\) are invariant. In the system, \(_{t}\), \(_{t}\), and \(R\) are observable, while \(r_{t}\) are unobservable and there are no unobserved confounders and instantaneous causal effects.

### Identifiability of Generative Process

Below we give the identifiability result of learning the latent causal structure and unknown functions in the above causal generative model.

**Proposition 1** (Identifiability): _Suppose the state \(_{t}\), action \(_{t}\), trajectory-wise long-term return \(R\) are observable while Markovian rewards \(r_{t}\) are unobservable, and they form an MDP, as described in Eq. 2. Then, under the global Markov condition and faithfulness assumption, the reward function \(g\) and the Markovian rewards \(r_{t}\) are identifiable, as well as the causal structure that is characterized by binary masks \(^{}\) and the transition dynamics \(f\)._

Details of the proof for Proposition 1 are deferred to Appendix B. Proposition 1 provides the foundation for us to identify the causal structure \(^{}\) and the unknown functions \(f\), \(g\) in the generative process from the observed data. As a result, with the identified structures and functions in Eq. 2, we can naturally construct an interpretable return decomposition. Here we clarify the difference between our work and the previous work for a better understanding: 1) compared with [17; 35], \(g\) in Eq. 2 is a general description to characterize the Markovian reward as the causal effect of a subset of the dimensions of state and action, 2) compared with [15; 16], treating long-term reward \(R\) as the causal effect of all the Markovian rewards in a sequence is an interpretation of return-equivalent assumption from the causal view, allowing a reasonable and flexible return decomposition.

## 5 Generative Return Decomposition

In this section, we propose a principled framework for reward redistribution from the causal perspective, named Generative Return Decomposition (GRD). Specifically, we first introduce how GRD recovers a generative model within the MDP environment in Sec. 5.1, and then show how GRD deploys the learned parameters of the generative model to facilitate efficient policy learning in Sec. 5.2. The GRD consists of two parts, \(_{}\) for the parameterized generative process and \(_{}\) for the policy. Therefore, the corresponding loss function includes \(L_{m}(_{m})\) and \(J_{}(_{})\), for causal generative model estimation and policy optimization, respectively. Hence, the overall objective can be formulated as

\[_{_{m},_{}}L(_{m},_{})=L_{m}(_{m})+J_{}(_ {}).\] (3)

In the following subsections, we will present each component of the objective function.

### Generative Model Estimation

In this subsection, we present how our proposed GRD recovers the underlying generative process. This includes the identification of binary masks (\(^{}\)), as well as the estimation of unknown functions (\(f\) and \(g\)). An overview of our method is illustrated in Figure 2. The parameterized model \(_{m}\) that incorporates the structures and functions in Eq. 2 is used to approximate the causal generative process. The optimization is carried out by minimizing \(L_{}\) over the replay buffer \(\), which consists of trajectories, with each trajectory \(=\{(_{t},_{t})_{t=1}^{T},R\}\), where \(R\) is the discounted sum of observed delayed rewards \(o_{t}\), as given in Eq. 1.

**Generative Model.** The parameterized model \(_{}\) consists of three parts, \(_{},_{},_{}\), which are described below. **(1)**\(_{}\) is used to identify the causal structure in \(\) by predicting the values of binary masks in Eq. 2. It consists of four parts, \(_{}^{}^{|||| 2},_{}^{}^{|| || 2}\), \(_{}^{}^{|| 2}\) and \(_{}^{}^{|| 2}\), which are used to predict \(^{}\), \(^{}\), \(^{}\) and \(^{}\), separately. We take the prediction of \(^{}\) with \(_{}^{}\) as an example to explain the process of predicting causal structure. \(_{}^{}\) characterizes \(||||\) i.i.d Bernoulli distributions for the existence of the edges organized in the matrix, \(^{}\). Each Bernoulli distribution is denoted by a two-element vector, where each element corresponds to the unnormalized probability of classifying the edge as existing or not, respectively. We denote the probability of the existence of an edge from \(i\)-th dimension of the state to \(j\)-th dimension of the next state in the causal graph as \(P(_{i,j}^{})\). Binary prediction of \(^{}\) is sampled by applying element-wise gumbel-softmax  during training, while using greedy selection during

Figure 2: The framework of the proposed Generative Return Decomposition (GRD). \(_{}\), \(_{}\), \(_{}\) in generative model \(_{}\) are marked as yellow, blue and green, while policy model \(_{}\) is marked as orange. The observable variables, state \(_{t}\), action \(_{t}\), and the delayed reward \(o_{t}\), are marked as gray. The mediate results, binary masks, \(^{}\), outputs of policy, the predicted Markovian rewards \(_{t}\) and the compact representation \(_{t}^{}\) are denoted as purple squares. The policy model \(_{}\) takes as input \(_{t}^{}\) and its training is supervised by the predicted Markovian reward \(_{t}\). The dotted lines represent the supervision signals, _i.e._, losses and predicted Markovian rewards.

inference. Similarly, we can obtain \(P(_{i,j}^{})\), \(P(_{i}^{ r})\), \(P(_{i}^{ r})\), \(^{}\), \(^{ r}\) and \(^{ r}\). (2) \(_{}\) is constructed with fully-connected layers and approximates the reward function \(g\) in Eq. 2, which takes as input the state \(_{t}\), action \(_{t}\), as well as the predictions of \(^{ r}\) and \(^{ r}\) to obtain the prediction of Markovian rewards. (3) \(_{}\) is constructed on a Mixture Density Network , and is used to approximate the dynamics function \(f\) in Eq. 2, which takes as inputs the state \(_{t}\), action \(_{t}\), the predictions of causal structures, \(^{}\) and \(^{}\). As an example, the predicted distribution for the \(i\)-th dimension of the next state is \(P(_{i,t+1}_{t},_{t},_{,i}^{}, _{,i}^{};_{})\). More details for \(_{}\) can be found in the Appendix C.2.

**Loss Terms.** Accordingly, the loss term \(L_{}\) contains three components with \(L_{}(_{})=L_{}+L_{}+L_{}\). Considering the long-term return \(R\) as the causal effect of the Markovian rewards \(r_{t}\), we optimize \(_{}\), \(_{}^{s r}\) and \(_{}^{ r}\) by

\[L_{}(_{},_{}^{s r}, _{}^{ r})=_{}[\|R-_{t =1}^{T}^{t-1}_{t}\|^{2}]=_{}[\|_ {t=1}^{T}^{t-1}o_{t}-_{t=1}^{T}^{t-1}_{t}\|^{2}],\] (4)

where \(_{t}\) is predicted by \(_{}\), _i.e._, \(_{t}=_{}(_{t},_{t},^{ r},^{ r})\). To optimize \(_{}\), \(_{}^{s}\) and \(_{}^{}\), we minimize

\[L_{}(_{},_{}^{s}, _{}^{})=_{_{t},_{t},_ {t+1}}[-_{i=1}^{||} P(_{i,t+1}_{t},_{t},_{,i}^{},_{,i}^{ };_{_{}})]\,.\] (5)

Additionally, we minimize the following cross-entropy terms to regulate the sparsity of learned causal structure to avoid trial solutions. It is achieved by force to optimize the parameters towards the direction of the nonexistence of the causal edge. Let \(D_{i}()= P(_{i})\), where \(P(_{i})\) is the possibility that the edge \(_{i}\) exists. The regularizer term is,

\[L_{}(_{})&=_{i}D_{i}(^{ r})}_{}+_{i}D_{i}(^{  r})}_{}+_{i,j}D_{i,j}(^{ })}_{}.\] (6)

where \(()\) is the indicator function and hyper-parameters \(_{()}\) are listed in Appendix C.5.

### Policy Optimisation with Generative Models

Here we explain how the learned generative model aids policy optimization with delayed rewards.

**Compact Representation as Inputs.** Inspired by , we identify a minimal and sufficient state set, \(^{}\), for policy learning as the input of the policy model, \(_{}\), called _compact representation_. It contains the states' dimensions that directly or indirectly impact the reward. To be more specific, it includes each dimension of state \(_{i,t}_{t}\), which either 1) has an edge to the reward \(r_{t}\)_i.e._, \(^{ r}_{i,j}=1\), or 2) has an edge to another state component in the next time-step, \(_{j,t+1}_{t}\), _i.e._, \(^{}_{i,j}=1\), such that the same component at time \(t\) is in the compact representation, _i.e._, \(_{j,t}_{t}^{}\). We first select \(^{}\) and \(^{}\) greedily from the i.i.d Bernoulli disturations characterized by \(_{}^{s}\) and \(_{}^{}\) (as illustrated in Appendix C.2) and organize the state dimensions which exist in \(_{t}^{}\) as \(^{}\). We define \(:=\{i,\ ^{ r}_{i}=1\}\) to denote the dimensions of \(_{t}\) which have impact on the Markovian reward \(r_{t}\) directly. Then we have,

\[^{}=\{^{ r}\}\{^{}_{ ,_{1}}^{}_{,_{  C}}\},\] (7)

where \( C\) denotes the size of \(\) and \(\) denotes element-wise logical or operation. Then, the compact representation which serves as the input of policy is defined as, \(_{t}^{}=^{}_{t}\). In this way, GRD considers the dimensions contributing to the current reward provider (the learned Markovian reward function) to choose action, thus leading to efficient policy learning.

**Markovian Rewards as Guidance.** With the predicted Markovian rewards, we adopt soft actor-critic (SAC)  for policy optimization. Specifically, given \(_{t}\) and \(_{t}\), we replace the observed delayed reward \(o_{t}\) by \(_{t}=_{}(_{t},_{t},^{ r},^{ })\), where \(^{ r}\) and \(^{ r}\) are selected greedily from \(_{}^{s r}\) and \(_{}^{}\), as illustrated in Appendix C.2. The policy model \(_{}\) contains two parts, a critic and an actor \(_{}\) due to the applied SAC algorithm. SAC alternates between a policy evaluation step, which trains a critic \(_{v}(s_{t}^{},_{t})\) to estimate \(Q^{}(_{t}^{},_{t})=_{}[_{t=1}^{T}^{t-1 }(_{t}^{},_{t})_{1}=,_{1}=]\) using the Bellman backup operator, and a policy improvement step, which trains an actor \(_{}(s_{t}^{})\) by minimizing the expected KL-divergence,

\[J_{}(_{})=_{_{t}}[D_{} (_{}\|(Q^{}-V^{}))],\] (8)

where \(V^{}\) is the value function of state-action pairs . More detailed implementation of \(_{}\) can be founded in Appendix C.3.

We optimize the generative model and the policy model simultaneously, cutting the need for collecting additional diverse data for causal discovery. It is worth noticing that SAC can be replaced by any policy learning backbones [51; 52; 53; 54]. Besides, for those model-based ones [53; 54], our learned generative model serves as a natural alternative to the used environment model.

## 6 Experiments

In this section, we begin by performing experiments on a collection of MuJoCo locomotion benchmark tasks to highlight the remarkable performance of our methods. We then conduct an ablation study to demonstrate the effectiveness of the compact representation. Finally, we investigate the interpretability of our GRD by visualizing learned causal structure and decomposed rewards.

### Setup

Below we introduce the tasks, baselines, and evaluation metric in our experiments.

**Tasks.** We evaluate our method on eight widely used classical robot control tasks in the MuJoCo environment , including _Half-Cheetah_, _Ant_, _Walker2d_, _Humanoid_, _Swimmer_, _Hopper_, _Humanoid-Standup_, and _Reacher_ tasks. All robots are limited to observing only one episode reward at the end of each episode, which is equivalent to the accumulative Markov reward that describes their performance throughout the episode. The maximum episode length is shared among all tasks and set to be \(1000\). In these tasks, the contributions across dimensions in the state to the Markovian reward can differ significantly due to the agents' different objectives, even in _Humanoid_ and _HumanoidStandup_, where the robots hold the same dynamic physics. A common feature is that the control costs are designed to penalize the robots if they take overly large actions. Therefore, the grounded Markovian reward functions consider different dimensions of state and all dimensions of action with varying weights.

**Baselines.** We compare our GRD with RRD (biased) , RRD (unbiased) , and IRCR , whose implementation details are provided in Appendix C.1. We use the same hyper-parameters for policy learning as RRD for a fair comparison. While RUDER and Align-RUDER are also popular methods for return decomposition, RRD and IRCR demonstrate superior performance compared to them. Moreover, Align-RUDER requires successful trajectories to form a scoring rule for state-action pairs, which are unavailable in MuJoCo. Therefore, we do not compare our performance with theirs.

**Evaluation Metric.** We report the average accumulative reward across \(5\) seeds with random initialization to demonstrate the performance of evaluated methods. Intuitively, a method that earns higher accumulative rewards in evaluation indicates better reward redistribution.

### Main Results

Figure 3 provides an overview of the performance comparison among the full version of our method, GRD, an ablation version without compact representation, GRD w/o CR, and the baseline methods. The ablation version GRD w/o CR uses all state dimensions instead of the compact representation as the input of policy model. For better visualization, we scale the time step axis to highlight the different converging speeds for each task. Our method GRD outperforms the baseline methods in all tasks, as shown by higher average accumulative rewards and faster convergence, especially in the tasks with high-dimensional states. For instance, in _HalfCheetah_ and _Ant_, GRD obtains the highest average accumulative reward of \(1.5 10^{4}\) and \(6.5 10^{3}\), separately, while the baseline methods achieve the best value of \(1.25 10^{4}\) and \(6 10^{3}\), respectively. In the task where the agents are equipped with high-dimension states, such as _HumanoidStandup_ of \(376\)-dimension states and _Ant_ of

\(111\)-dimension states, GRD gains significant improvement. Taking _HumanoidStandup_ as an example, GRD achieves a higher average accumulative reward of \(2.3 10^{5}\) at the end of the training and always demonstrates better performance at a certain time step during the training. The possible explanation is that GRD can quickly learn the grounded causal structure and filter out the nearly useless dimensions for approximating the real reward function. The visualization of the learned causal structure and the related discussion can be found in Sec. 6.4.

### Ablation Study

In this section, we first demonstrate that the compact representation improves policy learning and then investigate the potential impact of varying causal structures on policy learning.

**Compact representation for policy learning.** According to Figure 3, although the GRD w/o CR has access to more information (it takes all the dimensions of the state as the input of policy model), GRD earns higher accumulative rewards on all the tasks. This is because only the dimensions tied to the predicted Markovian rewards reflect the agents' goals. That is, the agent is supervised by the learned reward function while choosing an action based on the associated state dimensions, ensuring the input of the policy model aligns with its supervision signals. This constrains the policy model to be optimized over a smaller state space, leading to a more succinct and efficient training process.

**The impact of learned causal structure.** Define sparsity of causal structure of the state to reward, \(S=|}_{i=1}^{||}_{i}^{ r}\). We control the value of \(_{1}\) to obtain different sparsities of learned causal structure. The average accumulative reward and the sparsity of causal structure during the training process in _Swimmer_ are presented in Table 1. With the increase of \(_{1}\) (like from \(5e{-6}\) to \(5e{-5}\)), the causal structure generally gets more sparse (the sparsity \(S\) decreases), leading to less policy improvement. The most possible explanation is, GRD does not take enough states to predict Markovian rewards and thus can not reflect the true goal of the agent, leading to misguided policy learning. By contrast, if we set a relatively low \(_{1}\) to form a denser structure, GRD may consider redundant dimensions that harm policy learning. Therefore, a reasonable causal structure for the reward function can improve both the convergence speed and the performance of policy training.

  \(_{1}\) / \(t\) & \(1e5\) & \(2e5\) & \(3e5\) & \(5e5\) & \(8e5\) & \(1e6\) \\ 
0 & 87 \(\) 41(0.77) & 153\(\) 43(0.80) & 151\(\) 32(0.80) & 131\(\) 55(0.80) & 170\(\) 36(0.77) & 159\(\) 25(0.78) \\
3e-6 & **182\(\)103(0.72)** & 201\(\)109(0.71) & 217\(\)108(0.72) & 217\(\)100(0.68) & 229\(\) 95(0.52) & 220\(\)102(0.50) \\
5e-6 & 130\(\)103(0.56) & **245\(\)104(0.69)** & **261\(\)100(0.74)** & **286\(\) 77(0.77)** & **272\(\) 78(0.75)** & **262\(\) 94(0.66)** \\
5e-5 & 118\(\)100(0.46) & 109\(\)107(0.41) & 123\(\)103(0.34) & 141\(\) 96(0.25) & 152\(\) 93(0.19) & 158\(\) 90(0.17) \\  

Table 1: The mean and the standard variance of average accumulative reward and sparsity rate \(S\) regarding diverse \(_{1}\) at different time step \(t\) in _Swimmer_.

Figure 3: Learning curves on a suite of MuJoCo benchmark tasks with episodic rewards, based on 5 independent runs with random initialization. The shaded region indicates the standard deviation and the curves were smoothed by averaging the 10 most recent evaluation points using an exponential moving average. An evaluation point was established every \(10^{4}\) time steps.

**Robustness.** To evaluate the robustness of GRD, we provide results under noisy states in _Ant_. A significant characteristic of _Ant_ is that only the first \(28\) dimensions of state are used. Therefore, the noise on the other dimensions (\(28 111\)) should not impact a learned robust policy. To verify this, during policy evaluation, we introduce the independent Gaussian noises with the mean of \(0\) and standard deviation of \(0 1\) into those insignificant dimensions (\(28 111\)). As shown in Figure 6, GRD is unaffected by the injected noises, while the performance of the baseline methods decreases. That is because the insignificant dimensions are not in the compact representation, which serves as the policy input of GRD. Therefore, our method learns a more robust policy with the usage of compact representation.

**Results over different RL backbones.** We provide the results of training with DDPG  and TD3  in Figure 5. As the experimental result shows, on the tasks of _HalfCheetah_, GRD consistently outperforms the baseline methods, RRD-Bias, RRD-Unbias, and IRCR, which are modified to run based on the same policy optimization algorithm, DDPG and TD3. We also provide results of "None", which utilizes the observed delayed reward for policy learning directly.

Figure 4: The visualization of learned causal structure for _Ant_ when \(t[1e4,5e5,1e6]\). The color indicates the probability of the existence of causal edges, whereas darker colors represent higher probabilities. There are \(111\) dimensions in the state variables, but only the first \(27\) ones are used. (a) The learned causal structure among the first \(27\) dimensions of the state variable \(_{t}\) to the first \(54\) dimensions of the next state variable \(_{t+1}\). Due to the limited space, we only visualize the structure at \(t=1e4\) and \(t=1e6\). (b) The learned causal structure among all dimensions of the action variable \(_{t}\) to the first \(54\) dimensions of the next state variable \(_{t+1}\). (c) The learned causal structure among the first \(54\) dimensions of the state variable \(_{t}\) to the Markovian reward variable \(r_{t}\). (d) The learned causal structure among all dimensions of action variable \(_{t}\) to the Markovian reward variable \(r_{t}\).

Figure 5: Evaluation with different RL backbones, SAC, DDPG and TD3. “None” is training with the observed delayed rewards.

Figure 6: Evaluation with Gaussian Noise in the State.

**Consistent improvement.** We provide results to showcase the consistent improvement of our method. 1) On different RL backbones: we provide results to demonstrate the consistent improvement of our method over method over different RL backbones (DDPG  and TD3 ) as shown in Figure 5; 2) On manipulation tasks: we provide the results on manipulation tasks in MetaWorld . Please refer to Appendix D.

### Visualization

We visualize the learned causal structure and redistributed rewards in _Ant_, where the robot observes \(111\)-dimension states and \(84\) dimensions are not used . The actions are \(8\)-dimensional.

**Consistent improvement.** We provide results to showcase the consistent improvement of our method. 1) On different RL backbones: we provide results to demonstrate the consistent improvement of our method over method over different RL backbones (DDPG  and TD3 ) as shown in Figure 5; 2) On manipulation tasks: we provide the results on manipulation tasks in MetaWorld . Please refer to Appendix D.

### Visualization

We visualize the learned causal structure and redistributed rewards in _Ant_, where the robot observes \(111\)-dimension states and \(84\) dimensions are not used . The actions are \(8\)-dimensional.

**Decomposed Rewards.** We visualize the decomposed rewards and the ground truth rewards to demonstrate the accuracy of predicting Markovian reward by GRD. According to Figure 7, the redistributed rewards (blue lines) consistently align with the ground truth (red lines), indicating that our method indeed distinguishes the state-action pairs with less contribution to the long-term return from those with more contributions. More cases are provided in Appendix D.2.

## 7 Conclusion

In this paper, we propose Generative Return Decomposition (GRD) to address reward redistribution for the RL tasks with delayed rewards in an interpretable and principled way. GRD reformulates reward redistribution from a causal view and recovers the generative process within the MDPs with trajectory-wise return, supported by theoretical evidence. Furthermore, GRD forms a compact representation for efficient policy learning. Experimental results show that, in GRD, not only does the explainable reward redistribution produce a qualified supervision signal for policy learning, but also the identified compact representation promotes policy optimization.

**Limitation.** The limitations of our work arise from the made assumption. We presumed a stationary reward function, limiting our method's use in the case of a dynamic, non-stationary reward function. The assumed static causal graph and used causal discovery methods might not cater to situations with confounders. Besides, an important direction for future work is to extend our proposed method in the case of partial observable MDP and to discuss the identifiability in such scenarios.

**Broader Impacts.** Our motivation is to strive to make decisions that are both understood and trusted by humans. By enhancing the transparency and credibility of algorithms, we aim to harmonize human-AI collaboration to enable more reliable and responsible decision-making in various fields. Our GRD algorithm notably advances this by offering a causal view of reward generation and identifying a compact representation for policy learning.

Figure 7: The visualization of decomposed rewards (blue solid lines) and the grounded rewards (red dotted lines).