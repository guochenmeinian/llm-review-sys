# Practical and Asymptotically Exact Conditional Sampling in Diffusion Models

Luhuan Wu

Columbia University

lw2827@columbia.edu

&Brian L. Trippe

Columbia University

blt2114@columbia.edu

&Christian A. Naesseth

University of Amsterdam

c.a.naesseth@uva.nl

&David M. Blei

Columbia University

david.blei@columbia.edu

&John P. Cunningham

Columbia University

jpc2181@columbia.edu

Equal contribution, order by coin flip.

###### Abstract

Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the _Twisted Diffusion Sampler_, or _TDS_. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models through simulating a set of weighted particles. The main idea is to use _twisting_, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and in conditional image generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but with empirical improvements over heuristics with as few as two particles. We then turn to motif-scaffolding, a core task in protein design, using a TDS extension to Riemannian diffusion models; on benchmark tasks, TDS allows flexible conditioning criteria and often outperforms the state-of-the-art, conditionally trained model.2

## 1 Introduction

Conditional sampling is an essential primitive in the machine learning toolkit. One begins with a generative model that parameterizes a distribution \(p_{}(x)\) on data \(x\), and then augments the model to include information \(y\) in a joint distribution \(p_{}(x,y)=p_{}(x)p(y x)\). This joint distribution then implies a conditional distribution \(p_{}(x y)\), from which desired outputs are sampled.

For example, in protein design, \(p_{}(x)\) can represent a distribution of physically realizable protein structures, \(y\) a substructure that imparts a desired biochemical function, and samples from \(p_{}(x y)\) are then physically realizable structures that contain the substructure of interest [e.g. 34, 37].

Diffusion models are a class of generative models that have demonstrated success in conditional generation tasks . They parameterize distributions \(p_{}(x)\) through an iterative refinement process that builds up data from noise. When a diffusion model is used for conditional generation, this refinement process is modified to account for conditioning at each step .

One approach is to incorporate the conditioning information into training (e.g. 9; 27); one either modifies the unconditional model to take \(y\) as input or trains a separate conditional model to predict \(y\) from partially noised inputs. However, conditional training requires (i) assembling a large set of paired examples of the data and conditioning information \((x,y)\), and (ii) designing and training a task-specific model when adapting to new conditioning tasks. For example, image inpainting and class-conditional image generation can be both formalized as conditional sampling problems based on the same (unconditional) image distribution \(p_{}(x)\); however, the conditional training approach requires training separate models on two curated sets of conditioning inputs.

To avoid conditional training, a separate line of work uses heuristic approximations that directly operate on unconditional diffusion models: once an unconditional model \(p_{}(x)\) is trained, it can be flexibly combined with various conditioning criteria to generate customized outputs. These approaches have been applied to _inpainting_ problems (24; 33; 34), and other inverse problems (1; 6; 19; 32). But it is unclear how well these heuristics approximate the exact conditional distributions they are designed to mimic; for example on inpainting tasks they often fail to return outputs consistent with both the conditioning information and unconditional model (40). These concerns are critical in domains that require accurate conditionals. In molecular design, for example, even a small approximation error could result in atomic structures with chemically implausible bond distances.

This paper develops a practical and asymptotically exact method for conditional sampling from an unconditional diffusion model. We use sequential Monte Carlo (SMC), a general tool for asymptotically exact inference in sequential probabilistic models (5; 10; 25). SMC simulates an ensemble of weighted trajectories, or _particles_, through a sequence of proposals and weighting mechanisms. These weighted particles then form an asymptotically exact approximation to a desired target distribution.

The premise of this work is to recognize that the sequential structure of diffusion models permits the application of SMC for sampling from conditional distributions \(p_{}(x y)\). We design an SMC algorithm that leverages _twisting_, an SMC technique that modifies proposals and weighting schemes to approach the optimal choices (15; 38). While optimal twisting is intractable, we effectively approximate it with recent heuristic approaches to conditional sampling (eg. 19), and correct the errors by the weighting mechanisms. The resulting algorithm maintains asymptotic exactness to \(p_{}(x y)\), and empirically it can outperform heuristics alone even with just two particles.

We summarize our contributions: (i) We propose a practical SMC algorithm, _Twisted Diffusion Sampler_ or TDS, for asymptotically exact conditional sampling from diffusion models; (ii) We show that TDS applies to a range of conditional generation problems, and extends to Riemannian manifold diffusion models; (iii) On MNIST inpainting and class-conditional generation tasks we demonstrate TDS's empirical improvements beyond heuristic approaches; and (iv) On protein motif-scaffolding problems with short scaffolds TDS provides greater flexibility and achieves higher success rates than the state-of-the-art conditionally trained model.

## 2 Background: Diffusion models and sequential Monte Carlo

**Diffusion models.** A diffusion model generates a data point \(x^{0}\) by iteratively refining a sequence of noisy data \(x^{t}\), starting from pure noise \(x^{T}\). This procedure parameterizes a distribution of \(x^{0}\) as the marginal of a length \(T\) Markov chain

\[p_{}(x^{0})= p(x^{T})_{t=1}^{T}p_{}(x^{t-1} x^{t})dx ^{1:T},\] (1)

where \(p(x^{T})\) is an easy-to-sample noise distribution, and each \(p_{}(x^{t-1} x^{t})\) is the transition distribution defined by the \((T-t)^{}\) refinement step.

Diffusion models \(p_{}\) are fitted to match a data distribution \(q(x^{0})\) from which we have samples. To achieve this goal, a _forward process_\(q(x^{0})_{t=1}^{T}q(x^{t} x^{t-1})\) is set to gradually add noise to the data, where \(q(x^{t} x^{t-1})=(x^{t};x^{t-1},^{2})\), and \(^{2}\) is a positive variance. To fit a diffusion model, one finds \(\) such that \(p_{}(x^{t-1} x^{t}) q(x^{t-1} x^{t})\), which is the reverse conditional of the forward process. If this approximation is accomplished for all \(t\), and if \(T^{2}\) is big enough that \(q(x^{T})\) may be approximated as \(q(x^{T})(0,T^{2})=:p(x^{T})\), then we will have \(p_{}(x^{0}) q(x^{0})\).

In particular, when \(^{2}\) is small enough then the reverse conditionals of \(q\) are approximately Gaussian,

\[q(x^{t-1} x^{t})(x^{t-1};x^{t}+^{2}_{ x^{t}} q(x^{t}),^{2}),\] (2)where \(q(x^{t})= q(x^{0})q(x^{t} x^{0})dx^{0}\) and \(_{x^{t}} q(x^{t})\) is known as the (Stein) score . To mirror eq. (2), diffusion models parameterize \(p_{}(x^{t-1} x^{t})\) via a _score network_\(s_{}(x^{t},t)\)

\[p_{}(x^{t-1} x^{t}):=(x^{t-1};x^{t}+^{2}s_{ }(x^{t},t),^{2}).\] (3)

When \(s_{}(x^{t},t)\) is trained to approximate \(_{x^{t}} q(x^{t})\), we have \(p_{}(x^{t-1} x^{t}) q(x^{t-1} x^{t})\).

Notably, approximating the score is equivalent to learning a _denoising_ neural network \(_{}(x^{t},t)\) to approximate \(_{q}[x^{0} x^{t}]\). The reason is that by Tweedie's formula [12; 28]\(_{x^{t}} q(x^{t})=(_{q}[x^{0} x^{t}]-x^{t})/t ^{2}\) and one can set \(s_{}(x^{t},t):=(_{}(x^{t},t)-x^{t})/t^{2}\). The neural network \(_{}(x^{t},t)\) may be learned by denoising score matching (see [e.g. 18; 35]). For the remainder of paper we drop the argument \(t\) in \(_{}\) and \(s_{}\) when it is clear from context.

Appendix A generalizes the formulation above to diffusion process formulations that are commonly used in practice.

**Sequential Monte Carlo.** Sequential Monte Carlo (SMC) is a general tool to approximately sample from a sequence of distributions on variables \(x^{0:T}\), terminating at a final target of interest [5; 10; 14; 25]. SMC approximates these targets by generating a collection of \(K\)_particles_\(\{x^{t}_{k}\}_{k=1}^{K}\) across \(T\) steps of an iterative procedure. The key ingredients are proposals \(r_{T}(x^{T})\), \(\{r_{t}(x^{t} x^{t+1})\}_{t=0}^{T-1}\) and weighting functions \(w_{T}(x^{T})\), \(\{w_{t}(x^{t},x^{t+1})\}_{t=0}^{T-1}\). At the initial step \(T\), one draws \(K\) particles of \(x^{T}_{k} r_{T}(x^{T})\) and sets \(w^{T}_{k}:=w_{T}(x^{T}_{k})\), and sequentially repeats the following for \(t=T-1,,0\):

* _resample_ \[\{x^{t}_{k}\}_{k=1}^{K}(\{x^{t+1:T}_{k}\}_{k=1} ;\{w^{t+1}_{k}\}_{k=1}^{K})\]
* _propose_ \[x^{t}_{k} r_{t}(x^{t} x^{t+1}_{k}), k=1,,K\]
* _weight_ \[w^{t}_{k}:=w_{t}(x^{t}_{k},x^{t+1}_{k}), k=1,,K\]

The proposals and weighting functions together define a sequence of _intermediate target_ distributions,

\[_{t}(x^{0:T}):=_{t}}r_{T}(x^{T})_{t^{ }=0}^{T-1}r_{t^{}}(x^{t^{}} x^{t^{}+1}) w_{T}(x^{T})_{t^{}=t}^{T-1}w_{t^{}}(x^{t^{}},x^{ t^{}+1})\] (4)

where \(_{t}\) is a normalization constant. A classic example that SMC applies is the state space model [25, Chapter 1] that describes a distribution over a sequence of latent states \(x^{0:T}\) and observations \(y^{0:T}\). Each intermediate target \(_{t}\) is constructed to be the posterior \(p(x^{0:T} y^{t:T})\) given the first \(T-t+1\) observations, and the final target \(p(x^{0:T} y^{0:T})\) is the posterior given all observations.

The defining property of SMC is that the weighted particles at each \(t\) form discrete approximations \((_{x^{}=1}^{K}w^{t}_{k^{}})^{-1}_{k=1}^{K}w^{k}_{t}_{ x^{t}_{k}}(x^{t})\) (where \(\) is a Dirac measure) to \(_{t}(x^{t})\) that become arbitrarily accurate in the limit that many particles are used [5, Proposition 11.4]. So, by choosing \(r_{t}\) and \(w_{t}\) so that \(_{0}(x^{0})\) matches the desired distribution, one can guarantee arbitrarily low approximation error in the large compute limit.

## 3 Twisted Diffusion Sampler: SMC sampling for diffusion model conditionals

Consider conditioning information \(y\) associated with a given likelihood function \(p_{y|x^{0}}(y|x^{0})\). We embed \(y\) in a joint model over \(x^{0:T}\) and \(y\) as \(p_{}(x^{0:T},y)=p_{}(x^{0:T})p_{y|x^{0}}(y|x^{0}),\) where \(y\) and \(x^{1:T}\) are conditionally independent given \(x^{0}\). Our goal is to sample from the conditional \(p_{}(x^{0} y)\).

In this section, we develop Twisted Diffusion Sampler (TDS), a practical SMC algorithm targeting \(p_{}(x^{0} y)\). First, we describe how the Markov structure of diffusion models permits a factorization of an extended conditional distribution to which SMC applies. Then, we show how a diffusion model's denoising predictions support the application of _twisting_, an SMC technique in which one uses proposals and weighting functions that approximate the "optimal" ones. Lastly, we extend TDS to certain "inpainting" problems where \(p_{y|x^{0}}(y|x^{0})\) is not smooth, and to Riemannian diffusion models.

### Conditional diffusion sampling as an SMC procedure

The Markov structure of the diffusion model permits a factorization that is recognizable as the final target of an SMC algorithm. We write the conditional distribution, extended to include \(x^{1:T}\), as

\[p_{}(x^{0:T} y)=(x^{0:T},y)}{p_{}(y)}=(y)}p(x^{T})_{t=0}^{T-1}p_{}(x^{t} x^{t+1}) p_{y|x^{0}}(y|x^{0})\] (5)with the desired marginal, \(p_{}(x^{0} y)\). Comparing the diffusion conditional of eq. (5) to the SMC target of eq. (4) suggests SMC can be used.

For example, consider a first attempt at an SMC algorithm. Set the proposals as \(r_{T}(x^{T})=p(x^{T})\ \ \ \ r_{t}(x^{t} x^{t+1})=p_{}(x^{t}  x^{t+1})\ \ \ \ 1 t T,\) and weighting functions as \(w_{T}(x^{T})=w_{t}(x^{t},x^{t+1})=1\ \ \ \ 1 t T\ \ \ \ \ w_{0}(x^{0},x^{1})=p_{y  x^{0}}(y|x^{0}).\)

Substituting these choices into eq. (4) results in the desired final target \(_{0}=p_{}(x^{0:T} y)\) with normalizing constant \(_{0}=p_{}(y)\). As a result, the associated SMC algorithm produces a final set of \(K\) samples and weights \(\{x_{k}^{0};w_{k}^{0}\}_{k=1}^{K}\) that provides an asymptotically accurate approximation \(_{K}(x^{0} y):=(_{k}^{K}w_{k}^{0})^{-1}_{k=1}^{K}w_{0}^ {0}_{x_{k}^{0}}^{(x^{0})}\) to the desired \(p_{}(x^{0} y)\).

The approach above is simply importance sampling with proposal \(p_{}(x^{0:T})\); with all intermediate weights set to 1, one can skip resampling steps to reduce the variance of the procedure. Consequently, this approach will be impractical if \(p_{}(x^{0} y)\) is too dissimilar from \(p_{}(x^{0})\) as only a small fraction of unconditional samples will have high likelihood: the number of particles required for accurate estimation of \(p_{}(x^{0:T} y)\) is exponential in \([p_{}(x^{0:T} y)\ \|\ p_{}(x^{0:T})]\).

### Twisted diffusion sampler

_Twisting_ is a technique in the SMC literature intended to reduce the number of particles required for good approximation accuracy [15; 16]. Loosely, it introduces a sequence of _twisting functions_ that modify the naive proposals and weighting functions, so that the resulting intermediate targets are closer to the final target of interests. We refer the reader to Naesseth et al. [25, Chapter 3.2] for background on twisting in SMC.

**Optimal twisting.** Consider defining the twisted proposals \(r_{t}^{*}\) by multiplying the naive proposals \(r_{t}\) described in Section 3.1 by \(p_{}(y x^{t})\) as

\[r_{T}^{*}(x^{T}) p(x^{T})p_{}(y|x^{T})\ \ r_{t}^{*}(x^{t}|x^{t+1})  p_{}(x^{t}|x^{t+1})p_{}(y|x^{t})\ \ \ \ \ \ \ \ \ 0 t<T.\] (6)

The factors \(p_{}(y x^{t})\) are the _optimal_ twisting functions because they permit an SMC sampler that draws exact samples from \(p_{}(x^{0:T} y)\) even when run with a single particle.

To see that a single particle is an exact sample, by Bayes rule, the proposals in eq. (6) reduce to

\[r_{T}^{*}(x^{T})=p_{}(x^{T} y)\ \ r_{t}^{*}(x^{t} x^{t+ 1})=p_{}(x^{t} x^{t+1},y)\ \ \ \ \ \ \ \ \ 0 t<T.\] (7)

As a result, if one samples \(x^{T} r_{T}^{*}(x^{T})\) and \(x^{t} r_{t}^{*}(x^{t} x^{t+1})\) for \(t=T-1,,0\), by (i) the law of total probability and (ii) the chain rule of probability, one obtains \(x^{0} p_{}(x^{0} y)\) as desired.

However, we cannot readily sample from each \(r_{t}^{*}\) because \(p_{}(y x^{t})\) is not analytically tractable. The challenge is that \(p_{}(y x^{t})= p_{y x^{0}}(y|x^{0})p_{}(x^{0} x^{ t})dx^{0}\) depends on \(x_{t}\) through \(p_{}(x^{0} x^{t})\). The latter in turn requires marginalizing out \(x^{1},,x^{t-1}\) from the joint density \(p_{}(x^{0:t-1} x^{t})\), whose form depends on \(t\) calls to the neural network \(_{}\).

**Tractable twisting.** To avoid this intractability, we approximate the optimal twisting functions by

\[_{}(y x^{t}):=p_{y x^{0}}(y|_{}(x^{t})) p _{}(y x^{t}),\] (8)

which is the likelihood function evaluated at \(_{}(x^{t})\), the denoising estimate of \(x^{0}\) at step \(t\) from the diffusion model. This tractable twisting function \(_{}(y x^{t})\) is the key ingredient needed to define the Twisted Diffusion Sampler (TDS, Algorithm 1). We motivate and develop its components below.

The approximation in eq. (8) offers two favorable properties. First, \(_{}(y x^{t})\) is computable because it depends on \(x^{t}\) only though one call to \(_{}\), instead of an intractable integral over many calls as in the case of optimal twisting. Second, \(_{}(y x^{t})\) becomes an increasingly accurate approximation of \(p_{}(y x^{t})\), as \(t\) decreases and \(p_{}(x^{0} x^{t})\) concentrates on \(_{p_{}}[x^{0} x^{t}]\), which \(_{}\) is fit to approximate; at \(t=0\), where we can choose \(_{}(x^{0})=x^{0}\), we obtain \(_{}(y x^{0})=p_{y x^{0}}(y x^{0})\).

We next use eq. (8) to develop a sequence of twisted proposals \(_{}(x^{t} x^{t+1},y)\), to approximate the optimal proposals \(p_{}(x^{t} x^{t+1},y)\) in eq. (7). Specifically, we define twisted proposals as

\[_{}(x^{t} x^{t+1},y) :=(x^{t};x^{t+1}+^{2}s_{}(x^{t+1},y), ^{2}),\] (9) \[ s_{}(x^{t+1},y) :=s_{}(x^{t+1})+_{x^{t+1}}_{}(y  x^{t+1})\] (10)

is an approximation of the conditional score, \(s_{}(x^{t+1},y)_{x^{t+1}} p_{}(x^{t+1} y)\), and \(^{2}\) is the proposal variance. For simplicity one could choose \(^{2}=^{2}\) to match the variance of \(p_{}(x^{t} x^{t+1})\).

Equation (9) builds on previous works (e.g. ) that seek to approximate the reversal of a conditional forward process. The gradient in eq. (10) is computed by back-propagating through \(_{}(x^{t+1})\). We further discuss this technique, which has been used before (e.g. in , in Section 4.

Twisted targets and weighting functions.Because \(_{}(x^{t} x^{t+1},y)\) will not in general coincide with the optimal twisted proposal \(p_{}(x^{t} x^{t+1})\), we must introduce non-trivial weighting functions to ensure the resulting SMC sampler converges to the desired final target. In particular, we define _twisted_ weighting functions as

\[w_{t}(x^{t},x^{t+1}):=(x^{t} x^{t+1})_{}(y  x^{t})}{_{}(y x^{t+1})_{}(x^{t} x^ {t+1},y)}, t=0,,T-1\] (11)

and \(w_{T}(x^{T}):=_{}(y x^{T})\). The weighting functions in eq. (11) recover the optimal, constant weighting functions if all other approximations at play are exact.

These tractable twisted proposals and weighting functions define intermediate targets that gradually approach the final target \(_{0}=p_{}(x^{0:T} y)\). Substituting into eq. (4) each \(_{}(x^{t} x^{t+1},y)\) in eq. (9) for \(r_{t}(x^{t} x^{t+1})\) and \(w_{t}(x^{t},x^{t+1})\) in eq. (11) and then simplifying we obtain the intermediate targets

\[_{t}(x^{0:T}) p_{}(x^{0:T} y)_{ }(y x^{t})}{p_{}(y x^{t})}_{t^{}=t}^{t-1}_{}(x^{t^{}} x^{t^{}+1},y)}{p_{}(x^{t^{ }} x^{t^{}+1},y)}.\] (12)

The right-hand bracketed term in eq. (12) can be understood as the discrepancy of \(_{t}\) from the final target \(_{0}\) accumulated from step \(t\) to \(0\) (see Appendix A.3 for a derivation). As \(t\) approaches \(0\), \(_{}(y x^{t})\) improves as an approximation of \(p_{}(y x^{t}),\) and the \(t\)-term product inside the bracket consists of fewer terms - the latter accounts for the discrepancy between practical and optimal proposals. Finally, at \(t=0,\) because \(_{}(y x^{0})=p_{}(y x^{0})\) by construction, eq. (12) reduces to \(_{0}(x^{0:T})=p_{}(x^{0:T} y),\) as desired.

**The TDS algorithm and asymptotic exactness.** Together, the twisted proposals \(_{}(x^{t} x^{t+1},y)\) and weighting functions \(w_{t}(x^{t},x^{t+1})\) lead to _Twisted Diffusion Sampler_, or TDS (Algorithm 1). While Algorithm 1 states multinomial resampling for simplicity, in practice other resampling strategies (e.g. systematic (5, Ch. 9)) may be used as well. Under additional conditions, TDS provides arbitrarily accurate estimates of \(p_{}(x^{0} y)\). Crucially, this guarantee does not rely on assumptions on the accuracy of the approximations used to derive the twisted proposals and weights. Appendix A provides the formal statement with complete conditions and proof.

**Theorem 1**.: _(Informal) Let \(_{K}(x^{0})=(_{k^{}}^{K}w_{k^{}})^{-1}_{k=1}^{K}w_ {k}_{x^{0}_{k}}(x^{0})\) denote the discrete measure defined by the particles and weights returned by Algorithm 1 with \(K\) particles. Under regularity conditions on the twisted proposals and weighting functions, \(_{K}(x^{0})\) converges setwise to \(p_{}(x^{0} y)\) as \(K\) approaches infinity._

### TDS for inpainting, additional degrees of freedom

The twisting functions \(_{}(y x^{t}):=p_{y|x^{0}}(y|_{}(x^{t}))\) we introduced above are one convenient option, but are sensible only when \(p_{y|x^{0}}(y|_{}(x^{t}))\) is differentiable and strictly positive. We now show how alternative twisting functions lead to proposals and weighting functions that address inpainting problems and more flexible conditioning specifications. In these extensions, Algorithm 1 still applies with the new definitions of twisting functions. Appendix A provides additional details, including the adaptation of TDS to variance preserving diffusion models .

**Inpainting.** Consider the case that \(x^{0}\) can be segmented into observed dimensions \(\) and unobserved dimensions \(}\) such that we may write \(x^{0}=[x^{0}_{},x^{0}_{}}]\) and let \(y=x^{0}_{}\), and take \(p_{y|x^{0}}(y|x^{0})=_{y}(x^{0}_{})\). The goal, then, is to sample \(p_{}(x^{0} x^{0}_{}=y)=p_{}(x^{0}_{}}  x^{0}_{})_{y}(x^{0}_{}})\). Here we define the twisting function for each \(t>0\) as

\[_{}(y x^{t},):=(y;_{ }(x^{t})_{},t^{2}),\] (13)

and set twisted proposals and weights according to eqs. (9) and (11). The variance in eq. (13) is chosen as \(t^{2}=_{p_{}}[x^{t} x^{0}]\) for simplicity; in general, choosing this variance to more closely match \(_{p_{}}[y x^{t}]\) may be preferable. For \(t=0\), we define the twisting function analogously with small positive variance, for example as \(_{}(y x^{0})=(y;x^{0}_{},^{2})\). This choice of simplicity changes the final target slightly; alternatively, the final twisting function, proposal, and weights may be chosen to maintain asymptotic exactness (see Appendix A.4).

**Inpainting with degrees of freedom.** We next consider the case when we wish to condition on some observed dimensions, but have additional degrees of freedom. For example in the context of motif-scaffolding in protein design, we may wish to condition on a functional motif \(y\) appearing _anywhere_ in a protein structure, rather than having a pre-specified set of indices \(\) in mind. To handle this situation, we (i) let \(\) be a set of possible observed dimensions, (ii) express our ambivalence in which dimensions are observed as \(y\) using randomness by placing a uniform prior on \(p()=1/||\) for each \(\), and (iii) again embed this new variable into our joint model to define the degrees of freedom likelihood by \(p(y x^{0})=_{}p_{}(,y x^ {0})=||^{-1}_{}p(y x^{0},)\). Accordingly, we approximate \(p_{}(y x^{t})\) with the twisting function

\[_{}(y x^{t}):=||^{-1}_{ }_{}(y x^{t},),\] (14)

with each \(_{}(y x^{t},)\) defined as in eq. (13). Notably, eqs. (13) and (14) coincide when \(||=1\).

The sum in eq. (14) may be computed efficiently because each term depends on \(x^{t}\) only through the same denoising estimate \(_{}(x^{t})\), which must be computed only once. Since computation of \(_{}(x^{t})\) is the expensive step, the overall run-time is not significantly increased by using even large \(||\).

### TDS on Riemannian manifolds

TDS extends to Riemannian diffusion models on with little modification. Riemannian diffusion models  are structured like in the Euclidean case, but with conditionals defined by tangent normal distributions parameterized with a score approximation followed by a manifold projection step (see e.g. [4; 8]). When we assume that (as, e.g., in ) the model is associated with a denoising network \(_{}\), twisting functions are also constructed analogously. For conditional tasks defined by likelihoods, \(_{}(y x^{t})\) in eq. (8) applies. For inpainting (and by extension, degrees of freedom), we propose

\[_{}(y x^{t})=_{_{}(x^{t})_{}}(y;0,t^{2}),\] (15)

where \(_{_{}(x^{t})_{}}(0,t^{2})\) is a tangent normal distribution centered on \(_{}(x^{t})_{}\). As in the Euclidean case, \(_{}(x^{t} x^{t+1},y)\) is defined with conditional score approximation \(s_{}(x^{t},y)=s_{}(x^{t})+_{x^{t}}(y x^{t})\), which is computable by automatic differentiation. Appendix B provides details.

## 4 Related work

There has been much recent work on conditional generation using diffusion models. These prior works demand either task specific conditional training, or involve unqualified approximations and can suffer from poor performance in practice. We discuss additional related work in Appendix C.

**Gradient guidance.** We use _gradient guidance_ to refer to a general approach that incorporates conditioning information with gradients through the neural network \(_{}(x^{t})\). For example, in aninpainting setting, Ho et al.  propose a Gaussian approximation \((y_{}(x^{t})^{},)\) to \(p_{}(y x^{t})\). This approximation motivates a modified transition distribution as in Equation (9), with the corresponding approximation to the conditional score as \(s_{}(x^{t},y)=s_{}(x^{t})-_{x^{t}}\|y-_{}(x^{t} )_{M}\|^{2}/\). This approach coincides exactly with TDS applied to the inpainting task when a single particle is used. Similar Gaussian approximations have been used by [6; 32] for other inverse problems.

Gradient guidance can also be used with non-Gaussian approximations; e.g. using \(p_{y x^{0}}(y|_{}(x^{t})) p_{}(y x^{t})\) for a given likelihood \(p_{y x^{0}}(y|x^{0})\). This choice again recovers TDS with one particle.

Empirically, these heuristics can have unreliable performance, e.g. in image inpainting problems . By comparison, TDS enjoys the benefits of gradient guidance by using it in proposals, while also providing a mechanism to eliminate approximation error by simulating additional particles.

**Replacement method.** The replacement method  is a method introduced for image inpainting using only unconditional diffusion models. The idea is to replace the observed dimensions of intermediate samples \(x^{t}_{}\), with a noisy version of observation \(x^{0}_{}\). However, it is a heuristic approximation and can lead to inconsistency between inpainted region and observed region . Additionally, the replacement method applies only to inpainting problems. While recent work has extended the replacement method to linear inverse problems [e.g. 22], the approach provides no accuracy guarantees. It is unclear how to extend these methods to arbitrary differentiable likelihoods.

**SMC samplers for diffusion models.** Most closely related to the present work is SMC-Diff , which uses SMC to provide asymptotically accurate conditional samples for the inpainting problem. However, this prior work (i) is limited to the inpainting case, and (ii) provides asymptotic guarantees only under the assumption that the learned diffusion model exactly matches the forward noising process, which is rarely satisfied in practice. Also, SMC-Diff does not leverage twisting functions.

In concurrent work, Cardoso et al.  propose MCGdiff, an alternative SMC algorithm that uses the framework of auxiliary particle filtering  to provide asymptotically exact conditional inference. Compared to TDS, MCGdiff avoids computing gradients of the denoising network but applies only to linear inverse problems, with inpainting as a special case.

## 5 Simulation study and conditional image generation

We first test the dependence of the accuracy of TDS on the number of particles in synthetic settings with tractable exact conditionals in Section 5.1. Section 5.2 compares TDS to alternative approaches on class-conditional image generation, and an image inpainting experiment is included in Appendix D.2. See Appendix D for all additional details.

Our evaluation includes: (1) _TDS_; (2) _TDS-IS_, an importance sampler that uses TDS's proposal; (3) _IS_, a naive importance sampler described in Section 3.1; and (4) _Gradient Guidance_. For inpainting-type problems we further include: (5) _Replacement method_; and (6) _SMC-Diff_.

Each SMC sampler forms an approximation to \(p_{}(x^{0} y)\) with \(K\) weighted particles \((_{k^{}=1}^{K}w_{k^{}})^{-1}_{k=1}^{K}w_{k}_{x _{k}^{0}}\). Gradient guidance and replacement method are considered to form a similar particle-based approximation with \(K\) independent samples viewed as \(K\) particles with uniform weights.

Compute Cost:Compared to unconditional generation, TDS has compute cost that is (i) larger by a constant factor due to the need to backpropogate through the denoising network when computing the conditional score approximation and (ii) linear in the number of particles. As a result, the compute cost at inference cost is potentially large relative for accurate inference to be achieved.

Figure 1: Errors of conditional mean estimations with 2 SEM error bars averaged over 25 replicates. TDS applies to all three tasks and provides increasing accuracy with more particles.

By comparison, conditional training methods provide fast inference by _amortization_, in which most computation is done ahead of time. Hence they may be preferable for applications where sampling time computation is a primary concern. On the other hand, TDS may be preferable when the likelihood criterion is readily available (e.g. through an existing classifier on clean data) but training an amortized model poses challenges; for example, the labeled data, neural-network engineering expertise, and up-front compute resources required for amortization training can be prohibitive.

### Applicability and precision of TDS in two dimensional simulations

We explore two questions in this section: (i) what sorts of conditioning information can be handled by TDS and other methods, and (ii) how does the precision of TDS depend on the number of particles?

To study these questions, we first consider an unconditional diffusion model \(p_{}(x^{0})\) approximation of a bivariate Gaussian. For this choice, the marginals of the forward process are also Gaussian, and so we may define \(p_{}(x^{0:T})\) with an analytically tractable score function without neural network approximation. Consequently, we can analyze the performance without the influence of score network approximation errors. And the choice of a two-dimensional diffusion permits close approximation of exact conditional distributions by numerical integration that can then be used as ground truth.

We consider three test cases defining the conditional information: (1) Smooth likelihood: \(y\) is an observation of the Euclidean norm of \(x\) with Laplace noise, with \(p_{}(y x^{0})=\{\|\|x^{0}\|_{2}-y\|\}/2\). This likelihood is smooth almost everywhere.3 (2) Inpainting: \(y\) is an observation of the first dimension of \(x^{0}\), with \(p_{}(y x^{0},=0)=_{y}(x^{0}_{0})\). (3) Inpainting with degrees-of-freedom: \(y\) is a an observation of either the first or second dimension of \(x^{0}\), with \(=\{0,1\}\) and \(p_{}(y x^{0})=[_{y}(x^{0}_{0})+_{y}(x^{0}_{1 })]..\) In all cases we fix \(y=0\) and consider estimating \(_{p_{}}[x^{0} y]\).

Figure 1 reports the estimation error for the mean of the desired conditional distribution, i.e. \(\| w_{k}x^{0}_{k}-_{q}[x^{0} y]\|_{2}\). TDS provides a computational-statistical trade-off: using more particles decreases mean square estimation error at the \(O(1/K)\) parametric rate (note the slopes of \(-1\) in log-log scale) as expected from standard SMC theory [5, Ch. 11]. This convergence rate is shared by TDS, TDS-IS, and IS in the smooth likelihood case, and by TDS, SMCDiff and in the inpainting case; TDS-IS, IS and SMCDiff are applicable however only in these respective cases, whereas TDS applicable in all cases. The only other method which applies to all three settings is Gradient Guidance, which exhibits significant estimation error and does not improve with many particles.

### Class-conditional image generation

We next study the performance of TDS on diffusion models with neural network approximations to the score functions. In particular, we study the class-conditional image generation task, which involves sampling an image from \(p_{}(x^{0} y) p_{}(x^{0})p_{y x^{0}}(y|x^{0})\), where \(p_{}()\) is a pretrained diffusion model on images \(x^{0}\), \(y\) is a given image class, and \(p_{y x^{0}}(y|)\) is the classification likelihood. To assess the faithfulness of generation, we evaluate _classification accuracy_ on predictions of conditional samples \(x^{0}\) given \(y\), made by the same classifier that specifies the likelihood. In all experiments, we follow the standard practice of returning the denoising mean on the final sample .

On the MNIST dataset, we compare TDS to TDS-IS, Gradient Guidance, and IS. Figure 1(a) compares the conditional samples of TDS and Gradient Guidance given class \(y=7\). Samples from Gradient Guidance have noticeable artifacts, and most of them do not resemble the digit 7; by contrast, TDS produces authentic and correct digits.

Figure 1(b) presents an ablation study of the effect of # of particles \(K\) on MNIST. For all SMC samplers, more particles improve the classification accuracy, with \(K=64\) leading to nearly perfect accuracy. The performance of Gradient Guidance is constant with respect to \(K\) (in expectation). Notably, for fixed \(K\), TDS and TDS-IS have comparable performance and outperform Gradient guidance and IS.

We next apply TDS to higher dimension datasets. Figure 1(c) shows samples from TDS \((K=16)\) using a pre-trained diffusion model and a pretrained classifier on the ImageNet dataset (\(256 256 3\) dimensions). These samples are qualitatively good and capture the class label. Appendix D.2.3 provides more samples, comparision to Classifier Guidance , and results for the CIFAR-10 dataset.

TDS can be extended by exponentiating twisting functions with a _twist scale_. This extension is related to the existing literature of Classifier Guidance that considers re-scaling the gradient of the log classification probability. See Appendix D.2.1 for details and ablation study.

## 6 Case study in computational protein design: the motif-scaffolding problem

The biochemical functions of proteins are typically imparted by a small number of atoms, known as a _motif_, that are stabilized by the overall protein structure, known as the _scaffold_. A central task in protein design is to identify stabilizing scaffolds in response to motifs expected to confer function. We here describe an application of TDS to this task, and compare TDS to the state-of-the-art conditionally-trained model, RFdiffusion. See additional details in Appendix E.

Given a generative model supported on designable protein structures \(p_{}(x^{0})\), suitable scaffolds may be constructed by solving a conditional generative modeling problem . Complete structures are first segmented into a motif \(x^{0}_{}\) and a scaffold \(x^{0}_{}\), i.e. \(x^{0}=[x^{0}_{},x^{0}_{}]\). Putative compatible scaffolds are then identified by (approximately) sampling from \(p_{}(x^{0}_{}\,|\,x^{0}_{})\).

While the conditional generative modeling approach to motif-scaffolding has produced functional, experimentally validated structures for certain motifs , the general problem remains open. Moreover, current methods for motif-scaffolding require one to specify the location of the motif within the primary sequence of the full scaffold; this choice can require expert knowledge and trial and error.

We hypothesized that improved motif-scaffolding could be achieved through accurate conditional sampling. To this end, we applied TDS to FrameDiff, a Riemannian diffusion model that parameterizes protein backbones as a collection of \(N\) rigid bodies (known as residues) in the manifold \(SE(3)^{N}\).4 Each of the \(N\) elements of \(SE(3)\) consists of a rotation matrix and a translation that parameterize the locations of the backbone atoms of each residue.

**Likelihood, twisting, and degrees of freedom.** The basis of our approach to the motif scaffolding problem is analogous to the inpainting case described in Section 3.3. We let \(p_{y|x^{0}}(y|x^{0})=_{y}(x^{0}_{})\), where \(y SE(3)^{M}\) describes the coordinates of backbone atoms of an \(M=||\) residue motif. As such, to define twisting function we adopt the Riemannian TDS formulation described in Section 3.4.

To eliminate the requirement that the placement of the motif within the scaffold be pre-specified, we incorporate the motif placement as a degree of freedom. We (i) treat the indices of the motif within the chain as a mask \(\), (ii) let \(\) be a set of possible masks of size equal to the length of the motif, and (iii) apply Equation (14) to average over these possible masks.

For some scaffolding problems, it is known that all motif residues must appear with contiguous indices. In this case we choose \(\) to be the set of all possible contiguous masks. However, when motif residues are only known to appear in two or more possibly discontiguous blocks, the number of possible placements can be too large and we choose \(\) by randomly sampling at most some maximum number (# Motif Locs.) of masks.

We similarly eliminate the global translation and rotation of the motif as a degree of freedom. The motivation is that restricting to one possible pose of the motif narrows the conditional distribution,

Figure 2: Image class-conditional generation task.

thereby making inference more challenging. For translation, we use a likelihood that is invariant to the motif's center-of-mass and placement in the final scaffold by choosing \(p_{y|x^{0}}(y|x)=_{Py}(Px_{})\) where \(P\) is a projection matrix that removes the center of mass of a vector [see e.g. 39, section 3.3]. For rotation, we average the likelihood across some number (# Motif Rots.) of possible rotations.

**Ablation study.** We first examine the impact of several parameters of TDS on success rate in an in silico _self-consistency_ evaluation . We begin with single problem (5IUS) in the benchmark set introduced by , before testing on the full set. Figure 2(a) (Left) shows that success rate increases monotonically with the number of particles. Non-zero success rates in this setting required accounting for multiple motif locations; Figure 2(a) (Left) uses \(1{,}000\) possible motif locations and \(100\) rotations.

We tested the impact of the degrees of freedom by evaluating the success rate of TDS (K=1) with increasing motif locations and 100 rotations (Figure 2(a) Center Left), and increasing rotations and 1,000 locations (Figure 2(a) Center Right). The success rate was 0% without accounting for either degree of freedom, and increased with larger numbers of locations and rotations. We also explored including a heuristic twist scale as considered for image tasks (Section 5.2); in this case, the twist scale is a multiplicative factor on the logarithm of the twisting function. Figure 2(a) (Right) shows larger twist scales gave higher success rates on this test case, where we use 8 particles, 1,000 possible motif locations and 100 rotations. However, this trend is not monotonic for all problems (Figure P).

**Evaluation on full benchmark.** We next evaluate on TDS on a benchmark set of 24 motif-scaffolding problems  and compare to the previous state of the art, RFdiffusion. RFdiffusion operates on the same rigid body representation of protein backbones as FrameDiff. TDS is run with K=8, twist scale=2, and \(100\) rotations and \(1{,}000\) motif location degrees of freedom (\(100{,}000\) combinations total).

Overall, TDS (applied to FrameDiff) and RFdiffusion have comparable performance (Figure 2(b)); each provides a success rate higher than the other in 11/24 cases; on two problems both methods have a 0% success rate (full results in Figure O). This performance is obtained despite the fact that FrameDiff, unlike RFdiffusion, is not trained to perform motif scaffolding. The division between problems on which each method performs well is primarily explained by total scaffold length, with TDS providing higher success rates on smaller scaffolds.

We suspect the shift in performance with scaffold length owes properties of the underlying diffusion models. First, long backbones generated unconditionally by RFdiffusion are designable with higher frequency than those generated by FrameDiff . Second, unlike RFdiffusion, FrameDiff can not condition on the fixed motif sequence.

## 7 Discussion

We propose TDS, a practical and asymptotically exact conditional sampling algorithm for diffusion models. We compare TDS to other approaches and demonstrate the effectiveness and flexibility of TDS on image class conditional generation and inpainting tasks. On protein motif-scaffolding problems with short scaffolds, TDS outperforms the current (conditionally trained) state of the art.

A limitation of TDS is its requirement for additional computes to simulate multiple particles. While we observe improved performance with just two particles in some cases, the optimal number is problem dependent. Moreover, the computational efficiency depends on how closely the twisting functions approximate exact conditionals, which depends on the unconditional model and conditioning information. Lastly, choosing twisting functions for generic constraints may be challenging. Our future work will focus on addressing these limitations and improving the computational efficiency.

Figure 3: Protein motif-scaffolding case study results