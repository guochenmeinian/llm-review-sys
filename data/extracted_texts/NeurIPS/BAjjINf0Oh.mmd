# Oracle-Efficient Differentially Private Learning with Public Data

Adam Block

Department of Mathematics

MIT

Cambridge, MA 02139

ablock@mit.edu

Mark Bun

Department of Computer Science

Boston University

Boston, MA 02215

mbun@bu.edu

Rathin Desai

Department of Computer Science

Boston University

Boston, MA 02215

rathin@bu.edu

Abhishek Shetty

Department of Computer Science

University of California, Berkeley

Berkeley, CA 94720

shetty@berkeley.edu

Zhiwei Steven Wu

School of Computer Science

Carnegie Mellon University

Pittsburgh, PA 15213

zstevenwu@cmu.edu

###### Abstract

Due to statistical lower bounds on the learnability of many function classes under privacy constraints, there has been recent interest in leveraging public data to improve the performance of private learning algorithms. In this model, algorithms must always guarantee differential privacy with respect to the private samples while also ensuring learning guarantees when the private data distribution is sufficiently close to that of the public data. Previous work has demonstrated that when sufficient public, unlabelled data is available, private learning can be made statistically tractable, but the resulting algorithms have all been computationally inefficient. In this work, we present the first computationally efficient, algorithms to provably leverage public data to learn privately whenever a function class is learnable non-privately, where our notion of computational efficiency is with respect to the number of calls to an optimization oracle for the function class. In addition to this general result, we provide specialized algorithms with improved sample complexities in the special cases when the function class is convex or when the task is binary classification.

## 1 Introduction

Differential privacy (DP) [Dwork et al., 2006] is a standard guarantee of individual-level privacy for statistical data analysis. Algorithmic research on differential privacy aims to understand what statistical tasks are compatible with the definition, and at what cost, e.g., in terms of sample complexity or computational efficiency. Unfortunately, it is known that some tasks may become more expensive or outright impossible to conduct with differential privacy. For example, in the setting of binary classification, there is no differentially private algorithm for solving the simple problem of learning a one-dimensional classifier over the real numbers [Bun et al., 2015, Alon et al., 2019].

Motivated in part by such barriers to full-fledged private learning, many papers have considered relaxing the model to allow the use of auxiliary "public" data Balcan and Feldman (2013); Bassily et al. (2019, 2020, 2022, 2023); Kairouz et al. (2021); Amid et al. (2022); Lowy et al. (2023). Such data may be available if individuals can voluntarily opt-in to share or sell their information to enable a particular task. Alternatively, a data analyst might have background knowledge about the underlying data distribution from the results of previous analyses, or hold a plausible generative model for it. These situations are captured by semi-private learning, first discussed by Balcan and Feldman (2013), formally introduced by Beimel et al. (2014) and subsequently studied by Bassily et al. (2019); Hopkins et al. (2024). In this model, a learning algorithm is given \(n\) "private" samples from a joint distribution \(\) over example-label pairs, as well as \(m\)_unlabeled_ "public" samples from the same marginal distribution over examples. The algorithm must be differentially private with respect to its private dataset, but can depend arbitrarily on its public samples. For learning a binary classifier over a class \(\) with a VC-dimension \(()\), these papers showed that in the presence of \(O(())\) public unlabeled samples, every concept class \(\) is agnostically learnable with \(O(())\) private labeled samples, matching what is achievable without privacy guarantees.

While these results essentially resolve the statistical complexity of semi-private learning, they do not address the question of computational efficiency. These algorithms proceed by drawing enough public samples to construct a cover for the class \(\) with respect to the target marginal distribution on examples, and then using the exponential mechanism (McSherry and Talwar, 2007) to select a hypothesis from this cover that fits the private dataset. As the size of this cover is exponential in \(()\), constructing it explicitly is computationally expensive. This paper aims to address the following question: _Is such computational overhead really necessary if \(\) exhibits additional structure that make non-private learning tractable?_

In this work, we give new semi-private learners that are efficient whenever fast non-private algorithms are available. More specifically, our main result is generic semi-private algorithms for regression and classification that are _oracle-efficient_ in that they run in polynomial time given an oracle solving the _non-private_ empirical risk minimization problem for \(\), and have sample complexity polynomial in the usual parameters such as Gaussian complexity and VC dimension.

**Theorem 1** (Informal version of Theorem 2).: _Fix a function class \(:[-1,1]\). Then there is an oracle-efficient, \((,)\)-differentially private algorithm (Algorithm 2) using \((_{m}}_{m}())\) labeled private samples (where, \(}_{m}()\) denotes the Gaussian complexity of function class \(\)), unlabeled public samples, and calls to an empirical risk minimization oracle for \(\) that learns an approximately optimal predictor \(\)._

While Theorem 1 captures extremely broad learning settings, the polynomials governing its sample complexity are rather large. We identify several important cases in which the sample complexity can be improved and the number of oracle calls is only 2. In the case where the function class \(\) is convex, we give a variant of Algorithm 2, inspired by follow-the-regularized-leader, with significantly improved sample complexity as a function of the desired error (Theorem 3, Algorithm 3). Finally, in the special case of binary classification (i.e., Boolean \(\) under the 0-1 loss), we give a completely different oracle-efficient algorithm with improved sample complexity (Theorem 4, Algorithm 4), which requires the private sample size to grow at the rate of \(O((())^{2})\). Prior work of Bassily et al. (2018) gave an oracle-efficient algorithm in this setting with somewhat better sample complexity than ours, based on a reduction to private classification. Meanwhile, our algorithm has the advantage of being able to guarantee pure (rather than only approximate) differential privacy algorithm, as well as making only two oracle calls as opposed to the polynomially many as a function of \(\) and the target accuracy. Our results in the binary classification setting can also be viewed as an extension of Neel et al. (2019), which gives oracle-efficient private learners for structured function classes \(\) that have a small _universal identification set_(Goldman et al., 1993). Our results relax this stringent combinatorial condition by leveraging a small public unlabelled dataset, which allows us to design an oracle-efficient private learner for any function class \(\) with bounded VC-dimension.

In fact, our results also address a somewhat more general setting than the semi-private model described so far. Specifically, our results automatically handle the setting where there may be a bounded distribution shift between the public and private data. In particular, all of our results hold as long as the public unlabelled data distribution and the private marginal distribution over the feature space have a density ratio bounded by \(\).1 The standard semi-private setting corresponds to the special case where \(=1\). Taking this view, we can interpret our results as oracle-efficient private learning in the _smoothed learning_ setting. Our algorithms achieve accurate learning provided that the private marginal distribution does not deviate too much from a public reference distribution. However, our privacy guarantees hold even if the private data distribution has unbounded distribution shift from the public data.

### Related Work

Our work brings together ideas and techniques from multiple literatures.

Oracle efficiency in private and online learning.Our notion of oracle-efficiency is standard in (theoretical) machine learning to model reductions in a world where worst-case hardness bounds, but optimization heuristics (e.g., integer programming solvers, non-convex optimization) often enjoy success. Within the differential privacy literature, oracle-efficient algorithms are known for binary classification with classes \(\) that admits small universal identification sets (Neel et al., 2019), synthetic data generation (Gaboardi et al., 2014; Nikolov et al., 2013; Neel et al., 2019; Vietri et al., 2020), and certain types of non-convex optimization problems (Neel et al., 2020). Oracle-efficiency is also well-established approach in _online learning_(Kalai and Vempala, 2005; Hazan and Koren, 2016; Kozachinskiy and Steifer, 2023; Haghtalab et al., 2022; Block and Simchowitz, 2022; Block and Polyanskiy, 2023; Block et al., 2023; 20), an area with deep connections to differential privacy (Alon et al., 2019; Abernethy et al., 2019; Bun et al., 2020; Ghazi et al., 2021). Indeed, our new semi-private learning algorithm Algorithm 2 adapts a follow-the-perturbed-leader inspired algorithm (Block et al., 2022) from the setting of _smoothed_ online learning.

DP learning and release with public (unlabelled) dataOur results contribute to a long line of theoretical work that leverages public data for private data analysis. In particular, our work provides general computationally efficient algorithms (in the oracle efficiency sense) for semi-private learning (Beimel et al., 2014). In addition to the work in this direction we discussed above, several recent papers Bassily et al. (2022, 2023) developed efficient algorithms for private learning with domain adaptation from a public source. That work accommodates a more general notion of distribution shift than ours, but makes essential use of _labeled_ public data, as well as handling only restricted concept classes or loss functions. There has also been work that leverages public data to remove statistical barriers in private query release (Bassily et al., 2020) and density estimation Bie et al. (2022), Ben-David et al. (2023). Papernot et al. (2018); Yu et al. (2022), Golatkar et al. (2022), Zhou et al. (2021) and Liu et al. (2021, 2021) give empirical guarantees to the problem of private learning and private synthetic data from public samples respectively.

Smoothed Analysis in Online Learning.Smoothed analysis was pioneered in Spielman and Teng (2004) for the purpose of explaining the empirical success of algorithms whose worst-case behavior is provably intractable. More recently, the framework has come to online learning (Rakhlin et al., 2011; Haghtalab et al., 2020, 2022; Block et al., 2022; Haghtalab et al., 2022; Block and Simchowitz, 2022; Block et al., 2023, 20) in order to circumvent the strong statistical (Rakhlin et al., 2015) and computational (Hazan and Koren, 2016) lower bounds that worst-case data can induce. The assumption of smoothness has also been used in learning more broadly (Durvasula et al., 2023; Cesa-Bianchi et al., 2023) and its assumptions have been relaxed (Block and Polyanskiy, 2023).

## 2 Preliminaries

In this section, we formally introduce our setting. Let \(\) denote the feature space and \(\) be the label space. In general, we consider \(=[-1,1]\), but in the special case of binary classification setting, we have \(=\{0,1\}\). In general, we study learning algorithms \(\) that map a dataset \(\) with \(n\) examples from \(\) to a predictor in a function class \(\). We require \(\) to satisfy _differential privacy_, defined below.

**Definition 1** (Differential Privacy Dwork et al. (2006)).: Let \(:()^{n}\) be a randomized algorithm and \(,^{}()^{n}\) be data sets. We say that \(\) and \(^{}\) are _neighboring_ if \(|^{}|=|^{} | 1\), i.e. they differ in at most one datum. We say that \(\) is \((,)\)_-differentially private_ if for all neighboring datasets \(,^{}\), and for all measurable \(\), it holds that \((()) e^{ }((^{}))+\). If \(=0\), we say that \(\) is \(\)-(purely) differentially private.

As defined, it is trivial to construct algorithms that are differentially private by outputting functions independent of the data set; for an algorithm to be useful, however, we also require that it learns in a meaningful sense. Thus, in the context of learning, we consider the following accuracy desideratum.

**Definition 2**.: Let \(:()^{n}\) be a randomized algorithm. We say that \(\) is an \((,)\)-learner with respect to a measure \(\) on \(\) and loss function \(L:[-1,1]\) if, for \(\) sampled independently from \(\), it holds that \((L(())_{f}L(f)+ ) 1-\). For regression problems, we consider the loss function \(L\) to be induced by a function \(:\), convex and \(\)-Lipschitz in the first argument, such that \(L(f)=_{(X,Y)}[(f(X),Y)]\).

For simplicity, we will denote the empirical loss on a data set \(\) as \(L_{}(f)=_{(X_{i},Y_{i})}(f(X_ {i}),Y_{i})\). We emphasize that in contradistinction to the standard notion of PAC-learnability (Valiant, 1984), our requirement is weaker in that we only require _distribution-dependent_ learning, i.e., the algorithm \(\) is allowed to depend on \(\) in some to-be-specified way. This is necessary in our setting as it is well known that distribution-independent differentially private PAC learning is possible only for very restricted classes of functions \(\) with bounded Littlestone dimension (Alon et al., 2019; Bun et al., 2020). To make private learning statistically tractable for broader classes of functions, we consider the following restriction on \(\):

**Definition 3**.: Given a measure \(()\) and a parameter \((0,1]\), we say that \(_{x}\) is \(\)-smooth with respect to \(\) if \(\|}{d}\|_{}\). We suppose that the learner has access to \(m\) samples \(Z_{1},,Z_{m}\) that are independent of each other and the training data \(\) and thus \(\) may depend on these samples.

We remark that Definition 3 can be significantly relaxed by assuming only that \(D_{f}(_{x}||)\) as in Block and Polyanskiy (2023), where \(D_{f}(||)\) is a sufficiently strong \(f\)-divergence2. In this case, the statistical rates presented below will be worse and depend on \(f\), but the algorithms and privacy guarantees will remain unchanged. Critically, we do not require that our algorithms are private with respect to \(Z_{1},,Z_{m}\), which we treat as public, unlabelled data. The key reason that this public data helps us circumvent the lower bounds is that it gives us access (albeit indirectly) to a small subclass of the hypothesis set that still has approximately good hypotheses. Since our primary focus is to design computationally efficient private learners, we cannot directly handle either the original hypothesis class or the small proxy that the public data gives us access. Instead we suppose access to the following ERM oracle:

**Definition 4**.: Given a function class \(:\), a data set \(=\{x_{1},,x_{m}\}\) and loss functions \(_{1},,_{m}:\), we define the empirical risk minimization oracle \(:\) such that \((,_{})*{argmin}_{f }_{}(f)\), where \(_{}(f)=_{x_{i}}_{i}(f(x_{i}))\).

ERM oracles are standard computational models in many learning domains such as online learning (Kalai and Vempala, 2005; Hazan and Koren, 2016; Block et al., 2022; Hagthalab et al., 2022) and Reinforcement Learning (Foster and Rakhlin, 2020; Foster et al., 2021; Mahmmedi et al., 2023, 2023). Assuming access to ERM allows us to disentangle the computational challenges of optimizing over specific function classes from the specific challenge of differentially private learning as well as to avoid the well-known intractability results for nonconvex optimization (Blum and Rivest, 1988) that do not accurately reflect the realities of modern optimization techniques (e.g., integer program solvers, SGD). We note that our algorithms also work in the case of ERM oracles with additive error by minor modification to the analysis similar to the one in (Block et al., 2022). We remark that applying Neel et al. (2019, Theorem 8) gives a black-box robustification procedure for purely private, oracle-efficient algorithms, which ensures that the privacy guarantees continue to hold even when the oracle may fail to optimize the objective. In particular, Algorithms 3 and 4 below, when run in their pure DP forms can be made robust at a minimal cost on accuracy. We defer to Neel et al. (2019) for further discussion on this topic.

It is well-known that even absent differential privacy guarantees, learning arbitrary function classes is impossible; we now introduce the notions of complexity that are relevant to our results. We begin with the standard notion of VC dimension:

**Definition 5**.: Let \(:\{0,1\}\) be a function class. We say that a set of points \(x_{1},,x_{d}\) shatters \(\) if for all \(_{1:d}\{0,1\}^{d}\), there is some \(f_{}\) such that \(f_{}(x_{i})=_{i}\) for all \(i\). The VC dimension of \(\), denoted \(()\), is the largest \(d\) such that there exists a set of \(d\) points shattering \(\).

In addition to VC dimension, we also use the Gaussian complexity of a function class:

**Definition 6**.: Let \(:[-1,1]\) be a function class and \(x_{1},,x_{m}\) be arbitrary points. We let \(_{m}:\) be the canonical Gaussian process on \(\), i.e.,

\[_{m}(f)=}_{i=1}^{m}_{i} f(x_{i}),\] (1)

where \(_{i}\) are independent standard Gaussians. We define the (data-dependent) Gaussian complexity of \(\) to be \([_{f}_{m}(f)]\), the average Gaussian complexity as \(_{m}()=_{Z}[_{f} _{m}(f)]\), and the worst-case Gaussian complexity of \(\) to be \(}_{m}()=_{x_{1},,x_{m} }[_{f}_{m}(f)]\).

Both \(()\) and \(_{m}()\) are well known measures of complexity from learning theory and their relationships to other notions of complexity like covering number are well-understood (Mendelson and Vershynin, 2003; Wainwright, 2019; Van Handel, 2014). In particular, it is well-known that \(_{m}()=O(()})\)(Dudley, 1969; Mendelson and Vershynin, 2003) and that standard PAC-learning is possible if and only if \(}_{m}()=o()\)(Wainwright, 2019; Van Handel, 2014). We remark that different texts use different scalings for \(_{m}()\), with some replacing the \(m^{-1/2}\) factor in (1) with \(m^{-1}\) and others omitting it entirely; our choice of scaling is motivated by the fact that a natural complexity measure for many (Donsker (Wainwright, 2019)) function classes that our algorithms depend on is \(_{m}}_{m}()\), which is most compactly represented with the present scaling.

Notation.We always reserve \(\) and \(\) for probability and expectation with respect to measures that are clear from the context. We denote by \(()\) the space of measures on some \(\) and for any \(()\) we let \(\|\|_{}\) denote the \(L^{2}()\) norm, i.e., \(\|f\|_{}^{2}=_{Z}[f(Z)^{2}]\). Similarly, for \(m\) points \(Z_{1},,Z_{m}\), we let \(\|\|_{m}\) denote the empirical \(L^{2}\) norm on these points so that \(\|f\|_{m}^{2}=m^{-1}_{i=1}^{m}f(Z_{i})^{2}\). We reserve \(_{m}\) for the canonical empirical Gaussian process on \(\) as in (1) and \(\) for a functional on \(\).

## 3 Algorithms for Differentially Private Learning

In this section, we provide a general template for constructing differentially private learning algorithms with public data and instantiate this template with two oracle-efficient algorithms. Our first algorithm applies to arbitrary bounded function classes, whereas the second algorithm only applies to convex classes but has an improved sample complexity. Our general template is broken into the following two steps: (i) Use ERM (cf. Definition 4) and the public data to construct an initial estimate \(\) that is a good learner and satisfies stability with respect to \(\|\|_{m}\); (ii) Output \(\) as the function that minimizes \(\|f--\|_{m}\), where \( 0\) is a scale and \(=(_{1},,_{m})\) is a vector of independent random variables sampled according to some distribution \(\). The second step is accomplished through Algorithm 1 and is the same across our algorithms. The first step, however,is algorithm-specific and is the primary factor affecting the sample complexity. The intuition for our template is as follows. We need to show that \(\) is both a learner and is differentially private. To see why the template produces a good learner, note that if \(Z_{i}\) are independent and \(m\) is sufficiently large, then \(_{m}_{}\). Thus if \(\) is small, then \(-_{} 1\) and \(_{}[L()]-_{}[L( )] 1\) whenever \(\) is Lipschitz. By smoothness, a similar guarantee holds for expectations with respect to \(\) and thus \(\) is a good learner. To see why \(\) is differentially private, note that by choosing \(\) to be a standard Gaussian, we can ensure that the likelihood ratios of choosing \(\) given \(\) versus \(^{}\) are controlled by \(-^{}_{m}\). Thus, if \(\) is stable with respect to \(_{m}\), then \(\) will be private. The intuition of the stability of \(\) is discussed in Section 4.

We now make the above intuition precise by instantiating this template in our most general setting in Algorithm 2. We construct \(\) by running \(\) on a perturbed version of the empirical risk minimization problem and then averaging. Specifically, for \(j[J]\), we define \(^{(j)}:\) as a sample path of a noncentred Gaussian process in (2) and let \(_{j}\) denote the minimizer of \(^{(j)}\) over \(\). We then output \(\) as the average of \(_{1},,_{J}\). We present motivation for the particular choice of \(\), as well as the analogue in Algorithm 3, in the subsequent section. The following theorem shows that if \(\) is chosen correctly, this algorithm is an oracle-efficient differentially private learner whenever \(_{x}\) is \(\)-smooth with respect to \(\).

```
1:Input Oracle \(\), perturbation parameter \(>0\), public data set \(}_{x}=\{Z_{1},,Z_{m}\}\), private data set \(=\{(X_{i},Y_{i})|1 i n\}\), function class \(\), loss function \(\), noise level \(>0\), number of iterations \(J\), noise distribution \(()\).
2:for\(j=1,2,,J\)do
3: Sample\(_{1}^{(j)},,_{m}^{(j)}(0,1)\).
4: Define\(_{m}^{(j)}:\) such that \[_{m}^{(j)}(f)=}_{i=1}^{m}_{i} f(Z_{i}).\] (2)
5: Define\(^{(j)}:\) such that \(^{(j)}(f)=_{(X_{i},Y_{i})}(f(X_{i}),Y_{i})+ _{m}^{(j)}(f)\).
6: Define\(_{j}=(^{(j)},)\)
7:end
8:Define\(=_{j=1}^{J}_{j}\).
9:Output\(=(,,,}_{x})\)\(\) By running Algorithm 1 ```

**Algorithm 2**Oracle Efficient Private Learner (Perturbation)

**Theorem 2**.: _Suppose that \(:[-1,1]\) is a function class and \(()\) is a measure such that \(_{f} f_{}\). Let \(:[-1,1][-1,1]\) be a loss function that is convex and \(\)-Lipschitz in its first argument. If \(=(0,1)\) in Algorithm 1, then for any \(,,,(0,1)\), there are choices of \(,>0\) and \(J,m\), all polynomial in problem parameters and given in Appendix C.6, such that if_

\[n=(_{m}}_{m}(), (),(),) ^{-3}^{-14},\]

_then the \(\) returned by Algorithm 2 is \((,)\)-differentially private. If \(_{x}\) is \(\)-smooth with respect to \(\), then \(\) is an \((,)\)-learner with respect to \(_{x}\) and \(\)._

We emphasize that Algorithm 2 is _always differentially private_, independent of \(\); however, our algorithm is only a good learner if \(_{x}\) is smooth with respect to \(\). We remark that all of the conditions in Theorem 2 are standard with the exception of the assumption that \( f_{} 2/3\) for all \(f\). This condition is easy to ensure by setting \(=}{3}\), where \(z^{}\) is a distinguished point such that \(f(z^{})=1\) for all \(f\); note that this process deflates \(\) at most by a factor of 3 while ensuring the lower bound on the norm of \(f\). Replacing \(\) by \(\) then suffices to ensure that Theorem 2 holds.

We further remark that it is classical that the complexity notion \(_{m}}_{m}()\) is upper bounded by \(()}\) for binary function classes and \(|)}\) for finite classes (Wainwright, 2019), ensuring that the proven sample complexity is polynomial in all standard notions of function class complexity. For even more complex function classes, where \(}_{m}()=(1)\), similar results hold, although with worse rates; further dicussion, as well as the precise polynomial dependence of hyperparameters and sample complexity, can be found in Appendix C.

While Algorithm 2 succeeds in our desiderata under general assumptions, the sample complexity is a large polynomial of the desired accuracy. Indeed, the sample complexity of Algorithm 2 scales like \(O(()^{-3}^{-14})\), which is significantly worse than the \(O(()^{-2})\) sample complexity that a non-private algorithm such as ERM can achieve (Wainwright, 2019) or even the \(O(()^{-2}^{-2})\) sample complexity achievable by private, inefficient algorithms with public data (Bassily et al., 2020). Furthermore, we are unable to achieve a pure differential privacy guarantee with this algorithm. We now address both issues by providing an improved algorithm in the special case that the function class \(\) is _convex_. While we still use Algorithm 1 as a subroutine, in Algorithm 3, motivated by the difference between Follow the Perturbed Leader (FTPL) and Follow the Regularized Leader (FTRL) (Kalai and Vempala, 2005; Cesa-Bianchi and Lugosi, 2006) in online learning, we modify the way in which we choose our initial estimator \(\). In particular, we eliminate the averaging step and redefine \(\) to be a strongly convex _regularizer_ instead of a Gaussian Process _perturbation_. More specifically, we define \(\) in (3) as the empirical loss regularized by \(_{m}^{2}\) and output \(=(,)\). We have the following result:

**Theorem 3**.: _Suppose that \(:[-1,1]\) is a convex function class and \(:[-1,1][-1,1]\) is convex and \(\)-Lipschitz in its first argument. Suppose that \(Z_{1},,Z_{m}\) are independent and \(=(0,1)\). Then there are \(,,m\) polynomial in problem parameters and given in Appendix C.6 such that, if_

\[n=((^{-1}),(^{-1}),)(_{m}}_{m}())^{2} ^{-1}^{-5}\]

_then the \(\) returned by Algorithm 3 is \((,)\)-differentially private. If \(_{x}\) is \(\)-smooth with respect to \(\), then \(\) is an \((,)\)-learner with respect to \(_{x}\) and \(\). Furthermore, if \(=Lap(1)\), and_

\[n=(_{m}}_{m}(), (^{-1}),)^{-1}^{-6},\]

_then Algorithm 3 is \(\)-_purely _differentially private and an \((,)\)-PAC learner for any \(\)-smooth \(_{x}\)._

As in the case of Theorem 2, we can easily generalize Theorem 3 to apply to function classes \(\) where \(}_{m}()=(1)\) at the cost of worse polynomial dependence in the sample complexity. We again omit this case for the sake of simplicity. While the sample complexity of Algorithm 3 is a marked improvement over that of Algorithm 2, it remains a far cry from the desired \(O(^{-2})\) rates of non-private learning that computationally _inefficient_ private algorithms leveraging public data are able to achieve (Bassily et al., 2019); we leave the interesting question of producing an oracle-efficient private algorithm with optimal sample complexity to future work.

Finally, we remark that even in the case where \(\) is not convex, Algorithm 3 can be applied to \(()\), the convex hull of \(\), if we assume the learner has access to \(^{}\), a stronger ERM oracle that can optimize over \(()\). In this case, Theorem3 supercedes Theorem2 as it is easy to see that \(}_{m}()=}_{m}(())\) and thus the sample complexity of Algorithm3 is strictly better than that of Algorithm2 and the pure differential privacy result applies.

## 4 Analysis Techniques

In this section, we outline the proofs of our main results, with full details deferred to AppendixC. As is suggested by our template, the proof of the privacy part of Theorem2 rests on two results: the first shows that if \(\) is a standard Gaussian (resp. exponential) then stability of \(\) with respect to \(_{m}\) can be translated into differential privacy. The second shows that \(\) will be stable with respect to \(_{m}\). Similarly, the proof that \(\) is a good learner first shows that \(\) is a good learner and then that \(\) and \(\) are close. We begin with the more technically novel parts and show that, under standard assumptions, Algorithms2 and 3 result in \(\) that are stable in \(_{m}\). In our proof of Theorem2, we provide an improved analysis of the Gaussian anti-concentration result from Block et al. (2022), which may be of independent interest. We prove the stability of Algorithm3 using a technique common in online learning. We then show that stability in \(_{m}\) can be boosted to a differential privacy guarantee using the Gaussian and Laplace Mechanisms (Dwork et al., 2006). Finally, we apply standard learning theoretic techniques to show that \(\) is a good learner.

### Stability Analysis

In this section, we explain how to prove that Algorithms2 and 3 are stable with respect to \(_{m}\). Our stability results further cement the connections between differential privacy and online learning noted in Abernethy et al. (2019) as both algorithms are primarily motivated by online learning techniques. We begin by describing the stability analysis of Algorithm2. The key lemma underlying the stability of Algorithm2 is an improved version of a Gaussian anti-concentration result from Block et al. (2022), which may be of independent interest.

**Proposition 1**.: _Let \(\) denote a subspace of the unit ball with respect to a norm \(\) induced by an inner product \(,\) and let \(m,m^{}:\) be measurable functions such that \( m-m^{}_{}\). If \(\) is a centred Gaussian process on \(\) with covariance kernel given by \(,\), \((f)=m(f)+(f)\), \(=_{f}\,(f)\), and \(^{}\) and \(^{}\) are defined similarly, then for any \(,>0\), it holds that \((-^{}>) ^{2}}[_{f }(f)]\), where \(^{2}=_{f}[(f)^{2}]\)._

The proof proceeds in a similar way to that of Lemma33 from Block et al. (2022), but involves a tighter analysis in several steps in order to improve the bound. The intuition for the result is straightforward: if \(\) is the minimizer of the Gaussian process \(\), then with reasonable probability, _almost minimizers_ of \(\) (as measured by the tolerance \(\)) are within a radius \(\) of \(\) as long as the Gaussian process is nontrivial in the sense that all indices \(f\) have sufficiently high variance. Moreover, the quantitative control on the probability of this event depends in a natural way both on \(\) and \(\) as well as on the Gaussian process \(\): more complex spaces \(\) and lower variance processes lead to a worse anti-concentration guarantee. Finally, we note that Proposition1 is an improvement of Lemma33 from Block et al. (2022) in that the quantitative bound on the probability of anti-concentration is tighter by polynomial factors in \(,\), and \(\).

Like essentially all anti-concentration results (Chernozhukov et al., 2015), Proposition1 holds only with moderate probability in the sense that the guarantee is polynomial in the scale \(\); this fact is in contradistinction to _concentration_ inequalities which tend to hold with high probability exponential in the scale. This discrepancy is precisely what motivates the averaging in Line7 of Algorithm2. Indeed, we can use Proposition1 to show that if \(_{j}\) is as in Line6 of Algorithm2 and \(^{}_{j}\) is defined analogously with respect to \(^{}\), then with moderate probability \(_{j}-^{}_{j}_{m}\) is small. Using Jensen's inequality and a standard Chernoff bound, we can then boost this moderate probability guarantee into a high probability guarantee to show that if \(J\) is sufficiently large, then \(-^{}_{m}\) is small with high probability. We formalize this argument in the following lemma:

**Lemma 1** (Stability of Algorithm2).: _Suppose that \(:[-1,1]\) is a function class and \(:[-1,1]^{ 2}\) is a bounded loss function. Suppose that \(,^{}\) are neighboring datasets and let \(\) be as in Line5 of Algorithm2 and \(^{}\) be defined analogously with respect to \(^{}\). Then_for any \(,>0\), with probability at least \(1-\), over the Gaussian processes \(^{(j)}\), \(-^{}_{m}^{2/3}}([_{f}_{m}(f) ])^{1/3}+\}}{J}}\)._

We note that the worse dependence on \(\) in Lemma1 as compared to Proposition1 arises from integrating the tail bound to obtain the control on \(_{j}-^{}_{j}_{m}\) in expectation necessary to apply Jensen's inequality; details can be found in AppendixC.1.

We now turn to the stability of Algorithm3. The proof is based on a technique borrowed from online learning and the analysis of the _Follow the Regularized Leader_ (FTRL) algorithm (Gordon, 1999; Cesa-Bianchi and Lugosi, 2006).

**Lemma 2** (Stability of Algorithm3).: _Suppose that \(\) is convex and \(\)-Lipschitz in its first argument. Let \(,^{}\) denote neighboring data sets and let \(\) denote the output of Line3 in Algorithm3 and \(^{}\) be the analogous output evaluated on \(^{}\). If \(\) is convex, then, \(-^{}_{m}}\)._

The proof of Lemma2 can be found in AppendixC.2 and rests on elementary properties of strongly convex functions. We note that relative to Lemma1, the dependence on \(\) in Lemma2 is improved, which in turn leads to the better sample complexity exhibited in Theorem3. With stability of Algorithms2 and 3 thus established, we proceed to analyze the effect of the output perturbation.

### Output Perturbation Analysis

We now turn to the analysis of Algorithm1. In order to boost a stability-in-norm guarantee into one for differential privacy while remaining a good learner, we require the output perturbation to be sufficiently small as to not not affect the learning guarantee of \(\) while at the same time being sufficiently large as to ensure privacy. We balance these two competing objectives by tuning the variance of the added noise. This part of the analysis is relatively standard in the differential privacy literature (Chaudhuri et al., 2011; Neel et al., 2019), with the bound on the size of the output perturbation following from standard tail bounds on Gaussian and Laplace random vectors. The privacy guarantees are similarly standard and summarized in the following lemma:

**Lemma 3**.: _Suppose \(\) is the output of an algorithm \(:\) that is \(\)-stable with respect to \(_{m}\), i.e., for any neighboring data set \(^{}\), it holds that \(()-(^{}) _{m}\). Then applying Algorithm1 with \(=(0,1)\) to \(\) results in an \((,)\)-private algorithm if \(}(1+ )})\). Similarly, if \(=()\), then the algorithm is \(\)-purely private if \(m^{3/2}/\)._

This standard result is proved in AppendixC.3. Note that, perhaps counterintuitively, the privacy loss increases with the public data. This relationship occurs because the algorithm is implicitly discretizing the function class, where more public data leads to a finer discretization; though finer discretizations lead to higher accuracy, they also leads to more privacy loss. Furthermore, note that even were the whole marginal distribution known, the privacy-accuracy tradeoff is dictated by the number of _labelled_ samples, not \(m\).

The balance between privacy and learning is quantified in the choices of \(m\) and \(\). If \(\) is too large, then \(\) will be private but a poor learner, whereas the opposite occurs if \(\) is too small. Similarly, if \(m\) is too large then privacy is reduced whereas if \(m\) is too small then \(_{m}\) is a poor approximation for \(_{}\).

### Learning Guarantees and Concluding the Proof

By combining Lemma1 (resp. Lemma2) with Lemma9, we can establish the privacy of Algorithm2 (resp. Algorithm3) as long as the tuning parameters \(m,,\), and \(J\) are chosen correctly. We now sketch the proof that these algorithms comprise good learners in the sense of Definition2. We break our proof into three components, the first two of which are standard learning theoretic results. The first lemma says that if \(m 1\), then \(_{m}\) is a good approximation for \(_{}\):

**Lemma 4**.: _Let \(:[-1,1]\) be a bounded function class and let \(Z_{1},,Z_{m}\) be independent samples. Then for any \(>0\) it holds with probability at least \(1-\) that for all \(f\),_

\[ f_{} 2 f_{m}+ (}_{m}()+}{})\]Lemma 4 is a standard bound from learning theory (Bousquet, 2002; Rakhlin et al., 2017) and is proved in Appendix C.7 for the sake of completeness. The second component is given by Lemma 13 in Appendix C.5, which amounts to a classical uniform deviations bound for the empirical process, ensuring that if \(n 1\), then \(L_{}(f) L(f)\) for all \(f\). The final step is the following simple lemma, which ensures that if \(\) is not too large, then \(L_{}() L_{}(f_{})\):

**Lemma 5**.: _Let \(:[-1,1]\) be a bounded function class and let \(R:\) be an arbitrary, possibly random, regularizer. Let \(f_{}*{argmin}_{f}L_{}(f)\) and \(*{argmin}_{f}L_{}(f)+R(f)\). Then, \(L_{}() L_{}(f_{})+_{f,f^{ }}R(f)-R(f^{})\)._

Lemma 5 is a simple computation proved in Appendix C.5. Letting \(R(f)\) be either \(_{m}^{(j)}(f)\) in Algorithm 2 or \(\|f\|_{m}^{2}\) in Algorithm 3 demonstrates that if \(\) is not too large, then \(\) performs similarly to \(f_{}\). To prove that Algorithms 2 and 3 produce good learning algorithms then, it suffices to combine these three components, observing first that \(L()\) is close to optimal if \(n 1\) and \(\) is not too large, second that \(\|-\|_{} 1\) if \(m 1\) and \(\) is sufficently small, and third that \(|L()-L()|\|-\|_{}\) if \(_{x}\) is \(\)-smooth with respect to \(\) and \(\) is \(\)-Lipschitz in its first argument. Combining these results concludes the proofs of Theorems 2 and 3. A detailed and rigorous argument for both proofs is presented in Appendix C. As a final remark, we note that in the case of Algorithm 2, convexity of \(\) in the first argument is irrelevant to the privacy guarantee despite being necessary for learning. Indeed, for \(\) returned by Line 5 in Algorithm 2 to be proven a good learner, we apply Jensen's and the above argument that ensures that \(_{j}\) is a good learner. Interestingly, on the other hand, convexity in \(\) is irrelevant to the learning guarantee of Algorithm 3 while it is essential to the privacy guarantee. Further understanding the role that such structural assumptions play in allowing privacy is an interesting direction for future work.