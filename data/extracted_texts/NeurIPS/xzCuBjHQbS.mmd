# Random Function Descent

Felix Benning

University of Mannheim

felix.benning@uni-mannheim.de &Leif Doring

University of Mannheim

leif.doering@uni-mannheim.de

###### Abstract

Classical worst-case optimization theory neither explains the success of optimization in machine learning, nor does it help with step size selection. In this paper we demonstrate the viability and advantages of replacing the classical 'convex function' framework with a 'random function' framework. With complexity \((n^{3}d^{3})\), where \(n\) is the number of steps and \(d\) the number of dimensions, Bayesian optimization with gradients has not been viable in large dimension so far. By bridging the gap between Bayesian optimization (i.e. random function optimization theory) and classical optimization we establish viability. Specifically, we use a'stochastic Taylor approximation' to rediscover gradient descent, which is scalable in high dimension due to \((nd)\) complexity. This rediscovery yields a specific step size schedule we call Random Function Descent (RFD). The advantage of this random function framework is that RFD is scale invariant and that it provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup.

## 1 Introduction

Cost function minimization is one of the most fundamental mathematical problems in machine learning. Gradient-based methods, popular for this task, require a step size, typically chosen using established heuristics. This article aims to deepen the theoretical understanding of these heuristics and proposes a new algorithm based on this insight.

Classical optimization theory uses \(L\)-smoothness, which limits the rate of change of the gradient by \(L\), to provide some convergence guarantees for learning rates smaller than \(1/L\) [e.g. 38]. As this theory is based on an upper bound (the worst case), the learning rate \(1/L\) is naturally much more conservative than necessary on average. Even if \(L\) was known, this learning rate would therefore be impractical. Since line search algorithms typically require access to full cost function evaluations, the field of machine learning (ML) therefore relies heavily on step size heuristics [e.g. 48, 49, 42, 20]. To investigate these heuristics, we introduce new ideas based on a 'random function' perspective.

While automatic step size selection in the convex function framework is possible , convexity is generally only satisfied asymptotically and locally. So the understanding of the initial stages of optimization, which includes the warmup heuristic , greatly benefits from a framework which also admits non-convex functions. This objective is achieved by the 'random function' framework we investigate.

Many successful algorithms in computer science are significantly slower in the worst case than in the average case based on a probabilistic framework (e.g. Quicksort  or the simplex algorithm [e.g. 6]). On random quadratic functions the average case behavior of first order optimizers is already being investigated by the ML community [e.g. 58, 43, 33, 12, 9, 40, 41]. Interested in the landscape of high dimensional random functions as a model for'spin glasses', the physics community independently started studying the average case of optimization as well [e.g. 4, 15, 37, 51, 24], albeit not geared for ML algorithms.

Average case analysis fundamentally requires a prior distribution over possible cost functions. The evaluations seen so far then result in a posterior over the cost of other parameter inputs. Using this posterior for optimization is called "Bayesian optimization" (BO) (e.g. 32, 47, 16, 2), which is best known in the context of low dimensional optimization (e.g. hyperparameter tuning) in the ML community. BO is treated like a zero order method for low dimensional problems due to the \((n^{3})\) complexity for the covariance matrix inversion of the \(n\) evaluations seen so far, which increases to \((n^{3}d^{3})\) when gradient information is included (e.g. 35, 55), where \(d\) is the input dimension of our cost function. This limits classic BO to relatively small dimensions even under sparsity considerations (e.g. 45, 39).

While the BO algorithms developed in the 'random function framework' might not have been viable in high dimension so far, due to their computational complexity, this framework is already used to explain the high relative frequency of saddle points in high dimension (10) and to explain the highly predictable progress optimizers make on high dimensional cost functions (5).

In this work we bridge the gap between BO and (computationally viable) gradient based methods, derived from the first Taylor approximation, with the introduction of a stochastic Taylor approximation based on a forgetful BO posterior. The optimization method "Random Function Descent" (RFD), resulting from the minimization of this stochastic Taylor approximation, coincides with a specific form of gradient descent which establishes its viability in high dimension. The advantages of its BO heritage are scale invariance and an explicit step size schedule, which illuminates the inner workings of step size heuristics such as gradient clipping (42) and gradual learning rate warmup (20).

Our contributions and outlineThe main goal of this paper is to demonstrate the **viability** and **advantages** of replacing the classical "convex function" framework with a "random function" framework. Theorem 4.2 is the main theoretical result establishing **viability** (computatability and scalable complexity) for a given covariance model. Section 6 is concerned with practical estimation of the covariance model and viability is demonstrated with a practical example in the MNIST case study (Section 7). The **advantages** of this approach are scale invariance (Advantage 2.3) and an explicit step size schedule, which does not require expensive tuning and explains existing ML heuristics such as warmup (cf. Section 5.2). This explanation of the initial stage of optimization could never be delivered by the convex framework, because the convexity assumption is not fulfilled initially so it can at best explain asymptotic behavior.

**Sec. 2**: We motivate a stochastic Taylor approximation and RFD and prove its scale-invariance.
**Sec. 3**: We briefly motivate and discuss the common distributional assumptions in BO.
**Sec. 4**: We establish the connection between RFD and gradient descent.
**Sec. 5**: We investigate the step size schedule suggested by RFD. In particular we

1. calculate explicit formulas for the step size schedules resulting from common covariance models (Table 1, Sec. C),
2. analyze the general asymptotic behavior (Sec. 5.1),
3. discuss how RFD explains gradient clipping and learning rate warmup (Sec. 5.2),
**Sec. 6**: We develop a non-parametric variance estimation method, which is robust with respect to the choice of covariance kernel. Finally, we present an extension of RFD to mini-batch losses.
**Sec. 7**: We conduct a case study on the MNIST dataset.
**Sec. 8**: We discuss extensions (see also Sec. E) and limitations.

## 2 The random function descent algorithm

The classic derivation of gradient descent (e.g. 38, p. 29), adds an \(L\)-smoothness based trust bound to the first Taylor approximation, \(T[J() J(w), J(w)]\), of the cost function \(J\) around \(w\) resulting in the gradient step

\[w- J(w)=*{argmin}_{}T[J() J(w), J(w)]+\|-w\|^{2}.\]

Our unusual notation for the Taylor approximation \(T[J() J(w), J(w)]\) is meant to highlight the connection to the stochastic Taylor approximation we define below.

**Definition 2.1** (Stochastic Taylor approximation).: We define the first order stochastic Taylor approximation of a random (cost) function1\(\) around \(w\) by the conditional expectation

\[[()(w),(w)].\]

This is the best \(L^{2}\) approximation [30, Cor. 8.17] of \(()\) provided first order knowledge of \(\) at \(w\).

We call this the'stochastic Taylor approximation' because this approximation only makes use of derivatives in a single point. While the standard Taylor approximation is a polynomial approximation, the'stochastic Taylor approximation' is the best approximation in an \(L^{2}\) sense and already mean-reverting by itself, i.e. it naturally incorporates covariance-based trust (cf. Figure 1). While \(L\)-smoothness-based trust _guarantees_ that the gradient still points in the direction we are going (for learning rates smaller \(1/L\)), covariance based trust tells us whether the derivative is still negative _on average_. Minimizing the stochastic Taylor approximation is therefore optimized for the average case. Since convergence proofs for gradient descent typically rely on an improvement _guarantee_, proving convergence is significantly harder in the average case and we answer this question only partially in Corollary 5.3.

**Definition 2.2** (Random Function Descent - RFD).: Select \(w_{n+1}\) as the minimizer2 of the first order stochastic Taylor approximation

\[w_{n+1}:=*{argmin}_{w}[(w)(w_{n }),(w_{n})].\]

Properties of RFDBefore we make RFD more explicit in Section 4, we discuss some properties which are easier to see in the abstract form.

First, observe that RFD is greedy and forgetful in the same way gradient descent is greedy and forgetful when derived as the minimizer of the regularized first Taylor approximation, or the Newton method as the minimizer of the second Taylor approximation. This is because the Taylor approximation only uses derivatives from the last point \(w_{n}\) (forgetful), and we minimize this approximation (greedy). Since momentum methods retain some information about past gradients, they are not as forgetful. We therefore expect a similar improvement could be made for RFD in the future.

Second, it is well known that classical gradient descent with exogenous step sizes (and most other first order methods) lack the scale invariance property of the Newton method [e.g. 21, 13]. Scale invariance means that scaling the input parameters \(w\) or the cost itself (e.g. by switching from the mean squared error to the sum squared error) does not change the points selected by the optimization method.

**Advantage 2.3** (Scale invariance).: _RFD is invariant to additive shifts and positive scaling of the cost \(\). RFD is also invariant with respect to transformations of the parameter input of \(\) by differentiable bijections whose Jacobian is invertible everywhere (e.g. invertible linear maps)._

While invariance to bijections of inputs is much stronger than the affine invariance offered by the Newton method, non-linear bijections will typically break the 'isotropy' assumption of the following

Figure 1: The stochastic Taylor approximation naturally contains a trust bound in contrast to the classical one. Here \(\) is a Gaussian random function (with covariance as in Equation (11), with length scale \(s=2\) and variance \(^{2}=1\)). The ribbon represents two conditional standard deviations around the conditional expectation.

section which makes RFD explicit. This invariance should therefore be viewed as an opportunity to look for the bijection of inputs which ensures isotropy (e.g. a whitening transformation). The discussion of geometric anisotropy in Section E.1 is conducive to build an understanding of this.

## 3 A distribution over cost functions

It is impossible to make average case analysis explicit without a distribution over functions, so we use the canonical distributional assumption of Bayesian optimization (e.g. [16; 55; 44]), 'isotropic Gaussian random functions'. This assumption was also used in the high dimensional setting by Dauphin et al.  to argue that saddle points are much more common than minima in high dimension, which is often cited to explain why second order methods are uncommon in machine learning.

To motivate isotropy, we note that in average case analysis the uniform distribution is popular, since it weighs all problem instances equally (e.g. all possible permutations in sorting). Isotropy is such a uniformity assumption, which essentially requires "\((=J)=(=J)\)", for all isometries \(\). In other words, the probability that our cost function is equal to \(J\) is equal to the probability that it is equal to a shifted and turned version of \(J\), given by \(J\).

Since the probability of any single realization of a cost function \(J\) is zero, the equation we put in quotes is mathematically unsound. The formal definition follows below.

**Definition 3.1** (Isotropy).: A random function \(\) is called isotropic if its distribution stays the same under isometric transformations of its input, i.e. for any isometry \(\) we have

\[_{}=_{}.\]

If \(\) is Gaussian, isotropy is well known (e.g. 44; 1) to be equivalent to the condition that there exists \(\) and a function \(C:\) such that for all \(w,^{d}\) the expectation and covariance are

\[[(w)]=,((w),( ))=C(\|^{2}}{2}).\]

For these isotropic Gaussian random functions we use the notation \((,C)\).

We discuss generalizations to isotropy in Section F and E.1, but for ease of exposition we retain the (stationary) isotropy assumption throughout the main body. Note that the Gaussian assumption can be statistically tested in practice (cf. Figure 4), but it is also straightforward to reproduce our results with the "best linear unbiased estimator" (BLUE) (Section E.3) in place of the conditional expectation to remove the Gaussian assumption. We finally want to highlight that, in contrast to the uniformity assumption on finite sets, 'isotropic Gaussian random functions' leave us with a family of plausible distributions. It is therefore necessary to estimate \(\) and \(C\), which is the topic of Section 6.

## 4 Relation to gradient descent

While we were able to define RFD abstractly without any assumptions on the distribution \(_{}\) of the random cost \(\), an explicit calculation requires distributional assumptions and we have motivated isotropic Gaussian random functions in Section 3 for this purpose. The assumption of isotropy allows for an explicit version of the stochastic Taylor approximation which then immediately leads to an explicit version of RFD.

**Lemma 4.1** (Explicit first order stochastic Taylor approximation).: _For \((,C)\), the first order stochastic Taylor approximation is given by_

\[[(w-)(w),(w)]= +\|^{2}}{2})}{C(0)}((w)-)-\|^{2}}{2}}{C^{}(0)} ,(w).\]

The explicit version of RFD follows by fixing the step size \(=\|\|\) and optimizing over the direction first.

**Theorem 4.2** (Explicit RFD).: _Let \((,C)\), then RFD coincides with gradient descent_

\[w_{n+1}=w_{n}-_{n}^{*}(w_{n})}{\| (w_{n})\|},\]_where the RFD step sizes are given by_

\[_{n}^{*}:=*{argmin}_{}}{2}}{C(0)}((w_{n})-)-}{2}}{C^{}(0)}\|(w_{n})\|.\] (1)

While the descent direction is a universal property for all isotropic Gaussian random functions, it follows from (1) that the step sizes depend much more on the specific covariance structure. In particular it depends on the decay rate of the covariance acting as the trust bound.

_Remark 4.3_ (Scalable complexity).: While Bayesian optimization typically has computational complexity \((n^{3}d^{3})\) in number of steps \(n\) and dimensions \(d\)[55; 45], RFD under the isotropy assumption has the same computational complexity as gradient descent (i.e. \((nd)\)).

_Remark 4.4_ (Step until the given information is no longer informative).: While \(L\)-smoothness-based trust prescribes step sizes that _guarantee_ the slope to point downwards over the entire step, RFD prescribes steps which are exactly large enough that the gradient is no longer correlated to the previously observed evaluation. This is because the first order condition demands

\[0}{{=}}[(w) (w_{n}),(w_{n})]=[(w) (w_{n}),(w_{n})].\]

And for measurable functions \(:^{d+1}\) such that \(=((w_{n}),(w_{n}))\) is sufficiently integrable, \(\) is then uncorrelated from \(_{i}(w)\) by the first order condition

\[(_{i}(w),)= {[_{i}(w)(w_{n}),( w_{n})]}_{=0}(-[])=0.\]

## 5 The RFD step size schedule

While classical theory leads to 'learning rates', RFD suggests'step sizes' applied to normalized gradients representing the actual length of the step size. In the following we thus make the distinction

\[w_{n+1}=w_{n}-}_{}(w_{n})=w_{n}- }_{}(w_{n})}{\| (w_{n})\|}.\]

To get a better feel for the step sizes suggested by RFD, it is enlightening to divide (1) by \(-(w_{n})\) which results in a minimization problem

\[^{*}:=^{*}():=*{argmin}_{}q_{}()  q_{}():=-}{2} }{C(0)}-}{2}}{C^{ }(0)},\] (2)

which is only parametrized by the "gradient cost quotient"

\[_{n}=(w_{n})\|}{-(w_{n})},\]

i.e. \(_{n}^{*}=^{*}(_{n})\). This minimization problem can be solved explicitly for the most common [44; ch. 4] differentiable isotropic covariance models, see Table 1, Figure 2 and Appendix C for details.

    & \) for \((w)\)} &  \\   & & General case \(\)with \(=(w)\|}{-(w)}\) & \((w)=\) & \( 0\) \\   & \(\) & & & \\   &  & ^{2}+s^{2}\|(w)\|^{2}+ (w)}{2}\|(w)\|\)} &  & \)} \\  & & & & \\  Rational &  & _{}-1+}{s }+(1+)^{2}+}{s}^{3}\)} & }\)} & \)} \\   quadratic & & & & \\   

Table 1: RFD step size (cf. Figure 2 and Eq. (11), (13), (14) for the formal definitions of the models). In particular, \(s\) is the length scale in all covariance models.

Figure 2 can be interpreted as follows: At the start of optimization, the cost should be roughly equal to the average cost \((w)\), so the gradient cost quotient \(\) is infinite and the step sizes are therefore given by \(^{*}()\) (also listed in its own column in Table 1). As we start minimizing, the difference \(-(w)\) becomes positive. Towards the end of minimization this difference no longer changes as the cost no longer decreases. I.e. towards the end the gradient cost quotient \(\) is roughly linear in the gradient \(\|(w)\|\). The derivative \(^{*}(0)\) of \(^{*}()\) at zero then effectively results in a constant asymptotic learning rate.

### Asymptotic learning rate

To explain the claim above, note that the gradient cost quotient \(\) converges to zero towards the end of optimization, because the gradient norm converges to zero. A first order Taylor expansion of \(^{*}\) would therefore imply

\[^{*}()^{*}(0)+^{*}(0)= ^{*}(0)}_{}\|(w)\|\]

assuming \(^{*}(0)=0\) and differentiability of \(^{*}\), which is a reasonable educated guess based on the the examples in Figure 2. But since the RFD step sizes \(^{*}\) are abstractly defined as an \(*{argmin}\), it is necessary to formalize this intuition for general covariance models. First, we define asymptotic step sizes as an object towards which we can prove convergence. Then we prove convergence, proving they are well defined. In addition, we obtain a more explicit formula for the asymptotic learning rate.

**Definition 5.1** (A-Fd).: We define the step sizes of "asymptotic RFD" (A-RFD) to be the minimizer of the second order Taylor approximation \(T_{2}q_{}\) of \(q_{}\) around zero

\[():=*{argmin}_{}T_{2}q_{}()= {C(0)}{-C^{*}(0)}=(0)((w)- )}}_{}\|(w)\|.\]

In the following we prove that these are truly asymptotically equal to the step sizes \(^{*}\) of RFD.

**Proposition 5.2** (A-RFD is well defined).: _Let \((,C)\) and assume there exists \(_{0}>0\) such that the correlation for larger distances \(_{0}\) are bounded smaller than \(1\), i.e. \(/2)}{C(0)}<(0,1)\). Then the step sizes of RFD are asymptotically equal to the step sizes of A-RFD, i.e._

\[()^{*}() 0.\]

Note that the assumption is essentially always satisfied, since the Cauchy-Schwarz inequality implies

\[C\|^{2}}{2}=((w), ())((w)) {Var}(())}=C(0),\]

where equality requires the random variables to be almost surely equal . If the random function is not periodic or constant, this will generally be strict. In the proof, this requirement is only needed to ensure that \(^{*}\) is not very large. The smallest local minimum of \(q_{}\) is always close to \(\) even without this assumption (which ensures it is a global minimum).

Figure 2 illustrates that \(^{*} 0\) should imply \( 0\), resulting in a weak convergence guarantee.

**Corollary 5.3**.: _Assume \(^{*} 0\) implies \( 0\), the cost \(\) is bounded, has continuous gradients and RFD converges to some point \(w_{}\). Then \(w_{}\) is a critical point and the RFD step sizes \(^{*}\) are asymptotically equal to \(\)._

For the squared exponential covariance model we formally prove that \(^{*}\) is strictly monotonously increasing in \(\) and thus \(^{*} 0\) implies \( 0\) (Prop. C.3). The 'bounded' and 'continuous gradients' assumptions are almost surely satisfied for all sufficiently smooth covariance functions [cf. 1], where three times differentiable is more than enough smoothness.

### RFD step sizes explain common step size heuristics

Asymptotically, RFD suggests constant learning rates, similar to the classical \(L\)-smooth setting. We thus define these asymptotic learning rates (as the limit of the learning rates \(h_{n}\) of iteration \(n\)) to be

\[h_{}:=(0)((w_{})-)},\] (3)

where \((w_{})\) is the cost we reach in the limit. If we used these asymptotic learning rates from the start, step sizes would become too large for large gradients, as RFD step sizes exhibit a plateau (cf. Figure 2). To emulate the behavior of RFD with a piecewise linear function, we could introduce a cutoff whenever our step size exceeds the initial step size \(^{*}()\), i.e.

\[w_{n+1}=w_{n}-h_{},()}{\|(w_{n})\|}}(w_{n}).\] (gradient clipping)

At this point we have rediscovered 'gradient clipping' . Since the rational quadratic covariance has the same asymptotic learning rate \(h_{}\) for every \(\), its parameter \(\) controls the step size bound \(^{*}()\) of gradient clipping (cf. Table 1, Figure 2).

Pascanu et al.  motivated gradient clipping with the geometric interpretation of movement towards a 'wall' placed behind the minimum. This suggests that clipping should happen towards the end of training. This stands in contrast to a more recent step size heuristic, "(linear) warmup" , which suggests smaller learning rates at the start (i.e. \(h_{0}=()}{\|(w_{0})\|}\)) and gradual ramp-up to the asymptotic learning rate \(h_{}\). In other words, gradients are not clipped due to some wall next to the minimum, but because the step sizes would be too large at the start otherwise. Goyal et al.  further observe that 'constant warmup' (i.e. a step change of learning rates akin to gradient clipping) performs worse than gradual warmup. Since RFD step sizes suggest this gradual increase, we argue that they may have discovered RFD step sizes empirically (also cf. Figure 3).

## 6 Mini-batch loss and covariance estimation

Since we do not have access to evaluations of the cost \(\) in practice, we need to prove some results about stochastic losses \(_{i}\) before we can apply RFD in practice. For this, assume that we have independent identically distributed (iid) data \(X_{i}\) independent of the true relationship \(\) drawn from \(_{}\) resulting in labels \(Y_{i}=(X_{i})+_{i}\), where we have added independent iid noise \(_{i}\), resulting in loss and cost

\[_{i}(w):=w,(X_{i},Y_{i})(w):=[_{i}(w)].\]

In this setting we confirm (cf. Lemma D.9), that the stochastic approximation errors

\[_{i}(w):=_{i}(w)-(w)\]

are independent conditional on the true relationship \(\). In particular they (and all their derivatives) are uncorrelated and also uncorrelated from \(\). It follows that mini-batch losses

\[_{b}(w):=_{i=1}^{b}_{i}(w)=(w)+ {1}{b}_{i=1}^{b}_{i}(w)\] (4)

have variance

\[(_{b}(w))=((w))+ (_{1}(w))}{=}C(0 )+C_{}(0),\] (5)

where we assume \((,C)\) and \(_{i}(0,C_{})\) in the last equation for simplicity. But this step did not yet require the distributional Gaussian assumption beyond the mean and variance.

### Variance estimation

Recall that the asymptotic learning rate \(h_{}\) in Equation (3) only depends on \(C(0)\) and \(C^{}(0)\). So if we estimate these values, we are certain to get the right RFD step sizes asymptotically without knowing the entire covariance kernel \(C\).

Equation (5) reveals that for \(Z_{b}:=(_{b}(w)-)^{2}\) we have

\[[Z_{b}]=_{0}+_{1} Z_{b}= _{0}+_{1}+\]

with bias \(_{0}=C(0)\) and slope \(_{1}=C_{}(0)\). So a linear regression on samples \((},Z_{b_{k}})_{k n}\) allows for the estimation of \(_{0}\) and \(_{1}\). Using the Gaussian assumption from (5), the variance of \(Z_{b}\) is the (centered) fourth moment of \(_{b}\), which is given by

\[_{b}^{2}:=(Z_{b})=[Z_{b}^{4}]-[Z_ {b}^{2}]^{2}=2(_{b}(w))^{2}=2(_{0}+ {b}_{1})^{2}.\]

In particular the variance of \(Z_{b}\) depends on the batch size \(b\). The linear regression is therefore heteroskedastic. Weighted least squares (WLS) [e.g. 28, Theorem 4.2] is designed to handle this case, but for its application the variance of \(Z_{b}\) is needed. Since \(_{0},_{1}\) are the parameters we wish to estimate, we find ourselves in the paradoxical situation that we need \(\) to obtain \(\). Our solution to this problem is to start with a guess of \(_{0},_{1}\), apply WLS to obtain a better estimate and repeat this bootstrapping procedure until convergence. Since all \(Z_{b}\) have the same underlying cost \(\), we sample the parameters \(w\) randomly to reduce their covariance (details in Sec. B).

The same procedure can be applied to obtain \(C^{}(0)\), where the counterpart of Equation (5) is given by

\[(_{i}_{b}(w))=( _{i}(w))+(_{i} _{1}(w))}}{{=}}-(C^{}(0 )+C^{}_{}(0)).\]

_Remark 6.1_.: Under the isotropy assumption the partial derivatives are iid, so the expectation of \(\|_{b}(w)\|^{2}=_{i=1}^{d}(_{i}_{b}(w ))^{2}\) is this variance scaled by \(d\). In particular the variance needs to scale with \(\) to keep the gradient norms (and thus the Lipschitz constant of \(\)) stable. This observation is closely related to "isoperimetry" [e.g. 7], for details see . Removing the isotropy assumption and estimating the variance component-wise is most likely how "adaptive" step sizes [e.g. 14, 29], like the ones used by Adam, work (cf. Sec. E.1).

Batch size distributionBefore we can apply linear regression to the samples \((},Z_{b_{k}})_{k n}\), it is necessary to choose the batch sizes \(b_{k}\). As this choice is left to us, we calculate the variance of our estimator \(_{0}\) of \(_{0}\) explicitly (Lemma B.2), in order to minimize this variance subject to a sample budget \(\) over the selection of batch sizes

\[_{n,b_{1},,b_{n}}(_{0}) _{k=1\\ }^{n}b_{k}.\] (6)

Since this optimization problem is very difficult to solve, we rephrase it in terms of the empirical distribution of batch sizes \(_{n}=_{i=1}^{n}_{b_{i}}\). Optimizing over distributions is still difficult, but we explain in Section B.1 how to heuristically arrive at the parametrization

\[(b)_{1}^{2}}-_{2}b , b\]

where the parameters \(_{1},_{2} 0\) can then be used to optimize (6). Due to our usage of \(_{b}^{2}\) this has to be bootstrapped.

Covariance estimationWhile the variance estimates above ensure correct asymptotic learning rates, we motivated in Section 5.2 that asymptotic learning rates alone would result in too large step sizes at the beginning. We therefore use the estimates of \(C(0)\) and \(C^{}(0)\) to fit a covariance model, effectively acting as a gradient clipper while retaining the asymptotic guarantees. Note that covariance models with less than two parameters are generally fully determined by these values.

### Stochastic RFD (S-RFD)

It is reasonable to ask whether there is a'stochastic gradient descent'-like counterpart to the 'gradient descent'-like RFD. The answer is yes, and we already have all the required machinery.

**Extension 6.2** (S-RFD).: _For loss \((,C)\) and stochastic errors \(_{i}}}{{}}(0,C_{ })\) we have_

\[*{argmin}_{}[(w-) _{b}(w),_{b}(w)]=^{*}()_{b}(w)}{\|_{b}(w)\|}\]_with the same step size function \(^{*}\) as for RFD, but modified \(\)_

\[=(0)}{C^{}(0)+C^{}_{}(0)} C_{}(0)}{C(0)}_{b}(w)\|} {-_{b}(w)}.\]

Note, that our non-parametric covariance estimation already provides us with estimates of \(C_{}(0)\) and \(C^{}_{}(0)\), so no further adaptions are needed. The resulting asymptotic learning rate is given by

\[h_{}=C_{}(0)}{(C^{}(0)+C ^{}_{}(0))(_{b}(w_{})-)}.\] (7)

## 7 MNIST case study

For our case study we use the negative log likelihood loss to train a neural network  on the MNIST dataset . We choose this model as one of the simplest state-of-the-art models at the time of selection, consisting only of convolutional layers with ReLU activation interspersed by batch normalization layers and a single dense layers at the end with softmax activation. Assuming isotropy, we estimate \(\), \(C(0)\) and \(C^{}(0)\) as described in Section 6.1 and deduce the parameters \(^{2}\) and \(s\) of the respective covariance model (more details in Section B). We then use the step sizes listed in Table 1 for the'squared exponential' and 'rational quadratic' covariance in our RFD algorithm.

In Figure 3, RFD is benchmarked against step size tuned Adam  and stochastic gradient descent (SGD). Even with early stopping, their tuning would typically require more than 1 epoch worth of samples, _in contrast to RFD_ (Section A.1.1). We highlight that A-RFD performs significantly worse than either of the RFD versions which effectively implement some form of learning rate warmup. This is despite the RFD learning rates converging to the asymptotic one within one epoch (ca. \(30\) out of \(60\) steps per epoch). The step sizes on the other hand are (up to noise) monotonously decreasing. This stands in contrast to the "wall next to the minimum" motivation of gradient clipping.

**Code availability:** Our implementation of RFD can be found at https://github.com/FelixBenning/pyrfd and the package can also be installed from PyPI via 'pip install pyrfd'.

## 8 Limitations and extensions

To cover the vast amount of ground that lays between the 'formulation of a general average case optimization problem' and the 'prototype of a working optimizer with theoretical backing',

1. we used the common _isotropic_ and _Gaussian_ distributional assumption for \(\),
2. we used very _simple covariance models_ for the actual implementation,
3. we used WLS in our variance estimation procedure despite the _violation of independence_.

Since RFD is defined as the minimizer of an average instead of an upper bound - making it more risk affine - it naturally loses the improvement guarantee driving classical convergence proofs. It is therefore impossible to extend classical optimization proofs and new mathematical theory must be developed. This risk-affinity can also be observed in its comparatively large step sizes (cf. Fig. 3 and Sec. A). On CIFAR-100 , the step sizes were _too_ large and it is an open question whether assumptions were violated or whether RFD is simply too risk-affine. But since the variance of random functions vanishes asymptotic with high dimension  we highly suspect the former (cf. Remark E.5).

Future work will therefore have to target these assumptions. Some of the assumptions were already simplifications for the sake of exposition, and we deferred their relaxation to the appendix. The Gaussian assumption can be relaxed with a generalization to the 'BLUE' (Sec. E.3), isotropy can be generalized to 'geometric anisotropies' (Sec. E.1) and the risk-affinity of RFD can be reduced with confidence intervals (Sec. E.2). Since simple random linear models already violate stationary isotropy (Sec. F.1), we believe that stationarity is the most important assumption to attack in future work.

## 9 Conclusion

In this paper we have demonstrated the **viability** (computability and scalable complexity) and **advantages** (scale invariance, explainable step size schedule which does not require expensive tuning) of replacing the classical "convex function" framework with the "random function" framework. Along the way we bridged the gap between Bayesian optimization (not scalable so far) and classical optimization methods (scalable). This theoretical framework not only sheds light on existing step size heuristics, but can also be used to develop future heuristics.

We envision the following improvements to RFD in the future:

1. The _reliability_ of RFD can be improved by generalizing the distributional assumptions to cover more real world scenarios. In particular we are interested in the generalization to non-stationary isotropy because we suspect that regularization such as weight and batch normalization  are used to patch violations of stationarity (cf. Section F).
2. The _performance_ of RFD can also be improved. Since RFD is forgetful while momentum methods retains some information it is likely fruitful to relax the full forgetfulness. Furthermore, we suspect that adaptive learning rates [e.g. 14, 29], such as those used by Adam, can be incorporated with geometric anisotropies (cf. Sec. E.1). Performance could also be further improved by estimating the covariance (locally) online instead of globally at the start. Finally, the implementation itself can be made more performant.