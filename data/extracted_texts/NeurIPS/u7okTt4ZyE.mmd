# Taming Diffusion Prior for Image Super-Resolution

with Domain Shift SDEs

 Qinpeng Cui\({}^{*}\)\({}^{1}\)\({}^{1}\), Yixuan Liu\({}^{1}\), Xinyi Zhang\({}^{2}\), Qiqi Bao\({}^{2}\), Qingmin Liao\({}^{2}\)

Li Wang\({}^{1}\), Tian Lu\({}^{1}\), Zicheng liu\({}^{1}\), Zhongdao Wang\({}^{1}\)\({}^{2}\), Emad Barsoum\({}^{1}\)

\({}^{1}\)Advanced Micro Devices Inc.  \({}^{2}\)Tsinghua University

{qinpeng.cui, yixuan.liu, li.wang, lu.tian, zicheng.liu, ebarsoum}@amd.com;

{cqp22, xinyi-zh22, bqq19, liaoqm, wcd17}@tsinghua.edu.cn

Work done during an internship at AMD.Corresponding author.

###### Abstract

Diffusion-based image super-resolution (SR) models have attracted substantial interest due to their powerful image restoration capabilities. However, prevailing diffusion models often struggle to strike an optimal balance between efficiency and performance. Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency. In this paper, we present DoSSR, a **D**omain **S**hift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images. At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models. This integration not only improves the use of diffusion prior but also boosts inference efficiency. Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoSS-SDEs. This advancement leads to the fast and customized solvers that further enhance sampling efficiency. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on synthetic and real-world datasets, while notably requiring _only 5 sampling steps_. Compared to previous diffusion prior based methods, our approach achieves a remarkable speedup of 5-7 times, demonstrating its superior efficiency. Code: https://github.com/AMD-AIG-AIMA/DoSSR

## 1 Introduction

Image super-resolution (SR) is a classical task in computer vision that involves enhancing a low-resolution (LR) image to create a perceptually convincing high-resolution (HR) image . Traditionally, this field has operated under the assumption of simple image degradations, such as bicubic down-sampling, which has led to the development of numerous effective SR models . However, these models often fall short when confronted with real-world degradations, which are typically more complex than those assumed in academic settings. Recently, diffusion models has emerged as a pivotal research direction in real-world SR, using their robust generative capabilities to enhance perceptual quality. This shift highlights their superior performance in practical applications.

Currently, diffusion-based SR strategies can be broadly categorized into two approaches. The first approach leverages large-scale pretrained diffusion models (_e.g._, Stable Diffusion ) as generative prior, using LR images (or preprocessed LR images) as _conditional inputs_ to generate HR images . Despite achieving remarkable results, it exhibits low inference efficiency,as the inference starting point is a random Gaussian noise instead of the LR image. Although techniques such as sampler optimization [33; 38; 30; 9; 22] or model distillation [32; 37] have been proposed to mitigate this issue, they inevitably compromise SR performance. The second approach involves redefining the diffusion process and retraining a model from scratch for the SR task [53; 36]. Consequently, the generative prior from pretrained diffusion models is not leveraged. ResShift , as a typical representative, revises the forward process of DDPM  to better accommodate the SR task. By starting from LR rather than Gaussian noise, it improves inference efficiency. However, its modification of the diffusion pattern, which deviates significantly from existing noise schedules in diffusion models, hinders its integration with large-scale pretrained diffusion models for leveraging their generative prior. The diffusion generative prior has been proven to be highly beneficial for SR tasks , enabling models to transcend the limitations of knowledge learned solely from the training dataset, thereby equipping them to handle various complex real-world scenarios. Thus, crafting a diffusion process tailored for the SR task that also remains compatible with established diffusion prior presents a significant challenge.

To tackle this challenge, we propose DoSSR, a **D**omain **S**hift diffusion-based SR model. We initially view the SR task as a gradual shift from the LR domain to the HR domain, describing this transition with a linear equation, which is called _domain shift equation_. Then, we combine this domain shift equation with existing diffusion equations, facilitating the fine-tuning of large-scale pretrained diffusion models to harness diffusion prior effectively. Moreover, by carefully designing a shifting sequence, inference can begin from LR images rather than Gaussian noises, thereby boosting inference efficiency. To further enhance efficiency, we employ sampler optimization techniques, extensively explored in image generation [38; 30; 9], but not previously tailored for diffusion-based SR tasks. Specifically, we expand the customized diffusion equation from discrete to continuous, enabling its formulation as stochastic differential equations (SDEs). We subsequently present the corresponding backward-time SDE as Domain Shift SDE in the reverse process and provide an exact formulation of its solution. Based on our formulation, we customize fast solvers for sampling. Experimental results demonstrate that our method achieves superior or comparable performance compared to current state-of-the-art methods on both synthetic and real-world datasets, _with only 5 sampling steps_, striking an optimal balance between efficiency and effectiveness. Furthermore, our approach can match the performance of previous methods _even with just a single step_.

In summary, the main contributions of our work are as follows:

* We propose a novel diffusion equation, which models SR from the perspective of domain shift, enabling inference to start from LR images and leveraging diffusion prior to ensure both efficiency and performance.
* We further propose the SDEs to describe the process of domain shift and provide an exact solution for the corresponding reverse-time SDEs. Based on the solution, we design customized fast samplers, resulting in even higher efficiency, thereby achieving the state-of-the-art efficiency-performance trade-off.

Figure 1: (a) Latency, MANIQA, and complexity of model comparison on _RealLR200_ dataset in x4 SR task (for 128×128 LR images). (b) Qualitative comparisons of DoSSR and recent state-of-the-art methods on one typical real-world example. For diffusion-based methods, the suffix ”-N” appended to the method name indicates the number of inference steps. Zoom in for a better view.

Related work

Neural Network-based Super-Resolution.Neural network-based methods have emerged as the dominant approach in image SR tasks. The introduction of convolutional neural networks (CNNs) and Transformer architecture, with the primary focus on network architecture design [12; 10; 25; 26; 58; 21; 56; 57], have demonstrated superior performance over traditional methods. This improvement is facilitated by the introduction of residual blocks, dense blocks and attention mechanisms. These methods primarily aim for better image fidelity measures such as PSNR and SSIM  indices, therefore, they often yield over-smoothed outcomes. To enhance visual perception, Generative adversarial network (GAN)-based SR methods have been developed. By incorporating adversarial loss during training, many SR models [13; 23; 17] can generate perceptually realistic details, thereby enhancing visual quality. To further study SR problems in real-world scenarios, some studies [54; 46; 24] have proposed simulating the intricate real-world image degradation process through random combinations of fundamental degradation operations. Despite the remarkable advancements, GAN-based SR methods can introduce undesirable visual artifacts.

Diffusion-based Super-Resolution.Recently, diffusion-based SR methods [35; 36; 8; 7; 45; 27] have demonstrated excellent performance, especially in terms of perceptual quality. These methods can generate more authentic details while avoiding unpleasant visual artifacts like GAN-based methods. Current diffusion models for super-resolution can be broadly categorized into two main approaches. The first approach involves leveraging large-scale pretrained diffusion models, such as Stable Diffusion , as prior, and then using LR images as conditional inputs to generate HR images. StableSR  and DiffBIR  represent representative works that leverage diffusion prior, leading to enhanced fidelity when conditioning on LR or preprocessed LR. SeeSR  and CoSeR  demonstrate that extracting semantic text information from LR images as additional control conditions for the T2I model helps improve performance. The second approach involves redefining the diffusion process and retraining a model from scratch for SR [18; 36]. To address the slow inference speed issue of diffusion-based SR methods, ResShift  constructs a Markov chain that transitions between HR and LR images by shifting residuals between them, enabling accelerated sampling. SinSR  proposed a method of distilling ResShift to achieve comparable performance in a single step. Despite the remarkable advancements achieved by ResShift and SinSR, they necessitate retraining from scratch for SR tasks (or further distillation) and are unable to leverage diffusion prior. Therefore, improving the inference efficiency while leveraging the potential of large-scale pretrained diffusion models to assist SR requires thorough investigation, which is the goal of this work.

## 3 Methodology

We aim to optimize the balance between efficiency and performance in diffusion-based super-resolution (SR) models. Our approach is grounded in two key principles: First, initiating inference from LR images rather than noise; Second, effectively harnessing pretrained diffusion prior. In Section 3.1, we introduce a novel diffusion equation designed to fulfill both criteria simultaneously. Subsequently, in Section 3.2, we extend this diffusion process to continuous scenarios, formulating it through Stochastic Differential Equations (SDEs). Building on these SDEs, we develop an efficient solver detailed in Section 3.3, further enhancing inference efficiency.

### Diffusion Process with Domain Shift

Our goal is to characterize the shift from the source domain to the target domain as a diffusion process. In the task of SR, the distribution of LR images \(p_{}(}_{0})\) represents the source domain, while the distribution of HR images \(p_{}(_{0})\) represents the target domain. Firstly, we conceptualize domain shift as a gradual transition from the source domain to the target domain through a linear drift coefficient \(_{t}\), the domain shift equation is formulated as

\[(}_{0},_{0})=_{t}}_{0}+(1-_{t}) _{0},\ 0_{t} 1,\ t=1,2,,T,\] (1)

where shifting sequence \(\{_{t}\}_{t=1}^{T}\) monotonically non-decreases with timestep \(t\). In order to enable linear combination, we can interpolate \(}_{0}\) to match the same dimensions as \(_{0}\) if necessary. Secondly, we combine this domain shift with the diffusion equation. To integrate with pretrained diffusion models, we adopt the most commonly used diffusion scheme from DDPM  and express the formula of marginal distribution at any timestep \(t\) as follows:

\[q(_{t}|_{0},}_{0})=(_{t};_{t} (}_{0},_{0}),_{t}^{2}),\;t=1,2,,T,\] (2)

where \(_{t},_{t} 0\) and \(_{t}^{2}+_{t}^{2}=1\), \(\) is the identity matrix. Based on our proposed marginal distribution Eq. (2), we demonstrate the transition distribution as follows:

\[q(_{t}|_{t-1},}_{0})=(_{t};}{_{t-1}}_{t-1}+_{t}(_{t}-_{t-1})_{ 0},1-^{2}}{_{t-1}^{2}}),\;t=1,2,,T,\] (3)

where \(}=}_{0}-_{0}\) is the residual between the source and target domain.

Relation to DDPM .The formulation of Eq. (2) is based on the DDPM  forward process, with a crucial difference lying in its mean \(_{t}(}_{0},_{0})\) instead of \(_{t}_{0}\). This integration encapsulates the domain shift within the variation of its mean, while the diffusion process with added noise maintains consistency with it, thereby smoothing this transformation. Meanwhile, it enhances the diffusion model to learn the pathway from the source domain to the target domain. For an intuitive understanding, we plot and compare the score function \(_{} q_{t}(_{t})\) learned by a vanilla Stable Diffusion (SD) model and DoSSR in Fig. 2(c). While SD learns reasonable score field in the whole space, DoSSR inherits its capability in the ambient space and further learns more accurate scores along the path between LR and HR domains. Therefore sampling efficiency is improved. Furthermore, the choice of \(_{t}\) and \(_{t}\), referred to as the _noise schedule_, follows the existing pretrained diffusion model, allowing us to fine-tune it rather than training it from scratch.

Relation to ResShift .The form of equation Eq. (3) suggests that this shift essentially constructs a Markov chain in a manner similar to that described in ResShift . However, the equation constructed by ResShift adopts an entirely different noise schedule compared to the pretrained diffusion model. This makes it difficult to apply pretrained diffusion models for subsequent fine-tuning, necessitating training from scratch instead. Therefore, it is unable to utilize the diffusion prior, thereby limiting the model's performance. See Appendix A.7 for detailed theoretical differences from ResShift. In Appendix C.1, we present experimental results on ImageNet  showing that DoSSR uses two orders of magnitude less training data than ResShift while achieving superior performance.

Shifting Sequence.The parameter \(_{t}\) plays a crucial role in guiding the diffusion process, serving as a bridge between the source and target domains. Specifically, \(_{t}=1\) represents standard diffusion

Figure 2: Illustration of the proposed diffusion process with domain shift. (a) In the forward process, we merge the gradual shift from HR to LR domain with standard diffusion process. (b) In the reverse process, we initiate inference from LR domain (\(t=t_{1}\)) and use our fast sampler to generate SR images. (c) Comparison of the estimated score between SD and DoSSR. DoSSR inherits the capability of SD in ambient space and enhances learning a pathway from LR to HR domain. (d) The design of the shifting sequence which enables us to initiate inference from \(t_{1}\).

forward perturbations in the source domain, whereas \(_{t}=0\) corresponds to the target domain. The transition between these domains occurs for \(0<_{t}<1\), indicating a domain shift. To effectively utilize the diffusion prior, we adopt the noise schedule from DDPM. This adoption dictates that as \(t\) approaches the final time step \(T\), the scale parameter \(_{T}\) tends towards zero, and the distribution \(q(_{T})\) approximates a standard Gaussian, \((,)\). To retain prior information from the source domain while shortening the diffusion path, we set \(_{t}=1\) for \(t[t_{1},T]\), as defined by:

\[_{t}=})}{2}\ t[0,t_{1}], _{t}=1\ t[t_{1},T].\] (4)

The advantage of such a setting lies in the fact that during the reverse process, the values of \(_{t}\) for \(t[t_{1},T]\) are known and can be obtained through the forward process Eq. (2). Consequently, the inference does not need to start from time step \(T\), but can commence at \(t_{1}\), thereby preserving the prior information of the source domain while enhancing the efficiency of inference. An overview of the impact of \(_{t}\) is presented in Fig. 2.

### Diffusion DoS-SDEs

To improve the efficiency of inference in diffusion models, many prior works [30; 31; 9] have designed efficient samplers by solving the diffusion SDEs. Therefore, in this section, we extend the aforementioned discrete shift process to an SDE for description, in preparation for designing efficient samplers in the following section. Specifically, inspired by the work of , we generalize this finite shift process further to an infinite number of noise scales, such that the data distribution of domain shift evolves according to an SDE as noise intensifies. Then we provide the corresponding reverse-time SDE and elucidate the training of diffusion models from the perspective of score matching . Next, we will elaborate extensively on how to describe diffusion models using SDEs.

Forward Process.Expanding the time variable \(t\) in Eq. (2) to a continuous range, \(t[0,T]\), we have that \(_{t},_{t},_{t}\) are differentiable functions of \(t\) with bounded derivatives. Furthermore, Song _et al._ have demonstrated that the diffusion process can be modeled as the solution to an Ito SDE and we formulate the SDE as follows:

\[d_{t}=[f(t)_{t}+h(t)}_{0}]dt+g(t)d_{t},\ _{0} q_{0}(_{0}),\] (5)

where \(_{t}\) is the standard Wiener process, and \(q_{0}(_{0})\) is the target domain data distribution. It has the same marginal distribution \(q(_{t}|_{0},}_{0})\) as in Eq. (2) for any \(t[0,T]\) with the coefficients satisfying (proof in Appendix A.2)

\[f(t)=(1-_{t})}{dt},\ \ h(t)=}{1-_{t}} }{dt},\ \ g(t)=^{2}}{dt}-2(1-_{t})}{dt}_{t}^{2}}.\] (6)

Reverse Process.The reverse of a diffusion process is also a diffusion process  which can similarly be described by a reverse-time SDE (proof in Appendix A.3):

\[d_{t}=f(t)_{t}+h(t)}_{0}-g^{2}(t)_{ } q_{t}(_{t})dt+g(t)d}_{t}\] (7)

where \(}_{t}\) is also a standard Wiener process when time flows backwards. In this paper, we refer to this SDE as _Domain Shift_ SDE (DoS-SDE).

Score Matching.The only unknown term in Eq. (7) is the _score function_\(_{} q_{t}(_{t})\) that can be estimated by training a score-based model on samples with score matching . In practice, we use a neural network \(_{}(_{t},}_{0},t)\) conditioned on \(}_{0}\), parameterized by \(\), to estimate the scaled score function (alternatively referred to as noise), following [16; 40]. The parameter \(\) is optimized by minimizing the following objectives:

\[^{*}&=*{arg\, min}_{}_{t}w(t)_{q_{t}(_{t})}|| _{}(_{t},}_{0},t)+_{t}_{ } q_{t}(_{t})||}\\ &=*{arg\,min}_{}_{t}w(t) _{q_{0}(_{0})}_{q_{t}()}||_{}(_{t},}_{0},t)-|| },\] (8)

where \(w(t)\) is a weighting function, \(_{t}=_{t}(_{t}}_{0}+(1-_{t})_{0})+_ {t}\), and \((,)\).

Thus, we have completed the expression of the diffusion model using SDEs. Sampling from diffusion models can alternatively be seen as solving the corresponding diffusion DoS-SDEs.

### Solvers for Diffusion DoS-SDEs

In this section, we present an exact formulation of the solution of diffusion DoS-SDEs and design efficient samplers for fast sampling. To facilitate the solution of equation Eq. (7), we utilize the data prediction model \(_{}(_{t},}_{0},t)\), which directly estimates the original target data \(_{0}\) from the noisy samples. The relationship between score function and data prediction model is as follows (proof in Appendix A.4):

\[_{} q_{t}(_{t})=-_{t}-(_{t}(1-_{t}) _{}(_{t},}_{0},t)+_{t}_{t}}_ {0})}{_{t}^{2}}.\] (9)

In practice, we employ our trained noise prediction model \(_{}(_{t},}_{0},t)\) for data prediction \(_{}(_{t},}_{0},t)\) as described in Appendix A.4. By substituting Eq. (6) and Eq. (9) into Eq. (7) and introducing the substitutions \(_{t}=}{_{t}(1-_{t})}\) and \(_{t}=_{t}}{_{t}(1-_{t})}\) along with the notation \(d_{_{t}}:=}{dt}}d}_{t}\), \(_{}:=_{t()}\), \(_{}:=_{_{t}}\), we rewrite Eq. (7) w.r.t \(\) as

\[d_{}=_{}d+)^{2}}d_{}-}{1-_{}} d}_{0}-_{ }(_{},}_{0},)d+d {w}_{}\] (10)

We propose the exact solution for Eq. (10) using the _variation-of-constants_ formula, following [31; 9].

**Proposition 3.1** (Exact solution of diffusion DoS-SDEs).: _Given an initial value \(_{s}\) at time \(s>0\), the solution \(_{t}\) for the diffusion DoS-SDEs defined in Eq. (7) at time \(t[0,s]\) is as follows:_

\[_{t}&=(1-_{t })}{_{s}(1-_{s})}^{2}}{_{s}^{2}}_{s}+ _{t}(1-_{t})(}{1-_{t}}-}{1-_{s}} ^{2}}{_{s}^{2}})}_{0}\\ &-_{t}(1-_{t})_{_{s}}^{_{t}}^{2}}{^{3}}_{}(_{},}_{0 },)d+_{t}(1-_{t})^{2}-^{4}}{_{s}^{2}}}_{s},\] (11)

_where \(_{t}=}{_{t}(1-_{t})}\) and \(_{s}(,)\)._

The detailed derivation of this proposition is provided in Appendix A.5. Notably, the nonlinear term in Eq. (11) involves the integration of a non-analytical neural network \(_{}(_{},}_{0},)\), which can be challenging to compute. For practical applicability, we employ Ito-Taylor expansion to approximate the integral of \(_{}\) from \(_{s}\) to \(_{t}\) to compute \(}_{t}\), thereby approximating \(_{t}\). Additionally, we approximate the derivatives of \(_{}\) using the _forward differential method_. These approximations allow us to derive SDE solvers of any order for diffusion DoS-SDEs. For the sake of brevity, we employ a first-order solver for demonstration. In this case, Eq. (11) becomes

\[}_{t}&=(1- _{t})}{_{s}(1-_{s})}^{2}}{_{s}^{2}} _{s}+(1-_{t})(}{1-_{t}}-}{1-_{s}}^{2}}{_{s}^{2}})}_{0}}_{ }\\ &+_{t}(1-_{t})(1-^{2}}{_{s}^{2}}) _{}(_{s},}_{0},s)+_{t}(1-_{t})^{2}-^{4}}{_{s}^{2}}}_{s}.\] (12)

The detailed derivation, as well as high-order solvers, are provided in Appendix A.6, and detailed algorithms are proposed in Appendix B. Typically, higher-order solvers converge even faster because of more accurate estimation of the the nonlinear integral term. The solvers provided for sampling allow us to iteratively generate HR images using a trained diffusion model. It is worth noting that Eq. (12) comprises four terms, including the additional linear term \(}_{0}\), as compared to the ancestral sampling algorithm . We refer to this additional term as the _domain shift guidance_ (DoSG) which leverages prior information from the source domain and enhances the efficiency of inference.

## 4 Experiments

### Experimental setup

For training, we train our DoSSR using a variety of datasets including DIV2K , DIV8K , Flickr2K , and OST . To synthesize LR and HR training pairs, we adopt the degradation

[MISSING_PAGE_FAIL:7]

Notably, DoSSR consistently achieves the highest scores in CLIPIQA, MANIQA, and TOPIQ, with the exception of being second in TOPIQ on DIV2K, and attains the second highest score in MUSIQ across all four datasets. At the same time, we also note that diffusion-based methods generally achieve poorer performance in reference metrics compared to GAN-based methods due to their ability to generate more realistic details at the expense of fidelity. Additionally, our DoSSR manages to achieve improved no-reference metric performance compared to the data presented in Table 1 as NFE increases slightly, a detail further elaborated on in Section 4.3.

Qualitative Comparison.Figs. 1(b), 3 present visual comparisons on real-world images. By leveraging learning of domain shift and introducing DoSG, our DoSSR efficiently generates high-quality texture details consistent with contents of the LR image. In the example of Fig. 1(b), GAN-based methods fail to faithfully reconstruct the grid texture of clothing, leading to notable degradation. StableSR and ResShift produce specific erroneous textures. Both SeeSR and ours successfully restore correct textures, while our results display clearer textures. Similarly, in the first example of Fig. 3, our DoSSR generates a more perceptually convincing Spider-Man face as well as textures, while in the second example, it produces more realistic and high-quality details of ground-laid bricks compared to other methods. More visual examples are provided in Fig. 7.

Efficiency Comparison.The comparative analysis of model parameters and latency for competing SR models is shown in Fig. 1(a) and Table 1. The latency is calculated on the \(\)4 SR task for 128\(\)128 LR images with V100 GPU. StableSR, DiffBIR, SeeSR, and our DoSSR utilize the pretrained SD model, resulting in a similar parameter count, with SeeSR incorporating a prompt extractor to enhance SR results, making it the largest among these methods. ResShift, utilizing the network structure from LDM , is trained from scratch and has significantly fewer parameters. It employs a 15-step process to achieve faster inference speeds. Among the pretrained SD-based methods, DoSSR demonstrates superior performance efficiency, requiring only 5 function evaluations to achieve speeds 5-7 times faster than previous SD-based models such as SeeSR. Additionally, DoSSR not only demonstrates faster or comparable latency to ResShift but also achieves significantly better super-resolution performance.

Figure 3: Qualitative comparisons of different steps of our DoSSR and other diffusion-based SR methods. The “-N” suffix denotes inference steps. Please zoom in for a better view.

### Ablation Study

Effectiveness of DoSG.To verify the effectiveness of the DoSG introduced in the diffusion equation, we conduct an experiment using identical network architectures but with two different diffusion equations: the original diffusion equation as described by Ho _et al._ and our newly formulated equation (Eq. (2)). To isolate the impact of DoSG from our shifting sequence design, we set \(t_{1}=T\), ensuring that the starting point of our inference in both scenarios approximates Gaussian noise. Quantitative comparisons can be found in the first two rows of Table 2. It is evident that the introduction of DoSG leads to a significant improvement across all metrics in the table, highlighting the effectiveness of DoSG in enhancing the performance of diffusion-based SR models. Additionally, it is worth noting that the original diffusion equation can be considered a special case within our framework where \(_{t}=0\) and \(t_{1}=T\). Therefore, our sampler can accommodate the original diffusion equation, and for a fair comparison, we employ the same sampler for both models. More comprehensive comparison is provided in Appendix Table 6, where it can be seen that our DoSSR demonstrates superior performance compared to the corresponding order solver with DDPM, benefiting from the inclusion of DoSG in our DoS SDE-Solver.

The selection of \(t_{1}\).The starting point \(t_{1}\) serves as a pivotal parameter in DoSSR. We explore several options on the value of \(t_{1}\) and show the corresponding final SR performance in Table 2. It can be observed that SR performance improves as \(t_{1}\) gradually decreases from \(T\) to \(T/2\). However, further decreasing \(t_{1}\) from \(T/2\) to \(3/T\) conversely compromises SR performance. Intuitively, a larger \(t_{1}\) means less LR prior is preserved due to a larger magnitude of added noises, and the model behaves more like the vanilla pretrained model by hallucinating plausible HR contents; In contrast, a smaller \(t_{1}\) means less noises, so the prediction is prone to be more consistent with the LR image, but without HR details. Hence, we set \(t_{1}=T/2\) by default for a good trade-off.

The number of step.We assess the impact of different inference steps on DoSSR by analyzing changes in representative metrics for both reference-based and non-reference-based evaluations, as shown in Fig. 4(a). As the number of inference steps increases, reference-based metrics tend to decline, suggesting a loss in fidelity, while non-reference metrics improve, indicating enhanced realism and detail in the generated images. We also conduct visual comparisons in Fig. 4(b). Our DoSSR achieves performance comparable to SeeSR in just 5 steps and produces more realistic details in 7 steps. Remarkably, DoSSR is capable of delivering satisfactory results _even with just a

   Method & CLIPIQ\(\) & MUSIQ\(\) & MANIQA\(\) & TOPIQ\(\) \\   & 0.5379 & 54.09 & 0.3932 & 0.5180 \\   & \(T\) & 0.5776 & 55.69 & 0.4181 & 0.5427 \\  & \(2T/3\) & 0.6337 & 59.30 & 0.4589 & 0.5987 \\  & \(T/T/2\) & **0.6776** & **64.40** & **0.5214** & **0.6618** \\  & \(T/3\) & 0.6490 & 61.76 & 0.4895 & 0.6260 \\   

Table 2: Comparison across various selections of starting point \(t_{1}\), evaluated on the _DRealSR_ dataset. The baseline method is DDPM, which employs the original diffusion equation. In all setups, inference is carried out over 5 steps.

Figure 4: (a) Quality metrics vs. steps on _RealSR_ Dataset. (b) Qualitative comparisons of different steps of our DoSSR with other methods. The suffix “-N” appended to the method name indicates the number of inference steps. Please zoom in for a better view.

_single step_, achieving 0.5115 MANIQA score and 0.6258 CLIPIQA score on the _RealSR_ dataset, significantly boosting the efficiency of diffusion-based methods. More visual examples are provided in Fig. 9, where it can be observed that increasing the number of steps yields more realistic details.

The order of our sampler.We provide a suite of solvers for sampling in our DoSSR model, including a first-order solver presented in Eq. (11), and more advanced second- and third-order solvers detailed in Appendix A.6. We investigate the impact of samplers with different orders on our experimental results through qualitative and quantitative comparisons, as illustrated in Table 3 and Fig. 10. From Table 3, it becomes evident that high-order samplers can achieve superior non-reference metrics under the same limited inference step conditions. This is because the acceleration of higher-order samplers allows diffusion models to generate more details, as demonstrated in the first example of Fig. 10, where the tower generated by the high-order sampler exhibits richer textures. More comprehensive comparison is provided in Appendix Table 6. In our implementation, we use third-order sampler by default.

## 5 Conclusion

In this paper, we present DoSSR, a diffusion-based super-resolution framework that significantly enhances both efficiency and performance by integrating a domain shift strategy with pretrained diffusion models. This approach not only enhances generative capacity but also enhances further inference efficiency through our novel proposed DoSS-SDEs formulation and customized solvers. Empirical validation on diverse SR benchmarks confirms that DoSSR achieves a 5-7 times speed improvement over existing methods, setting a new state-of-the-art. Our work paves the way for more efficient diffusion-based solutions in image super-resolution.

Limitation.Despite the strong overall performance demonstrated by the proposed DoSSR, it occasionally generates visually unfriendly details when employing an unfavorable random seed, a challenge also encountered by other diffusion-based methods. Typically, we fix the random seed for all image super-resolution tasks to stabilize the results, but this particular seed may not be suitable for certain specific images. As depicted in Fig. 8, different initializations of random seeds result in significant variations in the details of the lion's eyes. Some of the initialized random seeds produce eyes that are reasonable and acceptable, while others exhibit noticeable inconsistencies with LR. For bad cases, we can also obtain a satisfactory result by adjusting the random seed multiple times. However, this often requires numerous attempts, and the quality of the results heavily relies on luck. This inspires us to find a suitable initialization for each specific LR image, which can enhance the performance of the model. Hence, for diffusion-based methods, exploring how to obtain a reasonable random seed based on known LR images may be a future research direction.

Societal impact.Our advancements in the diffusion-based image super-resolution model, DoSSR, present both positive and negative societal impacts. On the positive side, it enhances medical imaging, potentially leading to more accurate diagnoses and reducing the need for invasive procedures. In surveillance, it aids in better identification and tracking, improving public safety. Moreover, in remote sensing and environmental monitoring, it facilitates informed decision-making for disaster management and environmental conservation. However, there are concerns regarding privacy and surveillance. Enhanced resolution capabilities could infringe upon privacy rights and lead to increased surveillance in public spaces, raising questions about civil liberties. Additionally, in digital media, while high-resolution imagery enhances visual content, it may perpetuate unrealistic beauty standards and digital manipulation, impacting self-esteem. In summary, while DoSSR brings promising advancements, it's crucial to address concerns around privacy, security, and digital ethics to ensure responsible and ethical deployment of the technology.

   _Order_ & CLIPIQA\(\) & MUSIQ\(\) & MANIQA\(\) & TOPIQ\(\) \\ 
1 & 0.5907 & 59.12 & 0.4686 & 0.5907 \\
2 & 0.6749 & 64.09 & 0.5196 & 0.6571 \\
3 & **0.6776** & **64.40** & **0.5214** & **0.6618** \\   

Table 3: Comparison of performance of different sampler orders on the _DRealSR_ dataset. In all setups, inference is carried out over 5 steps.