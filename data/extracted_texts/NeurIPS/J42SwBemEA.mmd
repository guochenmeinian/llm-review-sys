# State Chrono Representation for Enhancing Generalization in Reinforcement Learning

Jianda Chen\({}^{1}\)  Wen Zheng Terence Ng\({}^{1,2}\)  Zichen Chen\({}^{3}\)

**Sinno Jialin Pan\({}^{4}\)  Tianwei Zhang\({}^{1}\)**

\({}^{1}\)Nanyang Technological University \({}^{2}\)Continental Automotive Singapore

\({}^{3}\)University of California, Santa Barbara \({}^{4}\)The Chinese University of Hong Kong

{jianda001, ngwe0099}@ntu.edu.sg zichen_chen@ucsb.edu

sinnopan@cuhk.edu.hk tianwei.zhang@ntu.edu.sg

###### Abstract

In reinforcement learning with image-based inputs, it is crucial to establish a robust and generalizable state representation. Recent advancements in metric learning, such as deep bisimulation metric approaches, have shown promising results in learning structured low-dimensional representation space from pixel observations, where the distance between states is measured based on task-relevant features. However, these approaches face challenges in demanding generalization tasks and scenarios with non-informative rewards. This is because they fail to capture sufficient long-term information in the learned representations. To address these challenges, we propose a novel State Chrono Representation (SCR) approach. SCR augments state metric-based representations by incorporating extensive temporal information into the update step of bisimulation metric learning. It learns state distances within a temporal framework that considers both future dynamics and cumulative rewards over current and long-term future states. Our learning strategy effectively incorporates future behavioral information into the representation space without introducing a significant number of additional parameters for modeling dynamics. Extensive experiments conducted in DeepMind Control and Meta-World environments demonstrate that SCR achieves better performance comparing to other recent metric-based methods in demanding generalization tasks. The codes of SCR are available in https://github.com/jianda-chen/SCR.

## 1 Introduction

In deep reinforcement learning (Deep RL), one of the key challenges is to derive an optimal policy from highly dimensional environmental observations, particularly images [5; 12; 33]. The stream of images received by an RL agent contains temporal relationships and significant spatial redundancy. This redundancy, along with potentially distracting visual inputs, poses difficulties for the agent in learning optimal policies. Numerous studies have highlighted the importance of building state representations that can effectively distinguish task-relevant information from task-irrelevant surroundings [45; 46; 8]. Such representations have the potential to facilitate the RL process and improve the generalizability of learned policies. As a result, representation learning has emerged as a fundamental aspect in the advancement of Deep RL algorithms, gaining increased attention within the RL community .

**Related Works.** The main objective of representation learning in RL is to develop a mapping function that transforms high-dimensional observations into low-dimensional embeddings. This process helps reduce the influence of redundant signals and simplify the policy learning process. Previous research has utilized autoencoder-like reconstruction losses [43; 18], which have shownimpressive results in various visual RL tasks. However, due to the reconstruction of all visual input including noise, these approaches often lead to overfitting on irrelevant environmental signals. Data augmentation methods  have demonstrated promise in tasks involving noisy observations. These methods primarily enhance perception models without directly impacting the policy within the Markov Decision Process (MDP) framework. Other approaches involve learning auxiliary tasks , where the objective is to predict additional tasks related to the environment using the learned representation as input. However, these auxiliary tasks are often designed independently of the primary RL objective, which can potentially limit their effectiveness in improving the overall RL performance.

In recent advancements, behavioral metrics, such as the bisimulation metric  and the MICo distance , have been developed to measure the dissimilarity between two states by considering differences in immediate reward signals and the divergence of next-state distributions. Behavioral metrics-based methods establish approximate metrics within the representation space, preserving the behavioral similarities among states. This means that state representations are constrained within a structured metric space, wherein each state is positioned or clustered relative to others based on their behavioral distances. Moreover, behavioral metrics have been proven to set an upper bound on state-value discrepancies between corresponding states. By learning behavioral metrics within representations, these methods selectively retain task-relevant features essential for achieving the optimal policy, which involves maximizing the value function and shaping agent behaviors. At the same time, they filter out noise that is unrelated to state values and behavioral metrics, thereby improving robustness in noisy environments.

However, behavioral metrics face challenges when handling demanding generalizable RL tasks and scenarios with sparse rewards . While they can capture long-term behavioral metrics to some extent through the temporal-difference update mechanism, their reliance on one-step transition data limits their learning efficiency, especially in the case of sparse rewards. This limitation may result in representations that encode non-informative signals  and can restrict their effectiveness in capturing long-term reward information. Some model-based approaches attempt to mitigate these issues by learning transition models . However, learning a large transition model with long trajectories requires a large number of parameters and significant computational resources. Alternatively, \(N\)-step reinforcement learning methods can be used to address the problem . Nevertheless, these methods introduce higher variance in value estimation compared to one-step methods, which may lead to unstable training.

**Contributions.** To overcome the above challenges, we introduce the State Chrono Representation (SCR) learning framework. This metric-based approach enables the learning of long-term behavioral representations and accumulated rewards that span from present to future states. Within the SCR framework, we propose the training of two distinct state encoders. One encoder focuses on crafting a state representation for individual states, while the other specializes in generating a _Chronological Embedding_ that captures the relationship between a state and its future states. In addition to learning the conventional behavioral metric for state representations, we introduce a novel behavioral metric specifically designed for temporal state pairs. This new metric is approximated within the chronological embedding space. To efficiently approximate this behavioral metric in a lower-dimensional vector space, we propose an alternative distance metric that differs from the typical \(L_{p}\) norm. To incorporate long-term rewards information into these representations, we introduce a "measurement" that quantifies the sum of rewards between the current and future states. Instead of directly regressing this measurement, we impose two constraints to restrict its range and value. Note that SCR is a robust representation learning methodology that can be integrated into any existing RL algorithm.

In summary, our contributions are threefold: 1) We introduce a new framework, SCR, for representation learning with a focus on behavioral metrics involving temporal state pairs. We also provide a practical method for approximating these metrics; 2) We develop a novel measurement specifically adapted for temporal state pairs and propose learning algorithms that incorporate this measurement while enforcing two constraints; 3) Through experiments conducted on the Distracting DeepMind Control Suite , we demonstrate that our proposed representation exhibits enhanced generalization and efficiency in challenging generalization tasks.

## 2 Preliminary

**Markov Decision Process:** A Markov Decision Process (MDP) is a mathematical framework defined by the tuple \(=(,,P,r,)\). Here \(\) represents the state space, which encompasses all possible states. \(\) denotes the action space, comprising all possible actions that an agent can take in each state. The state transition probability function, \(P\), defines the probability of transitioning from the current state \(s_{t}\) to any subsequent state \(s_{t+1}\) at time \(t\). Given the action \(a_{t}\) taken, the probability is denoted as \(P(s_{t+1}|s_{t},a_{t})\). The reward function, \(r:\), assigns an immediate reward \(r(s_{t},a_{t})\) to the agent for taking action \(a_{t}\) in state \(s_{t}\). The discount factor, \(\), determines the present value of future rewards.

A policy, \(:S A\), is a strategy that specifies the probability \((a|s)\) of taking action \(a\) in state \(s\). The objective in an MDP is to find the optimal policy \(^{*}\) that maximizes the expected discounted cumulative reward, \(^{*}=_{}_{a}[_{t}^{t}r(s_{t},a_{t})]\).

**Behavioral Metric:** In DBC , the bisimulation metric defines a pseudometric \(d:\) to measure the distance between two states. Additionally, a variant of bisimulation metric, known as \(\)-bisimulation metric, is defined with respect to a specific policy \(\).

**Theorem 2.1** (\(\)-bisimulation metric).: _The \(\)-bisimulation metric update operator \(_{bisim}:\) is defined as_

\[_{bisim}d(,):=|r_{}^{}-r_{}^{}|+(d)(P_{}^{},P_{}^{}),\]

_where \(\) is the space of \(d\), \(r_{}^{}=_{a}(a|)r_{}^{a}\), \(P_{}^{}=_{a}(a|)P_{}^{a}\), \(\) is the Wasserstein distance, and \(r_{}^{a}\) is \(r(,a)\) for short. \(_{bisim}\) has a unique least fixed point \(d_{bisim}^{}\)._

MICo  defines an alternative metric that samples the next states instead of directly measuring the intractable Wasserstein distance.

**Theorem 2.2** (MICo distance).: _The MICo distance update operator \(_{MICo}:\) is defined as_

\[_{MICo}d(,):=|r_{}^{}-r_{}^{}|+_{^{} P_{}^{}, ^{} P_{}^{}}d(^{}, ^{}),\]

\(_{MICo}\) _has a fixed point \(d_{MICo}^{}\)._

## 3 State Chrono Representation

Although the bisimulation metric  and MICo  have their strengths, they do not adequately capture future information. This limitation can degrade the effectiveness of state representations in policy learning. To address this issue and incorporate future details, we propose a novel approach, **State Chrono Representation** (SCR). The detailed architecture of SCR is illustrated in Figure 1.

SCR consists of two key components: a **state representation**\(()^{n}\) for a given state \(\), and a **chronological embedding**\((_{i},_{j})^{n}\) that captures the relationship between a current state \(_{i}\) and its future state \(_{j}\) within the same trajectory. The state representation, \(()\), is developed using a behavioral metric \(d\) that quantifies the reward and dynamic divergence between two states. On the other hand, the chronological embedding, \((_{i},_{j})\), fuses these two state representations using deep learning, with a focus on capturing the long-term behavioral correlation between the current state \(_{i}\) and the future state \(_{j}\). To compute the distance between any two chronological embeddings, we propose a "chronological" behavioral metric, which is further improved through a Bellman operator-like MSE loss . Moreover, \(()\) incorporates a novel **temporal measurement**\(m\) to evaluate the transition from the current state to a future state, effectively capturing sequential reward data from the optimal behavior. Due to the complexity of the regression target, this temporal measurement operates within the defined lower and upper constraints, providing a stable prediction target for both the measurement and state representation during the learning process.

Figure 1: Overall architecture of SCR.

### Metric Learning for State Representation

The state representation encoder \(\) is trained by approximating a behavioral metric, such as the MICo distance. In our model, we utilize a MICo-based metric transformation operator. Instead of using sampling-based prediction in MICo, we employ latent dynamics-based modeling to assess the divergence between two subsequent state distributions. This approach draws inspiration from the methodology in SimSR . The metric update operator for latent dynamics, denoted by \(\), is defined as follows.

**Theorem 3.1**.: _Let \(:^{n}^{n}\) be a metric in the latent state representation space, \(d_{}(_{i},_{i^{}}):=((_{i} ),(_{i^{}}))\) be a metric in the state domain. The metric update operator \(\) is defined as,_

\[d_{}(_{i},_{i^{}})=|r_{ _{i}}-r_{_{i^{}}}|+_{ {c}(_{i+1})(|(_{i}),a_{ _{i}})\\ (_{i^{}+1})(|(_{i^{}}),a_{_{i^{}}})}\ ((_{i+1}),(_{i^{}+1})),\] (1)

_where \(a_{_{i}}\) and \(a_{_{i^{}}}\) are the actions in states \(_{i}\) and \(_{i^{}}\), respectively, and \(\) is the learned latent dynamics model. \(}\) has a fixed point \(d_{}^{}\)._

Proof.: Refer to Appendix A.3.1 for the detailed proof. 

To learn the approximation for \(d_{}^{}\) in the embedding space, one needs to specify the form of distance \(\) for latent vectors. Castro et al.  showed that a behavioral metric with a sample-based next state distribution divergence is a diffuse metric, as the divergence of the next state distribution corresponds to the Lukaszyk-Karmowski distance  that measures the expected distance between two samples drawn from two distributions, respectively (detailed definition is in Appendix A.4.1).

**Definition 3.2** (Diffuse metric ).: A function \(d:\) based on a vector space \(\) is a diffuse metric if the following axioms hold: 1) \(d(,) 0\) for any \(,\); 2) \(d(,)=d(,)\) for any \(,\); 3) \(d(,)+d(,) d(,)\) for any \(,,\).

MICo provides an approximation of the behavioral metric using an angular distance: \(_{MICo}(,)=\|_{2}^{2}+\|\|_{2}^{2}}{2}+(,)\), where \(,^{n}\), \((,)\) represents the angle between vectors \(\) and \(\), and \(=0.1\) is a hyperparameter. This distance calculation includes a non-zero self-distance, which makes it compatible with expressing the Lukaszyk-Karmowski distance . However, the angle function \((,)\) only considers the angle between \(\) and \(\), which requires computations involving cosine similarity and the \(\) function. This can sometimes lead to numerical discrepancies. DBC  recommends using the \(L_{1}\) norm with zero self-distance, which is only suitable for the Wasserstein distance. On the other hand, SimSR  utilizes the cosine distance, derived from cosine similarity, but it does not satisfy the triangle inequality and does not have a non-zero self-distance.

To address the challenges mentioned above, we propose a revised distance, \((,)\), in the embedding space. It is characterized as a diffuse metric and formulated as follows:

**Definition 3.3**.: We define \(:^{n}^{n}\) as a distance function, where \((,)=\|_{2}^{2}+\|\|_{2}^ {2}-^{}}\), for any \(,^{n}\).

**Theorem 3.4**.: \(\) _is a diffuse metric._

Proof.: Refer to Appendix A.3.2 for the detailed proof. 

**Lemma 3.5** (Non-zero self-distance).: _The self-distance of \(\) is not strict to zero, i.e., \((,)=\|\|_{2} 0\). It becomes zero if and only if every element in vector \(\) is zero._

Theorem 3.4 validates that \(\) is a diffuse metric satisfying triangle inequality, and Lemma 3.5 shows \(\) has non-zero self-distance capable of approximating dynamic divergence, which is a Lukaszyk-Karmowski distance. Moreover, the structure of \(\) closely resembles the \(L_{2}\) norm, with the only difference being that the factor of \(^{}\) is \(-1\) instead of \(-2\). This construction, which involves only vector inner product and square root computations without divisions or trigonometric functions, helps avoid numerical computational issues and simplify the implementation.

To learn the representation function \(\), a common approach is to minimize MSE loss between the two ends of (1). The loss, which incorporates \(\) and is dependent on \(\), can be expressed as follows:

\[_{}()=_{_{i},_{i^{}},r_{_{i}},r_{_{i^{}}}\\ (_{i+1}),(_{i^{}+1})} |((_{i}),(_{i^{}})).-|r_{ _{i}}-r_{_{i^{}}}|-((_{i+ 1}),(_{i^{}+1}))|^{2},\] (2)

where \(\) represents the replay buffer or the sampled rollouts used in the RL learning process.

#### 3.1.1 Long-term Temporal Information

The loss \(_{}()\) in Eqn. (2) heavily relies on the temporal-difference update mechanism, utilizing only one-step transition information. However, this approach fails to effectively capture and encode long-term information from trajectories. Incorporating temporal information into the representation while maintaining the structure and adhering to behavioral metrics presents a complex challenge. To address this challenge, we propose two distinct methods: Chronological Embedding (Section 3.2) and Temporal Measurement (Section 3.3). Each technique is designed to capture the temporal essence of a rollout denoted as \((_{i};_{j})\), which represents a sequence starting from state \(_{i}\) and reaching a future state \(_{j}\).

As illustrated in Figure 2, the goal of the chronological embedding is to create a novel paired state embedding, denoted as \((_{i},_{j})\), capturing the temporal representation of the rollout \((_{i};_{j})\). This is achieved by transforming a pair of state representations \((_{i})\) and \((_{j})\) to a vector: \((_{i},_{j})\):=\(((_{i}),(_{j}))^{n}\). The objective of learning \(\) is to develop a "chronological" behavioral metric \(d_{}\) that quantifies the distance between rollouts \((_{i};_{j})\) and \((_{i^{}};_{j^{}})\). On the other hand, temporal measurement aims to compute a specific "distance" \(m\) between states \(_{i}\) and \(_{j}\), which provides insights into the cumulative rewards accumulated during the rollout \((_{i};_{j})\).

Note that learning the state representation solely based on either \(d_{}\) or \(m\) poses challenges. While \(d_{}\) improves the representation expressiveness, capturing both rewards and dynamics may be less efficient in the learning process. On the other hand, learning \(m\) is efficient but may not adequately capture the dynamics. Therefore, we propose a unified approach that leverages the strengths of both \(d_{}\) and \(m\) to overcome the challenges and improve the overall state representation. Empirical results in Section 4.3 support the effectiveness of our proposal.

### Chronological Embedding

The **chronological embedding**, denoted by \((_{i},_{j})^{n}\), is designed to capture the relationship between a given state \(_{i}\) and any of its future states \(_{j}\) over the same trajectory. To capture the comprehensive behavioral knowledge, we introduce a distance function \(d_{}:()() \), which is intended to reflect the behavioral metric and enable the encoder \(\) to incorporate behavioral information. Building upon the MICo distance in Theorem 2.2, we specify the metric update operator \(_{Chrono}\) for \(d_{}\) in the following theorem. Proof is detailed in Appendix A.3.3.

**Theorem 3.6**.: _Let \(_{}\) be the space of \(d_{}\). The metric update operator \(_{Chrono}:_{}_{}\) is defined as follows,_

\[_{Chrono}d_{}(_{i},_{j},_{i^{ }},_{j^{}})=|r_{_{i}}-r_{_{i^{} }}|+_{_{i+1} P_{}^{},_{i^{ }+1} P_{}^{}}d_{}(_{i+1},_{j}, _{i^{}+1},_{j^{}}),\] (3)

_where time step \(i<j\) and \(i^{}<j^{}\). \(_{Chrono}\) has a fixed point \(d_{}^{}\)._

Here \(d_{}^{}\) represents the "chronological" behavioral metric. Our objective is to closely approximate \(d_{}^{}\), which measures the distance between two sets of states \((_{i},_{j})\) and \((_{i^{}},_{j^{}})\), taking into account the difference in immediate rewards and dynamics divergence. To co-learn the encoder \(\) with \(d_{}^{}\), we represent \(d_{}^{}\) as \(d_{}^{}(_{i},_{j},_{i^{}},_{j^{}}):=((_{i},_{j}),(_{ i^{}},_{j^{}}))\), where \(\) is defined in Definition 3.3. Similar to Eqn. (1), we construct \(d_{}^{}\) to compute the distance in the embedding space.

\[d_{}^{}(_{i},_{j},_{i^{}},_{j^{}})=|r_{_{i}}-r_{_{i^{}}}|+ _{_{i+1} P_{}^{},_{i^{}+1} P _{}^{}}((_{i+1},_{j}),(_{ i^{}+1},_{j^{}})).\] (4)

Figure 2: An example with two rollouts.

To ensure computational efficiency, the parameters of the encoders \(\) and \(\) are shared. The encoder \(\) extracts outputs from \(\), and the distance measure is appropriately adapted as \(d^{}_{}(_{i},_{j},_{i^{}},_ {j^{}}):=(((_{i}),(_{j})),( (_{i^{}}),(_{j^{}})))\). The objective of learning the chronological embedding is to minimize the MSE loss between both sides of Eqn. (4) w.r.t. \(\) and \(\),

\[_{}(,)=_{ _{i},_{j},_{i^{}},_{j^{} },r_{_{i}},\\ _{_{i^{}}},_{i+1},_{i^{} +1}}|(((_{i}),( _{j})),((_{i^{}}),(_{j^{ }})))-|r_{_{i}}-r_{_{i^{}}}|.\] \[.-(((_{i+1}),(_ {j})),((_{i^{}+1}),(_{j^{}}))) |^{2}.\] (5)

Minimizing Eqn. (5) is to encourage the embeddings of similar state sequences to become closer in the embedded space, thereby strengthening the classification of similar behaviors.

### Temporal Measurement

To enhance the ability of SCR to gain future insights, we introduce the concept of **temporal measurement**, which quantifies the disparities between a current state \(_{i}\) and a future state \(_{j}\). This measurement, denoted by \(m(_{i},_{j})\), aims to measure the differences in state values or the cumulative rewards obtained between the current and future states. We construct an approximate measurement based on the state representation \(\), denoted by \(_{}(_{i},_{j}):=((_{i}), (_{j}))\). Here, \(:^{n}^{n}\) is a non-parametric asymmetric metric function (details are presented at the end of this section). This approach structures the representation space of \(()\) around the "distance" \(m\), ensuring that \(()\) contains sufficient information for future state planning.

We propose using \(m\) to represent the expected discounted accumulated rewards obtained by an optimal policy \(^{*}\) from state \(_{i}\) to state \(_{j}\): \(m(_{i},_{j})=_{^{*}}[_{t=0}^{j-i} ^{t}r_{_{i}}|_{0}=_{i},_{ j-i}=_{j}.]\). However, obtaining the exact value of \(m(_{i},_{j})\) is challenging because the optimal policy \(^{*}\) is unknown and is, in fact, the primary goal of the RL task. Instead of directly approximating \(m(_{i},_{j})\), we learn the approximation \((_{i},_{j})\) in an alternative manner that ensures it falls within a feasible range covering the true \(m(_{i},_{j})\).

To construct this range, we introduce two constraints. The first constraint, serving as a **lower** boundary, states that the expected discounted cumulative reward obtained by any policy \(\), whether optimal or not, cannot exceed \(m\):

\[_{}[_{t=0}^{j-i}^{t}r_{_{t}}| _{0}=_{i},_{j-i}=_{j}] m( _{i},_{j}).\] (6)

This constraint is based on a fact that any sub-optimal policy is inferior to the optimal policy. Based on this constraint, we propose the following objective to learn the approximation \(_{}\):

\[_{low}()=_{(_{i};_{j})} |(_{t=0}^{j-i}^{t}r_{_{t}}-( (_{i}),(_{j})))|^{2},\] (7)

where \((x)=(0,x)\). This objective is non-zero when the constraint in Eqn. (6) is not satisfied. Hence, it increases the value of \(m((_{i}),(_{j}))\) until it exceeds the sampled reward sum.

The second constraint serving as the upper boundary is proposed based on Fig. 2. To ensure that the absolute value \(|m(_{i},_{j})|\) remains within certain limits, we propose the following inequality:

\[|(_{i},_{j})| d(_{i},_{i^{ }})+|(_{i^{}},_{j^{}})|+d( _{j},_{j^{}}),\] (8)

where \(d\) is the behavioral metric introduced in Section 3.1. The right-hand side of the inequality represents the longer path from \(_{i}\) to \(_{j}\). This inequality demonstrates that the absolute temporal measurement \(|m(_{i},_{j})|\) should not exceed the sum of the behavioral metrics at the initial states (\(_{i}\) and \(_{i^{}}\)), i.e., \(d(_{i},_{i^{}})\), the final states pair (\(_{j}\) and \(_{j^{}}\)), i.e. \(d(_{j},_{j^{}})\), and the measurement \(|m(_{i},_{j})|\). This constraint leads to the following objective for training \(_{}\):

\[_{up}()\!=\!|\!(\!|(( _{i}),(_{j}))|\!-\!\!(\!( (_{i}),(_{i^{}}))\!+\!((_ {j}),(_{j^{}}))\!+\!((_{i^{}}), (_{j^{}}))\!)\!)\!)\!|^{2}\!\!\!,\]

where sg means "stop gradient". This objective aims to decrease the value of \(_{}\) when the constraint in Eqn. (8) is not satisfied. By simultaneously optimizing both constraints in a unified manner,we guarantee that the approximated temporal measurement, \(m\), is confined within a specific range, defined by the lower and upper constraints. The overall objective for \(\) is formulated as follows:

\[_{}()=_{low}()+_{up}().\] (9)

**Asymmetric Metric Function for \(\).** The measurement \((_{i},_{j})\), designed to measure the distance based on the rewards, is intended to be **asymmetric** with respect to \(_{i}\) and \(_{j}\). This is because we assume that state \(_{i}\) comes before \(_{j}\), resulting in a distinct relationship compared to the progression from \(_{j}\) to \(_{i}\). Recent research has focused on studying quasimetrics in deep learning [28; 39; 38] and developed various methodologies to compute asymmetric distances. In our method, we choose to utilize Interval Quasimetric Embedding (IQE)  to implement \(\) (details are in Appendix A.5.1).

### Overall Objective

As shown in Figure 1, the encoders are designed to predict state representations \(()\) for individual states and chrono embedding \((_{i},_{j})\) to capture the relationship between states \(_{i}\) and \(_{j}\). The measurement \(\) is then computed based on \((_{i})\) and \((_{j})\) to incorporate the accumulated rewards between these states. The components \(\) and \(\) work together to enhance the state representations \(\) and capture temporal information, thereby improving their predictive capabilities for future insights. The necessity of \(\) and \(\) is demonstrated in the ablation study in Section 4.3. Therefore, a comprehensive objective to minimize is formulated in a unified manner as follows,

\[(,)=_{}()+_{}(,) +_{}().\] (10)

Our method, SCR, can be seamlessly integrated with a wide range of deep RL algorithms, which can effectively utilize the representation \(()\) as a crucial input component. In our implementation, we specifically employ Soft Actor-Critic (SAC)  as our foundation RL algorithm. The state representation serves as the input state for both the policy network and Q-value network in SAC. Other implementation details can be found in Appendix A.5.

## 4 Experiments

### Configurations

**DeepMind Control Suite.** The primary objective of our proposed SCR is to develop a robust and generalizable representation for deep RL when dealing with high-dimensional observations. To evaluate its effectiveness, we conduct experiments using the DeepMind Control Suite (DM_Control) environment, which involves rendered pixel observations  and a distraction setting called Distracting Control Suite . This environment utilizes the MuJoCo engine, offering pixel observations for continuous control tasks. The DM_Control environment provides diverse testing scenarios, including background and camera pose distractions, simulating real-world complexities of camera inputs. (1) _Default setting_ evaluates the effectiveness of each RL approach. Frames are rendered at a resolution of \(84 84\) RGB pixels as shown in Figure 3(a). We stack three frames as states for the RL agents. (2) _Distraction setting_ evaluates both the generalization and effectiveness of each RL approach. The distraction  consists of 3 components, as shown in Figure 3(b): 1) **background video** distraction, where the clean and simple background is replaced with a natural video; 2) **object color** distraction, which involves slight changes in the color of the robots; and 3) **camera pose** distraction, where the camera's position and angle are randomized during rendering from the simulator. We observe that tasks become significantly more challenging when the camera pose distraction is applied.

**Baselines.** We compare our method against several prominent algorithms in the field, including: 1) SAC , a baseline deep RL method for continuous control; 2) DrQ , a data augmentation

Figure 3: Examples of DM_Control with (a) default setting and (b) distraction setting.

method using random crop; 3) DBC , representation learning with the bisimulation metric; 4) MICo , representation learning with MICo distance; and 5) SimSR , representation learning with the behavioral metric approximated by the cosine distance.

### Main Results

**Default Setting.** To verify the sample efficiency of our method, we compare it with others on eight tasks in DM_Control and report the experimental results in Table 1. We trained each method on each task for 500K environment steps. All results are averaged over 10 runs. We observe that SCR achieves comparable results to the augmentation method DrQ and the state-of-the-art behavioral metric approach SimSR. It is worth noting that for a DM_Control task, the maximum achievable scores are 1000, and a policy that collects scores around 900 is considered nearly optimal. These findings highlight the effectiveness of our SCR in mastering standard RL control tasks.

**Distraction Setting.** To further evaluate the generalization ability of our method, we perform comparison experiments on DM_Control with Distraction using the same training configuration as in the default setting. We evaluate 10 different seeds, and conduct 100 episodes per run. The scores from each run are summarized in Table 2. The std is computed across the mean scores of these 10 runs. The results show that our method outperforms the others in all eight tasks, notably in the sparse reward task _cartpole-swing_up_sparse_, where other methods achieve nearly zero scores. The camera-pose distraction poses a significant challenge for metric-based methods like DBC, MICo, and SimSR, as it leads to substantial distortion of the robot's shape and position in the image state. DrQ outperforms other behavioral metric methods, possibly due to its random cropping technique, which facilitates better alignment of the robot's position. The aggregate metrics  are shown in Figure 4, where scores are normalized by dividing by 1000. Figure 5 presents the training curves, while additional curves can be found in Appendix B.3. These results demonstrate the superior performance of our method in handling distractions and highlight our strong generalization capabilities.

### Ablation Study

**Impact of Different Components.** To evaluate the impact of each component in SCR, we perform an ablation study where certain components are selectively removed or substituted. Figure 6 shows the training curves on _cheetah-run_ and _walker-walk_ in the distraction setting. SCR denotes the full

   & BC-Catch & C-SwingUp & C-SwingUpSparse & Ch-Run & F-Spin & H-Stand & R-Easy & W-Walk \\  SAC & 447.5\(\)14.1 & 63.3\(\)10.3 & 30.1\(\)12.7 & 354.4\(\)15.4 & 518.6\(\)29.3 & 79.3\(\)15.6 & 321.1\(\)95.4 & 363.7\(\)154.4 \\ DrQ & **962.7\(\)**19.6 & 640.3\(\)8.2 & **709.0\(\)**9.4 & 44.87\(\)13.9 & 962.0\(\)11.3 & **861.2\(\)**0.9 & **970.2\(\)**12.7 & **972.3\(\)**14.5 \\ DBC & 104.2\(\)33.3 & 281.3\(\)27.6 & 65.9\(\)80.3 & 386.1\(\)16.6 & 730.3\(\)13.6 & 43.5\(\)5.6 & 179.8\(\)27.4 & 330.3\(\)41.4 \\ MICo & 215.4\(\)14.1 & 803.2\(\)12.0 & 0.2\(\)0.0 & 4.9\(\)18.1 & 2.0\(\)0.1 & 800.8\(\)21.1 & 186.1\(\)19.4 & 297.3\(\)3.3 \\ SimSR & 938.8\(\)14.9 & **854.8\(\)**11.5 & 217.9\(\)307.5 & 255.3\(\)327.5 & **962.4\(\)**19.0 & 6.2\(\)1.7 & 76.6\(\)25.1 & **929.9\(\)**21.1 \\ SCR & 944.2\(\)11.7 & **849.4\(\)**2.1 & **768.4\(\)**15.1 & **778.4\(\)**29.7 & **964.9\(\)**25.5 & **851.3\(\)**45.6 & 946.8\(\)26.0 & **919.0\(\)**20.0 \\  

Table 1: Results (mean\(\)std) on DM_Control with the default setting at 500K environment steps.

Figure 4: Aggregate metrics on distract setting.

   & BC-Catch & C-SwingUp & C-SwingUpSparse & Ch-Run & F-Spin & H-Stand & R-Easy & W-Walk \\  SAC & 82.6\(\)20.2 & 218.4\(\)4.9 & 0.7\(\)0.7 & 177.4\(\)8.4 & 22.5\(\)27.7 & 19.6\(\)26.2 & 149.4\(\)77.5 & 167.2\(\)8 \\ DrQ & 124.0\(\)99.6 & 230.0\(\)36.3 & 11.2\(\)6.4 & 103.0\(\)84.9 & 579.8\(\)28.2 & 16.8\(\)11.8 & 70.5\(\)40.1 & 33.6\(\)6.3 \\ DBC & 48.8\(\)24.4 & 127.7\(\)19.2 & 0.0\(\)0.0 & 9.2\(\)1.9 & 7.7\(\)10.7 & 5.6\(\)2.5 & 149.6\(\)24.8 & 30.9\(\)4.8 \\ MICo & 100.1\(\)21.0 & 200.2\(\)6.6 & 0.0\(\)0.1 & 7.4\(\)3.2 & 86.9\(\)76.1 & 118.1\(\)10.5 & 132.3\(\)37.8 & 27.5\(\)7.7 \\ SimSR & 106.4\(\)13.5 & 148.4\(\)17.3 & 0.0\(\)0.0 & 28.2\(\)23.8 & 0.4\(\)0.1 & 6.6\(\)1.2 & 78.4\(\)17 & 28.6\(\)3.1 \\ SCR & **211.3\(\)**55.4 & **565.1\(\)**59.9 & **185.7\(\)**93.0 & **331.7\(\)**1.4 & **738.3\(\)**24.5 & **400.8\(\)**19.2 & **666.1\(\)**14.5 & **555.1\(\)**31.2 \\  

Table 2: Results (mean\(\)std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.

model. SCR w/o \(\) removes the chronological embedding \(\). SCR w/o \(\) refers to the exclusion of the approximation \(\). SCR w/ \(\) only removes losses \(_{}(,)\) and \(_{}()\) but keep \(_{}()\). SCR w/ **cos** replaces the distance function \(\) for computing metrics on the representation space with cosine distance, akin to SimSR does. SCR w/ MICo replaces \(\) with MICo's angular distance. SCR w/ **L1** replaces \(\) with the \(L_{1}\) distance as adopted by DBC. The results demonstrate the superior performance of the full model and the importance of \(\) and \(\). The absence of these components can lead to worse performance and unstable training. The results also demonstrate that the proposed \(\) surpasses existing distance functions.

**Impact of Sampling Steps.** In our previous experiments, we uniformly sample the number of steps between \(i\) and \(j\) in the range \(\). In this experiment, we investigate the impact of this hyper-parameter on SCR. We conduct experiments on the _cheetah-run_ and _walker-walk_ tasks in the distraction setting with various step sampling ranges: \(\), \(\), \(\), and \(\). We also make comparisons with fixed step counts: 50 and 100. The results presented in Figure 7 show that sampling step counts in the range \(\) yields the optimal results and remains stable across tasks. Therefore, we set this hyper-parameter to uniformly sample in the range \(\) for all experiments.

### Experiments on Meta-World

We present experimental investigations conducted in Meta-World , a comprehensive simulated benchmark that includes distinct robotic manipulation tasks. Our focus is on six specific tasks: _basketball-v2_, _coffee-button-v2_, _door-open-v2_, _dranver-open-v2_, _pick-place-v2_, and _window-open-v2_, chosen for their diverse objects in environments. The observations are rendered as 84\(\)84 RGB pixels with a frame stack of 3, following the DM_Control format. Table 3 displays the average success rates over all tasks and five seeds, while Figure 8 shows the training curves for a subset of these tasks. Additional training curves for all tasks can be found in Appendix B.5. Our proposed SCR consistently achieves optimal performance in all tasks, while other baseline methods, except for DrQ, fail to converge to such high levels of performance. Even though DrQ demonstrates proficiency in achieving optimal performance, it shows less sampling efficiency than SCR, highlighting the effectiveness of SCR in the applied setting.

## 5 Conclusion

Our proposed State Chrono Representation learning framework (SCR) effectively captures information from complex, high-dimensional, and noisy inputs in deep RL. SCR improves upon previous metric-based approaches by incorporating a long-term perspective and quantifying state distances across temporal trajectories. Our extensive experiments demonstrate its effectiveness compared with several metric-based baselines in complex environments with distractions, making a promising avenue for future research on generalization in representation learning.

**Limitations.** This work does not address truly Partially Observable MDPs (POMDPs), which are common in real-world applications. In future work, we plan to integrate SCR with POMDP methods to solve real-world problems, or to design unified solutions that better accommodate these complexities.

## 6 Acknowledgement

We thank the anonymous reviewers for the constructive feedback. This study is supported under RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). Sinno J. Pan thanks the support of the Hong Kong Jockey Club Charities Trust to the JC STEM Lab of Integration of Machine Learning and Symbolic Reasoning.