# ResMem: Learn what you can and memorize the rest

Zitong Yang

Stanford University

Stanford, CA 94305

zitong@berkeley.edu &Michal Lukasik

Google Research

New York, NY, 10011

mukasik@google.com &Vaishnavh Nagarajan

Google Research

New York, NY, 10011

vaishnavh@google.com &Zonglin Li

Google Research

New York, NY, 10011

lizonglin@google.com &Ankit Singh Rawat

Google Research

New York, NY, 10011

ankitsrawat@google.com &Manzil Zaheer

Google Research

New York, NY, 10011

manzilzaheer@google.com &Aditya Krishna Menon

Google Research

New York, NY, 10011

adityakmenon@google.com &Sanjiv Kumar

Google Research

New York, NY, 10011

sanjivk@google.com

###### Abstract

The impressive generalization performance of modern neural networks is attributed in part to their ability to _implicitly_ memorize complex training patterns. Inspired by this, we explore a novel mechanism to improve model generalization via _explicit_ memorization. Specifically, we propose the _residual-memorization_ (_ResMem_) algorithm, a new method that augments an existing prediction model (e.g., a neural network) by fitting the model's residuals with a \(k\)-nearest neighbor based regressor. The final prediction is then the sum of the original model and the fitted residual regressor. By construction, ResMem can explicitly memorize the training labels, even when the base model has low capacity. We start by formulating a stylized linear regression problem and rigorously show that ResMem results in a more favorable test risk over a base linear neural network. Then, we empirically show that ResMem consistently improves the test set generalization of the original prediction model across standard vision and natural language processing benchmarks.

## 1 Introduction

Large neural networks achieve remarkable _generalization_ on test samples despite _memorization_ of training samples, in the sense of achieving zero training error . Several recent analyses have established that, under certain settings, memorization is _sufficient_ to achieve generalization , and, more surprisingly, can even be _necessary_. These works suggest that suitable memorization can be a valuable desiderata for learning. While increasing model size is a conceptually simple strategy to enable memorization, this has the obvious downside of significantly increasing the cost of model training and serving. This raises a natural question: _are there alternate mechanisms to improve the memorization (and thus generalization) of a relatively small model?_

In this paper, we propose _residual memorization_ (_ResMem_), a simple yet effective mechanism that achieves this goal (cf. Figure 1). Compared to the _implicit_ memorization performed by large neural models, the key idea behind ResMem is to perform _explicit_ memorization via a separate \(k\)-nearest neighbor component. Specifically, ResMem involves first training a standard neural network \(f_{}\), and then explicitly _memorizing the model's residuals_ with a \(k\)-nearest neighbor based regressor \(r_{}\)Memorization through \(k\)-nearest neighbor can be efficiently computed with various approximation schemes (e.g. ). Subsequently, the ResMem prediction on an instance \(x\) is given by the sum of the two components, i.e., \(f_{}(x)+r_{}(x)\).

We start by formulating a stylized linear regression problem that captures the essence behind ResMem (cf. Section 3). Our analysis (Theorem 3.3) shows that, without ResMem, the test risk of the base linear neural network decreases to an irreducible constant as the number of samples goes to infinity. In contrast, the test risk of ResMem decreases to zero. The insight of theoretical analysis is that ResMem augments the capacity of the parametric linear network by adding a non-parametric component (i.e., nearest-neighbor).

Empirically, we show that such explicit memorization indeed leads to generalization benefits: ResMem consistently improves the test accuracy of a baseline DeepNet on image classification tasks with CIFAR100 , and autoregressive language modeling on C4  (Section 4). Towards understanding this improved performance, we hypothesize that ResMem works by learning in two-stages (cf. Section 4.4). Specifically, we posit that the initial DeepNet \(f_{}\) learns some _coarse_ structure, and ResMem \(r_{}\) supplements the DeepNet prediction with _fine-grained_ details (cf. Figure 3). We verify our hypothesis via qualitative analysis on CIFAR100 and C4 (Section 4.4).

To summarize, our contributions are:

1. We propose _residual-memorization_ (_ResMem_), a two-stage learning algorithm that combines a base prediction model with a nearest neighbor regressor (cf. Figure 1);
2. We theoretically analyze the rate of convergence of ResMem on a stylized linear regression problem, and show that it can improve upon the base prediction model (Section 3).
3. We empirically demonstrate that ResMem improves test performance of neural networks (cf. Section 4), particularly when the training set is extremely large;

### Applicable scenarios of ResMem

From our theoretical and empirical analysis, we posit that ResMem (Figure 1) yields the largest margin of improvement over a base DeepNet when it is infeasible to perform _implicit_ memorization with the latter. We discuss three such scenarios below. Each of our main empirical or theoretical results roughly corresponds to one of these settings.

* **Complex dataset.** In this scenario, the Bayes-optimal decision boundary is very complex, and is beyond the capability of the neural network itself. To demonstrate this, we analyze a theoretical linear regression problem where the target regression function is not contained in the hypothesis class of linear neural networks (cf. Section 3).

Figure 1: Illustration of the _residual memorization_ (ResMem) algorithm. In a nutshell, we first fit a small deep network \(f_{}\) on the training sample (Figure 1(a)). When this network is non-memorizing, it incurs non-zero _residual_ errors in its predictions (Figure 1(b)). We then fit a \(k\)-nearest neighbor based regressor on these residuals (Figure 1(c)). The final prediction is given by the sum of the initial network and \(k\)-NN regressor predictions. In all three figures, the \(x\)-axis represents the features in a supervised learning problem. In Figure 1(a), the \(y\)-axis represents the targets of prediction. In Figure 1(b) and 1(c), the \(y\)-axis represents the residual of the initial fitting from **Step 1**.

* **Large sample size.** Here, the number of training samples is large enough to make training set interpolation (i.e., achieving zero training error) infeasible for a given neural network model. For example, current large language models (LLMs) may be trained for at most a single epoch over trillions of examples . By contrast, ResMem can circumvent this issue by explicitly memorizing the training samples. We emulate this scenario by considering a causal language modeling task on the C4 dataset (cf. Section 4.3).
* **Small model.** In many practical settings, one may prefer a smaller model over a state-of-the-art model due to the training and deployment cost constraints. We emulate such a setting through an image classification task where it is indeed feasible to memorize the training data perfectly using state-of-the-art neural networks, but instead, we use smaller neural networks for computational efficiency (cf. Section 4.2).

## 2 Related work

We discuss two types of related work: Section 2.1 for literature on memorization and generalization that motivates the ResMem algorithm; Section 2.2 for other related algorithms similar to ResMem.

### Memorization for generalization: prior work

Memorization is compitable for generalization.Overparameterized neural models with many more parameters than training samples have the capacity to perfectly fit (or _interpolate_) even random training labels ; i.e., they can drive the empirical loss to zero for _any_ training set. At the same time, when trained on real-world datasets, increasing model complexity tends to _improve_ model performance [40; 52]; that is, the models do not simply memorize the training sample, but rather learn generalizable patterns. Several works have sought to understand the reasons behind this behaviour, both empirically  and theoretically [3; 15; 8; 5; 40; 36; 38; 4; 48; 50; 53]. One recurring message from the theory is that memorization (in the form of interpolation) can be sufficient for generalization.

Memorization can be necessary for generalization.Some recent works [17; 11] showed that memorization -- either in the sense of interpolation, or in a more general sense of stability  -- may be _necessary_ for generalization. Feldman  considered a setting where the label distribution exhibits a _long-tailed_ distribution, and showed that to prevent incurring a large error on the large number of under-represented classes, it may be necessary to memorize many of their associated training samples. Cheng et al.  considered a linear regression setting where it is beneficial to fit the training targets to error lower than the Bayes-error (i.e., the inherent noise in the targets).

### Relation to existing algorithms

Nearest neighbor method.The \(k\)-nearest neighbor (\(k\)-NN) [14; 32; 26; 7] method assigns label to a test sample based on the label of its nearest neighbor(s) in the training set. Owing to its _simplicity_, _flexibility_ in defining input similarity, and _computational efficiency_ with various approximation schemes [22; 39], this method remains popular. However, the performance of \(k\)-NN drops as data becomes high dimensional [10; 39]. Therefore, to apply it to high dimensional data such as image and text , one approach is to learn a representation of data using neural networks . Following this approach,  finds that applying \(k\)-NN directly to memorize the training labels \(y_{i}\) yields similar performance with the original softmax based neural network classification. In contrast, ResMem applies \(k\)-NN to memorize the _residual_\(r_{i}\) over the predictions of a base network.

Boosting and residual fitting.Boosting algorithms such as AdaBoost  seek to construct an ensemble of "weak learner" models with good generalization. AdaBoost achieves this in an iterative manner, and can be interpreted as a particular instantiation of _forward stage-wise regression_, a classical procedure from statistics [23; 1; 47]. Intuitively, at each round, one builds a new weaker learner by fitting the residual of the ensemble of weak learners constructed thus far. This fitting is performed iteratively. ResMem can be loosely regarded as a two round boosting algorithm where the first "weak learner" is the base neural network and the second "weak learner" is the nearest-neighbor component. Note that there is no need for the thrid "weak learner", because the nearest-neighbor component already perfectly memorizes the neural network residuals.

Memory-augmented language models.In the language modelling literature, several works explore combining neural models with an external database or memory, which can be queried to retrieve additional context [34; 25; 6; 35]. Closer to our work, Khandelwal et al.  employ a linear combination of neural network and \(k\)-NN classifier components. However, a crucial difference is that our \(k\)-NN components memorizes the _residual_ of the DeepNet prediction, whereas Khandelwal et al.  memorizes the _target label_ directly; i.e., their approach is akin to an ensemble of \(k\)-NN and a deep network. Various forms of memory have also been considered in generic classification problems [41; 48; 51]. This line of literature again differs from ResMem in that their memory tries to memorize labels directly, whereas ResMem memorizes the _residuals_, leading to a natural combination of the neural network and the memory component.

Model compression for large neural networks.Since ResMem boosts the test accuracy of a small, non-memorizing neural network, we can also view it as a technique that allows a small network to match the performance of a larger one. This relates to the model compression literature. Distillation [29; 9] is a popular strategy for compressing a large neural model to a smaller one. For a survey of other effective strategies, including pruning, see Menghani . In Appendix C.2, we discuss how ResMem can be regarded as a "dual procedure" of distillation.

## 3 Theoretical results

As discussed in Section 1.1, ResMem yields the largest improvement when implicit memorization is infeasible. In this section, we formulate (cf. Section 3.1) and analyze (cf. Theorem 3.3) a stylized linear regression problem that concretizes such a setting.

Recall that ResMem (Figure 1) involves first training a base neural network \(f_{}\), and then fitting the residual of \(f_{}\) on the same training data using a nearest-neighbor regressor \(r_{}\). For feasibility of theoretical analysis, we simplify \(f_{}\) with a single layer linear neural network, i.e. linear regression, and we consider \(1\)-nearest neighbor instead of \(k\)-nearest neighbor to memorize the residual of this network. Our results suggests that ResMem improves test-time generalization by augmenting the capacity of the base model with a non-parametric nearest-neighbor component.

### Assumptions and setting

In this section, we present the setup and assumptions for the stylized linear regression problem. We consider a setting where the function class that we minimize over does _not_ include the ground-truth function that relates the covariates to the response. Therefore, even with infinite samples, the test loss will decay to a positive constant. We exactly characterize the rate of decay, and show that it converges to \(0\) under ResMem. Our analysis rests on the following assumptions.

**Assumption 3.1** (Distribution of covariates).: The distribution of covariate \(^{d}\), denoted by \(_{}\), is the uniform distribution1 over a Euclidean norm ball centered at the origin of radius \(\). The choice of radius ensures that \(_{_{}}^{}=\).

**Assumption 3.2** (Linear regression over norm ball).: Consider the problem of learning a linear function \(f_{}()=,_{}\) with \(\|_{}\|=1\) from training data \(\{(_{i},y_{i})\}_{i=1:n}\) where \(_{i}}}{{}}_{}\) and \(y_{i}=f_{}(_{i})\) using the function class

\[=\{,,\|\|<L\}.\] (1)

We assume \(L<1\) so that the problem belongs to the "hard generalization" scenario discussed in Section 1.1, where the hypothesis space is inadequate to fit the function on its own.

ResMem proceeds by first learning a linear function \(f_{n}()=_{n},\) from \(\) through empirical risk minimization (ERM):

\[_{n}=*{argmin}_{\|\| L} _{i=1}^{n}[_{i},-y_{i}]^{2}.\] (2)The empirical risk minimizer \(f_{n}\) should be thought of as the analog of \(f_{}\) in the deep learning context. It defines a ground-truth residual function \(r_{}()=f_{}()-f_{n}()\). Now we fix a test covariate \(}_{x}\). ResMem "memorizes" the residual function through the \(1\)-nearest neighbor to \(}\)

\[r_{n}(})=r_{}(}_{(1)})=f_{}( }_{(1)})-f_{n}(}_{(1)}),\] (3)

where \(}_{(1)}\) is the nearest neighbor to \(}\) among the training covariates \(_{1},,_{n}\):

\[}_{(1)}=*{argmin}_{\{_{1},,_{n}\}}\|-}\|.\]

The final prediction is

\[f_{n}^{}(})=f_{n}(})+r_{n}( }).\] (4)

Observe that if \(}\) coincides with any training sample, \(f_{n}^{}(})=f_{}(})\), i.e., we have explicit memorization. Note that we worked with \(1\)-nearest neighbor regressor for simplicity instead of the general \(k\)-nearest neighbor algorithm. The effect of choosing different \(k\) is not the main focus of this theoretical analysis.

### A decomposition of the target function

Next, we introduce a decomposition of \(f_{}\), which will help us analyze various components that make up the risk. Define

\[_{} =*{argmin}_{\|\| L}_{_{x}}[,- _{},]^{2},\] \[=*{argmin}_{\|\| L}\|- {}_{}\|=L_{},\]

which is what ERM learns in the limit of \(n\). We can think of \(_{}\) as the best function that ERM can learn. Then, we can decompose \(_{}\) into \(_{}=_{}+_{}\), where \(_{}=_{}-_{}\). This decomposition can be generalized beyond linear regression. Since \(_{}\) defines a function \(f_{}()=,_{}\), for general non-linear functions, the argument above can be generalized to the decomposition of \(f_{}\) to an learnable and non-learnable part

\[f_{}=f_{}+f_{}.\]

Intuitively, \(f_{}\) is the best function in \(\) that ERM can learn, and \(f_{}\) is beyond the capacity of ERM due to the particular choice of function class. ResMem approximates \(f_{}\) using the non-parametric nearest neighbor method, and therefore expanding the capacity of the original hypothesis class.

### A decomposition of the prediction error

We now introduce a decomposition of the prediction risk that reveals how \(\) algorithm boosts generalization. Note that the prediction error of \(\) is

\[[(f_{n}^{}(})-f_{}( }))^{2}].\] (5)

It can be decomposed into two components: \([f_{n}^{}(})-f_{}( })]^{2} 3\)

\[[\ (f_{n}(})-f_{}(}))^{2}+(f_{n}(}_{(1)})-f_{}(}_{(1)}))^{2}}_{}_{1}}+(f_{ }(})-f_{}(})-f_{}( }_{(1)})+f_{}(}_{(1)}))^{2}}_{ }_{2}}\ ].\]

We provide the detail of the decomposition in Section B.1. We can see that \(T_{1}\) arises due to the difference between \(f_{n}\) and \(f_{}\) (i.e., the estimation error), which, as we will show later, goes to \(0\) as \(n\) goes to infinity:

\[T_{1} 0n.\]

On the other hand, \(T_{2}\) arises due to the limited capacity of \(\). It captures an irreducible error of the risk, which in general is **not** asymptotically zero. However, because of the explicit memorization \(\) algorithm introduces (\(}_{(1)}}\) as \(n\)), we also have

\[T_{2} 0n.\]

This decomposition provides a statistical perspective on ResMem: it preserves the asymptotic consistency of \(T_{1}\) as in classical learning problems while enforcing the asymptotic consistency of \(T_{2}\) through the nearest-neighbor method.

### Main theoretical result

Given the set up above, we are ready to state the main theoretical result of the paper, which characterizes the rate at which test risk of ResMem approaches 0. The proof is in Appendix B.

**Theorem 3.3** (Risk for ResMem algorithm).: _For the problem defined in Assumption 3.2 with covariates distribution in Assumption 3.1, the ResMem prediction rule \(f_{n}^{}(})\) defined in equation (4) achieves risk (5)_

\[[f_{n}^{}(})-f_{}( })]^{2} d^{2}L^{2}n^{-2/3}+d^{2}(1-L)^{2}[ )}{n}]^{1/d},\]

_where \(\) denotes inequality up to a universal constant independent of \(d,n\) and \(L\)._

The result includes contribution from two terms introduced in Section 3.3:

* \(T_{1} d^{2}L^{2}n^{-2/3}\) that arises due to the difference between \(f_{n}\) and \(f_{}\).
* \(T_{2}[(n^{1/d})/n]^{1/d}\) that vanishes as the nearest neighbor of the test point approaches the test point itself \(}_{(1)}}\).

The two terms \(T_{1}\) and \(T_{2}\) can be viewed as "two stages of learning". Without the ResMem memorization component, we have the usual story of machine learning: \(T_{1} 0\) at the usual parametric rate, and \(T_{2}\) stays as an irreducible error, so the overall test error diminishes to a constant at a very fast rate. With the introduction of nearest neighbor memorization procedure, \(T_{2}\) can also be reduced to \(0\) at a slower rate, whereas the fast decay of \(T_{1}\) is still preserved.

This result shows why it is _not favorable_ to use the \(k\)-nearest neighbor component to memorize the response directly: as a corollary of setting \(L=0\) in Theorem 3.3, pure nearest neighbor would result in an overall slow rate of \( n^{-1/d}\). However, with ResMem, we can enjoy benefit of having the test loss being asymptotically \(0\), while also enjoying the fast rate of \(n^{-2/3}\) for smaller sample sizes.

## 4 Empirical results

In this section, we present empirical results on image classification and language modeling that showcase the efficacy of ResMem. In Section 4.1, we present details of applying the ResMem algorithm to classification problems on real dataset. In Section 4.2 and Section 4.3, we present the setup and the result for vision and language experiments, respectively. In Section 4.4 we conduct an empirical analysis to explain where the improved accuracy of ResMem comes from. Finally, in addition to evaluating the improvement ResMem algorithm over DeepNet itself, we compare ResMem with other reasonable baselines including  in Appendix F.

### Details of ResMem algorithm for classification

We consider multi-class classification problems over instances \(\) and labels \(\{1,2,,L\}=[L]\). Given training examples \(S=\{(x_{i},y_{i})\}_{i[n]}()^{n}\), the goal is to learn a scorer \(f^{L}\) that, given an instance, assigns an affinity score for each label. Such an \(f\) should minimize the _misclassification error_ on test samples:

\[L_{01}(f)_{(x,y)}(y(f(x))),\] (6)

where \((z)_{y^{}[L]}z_{y^{}}\), and \(\) is the distribution over labelled instances. To achieve this, one typically minimizes the _empirical loss_

\[_{}(f)_{i[n]}(y_{i},f(x_{i})),\]

where \([L]^{L}_{+}\) is a loss function. Ideally, one would like to use \(_{01}(y,f(x)) 1(y(f(x)))\); for computational tractability, it is popular to instead use a _surrogate loss_, such as the softmax cross-entropy.

Given the notation above, ResMem operates as follows:1. **Train the base DeepNet.** Train a neural network \(f_{}\) on the training samples \(S\) as usual.
2. **Prepare the residual data.** Compute the _residual_2 prediction of each training example as 3. **Predict via memorized residuals.** To obtain a prediction on a test sample \(\), first compute its embedding \(=()\). Then, use soft \(k\)-nearest neighbor method to build a function \(r_{}\) defined by weights \(_{i}()\): \[r_{}()=_{i=1}^{n}_{i}( ) r_{i}.\] (7) The weights \(_{i}()\) satisfy \(_{i}_{i}()=1\), and are computed from raw weights \(w_{i}\) as follows: \[w_{i}=(-\|-z_{i}\|_{2}/),_{i}( )(w_{i} w_{(k)})w_{i},\] where \(w_{(k)}\) represents the \(k\)-th largest entry of \(w_{i}\)'s. Note that \(k\) and \(\) are two hyperparameters that collectively controls the locality of nearest neighbor search.

We make the final prediction based on the following scorer:

\[f_{}()=(f_{}()/T)+r_{}().\] (8)

_Remark 4.1_ (Explicit memorization).: Smaller \(k\) or \(\) corresponds to putting higher weight on residuals of the closest neighboring training examples. For sufficiently small \(k\) and \(\), \(f_{}\) achieves exact memorization of the training sample, i.e., \((f_{}(x_{i}))=y_{i}\) for all \(i[n]\).

_Remark 4.2_ (Computation cost).: The vision experiments have moderate training sample size, so we perform exact nearest neighbor search and discuss the computation cost in Section 4.2. For language experiments, the training sample size is so large that the exact nearest neighbor computation is infeasible, so we rely on _approximate_ nearest neighbor search discussed in Section 4.2.

### Image classification

In this subsection, we mainly consider image classification task on ResNet  with CIFAR100  dataset. We provide additional ImageNet  results in Appendix D.

Setup.We use CIFAR-ResNet-\(\{8,14,20,32,44,56\}\) as the base DeepNet. For all six DeepNet training, we use SGD with batch size 128, trained for 256 epochs. We use a peak learning rate 0.4, and momentum 0.9. We warm up the learning rate linearly for the first 15 epochs, and decay the learning rate by \(0.1\) after epochs \(\{96,192,224\}\). For ResMem, we use the pre-logit layer as the

Figure 2: ResMem improvement on CIFAR100 with respect to training sample size and deep network architecture. **(a):** Using progressively larger CIFAR-ResNet architecture. **(b):** Using \(10\%,20\%,,100\%\) of training data.

image embedding, which has dimension 64. For the nearest neighbor search (Step 3, Section 4.1), we define the distance between two images to be the \(_{2}\) distance between their embeddings. We use \(=0.7\), \(k=53\), and \(T=1.4\) to compute the weights for the nearest neighbor regressor. We provide the sensitivity analysis of test accuracy against ResMem parameters in Appendix C (cf. Figure 5).

Results.The results for CIFAR-ResNet-\(\{8,14,20,32,44,56\}\) are reported in Figure 2(a). We can see that ResMem boosts the test accuracy of CIFAR-ResNet8 from 56.46% to **59.66%**, which is between the base DeepNet test accuracy for CIFAR-ResNet8 and CIFAR-ResNet14. To access the statistical reliability of the improvement, we repeat the CIFAR-ResNet-8 experiment 5 times over random initialization of DeepNet etc. We and that the average ResMem accuracy is \(59\%\) with standard deviation \(0.7\%\), and the average DeepNet accuracy is \(56.5\%\) with standard deviation \(0.8\%\).

Computationally, we estimate the CPU latency of a CIFAR-ResNet-8 to be 15.9 ms for a single test image. By contrast, the \(k\)-NN step takes 4.8 ms for the same test image. To contextualize the latency cost, the total cost of ResMem with ResNet-8 (15.9 ms + 4.8 ms) is lower than the cost of the next-sized model, i.e., ResNet-14 (26.2 ms). Regarding the memory cost, for a batch size of 1 and images of size 32 x 32, a ResNet-8 ( 68K params) requires 2.5MB, while a ResNet-14 ( 128K params) requires 4MB. Embeddings from a ResNet-8 and ResNet-14 are both 64 dimensional. To embed the entire CIFAR100 training set (50K examples) requires 15MB of disk space.

Varying sample size.We repeat the above experiment on CIFAR-ResNet-8 with subsets (\(10\%,20\%,,100\%\)) of CIFAR100 training data (subsampled uniformly across different classes). The size of the index set for nearest-neighbor search is the same as the training set for base neural networks (e.g., model with 10% CIFAR100 data also uses the same 10% data for nearest-neighbor search). On the left (right) of Figure 2(b), we report the test (training) accuracy of ResMem and baseline DeepNet. As a sanity check, we can see that ResMem always achieves perfect training accuracy, and the DeepNet training accuracy decreases as samples increase (since it's harder to fit larger dataset). We can see that ResMem yields _progressively larger margin of improvement when more data is used_. This trend suggests a desirable property of ResMem: in real problems where the dataset is extremely large, ResMem is expected to bring even greater benefit.

Figure 3: Examples from CIFAR100 and C4 test set with the property that **(i)**\(y^{}\) is correct; **(ii)**\(y^{}\) is wrong but close in meaning. We use red to denote the ResMem prediction and blue to denote DeepNet prediction. The DeepNet predictions capture _coarse_ structure (e.g., predicting poppy for a sample whose true label is rose), which can be refined by ResMem capturing the remaining _fine-grained_ structure.

### Language modeling

Setup.For the language experiment, we use a Decoder-Only T5-{small, large}  model and C4  dataset. C4 is generated from scraping the internet and commonly used as a pretraining dataset or part of the pretraining mix. We pre-trained the DeepNet on C4 training split with auto-regressive language modeling task. For experimental efficiency, we used 1% of the C4 training split (which corresponds to 1,639 million tokens) as the retrieval database, and extracted last transformer layer's pre-MLP, post-LayerNorm representations as the key embeddings for \(k\)NN search, and we created the query embeddings using the whole validation split and the same representation location. For each query, we retrieved 50 neighbors with \(L_{2}\) distance using approximate nearest neighbor search algorithm ScaNN . We used the temperature \(T=1\) for the residual computation and \(=1\) for computing the neighbor weights. The predicted token is the one with highest probability, similar to greedy decoding, and we measured the prediction accuracy to match the vision experiments.

Results.On T5-small, ResMem boosts test accuracy from 38.01% to **40.87%**, which is around the accuracy (40.08%) of a T5-base model without ResMem. On T5-large, ResMem boosts the test accuracy from 44.8% to **45.6%**. This demonstrates that with explicit memorization, we may leverage smaller base language models while reaping the performance benefits of large language models. Computationally, as the index set is quite large (1.6 billion tokens), exact k-nearest neighbor search is infeasible. So we use the approximate nearest neighbor search algorithm ScaNN  to reduce compute time. Please see Appendix E for details on base model training and data processing.

### Where does the improvement come from?

In this section, we identify test samples that contributes to the accuracy improvement of CIFAR100 with CIFAR-ResNet-8 and C4 with T5-small. Let \(}\) be the difference between the test accuracy of ResMem and baseline DeepNet:

\[}=L_{01}(f_{})-L_{01}(f_{}),\]

where \(L_{01}\) is the misclassification error as defined in equation (6). We offer a decomposition of \(}\) that sheds light into the mechanism behind ResMem. For a test set \(\{(x_{i},y_{i})\}_{i=1}^{m}\), let \(y_{i}^{}\) be the ResMem prediction on instance \(x_{i}\) and let \(y_{i}^{}\) be the baseline neural network prediction on \(x_{i}\). When \(y_{i}^{}=y_{i}^{}\), sample \(x_{i}\) does not contribute to \(}\). When \(y_{i}^{} y_{i}^{}\), this could arise either from the desirable event where the deep network misclassifies while ResMem classifies correctly; or from the undesirable event where the ResMem misclassifies, while the deep network classifies correctly. These can be summarized by the \(\) (_true positive rate_) and \(\) (_false positive rate_) respectively:

\[=_{i=1}^{m}\{y_{i}^{} y_{i}y_{i}^{}=y_{i}\}.\] (9)

\[=_{i=1}^{m}\{y_{i}^{}=y_{i}y_{i}^{} y_{i}\}.\] (10)

Note that \(}=-\). The decomposition of \(}\) says that the gain of ResMem came from the \(\) samples, provided they outweigh the \(\) samples.

On CIFAR-ResNet-8, we find \(\)\(=\)5.89% and \(\)\(=\)2.70%, leading to \(}\)=3.19%. On T5-small with C4 validation split, we find \(\)\(=\)5.37% and \(\)\(=\)2.44%, leading to \(}\)=2.93%.

Analysis of TPR samplesFocusing on the test samples where ResMem helps (\(y_{i}=y_{i}^{} y_{i}^{}\)), we identify a common underlying pattern: while the DeepNet makes an incorrect prediction, it still captures some coarse structure. For example, in CIFAR100, one sample has correct label \(y_{i}=y_{i}^{}=\), but the DeepNet predicts \(y_{i}^{}=\), i.e., the label of a different type of flower. (cf. Figure 3). We find similar behavior for the language modeling task (cf. Figure 3).

This empirical analysis suggests the DeepNet in isolation can already learn some large scale structures, but is unable to make fine-grained distinctions. This is where ResMem helps: _ResMem helps memorize information in the training label that the DeepNet cannot learn._Additional insights from the decomposition.In this paper, we choose the ResMem hyperparameters that minimizes the test error on the validation set or, equivalently, maximize \(}\). Inspired by the decomposition of \(}\), we propose an alternative hyperparameter selection procedure based on the following optimization problem:

\[}_{()<0.05}( ),\]

which ensures that ResMem modifies the DeepNet predictions in a more conservative manner. In particular, bounding \(\) implies that ResMem has minimal impact on the examples where DeepNet already makes correct predictions. At the same time, a higher value of \(\) corresponds maximizing the desirable occurrences where ResMem can correct a wrong prediction by DeepNet.

## 5 Discussion and future works

Joint training of \(k\)NN and DeepNet.The current formulation of ResMem builds the base DeepNet and \(k\)NN components sequentially. Consequently, the DeepNet is trained completely oblivious to the fact that there is a subsequent \(k\)NN model that will memorize its residuals. A natural direction of future work is to consider the _joint_ training of DeepNet and \(k\)NN, so that the models can dynamically interact during training to determine which portion of label is for DeepNet to learn, and the remaining is for \(k\)NN to memorize.

To explore the role of training during the first stage, we re-evaluate the CIFAR-ResNet-8 experiment by stopping DeepNet training at different epochs (Table 1). We can see that when the #epoch is small,

ResMem has a dramatic improvement in accuracy. One of the key roles of the first training phase is to learn good representations of the training data so the nearest neighbor retrieval is performed on more meaningful representations. This simple experiments suggests that the proposed direction has the potential to dramatically reduce the training time of DeepNet - while obtaining similar test accuracy with the help of ResMem.

Calibration of ResMem.A potential problem with applying ResMem to classification is _scorer mis-calibration_. The output of the ResMem prediction vector \(f_{}(x)\) (8) is not guaranteed to lie on the probability simplex. This is not an issue when we only care about the predicted class membership, since we take the argmax of \(f_{}(x)\). However, this limitation hinders us to access the _confidence_ of the ResMem prediction. To remedy this, a possible future work is to consider alternative notions of residual. For example, we can do memorization in the logit space instead of the probability space. Then, the one-hot encoding of the true label may be replaced by class mean when defining the residual.

Distribution shift.Finally, ResMem can be a promising approach to tackle test-time covariate shift. The nearest neighbor modifies the prediction of DeepNet based on the training covariate that are closer to the test covariate, making the algorithm more _adaptive_ to the specific test covariate .