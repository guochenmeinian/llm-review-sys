# Learning bounded degree polytrees with samples

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We establish finite-sample guarantees for efficient proper learning of bounded-degree _polytrees_, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical models. Very recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We considerably extend their results by providing an efficient algorithm which learns \(d\)-polytrees in polynomial time and sample complexity when the in-degree \(d\) is constant, provided that the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic lower bound, showing that the dependence of our sample complexity is nearly tight in both the dimension and target accuracy parameters.

## 1 Introduction

Distribution learning, or density estimation, is the task of obtaining a good estimate of some unknown underlying probability distribution \(P\) from observational samples. Understanding which classes of distributions could be or could not be learnt efficiently is a fundamental problem in both computer science and statistics, where efficiency is measured in terms of _sample_ (data) and _computational_ (time) complexities.

_Bayesian networks_ (or _Bayes nets_ in short) represent a class of high-dimensional distributions that can be explicitly described by how each variable is be generated sequentially in a directed fashion. Being interpretable, Bayes nets have been used to model beliefs in a wide variety of domains (e.g. see Jensen and Nielsen, 2007; Koller and Friedman, 2009) and references therein). A fundamental problem in computational learning theory is to identify families of Bayes nets which can be learned efficiently from observational data.

Formally, a Bayes net is a probability distribution \(P\), defined over some directed acyclic graphs (DAG) \(G=(V,E)\) on \(|V|=n\) nodes that factorizes according to \(G\) (i.e. Markov with respect to \(G\)) in the following sense: \(P(v_{1},,v_{n})=_{v_{1},,v_{n}}P(v(v))\), where \((v) V\) are the parents of \(v\) in \(G\). While it is well-known that given the DAG (structure) of a Bayes net, there exists sample-efficient algorithms that output good hypotheses Dasgupta, 1997; Bhattacharyya et al., 2020), there is no known computationally efficient algorithms for obtaining the DAG of a Bayes net. In fact, it has long been understood that Bayes net structure learning is computationally expensive, in various general settings (Chickering et al., 2004). However, these hardness results fall short when the goal is learning the distribution \(P\) in the probabilistically approximately correct (PAC) (Valiant, 1984) sense (with respect to, say, KL divergence or total variation distance), rather than trying to recover an exact graph from the Markov equivalence class of \(P\).

_Polytrees_ are a subclass of Bayesian networks where the undirected graph underlying the DAG is a forest, i.e., there is no cycle for the undirected version of the DAG; a polytree with maximum in-degree \(d\) is also known as a \(d\)-polytree. With an infinite number of samples, one can recover theDAG of a non-degenerate polytree in the equivalence class with the Chow-Liu algorithm (Chow and Liu, 1968) and some additional conditional independence tests (Rebane and Pearl, 1988). However, this algorithm does _not_ work in the finite sample regime. The only known result for learning polytrees with finite sample guarantees is for 1-polytrees (Bhattacharyya et al., 2021). Furthermore, in the agnostic setting, when the goal is to find the closest polytope distribution to an arbitrary distribution \(P\), the learning problem becomes NP-hard (Dasgupta, 1999).

In this work, we investigate what happens when the given distribution is a \(d\)-polytree, for \(d>1\). _Are \(d\)-polytrees computationally hard to learn in the realizable PAC-learning setting?_ One motivation for studying polytrees is due to a recent work of Gao and Aragam (2021) which shows that polytrees are easier to learn than general Bayes nets due to the underlying graph being a tree, allowing typical causal assumptions such as faithfulness to be dropped when designing efficient learning algorithms.

**Contributions.** Our main contribution is a sample-efficient algorithm for proper Bayes net learning in the realizable setting, when provided with the ground truth skeleton (i.e., the underlying forest). Crucially, our result does not require any distributional assumptions such as strong faithfulness, etc.

**Theorem 1**.: _There exists an algorithm which, given \(m\) samples from a polytree \(P\) over \(^{n}\), accuracy parameter \(>0\), failure probability \(\), as well as its maximum in-degree \(d\) and the explicit description of the ground truth skeleton of \(P\), outputs a \(d\)-polytree \(\) such that \(d_{}(P)\) with success probability at least \(1-\), as long as_

\[m=\!(}{})\.\]

_Moreover, the algorithm runs in time polynomial in \(m\), \(||^{d}\), and \(n^{d}\)._

We remark that our result holds when even given only an upper bound on the true in-degree \(d\). In particular, our result provides, for constant \(||\), \(d\), an upper bound of \((n/)\) on the sample complexity of learning \(O(1)\)-polytrees. Note that this dependence on the dimension \(n\) and the accuracy parameter \(\) are optimal, up to logarithmic factors: indeed, we establish in Theorem 15 an \((n/)\) sample complexity lower bound for this question, even for \(d=2\) and \(||=2\).1

We also state sufficient conditions on the distribution that enable recovery of the ground truth skeleton. Informally, we require that the data processing inequality hold in a strong sense with respect to the edges in the skeleton graph. Under these conditions, combining with our main result in Theorem 1, we obtain a polynomial-time PAC algorithm to learn bounded-degree polytrees from samples.

**Other related work.** Structure learning of Bayesian networks is an old problem in machine learning and statistics that has been intensively studied; see, for example, Chapter 18 of Koller and Friedman (2009). Many of the early approaches required faithfulness, a condition which permits learning of the Markov equivalence class, e.g. Spirtes and Glymour (1991), Chickering (2002), Friedman et al. (2013). Finite sample complexity of such algorithms assuming faithfulness-like conditions has also been studied, e.g. Friedman and Yakhini (1996). An alternate line of more modern work has considered various other distributional assumptions that permits for efficient learning, e.g., Chickering and Meek (2002), Hoyer et al. (2008), Shimizu et al. (2006), Peters and Buhlmann (2014), Ghoshal and Honorio (2017), Park and Raskutti (2017), Aragam et al. (2019), with the latter three also showing analyzing finite sample complexity. Specifically for polytrees, Rebane and Pearl (1988), Geiger et al. (1990) studied recovery of the DAG for polytrees under the infinite sample regime. Gao and Aragam (2021) studied the more general problem of learning Bayes nets, and their sufficient conditions simplified in the setting of polytrees. Their approach emphasize more on the exact recovery, and thus the sample complexity has to depend on the minimum gap of some key mutual information terms. In contrast, we allow the algorithm to make mistakes when certain mutual information terms are too small to detect for the given sample complexity budget and achieve a PAC-type guarantee. As such, once the underlying skeleton is discovered, our sample complexity only depends on the \(d,n,\) and not on any distributional parameters.

There are existing works on Bayes net learning with tight bounds in total variation distance, with a focus on sample complexity (and not necessarily computational efficiency); for instance, (Canonne et al., 2020). Acharya et al. (2018) consider the problem of learning (in TV distance) a bounded-degree causal Bayesian network from interventions, assuming the underlying DAG is known.

**Outline of paper.** We begin with some preliminary notions and related work in Section 2. Section 3 then shows how to recover a polytree close in KL divergence, assuming knowledge of the skeleton and maximum in-degree. Section 4 gives sufficient conditions to recover the underlying skeleton from samples, while Section 5 provides our sample complexity lower bound. We conclude in Section 6 with some open directions and defer some full proofs to the appendix.

## 2 Preliminaries and tools from previous work

### Preliminary notions and notation

We write the disjoint union as \(\). For any set \(A\), let \(|A|\) denotes its size. We use hats to denote estimated quantities, e.g., \((X;Y)\) will be the estimated mutual information of \(I(X;Y)\). We employ the standard asymptotic notation \(O()\), \(()\)\(()\), and write \(()\) to omit polylogarithmic factors. Throughout, we identify probability distributions over discrete sets with their probability mass functions (pmf). We use \(d^{*}\) to denote the true maximum in-degree of the underlying polytree.

### Probability distribution definitions

We begin by defining KL divergence and squared Hellinger distances for a pair of discrete distributions with the same support.

**Definition 2** (KL divergence and squared Hellinger distance).: For distributions \(P,Q\) defined on the same discrete support \(\), their KL divergence and squared Hellinger distances are defined as \(d_{}(P Q)=_{x}P(x)\) and \(d_{}^{2}(P,Q)=1-_{x}\) respectively.

Abusing notation, for a distribution \(P\) on variables \(X=\{X_{1},,X_{n}\}\), we write \(P_{S}\) to mean the projection of \(P\) to the subset of variables \(S X\) and \(P_{G}\) to mean the projection of \(P\) onto a graph \(G\). More specifically, we have \(P_{G}(x_{1},,x_{n})=_{x X}P(x_{G}(x))\) where \(_{G}(x)\) are the parents of \(x\) in \(G\). Note that \(P_{G}\) is the closest distribution2 on \(G\) to \(P\) in \(d_{}\), i.e. \(P_{G}=*{argmin}_{Q G}d_{}(P Q)\). By Chow and Liu (1968), we also know that

\[d_{}(P,P_{G})=-_{i=1}^{n}I(X_{i};_{G}(X_{i}))-H(P_{X})+_ {i=1}^{n}H(P_{X_{i}})\,\] (1)

where \(H\) is the entropy function. Note that only the first term depends on the graph structure of \(G\).

By maximizing the sum of mutual information (the negation of the first term in (1)), we can obtain an \(\)-approximated graph \(G\) such that \(d_{}(P P_{G})\). In the case of tree-structured distributions, this can be efficiently solved by using any maximum spanning tree algorithm; a natural generalization to bounded degree bayes nets remains open due to the hardness of solving the underlying optimization problem (Hoffgen, 1993). If any valid topological ordering of the target Bayes net \(P\) is present, then an efficient greedy approach is able to solve the problem.

**Definition 3** ((Conditional) Mutual Information).: Given a distribution \(P\), the mutual information of two random variables \(X\) and \(Y\), supported on \(\) and \(\) respectively, is defined as

\[I(X;Y)=_{x,y}P(x,y)()\.\]

Conditioning on a third random variable \(Z\), supported on \(\), the conditional mutual information is defined as:

\[I(X;Y Z)=_{x,y,z}P(x,y,z) ()\.\]

By adapting a known testing result from (Bhattacharyya et al., 2021, Theorem 1.3), we can obtain the following corollary, which we will use. We provide the full derivation in the supplementary materials.

**Corollary 4** (Conditional Mutual Information Tester, adapted from [Bhattacharyya et al., 2021, Theorem 1.3]).: _Fix any \(>0\). Let \((X,Y,Z)\) be three random variables over \(_{X},_{Y},_{Z}\) respectively. Given the empirical distribution \((,,)\) over a size \(N\) sample of \((X,Y,Z)\), there exists a universal constant \(0<C<1\) so that for any_

\[N(||_{Y}||_{Z}|}{ }||_{Y}||_{Z}|}{ }||_{Y}||_{Z}|( 1/)}{}),\]

_the following statements hold with probability \(1-\):_

_(1) If_ \(I(X;Y Z)=0\)_, then_ \((X;Y Z)<C\)_._

_(2) If_ \((X;Y Z) C\)_, then_ \(I(X;Y Z)>0\)_._

_(3) If_ \((X;Y Z) C\)_, then_ \(I(X;Y Z)<\)_._

_Unconditional statements involving \(I(X;Y)\) and \((X;Y)\) hold similarly by choosing \(|_{Z}|=1\)._

### Graph definitions

Let \(G=(V,E)\) be a graph on \(|V|=n\) vertices and \(|E|\) nodes where adjacencies are denoted with dashes, e.g. \(u-v\). For any vertex \(v V\), we use \(N(v) V\{v\}\) to denote the neighbors of \(v\) and \(d(v)=|N(v)|\) to denote \(v\)'s degree. An undirected cycle is a sequence of \(k 3\) vertices such that \(v_{1}-v_{2}--v_{k}-v_{1}\). For any subset \(E^{} E\) of edges, we say that the graph \(H=(V,E^{})\) is the edge-induced subgraph of \(G\) with respect to \(E^{}\).

For oriented graphs, we use arrows to denote directed edges, e.g. \(u v\). We denote \((v)\) to denote the parents of \(v\) and \(d^{in}(v)\) to denote \(v\)'s incoming degree. An interesting directed subgraph on three vertices is the v-structure, where \(u v w\) and \(u(G^{*})\). Let \(N^{in}(v) N(v)\) be the current set of incoming neighbors of \(v\). Let \(N^{out}(v) N(v)\) be the current set of outgoing neighbors of \(v\). Let \(N^{un}(v) N(v)\) be the current set of unoriented neighbors of \(v\). That is,

\[N(v)=N^{in}(v)\ \ N^{out}(v)\ \ N^{un}(v)\]

``` Input: Skeleton \((G^{*})=(V,E)\), max in-degree \(d^{*}\), threshold \(>0\), universal constant \(C\) Output: A complete orientation of \((G^{*})\)
1: Run Phase 1: Orient strong v-structures (Algorithm 3) \(\)\((n^{d^{*}})\) time
2: Run Phase 2: Local search and Meek \(R1(d^{*})\) (Algorithm 4) \(\)\((n^{3})\) time
3: Run Phase 3: Freely orient remaining unoriented edges (Algorithm 5) \(\)\((n)\) time via DFS
4:return\(\) ```

**Algorithm 1** Algorithm for known skeleton and max in-degree.

There are three phases to our algorithm. In Phase 1, we orient strong v-structures. In Phase 2, we locally check if an edge is forced to orient one way or another to avoid incurring too much error. In Phase 3, we orient the remaining unoriented edges as a 1-polytree. Since the remaining edges were not forced, we may orient the remaining edges in an arbitrary direction (while not incurring "too much error") as long as the final incoming degrees of any vertex does not increase by more than 1. Subroutine Orient (Algorithm 2) performs the necessary updates when we orient \(u-v\) to \(u v\).

``` Input: Vertices \(u\) and \(v\) where \(u-v\) is currently unoriented
1: Orient \(u-v\) as \(u v\).
2: Update \(N^{in}(v)\) to \(N^{in}(v)\{u\}\) and \(N^{un}(v)\) to \(N^{un}(v)\{u\}\).
3: Update \(N^{out}(u)\) to \(N^{out}(u)\{v\}\) and \(N^{un}(u)\) to \(N^{un}(u)\{v\}\). ```

**Algorithm 2**Orient: Subroutine to orient edges

### Analysis

Observe that we perform \((n^{d^{*}})\) (conditional) mutual information tests in Algorithm 1. The following lemma (Lemma 5) ensures us that _all_ our tests will behave as expected with sufficient samples. As such, in the rest of our analysis, we analyze under the assumption that our tests are correct.

**Lemma 5**.: _Suppose all variables in the Bayesian network has alphabet \(\). For \(^{}>0\), with_

\[m(+1}}{^{}} +1} n^{d^{*}}}{}+ 1}(n^{d^{*}}/)}{^{}})\]

_empirical samples, \((n^{d^{*}})\) statements of the following forms, where \(\) and \(\) are variable sets of size \(|\ \ | d\) and \(Z\) is possibly \(\), all succeed with probability at least \(1-\):_

Figure 1: Running polytree example with \(d^{*}=3\) where \(I(a;b,c)=I(b;a,c)=I(c;a,b)=0\) due to the deg-3 v-structure centered at \(d\). Since \(I(a;f d)=0\), Corollary 4 tells us that \((a;f d) C\). Thus, we will _not_ detect \(a d f\) erroneously as a strong deg-2 v-structure \(a d f\).

_(1) If \(I(; Z)=0\), then \((; Z)<C^{}\),_

_(2) If \((; Z) C^{}\), then \(I(; Z)>0\),_

_(3) If \((; Z) C^{}\), then \(I(; Z)<^{}\)._

Proof.: Use Corollary 4 and apply union bound over \((n^{d})\) tests. 

Recall that \((v)\) is the set of true parents of \(v\) in \(G^{*}\). Let \(H\) be the forest induced by the remaining unoriented edges after phase 2. Let \(\) be returned graph of the algorithm 1. Let us denote the final \(N^{in}(v)\) as \(^{in}(v)\) at the end of Phase 2, just before freely orienting, i.e. the vertices pointing into \(v\) in \( H\). Let \(^{un}(v)=(v)^{in}(v)\) be the set of ground truth parents that are not identified in Phase 1. Since the algorithm does not make mistakes for orientations in \( H\) (Lemma 6), all edges of in \(^{un}(v)\) will be unoriented.

**Lemma 6**.: _Any oriented arc in \( H\) is a ground truth orientation. That is, any vertex parent set in \( H\) is a subset of \((v)\), i.e. \(^{in}(v)(v)\), and \(N^{in}(v)\) at any time during the algorithm will have \(N^{in}(v)^{in}(v)\)._

Let \((v)\) be the proposed parents of \(v\) output by Algorithm 1. The KL divergence between the true distribution and our output distribution is essentially \(_{v V}I(v;(v))-_{v V}I(v;(v))\) as the structure independent terms will cancel out.

To get a bound on the KL divergence, we will upper bound \(_{v V}I(v;(v))\) and lower bound \(_{v V}I(v;(v))\). To upper bound \(I(v;(v))\) in terms of \(^{in}(v)(v)\) and \(I(v;u)\) for \(u^{un}(v)\), we use Lemma 8 which relies on repeated applications of Lemma 7. To lower bound \(_{v V}I(v;(v))\), we use Lemma 9.

**Lemma 7**.: _Fix any vertex \(v\), any \(S^{un}(v)\), and any \(S^{}^{in}(v)\). If \(S\), then there exists a vertex \(u S S^{}\) with_

\[I(v;S S^{}) I(v;S S^{}\{u\})+I(v;u)+ \;.\] (2)

**Lemma 8**.: _For any vertex \(v\) with \(^{in}(v)\), we can show that_

\[I(v;(v))|(v)|+I(v;^{in}(v))+_{u^{un}(v )}I(v;u)\;.\]

**Algorithm 3** Phase 1: Orient strong v-structures

```
1:\(d d^{*}\)
2:while\(d 2\)do
3:for\(v V\)do\(\) Arbitrary order
4: Let \(_{d} 2^{N(v)}\) be the set of \(d\) neighbors of \(v\)\(\)\(|_{d}|=\)
5:for\(S_{d}\) s.t. \(|S|=d\), \(|S N^{in}(v)| d^{*}\), and \((u;S\{u\} v) C\), \( u S\)do
6:for\(u S\)do\(\) Strong deg-\(d\) v-structure
7: orient(\(u\), \(v\))\(\) Decrement degree bound
8:\(d d-1\) ```

**Algorithm 4** Phase 2: Orient strong v-structures

Figure 2: Suppose we have the following partially oriented graph in the execution of Algorithm 4 (after Phase 1). Since \(N^{in}(d)=\{a,b\}\), we will check the edge orientations of \(c-d\) and \(f-d\). Since \(I(f;\{a,b\} d)=0\), we will have \((f;\{a,b\} d)\), so we will _not_ erroneously orient \(f d\). Meanwhile, \(I(c;\{a,b\})=0\), we will have \((c;\{a,b\})\), so we will _not_ erroneously orient \(d c\).

In Phase 3, we increase the incoming edges to any vertex by at most one. The following lemma tells us that we lose at most4 an additive \(\) error per vertex.

**Lemma 9**.: _Consider an arbitrary vertex \(v\) with \(^{in}(v)\) at the start of Phase 3. If Phase 3 orients \(u v\) for some \(u-v H\), then_

\[I(v;^{in}(v)\{u\}) I(v;^{in}(v))+I(v;u)-.\]

By using Lemma 8 and Lemma 9, we can show our desired KL divergence bound (Lemma 10).

**Lemma 10**.: _Let \((v)\) be the true parents of \(v\). Let \((v)\) be the proposed parents of \(v\) output by our algorithm. Then,_

\[_{v V}I(v;(v))-_{v V}I(v;(v)) n(d^{*}+1) \;.\]

With these results in hand, we are ready to establish our main theorem:

Proof of Theorem 1.: We first combine Lemma 10 and Lemma 5 with \(^{}=+1)}\) in order to obtain an orientation \(\) which is close to \(G^{*}\). Now, recall that there exist efficient algorithms for estimating the parameters of a Bayes net with in-degree-\(d\) (note that this includes \(d\)-polytrees) \(P\) once a close-enough graph \(\) is recovered (Dasgupta, 1997; Bhattacharyya et al., 2020), with sample complexity \(}(||^{d}n/)\). Denote the final output \(_{}\), a distribution that is estimated using the conditional probabilities implied by \(\). One can bound the KL divergences as follows:

\[d_{}(P P_{})-d_{}(P P_{G^{*}}) /2 d_{}(P_{ {G}})-d_{}(P P_{})/2\;.\]

Thus, we see that

\[d_{}(P_{})+d_{}(P  P_{G^{*}})=\;.\]

## 4 Skeleton assumption

In this section, we present a set of _sufficient_ assumptions (Assumption 11) under which the Chow-Liu algorithm will recover the true skeleton even while with finite samples.

**Assumption 11**.: For any given distribution \(P\), there exists a constant \(_{P}>0\) such that:

(1) For every pair of nodes \(u\) and \(v\), if there exists a path \(u--v\) of length greater than \(2\) in \(G^{*}\), then then \(I(u;v)+3_{P} I(a;b)\) for every pair of adjacent vertices \(a-b\) in the path.

(2) For every pair of directly connected nodes \(a-b\) in \(G^{*}\), \(I(a;b) 3_{P}\).

Suppose there is a large enough gap of \(_{P}\) between edges in \(G^{*}\) and edges outside of \(G^{*}\). Then, with \((1/_{P}^{2})\) samples, each estimated mutual information \((a;b)\) will be sufficiently close to the true mutual information \(I(a;b)\). Thus, running the Chow-Liu algorithm (which is essentially maximum spanning tree on the estimated mutual information on each pair of vertices) recovers \((G^{*})\).

**Lemma 12**.: _Under Assumption 11, running the Chow-Liu algorithm on the \(m\)-sample empirical estimates \(\{(u;v)\}_{u,v V}\) recovers a ground truth skeleton with high probability when \(m(^{2}})\)._

Combining Lemma 12 with our algorithm Algorithm 1, one can learn a polytree that is \(\)-close in KL with \(}(\{^{2}},  n}{}\})\) samples, where \(_{P}\) depends on the distribution \(P\).

## 5 Lower bound

In this section, we show that \((n/)\) samples are necessary _even when a known skeleton is provided_. For constant in-degree \(d\), this shows that our proposed algorithm in Section 3 is sample-optimal up to logarithmic factors.

We first begin by showing a lower bound of \((1/)\) on a graph with three vertices, even when the skeleton is given. Let \(G_{1}\) be \(X Z Y\) and \(G_{2}\) be \(X Z Y\), such that \((G_{1})=(G_{2})\) is \(X-Z-Y\). Now define \(P_{1}\) and \(P_{2}\) as follows:

Figure 4: The five different possible orientations of \(H\). Observe that the ground truth orientation of these edges is inconsistent with all five orientations shown here.

Figure 3: Consider the partially oriented graph before the final phase, where \(H\) is the edge-induced subgraph on the unoriented edges in red. Since \(d^{*}=3\) is known, we can conclude that \(g i\) was oriented due to a local search step and not due to Meek \(R1(3)\). We have the following sets before the final phase: \(^{in}(c)=\{a,b\}\), \(^{in}(g)=\{f,j\}\), \(^{i}=\{g\}\), \(^{un}(d)=\{c\}\), \(^{un}(f)=\{d,e\}\), and \(^{un}(e)=\{h\}\). With respect to the chosen orientation of \(H\) and the notation in Lemma 10, we have \(A=\{c,d,f,e,h\}\), \(a_{c}=d\), \(a_{d}=f\), \(a_{f}=e\), and \(a_{e}=h\). Observe that the \(^{un}\)’s and \(a\)’s are two different ways to refer to the set of red edges of \(H\).

\[P_{1}:X(1/2)\\ Z=X&\\ (1/2)&\\ Y=Z&\\ (1/2)&1- P_{2}:X(1/2)\\ Y(1/2)\\ Z=X&\\ Y&\\ (1/2)&1/2-\] (3)

The intuition is that we keep the edge \(X Z\) "roughly the same" and tweak the edge \(Y-Z\) between the distributions. We define \(P_{i,G}\) as projecting \(P_{i}\) onto \(G\). One can check that the following holds (see Supplemental for the detailed calculations):

**Lemma 13**.: _Let \(G_{1}\) be \(X Z Y\) and \(G_{2}\) be \(X Z Y\), such that \((G_{1})=(G_{2})\) is \(X-Z-Y\). With respect to Eq. (3), we have the following:_

1. \(d_{}^{2}(P_{1},P_{2})()\)__
2. \(d_{}(P_{1} P_{1,G_{1}})=0\) _and_ \(d_{}(P_{1} P_{1,G_{2}})()\)__
3. \(d_{}(P_{2} P_{2,G_{2}})=0\) _and_ \(d_{}(P_{2} P_{2,G_{1}})()\)__

Our hardness result (Lemma 14) is obtained by reducing the problem of finding an \(\)-close graph orientation of \(X-Z-Y\) to the problem of _testing_ whether the samples are drawn from \(P_{1}\) or \(P_{2}\): To ensure \(\)-closeness in the graph orientation, one has to correctly determine whether the samples come from \(P_{1}\) or \(P_{2}\) and then pick \(G_{1}\) or \(G_{2}\) respectively. However, it is well-known that distinguishing two distributions whose squared Hellinger distance is \(\) requires \((1/)\) samples (see, e.g., [1, Theorem 4.7]).

**Lemma 14**.: _Even when given \((G^{*})\), it takes \((1/)\) samples to learn an \(\)-close graph orientation of \(G^{*}\) for distributions on \(\{0,1\}^{3}\)._

Using the above construction as a gadget, we can obtain a dependency on \(n\) in our lower bound by constructing \(n/3\) independent copies of the above gadget, a la proof strategy of Bhattacharyya et al. [2021, Theorem 7.6]. For some constant \(c>0\), we know that a constant \(1/c\) fraction of the gadgets will incur an error or more than \(/n\) if less than \(cn/\) samples are used. The desired result then follows from the tensorization of KL divergence, i.e., \(d_{}(_{i}P_{i}_{i}Q_{i})=_{i}d_{ }(P_{i} Q_{i})\).

**Theorem 15**.: _Even when given \((G^{*})\), it takes \((n/)\) samples to learn an \(\)-close graph orientation of \(G^{*}\) for distributions on \(\{0,1\}^{n}\)._

## 6 Conclusion

In this work, we studied the problem of estimating a distribution defined on a \(d\)-polytree \(P\) with graph structure \(G^{*}\) using finite observational samples. We designed and analyzed an efficient algorithm that produces an estimate \(\) such that \(d_{}(P)\) assuming access to \((G^{*})\) and \(d\). The skeleton \((G^{*})\) is recoverable under Assumption 11 and we show that there is an inherent hardness in the learning problem even under the assumption that \((G^{*})\) is given. For constant \(d\), our hardness result shows that our proposed algorithm is sample-optimal up to logarithmic factors.

An interesting open question is whether one can extend the hardness result to arbitrary \(d 1\), or design more efficient learning algorithms for \(d\)-polytrees.