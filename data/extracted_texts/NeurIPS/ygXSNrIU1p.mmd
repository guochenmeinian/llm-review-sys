# Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials

Shengchao Liu1,2, Weitao Du3, Yanjing Li4, Zhuoxinran Li5, Zhiling Zheng6, Chernu Duan7,

Zhiming Ma3, Omar Yaghi6, Anima Anandkumar8, Christian Borgs6,

Jennifer Chayes6, Hongyu Guo9, Jian Tang1,10,11

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

[MISSING_PAGE_POST]

Footnoterepresentation, they facilitate a more robust representation of small molecules, proteins, and crystalline materials. Nevertheless, pursuing geometric learning research is still challenging due to its evolving nature and the knowledge gap between science (_e.g._, physics) and machine learning communities. These factors contribute to a substantial barrier for machine learning researchers to investigate scientific problems and hinder efforts to reproduce results consistently. To overcome this, we introduce Geom3D, a benchmarking of the geometric representation with four advantages, as follows. 1

**(1) A unified and novel aspect in understanding symmetry-informed geometric models.** The molecule geometry needs to satisfy certain physical constraints regarding the 3D Euclidean space. For instance, the molecules' force needs to be equivariant to translation and rotation (see SE(3)-equivariance in Fig. 1). In this work, we classify the geometric methods into three categories: _invariant_ model, SE(3)-equivariant model with _spherical frame basis_ and _vector frame basis_. The invariant models only consider features that are constant w.r.t. the SE(3) group, while the two families of equivariant models can be further unified using the _frame basis_ to capture equivariant symmetry. An illustration of three categories is in Fig. 2. Building equivariant models on the _frame basis_ provides a novel and unified view of understanding geometric models and paves the way for intriguing more ML researchers to explore scientific problems.

**(2) A unified platform for various scientific domains.** There exist multiple platforms and tools for molecule discovery, but they are (1) mainly focusing on molecule's 2D graph representation ; (2) using 3D geometry with customized data structures or APIs ; or (3) covering only a few geometric models . Thus, it is necessary to have a platform benchmarking the geometric models, especially for researchers interested in solving scientific problems. In this work, we propose Geom3D, a geometric modeling framework based on PyTorch Geometric (PyG) , one of the most widely-used platforms for graph representation learning. Geom3D benchmarks 16 geometric models on solving 52 scientific tasks, and these tasks include the three most fundamental molecule types: small molecules, proteins, and crystalline materials. Each of them requires distinct domain-specific preprocessing steps, _e.g._, crystalline materials molecules possess periodic structures and thus need a particular periodic data augmentation. By leveraging such a unified framework, Geom3D serves as a comprehensive benchmarking tool, facilitating effective and consistent analysis components to interpret the existing geometric representation functions in a fair and convenient comparison setting.

**(3) A framework for a wider range of ML tasks.** The geometric models in Geom3D can serve as a building block for exploring extensive ML tasks, including but not limited to studying the molecule dynamic simulation and scrutinizing the transfer learning effect on molecule geometry. For example, pre

Figure 1: Pipeline for Geom3D, including dataset preprocessing, feature extraction, geometric pretraining and representation, and target tasks. We additionally demonstrate the SE(3)-equivariant force prediction task.

Figure 2: Three categories of geometric modules. (a) Invariant models only consider type-0 features. Equivariant models use either (b) spherical harmonics frames or (c) vector frames by projecting the coordinate vectors.

training is an important strategy to quickly transfer knowledge to target tasks, and recent works explore geometric pretraining on 3D conformations (including supervised and self-supervised) [59; 80; 136] and multi-modality pretraining on 2D topology and 3D geometry [30; 79; 86]. Other transfer learning venues include multi-task learning [82; 84] and out-of-distribution or domain adaptation [133; 134; 58], yet no geometry information has been utilized. All of these directions are promising for future exploration, and Geom3D serves as an auxiliary tool to accomplish them. For example, as will be shown in Sec. 4, we leverage Geom3D to effectively evaluate 14 pretraining methods with benchmarks.

**(4) A framework for exploring data preprocessing and optimization tricks.** When comparing different symmetry-informed geometric models, we find that in addition to the model architecture, there are two important factors affecting the performance: the data preprocessing (_e.g._, energy and force rescaling and shift) and optimization methods (_e.g._, learning rate, learning rate schedule, number of epochs, random seeds). In this work, we explore the effect of four preprocessing tricks and around 2-10 optimization hyperparameters for each model and task. In general, we observe that each model may benefit differently in different tasks regarding the preprocessing and optimization tricks. However, data normalization is found to help improve performance hugely in most cases. We believe that Geom3D is an effective tool for exploring and understanding various engineering tricks.

## 2 Data Structures for Geometric Data

**Small molecule 3D conformation.** Molecules are sets of points in the 3D Euclidean space, and they move in a dynamic motion, as known as the potential energy surface (PES). The region with the lowest energy corresponds to the most stable state for molecules, and molecules at these positions are called **conformations**, as illustrated in Fig. 3(b). For notation, we mark each 3D molecular graph as \(=(,)\), where \(\) and \(\) are for the atom types and positions, respectively.

**Crystalline material with periodic structure.** The crystalline materials or extended chemical structures possess a characteristic known as periodicity: their atomic or molecular arrangement repeats in a predictable and consistent pattern across all three spatial dimensions. This is the key aspect that differentiates them from small molecules. In Fig. 3(d), we show an original unit cell (marked in green) that can repeatedly compose the crystal structure along the lattice. To model such a periodic structure, we adopt the data augmentation from CGCNN : for each original unit cell, we shift it along the lattice in three dimensions and connect edges within a cutoff value (hyperparameter). For more details on the two augmentation variants, please check Appendix A.

**Protein with backbone structure.** Protein structures can be classified into four primary levels, and the primary structure represents the linear arrangement of _amino acids_, and each amino acid is a molecule consisting of atoms. Geometric methods mainly focus on the tertiary structure, _i.e._, the 3D geometry of each atom, encompassing the complete organization of a single protein. However,

Figure 3: Fig. 3(a) illustrates 2D topology and 3D conformation for molecule Glycine. Fig. 3(c) displays the 3D structure of protein. Fig. 3(d) shows a simple cubic crystal of the element Po. Fig. 3(b) is a demo of PES.

atom-level modeling for proteins is consuming due to the large volume of atoms and the GPU memory limit. One solution is modeling each amino acid's _backbone structure_. The backbone structure of each amino acid is \(N-C_{}-C\), and the \(C_{}\) is bonded to the side chain. 20 common types of side chains corresponding to 20 amino acids, as illustrated in Fig. 3(c). Thus, modeling the backbone structure can balance the computational efficiency and the key geometric information.

## 3 Symmetry-Informed Geometric Representation

### Group Symmetry and Equivariance

Symmetry means the object remains invariant after certain transformations , and it is everywhere on Earth, such as in animals, plants, and molecules. Formally, the set of all symmetric transformations satisfies the axioms of a group. Therefore, the group theory and its representation theory are common tools to depict such physical symmetry. **Group** is a set \(G\) equipped with a group product \(\) satisfying:

\[(1)\  G,\ =,  G;(2)\ ^{-1}=^{-1}=;(3)\ ()=.\] (1)

**Group representation** is a mapping from the group \(G\) to the group of linear transformations of a vector space \(X\) with dimension \(d\) (see  for more rigorous definition):

\[_{X}():G^{d d}( )=1\ \ _{X}()_{X}()=_{X}(),\ , G.\] (2)

During modeling, the \(X\) space can be the input 3D Euclidean space, the equivariant vector space in the intermediate layers, or the output force space. This enables the definition of equivariance as below.

**Equivariance** is the property for the geometric modeling function \(f:X Y\) as:

\[f(_{X}())=_{Y}()f(),\  G,  X.\] (3)

As displayed in Fig. 1, for molecule geometric modeling, the property should be rotation-equivariant and translation-equivariant (_i.e._, SE(3)-equivariant). More concretely, \(_{X}()\) and \(_{Y}()\) are the SE(3) group representations on the input (_e.g._, atom coordinates) and output space (_e.g._, force space), respectively. SE(3)-equivariant modeling in Eq. (3) is essentially saying that the designed deep learning model \(f\) is modeling the whole transformation trajectory on the molecule conformations, and the output is the transformed \(\) accordingly. Further, we want to highlight that, in addition to the network architecture or representation function, the input features can also be represented as an equivariant feature mapping from the 3D mesh to \(^{}\), where \(\) depends on input data, _e.g._, \(=1\) (for atom type dimension) + 3 (for atom coordinate dimension) on small molecules. Such features are called steerable features in [5; 11] when only considering the subgroup SO(3)-equivariance.

**Invariance** is a special type of equivariance, defined as:

\[f(_{X}())=f(),\ \  G, X,\] (4)

with \(_{Y}()\) as the identity \( G\). The group representation helps define the equivariance condition for \(f\) to follow. Then, the question boils down to how to design such an equivariant \(f\). In the following, we will discuss geometric modelings from a novel and unified perspective using the frame. In the next sections, we will provide a novel and unified aspect of understanding the advanced geometric representation and pretraining methods using the frame basis (details in Appendix H).

Figure 4: Pipelines for seven single-modal geometric pretraining methods. (a-c) conduct self-prediction. (d) maximizes the MI between nodes and graphs. (e-g) are GeoSSL, maximizing the MI between views \(_{1}\) and \(_{2}\).

### Invariant Geometric Representation Learning

One simple way of achieving SE(3) group symmetry is invariant modeling. It means the geometric model only considers the type-0 features , _i.e._, features that are invariant with respect to rotation and translation. Existing works have been adopting the invariant features for modeling, including pairwise distance (SchNet ), bond angles (DimeNet ), and torsion angles (SphereNet  and GemNet ). Note that the torsion angles are angles between two planes defined by pairwise bonds.

### Equivariant Geometric Representation Learning

Invariant modeling only captures the type-0 features. However, equivariant modeling of higher-order particles may bring in extra expressiveness. For example, the elementary particles in high energy physics  inherit higher order symmetries in the sense of SO(3) representation theory, which makes the equivariant modeling necessary. Such higher-order particles include type-1 features like coordinates and forces in molecular conformation. There are many approaches to design such SE(3)-equivariant model satisfying Eq. (3). There are two main venues, as will be discussed below.

**Spherical Frame Basis.** This research line utilizes the irreducible representations  for building SO(3)-equivariant representations, and the first work is IFN . Its main idea is to project the 3D Euclidean coordinates into the spherical harmonics space, which transforms equivariantly according to the irreducible representations of SO(3), and the translation-equivariant can be trivially guaranteed using the relative coordinates. Following this, there have been variants combining it with the attention module (Equiformer ) or with more expressive network architectures (SEGNN , Allegro ).

**Vector Frame Basis.** An alternative philosophy of equivariant modeling utilizes the vector (in physics) frame basis. It constructs three vectors bases, serving as a reference frame to help locate the vectors in each corresponding local environment. Works along this line for molecule discovery include DeePMD  for dynamics simulation, 3D-EMGP  and MoleculeSDE  for geometric pretraining, and ClofNet  for conformation generation. For macromolecules like protein, the equivariant vector frame has been used for protein design (StructTrans ) and protein folding (AlphaFold2 ). We also want to highlight that, from a mathematical perspective, equivariance and invariance can be transformed to each other by the scalarization technique. Please check  for details.

The spherical frame basis can be easily extended to higher-order particles, yet it may suffer from the high computational cost. On the other hand, the vector frame basis is specifically designed for the 3D point clouds; thus, it is more efficient but cannot generalize to higher-order particles. Meanwhile, we would like to acknowledge other equivariant modeling paradigms, including using orbital features  and elevating 3D Euclidean space to SE(3) group [32; 52]. Please check Appendix F for details.

### Geometric Pretraining

Recent studies have started to explore **single-modal of geometric pretraining** on molecules. The GeoSSL paper  covers a wide range of geometric pretraining algorithms. The type prediction, distance prediction, and angle prediction predict the masked atom type, pairwise distance, and bond angle, respectively. The 3D InfoGraph predicts whether the node- and graph-level 3D representation are for the same molecule. GeoSSL is a novel geometric pretraining paradigm that maximizes the mutual information (MI) between the original conformation \(_{1}\) and augmented conformation \(_{2}\), where \(_{2}\) is obtained by adding small perturbations to \(_{1}\). RR, InfoNCE, and EBM-NCE optimize the objective in the latent representation space, either generative or contrastive. GeoSSL-DDM [80; 136] optimizes the same objective function using denoising score matching. 3D-EMGP  has the same strategy and utilizes an equivariant module to denoise the 3D noise directly. Another research line is the **multi-modal of topological and geometric pretraining**. GraphMVP  first proposes one contrastive objective (EBM-NCE) and one generative objective (VRR) to optimize the MI between the 2D topologies and 3D geometries in the representation space. 3D InfoMax  is a special case of GraphMVP, with the contrastive part only. MoleculeSDE  extends GraphMVP by introducing two SDE models for solving the 2D and 3D reconstruction. We illustrate these algorithms in Figs. 4 and 8.

### Discussion: Reflection-antisymmetric in Geometric Learning

Till now, we have discussed the SE(3)-equivariance, _i.e._, the translation and rotation equivariance. As highlighted in the recent work [61; 79], the molecules needlessly satisfy the reflection-equivariant,but instead, they should be reflection-antisymmetric . One classic example is that the energy of small molecules is reflection-antisymmetric in a binding system. Each of the two equivariant tackges discussed in Sec. 3.3 can solve this problem easily. The spherical frame basis can achieve this by adding the reflection into the Wigner-D matrix , and the vector frame basis can accomplish this using the cross-product during frame construction .

## 4 Geometric Datasets and Benchmarks

In Sec. 3, we introduce a novel aspect for understanding symmetry-informed geometric models. In this section, we discuss utilizing Geom3D framework for benchmarking 16 geometric models over 52 tasks. For the detailed dataset acquisitions and task specifications (_e.g._, _dataset size_, _splitting_, and _task unit_), please check Appendix B. Geom3D also covers 7 1D models and 10 2D graph neural networks (GNNs) and benchmarks the 14 pretraining algorithms to learn a robust geometric representation. Additionally, we want to highlight Geom3D enables exploration of important data preprocessing and optimization tricks for performance improvement, as will be introduced next.

### Small Molecules: QM9

QM9  is a dataset consisting of 134K molecules, each with up to 9 heavy atoms. It includes 12 tasks that are related to the quantum properties. For example, U0 and U298 are the internal energies at temperatures of 0K and 298.15K, respectively. On the QM9 dataset, we can easily get the 1D descriptors (Fingerprints/FPs , SMILES , SELFIES ), 2D topology, and 3D conformation. This enables us to build models on each of them respectively: (1) We benchmark 7 models on 1D descriptors, including multi-layer perception (MLP), random forest (RF), XGBoost (SGB), convolution neural networks (CNN), and BERT . (2) We benchmark 10 2D GNN models on the molecular topology, including GCN [23; 66], ENN-S2S , GraphSAGE , GAT , GIN , D-MPNN , PNA , Graphormer , AWARE , GraphGPS . (3) We benchmark 11 3D geometric models on the molecular conformation, including SchNet , DimeNet++ , SE(3)-Trans , EGNN , PaiNN , GemNet-T , SphereNet , SEGNN , Allegro , NequIP , Equformer . The evaluation metric is the mean absolute error.

The results of these 28 models are in Table 1, and two important insights are observed: (1) There is no one universally best geometric model, yet DimeNet++, PaiNN, GemNet, and Equiformer perform well in most tasks. However, PaiNN takes less than 20 GPU hours, and the other three models take up to 5 GPU days per task. (2) The geometric conformation is important for quantum property prediction. The performance of 3D models is better than all the 1D and 2D models _by orders of magnitudes_.

   &  &  & \( E\) & _Sigmoid_ & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  & & \(_{0}^{}\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) \\   & MLP & 2.231 & 196.72 & 131.27 & 164.94 & 0.526 & 0.919 & 2158.64 & 2358.23 & 68.621 & 2340.61 & 2314.77 & 1559.21 \\  & RF & 3.801 & 207.02 & 165.72 & 183.04 & 0.534 & 1.485 & 2391.79 & 279.294 & 944.512 & 2305.76 & 3678.25 & 2531.32 \\  & XGB & 2.748 & 1971.91 & 138.98 & 165.43 & 0.516 & 1.062 & 2653.93 & 2042.87 & 28.599 & 2786.28 & 2769.29 & 1809.89 \\   & CNN & 0.384 & 1652.52 & 124.65 & 114.81 & 0.566 & 0.173 & 1566.76 & 170.59 & 20.403 & 166.18 & 168.69 & 160.070 \\   & BERT & 0.313 & 117.50 & 84.93 & 98.88 & 0.446 & 0.176 & 170.01 & 853.43 & 180.02 & 183.84 & 188.60 & 13.410 \\   & C & 0.345 & 157.04 & 115.55 & 113.00 & 0.490 & 0.168 & 136.32 & 1465.56 & 20.080 & 143.00 & 140.40 & 101.049 \\  & BERT & 0.348 & 123.11 & 91.15 & 90.80 & 0.461 & 0.203 & 168.20 & 187.50 & 19.125 & 204.93 & 195.98 & 17.328 \\   & GCN & 1.338 & 145.82 & 96.21 & 106.66 & 0.434 & 0.526 & 1198.12 & 1291.57 & 37.585 & 1281.03 & 1303.38 & 158.03 \\  & ENN-S2S & 1.401 & 205.99 & 128.13 & 324.87 & 0.577 & 1060.47 & 4372.11 & 595.524 & 34.609 & 1800.79 & 1521.32 & 51.266 \\  & GraphSAGE & 1.601 & 131.45 & 88.78 & 93.21 & 0.402 & 0.544 & 1473.42 & 1617.73 & 38.112 & 1553.01 & 1566.55 & 95.344 \\  & GAT & 1.1325 & 139.50 & 94.70 & 98.52 & 0.406 & 0.291 & 911.82 & 993.11 & 165.03 & 1129.52 & 952.67 & 55.061 \\  & GIN & 1.165 & 175.82 & 90.66 & 110.74 & 0.539 & 0.691 & 848.20 & 1093.56 & 35.110 & 1498.32 & 3614.18 & 108.331 \\  & D-MPNN & 0.568 & 118.42 & 85.01 & 86.20 & 0.441 & 0.423 & 243.43 & 458.49 & 2416.70 & 401.459 & 28.291 \\  & PNA & 0.681 & 148.48 & 88.72 & 97.31 & 0.361 & 0.409 & 646.49 & 679.24 & 23.855 & 616.04 & 694.92 & 57.217 \\  & Graphmer & 2.836 & 70.52 & 54.24 & 52.42 & 0.343 & 0.308 & 0.206 & 286.26 & 3540.16 & 115.285 & 228.52 & 144.545 \\  & AWARE & 0.297 & 144.19 & 133.89 & 98.86 & 0.602 & 0.129 & 86.62 & 94.47 & 22.180 & 93.35 & 95.73 & 5.275 \\  & GraphGPS & 0.209 & 75.98 & 54.75 & 54.53 & 0.288 & 0.089 & 528.50 & 693.19 & 12.488 & 296.60 & 411.16 & 49.888 \\   & SchNet & 0.060 & 44.13 & 27.64 & 22.55 & 0.028 & 0.031 & 14.19 & 14.05 & 0.133 & 13.93 & 13.27 & 1.749 \\  & DimeNet & 0.044 & 362.22 & 20.01 & 16.66 & 0.028 & **0.022** & **7.45** & **6.14** & 0.323 & **6.33** & 7.18 & **1.118** \\  & SE(3)-Trans & 0.137 & 56.52 & 34.36 & 34.41 & 0.500 & 0.63 & 65.28 & 70.00 & 1.

[MISSING_PAGE_FAIL:7]

### Small Molecules & Proteins Binding: LBA & LEP

The binding affinity measures the strength of the binding interaction between a small molecule (ligand) to the target protein. In Geom3D, we consider modeling both the ligands and proteins with their 3D structures. During binding, a cavity in a protein can potentially possess suitable properties for binding a small molecule, and it is called a pocket . Due to the large volume of protein, Geom3D follows existing works  by only taking the binding pocket instead of the whole protein structure. Specifically, Geom3D models up to 600 atoms for each ligand and protein pair. For the benchmarking, we consider two binding affinity tasks. (1) The first task is ligand binding affinity (LBA) . It is gathered from , and the task is to predict the binding affinity strength between a ligand and a protein pocket. (2) The second task is ligand efficacy prediction (LEP) . The input is a ligand and both the active and inactive conformers of a protein, and the goal is to classify whether or not the ligand can activate the protein's function. The results on two binding tasks are in Table 4, and we can observe that PaiNN, SEGNN, and Equiformer are generally outstanding on the two tasks.

### Proteins: ECSingle, ECMultiple, Fold, GO, MSP, and PSR

**ECSingle** is a classification task  that classifies 37K proteins into 384 four-level Enzyme Commission (EC) types. This task aims to recognize the fundamental role of proteins as bio-catalysts or enzymes, which are essential in facilitating biological reactions. The EC numbering system  serves as a comprehensive numerical classification scheme, systematically organizing the varied functionalities of enzymes and providing a structured approach to understanding their biological roles.

**ECMultiple** is a multi-label classification task proposed by Gligorijevic et al. , where 19K proteins are associated with 538 distinct EC categories, including both three-level and four-level types and a single protein can be concurrently labeled with several three-level or four-level EC numbers.

**Fold** is a task classifying 16K proteins into 1,195 fold patterns [47; 74]. It is an important biological task in predicting the 3D structures from 1D amino acid sequences. We further consider three testsets (Fold, Superfamily, and Family) based on the sequence and structure similarity .

**GO** (Gene Ontology) is a dataset  with 36K proteins for GO term classification, where the GO term provides a consistent description of gene product attributes across species and databases . Concretely, each protein contains up to three types of GO terms, corresponding to three types of classification tasks: (1) Molecular Function (MF) has 489 classes; (2) Biological Process (BP) has 1,943

    &  &  \\   & RMSD \(\) & \(R_{P}\) & \(R_{C}\) & ROC \(\) & PR \(\) \\  SchNet & 1.521 \(\) 0.02 & 0.474 \(\) 0.01 & 0.452 \(\) 0.01 & 0.450 \(\) 0.03 & 0.379 \(\) 0.03 \\ DimeNet++ & 1.672 \(\) 0.09 & 0.550 \(\) 0.01 & 0.556 \(\) 0.01 & 0.509 \(\) 0.06 & 0.496 \(\) 0.05 \\ EGNN & 1.494 \(\) 0.04 & 0.503 \(\) 0.04 & 0.483 \(\) 0.05 & **0.657 \(\) 0.05** & **0.559 \(\) 0.05** \\ PaiNN & **1.434 \(\) 0.02** & 0.583 \(\) 0.02 & **0.580 \(\) 0.02** & 0.585 \(\) 0.02 & 0.432 \(\) 0.03 \\ GemNet-T & – & – & – & – & 0.659 \(\) 0.05 & 0.506 \(\) 0.05 \\ SphreNet & 1.581 \(\) 0.02 & 0.538 \(\) 0.01 & 0.529 \(\) 0.01 & 0.523 \(\) 0.04 & 0.432 \(\) 0.05 \\ SEGNN & 1.416 \(\) 0.03 & 0.566 \(\) 0.02 & 0.550 \(\) 0.02 & 0.574 \(\) 0.03 & 0.485 \(\) 0.03 \\ Nequip & 1.606 \(\) 0.02 & 0.537 \(\) 0.01 & 0.520 \(\) 0.01 & 0.538 \(\) 0.12 & 0.481 \(\) 0.07 \\ Allegro & 1.567 \(\) 0.02 & 0.547 \(\) 0.00 & 0.534 \(\) 0.00 & 0.627 \(\) 0.04 & 0.525 \(\) 0.03 \\ Equiformer & 1.392 \(\) 0.03 & **0.598 \(\) 0.02** & 0.578 \(\) 0.02 & 0.618 \(\) 0.06 & 0.510 \(\) 0.05 \\   

Table 4: Results on 2 binding affinity prediction tasks. We select three evaluation metrics for LBA: the root mean squared error (RMSD), the Pearson correlation (\(R_{p}\)) and the Spearman correlation (\(R_{S}\)). LEP is a binary classification task, and we use the area under the curve for receiver operating characteristics (ROC) and precision-recall (PR) for evaluation. We run cross-validation with 5 seeds, and the mean and std are reported.

    &  &  &  &  &  \\    & & Fold & Sup. & Fam. & MF & BP & CC & & \\    & ACC \(\) & \(F_{max}\) & ACC \(\) & ACC \(\) & \(F_{max}\) & \(F_{max}\) & \(F_{max}\) & ROC \(\) & Global \(\) & Mean \(\) \\  IEConv & – & – & 45.0 & 69.7 & 98.9 & – & – & – & – & – & – \\ GVP-GNN & 65.5 & 0.712 & 34.8 & 52.7 & 95.0 & 0.476 & 0.312 & 0.389 & 0.574 & 0.744 & 0.302 \\ GearNet & 78.8 & 0.799 & 29.1 & 43.1 & 95.9 & 0.477 & 0.283 & 0.373 & – & – & – \\ ProNet & 86.4 & 0.823 & 52.7 & 70.3 & 99.3 & 0.559 & 0.367 & 0.414 & 0.634 & **0.818** & 0.462 \\ CDConv & **86.9** & **0.862** & **60.0** & **79.9** & **99.5** & **0.649** & **0.435** & **0.450** & **0.717** & 0.817 & **0.500** \\   

Table 5: Results on 10 protein tasks from six datasets: ECSingle, ECMultiple, Fold (Fold, Sup., Fam.), GO (MF, BP, CC), MSP, and PSR. The evaluation metrics are Accuracy (ACC, %), \(F_{max}\) (definition in Appendix B), ACC, \(F_{max}\), receiver operating characteristics (ROC), and Spearman’s \(\), respectively.

classes; and (3) Cellular Component (CC) has 320 classes. Notice that each protein can be associated with multiple GO terms in each GO term type, thus all three tasks are multi-label classifications.

**MSP & PSR** are two protein tasks from a collection of benchmark datasets for machine learning in structural biology . MSP (Mutation Stability Prediction) aims to predict whether the stability of a protein increases after mutation. The dataset is a mutation dataset containing 4K proteins. It is constructed by incorporating single-point mutations given in the SKEMPI database . PSR (Protein Structure Ranking) is a regression task based on the Critical Assessment of Structure Prediction (CASP) . In CASP, a protein structure is predicted and a quality score, the global distance test (GDT_TS), is calculated between the predicted structure and experimentally determined structure. This task aims to predict this score for 44K proteins.

The results of 5 models are in Table 5. CDConv  outperforms other models by a large margin on almost all 10 tasks, while ProNet  performs second well in general, and reaches the best result on the PSR task with global \(\) metric. Notice that certain entries in the table are temporarily left blank due to memory constraints encountered. More detailed dataset specifications are in Appendix B.

### Crystalline Materials: MatBench and QMOF

**MatBench** is explicitly created to evaluate the performance of machine learning models in predicting properties of inorganic bulk materials covering mechanical, electronic, and thermodynamic material properties . Here we consider 8 regression tasks with crystal structures, including predicting the formation energy (Perovskites, \(E_{}\)), exfoliation energies (\(E_{}\)), band gap, shear and bulk modulus (\(log_{10}G\) and \(log_{10}K\)), etc. Please check Appendix B for more details.

**Quantum MOF (QMOF)** is a dataset of over 20K metal-organic frameworks (MOFs) and coordination polymers derived from DFT. The task is to predict the band gap, the energy gap between the valence band and the conduction band. The results of 8 geometric models on 8 MatBench tasks and 1 QMOF task are in Table 6, and we can observe that the performance of all the models is very close, while DimeNet++, PaiNN, GemNet-T, and Equiformer are slightly better.

We also conduct **ablation study on periodic data augmentation** on crystal materials. We note that there are two data augmentation (DA) methods: gathered and expanded. Gathered DA means that we shift the original unit cell along three dimensions, and the translated unit cells will have the _same_ node indices as the original unit cell, _i.e._, a multi-edge graph. However, expanded DA will assume the translated unit cells have different node indices from the original unit cell. (A visual demonstration is in Appendix A). We conduct an ablation study on the effect of these two DAs, and we plot MAE(expanded DA) - MAE(gathered DA) on six tasks in Fig. 6. It reveals that for most of the models (except EGNN), using gathered DA can lead to consistently better performance, and thus it is preferred. For more qualitative analysis, please check Appendix J.

    &  &  \\   & Per. \(E_{}\) & Dielectric \(\) & \(log_{10}G\) & \(log_{10}K\) & \(E_{}\) & Phonoms \(\) & Band Gap \(\) & \(E_{}\) & Band Gap \(\) \\  & 18.928 & 4,764 & 10,987 & 10,987 & 636 & 1,265 & 106,113 & 132,752 & 20.425 \\  SchNet & 0.040 & 0.334 & 0.081 & 0.060 & 65.201 & 42.586 & 0.327 & 0.026 & 0.236 \\ DimeNet++ & **0.037** & 0.357 & 0.081 & 0.058 & 68.685 & 38.339 & **0.208** & 0.025 & 0.234 \\ EGNN & 0.038 & 0.331 & 0.087 & 0.064 & 78.015 & 74.846 & 0.211 & 0.026 & 0.256 \\ PaiNN & 0.038 & 0.317 & **0.080** & **0.053** & 67.752 & 44.602 & **0.022** & 0.190 & 0.207 \\ GenNet-T & 0.042 & 0.325 & 0.088 & 0.061 & 68.425 & 48.986 & 0.186 & 0.026 & **0.207** \\ SphereNet & 0.043 & 0.388 & 0.087 & 0.061 & 72.987 & 36.300 & 0.217 & 0.029 & 0.251 \\ SEGNN & 0.046 & 0.360 & 0.087 & 0.059 & 65.052 & 43.638 & 0.330 & 0.047 & 0.330 \\ Equiformer & 0.046 & **0.280** & 0.087 & 0.057 & **62.977** & **37.381** & 0.202 & 0.027 & 0.234 \\   

Table 6: Results on the 8 tasks from MatBench and 1 task from QMOF (with optimal DA). The data split and task unit are in Appendix B, and the metric is the mean absolute error (MAE).

Figure 6: Ablation study on the performance gap with data augmentation (DA): MAE(expanded DA) - MAE(gathered DA).

[MISSING_PAGE_FAIL:10]