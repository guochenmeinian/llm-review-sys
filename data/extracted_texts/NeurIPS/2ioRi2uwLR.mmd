# Neuro-symbolic Learning Yielding Logical Constraints

Zenan Li\({}^{1}\) Yunpeng Huang\({}^{1}\) Zhaoyu Li\({}^{2}\)

Yuan Yao\({}^{1}\) Jingwei Xu\({}^{1}\) Taolue Chen\({}^{3}\) Xiaoxing Ma\({}^{1}\) Jian Lu\({}^{1}\)

\({}^{1}\)State Key Lab of Novel Software Technology, Nanjing University, China

\({}^{2}\)Department of Computer Science, University of Toronto, Canada

\({}^{3}\)School of Computing and Mathematical Sciences, Birkbeck, University of London, UK

{lizn, hyp}@smail.nju.edu.cn, zhaoyu@cs.toronto.edu,

t.chen@bbk.ac.uk, ty.yao,jingweix,xxm,lj}@nju.edu.cn

###### Abstract

Neuro-symbolic systems combine neural perception and logical reasoning, representing one of the priorities of AI research. End-to-end learning of neuro-symbolic systems is highly desirable, but remains to be challenging. Resembling the distinction and cooperation between System 1 and System 2 of human thought (a la Kahneman), this paper proposes a framework that fuses neural network training, symbol grounding, and logical constraint synthesis to support learning in a weakly supervised setting. Technically, it is cast as a game with two optimization problems which correspond to neural network learning and symbolic constraint learning respectively. Such a formulation naturally embeds symbol grounding and enables the interaction between the neural and the symbolic part in both training and inference. The logical constraints are represented as cardinality constraints, and we use the trust region method to avoid degeneracy in learning. A distinguished feature of the optimization lies in the Boolean constraints for which we introduce a difference-of-convex programming approach. Both theoretical analysis and empirical evaluations substantiate the effectiveness of the proposed framework.

## 1 Introduction

Perception and reasoning serve as fundamental human abilities that are intrinsically linked within the realm of human intelligence (Kahneman, 2011; Booch et al., 2021). The objective of our study is to develop a learning framework for neuro-symbolic systems (e.g., the one illustrated in Figure 1), enabling simultaneous learning of neural perception and symbolic reasoning.

The merit of neuro-symbolic learning lies in resembling the integration of System 1 and System 2 of human minds (Kahneman, 2011; Yoshua and Gary, 2020; LeCun, 2022). First, it eliminates unnatural and sometimes costly human labeling of the latent variables and conducts learning in an end-to-end fashion. Second, it generates not only a neural network for perception, but also a set of explicit (symbolic) constraints enabling exact and interpretable logical reasoning. Last but not least, the mutually beneficial interaction between the neural and the symbolic parts during both training and inference stages potentially achieves better performance than separated learning approaches.

However, existing approaches do not provide an adequate solution to the problem. They either (1) are not end-to-end, i.e., human intervention is employed to label the latent \(\) so that the task can be divided into a purely neural subtask of image classification and a purely symbolic subtask of constraint solving, or (2) are not interpretable, i.e., can only approximate symbolic reasoning with neural network but without explicit logical constraints generated (e.g., Wang et al. (2019); Yang et al. (2023)), which inevitably sacrifices the exactness and interpretability of symbolic reasoning, resulting in an inaccurate black-box predictor but not a genuine neuro-symbolic system.

We argue that end-to-end and interpretable neuro-symbolic learning is extremely challenging due to the _semantic and representation gaps_ between the neural part and the symbolic part. The semantic gap caused by the latency of the intermediate symbol (i.e., \(\) in Figure 1) makes the neural network training lack effective supervision and the logic constraint synthesis lack definite inputs. The representation gap between the differentiable neural network and the discrete symbol logic makes it difficult to yield explicit symbolic constraints given the continuous neural network parameters. Despite existing proposals mitigating some of these obstacles such as visual symbol grounding (Topan et al., 2021; Li et al., 2023), softened logic loss (Kimmig et al., 2012; Xu et al., 2018), semidefinite relaxation (Wang et al., 2019), etc., none of them realize the full merit of neuro-symbolic learning.

In this paper, we propose a new neuro-symbolic learning framework directly meeting the challenges. It bridges the semantic gap with an efficient symbol grounding mechanism that models the cooperative learning of both the neural network and the logical constraint as a bilevel optimization problem. It bridges the representation gap by employing difference-of-convex (DC) programming as a relaxation technique for Boolean constraints in the optimization. DC programming ensures the convergence to explicit logical constraints, which enables exact symbolic reasoning with powerful off-the-shell tools such as SAT/SMT solvers (Een and Sorensson, 2006; Bailleux and Bourkhad, 2003; Bailleux et al., 2006) during the inference stage. In addition, to address degeneracy in logical constraint learning, i.e., the tendency to learn only trivial logical constraints (e.g., resulting in simple rules insufficient to solve SudoKu), we introduce an additional trust region term (Boyd et al., 2004; Conn et al., 2000), and then employ the proximal point algorithm in the learning of logical constraints.

We provide a theoretical analysis of the convergence of our algorithm, as well as the efficacy of the DC relaxation in preserving the exactness and the trust region in preventing degeneracy. Empirical evaluations with four tasks, viz. Visual Sudoku Solving, Self-Driving Path Planning, Chained XOR, and Nonograms, demonstrate the new learning capability and the significant performance superiority of the proposed framework.

_Organization._ Section 2 formulates our neuro-symbolic learning framework. Section 3 details the algorithm and theoretical analysis. Section 4 presents empirical evaluations. Section 5 covers related work. Section 6 discusses the limitations. Section 7 concludes the paper.

## 2 Neuro-symbolic Learning Framework

In this paper, we focus on end-to-end neuro-symbolic systems comprising two components: (1) neural network \(f_{}\), which transforms the raw input \(\) into a latent state \(\); and (2) symbolic reasoning \(g_{}\), which deduces the final output \(\) from state \(\). Both components are built simultaneously, taking only the input \(\) and output \(\) as supervision. We assume that \(\) and \(\) are represented by finite sets, and thus we can encode \(\) and \(\) by binary vectors using

Figure 1: An example of neuro-symbolic learning for visual SudoKu solving. In this task, the neural network is employed to transform the puzzle image (strawberry etc.) into its corresponding symbols, while symbolic reasoning is utilized to produce the puzzleâ€™s solution. Importantly, the neuro-symbolic learning task is framed in a _weakly supervised_ setting, where only the raw input (the puzzle image \(\)) and the final output (the puzzle solution \(\), but without numbers in \(\)) is observed.

one-hot encoding. For ease of discussion, we directly define \(\) and \(\) to be spaces \(^{u}\) and \(^{v}\) of Boolean vectors, respectively, where \(=\{0,1\}\).

Unlike existing work (e.g., SATNet (Wang et al., 2019)) that simulates logical reasoning in an _implicit_ and _approximate_ way via a network layer, our key insight is to learn _explicit_ logical constraints \(h_{}^{u+v}\) on \((,)\) in the training phrase, which allow to perform _exact_ reasoning by off-the-shelf constraint solvers (e.g., SMT solvers) in the inference phrase. Fig. 1 illustrates our neuro-symbolic learning framework.

The latent state \(\) enables the interaction between neural perception and logical reasoning. If \(\) were observable, we could perform the learning of neural network and logical constraint by solving the following two separate optimization problems:

\[=_{}_{(,) _{1}}[_{1}(f_{}(),)],=_{}_{(,)_ {2}}[_{2}(h_{}(,),1)],\]

where \(_{1}(f_{}(),)\) refers to the error between network prediction \(f_{}()\) and the actual symbol \(\), and \(_{2}(h_{}(,),1)\) refers to the (un)satisfaction degree of the learned logical constraints \(h_{}(,)=1\).

Nevertheless, with \(\) being latent, these two problems become tightly coupled:

\[=_{}_{(,) }[_{1}(f_{}(),)], =_{}_{(,)}[_ {2}(h_{}(,),1)],\]

We further surrogate the constraint satisfaction by loss functions, and obtain essentially a game formulation as follows:

\[=_{}_{(,) }[_{1}(f_{}(),})], =_{}_{(, )}[_{2}(h_{}(}, ),1)],\] (1) s.t. \[}=_{}_{2}(h_{ }(,),1);}=_{}_{1}(f_{}( ),).\]

Three players are involved in this game: neural network \(f_{}\) and logical constraint \(h_{}\) pursue optimal (prediction/satisfaction) accuracy, while \(\) strives for the grounding that integrates the network prediction and logical reasoning.

### Efficient and Effective Logical Constraint Learning

For _efficient_ learning of logical constraints, we adopt cardinality constraint (Syrjanen, 2009, 2004; Fiorini et al., 2021) to represent logical constraint.1Cardinality constraints can be easily arithmetized, enabling the conventional optimization method and avoiding the computationally expensive model counting or sampling (Manhaeve et al., 2018; Xu et al., 2018; Li et al., 2020, 2023), which significantly boosts the learning efficiency. In addition, the conjunctive normal form and cardinality constraints can be easily converted from each other, ensuring not only the expressiveness of the learned constraints, but also the seamless compatibility with existing reasoning engines.

Formally, we denote the column concatenation of \(\) and \(\) by \((;)\), and define a cardinality constraint \(h_{}(,):=^{}(;)[b_{},b_{}]\), where \(=(,b_{},b_{})\), \(^{u+v}\) is a Boolean vector, and \(b_{},b_{}_{+}\) are two positive integers. Moreover, we can directly extend \(h_{}\) to the matrix form representing \(m\) logical constraints, i.e.,

\[h_{}(,):=(;)=(_{1 }^{}(;),,_{m}^{}(;))[_{},_{}],\]

where \(=(,_{},_{})\), \(:=(_{1}^{};;_{m}^{}) ^{m(u+v)}\), and \(_{},_{}_{+}^{m}\).

For _effective_ learning of logical constraints, one must tackle the frequent problem of _degeneracy_ that causes incomplete or trivial constraints2. First, for the bounding box \([_{},_{}]\), any of its superset is a feasible result of logical constraint learning, but we actually expect the tightest one. To overcome this difficulty, we propose to use the classic mean squared loss, i.e., defining \(_{2}(h_{}(,),1)=\|(;)- \|^{2}\), where \(\) can be computed in optimization or pre-defined. Then, our logical constraint learning problem can be formulated as a Boolean least squares problem (Vandenberghe and Boyd, 2004). The unbiased property of least squares indicates that \((_{}+_{})/2\), and the minimum variance property of least squares ensures that we can achieve a tight bounding box \([_{},_{}]\)(Henderson, 1975; Bjorck, 1990; Hastie et al., 2009).

Second, it is highly possible that some of the \(m\) distinct logical constraints as indicated by matrix \(\) eventually degenerate to the same one during training, because the Boolean constraints and stochastic gradient descent often introduce some implicit bias (Gunasekar et al., 2017; Smith et al., 2021; Ali et al., 2020). To mitigate this problem, we adopt the trust region method (Boyd et al., 2004; Conn et al., 2000), i.e., adding constraints \(\|_{i}-_{i}^{(0)}\|,i=1,,m\), where \(_{i}^{(0)}\) is a pre-defined centre point of the trust region. The trust region method enforces each \(_{i}\) to search in their own local region. We give an illustrative figure in Appendix D to further explain the trust region method.

To summarize, using the penalty instead of the trust region constraint, we can formulate the optimization problem of logical constraint learning in (1) as

\[_{(,)}&_{( ,)}[\|(};)- \|^{2}]+\|-^{(0)}\|^{2},\\ &}=_{ }_{1}(f_{}(),), ^{m(u+v)},_{+}^{m}.\] (2)

### Neural Network Learning in Tandem with Constraint Learning

A key challenge underlines end-to-end neuro-symbolic learning is _symbol grounding_, which is to tackle the chicken-and-egg situation between network training and logical constraint learning: training the network requires the supervision of symbol \(\) that comes from solving the learned logical constraints, but the constraint learning needs \(\) as input recognized by the trained network. Specifically, since matrix \(\) is often underdetermined in high-dimensional cases, the constraint \(}=_{}_{2}(h_{}( ,),1):=\|(;)-\|^{2}\) often has multiple minimizers (i.e., multiple feasible groundings of \(\)), all of which satisfy the logical constraints \(h_{}(,)=1\). Moreover, matrix \(\) is also a to-be-trained parameter, meaning that it is highly risky to determine the symbol grounding solely on the logical constraints.

To address these issues, instead of (approximately) enumerating all the feasible solutions via model counting or sampling (Manhaeve et al., 2018; Xu et al., 2018; Li et al., 2020; van Krieken et al., 2022; Li et al., 2023), we directly combine network prediction and logical constraint satisfaction to establish symbol grounding, owing to the flexibility provided by the cardinality constraints. Specifically, for given \([0,+)\), the constraint in network learning can be rewritten as follows,

\[}=_{}\|(; )-\|^{2}+\|-f_{}()\|^{2}.\]

The coefficient \(\) can be interpreted as the preference of symbolic grounding for network predictions or logical constraints. For \( 0\), the symbol grounding process can be interpreted as distinguishing the final symbol \(\) from all feasible solutions based on network predictions. For \(+\), the symbol grounding process can be viewed as a "correction" step, where we revise the symbol grounding from network's prediction towards logical constraints. Furthermore, as we will show in Theorem 1 later, both symbol grounding strategies can finally converge to the expected results.

The optimization problem of network training in (1) can be written as

\[_{}&_{( ,)}[\|}-f_{}( )\|^{2}],\\ &}=_{ }\ \|(};)-\|^{2}+\|} -f_{}()\|^{2}.\] (3)

where we also use the mean squared loss, i.e., \(_{1}(f_{}(),)=\|f_{}( )-\|^{2}\), for compatibility.

## 3 Algorithms and Analysis

Our general framework is given by (1), instantiated by (2) and (3). Both optimizations contain Boolean constraints of the form \(\|-_{1}\|^{2}+\|-_{2}\|^{2}\) where \(\) are Boolean variables.3 We propose to relax these Boolean constraints by _difference of convex_ (DC) programming (Tao and Hoai An, 1997; Yuille and Rangarajan, 2003; Lipp and Boyd, 2016; Hoai An and Tao, 2018). Specifically, a Boolean constraint \(u\{0,1\}\) can be rewritten into two constraints of \(u-u^{2} 0\) and \(u-u^{2} 0\). The first constraint is essentially a box constraint, i.e., \(u\), which is kept in the optimization. The second one is concave, and we can _equivalently_ add it as a penalty term, as indicated by the following proposition (Bertsekas, 2015; Hansen et al., 1993; Le Thi and Ding Tao, 2001).

**Proposition 1**.: _Let \(\) denote the all-one vector. There exists \(t_{0} 0\) such that for every \(t>t_{0}\), the following two problems are equivalent, i.e., they have the same optimum._

\[(P) _{\{0,1\}^{n}}q():=\|-_{1}\|^ {2}+\|-_{2}\|^{2},\] \[(P_{t}) _{^{n}}q^{t}():=\|-_{1} \|^{2}+\|-_{2}\|^{2}+t(^{}-^{ }).\]

_Remarks._ We provide more details, including the setting of \(t_{0}\), in Appendix A.

However, adding this penalty term causes non-convexity. Thus, DC programming further linearizes the penalty \(u-u^{2}-^{2}+(u-)(1-2)\) at the given point \(\), and formulates the problem in Proposition 1 as

\[_{^{n}}\|-_{1}\|^{2}+\|-_{ 2}\|^{2}+t(-2})^{}.\]

By applying this linearization, we achieve a successive convex approximation to the Boolean constraint (Razaviyayn, 2014), ensuring that the training is more stable and globally convergent (Lipp and Boyd, 2016). Furthermore, instead of fixing the coefficient \(t\), we propose to gradually increase it until the Boolean constraint is fully satisfied, forming an "annealing" procedure. We illustrate the necessity of this strategy in the following proposition (Beck and Teboulle, 2000; Xia, 2009).

**Proposition 2**.: _A solution \(\{0,1\}^{n}\) is a stationary point of (P\({}_{t}\)) if and only if_

\[[ q()]_{i}(1-2_{i})+t 0, i=1,,n.\]

_Then, if \(\{0,1\}^{n}\) is a global optimum of (P) (as well as (P\({}_{t}\))), it holds that_

\[[ q()]_{i}(1-2_{i})+_{i} 0, i=1,,n,\]

_where \(_{i}\) is the i-th diagonal element of \((^{}+)\)._

_Remarks._ The proposition reveals a trade-off of \(t\): a larger \(t\) encourages exploration of more stationary points satisfying the Boolean constraints, but a too large \(t\) may cause the converged point to deviate from the optimality of (P). Therefore, a gradual increase of \(t\) is sensible for obtaining the desired solution. Moreover, the initial minimization under small \(t\) results in a small gradient value (i.e., \(|[ q()]_{i}|\)), thus a Boolean stationary point can be quickly achieved with a few steps of increasing \(t\).

### Algorithms

For a given dataset \(\{(_{i},_{i})\}_{i=1}^{N}\), \(=(_{1},,_{N})\) and \(=(_{1},,_{N})\) represent the data matrix and label matrix respectively, and \(f_{^{(k)}}()\) denotes the network prediction at the \(k\)-th iteration.

**Logical constraint learning.** Eliminating the constraint in (2) by letting \(=f_{^{(k)}}()\), the empirical version of the logical constraint learning problem at the (\(k\)+1)-th iteration is

\[_{(,)}_{i=1}^{m}\|(f_{^{(k)}}(); )_{i}-_{i}\|^{2}+\|_{i}-_{i}^{(0)}\|^ {2}+t_{1}(-2_{i}^{(k)})^{}_{i},\]

where \(_{1}^{(k)},,_{m}^{(k)}\) are parameters of logical constraints at the \(k\)-th iteration. In this objective function, the first term is the training loss of logical constraint learning, the second term is the trust region penalty to avoid degeneracy, and the last term is the DC penalty of the Boolean constraint.

To solve this problem, we adopt the proximal point algorithm (PPA) (Rockafellar, 1976; Parikh et al., 2014; Rockafellar, 2021), as it overcomes two challenges posed by stochastic gradient descent. First, stochastic gradient descent has an implicit inductive bias (Gunasekar et al., 2017; Ali et al., 2020; Zhang et al., 2021; Smith et al., 2021), causing different \(_{i},i=1,,m\), to converge to a singleton. Second, the data matrix \((f_{^{(k)}}();)\) is a \(0\)-\(1\) matrix and often ill-conditioned, leading to diverse or slow convergence rates of stochastic gradient descent.

Given \((^{(k)},^{(k)})\) at the \(k\)-th iteration, the update of PPA can be computed by

\[^{(k+1)}=(+(+))^{-1}(f_{^{(k)}}();)^{}^{(k)}+^{ (0)}+(+t_{1})^{(k-1)}-}{2},\] \[^{(k+1)}=(1+)^{-1}(^{(k)}(f_{^{(k)}}();)^{}+^{(k)}),\] (4)

where \(>0\) is the step size of PPA, \(\) is an all-ones matrix, and \(=(f_{^{(k)}}();)^{}(f_{^{(k)}}();)\). Note that the computation of matrix inverse is required, but it is not an issue because \(\) is positive semidefinite, and so is the involved matrix, which allows the use of Cholesky decomposition to compute the inverse. Moreover, exploiting the low rank and the sparsity of \(\) can significantly enhance the efficiency of the computation.

**Neural network training.** By adding the DC penalty, the constraint (i.e., symbol grounding) in (3) is

\[}=\|(};)-\|^{2}+ \|}-f_{^{(k)}}()\|^{2}+t_{2}( -2(}^{(k)})^{})}.\]

We can also compute the closed-form solution, i.e.,

\[}=(+(+t_{2})}^{(k)}+  f_{^{(k)}}()-}{2})( ^{}+)^{-1}.\] (5)

Similarly, the low-rank and sparsity properties of \(^{}\) ensure an efficient computation of matrix inverse. Finally, the parameter \(\) of network is updated by stochastic gradient descent,

\[^{(k+1)}=^{(k)}-_{}f_{^ {(k)}}(_{i})_{i=1}^{N}(}_{i}-f_{^{(k) }}(_{i})).\] (6)

The overall algorithm is summarized in Algorithm 1, which mainly involves three iterative steps: (1) update the logical constraints by combining the network prediction and observed output; (2) correct the symbol grounding by revising the prediction to satisfy logical constraints; (3) update the neural network by back-propagating the corrected symbol grounding with observed input.

```  Set step sizes \((,)\), and penalty coefficients \((,t_{1},t_{2})\).  Randomly generate an initial matrix \(^{(0)}\) under \(\) uniform distribution. for\(k=0,1,,K\)do  Randomly draw a batch \(\{(_{i},_{i})\}_{i=1}^{N}\) from training data.  Compute the predicted symbol \(_{i}=f_{^{(k)}}(_{i}),i=1,,N\). \(\)Network prediction  Update \((,)\) from \((_{i},_{i}),i=1,,m\), by PPA update (Eq. 4). \(\)Constraint learning  Correct the symbol grounding \(}\) from \(_{i}\) to logical constraints \(h_{}\) (Eq. 5). \(\)Symbol grounding  Update \(\) from \((_{i},}_{i}),i=1,,m\), by SGD update (Eq. 6). \(\)Network training if\(^{m(u+v)}/}^{N (u+v)}\)then  Increasing \(t_{1}/t_{2}\). \(\)Enforcing DC penalty endif  Estimate \((_{},_{})\) based on network \(f_{}\) and logical constraints \(h_{}\) from training data. ```

**Algorithm 1** Neuro-symbolic Learning Procedure

### Theoretical Analysis

**Theorem 1**.: _With an increasing (or decreasing) \(\), the constraint learning and network training performed by Algorithm 1 converge to the stationary point of (2) and (3), respectively. Specifically, it satisfies_

\[[\|_{}_{1}^{k}(^{k})\|^{2}]=(}), and[\|_{}_{2}(^{k})\|^{2}]= (}).\]

_Remarks_.: The proof and additional results can be found in Appendix B. In summary, Theorem 1 confirms the convergence of our neuro-symbolic learning framework and illustrates its theoretical complexity. Furthermore, note that an increase (or decrease) in \(\) indicates a preference for correcting the symbol grounding over network prediction (or logical constraint learning). In practice, we can directly set a small (or large) enough \(\) instead.

Next, we analyze the setting of centre points \(^{(0)}\) in the trust region penalty as follows.

**Theorem 2**.: _Let \(_{1}^{(0)}^{n}\) and \(_{2}^{(0)}^{n}\) be two initial points sampled from the uniform distribution. For given \(t 0\), the probability that the corresponding logical constraints \(_{1}\) and \(_{2}\) converge to the same (binary) stationary point \(\) satisfies_

\[(_{1}=,_{2}=)_{i=1}^{n} ([ q()]_{i}(1-2_{i})+t),1}^ {2}.\]

_Remarks._ The proof is in Appendix C. In a nutshell, Theorem 2 shows that the probability of \(_{1}\) and \(_{2}\) degenerating to the same logical constraint can be very small provided suitably chosen \(\) and \(t\). Note that \(\) and \(t\) play different roles. As shown by Proposition 2, the coefficient \(t\) in DC penalty ensures the logical constraint learning can successfully converge to a sensible result. The coefficient \(\) in trust region penalty enlarges the divergence of convergence conditions between two distinct logical constraints, thereby preventing the degeneracy effectively.

## 4 Experiments

We carry out experiments on four tasks, viz., chained XOR, Nonogram, visual Sudoku solving, and self-driving path planning. We use Z3 SMT (MaxSAT) solver (Moura and Bjorner, 2008) for symbolic reasoning. Other implementation details can be found in Appendix E. The experimental results of chained XOR and Nonogram tasks are detailed in Appendix F due to the space limit. The code is available at https://github.com/Lizn-zm/Nesy-Programming.

### Visual Sudoku Solving

**Datasets.** We consider two \(9 9\) visual Sudoku solving datasets, i.e., the SATNet dataset (Wang et al., 2019; Topan et al., 2021)4 and the RRN dataset (Yang et al., 2023), where the latter is more challenging (17 - 34 versus 31 - 42 given digits in each puzzle). Both datasets contain 9K/1K training/test examples, and their images are all sampled from the MNIST dataset. We typically involve two additional transfer tasks, i.e., training the neuro-symbolic system on SATNet dataset (resp. RRN dataset), and then evaluating the system on RRN dataset (resp. SATNet dataset).

**Baselines.** We compare our method with four state-of-the-art methods, i.e., RRN (Palm et al., 2018), SATNet (Wang et al., 2019), SATNet* (Topan et al., 2021), and L1R32H4 (Yang et al., 2023). RRN is modified to match visual Sudoku as done by Yang et al. (2023). SATNet* is an improved version of SATNet that addresses the symbol grounding problem by introducing an additional pre-clustering step. As part of our ablation study, we introduce two variants of our method (NTR and NDC) where NTR removes the trust region penalty (i.e., setting \(=0\)), and NDC removes the DC penalty (i.e., fixing \(t_{1}=t_{2}=0\)) and directly binarizes \((,)\) as the finally learned logical constraints.

**Results.** We report the accuracy results (i.e., the percentage of correctly recognized boards, correctly solved boards, and both) in Table 1.5 A more detailed version of our experimental results is given in Appendix F. The results show that our method significantly outperforms the existing methods in all cases, and both trust region penalty and DC penalty are critical design choices. The solving accuracy is slightly higher than the perception accuracy, as the MaxSAT solver may still solve the problem correctly even when the perception result is wrong. Notably, our method precisely learns all logical constraints,6 resulting in a logical reasoning component that (1) achieves full accuracy when the neural perception is correct; (2) ensures robust results on transfer tasks, in comparison to the highly sensitive existing methods.

Furthermore, we plot some training curves in Figure 2. The left two figures depict the training curves of neural perception accuracy on the SATNet dataset and RRN dataset, which demonstrate the extremely higher efficiency of our method in symbol grounding compared with the best competitor L1R32H4. We also compute the rank of matrix \(\) to evaluate the degeneracy of logical constraint learning. The results are presented in the right two figures, illustrating that logical constraints learned by our method are complete and precise. In contrast, the ablation methods either fail to converge to the correct logical constraints or result in a degenerate outcome.

### Self-driving Path Planning

**Motivation.** Self-driving systems are fundamentally neuro-symbolic, where the primary functions are delineated into two components: object detection empowered by neural perception and path planning driven by symbolic reasoning. Neuro-symbolic learning has great potential in self-driving, e.g., for learning from demonstrations (Schaal, 1996) and to foster more human-friendly driving patterns (Sun et al., 2021; Huang et al., 2021).

**Datasets.** We simulate the self-driving path planning task based on two datasets, i.e., Kitti (Geiger et al., 2013) and nuScenes (Caesar et al., 2020). Rather than provide the label of object detection, we only use planning paths as supervision. To compute planning paths, we construct obstacle maps with \(10 10\) grids, and apply the \(A^{*}\) algorithm with fixed start points and random end points. Note that Kitti and nuScenes contain 6160/500 and 7063/600 training/test examples, respectively, where nuScenes is more difficult (7.4 versus 4.6 obstacles per image on average).

**Baselines.** We include the best competitor L1R32H4 (Yang et al., 2023) in the previous experiment as comparison. Alongside this, we also build an end-to-end ResNet model (denoted by ResNet) (He et al., 2016) and an end-to-end recurrent transformer model (denoted by RTNet) (Hao et al., 2019). These models take the scene image, as well as the start point and the end point, as the input, and directly output the predicted path. Finally, as a reference, we train a ResNet model with direct supervision (denoted by SUP) by using labels of object detection, and the logical reasoning is also done by the \(A^{*}\) algorithm.

**Results.** We include the F\({}_{1}\) score of predicted path grids, the collision rate of the planning path, and the distance error between the shortest path and the planning path (only computed for safe paths) in Table 2. The results show that our method achieves the best performance on both datasets, compared with the alternatives. Particularly, the existing state-of-the-art method L1R32H4 fails on this task,

    &  &  &  \\   & Percep. & Solving & Total & Percep. & Solving & Total & Percep. & Solving & Total \\  RRN & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ SATNet & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ SATNet* & 72.7 & 75.9 & 67.3 & 75.7 & 0.1 & 0.1 & 80.8 & 1.4 & 1.4 \\ L1R32H4 & 94.1 & 91.0 & 90.5 & 87.7 & 65.8 & 65.7 & 84.8 & 21.3 & 21.3 \\  NTR & 87.4 & 0.0 & 0.0 & 91.4 & 3.9 & 3.9 & 90.2 & 0.0 & 0.0 \\ NDC & 79.9 & 0.0 & 0.0 & 88.0 & 0.0 & 0.0 & 86.1 & 0.0 & 0.0 \\  Ours & **95.5** & **95.9** & **95.5** & **93.1** & **94.4** & **93.1** & **93.9** & **95.2** & **93.9** \\   

Table 1: Accuracy results (%) of visual Sudoku task. Our method performs the best.

Figure 2: Training curves of accuracy (left) and rank (right). Our method significantly boosts the efficiency of symbol grounding, and accurately converges to ground-truth constraints.

resulting in a high collision rate. Our method is nearly comparable to the supervised reference model SUP on the Kitti dataset. On the nuScenes dataset, our method even produces slightly less distance error of safe paths than the SUP method.

## 5 Related Work

**Neuro-symbolic learning.** Neuro-symbolic learning has received great attention recently. For instance, Dai et al. (2019) and Corapi et al. (2010) suggest bridging neural perception and logical reasoning via an abductive approach, where a logic program is abstracted from a given knowledge base. To reduce reliance on knowledge bases, Ciravegna et al. (2020) and Dong et al. (2019) directly represent and learn constraints using neural networks. However, the learned constraints are still uninterpretable. To improve interpretability, Wang et al. (2019) introduce SATNet, a method that relaxes the MaxSAT problem with semidefinite programming and incorporates it as a layer into neural networks. SATNet is further followed up by several works (Topan et al., 2021; Lim et al., 2022; Yang et al., 2023). However, how to explicitly extract and use the learned constraints is still unclear for these works. In contrast to the existing neuro-symbolic learning methods, our method can synthesize explicit logical constraints supporting exact reasoning by off-the-shelf reasoning engines.

**Constraint learning.** Our work is also related to constraint learning, which can be traced back to _Valiant's algorithm_(Valiant, 1984) and, more generally, _inductive logic programming_(Muggleton and De Raedt, 1994; Bratko and Muggleton, 1995; Yang et al., 2017; Evans and Grefenstette, 2018). However, Cropper and Dumancic (2022) highlight that inductive logic programming is limited when learning from raw data, such as images and speech, as opposed to perfect symbolic data. To this end, our method goes a step further by properly tackling the symbol grounding problem.

**Boolean quadratic programming and its relaxation.** Many constraint learning and logical reasoning tasks, e.g., learning Pseudo-Boolean function (Marichal and Mathonet, 2010), MaxSAT learning (Wang et al., 2019) and solving (Gomes et al., 2006), and SAT solving (Lipp and Boyd, 2016), can be formulated as Boolean quadratic programming (i.e., quadratic programming with binary variables) (Hammer and Rubin, 1970). However, commonly used techniques, such as branch and bound (Buchheim et al., 2012) and cutting plane (Kelley, 1960), cannot be applied in neuro-symbolic learning tasks. In literature, semidefinite relaxation (SDR) (d'Aspremont and Boyd, 2003; Gomes et al., 2006; Wang and Kolter, 2019) and difference-of-convex (DC) programming (Tao and Hoai An, 1997; Yuille and Rangarajan, 2003; Lipp and Boyd, 2016; Hoai An and Tao, 2018) are two typical methods to relax Boolean constraints. Although SDR is generally more efficient, the tightness and recovering binary results from relaxation are still an open problem (Burer and Ye, 2020; Wang and Kolnc-Karzan, 2022), compromising the exactness of logical reasoning. In this work, we choose DC programming and translate DC constraints to a penalty term with gradually increasing weight, so as to ensure that the Boolean constraints can be finally guaranteed.

## 6 Limitations

In this section, we discuss the limitations of our framework and outline some potential solutions.

**Expressiveness.** The theoretical capability of cardinality constraints to represent any propositional logic formula does not necessarily imply the practical ability to learn any such formula in our frame

    &  &  \\   & F\({}_{1}\) score \(\) & Coll. rate \(\) & Dist. Err. \(\) & F\({}_{1}\) score \(\) & Coll. Rate \(\) & Dist. Err. \(\) \\  ResNet & 68.5\% & 54.0\% & 2.91 & 51.8\% & 68.1\% & 3.60 \\ RTNet & 77.3\% & 36.8\% & 2.89 & 55.9\% & 63.8\% & 2.94 \\ L1R32H4 & 11.9\% & 100.0\% & NA. & 12.0\% & 91.5\% & 100.0 \\  Ours & **80.2\%** & **32.8\%** & **2.84** & **58.8\%** & **57.8\%** & **2.81** \\  SUP & 84.9\% & 28.3\% & 2.75 & 74.6\% & 52.9\% & 2.90 \\   

Table 2: Results of self-driving path planning task. Our method performs the best.

work; this remains a challenge. Fundamentally, logical constraint learning is an inductive method, and thus different learning methods would have different inductive biases. Cardinality constraint-based learning is more suitable for tasks where the logical constraints can be straightforwardly translated into the cardinality form. A typical example of such a task is Sudoku, where the target CNF formula consists of at least 8,829 clauses (Lynce and Ouaknine, 2006), while the total number of target cardinality constraints stands at a mere 324.

Technically, our logical constraint learning prefers equality constraints (e.g., \(x+y=2\)), which actually induce logical conjunction (e.g., \(x y=\)) and may ignore potential logical disjunction which is represented by inequality constraints (e.g., \(x y=\) is expressed by \(x+y 1\)). To overcome this issue, a practical trick is to introduce some auxiliary variables, which is commonly used in linear programming (Fang and Puthenpura, 1993). Consider the disjunction \(x y=\); here, the auxiliary variables \(z_{1},z_{2}\) help form two equalities, namely, \(x+y+z_{1}=2\) for \((x,y)=(,)\) and \(x+y+z_{2}=1\) for \((x,y)=(,)\) or \((x,y)=(,)\). One can refer to the Chain-XOR task (cf. Section F.1) for a concrete application of auxiliary variables.

**Reasoning efficiency.** The reasoning efficiency, particularly that of SMT solvers, during the inference phase can be a primary bottleneck in our framework. For instance, in the self-driving path planning task, when we scale the map size up to a \(20 20\) grid involving \(800\) Boolean variables (\(400\) variables for grid obstacles and \(400\) for path designation), the Z3 MaxSAT solver takes more than two hours for some inputs.

To boost reasoning efficiency, there are several practical methods that could be applied. One straightforward method is to use an integer linear program (ILP) solver (e.g., Gurobi) as an alternative to the Z3 MaxSAT solver. In addition, some learning-based methods (e.g., Balunovic et al. (2018)) may enhance SMT solvers in our framework. Nonetheless, we do not expect that merely using a more efficient solver can resolve the problem. The improve the scalability, a more promising way is to combine System 1 and System 2 also in the inference stage (e.g., Cornelio et al. (2023)). Generally speaking, in the inference stage, neural perception should first deliver a partial solution, which is then completed by the reasoning engine. Such a paradigm ensures fast reasoning via neural perception, drastically reducing the logical variables that need to be solved by the exact reasoning engine, thereby also improving its efficiency.

## 7 Conclusion

This paper presents a neuro-symbolic learning approach that conducts neural network training and logical constraint synthesis simultaneously, fueled by symbol grounding. The gap between neural networks and symbol logic is suitably bridged by cardinality constraint-based learning and difference-of-convex programming. Moreover, we introduce the trust region method to effectively prevent the degeneracy of logical constraint learning. Both theoretical analysis and empirical evaluations have confirmed the effectiveness of the proposed approach. Future work could explore constraint learning using large language models to trim the search space of the involved logical variables, and augment reasoning efficiency by further combining logical reasoning with neural perception.