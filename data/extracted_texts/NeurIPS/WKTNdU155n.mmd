# LLaMo: Large Language Model-based

Molecular Graph Assistant

Jinyoung Park  Minesong Bae  Dohwan Ko  Hyunwoo J. Kim

Department of Computer Science and Engineering, Korea University

{lpmm678, bms2002, ikodoh, hyunwoojkim}@korea.ac.kr

corresponding author.

###### Abstract

Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: **L**arge **L**anguage Model-based **M**olecular graph assistant, which is an end-to-end trained large molecular graph language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.

## 1 Introduction

In recent years, molecular machine learning  has received significant attention, addressing diverse tasks in the chemical domain. The predominant approach for molecular tasks is graph machine learning  that leverages the molecular graph structure, which is a natural and expressive representation of molecules. Although graph-based methods have successfully represented molecules, they have limited interpretability and incompatibility to solve multi-modal molecular tasks dealing with pairs of texts and molecules. To address these issues, recent works  train both a language model and a graph encoder with cross-modal contrastive learning. However, the models trained with cross-modal contrastive learning are insufficient to perform open-ended molecule-to-text generation tasks , which are more applicable to practical use.

Large Language Models (LLMs)  have shown impressive progress and accomplished human-like open-ended text generation with the power of billions of parameters. To leverage the instruction-following capability of LLMs, many works employ instruction-tuning approaches  for general-purpose language models. Motivated by the development of LLMs and instruction tuning, Large Vision-Language Models (LVLMs) have recently been explored and achieved success on image comprehension and image-to-text generation tasks . Despite the success of LLM-based approaches on natural language processing and machine vision domains, the research on the integration of language models and molecular graphs has been less studied due to the lack of consideration of the architecture design of Large Molecular Graph-Language Models (LMGLMs) and the molecular graph instruction data.

In this paper, we propose LLAMo: **L**arge **L**anguage Model-based **Mo**lecular graph assistant, which seamlessly integrates a molecular graph encoder and a large language model to enable the instruction-following response generation in molecular domain. Specifically, LLAMo consists of the molecular graph encoder, large language model, and multi-level graph projector that bridges the graph encoder and large language model. The multi-level graph projector abstracts the representation of each GNN layer and motif representation using a cross-attention mechanism, ensuring a thorough understanding of molecular structures. Furthermore, we introduce machine-generated molecular graph instruction data through the pipeline to convert molecular descriptions and IUPAC names into a multi-turn conversation format. The generated instruction-following data enhances the model's ability to perform general-purpose molecule and language understanding, bridging the gap between molecular graph analysis and language-based tasks. Our proposed LLAMo outperforms the LLM-based works such as GPT-4 across diverse tasks, including molecular description generation, property prediction, and IUPAC name prediction.

Our contributions are summarized as follows:

* We propose LLAMo: **L**arge **L**anguage Model-based **Mo**lecular graph assistant consisting of graph encoder, language model, and multi-level graph projector equipped with a multi-level graph projector that captures rich information of the graph structure at multiple levels.
* We introduce GPT-4 generated molecular graph-text multi-turn conversation data to address the data scarcity problem of molecule-text datasets and improve the instruction-following capabilities of a large molecular graph-language model.
* Our experiments demonstrate that LLAMo achieves the best performance on various tasks such as molecular description generation, property prediction, and IUPAC name prediction.

## 2 Related works

**Molecular graph modeling.** Molecular graphs serve as a natural and expressive representation of molecules, effectively capturing the structural information. Graph neural networks [19; 20; 21; 22] are commonly utilized architectures for molecular graph representations. To learn graph neural networks with the limited molecular graph data , self-supervised learning has been explored. For example, various approaches [24; 25] have been developed to capture multi-level features of molecular graphs, such as node-level masked atom modeling , motif-based self-supervised learning [25; 26], and graph-level contrastive learning [27; 28]. With the advance of multi-modal large language models, molecule-language tasks such as molecule-text retrieval  or molecule captioning  have recently drawn significant attention. Recent works [3; 4; 30] have attempted to enable language models to understand molecular graphs.  treated nodes of molecular graphs as tokens of language models. Some works have adopted GNN-based encoders, either by propagating their outputs to language models through MLP  or employing cross-modal projectors . However, these methods fail to consider molecular graphs at multiple levels and are hindered by inherent limitations of graph encoders, such as the over-smoothing problem . To address these challenges, we propose a novel architecture, LLAMo, which effectively propagates multi-level information of molecular graphs to language models.

**Instruction tuning.** Recent advancements of LLMs lead to extensive research on _instruction tuning_, aimed at improving the model's capability to follow human instructions [12; 32; 33; 34; 35]. To construct high-quality instruction tuning data, a line of previous approaches [34; 35] has adopted existing human-annotated datasets and integrated them with a new structure and template. On the other hand, recent studies [12; 36; 37] on instruction tuning have collected data samples from strong LLMs like GPT-4 . These works first manually construct annotated seed instruction samples and expand them by prompting LLMs. As a result, several instruction-tuned LLMs [14; 16; 37] have been proposed from the open-source LLMs, _e.g._, LLAMA  and shown generalizability across a wide range of instructions. More recently, those studies on instruction tuning have been expanded to visual instruction tuning in image [15; 17; 38] and video [39; 40] domain to enable the model to understand the visual contents. Inspired by the instruction tuning for multi-modal LLMs in other domains, in this work, we study instruction tuning specifically for molecule graphs, which has been underexplored in the literature.

## 3 LLaMo: Large Language Model-based Molecular Graph Assistant

The primary goal is to seamlessly integrate a molecular graph encoder and a Large Language Model (LLM) to generate instruction-following responses to the input texts and molecules. To achieve it, we propose LLaMo: **L**arge **L**anguage Model-based **Mo**lecular graph assistant, a general-purpose Large Molecular Graph-Language Model (LMGLM) equipped with a multi-level graph projector. Specifically, the proposed framework utilizes three input modalities: 1D SMILES , 2D molecular graph, and text (instruction). SMILES  is a 1D representation of a molecule, and a 2D molecular graph is processed by a GNN. The three input modalities are fed as a sequence of tokens and our LLaMo autoregressively generates text responses. Formally, given SMILES \(\), molecular graph tokens \(\), and text (instruction) \(\), the proposed method renders the response \(=\{_{i}\}_{i=1}^{K}\) as:

\[p(|,,)=_{i=1}^{K}p (_{i}|,,,_{<i}),\] (1)

where \(_{<i}\) indicates generated token sequences until \(i\)-th token.

### Model Architecture

The overall architecture of LLaMo is illustrated in Figure 1. LLaMo consists of a graph encoder, a multi-level graph projector, and a backbone large language model. The graph encoder \(g()\) takes a 2D molecule graph as an input and outputs their node representations as a sequence of tokens. The multi-level graph projector \(_{}()\) transforms the sequence of node representations into molecular tokens to align them with the LLM. Then, the LLM \(f()\) processes molecular and text tokens and provides a response in an autoregressive manner.

**Graph encoder.** We adopt Graph Neural Networks (GNNs) as a molecular graph encoder. Given the graph \(\), graph neural networks \(g()\) iteratively update node representation \(_{v}^{(l)}^{d^{(l)}}\) via the message-passing framework. With the message-passing, \(L\)-layer GNN provides node representations \(_{v}^{(L)}\) that express an \(L\)-hop ego-graph given the node \(v\) as a center node. More details about graph neural networks are in the Appendix C.

Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.

**Multi-level graph projector.** The goal of a multi-level graph projector is to align the graph encoder with the LLM by transforming a set of node representations \(_{}^{(L)}\) into a sequence of molecular graph tokens \(_{}\). It enables the language model to utilize graph information. In the literature, projectors have been proposed mainly for Large Vision-Language Models (LVLMs) [17; 18; 38; 42; 43]. They are usually implemented using a linear projection  or an abstraction of visual features [17; 42], which are outputs of the final layer of a visual encoder given input image. Analogously, we can design the projector for large molecular graph-language models with a linear projection or an abstraction of high-level node representations from the pre-trained graph encoder, which is formulated as:

\[_{}=(_{}^{(L)} ),_{}^{(L)}=g( ),\] (2)

where \(_{}^{(L)}=[_{0}^{(L)},,_ {||}^{(L)}]^{|| d^{(L)}}\) is the concatenation of node representation \(_{v}^{(L)}^{d^{(L)}}\) from \(L\)-th layer GNN and \(()\) is the projector.

However, we observe that the high-level representation is not effective in capturing the local information due to the over-smoothing problem , which means that the node representations become indistinguishable, as the number of layers in the GNN increases. Figure 2 depicts node representations (yellow dots) of graph encoder with 1,2,4,5 layers on one molecular graph sample. (More samples are in Appendix I.) As mentioned above, node representations become over-smoothed as the number of layers increases, leading to nearly identical node representations in the final layer. Consequently, conventional projectors relying on high-level node representations have a limited capability to preserve the detailed or local information of molecular graphs. Moreover, many tasks require multi-scale information, including atom, atomic group, and molecule levels. Hence, the projector that solely utilizes features from the top layer is suboptimal for the tasks.

Motivated by the observations, we propose a novel multi-level graph projector to generate graph tokens that contain richer information reflecting the graph structure at multiple levels. The multi-level graph projector \(_{}()\) is formulated as

\[_{}=_{}(\{_{ }^{(l)}\}_{l=0}^{L}),\{_{}^{(l)}\}_{l=0}^{L}=g( ).\] (3)

The method captures multi-hop graph information by leveraging node representations from all layers of a GNN. To handle an arbitrary number of nodes, yielding a variable length \(|| L\) features, we adopt the cross-attention with learnable tokens \(^{(l)}=[_{1}^{(l)},,_{b}^{(l)}] ^{b d}\) for \(l=0,,L\), where \(b\) is the number of learnable prompts. Here, \([,]\) indicates the concatenation operation. The learnable tokens aggregate \(l\)-th layer GNN representations into a fixed number of tokens as:

\[}^{(l)}=^{(l)}(^{(l)},_{ }^{(l)},_{}^{(l)})^{b  d},\] (4)

where \((Q,K,V)\) is the attention operation with query \(Q\), key \(K\), and value \(V\).

For more detailed representations of the input molecule, LLaMo also has learnable tokens \(^{()}\) for motif-level representations. We use the functional groups as motifs, which are the statistically important subgraphs in the molecular graphs. To construct functional group representations \(_{}\), we initially identify functional groups, following . Then, we vectorize the main characteristics of each functional group, which is represented as \(_{,i}\). Finally, the functional group representations \(_{}\)

Figure 2: Node representations of graph encoder with 1,2,4,5 layers. As the number of layers increases, node representations collapse.

are constructed by concatenating all individual functional group representations \(_{,i}\), which is formulated as \(_{}=[_{,0},,_{,M}]\), where \(M\) indicates the number of the functional groups in the given molecular graph. Given the functional group representations \(_{}\) of the input molecule, we obtain \(}^{}=^{}( ^{},_{},_{})\) with the cross-attention.

Then, we obtain the graph-level representations by applying MLP to the multi-hop and motif-level representations, _i.e._, \([}^{0},,}^{(L)},}^{}]\). It is formulated as:

\[_{}=([}^{(0)},,}^{(L)},}^{}]) ^{b(L+2) d},\] (5)

where \(_{}\) is a sequence of graph tokens to be fed into the LLM.

**Large language models.** After constructing tokens for encoding molecular graphs, LLaMo fuses SMILES representation, graph, and text tokens and puts them into the language model to generate an instruction-following response.

### Training LLaMo

Similar to most LVLMs [18; 38; 42], we train LLaMo in the two-stage pipeline: (1) pre-training for molecular graph-language alignment, and (2) instruction-tuning end-to-end as in Figure 3.

**Stage 1. Pre-training for molecular graph-language alignment.** The first stage focuses on the alignment between the graph encoder and a large language model by learning our multi-level graph projector. In this stage, with the LLM frozen, we train the multi-level graph projector and the graph encoder by generating molecule descriptions. For training, we use a molecule-description pair dataset (_e.g.,_ PubChem ) consisting of a 1D SMILES representation of molecule and molecular graph and its corresponding description.

**Stage 2. Instruction-tuning end-to-end.** In the second stage, we train the LLM to enhance the instruction-following capabilities and enable a deeper understanding of molecular graphs. In this stage, we freeze the graph encoder and train both the multi-level graph projector and the LLM. Since it is too expensive to train the full LLM, we employ LoRA  to adapt LLM to the data. For instruction-following, we use the GPT-generated instruction-following multi-turn conversation dataset, which will be introduced in Section 4. In addition to our generated instruction-following dataset, we use a diverse set of datasets with various instructions: molecule description generation, molecular property prediction, IUPAC name generation, forward reaction prediction, and retrosynthesis datasets.

## 4 GPT-assisted Molecular Graph Instruction Data Generation

Instruction data are essential for improving the instruction-following capabilities of LLM-based models. Despite active research on instruction-tuning, the instruction-following data for molecular

Figure 3: Two-stage training pipeline. Stage 1 involves training the graph encoder, and stage 2 entails fine-tuning the LLM using LoRA. In both stages, the multi-level graph projector is continuously trained. All training processes are performed by generating the instruction-following response.

graphs have been less explored in the literature since annotations require expertise. To alleviate the need for expertise and minimize the manual efforts, we utilize GPT-4  to generate molecular graph-text instruction-following data using graph-text pair datasets.

Inspired by previous results , we construct multi-turn conversation datasets, which are more diverse and effective for instruction tuning compared to simple pairs of questions and answers. We leverage GPT-4 to generate multi-turn conversations with tailored contexts/prompts that consist of two representations for molecular graphs and description: (i) SMILES representation that describes the chemical structures with special strings, (ii) captions that explain the molecule, and (iii) IUPAC name that describes the molecule based on its chemical composition and structure. These representations enable the GPT-4, which inherently lacks in-depth molecular knowledge, to understand and generate a diverse and high-quality set of examples. One example of the input representations is shown in the top block of Table 1.

Specifically, we generate the multi-turn conversation data in three steps: **1)** select exemplar conversations among machine-generated instruction-tuning data, **2)** generate multi-turn conversations via in-context learning with the exemplar conversations as prompts, and **3)** filter out incomplete conversations and those with many turns. In the first step, we generate exemplars with a brief human-written instruction as shown in Appendix H. However, we found that GPT-4 frequently fails to generate complete multi-turn conversations without the exemplars. To address this issue, we generate the instruction data with in-context learning. We sample exemplars from a small set of complete conversations generated by GPT-4 in the first step. Then, GPT-4 generates the complete multi-turn conversation data for the instruction tuning guided by the prompts wrapped with the generated exemplars. To validate the quality of the generated conversation, we sample 500 subsets generated via in-context learning. We find that some conversations consisting of a large number of turns are prone to generating incomplete and inaccurate outputs. So, we filter out incomplete conversations and those with many turns. The example of the generated multi-turn conversation is in the bottom block of Table 1. In total, we generate 12K unique molecular graph-language instruction-following samples using PubChem324k dataset .

|} 
**Context type 1: SMILES representation** \\ CCCCC(C)/C=C(C)/C=C/C=C/(=)NC1=C[C@][C@H](CC1=O))/(C=C/C=C/C=C/C(=O)NC2 =(CCC2=O))O) \\
**Context type 2: Description** \\ The molecule is a polyene antibiotic that is TMC-1A in which the 2,4-dimethyloct-2-enoyl group has been replaced by an (E,E)-4,6-dimethyldeca-2,4-dienoyl group. TMC-1C is an anticumour antibiotic isolated from Streptomyces sp. A-230...comitted \\
**Context type 3: IUPAC name** \\ (2E,4E)-N-[(3S,4R)-3,4-dihydroxy-3-([1E,3E,5E)-7-[(2-hydroxy-5-oxocyclopenten-1-yl)amino]-7-oxohepta-1,3,5-thienyl]-6-oxocyclohexen-1-yl]-4,6-dimethyldeca-2,4-dienamide \\ 
**Response: Multi-turn conversation** \\ Question: \\ What is the IUPAC name of the molecule you are analyzing? \\ === \\ Answer: \\ The IUPAC name of the molecule is (2E,4E)-N-[(3S,4R)-3,4-dihydroxy-3-[(1E,3E,5E)-7-[(2-hydroxy-5-oxocyclopenten-1-yl)amino]-7-oxohepta-1,3,5-trienyl]-6-oxocyclohexen-1-yl]-4,6-dimethyldeca-2,4-dienamide. \\... \\ 
**

* **Response: Multi-turn conversation** \\ Question: \\ What is the IUPAC name of the molecule you are analyzing? \\ === \\ Answer: \\ The IUPAC name of the molecule is (2E,4E)-N-[(3S,4R)-3,4-dihydroxy-3-[(1E,3E,5E)-7-[(2-hydroxy-5-oxocyclopenten-1-yl)amino]-7-oxohepta-1,3,5-trienyl]-6-oxocyclohexen-1-yl]-4,6-dimethyldeca-2,4-dienamide. \\... \\ 
**

* **Pass: Multi-turn conversation** \\ Question: \\ What is the IUPAC name of the molecule you are analyzing? \\ === \\ Answer: \\ This molecule can act as a weak acid due to the presence of the carboxylic acid group. \\  

Table 1: **One example to illustrate the instruction-following data.** The top block shows the contexts such as SMILES, description, and IUPAC name used to prompt GPT, and the bottom block shows the response of GPT-4.

## 5 Experiments

### Experimental Settings

**Benchmarks.** To evaluate the efficacy of the proposed method, we evaluate the model for three tasks such as **1)** molecule description generation, **2)** IUPAC name prediction, **3)** property prediction (regression). We conducted experiments under two major settings: generalist and specialist models. In the generalist setting, one model handles all three tasks, whereas in the specialist setting, we train a model for each downstream task. More details about benchmarks are in Appendix G.

**Implementation details.** For the generalist models, we train our LLaMo based on LLama-2-7b-chat  for a fair comparison with Mol-Instructions . For the specialist models, we train our LLaMo with Galactica 1.3B  for a fair comparison with MolCA . To train the generalist variant of LLaMo, we use a training split of molecular description generation dataset of Mol-Instruction  in stage 1. In stage 2, the model is instruction-tuned with a training split of description generation, property prediction, forward reaction, and retrosynthesis instruction dataset of Mol-Instruction , IUPAC name prediction from , and our GPT-generated instruction-following data. To train the specialist variant of LLaMo, we follow MolCA  to train the model with a pretraining split of PubChem324kV2 in the stage 1 phase and fine-tune the model for each specific downstream task in the stage 2. We adopt a long training schedule (epoch 1 pre-training, epoch 3 instruction tuning) for the final models. For analysis, we use a short training schedule (epoch 1 pre-training, epoch 1 instruction tuning). For further implementation details, refer to Appendix E.1.

**Baselines.** For the generalist models, we compare our LLaMo with (1) LLM-based generalist models including Galactica , LLaMA2-7B , GPT-3.5, and GPT-4, (2) Molecule-specialized LLM such as Text+Chem T5 , and (3) Molecule instruction-tuned generalist model such as Mol-Instructions . Since GPT-3.5 and GPT-4 have difficulty in solving the tasks without in-context learning, we additionally measure the performance of GPT-3.5 and GPT-4 with 4-shot in-context learning, which are GPT-3.5 (ICL) and GPT-4 (ICL). For the specialist models, we use single-task specialist molecule-language models as baselines, including MolT5 , MoMu , and MoICA .

### Experimental Results

**Generalist models.** We provide the experimental results of generalist models in molecular description generation, IUPAC name generation, and property prediction tasks. Our LLaMo is built on LLaMA-7B and it is fine-tuned by our instruction-tuning method. Table 2 shows that our LLaMo achieves the best performance in all three tasks. In comparison to **GPT-4 (ICL)**, which is GPT-4 with in-context-learning, LLaMo shows a performance improvement of 11.9 in BLEU-4 and 14.9 in METEOR for molecular description generation. Furthermore, LLaMo outperforms Mol-Instructions, an instruction-tuned model with molecular data, by a substantial performance gain of 41.7 in METEOR for molecular description generation and a 0.007 performance gain in MAE on the property prediction task. More experimental results on forward reaction prediction and retrosynthesis are in Appendix D.

    &  &  **Mol. Inst.** \\ **tuned** \\  } &  &  &  \\  & & BLEU (\(\)) & METEOR (\(\)) & BLEU (\(\)) & METEOR (\(\)) & MAE (\(\)) \\   GPT-3.5 & GPT-3.5 & 2.2 & 19.7 & 33.4 & 52.6 & 0.075 \\ GPT-3.5 (ICL) & GPT-3.5 & 28.4 & 56.1 & 50.3 & 62.0 & 0.028 \\ GPT-4 & GPT-4 & 0.8 & 16.7 & 29.0 & 48.1 & 0.098 \\ GPT-4 (ICL) & GPT-4 & 27.0 & 52.2 & 51.8 & 62.4 & 0.019 \\ Galactica\(\) & Galactica & 0.8 & 6.5 & – & – & 0.568 \\ Text+Chem T5\(\) & T5-Base & 3.6 & 13.9 & – & – & – \\  LLaMA2 & LLaMA2-7B & 0.0 & 14.1 & 0.0 & 0.4 & N/A\({}^{*}\) \\ Mol-Instructions\(\) & LLaMA2-7B & ✓ & 14.3 & 25.4 & – & – & 0.013 \\ 
**LLaMo (Ours)** & LLaMA2-7B & ✓ & **38.9** & **67.1** & **56.0** & **73.2** & **0.006** \\   

Table 2: Performance (%) of generalist models on three tasks: molecule description generation, IUPAC prediction, and property prediction. **Mol. Inst. tuned** denotes the molecular instruction-tuned model. \(*\) The result is not available since LLaMA2 fails generating numerical outputs. \(\) denotes the experimental results drawn from Mol-Instruction .

**Specialist models.** We also evaluate the performance of specialist models to validate the effectiveness of our LLaMo, which is individually fine-tuned for each dataset. Table 3 demonstrates that our LLaMo consistently achieves the best performance across all tasks and datasets. Specifically, LLaMo outperforms the second-best model MolCA with Galactica 1.3B, by 4.1 in BLEU-score and 2.4 in METEOR on the PubChem324kV2 dataset. For IUPAC name prediction, LLaMo also shows superior performance, achieving a METEOR score of 73.4, which surpasses MolCA with Galactica 1.3B by a margin of 1.3 points. This experimental result indicates that our LLaMo is consistently effective in comprehending molecular graphs based on diverse large language models.

### Analysis

**Impact of multi-level graph projector.** To validate the effectiveness of our multi-level graph projector, we compare the performance of the multi-level graph projectors (denoted by **MGProj**) with other projectors in Table 4, including two widely-used projectors such as MLPs and resamplers. Additionally, we measure the performance of the base model without a graph (and a projector) denoted as **w/o Graph** for the ablation study. **MLP (w/ low-level)** and **MLP (w/ high-level)** denote the MLP projectors where the input is low-level representation \(_{}^{(1)}\) and high-level representation \(_{}^{(L)}\), respectively. **MLP (w/ concat)** indicates the MLP projector with the concatenated representations of all GNN layers as an input. **Resampler** denotes the cross-attention based resampler projector designed in Qwen-VL . **MGProj (w/o motif)** and **MGProj** are our multi-level graph projector without and with motif tokens \(}^{}\).

Table 4 shows that our multi-level graph projector (**MGProj**) achieves the best performance across all three tasks. Specifically, the multi-level graph projector achieves 49.6 BLEU and 70.9 METEOR scores with a significant improvement compared to MLP projectors in the IUPAC prediction task. These experimental results demonstrate that our multi-level graph projector is more effective than conventional projectors by capturing multi-scale information, including atom, atomic group, and molecule-level information.

    &  &  &  \\  & BLEU (\(\)) & METEOR (\(\)) & BLEU (\(\)) & METEOR (\(\)) & MAE (\(\)) \\   w/o Graph & 26.1 & 56.6 & 36.3 & 62.2 & 0.013 \\ MLP (w/ low-level) & 32.4 & 62.1 & 42.2 & 68.4 & 0.009 \\ MLP (w/ high-level) & 33.8 & 63.4 & 45.5 & 67.4 & 0.008 \\ MLP (w/ concat) & 34.8 & 64.1 & 47.1 & 70.2 & **0.007** \\ Resampler & 34.4 & 62.8 & 43.4 & 65.2 & 0.009 \\ MGProj (w/o motif) & 36.1 & 65.3 & 48.8 & 69.8 & 0.008 \\ 
**MGProj (Ours)** & **37.8** & **66.1** & **49.6** & **70.9** & **0.007** \\   

Table 4: Performance comparison according to the projector type.

    &  &  &  &  &  \\  & & BLEU & METEOR & BLEU & METEOR & METEOR \\   MoIT5-Small & T5-Small & full ft & 8.5 & 18.5 & 43.6 & 55.1 & 42.5 \\ MoIT5-Base & T5-Base & full ft & 20.9 & 35.6 & 45.7 & 56.9 & 53.2 \\ MoIT5-Large & T5-Large & full ft & 22.2 & 36.6 & 50.8 & 61.4 & 58.5 \\  MoMu-Small & T5-Small & full ft & 12.0 & 21.8 & 44.5 & 57.6 & – \\ MoMu-Base & T5-Base & full ft & 21.5 & 34.2 & 46.2 & 57.6 & – \\ MoMu-Large & T5-Large & full ft & 22.8 & 36.2 & 51.5 & 59.7 & – \\ MoICA, Galactica-125M & Galactica-125M & full ft & 24.3 & 41.6 & 52.6 & 63.6 & 71.8 \\ MolCA, Galactica-1.3B & Galactica-1.3B & LoRA & 30.3 & 45.6 & 53.1 & 65.1 & 72.1 \\ 
**LLaMo (Ours)** & Galactica-1.3B & LoRA & **34.4** & **48.0** & **54.8** & **66.6** & **73.4** \\   

Table 3: Performance (%) of specialist models on molecule captioning with the PubChem324k and ChEBI-20 datasets and IUPAC name prediction. Full ft denotes full parameter fine-tuning.

**Impact of GPT-generated instruction-tuning data.** In Table 5, we provide the ablation studies of each training stage and our GPT-generated instruction dataset. The experimental results reveal that the instruction tuning with our generated multi-turn conversation data enhances the performance of LLaMo compared to the models trained via one or two-stage training without our GPT-generated instruction data. This indicates that instruction tuning with our GPT-generated multi-turn conversation data provides the model with more detailed and instruction-following guidance.

**Instruction tuning v.s. multi-task learning.** Table 6 shows the advantages of instruction-tuning based on task instructions compared to multi-task learning using the simple task identifier. We use the task name as a simple task identifier for multi-task learning. From the table, the model without instruction tuning (Stage 1) achieves BLUE score of 35.5 and 7.3 on molecule description and IUPAC prediction tasks, respectively. The multi-task learning approach improves the scores to 36.9 for molecule description and 49.4 for IUPAC prediction. However, the instruction-tuning method demonstrated the most significant enhancement, achieving the highest scores of 37.8 for molecule description and 49.6 for IUPAC prediction. These results indicate that instruction tuning outperforms both the baseline and multi-task learning methods, suggesting its effectiveness in improving model performance on general-purpose training.

**Visualization of attention maps.** We visualize the attention map to explore the effect of the multi-level graph projector in Figure 4. The figure illustrates the attention maps of graph tokens for generating coarse-grained (left) and fine-grained (right) descriptions. Interestingly, the attention scores of the low-level are relatively higher than the high-level when generating fine-grained captions, whereas the attention value of the high levels is high when generating coarse-grained captions. This indicates that both low and high-level graph structural information is crucial in expressing the molecules, and the attention matrix is adaptive to the caption types.

**Qualitative analysis.** Figure 5 shows a GT description and the molecular descriptions generated by the model with and without the molecular graph (SMILES representation only). As shown in the

    &  &  &  \\  & BLEU (\(\)) & METEOR (\(\)) & BLEU (\(\)) & METEOR (\(\)) & MAE (\(\)) \\   w/o inst. tuning (Stage 1) & 35.5 & 64.8 & 7.3 & 16.9 & N/A \\ Multi-task & 36.9 & 64.2 & 49.4 & 70.5 & 0.218 \\
**Instruction-tuning (Ours)** & **37.8** & **66.1** & **49.6** & **70.9** & **0.007** \\   

Table 6: Performance comparison according to the training type.

Figure 4: Visualization of attention maps for samples with coarse-grained caption (left) and fine-grained caption (right). The attention scores of high-level features are relatively high when generating coarse-grained captions, whereas those of low-level features are high for fine-grained captions.

    &  &  &  &  &  \\  & & BLEU (\(\)) & METEOR (\(\)) & BLEU (\(\)) & METEOR (\(\)) & MAE (\(\)) \\    & & & 0.0 & 14.1 & 0.0 & 0.4 & N/A \\ ✓ & & & 35.5 & 64.8 & 7.3 & 16.9 & N/A \\ ✓ & ✓ & & 37.2 & 65.1 & 47.5 & 70.2 & **0.007** \\ ✓ & ✓ & ✓ & **37.8** & **66.1** & **49.6** & **70.9** & **0.007** \\   

Table 5: Ablation studies on training stage and GPT-generated instruction tuning data.

figure, LLaMo with a graph denoted as **LLaMo w/ graph** generates a better molecular description compared to LLaMo without a graph (**LLaMo w/o graph**). The GT description explains the molecule with 'omega-hydroxy-long-chain fatty acid anion'. Since LLaMo w/o graph does not have any graph structural information, it fails to generate a description with an invalid IUPAC name ('1-hydroxy-2-oxo-4-oxocyclohexane-1,2-diol'), while LLaMo w/ graph generates a more related description with 'hydroxy-long-chain fatty acid anion'. In addition, we know that LLaMo w/ graph accurately predicts the long-chain structure of the molecule.

We also perform another qualitative analysis by comparing molecular descriptions generated from the model with and without our Multi-level Graph Projector (MGProj) denoted by **LLaMo w/ MG Proj** and **LLaMo w/o MGProj** in Figure 6. The figure shows that the multi-level graph projector plays a crucial role in capturing the details of the molecule. Compared to **LLaMo w/o MGProj** generating 'pyridine', the model with MGProj generates accurate molecular description including 'pyrazine' same as GT description. This demonstrates that the multi-level graph projector is effective in molecule understanding and generation by preserving the molecular graph structural information.

## 6 Conclusion

We propose LLaMo: Large Language Model-based Molecular graph assistant, an end-to-end trained Large Molecular Graph Language Model, to perform various molecule-related tasks with a single model. For the projector, we newly introduce a multi-level graph projector, which addresses the over-smoothing problem of the graph encoder and captures multi-hop graph information. We also present machine-generated instruction-following data in the form of multi-turn conversations to improve the instruction-following capabilities of the large language model.

Figure 5: An example of molecular description generation results of LLaMo w/o graph and LLaMo w/ graph given the molecule (“C(CCC/C=C\(\)C/C=C\(\)CCCCCCO)CCCCCC(=0)[O-1]”). In the top box, the molecular graphs of IUPAC and functional groups in the descriptions are depicted.

Figure 6: An example of molecular description generation results of LLaMo w/o MGProj and LLaMo w/ MGProj given the molecule (“C[C@@H1]CN(C(=O)C2=C(C(=CC=C2)NC(=O)C3=NC=CN=C3)O[C@H1]1CNC)[C@H1](C)CO”). In the top box, the molecular graphs of IUPAC and functional groups in the descriptions are depicted.