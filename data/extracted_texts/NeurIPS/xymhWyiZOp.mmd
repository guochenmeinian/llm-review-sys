# On the Use of Anchoring for Training Vision Models

Vivek Narayanaswamy

Lawrence Livermore National Laboratory

narayanaswam1@llnl.gov

&Kowshik Thopalli

Lawrence Livermore National Laboratory

thopalli@llnlnl.gov

&Rushil Anirudh

Amazon

rushil15anirudh@gmail.com

&Yamen Mubarka

Lawrence Livermore National Laboratory

mubarka1@llnl.gov

&Wesam Sakla

Lawrence Livermore National Laboratory

sakla1@llnl.gov

&Jayaraman J. Thiagarajan

Lawrence Livermore National Laboratory

jjthiagarajan@gmail.com

###### Abstract

Anchoring is a recent, architecture-agnostic principle for training deep neural networks that has been shown to significantly improve uncertainty estimation, calibration, and extrapolation capabilities. In this paper, we systematically explore anchoring as a general protocol for training vision models, providing fundamental insights into its training and inference processes and their implications for generalization and safety. Despite its promise, we identify a critical problem in anchored training that can lead to an increased risk of learning undesirable shortcuts, thereby limiting its generalization capabilities. To address this, we introduce a new anchored training protocol that employs a simple regularizer to mitigate this issue and significantly enhances generalization. We empirically evaluate our proposed approach across datasets and architectures of varying scales and complexities, demonstrating substantial performance gains in generalization and safety metrics compared to the standard training protocol. The open-source code is available at - https://software.llnl.gov/anchoring

## 1 Introduction

Anchoring  is a recent architecture-agnostic principle for training deep neural networks. It reparameterizes each input \(x\) into a tuple comprising a reference sample \(\) and the _residual_\(d=x-\), i.e., \([,d]\), \( P_{r}\) and \(d P_{}\). Here, \(P_{r}\) and \(P_{}\) denote the distributions of references and residuals respectively. The resulting tuple is then fed as input to a deep network instead of the original input \(x\), by concatenating the tuple elements along the feature axis for vector-valued data or the channel axis for image data. Although the first layer of the network needs to be modified to accommodate twice the number of input dimensions (due to concatenation), the rest of the model architecture and optimization strategies remain the same as in standard training. This simple re-parameterization of the input forces the neural network to model the joint distribution \(P_{(r,)}\) for predicting the target label \(y\). Formally, the training objective can be written as:

\[^{*}=_{}|}_{(x,y) }*{}_{ P_{r}} y,_{}([,x-]),\] (1)

where \((.)\) is a loss function such as cross-entropy, \(\) is the training dataset and \(\) is the underlying network parameterized by \(\). In effect, for a given \(x\) and reference samples \(_{1},,_{k}\), anchoringensures that \(_{}([}_{},_{}])= =_{}([}_{},_{}])\), where \(_{k}=-}_{k}\). In other words, regardless of the choice of reference the model must arrive at the same prediction for an input. This principle has been shown to produce models with improved calibration and extrapolation properties [2; 3], and to facilitate accurate epistemic uncertainty estimation . In this paper, we systematically explore the utility of anchoring as a generic protocol for building vision models and make a number of fundamental insights on its training and inferencing, applicability to different architecture families (conv-nets, transformers), and most importantly, the implications on model generalization and safety.

Our main contributions in this work can be summarized as follows:

**A closer look into anchored training and inferencing**: By studying the roles of reference set diversity and the inferencing protocol choice on the behavior of anchored models, we identify a critical limitation in current practice. More specifically, we find that conventional anchored training fails to effectively leverage the reference diversity, thus restricting its generalization capabilities, and that merely adopting sophisticated inference protocols  cannot circumvent this limitation.

**A new anchored training protocol**: We attribute the limited generalization power of anchored models to the increased risk of learning undesirable shortcuts, owing to insufficient sampling of \(P_{(,)}\) during training, particularly in cases of high reference diversity. To address this, we introduce a new training protocol for anchoring that relies on a novel reference-masking regularizer.

**Benchmarking generalization and safety of anchored models**: Since anchoring is architecture-agnostic, we benchmark it using a variety of conv-net/transformer architectures on CIFAR-10, CIFAR-100 and Imagenet-1K datasets. We demonstrate significant improvements in OOD generalization, calibration and anomaly resilience over standard training. We also show that, without incurring any additional training or inference overheads, anchoring is synergistic to existing training strategies (e.g., data augmentations, optimizers, schedulers).

## 2 A Closer Look into Anchored Training and Inference

### What makes anchoring a promising training protocol?

Anchored training forces the network to learn a mapping between the joint space of (reference, residuals) and the targets, rather than the original input-target pairs. At first glance, anchoring may seem like a trivial reposing of standard training, but it is conceptually very different. Through this reparameterization, anchoring creates different relative representations for a sample with respect to references drawn from \(P_{}\), and attempts to marginalize the effect of the reference when making a prediction for that sample. As demonstrated by , this process exploits the lack of shift invariance in the neural tangent kernel induced by deep networks , and implicitly explores a wider hypothesis class that is potentially more generalizable. Furthermore, anchored models have been found to extrapolate better to unseen data regimes through the use of transductive inferencing , i.e., identifying an optimal reference for each sample, such that the resulting residual is likely to have been exposed to the model during training. While anchoring offers promise, its success hinges on effectively leveraging the diversity of the reference-residual pairs and stably converging for the same protocols from standard training (e.g., architectures, data augmentations, optimizers etc.).

### Does reference diversity play a key role in anchored training?

A unique property of anchoring is its ability to utilize relative representations w.r.t. a reference distribution \(P_{}\) (realized using a reference set \(\)), effectively operating in the joint space \(P_{(,)}\). During implementation, the reference set \(\) is defined as a subset of the training data itself i.e., \(\). Intuitively, by controlling the construction of \(\), one can control the diversity of reference-residual combinations that anchored training is exposed to. We hope that with exposure to increasingly large and diverse reference sets, anchoring will explore a wide range of hypotheses, while also ensuring that the model can make consistent predictions for test samples using any randomly drawn reference \(}\). However, when the anchored training does not effectively characterize the joint distribution \(P_{(,)}\), the generalization can suffer, particularly when tested beyond the regimes of training data. To obtain a deeper understanding of anchored training, we conduct an empirical study on CIFAR10/100 datasets by varying the diversity of \(\).

Setup_. We first sub-sample \(\) to construct reference sets of varying sizes ranging between 5 and \(\), where the latter corresponds to the entire training dataset. The construction is such that each set represents an increasing level of sample diversity (i.e., samples from multiple classes). This is followed by anchored training based on the different reference sets with ResNet18 models . All other training specifics and hyper-parameters are fixed across the experiments. Post-training, we evaluate the model performance on the CIFAR10C/100C synthetic corruption benchmarks  and report the average corruption accuracy across \(5\) corruption severity levels.

_Observations._ Figure 0(a) and 0(b) illustrates the performance of CIFAR10/100 anchored training on the respective evaluation benchmarks. Interestingly, we observe that the anchoring performance remains fairly similar (minor improvements in accuracy) even with orders of magnitude growth in the reference set size. While anchoring provides consistent benefits over standard training (\(0.5\%-1\%\) on average), it is clear that the growing diversity of \(P_{(,)}\) is not fully leveraged. _This observation is in contrary to the insights from existing works, which recommend the use of the entire train data as the reference set for maximal benefits._ It is also worth noting that we utilize a single random reference (from the respective sets) to perform inference. This naturally raises the question if a more sophisticated inference protocol circumvent this limitation that we notice in anchored models.

### Can the choice of inference protocol improve the performance of anchored models?

From existing works on anchoring, we find that different inference protocols can be used to elicit improvements in uncertainty quantification and model extrapolation. For instance, Thiagarajan _et al._ employed a reference marginalization strategy that samples \(K\) random references from the reference set to obtain \(K\) independent predictions for a given input (similar to MC-dropout or deep ensembles). This is followed by computing the prediction average along with its standard deviation, wherein the latter was interpreted as an estimate of epistemic uncertainty. The intuition is that different reference-residual combinations can lead to slightly different predictions for test sample that has not been observed during training, and marginalizing across references can offer robustness. On the other hand, Netanyahu _et al._ introduced the bilinear transduction (BLT) protocol for performing extrapolation from unseen data regimes in regression tasks. It was found that generalizing to an "out of support" (OOS) sample \(_{t}\) (i.e., no evidence of observing such a sample in the training data) can be made more tractable by carefully choosing anchors \(} P_{}\) such that \(_{t}-}=} P_{}\). It was argued that, even if the specific combination of \([},_{t}-}]\) may not be observed during training, the anchored model can produce better calibrated predictions when \(} P_{}\) and \(} P_{}\). This is in contrast to , which hypothesized that when the tuple \([},_{t}-}] P_{(,)}\), the inconsistency in the prediction will manifest as epistemic uncertainties. However, neither of these clearly answer the impact of inference protocol choice on generalization performance, particularly when the reference

Figure 1: **Impact of reference set size on anchored training performance. With increase in reference set size, anchoring explores more diverse combinations of reference-residual pairs with the hope of demonstrating improved generalization performance. Surprisingly, the existing anchored training protocol does not effectively leverage this diversity even with increased reference set size albeit providing improvements in accuracy over standard training. We propose reference masking, a simple regularization strategy for training anchored models that recovers the lost performance.**

set diversity is high. To answer this, we conducted a systematic evaluation of these protocols with anchored models trained on CIFAR100 with the reference set \(=\).

_Setup_. We consider three evaluation protocols to make predictions for the CIFAR100C benchmark (i) \(\) Random, that utilizes a single reference (e.g., average of samples in \(\)) to obtain predictions; (ii) \(K\) Random that utilizes \(K\) random references followed by reference marginalization (\(K=10\) in our case); (iii) BLT that searches for the optimal reference in \(\) for each test sample. Since conducting such an exhaustive search can be expensive for bigger datasets, we pick a subset (set to \(50\) in our experiment).

_Observations_. The table in Figure 2 provides the average accuracies obtained from these inference protocols. Interestingly, while these protocols incurs varying inference times (column 3) (BLT \(>>K\) random \(>1\) random), their accuracies are statistically similar to each other (averaged across multiple seeds). _This observation implies that that the limitation of anchored training cannot be fixed through sophisticated inference protocols_. This motivates us to revisit anchoring and investigate if its behavior can be systematically improved during training itself.

## 3 Improving Anchored Training via Reference Masking Regularization

A close examination of anchored training reveals a critical limitation. As the size of the reference set increases, the number of reference-residual pairs grows combinatorially. For example, when \(=\), there are \(|}{2}\) possible pairs, making it impractical to explore all pairs within a fixed number of training iterations. This results in insufficient sampling of \(P_{(,)}\), increasing the risk that anchored training may overlook the reference and make predictions based solely on the residuals. Such non-generalizable shortcuts are problematic because a sample should not be identifiable without considering the reference. Therefore, it is crucial to enhance anchored training by more effectively utilizing the diversity present in large reference sets.

### Reference Masking Regularization

We propose a novel, yet simple regularization strategy for improving anchored training. Formally, for a given tuple \([},-}]\), and a user specified probability \(\) that controls how often the training is regularized, reference masking zeroes out the reference and keeps the residual fixed to obtain \([,-}]\). For comparison, the tuple for the same sample \(\) but with a "zero" reference (Note: zero vector/image can be a valid reference in our reference distribution) corresponds to \([,-]\). In order to preserve the integrity of the anchoring mechanism, we systematically discourage the model from making meaningful predictions when the reference is masked. This can be implemented by mapping randomly masked tuples to high-entropy predictions (i.e., uniform probabilities). We achieve this by minimizing the cross-entropy loss between the

Figure 3: PyTorch style pseudo code for our proposed approach.

Figure 2: **Impact of the choice of inference protocol on the performance of anchored models . (Left) A single random reference is chosen for sample prediction; (Middle) Obtaining predictions using K random references followed by averaging; (Right) Bilinear Transduction that identifies the optimal reference for each sample. We find that, while these protocols have varying computational complexities (time (s)/1000 samples), there are no apparent gaps in the performance, indicating that the limitation of anchored training cannot be fixed through sophisticated inference protocols.**

predictions from the masked tuple and the uniform prior \(\) over \(C\) classes (i.e, probability of any class \(=1/C\)). Figure 3 provides the algorithm our proposed approach.

Circling back to Figure 1, we observe that the proposed regularization significantly improves generalization accuracies compared to standard and original anchored training. This clearly demonstrates our strategy's effectiveness in leveraging the diversity in \(P_{,}\). Following the insights from the previous section, we use the simple \(1\) random inferencing protocol to obtain predictions for test samples. At low anchor set sizes (\(|| 50\)), there is high likelihood of exposing the model to all possible combinations of samples and references, and hence the risk of learning such shortcuts is minimal. In such a scenario, overemphasizing the masking-based regularization (i.e., high \(\)) leads to underfitting, as illustrated in Figure 1. Unsurprisingly, reducing the masking probability can circumvent this underfitting behavior, as evidenced by the original anchored training, where \(=0\). However, the benefits of our regularization become apparent at larger reference set sizes. Additionally, the table in Figure 2 demonstrates that our approach performs similarly to the original anchored training, thereby implying no discernible impact on the inference efficiency.

### Analysis

**How does the accuracy landscape look like?** We hypothesize that the improved generalization of anchoring stems from the training process itself, which inherently enables the model to find better solutions in the weight space. To validate this, we follow the analysis in , where it was shown that that a well-generalizable solution is typically associated with a wider or flatter local optima in the loss/accuracy landscape. To this end, following the open-source implementation from , we obtained 2D heatmaps of accuracy evaluated on the CIFAR100C benchmark over different weight perturbations from the local minima inferring using different training strategies. Figure 4 visualizes the accuracy landscapes, where the \(x\) and \(y\) axes represent the co-ordinates that correspond to the different weight realizations. It can be observed that our approach produces wider and flatter optima in comparison to the baselines, thus explaining the generalization behavior.

Figure 4: **Impact of the proposed regularizer on anchored training. Using the CIFAR100C accuracy landscape, i.e., 2D heatmaps of the parameter space, we find that our approach identifies flatter and wider optima, thus leading to improved generalization **

Figure 5: **Analysis of Anchored Models. Using evaluations on the CIFAR100C OOD generalization of ResNet18 models trained on CIFAR100, we study the behavior of the proposed approach when combined with data augmentation protocols (left) and in presence of training label noise (right).**

**Can anchoring be combined with data augmentations?** Using synthetic data augmentations during training is a widely adopted method for improving generalization of vision models. In this study, we investigate if anchoring can be utilized alongside existing augmentation protocols, including state-of-the-art techniques like PixMix ), and if the observed generalization improvements persist. Table 4(a) shows the CIFAR100C accuracies of models trained with different augmentation protocols. Note that, the architecture and the hyper-parameters of the augmentation protocols were fixed to be the same for a fair comparison. Remarkably, our approach consistently provides performance gains regardless of the augmentation protocols used, evidencing its utility as a generic training technique.

**Does training label noise impact anchoring?** In practice, we construct the reference set \(\) for anchored training. However, under label noise, a fraction (or all) noisy samples can be included in the reference set, and get used for obtaining relative representations. A natural question is if this will impact the anchored training; however, we remind that the tuple construction in anchoring does not use the target label of a reference, and the benefits of anchoring will persist even under label noise corruptions. We validate this using the following experiment: We randomly flip the labels of \(l\%\) (\(l=\{0.5,1,2,5,10,15,20\}\)) of training samples before training a ResNet18 model on CIFAR100, and evaluate the generalization performance on CIFAR100C. Figure 4(b) illustrates that, with increasing levels of label noise, the anchored models do not demonstrate any additional challenges in handling label noise. In fact, it provides superior generalization (\( 4\%\) improvements at 20% label noise) when compared to the standard and vanilla anchored training protocols.

## 4 Experiments

In this section, we empirically demonstrate the effectiveness of our proposed strategy in training models of varying scales (ResNets, Transformers) on datasets of different complexities (CIFAR10, CIFAR100, ImageNet). We systematically evaluate the generalization of these models under natural covariate shifts and synthetic corruptions. Additionally, we perform a comprehensive evaluation of model calibration, anomaly rejection, and robustness of task adapters in an effort to assess the safety of anchored models. For all experiments in this section, we utilize the entire training dataset as the reference set and train both the original and the proposed anchored models. During inference, we randomly select a single reference from the reference set and perform evaluation on the different test datasets.

**Training Datasets.** (i) CIFAR-10 and (ii) CIFAR-100  datasets contain \(50,000\) training samples and \(10,000\) test samples each of size \(32 32\) belonging to \(10\) and \(100\) classes, respectively; (iii) ImageNet-1K  is a large-scale vision benchmark comprising \(1.3\) million training images and \(50,000\) validation images across 1000 diverse categories.

**Architectures.** We utilize a suite of vision transformer and CNN architectures with varying levels of structural and parameter complexity. Specifically for training with ImageNet, we consider SWINv2-T (\(28.4\)M params), SWINv2-S (\(49.7\)M), SWINv2-B (\(87.8\)M)  and ViT-B-16 (\(86.6\)M) . For CIFAR100, we use ResNet-18 (\(11.7\)M)  and WideResNet40-2 (\(2.2\)M)  architectures, and ResNet-18 for CIFAR10. We provide the training recipes adopted for our models in Section A.3.

**Choice of \(\)**. Through extensive empirical studies with multiple architectures, we found using the masking schedule hyper-parameter \(=0.2\) (corresponds to every \(5^{}\) batch in an epoch), leads to stable convergence (closely match the top-\(1\) validation accuracy of standard training) on ImageNet and \(=0.25\) for CIFAR10/100. Note that, our approach performs reference masking for an entire batch as determined by \(\). We have included our analysis on the impact of choice of \(\) in Section A.1.

### Generalization to Covariate Shifts and Synthetic Corruptions

**OOD Datasets and Evaluation Metrics**. For models trained on CIFAR10, we evaluate generalization on CIFAR10C and CIFAR10C. While the former contains \(19\) different types of corruptions (e.g., noise, blur, weather, digital), CIFAR10C comprises \(10\) types of synthetic noise, at 5 different severity levels respectively. Equivalently, for CIFAR100, we use the CIFAR100C and CIFAR100C benchmarks. For ImageNet-1K, we consider (i) ImageNet-C  with \(19\) natural image corruptions across \(5\) severity levels, (ii) ImageNet-C  with \(10\) noise corruptions across \(5\) severity levels; (iii) ImageNet-R  containing different renditions of \(200\) classes from ImageNet; (iv) ImageNet-S comprising black and white sketch images from each class of ImageNet. We use the top@1 accuracy to evaluate generalization performance.

**Results and Discussions**. First, in Table 1, we report the averaged accuracy over all corruptions for every severity level on the CIFAR10C/C, CIFAR100C/C datasets, for the conv-nets trained on CIFAR10/100 respectively. We make a key finding that our proposed approach leads to significant gains in corruption accuracies across all severity levels over standard training (\(1.54\%-8.54\%\)) on an average. When compared to CIFAR10, the improvements of anchoring are apparent even at lower severity levels, for e.g., \(+3.74\) improvement with WRN 40-2 at CIFAR100C severity level 1.

Second, as shown in Table 2, we investigated the efficacy of anchored transformers trained on the large-scale ImageNet-1K dataset in terms of OOD generalization. It can be observed that our proposed approach consistently yields improvements in corruption accuracies over standard training across all architectures. A striking observation is that network capacity plays a significant role in effectively leveraging the increased diversity produced by anchored training (we used the entire ImageNet-1K as the reference set). For example, as we move from SWINv2-T (\(28.4\)M) to SWINv2-B (\(88\)M), we observe increasingly larger performance gains over standard training. Importantly, our proposed strategy handles high noise severity better, achieving improvements of \(2\%-7\%\) at severity 5 for both

    &  &  &  &  \\   & & & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** \\   &  & Standard & \(95.15\) & \(89.44\) & \(83.47\) & \(77.91\) & \(70.74\) & \(58.72\) & \(86.86\) & \(81.97\) & \(74.51\) & \(65.94\) & \(60.31\) \\  & & Vanilla Anchoring & \(94.92\) & \(88.99\) & \(84.28\) & \(79.16\) & \(72.09\) & \(59.82\) & \(87.04\) & \(82.79\) & \(75.00\) & \(66.73\) & \(61.52\) \\  & & Proposed & \(95.72\) & \(90.98\) & \(87.15\) & \(83.17\) & \(77.81\) & \(67.26\) & \(89.24\) & \(85.38\) & \(78.34\) & \(70.33\) & \(65.43\) \\  & & \(\) & \(+0.57\) & \(+1.54\) & \(+3.68\) & \(+5.26\) & \(+7.07\) & \(+8.54\) & \(+2.38\) & \(+3.41\) & \(+3.53\) & \(+4.40\) & \(+5.12\) \\   &  & Standard & \(77.6\) & \(65.56\) & \(56.77\) & \(51.25\) & \(45.47\) & \(34.13\) & \(62.0\) & \(54.08\) & \(44.98\) & \(36.55\) & \(32.27\) \\  & & Vanilla Anchoring & \(77.21\) & \(65.67\) & \(57.35\) & \(52.02\) & \(45.27\) & \(34.79\) & \(61.05\) & \(54.17\) & \(44.98\) & \(36.90\) & \(32.72\) \\  & & Proposed & \(77.89\) & \(67.0\) & \(59.51\) & \(54.88\) & \(47.8\) & \(36.66\) & \(64.75\) & \(58.10\) & \(49.78\) & \(41.42\) & \(36.81\) \\  & & \(\) & \(+0.29\) & \(+1.44\) & \(+2.74\) & \(+3.63\) & \(+4.21\) & \(+4.53\) & \(+2.47\) & \(+4.02\) & \(+4.89\) & \(+4.87\) & \(+4.54\) \\   &  & Standard & \(75.48\) & \(62.26\) & \(52.82\) & \(46.85\) & \(40.12\) & \(30.05\) & \(60.09\) & \(52.89\) & \(44.44\) & \(35.78\) & \(31.06\) \\  & & Vanilla Anchoring & \(76.67\) & \(64.55\) & \(53.54\) & \(49.43\) & \(24.84\) & \(23.75\) & \(61.59\) & \(54.42\) & \(45.50\) & \(36.12\) & \(31.11\) \\  & & Proposed & \(77.03\) & \(60.0\) & \(57.77\) & \(52.33\) & \(46.64\) & \(35.52\) & \(63.83\) & \(57.76\) & \(49.32\) & \(40.26\) & \(35.29\) \\  & & \(\) & \(+1.55\) & \(+3.74\) & \(+4.95\) & \(+5.48\) & \(+5.52\) & \(+5.47\) & \(+3.24\) & \(+4.87\) & \(+4.88\) & \(+4.48\) & \(+4.23\) \\  

Table 1: **Generalization performance of CNNs trained on CIFAR10/100**. We report the ID test and the OOD (CIFAR10 -C/C, CIFAR100 -C/C) accuracies of standard and anchored CNNs to evaluate generalization (\(\)). Note, we provide the difference (\(\)) between the proposed and the standard model in each case with blue.

    &  &  &  &  \\   & **Standard** & **Proposed** & \(\) & **Standard** & **Proposed** & \(\) & **Standard** & **Proposed** & \(\) & **Standard** & **Proposed** & \(\) \\  ImageNet (val) & \(82.07\) & \(82.03\) & \(-0.04\) & \(83.71\) & \(83.68\) & \(-0.03\) & \(81.07\) & \(80.76\) & \(-0.31\) & \(84.11\) & \(84.09\) & \(-0.02\) \\  ImageNet-R & \(40.84\) & \(41.17\) & \(+0.33\) & \(45.17\) & \(46.63\) & \(+1.46\) & \(44.06\) & \(46.39\) & \(+2.33\) & \(45.7\) & \(48.16\) & \(+2.46\) \\  ImageNet-S & \(27.08\) & \(27.68\) & \(+0.69\) & \(32.25\) & \(33.3\) & \(+1.05\) & \(29.4\) & \(33.0\) & \(+3.60\) & \(31.91\) & \(33.34\) & \(+1.43\) \\  ImageNet-C (Sev. 1) & \(71.63\) & \(72.13\) & \(+0.50\) & \(74.48\) & \(74.7\) & \(+0.22\) & \(72.37\) & \(72.52\) & \(+0.15\) & \(74.45\) & \(76.24\) & \(+0.79\) \\ ImageNet-C (Sev. 2) & \(64.89\) & \(65.71\) & \(+0.82\) & \(68.8\) & \(69.12\) & \(+0.32\) & \(66.57\) & \(67.38\) & \(+0.81\) & \(68.55\) & \(69.63Imagenet-C and C. All these observations clearly evidence the importance of leveraging the diversity of \(P_{(r,)}\) for enhanced generalization. Finally, we observe from Tables 1 and 2 that anchored training maintains competitive, and in a few cases, improved ID accuracies compared to standard training.

### Assessing Safety of Anchored Models

**Calibration and Anomaly Rejection**. While generalization is key to improve model utility, it must be ensured that the models are not over-confident on unknown inputs and produce well-calibrated prediction probabilities that match the likelihood of correctness. Hence, measuring calibration  is vital to understand how tempered the model predictions are under distribution shifts. On the other hand, when the inputs are semantically disconnected and do not share the same label space as the training data, we require the models to appropriately flag them as anomalies. To that end, we also conduct an extensive evaluation of model calibration under distribution shifts and anomaly rejection. For the former, we use the ImageNet-C/C/R/S variants, and for the latter, we consider two benchmarks: (a) Vision OOD, comprising commonly used anomaly rejection datasets - _iSUN_, _Textures_, and _Places365_; and (b) _NINCO_, a recent benchmark containing images with semantic overlap with ImageNet but with no class overlap. Following standard practice , we use the Smoothed ECE metric  to assess calibration. For anomaly rejection, we obtain the energy scores  for both ID validation and OOD data, and report the AUROC metric.

We report the anomaly rejection and calibration performance of of transformers trained with ImageNet-1K. We compare the anomaly rejection performance against standard training using common vision OOD benchmarks (Textures, Places365, and iSUN datasets) and the more recent NINCO dataset. For evaluation, we consider the AUROC (\(\)) metric. Moreover, we also provide Smoothed ECE scores (\(\)) (mean, std) across different Imagenet corruption benchmarks. We highlight the best performing model in each case with blue.

**Robustness to Task Adaptation**. Evaluating model adaptation under task shifts  becomes important to shed light onto the quality and re-usability of features inferred in a backbone network. To that end we employ two evaluation protocols: Adaptation(ID Eval) and Adaptation (OOD Eval). In the former, we assume that the distribution of the dataset used for linear probing is the same as that of the test set. In the latter, we first train the linear probe (LP) with our anchored training approach using a probing dataset but evaluate the same with data drawn from a shifted w.r.t the probing dataset. Note, for both evaluation protocols, we fix the ViTb16 architecture as the Imagenet pre-trained feature extractor backbone. Note, we set \(=0.4\), a higher value than the original task model training as we observed stable convergence.

**Adaptation (ID Eval)**: We consider the following target datasets: (i) CIFAR-10  ; (ii) CIFAR-100  ; (iii) UCF101 ; (iv) Flowers102 ; (v) StanfordCars . The results in Figure 6(a) demonstrate that the proposed approach achieves substantial performance gains over the baseline (\(0.81\%\) - \(2.68\%\)). These findings suggest that the reference masking regularizer yields feature representations that are transferable even under complex task shifts.

    &  &  &  &  \\    & & \(76.54\) & \(77.46\) & \(0.121 0.034\) \\   & Proposed & \(\) & \(\) & \(\) \\   & Standard & \(77.13\) & \(74.73\) & \(0.126 0.039\) \\  & Proposed & \(\) & \(\) & \(\) \\   & Standard & \(\) & \(65.98\) & \(0.109 0.037\) \\  & Proposed & \(76.88\) & \(\) & \(\) \\   & Standard & \(75.89\) & \(72.13\) & \(0.132 0.055\) \\  & Proposed & \(\) & \(\) & \(\) \\  

Table 3: **Anomaly rejection and calibration performance of transformers trained on ImageNet-1K. We compare the anomaly rejection performance against standard training using common vision OOD benchmarks (Textures, Places365, and iSUN datasets) and the more recent NINCO dataset. For evaluation, we consider the AUROC (\(\)) metric. Moreover, we also provide Smoothed ECE scores (\(\)) (mean, std) across different Imagenet corruption benchmarks. We highlight the best performing model in each case with blue.**

**Adaptation (OOD Eval)**: For training linear probes, we use the DomainNet , comprising of images from \(345\) categories across six diverse domains. Specifically, we pick four domains, namely _real_, _sketch_, _clipart_, and _painting_ and train probes on (i) images from the _real_ domain, and (ii) images from the _sketch_ domain respectively. We then evaluate the LPs on the remaining three held-out domains. As Figure 6(b) illustrates, our proposed reference masking continues to substantially outperform standard training baseline on all held-out domains under both configurations. We attribute this behavior to our approach being able to effectively leverage the diversity in the reference-residual space to produce robust and better generalizable features supporting transferability.

## 5 Related Work

**Anchoring in Predictive Models**. Our work is based on the principle of anchoring first introduced in  where it was used to achieve stochastic data centering for epistemic uncertainty estimation. Since then, the anchoring has been extended to a variety of use-cases and applications. For e.g, Netanyahu _et al._ utilized anchoring for extrapolating to unseen data regimes  in regression settings and Trivedi _et al._ employed the same for graph neural network calibration. In contrast, our paper is the first to explore and facilitate the utility of anchoring as a viable training protocol for large scale vision models.

**Data Augmentations**. Augmentation strategies enforce models to be robust under different pixel-space manipulations improving generalization. For e.g., strategies such as Augmix  or random convolutions (RandConv)  are known to improve generalization. Recent advancements in the field include strategies such as PixMix , which utilizes an external dataset with complex image patterns to augment the training data, and ALT , which learns adversarially robust augmentations. While the idea of enforcing prediction consistency in anchoring might appear similar to training with synthetic data augmentations, we emphasize that anchoring does not alter the data (e.g., with perturbations or geometric transformations) but only creates relative representations for each sample with respect to different reference choices. Furthermore, it can be combined with data augmentations to achieve further gains in generalization (Table 4(a)).

**Model Safety**. As models are being increasingly adopted in a variety of sensitive applications [38; 39], safe model deployment has become critical [40; 41]. In this context, generalization to data beyond the training distribution [42; 6], ability to accurately detect anomalies in the input data [43; 26; 44] as well producing calibrated prediction probabilities [21; 3] are all important facets of safety evaluation. Hendrycks _et al._ argued that most existing training strategies compromise for one safety objective to satisfy another objective, thus limiting their real-world utility. We find from our experiments that anchoring jointly produces better generalization, calibration and anomaly rejection properties, which makes it a promising choice for practical deployment.

Figure 6: Assessing anchored and standard pre-trained ImageNet backbones on robustness to task shifts.

Conclusion

Through this work, we showed that anchoring leads to significant performance gains in generalization and other safety metrics, including calibration, anomaly rejection, and task adaptation, across varying dataset sizes (CIFAR-10 to ImageNet) and model architectures (Conv-Nets to Transformers). Notably, when the training recipe includes high-capacity architectures or advanced mechanisms, our method yields even greater performance gains over the base models. Our observations suggest that anchored training with larger reference sets requires reference masking regularization to control the risk of learning undesirable shortcuts while making predictions. However, we realize that state-of-the-art results in OOD generalization are often obtained using model souping  or by fine-tuning large scale pre-trained models . Hence, we believe it will be valuable to integrate anchoring into these approaches. While we have not theoretically characterized the generalization of anchored models, our hypothesis is rooted in existing theory and our empirical results provide evidence for the hypothesis. Finally, it must be noted that anchoring is a domain-agnostic, architecture-agnostic, and task-agnostic training strategy for deep neural networks. However, developing a theoretical understanding of anchored models and understanding its benefits in domain-specific applications is crucial and forms an important future direction.