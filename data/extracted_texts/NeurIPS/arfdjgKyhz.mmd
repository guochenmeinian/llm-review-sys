# Human-like multiple object tracking through occlusion via gaze-following

Benjamin Peters1

School of Psychology and Neuroscience, University of Glasgow, UK

Eivinas Butkus1

Department of Psychology, Columbia University, New York, NY 10027

Nikolaus Kriegeskorte

N.kriegeskorte@columbia.edu

Zuckerman Mind Brain Behavior Institute and Departments of Psychology, Neuroscience, and Electrical Engineering, Columbia University, New York, NY 10027

###### Abstract

State-of-the-art multiple object tracking (MOT) models have recently been shown to behave in qualitatively different ways from human observers. They exhibit superhuman performance for large numbers of targets and subhuman performance when targets disappear behind occluders. Here we investigate whether human gaze behavior can help explain differences in human and model behavior. Human subjects watched scenes with objects of various appearances. They tracked a designated subset of the objects, which moved continuously and frequently disappeared behind static black-bar occluders, reporting the designated objects at the end of each trial. We measured eye movements during tracking and tracking accuracy. We found that human gaze behavior is clearly guided by task relevance: designated objects were preferentially fixated. We compared human performance to that of cognitive models inspired by state-of-the-art MOT models with object slots, where each slot represents the model's probabilistic belief about the location and appearance of one object. In our model, incoming observations are unambiguously assigned to slots using the Hungarian algorithm. Locations are tracked probabilistically (given the hard assignment) with one Kalman filter per slot. We equipped the computational models with a fovea, yielding high-precision observations at the center and low-precision observations in the periphery. We found that constraining models to follow the same gaze behavior as humans (imposing the human-measured fixation sequences) best captures human behavioral phenomena. These results demonstrate the importance of gaze behavior, allowing the human visual system to optimally use its limited resources.

1

Human dynamic object vision, multiple object tracking, visual occlusion, eye movements, computational modelling

## 1 Introduction

Human vision parses the world into representations of objects. These representations persist as objects in the world move, leave our field of view or disappear behind occluding objects. This ability emancipates vision from the immediate sensory input and enables us to see the world in terms of its physical constituent components, providing a basis for prediction and successful action. Such robust and efficient inference of task-relevant objects from thesensorium is a challenge for machine vision and so far lacks clear solutions (Peters and Kriegeskorte, 2021; Greff et al., 2020).

Human vision may, in part, derive its efficiency and robustness from targeted sampling of the environment through foveation (Gegenfurtner, 2016). Understanding inference-driven human foveation, i.e., why people look where, may inspire novel robust and data-efficient machine vision models. Understanding (and thus being able to predict) human foveation (Kummerer and Bethge, 2023) is also of high interest for applications like computer graphics (Padmanaban et al., 2017), accessibility (Ward and MacKay, 2002), UX design (Bylinskii et al., 2017), and relevant for the diagnostics of neurological and psychiatric disorders (Liu et al., 2021).

Multiple object tracking (MOT) is a class of tasks that directly taps into a system's ability to stably represent task-relevant objects in a dynamic visual input. MOT tasks have a long tradition in cognitive science (Pylyshyn and Storm, 1988; Scholl and Pylyshyn, 1999) and machine learning. We recently compared state-of-the art MOT models (Bewley et al., 2016; Wojke et al., 2017; Zhang et al., 2022; Cao et al., 2022) to humans on a novel task designed to take steps toward bridging the gap between the real-world complexity of machine learning tasks and the abstraction of cognitive tasks (Peters et al., 2022). The task combined object recognition demands, visual occlusions and tracking. We observed that models displayed qualitatively different behavior from humans: their performance was independent of the set size, i.e. the number of objects, but deteriorated below human performance when objects were subject to extended periods of full occlusions.

This study investigates whether the qualitative difference in behavior between models and humans may be due to the fundamental differences in how models and humans sample the world. In contrast to standard machine vision models, human vision obtains high-precision observations at the fovea and low-precision observations in the periphery, actively sampling the environment to build a visual representation of the world that is useful for current behavioral goals. Human gaze behavior has been shown to be highly relevant for successful object tracking (Fehd and Seiffert, 2008; Zelinsky and Neider, 2008; Hyona et al., 2019) and individual gaze behavior is predictive of MOT task performance (Upadhyavula and Flombbaum, 2020).

We here equip a computational model, inspired by machine learning multiple object tracking models, with two features of human vision. We first introduce observation noise to the model, lowering the fidelity of the visual input. We then equip the model with a fovea and constrain the model to follow the gaze trajectories of humans performing the same trial. We find that both features lead to more human-like performance as a function of set size and occlusion.

## 2 Related work

### Cognitive science

Previous work in cognitive science has modeled human multiple object tracking behavior in highly abstracted tasks, where objects have the same appearance (i.e., dots without texture) and are visible throughout the motion period (i.e., no occlusion) (Zhong et al., 2014; Vul et al., 2009; Srivastava and Vul, 2016). E.g., Vul et al. (2009) used a probabilistic slot-model (i.e., particle filter) to explain human multiple-object tracking behavior as a function of the number of objects, velocity, and object distance. Srivastava and Vul (2016) incorporated an attentional controller into the particle filter that modulates the attentional gain of internal object representations, explaining moment-by-moment spatial precision of tracked object representations. Here, we use gaze behavior to align the beliefs of computational models with those of human observers. Similar to Upadhya and Flombaum (2020), who could explain individual differences in MOT performance by "following" the gaze of individual participants, we implement gaze-following by incorporating fixation distance-dependent noise. Our work goes beyond the highly abstracted tasks typically employed in cognitive science, incorporating object appearance information, and modeling how active sampling of positions and appearances of objects may help maintain object representations through extended periods of occlusion.

### Machine learning

A central area of machine vision research is the development of real-time online methods to, e.g., track multiple objects in videos (Luo et al., 2021; Bewley et al., 2016; Wang et al., 2020). This may require online mechanisms that selectively process and combine certain aspects of the available information. Such attention mechanisms have been employed in computer vision (Guo et al., 2022; Sun et al., 2021; Meinhardt et al., 2022; Zeng et al., 2022), including spatial attention mechanisms (akin to overt and covert attention in humans) (Larochelle and Hinton, 2010; Mnih et al., 2014; Eslami et al., 2016) and in tracking tasks (Denil et al., 2012; Kahou et al., 2017; Kosiorek et al., 2017, 2018). While advancements in hardware and architectures (Vaswani et al., 2017) may eventually admit online processing of the full input for standard fixed camera MOT benchmarks (Milan et al., 2016), spatial sampling will remain highly relevant in areas like active robot vision (Zeng et al., 2020).

## 3 Methods

### Model

The computational model is a slot-based model, which tracks object identities over time by maintaining and updating beliefs about the position and appearances of objects in the scene (Fig. 1). The model is inspired by modern multiple object tracking models from machine learning like SORT (Bewley et al., 2016) and DeepSORT (Wojke et al., 2017). SORT is a simple and fast algorithm that for each of \(N\) objects, tracks a Gaussian belief about the object's location and size (represented as a bounding box) and its velocity. On each frame, \(M\) new observations are detected via an object detection network. These observations are then assigned to the current tracks (slots) such that the overall distances between observations and tracks is minimized by the assignments (using the Hungarian algorithm). Object beliefs are then updated using Kalman filtering. DeepSORT extends SORT with a deep appearance-based association metric. On each frame, a crop is extracted for each observation from the detected bounding box and embedded into latent space that was optimized to disambiguate tracked objects by their appearances. Each slot then, in addition to the Gaussian belief about the position, keeps a library of embeddings that have been previously associated with the track. The assignment of observations to tracks is then obtained by minimizing a combined cost of position-related beliefs and observations, as well as the distance between observed embeddings and the memory of past embeddings.

#### 3.1.1 Base model

On the first frame, the model initializes a series of tracks (slots), one for each detection, representing the objects to be tracked. Each track represents the belief about an object's position and appearance. In particular, the \(i\)-th object's position state at time \(t\) is an eight-dimensional variable (bounding box center, height, aspect ratio, and their velocities) and the belief over this variable is represented as Gaussian distribution with mean \(_{i,t}^{}\) and covariance \(_{i,t}^{}\). Similarly, the object's appearance is represented as a Gaussian belief in a 128-dimensional deep embedding space. Mean \(_{i,t}^{}\) and covariance \(_{i,t}^{}\) are the empirical moments computed over the past 10 embeddings associated with track \(i\) at time step \(t\).

Assignment of tracks to new observations is performed by minimizing an assignment cost using the Hungarian algorithm. We use the negative log-likelihood of position and appearance observations under the tracked beliefs about position and appearance. For position, we project the Gaussian filter belief about the track \(i\) into the observation space forming the prediction \((}_{i,t}^{},_{i,t}^{})\). In the base model (without observation noise), the uncertainty about the object's location in observation space \(_{i,t}^{}\) is purely a function of the belief uncertainty: \(_{i,t}^{}=}_{i,t}^{} ^{}\). Similarly, in the base model, the prediction of appearance is purely a function of the belief uncertainty about appearance \((_{i,t}^{},_{i,t}^{})\).

The position and appearance costs, \(d^{}(i,j)\) and \(d^{}(i,j)\) (see Appendix A for details), of associating track \(i\) with detection \(j\) are then the negative log-likelihoods of observed position and appearance of detection \(j\) that are combined into a single cost \(c(i,j)\) using \(\)

Figure 1: **Tracking model. The base tracking model is a slot-based tracking-by-detection model. See text for details.**

\[c(i,j)=(1-)d^{}(i,j)+ d^{}(i,j)\] (1)

We set \(=0.056\) to roughly match the negative log-likelihood cost scales for position and appearance, but we did not fit it to human behavior. After an observation is assigned to a track, the track updates its belief about the position using the Kalman update and incorporates the new embeddings into its embedding library.

#### 3.1.2 Observation noise model

The observation noise model adds constant normally distributed noise to the position detections \(}_{j,t}^{}\) and incoming embeddings \(}_{j,t}^{}\):

\[_{j,t}^{} =}_{j,t}^{}+(, }^{2}}{_{j,t,}^{2}})\] (2) \[_{j,t}^{} =}_{j,t}^{}+(, }^{2}}{_{j,t,}^{2}}^{ })\] (3)

Here, \(}_{j,t}^{}\) is the ground truth location of detection \(j\) (which is known to us because we control the generation of the stimuli), while \(}_{j,t}^{}\) is the embedding associated with detection \(j\) obtained by passing the bounding-box-cropped image to a neural network that extracts visual features. \(^{}\) denotes the empirically estimated covariance of the embeddings, and it is used to "color" the noise of the embeddings according to the covariance structure of the embedding space. We simulated model behavior with \(_{}=[15.0,25.0]\) and \(_{}=[0.2,0.3]\) as these approximately performed at human accuracy levels. When comparing to human behavior, we averaged across these noise levels, as they had negligible effects on the results. Since we treat the observation noise model as a special case of the fixation model, the precision values \(_{j,t,}^{2}\) and \(_{j,t,}^{2}\) are are fixed at 22.5 (which is the inverse of the intercept \(c\) from the cortical magnification formula below).

#### 3.1.3 Fixation model

Finally, we implemented an artificial fovea that spatially modulates the observation noise. Following neuroscientific literature (Dumoulin and Wandell, 2008; Rovamo and Virsu, 1979; Carrasco et al., 1995), detections in the fovea have a lower observation noise than detections in the periphery. This modification allows us to feed human eye fixation data to the model, and make the model "fixate" according to human gaze.

In practice, for each detection, we calculate its eccentricity (distance from fixation) \(E\) in visual angles. We follow vision science literature (Strasburger et al., 2011) and find the precision for appearance and position using an inverse linear function that relates eccentricity \(E\) to cortical magnification:

\[_{j,t,}^{2} =1/(b_{} E+c) n_{}\] (4) \[_{j,t,}^{2} =1/(b_{} E+c) n_{}\] (5)Slopes for position and appearance precision \(b_{}\) and \(b_{}\) are parameters in our model, while \(c=0.044\) is fixed according to plausible values from human peripheral vision literature (Strasburger et al., 2011). Intuitively, the slope parameters determine how fast the precision falls off with distance from fixation. We can recover the constant observation model by setting the slopes to 0, rendering precision independent of eccentricity \(E\). The normalization factors \(n_{}\) and \(n_{}\) ensure that average precision across the screen is the same for different slope values.

Importantly, the fixation model expects higher levels of observation noise for tracks that are far away from fixation. So it computes _track-based_ precision values for each track \(i\)\(_{i,t,}^{2}\) and \(_{i,t,}^{2}\) that are then used to make predictions for that track in position and embedding observation spaces (note that these are different from the detection-based precision values used in simulating observation noise in the detections). For instance, the uncertainty about the object's location in observation space \(_{i,t}^{}=}_{i,t}^{} ^{}+_{i,t}^{}\) becomes a function of the belief uncertainty \(}_{i,t}^{}\) and fixation-dependent observation noise \(_{i,t}^{}=}^{2}}{_{i,t,}^{2}}\).

### Task

The object-tracking task was identical for both humans and models, unless indicated differently (see Figure 2a). Each trial started with the display of either 4, 6, or 8 objects. In Phase 1, the _cueing period_, objects remained still for 1.5s. During this time, half of the objects were highlighted as target objects for human participants for 1s. Phase 2, the _motion period_, lasted five seconds, where objects moved independently on random trajectories inside the motion area. Objects moved with constant speed, and their trajectory was simulated either through a linear or more complex non-linear dynamics model (see Appendix B for details). Object speed vectors were reflected at the boundaries of the motion area. Two-thirds of all trials contained a central large rectangle ("occluder") that spanned the full vertical (horizontal) extent and either 20% or 40% of the horizontal (vertical) extent of the motion area. Object starting positions were sampled such that (1) all objects were unoccluded at the start of the trial and (2) all objects ended up in positions such that no two were closer to each other than a threshold distance. In phase 3, the _response period_, the objects stopped moving and target identification was required. Humans did so by clicking on them. If objects ended their motion hidden by the occluder, they were made visible either by making the occluder semi-transparent (for humans), or by moving their depth plane in front of the occluder (for models, which were not assumed to be familiar with semi

Figure 2: **Task**. **a** Sequence of events. **b** Experimental factors (see 3.2 for details).

transparency). Object stimuli were crops extracted from the MS Coco challenge (Lin et al., 2014) using the provided segmentation masks. For each of the 80 categories, we extracted a large set of different exemplars, excluding fragmented objects.

Humans and models were presented with 144 motion sequences, for which we varied several factors (Figure 2b). We varied the number of objects (4, 6, or 8; always half of them were targets), the extent of the occluder (no occluder, 20%, 40%), and its orientation (horizontal or vertical). Object dynamics were either linear or following a more complex motion model. Object appearances could be sampled either randomly from all 80 Coco categories or only from a single category.

### Human behavior

For the human participants (\(N=9\)), target objects were highlighted in the cueing phase with a glowing outline and a coin icon placed atop each target. In the response phase, participants selected half of the objects which they believed were the original targets. Once all responses were made, the selected objects' identities were revealed (target or non-target) by displaying an animation of the coin being collected for targets and no animation for selected non-targets. Humans were not constrained by a time limit for their responses. Trials were presented in 12 blocks of 12 sequences (trials) each. Human gaze position was recorded simultaneously using an eyetracker (see Appendix C).

### Model behavior

Following previous work (Peters et al., 2022), we provide models with ground truth bounding boxes for unoccluded objects in the current frame. This is similar to public detections in the MOT challenge (Milan et al., 2016) allowed us to focus on the tracking challenge rather evaluating the quality of the object detector. To obtain responses to objects at the end of the motion period, we determined whether a model tracked the target objects consistently throughout the whole trial. A target object was considered to be successfully tracked if its final detection was assigned to one of the target tracks of the first frame. The model then "selected" those final detections that were assigned to a target track. In case less than \(T\) detections were assigned to target tracks, the model randomly selected responses from the remaining detections (i.e., guessing) until \(T\) responses were made. We report the average accuracy over \(2 9=18\) model evaluation runs (2 independent runs for each of the 9 participant gaze data, or simply 18 runs in the case of no noise or constant noise models).

## 4 Results

### Human behavioral phenomena

We assessed the impact of the number of objects, occlusion levels, object category similarity, and trajectory complexity (see Figure 3). We found that tracking performance decreases with the number of objects tracked and the degree of occlusion, replicating previous findings in the MOT literature (Intriligator and Cavanagh, 2001; Pylyshyn and Storm, 1988; Scholl and Pylyshyn, 1999; Yantis, 1992). We also found that performance diminishes when targets and distractors belong to the same category, replicating our own previous findings (Peters et al., 2022). Moreover, tracking performance was slightly reduced when objects moved on more complex trajectories compared to linear trajectories. We were particularly interested in capturing these broad patterns of human object-tracking behavior with our computational modeling efforts.

### Models without fixation

We first modeled human behavior without any observation noise in the model, setting \(_{pos}=0\) and \(_{app}=0\). As seen in Figure 3 (blue bars, "model (no noise)"), this model outperforms humans in all conditions, yielding a qualitatively bad fit to human data.

In contrast to this model, humans have limited precision in their observations. In the next step, we therefore added observation noise to all detections (observed positions and appearance embeddings, as explained in the Methods section). This observation noise was identical (constant) across the whole visual field. We find that adding constant observation noise made the model capture human phenomena much more closely than the model without observation noise (Figure 3, green bars, "model (constant noise)"). However, state-of-the-art object tracking models tend to outperform humans when there is no occlusion and underperform when there is high occlusion (Peters et al., 2022). The constant noise model seems to still suffer from this problem (see Figure 3B).

### Introducing gaze-following makes model behavior more human-like

We hypothesized that a model equipped with a fovea and following human gaze would yield a better fit to human behavior. In particular, we observed that human gaze behavior is directly related to the task, revealing a participant's beliefs about tracked objects (see Appendix D). A qualitative inspection of the results (Figure 3, red bars, "model (fixation noise)") suggests that the fixation model does indeed better capture human behavior than the constant noise model. In particular, unlike the constant noise model, the fixation model does not seem to outperform humans in the lower occlusion settings ("None" and "0.2" in Figure 3B). To quantify the effects of introducing fixations into the model, we computed

Figure 3: **Human and model accuracy.****a.** Number of objects (half of them were targets), **b** Occlusion level (no occlusion, 20%, and 40% width of the occluder). Low and high category similarity (**c**) and trajectory complexity (**d**).

the correlation between model and human average accuracy across the 36 experimental conditions (3 levels of the number of objects \(\) 3 occlusion levels \(\) 2 levels of category similarity \(\) 2 levels of trajectory complexity) (Figure 4A). This metric reflects the extent to which the model captures key human behavioral phenomena while being invariant to overall level of performance (Figure 3).

We found that the best-performing fixation model (\(b_{}=1.0,b_{}=0.055\)) captured human phenomena significantly better than a model without fixation (i.e., \(b_{}=0,b_{}=0\)) (\(t(8)=8.29\), \(p<.001\)). In fact, every fixation model with non-zero slope for both fixation and appearance is a better fit to human behavior than any model in which either the appearance or the position slope is set to 0 (for any comparison, \(t(8)>2.33\), \(p<0.048\)); and every model with one non-zero slope is better than the zero-slope model (for any comparison, \(t(8)>3.13\), \(p<0.014\)). See Figure 4B first column/row for a visual intuition. In general, we found that model correlation to human behavior increases for higher slopes (Figure 4C), suggesting that fixations are indeed key to explaining human tracking behavior.

## 5 Limitations and future work

Our model is a slot-based tracking-by-detection algorithm, which allowed us to isolate the tracking challenge from the detection components of the task. Tracking-by-detection is a well-established paradigm in machine vision (Bewley et al., 2016; Wojke et al., 2017). However, the separation of detection from association is an oversimplification of human vision, in which detection and association across time and space occur continuously and concurrently at all stages of visual processing. Departing from the classic tracking-by-detection paradigm (e.g., Sun et al., 2021; Feichtenhofer et al., 2017; Bergmann et al., 2019) and perhaps even including less structured (i.e., non-slot) representations of the visual

Figure 4: **Human and model fit.****a** Accuracy of humans and models for each experimental condition (sorted by human accuracy in each condition). **b** Similarity between human and model behavior across experimental conditions. Note that models with at least one 0-slope (no fixation effect for observed position, appearance, or both, i.e., the last five rows/columns of the matrix) have a lower correlation to human behavior. **c** Correlation to human behavior as a function of position slope \(b_{}\) and appearance slope \(b_{}\). Models behave more human-like when increasing the slope (i.e. when increasing the effect of fixation).

world (Eslami et al., 2018; Vondrick et al., 2018) may yield a closer alignment of human and machine vision.

## 6 Conclusion

We here equipped a computational model, inspired by state-of-the-art multiple object tracking models in machine learning, with a fovea, yielding high-precision observations at the center and low-precision observations in the periphery. We found that constraining models to follow the same gaze behavior as humans (imposing the human measured fixation sequences) better captures key behavioral human phenomena compared to models without fixation. These results demonstrate the importance of gaze behavior and provide a stepping stone for building resource-efficient machine vision models that sample their environment adaptively.