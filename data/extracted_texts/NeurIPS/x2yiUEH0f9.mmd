# Probabilistic Proof State Compression: Optimizing LLM-Guided Formal Verification

Ali Rahim

Department of Mathematics

University of Rochester

aabdulra@u.rochester.edu

&Noor Rahim

Department of Computer Science

University of Colorado, Boulder

noor.sachdeva@colorado.edu

###### Abstract

Despite recent successes in LLM-guided formal proof search, scalability remains limited by the large search space. This paper introduces a novel approach that integrates off-the-shelf LLMs with conformal prediction-based compression to optimize proof search. By employing adaptive, probability-based binning informed by conformal prediction intervals, our method compresses the proof state space, reducing computational demands while retaining statistical proof guarantees. Preliminary results on the Lean miniF2F test set show similar success rates with 75% fewer passes, and on average 23% reduced wall clock time.

## 1 Introduction

Recent advances in large language models (LLMs) have significantly progressed the automation of formal proof generation. LLM-guided methods (, , ) demonstrate promising capabilities in navigating the complex search spaces of formal theorem proving, leveraging LLMs' pattern recognition and generalization strengths over traditional symbolic methods.

LLM-based formal proving typically follows either proof-step or whole-proof generation strategies. While recent systems, such as DeepSeek-Prover-V1.5 , have set state-of-the-art benchmarks using a truncate-and-resume approach, they still struggle to balance exploration and exploitation due to the sparse binary rewards of successful proofs, requiring extensive search with up to \(2^{17}\) typically.

We propose a method that enhances LLM-guided formal proof search with conformal proof state space compression. Using open-weight LLMs () for generating candidate steps, our conformal prediction framework provides calibrated success probabilities. We introduce a rigorous compression algorithm that preserves the most promising proof paths, which allows efficient exploration.

We evaluate our method on the MiniF2F and ProofNet benchmarks and demonstrate a 75% average reduction in the number of passes required compared to baseline open models. Additionally, our approach led to qualitatively simpler proofs on some examples. These results suggest promising directions for scaling automated theorem proving to more complex domains.

## 2 Proposed Method

We enhance LLM-guided formal proof search by introducing a novel _proof state space compression_ technique. Our approach integrates three key components to efficiently navigate and compress the proof search space: the LLM-based Proof Step Generator, the Conformal Prediction Module, and the Proof State Space Compression Module. Figure 1 illustrates the architecture of our system.

### LLM-based Proof Step Generator

We employ DeepSeek Prover V1.5 RL, an open-weights large language model fine-tuned on Lean 4, to generate candidate proof steps. Given the current proof state \(s=(g,a,t)\) and goal \(g\), the model produces a set of possible next steps \(\{s^{}_{1},,s^{}_{k}\}\) via repeated sampling.

### Conformal Prediction Module

The Conformal Prediction Module provides _calibrated probability intervals_ for the success of each candidate proof step, thereby guiding the proof search with reliable uncertainty estimates.

We define a nonconformity measure \(A(z,z^{})\) for a labeled proof attempt \(z=(s,y)\) as:

\[A(z,z^{})=|y-P(|\ s)|\] (1)

Here, \(P(|\ s)\) estimates the probability of proof success given state \(s\) using the same model, and \(y\) is the binary outcome (success or failure).

#### 2.2.1 Probability Interval Computation

To address the dependency in proof states and uphold the exchangeability assumption required by conformal prediction, we partition the calibration set into strata using the Proof State Space Compression Module. For each candidate step \(s^{}\), the module:

1. **Identify Stratum:** Assign \(s^{}\) to a stratum \(_{j}\) based on its similarity to existing proof states.
2. **Compute Nonconformity Scores:** Calculate \(A(z_{i},s^{})\) for all \(z_{i}_{j}\).
3. **Calculate P-Values:** For each outcome \(y\{0,1\}\), \[p(y\ |\ s^{})=,s^{}) A((s^{},y),z^{ })\}|+1}{|_{j}|+1}\] (2)
4. **Construct Prediction Region:** \[^{}(s^{})=\{y:p(y\ |\ s^{})>\}\] (3)
5. **Compute Probability Interval:** \[[L(s^{}),U(s^{})]=&^{}(s^{})=\{0,1\}\\ &^{}(s^{})=\{1\}\\ &^{}(s^{})=\{0\}\]

### Proof State Space Compression Module

The Proof State Space Compression Module manages the search space by grouping similar proof states into strata, facilitating efficient stratified conformal prediction.

Figure 1: Architecture of the Conformal Prediction-Based Theorem Proving System

#### 2.3.1 Similarity Measure

For proof states \(s_{1}=(G_{1},A_{1},T_{1})\) and \(s_{2}=(G_{2},A_{2},T_{2})\), we define:

\[(s_{1},s_{2})=w_{G}(G_{1},G_{2})+w_{A}(A_{1},A_{2})+w_{T}(T_{1},T_{2})\]

where \((G_{1},G_{2})\) is the binary goal similarity, \((A_{1},A_{2})\) is the Jaccard similarity of assumptions, \((T_{1},T_{2})\) is the normalized longest common subsequence of tactics, and \(w_{G}\), \(w_{A}\), and \(w_{T}\) are weights summing to 1.

#### 2.3.2 Binning Function

We map probability intervals to bin indices using:

\[b([l,u])= n(l,)\]

where \(n\) is the number of bins. To prioritize high-probability regions, we use adaptive bin widths:

\[w(p)=w_{0}(- p)\]

where \(w_{0}\) is the base width and \(\) controls adaptation rate.

#### 2.3.3 Compression Mapping

The compression process \(C\) maps a set of proof states to a set of representative states:

\[C(\{s_{1},,s_{k}\})=\{(B_{1}),,(B_{m})\}\]

where \(B_{1},,B_{m}\) are non-empty bins.

### Proof Search Algorithm

Our proof search algorithm integrates the three components to efficiently explore the proof space. Algorithm 1 outlines the procedure.

### Theoretical Guarantees

Our method provides theoretical guarantees on the coverage of the true probability of proof success. Specifically, for a given significance level \(\), we have:

\[P(L(s) p^{*}(s) U(s)) 1-\]

where \(p^{*}(s)\) is the true probability of a successful proof from state \(s\). This guarantee ensures that our compression technique preserves the most promising proof paths with high probability.

## 3 Experiments and Results

### Experimental Setup

#### 3.1.1 Datasets

We perform a preliminary evaluation of our method on two benchmark datasets: MiniF2F  and ProofNet . Specifically, we use the Lean 4 MiniF2F variant by Yang et al. .

#### 3.1.2 Baseline Models

We compare our method against several state-of-the-art models: GPT-4, DeepSeek-Prover-V1, and DeepSeek-Prover-V1.5.

### Results

#### 3.2.1 Performance on miniF2F

#### 3.2.2 Performance on ProofNet

#### 3.3.1 Efficiency Gains

Our method demonstrates a 75% reduction in passes, with 23% reduced wall clock time compared to DeepSeek-Prover-V1.5, This efficiency gain can be attributed to the effective pruning of the search space through our conformal prediction-based approach.

### Discussion

These preliminary results demonstrate that our method significantly improves upon existing approaches in both proof success rate and efficiency. The integration of conformal prediction with LLM-guided search allows for more effective exploration of the proof space, leading to higher-quality proofs. While promising, the approach requires further refinement and empirical validation to fully realize its capabilities. Nonetheless, it represents a meaningful step towards bridging the gap between generative AI models and statistically robust decision-making processes in formal reasoning tasks.

  Model & Pass Rate (\%) @ Number of Passes \\  GPT-4 & 23.0 @ 10 \\ DeepSeek-Prover-V1 & 50.0 @ 32 \\ DeepSeek-Prover-V1.5 & 60.2 @ 32 \\ Our Method & **63.5 @ 8** \\  

Table 1: Results on miniF2F test set

  Model & Validation Pass Rate (\%) & Test Pass Rate (\%) \\  DeepSeek-Prover-V1.5 & 21.6 & 23.7 \\ Our Method & **23.4** & **25.3** \\  

Table 2: Results on ProofNet