# On the Limitation of Backdoor Detection Methods

Georg Pichler\({}^{1}\) Marco Romanelli\({}^{2}\)

**Divya Prakash Manivannan\({}^{2}\)** **Prashanth Krishnamurthy\({}^{2}\)** **Farshad Khorrami\({}^{2}\)**

**Siddharth Garg\({}^{2}\)**

\({}^{1}\)TU Wien \({}^{2}\)New York University

###### Abstract

We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problem, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal backdoor detection is impossible, except for very small alphabet sizes. Furthermore, we link our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem, establishing a formal connection between backdoor and out-of-distribution detection.

## 1 Introduction

Safe and trustworthy Machine Learning (ML) systems remain elusive [8; 15], for reasons that are intrinsic, like poor interpretability [1; 13], and due to external threats, including inference time adversarial inputs [6; 8; 4] and training time poisoning and backdoor attacks . As the scale, complexity and training data requirements of modern deep neural network architectures has grown, many users resort to using and/or fine-tuning pre-trained models. Consequently, purposefully implanted backdoors pose a security risk for ML systems.

In the classic backdoor threat model, a malicious actor may provide poisoned data, affecting the behavior of trained ML model. For certain, _poisoned_ inputs, which are modified in a specific way, known to the attacker, the model then provides erroneous predictions. While there are many ways such a backdoor could be embedded into a model, prior work shows that poisoning even a small fraction of training data yields models with stealthy and effective backdoors . To detect a backdoor, the model user (i.e., the _defender_) has access to a, typically small, validation dataset of clean inputs. In the Model Backdoor Detection (MBD) problem [9; 7], the defender wishes to detect if the model itself contains a backdoor. In the Input Backdoor Detection (IBD) problem [10; 11], the defender wants to test if a specific test input is poisoned or not. Yet, despite several years of research, the field is still plagued by the cat-and-mouse game between attacks and defenses, with no end in sight. Unlike the work on adversarial perturbation attacks, for instance, "certifiable" defenses have remained elusive. We argue that this is in part because, despite the large body of work in the area, backdoor detection has not been formally defined, at least not in a precise and well-posed manner. This lack of formal treatment has negative consequences as it impedes fair and consistent comparison between methods.

**Contributions.** In this paper, we present the first precise statistical formulation of the MBD and IBD problems (Section 2.1). This formulation enables several new insights on backdoor detection.

(1) _Relationship to well-known statistical problems:_ Our formulation unifies MBD, IBD and even Out-Of-Distribution (OOD) detection within a common framework and we reduce these problems to standard statistical hypothesis testing problems. (2) _Infeasibility:_ Leveraging these reductions, we conclude that under realistic assumptions, universal (adversary-unaware) backdoor detection is not possible for an infinite alphabet of the training data. (3) _Bound for finite alphabet size:_ For a finitedata alphabet, we provide a bound on the achievable error probability given a fixed training set size. These bounds are evaluated for commonly used datasets in ML, showing that universal backdoor detection is only achievable for very small alphabets. (4) _Connections to Probably Asymptotically Correct (PAC) learning theory of OOD detection:_ Detecting a backdoor in training data is equivalent to a binary Neyman-Pearson hypothesis test if OOD detection is PAC learnable as defined in .

## 2 Theoretical Formulation and Results

We focus on MBD and IBD in the case where the attacker has limited control over the training data and is able to poison a certain portion of the dataset. The training itself is performed using a standard method, e.g., Stochastic Gradient Descent (SGD). For an extensive overview of other empirical backdoor problems, the reader is referred to, e.g., .

### Formulating Model Backdoor Detection (MBD)

**Overview.** After \(N\) samples of training data are collected, the backdoor attacker has the option of poisoning a portion of the training data, by replacing each _clean_ sample with a _poisoned_ sample. This poisoning may alter, e.g., images as well as their labels. Subsequently, an Artificial Neural Network (ANN) is trained on the resulting training set. Given the resulting trained network (i.e., the network parameters), the task of the backdoor detector is to determine whether the training data had been poisoned. The detector may obtain \(M\) additional _clean_ samples, e.g., by independently collecting additional data. We assume that the backdoor attacker has no access to these samples.

**Dataset and training.** Consider a, possibly stochastic, training algorithm \(\) (e.g., SGD), that trains a model on training data1\(_{N}=(X_{1},X_{2},,X_{N})\), consisting of \(N\) i.i.d. random variables, distributed like \(X P\), as input and produces a parameter vector \(=(_{N})\) as output.

**Clean data.** Let \(P_{0}()\) be the probability distribution on \(\) of clean samples and let \(_{N}^{(0)}=(X_{1}^{(0)},X_{2}^{(0)},,X_{N}^{(0)})\) be a clean dataset, consisting of \(N\) i.i.d. random variables, drawn from \(P_{0}\).

**Backdoor.** To _backdoor_ a model trained on the clean dataset, an adversary may replace some training samples with poisoned samples drawn from a different distribution \(P_{b}()\). As a training sample may include the data and the label, the adversary could change the data and label.

**Poisoned training data.** Assuming that a fraction \((0,1]\) of the training data is poisoned, the poisoned training dataset \(_{N}^{(1)}=(X_{1}^{(1)},X_{2}^{(1)},,X_{N}^{(1)})\) is independently drawn according to \(P_{1}= P_{b}+(1-)P_{0}\), i.e., according to \(P_{b}\) with probability \(\) and from \(P_{0}\) with probability \(1-\).

**Additional clean data.** Furthermore, let \(_{M}^{}=(X_{1}^{},X_{2}^{},,X_{M}^{})  P_{0}^{M}\) be \(M\) i.i.d. additional clean samples distributed according to \(P_{0}\). These samples correspond to clean validation data or may have been collected by the backdoor detector prior to making a decision.

**Model Backdoor Detection.** The backdoor detector is a function \(g\), that takes \(=(_{N}^{(j)})\) and additional data \(_{M}^{}\) as its input and outputs \(0\) for _"backdoor"_ and \(1\) for _"no backdoor"_. For MBD, we require the detector to determine \(j\) with high probability. For ease of notation, we use a Bernoulli variable \(J()\) and define the input for the detector as \(=((_{N}^{(J)}),_{M}^{})\), such that the error probability \(\{g() J\}\) of the detector is well-defined.

**Possible data distributions.** The last observation to obtain a well-defined backdoor detection problem is that we need to avoid the possibility of \(P_{0}=P_{b}\). Detection is impossible if the clean and the backdoor distributions are identical. We opt for the general approach of defining a suitable set \(()^{2}\) that contains all possible clean and backdoor distribution pairs \((P_{0},P_{b})\).

These discussions then naturally lead to the following central definition.

**Definition 1**.: _The MBD problem for a training algorithm \(\) is determined by the following quantities: \((0,1]\), \(N\), \(M\), and \(()^{2}\).__Fixing these quantities, we define the risk of a backdoor detector \(g\) associated with \((P_{0},P_{b})\) as_

\[R(g;P_{0},P_{b}):=\{g() J\}=_{j=0,1}\{g( (_{N}^{(j)}),_{M}^{}) j\}.\] (1)

_We say that a backdoor detector is \(\)-error for some \([0,]\) if, for every pair \((P_{0},P_{b})\), the risk is bounded by_

\[R(g;P_{0},P_{b}).\] (2)

_Remark 1_.: Instead of bounding the risk as in (2), it may seem more natural to require \(\{g() j|J=j\}\) for both \(j=0,1\), but note that \(\{g() j|J=j\} 2\) for \(j=0,1\) immediately follows from (2).

### (In)feasibility of Model Backdoor Detection

It will be useful to consider easier problems than \(\)-error detection, as defined in Definition 1 and establish reductions. To this end, we consider four different _Types_ of detectors. All these detectors need to infer \(J\), but different inputs are available to them:

1. \(g_{0}(_{0})\) with \(_{0}==((_{N}^{(J)}),_{M }^{})\): The default detector as used in Definition 1.
2. \(g_{1}(_{1})\) with \(_{1}=(_{N}^{(J)},_{M}^{})\): Provide the detector with the training dataset \(_{N}^{(J)}\) and \(M\) independent clean samples \(_{M}^{}\).
3. \(g_{2}(_{2})\) with \(_{2}=(_{N}^{(J)},P_{0})\): Provide the detector with the training dataset \(_{N}^{(J)}\), and with the clean data distribution \(P_{0}\).
4. \(g_{3}(_{3})\) with \(_{3}=(_{N}^{(J)},P_{0},P_{b})\): Provide the detector with the training dataset \(_{N}^{(J)}\), the clean distribution \(P_{0}\) and with the backdoor distribution \(P_{b}\). This is a binary Neyman-Pearson hypothesis testing problem between \(P_{0}^{N}\) and \(P_{1}^{N}\).

We assume that detectors of Types 2 and 3 have access to \(P_{0}\) (and \(P_{b}\) for a Type 3 detector) in terms of evaluation of the distribution, and also have the ability to sample from the distribution. We thus consider Types 2 and 3 as randomized detectors to account for sampling. The definitions of risk and \(\)-error detection of \(g_{2},g_{3}\) apply mutatis mutandis as in Definition 1, where the probability in (1) is also taken over the randomness of \(g\).

Remark 3 proposes an ordering of the Types of detectors, according to the information provided.

In Section 2.2.1 we will show that for a reasonable \(\), \(\)-error Type 2 detection is impossible with \(<\). The reduction argument in Remark 3 thus ensures that \(\)-error detection with \(<\) is also impossible for Type 0 and Type 1 detectors.

We can resolve the situation for a Type 3 detector using the Neyman-Pearson lemma.

**Lemma 1**.: _Given a Type 3 backdoor detector \(g_{3}(_{N},P_{0},P_{b})\), for any pair \((P_{0},P_{b})()^{2}\)_

\[R(g_{3};P_{0},P_{b})-(P_{0}^{N},P_ {1}^{N})-(P_{0},P_{b}),\] (3)

_where the first equality in (3) can be achieved by the Neyman-Pearson detector. Thus, an \(\)-error detector of Type 3 can only exist if \(-(P_{0},P_{b})\) for all \((P_{0},P_{b})\)._

See proof in Appendix A.3.

Before analyzing Types 1 and 2, we specify the set of allowable distributions \(\) using Lemma 1.

Merely excluding the identity \(P_{0} P_{b}\), i.e., \(=\{(P_{0},P_{b})()^{2}:P_{0} P_{b}\}\) is not sufficient.

_Example 1_.: Let \(g_{3}(_{N},P_{0},P_{1})\) be an \(\)-error Type 3 detector and assume that \(\) is infinite, i.e., \(||=\). Let \(\) be given as above, ensuring only that \(P_{0} P_{b}\). For any \(>0\), we can then choose2\((P_{0},P_{b})\) with \(0<(P_{0},P_{b})\). By Lemma 1, we have \(-(P_{0},P_{b}) -\). As \(>0\) was arbitrary, we have \(=\).

Lemma 1 and Example 1 show that even for a Type 3 detector, we need \((P_{0},P_{b})>\) for all \((P_{0},P_{b})\), in order for \(\)-error detection to be achievable. In the following we will assume that \(\) is the set of probability distributions \(P_{0},P_{b}\) with \((P_{0},P_{b}) 1-\), for some fixed \([0,1)\). This strong requirement is motivated by the fact that in this case, \(\)-error Type 3 detection is achievable with only \(N=1\) sample.

_Remark 2_.: Thorough reasoning and examples, illustrating why total variation distance is the preferred distance measure for distribution hypothesis testing can be found in [2, Section 1.2].

#### 2.2.1 Impossibility

In the following we prove an impossibility result, which implies that _for an infinite alphabet \(\), the error probability (as given in Definition 1) of any detector (of Type 0, Type 1 or Type 2) is \(\), the error probability of a random guess._ Additionally, for finite \(\), we provide a lower bound on the size of the training set \(N\), as a function of \(\).

**Theorem 1**.: _Fix \(N\), \((0,]\), \(\), and \(=\{(P_{0},P_{b}):(P_{0},P_{b}) 1-\}\). Let \(g_{2}(_{N},P_{0})\) be an \(\)-error Type 2 detector. For \(||=\), we then have necessarily \(=\), while for \(||<\), we have_

\[N+}{4}+(| |-1)}.\] (4)

See proof in Appendix A.3.

For a fixed dataset alphabet size \(||\) and allowed error probability \(\), the bound (4) gives the minimum size of the training set \(N\) for the error level \(\) to be achievable. Note the following special cases in terms of \(\), \(\): _i)_ For \(=\), the bound (4) is always satisfied as the RHS is \(0\), showing that \(\)-error detection is always achievable. This coincides with the error probability of a random guess. _ii)_ The bound (4) is monotonically decreasing in \(\) and for \( 0\), it approaches \(||\). _iii)_ In case \(=0\), the bound (4) is always satisfied as the RHS is zero for \((0,]\) in this case. This shows that \(\)-error detection is always possible if \(P_{0}\) and \(P_{b}\) have disjoint support, i.e. \((P_{0},P_{b})=1\).

For an infinite alphabet \(\), (4) needs to be satisfied for arbitrarily large values of \(||\). For finite training set size \(N\), this is only possible if \(=\) as then, \(=0\). Thus, in this case, for any Type 2 detector, there is a particular clean distribution and backdoor strategy, such that this detector performs no better than random guessing. For fixed \(\) and \(\), we can use (4) to determine the minimum size of the training set \(N\) for popular datasets, for \(\) error probability to be achievable by a Type 2 detector. To this end, we use the width \(W\), height \(H\), number of channels \(C\) and color depth \(P\) of an image dataset to compute \(||=P^{WHC}\). For categorical datasets, we may multiply the number of categories for all the properties recorded in the dataset to obtain \(||\). The resulting value for the bound in (4) is given in Table 1 for several popular datasets. As can be seen by these numbers, this universal backdoor detection is infeasible for all, but the smallest tabular datasets. Note also, that the impossibility of Type 2 backdoor detection automatically precludes the existence of Type 1 or Type 0 error detectors with equal performance by the reduction argument in Remark 3.

#### 2.2.2 Achievability

In this section we are going to show that achievability is possible and that it is related to the size of the alphabet \(||\). We consider a Type 2 detector and give a criterion for \(\)-error detection achievability:

**Theorem 2**.: _Considering the backdoor detection setup of Definition 1 with \(=\{(P_{0},P_{b}):(P_{0},P_{b}) 1-\}\) and a finite alphabet \(||<\). There exists an \(\)-error Type 2 detector if_

\[>2||(-(1-)^{2}}{| |^{2}}),\] (5)

See proof in Appendix A.3.

Note the following special cases in terms of \(\), \(\) and \(\): _i)_ For \(=0\), (5) cannot be satisfied, showing that \(0\)-error detection cannot be achieved. _ii)_ The case \(=1\) allows for \(P_{0}=P_{b}\) and thus no \(\)-error detector exists for \([0,)\) in this case and (5) cannot be satisfied. _iii)_ For \(=1\), \(P_{0}=P_{b}\) are identical, no \(\)-error detector exists for \([0,)\), and (5) cannot be satisfied.

#### 2.2.3 Connections to PAC-Learnability of OOD Detection

Note that a Type 1 detector essentially needs to solve an OOD detection problem. In this case, we are provided samples and the detector \(g_{1}\) needs to determine if the \(N\) samples \(_{N}\) were drawn from the same distribution as \(^{}_{M}\).

The goal of this section is to prove Theorem 3. This theorem has an interesting implication in case the OOD detection problem is PAC-learnable: If an \(\)-error Type 3 backdoor detector \(g_{3}\) exists, then \((+)\)-error detection is also possible for a Type 1 detector for any \(>0\). Thus, essentially Types 1 to 3 all become equivalent if OOD detection is PAC-learnable. Note here that Type 3 detection is completely characterized by Lemma 1.

The PAC-learnability of the detector in Type 1 was analyzed in . We fist restate a special case of the definition of (weak) PAC-learnability as given in [3, Def. 1].

**Definition 2**.: _For distributions \(P_{0},P_{b}\) on \(\), the OOD-risk of a function \(f:\{0,1\}\), w.r.t. the Hamming distance, is defined as_

\[(f,P_{0},P_{b}):=\{f(X^{(J)}) J\}=\{f(X^{(0)})=1\} +\{f(X^{(1)})=0\}.\] (6)

_Given a space of probability function \(\), OOD-detection is PAC-learnable on \(\) if there exists an algorithm \(_{m=1}^{}^{m}\{0,1\}^{}\) and a monotonically decreasing sequence \((m)\) such that \(_{m}(m)=0\) and for all \((P_{0},P_{b})\), and all \(m\) we have_

\[[((^{}_{m}),P_{0},P_{b})]-_{ f}(f,P_{0},P_{b})(m),\] (7)

_where the expectation is taken w.r.t. \(^{}_{m}\) and the infimum is over \(\{0,1\}^{}\), i.e., all \(f\{0,1\}\)._

Appendix A.2 shows how Definition 2 is a special case of [3, Def. 1]. We consider PAC-learnability on the \(N\)-dimensional product space, i.e., on \(^{N}\) with distributions \(P_{0}^{N},P_{b}^{N}\). We can now connect PAC-learnability to the existence of \(\)-error detectors of Types 1 and 3.

**Theorem 3**.: _Consider the setup of Definition 1, with fixed \((0,1]\), \(N\) and \(\). Let \(^{}\) be the set of \(N\)-fold products of \((P_{0},P_{1})\), i.e., \(^{}=\{(P_{0}^{N},( P_{b}+(1-)P_{0})^{N}):(P_{0},P_ {b})\}\). Then, OOD-detection is PAC-learnable on \(^{}\) if and only if the following holds for any \(>0\) and any Type 3 detector \(g_{3}(_{N},P_{0},P_{b})\): We can find \(M\) and a Type 1 detector \(g_{1}(_{N},^{}_{M})\), which satisfies \(R(g_{1},P_{0},P_{b}) R(g_{3},P_{0},P_{b})+\) for every \((P_{0},P_{b})\)._

See proof in Appendix A.3.

**Corollary 1**.: _If OOD-detection is PAC-learnable on \(^{}\), we have the following: If \(\)-error backdoor detection is possible in the easier case of Type 3 detection, which is completely characterized by Lemma 1, then \((+)\)-error detection is also possible for a Type 1 detector for any \(>0\). Consequently, the achievability of backdoor detection for Types 1 to 3 detectors are all equivalent up to topological closure if OOD-detection is PAC-learnable on \(^{}\)._

### Generalizing to Input Backdoor Detection

We generalize Definition 1 to IBD. Let \(g^{}(^{})\) take input \(^{}=(,X^{(I)})=((^{(J)}_{N}), ^{}_{M},X^{(I)})\), where a random variable \(I\) on \(\{0,1\}\) determines if \(X^{(I)}\) was drawn as \(X^{(0)} P_{0}\) (\(I=0\)) or as3\(X^{(1)} P_{b}\) (\(I=1\)). We define a general target function \(t(j,i)\{0,1\}\) and require that a backdoor detector satisfies \(g^{}(^{})=t(J,I)\) with high probability. In this case, it is beneficial to allow for an arbitrary probability distribution \(P_{JI}\) of \((J,I)\) on \(\{0,1\}^{2}\). This leads to the following definition

   Dataset & \(||\) & N \\  Lisa Traffic Sign & \(256^{307200}\) & \( 10^{369904}\) \\ ImageNet & \(256^{150528}\) & \( 10^{181252}\) \\ CIFAR10 & \(256^{3072}\) & \( 10^{3697}\) \\ MNIST & \(256^{784}\) & \( 10^{942}\) \\ B/W MNIST & \(27^{84}\) & \( 10^{116}\) \\ Adult & \( 10^{21.86}\) & \( 10^{9}\) \\ Heart Disease & \( 10^{13.51}\) & \( 10^{5}\) \\ Iris & \( 10^{6.35}\) & \( 10^{1}\) \\   

Table 1: Lower bound (4) on \(N\) evaluated for popular datasets with \(=0.1\) and \(=0.001\).

**Definition 3**.: _A backdoor detection problem for a training algorithm \(\) is determined by the following quantities: \((0,1]\), \(N\), \(M\), \(()^{2}\), \(P_{JI}(\{0,1\}^{2})\), and \(t\{0,1\}^{2}\{0,1\}\). Fixing these quantities, we define the risk of a backdoor detector \(g^{}\) associated with \((P_{0},P_{b})\) as \(R(g^{};P_{0},P_{b}):=\{g^{}(^{}) t(J,I)\}\), where the probability is w.r.t. \(^{}=((_{N}^{(J)}),_{M}^{ },X^{(I)})\) and \((J,I) P_{JI}\). We say that a backdoor detector is \(\)-error for some \([0,]\) if, for every pair \((P_{0},P_{b})\), the risk is bounded by \(R(g^{};P_{0},P_{b})\)._

OOD can be modeled using the target function \(t(j,i)\) for MBD, IBD and OOD Figure 1.

Note that several cells in the diagrams in Fig. 1 are grayed out. This reflects the fact that for certain flavors of backdoor detection, specific combinations of \((j,i)\) are not relevant. For MBD for instance, we are not interested in whether the target sample \(X^{(I)}\) contains a backdoor and we can thus assume \(I=0\) in this case, effectively reducing this case to the problem introduced in Section 2.1 with \(M+1\) samples being drawn from \(P_{0}\), i.e., \((_{M}^{},X^{(0)})=_{M+1}^{}\), available to the detector. Conversely, the case of a clean model, i.e., \(j=0\) and a sample with a backdoor, i.e., \(i=1\) is not realistic for IBD and we set \(P_{JI}(0,1)=0\) in this case. By setting \(J=0\) (i.e., model is trained on clean data and \(P_{JI}(1,0)=P_{JI}(1,1)=0\)) and using \(t_{}(j,i)=t_{}(i,0)=i\), we obtain an OOD detection problem, where the detector has access to a model \((_{N}^{(0)})\) trained on clean data and additional clean data \(_{M}^{}\). The detector then needs to determine whether \(X^{(I)}\) is in-distribution (\(I=0\)) or out-of-distribution (\(J=1\)). To showcase, how our result from Sections 2.2.1 and 2.2.2 carry over to other variants of backdoor detection, we will directly use Theorem 1 to derive a similar result for IBD. In analogy to the different Types of MBD detectors introduced in Section 2, we have a Type 2 detector \(g_{2}^{}(_{2}^{})\) with \(_{2}^{}=(_{N}^{(J)},P_{0},X^{(I)})\) for IBD. For such a detector we can leverage a reduction argument to obtain the following.

**Corollary 2**.: _Let \(g_{2}^{}(_{N^{}}^{(J)},P_{0},X^{(I)})\) be a Type 2 detector for an IBD problem with \(r=\{P_{JI}(0,0),P_{JI}(1,1)\}>0\) and \(=\{(P_{0},P_{b}):(P_{0},P_{b}) 1-\}\). Then, if \(g_{2}^{}\) is \(\)-error, we have \( r\) if \(||=\), and for \(||<\), we obtain_

\[N}{2}+)^{2}}{4} +(||-1)}.\] (8)

See proof in Appendix A.3.

## 3 Conclusions

We provided a formal statistical definition of backdoor detection and investigated the feasibility of backdoor detection. We concluded that under realistic assumptions, universal (adversary-unaware) backdoor detection is not possible. Thus, effective backdoor detectors need to be adversary-aware.