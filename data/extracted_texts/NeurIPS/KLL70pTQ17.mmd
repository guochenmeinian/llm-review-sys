# Oracle-Efficient Reinforcement Learning

for Max Value Ensembles

 Marcel Hussing

Dept. of Computer and Information Science

University of Pennsylvania

Philadelphia, PA 19104

mhussing@seas.upenn.edu

&Michael Kearns

Dept. of Computer and Information Science

University of Pennsylvania

Philadelphia, PA 19104

mkearns@cis.upenn.edu

&Aaron Roth

Dept. of Computer and Information Science

University of Pennsylvania

Philadelphia, PA 19104

aaroth@cis.upenn.edu

&Sikata Bela Sengupta

Dept. of Computer and Information Science

University of Pennsylvania

Philadelphia, PA 19104

sikata@seas.upenn.edu

&Jessica Sorrell

Dept. of Computer Science

Johns Hopkins University

Baltimore, MD 21218

jess@jhu.edu

This work was completed while this author was at the University of Pennsylvania.

###### Abstract

Reinforcement learning (RL) in large or infinite state spaces is notoriously challenging, both theoretically (where worst-case sample and computational complexities must scale with state space cardinality) and experimentally (where function approximation and policy gradient techniques often scale poorly and suffer from instability and high variance). One line of research attempting to address these difficulties makes the natural assumption that we are given a collection of base or _constituent_ policies (possibly heuristic) upon which we would like to improve in a scalable manner. In this work we aim to compete with the _max-following policy_, which at each state follows the action of whichever constituent policy has the highest value. The max-following policy is always at least as good as the best constituent policy, and may be considerably better. Our main result is an efficient algorithm that learns to compete with the max-following policy, given only access to the constituent policies (but not their value functions). In contrast to prior work in similar settings, our theoretical results require only the minimal assumption of an ERM oracle for value function approximation for the constituent policies (and not the global optimal policy or the max-following policy itself) on samplable distributions. We illustrate our algorithm's experimental effectiveness and behavior on several robotic simulation testbeds.

## 1 Introduction

Computationally efficient RL algorithms are known for simple environments with small state spaces such as tabular Markov decision processes (MDPs) [Kearns and Singh, 2002, Brafman and Tennenholtz, 2002], but practical applications often require dealing with large or even infinite state spaces.

Learning _efficiently_ in these cases requires computational complexity independent of the state space, but this is statistically impossible without strong assumptions on the class of MDPs (Jaksch et al., 2010; Lattimore and Hutter, 2012; Du et al., 2019; Domingues et al., 2021). Even in structured MDPs that admit statistically efficient algorithms, learning an optimal policy can still be computationally intractable (Kane et al., 2022; Golowich et al., 2024).

These obstacles to practical RL motivate the study of ensembling methods (Lee et al., 2021; Peer et al., 2021; Chen et al., 2021; Hiraoka et al., 2022), which assume access to multiple sub-optimal policies for the same MDP and aim to leverage these constituent policies to improve upon them. There are now several provably efficient ensembling algorithms, but their guarantees require strong assumptions on the representation of the target policy learned by the algorithm. Brukhim et al. (2022) use the boosting framework for ensembling developed in the supervised learning setting (Freund and Schapire, 1997) to learn an optimal policy, assuming access to a weak learner for a parameterized policy class. To efficiently converge to an optimal policy, the target policy must be expressible as a depth-two circuit over policies from a base class which is efficiently weak-learnable. The convergence guarantees additionally require strong bounds on the worst-case distance between state-visitation distributions of the target policy and policies from the base class.

Another line of ensembling work considers a weaker objective than learning an optimal policy (Cheng et al., 2020; Liu et al., 2023, 2024). These works instead aim to learn a policy competitive with a _max-aggregation policy_, which take whichever action maximizes the advantage function with respect to a max-following policy at the current state. When these works have provable guarantees, they require the assumption that the target max-aggregation policy can be approximated in an online-learnable parametric class, as well as the assumption that policy gradients within the class can be efficiently estimated with low variance and bias.

Our goal is to learn a policy competitive with a similar but incomparable benchmark to that of Cheng et al. (2020) under comparatively weak assumptions. We give an efficient algorithm for learning a policy competitive with a _max-following policy_ (Definition 2.1), assuming the learner has access to a squared-error regression oracle for the value functions of the constituent policies. Our algorithm exclusively queries this oracle on distributions over states that are efficiently samplable, thereby reducing the problem of learning a max-following competitive policy to supervised learning of value functions. Notably, our learnability assumptions pertain only to the value functions of the constituent policies and not to the more complicated class of max-following benchmark policies or their value functions. Our algorithm is simple and effective, which we demonstrate empirically in Section 5.

It is natural to wonder if access to an oracle such as ours could be leveraged to instead efficiently learn an optimal policy, obviating the need for weaker benchmarks (and our results). However, it was recently shown by (Golowich et al., 2024) that learning an optimal policy in a particular family of block MDPs is computationally intractable under reasonable cryptographic assumptions, even when the learner has access to a squared-error regression oracle. Their oracle captures a general class of regression tasks that includes value function estimation, and therefore also captures our oracle assumption. Our work shows that when we instead consider the simpler objective of efficiently learning a policy that competes with max-following, a regression oracle is in fact sufficient. We leave open the interesting question of whether such an oracle is necessary.

### Results

Our main contribution is a novel algorithm for improving upon a set of \(K\) given policies that is oracle efficient with respect to a squared-error regression oracle, and therefore scalable in large state spaces (Algorithm 1, Theorem 3.1). We consider the episodic RL setting in which the learner interacts with its environment for episodes of a fixed length \(H\). The algorithm incrementally constructs an improved policy over \(H\) iterations, learning an improved policy for step \(h[H]\) of the episode at iteration \(h\). This incremental approach allows the algorithm to explicitly construct efficiently samplable distributions over states visited by the improved policy at step \(h\) by simply executing the current policy for \(h\) steps. It can then query its oracle to obtain approximate value functions for all constituent policies with respect to this distribution. This in turn allows the algorithm to learn an improved policy for step \(h+1\) by following the policy with highest estimated value. By incrementally constructing an improved policy over steps of the episode, we can avoid making assumptions like those of Brukhim et al. (2022) about the overlap between state-visitation distributions of the target policy and the intermediate policies constructed by the algorithm.

As our oracle only gives us approximate value functions, we take as our benchmark class the set of _approximate max-following policies_ (Definition 2.3). This is a superset of the class of max-following policies and contains all policies that at each state follow the action of some constituent policy with near-maximum value. In Section 4, we prove that for any set of constituent policies, the worst approximate max-following policy is competitive with the best constituent policy (Lemma 4.1) and provide several example MDPs illustrating how our benchmark relates to other natural benchmarks.

Finally, we demonstrate the practical feasibility of our algorithm using a heuristic version on a set of robotic manipulation tasks from the CompoSuite benchmark Mendez et al. (2022); Hussing et al. (2024). We demonstrate that in all cases, the max-following policy we find is at least as good as the constituent policies and in several cases outperforms it significantly.

### Related work

Our work is related to a recent line of research learning a max-aggregation policy (Cheng et al., 2020; Liu et al., 2023, 2024), which can be viewed as a one-step look-ahead max-following policy and is incomparable to the class of max-following policies (see Cheng et al. (2020) for example MDPs demonstrating this fact). Sekhari et al. (2024) consider the problem of imitation learning from multiple noisy experts using selective sampling. For queried experts, their algorithm invokes an online regression oracle assumption and they leave as an open direction learning with offline regression oracles. These works all assume online learnability of the target policy class, which is strictly stronger than our batch learnability assumption for constituent policy value functions.

The work of Cheng et al. (2020) proposes an algorithm (MAMBA) that uses policy gradient methods, and the convergence of the learned policy to their benchmark depends on the bias and variance of those policy gradients. Liu et al. (2023, 2024) builds on the work of (Cheng et al., 2020). Their algorithm MAPS-SE modifies MAMBA to promote exploration when there is uncertainty about which constituent policy has the greatest value at a state, via an upper confidence bound (UCB) approach to policy selection. Reducing uncertainty about the constituent policies' value functions reduces the bias and variance of the gradient estimates, improving convergence guarantees. However, policy gradient techniques are known to generally have high variance (Wu et al., 2018), and this appears to affect the practical performance of MAPS-SE in certain cases (see Section 5 for additional discussion).

The boosting approach to policy ensembling of Brukhim et al. (2022) also necessitates very strong assumptions. This follows from the computational separation in Golowich et al. (2024), which shows that our oracle assumption is insufficient to learn an optimal policy, whereas the assumptions made in Brukhim et al. (2022) enable convergence to optimality.

Much work on policy improvement considers improving upon a single base policy and do not address the challenge of ensembling (Sun et al., 2017; Schulman et al., 2015; Chang et al., 2015). Barreto et al. (2017, 2020), Alegre et al. (2024) consider the problem of Generalized Policy Improvement (GPI) by decomposing complex tasks into a set of multiple smaller tasks where they use transfer learning. However, they make strong assumptions about the joint representation of rewards (tasks) as linear in successor feature representations, which may be challenging to explicitly learn in MDPs that are not tabular. Zaki et al. (2022) consider the setting of access to \(M\) base controllers with the aim of optimally combining them to produce a controller that is competitive with the base set. They approach this problem with the aim of considering a _single_ controller from the softmax policy class over the base set of policies that is competitive with all the others, but not in a _state-dependent_ manner. Empirical work on ensemble imitation learning (IL) also studies the problem of leveraging multiple base policies for learning (Li et al., 2018; Kurenkov et al., 2019), but these works lack provable guarantees of efficient convergence to a meaningful benchmark. (Song et al., 2023) provide a survey of a variety of more complex techniques to ensemble policies, mainly from a practical perspective.

## 2 Preliminaries

We consider an episodic fixed-horizon Markov decision process (MDP) (Puterman, 1994) which we formalize as a tuple \(=(,,R,P,_{0},H)\) where \(\) is the set of states, \(\) the set of actions, \(R\) is a reward function, \(P\) the transition dynamics, \(_{0}\) a distribution over starting states and \(H\) the horizon (Sutton and Barto, 2018). \([N]\) will denote the set \(\{0,...,N-1\}\). In the beginning, an initial state is sampled from \(_{0}\). At any time \(h[H]\), the agent is in some state \(s_{h}\) and chooses an action \(a_{h}\) based on a function \(_{h}\) mapping from states to distributions over actions \(:()\). As a consequence, the agent traverses to a new next state \(s_{h+1}\) sampled from \(P(|s_{h},a_{h})\) and obtains a reward \(R(s_{h},a_{h})\). Without loss of generality, we assume that rewards bounded within \(\). The sequence of functions \(_{h}\) used by the agent is referred to as its _policy_, and is denoted \(=\{_{h}\}_{h[H]}\). A _trajectory_ is the sequence of (state, action) pairs taken by the agent over an episode of length \(H\), and is denoted \(=\{(s_{h},a_{h})\}_{h[H]}\). We will use the notation \((_{0})\) to refer to sampling a trajectory by first sampling a starting state \(s_{0}_{0}\), and then executing policy \(\) from \(s_{0}\).

The goal of the learner is to maximize the expected cumulative reward \(_{s_{0}_{0},P}[_{t=0}^{H-1}R(s_{t},a_{t})]\) over episodes of length \(H\). We further define the value function as the expected cumulative return of following some policy \(\) from some state \(s\) as \(V^{}(s)=_{s_{0}_{0},P}[_{t=0}^{H-1}R(s_{t},a_{t})|,s _{0}=s]\). Due to the finite horizon of the episodic setting, we will also need to refer to the expected cumulative reward from state \(s\) under policy \(\) from time \(h[H]\). We denote this time-specific value function by \(V^{}_{h}(s)=_{P}[_{t=h}^{H-1}R(s_{t},a_{t})|,s_{h}=s]\). Finally, the key object of interest is a max-following policy. Given access to a set of \(k\) arbitrarily defined policies \(^{k}=\{^{k}\}_{k=1}^{K}\) and their respective value functions which we denote by the shorthand \(V^{_{h}}=V^{k}\), a max-following policy is defined as a policy that at every step follows the action of the policy with the highest value in that state.

**Definition 2.1** (Max-following policy class).: _Fix a set of policies \(^{k}\) for a common MDP \(\) and an episode length \(H\). The class of max-following policies \(^{k}_{}\) is defined_

\[^{k}_{}=\{: h[H], s,_{h}(s)=^{ k^{*}}(s)k^{*}*{argmax}_{k[K]}V^{k}_{h}(s)\}\]

Note that for any collection of constituent policies \(^{k}\) there may be many max-following policies, due to ties between the value functions. Different max-following policies may have different expected return, and we refer the reader to Observation 4.5 for an example demonstrating this fact.

We assume access to a value function oracle that allows us to approximate a value function of a policy under a samplable distribution at any specified time \(h[H]\). This oracle is intended to capture the common assumption that the value function of a policy can be efficiently well-approximated by a function from a fixed parameterized class. In practice, one might imagine implementing this oracle as a neural network minimizing the squared error to a target value function.

**Definition 2.2** (Oracle for \(\) value function estimates).: _We denote by \(^{}\) an oracle satisfying the following guarantee for a policy \(\). For any \((0,1]\), and any \(h[H]\), given as input a time \(h[H]\) and sampling access to any efficiently samplable distribution \(\), the oracle outputs \(^{}_{h}^{}(,,h)\) such that \(_{s}[(^{}_{h}(s)-V^{}_{h}(s))^{2}]\). We use the notation \(^{}_{}=^{}(,,)\) to denote \(^{}\) with fixed accuracy parameter \(\). We will also use the shorthand \(^{k}=^{^{k}}\)._

Looking ahead to Section 3, we note that for every distribution \(\) on which Algorithm 1 queries an oracle, \(\) is not only efficiently samplable, but samplable by executing an explicitly constructed policy \(_{}\) for \(h\) steps in MDP \(\), starting from \(_{0}\). Thus, for any distribution \(\), policy \(^{k}\), and time \(h\) for which we query \(^{k}\), we could efficiently obtain an unbiased estimate of \(_{s}[V^{k}_{h}(s)]\) by following a known \(_{}\) for \(h\) steps from \(_{0}\), and then switching to \(^{k}\) for the remainder of the episode. We mention this to highlight that our oracle is not editing any technical obstacles to sampling in the episodic setting. It is simply abstracting the supervised learning task of converting unbiased estimates of \(_{s}[V^{k}_{h}(s)]\) into an approximation \(^{k}_{h}\) with small squared error with respect to \(\).

Lastly, we define our benchmark class of policies. Given a set of constituent policies \(^{k}\), our benchmark defines for each state and time a set of permissible actions: any action taken by a policy \(^{t}^{k}\) for which the value \(V^{t}_{h}(s)\) is sufficiently close to the maximum value \(_{k[K]}V^{k}_{h}(s)\). The class of approximate max-following policies is then any policy that exclusively takes permissible actions. We refer the reader to Section 4 for further explanation of this benchmark.

**Definition 2.3** (Approximate max-following policies).: _We define a set of \(\)-good policies at state \(s\) and time \(h[H]\), selected from a set \(^{k}\), as follows._

\[T_{,h}(s)=\{^{k}:V^{}_{h}(s)_{k[K]}V^{k}_{h}(s)- \}.\]

_Then we define the set of approximate max-following policies for \(^{k}\) to be_

\[^{k^{*}}_{}=\{: h[H], s,_{h}(s)= ^{t}_{h}(s)^{t} T_{,h}(s)\}.\]The MaxIteration learning algorithm

In this section, we introduce our algorithm for learning an approximate max-following policy, MaxIteration (Algorithm 1. This algorithm learns a good approximation of a max-following policy at step \(h\), assuming access to a good approximation of a max-following policy for all previous steps.

For the first step (\(h=0\)), the algorithm learns a good approximation \(_{0}^{k}\) for all constituent policies \(^{k}\) on the starting distribution \(_{0}\). These approximate value functions can in turn be used to define the first action taken by the approximate max-following policy, namely \(_{0}(s)=_{*{argmax}_{k}}\,_{a}^{k}(s)\). Following \(_{0}(s)\) from \(_{0}\) generates a samplable distribution over states \(_{1}(s)=_{s_{0}_{0}}[P(s|s_{0},_{0}(s_{0}))]\), and so our oracle assumption allows us to obtain good estimates \(_{1}^{k}\) with respect to \(_{1}\) for all \(^{k}\). We can then define the second action of the approximate max-following policy, and so on, for all \(H\) steps.

Notice that sampling from \(_{h}\) does not require that the agent can reset the environment at will. It only requires what is typically required in the episodic setting - that the agent explores for an episode of \(H\) steps, where \(H\) is finite and fixed across all of training. After these \(H\) steps, the agent is then reset to a state sampled from the distribution over starting states. The distributions \(_{h}\) are (informally) defined as follows: at iteration \(h[H]\) of our algorithm, the agent has already learned a good approximate max-following policy for the first \(h\) steps of the episode. The distribution \(_{h}\) is the distribution over states visited by the agent at step \(h\) if it begins from a state drawn from the starting state distribution and then follows the approximate max-following policy it has learned thus far for \(h\) steps. That means to sample from \(_{h}\), the oracle can simply run the approximate max-following policy for \(h\) steps to arrive at a state \(s_{h}\), which is a sample from \(_{h}\). It can then do anything for the remainder of the episode, and so does not need to reset at arbitrary time steps. In practice, since the oracle needs to produce a good approximation of the value function \(V_{h}^{k}\) at time \(h\) for policy \(^{k}\) on states sampled from \(_{h}\), one should think of it as using the remainder of the episode to obtain an unbiased estimate of the expectation of \(V_{h}^{k}\) on the distribution \(_{h}\). That is, once it has sampled a state \(s_{h}\) by running the approximate max-following policy for \(h\) steps, it just executes policy \(^{k}\) for the remainder of the episode. The accumulated reward obtained by following policy \(^{k}\) from state \(s_{h}\) for steps \(h\) through \(H\) gives the oracle an unbiased estimate of \(_{s_{h}_{h}}[V_{h}^{k}(s_{h})]\). To implement this oracle assumption, one could use many such unbiased estimates as training data to train a neural network, to learn a good approximate value function for \(^{k}\) at time \(h\) on distribution \(_{h}\).

```
1:for\(h[H]\)do
2:for\(k[K]\)do
3: let \(_{h}\) be the distribution sampled by executing the following procedure:
4: sample a starting state \(s_{0}_{0}\)
5:for\(i[h]\)do
6:\(s_{i+1} P(\ \ |\ s_{i},^{*{argmax}_{k}_{i}^{k}(s_{i}) }(s_{i}))\)
7:endfor
8: output \(s_{h}\)
9:\(_{h}^{k}_{}^{k}(_{h},h)\)
10:endfor
11:endfor
12: return policy \(=\{_{h}\}_{h[H]}\) where \(_{h}(s)=^{*{argmax}_{k[K]}_{h}^{k}(s)}(s)\) ```

**Algorithm 1** MaxIteration\({}_{}^{}(^{k})\)

**Theorem 3.1**.: _For any \((0,1]\), any MDP \(\) with starting state distribution \(_{0}\), any episode length \(H\), and any \(K\) policies \(^{k}\) defined on \(\), let \((}{KH^{3}})\) and \(()\). Then \(_{}^{}(^{k})\) makes \(O(HK)\) oracle queries and outputs \(\) such that_

\[*{}_{s_{0}_{0}}V^{}(s_{0}) _{^{k}_{}}*{}_{s_{0}_{0}} [V^{}(s_{0})]-O().\]

Proof.: For all \(h[H]\), \(k[K]\), let \(_{h}^{k}\) denote the approximate value function obtained from \(_{}^{k}(_{h},h)\) in Algorithm 1. We then define, for every \(h[H]\), the set of states for which some approximate value function \(_{h}^{k}(s)\) has large absolute error (\(B_{h}\)) and the set of bad trajectories \((B_{})\) that pass through a state in \(B_{h}\) for any \(h[H]\): \(B_{h}=\{s S: k[K]|_{h}^{k}(s)-V_{h}^{k}(s)|\}\) and \(B_{}=\{\{(s_{h},a_{h})\}_{h[H]}: h[H]s_{h} B_{h}\}\). We will show that there exists an approximate max-following policy \(_{}^{k^{*}}\) such that for any trajectory \(^{} B_{}\), \(_{(_{0})}[=^{}]=_{(_{0}) }[=^{}]\). We then bound the probability \(_{(_{0})}[ B_{}]\), and the contribution to \(_{s_{0}_{0}}[V^{}(s_{0})]\) from these trajectories, proving the claim.

Let \(V_{h}^{k^{*}}(s)\) denote the value of the policy that \(\) follows at time \(h\) and state \(s\). From the definition of the bad set \(B_{h}\) and the setting of \(()\), for any state \(s B_{h}\),

\[V_{h}^{k^{*}}(s)_{h}^{k^{*}}(s)-_{k [K]}_{h}^{k}(s)-_{k[K]}V_{h}^{k}( s)-.\]

In other words, if a state \(s\) is not bad at time \(h\), then \(_{h}(s)=_{h}^{k}(s)\) for a policy \(^{k}\) that has value \(V_{h}^{k}(s)\) within \(\) of the true max value \(_{k[K]}V_{h}^{k}(s)\). It then follows from the definition of the class of approximate max-following policies \(_{}^{k^{*}}\) (Definition 2.3) that there exists some \(_{}^{k^{*}}\) such that for all \(h[H]\), for all \(s B_{h}\), \(_{h}(s)=_{h}(s)\).

For any trajectory \(^{}\), \(_{(_{0})}[=^{}]=_{_{0}}[s_{0}] _{h=0}^{H-1}P(s_{h+1}|s_{h},_{h}(s_{h}))\). Then for any trajectory \(^{} B_{}\), \(_{(_{0})}[=^{}]=_{(_{0} )}[=^{}]\), and therefore

\[}_{(_{0})}[_{h=0}^{H-1}R(s_{h},a_{h}) B_{}]=}_{(_{0 })}[_{h=0}^{H-1}R(s_{h},a_{h}) B_{}]\]

For \( B_{}\), we have lower and upper-bounds \(}_{(_{0})}[_{h=0}^{H-1}R(s_{h},a_{h }) B_{}] 0\) and \(}_{(_{0})}[_{h=0}^{H-1}R(s_{h},a_{h})  B_{}] H\). We can then write:

\[}_{s_{0}_{0}}[V^{}(s_{0})] =}_{(_{0})}[_{h=0}^{H-1}R(s_ {h},a_{h}) B_{}]_{( _{0})}[ B_{}]\] \[}_{(_{0})}[_{h=0}^{ H-1}R(s_{h},a_{h}) B_{}]_{( _{0})}[ B_{}]\] \[=}_{(_{0})}[_{h=0}^{H-1} R(s_{h},a_{h}) B_{}]_{(_{0} )}[ B_{}]\] \[}_{(_{0})}[_{h=0}^{ H-1}R(s_{h},a_{h})]-H_{(_{0})}[ B_{ }]\] \[_{_{}^{k^{*}}}}_{s_{0} _{0}}[V^{}(s_{0})]-H_{(_{0})}[ B _{}].\]

It remains to upper-bound \(_{(_{0})}[ B_{}]\). We have already argued \(_{(_{0})}[ B_{}]=_{(_{0})}[  B_{}]\). Observing that \(_{(_{0})}[ B_{}]_{h=0}^{H-1}_{ (_{0})}[s_{h} B_{h}]\), it is sufficient to show \(_{(_{0})}[s_{h} B_{h}] O(})\) to prove the claim. For all \(h[H]\), let \(_{h}(s)=_{(_{0})}[s_{h}=s]\), and note that this is the distribution supplied to the oracle at iteration \(h\) of Algorithm 1. It follows from our oracle assumption (Definition 2.2) that for all \(k[K]\), \(}_{s_{h}_{h}}[(^{k}(s_{h})-V^{k}(s_{h}))^{2}]<\). We apply Markov's inequality to conclude that for all \(k[K]\),

\[_{s_{h}_{h}}[\|_{h}^{k}(s_{h})-V_{h}^{k}(s_{h})\| ]<}{^{2}} O(}).\]

Union bounding over the \(K\) constituent policies gives \(_{s_{h}_{h}}[s_{h} B_{h}] O(})\), from the definition of \(B_{h}\). Union bounding over the trajectory length \(H\), we then have \(_{(_{0})}[ B_{}] O()\). It follows that

\[}_{s_{0}_{0}}[V^{}(s_{0})] _{_{}^{k^{*}}}}_{s_{0}_{0}}[V^{ }(s_{0})]-O(),\]

completing the proof.

## 4 The approximate max-following benchmark

In this section, we provide additional context for our benchmark class of approximate max-following policies. We show that the worst policy in our benchmark class competes with the best fixed policy from the set of constituent policies. We also provide examples of MDPs that showcase properties of the set of (approximate) max-following policies.

**Lemma 4.1** (Worst approximate max-following policy competes with best fixed policy).: _For any \((0,1]\) and any episode length \(H\), let \(()\). Then for any MDP \(\) with starting state distribution \(_{0}\), and any \(K\) policies \(^{k}\) defined on \(\),_

\[_{^{k^{*}}_{}}*{}_{s_{0}_{0}} [V^{}(s_{0})]_{k[K]}*{} _{s_{0}_{0}}[V^{k}(s_{0})]-O().\]

We defer the proof of Lemma 4.1 to Appendix B.

It is an immediate corollary of Theorem 3.1 and Lemma 4.1 that the policy learned by Algorithm 1 competes with the best constituent policy.

**Corollary 4.2**.: _For any \((0,1]\), any MDP \(\) with starting state distribution \(_{0}\), any episode length \(H\), and any \(K\) policies \(^{k}\) defined on \(\), let \((}{KH^{4}})\), and let \(\) denote the policy output by \(^{}_{}(^{k})\). Then_

\[*{}_{s_{0}_{0}}[V^{}(s_{0})] _{k[K]}*{}_{s_{0}_{0}}[V^{k}(s_ {0})]-O().\]

We provide diagrams of MDPs as examples for the observations that we make below. States in \(\) are denoted by the labels on the nodes. Actions in \(\) are indicated by arrows from given states with deterministic transition dynamics and the rewards \(R(s,a)\) are labeled over the corresponding arrows. Arrows may be omitted for transitions that are self-loops with reward \(0\).

**Observation 4.3**.: _The worst approximate max-following policy can be arbitrarily better than the best constituent policy._

Consider in Figure 0(a) two policies on this MDP: \(^{0}(s)=\) and \(^{1}(s)=\), for all \(s\). Note that for any episode length \(H 2\), for all \(k\{0,1\}\), \(_{s}V^{k}(s)=2\). For any \(<1\), \(^{k^{*}}_{}\) comprises policies \(\) such that \((s_{0})=\), \((s_{2})=\), and \((s_{1})\{,\}\). Therefore for any episode length \(H\), and state \(s\), \(_{^{k^{*}}_{}}V^{}(s)=H\). In this example, any approximate max-following policy is also an optimal policy, whose gap in expected return with the best constituent policy can be made arbitrarily large by increasing \(H\).

**Observation 4.4**.: _A max-following policy cannot always compete with an optimal policy._

In Figure 0(b), consider policies \(^{0}(s)=\), \(^{1}(s)=\), and \(^{2}(s)=\), for all \(s\). At state \(s_{2}\), \(^{0}\) is the only policy with non-zero value. Thus, any max-following policy will take action right from \(s_{2}\), receiving reward \(\) and then reward 0 for the remainder of the episode. Given a starting state distribution supported entirely on \(s_{2}\), for any episode length \(H 3\), the optimal policy will obtain cumulative reward \(H-2\), whereas any max-following policy will only obtain reward \(\).

Figure 1: Examples of MDPs with max-following policy performance comparison

### Observation 4.5

_Different max-following policies may have different expected cumulative reward._

We again consider Figure 0(b), but suppose now the starting state distribution is supported entirely on \(s_{0}\). For all \(k\), \(V^{k}(s_{0})=0\) and so a max-following policy may take any action from \(s_{0}\). A max-following policy that always takes actions left or up from \(s_{0}\) will only ever obtain cumulative reward 0, but a max-following policy that takes action right will move to \(s_{1}\) and (so long as more than one step remains in the episode) will then take action up and move to state \(s_{4}\), where it will stay to obtain cumulative reward \(H-2\).

If the value functions of constituent policies are exactly known, it is easy to construct a max-following policy, but the learner may not have access to these functions. If the learner only has access to approximations and follows whichever policy has the larger approximate value at the current state, the resulting policy can have much lower expected cumulative reward than the max-following policy. This is true even for state-wise bounds on the value approximation error. This observation previously motivated our definition of the approximate max-following class (Definition 2.3).

**Observation 4.6**.: _Small value function approximation errors can be an obstacle to learning a max-following policy._

In Figure 1(a), we again consider policies \(^{0}(s)=\) and \(^{1}(s)=\) for all states \(s\), color coding the actions taken by \(^{0}\) with red and \(^{1}\) with blue in Figure 1(a). For starting state distribution supported entirely on \(s_{0}\), a max-following policy \(\) will take action \((s_{0})=\), \((s_{2})=\), and \((s_{3})=\) for the remainder of the episode, obtaining reward \(H-2+2\). However, given only approximate value functions \(^{k}\) with state-wise absolute error bound \(|^{k}_{h}(s)-V^{k}_{h}(s)|\) for all states \(s\) and times \(h\), the policy \(\) that takes action \(^{k^{*}}_{h}(s)\) for \(k^{*}=*{argmax}_{k}^{k}_{h}(s)\) can have much lower expected cumulative reward than a max-following policy. For example if \(^{0}_{0}(s_{0})=\) and \(^{1}_{0}(s_{0})=0\) in our Figure 1(a) example, then \(\) will have expected return 0.

**Observation 4.7**.: _A max-following policy's value function is not always of the same parametric class as the constituent policies' value functions._

As a simple first example, consider an MDP with states \(=\) and actions \(=\{-1,1\}\). Every action leads to a self-loop (for all \(a\), \(P(s|s,a)=1\)) and for a fixed action, rewards are affine functions of the state (e.g. \(R(s,-1)=1-s\) and \(R(s,1)=s\)). We consider two policies: \(^{0}(s)=-1\) and \(^{1}(s)=1\) for all \(s\). Notice that for episode length \(H\), \(V^{0}(s)=HR(s,-1)\) and \(V^{1}(s)=HR(s,1)\). Since the dynamics keep the state at the same fixed place independent of the action, the max-following policy at state \(s\) will simply be the max of the two individual value functions at \(s\) and therefore its parametric class will be piecewise linear, unlike the constituent policies' which are affine (see Figure 1(b)). To provide a more complex MDP example, we consider a traditional control problem with continuous state and action spaces: the discrete linear quadratic regulator. In this example the constituent linear policies have quadratic value functions, but the max-following policy is not of the same parametric class. See Appendix A for further discussion.

Figure 2: Examples for Observation 4.6 and Observation 4.7Experiments

We proceed to examine our MaxIteration algorithm in a set of experiments that uses neural network function approximation as oracles. These experiments aim to provide a scenario to demonstrate the usefulness of max-following. While previous works in this line of research have studied the ability to integrate knowledge from the constituent policies to increase performance of a learnable policy (Cheng et al., 2020; Liu et al., 2023, 2024) our algorithm offers an alternative approach. We consider a common scenario from the field of robotics where one has access to older policies from a robotic simulator that were used in previous projects. As long as the dynamics of the MDP of interest do not differ, such old policies can be simply be re-used in new applications. In such cases, training completely from scratch can be incredibly expensive due to the vast search space (Schulman et al., 2017; Haarnoja et al., 2018). We note that this setup is related to the one used by Barreto et al. (2017, 2020) but we do not put any constraints on the reward functions.

Experimental setupA recent robotic simulation benchmark called CompoSuite (Mendez et al., 2022) and its corresponding offline datasets (Hussing et al., 2024) offer an instantiation of such a scenario. CompoSuite consists of four axes: robot arms, objects, objectives and obstacles. Tasks are simply constructed by combining one element from each axis.We consider tasks with a fixed IIWA robotic manipulator and no obstacle. This leaves us with a total of 16 tasks. These 16 tasks are randomly grouped into pairs of two. Each group is one experiment where the policies trained on tasks correspond to our constituents. To create a new target task, we change one element per task, creating novel combinations for each group. For example, we start with the constituent policies that can 1) put and place a box into a trashcan and 2) push a plate. The target task can be to push the box. We train our constituent policies on the expert datasets using the offline RL algorithm Implicit Q-learning (Kostrikov et al., 2022) (IQL). This ensures we obtain very strong constituent policies for their respective tasks. After training the constituents, we run MaxIteration and the baselines for a short amount of time in the simulator. We report mean performance and standard error over 5 seeds using an evaluation of \(32\) episodes.

AlgorithmsFor practical purposes, we use a heuristic version of MaxIteration which does not re-compute the max-following policy at every step \(h\) but rather after multiple steps. For our baselines, we ran the code provided by (Liu et al., 2023) to train the MAPS algorithm but were unable to obtain non-trivial return even after a reasonable amount of tuning. MAPS has been shown to have difficulties with leveraging very performant constituent policies such as the ones we are using (see the Walker experiment by Liu et al. (2023) in Figure 1 (d) in which the algorithm struggles to be competitive with the best, high-return constituent policy). They conjecture that in this case, their estimates of the constituent value functions will be less accurate in early training, resulting in gradient estimates with large bias and variance, weakening their convergence guarantees. We provide an evaluation of MaxIteration on tasks originally used by Liu et al. (2023) in Appendix C.3.

For now, we opt to use IQL's in fine-tuning capabilities that offer a policy improvement style method on top of the best-performing constituent policy for comparison. Fine-tuning provides a strong baseline in the sense that it has access to the already trained value functions of the constituent policies providing it with inherently more starting information. For comparability, we limit the number of episodes available for fine-tuning to the same number of episodes available for training MaxIteration. For more details we refer to Appendix C.

Experimental ResultsFigure 3 contains a set of demonstrative results. The full results are deferred to Appendix C. The selected results in Figure 3 highlight three properties of MaxIteration:

1. There are cases where max-following not only increases the return but actually leads to solving a task successfully even when none of the constituent policies achieve success.
2. With successful constituent policies, max-following can significantly increase the success rate.
3. max-following can sometimes increase return but not necessarily lead to success demonstrating the need to better understand which attributes make up good constituent policies in the future.

The results in Appendix C demonstrate that in all cases, MaxIteration is at least as good as the best constituent policy which is not the case for algorithms from prior work (Liu et al., 2023) as discussed earlier. Moreover, MaxIteration consistently leads to greater return improvement than fine-tuning given the same amount of data. Fine-tuning with substantially more resources would eventually surpass the performance of MaxIteration as MaxIteration is limited to competing with the max-following benchmark which can be suboptimal.

## 6 Conclusion

We introduce MaxIteration, an algorithm to efficiently learn a policy that is competitive with the approximate max-following benchmark (and hence also with all constituent policies). We provide empirical evidence that max-following utilizing skill-learning enables us to learn how to complete tasks that it would be inefficient to learn from scratch, but that are superior to other individually trained experts for fixed given skills.

Limitations and Future WorkOur goal in this work has been to learn a policy that competes with an approximate max-following policy under minimal assumptions. However, we still assume efficient batch learnability of constituent value functions, which will not always be feasible in practice. While it seems likely that our oracle assumption is necessary for learning an approximate max-following policy, we leave proving this claim for future work. We also leave consideration of alternative ensembling approaches to future work. Max-value ensembling is sensitive to slight differences in the values between constituent policies whereas, e.g., softmax takes into account the relative 'weighting' of values. In addition, it would be interesting to characterize the amount of improvement we can obtain over our constituent policies or prove conditions under which our approximate max-following policy is competitive with a true max-following policy or the optimal policy. One could also extend this analysis to ensembling methods like softmax and study the nature of guarantees in that setting. Extending beyond MDPs to the partially observable setting, and to the discounted infinite-horizon setting, would also add richness to the class of problems we could consider.

Figure 3: Policies \(0\) and \(1\) correspond to the pre-trained policies using IQL on the intial tasks above the arrow in each graph. That is, in the left most subfigure, Policy \(0\) corresponds to the policy of picking and placing a dumbbell, whereas Policy \(1\) corresponds to the policy of moving a box into the trashcan. Mean return and success rate over \(5\) seeds of MaxIteration compared to fine-tuning IQL on selected tasks. Error-bars correspond to standard error. Full bars correspond to returns and red lines indicate the success rate of each algorithm. MaxIteration can yield improvements in return but increased return does not always yield success.