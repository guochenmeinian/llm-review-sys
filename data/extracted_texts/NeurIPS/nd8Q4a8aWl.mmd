# A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models

A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models

 Hamidreza Kamkari Brendan Leigh Ross Rasa Hosseinzadeh Jesse C. Cresswell Gabriel Loaiza-Ganem

{hamid, brendan, rasa, jesse, gabriel}@layer6.ai

Layer 6 AI, Toronto, Canada

###### Abstract

High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the _local intrinsic dimension_ (LID) of a datum - i.e. the dimension of the submanifold it belongs to - is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models: diffusion models (DMs). In this work, we show that the Fokker-Planck equation associated with a DM can provide an LID estimator which addresses the aforementioned deficiencies. Our estimator, called FLIPD, is easy to implement and compatible with all popular DMs. Applying FLIPD to synthetic LID estimation benchmarks, we find that DMs implemented as fully-connected networks are highly effective LID estimators that outperform existing baselines. We also apply FLIPD to natural images where the true LID is unknown. Despite being sensitive to the choice of network architecture, FLIPD estimates remain a useful measure of relative complexity; compared to competing estimators, FLIPD exhibits a consistently higher correlation with image PNG compression rate and better aligns with qualitative assessments of complexity. Notably, FLIPD is orders of magnitude faster than other LID estimators, and the first to be tractable at the scale of Stable Diffusion.

## 1 Introduction

The manifold hypothesis , which has been empirically verified in contexts ranging from natural images  to calorimeter showers in physics , states that high-dimensional data of interest in \(^{D}\) often lies on low-dimensional submanifolds of \(^{D}\). For a given datum \(x^{D}\), this hypothesis motivates using its _local intrinsic dimension_ (LID), denoted \((x)\), as a natural measure of its complexity. \((x)\) corresponds to the dimension of the data manifold that \(x\) belongs to, and can be intuitively understood as the minimal number of variables needed to describe \(x\). More complex data needs more variables to be adequately described, as illustrated in Figure 1. Data manifolds are typically not known explicitly, meaning that LID must be estimated. Here we tackle the following problem: given a dataset along with a query datum \(x\), how can we tractably estimate \((x)\)?

This is a longstanding problem, with LID estimates being highly useful due to their innate interpretation as a measure of complexity. For example, these estimates can be used to detect outliers , AI-generated text , and adversarial examples . Connections between the generalization achieved by a neural network and the LID estimates of its internal representations have also been shown [4; 8; 44; 9]. These insights can be leveraged to identify which representations contain maximal semantic content , and help explain why LID estimates can be helpful as regularizers  and for pruning large models . LID estimation is thus not only of mathematical and statistical interest, but can also benefit the empirical performance of deep learning models at numerous tasks.

Traditional estimators of intrinsic dimension [22; 38; 43; 12; 30; 20; 1; 5] typically rely on pairwise distances and nearest neighbours, so computing them is prohibitively expensive for large datasets. Recent work has thus sought to move away from these _model-free_ estimators and instead take advantage of deep generative models which learn the distribution of observed data. When this distribution is supported on low-dimensional submanifolds of \(^{D}\), successful generative models must implicitly learn the dimensions of the data submanifolds, suggesting they can be used to construct LID estimators. However, existing _model-based_ estimators suffer from various drawbacks, including being inaccurate and computationally expensive , not leveraging the best existing generative models [62; 71] (i.e. diffusion models [56; 25; 58]), and requiring training several models  or altering the training procedure rather than relying on a pre-trained model . Importantly, _none_ of these methods scale to high-resolution images such as those generated by Stable Diffusion .

We address these issues by showing how LID can be efficiently estimated using only a single pre-trained diffusion model (DM) by building on LIDL , a model-based estimator. LIDL operates by convolving data with different levels of Gaussian noise, training a normalizing flow [50; 17; 18] for each level, and fitting a linear regression using the log standard deviation of the noise as a covariate and the corresponding log density (of the convolution) evaluated at \(x\) as the response; the resulting slope is an estimate of LID\((x)-D\), thanks to a surprising result linking Gaussian convolutions and LID. We first show how to adapt LIDL to DMs in such a way that only a single DM is required (rather than many normalizing flows). Directly applying this insight leads to LIDL estimates that require one DM but several calls to an ordinary differential equation (ODE) solver; we further show how to circumvent this with an alternative ODE that computes all the required log densities in a single solve. We then argue that the slope of the regression in LIDL aims to capture the rate of change of the marginal log probabilities in the diffusion process, which can be evaluated directly thanks to the Fokker-Planck equation. The resulting estimator, which we call FLIPD,1 is highly efficient, and circumvents the need for an ODE solver. Notably, FLIPD is _differentiable_; this property opens up exciting avenues for future research as it enables backpropagating through LID estimates.

Our contributions are: \((i)\) showing how DMs can be efficiently combined with LIDL in a way which requires a single call to an ODE solver; \((ii)\) leveraging the Fokker-Planck equation to propose FLIPD, thus improving upon the estimator and circumventing the need for an ODE solver altogether; \((iii)\) motivating FLIPD theoretically; \((iv)\) introducing an expanded suite of LID estimation benchmark

Figure 1: **(Left)** A cartoon illustration showing that LID is a natural measure of relative complexity. We depict two manifolds of MNIST digits, corresponding to 1s and 8s, as 1-dimensional and 2-dimensional submanifolds of \(^{3}\), respectively. The relatively simpler manifold of 1s exhibits a single factor of variation (“tilt”), whereas 8s have an additional factor of variation (“disproportionality”). **(Right)** The 4 lowest- and highest-LID datapoints from a subsample of LAION-Aesthetics, as measured by our method, FLIPD, applied to Stable Diffusion v1.5. FLIPD scales efficiently to large models on high-dimensional data, and aligns closely with subjective complexity.

tasks that reveals gaps in prior evaluations, and specifically that other estimators do not remain accurate as the complexity of the manifold increases; \((v)\) demonstrating that when using fully-connected architectures for diffusion models, FLIPD outperforms existing baselines - especially as dimension increases - while being much more computationally efficient; \((vi)\) showing that when applied to natural images, despite varying across network architecture (i.e., fully-connected network or UNet [52; 68]), FLIPD estimates consistently align with other measures of complexity such as PNG compression length, and with qualitative assessments of complexity, highlighting that the LID estimates provided by FLIPD remain valid measures of relative image complexity; and \((vii)\) demonstrating that when applied to the latent space of Stable Diffusion, FLIPD can estimate LID for extremely high-resolution images (\( 10^{6}\) pixel dimensions) for the first time.

## 2 Background and Related Work

### Diffusion Models

Forward and backward processesDiffusion models admit various formulations [56; 25]; here we follow the score-based one . We denote the true data-generating distribution, which DMs aim to learn, as \(p(,0)\). DMs define the forward (Ito) stochastic differential equation (SDE),

\[X_{t}=f(X_{t},t)t+g(t)W_{t}, X_{0} p( ,0),\] (1)

where \(f:^{D}^{D}\) and \(g:\) are hyperparameters, and \(W_{t}\) denotes a \(D\)-dimensional Brownian motion. We write the distribution of \(X_{t}\) as \(p(,t)\). The SDE in Equation 1 prescribes how to gradually add noise to data, the idea being that \(p(,1)\) is essentially pure noise. Defining the backward process as \(Y_{t} X_{1-t}\), this process obeys the backward SDE [3; 24],

\[Y_{t}=[g^{2}(1-t)s(Y_{t},1-t)-f(Y_{t},1-t)]t+g( 1-t)_{t}, Y_{0} p(,1),\] (2)

where \(s(x,t) p(x,t)\) is the unknown (Stein) score function,2 and \(_{t}\) is another \(D\)-dimensional Brownian motion. DMs leverage this backward SDE for generative modelling by using a neural network \(:^{D}(0,1]^{D}\) to learn the score function with denoising score matching . Once trained, \((x,t) s(x,t)\). To generate samples \(_{1}\) from the model, we solve an approximation of Equation 2:

\[_{t}=[g^{2}(1-t)(_{t},1-t)-f(_{t},1 -t)]t+g(1-t)_{t},_{0} (,1),\] (3)

with \(\) replacing the true score and with \((,1)\), a Gaussian distribution chosen to approximate \(p(,1)\) (depending on \(f\) and \(g\)), replacing \(p(,1)\).

Density EvaluationDMs can be interpreted as continuous normalizing flows , and thus admit density evaluation, meaning that if we denote the distribution of \(_{1-t}\) as \((,t)\), then \((x,t_{0})\) can be mathematically evaluated for any given \(x^{D}\) and \(t_{0}(0,1]\). More specifically, this is achieved thanks to the (forward) ordinary differential equation (ODE) associated with the DM:

\[_{t}=(f(_{t},t)-g^{2}(t)(_{t},t))t,_{t_{0}}=x.\] (4)

Solving this ODE from time \(t_{0}\) to time \(1\) produces the trajectory \((_{t})_{t[t_{0},1]}\), which can then be used for density evaluation through the continuous change-of-variables formula:

\[(x,t_{0})=(_{1},1)+_{t_{0}}^{1}(  v(_{t},t))t,\] (5)

where \((,1)\) can be evaluated since it is a Gaussian, and where \(v(x,t) f(x,t)-g^{2}(t)(x,t)/2\).

Trace estimationNote that the cost of computing \( v(_{t},t)\) for a particular \(_{t}\) amounts to \((D)\) function evaluations of \(\) (since \(D\) calls to a Jacobian-vector-product routine are needed ). Although this is not prohibitively expensive for a single \(_{t}\), in order to compute the integral in Equation 5 in practice, \((_{t})_{t[t_{0},1]}\) must be discretized into a trajectory of length \(N\). If we denote by \(F\) the cost of evaluating \(v(_{t},t)\) - or equivalently, \((_{t},t)\) - deterministic density evaluation is \((NDF)\), which is computationally prohibitive. The Hutchinson trace estimator  - which states that for \(M^{D D}\), \((M)=_{}[^{}M]\), where \(^{D}\) has mean \(0\) and covariance \(I_{D}\) - is thus commonly used for stochastic density estimation; approximating the expectation with \(k\) samples from \(\) results in a cost of \((NkF)\), which is much faster than deterministic density evaluation when \(k D\).

### Local Intrinsic Dimension and How to Estimate It

LidVarious definitions of intrinsic dimension exist [28; 21; 37; 11]. Here we follow the standard one from geometry: a \(d\)-dimensional manifold is a set which is locally homeomorphic to \(^{d}\). For a given disjoint union of manifolds and a point \(x\) in this union, the _local intrinsic dimension_ of \(x\) is the dimension of the submanifold it belongs to. Note that LID is not an intrinsic property of the point \(x\), but rather a property of \(x\) with respect to the manifold that contains it. Intuitively, \((x)\) corresponds to the number of factors of variation present in the manifold containing \(x\), and it is thus a natural measure of the relative complexity of \(x\), as illustrated in Figure 1.

Estimating LIDThe natural interpretation of LID as a measure of complexity makes estimating it from observed data a relevant problem. Here, the formal setup is that \(p(,0)\) is supported on a disjoint union of manifolds , and we assume access to a dataset sampled from it. Then, for a given \(x\) in the support of \(p(,0)\), we want to use the dataset to provide an estimate of \((x)\). Traditional estimators [22; 38; 43; 12; 30; 20; 1; 5] rely on the nearest neighbours of \(x\) in the dataset, or related quantities, and typically have poor scaling in dataset size. Generative models are an intuitive alternative to these methods; because they are trained to learn \(p(,0)\), when they succeed they must encode information about the support of \(p(,0)\), including the corresponding manifold dimensions. However, extracting this information from a trained generative model is not trivial. For example, Zheng et al.  showed that the number of active dimensions in the approximate posterior of variational autoencoders [33; 50] estimates LID, but their approach does not generalize to better generative models.

LidlTemczyk et al.  proposed LIDL, a method for LID estimation relying on normalizing flows as tractable density estimators [50; 17; 18]. LIDL works thanks to a surprising result linking Gaussian convolutions and LID [39; 62; 71]. We will denote the convolution of \(p(,0)\) and Gaussian noise with log standard deviation \(\) as \((,)\), i.e.

\[(x,) p(x_{0},0)(x-x_{0};0,e^{2}I_ {D})x_{0}.\] (6)

The aforementioned result states that, under mild regularity conditions on \(p(,0)\), and for a given \(x\) in its support, the following holds as \(-\):

\[(x,)=((x)-D)+(1).\] (7)

This result then suggests that, for negative enough values of \(\) (i.e. small enough standard deviations):

\[(x,)((x)-D)+c,\] (8)

for some constant \(c\). If we could evaluate \((x,)\) for various values of \(\), this would provide an avenue for estimating \((x)\): set some values \(_{1},,_{m}\), fit a linear regression using \(\{(_{i},(x,_{i}))\}_{i=1}^{m}\) with \(\) as the covariate and \((x,)\) as the response, and let \(_{x}\) be the corresponding slope. It follows that \(_{x}\) estimates \((x)-D\), so that \((x) D+_{x}\) is a sensible estimator of local intrinsic dimension.

Since \((x,)\) is unknown and cannot be evaluated, LIDL requires training \(m\) normalizing flows. More specifically, for each \(_{i}\), a normalizing flow is trained on data to which \((0,e^{2_{i}}I_{D})\) noise is added. In LIDL, the log densities of the trained models are then used instead of the unknown true log densities \((x,_{i})\) when fitting the regression as described above.

Despite using generative models, LIDL has obvious drawbacks. LIDL requires training several models. It also relies on normalizing flows, which are not only empirically outperformed by DMs by a wide margin, but are also known to struggle to learn low-dimensional manifolds [14; 39; 40]. On the other hand, DMs do not struggle to learn \(p(,0)\) even when it is supported on low-dimensional manifolds [48; 16; 40], further suggesting that LIDL can be improved by leveraging DMs.

Estimating LID with DMsThe only works we are aware of that leverage DMs for LID estimation are those of Stanczuk et al. , and Horvat and Pfister . The latter modifies the training procedure of DMs, so we focus on the former since we see compatibility with existing pre-trained models as an important requirement for DM-based LID estimators. Stanczuk et al.  consider _variance-exploding_ DMs, where \(f=0\). They show that, as \(t 0\), the score function \(s(x,t)\) points orthogonally towards the manifold containing \(x\), or more formally, it lies in the normal space of this manifold at \(x\). They thus propose the following LID estimator, which we refer to as the normal bundle (NB) estimator: first run Equation 1 until time \(t_{0}\) starting from \(x\), and evaluate \((,t_{0})\) at the resulting value; then repeat this process \(K\) times and stack the \(K\) resulting \(D\)-dimensional vectors into a matrix \(S(x)^{D K}\). The idea here is that if \(t_{0}\) is small enough and \(K\) is large enough, the columns of \(S(x)\) span the normal space of the manifold at \(x\), suggesting that the rank of this matrix estimates the dimension of this normal space, namely \(D-(x)\). Finally, they estimate \((x)\) as:

\[(x) D-S(x).\] (9)

Numerically, the rank is computed by performing a singular value decomposition (SVD) of \(S(x)\), setting a threshold, and counting the number of singular values exceeding the threshold. Computing \(S(x)\) requires \(K\) function evaluations, and intuitively \(K\) should be large enough to ensure the columns of \(S(x)\) span the normal space at \(x\); the authors thus propose using \(K=4D\), and recommend always at least ensuring that \(K>D\). Computing the NB estimator costs \((KF+D^{2}K)\), where \(F\) again denotes the cost for evaluating \(\). Thus, although the NB estimator addresses some of the limitations of LIDL, it remains computationally expensive in high dimensions.

## 3 Method

Although Tempczyk et al.  only used normalizing flows in LIDL, they did point out that these models could be swapped for any other generative model admitting density evaluation. Indeed, one could trivially train \(m\) DMs and replace the flows with them. Throughout this section we provide a sequence of progressive improvements to this naive application of LIDL with DMs, culminating with FLIPD. We assume access to a pre-trained DM such that \(f(x,t)=b(t)x\) for a function \(b:\). This choice implies that the transition kernel \(p_{t|0}\) associated with Equation 1 is Gaussian :

\[p_{t|0}(x_{t} x_{0})=(x_{t};(t)x_{0},^{2}(t)I_{D}),\] (10)

where \(,:\). We also assume that \(b\) and \(g\) are such that \(\) and \(\) are differentiable and such that \((t)(t)/(t)\) is injective. This setting encompasses all DMs commonly used in practice, including variance-exploding, variance-preserving (of which the widely used DDPMs  are a discretized instance), and sub-variance-preserving . In Appendix A we include explicit formulas for \((t)\), \(^{2}(t)\), and \((t)\) for these particular DMs.

### LIDL with a Single Diffusion Model

As opposed to the several normalizing flows used in LIDL which are individually trained on datasets with different levels of noise added, a single DM already works by convolving data with various noise levels and allows density evaluation of the resulting noisy distributions (Equation 5). Hence, we make the observation that LIDL can be used with a _single_ DM. All we need is to relate \((,)\) to the density of the DM, \(p(,t)\). In the case of variance-exploding DMs, \((t)=1\), so we can easily use the defining property of the transition kernel in Equation 10 to get

\[p(x,t)= p(x_{0},0)p_{t|0}(x_{t} x_{0})x_{0}= p(x_{0},0) (x_{t};x_{0},^{2}(t)I_{D})x_{0},\] (11)

which equals \((x,)\) from Equation 6 when we choose \(t=^{-1}(e^{})\).3 In turn, we can use LIDL with a single variance-exploding DM by evaluating each \(x,^{-1}(e^{_{i}})\) through Equation 5. This idea extends beyond variance-exploding DMs; in Appendix B.1 we show that for _any_ arbitrary DM with transition kernel as in Equation 10, it holds that

\[(x,)=Dt()+ pt ()x,t(),\] (12)

where \(t()^{-1}(e^{})\). This equation is relevant because LIDL requires \((,)\), yet DMs provide \( p(,t)\): linking these two quantites as above shows that LIDL can be used with a single DM.

### A Better Implementation of LIDL with a Single Diffusion Model

Using Equation 12 with LIDL still involves computing \((t(_{i}))x,t(_{i})\) through Equation 5 for each \(i=1,,m\) before running the regression. Since each of the corresponding ODEs in Equation 4 starts at a different time \(t_{0}=t(_{i})\) and is evaluated at a different point \((t(_{i}))x\), this means that a different ODE solver call would have to be used for each \(i\), resulting in a prohibitively expensive procedure. To address this, we aim to find an explicit formula for \(/\,(x,)\). We do so by leveraging the Fokker-Planck equation associated with Equation 1, which provides an explicit formula for \(/ t\,p(x,t)\). Using this equation along with the chain rule and Equation 12, we show in Appendix B.2 that, for DMs with transition kernel as in Equation 10,

\[(x,)=^{2}t( ) s(t())x,t( )+s(t())x,t() _{2}^{2}=:t();s,x).\] (13)

Then, assuming without loss of generality that \(_{1}<<_{m}\), we can use the above equation to define \((x,)\) through the ODE

\[(x,)=t();,x {d},(x,_{1})=0.\] (14)

Solving this ODE from \(_{1}\) to \(_{m}\) produces the trajectory \(((x,))_{[_{1},_{m}]}\). Since \((t();,x)\) does not depend on \((x,)\), when \(=s\) the solution to the ODE above will be off by a constant, i.e., \((x,)=(x,)+c_{}\) for some \(c_{}\) that depends on the initial condition of the ODE (\(0\) in this case) but not on \(\). Furthermore, while setting the initial condition to \(0\) might at first appear odd, recall that LIDL fits a regression using \(\{(_{i},(x,_{i}))\}_{i=1}^{m}\), and thus \(c_{}\) will be absorbed in the intercept without affecting the slope. In other words, the initial condition is irrelevant, and we can use LIDL with DMs by using a single call to an ODE solver on Equation 14.

### FLIPD: An Efficient Fokker-Planck-Based LID Estimator

The LIDL estimator with DMs presented in Section 3.2 provides a massive speedup over the naive approach of training \(m\) DMs, and over the method from Section 3.1 requiring \(m\) ODE solves. Yet, solving Equation 14 involves computing the trace of the Jacobian of \(\) multiple times within an ODE solver, which, as mentioned in Section 2.1, remains expensive. In this section we present our LID estimator, FLIPD, which circumvents the need for an ODE solver altogether. Recall that LIDL is based on Equation 8, which justifies the regression. Differentiating this equation yields that \(/\,(x,_{0})(x)-D\) for negative enough \(_{0}\), meaning that Equation 13 directly provides the rate of change that the regression in LIDL aims to estimate, from which we get

\[(x) D+(x,_{ 0})=D+t(_{0});s,x D+t(_{0}); ,x=:(x,t_{0}),\] (15)

where \(t_{0} t(_{0})\). Computing FLIPD is very cheap since the trace of the Jacobian of \(\) has to be evaluated only once when calculating \((t(_{0});,x)\). As mentioned in Section 2.1, exact evaluation of this trace has a cost of \((DF)\), but can be reduced to \((kF)\) when using the stochastic Hutchinson trace estimator with \(k D\) samples. Notably, FLIPD provides a massive speedup over the NB estimator - \((kF)\) vs. \((KF+D^{2}K)\) - especially in high dimensions where \(K>D k\). In addition to not requiring an ODE solver, computing FLIPD requires setting only a single hyperparameter, \(_{0}\). Furthermore, since \((t();,x)\) depends on \(\) only through \(t()\), we can directly set \(t_{0}\) as the hyperparameter rather than \(_{0}\), which avoids the potentially cumbersome computation of \(t(_{0})=^{-1}(e^{_{0}})\): instead of setting a suitably negative \(_{0}\), we set \(t_{0}>0\) sufficiently close to \(0\). In Appendix A we include explicit formulas for FLIPD\((x,t_{0})\) for common DMs.

Finally, we present a theoretical result further justifying FLIPD. Note that the \((1)\) term in Equation 7 need not be constant in \(\) as in Equation 8, even if it is bounded. The more this term deviates from a constant, the more bias we should expect in both LIDL and FLIPD. The following result shows that in an idealized linear setting, FLIPD is unaffected by this problem:

**Theorem 3.1** (FLIPD Soundness: Linear Case).: _Let \(\) be an embedded submanifold of \(^{D}\) given by a \(d\)-dimensional affine subspace. If \(p(,0)\) is supported on \(\), continuous, and with finite second moments, then for any \(x\) with \(p(x,0)>0\), we have:_

\[_{-}(x,)=d -D.\] (16)

Proof.: See Appendix B.3. 

We conjecture that our theorem can be extended to non-linear submanifolds since it is a local result and every manifold can be locally linearly approximated by its tangent plane. More specifically, \((x,)\) becomes "increasingly local as \(-\)" in the sense that its dependence on the values \(p(x_{0},0)\) becomes negligible as \(-\) when \(x_{0}\) is not in a close enough neighbourhood of \(x\); this is because \((0,e^{2}I_{D})\) concentrates most of its mass around \(0\) as \(-\) (see Equation 6). However, we leave generalizing our result to future work.

## 4 Experiments

Throughout this section, we use variance-preserving DMs, the most popular variant of DMs. We provide a "dictionary" to translate between the score-based formulation of FLIPD and DDPMs in Appendix C. We hope this will enable practitioners who are less focused on the theoretical aspects to effortlessly apply FLIPD to their pre-trained DMs. Our code is available at https://github.com/layer6ai-labs/flipd; see Appendix D for more experimental details.

### Experiments on Synthetic Data

The effect of \(t_{0}\)FLIPD requires setting \(t_{0}\) close to \(0\) since all the theory holds in the \(-\) regime. It is important to note that DMs fitted to low-dimensional manifolds are known to exhibit numerically unstable scores \(s(,t_{0})\) as \(t_{0} 0\)[65; 41; 40]. This fact does not invalidate FLIPD, but it suggests that it might be sensitive to the choice of \(t_{0}\). Our first set of experiments examines the effect of \(t_{0}\) on \((x,t_{0})\) by varying \(t_{0}\) within the range \((0,1)\).

In Figure 2, we train DMs on two distributions: \((i)\) a mixture of three isotropic Gaussians with dimensions \(2\), \(4\), and \(8\), embedded in \(^{10}\) (each embedding is carried out by multiplication against a random matrix with orthonormal columns plus a random translation); and \((ii)\) a "string within a doughnut", which is a mixture of uniform distributions on a 2d torus (with a major radius of \(10\) and a minor radius of \(1\)) and a 1d circle (aligning with the major circle of the torus) embedded in \(^{3}\) (this union of manifolds is shown in the upper half of Figure 3). While \((x,t_{0})\) is inaccurate at \(t_{0}=0\) due to the aforementioned instabilities, it quickly stabilizes around the true LID for all datapoints. We refer to this pattern as a _knee_ in the FLIPD curve. In Appendix D.2, we show similar curves for more complex data manifolds.

FLIPD is a multiscale estimatorInterestingly, in Figure 1(b) we see that the blue FLIPD curve (corresponding to "doughnut" points with LID of \(2\)) exhibits a second knee at \(1\), located at the \(t_{0}\) shown with a vertical line. This confirms the multiscale nature of convolution-based estimators, first postulated by Tempczyk et al.  in the context of normalizing flows; they claim that when selecting a log standard deviation \(\), all directions along which a datum can vary having log standard deviation less than \(\) are ignored. The second knee in Figure 1(b) can be explained by a similar argument: the torus looks like a 1d circle when viewed from far away, and larger values of \(t_{0}\) correspond to viewing the manifolds from farther away. This is visualized in Figure 3 with two views of the "string within a doughnut" and corresponding LID estimates: one zoomed-in view where \(t_{0}\) is small, providing fine-grained LID estimates, and a zoomed-out view where \(t_{0}\) is large, making both the string and

Figure 2: FLIPD curves with knees at the true LID.

doughnut appear as a 1d circle from this distance. In Appendix D.3 we have an experiment that makes the multiscale argument explicit.

Finding kneesAs mentioned, we persistently see knees in FLIPD curves. This in line with the observations of Tempczyk et al.  (see Figure 5 of ), and it gives us a fully automated approach to setting \(t_{0}\). We leverage kneedle, a knee detection algorithm which aims to find points of maximum curvature. When computationally sensible, rather than fixing \(t_{0}\), we evaluate Equation 15 for \(50\) values of \(t_{0}\) and pass the results to kneedle to automatically detect the \(t_{0}\) where a knee occurs.

Experimental setupWe create a benchmark for LID evaluation on complex unions of manifolds where the true LID is known. We sample from simple distributions on low-dimensional spaces, and then embed the samples into \(^{D}\). We denote uniform, Gaussian, and Laplace distributions as \(,\), and \(\), respectively, with sub-indices indicating LID, and a plus sign denoting mixtures. To embed samples into higher dimensions, we apply a random matrix with orthonormal columns and then apply a random translation. For example, \(_{10}+_{20}^{100}\) indicates a \(10\)-dimensional Gaussian and a \(20\)-dimensional Laplace, each of which undergoes a random affine transformation mapping to \(^{100}\) (one transformation per component). We also generate non-linear manifolds, denoted with \(\), by applying a randomly initialized \(D\)-dimensional neural spline flow  after the affine transformation (when using flows, the input noise is always uniform); since the flow is a diffeomorphism, it preserves LID. To our knowledge, this synthetic LID benchmark is the most extensive to date, revealing surprising deficiencies in some well-known traditional estimators. For an in-depth analysis, see Appendix D.4.

ResultsHere, we summarize our synthetic experiments in Table 1 using two metrics of performance: the mean absolute error (MAE) between the predicted and true LID for individual datapoints; and the concordance index, which measures similarity in the rankings between the true LIDs and the estimated ones (note that this metric only makes sense when the dataset has variability in its ground truth LIDs, so we only report it for the appropriate entries in Table 1). We compare against the NB and LIDL estimators described in Section 2.2, as well as two of the most performant model-free baselines: LPCA  and ESS . For the NB baseline, we use the exact same DM backbone as for FLIPD (since NB was designed for variance-exploding DMs, we use the adaptation to variance-preserving DMs used in , which produces extremely similar results), and for LIDL we use \(8\) neural spline flows. In terms of MAE, we find that FLIPD tends to be the best model-based estimator, particularly as dimension increases. Although model-free baselines perform well in simplistic scenarios, they produce unreliable results as LID increases or more non-linearity is introduced in the data manifold. In terms of concordance index, FLIPD achieves _perfect_ scores in all scenarios, meaning that even

  &  &  &  &  &  \\  String within doughnut \(^{3}\) & \(\) & \(1.00\) & \(1.48\) & \(0.48\) & \(1.10\) & \(0.99\) & \(0.02\) & \(1.00\) & \(\) & \(1.00\) \\ \(_{5}^{10}\) & \(0.17\) & - & \(1.00\) & - & \(\) & - & \(0.07\) & - & \(\) & - \\ \(_{50}^{100}\) & \(0.49\) & - & \(\) & - & \(0.33\) & - & \(\) & - & \(21.9\) & - \\ \(_{10}+_{40}_{90}^{100}\) & \(\) & \(1.00\) & \(61.6\) & \(0.34\) & \(8.46\) & \(0.74\) & \(21.9\) & - & \(0.74\) & \(\) & \(0.86\) \\ \(_{10}+_{25}+_{50}^{100}\) & \(\) & \(1.00\) & \(74.2\) & \(0.34\) & \(8.87\) & \(0.74\) & \(7.71\) & \(0.88\) & \(\) & \(0.91\) \\ \(_{10}+_{25}+_{50}^{100}\) & \(\) & \(1.00\) & \(74.2\) & \(0.34\) & \(18.6\) & \(0.70\) & \(9.20\) & \(0.90\) & \(\) & \(1.00\) \\ \(_{10}+_{40}+_{4200}^{800}\) & \(\) & \(1.00\) & \(715\) & \(0.34\) & \(120\) & \(0.70\) & \(1.39\) & \(1.00\) & \(\) & \(1.00\) \\ \(_{900}^{1000}\) & \(\) & - & \(100\) & - & \(24.9\) & - & \(\) & - & \(219\) & - \\  

Table 1: MAE (lower is better) \(|\)concordance indices (higher is better with \(1.0\) being the gold standard). Rows show synthetic manifolds and columns represent LID estimation methods. Columns are grouped based on whether they use a generative model, with the best results for each metric within each group being bolded.

Figure 3: “String within a doughnut” manifolds, and corresponding FLIPD estimates for different values of \(t_{0}\) (\(t_{0}=0.05\) on top and \(t_{0}=0.65\) on bottom). These results highlight the multiscale nature of FLIPD.

when its estimates are off, it always provides correct LID rankings. We include additional results in the appendices: in Appendix D.5 we ablate FLIPD, finding that using kneedle indeed helps, and that FLIPD also outperforms the efficient implementation of LIDL with DMs described in Section 3.2 that uses an ODE solver. We notice that NB with the setting proposed in  consistently produces estimates that are almost equal to the ambient dimension; thus, in Appendix D.6 we also show how NB can be significantly improved upon by using kneedle, although it is still outperformed by FLIPD in many scenarios. In addition, in Table 7 and Table 8 we compare against other model-free baselines such as MLE [38; 43] and FIS . We also consider datasets with a single (uni-dimensional) manifold where the average LID estimate can be used to approximate the _global_ intrinsic dimension. Our results in Table 9 demonstrate that although model-free baselines indeed accurately estimate global intrinsic dimension, they perform poorly when focusing on (pointwise) LID estimates.

### Experiments with Fully-Connected Architectures on Image Data

We first focus on the simple image datasets MNIST  and FMNIST . We flatten the images and use the same MLP architecture as in our synthetic experiments. Despite using an MLP, our DMs can generate reasonable samples (Appendix E.1) and the FLIPD curve for both MNIST and FMNIST is shown in Figure 3(a). The knee points are identified at \(t_{0}=0.1\), resulting in average LID estimates of approximately \(130\) and \(170\), respectively. Evaluating LID estimates for image data is challenging due to the lack of ground truth. Although our LID estimates are higher than those in  and , our experiments (Table 9 of Appendix D.4) and the findings in  and  show that model-free baselines underestimate LID of high-dimensional data, especially images.

### Experiments with UNet Architectures on Image Data

When moving to more complex image datasets, the MLP backbone fails to generate high-quality samples. Therefore, we replace it with state-of-the-art UNets [52; 68] (see Appendix E.2). Surprisingly, we find that using kneedle with UNets fails to produce sensible LID estimates with FLIPD (see curves in Figure 10 of Appendix E.1). We discuss why this might be the case in Appendix E.1, and from here on we simply set \(t_{0}\) as a hyperparameter instead of using kneedle. Although avoiding kneedle when using UNets results in increased sensitivity with respect to \(t_{0}\), we argue that FLIPD remains a valuable measure of complexity as it produces sensible image rankings and it is highly correlated with with PNG compression length. We took random subsets of \(4096\) images from each of FMNIST, MNIST, SVHN , and CIFAR10 , and sorted them according to their FLIPD estimates (obtained using UNet backbones). We show the top and bottom 5 images for each dataset in Figure 3(b), and include more samples in Appendix E.3. Our visualization shows that higher FLIPD estimates indeed correspond to images with more detail and texture, while lower estimates correspond to less complex ones. Additionally, we show in Appendix E.4 that using only \(k=50\) Hutchinson samples to approximate the trace term in FLIPD is sufficient for small values of \(t_{0}\).

Further, we quantitatively assess our estimates by computing Spearman's rank correlation coefficient between different LID estimators and PNG compression size, used as a proxy for complexity in the absence of ground truth. We highlight that although we expect this coefficient to be high, a perfect LID estimator need not achieve a correlation of \(1\). As shown in Table 2, FLIPD has a high correlation

Figure 4: Overview of image LID: **(a)** shows the FLIPD curves that are used to estimate average LID for MNIST and FMNIST when using MLP backbones; **(b)** compares images with small and large FLIPD estimates from FMNIST, MNIST, SVHN, and CIFAR10 when using UNet backbones; and **(c)** compares LAION images with small and large FLIPD estimates using Stable Diffusion (top, \(t_{0}=0.3\)) and PNG compression sizes (bottom).

with PNG, whereas model-free estimators do not. We find that the NB estimator correlates slightly more with PNG on MNIST and CIFAR10, but significantly less in FMNIST and SVHN. Moreover, in Appendix E.5, we analyze how increasing \(t_{0}\) affects FLIPD by re-computing the correlation with the PNG size at different values of \(t_{0}(0,1)\). We see that as \(t_{0}\) increases, the correlation with PNG decreases. Despite this decrease, we observe an interesting phenomenon: while the image orderings change, qualitatively, the smallest FLIPD\((,t_{0})\) estimates still represent less complex data compared to the highest FLIPD\((,t_{0})\), even for relatively large \(t_{0}\). We hypothesize that for larger \(t_{0}\), similar to the "string within a doughnut" experiment in Figure 3, the orderings correspond to coarse-grained and semantic notions of complexity rather than fine-grained ones such as textures, concepts that a metric such as PNG compression size cannot capture.

We also consider high-resolution images from LAION-Aeshetics  and, for the first time, estimate LID for extremely high-dimensional images with \(D=3 512 512=786{,}432\). We use Stable Diffusion , a latent DM pretrained on LAION-5B . This includes an encoder and a decoder trained to preserve relevant characteristics of the data manifold in latent representations. Since the encoder and decoder are continuous and effectively invert each other, we argue that the Stable Diffusion encoder can, for practical purposes, be considered a topological embedding of the LAION-5B dataset into its latent space of dimension \(4 64 64=16{,}384\). Therefore, the dimension of the LAION-5B submanifold in latent space should be unchanged. We leave an empirical verification of this hypothesis to future work and thus estimate image LIDs by carrying out FLIPD in the latent space of Stable Diffusion. Here, we set the Hutchinson sample count to \(k=1\), meaning we only require a _single_ Jacobian-vector-product. When we order a random subset of \(1600\) samples according to their FLIPD at \(t_{0}=0.3\), the more complex images are clustered at the end, while the least complex are clustered at the beginning: see Figure 1 and Figure 3(c) for the lowest- and highest-LID images from this ordering, and Figure 25 in Appendix E.6 to view the entire subset and other values of \(t_{0}\). In comparison to orderings according to PNG compression size (Figure 3(c)), FLIPD prioritizes semantic complexity over low-level details like colouration.

Finally, we compare the runtimes for computing FLIPD and NB for all models using UNet backbones. We show results in Table 3. For \(28 28\) greyscale (MNIST/FMNIST) and \(3 32 32\) low-resolution RGB (SVHN/CIFAR10) images, we use \(k=50\) Hutchinson samples: at these dimensions, FLIPD achieves \( 10\) and \( 100\) respective speedups over NB. For \(4 64 64\) LAION-Aeshetics images on the latent space of Stable Diffusion, we use \(k=1\) for FLIPD, which ensures it remains highly tractable. At this resolution NB is completely intractable: constructing \(S(x)\) for a single image \(x\) takes \(2.5\) hours, and computing NB would then still require performing a SVD on this \(16,384 65,536\) matrix.

## 5 Conclusions, Limitations, and Future Work

In this work we have shown that the Fokker-Planck equation can be utilized for efficient LID estimation with any pre-trained DM. We have provided strong theoretical foundations and extensive benchmarks showing that FLIPD estimates accurately reflect data complexity. Although FLIPD produces excellent LID estimates on synthetic benchmarks, its instability with respect to the choice of network architecture on images is surprising, and results in LID estimates which strongly depend on this choice. We see this behaviour as a limitation, even if FLIPD nonetheless still provides a meaningful measure of complexity regardless of architecture. Given that FLIPD is tractable, differentiable, and compatible with any DM, we hope that it will find uses in applications where LID estimates have already proven helpful, including OOD detection, AI-generated data analysis, and adversarial example detection.

   Method & MNIST & FMNIST & CIFAR10 & SVHN \\  FLIPD & \(0.837\) & \(\) & \(0.819\) & \(\) \\ NB & \(\) & \(0.480\) & \(\) & \(0.573\) \\ ESS & \(0.444\) & \(0.063\) & \(0.326\) & \(0.019\) \\ LPCA & \(0.413\) & \(0.01\) & \(0.302\) & \(-0.008\) \\   

Table 2: Spearman’s correlation between LID estimates and PNG compression size. FLIPD and NB were computed using the same UNet backbone.

   Method & MNIST/FMNIST & SVHN/CIFAR10 & LAION \\  FLIPD & \(\) & \(\) & \(\) \\ NB & \(1.6\) & \(10.8\) & \(>9 10^{3}\) \\   

Table 3: Time, in seconds, to estimate LID for a single image.