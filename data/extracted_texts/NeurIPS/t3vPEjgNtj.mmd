# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

task, consider a clean testing image of 'Agama' that can be correctly classified by some trained DNNs such as the four networks we tested, let's say a desired list of the ordered top-\(20\) attack targets as shown in the caption. An ordered top-\(K\) attack method would seek to find the adversarial perturbation to the input image that manipulates the DNN's predictions to ensure that the top-\(20\) predicted classes of the perturbed image match the specified list in the given order.

**Why to learn ordered top-\(K\) attacks?** They facilitate exploiting the principle of "class coherence" that reflect real-world scenarios where the relative importance or priority of certain classes is crucial to recognize the relationships or logic connecting classes within ordinal or nominal context. Unlike unordered top-\(K\) or top-\(1\) attacks, ordered top-\(K\) attacks can subtly manipulate predictions while maintaining coherence within the expected context.

* **An Ordinal Example.** Imagine a cancer risk assessment tool that analyzes 2D medical images (e.g., mammograms) to categorize patients' cancer risk into the ordinal 7-level risk ratings _([Extremely High Risk, Very High Risk, High Risk, Moderate Risk, Low Risk, Minimal Risk, No Risk])_, An oncologist could use this tool to triage patients, prioritizing those in the highest risk categories for immediate intervention. An attacker aiming to delay treatment might use _an ordered top-3 adversarial attack_ to change a prediction for a patient initially assessed as Very High Risk. They could target the classes _[Moderate, Low, Minimal]_, subtly downgrading the urgency without breaking the logical sequence of risk categories. An unordered attack, in contrast, might lead to a sequence like _[Low, Very High, Minimal]_, disrupting the ordinal relationship between classes. Such a disruption could raise red flags, making the attack easier to detect.
* **A Nominal Example.** Traffic control systems could use deep learning to optimize flow by adjusting the timing of traffic lights based on the types of vehicles seen. Priority might be given to certain vehicle classes, such as public transit or emergency vehicles, to improve response times. Imagine a city's traffic control system, which has specific traffic light timing behavior for the nominal vehicle categories _[Emergency Vehicle, Public Transit, Commercial Vehicle, Personal Car, Bicycle]_. Public transit might be given slightly extended green lights during rush hours to encourage public transportation use. An attacker wanting to cause delays for personal cars without raising alarms could launch _an ordered top-2 adversarial attack_, targeting the sequence _[Commercial Vehicle, Public Transit]_. This would cause the system to interpret most personal cars as commercial vehicles during the attack, applying the extended green light times meant for public transit to lanes primarily

Figure 1: Adversarial examples learned by our QuadAtac\(K\) (\(K=20\)) for a same clean image using four different networks: two popular convolutional neural networks (ResNet-50 [He et al., 2016] and DenseNet-121 [Huang et al., 2017]), and two widely used Vision Transformers (ViT-B [Dosovitskiy et al., 2020]) and DEiT-S [Touvron et al., 2021]). The ground-truth label is _agama_. **The ordered top-20 targets** (randomly sampled and kept unchanged for the four models) are: [sea cucumber, barrov, odometer, bloodabound, hen-of-the-woods, ringneck snake, snail, tiger shark, Pembroke, altar, wig, submarine, macaw, combination lock, ram, Irish wolfhound, confectionery, buckle, chime, garden spider]. For example, the top-20 classes of the clean image by DEiT-S are [_agama, frilled lizard, shoved, reel, common iguana, Yorkshire terrier, coho, alligator lizard, hand bluower, meerkat, ostrich, mongoose, fiddler crab, eth, wing, bustard, green lizard, whiptail, brass, American chameleon_], which have more or less visual similarities. More examples are the Appendix B.

used by commercial vehicles. An unordered top-2 attack that may result in _[Emergency Vehicle, Commercial Vehicle]_, would likely be quickly detected, as emergency vehicle priority changes are significant and could be easily noticed by traffic operators (this weakness is exacerbated in any top-1 attack or unordered attacks).

**Successful ordered top-\(K\) attacks can potentially provide several advantages:** enabling better controllability in learning attacks that are more difficult to detect, revealing deeper vulnerability of trained DNNs, and testing the robustness of an attack method itself, especially when \(K\) is relatively large (e.g., \(K 15\)) and the computing budget is relatively low (e.g., 60 steps of optimization).

**Learning ordered top-\(K\) attacks is an extremely challenging problem.** The adversarial distillation method (Zhang and Wu, 2020) is the state of the art method (see Sec. 3.1), which presents a heuristic knowledge-oriented design of the ordered top-\(K\) target distribution, and then minimizes the Kullback-Leibler divergence between the designed distribution and the DNN output distribution (after softmax). It often completely fails when \(K>10\). In this paper, we show it is possible to learn those attacks for a variety of DNNs, including ResNet-50 (He et al., 2016), DenseNet-121 (Huang et al., 2017) and Vision Transformers (Dosovitskiy et al., 2020; Touvron et al., 2021).

**The key to learning clear-box targeted attacks (\(K 1\)) lies in the objective function for optimization**, which usually consists of two terms, one is the \(_{p}\) norm (e.g., \(_{2}\)) of the learned adversarial perturbation (to be as small as possible to be visually imperceptible), and the other is the surrogate loss capturing the specified attack constraints such as the top-\(K\) extended C&W (hinge) loss (Carlini and Wagner, 2017) and the adversarial distillation loss proposed in (Zhang and Wu, 2020) (see Sec. 3.1). The trade-off between the two terms are often searched in optimization with respect to a certain computing budget (e.g., \(9 30\) means to test 9 different trade-off parameter assignments based on linear search in a predefined range, and to run \(30\) forward-backward computation iterations of the DNN per trade-off parameter search step). After the optimization, Attack Success Rates (ASR, higher is better) and some \(_{p}\) norms (e.g., \(_{1}\) and \(_{2}\)) of learned successful perturbations (smaller is better) are used as evaluation metrics. As illustrated in Fig. 2, in this paper, we propose a novel formulation which is different from the prior art in the three aspects as follows:

* We identify that while sufficient to capture the top-\(K\) attack constraint, hand-crafted surrogate losses are not necessary and often introduce inconsistency and artifacts in optimization (see Sec. 3.2). We eliminate the need of introducing surrogate losses. Instead, we keep the top-\(K\) attack constraints in the vanilla form and cast the optimization problem as quadratic programming (QP). We solve the QP by leveraging a recently proposed differentiable QP layer (for PyTorch) (Amos and Kolter, 2017). We present an efficient implement to parallelize the batched QP layer for any ordered top-\(K\) targets specified individually for each instance in a batch.
* We observe that directly minimizing the \(_{p}\) norm of learned perturbations together with the hand-crafted surrogate loss could miss the chance of exploiting semantic structures of the feature embedding space (i.e., the input to the final linear classifier). Instead, we minimize the Euclidean distance between the feature embedding vectors at two consecutive iterations in the optimization. _This can be understood as the latent perturbation learning versus the raw data perturbation learning_(see Sec. 3.3). Our proposed latent perturbation learning enables more consistent optimization trajectories in pursuing the satisfaction of the specified top-\(K\) attack constraints. The minimized Euclidean distance is then used as the loss together with the \(_{p}\) norm of the learned perturbation in computing the adversarial perturbation via back-propagation at each iteration.

Figure 2: Illustration of the proposed QuadAttack\(K\) method in comparison with the prior art (e.g., the adversarial distillation (AD) method (Zhang and Wu, 2020)).

Related Work and Our Contributions

**Adversarial Attacks** have remained as a critical concern for DNNs, where imperceptible perturbations to input data can lead to significant misclassification (Xie et al., 2017; Hendrik Metzen et al., 2017; Chen et al., 2019; Liu et al., 2018). Various approaches have been proposed to investigate the vulnerabilities of DNNs and exploit their sensitivity to non-robust features. Notable works include the seminal discovery of visually imperceptible adversarial attacks (Szegedy et al., 2013), which highlighted the need to evaluate the brittleness of DNNs and address it with explicit defense methods (Madry et al., 2017; Cohen et al., 2019; Wang et al., 2020; Wong et al., 2020). Clear-box attacks, which assume full access to the DNN model, have been particularly effective in uncovering vulnerabilities (Madry et al., 2017). The C&W method (Carlini and Wagner, 2017), for instance, introduced loss functions that have been widely adopted to generate adversarial examples. Momentum-based methods like (Dong et al., 2018; Lin et al., 2019) and gradient projection techniques like PGD (Madry et al., 2017) have also been successful in crafting adversarial examples.

**Unordered top-\(K\) Attacks** aim to manipulate the top-\(K\) predicted classes of a DNN without enforcing a specific order among them. A common approach is to optimize a loss function that combines multiple objectives, such as maximizing the probability of the target classes while minimizing the probabilities of other classes simultaneously. (Tursynbek et al., 2022) presents a geometry inspired method that allows taking gradient steps in favor of simultaneously maximizing all target classes while maintaining a balance between them. Other approaches, for example, may use sorting strategies (Kumano et al., 2022) to limit the set of logits involved simultaneously in a loss and target specific logits that contribute most to a misclassification. The "Superclass" attacks have been proposed in (Kumano et al., 2022), for which ordered top-\(K\) attacks can been seen as a generalization. Non-targeted top-\(K\) attacks are studied in (Zhang et al., 2022).

**Ordered top-\(K\) Attacks**(Zhang and Wu, 2020) require preserving both the attack success rate and the order of the top-\(K\) predicted classes. Following the intuitions from (Kumano et al., 2022) we note that adversarial sample detection and defense methods (Lee et al., 2018; Huang and Li, 2021; Aldadooh et al., 2022) may benefit from the fact that many adversarial attacks tend to generate a nonsensical class prediction (e.g. the top-\(K\) predictions may all be from a different super-class (Kumano et al., 2022)). An ordered top-\(K\) adversarial attack may choose to create an attack that is semantically plausible and have a higher potential to fool detection methods and defense methods. The sorting constraint in ordered top-\(K\) attacks adds a layer of complexity to the optimization problem. By enforcing a specific order among the top-\(K\) predictions, the attacker must not only manipulate the logits to maximize the target classes' probabilities but also ensure that the predicted order aligns with the desired order. In (Zhang and Wu, 2020), two methods are presented through the use of semantic information in the form of class language embedding.

**Constrained Optimization in DNNs**, such as OptNet (Amos and Kolter, 2017) and other differentiable optimization works (Agrawal et al., 2019; Butler and Kwon, 2023), have introduced powerful formulations for integrating optimization layers within the network. These types of works have produced many DNN based problem solutions while also being able to use domain knowledge to constrain solutions and reduce data requirements (Sangalli et al., 2021) and solve problems otherwise intractable with DNNs (Wang et al., 2019; Mandi et al., 2020). While OptNet's quadratic solver is typically used as a layer, our focus is on attacking a pre-trained model with a fixed architecture. Thus, we leverage the principles of constrained optimization to formulate an objective function that captures the constraints of ordered top-\(K\) adversarial attacks. This adaptation allows us to guide the attack process to satisfy the ordering constraints while maximizing target class probabilities, which provides new insights for enhancing attack effectiveness and robustness.

**Our Contributions** This paper makes two main contributions to the field of learning clear-box targeted adversarial attacks:

* It presents a quadratic programming (QP) approach to learning ordered top-\(K\) attacks, dubbed as QuadAttac\(K\). It eliminates hand-crafting surrogate loss functions to capture the top-\(K\) attack constraint. It provides a QP based formulation to better exploit the semantics of the feature embedding space in capturing the top-\(K\) attack requirement.
* It obtains state-of-the-art adversarial attack performance in ImageNet-1k classification using both ConvNets (ResNet-50 and DenseNet-121) and Transformer models (ViT-B and DEiT-S). It pushes the limit of the number of targets, \(K\) to a large number, for which the prior art completely fails.

Approach

In this section, we first define the problem of learning ordered top-\(K\) attacks following (Zhang and Wu, 2020), as well as the extended top-\(K\) C&W (CW\({}^{K}\)) method and the adversarial distillation (AD) method. We then present details of our proposed QP based formulation, i.e., QuadAttac\(K\).

### Learning Ordered top-\(K\) Clear-Box Targeted Attacks: the Problem and the Prior Art

We consider image classification DNNs, which consist of a feature backbone and a head linear classifier. Denote by \(F:x^{3 H W} z^{D h w}\) the feature backbone which transforms an input image to its feature map, where an input image \(x\) is often a RGB image of the spatial height and width, \(H\) and \(W\) (e.g., \(224 224\)), clean or perturbed, and the feature map \(z\) is in a \(D\)-dim feature space with the the spatial height and width, \(h\) and \(w\) (e.g., \(7 7\)) based on the overall spatial downsampling stride implemented in the feature backbone. Let \(^{D}\) be feature embedding vector for \(x\) via any spatial reduction method (e.g. global average pooling). Denote by \(f:^{D} l^{C}\) the linear head classifier, where \(C\) is the number of classes (e.g., 1000 in ImageNet-1k (Russakovsky et al., 2015)), and \(l\) is the output logit vector. We have,

\[l=f(F(x;);A,B)=A+B,\] (1)

where \(\) collects all the model parameters of the feature backbone, and \(A^{C D}\) and \(B^{C}\) the weight and bias parameters of the head linear classifier. In learning clear-box attacks, we assume all the information of the network are available, and the parameters \((,A,B)\) are frozen throughout.

Denote by \((x,y)\) a pair of clean image and its ground-truth label (\(y=\{1,2,,C\}\)). For learning attacks, we assume \(x\) can be correctly classified by the network, i.e., \(y=_{i}l_{i}\). Denote by \(T\) the ordered list of attack target(s) with the cardinality \(K=|T|\), where the ground-truth label is excluded, \(y T\), and by \(T^{c}= T\) the complement set. Let \((x,T;F,f)\) be the adversarial perturbation to be learned. **An ordered top-\(K\) adversarial example** is defined by \(=x+(x,T;F,f)\) if the top-\(K\) prediction classes for \(\) equal to \(T\) based on its logits \(\) (Eqn. 1).

Learning ordered top-\(K\) attacks is often posed as a constrained optimization problem,

\[} \|\|_{p},\] (2) subject to \[_{t_{i}}>_{t_{i+1}}, i[1,K-1], t_{i}  T,\] \[_{t_{K}}>_{j}, t_{K} T, j T ^{c},\] \[=x+^{3 H W},\] \[=f(F(;);A,B),\]

where the first two constraints capturing the ordered top-\(K\) attack requirement. This traditional formulation leads to the challenge in optimization, even with \(K=1\) as pointed out in the vanilla C&W method (Carlini and Wagner, 2017). Some surrogate loss, \(()\), is necessary to ensure the first two terms are satisfied when \(()\) is minimized. We have,

\[}{} ()+\|\|_{p},\] (3) subject to \[=x+^{3 H W},\]

where \(\) is the trade-off parameter between the visual imperceptibility of learned perturbations and the ASR. The remaining constraint can be addressed via a projected descent in optimization. So, the optimization can enjoy the straightforward back-propagation algorithm that is used in training the DNN on clean images.

The extended top-\(K\) C&W (hinge) loss (Zhang and Wu, 2020) is defined by,

\[^{K}_{CW}()=_{i=1}^{K}(0,_{j \{t_{1},,t_{i}\}}_{j}-_{t\{t_{1},,t_{i}\} }_{t}).\] (4)

And, the adversarial distillation loss (Zhang and Wu, 2020) is defined by,

\[^{K}_{AD}()=KL(\|P^{AD})=_{t_{i} T}_{t_ {i}}(_{t_{i}}- P^{AD}_{t_{i}})+_{j T^{c}}_{j}( _{j}- P^{AD}_{j}),\] (5)

where \(=()\) and \(P^{AD}\) is the knowledge-oriented heuristically-designed adversarial distribution with the top-\(K\) constraints satisfied (see (Zhang and Wu, 2020) for details). \(KL(\|)\) is the Kullback-Leibler divergence between two distributions.

### Limitations of the Prior Art

From the optimization perspective, we observe there are two main drawbacks in the aforementioned formulations (Eqns. 3, 4, 5):

* The two surrogate loss formulations (Eqns. 4 and 5) are sufficient, but not necessary. They actually introduce inconsistency and artifacts in optimization. The extended top-\(K\) C&W loss in Eqn. 4 is not aware of, and thus can not preserve, the subset of targets whose relative order has been satisfied. For example, consider there are 5 classes in total, and a specified ordered top-\(3\) list of targets, \(\). Assume at a certain iteration, the predicted classes for \(\) in sort are \(\), in which the relative order of the specified 3 targets has been satisfied. The loss \(_{CW}^{3}()=_{i=1}^{3}(_{4}-_{i})\), which mainly focuses on pushing down the logit \(_{4}\) and/or pulling up the logits, \(_{i}\)'s (\(i=1,2,3\)). So, at the next iteration, it may results in the sorted prediction like \(\), leading to a totally wrong relative order. The adversarial distillation loss in Eqn. 5 has similar problems, i.e., the first part, \(_{i T}_{i}(_{i}- P_{i}^{AD})\) is not aware of some satisfied relative order. It further enforces order between non-target classes since the adversarial distribution \(P^{AD}\) needs to be specified before the optimization, as shown in the second part, \(_{j T^{c}}_{j}(_{j}- P_{j}^{AD})\).
* Directly minimizing \(||||_{p}\) in Eqn. 3 (i.e., adversarial learning in the data space) may actually hinder the effectiveness of learning adversarial examples due to the fact that the \(_{p}\) norm is totally unaware of the underlying data structures in the complex data space. Since a trained DNN is kept frozen in learning attacks, we can first perform adversarial learning in the feature embedding space (i.e., the head linear classifier's input space, \(\) in Eqn. 1), which has been learned to be discriminatively and/or semantically meaningful. With a learned adversarial perturbation for \(\), we can easily compute the perturbation for the input data.

We address these limitations in this paper by proposing a novel QP based formulation - QuadAttack\(K\). We present the detail of our QuadAttac\(K\) in the following sub-sections.

### The Proposed QuadAttac\(K\)

We first show that any specified ordered top-\(K\) attack requirements, \(T\), can be cast as linear constraints in a compact matrix form, denoted by \(D_{T}\). Consider \(_{2}\) norm of Eqn. 2, we can rewrite it as,

\[} ||-x||_{2}^{2},\] (6) subject to \[D_{T}>0, D_{T}\{-1,0,1\}^{C-1 C},\] \[=x+^{3 H W},\] \[=f(F(;);A,B),\]

where \(D_{T}\) is a \(C-1 C\) matrix constructed from the specified targets \(T\), and \(C\) is the number of classes in total. Consider the aforementioned toy example where \(C=5\) and the ordered top-\(3\) attack targets are \(T=\), we have,

\[_{2}-_{3}>0,\] (7) \[_{3}-_{1}>0,\] \[_{1}-_{4}>0,\] \[_{1}-_{5}>0,\] (8)

where Eqn. 7 is the vanilla form of expressing the specified top-\(3\) attack requirements, and Eqn. 8 is the equivalent compact matrix form.

However, Eqn. 6 can not be easily solved via QP due to the highly nonconvex nature of the feature backbone \(F\) in the third term of the constraints. This nonconvexity hinders the ability to solve the problem and find an optimal solution while satisfying constraints. Eqn. 6 aims to directly seek the adversarial perturbations in the data space, which has to include the nonconvex feature backbone \(F\) in the optimization. Since \(F\) is a frozen transformation in learning attacks, and the head classifier \(f\) is a linear function, **we can separate the learning of ordered top-\(K\) attacks in two steps:**

i) We first satisfy the ordered top-\(K\) constraints without resorting to hand-crafted surrogate losses via QP in the feature embedding space (i.e., the output space of \(F\) and the input space of \(f\)). The lastconstraint in Eqn. 6 will be replaced by \(=A+B\) (see Eqn. 1). Denote by \(\) the image perturbation at the current iteration. We have the current feature embedding vector \(=(F(x+;))\). our proposed QuadAtac\(K\) first solves the perturbed \(\) via QP,

\[*{minimize}_{} \|-\|_{2}^{2},\] (9) subject to \[D_{T}>0,\] \[=A+B.\]

ii) After having found \(\) we then use the residual (Euclidean distance) \(||-||_{2}^{2}\) as the loss to find the perturbed image \(\). Specifically, with the optimized \(\) (i.e., the closest point in latent space where top-\(K\) attack constraints are satisfied), we compute the image perturbation using vanilla one-step back-propagation with a learning rate \(\) and a loss weighting parameter \(\). We find that both \(\) or \(\) may be used to control the tradeoff between attack success and magnitude of the perturbation found (see Fig. 3 and Appendix A). We have,

\[^{*}= -( ||-||_{2}+||||_{p}),\] (10) \[= (x+^{*};0,1)-x,=x+,\]

where \((;0,1)\) is an element-wise projection of the input onto \(\).

**Understanding QuadAtac\(K\):** In our QuadAtac\(K\), the matrix \(A\) plays a crucial role in encoding useful semantic class relationships. For instance, if classes like "cat" and "building" have distinct and separate representations in the latent space, the matrix \(A\) will reflect these differences. As a result, the optimization problem would naturally prioritize target logits that have high activations for either "cat" or "building," but not both simultaneously. This semantic constraint in the feature embedding space helps guide the search towards relevant and meaningful perturbations that maintain the desired order constraints while avoiding conflicting activations for disparate classes. By incorporating the matrix \(A\) to capture semantic relationships, our QuadAttac\(K\) not only overcomes the nonconvexity challenge of the original problem but also leverages meaningful class information to guide the search and generate effective adversarial perturbations.

### Fast and Parallel Quadratic Programming Solutions

An efficient solver is crucial for addressing the QP formulation of learning ordered top-\(K\) adversarial attacks. To that end, a fast, parallel, and GPU-capable quadratic programming solver is required. In this context, the _qpth_ package created by [Amos and Kolter, 2017] emerges as a suitable choice, providing a PyTorch-enabled differentiable quadratic programming solver that enables efficient optimization while harnessing the power of GPUs,

\[*{minimize}_{} ^{}Q+p^{}\] (11) subject to \[G h,\] \[W=b.\]

The main challenge lies in transforming our current formulation into one compatible with the _qpth_ package (Eqn. 11). This involves structuring the objective and constraints to match the required standard form, as well as building the required matrices \(Q,G\) and \(h\) from attack targets in an efficient and parallel way. Finding \(Q\) and \(p\) is straightforward from an expansion of our squared Euclidean distance objective since \(||-||_{2}^{2}=^{}-2^{}+^{}\). The term \(^{}\) is a constant in our optimization so we can just consider the optimization of \(^{T}-2^{T}\). From this we can trivially see \(Q=2I\) where \(I\) is the identity matrix and \(p=-2^{}\). Further, since we need no equality constraints \(W=0,b=0\). To formulate \(G\) and \(h\) we can rewrite the constraints in Eqn 9 as follows,

\[D_{T}(A+B)>0-D_{T} A D_{T}  B-,\] (12)

where \(\) is the slack variable which is a small non-zero constant to allow our constraints to define a closed-convex set allowing equality in our constraint but still maintaining top-\(K\) order when the constraint is satisfied. We have found \(=0.2\) is an acceptable value, but other small values will also work. Intuitively if our current value for \(\) does not satisfy our top-\(K\) constraints then \(=0\) would find a \(\) on the boundary of the latent space that satisfies our constraints. The higher \(\) is the further away from the boundary and inside the set \(\) becomes. From the above rearrangement we can easily see \(G=-D_{T} A\) and \(h=D_{T} B-\) in Eqn. 11.

## 4 Experiments

In this section, we evaluate our QuadAttac\(K\) with \(K=1,5,10,15,20\) in the ImageNet-1k benchmark [Russakovsky et al., 2015] using two representative pretrained ConvNets: the ResNet-50 [He et al., 2016] and the DenseNet-121 Huang et al. , and two representative pretrained Transformers: the vanilla Vision Transformer (Base) [Dosovitskiy et al., 2020] and the data-efficient variant DEiT (small) [Touvron et al., 2021]. The ImageNet-1k pretrained checkpoints of the four networks are from the mmpretrain package [Contributors, 2023], in which we implement our QuadAttac\(K\) and re-produce both CW\({}^{K}\) and AD [Zhang and Wu, 2020].

**Data and Metric.** In ImageNet-1k [Russakovsky et al., 2015], there are \(50,000\) images for validation. To study attacks, we utilize the subset of images for which the predictions of all the four networks are correct. To reduce the computational demand, we randomly sample a smaller subset following [Zhang and Wu, 2020]: We iterate over all 1000 categories and randomly sample an image labeled with it, resulting in 1000 test images in total. For each \(K\), we randomly sample 5 groups of \(K\) targets with ground-truth (GT) label exclusive for each image in our selected test set, and then compute the Best, Mean and Worst ASRs, as well as the associated \(_{1},_{2}\) and \(_{}\) energies. We mainly focus on low-cost learning of attacks (within 60 steps of optimization) for better practicality and better understanding of the underlying effectiveness of different attack methods. We also learn attacks with higher budgets (\(9 60\) and \(9 30\)) for \(K=5,10\). We use a default value for the trade-off parameter \(\) in Eqn. 3 and Eqn. 10. _Details are provided in the Appendix A and our released code repository._

**Baselines.** We compare our QuadAttac\(K\) with previous state-of-the-art methods, namely, the top-\(K\) extended CW\({}^{K}\) method and the Adversarial Distillation (AD) proposed in [Zhang and Wu, 2020]. We reproduce them in our code repository and test them on the four networks under the same settings for fair comparisons.

**An important detail in optimization:** In optimization, perturbations are initialized with some small energy white Gaussian noise. During the initial steps of optimization, the optimizer takes steps with large increases in perturbation energy since happens to be away from many required energies for a successful attack. These large increases in energy induces a momentum in the optimizer, which makes it difficult to reduce L2 energy in future iterations even if our objective function's gradient points towards a direction with minimal energy. By introducing a small number of warmup steps (e.g., 5, as commonly done in training a network on ImageNet from scratch) after which the optimizer's state is reset, we observe the performance of all analyzed methods are significantly improved. Our QuadAttac\(K\) benefits most.

**Results.** The comparison results are shown in Table 1. Our proposed QuadAttac\(K\) retains performance comparably for \(K=1\), and obtains consistently better performance, often by a large margin, for \(K=5,10,15,20\). Especially, it addresses the challenges associated with large values of \(K\) (e.g. \(K 10\)) under the low-cost budget setting (\(1 60\)). The prior art completely fails, while our QuadAttac\(K\) can still obtain appealing ASRs.

**Analyses on the Trade-Off Between ASRs and Attack Energies.** Since we have two metrics (ASR and attack energy), the trade-off between them needs to be compared in order to comprehensively understand different methods. The trade-off curves (Fig. 3) explore the concept of how a higher success rate may be achieved by choosing to have higher energies and conversely a lower energy may be achieved by choosing to have a lower success rate. They holistically compare the capacity of QuadAttac\(K\) against the prior art - the adversarial distillation method [Zhang and Wu, 2020].

Figure 3: ASR vs \(_{2}\) energy tradeoff curves, which holistically compare the capacity of our QuadAttac\(K\) against the prior art – the adversarial distillation method [Zhang and Wu, 2020], verifying our QuadAttac\(K\)’s advantages.

[MISSING_PAGE_FAIL:9]

**Qualitative Results.** We show more examples in Appendix B. From those, we note that for our QuadAttac\(K\), when the target classes deviate significantly from the original predicted classes, we often observe a perturbation that achieves the prescribed top-\(K\) targets without a substantial margin between each of the top-\(K\) targets. This outcome reflects the desirable effect of our approach, as the primary objective of the ordered top-\(K\) attack is to enforce a specific class order rather than optimizing for class probability differences. Our method's strength lies in its ability to handle such scenarios without relying on explicit assumptions about the distances between classes. By prioritizing the order constraints, our QuadAttac\(K\) offers a robust solution that aligns with the fundamental goal of enforcing class order in adversarial attacks.

## 5 Limitations of Our QuadAttac\(K\)

There are two main limitations that worth further exploring. Our proposed QuadAttac\(K\) is specifically designed for clear-box attack setting, which makes it not directly applicable to opaque-box attacks. The attacks learned by our QuadAttac\(K\) are not easily transferable between different networks, as they are specifically optimized in the feature embedding space for the target model. Exploring the transferability of attacks and developing more generalizable strategies across various networks could be an intriguing direction for future research. In addition, our QuadAttac\(K\) entails solving a QP at each iteration, which introduces additional computational overhead compared to methods like CW\({}^{K}\) and AD (Zhang and Wu, 2020). For example, for ResNet-50, we observed an average QuadAttac\(K\) performs 2.47 attack iterations per second whereas AD performs 32.02 iterations per second (a factor of 12.96). For ViT-B, QuadAtt\(K\) performs 2.96 attack iterations per second whereas AD performs 11.86 iterations per second (a factor of 4). We note that as the target model becomes larger, the adversarial loss constitutes a smaller fraction of total runtime thus the ratio tends toward one. Further, we note that quicker attack iterations of QuadAttac\(K\) on ViT-B which indicate our QP solver converges faster on ViT-B attacks. To address the overhead of our QuadAttac\(K\), we will explore and compare how the QP solver could be adjusted to initialize the QP solver at the previous iteration's solution to nearly eradicate the cost of the QP solver in future work.

## 6 Broader Impact

Our proposed QuadAttac\(K\) method showcases the effectiveness of utilizing QP techniques in the challenging domain of learning ordered top-\(K\) clear-box targeted adversarial attacks. The underlying QP formulation offers opportunities for exploring other applications beyond adversarial attacks. For instance, it could be leveraged to design new loss functions going beyond the traditional cross-entropy loss or the label smoothing variant (Szegedy et al., 2015), and thus jointly optimizing accuracy and robustness. We can enforce semantically meaningful class orders in training a network from scratch, thus allowing for the incorporation of explicit constraints in neural network predictions and potentially resulting in a more interpretable and controlled decision-making process.

_Potential Negative Societal Impact._ As discussed in the introduction, there are some potential scenarios in practice for which the proposed ordered top-\(K\) adversarial attacks may be risky if applied. However, since we focus on clear-box attacks, they are less directly applicable in practice compared to opaque-box attacks, which makes the concern less serious.

## 7 Conclusions

This paper presents a quadratic programming (QP) based method for learning ordered top-\(K\) clear-box targeted attacks. By formulating the task as a constrained optimization problem, we demonstrate the capability to achieve successful attacks with larger values of \(K\) (\(K>10\)) compared to previous methods. The proposed QuadAttac\(K\) is tested in the ImageNet-1k classification using ResNet-50 and DenseNet-121, and ViT-B and DEiT-S. It successfully pushes the boundary of successful ordered top-\(K\) attacks from \(K=10\) up to \(K=20\) at a cheap budget (\(1 60\)) and further improves attack success rates for \(K=5\) for all tested models, while retaining the performance for \(K=1\). The promising results highlight the potential of QP and constrained optimization as powerful tools opening new avenues for research in adversarial attacks and beyond.