# Pretrained Optimization Model for Zero-Shot Black Box Optimization

Xiaobin Li

Xidian University

22171214784@stu.xidian.edu.cn

&Kai Wu

Xidian University

kwu@xidian.edu.cn

&Yujian Betterrest Li

Xidian University

bebetterest@outlook.com

&Xiaoyu Zhang

Xidian University

xiaoyuzhang@xidian.edu.cn

&Handing Wang

Xidian University

hdwang@xidian.edu.cn

&Jing Liu

Xidian University

neouma@mail.xidian.edu.cn

Corresponding author

###### Abstract

Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/nnja-wm/POM/.

## 1 Introduction

Black box optimization, including tasks like hyperparameter optimization (HPO) , neuroevolution [2; 3; 4], neural architecture search (NAS) , and algorithm selection , is very important. In these scenarios, the algorithm can evaluate \(f()\) for any solution \(\); however, access to additional information about \(f\), such as the Hessian and gradients, is unavailable.

Addressing diverse BBO problems necessitates the tailored design of specific algorithms to achieve satisfactory performance. Crafting these algorithms typically demands substantial expertise. Therefore, it is crucial to ensure reliable and robust performance of the optimizer in various applications, called zero-shot optimization. Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer.

The studies  employed Transformer or diffusion models to pretrain model-based optimizers using offline datasets. While effective, these methods primarily fit optimization trajectories of other BBO algorithms to a specific task, potentially requiring retraining for new tasks, limiting their ability to zero-shot optimization. Subsequently,  introduced two learned optimization frameworks for meta-learning evolution strategy (ES) and genetic algorithm (GA). However, the performance of these two methods on zero-shot optimization is weaker than that of CMA-ES  (see Section 4.2).

To address zero-shot optimization, especially for continuous optimization, we introduce a population-based Pretrained Optimization Model, called POM. Leveraging multiple individuals, population-based optimizers gain a better understanding of the fitness landscape. The core of the optimizer is how to design optimization strategies that sample better solutions. Inspired by the solution-producing mechanism of evolutionary computation, we design powerful POM blocks to form a general optimization strategy representation framework. Drawing inspiration from , we introduce an end-to-end gradient-based training method for POM, termed _MetaGBT_ (Meta Gradient-Based Training), ensuring stable and rapid training for POM. Pretraining POM on a set of training functions with _MetaGBT_ ensures good optimization strategy. Our contributions can be summarized as follows:

* **Excellent ability to solve zero-shot BBO**. We develop a efficient POM for zero-shot BBO, demonstrating a substantial performance advantage over state-of-the-art black-box optimizers.
* **Excellent ability to solve few-shot BBO**. Few-shot optimization is the existence of a small budget of function evaluations for the target task to tune the optimizer for better performance. More than 30% performance improvement can be obtained with 25 random function evaluations.

## 2 Related Work

**Heuristic Population-based BBO Algorithms**. Numerous metaheuristic population-based algorithms, such as genetic algorithms , evolution strategies , particle swarm optimization , and differential evolution , have been devised to address optimization problems. Notably, CMA-ES  and L-SHADE  stand out as state-of-the-art methods for BBO. However, these approaches rely on manually designed components, exhibiting inefficiency and fragility when confronted with new tasks. In contrast, the proposed POM can autonomously acquire optimization strategies from problem instances, mitigating the aforementioned limitations.

**Pretrained Population-based BBO Algorithms**. Pre-training BBO algorithms can be categorized into two types within the meta-learning framework. The first type frames meta-learning BBO algorithms as a bi-level optimization problem . For instance,  leverages meta-learning to infer population-based black-box optimizers that automatically adapt to specific task classes. LES  designs a self-attention-based search strategy for discovering effective update rules for evolution strategies through meta-learning. Subsequent works like LGA  utilize this framework to discover the update rules of Gaussian genetic algorithms via Open-ES . The second type models the meta-learning of a BBO algorithm as a reinforcement learning problem.  meta-learn a policy that adjusts the mutation step-size parameters of CMA-ES . Category one faces the curse of dimensionality, where an escalating number of model parameters leads to skyrocketing training difficulty, impeding the development of intricate strategies. In contrast, category two, which models meta-learning optimizers as reinforcement learning tasks, grapples with training instability. POM, employing a gradient-based end-to-end training approach, successfully bypasses the curse of dimensionality, ensuring stable training.

**LLM for Optimization**. In line with POMs, various optimization approaches leveraging Large Language Models (LLMs) have emerged to address diverse problem domains, including NP-hard problems , algorithm evolution , reward design , and Neural Architecture Search (NAS) . Notably, LLMs play a role in sampling new solutions. However, their optimization strategies depend on externally introduced natural selection mechanisms and are less effective in numerical optimization scenarios . LLMaMoCo  and EoH  use LLM to generate code to solve optimization problems, but the performance of LLMaMoCo depends on carefully designed instructions and prompts, and EoH has expensive evaluation costs. TNPs , ExPT  and LICO  use transformer structures to solve the BBO problem and have achieved good results. However, TNPs requires contextual information of the target problem, and neither ExPT nor LICO can be directly used to solve tasks with different dimensions from the training task. These methods lack the universal applicability as pretrained BBO models due to a deficiency in generating capabilities across tasks.

All the above methods cannot be the zero-shot optimizer. The first two categories need to adjust the hyperParameters when optimizing the new tasks, while the latter must fine-tune the instructions to achieve satisfactory results.

## 3 Pretrained Optimization Model

### Problem Definition

A black-box optimization problem can be transformed as a minimization problem, and constraints may exist for corresponding solutions: \(_{}\ f(),s.t.\ x_{i}[l_{i},u_{i}]\), where \(=(x_{1},x_{2},,x_{d})\) represents the solution of optimization problem \(f\), the lower and upper bounds \(=(l_{1},l_{2},,l_{d})\) and \(=(u_{1},u_{2},,u_{d})\), and \(d\) is the dimension of \(\). For more background information on evolutionary algorithms, see Appendix A.

**Definition 1 Zero-shot Optimization**.: _Zero-shot optimization refers to an optimizer that is applied directly to solve a continuous black-box optimization problem \(f\) without any tuning. This means that the optimizer does not require any contextual information about \(f\) and can be directly used to handle problems of any dimensionality._

**Definition 2 Few-shot Optimization**.: _Alternatively, it is permissible to fine-tune the optimizer using a small portion of the function evaluation budget for the objective task, and then use the fine-tuned optimizer to solve \(f\)._

### Classic Population Optimization Algorithm

In this section, we use Differential Evolution (DE) as an example to review classic evolutionary algorithms. DE [20; 43] is a prominent family within evolutionary algorithms (EAs), known for its advantageous properties such as rapid convergence and robust performance [44; 45]. The optimization strategy of DE primarily involves mutation and crossover operations.

The classic DE/rand/1 crossover operator is illustrated in Eq. (1) (additional examples are listed in Appendix A.2). Each mutation strategy can be viewed as a specific instance of Eq. (2); Further details are provided in Appendix A.2. Additionally, we represent the mutation strategy in a matrix form, as shown in Eq. (3). The matrix \(\) evolves with the generation index \(t\), indicating that the mutation strategy adapts across different generations. Consequently, we propose a module to enhance the performance of the mutation operation, which leverages the information from the population of the \(t\)th generation to generate \(^{t}\). This serves as the motivation for our design of the LMM.

\[^{t}_{i}=^{t}_{r1}+F(^{t}_{r2}-^ {t}_{r3})\] (1)

In the crossover phase at step \(t\), DE uses a fixed crossover probability \(cr^{t}_{i}\) for each individual \(^{t}_{i}\) in the population, as shown in Eq. (9). The crossover strategy for the entire population can then be expressed as a vector \(^{t}=(cr^{t}_{1},cr^{t}_{2},,cr^{t}_{N})\). Our goal is to design a module that adaptively generates \(^{t}\) using the information from the population. This approach allows for the automatic design of the crossover strategy by controlling the parameter \(cr\). This serves as the motivation for our design of LCM.

### Design of POM

A population consists of \(n\) individuals, denoted as \(=\{_{1},_{2},,_{n}\}\). In this paper, \(\) is also treated as \(=[_{1},_{2},,_{n}]^{T}\) to support matrix operations. We feed POM an initial random population \(^{0}\) at step 0, specify the evolution generation \(T\) for it, and hope that it can generate a population \(^{T}\) close to the global optimum at step \(T\), as shown in \(^{T}=POM(^{0},T|)\), where \(\) is the parameters of POM, where \(\) stands for the strategy space. The goal of training POM is to find an optimal \(\) in \(\). As shown in Fig. 1, POM consists of LMM, LCM and SM.

LMM

LMM generates candidate solutions \(_{i}^{t}\) for individual \(_{i}^{t}\) through Eq. (2), which enables the population information to be fully utilized in the process of generating candidate solutions \(_{i}^{t}\).

\[_{i}^{t}=_{j}^{N}w_{i,j}_{j}^{t}( w_{i,j} ,w_{i,i} 0)\] (2)

Further, we organize Eq. (2) into a matrix form, as shown in Eq. (3).

\[^{t}=^{t}^{t}\] (3)

\(^{t}^{N d}\) is the population in generation \(t\) and \(^{t}^{N N}\). \(\) evolves with each change in \(t\), signifying a mutation strategy that adapts across generations. Consequently, it is imperative to devise a module that leverages information from the population at generation \(t\) to generate \(^{t}\). Any mutation operator of differential evolution, such as the classic DE/rand/1 mutation operator, can be converted into Equation (3) in the specific case of \(\) (see Appendix A.2 for details). At the same time, the crossover operation of GAs can also be generalized into the form of Equation (3) .

The function of LMM is designed based on Multi-head self-attention (MSA) , as shown as follows:

\[^{t}=LMM(^{t}|_{1})\] (4)

where \(_{1}=\{_{m1},_{m2},_{m3},_{m1}, _{m2},_{m3}\}\) denotes the trainable parameters within LMM, while \(^{t}=[_{1}^{t},_{2}^{t},,_{N}^ {t}]\) serves as LMM's input, encapsulating population information. Each \(_{i}^{t}\) incorporates details about \(_{i}^{t}\), encompassing: 1) \(_{i}^{t}\): the normalized fitness \(f(_{i}^{t})\) of \(_{i}^{t}\); 2) \(_{i}^{t}\): the centralized ranking of \(_{i}^{t}\). The method for calculating \(_{i}^{t}\) is:

\[_{i}^{t}=_{i}^{t})-^{t}}{^{t}}\] (5)

where \(^{t}\) and \(^{t}\) denote the mean and standard deviation, respectively, of individual fitness values within the population at time \(t\). We build \(_{i}^{t}\) as follows:

\[_{i}^{t}=(_{i}^{t},^{t})}{N}-0.5) 2\] (6)

where _rank_ yields the ranking of \(_{i}^{t}\) within the population \(^{t}\), with values ranging from 1 to \(N\). Thus, LMM utilizes information on the relative fitness of individuals to dynamically generate the strategy \(}^{t}\). \(_{i}^{t}\) serves as position encoding, explicitly offering the ranking information of individuals. Equation (7) details the computation of \(}^{t}\).

\[}^{t}&=Tanh(^ {t}_{m1}+_{m1}),\ \ \ ^{t}=Tanh(}^{t}_{m2}+_{m2})\\ ^{t}&=Tanh(}^{t} _{m3}+_{m3}),\ \ \ }^{t}=Tanh(^{t}(^{t})^{T}}{)}})\] (7)

where _Tanh_ is an activation function. \(_{m1}^{2 d_{m}}\) and \(_{m2},_{m3}^{d_{m} d_{m}}\). \(_{m1}\), \(_{m2}\), and \(_{m3}\) are vector with dimension \(d_{m}\). \(}^{t}^{N d_{m}}\), \(^{t},^{t}^{N d_{m}}\), and \(}^{t}^{N N}\).

The topological structure of the population significantly influences their information exchange . When all individuals engage in information exchange, the algorithm's convergence may suffer, diversity could diminish, and susceptibility to local optima increases. To address this, we introduce a _mask_ operation during both training and testing phases, where the probability of setting each element in \(}^{t}\) to 0 is \(r_{mask}\). This operation enhances POM's ability to learn efficient and robust strategies, as validated in our experiments. Consequently, \(^{t}\) is derived using Eq. (8).

\[^{t}=mask(}^{t}|r_{mask})\] (8)

Finally, we get \(^{t}\) via Eq. (3).

LcmFor each individual \(_{i}^{t}\) at step \(t\), a crossover probability \(cr_{i}^{t}\) is established. Consequently, the population's crossover strategy is encapsulated in the vector \(^{t}=(cr_{1}^{t},cr_{2}^{t},,cr_{N}^{t})\). The crossover operation, as depicted in Eq. (9), can be elucidated as follows:

\[_{i,k}^{t}=\{_{i,k}^{t},& rand(0,1) cr_{i}^{t}\\ _{i,k}^{t},&. i[1,N]\] (9)The module design should facilitate the adaptive generation of \(^{t}\) by leveraging population information. Executing the crossover operation with \(^{t}\) yields \(^{t}=[_{1}^{t},_{2}^{t},,_{N}^{t}]\).

LCM is designed based on FFN , as shown in Eq. (10),

\[^{t}=LCM(^{t}|_{2})\] (10)

where \(_{2}=\{_{c1},_{c1},_{c2},_{c2}, \}\) is the parameter of LCM and \(^{t}^{N 3}\) is the population information used by LCM. Here, \(^{t}=[_{1}^{t},_{2}^{t},,_{N}^ {t}]\). \(_{i}^{t}\) represents the relevant information of individual \(_{i}^{t}\) and \(^{t}\). For example, it can include the ranking information of \(_{i}^{t}\), the fitness information of \(_{i}^{t}\), the Euclidean distance between \(_{i}^{t}\) and \(_{i}^{t}\), and the distribution information of individuals within the population (such as the fitness distribution, the distance between pairs of individuals), etc. In this paper, \(_{i}^{t}\) includes the following information as a case study: 1) \(_{i}^{t}\): the normalized fitness \(f(_{i}^{t})\) of \(_{i}^{t}\); 2) \(_{i}^{t}\): the centralized ranking of \(_{i}^{t}\); 3) \(sim_{i}^{t}\): the cosine similarity between \(x_{i}^{t}\) and \(v_{i}^{t}\).

\[^{t}=Tanh(^{t}}+}),\ \ ^{t}}=layernorm(^{t}|),\ \ ^{t}=Sigmoid(^{t}}}+})\] (11)

where the activation function _Sigmoid_ maps inputs to the range \((0,1)\). \(}^{3 d_{c}}\), \(}^{d_{c} 1}\), \(\) is the learnable parameters of _layernorm_. \(}\) and \(}\) are vectors with dimensions \(d_{c}\) and \(1\), respectively.

Although we derive \(^{t}\) from Eq. (11) as in Eq. (9), the discrete nature of the crossover operator renders it non-differentiable, impeding gradient-based training of the _LCM_ module. To address this limitation, we introduce the _gumbel_softmax_ method , providing an efficient gradient estimator that replaces non-differentiable samples from a categorical distribution with differentiable samples from a novel Gumbel-Softmax distribution.

Eq. (12) shows how to perform crossover operations between \(_{i}^{t}\) and \(_{i}^{t}\) in _LCM_ (\( i[1,N]\)).

\[_{i}^{t}&=rand(d),\ \ \ _{i}^{t}=gumbel\_softmax(cat(_{i}^{t},tile(c_{i}^{t},d))),\\ _{i}^{t}&=_{i,0}^{t} _{i}^{t}+_{i,1}^{t}_{i}^{t},\ \ \ ^{t}=[_{1}^{t},_{2}^{t},,_{N}^{t}] \] (12)

First, the _rand_ function samples uniformly from the range \(\) to obtain a vector \(_{i}^{t}\). Then get \(cr_{i}^{t}\) from \(^{t}\) according to the index. The _tile_ function expands \(cr_{i}^{t}\) into a \(d\)-dimensional vector: \([cr_{i}^{t},cr_{i}^{t},,cr_{i}^{t}]\). The _cat_ function concatenates them into a matrix as shown below:

\[r_{i,1}^{t},&r_{i,2}^{t}&&r_{i,d}^{t}\\ cr_{i}^{t}&cr_{i}^{t}&&cr_{i}^{t}\] (13)

Here, _gumbel_softmax_ is executed column-wise. For any column, the larger element becomes 1 after _gumbel_softmax_ and 0 otherwise. Therefore, \(_{i}^{t}^{2 d}\) may be a matrix like this:

\[_{i}^{t}=1&0&0&0&1&1&&1&1\\ 0&1&1&1&0&0&&0&0\] (14)

Over all FrameworkWe design LMM and LCM to achieve the generation of sample strategy (that is, generate \(^{t}\)) and crossover strategy (that is, generate \(^{t}\)), respectively. The overall architecture of POM is shown in Fig. 1. The parameters that need to be trained in _POM_ are \(=\{_{1},_{2}\}\). At time step \(t\), the population is \(^{t}\). Initially, we amalgamate the information from \(^{t}\) to construct descriptive representations of the population, \(^{t}\) and \(^{t}\). _LMM_ adaptively generates \(^{t}\) based on \(^{t}\).

Figure 1: In the figure, \(^{0}\) is the initial random population. (a) The overall architecture of the POM. (b) POM training process. Here \(T\) is the size of the inner loop iteration step during training, and the training function should be differentiable. (c) POM testing process. Here, \(T\) is the number of iterations of the testing process and \(f\) is the target task. \(f\) does not have to be differentiable. Here we directly apply the trained POM to solve \(f\) without requiring gradient information.

The multiplication of \(^{t}\) and \(^{t}\) yields \(^{t}\) (see Eq. (3)). Next, _LCM_ adaptively generates \(^{t}\) based on its input \(^{t}\), and performs a crossover operation based on \(^{t}\) to obtain \(^{t}\). Finally, _SM_, a _1-to-1_ selection strategy is executed between \(^{t}\) and \(^{t}\) to produce the next-generation population \(^{t+1}\).

\[^{t+1}=SM(^{t},^{t})=tile(l_{x>0}(_{F^ {}}-_{F}))^{t}+tile(1-l_{x>0}(_{F^{ }}-_{F}))^{t}\] (15)

where \(l_{x>0}(x)=1\) if \(x>0\) and \(l_{x>0}(x)=0\) if \(x<0\), and the _tile_ copy function extends the indication matrix to a tensor with size \((N,d)\), \(_{F}(_{F^{}})\) denotes the fitness matrix of \(^{t}(^{t})\), and \(\) indicates the pairwise multiplication between inputs.

```
0:\(T\), \(n\), training set \(TS\).
0: The optimal \(\).
1: Randomly sample the parameter \(\) of POM.
2:while not done do
3: Sample \(|TS|\) populations of size \(n\) to obtain \([_{0}^{0},_{0}^{0},,_{|TS|}^{t}]\).
4:for\(i=1,2,,|TS|\)do
5: Randomly sample \(^{i}\) for the \(f_{i}\) in \(TS\).
6:endfor
7:for\(t=1,2,,T\)do
8:for\(i=1,2,,|TS|\)do
9:\(_{i}^{t} POM(_{i}^{t-1},1|)\).
10:\(loss_{i}^{t} l_{i}(_{i}^{t},_{i}^{t-1},f_{i}, ^{i},)\).
11:endfor
12:\(\) Update \(\) based on \(_{i}loss_{i}^{t}\).
13:endfor
14:endwhile ```

**Algorithm 1** MetaGBT

### Tasks, Loss Function & MetaGBT

POM is meticulously crafted as a model amenable to end-to-end training based on gradients. While POM necessitates gradient information from the training task during the training phase, it exhibits the ability to tackle BBO problems in the testing phase without relying on any gradient information. To ensure the acquisition of an efficient, highly robust, and broadly generalizable optimization strategy, POM undergoes training on a diverse set of tasks. Training on these tasks sequentially poses the risk of domain overfitting, local optima entrapment, and diminished generalization performance. Consequently, we introduce a training methodology named _MetaGBT_.

**Tasks**. We form a training task set \(TS=\{f_{i}(|^{j})\}\),where \(i\) and \(j[1,N]\), comprising \(4N\) tasks derived from Table 3 in appendix, where \(_{i}\) denotes the task parameter influencing the function's landscape offset. Our selection of these functions for the training task is motivated by their diverse landscape features. The specific landscape features encompassed in \(TS\) are detailed in Appendix B.

**Loss Function**. To avoid bias of different output scales in _TS_, for any function \(f_{i}\) in \(TS\), we design the normalized loss function \(l_{i}(^{t},^{t-1},f_{i},^{i},)\). In Equation (16), \(l_{i}^{1}\) calculates the average fitness difference between the input and output of the POM, further normalized within \(\). This encourages convergence of the algorithm. \(l_{i}^{2}\) uses standard deviation to simulate the distribution of the output population, encouraging diversity in the output population. \(std(^{t},j)\) is the standard deviation of the jth dimension of the population. \(\) is a hyperparameter, and we find that setting it to 0.005 can make model training more stable.

\[^{t}&= POM(^{t-1},1| )\\ l_{i}^{1}&=^{t}|} _{^{t}}f_{i}(|^{i})-^{t-1}|}_{^{t-1}}f_{i}(|^{i})}{|^{t-1}|}_{ ^{t-1}}f_{i}(|^{i})|},\ \ l_{i}^{2}=^{d}std(^{t},j)}{d},\ \ l_{i}=l_{i}^{1}-  l_{i}^{2}\] (16)

**MetaGBT**. The pseudocode for _MetaGBT_ is presented in Algorithm 1. Initially, we sample the _POM_ parameter \(\) from a standard normal distribution. The objective of _MetaGBT_ is to iteratively update \(\) to bring it closer to the global optimum \(^{*}\). In line 2, we sample a population for each task in _TS_. Lines 3, 4 and 5 involve the resampling of task parameters for all tasks in _TS_, thereby altering the task landscape, augmenting training complexity, and enhancing the learning of robust optimization strategies by POM. The final loss function (line 10) is determined by computing the average of the loss functions for all tasks. Subsequently, in line 12, we update \(\) using a gradient-based optimizer, such as Adam . The trained _POM_ is then ready for application in solving an unknown BBO problem, as depicted in Algorithm 1.

## 4 Experiments

### Experimental Setup

We test the performance of POM on the widely used BBO benchmark and two complex real-world problems (see Appendix C). Selected methods include DE (DE/rand/1/bin)  and ES ((\(\),\(\))-ES) as population-based baselines, L-SHADE  and CMA-ES  as state-of-the-art population-based BBO methods, and LES  and LGA  as state-of-the-art POMs. POM is trained on \(TS\) with \(T=100\), \(n=100\), and \(d=10\). Detailed parameters for all compared methods are provided in Appendix E. Please refer to Appendix D for the reasons for choosing these algorithms.

### Results

**BBOB **. We evaluate the generalization ability of POM across 24 BBOB functions with dimensions \(d=30\) and \(d=100\), where optimal solutions are located at **0**. Figure 2 presents the critical difference diagram comparing all algorithms (refer to Appendix Tables 4 and 6, and Figures 11, 12 and 13 for detailed results). POM significantly outperforms all methods, showcasing its efficacy across varying dimensions. Despite being trained solely on TF1-TF4 with \(d=10\), POM excels in higher dimensions (\(d=\{30,100,500\}\)), with its performance advantage becoming more pronounced with increasing dimensionality. Particularly on complex problems F21-F24, where global structure is weak, POM lags behind LSHADE but surpasses other methods, attributed to its adaptability through fine-tuning. TurBO  is the Bayesian optimization algorithm with the best performance on BBOB . Under little budget conditions, the performance of POM outperforms that of TurBO in most cases (see Appendix G for details).

**Bipedal Walker **. The Bipedal Walker task involves optimizing a fully connected neural network with \(d=874\) parameters over \(k=800\) time steps to enhance robot locomotion control. In Fig. 3(a), LSHADE shows ineffectiveness, while CMA-ES, LSHADE, and LGA suffer from premature convergence. Conversely, POM achieves stable and swift convergence, ultimately attaining the highest score.

**Enduro **. Enduro task entails controlling a strategy with \(d=4149\) parameters across \(k=500\) steps, posing greater difficulty than Bipedal Walker. As depicted in Fig. 3(b), LGA and LES exhibit premature convergence and limited exploration. While CMA-ES initially converges slightly

Figure 3: Experimental results are presented for the Bipedal Walker (a) and Enduro (b), with the vertical axis denoted as \(R\), representing the strategy score. The score corresponds to the total reward acquired by the agent during interactions with the environment.

Figure 2: The critical difference diagram illustrates the performance ranking of seven algorithms across 24 BBOB problems with dimensions \(d=30,100\), employing Wilcoxon-Holm analysis  at a significance level of \(p=0.05\). Algorithm positions are indicative of their mean scores across multiple datasets, with higher scores signifying a method consistently outperforming competitors. Thick horizontal lines denote scenarios where there is no statistically significant difference in algorithm performance.

[MISSING_PAGE_EMPTY:8]

initially trained on \(TF1\)-\(TF5\). We calculate the relative performance improvement (RFI) achieved by the fine-tuned POM compared to the base POM, with results displayed in Figure 6. Experimental results indicate that fine-tuning POM leads to significant performance improvements even with a small sample size. The method for obtaining fine-tuning samples is not restricted; for black-box tasks, a surrogate model can be constructed to facilitate fine-tuning.

Size of Training DatasetAny complex problem can be simulated by a polynomial composed of simple basic function terms. To ensure that the optimization strategy learned by POM has robust generalization ability and performance, we should train POM on a set of basic functions.

First, we tested the impact of increasing the number of basic functions in the training set on model performance. Next, we examined the effect of introducing complex functions into the training set. Functions \(TF1-TF5\) are basic simple terms. For example, \(TF1\) is an absolute value term, and \(TF5\) is a square summation term. Functions \(TF6-TF8\) are composite terms composed of several basic functions. For instance, \(TF6\) includes both a cumulative multiplier term and a cosine term. The test results are shown in Figure 5 (a) and (b) (see Appendix Table 8 for details), respectively.

Experimental results indicate that increasing the number of basic functions leads to an overall improvement in POM performance, whereas the introduction of composite terms results in a significant performance decline. This aligns with our hypothesis.

Scale of POMWe explore the performance of POM at different scales, which is shown in Fig. 4 (b) (refer to Appendix Table 9 for additional details). We increase POM's parameter count by perturbing the hidden layers of each module (\(d_{m},d_{c}\)). Six models are constructed in ascending order of parameter count, labeled as _VS_ (very small), \(S\) (small), \(M\) (medium), \(L\) (large), _VL_ (very large), and _XL_ (extra large) (details in the Appendix Table 2). _XL_ achieves the best performance, while _VS_ and \(M\) also perform well. \(S\) exhibits the worst performance, and _VL_ performs worse than \(L\). Two core factors contribute to this phenomenon: the number of parameters and training. We observe a complex relationship between the number of parameters and training difficulty. _VS_, with the fewest

Figure 8: Displayed are visualized outcomes of LMM \(S^{t}\) in BBOB with \(d=100\) using \(n=10\) for clarity. Blank squares in the matrix denote masked portions from Eq. (8). Steps 1, 50, and 100 correspond to the 1st, 50th, and 100th generations in population evolution. The horizontal and vertical axes denote individual rankings, with 1 as the best and 10 as the worst in the population. Each row illustrates the weight assigned to other individuals when executing mutation operations for the respective individual.

Figure 9: Visual analysis results of LCM on BBOB F1, F11, and F24 with \(d=100\), employing \(n=100\), are presented. “Rank” signifies an individual’s position, with rank 5 representing the fifth-ranked individual in the population. Subgraphs depict the evolution of the probability that an individual will undergo crossover across three tasks as the population progresses. For example, (a) illustrates the crossover probability change for the top-ranked individual on F1, F11, and F24 with the number of generations.

parameters, is the easiest to train and performs well on BBOB. Conversely, XL, with a large number of parameters, exhibits the strongest capability to represent strategies, resulting in the best performance. The performance of XL aligns with our expectations. We obtain the following principles: 1) Larger models can have stronger capabilities but are more challenging to train; 2) Training difficulty and model scale do not exhibit a simple linear relationship, warranting further research; 3) Larger models require more functions for effective training.

Time BudgetWe assess the training and test time efficiency of POM across various architectures on BBOB (\(d=10\)) and BBOB (\(d=100\)) respectively, as illustrated in Figure 7. POM demonstrates remarkable efficiency in tackling BBO problems, with negligible training costs relative to its exceptional generalization ability and high performance.

### Visualization Analysis

LMM Learning AnalysisFigure 8 displays \(S^{t}\) for an in-depth analysis of the LMM strategy (refer to Appendix Figure 15-20 for additional details). Key observations and conclusions include: 1) Generally, superior individuals receive higher weights during LMM, showcasing POM's ability to balance exploration and exploitation as the population converges. 2) Across diverse function problems, POM dynamically generates optimization strategies, highlighting its adaptability and contributing to robust generalization. 3) Disadvantaged individuals exhibit a more uniform weight distribution, potentially aiding in their escape from local optima and enhancing algorithm convergence.

LCM Learning AnalysisWe visually examine the LCM strategy, presenting the results in Fig. 9 (refer to Appendix Figure 21-26 for additional details). LCM displays the capacity to adaptively generate diverse strategies for individuals across different ranks in the population, revealing distinct patterns among tasks and rankings. Notably, top-ranking individuals within the top 20, such as those ranked 1st, 5th, and 18th, exhibit a flexible crossover strategy. The dynamic adjustment of crossover probability with population evolution aids in preserving dominant genes and facilitating escape from local optima. Conversely, lower-ranking individuals show an increasing overall probability of crossover, promoting exploration of disadvantaged individuals and enhancing the algorithm's exploration capability. LCM proficiently generates adaptive crossover strategies across tasks, individuals, and convergence stages, significantly boosting both convergence and exploration capabilities.

## 5 Conclusions

We present POM, a novel Pretrained Optimization Model designed to address the inefficiencies of existing methods in zero-shot optimization. Evaluation on BBOB and robot control tasks demonstrates POM's superiority over other black-box optimizers, particularly in high-dimensional scenarios. Additionally, POM excels in solving few-shot optimization problems. Future research avenues include designing enhanced loss functions to optimize POM for both population convergence and diversity, thereby improving overall algorithm performance. In addition, the limitations of model scale and time performance deserve further study (see Appendix I for details).