# Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs

Kevin Tan, Wei Fan, Yuting Wei

Department of Statistics and Data Science

The Wharton School, University of Pennsylvania

###### Abstract

Hybrid Reinforcement Learning (RL), where an agent learns from both an offline dataset and online explorations in an unknown environment, has garnered significant recent interest. A crucial question posed by Xie et al. (2022) is whether hybrid RL can improve upon the existing lower bounds established for purely offline or online RL without requiring that the behavior policy visit every state and action the optimal policy does. While Li et al. (2023) provided an affirmative answer for tabular PAC RL, the question remains unsettled for both the regret-minimizing and non-tabular cases. In this work, building upon recent advancements in offline RL and reward-agnostic exploration, we develop computationally efficient algorithms for both PAC and regret-minimizing RL with linear function approximation, without requiring concentrability on the entire state-action space. We demonstrate that these algorithms achieve sharper error or regret bounds that are no worse than, and can improve on, the optimal sample complexity in offline RL (the first algorithm, for PAC RL) and online RL (the second algorithm, for regret-minimizing RL) in linear Markov decision processes (MDPs), regardless of the quality of the behavior policy. To our knowledge, this work establishes the tightest theoretical guarantees currently available for hybrid RL in linear MDPs.

## 1 Introduction

Reinforcement learning (RL) holds great promise in attaining reliable decision-making in adaptive environments for a broad range of modern applications. Typical RL algorithms often require an enormous number of training samples, motivating a line of recent efforts to study the sample efficiency of RL algorithms. There are two mainstream paradigms of RL, distinguished by how samples are collected: online RL and offline RL. In online RL, an agent learns in a real-time manner, exploring the environment to maximize her cumulative rewards by executing a sequence of adaptively chosen policies (e.g. Azar et al. (2017); Jin et al. (2018); Sutton and Barto (2018); Zhang et al. (2023)). Whereas, in offline RL, an agent has only access to a pre-collected dataset, and tries to figure out how to perform well in a different environment without ever experiencing it (e.g. Jin et al. (2021); Lange et al. (2012); Levine et al. (2020); Li et al. (2024)). Online methods are often sample-hungry, but offline methods often impose stringent requirements on the quality of the pre-collected data.

To address the limitations of both, the setting of hybrid RL (Song et al., 2023; Xie et al., 2022) has recently received considerable attention from both theoretical and practical perspectives (Amortila et al., 2024; Ball et al., 2023; Kausik et al., 2024; Li et al., 2023; Nair et al., 2020; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Vecerik et al., 2017; Wagemmaker and Pacchiano, 2023; Zhou et al., 2023). In hybrid RL, an agent learns from a combination of both offline and online data, extracting information from offline data to enhance online exploration. Theoretical guarantees for hybrid RL algorithms can be categorized on: (1) the type of function approximation considered, (2) the level of coverage required by the behavior policy, (3) whether it improves on the _minimax lower bounds_ for online-only and offline-only learning, and (4) whether they minimize regret or obtain a PAC guarantee. We elaborate below, and summarize the prior art in Table 1.

While much of the prior literature (Amortila et al., 2024; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Zhou et al., 2023) tackles general function approximation in hybrid RL, they either require stringent concentrability assumptions on behavior policy quality, or fail to obtain tight theoretical guarantees. Under such single-policy concentrability assumptions (explained below), Xie et al. (2022) show the optimal RL algorithm is either a purely offline reduction or a purely online algorithm if the agent can choose the ratio of offline to online samples, rendering the benefits of hybrid RL questionable. Without this assumption, Li et al. (2023) show guarantees for PAC RL that improve over lower bounds for offline-only and online-only RL, but only for tabular MDPs.

This paper focuses on obtaining sharper theoretical guarantees in the setting of linear function approximation in _linear MDPs_. First proposed in Jin et al. (2019); Yang and Wang (2019), linear MDPs parameterize the transition probability kernel and reward function by linear functions of known features (e.g. pre-trained neural embeddings). It has been extensively studied due to its benefits in dimension reduction and mathematical tractability in both the online and offline settings (Du et al., 2019; Duan and Wang, 2020; He et al., 2023; Hu et al., 2023; Jin et al., 2019; Li et al., 2021; Min et al., 2021; Qiao and Wang, 2022; Xiong et al., 2023; Yang and Wang, 2019; Yin et al., 2022; Zanette et al., 2021). Despite these efforts, hybrid RL algorithms for linear MDPs (Amortila et al., 2024; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Wagenmaker and Pacchiano, 2023) have suboptimal worst-case guarantees (Table 2), which raises the question:

_Is it possible to develop sample efficient RL algorithms in the setting of hybrid RL that are provably better than online-only and offline-only algorithms for linear MDPs?_

### Hybrid RL: two approaches

To answer the question above, we introduce two types of approaches widely-adopted in hybrid RL.

**The offline-to-online approach:** Most of the current literature (e.g. Amortila et al. (2024); Nakamoto et al. (2023); Song et al. (2023); Tan and Xu (2024)) initializes the online dataset with offline samples to perform regret-minimizing online RL. We refer to this as the _offline-to-online_ approach. This method is simple and natural, and as the algorithm optimizes the reward during each online episode, it is suitable when the agent has to perform well during online exploration.

**The online-to-offline approach:** However, if our goal is to output a near-optimal policy, especially in real-world situations in medicine and defense, randomizing between policies can be suboptimal or even unethical. Recently, Wagenmaker and Pacchiano (2023) and Li et al. (2023) propose using reward-agnostic online exploration to explore parts of the state space unseen by the behavior policy, to construct a dataset that is especially amenable to leverage the sharp performance guarantees of offline RL. We refer to this as the _online-to-offline approach_. While this approach does not optimize the "true reward" during online exploration, it avoids the need to deploy mixed policies to achieve a PAC bound, allowing for the deployment of fixed, and thus more interpretable, policies.

### Our contributions

* We propose an online-to-offline method called _Reward-Agnostic Pessimistic PAC Exploration-initialized Learning (RAPPEL)_ in Algorithm 1. It employs reward-agnostic online exploration to enhance the offline dataset, then learns a policy through a pessimistic offline RL algorithm. Algorithm 1 significantly improves upon the sample complexity of the only dedicated hybrid RL algorithm for linear MDPs (Wagenmaker and Pacchiano, 2023) by a factor of at least \(H^{3}\). This performs no worse than the offline-only minimax-optimal error bound from Xiong et al. (2023), with the potential of significant gains from online data. This is the first work to explore the online-to-offline approach in linear MDPs.
* In addition, we propose an offline-to-online method called _Hybrid Regression for Upper-Confidence Reinforcement Learning (HYRULE)_ in Algorithm 2, where one warm-starts an

   Paper & Function Type & Concentrability? & Improvement? & Regret or PAC? \\  Song et al. (2023) & General & Required & No & Regret \\ Nakamoto et al. (2023) & & & & \\  Tan and Xu (2024) & General & Not Required & No & Regret \\ Amortila et al. (2024) & & & & \\  Wagenmaker and Pacchiano (2023) & Linear & Not Required & No & PAC \\  Li et al. (2023) & Tabular & Not Required & Yes & PAC \\  This work & Linear & Not Required & Yes & Regret, PAC \\   

Table 1: Comparison of our contributions to previous work in hybrid RL.

online RL algorithm with parameters estimated from offline data. In addition to improving the ambient dimension dependence, this algorithm enjoys a regret (or sample complexity) bound that is no worse than the online-only minimax optimal bound, with the potential of significant gains if the offline dataset is of high quality (Agarwal et al., 2022; He et al., 2023; Hu et al., 2023; Zhou et al., 2021). Our result demonstrates the provable benefits of hybrid RL in scenarios where offline samples are much cheaper or much easier to acquire.

To the best of our knowledge, we are the first to show improvements over the aforementioned lower bounds of hybrid RL algorithms (in the same vein as Li et al. (2023b)) in the presence of function approximation, without any explicit requirements on the quality of the behavior policy, and with both the offline-to-online and online-to-offline approaches. Our results are also, at the point of writing, the best bounds available in the literature for hybrid RL in linear MDPs (see Table 2).

Technical contributions.In this work, we build on recent advancements in offline and online RL, demonstrating that intuitive modifications suffice to achieve state-of-the-art sample complexity for hybrid RL in linear MDPs. At a high level, our sample efficiency gains are achieved by decomposing the error of interest into offline and online partitions, and optimizing them respectively, following the same idea in Tan and Xu (2024). Below, we summarize our specific technical contributions.

1. We sharpen the dimensional dependence from \(d\) to \(d_{}\) and \(c_{}(_{})\) via projections onto those partitions. The former is accomplished in Algorithm 1 by Kiefer-Wolfowitz in Lemma 1, and in Algorithm 2 by proving a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in Lemma 14 to reduce the dimensional dependence in the summation of bonuses, which helps achieve the desired result.
2. We maintain a \(H^{3}\) dependence for the error or regret for both algorithms, which is non-trivial, in Algorithm 1 and for the offline partition in Algorithm 2 by combining the total variance lemma with a novel truncation argument for "bad" trajectories in Lemma 17.

## 2 Preliminaries

### Basics of Markov decision processes

An episodic MDP is a tuple \(=,,H,(_{h})_{h=1}^{H},(r_{h} )_{h=1}^{H}\), where \(\) is the state space, \(\) the action space, \(H\) the horizon, \((_{h})_{h=1}^{H}\) the collection of transition probability kernels \(_{h}:()\), and \((r_{h})_{h=1}^{H}\) the collection of reward functions \(r_{h}:\). \(()\) is the collection of distributions over a set. At each \(h[H]=\{1,...,H\}\), an agent observes the current state \(s_{h}\), takes an action \(a_{h}\) according to \(_{h}:()\), and observes the reward \(r_{h}\) and next state \(s_{h+1}_{h}( s_{h},a_{h})\). We write \(\) for the set of policies \(=\{_{h}\}_{h=1}^{H}\), with value and Q-functions

\[(s,h)[H]:\ \ V_{h}^{}(s):= _{}[_{h^{}=h}^{H}r_{h^{}}|s_{h}=s], \] \[(s,a,h) [H]:\ \ Q_{h}^{}(s,a):=_{}[_{h^{}=h}^{H}r_{h^{ }}|s_{h}=s,a_{h}=a]. \]

\(^{*}=\{^{*}\}_{h=1}^{H}\) is the optimal policy attaining the highest value and Q-functions, and we write \(V^{*}=\{V_{h}^{*}\}_{h=1}^{H}\) and \(Q^{*}=\{Q_{h}^{*}\}_{h=1}^{H}\) for the optimal value and Q-functions. We consider the setting of hybrid RL, where an agent has access to two sources of data:

   & Upper Bound & Lower Bound \\  Offline (Error) & \(_{}^{H}_{^{*}}|(s_{h },a_{h})|_{_{,h}^{*-1}}\) & \(_{}^{H}_{^{*}}|(s_{h},a_ {h})|_{_{,h}^{*-1}}\) \\  & \(d^{2}H^{4}/N_{}}\)(Xiong et al., 2023) & \(d^{2}H^{2}/N_{}}\)(Xiong et al., 2023) \\  Online (Regret) & \(H^{3}T}\)(He et al., 2023) & \(H^{3}T}\)(Zhou et al., 2021) \\   \\  Hybrid & \(H^{3}/N}\)(Wagenmaker and Pacchiano, 2023) \\ (Online-to-offline Error) & \([}(_{})dif}d^{3}_{} (_{}),H/1)/N_{}+}dH^{3}} (_{},H)/N_{}]\) (Alg. 1) \\  Hybrid & \(C^{*}H^{3}N_{}}\)(Nakamoto et al., 2023; Song et al., 2023) \\ (Offline-to-online Regret) & \(+c_{}(_{})d)^{2}H^{3}N_{}}\)(Amorila et al., 2024) \\  & \(}(_{})dif}d^{3}N_{}^{2}/N_{ }+}dH^{3}N_{}}\)(Tan and Xu, 2024) \\  & \([}(_{})dif}d^{3}N_{}^{2}/N_{ }+}dH^{3}N_{}}]\)(Alg. 2) \\  

Table 2: Comparisons of our results to the best upper and lower bounds for offline and online RL, and existing results for hybrid RL, in linear MDPs. Often, offline data is cheaper or easier to obtain. When this happens, \(N_{} N_{}\), and the online term in our results (depending on \(N_{}\)) dominates.

* \(N_{ off}\) independent episodes of length \(H\) collected by a behavior policy \(_{b}\) where the \(n\)-th sample trajectory is a sequence of data \((s_{1}^{(n)},a_{1}^{(n)},r_{1}^{(n)},...,s_{H}^{(n)},a_{H}^{(n)},r_{H}^{(n)},s_{ H+1}^{(n)});\)
* \(N_{ on}\) sequential episodes of online data, where at each episode \(n=1,...,N_{ on}\), the algorithm has knowledge of the \(N_{ off}\) offline episodes and the previous online episodes \(1,...,n-1\).

The quality of the behavior policy \(_{b}\) is measured by the all-policy and single-policy concentrability coefficients proposed by Xie et al. (2023); Zhan et al. (2022):

**Definition 1** (Occupancy Measure).: _For a policy \(=\{_{h}\}_{h=1}^{H}\), its occupancy measure \(d^{}=\{d_{h}^{}\}_{h=1}^{H}\) corresponds to the collection of distributions over states and actions induced by running \(\) within \(\), where for some initial distribution \(\) and \(s_{1}\), we have_

\[d_{h}^{}(s,a):=(s_{h}=s,a_{h}=a s_{1},). \]

**Definition 2** (Concentrability Coefficient).: _The all-policy and single-policy concentrability coefficients of \(\) with regard to the occupancy measure \(=\{_{h}\}_{h=1}^{H}\) of a behavior policy \(_{b}\) are_

\[C_{}:=_{}_{h,s,a}^{}(s,a)}{_{h}(s,a)}\; \;\;\;C^{}:=_{h,s,a}^{}(s,a)}{_{h} (s,a)}, \]

Policy learning and regret minimization.Hybrid RL aims to either learn an \(\)-optimal policy \(\) such that \(V^{}-V^{}\) with high probability, or to minimize the regret. Here, the regret of an online algorithm \(:\) is \(_{}(T):=[_{t=1}^{T}(V_{1}^{}(s_{1}^{ (t)})-_{h=1}^{H}r_{h}^{(t)})]\). We write \(T=N_{ on}\) interchangeably for the number of episodes taken by a regret-minimizing online RL algorithm.

### Linear MDPs

Throughout this paper, we study linear MDPs, where the transition probabilities and rewards are linearly parametrizable as functions of known features. This was first proposed by Jin et al. (2019); Yang and Wang (2019), and further studied in He et al. (2023); Hu et al. (2023); Wagenmaker and Jamieson (2023); Xiong et al. (2023); Zanette et al. (2021).

**Assumption 1** (Linear MDP, Jin et al. (2019)).: _There exists a known feature map \(:^{d}\), \(d\) unknown signed measures \(_{h}=(_{h}^{(1)},,_{h}^{(d)})\) over \(\) for each \(h\), and an unknown vector \(_{h}^{d}\), such that for any \(s,a,h\) we have \(_{h}( s,a)=(s,a),_{h}(),r_{h}(s,a )=(s,a),_{h}\). Assume \(\|(s,a)\| 1\) for all \(s,a\), and \(\{\|_{h}()\|,\|_{h}\| \|h\)_

This allows for sample-efficient RL for a few reasons. Firstly, linear MDPs are Bellman complete (Jin et al., 2021), a common assumption for sample-efficient RL in the literature (Duan and Wang, 2020; Fan et al., 2020; Munos and Szepesvari, 2008). Secondly, the value and Q-functions are linearly parametrizable in the features, allowing one to learn them via ridge regression. This allows for sample-efficient online (He et al., 2023; Hu et al., 2023) and offline (Xiong et al., 2023; Yin et al., 2022) RL with function approximation. However, existing guarantees for hybrid RL in linear MDPs (Wagenmaker and Pacchiano, 2023) are loose (Li et al., 2023), inspiring our work.

**Further notation.** Write \(_{n,h}=(s_{h}^{(n)},a_{h}^{(n)})\) for the feature vector at episode \(n\) and horizon \(h\). Let \(_{h}=_{n=1}^{N}_{n,h}_{n,h}^{}+\) and \(_{ off,h}=_{n=1}^{N_{ off}}_{n,h}_{n,h}^{} +\) be the covariance matrices of the entire dataset and the offline dataset respectively, and \(\) the set of all covariates. We consider two kinds of variance-weighted covariance matrices, namely \(_{n,h}^{}=_{n=1}^{N}_{n,h}_{n,h}^{} _{h}V_{h+1}^{}(s_{h}^{},a_{h}^{} )+\) and \(_{n,h}=_{n=1}^{N}_{n,h}^{-2}_{n,h}_{n,h }^{}+\), where \(_{h}V_{h+1}^{}(s_{h}^{},a_{h}^{} )=\{1,_{h}\,V_{h+1}^{}( s,a)\}\) is the truncated variance of the optimal value function (where \(s,a\) are random variables) and \(_{n,h}^{-2}\) is the variance estimator from He et al. (2023).

### Exploring the state-action space

We aim to develop efficient hybrid RL algorithms for linear MDPs that do not rely on single-policy concentrability over the entire state-action space, which entails that the behavior policy covers every state-action pair that \(^{}\) visits. A natural idea from Li et al. (2023) is to partition this space into a component that is well-covered by the behavior policy, which we call the offline partition \(_{ off}\), and a component requiring further exploration, which we call the online partition \(_{ on}\). Based on this partition, similarly to Tan and Xu (2024), the estimation error or regret of a hybrid RL algorithm can be analyzed on each component separately. We define \(_{ on}_{ off}=[H]\), with their images under the feature map \(_{ off}=((_{ off,h}))_{he[H]} ^{d}\) and \(_{ on}=((_{ on,h}))_{he[H]}^ {d}\) being subspaces of dimension \(d_{ off}\) and \(d_{ on}\) respectively. Write \(_{ off},_{ on}\) for the orthogonal projection operators onto these subspaces respectively. Let \(_{k}(M)\) denote the \(k\)-th largest eigenvalue of a symmetric matrix \(M\). We borrow the definition of partial offline all-policy concentrability,1

\[c_{}(_{}):=_{h}\;1/_{d_{}}(_{_{h}}[(_{}_{h})(_{ }_{h})^{}]), \]

from Tan and Xu (2024), where we use the convention that \(c_{}()=0\). This corresponds to the inverse of the \(d_{}\)-th largest eigenvalue of the covariance matrix of the projected feature maps. Similarly, the partial all-policy analogue of the coevability coefficient from Xie et al. (2022) is

\[c_{}(_{}):=_{}_{h}\;1/_{d_{ }}(_{d_{h}^{*}}[(_{}_{h})( _{}_{h})^{}]). \]

As we shall see, these quantities characterize the estimation error of our proposed algorithms.

## 3 Algorithms and main results

We provide two algorithms with improved statistical guarantees to tackle the unsolved (Table 2) problem of achieving sharp guarantees with hybrid RL in linear MDPs, with different approaches:

1. Performing reward-agnostic online exploration (Wagenmaker and Pacchiano, 2023) to augment the offline data, then invoking offline RL (Xiong et al., 2023) to learn an \(\)-optimal policy on the combined dataset, in the same vein of Li et al. (2023). This is Algorithm 1.
2. Warm-starting an online RL algorithm (He et al., 2023) with parameters estimated from an offline dataset to minimize regret, as in Song et al. (2023), with details in Algorithm 2.

### Offline RL after online exploration

Algorithm 1 collects online samples informed by the degree of coverage (or lack thereof) of the offline dataset \(_{}\) with a reward-agnostic online exploration algorithm called OPTCOV from Wagenmaker and Jamieson (2023). OPTCOV explores so that the smallest eigenvalue of the covariance matrix, \(_{}(_{h})\), is no smaller than a tolerance parameter \(1/\). We then learn a policy from the combined dataset using a minimax-optimal pessimistic offline RL algorithm from Xiong et al. (2023), LinPEVI-ADV+. To employ OPTCOV, one requires a modified analogue of the full-rank covariate assumption from Wagenmaker and Pacchiano (2023) that ensures that the MDP is "explorable" enough. This assumption is only imposed for Algorithm 1.

**Assumption 2** (Full Rank Projected Covariates).: _For any partition \(_{}_{}=[H] \), \(c_{}(_{})<,\) or equivalently that \(_{}_{h}_{d_{}}(_{d_{}}[( _{}_{h})(_{}_{h})^{}] )=_{d_{}}^{*}>0\)._

Informally, this states that for any partition, there exists some "optimal exploration policy" that ensures that the projected covariates onto the online partition have the same rank as its dimension at every timestep. In practice, this is achievable for any linear MDP via projecting the features onto the eigenspace corresponding to the nonzero singular values. We can then establish the following:

**Lemma 1** (Partial Coverability Is Bounded In Linear MDPs).: _For any partition \(_{},_{}\), it satisfies that \(c_{}(_{}) d_{}\). Also, there exists at least one partition such that \(c_{}(_{})=O(d)\)._

The proof of this lemma is deferred to Appendix D. This result allows us to bound the error on the offline and online partitions by the dimensionality of the partitions, instead of the coevrability coefficient. Define \(_{}:=N_{}/N\), \(_{}:=N_{}/N\), and the minimal online samples for exploration

\[N^{*}():=_{N}N_{}_{}^{}(N(+I)+_{})^{-1} .\]

We now have, with full proof in Appendix B and sketch at the end of the subsection, the following:

**Theorem 1** (Error Bound for RAPPEL, Algorithm 1).: _For every \((0,1)\) and any partition \(_{},_{}\), when choosing \((\{d_{}/N_{},_{ }(_{})/N_{}\})\), RAPPEL achieves w.p. \(1-\):_

\[V_{1}^{*}-V_{1}^{} _{h=1}^{H}_{*}\|(s_{h},a_{ h})\|_{(^{*}_{,h}+^{*}_{,h})^{-1 }}_{h=1}^{H}_{*}\|(s_{h},a_{h})\|_{ ^{*,h}_{,h}}, \] \[V_{1}^{*}-V_{1}^{} \{}(_{ })dH^{4}}{N_{}}}+}dH^{4}}{N_{ }}},}(_{})^{2}dH^ {3}}{N_{}_{}}}+}^{2} dH^{3}}{N_{}_{}}}\}, \]

_given \(N\{_{}^{4}d_{}^{-4},_{ }^{4}c_{}(_{})^{-4}\} \{N^{*}(),(d,H,c_{}(_{}),  1/)\}\)._

This result, when applied to tabular MDPs with finite states and actions, yields:

**Corollary 1**.: _In tabular MDPs, for every \((0,1)\), it satisfies that with probability at least \(1-\),_

\[V_{1}^{*}(s)-V_{1}^{}(s)||^{2}| |}(}(_{})/N_{ }}+}/N_{}}). \]

In sum, Theorem 1 shows that with a \((d,H)\) burn-in cost that is no smaller than \(N^{*}\) (the minimal online samples for any algorithm to achieve our choice of OPTCOV tolerance), we require only

\[c_{}(_{})dH^{3}\{c_{}( _{}),H\}/^{2}+d_{}dH^{3}\{d_{ },H\}/^{2}\]

trajectories to learn an \(O()\)-optimal policy. \(N^{*}\), from Wagenmaker and Pacchiano (2023), is essentially unavoidable in reward-agnostic exploration for linear MDPs. To compare with prior literature, our result leads to a better worst-case guarantee than the error bound of \(H^{7}/N}\) attained in Wagenmaker and Pacchiano (2023) (by at least a factor of \(H^{3/2}\)), the only other work on hybrid RL in linear MDPs thus far. While we employ the same online exploration procedure, we combine our exploration phase with an offline learning algorithm LinPEVI-ADV+ from Xiong et al. (2023) and conduct a careful analysis. When comparing with the offline-only and online-only settings, Theorem 1 improves upon the offline-only minimax-optimal error bound of \(_{h=1}^{H}_{*}||(s_{h},a_{h})||_{^{*,h}_{,h}}\) from Xiong et al. (2023) as a consequence of \(^{*}_{,h}+^{*}_{,h}^{*}_{,h}\); the best offline-only error bound is \(H^{4}/N_{}}\) obtained under the "well-covered" assumption (Corollary 4.6, Jin et al. (2021b)) that \(_{}(_{h,})(1/d)\), Theorem 1 enjoys better dimension and horizon dependence as there is always a partition such that \(d_{},c_{}(_{}) d\) and \(d_{}H^{3}\{d_{},H\} d^{2}H^{4}\).

The literature has experienced considerable difficulty in sharpening the horizon dependence to \(H^{3}\) in offline RL for linear MDPs. While Yin et al. (2022) and Xiong et al. (2023) provide minimax-optimal algorithms for offline RL in linear MDPs, both only manage to achieve a \(H^{3}\) horizon dependence in the special case of tabular MDPs, even under the aforementioned "well-covered" assumption. We provide the same result in Corollary 1 with proof deferred to Appendix C, but encouragingly, hybrid RL lets us bypass the "well-covered" assumption. In Appendix B and G, we use a novel truncation argument and the total variance lemma (Lemma C.5 of Jin et al. (2018)) to improve the dependence on \(H\), but our result falls slightly short of \(}(_{})dH^{3}/N_{}}+ }dH^{3}/N_{}}\).

**Computational efficiency.** In terms of computational efficiency, Algorithm 1 inherits the computational costs of the previous proposed algorithms OPTCOV and LinPEVI-ADV+ (Wagenmaker and Jamieson (2023); Xiong et al. (2023). OPTCOV runs in polynomial time \((d,H,c_{}(_{}), 1/)\), and LinPEVI-ADV+ runs in \((d^{3}HN||)\) time when the action space is discrete. Algorithm 1 therefore remains computationally efficient in this case.

**Requirement of choosing \(d_{}\).** There is the caveat that we require the user to choose the tolerance for OPTCOV. In practice, one can achieve this by performing SVD on the offline dataset and looking at the plot of eigenvalues. One can also choose a tolerance of \(O(d/\{N_{},N_{}\})\), but this would not achieve the reduction in the dependence on dimension from \(d^{2}\) to \(c_{}(_{})d,d_{}d\).

**Practical benefits of the online-to-offline approach.** Algorithm 1 outputs a fixed policy satisfying a PAC bound. This enables policies to be deployed in critical real-world applications, such as in medicine or defense, where randomized policies from regret minimization are unacceptable.

**Algorithm 1 works for reward-agnostic hybrid RL.** The use of reward-agnostic online exploration in Algorithm 1 enables one to use the hybrid dataset \(\) to learn policies for different reward functions offline. As the online exploration is not influenced by any single reward function, the collected data satisfies good coverage for any possible reward function even if it is revealed only after exploration, enabling one to use a single dataset to achieve success on many different tasks. This therefore also serves as an algorithm for the related setting of _reward-agnostic hybrid RL_, where the reward function is unknown during online exploration and only revealed to the agent after it.

[MISSING_PAGE_FAIL:7]

**Theorem 2** (Regret Bound for HYRULE, Algorithm 2).: _Given any \((0,1)\), for every partition \(_{},_{}\), if \(N_{},N_{}=(d^{13}H^{14})\), the regret of HYRULE is bounded w.p. at least \(1-\) by_

\[(N_{})_{_{}, _{}}}(_{})^{2}dH^{3}N_{ }^{2}/N_{}}+}dH^{3}N_{}}.\]

**Corollary 2**.: _By the regret-to-PAC conversion, Algorithm 2 achieves a sub-optimality gap w.p. \(1-\):_

\[V_{1}^{*}(s)-V_{1}^{}(s)_{_{},_{}}}(_{})^ {2}dH^{3}/N_{}}+}dH^{3}/N_{}}.\]

To understand this result, we first note that bounding the regret over all possible partitions yields an improvement over the \(H^{3}N_{}}\) regret bound obtained by He et al. (2023), as we can take \(_{}=,_{}=\) to recover the \(H^{3}N_{}}\) bound. In the scenario where offline samples are abundant (where \(N_{} N_{}\)), it is possible to achieve significant improvements over online-only learning. Furthermore, in view of Lemma 1, there always exists a partition such that \(c_{}(_{}),d_{} d\). This result therefore yields provable improvements over the minimax-optimal online regret bound in linear MDPs (Agarwal et al., 2022; He et al., 2023; Hu et al., 2023; Zhou et al., 2021).

Additionally, Theorem 2 shows that Algorithm 2 attains the best known regret bound in hybrid RL for linear MDPs, as we illustrate in Table 2. The current best known result is that of Tan and Xu (2024), with a dependence of \(}(_{})dH^{5}N_{}^{2}/N_{}}+}dH^{5}N_{}}\). Notably, we achieve the same a reduction in the dimension dependence on the online partition from \(d^{2}\) to \(d_{}d\) that Tan and Xu (2024) do by proving a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in Lemma 14 to reduce the dimensional dependence in the summation of bonuses. Song et al. (2023) and Amortila et al. (2024), on the other hand, have bounds on the order of \(C^{*}H^{6}N_{}}\) and \(+c_{}())d^{3}H^{6}N_{}}\) respectively. We produce a better bound than Amortila et al. (2024); Song et al. (2023); Tan and Xu (2024) by at least a factor of \(H^{2}\) by combining the total variance lemma and a novel truncation argument that rules out "bad" trajectories in Lemma 17, which allows us to maintain a desirable \(H^{3}\) dependence on both partitions.

Computational efficiency.When the action space is finite and of cardinality \(||\), the computational complexity of Algorithm 2 is of order \((d^{4}H^{3}N||)\), as outlined in He et al. (2023). Algorithm 2 is therefore computationally efficient and runs in polynomial time in this case. When the action space is continuous, one may need to solve an optimization problem over the continuous action space, making the computational complexity highly problem-dependent.

Algorithm 2 is unaware of the partition.Unlike Algorithm 1, Algorithm 2 is fully unaware of the choice of partition, and there is therefore no need to estimate \(d_{}\) or any relevant analogue to the choice of tolerance for OPTCOV. The regret bound therefore automatically adapts to the best possible partition, even though Algorithm 2 is unaware of it.

Practical benefits of the offline-to-online approach.While Algorithm 2 only satisfies a PAC bound with a randomized policy, it minimizes the regret of the actions it takes. This enables the algorithm to be deployed in situations where its performance during online exploration is of critical importance, e.g. in applications like mobile health (Nahum-Shani et al., 2017).

Technical challenges.Although Algorithm 2 is a straightforward generalization of LSVI-UCB++ in He et al. (2023), with \(_{0}\) initialized with the offline dataset, we had to decompose the regret into the regret on the offline and online partitions to achieve the regret guarantee in Theorem 2. In the process, we faced the following challenges:

* Bounding the regret on the offline partition was challenging, as the argument of He et al. (2023) was not applicable. Instead, we used a truncation argument in Lemma 17 to bound the maximum eigenvalue of \(_{,h}^{-1}\), maintaining a \(H^{3}\) dependence on the offline partition.
* Bounding the regret on the online partition allowed us to use an analysis that was close to that of He et al. (2023). However, directly following their argument would have left us with a \(d^{2}H^{3}\) dependence. To reduce the dimensional dependence to \(d_{}d\), we prove a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18. We use this in Lemma 14 to reduce the dimensional dependence in the sum of bonuses, achieving the desired result.
* Without the above two techniques, one could have used a simpler analysis to achieve a far looser \(}(_{})^{2}d^{6}H^{8}N_{} ^{2}/N_{}}+H^{3}}\) regret bound by using the maximum magnitude of the variance weights for the offline partition and the analysis from He et al. (2023) verbatim for the online partition, but this would not have yielded the same improvement.

We accordingly provide a proof sketch below.

Proof sketch.We first adopt the regret decomposition as in He et al. (2023) and bound

\[(T)T}+_{h,t}\|_{t,h}^{-1/2} _{h}(s_{h}^{(t)},a_{h}^{(t)})\,_{}\|_{2}+ _{h,t}\|_{t,h}^{-1/2}_{h}(s_{h}^{(t)},a_{h}^{(t)}) \,_{}\|_{2}.\]

It then boils down to controlling the second and third terms separately. We prove in Lemma 12 that the sum of bonuses on the offline partition can be bounded by \(_{h}}}}}{N_{}}_{ _{h}_{}}_{}^{}}_{}^{-1}_{h}\). Further, \(_{h}_{_{h}_{}}^{}}_{}^{-1}_{h}} c_{}(_{})^{2}H^{3}\) by Lemma 13. Putting things together, the second term can be controlled as

\[\!_{h,t}\!\|_{t,h}^{-1/2}_{h}(s_{h}^{(t)},a_{h}^{(t)} )\,_{}\|_{2}}( _{})^{2}dH^{3}N_{}^{2}/N_{}}.\]

With respect to the third term, Lemma 14 (a sharpened version of Lemma E.1 in He et al. (2023)), combined with the Cauchy-Schwartz inequality, yields

\[\!_{h,t}\!|_{t,h}^{-1/2}_{h}(s_{h}^{(t)},a_{h}^{(t)}) \,_{}\|_{2} d^{4}H^{8}+ d^{7}H^{5}+ }HT+d_{}H_{h,t}_{t,h}^{2}}.\]

By the total variance lemma (Appendix B, He et al. (2023)), \(_{h,t}_{t,h}^{2}(H^{2}T+d^{10.5}H^{16})\). Taking everything collectively establishes the desired result.

## 4 Numerical experiments

To demonstrate the benefits of hybrid RL in the offline-to-online and online-to-offline settings, we implement Algorithms 1 and 2 on a scaled-down Tetris environment (as in Tan and Xu (2024)). For the purposes of brevity, we defer the details of the environment to Appendix H.3

Figure 1 depicts the coverage (defined by \(1/_{}(),1/_{d_{}}(_ {}),1/_{d_{}}(_{})\)) achieved by the reward-agnostic exploration algorithm, OPTCOV, when initialized with 200 trajectories from (1) a uniform behavioral policy, (2) an adversarial behavior policy obtained by the negative of the weights of a fully-trained agent under Algorithm 1, and (3) no offline trajectories at all. Although hybrid RL with the uniform behavior policy achieves the best coverage throughout as expected, hybrid RL with even adversarially collected offline data achieves better coverage than online-only exploration. This demonstrates the potential of hybrid RL as a tool for taking advantage of poor quality offline data.

Figure 2 shows the benefits of hybrid RL in the online-to-offline setting when the behavior policy is of poor quality. When applying LinPEVI-ADV to the hybrid dataset of \(200\) trajectories and \(100\) online trajectories, \(300\) trajectories of adversarially collected offline data, and \(300\) trajectories of online data under reward-agnostic exploration, we see that the hybrid dataset is most conducive for learning. Additionally, without a warm-start from offline data, online-only reward-agnostic exploration performs worse than the adversarially collected offline data due to significant burn-in costs. Hybrid RL, in this instance, performs better than both offline-only and online-only learning alone. Figure 3 compares the performances of LSVI-UCB++ and Algorithm 2. Initializing a regret-minimizing online algorithm (LSVI-UCB++, (He et al., 2023)) with an offline dataset as in Algorithm 2 yields lower regret than LSVI-UCB++ without an offline dataset. This shows that even a nearly minimax-optimal online learning algorithm can stand to benefit from offline data.

Figure 1: Coverage achieved by OPTCOV with 200 trajectories of offline data collected under a uniform and an adversarial behavior policy, and with no offline data. Results averaged over \(30\) trials, with the shaded area depicting \(1.96\)-standard errors. Lower is better.

## 5 Discussion, limitations and future work

In this paper, we develop two hybrid RL algorithms for linear MDPs with desirable statistical guarantees for the online-to-offline and offline-to-online settings. Both algorithms demonstrate provable gains over the minimax-optimal rates in offline or online-only reinforcement learning, and provide the sharpest worst-case bounds for the performance of hybrid RL in linear MDPs thus far.

Throughout this paper, we have used both optimism and pessimism in our algorithm design. Other work in hybrid RL (Amortila et al., 2024; Li et al., 2023; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Wagenmaker and Pacchiano, 2023) uses optimism, pessimism, or sometimes even neither. We conjecture that optimism is still helpful in aiding online exploration within hybrid RL and that pessimism helps in hybrid RL when learning from a combined dataset. However, determining if or when optimism or pessimism is beneficial in hybrid RL remains an open question.

Achieving a \(H^{3}\) horizon dependence in offline RL for linear MDPs has proven challenging. Even under strong coverage assumptions, Yin et al. (2022) and Xiong et al. (2023) only manage to achieve a \(H^{3}\) horizon dependence for tabular MDPs. Obtaining a \(H^{3}/N}\) bound is an open problem.

A result depending on a partial single-policy concentrability coefficient would be desirable, but may provide only limited benefits as we take the infimum over partitions. A good offline partition for the partial all-policy concentrability contains the portion of the state-action space well-covered by the offline dataset, while the same for the partial single-policy concentrability would be well-covered by both the offline dataset and the optimal policy. The smaller size of the latter offline partition may be offset by the larger size of the latter's online partition, and as such any gains may be limited.

Furthermore, while Algorithm 1 improves upon the offline-only error lower bound in Xiong et al. (2023) and Algorithm 2 improves upon the online-only regret lower bound in Zhou et al. (2021), we still desire a single algorithm that improves upon both the best possible offline-only and online-only rates at once. Additionally, the burn-in costs for Algorithms 1 and 2 are nontrivial. The former is inherited from OPTCOV (Wagenmaker and Jamieson, 2023), while the latter is inherited from He et al. (2023) and the truncation argument. Improving the former by devising new reward-agnostic exploration algorithms for linear MDPs, perhaps in the vein of Li et al. (2023), would be welcome.

While we tackle the setting of linear MDPs, it remains a first step towards showing that hybrid RL breaks minimax-optimal barriers in the presence of function approximation. Further work in this vein on other types of function approximation would be an interesting contribution to the literature.

Figure 3: Comparison of LSVI-UCB++ and Algorithm 2 over 10 trials, with \(1\) s.d. error bars.

Figure 2: Value of policies learned by applying LinPEVI-ADV to the hybrid, offline, and online datasets, with an adversarial behavior policy. The reward is negative as it is the negative of the excess height. Results over \(30\) trials. Higher is better.