# RGFN: Synthesizable Molecular Generation Using GFlowNets

Michal Koziarski\({}^{*,1,2}\), Andrei Rekesh\({}^{*,3}\), Dmytro Shevchuk\({}^{*,3}\), Almer van der Sloot\({}^{1,2}\),

**Piotr Gainski\({}^{4,1,2}\), Yoshua Bengio\({}^{1,2}\), Cheng-Hao Liu\({}^{1,5}\), Mike Tyers\({}^{6,3}\), Robert A. Batey\({}^{3,7}\)**

\({}^{1}\) Mila - Quebec AI Institute, \({}^{2}\) Universite de Montreal, \({}^{3}\) University of Toronto,

\({}^{4}\) Jagiellonian University, \({}^{5}\) McGill University, \({}^{6}\) The Hospital for Sick Children Research Institute,

\({}^{7}\) Acceleration Consortium, \({}^{*}\) Equal contribution

michal.koziarski@mila.quebec, {a.rekesh,dmytro.shevchuk}@mail.utoronto.ca

###### Abstract

Generative models hold great promise for small molecule discovery, significantly increasing the size of search space compared to traditional in silico screening libraries. However, most existing machine learning methods for small molecule generation suffer from poor synthesizability of candidate compounds, making experimental validation difficult. In this paper we propose Reaction-GFlowNet (RGFN), an extension of the GFlowNet framework that operates directly in the space of chemical reactions, thereby allowing out-of-the-box synthesizability while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and building blocks, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries coupled with low cost of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. We demonstrate the effectiveness of the proposed approach across a range of oracle models, including pretrained proxy models and GPU-accelerated docking.

## 1 Introduction

Traditionally, machine learning has been applied to drug discovery for screening existing libraries of compounds, whether actual physical collections or pre-configured in silico collections of readily synthesizable compounds, in a supervised fashion. However, supervised screening of the whole drug-like space, often estimated to contain approximately \(10^{60}\) different compounds, is infeasible in practice. Generative methods offer the potential to circumvent this issue by sampling directly from a distribution over desirable chemical properties without the need to evaluate every possible molecular structure. Despite these advances, existing generative approaches tend not to explicitly enforce synthesizability , generating samples that might be either very costly or altogether impossible to chemically synthesize. Ensuring that generative methods operate in the space of synthesizable compounds, yet at a much larger and more diverse scale than existing chemical libraries, remains an open challenge.

In this paper, we propose Reaction-GFlowNet (RGFN), an extension of the GFlowNet framework  that generates molecules by combining basic chemical fragments using a chain of reactions. We propose a relatively small collection of cheap and accessible chemical building blocks (reactants), as well as established high-yield chemical transformations, that together can still produce a search space orders of magnitude larger than existing chemical libraries. We additionally propose several domain-specific extensions of the GFlowNet framework for action representation and scaling to a larger space of possible actions.

We experimentally evaluate RGFN on a set of diverse screening tasks, including docking score approximation with a trained proxy model for soluble epoxide hydrolase (sEH), GPU-accelerated direct docking score calculations for multiple protein targets (Mpro, ClpP, TBLR1 and sEH), and biological activity estimation with a trained proxy model for dopamine receptor type 2 (DRD2) receptor activity and genotypic activity . We demonstrate that RGFN produces similar optimization quality and diversity to existing fragment-based approaches while ensuring straightforward synthetic routes for predicted hit compounds.

## 2 Related work

**Generative models for molecular discovery.** A plethora of methods have been developed for molecular generation [48; 7] using machine learning. These methods can be categorized depending on the molecular representation used, including textual representations such as SMILES [34; 3; 39], molecular graphs [33; 46; 54] or 3D atom coordinate representations , as well as the underlying methodology, for example, variational autoencoders [33; 46], reinforcement learning [54; 37] or diffusion models . Recently, Generative Flow Networks (GFlowNets) [4; 50; 59; 64; 72; 19] have emerged as a promising paradigm for molecular generation due to their ability to sample large and diverse candidate small molecule space, which is crucial in the drug discovery process. Traditionally, GFlowNets for molecular generation operated on the graph representation level, and candidate molecules were generated as a sequence of actions in which either individual atoms or small molecular fragments were combined to form a final molecule. While using graph representations, as opposed to textual or 3D representations, allows the enforcement of the validity of the generated molecules, it does not guarantee a straightforward route for chemical synthesis. Here, we expand on the GFlowNet framework by modifying the space of actions to consist of choosing molecular fragments and executing compatible chemical reactions/transformations, in turn guaranteeing both physical-chemical validity and synthesizability.

**Synthesizability in generative models.** One approach to ensuring the synthesizability of generated molecules is by using a scoring function, either utilizing it as one of the optimization criteria , or as a post-processing step for filtering generated molecules. Multiple scoring approaches, both heuristic [18; 23] and ML-based , exist in the literature. Another branch of research focuses on using reaction models and traversing predicted synthesis graph [8; 12; 38; 55]. However, reaction and synthesizability estimation is difficult in practice, and can fail to generalize out-of-distribution in the case of ML models. Furthermore, theoretical synthesizability does not necessarily account for the cost of synthesis. Because of this, a preferable approach might be to constrain the space of possible molecules to those easily synthesized by operating in a predefined space of chemical reactions and fragments. Several recent strategies employ this approach [22; 49; 67], including reinforcement learning-based methods [24; 27] and a concurrent work utilizing GFlowNets . We extend this line of investigation not only by translating the concept to the GFlowNet framework but also by proposing a curated set of robust chemical reactions and fragments that ensure efficient synthesis at low total costs.

## 3 Method

### Generative Flow Networks

GFlowNets are amortized variational inference algorithms that are trained to sample from an unnormalized target distribution over compositional objects. GFlowNets aim to sample objects from a set of terminal states \(\) proportionally to a reward function \(:X^{+}\). GFlowNets are defined on a pointed directed acyclic graph (_DAG_), \(G=(S,A)\), where:

* \(s S\) are the nodes, referred to as states in our setting, with the special starting state \(s_{0}\) being the only state with no incoming edges, and the terminal states \(\) have no outgoing edges,
* \(a=s s^{} A\) are the edges, referred to as actions in our setting, and correspond to applying an action while in a state \(s\) and landing in state \(s^{}\).

We can define a non-negative flow function on the edges \(F(s s^{})\) and on the states \(F(s)\) of the DAG such that \( xF(x)=(x)\). A perfectly trained GFlowNet should satisfy the following flow-matching constraint:\[ s S F(s)=_{(s^{} s) A}F(s^{} s)= _{(s s^{}) A}F(s s^{}).\] (1)

A state sequence \(=(s_{0} s_{1} s_{n}=x)\), with \(s_{n}=x\) and \(a_{i}=(s_{i} s_{i+1}) A\) for all \(i\), is called a complete trajectory. We denote the set of trajectories as \(\).

Another way to rephrase the flow-matching constraints is to learn a forward policy \(P_{F}(s_{i+1}|s_{i})\) such that trajectories starting at \(s_{0}\) and taking actions sampled by \(P_{F}\) terminate at \(x\) proportional to the reward.

**Trajectory balance.** Several training losses have been explored to train GFlowNets. Among these, trajectory balance  has been shown to improve credit assignment. In addition to learning a forward policy \(P_{F}\), we also learn a backward policy \(P_{B}\) and a scalar \(Z_{}\), such that, for every trajectory \(=(s_{0} s_{1} s_{n}=x)\), they satisfy:

\[Z_{}_{t=1}^{n}P_{F}(s_{t}|s_{t-1})=R(x)_{t=1}^{n}P_{B}(s_{t-1} |s_{t})\] (2)

### Reaction-GFlowNet

Reaction-GFlowNet generates molecules by combining basic chemical fragments using a chain of reactions. The generation process comprises the following steps (illustrated in Figure 1):

1. Select an initial building block (reactant or surrogate reactant; see Appendix C.2 for more details about surrogate reactants).
2. Select the reaction template (a graph transformation describing the reaction).
3. Select another reactant.
4. Perform the in silico reaction and select one of the resulting molecules.
5. Repeat steps 2-4 until the stop action is selected.

In the rest of this section, we describe the design of the Reaction-GFlowNet in detail.

**Preliminaries.** Reaction-GFlowNet uses a predefined set of reaction patterns and molecules introduced in Section 3.3. We denote these as \(R\) and \(M\) respectively. As a backbone for our forward policy \(P_{F}\), we use a graph transformer model \(f\) from . The graph transformer takes as an input a molecular graph \(m\) and outputs the embedding \(f(m)^{D}\), where \(D\) is the embedding dimension.

Figure 1: Illustration of RGFN sampling process. At the beginning, the RGFN selects an initial molecular building block. In the next two steps, a reaction and a proper reactant are chosen. Then the in silico reaction is simulated with RDKit’s RunReactants functionality and one of the resulting molecules is selected. The process is repeated until the stop action is chosen. The obtained molecule is then evaluated using the reward function.

In particular, \(f\) can embed an empty graph \(\). It can additionally be conditioned on the reaction \(r R\) which we denote as \(f(m,r)\). The reaction in this context is represented as its index in the reaction set \(R\).

**Select an initial building block.** At the beginning of each trajectory, Reaction-GFlowNet selects an initial fragment from the collection of building blocks \(M\). The probability of choosing \(i\)-th fragment \(m_{i}\) is equal to:

\[p(m_{i}|)=^{|M|}()_{i},\ s_{i}=_{M}(f( ))_{i},\] (3)

where \(_{M}:^{D}^{|M|}\) is a multi-layer perceptron (MLP). The \(^{k}\) is a standard softmax over the logits vector \(^{k}\) of the length \(k\):

\[^{k}()_{i}=)}{_{j=1}^{k}(s_{j})}.\]

**Select the reaction template.** The next step is to select a reaction that can be applied to the molecule \(m\). The probability of choosing \(i\)-th reaction from \(R\) is described as:

\[p(r_{i}|m)=^{|R|+1}()_{i},\ s_{i}=_{R}(f(m))_{i},\] (4)

where \(_{R}:^{D}^{|R|+1}\) is an MLP that outputs logits for reactions from \(R\) and an additional stop action with index \(|R|+1\). Choosing the stop action in this phase ends the generation process. Note that not all the reactions may be applied to the molecule \(m\). We appropriately filter such reactions and assume that the score \(s_{i}\) for non-feasible reactions is equal to \(-\).

**Select another reactant.** We want to find a molecule \(m_{i} M\) that will react with \(m\) in the reaction \(r\). The probability for selecting \(m_{i}\) is defined as:

\[p(m_{i}|m,r)=^{|M|}()_{i},\ s_{i}=_{M}(f(m,r))_{i}\] (5)

where \(_{M}\) is shared with the initial fragment selection phase. As in the previous phase, not all the fragments can be used with the reaction \(r\), so we filter these out.

**Perform the reaction and select one of the resulting molecules.** In this step, we apply the reaction \(r\) to the two fragment molecules chosen in previous steps. As the reaction pattern can be matched to multiple parts of the molecules, the result of this operation is a set of possible outcomes \(M^{}\). We choose the molecule \(m^{}_{i} M^{}\) by sampling from the following distribution:

\[p(m^{}_{i})=^{|M^{}|}()_{i},\ s_{i}=_ {M^{}}(f(m^{}_{i})),\] (6)

where \(_{M^{}}:^{D}\) scores the embedded \(m^{}_{i}\) molecule.

**Backward Policy.** A backward policy in RGFN is only non-deterministic in states corresponding to a molecule \(m\) which is a result of performing some reaction \(r R\) on molecule \(m^{}\) and reactant \(m^{} R\). We denote the set of such tuples \((r,m^{},m^{})\) that may result in \(m\) as \(T\). We override the indexing and let \((r_{i},m^{}_{i},m^{}_{i})\) be the \(i\)-th tuple from \(T\). The probability of choosing the \(i\)-th tuple is:

\[p((r_{i},m^{}_{i},m^{}_{i})|m)=^{|T|}()_{i}, \ s_{i}=_{B}(f(m^{}_{i},r_{i})),\] (7)

where \(_{B}:^{D}\) and \(f\) is a backbone transformer model similar to the one used in the forward policy. To properly define \(T\), we need to implicitly keep track of the number of reactions performed to obtain \(m\) (denoted as \(k\)). Only those tuples \((r,m^{},m^{})\) are contained in the \(T\) for which we can recursively obtain \(m^{}\) in \(k-1\) reactions.

**Action Embedding.** While the \(_{M}\) used to predict the probabilities of selecting a molecule \(m_{i} M\) works well for our predefined \(M\), it underperforms when the size of possible chemical building block library is increased. Such an \(_{M}\) likely struggles to reconstruct the relationship between the molecules. Intuitively, when a molecule \(m_{i}\) is chosen in some trajectory, the training signal from the loss function should also influence the probability of choosing a structurally similar \(m_{j}\). However, the \(_{M}\) disregregate the structural similarity by construction and it intertwines the probabilities of choosing \(m_{i}\) and \(m_{j}\) only with the softmax function. To incorporate the relationship between molecules into the model, we embed the molecular building blocks with a simple machine learning model \(g\) and reformulate the probability of choosing a particular building block \(m_{i}\):

\[p(m_{i}|m,r)=^{|M|}()_{i},\ s_{i}=(Wf(m,r))^{T}g(m_{i}),\] (8)where \(\) is some activation function (we use GELU) and \(W^{D D}\) is a learnable linear layer. Note that if we define \(g(m_{i})\) as an index embedding function that simply returns a distinct embedding for every \(m_{i}\), we will obtain a formulation equivalent to Equation (5). To leverage the structure of molecules during the training, we use \(g\) that linearly embeds a MACSS fingerprint  of an input molecule \(m_{i}\) along with the index \(i\). Note that this approach does not add any additional computational costs during the inference as the embeddings \(g(m_{i})\) can be cached. In Section 4.3, we show that this method greatly improves the performance when scaling to larger sets of fragments.

### Chemical language

We select seventeen reactions and 350 building blocks for our model. These include amide bond formation, nucleophilic aromatic substitution, Michael addition, isocyanate-based urea synthesis, sulfur fluoride exchange (SuFEx), sulfonyl chloride substitution, alkyne-azide and nitrile-azide cycloadditions, esterification reactions, urea synthesis using carbonyl surrogates, Suzuki-Miyaura, Buchwald-Hartwig, and Sonogashira cross-couplings, amide reduction, and peptide terminal thiourea cyclization reactions to produce iminohydantoins and tetrazoles. The chosen reactions are known to be typically quite robust  and are often high-yielding (75-100%), thus enforcing reliable synthesis pathways when sampling molecules from our model. To simulate couplings in Python, reactions are encoded as SMARTS templates. To ensure compatible building blocks yielding specific, chemically valid products and to enable parent state computation, we introduce multiple variants corresponding to differing reagent types for most of the proposed reactions. In some cases SMARTS templates encode reactions where one of the reagents is not specified. We describe these transformations as _implicit reactions_ (Appendix E). Additionally, we introduce _surrogate reactions_ where the SMARTS templates and SMILES strings encode for alternate building blocks (see Appendix C.2 for more details). Finally, once again for the sake of specificity, reactions are duplicated by swapping the order of the building block reactants. In total, 132 different SMARTS templates are used.

During the construction of the curated building block database, only affordable reagents (i.e., building blocks) are considered. For the purposes of this study we define affordable reagents to be those priced at less than or equal to $200 per gram. The mean cost per gram of reagents selected for this study is $22.52, the lowest cost $0.023 per gram, and the highest cost $190 per gram (see Appendix N for more details on cost estimation).

A crucial consideration when choosing the set of reactions and fragments used is the state space size (the number of possible molecules that can be generated using our framework). This is difficult to compute precisely since a different set of reactions or building blocks is valid for every state in a given trajectory. We estimate this based on 1,000 random trajectories instead (details can be found in Appendix A). In addition to our 350 low-cost fragments, we also perform this analysis with 8,000 additional random Enamine building blocks. Comparison for different numbers of maximum reactions is presented in Figure 2. We demonstrate that even with curated low-cost reactants and limiting the number to a maximum of four reactions, state space size is an order of magnitude greater than the number of molecules contained in Enamine REAL . This size can increase significantly with the addition of more fragments and/or an increase in the maximum number of reactions. Additional discussions regarding scaling can be found in Section 4.3.

## 4 Experimental study

In the conducted experiments we compare oracle scores and synthesizability scores of RGFN with several state-of-the-art reference methods. Secondly, we examine the capabilities of RGFN to scale to larger fragment libraries, in particular when using the proposed action embedding mechanism. Finally, we perform an in-depth examination of generated ligands across several biologically relevant targets.

### Set-up

Throughout the course of the conducted computational experimental study, we aim to evaluate the performance of the proposed approach across several diverse biological oracles of interest. This includes proxy models (machine learning oracles, pretrained on the existing data and used for higher computational efficiency): first, the commonly used sEH proxy as described in . Second, a graph neural network trained on the biological activity classification task of semolytic  recognition. Third, the Dopamine Receptor D2 (DRD2) oracle  from Therapeutics Data Commons . Proxy model details are provided in Appendix B.1.

Per the GFlowNet training algorithm, the reward is calculated for a batch of dozens to hundreds of molecules at each training step, rendering traditional computational docking score algorithms like AutoDock Vina  infeasible for very large training runs. As a result, previous applications of GFlowNets to biological design [4; 64] employed a fast pre-trained proxy model trained on docking scores instead. These proxies, while lightweight, present potential issues should the GFlowNet generate molecules outside their training data distributions and require receptor-specific datasets. To circumvent this, we use the GPU-accelerated Vina-GPU 2.1  implementation of the QuickVina 2  docking algorithm to calculate docking scores directly in the training loop of RGFN. This approach allows for drastically increased flexibility in protein target selection while eliminating proxy generalization failure. We selected X-ray crystal structures of human soluble epoxy hydrolase (sEH), ATP-dependent Clp protease proteolytic subunit (ClpP), SARS-CoV-2 main protease (Mpro), and transducin \(\)-like-related protein 1 as targets for evaluating RGFN using a docking reward (detailed motivation for specific target selection is provided in Appendix G).

### Comparison with existing methods

We begin experimental evaluation with a comparison to several state-of-the-art methods for molecular discovery. Specifically, we consider a genetic algorithm operating on molecular graphs (GraphGA)  as implemented in , which has been demonstrated to be a very strong baseline for molecular discovery , Monte Carlo tree search-based SyntheMol , cascade variational autoencoder (casVAE) , and a fragment-based GFlowNet (FGFFN)  as implemented in . For FGFN, we additionally considered its variant that had a SAScore as one of the reward terms (FGFN+SA). Training details can be found in Appendix B.2. It is worth noting that besides SyntheMol, which also operates in the space of chemical reactions and building blocks derived from the Enamine database, and casVAE, which used the set of reaction trees obtained from the USPTO database , our remaining benchmarks do not explicitly enforce synthesizability when generating molecules. Because of this, in this section, we will examine not only the quality of generated molecules in terms of optimized properties but also their synthesizability. We consider only two reaction-based approaches, as other existing methods employing this paradigm [27; 24] do not share code or curated reactions and building blocks, making reproduction difficult.

We first examine the distributions of rewards found by each method across four different oracles used for training: sEH proxy, semolytic proxy, DRD2 proxy, and GPU-accelerated docking for ClpP. The results are presented in Figure 3. As can be seen, while RGFN underperforms in terms of average reward when compared to the method not enforcing synthesizability (GraphGA), it outperforms SyntheMol's and casVAE's reaction-based sampling. Interestingly, when compared to standard FGFN, RGFN either performs similarly (ClpP docking) or achieves higher average rewards. This is most striking in the case of the challenging genotypic discovery task, in which a proxy is trained on a severely imbalanced dataset with less than 100 actives, resulting in a sparse reward function. We suspect that this, possibly combined with a lack of compatibility between the FGFN fragments and known sensolvtics, led to the failure to discover any high-reward molecules. However, RGFN succeeds in the task and finds a wide range of semolytic candidates. Finally, the gap in performance is

Figure 2: Estimation of the state space size of RGFN as a function of the maximum number of allowed reactions. RGFN (350) indicates a variant using 350 hand-picked inexpensive building blocks, while RGFN (8350) also uses 8,000 randomly selected Enamine building blocks. Enamine REAL (6.5B compounds) is shown as a reference.

even larger between RGFN and FGFR+SA, indicating that introducing synthesizability constraints reduces the ability of FGFN to discover high-reward molecules.

Secondly, we examine the number of discovered modes for each method, with a mode defined as a molecule with computed reward above a threshold (sEH: 7, sensolvtics: 50, DRD2: 0.95, ClpP docking: 10), and Tanimoto similarity to every other mode \(<0.5\). We use Leader algorithm for mode computation. The number of discovered modes across tasks as a function of normalized iterations is presented in Figure 4. Note that in the case of GraphGA, FGFN, FGFN+SA, and RGFN this simply translates to the number of oracle calls, but for SyntheticMol and casVAE, due to large computational overhead, we impose a maximum number of oracle calls such that training time was comparable to RGFN (see Appendix B.2 for details). Note that in the case of casVAE, this resulted in a very small number of molecules being visited in the allotted time. As can be seen, despite slightly worse average rewards, FGFN still outperforms other methods in terms of the number of discovered modes (with the exception of sensolytic discovery task, where it fails to discover any high-reward molecules). This includes FGFN+SA, despite its generally worse performance than FGFN. This suggests that RGFN samples are less diverse, possibly due to the relatively small number of fragments and reactions used. However, RGFN still outperforms remaining methods across all tasks, suggesting that it preserves some of the benefits of the diversity-focused GFlowNet framework.

Finally, we evaluate the synthesizability of the generated compounds as a key output. We present average values of several synthesizability-related metrics, computed over top-k modes generated for each method, in Table 1. We include measures indicating average molecular weight and drug-likeness (QED) to gauge the size of generated compounds. Furthermore, for completeness, we also include SAScores , but note that they are only a rough approximation of ease of synthesis. For a better estimate of synthesizability we perform retrosynthesis using AiZynthFinder  and count the average number of molecules for which a valid retrosynthesis pathway was found. However, it is important to note that both SAScores and AiZynthFinder scores are inherently noisy metrics. While we evaluate all methods using them for the sake of rigorousness, ultimately molecules generated by RGFN (as well as SyntheMol and casVAE) are guaranteed to be highly likely synthesizable. Note that to reduce variance, we compute molecular weight, QED, and SAScores over the top-500 modes, but due to high computational cost, AiZynthFinder scores are computed only over top-100 modes. As can be seen, while there is some variance across tasks, RGFN performs similarly to SyntheMol and casVAE in terms of both synthesizability scores, and significantly outperforms GraphGA and FGFN. Crucially, including SAScore as a reward does improve the performance of FGFN in terms of that metric, but does not drastically change the AiZynthFinder scores, demonstrating that it is insufficient to guarantee synthesizability. All RGFN modes were additionally inspected manually by an expert chemist and confirmed as synthesizable, which indicates that AiZynth scores are likely underestimated.

Figure 4: Number of discovered modes as a function of normalized iterations. Log scale used.

Figure 3: Distributions of rewards across different tasks.

### Scaling to larger sets of fragments

Next we investigate the influence of a fragment embedding scheme proposed in Section 3.2. In the standard implementation of the GFlowNet policy, actions are represented as independent embeddings in the MLP. These encode actions as indices, effectively disregarding their respective structures and all information contained therein. The model must thus select from a library of reagents without any knowledge as to their chemical makeup or properties. While finding similarities between actions may be a relatively easy task for small action spaces, it becomes more difficult when the size of the action space increases. To scale RGFN to a larger size of the building block library, we proposed to encode building block selection actions using molecular fingerprints, allowing the model to leverage their internal structures without any additional computational overhead during inference. In Figure 5, we observe that our fingerprint embedding scheme allows for drastically faster convergence compared to the standard independent action embedding, especially for large library sizes. The details on how the larger fragment libraries were created can be found in Appendix F.

### Examination of the produced ligands

In the final stage of experiments we examine the capabilities of RGFN to produce high quality ligands across multiple diverse docking targets (see Appendix G for more details). The aim is to evaluate whether 1) the chemical language used is expressive enough to produce structurally diverse molecules for different targets, and 2) whether the generated ligands form realistic poses in the binding pockets. We first demonstrate the diversity of ligands across targets on a UMAP plot of extended-connectivity fingerprints (Figure 6). Ligands assigned to specific targets form very distinct clusters, showcasing their diversity. Interestingly, we also observe structural differences between sEH proxy and sEH docking, possibly indicating poor approximation of docking scores by the proxy model. Secondly, we examine the docking poses of the highest scoring generated ligands (Figure 7). As can be seen, the generated molecules produce realistic docking poses, closely resembling the poses of known ligands (Appendix K), despite being diverse in terms of structural similarity (Appendix M). We further conduct a cost analysis and synthesis planning for top modes in Appendices N and O. Overall, this demonstrates the usefulness of the proposed RGFN approach in the docking-based screens.

   Task & Method & Mol. weight \(\) & QED \(\) & SAScore \(\) & AizSynth \(\) \\  sEH & GraphGA & \(528.6 42.3\) & \(0.21 0.06\) & \(3.87 0.24\) & \(0.04\) \\  & SyntheticMol & \(\) & \(\) & \(2.85 0.55\) & \(0.80\) \\  & casVAE & \(\) & \(0.52 0.23\) & \(\) & \(\) \\  & FGFN & \(473.4 58.9\) & \(0.39 0.13\) & \(3.43 0.48\) & \(0.14\) \\  & FGFN+SA & \(473.7 62.2\) & \(0.36 0.12\) & \(3.01 0.50\) & \(0.27\) \\  & RGFN & \(495.2 49.6\) & \(0.29 0.10\) & \(3.09 0.39\) & \(0.56\) \\  Seno. & GraphGA & \(485.7 75.6\) & \(0.09 0.05\) & \(2.92 0.26\) & \(0.05\) \\  & SyntMol & \(441.4 83.5\) & \(0.48 0.19\) & \(\) & \(0.53\) \\  & casVAE & \(\) & \(\) & \(2.82 0.46\) & \(\) \\  & FGFN & \(468.9 47.7\) & \(0.42 0.13\) & \(3.55 0.52\) & \(0.02\) \\  & FGFN+SA & \(451.8 54.5\) & \(0.32 0.12\) & \(2.83 0.44\) & \(0.13\) \\  & RGFN & \(558.7 62.8\) & \(0.21 0.09\) & \(3.24 0.32\) & \(0.58\) \\  Clp & GraphGA & \(521.0 31.8\) & \(0.32 0.07\) & \(4.14 0.51\) & \(0.00\) \\  & SyntMol & \(458.2 60.7\) & \(0.45 0.16\) & \(2.86 0.56\) & \(0.56\) \\  & casVAE & \(\) & \(\) & \(\) & \(\) \\  & FGFN & \(548.6 42.9\) & \(0.22 0.03\) & \(2.94 0.54\) & \(0.25\) \\  & FGFN+SA & \(509.2 52.4\) & \(0.24 0.04\) & \(2.61 0.49\) & \(0.33\) \\  & RGFN & \(526.2 37.6\) & \(0.23 0.04\) & \(2.83 0.22\) & \(0.65\) \\  DRD2 & GraphGA & \(475.4 53.2\) & \(0.42 0.12\) & \(2.50 0.23\) & \(0.41\) \\  & SyntMol & \(\) & \(\) & \(2.78 0.43\) & \(0.66\) \\  & casVAE & \(404.8 83.5\) & \(0.59 0.20\) & \(2.42 0.38\) & \(\) \\  & FGFN & \(386.5 45.0\) & \(0.63 0.11\) & \(2.58 0.54\) & \(0.76\) \\  & FGFN+SA & \(381.1 35.1\) & \(0.64 0.10\) & \(\) & \(0.78\) \\  & RGFN & \(447.1 45.7\) & \(0.44 0.10\) & \(2.79 0.34\) & \(\) \\   

Table 1: Average values of synthesizability-related metrics for top-k modes.

## 5 Limitations

The current proof-of-principle implementation of RGFN uses only 17 reaction types and 350 building blocks. Although these limited inputs already generate a vast chemical space, this represents only a small fraction of possible drug-like space, which in turn limits the quality and potency of the generated molecular structures. The scaling experiment demonstrates that the number of building blocks can readily be increased, and increasing the number and diversity of building blocks is a straightforward way to enhance and survey the accessible chemical space.

The current set of building blocks and reactions tends to generate linear and flat-shaped molecules. Adding a small set of cyclization reactions (such as peptide macrocyclization and ring-closing metathesis), along with more complex-shaped scaffold building blocks, as well as reactions that introduce sp\({}^{3}\) hybridized atoms and stereochemical complexity will therefore allow for greater shape diversity and the

Figure 5: The number of discovered Murcko scaffolds with sEH proxy value above 7 (a) and 8 (b) as a function of fragment library size. We compare standard independent embeddings of fragment selection actions (blue) with our fingerprint-based embeddings (orange) that account for the fragments’ chemical structure. The number of scaffolds is reported after 2k training iterations for 3 random seeds (the solid line is the median, while the shaded area spans from minimum to maximum values). We observe that our approach greatly outperforms independent embedding when scaling to a larger action space.

Figure 6: UMAP plot of chemical structures of top-500 modes generated for each target. RGFN generates sufficient chemical diversity to produce distinct clusters of compounds. See Appendix G for description of each target protein.

Figure 7: Top docked RGFN ligands after filtering steps (blue) overlaid with the PDB-derived ligand (purple) for each of sEH, ClpP, and Mpro.

generation of more potent molecules [51; 20]. It is also important to recognize that RGFN does not explicitly generate synthetic routes to the molecules, at least not in a strict chemical sense, which in addition to a sequence of reactions transforming sets of reactants into products (which RGFN does provide), would also include choice of reaction conditions, external reagents, catalysts, protection group strategies, etc.

Another significant limitation in the quality of the generated molecular structures is the reliance on molecular docking as a scoring oracle. Although molecular docking has been successfully used in large-scale virtual screening efforts [44; 61; 35], it has well-known shortcomings in its predictive power. First, docking scores correlate strongly with molecular weight (MW)  and do not account for drug-likeness requirements like optimal MW or ClogP. In this work, molecule size was constrained only by the number of reaction steps, encouraging RGFN to generate large molecules within the building block limit. This can be somewhat rectified by augmenting reward with a drug-likeness or ligand efficiency term. Second, its binding affinity predictions and rankings often correlate weakly with experimental values and are highly dependent on the nature of the target protein's binding site [73; 71]. This is further illustrated by the fact that known ligands are not necessarily characterized by highest possible docking scores (Appendix L). This limitation impacts the learning of the chemical structure-activity relationship space, leading to the generation of sub-optimal molecules. One solution to this limitation is to incorporate more accurate but computationally expensive methods (such as ensemble docking, MM-PBSA, and FEP) within a multi-fidelity framework . However, since we focus on robust, affordable, and facile synthesis methods, we ultimately aim to extend our approach beyond computational scoring methods by directly conducting experimental evaluation of synthesized compound batches within an active learning loop.

## 6 Conclusions

In this paper, we present RGFN, an extension of the GFlowNet framework that operates in the action space of chemical reactions. We propose a curated set of high-yield chemical reactions and low-cost molecular building blocks that can be used with the method. We demonstrate that even with a small set of reactions and building blocks, the proposed approach produces a state space with a size orders of magnitude larger than typical experimental screening libraries while ensuring high synthesizability of the generated compounds. We also show that the size of the search space can be further increased by including additional building blocks and that the proposed action embedding mechanism improves scalability to very large building block spaces.

In the course of our experiments, we show that RGFN achieves roughly comparable average rewards to state-of-the-art methods, and it outperforms another approach operating directly in the space of chemical reactions and, crucially, standard fragment-based GFlowNets. At the same time, it significantly improves the synthesizability of generated compounds when compared to a fragment-based GFlowNet. Analysis of ligands produced across the set of diverse tasks demonstrates sufficient diversity of proposed chemical space to generalize to various targets. While not yet demonstrated experimentally, ease of synthesis (due to the small stock of cheap fragments and high-yield chemical reactions used) combined with reasonably high optimization quality of bespoke ligands offer a promising alternative to standard high-throughput screening applications. In particular, it can be beneficial for active learning-based pipelines with significant wet lab component, reducing the reliance on inaccurate docking oracles. Facilitating the drug discovery process through the generation of novel small molecules can eventually lead to the discovery of novel medications leading to significant societal benefits.