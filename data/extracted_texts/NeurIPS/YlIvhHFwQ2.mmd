# DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos

Wen-Hsuan Chu\({}^{}\), Lei Ke\({}^{}\), Katerina Fragkiadaki

Carnegie Mellon University

{wenhsuac,leik,katef}@cs.cmu.edu

###### Abstract

View-predictive generative models provide strong priors for lifting object-centric images and videos into 3D and 4D through rendering and score distillation objectives. A question then remains: what about lifting complete multi-object dynamic scenes? There are two challenges in this direction: First, rendering error gradients are often insufficient to recover fast object motion, and second, view predictive generative models work much better for objects than whole scenes, so, score distillation objectives cannot currently be applied at the scene level directly. We present DreamScene4D, the first approach to generate 3D dynamic scenes of multiple objects from monocular videos via \(360^{}\) novel view synthesis. Our key insight is a "_decompose-recompose_" approach that factorizes the video scene into the background and object tracks, while also factorizing object motion into 3 components: object-centric deformation, object-to-world-frame transformation, and camera motion. Such decomposition permits rendering error gradients and object view-predictive models to recover object 3D completions and deformations while bounding box tracks guide the large object movements in the scene. We show extensive results on challenging DAVIS, Kubric, and self-captured videos with quantitative comparisons and a user preference study. Besides 4D scene generation, DreamScene4D obtains accurate 2D persistent point track by projecting the inferred 3D trajectories to 2D. We will release our code and hope our work will stimulate more research on fine-grained 4D understanding from videos.

## 1 Introduction

Videos are the result of entities moving and interacting in 3D space and over time, captured from a moving camera. Inferring the dynamic 4D scene from video projections in terms of complete 3D object reconstructions and their 3D motions across seen and unseen camera views is a challenging problem in computer vision. It has multiple important applications, such as 3D object and scene state tracking for robot perception , action recognition, visual imitation, digital content creation/simulation, and augmented reality.

Video-to-4D is a highly under-constrained problem since multiple 4D generation hypotheses project to the same video observations. Existing 4D reconstruction works  mainly focus on the visible part of the scene contained in the video by learning a differentiable 3D representation that is often a neural field  or a set of 3D Gaussians  with temporal deformation. _What about the unobserved views of the dynamic 3D scene_? Existing 4D generation works utilize generative models to constrain the appearance of objects in unseen views through score distillation losses. Text-to-4D  or image-to-4D  setups take a single text prompt or image as input to create a 4D object. Several works  explore the video-to-4Dsetup, but these methods predominantly focus on videos containing a single object with minor 3D deformations, where the object deforms in place without large motion in the 3D scene. This focus arises because current generative models perform significantly better at predicting novel views of individual objects  than multi-object scenes. Consequently, score distillation objectives for 3D object lifting are difficult to apply directly at a scene level. Also, optimization difficulty arises when neural fields or 3D Gaussians are trained to model large temporal deformations directly. This limits their practical real-world usage where input videos depicting real-world complex scenes containing multiple dynamic objects with fast motions, as illustrated in Figure 4.

In this paper, we propose DreamScene4D, the first video-to-4D scene generation approach to produce realistic 4D scene representation from a complex multi-object video with large object motion or deformation. To \(360^{}\) synthesize novel views for multiple objects of the scene, DreamScene4D proposes a "decompose-recompose" strategy. A video is first decomposed into objects and the background scene, where each is completed across occlusions and viewpoints, then recomposed to estimate relative scales and rigid object-to-world transformations in each frame using monocular depth guidance, so all objects are placed back in a common coordinate system.

To handle fast-moving objects, DreamScene4D factorizes the 3D motion of the static object Gaussians into 3 components: 1) camera motion, 2) object-centric deformations, and 3) an object-centric to world frame transformation. This factorization greatly improves the stability of the motion optimization process by leveraging powerful object trackers  to handle large motions and allowing view-predictive generative models to receive object-centric inputs that are in distribution. The camera motion is estimated by re-rendering the static background Gaussians to match the video frames.

We show the view renderings at various timesteps and diverse viewpoints of DreamScene4D using challenging monocular videos from DAVIS  in Figure 1. DreamScene4D achieves significant improvements compared to the existing SOTA video-to-4D generation approaches [43; 17] on DAVIS, Kubric , and our self-captured videos with fast moving objects (Figures 4). To evaluate the quality of the learned Gaussian motions, we measure the 2D endpoint error (EPE) of the inferred 3D motion trajectories across occlusions and show that our approach produces accurate and persistent point trajectories in both visible views and synthesized novel views.

## 2 Related Work

Video-to-4D ReconstructionDynamic 3D reconstruction extends static 3D reconstruction to dynamic scenes with the goal of 3D lifting the visible parts of the video. Dynamic NeRF-based

Figure 1: **DreamScene4D** extends video-to-4D generation to multi-object videos with fast motion. We present rendered images and the corresponding motions from diverse viewpoints at different timesteps using real-world DAVIS  videos with multiple objects and large motions.

methods [39; 33; 23; 27; 5] extend NeRF  to dynamic scenes, typically using grid or voxel-based representations [28; 6; 25], or learning a deformation field [6; 13] that models the dynamic portions of an object or scene. Dynamic Gaussian Splatting  extends 3D Gaussian Splatting , where scenes are represented as 4D Gaussians and show faster convergence than NeRF-based approaches. However, these 4D scene reconstruction works [30; 51; 59] typically take videos where the camera has a large number of multi-view angles, instead of a general monocular video input. This necessitates precise calibration of multiple cameras and constrains their potential real-world applicability. Different from these works [34; 51] on mostly reconstructing the visible regions of the dynamic scene, DreamScene4D can \(360^{}\) synthesize novel views for multiple objects of the scene, including the unobserved regions in the video.

Video-to-4D GenerationIn contrast to 4D reconstruction works, this line of research is most related by attempting to complete and 3D reconstruct a video scene across both visible and unseen (virtual) viewpoints. Existing text to image to 4D generation works [43; 17; 14; 32; 61; 62] typically use score distillation sampling (SDS)  to supply constraints in unseen viewpoints in order to synthesize full 4D representations of objects from single text [46; 2; 24; 1], image [58; 66], or a combination of both  prompts. They first map the text prompt or image prompt to a synthetic video, then lift the latter using deformable 3D differentiable NeRFs  or set of Gaussians  representation. Existing video-to-4D generation works [43; 17; 14; 32; 61; 62] usually simplify the input video by assuming a non-occluded and slow-moving object while real-world videos with multiple dynamic objects inevitably contain occlusions. Owing to our proposed scene decoupling and motion factorization schemes, DreamScene4D is the first approach to generate complicated 4D scenes and synthesize their arbitrary novel views by taking real-world videos of multi-object scenes.

## 3 Approach

To generate dynamic 4D scenes of multiple objects from a monocular video input, we propose DreamScene4D, which takes Gaussian Splatting [20; 51] as the 4D scene representation and leverages powerful foundation models to generalize to diverse zero-shot settings.

### Background: Generative 3D Gaussian Splatting

Gaussian Splatting  represents a scene with a set of 3D Gaussians. Each Gaussian is defined by its centroid, scale, rotation, opacity, and color, represented as spherical harmonics (SH) coefficients.

Generative 3D Gaussian Splatting via Score Distillation SamplingScore Distillation Sampling (SDS)  is widely used for text-to-3D or image-to-3D tasks by leveraging a diffusion prior for optimizing 3D Gaussians to synthesize novel views. For 3D object generation, DreamGaussian  uses Zero-1-to-3 , which takes a reference view and a relative camera pose as input and generates plausible images for the target viewpoint, for single frame 2D-to-3D lifting. The 3D Gaussians of the input reference view \(I_{1}\) are optimized by a rendering loss and an SDS loss :

\[_{}_{}^{}=_{t,,, p}[w()(_{}(_{t}^{p};,I_{1},p )-)_{t}^{p}}{}],\] (1)

where \(t\) is the timestep indices, \(w()\) is a weighting function for denoising timestep \(\), \(()\) represents the Gaussian rendering function, \(_{t}^{p}\) is the rendered image, \(_{}()\) is the predicted noise from Zero-1-to-3, and \(\) is the added noise. We take the superscript \(p\) to represent an arbitrary camera pose.

### DreamScene4D

We propose a "_decompose-recompose_" principle to handle complex multi-object scenes. As in Figure 2, given a monocular video of multiple objects, we first segment and track [44; 19; 9; 10] each 2D object and recover the appearance of the occluded regions (Section 3.2.1). Next, we decompose the scene into multiple amodal objects and use SDS  with diffusion priors to obtain a 3D Gaussian representation for each object (Section 3.2.2). To handle large object motions, we optimize the deformation of 3D Gaussians under various constraints and factorize the motion into three components (Figure 3): the object-centric motion, an object-centric to world frame transformation, and the camera motion (Section 3.2.3). This greatly improves the stability and quality of the Gaussian optimization and allows view-predictive image generative models to operate under in-distribution object-centric settings. Finally, we compose each individually optimized object to form a complete 4D scene representation using monocular depth guidance (Section 3.2.4).

#### 3.2.1 Video Scene Decomposition

Instead of taking the video scene as a whole, we first adopt mask trackers  to segment and track objects in the monocular video when GT object masks are not provided. From the monocular video and associated object tracks, we amodally complete each object track before lifting it to 3D as in Figure 2. To achieve zero-shot object appearance recovery for occluded regions of individual object tracks, we build off of inpainting diffusion models  and extend it to videos for amodal video completion. We provide the details of amodal video completion in the appendix.

#### 3.2.2 Object-Centric 3D Lifting from World Frame

After decomposing the scene into individual object tracks, we use Gaussians Splatting  with SDS loss  to lift them to 3D. Since novel-view generative models  trained on Objaverse  are inherently object-centric, we take a different manner to 3D lifting. Instead of directly using the first frame of the original video, where the object areas may be small and not centered, we create a new object-centric frame \(_{1}\) by cropping the object using its bounding box and re-scaling it. Then, we optimize the static 3D Gaussians with both the RGB rendering on \(_{1}\) and the SDS loss  in Eq. 1.

#### 3.2.3 Modeling Complex 3D Motions via Motion Factorization

To estimate the motion of the first-frame lifted 3D Gaussians \(\{G_{1}^{obj}\}\), one solution like DreamGaussian4D  is to model the object dynamics by optimizing the deformation of the 3D Gaussians directly in the world frame. However, this approach falls short in videos with large object motion, as the rendering loss yields minimal gradients until there is an overlap between the deformed Gaussians in the re-rendered frames and the objects in the video frames. Large motions of thousands of 3D Gaussians also increase the training difficulty of the lightweight deformation network .

Thus, we propose to decompose the motion into three components and independently model them: **1)** object-centric motion, modeled using a learnable deformation network; **2)** the object-centric to world frame transformation, represented by a set of 3D displacements vectors and scaling factors; and **3)** camera motion, represented by a set of camera pose changes. Once optimized, the three components can be composed to form the object motion observed in the video.

**Object-Centric Motion Optimization** The deformation of the 3D Gaussians includes a set of learnable parameters for each Gaussian: **1)** a 3D position for each timestep \(_{t}=( x_{t}, y_{t}, z_{t})\), **2)** a 3D rotation for each timestep, represented by a quaternion \(_{t}=(qu_{t},qx_{t},qy_{t},qz_{t})\), and **3)** a

Figure 2: **Method overview for DreamScene4D**: **(a)** We first _decompose_ and amodally complete each object and the background in the video sequence and use DreamGaussian  to obtain static 3D Gaussian representation. **(b)** Next, we factorize and optimize the motion of each object track independently, detailed in Figure 3. **(c)** Finally, we use the estimated monocular depth to _recompose_ the independently optimized 4D Gaussians into one unified coordinate frame.

3D scale for each timestep \(s_{t}=(sx_{t},sy_{t},sz_{t})\). The RGB (spherical harmonics) and opacity of the Gaussians are shared across all timesteps and copied from the first-frame 3D Gaussians.

To compute the 3D object motion in the object-centric frames, we take the cropped and scaled objects in the individual frames \(I_{t}\), forming a new set of frames \(_{t}^{r}\) for each object. Following DreamGaussian4D , we adopt a K-plane  based deformation network \(D_{}(G_{1}^{obj},t)\) to predict the 10-D deformation parameters (\(_{t},R_{t},s_{t}\)) for each object per timestep. We denote the rendered image at timestep \(t\) under the camera pose \(p\) as \(_{t}^{p}\), and optimize \(D_{}\) using the SDS loss in Eq. 1, as well as the rendering loss between \(_{t}^{r}\) and \(_{t}^{r}\) for each frame under the reference camera pose \(r\).

Since 3D Gaussians can freely move within uniformly-colored regions without penalties, the rendering and SDS loss are often insufficient for capturing accurate motion, especially for regions with near-uniform colors. Thus, we additionally introduce a flow rendering loss \(_{flow}\), which is the masked L1 loss between the rendered optical flow of the Gaussians and the flow predicted by an off-the-shelf optical flow estimator . The flow rendering loss only applies to the confident masked regions that pass a simple forward-backward flow consistency check.

**Physical Prior on Object-Centric Motion** Object motion in the real world follows a set of physics laws, which can be used to constrain the Gaussian deformations further. For example, objects usually maintain a similar size in temporally neighboring frames. Thus, we incorporate a scale regularization loss \(_{}=_{t=1}^{T} s_{t+1}-s_{t} _{1}\), where we penalize large Gaussian scale changes.

To preserve the local rigidity during deformations, we apply a loss \(_{}\) to penalize changes to the relative 3D distance and orientation between neighboring Gaussians following . We disallow pruning and densification of the Gaussians when optimizing for deformations like .

**Object-to-world Frame Transformation** We compute the translation \(_{t}=(_{x,t},\,_{y,t},\,_{z,t})\) and scaling factor \(s_{t}^{}\) that warps the Gaussians from the object-centric frame to the world frame. The 2D bounding-box-based cropping and scaling (Sec 3.2.2) from the original frames to the object-centric frames can be represented as an affine warp, which we use to compute and initialize \(_{x,t}\), \(_{y,t}\), and \(s_{t}^{}\) for each object in each frame. \(_{z,t}\) is initialized to \(0\). We then adopt the rendering loss on the original frames \(I_{t}\) instead of center-cropped frames \(_{t}\) to fine-tune \(_{t}\) with a low learning rate.

To further improve the alignment between renderings and the video frames, it is essential to consider the perceptual parallax difference. This arises when altering the object's 3D position while maintaining a fixed camera perspective, resulting in subtle changes in rendered object parts. Thus, we compose the individually optimized motion components and jointly fine-tune the deformation network \(D_{}\) and affine displacement \(_{t}\) using the rendering loss. This refinement process, conducted over a limited number of iterations, helps mitigate the parallax effect as shown in Figure 11 of the appendix.

**Camera Motion Estimation** We leverage differentiable Gaussian Splatting rendering to jointly reconstruct the 3D static video background and estimate camera motions. Taking multi-frame inpainted background images \(I_{t}^{}\) as input, we first use an off-the-shelf algorithm  to initialize the background Gaussians and relative camera rotation and translation \(\{R_{t},T_{t}\}\) between frame \(1\) and frame \(t\). However, the camera motion can only be estimated up to an unknown scale  as there is no metric depth usage. Therefore, we also estimate a scaling term \(\) for \(T_{t}\). Concretely, from

Figure 3: **3D Motion Factorization.** The 3D motion is decomposed into 3 components: 1) the object-centric deformation, 2) the camera motion, and 3) the object-centric to-world frame transformation. After optimization, they can be composed to form the original object motion observed in the video.

the background Gaussians \(G^{bg}\) and \(\{R_{t},T_{t}\}\), we find the \(\) that minimizes the rendering loss of the background in subsequent frames:

\[_{}=_{t=1}^{T}\|I_{t}^{}- (G^{bg},\,R_{t},\,_{t}T_{t})\|_{2},\] (2)

Empirically, optimizing a separate \(_{t}\) per frame  yields better results by allowing the renderer to compensate for erroneous camera pose predictions.

#### 3.2.4 4D Scene Composition with Monocular Depth Guidance

Given the individually optimized 4D Gaussians, we recompose them into a unified coordinate frame to form a coherent 4D scene. As illustrated in Step (c) of Figure 2, this requires determining the depth and scale for each object along camera rays.

Concretely, we use an off-the-shelf depth estimator  to compute the depth of each object and the background and exploit the relative depth relationships to guide the composition. We randomly pick an object as the "reference" object and estimate the relative depth scale \(k\) between the reference object and all other objects. Then, the original positions \(_{t}^{}\) and scales \(s_{t}^{}\) of the 3D Gaussians for the objects are scaled along the camera rays given this initialized scaling factor \(k\): \(_{t}^{}=^{r}-(^{r}-_{t})*k\) and \(s_{t}^{}=s_{t}*k\), where \(^{r}\) represents the position of the camera. Finally, we compose and render the depth map of the reference and scaled object, and minimize the affine-invariant L1 loss [56; 42] between the rendered and predicted depth map to optimize each object's scaling factor \(k\):

\[_{depth}=_{i=1}^{HW}\|_{i}^{*}-_{i}\|_{1},\,\,_{i}=-t(d)}{(d)}.\] (3)

Here, \(_{i}^{*}\) and \(_{i}\) are the scaled and shifted versions of the rendered depth \(d_{i}^{*}\) and predicted depth \(d_{i}\). \(t(d)\) is defined as the reference object's median depth and \((d)\) is defined as the difference between the \(90\%\) and \(10\%\) quantile of the reference object. The two depth maps are normalized separately using their own \(t(d)\) and \((d)\). Once we obtain the scaling factor \(k\) for each object, we can easily place and re-compose the individual objects in a common coordinate frame. The Gaussians can then be rendered jointly to form a scene-level 4D representation, as shown in Figure 5.

## 4 Experiments

**Datasets** While there exist datasets used in previous video-to-4d generation works , they only consist of a small number of single-object synthetic videos with small amounts of motion. Thus, we evaluate the performance of DreamScene4D on more challenging multi-object video datasets, including DAVIS , Kubric , and some self-captured videos with large object motion. We select a subset of 30 challenging real-world videos from DAVIS , consisting of multi-object monocular videos with various amounts of motion. We further incorporate the labeled point trajectories from TAP-Vid-DAVIS  to evaluate the accuracy of the learned Gaussian deformations. In addition, we generated 50 multi-object videos from the Kubric  simulator, which provides challenging scenarios where objects can be small or off-center with fast motion.

**Evaluation Metrics** The quality of 4D generation can be measured in two aspects: the view rendering quality of the generated 3D geometry of the scene, and the accuracy of the 3D motion. For the former, we follow previous works [17; 43] and report the CLIP  and LPIPS  scores between 4 novel-view rendered frames and the reference frame, and compute its average score per video. These metrics allow us to assess the semantic similarity between rendered and reference frames. We also conducted a user study to evaluate the 4D generation quality for the DAVIS videos using two-way voting to compare each baseline with our method, where 50% / 50% indicates equal preference.

The accuracy of the estimated motion can be evaluated by measuring the End Point Error (EPE) of the projected 3D trajectories. For Kubric, we report the mean EPE separately for fully visible points and points that undergo occlusion. For DAVIS, we report the mean and median EPE , as the annotations only exist for visible points.

**Implementation Details** We run our experiments on one 40GB A100 GPU. We crop and scale the individual objects to around 65% of the image size for object lifting. For static 3D Gaussian optimization, we optimize for 1000 iterations with a batch size of 16. For optimizing the dynamic components, we optimize for 100 times the number of frames with a batch size of 10. More implementation and running time details are provided in the appendix.

### Video to 4D Scene Generation

#### Baselines

We consider the following baselines and ablated versions of our model:

**(1)** Consistent4D , a recent state-of-the-art method for 4D generation from monocular videos that fits dynamic NeRFs per video using rendering losses and score distillation.

**(2)** DreamGaussian4D , which uses dynamic 3D Gaussian Splatting like us for 4D generation from videos, but does not use any video decomposition or motion factorization as DreamScene4D. This is most related to our method.

Figure 4: **Video to 4D Comparisons**. We render the Gaussians at various timesteps and camera views. We denote Motion Factorization as **MF** and Video Scene Decomposition as **VSD**. Our method produces consistent and faithful renders for fast-moving objects, while DreamGaussian4D  (2nd row) and Consistent4D  (1st row) produce distorted 3D geometry, blurring, or broken artifacts. Refer to our Supp. Materials for extensive qualitative comparisons.

[MISSING_PAGE_FAIL:8]

Depth-Anything model with MiDAS , a weaker depth prediction model, as well as the newly released Depth-Anything v2 . Furthermore, we added random noise to perturb the outputs of Depth-Anything v1 in various magnitudes.

Since depth estimation is only used for scene composition, we conducted experiments on multi-object DAVIS videos only to emphasize the differences and summarize the results in Table 3. While different depth predictions result in objects being placed in slightly different scene locations, we note that existing SOTA depth prediction models (such as Depth Anything series) meet the requirements in most cases, and the rendered quality of the 4D scene will not deteriorate much as long as the relative depth ordering of the objects is correct, which even holds when we add a small amount of noise to the predicted depth (second row) or use a weaker model like MiDAS (fourth row).

### Limitations

Despite the exciting progress and results presented in the paper, several limitations still exist: **(1)** The SDS prior fails to generalize to videos captured from a camera with steep elevation angles. **(2)** Scene composition may fall into local suboptimas if the rendered depth of the lifted 3D objects is not well

    &  &  &  &  \\   & & &  &  &  &  \\   \\  Baseline: DreamGaussian4D  & ✗ & ✗ & 26.65 & 6.98 & 101.79 & 120.95 \\
**w/** VSD & ✓ & ✗ & 20.95 & 6.72 & 85.27 & 92.42 \\  DreamScene4D (**Ours**) & ✓ & ✓ & **8.56** & 4.24 & **14.30** & **18.31** \\
**w/o**\(_{flow}\) & ✓ & ✓ & 10.91 & **3.83** & 18.54 & 24.51 \\
**w/o**\(_{rigid}\) and \(_{scale}\) & ✓ & ✓ & 10.29 & 4.78 & 16.21 & 22.29 \\   \\  PIPS++  & - & - & 19.61 & 5.36 & 16.72 & 29.65 \\ CoTracker  & - & - & 7.20 & 2.08 & 2.51 & 6.75 \\   

Table 2: **Gaussian Motion Accuracy**. We report the EPE in Kubric  and DAVIS . We denote methods with our Video Scene Decomposition in column **VSD** and methods with 3D Motion Factorization in column **MF**. Note that CoTracker is trained on Kubric.

Figure 5: **Grouping visualization of Gaussians. The grouping of the point cloud is visualized as colored point clouds from different camera views. The spatial relationships between objects are preserved after the composition.**

aligned with the estimated depth. **(3)** Despite the inpainting, the Gaussians are still under-constrained when heavy occlusions happen, and artifacts may occur. **(4)** Our runtime scales linearly with the number of objects and can be slow for complex videos. Addressing these limitations by pursuing more data-driven ways for video to 4D generation is a direct avenue of our future work.

## 5 Conclusion

We presented DreamScene4D, the first video-to-4D scene generation work to generate dynamic 3D scenes across occlusions, large object motions, and unseen viewpoints with both temporal and spatial consistency from multi-object monocular videos. DreamScene4D relies on decomposing the video scene into the background and individual object trajectories, and factorizes object motion to facilitate its estimation through pixel and motion rendering, even under large object displacements. We tested DreamScene4D on popular video datasets like DAVIS, Kubric, and challenging self-captured videos. DreamScene4D infers not only accurate 3D point motion in the visible reference view but also provides robust motion tracks in synthesized novel views.