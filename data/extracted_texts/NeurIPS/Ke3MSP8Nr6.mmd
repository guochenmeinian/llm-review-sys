# Information-theoretic Limits of Online Classification with Noisy Labels

Changlong Wu Ananth Grama Wojciech Szpankowski

CSoI, Purdue University

wuchangl@hawaii.edu, {ayg,szpan}@purdue.edu

###### Abstract

We study online classification with general hypothesis classes where the true labels are determined by some function within the class, but are corrupted by _unknown_ stochastic noise, and the features are generated adversarially. Predictions are made using observed _noisy_ labels and noiseless features, while the performance is measured via minimax risk when comparing against _true_ labels. The noisy mechanism is modeled via a general noisy _kernel_ that specifies, for any individual data point, a set of distributions from which the actual noisy label distribution is chosen. We show that minimax risk is tightly characterized (up to a logarithmic factor of the hypothesis class size) by the _Hellinger gap_ of the noisy label distributions induced by the kernel, _independent_ of other properties such as the means and variances of the noise. Our main technique is based on a novel reduction to an online comparison scheme of two-hypotheses, along with a new _conditional_ version of Le Cam-Birge testing suitable for online settings. Our work provides the first comprehensive characterization for noisy online classification with guarantees that apply to the ground truth while addressing _general_ noisy observations.

## 1 Introduction

Learning from noisy data is a fundamental problem in many machine learning applications. Noise can originate from various sources, including low-precision measurements of physical quantities, communication errors, or noise intentionally injected by methods such as differential privacy. In such cases, one typically learns by training on _noisy_ (or observed) data while aiming to build a model that performs well on the _true_ (or latent) data. This paper focuses on _online learning_ from noisy labels, where one receives noiseless, _adversarially_ generated features and corresponding noisy labels sequentially, and predicts the _true_ labels as the data arrive.

Online learning has been primarily studied in the _agnostic_ setting , where one receives the labels in their plain (noise-free) form and the prediction risk is evaluated on the _observed_ labels. It is typically assumed that both the features and observed labels are generated adversarially, and prediction quality is measured via the notion of _regret_, which compares the actual cumulative risk incurred by the predictor with the minimal cumulative risk incurred by the best expert in a hypothesis class. While this approach is mathematically appealing, it does not adequately characterize online learning scenarios when our goal is to achieve good performance with respect to _grand truth_ data that may be different from the observed (noisy) ones.

This paper considers an online learning scenario that differs from classical _agnostic_ online learning in two aspects: (i) we assume that the noisy labels are derived from a (semi-) _stochastic_ mechanism rather than from pure adversarial selections; (ii) our prediction risk is evaluated on the _true_ labels, not _noisy_ observations. To better motivate the study of such a scenario, we consider the following example first introduced by Ben-David et al. :

**Example 1**.: _Let \(\{0,1\}^{}\) be a finite hypothesis class. Consider the following online learning game between Nature/Adversary and Learner that is played over a time horizon \(T\). At the start, Nature fixes a ground truth classifier \(h\). At each time step \(t T\), Nature adversarially selects feature \(_{t}\) and reveals it to the learner. The learner makes a prediction \(_{t}\) based on prior features \(^{t}=\{_{1},,_{t}\}\) and noisy labels \(^{t-1}=\{_{1},,_{t-1}\}\). Nature then selects a (unknown) noise parameter \(_{t}[0,]\) for some given \(\) (known to learner), and generates 1:_

\[_{t}=(_{t}) y_{t},\]

_where \(\) denotes binary addition and \(y_{t}=h(_{t})\) is the true label. It was shown in [1, Thm 15] that there exists a predictor \(^{T}\) such that:_

\[_{h,^{T}^{T}}[_ {t=1}^{T}1\{_{t} h(_{t})\}]|}{1-2}.\] (1)

Note that the risk in (1) is significant, as the error introduced by noise to the true labels increases linearly as \( T\), yet the risk remains _independent_ of the time horizon \(T\). This mirrors the _fast rates_ known in the PAC learning literature when _benign noise_ is present. Despite its foundational nature, the understanding of this phenomenon beyond simple Massart's noise has been largely unexplored.

This paper introduces a novel online learning framework for modeling _general_ noisy mechanisms. In particular, it encompasses (1) as a very specific instance and provides a clear and comprehensive characterization of the underlying paradigm. Formally, let \(\) be the set of true (latent) labels and \(}\) be the set of noisy (observed) labels, which we assume are finite and of size \(N,M\), respectively. Let \(\) be the feature space. We model the noisy mechanism by a _noisy kernel_:

\[: 2^{(})},\] (2)

where \((})\) is the set of all distributions over \(}\). That is, the kernel \(\) maps each pair \((,y)\) to a _subset_\(_{y}^{}:=(,y) (})\) of distributions over \(}\). Observe that the noisy kernel provides a compact way of modeling _noisy label_ distributions without explicitly referring to the _noise_. This is more convenient for our discussion, as ultimately the statistical information is solely determined by the noisy label distributions.

For any given \(^{}\) and kernel \(\), we consider the following _robust (noisy) online classification_ scenario: Nature first selects \(h\); at each time step \(t\), Nature chooses (adversarially) \(_{t}\) and reveals it to the learner; the learner then makes a prediction \(_{t}\), based on the features \(^{t}\) and _noisy_ labels \(^{t-1}\); an _adversary_ then selects a distribution \(_{t}_{h(_{t})}^{_{t}}\), samples \(_{t}_{t}\) and reveals \(_{t}\) to the learner. Let \(\) and \(\) be the strategies of the learner and Nature/adversary, respectively. The goal of the learner is to minimize the following expected minimax _risk_:

\[_{T}(,)=_{}_{} [_{t=1}^{T}1\{h(_{t})_{t}\}],\] (3)

where \(_{t}=(^{t},^{t-1})\). Note that the adversarial selection of distribution \(_{t}\) from the kernel set \(_{h(_{t})}^{_{t}}\) provides more flexibility for modeling scenarios when the noisy label distribution changes even with the same true label, such as the Massart's noise in Example 1. We refer to Section 2 for a more complete specification of our setting.

### Main Contributions

Our main contributions in this paper establish the _fundamental limits_ of minimax risk in (3) by providing nearly matching lower and upper bounds across a wide range of hypothesis classes \(\) and noisy kernels \(\). Observe that, to allow for non-trivial prediction rules, the induced noisy label distributions must be statistically _distinguishable_ for distinct true labels. To formalize this intuition, we define, for any noisy kernel \(\) and feature \(\), the _Hellinger_ gap as \(_{}()=_{y y^{}}_{p _{y}^{},q_{y^{}}^{}}\{H^{ 2}(p,q)\}\), where \(H^{2}(p,q)=_{m=1}^{M}(-)^{2}\) is the squared Hellinger distance. That is, \(_{}()\) measures the minimal squared Hellinger distance of the induced noisy label distributions over all distinct true labels.

Our main result (see also Theorem 2) can be summarized as follows:

**Theorem 1**.: _Let \(^{}\) be any finite class, and \(\) be any noisy kernel such that \(_{}_{}()_{ }\) for some \(_{}>0\), and \(_{y}^{}(})\) is closed and convex for all \(,y\). Then:_

\[_{T}(,) O(|| }{_{}}).\]

_Moreover, for any \(K\) and any kernel \(\) with at least \( K\) features \(\) for which \(_{}()_{}\), there exists a class \(\) of size \(K\) that satisfies: \(_{T}(,)(| }{_{}}).\)_

Theorem 1 shows that the _Hellinger_ gap is the _right_ characterization for the minimax risk upto at most a logarithmic factor. Moreover, the risk bound depends solely on the gap parameter \(_{}\) and \(||\), _independent_ of time horizon \(T\), the size of \(\) and \(}\), and the properties of noise such as means and variances. For the bounded Bernoulli noise in Example 1, the set \(_{y}^{}\) corresponds to Bernoulli distributions with parameters in \([0,]\) if \(y=0\) and in \([1-,1]\) if \(y=1\), leading to the Hellinger gap \(_{}=1-2\). This matches the dependency on \(\) in Example 12. However, our result holds for _any_ noisy kernel. For instance, if we shift \(_{y}^{}\) to Bernoulli distribution with parameter \(0\) and \(_{y}^{}\) with parameters in \([1-2,1]\), then \(_{}=1-=(1-2)\). This is tighter than the dependency on \(\) in Example 1 (for \(\)), since \(1-2=((1-2)^{2})\).

Our main proof technique for establishing Theorem 1 is based on a novel (black box) reduction to an online comparison scheme of two-hypotheses in \(\), as demonstrated in Theorem 3. This allows us to reduce the noisy online _classification_ problem to a _hypothesis testing_ problem, which effectively decouples the _adversarial_ property of the features from the _stochastic_ property of the noisy labels. However, due to the adversarial selection of the noisy label distributions, the classical hypothesis testing techniques does not apply. To resolve this issue, we establish in Theorem 4, a generalization of the Le Cam-Birge Test with _varying_ conditional marginals for handling pairwise testing via the _Hellinger_ gap, which is a result of independent interest.

Tight dependency on \(||\).Although the lower and upper bounds in Theorem 1 differ by a \(||\) factor, this is compensated by the fact that we are dealing with the most general classes and kernels. This can be tightened for various special cases. Indeed, for a class \(\) with _binary_ true labels and arbitrary noisy labels, we demonstrate in Theorem 5 that the minimax risk is upper bounded by \(|}{_{}}\), where \(_{}\) is the \(L^{2}\)-gap that substitutes the Hellinger distance with \(L^{2}\)-distance in Theorem 1. This is proved via a novel reduction to online conditional distribution estimation under \(L^{2}\)-distance. Moreover, we demonstrate in Appendix G (Theorem 6) that the (optimal) \(O(|}{_{}})\) upper bound holds if \(|_{y}^{}|=1\) for all \(,y\), i.e., the noisy label distribution is _determined_ by data.

### Related Work

Online learning with noisy data was discussed in , which specifically considers generalized _linear_ functions with zero-mean and bounded variance noises. Our work differs in that we focus on classification instead of regression. Moreover, our noisy model does not require that the noise be zero-mean. To the best of our knowledge,  is the only work that has specifically considered the classification task, but this was limited to bounded Bernoulli noise. From a technical standpoint, analogous ideas of pairwise comparison have been considered in differential privacy literature, such as in , but only in _batch_ settings. The reduction to online conditional probability estimation was also explored in  within the context of _online decision making_. However, a distinguishing feature of our work is that our conditional probability estimation problem is necessarily _misspecified_, as our noisy label distributions are selected _adversarially_ and are unknown a priori to the learner. Our problem setup is further related to _differentially private_ conditional distribution learning, as in , and _robust hypothesis testing_, discussed in [17, Chapter 16]. Online conditional probability estimation has been widely studied, see . Conditional density estimation in the _batch_ setting has also been extensively studied, see  for KL-divergence with misspecification and  for \(L^{2}\) loss. Learning from noisy labels in the _batch_ case was discussed in  (see also the references therein) by leveraging suitably defined proxy losses. There has been a long line of research on online prediction with _adversarial_ observable labels in the _agnostic_ formulation, see .

## 2 Notation and Preliminaries

Let \(\) be a set of features (or instances), \(\) be a set of labels, and \(}\) be a set of noisy observations. We assume throughout the paper that \(||=N\) and \(|}|=M\) for some integers \(N,M 2\). We denote \((})\) as the set of all probability distributions over \(}\).

Let \(^{}\) be a hypotheses class and \(\) be a noisy kernel in (2). We consider the following robust online classification scenario: (1) _Nature_ first selects some \(h\); (2) At time \(t\), Nature adversarially selects \(_{t}\); (3) Learner predicts \(_{t}\), based on (noisy) history observed thus far (i.e., \(^{t},^{t-1}\)); (4) An _adversary_ then selects \(_{t}^{_{t}}_{h(_{t})}\), and generates a _noisy_ sample \(_{t}_{t}\).

The goal of the learner is to minimize the _cumulative error_: \(_{t=1}^{T}1\{h(_{t})_{t}\}\).

Note that the cumulative error is a _random variable_ that depends on all the randomness associated with the game. To remove the dependency on such randomness and to assess the fundamental limits of the prediction quality, we consider the following two measures 3:

**Definition 1**.: _Let \(^{}\) be a set of hypotheses and \(: 2^{(})}\) be a noisy kernel. We denote by \(\) the (possibly randomized) strategies of the learner. The expected minimax risk is:_

\[_{T}(,)=_{}_{h ,^{T}^{T}}^{T}_{} _{^{T}}[_{t=1}^{T}1\{h(_{t})_{t}\}],\] (4)

_where \(^{T}_{}_{_{1}^{_{h(_{1})}}_{h(_{1})}}_{_{1} _{1}}_{_{T}^{_{T}}_{h( _{T})}}_{_{T}_{T}}\), and \(_{t}(^{t},^{t-1})\)._

By _skolemization_, the operator \(^{T}_{}\) is equivalent to \(_{_{i}}_{^{T}_{i}}\), where \(\) runs over all (joint) distributions over \(}^{T}\) such that \( t[T],^{t-1}}^{t-1}\) the _conditional_ marginal \(_{_{i}|^{t-1}}^{_{t}}_{h( _{t})}\).

**Definition 2**.: _Let \(\), \(\), and \(\) be as in Definition 1. For any \(>0\), the high probability minimax risk at confidence \(\) is the minimum quantity \(B^{}(,) 0\) such that there exists a predictor \(\) satisfying:_

\[_{h,^{T}^{T},} _{^{T},^{T}}[_{t=1}^{T}1\{h( _{t})_{t}\} B^{}(,) ],\] (5)

_where \(\) is selected as in the discussion above and \(_{t}(^{t},^{t-1})\)._

Note that the kernel map \(\) is generally _known_ to the learner when constructing the predictor \(\). However, the induced kernel sets \(^{_{t}}_{h(_{t})}\) are not, since they depend on the _unknown_ ground truth classifier \(h\) and _adversarially_ generated features \(^{T}\). In certain cases, such as Theorem 3 and Example 4, the kernel map \(\) is also _not_ required to be known.

For any \(\) and \(y\), we denote by \(^{}_{y}\) the set induced by a kernel. We can assume, w.l.o.g., that the \(^{}_{y}\)s are _convex_ and _closed_ sets, since the adversary can select an arbitrary distribution from \(^{}_{y}\)s at each time step, including randomized strategies that effectively sample from a mixture (i.e., convex combination) of distributions in \(^{}_{y}\)s.

One must introduce some constraints on the kernel \(\) in order to obtain meaningful results. To do so, we introduce the following _well-separation_ condition:

**Definition 3**.: _Let \(L:(})^{2}^{20}\) be any divergence, we say a kernel \(\) is well-separated w.r.t. \(L\) at scale \(>0\), if \(\), \( y,y^{}\) with \(y y^{}\) we have \(L(^{}_{y},^{}_{y^{}})}}{{=}}_{p^{}_{y},q^{ }_{y^{}}}L(p,q)\)._

**Example 2**.: _For any \(y\), we specify a canonical distribution \(p_{y}(})\). A natural noisy kernel would be to define \(^{}_{y}=\{p(}):(p,p_{y})\}\), where \(\) denotes total variation. In this case, the kernel is well-separated with the gap \(\) under total variation if \(_{y y^{}}(p_{y},p_{y^{}})+2\). In particular, this subsumes Example 1 if, for \(y\{0,1\}\), we define \(p_{y}\) as the distribution that assigns probability 1 to \(y\), and take \(=\), where the TV-gap equals \(=1-2\)._Main Results

We begin by stating our main result of this paper.

**Theorem 2**.: _Let \(^{}\) be a finite class of size \(K\), and \(\) be a kernel that is well-separated at scale \(_{}\) w.r.t. squared Hellinger divergence (Definition 3). Then, the high probability minimax risk (Definition 2) with confidence \(>0\) is upper bounded by:_

\[B^{}(,)}}+(2/).\] (6)

_Moreover, for any kernel \(\) such that there exist at least \(L\) distinct features \(\)4 for which \(_{y y^{}}H^{2}(_{y}^{}, _{y^{}}^{})_{}\), one can find a class \(\) of size \(K\) such that:_

\[_{T}(,)(} {_{}}).\]

Observe that, the upper bound holds with _high probability_ and the risk is independent of the time horizon (i.e., the so-called fast rates known in the PAC-learning literature). Moreover, the bound is _independent_ of the size of \(\) and \(}\). A simple integration argument yields the _expected_ risk upper bound \((,) O(K}{_{}})\), which matches the lower bound upto only a \( K\) factor. This demonstrates that, the _Hellinger_ gap of the induced noisy label distributions is the _right_ characterization for the minimax risk. Moreover, the Hellinger distance can be transformed from other \(f\)-divergences (such total variation) without depending on the size of \(}\)[17, Chapter 7.6].

**Example 3**.: _Let \(\) be the kernel in Example 2. Let \(=_{y y^{}}(p_{y},p_{y^{}})\). Hence, the kernel is well-separated with TV-gap \(-2\). Since \(H^{2}(p,q)(p,q)^{2}\)[17, Eq. 7.22], the Hellinger gap is lower bounded by \((-2)^{2}\). Invoking Theorem 2, we have for any hypothesis class \(\), the following risk upper bound holds: \(B^{}(,) O(|(| |/)}{(-2)^{2}}).\)_

The rest of this section is devoted to establishing Theorem 2. Our main proof technique is based on a novel reduction to pairwise testing of two hypotheses as developed in Section 3.1, along with explicit testing rules in Section 3.2 based on a novel _conditional_ version of Le Cam-Birge testing.

### Reduction to Pairwise Comparison: a Generic Approach

We first introduce the following key technical concept. Recall that our robust online classification problem is completely determined by the tuple \((,)\).

**Definition 4**.: _A problem \((,)\) is said to be pairwise testable with confidence \(>0\) and error bound \(C() 0\) if, for any pair \(h_{i},h_{j}\), the sub-problem \((\{h_{i},h_{j}\},)\) admits a predictor (i.e., pairwise tester) \(_{i,j}\) that achieves cumulative risk \( C()\) w.p. \( 1-\) (see Definition 2)._

Clearly, any prediction rule for \((,)\) serves as a pairwise testing rule for all the sub-problems \((\{h_{i},h_{j}\},)\) with \(h_{i},h_{j}\). Perhaps surprisingly, we will show in this section that any pairwise testing rules for the sub-problems can also be converted into a prediction rule for \((,)\), incurring only an additional logarithmic factor on the risk bounds.

To this end, suppose that the tuple \((,)\) is _pairwise testable_ and the class \(=\{h_{1},,h_{K}\}\) is finite with size \(K\). Let \(_{i,j}\) be the testing rule (will be constructed in Section 3.2) for \(h_{i},h_{j}\) with error bound \(C()\) and confidence \(>0\). Let \(^{T},^{T}\) be any realization of problem \((,)\). We define, for any \(h_{i}\) and \(t[T]\), a _surrogate loss_ vector:

\[ j[K],\ _{t}^{i}[j]=1\{_{i,j}(^{t},^{t-1})  h_{i}(_{t})\}.\] (7)

That is, the loss \(_{t}^{i}[j]=1\) if and only if the test \(_{i,j}(^{t},^{t-1})\) differs from \(h_{i}(_{t})\). Given access to testers \(_{i,j}\)s, our prediction rule for \((,)\) is then presented in Algorithm 1.

**Theorem 3**.: _Let \(^{}\) be any hypothesis class of size \(K\) and \(\) be any noisy kernel. If \((,)\) is pairwise testable with error bound \(C()\) as in Definition 4, then for any \(>0\), the predictor in Algorithm 1 with \(C=C(/(2K))\) achieves the high probability minimax risk (Definition 2):_

\[B^{}(,) 2(1+2C(/(2K)) K)+(2/).\] (8)

Sketch of Proof.: At a high level, our goal is to identify the ground truth classifier \(h_{k^{*}}\) using the testing results of \(_{i,j}\)s. Note that pairwise testability implies, w.p. \( 1-\), the errors made by tester \(_{k,k^{*}}\) on \(h_{k^{*}}\) is upper bounded by \(C(/2K)\) for all \(k[K]\) simultaneously. However, for any other pair \(i,j k^{*}\), the tester \(_{i,j}\) does not provide any guarantees, since the samples used to test \(h_{i},h_{j}\) originate from \(h_{k^{*}}\) and is not _realizable_ for \(_{i,j}\). The key technical challenge is to extract the testing results for \(_{k,k^{*}}\) from the other irrelevant tests (i.e., \(_{i,j}\) with \(k^{*}\{i,j\}\)), even when the \(k^{*}\) is _unknown_. This is resolved by our definition of \(l^{i}_{t}\) in Algorithm 1, which computes for each \(i\) the _maximum_ testing loss over all of its competitors. This ensures that, for the ground truth \(k^{*}\), the loss \(l^{k^{*}}_{t} C(/2K)\). While for any other \(i k^{*}\), we have \(l^{i}_{t}_{r=1}^{t}_{r}^{i}[k^{*}]_{r=1}^{t}1\{h_{i}( _{r}) h_{k^{*}}(_{r})\}-C(/2K)\). Therefore, any hypothesis \(h_{i}\) for which \(l^{i}_{t}>C(/2K)\) cannot be the ground truth. Algorithm 1 then maintains an index set \(S^{t}\) that eliminates all \(h_{i}\) for which \(l^{i}_{t}>C(/2K)\), and makes prediction \(_{t}=h_{_{t}}(_{t})\) with \(_{t}\) sampling _uniformly_ from \(S^{t}\).

To derive the risk bound, we use a _potential_-based analysis that relates the size of \(S^{t}\)s with the prediction error \(1\{h_{k^{*}}(_{t})_{t}\}\). The intuition behind the analysis is that if \([1\{h_{k^{*}}(_{t})_{t}\}]\) is large, then there will be many elements \(i S^{t}\) for which \(h_{i}(_{t}) h_{k^{*}}(_{t})\), and thus the loss \(l^{i}_{t}\) will (potentially) increase. Since Algorithm 1 constructs \(S^{t+1}\) by eliminating all \(i S^{t}\) for which \(l^{i}_{t}>C(/2K)\), one can therefore bound the prediction error by the _change_ in the size of \(S^{t}\)s. The key technical challenge here is to control the hypotheses that differ from \(k^{*}\) but for which the tester \(_{k,k^{*}}\) errs, which is resolved by carefully defining a _potential_ function. The claimed upper bound then follows by a similar argument as [14, Thm 2]. See Appendix B for complete proof. 

Note that, the reduction of Theorem 3 is _general_ and does not rely on specific properties of the kernel \(\) (such as the well-separation condition). It provides a _black box_ reduction that converts any pairwise testing rule for two-hypotheses to a general online classification rule that introduces only a logarithmic factor on the risk bounds. This effectively decouples the _adversarial_ property of features from the _stochastic_ property of the noisy labels.

To understand how Theorem 3 operates, we consider the following example:

**Example 4**.: _Let \(\{0,1\}^{}\), and \(\) be the bounded Bernoulli noise kernel with parameter \(\) in Example 1. For any \(h_{i},h_{j}\), we construct the following testing rule. We may assume, w.l.o.g., that \(h_{i}() h_{j}()\) for all \(\), since any \(\) for which \(h_{i}()=h_{j}()\) do not affect our testing. Moreover, by relabeling, we can assume that \(h_{i}()=0\) and \(h_{j}()=1\) for all \(\). At time step \(t\), after observing the noisy labels \(^{t-1}\), we compute \(_{t}=_{r=1}^{t-1}_{r}\). If \(_{t}\), the tester predicts \(_{t}=1\); else, it predicts \(_{t}=0\). By Azuma's inequality, the probability of making an error at step \(t\) is upper bounded by \(e^{-(1-2)^{2}(t-1)/2}\). Thus, for any \(n T\), the probability of making any errors afterstep \(n\) is upper bounded by \(_{t=n}^{}e^{-(1-2)^{2}(t-1)/2}n/2}}{(1-2 )^{2}}\). Taking \(n=)}{(1-2)^{2}}\) one can upper bound the probability by \(\). Therefore, the tuple \((,)\) is pairwise testable with \(C())}{(1-2)^{2}}\). Invoking Theorem 3, we have:_

\[B^{}(,) O(|(| |/(1-2)^{2})}{(1-2)^{2}}).\] (9)

Note that the risk bound in (9) recovers the risk in Example 1 up to a logarithmic factor, though it employs a completely different approach (cf. ). Moreover, Example 4 provides the key advantage that the risk holds with _high probability_ and at a _fast rate_, which is known to be non-trivial for _cumulative_ errors (see, e.g., ). To our knowledge, it remains unclear whether the approach proposed in  admits a high probability guarantee.

### Proof of Theorem 2: the _conditional_ Le Cam-Birge Testing

As demonstrated in Section 3.1, the risk of noisy online _classification_ can be reduced to the _pairwise testing_ of two hypotheses. However, we still need to construct the explicit pairwise testing rules. This section is devoted to providing a generic testing rule for _general_ kernels.

Let \(h_{0}\) and \(h_{1}\) be any two hypotheses. We may assume, w.l.o.g., that \(h_{0}() h_{1}()\) for all \(\), since the features for which \(h_{0}\) and \(h_{1}\) agree do not affect the testing. We now provide a more compact characterization of the kernel without explicitly referring to true labels. Let \(^{T}\) be any realization of features. For any \(i\{0,1\}\), \(t[T]\), and kernel \(\), we write \(_{i}^{_{i}}:=(_{t},h_{i}(_{t}))\).

We define \(_{0}^{J}\) and \(_{1}^{J}\) as the sets of all (joint) distributions over \(}^{J}\) induced by the kernel upto time step \(J\) for \(h_{0},h_{1}\), respectively. Equivalently, for \(i\{0,1\}\), we have \(p_{i}^{J}\) if and only if for all \(t[J]\) and \(^{t-1}}^{t-1}\), the _conditional_ marginal \(p_{_{t}|^{t-1}}_{i}^{_{t}}\).

The pairwise testing of \(h_{0},h_{1}\) at time step \(J+1\) is then equivalent to the (robust) _hypothesis testing_ w.r.t. sets \(_{0}^{J}\) and \(_{1}^{J}\). This is typically resolved using Le Cam-Birge testing [17, Chapter 32.2] if the distributions are of _product_ form. However, this does not hold for our purpose, since the distributions in \(_{i}^{J}\) can have highly correlated marginals. Our main result for addressing this issue is a _conditional_ version of Le Cam-Birge testing, as stated in Theorem 4 below. To the best of our knowledge, this conditional version is novel.

**Theorem 4** (_conditional_ Le Cam-Birge Testing).: _Let \(_{0}^{J}\) and \(_{1}^{J}\) be the classes induced by a kernel upto time \(J\) as defined above. For any \(t J\), we denote \(_{t}=H^{2}(_{0}^{_{t}},_{i}^{_{t}})\) and assume that \(_{i}^{_{t}}\) is convex for all \(i\{0,1\}\). Then, there exists a testing rule \(:}^{J}\{0,1\}\) such that_

\[_{p_{0}^{J},q_{1}^{J}}\{_{^{ J} p}[(^{J}) 0]+_{^{J} q}[(^{J})  1]\} 2_{t=1}^{J}(1-_{t}/2) 2e^{-_{t= 1}^{J}_{t}}.\]

Sketch of Proof.: The proof requires a suitable application of the minimax theorem by expressing the testing error as a _linear function_ and arguing that the \(_{i}^{J}\)s are convex. The error bound is then controlled by a careful application of the _chain-rule_ of Renyi divergence. See Appendix C. 

Theorem 4 immediately implies the following _cumulative_ risk bound:

**Proposition 1**.: _Let \(h_{0},h_{1}\) be any hypotheses, \(^{T}\) be any realization of features and \(_{i}^{T}\), \(_{i}^{_{t}}\) be defined as above with \(_{t}=H^{2}(_{0}^{_{t}},_{1}^{_{t}})\). Then, there exists a tester \(^{T}\) such that for all \(>0\), \(i\{0,1\}\) and \(_{i}^{T}\), w.p. \( 1-\) over \(^{T}\), we have:_

\[_{t=1}^{T}1\{h_{i}(_{t})_{t}\}_{n}\{ n:_{t=1}^{n}_{t} 2(2/)\}.\]

Proof.: Let \(n^{*}\) be the minimal number satisfying the RHS. If \(t n^{*}\) (this can be checked at each time step \(t\) using only \(^{t}\) and \(\)), we predict arbitrarily. If \(t n^{*}+1\), we use the tester \(\) in Theorem 4 with \(J=n^{*}\) to produce an index \(}\{0,1\}\) and make the prediction \(h_{i}(_{t})\) for _all_ following time steps. That is, we only use the tester at step \(n^{*}+1\) and reuse the _same_ testing result for all following time steps. By Theorem 4, the probability of making errors after step \(n^{*}+1\) is upper bounded by \(\). Therefore, the cumulative risk is upper bounded by \(n^{*}\) with probability \( 1-\). 

Proof of Theorem 2.: Let \(h_{0},h_{1}\) be any two-hypotheses. For any time step \(t\) such that \(h_{0}(_{t}) h_{1}(_{t})\), we have, by the well-separation condition, that the gap \(_{t}_{}\) in Proposition 1. Consider the following testing rule: for any time step \(t\) such that \(h_{1}(_{t})=h_{2}(_{t})\), we predict the agreed label; else, we predict the same way as in Proposition 1. Clearly, we only make errors for the second case. Invoking Proposition 1 with \(_{t}=_{}\) for all \(t[T]\), we have \(n^{*}}}\). Therefore, the tuple \((,)\) is pairwise testable with \(C()=}}\). The upper bound on _classification_ risk then follows by Theorem 3. The lower bound follows by Le Cam's two point method and constructing a hard hypothesis class using an epoch approach. We refer to Appendix D for the complete details. 

**Remark 1**.: _Note that our techniques can be easily extended to infinite classes using the covering techniques from . Moreover, by applying Proposition 1, our results can be extended to scenarios where the gap parameters \(_{t}\) are not uniformly bounded, such as in the case of Tsybakov-type noise , which would lead to risk bounds that scale sublinearly with \(T\), in contrast to the constant risk in Theorem 2. We leave the details and extensions for a longer manuscript ._

## 4 Tighter Bounds for Binary Labels via \(L^{2}\) Gap

We have demonstrated in Theorem 2 that the minimax risk is tightly characterized by the Hellinger gap induced by the kernel. However, the dependency on \(||\) remains sub-optimal. We show in this section a tight dependency on \(||\) for classes with _binary_ true labels via the \(L^{2}\) gap.

**Theorem 5**.: _Let \(\{0,1\}^{}\) be any finite binary valued class, \(\) be any noisy kernel that is well-separated at scale \(_{}\) w.r.t. the \(L^{2}\)-distance 5 (Definition 3). Then, the expected minimax risk, as in Definition 1, is upper bounded by: \(_{T}(,)|}{_{ }}\)._

We begin with the following simple geometry fact that is crucial to our proof.

**Lemma 1**.: _Let \((})\) be a convex and closed set, \(p\) be a point outside of \(\) with \(}}{{=}}_{q}L^{2}(p,q)\). Denote by \(q^{*}\) the (unique) point that attains \(L^{2}(p,q^{*})=\). Then for any \(q\), we have \(L^{2}(q,p)-L^{2}(q,q^{*}) L^{2}(p,q^{*})=\)._

Proof.: By the _hyperplane separation theorem_, the hyperplane perpendicular to line segment \(p-q^{*}\) at \(q^{*}\) separates \(\) and \(p\). Therefore, the degree \(\) of angle formed by \(p-q^{*}-q\) is greater than \(/2\). By the law of cosines, \(L^{2}(q,p) L^{2}(q,q^{*})+L^{2}(q^{*},p)=L^{2}(q,q^{*})+\). 

Our key idea of proving Theorem 5 is to reduce the robust (noisy) online classification problem to a suitable conditional distribution estimation problem, as discussed next.

Online conditional distribution estimation.Let \((})^{}\) be a class of functions mapping \(\) to _distributions_ in \((})\). Online conditional distribution estimation is a game between _Nature_ and an _estimator_ that follows the following protocol: (1) at each times step \(t\), Nature selects some \(_{t}\) and reveals it to the estimator; (2) the estimator then makes an estimation \(_{t}(})\), based on \(^{t},^{t-1}\); (3) Nature then selects some \(_{t}(})\), samples \(_{t}_{t}\) and reveals \(_{t}\) to the estimator. The goal is to find a (deterministic) estimator \(\) that minimizes the _regret_:

\[_{T}(,)=_{f,^{T} ^{T}}^{T}[_{t=1}^{T}L(_{t},_{t})-L (_{t},f(_{t}))],\] (10)

where \(_{t}=(^{t},^{t-1})\), \(^{T}\) is the operator specified in Definition 1 by setting \(_{y}^{}:=(})\) for all \(,y\), and \(L\) is any divergence. We emphasize that the distributions \(^{T}\) are _not_ necessarily realizable by \(f\) and are selected completely arbitrarily. This contrasts with the _well-specified_ cases employed in , and is the key that enables us to handle the _unknown_ noisy label distributions.

We now establish the following key technical lemma, see Appendix E for proof.

**Lemma 2**.: _Let \((})^{X}\) be a finite distribution-valued function class. Then, for the \(L^{2}\) divergence, there exists an estimator \(\), i.e., the Exponential Weight Average (EWA) algorithm, such that_

\[_{T}(,) 4||.\]

_Moreover, estimation \(_{t}\) is a convex combination of \(\{f(_{t}):f\}\)._

Proof Sketch of Theorem 5.: We provide the high level ideas and refer to Appendix F for complete details. We define the following _distribution-valued_ function class \(\) using hypothesis class \(\) and noisy kernel \(\). For any \(\), we denote by \(_{0}^{}\) and \(_{1}^{}\) the sets of noisy label distributions corresponding to labels \(0\) and \(1\), respectively. Since the kernel \(\) is well-separated at scale \(_{}\) under \(L^{2}\) divergence, we have, by the _hyperplane separation theorem_, that there must be \(q_{0}^{}_{0}^{}\) and \(q_{1}^{}_{1}^{}\) such that \(L^{2}(q_{0}^{},q_{1}^{})=L^{2}(_{0}^{},_{1}^{})_{}\). We now define for any \(h\) the function \(f_{h}\) such that \(,\;f_{h}()=q_{h()}^{ }\). Let \(=\{f_{h}:h\}\) and \(\) be the estimator from Lemma 2 with class \(\) and \(L^{2}\) divergence (using \(^{T},^{T}\) from the _original_ noisy classification game). Our _classification_ rule is defined as \(_{t}=_{y}\{L^{2}(q_{y}^{_{t}},_{t}):y\{0,1\}\}\). That is, we predict the label \(y\) so that \(q_{y}^{_{t}}\) is closer to \(_{t}\) under \(L^{2}\) divergence, where \(_{t}=(^{t},^{t-1})\).

Let \(h^{*}\) be the underlying true classification function. We have by Lemma 2 that

\[_{^{T}^{T}}_{}^{T}[ _{t=1}^{T}L^{2}(_{t},_{t})-L^{2}(_{t},f_{h^{*}}( _{t}))] 4|| 4||,\] (11)

where \(_{}^{T}\) is the operator in Definition 1. Now, our key technical goal is to show that \(L^{2}(_{t},_{t})-L^{2}(_{t},f_{h^{*}}(_{t} )) L^{2}(_{t},f_{h^{*}}(_{t}))}}{4}1\{_{t} h^{*}(_{t})\}\) via Lemma 1 and a _geometric_ argument, as illustrated in the figure below:

The expected minimax risk bound \(_{t=1}^{T}1\{_{t} h^{*}(_{t})\}|}{_{}}\) then follows from (11). 

Although both our proofs and those provided in  are based on the EWA algorithm, the analysis and resulting algorithms are fundamentally different. For instance, in , the learning rate of EWA depends on the parameter \(\), while we set it to \(1/4\) (see Appendix E). More importantly, our proof applies to _any_ noisy kernel that satisfies the well-separation condition (including cases where \(|}|>2\)), which benefits from our _geometric_ interpretation of the kernels. Interestingly, for the specific setting investigated in  (i.e., Example 1), our result yields the same order up to a constant factor, since \(1-2=((1-2)^{2})\) for \([0,)\). In general, we have \(4_{}_{}}}\).

## 5 Discussion

In this paper, we provide nearly matching lower and upper bounds for online classification with noisy labels via the Hellinger gap of the induced noisy label distributions. Our approach works for a wide range of hypothesis classes and noisy mechanisms. We expect our results to have a wide range of applications, such as online learning under (local) differential privacy constraints and online denoising tasks involving data derived from (noisy) physical measurements (such as learning from quantum data ). The main open problem remaining is to close the logarithmic gap in Theorem 2 for _general_ kernels. While our work primarily focuses on the information-theoretically achievable minimax risks, we believe that finding computationally efficient predictors (including oracle-efficient methods as in ) would also be of significant interest.