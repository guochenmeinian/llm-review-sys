# SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution

Qi Tang\({}^{1,2}\), Yao Zhao\({}^{1,2}\), Meiqin Liu\({}^{1,2}\)1, Chao Yao\({}^{3}\)1

\({}^{1}\) Institute of Information Science, Beijing Jiaotong University

\({}^{2}\) Visual Intelligence + X International Cooperation Joint Laboratory of MOE,

Beijing Jiaotong University

\({}^{3}\) School of Computer and Communication Engineering,

University of Science and Technology Beijing

{qitang, yzhao, mqliu}@bjtu.edu.cn, yaochao@ustb.edu.cn

Corresponding Authors

###### Abstract

Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear-a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel condenser, which synergize to extract and upscale semantic details from low-resolution frames. The **I**nstance-**C**entric **A**lignment **M**odule (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the **C**hannel-wise **T**exture **A**ggregation **M**emory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.

## 1 Introduction

Video super-resolution (VSR) is a challenging low-level vision task that involves improving the resolution and visual quality of the given low-resolution (LR) observations and maintaining the temporal coherence of high-resolution (HR) components. Various deep-learning-based VSR approaches [1; 3; 30; 17; 13; 20; 21] explore effective inter-frame alignment to reconstruct satisfactory sequences. Despite establishing one new pole after another in the quantitative results, they struggle to generate photo-realistic textures.

With the explosion of diffusion model (DM) in visual generation [10; 31; 26], super-resolution (SR) from the generative perspective also garners the broad attention [28; 29; 45; 5]. DM breaks the generation process into sequential sub-processes and iteratively samples semantic-specific images from Gaussian noise, equipped with a paired forward diffusion process and reverse denoising process. The former progressively injects varied intensity noise into the image along a Markov chain to simulate diverse image distributions. The latter leverages a denoising network to generate an image based on the given noise and conditions. Early efforts directly apply the generation paradigm to super-resolution, overlooking its characteristic while generating pleasing content, thus trapping in huge sampling overhead.

Different from generation from scratch, SR resembles partial generation. The structural information that dominates the early stages of diffusion is contained in the LR priors, while SR tends to focus on generating high-frequency details [33; 15]. Besides, the loss of high-frequency information in LR videos stems from the limited sensing range of imaging equipment. As a result, solely disrupting frames with additive noise is inadequate to depict the degradation of HR videos . Moreover, prevalent VSR methods employ delicate inter-frame alignment (e.g., optical flow or deformable convolution) to fuse the sub-pixel information across adjacent frames. However, the disturbed pixels pose a severe challenge to these methods, rendering the accuracy to deteriorate in the pixel space.

To alleviate the above issue, we introduce **SeeClear**, an innovative diffusion model empowering distilled semantics to enhance the pixel condensation for video super-resolution. During the forward diffusion process, the low-pass filter is applied within the patch, gradually diminishing the high-frequency component, all while the residual is progressively shifted in the frequency domain to transform the HR frames to corresponding LR versions step by step. To reduce the computational overhead, the intermediate states are decomposed into various frequency sub-bands via 2D discrete wavelet transform and subsequently processed by the attention-based U-Net. Furthermore, we devise a dual semantic-controlled conditional generation schema to enhance the temporal coherence of VSR. Specifically, a segmentation framework for open vocabulary is employed to distill instance-centric semantics from LR frames. They serve as prompts, enabling the Instance-Centric Alignment Module (InCAM) to highlight and associate semantically related pixels within the local temporal scope. Besides, abundant semantic cues in channel dimensions are also explored to form an extensional memory dubbed Channel-wise Texture Aggregation Memory (CaTeGory). It aids in global temporal coherence and boosts performance. Experimental results demonstrate that our method consistently outperforms existing state-of-the-art methods.

In summary, the main contributions of this work are as follows:

* We present SeeClear, a diffusion-based framework for video super-resolution that distills semantic priors from low-resolution frames for spatial modulation and temporal association, controlling the condition of pixel generation.
* We reformulate the diffusion process by integrating residual shifting with patch-level blurring, and introduce an attention-based architecture to explore valuable information among wavelet spectra during the sampling process, incorporating feature modulation of intra-frame semantics.
* We devise a dual semantic distillation schema that extracts instance-centric semantics of each frame and further assembles them into texture memory based on the semantic category of channel dimension, ensuring both short-term and long-term temporal coherence.

## 2 Related Work

### Video Super-Resolution

Prominent video super-resolution techniques concentrate on leveraging sub-pixel information across frames to enhance performance. EDVR  employs cascading deformable convolution layers (DCN) for inter-frame alignment in a coarse-to-fine manner, tackling large amplitude video motion. BasicVSR  comprehensively explores each module's role in VSR and delivers a simple yet effective framework by reusing previous designs with slight modifications. Given the similarity between DCN and optical flow, BasicVSR++  devises flow-guided deformable alignment, exploiting the offset diversity of DCN without instability during the training. VRT  combines mutual attention with self-attention, which is respectively in charge of inter-frame alignment and information preservation.

Figure 1: The sketch of SeeClear. It consists of a Semantic Distiller and a Pixel Condenser, which are responsible for distilling instance-centric semantics from LR frames and generating HR frames. The instance-centric and assembled channel-wise semantics act as thermometer to control the condition for generation.

RVRT  extends this by incorporating optical flow with deformable attention, aligning and fusing features directly at non-integer locations clip-to-clip. PSBT  reassesses prevalent alignment methods in transformer-based VSR and implements patch alignment to counteract inaccuracies in motion estimation and compensation. DFVSR  represents video with the proposed directional frequency representation, amalgamating object motion into multiple directional frequencies, augmented with a frequency-based implicit alignment, thus enhancing alignment.

### Diffusion-Based Super-Resolution

Building on the success of diffusion models in the realm of image generation [10; 31; 26; 25; 48], diffusion-based super-resolution (SR) is advancing. SR3 , a pioneering approach, iteratively samples an HR image from Gaussian noise conditioned on the LR image. In contrast, StableSR  applies diffusion-based SR in a low-dimensional latent space using the pre-trained auto-encoder to reduce computation and generate improved results through the generative priors contained in weights of Latent Diffusion. ResDiff  combines a lightweight CNN with DM to restore low-frequency and predict high-frequency components, and ResShift  redefines the initial step as a blend of the low-resolution image and random noise to boost efficiency. Applying a different approach, DiWa  migrates the diffusion process into the wavelet spectrum to effectively hallucinate high-frequency information. Upscale-A-Video , for video super-resolution, introduces temporal layers into the U-Net and VAE-Decoder and deploys a flow-guided recurrent latent propagation module to ensure temporal coherence and overall video stability when applying image-wise diffusion model.

### Semantic-Assisted Restoration

Traditionally seen as a preparatory step for subsequent tasks [50; 12], restoration is now reformulated with the assistance of semantics. SFT  utilizes semantic segmentation probability maps for spatial modulation of intermediate features in the SR network, yielding more realistic textures. SKF  supports low-light image enhancement model to learn diverse priors encapsulated in a semantic segmentation model by semantic-aware embedding module paired with semantic-guided losses. SeD  integrates semantics into the discriminator of GAN-based SR for fine-grained texture generation rather than solely learning coarse-grained distribution. CoSeR  bridges image appearance and language understanding to empower SR with global cognition buried in LR image, regarding priors of text-to-image (T2I) diffusion model and a high-resolution reference image as powerful conditions. SeeSR  analyzes several types of semantic prompts and opts tag-style semantics to harness the generative potential of the T2I model for real SR. Semantic Lens  forgoes pixel-level inter-frame alignment and distills diverse semantics for temporal association in the instance-centric semantic space, attaining better performance.

## 3 Methodology

Given a low-resolution (LR) video sequence of \(N\) frames \(I^{LR}_{i}^{N C H W}\), where \(i\) is the frame index, \(H W\) represents spatial dimensions, and \(C\) stands for the channel of frame, SeeClear aims to exploit rich semantic priors to generate the high-resolution (HR) video \(I^{HR}_{i}^{N C sH sW}\), with \(s\) as the upscaling factor. In the iterative paradigm of the diffusion model, HR frames are corrupted according to handcrafted transition distribution at each diffusion step (\(t=1,2,,T\)). And a U-shaped network is employed to estimate the posterior distribution using LR frames as condition during reverse generation. As illustrated in Figure 1, it consists of a Semantic Distiller and a Pixel Condenser, respectively responsible for semantic extraction and texture generation.

The LR video is initially split into non-overlapping clips composed of \(m\) frames for parallel processing. Semantic Distiller, a pre-trained network for open-vocabulary segmentation, distills semantics related to both instances and background clip by clip, denoted as instance-centric semantics. Pixel Condenser is an attention-based encoder-decoder architecture, in which the encoder extracts multi-scale features under the control of LR frames, and the decoder generates HR frames from coarse to fine. They are also bridged via skip connections to transmit high-frequency information at the same resolution. To maximize the network's generative capacity, instance-centric semantics are utilized as conditions for individual frame generation in the decoder. They also serve as the cues of inter-frame alignment for temporal coherence within the video clip and further cluster into a semantic-texture memory along channel dimension for consistency across clips.

### Blurring ResShift

During the video capturing, frequencies exceeding the imaging range of the device are truncated, leading to the loss of high-frequency information in LR videos. Therefore, an intuition is to construct a Markov chain between HR frames and LR frames in the frequency domain. Inspired by blurring diffusion , the forward diffusion process of SeeClear initializes with the approximate distribution of HR frames. It then iterates and terminates with the approximate distribution of LR frames using a Gaussian kernel convolution in frequency space facilitated by the Discrete Cosine Transformation (DCT). Considering the correlation of neighboring information, blurring is conducted within a local patch instead of the whole image. The above process is formulated as:

\[q(_{t}_{0})=(_{t}_{t }_{0},_{t}), t\{1,,T\},\] (1)

\[_{0}=^{}I_{i}^{HR},\] (2)

where \(_{0}\) and \(_{t}\) denote HR frames and intermediate states in the frequency space for brevity. \(^{}\) denotes the projection matrix of DCT. \(_{t}=e^{t}\) is diagonal blurring matrix with \(_{x p+y}=-^{2}(}{p^{2}}+}{p^{2}})\) for coordinate \((x,y)\) within patch of size \(p p\), and \(_{t}\) is the variance of noise. \(\) is the identity matrix.

In the realm of generation, the vanilla destruction process progressively transforms the image into pure Gaussian noise, leading to numerous sampling steps and tending to be suboptimal for VSR. An alternative way is to employ a transition kernel that shifts residuals between HR and LR frames, accompanied by patch-level blurring. The forward diffusion process is formulated as:

\[q(_{t}_{0},_{l})= (_{t}_{t}_{0}+_{t}_{t},^{2}_{t} ), t\{1,,T\},\] (3) \[_{t}=_{l}-_{t}_{0},\] (4)

where \(_{l}\) denotes LR frames transformed into the frequency space. \(_{t}\) indicates the residuals between LR and blurred HR frames at time step \(t\). \(_{t}\) represents the shifting sequence and \(\) is a hyper-parameter determining the intensity of noise. Upon this, SeeClear can yield HR frames by estimating the posterior distribution \(p(_{0}|_{l})\) in the reverse sampling progress, formulated as:

\[p(_{0}_{l})= p(_{T}_{l} )_{t=1}^{T}p_{}(_{t-1}_{t},_{ l})_{1:T},\] (5)

\[p(_{T}_{l})(_{T} _{l},^{2}),\] (6)

where \(p_{}(_{t-1}_{t},_{l})\) represents the inverse transition kernel restoring \(_{t}\) to \(_{t-1}\). \(\) denotes learnable parameters of attention-based U-Net.

To alleviate the computational overhead, preceding methods introduce an autoencoder to transform pixel-level images in the perceptually equivalent space, concentrating on the semantic composition and bypassing the impedance of high-frequency details. However, the loss of high-frequency information during encoding is hard to recover in the decoding and will deteriorate the visual quality. Therefore, we forgo the autoencoding method in SeeClear and incorporate discrete wavelet transform (DWT) in the diffusion process. Specifically, the HR and LR frames are recursively decomposed into four sub-bands:

\[I_{ll}^{HR},I_{lh}^{HR},I_{hl}^{HR},I_{hh}^{HR}=_{ }(I_{i}^{HR}),\] (7)

where \(I_{ll}^{HR}\) denotes the low-frequency approximation, \(I_{lh}^{HR}\), \(I_{hl}^{HR}\) and \(I_{hh}^{HR}\) correspond to horizontal, vertical and diagonal high-frequency details. \(_{}()\) represents the 2D Discrete Wavelet Transform (DWT). After \(k\) decompositions, each of them possesses a size of \(}}\). These coefficients are contacted along the channel dimension and serve in the diffusion process. The rationale behind employing DWT as a substitute is two-fold. Firstly, it enables the U-Net to perform on a small spatial size without information loss. Secondly, it benefits from the U-Net scaling  hindered by additional parameters of the autoencoding network. To make full use of DWT, window-based self-attention followed by channel-wise self-attention is stacked as the basic unit of U-Net, in charge of the correlation of intra-sub-bands and inter-sub-bands, respectively.

### Instance-Centric Alignment within Video Clips

Due to the destruction of the diffusion process, pixel-level inter-frame alignment, such as optical flow, is no longer applicable. With the premise of semantic embedding, we devise the Instance-Centric Alignment Module (InCAM) within video clips, as illustrated in Figure 2. It establishes temporal association in the semantic space instead of intensity similarity among frames, avoiding the interference of noise and blurriness. Specifically, Semantic Distiller predicts a set of image features \(F_{img}\) and text embedding \(F_{txt}\) from LR frames and predefined vocabulary \(\). After that, the \(k\) image features with the highest similarity to the text embedding are retained, including token-level semantic priors and pixel-level segmentation features from LR frames. The above procedure is formulated as:

\[F_{img},F_{txt}=(I_{i}^{LR},),\] (8)

\[O_{i},P_{i}=top_{k}((F_{img},F_{txt})),\] (9)

where \(()\) represents Semantic Distiller. \(top_{k}()\) and \(()\) denote the operations of selecting the \(k\) largest items and calculating the similarity respectively. \(O_{i}\) and \(P_{i}\) are semantic tokens and segmentation features, in which the former represents high-level semantics and can locate related pixels in the segmentation features. The segmentation features contain both semantics and low-level structural information, which is suitable for bridging the semantics and features of the Pixel Condenser. It is utilized to generate spatial modulation pairs prepared for semantic embedding, formulated as:

\[(,)=(P_{i}),\] (10)

\[F_{i}=(f_{i}+)+f_{i},\] (11)

where \(\) and \(\) represent scale and bias for modulation. \(f_{i}\) and \(F_{i}\) correspond to original and modulated features. \(\) denotes two convolutional layers followed by a ReLU activation. "\(\)" represents the Hadamard product. After that, InCAM embeds semantics into modulated features based on multi-head cross-attention, yielding semantic-embedded features \(_{i}\):

\[_{i}=F_{i}W^{Q},\;_{i}=O_{i}W^{K},\;_{i}=O_{i}W^ {V},\] (12)

\[_{i}=(_{i}_{i}^{T}/ )_{i},\] (13)

where \(_{i}\), \(_{i}\) and \(_{i}\) denote matrices derived from modulated features and semantic tokens. \(W^{Q}\), \(W^{K}\) and \(W^{V}\) represent the linear projections, and \(d\) is the dimension of projected matrices. \(()\) denotes the SoftMax operation. To benefit from the adjacent supporting frames, it is necessary to establish semantic associations among frames. The frame-wise semantic tokens are further fed into the instance encoder-decoder for communicating semantics along the temporal

Figure 2: The illustration of Instance-Centric Alignment Module (InCAM). It utilizes the segmentation features to bridge the pixel-level information and instance-centric semantic tokens. And then, the semantic-aware features can be aligned in the semantic space based on their semantic relevance.

dimension, generating clip-wise semantic tokens. It gathers all the information of the same semantic object within a clip and serves as the guide for inter-frame alignment. Specifically, InCAM combines semantic guidance and enhanced features to activate the related pixels across frames and utilizes multi-frame self-attention for parallel alignment and fusion. The above procedure is formulated as:

\[O_{c}=((O_{i}),),\] (14)

\[_{i}=(O_{c}_{i}),\] (15)

where \(O_{c}\) and \(\) respectively denote clip-wise semantic and randomly initialized tokens. \(()\) and \(()\) represent instance encoder and decoder. \(()\) denotes the multi-frame self-attention , the extended version of self-attention in video. \(_{i}\) is aligned feature. The product of semantics and enhanced features is akin to the class activation mapping, which highlights the most similar pixels in the instance-centric semantic space among frames.

### Channel-wise Aggregation across Video Clips

Due to the limited size of the temporal window, the performance of clip-wise mutual information enhancement in long video sequences is unsatisfactory, which could lead to inconsistent content. To stabilize video content and enhance visual texture, the Channel-wise Texture Aggregation Memory (CaTeGory) is constructed to cluster abundant textures according to channel-wise semantics, as graphically depicted in Figure 3. It is comprised of the channel-wise semantic and the corresponding texture. Specifically, channels of instance-centric semantics also contain distinguishing traits, which are assembled and divided into different groups to form the channel-wise semantic. Concurrently, hierarchical features from the decoder of the Pixel Condenser are clustered into the corresponding semantic group to portray the textures. The connection between them is established in a manner similar to the position embedding in the attention mechanism. The above process can be formulated as:

\[(C_{j},T_{j})=(_{j},_{j},\{_{i,k}\}_{k=1}^{4}),\] (16)

where \(_{i,k}\) is the features benefited from adjacent frames of \(k\)-th layer. \(_{j}\) and \(_{j}\) respectively denote channel-wise semantics and textures of \(j\)-th group, which are zero-initialized as network parameters. They are iteratively updated towards the final version (i.e., \(C_{j}\) and \(T_{j}\)) by injecting external knowledge from the whole dataset and previous clips. And \(()\) represents the construction of CaTeGory. It concatenates the multi-scale features and incorporates them into channel-wise semantics and textures:

\[_{j}=_{j}_{j},\] (17)

\[T_{j}=((_{j},\{_{i,k} \}_{k=1}^{4})),\] (18)

where \(_{j}\) is the textures embedded channel-wise semantics. \(()\) and \(()\) indicate multi-head self-attention and cross-attention. The layer normalization and feed-forward network are omitted for brevity. It bridges the channel-wise semantics and textures via matrix multiplication and further fuses high-value information from the pyramid feature, delivering augmented semantic-texture pairs. The hierarchical features not only provide rich structural information but also carry relatively abstract amid features in order to benefit different decoder layers more effectively. At each layer, the prior knowledge stored in CaTeGory is firstly queried by the clip-wise semantics along channel dimension and aggregated for feature enhancement of the current clip, which is formulated as:

\[_{j}=(O_{c}^{T}C_{j}),\] (19)

\[_{i}=(_{i},_{j}T_{j})+_{i},\] (20)

Figure 3: The illustration of Channel-wise Texture Aggregation Memory (CaTeGory). It assembles the textures based on the semantic class along the channel dimension.

where \(_{j}\) depicts the similarity between the clip-wise semantics and items of CaTeGory along channel dimensions, and \(_{i}\) is the refined features as input of the next layer. As mentioned before, semantic-texture pairs are optimized as parts of the network during the training stage, absorbing ample priors from the whole dataset. In the sampling process, the update mechanism is reused to integrate the long-term information of video into memory to improve the super-resolution of subsequent clips.

## 4 Experiments

### Experimental Setup

**Datasets** To assess the effectiveness of the proposed SeeClear, we employ two commonly used datasets for training: REDS  and Vimeo-90K . The REDS dataset, characterized by its realistic and dynamic scenes, consists of three subsets used for training and testing. In accordance with the conventions established in previous works [1; 3], we select four clips2 from the training dataset to serve as a validation dataset, referred to as REDS4. The Vid4  dataset is used as the corresponding test dataset for Vimeo-90K. The LR sequences are degraded through bicubic downsampling (BI), with a downsampling factor of \(4\).

**Implementation Details** The pre-trained OpenSeeD  is opted as the semantic distiller with frozen weights, while all learnable parameters are contained in the pixel condenser. And the schedulers of blur and noise in the diffusion process follow the settings of IHDM  and ResShift . During the training, the pixel condenser is first trained to generate an HR clip with \(5\) frames under the control of instance-centric semantics. And then, Channel-wise Texture Memory is independently trained to inject valuable textures from the whole dataset and be capable of fusing long-term information. Finally, the whole network is jointly fine-tuned. All training stages utilize the Adam optimizer with \(_{1}=0.5\) and \(_{2}=0.999\), where the learning rate decays with the cosine annealing scheme. The Charbonnier loss  is applied on the whole frames between the ground truth and the reconstructed frame, formulated as \(L=^{HR}-I_{i}^{SR}||^{2}+^{2}}\). The SeeClear framework is implemented with PyTorch-2.0 and trained across 4 NVIDIA 4090 GPUs, each accommodating 4 video clips.

**Evaluation Metrics** Comparative analysis is conducted among different VSR methods, with the evaluation being anchored on both pixel-based and perception-oriented metrics. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are utilized to evaluate the quantitative performance as pixel-based metrics. All of them are calculated based on the Y-channel, with the exception of the REDS4, for which the RGB-channel is used. On the perceptual side, Learned Perceptual Image Patch Similarity (LPIPS)  is elected for assessment from the perspective of human visual preference. It leverages a VGG model to extract features from the generated HR video and the ground truth, subsequently measuring the extent of similarity between these features.

### Comparisons with State-of-the-Art Methods

We compare SeeClear with several state-of-the-art methods, including regression-based and diffusion-based ones. As shown in Table 3, SeeClear achieves superior perceptual quality compared to regression-based methods despite slightly underperforming in pixel-based metrics. We also provide an extended version, which leverages the generative capability of SeeClear to enhance the features of the regression-based model, akin to references such as [6; 2]. An observable increase in fidelity is accompanied by a notable further improvement in the perceptual metrics of the reconstructed results. Similar performance trends can be noted on Vid4 as those on REDS4. In particular, SeeClear achieves an LPIPS score of 0.1548, marking a relative improvement of 10.8% compared to the top competitor, SATeCo . When pitted against a variant of SATeCo, which is not modulated by LR videos, SeeClear demonstrates a higher PSNR value with a comparable LPIPS score. It suggests that SeeClear benefits from the control of dual semantics, striking a balance between superior fidelity and the generation of realistic textures.

As visualized in Figure 4, SeeClear showcases its ability to restore textures with high fidelity more effectively compared with other methods. Despite large blurriness, SeeClear still demonstrates robust restoration capabilities for video super-resolution, reinforcing the efficacy of utilizing instance-specific and channel-wise semantic priors for video generation control. To further substantiate the temporal coherence acquired by SeeClear, we also visualize two consecutive frames from the generated HR videos, constructed using different diffusion-based VSR methodologies, as depicted in Figure 5. ResShift synthesizes varied visual contents across two frames, such as the fluctuating figures on the license plate. Contrarily, HR frames generated via SeeClear maintain a higher temporal consistency and deliver pleasing textures.

### Ablation Study

To assess the contribution of each component within the proposed SeeClear, we begin with a baseline model and gradually integrate these modules. Specifically, all semantic-related operations are bypassed, retaining solely spatial and channel self-attention and residual blocks, degenerating into a diffusion-based image SR model without any condition. Subsequently, we incrementally introduce the crafted semantic-conditional module into the baseline and formulate several variants. Their results are listed in Table 2 and partially visualized in Figure 6.

   &  &  &  \\   & & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  Bicubic & - & 26.14 & 0.7292 & 0.3519 & 23.78 & 0.6347 & 0.3947 \\ TOFlow  & 7 & 29.98 & 0.7990 & 0.3104 & 25.89 & 0.7651 & 0.3386 \\ EDVR-M  & 5 & 30.53 & 0.8699 & 0.2312 & 27.10 & 0.8186 & 0.2898 \\ BasicVSR  & 15 & 31.42 & 0.8909 & 0.2023 & 27.24 & 0.8251 & 0.2811 \\ VRT  & 6 & 31.60 & 0.8888 & 0.2077 & 27.93 & 0.8425 & 0.2723 \\ IconVSR  & 15 & 31.67 & 0.8948 & 0.1939 & 27.39 & 0.8279 & 0.2739 \\  StableSR  & 1 & 24.79 & 0.6897 & 0.2412 & 22.18 & 0.5904 & 0.3670 \\ ResShift  & 1 & 27.76 & 0.8013 & 0.2346 & 24.75 & 0.7040 & 0.3166 \\ SATeCo  & 6 & 31.62 & 0.8932 & 0.1735 & 27.44 & 0.8420 & 0.2291 \\ SeeClear (Ours) & 5 & 28.92 & 0.8279 & 0.1843 & 25.63 & 0.7605 & 0.2573 \\ SeeClear\({}^{}\) (Ours) & 5 & 31.32 & 0.8856 & 0.1548 & 27.80 & 0.8404 & 0.2054 \\  

Table 1: Performance comparisons in terms of pixel-based (PSNR and SSIM) and perception-oriented (LPIPS) evaluation metrics on the REDS4  and Vid4  datasets. The extended version of SeeClear is marked with \(\). Red indicates the best, and blue indicates the runner-up performance (best view in color) in each group of experiments.

Figure 4: Qualitative results on the REDS4 and Vid4 datasets. SeeClear generates clearer content and sharper textures.

First, the intra-frame semantic condition brings about 1.5% improvements in LPIPS. Albeit the multi-frame self-attention further improves the perceptual quality, it also impairs the fidelity of the restored video. Under the control of InCAM, SeeClear can correlate semantically consistent pixels in adjacent frames by combining intra-frame and inter-frame semantic priors, elevating the PSNR from 27.99 dB to 28.46 dB, and bringing about 6.6% improvements in LPIPS. Furthermore, upon integrating the semantic priors from CaTeGory, the fully-fledged SeeClear notably enhances both the pixel-based and perception-oriented metrics simultaneously. It indicates that the cooperative control of semantics is more beneficial for generating videos of higher fidelity and better perceptual quality. As illustrated in Figure 6, the baseline struggles to restore tiny and fine patterns without semantic condition, and it gradually gains improvement accompanied by the strengthening of semantic control.

## 5 Conclusion

In this work, we present a novel diffusion-based video super-resolution framework named SeeClear. It formulates the diffusion process by incorporating residual shifting mechanism and patch-level blurring, constructing a Markov chain initiated with high-resolution frames and terminated at low-resolution frames. It employs a semantic distiller and a pixel condenser for super-resolution during the inverse sampling process. The instance-centric semantics distilled by the semantic distiller prompts spatial modulation and temporal association in the devised Instance-Centric Alignment Module. They are further assembled into Channel-wise Texture Aggregation Memory, providing abundant conditions for temporal coherence and realistic content.

**Acknowledgment.** This work is supported in part by the National Natural Science Foundation of China under Grant 62120106009 and Grant 62372036; and in part by the National Key Research and Development Program of China under Grant 2022ZD0118001 and Grant 2021ZD0112100.

   & Baseline & DWT & Semantic & MFSA & InCAM & CaTeGory & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\ 
1 & ✓ & ✓ & & & & & 28.05 & 0.7993 & 0.2120 \\
2 & ✓ & ✓ & ✓ & & & & 28.08 & 0.7998 & 0.2088 \\ 
3 & ✓ & ✓ & ✓ & ✓ & & & 27.99 & 0.7961 & 0.2053 \\
4 & ✓ & ✓ & ✓ & ✓ & ✓ & & 28.46 & 0.8098 & 0.1917 \\
5 & ✓ & ✓ & ✓ & & & ✓ & 28.21 & 0.7986 & 0.2149 \\ 
6 & ✓ & & ✓ & ✓ & ✓ & ✓ & 28.74 & 0.8267 & 0.1938 \\
7 & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 28.92 & 0.8279 & 0.1843 \\  

Table 2: Performance comparisons on REDS4 among variants with different semantic-condition control by integrating InCAM and CaTeGory.

Figure 5: Qualitative comparison of regions between consecutive frames. (a) and (b) are patches produced by ResShift , derived from Frames 76 and 77 respectively. (c) and (d) display the corresponding regions as generated through SeeClear.

Figure 6: Visual comparisons of ablation for investigating the contribution of key modules.