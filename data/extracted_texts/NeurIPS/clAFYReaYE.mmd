# Fractal Patterns May Illuminate the Success of Next-Token Prediction

Ibrahim Alabdulmohsin

Google Deepmind

Zurich, Switzerland

ibomohsin@google.com

&Vinh Q. Tran

Google Deepmind

New York, USA

vqtran@google.com

&Mostafa Dehghani

Google Deepmind

Mountain View, USA

dehghani@google.com

Corresponding author.

###### Abstract

We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) _self-similar_, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) _long-range dependent_ (LRD), with a Hurst parameter of approximately \(=0.70 0.09\). Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust. Finally, we demonstrate that the tiny variations in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs.

## 1 Introduction

How does the training objective of predicting the next token in large language models (LLMs) yield remarkable capabilities? Consider, for instance, the two models: Gemini  and GPT4 . These models have demonstrated capabilities that extend to quantitative reasoning, summarization, and even coding, which has led some researchers to ponder if there was more to intelligence than "on-the-fly improvisation" . While providing a satisfactory explanation is a difficult endeavor, a possible insight can be drawn from fractals and self-similarity. We elucidate the connection in this work.

**Self-Similarity.** Self-similar processes were introduced by Kolmogorov in 1940 . The notion garnered considerable attention during the late 1960s, thanks to the extensive works of Mandelbrot and his peers . Broadly speaking, an object is called "self-similar" if it is invariant across scales, meaning its statistical or geometric properties stay consistent irrespective of the magnification applied to it (see Figure 1). Nature and geometry furnish us with many such patterns, such as coastlines, snowflakes, the Cantor set and the Kuch curve. Despite the distinction, self-similarity is often discussed in the context of "fractals," another term popularized by Mandelbrot in his seminal book _The Fractal Geometry of Nature_. However, the two concepts are different . See Section 2.

In language, in particular, there have been studies arguing for the presence of a self-similar structure. Nevertheless, due to computational constraints, it was not feasible to holistically model the joint probability distribution of language. As such, linguists often resorted to rudimentary approximations in their arguments, such as by substituting a word with its frequency or length , or by focusing onthe recurrence of a specific, predetermined word . These studies fall short of fully capturing the structure of language due to the simplifying assumptions they make, as discussed in Section 4.

Highlighting the self-similar nature of a process can have profound implications. For instance, conventional Poisson models for Ethernet traffic were shown to fail because traffic was self-similar . In such cases, recognizing and quantifying self-similarity had practical applications, such as in the design of buffers . Similarly in language, we argue that self-similarity may offer a fresh perspective on the mechanisms underlying the success of LLMs. Consider the illustrative example shown in Figure 1, where the task is to predict the subsequent measurement in a time series, specifically predicting next tokens in a Wikipedia article (see Section 2 for details). The three plots in Figure 1 (left) represent different manifestations of the same process observed across three distinct time scales. Notably, we observe rich, self-similar details, such as burstiness, in _all_ of them. A well-established approach for quantifying self-similarity is the Holder exponent , which we denote by \(\). In language, we find it to be \(=0.59 0.08\), confirming statistical self-similarity.

Why is this important? We hypothesize that since LLMs are trained to predict the future of a self-similar process, they develop proficiency in capturing patterns across multiple levels of granularity for two interconnected reasons. First, self-similarity implies that the patterns at the level of a paragraph are reflective of the patterns seen at the level of a whole text, which is reminiscent of the _recursive_ structure of language . Thus, recognizing short-term patterns can aide in learning broader contexts. Second, because language displays intricate patterns at all levels of granularity, it would not be enough to rely only on the immediate context of a sentence to predict the next token. Instead, the model needs to identify patterns at higher levels of granularity; e.g. follow the direction of the argument and the broader intent. It must balance between short- and long-term contexts. Willinger et al.  and Altmann et al.  argue for self-similarity in language due to this hierarchical nature.

**Long-range dependence.** However, self-similarity alone is not sufficient for a predictive model to exhibit anything resembling "intelligent" behavior. In fact, some self-similar processes, despite their intricate details, remain entirely unpredictable. A quintessential example is the simple Brownian motion, which is a Wiener process with independent increments. Its discrete analog is \(B_{n}=_{i=1}^{n}_{i}\), where \(_{i}(0,^{2})\). Despite possessing rich details at all granularities, a model trained to predict \(B_{n}\) cannot learn anything useful from data since the process itself has _independent_ increments.

Thus, for strong capabilities to emerge, the process must have some degree of predictability or dependence as well. One classical metric for quantifying predictability in a stochastic process is the Hurst parameter , developed by the hydrologist H. E. Hurst in 1951 while studying the Nile river. It is generally considered to be a robust metric , unlike the wavelet estimator  and the periodogram method  that can be sensitive to errors . As discussed in Section 2.3, we find the Hurst parameter in language to be \(=0.70 0.09\). For context, \(\) only takes values in \(\). A value \(>0.5\) implies predictability in the data, while \(=0.5\) indicates random increments.

While it is compelling that our estimate of \(\) in language lies nearly _midway_ between predictability (\(=1\)) and noise (\(=0.5\)), a Hurst parameter of about \(0.75\) turns out to occur commonly in nature, including in river discharges, Ethernet traffic, temperatures, precipitation, and tree rings . For agents that learn from data, such as LLMs, this value is also reminiscent of processing-based

Figure 1: Manifestations of processes across different time scales. A region marked in red corresponds to the magnified plot beneath it. left: The process exhibits self-similarity with rich details at all levels of granularity. It is an integral process \((X_{t})_{t}\) calculated from Wikipedia (see Section 2). right: Example of a process that is not self-similar, looking smoother at larger time scales.

theories of curiosity, which suggest that a sweet spot of complexity exists (not too simple, nor too unpredictable) that facilities or accelerates learning .

Importantly, predictability and self-similarity _together_ imply long-range dependence (LRD). This follows from the definition of self-similarity, where the patterns at small scales mirror those at larger scales so, for example, the correlations established at micro levels are also pertinent at macro levels. LRD is arguably crucial for enhancing the functionality of predictive models because processes with only short-range dependence could be forecasted (somewhat trivially) with lookup tables that provide the likelihood of transitions over brief sequences. By contrast, this is not possible in LRD processes whose contexts extend indefinitely into the past.

Information Theoretic Complexity.To define fractal parameters, we follow recent works such as  in adopting an _information-theoretic_ characterization of the complexity in language using minimal-length codes or surprise. This corresponds to an intrinsic, _irreducible_ description of language and the minimum compute overhead to comprehend/decode it , which also correlates well with actual reading times . In this context, self-similarity means that the intrinsic complexity or surprise in language (measured in bits) cannot be smoothed out, even as we look into broader narratives. That is, surprising paragraphs will follow predictable paragraphs, in a manner that is statistically similar to how surprising sentences follow predictable sentences.

Analysis.How robust are these findings? To answer this question, we carry out an extensive empirical analysis across various model architectures and scales, ranging from 1B to over 500B parameters. We find that fractal parameters are quite robust to the choice of the architecture.

However, there exists _tiny_ variations across LLMs. Interestingly, we demonstrate that from a practical standpoint, these differences help in predicting downstream performance in LLMs compared to using perplexity-based metrics alone, such as bits-per-byte (BPB). Specifically, we introduce a new metric and show that using it to predict downstream performance can increase the adjusted \(R^{2}\) from approximately \(0.65\) when using solely BPB, to over \(0.86\) with the new metric2.

**Statement of Contribution.** In summary, we:

1. highlight how the fractal structure of language can offer a new perspective on the capabilities of LLMs, and provide a formalism to quantify properties, such as long-range dependence.
2. establish that language is self-similar and long-range dependent. We provide concrete estimates in language of the three parameters: the self-similarity (Holder) exponent, the Hurst parameter, and the fractal dimension. We also estimate the related Joseph exponent.
3. carry out a comparative study across different model architectures and scales, and different domains, such as ArXiv and GitHub, demonstrating that fractal parameters are robust.
4. connect fractal patterns with learning. Notably, we show that a "median" Hurst exponent improves upon perplexity-based bits-per-byte (BPB) in predicting downstream performance.

## 2 Fractal Structure of Language

### Preliminaries

Suppose we have a discrete-time, stationary stochastic process \((x_{t})_{t}\), with \([x_{t}]=0\) and \([x_{t}^{2}]=1\). We will refer to \((x_{t})_{t}\) as the _increment process_ to distinguish it from the _integral process_\((X_{t})_{t}\) defined by \(X_{t}=_{k=0}^{t}x_{k}\). While \((x_{t})_{t}\) and \((X_{t})_{t}\) are merely different representations of the same data, it is useful to keep both representations in mind. For example, self-similarity is typically studied in the context of integral processes whereas LRD is defined on increment processes.

In the literature, it is not uncommon to mistakenly equate parameters that are generally different. For example, the Hurst parameter H has had many definitions in the past that were not equivalent, and Mandelbrot himself cautioned against this . The reason behind this is because different parameters can agree in the idealized fractional Brownian motion, leading some researchers to equate them in general . We will keep the self-similarity exponent \(\) and \(\) separate in our discussion.

Experimental Setup.In order to establish self-similarity and LRD in language, we convert texts into sequences of bits using a large language model (LLM). Specifically, we use PaLM2-L (Unicorn)  to calculate the probability of the next token \(w_{t}\) conditioned on its entire prefix \(w_{[t-1]}=(w_{0},w_{1},,w_{t-1})\). As discussed in Section 1, this captures its intrinsic, irreducible description . By the chain rule , the corresponding number of bits assigned to \(w_{t}\) is \(z_{t}=- p(w_{t}|w_{[t-1]})\). Unlike in prior works, which rely on simplifications such as by substituting a word with its length  or by focusing on the recurrence of a single word , we use the LLM to approximate the full joint distribution of language since LLMs are known to produce calibrated probability scores at the token level . We carry out these calculations for prefixes of up to 2048 tokens (\( 8\) pages of text). With a suitable normalization, such as bits-per-byte (BPB), one obtains a standardized description of text, consistent across tokenizers. BPB is widely used as a tokenizer-agnostic metric to compare LM modeling performance, e.g. for The Pile .

Besides PaLM2, we also experiment and report on various model sizes of PaLM  and decoder-only T5 . Namely, we report results for models: PaLM2 XXS (Gecko), XS (Otter), S (Bison), M, and L (Unicorn); PaLM 8B, 62B, 540B; and decoder-only T5.1.1 at Base (110M), Large (341M), XL (1.2B), and XXL (5B) sizes. For PaLM and PaLM2, we use the checkpoints pretrained in Chowdhery et al.  and Anil et al. . All T5.1.1 decoder baselines, on the other hand, are trained with a casual language modeling objective for 262B tokens of C4 . All experiments are executed on Tensor Processing Units (TPUs). More details on how we train T5.1.1 baselines are in Appendix A.

Once \(z_{t}\) is computed for a document, we follow standard definitions in constructing the increment process \((x_{t})_{t}\) by normalizing \(z_{t}\) to have a zero-mean and unit variance. Intuitively, fractal parameters are intended to measure a fundamental property of the process (e.g. LRD) that should not be affected by scale, hence the normalization. The integral process \((X_{t})_{t}\) is calculated based on \((x_{t})_{t}\), as described earlier and depicted in Figure 1 (top). Normalizing bits (to have zero mean and unit variance) models language as a random walk. It is a standard approach used extensively in the literature in various contexts, such as in DNA sequences .

Figure 3: Rescaled range \(R(n)/S(n)\) is plotted against the number of normalized bits \(n\). We observe a power law \(R(n)/S(n) n^{}\) in all domains. When aggregating all datasets, \(=0.70 0.09\).

Figure 2: Peak probability \(p_{e}()\) is plotted against the granularity level \(\) (see Section 2.2). We observe power laws \(p_{e}()^{-}\), indicating self-similarity, with a median exponent of \(=0.59 0.08\).

For analysis, we use The Pile validation split , consisting of 22 subdomains such as Wikipedia and GitHub. We restrict analysis to sufficiently-long documents of length \(>4K\) tokens and use the first 2K tokens only, to sidestep potential effects of the finite length of documents and the model context. To mitigate noise, only domains with \(>1K\) documents are compared; we report results for them separately and their median. We use bootstrapping  to estimate the error margin.

**Notation.** We write \(f(x) x^{c}\) if \(f(x)=x^{c}L(x)\) for some function \(L\) that satisfies \(L(tx)/L(x) 1\) as \(x\) for all \(t>0\). Examples of slowly varying functions are constants \(L(x)=c\) and \(L(x)= x\). When \(f(x) x^{c}\), we abuse terminology slightly by referring to \(f(x)\) as a power law.

### Self-similarity exponent -- Scale invariance

An integral process is said to be self-similar if it exhibits _statistical_ self-similarity. More precisely, \((X_{t})_{t}\) is self-similar if \((X_{ t})_{t}\) is distributionally equivalent to \((^{S}X_{t})_{t}\) for some exponent \(\). Thus, scaling of time is equivalent to an appropriate scaling of space. We will refer to \(\) as the _granularity level_ and to the exponent \(\) as the self-similarity or Holder exponent . Many time series in nature exhibit self-similar structures, such as human blood pressure and heart rate .

One approach for calculating \(\) is as follows. Fix \( 1\) and denote the \(\)-increments by \((X_{t+}-X_{t})_{t}\). These would correspond, for instance, to the number of bits used for clauses, sentences, paragraphs and longer texts as \(\) increases. In terms of the increment process \((x_{t})_{t}\), this corresponds to aggregating increments into "bursts". Let \(p_{}()\) be the probability mass of the event \(\{|X_{t+}-X_{t}|\}_{t}\). Then, \(\) can be estimated by fitting a power law relation \(p_{}()^{-S}\). Generally, \(\) is robust to the choice of \([10^{-3},10^{-2}]\) as shown in Figure 4 (left) so we fix it to \(=5 10^{-3}\).

Figure 2 plots the probability \(p_{}()\) against \(\) using PaLM2-L. We indeed observe a power law relation over at least two orders of magnitude; i.e. linear in a log-log scale, with a median self-similarity exponent of \(=0.59 0.08\). Section 3 shows that the median \(\) is robust to the choice of the LLM.

### Hurst parameter -- Long-range dependence

The Hurst parameter \(\) quantifies the degree of predictability or dependence over time . It is calculated using the so-called rescaled-range (R/S) analysis. Let \((x_{t})_{t}\) be an increment process. For each \(n\), write \(y_{t}=x_{t}-_{k=0}^{t}x_{k}\) and \(Y_{t}=_{k=0}^{t}y_{t}\). The range and scale are defined, respectively, as \(R(n)=_{t n}Y_{t}-_{t n}Y_{t}\) and \(S(n)=(\{x_{k}\}_{k n})\), where \(\) is the standard deviation. Then, the Hurst parameter \(\) is estimated by fitting a power law relation \(R(n)/S(n) n^{}\). As stated earlier, for completely random processes, such as a simple Brownian motion, it can be shown that \(=1/2\). In addition, \(H>1/2\) implies dependence over time .

Writing \(_{n}=[(x_{t+n}x_{t}]\) for the autocovariance function of the increment process \((x_{t})_{t}\), the Hurst parameter satisfies \(=1-/2\) when \(_{n} n^{-}\) as \(n\). Since in self-similar processes, \(>1/2\) implies long-range dependence (LRD), LRD is equivalent to the condition that the autocovariances are not summable. In terms of the integral process, it can be shown that : \(_{n}(X_{n})}{n}=1+2_{i=1}^{}_{i}\). Hence, if \(<1/2\), the auto-covariances are summable and

Figure 4: left: Estimates of the self-similarity exponent \(\) are generally robust to the choice of \(\). right: The partial auto-correlation function calculated across domains. DM Mathematics has a much shorter dependence compared to the rest of the domains, in agreement with its Hurst parameter.

\((X_{n})\) grows, at most, linearly fast on \(n\). On the other hand, if the process has LRD, \((X_{n})\) grows superlinearly on \(n\). In particular, using the Euler-Maclaurin summation formula [7; 2], one obtains \((X_{n}) n^{2H}\) if \(H>1/2\). Figure 3 plots the rescaled range \(R(n)/S(n)\) against \(n\). We observe a power law relation with a median Hurst parameter of \(=0.70 0.09\).

### Fractal dimension -- Complexity at all levels

Broadly speaking, the fractal dimension of an object describes its _local_ complexity. For a geometric object \(Z\), such as the Koch curve, let \(\) be a chosen scale (e.g. a short ruler for measuring lengths or a small square for areas). Let \(N()\) be the minimum number of objects of scale \(\) that cover \(Z\); i.e. contain it entirely. Then, the fractal dimension of \(Z\), also called its Hausdorff dimension, is: \(=-_{ 0}\{\}\). For example, a line has a fractal dimension \(1\), in agreement with its topological dimension, because \(N()=C/\) for some constant \(C>0\).

By convention, an object is referred to as "fractal" if \(\) is different from its topological dimension. For example, the fractal dimension of the Koch curve is about 1.26 when its topological dimension is 1. Fractals explain some puzzling observations, such as why estimates of the length of the coast of Britain varied significantly from one study to another, because lengths in fractals are scale-sensitive. Mandelbrot estimated the fractal dimension of the coast of Britain to be 1.25 .

The definition above for the fractal dimension \(\) applies to geometric shapes, but an analogous definition has been introduced for stochastic processes. Let \((x_{t})_{t}\) be a stationary process with autocovariance \(_{n}\). Then, its fractal dimension \(\) is determined according to the local behavior of \(_{n}\) at the vicinity of \(n=0\), by first normalizing \((x_{t})_{t}\) to have a zero-mean and a unit variance, and modeling \(_{n}\) using a power law \(_{n} 1-n^{}\) as \(n 0^{+}\), for \((0,2]\). Then, the fractal dimension \([1,\,2]\) of \((x_{t})_{t}\) is defined by \(=2-/2\). It can be shown that \(=2-\). For language, this gives a median fractal dimension of \(=1.41 0.08\).

### Joseph effect -- Burstiness

Finally, we examine another related parameter that is commonly studied in self-similar processes. The motivation behind it comes from the fact that in processes with LRD, one often observes _burstiness_ as shown in Figure 1; i.e. clusters over time in which the process fully resides on one side of the mean, before switching to the other. This is quite unlike random noise, for instance, where measurements are evenly distributed on both sides of the mean. The effect is often referred to as the Joseph effect, named after the biblical story of the seven fat years and seven lean years [68; 46; 66].

A common way to quantify the Joseph effect for integral processes \((X_{t})_{t}\) is as follows . First, let \(_{}\) be the standard deviation of the \(\)-increments \(X_{t+}-X_{t}\). Then, fit a power law relation \(_{}^{}\). The exponent \(\) here is called the Joseph exponent. In an idealized fractional Brownian motion, both \(\) and the self-similarity exponent \(\) coincide. Figure 5 provides the detailed empirical results. Overall, we find that \(=0.49 0.08\).

## 3 Analysis

Comparative Analysis.Table 1 compares fractal parameters across different domains, such as ArXiv, Github and Wikipedia. In general, most domains share similar self-similarity and Hurst exponents with a few exceptions. The first notable exception is DM-Mathematics, which has a Hurst parameter of about 0.5, indicating a lack of LRD. Upon closer inspection, however, a value of \(=0.5\)

   & **OpenWeb** & **GitHub** & **FreeLaw** & **PileCC** & **Wiki** & **PubMed** & **Math** & **ArXiv** \\   \(\) & \(0.53.05\) & \(0.60.05\) & \(0.61.05\) & \(0.56.03\) & \(0.62.02\) & \(0.60.07\) & \(0.42.03\) & \(0.70.03\) \\ \(\) & \(0.68.01\) & \(0.79.01\) & \(0.68.00\) & \(0.70.00\) & \(0.74.01\) & \(0.65.00\) & \(0.50.01\) & \(0.72.01\) \\ \(\) & \(0.46.01\) & \(0.49.00\) & \(0.49.00\) & \(0.50.00\) & \(0.52.00\) & \(0.44.00\) & \(0.28.00\) & \(0.49.00\) \\  

Table 1: A comparison of the fractal parameters across 8 different domains with \(>1000\) documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

of the document's length . However, both Zipf's and Heap's laws are invariant to the semantic ordering of text, so they do not capture important aspects, such as long-range dependence (LRD) .

In terms of self-similarity in language, the Menzerath-Altmann law stipulates a self-similar behavior in the following sense: when the size of a language construct increases, the size of its constituents decreases, and this happens at all scales [49; 4]. In Ausloos , the authors model texts as a time series by replacing a word with its length. After that, they study the fractal behavior of language. However, as mentioned in , replacing a word with its length is invalid because it is not translation-independent (i.e. one could map every word to an arbitrary token, including tokens of equal length). In our work, we model language as a series of bits calculated from conditional entropies, reflecting the intrinsic structure of the language itself, inspired by findings in linguistics such as [28; 22; 41]. The existence of self-similarity in language is attributed to its hierarchical nature [68; 3], such as duality of patterning .

In Najafi and Darooneh , the authors define a fractal dimension for each word. Informally, they examine the recurrence of a single, predetermined word as a binary series, similar to the approach used in Altmann et al. . However, this only applies to individual words and cannot model higher-level clauses. For instance, it does not distinguish between "time" in the phrase "once upon a time" and "time" in "space and time." Kokol and Podgorelec  estimate LRD in natural language, and suggest that its LRD is close to that of pure noise! They conjecture this was due to their use of ASCII encoding. In computer languages, they observe LRD and suggest it is because they are formal.

Besides the above concerns in prior studies that examined the self-similar structure in language, another concern is that they sometimes give extremely large values of the fractal dimension, sometimes exceeding 10 ! Such values are difficult to interpret because the fractal dimension \(\) should fall in \(\) for time series. We do not observe such issues in our analysis. In our case, \(=1.41 0.08\).

Limitations and Future Research.Our analysis is currently limited to the English language so it may not apply to other languages that differ significantly. For instance, some languages such as Piraha (spoken in the Amazon) do not have a recursive structure like most languages do . We also do not model the semantic or lexical form of language. While our information-theoretic approach is well-founded and captures the intrinsic complexity of language, it does not account for the semantic nuances that contribute to meaning. Thirdly, self-similarity may explain why parameter sharing, such as in ALBERT , can be successful but exploiting self-similarity more directly in LLMs could lead to further optimizations. Exploring these aspects are promising directions for future research.

## 5 Concluding Remarks

In this work, we highlight intriguing insights into the underlying fractal structure of language and how it may be interconnected with the remarkable capabilities of LLMs. Our formalism quantifies properties of language that may have been suspected, but not previously formally shown. In particular, the need in LLMs to balance between short- and long-term contexts is reflected in the self-similar

Figure 7: Downstream metric, indicated by bubble size where larger is better, is plotted vs. the median Hurst and the median BPB for all 12 language models.

structure of language, while long-range dependence is quantifiable using the Hurst parameter. For instance, the absence of LRD in DM-Mathematics is reflected in its Hurst parameter of \( 0.5\). Interestingly, the estimated median Hurst value of \(=0.70 0.09\) in language reflects an intriguing balance between predictability and noise that is similar to many other phenomena, and combining both H with BPP together yields a stronger predictor of downstream performance. We carry out an extensive comparative analysis across different domains and model architectures, revealing that fractal parameters are generally robust. We hope that future research can further probe into these fractal properties, unearthing deeper understandings of the relation between intelligence and language.