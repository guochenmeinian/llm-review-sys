# Fed-CO\({}_{2}\): Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning

Fed-CO\({}_{2}\): Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning

Zhongyi Cai

ShanghaiTech University

caizhy@shanghaitech.edu.cn

&Ye Shi

ShanghaiTech University

shiye@shanghaitech.edu.cn

&Wei Huang

RIKEN Center for Advanced Intelligence Project

wei.huang.vr@riken.jp

&Jingya Wang

ShanghaiTech University

wangjingya@shanghaitech.edu.cn

Corresponding author

###### Abstract

Federated Learning (FL) has emerged as a promising distributed learning paradigm that enables multiple clients to learn a global model collaboratively without sharing their private data. However, the effectiveness of FL is highly dependent on the quality of the data that is being used for training. In particular, data heterogeneity issues, such as label distribution skew and feature skew, can significantly impact the performance of FL. Previous studies in FL have primarily focused on addressing label distribution skew data heterogeneity, while only a few recent works have made initial progress in tackling feature skew issues. Notably, these two forms of data heterogeneity have been studied separately and have not been well explored within a unified FL framework. To address this gap, we propose Fed-CO\({}_{2}\), a universal FL framework that handles both label distribution skew and feature skew within a **C**ooperation mechanism between the **O**nline and **O**ffline models. Specifically, the online model learns general knowledge that is shared among all clients, while the offline model is trained locally to learn the specialized knowledge of each individual client. To further enhance model cooperation in the presence of feature shifts, we design an intra-client knowledge transfer mechanism that reinforces mutual learning between the online and offline models, and an inter-client knowledge transfer mechanism to increase the models' domain generalization ability. Extensive experiments show that our Fed-CO\({}_{2}\) outperforms a wide range of existing personalized federated learning algorithms in terms of handling label distribution skew and feature skew, both individually and collectively. The empirical results are supported by our convergence analyses in a simplified setting.

## 1 Introduction

Federated Learning (FL) [1; 2] is a distributed learning framework that involves collaboratively training a global model with multiple clients while ensuring privacy protection. The pioneering work FedAvg  learns the global model by aggregating the local client models and obtains satisfactory performance when the client data are independently and identically distributed (IID). However, when the data are heterogeneous and non-IID among clients, performance with the global model can degrade substantially [3; 4]. Common data heterogeneity issues include label distribution skew and feature skew. Clients experiencing label distribution skew exhibit varying class distributions within the same domain, while clients encountering feature skew maintain consistent class distributionsbut belong to different domains. As a promising solution to data heterogeneity issues, Personalized Federated Learning (PFL) has emerged, where personalized models are trained for individual clients. Most previous studies have focused on addressing label distribution skew [5; 6; 7], while only a few recent works [8; 9] have made initial progress in tackling feature shift issues. So far, these two forms of data heterogeneity have been studied separately and have not been addressed within a unified FL framework. Therefore, in this paper, we propose a universal FL framework that effectively handles data heterogeneity issues arising from label distribution skew, feature skew, or their combination.

Several algorithms personalized some parts of the model in FL with label distribution imbalance or feature shifts to retain and leverage some of the local offline information [10; 11; 12; 13; 8]. However, in realistic FL scenarios, where extreme label distribution skew, severe feature skew, or even both are present, these algorithms fail to effectively harness local specialized knowledge for satisfactory adaptation. In fact, in some cases of extreme heterogeneity, models trained by those personalized algorithms may even perform worse than the locally-trained model, as the locally-trained model excels in capturing offline specialized knowledge. On the other hand, in FL scenarios with milder heterogeneity, partially personalized models trained by PFL algorithms perform better due to their ability to access online general information from other clients.

So, given that the model with partially personalized parameters and the locally trained model each perform better in different cases, the question is: Is there a more effective approach to fuse the online general knowledge and the offline specialized knowledge for better performance? From our investigations, the answer is yes. Accordingly, we propose a novel universal cooperation framework with these two models to address this challenge for both label distribution skew and feature skew data heterogeneity, referring to the model with partially personalized parameters as the online model, and the locally trained model as the offline model. Specifically, we personalize Batch Normalization layers in the online model and fuse the online and offline models' predictions as the final prediction. The prediction fusion between the online and offline models rectifies errors in their respective individual predictions and exhibits better performance.

In FL scenarios with feature skew, the general knowledge learned by the online model is domain-invariant, whereas the specialized knowledge learned by the offline model is domain-specific. Cooperation that occurs solely by fusing predictions is not sufficient as this process does not let the online and offline models communicate during the training phase. In turn, this impedes the transfer of domain-invariant and domain-specific knowledge between the models. Hence, to further encourage cooperation between the models, we propose two novel knowledge transfer mechanisms - one intra-client and one inter-client - which work at the model and client level, respectively. The intra-client knowledge transfer mechanism facilitates mutual learning between the online and offline models via knowledge distillation, which enables the online and offline models to benefit from both online domain-invariant and offline domain-specific knowledge. Conversely, the inter-client knowledge transfer mechanism enhances the model's domain generalization ability by introducing classifiers from the offline models of other clients to each local client.

**Contribution.** We propose Fed-CO\({}_{2}\), a universal cooperative FL framework for severe data heterogeneity including both label distribution skew and feature skew. By simply fusing the online and offline models, Fed-CO\({}_{2}\) can handle severe label distribution skew effectively. To enhance model performance in the presence of severe feature skew, Fed-CO\({}_{2}\) involves an intra-client knowledge transfer mechanism that improves model cooperation and an inter-client knowledge transfer mechanism that increases client cooperation. We theoretically show that Fed-CO\({}_{2}\) has a faster convergence rate than FedBN  in a simplified setting. Besides, extensive experiments on five benchmark datasets demonstrate that our Fed-CO\({}_{2}\) framework has a prominent edge over a range of state-of-the-art algorithms where the data contain label distribution skew, feature skew, or both1.

## 2 Related Work

**Federated Learning for Label Distribution Skew Data Heterogeneity.** In real-world FL scenarios, label distribution imbalance among clients is a common phenomenon that poses challenges to learning a single model that can effectively cater to all clients. A straightforward approach to personalizing the global model is fine-tuning it on local datasets [14; 15; 16; 17]. Other approaches attempt to overcome distribution heterogeneity by exerting a proximal regularization term on the global model,as seen in examples like FedProx , pFedMe , MOON , and Ditto . Instead of adapting the single global model, learning personalized models is another main track. Here, the more explicit methods [11; 20; 21; 22] personalize some of the model parameters while leaving other parameters for aggregation. Similarly, FedMask  learns distinct binary masks for the last several layers of the local models. Further, inspired by Hypernetworks , some methods [6; 13; 25] apply a hypernetwork to produce client-specific parameters for participants. Beyond directly personalizing parts of the model parameters, some of the newer methods involve a two-branch architecture where the aim is to calibrate the model's predictions [12; 26; 27]. For example, FedRoD  uses a personal head to strengthen local learning, while PerFCL  splits the features in the local clients into shared parts and personalized parts. FedProc  uses prototypes as global knowledge to correct local training. However, unlike these methods, Fed-CO\({}_{2}\) maintains two separate personalized models for each client - one that learns online general knowledge and the other that learns offline specialized knowledge - with a focus on facilitating cooperation between the two models.

Additionally, knowledge distillation, which transfers dark knowledge between models, has played an important role in addressing label distribution skew [28; 29; 30; 31; 32; 22]. Most of previous works in this arena rely heavily on a public dataset to transfer knowledge between the server and the local clients [29; 30; 31]. Only a limited number of studies have delved into the synergistic interplay between learning global knowledge and learning local knowledge, as well as strategies to optimize the mutual benefits of these two processes. In FML , the global model and the local model engage in mutual learning, but only the local model is used for personalized predictions. A very recent study, CD\({}^{2}\)-pFed , employs cyclical distillation as a regularization term in conjunction with the local training cross-entropy loss to mitigate the gaps between the learned representations from local weights and those from global weights. By contrast, our method utilizes knowledge distillation to facilitate the exchange of beneficial knowledge between the online and offline models before local training, as well as to ensure effective and adequate intra-client knowledge transfer.

**Federated Learning for Feature Skew Data Heterogeneity.** While various methods have shown promising results on label distribution data heterogeneity, they often suffer from significant performance degradation when confronted with the more challenging feature shift data heterogeneity. To adapt to the feature shifts among different domains, FedBN  personalizes Batch Normalization (BN) layers for each participant. Beyond personalizing BN layers, PartialFed  explores the strategy of selecting personalized parameters based on the feature characteristics of different clients. To handle the feature skew, FedAP  learns the similarities between participants by analyzing their private parameters in the BN layers. In a recent work called TrFedDis , the concept of feature disentangling is used to capture both domain-invariant and domain-specific knowledge. Our Fed-CO\({}_{2}\) deviates from such disentangling approaches and instead focuses on facilitating mutual learning between the online and offline models.

## 3 Method

In this section, we begin by formulating the research problem and subsequently introduce Fed-CO\({}_{2}\), our novel universal FL framework. Finally, we will present the intra-client and inter-client knowledge transfer mechanisms designed to further enhance model cooperation in FL with feature skew.

### Problem Formulation

We aim to train a set of \(N\) models for each client to fit local data distribution. Each client \(i\) has its own data distribution \(P_{i}\) with its private dataset \(D_{i}=\{(x_{i}^{k}\,,\,y_{i}^{k}):k\{1,,m_{i}\}\}\), \(i\{1,,N\}\) and \(m_{i}\) is the sample number in the private dataset. Due to label distribution skew or feature skew, the data distribution \(P_{i}(x\,,\,y)\) differs on a client-by-client basis. Our goal is to learn a good model \(F_{i}()\) with parameters \(_{i}\) for each client according to information in all clients:

\[_{\{_{i}\}_{i=1}^{N}}_{i=1}^{N} _{j=1}^{m_{i}}l(F_{i}(_{i}\,;\,x_{i}^{j})\,,\,y_{i}^{j}),\] (1)

where \(l(\,,\,)\) is a sample-wise loss function, \(F_{i}\) is constructed by a feature extractor \(f_{i}()\) with parameters \(_{i}\) and a classifier \(C_{i}()\) with parameters \(_{i}\). Accordingly, \(_{i}=\{_{i},\,_{i}\}\).

### Cooperation of Online and Offline Models

As shown in Fig. 1(a), we propose Fed-CO\({}_{2}\): a novel universal collaboration framework between the online and offline models to adapt to local data distribution in the presence of label distribution skew, feature skew, or both. Specifically, for each client \(i\), we train a partially personalized model, referred to as the online model \(F_{i}^{}\), and a locally-trained model, referred to as the offline model \(F_{i}^{}\). With two models in each client \(F_{i}=\{F_{i}^{},\,F_{i}^{}\}\), cooperation is achieved through prediction fusion of the online and offline models. In this manner, our framework aims to exploit both online general and offline specialized knowledge, enabling consistent high performance in FL across various cases of local data heterogeneity.

For the online model, we personalize only a few critical parameters to enable the online model to learn knowledge from other clients through model aggregation on the server, while still retaining essential local knowledge. Prior work FedBN  has shown that Batch Normalization (BN) layers can capture feature distribution in local clients. Inspired by such discovery, we personalize the BN layers in the online model and split its parameters \(_{i}^{}\) into \(\{_{p,i}^{},\,_{g}^{}\}\), where \(_{p,i}^{}\) denotes all BN layer parameters and \(_{g}^{}\) denotes other parameters. Model aggregation is done on the server to obtain the shared \(_{g}^{}\) among all clients with the formula:

\[_{g}^{}=_{i=1}^{N} _{g,i}^{}.\] (2)

For the offline model, all its parameters are personalized and do not engage in server aggregation. As a result, the offline model is able to learn local specialized knowledge without forgetting or being contaminated by other irrelevant information. With online and offline models, we employ a simple fusion technique and obtain the final prediction for model \(F_{i}\):

\[F_{i}(_{i}\,;\,x)=F_{i}^{}(_{p,i}^{}\,,\, _{g}^{}\,;\,x)+F_{i}^{}(_{i}^{} \,;\,x).\] (3)

Figure 1: The Fed-CO\({}_{2}\) framework. Fig. 1(a) shows an overview of the cooperation between the online and offline models in Fed-CO\({}_{2}\). Figures 1(b) and 1(c) respectively illustrate the intra-client and inter-client knowledge transfer mechanisms on FL with feature skew.

It's worth noting that the online-offline model cooperation does not require extra communication costs compared to standard federated aggregation operations since offline models are not uploaded to the server.

When feature skew data heterogeneity is present, relying solely on prediction fusion for cooperation is insufficient. This approach fails the communication between the online and offline models during the training phase, impeding the transfer of online domain-invariant and offline domain-specific knowledge. Therefore, to enhance collaboration among the models in FL with feature skew, we propose novel intra-client and inter-client knowledge transfer mechanisms. In the next two parts, we will introduce these mechanisms and discuss their role.

### Intra-Client Knowledge Transfer

In our framework, each client \(i\) is equipped with two models: an online model for capturing general domain-invariant knowledge and an offline model for capturing specialized domain-specific knowledge. Unlike previous algorithms that utilized a two-branch framework, aiming to make the knowledge learned by each branch as disjoint as possible through methods such as Contrastive Learning  or Feature Disentangling , we introduce a novel intra-client knowledge transfer mechanism to enhance the collaboration of the two models. This mechanism employs knowledge distillation to facilitate mutual learning between the two models during the training phase. However, transferring online general knowledge to the offline model could potentially conflict with the local training objective. The former emphasizes learning global information among \(\{D_{j}\}^{j i}\) and avoiding over-fitting on local data, whereas the latter focuses on fitting local information in \(D_{i}\). As a result, neither of the two goals can be fully accomplished. Furthermore, during local training, there is a risk of the online model losing general domain-invariant knowledge, which leads to a reduction in the amount of valuable information accessible to the offline model.

To address these challenges, we divide the local training process into two distinct phases: a mutual learning phase and a local adaptation phase to achieve their respective goals. Concretely, in the mutual learning phase, we create a duplicate of the initial online model, which consists of its personalized BNs from the last round and other parameters updated with the global aggregation model. Additionally, we retain a copy of the initial offline model from the last round. As illustrated in Fig. 1(b), these duplicated models are frozen and utilized as teacher networks, with the copied online model serving as a teacher for the offline model, and the copied offline model guiding the online model. We denote the duplicated initial online model and copied initial offline model as \(_{i}^{}\) and \(_{i}^{}\), respectively, with frozen parameters \(_{i}^{}\) and \(_{i}^{}\). In training step \(s\), given input sample \(x_{k}\), online and offline models conduct intra-client knowledge transfer and update their parameters in the mutual learning form:

\[_{i,s}^{} =_{i,s-1}^{}-_{_{i,s-1}^{ }}KL(_{i}^{}(_{i}^{}\,;\,x_{ k})\,,\,F_{i}^{}(_{i,s-1}^{}\,;\,x_{k})),\] (4) \[_{i,s}^{} =_{i,s-1}^{}-_{_{i,s-1}^{ }}KL(_{i}^{}(_{i}^{}\,;\,x_{k} )\,,\,F_{i}^{}(_{i,s-1}^{}\,;\,x_{k})),\] (5)

where \(KL(\,,\,)\) denotes the Kullback-Leibler (KL) divergence and \(\) is the learning rate. Through mutual learning, the online model and the offline model engage in model-level collaboration and share the knowledge acquired in the previous round.

### Inter-Client Knowledge Transfer

After the intra-client knowledge transfer in the first phase, both the online and offline models have gained beneficial knowledge from each other, and now it is opportune to let them adapt to the local data distributions. The pertinent question is whether we can leverage additional knowledge from other clients to bridge the domain gaps in the local training. Currently, the approach of acquiring knowledge from other clients is limited to aggregating shared parameters on the server. In comparison to various advanced approaches used in Domain Generalization (DG) research, parameter aggregation alone may struggle to learn effective domain-invariant features. The primary challenge stems from the isolation of private data, as FL prohibits cross-client access to data from other domains. Without data from multiple domains, it is tough to directly apply techniques in DG.

Considering that the offline model in each client is trained to capture local specialized domain-specific knowledge, we propose to leverage these lightweight classifiers of the offline model to transfer domain knowledge among clients, as illustrated in Fig. 1(c). The classifiers of the offline model from other clients are introduced to each local client, enabling the feature extractors of its online and offline models to generate robust and well-generalized features that can be effectively recognized by these introduced classifiers. We share similar design intuitions with COPA , but here our inter-client knowledge transfer mechanism stresses more on making use of knowledge from other clients to benefit the personalized model for each client rather than serving unseen novel clients.

To be specific, in Fed-CO\({}_{2}\), each client uploads its classifier \(C_{i}^{}\) from its offline model to the server, along with its shared parameters (BNs are not included) from its online model. These uploaded heads construct a classifier set for inter-client knowledge transfer: \(\{_{j}^{}\}_{j=1}^{N}\) with parameter set \(\{_{j}^{}\}_{j=1}^{N}\) and are transmitted to each client in the next round. In communication round \(t\), each client has its own personalized model \(F_{i}=\{F_{i}^{},F_{i}^{}\}\) and the downloaded knowledge-transfer classifier set \(\{_{j}^{}\}_{j=1}^{N}\). Similar to teacher models in the mutual learning phase, we freeze these knowledge-transfer classifiers to keep their knowledge unchanged. With input data \(x_{k}\) and its label \(y_{k}\) our inter-client knowledge transfer loss function \(L_{gen}\) for models on client \(i\) is formulated as:

\[L_{gen}(x_{k}\,,\,y_{k})=_{j i}L_{CE}( _{j}^{}(_{j}^{};\,f_{i }(_{i,t};\,x_{k})),\,y_{k}),\] (6)

Combined with its classification loss to adapt to local private data, the total loss in local adaptation phase for each model is:

\[L=L_{CE}(x_{k}\,,\,y_{k})+ L_{gen}(x_{k}\,,\,y_{k}),\] (7)

where \(\) is a penalized factor set as 1 by default. After local training, we upload parameters to the server and aggregate them with Eq. 2. Algorithm 1 in Appendix demonstrates the procedures of our Fed-CO\({}_{2}\) algorithm.

## 4 Theoretical Analysis

Here, we provide convergence analyses, which compare our Fed-CO\({}_{2}\) and FedBN  with the neural tangent kernel (NTK)  theory, to illustrate the effectiveness of our algorithm. For simplification, only the cooperation of online and offline models is considered while the intra-client and inter-client knowledge transfer mechanisms are ignored.

### Formulation

Before our analyses, we clarify the simplified setup and basic assumptions. Suppose we have \(N\) clients, each with \(M\) training examples, and we aim to jointly train them for \(T\) communication rounds. In each round, each model is locally trained for one epoch. For simplification, we assume that all clients share an identical two-layer neural network for a regression task, with the only source of heterogeneity being the feature skew. Namely, each client \(i\) has its private dataset \(_{i}=\{(_{j}^{i},\,y_{j}^{i})\}_{j=1}^{M}\) with \(_{j}^{i}^{d}\) and \(y_{j}^{i}\). For convenience, we adopt the same data distribution assumption as in :

**Assumption 4.1**: _For each client \(i\{1,,N\}\), the inputs \(_{j}^{i}\) are centered, meaning that \([^{i}]=0\), and they have a covariance matrix \(_{i}=[^{i}(^{i})^{}]\). \(_{i}\) is independent of the label \(y\) and varies for each client \(i\). Not all \(_{i}\) matrices are identity matrices. For any index pair \(p\) and \(q\) (\(p q\)), we have \(_{p} w_{q}\) for any non-zero \(w\)._

Let \(_{k}^{d}\) represent the parameters of the first layer, where \(k[m]\) and \(m\) is the width of the hidden layer. We define \(\|\|_{}:=^{}}\) as the induced vector norm for a positive definite matrix \(\) and a \(l_{2}\) vector norm \(\|\|_{2}\). The projections of \(\) onto \(\) and \(^{}\) are defined as \(^{}:=^{}}{\| \|_{2}^{2}}\), \(^{^{}}:=(-^{ }}{\|\|_{2}^{2}})\).

Based on Assumption 4.1, for client \(i\), the output of the first layer is normalized as \(_{k}^{}^{i}}{\|_{k}\|_{_{i}}}\). The shift parameter of BN is omitted. Further, we denote the scaling parameter of BN \(^{m N}\) and the second layer parameter \(c^{m}\).

With these parameters, we proceed to train the online model:

\[^{}(\,;\,,\,,\,c)=} _{k=1}^{m}c_{k}^{}_{i=1}^{N}( _{k,i}^{}_{k}^{^{}} }{\|_{k}^{}\|_{i}\,}) \{_{i}\},\] (8)

where \(()\) is the ReLU activation function and \(_{k,i}^{}\) is personalized BN parameters. For the offline model with all parameters personalized, we train:

\[^{}(\,;\,,\,,\,c)=}_{k=1}^{m}_{i=1}^{N}c_{k,i}^{} (_{k,i}^{}_{k,i}^{ ^{}}}{\|_{k,i}^{}\|_{i}\,})\{_{i}\}.\] (9)

With online and offline models, we form our Fed-CO\({}_{2}\) as model \(\):

\[(\,;\,,\,,\,c)=(^{}(\,;\,^{},\,^{},\,c^{ })+^{}(\,;\,^{},\,^{},\,c^{})).\] (10)

In our analysis, we adopt one random strategy  to initialize parameters:

\[_{k,i}^{}(0)=_{k}^{}(0)(0,\,^{2});\,c_{k,i}^{}=c_{k}^{} \{-1,\,1\};\,_{k,i}^{}=_{k,i}^{}=_{k}^{}(0)\|_{2}}{},\] (11)

where \(^{2}\) controls the magnitude of \(_{k}^{}\) and \(_{k,i}^{}\) at initialization. The initialization of the BN parameters \(_{k,i}^{}\) and \(_{k,i}^{}\) are independent of \(\). We use the MSE loss to train our model \(\):

\[ L()=& _{i=1}^{N}_{j=1}^{M}((_{i}^{j} )-y_{j}^{i})^{2}\\ =&_{i=1}^{N}_{j =1}^{M}(^{}(_{i}^{j})+^{ }(_{i}^{j})}{2}-y_{i}^{j})^{2}.\] (12)

### Convergence Analysis

We employ NTK  to analyze the trajectory of networks \(\) learned by Fed-CO\({}_{2}\) and networks \(^{}\) learned by FedBN (the online model in Fed-CO\({}_{2}\)). Existing studies [37; 38] have validated that the convergence rate of finite-width over-parameterized networks is controlled by the least eigenvalue of the induced kernel in the training process. Following the discovery in , we can decompose the NTK into a direction component \((t)/^{2}\) and a magnitude component \((t)\):

\[}{dt}=-(t)((t)-), (t):=(t)}{^{2}}+(t).\] (13)

The specific forms of \((t)\) and \((t)\) are provided in the Appendix. Here, let \(_{}(H)\) denote the minimum eigenvalue of matrix \(H\). It is worth noting that both matrices \((t)\) and \((t)\) are positive semi-definite as they can be interpreted as covariance matrices. Based on this, we can deduce that \(_{}((t))\{_{}((t)/ ^{2}),\,_{}((t))\}\). From the NTK theory, the value of \(_{}((t))\) controls the convergence rate. Then, considering that \(\) is the pre-defined parameter, for \((0,1)\), convergence is dominated by \((t)\). Let \((t)\), \(^{}(t)\) and \(^{}(t)\) denote the evolution dynamics of Fed-CO\({}_{2}\), the online model and the offline model, respectively. Based on the work in , the auxiliary version of the Gram matrices \(^{}\), \(^{}_{}\), and \(^{}_{}\) for three models are strictly positive definite. Let the least eigenvalues \(_{}(^{}_{}):=^{}\), \(_{}(^{}_{}):=^{}\), and \(_{}(^{}):=\), where \(^{}\), \(^{}\), and \(\) are all positive values. To be specific, we define Gram matrices \(^{}_{}\) and \(^{}_{}\) in Definition 4.2.

**Definition 4.2**: _Given sample points \(\{_{p}\}_{p=1}^{NM}\), we define the auxiliary Gram matrices \(^{}_{}^{NM NM}\) and \(^{}_{}^{NM NM}\) as_

\[^{}_{_{_{}}} :=_{(,^{2} )}( c)^{2}_{p}^{^{}} _{q}^{^{}},\] (14) \[^{}_{_{}} :=_{(,^{2} )}( c)^{2}_{p}^{^{}} _{q}^{^{}}\{i_{p}=i_{q}\},\] (Offline Model). (15)

With the prerequisite provided in Appendix, the convergence performance of the online model, the offline model, and our Fed-CO\({}_{2}\) can be analyzed by comparing \(_{}(^{}_{})\), \(_{}(^{}_{})\), and \(_{}(^{})\). Our main theoretical result is given in Theorem 4.3.

**Theorem 4.3**: _For the V-dominated convergence, the convergence rate of Fed-CO\({}_{2}\) is faster than that of FedBN (the online model in Fed-CO\({}_{2}\))._

**Proof sketch** The core is to show \(_{}(_{}^{})_{}( _{}^{})\). To prove the theorem, we first show that \(_{}(_{}^{})_{}( _{}^{})\). Comparing Eq. 14 and 15, \(_{}^{}\) takes the \(\) block matrices on the diagonal of \(_{}^{}\). Let \(_{i}^{}\) be the i-th \(\) block matrices on the diagonal of \(_{}^{}\). According to linear algebra, we have \(_{}(_{i}^{})_{}(_{}^{})\), for \(i[N]\). With \(_{}^{}=diag(_{1}^{},, _{N}^{})\), it can be obtained that \(_{}(_{}^{})_{}(_{}^{})\). Then, based on Eq. 10, 12, and 13, we can obtain \(_{}(^{})=_{}((_{ }^{}+_{}^{}))\). Therefore, we have \(_{}(^{})(_{}(_{ }^{})+_{}(_{}^{ }))\). Since we have proven that \(_{}(_{}^{})_{}(_{}^{})\), the result \(_{}(^{})_{}(_{}^{ })\) can be achieved.

## 5 Experiments

### Experiment Settings

**Dataset and Data Heterogeneity.** We followed the footstep of prior research [39; 7; 5; 13] to study the label distribution skew heterogeneity with image datasets: CIFAR10 and CIFAR100 . Specifically, we considered two different label distributions among participants: 1) Pathological Distribution, each client is randomly assigned 2 classes per client in CIFAR10 (10 classes per client in CIFAR100); 2) Dirichlet Distribution, each client gets its private data through partitioning of the datasets using a symmetric Dirichlet distribution with a default parameter \(=0.3\).

For feature skew heterogeneity, we conducted extensive experiments on three datasets: Digits, Office-Caltech10 , and DomainNet . In this non-IID data setting, each domain serves as a client, with each client having access to all the data from its respective domain. Digits is composed of 5 different digit datasets with feature shift: SVHN , USPS , SynthDigits , MNIST-M  and MNIST . Office-Caltech10  owns data from four different domains including Amazon, DSLR, WebCam, and Caltech-256. DomainNet  is a challenging dataset that comprises six distinct domains, namely Clipart, Infograph, Painting, Quickdraw, Real, and Sketch. To explore the more realistic FL scenarios with both label distribution skew and feature skew, we exert a Dirichlet Distribution to datasets with feature shift. More details are supplemented in Appendix.

**Compared Benchmarks.** We compared our universal framework **Fed-CO\({}_{2}\)** with fundamental algorithms including **SingleSet**, **FedAvg** and **FedProx**, together with some refined FL algorithms: **FedPer**, **MOON**, **FedBN**, and **FedRoD**. Here, **SingleSet** means each client trains their own model on their private data. We applied the linear version for **FedRoD**. For FL with feature skew issues, we further compared **COPA**, which was originally proposed to solve FDG and FDA problems. We now train the model on all domains in the dataset and evaluate its performance on each individual training client using the learned universal model.

### Performance Evaluation

**Model Evaluation on Feature Skew.** Experiments are conducted on datasets Digits, Office-Caltech10, and DomainNet. Here, we present the results of experiments on Office-Caltech10, and DomainNet in Table 1 and Table 2. Full experiments are shown in Appendix. The performance of our Fed-CO\({}_{2}\) exhibits a dominant edge over state-of-the-art algorithms in all experiments, where the average accuracy on each domain increases by nearly \(4\%\). The results confirm that our novel collaboration framework between online and offline models effectively addresses FL challenges associated with severe feature skew data heterogeneity.

Remarkably, when compared to algorithms FedBN  and COPA , which are explicitly designed to handle feature shift issues, our method consistently outperforms them across all sub-datasets. This phenomenon provides compelling evidence that our cooperation mechanism adapts to local feature shifts more effectively by leveraging both general domain-invariant and specialized domain-specific knowledge.

**Model Evaluation on Label Distribution Skew.** As shown in Table 3, our Fed-CO\({}_{2}\) outperforms a variety of state-of-the-art PFL algorithms in terms of average test accuracy across almost all cases in experiments with label distribution skew. Our state-of-the-art results across almost all casesprove that our method can more effectively excavate and utilize local information to overcome data heterogeneity caused by label distribution imbalance than previous methods. It is worth highlighting that our Fed-CO\({}_{2}\) outperforms both SingleSet and FedBN, regardless of which one performs better individually. This observation confirms that Fed-CO\({}_{2}\) effectively integrates the general knowledge obtained by the online model and the specialized knowledge acquired by the offline model, resulting in enhanced performance.

**Model Evaluation on Label Distribution Skew and Feature Skew.** Here, we evaluate our Fed-CO\({}_{2}\) and benchmark algorithms in FL scenarios with label distribution skew and feature skew. We exhibit the experiment results on Digits in Table 4, where we exerted a Dirichlet Distribution on each client with \(=0.3\). From the results, it is evident that, even in the presence of two types of data heterogeneity, our Fed-CO\({}_{2}\) outperforms various state-of-the-art algorithms with a significant margin in every sub-dataset. Therefore, we can conclude that under our universal cooperation framework, local clients not only make the most use of their local offline information but also benefit from the general knowledge shared by other clients for a better adaptation to local data, resulting in state-of-the-art performance in FL with both label distribution skew and feature skew. More experiments are provided in Appendix.

   &  \\   & Amazon & Caltech & DSLR & WebCam & Avg \\  SingleSet & 54.9\(\)1.5 & 40.2\(\)1.6 & 78.7\(\)1.3 & 86.4\(\)2.4 & 65.1\(\)1.7 \\ FedAvg  & 54.1\(\)1.1 & 44.8\(\)1.0 & 66.9\(\)1.5 & 85.1\(\)2.9 & 62.7\(\)1.6 \\ FedProx  & 54.2\(\)2.5 & 44.5\(\)0.5 & 65.0\(\)3.6 & 84.4\(\)1.7 & 62.0\(\)2.1 \\ FedPer  & 49.0\(\)1.2 & 37.1\(\)2.4 & 57.7\(\)3.7 & 79.7\(\)2.1 & 56.0\(\)1.1 \\ MOON  & 57.3\(\)0.7 & 44.4\(\)0.5 & 76.2\(\)2.5 & 83.1\(\)1.1 & 65.2\(\)0.5 \\ FedRobD  & 60.4\(\)2.3 & 45.3\(\)0.9 & 73.7\(\)2.5 & 83.7\(\)2.3 & 65.8\(\)1.4 \\  COPA  & 51.9\(\)2.5 & 46.7\(\)0.8 & 65.6\(\)2.0 & 85.0\(\)1.3 & 62.3\(\)0.9 \\ FedBN  & 63.0\(\)1.6 & 45.3\(\)1.5 & 83.1\(\)2.5 & 90.5\(\)2.3 & 70.5\(\)2.0 \\  Fed-CO\({}_{2}\) & **63.0\(\)1.6** & **49.1\(\)0.7** & **89.4\(\)2.5** & **96.6\(\)1.5** & **74.5\(\)0.3** \\  

Table 1: Experiment results for FL with Feature Skew on Office-Caltech10.

   &  \\   & Clipart & Infograph & Paining & Quickflow & Real & Sketch & Avg \\  SingleSet & 41.0\(\)0.9 & 23.8\(\)1.2 & 36.2\(\)2.7 & 73.1\(\)0.9 & 48.5\(\)1.9 & 34.0\(\)1.1 & 42.8\(\)1.5 \\ FedAvg  & 48.8\(\)1.9 & 24.9\(\)0.7 & 36.5\(\)1.1 & 56.1\(\)1.6 & 46.3\(\)1.4 & 36.6\(\)2.5 & 41.5\(\)1.5 \\ FedProx  & 48.9\(\)0.8 & 24.9\(\)1.0 & 36.6\(\)1.8 & 54.4\(\)3.1 & 47.8\(\)0.8 & 36.9\(\)2.1 & 41.6\(\)1.6 \\ FedPer  & 40.4\(\)0.8 & 25.7\(\)0.6 & 37.3\(\)0.6 & 62.5\(\)1.4 & 47.4\(\)0.5 & 32.8\(\)0.8 & 41.0\(\)0.3 \\ MOON  & 52.5\(\)1.1 & 25.7\(\)0.6 & 39.4\(\)1.7 & 50.8\(\)4.7 & 48.8\(\)0.8 & 40.1\(\)4.1 & 42.9\(\)1.5 \\ FedRoD  & 50.8\(\)1.6 & 26.3\(\)0.2 & 40.1\(\)1.8 & 66.8\(\)1.8 & 51.5\(\)1.1 & 39.1\(\)2.0 & 45.7\(\)0.7 \\  COPA  & 51.1\(\)1.0 & 24.7\(\)1.2 & 36.8\(\)0.8 & 54.8\(\)1.6 & 47.1\(\)1.8 & 41.0\(\)1.4 & 42.6\(\)0.4 \\ FedBN  & 51.2\(\)1.4 & 26.8\(\)0.5 & 41.5\(\)1.4 & 71.3\(\)0.7 & 54.8\(\)0.8 & 42.1\(\)1.3 & 48.0\(\)1.0 \\ Fed-CO\({}_{2}\) & **55.0\(\)1.1** & **28.6\(\)1.1** & **44.3\(\)0.6** & **75.1\(\)0.6** & **62.4\(\)0.8** & **45.7\(\)1.9** & **51.8\(\)0.2** \\  

Table 2: Experiment results for FL with Feature Skew on DomainNet.

   &  &  \\   & Pathological & Dirichlet & Pathological & Dirichlet \\  SingleSet & 85.85\(\)0.05 & 68.38\(\)0.06 & 49.54\(\)0.05 & 21.39\(\)0.05 \\ FedAvg  & 44.12\(\)3.10 & 57.52\(\)1.01 & 14.59\(\)0.40 & 20.34\(\)1.34 \\ FedProx  & 57.38\(\)1.08 & 56.46\(\)0.66 & 21.32\(\)0.71 & 19.40\(\)1.76 \\ FedPer  & 80.99\(\)0.71 & 74.21\(\)0.07 & 42.08\(\)0.18 & 20.06\(\)0.26 \\ MOON  & 48.43\(\)3.18 & 54.49\(\)1.87 & 17.89\(\)0.76 & 19.73\(\)0.71 \\ FedRoD  & **89.05\(\)0.04** & 73.99\(\)0.09 & 54.96\(\)1.30 & 28.29\(\)1.53 \\ FedBN  & 86.71\(\)0.56 & 75.41\(\)0.37 & 48.37\(\)0.56 & 28.70\(\)0.46 \\  Fed-CO\({}_{2}\) & 88.79\(\)0.25 & **77.45\(\)0.30** & **58.50\(\)0.43** & **32.43\(\)0.37** \\  

Table 3: Experiment results for FL with Label Distribution Skew on CIFAR10 and CIFAR100. Experiments are conducted with two kinds of label distribution data heterogeneity: Pathological setting and Dirichlet setting.

### Component Analysis

In the presence of feature skew, we add intra-client and inter-client knowledge transfer mechanisms to foster the collaboration between the online and offline models for better local adaptation. Here, we evaluate the efficacy of these two knowledge transfer mechanisms on the challenging DomainNet dataset through three additional experiments, as presented in Table 5. In these experiments, we selectively removed specific mechanisms from Fed-CO\({}_{2}\) to investigate their individual contributions and functionalities.

According to the experimental results, we can observe that: 1) In the absence of intra-client and inter-client knowledge transfer mechanisms, Fed-CO\({}_{2}\) exhibits a significant decline in performance, resulting in a performance level comparable to that of benchmark methods. Therefore, cooperation relying solely on prediction fusion proves to be inadequate for FL with feature skew. 2) The removal of either the intra-client or the inter-client knowledge transfer mechanisms will also result in an average decline in performance. This phenomenon validates the effectiveness of our intra-client and inter-client knowledge transfer mechanisms, both of which contribute to enhancing the models' capability to adapt to local data distributions. 3) Compared with all three ablation experiments, our Fed-CO\({}_{2}\) consistently achieves the highest performance on average and across the majority of sub-datasets. These results firmly demonstrate that our cooperation framework effectively leverages both model-level and client-level collaboration through the intra-client knowledge transfer and inter-client knowledge transfer mechanisms, respectively, to adapt to feature shifts resulting from domain gaps. Further analysis of these two cooperation mechanisms is supplemented in the Appendix.

## 6 Conclusions

In this work, we proposed a new universal FL framework called Fed-CO\({}_{2}\) that is capable of handling label distribution skew and feature skew even when both are present in the same data. The core of our approach is to foster cooperation between the online and offline models, leveraging the benefits of online general knowledge and offline specialized knowledge to effectively adapt to local data distribution. To improve model collaboration in FL with feature shifts, we designed two novel knowledge transfer mechanisms: one intra-client and the other inter-client. These mechanisms facilitate mutual learning between the online and offline models, concurrently enhancing the model's capacity to generalize across different domains. Comparisons with a wide range of state-of-the-art methods on five benchmark datasets consistently show that Fed-CO\({}_{2}\) yields superior performance in addressing both label distribution skew and feature skew challenges, both individually and in combination. Extending Fed-CO\({}_{2}\) to scenarios with noisy training data is under consideration in our future work.

  Methods &  &  &  \\  & MNIST & SVHN & USPS & SynthDigits & MNIST-M & Avg \\  SingleSet & 83.75\(\)7.58 & 74.63\(\)0.31 & 97.14\(\)0.06 & 87.95\(\)0.46 & 80.55\(\)0.26 & 84.80\(\)1.44 \\ FedAvg  & 89.27\(\)6.39 & 57.23\(\)5.43 & 94.60\(\)1.05 & 81.30\(\)2.51 & 71.71\(\)7.59 & 78.82\(\)4.03 \\ FedProx  & 87.09\(\)7.83 & 53.40\(\)7.38 & 90.55\(\)7.47 & 78.40\(\)7.05 & 69.98\(\)6.55 & 75.88\(\)6.90 \\ FedPer  & 96.92\(\)0.02 & 72.86\(\)0.11 & 97.09\(\)0.12 & 87.82\(\)0.02 & 83.69\(\)0.07 & 87.68\(\)0.02 \\ MOON  & 96.58\(\)0.07 & 74.10\(\)0.22 & 96.19\(\)0.10 & 88.15\(\)0.04 & 85.05\(\)0.14 & 88.01\(\)0.07 \\ FedRoD  & 96.65\(\)0.06 & 77.05\(\)0.16 & 96.88\(\)0.13 & 89.59\(\)0.06 & 86.18\(\)0.11 & 89.27\(\)0.05 \\  COPA  & 96.82\(\)0.08 & 78.32\(\)0.12 & 96.54\(\)0.15 & 89.36\(\)0.04 & 87.46\(\)0.11 & 89.70\(\)0.06 \\ FedSh  & 92.68\(\)3.45 & 70.26\(\)3.38 & 91.40\(\)8.72 & 83.17\(\)4.95 & 77.98\(\)3.84 & 83.10\(\)4.96 \\  Fed-CO\({}_{2}\) & **97.17\(\)0.67** & **83.16\(\)0.23** & **98.12\(\)0.13** & **93.04\(\)0.11** & **91.45\(\)0.14** & **92.59\(\)0.15** \\  

Table 4: Experiment results for FL with both Label Skew and Feature Skew on Digits.

  Methods &  &  &  & Quickdraw & Real & Sketch & Avg \\  Fed-CO\({}_{2}\) w/o Intra and Inter Transfer & 48.75\(\)0.94 & 26.49\(\)2.05 & 42.10\(\)1.05 & 72.864\(\)0.80 & 57.12\(\)1.08 & 39.96\(\)0.79 & 47.88\(\)0.70 \\ Fed-CO\({}_{2}\) w/o Intra Transfer & 53.88\(\)0.83 & 26.18\(\)0.71 & 42.94\(\)0.029 & **75.10\(\)0.28** & 91.94\(\)0.70 & **46.68\(\)0.75** & 1.21\(\)0.19 \\ Fed-CO\({}_{2}\) w/o Intra Transfer & 50.42\(\)40.72 & 26.97\(\)1.07 & 43.43\(\)0.69 & 74.10\(\)0.64 & 58.14\(\)0.85 & 42.02\(\)0.80 & 49.27\(\)0.46 \\  Fed-CO\({}_{2}\) & **55.0241.13** & **28.58\(\)1.10** & **44.27\(\)0.62** & 75.08\(\)0.62 & **62.37\(\)0.76** & 45.67\(\)1.95 & **51.83\(\)0.25** \\  

Table 5: Ablation study of Intra-client and Inter-client knowledge transfer mechanisms for FL with Feature Skew Data Heterogeneity on DomainNet.