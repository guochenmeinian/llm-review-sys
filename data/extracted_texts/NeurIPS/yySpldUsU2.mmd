# Changing the Training Data Distribution to

Reduce Simplicity Bias Improves

In-distribution Generalization

Tuan Hai Dang Nguyen  Paymon Haddad  Eric Gan  Baharan Mirzasoleiman

Department of Computer Science, UCLA

###### Abstract

Can we modify the training data distribution to encourage the underlying optimization method toward finding solutions with superior generalization performance on _in-distribution_ data? In this work, we approach this question for the first time by comparing the inductive bias of gradient descent (GD) with that of sharpness-aware minimization (SAM). By studying a two-layer CNN, we rigorously prove that SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD. We also show that examples containing features that are learned early are separable from the rest based on the model's output. Based on this observation, we propose a method that (i) clusters examples based on the network output early in training, (ii) identifies a cluster of examples with similar network output, and (iii) upsamples the rest of examples only once to alleviate the simplicity bias. We show empirically that USEFUL effectively improves the generalization performance on the _original_ data distribution when training with various gradient methods, including (S)GD and SAM. Notably, we demonstrate that our method can be combined with SAM variants and existing data augmentation strategies to achieve, to the best of our knowledge, state-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.

## 1 Introduction

Training data is a key component of machine learning pipelines and directly impacts its performance. Over the last decade, there has been a large body of efforts concerned with improving learning from a given training dataset by designing more effective optimization methods  or neural networks with improved structures  or higher-capacity . More recently, improving the quality of the training data has emerged as a popular avenue to improve generalization performance. Interestingly, higher-quality data can further improve the performance when larger models and better optimization methods are unable to do so . Recent efforts to improve the data quality have mainly focused on filtering irrelevant, noisy, or harmful examples . Nevertheless, it remains an open question if one can change the distribution of a _clean_ training data to further improve the _in-distribution_ generalization performance of models trained on it.

At first glance, the above question may seem unnatural, as it disputes a fundamental assumption that training and test data should come from the same distribution . Under this assumption, minimizing the training loss generalizes well on the test data . Nevertheless, for overparameterized neural networks with more parameters than training data, there are many zero training error solutions, all global minima of the training objective, with _different generalization_ performance . Thus, one may still hope to carefully change the data distribution to drive the optimization algorithms towards finding more generalizable solutions on the _original_ data distribution.

In this work, we take the first steps towards addressing the above problem. To do so, we rely on recent results in non-convex optimization, showing the superior generalization performance of sharpness-aware-minimization (SAM)  over (stochastic) gradient descent (GD). SAM finds flatter local minima by simultaneously minimizing the loss value and loss sharpness. In doing so, it outperforms (S)GD and obtains state-of-the-art performance, at the expense of doubling the training time [20; 81]. Our key idea is that if one can change the training data distribution such that learning shares similar properties to that of training with SAM, then the new distribution can drive (S)GD and even SAM toward finding more generalizable solutions.

To address the above question, we first theoretically analyze the dynamics of training a two-layer convolutional neural network (CNN) with SAM and compare it with that of GD. We rigorously prove that SAM learns different features in a more _uniform speed_ compared to GD, particularly _early_ in training. In other words, we show that _SAM is less susceptible to simplicity bias_ than GD. Simplicity bias of SGD makes the model learn simple solutions with minimum norm  and has long been conjectured to be the reason for the superior generalization performance of overparameterized models by providing implicit regularization [7; 25; 28; 51; 52; 70]. Nevertheless, the minimum-norm solution found by GD can have a suboptimal performance .

Following our theoretical results, we formulate changing the distribution of a training dataset such that different features are learned at a more uniform speed. First, we prove that the model output for examples containing features that are learned early by GD is separable from the rest of examples in their class. Then, we propose changing the data distribution by (i) identifying a cluster of examples with similar model output early in training, (ii) upsampling the remaining examples once to speed up their learning, and (iii) restarting training on the modified training distribution. Our method, UpSample Early For Uniform Learning (USEFUL), effectively alleviates the simplicity bias and consequently improves the generalization performance. Intuitively, learning features in a more uniform speed prevents the model to overfit underrepresented but useful features that otherwise are learned in late training stages. When the model overfits an example, it cannot learn its features in a generalizable manner. This harms the generalization performance on the original data distribution.

We show the effectiveness of USEFUL in alleviating the simplicity bias and improving the generalization via extensive experiments. First, we show that despite being relatively lightweight, USEFUL effectively improves the generalization performance of SGD and SAM. Additionally, we show that USEFUL can be easily applied with various optimizers and data augmentation methods to improve in-distribution generalization performance even further. For example, applying USEFUL with SAM and TrivialAugment (TA)  achieves, to the best of our knowledge, _state-of-the-art_ accuracy for image classification for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10. We also empirically confirm the benefits of USEFUL to out-of-distribution performance, but we emphasize that this is not the focus of our work.

## 2 Related Works

**Sharpness-aware-minimization (SAM).** Motivated by the generalization advantages of flat local minima, sharpness-aware minimization (SAM) was concurrently proposed in [22; 81] to minimize the training loss at the worst perturbed direction from the current parameters. SAM has been shown to obtain state-of-the-art on a variety of tasks . Additionally, SAM has been shown to be beneficial in other settings, including label noise [22; 81], and domain generalization [9; 72].

There have been recent efforts to understand the generalization benefits of SAM. The most popular explanation is based on the Hessian spectra, empirically [22; 36] and theoretically [6; 73]. Other works showed that SAM finds a sparser solution in diagonal linear networks , and exhibits benign overfitting under much weaker signal strength compared to (S)GD . More recently, SAM is shown to also benefit out-of-distribution (OOD). In particular,  suggested that SAM promotes diverse feature learning by empirically studying a simplified version of SAM which only perturbs the last layer. They showed that SAM upscales the last layer's weights to induce feature diversity, which benefits OOD. In contrast, we rigorously analyze a 2-layer non-linear CNN and prove that SAM learns (the same set of) features at a more uniform speed, which benefits the in-distribution (ID) settings. Our results reveal an orthogonal effect of SAM that benefits the ID generalization by reducing the simplicity bias, and provides a complementary view to prior works explaining superior ID generalization performance of SAM. We then propose a method to learn features more evenly by changing the data distribution.

**Simplicity bias (SB).** (S)GD has an inductive bias towards learning simpler solutions with minimum norm . It is empirically observed  and theoretically proved  that SGD learns linear functions in the early training phase and more complex functions later in training. SB of SGD has been long conjectured to be the reason for the superior in-distribution generalization performance of overparameterized models, by providing capacity control or implicit regularization [26; 52; 55; 63]. On the other hand, in the OOD setting, simplicity bias is known to contribute to shortcut learning by causing models to exclusively rely on the simplest spurious feature and remain invariant to the complex but more predictive features [63; 67; 75]. Prior works on mitigating simplicity bias have been shown effective in the OOD settings [67; 68]. In contrast, our work shows, for the first time, that reducing the simplicity bias also benefits the ID settings. By studying the mechanism of feature learning in a two-layer nonlinear CNN, we prove that SAM is less susceptible to simplicity bias than GD, in particular early in training, which contributes to its superior performance. Then, we show that training data distribution can be modified to reduce the SB and improve the in-distribution generalization. In Appendix D.7, we empirically confirm that existing simplicity bias mitigation methods also improve the in-distribution performance, but to a smaller extent than ours.

**Distinction from Existing Settings.** Our work is distinct from the following literature:

(1) _Distribution Shift._ Unlike distribution shift and shortcut learning [18; 39; 57; 61], we _do not_ assume existence of domain-dependent (non-generalizable) features or strong spurious correlations in the training data, or shift between training and test distribution. We focus on _in-distribution_ generalization, where training and test distributions are the same and all the features in the training data are relevant for generalization. In Appendix D.5 we empirically show the benefits of our method to distribution shift, but we emphasize that this is not the focus of our study and we leave this direction to future work.

(2) _Long-tail distribution._ Long-tailed data is studied as a special case of distribution shift in which (sub)classes are highly imbalanced in training but are (more) balanced in test data [15; 71]. Long-tail methods resample the data at the class or subclass level to match the training and test distribution. In contrast, in our settings, training and test data follow the same distribution. Nevertheless, our method can be applied to improve the performance of long-tail datasets, as we confirm in Appendix D.5.

(3) _Improving Convergence._ A body of work speeds up convergence of (S)GD to find the _same solution_ faster. Such methods iteratively sample or reweight examples based on loss or gradient norm during training [21; 33; 35; 80]. In contrast, our work does not intend to speed up training to find the same solution faster, but intends to find a _more generalizable solution_ on the original data distribution.

(4) _Data Filtering Methods._ Filtering methods identify and discard or downweight noisy labeled , domain mismatched , redundant [1; 44; 59], or adversarial examples crafted by data poisoning attacks . In contrast, we assume a _clean_ training data and no mismatch between training and test distribution. Our work can be applied to a filtered training data to further improve the performance.

## 3 Theoretical Analysis: SAM Learns Different Features More Evenly

In this section, we analyze and compare feature learning mechanism of SAM. First, we introduce our theoretical settings including data distribution and neural network model in Sec. 3.1. We then revisit the update rules of GD and SAM in Sec. 3.2 before presenting our theoretical results in Sec. 3.3.

### Theoretical Settings

**Notation.** We use lowercase letters, lowercase boldface letters, and uppercase boldface letters to denote scalars \((a)\), vectors \(()\), and matrices \(()\). For a vector \(\), we use \(\|\|_{2}\) to denote its Euclidean norm. Given two sequence \(\{x_{n}\}\) and \(\{y_{n}\}\), we denote \(x_{n}=O(y_{n})\) if \(|x_{n}| C_{1}|y_{n}|\) for some absolute positive constant \(C_{1}\), \(x_{n}=(y_{n})\) if \(|x_{n}| C_{2}|y_{n}|\) for some absolute positive constant \(C_{2}\), and \(x_{n}=(y_{n})\) if \(C_{3}|y_{n}||x_{n}| C_{4}|y_{n}|\) for some absolute constant \(C_{3},C_{4}>0\). Besides, we use \((),(),\) and \(()\) to hide logarithmic factors in these notations. Furthermore, we denote \(x_{n}=(y_{n})\) if \(x_{n}=O(y_{n}^{D})\) for some positive constant D, and \(x_{n}=(y_{n})\) if \(x_{n}=((y_{n}))\).

**Data distribution.** We use a popular data distribution used in recent works on feature learning [2; 8; 11; 12; 18; 32; 40] to represent data as a combination of two features and noise patches. Additionally, we introduce a probability \(\) to control the frequency of fast-learnable features in the data distribution.

**Definition 3.1** (Data distribution).: A data point \((,y)(^{d})^{P}\)\(\{ 1\}\) is generated from the distribution \((_{e},_{d},)\) as follows. We uniformly generate the label \(y\{ 1\}\). We generate \(\) as a collection of \(P\) patches: \(=(^{(1)},^{(2)},,^{(P)})(^{d})^{P}\), where

* **Slow-learnable Feature.** One and only one patch is given by \(_{d} y_{d}\) with \(_{d}_{2}=1\), \(_{e},_{d}=0\), and \(0_{d}<_{e}\).
* **Fast-learnable feature.** One and only one patch is given by \(_{e} y_{e}\) with \(_{e}_{2}=1\) with a probability \( 1\). With a probability of \(1-\), this patch is masked, i.e. \(\).
* **Random noise.** The rest of \(P-2\) patches are Gaussian noise \(\) that are independently drawn from \(N(0,(_{p}^{2}/d)_{d})\) with \(_{p}\) as an absolute constant.

For simplicity, we assume \(P=3\), and the noisy patch together with two features form an orthogonal set. Coefficients \(_{e}\) and \(_{d}\) characterize the feature strength in our data model. A larger coefficient means that the corresponding feature is learned faster.

**Two-layer nonlinear CNN.** To model modern state-of-the-art architectures, we analyze a two-layer nonlinear CNN which is also used in [8; 11; 18; 32; 40]. Unlike linear models, CNN can handle a data distribution that does not require a fixed position of patches as defined above. Formally,

\[f(;)=_{j[J]}_{p=1}^{P}(_{j}, ^{(p)}),\] (1)

where \(_{j}^{d}\) is the weight vector of the \(j\)-th filter, \(J\) is the number of filters (neurons) of the network, and \((z)=z^{3}\) is the activation function, i.e., the main source of non-linearity. \(=[_{1},,_{J}]^{d J}\) is the weight matrix of the CNN. Following [8; 18; 32], we assume a mild over-parameterization with \(J=(d)\). We initialize \(^{(0)}(0,_{0}^{2})\), where \(_{0}^{2}=(d)/d\).

### Empirical Risk Minimization: GD vs SAM

Consider a \(N\)-sample training dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) in which each data point is generated from the data distribution in Definition 3.1. The empirical loss function of a model \(f(;)\) reads

\[()=_{i=1}^{N}l(y_{i}f(_{i};)),\] (2)

where \(l\) is the logistic loss defined as \(l(z)=(1+(-z))\). The solution \(^{}\) of the empirical risk minimization (ERM) minimizes the above loss, i.e., \(^{}_{}()\).

**GD.** Typically, ERM is solved using gradient descent (GD). The update rule at iteration \(t\) of GD with learning rate \(>0\) reads

\[^{(t+1)}=^{(t)}-(^{(t)}).\] (3)

**SAM.** To find solutions with better generalization performance, Foret et al.  proposed the \(N\)-SAM algorithm that minimizes both loss and curvature. SAM's update rule at iteration \(t\) reads

\[^{(t+1)}=^{(t)}-(^{(t)}+^{(t)} (^{(t)})),\] (4)

where \(^{(t)}=>0\) is the inner step size that is usually normalized by gradient norm, i.e., \(^{(t)}=/(^{(t)})_{F}\).

### Comparing Learning Between fast-learnable & slow-learnable Features for GD & SAM

Next, we present our theoretical results on training dynamics of the two-layer nonlinear CNN using GD and SAM. We characterize the learning speed of features by studying the growth of the model outputs before the activation function, i.e., \(_{j}^{(t)},_{e}\) and \(_{j}^{(t)},_{d}\). We first prove that _early_ in training, both GD and SAM _only_ learn fast-learnable feature. Then, we show SAM learns slow-learnable and fast-learnable features at a more uniform speed.

**Theorem 3.2** (**GD Feature Learning)**.: _Consider training a two-layer nonlinear CNN model initialized with \(^{(0)}(0,_{0}^{2})\) on the training dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) with distribution \((_{e},_{d},)\) with \(^{1/3}_{e}>_{d}\). For a small-enough learning rate \(\), after training for \(T_{}\) iterations, w.h.p., the model: (1) learns the fast-learnable feature \(_{e}_{j[J]}_{j}^{(T_{})},_{e} (1/_{e})\); (2) does not learn the slow-learnable feature \(_{d}_{j[J]}_{j}^{(T_{})},_{d} =(_{0})\)._

**Theorem 3.3** (**SAM Feature Learning)**.: _Consider training a two-layer nonlinear CNN model initialized with \(^{(0)}(0,_{0}^{2})\) on the training dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) with distribution \((_{e},_{d},)\) with \(^{1/3}_{e}>_{d}\). For small-enough learning rate \(\) and perturbation radius \(\), after training for \(T_{}>T_{}\) iterations, w.h.p., the model: (1) learns the fast-learnable feature \(_{e}:_{j[J]}_{j}^{(T_{})},_{e} (1/_{e})\); (2) does not learn the slow-learnable feature \(_{d}:_{j[J]}_{j}^{(T_{})},_{d} =(_{0})\)._

The detailed proof of Theorems 3.2 and 3.3 are deferred to Appendices A.1 and A.2.

**Discussion.** Note that a larger value of \(_{j}^{(t)},\) for \(\{_{e},_{d}\}\) indicates better learning of the feature vector \(\) by neuron \(_{j}\) at iteration \(t\). From the above two theorems, the growth rate of the fast-learnable feature is significantly faster than that of the slow-learnable feature. As a small portion \((1-)\) of the dataset does not have the fast-learnable feature, the model needs to learn the slow-learnable feature to improve the performance.

Next, we show that SAM learns fast-learnable and slow-learnable features more evenly. We denote by \(G_{e}^{(t)}=_{j[J]}_{j}^{(t)},_{e}\) and \(G_{d}^{(t)}=_{j[J]}_{j}^{(t)},_{d}\) the alignment of model weights with fast-learnable and slow-learnable features, when training with GD. Similarly, we denote by \(S_{e}^{(t)}\) and \(S_{d}^{(t)}\) the alignment of model weights with fast-learnable and slow-learnable, when training with SAM.

**Theorem 3.4** (**SAM learns features more evenly than GD)**.: _Consider the same model and training dataset as Theorems 3.2 and 3.3. Assume that the learning rate \(\) and the perturbation radius \(\) are sufficiently small. Starting from the same initialization, the growth of fast-learnable and slow-learnable features in SAM is more balanced than that in SGD, i.e., for every iteration \(t[1,T_{0}]\):_

\[S_{e}^{(t)}-S_{d}^{(t)}<G_{e}^{(t)}-G_{d}^{(t)}.\] (5)

We prove Theorem 3.4 by induction in Appendix A.2 and back it by toy experiments in Section 5.1.

**Discussion.** Intuitively, our proof is based on the fact that the difference between the growth of fast-learnable and slow-learnable features in SAM is smaller than that of GD. Thus, starting from the same initialization, the slow-learnable feature contributes relatively more to the model prediction in SAM than it does in SGD. Thus, the slow-learnable feature benefits SAM, by reducing its overreliance on the fast-learnable features. We note that as neural networks are nonlinear, a small change in the output can actually result in a big change in the model and its performance. Even in the extreme setting when two features have identical strength and the fast-learnable feature exists in all examples, i.e., \(_{e}=_{d}==1\), the gap in Eq. 5 is significant as we confirm in Figure 8 in Appendix D.

**Remark.** The network often overfits slow-learnable features that are learned late during the training and do not learn them in a generalizable manner. This harms the generalization performance on the test set sampled from the _original_ data distribution.

Theorems 3.2 and 3.3 show that we can make the model learn more from the slow-learnable feature by increasing the value of \(_{d}\). Based on this intuition, we have the following theorem.

**Theorem 3.5** (**One-shot upsampling**).: _Under the assumptions of Theorems 3.2, 3.3, for a sufficiently small noise, from any iteration \(t\) during early training, we have the following results:_

1. _The slow-learnable feature has a larger contribution to the normalized gradient of the 1-step SAM update, compared to that of GD._
2. _Amplifying the strength of the slow-learnable feature increases its contribution to the normalized gradients of GD and SAM._
3. _There exists an upsampling factor_ \(k\) _s.t. the normalized gradient of the 1-step GD update on_ \((_{e},k_{d},)\) _recovers the normalized gradient of the 1-step SAM update on_ \((_{e},_{d},)\)

**Discussion.** Proof of Theorem 3.5 is given in Appendix A.4. We see that we can learn features at a more uniform speed by training on a new dataset \((_{e},_{d}^{},)\) with a larger strength \(_{d}^{}>_{d}\). But, the value of coefficient \(_{d}^{}\) varies by the model weights and gradient at each iteration \(t\).

**Remark.** Intuitively, Theorem 3.5 implies that by descending over a flatter trajectory, SAM learns slow-learnable features relatively earlier in training, compared to GD. While the largest difference between feature learning of SAM and (S)GD is attributed to early training dynamics (due to the simplicity bias of (S)GD and its largest contribution early in training), SAM learns features at a more uniform speed during the _entire_ training. This effect is, however, difficult to theoretically characterize exactly. Therefore, learning features at a more uniform speed via SAM help yield flatter minima with better generalization performance. We note that while SAM learns fast-learnable and slow-learnable features at a more uniform speed, it still suffers from simplicity bias and learns fast-learnable features earlier (although less so than GD) as evidenced in our Theorem 3.3.

## 4 Method: UpSample Early For Uniform Learning (USEFUL)

Motivated by our theoretical results, we aim to speed up learning the slow-learnable features in the training data. This drive the network to learn fast-learnable and slow-learnable features at a more uniformly speed, and ultimately improves the in-distribution generalization performance.

**Step 1: Identifying examples with fast-learnable features.** As shown in Theorems 3.2 and 3.3, fast-learnable features are learned early in training, and the _model output for examples containing fast-learnable features are highly separable_ from the rest of examples in their class, early in training. This is illustrated for one class of a toy example and CIFAR-10 in Fig. 2. Motivated by our theory, we seek to find a cluster of examples with similar model outputs early in training. To do so, we apply \(k\)-means clustering to the last-layer activation vectors of examples in every class, to separate examples with fast-learnable features from the rest of examples. Formally, for examples in every class with \(y_{j}=c\), we find:

\[*{arg\,min}_{C}_{i\{1,2\}}_{y_{j}=c,j C_{i}}\|f( _{j};^{(t)})-_{i}\|^{2},\] (6)

Figure 1: Examples of slow-learnable (top) and fast-learnable (bottom) in CIFAR-10 found by our method. Examples in the top row (slow-learnable) are harder to identify visually and look more ambiguous (part of the object is in the image or the object is smaller and the area associated with the background is larger). In contrast, examples in the bottom row (fast-learnable) are not ambiguous and are clear representatives of their corresponding class, hence are very easy to visually classify (the entire object is in the image and the area associated with the background is small).

where \(_{i}\) is the center of cluster \(S_{i}\). The cluster with lower average loss will contain the majority of examples containing fast-learnable features, whereas the remaining examples contain slow-learnable features in the training data. Examples of images in fast-learnable and slow-learnable clusters of CIFAR-10 found by USEFUL are illustrated in Fig. 1.

**The choice of clustering.** Our choice of clustering is motivated by our Theorems 3.2 and 3.3 which show that examples with fast-learnable features are separable based on model output from the rest of examples in their class. While examples with fast-learnable features are expected to have a lower loss, loss of examples may oscillate during the training and makes it difficult to find an accurate cut-off for separating the examples. Besides, as fast-learnable features may not be _fully_ learned early in training, examples containing fast-learnable features may not necessarily have the right prediction, thus misclassification cannot separate examples accurately. In contrast, clustering does not require hyperparameter tuning and performs well for separating examples, as we confirm in our ablation studies.

**Step 2: One-shot upsampling of slow-learnable features.** Next, we upsample examples that are not in the cluster of points containing fast-learnable features. This speeds up learning slow-learnable features and encourages the model to learn different features at a more uniform speed. Thus, it improves the in-distribution performance based on Theorem 3.5. As discussed earlier, the number of times we upsample these examples should change based on the model weight at each iteration. Hence, a multi-stage clustering and sampling can yield the best results. Nevertheless, we empirically confirm that a 1-shot algorithm that finds fast-learnable examples at an _early_ training iteration and upsample the remaining examples by a factor of \(k=2\) effectively improves the performance. Notably, in contrast to dynamic sampling or reweighting, USEFUL upsamples examples only once and restart training on the modified but fix distribution.

**When to separate the examples.** It is crucial to separate examples _early_ in training, to accurately identify examples that contribute the most to simplicity bias. We empirically verify the intuition that the optimal epoch \(t\) to separate examples is when the change in training error starts to shrink as visualized in Figure 14(a). More details can be found in Appendices C.2 and D.8.

The pseudocode of USEFUL is illustrated in Alg. 1 and the workflow is shown in Appendix Fig. 7.

Figure 3: **GD (blue) vs. SAM (orange) on toy datasets.** Data is generated based on Definition 3.1 with different \(_{d}\) and fixed \(_{e}=1,\)\(=0.9\). \(\) and \(--\) lines denote the alignment (i.e., inner product) of fast-learnable (\(_{e}\)) and slow-learnable (\(_{d}\)) features with the model weight (\(_{j}^{(t)}\)). (a), (b) GD and SAM first learn the fast-learnable feature. Notably, GD learns the fast-learnable feature very early. (c) Test accuracy of GD & SAM improves by increasing the strength of the slow-learnable feature.

Figure 2: TSNE visualization of output vectors. (left) ResNet18/CIFAR-10 at epoch 8. (right) CNN/toy data generated based on Definition 3.1 with \(_{d}=0.2,_{e}=1,=0.9\), iteration 200.

## 5 Experiments

**Outline.** In Sec. 5.1, we empirically validate our theoretical results on toy datasets. We then evaluate the performance of USEFUL on several real-world datasets in Sec. 5.2 and different model architectures in Sec. 5.3. In addition, Sec. 5.3 highlights the advantages of USEFUL over random upsampling. Furthermore, we show that USEFUL shares several properties with SAM in Sec. 5.4. Additional experimental results are deferred to Appendix D where we show that USEFUL also boosts the performance of other SAM variants, and present promising results for USEFUL applied to the OOD setting (spurious correlation, long-tail distribution), transfer learning, and label noise settings. We further conduct ablation studies on the effect of our data selection strategy for upsampling, training batch size, learning rate, upsampling factor, and separating epoch in Appendix D.8.

**Settings.** We used common datasets for image classification including CIFAR10, CIFAR100 , STL10 , CINIC10 , and Tiny-ImageNet . Both CINIC10 and Tiny ImageNet are large-scale datasets containing images from the ImageNet dataset . We trained ResNet18 on all datasets except for CIFAR100 on which we trained ResNet34. We closely followed the setting from  in which our models are trained for 200 epochs with a batch size of 128. We used SGD with the momentum parameter of 0.9 and set weight decay to 0.0005. We also fixed \(=0.1\) for SAM in all experiments unless explicitly stated. We used a linear learning rate schedule starting at 0.1 and decay by a factor of 10 once at epoch 100 and again at epoch 150. More details are given in Appendix C.

### Toy Datasets

**Datasets.** Following , our toy dataset consists of training and test sets, each containing 10K examples generated from the data distribution defined in 3.1 with dimension \(d\!=\!50\) and \(P\!=\!3\). We set \(_{e}\!=\!1,_{d}\!=\!0.2,\!=\!0.9\), and \(_{p}/\!=\!0.125\). We also consider a scenario with larger \(_{d}\!=\!0.4\). We shuffle the order of patches randomly to confirm that our theory holds with arbitrary order of patches.

**Training.** We used the two-layer nonlinear CNN in Section 3.1 with \(J=40\) filters. For GD, we set the learning rate to \(=0.1\) and did not use momentum. For SAM, we used the same base GD optimizer and chose a smaller value of the inner step, \(=0.02\), than other experiments to satisfy the constraint in Theorem 3.4. We trained the model for 600 iterations till convergence for GD and SAM.

**Results.** Figure 2(a) illustrates that both GD (blue) and SAM (orange) first learn the fast-learnable feature. In particular, the blue dotted line (\(G_{e}\)) accelerates quickly at around epoch 250 while the orange dotted line (\(S_{e}\)) increases drastically much later at around epoch 450. That is: _(1) GD learns the fast

Figure 4: **Test classification error of ResNet18 on CIFAR10, STL10, TinyImageNet and ResNet34 on CIFAR100. The numbers below bars indicate the approximate training cost and the tick on top shows the std over three runs. USEFUL enhances the performance of SGD and SAM on all 5 datasets. TrivialAugment (TA) further boosts SAM’s performance (except for CINIC10). Remarkably, USEFUL consistently boosts the performance across all scenarios and achieves (to our knowledge) SOTA performance for ResNet18 and ResNet34 on the selected datasets when combined with SAM and TA.**_learnable feature very early in training._ This is well-aligned with our Theorems 3.2 and 3.3 and their discussion. Furthermore, the gap between contribution of fast-learnable and slow-learnable features towards the model output in SAM \((S_{e}^{(t)}-S_{d}^{(t)})\) is much smaller than that of GD \((G_{e}^{(t)}-G_{d}^{(t)})\). That is: _(2) fast-learnable and slow-learnable features are learned more evenly in SAM._ This validates our Theorem 3.4. From around epoch 500 onwards, the contribution of the slow-learnable feature in SAM surpasses the level of that in GD while the contribution of the fast-learnable feature in SAM is still lower than the counterpart in GD. When increasing the slow-learnable feature strength \(_{d}\) from 0.2 to 0.4 in Figure 2(b), the same conclusion for the growth speed of fast-learnable and slow-learnable features holds. Notably, there is a clear increase in the classification accuracy of the model trained with either GD or SAM by increasing \(_{d}\), as can be seen in Figure 2(c). That is: _(3) amplifying the strength of the slow-learnable feature improves the generalization performance._ Effectively, this enables the model successfully predict examples in which the fast-learnable feature is missing.

### USEFUL is Effective across Datasets

Figure 4 illustrates the performance of models trained with SGD and SAM on original vs modified data distribution by USEFUL. We see that USEFUL effectively reduces the test classification error of both SGD and SAM. Interestingly, USEFUL further improves SAM's generalization performance by reducing its simplicity bias. Notably, on the STL10 dataset, USEFUL boosts the performance of SGD to surpass that of SAM. The percentages of examples found for upsampling by USEFUL for CIFAR10, CIFAR100, STL10, CINIC10, and Tiny-ImageNet are roughly 30%, 50%, 50%, 45%, and 60%, respectively. Thus, training SGD on the modified data distribution only incurs a cost of 1.3x, 1.5x, 1.5x, 1.45x, and 1.6x compared to 2x of SAM.

**USEFUL+TA is particularly effective.** Stacking strong augmentation methods e.g. TrivialAugment  further improves the performance, achieving state-of-the-art for ResNet on all datasets. When strong augmentation is combined with USEFUL, it makes more variations of the (upsampled) slow-learnable features and enhances their learning. Hence, it further boost the performance.

### USEFUL is Effective across Architectures & Settings

**Model architectures: CNN, ViT, MLP.** Next, we confirm the versatility of our method, by applying it to different model architectures including 3-layer MLP, CNNs (ResNet18, VGG19, DenseNet121), and Transformers (ViT-S). Figure 5 shows that USEFUL is effective across different model architectures. Remarkably, when applying to non-CNN architectures, it reduces the test error of SGD to a lower level than that of SAM alone. Detailed results for 3-layer MLP is given in Appendix D.2.

**Settings: batch-size, learning rate, and SAM variants.** In Appendix D, we confirm the effectiveness of USEFUL for different batch sizes of 128, 256, 512, and different initial learning rates of 0.1, 0.2, 0.4. In Appendix D.4, we confirm that USEFUL applied to ASAM --a SAM variant which uses a scale-invariant sharpness measure--further reduces the test error.

**USEFUL vs Random Upsampling.** Fig. 6 shows that USEFUL considerably outperforms SGD and SAM on randomly upsampled CIFAR10 & CIFAR100. This confirms that the main benefit of USEFUL is due to the modified distribution and not longer training time. In Appendix D.8, we also confirm that upsampling outperforms upweighting for SAM & SGD.

Figure 5: **Test classification errors of different architectures on CIFAR10. USEFUL improves the performance of SGD and SAM when training different architectures. TrivialAugment (TA) further boosts SAM’s capabilities. The results for 3-layer MLP can be found in Figure 9.**

### USEFUL's Solution has Similar Properties to SAM

**SAM & USEFUL Find Sparser Solutions than SGD.** showed that SAM's solution has a better sparsity-inducing property indicated by the L1 norm than the standard ERM. Fig. 10 shows the L1 norm of ResNet18 trained on CIFAR10 and ResNet34 trained on CIFAR100 at the end of training. We see that USEFUL drives both SGD and SAM to find solutions with smaller L1 norms.

**SAM & USEFUL Find Less Sharp Solutions than SGD.** While our goal is not to directly find a flatter minimum or the same solution as SAM, we showed that USEFUL finds flatter minima. Following , we used the maximum Hessian eigenvalue (\(_{max}\)) and the bulk of the spectrum (\(_{max}/_{5}\)) , which are commonly used metrics for sharpness [10; 31; 37; 74]. Table 1 illustrates that SGD+USEFUL on CIFAR10 reduces sharpness metrics significantly compared to SGD, proving that USEFUL successfully reduces the sharpness of the solution. We note that to capture the sharpness/flatness, multiple different criteria have been proposed (largest Hessian eigenvalue and bulk of Hessian), and one criterion is not enough to accurately capture the sharpness. While the solution of SGD+USEFUL has a higher largest Hessian eigenvalue than SAM, it achieves the smallest bulk.

**SAM & USEFUL Reduce Forgetting Scores.** Forgetting scores  count the number of times an example is misclassified after being correctly classified during training and is an indicator of the learning speed and difficulty of examples. We show in Appendix D.3 that both SAM and USEFUL successfully reduce the forgetting scores, thus learn slow-learnable features faster than SGD. This aligns with our Theorem 3.4 and results on the toy datasets. By upsampling slow-learnable examples in the dataset, they contribute more to learning and hence SGD+USEFUL learns them faster than SGD.

**USEFUL also Benefits Distribution Shift.** While our main contribution is providing a novel and effective method to improve the in-distribution generalization performance, we conduct experiments confirming the benefits of our method to distribution shift. We discuss this experiment and its results in Appendix D.5. On Waterbirds dataset  with strong spurious correlation (95%), both SAM and USEFUL successfully improve the performance on the balanced test set by 6.21% and 5.8%, respectively. We also show the applicability of USEFUL to fine-tuning a ResNet50 pre-trained on ImageNet.

## 6 Conclusion

In this paper, we made the first attempt to improve the in-distribution generalization performance of machine learning methods by modifying the distribution of training data. We first analyzed learning dynamics of sharpness-aware minimization (SAM), and attributed its superior performance over GD to mitigating the simplicity bias, and learning features at a more speed. Inspired by SAM, we upsampled the examples that contain slow-learnable features to alleviate the simplicity bias. This allows learning features more uniformly, thus improving the performance. Our method boosts the performance of image classifiers trained with SGD or SAM and easily stacks with data augmentation.