# AdaNeg: Adaptive Negative Proxy Guided

OOD Detection with Vision-Language Models

 Yabin Zhang

The Hong Kong Polytechnic University

csybzhang@comp.polyu.edu.hk &Lei Zhang

The Hong Kong Polytechnic University

cslzhang@comp.polyu.edu.hk

Corresponding Author.

###### Abstract

Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance. However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images. To overcome this issue, we introduce _adaptive negative proxies_, which are dynamically generated during testing by exploring actual OOD images, to align more closely with the underlying OOD label space and enhance the efficacy of negative proxy guidance. Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution. This facilitates the creation of proxies that can better align with specific OOD datasets. While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances. The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. Our method is training-free and annotation-free, and it maintains fast testing speed. Extensive experiments across various benchmarks demonstrate the effectiveness of our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a 2.45% increase in AUROC and a 6.48% reduction in FPR95. Codes are available at https://github.com/YBZh/OpenOOD-VLM.

## 1 Introduction

In real applications, artificial intelligence (AI) systems often encounter test samples of unknown classes, termed out-of-distribution (OOD) data. These OOD data often result in overly confident errors , posing security threats. Therefore, accurately identifying OOD data is essential for ensuring the reliability and security of AI systems in open-world environments.

Traditional OOD detection methods in image domain primarily rely on vision-only models . Recent advancements in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by leveraging multi-modal knowledge . Recently, NegLabel  explores negative labels by identifying text labels that are semantically distant from the in-distribution (ID) labels. This method achieves state-of-the-art performance by detecting test images closer to negative labels as OOD. In other words, NegLabel regards these negative labels as proxies of OOD data. However, employing consistent negative labels across different OOD datasets often leads to semantic misalignment, where these text labels may not accurately reflect the actual label spaceof OOD images, as shown in Fig. 1. This misalignment between the proxies and targeted OOD distribution leads to sub-optimal performance.

To promote the alignment between the negative proxies and target OOD distribution, we introduce the **Ad**aptive **N**egative proxies (**AdaNeg**), which are dynamically generated during testing by exploring actual OOD images. Specifically, we start by initializing an empty category-split memory bank for each OOD dataset and selectively cache features of discriminative OOD images during testing. The OOD discrimination is assessed using mined negative labels, as detailed in . With this feature memory, we develop task-adaptive proxies by simply averaging cached features within each category. These proxies, derived from actual OOD images, reflect the distinct characteristics of the target OOD dataset and align more closely with the underlying OOD label space.

The task-adaptive proxies mentioned previously provide unique proxies for different OOD datasets while maintaining consistency across various test samples within the same dataset. To delve into the fine-grained nuances at the sample level, we introduce the sample-adaptive proxies by weighting cached features based on their similarity to a particular test sample. This is achieved with an attention mechanism, where the feature memory serves as both keys and values, and the test feature acts as the query. The final score for detecting OOD samples integrates static negative labels with our adaptive proxies, effectively combining textual and visual knowledge for enhanced performance.

We conduct extensive experiments on standard benchmarks to validate the effectiveness of AdaNeg, where our proposed adaptive proxies outperform the negative-label-based one, enhancing performance with complementary multi-modal knowledge. Particularly, on the large-scale ImageNet dataset, our AdaNeg method outperforms existing methods by 2.45% AUROC and 6.48% FPR95. Notably, our method is training-free and annotation-free, and it maintains fast testing speed, as analyzed in Tab. 4. The ability to dynamically adjust to new OOD datasets without affecting testing speed or labor-intensive annotation/training makes our approach particularly valuable for real-world applications where adaptability and efficiency are crucial. We summarize our contribution as follows:

* We first identify the label space misalignment between existing negative-label-based proxies and the target OOD distributions. In response, we introduce adaptive negative proxies that are dynamically generated during testing by exploring actual OOD images, resulting in a more effective alignment with the OOD label space.
* Our adaptive negative proxies are constructed with a feature memory bank that selectively caches discriminative image features during testing. We instantiate this concept by developing task-adaptive proxies to reflect the unique characteristics of each OOD dataset and

Figure 1: Qualitative and quantitative analyses of semantic misalignment between OOD labels and negative proxies using ImageNet (ID) and SUN (OOD) datasets. (a) Visualization of ID labels, OOD labels, negative labels from NegLabel, and adaptive negative proxies (AdaNeg). (b) Quantitative analysis based on ID-Similarity to OOD Ratio (ISOR in short, see Appendix A.1). Lower ISOR indicates a higher similarity to OOD labels and reduced similarity to ID labels. AdaNeg consistently achieves lower ISOR, demonstrating enhanced alignment with OOD characteristics. Visualizations include the top 1,000 discriminative proxies from both NegLabel and AdaNeg.

sample-adaptive proxies to capture detailed sample-level nuances. The final OOD detection score combines these insights with complementary textual and visual knowledge.
* We conduct thorough analyses of the proposed components and perform extensive experiments on standard benchmarks. Our method is training-free and annotation-free, and it maintains fast testing speed and achieves state-of-the-art performance. Notably, our method significantly outperforms existing methods, with a 2.54% increase in AUROC and a 6.48% reduction in FPR95 on the large-scale ImageNet dataset.

## 2 Related Work

**OOD Detection** focuses on identifying OOD test samples with semantic shifts, thus distinguishing it from generalization studies which typically focus on covariate shifts [3; 4; 5; 75]. A variety of OOD detection techniques have been developed, which can be roughly categorized into score-based [20; 33; 36; 37; 66; 26; 64; 56], distance-based [58; 59; 57; 12; 41; 53], and generative-based [50; 29] methods. Among them, score-based methods are particularly notable by employing a variety of scoring functions to differentiate between ID and OOD samples. These functions include confidence-based [20; 36; 56; 64], discriminator-based , energy-based [37; 66], and gradient-based  scores. In contrast, distance-based methods determine OOD samples by evaluating the distance in the feature space between test data and the closest ID samples  or ID prototypes , using metrics such as KNN [57; 12; 41] or Mahalanobis distance [33; 53].

Despite their achievements, traditional OOD detection methods generally rely on manually annotated ID images and often overlook the integration of textual information. To leverage the textual knowledge, recent advancements have focused on employing VLMs [39; 40; 27; 77; 76; 15; 42; 65; 45]. Specifically, ZOC  applies VLMs to discern OOD instances by training a captioner that generates potential OOD labels. Nevertheless, this captioner often fails to produce effective OOD labels, particularly for ID datasets containing many classes. LoCoOp  adopts a novel approach by learning ID prompts from few-shot ID samples, and further enhances the robustness of these prompts by incorporating OOD features mined from the backgrounds of images. CLIPN , LSN  and LAPPT  explore learning text prompts for expressing negative concepts. In specific, CLIPN initializes text prompts with the word 'no' combined with ID labels and refines them with large-scale multi-modal data, LSN starts with manually collected ID samples to learn negative prompts, offering a different approach to leveraging textual information in OOD detection; LAPT conducts automated prompt tuning with automatically collected training samples, boosting OOD detection without any manual effort. MCM  utilizes ID class names to facilitate effective zero-shot OOD detection. It is further refined by NegLabel , which incorporates additional negative class names mined from available data sources as negative proxies. However, as illustrated in Fig. 1, there is a mismatch between the negative-label-based proxies and the target OOD distribution, underscoring the limitations of this strategy. This observation has inspired us to construct adaptive proxies by exploring potential OOD test images during testing. This leads to an efficient method that aligns better with the target OOD distribution, resulting in enhanced OOD detection performance.

Furthermore, we clarify the relationship between our method and existing approaches on OOD exposure [17; 21; 73]. Most OOD exposure methods introduce manually collected negative images during training, where manual labor is necessary to ensure that the labels of negative images are different from ID ones. Moreover, involving negative images in training typically introduces additional computational overhead, impeding its practical deployment. Unlike these methods, NegLabel  is exposed to negative labels during the test phase in a training-free manner. However, given a fixed ID dataset, the exposed negative texts remain consistent for different OOD datasets, inevitably resulting in label misalignment, as shown in Fig. 1. To address this, we expose the VLMs to adaptive negative proxies, which explore actual OOD samples during testing and align more effectively with OOD distribution. Our method does not require manual annotations and works in a training-free manner, making it an appealing solution for real applications.

**Test-time Adaptation.** We adopt an online update of the negative proxies during testing, resembling test-time adaptation (TTA) methods [35; 63; 54]. Existing TTA methods primarily address covariate shifts between training and testing domains. In contrast, our approach mitigates the label shift between negative proxies and the target OOD distribution by exploring online test samples. Recently, TTA strategies have been considered in the field of OOD detection. However, these methods typically require test-time optimization [18; 72; 16], slowing down the testing process. In contrast, our method is optimization-free and introduces only a lightweight memory interaction operation, enabling rapid and accurate testing, as analyzed in Tab. 4.

**Memory Networks.** The use of memory networks for storing and retrieving past knowledge [67; 55] has been extensively applied across various fields, including classification [28; 51; 77], segmentation [46; 69], detection [8; 6], and NLP . To our best knowledge, our work is the first to apply memory networks to the field of OOD detection. By caching and retrieving test images with a feature memory, we propose adaptive proxies to more effectively align with the OOD distribution in a training-free manner. This innovative approach significantly enhances the OOD detection performance.

## 3 Methodology

### Preliminaries

**OOD Detection Setup.** Consider \(\) as the image domain and \(=\{y_{1},,y_{C}\}\) as the space of ID class labels, where \(\) comprises text elements such as \(=\{cat,dog,,bird\}\), and \(C\) represents the total number of classes. Let \(^{in}\) and \(^{ood}\) be random variables representing ID and OOD samples from \(\), respectively. We define \(^{in}\) and \(^{ood}\) as the marginal distributions for ID and OOD, respectively. In conventional classification scenarios, it is assumed that the test image \(\) originates from the ID and is associated with a specific ID label, specifically, \(^{in}\) and \(y\), with \(y\) being the label of \(\). However, in real applications, AI systems often face data from unknown classes, denoted by \(^{ood}\) and \(y\). Such occurrences can make AI models incorrectly categorize these instances into familiar ID categories with substantial certainty [52; 44], resulting in security concerns. To address these challenges, OOD detection is proposed to accurately categorize ID samples into their respective classes and reject OOD samples as non-ID. Recognition within the ID categories is performed using a \(C\)-way classifier, following standard classification approaches [31; 19]. Concurrently, OOD detection typically employs a scoring mechanism \(S\)[33; 36; 37] to differentiate between ID and OOD inputs:

\[G_{}()=,\,\,S();,\,G_{}()=,\] (1)

where \(G_{}\) represents the OOD detector set at a threshold \(\). The test sample \(\) is identified as an ID sample if and only if \(S()\).

**CLIP and NegLabel.** For an ID test image \(\) within the label space \(\), we derive the image feature vector \(=f_{img}()^{D}\) and the text feature matrix \(_{id}=f_{txt}(())^{C D}\) using pre-trained CLIP encoders, where \(D\) represents the feature dimension. The functions \(f_{img}()\) and \(f_{txt}()\) are the encoders for images and text, respectively. The function \(()\) is the text prompt mechanism, typically defined as 'a photo of a <label>.', where label is the actual class name, for example, 'cat' or 'dog'. Both \(\) and \(_{id}\) undergo \(L_{2}\) normalization across the dimension \(D\). Then, zero-shot classification probabilities are computed utilizing \(\) as the classifier:

\[^{id}=(_{id}^{T}/)^{C},\] (2)

Figure 2: The overall framework of AdaNeg, where we selectively cache test images and generate adaptive proxies with an external feature memory bank. The final score combines textual and visual knowledge from static negative labels and our adaptive proxies, integrating multi-modal information.

where \(>0\) is the scaling temperature.

The vanilla CLIP is proposed for zero-shot ID recognition and has recently been extended to OOD detection. Specifically, the NegLabel approach  introduces negative class names \(^{-}=\{y_{C+1},,y_{C+M}\}\) sourced from broad text corpora, where \(M\) is the length of negative classes and \(^{-}=\). Then, we can obtain the full text feature matrix \(=f_{txt}((^{-}))^{(C+M)  D}\) with the pre-trained CLIP text encoder, leading to the classification probability across \(C+M\) classes:

\[=(^{T}/)^{C+M}.\] (3)

Assuming that ID samples exhibit greater similarity to ID labels and lesser similarity to negative labels compared to OOD samples, NegLabel introduces the following score for OOD detection:

\[S_{nl}()=_{i=1}^{C}_{i},\] (4)

where \(_{i}\) is the \(i\)-th entry of \(\), indicating the classification probability of the \(i\)-th class. Intuitively, the NegLabel method uses negative labels as proxies of the OOD distribution, detecting OOD images based on the similarity to these negative labels.

### AdaNeg

Although NegLabel has successfully employed negative labels as the OOD proxies, there is a semantic misalignment between such OOD proxies and actual OOD labels, as illustrated in Fig. 1. We aim to obtain OOD proxies that align better to the targeted OOD distribution. However, acquiring such OOD proxies is challenging, as the OOD information is unknown prior to actual testing.

From another perspective, we can access real OOD information during testing, motivating us to refine OOD proxies in the testing stage. We can identify discriminative negative images during testing and then adjust the OOD proxies toward detected images. This is achieved by selectively caching features of test images into a task-aware memory bank, followed by memory reading operations to produce adaptive proxies. We detail our implementation as follows.

**Task-aware Memory.** We construct a task-aware memory as a category-split tensor \(^{(C+M) L D}\), where \(L\) is the memory length for each category. \(\) is initialized with zero values and gradually filled with features of selected images during testing. Specifically, for a test image with feature \(\), we first calculate its score \(S_{nl}()\) with Eq. (4). If \(S_{nl}()<\), we detect this test point as a negative image, and otherwise, it is identified as a positive sample. For negative and positive images, we respectively identify their closest labels as:

\[:y= *{arg\,max}_{i}_{i}^{ood}+C,\] (5) \[:y= *{arg\,max}_{i}_{i}^{id},\] (6)

where \(^{ood}=[C:C+M]\) and \(^{id}=[0:C]\) are the classification probabilities corresponding to negative and ID class names, respectively. Then, we cache this feature \(\) into the task-aware memory:

\[_{y,l}=,\] (7)

where \(l\) indicates an empty slot of \(_{y}^{L D}\). Once \(_{y}\) is filled, we drop the image feature with the highest prediction entropy, as detailed in Appendix A.2. In one word, we keep confident image features with low prediction entropy in the memory.

The aforementioned strategy attempts to cache all test images, including those with high confusion between ID and OOD. However, we found that selectively retaining only those image features that exhibit strong ID/OOD distinguishing capabilities can effectively reduce this confusion. Specifically, we modify the selection criterion for memorization as follows:

\[:S_{nl}()< S_{nl}()< -g,\] \[:S_{nl}() S_{nl}() +g(1-),\] (8)

where \(g\) is a hyperparameter that introduces a gap in the score space. Consequently, image features falling within the gap \(-g S_{nl}()<+g(1-)\) are considered to have low distinguishing confidence and are not cached.

**Task-adaptive Proxies.** Given the updated \(\), we can easily get the task-adaptive proxy by averaging along the length dimension \(L\):

\[^{ta}=_{2}(_{l=1}^{L+1}}_{:,l,:})^{(C+M) D},\] (9)

where \(_{2}()\) indicates the \(L_{2}\) normalization along feature dimension \(D\), and \(}=[,]^{(C+M)(L+1) D}\) is the extended memory with vanilla text proxies \(\). Such a memory extension is necessary since \(\) is initially empty and uninformative. Initially, this extension initializes the adaptive proxies \(^{ta}\) with the basic text proxies \(\). However, there are two key distinctions between the adaptive \(^{ta}\) and the vanilla \(\). First, unlike the NegLabel approach, which employs a fixed proxy \(\) across various OOD datasets, our \(^{ta}\) dynamically adjusts to the targeted OOD domain as the memory accumulates data, thereby providing dataset-specific adaptive proxies. Second, the \(^{ta}\) primarily incorporates image features, offering modal knowledge that complements the text-based proxies \(\). The benefits of this approach are further analyzed in Tab. 3.

The score function for OOD detection with the task-adaptive proxy \(^{ta}\) is derived as:

\[S_{ta}()=^{C}e^{(,_{i}^{ta})/}}{ _{i=1}^{C}e^{(,_{i}^{ta})/}+_{j=C+1}^{C+M}e^{(,_{j}^{ta})/}},\] (10)

where \((,)\) measures the cosine similarity, and \(_{i}^{ta}\) is the \(i\)-th entry of \(^{ta}\).

**Sample-adaptive Proxies.** As evidenced in Table 3, our task-adaptive proxies significantly outperform the fixed text proxies by effectively adapting to the characteristics of target OOD dataset. Building on this success, we further refine our approach by leveraging finer-grained, sample-level nuances to introduce even more effective sample-adaptive proxies. Specifically, given the extended memory \(}\) and the test image feature \(\), we introduce the sample-adaptive proxy \(^{sa}^{(C+M) D}\) via the following cross-attention operation:

\[_{i}^{sa}=_{2}(((}_{i}) ^{})}_{i})^{D},\] (11)

where \((}_{i})^{}^{1(L+1)}\) measures the cosine similarities between normalized features of \(\) and \(}_{i}\), and \((x)=(-(1-x))\) modulates the sharpness of \(x\) with hyper-parameter \(\). \(_{i}^{sa}\) and \(}_{i}\) are the \(i\)-th entry of \(^{sa}\) and \(}\), respectively.

Both the task-adaptive and the sample-adaptive proxies are derived from the memorized image features stored in \(}\). The primary distinction between them lies in their respective weighting strategies. For \(^{ta}\), each feature \(}_{:,l,:}\) in the memory is assigned a uniform weighting coefficient of \(\). Conversely, in constructing \(^{sa}\), the weighting coefficient for each cached feature is dynamically determined based on its cosine similarity to the test image feature, denoted as \((}_{i})^{}\). Consequently, while \(^{ta}\) remains constant across different test samples, \(^{sa}\) adapts to each individual test sample. This adaptability allows \(^{sa}\) to tailor its response based on the specific characteristicsof each test image, thereby enhancing discrimination between ID and OOD samples, particularly in diverse and variable testing scenarios.

The score function for OOD detection with the sample-adaptive proxies \(^{sa}\) is derived as:

\[S_{sa}()=^{C}e^{(,c_{i}^{sa})/}}{_{i=1}^{ C}e^{cos(,c_{i}^{sa})/}+_{j=C+1}^{C+M}e^{(,c_{j}^{sa})/ }}.\] (12)

**Multi-modal Score.** As previously discussed, the score function \(S_{nl}()\) relies primarily on text features, whereas the sample-adaptive score function \(S_{sa}()\) utilizes cached image features. Given the complementary nature of text and image modalities, we derive the final score function by integrating knowledge from both modalities:

\[S_{all}()=S_{nl}()+ S_{sa}(),\] (13)

where \(>0\) is the hyperparameter balancing the two modalities. The overall pipeline of our method is illustrated in Fig. 2 and summarized in Algorithm 1.

## 4 Experiments

### Setup

**Datasets.** We conduct extensive experiments with the large-scale ImageNet-1k  as ID data. Following prior practice [26; 39; 27], four OOD datasets of iNaturalist , SUN , Places , and Textures  are evaluated. We also validate our method on the OpenOOD benchmark [74; 71], where OOD datasets are grouped into near-OOD (_e.g._, SSB-hard , NINCO ) and far-OOD (_e.g._, iNaturalist , Textures , OpenImage-O ) according to their similarity to ImageNet dataset. Besides ImageNet, we also evaluate our method on smaller-sized CIFAR10/100 datasets  with the OpenOOD setup. Specifically, with the ID dataset of CIFAR10/100, we adopt near-OOD datasets of CIFAR100/10 and TIN , and far-ood datasets of MNIST , SVHN , Texture , and Places365 . These experiments with various ID and OOD datasets enable a comprehensive evaluation on various OOD settings.

**Implementation Details.** We adopt the visual encoder of VITB/16 pretrained by CLIP  and analyze more backbone architectures in Tab. A11. For hyper-parameters, we adopt the memory length \(L\)=10, threshold \(\)=0.5 with the gap \(g\)=0.5 in Eq. 8, \(\)=5.5 in Eq. 11, and \(\)=0.1 in Eq. 13 in all experiments. These hyper-parameters are carefully analyzed in Sec. 4.3. Following NegLabel, we adopt the text prompt of 'The nice <label>.', set temperature \(\)=0.01, and define the number \(M\) of negative labels as \(10,000\) for the ImageNet dataset. For the CIFAR datasets, we set the number \(M\) as \(70,000\) since we find that a larger \(M\) leads to better results for CIFAR. Following common practice [26; 39; 27], we employ the evaluation metrics of FPR95, AUROC, and ID ACC, which are detailed in Appendix A.3. All experiments are conducted with a single Tesla V100 GPU.

### Main Results

**ImageNet Results with Four OOD Datasets.** As illustrated in Tab. 1, our AdaNeg significantly outperforms existing training-free methods and even surpasses approaches requiring additional training. Specifically, we report traditional methods [20; 36; 37; 25; 64; 57; 13; 59] by fine-tuning CLIP-encoders with labeled training samples following , and report results of [45; 42; 27; 34; 1] from their original paper. Compared to the closest competitor , our method achieves consistent and notable improvements, validating the advantage of the proposed adaptive proxies over the negative-label-based ones.

**ImageNet Results with OpenOOD Setup.** Our method is extensively evaluated against a range of OOD datasets in Tab. 2 on the OpenOOD benchmark. Competing methods that require training are referred from OpenOOD. These methods utilize the full ImageNet training set and hold an unfair advantage over zero-shot, training-free methods like ours. Our AdaNeg consistently outperforms its closest competitor  in both near-OOD and far-OOD scenarios. Additionally, AdaNeg not only enhances OOD detection capabilities but also improves ID classification, presenting higher robustness under diverse conditions.

Results on CIFAR10/100 datasets are provided in Appendix A.5, where our advantages still hold.

### Analyses and Discussions

**Score Functions.** As illustrated in Tab. 3, the adaptive score functions \(S_{ta}\) and \(S_{sa}\) consistently outperforms the fixed \(S_{nl}\), validating the advantage of adaptive proxies over fixed label proxies. The sample-adaptive score \(S_{sa}\) slightly surpasses the task-adaptive one \(S_{ta}\), verifying the usefulness of fine-grained sample characteristics. Combining adaptive image-based proxies and fixed text-based proxies leads to the best performance, proving their complementarity.

    &  \\   &  &  &  &  &  \\   & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) \\   \\ MSP  & 87.44 & 58.36 & 79.73 & 73.72 & 79.67 & 74.41 & 79.69 & 71.93 & 81.63 & 69.61 \\ ODN  & 94.65 & 30.22 & 87.17 & 54.04 & 85.54 & 55.06 & 87.85 & 51.67 & 88.80 & 47.75 \\ Energy  & 95.33 & 26.12 & 92.66 & 35.97 & 91.41 & 39.87 & 86.76 & 57.61 & 91.54 & 39.89 \\ GradNorm  & 72.56 & 81.50 & 72.86 & 82.00 & 73.70 & 80.41 & 70.26 & 79.36 & 72.35 & 80.82 \\ VIM  & 93.16 & 32.19 & 87.19 & 54.01 & 83.75 & 60.67 & 87.18 & 53.94 & 87.82 & 50.20 \\ KNN  & 94.52 & 29.17 & 92.67 & 35.62 & 91.02 & 39.61 & 85.67 & 64.35 & 99.97 & 42.19 \\ VOS  & 94.62 & 28.99 & 92.57 & 36.88 & 91.23 & 38.39 & 86.33 & 61.02 & 91.19 & 41.32 \\ NPOS  & 96.19 & 16.58 & 90.44 & 43.77 & 89.44 & 45.27 & 88.80 & 46.12 & 91.22 & 37.93 \\ ZOC  & 86.09 & 87.30 & 81.20 & 81.51 & 83.39 & 73.06 & 76.46 & 98.90 & 81.79 & 85.19 \\ LSN  & 95.83 & 21.56 & 94.35 & 26.32 & 91.25 & 34.48 & 90.42 & 38.54 & 92.96 & 30.22 \\ CLIPN  & 95.27 & 23.94 & 93.93 & 26.17 & 92.28 & 33.45 & 90.93 & 40.83 & 93.10 & 31.10 \\ LCoCo  & 96.86 & 16.05 & 95.07 & 23.44 & 91.98 & 32.87 & 90.19 & 42.28 & 93.52 & 28.66 \\ LAPT  & 99.63 & 1.16 & 96.01 & 19.12 & 92.01 & 33.01 & 91.06 & 40.32 & 94.68 & 23.40 \\ NegPrompt  & 98.73 & 6.32 & 95.55 & 22.89 & 93.34 & 27.60 & 91.60 & 35.21 & 94.81 & 23.01 \\   \\ Mahalanobis  & 55.89 & 99.33 & 59.94 & 99.41 & 65.96 & 98.54 & 64.23 & 98.46 & 61.50 & 98.94 \\ Energy  & 85.09 & 81.08 & 84.24 & 79.02 & 83.38 & 75.08 & 65.56 & 93.65 & 79.57 & 82.21 \\ MCM  & 94.59 & 32.00 & 92.25 & 38.80 & 90.31 & 46.20 & 86.12 & 58.50 & 90.82 & 43.93 \\ NegLabel  & 99.49 & 1.91 & 95.49 & 20.53 & 91.64 & 35.59 & 90.22 & 43.56 & 94.21 & 25.40 \\ **AdaNeg (Ours)** & **99.71** & **0.59** & **97.44** & **9.50** & **94.55** & **34.34** & **94.93** & **31.27** & **96.66** & **18.92** \\   

Table 1: OOD detection results with ImageNet-1k, where a VITB/16 CLIP encoder is adopted.

    &  &  &  \\   & Near-OOD & Far-OOD & Near-OOD & Far-OOD & ID \\   \\ GEN  & – & – & 78.97 & 90.98 & 81.59 \\ AugMix  + ReAct  & – & – & 79.94 & 93.70 & 77.63 \\ RMDS  & – & – & 80.09 & 92.60 & 81.14 \\ SCALE  & – & – & 81.36 & 96.53 & 76.18 \\ AugMix  + ASH  & – & – & 82.16 & 96.05 & 77.63 \\ LAPT  & 58.94 & 24.86 & 82.63 & 94.26 & 67.86 \\   \\ MCM  & 79.02 & 68.54 & 60.11 & 84.77 & 66.28 \\ NegLabel  & 69.45 & 23.73 & 75.18 & 94.85 & 66.82 \\ 
**AdaNeg (Ours)** & **67.51** & **17.31** & **76.70** & **96.43** & **67.13** \\   

Table 2: OOD detection results on the OpenOOD benchmark, where ImageNet-1k is adopted as ID dataset. Full results are available in Tab. A7.

   \(S_{nl}\) & \(S_{ta}\) & \(S_{sa}\) & Near-OOD AUROC \(\) & Far-OOD AUROC \(\) \\  ✓ & & & 75.18 & 94.85 \\  & ✓ & & 75.76 & 96.20 \\  & & ✓ & 76.03 & 96.35 \\  ✓ & ✓ & & 76.49 & 96.45 \\ ✓ & & & 76.70 & 96.63 \\    

Table 3: OOD detection results with different score functions, where results are reported with ImageNet ID dataset under the OpenOOD setup.

**Threshold \(\) and gap \(g\) in Eq. 8.** As illustrated in Fig. 3, employing a moderate threshold \(\) (_e.g._, 0.4 < \(\) < 0.6) proves effective in distinguishing OOD samples across various scenarios. However, the efficacy of the gap parameter \(g\) depends upon the specific characteristics of different OOD datasets. A larger \(g\) facilitates the identification of more confidently classified ID/OOD samples, thereby improving the detection of far-OOD samples. Conversely, a smaller \(g\) is advantageous for near-OOD detection as it accommodates low-confidence OOD samples, which are typically more prevalent in near-OOD scenarios.

In scenarios where the OOD distribution is entirely unknown, we adopt a conservative approach by setting \(g\) to 0.5 in all experiments. This balanced setting provides a robust baseline for performance across a variety of conditions. However, if prior knowledge regarding the difficulty levels of OOD datasets is accessible, tailoring the hyperparameters--such as opting for a smaller \(g\) in more challenging OOD contexts--can yield enhanced detection performance.

**ID-OOD imbalanced Test Data.** To investigate the stability of our method with imbalanced ID-OOD test data, we construct test sets with various mixture ratios of ID and OOD samples. Specifically, we adopted the 1.28M ImageNet training data as ID and randomly sampled 12.8K and 1.28K instances from the SUN OOD dataset to construct the ID:OOD ratios of 100:1 and 1000:1 settings, respectively. To construct the ID:OOD ratios of 1:100, 1:10, 1:1, and 10:1 settings, we randomly sampled 40K samples from the SUN dataset as OOD and randomly sampled 400, 4K, 40K, and 400K instances from the ImageNet training data. As shown in Tab. 5, our method outperforms NegLabel across a wide range of mixture ratios (from 1:100 to 100:1), validating the robustness and reliability of our approach. However, unbalanced mixture ratios do pose a challenge to our method. Our approach performs the best in scenarios with a balanced mixture of ID and OOD samples, reducing the FPR95 by 11.18%. As the mixture ratio becomes increasingly unbalanced, the improvement brought by our method gradually decreases. When the unbalanced ratio reaches 1000:1, our method shows some negative impact.

After a more detailed analysis, we discovered that the fundamental issue stems from the increased proportion of misclassified samples in the memory due to the growing ID-OOD imbalance. To effectively address this problem, we refine the selection criterion for memorizing OOD samples by adaptively adjusting the gap \(g\). We refer to this adaptive gap strategy as AdaGap, which significantly improves the robustness of our method against ID-OOD imbalanced test data, as demonstrated in

  
**Methods** & **Training** & **FPS** & **Param.** & **FPR95 \(\)** \\  ZOC  & \(>\)24h & 287 & 336M & 85.19 \\ CLIPN  & \(>\)24h & 605 & 37.8M & 31.10 \\ LoCoOp  & 9h & 625 & 8K & 28.66 \\  MCM  & – & 625 & – & 43.93 \\ NegLabel  & – & 592 & – & 25.40 \\
**AdaNeg (Ours)** & – & 476 & – & 18.92 \\   

Table 4: Analyses on the time complexity of our AdaNeg and its competitors. ’Training’ measures the training time, and ‘Param.’ presents the number of learnable parameters. ‘FPS’ reflects the inference speed, measured with a batch size of 256. Results are achieved with a NVIDIA V100 GPU.

Figure 3: Analyses on the hyper-parameters of (a) threshold \(\) in Eq. 8, (b) gap value \(g\) in Eq. 8, and (c) memory length \(L\) on the ImageNet dataset under OpenOOD setting.

Table 5. To summarize, we first estimate the ratio of ID to OOD in the test data online. If there is a higher proportion of ID/OOD compared to OOD/ID, we adjust the standards for caching ID/OOD data into memory by dynamically modifying the gap in Eq. 8. For more detailed implementation, please refer to the Appendix A.6.

**Generalization to Other Domains.** Besides experiments with natural images, we further validate our method on the BIMCV-COVID19+ dataset , which includes medical images, following the OpenOOD setup. Specifically, we select BIMCV as the ID dataset, which includes chest X-ray images CXR (CR, DX) of COVID-19 patients and healthy individuals. For the OOD datasets, we follow the OpenOOD setup and use CT-SCAN and X-Ray-Bone datasets. The CT-SCAN dataset includes computed tomography (CT) images of COVID-19 patients and healthy individuals, while the X-Ray-Bone dataset contains X-ray images of hands. As illustrated in Tab. 6, our AdaNeg method consistently outperforms NegLabel on this medical image dataset.

**Memory Length.** As demonstrated in Fig. 2(c), there is a positive correlation between the memory length \(L\) and the performance outcomes, affirming the efficacy of feature memorization from another perspective. In all our experiments, we have set \(L\) to 10.

**Complexity Analyses.** As analyzed in Table 4, our AdaNeg approach does not introduce any learnable parameters or require model training. Furthermore, it significantly enhances performance while maintaining a fast testing speed.

More analyses and discussions on the \(\) in Eq. 13, \(\) in Eq. 11, various backbone architectures, the ordering of test samples, complementarity to training-based method, and the number of test data can be found in Appendix A.6.

## 5 Conclusion and Limitations

We proposed adaptive negative proxies that aligned more effectively with OOD distributions, thereby providing more effective guidance for OOD detection. These proxies were constructed using a task-aware feature memory that selectively cached discriminative image features during testing. Our approach facilitated the generation of both task-adaptive and sample-adaptive proxies through carefully designed memory reading operations. Notably, our method was training-free and annotation-free, and it maintained fast testing speed and achieved state-of-the-art results on various benchmarks. These results validated the effectiveness of the proposed adaptive proxies.

One minor limitation of our method is the introduction of an external memory requirement. For example, this memory occupies a storage space of 214.75MB when using the ImageNet dataset as the ID, which may pose challenges for storage-constrained applications.

    &  \\   &  &  &  \\   & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) \\  NegLabel & 63.53 & 100 & 99.68 & 0.56 & 81.61 & 50.28 \\
**AdaNeg (Ours)** & **93.48** & 100 & **99.99** & **0.11** & **96.74** & **50.06** \\   

Table 6: OOD detection results with BIMCV-COVID19+ , where a VITB/16 CLIP encoder is adopted.

   ID:OOD Ratio & 1:100 & 1:10 & 1:1 & 10:1 & 100:1 & 1K:1 \\  NegLabel & 22.42 & 21.11 & 20.99 & 20.92 & 21.48 & 23.69 \\ AdaNeg & 21.00 & 12.49 & 9.81 & 15.61 & 20.71 & 26.28 \\ AdaNeg (With AdaGap) & 20.50 & 12.22 & 9.73 & 12.98 & 15.61 & 18.43 \\   

Table 5: FPR95 (\(\)) with different mixture ratios of ID and OOD samples.