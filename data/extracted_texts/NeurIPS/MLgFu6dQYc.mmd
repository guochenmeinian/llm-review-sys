# How to Boost Any Loss Function

Richard Nock

Google Research

richardnock@google.com &Yishay Mansour

Tel Aviv University

Google Research

mansour@google.com

###### Abstract

Boosting is a highly successful ML-born optimization setting in which one is required to computationally efficiently learn arbitrarily good models based on the access to a weak learner oracle, providing classifiers performing at least slightly differently from random guessing. A key difference with gradient-based optimization is that boosting's original model does not requires access to first order information about a loss, yet the decades long history of boosting has quickly evolved it into a first order optimization setting - sometimes even wrongfully _defining_ it as such. Owing to recent progress extending gradient-based optimization to use only a loss' zeroth (\(0^{th}\)) order information to learn, this begs the question: what loss functions can be efficiently optimized with boosting and what is the information really needed for boosting to meet the _original_ boosting blueprint's requirements? We provide a constructive formal answer essentially showing that _any_ loss function can be optimized with boosting and thus boosting can achieve a feat not yet known to be possible in the classical \(0^{th}\) order setting, since loss functions are not required to be be convex, nor differentiable or Lipschitz - and in fact not required to be continuous either. Some tools we use are rooted in quantum calculus, the mathematical field - not to be confounded with quantum computation - that studies calculus without passing to the limit, and thus without using first order information.

## 1 Introduction

In ML, zeroth order optimization has been devised as an alternative to techniques that would otherwise require access to \( 1\)-order information about the loss to minimize, such as gradient descent (stochastic or not, constrained or not, etc., see Section 2). Such approaches replace the access to a so-called _oracle_ providing derivatives for the loss at hand, operations that can be consuming or not available in exact form in the ML world, by the access to a cheaper function value oracle, providing loss values at queried points.

Zeroth order optimization has seen a considerable boost in ML over the past years, over many settings and algorithms, yet, there is one foundational ML setting and related algorithms that, to our knowledge, have not yet been the subject of investigations: boosting . Such a question is very relevant: boosting has quickly evolved as a technique requiring first-order information about the loss optimized [6, Section 10.3], [41, Section 7.2.2]. It is also not uncommon to find boosting reduced to this first-order setting . However, originally, the boosting model did not mandate the access to any first-order information about the loss, rather requiring access to a weak learner providing classifiers at least slightly different from random guessing . In the context of zeroth-order optimization gaining traction in ML, it becomes crucial to understand not just whether differentiability is necessary for boosting, but more generally what are loss functions that can be boosted with a weak learner and _in fine_ where boosting stands with respect to recent formal progress on lifting gradient descent to zeroth-order optimisation.

**In this paper**, we settle the question: we design a formal boosting algorithm for any loss function whose set of discontinuities has zero Lebesgue measure. With traditional floating point encoding (e.g. float64), any stored loss function would _de facto_ meet this condition; mathematically speaking, we encompass losses that are not necessarily convex, nor differentiable or Lipschitz. This is a key difference with classical zeroth-order optimization results where the algorithms are zeroth-order _but_ their proof of convergence makes various assumptions about the loss at hand, such as convexity, differentiability (once or twice), Lipschitzness, etc.. Our proof technique builds on a simple boosting technique for convex functions that relies on an order-one Taylor expansion to bound the progress between iterations . Using tools from quantum calculus*, we replace this progress using \(v\)-derivatives and a quantity related to a generalisation of the Bregman information . The boosting rate involves the classical weak learning assumption's advantage over random guessing and a new parameter bounding the ratio of the expected weights (squared) over a generalized notion of curvature involving \(v\)-derivatives. Our algorithm, which learns a linear model, introduces notable generalisations compared to the AdaBoost / gradient boosting lineages, chief among which the computation of acceptable _offsets_ for the \(v\)-derivatives used to compute boosting weights, offsets being zero for classical gradient boosting. To preserve readability and save space, all proofs and additional information are postponed to an Appendix.

Footnote *: Calculus “without limits”  (thus without using derivatives), not to be confounded with calculus on quantum devices.

## 2 Related work

Over the past years, ML has seen a substantial push to get the cheapest optimisation routines, in general batch , online , distributed , adversarial [20; 18] or bandits settings  or more specific settings like projection-free [26; 28; 51] or saddle-point optimisation [25; 38]. We summarize several dozen recent references in Table 1 in terms of assumptions for the analysis about the loss optimized, provided in Appendix, Section A. _Zeroth-order_ optimization reduces the information available to the learner to the "cheapest" one which consists in (loss) function values, usually via a so-called function value _oracle_. However, as Table 1 shows, the loss itself is always assumed to have some form of "niceness" to study the algorithms' convergence, such as differentiability, Lipschitzness, convexity, etc.. Another quite remarkable phenomenon is that throughout all their diverse settings and frameworks, not a single one of them addresses boosting. Boosting is however a natural candidate for such investigations, for two reasons. First, the most widely used boosting algorithms are first-order information hungry [6; 41; 53]: they require access to derivatives to compute examples' weights and classifiers' leveraging coefficients. Second and perhaps most importantly, unlike other optimization techniques like gradient descent, the original boosting model _does not_ mandate the access to a first-order information oracle to learn, but rather to a weak learning oracle which supplies classifiers performing slightly differently from random guessing [32; 31]. Only few approaches exist to get to "cheaper" algorithms relying on less assumptions about the loss at hand, and to our knowledge do not have boosting-compliant convergence proofs, as for example when alleviating convexity [16; 46] or access to gradients of the loss . Such questions are however important given the early negative results on boosting convex potentials with first-order information  and the role of the classifiers in the negative results .

Finally, we note that a rich literature has developed in mathematics as well for derivative-free optimisation , yet methods would also often rely on assumptions included in the three above (_e.g._). It must be noted however that derivative-free optimisation has been implemented in computers for more than seven decades .

## 3 Definitions and notations

The following shorthands are used: \([n]\{1,2,...,n\}\) for \(n_{}\), \(z[a,b][\{za,zb\},\{za,zb\}]\) for \(z\), \(a b\). In the batch supervised learning setting, one is given a training set of \(m\) examples \(S\{(_{i},y_{i}),i[m]\}\), where \(_{i}\) is an observation (\(\) is called the domain: often, \(^{d}\)) and \(y_{i}\{-1,1\}\) is a label, or class. We study the empirical convergence of boosting, which requires fast convergence on training. Such a setting is standard in zeroth order optimization . Also, investigating generalization would entail specific design choices about the loss at hand and thus would restrict the scope of our result (see _e.g._). The objective is tolearn a _classifier_, _i.e._ a function \(h:\) which belongs to a given set \(\). The goodness of fit of some \(h\) on \(S\) is evaluated from a given function \(F:\) called a loss function, whose expectation on training is sought to be minimized:

\[F(S,h)_{i[m]}[F(y_{i}h(_{i}))].\]

The set of most popular losses comprises convex functions: the exponential loss (\(F_{}(z)(-z)\)), the logistic loss (\(F^{}(z)(1+(-z))\)), the square loss (\(F_{}(z)(1-z)^{2}\)), the Hinge loss (\(F_{}(z)\{0,1-z\}\)). These are surrogate losses because they all define upperbounds of the 0/1-loss (\(F_{}(z) 1_{z 0}\), "1" being the indicator variable).

Our ML setting is that of boosting . It consists in having primary access to a weak learner wl that when called, provides so-called weak hypotheses, weak because barely anything is assumed in terms of classification performance relatively to the sample over which they were trained. Our goal is to devise a so-called "boosting" algorithm that can take any loss \(F\)_as input_ and training sample \(S\) and a target loss value \(F_{*}\) and after some \(T\) calls to the weak learner crafts a classifier \(H_{T}\) satisfying \(F(S,H_{T}) F_{*}\), where \(T\) depends on various parameters of the ML problem. Our boosting architecture is a linear model: \(H_{T}_{t}_{t}h_{t}\) where each \(h_{t}\) is an output from the weak learner and leveraging coefficients \(_{t}\) have to be computed during boosting. Notice that this is substantially more general than the classical boosting formulation where the loss would be fixed or belong to a restricted subset of functions.

## 4 \(v\)-derivatives and Bregman secant distortions

Unless otherwise stated, in this Section, \(F\) is a function defined over \(\).

**Definition 4.1**.: _[_30_]_ _For any \(z,v\), we let \(_{v}F(z)(F(z+v)-F(z))/v\) denote the \(v\)-derivative of \(F\) in \(z\)._

This expression, which gives the classical derivative when the _offset_\(v 0\), is called the \(h\)_-derivative_ in quantum calculus [30, Chapter 1]. We replaced the notation for the risk of confusion with classifiers. Notice that the \(v\)-derivative is just the slope of the secant that passes through points \((z,F(z))\) and \((z+v,F(z+v))\) (Figure 1). Higher order \(v\)-derivatives can be defined with the same offset used several times . Here, we shall need a more general definition that accommodates for variable offsets.

**Definition 4.2**.: _Let \(v_{1},v_{2},...,v_{n}\) and \(\{v_{1},v_{2},...,v_{n}\}\) and \(z\). The \(\)-derivative \(_{}F\) is:_

\[_{}F(z)\{F(z)&& =\\ _{v_{1}}F(z)&&=\{v_{1}\}\\ _{\{v_{n}\}}(_{\{v_{n}\}}F)(z)& ..\]

_If \(v_{i}=v, i[n]\) then we write \(_{v}^{(n)}F(z)_{}F(z)\)._

In the Appendix, Lemma B.1 computes the unravelled expression of \(_{}F(z)\), showing that the order of the elements in \(\) does not matter; \(n\) is called the order of the \(\)-derivative.

We can now define a generalization of Bregman divergences called _Bregman Secant distortions_.

**Definition 4.3**.: _For any \(z,z^{},v\), the Bregman Secant distortion \(S_{F|v}(z^{}\|z)\) with generator \(F\) and offset \(v\) is:_

\[S_{F|v}(z^{}\|z) F(z^{})-F(z)-(z^{}-z)_{v}F(z).\]

Even if \(F\) is convex, the distortion is not necessarily positive, though it is lowerbounded (Figure 1). There is an intimate relationship between the Bregman Secant distortions and Bregman divergences. We shall use a definition slightly more general than the original one when \(F\) is differentiable [11, eq. (1.4)], introduced in information geometry [5, Section 3.4] and recently reintroduced in ML .

**Definition 4.4**.: _The Bregman divergence with generator \(F\) (scalar, convex) between \(z^{}\) and \(z\) is \(D_{F}(z^{}|z) F(z^{})+F^{}(z)-z^{}z\), where \(F^{}(z)_{t}tz-F(t)\) is the convex conjugate of \(F\)._

We state the link between \(S_{F|v}\) and \(D_{F}\) (proof omitted).

**Lemma 4.5**.: _Suppose \(F\) strictly convex differentiable. Then \(_{v 0}S_{F|v}(z^{}\|z)=D_{F}(z^{}\|F^{}(z))\)._

Relaxed forms of Bregman divergences have been introduced in information geometry .

**Definition 4.6**.: _For any \(a,b,\), denote for short \(_{a,b}[\{a,b\},\{a,b\}]\) and \((uv)_{} u+(1-)v\). The Optimal Bregman Information (OBI) of \(F\) defined by triple \((a,b,c)^{3}\) is:_

\[Q_{F}(a,b,c)_{:(ab)_{}_{a,c}}\{(F(a)F(b))_ {}-F((ab)_{})\}.\] (1)

As represented in Figure 1 (right), the OBI is obtained by drawing the line passing through \((a,F(a))\) and \((b,F(b))\) and then, in the interval \(_{a,c}\), look for the maximal difference between the line and \(F\). We note that \(Q_{F}\) is non negative because \(a_{a,c}\) and for the choice \(=1\), the RHS in (1) is 0. We also note that when \(F\) is convex, the RHS is indeed the maximal Bregman information of two points in [7, Definition 2], where maximality is obtained over the probability measure. The following Lemma follows from the definition of the Bregman secant divergence and the OBI. An inspection of the functions in Figure 1 provides a graphical proof.

**Lemma 4.7**.: _For any \(F\),_

\[ z,v,z^{},S_{F|v}(z^{}\|z)-Q_{F}(z,z+v,z^ {}).\] (2)

_and if \(F\) is convex,_

\[ z,v, z^{}_{z,z+v},S_{F|v}( z^{}\|z) 0,\] \[ z,v,z^{},S_{F|v}(z^{}\|z)-Q_{ F}(z,z+v,z+v).\] (3)

We shall abbreviate the two possible forms of OBI in the RHS of (2), (3) as:

\[Q_{F}^{}(z,z^{},v)\{Q_{F}(z,z+v,z+v)& \\ Q_{F}(z,z+v,z^{})&..\] (4)

## 5 Boosting using only queries on the loss

We make the assumption that predictions of so-called "weak classifiers" are finite and non-zero on training without loss of generality (otherwise a simple tweak ensures it without breaking the weak learning framework, see Appendix, Section B.2). Excluding 0 ensures our algorithm does not make use of derivatives.

**Assumption 5.1**.: \( t>0, i[m]\)_, \(|h_{t}(_{i})|(0,+)\) (we thus let \(M_{t}_{i}|h_{t}(_{i})|\))._

For short, we define two _edge_ quantities for \(i[m]\) and \(t=1,2,...\),

\[e_{ti}=_{t} y_{i}h_{t}(_{i}),_{ti} y_{i}H _{t}(_{i}),\] (5)

where \(_{t}\) is a leveraging coefficient for the weak classifiers in an ensemble \(H_{T}(.)_{t[T]}_{t}h_{t}(.)\). We observe

\[_{ti}=_{(t-1)i}+e_{ti}.\]

### Algorithm: SecBoost

#### 5.1.1 General steps

Without further ado, Algorithm SecBoost presents our approach to boosting without using derivatives information. The key differences with traditional boosting algorithms are red color framed. We summarize its key steps.

**Step 1** This is the initialization step. Traditionally in boosting, one would pick \(h_{0}=0\). Note that \(_{1}\) is not necessarily positive. \(v_{0}\) is the initial offset (Section 4).

**Step 2.1** This step calls the weak learner, as in traditional boosting, using variable "weights" on examples (the coordinate-wise absolute value of \(_{t}\), denoted \(|_{t}|\)). The key difference with traditional boosting is that examples labels can switch between iterations as well, which explains that the training sample, \(S_{t}\), is indexed by the iteration number.

**Step 2.3** This step computes the leveraging coefficient \(_{t}\) of the weak classifier \(h_{t}\). It involves a quantity, \(_{2,t}\), which we define as any strictly positive real satisfying

\[_{i[m]}[_{\{e_{it},v_{(t-1)i}\}}F(_{(t-1)i}) ((_{i})}{M_{t}})^{2}]_ {2,t}.\] (8)

For boosting rate's sake, we should find \(_{2,t}\) as small as possible. We refer to (5) for the \(e_{},\). notations; \(v_{}\) is the current (set of) offset(s) (Section 4 for their definition). The second-order \(\)-derivative in the LHS plays the same role as the second-order derivative in classical boosting rates, see for example [45, Appendix, eq. 29]. As offsets \( 0\), it converges to a second-order derivative; otherwise, they still share some properties, such as the sign for convex functions.

**Lemma 5.2**.: _Suppose \(F\) convex. For any \(a,b,c_{s}\), \(_{\{b,c\}}F(a)>0\)._

(Proof in Appendix, Section B.3) We can also see a link with weights variation since, modulo a slight abuse of notation, we have \(_{\{e_{it},v_{(t-1)i}\}}F(_{(t-1)i})=_{e_{it}}w_{ti}\). A substantial difference with traditional boosting algorithms is that we have two ways to pick the leveraging coefficient \(_{t}\); the first one can be used when a convenient \(_{2,t}\) is directly accessible from the loss. Otherwise, there is a simple algorithm that provides parameters (including \(_{2,t}\)) such that (8) is satisfied. Section 5.3details those two possibilities and their implementation. In the more favorable case (the former one), \(_{t}\) can be chosen in an interval, furthermore defined by flexible parameters \(_{t}>0,_{t}(0,1)\). Note that fixing beforehand these parameters is not mandatory: we can also pick _any_

\[_{t}_{t}(0,^{2}_{2,t}}),\] (9)

and then compute choices for the corresponding \(_{t}\) and \(_{t}\). \(_{t}\) is important for the algorithm and both parameters are important for the analysis of the boosting rate. From the boosting standpoint, a smaller \(_{t}\) yields a larger \(_{t}\) and a smaller \(_{t}\) reduces the interval of values in which we can pick \(_{t}\); both cases tend to favor better convergence rates as seen in Theorem 5.3.

**Step 2.4** is just the crafting of the final model.

**Step 2.5** is new to boosting, the use of a so-called offset oracle, detailed in Section 5.1.2.

**Step 2.6** The weight update does not rely on a first-order oracle as in traditional boosting, but uses only loss values through \(v\)-derivatives. The finiteness of \(F\) implies the finiteness of weights.

**Step 2.7** Early stopping happens if all weights are null. While this would never happen with traditional (_e.g._ strictly convex) losses, some losses that are unusual in the context of boosting can lead to early stopping. A discussion on early stopping and how to avoid it is in Section 6.

#### 5.1.2 The offset oracle, oo

Let us introduce notation

\[_{ti}(z)\{v:Q_{F}^{}(_{ti},_{(t-1) i},v) z\}, i[m], z>0.\] (10)

(see Figure 3 below to visualize \(_{ti}(z)\) for a non-convex \(F\)) The offset oracle is used in Step 2.5, which is new to boosting. It requests the offsets to carry out weight update in (7) to an _offset oracle_, which achieves the following, for iteration \(\#t\), example \(\#i\), limit OBI \(z\):

\[(t,i,z)v_{ti}(z)\] (11)

Note that the offset oracle has the freedom to pick the offset in a whole set. Section 5.4 investigates implementations of the offset oracle, so let us make a few essentially graphical remarks here. oo does not need to build the whole \(_{ti}(z)\) to return some \(v_{ti}(z)\) for Step 2.5 in SecBoost. In the construction steps of Figure 3, as soon as \(\), one element of \(\) can be returned. Figure 4 presents more examples of \(_{ti}(z)\). One can remark that the sign of the offset \(v_{ti}\) in Step 2.5 of SecBoost is the same as the sign of \(_{(t-1)i}-_{ti}=-y_{i}_{t}h_{t}(_{i})\). Hence, unless \(F\) is derivable or all edges \(y_{i}h_{t}(_{i})\) are of the same sign (\( i\)), the set of offsets returned in Step 2.5 always contain at least two different offsets, one non-negative and one non-positive (Figure 4, (a-b)).

### Convergence of SecBoost

The offset oracle has a technical importance for boosting: \(_{ti}(z)\) is the set of offsets that limit an OBI for a training example (Definition 4.6). The importance for boosting comes from Lemma 4.7: upperbounding an OBI implies lowerbounding a Bregman Secant divergence, which will also guarantee a sufficient slack between two successive boosting iterations. This is embedded in a blueprint of a proof technique to show boosting-compliant convergence which is not new, see _e.g._. We now detail this convergence.

Remark that the expected edge \(_{t}\) in Step 2.2 of SecBoost is not normalized. We define a normalized version of this edge as:

\[[-1,1]_{t}_{i}|}{W_{t}}_{ ti}(_{i})}{M_{t}},\] (12)

with \(_{ti} y_{i}(w_{ti})\), \(W_{t}_{i}|w_{ti}|=_{i}|_{v_{(t-1)i}}F(_{(t-1)i})|\). Remark that the labels are corrected by the weight sign and thus may switch between iterations. In the particular case where the loss is non-increasing (such as with traditional convex surrogates), the labels do not switch. We need also a quantity which is, in absolute value, the expected weight:

\[_{1,t}|_{i[m]}[_{v_{(t-1)i}}F (_{(t-1)i})]|_{1,t}=|_{i[m]}[w_{ti}] |).\] (13)In classical boosting for convex decreasing losses+, weights are non-negative and converge to a minimum (typically 0) as examples get the right class with increasing confidence. Thus, \(_{1,t}\) can be an indicator of when classification becomes "good enough" to stop boosting. In our more general setting, it shall be used in a similar indicator. We are now in a position to show a first result about SecBoost.

Footnote †: This is an important class of losses since it encompasses the convex surrogates of symmetric proper losses 

**Theorem 5.3**.: _Suppose assumption 5.1 holds. Let \(F_{0} F(S,h_{0})\) in SecBoost and \(z\)* any real such that \(F(z) F_{0}\). Then we are guaranteed that classifier \(H_{T}\) output by SecBoost satisfies \(F(S,H_{T}) F(z)\) when the number of boosting iterations \(T\) yields:_

\[_{t=1}^{T}_{1,t}^{2}(1-_{t}^{2})}{_{2,t}( 1+_{t})}_{t}^{2} 4(F_{0}-F(z)),\] (14)

_where parameters \(_{t},_{t}\) appear in Step 2.3 of SecBoost._

(proof in Appendix, Section B.4) We observe the tradeoff between the freedom in picking parameters and convergence guarantee as exposed by (14): to get more freedom in picking the leveraging coefficient \(_{t}\), we typically need \(_{t}\) large (Step 2.3) and to get more freedom in picking the offset \(v_{t} 0\), we typically need \(_{t}\) large (Step 2.5). However, allowing more freedom in such ways reduces the LHS and thus impairs the guarantee in (14). Therefore, there is a subtle balance between "freedom" of choice and convergence. This balance becomes more clear as boosting compliance formally enters convergence requirement.

Boosting-compliant convergenceWe characterize convergence in the boosting framework, which shall include the traditional weak learning assumption.

**Assumption 5.4**.: _(\(\)-Weak Learning Assumption, \(\)-WLA) We assume the following on the weak learner: \(>0\) such that \( t>0\), \(|_{t}|\)._

As is usually the case in boosting, the weights are normalized in the weak learning assumption (12). So the minimization "potential" of the loss does not depend on the absolute scale of weight. This is not surprising because the loss is "nice" in classical boosting: a large \(\) guarantees most examples' edges moving to the right of the \(x\)-axis after the classifier update which, because the loss is strictly decreasing (exponential loss, logistic loss, etc.), is sufficient to yield a smaller expected loss. In our case it is not true anymore as for example there could be a local bump in the loss that would have it increase after the update. This is not even a pathological example: one may imagine that instead of a single bump the loss jiggles a lot locally. How can we keep boosting operating in such cases? A sufficient condition takes the form of a second assumption that also integrates weights, ensuring that the _variation_ of weights is locally not too large compared to (unnormalized) weights, which is akin to comparing local first- and second-order variations of the loss in the differentiable case. We encapsulate this notion in what we call a weight regularity assumption.

**Assumption 5.5**.: _(\(\)-Weight Regularity Assumption, \(\)-WRA) Let \(_{t}_{1,t}^{2}/_{2,t}\). We assume there exists \(>0\) such that \( t 1\), \(_{t}>\)._

In Figure 2 we present a(n overly) simplified depiction of the cases where \(_{2,t}\) is large for "not nice" losses, and two workarounds on how to keep it small enough for the WRA to hold. Keep in mind that \(_{1,t}\) is an expected local variation of the loss (13), (5), so as it goes to zero, boosting converges to a local minimum and it is reasonable to expect that the WRA breaks. Otherwise, there are two strategies that keep \(_{2,t}\) relatively small enough for WRA to hold: either we pick small enough offsets, which essentially works for most losses but make us converge in general to a local minimum (this is in essence our experimental choice) _or_ we optimize the offset oracle so that it sometimes "passes" local jiggling (Figure 2 (d)). While this eventually requires to tune the weak learner jointly with the offset oracle and fine-tune that latter algorithm on a loss-dependent basis, such a strategy can be used to eventually pass local minima of the loss. To do so, "larger" offsets directly translate into corresponding requests for larger magnitude classification for the next weak classifier, for the related examples. We are now in a position to state a simple corollary to Theorem 5.3.

**Corollary 5.6**.: _Suppose assumptions 5.1, 5.5 and 5.4 hold. Let \(F_{0} F(S,h_{0})\) in SecBoost and \(z\) any real such that \(F(z) F_{0}\). If SecBoost is run for a number \(T\) of iterations satisfying_

\[T-F(z))}{^{2}}_ {t}}{1-_{t[T]}^{2}}},\] (15)

_then \(F(S,H_{T}) F(z)\)._

We remark that the dependency in \(\) is optimal .

### Finding \(_{2,t}\)

There is lots of freedom in the choice of \(_{t}\) in Step 2.3 of SecBoost, and even more if we look at (9). This, however, requires access to some bound \(_{2,t}\). In the general case, the quantity it upperbounds in (8) also depends on \(_{t}\) because \(e_{ti}_{t} y_{i}h_{t}(_{i})\). So unless we can obtain such a "simple" \(_{2,t}\) that does _not_ depend on \(_{t}\), (6) - and (9) - provide a _system_ to solve for \(_{t}\).

\(_{2,t}\) **via properties of \(F\)** Classical assumptions on loss functions for zeroth-order optimization can provide simple expressions for \(_{2,t}\) (Table 1). Consider smoothness: we say that \(F\) is \(\)-smooth if it is derivable and its derivative satisfies the Lipschitz condition \(|F^{}(z^{})-F^{}(z)||z^{}-z|, z,z^{}\). Notice that this implies the condition on the \(v\)-derivative of the derivative: \(|_{v}F^{}(z)|, z,v\). This also provides a straightforward useful expression for \(_{2,t}\).

**Lemma 5.7**.: _Suppose that the loss \(F\) is \(\)-smooth. Then we can fix \(_{2,t}=2\)._

(Proof in Appendix, Section B.5) What the Lemma shows is that a bound on the \(v\)-derivative of the derivative implies a bound on order-2 \(\)-derivatives (in the quantity that \(_{2,t}\) bounds (8)). Such a condition on \(v\)-derivatives is thus weaker than a condition on derivatives, and it is strictly weaker if we impose a strictly positive lowerbound on the offset's absolute value, which would be sufficient to characterize the boosting convergence of SecBoost.

**A general algorithm for \(_{2,t}\)** If we cannot make any assumption on \(F\), there is a simple way to _first_ obtain \(_{t}\) and then \(_{2,t}\), from which all other parameters of Step 2.3 can be computed. We first need a few definitions. We first generalize the edge notation appearing in Step 2.2:

\[(,h)_{i[m]}[w_{i}y_{i}h(_{i})],\]

Figure 2: Simplified depiction of \(_{2,t}\) “regimes” (Assumption 5.5). We only plot the components of the \(v\)-derivative part in (8): removing index \(i\) for readability, we get \(_{\{e_{t},v_{t-1}\}}F(_{t-1})=(B_{t}-A_{t})/(yH_{t}()-yH_ {t-1}())\) with \(A_{t}_{v_{t-1}}F(yH_{t-1}())=-w_{t}\) and \(B_{t}=_{v_{t-1}}F(yH_{t}())\) (= \(-w_{t+1}\) iff \(v_{t-1}=v_{t}\)). If the loss is “nice” like the exponential or logistic losses, we always have a small \(_{2,t}\) (a). Place a bump in the loss (b-d) and the risk happens that \(_{2,t}\) is too large for the WRA to hold. Workarounds include two strategies: picking small enough offsets (b) or fit offsets large enough to pass the bump (c). The blue arrow in (d) is discussed in Section 6.

so that \(_{t}(_{t},h_{t})\). Remind the weight update, \(w_{ti}-_{v_{(t-1)i}}F(y_{i}H_{t-1}(_{i}))\). We define a "partial" weight update,

\[_{ti}()-_{v_{(t-1)i}}F( y_{i}h_{t}(_{i}) +y_{i}H_{t-1}(_{i}))\] (17)

(if we were to replace \(v_{(t-1)i}\) by \(v_{ti}\) and let \(=_{t}\), then \(_{ti}()\) would be \(w_{(t+1)i}\), hence the partial weight update). Algorithm 2 presents the simple procedure to find \(_{t}\). Notice that we use \(}\) with sole dependency on the prospective leveraging coefficient; we omit for clarity the dependences in the current ensemble (\(H\).), weak classifier (\(h\).) and offsets (\(v_{ i}\)) needed to compute (17).

**Theorem 5.8**.: _Suppose Assumptions 5.1 and 5.4 hold and \(F\) is continuous at all abscissae \(\{_{(t-1)i}=y_{i}H_{t-1}(_{i}),i[m]\}\). Then there are always solutions to Step 1 of Solve\({}_{}\) and if we let \(_{t}\)Solve\({}_{}\)(\(S,_{t},h_{t}\)) and then compute_

\[_{2,t}|_{i[m]}[^{2}(_{i})}{M_{t}^{2}}_{\{_{t}y_{i}h_{t}(_{i}),v_{(t-1)i} \}}F(_{(t-1)i})]|,\]

_then \(_{2,t}\) satisfies (8) and \(_{t}\) satisfies (6) for some \(_{t}>0,_{t}(0,1)\)._

The proof, in Section B.6, proceeds by reducing condition (9) to (16). The Weak Learning Assumption (5.4) is important for the denominator in the LHS of (16) to be non zero. The continuity assumption _at all abscissae_ is important to have \(_{a 0}(}_{t}(a),h_{t})=_{t}\), which ensures the existence of solutions to (16), also easy to find, _e.g._ by a simple dichotomic search starting from an initial guess for \(a\). Note the necessity of being continuous only at abscissae defined by the training sample, which is finite in size. Hence, if this condition is not satisfied but discontinuities of \(F\) are of Lebesgue measure 0, it is easy to add an infinitesimal constant to the current weak classifier, ensuring the conditions of Theorem 5.8 and keeping the boosting rates.

### Implementation of the offset oracle

Figure 3 explains how to build graphically \(_{ti}(z)\) for a general \(F\). While it is not hard to implement a general procedure following the blueprint (_i.e._ accepting the loss function as input), it would be far

Figure 4: More examples of ensembles \(_{ti}(z)\) (in blue) for the \(F\) in Figure 3. (a): \(_{ti}(z)\) is the union of two intervals with all candidate offsets non negative. (b): it is a single interval with non-positive offsets. (c): at a discontinuity, if \(z\) is smaller than the discontinuity, we have no direct solution for \(_{ti}(z)\) for at least one positioning of the edges, but a simple trick bypasses the difficulty (see text).

Figure 3: A simple way to build \(_{ti}(z)\) for a discontinuous loss \(F\) (\(_{ti}<_{(t-1)i}\) and \(z\) are represented), \(\) being the set of solutions as it is built. We rotate two half-lines, one passing through \((_{ti},F(_{ti}))\) (thick line, \(()\)) and a parallel one translated by \(-z\) (dashed line) (a). As soon as \(()\) crosses \(F\) on any point \((z^{},F(z^{}))\) with \(z_{ti}\) while the dashed line stays below \(F\), we obtain a candidate offset \(v\) for oo, namely \(v=z^{}-_{ti}\). In (b), we obtain an interval of values. We keep on rotating \(()\), eventually making appear several intervals for the choice of \(v\) if \(F\) is not convex (c). Finally, when we reach an angle such that the maximal difference between \(()\) and \(F\) in \([_{ti},_{(t-1)i}]\) is \(z\) (\(z\) can be located at an intersection between \(F\) and the dashed line), we stop and obtain the full \(_{ti}(z)\) (d).

from achieving computational optimality: a much better choice consists in specializing it to the (set of) loss(es) at hand via hardcoding specific optimization features of the desired loss(es). This would not prevent "loss oddities" to get absolutely trivial oracles (see Appendix, Section B.7).

## 6 Discussion

For an efficient implementation, boosting requires specific design choices to make sure the weak learning assumption stands for as long as necessary; experimentally, it is thus a good idea to adapt the weak learner to build more complex models as iterations increase (_e.g._ learning deeper trees), keeping Assumption 5.4 valid with its advantage over random guessing parameter \(>0\). In our more general setting, our algorithm SecBoost pinpoints two more locations that can make use of specific design choices to keep assumptions stand for a larger number of iterations.

The first is related to handling local minima. When Assumption 5.5 breaks, it means we are close to a local optimum of the loss. One possible way of escaping those local minima is to adapt the offset oracle to output larger offsets (Step 2.5) that get weights computed outside the domain of the local minimum. Such offsets can be used to inform the weak learner of the specific examples that then need to receive larger magnitude in classification, something we have already discussed in Section 5. There is also more: the sign of the weight indicates the polarity of the next edge (\(e_{t.}\), (5)) needed to decrease the loss _in the interval spanned by the last offset_. To simplify, suppose a substantial fraction of examples have an edge \(_{t.}\) in the vicinity of the blue dotted line in Figure 2 (d) so that the loss value is indicated by the big arrow and suppose their current \(=v_{t-1}\) so that their weight (positive) signals that to minimize further the loss, the weak learner's next weak classifier has to have a positive edge over these examples. Such is the polarity constraint which essentially comes to satisfy the WLA, but there is a magnitude constraint that comes from the WRA: indeed, if the positive edge is too small so that the loss ends up in the "bump" region, then there is a risk that the WRA breaks because the loss around the bump is quite flat, so the numerator of \(_{t}\) in Assumption 5.5 can be small. Passing the bump implies escaping the local minimum at which the loss would otherwise be trapped. Section 5.4 has presented a general blueprint for the offset oracle but more specific implementation designs can be used; some are discussed in the Appendix, Section B.7.

The second is related to handling losses that take on constant values over parts of their domain. To prevent early stopping in Step 2.7 of SecBoost, one needs \(_{t+1}\). The update rule of \(_{t}\) imposes that the loss must then have non-zero _variation_ for some examples between two successive edges (5). If the loss \(F\) is constant, then clearly the algorithm obviously stops without learning anything. If \(F\) is piecewise-constant, this constrain the design of the weak learner to make sure that some examples receive a different loss with the new model update \(H\). As explained in Appendix, Section B.11, this can be efficiently addressed by specific designs on \(_{}\).

In the same way as there is no "1 size fits all" weak learner for all domains in traditional boosting, we expect specific design choices to be instrumental in better handling specific losses in our more general setting. Our theory points two locations further work can focus on.

## 7 Conclusion

Boosting has rapidly moved to an optimization setting involving first-order information about the loss optimized, rejoining, in terms of information needed, that of the hugely popular (stochastic) gradient descent. But this was not a formal requirement of the initial setting and in this paper, we show that essentially any loss function can be boosted without this requirement. From this standpoint, our results put boosting in a slightly more favorable light than recent development on zeroth-order optimization since, to get boosting-compliant convergence, we do not need the loss to meet any of the assumptions that those analyses usually rely on. Of course, recent advances in zeroth-order optimization have also achieved substantial design tricks for the implementation of such algorithms, something that undoubtedly needs to be adressed in our case, such as for the efficient optimization of the offset oracle. We leave this as an open problem but provide in Appendix some toy _experiments_ that a straightforward implementation achieves, hinting that SecBoost can indeed optimize very "exotic" losses.