# Benchmark Inflation: Revealing LLM Performance

Gaps Using Retro-Holdouts

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Public benchmarks are compromised, as the training data for many Large Language Models (LLMs) is contaminated with test data, suggesting a _performance gap_ between benchmark scores and actual capabilities. Ideally, a private holdout set could be used to accurately verify scores. Unfortunately, such datasets do not exist for most benchmarks, and post-hoc construction of sufficiently similar datasets is non-trivial. To address these issues, we introduce a systematic methodology for (i) retrospectively constructing a holdout dataset for a target dataset, (ii) demonstrating the statistical indistinguishability of this _retro-holdout_ dataset, and (iii) comparing LLMs on the two datasets to quantify the performance gap due to the dataset's public availability. Applying these methods to TruthfulQA, we construct and release Retro-TruthfulQA, on which we evaluate twenty LLMs and find that some have inflated scores by as much as 16 percentage points. Our results demonstrate that public benchmark scores do not always accurately assess model properties, and underscore the importance of improved data practices in the field.

## 1 Introduction

Concerns have emerged about the reliability of public benchmarks to accurately assess the performance of large language models [1; 56; 15]. First, there is a notable discrepancy between the reported performance of models on evaluation datasets and their actual capabilities in practical settings . Second, achieving high scores on these evaluations is strongly incentivized, as higher scores are closely linked to increased publicity and wider adoption of the given model . This emphasis on benchmarks fosters a competitive environment where optimizing for benchmark performance can take precedence over real-world performance, potentially compromising the practical effectiveness or safety of models. This situation resembles specification gaming, where models meet the requirement of scoring well on benchmarks without genuinely improving on the capabilities that these benchmarks aim to assess . Extending this framing, we define the mechanisms leading to a systematic gap between benchmark scores and real-world performance as _evaluation gaming_.

Recent research has shown that evaluation datasets have, in some cases, been included in the training data [43; 38; 47; 49; 26; 50], demonstrating that evaluation gaming is occurring. Such data leakage can destroy the predictive power of benchmarks, leading to large performance gaps between a model's evaluation scores and its actual performance, as well as undermining trust in the reported model scores  - this highlights the need to improve practices for dataset release, and data collection. Such issues are particularly problematic given the significant role that evaluations are likely to play in the governance of machine learning technologies; stronger economic incentives will only increase the likelihood and severity of evaluation gaming. Furthermore, by misrepresenting model capabilities, current evaluations may create a false sense of safety. To accurately gauge the difference in a model'sperformance between the specific evaluation task and an analogous real-world task, we need access to a dataset originating from the same data distribution that has not been used during model training.

This is the idea of _holdout_ datasets, which are used to assess a machine learning model's performance after training. By definition, a holdout dataset comes from the same distribution as its corresponding target dataset, meaning that any evaluation conducted on both datasets should have the same result within some statistical tolerance . Systematic differences in performance between holdout and target datasets can point to overfitting caused by data leakage. Comparing a model's performance on a public benchmark and a corresponding holdout dataset could reveal whether data from the public benchmark has influenced the training process. Unfortunately, holdout datasets are typically not available; benchmark developers usually release all evaluation data, although there are notable exceptions, e.g. Li et al. .

To address these challenges, we propose _retroactive holdout_, or _retro-holdout_, datasets, which are verified to be similar to their corresponding target dataset through various tests, despite being created independently and retroactively. Utilizing a retro-holdout, we can quantify the evaluation performance gap of any given model. Our research advances the field by introducing a general and scalable methodology to create a retro-holdout dataset for a fully disclosed evaluation dataset, followed by rigorous testing to verify that the retro-holdout dataset closely mirrors the target dataset.

We detail our methodology for generating and validating retro-holdout datasets, along with recommendations and tools. We conduct a demonstrative case study using the TruthfulQA benchmark , a question answering dataset that was designed to assess the propensity of language models to mimic human falsehoods. TruthfulQA was selected for two key reasons: (i) it has become a popular dataset for developers to test against  and (ii) it has clear safety implications, as models performing poorly are likely to respond to user input with believable falsehoods.

### Contributions

In this work, we:

* Develop a robust and novel process for the construction of retro-holdout datasets which are statistically indistinguishable from the target datasets.
* Introduce four tests for determining the similarity between two evaluation datasets, enabling identification of appropriate retro-holdout datasets for accurate model evaluations.
* a retro-holdout dataset for TruthfulQA, which can be used to quantify the performance gaps of a model on the original dataset.1 * Conduct a comprehensive evaluation of 20 models using Retro-TruthfulQA to demonstrate measurable score inflation.

## 2 Methods

Holdout datasets were first used in machine learning to accurately assess model performance. Unlike conventional holdout sets, retro-holdout datasets are not just randomly selected subsets; they are independently created post-hoc to match the statistical properties of the target dataset, thereby ensuring that they serve as effective and unbiased benchmarks for assessing real-world performance of the model post-training.

For brevity, we define

target \[:=,\] retro \[:=.\]

We assume that the entries in target were drawn a parent distribution which we denote as parent. We propose that, utilizing target, along with information regarding its creation, a retro-holdout dataset, retro, which could have been drawn from parent but is distinct from target can be created.

### Creating the retro

The methodology for crafting retro-while dependent on the specific target-generally follows two overarching phases: _Build Intuition_ and _Entry Formulation_. Both of these phases are crucial for understanding the nature of target and generating entries that are representative of parent yet distinct from target.

Build IntuitionTo create a robust retro, one must have a strong understanding of the target, focusing primarily on its intended purpose and the methodology of its creation. We recommend an initial thorough review of the dataset documentation and relevant literature, as well as looking at many entries within target. This phase, though straightforward, has proven to yield critically valuable insights for the subsequent formulation, and later iteration, processes.

Entry FormulationUsing the insights from the **Build Intuition** phase, the creation of entries in retro proceeds by mirroring the structure and statistical properties of target while ensuring distinctiveness. Further details and step-by-step documentation for this process, as applied to the TruthfulQA dataset, are provided in Appendix A. This appendix includes all materials and tools used during the creation of the Retro-TruthfulQA dataset.

### retro Tools

Creating a retro that meets our rigorous standards for sufficient indistinguishability (see SS2.3) is non-trivial and will typically only be achieved in an iterative manner. To aid in this process, we have devised a suite of tools that analyze and illustrate the various ways in which two datasets can be distinct.

* **Fine-Tuned Prediction Model Attention:** A BERT model  is fine-tuned to classify entries as belonging to either target or retro. _Transformers Interpret_,2 a library based on Integrated Gradients for explaining model output attribution  is then leveraged to identify which input tokens the model considered most relevant when differentiating between target and retro.

Figure 1: Visualization of our methodology.

* **Datapoint Embeddings:*
* We use the all-mpnet-base-v2 embedding model through the HuggingFace Sentence Transformers library to generate embedding vectors for all data points. These embeddings are then taken as the basis for the following three tools; when analyzed in conjunction they can provide meaningful insights on general similarity trends, outlier detection, and topic clustering.
* **Embedding Space Visualization:*
* We employ Uniform Manifold Approximation and Projection (UMAP) to project these embedding vectors onto a two-dimensional plane . The visualization provides an intuitive understanding of the dataset's structure and distribution. An example output of this visualization tool is provided in Figure 2.
* **Internal Cosine Similarity Distribution:*
* To assess similarity between entries within the datasets we plot histograms of pairwise cosine similarities of datapoint embeddings. This representation aids in identifying outliers and assessing overall similarity within the datasets, as demonstrated in Figure 2.
* **Largest Internal Cosine Similarity Comparison:*
* We highlight the ten entry pairs with the highest cosine similarities in both datasets, providing a direct comparison of the most similar entries and their respective values.

These tools are documented in more detail in Appendix C.

### Sufficient indistinguishability

Establishing absolute certainty that the two datasets have originated from the same distribution is impossible. Therefore, we resort to multiple statistical tests designed to robustly test and reject the null hypothesis that target and retro have a common origin. If the result of each test indicates that we cannot reject our null hypothesis, we designate our retro to be statistically indistinguishable from target. The core motivation behind this is that, if our retro could have indeed been drawn from \((-)\), then it should be challenging for our statistical tests to distinguish between target and retro. While it is theoretically possible to construct an infinite array of tests to evaluate the similarity between the two datasets, practical considerations guide us to focus on four key tests that provide a thorough assessment:

* **Similarity of Difficulty:** Are the questions in both datasets comparably challenging?
* **Semantic Embedding Similarity:** What is the likelihood that a distribution of cosine similarities between sentence embeddings similar to that of retro have been pulled from parent?

Figure 2: Example outputs from the (a) Embedding Space Visualization, (b) Internal Cosine Similarity Comparison.

* **Prediction Accuracy:** Can a model, fine-tuned on randomized splits of the datasets, differentiate between target and retro?
* **Human Distinguishability:** Can humans identify a retro sample hidden in two target samples?

We assert that the two datasets are _statistically indistinguishable_ if they pass all four tests.

Similarity of DifficultyAssessing whether the retro-holdout dataset, retro, matches the difficulty of the target dataset, target, is crucial for drawing meaningful conclusions about evaluation gaming; otherwise performance differences could be attributed to the varying levels of difficulty, rather than the models' true capabilities. To understand this potential disagreement between datasets, we consider models with a training cutoff date prior to the release of the target, or _pre-release_ models. Since pre-release models could not possibly have been effected by exposure to target, their performance on both target and retro should be comparable, with a margin of statistical uncertainty.

It is essential to note that with access to a diverse array of LLMs spanning various capability levels, our testing methodology, combined with simple human assessment, would likely suffice to ascertain whether two evaluation datasets are statistically indistinguishable. However, performance of cutting-edge models continues to improve, meaning that pre-release models almost certainly won't be stronger than the most advanced models, assuming they are accessible at all. The nature and implications of this constraint are discussed further in SS3, and Appendix D. To address this limitation, we use a number of techniques to amplify model performance. These include allowing the model to choose multiple answers (top-\(k\)), including examples of other questions within the dataset (5-shot), including a routine prompt which aims to elicit intermediary outputs from the model (chain-of-thought), and using the 'helpful' prompt from Lin et al. .

For target and retro to be statistically indistinguishable, pre-release models (with and without performance-amplifying techniques) should score similarly on both datasets. Complete specifications and the rationale for the difficulty test are provided in Appendix D.

Prediction AccuracyWe adopt a modification of prediction accuracy as detailed by Dankar and Ibrahim  to train a model to classify an entry as either belonging to target, or to retro, using an equivalent number of entries from each dataset. Contrary to the conventional use of logistic regression in synthetic data evaluations , we fine-tune BERT  on the prediction task. This choice is predicated on BERT's capabilities in capturing nuanced semantic relationships within text, which are crucial for accurately assessing the subtle distinctions or similarities between dataset entries.

We test this model on the remainder of the samples, as theoretically, if the model's prediction accuracy on the test samples converges to 50%, within a margin allowing for statistical fluctuation, it suggests that the model fails to distinguish between the two datasets. This condition is rigorously tested to ensure the model is not merely performing at chance level but is genuinely indicative of dataset equivalence.

Semantic Embedding SimilarityUsing well established techniques for multi-dimensional data analysis, we conduct a random permutation test to determine the likelihood that a distribution with similar properties to retro could be randomly drawn from parent. For the test statistic used in our random permutation test we compute the mean of all unique, non-trivial cosine similarities between embeddings from parent and a randomly sampled subset of parent with the \(n=(n_{},n_{})\) entries. The test statistics of both target and retro are then compared with the test statistics for our \(N\) random samples, yielding one \(p\)-value for target, and one for retro. To successfully pass this test,

\[p_{},p_{}[0.05,0.95].\]

This range is chosen to ensure that retro is neither too similar nor too dissimilar from target, promoting a balance that supports our hypothesis of indistinguishability under realistic conditions. It is worth noting that, unless \(n_{}=n_{}\), an external loop outside of the core permutation test must also be defined in order to understand variance of our test statistic. Detailed visualizations and explanations of these tests are documented in Appendix F.

Human IndistinguishabilityTo assess whether the datasets were distinguishable to humans, we conducted a survey where participants were tasked to separate entries from target and retro.

Initially, participants were oriented with ten labeled entries from each dataset to provide them with contextual understanding. They then undergo a series of ten tests, each comprising of three dataset entries - two from the target and one from retro. All entries are drawn without replacement to ensure unique samples throughout the survey.

Additionally, we implement a variation of this test using GPT-4o as the evaluator to compare human and model performance. See Appendix E for comprehensive details on the survey methodology, including specifics on participant recruitment, the structure of the test, and survey instructions.

#### 2.3.1 Iterating on Failures

Although the iterative tools described in SS2.2 will limit significant differences between the datasets, our stringent standard for required similarity render it improbable that the initial retro tested will be statistically indistinguishable. Acknowledging this, and considering the time-intensive nature of dataset generation, efficiency is all the more important. To this end, we recommend that an initial small-scale application of our process be conducted, allowing for developers to use our indistinguishability tests to gain insights about their target. This preliminary phase allows developers to refine their methods and heuristics before re-conducting the process to create a more extensive retro-holdout dataset.

This process was used for the construction of Retro-TruthfulQA. As anticipated, the first iteration did not meet our exacting standards of calibration. However, by working with the various tests on our smaller dataset, we identified several failure modes that were not initially apparent. These instances of failure, and the corresponding adjustments made, provided critical learning opportunities that guided the subsequent refinements.

### Evaluating Models

The evaluation framework described in Section 2.3 was applied to assess the performance of current models. Experiments were conducted using the OpenAI chat completion API and various models from Huggingface with mostly default settings. The generation length was adjusted, and a temperature of 0.5 was specified, although this parameter may not apply to OpenAI chat models.

During the construction of TruthfulQA , the authors envisioned that language models would be evaluated by the max-probability assigned to any of a predefined list of available options. This approach may suffer from three issues. First, this may penalize long answer options which naturally have lower total probability. Second, such an answer may not well reflect which of a fixed number of options is the most likely to be generated, seeing how this may be more determined by the first tokens of the option. Finally, the OpenAI API no longer provides probability output, and other API providers may have never had such an option.

For these reasons, it was decided to evaluate models by providing an enumerated list of all TruthfulQA _mc1_-choices and generating tokens to select a preferred option. To minimize potential model bias, answers were resampled with options rotated at minimum ten times and until one option had been selected an additional four times over alternatives. A Vicuna-inspired prompt was used for all models and is described in Appendix G.1.2.

Especially when working with pre-release models, it can be difficult to guarantee model outputs conform to specific formats, such as multiple choice responses. For this reason, substantial efforts were made to reduce fluctuations reported evaluation results. Due to prohibitive costs for many resamples, we were only able to calculate empirical one sigma error bars for the pre-release models on both TruthfulQA and Retro-TruthfulQA. On TruthfulQA, babbage-002, davinci-002, and neox-20b had had statistical error of \( 1.27\%\), \( 0.83\%\), and \( 2.84\%\) respectively, while their errors on Retro-TruthfulQA were \( 2.47\%\), \( 1.96\%\), and \( 1.34\%\).

## 3 Results and Discussion

### Retro-holdout TruthfulQA Dataset

We release Retro-TruthfulQA, a retro-holdout dataset designed to quantify the evaluation gap for models tested on the TruthfulQA dataset, _provided that the model's training cutoff date is prior to January 1st, 2024_. Retro-TruthfulQA mirrors the structure and content of the original TruthfulQA dataset across all measured categories and comprises 817 entries.

Notably, Retro-TruthfulQA has passed all four of our indistinguishability tests, establishing it as the first retro-holdout dataset to be _statistically indistinguishable_ from its corresponding target dataset. The tests covered aspects of the dataset to ensure semantic similarity, prediction accuracy, and human and model-based distinguishability, confirming that Retro-TruthfulQA accurately mirrors the original dataset in all essential aspects. The detailed results, complete with confidence intervals for each metric, are summarized in Table 1, and Figure 3(a).

### TruthfulQA Evaluation Details

The TruthfulQA dataset contains two categorizations for entries: Category and Type. Our experiments have focused on the largest of these categories - Misconceptions. The Type for the dataset is either _adversarial_ or _non-adversarial_. Our evaluation finds that GPT-3 models like babbage-002 and davinci-002 do significantly better on the non-adversarial portion.

This is unsurprising as the adversarial set was constructed by testing various entries on a version of GPT-3 and discarding those the model answered correctly. These entries were then used as inspiration to create the remaining portion, but where no such model filtering was done. Due to this potential filtering bias and the performance difference between the two sets, we have additional chosen to focus on the non-adversarial portion of TruthfulQA. While these changes are deviations from the original TruthfulQA evaluation, it is worth noting that all experiment compare the performance of this same

  
**Description** & \(_{0}\) & **Outcome** & **Test \(p\)-value** \\  babbage-002 difficulty gap & \(0\%\) & \(-1.2 7.4\%\) & \( 50\%\) \\ davinci-002 difficulty gap & \(0\%\) & \(-3.3 8.0\%\) & \( 50\%\) \\  Prediction accuracy & \(50\%\) & \(53.7 3.26\%\) & \(47.4\%\) \\  target Random permutation & – & – & \(6.67 1.86\%\) \\ retro Random permutation & – & – & \(93.48 1.85\%\) \\  GPT-4 Distinguishability & \(33.\%\) & \(28.0 9.0\%\) & \( 50\%\) \\ Human Distinguishability & \(33.\%\) & \(31.3 7.1\%\) & \( 50\%\) \\   

Table 1: Retro-TruthfulQA Indistinguishability Tests Results

Figure 3: Model accuracy on Retro-TruthfulQA vs. TruthfulQA. (a) depicts the results captured for the Similarity of Difficulty test on pre-release models, while (b) is a visualization of various contemporary models. In both plots, a 95% confidence band for two samples of boolean values, i.e. correct or incorrect, of sizes equal to our two datasets is shown.

evaluation method on the original vs the retro-holdout dataset, along with calibration such that any statistically-significant gap between these must be explained by some form of evaluation gaming.

### The Performance Gap

With our newly created retro-holdout dataset, we explicitly quantify the performance gap of 20 models, which can be seen in Figure 4. Our analysis covers both larger API models such as Claude3 and GPT-4, as well as several open-release models that have been either speculated or confirmed to exhibit data leakage .

### Contemporaneous Work

Coinciding with our efforts, Zhang et al.  introduce the GSM1k dataset for assessing mathematical reasoning. This study employs several human tests to ensure an "apples-to-apples" similarity to their target dataset GSM8k . Similar to our findings, Zhang et al.  report an overperformance by many models on their target evaluations.

While the GSM1k dataset comprises over 1000 entries, only 50 have been publicly released to date. Zhang et al.  recognize that releasing the entire dataset will likely result in the same data leakage current benchmark suffer from. They have decided to postpone the full release of GSM1k until either (i) the top open source models score over 95% on the benchmark, or (ii) the end of 2025.

Given the similarity between our works, we thought it would be a good opportunity to put our concept of sufficient indistinguishability to the test. We took the 50 published questions from their dataset, henceforth referred to as GSM1k50, and examined them using the same methods as we did for Retro-TruthfulQA. Our semantics tools and Semantic Embedding Similarity test suggest that GSM1k50 can be adjusted to more closely resemble original GSM8k entries, generating a target and retro random permutation of \(3.02 0.05\%\) and \(98.7 0.02\%\), respectively. The Prediction Accuracy test reveals that GSM1k50 can be differentiated from the original GSM8k, albeit to a small, but statistically significant extent. These finding highlights the rigor of our notion of sufficient indistinguishability, but also suggests that in practical scenarios, slightly relaxed criteria might still produce effective retro-holdout datasets without significantly compromising evaluation quality.

Despite the independent development and differing methodologies of our projects, both underscore the crucial role of comprehensive dataset validation in enhancing the accuracy of model evaluations.

Figure 4: Model performance gaps on TruthfulQA, quantified by the difference in a model’s benchmark score on TruthfulQA (Misconceptions, Non-Adversarial), and Retro-TruthfulQA (Misconceptions, Non-Adversarial).

### Limitations

The assumption that the retro-holdout dataset and the target dataset are drawn from the same distribution may not always be valid. This assumption is challenged if the target dataset itself is subject to distribution shifts over time; such shifts can alter the underlying data characteristics over time. Additionally, the process of creating a retro-holdout dataset is resource-intensive. It demands significant computational resources for generating and validating the dataset, as well as human experts for iterative adjustments based on indistinguishability tests, which may mitigate the wide adoption of our methodology.

Another limitation arises from the inherent approach of matching the distribution of the target dataset. While this method ensures that the retro-holdout dataset mirrors the target dataset as closely as possible, it also inadvertently perpetuates any implicit biases that are present in the target dataset. Consequently, while the retro-holdout dataset might excel in mimicking the target dataset's distribution, it may not provide a truly independent measure of a model's generalization capabilities across broader contexts.

## 4 Related Works

Development of large language models (LLMs) continues to outpace the advancement of evaluation methods, raising concern about benchmark integrity . Evaluation datasets are frequently used during an LLM's training process, causing inflated benchmark scores; no standard methodology exists to detect this issue . Data quality, essential for model performance, remains undervalued and under-incentivized . Data contamination, where test data is included in training sets, results in models "cheating" by memorizing tests rather than generalizing . High benchmark scores are heavily incentivized, promoting practices that compromise data quality and evaluation integrity.

Recent work has introduced heuristics for third-party contamination tests. Sainz et al.  propose a technique to detect test set contamination by eliciting reproduction of specific test set examples. Golchin and Surdeanu  suggest a method for identifying contamination in black-box models by comparing the similarity between model completions of randomly selected example prefixes and the actual data using GPT-4. Concurrent work by  is notable for its use of a holdout set, a concept central to our approach, and shows accuracy drops of up to 13% and highlights a positive correlation between memorization and performance gaps.

It is well known that metrics lose their predictive power when incentives are attached to them Goodhart , Strathern , Karwowski et al. . As  state, "overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences." Current AI risk metrics fail to address emerging failure modes , and Bengio  emphasize that high benchmark scores do not necessarily equate to effective real world performance.

Empirical findings highlight the necessity for immediate structural reforms in AI research and development to prioritize and encourage data quality . Recent calls for a _science of evaluations_ underscore the urgent need for rigorous evaluation frameworks to inform policy and ensure responsible AI development [5; 42].

## 5 Conclusion

Our findings demonstrate significant discrepancies between benchmark performances and real-world capabilities of LLMs, underscoring the need for robust and reliable evaluation methodologies. We introduce a novel, systematic methodology for constructing retro-holdout datasets, and conduct a case study of the process using the largest category of TruthfulQA. The result is Retro-TruthfulQA, a retro-holdout for TruthfulQA which has been shown to be statistically indistinguishable from the target dataset. This methodology, designed to be generally applicable across various public benchmark evaluations, provides tools that significantly enhance the accuracy and reliability of model evaluations, offering a practical path forward for the field. In a recent work Anwar et al.  explicitly challenge "How can the evaluations of LLMs be made trustworthy given the difficulty of assuring that there is no test-set contamination?" Our work provides a succinct and powerful response: Retro-Holdouts.