# FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner

Wenliang Zhao

Department of Automation

Tsinghua University

wenliangzhao.thu@gmail.com

&Minglei Shi

Department of Automation

Tsinghua University

stephenserrylei@gmail.com

Equal contribution. \({}^{}\)Corresponding author.

Xumin Yu

Department of Automation

Tsinghua University

yuxumin98@gmail.com

&Jie Zhou

Department of Automation

Tsinghua University

jzhou@tsinghua.edu.cn

&Jiwen Lu

Department of Automation

Tsinghua University

lujiwen@tsinghua.edu.cn

Equal contribution. \({}^{}\)Corresponding author.

###### Abstract

Building on the success of diffusion models in visual generation, flow-based models reemerge as another prominent family of generative models that have achieved competitive or better performance in terms of both visual quality and inference speed. By learning the velocity field through flow-matching, flow-based models tend to produce a straighter sampling trajectory, which is advantageous during the sampling process. However, unlike diffusion models for which fast samplers are well-developed, efficient sampling of flow-based generative models has been rarely explored. In this paper, we propose a framework called FlowTurbo to accelerate the sampling of flow-based models while still enhancing the sampling quality. Our primary observation is that the velocity predictor's outputs in the flow-based models will become stable during the sampling, enabling the estimation of velocity via a lightweight velocity refiner. Additionally, we introduce several techniques including a pseudo corrector and sample-aware compilation to further reduce inference time. Since FlowTurbo does not change the multi-step sampling paradigm, it can be effectively applied for various tasks such as image editing, inpainting, _etc_. By integrating FlowTurbo into different flow-based models, we obtain an acceleration ratio of 53.1%\(\)58.3% on class-conditional generation and 29.8%\(\)38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img), achieving the real-time image generation and establishing the new state-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.

## 1 Introduction

In recent years, diffusion models have emerged as powerful generative models, drawing considerable interest and demonstrating remarkable performance across various domains. Diffusion models utilize a denoising network, \(_{}\), to learn the reverse of a diffusion process that gradually adds noise to transform the data distribution into a Gaussian distribution. While the formulation of diffusion models enables stable training and flexible condition injection, sampling from these models requires iterative denoising. This process necessitates multiple evaluations of the denoising network, thereby increasing computational costs. To address this, several techniques such as fastdiffusion samplers[22; 18; 42] and efficient distillation[31; 37] have been proposed to reduce the sampling steps of diffusion models.

Alongside the research on diffusion models, flow-based models[5; 19; 17] have garnered increasing attention due to their versatility in modeling data distributions. Flow is defined as a probability path that connects two distributions and can be efficiently modeled by learning a neural network to estimate the conditional velocity field through a neural network \(_{}\) via flow matching . Encompassing the standard diffusion process as a special case, flow-based generative models support more flexible choices of probability paths. Recent work has favored a simple linear interpolant path [20; 24; 8], which corresponds to the optimal transport from the Gaussian distribution to the data distribution. This linear connection between data and noise results in a more efficient sampling process for flow-based models. However, unlike diffusion models, which benefit from numerous efficient sampling methods, current samplers for flow-based models primarily rely on traditional numerical methods such as Euler's method and Heun's method . These traditional methods, while functional, fail to fully exploit the unique properties of flow-based generative models, thereby limiting the potential for faster and more efficient sampling.

In this paper, we propose FlowTurbo, a framework designed to accelerate the generation process of flow-based generative models. FlowTurbo is motivated by comparing the training objectives of diffusion and flow-based generative models, as well as analyzing how the prediction results \(_{}\) and \(_{}\) vary over time. Our observation, illustrated in Figure 1, indicates that the velocity predictions of a flow-based model remain relatively stable during sampling, in contrast to the more variable predictions of \(_{}\) in diffusion models. This stability allows us to regress the offset of the velocity at each sampling step using a lightweight velocity refiner, which contains only \(5\%\) of the parameters of the original velocity prediction model. During the sampling process, we can replace the original velocity prediction model with our lightweight refiner at specific steps to reduce computational costs.

As a step towards real-time image generation, we propose two useful techniques called pseudo corrector and sample-aware compilation to further improve the sampling speed. Specifically, the pseudo corrector method modifies the updating rule in Heun's method by reusing the velocity prediction of the previous sampling step, which will reduce the number of model evaluations at each step by half while keeping the original convergence order. The sample-aware compilation integrates the model evaluations, the sampling steps as well as the classifier-free guidance  together and compile them into a static graph, which can bring extra speedup compared with standard model-level compilation. Since each sample block is independent, we can still adjust the number of inference steps and sampling configurations flexibly.

Our FlowTurbo framework is fundamentally different from previous one-step distillation methods for diffusion models [20; 40; 32], which require generating millions of noise-image pairs offline and conducting distillation over hundreds of GPU days. In contrast, FlowTurbo's velocity refiner can be efficiently trained on pure images in less than 6 hours. Moreover, one-step distillation-based methods are limited to image generation and disable most of the functionalities of the original base model. Conversely, FlowTurbo preserves the multi-step sampling paradigm, allowing it to be effectively applied to various tasks such as image editing, inpainting, and more.

We perform extensive experiments to evaluate our method. By applying FlowTurbo to different flow-based models, we obtain an acceleration ratio of 53.1%\(\)58.3% on class-conditional generation and 29.8%\(\)38.5% on text-to-image generation. Notably, FlowTurbo attains an FID score of 2.12 on ImageNet with 100 (ms / img) and FID of score 3.93 with 38 (ms / img), thereby enabling real-time image generation and establishes the new state-of-the-art. Additionally, we present qualitative comparisons demonstrating how FlowTurbo generates superior images with higher throughput and how it can be seamlessly integrated into various applications such as image editing, inpainting, _etc_. We believe our FlowTurbo can serve as a general framework to accelerate flow-based generative models and will see wider use as these models continue to grow [24; 20; 8; 9].

## 2 Related Work

**Diffusion and flow-based models.** Diffusion models [10; 38] are a family of generative models that have become the de-facto method for high-quality generation. The diffusion process gradually adds noise to transform the data distribution to a normal distribution, and the goal of diffusion models is to use a network \(_{}\) to learn the reverse of the diffusion process via score-matching [10; 38]. Rombach _etal_.  first scales up diffusion models to large-scale text-to-image generation by performing the diffusion on latent space and adopting cross-attention to inject conditions. The pre-trained diffusion models can also be easily fine-tuned to achieve generation with more diverse conditions [41; 27] and have attracted increasing attention in the community. Flow-based generative models are different from diffusion models in both data modeling and training objectives. Flow-based models [20; 17; 8; 24] consider the probability path from one distribution to another, and learn the velocity field via flow matching . By choosing the linear interpolant as the probability path which corresponds to the optimal transport from the normal distribution to the data distribution, the trajectory from noise to data becomes more straighter which is beneficial to the sampling. Recent work [24; 8] have demonstrates the effectiveness and scalability of flow-based generation models. However, both diffusion and flow-based models requires multiple evaluations of the prediction model, leading to lower inference speed than traditional architectures like GAN. In this work, we focus on this issue and aim to accelerate flow-based generative models.

**Efficient visual generation.** Accelerating the generation of diffusion models has become an increasingly important topic. Existing methods can be roughly categorized as training-free and training-based methods. Training-free methods aim to design faster samplers that can reduce the approximation error when sampling from the diffusion SDE or ODE [36; 22; 18; 42], while keeping the weights of diffusion models unchanged. Training-based methods often aim to reshape the sampling trajectory by distillation from the diffusion model [31; 40] to achieve the few-step or even one-step generation. These training-based methods usually requires multiple-round of distillation [31; 20] and expensive training resources (_e.g._, \(>\)100 GPU days in ). Besides, the distilled one-step model no longer supports image editing due to the lack of multi-step sampling. Although there are a variety of methods for accelerating diffusion models, there are few fast sampling methods designed for flow-based generative models. Existing flow-based models adopt traditional numerical methods like Euler's method or Heun's method during the inference . In this work, we provide a framework called FlowTurbo to accelerate the generation of flow-based models by learning a lightweight velocity refiner (which only requires <6 GPU hours) to regress the offset of the velocity. Together with other proposed techniques, FlowTurbo addresses the previously unmet need for an efficient flow-based generation framework, paving the way for real-time generative applications.

## 3 Method

### Preliminaries: Diffusion and Flow-based Models

**Diffusion models.** Recently, diffusion models [10; 38; 35; 30] have emerged as a powerful family of generative models. The diffusion models are trained to learn the inverse of a diffusion process such

Figure 1: **Visualization of the curvatures of the sampling trajectories of different models. We compare the curvatures of the model predictions of a standard diffusion model (DiT ) and several flow-based models (SiT , SD3-Medium , FLUX.1-dev , and Open-Sora ) during the sampling. We observe that the \(_{}\) in flow-based models is much more stable than \(\) of diffusion models during the sampling, which motivates us to seek a more lightweight estimation model to reduce the sampling costs of flow-based generative models.**

that it can recover the data distribution \(p_{0}(_{0})\) from the Gaussian noise. The diffusion process can be represented as:

\[_{t}=_{t}_{0}+_{t}, t, (0,),\] (1)

where \(_{t},_{t}\) are the chosen noise schedule such that the marginal distribution \(p_{1}(_{1})(0,)\). The optimization of diffusion models can be derived by either minimizing the ELBO of the reverse process  or solving the reverse diffusion SDE , which would both lead to the same training objective of score-matching, _i.e_., to learn a noise prediction model \(_{}(_{t},t)\) to estimate the scaled score function \(-_{t}_{} p_{t}(_{t})\):

\[_{}()=_{t,p_{0}(_{0}),p( _{t}|_{0})}[(t)\|_{}( _{t},t)+_{t}_{} p_{t}(_{t}) \|_{2}^{2}],\] (2)

where \((t)\) is a time-dependent coefficient. Sampling from a diffusion model can be achieved by solving the reverse-time SDE or the corresponding diffusion ODEs , which can be efficiently achieved by modern fast diffusion samplers [36; 22; 42].

**Flow-based models.** Flow-based models can be traced back to Continuous Normalizing Flows  (CNF), which is a more generic modeling technique and can capture the probability paths of the diffusion process as well . Training a CNF becomes more practical since the purpose of the flow matching technique , which learns the conditional velocity field of the flow. Similar to (1), we can add some constraints to the noise schedule such that \(_{0}=1,_{0}=0\) and \(_{1}=0,_{1}=1\), and then define the flow as:

\[_{t}(|):_{0}_{t}_{0}+ _{t},\] (3)

In this case, the velocity field that generates the flow \(_{t}\) can be represented as:

\[u_{t}(_{t}(_{0}|)|)=} {t}_{t}(_{0}|)=_{t}_{0}+_{t}.\] (4)

The training objective of conditional flow matching is to train a velocity prediction model \(_{}\) to estimate the conditional velocity field:

\[_{}()=_{t,p_{1}(),p_{0}( _{0})}\|_{}(_{t}(_{0}|),t)-}{t}_{t}(_{0}|)\|_{2}^{2}\] (5)

The sampling of a flow-based model can be achieved by solving the probability flow ODE with the learned velocity

\[_{t}}{t}=_{}(_{ t},t),_{1} p_{1}(_{1}).\] (6)

Since the formulation of the flow \(_{t}\) can be viewed as the interpolation between \(_{0}\) and \(\), it is also referred to as interpolant in some literature [1; 24]. Among various types of interpolants, a very simple choice is linear interpolant [24; 8], where \(_{t}=(1-t)\) and \(_{t}=t\). In this case, the velocity field becomes a straight line connecting the initial noise and the data point, which also corresponds to the optimal transport between the two distributions [19; 17]. The effectiveness and scalability of the linear interpolant have also been proven in recent work [17; 24; 8].

### Efficient Estimation of Velocity

We consider the velocity estimation in flow-based generative models with the linear interpolant [24; 8; 20]. As shown in (5), the training target of the velocity prediction model \(_{}\) is exactly \(-_{0}\), a constant value independent of \(t\). Our main motivation is to efficiently estimate the velocity during the sampling, instead of evaluating the whole velocity prediction model \(_{}\) every time.

**Analyzing the stability of velocity.** We start by analyzing the stability of the output value of \(_{}\) along the sampling trajectory. By comparing the training objectives of diffusion and flow-based models (2)(5), we know that the target of \(_{}\) is independent of \(t\). A more in-depth discussion is provided in Appendix A.3, where we show the two training objectives have different time-dependent weight functions. To verify whether there are similar patterns during the sampling, we compare how the prediction results change across the sampling steps in Figure 1. Specifically, we compare the curvatures of \(_{}\) of a diffusion model (DiT ) and the \(_{}\) of flow-based models (SiT , SD3 , etc) during the sampling steps. For each model, we sample from 8 random noises and set the total sampling steps as 20. It can be clearly observed that \(_{}\) of a flow-based model is much more stable than the \(_{}\) of a diffusion model. Therefore, We define the \(_{}\) as a _"stable value"_. The stability of \(_{}\) makes it possible to obtain the velocity more efficiently rather than performing the forward pass of the whole velocity prediction network \(_{}\) at every sampling step.

**Learning a lightweight velocity refiner.** Since the velocity in a flow-based model is a "stable value", we propose to learn a lightweight refiner that can adjust the velocity with minimal computational costs. The velocity refiner takes as inputs both the current intermediate result and the velocity of the previous step, and returns the offset of velocity:

\[_{t_{i}}=_{}(_{t_{i}},_{t_{i-1}}, t_{i})+_{t_{i-1}}.\] (7)

The velocity refiner \(_{}\) can be designed to be very lightweight (\(<5\%\) parameters of \(_{}\)). The detailed architecture can be found in Appendix C.

To learn the velocity refiner, we need to minimize the difference between the output of \(_{}\) and the actual offset \(_{t_{i}}-_{t_{i-1}}\). However, it requires multiple-step sampling to obtain an intermediate result \(_{t_{i}}\) to make the training objective perfectly align with our target. To reduce the training cost, we simulate the \(_{t}\) with one-step sampling starting from \(_{t_{i-1}}\), which is directly obtained by the flow \(_{t_{i-1}}\). The detailed procedure to compute the loss is listed as follows:

\[_{t_{i-1}} _{t_{i-1}}(_{0}|), _{0} p_{0}(), p_{1}()\] (8) \[_{t_{i-1}} _{}(_{t_{i-1}},t_{i-1}), _{t_{i}}(_{t_{i-1}}, _{t_{i-1}}, t)\] (9) \[_{} \|_{}(_{t_{i}},t_{ i})-(_{}(_{t_{i}},_{t_{i-1}},t_{i})+_{t_{i-1}})\|_{2}^{2}\] (10)

Where \( t=t_{i}-t_{i-1}\) and we use a simple Euler step for the Solver to obtain \(_{t_{i}}\). Once the velocity refiner is learned, we can use it to replace the original \(_{}\) at some specific sampling steps. We will demonstrate through experiments that adding the velocity refiner can improve the sampling quality without introducing noticeable computational overhead.

**Compatibility with classifier-free guidance.** Classifier-free guidance  is a useful technique to improve the sampling quality in conditional sampling. Let \(\) be the condition, the classifier-free guidance for a velocity prediction model  can be defined as:

\[^{}(,t|)=(1-)_{}( ,t|)+_{}(,t|),\] (11)

where \(\) is the guidance scale and \(\) denotes the null condition. To make our velocity refiner support classifier-free guidance, we only need to make sure both the conditional prediction \(_{}(,t|)\) and the

Figure 2: **Overview of FlowTurbo. (a) Motivated by the stability of the velocity predictor’s outputs during the sampling, we propose to learn a lightweight velocity refiner to regress the offset of the velocity field. (b)(c) We propose the pseudo corrector which leverages a velocity cache to reduce the number of model evaluations while maintaining the same convergence order as Heun’s method. (d) During sampling, we employ a combination of Heun’s method, the pseudo corrector, and the velocity refiner, where each sample block is processed with the proposed sample-aware compilation.**

unconditional prediction \(_{}(,t|)\) appear during the training. Note that we always feed the velocity prediction model \(_{}\) and the velocity refiner \(_{}\) with the same condition.

\[_{}=_{_{1}} +_{>_{1}},,\] (12) \[_{}^{}\|_{}(_{t_{i}},t_{i}|_{})-(_{}( _{t_{i}},_{t_{i-1}},t_{i}|_{})+_ {t_{i-1}})\|_{2}^{2},\] (13)

where we set the \(_{1}=0.1\) as the probability of using an unconditional velocity.

### Towards Real-Time Image Generation

The sampling costs of a flow-based model can be significantly minimized by integrating our lightweight velocity refiner \(_{}\) in place of the velocity prediction network \(_{}\) at selected sampling steps. In this section, we propose two techniques to further improve the sampling speed towards real-time image generation.

**Pseudo corrector.** Traditional numerical ODE solvers are usually used to sample from a probability flow ODE. For example, SiT  adopt a Heun method (or improved Euler's method)  as the ODE solver. The update rule from \(t_{i-1}\) to \(t_{i}\) can be written as:

\[_{i-1}_{}(_{t_{i-1}},t_{i-1}| ),}_{t_{i}}_{t_{i-1}}+  t_{i-1}\] (14)

\[_{i}_{}(}_{t_{i}},t_{i}| ),_{i}_{i-1}+ [_{i-1}+_{i}]\] (15)

Each Heun step contains a predictor step (14) and a corrector step (15), thus includes two evaluations of the velocity predictor \(_{}\), bringing extra inference costs. Motivated by , we propose to reuse the \(_{i}\) in the next sampling step, instead of re-computing it via \(_{i}_{}(_{t_{i}},t_{i}|)\) (see Figure 2 (b)(c) for illustration). We call this a pseudo corrector since it is different from the predictor-corrector solvers in numerical analysis. It can be proved (see Appendix B) that the pseudo corrector also enjoys 2-order convergence while only having one model evaluation at each step.

**Sample-aware compilation.** Compiling the network into a static graph is a widely used technique for model acceleration. However, all the previous work only considers network-level compilation, _i.e_., only compiling the \(_{}\) or \(_{}\). We propose the sample-aware compilation which wraps both the forward pass of \(_{}\) or \(_{}\) and the sampling operation together (including the classifier-free guidance) and performs the compilation. For example, the sample blocks illustrated in Figure 2 (b, c) are compiled into static graphs. Since each sample block is independent, we can still adjust the number of inference steps and sampling configurations flexibly.

### Discussion

Recently, there have been more and more training-based methods [20; 40; 32] aiming to accelerate diffusion models or flow-based models through one-step distillation. Although these methods can achieve faster inference, they usually require generating paired data using the pre-trained model and suffer from large training costs (_e.g_., >100 GPU days in [40; 20]). Besides, one-step methods only keep the generation ability of the original model while disabling more diverse applications such as image inpainting and image editing. In contrast, our FlowTurbo aims to accelerate flow-based models through velocity refinement, which still works in a multi-step manner and performs sampling on the original trajectory. For example, FlowTurbo can be easily combined with existing diffusion-based image editing methods like SDEdit  (see Section 4.4).

## 4 Experiments

We conduct extensive experiments to verify the effectiveness of FlowTurbo. Specifically, we apply FlowTurbo to both class-conditional image generation and text-to-image generation tasks and demonstrate that FlowTurbo can significantly reduce the sampling costs of the flow-based generative models. We also provide a detailed analysis of each component of FlowTurbo, as well as qualitative comparisons of different tasks.

### Setups

In our experiments, we consider two widely used benchmarks including class-conditional image generation and text-to-image generation. For class-conditional image generation, we adopt a transformer style flow-based model SiT-XL  pre-trained on ImageNet 256\(\)256. For text-to-image generation, we utilize InstaFlow  as the flow-based model, whose backbone is a U-Net similar to Stable-Diffusion . Note that we use the 2-RF model from  instead of the distilled version since our FlowTurbo is designed to achieve acceleration within the multi-step sampling framework. The velocity refiner only contains 4.3% and 5% parameters of the corresponding predictor, and the detailed architecture can be found in Appendix C. During training, we randomly sample \( t(0,0.12]\) and compute the training objectives in (13). In both tasks, we use a single NVIDIA A800 GPU to train the velocity refiner and find it converges within 6 hours. We use a batch size of 8 on a single A800 GPU to measure the latency of each method. Please refer to Appendix C for more details.

### Main Results

**Class-conditional image generation.** We adopt the SiT-XL  trained on ImageNet  of resolution of \(256 256\). Following common practice [24; 30], we adopt a classifier-free guidance scale (CFG) of 1.5. According to , a widely used sampling method of the flow-based model is Heun's method . In Table 0(a), we demonstrate how our FlowTurbo can achieve faster inference than Heun's method in various computational budgets. Specifically, we conduct experiments with different sampling configurations (the second column of Table 0(a)), where we use the suffix to represent the number of Heun's method block (\(H\)), pseudo corrector block (\(P\)), and the velocity refiner block (\(R\)). Note that each Heun's block contains two evaluations of the velocity predictor while each pseudo corrector block only contains one. We also provide the total FLOPs during the sampling and the

Table 1: **Main results.** We apply our FlowTurbo on SiT-XL  and the 2-RF of InstaFlow  to perform class-conditional image generation and text-to-image generation, respectively. The image quality is measured by the FID 50K\(\) on ImageNet (\(256 256\)) and the FID 5K\(\) on MS COCO 2017 (512\(\)512). We use the suffix to represent the number of Heun’s method block (\(H\)), pseudo corrector block (\(P\)), and the velocity refiner block (\(R\)). Our results demonstrate that FlowTurbo can significantly accelerate the inference of flow-based models while achieving better sampling quality.

Table 2: **Comparisons with the state-of-the-arts.** We compare the sampling quality and speed of different methods on ImageNet \(256 256\) class-conditional sampling. We demonstrate that FlowTurbo can significantly improve over the baseline SiT-XL  and achieves the fastest sampling (38 ms / img) and the best quality (2.12 FID) with different configurations.

inference speed of each sample configuration. In each group of comparison, we choose the sampling strategy of FlowTurbo to make the sampling quality (measured by the FID 50K\(\)) similar to the baseline. Our results demonstrate that FlowTurbo can accelerate the inference by \(37.2\% 43.1\%\), while still achieving better sampling quality. Notably, FlowTurbo obtains 3.63 FID with a sampling speed of 41.6 ms/img, achieving real-time image generation.

**Text-to-image generation.** We adopt the 2-RF model in  as our base model for text-to-image generation. Note that we do not adopt the distilled version in  since we focus on accelerating flow-based models within the multi-step sampling paradigm. Following , we compute the FID 5K\(\) between the generated \(512 512\) samples and the images on MS COCO 2017  validation set. The results are summarized in Table 1b, where we compare the sampling speed/quality with the baseline Heun's method. Note that the notation of the sampling configuration is the same as Table 1a. The results clearly demonstrate that Our FlowTurbo can also achieve significant acceleration (29.8%\(\)38.5%) on text-to-image generation.

### Comparisons to State-of-the-Arts

In Table 2, we compare our FlowTurbo with state-of-the-art methods on ImageNet \(256 256\) class-conditional generation. We use SiT-XL  as our base model and apply FlowTurbo with different sampling configurations on it. We show that FlowTurbo with \(H_{1}P_{5}R_{3}\) achieves the sampling speed of 38 (ms / img) with 3.93 FID (still better than most methods like Mask-GIT , ADM ). On the other hand, FlowTurbo with \(H_{8}P_{9}R_{5}\) archives the lowest FID 2.12, outperforming all the other methods. Besides, we also provide a comparison of the sampling speed/quality trade-offs of SiT (by changing the number of sampling steps of Heun's method) and FlowTurbo (by changing the sampling configurations) in 3, where the results of some other state-of-the-arts methods are also included. The comparison shows our FlowTurbo exhibits favorable sampling quality/speed trade-offs.

### Analysis

**Ablation of components of FlowTurbo.** We evaluate the effectiveness of each component of FlowTurbo in Table 3a. Specifically, we start from the baseline, a 7-step Heun's method and gradually add components of FlowTurbo. In the sample config A, we show that adding a velocity refiner can significantly improve the FID\(\) (\(4.42 2.80\)), while introducing minimal computational costs (only \(+0.7\%\) in the latency). From B to E, we adjust the ratios of Heun's method block, the pseudo corrector block, and the velocity refiner block to achieve different trade-offs between sampling speed and quality. In the last two rows, we show that our sample-aware compilation is better than standard model-level compilation, further increasing the sampling speed.

**Choice of \( t\).** We find the choice of \( t\) during training is crucial and affects the sampling results a lot in our experiments, as shown in Table 3b. We find \( t(0.0,0.1]\) works well for more sampling steps like \(H_{12}R_{5}\), while \( t[0.06,0.12]\) is better for fewer sampling steps like \(H_{6}R_{2}\) and \(H_{9}R_{3}\). Besides, we find \( t(0.0,0.12]\) yields relatively good results in all the situations.

**Effects of velocity refiner.** We evaluate the effects of the different number of velocity refiners in Table 3c, and find that appropriately increasing the number of velocity refiners can improve the trade-off between sampling quality and speed. Specifically, we find \(H_{6}R_{2}\) can achieve better image quality and generation speed than the baseline \(H_{8}\).

**Effects of pseudo corrector.** In Table 3d, we fix the total number of both Heun's sample block and pseudo corrector block and adjust the ratio of the pseudo corrector. Our results demonstrate that increasing the number of pseudo corrector blocks can significantly improve the sampling speed while introducing neglectable performance drop (, FlowTurbo with \(H_{1}P_{6}R_{2}\) performs better than \(H_{8}\)).

Figure 3: **FlowTurbo exhibits favorable trade-offs compared with SOTA methods.**

**Ablation of refiner architectures.** When designing the architecture for the velocity refiner, we followed a simple rule to make the refiner have a similar architecture as the base velocity predictor but with much fewer parameters ( 5% of the base model). The detailed architecture is described in Section 4.1 and Appendix C. For example, since SiT consists of multiple transformer blocks, we simply use a single block as the refiner. For text-to-image generation, we reduce the number of layers and channels of the UNet. In our early experiments, we have tried another architecture for class-conditional image generation, where a SiT-S (a smaller version of the base velocity predictor SiT-XL) is adopted as the refiner (as shown in Table 5). We find that using a block of SiT-XL as the refiner is slightly better than the SiT-S. These results demonstrate that our framework is robust to the choice of model architectures for the velocity refiner.

**Comparisons of different order of the blocks.** According to the observation in Figure 1, the velocity during the sampling would become stable at the final few steps, where we adopted a lightweight refiner to regress the velocity offset. Besides, our pseudo corrector is designed to efficiently achieve

Table 4: **Comparisons of different orders of the blocks.** We compare the results of changing the orders of Heun’s block (\(H\)), pseudo corrector block (\(P\)), and the velocity refiner block (\(R\)). Our results show applying the blocks in \(H_{N_{H}}P_{N_{P}}R_{N_{R}}\) order yields the best trade-off between generation quality and speed.

Table 5: **Ablation of refiner architectures.**2-order convergence, which requires a 2-order intermediate result as initialization. This explains why we need several Heun's steps at the beginning. To further investigate how the order of the blocks would affect the sampling speed and quality, we perform experiments and summarize the results in Tables 3(a) and 3(b). First, we find changing the order of the sampling blocks will cause worse sampling quality. Second, we show that sequentially using multiple blocks (_e.g._, \([H_{N_{H}}P_{N_{P}}R_{N_{R}}]_{ k}\)) will cost the same inference time as \(H_{kN_{H}}P_{kN_{P}}R_{kN_{R}}\) but lead to worse visual quality.

**Qualitative results and extensions.** We provide qualitative results of high-resolution text-to-image generation by applying FlowTurbo to the newly released flow-based model Lumina-Next-T2I . Since Lumina-Next-T2I adopts a heavy language model Gemma-2B  to extract text features and generates high-resolution images (\(1024 1024\)), the inference speed of it is slower than SiT . In Figure 3(a), we show that our FlowTurbo can generate images with better quality and higher inference speed compared with the baseline Heun's method. Besides, since FlowTurbo remains the multi-step sampling paradigm, it can be seamlessly applied to more applications like image inpainting, image editing, and object removal (Section 4.4). Please also refer to the Appendix C for the detailed implementation of various tasks.

**Limitations and broader impact.** Despite the effectiveness of FlowTurbo, our velocity refiner highly relies on the observation that the velocity is a "stable value" during the sampling. However, we have not found such a stable value in diffusion-based models yet, which might limit the application. Besides, the abuse of FlowTurbo may also accelerate the generation of malicious content.

## 5 Conclusion

In this paper, we introduce FlowTurbo, a novel framework designed to accelerate flow-based generative models. By leveraging the stability of the velocity predictor's outputs, we propose a lightweight velocity refiner to adjust the velocity field offsets. This refiner comprises only about 5% of the original velocity predictor's parameters and can be efficiently trained in under 6 GPU hours. Additionally, we have proposed a pseudo corrector that reduces the number of model evaluations while maintaining the same convergence order as the second-order Heun's method. Furthermore, we propose a sample-aware compilation technique to enhance sampling speed. Extensive experiments on various flow-based generative models demonstrate FlowTurbo's effectiveness on both class-conditional image generation and text-to-image generation. We hope our work will inspire future efforts to accelerate flow-based generative models across various application scenarios.