# Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm

Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm

 Sattar Vakili

MediaTek Research

sattar.vakili@mtkresearch.com &Julia Olkhovskaya

TU Delft

julia.olkovskaya@gmail.com

###### Abstract

Reinforcement learning utilizing kernel ridge regression to predict the expected value function represents a powerful method with great representational capacity. This setting is a highly versatile framework amenable to analytical results. We consider kernel-based function approximation for RL in the infinite horizon average reward setting, also referred to as the undiscounted setting. We propose an _optimistic_ algorithm, similar to acquisition function based algorithms in the special case of bandits. We establish novel _no-regret_ performance guarantees for our algorithm, under kernel-based modelling assumptions. Additionally, we derive a novel confidence interval for the kernel-based prediction of the expected value function, applicable across various RL problems.

## 1 Introduction

Reinforcement learning (RL) has demonstrated substantial practical success across a variety of application domains, including gaming (Silver et al., 2016; Lee et al., 2018; Vinyals et al., 2019), autonomous driving (Kahn et al., 2017), microchip design (Mirhoseini et al., 2021), robot control (Kalashnikov et al., 2018), and algorithmic search (Fawzi et al., 2022). This empirical success has prompted deeper investigations into the analytical understanding of RL, especially in complex environments. Over the past decade, significant advances have been made in establishing theoretically grounded algorithms for various settings. In this work, we focus on the infinite horizon average reward setting, also known as the undiscounted setting (Wei et al., 2020, 2021). This setting is particularly well-suited for applications that involve continuing operations not divided into episodes such as load balancing and stock market operations. In contrast to the episodic setting (Jin et al., 2020) and the discounted setting (Zhou et al., 2021), theoretical understanding of RL algorithms is relatively limited for the undiscounted setting. We develop a computationally efficient algorithm and establish its theoretical performance guarantees in the undiscounted case.

There is a natural progression in the complexity of RL models corresponding to the structural complexity of the Markov Decision Process (MDP). This progression ranges from tabular models to linear, kernel-based, and deep learning-based models. The kernel-based structure is an extension of linear structure to an infinite-dimensional linear model in the feature space of a positive definite kernel, resulting in a highly versatile model with great representational capacity for nonlinear functions. In addition, the closed-form expressions for the prediction and the uncertainty estimate in kernel-based models allow the development of algorithms based on nonlinear function approximation that are amenable to theoretical analysis. Kernel-based models also serve as an intermediate step towards understanding the deep learning-based models (see, e.g., Yang et al., 2020) based on the Neural Tangent (NT) kernel approach (Jacot et al., 2018).

The infinite-horizon average-reward setting has been extensively explored under the tabular structure (Auer et al., 2008; Wei et al., 2020; Zhang and Xie, 2023). Under the performance measure of _regret_, defined as the difference in the total reward achieved by a learning algorithm over \(T\) steps and that of the optimal stationary policy, performance bounds of \(((||,||))\) have been established (see, e.g., Zhang et al., 2020), where \(\) and \(\) represent the state and action spaces, respectively, and the regret grows polynomial with their sizes. It is assumed for these results that the MDP is weakly communicating, a condition necessary for achieving sublinear regret (Bartlett and Tewari, 2009). Averaged over \(T\) steps, the regret diminishes as \(T\) increases, thereby offering what is known as a _no-regret_ performance guarantee. The applicability of the tabular setting is limited, as many real-world problems feature very large or potentially infinite state-action spaces. Consequently, recent literature has explored the use of function approximation in RL, particularly through linear models (Abbasi-Yadkori et al., 2019, 2019; Hao et al., 2021; Wei et al., 2021). This approach represents the value function or the transition model via a linear transformation applied to a predefined feature mapping. In the linear setting, regret bounds of \(((dT)^{})\) have been established (Wei et al., 2021), where \(d\) represents the ambient dimension of the linear feature map. Kernel-based models can be considered as linear models in the feature space of the kernel. That, however, is often infinite dimensional (\(d=\)). As such, the results with linear models do not translate to the kernel-based settings, necessitating novel analytical techniques. Also, for a discussion on further limitations of the linear models, see Lee and Oh (2023).

In this work, we propose the first RL algorithm in the infinite horizon average reward setting with non-linear function approximation using kernel-ridge regression. This is one of the most general models that lends well to theoretical analysis. Our algorithm, referred to as Kernel-based Upper Confidence Bound (KUCB-RL), utilizes kernel ridge regression to build predictor and uncertainty estimates for the expected value function. Inspired by the principle of optimism in the face of uncertainty and equipped with these statistics, KUCB-RL builds an upper confidence bound on the state-action value function over a future window of \(w\) steps. This bound serves as a proxy \(q_{t}\), at each step \(t\), for the state-action value function over this future window. At each step \(t\) with the current state \(s_{t}\), the action is selected greedily with respect to this proxy: \(a_{t}=_{a}\,q_{t}(s_{t},a)\). This approach resembles the acquisition function based algorithms such as GP-UCB and GP-TS, using Upper Confidence Bound and Thompson sampling, respectively, in the context of kernel-based bandits, also known as Bayesian optimization (Srinivas et al., 2010; Chowdhury and Gopalan, 2017). Kernel-based bandit setting corresponds to the degenerate case of \(||=1\). In comparison, in the RL setting, the action is selected based on the current state, and the reward depends on both the state and the action. A kernel-based model is used to provide predictions for the expected value function, which varies due to the Markovian nature of the temporal dynamics. This makes the RL problem significantly more challenging than the bandit problem where the predictions are derived for a fixed reward function. To address this latter challenge, we derive a novel kernel-based confidence interval that is applicable across RL problems.

### Contributions

To summarize, our contributions are as follows. We develop a kernel based optimistic algorithm for the infinite horizon average reward setting, referred to as KUCB-RL. We establish no-regret guarantees for the proposed learning algorithm, which is the first for this setting to the best of our knowledge. Specifically, in Theorem 3, we prove a regret bound of \((+(w+})})w^{2}( T;)(T/w;)})\), at a \(1-\) confidence level, where \(\) is the parameter of kernel ridge regression and \((T;)\) is the maximum information gain, a kernel specific complexity term (see Section 2). This regret bound translates to \(}(d^{}T^{})\) in the special case of a linear model, recovering the best existing results (Wei et al., 2021) in dependence on \(T\), and improving by a factor of \(d^{}\). When applied to very smooth kernels with exponential eigendecay such as the Squared Exponential (SE) kernel, we obtain a regret of \(}(T^{})\), with the notation \(}\) hiding logarithmic factors. For one of the most general cases, the kernels with polynomial eigendecay with parameter \(p>1\) (See Definition 1), that includes, for example, the Matern family and NT kernels, we show that our regret bound translates to \(}(T^{})\), which constitutes a no-regret guarantee. To highlight the significance of this result, we point out that no-regret guarantees for GP-UCB in the degenerate case of bandits were established only recently in Whitehouse et al. (2024), while the initial studies of GP-UCB (as well as GP-TS) (Srinivas et al., 2010; Chowdhury and Gopalan, 2017) did not provide no-regret guarantees for the case of polynomial eigendecay. As part of our analysis, in Theorem 1, we develop a novel confidence interval applicable across kernel-based RL problems that contributes to the eventual improved results.

### Related Work

The vast RL literature can be categorized across various dimensions. In addition to the average reward, episodic, and discounted settings, as well as tabular, linear, and kernel-based structures mentioned above, other notable distinctions among settings include model-based versus model-free approaches, and offline versus online versus settings where the existence of a generative model is assumed (allowing the learning algorithm to sample the state-action of its choice at each step, rather than following the Markovian trajectory). Covering the entire breadth of RL literature is challenging. Here, we will focus on highlighting and providing comparisons with the most closely related works, particularly in terms of their setting and structure.

The kernel-based MDP structure has been considered in several recent works under the episodic setting (Yang et al., 2020; Vakili and Olkhovskaya, 2023; Chowdhury and Oliveira, 2023; Domingues et al., 2021; Vakili, 2024). The regret bound proven in Yang et al. (2020) for the episodic setting applies only to very smooth kernels such as SE kernel. Vakili and Olkhovskaya (2023) addressed this limitation by extending the results to Matern and NT families of the kernels, albeit with a sophisticated algorithm that actively partitions the state-action domain into possibly many subdomains, using only the observations within each subdomain to obtain kernel-based prediction and uncertainty estimates. Their work is also based on a particular assumption that relates the kernel eigenvalues to the size of the domain. The work of Chowdhury and Oliveira (2023) is most closely related to ours in terms of kernel-related assumptions. Specifically, our Assumption 4 is identical to Assumption 1 of Chowdhury and Oliveira (2023). They establish a regret bound of \((H(N;))\) for the episodic MDP setting, where \(N\) is the number of episodes, \((N;)\) is the maximum information gain, a kernel-related complexity term, \(H\) is the episode length and the value of \(\) is a fixed constant close to \(1\). However, their regret bounds do not apply to general families of kernels, such as those with polynomially decaying eigenvalues (see Section 2.2 for the definition) including Matern and NT kernels, as for this family of kernels \((N;)\) possibly grows faster than \(\). As a result, a no-regret guarantee cannot be established in many cases of interest. In comparison, the infinite horizon setting considered in this work is more challenging than the episodic setting as evident when comparing these settings with linear modeling. For this more challenging setting, we establish no-regret guarantees. A key element of our improved results is the novel confidence interval we utilize in our analysis (Theorem 1). This result is general and can be used across RL problems, for example, improving the results of Chowdhury and Oliveira (2023) as well.

In the tabular case, a lower bound of \((|||T})\) on regret was established in Auer et al. (2008) in the infinite-horizon average-reward setting, where \(D\) is the diameter of the MDP. For ergodic MDPs, Wei et al. (2020) shows a regret bound of \(}(}^{3}||||T})\), where \(t_{}\) is the mixing time of an ergodic MDP. Furthermore, under the broader assumption of weakly communicating MDPs, which is necessary for low regret (Bartlett and Tewari, 2012), the best existing regret bound of model-free algorithms is \(}(||^{5}||^{2})\), achieved by the recent work of Zhang and Xie (2023). Several works have studied linear function approximation in the infinite horizon average reward setting under strong assumptions of uniformly mixing and uniformly excited feature conditions (Abbasi-Yadkori et al., 2019, 2019; Hao et al., 2021). Notably, Hao et al. (2021) achieved a regret bound of \(}(}^{3}T})\) under the linear bias function assumption, where \(\) is the smallest eigenvalue of policy-weighted covariance matrix. Under the much less restrictive setting of Bellman optimality equation assumption (Assumption 1) for linear MDP, Wei et al. (2021) provides an algorithm with regret guarantee of \(}((dT)^{3/4})\). We also consider our kernel-based approach under this general assumption on MDP. Furthermore, for examples of infeasible algorithms in the literature, see Wei et al. (2021), Algorithm 1. There also exists a separate model-based approach to the problem where the transition probability distribution (model) is learned and used for planning, usually requiring high memory and computational complexity and utilizing substantially different techniques and assumptions. While this approach is studied under tabular settings (Bartlett and Tewari, 2009; Auer et al., 2008) and linear settings (Wu et al., 2022), it is not clear whether model-based approaches can be feasibly constructed in the kernel-based setting, due to the space complexity of a kernel-based model.

Our work is also related to the simpler problem of kernelized bandits (Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021; Li and Scarlett, 2022; Salgia et al., 2021). Our construction of the confidence interval for the RL setting has been inspired by the previous work on bandits, utilizing novel analysis introduced in Whitehouse et al. (2024). Bandit settings can be considered a degenerate case of the RL framework with \(||=1\). In comparison, the temporal dependencies of MDP introduce substantial challenges, and the confidence intervals used in the bandit setting cannot be directly applied.

We summarize the most closely related work with a focus on model-free feasible algorithms in Table 1. We present the existing regret bounds under various assumptions on MDP and its structure (tabular, linear, kernel-based). The assumptions include weakly communicating MDP (See Puterman, 1990, Section 8.3.1), Bellman optimality equation (our Assumption 1), and uniform mixing assumption (see Wei et al., 2021, Assumption 3). For a formal definition of linear MDP, see Wei et al. (2021), Assumption 2, and for the linear bias function case, see Wei et al. (2021), Assumption 4.

## 2 Problem Formulation

In this section, we overview the background on infinite horizon average reward (undiscounted) MDPs and kernel based modelling.

### Infinite Horizon Average Reward MDP

An undiscounted MDP is described by the tuple \((,,r,P)\) where \(\) is a state space with a possibly infinite number of elements, \(\) is a finite action set, \(r:\) is the reward function, and \(P(|s,a)\) is the unknown transition probability distribution over \(\) of the next state when action \(a\) is selected at state \(s\). Throughout the paper we use the notation \(z=(s,a)\) for the state-action pairs, and \(=\).

The learner interacts with the MDP through \(T\) steps, starting from an arbitrary initial state \(s_{1}\). At each step \(t\), the learner observes state \(s_{t}\) and takes an action \(a_{t}\) resulting in a reward \(r(s_{t},a_{t})\). The next state \(s_{t+1}\) is revealed as a sample drawn from the transition probability distribution: \(s_{t+1} P(|s_{t},a_{t})\).

The goal of the learner is to compete against any fixed stationary policy. A stationary policy \(:\) is a possibly random mapping from the states to actions. The long-term average reward of a stationary policy \(\), starting from state \(s\), is defined as:

\[J^{}(s)=_{T}[._ {t=1}^{T}r(s_{t},a_{t})|s_{1}=s, t 1,a_{t}=(s_{t}),s_{t+1}  P(|s_{t},a_{t})].\]

We assume that the MDP belongs to the broad class of MDPs where the following form of the Bellman optimality equation holds:

**Assumption 1** (Bellman optimality equation).: _There exists \(J^{}\) and bounded measurable functions \(v^{}:\) and \(q^{}:\) such that the following conditions are satisfied for all

  
**Algorithm** & **Regret** & **MDP Assumption** & **Structure** \\  UCB-AVG (Zhang and Xie, 2023) & \(}(||^{5}||^{2})\) & Weakly Communicating & Tabular \\ OLSVLFH (Wei et al., 2021) & \(}((dT)^{3/4})\) & Bellman Optimality Eq. & Linear \\ MDP-Exp2 (Wei et al., 2021) & \(}(}^{2}}|||T)\) & Uniform Mixing & Linear Bias function \\
**KUCB-RL** (Algorithm 1) & \(}(d^{}T^{})\) & Bellman Optimality Eq. & Linear \\
**KUCB-RL** (Algorithm 1) & \(}(T^{})\) & Bellman Optimality Eq. & Kernel-based (exponential) \\
**KUCB-RL** (Algorithm 1) & \(}(T^{})\) & Bellman Optimality Eq. & Kernel-based (polynomial) \\   

Table 1: Summary of the existing regret bounds in the infinite horizon average reward setting under various cases with respect to MDP structure (tabular, linear, kernel based) and assumptions.

states \(s\) and actions \(a\) :_

\[J^{}+q^{}(s,a)=r(x,a)+_{s^{} P(|s,a)}[v^{ }(s^{})], v^{}(s)=_{a}q^{}(s,a).\] (1)

This assumption was also used for the linear MDP case in Wei et al. (2021). By applying the Bellman optimality equation, it can be shown that a policy \(^{}(s)=*{arg\,max}_{a}q^{}(s,a)\), which deterministically selects actions that maximize \(q^{}\) in the current state, is the optimal policy \(^{}=*{arg\,max}_{}J^{}\), with \(J^{^{}}(s)=J^{}\), for all \(s\)(Wei et al., 2021).

For the finite state setting, Assumption 1 follows from the weakly communicating MDP assumption (see, e.g., Puterman, 1990, Chapter \(9\)). Assumption 1 also holds under several other common conditions (Hernandez-Lerma (2012), Section 3.3).

The learner's performance is measured by _regret_, which is defined as the difference in total reward between the learner and the optimal stationary policy. Specifically,

\[(T)=_{t=1}^{T}(J^{}-r(s_{t},a_{t})).\] (2)

We emphasize that under Assumption 1, for any initial state \(s_{1}\), \(J^{^{}}(s_{1})=J^{}\), that is reflected in our regret definition.

For any value function \(v:\), throughout the paper, we use the notation

\[[Pv](z)=_{s^{} P(|z)}[v(s^{})]\]

for the expected value function of the next state.

### Kernel-Based Models and the RKHS

Consider a positive definite kernel \(k:\). Let \(_{k}\) be the reproducing kernel Hilbert space (RKHS) induced by \(k\), where \(_{k}\) contains a family of functions defined on \(\). Let \(,_{_{k}}:_{k} _{k}\) and \(\|\|_{_{k}}:_{k}\) denote the inner product and the norm of \(_{k}\), respectively. The reproducing property implies that for all \(f_{k}\), and \(z\), \( f,k(,z)_{_{k}}=f(z)\). Mercer theorem implies that \(k\) can be represented using a possibly infinite dimensional feature map:

\[k(z,z^{})=_{m=1}^{}_{m}_{m}(z)_{m}(z^{ }),\] (3)

where \(_{m}>0\), and \(}_{m}_{k}\) form an orthonormal basis of \(_{k}\). In particular, any \(f_{k}\) can be represented using this basis and weights \(w_{m}\) as

\[f=_{m=1}^{}w_{m}}_{m},\]

where \(\|f\|_{_{k}}^{2}=_{m=1}^{}w_{m}^{2}\). A formal statement and the details are provided in Appendix 8. We refer to \(_{m}\) and \(_{m}\) as (Mercer) eigenvalues and eigenfunctions of kernel \(k\), respectively.

### Kernel-Based Prediction

Kernel-based models provide powerful predictors and uncertainty estimators which can be leveraged to guide the RL algorithm. In particular, consider a fixed unknown function \(f_{k}\). Assume a \(t 1\) vector of noisy observations \(_{t}=[y_{i}=f(z_{i})+_{i}]_{i=1}^{t}\) at observation points \(\{z_{i}\}_{i=1}^{t}\) is provided, where \(_{i}\) are independent zero mean noise terms. Kernel ridge regression provides the following predictor and uncertainty estimate, respectively (see, e.g., Scholkopf et al., 2002),

\[_{t}(z) =k_{t}^{}(z)(K_{t}+ I)^{-1}_{t},\] \[_{t}^{2}(z) =k(z,z)-k_{t}^{}(z)(K_{t}+ I)^{-1}k_{t}(z),\] (4)

where \(k_{t}(z)=[k(z,z_{1}),,k(z,z_{t})]^{}\) is a \(t 1\) vector of the kernel values between \(z\) and observations, \(K_{t}=[k(z_{i},z_{j})]_{i,j=1}^{t}\) is the \(t t\) kernel matrix, \(I\) is the identity matrix appropriately sized to match \(K_{t}\), and \(>0\) is a free regularization parameter.

Confidence bounds of the form \(|f(z)-_{t}(z)|()_{t}(z)\) are established, for a confidence interval width multiplier \(()\) at a confidence level \(1-\), which depends on the assumptions on the setting and the noise. We will establish a such confidence interval specific to the RL setting, in Theorem 1, and utilize it in our regret analysis.

### Kernel-Based Modelling in RL

In our RL setting, we use a kernel-based model to predict the expected value function. In particular, for a given transition probability distribution \(P(s^{}|,)\) and a value function \(v:\), we define \(f=[Pv]\) and use past observations to form predictions and uncertainty estimates for \(f\), as detailed in the following section. The value functions vary due to the Markovian nature of the temporal dynamics. To effectively use the confidence intervals established by the kernel-based models on \(f\), we require the following assumption.

**Assumption 2**.: _We assume \(P(s^{}|,)_{k}\), for some positive definite kernel \(k\), and \(\|P(s^{}|,)\|_{_{k}} 1\) for all \(s^{}\)._

### Eigendecay and Information Gain

Our regret bounds are presented in terms of maximum information gain which is a kernel-specific complexity term. Specifically, for a kernel \(k\) and sets of observation points \(\{z_{i}\}_{i=1}^{t}\), we define the maximum information gain \((t;)\) as follows

\[(t;)=_{\{z_{i}\}_{i=1}^{t}} (I+}{}),\]

where \(>0\), and \(K_{t}\) is the kernel matrix defined in Section 2.3. Several works have established upper bounds on \((t;)\). In the special case of a \(d\)-dimensional linear kernel, we have \((t;)=(d(t))\). For kernels with exponential eigendecay, including SE, \((t;)=((t))\)(Srinivas et al., 2010; Vakili et al., 2021). For kernels with polynomial eigendecay, which represent a crucial case due to challenges in establishing no-regret guarantees in RL and bandits, and include kernels of both practical and theoretical interest such as the Matern family and NT kernels, we first provide the definition below and then the bound on \(\).

**Definition 1**.: _A kernel \(k\) is said to have a \(p\)-polynomial eigendecay if \( m 1\), \(_{m} Cm^{-p}\), for some \(p>1\), \(C>0\) where \(_{m}\) are the Mercer eigenvalues of the kernel in decreasing order._

For kernels with \(p\)-polynomial eigendecay, we have (Vakili et al., 2021, Corollary 1):

\[(t;)=(()^{} ((1+))^{1-}).\]

## 3 KUCB-RL Algorithm

In this section, we introduce our algorithm, Kernel-based Upper Confidence Bound for Reinforcement Learning (KUCB-RL). The algorithm's structure is similar to acquisition-based kernel bandit algorithms such as GP-UCB (Srinivas et al., 2010), where each action is chosen as the maximizer of an acquisition function. We construct an optimistic proxy \(q_{t}\) for the state-action value function. At each step \(t\), given the current state \(s_{t}\), the action \(a_{t}\) is selected as the maximizer of \(q_{t}(s_{t},a)\) over \(a\). This proxy \(q_{t}\) is derived using past observations of transitions, employing kernel ridge regression to provide a prediction and uncertainty estimate for the state-action value function over a future window of size \(w\). The proxy is established as an upper confidence bound, following the principle of optimism in the face of uncertainty. The value functions are computed in batches of \(w\) steps, and the derived policies are unrolled over the subsequent \(w\) steps. The details are presented next.

We define a fixed window size, \(w\), which represents the future interval that the algorithm will consider. For a given \(t_{0}\) where \((t_{0} w)=0\), including \(t_{0}=0\), we initialize \(v_{t_{0}+w+1}(s)=0, s\), reflecting the algorithm's consideration of the reward within this future window of size \(w\). Subsequently, we recursively obtain proxies \(q_{t}\) and \(v_{t}\) for all steps \(t\{t:t_{0}+1 t t_{0}+w\}\). Let \(f_{t}\) denote \([Pv_{t+1}]\), \(_{t}\) represent the kernel ridge predictor of \([Pv_{t+1}]\), and \(_{t}\) be its uncertainty estimator. The predictor and the uncertainty estimator are derived using the data set \(_{t_{0}}\), which contains observations of past transitions up to \(t_{0}\). We use the notation \(_{t}=\{(s_{j},a_{j},s_{j+1})\}_{j t}\) for the past transitions, and also define \(_{t+1,t_{0}}=[v_{t+1}(s_{2}),v_{t+1}(s_{3}),,v_{t+1}(s_{t_{0}+ 1})]^{}\), for the values of the proxy value function at the history of state observations. We then have

\[_{t}(z)=k_{t_{0}}^{}(z)(K_{t_{0}}+ I)^{ -1}_{t+1,t_{0}},\] \[_{t}^{2}(z)=k(z,z)-k_{t_{0}}^{}(z)(K_{t_{0}}+ I )^{-1}k_{t_{0}}(z),\] (5)

where \(k_{t}(z)=[k(z,z_{1}),k(z,z_{2}),,k(z,z_{t}))]^{}\) denotes the vector of kernel values between \(z\) and \((z_{j}=(s_{j},a_{j}))_{j t}\) in the history of observations, and \(K_{t}=[k(z_{i},z_{j})]_{i,j=1}^{t}\) denotes the kernel matrix.

Equipped with the kernel ridge predictor and uncertainty estimator, we define \(q_{t}\) as an upper confidence bound for \(f_{t}\), as follows:

\[q_{t}(z)=_{[0,w]}(r(z)+_{t}(z)+()_{t}(z) ),\ \  z,\] (6)

where \(1-\) represents a confidence level, and \(()\) is a confidence interval width multiplier; the specific value of which is given in Theorem 3. The notation \(_{[a,b]}()\) is used for projection on \([a,b]\) interval. This step is natural since with the assumption \(r:\) the value over a window of size \(w\) can not be more than \(w\). We also define

\[v_{t}(s)=_{a}q_{t}(s,a),\ \  s.\] (7)

By iteratively updating from \(t=t_{0}+w\) to \(t=t_{0}+1\), we compute the values of \(q_{t}\) and \(v_{t}\) for all \(t\) from \(t_{0}+1\) to \(t_{0}+w\). Then, we unroll the learned policy over the subsequent \(w\) steps, as the greedy policy with respect to \(q_{t}\):

\[a_{t}=*{arg\,max}_{a}q_{t}(s_{t},a).\] (8)

A pseudocode is provided in Algorithm 1.

```
0: Regularization parameter \(\), window size \(w\), confidence interval width multiplier \(\), confidence level \(1-\), \(,,r\).
1:for\(t=0,1,2,\)do
2:if\((t w)=0\)then
3: Let \(v_{t+w+1}=\);
4:for\(h=1,2,,w\)do
5: Compute \(q_{t+w+1-h}\) and \(v_{t+w+1-h}\) using equations (6) and (7).
6:endfor
7:endif
8: Select \(a_{t}=*{arg\,max}_{a}q_{t}(s_{t},a)\); Observe \(s_{t+1} P(|s_{t},a_{t})\) and receive \(r(s_{t},a_{t})\).
9:endfor ```

**Algorithm 1** Kernel-based Upper Confidence Bound for Reinforcement Learning (KUCB-RL)

Computational Complexity.KUCB-RL enjoys a polynomial computational complexity of \((}{w})\), where the bottleneck is the matrix inversion step in (5) in kernel ridge regression every \(w\) steps. This is not unique to our work and is common across kernel-based supervised learning, bandit, and RL literature. Luckily, sparse approximation methods such as Sparse Variational Gaussian Processes (SVGP) or the Nystrom method significantly reduce the computational complexity of matrix inversion step (to as low as linear in some cases), while maintaining the kernel-based confidence intervals and, consequently, the eventual rates (see, e.g., Vakili et al., 2022, and references therein). These results are, however, generally applicable and not specific to our problem.

## 4 Regret Bounds for KUCB-RL

In this section, we provide analytical results on the performance of KUCB-RL. We prove the first sublinear regret bounds in undiscounted RL setting under general assumptions based on kernel-based modelling. We first derive a novel confidence interval that is broadly applicable to the kernel-based RL problems. We then utilize this result to establish bounds on the regret of KUCB-RL.

### Confidence Intervals for Kernel Based RL

The analysis of our algorithm utilizes confidence intervals of the form \(|f_{t}(z)-_{t}(z)|()_{t}(z)\), where \(f_{t}=[Pv_{t}]\) denotes the expected value of a value function \(v_{t}\), and \(_{t}\) and \(_{t}\) represent the kernel ridge predictor and the uncertainty estimate of \(f_{t}\). Here, \(()\) represents the width multiplier for the confidence interval at a \(1-\) confidence level. Similar confidence intervals are established in kernel ridge regression for a fixed function \(f\) in the RKHS of a specified kernel \(k\)(see, e.g., Abbasi-Yadkori, 2013; Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021a; Whitehouse et al., 2024). In the RL context, specific considerations are required as both \(f_{t}=[Pv_{t}]\) and the observation noise depend on the value function \(v_{t}\) that varies due to the Markovian nature of the temporal dynamics. We note that in this setting, for a given value function \(v:\), the observation noise is captured by \(v(s_{t+1})-[Pv](s_{t},a_{t})\). A possible approach involves deriving confidence intervals that apply to a class \(\) of value functions. Such results appear in some of the existing work (Chowdhury and Oliveira, 2023; Vakili and Olkhovskaya, 2023). The result most closely related to our is Chowdhury and Oliveira (2023), which derives its confidence interval under the exact same kernel related assumptions as our work, but for the episodic MDP setting. With the same assumptions, the confidence interval that we establish is different from the one in Chowdhury and Oliveira (2023). In particular, their confidence interval is applicable to a specific value of kernel ridge regression parameter \(\), constrained by their proof techniques. Inspired by Whitehouse et al. (2024), which established a confidence interval for kernel ridge regression (not within the RL context) but allowed for a judicious selection of \(\), we prove a new confidence interval suitable for the RL setting that allows tuning parameter \(\). As a result, we obtain the first improved no-regret algorithms in this setting.

**Theorem 1** (Confidence Bound).: _Consider \(v:\), a conditional probability distribution \(P(s|z)\), \(s\), \(z\), and two positive definite kernels \(k:\) and \(k^{}:\), where \(=\) is compact subset of \(^{d}\). Let \(f=[Pv]\) and assume \(\|v\|_{_{k^{}}} C_{v}\), \(v(s) w, s\), and \(\|f\|_{_{k}} C_{f}\), for some \(C_{v},w,C_{f}>0\). A dataset \(\{(z_{i},s^{}_{i})\}_{i=1}^{n}()^{n}\) is provided such that \(s^{}_{i} P(|z^{i})\). Let \(_{m},\,m=1,2,\) denote the Mercer's eigenvalues of \(k^{}\) in a decreasing order and \(_{m}\) denote the corresponding eigenfunctions, with \(_{m}_{}\) for some \(_{}>0\)._

_Let \(_{n}\) and \(_{n}\) be the kernel ridge predictor and the uncertainty estimate of \(f\) using the observations:_

\[_{n}(z)=k_{n}^{}(z)( I+K_{n})^{-1}_{n},\ \ \ \ _{n}^{2}(z)=k(z,z)-k_{n}^{}(z)( I+K_{n})^{-1}k_{n}(z),\]

_where \(_{n}=[v(s^{}_{1}),v(s^{}_{2}),,v(s^{}_{n}))]^{}\) is the vector of observations._

_For all \(z\) and \(v:\|v\|_{_{k^{}}} C_{v}\), the following holds, with probability at least \(1-\),_

\[|f(z)-_{n}(z)|()_{n}(z),\]

_with \(()=\)_

We can simplify the presentation of \(\) under the following assumption.

**Assumption 3**.: _For the kernel \(k^{}\), we assume that for some \(C_{1},C_{2}\) and \(q>0\), \(_{m=1}^{M}_{m} C_{1}\) and, \(_{m=M+1}^{}_{m} C_{2}M^{-q}\) for any \(M\)._

This is a mild assumption. For example, a \(p\)-polynomial eigendecay profile with \(p>1\), which applies to a large class of common kernels including SE, Matern and NT kernels, satisfies this assumption with \(C_{1}=\), \(C_{2}=\), and \(q=p-1\), where \(C\) is the constant specified in Definition 1.

**Remark 2**.: _Under Assumption 3, the expression of \(\) in Theorem 3 can be simplified as_

\[()=(C_{f}+}{})+(;n)}).\]

Remark 2 can be observed by selecting \(M= n^{}\) in the expression of \(()\), which provides a straightforward presentation of the confidence interval width multiplier.

The proof of Theorem 1 involves the Mercer representation of \(v\) in terms of \(_{m}\). The expression of the prediction error \(|f(z)-_{n}(z)|\) is then decomposed into error terms corresponding to each \(_{m}\). We then partition these terms into the first \(M\) elements corresponding to eigenfunctions with the largest \(M\) eigenvalues and the rest. For each of the first \(M\) eigenfunctions, we obtain high probability bounds using existing confidence intervals from Whitehouse et al. (2024). Summing up over \(m\), and using a bound based on uncertainty estimates, we achieve a high probability bound--corresponding to the second term in the expression of \(()\). We then bound the remaining \(m>M\) elements based on the decay of Mercer eigenvalues--corresponding to the third term in the expression of \(()\). A detailed proof is provided in Appendix 6.

Theorem 1 is presented in a self-contained way, making it broadly applicable across various RL settings. In the following section, we apply this theorem within the analysis of the infinite horizon average reward setting to obtain a no-regret algorithm. This is the first no-regret algorithm within this setting under general kernel-related assumptions.

### Regret Analysis of KUCB-RL

The weakest assumption regarding value functions is realizability, which suggests that the optimal value function \(v^{}\) either belong to the an RKHS or are at least well-approximated by its elements. In the degenerate case of bandits with \(||=1\), realizability alone is sufficient for provably efficient algorithms (Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021). However, for general MDPs, realizability is inadequate, necessitating stronger assumptions (Jin et al., 2020; Wang et al., 2019; Chowdhury and Oliveira, 2023). Building on these works, our main assumption involves a closure property for all value functions within the following class:

\[=\{s\{w,_{a}\{r(s, a)+^{}(s,a)+^{}(s,a)^{-1}(s,a)}\}\}\},\] (9)

where \(\) and \(>0\), \(\|\|<\), and \(\) is an \(\) matrix operator such that \( I\) for some \(>0\), and \(=[_{1},_{2},]\), where \(_{m}=}_{m}\), and \(_{m}\) and \(_{m}\) are the Mercer eigenvalues and eigenfunctions corresponding to a kernel \(k\) defined on \(\). We assume \(\) is a subset of the RKHS of a kernel \(k^{}\) defined on \(\).

**Assumption 4** (Optimistic Closure).: _For any \(v\), and for some positive constant \(C_{v}\), we have \(\|v\|_{k^{}} C_{v}\). Additionally, for \(v:[0,w]\), we assume \(C_{v}=(w)\)._

This technical assumption is the same as Assumption 1 in Chowdhury and Oliveira (2023). The optimistic closure assumption in the kernel-based setting is strictly weaker than the ones explored in the context of generalized linear function approximation (Wang et al., 2020).

**Theorem 3**.: _Consider the undiscounted MDP setting described in Section 2. Run KUCB-RL given in Algorithm 1 for \(T\) steps. Under Assumptions 1, 2, 3, and 4, the regret of KUCB-RL, defined in Equation (2), satisfies, with probability at least \(1-\)_

\[(T)=(+(w+} {(T;)+()})w^{2}(T;)(T/w;)}).\]

The proof of Theorem 3 utilizes standard methods from the analysis of optimistic algorithms in RL and bandits, such as the elliptical potential lemma, leverages the confidence interval proven in Theorem 1, and also incorporates novel techniques. Algorithm 1 updates the observation set every \(w\) steps, requiring us to characterize and bound the effect of this delay in the proof. A straightforward application of the elliptical potential lemma results in loose bounds that do not guarantee no-regret. In Lemma 4, we establish a tight bound on the sum of standard deviations of a sequence of points with delayed updates of the observation sets, contributing to the improved regret bounds. This is independently a useful result in other settings with delayed updates, such as delayed feedback settings (Vakili et al., 2023; Kuang et al., 2023) or when observations are provided in a batch (Chowdhury and Gopalan, 2019). The details are provided in Appendix 7.

There is an apparent trade-off in choosing the window size. Intuitively, this trade-off balances the strength of the value function against the strength of the noise. A larger \(w\) is preferred to capture the long-term performance of the policy, but a larger \(w\) also increases the observation noise affecting the prediction error in kernel ridge regression. The optimal window size results from an interplay between these two factors, which is reflected in the regret bound.

We next instantiate our regret bounds for some special cases. In the linear case, with a choice of \(w=T^{}d^{}\) and replacing the bound on \((T;)\), we obtain \((T)=}(d^{}T^{})\), recovering the existing results in their dependence on \(T\) and improving by a factor of \(d^{}\). For kernels with exponential eigendecay, with a choice of \(w=T^{}\) and replacing the bound on \((T;)\), we obtain \((T)=}(T^{})\). We formalize the result with \(p\)-polynomial kernels in the following remark as it may be of broader interest.

**Remark 4**.: _Under the setting of Theorem 3, with a \(p\)-polynomial kernel, with the choice of parameters, \(w=T^{}\) and \(=T^{}\), we obtain the following no-regret guarantee \((T)=}(T^{})\)._

In the case of a Matern kernel with smoothness parameter \(\), where \(p=1+\), the regret bound translates to \((T)=(T^{})\). This also directly extends to NT kernels using the equivalence between the RKHS of Matern kernels and NT kernels with the appropriate smoothness (Vakili et al., 2023).

## 5 Discussion and Limitations

We proposed KUCB-RL in the infinite horizon average reward setting and proved no-regret guarantees with general kernels, including those with polynomial eigendecay such as Matern and NT kernels. To highlight the significance of our results, we note that in the case of episodic MDPs, the existing work of (Yang et al., 2020; Chowdhury and Oliveira, 2023) do not provide no-regret guarantees with general kernels. The work of Vakili and Olkhovskaya (2023) utilizes sophisticated domain partitioning techniques and relies on a specific assumption about the scaling of kernel eigenvalues with the size of the domain. We achieve improved rates on regret leveraging a confidence interval proven in Theorem 1, which is applicable across various RL problems. We next point out two main limitations of our work.

Regarding optimality, we can juxtapose our results with the \((T^{})\) lower bounds proven in (Scarlett et al., 2017), for the degenerate case of bandits with Matern kernel. Sophisticated algorithms, such as the _sup_ variation of optimistic algorithms and those based on sample or domain partitioning (Valko et al., 2013; Salgia et al., 2021; Li and Scarlett, 2022), achieve this lower bound up to logarithmic factors in the case of bandits. However, a no-regret \(}(T^{})\) guarantee, though suboptimal, for standard acquisition-based algorithms like GP-UCB has been provided only recently (Whitehouse et al., 2024). While we offer the first no-regret \(}(T^{})\) guarantee in the much more complex setting of RL, we cannot determine whether our results are improvable. This remains an area for future investigation.

Although RKHS elements of common kernels can approximate almost all continuous functions on compact subsets of \(^{d}\)(Srinivas et al., 2010), the optimistic closure assumption is somewhat limiting. A rigorous approach involves relaxing this assumption and finding an RKHS element that serves as an upper confidence bound on a function of interest \(f\) within the same RKHS. While this method appears to reasonably address the assumption, it is a technically involved problem that invites further contributions from researchers in the field.