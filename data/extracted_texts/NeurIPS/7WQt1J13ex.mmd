# Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport

Jaemoo Choi

Seoul National University

toony42@snu.ac.kr

Equal contribution. Correspondence to: Myungjoo Kang <mkang@snu.ac.kr>.

Jaewoong Choi

Korea Institute for Advanced Study

chwj1475@kias.re.kr

Myungjoo Kang

Seoul National University

mkang@snu.ac.kr

Equal contribution. Correspondence to: Myungjoo Kang <mkang@snu.ac.kr>.

###### Abstract

Optimal Transport (OT) problem investigates a transport map that bridges two distributions while minimizing a given cost function. In this regard, OT between tractable prior distribution and data has been utilized for generative modeling tasks. However, OT-based methods are susceptible to outliers and face optimization challenges during training. In this paper, we propose a novel generative model based on the semi-dual formulation of Unbalanced Optimal Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution matching. This approach provides better robustness against outliers, stability during training, and faster convergence. We validate these properties empirically through experiments. Moreover, we study the theoretical upper-bound of divergence between distributions in UOT. Our model outperforms existing OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36 on CelebA-HQ-256. The code is available at [https://github.com/Jae-Moo/UOTM](https://github.com/Jae-Moo/UOTM).

## 1 Introduction

Optimal Transport theory  explores the cost-optimal transport to transform one probability distribution into another. Since WGAN , OT theory has attracted significant attention in the field of generative modeling as a framework for addressing important challenges in this field. In particular, WGAN introduced the Wasserstein distance, an OT-based probability distance, as a loss function for optimizing generative models. WGAN measures the Wasserstein distance between the data distribution and the generated distribution, and minimizes this distance during training. The introduction of OT-based distance has improved the diversity , convergence , and stability  of generative models, such as WGAN  and its variants . However, several works showed that minimizing the Wasserstein distance still faces computational challenges, and the models tend to diverge without a strong regularization term, such as gradient penalty .

Recently, there has been a surge of research on directly modeling the optimal transport map between the input prior distribution and the real data distribution . In other words, the optimal transport serves as the generative model itself. These approaches showed promising results that are comparable to WGAN models. However, the classical optimal transport-based approaches are known to be highly sensitive to outlier-like samples . The existence of a few outliers can have a significant impact on the overall OT-based distance and the corresponding transport map. This sensitivity can be problematic when dealing with large-scale real-world datasets where outliers and noises are inevitable.

To overcome these challenges, we suggest a new generative algorithm utilizing the semi-dual formulation of the Unbalanced Optimal Transport (UOT) problem . In this regard, we refer to our model as the _UOT-based generative model (UOT)_. The UOT framework relaxes the hard constraintsof marginal distribution in the OT framework by introducing soft entropic penalties. This soft constraint provides additional robustness against outliers [8; 66]. Our experimental results demonstrate that UOTM exhibits such outlier robustness, as well as faster and more stable convergence compared to existing OT-based models. Particularly, this better convergence property leads to a tighter matching of data distribution than the OT-based framework despite the soft constraint of UOT. Our UOTM achieves FID scores of 2.97 on CIFAR-10 and 6.36 on CelebA-HQ, outperforming existing OT-based adversarial methods by a significant margin and approaching state-of-the-art performance. Furthermore, this decent performance is maintained across various objective designs. Our contributions can be summarized as follows:

* UOTM is the first generative model that utilizes the semi-dual form of UOT.
* We analyze the theoretical upper-bound of divergence between marginal distributions in UOT, and validate our findings through empirical experiments.
* We demonstrate that UOTM presents outlier robustness and fast and stable convergence.
* To the best of our knowledge, UOTM is the first OT-based generative model that achieves near state-of-the-art performance on real-world image datasets.

## 2 Background

NotationsLet \(\), \(\) be two compact complete metric spaces, \(\) and \(\) be probability distributions on \(\) and \(\), respectively. We regard \(\) and \(\) as the source and target distributions. In generative modeling tasks, \(\) and \(\) correspond to _tractable noise_ and _data distributions_. For a measurable map \(T\), \(T_{\#}\) represents the pushforward distribution of \(\). \((,)\) denote the set of joint probability distributions on \(\) whose marginals are \(\) and \(\), respectively. \(_{+}()\) denote the set of joint positive measures defined on \(\). For convenience, for \(_{+}()\), let \(_{0}(x)\) and \(_{1}(y)\) be the marginals with respect to \(\) and \(\). \(c(x,y)\) refers to the transport cost function defined on \(\). Throughout this paper, we consider \(=^{d}\) with the quadratic cost, \(c(x,y)=\|x-y\|_{2}^{2}\), where \(d\) indicates the dimension of data. Here, \(\) is a given positive constant. For the precise notations and assumptions, see Appendix A.

Optimal Transport (OT)OT addresses the problem of searching for the most cost-minimizing way to transport source distribution \(\) to target \(\), based on a given cost function \(c(,)\). In the beginning, Monge  formulated this problem with a deterministic transport map. However, this formulation is non-convex and can be ill-posed depending on the choices of \(\) and \(\). To alleviate these problems, the Kantorovich OT problem  was introduced, which is a convex relaxation of the Monge problem. Formally, the OT cost of the relaxed problem is given by as follows:

\[C(,):=_{(,)}[_{}c (x,y)d(x,y)], \]

where \(c\) is a cost function, and \(\) is a coupling of \(\) and \(\). Unlike Monge problem, the minimizer \(^{*}\) of Eq 1 always exists under some mild assumptions on \((,)\), \((,)\) and the cost \(c\) (, Chapter 5). Then, the _dual form_ of Eq 1 is given as:

\[C(,)=_{u(x)+v(y) c(x,y)}[_{}u(x)d(x)+ _{}v(y)d(y)], \]

Figure 1: **Generated Samples from UOTM trained on Left: CIFAR-10 and Right: CelebA-HQ.**

where \(u\) and \(v\) are Lebesgue integrable with respect to measure \(\) and \(\), i.e., \(u L^{1}()\) and \(v L^{1}()\). For a particular case where \(c(x,y)\) is equal to the distance function between \(x\) and \(y\), then \(u=-v\) and \(u\) is 1-Lipschitz . In such case, we call Eq 2 a _Kantorovich-Rubinstein duality_. For the general cost \(c(,)\), the Eq 2 can be reformulated as follows (, Chapter 5):

\[C(,)=_{v L^{1}()}[_{}v^{c}(x)d(x)+_ {}v(y)d(y)], \]

where the \(c\)-transform of \(v\) is defined as \(v^{c}(x)=_{y}(c(x,y)-v(y))\). We call this formulation (Eq 3) a _semi-dual formulation of OT_.

Unbalanced Optimal Transport (Uot)Recently, a new type of optimal transport problem has emerged, which is called _Unbalanced Optimal Transport (UOT)_[12; 44]. The _Csiszar divergence_\(D_{f}(|)\) associated with \(f\) is a generalization of \(f\)-divergence for the case where \(\) is not absolutely continuous with respect to \(\) (See Appendix A for the precise definition). Here, the entropy function \(f:[0,)[0,]\) is assumed to be convex, lower semi-continuous, and non-negative. Note that the \(f\)-divergence families include a wide variety of divergences, such as Kullback-Leibler (KL) divergence and \(^{2}\) divergence. (See  for more examples of \(f\)-divergence and its corresponding generator \(f\).) Formally, the UOT problem is formulated as follows:

\[C_{ub}(,):=_{_{+}()} [_{}c(x,y)d(x,y)+D_{_{1}}(_{0}| )+D_{_{2}}(_{1}|)]. \]

The UOT formulation (Eq 4) has two key properties. First, UOT can handle the transportation of any positive measures by relaxing the marginal constraint [12; 23; 44; 57], allowing for greater flexibility in the transport problem. Second, UOT can address the sensitivity to outliers, which is a major limitation of OT. In standard OT, the marginal constraints require that even outlier samples are transported to the target distribution. This makes the OT objective (Eq 1) to be significantly affected by a few outliers. This sensitivity of OT to outliers implies that the OT distance between two distributions can be dominated by these outliers [8; 66]. On the other hand, UOT can exclude outliers from the consideration by flexibly shifting the marginals. Both properties are crucial characteristics of UOT. However, in this work, we investigate a generative model that transports probability measures. Because the total mass of probability measure is equal to 1, we focus on the latter property.

## 3 Method

### Dual and Semi-dual Formulation of UOT

Similar to OT, UOT also provides a _dual formulation_[12; 21; 72]:

\[C_{ub}(,)=_{u(x)+v(y) c(x,y)}[_{}-_{1}^{ *}(-u(x))d(x)+_{}-_{2}^{*}(-v(y))d(y)], \]

with \(u()\), \(v()\) where \(\) denotes a set of continuous functions over its domain. Here, \(f^{*}\) denotes the _convex conjugate_ of \(f\), i.e., \(f^{*}(y)=_{x}\{ x,y-f(x)\}\) for \(f:[-,]\).

**Remark 3.1** (**Uot as a Generalization of OT)**.: Suppose \(_{1}\) and \(_{2}\) are the convex indicator function of \(\{1\}\), i.e. have zero values for \(x=1\) and otherwise \(\). Then, the objective in Eq 4 becomes infinity if \(_{0}\) or \(_{1}\), which means that UOT reduces into classical OT. Moreover, \(_{1}^{*}(x)=_{2}^{*}(x)=x\). If we replace \(_{1}^{*}\) and \(_{2}^{*}\) with the identity function, Eq 5 precisely recovers the dual form of OT (Eq 2). Note that the _hard constraint corresponds to \(^{*}(x)=x\)_._

Now, we introduce the _semi-dual formulation of UOT_. For this formulation, we assume that \(_{1}^{*}\) and \(_{2}^{*}\) are non-decreasing and differentiable functions (\(f^{*}\) is non-decreasing for the non-negative entropy function \(f\)):

\[C_{ub}(,)=_{v}[_{}-_{1}^{*} (-v^{c}(x)))\,d(x)+_{}-_{2}^{*}(-v(y))d(y )], \]

where \(v^{c}(x)\) denotes the \(c\)-transform of \(v\) as in Eq 3.

**Remark 3.2** (\(^{*}\) **Candidate)**.: Here, we clarify the feasible set of \(^{*}\) in the semi-dual form of UOT. As aforementioned, the assumption for deriving Eq 6 is that \(^{*}\) is a non-decreasing, differentiable function. Recall that for any function \(f\), its convex conjugate \(f^{*}\) is convex and lower semi-continuous . Also, for any convex and lower semi-continuous \(f\), \(f\) is a convex conjugate of \(f^{*}\). Combining these two results, _any non-decreasing, convex, and differentiable function can be a candidate of \(^{*}\)_.

### Generative Modeling with the Semi-dual Form of UOT

In this section, we describe how we implement a generative model based on the semi-dual form (Eq 6) of UOT. Following [18; 28; 41; 54; 61], we introduce \(T_{v}\) to approximate \(v^{c}\) as follows:

\[T_{v}(x)*{arginf}_{y}[c(x,y)-v(y)]  v^{c}(x)=c(x,T_{v}(x))-v(T_{v}(x) ), \]

Note that \(T_{v}\) is measurable (, Prop 7.33). Then, the following objective \(J(v)\) can be derived from the equation inside supremum in Eq (6) and the right-hand side of Eq 7:

\[J(v):=_{}_{1}^{*}(-[(c(x,T_{v}(x))-v(T _{v}(x)))]d(x)+_{}_{2}^{*}(-v(y) )d(y). \]

In practice, there is no closed-form expression of the optimal \(T_{v}\) for each \(v\). Hence, the optimization \(T_{v}\) for each \(v\) is required as in the generator-discriminator training of GAN . In this work, we parametrize \(v=v_{}\) and \(T_{v}=T_{}\) with neural networks with parameter \(\) and \(\). Then, our learning objective \(_{v_{},T_{}}\) can be formulated as follows:

\[_{v_{},T_{}}=_{v_{}}[_{}_ {1}^{*}(-_{T_{}}[c(x,T_{}(x))-v_{} (T_{}(x))])d(x)+_{}_{2}^{*} (-v_{}(y))d(y)]. \]

Finally, based on Eq 9, we propose a new training algorithm (Algorithm 1), called the _UOT-based generative model (UOTM)_. Similar to the training procedure of GANs, we alternately update the potential \(v_{}\) (lines 2-4) and generator \(T_{}\) (lines 5-7).

```
1:The source distribution \(\) and the target distribution \(\). Non-decreasing, differentiable, convex function pair \((_{1}^{*},_{2}^{*})\). Generator network \(T_{}\) and the discriminator network \(v_{}\). Total iteration number \(K\).
2:for\(k=0,1,2,,K\)do
3: Sample a batch \(X,Y\), \(z(,)\).
4:\(_{v}=_{x X}_{i}^{*}(-c(x,T_{ }(x,z))+v_{}(T_{}(x,z)))+_ {y Y}_{2}^{*}(-v_{}(y))\)
5: Update \(\) by using the loss \(_{v}\).
6: Sample a batch \(X,z(,)\).
7:\(_{T}=_{x X}(c(x,T_{}(x,z)) -v_{}(T_{}(x,z)))\).
8: Update \(\) by using the loss \(_{T}\).
9:endfor
```

**Algorithm 1** Training algorithm of UOTM

Following [26; 42], we add stochasticity in our generator \(T_{}\) by putting an auxiliary variable \(z\) as an additional input. This allows us to obtain the stochastic transport plan \((|x)\) for given input \(x\), motivated by the Kantorovich relaxation . The role of the auxiliary variable has been extensively discussed in the literature [13; 26; 42; 80], and it has been shown to be useful in generative modeling. Moreover, we incorporate \(R_{1}\) regularization , \(_{reg}=\|_{x}v_{}(y)\|_{2}^{2}\) for real data \(y\), into the objective (Eq 9), which is a popular regularization method employed in various studies [13; 51; 80].

### Some Properties of UOTM

Divergence Upper-Bound of MarginalsUOT relaxes the hard constraint of marginal distributions in OT into the soft constraint with the Csiszar divergence regularizer. Therefore, a natural question is _how much divergence is incurred in the marginal distributions_ because of this soft constraint in UOT. The following theorem proves that the upper-bound of divergences between the marginals in UOT is linearly proportional to \(\) in the cost function \(c(x,y)\). This result follows our intuition because down-scaling \(\) is equivalent to the relative up-scaling of divergences \(D_{_{1}},D_{_{2}}\) in Eq 4. (Notably, in our experiments, the optimization benefit outweighed the effect of soft constraint (Sec 5.2).)

**Theorem 3.3**.: _Suppose that \(\) and \(\) are probability densities defined on \(\) and \(\). Given the assumptions in Appendix A, suppose that \(,\) are absolutely continuous with respect to Lebesgue measure and \(^{*}\) is continuously differentiable. Assuming that the optimal potential \(v^{*}=_{v}J(v)\) exists, \(v^{*}\) is a solution of the following objective_

\[(v)=_{}-v^{c}(x)d(x)+_{}-v(y )d(y), \]

_where \((x)=_{1}^{*}(-v^{*c}(x))(x)\) and \((y)=_{2}^{*}(-v^{*}(y))(y)\). Note that the assumptions guarantee the existence of optimal transport map \(T^{}\) between \(\) and \(\). Furthermore, \(T^{}\) satisfies_

\[T^{}(x)*{arginf}_{y}[c(x,y)-v^{*}(y) ], \]

\(\)_-almost surely. In particular, \(D_{_{1}}(|)+D_{_{2}}(|) _{2}^{2}(,)\) where \(_{2}(,)\) is a Wasserstein-2 distance between \(\) and \(\)._

Theorem 3.3 shows that \(T_{v^{*}}\). (Eq 7) is a valid parametrization of the optimal transport map \(T^{}\) that transports \(\) to \(\). Moreover, if \(\) is sufficiently small, then \(\) and \(\) are close to \(\) and \(\), respectively. Therefore, we can infer that \(T_{v^{*}}\) will transport \(\) to a distribution that is similar to \(\).

Stable ConvergenceWe discuss convergence properties of UOT objective \(J\) and the corresponding OT objective \(\) for the potential \(v\) through the theoretical findings of Gallouet et al. .

**Theorem 3.4** ().: _Under some mild assumptions in Appendix A, the following holds:_

\[J(v)-J(v^{*})_{}[\|( v^{c}-v^{*c})\|_{2}^{2}]+C_{1}_{}[(v^{c}-v^{*c})^{2} ]+C_{2}_{}[(v-v^{*})^{2}], \]

_for some positive constant \(C_{1}\) and \(C_{2}\). Furthermore, \((v)-(v^{})_{}[\|(v^{c}-v^{*c})\|_{2}^{2}]\)._

Theorem 3.4 suggests that the UOT objective \(J\) gains stability over the OT objective \(\) in two aspects. First, while \(\) only bounds the gradient error \(\|(v^{c}-v^{*c})\|_{2}^{2}\), \(J\) also bounds the function error \((v^{c}-v^{*c})^{2}\). Second, \(J\) provides control over both the original function \(v\) and its \(c\)-transform \(v^{c}\) in \(L^{2}\) sense. We hypothesize this stable convergence property conveys practical benefits in neural network optimization. In particular, UOTM attains better distribution matching despite its soft constraint (Sec 5.2) and faster convergence during training (Sec 5.3).

## 4 Related Work

Optimal TransportOptimal Transport (OT) problem addresses a transport map between two distributions that minimizes a specified cost function. This OT map has been extensively utilized in various applications, such as generative modeling , point cloud approximation , and domain adaptation . The significant interest in OT literature has resulted in the development of diverse algorithms based on different formulations of OT problem, e.g., primary (Eq 1), dual (Eq 2), and semi-dual forms (Eq 3). First, several works were proposed based on the primary form . These approaches typically involved multiple adversarial regularizers, resulting in a complex and challenging training process. Hence, these methods often exhibited sensitivity to hyperparameters. Second, the relationship between the OT map and the gradient of dual potential led to various dual form based methods. In specific, when the cost function is quadratic, the OT map can be represented as the gradient of the dual potential . This correspondence motivated a new methodology that parameterized the dual potentials to recover the OT map . Seguy et al.  introduced the entropic regularization to obtain the optimal dual potentials \(u\) and \(v\), and extracted the OT map from them via the barycentric projection. Some methods  explicitly utilized the convexity of potential by employing input-convex neural networks .

Recently, Korotin et al.  demonstrated that the semi-dual approaches  are the ones that best approximate the OT map among existing methods. These approaches  properly recovered OT maps and provided high performance in image generation and image translation tasks for large-scale datasets. Because our UOTM is also based on the semi-dual form, these methods show some connections to our work. If we let \(_{1}^{*}\) and \(_{2}^{*}\) in Eq 9 as identity functions, our Algorithm 1 reduces to the training procedure of Fan et al.  (See Remark 3.2). For convenience, we denote Fan et al.  as a _OT-based generative model (OTM)_. Moreover, note that Rout et al.  can be considered as a minor modification of parametrization from OTM. In this paper, we denote Rout et al.  as _Optimal Transport Modeling (OTM*)_, and regard it as one of the main counterparts for comparison.

Unbalanced Optimal TransportThe primal problem of UOT (Eq 4) relaxes hard marginal constraints of Kantorovich's problem (Eq 1) through soft entropic penalties . Most of the recent UOT approaches [11; 19; 48; 58] estimate UOT potentials on discrete space by using the dual formulation of the problem. For example, Pham et al.  extended the Sinkhorn method to UOT, and Lubeck et al.  learned dual potentials through cyclic properties. To generalize UOT into the continuous case, Yang and Uhler  suggested the GAN framework for the primal formulation of unbalanced Monge OT. This method employed three neural networks, one for a transport map, another for a discriminator, and the third for a scaling factor. Balaji et al.  claimed that implementing a GAN-like procedure using the dual form of UOT is hard to optimize and unstable. Instead, they proposed an alternative dual form of UOT, which resembles the dual form of OT with a Lagrangian regularizer. Nevertheless, Balaji et al.  still requires three different neural networks as in . To the best of our knowledge, our work is the first generative model that leverages the semi-dual formulation of UOT. This formulation allows us to develop a simple but novel adversarial training procedure that does not require the challenging optimization of three neural networks.

## 5 Experiments

In this section, we evaluate our model on the various datasets to answer the following questions:

1. Does UOTM offer more robustness to outliers than OT-based model (OTM)? (SS5.1)
2. Does the UOT map accurately match the source distribution to the target distribution? (SS5.2)
3. Does UOTM provide stable and fast convergence? (SS5.3)
4. Does UOTM provide decent performance across various choices of \(_{1}^{*}\) and \(_{2}^{*}\)? (SS5.4)
5. Does UOTM show reasonable performance even without a regularization term \(_{reg}\)? (SS5.4)
6. Does UOTM provide decent performance across various choices of \(\) in \(c(x,y)\)? (SS5.4)

Unless otherwise stated, we set \(:=_{1}=_{2}\) and \(D_{}\) as a _KL divergence_ in our UOTM model (Eq 9). The source distribution \(\) is a standard Gaussian distribution \((,)\) with the same dimension as the target distribution \(\). In this case, the entropy function \(\) is given as follows:

\[(x)=x x-x+1,&x>0\\ ,&x 0,^{*}(x)=e^{x}-1. \]

In addition, when the generator \(T_{}\) and potential \(v_{}\) are parameterized by the same neural networks as OTM* , we call them a _small_ model. When we use the same network architecture in RGM

Figure 3: **Outlier Robustness Test on Image dataset** (CIFAR-10 + 1% MNIST). **Left**: OTM exhibits artifacts on both in-distribution and outlier samples. **Right**: UOTM attains higher-fidelity samples while generating MNIST-like samples more sparingly, around 0.2%. FID scores of CIFAR-10-like samples are 13.82 for OTM and 4.56 for UOTM, proving that UOTM is more robust to outliers.

Figure 2: **Outlier Robustness Test on Toy dataset** with 1% outlier. For each subfigure, **Left**: Comparison of target density \(\) and generated density \(T_{\#}\) and **Right**: Transport map of the trained model \((x,T(x))\). While attempting to fit the outlier distribution, OTM generates undesired samples outside \(\) and learns the non-optimal transport map. In contrast, UOTM mainly generates in-distribution samples and achieves the optimal transport map. (For better visualization, the y-scale of the density plot is manually adjusted.)

, we refer to them as _large_ model. Unless otherwise stated, we consider the _large_ model. For implementation details of experiments, please refer to Appendix B.2.

### Outlier Robustness of UOTM

One of the main features of UOT is its robustness against outliers. In this subsection, we investigate how this robustness is reflected in generative modeling tasks by comparing our UOTM and OT-based model (OTM). For evaluation, we generated two datasets that have 1-2% outliers: **Toy and Image datasets**. The toy dataset is a mixture of samples from \((1,0.5^{2})\) (in-distribution) and \((-1,0.5^{2})\) (outlier). Similarly, the image dataset is a mixture of CIFAR-10  (in-distribution) and MNIST  (outlier) as in Balaji et al. .

Figure 2 illustrates the learned transport map \(T\) and the probability density \(T_{\#}\) of generated distribution for OTM and UOTM models trained on the toy dataset. The density in Fig 2a demonstrates that OTM attempts to fit both the in-distribution and outlier samples simultaneously. However, this attempt causes undesirable behavior of transporting density outside the target support. In other words, in order to address 1-2% of outlier samples, _OTM generates additional failure samples that do not belong to the in-distribution or outlier distribution._ On the other hand, UOTM focuses on matching the in-distribution samples (Fig 2b). Moreover, the transport maps show that OTM faces challenging optimization. The right-hand side of Figure 2a and 2b visualize the correspondence between the source domain samples \(x\) and the corresponding target domain samples \(T(x)\). **Note that the optimal \(T^{}\) should be a monotone-increasing function.** However, OTM failed to learn such a transport map \(T\), while UOTM succeeded.

Figure 3 shows the generated samples from OTM and UOTM models on the image dataset. A similar phenomenon is observed in the image data. Some of the generative images from OTM show some unintentional artifacts. Also, MNIST-like generated images display a red or blue-tinted background. In contrast, UOTM primarily generates in-distribution samples. In practice, UOTM generates MNIST-like data at a very low probability of 0.2%. Notably, UOTM does not exhibit artifacts as in OTM. Furthermore, for quantitative comparison, we measured Frechet Inception Distance (FID) score  by collecting CIFAR-10-like samples. Then, we compared this score with the model trained on a clean

   Model & Toy (\(=0.1\)) & Toy (\(=0.02\)) & CIFAR-10 \\ Metric & (T_{\#}|)\) (\(\))} & FID (\(\)) \\  OTM\({}^{}\) & 0.05 & 0.05 & 7.68 \\ Fixed-\(^{}\) & **0.02** & **0.004** & 7.53 \\
**UOTM\({}^{}\)** & **0.02** & 0.005 & **2.97** \\   

Table 1: **Target Distribution Matching Test.** UOTM achieves a better approximation of target distribution \(\), i.e., \(T_{\#}\). \(\) indicates the results conducted by ourselves.

   Class & Model & FID (\(\)) & IS (\(\)) \\   & SNGAN+DGflow  & 9.62 & 9.35 \\  & AutoAAN  & 12.4 & 8.60 \\  & TransGAN  & 9.26 & 9.02 \\  & StyleGAN2 w/o ADA  & 8.32 & 9.18 \\  & StyleGAN2 w/o ADA  & 2.92 & **9.83** \\  & DDGAN (T=1) & 16.68 & - \\  & DDGAN  & 3.75 & 9.63 \\  & RGM  & **2.47** & 9.68 \\   & NCSN  & 25.3 & 8.87 \\  & DDFM  & 3.21 & 9.46 \\  & Score SDE (VE)  & 2.20 & 9.89 \\  & Score SDE (VP)  & 2.41 & 9.68 \\  & DDIM (50 steps) & 4.67 & 8.78 \\  & CLD  & 2.25 & - \\  & Subspace Diffusion  & 2.17 & **9.94** \\  & LSGM  & **2.10** & 9.87 \\   & NVAE  & 23.5 & 7.18 \\  & Glow  & 48.9 & 3.92 \\   & PixelCNN  & 65.9 & 4.60 \\   & VAEBM  & 12.2 & **8.43** \\   & Recovery EDM  & **9.58** & 8.30 \\   & WGAN  & 55.20 & - \\  & WGAN-G(P27) & 39.40 & 6.49 \\   & Robust-OT  & 21.57 & - \\   & AE-OT-GAN  & 17.10 & 7.35 \\   & OTM\({}^{}\) (Small)  & 21.78 & - \\   & OTM (Large)\({}^{}\) & 7.68 & 8.50 \\   & **UOTM** (Small)\({}^{}\) & 12.86 & 7.21 \\   & **UOTM** (Large)\({}^{}\) & **2.97\(\)**0.07** & **9.68** \\   

Table 2: **Image Generation on CIFAR-10.**dataset. The presence of outliers affected OTM by increasing FID score over 6 (\(7.68 13.82\)) while the increase was less than 2 in UOTM (\(2.97 4.56\)). In summary, the experiments demonstrate that UOTM is more robust to outliers.

### UOTM as Generative Model

Target Distribution MatchingAs aforementioned in Sec 3.3, UOT allows some flexibility to the marginal constraints of OT. This means that the optimal generator \(T_{}^{*}\) does not necessarily transport the source distribution \(\) precisely to the target distribution \(\), i.e., \(T_{\#}\). However, the goal of the generative modeling task is to learn the target distribution \(\). In this regard, _we assessed whether our UOTM could accurately match the target (data) distribution._ Specifically, we measured KL divergence  between the generated distribution \(T_{\#}\) and data distribution \(\) on the toy dataset (See the Appendix B.1 for toy dataset details). Also, we employed the FID score for CIFAR-10 dataset, because FID assesses the Wasserstein distance between the generated samples and training data in the feature space of the Inception network .

In this experiment, our UOTM model is compared with two other methods: _OTM_ (Constraints in both marginals) and _UOTM with fixed \(\)_(Constraints only in the source distribution). We introduced _Fixed_-\(\) variant because generative models usually sample directly from the input noise distribution \(\). This direct sampling implies the hard constraint on the source distribution. Note that this hard constraint corresponds to setting \(_{1}^{*}(x)=x\) in Eq 9 (Remark 3.1). (See Appendix B.2 for implementation details of UOTM with fixed \(\).)

Table 1 shows the data distribution matching results. Interestingly, despite the soft constraint, **our UOTM matches data distribution better than OTM in both datasets.** In the toy dataset, UOTM achieves similar KL divergence with and without fixed \(\) for each \(\), which is much smaller than OTM. This result can be interpreted through the theoretical properties in Sec 3.3. Following Thm 3.3, both UOTM models exhibit smaller \(D_{KL}(T_{\#}|)\) for the smaller \(\) in the toy dataset. It is worth noting that, unlike UOTM, \(D_{KL}(T_{\#}|)\) does not change for OTM. This is because, in the standard OT problem, the optimal transport map \(^{*}\) does not depend on \(\) in Eq 1. Moreover, in CIFAR-10, while Fixed-\(\) shows a similar FID score to OTM, UOTM significantly outperforms both models. As discussed in Thm 3.4, we interpret that relaxing both marginals improved the stability of the potential network \(v\), which led to better convergence of the model on the more complex data. This convergence benefit can be observed in the learned transport map in Fig 4. Even on the toy dataset, the UCOM transport map presents a better-organized correspondence between source and target samples than OTM and Fixed-\(\). In summary, our model is competitive in matching the target distribution for generative modeling tasks.

Image GenerationWe evaluated our UOTM model on the two generative model benchmarks: CIFAR-10  (\(32 32\)) and CelebA-HQ  (\(256 256\)). For quantitative comparison, we adopted FID  and Inception Score (IS) . The qualitative performance of UOTM is depicted in Fig 1. As shown in Table 2, UOTM model demonstrates state-of-the-art results among existing OT-based methods, with an FID of 2.97 and IS of 9.68. Our UOTM outperforms the second-best performing OT-based model OTM(Large), which achieves an FID of 7.68, by a large margin. Moreover, UOTM(Small) model surpasses another UOT-based model with the same backbone network (Robust-OT), which achieves an FID of 21.57. Furthermore, our model achieves the state-of-the-art FID score of 6.36 on CelebA-HQ (256\(\)256). To the best of our knowledge, UOTM is the first OT-based generative model that has shown comparable results with state-of-the-art models in various datasets.

Figure 4: **Visualization of OT Map \((,())\) trained on clean Toy data. The comparison suggests that Fixed-\(\) and UOTM models are closer to the optimal transport map compared to OTM.**

### Fast Convergence

The discussion in Thm 3.4 suggests that UOTM offers some optimization benefits over OTM, such as faster and more stable convergence. In addition to the transport map visualization in the toy dataset (Fig 2 and 4), we investigate the faster convergence of UOTM on the image dataset. In CIFAR-10, UOTM converges in 600 epochs, whereas OTM takes about 1000 epochs (Fig 5). To achieve the same performance as OTM, UOTM needs only 200 epochs of training, which is about five times faster. In addition, compared to several approaches that train CIFAR-10 with NCSN++ (_large model_) , our model has the advantage in training speed; Score SDE  takes more than 70 hours for training CIFAR-10, 48 hours for DDGAN , and 35-40 hours for RGM on four Tesla V100 GPUs. OTM takes approximately 30-35 hours to converge, while our model only takes about 25 hours.

### Ablation Studies

Generalized \(_{1}\), \(_{2}\)We investigate the various choices of Csiszar divergence in UOT problem, i.e., \(_{1}^{*}\) and \(_{2}^{*}\) in Eq 9. As discussed in Remark 3.2, the necessary condition of feasible \(^{*}\) is non-decreasing, convex, and differentiable functions. For instance, setting the Csiszar divergence \(D_{}\) as \(f\)-divergence families such as KL divergence, \(^{2}\) divergence, and Jensen-Shannon divergence satisfies the aforementioned conditions. For practical optimization, we chose \(^{*}(x)\) that gives finite values for all \(x\), such as KL divergence (Eq 13) and \(^{2}\) divergence (Eq 14):

\[(x)=(x-1)^{2},&x 0\\ ,&x<0,^{*}(x)=x^{2}+x,&x-2\\ -1,&x<-2. \]

We assessed the performance of our UOTM models for the cases where \(D_{}\) is KL divergence or \(^{2}\) divergence. Additionally, we tested our model when \(^{*}=\) (Eq 37), which is a direct parametrization of \(^{*}\). Table 4 shows that our UOTM model achieves competitive performance across all five combinations of \((_{1}^{*},_{2}^{*})\).

\(\) in Regularizer \(_{reg}\)We conduct an ablation study to investigate the effect of the regularization term \(_{reg}\) on our model's performance. The WGAN family is known to exhibit unstable training dynamics and to highly depend on the regularization term . Similarly, Figure 6 shows that OTM is also sensitive to the regularization term and fails to converge without it (OTM shows an FID score of 152 without regularization. Hence, we excluded this result in Fig 6 for better visualization). In contrast, our model is robust to changes in the regularization hyperparameter and produces reasonable performance even without the regularization.

\(\) in the Cost FunctionWe performed an ablation study on \(\) in the cost function \(c(x,y)=\|x-y\|_{2}^{2}\). In our model, the hyperparameter \(\) controls the relative weight between the cost \(c(x,y)\) and the marginal matching \(D_{_{1}},D_{_{2}}\) in Eq 4. In Fig 7, our model maintains a decent performance of FID(\( 5\)) for \(=\{0.5,1,2\} 10^{-3}\). However, the FID score sharply degrades at \(=\{0.1,5\} 10^{-3}\). Thm 3.3 suggests that a smaller \(\) leads to a tighter distribution matching. However, this cannot explain the degradation at \(=0.1 10^{-3}\). We interpret this is because the cost function provides some regularization effect that prevents a mode collapse of the model (See the Appendix C.2 for a detailed discussion).

Conclusion

In this paper, we proposed a generative model based on the semi-dual form of Unbalanced Optimal Transport, called UOTM. Our experiments demonstrated that UOTM achieves better target distribution matching and faster convergence than the OT-based method. Moreover, UOTM outperformed existing OT-based generative model benchmarks, such as CIFAR-10 and CelebA-HQ-256. The potential negative societal impact of our work is that the Generative Model often learns the dependence in the semantics of data, including any existing biases. Hence, deploying a Generative Model in real-world applications requires careful monitoring to prevent the amplification of existing societal biases present in the data. It is important to carefully control the training data and modeling process of Generative Models to mitigate potential negative societal impacts.