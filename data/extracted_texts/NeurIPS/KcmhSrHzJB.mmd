# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

* Clear statistical semantics for a trained DNN model, beyond merely providing the conditional distribution over output variables given input variables. Instead, PGMs provide a joint distribution over all variables including the latent variables.
* Ability to import any PGM algorithm into DNNs, such as belief propagation or MCMC, for example to _reverse any_ DNN to use output variables as evidence and variables anywhere earlier as query variables.
* Improved calibration, i.e., improved predictions of probabilities at the output nodes by incorporation of PGM algorithms.

In this paper, we establish such a correspondence between DNNs of any structure and PGMs. Given an arbitrary DNN architecture, we first construct an infinite-width tree-structured PGM. We then demonstrate that during training, the DNN executes approximations of precise inference in the PGM during the forward propagation step. We prove our result exactly in the case of sigmoid activations. We indicate how it can be extended to ReLU activations by building on a prior result of Nair and Hinton (2010). Because the PGM in our result is a Markov network, the construction can extend even further, to all nonnegative activation functions provided that proper normalization is employed. We argue that modified variants of layer normalization and batch normalization could be viewed as approximations to proper MN normalization in this context, although formal analyses of such approximations are left for future work. Finally, in the context of sigmoid activations we empirically evaluate how the second and third benefits listed above follow from the result, as motivated and summarized now in the next paragraph.

A neural network of any architecture with only sigmoid activations satisfies the definition of a Bayesian network over binary variables--it is a directed acyclic graph with a conditional probability distribution at each node, conditional on the values of the node's parents--and thus is sometimes called a Bayesian belief network (BBN). Nevertheless, it can be shown that the standard gradient used in neural network training (whether using cross-entropy or other common error functions) is inconsistent with the BBN semantics, that is, with the probability distribution defined by this BBN. On the other hand, Gibbs sampling is a training method consistent with the BBN semantics, and hence effective for calibration and for reversing any neural network, but it is worth inefficient for training. Hamiltonian Monte Carlo (HMC) is more efficient than Gibbs and better fits BBN semantics than SGD. We demonstrate empirically that after training a network quickly using SGD, calibration can be improved by fine-tuning using HMC. The specific HMC algorithm employed here follows directly from the theoretical result, being designed to approximate Gibbs-sampling in the theoretical, infinite-width tree structured Markov network. The degree of approximation is controlled by the value of a single hyperparameter that is also defined based on the theoretical result.

The present paper stands apart from many other theoretical analyses of DNNs that view DNNs purely as _function approximators_ and prove theorems about the quality of function approximation. Here we instead show that DNNs may be viewed as statistical models, specifically PGMs. This work is also different from the field of _Bayesian neural networks_, where the goal is to seek and model a probability distribution over neural network parameters. In our work, the neural network itself defines a joint probability distribution over its variables (nodes). Our work therefore is synergistic with Bayesian neural networks but more closely related to older work on learning stochastic neural networks via expectation maximization (EM) (Amari, 1995) or approximate EM (Song et al., 2016).

Although the approach is different, our motivation is similar to that of Dutordoir et al. (2021) and Sun et al. (2020) in their work to link DNNs to deep Gaussian processes (GPs) (Damianou and Lawrence, 2013). By identifying the forward pass of a DNN with the mean of a deep GP layer, they aim to augment DNNs with advantages of GPs, notably the ability to quantify uncertainty over both output and latent nodes. What distinguishes our work from theirs is that we make the DNN-PGM approximation explicit and include _all_ sigmoid DNNs, not just unsupervised belief networks or other specific cases.

All code needed to reproduce our experimental results may be found at [https://github.com/engelhard-lab/DNN_TreePGM](https://github.com/engelhard-lab/DNN_TreePGM).

Background: Comparison to Bayesian Networks and Markov Networks

Syntactically a Bayesian network (BN) is a directed acyclic graph, like a neural network, whose nodes are random variables. Here we use capital letters to stand for random variables, and following Russell and Norvig (Russell and Norvig, 2020) and others, we take a statement written using such variables to be a claim for all specific settings of those variables. Semantically, a BN represents a full joint probability distribution over its variables as \(P()=_{i}P(V_{i}|pa(V_{i}))\), where \(pa(V_{i})\) denotes the parents of variable \(V_{i}\). If the conditional probability distributions (CPDs) \(P(V_{i}|pa(V_{i}))\) are all logistic regression models, we refer to the network as a sigmoid BN.

It is well known that given sigmoid activation and a cross-entropy error, training a single neuron by gradient descent is identical to training a logistic regression model. Hence, a neural network under such conditions can be viewed as a "stacked logistic regression model", and also as a Bayesian network with logistic regression CPDs at the nodes. Technically, the sigmoid BN has a distribution over the input variables (variables without parents), whereas the neural network does not, and all nodes are treated as random variables. These distributions are easily added, and distributions of the input variables can be viewed as represented by the joint sample over them in our training set.

A Markov network (MN) syntactically is an undirected graph with potentials \(_{i}\) on its cliques, where each potential gives the relative probabilities of the various settings for its variables (the variables in the clique). Semantically, it defines the full joint distribution on the variables as \(P()=_{i}_{i}()\) where the partition function \(Z\) is defined as \(_{}_{i}_{i}()\). It is common to use a loglinear form of the same MN, which can be obtained by treating a setting of the variables in a clique as a binary feature \(f_{i}\), and the natural log of the corresponding entity for that setting in the potential for that clique as a weight \(w_{i}\) on that feature; the equivalent definition of the full joint is then \(P()=e^{_{i}w_{i}f_{i}()}\). For training and prediction at this point the original graph itself is superfluous.

The potentials of an MN may be on subsets of cliques; in that case we simply multiply all potentials on subsets of a clique to derive the potential on the clique itself. If the MN can be expressed entirely as potentials on edges or individual nodes, we call it a "pairwise" MN. An MN whose variables are all binary is a binary MN.

A DNN of any architecture is, like a Bayesian network, a directed acyclic graph. A sigmoid activation can be understood as a logistic model, thus giving a conditional probability distribution for a binary variable given its parents. Thus, there is a natural interpretation of a DNN with sigmoid activations as a Bayesian network (e.g., Bayesian belief network). Note, however, that when the DNN has multiple, stacked hidden nodes, the values calculated for those nodes in the DNN by its forward pass do not match the values of the corresponding hidden nodes in a Bayesian network. Instead, for the remainder of this paper, we adopt the view that the DNN's forward pass might serve as an approximation to an underlying PGM and explore how said approximation can be precisely characterized. As reviewed in Appendix A, this Bayes net in turn is equivalent to (represents the same probability distribution) as a Markov network where every edge of weight \(w\) from variable \(A\) to variable \(B\) has a potential of the following form:

   & \(B\) & \( B\) \\  \(A\) & \(e^{w}\) & 1 \\  \( A\) & 1 & 1 \\  

For space reasons, we assume the reader is already familiar with the Variable Elimination (VE) algorithm for computing the probability distribution over any query variable(s) given evidence (known values) at other variables in the network. This algorithm is identical for Bayes nets and Markov nets. It repeatedly multiplies together all the potentials (in a Bayes net, conditional probability distributions) involving the variable to be eliminated, and then sums that variable out of the resulting table, until only the query variable(s) remain. Normalization of the resulting table yields the final answer. VE is an exact inference algorithm, meaning its answers are exactly correct.

The Construction of Tree-structured PGMs

Although both a binary pairwise Markov network (MN) and a Bayesian network (BN) share the same sigmoid functional structure as a DNN with sigmoid activations, it can be shown that the DNN does not in general define the same probability for the output variables given the input variables: forward propagation in the DNN is very fast but yields a different result than VE in the MN or BN, which can be much slower because the inference task is NP-complete. Therefore, if we take the distribution \(\) defined by the BN or MN to be the correct meaning of the DNN, the DNN must be using an approximation \(^{}\) to \(\). Procedurally, the approximation can be shown to be exactly the following: the DNN repeatedly treats the _expectation_ of a variable \(V\), given the values of \(V\)'s parents, as if it were the actual _value_ of \(V\). Thus previously binary variables in the Bayesian network view and binary features in the Markov network view become continuous. This procedural characterization of the approximation of \(^{}\) to \(\) yields exactly the forward pass in the neural network within the space of a similarly structured PGM, yet, on its own, does not yield a precise joint distribution for said PGM. We instead prefer in the PGM literature to characterize approximate distributions such as \(^{}\) with an alternative PGM that precisely corresponds to \(^{}\); for example, in some variational methods we may remove edges from a PGM to obtain a simpler PGM in which inference is more efficient. Treewidth-1 (tree-structured or forest-structured) PGMs are among the most desirable because in those models, exact inference by VE or other algorithms becomes efficient. We seek to so characterize the DNN approximation here.

This approach aligns somewhat with the idea of the computation tree that has been used to explore the properties of belief propagation by expressing the relevant message passing operations in the form of a tree (Tatikonda and Jordan, 2002; Ihler et al., 2005; Weitz, 2006). Naturally the design of the tree structured PGM proposed here differs from the computation trees for belief propagation as we instead aim to capture the behavior of the forward pass of the neural network. Nonetheless, both methods share similar general approaches, the construction of a simpler approximate PGM, and aims, to better understand the theoretical behavior of an approximation to a separate original PGM.

To begin, we consider the Bayesian network view of the DNN. Our first step in this construction is to copy the shared parents in the network into separate nodes whose values are not tied. The algorithm for this step is as follows:

1. Consider the observed nodes in the Bayesian network that correspond to the input of the neural network and their outgoing edges.
2. At each node, for each outgoing edge, create a copy of the current node that is only connected to one of the original node's children with that edge. Since these nodes are observed at this step, these copies do all share the same values. The weights on these edges remain the same.
3. Consider then the children of these nodes. Again, for each outgoing edge, make a copy of this node that is only connected to one child with that edge. In this step, for each copied node, we then also copy the entire subgraph formed by all ancestor nodes of the current node. Note that while weights across copies are tied, the values of the copies of any node are not tied. However, since we also copy the subtree of all input and intermediary hidden nodes relevant to a forward pass up to each copy, the probability of any of these copied nodes being true remains the same across copies (ignoring the influence of any information passed back from their children).
4. We repeat this process across each layer until we have separate trees for each output node in the original deep neural network graph.

This process ultimately creates a graph whose undirected structure is a tree or forest. In the directed structure, trees converge at the output nodes. The probability of any copy of a latent node given the observed input (and ignoring any information passed back through a node's descendant) is the same across all the copies, but when sampling, their values may not be.

The preceding step alone is still not sufficient to accurately express the deep neural network as a PGM. Recall that in the probabilistic graphical model view of the approximation made by the DNN's forward pass, the neural network effectively takes a local average, in place of its actual value, from the immediately previous nodes and passes that information only forward. The following additional step in the construction yields this same behavior. This next step of the construction creates \(L\) copies of every non-output node in the network (starting at the output and moving backward) while also copying the entire ancestor subtrees of each of these nodes, as was done in step 1. The weight of a copied edges is then set to its original value divided by \(L\). Note that this step results in a number of total copies that grows exponentially in the number of layers (i.e. \(L\) copies in the 2nd to last layer, \(L^{2}\) copies in the layer before, etc). Detailed algorithms for the two steps in the construction of the infinite tree-structured PGM are presented in Appendix B. As \(L\) approaches infinity, we show that both inference and the gradient in this PGM construction matches the forward pass and gradient in the neural network exactly.

This second step in the construction can be thought of intuitively by considering the behavior of sampling in the Bayesian network view. Since we make \(L\) copies of each node while also copying the subgraph of its ancestors, these copied nodes all share the same probabilities. As \(L\) grows large, even if we sampled every copied node only once, we would expect the average value across these \(L\) copies to match the probability of an individual copied node being true. Given that we set the new weights between these copies and their parents as the original weights divided by \(L\), the sum of products (new weights times parent values) yields the average parent value multiplied by the original weight. As \(L\) goes to infinity, we remove sampling bias and the result exactly matches the value of the sigmoid activation function of the neural network, where this expectation in the PGM view is passed repeatedly to the subsequent neurons. The formal proof of this result, based on variable elimination, is found in Appendix C. There, we show the following:

**Theorem 3.1** (Matching Probabilities).: _In the PGM construction, as \(L\), \(P(H=1|)(_{j=1}^{M}w_{j}g_{j}+_{i}^{N}_{ i}(p_{i}))\), for an arbitrary latent node \(H\) in the DNN that has observed parents \(g_{1},...,g_{M}\) and latent parents \(h_{1},...,h_{N}\) that are true with probabilities \((p_{1}),...,(p_{N})\). Here, \(()\) is the logistic sigmoid function and \(w_{1},...,w_{M}\) and \(_{1},...,_{N}\) are the weights on edges between these nodes and \(H\)._

The PGM of our construction is a Markov network that always has evidence at the nodes \(\) corresponding to the input nodes of the neural network. As such, it is more specifically a conditional random field (CRF). Theorem 1 states the probability that a given node anywhere in the CRF is true given \(\) equals the output of that same node in the neural network given input \(\). The CRF may also have evidence at the nodes \(\) that correspond to the output nodes of the neural network. Given the form of its potentials, as illustrated at the end of Section 2, the features in the CRF's loglinear form correspond exactly to the edges and are true if and only if the nodes on each end of the edge are true. It follows that the gradient of this CRF can be written as a vector with one entry for each feature \(f\) corresponding to each weight \(w\) of the neural network, of the form \(P(f|)-P(f|,)\). Building on Theorem 1, this gradient of the CRF can be shown to be identical to the gradient of the cross-entropy loss in the neural network: the partial derivative of the cross-entropy loss with respect

Figure 1: The first step of the PGM construction where shared latent parents are separated into copies along with the subtree of their ancestors. Copies of nodes H1 and H2 are made in this example.

to the weight \(w\) on an edge, or feature \(f\) of the CRF, is \(P(f|)-P(f|,)\). This result is more precisely stated below in Theorem 2 below, which is proven in Appendix D.

**Theorem 3.2** (Matching Gradients).: _In the PGM construction, as \(L\), the derivative of the marginal log-likelihood, where all hidden nodes have been summed out, with respect to a given weight exactly matches the derivative of the cross entropy loss in the neural network with respect to the equivalent weight in its structure._

## 4 Implications and Extensions

We are not claiming that one should actually carry out the PGM construction used in the preceding section, since that PGM is infinite. Rather, its contribution is to give precise semantics to an entire neural network, as a joint probability distribution over all its variables, not merely as a machine computing the probability of its output variables given the input variables. Any Markov network, including any CRF, precisely defines a joint probability distribution over all its variables, hidden or observed, in a standard, well-known fashion. The particular CRF we constructed is the _right_ one in the very specific sense that it agrees with the neural network exactly in the gradients both use for training (Theorem 2). While the CRF is infinite, it is built using the original neural network as a template in a straightforward fashion and is tree-structured, and hence it is easy to understand. Beyond these contributions to pedagogy and comprehensibility, are there other applications of the theoretical results?

One application is an ability to use standard PGM algorithms such as Markov chain Monte Carlo (MCMC) to sample latent variables given observed values of input and output variables, such as for producing confidence intervals or understanding relationships among variables. One could already do so using Gibbs sampling in the BN or MN directly represented by the DNN itself (which we will call the "direct PGM"), but then one wouldn't be using the BN or MN with respect to which SGD training in the DNN is correct. For that, our result has shown that one instead needs to use Gibbs sampling in the infinite tree-structured PGM, which is impractical. Nevertheless, for any variable \(V\) in the original DNN, on each iteration a Gibbs sampler takes infinitely many samples of \(V\) given infinitely many samples of each of the members of \(V\)'s Markov blanket in the original DNN. By treating the variables of the original DNN as continuous, with their values approximating their sampled probabilities in the Gibbs sampler, we can instead apply Hamiltonian Monte Carlo or other MCMC methods for continuous variables in the much smaller DNN structure. We explore this approach empirically rather than theoretically in the next section. Another, related application of our result is that one could further fine-tune the trained DNN using other PGM algorithms, such as contrastive divergence. We also explore this use in the next section.

One might object that most results in this paper use sigmoid activation functions. Nair and Hinton showed that rectified linear units (ReLU) might be thought of as a combination of infinitely many sigmoid units with varying biases (Nair and Hinton, 2010). Hence our result in the previous section can be extended to ReLU activations by the same argument. More generally, with any non-negative activation function that can yield values greater than one, while our BN argument no longer holds, the MN version of the argument can be extended. An MN already requires normalization to represent a probability distribution. While Batch Normalization and Layer Normalization typically are motivated procedurally, to keep nodes from "saturating," and consequently to keep gradients from "exploding" or "vanishing," as the names suggest, they might also be used to bring variables into the range \(\) and hence to being considered as probabilities. Consider an idealized variant of these that begins by normalizing all the values coming from a node \(h\) of a neural network, over a given minibatch, to sum to \(1.0\); the argument can be extended to a set of \(h\) and all its siblings in a layer (or other portion of the network structure) assumed to share their properties. It is easily shown that if the parents of any node \(h\) in the neural network provide to \(h\) approximate probabilities that those parent variables are true in the distribution defined by the Markov network given the inputs, then \(h\) in turn provides to its children an approximate probability that \(h\) is true in the distribution defined by the Markov network given the inputs. Use of a modified Batch or Layer Normalization still would be only approximate and hence adds an additional source of approximation to the result of the preceding section. Detailed consideration of other activation functions is left for further work; in the next section we return to the sigmoid case.

Application of the Theory: A New Hamiltonian Monte Carlo Algorithm

To illustrate the potential utility of the infinite tree-structured PGM view of a DNN, in this section we pursue one of its implications in greater depth; other implications for further study are summarized in the Conclusion. We have already noted we can view forward propagation in an all-sigmoid DNN as exact inference in a tree-structured PGM, such that the CPD of each hidden variable is a logistic regression. In other words, each hidden node is a Bernoulli random variable, with parameter \(\) being a sigmoid activation (i.e. logistic function) applied to a linear function of the parent nodes. This view suggests alternative learning or fine-tuning algorithms such as contrastive divergence (CD) (Carreira-Perpinan and Hinton, 2005; Bengio and Delalleau, 2009; Sutskever and Tieleman, 2010). CD in a CRF uses MCMC inference with many MCMC chains to estimate the joint probability over the hidden variables given the evidence (the input and output variables in a standard DNN), and then takes a gradient step based on the results of this inference. But to increase speed, CD-\(n\) advances each MCMC chain only \(n\) steps before the next gradient step, with CD-1 often being employed. CD has a natural advantage over SGD, which samples the hidden variable values using only evidence in input values; instead, MCMC in CD uses _all_ the available evidence, both at input and output variables. Unfortunately, if the MCMC algorithm employed is Gibbs sampling on the many hidden variables found in a typical neural network, then it suffers from high cost in computational resources. MCMC has now advanced far beyond Gibbs sampling with methods such as Hamiltonian Monte Carlo (HMC), but HMC samples values in \(\) rather than \(\{0,1\}\). Neal (2012) first applied HMC to neural nets to sample the weights in a Bayesian approach, but still used Gibbs sampling on the hidden variables. Our theoretical results for the first time justify the use of HMC over the hidden variables rather than Gibbs sampling in a DNN, as follows.

Recall that the DNN is itself a BN with sigmoid CPDs, but if we take the values of the hidden variables to be binary then DNN training is not correct with respect to this BN. Instead, based on the correctness of our infinite tree-structured PGM, the probabilistic behavior of one hidden node in the BN is the result of sampling values across its \(L\) copies in the PGM. Within any copy, the value of the hidden node follows the Bernoulli distribution with the same probability distribution as the other copies, determined by the parent nodes. Since all the copies share the same parent nodes by the construction and are sampled independently, the sample average follows a normal distribution as the asymptotic distribution when \(L\) by the central limit theorem. In practice, \(L\) is finite and this normal distribution is a reasonable approximation to the distribution of the hidden node. Thus in the BN, whose variables correspond exactly to those of the DNN, the variables have domain  rather than \(\{0,1\}\), as desired. We next precisely define this BN and the resulting HMC algorithm.

### Learning via Contrastive Divergence with Hamiltonian Monte Carlo Sampling

Consider a Bayesian network composed of input variables \(=_{0}\), a sequence of layers of hidden variables \(_{1},...,_{K}\), and output variables \(\). Each pair of consecutive layers forms a bipartite subgraph of the network as a whole, and the variables \(_{i}=(h_{i1},...,h_{iM_{i}})\) follow a multivariate normal distribution with parameters \(_{i}=(p_{i1},...,p_{iM_{i}})\) that depend on variables in the previous layer \(_{i-1}\) as follows:

\[h_{ij}(p_{ij},p_{ij}(1-p_{ij})/L)}= (_{i-1}_{i-1}+_{i-1}), \]

where \(:(0,1)\) is a non-linearity - here the logistic function - that is applied element-wise, and \(_{i}=(_{i},_{i})\) are parameters to be learned. The distribution in equation (1) is motivated by supposing that \(h_{ij}\) is the average of \(L\) copies of the corresponding node in the PGM, each of which is 1 with probability \(p_{ij}\) and zero otherwise, then applying the normal approximation to the binomial distribution. Importantly, this approximation is valid only for large \(L\).

For a complete setting of the variables \(\{,,\}\), where \(=\{_{1},...,_{K}\}\), and parameters \(=\{_{i}\}_{i=0}^{K}\), the likelihood \(p(,|;)\) may be decomposed as:

\[p(,|;)=p(|_{K};_{K}) _{i=1}^{K}_{j=1}^{M_{i}}p_{}(h_{ij}|p_{ij}(_{i-1}; _{i-1})), \]

where \(p_{}(|)\) denotes the normal density, and a specific form for \(p(|_{K};_{K})\) has been omitted to allow variability in the output variables. In our experiments, \(\) is a Bernoulli(binary) or categorical random variable parameterized via the logistic(sigmoid) or softmax function, respectively.

[MISSING_PAGE_FAIL:8]

probabilistic distribution \(P(y|)\) of the corresponding BN/MN is calculated by sampling or applying the VE algorithm on it.

To explore how the proposed algorithm performs in model calibration, a DNN is first trained with SGD for 100 or 1000 epochs, and then fine-tuned by Gibbs or HMC with different \(L\)'s for 20 epochs based on the trained DNN model. Here \(L\) defines the normal distribution for hidden nodes in Eqn. 1 and is explored across the set of values: \(\{10,100,1000\}\). The calibration is assessed by mean absolute error (MAE) in all the synthetic experiments and compared between non-extra fine-tuning (shown in the "DNN" column in Table 1) and fine-tuning with Gibbs or HMC. Since the ground truth of \(P(y|)\) in the synthetic dataset can be achieved from the BN/MN, the MAE is calculated by comparing the predicted \(P(y|)\) from the finetuned network and the true probability.

Table 1 shows that in general, DNN results tend to get worse with additional training, particularly with smaller weights, and the HMC-based fine-tuning approaches can mitigate this negative impact of additional training on the model calibration. Across all the HMC with different \(L\)'s, HMC (\(L\)=10) performs better than the others and DNN training itself for BNs and MNs with smaller weights. Additionally, the MAE of HMC (\(L\)=10) tends to be similar to Gibbs but runs much faster, especially in the BN simulations, whereas HMC (\(L\)=1000) is more similar to the NN. This is consistent with what we have argued in the theory that when \(L\) goes smaller, the number of the sampled copies for each hidden node decreases in our tree-PGM construction and HMC sampling performs more similar to Gibbs sampling; and as \(L\) increases, the probability of each hidden node given the input approaches the result of the DNN forward propagation and thus HMC performs more similar to DNN training.

#### 5.2.2 Covertype Experiments

Similar experiments are also run on the Covertype dataset to compare the calibration of SGD in DNNs, Gibbs and the HMC-based algorithm. Since the ground truth for the distribution of \(P(y|)\) cannot be found, the metric for the calibration used in this experiment is the expected calibration error (ECE), which is a common metric for model calibration. To simplify the classification task, we choose the data with label 1 and 2 and build two binary subsets, each of which contains 1000 data points. Similarly, the number of training epochs is also 100 or 1000, while the fine-tuning epochs shown in Table 2 is 20.

Table 2 shows that HMC with \(L=10\) fine-tuning generally performs better than DNN results, and HMC with \(L=1000\) has the similar ECE as that in DNN. It meets the conclusion made in the synthetic experiments. Gibbs sampling, however, could perform worse than just using DNN. It could be because Gibbs may be too far removed from the DNN, whereas our proposed HMC is more in the middle. This suggests perhaps future work testing the gradual shift from DNN to HMC to Gibbs.

  &  & \)) (p-value)} \\   & & & DNN & Gibbs & HMC-10 & HMC-100 & HMC-1000 \\   & 100 & 6.593 & 16.09 (1.0000) & **5.300 (c0.0001)** & 6.864 (0.9982) & 6.658 (1.0000) \\  & 1000 & 34.44 & 36.09 (0.9916) & 23.53 (c0.0001) & 34.96 (1.0000) & 34.55 (1.0000) \\   & 100 & 22.90 & **20.84 (0.0001)** & **22.48 (0.0001)** & 24.71 (1.0000) & 22.95 (0.9928) \\  & 1000 & 42.59 & **33.64 (0.0001)** & **33.07 (0.0001)** & 43.03 (1.0000) & 42.63 (0.9995) \\   & 100 & 72.76 & 76.12 (1.0000) & 76.54 (1.0000) & **72.62 (c0.0001)** & 72.63 (c0.0001) \\  & 1000 & 28.28 & 32.59 (1.0000) & 32.98 (1.0000) & 28.84 (1.0000) & 28.40 (1.0000) \\   & 100 & 186.10 & 192.8 (1.0000) & 196.1 (1.0000) & **184.6 (c0.0001)** & **184.8 (c0.0001)** \\  & 1000 & 54.89 & 79.69 (1.0000) & 72.64 (1.0000) & 54.81 (1.266) & **54.63 (c0.0001)** \\   & 100 & 6.031 & 14.03 (1.0000) & 4.515 (c0.001) & 6.382 (1.0000) & 6.070 (1.0000) \\  & 1000 & 38.11 & **34.83 (c0.0001)** & 26.54 (c0.0001) & 38.71 (1.0000) & 38.22 (1.0000) \\   & 100 & 9.671 & 17.81 (1.0000) & 8.887 (c0.0001) & **9.018 (c0.0001)** & 9.284 (c0.0001) \\  & 1000 & 27.80 & 23.44 (1.0000) & 19.92 (c0.0001) & 27.98 (0.9994) & 27.23 (c0.0001) \\   & 100 & 8.677 & 23.60 (1.0000) & **5.685 (c0.0001)** & 5.912 (c0.0001) & 5.964 (c0.0001) \\  & 1000 & 5.413 & 28.03 (1.0000) & 5.792 (0.9957) & 5.671 (1.0000) & 5.443 (1.0000) \\  

Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.

## 6 Conclusion, Limitations, and Future Work

In this work, we have established a new connection between DNNs and PGMs by constructing an infinite-width tree-structured PGM corresponding to any given DNN architecture, then showing that inference in this PGM corresponds exactly to forward propagation in the DNN given sigmoid activation functions. This theoretical result is valuable in its own right, as it provides new perspective that may help us understand and explain relationships between PGMs and DNNs. Moreover, we anticipate it will inspire new algorithms that merge strengths of PGMs and DNNs. We have explored one such algorithm, a novel HMC-based algorithm for DNN training or fine-tuning motivated by our PGM construction, and we illustrated how it can be used to improve to improve DNN calibration.

Limitations of the present work and directions for future work include establishing formal results about how closely batch- and layer-normalization can be modified to approximate Markov network normalization when using non-sigmoid activations, establishing theoretical results relating HMC in the neural network to Gibbs sampling in the large treewidth-1 Markov network, and obtaining empirical results for HMC with non-sigmoid activations. Also of great interest is comparing HMC and other PGM algorithms to Shapley values, Integrated Gradients, and other approaches for assessing the relationship of some latent variables to each other or to inputs and/or outputs in a neural network. We note that the large treewidth-1 PGM is a substantial approximation to the _direct_ PGM of a DNN - in other words, the PGM whose structure exactly matches that of the DNN. In future work, we will explore other DNN fine-tuning methods, perhaps based on loopy belief propagation or other approximate algorithms often used in PGMs, that may allow us to more closely approximate inference in this direct PGM.

Another direction for further work is in the original motivation for this work. Both DNNs and PGMs are often used to model different components of very large systems, such as the entire gene regulatory network in humans. For example, in the National Human Genome Research Institute (NHGRI) program Impact of Genetic Variation on Function (IGVF), different groups are building models of different parts of gene regulation, from genotypic variants or CRISPRi perturbations of the genome, to resulting changes in transcription factor binding or chromatin remodeling, to post-translational modifications, all the way to phenotypes characterized by changes in the expression of genes in other parts of the genome IGVF Consortium (2024). Some of these component models are DNNs and others are PGMs. As a community we know from years of experience with PGMs that passing the outputs of one model to the inputs of another model is typically less effective than concatenating them into a larger model and fine-tuning and using this resulting model. But this concatenation and fine-tuning and usage could not be done with a mixture of PGM and DNN components until now. Having an understanding of DNN components as PGMs enables their combination with PGM components, and then performing fine-tuning and inference in the larger models using algorithms such as the new HMC algorithm theoretically justified, developed, and then evaluated in this paper. Furthermore, the same HMC approach can be employed to reason just as easily from desired gene expression changes at the output nodes back to variants or perturbations at the input nodes that are predictive of the desired changes. Ordinarily, to reason in reverse in this way in a DNN would require special invertible architectures or training of DNNs that operate only in the other direction such as diffusion. Experiments evaluating all these uses of HMC (or other approximate algorithms in PGMs such as loopy belief propagation or other message passing methods) are left for future work.

   &  & \))} \\   & & DNN & Gibbs & HMC-10 & HMC-100 & HMC-1000 \\  Covertype (label 1) & 100 & 4.207 & 4.229 & 2.352 & 3.893 & 3.987 \\  & 1000 & 10.85 & 8.513 & 6.875 & 7.730 & 11.60 \\  Covertype (label 2) & 100 & 4.268 & 7.796 & 7.719 & 4.913 & 4.354 \\  & 1000 & 6.634 & 14.67 & 5.233 & 5.394 & 7.713 \\  

Table 2: Calibration performance on Covertype datasets. Highlighted cells show the best calibrations among each row.