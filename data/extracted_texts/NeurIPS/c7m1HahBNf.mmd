# Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation

Mingjia Li

Beijing Institute of Technology

mingjiali@bit.edu.cn

&Shuang Li\({}^{*}\)

Beihang University

shuangliai@buaa.edu.cn

&Tongrui Su

Beijing Institute of Technology

molarsu@bit.edu.cn

&Longhui Yuan

Beijing Institute of Technology

longhuiyuan@bit.edu.cn

&Jian Liang

Kuaishou Technology

liangjian03@kuaishou.com

&Wei Li\({}^{*}\)

Inceptio Technology

liweimcc@gmail.com

Corresponding author.

###### Abstract

Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA.

## 1 Introduction

The combination of generative and discriminative modeling has always been appealing due to their distinct nature in data comprehension [1; 2; 3; 4]. Discriminative models are adept at making accurate predictions on training data [5; 6; 7; 8; 9; 10; 11], but can be fragile when confronted with unseen data . This vulnerability can be attributed to their tendency to learn spurious correlation as a shortcut, hindering their transferability . Generative models, however, are proficient in capturing the underlying structure of the data, giving them an edge in grasping the whole picture [14; 15; 16] and enhancing robustness [17; 18]. Prior works have verified the effectiveness of generative objectives in discriminative learning [4; 19], yet the utilization of pre-trained generative models is under-explored.

The recent surge of diffusion models [20; 21; 22] has ignited interest in adopting them for applications beyond image generation [23; 24; 25; 26]. In the context of test-time adaptation, a pre-trained task model is updated on the fly to make accurate predictions on incoming target samples without access to their labels. This presents challenges, as the target data distribution may differ from that encountered during pre-training. The literature reveals that we can not only extract discriminative features from capacious diffusion models [23; 24; 26; 27; 28], but also convert these models into generative classifiers that demonstrate human-level generalization on out-of-distribution samples [25; 29; 30]. Such properties render them viable choices for facilitating the test-time adaptation of discriminative models, which may underperform on unseen data . Diffusion-TTA  ranks among the firstto employ diffusion models for test-time adaptation, where task outputs are used to modulate the conditioning of a diffusion model with the objective of likelihood maximization. While Diffusion-TTA is competitive, there's still room to unleash the full potential of diffusion models. To achieve this, two key aspects warrant further exploration. First, the conditioning space is typically low-dimensional in diffusion models [20; 22], which restricts its ability to capture the intricacies of complex data and thus impedes its expressiveness as a discriminative prior. Further, the common image-level condition lacks a fine-grained connection with data, limiting its potential in guiding dense prediction tasks. Conversely, the high-dimensional latent space of diffusion models exhibits a surprisingly interpretable semantic structure [33; 34; 35; 36; 37], making it a good fit for assisting discriminative tasks and easily extensible to dense prediction. Second, Diffusion-TTA is heavily reliant on the Monte Carlo method over as many as 180 timesteps to estimate a biased approximation of likelihood [32; 38], resulting in high computational complexity proportional to sampled timesteps.

With this work, we aim to boost test-time adaptation performance by digging into the semantic structure of diffusion models in the latent space, while lifting reliance on the Monte Carlo sampling of timesteps. Although a few works elucidate the semantic properties of the latent space [34; 37], they all take a generative viewpoint and are not tailored for discriminative tasks. We instead depart from the perspective of score functions [39; 40; 41] on the latent space, which is closely related to the denoising diffusion formulation [38; 42; 43]. Our method features exploring the structured semantic priors underlying **D**ift**U**sion models as **S**core estimators for test-time **A**daptation **(**DUSA**).

Concretely, we start by providing a theoretical illustration of the semantic structure underneath the score functions \(_{} p( y)\), where the conditional probability \(p(y)\) is implicitly embedded. The theoretical findings not only unveil discriminative priors hidden within score-based diffusion models, but applies to every single timestep and avoids likelihood estimation. A test-time objective is then derived by substituting the pre-trained task model and diffusion model for the implicit priors and score functions, respectively. Intuitively, the precise score estimation by diffusion models forms a well-structured semantic space, where the task model can learn implicit discriminative priors. Given their generative nature, the priors are further blessed with improved robustness, ultimately benefiting task prediction. Another key advantage of our approach lies in shifting computational complexity from timesteps to the number of classes, which aligns closely with our focus on discriminative tasks. Thereby, a more efficient adaptation scheme can be enabled through our practical designs.

Besides, the capacity of our DUSA is testified across a variety of task model families, test-time adaptation protocols, and task categories. Our DUSA consistently outperforms the competitive counterparts in adapting pre-trained classifiers with different backbones to out-of-distribution scenarios, whether in the mild protocol of data from a single domain  or the more challenging one with a continually changing datastream . We also showcase the versatility of our DUSA by applying it almost as-is to test-time semantic segmentation. All diffusion models employed are trained on the corresponding source domain of the task model. Extensive analyses of the components in our method back the validity of DUSA and underline the benefits of borrowing knowledge from generative modeling.

Our main contributions can be summarized as:

* A novel proposition is given from a theoretical perspective to extract discriminative priors from score-based diffusion models, which are single-timestep-based and versatile enough to handle both classification and dense prediction tasks at test time.
* Inspired by the proposition and enhanced by practical designs, our DUSA effectively leverages the structured semantic priors and rivals in test-time adaptation with improved efficiency.
* DUSA outperforms the best existing methods by \(+5.1\%\) and \(+7.3\%\) in fully and continual test-time adaptation on ConvNeXt-L and \(+4.2\%\) in test-time semantic segmentation on SegFormer-B5, validating the excellence of our method in extracting valuable priors from diffusion models.

## 2 Preliminaries

Test-time adaptation.A model well-trained on source data can face severe performance degradation on out-of-distribution (OOD) target samples. To tackle this, test-time adaptation (TTA)  is proposed to boost model performance at inference time. Formally, an off-the-shelf model \(f_{}()\) pre-trained on labeled source data \(_{S}=\{(_{i},y_{i})\}_{i=1}^{N}\) is adopted as the task model, where the source data follows a probability distribution \(_{i} P_{S}()\) and is inaccessible during adaptation. TTA aimsat pushing the limits of model performance on unlabeled target data \(_{}=\{_{j}\}_{j=1}^{M}\) on the fly, where the target data follows \(_{j} P_{}()\) and \(P_{}() P_{}()\). With batched target data arriving online, we obtain predictions from the task model \(f_{}()\) and update it on live target samples without labels.

Diffusion models.Diffusion models excel at modeling data distribution \(p()\) by learning to restore the gradually destroyed data structure [20; 38; 46]. For diffusion models, a forward and a reverse process is defined. In the forward process, small Gaussian noise is iteratively applied to real data \(_{0}\):

\[q(_{t}_{t-1})(_{t}; }_{t-1},_{t}),\] (1)

where \(\{_{t}(0,1)\}_{t=1}^{T}\) is a variance schedule defining the noise added at each timestep \(t\).

With the reparameterization trick, the noisy version of \(_{0}\) can be directly obtained in a single step:

\[_{t}=_{t}}_{0}+_{t }},\ \ (,),\] (2)

where \(_{t} 1-_{t}\), \(_{t}_{i=1}^{t}_{i}\), and the sampled noise \(\) is of the same dimensionality as \(_{0}\).

In the reverse process, a conditional diffusion model [20; 21; 22; 47; 48]\(_{}(_{t},t,)\) is trained to predict the noise added to \(_{t}\) with condition \(\), by minimizing the simplified denoising objective:

\[_{simple}()_{(_{0},), ,t}\|-_{ }(_{t},t,)\|_{2}^{2}.\] (3)

## 3 Structured Semantic Priors in Diffusion Score for Test-Time Adaptation

In this section, we first review a relevant method (Sec. 3.1), then provide the theoretical insight behind our DUSA (Sec. 3.2). At last, we advocate a few practical designs with efficiency in mind (Sec. 3.3). The framework of our DUSA is illustrated in Fig. 1. Given a pre-trained task model, a set of classes to optimize is selected by the Candidate Selection Module (CSM) based on task model prediction to improve adaptation efficiency. We focus on the selected classes and aggregate the conditional noise estimations with CSM-modulated probabilities on these classes, upon which our DUSA objective is constructed. The structured semantic priors of the diffusion model are then propagated to the task model through our objective. For more details please refer to Alg. 1 in Appendix E.

### A Brief Review of Diffusion-TTA

Diffusion-TTA  takes the first step in exploring conditional diffusion models for test-time adaptation. In Diffusion-TTA, the task model prediction \(p_{}(y_{0})\) is integrated with class embeddings

Figure 1: **Overview of DUSA**. Our method adapts a discriminative task model \(f_{}\) with a generative diffusion model \(_{}\). Given image \(_{0}\) at test-time, the task model outputs logits. To improve efficiency, we devise a CSM to select classes to adapt and return their probabilities (probs). The embeddings of the classes are then queried as diffusion model conditions, yielding conditional noise predictions from noisy image \(_{t}\). The aggregated noise \(}_{,}\) is then constructed from ensembling conditional noises with probs, which is aligned with the added noise \(\) following Eq. (10). Both models are updated.

\(\{_{y}\}_{y=1}^{N}\) to get a soft condition \(=_{y}p_{}(y_{0})_{y}\), where \(N\) is the number of classes. The sample-wise adaptation is performed with the diffusion loss: \((,)=_{t,}\| -_{}(_{t},t, )\|_{2}^{2}\). Inspired by , the loss is averaged over hundreds of \((t,)\) pairs to remedy the performance degradation caused by incorrect class prediction using a single timestep , at the cost of reduced efficiency.

### Unlocking the Discriminative Power of Conditional Diffusion Models

Unlike previous attempts that rely heavily on a massive number of timesteps to provide an appropriate estimation of likelihood \(p()\)[25; 29; 32], we shed light on the semantic structure underneath the denoising capability of conditional diffusion models from the perspective of score functions [39; 40; 41; 42], which will be shown to hold for every single timestep. Proofs can be found in Appendix C.

We first present our main theoretical contribution to reveal the semantic structure of score functions:

**Proposition 1**.: _Let \(p()\) and \(\{p( y):y\}\) be continuously differentiable probability densities, their score functions \(_{} p()\) and \(\{_{} p( y):y\}\), the following equation holds:_

\[_{} p()=_{y}p(y) _{} p( y).\] (4)

**Remark**.: _The equation holds under mild assumptions about the densities but applies to any data \(\) with an entire set of conditions \(\{y:y\}\). All score functions can be estimated by score matching or denoising diffusion. Note that the posteriors \(\{p(y):y\}\) are **not** directly modeled, and thus can be seen as the **implicit priors** hidden in the construction of (conditional) score functions._

To link our Proposition 1 with a trained diffusion model, we revisit Tweedie's Formula , which serves as a key connection between score functions and the formulation of diffusion models :

**Lemma 1** (Tweedie's Formula).: _Let \(_{}(; _{},_{})\), then the posterior expectation of \(_{}\) given \(\) can be estimated by:_

\[[_{}]=+ {}_{}_{} p().\] (5)

As Eq. (2) indicates \(q(_{t}_{0})=(_{t};_{t}}_{0},(1-_{t}))\), we have the following corollary:

**Corollary 1**.: _Let \((,)\) be a sampled noise in a forward process parameterized by \(\{_{t}\}_{t=1}^{T}\) to get noisy data \(_{t}\), the connection between score function \(_{_{t}} p(_{t})\) and noise \(\) can be given by:_

\[_{_{t}} p(_{t})=-}{ _{t}}}.\] (6)

Recall that our Proposition 1 makes no assumptions on the form of data \(\), thus it holds for the noisy data \(_{t}\) at **any single timestep**\(t\). Applying Eq. (6) to Eq. (4) at timestep \(t\), we get the following:

\[-}{_{t}}}=_{y}p(y _{t})_{_{t}} p(_{t} y).\] (7)

For diffusion models, the conditional score function \(_{_{t}} p(_{t} y)\) can be estimated as follows:

\[_{_{t}} p(_{t} y)-_{}(_{t},t,_{y})}{_{t}}},\] (8)

and therefore we can combine Eq. (7) and Eq. (8) to give a structured estimation in the latent space:

\[_{y}p(y_{t}) {}_{}(_{t},t,_{y}),\] (9)

where all score functions are now replaced by noise predictors. Note that \(p(y_{t})\) is the implicit prior of a conditional diffusion model at a single timestep \(t\), and thus can be learned by directly plugging the task model prediction on \(_{0}\), dubbed as \(p_{}(y_{0})\), into Eq. (9). The objective to minimize is:

\[_{}(,)=_{} \|-_{y}p_{}(y_{0}) _{}(_{t},t,_{y})\|_{2}^{2}.\] (10)

Intuitively, the optimization of this objective encourages the task model to extract knowledge from the semantic structure of a capacious diffusion model, promising better robustness for adaptation.

**Corollary 2**.: _The objective in Eq. (10) is extensive to \(_{0}\)-prediction or \(\)-prediction  in diffusion._

With a total of \(K\) classes and \(T\) timesteps and computational burden primarily borne by diffusion models, our DUSA shows a task-relevant time complexity of \((K)\), while that of Diffusion-TTA is the diffusion-relevant \((T)\). Enhanced by practical designs in Sec. 3.3, we empirically find that our DUSA establishes leading performance even with a small budget, leaving Diffusion-TTA behind.

Free lunch in modern diffusion models.The proposed objective in Eq. (10) requires the joint training of task model \(f_{}()\) and diffusion model \(_{}(_{t},t,_{y})\) over all conditions \(\{_{y}:y\}\) simutaneously. The training inefficiency largely stems from the excessive adaptation of the diffusion model, which may not always require knowledge from the task model.

Indeed, Eq. (10) can be interpreted from two distinct perspectives. From one viewpoint, the task model extracts knowledge from the implicit priors of the diffusion model. From another, a weighted optimization is applied to conditional noise estimations, allowing the diffusion model to adapt to the incoming test-time data based on task model predictions. Alternatively, the adaptation of diffusion models can be achieved by introducing unconditional noise estimations with null condition \(\):

\[_{}(_{t},t,).\] (11)

Combining Eq. (9) and Eq. (11) reveals another semantic structure within the diffusion model:

\[_{}(_{t},t,)_{y}p(y _{t})_{}(_{t},t,_{y}),\] (12)

where \(_{}(_{t},t,)\) and \(\{_{}(_{t},t,_{y}):y\}\) represent noise estimations from a specific diffusion model capable of handling both unconditional and conditional generation. Note that the implicit priors \(\{p(y_{t}):y\}\) in Eq. (12) serve as a critical link between the unconditional and conditional noise estimations. Therefore, an unconditional adaptation of the diffusion model implicitly facilitates its conditional adaptation to the test-time scenario, without reliance on the task model.

Modern conditional diffusion models [20; 22] maintain their unconditional generation capability by employing an additional null condition that replaces the original class conditions with a certain probability (typically \(10\%\)). Leveraging this feature, we can adjust the objective to enhance efficiency:

\[_{}()=_{} \|-_{y}p_{}(y_{0})_{}(_{t},t,_{y})\|_{2}^{2}, _{}()=_{}\|-_{}(_{t},t,)\|_{2}^{2},\] \[_{}(,)=_{ {cond}}()+_{}(),\] (13)

where the diffusion model is now unconditionally adapted. An immediate concern is whether such modifications would impact adaptation performance. We empirically find it can significantly boost training efficiency with minor to no performance degradation, please refer to Sec. 4.1 for more details.

Readily applicable to dense prediction tasks.It is worth noting that our method is not confined to classification tasks, but can be easily applied to a handful of dense prediction tasks as well. Taking semantic segmentation as an example, the task model \(p_{}(_{0})\) is now a dense labeler assigning per-pixel class labels to the input, where \(\) is the predicted segmentation map of shape \(H W K\), \(H W\) is the size of input image \(_{0}\), and \(K\) is the number of interested classes. Again, our proposition is nowhere strict on the form of data, and therefore should be readily applicable to every single pixel in \(_{0}\). The new objective to minimize is then easily obtained by utilizing Eq. (4) in a per-pixel fashion:

\[_{}(,)=_{,(h,w)} \|-_{k=1}^{K}p_{}( _{0})_{h,w,k}_{}(_{t},t,_ {k})_{h,w}\|_{2}^{2},\] (14)

where \((h,w)\) denotes the pixel location in an image sample of size \(H W\), and \(_{k}\) represents the class embedding of a class \(k\) in the segmentation task. We highlight that per-pixel noise can be efficiently acquired by extracting elements from the image-level noise estimation \(_{}(_{t},t,_{k})\), which takes the entire data sample \(_{t}\) and class-wise condition \(_{k}\) as inputs. As a vast majority of diffusion models are trained with image-level annotations, this design is advantageous as it allows the use of off-the-shelf diffusion models without modifying their training schemes. In contrast, Diffusion-TTA  requires the integration of per-pixel conditions into diffusion models to accommodate dense prediction tasks.

### Improving Adaptation Efficiency with Practical Designs

Identifying appropriate timestep.Since our DUSA intends to extract structured semantic priors from a single timestep, a critical question emerges: which timestep should we utilize to maximize adaptation performance? Iterating over all \(T\) timesteps for a certain task model on a specific task is just not practical, and thus a universal preference must be advocated. While the semantic structure uncovered in Eq. (4) is valid for all timesteps in theory, the estimation of score functions by denoising diffusion models [20; 22] can be unreliable. As pointed out by [51; 52], for diffusion models we have a scheduled \(_{t}\) decreasing with \(t\) and \(_{t} 1\) when \(t 0\), which directly amplifies error in score estimation by Eq. (9) at smaller timesteps. A large timestep is also not recommended, as denoising at higher noise levels is more challenging , posing a greater challenge to score estimation. Based on the preceding conclusions, we select timestep \(t=100\) and find it suits well for all our experiments.

Utilizing task model for candidate selection.Similar to Diffusion Classifiers [25; 29], our DUSA has a computational complexity that scales proportional to class number. To circumvent the slowdown by a large number of classes, we utilize task model prediction to significantly improve adaptation efficiency. With the observation that classifiers typically maintain a \(top\)-\(k\) accuracy  and \(top\)-\(1\) accuracy is our main concern in the test-time adaptation of discriminative models, we opt to apply our DUSA to the most promising classes. Specifically, we deem the posterior \(p(y)\) of less likely class candidates to be zero and only optimize the semantic structure among the selected classes.

A mere decrease in class number can lead to optimization issues, as the task model can be biased towards certain classes, especially with a small batch size. This can be blamed on the underutilization of the semantic structure, further exacerbated by the erratic task model prediction on the pruned classes due to a lack of constraints. To tackle this, we devise a Candidate Selection Module (CSM). In the module, we first adopt LogitNorm [54; 55] to force constraints on the pruned classes to stabilize training, where the logits output of the task model are \(_{2}\) normalized before selection. Intuitively, we discourage the optimization in the magnitude of logits to mitigate overconfidence, especially on pruned classes. Then we handle the class bias problem by introducing randomness in selection. In detail, with a selection budget \(b=k+m\), we split it into two parts: \(k\) for \(top\)-\(k\) classes select, and \(m\) for a multinomial selection without replacement from the remaining classes, where the sampling probabilities are calculated from the logits before normalization. We only focus on the selected \(b\) normalized logits, and apply softmax to get their probabilities for our DUSA objective. After a series of practical designs, we succeed in reducing the time complexity of DUSA from \((K)\) to \((b)\), where a small \(b\) should be valid for a large number of classes, as will be shown in Sec. 4.4.

## 4 Experiments

Datasets and models.Our experiments are conducted on three benchmarks: ImageNet-C  for fully and continual test-time classification, ADE20K  with corruptions defined in  (dubbed as ADE20K-C) for test-time semantic segmentation. All image corruptions are at the highest severity level 5. We use ResNet-50 , ViT-B/16 , ConvNeXt-L  pre-trained on ImageNet for ImageNet-C experiments, and SegFormer-B5  pre-trained on ADE20K for ADE20K-C ones. We follow  and use the GN variant of ResNet-50 for stability. More details are in Appendix F.1.

Compared methods.We compare our DUSA with Tent , CoTTA , EATA , SAR , RoTTA , and Diffusion-TTA  for image classification. For semantic segmentation, we compare with BN Adapt [62; 63], Tent  and CoTTA . More details are in Appendix F.2.

Evaluation metrics.\(Top\)-\(1\) accuracy (Acc) is reported on each corruption type for image classification. For semantic segmentation, the mean Intersection-over-Union (mIoU) is reported. The main results of our DUSA all come with mean and standard deviation statistics over 3 independent runs.

Implementation details.The batch size is \(64\) for test-time classification tasks unless otherwise stated. Following , we use Adam optimizer  with a learning rate of \(0.00001\) for our DUSA and Diffusion-TTA. For other baselines, SGD with momentum \(0.9\) or Adam optimizer is used in line with the literature [44; 59; 60]. As for test-time semantic segmentation, the batch size is \(1\) and Adam with a learning rate of \(0.00006/8\) is used, following . We use ImageNet  trained DiT  and ADE20K trained ControlNet  as diffusion models. More details are in Appendix F.3.

### Fully Test-Time Adaptation of ImageNet Pre-trained Classifiers

Table 1 shows our DUSA in comparison with relevant methods under the online setting of test-time adaptation to every single corruption domain in ImageNet-C, also known as fully TTA . Generally, our DUSA and Diffusion-TTA both achieve a substantial performance gain, thanks to the knowledge from a capacious generative diffusion model. Sepecifically, our DUSA yields a significant improvement of \(+21.9\%\), \(+23.3\%\), \(+15.7\%\) on ResNet-50, ViT-B/16, and ConvNeXt-L over pre-trained classifiers. Besides, DUSA consistently outperforms the multi-timestep enhanced

Diffusion-TTA by \(+2.0\%\), \(+4.5\%\), \(+5.1\%\) on these classifiers, justifying the exploration of semantic priors underneath the diffusion score estimations and demonstrating a clear superiority of our DUSA.

Furthermore, a thrilling finding is that our DUSA is not reliant on the integration of task model prediction in objective Eq. (10) to maintain the powerful semantic priors implicitly embedded. In Table 1, we provide the adaptation results from Eq. (13), namely DUSA-U. Despite the diffusion model being trained unconditionally in DUSA-U and thus having no chance to borrow knowledge from task models, the performance is still on par with DUSA. This reinforces our conviction that diffusion models inherently possess such semantic priors, even without explicit inclusion of task models. Although DUSA-U is more lightweight (diffusion model only trained on null condition), we stick to DUSA when benchmarking against Diffusion-TTA to ensure fairness in the training budget \(b\).

### Continual Test-Time Adaptation of ImageNet Pre-trained Classifiers

We also experiment under the online continual test-time adaptation protocol, where the task model should adapt to continually changing scenarios . The outcomes are reported in Table 2. Our DUSA withstands a long period of adaptation and outperforms Diffusion-TTA by a large margin of \(+7.3\%\) on ConvNeXt-L. During adaptation, Diffusion-TTA witnesses a performance drop when the OOD datastream type shifts from Weather (Brit.) to Digital (Contr.), while our DUSA shows remarkable adaptation stability. This suggests that DUSA effectively learns from robust and transferable semantic priors from score-based generative modeling, allowing it to shine over prolonged adaptation.

### Fully Test-Time Adaptation of ADE20K Pre-trained Segmentors

The versatility of our DUSA to dense prediction tasks is evaluated by fully test-time semantic segmentation on the ADE20K dataset with corruptions. We experiment on SegFormer-B5 and report

    &  &  &  &  \\  Method & Gauss. & Shot & Impal. & Defoc. & Glass & Motion & Zoom & Snow & Frost & Fog & Brit. & Contr. & Elastic & Pixel & JPEG & Avg. \\  ResNet-30 (GN) & 22.1 & 23.0 & 22.0 & 19.8 & 11.4 & 21.5 & 25.0 & 40.3 & 47.0 & 34.0 & 68.8 & 36.3 & 18.5 & 29.3 & 52.6 & 31.4 \\
7ent & 25.3 & 29.1 & 24.5 & 14.9 & 9.9 & 21.6 & 22.3 & 27.5 & 32.1 & 3.5 & 69.9 & 42.0 & 10.3 & 48.6 & 54.6 & 29.1 \\
8 CurTA & 22.1 & 23.0 & 22.0 & 19.8 & 11.4 & 21.5 & 25.1 & 40.3 & 47.0 & 34.0 & 68.8 & 36.4 & 18.5 & 29.3 & 52.6 & 31.5 \\
8 EATA & 38.6 & 40.9 & 39.7 & 27.3 & 26.7 & 36.5 & 38.6 & 50.8 & 49.1 & 55.6 & 72.0 & 49.9 & 48.5 & 55.7 & 58.2 & 45.3 \\
8 SAK & 39.6 & 42.4 & 41.0 & 19.8 & 22.9 & 37.1 & 38.7 & 27.3 & 47.4 & 55.1 & 72.4 & 48.3 & 77.2 & 54.9 & 57.4 & 40.8 \\
8-RTTA & 22.8 & 23.5 & 22.5 & 19.7 & 12.0 & 21.8 & 25.2 & 41.3 & 47.5 & 34.6 & 69.2 & 36.8 & 19.2 & 29.9 & 52.9 & 31.9 \\
8 Diffono-TTA & 42.0 & 44.6 & 42.4 & **38.3** & **39.5** & 46.9 & 48.2 & 56.5 & **56.3** & 60.0 & 72.6 & 45.6 & **57.9** & 61.4 & 58.0 & 51.3 \\ 
**DUSA (Ours)** & **45.2**,0.0 & 43.0 & 46.4 & 37.3 & **27.3** & **27.6** & **48.0** & **68.0** & **69.4** & **55.6** & **63.3** & **63.3** & **73.3** & **50.1** & **56.5** & **63.2** & **60.0** & **53.3** \\ 
**+DUSA (Ours)** & **45.0**,0.1 & 47.1 & 46.1 & 38.6 & 38.2 & 27.1 & 49.1 & 49.3 & 55.9 & 59.4 & 54.4 & 60.0 & 62.7 & 45.1 & 53.4 & 54.6 & 62.9 & 50.0 & 53.0 \\  VAT-B16 (LN) & 38.3 & 35.4 & 38.1 & 29.5 & 24.2 & 32.8 & 30.5 & 36.4 & 45.0 & 50.4 & 68.3 & 22.5 & 39.4 & 52.7 & 53.5 & 39.8 \\
7ent & 53.9 & 54.5 & 54.1 & 44.4 & 47.2 & 53.8 & 6.7 & 4.6 & 61.9 & 65.4 & 72.9 & 54.9 & 58.0 & 65.1 & 64.1 & 50.8 \\
8 CUTTA & 38.3 & 35.4 & 38.1 & 29.5 & 24.2 & 32.8 & 30.5 & 36.4 & 45.0 & 50.4 & 68.3 & 22.5 & 39.4 & 52.7 & 53.5 & 39.8 \\
8 EATA & 55.4 & 56.3 & 55.3 & 48.9 & 53.4 & 58.6 & 58.2 & 63.5 & 61.1 & 67.5 & 74.3 & 56.5 & 65.7 & 65.5 & 66.6 & 60.9 \\
8-SAR & 53.9 & 54.3 & 54.1 & 46.0 & 47.8 & 54.2 & 49.4 & 38.2 & 61.4 & 64.3 & 72.8 & 54.3 & 54.9 & 62.6 & 63.5 & 55.2 \\
8-RTTA & 42.6 & 39.9 & 42.9 & 30.6 & 26.4 & 34.8 & 31.7 & 39.2 & 47.8 & 52.4 & 68.8 & 23.3 & 42.0 & 55.0 & 40.4 & 24.1 \\
8-DRIA-TTA & 52.1 & 54.5 & 53.5 & 49.3 & 52.9 & 56.9 & 55.6 & 60.3 & 60.8 & 64.2 & 72.6 & 47.4 & 66.4 & 67.6 & 62.5 & 58.6 \\
**+DUSA (Ours)** & **56.6**,0.2 & **57.9** & **57.0** & **57.6** & **58.3** & **53.0** & **56.7** & **62.8** & **41.0** & **61.0** & **65.9** & **65.1** & **70.1** & **75.1** & **60.2** & **67.9** & **60.7** & **65.8** & **63.1** \\  DUSA-U (Ours) & **56.3**,0.1 & 56.1 & 56.0 & 57.4 & 52.1 & 56.4 & 69.1 & 60.4 & 60.4 & 65.4 & 65.4 & 64.2 & 70.0 & 53.3 & 55.6 & 62.4 & 66.4 & 64.1 & 62.6 \\  ConvNeXt-L (LN) & 57.6 & 56.2 & 58.3 & 35.1 & 20.7 & 47.6 & 43.5 & 59.8 &the results in Table 3. Notably, previous methods fail on most tasks, except for CoTTA which achieves modest improvements on a few tasks through a combination of stochastic weight restoration and data augmentation. For all tasks involved, our DUSA takes the lead by exploiting the semantic priors from the high-dimensional latent space of a pre-trained **text-to-image** diffusion model, justifying the extensiveness of our proposition to a wider range of discriminative tasks beyond classification. Segmentation results for all the methods involved are visualized in Fig. 2. Our DUSA, as shown in the illustration, showcases its ability to address errors by incorporating semantic priors from diffusion models, overcoming a limitation faced by other methods that depend solely on the task model's precision. We provide more visualizations over a wide range of scenarios in Appendix I.

### Ablation Study

Selection of timestep \(t\).As discussed in Sec. 3.2, our DUSA significantly reduces the number of timesteps to a single timestep of diffusion models. Fig. 4 illustrates the influence of timestep selection on DUSA through adapting the ConvNeXt-L classifier to corruptions from the four main categories. Consistent with our analysis in Sec. 3.3, the guidance from diffusion models is far from perfect when the chosen timestep is either too small (\(t 0\)) or too large (\(t T\)). We empirically find that \(t=100\) shows a good performance here, and generalizes well to other backbones and tasks as well. The other timesteps, e.g., \(t=50\), however also emerge as strong contenders and outperform Diffusion-TTA by a considerable margin. For simplicity, we adopt \(t=100\) in all our experiments.

Effect of components in DUSA.To grasp a deeper understanding of DUSA, we provide a detailed ablation of critical designs in Table 4, where the number in parentheses means the budget \(b=k+m\) allowed for diffusion model forward (D.F.) in Eq. (10), i.e., number of classes to adapt for each sample. The results are obtained on ResNet-50 and ConvNeXt-L over the corruptions within the Noise category. We also include Pixelate corruption results on ConvNeXt-L to offer a well-rounded understanding, upon which the transferability of diffusion models to OOD data is demonstrated.

Before adaptation, the source-only models serve as the baseline. Introducing the objective in Eq. (10) when freezing the diffusion model brings about a performance gain of \(+3.6\%\) on ResNet-50 for Noise, but causes a degradation of \(-13\%\) on ConvNeXt-L. This is largely due to the instability from discarding classes, as pointed out in Sec. 3.3. Applying LogitNorm instantly mitigates the issue and brings about a consistent gain of \(+21.6\%\) and \(+3.8\%\) against baselines. The improvement is made without training diffusion models and therefore can be viewed as exploiting the generative semantic priors formed in diffusion pre-training. However, the outcomes are still below the baseline for Pixelate. We conjecture that such corruption might be OOD even for a diffusion model with strong robustness. The further adaptation of diffusion models removes this concern, pushing all results to a competitive level. We attribute this finding to the fast convergence of generative modeling  on unseen data, which is favorable to the online nature of test-time adaptation. Again, the inclusion of LogitNorm yields a significantly boosted accuracy at \(46.4\%,65.0\%\) and \(70.4\%\).

To provide a basis for the further ablation of budget schemes, the budget is raised from 4 to 6 with a slight increase in performance. A mild drop in accuracy is witnessed when handing the class bias problem in Sec. 3.3 with a budget \(m=2\) used with uniform sampling, which is then improved by our multinomial selection in the penultimate line. We underline that such a design is indispensable for a small batch size, which is also practical . For better consistency, \(k=4,m=2\) are universally adopted for DUSA in classification, the ratio between them to be delved into below. Interestingly, with access to a diffusion model capable of unconditional generation, DUSA-U could achieve performance comparable to DUSA using Eq. (13), while significantly reducing the computational cost associated with diffusion model backward (D.B.). We believe this observation can back our claim of the existence of structured semantic priors inherently embedded in diffusion models.

Specifying a budget scheme.As a justification for our design of the selection strategies in CSM, we take a thorough investigation into the effects of different budget schemes over varied classifiers (ResNet-50 & ViT-B/16) and batch size (\(4\) & \(64\)) in Fig. 4. At a smaller batch size (bs) of \(4\) (dashed lines), the budget scheme \(k:m\) plays a vital role in performance. Concretely, a large \(k\) is favorable to weaker task models (ResNets) for eager adaptation, while a proper \(m\) is a must to prevent more powerful ones (ViTs) from overfitting to a subset of classes. When the batch size is increased to the standard \(64\) (solid lines), our DUSA becomes insensitive to budget schemes, and a consistent gain is observed for both classifiers. DUSA with \(b=4\) even exceeds Diffusion-TTA with \(b=6\), underscoring the advanced efficiency made possible by our proposition and practical designs. For a budget scheme, we find \(m=2\) competitive across varied \(b\), and stick to \(k=4,m=2\) for DUSA.

## 5 Related Work

Test-time adaptation.Test-time adaptation  focuses on improving source data pre-trained model performance on out-of-distribution target data without label access during inference time. Early works lay more emphasis on adapting the activation statistics of batch normalization (BN) [62; 63; 66; 67]. Test-time training [68; 69; 70; 71] methods manage to adapt through devising a test-time self-supervised objective which is also injected into the pre-training stage, resulting in complicated pipelines and increased computational cost. To lessen dependence on source data and extra loss

   & & & & &  & ConvNeXt-L \\  Variants & \(k\) & \(m\) & D.F. & D.B. & Noise & Noise & Pixel \\  Source-only & 0 & 0 & 0 & 0 & 0 & 22.4 & 57.1 & 42.3 \\ + score priors inspired loss (4) & 4 & 0 & 4 & 0 & 26.0 & 44.1 & 9.2 \\  +LogitNorm (4) & 4 & 0 & 4 & 0 & 44.0 & 60.9 & 9.6 \\ + adapt diffusion (4) & 4 & 0 & 4 & 4 & 41.2 & 57.8 & 49.3 \\ + LogitNorm (4) & 4 & 0 & 4 & 4 & 46.4 & 65.0 & 70.4 \\ + LogitNorm (6) & 6 & 0 & 6 & 6 & 46.5 & 65.1 & 70.7 \\ + uniform select (6) & 4 & 2 & 6 & 6 & 46.2 & 65.1 & 70.7 \\ + multinomial select (6) (DUSA) & 4 & 2 & 6 & 6 & 46.3 & 65.1 & 70.8 \\ + null conditioning (6) (DUSA-U) & 4 & 2 & 7 & 1 & 46.1 & 64.7 & 70.5 \\  

Table 4: Ablation on critical components in DUSA. Components in colored rows are not carried over to subsequent rows.

injection, fully test-time adaptation  is advocated to achieve adaptation with only unlabeled target data. Concretely, previous works are majorly based on entropy minimization objectives: Tent  directly minimizes entropy on batched data predictions, MEMO  proposes marginal entropy minimization via data augmentation, EATA  pursues a sample efficient entropy minimization and anti-forgetting regularization, while SAR  advocates sharpness-aware and reliable entropy minimization. Other works delve into extensive distributional shifts in TTA , e.g., continual adaptation without forgetting , correlative data streams  and label shifts . Our DUSA is much different as it is not reliant on error-prone entropy-based objectives and rather extracts knowledge from semantic priors of generative models for better adaptation of the task model.

Generative models for discriminative tasks.The long-standing discussion on the connections between generative and discriminative models  has inspired a handful of attempts to integrate the two seemingly disparate paradigms . Specifically, a collection of works showcase the impressive power of generative pre-training followed by supervised fine-tuning . Besides, a few works utilize generative models as zero-shot recognizers . Integrating generative modeling into the task of test-time adaptation is gaining traction. Prior works manage to boost task model performance with a variety of generative techniques, including GANs , MAEs , energy-based  and flow-based . The recent prevalence of diffusion models with extraordinary generation capability stimulates a range of works on adapting them for discriminative tasks. As for TTA, two distinct research directions arise. Appreciating the generative power  of diffusion models, a series of works propose to adapt samples in the input space . Another largely under-explored direction is to repurpose the generative objective of diffusion models as a proxy of discriminative ability enhancement, with Diffusion-TTA  pioneering in incorporating task predictions into the class condition of denoising objective in an inversion  style. Our DUSA belongs to the latter direction but is fundamentally different from , in that we delve deeper into the semantic priors of diffusion models from the perspective of score functions and achieve better adaptability and versatility.

Timestep selection in diffusion models.The importance of timestep selection in diffusion models has been widely recognized in the literature. In image editing tasks, diffusion models are observed to exhibit a natural coarse-to-fine pattern during the reverse process . Consequently, the trade-off between realism and fidelity in editing largely stems from the chosen intervals of timesteps. In discriminative tasks, timestep selection is also crucial to the quality of extracted features. An early study  elucidated the features within diffusion models, revealing that the most informative ones are derived from smaller timesteps. Subsequent works have reinforced this finding by utilizing a single small timestep across various tasks, including semantic segmentation, referring image segmentation, depth estimation , object detection , semantic correspondence , and one-shot image segmentation . While our DUSA aligns with the existing literature on using a single timestep, it differs by extracting semantic priors rather than focusing on feature extraction.

## 6 Conclusion

In this paper, we introduce DUSA, a competitive test-time adaptation method built on the structured semantic priors underlying diffusion models, which serve as score estimators. A proposition is offered to unveil the semantic structure in these score-based models, upon which a test-time objective is derived to fully exploit the implicit semantic priors. Our approach is also shown to generalize well to modern diffusion models and dense prediction tasks. Additionally, we enhance the adaptation efficiency through a few practical designs. The effectiveness of our DUSA is demonstrated across three challenging benchmarks, where it consistently outperforms competing methods. We hope our method will pave the way for better utilization of generative modeling for discriminative tasks.