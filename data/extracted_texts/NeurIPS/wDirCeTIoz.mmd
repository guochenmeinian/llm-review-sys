# Distributed Lion for Communication Efficient Distributed Training

Bo Liu

The University of Texas at Austin

bliu@cs.utexas.edu

&Lemeng Wu1

Meta AI

lmwu@google.com

&Lizhang Chen1

The University of Texas at Austin

lzchen@utexas.edu

&Kaizhao Liang

The University of Texas at Austin

kaizhaol@utexas.edu

&Jiaxu Zhu

Meta AI

jiaxuzhu@meta.com

&Chen Liang

Google

crazydonkey@google.com

&Raghuraman Krishnamoorthi

Meta AI

raghuraman@meta.com

&Qiang Liu

The University of Texas at Austin

lqiang@cs.utexas.edu

Equal contribution.

###### Abstract

The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that Distributed Lion presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.

## 1 Introduction

The pursuit of modern artificial intelligence hinges on the training of large-scale models like large language models and large vision models (LVM). As the stakes - in terms of time, cost, and environmental impact - grow ever higher for training expansive AI systems, the hunt for efficient optimizers becomes critical.

Recently, a new optimization named Lion (evolved sign momentum)  has been discovered with an evolutionary program. It was shown that it exhibits performance on par with the current state-of-the-art AdamW  across a wide range of tasks, while reducing the memory cost and training time.

Consider optimizing a loss function \(f_{}(x)\) on \(^{d}\) with a dataset \(\), the update rule of Lion is:

\[m_{t+1}=_{2}m_{t}+(1-_{2}) f_{}(x_{t}),\] (1) \[_{t}=(x_{t},)}{{=}}(_{1}m_{t}+(1-_{1}) f_{ }(x_{t})),\] \[x_{t+1}=x_{t}-_{t}+ x_{t},\]

where \(m_{t}\) plays the role of the momentum, \(\) is the learning rate, \(_{1},_{2}^{2}\) are two momentum related coefficients, and \( 0\) is the weight decay coefficient. Comparing Lion against AdamW, one observes that Lion only requires the storage of the first-order momentum term, which results in a more relaxed memory requirement.

In this study, we tailor the Lion optimizer for distributed training. The Lion optimizer is particularly suitable for this context due to two main attributes: (1) its simple update mechanism that relies solely on first-order momentum, and (2) its use of the \(()\) function. We showcase the effective employment of the \(()\) function to streamline communication processes, leading to the development of a novel distributed training framework named Distributed Lion. Within the Distributed Lion framework, each participating worker independently adjusts the model parameters using a distinct instance of the Lion optimizer, thereby maintaining separate optimizer states. A distinctive feature of this framework is the mode of communication between workers and the central server, which is restricted to binary or low-precision vectors.

Crucially, in this setup, workers convey updates rather than raw gradients to the central server. The server, in turn, aggregates these updates through either a straightforward averaging process (Distributed Lion-Avg) or a majority voting mechanism (Distributed Lion-MaVo). In the case of Distributed Lion-MaVo, the consolidated update is maintained as a binary vector, whereas for Distributed Lion-Avg, given the presence of \(n\) workers, each element of the update vector is encoded using \((n)\) bits. This approach markedly reduces the bandwidth requirements compared to traditional distributed training methods, which typically rely on high-precision floating-point vectors for communication. The bandwidth efficiencies achieved by our method are detailed in Table 1. Our contributions are: **1)** We introduce the Distributed Lion algorithm, a simple yet effective approach to extend Lion to distributed training, where all communications between workers and the server are done through binary or low-precision vectors (Section 2); **2)** We provide theoretical analysis to ensure the convergence of Distributed Lion (Section 3); **3)** Empirically, we demonstrate that on both vision and language modeling tasks, Distributed Lion achieves comparable performance against applying Lion and Adam with the synchronized gradients from all workers, while being significantly more communication efficient. In addition, we show that Distributed Lion achieves a better trade-off than existing efficient distributed training methods like deep gradient compression  and ternary gradients  (Section 5).

Figure 1: Illustration of Distributed-Lion. Each worker keeps its _own_ optimizer state and applies the Lion optimizer individually to a binary update \(_{i,t}=(x,_{i})\) (without the weight decay), then the server aggregates all \(_{i,t}\) to produce a binary \(_{t}\) by majority vote (or an integer \(_{t}\) by averaging) and send it back to all workers. The workers then apply \(_{t}\) and weight decay to update their model parameters (Algorithm 1).

## 2 The Distributed Lion

We introduce the distributed learning problem and then our Distributed Lion framework.

### Distributed Training

In distributed training, we aim to minimize the following learning objective:

\[_{x}F(x)=_{i=1}^{N}_{_{i}_{i}} f(x;_{i}).\] (2)

Here, \(N\) denotes the number of workers, \(\{_{i}\}\) are \(N\) datasets,3 and \(x\) is the model parameter (e.g., the weights of a neural network). In the distributed learning setting, each worker \(i[n]\) will get its own dataset \(_{i}\), and we assume there is a centralized server that all workers can communicate with. The simplest distributed training technique is to perform distributed gradient aggregation:

\[g_{}=_{i=1}^{N}g_{i},\ \ \ \ \ \ g_{i}=_{_{i}_{i}}_{x}f(x;_{i}) .\] (3)

Here, each local gradient \(g_{i}\) is an unbiased estimation of the true gradient \(_{x}F(x)\) when \(_{i}\) are i.i.d. drawn from the same underlying distribution. The server aggregates all local gradients into \(g_{}\), and then applies an optimizer like Adam  on top of \(g_{}\). However, the aggregation step requires communicating the full gradient vectors \(g_{i}\), which can be expensive for large models.

Notation.Given a function \(f(x;)\), the gradient \( f(x;)\) is taken with respect to variable \(x\). We use \(\|\|\), \(\|\|_{1}\), and \(\|\|_{}\) to denote the \(_{2}\), \(_{1}\), and \(_{}\) norm, respectively. \(_{i,t}\) is the sampled data at time \(t\) for the \(i\)-th worker and \(g_{i,t}= f(x_{t};,_{i,t})\). We similarly denote \(z_{i,t}\) as any variable \(z\) at time \(t\) from worker \(i\).

### Distributed Lion

The main idea of Distributed Lion is to leverage the binary nature of the Lion's update for efficient communication. To enable that, we want the workers to _only send the binary updates_ to the server. As a result, we let each worker keep tracks of its own optimizer state, i.e., the momentum \(m_{i,t}\). Then at each step, each worker \(i\) first computes:

\[m_{i,t+1}=_{2}m_{i,t}+(1-_{2})g_{i,t},\] (4) \[_{i,t}=(_{1}m_{i,t}+(1-_{1})g_{i,t}).\]

Then all workers send the \(_{i,t}\) back to the server. The server receives the binary "updates" from all workers and then aggregates them. Here, we propose two simple ways for aggregation. Denote \(S_{t}=_{i=1}^{N}_{i,t}\), which is a vector of integers in \(\{0, N\}\). Define the aggregation as follows:

\[_{t}=(S_{t})=S_{t}&\\ (S_{t})&.\] (5)

    &  \\   & Worker\(\)Server & Server\(\)Worker \\  Global Lion/AdamW & \(32d\) & \(32d\) \\ TernGrad  & \(1.5d\) & \((2n+1)d\) \\ DGC  & \((1-)32d\) & \(32d\) \\  Distributed Lion-Avg & \(d\) & \((n)d\) \\ Distributed Lion-MaVo & \(d\) & \(d\) \\   

Table 1: Minimum bandwidth requirements of different methods for a model with \(d\) parameters and \(n\) workers. For Deep Gradient Compression (DGC), \(\) denotes the compression rate (default: \(=0.96\)).

**Algorithm 1** Distributed Lion Training

So we simply average or take the majority vote from all \(\{_{i,t}\}\). Here, we denote binary vectors in magenta and low precision vectors in cyan. In the end, the server broadcasts \(_{t}\) back to each worker \(i\), and each worker performs \(x_{i,t+1}=x_{i,t}-(_{t}+ x_{i,t})\), where \(\) is the step size and \(\) is the weight decay coefficient.

Communication CostIn both variants of Distributed Lion, the \(N\) workers only need to send the binary vectors \(_{i,t}\) to the server. The server then sends the aggregated update \(_{t}\) back to the workers, which is binary when using the majority vote aggregation, and an integer in \(\{0,,N\}\) when using the averaging aggregation. Note that an integer in \(\{0,,N\}\) can be represented by at most \((N)\) bits. In practice, usually \(N 2^{32}\) hence \((N)<32\) and we still save the communication bandwidth even with the average aggregation, comparing against communicating with floating point numbers (Check Table 1). The full Distributed Lion algorithm is summarized in Algorithm 1.

## 3 Theoretical Analysis

We provide our theoretical analysis of the Distributed Lion algorithm, both with the averaging and the majority vote aggregation methods. In the following, we first describe that the distributed training problem can be viewed as a constrained optimization problem when Distributed Lion is used. We provide convergence results for Distributed Lion with both aggregation methods.

### Lion as Constrained Optimization

Chen et al.  showed that the (global) Lion is a theoretically novel and principled approach for minimizing a general loss function \(f(x)\) while enforcing a box-constrained optimization problem:

\[_{x^{d}}f(x) s.t.\| x\|_{}  1,\] (6)

where the constraint is introduced due to the use of the weight decay coefficient \(\). Moreover, Chen et al.  showed that the Lion dynamics consists of two phases:

1) **[Phase 1]** When the constraint is not satisfied, that is, \(x\), where \(\) is the feasible set \(}{{=}}\{x\| x \|_{} 1\}\), it exponentially decays the distance to \(\): \(\ (0,1)\), such that

\[(x_{t+n},)^{n}(x_{t},).\]

where \(n 0\). Hence, \(x_{t}\) converges to \(\) rapidly and stays within \(\) once it reaches it.

2) **[Phase 2]** After \( x_{t}\) enters \(\), the dynamics minimizes the objective \(f(x)\) while being confined within the set \(\). This step is proved in  by constructing a Lyapunov function when \(()\) is treated as the sub-gradient of a convex function.

### Convergence Analysis

In this section, we analyze the convergence of distributed Lion algorithms. Similar to the case of global Lion, we show that distributed Lion also solves the box constrained optimization (6). Its dynamics also unfolds into two phases aligning with Lion's dynamics: Phase I shows rapid convergence to a feasible set \(\), while Phase II seeks to minize the objective \(f(x)\) within the feasible set \(\). Different from the Lyapunov approach used in Chen et al. , the proof of our Phase II result is made by introducing a surrogate metric \((x)\) of constrained optimality, and providing upper bound of \((x_{t})\) following the algorithm. Our analysis makes the following assumptions.

**Assumption 3.1** (Variance bound).: \(_{i}\) _is i.i.d. drawn from a common distribution \(_{*}\), and the stochastic sample \(^{i}_{i}\) is i.i.d. and upon receiving query \(x^{d}\), the stochastic gradient oracle gives us an independent unbiased estimate \( f(x;^{i})\) from the \(i\)-th worker that has coordinate bounded variance:_

\[_{}[ f(x;^{i})]= f(x),_{}[ \| f(x;^{i})- f(x)\|^{2}]^{2}.\]

**Assumption 3.2** (Smooth and Differentiable \(f\)).: _Function \(f()\) is differentiable and L-smooth._

**Assumption 3.3** (Bias Correction).: _Consider the sequence \(\{m_{t}^{i}\}_{t>0,i[N]}\) generated by Algorithm 1, \([_{t}^{i}]/[(_{t}^{i})] 0\)._

Note that assumption 3.1 and 3.2 are standard in the analysis of stochastic optimization algorithms . When Assumption 3.1 holds, \(\|_{i=1}^{N} f(x;_{i})- f(x)\|^{2} ^{2}/N\). In distributed training setting, \(m_{1,t},m_{2,t},,m_{N,t}\) are i.i.d., so \([_{1}m_{i,t}+(1-_{1})g_{i,t}]\) and \([(_{t+1}^{i})]\) don't depend on \(i\). Assumption 3.3 evaluates the discrepancy between the expected value and the expected sign of a measure, positing that the expected values of \(_{t}^{i}\) and \((m_{t}^{i})\) ought to share the same sign.

We now present our results. Similar to the case of global Lion, the dynamics of distributed lion can also be divided into two phases depending on if the constraint \(x\) is satisfied.

Phase I (\(x\))In line with the behavior observed in the global Lion, when the constraint is not satisfied, both variants of distributed Lion decrease the distance to the feasible set exponentially fast.

**Theorem 3.4** (Phase I).: _Assume \(f^{d}\) is L-smooth, \(_{1},_{2}(0,1)\), and \(_{2}>_{1}\), and \(,>0\). Let \((x_{t})_{t 0}\) be generated by Algorithm 1. Define \(=\{x\| x\|_{} 1\}\), and \((x_{t},)=_{z}\|-x_{t}\|\) w.r.t. any norm \(\|\|\). For any two non-negative integers \(s t\), then \( s t\), we have_

\[(x_{t},)(1-)^{t-s}(x_{ s},).\]

Hence, \(x_{t}\) converges to \(\) rapidly and stays within \(\) once it arrived.

Phase II (\(x\))Now, we present the main result of the analysis for Phase II in Theorems 3.6, 3.7, and 3.8. We start with introducing a surrogate metric that quantifies the optimality of the solution within Phase II:

\[(x):= f(x),( f(x))+ x.\] (7)

Let's delve into the implications of \((x)=0\).

**Proposition 3.5**.: _Assume \(f\) is continuously differentiable, \(>0\), and \(\| x\|_{} 1\). Then \((x)=0\) implies a KKT stationary condition of \(_{x}f(x)\ s.t.\ \| x\|_{} 1\)._

This KKT score (7) is tailored to encompass the stationary solutions of the box constrained problem as described in (6). Building on this, we then proceed to analyze the convergence for the majority vote, averaging, and global LION strategies throughout this section.

**Theorem 3.6** (Majority Vote).: _Assumptions 3.1, 3.2, and 3.3 hold, consider the Majority vote scheme in Algorithm 1, \(_{1},_{2}(0,1)\), and \(_{2}>_{1}\), and \( 2_{1}_{2}^{i}\| f(x_{0})\|,1 t T\), and \(,>0\). Let \((x_{t})_{t 0}\) be generated by Majority Vote, and it is in Phase II: \(\| x_{t}\|_{} 1\) for all \(t\)._

_We have_

\[_{t=1}^{T}[(x_{t})])-f^{*} }{T}+_{2}\| f(x_{0})\|}{T(1- _{2})}+Led}{1-_{2}}+(1+)+2 }{}+2Led,\] (8)_where \(C=_{1}^{2}(1-_{2})}+(1-_{1})^{2}\), and \(D=\{1,/2_{1}_{2}^{T}\| f(x_{0})\|\}\),_

\[_{t}[k]=0&\ [(_{t+1}^ {i}[k])]=0,\\ [_{t+1}^{i}[k]]/[(_{t+1}^{i }[k])]&\]

_, and \(=_{1 t T}\|_{t}\|\)._

The result above shows that \(_{t=1}^{T}[(x_{t})]\) decays with a rate of \((+)}++})\). This rate is in fact on par with global Lion as we show in the following result.

**Theorem 3.7** (Global).: _Assumptions 3.1 and 3.2 hold, Consider the scheme in Algorithm (16) with the same settings in Theorem 3.6, we have_

\[_{t=1}^{T}[(x_{t})])-f^{ *}}{T}+_{2}\| f(x_{0})\|}{T(1- _{2})}+L d}{1-_{2}}+) }{}+2L d.\] (9)

**Theorem 3.8** (Averaging).: _Assumptions 3.1 and 3.2 hold, consider the Averaging scheme in Algorithm 1, with the same settings in Theorem 3.6, we have_

\[_{t=1}^{T}[(x_{t})])-f^{ *}}{T}+_{2}\| f(x_{0})\|}{T(1- _{2})}+L d}{1-_{2}}+}{}}+2(1-_{1})+2L d\] (10)

The Averaging method's convergence bound doesn't improve with more workers since \(_{i=1}^{N}(_{i,t})\) doesn't approximate \((_{i=1}^{N}_{i,t})\) effectively, unlike the Majority Vote's approach \((_{i=1}^{N}(_{i,t}))\).

## 4 Related Work

In this section, we provide a summary of optimizers that use the sign function and existing literature on bandwidth-friendly distributed training.

Sign Operation in OptimizationThe sign operation is integral to optimization for several reasons. Primarily, it acts as a normalization mechanism by disregarding the magnitude of gradients, thereby equilibrating updates across different dimensions and potentially facilitating the avoidance of saddle points. Additionally, the binary nature of the sign function's output significantly reduces the memory footprint required for storing gradient updates. The concept of sign-based optimization dates back to RProp  and has seen renewed interest with the advent of SignSGD and its momentum-enhanced variant, Signsum . A more recent advancement is the generalized SignSGD algorithm introduced by , which incorporates a preconditioner, making it a superset of SignSGD and akin to Adam in certain aspects. A noteworthy addition to sign-based optimizers is the Lion optimizer, which emerged from evolutionary program search, achieving performance comparable to Adam  and AdamW  for the first time. Lion distinguishes itself from Signum by employing a different convex combination for outputting local updates, a technique referred to as the double-\(\) scheme, reminiscent of Nesterov's momentum update, and encapsulates Signum as a particular case. On the theoretical front, SignSGD and Signum have been shown to exhibit convergence rates comparable to traditional SGD . Recent work by  has extended the theoretical understanding by providing a convergence theory that relaxes the requirements for bounded stochastic gradients and enlarged batch sizes. Additionally, Lion has demonstrated its capability in performing constrained optimization under the \(_{}\)-norm constraint .

Distributed TrainingIn addressing the communication constraints of distributed training, the research community has devised several innovative strategies, prominently featuring asynchronous Stochastic Gradient Descent (SGD), gradient quantization, and sparsification techniques. Asynchronous SGD offers a solution by enabling parameter updates immediately after back-propagation, bypassing the need for gradient synchronization, thereby expediting the training process [9; 40; 25]. Li et al.  utilizes sketch-based algorithms for lossless data compression , achieving an asymptotically optimal compression ratio . However, its applicability is limited to highly sparse gradients, making it orthogonal to our research. In the realm of gradient quantization, methods such as 1-bit SGD , QSGD , and TernGrad  are pivotal. These approaches compact the gradient data, substantially reducing the required communication bandwidth, with 1-bit SGD demonstrating a tenfold acceleration in speech applications and both QSGD and TernGrad confirming the feasibility of quantized training in maintaining convergence. Moreover, gradient sparsification further mitigates the communication load by transmitting only the most substantial gradients. Techniques like threshold quantization and Gradient Dropping  exemplify this, with Gradient Dropping notably achieving a 99 reduction in gradient exchange with minimal impact on performance metrics, such as a mere 0.3 loss in BLEU score for machine translation tasks. The recent Deep Gradient Compression (DGC) strategy  also contributes to this field by incorporating momentum correction and local gradient clipping among other methods to maintain accuracy while significantly reducing communication demands, albeit at the cost of increased computational overhead. Compared to gradient quantization methods, Distributed Lion uniquely leverages the binary nature of Lion's update and can be viewed as performing quantization on updates rather than the gradient.

## 5 Experiment

In this section, we perform a thorough evaluation of the Distributed Lion algorithm, employing both the averaging and majority vote aggregation methods. The design of our experiments is aimed at addressing the following questions to ascertain the algorithm's efficacy and performance:

**(Q1)** How does \(\) perform in comparison to traditional global distributed training methods, which aggregate gradients from local workers to apply an optimizer to the collective gradient?

**(Q2)** How does \(\) measure up against established methodologies known for their communication efficiency in distributed training?

**(Q3)** How does Distributed Lion scale on large vision or language problems?

### Comparing Distributed Lion Against Established Methods on CIFAR-10

To address **Q1** and **Q2**, we compare Distributed Lion with both the averaging and the majority vote methods, against established low-bandwidth distributed training techniques and the global distributed training methods. We consider the following baseline methods: **1) Global AdamW (G-AdamW)**, where we apply AdamW with the averaged gradients from all workers. **2) Global Lion (G-Lion)**, where we apply Lion with the averaged gradients from all workers. Note that Global AdamW and Global Lion serve as the performance and communication upper bounds. **3) Distributed Lion with Averaged Updates (D-Lion (Avg))**, In contrast to the majority vote mechanism used in Distributed Lion, this variant averages the binary update vectors from all workers. While D-Lion (Avg) might offer improved performance in principle, it comes at the cost of non-binary communication from the server to the workers. **4) TernGrad**. The main idea is to tennarize the gradient into a vector of \(\{-1,0,1\}\), which is similar to what Lion does. But this process is done on the gradient level instead of on the update level **5) Gradient Dropping (GradDrop)**. The main idea is to drop insignificant gradient entries and only transmit sparse gradient signals. **6) Deep Gradient Compression (DGC)**. DGC is built on top of the GradDrop, but additionally applies momentum correction, local gradient clipping, momentum factor masking, and warm-up training.

Experiment SetupFor GradDrop, DGC, and TernGrad, we choose the compression rate of \(0.04\) (note that \(1/32=0.03125\)) to match the bandwidth of the D-Lion (MaVo). We conduct experiments on the CIFAR-10 dataset using a vision transformer (ViT) with 6 layers, 8 heads, and a hidden dimension of 512. This is because ViT has arguably become the most widely used architecture in computer vision, and we empirically found no additional gain in performance when using a larger ViT on CIFAR-10. In addition, to validate how Distributed Lion performs with different numbers of workers, we consider \(k\{4,8,16,32\}\), each worker at each step samples an i.i.d batch of size 32.

We list the optimal hyperparameters selected for each method from Figure 2 in Table 4. The learning rates are selected from \(\{0.00005,0.001,0.005,0.01\}\) and the weight decays are selected from \(\{0.0005,0.001,0.005\}\). For each experiment, we use a cosine learning rate scheduler and run for 200 epochs, and we ensure that in each epoch, each local worker sees the entire dataset once.

Each experiments are conducted with three random seeds \(\{42,52,62\}\), which results in a total of \(4 7 3=84\) experiments.

**Observation** We plot the testing accuracy (Test Acc.) over epochs for different methods in Figure 2, the best testing accuracy of different methods over the number of workers in Figure 3, and the performance versus per-iteration bandwidth in Figure 4 when using \(k=4\) workers. From the above plots, we make the following observations.

* Compared to global methods, D-Lion (MaVo) performs on par with G-Lion. D-Lion (Avg) performs slightly worse than G-Lion but is on par with G-Adamw (Figure 2).
* Compared to established communication efficient methods, both D-Lion (MaVo) and D-Lion (Avg) outperform GradDrop, DGC and TernGrad by a large margin (Figure 2).
* We observe that both D-Lion (MaVo) and D-Lion (Avg) exhibit strong performance while being 30x more communication efficient than global distributed training methods like G-AdamW. To broaden our comparison, we introduced two additional baseline methods: **D-SIGNUM (Avg)** and **D-SIGNUM (MaVo)**. These baselines apply our proposed techniques to the SIGNUM framework instead of Lion.5 We set \(=0.99\) for D-SIGNUM. According to our results, depicted in Figure 4, these SIGNUM-based methods do not perform as well as their Lion-based counterparts. * We notice that the overall performance of the same optimizer is worse as \(k\) is larger, this is consistent with the observation made in DGC . We hypothesize that this may be due to the larger effective batch size resulting in smaller stochasticity, which is consistent with why D-Lion (MaVo) performs a bit better than G-Lion on CIFAR-10 (Figure 3).

Figure 4: Test Error v.s. Communication Bits per Iteration (closer to the lower-left is better). Note that we set G-Lion and G-AdamW are both 64, because they require 32 bits per parameter, and there are both worker-to-server and server-to-worker communications.

Figure 3: Performance of G-Lion, G-AdamW, GradDrop, DGC, TernGrad, and D-Lion (Avg/MaVo) v.s. the number of workers \(k\).

Figure 2: Performance of Distributed Lion v.s. baseline distributed optimizers on CIFAR-10 with 4, 8, 16, and 32 workers, each worker at each step runs on a local batch with size 32. All results are averaged over three seeds.

### Scale to Larger Models on Larger Datasets

To answer **Q3**, we validate Distributed Lion on several large-scale setups including both vision and natural language processing tasks. Under this setting, we compare D-Lion (MaVo) and D-Lion (Avg) against G-AdamW and G-Lion. For the vision task, we tested ViT-S/16  and ViT-B/16 on the ImageNet-1K  classification benchmark. For the natural language processing task, we perform both language pretraining and finetuning tasks. This is because Lion has shown good results on language modeling. For the language model pretraining task, we pretrain GPT2++  (the GPT-2 model with modern training techniques adopted from the LLaMA model ) on the OpenWebText  benchmark, for both 350M and 760M size models. For the language model finetuning task, we conduct few-shot finetuning of the LLaMA 7B model  and evaluate the models' downstream performance on standard downstream evaluation benchmarks [13; 37; 12; 27; 7; 32].

Experiment SetupFor the ImageNet-1K benchmark, we train all methods for 300 epochs, using a global batch size of 4096 and data augmentations MixUp  of 0.5 and AutoAug . When training ViT-S/16, we use a learning rate of \(3e^{-3}\) for G-AdamW, with betas of \((0.9,0.999)\) and a weight decay of 0.1. For G-Lion, D-Lion (MaVo), and D-Lion (Avg), we use a learning rate of \(3e^{-4}\), betas of \((0.9,0.99)\), and a weight decay of 1.0. As for ViT-B/16, we use a learning rate of \(1e^{-3}\) for G-AdamW, with betas of \((0.9,0.999)\) and a weight decay of 1.0, while for all Lion variants, we use a learning rate of \(1e^{-4}\), betas of \((0.9,0.99)\), and a weight decay of 10.0. For pretraining language models on the OpenWebText dataset, we build GPT2++ models using the original GPT2 model, but with modern training techniques from the LLaMA model, including using the Gated Linear Unit activation for the multilayer layer perceptron layers (MLPs) and the RMSNorm  instead of the LayerNorm . Following the Chinchilla scaling law , we trained the 350M model for 14,000 iterations and the 760M model for 30,000 iterations, both with 1,024 tokens. For G-AdamW, we use a learning rate of \(3e^{-4}\), betas of \((0.95,0.99)\), and a weight decay of 0.1. For all Lion variants, we use a learning rate of \(9e^{-5}\), betas of \((0.9,0.99)\), and a weight decay of 1.0. All the models are trained under a global batch size of 480. For the instruction finetuning task, we instruct finetune a LLaMA 7B model for 3 epochs with batch size 32. We use \(2e^{-5}\) learning rate, betas of \((0.9,0.999)\), 0 weight decay for G-AdamW and \(6e^{-6}\), \((0.9,0.99)\) betas, \(0.01\) weight decay for all Lion variants. For all pretraining experiments, we use 4nodes \(\) 8gpus \(=32\) workers. For instruction finetuning experiments, we use 4 workers per experiment.

ObservationWe summarize the results in Table 2 (ImageNet 1K and OpenWebText Language Model Pretraining) and Table 3 (Instruction Finetuning). Both D-Lion (Avg) and D-Lion (MaVo)

    &  &  \\   & ViT-S/16 & ViT-B/16 & GPT-2++ (350M) & GPT-2++ (760M) \\  AdamW & 79.74 & 80.94 & 18.43 & 14.70 \\ G-Lion & 79.82 & 80.99 & **18.35** & **14.66** \\ D-Lion (MaVo) & 79.69 & 80.79 & 18.37 & **14.66** \\ D-Lion (Avg) & **80.11** & **81.13** & 18.39 & 14.69 \\   

Table 2: Results on ImageNet classification and OpenWebText language modeling. For ImageNet experiments, we report the Top-1 accuracy. For language modeling experiments, we report the validation perplexity. The best performance is marked with bold text, and the second best with an underline.

   Method & Arc-Easy & Arc-Challenge & BoolQ & PIQA & SIQA & HellaSwag & OBQA \\ 
0-Shot & 76.64 & 43.06 & 76.43 & 78.64 & 45.96 & 56.87 & 33.53 \\  G-AdamW & 77.06 & **46.06** & 77.23 & **79.18** & 48.97 & **59.23** & 35.51 \\ G-Lion & **77.11** & 45.54 & **77.50** & **79.18** & 49.64 & 58.93 & 35.51 \\ D-Lion (MaVo) & 76.86 & 45.72 & 77.14 & 78.92 & **49.75** & 58.96 & **35.71** \\ D-Lion (Avg) & 76.35 & 45.54 & 76.90 & 78.76 & 48.06 & 59.06 & 32.14 \\   

Table 3: 3-Shot instruction finetuning downstream evaluation results on various datasets. We mark the best performance with bold text and the second one with an underline.

can maintain a performance similar to, or even better than, that of G-AdamW and G-Lion, on both large-scale vision and language tasks. We observe that D-Lion (Avg) outperforms D-Lion (MaVo) on ImageNet, and observe the opposite on language modeling and instruction finetuning. We hypothesize that these differences are due to the impact of global batch size. As a result, we recommend using D-Lion (Avg) / (MaVo) when the global batch size is large / small.

## 6 Conclusion and Future Work

In this paper, we introduced Distributed Lion, a communication-efficient distributed training strategy that builds upon the Lion optimizer's binary update mechanism. Distributed Lion is designed to minimize communication overhead by allowing workers to independently manage their optimizer states and exchange only binary or low-precision update vectors with the server. We proposed two aggregation techniques within the Distributed Lion framework: average-based (Distributed Lion Avg) and majority vote-based (Distributed Lion MaVo) algorithms. We provide both theoretical and empirical results to demonstrate Distributed Lion's effectiveness, scalability, and efficiency. Notably, we show that Distributed Lion performs significantly better than existing communication-friendly methods. In the meantime, Distributed Lion demonstrates performance on par with strong global distributed training baselines, while being 32x more communication efficient. As our method is orthogonal to existing communication-efficient methods, an interesting future direction is to combine both techniques for further improvement. As a limitation, currently Distributed Lion (Avg / MaVo) performs inconsistently across different datasets and benchmarks, it will be an interesting future research direction to understand when and why one performs better than the other.

## 7 Acknowledgment

The research is conducted in Statistics & AI group at UT Austin, which receives supports in part from NSF CAREER1846421, SenSE2037267, Office of Navy Research, and NSF AI Institute for Foundations of Machine Learning (IFML).