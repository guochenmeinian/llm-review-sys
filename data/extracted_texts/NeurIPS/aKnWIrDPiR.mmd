# Aman Kansal\({}^{2*}\)

Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine

 Emma Chen\({}^{1,4}\)**Julie Chen\({}^{2}\)**

**Boyang Tom Jin\({}^{2}\)**

**Julia Rachel Reisler\({}^{2}\)**

**David A Kim\({}^{3}\)**

**Pranav Rajpurkar\({}^{1}\)**

\({}^{1}\)Department of Biomedical Informatics, Harvard Medical School

\({}^{2}\)Department of Computer Science, Stanford University

\({}^{3}\)Department of Emergency Medicine, Stanford University School of Medicine

\({}^{4}\)Harvard John A. Paulson School of Engineering and Applied Sciences, Harvard University

yingchen@g.harvard.edu

{amkansal, jchen80, tomjin, jreisler, davidak}@stanford.edu

###### Abstract

We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine using a dataset of 100K+ continuously monitored Emergency Department visits from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at timescales from minutes to days, including predicting patient decompensation, disposition, and emergency department (ED) revisit, and includes a standardized evaluation framework with train-test splits and evaluation metrics. The multimodal dataset includes a wide range of detailed clinical data, including triage information, prior diagnoses and medications, continuously measured vital signs, electrocardiogram and photoplethysmograph waveforms, orders placed and medications administered throughout the visit, free-text reports of imaging studies, and information on ED diagnosis, disposition, and subsequent revisits. We provide performance baselines for each prediction task to enable the evaluation of multimodal, multitask models. We believe that MC-BEC will encourage researchers to develop more effective, generalizable, and accessible foundation models for multimodal clinical data.

## 1 Introduction

Emergency Medicine is a critical area of healthcare in which timely and accurate decisions, drawing appropriately on a wide variety of data sources, have a significant impact on patient outcomes . However, developing effective foundation models for electronic health record (EHR) data in Emergency Medicine requires addressing several challenges. EHR data in Emergency Medicine is heterogeneous, including clinical notes, orders, lab results, imaging studies, and physiological waveforms. This heterogeneity can make it difficult to extract meaningful features from the data and integrate them into a single model. Data quality and missingness can also be a significant issue, due to the fast-paced and high-pressure nature of emergency care. Inaccurate or incomplete data can limit the reliability of model results. Clinical interpretability is also critical. Clinicians must be able to understand the model's predictions to make informed decisions. Therefore, models developed for EHR analysis in Emergency Medicine must be transparent and explainable. Finally, the limited scope and standardization of existing datasets are significant challenges to developing foundation models for Emergency Medicine. Many datasets focus on specific patient populations, such as trauma patients, or specific tasks, such as predicting mortality. This orientation to specific subgroups and tasks makes it difficult to compare and evaluate models across tasks and patient populations.

To address these challenges, and to promote the development of robust and clinically useful foundation models for Emergency Medicine, we propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine. MC-BEC is built on a dataset 1 of 102,731 monitored visits made by 63,389 unique patients between 2020 and 2022, and provides a unique opportunity to study acute care in the COVID-19 era. It is the only multimodal medical dataset that exclusively covers patients during this period, while also capturing a wide range of non-COVID pathology. This dataset covers a wide range of information for emergency department (ED) patients, including triage information, prior diagnoses and medications, orders placed in the ED, medication administrations, lab results, continuously monitored vital signs and physiologic waveforms, and free-text reports for radiology studies. With its emphasis on multiple modalities, including continuous waveforms and vital signs providing physiologic context for heterogeneous and often rapidly evolving patients, MC-BEC presents opportunities to study the uniquely dynamic and complex nature of emergency care.

MC-BEC emphasizes clinically relevant downstream tasks at multiple timescales, specifically predicting patient decompensation (within minutes), disposition (within hours), and ED revisit (within days), and provides a standardized evaluation framework with train-test splits and evaluation metrics. We also provide baselines for each task to enable model comparison and evaluation. With MC-BEC, we hope to encourage researchers and clinicians to develop more effective, generalizable, and accessible foundation models for EHR analysis in Emergency Medicine, ultimately improving patient outcomes and advancing the analysis of real-world EHR data.

## 2 Related Work

### Current ED benchmarks are not multimodal

Existing EHR datasets for ED or critically ill patients are often limited to structured EHR data and intermittent vital sign recordings. These datasets fail to capture the comprehensive multimodal information obtained from the intensive evaluation and monitoring of ED patients. To our knowledge, only two ED-specific benchmarks exist. Xie et al. (2022) Xie et al. (2022) introduced an ED benchmark using the MIMIC-IV-ED dataset Xie et al. (2022). While this dataset represents the only publicly available general-purpose ED dataset, it contains only tabular EHR data for all patients, with radiology reports for a subset. EHRShot Xu et al. (2022) is the other recent ED benchmark, but its underlying dataset includes only coded data such as ICD diagnosis codes, and is not publicly available.

Due to the lack of robust ED benchmarks, we also reviewed existing critical care benchmarks, since patient monitoring practices in the ED and intensive care unit (ICU) are similar. A comparison in Table 1 shows most ICU datasets also focus on structured EHR data and intermittent vital signs, lacking free text or waveforms. HiRIDHJiang et al. (2020) provides high-resolution physiological data but no text/reports. Strikingly, neither ICU nor ED datasets include electrocardiogram (ECG) and photoplethysmograph (PPG) waveforms, which represent essential data on the physiology of critically ill patients.

To address the lack of multimodal ED benchmarks, we propose MC-BEC, using a novel multimodal dataset including vital signs, continuous physiologic waveforms (ECG, PPG, respiration), radiology reports, and diverse structured EHR data. This supports improved evaluation of model performance on a wider range of salient patient information.

### Current ED benchmarks are not suitable for generalist medical AI

Existing medical AI benchmarks fall short in comprehensively evaluating generalist medical AI (GMAI) and clinical foundation models. GMAI was recently proposed as a goal for foundation models in healthcare and medicine Xie et al. (2022). GMAI would perform a wide range of medical tasks and flexibly interpret different combinations of data modalities, enabled by foundation models' capability to learn broadly useful data representations from massive pretraining Xie et al. (2022). However, as discussed in a recent survey Xie et al. (2022), current benchmarks focus narrowly on accuracy for predefined tasks, inadequate for evaluating key GMAI capabilities. A major limitation is the lack of assessment for multimodal integration (Table 1). While GMAI is designed to integrate diverse data modalities, performance can decline with more modalities . Yet accuracy metrics alone will not expose these nuances. Current benchmarks also rarely evaluate how models handle missing modalities, though incomplete data is pervasive in real-world EHRs.

To address these gaps, benchmarks must move beyond accuracy to rigorously assess multimodal performance, tolerance to missing data, and other facets critical for GMAI. The proposed MC-BEC benchmark exemplifies this comprehensive approach through evaluating multitask learning, multimodal performance, and fairness analyses beyond predictive accuracy alone. Rethinking evaluation is imperative as GMAI diverges from narrow benchmarks of the past toward more expansive and integrative capabilities. Robust benchmarks will be central to steering progress in this promising new direction.

### GMAI for multimodal EHR data

Our baseline model is inspired by related work in the field. A common approach to foundation models is to first pretrain them on tasks with abundant data to generate robust representations, then to fine-tune on downstream tasks. ClinicalBERT  is a widely used pretrained model specifically designed for clinical text embeddings. Categorical EHR data also has pretraining options like BEHRT, CLMBR and MOTOR , which predict future EHR codes based on past codes. However, these representations of EHR codes depend on the code formats used during training, limiting transferability and robustness to heterogeneous EHR formats. Instead, we employed CodeEmb's  approach that uses pretrained text embeddings associated with EHR code descriptions regardless of the format or code system used by the EHR. For fine-tuning on multimodal data, we adopt the HAIM framework , using a modular ML pipeline that integrates modality-specific pretrained embeddings.

### Multitask learning

Our multitask training approach draws inspiration from prompting techniques in language models. Rather than using task-specific prediction heads, we employ a unified task representation combined with task-specific queries. This allows creating broadly capable models without architectural changes for new tasks.

Conventional multitask learning often designates separate prediction heads for each task, as in MOTOR which has heads for distinct medical predictions. However, the promising avenue of using unified task representations combined with task-specific queries has gained traction, as showcased in studies like OFS  and Perceiver IO . We enhance our baseline LightGBM model  by incorporating contextual embeddings from the BERT model. For each task, we concatenate BERT embeddings of the task-specific queries with other input features. This integrates BERT's rich semantic understanding, allowing the model to adeptly differentiate tasks within the shared

  
**Benchmark** &  &  &  \\   & MT & MM & Fairness & Dataset &  & ED &  Num. \\ patients \\  &  Num. \\ visits \\  &  EHR \\ Codes \\  &  Free-Text \\ Notes \\  &  Vital \\ Signs \\  & 
 ECG/PPG \\ Waveform \\  \\   MIMIC-Extract & & & & & & & & & & & & & & \\ Purushotham 2018 & & & & MIMIC-III & x & 39K\({}^{*}\) & 53K\({}^{*}\) & x & x & x & \\ Harutyunyan 2019 & x & & & & & & & & & & & \\  Xie 2022 & & & & MIMIC-IV-ED (v1.0) & x & 217K & 449K & x & x & x & \\  Gupta 2022 & & & x & & MIMIC-IV (v1.0)  & x & x & 257K & 524k & x & x & x & \\  eICU & & & & eICU & x & & 139K & 201K & x & & x & \\  EHR PT & x & & & MIMIC-III / eICU & & & & & & & & \\  HiRD-ICU & & & & HiRID (v1.1.1) & x & & 34K & 56K & x & & x & \\  EHRSHOT & & & & Stanford Med. & x & x & 7k & 894k & x & & & \\ 
**MC-BEC** & x & x & x & Stanford EM & & x & 63K & 102K & x & x & x & x \\   

Table 1: Comparison of critical and emergency care EHR benchmarks. MT Learning stands for multitask learning; MM Analysis stands for multimodal analysis; the number of patients and visits of MIMIC-III excludes neonatal patients.

embedding space. This provides flexibility to expand to new prediction tasks without architectural changes.

## 3 Benchmark

MC-BEC is a benchmark for evaluating clinical foundation models/GMAI on multimodal EHR data in the ED setting. It provides three clinically relevant prediction tasks spanning different stages of an ED visit: prediction of near-term decompensation early in the visit, prediction of disposition at the end of a visit, and prediction of revisit after the visit. MC-BEC includes evaluation metrics beyond prediction performance, capturing modality interaction, fairness, and consistency of predictions with respect to time and modalities.

### Data source

The MC-BEC dataset consists of 102,731 ED visits made by 63,389 patients between September, 2020 and September, 2022. The dataset is IRB approved, with a waiver of informed consent for retrospective research on de-identified data. The de-identified data for each ED visit spans the entire visit from department arrival to departure, including triage after ED arrival, rooming and initiation of monitoring, and all interventions and results up to the point of departure from the ED. Patients in the dataset have multiple ED visits, but MC-BEC ensures no patient overlap (i.e., visits from the same patient) across the training, testing and validation cohorts. Unlike previous ED datasets, MC-BEC contains both categorical and unstructured clinical data. These modalities and data structures are described below.

**Categorical data with single observation:** chief complaint, triage acuity level, gender, race, ethnicity, means of arrival, disposition of most recent visit, disposition of current visit, and payor class.

**Categorical data with repeated observations:** current and previous ICD-10 diagnosis codes with timestamps and corresponding text descriptions; home medications and medications administered during visit with IDs, names, and timestamps of administration; orders placed during visit with IDs, names, and timestamps; lab results with IDs, names, result values, and timestamps.

**Numeric data:** age, hours since previous visit, and vital signs including heart rate, respiratory rate, blood pressure, oxygen saturation, and temperature.

**Time-series data:** continuously recorded vital signs (heart rate, respiratory rate, oxygen saturation) and secondary features (heart rate variability, pulse transit time, perfusion index) throughout the patient's visit. These measurements were averaged over 1-minute intervals.

**Waveform data:** electrocardiogram (ECG), photoplethysmogram (PPG) and respiratory waveforms.

**Free-text data:** radiology reports for imaging studies ordered during the ED visit.

Figure 1: MC-BEC evaluates foundation model performance on predictions of ED patient decompensation, disposition, and revisit, using a unique multimodal dataset of 102,731 monitored ED visits.

### Benchmark tasks

We chose decompensation, disposition, and revisit as prediction tasks for MC-BEC. These tasks have high clinical and operational relevance in the ED setting: early identification of patients at risk of decompensation allows healthcare providers to allocate appropriate resources; accurate disposition prediction helps optimize resource allocation, bed management, and patient flow within the hospital system; and understanding the probability of revisit allows healthcare providers to identify patients who may require closer follow-up or additional interventions to prevent complications or ensure proper continuity of care. These prediction tasks collectively encompass the entire timeframe of an ED visit, focusing on different aspects in the patient's journey through the ED encounter (Figure 1). We provide brief descriptions of each task as defined for our benchmark:

1. **Decompensation**: The goal of this binary classification task is to predict which patients are likely to experience clinical decompensation, defined as new onset of tachycardia (heart rate [HR] > 110), hypotension (mean arterial pressure [MAP] < 65mmHg), or hypoxia (oxygen saturation by pulse oximetry [SpO2] < 90%) in patients with initially normal vital signs. The task uses the first 15 minutes of data acquired after the patient is roomed (the assessment period) to predict decompensation in a 60, 90 or 120 minute-window (evaluation period) following the assessment period. Patients with abnormal vital signs at triage or at any point during the assessment period are excluded from this task, in order to prevent trivial predictions (e.g., the prediction of future hypotension in an already hypotensive patient).
2. **Disposition:** The objective is to predict the binary outcome of a patient's disposition from the ED: whether they will be discharged home or admitted to the hospital. This is a summative clinical decision that reflects the patient's overall clinical stability and risk as determined by the totality of evidence accrued during the visit. To avoid data leakage, any information that could reveal the disposition decision (such as a consult order to an admitting service) is excluded for this task.
3. **Revisit:** The goal is to predict whether a patient will revisit the ED within a 3, 7, or 14-day period after being discharged. ED revisits are a common quality metric, because return visits can sometimes suggest incomplete workup or inappropriate disposition (i.e., a patient sent home who should have been admitted). All data from a visit can be used to make the revisit prediction, and no patients or visits from the dataset need to be excluded for this task.

### Evaluation framework

**Prediction performance.** We propose to evaluate prediction performance with area under the precision-recall curve (AUPRC) as a threshold-independent metric. AUPRC is a clinically meaningful metric because it reflects the model's performance in correctly identifying positive instances while minimizing false positives. In medical scenarios, correctly identifying positive cases is crucial, as it ensures accurate diagnosis or prediction of a particular condition. AUROC is also particularly important when dealing with unbalanced data, which is common among medical datasets, including ours.

**Monotonicity in modalities.** We propose to assess the performance change of a model when increasing the data modalities available for training. We hypothesize that a well-designed multimodal model should not perform worse when more data modalities are used for training. However, such monotonicity of performance in modalities is not always observed. For example,  found that a unimodal network performed better than the multimodal network obtained through joint training. A reduction in performance caused by the inclusion of more modalities may be attributed to modality competition during training, wherein the model learns to rely on only a subset of modalities to make predictions . The evaluation of monotonicity in modalities can help guide the development of more effective and robust multimodal models.

We evaluate monotonicity in modalities \(m_{modalities}\) by calculating the concordance index \(C\) between the actual performance \(p_{m}(i)\) and the theoretical ground truth which would expect better performance \(y_{m}(i)\) for increasing numbers \(n\) of modalities \(m\):

\[y_{m1} y_{m2} y_{m(n-1)} y_{m(n)}\] (1) \[m_{modalities}=C=^{n}_{j=i+1}^{n}f(y_{i},y_{ j},p_{i},p_{j})}{n(n-1)/2}\] (2)\[f(y_{i},y_{j},p_{i},p_{j})=1&(y_{i}<y_{j})(p_{i} <p_{j})+(y_{i}>y_{j})(p_{i}>p_{j})\\ 0.5&p_{i}=p_{j}\\ 0&\] (3)

**Robustness against missing modalities.** We evaluate the model's robustness and generalization capabilities when one or more modalities of data used in training are unavailable in testing. This metric encourages the development of models with more reliable performance in real-world applications where not all training modalities will be present in all cases.

**Bias.** We evaluate bias as a crucial metric to detect systematic errors or prejudices in the model's predictions or outputs. MC-BEC reflects the ED, where false negative prediction of adverse events can result in severe negative health outcomes. Incorrect disposition decisions might exacerbate health issues; misjudged revisit predictions can overlook preventive care needs; and unforeseen clinical decompensation, like hypotension or hypoxia, can be life-threatening. Hence, we chose the true positive rate (TPR) difference as our primary bias metric. Using a sensitivity threshold of 0.85 on the validation set, we assess TPR differences between different demographic groups such as age, gender, race, and ethnicity. Our approach for evaluating model fairness seeks to identify underdiagnosis bias, which is arguably most relevant and consequential in healthcare settings .

## 4 Experiments

We developed and evaluated baseline models to demonstrate the use of MC-BEC. Our models used pretrained text and waveform embeddings and employed a modular approach for modality fusion. We trained multimodal representations using LightGBM , as gradient boosting ensembles are often highly performant in clinical tasks [33; 34; 35; 36]. We also proposed a novel multitask training schema with LightGBM and compared its performance against models trained on individual tasks. Our aim is to provide an example of MC-BEC evaluation, and a reference for future models evaluated with this benchmark.

### Baseline models

**Featurization of multimodal data.** In our baseline model, we employ a modular approach in multimodal fusion that combines different embeddings to create a multimodal feature representation (Figure 2). The data featurization module is highly specialized for a given modality. Singly observed categorical and numeric data are fed directly into the fusion and prediction modules. All other data modalities undergo an additional step of featurization. For continuous vital sign monitoring, we calculate minimum and maximum values and linear trends. We manually engineer features such as heart rate variability, pulse transit time, and perfusion index from the ECG and PPG waveforms, and produce ECG embeddings from a pre-trained transformer . To encode EHR codes with text descriptions such as diagnoses, medications, and lab results, we leverage ClinicalBERT embeddings to capture the richer and more general information provided by the text description, rather than relying on one-hot embeddings for the codes themselves. For orders, we employ Word2Vec-like embeddings, which are trained based on the co-occurrence of order pairs within patient visits. Lastly, from radiology reports, we produce text embedding features using the pre-trained RadBERT transformer as outlined in Yan et al.'s work . These diverse information sources--ClinicalBERT, Word2Vec for orders, and RadBERT for radiology reports--are then concatenated to form a comprehensive multimodal embedding representation.

**Multitask learning.** We propose a novel multitask training schema for our LightGBM models inspired by prompting in language models, whereby a model's predictions are guided by simply providing specific task descriptions. Depending on the training schema, whether multitask or single-task, we further concatenate the multimodal representation with a task description encoded by BERT-Tiny [39; 40] embeddings. The task descriptions used are "predict disposition" for disposition prediction; "predict revisit within [D] days" for revisit prediction, where D is 3, 7 or 14 depending on the revisit horizon; and "predict decompensation within [M] minutes" for decompensation prediction, where M is 60, 90 or 120 depending on the decompensation window of interest. We chose to encode the task as a text embedding because this approach will have the capability to generalize for predictions on a non-discrete set of tasks. Finally, we train a LightGBM model to make predictions based on these enriched multimodal representations.

### Benchmark results

**Prediction performance.** We compare the performance of models trained on a single task (de-compensation, disposition, or revisit) with a model trained simultaneously on all three prediction tasks. Table 2 presents the model performance as assessed by AUPRC using all available data modalities. Overall, task-specific models perform better than the multitask model in decompensation and disposition predictions, but similarly in revisit predictions. For decompensation prediction, the task-specific model has AUPRCs of 0.33, 0.35 and 0.40 (for 60-, 90- and 120-minute prediction windows), while AUPRCs for the multitask model are 0.25, 0.32, and 0.35. Disposition is predicted with AUPRC 0.87 for the task-specific model and 0.82 for the multitask model. When predicting revisit at 3, 7 and 14-day time windows, we report AUPRC scores of 0.11, 0.16, and 0.24 for the task-specific model, and 0.10, 0.16, 0.25 for the multitask prediction model. The prevalence of the positive class in the dataset is provided as baseline AUPRC . Our results indicate slightly worse performance with a multitask prediction model which conforms with the assumption that the single-task prediction models will be better specialized for their specific task. However, the overall performance gap between single and multitask approaches is small. This suggests potential for more advanced multitask modeling to match or surpass the single-task results by better leveraging commonalities and differences between tasks. Recent work has shown promise for sophisticated multitask architectures surpassing single-task models in clinical predictions. For instance, a multitask channel-wise LSTM exceeded single-task models in predicting in-hospital mortality. Future research can use MC-BEC as an avenue to improve upon multitask models.

**Monotonicity in modalities.** To assess monotonicity in modalities, we train the multitask prediction model on an increasing subset of data modalities. We start with a model trained only on categorical and numeric features, then incrementally add features from diagnoses, medications, orders, lab results, radiology reports, continuous monitoring, and waveforms. Table 3 shows the AUPRC of

  
**TaskModel** &  &  &  \\ 
**Decompensation** & 60 min & 0.33 (0.27 - 0.40) & 0.25 (0.20 - 0.31) & 0.11 \\
90 min & 0.35 (0.30 - 0.41) & 0.32 (0.27 - 0.38) & 0.15 \\
120 min & 0.40 (0.35 - 0.46) & 0.35 (0.30 - 0.41) & 0.17 \\ 
**Disposition** & & 0.87 (0.85 - 0.87) & 0.82 (0.81 - 0.83) & 0.37 \\ 
**Revisit** & 3 days & 0.11 (0.08 - 0.14) & 0.10 (0.08 - 0.13) & 0.05 \\
7 days & 0.16 (0.14 - 0.19) & 0.16 (0.14 - 0.19) & 0.08 \\  & 14 days & 0.24 (0.21 - 0.27) & 0.25 (0.22 - 0.28) & 0.12 \\   

Table 2: AUPRC of task-specific and multitask models (point estimate and 95% confidence interval).

Figure 2: Summary of data modalities represented in MC-BEC and modality-specific featurization strategies.

[MISSING_PAGE_FAIL:8]

We also present supplementary fairness metrics like Statistical Parity Difference (SPD), Disparate Impact (DI), and Equal Opportunity Difference (EOD) between minority and majority cohorts in Table 15 in the Appendix. Models applied to the MC-BEC benchmark should be attentive to potential biases as assessed by our framework, and any identified biases must be carefully considered before deployment to ensure fairness and equity.

## 5 Limitations

While the benchmark provides valuable insights, MC-BEC has limitations. The benchmark does not assess the potential of foundation models in automating novel clinical tasks, such as summarizing all data obtained throughout a visit into an easy-to-understand report for the provider or patient. Future iterations of the benchmark should include novel tasks specific to the ED environment to further highlight the capabilities of clinical foundation models. Although the benchmark evaluates bias on measured patient characteristics, it does not account directly for socioeconomic status. Due to privacy and legal concerns, socioeconomic information, such as income or education level, are not included in the underlying dataset. However, the dataset does include visit payor information, which is correlated with socioeconomic status (for instance, patients whose visits are paid by Medicaid are more likely to have lower socioeconomic status). Finally, we used a relatively simple baseline model to demonstrate the benchmark. Although this choice may limit the model's performance, the primary contribution of this work is the proposal of a new benchmark using a novel multimodal dataset. LightGBM, despite its simplicity, remains a commonly used and highly performant approach for many clinical tasks. Therefore, the baseline model serves not only as a demonstration of the benchmark but also as a reasonable reference for future models.

## 6 Conclusion and Future Work

Our work introduces a novel benchmark (MC-BEC), enabling robust multitask evaluation of prediction models for Emergency Medicine, using a unique multimodal clinical dataset. We evaluate multiple baseline models, and compare the performance of multitask and task-specific models. We present an evaluation schema suited to the demands of multitask clinical predictions on complex multimodal data, which assesses a model's ability to make full use of multimodal data, to make robust

    &  &  &  \\   & & 60 min & 90 min & 120 min & & 3 day & 7 day & 14 day \\ 
**Age** &  55 & 0.87 & 0.86 & 0.88 & 0.93 & 0.69 & 0.75 & 0.88 \\  & = 55 & 0.94 & 0.91 & 0.92 & 0.97 & 0.65 & 0.79 & 0.92 \\   & TPR difference & **0.07** & 0.05 & 0.05 & 0.04 & 0.04 & 0.03 & 0.04 \\ 
**Gender** & Male & 0.92 & 0.93 & 0.94 & 0.96 & 0.68 & 0.79 & 0.91 \\  & Female & 0.88 & 0.85 & 0.85 & 0.95 & 0.67 & 0.74 & 0.87 \\   & TPR difference & 0.05 & **0.08** & **0.09** & 0.01 & 0.01 & 0.04 & 0.04 \\ 
**Race** & White & 0.88 & 0.88 & 0.88 & 0.96 & 0.63 & 0.77 & 0.88 \\  & Black/African American & 1.00 & 0.90 & 0.95 & 0.95 & 0.82 & 0.90 & 0.94 \\  & Asian & 0.88 & 0.87 & 0.88 & 0.96 & 0.64 & 0.73 & 0.85 \\  & Native Hawaiian/Other Pacific & 1.00 & 0.86 & 1.00 & 0.93 & 0.80 & 0.86 & 0.81 \\  & Islander & 0.90 & 0.90 & 0.91 & 0.95 & 0.70 & 0.74 & 0.90 \\  & Other & 0.07 & 0.02 & 0.06 & 0.01 & **0.11** & **0.09** & 0.06 \\ 
**Ethnicity** & Non-Hispanic/Non-Latino & 0.89 & 0.88 & 0.89 & 0.96 & 0.68 & 0.77 & 0.88 \\  & Hispanic/Latino & 0.92 & 0.91 & 0.92 & 0.94 & 0.66 & 0.75 & 0.91 \\   & TPR difference & 0.03 & 0.03 & 0.03 & 0.01 & 0.02 & 0.02 & 0.03 \\   

Table 5: Evaluation of model bias across demographic groups. True positive rate (TPR) and absolute TPR difference between different groups are reported below.

predictions despite missing data, and to make fair and unbiased predictions across demographic groups.

The benchmark models introduced in this work employed an explicit data featurization step, which highlights the challenges of working with diverse, complex and data-rich modalities. We expect future models applied to MC-BEC to leverage alternative approaches, such as with deep end-to-end trainable networks, and integration of a temporal dimension both within and across visits. MC-BEC is centered on predictions of decompensation, disposition, and revisit because these encompass the entire time scale of ED visit prediction tasks. However, future work in benchmarks for generalizable ED prediction models should also aim to evaluate prediction over a non-discrete set of tasks, such as prediction of therapeutic complications or need for post-discharge follow-up. This would enable more granular and comprehensive predictions that could help providers to target and prioritize interventions.