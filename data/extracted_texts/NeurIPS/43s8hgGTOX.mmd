# OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset

Allen Roush

Wand.AI

allen.roush@wand.ai

&Yusuf Shabazz

Howard Community College

yusufishabazz@gmail.com

&Arvind Balaji

Texas AM University

arvindb02@gmail.com

&Peter Zhang

UC Berkeley

petez@berkeley.edu

&Stefano Mezza

University of New South Wales

s.mezza@unsw.edu.au

&Markus Zhang

Stanford University

m.zhang@stanford.edu

&Sanjay Basu

Oracle Corporation

sanjay.basu@oracle.com

&Sriram Vishwanath

University of Texas

sriram@utexas.edu

&Mehdi Fatemi

Wand.AI

mehdi@wand.ai

&Ravid Shwartz Ziv

Wand.AI

New York University

ravid@wand.ai

These authors contributed equally.

###### Abstract

We introduce OpenDebateEvidence, a comprehensive dataset for argument mining and summarization sourced from the American Competitive Debate community. This dataset includes over 3.5 million documents with rich metadata, making it one of the most extensive collections of debate evidence. OpenDebateEvidence captures the complexity of arguments in high school and college debates, providing valuable resources for training and evaluation. Our extensive experiments demonstrate the efficacy of fine-tuning state-of-the-art large language models for argumentative abstractive summarization across various methods, models, and datasets. By providing this comprehensive resource, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. OpenDebateEvidence is publicly available to support further research and innovation in computational argumentation. Access it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist.

## 1 Introduction

Argument mining plays a pivotal role in developing advanced language models (LLMs) capable of sophisticated reasoning and understanding. Engaging with complex argumentative texts enhances LLMs' abilities to comprehend, generate, and evaluate arguments. This improves their performance in applications such as legal document analysis, educational tools, and more.

Existing argument mining datasets, such as DebateSum introduced by Roush & Balaji (2020), are limited in scope. DebateSum, with 240,566 examples, primarily focuses on pre-season evidence fromsummer camps, excluding the rich argumentative structures in regular-season debates. This limitation affects dataset size, representativeness, and utility for large-scale argument mining.

To address these gaps, we introduce OpenDebateEvidence, a large-scale dataset for argument mining and summarization sourced from the OpenCaseList project (Hardy, 2024). This dataset comprises 3.5 million documents, making it the most extensive collection of debate evidence available. It captures the full spectrum of arguments presented throughout the debate season. OpenDebateEvidence's comprehensive nature, with its detailed metadata, makes it highly valuable for training language models.

In this paper, we provide an in-depth overview of OpenDebateEvidence, detailing our data collection and preprocessing methods. We demonstrate that training LLMs on OpenDebateEvidence significantly improves their performance not only on this dataset but also on other related argumentative datasets. We conducted extensive evaluation experiments using state-of-the-art language models: LLaMA3-8B 2 and Mistral-7B3. These models were fine-tuned using advanced techniques such as Low-Rank Adaptation (LoRA) (Hu et al., 2021), Representation Fine-Tuning (ReFT) (Wu et al., 2024), and Orthogonalization (Arditi et al., 2023). The results show substantial improvements in model performance compared to those trained on previous argument mining datasets. This underlines OpenDebateEvidence's effectiveness in enhancing argument-mining capabilities.

Our contributions are:

1. We introduce **OpenDebateEvidence**, the largest and most comprehensive dataset for argument mining and summarization, encompassing 3.5 million documents with detailed metadata.
2. We provide **rich metadata** that facilitates various NLP tasks and applications, enhancing the dataset's utility for researchers and practitioners.
3. We **demonstrate significant performance improvements** of state-of-the-art language models not only on OpenDebateEvidence but also on other related argumentative datasets through extensive fine-tuning experiments.
4. We **evaluate the dataset's effectiveness in different scenarios and methods**, including various fine-tuning techniques such as Low-Rank Adaptation (LoRA), Representation Fine-Tuning (ReFT), and Orthogonalization, showcasing substantial gains in model performance.

Our experiments highlight that training on OpenDebateEvidence not only enhances model performance on this dataset but also significantly improves results on other related argumentative datasets. This underscores the dataset's superiority and its potential to drive advancements in computational argumentation research.

## 2 Background and Related Work

Competitive debate in the United States encompasses several prominent styles, each with unique formats, rules, and emphasis. The three most notable styles are Policy Debate, Lincoln-Douglas Debate, and Public Forum Debate, popular at both high school and collegiate levels. While sharing structural similarities, these debate formats differ in focus, speech times, and the importance placed on evidence. OpenDebateEvidence includes evidence from all of these formats.

### Policy Debate

The High School and College Policy Debate, also known as the "cross-examination debate" (CX), involves two teams of students arguing for and against a specific policy proposal based on an annually changing broad resolution. Each debate round lasts about 90 minutes, comprising eight speeches (four by each team and two by each speaker) and cross-examination periods. The structure includes constructive speeches followed by refutations, with cross-examination periods allowing debaters to clarify arguments or challenge assumptions. New arguments are restricted to constructive speeches.

During a debate, teams present evidence from various sources to support their arguments. This evidence is usually in the form of written "cards" 1, such as research publications, academic articles, news reports, or government documents. Figure 1 shows an example of a "card." The quality and quantity of evidence used in a debate round often determine the winner. Policy Debate is unique among competitive debate styles in that the quality of the speech act is secondary2 compared to the quality, quantity, and factuality of the evidence.

### Lincoln Douglas Debate

Lincoln-Douglas Debate (LD), a one-on-one format with a bimonthly topic, originated from the historic debates between Abraham Lincoln and Stephen Douglas. Popular in high school and college competitions, LD debates share structural similarities with Policy Debate but feature shorter speech times and cross-examination periods. LD debates emphasize ethical and moral reasoning, focusing more on philosophical arguments rather than policy implications. However, they still prioritize the quality and quantity of evidence presented.

### Public Forum Debate

Public Forum Debate is a two-on-two format debating a monthly topic designed to be accessible to a broader audience. Compared to Policy and LD debates, Public Forum rounds have shorter speaking times and place less emphasis on evidence. Public Forum Debate constitutes a smaller portion of the evidence in OpenDebateEvidence and was not included in DebateSum.

### Existing Datasets and Research

Significant prior work in argument mining has focused on competitive formal debate. IBM's Project Debater has been a leading effort, publishing extensively on argument detection (Ein-Dor et al., 2019), argument quality (Gleize et al., 2019), key point analysis/summarization (Bar-Haim et al., 2020; Magnusson & Friedman, 2021), and autonomous debating systems (Slonim et al., 2021). However, their work does not focus on the real-world competitive debate evidence found in our dataset.

Other notable contributions include VivesDebate, a multilingual audio dataset of debate tournaments (Ruiz-Dolz & Iranzo-Sanchez, 2024); ArgAnalysis35K, focusing on single argument analysis pairs in evidence-free parliamentary debate (Joshi et al., 2023); IAM (Integrated Argument Mining), a highly annotated dataset for integrated argument mining tasks with only \(1,\!000\) articles (Cheng et al., 2022); and DebateSum, a dataset with \(240,\!566\) examples focusing on pre-season debate evidence (Roush & Balaji, 2020). Additionally, several legal summarization datasets have been developed, including ArgLegalSumm (Elaraby & Litman, 2022), Multi-LexSum (Shen et al., 2022), and datasets targeting Indian and British case law (Shukla et al., 2022), which together total fewer than \(10,\!000\) examples.

Other resources include logos.app Community (2024c), debate.cards (Community, 2024b), and contention.ai (Community, 2024a), which index various debate evidence and generate new evidence from web searches. Datasets targeting biased or query-focused summarization include QBSUM, a Chinese dataset with \(49,\!000\) samples (Zhao et al., 2021); QMSum, which studies meeting summarization with \(1,\!808\) samples (Zhong et al., 2021); and LMGQS, a dataset with over 1 million documents converted to query-focused summarization (Xu et al., 2023). In contrast, our dataset is fully human-created and human-annotated by active debate competitors.

Compared to these datasets, OpenDebateEvidence offers a significantly larger scale and scope, with over 3.5 million documents enriched with detailed metadata.

**INC--Capitalism Critique**

**1NC--Top Level**

**Capitalist accumulation is unsustainable, causes extinction and planetary immiseration. Robinson, 23--Distinguished Professor of Sociology, Global and International Studies, and Latin American Studies at the University of California at Santa Barbara (William, "The Violent Crackup of the Post-WWII International Order: Notes on the Geopolitical Crisis and Global Capital," Journal of World-Systems Research, Vol. 29, No. 1, dml)**

Some scholars have framed the crisis of global capitalism in terms of a declining U.S. hegemony and the rise of a Chinese competitor. But no new nation-state power can supply the political authority necessary to stabilize the now-inextichy integrated global economy. The crisis of hegemony in the international system takes place within this single, integrated global economy. No one state, no matter how powerful, can control the process of global accumulation. This disjunctive between a globalized economy and a nation-state-based system of political authority, generates enormous geopolitical tensions. The end of Western domination of world capitalism is upon us as the center of gravity of the global economy, shifts to China. But China will not become a new hegemon. Rather, we are moving towards political multipolarity at a time of acute crisis in global capitalism- prolonged economic turbulence and political decay. The breakdown of the political organization of world capitalism is not the cause but the consequence of contradictions internal to a globally integrated system of capital accumulation. Escalating geopolitical conflict is pushing us towards global configuration. Wars provide enormous Butler for surplus accumulated capital Historically they have pulled the capitalist system out of accumulation crisis while they serve to deflect attention from political tensions and problems of legitimacy. The most urgent task at this time is to prevent World Warill. The more we understand the changing nature of this beast that is global capitalism the better we are situated to work out strategies of resistance and transformation. The task before us ever more urgent in the face of the threat of nuclear

**holocaust: the collapse of the biosphere and ever more acute inequality**

**immiseration and social disintegration around the world.**

Figure 1: An example of a piece of debate evidence, colloquially known as a ”card,” from OpenDebateEvidence before parsing. Lines 1 and 2 are the hat and the pocket, used for organizing the evidence by argument and speech. Lines 3-4 are the ”tag,” a biased abstractive summary of the document. The beginning of line 5 shows the author and the year. The rest of lines 5-8 provide the evidence’s citation. The remainder of the document is the evidence itself. Underlined, bolded, or boxed parts are crucial for the argument, and highlighted sections are read aloud during the speech. These elements form various hierarchical levels of biased token-level extractive summaries.

OpenDebateEvidence Dataset

### Data Collection

OpenDebateEvidence is sourced from the OpenCaseList project (Hardy, 2024), an online platform where high school and college debate teams disclose and open-source their evidence. The dataset contains over 3.5 million documents, covering all NSDA debate topics from 2014 to 2022.3. Each document corresponds to a single piece of evidence used in a debate, categorized by debate format (Policy, LD, Public Forum), and includes comprehensive metadata such as author, date, title, source, citation details, and the debate round in which it was used4.

The dataset also includes standardized tags to describe the type of argument made by the document, such as topicality, disadvantages, advantages, and counter plans, along with details of the structure and location in the debate file from which the document was extracted. To protect privacy, identifying information has been anonymized.

### Data Preprocessing

Debate evidence is stored in the.dox file format, requiring a specialized parsing process to extract relevant information. The parsing pipeline begins by unzipping the.dox file to access the internal XML files. Ensuring accurate preprocessing is paramount for maintaining dataset quality. This process involves detailed steps to preserve the integrity and consistency of the data, including tokenization, simplification, and structuring of text blocks, followed by extracting and organizing individual debate cards into a structured format that captures both metadata and content.

The XML files are parsed to extract formatting details such as underlining, bold, and highlighting. Next, the document undergoes tokenization, creating a structured representation with text blocks representing paragraphs or coherent units of text along with their formatting information. A simplification step removes unnecessary formatting and merges adjacent tokens with similar styling.

To extract individual debate cards, the parsing procedure identifies card boundaries based on formatting and structure, extracting components such as the tag, citation, and body text. This information is organized into a structured format that captures the metadata and content of each debate card. Finally, the parsed dataset is converted back into a cleaned Hugging Face dataset, providing a human-readable version of the dataset. This structured dataset serves as the foundation for further natural language processing tasks.

### Data Deduplication

After parsing the dataset and extracting individual cards, identifying and removing duplicates is essential to ensure data quality. Deduplication involves comparing the textual content of each card to identify those sharing significant portions of their text. This process enhances dataset usability by eliminating redundancy, ensuring each unique argument is represented only once.

The deduplication algorithm splits each card's text into sentences. These sentences are then preprocessed by removing non-letter characters and converting them to lowercase. Short sentences below a certain length are filtered out to focus on meaningful content.

The algorithm retrieves and compares card IDs with a significant number of shared sentences. If the number of matching sentences exceeds a predefined threshold and their positions within the cards are within a certain range, the cards are considered duplicates. Duplicate clusters are formed by identifying all cards connected through shared sentences. A representative card is then selected from each cluster based on factors such as sentence count and content quality, and duplicates are removed iteratively.

We note that this is a description of how we created the "Duplicates" metadata column. We performed a separate neural semantic de-duplication procedure which is described in the Appendix.

### Data Statistics

The OpenDebateEvidence dataset offers a comprehensive collection of over 3.5 million documents categorized by debate format (Policy, Lincoln-Douglas, and Public Forum). Each document is enriched with over 40 columns of extensive metadata, including author, date, title, source, citation details, and debate round information. Standardized tags describe the type of argument, such as topicality, disadvantages, advantages, and counterplans.

Policy Debate evidence constitutes approximately two-thirds of the dataset, Lincoln-Douglas Debate evidence comprises about one-third, and Public Forum Debate evidence makes up a smaller percentage. Spanning topics from 2014 to 2022, the dataset represents over 1,300 schools and includes contributions from more than 6,400 authors.

Key statistics of the dataset are provided in Table 1, and more detailed statistics and information can be found in Appendix F.

### Rich Metadata for Argument Structure

Each evidence document is organized with a "hat," "pocket," and "tag" to represent its role within a debate case.

The "pocket" indicates the top-level speech section the evidence supports, such as "1NC" for the first negative constructive speech. The "hat" denotes the broad argument category, like "Oil Disadvantage," which aligns with a structured argument against an affirmative case. The "tag" provides a concise, biased summary of the specific argument made by the evidence. Debaters often create these tags first and then find the evidence that fits the tag.

This metadata encodes the rhetorical structure and purpose of the evidence in a practical and real-world context. The "hat" and "pocket" provide the argument's context, while the "tag" offers a concise summary of the core claim.

For argument mining, this metadata offers valuable semantic annotations for training models on argument components and relations. "Hats" and "pockets" help models learn the overarching structure, while "tags" summarize key points.

For summarization, the hierarchical metadata enables multi-level summaries: "pockets" for high-level overviews, "hats" for key categories, and "tags" for concise core claims. The biased nature of "tags" illustrates how debaters rhetorically summarize their claims and arguments. OpenDebateEvidence is particularly rich as it includes both hierarchical biased abstractive and token-level extractive summaries.

## 4 Experiments

To evaluate the efficacy of the OpenDebateEvidence dataset for argument mining and summarization, we conducted a series of fine-tuning experiments using state-of-the-art language models. We also evaluated the performance of these models on two related datasets.

 
**Feature** & **Count** \\  Total Rows & 4,830,561 \\ Total Documents (valid full-text column) & 3,512,280 \\ Policy Debate Evidence & 2,768,419 \\ Lincoln-Douglas Debate Evidence & 1,526,383 \\ Public Forum Debate Evidence & 43,131 \\  Years Covered & 2014-2022 \\  Average Document Length (characters) & 5,270 \\  Total Schools Represented & 1,366 \\  Unique Authors & 6,455 \\ Unique Topics & 68 \\  Number of Features (columns) & 45 \\  

Table 1: Key Statistics of OpenDebateEvidence Dataset

### Experimental Setup

We employed three recent fine-tuning techniques for adapting our models to OpenDebateEvidence: Low-Rank Adaptation (LoRA) (Hu et al., 2021), Representation Fine-Tuning (ReFT) (Wu et al., 2024), and Orthogonalization (Arditi et al., 2023). These methods are chosen for their parameter efficiency and ability to prevent catastrophic forgetting. The details of these techniques are provided in Appendix D.

We perform our experiments on three datasets: OpenDebateEvidence, DebateSum, which is also a dataset of Policy Debate Evidence, and the billsum dataset from Kornilova and Eidelman (2019), a dataset of US legislation and summaries, to illustrate our fine-tuned models capabilities at performing argumentative summarization in many contexts.

We conducted two types of experiments: traditional NLP evaluation metrics and using GPT-4o as a judge model. All experiments were conducted on a 4xA100 machine from Microsoft Azure with parallelism, attention optimization, and 16-bit quantization enabled. All decoding/sampling settings were kept default. The seed value of "42" was used wherever possible.

#### 4.1.1 Traditional NLP Metrics

For the traditional NLP metrics, we evaluated the models on validation datasets of the whole BillSum dataset and \(10{,}000\) examples from OpenDebateEvidence. Each model was tasked with generating a short "abstract" summarizing the key arguments made in each document. We computed ROUGE F1 scores between the generated text and the ground-truth "tag" provided in the OpenDebateEvidence metadata and the reference summaries in BillSum. For more details see Appendix E. Additionally, we evaluated each language model's perplexity on the sampled subsets to assess how well the models captured the overall distribution of debate and legislative language.

#### 4.1.2 LLM as Judge

In the "LLM as Judge" experiments, we evaluated the quality of the generated abstracts using GPT-4o as the judge. Each model's output was assessed on two criteria: the quality of the output and the quality of supporting the argument, both rated on a scale from 1 to 10. The evaluation was conducted on \(1{,}000\) results from both datasets. This approach allows us to measure not only the linguistic quality of the summaries but also their effectiveness in supporting the arguments. For more details see Appendix E.2.2.

#### 4.1.3 OpenDebateEvidence Performance

For the OpenDebateEvidence dataset (Table 2), the **LLaMA3-70B** models significantly outperformed both the LLaMA3-8B and Mistral-7B models across all ROUGE metrics. The larger model capacity of LLaMA3-70B contributed to its superior performance, highlighting the importance of model size in complex summarization tasks. Fine-tuning techniques, particularly LoRA, further enhanced the LLaMA3-70B model's performance, with the LoRA fine-tuned model achieving the highest scores across all metrics. This underscores LoRA's effectiveness in adapting large models with minimal additional parameters, allowing for improved summarization quality without extensive computational resources. ReFT also showed strong performance improvements on the LLaMA3-70B model, indicating its robustness in refining hidden representations for better output. Orthogonalization, while still providing gains, was less impactful compared to LoRA and ReFT.

The base versions of **Google Gemini** and **Anthropic Claude** models also demonstrated competitive performance, outperforming the base LLaMA3-8B and Mistral-7B models. However, they did not surpass the fine-tuned LLaMA3-70B models, suggesting that while these models are strong out-of-the-box, fine-tuning large models like LLaMA3-70B with techniques such as LoRA can yield superior results.

#### 4.1.4 BillSum Performance

On the BillSum dataset (Table 3), the **LLaMA3-70B** models again demonstrated superior performance compared to smaller models and the base versions of Google Gemini and Anthropic Claude. The LoRA fine-tuned LLaMA3-70B model achieved the highest ROUGE scores and the lowest perplexity, indicating not only better summarization quality but also greater fluency and coherence

  
**Model** & **R-1 (\%)** & **R-2 (\%)** & **R-L (\%)** & **Perplexity** & **Output** & **Support** \\  & & & & & **Quality** & **Quality** \\ 
**Mistral-7B** & & & & & & \\ Base & \(27.8 0.3\) & \(8.2 0.5\) & \(24.5 0.8\) & \(150.2 5.1\) & \(7.5 0.2\) & \(7.3 0.2\) \\ LoRA & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ ReFT & \(29.9 0.2\) & \(9.3 0.1\) & \(25.6 0.6\) & \(50.3 3.4\) & \(7.6 0.3\) & \(7.4 0.3\) \\ Orthogonal & \(27.9 0.5\) & \(8.3 0.2\) & \(24.7 1.2\) & \(76.4 4.4\) & \(7.6 0.2\) & \(7.4 0.2\) \\ 
**LLaMA3-8B** & & & & & \\ Base & \(25.4 0.6\) & \(7.6 0.2\) & \(22.8 1.3\) & \(100.3 5.3\) & \(7.2 0.3\) & \(7.0 0.3\) \\ LoRA & \(25.7 0.7\) & \(7.8 0.1\) & \(23.0 0.7\) & \(77.5 4.6\) & \(7.3 0.2\) & \(7.1 0.2\) \\ ReFT & \(27.6 1.0\) & \(8.7 0.4\) & \(24.9 0.7\) & \(47.8 1.9\) & \(7.3 0.3\) & \(7.1 0.3\) \\ Orthogonal & \(25.5 1.2\) & \(7.7 0.4\) & \(22.9 2.1\) & \(88.0 5.7\) & \(7.2 0.3\) & \(7.0 0.3\) \\ LoRA (1M Ex) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**LLaMA3-70B** & & & & & \\ Base & \(33.8 1.2\) & \(14.1 0.9\) & \(30.2 1.3\) & \(45.7 3.2\) & \(7.9 0.2\) & \(7.5 0.2\) \\ LoRA & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ ReFT & \(35.9 1.1\) & \(15.1 0.7\) & \(32.6 1.2\) & \(31.4 2.8\) & \(8.0 0.3\) & \(7.9 0.2\) \\ Orthogonal & \(34.2 1.2\) & \(14.3 0.8\) & \(31.1 1.3\) & \(39.9 3.1\) & \(8.1 0.2\) & \(7.7 0.3\) \\ 
**Google Gemini** & & & & & \\ Base & \(32.5 1.3\) & \(13.5 0.9\) & \(29.4 1.4\) & \(49.8 3.6\) & \(\) & \(7.9 0.3\) \\ 
**Anthropic Claude** & & & & & \\ Base & \(31.2 1.4\) & \(12.9 0.9\) & \(28.1 1.3\) & \(52.1 3.7\) & \(\) & \(7.8 0.3\) \\   

Table 2: Performance on OpenDebateEvidence. ROUGE F1 scores and perplexity on \(10{,}000\) sampled documents, and LLM as Judge scores on \(1{,}000\) results. Scores are averaged over three runs. R-1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L respectively. Error bars represent one standard error over 3 trials.

  
**Model** & **R-1 (\%)** & **R-2 (\%)** & **R-L (\%)** & **Perplexity** & **Output** & **Support** \\  & & & & **Quality** & **Quality** \\ 
**Mistral-7B** & & & & & \\ Base & \(44.8 0.3\) & \(21.2 0.5\) & \(40.5 0.8\) & \(25.2 1.1\) & \(7.2 0.3\) & \(7.0 0.3\) \\ LoRA & \(\) & \(\) & \(\) & \(\) & \(7.4 0.2\) & \(7.2 0.2\) \\ ReFT & \(46.9 0.2\) & \(23.3 0.1\) & \(42.6 0.6\) & \(24.3 0.4\) & \(\) & \(\) \\ Orthogonal & \(44.9 0.5\) & \(21.3 0.2\) & \(40.7 1.2\) & \(25.4 1.4\) & \(7.3 0.2\) & \(7.1 0.2\) \\ 
**LLaMA3-8B** & & & & & \\ Base & \(42.4 0.6\) & \(19.6 0.2\) & \(38.8 1.3\) & \(27.3 1.3\) & \(7.0 0.3\) & \(6.8 0.3\) \\ LoRA & \(42.7 0.7\) & \(19.8 0.1\) & \(39.0 0.7\) & \(27.0 1.1\) & \(7.1 0.2\) & \(6.9 0.2\) \\ ReFT & \(44.6 1.0\) & \(20.7 0.4\) & \(40.9 0.7\) & \(26.8 1.0\) & \(7.2 0.3\) & \(7.0 0.3\) \\ Orthogonal & \(42.5 1.2\) & \(19.7 0.4\) & \(38.9 2.1\) & \(27.5 1.5\) & \(7.1 0.3\) & \(6.9 0.3\) \\ LoRA (1M Ex) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**LLaMA3-70B** & & & & & & \\ Base & \(47.3 1.4\) & \(24.8 1.1\) & \(42.9 1.6\) & \(26.7 3.0\) & \(\) & \(7.8 0.3\) \\   

Table 3: Performance on BillSum. ROUGE F1 scores and perplexity on \(10{,}000\) sampled documents, and LLM as Judge scores on \(1{,}000\) results. Scores are averaged over three runs. R-1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L respectively. Error bars represent one standard error over 3 trials.

in the generated summaries. ReFT also provided significant improvements, reinforcing its effectiveness across different datasets. The consistent performance gains of LoRA and ReFT across the datasets suggest that these fine-tuning techniques are effective in enhancing model capabilities in summarization tasks.

Google Gemini and Anthropic Claude performed competitively, especially considering they were evaluated in their base configurations. Their performance on BillSum highlights their strong capabilities in handling legislative texts, but they were still outperformed by the fine-tuned LLaMA3-70B models. Orthogonalization showed less improvement compared to LoRA and ReFT, which may be due to its more conservative approach in parameter adjustments.

#### 4.1.5 DebateSum Performance

In evaluating the DebateSum dataset (Table 4), the fine-tuned **LLaMA3-70B** models demonstrated notable improvements over all other models. The LoRA fine-tuned LLaMA3-70B model achieved the highest scores across all ROUGE metrics and exhibited the lowest perplexity, indicating superior summarization capabilities. ReFT also significantly enhanced the LLaMA3-70B model's performance, confirming its robustness in refining models for complex summarization tasks involving argumentative content.

The base versions of Google Gemini and Anthropic Claude also performed well on DebateSum, outperforming the base versions of smaller models but not reaching the performance levels of the fine-tuned LLaMA3-70B models. This suggests that while these models have strong general capabilities, targeted fine-tuning can lead to substantial performance gains in specific tasks. Orthogonalization, while providing some improvements, was less effective compared to LoRA and ReFT, consistent with observations on the other datasets.

In summary, the new experimental results demonstrate the effectiveness of fine-tuning techniques, especially LoRA, when applied to larger models like LLaMA3-70B. These techniques consistently improved performance across all datasets, highlighting their utility in enhancing large language models for complex summarization tasks. The base versions of Google Gemini and Anthropic Claude

  
**Model** & **R-1 (\%)** & **R-2 (\%)** & **R-L (\%)** & **Perplexity** & **Output** & **Support** \\  & & & & & **Quality** & **Quality** \\ 
**Mistral-7B** & & & & & & \\ Base & \(26.3 0.4\) & \(7.5 0.3\) & \(23.1 0.6\) & \(130.5 4.2\) & \(7.3 0.3\) & \(7.0 0.3\) \\ LoRA & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ ReFT & \(28.3 0.3\) & \(8.8 0.2\) & \(24.5 0.4\) & \(45.1 2.7\) & \(7.4 0.2\) & \(7.2 0.2\) \\ Orthogonal & \(26.4 0.5\) & \(7.6 0.2\) & \(23.3 0.7\) & \(70.3 3.5\) & \(7.4 0.3\) & \(7.2 0.3\) \\ 
**LLaMA3-8B** & & & & & & \\ Base & \(24.2 0.5\) & \(6.9 0.3\) & \(21.9 0.8\) & \(95.7 4.5\) & \(7.0 0.3\) & \(6.7 0.3\) \\ LoRA & \(24.5 0.6\) & \(7.0 0.2\) & \(22.1 0.6\) & \(73.9 3.2\) & \(7.1 0.2\) & \(6.8 0.2\) \\ ReFT & \(26.6 0.9\) & \(8.0 0.3\) & \(23.8 0.7\) & \(44.7 1.7\) & \(7.1 0.3\) & \(6.9 0.3\) \\ Orthogonal & \(24.3 1.1\) & \(7.0 0.4\) & \(22.0 1.9\) & \(82.5 4.2\) & \(7.0 0.3\) & \(6.8 0.3\) \\ LoRA (1M Ex) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**LLaMA3-70B** & & & & & & \\ Base & \(36.7 1.2\) & \(16.9 0.9\) & \(32.8 1.3\) & \(42.1 3.1\) & \(7.7 0.2\) & \(7.4 0.2\) \\ LoRA & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ ReFT & \(38.1 1.2\) & \(17.8 0.7\) & \(34.2 1.2\) & \(30.5 2.8\) & \(7.9 0.3\) & \(7.8 0.2\) \\ Orthogonal & \(36.9 1.3\) & \(17.0 0.8\) & \(33.0 1.4\) & \(39.3 3.2\) & \(7.8 0.3\) & \(7.6 0.3\) \\ 
**Google Gemini** & & & & & & \\ Base & \(35.5 1.4\) & \(15.8 0.9\) & \(31.4 1.5\) & \(44.5 3.4\) & \(\) & \(7.9 0.3\) \\ 
**Anthropic Claude** & & & & & & \\ Base & \(34.2 1.5\) & \(15.0 1.0\) & \(30.1 1.6\) & \(46.8 3.6\) & \(\) & \(7.8 0.3\) \\   

Table 4: Performance on DebateSum. ROUGE F1 scores and perplexity on \(10{,}000\) sampled documents, and LLM as Judge scores on \(1{,}000\) results. Scores are averaged over three runs. R-1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L respectively. Error bars represent one standard error over 3 trials.

showed strong capabilities, but targeted fine-tuning of large models remains crucial for achieving state-of-the-art results.

### Additional Experiments

We conducted additional experiments to further evaluate the robustness and generalizability of our fine-tuning techniques. These experiments included assessments on the xSum dataset, a non-argumentative summarization task, and an analysis of inter-human and human-GPT-4 agreement rates in evaluating summary quality. We also conducted further experiments to evaluate our models on argument detection, classification tasks, stance detection, and downstream tasks. We also assessed the reliability of GPT-4 as an evaluator by comparing its ratings with those of human experts. These additional experiments are included in the appendix

## 5 Conclusion

In this paper, we introduce OpenDebateEvidence, a large-scale dataset for argument mining and summarization, comprising over 3.5 million documents from the OpenCaseList project. After extensive preprocessing and deduplication, we created a high-quality dataset enriched with metadata that captures the hierarchical structure and semantics of debate arguments. Our experiments demonstrated the potential of fine-tuning modern large language models for argumentative abstractive summarization in a parameter-efficient manner. The results showed significant improvements in performance on the OpenDebateEvidence, DebateSum, and BillSum datasets, validating the effectiveness of our approach.

By providing this resource to the community, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. The OpenDebateEvidence dataset, with its rich metadata and diverse collection of debate formats, offers an excellent resource for developing and evaluating argument mining and summarization models.

Future work includes exploring additional fine-tuning techniques and expanding the dataset to include more diverse debate formats. We also plan to investigate the integration of multimodal data to enhance argument comprehension and explore cross-linguistic adaptations to broaden the applicability of our models. By continuing to refine and expand this resource, we hope to further enhance language models' capabilities in understanding and generating complex argumentative discourse.