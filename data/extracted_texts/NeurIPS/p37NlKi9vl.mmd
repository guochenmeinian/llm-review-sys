# Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization

Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization

 Davide Buffelli

MediaTek Research

&Jamie McGowan

MediaTek Research

&Wangkun Xu

Imperial College London

&Alexandru Cioba

MediaTek Research

&Da-shan Shiu

MediaTek Research

&Guillaume Hennequin

MediaTek Research & University of Cambridge

&Alberto Bernacchia

MediaTek Research

Equal Contribution. Correspondence to {davide.buffelli,jamie.mcgowan}@mtkresearch.comWork done while at MediaTek Research.

###### Abstract

Second-order optimization has been shown to accelerate the training of deep neural networks in many applications, often yielding faster progress per iteration on the training loss compared to first-order optimizers. However, the generalization properties of second-order methods are still being debated. Theoretical investigations have proved difficult to carry out outside the tractable settings of heavily simplified model classes - thus, the relevance of existing theories to practical deep learning applications remains unclear. Similarly, empirical studies in large-scale models and real datasets are significantly confounded by the necessity to approximate second-order updates in practice. It is often unclear whether the observed generalization behaviour arises specifically from the second-order nature of the parameter updates, or instead reflects the specific structured (e.g. Kronecker) approximations used or any damping-based interpolation towards first-order updates.

Here, we show for the first time that exact Gauss-Newton (GN) updates take on a tractable form in a class of deep reversible architectures that are sufficiently expressive to be meaningfully applied to common benchmark datasets. We exploit this novel setting to study the training and generalization properties of the GN optimizer. We find that exact GN generalizes poorly. In the mini-batch training setting, this manifests as rapidly saturating progress even on the _training_ loss, with parameter updates found to overfit each mini-batchch without producing the features that would support generalization to other mini-batches. We show that our experiments run in the "lazy" regime, in which the neural tangent kernel (NTK) changes very little during the course of training. This behaviour is associated with having no significant changes in neural representations, explaining the lack of generalization.

## 1 Introduction

Efficient optimization of overparameterized neural networks is a major challenge for deep learning. For large models, training remains one of the main computational and time bottlenecks. Much work has therefore been devoted to the development of neural network optimizers that could accelerate training, enabling researchers and engineers to iterate faster and at lower cost in their search for better performing models. Second-order optimizers, in particular, have been shown to deliver substantially faster per-iteration progress on the training loss (Martens and Grosse, 2015; Botev et al., 2017; George et al., 2018; Goldfarb et al., 2020; Bae et al., 2022; Petersen et al., 2023; Garcia et al., 2023), and much work has been done to scale them to large models via suitable approximations (Ba et al., 2017; Anil et al., 2021). However, the generalization properties of second-order optimizers remain poorly understood. Here, we focus on the training and generalization properties of the Gauss-Newton (GN) method, which - in many cases of interest - also encompasses natural gradient descent (NGD) (Martens, 2020).

Theoretical studies of generalization in GN/NGD have been limited to simplified models, such as linear models (Amari et al., 2021) or nonlinear models taken to their NTK limit (Zhang et al., 2019). When applied to real-world networks and large datasets, GN/NGD has so far required approximations, such as truncated conjugate gradient iterations in matrix-free approaches (Martens et al., 2010), or block-diagonal and Kronecker-factored estimation of the Gauss-Newton / Fisher matrix (Martens and Grosse, 2015; Botev et al., 2017; George et al., 2018; Goldfarb et al., 2020; Bae et al., 2022; Petersen et al., 2023; Garcia et al., 2023). Those approximations are exact only in the limit of constant NTK (Karakida and Osawa, 2020), in which models cannot learn any features (Yang and Hu, 2021; Aitchison, 2020). To our knowledge, the only case in which exact and tractable GN updates have been obtained is that of deep linear networks (Bernacchia et al., 2018; Huh, 2020), which - despite exhibiting non-trivial learning dynamics (Saxe et al., 2013) - cannot learn interesting datasets nor yield additional insights into generalization beyond the linear regression setting. Critically, the use of necessary approximations makes it difficult to understand how much of the observed generalization (or lack thereof) can be attributed to the GN method itself, or to the various ways in which it has been simplified.

Here, we derive an exact, computationally tractable expression for Gauss-Newton updates in deep _reversible_ networks (Dinh et al., 2015; Mangalam et al., 2022). In reversible architectures made of stacked, volume-preserving MLP-based coupling layers (which we call RevMLPs), we show that it is possible to analytically derive a specific form of a generalized inverse for the network's Jacobian. This generalized inverse enables fast, exact GN updates in the overparameterized regime. We highlight that, in contrast to the work of Zhang et al. (2019); Cai et al. (2019); Rudner et al. (2019); Karakida and Osawa (2020), we do not assume constant NTK, instead we only require the NTK to remain non-singular during training (Nguyen et al., 2021; Liu et al., 2022; Charles and Papailiopoulos, 2018) as, for example, in the mean-field limit (Arbel et al., 2023). Equipped with this new model, we study for the first time the generalization behaviour of GN in realistic settings. In the stochastic regime, we find that GN trains _too_ well, overfitting single mini-batch at the expense of impaired performance not only on the test set, but also on the training set. To understand this severe lack of generalization, we conduct a careful examination of the model's neural tangent kernel and show that the NTK remains almost unchanged during training, and that the neural representations that arise from after training are not different from those set by the network's initialization. Thus, GN tends to remain in the "lazy" regime (Jacot et al., 2018; Chizat et al., 2019), in which representations remain close to those at initialization, lacking generalization.

In summary:

* We show that GN updates computed with any generalized inverse of the model Jacobian results in the same dynamics of the loss, provided that the NTK does not become singular during training (Theorem 4.3).
* We derive an exact and tractable generalized inverse of the Jacobian in the case of deep reversible neural networks (Proposition 4.4). The corresponding GN updates have the same complexity as gradient descent.
* We study the generalization properties of GN in models up to 147 million parameters on MNIST and CIFAR-10, and we show that neural representations do not change during training, as the model remains in the "lazy" regime.

## 2 Related Work

Exact vs approximate Gauss-Newton in deep learning.Previous work on second-order optimization of deep learning models focused on either Natural Gradient Descent (NGD), or Gauss-Newton(GN). Since the two are equivalent in many important cases (Martens, 2020), here we do not distinguish them and we refer simply to GN. The most popular methods for computing Gauss-Newton updates assume block-diagonal and Kronecker-factored pre-conditioning matrices (Martens and Grosse, 2015; Botev et al., 2017; George et al., 2018; Goldfarb et al., 2020; Bae et al., 2022; Petersen et al., 2023; Garcia et al., 2023). Such approximations are known to be exact only in deep linear networks (Bernacchia et al., 2018; Huh, 2020) and in the Neural Tangent Kernel (NTK) limit (Karakida and Osawa, 2020), both of which cannot learn features (Yang and Hu, 2021; Aitchison, 2020). Recent work focused on exact Gauss-Newton in the feature learning (mean-field) regime (Arbel et al., 2023) but they studied only small models applied to synthetic data. The work of Cai et al. (2019) studies exact Gauss-Newton on real data but only models with one-dimensional outputs. Our work is the first to investigate exact Gauss-Newton in the feature learning regime on real data and sizeable neural networks.

Reversible neural networks.Reversible neural networks (Dinh et al., 2015) allow saving memory during training of large models, because they do not require storing activations (Gomez et al., 2017; MacKay et al., 2018), and achieve near state-of-the-art performance (Mangalam et al., 2022). Reversible neural networks also feature in normalizing likelihood-based generative models, or normalizing flows (Dinh et al., 2016). In different reversible models, the inverse is either computed analytically with coupling layers (Kingma and Dhariwal, 2018; Chang et al., 2018; Jacobsen et al., 2018) and similar algebraic tricks (Papamakarios et al., 2017; Hoogeboom et al., 2019; Finzi et al., 2019; Xiao and Liu, 2020; Lu and Huang, 2020), is computed numerically (Behrmann et al., 2019; Song et al., 2019; Huang et al., 2020), or is learned (Keller et al., 2021; Teng and Choromanska, 2019). In this work, we use analytical inversion with coupling layers, because of the efficiency of automatic differentiation through the inverse function. Our work is the first to use reversible neural networks to compute Gauss-Newton updates. A previous work made a connection between reversible models and Gauss-Newton (Meulemans et al., 2020), but they studied Target Propagation, a very different optimizer.

Generalization of Gauss-Newton in overparameterized models.The generalization properties of Gauss-Newton are currently debated. While Wilson et al. (2017) shows worst-case scenarios for adaptive methods, Zhang et al. (2019) suggests that GN has similar generalization properties as gradient descent (GD) in the NTK limit. In overparameterized linear models, GN and GD find the same optimum (Amari et al., 2021), however GD transiently achieves better test loss before convergence (Wadia et al., 2021). The loss dynamics of Gauss-Newton is approximately re-parameterization invariant, and it remains unclear whether a specific parameterizations allows GD to generalize better (Kerekes et al., 2021). Previous work also suggests a trade-off between training speed and generalization of GN: a good generalization is obtained only when slowing down training, either by damping (Wadia et al., 2021) or by small learning rates (Arbel et al., 2023). Here we study for the first time generalization for exact GN in sizeable neural networks and real data, and we show that GN achieves poor generalization with respect to gradient descent and similar first order optimizers.

## 3 Background

We provide a brief introduction to Gauss-Newton and Generalized Gauss-Newton. Given input and target data pairs \((x,y)^{d_{x}}^{d_{y}}\) and parameters \(^{p}\), the loss is a sum over a batch \(=\{(x_{i},y_{i})_{i=1}^{n}\}\) of \(n\) data points

\[()=_{i=1}^{n}(y_{i},f(x_{i},) )=}(())\] (1)

with a twice differentiable and convex function \(\) (e.g. square loss or cross entropy) and a parameterized model \(f(x_{i},)\) (e.g. a deep neural network). In the second equality of (1), we concatenate the model outputs \(f(x_{i},)^{d_{y}}\) for all \(n\) data points in a single (column) vector \(()^{nd_{y}}\) with entries \(_{i+n(j-1)}=f(x_{i},)_{j}\), and define concisely the loss in function space as \(}(())\). The loss \(}()\) is a convex and twice differentiable function of the model \(\), but \(()\) is usually a non-convex function of the parameters \(\), due to the non-linearity of the model \(()\). Gradient descent optimizes parameters according to:

\[_{t+1}=_{t}-\ _{}\] (2)where \(\) is the learning rate and \(_{}\) is the gradient of the loss with respect to the parameters. In the full-batch setting, \(\) is the full training dataset. In the mini-batch setting (stochastic gradient descent, SGD), a batch of data \(\) is drawn at random from the dataset at each iteration, without replacement, until all data is covered (one epoch), after which random batches are re-drawn.

Gauss-Newton.We review two alternative but equivalent views on Gauss-Newton: the _Hessian_ view and the the _functional_ view, which provide different intuitions into the method. The _Hessian_ view understands Gauss-Newton as a second-order optimization method, from the point of view of the curvature of the loss. The _functional_ view understands Gauss-Newton as model inversion, and is more appropriate in the context of our work.

In the _functional_ view, Gauss-Newton corresponds to gradient descent in function space (Zhang et al., 2019; Cai et al., 2019; Bae et al., 2022; Amari, 1998; Martens, 2020). By assumption, the loss \(}\) is a convex function of the model outputs \(\), thus it would be convenient to optimize the model outputs directly. Gradient flow in function space is given by

\[}{dt}=-_{}}_{ (t)}\] (3)

However, we need to optimize parameters \(\) to have a model that can be applied to new data. If \((t)=((t))\), we use the chain rule to find the update in parameter space that corresponds to gradient flow in function space,

\[}{}}{dt}=-_ {}}_{((t))}\] (4)

Given the gradient \(_{}}\) and the Jacobian \(J=}{}\) defined as \(J_{ab}=_{a}}{_{b}}\) (of shape \(nd_{y} p\)), this is a linear system of equations that can be solved for the update \(}{dt}\), by pseudo-inverting the Jacobian. In discrete time, with learning rate \(\), the update is equal to (Bjorck, 1996; Ben-Israel, 1965)

\[_{t+1}=_{t}-\ J^{+}\ _{}}\] (5)

where the superscript \(+\) denotes matrix pseudo-inversion. We use this update in our work, in either the full-batch or mini-batch setting. We note that equation (5) implies equation (3), in the continuous time limit, only if the Jacobian has linearly independent rows (\(JJ^{+}=_{nd_{y}}\)), which also guarantees convergence to a global minimum (full-batch). This requires overparameterization \(p nd_{y}\), however, even if the model is underparameterized and does not converge to a global minimum, equation (5) is still equivalent to Gauss-Newton in the _Hessian_ view, as shown below.

In the _Hessian_ view, Gauss-Newton corresponds to Newton's method with a positive-definite approximation of the Hessian, in the case of square loss (Dennis Jr and Schnabel, 1996; Nocedal and Wright, 1999; Bottou et al., 2018). The approximation is accurate near a global minimum of the loss, therefore Gauss-Newton inherits the accelerated convergence of Newton's method near global minima (Dennis Jr and Schnabel, 1996). The Gauss-Newton update, with learning rate \(\), is equal to

\[_{t+1}=_{t}-\ (J^{T}J)^{+}_{}\] (6)

Matrix pseudo-inverse is used instead of inverse when \(J^{T}J\) is singular (damping is also a popular choice, see Nocedal and Wright (1999)). It is straightforward to prove that equations (6) and (5) are identical, by noting that, since \(()=}(())\), then \(_{}=J^{T}_{}}\) by chain rule, and \((J^{T}J)^{+}J^{T}=J^{+}\) by the properties of matrix pseudo-inverse. The _Gram_-Gauss-Newton update of Cai et al. (2019) is also equivalent to equation (5), it just requires the formula for the Jacobian pseudo-inverse in the case of linearly independent rows.

Generalized Gauss Newton.Following the _Hessian_ view, Generalized Gauss-Newton (GGN) was introduced for convex losses that are different from square loss (Ortega and Rheinboldt, 2000; Schraudolph, 2002). The Hessian is approximated by the positive semi-definite matrix \(J^{T}HJ\), where \(H=_{}^{2}}\). As in the case of square loss, the approximation is accurate near a global minimum. That leads to the following update:

\[_{t+1}=_{t}-\ (J^{T}HJ)^{+}_{}\] (7)

Note that GGN reduces to GN for \(H=_{nd_{y}}\) (square loss). In the _functional_ view, Appendix A shows that Generalized Gauss-Newton corresponds to Newton's method in function space, provided that the Jacobian has linearly independent rows and \(}\) is strongly convex. Furthermore, Appendix B provides some intuition into the convergence of GGN flow.

Exact and tractable Gauss-Newton

The main hurdle in the GN update of equation (5) is the computation of the Jacobian pseudo-inverse. For a batch size \(n\), number of parameters \(p\) and output dimension \(d\), that requires \((ndp(nd,p))\) compute and \((ndp)\) memory. In this Section, we show that the GN update can be computed efficiently for reversible models. For a dense neural network of \(L\) layers and dimension \(d\), implying \(p=(Ld^{2})\) parameters, our GN update requires the same memory as gradient descent and \((Lnd^{2}+Ln^{2}d)\) compute, compared to \((Lnd^{2})\) compute of gradient descent.

Our method consists of two steps: first, we replace the Jacobian pseudoinverse with a generalized inverse, and show that it has identical convergence properties (Theorem 4.3). Second, we show that a specific generalized inverse can be computed efficiently in reversible neural networks (Proposition 4.4). We present both results in the case of square loss (GN). Results for other convex loss functions (GGN) can be derived following steps similar to Appendix B.

Replacing the pseudoinverse with a generalized inverse.We show that the Jacobian pseudoinverse in equation (5) can be replaced by a generalized inverse that has the same convergence properties. A similar approach was proposed by Bernacchia et al. (2018); Karakida and Osawa (2020), but it was only valid in the case of, respectively, deep linear networks or constant Neural Tangent Kernel (NTK) limit. Here we provide a more general formulation that holds under less restrictive assumptions, e.g. it holds in the mean field regime (Arbel et al., 2023). We need the following assumption

**Assumption 4.1**.: Assume \(J()\) has linearly independent rows (is surjective) for all \(\) in the domain where GN dynamics takes place.

Note that this implies that the network is overparametrized, i.e. \(p nd_{y}\). While, in practice, this assumption may seem strong, it is only slightly stronger than the following version, employed in Arbel et al. (2023):

**Assumption 4.2**.: \(J(_{0})\) is surjective at initialization \(_{0}\).

In Arbel et al. (2023), the authors argue that since surjectivity of \(J\) is an open condition, it holds for a neighbourhood of \(_{0}\), and moreover continue to prove that the dynamics of GN is well defined up to some exit time \(T\) from this neighbourhood. They then continue to give assumptions guaranteeing that this dynamics extends to \(\). We directly assume we are in this latter setting.

**Theorem 4.3**.: _Under Assumption 4.1 so that there is a right inverse \(J^{+}\) satisfying \(JJ^{+}=I\), consider the update in parameter space with respect to the flow induced by an arbitrary right inverse \(J^{+}\):_

\[_{t+1}=_{t}- J^{-}_{}} {}.\] (8)

_Then the loss along these trajectories is the same up to \(()\), i.e. for any two choices \(J^{+}_{1}\) and \(J^{+}_{2}\), the corresponding iterates \(^{(1)}_{t}\) and \(^{(2)}_{t}\) satisfy:_

\[\|_{}}}((^{(1)}_ {t}))-_{}}}((^{( 2)}_{t}))\|().\] (9)

_Moreover, as the Moore-Penrose pseudo-inverse is a right inverse under the assumptions, the result applies to \(J^{+}\), and consequently to the dynamics of (5)._

The proof is in Appendix C. The intuition behind this result becomes clearer once we examine the differential of the loss w.r.t. the function outputs, \(_{}}}\). Notice that, as \(}\) is a convex function, it has a unique stationary point, and hence it is natural to interpret \(_{}}}()\) as the error at \(\), especially close to the global minimum. We will therefore adopt the notation

\[():=_{}}}( {})\] (10)

here and throughout the proofs to refer to the deviation from the global minimum at the current parameter value. A key ingredient of the proof of Theorem 4.3 will be to establish that, for trajectories induced by GGN or the update in equation (8), \((t):=((t))\) satisfies:

\[}{dt}=-(t)\] (11)

This trivially implies that \( 0\) from any initial condition \(_{0}\), so that the evolution of the weights approaches a stationary point for the loss, and hence its global minimum.

The right inverse of the Jacobian, \(J^{+}\) is non-unique, and, in general, it is not feasible to compute for large models. However, it turns out that in the case of reversible models, we have an analytic expression for \(J^{+}\), which allows computing exact GN at nearly the same cost as SGD.

[MISSING_PAGE_FAIL:6]

The last factor requires pseudo-inverting a matrix of size \(n d/2\), which requires \((nd(n,d))\). Since these operations are required in each layer, the overall cost of the update for the full network is \((Lnd^{2}+Ln^{2}d)\), compared to \((Lnd^{2})\) of SGD, while the memory complexity is the same.

## 5 Experiments

For our experiments we train RevMLPs (equations (14) and (15)) with 2 (6) blocks for MNIST  (CIFAR-10; Krizhevsky, 2009), ReLU non-linearities at all half-coupled layers, and an inverted bottleneck of size 8000 resulting in models with 12M (MNIST) and 147M (CIFAR-10) parameters. We train these models to classify images flattened to 1D vectors, using a cross-entropy objective. Note that the chosen size of the inverted bottleneck ensures that the assumptions of Proposition 4.4 hold.

At each training iteration, we compute the pseudoinverses in equations (16), (17) using an SVD. For numerical stability we truncate the SVD to a 1% tolerance relative to the largest singular value and an absolute tolerance of \(10^{-5}\), whichever gives the smallest rank - our main findings are qualitatively robust to these tolerance levels. Full hyperparameters and additional details are reported in Appendix L, and code is provided with the submission. We report results averaged over 3 random seeds. All experiments are performed on a single NVIDIA RTXA6000 GPU.

Figure 1: Training loss and accuracy on (a) MNIST and (b) CIFAR-10 in a full-batch scenario where each dataset is trimmed to a fixed subset of \(n=1024\) images. GN converges much faster than Adam and SGD.

Figure 3: Percentage change in training loss after each update. GN decreases the loss for the current mini-batch more than SGD and Adam early in training.

Figure 2: Training loss, test loss, and test accuracy on (a) MNIST and (b) CIFAR-10 in a mini-batch scenario. GN does not exhibit the same properties observed in the full-batch setting. In fact, Adam reaches lower training and test loss.

[MISSING_PAGE_FAIL:8]

Feature Learning with Gauss-Newton.Even if features (i.e., the NTK) change during GN training, it remains unclear whether they are associated with changes in neural representations. We examine the change in neural representations during training by computing the Centered Kernel Alignment (CKA; Kornblith et al., 2019) measure of similarity between the representations at initialization and those learned at each epoch, across all layers of the model.

Figures 3(c), 3(d) and 3(e) illustrate the evolution of CKA similarities for the last, middle and first block (i.e., a "full" coupling layer as described by equations 14, 15) of a 12 layer RevMLP trained on CIFAR-10. Plots for all blocks are provided in Appendix H along with pairwise similarities across optimizers (Figures 10 and 11). Similar to the NTK, neural representations do not change during training with GN. SGD behaves similarly, with little change in CKA. Adam, on the contrary, has changes in neural representations that coincide with changes in NTK. Appendix G shows that the lack of changes in neural representations for GN cannot be explained by a smaller change in parameters, in fact both GN and Adam show changes in weight space, while weights of SGD change little (Figure 9).

Contrary to the findings of Arbel et al. (2023), it is evident that the CKA similarities in Figures 3(c), 3(d) and 3(e) remain higher for longer in earlier blocks for GN, implying that GN is slower than Adam at adapting its deeper internal representations. Furthermore, in Appendix F and I, we address two suggestions from Arbel et al. (2023) and find that the generalization improvements when using smaller learning rates and/or different initializations (close-to-optimal in Figure 7 and low variance in Figure 8) do not carry over to deeper networks. In particular, Figure 7 shows that continuing training with GN after initially training with Adam exhibits the same phenomena as training with GN throughout - albeit at a slightly lower loss than can be achieved using only GN.

### Experiments without Inverted Bottleneck

The previous experiments used inverted bottlenecks to ensure "linear independence", i.e., to ensure that the model is adequately overparametrized such that the proposed efficient generalized inverse is valid. In other words, inverted bottlenecks ensure that the scalable GN weight updates (equations 16) do implement gradient flow in function space (equation 3), such that our results are not potentially confounded by broken theoretical assumptions. Nevertheless, the proposed GN update can still be applied in the absence of inverted bottlenecks. In Figure 5 we report results on the CIFAR10 dataset, following the same experimental procedure of the previous experiments, but removing all inverted bottlenecks. In the full-batch setting, GN is still performing much better than Adam and SGD. In the mini-batch setting we observe a very similar trend to what is observed in the previous experiments: GN leads to an early saturation of the loss, which instead does not appear in Adam and SGD.

### Regression Experiments

We further performed some experiments on regression tasks from the UCI regression dataset2. In more detail, we used the Superconductivity Hamidieh (2018) and Wine Aeberhard and Forina (1992) datasets, and followed the same experimental procedure used for the classification datasets (i.e., we use the same RevMLP architecture with an inverted bottleneck for all optimizers and select the highest learning rate that does not cause the loss to diverge). Results are shown in Appendix M and follow the same trend observed in the classification experiments: in the full-batch case GN is

Figure 5: Experiments on CIFAR using a model without Inverted Bottleneck (Full-batch on the left, mini-batch on center and right). While the theoretical guarantees do not hold in this setting, the results follow the same trend observed in Figure 2.

[MISSING_PAGE_FAIL:10]

M. Arbel, R. Menegaux, and P. Wolinski. Rethinking Gauss-Newton for learning over-parameterized models. _Advances in Neural Information Processing Systems_, 37, 2023.
* Ba et al.  J. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using kronecker-factored approximations. In _International Conference on Learning Representations_, 2017.
* Bachmann et al.  G. Bachmann, S. Anagnostidis, and T. Hofmann. Scaling MLPs: A tale of inductive bias. _Advances in Neural Information Processing Systems_, 36, 2024.
* Bae et al.  J. Bae, P. Vicol, J. Z. HaoChen, and R. B. Grosse. Amortized proximal optimization. _Advances in Neural Information Processing Systems_, 35:8982-8997, 2022.
* Behrmann et al.  J. Behrmann, W. Grathwohl, R. T. Chen, D. Duvenaud, and J.-H. Jacobsen. Invertible residual networks. In _International conference on machine learning_, pages 573-582. PMLR, 2019.
* Ben-Israel  A. Ben-Israel. A modified Newton-Raphson method for the solution of systems of equations. _Israel journal of mathematics_, 3:94-98, 1965.
* Bernacchia et al.  A. Bernacchia, M. Lengyel, and G. Hennequin. Exact natural gradient in deep linear networks and its application to the nonlinear case. _Advances in Neural Information Processing Systems_, 31, 2018.
* Bjorck  A. Bjorck. _Numerical methods for least squares problems_. SIAM, 1996.
* Botev et al.  A. Botev, H. Ritter, and D. Barber. Practical Gauss-Newton optimisation for deep learning. In _International Conference on Machine Learning_, pages 557-565, 2017.
* Bottou et al.  L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. _SIAM Review_, 60(2):223-311, 2018.
* Cai et al.  T. Cai, R. Gao, J. Hou, S. Chen, D. Wang, D. He, Z. Zhang, and L. Wang. Gram-gauss-newton method: Learning overparameterized neural networks for regression problems. 2019.
* Chang et al.  B. Chang, L. Meng, E. Haber, L. Ruthotto, D. Begert, and E. Holtham. Reversible architectures for arbitrarily deep residual neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Charles and Papailiopoulos  Z. Charles and D. Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In _International conference on machine learning_, pages 745-754. PMLR, 2018.
* Chizat et al.  L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. _Advances in Neural Information Processing Systems_, 32, 2019.
* Dennis Jr and Schnabel  J. E. Dennis Jr and R. B. Schnabel. _Numerical methods for unconstrained optimization and nonlinear equations_. SIAM, 1996.
* Dinh et al.  L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation. _ICLR 2015 Workshop Track_, 2015.
* Dinh et al.  L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using Real NVP. _International Conference on Learning Representations_, 2016.
* Finzi et al.  M. Finzi, P. Izmailov, W. Maddox, P. Kirichenko, and A. G. Wilson. Invertible convolutional networks. In _Workshop on Invertible Neural Nets and Normalizing Flows, International Conference on Machine Learning_, volume 2, 2019.
* Fort et al.  S. Fort, G. K. Dziugaite, M. Paul, S. Kharaghani, D. M. Roy, and S. Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. _Advances in Neural Information Processing Systems_, 33:5850-5861, 2020.
* Garcia et al.  J. R. Garcia, F. Freddi, S. Fotiadis, M. Li, S. Vakili, A. Bernacchia, and G. Hennequin. Fisher-Legendre (FishLeg) optimization of deep neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* George et al.  T. George, C. Laurent, X. Bouthillier, N. Ballas, and P. Vincent. Fast approximate natural gradient descent in a Kronecker factored eigenbasis. _Advances in Neural Information Processing Systems_, 31, 2018.
* Glorot et al. X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. _AISTATS_, 2010.
* Goldfarb et al.  D. Goldfarb, Y. Ren, and A. Bahamou. Practical quasi-newton methods for training deep neural networks. _Advances in Neural Information Processing Systems_, 33:2386-2396, 2020.
* Gomez et al.  A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. The reversible residual network: Backpropagation without storing activations. _Advances in neural information processing systems_, 30, 2017.
* Hamidieh  K. Hamidieh. Superconductivty Data. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C53P47.
* Hoogeboom et al.  E. Hoogeboom, R. Van Den Berg, and M. Welling. Emerging convolutions for generative normalizing flows. In _International conference on machine learning_, pages 2771-2780. PMLR, 2019.
* Huang et al.  C.-W. Huang, R. T. Chen, C. Tsirigotis, and A. Courville. Convex potential flows: Universal probability distributions with optimal transport and convex optimization. _International Conference on Learning Representations_, 2020.
* Huh  D. Huh. Curvature-corrected learning dynamics in deep neural networks. _ICML_, page 9, 2020.
* Iserles  A. Iserles. _A first course in the numerical analysis of differential equations_. Cambridge University Press.
* Jacobsen et al.  J.-H. Jacobsen, A. W. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. _International Conference on Learning Representations_, 2018.
* Jacot et al.  A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Karakida and Osawa  R. Karakida and K. Osawa. Understanding approximate Fisher information for fast convergence of natural gradient descent in wide neural networks. _Advances in neural information processing systems_, 33:10891-10901, 2020.
* Keller et al.  T. A. Keller, J. W. Peters, P. Jaini, E. Hoogeboom, P. Forre, and M. Welling. Self normalizing flows. In _International Conference on Machine Learning_, pages 5378-5387. PMLR, 2021.
* Kerekes et al.  A. Kerekes, A. Meszaros, and F. Huszar. Depth Without the Magic: Inductive Bias of Natural Gradient Descent. _arXiv:2111.11542_, 2021.
* Kingma and Dhariwal  D. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. _Advances in neural information processing systems_, 31, 2018.
* Kornblith et al.  S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In _International conference on machine learning_, pages 3519-3529. PMLR, 2019.
* Krizhevsky  A. Krizhevsky. Learning multiple layers of features from tiny images. pages 32-33, 2009.
* LeCun et al.  Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* Liu et al.  C. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 59:85-116, 2022.
* Lu and Huang  Y. Lu and B. Huang. Woodbury transformations for deep generative flows. _Advances in Neural Information Processing Systems_, 33:5801-5811, 2020.
* MacKay et al.  M. MacKay, P. Vicol, J. Ba, and R. B. Grosse. Reversible recurrent neural networks. _Advances in Neural Information Processing Systems_, 31, 2018.
* Mangalam et al.  K. Mangalam, H. Fan, Y. Li, C.-Y. Wu, B. Xiong, C. Feichtenhofer, and J. Malik. Reversible vision transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10830-10840, 2022.
* Moser et al. J. Martens. New insights and perspectives on the natural gradient method. _The Journal of Machine Learning Research_, 21(1):5776-5851, 2020.
* Martens and Grosse (2015) J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* Martens et al. (2010) J. Martens et al. Deep learning via Hessian-free optimization. In _ICML_, volume 27, pages 735-742, 2010.
* Meulemans et al. (2020) A. Meulemans, F. Carzaniga, J. Suykens, J. Sacramento, and B. F. Grewe. A theoretical framework for target propagation. _Advances in Neural Information Processing Systems_, 33:20024-20036, 2020.
* Nguyen et al. (2021) Q. Nguyen, M. Mondelli, and G. F. Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks. In _International Conference on Machine Learning_, pages 8119-8129. PMLR, 2021.
* Nocedal and Wright (1999) J. Nocedal and S. J. Wright. _Numerical optimization_. Springer, 1999.
* Ortega and Rheinboldt (2000) J. M. Ortega and W. C. Rheinboldt. _Iterative solution of nonlinear equations in several variables_. SIAM, 2000.
* Papamakarios et al. (2017) G. Papamakarios, T. Pavlakou, and I. Murray. Masked autoregressive flow for density estimation. _Advances in neural information processing systems_, 30, 2017.
* Paszke et al. (2019) A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library, 2019.
* Petersen et al. (2023) F. Petersen, T. Sutter, C. Borgelt, D. Huh, H. Kuehne, Y. Sun, and O. Deussen. ISAAC Newton: Input-based approximate curvature for Newton's method. In _The Eleventh International Conference on Learning Representations_, 2023.
* Rudner et al. (2019) T. G. Rudner, F. Wenzel, Y. W. Teh, and Y. Gal. The natural neural tangent kernel: Neural network training dynamics under natural gradient descent. In _4th workshop on Bayesian Deep Learning (NeurIPS 2019)_, 2019.
* Saxe et al. (2013) A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_, 2013.
* Schraudolph (2002) N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. _Neural computation_, 14(7):1723-1738, 2002.
* Song et al. (2019) Y. Song, C. Meng, and S. Ermon. Mintnet: Building invertible neural networks with masked convolutions. _Advances in Neural Information Processing Systems_, 32, 2019.
* Teng and Choromanska (2019) Y. Teng and A. Choromanska. Invertible autoencoder for domain adaptation. _Computation_, 7(2):20, 2019.
* Wadia et al. (2021) N. Wadia, D. Duckworth, S. S. Schoenholz, E. Dyer, and J. Sohl-Dickstein. Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization. In _International Conference on Machine Learning_, pages 10617-10629. PMLR, 2021.
* Wilson et al. (2017) A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive gradient methods in machine learning. _Advances in neural information processing systems_, 30, 2017.
* Xiao and Liu (2020) C. Xiao and L. Liu. Generative flows with matrix exponential. In _International Conference on Machine Learning_, pages 10452-10461. PMLR, 2020.
* Yang and Hu (2021) G. Yang and E. J. Hu. Feature learning in infinite-width neural networks. In _International Conference on Machine Learning_, pages 11727-11737. PMLR, 2021.
* Zhang et al. (2019) G. Zhang, J. Martens, and R. B. Grosse. Fast convergence of natural gradient descent for over-parameterized neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.

## Appendix A Functional view of Generalized Gauss-Newton

Assuming that \(}\) is strongly convex, Newton's flow in function space is equal to

\[}{dt}=-H^{-1}\ _{}}\] (18)

with \(H=_{}}^{2}}\). Following steps similar to Section 3, we use \(}{dt}=J}{dt}\) and pseudo-invert the Jacobian. In discrete time, with learning rate \(\), the functional view of Generalized Gauss Newton is given by

\[_{t+1}=_{t}-\ J^{+}H^{-1}_{ }}\] (19)

It is straightforward to show that equations (19) and (7) are identical when the Jacobian has linearly independent rows. Under these assumptions, \((J^{T}HJ)^{+}=J^{+}H^{-1}J^{T+}\) and \(J^{T+}J^{T}=I\). Furthermore, as in Section 3, \(_{}=J^{T}_{}}\).

## Appendix B Convergence of GGN flow

In this Section, we give an informal derivation of the continuous-time dynamics of GGN. See Ortega and Rheinboldt (2000); Bottou et al. (2018) for convergence of GGN in discrete time. We consider the optimization of the error under GGN flow in continuous time. Using the definition of the error \(=_{}}\) (equation (10)), the optimization flow of the error can be derived using the chain rule

\[}{dt}=HJ}{dt}\] (20)

The definition of GGN flow is

\[}{dt}=-\ (J^{T}HJ)^{+}_{}=-\ (J^{T}HJ)^{+}J^{T}\] (21)

Therefore, optimization of the error under GGN flow is equal to

\[}{dt}=-\ HJ(J^{T}HJ)^{+}J^{T}=-\ A\] (22)

where we defined the matrix \(A=HJ(J^{T}HJ)^{+}J^{T}\). Using the properties of the matrix pseudoinverse, we note that \(A\) is a projection operator, namely \(A^{n}=A\) for any integer power \(n\). Therefore, eigenvalues of \(A\) are either zero or one, implying that the error decreases exponentially in the range of \(A\), while it remains constant in the null space of \(A\). In general, the range and null space of \(A\) change during training. We note that the projection is orthogonal with respect to the inner product \(^{T}H\). Using the same steps as in Appendix A, if \(J\) has linearly independent rows and \(}\) is strongly convex, then it is straightforward to show that \(A=_{nd_{y}}\).

## Appendix C Proof of Theorem 4.3

During the proof we will compare the dynamics of the flow curves of two vector fields, namely \(-J^{+}()()\) and the corresponding \(-J^{+}()()\). The dependence on \(\) is assumed throughout and we will write \((-J^{+})_{}\) or just \(-J^{+}\). The flowlines of these vector fields are given by:

\[}{dt}=(-J^{+})_{ (t)}\] (23)

and

\[}{dt}=(-J^{+})_{ (t)}\] (24)

respectively. We will further write plain \((t)\) for the solutions of equation (23) and \(}(t)\) for the solutions of equation (24). Moreover, we'll write \(}\) to mean \((}(t))\) for arbitrary (possibly tensor valued) functions \(\), to distinguish from \(((t))\).

First notice that the right inverses \(J^{}\) are not canonical and hence equation (24) represents a family of equations and associated flowlines. However, the dynamics of the associated error \(}\) is the same regardless of the choice, and moreover, the same as that of \(\) itself. Note that \(((t))=J_{(t)}}{dt}\), which gives:

\[}{dt}=-JJ^{+}\] (25)

If \(J\) has linearly independent rows we have \(JJ^{+}=\), therefore

\[}{dt}=-\] (26)

If we consider the flow of \(}:=(}(t))\), replacing \(J^{+}\) with a right inverse \(J^{}\) satisfies \(JJ^{}=\), and again we obtain

\[}}{dt}=-}\] (27)

Integrating these equations from the same initial condition \(}(t_{0})=(t_{0})\) indeed gives the same error flowlines.

So despite the different dynamics of the \(\) and \(}\), errors propagate identically. We can use this to derive a bound between the errors incurred by the \(k\)-th forward Euler iterates of equations (23) and (24), which define the gradient descent equations. Denote by \(_{i}\) the \(i\)-th Euler iterate of \((t)\) and by \(}_{i}\), the corresponding iterate of \(}(t)\). Then, we have:

\[\|_{i}-(t_{i})\| (e^{L(t_{i}-t_{0})}-1)\] (28) \[\|}_{i}-}(t_{i})\| }(e^{(t_{i}-t_{0})}-1)\] (29)

where \(\) is the step-size, \(i\), the number of steps can be computed as \(i=-t_{0}}{}\), \(L\) and \(\) are the Lipschitz constants of \((-J^{+})\) and \((-J^{-})\) respectively, and \(K\) and \(\) are constants which depend on the maximum norm of \(}{dt^{2}}(t)\) and \(}{dt^{2}}(t)\) across our domain, see e.g. Iserles. Since \(\) is at least \(^{2}\) and the dynamics of \(\) and \(}\) take place over a bounded domain, \(\) is Lipschitz with constant \(L_{}\). Then we have:

\[\|(_{i})-(}_{i})\| \|(_{i})-((t_ {i}))\|+\|((t_{i}))-( }(t_{i}))\|+\|(}(t_{i}))-(}_{i})\|\] (30) \[ L_{}\|_{i}-(t_{i})\|+0+ L_{}\|}_{i}-}(t_{i})\|\] (31) \[ L_{}}(e^{(t _{i}-t_{0})}-1)\] (32)

where \(=(K,)\) and \(=(L,)\). This shows that the dynamics of the gradient descent iterates coincides up to first order in \(\).

## Appendix D Proof of Proposition 4.4

Proof.: We rewrite the layer-wise Jacobian as

\[J_{} =_{L}}{_{}}_{}}{_{}}=_{L}}{(_{}^{(1)},_{}^{(2)})}_{}^{(1)},_{}^{(2)})}{(_ {}^{(1)},_{}^{(2)})}\] (33) \[=(_{L}}{_{}^ {(1)}},_{L}}{_{}^{(2)}}) ((V_{-1}^{(2)}X_{-1}^{(2)})^{T} _{d/2}&0\\ _{}^{(2)}}{_{}^{(1)}}& (V_{}^{(1)}X_{}^{(1)})^{T}_{d/2})\] (34) \[=(_{L}}{_{}^{( 1)}},_{L}}{_{}^{(2)}}) (_{-1}^{(2)}{}^{T}_{d/2}&0\\ _{}^{(2)}}{_{}^{(1)}}& _{}^{(1)}{}^{T}_{d/2})\] (35)

[MISSING_PAGE_EMPTY:16]

## Appendix E Longer training curves

Figure 6 displays the result of training the same RevMLP as in Section 5.2 for 1000 epochs on the CIFAR-10 dataset in a mini-batch setting (\(n=1024\)). We observe that by continuing the training for longer on CIFAR-10, SGD is able to reach lower values of the training loss when compared to Gauss-Newton. In particular, we highlight that even in 1000 epochs, Gauss-Newton appears unable to increase its training performance further than the value it reaches after just 50 epochs. In fact, the results in Figure 6 show that Gauss-Newton tends to increase its training loss after 150 epochs of training.

## Appendix F Initialization dependencies for Gauss-Newton

To examine if the poor performance of Gauss-Newton depends on a poor initialization point, we first train a model with Adam for 50 epochs, before continuing the training with Gauss-Newton. For comparison, we also train a model with Gauss-Newton for 50 epochs and subsequently continue training with Adam - to observe if Gauss-Newton reaches reaches a "bad" local minimum that is hard to escape from. These results are provided in Figure 7 and compared with their single optimizer counterparts. We choose a "good" initialization point as an Adam trained model at 50 epochs (indicated by the dashed line in Figure 7), which has a lower training loss than GN can achieve in the same (or larger) number of iterations. One can observe that, even when starting from this "good" initialization, GN eventually saturates at a higher value of the loss when compared with the values achievable by continuing training with Adam. We also find that Adam can start from a point found by GN and continue training without issues, reaching a value of the loss that is lower than the saturation point of GN.

In reference to Arbel et al. (2023), we also provide additional results in Figure 8 to show the dependency of Gauss-Newton on the initial weight variance chosen. Interestingly, our results are different from those in Arbel et al. (2023) and suggest that choosing a higher variance is preferable. However, all curves exhibit the same phenomena as discussed in Section 5 and under-perform with respect to Adam. The default choice for all experiments we report is \(=10^{-3}\).

## Appendix G Change in weights during training

We analyze the change in norm and cosine similarity between the weights at initialization and at the end of training when a model is trained with different optimizers. Results are shown in Figure 9. We

Figure 6: Training loss, test loss, and test accuracy when training for a longer amount of epochs on CIFAR-10 shows that Gauss-Newton is unable to further decrease the training loss, while even plain SGD can reach lower values.

Figure 7: Training loss when first using Adam (or GN), and then continuing with GN (or Adam) – the purple dashed line indicates the 50 epochs mark at which the optimizers are switched. GN shows early saturation of the loss even when starting from a better initialization point.

Figure 8: GN Train and test loss with weights initialized accounting to different variances at initialization \(\).

Figure 9: Cosine similarity with the initial weight initialization across training. ADAM and GN move similarly in weight space indicating a consistent behaviour in weight space between the two optimizers. Note that, in this Figure, we use the term “layer” to refer to half-coupled layer in the reversible blocks.

observe that Gauss-Newton changes the weights to an extent similar to Adam, while SGD show much smaller weight changes, suggesting a _lazy training_ regime.

## Appendix H Extended Centered Kernel Alignment (CKA) analysis

In Figure 10 we present the full spectrum of CKA similarities from initialization across blocks (i.e., full coupling layer) when training on CIFAR-10. We observe that GN behaves very much SGD: the representqations remain very similar to those at initialization. Adam instead tends to change the representations during training, more quickly for later blocks, and more slowly for earlier blocks.

In addition to the CKA evolution results for single optimizers, Figure 11 presents the pairwise similarities between models at each epoch trained with different optimization strategies. These results demonstrate explicitly the close correspondence between SGD and GN learned features for each block.

## Appendix I Learning rate variations of Gauss-Newton

Figure 12 provides additional training runs of Gauss-Newton with different learning rates. These results indicate that forcing GN to learn slower is not sufficient to reduce the effect of the observed saturation of performance.

Figure 11: Pairwise CKA similarity evolution across training between GN and models trained with Adam and SGD.

Figure 10: CKA similarity evolution across training for GN, Adam and SGD. GN maintains a high CKA similarity with its initial feature space, very similarly to SGD.

## Appendix J Adding Regularization to Gauss-Newton

In this Section we explore whether weight-decay can be used to improve the performance of Gauss-Newton. We use a RevMLP on the full CIFAR-10 dataset with the same setting presented in Section 5 and we add weight decay to the loss during training. We tune the strength of the weight decay using the validation set. Results are shown in Figure 13. We notice that weight decay has a minimal effect of the performance of Gauss-Newton.

## Appendix K Pseudo-Inverse Regularization

In this Section we explore the effects of different strategies for regularizing the pseudoinverse in the proposed Gauss-Newton update (see equations (16), (17)). We note that regularization is necessary, as the presence of very small singular values causes numerical instabilities. We compute the pseudoinverse using a singular value decomposition, and we try three different strategies:

* Damping: we add a constant to all the singular values. In particular we add a quantity equal to \(1\%\) of the maximum singular value (this quantity of damping was tuned by selecting the best performing one over the values \(1\%,10\%,0.1\%\)).
* Truncation: we set to zero all the singular values smaller than a certain threshold. In particular, we use relative tolerance of 1% with respect to the largest singular value and an absolute tolerance of \(10^{-5}\) (we tune this values in a similar fashion to the previous method). This is the strategy used for the results in Section 5.
* Noise: we add noise to the matrix to be pseudoinverted; we then compute the SVD and use all singular values. The noise is sampled from a zero-mean Gaussian with a standard deviation equal to \(10\%\) (this value was selected though a tuning procedure as above) of the standard deviation of the matrix to be pseudoinverted.

Results are shown in Figure 14, where Adam is also added for comparison. We notice that there is a small difference between damping and truncating (with the former performing slightly better), while adding noise does not seem as effective. Nevertheless, Gauss-Newton is always under-performing when compared to Adam.

## Appendix L Full Hyperparameters & Experimental Details

In this Section we provide additional details on the hyperparameters and experimental details used for our experiments. Full code to reproduce our results is also provided with the submission.

Figure 12: Train loss, test loss and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton using different learning rates.

Figure 13: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton with weight-decay. Adam is also added for comparison.

Implementation details.Our code is based on the PyTorch framework Paszke et al. (2019). In more detail we use version 2.0 for Linux with CUDA 12.1.

Weight initialization.We use standard Xavier Glorot and Bengio (2010) initialization for the weights, while we initialize the biases to zero.

Sampling the inverted bottleneck.We sample the entries of each inverted bottleneck from a zero-centered gaussian with a variance of \(}\).

Data augmentations.For the MNIST dataset we do not use any data augmentations. For the CIFAR-10 dataset we follow the standard practice of applying random crops and resizes. We do not use data augmentations for the regression datasets.

Additional hyperparameters for Adam.We tune the learning rate for each experiment and method, as explained in Section 5, and we use the PyTorch default values for the _betas_ parameters in Adam.

## Appendix M Regression Results

We report the results for the regression experiments in Figure 15.

Figure 14: Train loss, test loss, and test accuracy (left to right) for a RevMLP trained on CIFAR-10 with Gauss-Newton using different regularization strategies for the pseudoinverse. Adam is also added for comparison.

Figure 15: Train loss and test loss on UCI (a) wine and (b) superconductivity regression datasets. (Full-batch on top, mini-batch on bottom).

Proof Layer-wise Right-inverse is the Moore-Penrose Pseudo-inverse

Consider the Jacobian for layer \(\):

\[J_{}=}{ x_{}^{ (1)}}&}{ x_{}^{(2)}} A&0\\ B&C=\] (46) \[}{ x_{}^{(1)}}& }{ x_{}^{(2)}} _{}V_{-1}^{(2)}X_{-1}^{(2)}^{T} _{d/2}&\\ ^{(2)}}{ w_{}^{(1)}}&_{} V_{}^{(1)}X_{}^{(1)}^{T}_{d/2}\] (47)

Denote \(J_{,1}=A&0\\ B&C\). For our method, we used

\[J_{,1}^{+} =A^{+}&0\\ -C^{+}BA^{+}&C^{+}\] (48) \[A^{+} =_{}V_{-1}^{(2)}X_{-1}^{(2)} ^{T+}_{d/2},\] (49) \[B =^{(2)}}{ w_{}^{(1)}},\] (50) \[C^{+} =_{}V_{}^{(1)}X_{}^{(1)} ^{T+}_{d/2}\] (51)

Then:

\[J_{,1}^{+}J_{,1}=A^{+}A&0\\ C^{+}B-C^{+}BA^{+}A&C^{+}C\] (52)

We show below that \(B=BA^{+}A\), and hence that \(J_{,1}^{+}=J_{,1}^{+}\) (i.e., our right-inverse corresponds to the Moore-Penrose Pseudo-inverse).

We can show that (see Section N.1):

\[B_{i+(j-1),a+(b-1)} =^{(2)}}{ w_{}^ {(1)}}_{i+(j-1),a+(b-1)}\] (53) \[=_{}V_{-1}^{(2)}X_{-1}^{(2)} _{j,b}^{T}_{k}W_{,G}_{i,k} V_{}^{(1)}_{k,a}_{}^{} V_{}^{(1)}X_{}^{(1)}_{k,j}.\] (54)

With the above, we first compute \((BA^{+})\):

\[(BA^{+})_{i_{1}+(j_{1}-1),i_{4}+(j_{4}-1)}\] (55) \[=_{i_{2},j_{2}}(B)_{i_{1}+(j_{1}-1),i_{2}+ {2}(j_{2}-1)}(A^{+})_{i_{2}+(j_{2}-1),i_{4}+(j_{4}-1)}\] (56) \[=_{i_{2},j_{2}}(B)_{i_{1}+(j_{1}-1),i_{2}+ {2}(j_{2}-1)}_{}V_{-1}^{(2)}X_{-1}^{(2)} _{j_{2},j_{4}}^{T+}(i_{2}=i_{4})\] (57) \[=_{j_{2},k}_{}V_{-1}^{(2)}X_{ -1}^{(2)}_{j_{1},j_{2}}^{T}(W_{,G})_{i_{1},k} V_{}^{(1)}_{k,i_{4}}_{}^{} V_{}^{(1)}X_{}^{(1)}_{k,j_{1}}_{ }V_{-1}^{(2)}X_{-1}^{(2)}_{j_{2},j_{4}}^{T+}\] (58) \[=_{k}W_{,G}_{i_{1},k} V_{}^{(1)}_{k,i_{4}}_{}^{}V_{ }^{(1)}X_{}^{(1)}_{k,j_{1}}.\] \[_{j_{2}}_{}V_{-1}^{(2) }X_{-1}^{(2)}_{j_{1},j_{2}}^{T}_{}V_{ -1}^{(2)}X_{-1}^{(2)}_{j_{2},j_{4}}^{T+}}\] (59) \[=_{k}W_{,G}_{i_{1},k} V_{}^{(1)}_{k,i_{4}}_{}^{}V_{ }^{(1)}X_{}^{(1)}_{k,j_{1}}_{} V_{-1}^{(2)}X_{-1}^{(2)}^{T}_{} V_{-1}^{(2)}X_{-1}^{(2)}^{T+}_{j_{1}, j_{4}}\] (60)and finally

\[(BA^{+}A)_{i_{1}+(j_{1}-1),i_{3}+(j_{3}-1)}\] (61) \[=_{i_{4},j_{4}}(BA^{+})_{i_{1}+(j_{1}-1),i_{4}+ (j_{4}-1)}A_{i_{4}+(j_{4}-1),i_{3}+(j_{3}-1)}\] (62) \[=_{i_{4},j_{4}}(BA^{+})_{i_{1}+(j_{1}-1),i_{4}+ (j_{4}-1)}_{}(V_{-1}^{(2)}X_{-1}^{(2)}) _{j_{4},j_{3}}^{T}(i_{4}=i_{3})\] (63) \[=_{j_{4},k}(W_{,G})_{i_{1},k}(V_{ }^{(1)})_{k,i_{3}}_{}^{}(V_{}^{(1)}X_{}^{( 1)})_{k,j_{1}}\] \[(_{}(V_{-1}^{(2)}X_{-1}^{ (2)})^{T}_{}(V_{-1}^{(2)}X_{-1}^{(2)})^{T+ })_{j_{1},j_{4}}_{}(V_{-1}^{(2)}X_{-1}^{(2)} )_{j_{4},j_{3}}^{T}\] (64) \[_{j_{4}}(_{}(V_{ -1}^{(2)}X_{-1}^{(2)})^{T}_{}(V_{-1}^{(2)}X_{ -1}^{(2)})^{T+})_{j_{1},j_{4}}_{}(V_{-1}^{(2 )}X_{-1}^{(2)})_{j_{4},j_{3}}^{T}}\] (65) \[(_{}(V_{-1}^{(2)}X_{-1}^{ (2)})^{T}_{}(V_{-1}^{(2)}X_{-1}^{(2)})^{T+ }_{}(V_{-1}^{(2)}X_{-1}^{(2)})^{T}_{j_{1},j_{3}}\] (66) \[=_{k}(W_{,G})_{i_{1},k}(V_{}^{(1 )})_{k,i_{3}}_{}^{}(V_{}^{(1)}X_{}^{(1)} )_{k,j_{1}}_{}(V_{-1}^{(2)}X_{-1}^{(2)})_{j _{1},j_{3}}^{T}\] (67) \[=B_{i_{1}+(j_{1}-1),i_{3}+(j_{3}-1)}.\] (68)

### Expression for \(B\)

\[B_{i+(j-1),a+(b-1)} =(^{(2)}}{ w_{}^{(1)}} )_{i+(j-1),a+(b-1)}\] \[=(^{(2)})_{i+(j-1)}}{ (w_{}^{(1)})_{a+(b-1)}})\] \[=(^{(2)})_{i,j}}{(W_{}^{( 1)})_{a,b}})\] \[=_{}(V_{}^{(1)}X_{ }^{(1)}))_{i,j}}{(W_{}^{(1)})_{a,b}}\] \[=_{k}(W_{,G})_{i,k}(V_{}^{(1)}X_{}^{(1)})_{k,j}}{(W_{}^{(1)})_{a,b}}\] \[=_{k}(W_{,G})_{i,k}_{}^{}( V_{}^{(1)}X_{}^{(1)})_{k,j}^{(1)}X_{ }^{(1)})_{k,j}}{(W_{}^{(1)})_{a,b}}\] \[=_{k,t}(W_{,G})_{i,k}_{}^{} (V_{}^{(1)}X_{}^{(1)})_{k,j}(V_{}^{(1)})_{k,t }^{(1)})_{t,j}}{(W_{}^{(1)})_{a,b}}\]\[=_{k,t}(W_{,G})_{i,k}_{}^{}(V_{ }^{(1)}X_{}^{(1)})_{k,j}(V_{}^{(1)})_{k,t}^{(1)}_{}(V_{-1}^{(2)}X_{-1}^{(2) }))_{t,j}}{(W_{}^{(1)})_{a,b}}\] \[=_{k,t,s}(W_{,G})_{i,k}_{}^{} (V_{}^{(1)}X_{}^{(1)})_{k,j}(V_{}^{(1)})_{k,t}_{}(V_{-1}^{(2)}X_{-1}^{(2)})_{s,j}^{(1)})_{t,s}}{(W_{}^{(1)})_{a,b}}\] \[=_{k}(W_{,G})_{i,k}_{}^{} (V_{}^{(1)}X_{}^{(1)})_{k,j}(V_{}^{(1)})_{k,a}_{}(V_{-1}^{(2)}X_{-1}^{(2)})_{b,j}\] \[=_{}(V_{-1}^{(2)}X_{-1}^{(2)})_{j,b }^{T}_{k}(W_{,G})_{i,k}(V_{}^{(1)})_{k,a} _{}^{}(V_{}^{(1)}X_{}^{(1)})_{k,j}.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims made in the abstract and introduction are reflected in the paper. We show that GN updates computed with any generalized inverse of the model Jacobian results in the same dynamics of the loss, and we introduce a tractable form of the Gauss-Newton update for reversible neural networks in Section 4. We then study its behaviour showing poor generalization in Section 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide full assumptions and proofs for all the theoretical results introduced in the paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all details about architecture, datasets, and hyperparameters in the main text and appendix. Code is also provided with the submission. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide code with instructions to replicate our results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes we provide all information regarding the training details and test details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Yes our results are averaged over multiple seeds, and we include error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In the experimental Section we provide a description of our computational setting (single NVIDIA RTXA6000 GPU). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper adheres to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: the paper has mainly theoretical motivations and outcomes, so there is no direct societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: we provide a reference for the datasets used in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.