# Regression under demographic parity constraints via unlabeled post-processing

Gayane Taturyan

IRT SystemX, Universite Gustave Eiffel,

Universite Paul-Sabatier

gayane.taturyan@univ-eiffel.fr

Evgenii Chzhen

CNRS, Universite Paris-Saclay

evgenii.chzhen@cnrs.fr

Mohamed Hebiri

Universite Gustave Eiffel

mohamed.hebiri@univ-eiffel.fr

###### Abstract

We address the problem of performing regression while ensuring demographic parity, even without access to sensitive attributes during inference. We present a general-purpose post-processing algorithm that, using accurate estimates of the regression function and a sensitive attribute predictor, generates predictions that meet the demographic parity constraint. Our method involves discretization and stochastic minimization of a smooth convex function. It is suitable for online post-processing and multi-class classification tasks only involving unlabeled data for the post-processing. Unlike prior methods, our approach is fully theory-driven. We require precise control over the gradient norm of the convex function, and thus, we rely on more advanced techniques than standard stochastic gradient descent. Our algorithm is backed by finite-sample analysis and post-processing bounds, with experimental results validating our theoretical findings.

## 1 Introduction

Algorithmic fairness is an umbrella term for a subset of machine learning research that aims to better understand, quantify, mitigate, evaluate, and conceptualize negative and/or positive effects of data-driven algorithms on the society. At least one direction in this field falls within theoretical machine learning, where a form of fairness constraint, mainly inspired by common sense and formalized within mathematical framework, is proposed as an arguably reasonable proxy for a definition of ethical and non-discriminatory prediction. Even more particular sub-field of this research direction is formalized within a paradigm of group fairness, that aims at mitigating negative impact (or provide equal treatment to) towards sub-populations that share a common sensitive characteristic. Many works fall within this category (Barocas et al., 2018; Calders et al., 2009; Chiappa et al., 2020; Dwork et al., 2011; Feldman et al., 2015; Gordaliza et al., 2019; Hardt et al., 2016; Jiang et al., 2020; Lum and Johndrow, 2016; Zafar et al., 2017; Zemel et al., 2013, just to name a few).

Even without going into debates on the relevance of a given definition of fairness, many, purely mathematical and algorithmic questions remain unanswered in this field. The best theoretical understanding of the problem is available for the demographic parity constraint in case of _awareness_--the situation when the sensitive attribute is available at inference time (Agarwal et al., 2019; Chiappa et al., 2020; Chzhen and Schreuder, 2020; Chzhen et al., 2019; Denis et al., 2024; Gaucher et al., 2023; Le Gouic et al., 2020). The latter case is well studies both in classification and regression setups. This is no longer the case for other fairness constraints or the _unawareness_ setup--the situation when the sensitive attribute is not available at inference time. In particular, while the case ofclassification has been studied before from algorithmic and mathematical perspectives (Chzhen et al., 2019; Gaucher et al., 2023; Gordaliza et al., 2019; Hardt et al., 2016), the regression setup remains largely under explored and many methods lack strong theoretical evidences. In particular, to date, none of previous works effectively build computationally-efficient, fully theory-driven algorithm for the problem of regression under the demographic parity constraint in the case of unawareness. The present work fills this gap. Relying on previous ideas of discretization that goes back to Agarwal et al. (2019), we design a smooth convex objective function whose exact solution yields a fair and optimal prediction function. It turns out that this objective admits a first-order stochastic oracle that can be evaluated using only one independent sample of feature vector, thus allowing for stochastic optimization approach. Furthermore, despite the convexity, we show that the key quantity to control is the gradient (or rather a gradient-map) of this objective function, deviating from the more common setup of controlling the optimization error measured by the objective function. We deploy recent machinery of Allen-Zhu (2021) and Foster et al. (2019) that allows to achieve this goal, properly setting all the hyper-parameters and recovering the usual statistical rate \(1/\) for both fairness and risk guarantees -- \(T\) being the number of samples.

Our work falls withing the realm of post-processing methods--another umbrella term that combines all the methods that perform a refitting of a base estimator to satisfy a certain constraint.

Importantly, due to the careful design of the above mentioned objective function, we can perform this post-processing in an online manner using a stream of i.i.d. _unlabeled_ data without keeping it in memory, making it attractive in practice. Our approach is based on a combination of ideas from previous contributions to fairness from Agarwal et al. (2019) and Chzhen et al. (2020) and recent stochastic optimization literature (Allen-Zhu, 2021; Foster et al., 2019) that deals with stationary point-type guarantees in the case of convex optimization.

ContributionsOur contribution is three-fold: **i)** we significantly enhance the discretization strategy of Chzhen et al. (2020) accommodating multiple sensitive features, relaxed fairness constraints, and unawareness setup; we introduce entropic regularization for this problem and design a dual convex objective from it; **ii)** we design a semi-supervised post-processing algorithm and show that it enjoys strong theoretical guarantees; **iii)** we perform numerical simulations demonstrating the relevance of our approach in practice.

Organization.This paper is organized as follows: in Section 2 we present the problem setup and introduce main problem-related notation; in Section 3 we describe our methodology step-by-step and highlight main challenges and relations to other results; in Section 4 we gives technical details of the proposed approach; Section 5 contains main theoretical results of the work; finally, Section 6 contains empirical evaluation of our method. All the proofs are postponed to the appendix.

Notation.Let us present generic notation that is used throughout this work. For a positive integer \(K\), we write \([K]\) to denote \(\{1,,K\}\) and \( K\) to denote \(\{-K,,0,,K\}\). For \(a>0\) denote by \( a\) largest non-negative integer that is smaller or equal to \(a\). For a univariate probability measure \(\), we denote by \(()\) its support. For every \(>0,m\), and \(=(w_{1},,w_{m})^{}^{m}\), we denote by \(_{}:^{m}\) the log-sum-exp function, defined as

\[_{}()=^{-1}_{j=1}^{m} ( w_{j})\,.\]

For every \(m\), \(=(w_{1},,w_{m})^{}^{m}\), we denote by \(=(_{1},,_{m}):^{m}^{m}\) the soft-argmax as \(_{j}()=(w_{j})/(_{i=1}^{m}(w_{i}))\). For any matrix \(\), the notation \( 0\) means that \(\) is positive coordinate-wise. For any \(a\) and \(^{m}\) we set \((a)_{+}=\{0,a\}\) and \(()_{+}=((w_{1})_{+},,(w_{m})_{+})^{}\). The notation \(}\) hides (unimportant) constants and polylogarithmic factors. For a pair of random elements \((A,B)\), we denote by \((A)\), the law of \(A\), by \((A B)\), the conditional law of \(A\) given \(B\), and we write \(A B\) to denote that variables \(A\) and \(B\) are independent. For two vectors \(,^{}^{m}\), we write \(/^{}=(w_{j}/w^{}_{j})_{j[m]}^{m}\) to denote element-wise division. The Euclidean norm of a vector and the Frobenius norm of a matrix are denoted by \(\|\|\), while the spectral norm of a matrix is denoted by \(\|\|_{}\). We denote by \(()\), the Borel sigma-algebra on \(\), induced by the usual topology. We write \(\) to denote the natural logarithm and \(_{a}\), the base \(a>0\) logarithm.

## 2 Problem setup

Let \((,S,Y)\) be a triplet of nominally non-sensitive, nominally sensitive, and output characteristics, taking values in \(^{d}[K]\) for some \(K 2\). We assume that \((,S,Y)\), for some unknown distribution \(\). The main quantities of interest are the following: the _regression function_\(()}}{{=}}[Y= ]\); the marginal distribution of sensitive vectors \(}}{{=}}(p_{s})_{s[K]}\) with \(p_{s}}}{{=}}(S=s)\); the conditional distribution of \(S\) given \(\), defined as \(()}}{{=}}(_{s}())_ {s[K]}\) with \(_{s}()}}{{=}}(S=s =)\). A _randomized_ prediction function is a map \(:()^{d}\) such that the map \(B(B)\) for \(B()\) is a probability measure on \((,())\) for all \(^{d}\). For any prediction \(\) we define a random variable \(_{}\) as

\[(_{}=,S=s)=( )^{d},s[K]\,.\]

**Remark 2.1**.: _Note that if \(()\) is a Dirac measure for all \(^{d}\), the above condition just means that \(_{}=g()\) almost surely for some deterministic \(g:^{d}\). The above condition is not to be confused with the fairness constraint, which is not formulated point-wise. It is only viewed as an extension of the unawareness framework to the case of randomized predictions. The above condition completely specifies the distribution of the triplet \((,S,_{})\) but leaves the relation between \(_{}\) and \(Y\) ambiguous. To be more formal, one needs to add the condition \((_{})(,S)\), that is, the prediction \(_{}\) is independent from the true label \(Y\), conditionally on \((,S)\). That would define a complete joint distribution of \((,S,Y,_{})_{}=_{(,S)} _{Y|(,S)}()\)._

We consider the following risk of a prediction function \(\)

\[()}}{{=}}[( {Y}_{}-())^{2}]=[_{}(- ())^{2}(\,)]\,.\]

A prediction function \(\) is said to satisfy the _demographic parity constraint_, if \(_{} S\).

That is, \(_{}\) is stochastically _independent_ of \(S\) viewed from the perspective of the joint distribution of \((\,,S,_{})\). On the high-level, the goal in this setup is to find a prediction function \(\), whose risk is small and whose violation of the demographic parity constraint is controlled as quantified by some measure of unfairness. The above problem is well understood in the case of _awareness_--the situation when \(\) is expressed as \((,s)\)(Chiappa et al., 2020; Chzhen et al., 2020, 2021; Jiang et al., 2020; Le Gouic et al., 2020)--revealing an intimate connection of this problem with Wasserstein barycenters. Yet, when the sensitive attribute is not an input of the prediction function, the situation is drastically different. Some attempts have been made to either (so far only partially) characterise the optimal prediction function (Chzhen and Schreuder, 2020; Gaucher et al., 2023; Zhao, 2021) or to design efficient algorithms for this problem (Agarwal et al., 2019; Maheshwari and Perrot, 2022; Narasimhan et al., 2020) that are only partially supported by a sound theory. One of the principal goals of this work is to design a computationally efficient algorithm that admits a (near) end-to-end theoretical guarantees. The main difficulty of the problem lies in very different natures of the risk and the fairness constraint--the latter involves image measures, while the former is a simple linear functional of \(\). In the case of awareness this issue can be bypassed by lifting the problem in the space of measures, working there directly and, then, returning to the initial space of prediction functions. Crucially, this is achieved only thanks to the fact that \(S\) is known at inference time, which is not the case for the considered problem.

**Remark 2.2**.: _In what follows we will exclusively focus on the squared risk and the regression setup. However, one can observe that the proposed methodology can be extended or even simplified for \(()=[r(,_{})]\) and multi-class classification respectively under the demographic parity constraint. Here \(r(,)\) quantifies fit of \(\) for an individual \(\) and can be either known or unknown._

## 3 Our methodology

The starting point of our work is similar to the one of Chzhen et al. (2020) and relies on a simple observation--if \(|(())|<\) and stays the same for all \(\), the independence constraintis reduced to a finite amount of constraints that only involve the image of \(()\). In particular, assuming that \((())=}\) for all \(^{d}\), \(_{}\) is independent from \(S\) iff \((_{}= S=s)=(_{} =)\) for all \(s[K]\) and all \(}\). In view of the definition of \(_{}\), the latter is equivalent to

\[[() S]=[( )] s[K],\,}\,,\] (1)

which, assuming that \(}\) is fixed, correspond to linear constraints on \(\). Combined with the observation that \(()\) is also linear, we end up with a problem that is significantly easier to handle. Furthermore, again assuming that \(}\) is fixed, the sketched direction gives a natural way to introduce some slack to the independence constraint--simply requiring an approximate equality in (1). Set

\[_{s}(,)}}{{=}}| [() S=s]-[ ()]|\,\] (2)

for all \(s[K]\) and \(}\). Thus, for a fixed support (whose choice will be discussed in the next paragraph) and a fixed vector \(}}{{=}}(_{1}, ,_{K})^{}\), our goal is to build an estimator of a solution to

\[_{:()^{d}}\{ ()\,:\,(())=}^{d},_{s}(, )_{s}},s[K]\}\,.\] (3)

Let us now describe the methodology for selecting \(}\) and the trade-offs that are introduced.

Introducing discretization.Having in mind the above discussion, for every integer \(L 0\) and real \(B>0\), we introduce a uniform grid \(}_{L}}}{{=}}B  L/L\) on \([-B,B]\), so that \(|}_{L}|=2L+1\), which is viewed as a support of prediction functions \(()\). For the sake of simplicity, we will assume that the regression function \(()\) is bounded in \([-B,B]\) for some known \(B>0\).

**Assumption 3.1** (Bounded signal).: _There exists \(B>0\) such that \(|()| B\) almost surely._

Thus, for a given \(B\), the main parameter to tune is \(L 1\)--the higher the \(L\) is, the more accurate prediction functions can be produced, while lower values of \(L\) ensure that the demographic parity requirement reduces to a small number of constraints. Thus, there is a trade-off that is introduced by \(L\). A natural attempt to tackle the problem of fairness in this context would be to estimate a solution to (3) with \(}=}_{L}\). Of course, \(L\) needs to be chosen so that the aforementioned solution attains the risk that is close to the risk of some benchmark prediction function that does not involve any discretization. This will be discussed later in the text. For now, let us address another subtle issue. Even assuming a complete knowledge of the underlying distribution \(\), solving (3) requires solving a linear program in dimension \((LK)\) which can be infeasible in practice for large values of \(L\) and \(K\). Instead of (3), we rather focus on the entropic regularized version of it. For \(>0\), we consider

\[_{:()^{d}}\{ _{}()\,:\,(())= }^{d},_{s}(, )_{s}},s[K]\}\,,\] (4)

where \(_{}()=()+[(( ))]\) and for any discrete univariate distribution \(\), we define its negative entropy \(()}}{{=}}_{ ()}()(())\).

**Remark 3.1** (On abuse of notation).: _Note that for every \(}_{L}\) there is a unique \( L\) such that \(= B/L\) and we will write \(()\) instead of \(()\). Similarly, we write \(_{s}(,)\) instead of \(_{s}(,)\), defined in (2), when no confusion is possible and the support \(}_{L}\) is fixed._

An extremely attractive feature of the problem in (4) is the fact that the solution to it can be written explicitly as a function of optimal dual variables, with the latter being a solution of a stochastic convex program with Lipschitz gradient--the main observation of our approach, that shares many similarities with the smoothing technique of Nesterov (2005). This is summarized in the following lemma.

**Lemma 3.1**.: _Let \(L\) and \(>0\). Let \(^{}=(^{}_{ s})_{ L ,s[K]}\) and \(}^{}=(^{}_{ s})_{ L,s [K]}\) be two matrices that are solutions to_

\[_{,} 0}\{F(,}) }}{{=}}[_{ }(_{}-_{},\,() -r_{}()_{ L})]+ _{ L}_{}+_{},\,\,\},\] (5)

_where \(()}}{{=}}1-) }{}\), \(r_{}()}}{{=}}(()- )^{2}\), and \(_{}=(_{ s})_{s[K]}\), \(_{}=(_{ s})_{s[K]}\). Then, (4) admits a solution in the form_

\[_{^{},}^{}}()}}{{=}}_{}((^{}_{ ^{}}-^{}_{^{}},\,()-r_{ ^{}}())_{^{} L}) L\,.\] (6)Assuming perfect knowledge of \(\) and \(\), the above lemma suggests a natural approach to estimating the \(_{^{},^{}}\)--we can run a (version of) stochastic gradient descent on \(F(,)\) and then plug-in the resulting dual variables in the formula for \(_{^{},^{}}\). Notably, a stochastic gradient of \(F(,)\) can be obtained by simply sampling one \(\) from \(_{}\)--it does not require labels for this step. Yet, even in the above idealized case, it is not clear which optimization criteria would allow us to prove that the resulting solution would yield good properties in terms of risk and fairness. As we will see, despite the problem in (5) being convex with Lipschitz gradient, it is crucial to control the norm of the gradient of \(F\) for good statisitcal properties of the algorithm. That goes without saying that this relaxation has its price--the smaller the regularization parameter \(\) the less accurate the resulting solution, but the resulting dual optimization problem is easier and vice-versa.

Properties of \(F\) and \(_{^{},^{}}\).Let us summarized key properties of the objects introduced in Lemma 3.1. The first two results concern the population properties of \(_{^{},^{}}\):

**Lemma 3.2** (Fairness quantification).: _Let \(L\), \(=(_{s})_{s[K]}^{K},>0\), and \(_{^{},^{}}\) be defined in Lemma 3.1. Then, \(_{s}(_{^{},^{}},) _{s}\) for all \(s[K],[L]\)._

In words, the optimal entropic-regularized prediction function is feasible for (3), that is, it satisfies the relaxed fairness constraints as quantified by (2). Furthermore, we can show that its risk is also controlled by the regularization parameter \(>0\).

**Lemma 3.3** (Risk gain).: _Let \(L,>0\), and \(_{^{},^{}}\) be defined in Lemma 3.1. For any \(:()^{d}\) that is feasible for (3), we have_

\[(_{^{},^{}})( )+}_{L}|}{}\,.\]

The above result is rather instructive, it quantifies the price of the introduced regularization. Intuitively, one wants to set \(\) high enough, so that the additive term in the above bound is vanishing. Unfortunately, we cannot set it arbitrarily high, since it will introduce instabilities from the optimization perspective--the function \(F\) becomes less regular as \(\) growth. This is summarized below.

**Lemma 3.4** (Regularity of \(F\)).: _Let \(^{2}}}{{=}}2_{s[K]}}{p_{s}}\). The objective function in (5) is convex and its gradient is \((^{2})\)-Lipschitz._

As mentioned, we see that the larger the \(\) is, the less regular the function \(F\) is, making it harder to minimize. Thus, \( 0\) controls the trade-off between the optimization error and statistical bias.

Gradient of \(F\) is crucial.Let us show that the control of the gradient of \(F\) is the most important and non-trivial part that allows to demonstrate strong statistical properties of the plug-in rule derived from the above strategy.

To this end, let us introduce parametric family of prediction functions, defined for any \(, 0\) as

\[_{,}()}}{{=}}_{}((_{^{ }}-_{^{}},\,()-r_{^{ }}())_{^{}[L]})[L]\,.\] (7)

We want to show that if \(, 0\) is nearly stationary point of \(F\), then \(_{,}\) is nearly optimal in terms of risk and its violation of the demographic parity constraint is controlled. Note that the optimization problem in (5) is constrained, thus, unless the minimum lies in the interior of the domain, we cannot hope for the gradient of \(F\) to go to zero. Instead, we introduce gradient mapping--a quantity that shares many properties of the gradient in the case of constraint optimization problem. For \(>0\),

\[_{}(,)}}{{=}},)- ((,)- F(,))_{+}}{}\,.\] (8)

Our main observation is summarized in the next lemma.

**Lemma 3.5**.: _Let \(^{2}}}{{=}}2_{s[K]}}{p_{s}}\), \(L\), \(, 0\), then for any \(>0,>0\), the unfairness of \(_{,}\) satisfies_

\[_{[L][K]}(_{s}_{,},-_{s})_{+}^{2}\| _{}(,)\|^{2}\,.\]

_Furthermore,_

\[(_{,})(_{ ^{},^{}})+(\|(, )\|+\{+\|\|}_{L}|}\})\|_{}(,)\|+}_{L}|}{}\,.\]Lemma 3.5 is very instructive on its own--we can obtain a good estimator of \(_{^{*},^{*}}\) in terms of risk and unfairness by performing stochastic optimization on \(F\) and controlling the norm of gradient mapping for a suitable parameter \(>0\). The final choice of the parameter \(\) will depend on the optimization algorithm used and will be purely theoretical. In particular, for our purposes, it is sufficient to guarantee an existence of some value of \(>0\) that yields desired statistical properties. A naive approach in doing so relies on a well-known relation between \(F(,)-F(^{*},^{*})\) and \(\|_{}(,)\|^{2}\) using the Lipshitzness of the gradient of \(F\) (see e.g., Beck, 2014, Lemma 9.11). More concretely, forgetting about the constraints1, one has

\[\| F(,)\|^{2} 2MF(,)-F(^{*},^{*})\,,\] (9)

where \(M\) is the Lipschitz constant of \( F\). Thus, the above inequality suggests that it is sufficient to control the standard optimization error in order to control the norm of the gradient. Unfortunately this approach is deemed to fail for two reasons: the first being that we control only the squared norm of the gradient map and not the norm itself, thus loosing in the rate of convergence; the second, and more subtle reason, is the separation of the purely "statistical" rate that depends only on the variance of the stochastic gradient and scales as \(1/\), with \(T\) being the number of future samples from \(_{}\), and "optimization" rate of convergence that depends on \(M\) and the diameter of the problem and typically scales as \(1/T\) or even \(1/T^{2}\) if acceleration is used.

Indeed, in our setup, Lipschitz constant \(M\) of \( F\) is not a fixed constant, but a parameter to be set--it relates to \(\) (cf. Lemma 3.4). Ideally, seeing Lemma 3.3, we want to set \(=()\), leading to \(M=()\). Thus, in view of (9), a term of the form \(M/\) appears in the convergence rate, which destroys consistency of the resulting estimator. Arguably, this is less of an issue in case of convex optimization with constant Lipschitz constant \(M\), especially if we only want the norm to go to zero. This discussion highlights that it is crucial to keep the separation between the statistical part of the rate and the optimization part of the rate, while controlling the norm of the gradient. Lucky for us, it is known that for convex problems one can indeed control the gradient mapping keeping this separation of the rate (Allen-Zhu, 2021, Foster et al., 2019). Note that it is not the case for non-convex problems as demonstrated by Arjevani et al. (2023).

Summary of our approach and why is it different from others.Now, keeping in mind the above, rather long justification, we are in position to sketch our approach and the formal presentation is deferred to the next section. For well selected parameters \(>0\), \(L\), we are going to perform stochastic optimization of \(F\), relying on the SGD3 algorithm of Allen-Zhu (2021). In order to compute the stochastic gradient of \(F\), we are simply going to sample one \(_{}\) and it appears that this stochastic gradient has a well-behaved variance (see Appendix B-C for details). To make our approach completely data-driven (or at least to understand the order of magnitude of the parameters), we will compute or bound all the oracle quantities that appear in the used optimization algorithm (essentially related to the step-size tuning). We will show that for any sufficiently small \(>0\), the term \(\|_{}(},})\| ^{2}\) is controlled and then rely on Lemma 3.5 and some additional results to demonstrate that the resulting \(_{},}}\) possesses good statistical properties.

**Remark 3.2** (On the dynamic of algorithm).: _Note that for \(==\), the corresponding_

\[_{,}()_{[L]}=((-(()-^{}B/L)^{2})_{^{ }[L]})\,.\]

_That is, the above prediction puts the most amount of mass on the atom \(\) which minimizes \((()- B/L)^{2}\)--the most accurate, but unfair prediction. Since our algorithm is based on a SGD-type algorithm, initialized at \(_{0}=_{0}=\), then we expect that during the dynamic of the algorithm, the risk of \(_{_{i},_{i}}\) increases, while the unfairness decreases. This phenomena coincides with the intuition of post-processing--we want to gain in fairness, while sacrificing some accuracy._

As it has been already mentioned, the idea of discretizing the image of (randomized) predictions is not novel and has been successfully deployed by Agarwal et al. (2019) for an in-processing estimator and by Chzhen et al. (2020) for a post-processing estimator. We use this insight as a building block, but significantly deviate from both algorithms. Compared to Agarwal et al. (2019), our algorithm is positioned in the realm of post-processing and even _online_ post-processing, where i.i.d. samplesfrom \(_{}\) comes in a stream and we do not need to store them in memory. Also, while their algorithm is partially inspired by theory, the same theory suggests that this algorithm is not computationally efficient and it relies on some black-box parts that assume perfect solutions to some optimization problems. That being said, the algorithm of Agarwal et al. (2019) seem to be the gold standard method for the generic in-processing method in this problem. Compared to Chzhen et al. (2020), we have made a sequence of improvements. First, our setup is unawareness, which is not the case in their paper; second, our algorithm is able to handle multiple protected attributes as well as approximate fairness constraints; finally, and most importantly, we do not make black-box assumptions about having access to exact minimizers of convex problems and provide an end-to-end analysis of out approach. Let us also remark that our method cannot be considered as a simple extension of Chzhen et al. (2020) as we rely on different phenomenons and provide a very different algorithm. On a more subjective note, we believe that our approach is a nice example of a real convex optimization problem, where the norm of the gradient plays the central role, while the optimization error in term of the objective function does not matter2. This is precisely the phenomena highlighted by Nesterov (2012).

```
1:Input: discretization parameter \(L 1\); regularization \(>0\), number of stochastic gradient evaluations \(T 1\); marginal distribution \(\) of \(S\); regression function \(\); conditional distribution \(\) of \(S\); bound \(B>0\) on \(\).
2:Build uniform grid \(}_{L}\) over \([-B,B]\);
3:Set parameters: \(^{2}=2_{s[K]}}{p_{s}}\), \(M=^{2}\);
4:Set \((,) F(,)\) as defined in Lemma 3.1
5:Run a black-box optimizer \((F,^{2},M,T)\) on function \(F\) having access to \(T\) stochastic gradient evaluations (see (11)) with variance \(^{2}\) and smoothness parameter \(M\) to obtain \((},})\);
6:return\(_{(},})}()\) as defined in (7); ```

**Algorithm 1**DP post-processing\((L,T,,,B,,)\)

In this section, we provide all the details about the proposed algorithm in case \(\) and \(\) are known. If they are unknown, these quantities are replaced by their estimates \(\) and \(}\) that are constructed on a separate labeled dataset. First, for \(=(_{ s})_{[],s[K]}\), \(=(_{ s})_{[],s[K]}\), let us provide the expression for the gradient of \(F\):

\[_{_{ s}}F(,)=[ _{}((_{^{}}-_{ ^{}},\,()-r_{^{}}())_{^ {}=-L}^{L})t_{s}()]+_{s}\,,\] (10)

where \(\{,\}\) and \(=1\) if \(=\) and \(=-1\) otherwise. Thus, a _stochastic gradient_\((,)=(g_{_{ s}}(,),g_{_{ s}}(,))_{[],s[K]}\) of \(F\) at a point \((,)\) can be computed by erasing expectation in (10), _i.e._, by sampling one \(_{}\), using the same convention as above about \(\), \(\):

\[g_{_{ s}}(,)=_{}( (_{^{}}-_{^{}},\, ()-r_{^{}}())_{^{}=-L}^{L} )t_{s}()+_{s}\,.\] (11)

The next result controls the variance of the above stochastic gradient.

**Lemma 4.1**.: _Let \(^{2}}}{{=}}2_{s[K]}}{p_{s}}\). It holds that \(\|(,)- F(,)\|^ {2}^{2}\)._

The proposed method is summarized in Algorithm 1. It uses a black-box stochastic optimization algorithm \(\), that operates on a convex function \(F\) and a stochastic first-order oracle. The stochastic-first order oracle is implemented by (11) and only requires to sample \(\) in an i.i.d. manner. We also pass two additional parameters to this algorithm: namely, we pass the variance \(^{2}\) from Lemma 4.1 and the Lipschitz constant of the gradient of \(F\) from Lemma 3.4. Then one can use any such algorithm. However, as shown in Lemma 3.5, those algorithms that are tailored to control expected norm of gradient mapping are preferred. For example, one can use SGD3 of Allen-Zhu (2021) or an improved version of Foster et al. (2019) that relies on restarted accelerated SGD of Ghadimi and Lan (2012).

Theoretical guarantees.

Let us first provide main results for Algorithm 1 assuming that \(\) and \(\) are known. Note that Algorithm 1 can rely on any optimization algorithm. We provide a complete analysis using a refined version of SGD3 algorithm of Allen-Zhu (2021) that is due to Foster et al. (2019) with additional modifications taking into account the specific structure of our problem. We state the main result in existential form and postpone all the details on the implementation of the algorithm and a primer on optimization to the supplementary material (Appendix C-D).

**Theorem 5.1**.: _Let \(=(_{s})_{s[K]}^{K}\) and \(^{2}=2_{s[K]}(1-p_{s})/p_{s}\). Setting \(=(T)}\) and \(L=\), there exists an optimizer \(\) to be used in Algorithm 1 that, for \(T\) larger than some absolute constant, ensures_

\[^{}_{[L][K]}_{s}_{,},-_{s}_{+} ^{2}}(} (1+}\|(^{},^{} )\|))\,.\]

_Furthermore, if Assumption 3.1 is satisfied and let_

\[^{}}}{{=}}_{n:^{ }[-B,B]}\{(h)\,:\,_{t R}|(h()  t S=s)-(h() t)|}{2}, s[K]\}\] (12)

_and \((_{},}})}}{{=}}[(_{}, }})]-^{}\), then for the same algorithm_

\[(_{},}}) }((}^{}[\|(},})\|^{2}]+ \|}{T^{}})(1+}\|(^{},^{})\|)+ })\,.\]

Theorem 5.1 gives two results: the first one being on the unfairness of the proposed estimator and the second one on the risk of thereof compared to a benchmark prediction function in (12). The benchmark that we pick is rather natural, we compare to the risk of a deterministic prediction that minimizes the risk and whose unfairness is controlled by a Kolmogorov-Smirnov distance. One first main observation is that both fairness and risk decrease at the rate \(1/\) and \(T\) is the number of unlabeled data. From our numerical experiments, we observed that we can keep the number of unlabeled data unchanged and iterate several times through them. As a result, we increase artificially \(T\)--without generating new data--which gives a significant empirical improvement. We also remark that \(\) is the parameter that depends on the number of groups. For example, in the case of uniform distribution of sensitive groups \(=O(K)\). We finally remark that both bounds involve a single unknown quantity--\(\|(^{},^{})\|\), which from standard duality argument can be shown to be bounded by \(O(1/_{s[K]}\{_{s}\})\)(see e.g., Nedic and Ozdaglar, 2009, Lemma 3). Thus, having this norm multiplied by \(T^{-1/2}\) is a very attractive property of the bound. It allows to set \( T^{-1/2}\) without damaging the parametric convergence rate.

To derive the above result, we slightly extend the analysis of Foster et al. (2019), who, relying on the SGD3 algorithm of Allen-Zhu (2021), gave an optimal algorithm that controls the expected norm of the gradient in the convex case. More concretely, we incorporate a projection step into their analysis and extend the control to the squared norm of the gradient map. Interestingly, due to our smoothing step and the choice of the parameter \(\), we noticed that there is no need to restart the accelerated SGD as it is done by Foster et al. (2019) because it leads to identical statistical convergence rates. The interested reader can take a closer look into the Appendix C, where all the optimization results are either recalled or derived for the sake of completeness. Finally, having a control of the squared norm of the gradient map, the proof of Theorem 5.1 follows from Lemma 3.5 and a careful and practical choice of all the parameters of the algorithm.

Extension to unknown \(\) and \(\).In this part we show that if we replace \(\) and \(\) with their estimates \(\) and \(}\) and run DP post-processing\((L,T,,,B,,})\) algorithm with the same choice of parameters, Theorem 5.1 remains if we pay additional price for the estimation of \(\) and \(\). From now on, we assume that \(\) and \(}\) are provided and are trained on its own labeled data sample, while the refitting is performed on an independent stream of i.i.d. data from \(_{}\). So, we essentially treat \(\) and \(}\) as deterministic functions. Let us introduce a family of prediction functions

\[_{,}()}}{{=}}_{}((_{ ^{}}-_{^{}},\,}() -_{^{}}())_{^{}[L]})[L]\,.\] (13)

[MISSING_PAGE_FAIL:9]

Comparison with Agarwal et al. (2019).Surprisingly, we were unable to find many open source competitors that target regression with demographic parity constraint in unaware setting, even the FairLearn--a popular python package--does not deal with the demographic parity constraint in regression. The only easily accessible algorithm that deals with our problem was kindly provided by Agarwal et al. (2019) (from now on referenced as ADW). We train ADW method in two ways: we use \(_{}\) and \(_{}\) as training set for ADW-1, whereas for ADW-2 we use only \(_{}\). The second situation is realistic, when unlabeled data is available and unlike ADW, our approach is able to take advantage of it. We take the set \(\{(2^{-i},2^{-i})_{i}\}\), where \(=\{1,2,4,8,16\}\) as unfairness thresholds for training both datasets. We train ADW-1 and ADW-2 for each pair of epsilons for 10 times. With our available computing power and the code provided by the authors, the algorithm runs for 13.5 hours (see Appendix G for additional details).4

On Figure 2 we illustrate the comparison of risk and unfairness between ADW-1, ADW-2, base (LinearRegression) and our model. We plot the mean and standard deviation of risk and unfairness for each epsilon threshold on both datasets. We observe that our method is competitive or eventually outperforms ADW in both training regimes.

## 7 Conclusion

Deriving a dual convex surrogate, we have provided a generic way to build a post-processing estimator of any off-the-shelf method that achieves the demographic parity constraint. Our approach is fully data and theory driven, revealing a key role of stationary point guarantees in stochastic convex optimization. Following Remark 2.2, we intend to extend our approach, which is general enough, to other learning problems, beyond algorithmic fairness.

Limitations.From the theoretical perspective, the knowledge of \(B\) seems to be the main limitation. While it is available for many applications, it does not have to be the case all the time. Replacing this assumption with some tail conditions, could be more realistic. From the applied perspective, it would be beneficial to further investigate stationary point guarantees for convex optimization to yield a better practical performance.

AcknowledgementsThe work of Gayane Taturyan has been supported by the French government under the "France 2030" program, as part of the SystemX Technological Research Institute within the Confinance.ai project.

Figure 1: Risk and unfairness of our estimator on _Communities and Crime_ and _Law School_ datasets.

Figure 2: Comparison with ADW model on _Communities and Crime_ and _Law School_ datasets.