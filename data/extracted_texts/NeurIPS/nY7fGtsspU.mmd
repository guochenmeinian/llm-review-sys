# Graph Neural Networks Do Not Always Oversmooth

Bastian Epping\({}^{1}\), Alexandre Rene\({}^{1}\), Moritz Helias\({}^{2,3}\), Michael T. Schaub\({}^{1}\)

\({}^{1}\)RWTH Aachen University, Aachen, Germany

\({}^{2}\)Department of Physics, RWTH Aachen University, Aachen, Germany

\({}^{3}\)Institute for Advanced Simulation (IAS-6), Computational and Systems Neuroscience,

Julich Research Centre, Julich, Germany

epping@cs.rwth-aachen.de, rene@cs.rwth-aachen.de,

m.helias@fz-juelich.de, schaub@cs.rwth-aachen.de

###### Abstract

Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of oversmoothing, the property that features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs. In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features. By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing. The theory, however, allows us to identify a new, non-oversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs _do not_ oversmooth, and node features remain informative even at large depth. We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output. Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase. We test the predictions of our approach and find good agreement with finite-size GCNs. Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive.

## 1 Introduction

Graph neural networks (GNNs) reach state of the art performance in diverse application domains with relational data that can be represented on a graph, transferring the success of machine learning to data on graphs . Despite their good performance, GNNs come with the limitation of _oversmoothing_, a phenomenon where node features converge to the same state exponentially fast for increasing depth . Consequently, only shallow networks are used in practice . In contrast, it is known that the depth (i.e. the number of layers) is key to the success of deep neural networks (DNNs) . While for conventional DNNs shallow networks are proven to be highly expressive , in practice deep networks are much easier to train and are thus the commonly used architectures . Furthermore, in most GNN architectures each layer only exchanges information between neighboring nodes. Deep GNNs are therefore necessary to exchange information between nodes that are far apart in the graph . In this study, we investigate oversmoothing in graph convolutional networks (GCNs) .

To study the effect of depth, we consider the propagation of features through the network: given some input \(_{}^{(0)}\), each intermediate layer \(l\) produces features \(_{}^{(l)}\) which are fed to the next layer. Wefollow the same approach that has successfully been employed in previous work to design trainable DNNs : consider two nearly identical inputs \(_{}^{(0)}\) and \(_{}^{(0)}\) and ask whether the intermediate features \(_{}^{(l)}\) and \(_{}^{(l)}\) become more or less similar as a function of depth \(l\). In the former case, the inputs may eventually become indistinguishable. In the latter case, the inputs become less similar over layers: the distance between them increases over layers [32; 37] until eventually it is bounded by the non-linearities of the network. The distance then typically converges to a fixed value determined by the network architecture, independent of the inputs.

One can therefore identify two phases: One says that a network is _regular_ if two inputs eventually converge to the same value as function of \(l\); conversely, one says that a network is _chaotic_ if two inputs remain distinct for all depths . Neither phase is ideal for training deep networks since in both cases all the information from the inputs is eventually lost; the typical depth at which this happens is called the _information propagation depth_. However, this propagation depth diverges at the transition between the two phases, allowing information - in principle - to propagate infinitely deep into the network. While these results are calculated in the limit of infinitely many features in each hidden layer, the information propagation depth has been found to be a good indicator of how deep a network can be trained . A usual approach for conventional DNNs is thus to initialize them at the transition to chaos. Indeed, Schoenholz et al.  were able to use this approach to train fully-connected, feedforward networks with hundreds of layers. Similar methods have recently been adapted to the study of transformers, successfully predicting the best hyperparameters for training .

In this work we address the oversmoothing problem of GCNs by extending the framework described above in the limit of infinite feature dimensions from DNNs to GCNs: here the two different inputs \(_{}^{(0)}\) and \(_{}^{(0)}\) correspond to the input features on two nodes, labeled \(\) and \(\). The mixing of information across different nodes implies that output features on node \(\) depend on input features on node \(\) and vice versa. Thus it is not possible to look at the distance of \(_{}^{(l)}\) and \(_{}^{(l)}\) independently for each pair \(\) and \(\) as in the DNN case. Rather, one has to solve for the distance between each distinct pair of nodes in the graph simultaneously: the one dimensional problem for DNNs thus becomes a multidimensional problem for GCNs. However, by linearizing the multidimensional GCN dynamics, we can generalize the notion of information propagation depth to GCNs: instead of being a single value, we find that a given GCN architecture comes with a set of potentially different information propagation depths, each corresponding to one eigendirection of the linearized dynamics of the system.

This approach allows us to extend the concept of a regular and a chaotic phase to GCNs: in the regular phase, which describes most of the GCNs studied in the current literature, distances between node features shrink over layers and exponentially attain the same value. We therefore call this the oversmoothing phase. On the other hand, if one increases the variance of the weights at initialization, it is possible to transition into the chaotic phase. In this phase, distances between node features converge to a fixed but finite distance at infinite depth. The convergence point is fully determined by the underlying graph structure and the hyperparameters of the GCN and may differ for different pairs of nodes. GCNs initialized in this phase thus do not suffer from oversmoothing. We find that the convergence point is informative about the topology of the underlying graph and may be used for node classification with GCNs of more than \(1,000\) layers. Near the transition point, GCNs at large depth offer a trade-off between feature information and information contained in the neighborhood relation of the graph. We test the predictions of this theory and find good agreement in comparison to finite-size GCNs applied to the contextual stochastic block model . On the citation network Cora  we reach the performance reported in the original work by Kipf and Welling  beyond \(100\) layers. Our approach applies to graphs with arbitrary topologies, depths, and non-linearities.

## 2 Related Work

Oversmoothing is a well-known challenge within the GNN literature [21; 35]. On the theoretical side, rigorous techniques have been used to prove that oversmoothing is inevitable. The authors in  show that GCNs with the ReLU non-linearity exponentially lose expressive power and only carry information of the node degree in the infinite layer limit. While the authors notice that their upper bound for oversmoothing does not hold for large singular values of the weight matrices, they do not identify a non-oversmoothing phase in their model. These results have been extended in  to handle non-linearities different from ReLU. Also graph attention networks have been proven to oversmooth inevitably . Their proof, however, makes assumptions on the weight matrices which, as we will show, exclude networks in the chaotic, non-oversmoothing phase.

On the applied side, a variety of heuristics have been developed to mitigate oversmoothing [49; 4; 3; 22; 40; 15]. E.g., the authors in  introduce a normalization layer which can be added to a variety of deep GNNs to make them trainable. Another approach is to introduce residual connections and identity mappings, directly feeding the input to layers deep in the network [4; 46]. Other studies suggest to train GNNs to a limited number of layers to obtain the optimal amount of smoothing [18; 46]. The recent review  proposes a unified view to order existing heuristics and guide further research. While these heuristics improve performance at large depths, they also add to the complexity of the model and impose design choices. Our approach, on the other hand, explains why increasing the weight variance at initialization is sufficient to prevent oversmoothing.

## 3 Background

### Network architecture

In this paper we study a standard graph convolutional network (GCN) architecture  with an input feature matrix \(^{(0)}^{N d_{0}}\), where \(N\) is the number of nodes in the graph and \(d_{0}\) the number of input features. Bold symbols throughout represent vector or matrix quantities in feature space. The structure of the graph is represented by a shift operator \(A^{N N}\). We write the features of the network's \(l\)-th layer as \(^{(l)}^{N d_{l}}\); they are computed recursively as

\[^{(l)}=(A^{(l-1)}^{(l)}+1^{(l)})\,,\] (1)

with \(\) an elementwise non-linear activation function, \(^{(l)}\) an optional bias term, weight matrices \(^{(l)}^{d_{l} d_{l-1}}\), and \(1^{N}\) a vector of all ones. We note that many GNN architectures studied in the literature are unbiased, which can be recovered by setting \(^{(l)}\) to zero. We use a noisy linear readout, so that the output of the network is given by

\[=A^{(L)}^{(L+1)}+1^{(L+1)}+\,,\] (2)

with \(^{N d_{L+1}}\) being independent Gaussian random variables: \(_{,i}}}{{}}( 0,_{ro}^{2})\). The readout noise \(\) is included both to promote robust outputs and to prevent numerical issues in the matrix inversion in Equations (11) and (12). We use \(d_{L+1}\) to denote the dimension of outputs.

For the following it will be useful to consider the activity of individual nodes. To avoid ambiguity in the indexing, we use lower Greek indices for nodes and upper Latin indices for layers. We thus rewrite (1) as

\[_{}^{(l)} =(_{}^{(l)})\,,\] (3) \[_{}^{(l)} =_{}A_{}^{(l)}_{}^{(l-1)}+ ^{(l)}\,,\] (4) \[_{} =_{}^{(L+1)}+_{}\,,\] (5)

where \(_{}^{(l)}^{d_{l}}\) is the feature vector of node \(\) in layer \(l\) and \(_{}^{d_{L+1}}\) the network output for node \(\). The values \(_{}^{(l)}^{d_{l}}\) are linear functions of the features \(_{}^{(l-1)}\) and represent the input to the activation functions; we therefore refer to them as preactivations. The non-linearity \((x)\) is applied elementwise to the preactivations \(_{}^{(l)}\). While we leave \((x)\) general for the development of the theory, we use \((x)=(}{2}x)\) for the experiments in Section 4; this choice allows us to carry out certain integrals analytically. The scaling factor in the \(\) is chosen such that \((0)=1\). We use independent and identical Gaussian priors for all weight matrices and biases, \(W_{ij}^{(l)}}}{{}}(0,^{2}}{d_{l}})\) and \(b_{i}^{(l)}}}{{}}(0,_ {ro}^{2})\) with \(W_{ij}^{(l)}\) and \(b_{i}^{(l)}\) being the matrix or vector entries of \(^{(l)}\) and \(^{(l)}\), respectively. As a shift operator, we choose

\[A=-}(D-)\,,\] (6)where \(\) is the adjacency matrix, \(\) the identity in \(^{N N}\), \(D_{}=_{}_{}_{}\) is the degree matrix and \(d_{}\) is the maximal degree. The parameter \(g(0,1)\) allows us to weigh the off-diagonal elements compared to the diagonal ones. By construction the shift operator is row-stochastic, which means that it has constant sums over columns \(_{}A_{}=1\). We will make use of this property in our analysis in Section 4.2. The generalization to non-stochastic shift operators will be shortly addressed later.

### Gaussian process equivalence of GCNs

In a classic machine learning setting, such as classification, one draws random initial values for all parameters and subsequently trains the parameters by optimizing the weights and biases to minimize a loss function. This learned parameter set is then used to classify unlabeled inputs. In this paper we take a Bayesian point of view in which the network parameters are random variables, inducing a probability distribution over outputs which becomes Gaussian in the limit of infinitely many features. Thus infinitely wide neural networks are equivalent to Gaussian processes (GPs) [27; 43; 34]. In the study of DNNs this is a standard approach, yielding results which empirically hold also for finite-size networks trained with gradient descent .

In previous work [28; 14; 29] it has been shown that also the GCN architecture described in Section 3.1 is equivalent to a GP in the limit of infinite feature space dimensions, \(d_{l}\) for all hidden layers \(l=1,,L\), while input and readout layer still have tunable, finitely many features. In the GP description, all features are Gaussian random variables with zero mean and identical prior variance in each feature dimension. The description of the GCN thus reduces to a multivariate normal,

\[H^{(l)}(0,K^{(l)})\,,\] (7)

where \(H^{(l)}\) is the vector of hidden node features of layer \(l\), \(H^{(l)}=(h_{0}^{(l)},h_{1}^{(l)},,h_{N}^{(l)})^{}\) under the prior distribution of weights and biases. The covariance matrices \(K^{(l)}^{N N}\) are determined recursively: knowing that the \(h_{}^{(l)}\) follow a zero-mean Gaussian with covariance \( h_{}^{(l)}h_{}^{(l)}=K_{}^{(l)}\), we define

\[C_{}^{(l)}=h_{}^{(l)} h_{}^{(l)}_{h_{}^{(l)},h_{}^{(l) }}.\] (8)

For simplicity we use \((x)=(}{2}x)\) for which Equation (8) can be evaluated analytically; see Appendix A for details. It follows from (3) that

\[K_{}^{(l+1)}=_{b}^{2}+_{w}^{2}\,_{,}A_ {}A_{}C_{}^{(l)}\,,\] (9)

as shown in [28; 29].

In a semi-supervised node classification setting, we split the underlying graph into \(N^{}\) unlabeled test nodes and \(N^{}\) labeled training nodes (\(N^{}+N^{}=N\)); we correspondingly split the output random variable \(Y_{i}^{N}\) for output dimension \(i\) into \(Y_{i}^{}^{N^{}}\) and \(Y_{i}^{D}^{N^{}}\). Features on the test nodes are predicted by conditioning on the values of the training nodes: \(pY_{i}^{}=y_{i}^{} Y_{i}^{D}=y_{i}^{D}\,.\) This leads to the following posterior for the unobserved labels (see [34; 20] for details):

\[Y_{i}^{} (m_{i}^{},K^{})\,,\] (10) \[m_{i}^{} =K_{ D}^{(L+1)}(K_{DD}^{(L+1)}+_{ro}^{2})^{- 1}Y_{i}^{D}\,,\] (11) \[K^{} =K_{}^{(L+1)}-K_{ D}^{(L+1)}(K_{DD}^{(L+1)}+ _{ro}^{2})^{-1}(K_{ D}^{(L+1)})^{}\,.\] (12)

Here the \(\) and \(D\) indices represent test and training data, respectively, i.e. \(K_{DD}^{N^{} N^{}}\) is the covariance matrix of outputs of all training nodes and \(K_{ D}^{N^{} N^{}}\) is the covariance between test data and training data. Finally, \(\) is here the identity in \(^{N^{} N^{}}\).

### Feature distance

To measure and quantify how much a given GCN instance oversmoothes we use the squared Euclidean distance between pairs of nodes, and normalize by the number of node features \(d_{l}\) so that the measure stays finite in the GP limit \(d_{l}\). This allows us to quantitatively test the predictions of our approach on the node-resolved distances of features. To summarize the amount of oversmoothing across the GCN, we also define the measure \((X)\) as the average squared Euclidean distance across all pairs of nodes:

\[d(_{},_{}) =}||_{}-_{}||_{2}^{2}=C^{ }_{}+C^{}_{}-2C^{}_{}\,,\] (13) \[() =_{=1}^{N}_{=+1}^{N}d( _{},_{})\,.\] (14)

Here \(C^{}_{}=_{}_{}}{d_{l}}\) is the normalized scalar product. We use the notation \(C^{}_{}\) to avoid confusion with the expectation value \(C_{}\) defined in the GCN GP (8). In the infinite feature dimensions limit, the quantities \(C^{}_{}\) in Equation (14) converge to the GCN GP quantities \(C_{}\) defined by (8). In the following sections we will therefore use the \(C_{}\) as predictions for the \(C^{}_{}\) of finite-size GCNs. The normalization for \(d(_{},_{})\) and \(()\) can be interpreted as an average (squared) feature distance, independent of the size of the graph and the number of feature dimensions.

## 4 Results

### Propagation depths

We are interested in analyzing GCNs at large depth. We a priori assume that at infinite depth the GCN converges to an equilibrium in which covariances are static over layers \(K^{(l)}_{}K^{}_{}\), irrespective of whether the GCN is in the oversmoothing or the chaotic phase. A posteriori we show that this assumption indeed holds. Since the fixed point \(K^{}\) is independent of the input, a GCN at equilibrium cannot use information from the input to make predictions (although, as we will see, in the non-oversmoothing phase it can still use the graph structure). In the following we analyze the equilibrium covariance \(K^{}\) to which GCNs with different \(_{w}^{2}\), \(_{b}^{2}\) and \(A\) converge to, how they behave near this equilibrium, and at which rate it is approached.

Close to equilibrium, the covariance matrix \(K^{(l)}\) can be written as a perturbation around \(K^{}_{}\):

\[K^{(l)}_{}=K^{}_{}+^{(l)}_{}\,.\] (15)

Under the assumption that the perturbation \(^{(l)}_{}\) is small, we can linearize the GCN GP

\[^{(l+1)}_{} =_{,}H_{,}^{(l)}_{ }+((^{(l)})^{2})\,,\] (16) \[H_{,} =_{w}^{2}_{,}(1+_{, })A_{}A_{}}{ K _{}}[K^{}]\,,\] (17)

where we use square brackets to denote the point around which we linearize. The factor \((1+_{,})\) is introduced to correctly count on- and off-diagonal elements of the covariance matrix, while the shift operators \(A\) and the derivative \(}{ K_{}}[K^{}]\) originate from the message passing and the non-linearity \(\), respectively. The latter would result in a Kronecker delta \(}{ K_{}}[K^{}]= _{,}\) for linear networks. The calculation for \(H_{,}\) is done in detail in Appendix B.

A conceptually similar linearization has been done in  for DNNs. In the DNN case, different inputs to the networks--which correspond to input features on different nodes here--can be treated separately, leading to decoupling of Equation (16). The shift operator in the GCN dynamics, in contrast, couples features on neighboring nodes - the matrix \(H_{,}\) is in general not diagonal.

We can still achieve a decoupling by interpreting Equation (16) as a matrix multiplication, if \(\) and \(\) are understood as double indices, and by finding the eigendirections of the matrix \(H^{N^{2} N^{2}}\). Taking the right eigenvectors \(V^{(i)}_{}\) as basis vectors, we can decompose the covariance matrix \(^{(l)}_{}=_{i}^{(l)}_{i}V^{(i)}_{}\) and thus obtain the overlaps \(^{(l)}_{i}\) which evolve independently over layers. If the fixed point \(K^{ eq}\) is attractive, all eigenvalues have absolute values smaller than one: \(|_{i}|<1\). This allows us to define the propagation depth \(_{i}-)}\) for each eigendirection, very similar to the DNN case . In this form, the linear update equation (16) simplifies to

\[_{i}^{(l+d)}=_{i}^{d}_{i}^{(l)}=(-d/_{i})_{i}^ {(l)}\,,\] (18)

thus decoupling the system. For details on the linearization and some properties of the transition matrix \(H\) refer to Appendix B.

### The non-oversmoothing phase of GCNs

In this section we establish the chaotic, non-oversmoothing phase of GCNs, and show that this phase can be reached by simple tuning of the weight variance \(_{w}^{2}\) at initialization. We start by noticing that a GCN is at a state of zero feature distance \((^{(l)})=0\), if the covariance matrix has constant entries, \(K_{}^{(l)}=k^{(l)}\): Constant entries in \(K_{}^{(l)}\) imply that all preactivations are the same, \(_{}^{(l)}=_{}^{(l)}\), which in turn implies \(C_{}^{(l)}=c^{(l)}\) (by Equation (8)); the latter is equivalent to features being the same, \(_{}^{(l)}=_{}^{(l)}\). Due to our choice of the shift operator, the state of zero distance (and thus of \(K_{}^{(l)}=k^{(l)}\)) is always a fixed point. Assuming that \(C_{}^{(l)}=c^{(l)}\), we obtain

\[K_{}^{(l+1)}=_{b}^{2}+_{w}^{2}A_{}}_{=1}A_{}}_{=1}c^{(l)}= k^{(l+1)}\,.\] (19)

In an overmoothing GCN, this fixed point is also attractive, meaning that also pairs of feature inputs \(_{}^{(0)}\), \(_{}^{(0)}\) which initially have non-zero distance \(d(_{}^{(0)},_{}^{(0)}) 0\) (and thus \((^{(0)}) 0\)) eventually converge to the point of vanishing distance. The chaotic, non-oversmoothing phase of a GCN is determined by the condition that this point of constant covariance \(K_{}^{(l)}=k^{(l)}\) becomes unstable. More formally, this can be written in terms of eigenvalues of the linearized dynamics as

\[\{|_{i}^{p}|\}}{{>}}1\,.\] (20)

Here and in the following we will use the superscript \({ p}\) to denote that the linearization is done around the state of constant covariance across nodes in both the oversmoothing and non-oversmoothing phase. The propagation depth \(_{i}-)}\) diverges at the phase transition where one \(_{i}\) approaches \(1\). Intuitively speaking, Equation (20) asks whether a small perturbation from the zero distance case diminishes (\(\{|_{i}^{ p}|\}<1\)), in which case the network dynamics is regular, or grows (\(\{|_{i}^{ p}|\}>1\)), in which case the network is chaotic and thus does not oversmooth. The value of \(\{|_{i}^{ p}|\}\) depends on the choices of \(A\), \(_{w}^{2}\) and \(_{b}^{2}\) (by the dependence of \(K^{ eq}\) on \(_{b}^{2}\)). In the following we will concentrate on tuning \(_{w}^{2}\) to reach the non-oversmoothing phase.

#### 4.2.1 Complete graph

To illustrate the implications of the analysis described above, we first consider a particularly simple GCN on a complete graph; this allows us to calculate the condition for the transition to chaos analytically, and gain some insight into the interesting parameter regimes. Moreover, we use this pedagogical example to show that although the GP equivalence is only true in the limit of infinite hidden feature dimensions, \(d_{l}\), our results still describe finite-size GCNs well.

For a complete graph with adjacency matrix \(_{}=1-_{}\), our choice of shift operator \(A\) in (6) has entries \(A_{}=+_{}(1-)\). This model is a worst-case scenario for oversmoothing, since the adjacency matrix leads to inputs that are shared across all nodes of the network. We make the ansatz that the equilibrium covariance is of the form \(K_{}^{ eq}=K_{c}^{ eq}+_{}(K_{a}^{ eq}-K_ {c}^{ eq})\) due to symmetry which reduces the problem to only two variables. In this formulation we can use similar methods as in the DNN case  to determine the non-oversmoothing condition on the l.h.s in (20) (Details are given in Appendix C).

Figure 1 shows how a GCN on a complete graph can be engineered to be non-oversmoothing by simple tuning of the weight variance \(_{w}^{2}\). Panel a) shows that increasing the weight variance \(_{w}^{2}\) or decreasing

the size of the off-diagonal elements \(g\) both shift the network towards the non-oversmoothing phase. Both parameters also increase the equilibrium feature distance beyond the transition. The theoretical prediction for the transition is calculated in Appendix C and shown as the red line. Panel b) confirms the accuracy of this calculation. Larger values of \(g\) increase smoothing, and thus larger values of \(_{w}^{2}\) are needed to compensate. Moreover, our formalism allows us to predict the evolution of feature distances over layers correctly, as can be confirmed in panel c). We find again that GCNs with parameters past the transition do not oversmooth.

#### 4.2.2 General graphs

For general graphs, the transition to the non-oversmoothing phase given by Equation (20) can be determined numerically. As a proof of concept, we demonstrate this approach for the Contextual Stochastic Block Model (CSBM) , a common synthetic model which allows generating a graph with two communities and community-wise correlated features on the nodes. Pairs of nodes within the same community have higher probability of being connected and have feature vectors which are more strongly correlated, compared with pairs of nodes from different communities.

Given the underlying graph structure, we can construct the linear map \(H\) from Equation (16) and the analytical solution for \(C_{}\) in Appendix A. Finding the set of eigenvalues is then a standard task. We show the applicability of our formalism in Figure 2 by showing that GCNs degenerate to a zero distance state state exactly when \(_{w}^{2}<_{w,}^{2}\). Panel a) shows how this procedure correctly predicts the transition in the given CSBM instance: The maximum feature distance between any pair of nodes increases from zero at the point where the state \(K_{}^{(l)}=k^{(l)}\) becomes unstable. This means that beyond this point, the GCN has feature vectors that differ across nodes and therefore does not oversmooth. This is more explicitly shown in panels b) and c), where the equilibrium feature distance is plotted as a heatmap. At point \(A\) (panel b)), within the oversmoothing phase, all equilibrium feature distances are indeed zero, the network therefore converges to a state in which all features are the same. At point \(B\) (panel c)) on the other hand, pairs of nodes exist that have finite distance. In the latter case, one can recognize the community structure of the CSBM: the lower left and upper right quadrants are lighter than the diagonal ones, indicating larger feature distances across communities than within. The equilibrium state thus contains information about the graph topology. This phenomenon is also observed in panel d), where we show the predicted feature distance averaged for nodes within or between classes as a function of layers compared to finite-size simulations. Again, theoretical predictions match with simulations. Thus also on more general graphs the presented formalism predicts the transition point between the oversmoothing and the non-oversmoothing phase, corresponding to a transition between regular and chaotic behavior.

Figure 1: Simulations and GP prior of a GCN on a complete graph with \(N=5\) nodes, shift operator \(A_{}=+_{}(1-)\), vanishing bias \(_{b}^{2}=0\) and \((x)=(}{2}x)\). **a)** The phase diagram dependent on \(_{w}^{2}\) and \(g\). The equilibrium feature distance \(()\) obtained from computing the GCN GP prior for \(L=4,000\) layers is shown as a heatmap, the red line is the theoretical prediction for the transition to the non-oversmoothing phase. **b)** Same as in a) but color coding shows whether \(()\) is close to zero (black) or not (white) with precision \(10^{-5}\). The red line again shows the theoretically predicted phase transition. **c)** Feature distance \((^{(l)})\) for a random input \(X_{ i}^{(0)}}}{{}}( 0,1)\) as a function of layer \(l\). Parameters are written in the panel in matching colors and marked with color coded crosses in the phase diagram in panel b). Feature dimension of the hidden layers is \(d_{l}=200\), crosses show the mean of \(50\) network realizations, solid curves the theoretical predictions.

We discuss how the assumptions on weight matrices in related theoretical work [2; 45] exclude networks in the chaotic phase in Appendix D, explaining why the non-oversmoothing phase has not been reported before. In Appendix E we observe how increasing the weight variance increases the oversmoothing measure \((X)\) in equilibrium also in the case of the more common shift operator proposed in the original work , despite the fact that this shift operator does not have the oversmoothed fixed point in the sense of Equation (19).

### Implications for performance

Lastly we want to investigate the implications of the non-oversmoothing phase on performance. We do this by applying the GCN GP as well as a finite-size GCN to the task of node classification in the CSBM model and measure their performance, shown in Figure 3. Panel a) shows how the generalization error of the GCN GP changes depending on the weight variance \(_{w}^{2}\) and the number of layers \(L\). In the oversmoothing phase where most GCNs in the literature are initialized (see Appendix D), the generalization error increases significantly already after only a couple of layers. We observe the best performance near the transition to chaos where the GCN GP stays informative up to \(100\) layers. In panel b) we test the generalization error for even deeper networks. While the generalization error increases to one (being random chance) in the oversmoothing phase, GCN GPs in the chaotic phase stay informative even at more than a thousand layers. This can be explained by Figure 2: For such deep networks, the dynamics are very close to the equilibrium and thus no information of the input features \(^{(0)}\) is transferred to the output. The state, however, still contains information of the network topology from the adjacency matrix, leading to better than random chance performance. In panel c) we explicitly show the layer dependence of the generalization error for the GCN GP at the critical point, in the oversmoothing and in the chaotic phase. Again, we see a fast performance drop for oversmoothing networks, while in the chaotic phase and at the critical point the GCN GP obtains good performance also at large depths, with performance peaking at \(L 15\) layers. Tuning the weight variance thus not only prevents oversmoothing, but may also allow the construction of GCNs with more layers and possibly better generalization performance.

In the study of deep networks, results obtained in the limit of infinite feature dimensions \(d_{l}\) often are also applicable for finite-size networks [32; 37]. In panel d) we conduct a preliminary analysis for finite-size GCNs by measuring the performance of randomly initialized GCNs for which we only train the readout layer via gradient descent. Indeed, we observe similar behavior as for the GCN GP: Performance drops rapidly over layers in the oversmoothing phase, while performance stays high over many layers at the critical point and in the chaotic phase and peaks at \(L 15\) layers.

Figure 2: The non-oversmoothing phase in a contextual stochastic block model instance with parameters \(N=100\), \(d=5\), \(=1\). The shift operator is chosen according to (6) with \(g=0.3\), and \(_{b}^{2}=0\) and \((x)=(}{2}x)\). **a)** The maximum feature distance between any pair of nodes in equilibrium obtained from computing the GCN GP prior for \(L=4,000\) layers (blue) and the largest eigenvalue of the linearized GCN GP dynamics at the zero distance state as a function of weight variance \(_{w}^{2}\). The red line marks the point where \(_{i}\{|_{i}^{}|\}=1\). **b)** Heatmap of the equilibrium distance matrix with entries \(d_{}=d(_{},_{})\) (Equation (13)) at \(_{w}^{2}=1.3\), marked as point \(A\) in panel a). Colorbar shared with the plot in c). **c)** Same as b) but at point \(B\) with \(_{w}^{2}=2\). **d)** Features distances \(d_{}^{(l)}=d(_{}^{(l)},_{}^{(l)})\) as a function of layers for random inputs \(X_{ i}^{(0)}}{}(0,1)\) and a finite-size GCN with \(d_{l}=200\), averaged for distances for pairs of nodes within the same community (red) and across communities (purple).

Additionally, we test the performance of the GCN GP on the real world citation network Cora . Evaluating the eigenvalue condition (20) would be computationally expensive for such a large dataset, therefore we find the transition by numerically evaluating the feature distance \((X)\) in equilibrium and search for the \(_{w}^{2}\) at which this distance becomes non-zero. This procedure results in \(_{w,}^{2} 1\) and is presented in more detail in Appendix G. Figure 4 panel a) shows the performance of the GCN GP dependent on the number of layers \(L\) and weight variance \(_{w}^{2}\): as for the CSBM in Figure 3 we observe that the performance for deep GCN GPs is best near the transition in the non-oversmoothing phase. Furthermore, GCN GPs with more layers achieve lower generalization error. This is shown more directly in panel b). There we observe the layer dependence for GCN GPs near the transition in the non-oversmoothing regime. Indeed, the accuracy increases up to a hundred layers, reaching the accuracy of finite-size GCNs stated in .

Near the transition, accuracy increases for up to \(L=100\), and the generalization error improves even beyond this. We hypothesize that this many layers are required for high performance partly due to our choice of the shift operator. The Cora dataset has a maximum degree \(d_{}=168\) leading to small off-diagonal elements for the choice of our shift operator: Recall that in Equation (6), the parameter \(g\) is constrained to be \(g(0,1)\). As a consequence, the off-diagonal elements of the shift operator are \(A_{ij}<}}=\). Many convolutional layers are then needed to incorporate information from a node's neighbors.

One might wonder whether it is possible to initialize weights in say the oversmoothing regime, and transition to the non-oversmoothing regime during training. We argue that this is possible in the case of Langevin training (Appendix H).

Figure 4: GCN GP performance on the Cora datset . **a)** Generalization error (mean squared error) as a function of layers \(L\) and weight variance \(_{w}^{2}-_{w,}^{2}\) for our stochastic shift operator (6) with \(g=0.9\). The value of \(_{w,}^{2} 1\) is determined numerically in Appendix G. **b)** Layer dependent generalization error and accuracy for GCNs near the transition \(_{w}^{2}=_{w,}^{2}+0.1\). Grey dashed line shows accuracy obtained for GCNs in the original work . Numerical details in Appendix F.

Figure 3: Generalization error (mean squared error) of the Gaussian process for a CSBM with parameters \(N=20\), \(d=5\), \(=1\), \(=1\) and \(=4\). The shift operator is defined in (6) with \(g=0.1\), other parameters are \(_{b}^{2}=0\), \((x)=(}{2}x)\) and \(_{ro}=0.01\). In all panels we use \(N^{}=10\) training nodes and \(N^{}=10\) test nodes, five training nodes from each of the two communities. Labels are \( 1\) for the two communities, respectively. For all panels, we show averages over \(50\) CSBM instances. **a)** Heatmap of the generalization error of the GCN GP dependent on number of layers \(L\) and weight variance \(_{w}^{2}\). The red line shows the transition to the non-oversmoothing phase. **b)** Generalization error dependent on weight variance \(_{w}^{2}\) and depths \(L=1,4,16,64,256,1024\) from turquoise to dark blue. **c)** Generalization error dependent on the layer for the GCN GP at the critical line \(_{w}^{2}=_{w,}^{2}\), in the oversmoothing phase \(_{w}^{2}=_{w,}^{2}-1\) and the non-oversmoothing phase \(_{w}^{2}=_{w,}^{2}+1\). **d)** Performance of randomly initialized finite-size GCNs with \(d_{l}=200\) for \(l=1,,L\) where only the linear readout layer is trained with gradient descent (details in Appendix F) at the critical line \(_{w}^{2}=_{w,}^{2}\), in the oversmoothing phase \(_{w}^{2}=_{w,}^{2}-1\) and the non-oversmoothing phase \(_{w}^{2}=_{w,}^{2}+1\).

Discussion

In this study we used the equivalence of GCNs and GPs to investigate oversmoothing, the property that features at different nodes converge over layers to the same feature vector in an exponential manner. By extending concepts such as the propagation depth and chaos from the study of conventional deep feedforward neural networks , we are able to derive a condition to avoid oversmoothing. This condition is phrased in terms of an eigenvalue problem of the linearized GCN GP dynamics around the state where all features are the same: This state is stable if all eigenvalues are smaller than one, thus the networks do oversmooth. If one eigenvalue, however, is larger than one, the state where the features are the same on all nodes becomes unstable. While most GCNs studied in the literature are in the oversmoothing phase [2; 45], the non-oversmoothing phase can be reached by a simple tuning of the weight variance at initialization. An analogy can be drawn between the chaotic phase of DNNs and the non-oversmoothing phase of GCNs. Previous theoretical works have proven that oversmoothing is inevitable in some GNN architectures, among them GCNs; these works, however, make crucial assumptions on the weight matrices, constraining their variances to be in what we identify as the oversmoothing phase. Near the transition, we find GCNs which are both deep and expressive, matching the originally reported GCN performance  on the Cora dataset with GCN GPs beyond \(100\) layers.

**Limitations.** The current analysis is based on the equivalence of GCNs and GPs which strictly holds only in the limit of infinite feature dimension. GCNs with large feature vectors (\(d_{l}=200\)) are well described by the theory, as shown in Section 4. For a small number of feature dimensions, however, we expect deviations from the asymptotic results. Throughout the main part of this work, we assumed a row-stochastic shift operator which made the equilibrium \(K^{}\) in the oversmoothing phase particularly simple. For other shift operators, we expect qualitatively similar results while the equilibrium \(K^{}\) may look different in detail. In our preliminary experiments on the common shift operator from  (Appendix E), we indeed find that increasing the weight variance increases the distances between features also in this case. We hypothesize that this effect makes the equilibrium more informative of the graph topology, as in the stochastic shift operator case. The choice of non-linearity is unrestricted, but in the general case numerical integration of (8) is needed.

To determine whether a given weight variance is in the non-oversmoothing phase, one calculates the eigenvalues of the linearized GCN GP dynamics which take the form of an \(N^{2} N^{2}\) matrix (see Equation (16)), this has a run time of \((N^{6})\). While this becomes computationally expensive for large graphs, the conceptual insights of the presented analysis remain. In practical applications with large graphs one may reduce the computational load by determining the transition point via computation of the GCN GP prior until it is close to equilibrium. This procedure has a runtime of \((N^{3}L^{})\) where \(L^{}\) is the number of layers after which the process is sufficiently close to equilibrium. One might then do an interval search on the weight variance until the transition point is determined with sufficient accuracy.

**Oulook.** Formulating GCNs with the help of GPs can be considered the leading order in the number of feature space dimension \(d_{l}\) when approximating finite-size GCNs. Computing corrections for finite numbers of hidden feature dimensions would allow the characterization of feature learning in such networks, similar as in standard deep networks [26; 48; 39]. Moreover, the generalization of this formalism to more general shift operators and other GNN architectures [49; 4] like GATs  are possible directions of future research. In the special case of GATs we expect similar results to the GCN analyzed here, since the shift operator is constructed using a softmax and therefore also is row-stochastic.