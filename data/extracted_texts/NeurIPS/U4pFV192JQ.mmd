# Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning

Chengliang Liu1, Jie Wen1, Yabo Liu1, Chao Huang2, Zhihao Wu1,

Xiaoling Luo3, Yong Xu1

Corresponding author

###### Abstract

Multi-view learning has become a popular research topic in recent years, but research on the cross-application of classic multi-label classification and multi-view learning is still in its early stages. In this paper, we focus on the complex yet highly realistic task of incomplete multi-view weak multi-label learning and propose a masked two-channel decoupling framework based on deep neural networks to solve this problem. The core innovation of our method lies in decoupling the single-channel view-level representation, which is common in deep multi-view learning methods, into a shared representation and a view-proprietary representation. We also design a cross-channel contrastive loss to enhance the semantic property of the two channels. Additionally, we exploit supervised information to design a label-guided graph regularization loss, helping the extracted embedding features preserve the geometric structure among samples. Inspired by the success of masking mechanisms in image and text analysis, we develop a random fragment masking strategy for vector features to improve the learning ability of encoders. Finally, it is important to emphasize that our model is fully adaptable to arbitrary view and label absences while also performing well on the ideal full data. We have conducted sufficient and convincing experiments to confirm the effectiveness and advancement of our model.

## 1 Introduction

With the increasing popularity of multi-view learning, researchers have discovered that heterogeneous multi-source features obtained from multiple sensors or techniques can provide a richer and more diverse description of samples [1; 2; 3; 4]. For example, features can be extracted using scale-invariant feature transform (SIFT) operator and deep neural networks from different perspectives of an image, or features can be extracted from multimedia data, such as images and texts, to express higher-level semantic objects [5; 6; 7]. Many multi-view learning methods are based on the assumptions of multi-view semantic consistency and feature complementarity [8; 9]. They aim to obtain unique sample labels from multi-source features to maintain the unity of multiple views at the semantic level and to leverage the unique advantages of each view to compensate for the limitations of single-view observation [10; 11; 12]. For instance, Nie et al. emphasized the importance of learning a cross-view similarity matrix in their work . Li et al. proposed to learn a common graph in the spectral embedding space for final clustering . Han et al. attempted to compute confidence scores of samples on each view to facilitate classification predictions across multiple views . Lin etal. proposed dual contrastive prediction for incomplete multi-view representation learning, which performs contrastive learning at both the instance level and category level to ensure cross-view consistency and recover missing information . However, the instance-level contrastive loss of this method only aggregates cross-view representations in the potential space from the perspective of consistency, ignoring the view-private information.

On the other hand, as a classical classification problem, multi-label classification has occupied an important area of pattern recognition for a long time . However, under the upsurge of multi-view learning, new composite problem, i.e., multi-view multi-label classification (MvMLC), are attracting increasing attentions from researchers . As the name suggests, MvMLC can be seen as a supervised subtask within the scope of multi-view learning and can also be considered as an extension of traditional single-view multi-label classification with respect to feature diversity. As a result, existing MvMLC methods not only take into account the basic characteristics of multi-view learning but also consider the requirements of downstream multi-label classification . For example, Zhao et al. proposed a framework called CDMM that mines multi-view consistency and diversity information, which uses the Hilbert-Schmidt Independence Criterion  to enhance the diversity of each view and obtain consistent classification results through late fusion . Another advanced non-aligned MvMLC model proposed by Zhao et al. is LVSL (learn view-specific labels), which not only considers the local geometric structure in the original feature space but also introduces a learnable label correlation matrix with low-rank structure .

However, these ideal MvMLC methods ignore the possibility of missing multi-view features and labels, making it difficult to achieve good performance on incomplete multi-view weak multi-label data, in which sample's partial views and tags are unknown . In recent years, researchers have proposed some incomplete multi-view weak multi-label classification (iMvWMLC) frameworks to address the adverse effects caused by missing views and labels. For instance, Tan et al. proposed an incomplete multi-view weak label learning model (iMvWL) that decomposes multi-view features into a latent common representation and multiple view-specific basis matrices, and connects the latent representation with labels through a mapping matrix . Additionally, two prior missing indicator matrices are introduced to skip the missing views and labels. Recently, Li et al. developed a non-aligned iMvWMLC model based on traditional matrix factorization, named NAIM3L, to align multiple views in the label space and impose global high-rank and local low-rank constraints on the predicted multi-label matrix . For the double-missing problem, Li et al. introduced a composite indicator matrix for NAIM3L, containing view and label missing information. Another representative deep iMvWMLC framework is DICNet proposed by Liu et al., which applies the contrastive learning technique to aggregate cross-view instances of the same sample and separate instances belonging to different samples . Although DICNet has achieved remarkable improvement, there is a non-negligible category collision problem, i.e., instances belonging to similar samples will be forcibly separated due to this contrastive learning mechanism.

To tackle these issues, we present the **M**asked **T**wo-channel **D**ecoupling framework (**MTD** for short), capable of handling cases where partial views and labels are both missing. The motivation behind this framework is that we both want the underlying representations of the same sample in multiple views to be consistent, while also wanting each view to display its own characteristics, creating a difficult balancing act . This is especially challenging when each view of the same sample is represented by only one feature vector. To address this, we explicitly decouple each view's latent feature into two types of features, i.e., _shared_ feature and _view-proprietary_ feature, via two-channel encoders. We also propose a novel cross-channel contrastive loss that narrows the distance between positive pairs (shared instances in different views of the same sample) and expands the distance between negative pairs (any cross-view and channel instance pairs of a sample, except positive pairs). Note that our cross-channel contrastive loss does not involve the comparison among samples, so it can well avoid the above class conflict problem and improve the framework's feature decoupling ability. Additionally, we draw inspiration from famous image patch masks  and introduce random mask fragments in the input data to guide the encoder's learning of valuable information. Lastly, we design a weak label-guided graph regularization loss to preserve the geometric structure in the embedding space, accounting for the non-equidistance among samples in label space. In summary, our paper makes three key contributions:

1. We propose a novel masked two-channel decoupling framework (MTD) for the iMvWMLC task, which extracts shared and private features for each view separately and enforces this design with a cross-channel contrastive loss. At the same time, this framework is compatible with arbitrary incomplete multi-view and weak multi-label data.
2. To our best knowledge, we are the first to apply random fragment mask in the field of multi-view learning and achieve significant performance gains, which supports a new multi-view vector data enhancement mechanism for the communication.
3. Different from existing graph regularization based approaches, we utilize supervised label information to build a more reliable topological graph, inducing the embedding features extracted by the encoders to preserve the geometric structure among samples.

## 2 Method

In this section, we comprehensively introduce our model from the following four aspects, namely, two-channel decoupling framework, weak label-guided graph regularization, random fragment masking mechanism, and multi-label classification. First, for the convenience of description, we briefly introduce the formal problem definition and common notations.

### Problem definition

In this section, we define the iMvWMLC task as follows: Given multi-view multi-label data \(\{\{^{(v)}\}_{v=1}^{m},\}_{v=1}^{(m)}\), where \(^{(v)}\) is \(v\)-th view's feature matrix containing \(n\) samples and \(m\) is the number of views. The \(i\)-th row of \(^{(v)}^{n d_{v}}\) denotes the instance of \(i\)-th sample on \(v\)-th view with dimension \(d_{v}\). \(\{0,1\}^{n c}\) is the label matrix with \(c\) categories and \(_{i,j}=1\) denotes the \(i\)-th sample is tagged as \(j\)-th class. For missing views, we introduce \(}\{0,1\}^{n m}\) as the missing-view index matrix, where \(_{i,j}=1\) represent \(j\)-th view of \(i\)-th sample is available, otherwise \(_{i,j}=0\). For missing labels, we let \(\{0,1\}^{n c}\) be the missing-label index matrix, whose element \(_{i,j}=1\) means \(i\)-th sample's \(j\)-th label is known, otherwise \(_{i,j}=0\). Finally, we fill the unavailable views and the unknown labels with '0' in the data pre-processing step to get final incomplete multi-view feature \(\{^{(v)}\}_{v=1}^{m}\) and weak label matrix \(\). Our goal is to train a neural network that can perform multi-label classification reasoning on incomplete multi-view data. Subscript access \(_{i,j}\), \(_{i,:}\), and \(_{:,j}\) mean the element, row, and column of any matrix \(\).

### Two-channel decoupling framework

As we know, due to the diversity of multi-view data acquisition methods, each view of the original data may have different feature dimensions, which is not conducive to the parallel execution of deep networks. To this end, we map the heterogeneous raw data into a unified embedding space with dimension \(d_{e}\) through stacked encoders. Different from conventional deep multi-view networks, we

Figure 1: Main architecture of our MTD. Masked data \(\{^{(v)}\}_{v=1}^{m}\) is real input of the MTD. And the cross-channel contrastive loss aims to enhance the semantic of ‘shared-proprietary’ channels.

employ two groups of multi-layer perceptrons as two channels, shared channel and view-private channel, to extract shared information and view-proprietary information from raw data, respectively, i.e., \(\{_{v}^{S}:^{(v)}^{(v)}\}_{v=1}^ {m}\) and \(\{_{v}^{O}:^{(v)}^{(v)}\}_{v=1}^ {m}\). \(_{v}^{S}\) and \(_{v}^{O}\) denote the shared feature encoder and view-proprietary feature encoder of view \(v\), respectively. \(^{(v)}\) represents the masked input (See Section 2.4). \(^{(v)}^{n d_{e}}\) and \(^{(v)}^{n d_{e}}\) are the extracted multi-view shared feature matrix and view-proprietary feature matrix of \(n\) samples. The architectures of these two channels are the same but serve different purposes. Specifically, \(_{v}^{S}\) tries to mine common features across views, which preserve the sample's basic properties held by all views, while \(_{v}^{O}\) focuses on extracting features unique to each view. This design decomposes the discriminative information of each view into two parts, which can satisfy the multi-view consistency and complementarity assumptions at the same time. Of course, it is difficult to achieve the above purposes with only two encoding channels due to the lack of guidance, so we propose a multi-view cross-channel contrastive loss \(_{ccc}\) to guide the separation of these two kinds of features as follows:

\[_{ccc}=_{i=1}^{n}-N}2_{ u=1}^{m}_{v=1}^{m}_{[]}s_{i}^{(u)}, o_{i}^{(v)}^{2}+_{u=1}^{m}_{v u}^{m} _{[]}o_{i}^{(u)},o_{i}^{(v)}^{2} }{-N}_{u=1}_{v=u}^{m}_{ []}s_{i}^{(u)},s_{i}^{(v)}+1 /2},\] (1)

where \(\) is the similarity measure function: \((x,y)=y}{\|x\|_{2}\|y\|_{2}}\) and \(_{[]}\) is the condition function: \(_{[]}=1\) if condition \(\{:_{i,u}_{i,v}=1\}\) is true, 0 otherwise. \(N=_{u,v}_{i,u}_{i,v}\) denotes the number of valid instance pairs. That is to say we only consider instance pairs that are not missing on both sides. \(s_{i}^{(v)}\) and \(o_{i}^{(v)}\) are features corresponding to \(i\)-th sample in the shared feature \(^{(v)}\) and view-proprietary feature \(^{(v)}\), respectively. According to Eq. (1), our cross-channel contrastive loss consists of two parts, namely, the numerator is the mean of negative pairs' similarities and the denominator is that of positive pairs' similarities. To be more specific, we pair instances from \(2N\) channels together, where positive pairs consist of shared features from different views, and the remaining pairs are considered negative. We aim to maximize the similarity between the positive pairs of shared features and minimize the similarity between the negative pairs. This approach fulfills the design objective of the dual-channel model, which is to encourage consistency between shared features from different views while maintaining a clear distinction between each view's view-proprietary feature and other shared or view-proprietary features.

To further enhance the feature extraction capability of the encoders, we introduce stacked decoders to restore the embedding features extracted by the two-channel encoders to original feature space, i.e., \(\{_{v}:^{(v)}^{n d_{e}} }^{(v)}^{n d_{v}}\}_{v=1}^{m}\), where \(_{v}\) is the decoder corresponding to \(v\)-th view. \(}^{(v)}\) is the reconstructed feature. Finally, we measure the reconstruction quality using the weighted mean square error loss:

\[_{re}=_{v=1}^{m}_{i=1}^{n} {d_{v}}}_{i,:}^{(v)}-_{i,:}^{(v)}_{2} ^{2}_{i,v},\] (2)

where \(\) is introduced to mask the unavailable instances. With the \(^{(v)}\) and \(^{(v)}\) on each view, we can naturally perform cross-view fusion to obtain the unique shared representation \(}\) and \(}\) of all samples [28; 20]:

\[}_{i,:}=_{v=1}^{m}_{i,:}^{(v)} _{i,v}}{_{v}_{i,v}},}_{i,:}= _{v=1}^{m}_{i,:}^{(v)}_{i,v}}{_{v}_{i,v}}.\] (3)

Successively, we try to combine shared information and proprietary information to learn consistent representations of samples. Here, unlike the commonly used addition or connection operations, we adopt a novel feature interaction approach, i.e., enhancing the shared information via the proprietary information:

\[_{i,j}=(}_{i,j})}_{i,j},\] (4)

where \(\) is sigmoid activation function and \(^{n d_{e}}\) is the final fusion representation.

### Weak label-guided graph regularization

Many unsupervised multi-view learning methods apply graph regularization technique to maintain the intrinsic structure of data, based on the assumption that two similar samples in the original feature space should also be similar in the latent space, which has shown good performance in unsupervised scenarios [29; 30; 31]. When transferring this assumption to multi-label classification tasks with the characteristics of supervised tasks, we can get a new assumption: two similar samples in the label space should also be similar in the embedding space. Based on this, we attempt to guide the feature extraction process of the encoders by imposing a graph constraint with the supervised information. First, we calculate similarity matrix according to the weak label matrix \(\):

\[_{i,j}=_{i,j}(y_{i}y_{j}^{T})}{_{i,j }(y_{i}y_{j}^{T})+},\] (5)

where \(^{n n}\) is the sample similarity graph, describing the similarity between any two samples. \(_{i,j}=_{i,j}_{j,i}^{T}\) is the number of labels available for both samples \(i\) and \(j\). \(y_{i}\) and \(y_{j}\) denote the \(i\)-th and \(j\)-th rows of \(\). And \(\) is a constant, empirically set to 100 for simplicity. Taking into account the fact that \(C_{ij}\) is much larger than \(y_{i}y_{j}^{T}\) on most datasets, when the number of categories is large, even if two samples only have one same label, it means that they are much more similar than other sample pairs.

With such a similarity graph, we can constrain the distance between any two samples in the embedding feature space to achieve the purpose of structure preservation by following loss:

\[_{gc}=}_{i=1}^{n}_{j=1}^{n}\|_{i,i}-_{j,:}\|_{2}^{2}_{i,j}.\] (6)

In view of Eq. (6), the more similar two samples are, the greater the penalty weight they are imposed. In order to improve the computational efficiency of the model in the GPU, we rewrite Eq. (6) in the form of matrix product:

\[_{gc}=}Tr(^{T}),\] (7)

where \(Tr()\) is the trace operation. \(\) denotes the Laplacian matrix calculated by \(=-\), where \(\) is a diagonal matrix with diagonal element \(_{i.i}=_{j=1}^{n}_{i,j}\).

### Masking random fragment of feature

As we all know, the original data contains massive redundant information, which inevitably interferes with the feature extraction process of the model. Inspired by the well-known masked autoencoders (MAE)  that randomly mask image patches, we attempt to apply a masking mechanism to the input multi-view vector data. To this end, for any view \(v\), we initialize a matrix \(^{(v)}\{0,1\}^{n d_{v}}\) whose elements are all 1 at the same size as the raw data \(^{(v)}\), and randomly sample \(n\) integers \([b_{1},b_{2},...,b_{n}]\) from the range of \([1,d_{v}-l]\) as the mask starting point of each instance in view \(v\), where \(l\) is the mask length. Then the \(b_{i}\)-th to \((b_{i}+l)\)-th bits of feature \(_{i,:}^{(v)}^{d_{v}}\) are set to 0 to get the final mask matrix \(^{(v)}\). This matrix stores consecutive mask segments starting at random, enabling local deactivation of feature vectors. We use \(m\) mask matrices to mask the original input data, and get corresponding masked feature matrices as the input of the MTD:

\[\{^{(v)}\}_{v=1}^{m}=\{^{(v)}^{(v)} \}_{v=1}^{m},\] (8)

where \(\) means the element-wise multiplication.

At first glance, this is similar to MAE masking random image patches to encourage the model to learn useful features. The difference is that our MTD only partially masks the input features but does not do anything to the encoders, i.e., the encoders still works as if processing the unmasked data. In addition, in our model, we uniformly set the mask rate \(=l/d_{v}\) to 0.25 for all datasets instead of a larger masking rate like MAE, because we notice the difference that the information density of vector data is much larger than that of image data. Extensive experiments are performed in Section 3.5 to verify the effectiveness of this masking approach.

### Weighted multi-label classification and overall loss function

With the fused embedding representation \(\), a fully connected layer is employed as a classifier to map \(\) to the label space to get the prediction result:

\[=(+),\] (9)where \(^{2d_{e} c}\) and \(^{n c}\) are learnable parameters of classifier and \(^{n c}\) is our predicted score matrix. In single-label classification tasks, the cross-entropy loss is usually adopted to guide the learning of model, while in multi-label classification tasks, the prediction of each category is regarded as an independent binary classification problem. Besides, considering the unknown labels in label matrix, we use following multi-label cross-entropy loss as our main classification loss:

\[_{mc}=-_{i=1}^{n}_{j=1}^{c}(_{i,j} _{i,j})}+(1-_{i,j})(1-_{i,j}))_{i,j},\] (10)

where \(\) is introduced to mask the unknown label in the calculation of \(_{mc}\).

Combined with cross-channel contrastive loss Eq. (1), reconstructed loss Eq. (2), graph embedding loss Eq. (7), and weighted multi-label classification loss Eq. (10), our total loss function can be expressed as:

\[_{all}=_{mc}+_{gc}+_{ccc }+_{re},\] (11)

where \(\), \(\), and \(\) are corresponding penalty parameters. We show the detailed training process in algorithm 1.

```
0: Incomplete multi-view data \(\{^{(v)}\}_{v=1}^{m}\), missing-view index matrix \(\), weak label \(\), missing-label index matrix \(\).
0: Prediction results \(\).
1: Initialize model parameters and set hyperparameters (\(\), \(\), \(\), learning rate, and training epochs \(e\)).
2: t=0.
3:while\(t<e\)do
4: Construct random feature mask matrices \(\{^{(v)}\}_{v=1}^{m}\).
5: Compute masked input data \(\{^{}{}^{(v)}\}_{v=1}^{m}\) by Eq. (8).
6: Extract shared embedding feature \(\{^{(v)}\}_{v=1}^{m}\) and view-private embedding feature \(\{^{(v)}\}_{v=1}^{m}\) by two-channel encoders \(\{_{v}^{v}\}_{v=1}^{m}\) and \(\{_{v}^{(v)}\}_{v=1}^{m}\), respectively.
7: Compute fused shared embedding feature \(\) and fused view-proprietary feature \(}\) by Eq. (3).
8: Compute final fused representation \(\) by Eq. (4).
9: Compute similarity graph \(\) by Eq. (5) and corresponding Laplacian matrix \(\).
10: Compute cross-channel contrastive loss \(_{ccc}\) by Eq. (1), reconstruction loss \(_{re}\) by Eq. (2), and graph embedding loss \(_{gc}\) by Eq. (7).
11: Obtain predictions \(\) by Eq. (9) and compute classification loss \(_{mc}\) by Eq. (10).
12: Compute total loss \(_{all}\) by Eq. (11).
13: Update network parameters.
14:\(t=t+1\).
15:endwhile ```

**Algorithm 1** Training process of **MTD**

## 3 Experiments

### Datasets and metrics

Consistent with existing iMvWMLC methods [26; 25; 20], we adopt five famous multi-view multi-label datasets2 to validate our model, i.e., **Corel5k**, **Pascal07**, **ESPGame**, **IAPRTC12**, and **MIRFLICKR**. Six kinds of features are extracted from these datasets as their six views, namely GIST, HSV, Hue, Sift, RGB, and LAB feature. The number of samples in these datasets ranges from 4999 to 25000, and the number of categories ranges from 20 to 291. Please refer to the appendix for more detailed information about the five datasets.

Following plenty of existing works [26; 25; 23], six metrics are selected to evaluate all comparison methods, i.e., Ranking Loss (**RL**), Average Precision (**AP**), Hamming Loss (**HL**), adapted area under curve (**AUC**), OneError (**OE**), and Coverage (**Cov**). To compare performance more intuitively, we actually report results with respect to **1-HL**, **1-RL**, **1-OE**, and **1-Cov**, so for all metrics, higher values indicate better performance.

### Incomplete multi-view weak multi-label dataset setting

Analogous to previous works [26; 25; 20], we manually construct incomplete multi-view weak multi-label datasets based on above five complete datasets to evaluate the performance of all methods in the case of missing views and missing labels. Concretely, 50% of instances on each view are randomly selected as unavailable instances, which are replaced by '0' value. And we ensure that there is no invalid sample in the dataset, i.e., each sample holds at least one unmissing view. For the weak labels, we set half of the positive and negative tags for each category to be unknown. 70% of samples with missing views and missing labels are randomly selected as the training set. The construction of incomplete data is repeated many times to reduce the randomness of experiment.

### Comparison methods

In order to evaluate the advancement of our framework, eight top comparison methods are selected in our experiment, i.e., **C2AE**, **GLOCAL**, **CDMM**, **DM2L**, **LVSL**, **NAIM3L**, **iMVWL**, and **DICNet**, to compare with our MTD on the five incomplete multi-view weak multi-label datasets. To date, few MVMLC methods can handle missing views and weak labels at the same time, thus we introduce some related multi-label classification methods in our experiments. To be more specific, of the nine methods, iMVWL, NAIM3L, DICNet, and our MTD are perfectly applicable to the iMVWLC task. C2AE, GLOCAL, and DM2L are single-view multi-label classification methods that can deal with incomplete labels, so we conduct these methods on each view and record the best result. CDMM and LVSL are the MvMLC methods unable to handle any data missing, so we simply complete the missing views using the mean of available instances and fill the unknown tags with '0'. The statistical information of competitors is shown in the appendix.

### Experimental results and analysis

To evaluate the performance of our method in scenarios with missing views and labels, we compare it against eight state-of-the-art algorithms on five datasets. The mean and standard deviation (decimal form in the bottom right) of the results are reported in Table 1. In addition to the six performance metrics mentioned above, we also calculate the average ranking of each algorithm based on these

Figure 2: Experimental results of nine methods on the five full databases without any missing views or labels. The center of the radar map shows the worst results and the vertexes mean the best results on the six metrics.

[MISSING_PAGE_FAIL:8]

ranks 1st on the other four datasets. Whether compared with traditional methods (such as GLOCAL and NAIM3L) or deep methods (such as C2AE and DICNet), our MTD shows excellent compatibility with missing views and weak labels.
2. We can find that methods based on deep neural networks generally perform better than traditional methods. Specifically, our MTD, DICNet, and CDMM are consistently ranked in the top 3 across all five datasets, which can be attributed to the strong feature extraction capabilities of deep encoders and the flexible multi-view fusion strategy they employ.
3. In the comparison among traditional methods, the method that is specifically designed for double incompleteness shows obvious advantages compared to other methods. This indicates the necessity of considering possible missing views and labels during the model design process.

In addition to experimenting on datasets with 50% missing views and missing labels, we also perform comparison experiments on the full dataset without any missing data and use radar charts to show the results in Fig. (2). For more intuitive observation, we set the range for each axis to the min-max values of the nine methods on the corresponding metric. It can be seen from Fig. (2) that the performance of our method on the complete datasets is still outstanding, which verifies the good adaptation of it.

To demonstrate that our cross-channel contrastive loss works as expected, we plot a random sample's channel similarity heat maps across all channels in different training epochs on the Core15k dataset with half of missing views and labels in Fig. (3). Each patch of the heat map shows the cosine similarity of corresponding two instances, i.e., \(i\)-th sample's heat map \(_{i}=^{T}\), where \(=(s_{i}^{(1)},s_{i}^{(2)},...,s_{i}^{(m)},o_{i}^{(1)},o_{i}^{(2)},...,o_{i}^{(m)})^{T}^{2m d_{e}}\). \(s_{i}^{(1)}^{d_{e}}\) and \(o_{i}^{(1)}^{d_{e}}\) are the instances on shared and view-proprietary channels of first view of sample \(i\), respectively. In Fig. (3), it is evident that in the initial state, the similarities between different channels are relatively small. However, as training progresses, the similarities between instance pairs on the first \(m\) shared channels increase rapidly, while the similarities for the remaining channels are close to zero. This observation indicates that our cross-channel contrastive loss is functioning as expected. Additionally, we notice that there is a shared feature on channel 3 that is not well-aggregated. This may be due to the third view of this sample sharing less information with the other views.

### Ablation study

Our model contains multiple loss functions and strategies, in order to further study the effectiveness of each component, we perform extensive ablation experiments on the Core15k dataset with 50% missing labels and views, and report results in Table 2. Specifically, we take the common single-channel model [20; 40] as a baseline architecture and extend it to a two-channel baseline framework. We iteratively add our individual loss functions \(_{gc}\), \(_{re}\), and \(_{ccc}\) into it and remove the masking operation from different versions. From Table 2, we observe that all designs play a role to varying degrees, especially the fragment masking mechanism's performance increase on the Core15k dataset is the most noticeable. This confirms that the random masking mechanism to raw data, commonly used in the image and text analysis domain, is also effective in the field of multi-view learning. However, this strategy still needs to be further studied for the underlying mechanism of action.

   Method & AP & 1-HL & 1-RL & AUC & 1-OE & 1-Cov \\  s\_ch baseline & 0.396 & 0.988 & 0.888 & 0.891 & 0.469 & 0.738 \\ d\_ch baseline & 0.408 & 0.988 & 0.890 & 0.893 & 0.482 & 0.744 \\ d\_ch+\(_{gc}\) & 0.409 & 0.988 & 0.890 & 0.893 & 0.488 & 0.744 \\ d\_ch+\(_{re}\)+\(_{gc}\) & 0.413 & 0.988 & 0.890 & 0.894 & 0.487 & 0.746 \\
**MTD** & 0.415 & 0.988 & 0.893 & 0.896 & 0.491 & 0.750 \\ MTD _w/o_ mask & 0.397 & 0.987 & 0.888 & 0.891 & 0.466 & 0.738 \\ s\_ch _w/o_ mask & 0.372 & 0.987 & 0.881 & 0.884 & 0.449 & 0.721 \\ d\_ch _w/o_ mask & 0.387 & 0.987 & 0.885 & 0.889 & 0.452 & 0.732 \\   

Table 2: Ablation results of our MTD on Core15k dataset with 50% missing views, 50% missing labels. ‘s_ch’ denotes the conventional single-channel network and ‘d_ch’ denotes our double-channel framework. ‘_w/o_’ means ‘without’.

### Implementation details

Our TMD is implemented by Pytorch and MindSpore frameworks on Ubuntu operating system with a single RTX 3090 GPU and an i7-12900k CPU. The learning rate is set to \(0.1\) and the Stochastic Gradient Descent (SGD) optimizer is chosen for training model. The batch size and momentum are 128 and 0.9 for all five datasets. Please refer to the appendix for the setting of other hyperparameters. The code can be found at https://justsmart.github.io/.

## 4 Conclusion

In this paper, we propose a novel masked two-channel decoupling framework (MTD) for the iMvWMLC task. Our MTD decouples single-channel features commonly used in previous methods into two-channel features for consensus and complementarity learning, and we design a cross-channel contrastive loss for this setting. Additionally, we employ a random fragment masking strategy during the training phase to reduce redundancy of raw data, inspired by image and text masking strategies, which leads to an obvious performance boost. Furthermore, we introduce a label-guided graph constraint approach to ensure that the learned embedding features maintain structure information among samples. Our extensive comparison and ablation experiments demonstrate that MTD outperforms other advanced methods and is compatible with arbitrary incomplete multi-view and weak multi-label data. Looking back at our work, although we propose many strategies and techniques to advance the frontier research progress of iMvWMLC, the complexity of this issue leaves room for further optimization, such as considering multi-label correlation and missing data recovery as directions of future works.