# Externally Valid Policy Evaluation from Randomized Trials Using Additional Observational Data

Sofia Ek

Uppsala University

sofia.ek@it.uu.se&Dave Zachariah

Uppsala University

dave.zachariah@it.uu.se

###### Abstract

Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. Additional covariate data from the target population is used to model the sampling of individuals in the trial study. We develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. The method is nonparametric and the validity is assured even with finite samples. The certified policy evaluations are illustrated using both simulated and real data.

## 1 Introduction

Randomized controlled trials (rct) are often considered to be the 'gold standard' when evaluating the effects of different decisions or, more generally, decision policies. rct studies circumvent the need to identify and model potential confounding variables that arise in observational studies. They enable the evaluation and learning of decision policies for use in, e.g., clinical decision support, precision medicine and recommendation systems (Qian and Murphy, 2011; Zhao et al., 2012; Kosorok and Laber, 2019).

However, rct sample individuals that may differ systematically from a target population of interest. For instance, clinical trials usually involve only individuals who do not have any relevant comorbidities and those who volunteer for trials may very well exhibit different characteristics than the target population. Invalid inferences about a decision policy can be potentially harmful in safety-critical applications, where the cautionary principle of "above all, do no harm" applies (Smith, 2005). This is especially challenging since the distributions of population characteristics are unknown. How can we _generalize_ results from the trial sample to the intended population?

The focus of this paper is the problem of establishing _externally_ valid inferences about outcomes in a target population, when using experimental results from a trial population (Campbell and Stanley, 1963; Manski, 2007; Westreich, 2019). We consider evaluating a decision policy, denoted \(\), that maps covariates \(X\) of an individual onto a recommended action \(A\). The outcome of this decision has an associated loss \(L\) (aka. disutility or negative reward). Thus \(L\) may directly represent the outcome of interest. We assume the availability of samples \((X,A,L)\) from the trial population and _additional_ covariate data \(X\) from the target population (Lesko et al., 2016; Li et al., 2022; Colnet et al., 2024). The covariate data is used to model the sampling of individuals in the trial study. We propose a method for evaluating policy \(\) that

* is nonparametric and makes no assumptions about the distributional forms of the data,
* takes into account possible covariate shifts from trial to target distribution, even when using miscalibrated sampling models with unmeasured selection factors,* and certifies valid finite-sample inferences of the out-of-sample loss, up to any specified degree of model miscalibration.

Many policy evaluation methods are focused on estimating the expected loss \(_{}[L]\) of \(\). However, since a substantial portion of losses \(L\) may exceed the mean, this focus can miss important tail events (Wang et al., 2018; Huang et al., 2021). By contrast, evaluating a policy in terms of its out-of-sample loss provides a more complete characterization of its performance and is consonant with the cautionary principle. Figure 1 illustrates the evaluation of \(\) using limit curves which upper bound the out-of-sample loss \(L\) with a given probability \(1-\). A limit curve based on rct-data alone is only ensured to be valid for a trial population. Using additional covariate data, however, we can certify the validity of the inferences for the target population up to any specified degree of miscalibration of the sampling model.

The rest of the paper is outlined as follows. We first state the problem of interest in Section 2 and relate it to the existing literature in Section 3. We then propose a policy evaluation method in Section 4 and demonstrate its properties using both synthetic and real data in Section 5. We conclude the paper with a discussion about the properties of the method in Section 6 and its broader impact in Section 7.

## 2 Problem Formulation

Any policy \(\), whether deterministic or randomized, can be described by a distribution \(p_{}(A|X)\). Each covariate \(X\), unmeasured selection factor \(U\) and action \(A\) has an associated loss \(L(-,L_{})\). We consider here a discrete action space, i.e. \(A\{1,K\}\). The decision process has a causal structure that can be formalized by a directed acyclic graph, visually summarized in Figure 2(Peters et al., 2017). The sampling indicator \(S\) indicates whether individuals are drawn from a _target_ population, \(S=0\), or a _trial_ population, \(S=1\). The causal structure allows us to decompose the two distributions. Specifically, the target distribution factorizes as

\[p_{}(X,U,A,L|S=0)=p(X,U|S=0) p_{}(A|X) p(L|X,U,A),\] (1)

where only the policy \(p_{}(A|X)\) is known. Similarly, the trial distribution factorizes as

\[p(X,U,A,L|S=1)=p(X,U|S=1) p(A|X) p(L|X,U,A),\] (2)

where only the randomization mechanism \(p(A|X)\) is known. In general the characteristics of target and trial populations may _differ_, that is, \(p(X,U|S=0) p(X,U|S=1)\). The unmeasured \(U\) may include self-selection factors that are challenging to record. Note, however, that only factors that also affect the loss \(L\) are relevant here.

Figure 1: Inferring the out-of-sample losses of a policy \(\). (a) The loss \(L\) is bounded by an upper limit \(_{}\) with a probability of at least \(1-\). The rct-based limit curve uses only trial data, whereas the other limit curves also utilize a sampling model trained using additional covariate data \(X\) from the target population. Each limit curve in blue is certified to provide valid inferences for models miscalibrated up to a degree \(\) defined in (3). (b) Gap between the actual probability of exceeding the limit, \(L>_{}\), and the nominal probability of miscoverage \(\). A negative gap means the inference \(_{}\) is _invalid_, while a positive gap implies it is _valid_ but conservative. Details of the experiment are presented in Section 5.1.

From the trial distribution (2), we sample \(m\) individuals \(=(X_{i},A_{i},L_{i})_{i=1}^{m}\) independently. In addition, we also obtain \(n\) independent samples of _covariate-only_ data \(X_{1},X_{2},,X_{n}\) from the target population (1). Our aim is to infer the out-of-sample loss \(L_{n+1}\) for individual \(n+1\) under any policy \(\). Specifically, we seek a loss limit \(_{}\) as a function of \(1-\), such that \(L_{n+1}_{}\) holds with probability \(1-\) as illustrated by the 'limit curves' in Figure 0(a).

The sampling pattern of individuals is described by \(p(S|X,U)\). This distribution is unknown, but we assume that a model \((S|X)\) is available. This model was fitted using held-out data \(\{(X_{j},S_{j})\}\) employing either the conventional logistic model or any state-of-the-art machine learning models (as exemplified below). It can also be obtained from previous studies. There is, however, no guarantee that \((S|X)\) is calibrated and it may indeed diverge from the unknown sampling pattern. Nevertheless, we want inferences about the out-of-sample loss to be valid also for miscalibrated models. We therefore express the degree of miscalibration in terms of the selection odds:

\[\ \ }}\ \ (S=0|X)}{(S=1|X)}{}}\ \ \ (a.s.)\] (3)

That is, the nominal selection odds can diverge by a factor \(\), where \(=1\) implies a perfectly calibrated model. This model includes all sources of errors (selection bias, model misspecification, estimation error).

A limit \(_{}^{}\) provides an _externally valid_ inference of \(L_{n+1}\), up to any specified degree of miscalibration \(\), if it satisfies

\[_{}L_{n+1}_{}^{}( ) S=0} 1-,.}\] (4)

The problem we consider is to construct this externally valid limit \(_{}^{}\). This limit allows us to infer the full loss distribution of a future individual with \(L_{n+1}\) under policy \(\), rather than merely the expected loss \(_{}[L]\). Specifically, the tail losses are important in healthcare and other safety critical applications where erroneous inferences could be harmful, and a cautious approach when implementing new policies is needed.

The limit curve \(_{}^{}\) for policy \(\) is valid up to any declared degree of odds miscalibration \(\), which establishes the credibility of the policy evaluation, cf. Manski (2003). While \(\) is unknowable, especially under unmeasured \(U\), we can use ideas from sensitivity analysis to guide its selection using measured data (Rosenbaum and Rubin, 1983; Tan, 2006). Following the method in Huang (2024), we treat observed selection factors in \(X\) as unmeasured \(U\) in (3) to benchmark appropriate values for \(\), as detailed in subsection 4.1.

By increasing the range of \(\), we certify the validity of the inference under increasingly credible assumptions on \((S|X)\)(Manski, 2003). As the model credibility increases, however, the informativeness of the inferences decreases. Since the upper bound on the losses, \(L_{}\), is a trivial and uninformative limit, we may define the informativeness of \(_{}^{}\) as

\[=1-\{:_{}^{}()<L_{ }\}.\] (5)

Figure 2: Causal structure of process (a) under policy \(\) as well as (b) the trial study. Sampling indicator \(S\) distinguishes between the two. For the important case of RCT, assignment of \(A\) is not influenced by any covariates so that the path \(X A\) is eliminated.

That is, the right limit of a limit curve, which decreases with the degree of miscalibration. Figure 0(a) shows curves that are valid for miscalibration in the range \(\), where the informativeness is \(95\%\) and \(90\%\), respectively. The latter figure means that we can infer a non-trivial bound on the loss for \(90\%\) of the target population.

_Remark:_ This paper addresses the evaluation of a given policy, whether proposed by a clinical expert or learned from historical data. By setting aside samples from an rct study, a policy \(\) can be learned, and its out-of-sample performance evaluated using the proposed methodology.

## 3 Background Literature

The problem considered herein is related to the problem of causal inference when combining data from randomized controlled trials and observational studies. Examples of the setting where only covariate data from the observational study is available can be found in Lesko et al. (2016) and Li et al. (2022). For additional examples, refer to the survey by Colnet et al. (2024). Within the broader area of generalizability and transportability, the problem represents the case were the sampling probability depends solely on the covariates \(X\), and is independent of the action and the loss (Pearl and Bareinboim, 2014; Lesko et al., 2017; Degtiar and Rose, 2023). The problem is also related to a broader literature of statistical learning under covariate shifts, see for instance Shimodaira (2000); Sugiyama et al. (2007); Quinonero-Candela et al. (2008); Reddi et al. (2015); Chen et al. (2016).

The most common object of inference in policy evaluation is the expected loss \(_{}[L]\) and a popular method for estimating it is inverse probability of sampling weighting (IPSW), which models covariate shifts from trial to target populations. The estimator using rct-data is defined as

\[V_{}^{}=_{i=1}^{m}(S=0|X_{i})}{ (S=1|X_{i})}(A_{i}|X_{i})}{p(A_{i})} L_{i}.\] (6)

This methodology has been applied in various studies, see for example Cole and Stuart (2010); Stuart et al. (2011); Westreich et al. (2017); Buchanan et al. (2018). It is widely recognized that misspecified logistic models can introduce bias when estimating the weights (Colnet et al., 2024) and more recent works have suggested using flexible models, such as generalized boosted methods (Kern et al., 2016), instead. The counterpart to IPSW, used in off-policy evaluation with observational data, is inverse propensity weighting (IPW). The problem with misspecified logistic models also applies here when estimating the classification probabilities, aka. propensity scores (McCaffrey et al., 2004; Lee et al., 2010). In this case, generalized boosted methods and covariate-balancing methods have shown to be promising alternatives (Setodji et al., 2017; Tu, 2019). The literature on IPSW and IPW is mainly focused on average treatment effect estimation, that is \(_{_{1}}[L]-_{_{0}}[L]\) where \(_{1}\) and \(_{0}\) denote the 'treat all' and 'treat none' policies, respectively. By contrast, we want to certify the distributional properties of \(L_{n+1}\) for any \(\), even under miscalibration of \((S|X)\).

Figure 3: (a) Omitting measured selection factors to benchmark credible values for \(\) in (3). (b) Inferred blood mercury levels [\(\)g/L] in a target population under ‘high’ and ‘low’ seafood consumption (\(_{1}\) and \(_{0}\), respectively). Limit curves for degrees of odds miscalibration \(\).

Conformal prediction is a distribution-free methodology focused on creating covariate-specific prediction regions that are valid for finite-samples (Vovk et al., 2005; Shafer and Vovk, 2008). The methodology was extended by Tibshirani et al. (2019) to also work for known covariate shifts. Jin et al. (2023) combined the marginal sensitivity methodology developed in Tan (2006) with the conformal prediction for covariate shifts to perform sensitivity analysis of treatment effects in the case of unobserved confounding. Our methodology draws upon techniques in conformal prediction, but instead of providing covariate-specific prediction intervals under a policy \(\), we are concerned with evaluating _any_\(\) over a target population.

When full identification of the causal effect is not possible due to unmeasured confounders, partial identification sensitivity analysis can be used to evaluate the robustness of the estimates. Rosenbaum and Rubin (1983) introduced a sensitivity parameter to bound the odds ratio of the probability of treatment. More recent work has extended this approach to account for treatment effect heterogeneity and non-binary treatments, as seen in Tan (2006); Shen et al. (2011); Zhao et al. (2019); Dorn and Guo (2023) among others. However, it is often challenging to interpret the absolute value of the sensitivity parameter in practical scenarios. To address this, recent research has proposed benchmarking, or calibrating, results by estimating the effects of unmeasured confounders, see for example Hsu and Small (2013); Franks et al. (2019); Veitch and Zaveri (2020); De Bartolomeis et al. (2024). Note that all these papers work with sensitivity analysis for observational studies. Huang (2024) instead combines sensitivity analysis for generalization with benchmarking to determine reasonable values of \(\).

A biased sample selection, similar to that described by (3), was considered in the context of average treatment effect estimation by Nie et al. (2021). In contrast to that work, our primary focus is on ensuring the validity of inferences regarding out-of-sample losses, even when dealing with finite training data. We achieve this using a sample-splitting technique.

## 4 Method

Here we construct a limit \(_{}^{}()\) on the out-of-sample losses under policy \(\) that satisfies (4) for any given specified degree of miscalibration \(\).

### Benchmarking degree of miscalibration

The limit curve \(_{}^{}()\) holds up to the specified degree of odds miscalibration \(\). Although \(\) is inherently unknown, sensitivity analysis and methods for assessing the calibration of models can guide its estimation using available data.

We start with a benchmarking method that specifically accounts for the potential impact of unmeasured selection factors, \(U\). Building on the approach in Huang (2024), we treat observed selection factors in \(X\) as proxies for unmeasured \(U\) in equation (3), providing a benchmark for selecting suitable \(\) values. Specifically, let the the omitted selection factor \(X_{k}\) be \(U\) and \(X_{-k}\) denotes the remaining covariates. Then the ratio in (3) is estimated by dividing \(}(X_{-k},X_{k})=(S=0|X)}{(S= 1|X)}\) by \(}(X_{-k})=(S=0|X_{-k})}{(S= 1|X_{-k})}\). Figure 2(a) summarizes the distribution of \(}(X_{-k},X_{k})/}(X_{-k})\) using a real data set, where the omitted \(X_{k}\) is either age, income or education. We see that the corresponding ratios all fall within \(=2\). We can therefore conclude that if any unmeasured selection factor \(U\) is weaker than the omitted factors, it is credible to set \(\) in the range of \(1.5\) to \(2\). The corresponding loss curves are shown in Figure 2(b), which illustrates the blood mercury levels in a target population under policies of 'high' respective 'low' seafood consumption that can credibly be extrapolated from trial data. More details are available in Section 5.2. Note that this benchmarking method serves as a guide for assessing the influence of unmeasured selection factors \(U\)(Cinelli and Hazlett, 2019).

Without unmeasured selection factors, methods for assessing the calibration of models \((S|X)\) discussed in, e.g., Murphy and Winkler (1977); Naeini et al. (2015); Widmann et al. (2019) can guide the specified lower bound of the range of miscalibration. The nominal selection odds in (3), i.e., \(}(X)=(S=0|X)}{(S=1|X)}\), can be quantized into several bins and for each bin the unknown selection odds, i.e., \((X)=\), can be estimated by counting the samples from both the target and trial distributions present. In the case of a well-calibrated model, estimated unknown odds should match the quantized nominal odds for each bin. To assess calibration, this process can be iterated across multiple ranges within the dataset and visualized through a reliability diagram, as exemplified in Figure 4. Using (3), we have that

\[}(X)\ \ (X)\ \ }(X).\]

Take the expectation with respect to \(X\), conditional on the nominal odds in a specified interval (or bin) \(I\), so that:

\[[}(X)}(X) I]\ \ [(X)}(X) I]\ \ [}(X)}(X)  I].\]

The expected odds is then estimated for each bin \(I\) by counting samples from the target and trial distributions.

### Inferring out-of-sample limit

We will now construct the limit \(^{}_{}()\). For this we need to describe the distribution shift from trial to target distribution for all samples, including the \(n+1\) sample. We begin by considering the true distribution shift expressed using the ratio

\[(X,U,A,L|S=0)}{p(X,U,A,L|S=1)}.\] (7)

Inserting the factorizations (1) and (2) into this ratio shows that specifying the distribution shift requires the unknown (conditional) covariate distribution \(p(X,U|S)\). We can, however, bound (7) using the model of the sampling pattern, \((S|X)\), as follows:

\[c^{}(X,U,A,L|S=0)}{p(X,U,A,L|S=1)}  c^{},\] (8)

where

\[^{}=(S=0|X)}{ (S=1|X)}(A|X)}{p(A|X)},^{ }=(S=0|X)}{(S=1|X)} (A|X)}{p(A|X)},\] (9)

and \(c=\) is a constant. To see this, we note that the ratio in (7) can be expressed as

\[c(A|X)}{p(A|X)},\]

using Bayes' rule. The bound (8) follows by applying (3). We proceed to show that the factors (9) are sufficient to construct an externally valid limit \(^{}_{}\) for odds divergences up to degree \(\), similar to Ek et al. (2023).

To ensure finite-sample guarantees, the trial data is randomly divided into two sets, \(=^{}^{}\), with respective samples sizes of \(m^{}\) and \(m-m^{}\). The set \(^{}\) is used to construct

\[^{}_{}(^{})= ^{}_{|[(m^{}+1)(1-)]|},&(m^{}+1)(1-) m^{ },\\ ,&,\] (10)

where \(^{}_{|}\) is the upper limit in (9) evaluated over \(^{}\) and ordered \(^{}_{}^{}_{} ^{}_{[m^{}]}\). We show that (10) upper bounds the ratio (7) for a future sample with probability \(1-\) for any choice of \((0,1)\). The set \(^{}\) is used to construct a stand-in for the unknown cumulative distribution function of the out-of-sample loss:

\[(;^{},w)=^{ }}^{}_{i}(L_{i})}{_{i ^{}}[^{}_{i}(L_{i })+^{}_{i}(L_{i}>)]+w},\] (11)

where \(w>0\) is a free variable representing the unknown out-of-sample weight \(^{}_{n+1}\) for a future sample. Based on (10) and (11), define \(^{}_{,}\) as the quantile function

\[^{}_{,}=\{:(;^{ },^{}_{}(^{}))\}.\] (12)

This enables us to construct a valid limit on the future loss \(L_{n+1}\) for any miscoverage probability \((0,1)\).

**Theorem 4.1**.: _For any odds miscalibration up to degree \(\),_

\[_{}^{}()=_{:0<<}_{, }^{},\] (13)

_is an externally valid limit on the out-of-sample loss \(L_{n+1}\) of policy \(\). That is, (13) is certified to satisfy (4)._

The method seeks the level \(\) for the bound (10) that yields the tightest limit. The proof is presented in the supplementary material and builds on several techniques developed in Vovk et al. (2005); Tibshirani et al. (2019); Jin et al. (2023); Ek et al. (2023).

Algorithm 1 summarizes the implementation of a set of limit curves given a model of the sampling pattern \((S|X)\) and a set of miscalibration degrees \([1,_{}]\). Note that (10) and (11) are step functions in \(\) respective \(\). Therefore (12) and (13) can be solved by computing the functions at a grid of points. Calculating (10) and (11) requires the sorting of weights, but the sorting operation is a one-time requirement.

Increasing the degree of miscalibration \(\) results in a decrease in \(^{}\) and an increase in \(^{}\) in (9). As weights associated with lower and higher losses decrease and increase, respectively, in (11), the resulting limit \(_{}^{}()\) becomes more conservative.

_Remark 1:_ If the trial population is a small subgroup of the target population (for example in the case of weak overlap), the nominal odds \((S=0|X)}{(S=1|X)}\) will tend to be high. As this yields a significant weight \(_{}^{}(^{})\), the informativeness (5) of \(_{}^{}()\) diminishes. Nevertheless, the guarantee in (4) holds.

_Remark 2:_ The split of \(\) can be performed in a sample-efficient manner in the case of rct, where actions are randomized so that \(p(A|X) p(A)\): For the \(i\)th sample, \((X_{i},A_{i},L_{i})\), draw \(_{i} p_{}(A|X_{i})\). Include sample \(i\) in \(^{}\) if \(A_{i}=_{i}\), otherwise include it in \(^{}\). This sample splitting method ensures that inferences on the loss are based on actions that match those of the policy.

## 5 Numerical Experiments

We will use both synthetic and real-world data to illustrate the main concepts of policy evaluation with limit curves \((,_{}^{})\). As a performance benchmark, we estimate the quantile - which yields the tightest limit - using the inverse probability of sampling weighting (Colnet et al., 2024)

\[_{}()=:_{}(; ) 1-},\] (14)

where

\[_{}(;)=_{i=1}^{m}(S\!=\!0|X_{i})}{(S\!=\!1|X_{i})}(A_{i}|X_{i})}{p(A_{i})}(L_{i}).\]

Figure 4: Reliability diagram of the observed odds against the average predicted nominal odds obtained from models \((S|X)\).

This is similar to the approach in Huang et al. (2021) but adapted to problems involving data from trial and observational studies.

We examine the impact of increasing the credibility of our model assumptions, i.e., by increasing the miscalibration degree \(\), on the informativeness (5) of the limit curve. In addition, for the simulated data, we also assess the miscoverage gap of the curves

\[=-_{}\{L_{n+1}>_{}()|S{=}0\},\]

where a negative gap indicates an invalid limit.

### Illustrations using synthetic data

We consider target and trial populations of individuals with two-dimensional covariates, distributed as follows:

\[X|S=X_{0,S}\\ X_{1,S}(_{0,S}\\ _{1,S},_{0,S}^{2}&0\\ 0&_{1,S}^{2}), U|S(_{U,S}, _{U,S}^{2}),\] (15)

where the parameters are given in Table 1. The distributions for populations A, B and Trial are taken to be unknown.

The actions are binary \(A\{0,1\}\) and corresponds to 'do not treat' versus 'treat'. We evaluate the 'treat all' policy, i.e. \(p_{_{1}}(A=1|X)=1\). The trial is an rct with equal probability of assignment, i.e., \(p(A) 0.5\). The unknown conditional loss distribution is given by

\[L|(A,X,U)(A X_{0}^{2}+X_{1}+A U+(1-A),1).\]

The sampling probability \(p(S|X,U)\) is treated as unknown. The complete set of hyperparameters used is provided in the supplementary material.

For the observational data \(X_{i}_{i=1}^{n}\), we drew \(n=2000\) samples. For the trial data we drew \(1000\) samples: \(m=500\) samples and \((X_{i},A_{i},L_{i})_{i=1}^{m}\) was used to compute the limit curves. The remaining \(500\) samples were used to train \((S|X)\).

Figure 4(a) compares nominal selection odds obtained from the fitted models with the unknown odds \(p(S=0|X)/p(S=1|X)\) for target population A. We consider two different fitted models \((S|X)\): a logistic model, which is conventionally used in the causal inference literature (Westreich et al., 2017), and the more flexible tree-based ensemble model trained by xgboost(Chen and Guestrin, 2016). In this case, the logistic model happens to be well-specified so that the learned odds approximate the true ones well. The more flexible xgboost model also provide visually similar odds, albeit less accurate. By contrast, Figure 4(b) repeats the same exercise for target population B. Here the logistic model is misspecified and severely miscalibrated while the xgboost model continues to provide

   Population & \(_{0,S}\) & \(_{1,S}\) & \(_{U,S}\) & \(_{0,S}^{2}\) & \(_{1,S}^{2}\) & \(_{U,S}^{2}\) \\  A \((S=0)\) & 0.5 & 0.5 & 0.5 & 1.0 & 1.0 & 1.0 \\ B \((S=0)\) & 0.0 & 0.5 & 0.0 & 1.25 & 1.5 & 1.25 \\  Trial \((S=1)\) & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 \\   

Table 1: Means and variances of covariate distribution \(p(X,U|S)\) in (15).

Figure 5: Odds \(p(S=0|X)/p(S=1|X)\) compared with nominal odds obtained from logistic and xgboost models \((S|X)\). The dots are a random subsample of the trial samples.

visually similar odds. A well-performing and flexible model is required for a meaningful benchmark of the upper value of \(\). Therefore, we will use the xgboost model.

In Figure 4, we use the reliability diagram technique to assess the performance of the xgboost model \(}(X)\) of the nominal odds for target population B. The model is close to the diagonal suggesting that it is sufficiently flexible to accurately model the odds. This is in line with the result in Figure 4(b). We then use observed covariates to calibrate appropriate upper bounds for \(\). Figure 5(a) shows the evaluation with respect to population B. Assuming that the unmeasured \(U\) have no greater selection effect than \(X_{0}\), a degree of miscalibration \(=2\) is a credible choice as it covers more than \(95\%\) of the ratios. Since the data is simulated, we evaluate the miscoverage gap of the limit curves of the 'treat all' policy \(_{1}\) in Figure 5(b) for the benchmark and the limit curves for the proposed method. The gap is estimated using 1000 independent runs and for each run drawing \(1000\) independent new samples \((X_{n+1},U_{n+1},A_{n+1},L_{n+1})\). We see that the benchmark and the limit curve for \(=1\) yields a negative miscoverage gap. As the degree of miscalibration \(\) increases to 2, the limit curves exhibit positive miscoverage gaps.

The evaluation of \(_{1}\) with respect to population A is shown in Figure 1. Results for the logistic model, another policy \(_{0}\), and for additional populations are available in the supplementary material.

### Evaluating seafood consumption policies

To illustrate the application of policy evaluation with real data, we study the impact of seafood consumption on blood mercury levels with data from the 2013-2014 National Health and Nutrition Examination Survey (NHANES). Following Zhao et al. (2019), each individual's data includes eight covariates, encompassing gender, age, income, the presence or absence of income information, race, education, smoking history, and the number of cigarettes smoked in the last month. All covariates, except for smoking history \(U\), are treated as measured and denoted \(X\). We excluded one individual due to missing education data and seven individuals with incomplete smoking data. We impute the median income for 175 individuals with no income information. The continuous covariates - age, income and number of cigarettes smoked in the last month - are standardized. After the preprocessing, our data set comprises 1107 individuals. The data is then split into observational data \(_{0}\) and trial data \(_{1}\) based on the covariates gender, age, income and smoking history (more details are available in the supplementary material) resulting in \(646\) samples in the observational data and \(461\) samples in the trial data. The action \(A\) describes individual fish or shellfish consumption, categorizing an individual as having either low (\(\)1 serving in the past month) or high (\(>\)12 servings in the past month) consumption. The loss \(L\) represents the total concentration of blood mercury (measured in \(\)g/L).

To generate counterfactual actions and losses, we consider a balanced rct so that \(p(A) 0.5\) and learn a model of \(p(L|A,X,U)\) from the data using gradient boosting (Freund and Schapire, 1997;

Figure 6: Evaluating a ‘treat all’ policy \(_{1}\) for target population B. (a) Benchmarking the degree of miscalibration \(\) using omitted covariates. (b) Miscoverage gaps when degree of miscalibration \(\).

Friedman et al., 2000). Thus during training, the trial data consists of samples \((X_{i},A_{i},L_{i})\) whereas the observational data only contains \(X\).

In Figure 2(a) we use observed covariates to benchmark appropriate upper bounds for \(\). We exclude each of the seven covariates one by one, highlighting the three most dominant ones in the figure. If the unmeasured covariates have weaker influence than these, a \(\) value in the range of \(1.5\) to \(2\) is appropriate. In Figure 2(b) we compare the limit curve for a policy \(_{0}\) corresponding to low \((A 0)\) seafood consumption with the limit curve for a policy \(_{1}\) corresponding to high \((A 1)\) seafood consumption. We use an xgboost-trained model \((S|X)\). For reference, a mercury level of 8 \(\)g/L is guidance limit for women of child-bearing age. We see that under a low consumption policy a lower mercury level can be certified for miscalibrated odds \(\). In this case, we can infer a 90% probability that the out-of-sample mercury level falls below the reference value of 8 \(\)g/L.

## 6 Discussion

We have proposed a method for establishing externally valid policy evaluation based on experimental results from trial studies. The method is nonparametric, making no assumptions on the distributional forms of the data. Using additional covariate data from a target population, it takes into account possible covariate shifts between target and trial populations, and certifies valid finite-sample inferences of the out-of-sample loss \(L_{n+1}\), up to any specified degree of model miscalibration.

Conventional policy evaluation methods focus on \(_{}[L]\) and can easily introduce a bias without the user's awareness, particularly when the model of the sampling pattern \((S|X)\) is misspecified. Lacking any control for miscalibration undermines the possibility to establish external validity. In safety-critical applications, making invalid inferences about a decision policy can be potentially harmful. Hence, adhering to the cautionary principle of "above all, do no harm" is important. The proposed method is designed with this principle in mind, and the limit curve represents the worst-case scenario for the selected degree of miscalibration \(\).

We also exemplify how the reliability diagram technique and the benchmark approach of omitting observed selection factors facilitate a more systematic guidance on the specification of the odds miscalibration degree \(\) in any given problem.

## 7 Broader Impact and Limitations

The method we propose is designed for safety-critical applications, such as clinical decision support, with the cautionary principle in mind. We believe that it offers a valuable tool for policy evaluation in such scenarios. Our approach focuses on limit curves, coupled with a statistical guarantee, for a more detailed understanding of the out-of-sample loss. This facilitates a fairer evaluation by bringing attention to sensitive covariates in the tails. However, it remains important to be aware of biases, and it might be necessary to address them separately to prevent their replication. It is also important to note that the method requires independent samples, a condition that may not be met during major virus outbreaks or similar situations.