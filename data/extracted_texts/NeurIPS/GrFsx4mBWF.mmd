# Demographic Parity Constrained Minimax Optimal Regression under Linear Model

Kazuto Fukuchi\({}^{1,3}\)1

Jun Sakuma\({}^{2,3}\)

\({}^{1}\) University of Tsukuba \({}^{2}\) Tokyo Institute of Technology \({}^{3}\) RIKEN

fukuchi@cs.tsukuba.ac.jp

sakuma@c.titech.ac.jp

###### Abstract

We explore the minimax optimal error associated with a demographic parity-constrained regression problem within the context of a linear model. Our proposed model encompasses a broader range of discriminatory bias sources compared to the model presented by Chzhen and Schreuder . Our analysis reveals that the minimax optimal error for the demographic parity-constrained regression problem under our model is characterized by \((^{dM}/n)\), where \(n\) denotes the sample size, \(d\) represents the dimensionality, and \(M\) signifies the number of demographic groups arising from sensitive attributes. Moreover, we demonstrate that the minimax error increases in conjunction with a larger bias present in the model.

## 1 Introduction

Machine learning techniques have been incorporated into numerous automated decision-making systems, spanning critical domains such as employment, credit assessment, insurance, and security. Nevertheless, these systems can exhibit discriminatory behavior towards specific demographic groups, including gender, race, and ethnicity, potentially causing significant societal ramifications. This issue, known as the fairness problem, has attracted substantial attention within the machine learning research community. The growing focus on the fairness problem primarily arises from reported instances of unfair behavior in real-world systems, encompassing recidivism risk prediction , hiring practices , facial recognition , and credit scoring .

Motivated by these concerns, a considerable body of research has explored regression problems subject to fairness constraints . Numerous regression algorithms incorporating various fairness constraints have been developed to accommodate diverse contexts, with demographic parity  and equalized odds  being the predominant fairness constraints adopted by these methods.

In this study, we focus on the regression problem under the fairness constraint of demographic parity . Existing literature primarily concentrates on the development of fair regression algorithms, and their performance evaluation predominantly relies on empirical analyses. Such evaluations, however, only offer performance guarantees for specific scenarios explored in the experiments, which may result in poor performance in unexamined situations. To ensure the algorithm's robust performance across a wider range of contexts and obtain a comprehensive understanding of the fair regression problem, a theoretical analysis of statistical efficiency is indispensable.

Several studies have introduced fair regression algorithms accompanied by theoretical analyses of their statistical efficiency in terms of accuracy and fairness. Agarwal et al.  designed a demographic parity-based fair regression algorithm using reduction methods  and established upper bounds on its empirical approximation errors for accuracy and fairness using Rademacher complexity. Chzhenet al.  proposed a discretization-based fair regression algorithm, deriving upper bounds on the mean squared excess risk for accuracy and a Kolmogorov distance-based score for demographic parity as fairness guarantees. Chzhen et al.  derived the Bayes optimal regressor under a demographic parity constraint, providing upper bounds on the mean absolute deviation from the Bayes optimal regressor for accuracy and a Kolmogorov distance-based score for demographic parity as fairness guarantees. Despite ensuring low error and fair treatment even in non-linear models, it remains unclear if these guarantees represent optimal performance among possible algorithms.

**Minimax optimal fair regression.** Numerous researchers have investigated minimax optimal regression algorithms, the best possible algorithm, without addressing fairness considerations . In contrast to standard regression problems, minimax optimality in fair regression problems remains relatively unexplored, with a notable exception being the recent work by Chzhen and Schreuder . They examine minimax optimality in fair regression problems, incorporating demographic parity constraints within the following linear model:

\[Y=^{*},X+b_{S}+X N(0,). \]

In this model, \(Y\), \(X\), and \(S\) represent the outcome, non-sensitive features, and a sensitive attribute, respectively. \(,\) denotes the inner product, \(\) represents zero-mean noise, and \(\) is an arbitrary covariance matrix. For example, in salary calculations, \(X\) and \(S\) correspond to working hours and gender, respectively, with \(b_{S}\) and \(^{*}\) signifying the base salary and hourly wage. In Eq (1), the salary \(Y\) is determined by the base salary \(b_{S}\) and the product of working hours \(X\) and an hourly wage \(^{*}\).

The model in Eq (1) exhibits limitations pertaining to its applicability across various scenarios. We elucidate these limitations by discussing the notion of _direct discrimination_ and _indirect discrimination_, summarized succinctly in the second row of Table 1. Direct discrimination occurs when the sensitive attribute influences the outcome, regardless of non-sensitive features. The model in Eq (1) can treat direct discrimination resulting from the dependency of the intercept \(b_{S}\) on \(S\); for example, it can capture discrimination due to basing base salary on gender (third column in Table 1). However, it is imperative to underscore that the model in Eq (1) fails to handle direct discrimination arising from the partial (regression) coefficients \(^{*}\), as these are independent of \(S\); for instance, it cannot accommodate discrimination due to gender-dependent hourly wages (second column in Table 1).

Indirect discrimination (or redlining effect ) constitutes another source of unfair bias, arising when the sensitive attribute influences the outcome through its correlation with non-sensitive features. The presence of the dependency between non-sensitive features and the sensitive attribute signifies indirect discrimination. In the model in Eq (1), non-sensitive features \(X\) is independet from the sensitive attribute \(S\), thereby implying an absence of indirect discrimination (forth column in Table 1).

Chzhen and Schreuder  effectively revealed the minimax optimal error for fair regression problems involving direct discrimination due to varying intercepts associated with sensitive attributes. However, their research does not address direct discrimination from partial coefficients and indirect discrimination through non-sensitive features.

**Our model and contributions.** In this study, we investigate the minimax optimality of the fair regression problem in the context of the following model:

\[Y=_{S}^{*},X+X N(_{S},_{X}I), \]

where \(_{X}>0\), and \(I\) denotes the identity matrix. The subscript in \(_{S}^{*}\) and \(_{S}\) signifies that our model varies regression coefficients and the mean of non-sensitive features based on the sensitive attribute.

Compared to the model proposed by Chzhen and Schreuder , our model accommodates a broader range of direct and indirect discrimination. These discriminations can be characterized as follows:

    & partial & intercept & non-sensitive features \\  Chzhen and Schreuder  & & ✓ & \\ ours & ✓ & ✓ & ✓ \\   

Table 1: Comparison between Chzhen and Schreuder ’s and our models. Each checkmark signifies the presence of an influence exerted by the sensitive attribute on the respective variable.

* (Direct discrimination) Our model accommodates direct discrimination through discrepancies in \(_{S}^{s}\) concerning \(S\), as the regression coefficients \(_{S}^{s}\) hinge on the sensitive attribute \(S\) (second and third columns on the third row in Table 1). This includes, for instance, discrimination arising from varying base salaries and hourly wages. Divergent partial coefficients yield varied outcome variance amongst \(S\), while disparate intercepts relative to \(S\) merely alter the outcome's mean. Hence, our model introduces an additional challenge of attenuating direct discrimination through disparate variance, alongside mitigating direct discrimination through disparate mean. This presents a stark contrast to Chzhen and Schreuder 's model, which solely focuses on mitigating discrimination via the mean without considering the variance.
* (Indirect discrimination) The sensitive attribute \(S\) affects the mean of non-sensitive features \(X\), as denoted by the subscript of \(_{S}\). Our model thereby introduces indirect discrimination through variations in \(_{S}\) with respect to \(S\) (e.g., disparate working hours by gender). To alleviate this form of indirect discrimination, \(_{S}\) needs to be estimated to adjust the learned regressor, thereby ensuring its output remains invariant to differing \(_{S}\). Therefore, our model presents an additional complexity in estimating \(_{S}\) for mitigating indirect discrimination.

Overall, our model demonstrates an expanded dependency of partial coefficients (direct discrimination) and non-sensitive features (indirect discrimination) on the sensitive attribute (second and fourth columns of Table 1).

The principal contribution of this paper lies in the establishment of matching upper and lower bounds on the minimax optimal error (i.e., the error corresponding to the minimax optimal regression algorithm) and the proposition of a regression algorithm that achieves this optimal error under Eq (2). The optimal error elucidates several insights:

* (Direct discrimination) The optimal error comprises a term reflecting the outcome's variance heterogeneity but excludes that of the outcome's mean. This insight implies that mitigating direct discrimination due to the outcome's variance sacrifices statistical efficiency, whereas addressing direct discrimination due to the outcome's mean does not entail this cost. This term effectively quantifies the cost of mitigating direct discrimination in variance and is absent from the optimal error of the Chzhen and Schreuder 's model. Its identification, thus, signifies a crucial contribution of our research.
* (Indirect discrimination) Our lower bound is independent of the term associated with indirect discrimination. Although this evidence is not definitive, it hints at the potential for mitigating indirect discrimination without additional costs under certain conditions. This observation sets the stage for future research focused on developing cost-effective strategies to tackle indirect discrimination.

Our technical contributions to establish these bounds are detailed in Section 4.

**Notations.** Given a positive integer \(m\), define \([m]=\{1,...,m\}\). For a finite set \(A\), denote its cardinality by \(|A|\). Given an event \(\), its complement is represented as \(^{c}\), and its probability is denoted by \(\{\}\). For a random variable \(X\), its expectation is \([X]\), and its associated sigma-algebra is \((X)\). For two real values \(a\) and \(b\), the notations \(a b=\{a,b\}\) and \(a b=\{a,b\}\) are used. For a square matrix \(A^{d d}\), its maximum and minimum eigenvalues are denoted by \(_{}(A)\) and \(_{}(A)\), respectively, and its transpose is represented by \(A^{}\). The set of unit vectors is given by \(_{d-1}\). For a sequence \(a_{t}\) indexed by \(t\), the notation \(a\). denotes the sequence \((a_{t})_{t}\).

## 2 Problem Setup

### Model and Learning Algorithm

**Model.** The proposed model, described in the introduction, is formulated according to Eq (2). We consider \(X^{2}\) and \(S[M]\) where \(M 2\). The noise variable, \(\), is assumed to follow a Gaussian distribution with zero mean and variance \(_{}^{2}>0\). We define \(p_{s}=\{S=s\}\) for all \(s[M]\), and the optimal regression function is denoted as \(f^{*}(x,s)=_{s}^{*},x\).

**Learning algorithm.** Given \(n\) i.i.d. copies of the tuple \((X,S,Y)\), denoted as \(D_{n}=\{(X_{1},S_{1},Y_{1}),...,(X_{n},S_{n},Y_{n})\}\), the goal is to construct a regression function \(f\) that maps \((X,S)\) to \(Y\), represented as \(_{n}\). The learner seeks to optimize the accuracy of \(_{n}\) while satisfying a fairness constraint. The definitions of fairness and accuracy are provided in subsequent subsections.

### Fairness

**Demographic parity.** We utilize demographic parity  as our fairness criterion. A regressor \(f\) adheres to demographic parity if its output distribution is invariant when conditioned on \(S=s\).

**Definition 1**.: _A regressor \(f\) satisfies (strong) demographic parity if, for all \(s,s^{}[M]\), and for all \(E(f(X,S))\), \(\{f(X,S) E|S=s\}=\{f(X,S) E|S=s^{}\}\)._

Denote the set of all regressors fulfilling demographic parity for a given distribution of \(X\), parameterized by \(_{}\), as \(_{}(_{})\).

**Fairness consistency.** Instead of enforcing strict demographic parity (Definition 1), which results in the regressor to be a constant function due to the unknown \((X,S)\) distribution, we introduce _fairness consistency_ (Definition 2). This concept demands the learned regressor to converge to a fair regressor as the sample size \(n\) approaches infinity.

To define "convergence", we introduce the _unfairness score_\(U(f) 0\), where a lower \(U(f)\) indicates a higher fairness level. \(U(f)=0\) if and only if \(f\) achieves demographic parity (Definition 1). We claim the learned regressor \(_{n}\) converges to an exactly fair regressor when \(U(_{n}) 0\) as \(n\).

**Definition 2**.: _A learning algorithm is \((,)\)-consistently fair for an unfairness score \(U\) if there exist constants \(n_{0} 0\) and \(C>0\), independent of \(n\), such that \(\{U(_{n})>Cn^{-}\}\) for all \(n n_{0}\), with randomness arising from the training sample via \(_{n}\)._

Note that an \((,)\)-consistently fair regressor \(_{n}\) exhibits \((^{},)\)-consistent fairness for any \(^{}(0,]\).

We adopt a specific unfairness score using the Wasserstein distance. Given two probability measures \(\) and \(^{}\) over \(\), \((,^{})\) denotes the set of all coupling measures \(\) satisfying \((A)=(A)\) and \(( A^{})=^{}(A^{})\) for every measurable sets \(A,A^{}\). The 2-Wasserstein distance \(W_{2}\) between \(\) and \(^{}\) is expressed as \(W_{2}^{2}(,^{})=_{(,^{})}(z-z^{ })^{2}(dz,dz^{})\). Our unfairness score is then formulated as:

\[U(f)=_{s,s^{}[M]}W_{2}(_{f|s},_{f|s^{}}),\]

where \(_{f|s}\) represents the distribution of \(f(X,S)\) conditioned on \(S=s\). Prior works, including [2; 7; 8; 6], have adopted different unfairness scores (see the appendix for details2).

### Accuracy

Under the fairness consistency constraint, the learner's objective is to obtain a fair approximation of \(f^{*}\), denoted as \(f^{*}_{}\), which is the closest regressor to \(f^{*}\) within \(_{}(_{})\) using the \(L^{2}\) distance:

\[f^{*}_{}=*{arg\,min}_{f_{}( _{})}f(X,S)-f^{*}(X,S)^{2}.\]

To evaluate the inaccuracy of a regressor \(f\), we compute the mean squared deviation from \(f^{*}_{}\):

\[(f;^{*}_{},_{})=f(X,S)-f ^{*}_{}(X,S)^{2}. \]

Chzhen et al. [7; 8] employ similar definitions, differing only in the choice of deviation metric.

This paper aims to identify the minimax optimal regression algorithm, which minimizes Eq (3) while maintaining fairness consistency. Given parameters \(>0\) and \((0,1)\), the optimal error is formulated as:

\[_{n}(,)=_{f_{n}:(,)^{*}}_{^{*},_{}}[ (_{n};^{*}_{},_{})],\]

where the infimum is taken over all \((,)\)-consistently fair algorithms, and \(\) and \(\) represent the sets of possible \(^{*}_{}\) and \(_{}\), respectively.

Main Results

Our main result is to establish the minimax optimal error bound, delineating the dependency on the diversity of conditional outcome variances concerning the sensitive attribute. This diversity of the variances is quantified via a parameter \(B>0\), which is defined such that it satisfies:

\[_{s}_{s}^{*} Bp_{s} _{s}^{*})^{2}}{M}_{s}^{*}^{2}}  B^{2}. \]

The left-hand side of the second inequality in Eq (4) forms as a product of two factors: the weighted average norms, \((_{s}p_{s}_{s}^{*})^{2}\), and the averaged inverse norms, \(_{s}^{*}^{2}}\). As the norms increase, the first factor (weighted average norms) has the propensity to grow, while the second factor (averaged inverse norms) tends to rise when the norms decrease. Maximizing the product of these two elements involves a delicate balancing act: the norms of some groups need to be large, while the norms of other groups need to be smaller. As such, the left-hand side of the second inequality in Eq (4) can increase when the norms \(_{s}^{*}\) display diversity.

We adopt mild assumptions on \(^{*}\) and \(\). Let \(\) denote the set of \(^{*}\) satisfying Eq (4). Assume there exists a finite universal constant \(U>0\) such that \(_{s} U\) for all \(s[M]\), leading to \(=\{^{d M}: s[M],_{s}  U\}\). Our analysis relies on these assumptions.

Our main results are as follows:

**Theorem 1**.: _Given \((0,}{{2}}]\) and \((0,1)\), suppose \(M(d-1)>16\) and \(n 12(3d 4(M/))/_{s[M]}p_{s}\). Then, there exist universal constants \(C>0\) and \(c>0\) such that_

\[c^{2}B^{2}dM}{n}-o _{n}(,) C^{2}B^{2}dM_{X}^{2}B^{2}M  B^{2}U^{2}}{n}+o.\]

Theorem 1 illustrates that the optimal error is \( B^{2}dM}}{{n}}\) up to a constant factor which may potentially depend on \(_{X}\) and \(U\). The implications of Theorem 1 can be summarized as follows:

1. The optimal error for the standard linear regression problem can be denoted as \(}{{n}}\). The dependency on \(n\) and \(d\) is consistent with the standard case, provided \((0,}{{2}}]\).
2. The term \(dM\) denotes the number of unknown parameters in Eq (2), comprising \(_{1}^{*},..,_{M}^{*}^{d}\) and \(_{1},...,_{M}^{d}\). This dependency on the number of unknown parameters is a common characteristics observed in statistical estimation problems.
3. **(Direct discrimination)** The minimax error delineated in Theorem 1 demonstrates a dependency on parameter \(B\). As the variation of \(_{s}^{*}\) with respect to \(s\) increases, so does the magnitude of \(B\). Hence, \(B\) serves as a measure of the difficulty in mitigating direct discrimination due to the outcome's variance. This unique quantification of difficulty is absent in standard regression problems and specific to fair regression problems.
4. **(Indirect discrimination)** The lower bound precludes parameters associated with indirect discrimination. It is conceivable that biases arising from indirect discrimination can be reduced without extra costs, provided the dependence of \(X\) on \(S\) exists only in its mean. Investigating and clarifying this aspect offers a promising direction for future research.
5. The minimax error is invariant to \(\) and \(\), implying that the learning process does not introduce unfair bias for \((0,}{{2}}]\). However, the case for \(}{{2}}\) remains unexplored and poses a significant research challenge.
6. The gap between the upper and lower bounds regarding \(_{X}\) and \(U\) remains, making narrowing this gap an essential future research direction.

**Remark 1**.: _Direct comparison of the minimax error between our model and that of Eq (1) is not feasible due to the differing \(f_{}^{*}\) across the models. However, the emergence of the fairness-specific term \(B\) can be unequivocally identified as a novel contribution in our study. Notably, the minimax error validated by Chzhen and Schreuder  is congruent with the minimax optimal error of standard linear regression within their model, a contrast to our findings._

To prove Theorem 1, we initiate by constructing the estimator detailed in Section 5. We then prove in Section 6 that the estimator satisfies 1) \((,)\)-fairness consistency for \((0,}{{2}}]\), and 2) the error aligns with the upper bound specified in Theorem 1. Subsequently, we present a sketch of the proof for the lower bound in Theorem 1 in Section 7. All omitted proofs can be found in the appendices.

Technical Difficulties in Minimax Optimality Analyses

In this section, we expound on the challenges arising from the analysis of minimax optimality for our problem. First, we introduce the closed-form expression for the Bayes optimal fair regressor \(f^{*}_{}\). We then outline the technical difficulties encountered during the analysis.

**Bayes optimal fair regressor under Eq (2).** Chzhen et al.  present a characterization of regression error and the corresponding regressor minimizing the mean squared error under the demographic parity constraint. Building upon the results from Chzhen et al. , we derive the closed-form expression for \(f^{*}_{}\) in the following lemma.

**Lemma 1**.: _Given the model in Eq (2), the Bayes optimal regressor adhering to the demographic parity constraint can be formulated as_

\[f^{*}_{}(x,s)=\|} _{s}}{\|^{*}_{s}\|},x-_{s}+ p_{s^{ }}^{*}_{s^{}},_{s^{}}, \]

_where \(_{s}\|}=_{s[M]}p_{s}\|^{*}_{s}\|\)._

**Technical difficulty in deriving the upper bound in Theorem 1.** To obtain the upper bound in Theorem 1, we first construct an estimator for the regression function in Eq (5) and analyze its regression error. This entails developing estimators for individual components in Eq (5) (e.g., \(\|}\), \(^{*}_{s}/\|^{*}_{s}\|\), \(_{s}\), etc.) and substituting them into Eq (5). The upper bound on \(_{n}(,)\) is derived by combining estimation error bounds for each component's estimator. However, to our best knowledge, no existing estimators provide bounds for the norm (\(\|^{*}_{s}\|\)) and direction (\(^{*}_{s}/\|^{*}_{s}\|\)) of regression coefficients. A direct approach involves computing the norm and direction of the OLS estimator, but standard analyses for OLS do not yield bounds on the estimation errors.

The main challenge in deriving the upper bound of Theorem 1 lies in analyzing the following problem: given \(X\) following a non-isotropic Gaussian distribution with mean \(\), find upper bounds on \([(X/\|X\|-/\|\|)^{2}]\) and \([(\|X\|-\|\|)^{2}]\). Solving this problem provides estimation errors for the norm and direction estimators, as the OLS estimator is an unbiased estimator with noise following the non-isotropic Gaussian distribution. Our key technical contribution is the derivation of these bounds (Theorems 4 and 5).

**Technical difficulty in deriving the lower bound in Theorem 1.** The minimax optimal error characterizes the intrinsic complexity of the regression problem, as no algorithm can surpass this error. In our analysis of the lower bound presented in Theorem 1, we demonstrate that the fair regression problem's complexity, under the model Eq (2), is characterized by the complexity in estimating the direction \(^{*}_{s}/\|^{*}_{s}\|\). The primary challenge lies in establishing this characterization.

To overcome this challenge, we investigate the geometric structure of the error term \(_{n}(f;^{*},_{.})\) concerning the parameters \(^{*}\) and \(_{.}\). We then reveal that the geometric structure of \(_{n}(f;^{*}_{.},_{.})\) is characterized by the geometric structure of the direction \(^{*}_{s^{}}/\|^{*}_{s}\|\) (Theorem 7).

## 5 Estimator

In this section, we present a detailed construction of the estimators that attain the minimax error as delineated in Theorem 1. Existing theoretical results, such as those found in Agarwal et al. , Chzhen et al. [7; 8], are incapable of addressing unbounded non-sensitive features \(X\) or unbounded outcomes \(Y\), rendering them inapplicable to our problem. Consequently, we have developed a novel estimator accompanied by rigorous analytical techniques.

**Estimator construction.** In constructing the optimal regressor for model Eq (2), we leverage the results from Lemma 1 and employ a plugin estimator. The method involves estimating the components of terms in Eq (5) and substituting the obtained estimates into the same equation. Concretely, we derive estimators \(\), \(_{s}\), \(_{s}\), \(_{s}\), \(^{}_{s}\), and \(^{}_{s}\), with the following correspondence:

\[\|}_{s}}{\|\|}}{ }_{s}}{_ {s}\|}},x-}{_{s}}+_{s^{}[M]} ^{}_{s^{}}_{s^{}}}{^{}_{s^{}}},}}{ ^{}_{s^{}}}}_{^{}_{s^{ }}}.\]For technical reasons, we partition the sample to calculate each estimand. Each estimator is assigned a corresponding subset, as shown in Table 2. Under specific conditions, \(n_{s}>18d\) or \(n_{s}>12d\), estimators may exhibit altered behavior, primarily as technical considerations for subsequent analyses. We detail the partitioning process as follows. First, we create a histogram of the sensitive attribute \(S_{i}\), denoted as \(n_{.}=(n_{1},..,n_{M})\), with \(n_{s}=|\{i[n]:S_{i}=s\}|\). Simultaneously, we form group-wise samples \(D_{s}=\{(X_{i},Y_{i}):i[n],S_{i}=s\}\). For each \(s[M]\), we partition \(D_{s}\) into \(D_{1,s}\), \(D_{2,s}\), and \(D_{3,s}\), ensuring \(|D_{b,s}| n_{b,s} n_{s}/3\) for \(b\). Using \(n_{.}\), \(D_{1,s}\), \(D_{2,s}\), and \(D_{3,s}\), we estimate \(_{s}\), \(\|}\|\), \(_{s}\), and \(_{s}\), respectively. The combination of \(_{s}\) and \(\|}\|\) yields \(\|}\|\). Furthermore, we generate a duplicate of \(D_{s}\), denoted as \(D^{}_{s}\), and partition it into \(D^{}_{1,s}\) and \(D^{}_{2,s}\), satisfying \(|D^{}_{b,s}| n^{}_{b,s} n_{s}/2\) for \(b\). We then use \(D^{}_{1,s}\) and \(D^{}_{2,s}\) to estimate \(^{}_{s}\) and \(^{}_{s}\). Precise definitions of the estimator construction and subset partitioning can be found in the appendices.

Incorporating the derived estimators, we construct the final regressor as:

\[_{n}(x,s)=\|}},x-_{s}+_{s^{}[M]}_{s^{}} ^{}_{s},^{}_{s}. \]

## 6 Upper Bound Analyses

In this section, we demonstrate the achievability of the upper bound presented in Theorem 1 utilizing the estimator delineated in Section 5. Initially, we conduct an analysis of the estimator's fairness guarantee, subsequently progressing to an examination of the estimator's mean squared deviation.

### Analysis of Fairness

For our fairness guarantee on \(_{n}\), we demonstrate the following theorem.

**Theorem 2**.: _If \(n 48(M/)/_{s}p_{s}\), we have for \((0,1)\),_

\[_{s,s^{}[M]}W_{2}_{_{n}|s^{ }},_{_{n}|s^{}}>4B_{X}_{X}[M]}np_{s^{}}}}} .\]

By proving Theorem 2, we can immediately confirm that the estimator adheres to \((,)\)-fairness consistency with \((0,]\).

### Analysis of Estimation Error

In this subsection, we derive an upper bound for the estimation error presented in Theorem 1, focusing on the estimator introduced in Section 5. To derive the upper bound in Theorem 1, we begin by decomposing the mean squared deviation of the estimator in Eq (6) as follows:

   Estimator & Sample & Definition \\  \(_{s}\) & \(n_{.}\) & \(_{s}=}}{{n}}\) \\ \(\|}\|\) & \(D_{1,s}\) & \(\|}\|=\|_{1,s}\|\) if \(n_{s}>18d\), and \(\|}=0\) otherwise \\ \(\|}\|\) & - & \(\|}\|=_{s[M]}_{s}\|}\|\) \\ \(_{s}\) & \(D_{2,s}\) & \(_{s}=_{2,s}/\|_{2,s}\|\) if \(n_{s}>18d\), and \(_{s}=0\) otherwise \\ \(_{s}\) & \(D_{3,s}\) & \(_{s}=}_{i=1}^{n_{s,s}}X_{3,s,i}\) \\ \(^{}_{s}\) & \(D^{}_{1,s}\) & \(^{}_{s}^{}_{1,s}\) if \(n_{s}>12d\), and \(^{}_{s}=0\) otherwise \\ \(^{}_{s}\) & \(D^{}_{2,s}\) & \(^{}_{s}=^{}}_{i=1}^{n_{s,s}}X^{}_ {2,s,i}\) \\   

Table 2: Estimator construction. In this table, \(_{b,s}\) and \(^{}_{b,s}\) denote OLS estimands obtained from subsets \(D_{b,s}\) and \(D^{}_{b,s}\), respectively. “Sample” refers to the subset utilized for estimand calculation, while “Definition” provides the corresponding estimator’s definition. “Sample” in \(\|}\) is left empty, as it is derived from \(_{s}\) and \(\|}\).

**Theorem 3**.: _For the estimator defined in Eq (6), the mean square deviation from \(f^{*}_{}\) is bounded above by_

\[_{s[M]}p_{s} ^{2}n.^{}{{2}}} _{s},_{s}-_{s}^{2}n. ^{}{{2}}}+_{X}-\|}^{2}n.^{}{{2}}}+\\ _{X}\|} _{s}-^{*}_{s}/\|^{*}_{s}\|^{2}n. ^{}{{2}}}+_{s^{}[M]} _{s^{}}^{}_{s^{}}-^{*}_{s^{}},^{}_{s^{}}^{2}n.^{ }{{2}}}+\\ _{s^{}[M]}_{s^{ }}^{*}_{s^{}},^{}_{s^{}}-_{s^{ }}^{2}n.^{}{{2}}}+ _{s^{}[M]}(_{s^{}}-p_{s^{}})^{*}_{ s^{}},_{s^{}}^{2}. \]

In Eq (7), the terms correspond to the estimation errors of \(_{s},,_{s},^{}_{s},^{}_{s},\) and \(_{s},\) respectively. Standard techniques for the OLS estimator and empirical average yield upper bounds for the first, fourth, fifth, and sixth terms. Nevertheless, the second and third terms in Eq (7) involve non-linear transformations of the OLS estimator (i.e., taking the norm or dividing by the norm), complicating their error analysis. This section's primary technical contributions involve establishing tight upper bounds for the second and third terms in Eq (7).

**Estimation error of norm and direction of \(^{*}_{s}\).** Consider \(X_{1},...,X_{n}}{}N(,^{2}_{X}I)\), \(^{*}^{d}\) with \(\|^{*}\| B\) for some \(B>0\), and \(_{1},...,_{n}}{}N(0,^{2}_{})\). Define \(Y_{i}=^{*},X_{i}+_{i}\). The OLS estimator of \(^{*}\) is given by \(=(X^{}X)^{-1}(X^{}Y)\), where \(X=(X_{1} X_{n})^{}\) and \(Y=(Y_{1} Y_{n})^{}\). The direction estimator is \(/\|\|\), while the norm estimator is \(\|\|\).

We present the estimation errors for direction and norm in Theorems 4 and 5:

**Theorem 4**.: _For \(n>6d\), we have_

**Theorem 5**.: _For \(n>6d\), we have_

\[\|\|-\|^{*}\|^{2} ^{2}_{}d}{^{2}_{X}n}1+ .\]

The direction's estimation error (Theorem 4) is \(O(^{2}_{}d/^{2}_{X}\|^{*}\|^{2}n)\), while the norm's estimation error (Theorem 5) is \(O(B^{2}^{2}_{}d/^{2}_{}n)\). Integrating Theorems 3 to 5 yields the \(^{2}_{}B^{2}dM/_{n}\) term in the upper bound in Theorem 1. The remaining part, \(^{^{2}_{}}_{}/n\), arises from the estimation error of \(^{}_{s}\) (the third term in Eq (7)), dominating other terms in Eq (7).

## 7 Lower Bound Analyses

In this section, we provide a proof sketch for the lower bound, outlined in Theorem 1. To facilitate a clear and concise presentation of the proof sketch, we introduce several notations. Let \(\) denote the tuple of distribution parameters \((,,)\), and let \(\) represent the set of all such parameters, defined as \(=\). We use \(_{}\) and \(_{}\) to denote the probability and expectation operators, respectively, given \(X N(_{S},^{2}_{X}I)\) and \(Y=(_{S},X)+\), where \( N(0,^{2}_{})\). We adopt the shorthand \((f;)=(f;,,)\) for \(=(,.)\). Moreover, we define \(f_{}=_{f_{}}R(f;,.)\) for \(=(,.)\). For two probability distributions \(\) and \(^{}\), the Kullback-Leibler (KL) divergence is denoted as \(D_{}(,^{})=(}(z))(dz)\). Finally, we denote the set of all \(L^{2}\) integrable functions \(f:^{d}[M]\) as \(^{2}\).

By utilizing Fano's inequality, we establish a lower bound for the minimax error as presented in Theorem 1. Due to the invariance of the distribution of \(S_{1},...,S_{n}\) under parameter alterations \(\)Fano's inequality can be applied after conditioning on \(S_{1},...,S_{n}\), or equivalently, \(n.\) Consequently, we derive the following theorem:

**Theorem 6**.: _Let \(\) be a finite set of the parameters such that there exists \(>0\) such that for any \(,^{},_{f}(f;) (f;^{})\), where \(\) and \(\) is possibly dependent on \(n.\). Let \(||=K\). Then, for arbitrary \(>0\) and \((0,1)\), we have_

\[_{n}(,)1-_{}D_{}_{ |n.},+(2)}{(K)},\]

_where \(_{|n.}\) denotes the distribution of \(D_{n}\) conditioned on \(n.\) with parameter \(\), and the expectation is taken over \(n.\)._

As demonstrated in Theorem 6, the lower bound for the minimax error can be obtained by constructing \(\) such that: 1) \(_{f}(f;)(f;^{})\) for any \(,^{}\), and 2) \(_{}_{}D_{}_{ |n.},(K/4)/2\). With the construction of such a \(\), a lower bound of \([]\) is attained.

We present a theorem that establishes a tight lower bound on \(_{f}(f;)(f;^{})\).

**Theorem 7**.: _Let \(\) and \(^{}\) be the parameters of the distributions such that \(^{2}}\|_{s}-_{s}^{}\|^{2} d_{s}<1\) for all \(s[M]\). Then, we have_

\[_{f^{2}}(f;)(f;^{ })_{s[M]}p_{s}^{2}e^{-d_{s}}}{4} _{s}}{\|_{s}\|}-\|}_{s}^{}}{\|_{s}^{}\|}^{2}1+ }{2}^{1+}.\]

The term \(\|_{s}/\|_{s}\|-.\|} _{s}^{}/\|_{s}^{}\|\|^{2}\) characterizes the lower bound, which is different from the characteristic term in standard linear regression, \(\|_{s}-_{s}^{}\|^{2}\).

We next present the construction of \(\). We construct \(\) such that each of its elements corresponds to an index from the set \(=\{-1,1\}^{M(d-1)}\), denoted by \(_{v}=\{_{v.},_{v.}\}\), where \(_{v,s}\) is controlled such that its norm is equivalent to a specified value \(B_{s}\), i.e., \(\|_{v,s}\|=B_{s}\). This construction ensures that \(\). Given positive values \(_{1},...,_{M}\) and \(B_{1},...,B_{M}\), we construct \(\) as follows:

\[_{v,s}=0,\|_{v,s}\|=B_{s},}{\|_{v,s}\|}=^{2}},}{\|_{v,s}\|}=v_{s,i-1} }{}i=2,...,d. \]

We demonstrate the following properties for \(\) defined in Eq (8).

**Theorem 8**.: _Given \(_{1},...,_{M}>0\) and \(B_{1},...,B_{M}>0\), let \(\) represent the set of parameters defined in Eq (8). Let \(_{|n.}\) be the distribution of the sample \(D_{n}\) conditioned on \(n.\) with the distribution parameter \(\). Then, we have 1) for any \(v,v^{}\),_

\[_{f^{2}}(f;_{v})(f;_{v ^{}})_{s[M]}p_{s}_{s^{}[M]}p_{s^{ }}B_{s^{}}^{2}^{2}_{s}^{2}}{d-1}d_{H}(v_ {s},v^{}s),\]

_and 2) for \(v,v^{}\),_

\[D_{}_{_{v}|n.},_{_{v^{}}|n.} =_{s[M]}^{2}B_{s}^{2}n_{s}_{s}^{2}}{_{ }^{2}(d-1)}d_{H}(v_{s},v^{}_{s}).\]

By integrating Theorems 6 to 8 and employing the renowned Varshamov-Gilbert bound, we derive the lower bound in Theorem 1.

## 8 Conclusion

This paper investigates a regression problem with \((,)\)-fairness consistency as a fairness constraint. Specifically, we demonstrate that, under the constraint of \((,)\)-fairness, the minimax optimal error scales as \(_{}^{2}B^{2}dM/n\) up to a constant factor, when \((0,}{{2}}]\). Additionally, we provide the fair regressor that achieves this optimal error.

**Potential negative societal impacts.** Our study aims to mitigate the negative impact of regression models on social groups, rather than to cause harm. However, our results are only valid for linear models, as defined in Eq (2). Misapplication of our findings to other models may result in discriminatory treatment, which should be avoided.