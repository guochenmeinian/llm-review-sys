# What Matters in Graph Class Incremental Learning?

An Information Preservation Perspective

 Jialu Li\({}^{1,2,3,}\)

jialuli@tju.edu.cn

&Yu Wang\({}^{1,2,3,}\)

wang.yu@tju.edu.cn

&Pengfei Zhu\({}^{1,2,3,}\)

zhupengfei@tju.edu.cn

&Wanyu Lin\({}^{4}\)

wan-yu.lin@polyu.edu.hk

&Qinghua Hu\({}^{1,2,3}\)

huqinghua@tju.edu.cn

Equal contribution.Corresponding author.

\({}^{1}\)College of Intelligence and Computing, Tianjin University, Tianjin, China

\({}^{2}\)Engineering Research Center of City Intelligence and Digital Governance,

Ministry of Education of the People's Republic of China, Tianjin, China

\({}^{3}\)Haile Lab of ITAI, Tianjin, China

\({}^{4}\)Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China

###### Abstract

Graph class incremental learning (GCIL) requires the model to classify emerging nodes of new classes while remembering old classes. Existing methods are designed to preserve effective information of old models or graph data to alleviate forgetting, but there is no clear theoretical understanding of what matters in information preservation. In this paper, we consider that present practice suffers from high semantic and structural shifts assessed by two devised shift metrics. We provide insights into information preservation in GCIL and find that maintaining graph information can preserve information of old models in theory to calibrate node semantic and graph structure shifts. We correspond graph information into low-frequency local-global information and high-frequency information in spatial domain. Based on the analysis, we propose a framework, Graph Spatial Information Preservation (GSIP). Specifically, for low-frequency information preservation, the old node representations obtained by inputting replayed nodes into the old model are aligned with the outputs of the node and its neighbors in the new model, and then old and new outputs are globally matched after pooling. For high-frequency information preservation, the new node representations are encouraged to imitate the near-neighbor pair similarity of old node representations. GSIP achieves a 10% increase in terms of the forgetting metric compared to prior methods on large-scale datasets. Our framework can also seamlessly integrate existing replay designs. The code is available through https://github.com/Jillian555/GSIP.

## 1 Introduction

In real-world applications, graph data is continuously generated. For instance, in citation networks, new types of papers and their citations may constantly emerge, an ideal literature classifier needs to continuously distinguish literature in emerging research areas . Therefore, it is critical for a graph model to incrementally integrate new classes on an extended graph, which is referred to as Graph class incremental learning (GCIL). However, this poses a major challenge known as catastrophic forgetting, where the model needs to preserve previous information while continuously acquiring new information .

Many approaches attempt to preserve information from previous models or graph data to prevent catastrophic forgetting in GCIL, which can be divided into four groups. The parameter isolation methods entirely or partially preserve parameters of different tasks to protect model performance, such as dynamically incrementing feature extractors and prototypes . Regularization methods, on the one hand, preserve important parameters, such as assessing parameter importance by considering loss and topology , and maintaining orthogonality with parameters from previous tasks , on the other hand, preserve the absolute position of nodes in feature space or output space, such as aligning the outputs of samples on old and new models . The replay methods preserve a few nodes or subgraphs to retrain the model to prevent forgetting, such as saving representative nodes , selecting subgraphs according to node degree , and compressing training graphs . The hybrid methods combine different learning paradigms (_i.e._ the combination of replay and regularization methods), such as feature distillation after identifying critical nodes  and minimizing distribution disparity of selected nodes across new and prior models . These hybrid methods have demonstrated considerable potential and achieved state-of-the-art results. Despite their effectiveness, the information preservation mechanism by existing methods remains unclear, making it challenging to develop effective solutions for GCIL. This motivates us to explore a fresh perspective: _What matters in information preservation when learning from the old model to the new model for GCIL?_

We investigate the unique characteristics of catastrophic forgetting on graphs and find **node semantic and graph structure shifts** in GCIL. The visualization of node embeddings for new model of baseline (ERGNN ) and our method on old classes of CoraFull dataset are exhibited in Figure 1. The structure learned by old model is selected as target instead of original topology due to noisy real structure . Five nodes belonging to two old classes are randomly selected and connected (darker edges indicate more similar features). We detect distortion in the features of baseline (Figure 1(a)) relative to those of the target (Figure 1(c)), especially nodes located within the black dotted box. The two categories can be well separated in the feature distribution of target but not in the baseline model and lead to false predictions (grey nodes in Figure 1(a)). The topological correlation is also significantly changed, node #2 is in proximity to node #1 but is distant from node #3. Surprisingly, within the representation space of baseline, node #2 appears to be moving closer to node #3 while simultaneously becoming more distant from node #1, which exacerbates catastrophic forgetting. Our method (Figure 1(b)) designs graph information preservation modules to mitigate shifts successfully.

In this paper, we inspect GCIL from the perspective of information preservation and theoretically find a key factor in reducing catastrophic forgetting risk with hybrid methods is preserving old graph information. We correspond graph information into low-frequency local-global information and high-frequency information in spatial domain. Subsequently, a Graph Spatial Information Preservation (GSIP) framework is proposed for calibrating semantic and structural shifts. In detail, the old and new representations of nodes are obtained after replay graph data is input to old and new models. The old representations of a node are locally aligned with new representations of a node and its neighbors. Further, old and new representations are globally matched after mean pooling. Finally, new representations of nodes are encouraged to mimic neighbor distance similarities that appear in old representations.

The proposed GSIP can outperform existing replay designs by up to 10% in terms of the forgetting metric on large-scale datasets. It is easy to implement and can be easily adapted to information-preserving approaches to boost their performance. Experiments show that GSIP greatly improves

Figure 1: Visualizations of semantic shift and structure shift.

over current information-preserving methods under different experimental settings and calibrates node semantic and graph structure shifts. Our main contributions can be summarized as follows:

* We provide theoretical insights into GCIL and find that preserving old graph information corresponding to low-frequency local-global and high-frequency information in spatial domain can calibrate semantic and structural shifts and reduce catastrophic forgetting risk.
* We propose a simple yet effective method that utilizes node representations on old and new models to preserve node features, graph representations, and neighbor distances.
* By combining with graph replay-based methods, our framework consistently achieves performance improvements across several benchmark datasets and shows the effectiveness of all the proposed components.

## 2 Related Work

### Incremental Learning

Incremental learning requires the model to retain the capability of predicting old tasks while acquiring information about new ones [3; 13; 14; 15; 16; 17; 18]. Class incremental learning is not assigned a task ID and has greater training difficulty than task incremental learning [19; 20]. Existing methods can be categorized into three groups. Parameter isolation methods dynamically adapt the model without restricting its structure and capacity, providing distinct parameters for each task [21; 22; 23; 24; 25]. Replay-based methods replay a subset of examples stored in previous tasks or generated using generative models to mitigate forgetting [26; 27; 28; 29]. Regularization-based methods introduce an additional regularization term in the loss function to prevent modifications to crucial parameters related to previous tasks [6; 30; 31; 32; 33].

Traditional incremental learning methods for images or text lack topology learning, making it challenging to achieve effective topology mining and information preservation. By contrast, we analyze the basics of preventing catastrophic forgetting in GCIL from information preservation and solve them in the spatial domain.

### Graph Incremental Learning

Graph incremental learning focuses on handling streaming graph data, and numerous methods have been developed explicitly for graph data [34; 35; 36; 37; 38; 39; 40; 41]. Topology-aware Weight Preserving (TWP) preserves key parameters and topology of previous tasks through regularization terms . Experience Replay Graph Neural Network (ERGNN) framework incorporates memory replay by storing representative nodes . Sparsified Subgraph Memory (SSM) stores sampled sparse subgraphs in a memory repository to preserve structural information . Su _et al._ introduced regularization terms to mitigate catastrophic forgetting from structural drift . Zhang _et al._ redesigned the architecture into a three-layer prototype that adaptively selects different parameter combinations for different tasks . The Condense and Train (CaT)  framework compresses the graph into a small but informative synthetic replay graph. Furthermore, two graph incremental learning benchmarks have recently been developed [42; 43].

In comparison, GSIP combines graph information preservation to avoid catastrophic forgetting through low-frequency local-global and high-frequency information preservation.

## 3 Problem Analysis

Graph Class Incremental Learning (GCIL).GCIL addresses the problem of supervised node classification within the context of an expanding graph. Specifically, each \(G^{t}\) denotes a newly emerging subgraph within the overarching graph. A \(G^{t}\) consists of a node set \(V^{t}\) and an edge set \(^{t}\) with its connectivity captured by adjacency matrix \(A^{t}^{n n}\), where \(n\) is the number of nodes. Each vertex \(v\) is associated with node features \(X_{v}\) and a target label \(Y_{v}\{0,1\}^{c}\), where \(c\) represents the total number of classes. At time \(t\), the GCIL problem denoted as \(_{GCIL}\) is provided with a subgraph \(G^{t}=\{X^{t},A^{t}\}\). The \(_{GCIL}\) problem is formally defined with the following signature:

\[_{GCIL}^{t}: f^{t-1},(G^{t},Y^{t}),^{t-1} f^{t},^{t},\] (1)where \(f\) is graph neural networks and \(\) signifies an external memory capable of storing a subset of training nodes or other useful graph data.

In the scenario of graph incremental learning, disparate tasks are mapped into distinct partitions of the graph. Once the learning for a specific task is completed, access to the corresponding data is restricted. Our objective is to learn a shared graph neural network model that distinguishes all classes from existing ones. Formally, we aim to minimize the loss caused by previously seen nodes at time step \(\) in \(_{}\), the statistical risk of catastrophic forgetting is defined as:

\[_{^{}}_{t=1}^{}_{(G^{t},Y^{t})}[(Y^{t},(f(G^{t};^{}))) ],\] (2)

where \(\) indicates parameters of the model, \(\) represents cross-entropy loss, and \(\) denotes softmax activation function.

Replay-Based GCIL.Replay-based methods store replayed nodes or subgraphs in memory \(\) by sampling. Catastrophic forgetting is solved by maintaining the historical distribution. These methods train a new model by minimizing loss of old task nodes on new model concerning the true labels. Given node representations on new model \(Z^{new}(Z=f(;))\), the replay loss is calculated as:

\[_{replay}=|}_{i} (Y_{i},(Z_{i}^{new})).\] (3)

Semantic Shift and Structural Shift.Due to memory and privacy limitations, a large amount of old graph data cannot be accessed in graph incremental learning, which leads to material information of old models being gradually forgotten and seriously damages new model performance on old classes. We design two novel shift metrics measuring semantic and structural forgetting degrees when trained on novel classes to show that model divergence manifests in node-level semantics and graph-level structure aspects. On CoraFull dataset, we conduct shift tests using model representations \(Z^{old}\) and \(Z^{new}\) generated by classical replay method ERGNN . Specifically, central kernel alignment  scheme is leveraged to compute Semantic Shift Score (\(SSS_{X}\)):

\[SSS_{X}(Z^{old},Z^{new})=1-,Z^{new})}{,Z^{old} )HS(Z^{new},Z^{new})}},HS(Z^{old},Z^{new})=CZ^{new}C)}{(n-1)^{2 }},\] (4)

where \(C\) is centering matrix \(C_{n}=I_{n}-1/n^{}\). In particular, Structural Shift Score (\(SSS_{A}\)) is derived by performing structure \(\) inference using feature cosine similarity, then computing differences between graph representations obtained by Anonymous Walk Embedding (AWE) :

\[SSS_{A}(Z^{old},Z^{new})=1-COS(AWE(^{old}),AWE(^{new})),_{ij}=[COS(Z_{i},Z_{j})>],\] (5)

where cosine similarity function \(COS(a,b)=a^{}b/(\|a\|\|b\|)\) is used to calculate feature similarity degree, and \(\) is similarity threshold. Each task is trained for 200 epochs, and shift scores range from 0 (no shift) to 1 (completely different). We observe that \(SSS_{X}\) and \(SSS_{A}\) in Figure 2 gradually rise with the increase of epochs. Serious shifts are found in both node semantic and graph structure between old and new models as new classes are trained.

## 4 Graph Spatial Information Preservation

### Graph Information Preservation

Model information preservation for GCIL can be defined as the mutual information of graph information across old and new models when considering the corresponding model parameters:

\[_{^{old}^{new}}=(^{old};^{new}),\] (6)

here, \(^{old}\) and \(^{new}\) are graph information on old and new models. We directly maximize mutual information between \(^{old}\) and \(^{new}\), which inherits powerful encoding capability of \(^{old}\) to \(^{new}\)

Figure 2: Semantic shift (left) and structural shift (right) between old and new models.

**Proposition 1**.: _The upper bound on graph information preservation can be estimated as:_

\[-(^{old};^{new})\|^{old}-^{new}\|_{2}^{2}=\|\|_{2}^{2},\] (7)

_we expect to maximize mutual information \((^{old};^{new})\), thus minimizing \(-(^{old};^{new})\) in estimation is needed._

Proposition 1 is proved in Appendix A.1, which suggests that graph information preservation is bounded with the square of Euclidean norm between old graph information \(^{old}\) and new graph information \(^{new}\).

### Spatial Property

Based on the spatial properties of graphs, we analyze the maintenance of graph information in spatial domain to capture complex spatial relationships between nodes and edges in graphs.

**Lemma 1**.: _(Graph spatial information factorization ) The graph convolution between convolution kernel \(\) and the signal \(\) to obtain graph information is formulated as follows:_

\[=(^{l}+^{h} )=(^{l}+^{h})=,\] (8)

_where \(^{l}\) and \(^{h}\) are low-/high- frequency graph information, \(^{l}=I_{n}+^{-} ^{-}\), \(^{h}=I_{n}-^{-} ^{-}\), \(\) is diagonal degree matrix with \(_{i,i}=_{j}_{i,j}\), and \(=A+I_{n}\) represents adjacency matrix with self-loop. Two pieces of information in spatial domain can be derived as follows:_

\[^{l}_{i}_{i}^{l}=x_{i}+_{j _{i}}}{_{i}|| _{j}|}},^{h}_{i}_{i}^{h}=x_{i}-_{j_{i}}}{_{i}||_{j}|}},\] (9)

_where \(\) represents node neighbors._

According to Lemma 1, there is an identity map that filters out graph information \(\) with graph convolution, which provides effective solutions to correspond \(^{old}\) and \(^{new}\) to spatial domain. For each component \(i\), low-frequency information preserving \(\|_{i}^{l}\|_{2}^{2}\) is defined as:

\[\|_{i}^{l}\|_{2}^{2}=\|(Z_{i}^{old}+ _{j_{i}}^{old}}{_{i}| |_{j}|}})-(Z_{i}^{new}+_{j_ {i}}^{new}}{_{i}||_{j }|}})\|_{2}^{2},\] (10)

where \(Z^{old}\) and \(Z^{new}\) denote obtained representations on old and new models. This implies that the low-frequency information preserving is a semantic gap between the sum of node features and their neighbor features of replay data on old and new models.

Sustained global connectivity is crucial to avert the erasure of global semantic information inherited from the preceding model. As displayed in Figure 3, global semantic shift does exist. We extend the concept of first-hop neighbor nodes in the previous equation to include the entire replay graph (_i.e._ multi-hop neighbors), which is denoted as:

\[\|_{i}^{l}\|_{2}^{2}=\|(Z_{i}^{old}+ _{j}^{old}}{| ||}})-(Z_{i}^{new}+_{j} ^{new}}{|||}} )\|_{2}^{2}.\] (11)

Similarly, the generalized low-frequency information preserving is the gap between the sum of node features and all replay data features on old and new models. It is worth noting that Eq. (10) provides a semantic comparison from a local perspective, whereas Eq. (11) compares from a global perspective.

For each component \(i\), high-frequency information preserving \(\|_{i}^{h}\|_{2}^{2}\) is defined as:

\[\|_{i}^{h}\|_{2}^{2}=\|(Z_{i}^{old}- _{j_{i}}^{old}}{_{i} ||_{j}|}})-(Z_{i}^{new}-_{j _{i}}^{new}}{_{i}|| _{j}|}})\|_{2}^{2}.\] (12)

Figure 3: Global semantic shift on old and new models.

High-frequency information preserving captures the gap between the difference in node features and neighbor features on old and new models from topological space.

Motivated by the above concepts, we introduce the following definition:

**Definition 1**.: _(Graph spatial information preservation) A graph spatial information preservation model mainly consists of three kinds of information preservation \(\|\,\|_{2}^{2}\|\,^{t} \|_{2}^{2}\|\,^{t}\|_{2}^{2}\| \,^{t}\|_{2}^{2}\) (defined in Eq. (10), (11), and (12))._

### Instantiations for Graph Spatial Information Preservation

The above analysis yields two crucial insights: (1) old model information preservation can be solved by preserving the learned graph information; (2) graph information preserving can correspond to low-frequency local-global information and high-frequency information from spatial domain to calibrate node semantic and graph structure shifts. Inspired by these two insights, we propose low-/high-frequency information preservation to adequately capture the old model's information. A high-level overview of GSIP framework is shown in Figure 4. The pseudo-code can be found in Appendix A.4.

Low-Frequency Information Preservation.The node representations within the previous model are derived via iterative feature integration and neighborhood communication, so it contains low-frequency graph information. The information of old model is aligned into the neighborhood of new model to better utilize low-frequency information, which can be represented as:

\[_{l}=_{i}_{j_{i} i}\| Z_{i}^{old},Z_{j}^{new}\|_{2}^{2},\] (13)

where \(Z\) is the output given by model. Low-frequency local information preserving loss uses Mean Squared Error (MSE) loss to locally match representations of nodes on old model \(Z^{old}\) with representations of nodes and their neighbors on new model \(Z^{new}\). For replay methods that do not explicitly save neighbors, neighbor selection can be found in Appendix A.2. It is worth noting that since inputs become sparse when converted to probabilities, the softmax followed by Kullback Leibler (KL) divergence loss is not applied .

Preserving global information about low-frequency components aligns old model information as a whole and prevents catastrophic forgetting. Low-frequency global information preserving loss is introduced to minimize difference between global representations of old and new models, which is defined as:

\[_{}=\|R(Z^{old}),R(Z^{new})\|_{2}^{2},\] (14)

where \(R\) represents pooling method, which is computed by mean pooling \(R(Z)=1/||_{i}Z_{i}\). Similarly, MSE loss is used to calculate global representation gaps.

Figure 4: A high-level overview of GSIP framework. It consists of low-/high- frequency modules to preserve old information. The old and new node representations are used to calculate information preserving loss of node representations, graph representations, and neighbor distances.

High-Frequency Information Preservation.The high-frequency part of spatial domain represents the difference between the features of nodes and neighbors. The updated model preserves old topology by incorporating prior local contextual information and then mitigates heterogeneous information propagation blockage caused by smoothness assumption. Specifically, for node \(v_{i}\), \(_{i}\) denotes neighborhood node set and defines \(S^{old}(v_{i},_{i})\) as the similarity of selected node vector with adjacent nodes computed by old model:

\[S^{old}(v_{i},_{i})=[S_{1}^{old},,S_{| _{i}|}^{old}],S_{j}^{old}=(Z _{i}^{old},Z_{j}^{old}))}{_{j^{}_{i}} ((Z_{i}^{old},Z_{j^{}}^{old}))},\] (15)

where \((,)\) represents kernel function that measures pairwise distances between each node and its neighbors in the latent feature space, and element-wise absolute values \((Z_{i},Z_{j})=|Z_{i}-Z_{j}|\) is used. Then, we measure similarity distribution from new model \(S^{new}(v_{i},_{i})\), which is formed by:

\[S^{new}(v_{i},_{i})=[S_{1}^{new},,S_{| _{i}|}^{new}],S_{j}^{new}=( Z_{i}^{new},Z_{j}^{new}))}{_{j^{}_{i}} ((Z_{i}^{new},Z_{j^{}}^{new}))}.\] (16)

High-frequency information preserving is proposed to map neighborhood pairwise differences between old and new models in topological space, information loss from old structure to new structure is more easily recognized with the help of KL divergence, which is denoted as follows:

\[_{h}=_{i}S^{old}(v_{i},_{i} )(v_{i},_{i})}{S^{new}(v_{ i},_{i})}.\] (17)

Model Learning.To combine different preserving losses, the final graph information preservation loss function is defined as:

\[_{gip}=_{l}+_{}+ _{h},\] (18)

where \(\) and \(\) are loss weights.

Node classification loss is obtained by \(_{nc}=1/|G|_{i G}(Y_{i}, (f(G;^{new}))))\). Therefore, the overall model learning objective is the weighted sum of current node classification loss, replay loss, and graph information preserving loss:

\[=_{nc}+_{replay}_{replay}+_{gip} _{gip},\] (19)

where \(_{replay}\) and \(_{gip}\) are loss weights, and the value of \(_{replay}\) is relevant to the design of replayed method. More analysis about the preservation of other graph frequency information (_i.e._ mid-frequency information and high-frequency global information) is given in Appendix A.3.

## 5 Experiments

### Datasets and Setups

Datasets and Settings.We utilize five public datasets to evaluate the effectiveness of the proposed method in GCIL, the statistics of datasets are reported in Appendix B.1. Three ways of dividing classes are used: one is divided unequally, and the other two are divided equally, with equal classes per task. The first dataset is CoraFull , which has 70 classes, 30 classes are used as base classes for dividing unequally, then 20 classes are used as an increment, and we divide classes equally into 10 or 2 classes per task. Arxiv  and Reddit , both containing 40 classes, dividing unequally using 10 classes as base classes, then in increments of 5 classes, and dividing equally with 10 or 2 classes per task. Each dataset has 3 tasks with 2 classes per task on Cora  and Citeseer . The latest benchmark  is employed to implement ERGNN, along with CaT  is used to implement SSM and CaT, we follow their settings in graph class incremental learning. Our implementation and detailed settings are available in Appendix B.4 and B.5.

Baselines and Metrics.We compare our method with the following baselines, including Finetuning, Joint, EWC , GEM , MAS , LwF , TWP , SSRM , and three replay-based methods (_i.e._ ERGNN , SSM , and CaT ), where three graph replay methods apply our framework. Finetuning is the lower bound baseline updating the model only with newly emerging graph data. Joint is the ideal upper bound and inputs contain all previous graph data. We choose two widely used metrics to evaluate the performance of the compared methods, including Average Performance (AP) and Average Forgetting (AF) .

[MISSING_PAGE_FAIL:8]

GSIP can offer information preservation capability to calibrate semantic shift and structural shift.Figure 5(c) exhibits the curves of shift scores during the incremental process for ERGNN on CoraFull. It can be noted that shift scores start at a relatively high value, gradually decrease, and smooth out after graph spatial information is maintained, demonstrating that the low-frequency local-global information and high-frequency information of old model are well captured, and semantic and structural shifts are nicely calibrated.

### Ablation Study

We investigate the effectiveness of low-frequency local modules (LL), low-frequency global modules (LG), and high-frequency modules (H), the experimental results use ERGNN as baseline (B). The results of ERGNN, SSM, and CaT with standard deviation are summarized in Appendix C.2. The above components are added one by one to baseline for performance comparison. From Table 3 we observe that: (1) When LL is utilized, the model can easily learn the aggregation rules of nodes and neighbors from old model locally. AP (AF) improves by about 2% (6%) to 45% (55%) over the baseline demonstrating the superiority of LL. (2) Combining LG significantly improves performance, especially on Reddit. The reason may be that larger datasets have greater overall shifts during increments. (3) H also brings significant improvements, with AP (AF) improving by about 1.6% (0.9%) to 2.4% (7.4%) over B+LL+LG on Reddit. This indicates that the H module can extract more topology information for better performance.

### Further Analysis

Hyper-Parameter Analysis.We analyze the impact of the number of storage nodes for each task \(\#\) on performance. As depicted in Figure 5(d), it can be observed that the proposed method consistently outperforms the original method in terms of the -AF metric (the lower, the better), regardless of the value of \(\#\). Interestingly, even with less memory, the proposed method still achieves better performance. CaT cannot be trained on 400 nodes due to Cuda memory constraints. We analyze the impact of loss weight \(_{gip}\) on ERGNN, SSM, and CaT across CoraFull, Arxiv, and Reddit datasets with increments of 2 in Figure 6. For ERGNN, SSM, and CaT, \(_{gip,1}\) is set to \([1,1,0.1]\), \([0.01,0.01,0.01]\), and \([0.1,0.01,0.01]\) for three datasets. It can be observed that the performance change is not as significant with the variation of \(_{gip}\) on SSM and CaT. However, different \(_{gip}\) has a greater impact on performance with ERGNN-GSIP. The possible reason is that ERGNN selects representative nodes for replay, which may cause class imbalance and topology discarding. For ERGNN, SSM, and CaT, the optimal hyper-parameters \(_{gip}\) on three datasets are \(\), \([0.1,0.1,0.1]\), and \([1,0.5,0.5]\). Because of space limitation, we provide more curves about \(\#\) in Appendix C.3 and hyper-parameter analysis of loss weights \(\) and \(\) in Appendix C.4.

Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by \(\#\) on CoraFull dataset.

    &  &  &  \\   & Unequally & Equally (10) & Equally (2) & Unequally & Equally (10) & Equally (2) & Unequally & Equally (2) & Unequally & Equally (10) & Equally (2) \\   & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) & AP\(|\) \\  B & 60.91 & -19.47 & 24.39 & -69.31 & 3.01 & -94.94 & 31.18 & -45.45 & 24.47 & -49.11 & 24.70 & -62.76 & 76.60 & -23.22 & 75.22 & -25.26 & 83.16 & -16.21 \\ B+LL & 65.79 & -12.32 & 69.02 & -14.13 & 41.37 & -47.39 & 33.27 & -34.66 & 27.10 & -40.95 & 38.09 & -35.04 & 84.63 & -12.09 & 84.26 & -12.37 & 87.52 & -10.97 \\ B+LL+LG & 66.22 & -12.78 & 69.77 & -13.13 & 41.84 & -46.73 & 34.00 & -33.61 & 32.80 & -34.40 & 39.39 & -88.88 & **85.92** & -6.97 & 81.77 & 9.45 & 91.34 & -7.12 \\ B+LL+LG+H & **67.22** & **-10.91** & **71.15** & **-11.37** & **44.79** & **-44.60** & **34.09** & **-32.59** & **33.88** & **-27.97** & **40.21** & -28.96 & **90.82** & **-6.05** & **89.59** & **-2.03** & **93.03** & **-5.50** \\   

Table 3: Ablation comparisons of graph spatial information preservation.

Visualization.To qualitatively demonstrate the effectiveness of our representations, we adopt t-SNE  to visualize the learned node embeddings. After learning the last task, Figure 7(a) and Figure 7(b) show the results of the learned node embeddings in Task 1 on CoraFull, while Figure 7(c) and Figure 7(d) demonstrate the results of the last task. We can clearly observe that GSIP possesses better representation ability by considering representations and classifying old and new classes well.

## 6 Conclusion

We contribute to the literature of GCIL by addressing the issue of information preservation from old model when adapting to new classes. The key insight is that preserving graph information from spatial domain plays a vital role in preserving information about old model, and subsequently calibrates semantic and structural shifts and reduces catastrophic forgetting risk. To accomplish this objective, we introduce a framework, GSIP, which utilizes the outputs of nodes in old model to diffuse the outputs of new model and its neighbors, then aligns the outputs of new model with old model after pooling. Finally, GSIP maintains kernel distance of neighbor pairs on both old and new models. The graph information is remembered by low-frequency local-global information preserving and high-frequency information preserving in feature and topological space. Evaluations over benchmark datasets show the superiority of GSIP in handling different dataset splitting cases. In the future, we will investigate comprehensive analysis for the preservation of complicated graph signals.