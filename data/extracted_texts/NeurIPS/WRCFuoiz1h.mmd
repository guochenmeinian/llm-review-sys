# Query-Efficient Correlation Clustering

with Noisy Oracle

 Yuko Kuroki

CENTAI Institute

Turin, Italy

yuko.kuroki@centai.eu

&Atsushi Miyauchi

CENTAI Institute

Turin, Italy

atsushi.miyauchi@centai.eu

&Francesco Bonchi

CENTAI Institute, Turin, Italy

Eurecat, Barcelona, Spain

bonchi@centai.eu

&Wei Chen

Microsoft Research

Beijing, China

weic@microsoft.com

###### Abstract

We study a general clustering setting in which we have \(n\) elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the weighted similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We introduce two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.

## 1 Introduction

Given a set \(V=[n]\) of \(n\) objects and a pairwise similarity measure \(:\) (where \(\) is the set of unordered pairs of elements of \(V\), and the value closer to \(1\) means higher similarity), the goal of _Correlation Clustering_ is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. Assuming that cluster identifiers are represented by natural numbers, a clustering \(\) can be represented as a function \(:V\), where each cluster is a maximal set of objects sharing the same label. The objective is to minimize the following cost:

\[}()=_{(x,y),\\ (x)=(y)}(1-(x,y))+_{( x,y),\\ (x)(y)}(x,y).\] (1)

The intuition underlying the above problem definition is that if two objects \(x\) and \(y\) are dissimilar, expressed by a small value of \((x,y)\), yet they are assigned to the same cluster, we should incur a high cost. Conversely, if \((x,y)\) is high, indicating that \(x\) and \(y\) are very similar, but they are assigned to different clusters, we should also incur a high cost.

Two key features make correlation clustering quite suitable in real-world applications. Firstly, it does not require the number of clusters as part of the input; instead, it automatically finds the optimal number, performing model selection. Secondly, it only requires the pairwise information withoutassuming any specific structure of the data. This reasonably eliminates the need for domain knowledge about complex data. Correlation clustering has been applied to a wide range of problems across various domains, including duplicate detection and similarity joins , spam detection , co-reference resolution , biology , image segmentation , social network analysis , and clustering aggregation .

Correlation clustering is NP-hard even in the simplest formulations , and minimizing the cost function in (1) is APX-hard ; thus, we cannot expect a polynomial-time approximation scheme. Nevertheless, there are a number of constant-factor approximation algorithms for various settings . For the formulation of (1), Ailon et al.  presented \(\), a simple 5-approximation algorithm. The algorithm randomly picks a _pivot_\(v V\) and constructs a cluster by taking all the vertices _similar_ to \(v\); then, the algorithm removes the cluster and repeats the process until \(V\) is fully clustered. The simplicity and theoretical guarantees of \(\) have produced a lot of variations in different scenarios .

In practice, preparing the similarity function involves _costly measurements_. Given \(n\) items to be clustered, \((n^{2})\) similarity computations are needed to prepare the input to correlation clustering algorithms. Moreover, computing the similarity \((x,y)\) might have additional expenses (e.g., human effort or financial resources) besides the mere computational cost. To mitigate these issues, some query-efficient methods have been proposed based on the active learning framework . In this framework, the similarity function is initially unknown but an oracle that returns the true similarity in \(\{0,1\}\) for a pair of objects is sequentially queried. In particular, these studies provided a randomized algorithm that, given a budget \(T\) of queries, attains a solution whose expected cost is at most \(3+(}{T})\), where \(\) is the optimal value of the problem.

However, the above methods for query-efficient correlation clustering have significant limitations. Indeed, all the aforementioned works  only consider the _binary similarity_ of \(\{0,1\}\), while the similarity between two objects are often non-binary in real-world scenarios. For example, in biological sciences, protein-protein interaction networks are commonly analyzed, where the strength of the interactions among proteins is represented as a real-valued similarity . As another example, in entity resolution, i.e., a task central to data integration , real-valued similarity is used to indicate the likelihood of matches of objects instead of binary decisions. Therefore, allowing the similarity to be real-valued in the interval \(\) would be more practical and flexible. Furthermore, the above works assume the access to the _strong oracle_ that returns the true value of \((x,y)\) (\(=0\) or \(1\)), while evaluating \((x,y)\) might be inherently _noisy_, due to error-prone experiments, noisy measurements, or biased judgments. In the above first example the strength of the interactions among proteins is often measured based on biological experiments involving unavoidable noise, while in the second example the likelihood of matches of objects is usually obtained based on biased human judgements.

In this paper, we focus on the challenging scenario where (i) the underlying similarity measure can take any real value in \(\) rather than being binary, and (ii) we can only query a noisy oracle that provides inaccurate evaluations of the weighted similarity \((x,y)\). The goal of this paper is _to devise clustering algorithms that perform as few queries on \((x,y)\) as possible to an oracle that returns noisy answers to \((x,y)\)_. In pursuit of this goal, we introduce two novel formulations based on multi-armed bandits problems, both of which achieve a reasonable trade-off between the number of queries to the oracle and the quality of solutions.

While our problem formulations are novel, recent prior work has explored related issues. Silwal et al.  proposed a practical model using the strong oracle along with a cheaper but inaccurate oracle. Their algorithm achieves a cost of \(3+ n^{2}\) using \(n+()\) queries to the strong oracle, where \(>0\) is the error level of noisy oracle and \(>0\) is the additive error. However, they still focus on the binary similarity and there is no guarantee on the query upper bound for the noisy oracle. Unlike theirs, our models are designed to handle the weighted similarity and do not rely on any strong oracle. Aronsson and Chehreghani  studied a non-persistent noise model where the oracle returns the true value of \((x,y)\) with probability \(1-\) and a noisy value otherwise. Their algorithm handles a general weighted similarity but provides neither query complexity nor approximation guarantee.

### Our contributions

In this paper, we study the problem of _query-efficient correlation clustering with noisy oracles_, where the similarity function \(:\) is _initially unknown_, and only _noisy feedback_ instead of the true similarity \((x,y)\) is observed when querying a pair of objects \((x,y)\). In this scenario, it is desired to achieve a reasonable trade-off between the number of queries to the oracle and the cost of clustering. To this end, we introduce two formulations of online learning problems rooted in the paradigm of _Pure Exploration of Combinatorial Multi-Armed Bandits_ (PE-CMAB). In the _fixed confidence setting_ (Problem 1), given a confidence level \((0,1)\), the learner aims to find a well-approximate solution with probability at least \(1-\) while minimizing the number of queries required to determine the output. Conversely, in the _fixed budget setting_ (Problem 2), given a querying budget \(T\), the learner aims to maximize the probability that the output is a well-approximate solution. Our contributions can be summarized as follows:

* For Problem 1, we design \(\)-\(\) (Algorithm 1), which effectively combines _threshold bandits_ with \(\). We prove that given confidence level \((0,1)\), \(\)-\(\) finds a solution whose expected cost is at most \(5+\) with probability at least \(1-\), where \(\) is the optimal value of the problem, and provide the upper bound of the number of queries (Theorem 1).
* We design \(\)-\(\) (Algorithm 3) for Problem 2, which adaptively determines the number of queries for each pair of objects based on \(\). We prove that the error probability of the expected cost being worse than \(5+\) decreases exponentially with budget \(T\) (Theorem 2).
* We empirically validate our theoretical findings by demonstrating that \(\)-\(\) and \(\)-\(\) outperform baseline methods in terms of the sample complexity and cost of clustering, respectively.

It is worth noting that our approximation guarantees in Theorems 1 and 2 match the approximation ratio 5 of \(\), where \(:\) is known in advance, up to the additive error \(>0\). These results are not achievable using existing PE-CMAB algorithms due to the NP-hardness of correlation clustering. In the standard PE-CMAB, a learner aims to identify the best action that maximizes the linear reward from the combinatorial decision set \( 2^{[m]}\) with \(m\)-base arms. Existing algorithms for PE-CMAB (e.g., [23; 25; 35; 52; 82]) rely on the assumption that the offline problem is polynomial-time solvable. Redesigning them to obtain a well-approximate solution while running efficiently is quite challenging, as the exact optimization of the offline problem is crucial to achieving statistical validity and a correctness guarantee for the output. Ours are the first polynomial-time algorithms that work for the case of PE-CMAB where the underlying offline optimization is NP-hard, filling a critical gap in existing PE-CMAB algorithms, which is of independent interest.

### Related work

Correlation clustering with noisy input.The bulk of the literature on noisy correlation clustering (see Section 4.6 of Bonchi et al. ) considers the binary similarity and assumes that there is the ground-truth clustering but some of the \((x,y)\) are wrong: they are 0 instead of 1, or vice versa. The seminal work by Bansal et al.  and Joachims and Hopcroft  provided the bounds on the error with which correlation clustering recovers the ground truth under a simple probabilistic model over graphs. Mathieu and Schudy  studied the model starting from an arbitrary partition of the \(n\) elements into clusters, where \((x,y)\) is perturbed independently with probability \(p\), and a more general model with the adversary. They proposed an algorithm that achieves some approximation ratio and manages to approximately recover the ground truth. Chen et al.  extended the framework to sparse Erdos-Renyi random graphs and obtained an algorithm that conditionally recovers the ground truth. Finally, Makarychev et al.  overcame some limitations of Mathieu and Schudy  and Chen et al. ; they assumed very little about the observations and gave two approximation algorithms. Unlike the above models, ours is based on online learning with an unknown distribution with mean of \((x,y)\), which is in general not binary, and does not assume any ground-truth clustering.

Combinatorial multi-armed bandits._Multi-Armed Bandit_ (MAB) is a classical decision-making model [57; 59; 73]: There are \(m\) possible actions (called _arms_), whose expected reward \(_{i}\) for each \(i[m]\) is unknown. At each round, a learner chooses an arm to pull and observes a stochastic reward sampled from an unknown probability distribution. The most popular objective is to minimize the cumulative regret [16; 18]. Another popular objective is to identify the arm with the maximum expected reward. This problem, called the _Best Arm Identification_ (BAI) or _Pure Exploration_ (PE) in MAB, has also received much attention [6; 8; 17; 21; 24; 37; 38; 41; 48; 53]. The model of _Combinatorial Multi-Armed Bandits_ (CMAB) is a generalization of MAB [19; 26], where an interested subset of arms forms a certain combinatorial structure such as a spanning tree, matching, or path. Since its introduction by Chen et al. , the study of PE-CMAB has been actively pursuedin various settings [23; 35; 36; 47; 50; 52; 55; 56; 67; 78; 82]. Notably, Gullo et al.  addressed regret minimization for correlation clustering by adapting UCB-type algorithms. However, regret minimization in CMAB is quite different from pure exploration framework when working with approximation oracles (i.e., offline approximation algorithms) for solving NP-hard problems. For regret minimization, we can incorporate approximation oracles with the UCB framework, consistent with the optimization under uncertainty principle (e.g., [26; 27; 81]). However, in pure exploration, the lack of uniqueness of \(\)-approximate solutions makes it difficult to determine the stopping condition in the FC setting. In the FB setting, the Combinatorial Successive Accept Reject algorithm proposed by Chen et al.  iteratively solves the so-called Constrained Oracle problem, which is often NP-hard, as later addressed in Du et al. . We anticipate a similar NP-hard problem in correlation clustering, requiring a different approach.

Other clustering settings.Ailon et al.  and Saha and Subramanian  studied correlation clustering with _same-cluster queries_, where all similarities of \(\) are known in advance and their query is further allowed to access the optimal clustering. Our setting differs significantly as we are interested in the case where similarities are unknown and only noisy similarity values are received rather than same-cluster queries. Finally, it is worth mentioning that Xia and Huang  and Gupta et al.  proposed a MAB approach for clustering reconstruction with noisy same-cluster queries [58; 65; 70; 71; 77]. However, this clustering reconstruction problem does not directly offer any algorithmic result for correlation clustering. The detailed comparison is deferred to Appendix A.

## 2 Problem statements

Here we formally define our formulations of PE-CMAB for correlation clustering. Our problem instances are characterized by \((V,)\), where \(V=[n]\) is the set of elements to be clustered and \(:\) is the pairwise similarity function, which is _unknown_ to the learner. Define the set of unordered pairs as \(E=\) with \(m:=|E|\).

At each round \(t=1,2,\), a learner will pull (i.e., query) one arm (i.e., pair of elements in \(V\)) from action space \(E=\) based on past observations. After pulling \(e E\), the learner can observe the random feedback \(X_{t}(e)\), which is independently sampled from an _unknown_ distribution such as Bernoulli or \(R\)-sub-Gaussian with unknown mean \((e)\).1 After some exploration rounds, the learner must identify a well-approximate solution. Let \(()\) be the optimal value of the offline problem minimizing the cost function (1) and let \(_{}\) be the output by an algorithm. For \( 1\) and \(>0\), we say \(_{}\) to be an \((,)\)-approximate solution if \(}(_{})( )+\). We study the following two formulations: Fixed Confidence (FC) and Fixed Budget (FB) settings.

**Problem 1** (Fixed confidence setting).: _Let \( 1\). Given a confidence level \((0,1)\) and additive error \(>0\), the learner aims to guarantee that the output \(_{}\) is an \((,)\)-approximate solution with probability at least \(1-\). The evaluation metric of an algorithm is the sample complexity, i.e., the number of queries to the oracle the learner uses._

**Problem 2** (Fixed budget setting).: _Let \( 1\). Given a querying budget \(T\) and additive error \(>0\), the learner aims to maximize the probability that the output \(_{}\) is an \((,)\)-approximate solution._

Note that the case of \(=1\) corresponds to the standard PE-CMAB formulations. However, as the offline problem minimizing the cost function (1) is APX-hard , we cannot expect any polynomial-time algorithm that can handle \(=1\) in the above formulations.

## 3 Fixed confidence setting

In this section, we design KC-FC (Algorithm 1) for Problem 1, built on a novel combination of KwikCluster (detailed in Algorithm 4 in Appendix B) and techniques of threshold bandits. The key idea of the proposed method is to first identify pairs with seemingly high similarity, which are then passed to KwikCluster to produce a high-quality clustering.

For the first phase, we leverage one of the variants of MAB, called the _threshold bandits_, which is defined as follows: Given a confidence level \(\) and \(m\)-arms, the learner must return the set of _good_ arms, i.e., arms whose expected rewards are greater than a given threshold \(>0\), as soon as possible, and stops when the learner believes that there is no remaining good arm, w.p. at least \(1-\). TB-HS (detailed in Algorithm 2) is our key procedure, which is designed for identifying seemingly high similarity pairs. Note that, if we naively use the existing algorithm by Kano et al.  for threshold bandits where the set of arms is \(E=\) and the threshold is \(=0.5\), the algorithm is not even guaranteed to terminate; the resulting sample complexity becomes infinitely large if \((e)=(e^{})\) for different \(e,e^{} E\) or if there exists \(e E\) with \((e)=0.5\), which may frequently happen in practice. Our strategy to avoid such an unbounded sample complexity is to allow TB-HS to misidentify pairs of elements with similarity close to 0.5, taking advantage of the fact that the output accuracy can be guaranteed despite such misidentification.

Algorithm details.Let \(}_{t}(e)\) be the empirical mean of the similarity for each pair \(e E\) kept at round \(t\). Let \(N_{t}(e)\) be the number of queries of \(e E\) that has been pulled by the end of round \(t\). TB-HS maintains the confidence bound defined as \(_{t}(e):=(e)^{2}/)}{2N_{t}(e)}}\) for each \(e E\). The arm selection at round \(t\) is based on the Lower-Confidence-Bound (LCB) score, i.e., \(_{t}(e):=}_{t}(e)-_{t}(e)\) and the Upper-Confidence-Bound (UCB) score, i.e., \(}_{t}(e):=}_{t}(e)+_{t}(e)\). We pull the arm \(^{g}_{t}\) with the highest LCB (line 5) and the arm \(^{b}_{t}\) with the lowest UCB (line 6). Then \(^{g}_{t}\) will be added to \(_{}\) if its LCB is no less than \(0.5-\), and \(^{b}_{t}\) will be added to \(_{}\) if its UCB is no greater than \(0.5+\). TB-HS continues this procedure until every \(e E\) is added to either \(_{}\) or \(_{}\). Our main algorithm KC-FC invokes TB-HS to compute \(_{^{}}\) with parameter \(^{}=\). Then it carries out KwikCluster using the predicted similarity by \(_{^{}}\) as follows. Until an unclustered element exists, it picks one pivot element \(p_{r}\) uniformly at random, builds a cluster \(C_{r}\) around it by adding those among the unclustered elements that seemingly have high similarity with a pivot \(p_{r}\) (based on \(_{^{}}\)), and removes all the elements in \(C_{r}\) from the list of unclustered elements.

``` Input : Confidence level \(\), set \(V\) of \(n\) objects, and error \(\)
1\(E_{1} E\), \(V_{1} V\), \(r 1\), and \(_{}\);
2 Compute \(_{^{}}\) by TB-HS (Algorithm 2) with \(^{}=\);
3 Define \((v):=\{u V:\{u,v\}_{^{}}\}\);
4while\(|V_{r}|>0\)do
5 Pick a pivot \(p_{r} V_{r}\) uniformly at random;
6\(_{}_{}\{C_{r}\}\), where \(C_{r}:=(\{p_{r}\}(p_{r})) V_{r}\);
7\(V_{r+1} V_{r} C_{r}\) and \(r r+1\);
8return\(_{}\) ```

**Algorithm 1**KwikCluster with Fixed Confidence (KC-FC)

Analysis.For a given \((0,0.5)\), we define the following sets, which appear only in the theoretical analysis and are unknown to the learner: \(E_{[0.5]}:=\{e E:|0.5-(e)|\}\), \(E_{(0.5+,1)}:=\{e E:(e)>0.5+\}\), and \(E_{[0,0.5-)}:=\{e E:(e)<0.5-\}\). For \((0,0.5)\), we introduce the definition of the gaps that characterize our sample complexity:

\[_{e,}\!:=\!(_{e}+\{ -_{},\,\})\,\,\,e[m],\] (2)

where \(_{e}:=|(e)-0.5|\) for \(e[m]\) and \(_{}:=_{e[m]}_{e}\).

Now we present our theorem, guaranteeing that KC-FC finds a \((5,)\)-approximate solution with high probability and provides an upper bound of the number of queries, i.e., the sample complexity.

**Theorem 1**.: _Given a confidence level \((0,1)\) and additive error \(>0\), KC-FC (Algorithm 1) guarantees that_

\[[}(_{}) 5+ ] 1-,\]and letting \(^{}=\), the sample complexity \(T\) is_

\[(_{e E}_{e,^{}}^ {2}}(_{e,^{}}^{2}} (_{e,^{}}^{2}}) )+}{\{_{},}{2} \}^{2}}).\]

_Furthermore, \(\)-\(\) runs in time polynomial in \(n\)._

Proof Sketch.: For the outputs \(_{}\) and \(_{}\) of TB-HS (Algorithm 2) with parameters \((0,0.5)\) and \((0,1)\), by using the Hoeffding inequality and the procedure of TB-HS (lines 8 and 10), it is easy to see that \(E_{(0.5+,1]}_{}\) and \(E_{[0.0.5-)}_{}\) w.p. at least \(1-\). Consider the similarity function \(}:E\) such that for each \(e E\), \(}(e)=s(e)\) if \(e E_{[0.0.5-)} E_{(0.5+,1]}\), and \(}_{e}\) otherwise, where \(}_{e}\) is an arbitrary value that satisfies \(|(e)-}_{e}|<2\). Noticing that \(\)-\(\) corresponds to \(\) associated with a certain choice of \(}\) (i.e., \(}_{e}\) for \(e E_{[0.5]}\)), we can show that \([_{}(_{})] 5 ()+12|E_{[0.5]}|\) for the output \(_{}\), providing the approximation guarantee. The rest of the proof requires the analysis of the upper bound of the number of queries that TB-HS used to stop. This can be done based on a prior analysis of threshold bandits , while carefully handling \(>0\). The complete proof for analysis is given in Appendix C.

For the time complexity, each iteration of sub-routine TB-HS takes \((m)\) steps in a naive implementation or amortized \(( T)\) steps if we manage arms using two heaps corresponding to LCB/UCB values, and the other procedure in \(\)-\(\) runs in time polynomial in \(n\). 

Comparison with existing PE-CMAB methods in the FC setting.Existing methods for PE-CMAB (e.g., ) are limited by their reliance on the polynomial-time solvability of the underlying offline problem. If we use an efficient approximation algorithm in those existing methods, their stopping conditions no longer have a guarantee of the quality of the output. Specifically, such existing methods use the LUCB-type strategy, and its stopping condition requires the exact computation of the empirical best solution and the second empirical best solution to check if the current estimation is enough or not. When we only have an approximate oracle (i.e., approximation algorithm), such existing stopping conditions are no longer valid, and the algorithm is not guaranteed to stop. In contrast, \(\)-\(\) runs in time polynomial in \(n\) while ensuring sample complexity and approximation guarantee. We also note that \(_{e}\), the distance between \((e)\) and \(0.5\), interestingly characterizes our sample complexity, as we show that the learning task boils down to identifying \(E_{(0.5+^{},1]}\) and \(E_{[0.0.5-^{})}\) thanks to the behavior of \(\)-\(\) - they leverage the property that by accurately estimating the mean of the base arms (i.e., pairs of elements), we can maintain the approximation guarantee of \(\) in the offline setting with small additive error.

Statistical efficiency.In the noise-free setting, \(\) queries are sufficient, while in the noisy setting, there is even no trivial upper bound on the sample complexity to achieve some desired approximation guarantee (e.g., our \((5,)\)-approximation). Note that the value of \(_{e,}\) defined in (2) always has the following lower bound: \(_{e,}=_{e}+/2\,(>0)\) if \(/2_{}\) holds and \(_{e,}=_{e}+-_{}\ (>0)\) otherwise. Therefore, our sample complexity \(T\) given in Theorem 1 is always bounded, contrasting existing results for threshold bandits . The naive sampling algorithm (Uniform-FC in Appendix E) requires \(O(}{^{2}})\) samples to achieve the \((5,)\)-approximation w.p. at least \(1-\). KC-FC achieves a much better sample complexity than Uniform-FC, as \(_{e E}_{e,^{}}^{-2}=_{e E}(_{e }+}{2})^{-2}}{^{2}}\) when \(_{}}{2}_{e}\) for most \(e E\), which is often the case in practice. To the best of our knowledge, lower bounds on the sample complexity related to PE-CMAB are known only for the following settings: (i) the time complexity of algorithms can be exponential, or (ii) the underlying offline problem is assumed to be polynomial-time solvable and to have the unique correct (namely optimal) solution [25; 35; 39]. Deriving an effective lower bound on the number of samples required to guarantee an approximate solution is particularly challenging because it necessitates dealing with multiple correct solutions , while most existing approaches rely on the uniqueness of the correct solution. Evaluating the necessity of the second term \(}{\{_{},}{2}\}^{2}}\) and investigating a lower bound for our case are crucial and remain important future work. However, it is worth noting that the additional term is independent of a dominating term involving \(\).

**Remark.** If we utilize TB-HS within the loop (Algorithm 5 in Appendix B), the algorithm achieves \((5,)\)-approximation guarantee with probability at least \(1-\), and the sample complexity \(T\) is:

\[(_{r=1}^{k}(_{e I_{V_{r}}(p_{r})}_{e,^{}_{r}}^{2}}(_{e,^{}_{r}}^{2}}( _{e,^{}_{r}}^{2}}))+|}{( _{,r,_{r}}{2}})^{2}})),\]

where \(k\) is the total number of loops in Algorithm 5, \(^{}_{r}:=/(12|I_{V_{r}}(p_{r})|)\), \(I_{V_{r}}(p_{r}) E\) represents the set of pairs between the pivot \(p_{r}\) selected in phase \(r\) and its neighbors in \(V_{r}\), and \(_{,r}:=_{e I_{V_{r}}(p_{r})}_{e}\). When \(k n\), the above sample complexity can be better than that of Theorem 1. However, it should be noted that the symbols related to \(r\) and the total number of loops \(k\), especially instance-dependent gaps \(_{e,^{}_{r}}\), are all random variables. In contrast, the current Theorem 1 does not contain any random variables. Specifically, the significant term related to \(^{-1}\) is characterized by the gap \(_{e,}\) or \(_{e}\), which represents the distance from 0.5 and not a random variable.

## 4 Fixed budget setting

In this section, we investigate Problem 2 and design KC-FB (Algorithm 3). KC-FB is inspired by the successive reject algorithm  and exploits KwikCluster to determine the number of queries for each pair adaptively.

Algorithm.KC-FB proceeds in at most \(n\) phases and maintains the subset of elements \(V_{r} V\) in each phase \(r[n]\) starting with \(V_{1}=V\). We denote the set of pairs that can be formed with \(v\) in \(V_{r}\) by \(I_{V_{r}}(v):=\{\{v,u\}}{2}:u V_{r}\}\). In each phase \(r\), the algorithm chooses the pivot \(p_{r}\) uniformly at random from \(V_{r}\), and pulls each \(e I_{V_{r}}(p_{r})\) for appropriately determined \(_{r}\) times. Based on the empirical mean \(}_{r}(e):=_{k=1}^{_{r}}X_{k}(e)/_{r}\) for each \(e I_{V_{r}}(p_{r})\), it finds one cluster \(C_{r}=\{p_{r}\}_{V_{r}}(p_{r},}_{r})\), where \(_{V_{r}}(p_{r},}_{r}):=\{u V_{r}:}_{r}(p_{r},u)>0.5\}\), and updates \(V_{r+1} V_{r} C_{r}\). This procedure will be continued until \(|V_{r}|=0\) and finally the algorithm outputs \(_{}\) consisting of all clusters computed. Updating the number of pulls \(_{r}\) (line 8) is a key to prove the statistical property. Intuitively, \(_{r}\) represents a pre-fixed budget of queries when \(e}{2}\) would be pulled: In the initial phase, we allocate \(_{1}:= T/m\) to each \(e}{2}\). Notice that the surplus, the sum of the pre-fixed budgets of pairs that have been removed without being queried, is \(_{1}(}{2}|-|}{2}|-(|V_{1}|-1))\), because the number of pairs that have been removed in this phase is \(|}{2}|-|}{2}|\), and among those pairs, the number of pairs that have been actually pulled by the algorithm is \((|V_{1}|-1)\). This surplus is additionally redistributed equally to each \(e}{2}\). This will be also done for the remaining phases \(r=2,,n\).

**Input :** Budget \(T>0\), set \(V\) of \(n\) objects, additive error \(\)

\(V_{1} V\), \(r 1\), \(_{1} T/m\), and \(_{}\);

**while \(|V_{r}|>0\)do**

Pick a pivot \(p_{r} V_{r}\) uniformly at random;

 Pull each \(e I_{V_{r}}(p_{r})\) for \(_{r}\) times and observe random feedback \(\{X_{k}(e)\}_{k=1}^{_{r}}\);

 Compute empirical mean \(}_{r}(e)=_{k=1}^{_{r}}X_{k}(e)/_{r}\) for each \(e I_{V_{r}}(p_{r})\);

\(_{}_{}\{C_{r}\}\) where \(C_{r}:=\{p_{r}\}_{V_{r}}(p_{r},}_{r})\);

\(V_{r+1} V_{r} C_{r}\);

\(_{r+1}_{r}+(}{2}-}{2})-( V_{r}-1))}{ }{2}}\) and \(r r+1\);

**return \(_{}\)**

**Analysis.** The following theorem states that \(\)-\(\) outputs a well-approximate solution with high probability. The proof of Theorem 2 is deferred to Appendix D.

**Theorem 2**.: _For \(>0\), define the minimal gap \(_{,}\) as_

\[_{e E}\{|\}},_{e}\} \;(0,0.5),\] _where \[_{e}=|(e)-0.5|\;\;( e E).\]_

_Then, \(\)-\(\) (Algorithm 3) uses at most \(T\) queries to output \(_{}\) that satisfies_

\[[[_{}(_{})] 5 ()+] 1-\;\;\;  2n^{3}(-^{2}}{n^{2}}).\] (3)

_Assuming that each query takes \((1)\) time, the time complexity of \(\)-\(\) is \((T+n^{2})\)._

Proof Sketch.: We can show the random event \([_{r=1}^{n}_{r}]\) occurs with high probability, where \(_{r}:=\{ e I_{V_{r}}(p_{r}),\,|(e)- }_{r}(e)|<\{,_{e}\}\}\) for each phase \(r[n]\) (See Lemma 8 in Appendix D.1). Under the assumption of such estimation success event \(_{r=1}^{n}_{r}\), by utilizing the unique feature of \(\), we can maintain the approximation guarantee of \(\) in the noise-free setting up to additive error (See Lemma 9 in Appendix D.2). Simply combining these lemmas with adjusted parameter \(^{}(0,0.5)\), defined as \(|\}}\) if \(<0.5\) and \(\) otherwise, will conclude the proof (See Appendix D.3 for details). 

The parameter \((0,1)\) in (3) represents the _error probability_ of \(_{}\) being worse than any \((5,)\)-approximate solution, and it decays exponentially to the querying budget \(T\). A larger parameter \(_{,}\) provides the better guarantee; \(\)-\(\) performs better when the similarity function clearly expresses similarity (\(+1\)) or dissimilarity (\(-1\)), as \(_{e E}_{e}\) tends to be large.

To evaluate the significance of our results, we analyze the uniform sampling algorithm (Uniform-\(\) in Appendix E); \(\)-\(\) queries each \(e E\) uniformly \( T/m\) times to obtain \(}(e)\), and then applies any \(\)-approximation algorithm to instance \((V,})\) of the offline problem minimizing (1). We see that the error probability that the output is not an \((,)\)-approximate solution is bounded by \((n^{2}(-}{^{2}n^{6}} ))\). In contrast, \(\)-\(\) adaptively allocates the budget to the remaining pairs, which enables us to query essential pairs of elements, i.e., pairs whose estimated similarity values affect the behavior of cluster construction, more times than \( T/m\). This leads to a better performance in the cost of clustering in practice (see Section 5).

Comparison with existing PE-CMAB methods in the FB setting.In the literature of PE-CMAB, the FB setting presents even more computational challenges and a scarcity of theoretical results. The current state-of-the-art algorithms  suffer from one or more of the following issues: (i) inability to handle a partition structure in correlation clustering, (ii) requiring exponential running time, and (iii) lacking any approximation guarantees when the underlying problem is NP-hard. By leveraging the properties of \(\), our approach ensures the polynomial-time complexity of \(O(T+n^{2})\) while guaranteeing that the probability of obtaining a well-approximate solution exponentially increases with the budget \(T\), along with instance-dependent analysis.

Experimental evaluation

In this section, we evaluate the performance of our proposed algorithms, KC-FC and KC-FB, using various datasets, providing empirical evidence to support our theoretical findings.

**Datasets.** We use publicly-available real-world graphs presented in Table 1. In the FC setting, to observe the behavior of the sample complexity with respect to the hidden minimum gap \(_{}\) in (2), we generate our instances as follows. For each graph, we vary the lower bound on \(_{}\), which we denote by \(_{_{}}\), in \(\{0.10,0.15,0.20,,0.50\}\). For each pair of vertices \(u,v\), we set \((u,v)=[0.5+_{_{}},1]\) if \(u,v\) have an edge in the graph, and \((u,v)=[0,0.5-_{_{}}]\) otherwise, where \([a,b]\) is the value drawn from the interval \([a,b]\) uniformly at random. On the other hand, in the FB setting, we employ a more realistic setting: For each graph, our problem instance is generated by embedding the vertices into a \(d\)-dimensional Euclidean space using node2vec , obtaining a vector \((v)^{d}\) for each vertex \(v\). Specifically, we used the publicly-available Python module of node2vec2 with default parameter settings (particularly \(d=64\)). Then, define the unknown similarity of each pair of vertices \(u,v\) as \((u,v)=_{}((u), (v))-_{}}{_{}-_{ }}\), where \(\_{}\) and \(\_{}\) are the minimal and maximal cosine similarities, respectively, among all pairs of vertices. We note that \(\_{}>\_{}\) holds for all instances. In all experiments, noisy feedback when querying a pair \(e E\) is generated by a Bernoulli distribution with mean \((e)\).

**Baselines.** We compare our methods with Uniform-FC in the FC setting and Uniform-FB in the FB setting, whose pseudocode and full analysis are given in Appendix E. Uniform-FC pulls each \(e E\) for \(}{e^{2}}\) times and employs KwikCluster with respect to the empirical similarity, while Uniform-FB is its adaption to the FB setting. Moreover, we compare the cost of clustering of our algorithms with that of KwikCluster having access to the unknown (true) similarity, which is regarded as the stronger baseline than other KwikCluster-based methods for the binary case .

**Machine and code.** The experiments were performed on a machine with Apple M1 Chip and 16 GB RAM. The code was written in Python 3, which is available online.3

**Performance of KC-FC.** We evaluate the performance of algorithms in terms of not only the cost of clustering but also the sample complexity. In both KC-FC and Uniform-FC, we set \(=\) allowing each element to make only \(1/\) mistakes, and \(=0.01\) following a standard choice in PE-MAB. Taking into account the limited scalability of the algorithms, we only use the instances with \(n<\) 1,000. In particular, as will be shown later, Uniform-FC requires a large number of samples, which makes the algorithm prohibitive even for quite small instances. Therefore, we do not run the algorithm and just report the sample complexity, which can be calculated without running it. For each \(_{_{}}\), we run both KC-FC and KwikCluster having access to the unknown similarity 100 times and report the average value and the standard deviation.

The results are depicted in Figures 1 and 2. As can be seen, the sample complexity of KC-FC is much smaller than that of Uniform-FC. In fact, the sample complexity of Uniform-FC makes the algorithm

   Name & \# of vertices & \# of edges & Description \\   Lesmis & 77 & 254 & Co-appearance network \\ Adjnoun & 112 & 425 & Word-adjacency network \\ Football & 115 & 613 & Sports team network \\ Jazz & 198 & 2,742 & Social network \\ Enall & 1,133 & 5,451 & Communication network \\ ego-Facebook & 4,039 & 88,234 & Social network \\ Wiki-Vote & 7,066 & 100,736 & Wikipedia voting network \\   

Table 1: Real-world graphs used in our experiments.

Figure 1: Sample complexity of KC-FC & Uniform-FC.

prohibitive even for very small instances. Moreover, consistent with the theoretical analysis, as (the lower bound \(_{_{}}\) on) \(_{}\) increases, the sample complexity of \(\)-\(\) becomes smaller. This desirable property is not possessed by \(\)-\(\). Remarkably, looking at Figure 2, we see that \(\)-\(\) outputs a clustering whose quality is comparable with that of \(\) having access to the unknown similarity.

Performance of \(\)-\(\).Here we evaluate the performance of \(\)-\(\). For small instances with \(n<\) 1,000, we vary \(T\) in \(\{n^{2.1},n^{2.2},,n^{3.0}\}\) and observe the cost of clustering with respect to the budget \(T\). For large instances with \(n\) 1,000, we fix \(T=n^{2.2}\) for scalability. For each instance and \(T\), we run both \(\)-\(\) and \(\)-\(\) 100 times and report the average value and the standard deviation. As \(\) having access to the unknown similarity is independent of \(T\), we just run it 100 times for each instance.

The results are shown in Figure 3 and Table 2. As can be seen, \(\)-\(\) outperforms the baseline method \(\)-\(\). In fact, for all instances and almost all values of \(T\), \(\)-\(\) outputs a better clustering than that of \(\)-\(\). We can see that this superiority comes from the fact that \(\)-\(\) estimates the unknown similarity better than \(\)-\(\) thanks to its sophisticated sampling strategy. Indeed, \(\) having access to the unknown similarity showcases the best performance, verifying the importance of the precise estimation of the unknown similarity.

## 6 Conclusions

We studied the online learning problems of correlation clustering, where the similarity function is initially unknown and only noisy feedback is observed. For the FC setting, we devised \(\)-\(\) and proved the upper bound of the number of queries required to find a clustering whose cost is at most \(5+\) with high probability. For the FB setting, we devised \(\)-\(\) and showed that the error probability of the expected cost being worse than \(5+\) decays exponentially with budget \(T\). Importantly, our algorithms are the first examples of PE-CMAB with NP-hard offline problems. One future work, yet a significant challenge, is to derive information-theoretic lower bounds of PE-CMAB in the case where the offline problem is NP-hard. Investigating other variants of correlation clustering or exploring the case where the variance of random feedback differs across pairs, namely heteroscedastic noise, would also be worthwhile directions.

  Name & \(\)-\(\) & \(\)-\(\) & \((V,s)\) \\  Email & 218\(\)1.1k & 221\(\)0.5k & 209\(\)0.5k \\ ego-Facebook & 3,716\(\)36.5k & 3,780\(\)29.6k & 3,373\(\)59.8k \\ Wiki-Vote & 10,222\(\)45.5k & 10,428\(\)32.0k & 9,749\(\)34.7k \\  

Table 2: Cost of clustering of \(\)-\(\) & baselines (\(n\) 1,000).

Figure 3: Cost of clustering of \(\)-\(\) & baselines (\(n<\) 1,000).

Figure 2: Cost of clustering of \(\)-\(\) & \(\) having the access to the unknown similarity.