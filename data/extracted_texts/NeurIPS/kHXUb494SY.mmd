# Nesterov acceleration despite very noisy gradients

Kanan Gupta

Department of Mathematics

University of Pittsburgh

kanan.g@pitt.edu

&Jonathan W. Siegel

Department of Mathematics

Texas A&M University

jwsiegel@tamu.edu

&Stephan Wojtowytsch

Department of Mathematics

University of Pittsburgh

s.woj@pitt.edu

###### Abstract

We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex and strongly convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient at every point. Nesterov's method converges at an accelerated rate if the constant of proportionality is below \(1\), while AGNES accommodates any signal-to-noise ratio. The noise model is motivated by applications in overparametrized machine learning. AGNES requires only two parameters in convex and three in strongly convex minimization tasks, improving on existing methods. We further provide clear geometric interpretations and heuristics for the choice of parameters.

## 1 Introduction

The recent success of deep learning (LeCun et al., 2015) is built on stochastic first order optimization methods such as stochastic gradient descent (LeCun et al., 1998) and ADAM (Kingma and Ba, 2014), which have enabled the large-scale training of neural networks. While such tasks are generally non-convex, accelerated first order methods for convex optimization have proved practically useful. Specifically, Nesterov (1983)'s accelerated gradient descent has become a standard training method (Sutskever et al., 2013).

Modern neural networks tend to operate in the _overparametrized_ regime, i.e. the number of model parameters exceeds the number of data points to be fit (Belkin, 2021). In this setting, minibatch gradient estimates are exact (namely, exactly 0) on the set of global minimizers since data can be interpolated exactly. Motivated by such applications, Vaswani et al. (2019) proved that Nesterov (2012)'s accelerated coordinate descent method (ACDM) achieves acceleration in (strongly) convex optimization with _multiplicative noise_, i.e. when assuming stochastic gradient estimates for which the noise intensity scales linearly with the magnitude of the gradient. Conversely, Liu and Belkin (2018) show that the original version of Nesterov (1983)'s method generally does not achieve acceleration in this setting.

Another algorithm with a similar goal is the continuized Nesterov method (CNM), which has been studied by Even et al. (2021), Berthier et al. (2021) in convex optimization (deterministic or with additive noise) and with multiplicative noise for overparametrized linear least squares regression. For a more extensive discussion of the context of our work in the literature, please see Section 2.

Vaswani et al. (2019)'s algorithm is a four parameter scheme in the strongly convex case, which reduces to a three parameter scheme in the convex case. Liu and Belkin (2018) introduce a simpler three parameter scheme, but only prove that it achieves acceleration for overparametrized _linear problems_. In this work, we demonstrate that it is possible to achieve the same theoretical guarantees as Vaswani et al. (2019) with a simpler scheme, which can be considered as a reparametrized version of Liu and Belkin (2018)'s Momentum-Added Stochastic Solver (MaSS) method. More precisely, we prove the following:1. We show that Nesterov's accelerated gradient descent achieves an accelerated convergence rate, but _only with noise which is strictly smaller than the gradient in the \(L^{2}\)-sense_. We also show numerically that when the noise is larger than the gradient, the algorithm diverges for a choice of step size for which gradient descent remains convergent.
2. Motivated by this, we introduce a generalization of Nesterov's method, which we call Accelerated Gradient descent with Noisy EStimators (AGNES), which provably achieves acceleration _no matter how large the noise is relative to the gradient, both in the convex and strongly convex cases_.
3. When moving from NAG to AGNES, the learning rate 'bifurcates' to two parameters in order to accommodate stochastic gradient estimates. The extension requires three hyperparameters in the strongly convex case and two in the convex case.
4. We provide a transparent geometric interpretation of the AGNES parameters in terms of their scaling with problem parameters (Appendix F.3) and the continuum limit models for various scaling regimes (Appendix C).
5. We build strong intuition for the choice of hyperparameters for machine learning applications and empirically demonstrate that AGNES improves the training of CNNs relative to SGD with momentum and Nesterov's accelerated gradient descent.

## 2 Literature Review

Accelerated first order methods.Accelerated first order methods have been extensively studied in convex optimization. Beginning with the conjugate gradient (CG) algorithm introduced by Hestenes and Stiefel (1952), the Heavy ball method of Polyak (1964), and Nesterov (1983)'s seminal work on accelerated gradient descent, many authors have developed and analyzed accelerated first order methods for convex problems, including Beck and Teboulle (2009), Nesterov (2012, 2013), Chambolle and Dossal (2015), Kim and Fessler (2018) to name just a few.

An important line of research is to gain an understanding of how accelerated methods work. After Polyak (1964) derived the original Heavy ball method as a discretization of an ordinary differential equation, Alvarez et al. (2002), Su et al. (2014), Wibisono et al. (2016), Zhang et al. (2018), Siegel (2019), Shi et al. (2019), Muehlebach and Jordan (2019), Wilson et al. (2021), Shi et al. (2021), Suh et al. (2022), Attouch et al. (2022), Aujol et al. (2022b, a), Dambrine et al. (2022) studied accelerated first order methods from the point of view of ODEs. This perspective has facilitated the use of Lyapunov functional analysis to quantify the convergence properties. We remark that in addition to the intuition provided by differential equations, Joulani et al. (2020) and Gasnikov and Nesterov (2018) have also proposed interesting ideas for explaining and deriving accelerated first-order methods. In addition, there has been a large interest in deriving adaptive accelerated first order methods, see for instance Levy et al. (2018), Cutkosky (2019), Kavis et al. (2019).

Stochastic optimization.Robbins and Monro (1951) first introduced optimization algorithms where gradients are only estimated by a stochastic oracle. For convex optimization, Nemirovski et al. (2009), Ghadimi and Lan (2012) obtained minimax-optimal convergence rates with additive stochastic noise.

In deep learning, stochastic algorithms are ubiquitous in the training of deep neural networks, see (LeCun et al., 1998, 2015; Goodfellow et al., 2016; Bottou et al., 2018). Here, the additive noise assumption not usually appropriate. As Wojtowytsch (2023), Wu et al. (2022a) show, the noise is of low rank and degenerates on the set of global minimizers. Stich (2019), Stich and Karimireddy (2022), Bassily et al. (2018), Gower et al. (2019), Damian et al. (2021), Wojtowytsch (2023), Zhou et al. (2020) consider various non-standard noise models and (Wojtowytsch, 2021; Zhou et al., 2020; Li et al., 2022) study the continuous time limit of stochastic gradient descent. These include noise assumptions for degenerate noise due to Bassily et al. (2018), Damian et al. (2021), Wojtowytsch (2023, 2021), low rank noise studied by Damian et al. (2021), Li et al. (2022) and noise with heavy tails explored by Zhou et al. (2020).

Acceleration with stochastic gradients.Kidambi et al. (2018) prove that there are situations in which it is impossible for any first order oracle method to improve upon SGD due to information theoretic lower bounds. More generally, lower bounds in the stochastic first order oracle (SFO) model were presented by Nemirovski et al. (2009) (see also (Ghadimi and Lan, 2012)).

A partial improvement on the state of the art is given by Jain et al. (2018), who present an accelerated stochastic gradient method motivated by a particular low-dimensional and strongly convex problem. Laborde and Oberman (2020) obtain faster convergence of an accelerated method under an additive noise assumption by a Lyapunov function analysis. Bollapragada et al. (2022) study an accelerated gradient method for the optimization of a strongly convex quadratic objective function with minibatch noise.

Closest to our work are Liu and Belkin (2018); Vaswani et al. (2019); Even et al. (2021); Berthier et al. (2021) who study generalizations of Nesterov's method in stochastic optimization. Liu and Belkin (2018); Even et al. (2021) obtain guarantees with noise of approximately multiplicative noise in overparametrized linear least squares problems and for general convex objective functions with additive noise and in deterministic optimization. Vaswani et al. (2019) obtain comparable guarantees for the more complicated method of Nesterov (2013).

## 3 Algorithm and Convergence Guarantees

### Assumptions

In the remainder of this article, we consider the task of minimizing an objective function \(f:^{m}\) using stochastic gradient estimates \(g\). We assume that \(f\), \(g\) and the initial condition \(x_{0}\) satisfy:

1. The initial condition \(x_{0}\) is a (potentially random) point such that \([f(x_{0})+\|x_{0}\|^{2}]<\).
2. \(f\) is \(L-\)smooth, i.e. \( f\) is \(L-\)Lipschitz continuous with respect to the Euclidean norm.
3. There exists a probability space \((,,)\) and a gradient estimator, i.e. a measurable function \(g:^{m}^{m}\) such that for all \(x^{m}\) the properties * \(_{}[g(x,)]= f(x)\) (unbiased gradient oracle) and * \(_{}\|g(x,)- f(x)\|^{2}^{2} \,\| f(x)\|^{2}\) (multiplicative noise scaling) hold.

A justification of the multiplicative noise scaling is given in Section 4. In the setting of machine learning, the space \(\) is given by the random subsampling of the dataset. A rigorous discussion of the probabilistic foundations is given in Appendix D.

### Nesterov's Method with Multiplicative Noise

First we analyze Nesterov (1983)'s accelerated gradient descent algorithm (NAG) in the setting of multiplicative noise. NAG is given by the initialization \(x_{0}=x_{0}^{}\) and the two-step iteration

\[x_{n+1}=x_{n}^{}- g_{n}^{}, x_{n+1}^{}=x_{n+1}+ _{n}x_{n+1}-x_{n}=x_{n+1}+_{n}(x_{n}^{}- g_{n} ^{}-x_{n})\] (1)

where \(g_{n}^{}=g(x_{n}^{},_{n})\) and the variables \(_{n}\) are iid samples from the probability space \(\), i.e. \(g_{n}^{}\) is an unbiased estimate of \( f(x_{n}^{})\). We write \(\) instead of \(_{n}\) in cases where a dependence on \(n\) is not required. We show that this scheme achieves an \(O(1/n^{2})\) convergence rate for convex functions but _only in the case that \(<1\)_. To the best of our knowledge, this analysis is optimal.

**Theorem 1** (NAG, convex case).: _Suppose that \(x_{n}\) and \(x_{n}^{}\) are generated by the time-stepping scheme (1), \(f\) and \(g\) satisfy the conditions laid out in Section 3.1, \(f\) is convex, and \(x^{*}\) is a point such that \(f(x^{*})=_{x^{m}}f(x)\). If \(<1\) and the parameters are chosen such that_

\[0<}{L(1+^{2})},_{n}= ,[f(x_{n})-f(x^{*})][\|x_{0}-x^{*}\|^{2}]}{ n^{2}}.\]

_The expectation on the right hand side is over the random initialization \(x_{0}\)._

The proof of Theorem 1 is given in Appendix E. Note that the constant \(1/\) blows up as \( 1\) and the analysis yields no guarantees for \(>1\). This mirrors numerical experiments in Section 5.

**Theorem 2** (NAG, strongly convex case).: _In addition to the assumptions in Theorem 1, suppose that \(f\) is \(\)-strongly convex and the parameters are chosen such that_

\[0<}{L(1+^{2})}= }{1+},[f(x_{n})-f(x^{*})] 2(1-)^{n} \,[f(x_{0})-f(x^{*})].\]

[MISSING_PAGE_FAIL:4]

There, we also present an alternative version of Theorem 3 for a different choice of parameters

\[)},<}, _{n}=}{n+n_{0}+3}\]

for a potentially large \(n_{0}}{-(1+^{2})} 2^{2}\). The convergence guarantees are similar in both cases.

The benefit of the accelerated scheme is an improvement from a decay rate of \(O(1/n)\) to the rate \(O(1/n^{2})\), which is optimal under the given assumptions even in the deterministic case. While the noise can be orders of magnitude larger than the quantity we want to estimate, it only affects the constants in the convergence, not the rate. We get an analogous result for strongly convex functions.

**Theorem 4** (AGNES, strongly convex case).: _In addition to the assumptions in Theorem 3, suppose that \(f\) is \(\)-strongly convex and the parameters are chosen such that_

\[0<)},=}}}{1+}}}, =}}{1-}+^{2}}\, \]

\[[f(x_{n})-f(x^{*})] 2(1-} })^{n}[f(x_{0})-f(x^{*})].\]

Choosing \(\) too small can be interpreted as overestimating \(L\) or \(\). Choosing \(\) too small (with respect to \(\)) can be interpreted as overestimating \(\). Since every \(L\)-Lipschitz function is \(L^{}\)-Lipschitz for \(L^{}>L\), and since the multiplicative noise bound with constant \(\) implies the same bound with \(^{}>\), exponential convergence still holds at a generally slower rate.

We note that since \(| f(x)|^{2} 2L(f(x)- f)\) (Lemma 12 in Appendix D), Theorems 3 and 4 lead to analogous convergence results for \([ f(x_{n})]\) as well. Due to the summability of the sequences \(n^{-2}\) and \(r^{n}\) for \(r<1\), we get not only convergence in expectation but also almost sure convergence. The proof is given in Appendix E.

**Corollary 5**.: _In the setting of Theorems 3 and 4, \(f(x_{n}) f\) with probability 1._

In the deterministic case \(=0\), we have \(=\) in both Theorems 3 and 4. In Theorem 4, the parameters coincide with the usual choice for NAG, while we opted for a simple statement in Theorem 3 which does not exactly recover the standard choice \(=1/L\) and \(_{n}=n/(n+3)\). The proofs below easily cover these special cases as well. If \(0<<1\), both AGNES and NAG converge with the same rate \(n^{-2}\) in the convex case, but the constant of NAG is always larger. In the strongly convex case, even the decay rate of NAG is slower than AGNES for \((0,1)\) since \(1-^{2}<(1+^{2})^{-1}\). We see the real power of AGNES in the stochastic setting where it converges for very high values of \(\) when Nesterov's method may diverge. For the optimal choice of parameters, we summarize the results in terms of the time-complexity of SGD and AGNES in Figure 1. For the related guarantee for SGD, see Theorems 17 and 22 in Appendices E and F respectively.

_Remark 6_ (Batching).: Let us compare AGNES with two families of gradient estimators:

1. \(g^{}_{n}=g(x^{}_{n},_{n})\) as studied in Theorems 3 and 4.
2. A gradient estimator \(g^{}_{n}:=}_{j=1}^{n_{b}}g(x^{}_{n},_{n,j})\) which averages multiple independent estimates to reduce the variance.

The second gradient estimator falls into the same framework with \(=^{n_{b}}\) and \(^{2}=^{2}/n_{b}\). Assuming vector additions cost negligible time, optimizer steps are only as expensive as gradient evaluations. In this setting - which is often realistic in deep learning - it is appropriate to compare \([f(x_{n_{b}n})]\) (\(n_{b} n\) iterations using \(g^{}_{n}\)) and \([f(X_{n})]\) (\(n\) iterations with \(g^{}_{n}\)). For the strongly convex case, we note that \((1-}\,})^{n_{b}} 1- }\,/n_{b}}\) if and only if

\[n_{b}}\,/n_{b}} )}{(1-}\,})} }\,/n_{b}}}{}\,}}=}{1+^{2}/n_{b}}= }{n_{b}+^{2}}\,n_{b}.\]The approximation is well-justified in the important case that \( L\). In particular, the upper bound for non-batching AGNES is _always_ favorable compared to the batching version as \(n_{b}_{ 1}\), and the two only match for the optimal batch size \(n_{b}=1\). The optimal batch size for minimizing \(f\) is the largest one that can be processed in parallel without increasing the computing time for a single step. A similar argument holds for the convex case.

With a slight modification, the proof of Theorem 3 extends to the situation of convex objective functions which do not have minimizers. Such objectives arise for example in linear classification with the popular cross-entropy loss function and linearly separable data.

**Theorem 7** (Convexity without minimizers).: _Let \(f\) be a convex objective function satisfying the assumptions in Section 3.1 and \(x_{n}\) be generated by the time-stepping scheme (3). Assume that \(,\) and \(_{n}\) are as in Theorem 3. Then \(_{n}[f(x_{n})]=_{x^{m}}f(x)\)._

The proof and more details are given in Appendix E. For completeness, we consider the case of non-convex optimization in Appendix G. As a limitation, we note that multiplicative noise is well-motivated in machine learning for global minimizers, but not at generic critical points.

### Geometric Interpretation

Let us briefly discuss the parameter choices in Theorem 4. As we consider larger \(\) for fixed \(\) and \(L\), the decay factor \(\) moves closer to \(1\). This slows the 'forgetting' of past gradients in \(v_{n}\), allowing us to better average out stochastic noise. The price we pay is computing with more outdated gradients, slowing convergence. Our choice balances these effects.

In AGNES, \(\) inadvertently also governs magnitude of the momentum variable \(v_{n}\), which scales as \((1-)^{-1}\) for objective functions with constant gradient and \(n 1\). To compensate, we choose \(\) smaller compared to \(\) when \(\) (and thus \((1-)^{-1}\)) is large. Nevertheless, the effect of the momentum step does not decrease. For further details, see Appendix F.3.

For further interpretability, we obtain several ODE and SDE continuous time descriptions of AGNES in Appendix C.

## 4 Motivation for Multiplicative Noise

In supervised learning applications, the learning task often corresponds to minimizing a risk or loss function \((w)=_{i=1}^{N}h(w,x_{i}),y_{i}=: _{i=1}^{N}_{i}(w)\), where \(h:^{m}^{d}^{k}\), \((w,x) h(w,x)\) and \(:^{k}^{k}[0,)\) are a parametrized function of weights \(w\) and data \(x\) and a loss function measuring compliance between \(h(w,x_{i})\) and \(y_{i}\) respectively.1Safran and Shamir (2018); Chizat and Bach (2018); Du et al. (2018) show that working in the overparametrized regime \(m N\) simplifies the optimization process and Belkin et al. (2019, 2020) illustrate that it facilitates generalization to previously unseen data. Cooper (2019) shows that fitting \(N\) constraints with \(m\) parameters typically leads to an \(m-N\)-dimensional submanifold \(\) of the parameter space \(^{m}\)

Figure 1: The minimal \(n\) for AGNES and SGD such that \([f(x_{n})- f]<\) when minimizing an \(L\)-smooth function with multiplicative noise intensity \(\) in the gradient estimates and under a convexity assumption. The SGD rate of the \(\)-strongly convex case is achieved more generally under a PL condition with PL-constant \(\). While SGD requires the optimal choice of one variable to achieve the optimal rate, AGNES requires three (two in the determinstic case).

such that all given labels \(y_{i}\) are fit exactly by \(h(w,)\) at the data points \(x_{i}\) for \(w\), i.e. \( 0\) on the smooth set of minimizers \(=^{-}(\{0\})\).

If \(N\) is large, it is computationally expensive to evaluate the gradient \((w)=_{i=1}^{N}_{i}\) of the risk function \(\) exactly and we commonly resort to stochastic estimates

\[g=}_{i I_{b}}_{i}(w)=}_{i I _{b}}_{j=1}^{k}(_{h_{j}})h(w,x_{i}),y_{i}\, _{w}h_{j}(w,x_{i}),\]

where \(I_{b}\{1,,N\}\) is a subsampled collection of \(n_{b}\) data points (a batch or minibatch). Minibatch gradient estimates are very different from the stochasticity we encounter e.g. in statistical mechanics:

1. The covariance matrix \(=_{i=1}^{N}_{i}- (_{i}-)\) of the gradient estimators \(_{i}\) has low rank \(N m\).
2. Assume specifically that \(\) is a loss function which satisfies \((y,y)=0\) for all \(y^{k}\), such as the popular \(^{2}\)-loss function \((h,y)=\|h-y\|^{2}\). Then \(_{i}(w)=0\) for all \(i\{1,,N\}\) and all \(w=^{-1}(0)\). In particular, minibatch gradient estimates are exact on \(\).

The following Lemma makes the second observation precise in the overparameterized regime and bounds the stochasticity of mini-batch estimates more generally.

**Lemma 8** (Noise intensity).: _Assume that \((h,y)=\|h-y\|^{2}\) and \(h:^{m}^{d}^{k}\) satisfies \(\|_{u}h(w,x_{i})\|^{2} C1+\|w\|^{p}\) for some \(C,p>0\) and all \(w^{m}\) and \(i=1,,N\). Then for all \(w^{m}\):_

\[_{i=1}^{N}\|_{i}-\|^{2} \;\;4C^{2}(1+\|w\|)^{2p}(w).\]

Lemma 8 is proved in Appendix H. It is a modification of [23, Lemma 2.14] for function models which are locally, but not globally Lipschitz-continuous in the weights \(w\), such as deep neural networks with smooth activation function. The exponent \(p\) may scale with network depth.

Lemma 8 describes the variance of a gradient estimator which uses a random index \(i\{1,,N\}\) and the associated gradient \(_{i}\) is used to approximate \(\). If a batch \(I_{b}\) of \(n_{b}\) indices is selected randomly with replacement, then the variance of the estimates scales in the usual way:

\[_{I_{b}}[\|}_{i I_{b}}_{i}- \|^{2}](1+\|w\|)^{2p}}{n_{b}} \,(w).\] (4)

Figure 2: To be able to quantify the gradient noise exactly, we choose relatively small models and data sets. **Left:** A ReLU network with four hidden layers of width 250 is trained by SGD to fit random labels \(y_{i}\) (drawn from a 2-dimensional standard Gaussian) at \(1,000\) random data points \(x_{i}\) (drawn from a 500-dimensional standard Gaussian). The variance \(^{2}\) of the gradient estimators is \( 10^{5}\) times larger than the loss function and \( 10^{6}\) times larger than the parameter gradient. This relationship is stable over approximately ten orders of magnitude. **Right:** A ReLU network with two hidden layers of width 50 is trained by SGD to fit the Runge function \(1/(1+x^{2})\) on equispaced data samples in the interval \([-8,8]\). Also here, the variance in the gradient estimates is proportional to both the loss function and the magnitude of the gradient.

As noted by Wu et al. (2019, 2022), \(\) and \(\|\|^{2}\) often behave similarly in overparametrized deep learning. We illustrate this in Figure 2 together with Lemma 8. Heuristically, we therefore replaced (4) by a more manageable assumption akin to \([_{i=1}^{N}\|_{i}-\|^{2}] ^{2}\|\|^{2}\) in Section 3.1. The setting where the signal-to-noise ratio (the quotient of estimate variance and true magnitude) is \((1)\) is often referred to as'multiplicative noise', as it resembles the noise generated by estimates of the form \(g=(1+ Z)\), where \(Z(0,1)\). When the objective function is \(L\)-smooth and satisfies a PL condition (see e.g. (Karimi et al., 2016)), both scaling assumptions are equivalent.

## 5 Numerical Experiments

### Convex optimization

We compare the optimization algorithms for the family of objective functions

\[f_{d}:, f_{d}(x)=|x|^{d}& |x|<1\\ 1+d(|x|-1)&\]

for \(d 2\) with gradient estimators \(g=(1+ N)f^{}(x)\), where \(N\) is a unit normal random variable. The functions are convex and their derivatives are Lipschitz-continuous with \(L=d(d-1)\). Various trajectories are compared for different values of \(d\) and \(\) in Figure 3. We run AGNES with the parameters \(=}\), \(=)}\), \(_{n}=\) derived above and SGD with the optimal step size \(=)}\) (see Lemmas 16 and 17). For NAG, we select \(=)}\) and \(_{n}=\). We present a similar experiment in the strongly convex case in Appendix A.

We additionally compare to two other methods of accelerated gradient descent which were recently proposed for multiplicative noise models: The ACDM method of Nesterov (2012), Vaswani et al. (2019), and the continuized Nesterov method (CNM) of Even et al. (2021), Berthier et al. (2021) with the proposed parameters. In this simple setting where all constants are known, AGNES, ACDM and CNM perform comparably in the long run and on average.

### Neural network regression

We generated \(n=100,000\) 12-dimensional random vectors. Using a fixed, randomly initialized neural network \(f^{*}\) (with 10 hidden layers, each with width 10, and output dimension 1), we produced labels \(y_{i}=f^{*}(x_{i})\). The resulting dataset was split into 90% training and 10% testing data. We then trained identically initialized copies of a larger neural network (15 hidden layers, each with width 15) using Adam, NAG, SGD with momentum, and AGNES to minimize the mean-squared error (MSE) loss.

Figure 3: We plot \([f_{d}(x_{n})]\) on a loglog scale for SGD (blue), AGNES (red), NAG (green), ACDM (orange) and CNM (maroon) with \(d=4\) (left) and \(d=16\) (right) for noise levels \(=0\) (solid line), \(=10\) (dashed) and \(=50\) (dotted). The initial condition is \(x_{0}=1\) in all simulations. Means are computed over 200 runs. After an initial plateau, AGNES, CNM and ACDM significantly outperform SGD in all settings, while NAG (green) diverges if \(\) is large. The length of the initial plateau increases with \(\).

We selected the learning rate \(10^{-3}\) for Adam as it performed poorly at higher or lower rates \(10^{-2}\) and \(10^{-4}\). For AGNES, NAG, and SGD, based on initial exploratory experiments, we used a learning rate of \(10^{-4}\), a momentum value of 0.99, and for AGNES, a correction step size \(=10^{-3}\). The experiment was repeated 10 times each for batch sizes 100, 50, and 10, and run for 45,000 optimizer steps each time. The average loss and standard deviation for each algorithm are reported in Figure 4. The results show that AGNES performs better than SGD and NAG for all batch sizes. With large batch size, Adam performs well with default hyperparameters. The performance of AGNES relative to other algorithms especially improves as the batch size decreases.

### Image classification

We trained ResNet-34 (He et al., 2016) with batch sizes 50 and 10, and ResNet-50 with batch size 50 on the CIFAR-10 image dataset (Krizhevsky et al., 2009) with standard data augmentation (normalization, random crop, and random flip) using Adam, SGD with momentum, NAG, and AGNES. The model implementations were based on (Liu, 2017). Each algorithm was provided an identically initialized model and the experiment was repeated 5 times for 50 epochs each. The averages and standard deviations of training loss and test accuracy are reported in Figure 5. We used the same initial learning rate \(10^{-3}\) for all the algorithms, which was dropped to \(10^{-4}\) after 25 epochs. A momentum value of 0.99 was used for SGD, NAG, and AGNES and a constant correction step size \(=10^{-2}\) was used for AGNES.

AGNES reliably outperforms SGD and NAG both in terms of training loss and test accuracy. The gap in performance appears to increase as model size increases or batch size decreases, suggesting that AGNES primarily excels in situations where gradients are harder to estimate accurately. For the sake of completeness, we include Adam with default hyperparameters as a comparison.

In congruence with convergence guarantees from convex optimization, grid search suggests that \(\) is the primary learning rate and \(\) should be chosen larger than \(\). We tried NAG and Adam with higher learning rates \(10^{-2}\) and \(10^{-1}\) as well to ensure a fair comparison with AGNES, but found that they become unstable or perform worse for larger learning rates in our experiments. The AGNES default parameters \(=10^{-3}\), \(=10^{-2}\), \(=0.99\) in Algorithm 1 give consistently strong performance on different models but can be further tuned to improve performance. While the numerical experiments we performed support our theoretical predictions, we acknowledge that our focus lies on theoretical guarantees and we did not test these predictions over a broad set of benchmark problems.

Figure 4: We report the training loss as a running average with decay rate 0.99 (top row) and test loss (bottom row) for batch sizes 100 (left column), 50 (middle column), and 10 (right column) in the setting of Section 5.2. The horizontal axis represents the number of optimizer steps. The performance gap between AGNES and other algorithms widens for smaller batch sizes, where the gradient estimates are more stochastic and the two different parameters \(,\) add the most benefit.

We present a more thorough comparison of NAG and AGNES with various parameter selections in Figure 8 in Appendix A. With default parameters or minimal parameter tuning, AGNES reliably achieves superior performance compared to NAG (training loss) and smoother curves, suggesting more stable behavior (test accuracy).

### Hyperparameter comparison

We tried various combinations of AGNES hyperparameters \(\) and \(\) to train LeNet-5 on the MNIST dataset to determine which hyperparameter has a greater impact on training. With a fixed batch size of 60 and a momentum value \(=0.99\), we trained independent copies of the model for 6 epochs for each combination of the hyperparameters. The average training loss over the epoch was recorded after each epoch. The results are reported in Figure 6. We see that \(\) has the largest impact on the rate of decay of the loss, which establishes it as the 'primary learning rage'. If \(\) is too small, the algorithm converges slowly and if \(\) is too large, it diverges. If \(\) is chosen correctly, a good choice of the correction step size \(\) (which can be orders of magnitude larger than \(\)) further accelerates convergence, but \(\) cannot compensate for a poor choice of \(\).

Figure 5: We report the training loss as a running average with decay rate 0.99 (top row) and test accuracy (bottom row) for ResNet-34 trained on CIFAR-10 with batch sizes 50 (left column) and 10 (middle column), and ResNet-50 trained with batch size 50 (right column). The performance of AGNES with the proposed hyperparameters is stable over the changes in model and batch size.

Figure 6: We report the average training loss after each epoch for six epochs for training LeNet-5 on MNIST with AGNES for various combinations of the hyperparameters \(\) and \(\) to illustrate that \(\) is the algorithm’s primary learning rate. **Left:** For a given \(\) (color coded), the difference in the trajectory for the three values of \(\) (line style) is marginal. On the other hand, choosing \(\) well significantly affects performance. **Middle:** For any given \(\), the largest value of \(\) performs much better than the other three values which have near-identical performance. Nevertheless, the worst performing value of \(\) with well chosen \(=5 10^{-3}\) performs better than the best performing value of \(\) with \(=5 10^{-4}\). **Right:** When \(\) is too large, the loss increases irrespective of the value of \(\).