# Learning Functional Transduction

Mathieu Chalvidal

Capital Fund Management

Paris, France

mathieu.chalvidal@gmail.com &Thomas Serre

Carney Institute for Brain Science

Brown University, U.S.

thomas_serre@brown.edu &Rufin VanRullen

Centre de Recherche Cerveau & Cognition

CNRS, Universite de Toulouse, France

rufin.vanrullen@cnrs.fr

###### Abstract

Research in statistical learning has polarized into two general approaches to perform regression analysis: Transductive methods construct estimates directly based on exemplar data using generic relational principles which might suffer from the curse of dimensionality. Conversely, inductive methods can potentially fit highly complex functions at the cost of compute-intensive solution searches. In this work, we leverage the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS) to propose a hybrid approach: We show that transductive regression systems can be meta-learned with gradient descent to form efficient _in-context_ neural approximators of function defined over both finite and infinite-dimensional spaces (operator regression). Once trained, our _Transducer_ can almost instantaneously capture new functional relationships and produce original image estimates, given a few pairs of input and output examples. We demonstrate the benefit of our meta-learned transductive approach to model physical systems influenced by varying external factors with little data at a fraction of the usual deep learning training costs for partial differential equations and climate modeling applications.

## 1 Introduction

**Transduction vs. induction**\(\) In statistical learning, transductive inference (Vapnik, 2006) refers to the process of reasoning directly from observed (training) cases to new (testing) cases and contrasts with inductive inference, which amounts to extracting general rules from observed training cases to produce estimates. The former principle powers some of the most successful regression algorithms benefiting from straightforward construction properties, from \(k\)-Nearest Neighbors (Cover and Hart, 1967) to Support Vector Machines (Boser et al., 1992) or Gaussian Processes (Williams and Rasmussen, 1995). In contrast, deep learning research has mostly endeavored to find inductive solutions, relying on stochastic gradient descent to faithfully encode functional relationships described by large datasets into the weights of a neural network. Although ubiquitous, inductive neural learning with gradient descent is compute-intensive, necessitates large amounts of data, and poorly generalizes outside of the training distribution (Jin et al., 2020) such that a slight modification of the problem might require retraining and cause "catastrophic" forgetting of the previous solution (McCloskey and Cohen, 1989). This may be particularly problematic for real-world applications where data has heterogeneous sources, or only a few examples of the target function are available.

**Meta-learning functional regression \(\)** In this work, we meta-learn a regression program in the form of a neural network able to approximate an infinity of functions defined on finite or infinite-dimensional spaces through a transductive formulation of the solution based on the representer theorem. Namely, our model is meta-trained to take as input any dataset \(_{}\) of pairs \((_{i},(_{i}))_{i I}\) of some target function \(\) together with a query element \(^{}\) and produces directly an estimate of the image \((^{})\). After meta-training, our network is able to perform regression of unseen operators \(^{}\) from varying dataset sizes in a single feedforward pass, such that our model can be interpreted as performing _in-context functional learning_. In order to build such a model, we leverage the theory of Reproducing Kernel Banach Spaces (RKBS) (Micchelli and Pontil, 2004; Zhang, 2013; Lin et al., 2022) and interpret the Transformer's (Vaswani et al., 2017) attention mechanism as a parametric vector-valued reproducing kernel. While kernel regression might be plagued by the "curse of dimensionality" (Bellman, 1966; Aggarwal et al., 2001), we show that our meta-learning approach can escape this pitfall, allowing, for instance, to perform instantaneous regressions over spaces of operators from a few example points, by building solutions to regression problem instances directly from the general reproducing kernel associated with such spaces.

**Contributions \(\)** We introduce the _Transducer_, a novel meta-learning approach leveraging reproducing kernel theory and deep learning methods to perform instantaneous regression of an infinity of functions in reproducing kernel spaces.

* Our model learns an implicit regression program able to identify, in a single feedforward pass, elements of specific functional spaces from any corresponding collection of input-output pairs describing the target function. Such ultra-fast regression program, which bypasses the need for gradient-based training, is also general and can be applied to functions either defined on finite dimensional spaces (scalar-valued function spaces) or infinite dimensional spaces (function-valued operator spaces).
* In particular, we demonstrate the flexibility and efficiency of our framework for fitting function-valued operators in two PDEs and one climate modeling problem. We show that our transductive approach allows for better generalization properties of neural operator regression, better precision when relevant data is available, and can be combined with iterative regression schemes that are too expensive for previous inductive approaches, thus holding potential to improve neural operators applicability.
* To the best of our knowledge, our proposal is the first to marry vector-valued RKBS theory with deep meta-learning and might also shed new light on the in-context learning abilities observed in deep attentional architectures.

Figure 1: Batches of functional images \(_{}(_{_{i}})(_{i}) _{i}(_{j})=_{j}(x,t) C(^{2},)\) obtained with the same _Transducer_ model \(_{}\) but conditioned, at each row, by a different dataset \((_{_{i}})_{i 3}\) during feedforward computation. Each underlying operator \(_{i}\) corresponds to a different advection-diffusion-reaction equation (defined in Sec. 5.1) with spatially varying advection, diffusion, and reaction parameters unseen during training, and functions \((_{j})_{j 9}\) correspond to initial conditions. **While usual neural regression approaches learn a single target function (one row), our model learns to approximate instantaneously an infinity of them.**

## 2 Problem formulation

Let \(\) and \(\) be two (finite or infinite-dimensional) Banach spaces, respectively referred to as the input and output space, and let \(\) a Banach space of functions from \(\) to \(\). We also note \(L(,)\) (resp. \(L()\)) the set of bounded linear operators from \(\) to \(\) (resp. to itself). We consider the _meta-learning_ problem of creating a function \(\) able to approximate any functional element \(\) in the space \(\) from any finite collection of example pairs \(_{}=\{(_{i},_{i})_{i},_{i}=(_{i})\}_{i n}\). A prominent approach in statistical learning is _empirical risk minimization_ which consists in predefining a class \(}\) of computable functions from \(\) to \(\) and subsequently selecting a model \(}\) as a minimizer (provided its existence) of a risk function \(:}\):

\[(_{})*{argmin}_{}}}(},_{})\] (1)

For instance, the procedure consisting in performing gradient-based optimization of objective (1) over a parametric class \(}\) of neural networks defines implicitly such a function \(\). Fundamentally, this technique works by induction: It captures the statistical regularities of a single map \(\) into the parameters of the neural network \(}\) such that \(_{}\) is discarded for inference. Recent examples of gradient-based optimization of neural networks for operator regression (i.e when \(\) and \(\) are infinite-dimensional) are DeepOnet (Lu et al., 2019) or Fourier Neural Operator (FNO) (Li et al., 2020). As previously discussed, for every regression problem instance, evaluating \(\) with these approaches requires a heavy training procedure. Instead, we show in this work that for specific spaces \(}\), we can meta-learn a parametric map \(_{}\) that transductively approximates (in a certain functional sense) any target function \(\) given a corresponding dataset \(_{}\) such that:

\[,\ \ (_{})()= _{}(_{1},(_{1}),,_{n },(_{n}),)()\] (2)

## 3 Vector-valued Reproducing Kernel Banach Space regression

In order to build \(_{}\), we leverage the structure of Reproducing Kernel Banach Spaces (RKBS) of functions \(\) and combine it with the universal approximation abilities of deep networks. As we will see in the experimental section, RKBS are very general spaces occurring in a wide range of machine learning applications. We start by recalling some elements of the theory of vector-valued RKBS developed in Zhang (2013). Namely, we will consider throughout _uniform_ Banach spaces \(\) (such condition guarantees the unicity of a compatible semi-inner product \(.,._{}:\), i.e. \(,,_{}=||||_{}^{2}\) and allows to build a bijective and isometric dual space \(^{*}\)).

**Theorem 1** (**Vector-valued RKBS (Zhang, 2013)**).: _A \(\)-valued reproducing kernel Banach space \(\) of functions from \(\) to \(\) is a Banach space such that for all \(\), the point evalutation \(_{}:\) defined as \(_{}()=()\) is continuous. In this case, there exists a unique function \(: L()\) such that for all \((,)\):_

\[^{}(,^{})( )\\ \ ,\ (),_{ }=,(,.)()_{ }\\ \ ^{},\ \|(,^{})\|_{L( )}\|_{}\|_{L(,)}\| _{^{}}\|_{L(,)}\] (3)

Informally, theorem (1) states that RKBS are spaces sufficiently regular such that the image of _any_ element \(\) at a given point \(\) can be expressed in terms of a unique function \(\). The latter is hence called the _reproducing kernel_ of \(\) and our goal is to leverage such unicity to build the map \(_{}\). Let \(}\) be the set of all datasets \(_{}\) previously defined. The following original theorem gives the existence of a solution to our meta-learning problem and relates it to the reproducing kernel.

**Theorem 2** (**RKBS representer map**).: _Let \(\) be a \(\)-valued RKBS from \(\) to \(\), if for any dataset \(_{}}\), \((.,_{})\) is lower semi-continuous, coercive and bounded below, then there exists a function \(:}\) such that \((_{})\) is a minimizer of equation (1). If \(\) is of the form \((.,_{})=}\{_{ {v}_{i}}\}_{i n}\) with \(}:^{n}\), then the dual \((_{})\)* is in \(\{(_{i},.)()^{*},i n, \}\). Furthermore, if for any \(_{}\), \((.,_{})\) is strictly-convex, then \(\) is unique._

While theorem (2) provides conditions for the existence of solutions to each regression problem defined by (1), the usual method consisting in solving instance-specific minimization problemsderived from representer theorems characterizations is generally intractable in RKBS for several reasons (non-convexity and infinite-dimensionality of the problem w.r.t to variable \(\), non-additivity of the underlying semi-inner product). Instead, we propose to define image solutions \((_{})=_{i n}_{}( _{i},.)(}_{i})\) where \(_{}\) and \((}_{i})\) are respectively the learned approximation of the \(\)-valued reproducing kernel \(\) and a set of functions in \(\) resulting from a sequence of deep transformations of image examples \((_{i})\) that we define below.

**Transformers attention as a reproducing kernel \(\)** We first need to build \(\). Several pieces of work have proposed constructions of \(\) in the context of a non-symmetric and nonpositive semi-definite real-valued kernel (Zhang et al., 2009; Georgiev et al., 2014; Lin et al., 2019; Xu and Ye, 2019). In particular, the exponential key-query function in the popular Transformer model (Vaswani et al., 2017) has been interpreted as a real-valued reproducing kernel \(}:\) in Wright and Gonzalez (2021). We extend below this interpretation to more general vector-valued RKBS:

**Proposition 1** (**Dot-product attention as \(\)-valued reproducing kernel**).: _Let \((p_{j})_{j J}\) a finite sequence of strictly positive integers, let \((A^{j}_{})_{j J}\) be applications from \(\) to \(\), let \(V^{j}_{}\) be linear applications from \(L(,^{p_{j}})\) and \(W_{}\) a linear application from \(L(_{j J}^{p_{j}},)\), the (multi-head) application \(}: L()\) defined by_

\[}(,^{})() W_{}[...,A^{j}_{}(,^{}) V^{j}_{}(),...]_{j J}\] (4)

_is the reproducing kernel of an \(\)-valued RKBS. In particular, if \(==^{p}\), for \(p^{+}\) and \(A^{j}_{}=(Q^{j}_{})^{T}( K^{j}_{}^{})/(,^{})\) with \((Q^{j}_{},K^{j}_{})_{j J}\) applications from \(L(,^{d})\), \(}\) corresponds to the dot-product attention mechanism of Vaswani et al. (2017)._

Note that in (4), the usual softmax normalization of the dot-product attention is included in the linear operations \(A^{j}_{}\) through \(\). We show in the next section how such kernel construction can be leveraged to build the map \(_{}\) and that several variations of the kernel construction are possible, depending on the target space \(\) and applications. Contrary to usual kernel methods, our model jointly builds the full reproducing kernel approximation \(_{}\) and the instance-specific parametrization \((}_{i})_{i I}\) by integrating the solutions iteratively over several residual kernel transformations. We refer to our system as a _Transducer_, both as a tribute to the Transformer computation mechanism from which it is inspired and by analogy with signal conversion devices.

## 4 The Transducer

**Model definition \(\)** We define \(_{}\) as the sum of \(L\) residual kernel transformations \(\{_{}}\}_{ L}\) whose expression can be written:

\[\ ,\ \ _{}(_{ })()=_{i I}_{}(_{i},)(}_{i})=_{i I}_{}_{}}(^{}_{i},^{})(^{}_{i})\] (5)

where \((^{}_{i},^{}_{i})_{i n,l L}\) and \((^{})_{l L}\) refer to sequences of representations starting respectively with \((^{1}_{i},^{1}_{i})_{i n}=_{}\), \(^{1}=\) and defined by the following recursive relation:

\[^{+1}_{i}=F^{}_{}(^{}_{i})\,\ ^{ +1}=F^{}_{}(^{})\\ ^{+1}_{i}=}^{}_{i}+_{j}_{}}(^{+1}_{j},^{+1}_{i})(}^{}_{j}) { where }}^{}_{i}=G^{}_{}(^{}_{i})\] (6)

where \((F^{}_{},G^{}_{})_{ L}\) correspond to (optional) parametric non-linear residual transformations applied in parallel to representations \((^{}_{i},^{}_{i})_{i n}\) while \((_{}})_{ L}\) are intermediate kernel transformations of the form \(:()\) such as the one defined in equation (4). Breaking down kernel estimation through this sequential construction allows for iteratively refining the reproducing kernel estimate and approximating on-the-fly the set of solutions \((}_{i})_{i I}\). We particularly investigate the importance of depth \(L\) in the experimental section. Note that equations (5) and (6) allow to handle both varying dataset sizes and efficient parallel inference by building the sequences \((^{})_{ L}\) with \((^{}_{i})_{i n, L}\) in batches and simply masking the unwanted cross-relational features during the kernel operations. All the operations are parallelizable and implemented on GPU-accelerated tensor manipulation libraries such that each regression with \(_{}\) is orders of magnitude faster than gradient-based regression methods.

**Discretization \(\)** In the case of infinite-dimensional functional input and output spaces \(\) and \(\), we can accommodate, for numerical computation purposes, different types of function representations previously proposed for neural operator regression and allowing for evaluation at an arbitrary point of their domain. For instance, output functions \(\) can be defined as a linear combination of learned or hardcoded finite set of functions, as in Lu et al. (2019) and Bhattacharya et al. (2020). We focus instead on a different approach inspired by Fourier Neural Operators (Li et al., 2020), by applying our model on the \(M\) first modes of a fast Fourier transform of functions \((_{i},_{i})_{i n}\), and transform back its output, allowing us to work with discrete finite function representations.

**Meta-training \(\)** In order to train \(_{}\) to approximate a solution for all problems of the form (1), we jointly learn the kernel operations \((_{}^{})_{ L}\) as well as transformations \((F_{}^{})_{ L}\). Let us assume that \(\) is of the form \((^{},_{})=_{j}(^{}(_{j}),(_{j}))\), that datasets \(_{}\) are sampled according to a probability distribution \(\) over the set of possible example sets with finite cardinality and that a random variable \(\) select the indices of each test set \(_{}^{}=\{(_{i},_{i})(_{j},_{j})_{},j\}\) such that the train set is \(_{}^{}=_{} _{}^{}\). Our meta-learning objective is defined as:

\[()=_{,}_{j }}(_{}(_{ }^{})(_{j}),(_{j}))\] (7)

which can be tackled with gradient-based optimization w.r.t parameters \(\) provided \(\) is differentiable (see S.I for details). In order to estimate gradients of (7), we gather a meta-dataset of \(M\) operators example sets \((_{_{m}})_{m M}\) and form, at each training step, a Monte-Carlo estimator over a batch of \(k\) datasets from this meta-dataset with random train/test splits \((_{k})\). For each dataset in the batch, in order to form outputs \(_{}(_{}^{})(_{j})\) defined by equation (5), we initialize the model sequence in (6) by concatenating \(_{}^{}\) with \(_{}^{}=\{(_{i},0_{ k})_{i}_{}^{}\}\) and obtain each infered output \(_{}(_{}^{})(_{j})\) as \(_{_{i}_{}^{}}_{ }(_{i},_{j})(}_{i})\). Since each regression consists in a single feedforward pass, estimating gradients of the meta-parameters \(\) with respect to \(\) for each batch consists in a single backward pass achieved through automatic differentiation.

## 5 Numerical experiments

In this section, we show empirically that our meta-optimized model is able to approximate any element \(\) of diverse function spaces \(\) such as operators defined on scalar and vector-valued function spaces derived from parametric physical systems or regression problems in Euclidean spaces. In all experiments, we use the Adam optimizer (Kingma and Ba, 2014) to train for a fixed number of steps with an initial learning rate gradually halved along training. All the computation is carried on a single Nvidia Titan Xp GPU with 12GB memory. Further details can be found in S.I.

### Regression of Advection-Diffusion Reaction PDEs

Figure 2: **Left: RMSEs (and 95% C.I) on unseen operators as a function of the dataset size. The grey area corresponds to dataset cardinalities seen during the _Transducer_ meta-training. For comparison, we train baselines from scratch with the corresponding number of examples. Middle: Training losses of _Transducers_ with different depths. Applying several times the kernel improves performance. Untied weights yield the best performance. Right: (_Up_) 3 examples of the evolution of \(s(x,t)\) for different ADR equations and (_bottom_) spatial MSEs of intermediate representations \((^{})\) colored by iteration \(\). The decreasing error, consistent with the MSE reduction of deeper models, suggests that network depth allows for progressively refining function estimates.**First, we examine the problem of regressing operators \(\) associating functions \(\) from \( C(,)\) to their solutions \(=() C(,)\) with respect to advection-diffusion-reaction equations defined on the domain \(=[0,t]\) with Dirichlet boundary conditions \((0,t)=(1,t)=0\). We consider the space \(\) of operators \(_{(,,,t)}\) specifically defined by \((x)=(x,0)\), \((x)=(x,t)\) and \(\) follows an equation depending on unknown random continuous spatially-varying diffusion \((x)\), advection \((x)\), and a scalar reaction term \([0,0.1]\):

\[_{t}(x,t)=(x)_{x}(x,t))}_{}+(x)_{x}(x,t)}_{ }+((x,t))^{2}}_{}\] (8)

Eq. (8) is generic with components arising in many physical systems of interest, leading to various forms of solutions \((x,t)\). (We show examples for three different operators in figure 2.) Several methods exist for modeling such PDEs, but they require knowledge of the underlying parameters \((,,)\) and often impose constraints on the evaluation point as well as expensive time-marching schemes to recover solutions. Here instead, we assume no _a priori_ knowledge of the solution and directly regress each operator \(\) behavior from the example set \(_{}\).

**Baselines and evaluation \(\)** We meta-trained our model to regress \(500\) different operators \(_{(,,,1)}\) with \(t=1\) fixed and varying number of examples \(n\) with images evaluated at 100 equally spaced points \((x_{k})_{k 0,100}\) on the domain \(\) and meta-tested on a set of \(500\) operators with new parameters \(,,\) and initial states \(\). Although not directly equivalent to existing approaches, we compared our method with standard regression methods as well as inductive neural operator approximators. We applied standard finite-dimensional regression methods, \(K\)-Nearest-Neighbors (Fix and Hodges, 1989), Decision Trees (Quinlan, 1986) and Ridge regression with radial basis kernel (Hastie et al., 2009) to each discretized problems \(\{(_{j})(x_{k})=_{j}(x_{k})\}_{j,k}\) as well as two neural-based operators to each dataset instance: DeepONet (Lu et al., 2021) and FNO (Li et al., 2020). Finally, we also tried to meta-learn a parametrization of FNO that could adapt in 100 gradient steps following MAML (Finn et al., 2017) using the same meta-dataset. For all but this approach, an explicit optimization problem is solved before inference in order to fit the target operator. On the other hand, after meta-training of the _Transducer_, which takes only a few minutes to converge, each regression is solved in a single feedforward pass of the network, which is orders of magnitude faster and can be readily applied to new problems (Table 1).

**Results \(\)** We first verified that our model approximates well unseen operators from the test set (Table 1). We noted that our model learns a non-trivial kernel since the estimation produced with \(_{2}\)-Nearest Neighbors remains poor even after \(1e^{3}\) examples. Moreover, since our model can perform inference for varying input dataset sizes, we examined the _Transducer_ accuracy when varying the number of examples and found that it learns a converging regression program (Figure 2) which consistently outperforms other instance-specific regression approaches with the exception of FNO when enough data is available (\(>60\)). We also found that deeper _Transducer_ models with more layers increase kernel approximation accuracy, with untied weights yielding the best performance (figure 2.)

  Method & RMSE & Time (s) & GFLOPs \\  FNO & \(2.96e^{-4}\) & \(1.72e^{2}\) & \(1.68e^{2}\) \\ DEEPONET & \(2.02e^{-2}\) & \(7.85e^{1}\) & \(1.54e^{2}\) \\ FNO-MAML & \(1.4e^{-1}\) & \(2.10e^{0}\) & \(1.6e^{-1}\) \\ TRANSDUCER & \(}\) & \(}\) & \(}\) \\  

Table 1: RMSE and compute costs of regression over 50 unseen datasets with \(n=50\) examples. Note that DeepONet and FNO are optimized from scratch while the _Transducer_ and _FNO-MAML_ have been pre-trained. GFLOPs represent the total number of floating point operations for regression.

Figure 3: Example of _Transducer_ regression extrapolation and RMSEs on OOD tasks with \(n=100\) examples. Color code corresponds to different correlation lengths used to generate the random functions \((x)\) and \((x)\). Much of the result remains below 1% error despite never being trained on such operators.

**Extrapolation to OOD tasks \(\)** We further tested the _Transducer_ ability to regress different operators than those seen during meta-training. Specifically, we varied the correlation length (C.L) of the Gaussian processes used to generate functions \((x)\) and \((x)\) and specified a different target time \(t^{} 1\). We showed that the kernel meta-optimized for a solution at \(t=1\) transfers well to these new regression problems and that regression performance degrades gracefully as the target operators behave further away from the training set (figure 3), while inductive solutions do not generalize.

### Outliers detection on 2D Burgers' equation

We further show that our regression method can fit operators of vector-valued functions by examining the problem of predicting 2D vector fields defined as a solution of a two-dimensional Burgers' equation with periodic spatial boundary condition on the domain \(=^{2}\):

\[_{t}(},t)=_{} (},t)}_{}-(},t) _{}(},t)}_{}\] (9)

Here, we condition our model with operators of the form, \((})=(},t)\), \((})=(},t^{})\) such that our model can regress the evolution of the vector field \(}\) starting at any time, with arbitrary temporal increment \(t^{}-t 10\) seconds and varying diffusion coefficient \([0.1,0.5]\). We show in figure (4) and table (2) that our model is able to fit new instances of this problem with unseen parameters \(\).

**Fast and differentiable regression \(\)** Since the fitting operation is orders of magnitude faster than other operator regression approaches as well as fully differentiable, it allows for quickly executing expensive schemes requiring multiple regressions. This can have several applications, from bootstrapping or producing confidence intervals by varying the example set \(^{}_{}\), or performing inverse problems using Monte-Carlo Markov Chain in the dataset space. We showcase an example of this potential with an outlier detection experiment: We use the _Transducer_ to identify outliers of a dataset of Burgers' equation with coefficient \(_{1}\) artificially contaminated with elements from another dataset \(_{2}>_{1}\) at 5\(\%\) level. We identify outliers by estimating RMSEs over 5000 different regressions using random 50 \(\%\) splits with outliers potentially present in both training and testing sets. This technique takes only a few seconds to estimate while outliers are clearly identified as data points with significantly higher RMSE than the dataset average (figure 5). As a comparison, performing Spectral Clustering (Yu and Shi, 2003) on the FFT of elements \((_{i})\) yields very poor precision (table 2)

   & \(t\) = 5s & \(t\) = 10s \\  RMSE (test sets) & \(}\) & \(}\) \\ Outliers (Pre./Rec.) & \(\) & \(\) \\ S.C. (Pre./Rec.) & \(6\%/85\%\) & \(7\%/85\%\) \\  

Table 2: **& Figure 5: Left**: Meta-test regression and outlier detection results at two target times. RMSEs on Burgers’ equations averaged over 200 different parameter conditions \([0.1,0.5]\) each with 100 train examples. Precision/Recall in outlier detection of the Transducer versus Spectral clustering. **Right**: RMSE distributions of each element in the contaminated dataset over the 5000 regressions. Outliers are clearly identified.

Figure 4: Illustrative example of initial \((t=0)\), target \((t=10)\) and _Transducer_ estimation of the vector field \(s(},t)\) discretized at resolution \(64 64\) over the domain \(^{2}\) for the Burgers’ equation experiment. The last panel represents absolute error to ground truth.

### Climate modeling with seasonal adaptation

One advantage of our approach is the ability to select the data that is most relevant with respect to a certain prediction task and subsequently adapt the model response. For instance, robust and precise prediction of climate variables is difficult because models need to account for seasonal variability and adapt to drifting parameters. Even with a globally large amount of available data, the underlying operator of interest might change over time or be affected by unobserved phenomena. Hence, in order to fully exploit the potential of data-driven methods, being able to capture such variations might greatly help prediction performance on fluctuating and drifting data distributions. In order to illustrate the applicability and scalability of deep transductive learning, we considered the problem of predicting the Earth's surface air pressure solely from the Earth's surface air temperature at a high resolution. Data is taken from the ERA5 reanalysis (Hersbach et al., 2020) publicly made available by the ECMWF, which consists of hourly high-resolution estimates of multiple atmospheric variables from 1979 to the current day. We model pressure estimate on a \(720 720\) grid, resulting in a spatial resolution of \(0.25^{} 0.5^{}\), allowing us to capture small features such as local dynamics and geographic relief.

Similar to (Pathak et al., 2022), we modify a ViT backbone to incorporate a kernel transduction layer before every patch attention and compare our model to an unmodified ViT baseline with a matching number of parameters. We additionally compare with a fully transductive Nearest Neighbors approach. In Figure 6 and Table 3, we present results obtained on training a Transducer with data from 2010 to 2014 and testing it on data from 2016 to 2019. We trained our model by predicting 5 random days sampled from random 20-day windows and present two test configurations: We either condition the _Transducer_ with a window centered at the previous year's same date (P.Y) or with a 15 days window lagging by a week (P.W) (see SI for details). Both cases outperform transductive and inductive baselines with fast inference time, confirming that our solution can scale to large problems and be combined with other deep learning modules.

### Finite-dimensional case: MNIST-like datasets classification

We finally confirm the generality of our approach in the case of finite-dimensional spaces \(\) and \(\) by studying the meta-learning problem presented in Kirsch et al. (2022) which consists in regressing classification functions from the 784-dimensional space of MNIST-like images (LeCun

Figure 6: _Up_ - Illustrative examples of \(720 720\) temperature (_left_) and pressure (_right_) fields of the ERA5 dataset. _Bottom_ - Estimated pressure field from conditioning the _Transducer_ with 15 days data dating 1 week before the target date. Insets show recovered details of the estimation (_blue_) compared with ground truth (_red_).

  Method & LWMSE (hPa) & Time (s) \\  Nearest-Neighbors & \(67.326\) & \(5.915\) \\ ViT & \(32.826\) & **0.053** \\ Transducer - (P.Y) & \(25.293\) & \(0.192\) \\ Transducer - (P.W) & **22.718** & \(0.192\) \\  

Table 3: Latitude-weighted mean-square error (in hectopascals) and inference time for the earth surface pressure prediction task.

and Cortes, 2010) to a 10-dimensional space of one-hot class encoding (i.e functions considered are \(:^{784}^{10}\)). We meta-train a 2-layer Transducer to classify consistently pixel-permuted and class-permuted versions of MNIST. We then meta-test the Transducer to classify the unpermuted MNIST dataset and how the regression map transfer to Fashion-MNIST (Xiao et al., 2017) and KMNIST (Clanuwat et al., 2018). We show in Table 7 that the Transducer outperforms previous meta-learning approaches on both the original MNIST classification task as well as transfer better on Fashion MNIST and K-MNIST classfication tasks.

## 6 Related work

**Transductive Machine learning**\(\) Principles of transductive statistical estimation have been formally described in Gammerman et al. (1998); Vapnik (1999). Algorithms relying on relational structures between data points such as \(K\)-nearest neighbors (Cover and Hart, 1967) and kernel methods (Nadaraya, 1964; Watson, 1964) build estimates by weighing examples with respect to a certain metric space. Further, the "kernel trick" allows to embed possibly infinite-dimensional features (Ferraty and Vieu, 2006) into finite Gram matrix representations that are also well-suited for multi-task regression (Evgeniou et al., 2005; Caponnetto et al., 2008). Distinctively, Gaussian processes regression (Williams and Rasmussen, 1995) combines transduction with Bayesian modeling to estimate a posterior distribution over possible functions. These techniques might suffer from the so-called "curse of dimensionality": with growing dimensionality, the density of exemplar point diminishes, which increases estimators' variance. More recent work combining deep learning with transductive inference has shown promising results even in high-dimensional spaces for few-shot learning (Snell et al., 2017; Sung et al., 2018) or sequence modeling (Jaitly et al., 2015), but the vast majority of neural networks still remain purely inductive.

**Neural operator learning**\(\) The universal approximation abilities of neural networks have been generalized to infinite-dimensional function spaces: Chen and Chen (1995) showed that finite neural parametrization can approximate well infinite-dimensional operators. More recent work using neural networks to perform operator regression has shown strong results (Lu et al., 2019), especially when mixed with tools from functional analysis and physics (Raissi et al., 2017; Li et al., 2020; Gupta et al., 2021; Li et al., 2020; Nelsen and Stuart, 2021; Wang et al., 2021; Roberts et al., 2021) and constitutes a booming research direction in particular for physical applications (Goswami et al., 2022; Pathak et al., 2022; Vinuesa and Brunton, 2022; Wen et al., 2022; Pickering et al., 2022). Recently, the Transformer's attentional computation has been interpreted as a Petrov-Galerkin projection (Cao, 2021) or through Reproducing Kernel Hilbert Space theory (Kissas et al., 2022) for building such neural operators, but these perspectives apply attention to fit a single target operator.

**Meta-learning and in-context learning**\(\) Promising work towards more general and adaptable machines has consisted in automatically "learning to learn" or meta-learning programs (Schmidhuber et al., 1997; Vilalta and Drissi, 2002), by either explicitly treating gradient descent as an optimizable object (Finn et al., 2017), modeling an optimizer as a black-box autoregressive model (Ravi and Larochelle, 2017) or informing sequential strategies via memorization (Santoro et al., 2016; Ortega et al., 2019) More recently, converging findings in various domains from reinforcement learning (Mishra et al., 2018; Laskin et al., 2022), natural language processing (Brown et al., 2020; Xie et al., 2021; Olsson et al., 2022) and functional regression (Garg et al., 2022) have established the ability of set-based attentional computation in the Transformer (Vaswani et al., 2017) for _in-context_ learning by flexibly extracting functional relationships and performing dynamic association such as linguistic analogy or few-shot behavioral imitation. We show that the theory of RKBS can help interpret such property and extends it to function-valued operators regression.

Figure 7: Comparison of meta-test accuracies of MNIST-like datasets classification tasks. Meta-learning models are trained on transformations of MNIST and are meta-tested on original MNIST, FashionMNIST and KMNIST.

Discussion

We proposed a novel transductive model combining kernel methods and neural networks that is capable of performing regression over entire function spaces. We based our model on the theory of vector-valued Reproducing Kernel Banach Spaces and showcased several instances where it learns a regression program able, in a single feedforward pass, to reach performance levels that match or outperform previous instance-specific neural operators or meta-learning systems. Our approach holds potential to create programs flexibly specified by data and able to model entire families of complex physical systems, with particular applications in functional hypothesis testing, dataset curation or fast ensemble learning. However, one limitation is that our model relies on meta-training, which requires collecting a sufficiently diverse meta-dataset to explore the kernel space. In future work, we plan to investigate methods such as synthetic augmentation to reduce the costs of meta-training.

## 8 Acknowledgements

This work was funded by ANR-31A Artificial and Natural Intelligence Toulouse Institute (ANR-19-PI3A0004) ONR (N00014-19- 1-2029), NSF (IIS-1912280 and EAR-1925481), DARPA (D19AC00015) and NIH/NINDS (R21 NS 112743). Additional support provided by the Carney Institute for Brain Science and the Center for Computation and Visualization (CCV) and the NIH Office of the Director grant S10OD025181.