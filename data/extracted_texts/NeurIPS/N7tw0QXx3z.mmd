# SLaM: Student-Label Mixing for Distillation with Unlabeled Examples

Vasilis Kontonis

UT Austin

vasilis@cs.utexas.edu

&Fotis Iliopoulos

Google Research

fotisi@google.com

&Khoa Trinh

Google Research

khoatrinh@google.com

&Cenk Baykal

Google Research

baykalc@google.com

&Gaurav Menghani

Google Research

gmenghani@google.com

&Erik Vee

Google Research

erikvee@google.com

###### Abstract

Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates "soft" pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with margin under random classification noise, and provide the first convergence analysis for so-called "forward loss-adjustment" methods.

## 1 Introduction

While good quality human-labeled data are often hard to obtain, finding huge amounts of unlabeled data is relatively easy. Therefore, in modern machine learning applications, we often face the situation where we have a small "golden" dataset with human labels and a large unlabeled dataset. In _Distillation with Unlabeled Examples_ a large teacher model is first trained (or fine-tuned) on the human-labeled data and is then used to generate "soft" _pseudo-labels_ for the unlabeled dataset. Then the (typically smaller) student model, i.e., the model that will be deployed for the purposes of the application, is trained on the combined dataset that contains both the labels generated by humans and the pseudo-labels generated by the teacher model. This general-purpose training paradigm has been applied in a wide variety of contexts  including but not limited to distilling knowledge from large-scale foundational models like BERT  and GPT-3 . We remark that in such settings one does not have access to the teacher model but only on its pseudo-labels (which were generated during some previous "bulk-inference" phase). This "bulk-inference" step is typically computationally expensive and happens once: one cannot modify the teacher network (or even use it for inference) during the training process of student.

Despite its widespread success in practice, the effectiveness of this powerful approach generally depends on the quality of the pseudo-labels generated by the teacher model. Indeed, training the student model on noisy pseudo-labels often leads to significant degradation of its generalizationperformance, and this is a well-known phenomenon that has been observed and studied in a plethora of papers in the literature, e.g., [6; 36; 44; 51; 53; 8; 27].

In this work, we propose Student-Label Mixing (SLaM), a principled method for knowledge distillation with unlabeled examples that accounts for the teacher's noise and consistently improves over prior approaches. At the heart of our method lies the observation that the noise introduced by the teacher is neither random nor adversarial, in the sense that it correlates well with metrics of "confidence" such as the margin score or the entropy of the teacher's predictions. We exploit this empirical fact to our benefit in order to introduce a model for the teacher's noise, which we use to appropriately modify the student's loss function. At a high level, for any given example during the student's training process, we evaluate the student's loss function on a convex combination of the student's current prediction and another (soft-)label that we estimate using our model for the teacher's noise (hence the name "student-label mixing").

Our contributions can be summarized as follows:

1. We propose SLaM: a principled method for improving knowledge distillation with unlabeled examples. The method is efficient, data-agnostic and simple to implement.
2. We provide extensive experimental evidence and comparisons which show that our method consistently outperforms previous approaches on standard benchmarks. Moreover, we show that SLaM can be combined with standard distillation techniques such as temperature scaling and confidence-based weighting schemes.
3. We give theoretical guarantees for SLaM under standard assumptions. As a byproduct of our analysis we obtain a simple "forward loss-adjustment" iteration that provably learns halfspaces with \(\)-margin under Random Classification Noise with \(O(1/(^{2}^{2}))\) samples improving over prior works that had worse dependence on either the margin \(\) or the generalization error \(\) (see Theorem 5.1 and Remark 5.2).

## 2 Related Work

**Knowledge Distillation.** Most of the literature on knowledge distillation has been focused on the _fully supervised/labeled setting_, i.e., when distillation is performed on the labeled training data of the teacher model rather than on new, unlabeled data -- see e.g. the original paper of . Naturally, in this setting the pseudo-labels generated by the teacher are almost always accurate and so many follow-up works [2; 14; 15; 41; 52] have developed advanced distillation techniques that aim to enforce greater consistency between the teacher's and the student's predictions, or even between the intermediate representations learned by the two models. Applying such methods in our setting where the training dataset contains mainly unlabeled examples is still possible but, in this case, it is known [51; 27] that fully trusting the teacher model can be actually harmful to the student model, making these methods less effective. (In fact, when the teacher is highly noisy these methods even underperform vanilla distillation with unlabeled examples.) In Section 4.2 we present results that show the improved effectiveness of SLaM relative to the state-of-the-art supervised knowledge distillation methods like the Variational Information Distillation for Knowledge Transfer (VID) framework . Moreover, in Appendix D.5 we show that our method can be combined with (i.e., provide an additional improvement) the most simple, yet surprisingly effective, methods of improving knowledge distillation, namely the temperature-scaling idea introduced by .

For distillation with unlabeled examples, many approaches [17; 33; 29] propose filtering-out or reweighting the teacher's pseudo-labels based on measures of teacher's uncertainty, such as dropout variance, entropy, margin-score, or the cut-statistic. These methods are independent of the student model and can be synergistically combined with our technique. For instance, in Section D.4 we demonstrate that combining our method with teacher-uncertainty-based reweighting schemes leads to improved student performance relative to applying the reweighting scheme alone.

Much more closely related to our approach is the recently introduced approach of . There, the authors design a model for the teacher's noise and utilize it in order to modify the student's loss function so that, in expectation, the loss simulates the loss with respect to noise-free pseudo-labels. One of the main advantages of our method compared to that of  is that our model for the teacher's noise is more structured and easier to learn, which -- as our experiments in Section 4.2 show -- leads to consistently better student performance.

**Learning From Noisy Labels.** Learning from noisy labels is an important and well-studied problem with a vast literature [7; 21; 23; 28; 31; 37; 40; 42; 45; 47] -- see  for a recent survey. The fundamental difference between our setting and papers in this literature is that the noise introduced by the teacher is structured, and this is a crucial observation we utilize in our design. Specifically, our approach is inspired by the so-called _forward loss-adjustment methods_, e.g. , but it is specifically tailored to the structure of the distillation with unlabeled examples setting. Indeed, forward methods typically attempt to estimate a noise transition matrix whose \((i,j)\) entry is the probability of the true label \(i\) being flipped into a corrupted label \(j\), which can be rather problematic when dealing with general, instance specific noise like in the case of distillation with unlabeled examples. On the other hand, we exploit that (i) we have access to confidence metrics of the teacher's predictions; and (ii) that often times, when the teacher model's top-\(1\) prediction is inaccurate the true label is within its top-\(k\) predictions for some appropriate \(k\), to design and estimate a much more refined model for the teacher's noise that we use to inform the design of the student's loss function.

Another related technique for dealing with noisy data is using "robust" loss functions [4; 20; 24; 35; 56] such that they achieve a small risk for new clean examples even under the presence of noise in the training dataset. In Section 4.2 we compare our method with the general framework of  for designing robust loss functions and we show that our approach, when applied to the standard cross-entropy loss, consistently outperforms  in the setting of distillation with unlabeled examples. That said, we stress that our method is not tied to the cross-entropy loss and, in fact, it often gives better results when combined with more sophisticated loss functions. We demonstrate this in Appendix D.6 where we apply our method in cases where the student loss function comes from the families of losses introduced in  and .

**Semi-Supervised Learning.** Akin to our setting, in semi-supervised learning (SSL) (see e.g.  for a recent survey) the learner is presented with a small labeled dataset \(A\) and a typically much larger unlabeled dataset \(B\). Unlike to our setting though, there is typically no distinction between the student and teacher: the model of interest generates pseudo-labels on \(B\) which are utilized by using appropriate loss functions or preprocessing procedures (e.g. "filtering" or "correcting") -- often times in an iterative fashion with the goal of improving the quality of the newly-generated pseudo-labels. It is also worth noting that in many real-world applications of distillation with unlabeled examples either the teacher model is unavailable or it is too expensive to retrain it and create fresh pseudo-labels on the data (e.g., when we request labels from a pretrained large language model). Therefore, SSL approaches that either (i) update the "teacher" model (e.g., ), or (ii) require several fresh teacher-generated pseudo-labels (e.g., by requesting teacher-predictions on random data-augmentations or perturbed version of the unlabeled examples of \(B\) e.g., ) are not applicable in our setting. We implement the recent SSL technique of  and show that our method outperforms it in the context of distillation with unlabeled examples. Besides performing on par with state-of-the-art SSL approaches like , the method of  is free of inherent limitations like using domain-specific data augmentations -- which is also an important feature of our approach.

**Learning Halfspaces with Random Classification Noise.** The theoretical study of classification with Random Classification Noise (RCN) was initiated by . For the fundamental class of linear classifiers (halfspaces) the first polynomial time algorithms for the problem where given in  and . The iteration proposed in  is a "backward loss-adjustment" method  for which it is known that resulting optimization landscape is convex (for linear classifiers). In  an improved analysis of the method of  was given, showing that SGD on this convex loss learns \(\)-margin halfspaces with RCN with \((1/(^{4}^{2}))\) samples. On the other hand, forward loss-adjustment methods for dealing with RCN are known to result in an inherently non-convex landscape, see  and Figure 9). Our theoretical result for SLaM (see Theorem 5.1) is the first convergence result for a "forward loss-adjustment" method and, at the same time, achieves a sample complexity of \(O(1/(^{2}^{2}))\) improving over the prior work.

## 3 SLaM: Student-Label Mixing Distillation

In this section, we describe our distillation with unlabeled examples setting and present SLaM. In what follows, we assume that examples are represented by feature-vectors in some space \(\). We shall denote by \(X\) the distribution over examples. We consider multi-class classification with \(L\) classes and assume that the ground-truth label of an example \(x\) is represented by a one-hot vector in \(=\{0,1\}^{L}\) given by some unknown function \(g(x):\). In multi-class classification the learning algorithm typicallyoptimizes a parametric family of classification models \(=\{f(;w):^{L}:w\}\), i.e., for every parameter \(w\), \(f(x;w)\) is an \(L\)-dimensional "score vector", where \(f(x;w)_{i}\) corresponds to the probability that the model assigns to the class \(i\) for the example \(x\). We shall denote by \((,):^{L}^{L}\) the classification loss function used by the learning algorithm. During training the algorithm considers a set of labeled examples \(S=\{(x^{(1)},g(x^{(1)})),,(x^{(n)},g(x^{(n)})\}\) and optimizes the loss \((,)\) over \(S\), i.e., solves the problem \(_{w}_{(x,g(x)) S}(g(x),f(x;w))\,.\) For two vectors \(v,u^{L}\) we denote by \((v,u)=\{(v) {argmax}(u)\}\) the indicator of the event that the positions of the maximum elements of \(v,u\) agree. Similarly, for two classifiers \(h(x),f(x):^{L}^{L}\) we can use \((h(x),f(x))\) to denote whether their top-1 predictions for the example \(x\) agree. Our goal is to train a classifier over the sample \(S\) so that its generalization error, i.e., \(_{x X}[(f(x;w),g(x))]\), is small.

Distillation with Unlabeled Examples.We assume that we are given a (usually small) dataset \(A\) of correctly labeled examples \((x,g(x))\) and a set of unlabeled data \(U\). A "teacher" model \(y_{s}():^{L}\) is first trained on the labeled dataset \(A\) and then provides soft-labels for the examples of dataset \(U\), i.e., we create a dataset \(B=\{(x,y_{s}(x)):x U\}\) containing examples labeled with the corresponding probability distribution over classes (soft-labels) of the teacher model. We then train a (typically smaller) student model using both the original labeled data \(A\) and the teacher-labeled dataset \(B\), i.e., \(_{w}_{(x,j) A B}(z,f(x;w))\). In what follows, we shall call the above training procedure as "vanilla-distillation".

_Remark 3.1_ ("Hard-" vs "Soft-" Distillation).: We remark that the process where instead of using the soft-labels provided by the teacher model on the unlabeled dataset U, we use one-hot vectors representing the class with maximum score according to the teacher, is known as hard-distillation. We will denote by \(y_{s}(x)\) the soft-label of the teacher and by \(y(x)\) the corresponding hard-label, i.e., \(y(x)\) is the one-hot representation of \(y_{s}(x)\). When it is clear from the context we may simply write \(y\) instead of \(y(x)\).

Modelling the Teacher as a "Noisy" Label Oracle.In the distillation setting described in the previous paragraph, it is known  that _the teacher model often generates incorrect predictions on the unlabeled examples, impairing the student's performance_. Given any \(x U\), we model the teacher's prediction \(y\) as a random variable. Similarly to  we assume that, for every unlabeled datapoint \(x U\), the provided teacher label \(y\) is correct with probability \((x)\) and incorrect with probability \(1-(x)\). However, in contrast with , our noise model prescribes a non-adversarial (semi-random) behavior of the teacher when its top-1 prediction is incorrect.

A first step towards more benign noisy teachers is to assume that, conditionally on being wrong, the teacher label is a uniformly random class of the remaining \(L-1\) classes. We remark that this model is already enough to give improvements in datasets with a moderately large number of classes (e.g., up to 100). In particular, it perfectly captures the noisy teacher in binary classification: when the teacher label is different than the ground-truth \(g(x)\) then it has to be equal to the "flipped" ground-truth \(1-g(x)\).

We now further refine our model so that it is realistic for datasets with thousands of classes. Even though the top-1 accuracy of the teacher model may not be very high on the unlabeled data \(U\), the true label is much more likely to belong in the top-5 or top-10 predictions of the teacher rather than being completely arbitrary. For example, training a ResNet50 network on \(10\%\) of ImageNet  yields an average top-1 accuracy about \(52.78\%\) on the test dataset whereas the top-10 accuracy of the same model is about \(83.55\%\). In datasets with a large number of classes, this observation significantly reduces the number of potential correct classes of the examples where the teacher label is incorrect. Motivated by the above, we assume the following structured, semi-random noise model for the teacher, tailored to multi-class settings.

**Definition 3.2** (Noisy Teacher Model).: Let \(x\) be any example of the unlabeled data \(U\) and denote by \(g(x)\) its ground-truth label. Let \(y_{s}(x)\) resp. \(y(x)\) be the random variable corresponding to the soft resp. hard prediction of the teacher model for the example \(x\). We assume that for every \(x\) there exist (unknown to the learner) \((x)\) and \(k(x)\{2,,L\}\) such that the teacher's top-1 prediction \(y\) agrees with the ground-truth \(g(x)\) with probability \((x)\) and, with probability \(1-(x)\): (i) the ground-truth belongs in the top-\(k(x)\) predictions of the teacher; and (ii) the teacher's (hard)-prediction is a uniformly random _incorrect_ class out of the top-\(k(x)\) predictions of the teacher soft-label \(y_{s}(x)\)1.

_Remark 3.3_.: We remark that the model of Definition 3.2 captures having a "perfect" teacher model by setting \((x)=1\) for all \(x\) and also generalizes the binary case described above by taking \(k(x)=2\) for all \(x X\).

Given the above noise model for the teacher, the problem of improving knowledge-distillation consists of two main tasks: (i) obtaining estimates for accuracy statistics \((x),k(x)\) for each example \(x U\); and (ii) using those estimated values to improve the training of the student model so that it is affected less by the mistakes of the teacher on dataset \(B\).

Training Better Students Using \((x),k(x)\)We first assume that for every \(x\) we have oracle access to the values \((x),k(x)\) and present our Student-Label Mixing loss function. Instead of using \((x),k(x)\) to "denoise" the teacher's label, we use them to _add noise to the student's predictions_. To make notation more compact, in what follows, given a vector \(z^{L}\) we denote by \((z;k)\) the vector that has the value 1 in the positions of the of the \(1\)-st up to \(k\)-th largest elements of \(z\) and \(0\) in all other positions, e.g., \(((1,2,3);1)=(0,0,1)\) and \(((-1,1,0,2);3)=(0,1,1,1)\). Assuming that the student-label for some \(x U\) is \(f(x;w)\) we "mix" it (hence the name Student-Label Mixing) using \((x),k(x)\) to obtain the mixed prediction

\[(f(x;\!w);(x),k(x))=(x)f(x;w)\ +\ (1-(x)) (y_{s}(x);k(x))*\,,\] (1)

where \(q*p\) is the element-wise multiplication of the vectors \(p,q\). We then train the **mixed** student model, on the "noisy" dataset \(B\):

\[_{w W}\!_{(x,z) A}(z,f(x;w))+ _{(x,y) B}(y,(f(x;w);(x),k(x))\] (2)

The main intuition behind the mixing of the student's labels is that _by training the "noisy" student to match the "noisy" teacher label \(y\) on dataset \(B\), the underlying (non-mixed) student \(f(x;w)\) will eventually learn the ground-truth_. In particular, when \((,)\) is the Cross-Entropy loss we have that the expected mixed loss conditioned on any \(x\) is

\[[(y;(f(x;w),a(x),k(x))) x]=( (g(x);(x),k(x)),(f(x;w);(x),k(x)))\,,\]

where we used the fact that the cross-entropy is linear in its first argument, and that by the definition of our noise model (Definition 3.2) it holds that \([y x]=(g(x);(x),k(x))\). Therefore, when the student is equal to the ground-truth \(f(x;w)=g(x)\), we obtain that the mixed student-model will satisfy \((g(x);(x),k(x))=(f(x;w);(x),k (x))\) for all \(x X\), and (by Gibb's inequality), we obtain that \(g(x)\) is a minimizer of the SLAM loss. We show the following proposition, see Appendix C for the formal statement and proof.

**Proposition 3.4** (SLaM Consistency (Informal)).: _Let \(D\) be the distribution of the teacher-labeled examples of dataset \(B\), i.e., we first draw \(x X\) and then label it using the noisy teacher of Definition 3.2. Moreover, assume that there exists some parameter \(w^{*}\) such that the ground-truth \(g(x)=f(x;w^{*})\). Then \(w^{*}\) is the minimizer of the (population) SLAM objective: \(_{w}_{(x,y) D}[(y,(f(x;w );(x),k(x)))]\), where \((,)\) is the Cross-Entropy loss._

Estimating the Teacher's Accuracy Statistics \((x),k(x)\) via Isotonic RegressionWe first show how we estimate \((x)\) for each \(x\) of dataset \(B\), i.e., the dataset labeled by the teacher model. In  the authors empirically observed that \((x)\) correlates with metrics of teacher's confidence such as the "margin", i.e., the difference between the probabilities assigned in the top-1 class and the second largest class according to the teacher's soft label \(y_{s}\). In particular, the larger the margin is the more likely is that the corresponding teacher label is correct. We exploit this monotonicity by employing isotonic regression on a small validation dataset to learn the mapping from the teacher's margin at an example \(x\) to the corresponding teacher's accuracy \((x)\). For more details, see Appendix B.1.

To perform this regression task we use a small validation dataset \(V\) with correct labels that the teacher has not seen during training. For every example \(x V\) we compute the corresponding soft-teacher label \(y_{s}(x)\) and compute its margin \((x)=_{1}(y_{s}(x))-_{2}(y_{s}(x))\). For every \(x V\) we also compute the hard-prediction of the teacher and compare it with the ground-truth, i.e., for every \(x V\) the covariate and responce pair is \(((x),1-(g(x),y(x)))\). We then use isotonic regression to fit a piecewise constant, increasing function to the data. We remark that isotonic regression can be implemented very efficiently in \(O(n n)\) time (where \(n\) is the size of the validation dataset).

For \(k(x)\) we consider two different options: (i) using the same value for all examples (e.g., using \(k\) so that the top-k accuracy of teacher is above some threshold on the validation data); and (ii) using a "data-dependent" \(k(x)\) that we estimate by solving \(L\) (recall that \(L\) is the number of classes) isotonic-regression problems (similar to that for estimating \((x)\) above). We refer to Appendix B.1 for more details.

## 4 Experimental Evaluation

In this section, we present our experimental results. In Section 4.1 we describe our experimental setup and in Section 4.2 we compare the performance of our method with previous approaches on standard benchmarks. In Section D.4 we show that our method can be combined with teacher-uncertainty-based reweighting techniques. Finally, due to space limitations, we provide additional empirical results in the Appendix: in Appendix D.5 we show that SLaM can effectively be used with distillation temperature, and in Appendix D.6 we consider using SLaM with other losses beyond the Cross-Entropy.

### The Setup

Here, we describe our procedure for simulating knowledge distillation with unlabeled examples on academic datasets. We start by splitting the training dataset in two parts: dataset A and dataset C. We then train the teacher and student models on dataset A (using the standard cross-entropy loss).2 Then we perform multiple independent trials where, for each trial, we randomly split dataset C into a small (e.g., 500 examples validation dataset V and an unlabeled training dataset U. For each trial we (i) use the teacher model to label the points on dataset U to obtain the teacher-labeled dataset B (ii) initialize the weights of the student to those of the student model that was pre-trained on dataset A; (iii) train the student model (using each distillation method) on the combined labeled data of A, V (that have true labels) and the data of B (that have teacher labels). We remark here that we include the validation data V during the training of the student to be fair towards methods that do not use a validation dataset. However, while it is important that the teacher has not seen the validation data during training, the performance of no method was affected significantly by including (or excluding) the validation data from the training dataset.

### Comparison with Previous Approaches

The BaselinesA natural question is whether a more sophisticated distillation method that enforces greater consistency between the teacher and the student, would improve distillation with unlabeled examples: we use the VID method  that incorporates the penultimate layer of the student model (after a suitable trainable projection) in the loss. We also compare our method against the weighted distillation method of  that reweights the examples of dataset \(B\) in order to "correct" the effect of the noisy pseudo-labels provided by the teacher. The Taylor cross-entropy method of  is

Figure 1: Learning \((x)\) via isotonic regression. The data were generated by a ResNet 110 teacher trained on \(5000\) examples of CIFAR-100 and evaluated on a validation dataset \(V\) of \(500\) examples. The regression data \(\{((y_{s}(x)),1-(y_{s}(x),g(x))):x V\}\) are shown in gray (the response is binary \(0/1\)). By enforcing monotonicity, isotonic regression yields a more stable and robust curve than, for example, the KNN predictor.

a modification of CE that truncates the taylor-series of the CE loss. In  it was shown that it offers significant improvements when the labels are corrupted by random classification noise. The fact that the teacher's noise is much closer to random than to adversarial makes this approach a natural baseline. The UPS loss of  is a semi-supervised technique that takes into account the variance (uncertainty) of the teacher model on the examples of dataset \(B\) in order to transform the soft pseudo-labels provided by the teacher to more "robust" binary vectors and then use a modified binary CE loss. To estimate the uncertainty of the teacher model, we used either dropout with Monte-Carlo estimation or random data-augmentations as suggested in . We remark that, as we discussed in Section 2 and Section 1, strictly speaking, this method is not applicable in our setting because it requires multiple forward passes of the teacher model to estimate its variance but we implement it as it is a relevant approach that aims to improve the pseudo-labels of the teacher.

Cifar-{10,100} and CelebAHere we present our results on CIFAR-{10, 100}  and CelebA . CIFAR-10 and CIFAR-100 are image classification datasets with 10 and 100 classes respectively. They contain 60000 labeled images, which are split to a training set of 50000 images, and a test set of 10000 images. From the 50000 images of the train set we use the \(10\%,15\%,20\%,25\%,30\%,35\%\) (or 5000, 7500, 10000, 12500, 15000, and 17500 examples) as the labeled dataset A where we train the teacher and pre-train the student models. For each size of dataset A, we perform a random split on the remaining training data and use 500 labeled examples as the validation dataset and the remaining examples as the unlabeled dataset U. For the CIFAR-10 experiments, we use a Mobilenet with depth multiplier 2 as the teacher, and a Mobilenet with depth multiplier 1 as the student. For CIFAR-100, we use a ResNet-110 as a teacher, and a ResNet-56 as the student. We compare the methods both on soft- and hard-distillation. For each trial we train the student model for \(200\) epochs and keep the best test accuracy over all epochs. We perform 3 trials and report the average of each method and the variance of the achieved accuracies over the trials. The results of our experiments for soft-distillation can be found in Table 1 and Table 2. The corresponding plots are given inFigure 2. We include our results for hard-distillation in Appendix D.2.

We consider the male/female binary classification task using the CelebA dataset  consisting of a training set of 162770 images and a test set of 19962 images. We use a MobileNet with depth multiplier 2 as the teacher, and a ResNet-11 as the student. As the labeled dataset A we used \(2\%,3\%,4\%,5\%,6\%\) percent (or 3256, 4883, 6510, 8138, 9766, 11394 examples) of the training dataset and split the remaining data in a validation dataset of 500 examples and an unlabeled dataset U. Our results for CelebA can be found in Table 3 (soft-distillation) and in Table 7 (hard-distillation).

  Labeled Examples & \(5000\) & \(7500\) & \(10000\) & \(12500\) & \(15000\) & \(17500\) \\  Tester & \(61.30\) & \(68.98\) & \(72.42\) & \(73.92\) & \(76.63\) & \(78.63\) \\  Vanilla & \(63.33 0.29\) & \(70.39 0.11\) & \(73.23 0.15\) & \(74.29 0.25\) & \(76.64 0.20\) & \(78.63 0.16\) \\  Taylor-CE  & \(64.07 0.26\) & \(71.19 0.17\) & \(74.18 0.25\) & \(74.65 0.24\) & \(77.17 0.04\) & \(78.67 0.13\) \\  UPS  & \(64.56 0.13\) & \(71.10 0.34\) & \(74.17 0.06\) & \(75.05 0.24\) & \(77.64 0.12\) & \(\) \\  VID  & \(63.76 0.13\) & \(70.58 0.17\) & \(73.77 0.40\) & \(74.95 0.21\) & \(77.25 0.06\) & \(78.23 0.09\) \\  Weighted  & \(63.85 0.13\) & \(71.04 0.24\) & \(73.64 0.36\) & \(75.00 0.17\) & \(77.40 0.17\) & \(78.93 0.19\) \\  SLAM (Ours) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 1: Experiments on CIFAR-10 (**soft**-distillation). See Section 4.2 for details.

Figure 2: Comparison of distillation methods on CIFAR-10,100 and CelebA. On the horizontal axis we plot the size of Dataset A as a percentage of the whole training dataset. On the vertical axis we plot the accuracy of the trained student-model on the test dataset.

The corresponding plots are given in Figure 2. Due to space limitations our results for hard-distillation can be found in Appendix D.2.

Taken together, our comparisons show that SLaM consistently outperforms the baselines, often by a large margin. The reader is referred to Appendix D.1 for additional details.

_Remark 4.1_ (Soft-Distillation and Temperature Scaling).: We remark that in the comparisons we performed soft-distillation with temperature set to \(1\), i.e., for every example we do not scale the corresponding teacher and student logits. In Appendix D.5 we show that our method can readily be used together with temperature scaling to improve the accuracy of the student model.

ImageNetHere we present the results on ImageNet . ImageNet is an image classification dataset with 1000 classes consisting of a training set of approximately \(1.3\) million images, and a test set of 50000 images. From the \(1.3\) million images of the training set we use the \(5\%,10\%,15\%,20\%\) percent (or 64058, 128116, 192174, 256232 examples) as the labeled dataset \(A\) where we train the teacher and pre-train the student models. For each size of dataset \(A\), we perform a random split on the remaining training data and use \(10000\) labeled examples as the validation dataset and the remaining examples as the unlabeled dataset \(U\). We use a ResNet-50 as the teacher, and a ResNet-18 as the student. We compare the methods on soft-distillation. For each trial, we train the student model for \(100\) epochs and keep the best test accuracy over all epochs. We perform \(4\) trials and report the average of each method and the variance of the achieved accuracies over the trials. Our results for ImageNet can be found in Table 4. We remark that we do not include the results of the UPS method in Table 4 because it did not improve over the accuracy achieved after pre-training the student model on dataset \(A\). The reader is referred to Appendix D.1 for additional details.

  Labeled Examples & \(5000\) & \(7500\) & \(10000\) & \(12500\) & \(15000\) & \(17500\) \\  Tuscher & \(35.97\) & \(44.65\) & \(49.62\) & \(55.68\) & \(59.19\) & \(62.05\) \\  Vanilla & \(37.94 0.10\) & \(46.42 0.24\) & \(52.17 0.21\) & \(57.72 0.17\) & \(60.91 0.07\) & \(63.47 0.23\) \\ Taylor-CE  & \(40.18 0.07\) & \(48.05 0.29\) & \(54.08 0.24\) & \(58.45 0.17\) & \(61.13 0.10\) & \(63.54 0.26\) \\  UPS  & \(39.62 0.23\) & \(48.48 0.15\) & \(54.43 0.27\) & \(58.17 0.07\) & \(60.74 0.10\) & \(62.13 0.12\) \\  YID  & \(38.93 0.39\) & \(46.76 0.10\) & \(52.56 0.17\) & \(57.94 0.37\) & \(61.14 0.28\) & \(63.56 0.18\) \\  Weighted  & \(38.63 0.32\) & \(47.11 0.29\) & \(53.16 0.25\) & \(58.20 0.11\) & \(61.29 0.15\) & \(63.58 0.07\) \\  SLAM (Ours) & \(\) & \(\) & \(\) & \(\) & \(61.30 0.09\) & \(63.98 0.19\) \\  

Table 2: Experiments on CIFAR-100 (**soft-distillation**). See Section 4.2 for details.

Figure 3: Comparison of distillation methods on ImageNet. On the horizontal axis we plot the size of Dataset A as a percentage of the whole training dataset. On the vertical axis we plot the accuracy of the trained student-model on the test dataset.

  Labeled Examples & \(2\%\) & \(3\%\) & \(4\%\) & \(5\%\) & \(6\%\) & \(7\%\) \\  Tuscher & \(86.19\) & \(88.25\) & \(88.95\) & \(91.31\) & \(92.09\) & \(92.62\) \\  Vanilla & \(89.96 0.08\) & \(91.55 0.14\) & \(92.16 0.10\) & \(93.42 0.06\) & \(93.98 0.04\) & \(94.29 0.03\) \\  Taylor-CE  & \(\) & \(\) & \(92.56 0.14\) & \(93.80 0.20\) & \(94.17 0.07\) & \(94.47 0.01\) \\  UPS  & \(89.96 0.11\) & \(92.03 0.09\) & \(92.44 0.04\) & \(93.95 0.05\) & \(94.28 0.07\) & \(94.68 0.03\) \\  YID  & \(89.91 0.10\) & \(91.75 0.21\) & \(92.21 0.10\) & \(93.67 0.21\) & \(94.15 0.07\) & \(94.33 0.16\) \\  Weighted  & \(89.92 0.12\) & \(91.73 0.09\) & \(92.31 0.22\) & \(93.64 0.10\) & \(93.93 0.14\) & \(94.23 0.11\) \\  SLAM (Ours) & \(90.37 0.17\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 3: Experiments on CelebA (**soft-distillation**). See Section 4.2 for details.

Large Movies Reviews DatasetHere we present results on the Large Movies Reviews Dataset . This is a dataset for binary sentiment classification containing 25000 movie reviews for training and 25000 for testing. We use an ALBERT-large model  as a teacher, and an ALBERT-base model as a student. We use \(2\%,4\%,8\%,40\%\) percent (or 500, 1000, 2000, 10000 examples) from the training dataset and split the remaining data in a validation dataset of 500 examples and an unlabeled dataset \(U\). Our results and more experimental details can be found in Appendix D.3.

Performance Gains of SLAM as a Function of The Number of Labeled ExamplesIn our experiments, the fraction of examples we consider "labeled" controls two things at the same time: (i) the accuracy of the teacher model -- as the teacher is trained on the labeled examples available; and (ii) the number of unlabeled examples the teacher model provides pseudo-labels for. The more inaccurate the teacher model is, the better the improvements provided by our method. (Given a "perfect" teacher that never generates incorrect pseudo-labels for the unlabeled examples, our method is mathematically equivalent to the "vanilla" approach (see the mixing operation in Equation (1)). Therefore, the smaller the number of labeled examples available, the bigger the performance gains of SLaM as (i) the teacher will be less accurate; and (ii) it has to generate labels for more unlabeled examples (and therefore the absolute number of inaccurate predictions that SLaM "corrects" increases statistically). It is worth emphasizing that the main reason behind the enormous success of distillation is exactly that the teacher network can blow up the size of the student's training dataset: in practice, the ratio of labeled examples to unlabeled examples is typically (much) less than 1%.

## 5 Distilling Linear Models and Learning Noisy Halfspaces

In this section we show that, when the dataset is separable by a halfspace, i.e., for every example \(x\), the ground-truth is \(g(x)=(\{w^{*} x>0\},\{w^{*} x 0\})\) for some unknown weight vector \(w^{*}\), then using SLaM with a linear model as the student will recover the ground truth classifier. We make the standard assumption that the ground-truth halfspace has \(\)-margin, i.e., that \(\|w^{*}\|_{2}=1\) and that it holds \(|w^{*} x|\) for all examples \(x\). For a fixed example \(x\), the observed noisy teacher-label \(y\) satisfies Definition 3.2, i.e., \(y=g(x)\) w.p. \((x)\) and \(y=1-g(x)\) w.p. \(1-(x)\) (since \(k=2\) for binary classification). Our approach consists of using the standard cross-entropy loss \((p,q)\) and training a student-model consisting of a linear layer plus a soft-max activation, i.e., \(f(x;w)=(},}{1+e^{-w x}} )\).

**Theorem 5.1** (SLaM Convergence).: _Let \(X\) be a distribution on \(^{d}\) and \(g(x)\) be the ground-truth halfspace with normal vector \(w^{*}^{d}\). Let \(D\) be the distribution over (noisy) teacher-labeled examples \((x,y)\) whose \(x\)-marginal is \(X\). Assume that there exist \(,>0\) such that for all examples \(x\) in the support of \(X\) it holds that \(|w^{*} x|\) and \(|1/2-(x)|\). Let \(>0\). After \(T=O(1/(^{2}^{2}^{2}))\) SGD iterations on the SLaM objective (see Algorithm 3), with probability at least \(99\%\), there exists an iteration \(t T\) where \(_{x X}[(f(x;w^{(t)}),g(x))]\)._

_Remark 5.2_ (Learning Halfspaces with RCN).: The problem of learning halfspaces with Random Classification Noise (RCN) can be modeled as having a teacher with constant accuracy probability, i.e., \((x)=>1/2\) for all \(x\). As a corollary of Theorem 5.1 we obtain an efficient learning algorithm for \(\)-margin halfspaces under RCN achieving a sample complexity of \(O(1/(^{2}^{2}))\). Prior to our work, the best known sample complexity for provably learning halfspaces with RCN was \((1/(^{4}^{2}))\) where the "backward loss-adjustment" of  was used.

  Labeled Examples & \(5\%\) & \(10\%\) & \(15\%\) & \(20\%\) & \(25\%\) & \(30\%\) \\   Resher & \(39.48\) & \(52.96\) & \(59.64\) & \(63.62\) & \(66.00\) & \(67.85\) \\  Vanilla & \(41.67 0.05\) & \(55.9 0.06\) & \(62.3 0.09\) & \(65.91 0.05\) & \(67.98 0.07\) & \(69.12 0.08\) \\  Taylor-CE  & \(41.61 0.06\) & \(56.43 0.06\) & \(62.38 0.11\) & \(65.86 0.08\) & \(67.70 0.22\) & \(68.62 0.07\) \\  VID  & \(40.12 0.04\) & \(52.75 0.04\) & \(58.01 0.03\) & \(61.21 0.06\) & \(62.37 0.06\) & \(63.05 0.07\) \\  Weighted  & \(41.67 0.04\) & \(55.96 0.07\) & \(62.29 0.08\) & \(65.91 0.05\) & \(67.96 0.06\) & \(\) \\  SLAM (Ours) & \(\) & \(\) & \(\) & \(\) & \(\) & \(69.07 0.05\) \\  

Table 4: Experiments on ImageNet (**soft-distillation**). See Section 4.2 for details.

Conclusion, Limitations, and Broader Impact

In this work we propose SLaM, a novel and principled method for improving distillation with unlabeled examples. We empirically show that SLaM consistently outperforms the baselines, often by a large margin. We also showed that SLaM can be used with and improve (i) knowledge distillation with temperature scaling; (ii) loss functions beyond the standard Cross-Entropy loss; and (iii) confidence-based weighting schemes that down-weight examples where the teacher model is not very confident. Apart from extensive experimental evaluation, we provide strong theoretical guarantees establishing the consistency and optimality of SLaM. As a byproduct of our theoretical analysis, we obtain a new iteration for learning \(\)-margin halfspaces with RCN that improves the best known sample complexity for this problem.

A limitation of SLaM is that it does not necessarily improve over vanilla distillation when the teacher model makes only a few mistakes (this is to be expected as our method is designed for the case where the teacher-model is imperfect).

Knowledge-distillation is a very popular deep learning method, and therefore, potentially malicious usage of our work is an important societal issue, as deep learning has far-reaching applications from NLP to Robotics and Self-Driving cars.