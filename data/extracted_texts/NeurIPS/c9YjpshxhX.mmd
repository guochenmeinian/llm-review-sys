# Pre-Training Multimodal Hallucination Detectors

with Corrupted Grounding Data

Spencer Whitehead\({}^{1}\)1 &Jacob Phillips\({}^{2}\) &Sean Hendryx\({}^{2}\)

\({}^{1}\)Microsoft \({}^{2}\)Scale AI

spwhitehead@microsoft.com &{jacob.phillips, sean.hendryx}@scale.com

###### Abstract

Multimodal language models can exhibit hallucinations in their outputs, which limits their reliability. The ability to automatically detect these errors is important for mitigating them, but has been less explored and existing efforts do not localize hallucinations, instead framing this as a classification task. In this work, we first pose multimodal hallucination detection as a sequence labeling task where models must localize hallucinated text spans and present a strong baseline model. Given the high cost of human annotations for this task, we propose an approach to improve the sample efficiency of these models by creating corrupted grounding data, which we use for pre-training. Leveraging phrase grounding data, we generate hallucinations to replace grounded spans and create hallucinated text. Experiments show that pre-training on this data improves sample efficiency when fine-tuning, and that the learning signal from the grounding data plays an important role in these improvements.

## 1 Introduction

The capabilities of Multimodal Language Models (MLMs) continue to increase , making it enticing to use them in a wide range of scenarios. However, questions around their reliability may limit this adoption . For instance, when serving as a multimodal assistant for users with visual impairments, incorrect answers to questions  or hallucinations in output descriptions  can have negative consequences as users may base decisions on these outputs.

A critical step towards mitigating hallucinations is accurately detecting them, and a well-trained hallucination detector can be employed in many different ways (_e.g.,_ as a reward model for fine-tuning the MLM  or as an output filter/re-ranker at inference time ). In this work, we pose multimodal hallucination detection as a sequence labeling task where, given an image, prompt, and response, models must _localize_ hallucinated spans in the response. In contrast to prior work (_e.g.,_), we do not assume access to pre-defined spans to classify, which we argue is a more realistic setting as pre-defined spans are likely unavailable in real scenarios. We present a strong baseline detector for this task.

Further, training hallucination detectors requires fine-grained annotations, like error spans  or corrections , that can be non-trivial to collect and scale due to the need for human annotators and/or powerful teacher models. Hence, most effectively using this data is important. We benchmark the sample efficiency when fine-tuning on human annotations, showing much room for improvement.

Therefore, we propose a simple approach to increase the sample efficiency by pre-training on corrupted grounding data, which we automatically create. Using phrase grounding data , we replace some grounded spans with hallucinated phrases from a text-only Language Model (LM). TheLM does not take the image as input so it proposes phrases that are plausible for the text context but likely incorrect given the visual context. We find that pre-training on this corrupted grounding data improves sample efficiency when fine-tuning (_e.g.,_ up to +7 F1 with 500 fine-tuning samples). We also show that using grounding annotations for our data is important, suggesting that grounding can offer a useful learning signal for training hallucination detectors.

In summary, our contributions are: 1) We formalize multimodal hallucination detection as a sequence labeling task and present a baseline. 2) We propose an approach to improve the sample efficiency of the detectors by creating corrupted grounding data and pre-training on this data. 3) Our experiments show that this improves sample efficiency when fine-tuning across different model and data scales. 4) We find that utilizing grounding data is important in our approach, suggesting that grounding offers a valuable learning signal for pre-training these detectors.

## 2 Hallucination Detection

**Task.** Given an image and associated prompt-response pair, the goal is to predict which text spans in the response are hallucinated and which are not. Prior work frames this as a _classification_ task where pre-defined spans are given as input . However, in an end-to-end setting, spans are either not provided or must be artificially imposed (_e.g.,_ sentence boundaries). We explore hallucination _detection_, which we pose as a sequence labeling task where models predict a label for each token that indicates whether the token is part of a hallucinated segment. We adopt the binary setup from prior work , with non-hallucinated/hallucinated labels. We evaluate using span F1 scores for a given intersection-over-union (IoU) threshold, so models must identify span boundaries and classify the spans, much like other localization tasks [16; 22]. We compute macro F1 scores across the two classes to handle imbalances.

**Model.** We use a MLM as our base model and replace the next-token-prediction head with an output head that predicts a label based on the representation of each token from the base model. Since we predict per-token labels, we let transitions between labels in the token sequence demarcate the spans. This setup is compatible with a wide variety of base models. We use this modeling setup for both pre-training on our corrupted grounding data (Sec. 3) and fine-tuning (Sec. 4).

More details on the task and models are in Appendix F and Appendix H, respectively.

## 3 Corrupted Grounding Data

We want a scalable way to bolster the sample efficiency of hallucination detectors. Pre-training and transfer learning has been effective for improving downstream performance and sample efficiency in other areas (_e.g.,_). However, pre-training requires more data and, as discussed, human annotations can be expensive to collect.

A promising alternative is to create synthetic or pseudo-labeled data that can be used for pre-training, which has been powerful for LMs [2; 9; 28]. In our setting, grounding data can be automatically created at large scales, albeit with some noise [12; 19; 44; 48]. Moreover, hallucinations and grounded phrases are linked since correctly grounded phrases are, by definition, not hallucinated. By replacing

Figure 1: Our approach for creating corrupted grounding data to pre-train multimodal hallucination detectors. Examples of this data are in Appendix I.

grounded phrases with other phrases that are not aligned with the image, we can create text that contains hallucinations.

Shown in Fig. 1, we take multimodal data with grounding annotations and corrupt it to create hallucinated text. First, we mask out grounded spans and use a text-only LM to propose phrases to fill in the masked spans. This LM does not take the image as input, so it proposes phrases that are plausible for the _text context_ but are likely incorrect for the _visual context_. We take measures to increase the likelihood that the proposals are hallucinations, such as restricting the LM from generating the original phrases and sampling during decoding to encourage more diversity . Next, we randomly select a subset of the masked spans to fill in with the proposed phrases, keeping the original phrases for the remaining. We label any in-filled spans as hallucinated, while the remaining spans are labeled as non-hallucinated. Since most grounded spans tend to be noun phrases , the hallucinated labels may be sparse. Therefore, if a sentence contains any hallucinated spans, then we randomly decide whether to label the entire sentence as hallucinated. This noisy, corrupted data simulates hallucinations in the text that we can use to pre-train hallucination detectors. Approach details are in Appendix E and pre-training data analysis is in Appendix I.

## 4 Experiments

We experiment on M-HalDetect , a multimodal hallucination detection benchmark that has image-prompt-response triples with hallucinated span annotations (details in Appendix G). M-HalDetect has a training set of 11k samples and a test set of 3k. We fine-tune models on 500, 1k, and 10k subsets of the M-HalDetect training data to examine sample efficiency at distinct scales. We use the remaining 1k training samples as a validation set. We report F1 scores on the test set with an IoU threshold of 0.5 (Sec. 2).

For base models, we use LLaVA-1.5 and LLaVA-1.6 [24; 25], two strong and widely adopted MLMs. While structurally similar, they are distinct in important ways, such as their encoding of images, vision-language connector, and training data. For each model, we experiment with the 7B and 13B sizes to explore scaling. We do a light hyperparameter search and report the best result for each model at each data scale.

To create corrupted grounding data, we start from the Grounded Visual Chat dataset , which is automatically generated. We use 121k samples from this dataset. T5  serves as our LM to propose hallucinated phrases since it is inexpensive to use and supports in-filling without prompt engineering.

Detailed settings are in Appendix E-H.

### Benchmarking Detector Sample Efficiency

We explore sample efficiency on the detection task at different scales of fine-tuning data. We compare only fine-tuning (FT) to pre-training with our corrupted grounding data then fine-tuning (PT+FT). Qualitative examples are in Appendix I.

**FT baseline.** Looking at the FT results at 10k samples (_i.e.,_ the full fine-tuning set), we see that all models achieve non-trivial F1 scores. The best performing detection model uses LLaVA-1.6 13B as

Figure 2: Sample efficiency of different models at 500, 1k, and 10k fine-tuning samples. Dotted lines are models that only fine-tune (FT), while solid lines are models that first pre-train on our data then fine-tune (PT+FT). Pre-training with our corrupted grounding data consistently improves the sample efficiency. Scores are listed in Appendix D.

the base model, with 31.52% F1. These models serve as our strong baseline to which we compare our pre-training approach.

**Pre-training improves sample efficiency.** In Fig. 2, we see consistent improvements in sample efficiency across each of the models. For instance, with 500 samples, LLaVA-1.6 13B reaches 25.30% F1 with pre-training and 17.98% without. With this same model, the difference in performance between 500 and 10k samples decreases from 13.54% to 6.22% when pre-training. This suggests that by pre-training on our data, the model is able to make more effective use of the expensive human annotations. Similar observations hold for the other models as well. Finally, we see that our pre-training is most effective at lower scales (500, 1k), whereas the difference is less pronounced when fine-tuning on the full 10k samples. Though scaling up the pre-training data may improve this.

**Larger models tend to benefit more from pre-training at lower data scales.** Comparing Figs. 1(a) and 1(b) with Figs. 1(c) and 1(d), at 500 samples, the difference between PT+FT and FT is larger for the 13B models. The 7B models also benefit from the pre-training (Figs. 1(c) and 1(d)), though the gap is less than the larger ones. This aligns with similar observations on pre-training reward models for LM alignment .

**Hallucination detection is a challenging task.** Based on Fig. 2, we see that when fine-tuning on the 10k training split, models have up to \(\)33% F1 score. Although we do not know the upper bound for this detection task on M-HalDetect (_i.e.,_ human performance), the combination of these scores and the qualitative examples we show in Appendix I.2 suggest that our models represent a strong baseline, but there is much room to improve the performance.

**Detection vs Classification.** Classification can be viewed as a subtask of detection. To demonstrate this, we adapt our fine-tuned detection models to perform classification on pre-defined spans by taking a majority vote over the predicted token labels in each given span. We present the results in Appendix B, where we find that our detection models can achieve 81.63% F1 on classification.

### Ablations

**Grounded spans are important.** In Fig. 3, we evaluate masking out random spans instead of grounded ones to examine the need for grounding data. We see noticeably lower performance across each data scale. Interestingly, pre-training on this data even significantly lowers the performance when fine-tuning on 10k samples. This suggests that incorporating a notion of "_groundability_" into the pre-training data is important for improving sample efficiency when fine-tuning.

**Plausible hallucinations are necessary at smaller data scales.** We ablate our use of a LM to generate plausible hallucinated phrases by in-filling the grounded spans with random phrases. The curve in Fig. 3 illustrates that this also has a significant negative effect at smaller data scales, but is not as harmful as using random, ungrounded spans.

**Pre-training outperforms augmentation**. We also explore augmenting with our data rather than pre-training, with results in Appendix A. We find that pre-training outperforms augmentation likely, in part, due to differences in distribution and/or noise in our data.

We also explore freezing the base model during pre-training in Appendix A.

## 5 Conclusions

Localizing hallucinations is important for mitigating them. We pose multimodal hallucination detection as a sequence labeling task and present a strong baseline detector. Given the cost of annotating hallucination detection data, we propose to improve the sample efficiency of detectors by creating corrupted grounding data and using this data for pre-training. We find that pre-training on this data improves sample efficiency across model and data scales, and that using grounded spans is important for these improvements.

Figure 3: Ablations with LLaVA-1.6 13B for using grounding annotations and LMs for our data. Random Spans indicates that random text spans are masked and in-filled instead of grounded spans. Random In-Fill uses grounded spans but fills them in with random phrases.