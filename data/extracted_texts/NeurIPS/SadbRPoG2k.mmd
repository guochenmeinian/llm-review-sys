# Bayesian Strategic Classification

Lee Cohen

Stanford University. Email: leeco@stanford.edu

Saeed Sharifi-Malvajerdi

Kevin Stangl

Ali Vakilian

Juba Ziani

###### Abstract

In strategic classification, agents modify their features, at a cost, to obtain a positive classification outcome from the learner's classifier, typically assuming agents have full knowledge of the deployed classifier. In contrast, we consider a Bayesian setting where agents have a common distributional prior on the classifier being used and agents manipulate their features to maximize their expected utility according to this prior. The learner can reveal truthful, yet not necessarily complete, information about the classifier to the agents, aiming to release just enough information to shape the agents' behavior and thus maximize accuracy. We show that partial information release can counter-intuitively benefit the learner's accuracy, allowing qualified agents to pass the classifier while preventing unqualified agents from doing so. Despite the intractability of computing the best response of an agent in the general case, we provide oracle-efficient algorithms for scenarios where the learner's hypothesis class consists of low-dimensional linear classifiers or when the agents' cost function satisfies a sub-modularity condition. Additionally, we address the learner's optimization problem, offering both positive and negative results on determining the optimal information release to maximize expected accuracy, particularly in settings where an agent's qualification can be represented by a real-valued number.

## 1 Introduction

Machine Learning critically relies on the assumption that the training data is representative of the unseen instances a learner faces at test time. Yet, in many real-life situations, this assumption fails when individuals (agents) manipulate decision-making algorithms for personal advantage, by modifying their features at a cost. A typical example of such manipulations or _strategic behavior_ is seen in loan applications or credit scoring: for example, an individual may open new credit card accounts to lower their credit utilization and increase their credit score artificially. In the context of job interviews, a candidate can spend time and effort to memorize solutions to common interview questions and potentially look more qualified than they are at the time of an interview. A student might cram to pass an exam this way without actually understanding or improving their knowledge of the subject.

The prevalence of such behaviors has led to the rise of an area of research known as _strategic classification_. Strategic classification, introduced by Hardt et al. (2016), aims to understand how a learner can optimally modify decision making algorithms to be robust to such strategic manipulations of agents, if and when possible.

Most of the strategic classification literature makes the assumption that the model deployed by the learner is _fully observable_ by the agents, granting them the ability to optimally best respond to the learner using resources such as effort, time, and money. Yet, this _full information_ assumption can be unrealistic in practice. There are several reasons for this: some machine learning models areproprietary and hide the details of the model to avoid leaking "trade secrets": e.g., this is the case for the credit scoring algorithms used by FICO, Experian, and Equifax.1 Some classifiers are simply too complex in the first place to be understood and interpreted completely by a human being with limited computational power, such as deep learning models. Other classifiers and models may be obfuscated for data privacy reasons, which are becoming an increasingly major concern with new European consumer protection laws such as GDPR (Regulation, 2018) and with the October 2023 Executive Order on responsible AI (Biden, 2023). In turn, there is a need to study strategic classification when agents only have _partial_ knowledge of the learner's model.

There has been a relatively short line of work trying to understand the impact of incomplete information on strategic classification. Jagadeesan et al. (2021) and Bechavod et al. (2021) study the optimal classifiers in settings where agents can only gain partial or noisy information about the deployed model. Hagthalab et al. (2023) study calibrated Stackelberg games, a more general form of strategic classification; in their framework, the learner engages in repeated interactions with agents who base their actions on calibrated forecasts about the learner's classifier. They characterize the optimal utility of the learner for such games under some regularity assumptions. While we also model agents with prior knowledge of the learner's actions, Hagthalab et al. (2023) focus on an online learning setting and the selection of a strategy for the learner without incorporating any form of voluntary _information release_ by the learner.

In contrast, we focus on this additional critical aspect of voluntary _information release_ by the learner that these works do not study. Namely, we ask:

**How to release partial and truthful information about the classifier to maximize accuracy?**

This should give the reader pause: why should a learner release information about their deployed classifier since presumably such information only makes it easier for agents to manipulate their features and "trick" the learner. In fact, Ghalme et al. (2021) showed that information revelation can help--a learner may prefer to fully reveal their classifier as opposed to hiding it. While they consider either fully revealing the classifier or completely hiding it, our model considers a wider spectrum of information revelation that includes both "full-information-release" and "no-information-release". We show that there exist instances where it is optimal to reveal only _partial_ information about the classifier, in a model where a learner is _allowed to reveal a subset of the classifiers containing the true deployed classifier_. For example, a tech firm might reveal to candidates that they will ask them about a new type of data structure during their job interviews. Lenders might reveal to clients that they do not consider factors like credit score (Lake, 2024). This selective disclosure can discourage unfit individuals, ultimately saving time and energy for both sides. In the following, we summarize our contributions.

**Summary of contributions:**

* In Section 2, we propose a new model of interactions between strategic agents and a learner, under partial information. The two novel modeling elements compared to the standard strategic classification literature are: i) agents have partial knowledge about the learner's classifier in the form of a _distributional prior_ over the hypothesis class, and ii) the learner can _release partial information_ about their deployed classifier. Specifically, our model allows the learner to release a _subset_ of the hypothesis class to narrow down the agents' priors. Given our model, we consider a (Stackelberg) game between agents with partial knowledge and a learner that can release partial information about its deployed model. On the one hand, the agents aim to manipulate their features, at a cost, to increase their likelihood of receiving a positive classification outcome. On the other hand, the learner can release partial information to maximize the expected accuracy of its model, after agent manipulations.
* In Section 3, we study the agent's best response in our game. We show that while in general, it is intractable to compute the best response of the agents in our model, there exist oracle-efficient algorithms2 that can _exactly_ solve the best response when the hypothesis class is the class of low-dimensional _linear_ classifiers. We then move away from the linearity assumption and consider a natural condition on the agents' cost function for which we give an oracle-efficient _approximation_ algorithm for the best response of the agents for _any_ hypothesis class.
* In Section 4, we study the learner's optimal information release problem. We consider screening/classification settings where agents are represented to the learner by a real-valued number that measures their qualification level for a certain task. Prior work has focused on similar one-dimensional settings in the context of strategic classification; see, e.g., (Beyhaghi et al., 2023; Braverman and Garg, 2020). We first show that the learner's optimal information release problem is NP-hard when the agents' prior can be arbitrary. In light of this hardness result, we focus on _uniform_ prior distributions and provide closed-form solutions for the case of _continuous_ uniform priors, and an efficient algorithm to compute the optimal information release for _discrete_ uniform priors.
* We finally consider alternative utility functions that are based on false positive (or negative) rates for the learner and provide insights as to what optimal information release should look under these utility functions, without restricting ourselves to uniform priors.

Related Work.Strategic classification was first formalized by Bruckner and Scheffer (2011); Hardt et al. (2016); Hardt et al. (2016) is perhaps the most seminal work in the area of strategic classification: they provide the first computationally efficient algorithms (under assumptions on the agents' cost function) to efficiently learn a near-optimal classifier in strategic settings. Importantly, this work makes the assumption that the agents fully know the exact parameters of the classifier due to existing "information leakage", even when the firm is obscuring their model. Hardt et al. (2016) also do not consider a learner that can release partial information about their model.

Closest to our work, Jagadeesan et al. (2021); Ghalme et al. (2021); Bechavod et al. (2022); Bechavod et al. (2022); Haghtalab et al. (2023) relax the full information assumption and characterize the impact of opacity on the utility of the learner and agents. Jagadeesan et al. (2021) are the first to introduce a model of "biased" information about the learner's classifier: instead of observing the learner's deployed classifier exactly, agents observe and best respond to a noisy version of this classifier; one that is randomly shifted (by an additive amount) from the true deployed classifier.

In contrast, Ghalme et al. (2021); Bechavod et al. (2022) consider models of _partial_ information on the classifiers, where agents can access samples in the form of historical (feature vector, learner's prediction) pairs. More precisely, Ghalme et al. (2021) study what they coin the "price of opacity" in strategic classification, defined as the difference in prediction error when not releasing vs fully releasing the classifier. They are the first to show that this price can be positive (in the context of strategic classification), meaning that a learner can reduce their prediction error by fully releasing their classifier in strategic settings. Our work considers more general, intermediate forms of information release, instead of the all-or-nothing, binary approach of Ghalme et al. (2021).

Bechavod et al. (2022) consider a strategic regression setting in which the learner does not release their regression rule, but agents have access to (feature, score) samples as described above. They study how disparity in sample access (e.g., agents may only access samples from people similar to them) about the classifier across different groups induce unfairness in classification outcomes across these groups. Haghtalab et al. (2023) consider agents with (calibrated) forecasts over the actions of the learner, but do not consider the learner's information release which is our focus. Additionally, in our model, we do not constrain the agent's prior distribution to be calibrated.

Beyond strategic classification, there are a few related lines of work where such partial information is considered. One is Bayesian Persuasion (Kamenica and Gentzkow, 2011): in Bayesian persuasion, the state of the world is randomly drawn from the prior, and there is a mapping from the state of the world to signal distributions. This mapping, i.e. the "signaling scheme", must be revealed to the agents in addition to the signal. In our setting, there is a _fixed_ state of the world (the learner's classifier), and there is no need for the signaling scheme to be known, since the signal itself (the subset) reveals all the information needed for the agents. The agents only need to know that the learner is truthful, which is an assumption made in Bayesian persuasion too.

Relatedly, algorithmic recourse studies an "intermediate" information release problem where the learner publishes a recommended action or recourse for each agent to take, rather than a set of potential classifiers used by the learner; e.g., Harris et al. (2022). In our model, we release the same signal or information to all agents based on the underlying distribution over these agents' features.

Model

Our model consists of a population of _agents_ and a _learner_. Each agent in our model is represented by a pair \((x,y)\) where \(x\) is a feature vector, and \(y\{0,1\}\) is a binary label. Throughout, we call an agent with \(y=0\) a "negative", and an agent with \(y=1\) a "positive". We assume there exists a mapping \(f:\{0,1\}\) that governs the relationship between \(x\) and \(y\); i.e., \(y=f(x)\) for every agent \((x,y)\). We will therefore use \(x\) to denote agents from now on. We denote by \(D\) the distribution over the space of agents \(\). Agent manipulations are characterized by a cost function \(c:[0,)\) where \(c(x,x^{})\) denotes the cost that an agent incurs when changing their features from \(x\) to \(x^{}\). We assume, similar to standard strategic classification settings, that manipulation does _not_ change one's true label: manipulation is seen purely as "gaming"; it does not change the qualification of an agent. Let \(\{0,1\}^{}\) denote our hypothesis class, and let \(h\) be the _fixed_ classifier that the learner is using for classification.

A Partial Knowledge Model for the Agents.We move away from the standard assumption that agents fully know \(h\) and model agents as having a _common_ (shared by all agents) _prior distribution_\(\) over \(\). This distribution captures their _initial_ belief about which classifier is deployed by the learner. Formally, for every \(h^{}\), \((h^{})\) is the probability that the learner is going to deploy \(h^{}\) for classification _from the agents' perspective_. We emphasize that the learner is committed to using a fixed classifier \(h\). The prior \(\) captures the agents' belief about the deployed classifier and is known to the learner.

For example, job seekers may use Glassdoor to prepare for interviews. They may not know the exact hiring algorithm (\(h\)) of a specific company but can observe patterns from other companies for similar roles. This forms their initial belief, represented by \(\), about the classifier a company might use. Thus, \(\) captures the agents' probabilistic beliefs rather than assuming full knowledge of \(h\).3

A Partial Information Release Model for the Learner.The learner has the ability to influence the agents' prior belief \(\) about the deployed classifier \(h\) by releasing partial information about \(h\). We model information release by releasing a subset \(H\) such that \(h H\). We note that we reveal information truthfully, meaning that the deployed classifier is required to be in \(H\).

Note that this is a general form of information release because it allows the learner to release _any_ subset of the hypothesis class, so long as it includes the deployed classifier \(h\). Below, we provide natural examples of information release that can be captured by our model.

**Example 2.1** (Examples of Information Release via Subsets).: Consider the class of linear halfspaces in \(d\) dimensions: \(=\{h_{w,b}:w=[w^{1},w^{2},,w^{d}]^{}^{d}_{+ },\,b\}\) where \(h_{w,b}(x)[w^{}x+b 0]\) and \(x=^{d}\) is the feature vector. Let \(h=h_{w_{0},b_{0}}\) be the classifier deployed by the learner for some \(w_{0},b_{0}\). Under this setting, revealing the corresponding parameter of a feature, say \(x^{j}\), in \(h\) corresponds to releasing \(H_{1}=\{h_{w,b}:w^{j}=w_{0}^{j}\}\) (e.g.,'minimal GPA of \(3.8\) for grad school'). Revealing the top \(k\) features of \(h\) (e.g., the most significant class grades are algorithms and calculus) corresponds to releasing \(H_{2}=\{h_{w,b}:w^{i_{1}},w^{i_{2}},w^{i_{k}}\) are the \(k\) largest coordinates of \(w\}\). Let \(I_{0}\) be such that \(w_{0}^{i} 0\) iff \(i I_{0}\). Revealing the relevant features of \(h\), i.e. features with nonzero coefficients (e.g., sensitive attributes like race or gender will not be used in the decision) corresponds to releasing \(H_{3}=\{h_{w,b}:w^{i} 0,\, i I_{0}\}\). This is a common form of information release in the real world4.

The Strategic Game with Partial Information Release.Once the partial information \(H\) is released by the learner, agents best respond as follows: each agent first computes their _posterior_ belief about the deployed classifier by projecting their prior \(\) onto \(H\), which we denote by \(|_{H}\), and is formally defined by: \( h^{},\ |_{H}(h^{}))}{(H)}[h^{} H]\). Given this posterior distribution, the agent then moves to a new point that maximizes their utility. The utility is _quasi-linear_ and measured by the probability (according to \(|_{H}\)) of receiving a positive outcome minus the manipulation cost. Formally, the utility of agent \(x\) that manipulates to \(x^{}\), under the partial information \(H\) released by the learner is given by

\[u_{x}(x^{},H)_{h^{}|_{H}}[h^{}(x^{ })=1]-c(x,x^{}).\] (1)

We let \((x,H)\) denote the best response of agent \(x\), i.e. a point \(x^{}\) that maximizes \(u_{x}(x^{},H)\). 5 The goal of the learner is to release \(H\) that includes its deployed classifier \(h\) so as to maximize its utility which is measured by its expected strategic accuracy.

\[U(H)_{x D}[h((x,H))=f(x)].\] (2)

**Definition 2.2** (Strategic Game with Partial Information Release).: _The game, between the learner who is using \(h\) for classification, and the agents who have a prior \(\) over \(\), proceeds as follows:_

1. _The learner (knowing_ \(f\)_,_ \(D\)_,_ \(c\)_,_ \(\)_) publishes a subset of hypotheses_ \(H\) _such that_ \(h H\)_._
2. _Every agent_ \(x\) _best responds by moving to a point_ \((x,H)\) _that maximizes their utility:_ \((x,H)*{argmax}_{x^{}}u_{x}(x^{ },H)\)_._

_The learner's goal is to find a subset \(H^{}\) with \(h H^{}\), that maximizes its utility6: \(H^{}*{argmax}_{H,\,h H}U(H)\)_

We note that similar to standard strategic classification, the game defined in Definition 2.2 can be seen as a _Stackelberg_ game in which the learner, as the "leader", commits to her strategy first and then the agents, as the "followers", respond. The optimal strategy of the learner, \(H^{}\), corresponds to the _Stackelberg equilibrium_ of the game, assuming best response of the agents.

Contrasting with the Standard Setting of Strategic Classification.The game defined in Definition 2.2 not only captures both the partial knowledge of the agents and the leaner's partial information release, but can also be viewed as a _generalization_ of the standard strategic classification game where the agents fully observe the classifier \(h\), which we refer to as the _full information release_ game (e.g., see ). This is because the learner can always choose \(H=\{h\}\). Next, we ask:

_Can partial information release increase the learner's utility compared to full information release?_

Observe that by definition, \(U(H^{}) U(\{h\})\), i.e., the learner can only gain utility when they optimally release partial information instead of fully revealing the classifier. In the following examples, we show that there exist instantiations of the problem where \(U(H^{})>U(\{h\})\), even when \(h\) is picked to be the optimal classifier in the full information release game, i.e., one that maximizes \(U(\{h\})\). In other words, we show that the learner can gain _nonzero_ utility by releasing a subset that is not \(\{h\}\), _even if the choice of \(h\) is optimized for the full information release game_.

**Example 2.3** (Partial vs. Full Information Release).: Suppose \(=\{x_{1},x_{2}\}\), and that their probability weights under the distribution7 are given by \(D(x_{1})=2/3,D(x_{2})=1/3\), and their true labels are given by \(f(x_{1})=1\), \(f(x_{2})=0\). Suppose the cost function is given as follows: \(c(x_{1},x_{2})=2,c(x_{2},x_{1})=3/4\). Let \(=\{h_{1},h_{2},h_{3}\}\) be given by table 1. One can show that under this setting, \(h=h_{1}\) is the optimal classifier under full information release, i.e., it optimizes \(U(\{h\})\), and that for such \(h\), \(U(\{h\})=2/3\). However, suppose the prior distribution over \(\) is uniform. One can show that under this setting, and when \(h=h_{1}\) is the deployed classifier, releasing \(H^{}=\{h_{1},h_{2}\}\) implies \(U(H^{})=1>U(\{h\})=2/3\). In other words, the learner can exploit the agent's prior by releasing information in a way that increases its own utility by a significant amount.

  & \(h_{1}(=f)\) & \(h_{2}\) & \(h_{3}\) \\  \(x_{1}\) & 1 & 0 & 0 \\ \(x_{2}\) & 0 & 1 & 0 \\ 

Table 1: Hypothesis class \(\) in Example 2.3In the next example, we consider the more natural setting of single-sided threshold functions in one dimension and show that the same phenomenon occurs: the optimal utility achieved by partial information release is strictly larger than the utility achieved by the full information release of \(h\), _even after the choice of \(h\) is optimized for full information release_.

**Example 2.4** (Partial vs. Full Information Release).: Suppose \(=\), \(D\) is the uniform distribution over \(\), \(f(x)=[x 1.9]\), \(=\{h_{t}:t\}\) where \(h_{t}(x)[x t]\). Suppose the cost function is given by the distance \(c(x,x^{})=|x-x^{}|\). We have that under this setting, the optimal classifier in \(\) under full information release is \(h=h_{2}\), and that its corresponding utility is \(U(\{h\})=1-_{x Unif}[1 x<1.9]=0.55\). Now suppose the agents have the following prior over \(\): \((h^{})=0.1[h^{}=h_{2}]+0.9[h^{ }=h_{1.8}]\). Under this setting, and when \(h=h_{2}\) is deployed for classification, one can see that releasing \(H^{}=\{h_{2},h_{1.8}\}\) leads to perfect utility for the learner. We therefore have \(U(H^{})=1>U(\{h\})=0.55\).

## 3 The Agents' Best Response Problem

In this section we focus on the best response problem faced by the agents in our model, as described in Definition 2.2. We consider a natural optimization oracle for the cost function of the agents that can solve simple projections. We will formally define this oracle later on. Given access to such an oracle, we then study the _oracle complexity8_ of the agent's best response problem. First, we show that the best response problem is computationally hard even for a common family of \(_{p}\)-norm cost functions. Next, we provide an _oracle-efficient_ algorithm9 for solving the best response problem when the hypothesis class is the class of low-dimensional linear classifiers. In Appendix B, we consider _submodular cost functions_ and show that for any hypothesis class, there exists an oracle-efficient algorithm that _approximately_ solves the best response problem in this setting.

Recall that the agents' best response problem can be cast as the following: given an agent \(x\), and a distribution \(P\) (e.g., \(P=|_{H}\) where \(\) is the prior and \(H\) is the released information) over a set \(\{h_{1},,h_{n}\}\), we want to solve \(*{argmax}_{z}\{_{h^{} P}[h^ {}(z)=1]-c(x,z)\}\). We consider an oracle that given any region \(R\), specified by the intersection of positive (or negative) regions of \(h_{i}\)'s, returns the projection of the agent \(x\) onto \(R\) according to the cost function \(c\): \(*{argmin}_{z R}c(x,z)\). For example, when \(\) is the class of linear classifiers and \(c(x,z)=\|x-z\|_{2}\), the oracle can compute the \(_{2}\)-projection of the agent \(x\) onto the intersection of any subset of the linear halfspaces in \(\{h_{1},,h_{n}\}\). We denote this oracle by \(*{Oracle}(c,)\) and formally define it in Algorithm 1.

```
0: agent \(x\), region \(R=R^{+} R^{-}\) specified as, \(R^{+}=\ _{i I^{+}}\{z:h_{i}(z)=1\}\) and \(R^{-}=\ _{i I^{-}}\{z:h_{i}(z)=0\}\) for some \(I^{+}\) and \(I^{-}\). \(*{argmin}_{z R}c(x,z)\) ```

**Algorithm 1**Oracle\((c,)\)

Having access to such an oracle, and without further assumptions, the best response problem can be solved by exhaustively searching over all subsets of \(\{h_{1},,h_{n}\}\) because:

\[_{z}\{_{h^{} P}[h^{}(z)=1] -c(x,z)\}=_{S\{h_{1},,h_{n}\}}\{_{h^{}  S}P(h^{})-_{z:h^{}(z)=1,\, h^{} S}c(x,z)\}\] (3)

This algorithm is inefficient because it makes exponentially many oracle calls. In what follows, we consider natural instantiations of our model and examine if we can get algorithms that make only \(poly(n)\) oracle calls. All missing proof of this sections are provided in Appendix C.

\(p\)-Norm Cost Functions.First, we consider Euclidean spaces and the common family of \(p\)-norm functions for \(p 1\) and show that even under the assumption that the cost function of the agent belongs to this family, the problem of finding the best response is computationally hard. Formally, a \(p\)-norm cost function is defined by: for every \(x,x^{}^{d}\), \(c_{p}(x,x^{})=\|x-x^{}\|_{p}\) where \(p 1\).

**Theorem 3.1**.: \((2^{n}/)\) _calls to the oracle (Algorithm 1) are required to compute the best response of an agent with \(2/3\) probability of success, even when \(=^{2}\) and the cost function is \(c_{p}\) for \(p 1\)._
Low-Dimensional Linear Classifiers.Next, we show that when \(=^{d}\) for some \(d\), and when \(\) contains only linear classifiers, i.e., every \(h\) can be written as \(h(x)=[w^{}x+b 0]\) for some \(w^{d}\) and \(b\), then the best response of the agents can be computed with \(O(n^{d})\) oracle calls when \(d n\).

The algorithm, which is described in Algorithm 2, first computes the partitioning (\(R_{n}\)) of the space \(\) given by the \(n\) linear classifiers. For any element of the partition in \(R_{n}\), it then solves the best response when we restrict the agent to select its best response from that particular element. This gives us a set of points, one for each element of the partition. The algorithm finally outputs the point that has maximum utility for the agent. This point, by construction, is the best response of the agent. The oracle-efficiency of the algorithm follows from the observation that \(n\) linear halfspaces in \(d\) dimensions partition the space into at most \(O(n^{d})\) elements when \(d n\). Formally,

**Theorem 3.2**.: _Suppose \(=^{d}\) for some \(d n\), and \(\) contains only linear classifiers. Then for any agent \(x\), any cost function \(c\), and any distribution \(P\) over \(\{h_{1},,h_{n}\}\), Algorithm 2 returns the best response of the agent in time \(O(n^{d+1})\), while making \(O(n^{d})\) calls to the oracle (Algorithm 1)._

### Generalizing to Arbitrary \(P\)

In Theorem 3.2 we require the distribution \(P\) be over \(\{h_{1}, h_{n}\}\), e.g. to have finite support.

When this does not hold, ie. \(P\) has infinite support size, we can ignore classifiers with sufficiently small probabilities (i.e., \(poly()\)), as they do not affect the manipulation strategy when searching for an \((1+)\)-approximate solution. The number of classifiers in the support with probability at \(poly()\) for a fixed \(>0\) is at most \(1/poly()\) which is a finite number. Therefore, to obtain a nearly optimal solution, it suffices to only consider probability distributions with finite support size.

## 4 The Learner's Optimal Information Release Problem

In this section we focus on the learner's optimization problem as described in Definition 2.2. The learner is facing a population of agents with prior \(\) and wants to release partial information \(H\) so as to maximize its utility \(U(H)\). We note that the learner's strategy space can be restricted to the support of the agents' prior \(\) because including anything in \(H\) that is outside of \(\)'s support does not impact \(U(H)\). Therefore, one naive algorithm to compute the utility maximizing solution for the learner is to evaluate the utility on all subsets \(H()\) and output the one that maximizes the utility. However, this solution is inefficient; instead, can we have computationally efficient algorithms? We provide both positive and negative results for a natural instantiation of our model which is introduced below.

The Setup: Classification Based on Test Scores.We adopt the following setup for the learner's information release problem. We are motivated by screening problems such as school admissions and hiring where an individual's qualification level can be captured via a real-valued number, say, a test score. We therefore consider agents that live in the one dimensional Euclidean space: \(=[0,B]\) for some \(B\). One can think of each \(x\) as the corresponding qualification level or test score of an agent where larger values of \(x\) correspond to higher qualification levels or higher test scores. As we are in a strategic setting, agents can modify their true feature \(x\) and "game" the learner by appearing more qualified than they actually are.

We let \(f(x)=[x t]\) for some \(t\): there exists some threshold \(t\) that separates qualified and unqualified agents. We take the hypothesis class \(\) to be the class of all single-sided threshold classifiers: every \(h^{}\) can be written as \(h^{}(x)[x t^{}]\) for some \(t^{}\). We further take the cost function of the agents to be the standard distance metric in \(\): \(c(x,x^{})=|x^{}-x|\).10

**Remark 4.1**.: _We emphasize that considering agents in the one-dimensional Euclidean space is only for simplicity of exposition. We basically assume, for an arbitrary space of agents \(\), there exists a function \(g:[0,B]\) such that \(f(x)=[g(x) t]\) for some \(t\), and that the cost function is given by \(c(x,z)=|g(z)-g(x)|\). Here, \(g(x)\) captures the qualification level or test score of an agent \(x\). Now observe that we can reduce this setting to the introduced setup of this section: take \(^{}=\{g(x):x\}[0,B]\), \(f:^{}\{0,1\}\) is given by \(f(x^{})=[x^{} t]\), and that the cost function \(c:^{}^{}[0,)\) is given by \(c(x^{},z^{})=|z^{}-x^{}|\)._

**Remark 4.2**.: _Note that because every classifier \(h^{}\) is uniquely specified by a real-valued threshold, for simplicity of our notations, we abuse notation and use \(h^{}\) interchangeably as both a mapping (the classifier) and a real-valued number (the corresponding threshold) throughout this section. The same abuse of notation applies to \(f\) as well._

The classifier deployed by the learner is some \(h f\). We note that it is natural to assume \(h f\) because in our setup, higher values of \(x\) are considered "better". So given the strategic behavior of the agents, the learner only wants to make the classification task "harder" compared to the ground truth \(f\) -- choosing \(h<f\) will only hurt the learner's utility. Because we will extensively make use of the fact that \(h f\), we state it as a remark below.

**Remark 4.3**.: _The learner's adopted classifier is some \(h\) such that \(h f\)._

First, we show that under the introduced setup, the learner's optimization problem is NP-hard if the prior can be chosen arbitrarily. This is shown by a reduction from the NP-hard _subset sum_ problem. The formal NP-hardness statement and its proof, as well as further useful facts about the agents' best response under this setup are in Appendix D.

### An Efficient Algorithm for Discrete Uniform Priors

Given the hardness of the learner's problem for arbitrary prior distributions, we focus on a specific family of priors, namely, _uniform_ priors over a given set, and examine the existence of efficient algorithms for such priors. In Appendix D.2, we consider _continuous_ uniform priors and provide closed form solutions for the learner's optimal partial information release problem.

In this section, we provide an efficient algorithm for computing the learner's optimal information release when the prior \(\) is a _discrete_ uniform distribution over a set \(\{h_{1},h_{2},,h_{n}\}\) that includes the adopted classifier \(h\). Here, the objective of the learner is to release a subset \(H\{h_{1},h_{2},,h_{n}\}\) such that \(h H\). Throughout, we take \(h=h_{k}\) where \(1 k n\), and assume \(h_{1} h_{2} h_{n}\). The complete exposition of this section, including all details, proofs, necessary lemmas, and the description of the proposed algorithm, can be found in Appendix E.

We first characterize the utility of any subset \(H\) released by the learner using a real-valued function of \(H\). Define, for any \(H\{h_{1},,h_{n}\}\) such that \(h H\),

\[R_{H}\{x:(x,H) h\}\] (4)

Note that \((x=h,H) h\) for any \(H\) such that \(h H\). Therefore, \(\{x:(x,H) h\}\) is nonempty, and that \(R_{H} h\) for any \(H\) such that \(h H\). In the following lemma, we show that \(R_{H}\) characterizes the utility of \(H\) for the learner, for any prior \(\) over \(\{h_{1},,h_{n}\}\).

**Lemma 4.4**.: _Fix any prior \(\) over \(\{h_{1},,h_{n}\}\). We have that for any \(H\{h_{1},,h_{n}\}\) such that \(h H\), the utility of the learner, given by Equation 2, can be written as_

\[U(H)=1-_{x D}[R_{H}<x<f]&R_{H}<f\\ 1-_{x D}[f x R_{H}]&R_{H} f\] (5)Given such characterization of the learner's utility by \(R_{H}\), we show that when the agents' prior is uniform over \(\{h_{1},,h_{n}\}\), there are only _polynomially many_ possible values that \(R_{H}\) can take, even though _there are exponentially many \(H\)'s_. We characterize the set of possible values for \(R_{H}\) as well. For any possible value \(R\) of \(R_{H}\), our algorithm efficiently finds a subset \(H\) such that \(R_{H}=R\), if such \(H\) exists, and finally outputs the one with maximal utility.

More formally, we consider the following partitioning of the space of subsets of \(\{h_{1},,h_{n}\}\). For any \(\{1,2,,n\}\), and for any \(i\{k,k+1,,n\}\), define

\[S_{i,}\{H\{h_{1},,h_{n}\}:h H,\,|H|=,\, (h,H)=h_{i}\}\]

Note that \((h h_{k},H)\{h_{i}:i k\}\) for any \(H\). Therefore, \(\{S_{i,}\}_{i,}\) gives us a proper partitioning of the space of subsets, which implies

\[_{H\{h_{1},,h_{n}\},h H}U(H)=_{i,}_{H S_ {i,}}U(H)\]

Given this partitioning of the space, we show that \(R_{H}\) can be characterized as follows.

**Lemma 4.5**.: _If the prior \(\) is uniform over \(\{h_{1},,h_{n}\}\), then for any \(H S_{i,l}\), \(R_{H}=h_{i}-j/\) where \(j=|\{h^{} H:h^{}(R_{H},h_{i}]\}|\)._

Given such characterization, our proposed algorithm (Algorithm 3), for any \(i,\), enumerates over all possible \(j\{1,,\}\) and returns a \(H\) such that \(H S_{i,}\) and \(R_{H}=h_{i}-j/\), if such \(H\) exists. The algorithm then outputs the subset \(H\) with maximal utility according to Equation 5.

**Theorem 4.6**.: _There exists an algorithm (Algorithm 3) that for any \(n\), any uniform prior over \(\{h_{1},,h_{n}\}\) that includes \(h\), and any data distribution \(D\), returns \(H^{}\{h_{1},,h_{n}\}\) in time \(O(n^{3})\) such that \(h H^{}\), and that \(U(H^{})=_{H\{h_{1},,h_{n}\},h H}U(H)\)._

### Minimizing False Positive (Negative) Rates for Arbitrary Priors

While so far we worked with _accuracy_ as the utility function of the learner, in this section, we consider other natural performance metrics and provide insights on the optimal information release for the proposed utility functions, without restricting ourselves to uniform priors. In particular, we consider utility functions that are based on _False Negative Rate_ (FNR) and _False Positive Rate_ (FPR) which are formally defined below. For any \(H\) such that \(h H\),

\[U_{FPR}(H)  1-FPR(H) 1-_{x D}[h(BR(x,H))=1|f(x)=0]\] (6) \[U_{FNR}(H)  1-FNR(H) 1-_{x D}[h(BR(x,H))=0|f(x)=1]\] (7)

In the following theorem, we establish that for any given prior \(\) over a set \(\{h_{1},h_{2},,h_{n}\}\), if the learner aims to minimize the FPR, _no-information-release_ is preferable to _full-information-release_.11 Additionally, we show that for minimizing the FNR, an optimal strategy for the learner is _full-information-release_. By "no-information-release", we mean releasing any subset \(H\) such that \(H\) includes the support of the prior \(\): \(H\{h_{1},,h_{n}\}\) which results in \(|_{H}=\).By "full-information-release", we mean revealing the classifier: \(H=\{h\}\).

**Theorem 4.7**.: _Fix any \(h f\). For any prior \(\) over \(\{h_{1},,h_{n}\}\) that includes \(h\), we have 1) \(FPR() FPR(\{h\})\). 2) \(FNR(\{h\}) FNR(H)\) for every \(H\) such that \(h H\)._

The proof is provided in Appendix F. In Appendix G, we show that minimizing FPR, unlike minimizing FNR, does not always have a clear optimal solution. We provide three instances such that full-information-release is optimal for the first, no-information-release is optimal for the second, and neither is optimal for the third.

## 5 Conclusion

We introduce _Bayesian Strategic Classification_, meaning strategic classification with partial knowledge (of the agents) and partial information release (of the learner). Our model relaxes the often unrealistic assumption that agents fully know the learner's deployed classifier. Instead, we model agents as having a distributional prior on which classifier the learner is using. Our results show the existence of previously unknown intriguing informational middle grounds; we also demonstrate the necessity of revisiting the fundamental modeling assumptions of strategic classification in order to provide effective recommendations to practitioners in high-stakes, real-world prediction tasks.