# The Empirical Impact of Data Sanitization on Language Models

Anwesan Pal\({}^{*}\)

Equal contribution

Radhika Bhargava\({}^{*}\)

Kyle Hinsz

Jacques Esterhuizen

Sudipta Bhattacharya

Amazon Web Services

{anwesanp, radhikb, khinsz, jesterhu, sudibhat}@amazon.com

###### Abstract

Data sanitization in the context of language modeling involves identifying sensitive content, such as personally identifiable information (PII), and reducing them from a dataset corpus. It is a common practice used in natural language processing (NLP) to maintain privacy. Nevertheless, the impact of data sanitization on the language understanding capability of a language model remains less studied. This paper empirically analyzes the effects of data sanitization across several benchmark language-modeling tasks including comprehension question answering (Q&A), entailment, sentiment analysis, and text classification. Our experiments cover a wide spectrum comprising finetuning small-scale language models, to prompting large language models (LLMs), on both original and sanitized datasets, and comparing their performance across the tasks. Interestingly, our results suggest that for some tasks such as sentiment analysis or entailment, the impact of redaction is quite low, typically around 1-5%, while for tasks such as comprehension Q&A there is a big drop of >25% in performance observed in redacted queries as compared to the original. For tasks that have a higher impact, we perform a deeper dive to inspect the presence of task-critical entities. Finally, we investigate correlation between performance and number of redacted entities, and also suggest a strategy to repair an already redacted dataset by means of content-based subsampling. Additional details are available at [https://sites.google.com/view/datasan](https://sites.google.com/view/datasan).

## 1 Introduction

Data privacy is a critical concern in the development and use of language models (LMs) specially due to the sensitive nature of personally identifiable information (PII) that can be present in the text. PII commonly includes sensitive information such as person names, addresses, emails, or social security numbers. Data privacy concerns are in part motivated by security issues that arise from LMs memorizing portions of the training data, which can then be extracted via adversarial attacks . PII data breaches are a serious concern for large corporations, as they can lead to severe damage to the reputation and finances of an organization. Furthermore, corporate data governance policies are driven by applicable privacy laws which place strict legal limitations on the use of PII.

Some popular techniques to anonymize data in the Natural Language Processing (NLP) domain include differential privacy and data sanitization. Differential privacy  involves development of a mathematical framework that adheres to a rigorous definition of privacy by injecting noise into the data. While this guarantees that a trained model will not reveal any user identifiable information, adding noise comes with the price of great loss in data fidelity, which is not ideal for studying the impact of anonymization on model performance . On the other hand, data sanitizationinvolves complete and irreversible removal of personally identifiable information from data without the introduction of additional noise. Such a masking approach ensures that information pertaining to an individual cannot be recovered, either directly or in collaboration with a third party. In recent years, a number of organizations like Microsoft, Paypal, and Mastercard  have employed sanitized data for training LMs to leverage information present in their free text corpora while minimizing data leakage and privacy violations. Therefore, in this work, we adopt the data sanitization approach for analyzing impact on performance of language models via redaction.

Despite the wide adoption of data sanitization methods for protecting sensitive information, the exact impacts of redacting PII content from natural language data on the performance of language models has not been studied in-depth to the best of the authors' knowledge. Making inferences based on context is core to how language models function, and therefore stripping away contextual identifiers will likely reduce a model's ability to comprehend text, thereby leading to a decrease in performance. Additionally, replacing diverse PII with generic tokens, for instance replacing two different names with the same <NAME> token introduces ambiguity, making it harder for the model to differentiate between unique entities. An example of how redacting PII impacts a large language model's (LLM's) thinking process is depicted in Figure 1.

This work aims to perform an empirical analysis of the impact of data sanitization on the language understanding capabilities of both small and large language models. Our main contributions include:

* We perform a number of ablation experiments by fine-tuning small models like BART  and GPT-2  across several benchmark NLP tasks to better understand the impact of PII redaction.
* We also conduct prompting experiments with popular large language models like Claude 3.5 Sonnet , Mistral 7B , and GPT-4o  to investigate the impact of PII redaction on some common generative artificial intelligence (GenAI) datasets. This analysis is first done using a full set of redacted entities, and subsequently on a limited set by not redacting task-critical entities.
* Finally, we investigate the correlation between task performance and number of named entities being redacted by using different sampling techniques, and suggest a strategy to best utilize an already redacted dataset without compromising on accuracy.

Figure 1: LLM chain-of-thought for a query in the original vs redacted forms. In the redacted case, the reasoning module correctly highlights missing information in the query, and is therefore unable to provide the answer to an otherwise straight-forward question.

Related Work

### Data Sanitization Tools

Data sanitization is generally achieved through replacing PII with non-sensitive tokens (e.g. <NAME> or <SSN>) prior to use. Sanitization techniques typically use sequence labeling approaches, such as named entity recognition (NER) algorithms, to identify potential PII entities, which are then replaced [9; 16]. One such open-source anonymization tool from Kleinberg and Mozes, called NETANOS (named entity-based text anonymization for open science) , uses the Stanford Named Entity Tagger  to identify PII in data and then replaces the entities with their types (e.g., a name is replaced with a [NAME] token).

A central criticism of data-sanitization-based data protection techniques is that they only redact PII. A newer data sanitization technique called Textwash  includes a tag that comprises a meta-category which encapsulates the potentially sensitive information (PSI) concept. Concretely, PSI refers to the full spectrum of textual information that could reveal an identity but cannot be attributed to a well-defined PII entity.

### Training with Sanitized Data

Many related works have incorporated data sanitization into multi-step training pipelines. For example, Shi _et al._ propose a two step fine-tuning method in which they train on redacted data in phase one, and again with the original unredacted data using a private training mechanism in phase two. Kong _et al._ propose a systematic framework for redacting data from pre-trained generative models. They compare data redaction to data deletion and look at which data samples are 'hard' to redact. While there is abundant research on how to anonymize data [14; 28] and the associated privacy risks [25; 3; 29], there is limited research on the effects of PII redaction on model performance.

In the current era where large language models (LLMs) have made a pronounced impact across a number of fields of research, an analysis into the impact of data sanitization on their performance is of profound importance. Decisions about which LLMs to use for a task are often based on their performance on public benchmarks . However, when these models are applied on domain-specific tasks with high amounts of sensitive entities redacted, their performances drastically drop at times. This paper aims to showcase this analysis across a number of popular generative artificial intelligence (GenAI) benchmarks.

## 3 Experimental Design

To thoroughly understand the impact of data sanitization on LM performance, we perform a number of experiments spanning small to large language models, and across a variety of NLP and GenAI datasets. To perform data sanitization we used the named entity recognition (NER) tool provided by the spaCy package . The entities that we redact in this work are person names, locations, organization names, dates, times, sensitive encoded numbers such as credit card or social security numbers, email addresses, and intellectual property such as names of movies or various works of art.

### Datasets

**Traditional NLP Datasets**: We begin our analysis on a set of traditional natural language processing tasks corresponding to the GLUE  and LexGLUE  benchmarks: semantic similarity with the Quora Question Pairs (QQP) dataset , textual entailment prediction on the Multi-Genre Natural Language Inference (MultiNLI) dataset , reading comprehension on the Winograd Schema Challenge dataset , multi-class classification on the LEDGAR dataset , and multi-label classification on the EURLEX dataset . Additionally, we also consider tasks such as extractive question answering (Q&A) on the SQuADv2.0 dataset  and sentiment analysis on the IMDB movie review sentiment classification dataset . These tasks are selected due to a substantial fraction of the total samples in each task containing PII entities. Specifically, all tasks contain at least 49% of samples with PII entities, and some tasks contain as much as 90% of samples with PII entities. Table 4 in Appendix 6.1 shows the full statistics on the composition of these datasets.

**GenAI Datasets**: We include the following recently released datasets that are used to benchmark modern large language model (LLM) performances in our analysis: Discrete reasoning over content of paragraphs on DROP  dataset, linguistically diverse grade school math word problems on GSM8K , and a set of tasks from Big-Bench-Hard (BBH) benchmark . For BBH, we excluded some tasks which did not have any PII entity, or contained non-english words. Details of the inclusion/exclusion criteria for BBH tasks is provided in Appendix 6.1.2. We also retained the SQuADv2.0 and IMDB datasets from our earlier study here, as they are still popular in the LLM community.

For both sets of datasets, we redacted the entities mentioned in Section 3 using the spaCy tool. Since the SQuADv2.0 dataset requires computing the span of the answer, in other words the indices at which the answer begins and ends, we modified the indices in the train and dev sets based on the length of the redaction token to ensure that the answer spans remain consistent.

### Language Models

To separately study the impact of data sanitization across fine-tuning and prompting tasks, we performed our analysis using a host of small and large language models.

**Fine-Tuning with small language models**: We consider models with <5B parameters in this category. For this study, we investigate the effects of data sanitization across BART  and GPT-2  models. We selected these models to cover both encoder and decoder blocks (BART) and decoder-only blocks (GPT-2). We have fine tuned these models on the NLP datasets discussed in Section 3.1, and evaluated the performance on several different train/test dataset pairings as mentioned in 4.1.

**Prompting large language models**: For large language models (LLMs) with >5B parameters, we have used chain-of-thought (CoT) prompting with few-shot examples  to study the effects of data sanitization on the following models: Anthropic's Claude 3.5 Sonnet , Mistral AI's Mistral 7B , and OpenAI's GPT 4o . It should be noted that for both the original and redacted evaluation sets, the few-shot examples used here were from the original unredacted train set, under the hypothesis that prior to evaluation in many real-world applications on sanitized, we may not have access to the test data. The results for these experiments are found in Section 4.2.

## 4 Results

In this section, we discuss the results of our various experiments. We start with the analysis on fine-tuning smaller language models in Section 4.1, followed by discussion on prompting large language models in Section 4.2.

### Fine-Tuning Results

We present the evaluation results for transformer models trained with both sanitized and unsanitized data for the benchmark NLP tasks discussed in Section 3.1. Within each of these tasks, we calculated performance metrics for the following train/test dataset pairings1:

* None/None: The performance of the model trained on unredacted data and evaluated on unredacted data. This is our baseline result where no PII redaction is applied.
* Redact/Redact: The performance of the model trained on redacted data and evaluated on redacted data. This is the most conservative redaction policy, where all PII is redacted for both training and inference. Such a redaction policy may be required when inference is performed in batch.
* None/Redact: The performance of the model trained on unredacted data and evaluated on redacted data. This may be applicable if third party models are applied to redacted data.

In cases where the \(test\) set label is not provided for independent evaluation, we tested on the labeled \(dev\) set.

**Model and Task Comparison**: The performance comparison of redaction on BART and GPT-2 models across NLP tasks is provided in Table 1. For all datasets, we observe that the performance impact between None/None and Redact/Redact is less as compared to None/None and None/Redact.

This is likely due to the misalignment between the training and the test sets when redaction is introduced to the model. Another observation is that the results suggest only minimal degradation in model performance when training on redacted data, with model performance decreasing less than 2.2% on the average. The models are robust to these tasks and in some cases (for instance in the case of IMDB and LexGLUE) no impact is observed because for these tasks the language models needs the entire context to make a prediction rather than relying on specific PII entities.

An exception to the above mentioned observation is seen for GLUE: QQP and SQuADv2.0 datasets. For QQP, even though it is classified as low impact the performance difference between None/None and None/Redact is high when compared to other tasks (7% vs 2% on average). A possible reason for this is that the redaction of PII entities may cause a language model to pay more attention on surrounding context rather than query-critical PII entities, potentially leading to inaccurate semantic similarity scores. Additionally, SQuADv2.0 is an extractive question answering task that suffers the largest degradation in model performance when trained on redacted data. We hypothesize that this is an expected result for models trained on Q&A tasks, which are likely more reliant on leveraging contextual PII entities and references to identify answers than models trained for other tasks, such as sentiment analysis or entailment.

### Prompting generative model Results

As many of the popular large language models are not open-source, we do not have the flexibility to fine-tune them on a redacted set. In lieu of that, we discuss the impact of PII redaction on performance of large language models that are _prompted_ to solve a task. This is similar to the None/Redact setup that we used for the smaller models in Section 4.1. Table 2 provides the scores for Claude 3.5 Sonnet, Mistral 7B, and GPT 4o on the GenAI datasets mentioned in Section 3.1. For each dataset, we report the performance on the original (None) and redacted (Redact) sets, along with the relative impact.

#### 4.2.1 General Observations

**Low vs Medium vs High impact datasets**: We observe that the impact of redaction on the different tasks range from 0.3% to 95% for Claude, -2.7% to 67.3% for Mistral and -6.5% to 100% for GPT. Based on these results, we have classified the datasets as low impact if the impact on performance was < 10%, medium impact if the impact on performance was between 10 and 25% and high impact for those datasets where the impact was greater than 25%. If a dataset such as BBH: formal fallacies has diverse impact across the three models, then we based our classification on the majority vote. One observation is that the severity of the impact is not only dependant on the number of records redacted in a dataset but also on the type of the task and how important the entities are to reason about a given task e.g. BBH: causal judgement had 91% of the dataset redacted but is still a low impact dataset as the correct answer in that task is dependant on the entire context provided to the LLM rather than the

    &  &  \\  Datasets & None/None & Redact/Redact & None/Redact & None/None & Redact/Redact & None/Redact \\  10\%)_} \\  IMDB (Acc) & 93.7 & 93.7 & 93.6 & 93.1 & 93.2 & 92.7 \\ LexGLUE: LEDGAR (F1) & 87.0 & 87.0 & 85.7 & 87.5 & 87.0 & 85.8 \\ LexGLUE: EURLEX (F1) & 66.3 & 66.3 & 65.2 & 64.1 & 62.1 & 60.1 \\ GLUE: MNLI (m) (Acc) & 85.9 & 83.7 & 81.7 & 81.8 & 81.5 & 77.8 \\ GLUE: MNLI (mm) (Acc) & 86.1 & 84.3 & 82.5 & 82.5 & 82.0 & 79.1 \\ GLUE: WNLI (Acc) & 47.9 & 47.9 & 47.9 & 47.0 & 47.0 & 47.9 \\ GLUE: QQP (Acc) & 90.4 & 88.5 & 83.8 & 89.0 & 86.9 & 82.9 \\   \\  SQuADv2.0 (F1) & 74.9 & 57.7 & 60.2 & 55.8 & 48.7 & 48.7 \\   

Table 1: Performance results on NLP datasets. For each dataset, the model performances are shown for different combinations of original and redacted versions across training and validation splits. All numbers are in %.

entity value. This also explains why the BBH tasks are split across low, medium and high impact datasets.

**Oddities in Mistral's performance on Redacted Datasets**: A consistent observation from Table 2 is that the Claude 3.5 and GPT 4o models tend to perform similarly in terms of redaction impact, while Mistral demonstrates some uncommon properties. This happens because Mistral has a tendency to hallucinates and assign placeholder values for redacted entities, and then reason about them incorrectly to arrive at the correct answer. An example of this is depicted in Figure 2. The extreme manifestation of this type of hallucination is seen for IMDB, BBH: logical deduction (#5), and BBH: disambiguation qa datasets, where the performance on the redaction set is even better than the original unredacted set.

#### 4.2.2 Weaker Redaction for High Impact Datasets

In the previous section we have discussed the redaction impact on language models by covering the full set of entities redacted as mentioned in Section 3. This leads to the low, moderate, and high impact clusters of datasets. Upon diving deep into the high impact datasets, we observed that some of the entities redacted are too harsh, and removing them often makes the dataset impossible to solve. An example of this is redacting the <DATE> entity in BBH: date understanding task,

    &  &  &  \\  Datasets & None & Redact & Impact & None & Redact & Impact & None & Redact & Impact \\  10\%)_} \\  IMDB & 95.8 & 95.5 & 0.3 & 86.5 & 86.6 & -0.1 & 93.9 & 93.1 & 0.9 \\ BBH: Hyperbaton & 99.6 & 99.2 & 0.4 & 49.6 & 49.2 & 0.8 & 100 & 98.8 & 1.2 \\ BBH: Disambiguation QA & 75.5 & 74.0 & 2.0 & 58.8 & 60.4 & -2.7 & 80.0 & 85.2 & -6.5 \\ BBH: Snarks & 90.4 & 88.1 & 2.5 & 52.2 & 48 & 8.04 & 89.8 & 87 & 3.2 \\ BBH: Run Names & 90.4 & 84.3 & 6.7 & 34 & 32.8 & 3.52 & 90.8 & 85.6 & 5.7 \\ BBH: Logical Deduction (\#3) & 98.7 & 91.2 & 7.6 & 46.4 & 41.2 & 11.2 & 100 & 90.0 & 9.6 \\ BBH: Causal Judgement & 69.0 & 63.0 & 8.7 & 42.8 & 42.2 & 1.35 & 67.0 & 65.0 & 2.7 \\ BBH: Formal Fallacies & 88.0 & 75.0 & 14.8 & 60 & 57.2 & 4.7 & 78.0 & 74.0 & 5.6 \\   \\  SQuADv2.0 & 65.8 & 57.8 & 12.2 & 46.1 & 30.5 & 33.8 & 68.3 & 51.4 & 24.7 \\ BBH: Logical Deduction (\#5) & 93.6 & 82.7 & 11.6 & 24.4 & 26.0 & -6.5 & 91.6 & 80 & 12.6 \\ BBH: Logical Deduction (\#7) & 83.5 & 64.7 & 22.6 & 22.8 & 18.4 & 19.3 & 79.6 & 66.8 & 16.1 \\  25\%)_} \\  DROP & 92.1 & 54.2 & 41.1 & 46.1 & 25.9 & 43.8 & 91.6 & 49.3 & 46.2 \\ GSM8K & 96.9 & 44.6 & 54.0 & 45.3 & 19.0 & 58.1 & 57.6 & 25.5 & 55.7 \\ BBH: Date Understanding & 92.8 & 40.6 & 56.3 & 40.4 & 13.2 & 67.3 & 90.8 & 19.6 & 78.4 \\ BBH: Penguins in a Table & 99.3 & 30.8 & 69 & 43.8 & 29.4 & 32.9 & 99.0 & 47.0 & 48.2 \\ BBH: Tracking Shuffled Objects (\#5) & 100.0 & 8.8 & 91.2 & 27.6 & 18 & 34.8 & 98.4 & 4 & 95.9 \\ BBH: Tracking Shuffled Objects (\#7) & 100.0 & 4.8 & 95.2 & 22.8 & 12.8 & 43.9 & 99.6 & 0 & 100 \\   

Table 2: Performance results on GenAI datasets. For each dataset, the model performances are shown on the original and redacted versions, along with their relative impact. All numbers are in %.

    &  &  &  \\ Datasets & None & Full & Limited & Impact & PII Entities \\  DROP & 92.1 & 54.2 & 79.3 & 13.8 & NAME, LOC, ORG \\ GSM8K & 96.9 & 44.6 & 90.1 & 7.0 & NAME, LOC, ORG \\ BBH: Date Understanding & 92.8 & 40.6 & 86.3 & 7.0 & NAME \\ BBH: Penguins in a Table & 99.3 & 30.8 & 82.9 & 16.5 & NAME, ORG \\ BBH: Tracking Shuffled Objects (\#5) & 100.0 & 8.8 & 95 & 5 & LOC, ORG, DATE \\ BBH: Tracking Shuffled Objects (\#7) & 100.0 & 4.8 & 93 & 7 & LOC, ORG, DATE \\   

Table 3: Performance results for limited redaction across tasks using Claude 3.5 Sonnetwhich results in a huge drop in performance as expected. To investigate whether excluding some task-critical entities from the redaction set can restore the full accuracy of these impacted tasks, we run another experiment with weaker redaction. Our results are presented in Table 3, where for each dataset, we show the limited set of PII entities that we redacted. We run these experiments only for the Claude model since that was the best performing among all the three models we tested. With weaker redaction, we observe that nearly all of the high impact tasks can now be re-classified as low impact as per our thresholds. The exceptions are DROP and BBH: penguins in a table, which are still moderate impact. We hypothesize that for these two datasets, there is at least one non-critical PII entity (<QUANTITY> for the penguins task and <NAME> and <ORG> for DROP) which is dominant, causing its performance to still not be comparable to the unredacted dataset's performance.

#### 4.2.3 Correlation between number of Redacted Entities and Performance

Given that it is extremely difficult to guarantee that a provided test dataset has been completely sanitized, an evident detail to investigate is the correlation between task performance and number of named entities (NE) being redacted. For this, we performed a systematic experiment where we progressively redacted parts of the dataset, and evaluated the models' performances. Naturally, there are two ways to choose the number of samples to redact, (i) random and (ii) based on named entity (NE) content. The plot in Figure 3 shows the relative performance between these sampling options. It is interesting to see that if we randomly redact samples, there is a linear decline as expected. However, with content sampling, we can see better performance on the dataset when redacting low content entities (see Figure 3). Moreover, for DROP dataset, the correlation between performance drop and number of redacted entities is not that strong (the lines are collinear with random dropping). We hypothesize this to be related to the presence of a diverse set of PII entities present in this dataset. Please refer to Appendix 6.1.1 for more details about this.

#### 4.2.4 Strategy to repair a given Redacted Dataset

In many real-world applications involving GenAI algorithms, developers often do not have control over the degree of redaction within the dataset, and have to make the best possible use of it in its

Figure 2: Mistral’s hallucination in the context of entity redaction. As shown in the figure, Claude and GPT models correctly highlight the lack of information present in the query due to redaction, and proceed to not provide any answer. In contrast, Mistral assigns sequential values to various <NAME> tags, and reasons about them to arrive to the correct answer. This explains the trend of Mistral’s performance getting less impacted by redaction as compared to the other models.

redacted state. A prime example of this is in the domain of handling customer-centric data. For these use-cases, there is a strong need of developing a strategy to filter out portions of the data which can impact performance. One such strategy involves subsampling a given redacted dataset by removing high PII-content records, and using the remaining ones. This is shown in Figure 4. As seen for the SQuADv2.0 and GSM8k datasets, there is a wide difference in performance as we filter out records based on content as opposed to randomly. An interesting exception to this is DROP dataset, which does not follow the trend. This is possibly related to the fact that as that dataset contains a large amount of text, greedily filtering out high content records might not always be detrimental to the main task. An illustration of this is shown in the Appendix 6.1.1.

## 5 Conclusions & Future Work

In this paper we have demonstrated that on smaller language models, training data sanitization has minimal impact on model performance across most benchmark NLP tasks, with extractive question answering being one notable exception. For all other tasks studied, we observe less than a 2.5% drop in performance, which may be tolerable in production settings to meet data sanitization requirements. When prompting LLMs, we find that GPT 4o and Claude 3.5 have similar impact with respect to redaction, while Mistral 7B demonstrates some interesting exceptions. Furthermore, this impact varies by task, and for high impact tasks, care is necessary in selecting the entities to redact. Some possible directions of future research include investigating further into specific domains of datasets such as medical or legal where PII presence is strong, experimenting with diverse redaction techniques such as pseudo-anonymization, and development of better PII redaction tools.

Figure 4: Performance of LLMs on a fraction of the dataset obtained by random vs content sampling. The trend shows that for SQuADv2.0 and GSM8k datasets, it is possible to _repair_ these datasets by removing samples that are heavily redacted. Interestingly, DROP does not follow this trend. We hypothesize this to be due to the _uniformly diverse_ PII present content there, ensuring that simply by removing samples based on the count does not ensure performance improvement.

Figure 3: Performance of random vs content sampling with replacement for all high-impact datasets. The trend shows that randomly redacting a portion of the dataset leads to a linear drop in performance, whereas by redacting samples based on the PII content leads to a non-linear drop. This non-linearity trend is more prominent for GSM8k and BBH datasets, while less for DROP dataset. We hypothesize the reason to be related to a more uniform distribution of PII content in DROP dataset, thereby making the sampling methods equivalent.