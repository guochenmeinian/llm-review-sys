# Autoformalizing Mathematical Statements by

Symbolic Equivalence and Semantic Consistency

 Zenan Li\({}^{1}\)1 Yifan Wu\({}^{2}\)2 Zhaoyu Li\({}^{3}\) Xinming Wei\({}^{2}\)

**Fan Yang\({}^{4}\) Xian Zhang\({}^{4}\) Xiaoxing Ma\({}^{1}\)**

\({}^{1}\)State Key Lab of Novel Software Technology, Nanjing University, China

\({}^{2}\)Peking University, \({}^{3}\)University of Toronto

\({}^{4}\)Microsoft Research Asia

lizn@smail.nju.edu.cn, yifan.wu@stu.pku.edu.cn,

zhxian@microsoft.com, xxm@nju.edu.cn

Equal contribution. This work was partially done during Zenan's and Yifan's internships at MSRA.

###### Abstract

Autoformalization, the task of automatically translating natural language descriptions into a formal language, poses a significant challenge across various domains, especially in mathematics. Recent advancements in large language models (LLMs) have unveiled their promising capabilities to formalize even competition-level math problems. However, we observe a considerable discrepancy between pass@1 and pass@\(k\) accuracies in LLM-generated formalizations. To address this gap, we introduce a novel framework that scores and selects the best result from \(k\) auto-formalization candidates based on two complementary self-consistency methods: _symbolic equivalence_ and _semantic consistency_. Elaborately, symbolic equivalence identifies the logical homogeneity among autoformalization candidates using automated theorem provers, and semantic consistency evaluates the preservation of the original meaning by normalizing the candidates and computing the similarity between the embeddings of the original and informalized texts. Our extensive experiments on the MATH and miniF2F datasets demonstrate that our approach significantly enhances autoformalization accuracy, achieving up to 0.22-1.35x relative improvements across various LLMs and baseline methods. The data and code are available at https://github.com/Miracle-Messi/Isa-AutoFormal

## 1 Introduction

Autoformalization is the automated process of translating from natural language expressions into a formal language [1; 2; 3; 4]. Successful autoformalization can alleviate the demand for extensive human expertise and reduce the substantial manual formalization efforts [5; 6; 7; 8], as well as fundamentally bridging the gap between natural (or so-called "informal") and formal languages , which potentially catalyzes breakthroughs in many fields such as mathematical theorem proving, software/hardware verification, and autonomous planning [10; 11; 12; 13; 14]. Despite decades of research, the practical application of autoformalization remains limited because traditional methods often necessitate either predefined domain-specific languages or hard-coded translation rules [15; 16; 17; 18].

Recently, large language models (LLMs) have shown promising performance in autoformalization, especially in formalizing mathematical statements [19; 20; 21]. For instance, using Codex  with few shot examples can achieve a 25.3% success rate in formalizing high-school level problems from the MATH  dataset. Nevertheless, the autoformalization capability of LLMs has not been fully exploited. As shown in Figure 1, even advanced LLMs like GPT-4  struggle with translating a

[MISSING_PAGE_EMPTY:2]

can significantly reduce the manual effort required for verifying or labeling formalization results, efficiently minimizing human intervention in correcting and validating outputs. Additionally, we extend our experiments to five proprietary or open-source LLMs, showing the consistent effectiveness of the proposed methods.

In summary, this paper makes the following main contributions: (1) identifying the performance gap between pass@1 and pass@_k_ for LLMs in autoformalization tasks; (2) introducing two self-consistency methods, symbolic equivalence and semantic consistency, and three combination strategies to enhance LLM autoformalization performance; (3) providing extensive experiments across various model sizes on two popular datasets, confirming the efficacy of the proposed approach.

## 2 Background and Related Work

Formal mathematics.Formal mathematics aims to establish a rigorous framework to express mathematical theorems and proofs in a format that can be verified by a computer through the application of logical rules. Interactive theorem provers, such as Isabelle/HOL , Coq , and Lean , provide environments for encoding and verifying mathematical proofs programmatically. For decades, researchers have used these tools to manually formalize a range of challenging mathematical concepts and theorems . However, translating mathematics into a language that theorem provers can interpret often requires a deep understanding of both the mathematics involved and the syntax of the target formal language. Therefore, the formalization process is always labor-intensive even for large groups of experts, creating a significant bottleneck in this field.

Autoformalization with LLMs.To mitigate the laborious process of manual formalization, recent advances have explored the potential of LLMs in autoformalization . A stream of research focuses on autoformalizing mathematical statements . For instance, FIMO  employs GPT-4 with reflection to formalize problems from the International Mathematical Olympiad. ProofGPT  and MMA  train LLMs on large-scale datasets with both informal and formal mathematical data to evaluate their performance on statement autoformalization. Concurrently, another research direction investigates the autoformalization of mathematical proofs . For example, DSP  utilizes LLMs to draft informal proofs and map them into formal sketches, with automated theorem provers employed to fill in the missing details in the proof sketch. Besides these efforts, several studies  explore the performance of LLMs for the inverse process of formalization, i.e., informalization, which translates formal statements back into natural language.

Self-consistency for LLMs.Self-consistency was originally proposed to boost the mathematical reasoning capability of LLMs . This approach aims to identify the homogeneity among multiple generations, thereby bridging the performance disparity between pass@1 and pass@_k_. In contrast to other techniques, such as training an additional verifier/re-ranker , or directly fine-tuning the model , self-consistency is entirely data-free, making it readily implementable with off-the-shelf LLMs without incurring the so-called "alignment tax" associated with additional computational costs . Recently, self-consistency has been further adapted to code generation, which closely resembles autoformalization since they both involve formalizing natural language statements. However, in code generation, self-consistency for LLMs typically relies on the execution information from test cases , e.g., whether the two programs produce the same output for identical test inputs. Therefore, this strategy is not applicable to autoformalization due to the absence of test cases for mathematical statements.

## 3 Methodology

Our framework, as illustrated in Figure 3, comprises four steps to enhance the autoformalization process of LLMs. Initially, LLMs generate \(k\) autoformalization candidates for a given mathematical statement in natural language. Subsequently, our framework establishes the symbolic equivalence among these candidates and assigns a symbolic score to each based on the derived equivalence classes. Each formal statement is then re-informalized using LLMs, and the semantic score is computed by comparing the embeddings of the re-informalized text and the original statement. Finally, our framework normalizes and combines these scores to rank the autoformalization candidates and determine the final formalization results.

### Symbolic Equivalence

We first instantiate self-consistency as symbolic equivalence among autoformalization candidates. The rationale behind the symbolic equivalence is straightforward: _correct formalizations are logically equivalent, even when expressed with varied symbols._ To establish symbolic equivalence, we decompose formal statements into their premises and conclusions. Symbolic equivalence between two statements is then defined by the logical equivalence of both their premises and conclusions.

The formal definition of symbolic equivalence is presented in the following. Within this definition, we further assume that the premises are not intrinsically contradictory, ensuring that the two involved mathematical statements are well-defined.

**Definition 1** (Symbolic equivalence): _Let two mathematical statements \(_{1}\) and \(_{2}\) in formal language be expressed as \(_{1}_{1}\) and \(_{2}_{2}\), and suppose their premises \(_{1}\) and \(_{2}\) are not tautologies. Then, the two statements are called symbolically equivalent if the two logical equivalences, i.e., \(_{1}_{2}\) and \(_{1}_{2}\), both hold._

The two logical equivalences induced by the symbolic equivalence can be determined through existing automated theorem provers (ATPs). Additionally, the validity of the premises can also be checked by replacing the conclusion \(\) with a contradictory result (e.g., \(0=1\)) and verifying \(\). If this vacuous form can be proved, then the corresponding premise \(\) is a contradiction.

It is also worth noting that variable misalignment between two statements remains a challenge for validating symbolic equivalence using ATPs. For instance, the examples in Figure 1 (No.2 and No.3) can not be proved symbolically equivalent due to the inconsistent variable declarations. Therefore, we should perform variable matching beforehand, ensuring that the symbolic equivalence can be well recognized even in these cases.

Nevertheless, exhaustively checking all possible variable mappings is always impractical due to the combinatorial explosion. For two statements expressed as \(_{1}(x_{1},,x_{n})_{1}(x_{1},,x_{n})\) and \(_{2}(y_{1},,y_{n})_{2}(y_{1},,y_{n})\), each has \(n\) variables, there are \(n!\) possible bijective mappings to be checked, which is excessively time-consuming when \(n\) is large. To address this issue, we propose to standardize the formal statement \((x_{1},,x_{n})(x_{1},,x_{n})\) by the following two cases:

(1) If the conclusion is in the form of a numerical relation, i.e., \((x_{1},,x_{n}):=f(x_{1},,x_{n}) 0\), where \(f\) represents any function and \(\{,_{\_}{<},>,=,\}\), we introduce a new variable \(\) and derive the standard format \(}(;x_{1},,x_{n})}()\) with

\[}(;x_{1},,x_{n}):=(x_{1},,x_{n} )(=f(x_{1},,x_{n})),}():=  0.\]

The two statements are reduced to \(}_{1}(;x_{1},,x_{n})}_{1}()\) and \(}_{2}(;y_{1},,y_{n})}_{2}()\), and thus the logical equivalences \(}_{1}()}_{2}()\) and \(}_{1}()}_{2}()\) can be checked through leaving \(x_{1},,x_{n}\) and \(y_{1},,y_{n}\) as auxiliary variables.

(2) For non-numerical cases (e.g., \((x):=(x)\)), we have to conduct a variable alignment. Instead of enumerating all variable mappings, we view the variables in each statement as a set of graph vertices, and thus the variable alignment is transformed into a bipartite matching task (where

Figure 3: The overview of our autoformalization framework. In the framework, symbolic equivalence is constructed among formalized statements, and semantic consistency is computed between the informalized statements and the original statement. The scores from these two evaluations are combined to rank and select the final formalization results.

\((x_{1},,x_{n})\) and \((y_{1},,y_{n})\) form two disjoint and independent vertex sets) [59; 60]. Furthermore, we simply set the edge weight of the graph by the string edit distance [61; 62], and only partially enumerate variable mappings corresponding top-\(k\) maximum bipartite matching.

To further clarify, we provide an example for each case in Appendix C. For evaluating symbolic equivalence, we assign the symbolic score to each formalization using the proportion of its corresponding equivalence class, which is also commonly used in mathematical reasoning and code generation.

### Semantic Consistency

Next, the self-consistency is instantiated as the semantic consistency between the formalization and its corresponding informal version. The rationale of the semantic consistency is also clear: _An autoformalization result is accurate if it can be re-informalized to a version consistent with the original statement in natural language._ By introducing the embedding similarity [63; 64] to measure the consistency between the original text and the twice-processed (autoformalized then informalized) version, we can define the \(\)-semantic consistency as follows.

**Definition 2** (Semantic consistency): _Let the original mathematical statement in natural language and its formalization candidate be \(\) and \(\), and suppose that \(\) is further informalized into a new natural language statement \(\). Then, the formal statement \(\) is \(\)-semantically consistent with the original statement \(\) if the embedding similarity between \(\) and \(\) satisfies \((,)\)._

Semantic consistency primarily measures the error incurred in both the formalization and informalization processes. However, in most cases, it can be approximately reduced to measuring the error in autoformalization. This is because informalization is much easier and accurate than formalization . For instance, considering the formalization of the statement "Determine the range of \(e^{2}\)", LLMs should inference the type of the exponential \(e\), determining whether to use power or \(\) as the grounding of 'power'. On the contrary, these two expressions are both translated back into the term 'power' during informalization.

Compared to symbolic equivalence, semantic consistency can avoid the _unintended reasoning_ problem. Elaborately, continuing the example in Figure 1, the correct formalization "\((x=2/3 y=6) x*y=4\)" is identified as symbolically equivalent to the formalization "\(4=4\)", while the latter is trivial and unexpected. However, the difference between these formalizations can be successfully recognized by semantic consistency since the latter ruins the semantics in the informal statement.

Following existing machine translation techniques [65; 66; 67], we employ the BERT model  to generate embeddings for the informal statements. These embeddings are then compared using cosine similarity to evaluate semantic consistency.

### Combination of Two Scores

Given \(k\) autoformalization candidates, and denote their scores of symbolic equivalence and semantic consistency by \(s_{1}^{},,s_{k}^{}\) and \(s_{1}^{},,s_{k}^{}\), respectively. We first normalize them using the softmax function, i.e., \(_{i}^{}=s_{i}^{}/{_{j=1}^{k}s_{j}^{ }}\) and \(_{i}^{}=s_{i}^{}/{_{j=1}^{k}s_{j}^{ }}\) for \(i=1,,k\). Then, we propose three strategies, i.e., log, linear, and quadratic, for the combination of two scores. In particular, the final score \(_{i}\) of the \(i\)-th autoformalization candidate is computed by

\[&_{i}=_{i }^{}+(1-)_{i}^{},\\ &_{i}=_{i}^{}+(1- )_{i}^{},\\ &_{i}=(_{i}^{})^{2 }+(1-)(_{i}^{})^{2},\]

where \(\) is the hyperparameter controlling the trade-off between the symbolic equivalence and the semantic consistency, which practically can be tuned based on the validation set.

The overall procedure of our autoformalization framework is presented in Algorithm 1. The primary efficiency bottleneck of the algorithm lies in verifying symbolic equivalence. In the worst case, where no pair of autoformalization candidates are symbolically equivalent, and symbolic equivalence must be exhaustively validated \(k(k-1)/2\) times. However, many verifications of symbolic equivalence can be bypassed by leveraging the transitivity property of symbolic equivalence. Moreover, when requested to provide \(n\) formalization results, we iteratively conduct the algorithm to rank the formal statements, with the selected result and formal statements in its equivalence class removed.

## 4 Evaluation

In this section, we conduct a series of experiments to answer the following four research questions:

**RQ1: Efficacy** - Compared with baselines and alternatives, do our proposed methods (symbolic equivalence and semantic consistency) achieve better autoformalization performance?

**RQ2: Synergy** - Are symbolic equivalence and semantic consistency mutually complementary? Does the combination of them further boost the autoformalization performance?

**RQ3: Labeling-efficiency** - How much human effort in verifying or labeling the formalization results can be saved using our proposed methods?

**RQ4: Scalability** - Can our proposed methods be further enhanced by using stronger LLMs or ATPs?

### Experimental Setup

**Dataset.** We evaluate the proposed methods on the MATH  and miniF2F  datasets, both of which encompass a wide range of mathematical problems designed for different levels of complexity and abstraction. The MATH dataset includes a variety of problem types, e.g., Algebra, Number Theory, Geometry, and so on. We randomly select a subset of 400 problems from the dataset to serve as our benchmark. The miniF2F dataset is specifically curated for evaluating LLM abilities in autoformalization and mathematical reasoning. It contains 488 Olympiad-level mathematical problems, each equipped with a formal statement as an oracle in Isabelle and Lean.

**Model.** We carry out the experiments on five proprietary and open-source models of varying parameter sizes, including Mistral-7B , Lemma-34B , DeepSeek-v2 , Codex (completion api) , and GPT-4 (version 0710) . In addition, we employ few-shot prompting, and set the temperature of the generation process to \(0.7\) for all LLMs. The eight examples used, along with detailed prompts for autoformalization and informalization, are provided in Appendix F.

**Metric.** We use the unbiased _n@k_ accuracy (with \(k\) generations) for performance evaluation, i.e., the percentage of problems for which the top-\(n\) formalizations of \(k\) generations can cover a correct version . We apply different policies to determine autoformalization correctness on the MATH and miniF2F datasets, respectively. Specifically, the MATH dataset does not contain aligned formalstatements, we manually check each formalization result. For the miniF2F dataset, the correctness is automatically derived by checking the symbolic equivalence between the formalization result and the provided oracle using ATPs. In the experiments, the number of generations (\(k\)) is fixed at 10, as we observe that improvements in pass@\(k\) (see Figure 2) become marginal with more generations.

**Baseline.** In our experiments, we compare our methods, symbolic equivalence (SymEq), semantic consistency (SemCo), as well as combination strategy (log-comb, linear-comb, and quad-comb), with one baseline and two alternatives. The baseline method uses the log-probability predicted by LLMs to score the \(k\) autoformalization candidates. For Codex and GPT-4, which do not provide access to log-probability, they are prompted to rank the candidates instead. We also introduce two additional methods as the alternatives, i.e., a naive strategy that filters the candidates by whether the ATPs can prove the formalization, and a clustering method that applies the adaptive \(k\)-means algorithm  on BERT embeddings of formal statements.

**Implementation.** For the SymEq method, we implement an equivalence checker as well as peripheral logic based on scala-isabelle . Specifically, the equivalence checker integrates 12 tactics (i.e., auto, simp, eval, smt, blast, fastforce, force, arith, linarith, presburger, (auto simp:field_simps), and sledgehammer[timeout=300s]) provided in Isabelle/HOL , as well as two SMT solvers Z3  and CVC5 . For the SemCo method, we use the pretrained BERT  to compute the embedding of the informal statement.

### Empirical Results

**RQ1: Efficacy.** We compute _n@k_ results for \(n=1,2,3\) and five LLMs on the two datasets. As shown in Table 1, SymEq demonstrates superior performance on all cases. For Codex and GPT-4, SymEq achieves the best _n@k_ accuracy with 46.1% (Codex) at \(n=1\) on the MATH dataset, and 41.1% (GPT-4) at \(n=1\) on the miniF2F dataset. For the two smaller LLMs Mistral-7B and Llemma-34B, SymEq also exhibits a notable improvement in 1@\(k\) accuracy, surpassing the competitors by at least 4.9%. As for the recently released LLM DeepSeek-v2, SymEq is still effective, resulting in 1@\(k\) improvements of 5.1% and 0.8% on the two datasets, respectively.

For SemCo, it successfully achieves the best performance in three cases (3@\(k\) of Mistral-7B, 2@\(k\) of Llemma-34B, and 3@\(k\) of GPT-4) on the MATH dataset. Compared to the baseline, SemCo also performs an improvement, ranging from 0.8% to 3.9% in 1@\(k\) accuracy. Compared with the alternatives, SemCo is still slightly more effective, wining 4 out of 5 cases (except for GPT-4) in 1@\(k\) accuracy. On the miniF2F dataset, although the improvement of SemCo is narrower, it still wins the alternatives 4 out of 5 cases, and achieves equal results for the rest (DeepSeek-v2) in 1@\(k\) accuracy. However, SemCo is much less effective than SymEq in most cases, as it does not grasp the logical nature of formal statements.

**RQ2: Synergy.** We first conduct a detailed analysis of the performance of SymEq and SemCo across different categories of the MATH dataset. The results presented in Table 2 reveal an interesting finding: SymEq and SemCo demonstrate distinctly different performances for each category. This

variation likely stems from the differing nature and requirements of autoformalizing problems in different categories. For example, geometry problems are more sensitive to semantic consistency since they often involve the translation of visual images, while number theory problems pose greater challenges for checking symbolic equivalence.

Therefore, combining SymEq and SemCo (i.e., log-, linear-, and quad-comb) to further improve autoformalization accuracy is reasonable. We explore the optimal setting for the hyperparameter \(\). In particular, we compute the 1@\(k\) results of log-comb for various values of \(\) on the MATH dataset, and plot the performance curve in Figure 4. The results demonstrate that the combination strategy can further improve autoformalization accuracy, with a large sweet spot for \(\) (\(0.32-0.6\)).

The performance curves for the other two combination strategies (linear-comb and quad-comb) are provided in Appendix D. We observe that log-comb is more effective and stable than the other two strategies. Furthermore, we fix \(=0.5\) based on the performance curve and present the overall performance (\(n@k\)) of log-comb in Figure 5. The results show that log-comb consistently improves autoformalization accuracy across various LLMs on the two datasets, by ranging from 2.3% to 22.6%. Particularly, compared to SymEq, even for the most powerful model GPT-4, log-comb can still further boost the 1@\(k\) accuracy by 3.3% on the MATH dataset and by 2.5% on the miniF2F dataset. For the Mistral-7B, log-comb presents significant improvements, i.e., 22.6% and 10.5%, respectively.

**RQ3: Labeling-efficiency.** We define average labeling cost for given \(k\) autoformalization candidates: \(=_{n=2}^{k-1}n(n@k-(n-1)@k)+(1-k@k)\). Based on the average labeling cost \(\), for \(N\) mathematical problems, the total number of formal statements to be labeled can be computed by \( N\). Subsequently, we introduce the relative efficiency of two methods as \(E=1-/\). By using the baseline as a reference (\(\)), we compute the relative efficiency of each method in Table 3.

It can be observed that our methods, especially log-comb, achieve higher labeling-efficiency compared to the alternatives. On the MATH dataset, log-comb achieves the relative efficiency ranging from 17.7% to 21.6%, with up to three times improvement (on Mistral-7B) than the Cluster method. On the miniF2F dataset, log-comb is still very efficient, by using GPT-4, the relative efficiency achieves 21.9%, outperforming Cluster by 9.2%, SymEq by 3.6%, and SemCo by 5.4%, respectively.

   Category & \#Probs & SymEq & SemCo \\  Algebra & 102 & 57.8 & **59.8** \\ Counting and Probability & 46 & **36.9** & 30.3 \\ Geometry & 32 & **28.1** & 25.1 \\ Intermediate Algebra & 77 & **31.1** & 25.9 \\ Number Theory & 42 & 33.3 & **38.0** \\ Prealgebra & 62 & **51.6** & 40.3 \\ Precalculus & 39 & 33.3 & **35.8** \\   

Table 2: Performance (1@\(k\)) of our methods (SymEq and SemCo) across various categories from MATH dataset. The formalization results are generated by GPT-4, and the best performance is in bold. The results show that SymEq and SemCo exhibit different behaviors on various categories.

Figure 4: Performance curve of log-comb for different values of \(\). The formalization results are generated by GPT-4. The results show that the combination can further improve the autoformalization accuracy with a large sweet spot.

Figure 5: The performance of our proposed combination strategy (log-comb) on the MATH (left) and miniF2F (right) datasets. The results show that the log-comb further boost the autoformalization performance across various LLMs on the two datasets.

**RQ4: Scalability.** As illustrated by our experimental results in Table 1 and Figure 5, a more powerful LLM, such as GPT-4, often exhibits better autoformalization performance. We provide an additional evidence by examining the performance differences across various difficulty levels in the MATH dataset. The results shown in Table 4 reveal a significant gap in autoformalization performance between Levels 1-3 and Levels 4-5. Hence, the difficulty of the problem is highly correlated with the autoformalizaiton performance, suggesting that an LLM with stronger mathematical reasoning capabilities is more effective in this task.

To investigate the impact of ATP capability, we conduct an additional ablation study. Specifically, we build a limited equivalence checker, in which only two tactics in Isabelle/HOL (auto and simp) are reserved and the other tactics and SMT solvers are removed. The results are shown in Figure 6, which illustrate that the performance improvement of SymEq is minimal when using ATPs with stronger capability. One possible reason is that, although normal ATPs can prove more symbolic equivalences (2.13 vs. 2.33 per problem on average) than the limited version, this is still not enough to have a major impact on the final symbolic equivalence score.

## 5 Conclusion

In this paper, we present a new framework for improving the autoformalization performance of LLMs. Our techniques address the inherent challenges in autoformalization by overcoming the limitations of traditional self-consistency methods, which struggle to cope with the variance in LLM outputs. Specifically, our framework achieves this goal by combining symbolic equivalence, which grasps the logical nature among formal statements, with semantic consistency, which inspects the semantic coherence between the re-normalization result and the original text. Empirical evaluation on the MATH and miniF2F datasets demonstrates a new level of autoformalization accuracy. Furthermore, our quantitative and case analysis elaborates on the limitations of current LLMs and automatic theorem provers in the task of autoformalization, shedding light on directions for future optimization.

The future directions for our proposed framework could include: (1) Method Adaptation: Extending the framework to support additional theorem provers, such as Lean 4; (2) Model Enhancement: Integrating more advanced or specifically fine-tuned LLMs like ProofGPT  and MMA  to further enhance the framework's performance; (3) Data Synthesis: Generating higher-quality, aligned informal and formal datasets using the framework. A detailed discussion of the limitations and broader impacts can be found in Appendix A and Appendix B.

   Diff.\({}^{}\) & Baseline & SymEq & SemCo & Log-comb \\ 
1 (37) & 64.8 & 67.5 & 64.8 & 75.6 \\
2 (70) & 44.2 & 47.1 & 47.1 & 52.8 \\
3 (91) & 48.3 & 57.1 & 47.2 & 53.8 \\
4 (91) & 26.3 & 34.0 & 31.8 & 40.6 \\
5 (111) & 24.3 & 24.3 & 26.1 & 27.0 \\   

* \(a(b)\) refers to the difficulty level (# problems).

Table 4: Performance (1@\(k\)) across various difficult levels from the MATH dataset, with formalization results generated by GPT-4. The results indicate that the performance improvement is very narrow by increasing the capability of ATPs.

   Methods &  &  &  &  &  \\  Dataset & MATH & miniF2F & MATH & miniF2F & MATH & miniF2F & MATH & miniF2F & MATH & miniF2F \\  Mistral-7B & -14.2 & 1.5 & 5.6 & 2.6 & 12.9 & 6.9 & 12.6 & 4.6 & **21.6** & **8.4** \\ Lemma-3dB & -14.2 & -4.1 & 10.4 & 4.6 & 15.2 & 15.8 & 14.3 & 8.4 & **18.9** & **19.5** \\ DeepSeek-v2 & -30.9 & -6.9 & 17.2 & 8.3 & 18.7 & 8.0 & 16.3 & 6.8 & **20.5** & **10.0** \\ Codex & -31.4 & -5.1 & 13.7 & 10.4 & 15.3 & 9.6 & 13.5 & 13.6 & **19.9** & **15.3** \\ GPT-4 & -16.9 & -7.0 & 15.6 & 12.7 & 16.3 & 18.3 & 14.7 & 16.5 & **17.7** & **21.9** \\   

Table 3: Relative efficiency (%) of our methods (SymEq, SemCo, and Log-comb) and alternatives (Naive, and Cluster) on MATH and miniF2F datasets. The best performance is in bold. Note that the negative results achieved by Naive are reasonable since it is less effective compared to the baseline. The results show that our proposed methods exhibit higher efficiency enhancement.

Figure 6: Performance of SymEq using different ATP settings, with formalization results generated by GPT-4. The results indicate that the performance improvement is very narrow by increasing the capability of ATPs.