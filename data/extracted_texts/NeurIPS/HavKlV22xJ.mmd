# Model-free Low-Rank Reinforcement Learning

via Leveraged Entry-wise Matrix Estimation

 Stefan Stojanovic

KTH, Stockholm, Sweden

stesto@kth.se

&Yassir Jedra

MIT, Cambridge, USA

jedra@mit.edu

&Alexandre Proutiere

KTH, Digital Futures, Stockholm, Sweden

alepro@kth.se

###### Abstract

We consider the problem of learning an \(\)-optimal policy in controlled dynamical systems with low-rank latent structure. For this problem, we present LoRa-PI (Low-Rank Policy Iteration), a model-free learning algorithm alternating between policy improvement and policy evaluation steps. In the latter, the algorithm estimates the low-rank matrix corresponding to the (state, action) value function of the current policy using the following two-phase procedure. The entries of the matrix are first sampled uniformly at random to estimate, via a spectral method, the _leverage scores_ of its rows and columns. These scores are then used to extract a few important rows and columns whose entries are further sampled. The algorithm exploits these new samples to complete the matrix estimation using a CUR-like method. For this leveraged matrix estimation procedure, we establish entry-wise guarantees that remarkably, do not depend on the coherence of the matrix but only on its spikiness. These guarantees imply that LoRa-PI learns an \(\)-optimal policy using \(((1-)^{2}})\) samples where \(S\) (resp. \(A\)) denotes the number of states (resp. actions) and \(\) the discount factor. Our algorithm achieves this order-optimal (in \(S\), \(A\) and \(\)) sample complexity under milder conditions than those assumed in previously proposed approaches.

## 1 Introduction

Reinforcement Learning (RL) methods when applied to dynamical systems with large state and action spaces suffer from the curse of dimensionality. For example, learning an \(\)-optimal policy in tabular discounted Markov Decision Processes (MDPs) with \(S\) states and \(A\) actions requires a number of samples scaling at least as \(^{2}}\). Fortunately, many real-world systems exhibit a latent structure that if learnt and exploited could drastically improve the statistical efficiency of RL methods . In this paper, we are interested in developing methods to leverage low-rank latent structures. These structures have attracted a lot of attention recently, see e.g. . Here, we consider a structure where the (state, action) value functions of policies, viewed as \(S A\) matrices, are low-rank. This structure has been empirically motivated and studied in . The hope is that when exploiting it optimally, learning an \(\)-optimal policy would only require \(O(^{2}})\) samples. Such an improvement would also imply significant statistical gains in MDPs with continuous state and action spaces. If these spaces are of dimensions \(d_{1}\) and \(d_{2}\), under natural smoothness conditions and using an appropriate discretization , the sample complexity would indeed be reduced from \(+d_{2}+2}}\) (without structure) to \(,d_{2})+2}}\).

In this paper, we present LoRa-PI (Low Rank Policy Iteration), a model-free algorithm that learns and exploits an initially hidden low-rank structure in MDPs. Unlike existing algorithms, LoRa-PI does not require any prior information on the structure. Yet, the algorithm offers the promised statistical gains: its sample complexity essentially exhibits an order-optimal dependence in \(S\), \(A\) and \(\) (i.e., \(}\)).

**Contributions.** Our algorithm LoRa-PI relies on approximate policy iteration . As such, it alternates between policy evaluation and policy improvement steps. The design and performance analysis of these two steps constitute our main contributions.

_1. Leveraged matrix estimation with entry-wise guarantees._LoRa-PI sequentially updates a candidate policy whose (state, action) value function has to be estimated. This function can be seen as an \(S A\) matrix that we consider to be low rank. The policy evaluation step then boils down to a novel low-rank matrix estimation procedure. We have two main constraints for this procedure. (i) To be sample efficient, the matrix should be estimated from noisy observations of only a few of its entries. (ii) For RL purposes (when integrated to LoRa-PI), the procedure should offer entry-wise performance guarantees. We present LME (Leveraged Matrix Estimation), a low-rank matrix estimation algorithm that meets these constraints. LME does not require knowledge of a priori unknown parameters of the matrix (such as its rank, condition number, spikiness, or coherence), and it is the first algorithm enjoying non-vacuous entry-wise guarantees even for coherent matrices.

More precisely, LME guarantees an entry-wise estimation error within \(\) using only \((^{4}^{2}}{(1-)^{ 3}^{2}})\) samples, where \(\) and \(\) denote the spikiness and the condition number of the matrix, respectively. Note that in particular, this sample complexity does not depend on the coherence of the matrix. Its dependence in \(S\), \(A\) and \(\) cannot be improved. To reach this level of performance, LME relies on an adaptive sampling strategy. It first estimates, via a spectral method, the so-called _leverage scores_ of the matrix. These scores quantify the amount of information about the matrix available in the different rows and columns. The algorithm then exploits the leverage scores to adapt its strategy and in turn, drive the sampling process towards more informative entries.

_2. Design and sample complexity of_LoRa-PI. Our RL algorithm LoRa-PI is a policy iteration algorithm that relies on LME to perform policy evaluation steps. The algorithm inherits the advantages of LME. In contrast to existing algorithms, it is parameter-free and its performance can be analyzed and guaranteed under mild assumptions on the (state, actions) value functions. In particular, the corresponding low-rank matrices do not need to be incoherent. We establish that LoRa-PI learns an \(\)-optimal policy using \((^{4}^{2}}{(1-)^{ 5}^{2}})\) samples, where \(\) and \(\) are upper bounds on the spikiness and the condition number of the (state, action) value functions.

_3. Numerical experiments._ We illustrate numerically the performance of our algorithms, LME and LoRa-PI, using synthetically generated low-rank MDPs. The experiments are presented in Appendix A due to space constraints.

Notation.We denote the Euclidean norm of a vector \(x\) by \(\|x\|_{2}\). Let \(M\) be an \(m n\) matrix. We we denote its \(i\)-th row (resp. \(j\)-th column) by \(M_{i,}\) (resp. by \(M_{,j}\)). We denote its operator norm by \(\|M\|_{}\), it Frobenius norm by \(\|M\|_{}\), its infinity norm by \(\|M\|_{}=_{i[m],j[n]}|M_{ij}|\), and its two-to-infinity norm by \(\|M\|_{2}=_{i m}\|M_{i,}\|_{2}\). We denote by \(M^{}\) the Moore-Penrose inverse of \(M\). For given subsets \([m]\), \([n]\), we denote by \(M_{,}\) the sub-matrix whose entries are \(\{M_{ij}:(i,j)\}\). Finally, we use \(a b=(a,b)\) and \(a b=(a,b)\).

 
**Method** & **Err. Guarantees** & **Sampling** & **Assumption** & **Complexity** \\   LME (ours) & entry-wise & adaptive & bounded spikiness & \(^{2}(S+A)/^{2}\) \\ Algorithm  & entry-wise & apriori fixed anchors & anchors apriori known & \(^{2}(S+A)/^{2}\) \\ LR-EVI (Thm 9 ) & entry-wise & unif. anchors & incoherence & \(^{2}^{2}(S+A)/^{2}\) \\ NNM  (Thm 21 ) & entry-wise & unif. anchors & incoherence & \(^{2}^{2}(S+A)/^{2}\) \\ Two-phase MC  & exact recovery & adaptive & noiseless & no applicable \\  

Table 1: Comparison of methods with entry-wise guarantees. For brevity, the factors \((1-)^{-1},\) and \(d\) are omitted. NNM: nuclear norm minimization, MC: matrix completion.

Related Work

**Low-rank MDPs.** MDPs with low-rank latent structure have been extensively studied recently. We may categorize these studies according to the type of the underlying low-rank structure and to the nature of the algorithms used to learn this structure.

The most studied low-rank structure concerns MDPs whose transition kernels and the expected reward functions are low-rank. For instance, it is assumed that the transition probabilities can be written as \(p(s^{}|s,a)=(s,a)^{}(s^{})\), where \((s,a)\) and \((s^{})\) are \(d\)-dimensional feature maps . These work additionally assume that the feature map \(\) (and similarly for \(\)) belongs to a rich function class \(\). In this setting, the typical upper bounds derived for the sample complexity of identifying an \(\)-optimal policy scale as \((A,(1-)^{-1})|}{^{2}}\). When no restrictions are imposed on the class \(\), one can find a low-rank structure such that \(||\) scales as the number \(S\) of states . In this case, the aforementioned upper bounds are the same those for MDPs without structure. We also note that most algorithms using this framework rely on strong computational oracles (e.g., empirical risk minimizers, maximum likelihood estimators), see  for detailed discussions. In this paper, we do not limit our analysis to low-rank structures based on a given restricted class of functions, and our algorithms do not rely on any kind of oracle.

The low-rank structure we consider is similar to that in  and just assumes that the (state, action) value functions are low-rank. Actually,  considers the case where only the optimal Q-function is low-rank, say of rank \(d\). As shown in , such a structure naturally arises when discretizing smooth MDPs with continuous state and action spaces. In both papers , the authors devise algorithms with a minimax-optimal sample complexity to identify an \(\)-optimal policy roughly scaling as \((S+A)/^{2}\). But the analysis presented in  suffers from the following important limitations. 1. First, it is assumed that the learner is aware of a set \(\) (resp. \(\)) of so-called anchors states (resp. actions), such that the rank of the matrix \(Q_{,}:=(Q(s,a))_{(s,a)}\) is the same as that of the entire matrix \(Q\). Such anchors are however initially unknown (since \(Q\) is unknown). Importantly, the proposed RL algorithms rely on a low-rank matrix estimation procedure whose performance strongly depends on the smallest singular value \(_{d}(Q_{,})\) of \(Q_{,}\). The authors circumvent this difficulty by actually parametrizing their algorithms using \(_{d}(Q_{,})\). But again, the latter is unknown, and it remains unclear how one can avoid this issue. 2. The second limitation is that the analysis is valid for small values of the discount factor \(\) (the authors need to impose an upper bound on \(/_{d}(Q_{,})\)). When \(_{d}(Q_{,})\) is small, the analysis is limited to very short horizons. Note that in addition,  assumes that the collected rewards are deterministic, which together with the short horizon issue, greatly simplifies the learning problem.

To address the first limitation, the authors of  propose to sample rows and columns uniformly at random to get anchors. This solution requires to sample at least \(d^{2}\) states and actions (Lemma 10 in ) where \(\) is the (unknown) coherence of the matrix to be estimated. Hence this essentially amounts to sampling almost the whole matrix for coherent matrices. The authors of  also propose a solution to the second limitation, but at the expense of imposing additional restrictive conditions. In this paper, we address both limitations and devise RL algorithms that rely on a new low-rank matrix estimation procedure that works without imposing the incoherence of the matrix and that does not require knowledge on a priori unknown parameters of this matrix.

**Low-rank matrix estimation with entry-wise guarantees.** Until recently, most results on low-rank matrix recovery concerned guarantees with respect to the spectral or Frobenius norms, see e.g.  and references therein. Over the past few years, methods to derive entry-wise guarantees have been developed. These include spectral approaches , nuclear-norm penalization and convex optimization techniques , CUR-based (or Nystrom-like) methods .

The aforementioned literature provides guarantees not for all low-rank matrices, but for those typically enjoying additional structural properties such as incoherence. Relaxing the incoherence assumption is not easy, but can be achieved using adaptive sampling . As far as we are aware, all results applicable to somewhat coherent matrices provide guarantees with respect to the spectral or Frobenius norms. In this paper, we develop a first adaptive matrix estimation method with provable entry-wise guarantees, valid for matrices with well-defined spikiness but not necessarily incoherent. Refer to  and to SS3.2 for a detailed discussion about the notion of spikiness.

Preliminaries

### Low-rank Markov Decision Processes

We consider a discounted MDP with finite state and action spaces \(\) and \(\). These spaces are of cardinality \(S\) and \(A\), respectively. The dynamics are described by the transition kernel \(p\) where \(p(s^{}|s,a)\) denotes the probability to move to state \(s^{}\) given current state \(s\) and that the action \(a\) is selected. The collected rewards are random but bounded by \(r_{}\), and \(r(s,a)\) is the expected reward collected when action \(a\) is selected in state \(s\). A deterministic Markovian policy \(\) is described by a mapping from \(\) to \(\). We denote by \(V^{}\) the state value function of \(\): for all \(s\), \(V^{}(s)=[_{t=0}^{}^{t}r(s_{t}^{},a_{t}^{} )|s_{0}^{}=s]\), where \(s_{t}^{}\) and \(a_{t}^{}\) are, at time \(t\), the state and the action selected under \(\). Similarly, the (state, action) value function of \(\) is defined by: for all \((s,a)\), \(Q^{}(s,a)=r(s,a)+_{s^{}}p(s^{}|s,a)V^{}(s^{})\). \(Q^{}\) can be seen as a \(S A\) matrix, referred to as the _value matrix of \(\)_ in the remainder of the paper. Let \(_{}\) denote the condition number of \(Q^{}\). Finally, let \(V^{}\) be the value function of the MDP (the value function of the optimal policy).

The objective is to learn an \(\)-optimal policy by interacting with the MDP. Such a policy satisfies: for all \(s\), \(V^{}(s) V^{}(s)-\). Without any assumption on the structure of the MDP, to identify such a policy, the learner needs to gather, even with a generative model, a number of samples1 that scales as \((1-)^{3}}\)[17; 36]. The hope is that exploiting an a-priori known structure in the MDP may considerably accelerate the learning process. In this paper, we focus on a low-rank latent structure. Formally, we define:

**Definition 1** (Rank of a policy, rank of the MDP).: _The rank \(d_{}\) of a deterministic policy \(\) is the rank of its value matrix \(Q^{}\). The rank of an MDP is then defined as \(d=_{}d_{}\), where the maximum is over all deterministic policies._

Throughout the paper, we assume that the MDP is low-rank: its rank \(d\) satisfies \(d(S+A)\). This assumption is merely made to simplify the exposition of our results and proof techniques. As we shall argue in Appendix E, our findings can naturally be extended to MDPs that are only low-rank in an approximate and well-precised sense.

### Matrix estimation: coherence and spikiness

Our learning algorithm relies on the approximate policy iteration method, and in particular, in each iteration, it needs to estimate the low-rank value matrix of the current policy. To be sample efficient, the algorithm will estimate the matrix from the noisy observations of a few of its entries. Recovering a low-rank matrix from a few of its entries is not always possible (see e.g.  for a survey), and conditions on the degree to which information about a single entry is spread out across a matrix must be imposed. Examples of such conditions pertain to the _coherence_[6; 31] or the _spikiness_ of the matrix.

**Matrix coherence.** Let \(Q\) be a rank-\(d\)\(S A\) matrix with SVD \(U W^{}\). The coherence of \(Q\) is defined as \((Q)=\{\|U\|_{2},\|W\|_{2}\}\). \(Q\) is \(\)-coherent if \((Q)\). Low coherence means that the energy of \(U\) and \(W\) are not concentrated around a few rows and columns.

**Matrix spikiness.** The spikiness of \(Q\) is defined as \((Q)=\|Q\|_{}/\|Q\|_{}[1,]\). \(Q\) is \(\)-spiky if \((Q)\). A matrix has low spikiness if the amplitude of its maximal entry is not much larger than the average amplitude of its entries, in which case, it is intuitively easier to estimate.

Most existing guarantees for low-rank matrix estimation are expressed through the spectral or Frobenius norm of the error matrix. For this type of guarantees, the estimation error scales polynomially either with the matrix coherence or with its spikiness [12; 30]. The matrix spikiness was introduced in the matrix completion literature  to obtain guarantees under less restrictive conditions than the incoherence conditions imposed in previous work. Indeed, there are matrices with bounded spikiness but high coherence (say close to \(\), in which case the aforementioned coherence-based guarantees are vacuous). In contrast, bounded incoherence provides an upper bound on spikiness since \((Q)=\|Q\|_{}/\|Q\|_{}\|U\|_{2 }\|Q\|_{}\|W\|_{2}/\|Q\|_{}(Q)^{2}d\).

For RL purposes, we need to derive entry-wise guarantees for the estimate of the value matrix of some policy as demonstrated in [35; 37; 21]. Existing upper bounds for the entry-wise estimation error exhibit a strong dependence in the matrix coherence and its condition number, see e.g. [9; 8; 37]. For instance, in , this dependence comes as a multiplicative factor \((Q)^{2}(Q)^{2}(Q)^{2}\) in the number of samples required for a given level of estimation accuracy. As far as we are aware, our matrix estimation method is the first able to yield entry-wise guarantees that do not exhibit a dependence on the matrix coherence but only on its spikiness (see Table 1). Our algorithm is better by a factor of \((Q)^{2}\) than algorithms based on uniform sampling (studied in ), and requires the same sample complexity as the algorithm of , which has prior knowledge of anchor states. It remains however unclear whether the dependence of the entry-wise estimation error in the condition number can be avoided. This last observation guides the design of RL algorithms for low-rank MDPs as we discuss next.

### Policy vs. Value Iteration: the condition number issue

We aim at devising an algorithm learning an efficient policy with provable guarantees while imposing conditions on the MDP that are as mild as possible. To this aim, one may think of applying either a policy iteration approach, as we do, or a value iteration approach.

_Policy Iteration_. Using this approach, in each iteration, we need to estimate the low-rank value matrix of the current candidate policy. As mentioned above, the entry-wise error of this estimation procedure depends on the condition number of the matrix. Note that this matrix belongs to the finite set of (state, action) value functions of deterministic policies. As shown in [10; 42], this set can be seen as the vertices of a simple polytope \(\). Hence to get performance guarantees when applying a PI approach, it is sufficient to impose an upper bound on the condition numbers \(_{}\) for all deterministic policies \(\), or equivalently, on the condition numbers of matrices corresponding to the vertices of \(\).

_Value Iteration_. Here, we would maintain, in iteration \(t\), an estimate \(V^{(t)}\) of the value function \(V^{}\), and samples would be used to compute \(V^{(t+1)}\), an estimate of \(^{}(V^{(t)})\), where \(^{}\) denotes Bellman's operator. More precisely, starting from \(V^{(t)}\), we would estimate the low-rank matrix \(Q^{(t+1)}=(V^{(t)})\) defined by for all \((s,a)\), \((V^{(t)})(s,a)=r(s,a)+_{s^{}}p(s^{}|s,a)V^{( t)}(s^{})\). Then we would define \(V^{(t+1)}\) as the value function of the greedy policy with respect to \(Q^{(t+1)}\). Hence to get provable performance guarantees using a value iteration approach, we would need to impose an upper bound on the condition number of \(Q^{(t)}\) in all iterations \(t\). The main issue is that the set of matrices \(\{Q^{(t)},\,t 1\}\) is stochastic and hard to predict. Indeed, we have no way of confining the iterates \(Q^{(t+1)}\) to the polytope \(\): as shown in , the polytope is not stable by Bellman's operator. As a consequence, if we wish to get performance guarantees for a value iteration approach, we would need to impose an upper bound on the condition number of all possible matrices of the form \((V)\) for some vector \(V\).

In summary, policy iteration approaches offer a theoretical advantage compared to value iteration. It requires the control of the condition numbers of matrices in a set much smaller than that for value iteration. This advantage is illustrated in Figure 1 on a toy example of an MDP. Refer to Appendix A for additional numerical experiments (with larger MDPs).

## 4 Leveraged Matrix Estimation

In this section, we present Leveraged Matrix Estimation (LME), an algorithm that estimates the value matrix \(Q^{}\) of a policy \(\). The algorithm relies on an active strategy for sampling the entries of the matrix based on its estimated leverage scores as defined below. This active strategy accelerates the learning process and allows us to obtain entry-wise guarantees that do not depend on the coherence of the matrix but on its spikiness only.

**Definition 2** (Leverage scores2).: _Let \(Q\) be a rank-\(d\)\(S A\) matrix with SVD \(U W^{}\). Its left and right leverage scores \(\) and \(\) are defined as \(_{s}=\|U_{s,:}\|_{2}^{2}/d\) for all \(s\), and \(_{a}=\|W_{a,:}\|_{2}^{2}/d\) for all \(a\)._

LME only takes as inputs a policy \(\) and a sampling budget \(T\). It proceeds in two phases: first, it uses half of the sampling budget to estimate the leverage scores of \(Q^{}\) via singular subspace recovery.

Second, it selects a few anchor rows and columns sampled using the estimated leverage scores, and uses the remaining budget to sample the entries of these rows and columns. It finally completes the matrix estimation using a CUR-based method. The full pseudo-code of LME is presented in Appendix C. Observe that LME is parameter-free: it does not require knowledge of the policy rank \(d_{}\), nor upper bounds on unknown parameters such as \(_{}\) or \((Q^{})\) or \((Q^{})\). Throughout this section, when presenting our guarantees, we will abuse notation and use \(d\), \(\) and \(\), instead of \(d_{}\), \(_{}\) and \((Q^{})\).

### Preliminaries

LME exploits a natural empirical estimator of \(Q^{}\) entries at numerous stages. This empirical estimator is essentially based on Monte-Carlo rollouts with truncation as described next. Define the truncated value matrix at a horizon \(\) as follows: for all \((s,a)\), \(Q^{}_{}(s,a)=[_{t=0}^{}^{t}r_{t}(s_{t}^{ },a_{t}^{})_{\{t\}}s_{0}^{}=s,a_{0}^{}=a ]\). By choosing \(\) appropriately, we may control the level of the approximation error \(Q^{}_{}-Q^{}\). We make this observation precise in the following lemma, proved in Appendix F.1.

**Lemma 1**.: _For any \(>0\) and any \((}{(1-)})\), we have \(\|Q^{}-Q^{}_{}\|_{}\)._

In view of the above, to estimate an entry, say \((s,a)\), of \(Q^{}\), we will use an empirical estimator based on trajectories of length \(+1\) of the system under \(\) and starting with (state, action) pair \((s,a)\). In our algorithms, this length is chosen to get an appropriate accuracy level. Specifically, we choose \(\) and \(\) as follows:

\[=}{T}=().\] (1)

These choices will become apparent from our analysis.

### Phase 1: Leverage scores estimation via spectral subspace recovery

The first phase of LME is devoted to the estimation of the leverage scores of \(Q^{}\). To this aim, using half of the sampling budget \(T/2\), we estimate the singular subspaces of the matrix via a spectral method.

Figure 1: Consider an MDP with two states and two actions (see Appendix A.1 for details). The 4 black crosses correspond to the value function of the 4 possible policies. When combining policy iteration with a low rank estimation procedure, we just need to control the condition number of the 4 corresponding value matrices. The red dots correspond to the successive estimates \(V^{(t)}\) of \(V^{}\) when running value iteration. When applying a value iteration approach, we would need to upper bound the condition number of all the corresponding matrices \(Q^{(t)}=(V^{(t-1)})\) for \(t 1\). For a given \(V\), the background color in the figure indicates the value of the condition number of \((V)\). We see that the dynamics of \(V^{(t)}\) under the value iteration algorithm are such that the trajectory \((Q^{(t)},t 1)\) has to go through regions where the condition number is very high. Hence on this example, a value iteration approach would not work well.

_Phase 1a. Data collection and the empirical truncated value matrix._ As suggested in SS4.1, to estimate individual entries of \(Q^{}\), we sample system trajectories of length \(+1\). More precisely, for each of the \(N:=T/(2(+1))\) trajectories, we first sample the starting (state, action) pair uniformly at random, and then observe the trajectory obtained under the policy \(\) and initiated at this pair. The data collected this way is \(=\{(s_{k,0}^{},a_{k,0}^{},r_{k,0}^{},,s_{k,}^{ },a_{k,}^{},r_{k,}^{}):k[N]\}\). Using this data, we construct an empirical estimate of the truncated value matrix as follows \((s,a)\):

\[_{}^{}(s,a)=_{k=1}^{N}( _{t=0}^{}^{t}r_{k,t}^{})\{(s_{k,0}^{},a_{ k,0}^{})=(s,a)\},\] (2)

_Phase 1b. Singular subspace recovery._ We compute the SVD of the empirical truncated value matrix \(_{}^{}\). We obtain \(_{}^{}=_{i=1}^{S A}_{i}_{i} _{i}^{}\), where \(_{1},,_{S A}\) correspond, in decreasing order, to its singular values and \(_{1},,_{S}\) (resp. \(_{1},,_{A}\)) to its left (resp. right) singular vectors. Using this decomposition, we construct our estimate of \(Q^{}\) as follows:

\[^{}=_{i=1}^{S A}_{i}\{_{i}\}_{i}_{i}^{},\] (3)

where \(>0\) is a threshold that we will precise shortly. We view \(^{}\) as a biased estimate of \(Q^{}\) with controlled bias through \(\). We also use \(\) to estimate the rank of \(Q^{}\): \(=_{i=1}^{S A}\{_{i}\}\). Finally, the estimated left (resp. right) singular subspace is denoted \(=[_{1}_{}]^{ S}\) (resp. \(=[_{1}_{}]^{ A}\)). In the following proposition, we provide a choice for the threshold \(\) that yields appropriate guarantees regarding our subspace recovery.

**Proposition 1**.: _Let \((0,1)\) and choose the threshold \(\) as_

\[=^{2}SA(S+A)}{(1-)^{3}T}^{4} ()}+}{T}.\] (4)

_Then, provided that3:_

\[T=_{}(^{2}SA}{ _{d}^{2}(Q^{})}})\] (5)

_we have that events: \(=d_{}\), and for all \(s\),_

\[\|U_{s,:}-_{s,:}(^{}U)\|_{2} }{(1-)^{3/2}_{d}(Q^{})}(}+\|U_{s,:}\|_{2}})^{2}( )\]

_hold with probability at least \(1-\). An analogous result holds for \(\)._

The precise statement (Theorem 4) and the proof are presented in Appendix B.3 and B.4.

_Phase 1c. Leverage Scores Estimation._ To conclude, using the recovered subspaces \(\) and \(\), we estimate the leverage scores as follows \(=\|\|_{1}^{-1}\) and \(=\|\|_{1}^{-1}\), where:

\[ s:\ _{s}=\|_{s,:}\|_{2}^{2} , a:\ _{a}=\|_{a,:}\|_{2}^{2}.\] (6)

The performance of the estimation of the leverage scores is summarized in the following theorem, proved in Appendix B.2.

**Theorem 1** (Leverage Scores Estimation).: _Let \((0,1)\). Suppose the threshold \(\) is chosen as in (4). Then, we have that: \(( s,\ \ _{s} 4\,_{s}) 1-,\) provided that_

\[T=_{}(^{2}^{2}SA}{_{d} ^{2}(Q^{})}}),\]

_An analogous result holds for \(\)._

### Phase 2: Leveraged CUR-based Matrix Completion

Before we proceed with the description of the second phase, we briefly recall the so-called CUR decomposition [19; 27] for low-rank matrices. The decomposition says that for a given rank-\(d\,S A\) matrix \(Q\), there always exists \([S]\), \([A]\), with \(||=||=d\), such that the sub-matrix \(Q_{,}\) is full rank and for all entries \((i,j)\), \(Q_{ij}=Q_{i,}(Q_{,})^{}Q_{,j}\). As in [35; 34; 3], we leverage this decomposition in our matrix estimation procedure, but without any requirement such as knowledge of \(,\) for which \(_{d}(Q_{,})\) bounded away from zero or upper bounds on parameters like the matrix coherence.

_Phase 2a, Data collection to estimate the skeleton of the value matrix._ We start by sampling \(K:=64d(64d/)\) rows (resp. columns) without replacement according to \(\) (resp. \(\)) to form a _skeleton_ of the matrix. These rows and columns are referred to as anchors. We denote the set of selected rows (resp. columns) by \(\) (resp. \(\)). We use the remaining sample budget \(T/2\) to get samples of the entries of \(Q^{}\) in the skeleton. To this aim, we use the procedure described in SS4.1, and sample trajectories of length \(+1\). For each entry \((s,a)_{}:=\), we use \(N_{1}:=T/(4(+1)K^{2})\) trajectories to compute \(_{}^{}(s,a)\), an empirical estimate of \(Q^{}(s,a)\) (see (2)). For each entry \((s,a)_{i}:=(()) (())\), we use \(N_{2}:=T/(4(+1)(K(S+A)-2K^{2})\) trajectories. Note that \(N_{2} N_{1}\) (this plays a role in the analysis).

_Phase 2b, CUR-based completion with Inverse Leverage Scores Weighting._ First, using the leverage scores, and the set of rows \(\) and columns \(\), we define \(K K\) diagonal matrices \(L\) and \(R\) as follows:

\[ i, L_{ii}=_{ i}}\}}, j, R_{jj}=_{j}}\}}.\] (7)

Next, starting from the values of \(_{}^{}(s,a)\) for \((s,a)\) in the skeleton, we perform a CUR matrix completion to obtain \(^{}\): _(i)_ for all \((s,a)()()\), we set \(^{}(s,a)=_{}^{}(s,a)\); _(ii)_ for all \((s,a)()( )\), we set

\[^{}(s,a)=_{}^{}(s,)R(L\, _{}^{}(,)R)^{}L\, _{}^{}(,a).\] (8)

Note that the use of \(L\) and \(R\) in (8), referred to as Inverse Leverage Scores Weighting, corresponds to an importance sampling procedure. It allows us to account for the fact that the skeleton has been sampled using the (estimated) leverage scores.

The next theorem summarizes the performance guarantees under LME. Its proof is presented in Appendix C.1.

**Theorem 2**.: _Let \(>0\), \((0,1)\). Given a deterministic policy \(\), and a sampling budget \(T\), the algorithm_ LME _ensures that \((\|^{}-Q^{}\|_{}) 1-\), provided that \(\|Q^{}\|_{}\) and_

\[T=_{}(d}{(1-)^{3} ^{2}}(r_{}^{2}^{4}^{2}d^{2})).\]

Theorem 2 states that the sample complexity of LME to obtain entry-wise guarantees does not depend on the coherence \(\) of \(Q^{}\) but rather on its spikiness \(\) and condition number \(\) only. Hence LME provides entry-wise guarantees even for coherent matrices. In addition, its sample complexity scales with \(S\), \(A\), \(\) and \(\) optimally. Indeed if \(,=(1)\) and \(d S+A\), it scales as \((1-)^{3}}\). We also wish to emphasize that LME is parameter-free, in the sense that it does not require knowledge of the so-called anchor rows and columns, nor does it require upper bounds on unknown parameters such as coherence, spikiness, rank or condition number. These properties are desirable for RL purposes.

## 5 Low-Rank Policy Iteration

In this section, we present and evaluate LoRa-PI (Low Rank Policy Iteration), a model-free variant of the approximate policy iteration algorithm . It alternates between policy improvement and policy evaluation steps and uses LME, our low rank matrix estimation procedure for policy evaluation. Refer to Algorithm 1 for the pseudo-code.

The following theorem provides performance guarantees for LoRa-PI.

We state the results under the assumption that for any deterministic policy \(\), \(Q^{}\) is \(\)-spiky and has a condition number upper bounded by \(\).

**Theorem 3**.: _Let \((0,1)\) and \(=(\|Q^{{}^{(1)}}\|_{})\). Under LoRa-PI, we have \((\|V^{}-V^{}\|_{}) 1-\), provided_

\[T=_{}(d}{(1-)^{8} ^{2}}(r_{}^{2}^{4}^{2}d^{2})).\]

The proof of Theorem 3 is presented in Appendix D. Having entry-wise guarantees (as stated in Theorem 2) at each iteration of the algorithm is critical to establish Theorem 3. In fact, the proof starts from the observation (see Lemma 9) that

\[(1-)\|V^{}-V^{}\|_{} 2r_{}^{N_{}}+2_{t[N_{}]}\|^{(t)}-Q^{^{(t)}}\|_{}.\]

LoRa-PI combines numerous advantages. (i) It is parameter-free: it does not require the knowledge of upper bounds on parameters such as the ranks, condition numbers, and spikiness of the value matrices of policies. This is thanks to LME, which is itself parameter-free. (ii) Its sample complexity does not depend on the coherence of the value matrices but only on their spikiness; which is an important improvement over existing algorithms . (iii) LoRa-PI offers performance guarantees without having access to good anchor states and actions, without assuming that the rewards are deterministic and that the discount factor is (far too) small, as in  (refer to Section 2 for a detailed discussion). (iv) Its sample complexity has an order-optimal scaling in \(S\), \(A\) and \(\). (v) Finally, since LoRa-PI uses policy iteration, its theoretical guarantees can be established under milder assumptions than if value iteration was used instead (see SS3.3).

The dependence of order \((1-)^{-8}\) is far from the ideal minimal dependence of order \((1-)^{-3}\) that one would typically obtain in RL without low-rank structure. This is an artifact of using a model-free approach, and more specifically the Monte-Carlo estimator of entries of the value matrices. Avoiding such high dependence requires further assumptions and a model-based approach.

Furthermore, it is worth mentioning that the guarantees enjoyed by LoRa-PI can be naturally extended to MDPs that are low-rank only in an approximate sense. We refer the reader to Appendix E for further details.

## 6 Conclusion

In this work, we considered a class of MDPs where the Q-function, viewed as a state-action matrix, admits a low-rank representation under any deterministic policy. We devised LoRa-PI, a model-free learning algorithm based on approximate policy iteration, that provably exploits such low-rank representation to output a near-optimal policy. Critical to the design and performance guarantee of LoRa-PI is a novel low-rank matrix estimation procedure referred to as LME. LME is shown to enjoy a tight entry-wise guarantee while being parameter-free, i.e., it does not require knowledge of the so-called anchor rows and columns, nor upper bounds on unknown parameters such as spikiness, coherence, rank, or condition number. More importantly, its sample complexity does not scale with the coherence but instead with the spikiness of the matrix. This allows us to estimate a wider class of low-rank matrices with entry-wise guarantees than previous work. Such desirable properties are what make LME appealing for RL purposes, and in particular what allows us to show that LoRa-PI is sample-efficient under mild conditions. From a design perspective, LME and its analysis features many interesting tools and ideas. Notably, (i) we derived instance-dependent row-wise singular subspace recovery guarantees, and (ii) we combined the use of the so-called leverage scores with a CUR-based approximation for matrix estimation. We believe such tools and ideas to be of independent interest. Finally, we provided experimental results that suggest the superior performance of our proposed algorithms.