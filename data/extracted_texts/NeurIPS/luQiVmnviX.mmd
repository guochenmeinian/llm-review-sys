# UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation

Hanzhang Zhou\({}^{1,2}\), Zijian Feng\({}^{1,2}\), Zixiao Zhu\({}^{1,2}\), Junlang Qian\({}^{1}\), Kezhi Mao\({}^{1,2}\)

\({}^{1}\)Nanyang Technological University  \({}^{2}\)Singapore-ETH Centre

{hanzhang001, feng0119, zixiao001, junlang001}@e.ntu.edu.sg

ekzmao@ntu.edu.sg

###### Abstract

Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness--sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs. The code is available at https://github.com/hzzhou01/UniBias.

## 1 Introduction

Large language models (LLMs) have shown exceptional capabilities in various natural language processing (NLP) tasks, employing the in-context learning (ICL) paradigm. This paradigm conditions LLMs on a context prompt comprising of a few example-label pairs .

Despite their impressive performance, LLMs are prone to prompt brittleness, characterized by high sensitivity to the choice  and order  of examples, and prompt formatting , as demonstrated in Figure 1. Such prompt brittleness is found to be arise from the bias in LLMs towards predicting certain answers . The presence of the LLM bias undermines the robustness and adaptability of LLMs in diverse applications.

Extensive research has focused on identifying factors that lead to LLM bias and strategies for mitigation. For instance, vanilla label bias  and recency bias  demonstrate the LLM's inherent non-contextual preference for certain labels and contextual preference for specific positions, respectively. Additionally, several calibration methods  are proposed to counteract the bias by adjusting decision boundaries of model output probabilities. However, these approaches are derived from _external_ observations or adjustments of LLM outputs, leaving **the _internal_ mechanisms within LLMs that cause such bias poorly understood**.

In this work, we investigate the internal mechanism of LLM bias, specifically how feedforward neural networks (FFNs) and attention heads contribute to such bias. Building on findings in mechanisticinterpretability (Elhage et al., 2021; Dar et al., 2023), we assess the contribution of individual attention heads and FFN vectors1 to label predictions in LLMs. By identifying FFN vectors and attention heads that convey biased influences towards label prediction, we reveal the internal mechanisms behind several key bias factors, including vanilla label bias (Fei et al., 2023), recency bias (Zhao et al., 2021), and selection bias (Zheng et al., 2023). For instance, our analysis of FFN vectors without input context demonstrates that their cumulative impact biases the LLM towards specific labels, indicating a non-contextual preference for certain labels, i.e., vanilla label bias. We elaborate on the background of mechanistic interpretability in Section 2.1 and present our findings on the internal mechanisms of LLM biases in next section.

Given our findings that various bias factors stem from the biased behaviors of attention heads and FFN vectors, we are prompted to ask: Can we identify the biased components of LLMs and mitigate their detrimental impact on label prediction? Motivated by this intuition, we propose **UniBias**, an inference-only method designed to identify and eliminate biased FFN vectors and attention heads in LLMs. Specifically, we begin by projecting each FFN vector and attention head into the vocabulary space to interpret the information conveyed by their outputs. We then detect biased components based on three criteria we defined: the relatedness criterion, the bias criterion, and the low variance criterion. After identification, we mitigate their impact by masking these biased components. Extensive experimental results demonstrate that LLMs, from which biased components have been removed, consistently outperform their original counterparts by a significant margin. Further, as illustrated in Figure 1, our method significantly improves both the performance and robustness of ICL with perturbations of various design settings.

The contributions of our work are summarized as follows:

* In contrast to existing works based on external adjustments of LLM outputs, we mitigate LLM bias through manipulation of LLM internal structure. This novel perspective potentially offers a new direction for the field. Moreover, our method demonstrate an effective way to manipulate internal structures of LLMs.
* We conduct a thorough investigation of the internal mechanisms underlying biases in LLMs, revealing the inner causes of these biases.
* Extensive experiments across 12 NLP datasets demonstrate that, by removing the biased components, our UniBias method significantly enhances ICL performance and achieve state-of-the-art results. Additionally, it effectively addresses the issue of prompt brittleness.

## 2 Internal Mechanisms Causing the Bias of LLMs

This section reveals the internal mechanisms within LLMs that lead to various bias factors.

Figure 1: illustrates the prompt brittleness of ICL and the effectiveness of our method in mitigating this issue. Experiments are conducted in one-shot setting, using SST2 (Socher et al., 2013) dataset for experiments on example selection and prompt formatting and AGnews (Zhang et al., 2015) dataset for example order experiment due to more diverse combination of orders.

### Background

The theoretical background of this work is based on research on mechanistic interpretability (Elhage et al., 2021; Wang et al., 2022; Geva et al., 2021), which aims to explain the internal processes in language models (LMs), facilitating the interpretation of the contributions of individual model components to the final prediction.

We are focusing on decoder-only LMs in this paper. They are composed by a sequence of transformer layers, each composed of a multi-head self-attention layer and an feedforward neural network layer. The background knowledge for interpreting the contribution of each FFN vector and attention head to the models' prediction are demonstrated as follows.

**The Residual Stream** We interpret Transformers following the view of residual stream (Elhage et al., 2021; Dar et al., 2023). Due to the residual connection of Transformers, each layer takes a hidden state as input, and adds information obtained by its attention layer and FFN layer to the hidden state through residual connection. In this section, the hidden state is a residual stream passed along layers, and each attention layer and FFN layer contribute to the final prediction by adding information to the residual stream.

**Attention Heads** Following Elhage et al. (2021); Dar et al. (2023), the output of each attention layer of LM can be computed as the sum of all its attention heads. Specifically, for \(l\)-th layer, the input is \(X^{l}^{N d}\), and the attention layer is parameterized by four matrices \(W_{Q}^{l}\), \(W_{K}^{l}\), \(W_{V}^{l}\), \(W_{O}^{l}^{d d}\). The columns of each projection matrix and the rows of the output matrix can be split into \(H\) parts: \(W_{Q}^{,j},W_{K}^{,j},W_{V}^{,j}^{d}\) and \(W_{O}^{,j}^{ d}\), where \(H\) is the number of attention heads. We then find that:

\[^{}(X^{})=[A^{,1}X^{}W_{V}^{ ,1},A^{,2}X^{}W_{V}^{,2},,A^{,H}X^{}W_{V}^{,H }]W_{O}^{}=_{j=1}^{H}A^{,j}(X^{}W_{V}^{,j})W_{O}^{ ,j}\]

\[A^{,j}=(W_{O}^{,j})(X^{ }W_{K}^{,j})^{T}}{}+M^{,j})M^{,j}^{}^{d}\), FFN parameter matrices \(^{},^{}^{d_{m} d}\), the FFN output can be derived as:

\[^{}(^{})=f(^{}^{^{T}} )^{}=_{i=1}^{d_{m}}f(^{}_{i}^{ })_{i}^{}=_{i=1}^{d_{m}}m_{i}^{}_{i}^{}\]

where \(f\) is the activation function, \(i\) is the index of the vector. Then, the FFN layer can be viewed as a linear combination of vectors: the multiplication of \(^{}\) and the key vector \(_{i}\) produces the coefficient \(m_{i}^{}\) that weights the corresponding value vector \(_{i}\).

**Logit Lens** The logit lens (Nostalgebraist, 2020) is a technique that directly decode hidden states into the vocabulary space using the unembedding matrix of the LLM for interpretation. This approach has been validated in various studies as an efficient method for interpreting the weight matrix or hidden states of LLMs (Dar et al., 2023; Hanna et al., 2023; Feng et al., 2024; Yu et al., 2023; Geva et al., 2021).

In summary, each attention layer and FFN layer contribute to the final prediction by adding their output hidden states to the residual stream. These outputs can be viewed as the sum of their respective attention heads and FFN vectors. Each attention head or FFN vector's output can be interpreted through the logit lens.

### Internal Mechanisms of Bias Factors

We delve into the mechanisms behind several bias factors, analyzing the contributions of attention heads and FFN vectors to the biased predictions in LLMs. We explore vanilla label bias, position bias, and selection bias using the Llama-2 7B model (Touvron et al., 2023).

**Vanilla Label Bias** The vanilla label bias (Fei et al., 2023), also known as common token bias (Zhao et al., 2021), is the inherent, uncontextual preference of the model towards predicting certain label names. Given the contextual nature of attention layers, our investigation focuses on the FFN layers, where we identified a corresponding uncontextual preference. Specifically, by projecting the FFN value vectors into the vocabulary space, we compute the logits for various label names for each FFN vector. Utilizing the residual stream insight, we then aggregate these logits for all FFN vectors whose label logits rank within the top \(10\) over the vocabulary, reflecting unconttextual influences of FFN vectors that are effective in label prediction. This process yields what we term _uncontextual accumulated FFN logits_, revealing the intrinsic bias of the LLM towards predicting label names without the influence of input.

Figure 2 illustrates the accumulated uncontextual FFN logits across different label names in the sentiment analysis task, alongside their corresponding zero-shot prediction frequencies on the SST-2 dataset. For example, the label name 'positive' exhibits higher uncontextual accumulated FFN logits compared to 'negative,' leading to a higher frequency of 'positive' predictions. Additionally, when comparing the labels 'good' and 'bad', the difference in their uncontextual accumulated FFN logits is more pronounced than that between 'positive' and 'negative,' resulting in a larger discrepancy in prediction frequency. Conversely, the accumulated logits for the labels'satisfied' and 'disappointed' show a reverse trend relative to 'positive' and 'negative', which results in a corresponding reverse trend in their prediction frequency ratios.

**Recency Bias** Recency bias refers to the tendency of LLMs to favor the label of the example at the end of the prompt (Zhao et al., 2021). By examining the behavior of attention heads within LLMs, we observe that specific heads consistently prioritize the example at the end of the prompt, providing an internal perspective on the origin of recency bias.

We identify the biased attention head using the method introduced in Section 3. We compare the behaviors of a biased attention head (layer 16, head 29) and an unbiased attention head (layer 16, head 19) in terms of the attention weight assigned to examples at different positions and the label logits of the corresponding attention head for the last output. Specifically, we use the SST-2 dataset, including one positive and one negative example in the prompt, and test with 40 samples, evenly split between positive and negative examples. More experimental details are provided in Appendix A.

Experimental results in Figure 3 reveal that the biased attention head (layer 16, head 29) consistently assigns significantly larger attention weights to the final example, irrespective of the ground truth labels of the test samples. This bias persists even when the sequence of examples is reversed, as shown in the second subfigure, indicating a biased preference of this attention head for the last example in the prompt. Furthermore, the biased attention weight assignment leads to biased logits, as shown in the third subfigure. In contrast, the unbiased attention head (layer 16, head 19) assigns very close averaged attention weights to both examples in the prompt. Interestingly, we observe that this unbiased head generally assigns larger weights to the example whose label matches the ground

Figure 3: The internal mechanism of the recency bias.

Figure 2: Unveiling vanilla label bias by uncontextual accumulated FFN logits.

truth label of the test sample, resulting in 35 out of 40 samples being correctly classified based on this pattern by this single attention head. The preference shown by specific attention heads for the example at the end of the prompt reveals the internal mechanism of recency bias.

Selection BiasThe selection bias refers that LLMs prefer to select specific option ID (like "Option A") as answers for multiple choice questions [Zheng et al., 2023]. We have identified both FFN vectors and attention heads that consistently favor a specific option regardless of the ground truth label of the test sample, revealing the internal mechanism of selection bias.

We evaluate the Llama-2 7B model on the ARC dataset, which contains four options (A, B, C, D). We use a zero-shot setting to avoid the influence of position bias from multiple examples. More details are provided in Appendix A. Experimental results are illustrated in Figure 4. Firstly, we observe that the LLM exhibits a vanilla label bias favoring option "A", as shown in the first subfigure. Additionally, we identify a biased attention head that demonstrates a position bias consistently favoring the first option regardless of the ground truth labels of the test samples (second subfigure) or changes in the sequence of options (third subfigure). Since option A is usually the first option, these two biases both lead to the LLM's preference for option A.

## 3 Methodology

In the previous section, we unveil that **various bias factors are stem from the biased behaviors of attention heads and FFN vectors**. Naturally, we pose the question: _Can we identify the biased components of LLMs and mitigate their impact on label prediction?_ Therefore, we propose our **UniBias** method to **Un**veil and **m**itigate LLMs' label **Bias** through internal attention and FFN manipulation. Notably, our method is proposed for decoder-only LLMs.

### Biased FFN Vectors Identification

Identifying biased FFN vectors in LLMs hinges on whether the contribution of each FFN vector is independent and interpretable. As discussed in Section 2.1, the output of an FFN layer can be cast as a linear combination of FFN vectors. Each FFN vector contributes to the final prediction by adding information encoded in its value vector, \(_{i}^{}\), weighted by its corresponding coefficient, \(m_{i}^{}\). This information within \(_{i}^{}\) can be interpreted through the logit lens, enabling us to interpret it as a distribution of logits across the vocabulary space.

How to identify an FFN vector as biased? we assess whether it consistently introduces a biased preference towards specific labels into the residual stream, regardless of variations in the test samples. Such consistent biases can skew the LLM's predictions. We introduce the following criteria to detect biased components in LLMs, which are also applicable for identifying biased attention heads:

* **Relatedness Criterion**: The information introduced by the FFN vector (or attention head) should closely relate to label prediction.
* **Biased Criterion**: The information contributed to the residual stream by the FFN vector (or attention head) exhibits a biased distribution, favoring certain labels over others.
* **Low Variance Criterion**: The label prediction information added by the FFN vector (or attention head) to the residual stream is almost identical across a set of test samples with different labels, i.e., exhibits very small variance.

The third criterion is key to identifying biased FFN vectors (or attention heads), as consistently low variance indicates that the FFN vector is not adequately responsive to varying inputs. Combined with the second criterion, this suggests a bias towards certain predictions regardless of the input's contextual differences.

Figure 4: The internal mechanism of the selection bias.

To examine these criteria, we interpret the information contributed by each FFN vector, i.e., \(m\). For simplicity, we omit the layer number \(\) and FFN index \(i\). Since the FFN value vector \(\) is fixed, changes in the FFN coefficient \(m\) across different samples reflect the change in information brought by the FFN vector. We interpret this information by projecting each FFN value vector into the vocabulary space and analyzing the logit distribution over label tokens, termed _label logits_.

Specifically, given an FFN value vector \(^{d}\), the unembedding matrix \(E^{d d_{e}}\), a label token mapping matrix \(L^{N d_{e}}\), where each row is a one-hot vector indicating the token id of the first token of each label name, the label logits \(^{()}=[g_{0}^{(k)},g_{1}^{(k)},,g_{c-1}^{(k)}]^{}\) (where \(c\) is the class number) corresponding to the FFN value vector \(\) of \(k\)-th sample can be obtained by:

\[= E L^{}\]

We use \(p\) unlabeled samples from the task to assess the three criteria we defined. The coefficients and label logits of an FFN vector for these samples are denoted as \(=[m_{0},m_{1},,\ m_{p-1}]\) and \(=[^{(0)},^{(1)},,^{(p-1)}]^{ }^{p c}\), respectively. An FFN vector is considered biased if it meets the following conditions, each corresponding to one of the three criteria we defined:

\[_{k=0}^{p-1}(_{k,:}) =_{k=0}^{p-1}(^{()} )=_{k=0}^{p-1}_{j=0}^{c-1}g_{j}^{(k)}>th_{FFN}^{1}\] (1) \[_{k=0}^{p-1}(_{k,:}) =_{k=0}^{p-1}(^{()} )=_{k=0}^{p-1}_{j=0}^{c-1}(g_{j}^{(k) }-(^{()}))>th_{FFN}^{2}\] (2) \[CV()=)}{()}=_{j=0}^{p-1}(m_{k}-() )^{2}}}{_{k=0}^{p-1}m_{k}}<th_{FFN}^{3}\] (3)

where \((^{(k)})=_{j=0}^{c-1}g_{j}^{(k)}\), \(()=_{k=0}^{p-1}m_{k}\). The thresholds \(th_{FFN}^{1},th_{FFN}^{2},th_{FFN}^{3}\) are set by grid search, which is elaborated in Section 3.4

The first equation corresponds to the relatedness criterion, measured by the sum of label logits. A higher sum indicates that the information introduced by the FFN vector is more relevant to label prediction. The second equation relates to the bias criterion, quantified by the deviation of the average logit for each label from the overall average logit across all labels. Ideally, for a set of test samples with different labels, the average logits for each label should be relatively balanced. A greater deviation from each label's average compared to the overall average across all labels indicates a more biased distribution. The third equation addresses the low variance criterion, measured by the coefficient of variation (CV) of the FFN vector coefficients across different samples. The CV, calculated as the standard deviation normalized by the mean, indicates whether the label prediction information added by the FFN vector remains almost the same across different samples.

### Biased Attention Heads Identification

The identification of biased attention heads closely resembles the process of identifying biased FFN vectors. As discussed in Section 2.1, each attention head's contribution to the final prediction is independent and interpretable. Therefore, we project the output hidden states of each attention head into the vocabulary space to interpret the information they contribute.

To identify biased attention heads, we use the same three criteria introduced for identifying biased FFN vectors. To apply these criteria, we project the output hidden states from each attention head into the vocabulary space and analyze their label logits as the information contributes to label prediction. The output from each attention head consists of hidden states generated for every token in the sequence. For our analysis, we specifically use the hidden state of the last token preceding the prediction of label names, interpreting it as the most direct contribution of the attention head to the prediction, given the autoregressive nature of LLMs.

Specifically, to obtain the label logits for an attention head, consider the output hidden states \(H^{N d}\) of this head, the unembedding matrix \(E^{d d_{e}}\), and the label token mapping matrix \(L^{N d_{e}}\). Given the token position \(p_{}\{0,1,,N-1\}\), which indicates the index of the first token of the predicted label names, the label logits \(^{(k)}=[a_{1}^{(k)},a_{2}^{(k)},,a_{c}^{(k)}]^{}\) of the attention head for the \(k\)-th sample are derived by:

\[^{(k)}=H_{(p_{}-1),:} E L^{}.\]we employ the same \(p\) unlabeled samples from the task to assess the criteria for identifying biased attention head. The label logits for these samples are formed as \(A=[^{(0)},^{(2)},,^{(m-1)}]^{}^{m c}\). An attention head is considered biased if it meets the following conditions:

\[_{k=0}^{p-1}(A_{k,:})= {p}_{k=0}^{p-1}(^{(k)})=_{k= 0}^{p-1}_{j=1}^{c}_{j}^{(k)}>th_{Att}^{1}\\ _{k=0}^{p-1}(A_{k,:})=_{k= 0}^{p-1}(^{(k)})=_{ k=0}^{p-1}_{j=0}^{c-1}(a_{j}^{(k)}-(^{(k)}))>th_{Att}^{2}\\ _{j=0}^{c-1}w_{j} CV(A_{,:})=w_{j})}{(A_{,:})}<th_{Att}^{3}\]

where \(w_{j}=_{j=0}^{c-1})}{(A_{,:})}\), \((A_{,:})=_{k=0}^{p-1}A_{,:}\), \((A_{,:})=_{k=0}^{p-1}(A_{,:}-(A_{,:}))^{2}}\).

The functions of the first two criteria are identical to those for biased FFN vector identification. The third function is the weighted sum of the coefficient variance of each label across test samples. The thresholds for biased attention head identification are also derived by grid search.

### Biased FFN Vectors and Attention Heads Manipulation

After identifying the biased components of the LLM, we eliminate their influence by masking these biased FFN vectors and attention heads. Specifically, we create masks for the attention heads in each attention layer and reset the coefficient of the biased FFN vector and biased attention head mask.

### Grid Searching

Specifically, we utilize a small subset of training data as a support set, with 20 samples for each class. We then grid search all combinations of threshold values and select the combination that results in the most balanced distribution of average label logits. Specifically, let \(\) represents the set of threshold combinations, and \(P(t)\) denote the average label logits for a threshold combination \(t\), we aim to find the combination \(t^{*}\) that minimizes the bias of label logits: \(t^{*}=_{t}((t))\).

It is noteworthy that although there are multiple combinations of thresholds, they usually result in a few set of different biased components. For example, for a grid search of thresholds of FFN vectors with 80 combinations, it only result in 4 different sets of biased FFN vectors that need to be examined with the support set on the SST-2 dataset. Additionally, during the inference stage of evaluating test samples, the computation time of the UniBias method is completely identical to that of the original LLMs.

Additionally, the support set can be replaced with unlabeled samples, using approximately twice the number of unlabeled samples compared to labeled ones. For further details, please see Appendix F.

## 4 Experiments

In this section, we aims to investigate a few research questions (RQ). **RQ 1**: After eliminating biased components from LLMs, does the ICL performance improve compared to the original LLM? Additionally, how does our UniBias method compare to existing calibration methods? **RQ 2**: Given that ICL suffers from prompt brittleness, can our UniBias method contribute to more robust ICL performance? **RQ 3**: Are there any observable patterns of biased FFN vectors and attention heads within and across tasks? **RQ 4**: What is the performance of LLMs after eliminating only the biased FFN vectors and only the biased attention heads, respectively? **RQ 5**: What is the impact of support set size on the performance of the UniBias method?

### Experimental Setup

DatasetsWe evaluate our UniBias method on 12 diverse natural language processing datasets across various tasks, including sentiment analysis, topic classification, natural language inference, reasoning, and word disambiguation. Statistics and details about the datasets can be found in Table 4 in Appendix.

[MISSING_PAGE_FAIL:8]

### Alleviating Prompt Brittleness

Existing studies have found that LLMs are prone to prompt brittleness, with various factors such as the selection and order of examples, as well as the prompt formatting. To address **RQ 2**, we simulate these brittle scenarios by choosing different demonstration samples, using different prompt formats, and changing the example order to observe variations in LLM performance.

Figure 1 presents Llama-2 7b's performance both with and without UniBias. Without UniBias, the standard ICL's performance varies significantly, ranging from 8% to 26%, demonstrating its instability. After applying UniBias, the accuracy remains consistently high and stable, with variations consistently less than 4% under perturbations of various design settings. We provide further theoretical analysis on why UniBias can mitigate prompt brittleness and address various bias factors in Appendix G.

### Biased Components Analysis and Common Biased Components Elimination

In response to **RQ3**, we present the frequency counts of identified biased attention heads (AHs) and FFNs under repeated experiments in Figure 6. A large frequency count for an LLM component indicates a higher repeat of being identified as biased in the corresponding dataset. The first subfigure displays the biased components for various example selections, revealing several commonly biased LLM components across different prompts within a single dataset. The second subfigure highlights the common biased components across different datasets (ARC and MMLU) for the reasoning task, indicating that different datasets with similar tasks could share common biased LLM components. The third subfigure demonstrates the presence of common biased LLM components across different tasks.

Experimental results suggest an interesting future direction: we may identify global biased components that would mitigate bias across multiple tasks and diverse prompt design settings. We conduct an preliminary experiment to explore the potential of eliminating common biased components. Specifically, we eliminate attention heads that are frequently identified as biased and apply this setting to diverse tasks, rather than handling each task individually. Experimental results in Table 2 demonstrate that although not as effective as our full Unibias method, eliminating common biased components outperforms the vanilla ICL by a large margin. Experiment details are in Appendix D.

### Ablations

We conduct ablation studies to analyze the impact of exclusively eliminating biased AHs or FFNs to address **RQ 4**. Table 3 presents the results of removing only biased FFN vectors (FFN-only) and only

    & SST2 & MMLU & COPA & RTE & MR & Trec & Avg. \\  ICL & 87.22\({}_{6.03}\) & 41.73\({}_{2.25}\) & 67.60\({}_{2.30}\) & 66.21\({}_{7.30}\) & 89.37\({}_{1.83}\) & 72.92\({}_{12.42}\) & 70.84 \\ Unibias & **94.54\({}_{0.62}\)** & **44.83\({}_{0.24}\)** & **69.00\({}_{2.74}\)** & **67.65\({}_{6.44}\)** & 92.19\({}_{0.37}\) & **80.80\({}_{0.17}\)** & **74.84** \\ Eliminating Common & 94.32 \({}_{0.60}\) & 44.20\({}_{1.14}\) & 68.00\({}_{2.87}\) & 67.37\({}_{4.60}\) & **92.43\({}_{0.09}\)** & 77.60\({}_{4.75}\) & 73.98 \\ Biased Components & & & & & & \\   

Table 2: Experiments on eliminating common biased components. Attention heads that are frequently identified as biased are removed from the original Llama-2 7b model.

Figure 6: Analysis of biased attention heads (AHs) and FFN vectors (FFNs). The frequency count of biased LLM components across five repeat experiments with different example selections is reported.

biased attention heads (attention-only). Both FFN-only and attention-only methods outperform the standard ICL, demonstrating their effectiveness. When combined as UniBias, the method achieves the best results across most datasets, indicating that the two approaches are complementary.

Additionally, we further conduct experiments to investigate the impact of support set size (**RQ 5**), which is detailed in Appendix E.

## 5 Related Work

Bias in LLMs: It is well recognized that LLMs are unstable under various ICL design settings, and this instability arises from biases in LLMs toward predicting certain answers (Zhao et al., 2021; Lu et al., 2022). To understand these biases, existing studies have identified various bias factors, including recency bias, majority label bias, common token bias (Zhao et al., 2021), and domain label bias (Fei et al., 2023) in classification tasks. More recently, selection bias, which consistently favors specific options in multiple-choice questions, has also been identified (Zheng et al., 2023; Wang et al., 2023b). To address these biases, several calibration methods have been proposed, including contextual calibration (Zhao et al., 2021), domain-context calibration (Fei et al., 2023), and prototypical calibration (Han et al., 2023). However, these identified bias factors and calibration methods are derived from external observations or adjustments of LLM outputs, leaving the underlying mechanisms within LLMs that cause such biases poorly understood.

Prompt Brittleness: Regarding prompt brittleness, it is demonstrated in the literature that this instability of prompt arises from LLMs' inherent bias towards predicting certain answers (Zhao et al., 2021). Therefore, current research efforts address the prompt brittleness by mitigating LLMs' bias towards labels (Fei et al., 2023; Han et al., 2023; Zhao et al., 2021).

Mechanistic Interpretability: Mechanistic interpretability (Elhage et al., 2021; Wang et al., 2022) aims to explain the internal processes in language models, facilitating the interpretation of the contributions of individual model components to the final prediction. Our work builds on the understanding of the residual stream (Elhage et al., 2021), the logit lens (Nostalgebraist, 2020), and the interpretation of LLM components in the vocabulary space (Dar et al., 2023; Geva et al., 2021).

## 6 Conclusion

In this work, we have deepened the understanding of biases in LLMs by unveiling the internal mechanisms that contribute to various bias factors. Building on this understanding, we proposed our UniBias method to mitigate these biases by identifying and eliminating biased FFN vectors and attention heads, demonstrating an effective way to manipulate the internal structures of LLMs. Extensive experiments show that our UniBias method achieves state-of-the-art performance across 12 NLP datasets and different ICL settings. Additionally, our method successfully alleviates prompt brittleness and enhances the robustness of ICL.