# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

data inefficient as actions are sampled according to given policies, potentially overlooking actions from states where the expected return is uncertain. To achieve better value estimation with fewer samples, it is essential to focus on state-action pairs with high variance in return, as these pairs would exhibit greater uncertainty in their mean return. Therefore, a behavior policy should visit such pairs more frequently to offset higher variance in return. Consider an analogy of a two-arm bandit problem: fewer samples are needed to accurately evaluate a constant reward arm, whereas an arm with a variable reward demands a greater number of samples to achieve the same level of certainty. We empirically support this claim by comparing round-robin and our approach later in this paper.

With this motivation, we introduce GVFExplorer, that adaptively learns a behavior policy which minimize the total MSE across all GVF predictions. GVFExplorer leverages the existing off-policy temporal difference (TD) based estimator of variance in return distribution (Sherstan et al., 2018; Jain et al., 2021) to guide the behavior policy. The strategy is to frequently take actions that might have more unpredictable outcomes (high variance in return). By sampling them more, agent can estimate the mean return better with fewer interactions, thus effectively lowering the overall MSE in the GVF predictions.

GVFExplorer optimizes the data usage and reduces the prediction error, offering a scalable solution for complex environments. This is particularly valuable for real-world applications like personalized recommender systems (Parapar and Radlinski, 2021; Tang et al., 2015), where it can enable efficient evaluation of personalized policies based on diverse user preferences (reward functions) (Li et al., 2024), leveraging shared knowledge for improved accuracy.

Contributions:(1) We design an adaptive behavior policy that enables accurate and efficient learning of multiple GVF predictions in parallel [Algorithm 1]. (2) We derive an iterative behavior update rule that directly minimizes the overall prediction error [Theorem 4.1]. (3) We prove in the tabular setting that each iterative update to the behavior policy causes the total MSE across GVFs to be less than equal to one from the old policy [Theorem 4.2]. (4) We establish the existence of a variance operator that enables us to use TD-based variance estimation [Lemma 5.1]. (5) We empirically demonstrate in both tabular and Mujoco environments that GVFExplorer lowers the total MSE when estimating multiple GVFs compared to baseline approaches and enables evaluating a larger number of GVFs in parallel.

## 2 Related Work

Exploration in reinforcement learning (RL) has predominantly focused on improving policy performance for a single objective (Oudeyer et al., 2007; Schmidhuber, 2010; Jaderberg et al., 2016; Machado et al., 2017; Eysenbach et al., 2018; Burda et al., 2018; Guo et al., 2022). Refer to Ladosz et al. (2022) for a detailed survey on exploration techniques in RL. While related to exploration, these works differ from ours, as they concentrate on optimizing policies for single objective rather than evaluating multiple GVFs (policy-cumulant pair) simultaneously.

Our work is most closely related to other works on learning multiple GVFs. Xu et al. (2022) address a similar problem by evaluating multiple GVFs using an offline dataset, but our method operates online, avoiding the data coverage limitations of offline approaches. Linke et al. (2020) develops exploration strategies for GVFs in a stateless bandit context, which does not deal with the off-policy learning or function approximation challenges present in the full Markov Decision Process (MDP) context. In a single bandit problem, Antos et al. (2008); Carpentier et al. (2015), show that the optimal data collection strategy to estimate mean rewards of arms is to sample proportional to each arm's variance in reward. Prior works like Hanna et al. (2017) learned a behavior policy for a _single_ policy evaluation problem using a REINFORCE-style (Williams, 1992) variance-based method called BPS. This idea extends on the similar principles of using Importance Sampling in Monte Carlo simulations for finding optimal sampling policy based on variance minimization (Owen, 2013; Frank et al., 2008). Metelli et al. (2023) extends this idea to the control setting. However, these methods are limited to single-task evaluation or control. Evaluating multiple policies simultaneously is more complex, requiring careful balance in action selection among interrelated learning problems. Perhaps the closest work to ours is by McLeod et al. (2021), which uses the changes in the weights of Successor Representation (SR) (Dayan, 1993) as an intrinsic reward to learn a behavior policy that supports multiple predictive tasks. GVFExplorer approach is simpler, as it directly optimizes the behavior policy to minimize the total prediction error over GVFs, resulting in an intuitive variance-proportional sampling algorithm. We will compare the two approaches empirically as well.

## 3 Preliminaries

Consider an agent interacting with the environment to obtain estimates of \(N\) different _General Value Function_ (GVF) (Sutton et al., 2011). We assume an episodic, discounted Markov decision process (MDP) where \(\) is the set of states, \(\) is the action set, \(:_{}\) is the transition probability function, \(_{}\) is the \(||\)-dimensional probability simplex, and \([0,1)\) is the discount factor.

Each GVF is conditioned on a fixed policy \(_{i}:_{}\), \(i=\{1,,N\}\) and has a cumulant \(c_{i}:\). For simplicity, we assume that all cumulants are scalar, and that the GVFs share the environment discount factor \(\). This eases the exposition, but our results can be extended to general multidimensional cumulants and state dependent discount factor. Each GVF is a value function \(V_{_{i}}(s)=_{_{i}:}[G_{i}^{i}|s_{t}=s]\) where \(G_{i}^{i}=c_{i,t}+ G_{t+1}^{i}\). Each GVF can be viewed as answering the question, "what is the expected discounted sum of \(c_{i}\) received while following \(_{i}?\)" We can also define action-value GVFs: \(Q_{_{i}}(s,a)=c_{i}(s,a)+_{s^{}( |s,a)}[V_{_{i}}(s^{})]\), with \(V_{_{i}}(s)=_{a_{i}(|s)}[Q_{_{i}}(s,a)]\).

At each time step \(t\), the agent in state \(s_{t}\), takes an action \(a_{t}\) and receives cumulant values \(c_{i,t}\) for all \(i\{1,,N\}\), transitioning to a new state \(s_{t+1}\). This repeats until reaching a terminal state or a maximum step count. Then the agent resets to a new initial state and starts again. The agent interacts with environment using a behavior policy, \(:_{}\). The goal is to approximate values \(_{i}\) corresponding to the true GVFs value \(V_{_{i}}\). We formalize the objective as **minimizing the Mean Squared Error (MSE)** under some state weighting \(d(s)\) for all GVFs:

\[(V,)=_{i=1}^{N}_{s}d(s)V_{ _{i}}(s)-_{i}(s)^{2}.\] (1)

In our experiments, we use the uniform distribution for \(d(s)\). This objective can be generalized to prioritize certain GVFs using a weighted MSE.

Importance Sampling (IS).To estimate multiple GVFs with distinct target policies \(_{i}\) in parallel, off-policy learning is essential. Importance sampling (IS) is one of the primary tools for off-policy value learning (Hesterberg, 1988; Precup, 2000; Rubinstein and Kroese, 2016), allowing estimation of value function under target policy \(\) using samples from different behavior policy \(\). In the context of off-policy Temporal Difference (TD) learning (Sutton and Barto, 2018), the IS ratio, \(_{t}=|s_{t})}{(a_{i}|s_{t})}\), is used to adjust the updates to ensure _unbiased value estimates_. The update rule is given as \((s_{t},a_{t})=(s_{t},a_{t})+(c_{t}+_{t+1} (s_{t+1},a_{t+1})-(s_{t},a_{t})),\) where \(\) is the learning rate. This update rule ensures that estimated value function \(\) converges to correct value \(Q_{}\) under policy \(\), despite the samples being generated from a behavior policy \(\).

## 4 Behavior Policy Optimization

As described in the previous section, the goal of the agent is to minimize the total mean squared error (MSE) across the given GVFs (Eq. (1)). Note that \(=+^{2}\). For the algorithm's derivation, we will use **unbiased** IS estimation for off-policy correction, which shifts the task of minimizing MSE to reducing the total variance across GVFs. _Thus, the core problem is to design a behavior policy that collects data to **minimize the variance in return** across all GVFs in order to accurately estimate multiple GVF value functions._

The problem of estimating a _single target policy_'s value, \(V_{}\), is well studied in the literature (see Sec. 2). In Monte Carlo sampling literature, it is well known that there exists an optimal sampling distribution (i.e., behavior policy) that provides optimal variance reduction compared to simply running the target policy Kahn and Marshall (1953); Owen (2013). Unfortunately, the analytical solution of obtaining this optimal behavior policy, \(^{*}\), requires foreknowledge of \(V_{}\), making it impractical when our overall purpose is to estimate \(V_{}\). Nonetheless, in this work, we take inspiration from this earlier work and develop a practical method that iteratively solves for a single behavior policy that minimizes the total variance when estimating multiple general value functions in parallel.

### Objective Function

We propose GVFExplorer to address the above limitation and extend the problem to accurately estimate multiple GVF values. GVFExplorer takes as input the GVF target policies \(_{i=\{1,,N\}}\), collects data from a single (non-stationary) behavior policy \(\), and outputs the GVF estimates \(_{i=\{1,,N\}}\). Since our objective is to find a behavior policy that minimizes the variance in return across multiple GVFs, we use an existing off-policy TD-style variance estimator (Sherstan et al., 2018). This estimator allows us to bootstrap the target values and iteratively update the variance function, making the solution scalable to complex domains.

We define the variance function by \(_{}^{}()\), which measures the **variance in the return** of target policy \(\) in a given state \(s\) when actions are sampled under a different behavior policy \(\). We describe how to learn this function in Sec. 5. The variance function for a given state and a given state-action pair are defined respectively as:

\[M_{}^{}(s)=_{}(G_{t}|s_{t}=s,a) M _{}^{}(s,a)=_{}(G_{t}|s_{t}=s,a_{t}=a,a^{}).\]

**Our objective is to find an optimal behavior policy \(^{*}\) that efficiently collects a single stream of experience to minimize the sum of variances \(M_{_{\{1 N\}}}^{}\)** under some state distribution \(d(s)\), as,

\[^{*}=_{}_{i=1}^{N}_{s}d(s)M_{_{i}}^{}(s) {s.t.}(a|s) 0\,\&\,_{a}(a|s)=1.\] (2)

We solve the above objective function iteratively. At each iteration \(k\), GVFExplorer produces a behavior policy \(_{k}\). The behavior policy interacts with the environment and gathers data. Using this data, any off-policy TD algorithm can be used to iteratively estimate the variance function \(M_{}^{_{k}}\). This variance function is plugged into the optimization problem given in Eq. (3) to update to a policy \(_{k+1}\) that reduces variance. The iterative procedure is analogous to _policy iteration_, which alternates policy evaluation with policy improvement. Here, we alternate between the variance evaluation and the improvement of behavior policy to minimize the overall sum of variance across all given GVFs.

Here, our aim is to iteratively improve behavior policy and decrease variance functions to estimate the GVF values \(V_{_{i=\{1,2,,N\}}}\) with reducing MSE:

\[_{0}M_{_{i=1,2,}}^{_{0}}_{1} M_{_{i=1,2,}}^{_{1}}_{K},\]

where \(\) denotes _variance estimation_ and \(\) denotes _behavior policy improvement_. Next, we present Theorem 4.1 which principally derives the behavior policy update from \(_{k}\) to \(_{k+1}\) by solving the objective in Eq. (7). We demonstrate that the behavior policy update in Eq. (3) minimizes the objective by showing that \(_{k+1}\) is a better policy than \(_{k}\). The policy \(_{k+1}\) is considered as good as, or better than \(_{k}\), if it obtains lesser or equal total variance across all GVFs:\(_{i}M_{_{i}}^{_{k+1}}(s)_{i}M_{_{i}}^{_{k}}(s)\). The proof of behavior policy improvement is detailed in Theorem 4.2.

### Theoretical Solution

**Theorem 4.1**.: _(**Behavior Policy Update:**) To find the behavior policy \(\) that minimize the variance objective across \(N\) given target policies \(\{_{i}\}_{i=1}^{N}\), we iteratively update \(\) by solving the objective in Eq. (2). Given state-action variance function \(M_{_{i}}^{_{k}}(s,a)\), the solution to Eq. (2) at iteration \(k\) is:_

\[_{k+1}(a|s)=_{i}(a|s)^{2}M_{_{i}}^{_{k}}(s,a) }}{_{a^{}}_{i}(a^{}|s)^{2}M_{_{i}}^{_{k} }(s,a^{})}}.\] (3)

Proof.: The proof is presented in App. A.1. 

Theorem 4.1 describes how to iteratively update the behavior policy \(_{k}\) that minimizes objective in Eq. (2) by using the return variance \(M_{_{i}}^{_{k}}\). The policy \(_{k+1}\) selects actions proportional to their variance, meaning high-variance return \((s,a)\) pairs are explored frequently. By visiting high-variance return pairs, policy gains informative samples and reduce the overall uncertainty. Consequently, this process improves the GVF value predictions and decrease the number of interactions needed for effective learning.

Next Theorem 4.2 ensures that behavior policy \(_{k+1}\) either decreases or maintains the total variance across all GVFs relative to \(_{k}\), ensuring consistent progress towards minimizing the variance without oscillation. In simple terms, each policy update ensures the variance does not increase.

**Theorem 4.2**.: _(Behavior Policy Improvement:) The behavior policy update in Eq.(3) ensures that the aggregated variances across all target policies \(\{_{i}\}_{i=1}^{N}\) either decreases or remains unchanged at each iteration \(k\). This non-increasing variance property demonstrates that each successive behavior policy \(_{k+1}\) is improvement over \(_{k}\) in terms of reducing the total variance. Formally we have,_

\[_{i=1}^{N}M_{_{i}}^{_{k+1}}(s)_{i=1}^{N}M_{_{i}}^{_{k}} (s),\, k, s.\]

Proof.: The proof is in App. A.1. 

## 5 Variance Function

The theorems provided in the previous section rely on the variance function \(M_{_{i}}^{_{k}}\). Here, we study this variance function in detail.

What is the Variance Function \(M\)?In an off-policy context (Sherstan et al. (2018), Jain et al. (2021)), introduced the variance function \(M_{}^{}\), which estimates the variance in return under a target policy \(\) using data from a different behavior policy \(\). We will directly use this function \(M_{}^{}\) as our variance estimator and present it here for completeness. The function \(M_{}^{}\) for a state-action pair under \(\), with an importance sampling correction factor \(_{t}=|s_{t})}{(a_{t}|s_{t})}\), is defined as:

\[M_{}^{}(s,a)=_{a}(G_{t,}|s_{t}=s,a_{t}=a) =_{a}[_{t}^{2}+^{2}_{t+1}^{2}M_{}^{ }(s_{t+1},a_{t+1})|s_{t}=s,a_{t}=a]\] (4)

Here, \(G_{t,}\) is the return at time \(t\), and \(_{t}=r_{t}+_{a^{}}[Q_{}(s_{t+1},a^{ })]-Q_{}(s_{t},a_{t})\) is the TD error. We use **Expected Sarsa**(Sutton and Barto, 2018) to compute \(_{t}\), eliminating the need for IS by using the expected value of the next state-action pair under \(\) for bootstrapping, thus stabilizing the update and lowering the variance. \(M_{}^{}(s,a)\) relates the variance under \(\) from the current state-action pair to the next, where actions are sampled under \(\). This allows effective bootstrapping and iterative update using a TD-style method. The state variance function is defined as \(M_{}^{}(s)=_{a}(a|s)^{2}(s,a)M_{}^{}(s,a)\).

Note, the true \(Q_{}\) is required to compute the TD error \(_{t}\) in Eq. (4). Following Sherstan et al. (2018), we substitute the value estimate \(\) for the true function \(Q_{}\) to compute \(_{t}\) in Eq. (4). Additionally, we use variance estimates \(_{}^{_{k}}\) to update the next step policy \(_{k+1}\) instead of true variance in Eq. (3). This approach is similar to _generalized policy iteration_(Sutton and Barto, 2018), which alternatively updates the value estimator and then improves the policy.

Next, we prove the existence of \(M_{}^{}\) in Lemma 5.1, which was not covered in Jain et al. (2021). This proof establishes a loose upper bound on the IS ratio \(\), limiting the divergence of the behavior policy \(\) from the target policy \(\) for effective off-policy variance estimation. This aligns with methods like TRPO (Schulman et al., 2015) and Retrace (Munos et al., 2016), which stabilize policy updates by controlling divergence.

**Lemma 5.1**.: _(Variance Function \(M\) Existence:) Given a discount factor \(0< 1\), the variance function \(M_{}^{}\) exists, if the below condition satisfies, \(_{a}[^{2}(s,a)]<}\) for all states._

Proof.: Proof in App. A.2. 

Note, the optimal \(\) for the objective in Eq. (2), might violate the above constraint on \(\); we empirically clip \(\) to mitigate this problem. Additionally, IS requires \((a|s)=0\) when \((a|s)=0\). We empirically ensure \((a|s)><<1\) for all actions. The same constraint is added for all the baselines for fair comparison.

Algorithm

We present GVFExplorer algorithm, detailed in Algorithm 1. Our approach uses two networks: \(Q_{}\) for value function and \(M_{w}\) for variance, each with \(N\) heads (one head for each GVF). Starting with a randomly initialized behavior policy, the agent observes cumulants for \(N\) GVFs at each step and updates \(Q_{}\) using off-policy TD. We use **Expected Sarsa**(Sutton & Barto, 2018) for both \(Q\) and \(M\), eliminating off-policy corrections. The target \(Q\) updates follow:

\[Q_{tar}(s_{t},a_{t},s_{t+1})=c_{t}+_{a^{}}(a^{}|s_{t+1 })Q_{}(s_{t+1},a^{}).\] (5)

We use the TD error from \(Q\)-learning, \(_{Q}=Q_{tar}-Q_{}\), to update target \(M\),

\[M_{tar}(s_{t},a_{t},s_{t+1})=_{Q}^{2}+^{2}_{a^{}}(a^{ }|s_{t+1})M_{w}(s_{t+1},a^{}).\] (6)

Both networks are updated via an MSE loss. The behavior policy is iteratively updated using the new variance estimates for \(K\) steps, with learned \(Q\) values used for MSE metrics in Eq. (1).

To ensure reliable estimates, we initialize \(M\) values to small non-zero constants and apply epsilon exploration, which decays over time, ensuring coverage of the state-action space. This guarantees that agents visit a broad range of state-action pairs early on, preventing issues of zero variance for unvisited pairs. We applied epsilon-exploration to both GVFExplorer and the baselines for fair comparison.

We also use techniques like experience replay Lin (1992) for data reuse and target networks for both \(Q\) and \(M\) to improve learning stability. Expected Sarsa is used consistently across all baselines for fair comparison. Refer to Algorithm 1 for further details.

```
0: Target policies \(_{i\{1, n\}}\), initial behavior policy \(_{1}\), replay buffer \(\), primary networks \(Q_{},M_{w}\) (small non-zero \(M\)), target networks \(Q_{},M_{}\), learning rates \(_{Q}\), \(_{M}\), mini-batch size \(b\), trajectory length \(T\), target update frequency \(l=100\), value/variance update frequencies \(p=4\), \(m=8\), training steps \(K\), exploration rates \(_{0}\), \(_{}\), \(_{}\)
1forenvironment step \(k=1, K\)do
2 Set exploration rate: \(_{k}=(_{},_{0}_{ }^{k})\)
3 Select action \(a_{t}_{k}(|s_{t})& 1-_{k}\\ &\)
4 Observe next state \(s_{t+1}\) and cumulants \(c_{t}=(n))\)
5 Store transition \((s_{t},a_{t},s_{t+1},c_{t})\) in \(\)
6if\(k\%p==0\)then
7 //Update the Value \(Q_{}\) network
8 Sample mini-batch of size \(b\) of transition \((s_{t},a_{t},s_{t+1},c_{t})\).
9 Update \(Q_{}\) using MSE loss \((Q_{tar}(s_{t},a_{t})-Q_{}(s_{t},a_{t}))^{2}\), where \(Q_{tar}\) is Eq. (5).
10
11 end for
12if\(k\%m==0\)then
13 //Update the Variance \(M_{w}\) network
14 Sample mini-batch of size \(b\) of transition \((s_{t},a_{t},s_{t+1},c_{t})\)
15 Update \(M_{w}\) using MSE loss \((M_{tar}(s_{t},a_{t})-M_{w}(s_{t},a_{t}))^{2}\), where \(M_{tar}\) is Eq. (6).
16
17 end for
18if\(k\%l==0\)then
19\(=w\) and \(=\) //Update both target networks weights
20
21 end if
22 //Update the behavior policy \(\) using the new Variance \(M_{w}\)
23 Behavior policy becomes: \(_{k+1}(a|s)=^{n}_{i}(a|s)^{2}M_{w}^{i}(s,a)}}{_ {a^{}}^{n}_{i}(a^{}|s)^{2}M_{w}^{i }(s,a^{})}}, s,a\).
24
25 end for
26Returns GVFs Values \(V_{i}(s)=_{a}_{i}(a|s)Q_{}^{i}(s,a)\) for \(i=\{1,,n\}\) ```

**Algorithm 1**GVFExplorer: Efficient Behavior Policy Iteration for Multiple GVFs EvaluationsExperiments

We investigate the empirical utility of our proposed algorithm in both discrete and continuous state environments. Our experiments are designed to answer the following questions: (a) How does GVFExplorer compare with the different baselines (explained below) in terms of convergence speed and estimation quality? (b) Can GVFExplorer handle a large number of GVFs evaluations? (c) Can GVFExplorer work with non-stationary GVFs which change with time? (d) Can GVFExplorer work with non-linear function approximations and complex Mujoco environments? 1

Baselines.We use Off-policy Expected Sarsa updates for parallel GVF estimations for all the experiments (including baselines) for fair comparison. We benchmark against several different **baselines**: (1) RoundRobin: uses a round-robin strategy sampling episodically from all target policies (2) MixturePolicy: Aggregated policy sampling from all target policies; (3) SR: a Successor Representation (SR) method using intrinsic reward of total change in SR and reward weights to learn behavior policy (McLeod et al., 2021). (4) BPS: behavior policy search method originally designed for single policy evaluation using a REINFORCE variance estimator (Hanna et al., 2017); we adapted it by averaging variance across multiple GVFs (similar to our objective). BPS results are limited to tabular settings due to scalability issues with it. (5) UniformPolicy: a uniform sampling policy over the action space. Implementation details and hyperparameters are in App. B.

Type of Cumulants.We experiment with three different types of cumulants, similar to McLeod et al. (2021) - **constant** with a fixed value; **distractor**, a _stationary_ signal with fixed mean and constant variance (normal distribution); **drifter**, a _non-stationary_ cumulant with zero-mean random walk with low variance (vary with time). Further description of cumulants is in App. B.2.

Experimental Settings.To answer the questions presented above, we consider different settings: **(Two Distinct Policies & Identical Cumulants):** In a tabular setting, we examine two GVFs with distinct target policies but identical _distractor cumulant_, \((_{1},c),(_{2},c)\). **(Two Distinct Policies & Distinct Cumulants):** In the same environment, we assess two GVFs with distinct target policy and distinct _distractor cumulant_ with different fixed means, \((_{1},c_{1}),(_{2},c_{2})\). **(Large Scale Evaluation with \(40\) distinct GVFs):** To verify the scalability of proposed method with high number of GVFs, we evaluate combinations of \(4\) different target policies \(_{1}_{4}\) with \(10\) different _constant cumulants_\(c_{1} c_{10}\), resulting in 40 GVFs. **(Non-Stationary Cumulants in FourRooms):** In FourRooms environment, we assess with two distinct GVFs - stationary distractor and non-stationary _drifter cumulant_\(-(_{1},c_{1}),(_{2},c_{2})\). **(Non-Linear Function Approximation):** In a continuous state environment with non-linear function approximator, we evaluate two distinct _distractor_ GVFs, \((_{1},c_{1}),(_{2},c_{2})\). **(Mujoco environments):** In Mujoco environments - walker and cheetah - evaluate different GVF tasks like walk, run and flip. Across these varied settings, we measure the averaged MSE across multiple GVFs.

### Tabular Experiments

We conducted experiments in \(20 20\) gridworld with four cardinal actions and a tabular \(20 20\) FourRooms environment for added complexity. The discount factor is \(=0.99\), and the environment is stochastic with a \(0.1\) probability of random movement. The cumulants are zero everywhere except for at the goals. Episode terminates after \(500\) steps or upon reaching the goal. True value function for MSE computation is calculated analytically \(V_{}=(I- P_{})^{-1}c_{}\). Detailed description of target policies and cumulants is provided in App. B.3. Table 1 summarizes the below results for tabular experiments.

In **Two Distinct Policies & Identical Cumulants**, we consider gridworld environment with _distractor_ cumulant at top left corner with a reward drawn from normal distribution. Fig. 1a shows the averaged MSE across the two GVFs, with GVFExplorer showing much lower MSE compared to baselines.

Next, in **Two Distinct Policies & Distinct Cumulants**, we consider two distinct _distractor_ cumulant (with different mean) GVFs placed at top-left and top-right corner respectively. Fig. 2a shows GVFExplorer with reduced MSEs compared to baselines. Figs. 2b and 2c qualitatively analyze the average absolute difference between true and estimated GVF values across states,\(_{_{i}}^{c_{i}}\|\), showing smaller errors (duller colors) for GVFExplorer. Fig. 8 (in App. B.3.2) presents the individual variance and MSE for both GVFs in GVFExplorer. Further, we conduct an ablation study to experiment with how GVFExplorer performance changes with poorer feature approximations. Fig. 10 (in App. B.3.3) shows that MSE increases as the feature quality deteriorates, but GVFExplorer remains robust with moderately coarse approximations.

For **Non-Stationary Cumulant in FourRooms**, we evaluate the performance in FourRooms (FR) environment (Sutton et al., 1999) with two distinct GVFs: stationary **distractor** cumulant and a non-stationary **drifter** cumulant which changes value over time. As shown in Fig. 0(b), GVFExplorer reduces MSE faster than other baselines, even with the non-stationary cumulant. Fig. 11 (in App. B.3.4) demonstrates the effectiveness of GVFExplorer in tracking the non-stationary cumulant signal in the later stages of learning.

In **Large Scale Evaluation with 40 Distinct GVFs**, we evaluate our method's scalability to large number of GVFs (refer App. B.3.5). We use **constant** cumulants with values ranging in \(\). Fig. 0(c) compares the average MSE across the GVFs, showing that GVFExplorer scales well with an increasing number of GVFs. In contrast, the SR baseline struggles with scalability due to the varying cumulant scales affecting the intrinsic reward (the summation of all SRs and reward weights) of behavior policy.

### Continuous State Environment with Non-Linear Function Approximation

We use a continuous grid environment that extends the tabular experiments to a continuous state space (similar to McLeod et al. (2021)) and four discrete actions. For **Non-Linear Function Approximation**, we consider two distinct GVFs with distractor cumulants. An **Experience Replay

Figure 1: **MSE Performance**: Averaged MSE over \(25\) runs with standard error in different experimental settings. GVFExplorer demonstrate notably lower MSE compared to the baselines.

Figure 2: **Two Distinct Policies & Distinct Cumulants**: Evaluate averaged MSE over 25 runs with two distinct distractor GVFs \((_{1},c_{1}),(_{2},c_{2})\) in gridworld. Green dots at top show two GVF goals. (a) Averaged MSE, (b) averaged absolute error in GVFs value predictions for baseline RoundRobin and (c) GVFExplorer. _The color bar uses log scale & vibrant colors indicate higher values._

**Buffer** with a capacity of 25K and a batch size of 64 is used for all experiments. Further details on computing true value functions using Monte Carlo and network architectures are in App. B.4.

Prioritized Experience Replay (PER).We investigate the integration of PER (Schaul et al., 2015) with our algorithm. Unlike the standard Experience Replay Buffer, which uniformly samples experiences, PER assigns priorities based on the TD error magnitude in the Q-network. PER and GVFExplorer are complementary approaches: PER re-weights the collected data in replay buffer based on the priority, while GVFExplorer adjusts the behavior policy to influence data collection.

Combining PER with GVFExplorer drastically lowers MSE compared to other baselines (even when compared to all baselines + PER). We use the absolute sum of TD errors across multiple GVF Q-functions as a priority metric for PER in all baselines, including GVFExplorer. Placing the priority on the TD error of the variance function in GVFExplorer yields less favorable results compared to priority on Q-function's TD error. In Fig. 3, we present the MSE for both standard experience replay (solid lines) and PER (dotted lines) for all algorithms. PER generally reduces MSE, but its integration with GVFExplorer shows much lower MSE. This is likely as GVFExplorer could over-sample high variance return samples, causing a skewed buffer distribution. PER's non-uniform sampling maintains a balanced data distribution, which helps in stringent MSE reduction. For the SR baseline, using the TD error in SR predictions as a priority for PER led to performance degradation, suggesting non-stationarity in SRs' TD errors might mislead PER to prioritize less relevant states under the current policy. The original SR work by McLeod et al. (2021) does not use PER in the experiments. For PER scenario, we qualitatively compare the absolute value error for baseline RoundRobin and GVFExplorer by discretizing the state space in Figs. 2(b) and 2(c) and observe that our algorithms results in smaller value prediction error. Further insights into the variance estimation by GVFExplorer is shown in Figs. 15 and 16 (App. B.4). Table 3((App. B.4) summarizes the results highlighting the performance of various algorithms.

### Mujoco Environments with Continuous State-Action Tasks

We use DM-Control (Tassa et al., 2018) based continuous state-action tasks to experiment with Mujoco environments, _Walker_ and _Cheetah_ domain. To expand the proposed method to continuous action environments, any policy-gradient (PG) based algorithm can be used. In our experiments, we use Soft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018) as a base PG method to incorporate the proposed variance-minimization objective.

A separate network for variance estimation is added to SAC. Further implementation details are provided in App. B.5. To experiment in _Walker_ environment, we use two GVF tasks, namely 'walk' and 'flip'. Similarly, for _Cheetah_ environment, we use 'walk' and 'run' GVF tasks. We also added KL regularize between the learned behavior policy and the given GVFs target policies to prevent divergence. We use MC to compute the true Q-value GVF estimates and compare the MSE between these MC values and the Q-critic network. We use the same Q-critic architecture for the baseline

Figure 3: **Non-Linear Function Approximation**: (a) Averaged MSE over \(50\) runs with standard error using **Experience Replay Buffer** (solid lines) and **PER** (dotted lines). GVFExplorer show lower MSE with both buffers. PER generally reduces MSE across all algorithms except SR. Log-scale absolute value error for RoundRobin (b) and GVFExplorer (c); GVFExplorer achieves smaller errors (vibrant colors represent higher values).

algorithms - UniformPolicy and RoundRobin - for fair comparison. In Fig. 4 we observe that GVFExplorer reduces MSE faster than the baselines.

## 8 Conclusion

We addressed the problem of parallel evaluations of multiple GVFs, each conditioned on a given target policy and cumulant. We developed a method to adaptively learn a behavior policy that uses a single experience stream to estimate all GVF values in parallel. The resulting behavior policy update selects the actions in proportion to the total variance of the return across GVFs. This guides the policy to frequently explore less understood areas (high variance in return), which helps to better estimate the mean return with fewer samples. Therefore, our approach lowers the overall MSE in GVF predictions while reducing the number of interactions required. We theoretically proved that each behavior policy update reduces or maintains the total prediction error. Empirically, we showed that GVFExplorer scales effectively with an increasing number of distinct GVFs, robustly handles non-stationary cumulants in a tabular setting, and adapts well to non-linear function approximation. Additionally, we showcased its performance in complex continuous state-action Mujoco environments, showing that GVFExplorer can be seamlessly integrated with existing policy-gradient methods.

Limitations and Future Work.One notable drawback of GVFExplorer is the increased time complexity, due to simultaneously learning two networks for value and variance estimation respectively. Additionally, GVFExplorer has not been evaluated in environments with significant difference in the cumulant value range. Such disparities could lead to varying variances, potentially resulting in oversampling areas with higher cumulant values. Calibration across cumulants may be necessary in these cases.

In this work, we focused on minimizing the total MSE, but other loss functions, such as weighted MSE could also be considered. However, weighted MSE requires prior knowledge about the weighting of errors in different GVFs, which is not readily available. A potential future direction could be to use variance scales to automatically adjust these weights to provide uniform MSE reduction across all GVFs. Looking ahead, we are interested in testing our approach with multi-dimensional cumulants and general state-dependent discount factors, as well as, extending the applicability of GVFExplorer to control settings where the target policies are unknown.