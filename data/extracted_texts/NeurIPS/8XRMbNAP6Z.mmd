# Near-Optimal \(k\)-Clustering in the Sliding Window Model

David P. Woodruff CMU

dwoodruf@cs.cmu.edu &Peilin Zhong

Google Research

peilinz@google.com &Samson Zhou

Texas A&M University

samsonzhou@gmail.com

###### Abstract

Clustering is an important technique for identifying structural information in large-scale data analysis, where the underlying dataset may be too large to store. In many applications, recent data can provide more accurate information and thus older data past a certain time is expired. The sliding window model captures these desired properties and thus there has been substantial interest in clustering in the sliding window model.

In this paper, we give the first algorithm that achieves near-optimal \((1+)\)-approximation to \((k,z)\)-clustering in the sliding window model, where \(z\) is the exponent of the distance function in the cost. Our algorithm uses \(,^{2+z})}\) words of space when the points are from \([]^{d}\), thus significantly improving on works by Braverman et. al. (SODA 2016), Borassi et. al. (NeurIPS 2021), and Epasto et. al. (SODA 2022).

Along the way, we develop a data structure for clustering called an online coreset, which outputs a coreset not only for the end of a stream, but also for all prefixes of the stream. Our online coreset samples \(,^{2+z})}\) points from the stream. We then show that any online coreset requires \((} n)\) samples, which shows a separation from the problem of constructing an offline coreset, i.e., constructing online coresets is strictly harder. Our results also extend to general metrics on \([]^{d}\) and are near-optimal in light of a \((})\) lower bound for the size of an offline coreset.

## 1 Introduction

Clustering is a fundamental procedure frequently used to help extract important structural information from large datasets. Informally, the goal of clustering is to partition the data into \(k\) clusters so that the elements within each cluster have similar properties. Classic formulations of clustering include the \(k\)-median and \(k\)-means problems, which have been studied since the 1950's . More generally, for a set \(X\) of \(n\) points in \(^{d}\), along with a metric \(\), a cluster parameter \(k>0\), and an exponent \(z>0\) that is a positive integer, the clustering objective can be defined by

\[_{C^{d},|C|=k}_{i=1}^{n}_{c C}(x_{i},c)^{z}.\]

When \(\) is the Euclidean distance, the problem is known as \((k,z)\)-clustering and more specifically, \(k\)-median clustering and \(k\)-means clustering, when \(z\) is additionally set to \(1\) and \(2\), respectively.

As modern datasets have significantly increased in size, attention has shifted to large-scale computational models, such as the streaming model of computation, that do not require multiple passes over the data. In the (insertion-only) streaming model, the points \(x_{1},,x_{n}\) of \(X\) arrive sequentially,and the goal is to output an optimal or near-optimal clustering of \(X\) while using space sublinear in \(n\), ideally space \(k\ (n,d)\), since outputting the cluster centers uses \(k\) words of space, where each word of space is assumed to be able to store an entire input point in \(^{d}\). There exist slight variants of the insertion-only streaming model and a long line of active research has been conducted on clustering in these models .

The sliding window model.Unfortunately, an important shortcoming of the streaming model is that it ignores the time at which a specific data point arrives and thus it is unable to prioritize recent data over older data. Consequently, the streaming model cannot capture applications in which recent data is more accurate and therefore considered more important than data that arrived prior to a certain time, e.g., Census data or financial markets. Indeed, it has been shown that for a number of applications, the streaming model has inferior performance  compared to the sliding window model , where only the most recent \(W\) updates in the stream comprise the underlying dataset. Here, \(W>0\) is a parameter that designates the window size of the active data, so that all updates before the \(W\) most recent updates are considered expired, and the goal is to aggregate statistics about the active data using space sublinear in \(W\). In the setting of clustering, where the data stream is \(x_{1},,x_{n}^{d}\), the active data set is \(X=\{x_{n-W+1},,x_{n}\}\) for \(n W\) and \(X=\{x_{1},,x_{n}\}\) otherwise. Thus the sliding window model is a generalization of the streaming model, depending on the choice of \(W\), and is especially relevant for time-sensitive settings, such as data summarization , event detection in social media , and network monitoring .

The sliding window model is especially relevant for applications in which computation _must_ be restricted to data that arrived after a certain time. Data privacy laws such as the General Data Protection Regulation (GDPR) mandate that companies cannot retain specific user data beyond a certain duration. For example, the Facebook data policy  states that user search histories are retained for \(6\) months, the Apple differential privacy overview  states that collected user information is retained for \(3\) months, and the Google data retention policy states that browser information may be stored for up to \(9\) months . These retention polices can be modeled by the sliding window model with the corresponding setting of the window parameter \(W\) and thus the sliding window model has been subsequently studied in a wide range of applications .

Clustering in the sliding window model.Because the clustering objective is not well-suited to popular frameworks such as the exponential histogram or the smooth histogram, there has been significant interest in clustering in the sliding window model. We now describe the landscape of clustering algorithms in the sliding window model; these results are summarized in Table 1. In 2003,  first gave a \(2^{O(1/)}\)-approximation algorithm for \(k\)-median clustering in the sliding window model using \(O(W^{2}^{2}W)\) words of space, where \((0,)\) is an input parameter. Subsequently,  gave an \(O(1)\)-approximate bicriteria algorithm using \(2k\) centers and \(k^{2}\ (W)\) space for the \(k\)-median problem in the sliding window model. The question of whether there exists a \((k W)\) space algorithm for \(k\)-clustering on sliding windows remained open until  gave constant-factor approximation sliding window algorithms for \(k\)-median and \(k\)-means using \(O(k^{3}^{6}W)\) space and  gave constant-factor approximation algorithms for \(k\)-center clustering using \(O(k)\) space, where \(\) is the aspect ratio, i.e., the ratio of the largest to smallest distances between any pair of points. Afterwards,  gave a \(C\)-approximation algorithm for some constant \(C>2^{14}\), though it should be noted that their main contribution was the first constant-factor approximation algorithm for \(k\)-clustering using space linear in \(k\), i.e., \(k\ (W,)\) space, and thus they did not attempt to optimize the constant \(C\). Recently,  gave the first \((1+)\)-approximation algorithm for \((k,z)\)-clustering using \()}{^{3}}\ (W,,)\) words of space, for some constant \(C 7\). Using known dimensionality reduction techniques, i.e., , the algorithm's dependence on \(d^{C}\) can be removed in exchange for a \(}\ (W,)\) overhead. However, neither the \(d^{C}\) dependency nor the \(}\ (W,)\) trade-off is desirable for realistic settings of \(d\) and \(\) for applications of \(k\)-clustering on sliding windows. In particular, recent results have achieved efficient summarizations, i.e., coresets, for \(k\)-median and \(k\)-means clustering in the offline setting using \((} n)\) words of space  when the input is from \([]^{d}\) and it is known that this is near-optimal, i.e., \((} n)\) samples are necessary to form coresets for \((k,z)\)-clustering  in that setting. Thus a natural question is to ask whether such near-optimal space bounds can be achieved in the sliding window model.

### Our Contributions

In this paper, we answer the question in the affirmative. That is, we give near-optimal space algorithms for \(k\)-median and \(k\)-means clustering in the sliding window model. In fact, we give more general algorithms for \((k,z)\)-clustering in the sliding window that nearly match the space used by the offline coreset constructions of :

**Theorem 1.1**.: _There exists an algorithm that samples \(,^{2+z})}\ \ \) points and with high probability, outputs a \((1+)\)-approximation to \((k,z)\)-clustering for the Euclidean distance on \([]^{d}\) in the sliding window model._

In particular, our bounds in Theorem1.1 achieve \(}\ \ \) words of space for \(k\)-median clustering and \(k\)-means clustering, i.e., \(z=1\) and \(z=2\), respectively, matching the lower bounds of  up to polylogarithmic factors.

Moreover, our algorithm actually produces a coreset, i.e., a data structure that approximately answers the clustering cost of the underlying dataset with respect to any set of \(k\) centers, not just the optimal \(k\) centers.

**Theorem 1.2**.: _There exists an algorithm that samples \(,^{2+z})}\ \ \) points and with high probability, outputs a \((1+)\)-coreset to \((k,z)\)-clustering in the sliding window model for general metrics on \([]^{d}\)._

We emphasize that the guarantees of Theorem1.2 are for general metrics on \([]^{d}\), such as \(L_{p}\) metrics. Note that in light of the properties of coresets, the guarantee of Theorem1.1 follows from taking a coreset for \((k,z)\)-clustering on Euclidean distances and then using an offline algorithm for \((k,z)\)-clustering for post-processing after the data stream, i.e., see Theorem2.4.

Along the way, we provide a construction for a \((1+)\)-online coreset for \((k,z)\)-clustering for general metrics on \([]^{d}\). An online coreset for \((k,z)\)-clustering is a data structure on a data stream that will not only approximately answer the clustering cost of the underlying dataset with respect to any set of \(k\) centers, but also approximately answer the clustering cost of _any prefix of the data stream_ with respect to any set of \(k\) centers.

**Theorem 1.3**.: _There exists an algorithm that samples \(,^{2+z})}\ \ \) points and with high probability, outputs a \((1+)\)-online coreset for \((k,z)\)-clustering._

We remark that Theorem1.3 further has the attractive property that once a point is sampled into the online coreset at some point in the stream, then the point irrevocably remains in the online coreset. That is, the online coreset essentially satisfies two different definitions of online: 1) the data structure is a coreset for any prefix of the stream and 2) points sampled into the data structure will never be deleted from the data structure.

We further remark that due to leveraging the coreset construction of , we can similarly trade a factor of \(}\) for a \((k)\) in the guarantees of Theorem1.1, Theorem1.2, and Theorem1.3.

By contrast, the lower bound by  states that any offline coreset construction for \(k\)-means clustering only requires \((})\) points. This lower bound was later strengthened to \((})\) points by , for which matching upper bounds are given by . Thus our online coreset constructions are near-optimal in the \(k\) and \(\) dependencies for \(z>1\) and nearly match the best known offline constructions for \(z=1\).

  Reference & Accuracy & Space & Setting \\ 
 & \(2^{O(1/)}\) & \(O(}W^{2}^{2}W)\) & \(k\)-median, \((0,)\) \\ 
 & \(C>2\) & \(O(k^{3}^{6}W)\) & \(k\)-median and \(k\)-means \\ 
 & \(C>2^{14}\) & \(k\ (W,)\) & \((k,z)\)-clustering \\ 
 & \((1+)\) & \(})}{^{3}}\ (W,, )\), \(C 7\) & \((k,z)\)-clustering \\  Our work & \((1+)\) & \(,^{2+z})}\ \ \) & \((k,z)\)-clustering \\  

Table 1: Summary of \((k,z)\)-clustering results in the sliding window model for input points in \([]^{d}\) on a window of size \(W\)It is thus a natural question to ask whether our polylogarithmic overheads in Theorem1.3 are necessary for an \((1+)\)-online coreset. We show that in fact, a logarithmic overhead is indeed necessary to maintain a \((1+)\)-online coreset.

**Theorem 1.4**.: _Let \((0,1)\). For sufficiently large \(n\), \(d\), and \(\), there exists a set \(X[]^{d}\) of \(n\) points \(x_{1},,x_{n}\) such that any \((1+)\)-online coreset for \(k\)-means clustering on \(X\) requires \((} n)\) points._

We emphasize that combined with existing offline coreset constructions , Theorem1.4 shows a separation between the problems of constructing offline coresets and online coresets. That is, the problem of maintaining a data structure that recovers coresets for all prefixes of the stream is provably harder than maintaining a coreset for an offline set of points.

### Technical Overview

In this section, we give a high-level overview of our techniques. We also describe the limitations of many natural approaches.

Shortcomings of histograms and sensitivity sampling.A first attempt at clustering in the sliding window model might be to adapt the popular exponential histogram  and smooth histogram techniques . These frameworks convert streaming algorithms to sliding window algorithms in the case that the objective function is smooth, which informally means that once a suffix of a data stream becomes a good approximation of the overall data stream, then it always remains a good approximation, regardless of the values of new elements that arrive in the stream. Unfortunately,  showed that the \(k\)-clustering objective function is not smooth and thus these histogram-based frameworks cannot work. Nevertheless, they gave the first constant-factor approximation by showing that the \(k\)-clustering objective function is almost-smooth using a generalized triangle inequality, which inherently loses constant factors and thus will not suffice for our goal of achieving a \((1+)\)-approximation.

Another approach might be to adapt the popular sensitivity sampling framework of coreset construction . The sensitivity sampling framework assigns a value to each point, called the sensitivity, which intuitively quantifies the "importance" of that point, and then samples each point with probability proportional to its sensitivity.  observed that sliding window algorithms can be achieved from _online_ sensitivity sampling, where the importance of each point is measured against the prefix of the stream, and then running the process in reverse at each time, so that more emphasis is placed on the suffix of the sliding window. At a high level, this is the intuition taken by , which leverage data structures that prioritize more recent elements of the data stream. However, it is not known how to achieve optimal bounds simply using sensitivity sampling, and indeed the optimal coreset constructions use slightly more nuanced sampling schemes .

Sliding window algorithms from online coresets.Instead, we recall an observation by , who noted that deterministic constructions for online coresets for linear algebraic problems can be utilized to obtain sliding window algorithms for the corresponding linear algebraic problems. We first extend this observation to randomized constructions for online coresets for \(k\)-clustering problem.

The intuition is quite simple. Given an \((1+)\)-online coreset algorithm for a \(k\)-clustering problem on a data stream of length \(n\) from \(^{d}\) that stores \(S(n,d,k,,)\) weights points and succeeds with probability \(1-\), we store the \(S(n,d,k,^{},^{})\) most recent points in the stream, where \(^{}=O()\) and \(^{}=(n)}\). We then feed the \(S(n,d,k,^{},^{})\) points to the online coreset construction in _reverse order of their arrival_. Since the online coreset preserves all costs for all prefixes of its input, then the resulting data structure will preserve all costs for all _suffixes_ of the data stream. To extend this guarantee to the entire stream, including the sliding window, we can then use a standard merge-and-reduce framework. It thus remains to devise a \((1+)\)-online coreset construction for \(k\)-clustering with near-optimal sampling complexity.

Online coreset construction.To that end, our options are quite limited, as to the best of our knowledge, the only offline coreset constructions using \((} n)\) words of space when the input is from \([]^{d}\) are due to . Fortunately, although the analyses of correctness for these sampling schemes are quite involved, the constructions themselves are quite accessible. For example,  first uses an \((,)\)-approximation, i.e., a clustering that achieves \(\)-approximation to the optimal cost but uses \( k\) centers, to partition the underlying dataset \(X\) into disjoint concentric rings around each of the \( k\) centers. These rings are then gathered into groups and it is shown that by independently sampling a fixed number of points with replacement from each of the groups suffices to achieve a \((1+)\)-coreset. Their analysis argues that the contribution of each of the groups toward the overall \(k\)-clustering cost is preserved through an expectation and variance bounding argument, and then taking a sophisticated union bound over a net over the set of possible centers. Thus their argument still holds when each point of the dataset is independently sampled by the data structure with probability proportional to the probability it would have been sampled by the group. Moreover, independently sampling each point with a higher probability can only decrease the variance, so that correctness is retained, though we must also upper bound the number of sampled points. Crucially, independently sampling each point can be implemented in the online setting and the probability of correctness can be boosted to union bound over all times in the stream, which facilitates the construction of our \((1+)\)-online coreset, given an \((,)\)-approximation.

Consistent \((,)\)-approximation.It seemingly remains to find \((,)\)-approximations for \(k\)-clustering at all times in the stream. A natural approach would be to use an algorithm that achieves a \((,)\)-approximation at a certain time in the stream with constant probability, e.g., , boost the probability of success to \(1-(n)}\), and the union bound to argue correctness over all times in the stream. However, a subtle pitfall here is that the rings and groups in the offline coreset construction of  are with respect to a specific \((,)\)-approximation. Hence their analysis would no longer hold if a point \(x_{t}\) was assigned to cluster \(i_{1}\) at time \(t\) when the sampling process occurs but then assigned to cluster \(i_{2}\) at the end of the stream. Therefore, we require a consistent \((,)\)-approximation, so that once the algorithm assigns point \(x_{t}\) to cluster \(i\), then the point \(x_{t}\) will always remain in cluster \(i\) even if a newer and closer center is subsequently opened later in the stream. To that end, we invoke a result of  that analyzes the popular Meyerson online facility location algorithm, along with a standard guess-and-double approach for estimating the input parameter to the Meyerson subroutine.

Lower bound.The intuition for our lower bound that any \((1+)\)-online coreset for \((k,z)\)-clustering requires \((})\) is somewhat straightforward and in a black-box manner. We first observe that  showed the existence of a set \(X\) of \((})\) unit vectors in \(^{d}\) such that any coreset with \(o(})\) samples provably cannot accurately estimate the \((k,z)\)-clustering cost for a set \(C\) of \(k\) unit vectors.

Since an online \((1+)\)-coreset must answer queries on all prefixes of the stream, we embed \(( n)\) instances of \(X\). We first increase the dimension by a \( n\) factor so that each of these instances can have disjoint support. We then give each of the instances increasingly exponential weight to force the data structure to sample \((})\) points for each instance. Specifically, we insert \(^{i}\) copies of the \(i\)-th instance of \(X\), where \(>1\) is some constant. Because the weight of the \(i\)-th instance is substantially greater than the sum of the weights of all previous instances, then any \((1+)\)-online coreset must essentially be a \((1+)\)-offline coreset for the \(i\)-th instance, thus requiring \((})\) points for the \(i\)-th instance. This reasoning extends to all \(( n)\) instances, thus showing that any online \((1+)\)-coreset requires \((} n)\) points.

## 2 Algorithm

In this section, we describe our sliding window algorithm for \(k\)-clustering. We first overview the construction of an online \((1+)\) coreset for \((k,z)\)-clustering under general discrete metrics. We then describe how our online coreset construction for \((k,z)\)-clustering on general discrete metric spaces can be used to achieve near-optimal space algorithms for \((k,z)\)-clustering in the sliding window model.

Online \((1+)\)-coreset.We first recall the following properties from the Meyerson sketch, which we formally introduce in A.

**Theorem 2.1**.: _[_8_]_ _Given an input stream \(x_{1},,x_{n}^{d}\) defining a set \(X[]^{d}\), there exists an online algorithm MultMeyerson that with probability at least \(1-(n)}\):_1. _on the arrival of each point_ \(x_{i}\)_, assigns_ \(x_{i}\) _to a center in_ \(C\) _through a mapping_ \(:X C\)_, where_ \(C\) _contains at most_ \(O(2^{2z}k n)\) _centers_
2. \(_{x X}\|x_{i}-(x_{i})\|_{2}^{z} 2^{z+7}_{|S|  k}(X,S)\)__
3. MultMeyerson _uses_ \(O(2^{z}k^{3}(nd))\) _words of space_

We also use the following notation, adapted from  to the online setting.

Let \(\) be an \((,)\)-approximation for a \(k\)-means clustering on an input set \(X[]^{d}\) and let \(C_{1},,C_{ k}\) be the clusters of \(X\) induced by \(\). Suppose the points of \(X\) arrive in a data stream \(S\). For a fixed \(>0\), define the following notions of rings and groups:

* The average cost of cluster \(C_{i}\) is denoted by \(_{C_{i}}:=(C_{i},)}{|C_{i}|}\).
* For any \(i,j\), the ring \(R_{i,j}\) is the set of points \(x C_{i}\) such that \(2^{j}_{C_{i}}(x,)<2^{j+1}_{C_{i}}\). For any \(j\), \(R_{j}= R_{i,j}\).
* The inner ring \(R_{I}(C_{i})=_{j 2z}R_{i,j}\) is the set of points of \(C_{i}\) with cost at most \(()^{2z}_{C_{i}}\). More generally for a solution \(\), let \(R_{I}^{}\) denote the union of the inner rings induced by \(\).
* The outer ring \(R_{O}(C_{i})=_{j 2z}R_{i,j}\) is the set of points of \(C_{i}\) with cost at least \(()^{2z}_{C_{i}}\). More generally for a solution \(\), let \(R_{O}^{}\) denote the union of the outer rings induced by \(\).
* The main ring \(R_{M}(C_{i})\) is the set of points of \(C_{i}\) that are not in the inner or outer rings, i.e., \(R_{M}(C_{i})=C_{i}(R_{I}(C_{i}) R_{O}(C_{i})\).
* For any \(j\), the group \(G_{j,b}\) consists of the \((2^{b-1}+1)\)-th to \((2^{b})\)-th points of each ring \(R_{i,j}\) that arrive in \(S\).
* For any \(j\), we use \(G_{j,}\) to denote the union of the groups with the smallest costs, i.e., \[G_{j,}=\{x| i,x R_{i,j},(R_{i,j}, )<2()^{z}(R_{j},)}{ k}\}.\]
* The outer groups \(G_{b}^{O}\) partition the outer rings \(R_{O}^{}\) so that \[G_{b}^{O}=\{x| i,x C_{i},() ^{z}(R_{O}^{},)}{ k} 2^ {b}(R_{O}(C_{i}),)<( {4z})^{z}(R_{O}^{},)}{  k} 2^{b+1}\}.\]
* We define \(G_{}^{O}=_{b 0}G_{b}^{O}\) and \(G_{}^{O}=_{b z}G_{b}^{O}\).

```
0: Points \(x_{1},,x_{n}[]^{d}\)
0: A set \(W\) of weighted points and timestamps
1: Initiate an instance of \((,)\)-bicriteria algorithm MultMeyerson
2:\(,^{})}{( ^{2},^{2})}^{2}(k||+ + n)^{2}\)
3:\(W\)
4:for each point \(x_{t}\), \(t[n]\)do
5: Let \(c_{i}\) be the center assigned for \(x_{t}\) by MultMeyerson
6: Let \(2^{j}\|x_{t}-c_{i}\|_{2}^{z}<2^{j+1}\) for \(j\)
7: Let \(b\) so that the number of points in \(R_{i,j}\) is between \(2^{b-1}+1\) and \(2^{b}\)
8: Let \(r_{t}\) be the number of points in \(G_{j,b}\) at time \(t\)
9:\(p_{x}(} n,1)\)
10: With probability \(p_{x}\), add \(x\) to \(W\) with timestamp \(t\) and weight \(}\)
11:return\(W\)
```

**Algorithm 1**RingSample

We then adapt the offline coreset construction of  to an online setting at the cost of logarithmic overheads, which suffice for our purpose. The algorithm (Algorithm 1) has the following guarantees:

**Lemma 2.2**.: _Let \(\) be an \(\)-approximate centroid set for a fixed group \(G\). There exists an algorithm RingSample that samples_

\[O(,^{2})}{(^{2},^ {2})}^{2}(k||++ n)^{2}n^{2}^{2})\]

_points and with high probability, outputs a \((1+)\)-online coreset for the \(k\)-means clustering problem._

Informally, an approximate centroid set is a set of possible points so that taking the centers from this set generates an approximately accurate solution (see Appendix B for a formal definition). To bound \(||\), we construct and apply a terminal embedding to project each point to a lower dimension and then appeal to known bounds for approximate centroid sets in low-dimensional Euclidean, thereby giving our online coreset algorithm with the guarantees of Theorem 1.3.

Sliding window model.We first recall a standard approach for using offline coreset constructions for insertion-only streaming algorithms. Suppose there exists a randomized algorithm that produces an online coreset algorithm that uses \(S(n,,)\) points for an input stream of length \(n\), accuracy \(\), and failure probability \(\), where for the ease of discussion, we omit additional dependencies. A standard approach for using coresets on insertion-only streams is the merge-and-reduce approach, which partitions the stream into blocks of size \(S(n,,(n)})\) and builds a coreset for each block. Each coreset is then viewed as the leaves of a binary tree with height at most \( n\), since the binary tree has at most \(n\) leaves. Then at each level of the binary tree, for each node in the level, a coreset of size \(S(n,,(n)})\) is built from the coresets representing the two children of the node. Due to the mergeability property of coresets, the coreset at the root of the tree will be a coreset for the entire stream with accuracy \((1+)^{ n}(1+)\) and failure probability \(\).

This approach fails for sliding window algorithms because the elements at the beginning of the data stream can expire, and so coresets corresponding to earlier blocks of the stream may no longer accurate, which would result in the coreset at the root of the tree also no longer being accurate. On the other hand, suppose we partition the stream into blocks consisting of \(S(n,,(n)})\) elements as before, but instead of creating an offline coreset for each block, we can create an online coreset for the elements _in reverse_. That is, since the elements in each block are explicitly stored, we can create offline an artificial stream consisting of the elements in the block in reverse and then give the artificial stream as input to the online coreset construction. Note that if we also first consider the "latter" coreset when merging two coresets, then this effectively reverses the stream. Moreover, by the correctness of the online coreset, our data structure provides correctness over any prefix of the reversed stream, or equivalently, any suffix of the stream and specifically, correctness over the sliding window. We thus further adapt the merge-and-reduce framework to show that randomized online coresets for problems in clustering can also be used to achieve randomized algorithms for the corresponding problems in the sliding window model. We formalize this approach in Algorithm 2.

**Theorem 2.3**.: _Let \(x_{1},,x_{n}\) be a stream of points in \([]^{d}\), \(>0\), and let \(X=\{x_{n-W+1},,x_{n}\}\) be the \(W\) most recent points. Suppose there exists a randomized algorithm that with probability at least \(1-\), outputs an online coreset algorithm for a \(k\)-clustering problem with \(S(n,d,k,,)\) points. Then there exists a randomized algorithm that with probability at least \(1-\), outputs a coreset for the \(k\)-clustering problem in the sliding window model with \(O(S(n,d,k,,})  n)\) points._

By Theorem 1.3 and Theorem 2.3, we have:

**Theorem 2.4**.: _There exists an algorithm that samples \(,^{2+})}\ \ \) points and with high probability, outputs a \((1+)\)-coreset to \((k,z)\)-clustering in the sliding window model._

Using an offline algorithm for \((k,z)\)-clustering for post-processing after the data stream, we have Theorem 1.1.

## 3 Experimental Evaluations

In this section, we conduct simple empirical demonstrations as proof-of-concepts to illustrate the benefits of our algorithm. Our empirical evaluations were conducted using Python 3.10 using a 64-bitoperating system on an AMD Ryzen 7 5700U CPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. The general approach to our experiments is to produce a data stream \(S\) that defines dataset \(X\), whose generation we describe below, as well as in Appendix F. We then compare the performance of a simplified version of our algorithm with various state-of-the-art baselines.

Baselines.Our first baseline (denoted off for offline) is the simple Lloyd's algorithm on the entire dataset \(X\), with multiple iterations using the k-means++ initialization. This is a standard approach for finding a good approximation to the optimal clustering cost, because finding the true optimal centers requires exponential time. Because this offline Lloyd's algorithm has access to the entire dataset, the expected behavior is that this algorithm will have the best objective, i.e., smallest clustering cost. However, we emphasize that this algorithm requires storing the entire dataset \(X\) in memory and thus its input size is significantly larger than the sublinear space algorithms.

To compare with the offline Lloyd's algorithm, we run a number of sublinear space algorithms. These algorithms generally perform some sort of processing on the datastream \(X\) to create a coreset \(C\). We normalize the space requirement of these algorithms by permitting each algorithm to store \(m\) points across specific ranges of \(m\). We then run Lloyd's algorithm on the coreset \(C\), with the same number of iterations using the k-means++ initialization.

Our first sublinear space algorithm is uniform sampling on the dataset \(X\). That is, we form \(C\) by uniformly sampling \(m\) points from \(X\), before running Lloyd's algorithm. We use uni to denote this algorithm whose first step is based on uniformly sampling. Our second sublinear space algorithm is the importance sampling approach used by histogram-based algorithms, e.g., . These algorithms perform importance sampling, i.e., sample points into the coreset \(C\) with probability proportional to their distances from existing samples and delete points once the clustering cost of \(C\) is much higher than the clustering cost of the dataset \(X\). We use \(\) to denote this algorithm that is based on the histogram frameworks for sliding windows.

Our final algorithm is a simplification of our algorithm. As with the histogram-based algorithm, we perform importance sampling on the stream \(S\) to create the coreset \(C\) of size \(m\). Thus we do not implement the ring and group sampling subroutines in our full algorithm. However, the crucial difference compared to the histogram-based approach is that we forcefully discard points of \(C\) that have expired. We use \(\) to denote this algorithm whose first step is based on importance sampling.

Dataset.We first describe the methodology and experimental setup of our empirical evaluation on a real-world dataset with an amount of synthetic noise before detailing the experimental results. The first component of our dataset consists of the points of the SKIN (Skin Segmentation) dataset \(X^{}\) from the publicly available UCI repository , which was also used in the experiments of . The dataset \(X^{}\) consists of \(245,057\) points with four features, where each point refers to a separate image, such that the first three features are constructed over BGR space, and the fourth feature is the label for whether or not the image refers to a skin sample. We subsequently pre-process each dataset to have zero mean and unit standard deviation in each dimension.

We then form our dataset \(X\) by augmenting \(X^{}\) with \(201\) points in four-dimensional space, where \(100\) of these points were drawn from a spherical Gaussian with unit standard deviation in each direction and centered at \((-10,10,0,0)\) and \(100\) of these points were drawn from a spherical Gaussian with unit standard deviation in each direction and centered at \((10,-10,0,0)\). The final point of \(X\) was drawn from a spherical Gaussian with unit standard deviation centered at \((500,500,0,0)\). Thus our dataset \(X\) has dimensions \(n=245,258\) and \(d=4\). We then create the data stream \(S\) by prepending two additional points drawn from spherical Gaussians with standard deviation \(2.75\) centered at \((-10,10,0,0)\) and \((-10,-10,0,0)\) respectively, so that the stream has length \(245,260\). We set the window length to be \(245,258\) in accordance with the "true" data set, so that the first two points of the stream will be expired by the data stream.

Experimental setup.For each of the instances of Lloyd's algorithm, either on the entire dataset \(X\) or the sampled coreset \(C\), we use 10 iterations using the k-means++ initialization. While the offline Lloyd's algorithm stores the entire dataset \(X\) of \(245,258\) points in memory, we only allow each of the sublinear-space algorithms to store a fixed \(m\) points. We compare the algorithms across \(m\{5,10,15,20,25,30\}\) and \(k\{2,3,4,5,6,7,8,9,10\}\). Note that in the original dataset, each of the points has a label for either skin or non-skin, which would be reasonable for \(k=2\). However, due to the artificial structure possibly induced by the synthetic noise, it also makes sense to other values of \(k\). In particular, preliminary experiments from uniform sampling by the elbow method indicated that \(k=3\) would be a reasonable setting. Thus we fix \(k=3\) while varying \(m\{5,10,15,20,25,30\}\) and we arbitrarily fix \(m=25\) while varying \(k\{2,3,4,5,6,7,8,9,10\}\).

Experimental results.For each choice of \(m\) and \(k\), we ran each algorithm \(30\) times and tracked the resulting clustering cost. Our algorithm demonstrated superior performance than the other sublinear-space algorithms across all values of \(m\{5,10,15,20,25,30\}\) and \(k\{2,3,4,5,6,7,8,9,10\}\), and was even quite competitive with the offline Lloyd's algorithm, even though our algorithm only used memory size \(m 30\), while the offline algorithm used memory \(245,258\).

Uniform sampling performed well for \(k=2\), which in some case captures the structure imposed on the data through the skin vs. non-skin label, but for larger \(k\), the optimal solutions start placing centers to handle the synthetic noise, which may not be sampled by uniform sampling. Thus uniform sampling performed relatively poorly albeit quite stably for larger \(k\). In contrast, the histogram-based algorithm performed poorly for small \(k\) across all our ranges of \(m\), due to sampling the extra points in \(S X\), so that the resulting Lloyd's algorithm on \(C\) moved the centers far away from the optimal centers of \(X\). On the other hand, the histogram-based algorithm performed well for larger \(k\), likely due to additional centers that could be afforded to handle the points in \(S X\). We plot our results in Figure 1 and defer additional experiments to Appendix F.

## 4 Conclusion

In this paper, we give an algorithm outputs a \((1+)\)-approximation to \((k,z)\)-clustering in the sliding window model, while using \(,^{2+z})}\) polylog\(\) words of space when the points are from \([]^{d}\). Our algorithm not only improves on a line of work , but also nearly matches the space used by the offline coreset constructions of , which is known to be near-optimal in light of a \((})\) lower bound for the size of an offline coreset .

We also give a lower bound that shows a logarithmic overhead in the number of points is needed to maintain a \((1+)\)-online coreset compared to a \((1+)\)-coreset. That is, we gave in Theorem 1.4 a set \(X[]^{d}\) of \(n\) points \(x_{1},,x_{n}\) such that any \((1+)\)-online coreset for \(k\)-means clustering on \(X\) requires \((} n)\) points. However, this does not rule out whether the \( n\) overhead is necessary for \((k,z)\)-clustering in the sliding window model, since a sliding window algorithm does not necessarily need to maintain an online coreset. We leave this question as a possible direction for future work.