# Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models

Jierun Chen

HKUST

jcheneh@cse.ust.hk

&Fangyun Wei

Microsoft Research Asia

fawe@microsoft.com

&Jinjing Zhao

The University of Sydney

jzha0100@uni.sydney.edu.au

&Sizhe Song

HKUST

ssongad@cse.ust.hk

&Bohuai Wu

HKUST

bwual@cse.ust.hk

&Zhuoxuan Peng

HKUST

zpengac@cse.ust.hk

&S.-H. Gary Chan

HKUST

gchan@cse.ust.hk

&Hongyang Zhang

University of Waterloo

hongyang.zhang@uwaterloo.ca

Corresponding author.

###### Abstract

Referring expression comprehension (REC) involves localizing a target instance based on a textual description. Recent advancements in REC have been driven by large multimodal models (LMMs) like CogVLM, which achieved 92.44% accuracy on RefCOCO. However, this study questions whether existing benchmarks such as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive capabilities. We begin with a manual examination of these benchmarks, revealing high labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg, which undermines the authenticity of evaluations. We address this by excluding problematic instances and reevaluating several LMMs capable of handling the REC task, showing significant accuracy improvements, thus highlighting the impact of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC benchmark, specifically designed to evaluate modern REC models. Ref-L4 is distinguished by four key features: 1) a substantial sample size with 45,341 annotations; 2) a diverse range of object categories with 365 distinct types and varying instance scales from 30 to 3,767; 3) lengthy referring expressions averaging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique words. We evaluate a total of 24 large models on Ref-L4 and provide valuable insights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as our Ref-L4 benchmark and evaluation code, are available at https://github.com/JierunChen/Ref-L4.

## 1 Introduction

Referring expression comprehension (REC)  involves the task of localizing a specific target instance based on a given textual description. The advancement of REC has been significantly propelled by the superior language processing capabilities of large language models (LLMs) . This progress is particularly evident in the exceptional performance of large multimodal models (LMMs)  on well-known benchmarks such as RefCOCO , RefCOCO+ , and RefCOCOg . These modelshave demonstrated remarkable accuracy, with CogVLM , for instance, achieving an impressive accuracy rate of 92.44% on the RefCOCO benchmark.

This paper begins with a critical question: do existing REC benchmarks truly capture the comprehensive capabilities of LMMs? The foundational benchmarks, RefCOCO , RefCOCO+ , and RefCOCOg , were introduced sequentially in 2015, 2016, and 2016, respectively. In RefCOCO, the referring expressions are notably succinct, ranging from single words like "_lady_" and "_yellow_" to brief descriptions such as "_far left person_" and "_white shirt_". RefCOCO+ intentionally excludes locational prepositions commonly found in RefCOCO, favoring short yet semantically rich expressions like "_plastic cup with just ice_" and "_man on screen_". Conversely, RefCOCOg provides more elaborate annotations, including examples such as "_a table of food, with plates, a pizza, pitchers, and glasses_" and "_a red and white checked table with two wooden chairs_". These variations highlight the evolution and complexity of referring expressions across different benchmarks, raising the question of whether they can effectively assess the nuanced capabilities of modern LMMs in understanding diverse linguistic inputs and associating languages with visual elements.

**Labeling Error Rates of Existing Benchmarks.** To begin, we manually assess the labeling error rates of the validation and test sets in RefCOCO, RefCOCO+, and RefCOCOg, discovering a high error rate across these benchmarks. The labeling errors include, typos, misalignment between referring expressions and target instances, as well as inaccurate bounding box annotations, as depicted in Section A. As illustrated in Table 1, the labeling error rates for RefCOCO, RefCOCO+, and RefCOCOg are 14%, 24%, and 5%, respectively, indicating that evaluations performed on these benchmarks may lack authenticity.

**Reevaluation on RefCOCO, RefCOCO+ and RefCOCOg.** In response, we manually exclude the problematic instances from the validation and test sets of RefCOCO, RefCOCO+, and RefCOCOg. Subsequently, we reevaluate four LMMs capable of handling the REC task--namely ONE-PEACE , OFA-L , Qwen-VL , and CogVLM-Grounding --on both the cleaned and original versions of these datasets, as shown in Table 2. Across all models and cleaned benchmarks, we observe a significant accuracy improvement, ranging from 1.57 to 3.08, compared to their performance on the original versions. This demonstrates that noise in the benchmarks has impacted the models' true capabilities. _To support further research in the REC field, we release the cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg._

**Ref-L4: A Comprehensive REC Benchmark for Modern LMM Evaluation.** We present Ref-L4, where L4 signifies four key aspects: a Large number of testing samples, Large diversity in object categories and instance scales, Long referring expressions, and a Large vocabulary. These features

  Benchmark & Annotations & Errors & Labeling Error Rate \\  RefCOCO  & 21,586 & 3,054 & 14\% \\ RefCOCO+  & 21,373 & 5,201 & 24\% \\ RefCOCOg  & 14,498 & 675 & 5\% \\  

Table 1: Statistics of the labeling error rates for RefCOCO, RefCOCO+, and RefCOCOg, respectively. For each benchmark, the statistics are conducted on the combination of the validation and test sets.

   & ONE- & ^{}\)} &  &  &  CogVLM- \\ Grounding  \\  } \\   RefCOCO  & 92.15 & & 89.85 & 85.13 & 88.51 & 92.44 \\ RefCOCO (Cleaned) & 94.11 (**+1.96**) & 92.06 (**+2.22**) & 87.95 (**+2.81**) & 90.68 (**+2.18**) & 94.58 (**+2.13**) \\  RefCOCO+  & 88.14 & 85.06 & 77.56 & 82.52 & 88.55 \\ RefCOCO+ (Cleaned) & 90.79 (**+2.66**) & 87.38 (**+2.32**) & 80.50 (**+2.94**) & 85.60 (**+3.08**) & 91.43 (**+2.87**) \\  RefCOCOg  & 89.18 & 84.77 & 79.25 & 85.11 & 90.67 \\ RefCOCOg (Cleaned) & 90.75 (**+1.57**) & 86.39 (**+1.62**) & 80.89 (**+1.64**) & 86.79 (**+1.68**) & 92.36 (**+1.68**) \\  

Table 2: The performance of four LMMs capable of handling the REC task on both the cleaned and original versions of the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, using the conventional accuracy as the evaluation metric. The evaluation is performed on the combination of the validation and test sets for each benchmark. \(\): models fine-tuned on the specific dataset.

make Ref-L4 a comprehensive benchmark for assessing the REC capabilities of contemporary LMMs. Table 3 provides a detailed comparison between Ref-L4 and other benchmarks including RefCOCO, RefCOCO+, and RefCOCOg. Our Ref-L4 benchmark stands out due to the following characteristics:

* _Large-Scale._ Ref-L4 includes \(9,735\) images, \(18,653\) unique instances, and a total of \(45,341\) annotations, significantly surpassing RefCOCO, RefCOCO+, and RefCOCOg. For instance, RefCOCOg offers \(3,900\) images, \(7,596\) instances, and \(14,498\) annotations.
* _High Diversity._ Ref-L4 features \(365\) unique categories. Since the RefCOCO series derive from the COCO 2014 dataset, they encompass up to \(78\) categories. Additionally, our benchmark covers a wider range of instance scales, from \(30\) to \(3,767\), measured by the square root of the instance area.
* _Lengthy Referring Expressions._ Each referring expression in Ref-L4 is a detailed description of a specific instance, with lengths ranging from \(3\) to \(117\) words and an average of \(24.2\) words. In comparison, the average annotation lengths in RefCOCO, RefCOCO+, and RefCOCOg are \(3.6\), \(3.6\), and \(8.4\) words, respectively. Examples can be found in Figure 1.
* _Extensive Vocabulary._ Due to the detailed nature of the referring expressions, Ref-L4 boasts a large vocabulary of \(22,813\) words, which is four to six times larger than those of RefCOCO, RefCOCO+, and RefCOCOg.

**Evaluation on Ref-L4.** We conduct an evaluation of 24 representative LMMs that can perform the REC task. In addition to the standard accuracy metric, which considers predictions with an IoU greater than 0.5 as accurate (Acc\({}_{0.5}\)), we also report accuracies at higher IoU thresholds: Acc\({}_{0.75}\) and Acc\({}_{0.9}\). Furthermore, we introduce a mean accuracy (mAcc), calculated as the average accuracy from

Figure 1: Examples from our Ref-L4 benchmark. We offer a detailed referring expression for each target instance represented by a bounding box. Zoom in for better visualization.

   Benchmark & Images & Instances & Annotations & Categories & Avg. & Instance & Image & Vocab. \\  & & & & Length & Size & Size & Size & Vocab. \\  RefCOCO  & 3,000 & 7,596 & 21,586 & 71 & 3.6 & 105 - 607 & 230 - 640 & 3,525 \\ RefCOCO+  & 3,000 & 7,578 & 21,373 & 71 & 3.6 & 105 - 607 & 230 - 640 & 4,387 \\ RefCOCOg  & 3,900 & 7,596 & 14,498 & 78 & 8.4 & 83 - 610 & 277 - 640 & 5,050 \\  Ref-L4 (Ours) & 9,735 & 18,653 & 45,341 & 365 & 24.2 & 30 - 3,767 & 230 - 6,606 & 22,813 \\   

Table 3: Comparison between our Ref-L4 benchmark and other REC benchmarks, including RefCOCO , RefCOCO+ , and RefCOCOg . For the latter three benchmarks, we combine their validation and test sets for statistics. The instance size and image size are represented by their respective square roots. Avg. length: average length of annotations. Vocab.: vocabulary size.

\(_{0.5}\) to \(_{0.9}\) in increments of 0.05. To gain deeper insights into the models' capabilities, we conduct a detailed analysis of REC performance across different instance scales and categories. _The Ref-L4 benchmark and the evaluation code are available at https://github.com/JierunChen/Ref-L4_.

## 2 Related Work

**REC and Its Benchmarks.** Referring Expression Comprehension (REC) [47; 38; 81; 21; 43; 75] is a task that involves identifying a specific object within an image based on a given referring expression. Unlike object detection [30; 23; 52; 50; 4], which operates within fixed categories and a single visual modality, REC necessitates understanding free-form text to locate objects of any category. Phrase Grounding [44; 67; 14; 34; 27; 76; 61] is similar but typically involves shorter phrases and identifies multiple regions, whereas REC requires parsing longer expressions to pinpoint a single unique region. This complexity makes REC an ideal task for evaluating emerging large multimodal models. Current REC benchmarks such as RefCOCO , RefCOCO+, and RefCOCOg include tens of thousands of annotations but are limited by their short expression lengths--averaging 3.6, 3.6, and 8.4 words, respectively. Additionally, they encompass fewer than 80 categories, lacking real-world diversity. Other REC benchmarks [33; 8; 48; 7; 64; 24; 58; 10; 3; 12; 11; 18] are often designed for specific scenarios. For example, CLEVR-Ref+ focuses on simple objects like boxes, spheres, and cylinders. SK-VG integrates prior scene knowledge as additional input, while RefCrowd  targets identifying a person within a crowd. By contrast, we introduce Ref-L4, a more general and comprehensive benchmark encompassing 365 categories and 45,341 annotations. Ref-L4 features expressions averaging 24.2 words and a vocabulary of 22,813 words, facilitating the accurate evaluation of REC models on complex expressions and diverse objects.

**REC Models.** The evolution of REC models has transitioned from specialized models [20; 72; 32; 54; 82; 68; 83] to generalist models or large multimodal models (LMMs)[62; 31; 13; 60; 2; 5; 66; 78; 73; 74; 45; 77; 63; 53; 35; 46; 22]. Notable examples of these LMMs include CogVLM-Grounding, SPHINX [31; 13], ONE-PEACE , Qwen-VL-Chat , MiniGPTv2 , and Lenna . These models, benefiting from larger model sizes and extensive training on diverse datasets, exhibit remarkable performance on conventional REC datasets. For example, CogVLM-Grounding achieves an accuracy of \(94.58\%\) on RefCOCO (cleaned). Additionally, the performance gap among models is shrinking, with many LMMs surpassing \(90\%\) accuracy. This performance saturation raises concerns about the adequacy of current REC benchmarks for making meaningful comparisons. In response, we propose Ref-L4, a more comprehensive and challenging benchmark. We have also conducted rigorous evaluations of 24 LMM models, offering holistic comparisons that highlight their weaknesses and suggest directions for improvement.

## 3 Ref-L4

### Benchmark Creation

**Data Sources.** Our benchmark is derived from two sources: 1) our cleaned validation and test sets of the RefCOCO , RefCOCO+ , and RefCOCOg  datasets; and 2) the test set from the large-scale object detection dataset Objects365 . The Objects365 dataset provides a broader range of categories, varying instance sizes, higher image resolutions, and more intricate scenes. In the RefCOCO series, each instance includes a bounding box, a category name, and an extremely brief expression like "right teddy bea". In contrast, the Objects365 benchmark labels each instance with mainly a bounding box and the relevant category.

For the RefCOCO (cleaned) series, we begin by consolidating duplicate images and instances, resulting in a subset of \(6,502\) images containing \(14,186\) unique instances. For Objects365, we select samples from its testing set based on several criteria: 1) each image has both height and width greater than 800 pixels; 2) each image is sufficiently complex, containing more than 10 categories and 20 instances; 3) each instance has a square normalized size \(\) greater than 0.05, where \((h,w)\) represents the instance size and \((H,W)\) denotes the image size; 4) we randomly sample \(N\) instances for each of the 365 classes defined in Objects365, with \(N\,=\,(35,\) the number of instances for the specific class); 5) we review and exclude instances with erroneous bounding box annotations or those difficult to describe uniquely. For a few rare classes, we relax criterion-1 to 512 pixels and criterion-2 to 10 instances. Consequently, we collect \(3,233\) images and \(4,467\) instances from Objects365. Overall, our Ref-L4 benchmark comprises \(9,735\) images and \(18,653\) instances, sourced from the RefCOCO series and Objects365.

**Referring Expression Generation.** Given a target instance and its corresponding image, we leverage GPT-4V with human reviewers in the loop to generate its precise and detailed referring expressions. Figure 2 illustrates the three-step generation process:

_Step-1:_ Each instance in the Objects365 dataset is linked to a bounding box and a _category name_. We begin by cropping these instances from the original images. Next, we input each cropped area along with the prompt detailed in Section B.1 into GPT-4V to produce a context-independent description. For instances from the RefCOCO series, this step is omitted as each instance already has a brief expression.

_Step-2:_ Drawing inspiration from recent studies on GPT-4V , where GPT-4V is able to pay more attention to instances highlighted by a red circle within an image, we similarly encircle the target instance in red to facilitate GPT-4V in generating a context-aware referring expression. Following this, as depicted in Figure 2, we process the image and use the prompt outlined in Section B.2 to generate a context-aware referring expression for each instance. We instruct GPT-4V to describe various features such as color, size, position, and context. Additionally, we provide a hint (the context-independent description from Step-1) in the prompt to mitigate hallucination issues, resulting in more accurate descriptions.

_Step-3:_ We manually review all generated referring expressions to correct any hallucination issues. We ensure that each expression uniquely describes the instance and is factual, accurate, and harmless.

**Annotation Expansion.** To date, we have compiled 18,653 unique referring expressions, each describing a distinct instance. To assess the robustness of REC models to diverse language inputs, we employ a two-stage rephrasing process to expand our benchmark: 1) utilizing GPT-4 with the prompt detailed in Section B.3, to generate rephrased versions of each expression; 2) conducting a manual review to ensure that the rephrased expressions are unique, factual, relevant, and harmless. Consequently, our final Ref-L4 benchmark encompasses 9,735 images with 45,341 referring expressions, each accurately describing one of the 18,653 unique instances.

Figure 2: Pipeline of generating a referring expression for a target instance.

### Analysis

**Expression Length.** Figure 2(a) illustrates the distribution of expression lengths across four different datasets: RefCOCO, RefCOCO+, RefCOCOg, and our Ref-L4. Due the the high overlap of data samples, RefCOCO and RefCOCO+ exhibit similar distributions, with a high density of shorter expressions peaking at around 3.6 words. RefCOCOg features slightly longer expressions on average, peaking at approximately 8.4 words. In contrast, our Ref-L4 displays a significantly different distribution, with expressions ranging much longer, peaking at around 24.2 words and having a long tail extending up to 117 words. This suggests that our Ref-L4 benchmark is designed to push the boundaries of current REC models, requiring them to process and comprehend more intricate and detailed descriptions.

**Instance Size.** In Figure 2(b), we present a density plot comparing the instance sizes across four benchmarks. We define the instance size as the square root of the normalized size, \(\), where \((h,w)\) represents the dimensions of the instance and \((H,W)\) represents the dimensions of the image. All benchmarks exhibit a peak density around an instance size of 160. Our Ref-L4 benchmark

Figure 4: The frequency of the 10 most frequently used words in each part-of-speech category, as parsed using the SpaCy library.

Figure 3: Analysis of referring expression length, instance size, and category distribution.

shows a wider distribution range compared to the other three, indicating that our Ref-L4 captures a broader spectrum of instance sizes.

**Categories.** Our Ref-4L benchmark comprises 18,653 instances spanning 365 distinct categories, providing more complex and diverse evaluation scenarios. In contrast, RefCOCO and RefCOCO+ consists of 71 categories, while RefCOCOg covers 78 categories. Figure 2(c) presents the distribution of instances among these 365 categories. Notably, the ten categories with the highest number of instances are "Person", "Chair", "Hat", "Desk", "Lamp", "Cabinet/shelf", "Car", "Sneakers", "Handbag/Satchel", and "'Flag".

**Vocabulary.** Our benchmark's referring expressions comprise a vocabulary totaling 22,813 unique words. This is significantly larger than the vocabulary sizes of RefCOCO, RefCOCO+, and RefCOCOg, which are 3,525, 4,387, and 5,050 words, respectively. Figure 4 illustrates the 10 most frequently used nouns, verbs, adverbs, and prepositions.

### Evaluation

**Evaluation Metrics.** We propose three distinct evaluation protocols:

1. _Accuracy._ This is the conventional metric used in REC. For a given referring expression and corresponding image, the target instance is considered successfully localized if the IoU between the predicted bounding box and the ground truth exceeds 0.5. Accuracy is then calculated as the ratio of successfully localized samples to the total number of samples, referred to as Acc\({}_{0.5}\) in this work. To better assess the localization capabilities of modern REC models, we also report accuracies at higher IoU thresholds: Acc\({}_{0.75}\), Acc\({}_{0.9}\), and mAcc, which is the average accuracy from Acc\({}_{0.5}\) to Acc\({}_{0.9}\) in increments of 0.05.
2. _Scale-Aware Performance._ To gain deeper insights into model capabilities, we report performance based on instance sizes: small, medium, and large. The size of an instance is defined as the square root of its area, \(\), where \((h,w)\) are the dimensions of the instance. Small instances are those with a size less than \(128\), medium instances are between \(128\) and \(256\), and large instances exceed \(256\). In total, there are \(9345\), \(23280\), and \(12716\) referring expressions describing \(2,954\) small, \(10,442\) medium, and \(5,257\) large instances, respectively.
3. _Per-Category Performance._ Our benchmark encompasses a wide range of categories, up to \(365\) in total. We provide an evaluation protocol to assess performance on a per-category basis.

**Benchmark Division.** Modern large multimodal models (LMMs) that are able to handle the REC task typically use unrestricted and extensive data for training. Our Ref-L4 benchmark is designed to assess the capabilities of these advanced models without imposing any limitations on the training data sources. The benchmark is divided into two subsets: a validation set, comprising 30% of the data with \(7,231\) images, \(10,311\) instances, and \(13,420\) referring expressions; and a test set, comprising 70% of the data with \(9,467\) images, \(17,242\) instances, and \(31,921\) referring expressions. Given that our benchmark includes instances from \(365\) categories, we ensure that each category has at least one sample in both the validation and test sets. While we provide these two splits, we encourage the combined use of both sets for model evaluation, especially in the current LMM era, where the use of unrestricted training data is prevalent.

## 4 Experiments

**Main Result.** We evaluate a total of 24 LMMs that can perform the REC task, dividing them into two categories based on their output type: those that produce bounding boxes and those that produce segmentation masks. For models that output segmentation masks, we convert these masks into tight bounding boxes to enable evaluation on our Ref-L4 benchmark. Table 4 presents the performance of these models on the validation set, test set, and the combined set, using the metrics defined in Section 3.3. The evaluation prompt of GPT-4V is available in Section B.4. Among the models that output bounding boxes, CogVLM-Grounding  shows the best performance, while GlaMM  leads in performance among the models that output masks.

**Category-Wise Performance.** Each instance in our benchmark is assigned a category label from one of 365 classes. Figure 5 illustrates the performance of the top four models across these categories,sorted in descending order based on their average per-category performance. The results indicate a training bias issue, as all four models exhibit poor performance on some common categories.

**Scale-Aware Evaluation.** In Section 3.3, we present a scale-aware evaluation to assess the model's ability to handle different instance scales. Specifically, we categorize all samples in our benchmark into three sets based on instance size: small, medium, and large. The performance of 24 models is detailed in Table 5. Among the bounding-box-output models, CogVLM-Grounding  excels with small and medium instances, while SPHINX-v2-1k  achieves the best performance with large instances. For mask-output models, GlaMM  outperforms all other models across all three sets.

**Evaluation on Diverse Data Sources.** Our benchmark is derived from COCO and Objects365 datasets. We assess the performance of the top four models with bounding box outputs and the top

    &  &  & Test \\   & Acc\({}_{0.5}\) & Acc\({}_{0.75}\) & Acc\({}_{0.9}\) & mAcc & mAcc & mAcc \\  GPT-4V  & 9.91 & 1.19 & 0.12 & 2.88 & 2.96 & 2.85 \\ KOSMOS-2  & 48.53 & 38.34 & 17.54 & 34.72 & 34.89 & 34.64 \\ OFA-Tiny  & 55.21 & 43.22 & 27.70 & 41.44 & 41.53 & 41.40 \\ OFA-Large  & 72.53 & 62.31 & 45.02 & 59.17 & 59.42 & 59.07 \\ Ferret-7b  & 57.54 & 42.44 & 21.01 & 40.29 & 40.31 & 40.28 \\ Ferret-13b  & 64.44 & 49.04 & 27.46 & 46.88 & 47.31 & 46.71 \\ GroundingGPT  & 60.84 & 40.48 & 12.00 & 38.19 & 38.42 & 38.09 \\ Shikra-7b  & 65.06 & 39.62 & 10.45 & 38.60 & 38.91 & 38.47 \\ Lenna  & 65.90 & 58.55 & 45.58 & 55.69 & 55.88 & 55.60 \\ MiniGPTv2  & 66.93 & 50.50 & 25.30 & 47.15 & 47.43 & 47.03 \\ Qwen-VL-Chat  & 73.80 & 58.05 & 37.16 & 55.94 & 56.18 & 55.83 \\ ONE-PEACE  & 70.82 & 60.09 & 36.12 & 55.07 & 55.49 & 54.89 \\ SPHINX-MoE  & 66.23 & 44.90 & 15.32 & 42.38 & 42.80 & 42.21 \\ SPHINX-MoE-1k  & 74.45 & 62.70 & 38.85 & 58.07 & 58.35 & 57.95 \\ SPHINX  & 74.78 & 53.65 & 21.15 & 50.09 & 50.33 & 49.99 \\ SPHINX-1k  & 78.52 & 62.17 & 32.95 & 57.57 & 57.91 & 57.42 \\ SPHINX-v2-1k  & 81.31 & 70.49 & 46.59 & 65.39 & 65.67 & 65.27 \\ CogVLM-Grounding  & **81.70** & **70.77** & **48.35** & **66.09** & **66.25** & **66.02** \\  PixelLM-7B\({}^{}\) & 41.83 & 27.57 & 13.32 & 27.10 & 27.09 & 27.11 \\ PixelLM-13B\({}^{}\) & 49.89 & 35.37 & 18.42 & 34.10 & 34.52 & 33.92 \\ LISA-Explanatory\({}^{}\) & 65.12 & 52.35 & 38.26 & 50.77 & 50.89 & 50.72 \\ LISA\({}^{}\) & 66.23 & 54.02 & 39.73 & 52.18 & 52.44 & 52.07 \\ PSALM\({}^{}\) & 67.26 & 58.22 & 44.11 & 55.46 & 55.68 & 55.37 \\ GlaMM\({}^{}\) & **71.90** & **60.27** & **45.15** & **57.89** & **58.16** & **57.78** \\   

Table 4: Performance evaluation across 24 models on our Ref-L4 benchmark. NVIDIA A100 GPUs (80G) are utilized. The symbol \(\) denotes models that outputs segmentation masks.

Figure 5: Category-wise performance of the four top-performing models on the val+test set, sorted in descending order based on their average per-category performance. The performance of all models can be found in Section C.1.

two models with mask outputs across various subsets originating from either COCO or Objects365. These subsets are: 1) the COCO-derived set (referred to as "COCO"); 2) a subset from Objects365, where the instances have categories that also exist in COCO (referred to as "O365-P1"); 3) another subset from Objects365, where the instances have categories not found in COCO (referred to as "O365-P2"). Figure 6 presents the performance of these models across the three subsets. The "COCO" set shows higher accuracy compared to the other two sets, partially because most models are trained on the RefCOCO series and have limited exposure to Objects365 images. "O365-P1" exhibits higher accuracy than "O365-P2", as the latter includes more rare categories.

## 5 Conclusion

In this work, we first point out several limitations of the current REC benchmarks, such as substantial labeling inaccuracies and very brief referring expressions. To better assess the capabilities of models, particularly those LMMs that can perform the REC task, we present Ref-L4, which features four key characteristics: 1) a large-scale dataset with 45,341 annotations; 2) a wide range of object categories and varying instance scales; 3) detailed referring expressions; and 4) an extensive vocabulary comprising 22,813 unique words. We evaluate a total of 24 models using various evaluation protocols. We wish that Ref-L4 could serve as a valuable resource for researchers and developers, fostering the development of more robust and versatile REC models in the LMM era.

    &  &  &  \\   & Acc\({}_{0.5}\) & mAcc & Acc\({}_{0.5}\) & mAcc & Acc\({}_{0.5}\) & mAcc \\  GPT-4V  & 2.13 & 0.49 & 10.29 & 2.78 & 14.93 & 4.83 \\ KOSMOS-2  & 24.19 & 11.63 & 46.95 & 32.91 & 69.32 & 54.98 \\ OFA-Tiny  & 17.91 & 11.49 & 65.13 & 49.00 & 64.46 & 49.61 \\ OFA-Large  & 40.13 & 27.07 & 81.03 & 66.49 & 80.78 & 69.36 \\ Ferret-7b  & 30.93 & 14.57 & 62.40 & 43.72 & 68.18 & 52.92 \\ Ferret-13b  & 36.46 & 17.88 & 70.50 & 51.86 & 73.92 & 59.09 \\ GroundingGPT  & 24.43 & 10.28 & 67.67 & 41.04 & 75.09 & 53.47 \\ Shikra-7b  & 43.91 & 18.50 & 75.98 & 46.27 & 60.60 & 39.34 \\ Lenna  & 31.02 & 23.48 & 72.90 & 61.53 & 78.72 & 68.66 \\ MiniGPTv2  & 32.99 & 14.85 & 73.67 & 51.16 & 79.52 & 63.53 \\ Qwen-VL-Chat  & 47.66 & 26.26 & 79.80 & 61.06 & 82.01 & 68.37 \\ ONE-PEACE  & 22.18 & 13.98 & 83.26 & 63.39 & 83.81 & 70.04 \\ SPHINX-MoE  & 39.48 & 16.39 & 72.97 & 46.38 & 73.55 & 54.17 \\ SPHINX-MoE-1k  & 58.96 & 37.61 & 77.80 & 61.53 & 79.70 & 66.77 \\ SPHINX  & 48.82 & 22.08 & 80.56 & 54.10 & 83.27 & 63.34 \\ SPHINX-1k  & 59.48 & 33.21 & 82.95 & 61.82 & 84.40 & 67.68 \\ SPHINX-v2-1k  & 65.23 & 43.43 & 84.00 & 68.45 & **88.21** & **75.91** \\ CogVLM-Grounding  & **75.06** & **52.85** & **86.43** & **71.31** & 77.91 & 66.25 \\  PixelLM-7B\({}^{}\) & 8.25 & 4.05 & 43.90 & 27.33 & 62.72 & 43.64 \\ PixelLM-13B\({}^{}\) & 17.05 & 8.54 & 53.40 & 35.48 & 67.59 & 50.34 \\ LISA-Explanatory\({}^{}\) & 39.11 & 27.16 & 70.03 & 54.61 & 75.25 & 61.09 \\ LISA\({}^{}\) & 39.24 & 27.49 & 71.17 & 56.05 & 77.01 & 63.22 \\ PSALM\({}^{}\) & 37.35 & 28.43 & 75.06 & 61.79 & 74.97 & 63.74 \\ GlaMM\({}^{}\) & **47.07** & **34.36** & **77.17** & **62.28** & **80.50** & **67.14** \\   

Table 5: Scale-aware evaluation across 24 models on our Ref-L4 benchmark.

Figure 6: Evaluation of six models on various data sources, with mAcc acting as the metric. The results of all models can be found in Section C.2.