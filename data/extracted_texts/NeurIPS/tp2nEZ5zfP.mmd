# NetHack is Hard to Hack

Ulyana Piterbarg

NYU

Lerrel Pinto

NYU

Rob Fergus

NYU

Correspondence to up2021@cims.nyu.edu.

###### Abstract

Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.

## 1 Introduction

Reinforcement Learning (RL) combined with deep neural policies has achieved impressive results in control problems, such as short-horizon simulated locomotion tasks . However, these methods struggle in long-horizon problem domains, such as NetHack , a highly challenging grid-world game. NetHack poses difficulties due to its vast state and action space, multi-modal observation space (including vision and language), procedurally-generated randomness, diverse strategies, and deferred rewards. These challenges are evident in the recent NetHack Challenge , where agents based on hand-crafted symbolic rules outperform purely neural approaches (see Figure 1), despite the latter having access to high-quality human demonstration data  and utilizing large-scale models.

We propose three reasons for the poor performance of large-scale neural policies compared to symbolic strategies. First, symbolic strategies implement hierarchical control schemes, which are generally absent in neural policies used for NetHack. Second, symbolic models use hand-crafted parsers for multi-modal observations, suggesting that larger networks could enhance representations extracted from complex observations. Third, symbolic strategies incorporate error correction mechanisms, which could be crucial for improving neural policies if integrated with RL based error correction.

In this work, we conduct a comprehensive study of NetHack and examine various learning mechanisms to enhance the performance of neural models. We bypass traditional RL obstacles, such as sparse rewards or exploration challenges, by focusing on imitation learning. However, we find that existing datasets lack crucial information, such as hierarchical labels and symbolic planning traces. To address this, we augment the codebase of AutoAscend, the top-performing symbolic agent in the 2021 NetHack Challenge, and extract hierarchical labels tracking the agent's internal strategy selection in order to construct a large-scale dataset containing \(10^{9}\) actions.

Using this dataset, we train a range of deep neural policies and investigate: (a) the advantages of hierarchy; (b) model architecture and capacity; and (c) fine-tuning with reinforcement learning. Our main findings are as follows:

* Hierarchical behavioral cloning (HBC) significantly outperforms BC and baseline methods, provided that the model has adequate capacity.
* Large transformer models exhibit considerable improvements over baselines and other architectures, such as LSTMs. However, the power-law's shallow slope indicates that data scaling alone will not suffice to solve the game.
* Online fine-tuning with RL further enhances performance, with hierarchy proving beneficial for exploration.
* The combined effects of hierarchy, scale, and RL lead to state-of-the-art performance, narrowing the gap with AutoAscend but not eliminating it.

We validate the statistical significance of our findings with low-sample hypothesis testing (see Appendix H). Additionally, we open-source our code, models, and the HiHack repository2, which includes (i) our \(10^{9}\) dataset of hierarchical labels obtained from AutoAscend and (ii) the augmented AutoAscend and NLE code employed for hierarchical data generation, encouraging development.

Though we base our experiments in NetHack, we take measures to preserve the generality of insights yielded by our investigations of neural policy learning. Specifically, we intentionally forgo the addition of any environment-specific constraints in the architectural design or training setup of all models explored in this paper. This is in contrast to NetHack agents RAPH and KakaoBrain, the first and second place winners of the neural track of the NHC Competition, respectively, which incorporate augmentations such as hand-engineered action spaces, role-specific training, and hard-coded symbolic sub-routines . While this choice prevents us from achieving absolute state-of-the-art performance in NLE in this paper, we believe it to be crucial in preserving the general applicability of our insights to neural policy learning for general open-ended, long-horizon environments.

## 2 Related Work

Our work builds upon previous studies in the NetHack environment, imitation learning, hierarchical learning, and the use of transformers as policies. In this section, we briefly discuss the most relevant works.

Figure 1: _Left:_ The per-step observation for NetHack agents consists of an ego-centric pixel image (blue) and two text fields containing in-game messages (green) and statistics (pink). _Right:_ Selected results from the NeurIPS 2021 NetHack Challenge (NHC)  showing game score on a log-scale. Error bars reflect standard error. Neural baseline models (grey) trained with BC perform poorly, but are somewhat improved when fine-tuned with RL. We find that the introduction of hierarchy and changes in model architecture yield significant improvements (light/dark blue), resulting in state-of-the-art performance for a neural model. However all neural approaches are significantly worse than AutoAscend, a hand-crafted symbolic policy. Our paper explores this performance gap.

NetHackFollowing the introduction of the NetHack Learning Environment (NLE) , the NetHack Challenge (NHC) competition  enabled comparisons between a range of different agents. The best performing symbolic and fully data-driven neural agents were AutoAscend and Chaotic-Dwarven-GPT-5 (CDGPT5), respectively, and we base our investigations on them, as well as on the NetHack Learning Dataset .

Several notable works make use of the NetHack environment. Zhong et al.  show how a dynamics model can be learned from the Nethack text messages and leveraged to improve performance; on account of their utility, we also encode the in-game message but instead use a model-free policy to pick actions. Bruce et al.  show how a monotonic progress function in NetHack can be learned from human play data and then combined with a reinforcement learning (RL) reward to solve long-range tasks in the game. This represents a complementary way of employing NetHack demonstration data without direct action imitation.

Imitation LearningPomerleau  demonstrated the potential of driving an autonomous vehicle using offline data and a neural network, which has since become an ongoing research topic for scalable behavior learning . Formally, behavior learning is a function approximation problem, where the goal is to model an underlying expert _policy_ mapping states or observations to actions, directly from data. Approaches to behavior learning can be categorized into two main classes: _offline RL_, which focuses on learning from mixed-quality datasets with reward labels; and _imitation learning_, which emphasizes learning behavior from expert datasets without reward labels. Our work primarily belongs to the latter category as it employs a behavior cloning model. Behavior cloning, a form of imitation learning, aims to model the expert's actions given the observation and is frequently used in real-world applications .

Since behavior cloning algorithms typically address a fully supervised learning problem, they are often faster and simpler than offline RL algorithms while still yielding competitive results . A novel aspect of our work is the use of hierarchy in conjunction with behavioral cloning, i.e. supervision at multiple levels of abstraction, a topic which has received relatively little attention. Recent efforts to combine large language models with embodied agents use the former to issue a high-level textual "action" to the low-level motor policy. Approaches such as Abramson et al.  have shown the effectiveness of hierarchical BC for complex tasks in a simulated playroom settings.

Hierarchical Policy LearningHierarchical reinforcement learning (HRL) based techniques  extend standard reinforcement learning methods in complex and long-horizon tasks via temporal abstraction across hierarchies, as demonstrated by Levy et al.  and Nachum et al. . Similarly, numerous studies have concentrated on showing that primitives  can be beneficial for control. These concepts have been combined in works such as Stochastic Neural Networks by Florensa et al. , where skills are acquired during pretraining to tackle diverse complex tasks. Likewise, Andreas et al.  learn modular sub-policies for solving temporally extended tasks. However, most prior work focus on learning both levels of the hierarchy, i.e. a decomposition across primitives, skills, or sub-policies as well as the primitives, skills, or sub-policies themselves, which makes training complex. Correspondingly, the resultant approaches have had limited success on more challenging tasks and environments. Le et al.  explores the interaction between hierarchical learning and imitation and finds benefits, albeit in goal-conditioned settings. In contrast, our work explores the benefits of access to a fixed hierarchy chosen by the domain-expert designer of AutoAscend, which simplifies our study of overall learning mechanisms for NetHack.

Transformers for RLThe remarkable success of transformer models  in natural language processing  and computer vision  has spurred significant interest in employing them for learning behavior and control. In this context,  apply transformers to RL and offline RL, respectively, while  utilize them for imitation learning. Both  primarily use transformers to summarize historical visual context, whereas  focuses on their long-term extrapolation capabilities. More recent work have explored the use of multi-modal transformers  to fit large amounts of demonstration data . To enable transformers to encode larger context lengths, recurrent transformer models have been proposed . Our work draws inspiration from these use cases, employing a transformer to consolidate historical context and harness its generative abilities in conjunction with a recurrent module.

## 3 Data Generation: Creating the HiHack Dataset

### Extending the NetHack Learning Environment

The NetHack Learning Environment (NLE) is a gym environment wrapping the NetHack game. Like the game itself, the action and state spaces of NLE are complex, consisting of 121 distinct actions and ten distinct observation components. The full observation space of NLE is far richer and more informed than the view afforded to human players of NetHack, who observe only the more ambiguous "text based" components of NLE observations, denoted as tty_chars, tty_colors, and tty_cursor. This text based view corresponds also to the default format in which both NetHack and NLE gameplay is recorded, loaded, and streamed via the C based ttyrec library native to the NetHack game.

The popular NetHack Learning Dataset (NLD) offers two large-scale corpuses of NetHack gameplay data, NLD-AA, consisting of action-labeled demonstrations from AutoAscend, and NLD-NAO, consisting of unlabeled human player data . NLD adopts the convention of recording only tty* components of NLE observations as a basis for learning, hence benefiting from the significant speedups in data operations offered via integration with the ttyrec library. We adhere to this convention with the hierarchical HiHack Dataset introduced in this paper. Thus, in order to generate our dataset, we extend the ttyrec library to store hierarchical _strategy_ or, equivalently, _goal_ labels alongside action labels. We further integrate this extension of ttyrec with NLE, modifying the gym environment to accept an additional hierarchical label at each step of interaction. This input hierarchical label does not affect the underlying state of the environment, and is instead employed strictly to enable the recording of hierarchically-informed NetHack game-play to the ttyrec data format.

### AutoAscend: A Hierarchical Symbolic Agent

An inspection of the fully open-source code base underlying the AutoAscend bot reveals the internal structure of the bot to be composed of a directed acyclic graph of explicitly defined _strategies_. The bot's underlying _global controller_ switches between strategies in an imperative manner via sets of strategy-specific predicates, as visualised in Figure 2. Among these strategies are hand-engineered routines for accomplishing a broad range of goals crucial to effective survival in the game and successful descent through the NetHack dungeons. These include routines for fighting off arbitrary monsters, selecting food that is safe to eat from an agent's inventory, and efficiently exploring the dungeon while gathering and identifying valuable items, among many others. The various strategies are supported in turn by shared _sub-strategies_ for accomplishing simpler "sub-goals" (see Appendix B for the full graph).

We exploit this explicit hierarchical structure in the generation of HiHack, extending the AutoAscend codebase to enable per-step logging of the strategy responsible for yielding each action executed by the bot, as supported by our modifications to the C based ttyrec writer library and NLE.

Figure 2: A diagrammatic visualization of the internal structure of AutoAscend. The bot is composed of eleven goal-directed, high-level _strategies_. The “global controller” underlying AutoAscend employs a complex predicate based control flow scheme to determine which strategy to query for an action on a per-timestep basis .

### The HiHack Dataset

Our goal in generating the HiHack Dataset (HiHack) is to create a hierarchically-informed analogue of the large-scale AutoAscend demonstration corpus of NLD, NLD-AA. Thus, as previously described, HiHack is composed of demonstrations recorded in an extended version of the ttyrec format, consisting of sequences of tty+s observations of the game state accompanied by AutoAscend action and strategy labels. HiHack contains a total of 3 billion recorded game transitions, reflecting more than a hundred thousand AutoAscend games. Each game corresponds to a unique, procedurally-generated "seed" of the NetHack environment, with AutoAscend playing as one of thirteen possible character "starting roles" across a unique layout of dungeons.

We verify that the high-level game statistics of HiHack match those of NLD-AA in Table 1. Indeed, we find a high degree of correspondence across mean and median episode score, total number of transitions, and total number of game turns. We attribute the very slightly diminished mean scores, game transitions, and turns associated with HiHack to a difference in the value of the NLE timeout parameter employed in the generation of the datasets. This parameter regulates the largest number of contiguous keypresses failing to advance the game-state that is permitted before a game is terminated. The value of the timeout parameter was set to \(1000\) in the generation of HiHack.

## 4 Hierarchical Behavioral Cloning

Our first set of experiments leverage the hierarchical strategy labels recorded in HiHack for offline learning with neural policies, via _hierarchical behavior cloning_ (HBC).

MethodMimicking the imperative hierarchical structure of AutoAscend, we introduce a bilevel hierarchical decoding module over a popular NetHack neural policy architecture, namely the ChaoticDwarvenGPT5 (CDOPT5) model. This model achieved 3rd place in the neural competition track of the NeurIPS 2021 NetHack Challenge when trained from scratch with RL, making it the top-performing fully data-driven neural model for NetHack .

The CDGPT5 model consists of three separate encoders: a 2-D convolutional encoder for pixel-rendered visual observations of the dungeon \(o_{t}\), a multilayer perceptron (MLP) encoder for the environment message \(m_{t}\), and a 1-D convolutional encoder for the bottom-line agent statistics \(b_{t}\). These three observation portions are extracted from the tty\(*\) NLE observations of HiHack. The core module of the network is an LSTM, which is employed to produce a recurrent encoding of an agent's full in-game trajectory across what may be hundreds of thousands of keypresses, both in training and at test-time. The core module may also receive a one-hot encoding of the action executed at the previous time-step \(a_{t-1}\) as input.

Our hierarchically-extended version of this LSTM based policy is shown in Figure 3(left). We replace the linear decoder used to decode the LSTM hidden state into a corresponding action label in the CDGPT5 model with a hierarchical decoder consisting of (i) a single "high level" MLP responsible for predicting the strategy label \(g_{t}\) given the environment observation tuple \(\{m_{t},o_{t},b_{t}\}\), and (ii) a set of "low level " MLPs, one for each of the discrete strategies in the AutoAscend hierarchy (see Figure 2), with a SoftMax output over discrete actions. The strategy prediction \(g_{t}\) selects which of these low-level MLPs to use.

    & NLD-AA & HiHack \\  Total Episodes & 109,545 & 109,907 \\ Total Transitions & 3,481,605,009 & 3,244,729,367 \\ Mean Episode Score & 10,105 & 8,166 \\ Median Episode Score & 5,422 & 5,147 \\ Median Episode Game Transitions & 28,181 & 27,496 \\ Median Episode Game Turns & 20,414 & 19,991 \\ Hierarchical Labels & ✗ & ✓ \\   

Table 1: A comparison of dataset statistics between NLD-AA and our generated HiHack Dataset, produced by running AutoAscend in NLE v0.9.0 with extended tty* observations.

We employ a simple cross-entropy loss to train both the baseline non-hierarchical LSTM CDGPT5 policy, as well as our Hierarchical LSTM policy, aggregating gradients across the bilevel decoders of the latter via the Gumbel-Softmax reparameterization trick .

Training and evaluation detailsWe train all policies on a single GPU for 48 hours with the full \(3.2B\) HiHack Dataset. As with all experiments in the paper, a total of 6 seeds are used to randomize dataloading and neural policy parameter initialization. We employ mean and median NLE score on a batch of withheld NLE instances (\(n=1024\)) as our central metrics for evaluating model performance and generalization at the conclusion of training, following the convention introduced in the NetHack Challenge competition . Reported performance is aggregated over random seeds. Further details of architectures as well as training and evaluation procedures can be found in Appendices D, E, and F of the supplemental material.

ResultsWe find that the introduction of hierarchy results in a significant improvement to the test-time performance of LSTM policies trained with behavioral cloning, yielding a 40% gain over the baseline in mean NLE score as shown in Figure 3(right), and 50% improvement in median score across seeds as shown in Table 2. Additionally, to verify that this improvement in performance is indeed due to hierarchy and not simply a result of the increased parameter count of the hierarchical LSTM policy decoder, we run ablation experiments with a modified, large-decoder version of the baseline (non-hierarchical) policy architecture. The results, shown in Figure 3(right), show that increasing the size of the LSTM decoder, without the introduction of a hierarchy, does not result in performance improvements over the baseline.

## 5 Architecture and Data Scaling

Despite the benefits of introducing hierarchical labels, the performance of the Hierarchical LSTM policy trained with HBC remains significantly behind that of symbolic policy used to generate the HiHack demonstrations in the first place, AutoAscend. This finding prompts us to explore scaling: perhaps increasing the quantity of demonstration data or the model capacity may close the observed performance gap.

MethodTo test this new hypothesis, we conduct a two-pronged investigation: (i) to explore model capacity, we develop a novel base policy architecture for NetHack that introduces a transformer module into the previous CDGPT5 based architecture; and (ii) for data scaling, we run a second set of "scaling-law"  experiments that use subsets of the HiHack Dataset to quantify the relationship between dataset size and the test-time performance of BC policies.

Figure 3: _Left:_ Hierarchical LSTM policy, where \(g_{t}\) is the high-level strategy prediction (purple) that is used to select over the \(k\) low-level policies (yellow). Figure 1 shows the three different components of the input observation: message \(m_{t}\), ego-centric pixel view of the game \(o_{t}\), and “bottom line” statistics \(b_{t}\). _Right:_ Mean score for baseline LSTM model  (grey), our hierarchical model (blue) at the conclusion of training. The addition of hierarchical labels in decoding provides a significant performance gain, not matched by an (extra) large-decoder version of the baseline (dark grey). Error bars indicate standard error.

Our novel base policy architecture is visualized in Figure 4 (left). This architecture features two copies of the encoders employed in CDGPT5. One set is kept frozen and employed strictly to yield a recurrent encoding of the complete trajectory up to the current observation step via a pre-trained frozen LSTM, while the second is kept "unlocked" and is employed to provide embeddings of observations directly to a causal transformer, which receives a fixed context length during training.

Training and evaluation detailsThe training and evaluation procedures employed here echo those of section 4. In our data-scaling experiments, the subset of sampled HiHack games seen in offline training is randomized over model seeds. The causal transformer component of our transformer-LSTM models is trained with the same fixed, context-length (\(c=64\)) in all experiments. Hyperparameter sweeps were employed to tune all transformer hyperparameters (see Appendix D).

ResultsThe architecture experiments in Figure 4(right) show that both variants of our combined transformer-LSTM policy architecture yield gains eclipsing those granted solely by the introduction of hierarchy in the offline learning setting.

Figure 4: _Left:_ Transformer based architecture (non-hierarchical version). The LSTM encoder (grey) is used to provide a long temporal context \(h_{t}\) to the transformer. _Right:_ The transformer-LSTM models (light blue and green) outperform pure LSTM models with & without hierarchy (see Section 4 and  respectively). Ablating the LSTM encoder component from transformer-LSTM models to yield a _flat transformer_ policy architecture (orange) causes a substantial decline in policy performance. Error bars indicate standard error.

Figure 5: _Left:_ Model capacity versus mean score for our transformer-LSTM model. The larger model performs worse. Error bars indicate standard error. _Right:_ Dataset scaling experiments showing diminishing returns in mean NLE score \(s\) achieved during training with BC as the number of training games \(g\) reaches \(10^{5}\). We employ non-linear least-squares to estimate the power law coefficients relating log-game count and mean policy score outside of the low-data regime (\(g>10^{2}\)). Curves of best fit are displayed. Errors bars indicate mean score ranges across seeds. Collectively these two plots show that scaling of data and model size are not sufficient to close the performance gap to symbolic models.

[MISSING_PAGE_FAIL:8]

Training and evaluation detailsOur high-level training procedure here mirrors that of our hierarchical behavioral cloning experiments: we evaluate model performance under the constraint of computation time, training all policies for exactly 48 hours on a single GPU, using 6 random seeds to randomize data loading and environment seeding only. We test the effect of warm-starting from a checkpoint pre-trained with BC or HBC alone (via the procedure delineated in section 4) for all model classes. We report APPO + BC results with warm-starting for all model classes where we find it to be beneficial in improving stability and/or evaluation policy performance.

ResultsTable 2 summarizes the performance of all our models. A number of observations can be made: (a) In the offline setting, our best performing model (non-hierarchical transformer-LSTM) outperforms the vanilla CDGPT5 model baseline by 100% in mean NLE score and 127% in median NLE score; (b) RL fine-tuning offers a clear and significant performance boost to all models, with gains in the test-time mean NLE score associated with all model classes; (c) The overall best performing approach is APPO + BC using the hierarchical LSTM model. The mean score of \(1551\) represents a new state-of-the-art for neural policies in NLE, beating the performance of the vanilla CDGPT5 model when trained with APPO + BC by \(29\%\) in mean NLE score and \(25\%\) in median NLE score; (d) The transformer-LSTM models, which are slower and more unstable to train with APPO + BC than LSTM models (see training curves in Appendix G), see a smaller margin of improvement during RL fine-tuning; (e) Other metrics commonly employed to evaluate meaningful NetHack gameplay in previous work, such as dungeon level reached and lifetime of the agent in game turns [34; 23], show a broadly similar pattern to the chief median and mean score metrics employed in the NetHack Challenge ; and (f) Lastly, for BC, hierarchy seems to hurt performance for the larger Transformer-RL models, though this gap is closed once APPO fine-tuning is applied. See Appendix H for low-sample hypothesis tests validating the statistical significance of these findings.

In NetHack, the player can choose from 13 distinct roles (barbarian, monk, wizard, etc.), each of which require distinctive play-styles. In NLE, starting roles are randomized, by default. Figure 6 shows a score distribution breakdown across role for different neural policy classes trained with BC and APPO + BC. In general, we observe that fine-tuning with RL improves the error-correction capability of models of all classes (as indicated by positive shifts in NLE score distributions) over their purely offline counterparts.

## 7 Conclusion and Discussion

In this work, we have developed a new technique for training NetHack agents that improves upon prior state-of-the-art neural models by 127% in offline settings and 25% in online settings, as evaluated by median NLE score. We achieve this by first creating a new dataset, the HiHack Dataset (HiHack), by accessing the best symbolic agent for NetHack. This dataset, combined with new architectures, allows us to build the strongest purely data-driven agent for NetHack as of the writing of this paper.

Figure 6: Aggregate NLE score breakdown versus player role. Our model refinements (hierarchy, transformer, RL fine-tuning) show gains over the LSTM based CDGPT5 baseline  across all roles. As in Table 2, we employ (i) to confer that these score distributions were computed only for the top-performing neural policy seed (out of 6) across each model class.

More importantly, we analyze several directions to improve performance, including the importance of hierarchy, the role of large transformer models, and the boosts that RL could provide. Our findings are multifaceted and provide valuable insights for future progress in training neural agents in open-ended environments and potentially bridging the gap to symbolic methods, as well as to the very best human players.

* Hierarchy can improve underfitting models. Prior LSTM based models severely underfit on the HiHack Dataset (see training curves in Appendix G). The addition of hierarchy improves such models, whether trained offline or online (Figure 3(right) and Table 2).
* Hierarchy can hurt overfitting models. Transformer based models are able to overfit on the HiHack Dataset (see training curves in Appendix G). Hierarchy hurts this class of models when trained offline at test-time (Figure 4(right) and Table 2).
* Reinforcement learning provides larger improvements on light, underfitting models. We obtain little and no improvement with RL fine-tuning on our hierarchical transformer-LSTM and non-hierarchical transformer-LSTM models, respectively. However, the underfit LSTM models enjoy significant gains with RL, ultimately outperforming transformer based models under a compute-time constraint (Table 2).
* Scale alone is not enough. Our studies on increasing both model and dataset size show sub-log-linear scaling laws (Figure 5). The shallow slope of the data scaling laws we observe suggests that matching the performance of AutoAscend with imitation learning (via games played by bot) will require more than just scaling up demonstration count.

Possible avenues for future exploration include: (a) alternate methods for increasing the transformer context length to give the agent a more effective long-term memory; (b) explicitly addressing the multi-modal nature of the demonstration data (i.e. different trajectories can lead to the same reward), which is a potential confounder for BC methods. Some forms of distributional BC (e.g. GAIL , BeT ) might help alleviate this issue.

Finally, we hope that the hierarchical dataset that we created in this paper may be of value to the community in exploring the importance of hierarchy and goal-conditioning in long-horizon, open-ended environments.

## 8 Acknowledgements

We would like to thank Alexander N. Wang, David Brandfonbrener, Nikhil Bhattasali, and Ben Evans for helpful discussions and feedback. Ulyana Piterbarg is supported by the National Science Foundation Graduate Research Fellowship Program (NSF GRFP). This work was also supported by grants from Honda and Meta as well as ONR awards N00014-21-1-2758 and N00014-22-1-2773.