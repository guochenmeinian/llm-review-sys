# DiffGS: Functional Gaussian Splitting Diffusion

Junsheng Zhou

School of Software, Tsinghua University, Beijing, China

{zhou-js24,zwq23}@mails.tsinghua.edu.cn

Equal contribution. Yu-Shen Liu is the corresponding author.

###### Abstract

3D Gaussian Splitting (3DGS) has shown convincing performance in rendering speed and fidelity, yet the generation of Gaussian Splatting remains a challenge due to its discreteness and unstructured nature. In this work, we propose DiffGS, a general Gaussian generator based on latent diffusion models. DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization. The key insight is to represent Gaussian Splatting in a disentangled manner via three novel functions to model Gaussian probabilities, colors and transforms. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally. Meanwhile, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting. Project page: https://junshengzhou.github.io/DiffGS.

## 1 Introduction

3D content creation is a vital task in computer graphics and 3D computer vision, which shows great potential in real-world applications such as virtual reality, game design, film production, and robotics. Previous 3D generative models usually take Neural Radiance Field (NeRF)  as the representation. However, the volumetric rendering for NeRF requires considerable computational cost, leading to sluggish rendering speeds and significant memory burden. Recent advances of 3D Gaussian Splatting (3DGS)  have demonstrated its potential to serve as the next-generation 3D representation by enabling both real-time rendering and high-fidelity appearance modeling. Designing 3D generative models for 3DGS provides a scheme for real-time interaction with 3D creations.

The core challenge in generative 3DGS modeling lies in its discreteness and unstructured nature, which prevents the well-studied frameworks in structural image/voxel/video generation from transferring to directly generate 3DGS. Concurrent works  alternatively transport Gaussians into structural voxel grids with volume generation models  for generating Gaussians. However, these methods lead to 1) abundant computational cost for high-resolution voxels, and 2) limited number of generated Gaussians constrained by the voxel resolutions. Certain voxelization schemes  also introduce information loss, making it challenging to maintain high-quality Gaussian reconstructions.

To address these challenges, we present DiffGS, a novel diffusion-based generative model for general 3D Gaussian Splatting, which is capable of efficiently generating high-quality Gaussian primitives at arbitrary numbers. The key insight of DiffGS is to represent Gaussian Splatting in a disentangled manner via three novel functions: Gaussian Probability Function (GauPF), Gaussian Color Function (GauCF) and Gaussian Transform Function (GauTF). Especially, GauPF indicates the geometry of 3DGS by modeling the probabilities of each sampled 3D location to be a Gaussian location. GauCF and GauTF predict the Gaussian attributes of appearances and transformations given a 3D location as input, respectively. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with three continuous Gaussian Splatting functions.

With the disentangled and powerful representation, the next step is to design a generative model with the target of generating these Gaussian Splatting functions. We propose a Gaussian VAE model for creating compressed representation for the Gaussian Splatting functions. The Gaussian VAE learns a regularized latent space which maps the Gaussians Splitting functions of each shape into one latent vector. A latent diffusion model (LDM) is simultaneously trained at the latent space for generating novel 3DGS shapes. With the powerful LDM, we explore DiffGS to generate diverse 3DGS both conditionally and unconditionally. Finally, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. The key idea is to first extract 3D Gaussian geometry from GauPF by sampling 3D locations at the 3D spaces with the highest Gaussian probabilities, and then predict the Gaussian attributes with GauCF and GauTF. We illustrate the overview of DiffGS in Fig. 1.

We systematically summarize the superiority of DiffGS in terms of: 1) Efficiency, we design DiffGS based on Gaussian Splatting and Latent Diffusion Models, which shows significant efficiency in model training, inference and shape rendering. 2),3) Generality and quality, we generate native 3DGS without processes like voxelization, leading to unimpaired quality and generality in applying to downstream 3DGS applications. 4) Scalability, we scalably generate Gaussian primitives at arbitrary numbers. We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset, which demonstrate our non-trivial improvements over the state-of-the-art methods. In summary, our contributions are given as follows.

* We propose DiffGS, a novel diffusion-based generative model for general 3D Gaussian Splatting, which is capable of efficiently generating high-quality Gaussian primitives at arbitrary numbers.
* We introduce a novel schema to represent Gaussian Splatting in a disentangled manner via three functions to model Gaussian probabilities, Gaussian colors and Gaussian transforms, respectively. We simultaneously propose a discretization algorithm to extract Gaussians from these functions via octree-guided sampling and optimization.
* DiffGS achieves remarkable performances under various tasks including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation.

Figure 1: The illustration of DiffGS. We fit 3DGS from multi-view images and then disentangle it into three Gaussian Splatting Functions. We train a Gaussian VAE with a latent diffusion model for generating these functions, followed by a Gaussian extraction algorithm to obtain the final generated Gaussians.

## 2 Related Work

### Rendering-Guided 3D Representation

Neural implicit representations which learn signed [46; 79; 37] (unsigned [77; 78; 75]) distance functions or occupancy functions  have largely advanced the field of 3D generation [84; 76; 66; 36], reconstruction [80; 26; 24; 25; 45] and perception [82; 83; 81; 32; 31]. Remarkable progress have been achieved in the field of novel view synthesis (NVS) [41; 47; 62; 2; 43], with the proposal of Neural Radiance Field (NeRF) . NeRF implicitly represents scene appearance and geometries using MLP-based neural networks, optimized through volume rendering to achieve outstanding NVS quality. Some subsequent variants [1; 15; 49] have shown promising performance by advancing NeRF in terms of rendering quality, scalability and view-consistency. Additionally, more recent methods [43; 7; 14; 64] explore the training and rendering efficiency of NeRF by introducing feature-grids based 3D representations. Instant-NGP  highly accelerates NeRF learning by introducing multi-resolution feature grids based on hash table with fully-fused CUDA kernel implementations. However, the NeRF representations which require expensive neural network inferences during volume rendering, still struggles in the applications where real-time rendering is required.

Recently, the emergence of 3D Gaussian Splitting (3DGS) [28; 59; 30; 68; 71; 18; 74] has showcased impressive real-time results in novel view synthesis (NVS). 3DGS  has led to revolutions in the NVS field by demonstrating superior performances in multiple domains. However, the generation of Gaussian Splitting remains a challenge due to its discreteness and unstructured nature. In this paper, we introduce a novel schema to represent the discrete and unstructured 3DGS with three continuous Gaussian Splitting Functions, thus ingeniously tackle the challenge by designing generative models for the functions.

### 3D Generative Models

The field of creating 3D contents with generative models has emerged as a particularly captivating research direction. A series of studies [48; 33; 65; 52; 40; 36; 9; 56; 69; 60] focus on optimization-based frameworks based on Score Distillation Sampling (SDS), which achieve convincing generation performances by distilling 3D geometry and appearance of the radiance fields with pretrained 2D diffusion models [44; 21] as the prior. However, these studies entail significant computational costs due to time-consuming per-scene optimization. Going beyond optimization-based 3D generation, recent methods [42; 61; 63; 27] explore 3D generative methods based on diffusion models to directly learn priors from 3D datasets for generative radiance fields modeling, which typically represent radiance fields as structural triplanes [63; 55; 17] or voxels [61; 42; 11]. DiffRF  leverage a voxel based NeRF representation with 3D U-Nets as the backbone to train a diffusion model.

With the recent advances in 3DGS , designing a powerful 3D generative model for generating 3DGS is expected to be a popular research topic. This also brings significant challenges due to the discreteness and unstructured nature of 3DGS, which prevents the well-studied frameworks in structural image/voxel/video generation from transferring to directly generate 3DGS. A series of studies [70; 86; 22; 73] follow the schema of image-based reconstruction without generative modeling, which lack the ability to generate diverse shapes. Concurrent studies GaussianCube  and GVGEN  follow the voxel-based representations to transport Gaussians into structural voxel grids with volume generation models for generating Gaussians. However, these methods come with several drawbacks, including abundant computational costs for high-resolution voxels and a restricted number of generated Gaussians constrained by voxel resolutions. Some voxelization strategies  may introduce information loss, leading to difficulties in preserving high-quality Gaussian reconstructions. In contrast, our proposed DiffGS explores a new perspective to directly represent the discrete and unstructured 3DGS with three continuous Gaussian Splatting Functions. Though the insight, we design a latent diffusion model for efficiently generating high-quality Gaussian primitives by learning to generate the Gaussian Splatting Functions. DiffGS generates general Gaussians at arbitrary numbers with a specially designed octree-based extraction algorithm.

## 3 Method

We introduce DiffGS, a novel diffusion-based generative model for general 3D Gaussian Splatting, which is capable of efficiently generating high-quality Gaussian primitives at arbitrary numbers.

The overview of DiffGS is shown in Fig. 2. We first preview Gaussian Splatting in Sec. 3.1 and present the novel functional schema for representing Gaussian Splatting with three disentangled Gaussian Splatting Functions in Sec. 3.2. We then introduce the Gaussian Variational Auto-encoder and the Latent Diffusion Model for compressing and generative modeling on Gaussian Splatting Functions, as shown in Sec. 3.3. A novel discretization algorithm is further developed in Sec. 3.4 to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization.

### Preview Gaussian Splatting

3D Gaussian Splatting (3DGS)  represents a 3D shape or scene as a set of Gaussians with attributes to model the geometries and view-dependent appearances. For a 3DGS \(G=\{g_{i}\}_{i=1}^{N}\) containing \(N\) Gaussians, the geometry of i-\(th\) Gaussian is explicitly parameterized via 3D covariance matrix \(_{i}\) and its center \(_{i}^{3}\), formulated as:

\[g_{i}(x)=(-(x-_{i})^{T}^{-1}(x-_{i}) ),\] (1)

where the covariance matrix \(_{i}=r_{i}s_{i}s_{i}^{T}r_{i}^{T}\) is factorized into a rotation matrix \(r_{i}^{4}\) and a scale matrix \(s_{i}\). The appearance of the Gaussian \(g_{i}\) is controlled by an opacity value \(o_{j}\) and a color value \(c_{i}^{3}\). Note that the color is represented as a series of sphere harmonics coefficients in practice of 3DGS, yet we still keep its definition as three-dimension color \(c_{i}\) in our paper for a clear understanding on our method. To this end, the 3DGS \(G\) is defined as \(\{g_{i}=\{_{i},r_{i},s_{i},o_{i},c_{i}\}^{K}\}_{j=1}^{N}\), where \(K\) is dimension of the combined attributes in each Gaussian.

### Functional Gaussian Splatting Representation

The key challenge in generative 3DGS modeling lies in its discreteness and unstructured nature, which prevents the well-studied generative frameworks from transferring to directly generate 3DGS. We address this challenge by introducing to represent Gaussian Splatting in a disentangled manner via three novel functions: Gaussian Probability Function (GauPF), Gaussian Color Function (GauCF) and Gaussian Transform Function (GauTF), respectively. Through the novel disentangling of 3DGS, we represent the discrete and unstructured 3DGS with three continuous Gaussian Splatting Functions.

Figure 2: The overview of DiffGS. (a) We disentangle the fitted 3DGS into three Gaussian Splatting Functions to model the Gaussian probability, colors and transforms, respectively. We then train a Gaussian VAE with a conditional latent diffusion model for generating these functions. (b) During generation, we first extract Gaussian geometry from the generated GauPF, followed by the GauCF and GauTF to obtain the Gaussian attributes.

**Gaussian Probability Function.** Gaussian Probability Function (GauPF) indicates the geometry of 3DGS by modeling the probabilities of each sampled 3D location to be a Gaussian location. Given a set of 3D query location \(Q=\{q_{j}^{3}\}_{i=1}^{M}\) sampled in 3D space around a fitted 3DGS \(G=\{g_{i}^{3}\}_{j=1}^{N}\), the GauPF of \(G\) predicts the probabilities \(p\) of queries \(\{q_{j}\}_{i=1}^{M}\) to be a Gaussian location in \(G\), formulated as:

\[p_{j}=(q_{j}).\] (2)

The idea of Gaussian probability modeling comes from the observation that the further a 3D location \(q_{j}\) is from all Gaussians, the lower the probability that any Gaussian occupies the space at \(q_{j}\). Therefore the ground truth Gaussian probability of \(q_{j}\) is defined as:

\[(q_{j})=((||q_{j}-_{i }||_{2})),\] (3)

where \(||q_{j}-_{i}||_{2}\) indicates the distance from \(q_{j}\) to the nearest Gaussian center in \(\{_{i}\}_{i=1}^{N}\), \(\) is a truncation function which filters the extremely large values and \(\) is a continuous function which maps the query-to-Gaussian distances to probabilities in the range of .

A learned GauPF implicitly models the locations of 3D Gaussian centers, which is the key factor for generating high-quality 3DGS. The extraction of 3DGS centers from GauPF is then achieved with our designed Gaussian extraction algorithm which will be introduced in Sec. 3.4.

**Gaussian Color and Transform Modeling.** Gaussian Color Function (GauCF) and Gaussian Transform Function (GauTF) predict the Gaussian attributes of appearances and transformations from Gaussian geometries. Specifically, given the center \(_{i}\) of a Gaussian \(g_{i}\) in \(G\) as input, GauCF predicts the color attribute \(c_{i}\) and GauTF predicts the rotation \(r_{i}\), scale \(s_{i}\) and opacity \(o_{i}\), formulated as:

\[\{c_{i}\}=(_{i});\ \ \{r_{i},s_{i},o_{i}\}=( _{i}).\] (4)

Note that GauCF and GauTF mainly focus on predicting the Gaussian colors and transforms from 3D Gaussian centers. This is different from the GauPF which models the probabilities of query samples in the 3D space. The reason is that GauPF focuses on exploring the geometry of 3DGS from the 3D space, while GauCF and GauTF learn to predict the Gaussian attributes from the known geometries.

Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with three continuous Gaussian Splatting Functions. The functional representation is a general and flexible term for 3DGS which has no restrictions on the Gaussian numbers, densities, geometries, etc.

### Gaussian Variational Auto-encoder and Latent Diffusion

With the disentangled and powerful representation, the next step is to design a generative model with the target of generating these Gaussian Splatting Functions. We follow the common schema to design a Gaussian Variational Auto-encoder (VAE)  with a Latent Diffusion Model (LDM)  as the generative model. The detailed framework and the training pipeline of DiffGS are illustrated in Fig. 1(a).

**Gaussian VAE.** The Gaussian VAE compresses the Gaussian Splatting Functions into a regularized latent space by mapping the Gaussian Splatting Functions of each 3DGS shape into a latent vector, from which we can also recover the Gaussian Splatting Functions. Specifically, the Gaussian VAE consists of 1) a GS encoder \(_{en}\) to learn representations from 3DGS and encodes each 3DGS into a latent vector \(z\), 2) a triplane decoder \(_{de}\) which decodes the latent \(z\) into a feature triplane, and 3) three neural predictors \(_{pf},_{cf}\) and \(_{tf}\) which serve as the implementation of GauPF, GauCF and GauTF to predict Gaussian probabilities, colors and transforms, respectively.

Given a fitted 3DGS \(G=\{g_{i}\}_{j=1}^{N}\) as input, the GS encoder \(_{en}\) extracts a global latent feature \(z\) from \(G\), which is then decoded into a feature triplane \(t^{H W C 3}\) with the decoder \(_{de}\), formulated as:

\[z=_{en}(G);\ \ t=_{de}(z).\] (5)

The triplane \(t\) consists of three orthogonal feature planes \(\{t_{XY},t_{XZ},t_{YZ}\}\) which are aligned to the axex. For a 3D location \(q_{j}\), we obtain its corresponding feature \(f_{j}=interp(t,q_{j})\) from the triplane \(t\) by projecting \(q_{j}\) onto the orthogonal feature planes and concatenating the tri-linear interpolatedfeatures at the three planes. We then predict the Gaussian probability, color and transform of \(q_{j}\) with the neural Gaussian Splatting Function predictors as:

\[\{_{i}\}=_{pf}(f_{j});\ \ \{_{i}\}=_{cf}(f_{j});\ \ \{_{i},_{i},_{i}\}=_{tf}(f_{j}).\] (6)

The Gaussian VAE is trained with the target of accurately predicting Gaussian attributes and robustly regularizing the latent space. In practice, the training objective is formulated as:

\[_{}=\|\{,,,,\} -\{p,c,r,s,o\}\|_{1}+(D_{KL}(_{ }(z|G)\|(z))).\] (7)

The first loss term indicates the \(_{1}\) loss between the predicted Gaussian attributes in Eq. (6) and the target ones defined by the ground truth Gaussian Splatting Functions in Eq. (2) and Eq. (4). The second term in Eq. (7) is the KL-divergence loss with a factor of \(\), which constrains on the regularization of the learned latent space of \(z\). Specifically, we define the inferred posterior of \(z\) as the distribution \(_{}(z|G)\), which is regularized to align with the Gaussian distribution prior \((z)=(0,I)\), where \(I\) is the standard deviation.

**Gaussian LDM.** With the trained Gaussian VAE in place, we are now able to encode any 3DGS into a compact 1D latent vector \(z\). We then train a latent diffusion model (LDM)  efficiently on the latent space. A diffusion model is trained to generate samples from a target distribution by reversing a process that incrementally introduces noise. We define \(\{z_{0},z_{1},...,z_{K}\}\) as the forward process \((z_{0:K})\) which gradually transforms a real data \(z_{0}\) into Gaussian noise (\(z_{T}\)) by adding noises. The backward process \((z_{0:K})\) leverages a neural generator \(\) to denoise \(z_{K}\) into a real data sample.

To achieve controllable generation of 3DGS, we introduce a conditioning mechanism  into the diffusion process with cross-attention. Given an input condition \(y\) (e.g. text, image, partial 3DGS), we leverage a custom encoder \(\) to project \(y\) into the condition embedding \((y)\). The embedding is then fused into the generator \(\) with cross attention modules. Following DDPM, we simply adopt the optimizing objective to train the generator for predicting noises \(_{}\), formulated as:

\[_{}=_{z_{0},t,(0,I)} [\|-_{}(z_{t},(y),t)\| ^{2}],\] (8)

where \(t\) is a time step and \(\) is a noise latent sampled from the Gaussian distribution \((0,I)\), respectively. We adopt the well-studied architecture DALLE-2  as the LDM implementation.

### Gaussian Extraction Algorithm

The final step for the generation process of DiffGS is to extract 3DGS from the generated Gaussian Splatting Functions, similar to the effect of Marching Cubes algorithm  which extracts meshes from Signed Distance Functions. The key factor is to extract the geometries of 3DGS, i.e., Gaussian locations and the appearances of 3DGS, i.e., colors and transforms. The full generation pipeline is shown in Fig. 2(b).

**Octree-Guided Geometry Sampling.** The locations of 3D Gaussian centers indicate the geometry of the represented 3DGS. We aim to design a discretization algorithm to obtain the discrete 3D locations from the learned continuous Gaussian Probability Function parameterized with the neural network \(_{pf}\), which models the probability of each query sampled in the 3D space to be a 3D Gaussian

Figure 3: Gaussian geometry extractions from generated GauPF. The yellow and green regions indicate the high probability area and the low probability area judged by GauPF. (a),(b) and (c) show the progressively octree build process at depth 1,2 and \(L\). (d) We sample proxy Gaussian centers from the octree at final depth \(L\). (e) We optimize proxy centers to the exact geometry indicated in GauPF.

location. To achieve this, we design an octree-based sampling and optimization algorithm which generates accurate center locations of 3D Gaussians at arbitrary numbers.

We show the 2D illustration of the algorithm in Fig. 3. Assume that the 3D space is divided into the high probability area (the yellow region) and the low probability area (the green region) by the generated GauPF. We aim to extract the geometry as the locations with high probabilities. A naive implementation is to densely sample queries in the 3D space and keep the ones with large probabilities as outputs. However, it will lead to high computational cost for inferencing and the discrete sampling also struggles to accurately reach the locations with largest probabilities in the continuous GauPF. We get inspiration from octree  to design a progressive strategy which only explores the 3D regions with large probabilities in current octree depth for further subdivision in the next octree depth. After \(L\) layers of octree subdivision, we reach the local regions with largest probabilities, from where we uniformly sample \(N\) 3D points as the proxy points \(\{_{i}\}_{i=1}^{N}\) representing coarse locations of Gaussian centers.

**Optimizing Geometry with GauPF.** To further refine the proxy points to the exact locations of Gaussian centers with largest probabilities in GauPF, we propose to further optimize the proxy points with the supervision from learned GauPF \(_{pf}\). Specifically, we set the position of proxy points \(\{_{i}=\{ x_{i}, y_{i}, z_{i}\}\}_{i=1}^{N}\) to be learnable and optimize them to reach the positions \(\{}\}_{i=1}^{N}\) with largest probabilities of \(_{pf}\). The optimization target is formulated as:

\[_{}=-_{i=1}^{N}_{pf}(_{i}).\] (9)

Note that we can set \(N\) to arbitrary numbers, enabling DiffGS to generate 3DGS with no limits on the density and resolution.

**Extracting Gaussian Attributes.** We now obtain the estimated geometry indicating the predicted Gaussian centers \(\{}\}_{i=1}^{N}\). We then extract the appearances and transforms from the generated triplane \(t\), Gaussian Color Function \(_{cf}\) and Gaussian Transform Function \(_{tf}\) as

    &  &  \\   & FID-50K \(\) & KID-50K (\%e) \(\) & FID-50K \(\) & KID-50K (\%e) \(\) \\  GET3D  & — & — & 59.51 & 2.414 \\ DiffTF  & 110.8 & 9.173 & 93.02 & 6.708 \\ Ours & **47.03** & **3.436** & **35.28** & **2.148** \\   

Table 1: Comparisons of unconditional generation under ShapeNet  dataset.

Figure 4: Visual comparisons with state-of-the-arts on unconditional generation of ShapeNet Chairs.

\(\{}\}=_{cf}(interp(t,}))\) and \(\{},},}\}=_{tf}(interp(t,}))\). Finally, the general 3DGS is now generated as \(=\{},},},},}\}_{i=1}^ {N}\).

## 4 Experiment

### Unconditional Generation

**Dataset and Metrics.** For unconditional generation of 3D Gaussian Splatting, we conduct experiments under the airplane and chair classes of ShapeNet  dataset. Following previous works [42; 4], we report two widely-used image generation metrics Frechet Inception Distance (FID)  and Kernel Inception Distance (KID)  for evaluating the rendering quality of our proposed DiffGS and previous state-of-the-art works. The metrics are evaluated between 50K renderings of the generated shapes and 50K renderings of the ground truth ones, both at the resolution of 1024\(\)1024.

**Comparisons.** We compare DiffGS with the state-of-the-art methods in terms of the rendering quality of generated shapes, including the GAN-based methods GET3D  and the diffusion-based method DiffTF . The quantitative comparison is shown in Tab. 1, where DiffGS achieves the best performance over all the baselines. We further show the visual comparison on the renderings of some generated shapes in Fig. 4, where the GAN-based GET3D struggles in generating complex shapes and the generations of DiffTF is blurry with poor textures. In contrast, DiffGS produces significantly more visual-appealing and high-fidelity generations in terms of rendering and geometry qualities.

### Conditional Generation.

We explore the conditional generation ability of DiffGS given texts, images and partial 3DGS as the input conditions. All the experiments are conducted under the chair class of ShapeNet  dataset with commonly used data splits in previous methods [10; 35].

**Text/Image-conditional Gaussian Splatting Generation.** For introducing texts/images as the conditions for controllable Gaussian Splatting generation, we leverage the frozen text and image encoder from the pretrained CLIP  model as the implementation of custom text encoder \(_{text}\) and \(_{image}\) for achieving text/image embeddings. We then train DiffGS with the conditional optimization objective in Eq.(8). We show the visualization of some text/image conditional generations produced by DiffGS in Fig. 5(a) and Fig. 5(b). The results show that DiffGS accurately recovers the semantics and geometries described in the text prompts and the images, demonstrating the powerful capability of DiffGS in generating high-fidelity 3DGS from text descriptions or vision signals.

**Gaussian Splatting Completion.** Additionally, we explore an interesting task of Gaussian Splatting completion. To the best of our knowledge, we are the first to focus and introduce solutions for this task. Specifically, the Gaussian Splatting completion task is to recover the complete 3DGS from a

Figure 5: Visualization of conditional 3DGS generation results on ShapeNet. (a) Text conditional generation. (b) Image conditional generation. (c) Gaussian Splatting completion.

partial 3DGS which contains large occlusions. In real-world applications, having only sparse views with limited viewpoint movement available for optimizing 3DGS often results in a partial 3DGS. Solving Gaussian Splatting completion task enables us to infer the complete and dense 3DGS from the partial ones for improving the rendering quality at invisible viewpoints.

We introduce DiffGS with partial 3DGS as the conditions for solving this task. Specifically, we simply leverage a modified PointNet  as the custom encoder \(_{partial}\) for partial 3DGS. Fig. 5(c) presents the visualization of Gaussian Splatting completion results produced by DiffGS. The results show that DiffGS is capable of recovering complex geometries and detailed appearances from highly occluded 3DGS. Please refer to the appendix for implementation details on Gaussian Splatting completion.

### Point-to-Gaussian Generation

We further introduce DiffGS for another challenging and vital task of Point-to-Gaussian generation. This task aims to generate the Gaussian attributes given a 3D point cloud as input. The task serves as the bridge between the easily accessible point clouds and the powerful 3DGS representation which efficiently models high-quality 3D appearances.

**Dataset and Implementation.** We conduct experiments under the chair and airplane classes of ShapeNet and also the widely-used garment dataset DeepFashion3D . The DeepFashion3D dataset is a real-captured 3D dataset containing complex textures. For implementing Point-to-Gaussian, we simply train the Gaussain VAE with the three-dimension point clouds as inputs, instead of the 3DGS with attributes. Please refer to the Appendix for more details on data preparation and implementation.

**Performances.** We provide the visualization of some Point-to-Gaussian fitting and generation results in Fig. 6. We shown the fitting results for Deepfashion3D  dataset and the generation results for the test set of airplane and chair classes in ShapeNet . DiffGS produces visual-appealing 3DGS generations given only 3D point cloud geometries as inputs. The results demonstrate that DiffGS can accurately predict Gaussian attributes for 3D point clouds. We believe DiffGS provides a new direction for 3DGS content generation by connecting 3DGS with point clouds.

### Ablation Study

To evaluate some major designs and important hyper-parameters in DiffGS, we conduct ablation studies under the chair class of ShapeNet dataset. We report the performance in terms of PSNR, SSIM and LPIPS of the reconstructed 3DGS with Gaussian VAE.

**Framework Designs.** We first evaluate some major designs of our framework in Tab. 2. We justify the effectiveness of introducing the truncation function \(\) when modeling GauPF and report the results without \(\) as 'w/o truncation'. We then explore implementing the projection function \(\) either as \((x)=e^{-x}\) (as shown in 'Exponent') or as a linear projection (as

  Method & PSNR & SSIM & LPIPS \\  w/o Trunction & 29.39 & 0.9792 & 0.0173 \\ Exponent & 29.74 & 0.9765 & 0.0188 \\ w/o Optimization & 30.34 & 0.9875 & 0.0152 \\  Ours & **34.01** & **0.9879** & **0.0149** \\  

Table 2: Ablations on framework designs.

Figure 6: Visualization of Point-to-Gaussian fittings on Deepfashion3D and generations on ShapeNet.

shown in 'Ours'). We also show the results without the optimization process during Gaussian extraction as 'w/o Optimization', which demonstrates the effectiveness of optimizing Gaussians to the exact locations.

**Gaussian Numbers.** One significant advantage of DiffGS lies in the ability of generating high-quality Gaussians at arbitrary numbers. To explore how the number of Gaussians affects the rendering quality, we conduct ablations on the Gaussian numbers as shown in Fig. 3. The results demonstrate that denser Gaussians lead to better quality.

## 5 Conclusion

In this paper, we introduce DiffGS for generative modeling of 3DGS. DiffGS disentangled represent 3DGS via three novel functions to model Gaussian probabilities, colors and transforms. We then train a latent diffusion model with the target of generating these functions both conditionally and unconditionally. DiffGS generates 3DGS with arbitrary numbers by an octree-guided extraction algorithm. The experimental results on various tasks demonstrate the superiority of DiffGS.

## 6 Acknowledgement

This work was supported by National Key R&D Program of China (2022YFC3800600), the National Natural Science Foundation of China (62272263, 62072268), and in part by Tsinghua-Kuaishou Institute of Future Media Data.