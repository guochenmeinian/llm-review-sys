# ActAnywhere: Subject-Aware Video Background Generation

Boxiao Pan

Stanford University

&Zhan Xu

Adobe Research

&Chun-Hao Paul Huang

Adobe Research

&Krishna Kumar Singh

Adobe Research

&Yang Zhou

Adobe Research

&Leonidas J. Guibas

Stanford University

&Jimei Yang

Runway

Work done during an internship at Adobe.

###### Abstract

We study a novel problem to automatically generate video background that tailors to foreground subject motion. It is an important problem for the movie industry and visual effects community, which traditionally requires tedious manual efforts to solve. To this end, we propose ActAnywhere, a video diffusion model that takes as input a sequence of foreground subject segmentation together with an image of a novel background, and generates a video of the subject interacting in this background. We train our model on a large-scale dataset of 2.4M videos of human-scene interactions. Through extensive evaluation, we show that our model produces videos with realistic foreground-background interaction while strictly following the guidance of the condition image. Our model generalizes to diverse scenarios including non-human subjects, gaming and animation clips, as well as videos with multiple moving subjects. Both quantitative and qualitative comparisons demonstrate that our model significantly outperforms existing methods, which fail to accomplish the studied task. Please visit our project webpage at https://actanywhere.github.io.

## 1 Introduction

Compositing an acting video of a subject onto a novel background is central for creative story-telling in filmmaking and visual effects. The key requirement is seamlessly integrating the foreground subject with the background in terms of camera motions, interactions, lighting and shadows, so that the composition looks realistic and vivid as if the subject acts physically in the scene. In movie industry, this process is often conducted by virtual production  that requires artists to first create a 3D scene and then to film the acting video in an LED-walled studio or to render the video in 3D engines. This process is not only tedious and expensive, but most importantly, prevents artists from quickly iterating their ideas.

Inspired by this artistic workflow, we study the novel problem of _automated_ subject-aware video background generation. As shown in fig. 1, given a foreground segmentation sequence that provides the subject motion, as well as a condition frame that describes a scene, we aim to generate a video that adapts the subject to this scene with realistically synthesized foreground-background interaction. This condition frame can be either a background-only image, or a composite frame consisting of both background and foreground created via photo editing tools  or generative image model .

This problem, at its core, requires retaining part of the input video while generating the rest to adapt to it. To the best of our knowledge, the closest works to this setting are those on video editing and inpainting / outpainting. Video editing methods assume a source video as input and make editsbased on condition signals, such as natural language or image. In comparison, our task poses unique challenges that these methods cannot solve. First, the foreground of the video needs to be retained and sometimes refined on the boundary (as shown in fig. 8 in appendix), while prior works generally perform holistic changes to the entire video . Second, existing methods most commonly condition the editing process on a short text prompt , which only poses loose constraints that are not enough for artists' creative intentions. Those that can condition on an image  are not able to generate contents that strictly follow the guidance. We demonstrate such comparisons in fig. 4. On the other hand, video inpainting / outpainting methods  aim to perform context-aware removal / expanding for a video. However, these methods focus on pixel harmonization, while our emphasis is on taking additional control signal that indicates the desired background, and synthesizing large and dynamic background region with reasonable interaction with the given foreground.

To this end, we propose a video diffusion model that explicitly controls the foreground motion with the foreground segmentation sequence, and additionally, conditions the generation on an image that describes the desired background. We train the model with a designed self-supervised learning procedure that takes the foreground segmentation sequence of the input video to predict the original video, conditioned on a randomly sampled frame. This training procedure enables the model to 1) retain the foreground subject; 2) hallucinate missing details from imperfect segmentation; and 3) adhere to the guidance of the condition frame while being robust to the subject's pose in it.

Figure 1: Given a sequence of foreground segmentation as input (top), and one frame that describes the background as the condition (left), ActAnywhere generates coherent video background that adapts to the subject motion. We show two subjects here, each with two generated samples. ActAnywhere is able to generate videos consistent with the condition frame with highly realistic details such as splatting water, moving smoke and flame, shadows, duck feet, etc. It generalizes to a diverse distribution of subjects and backgrounds, including non-human subjects. Our method works with both composited frames and background-only images as the condition.

Moreover, we propose to express the condition signal as the CLIP features of the image, which we show empirically outperforms its alternatives. We also concatenate a background mask with the segmentation as the input to better indicate the region to be generated.

We train our model on a large-scale dataset  that consists of 2.4M videos of human-scene interactions and evaluate both on a held-out set as well as on videos from DAVIS . ActAnywhere is able to generate highly realistic videos that follow the condition frame, and at the same time synthesizes video background that conforms to the foreground motion. Notably, despite being trained solely on videos of humans, ActAnywhere generalizes to non-human subjects, such as animals and man-made objects, in a zero-shot manner.

In summary, our contributions are:

1. We introduce a novel problem of automated subject-aware video background generation.
2. We propose ActAnywhere, a video diffusion model to solve this task, and train it on a large-scale human-scene interaction video dataset in a self-supervised manner.
3. Extensive evaluations demonstrate that our model generates coherent videos with realistic subject-scene interactions, camera motions, lighting and shadows, and generalizes to out-of-distribution data including non-human subjects, gaming and animation clips, and videos with multiple moving subjects.

## 2 Related Work

**Video generation**. There have been a long thread of works on video generation. The core architecture has evolved from GANs [11; 38; 41] to more recent transformers [15; 40; 46; 49] and diffusion models [6; 9; 17; 19; 20; 24; 47]. Below we review the most related diffusion-based works. Most of these works leverage temporal self-attention blocks inside the denoising U-Net in order to acquire temporal awareness. On top of that, Text2Video-Zero  introduces additional noise scheduling to correlate the latents in a video. LVDM  and Align Your Latents  both design a hierarchical approach to generate longer-term videos. Align Your Latents additionally fine-tunes a spatial super-resolution model for high-resolution video generation. AnimateDiff  proposes to train the temporal attention blocks on a large-scale video dataset, which can then be inserted into any text-to-image diffusion models (given that the architecture fits) to turn that into a text-to-video model, in a zero-shot manner. VideoCrafter1  further uses dual attention to enable joint text and image-conditioned generation. These works focus on unconditional generation or with text or image conditioning, but are not able to follow the guidance of additional foreground motion.

**Video editing**. Another thread studies the problem of video editing, where a source video is given as input, and edits are performed according to some condition signals. Text2Live  uses pre-trained video atlases of the input video, and performs text-guided edits on the foreground or background. Gen1  leverages depth maps estimated by a pre-trained network  as an additional condition to improve the structural consistency. Tune-A-Video  proposes to finetune only part of the spatial-attention blocks and all of the temporal-attention blocks on a single input video. TokenFlow  uses latent nearest neighbor fields computed from the input video to propagate edited features across all frames. Both VideoControlNet  and Control-A-Video  adopt a ControlNet -like approach to condition the video diffusion process with additional signals such as depth maps or Canny edges extracted from the input video. As stated above, these works apply holistic changes to the entire video and are not able to retain the subject. Moreover, they either take only loose constraints from text conditioning, or are not able to strictly follow the image guidance.

One major downside of these works is hence that the generated videos tend to keep the spatial structure from the source video, which limits the edits that the model can perform to stylistic changes. In our work, we propose to condition on the foreground segmentation for the motion, while extract the background information only from one condition frame. In particular, using the masked foreground as input endows a nice separation as in what to preserve and what to generate.

**Image and video inpainting**. Image / video inpainting aims to fill a missing region, often expressed as a mask. Image inpainting methods either take condition signals such as natural language and image [34; 44; 45], or rely solely on the context outside the masked region [14; 36; 46; 49]. Recent diffusion-based image inpainting methods use a combination of masked image and the mask itself, and condition the diffusion process either on natural language [34; 44] or an image of the condition object , or perform unconditional diffusion . For video inpainting, MAGVIT  proposes a generative video transformer trained through masked token prediction, and is able to inpaint small masked regions afterwards. ProPainter  designs a flow-based method by propagating pixels and features through completed flows. M3DDM  leverages a video diffusion model, and conditions the diffusion process on global video features extracted by a video encoder. Different from these works, we aim to generate large background regions that strictly follow the condition frame. Moreover, the generated background needs to adapt to the foreground subject motion in a coherent way. This poses significant challenges that previous inpainting methods cannot tackle.

## 3 Method

We first provide essential preliminary background on latent diffusion in section 3.1. We then formally define our problem in section 3.2 and delve into our model design in section 3.3. Finally, we specify the training details in section 3.4.

### Preliminaries on Latent Diffusion Models

Diffusion models such as DDPM , encapsulate a forward process of adding noise and a backward process of denoising. Given a diffusion time step \(\), the forward process incrementally introduces Gaussian noises into the data distribution \(x_{0} q(x_{0})\) via a Markov chain, following a predefined variance schedule denoted as \(\):

\[q(_{}|_{-1})=(_{};}_{-1},_{})\] (1)

For the backward process, a U-Net \(_{}\) is trained to denoise \(_{}\) and recover the original data distribution:

\[p_{}(_{-1}|_{})=(_{ -1};_{}(_{},),_{}( _{},))\] (2)

\(_{}\) and \(_{}\) are parametrized by \(_{}\). The discrepancy between the predicted noise and the ground-truth noise is minimized as the training objective.

Stable Diffusion  further proposes to train the diffusion model in the latent space of a VAE . Specifically, an encoder \(\) learns to compress an input image \(x\) into latent representations \(z=(x)\), and a decoder \(\) learns to reconstruct the latents back to pixel space, such that \(x=((x))\). In this way, the diffusion is performed in the latent space of the VAE.

### Problem Formulation

Given an input video \(^{T H W 3}\) featuring a foreground subject, we first deploy a segmentation algorithm, such as Mask R-CNN , to obtain a subject segmentation sequence, \(^{T H W 3}\), along with the corresponding masks, \(^{T H W 1}\). Both \(\) and \(\) serve as input to our model. \(\) contains the segmentation of the foreground subject, with background pixels set to 127 (grey). \(\) has the foreground pixels set to 0 and background to 1. Across all our experiments, \(H=W=256\) and \(T=16\).

Additionally, we also incorporate a single condition frame \(^{H W 3}\) describing the background that we want to generate. As shown in fig. 2, \(\) is a randomly sampled frame from \(\) at training time, while can be either a frame showing foreground-background composition or a background-only image at inference time. The goal is thus to generate an output video \(\) with the subject dynamically interacting with the synthesized background. The motivation of using an image not language as the condition is that image is a more straightforward media to carry detailed and specific information of the intended background, especially when users already have a pre-defined target scene image.

### Subject-Aware Latent Video Diffusion

We build our model based on latent video diffusion models . In our architecture design, we address two main questions: 1) providing the foreground subject sequence to the network to enable proper motion guidance, and 2) injecting the condition signal from the background frame to make the generated video adhere to the condition.

We present our pipeline in fig. 2. For the foreground segmentation sequence \(\), we use the pre-trained VAE  encoder \(\) to encode the foreground segmentation into latent features \(}^{16 32 32 4}\). We downsample the foreground mask sequence \(\) 8 times to obtain the resized mask sequence \(}^{16 32 32 1}\) to align with the latent features \(}\). To train the denoising network \(_{}\), we encode the original frames \(\) with the same VAE encoder into latent representation \(^{16 32 32 4}\), and add noises at diffusion time step \(\) with the forward diffusion process denoted in eq. (1) to get noisy latent feature \(_{}\). We subsequently concatenate \(}\), \(}\) and \(_{}\) along the feature dimension, forming a 9-channel input feature \(_{}^{1}^{16 9 32 32}\) to the U-Net. During inference, \(_{0}\) is initialized as Gaussian noises, and gets auto-regressively denoised for multiple time steps to sample a final result, according to the backward diffusion process described in eq. (2). The denoised latents are then decoded to a video via the VAE decoder \(\).

We build our 3D denoising U-Net based on AnimateDiff . AnimateDiff works by inserting a series of motion modules in between the spatial attention layers in the denoising U-Net of a pre-trained T2I diffusion model. These motion modules consist of a few feature projection layers followed by 1D temporal self-attention blocks.

For the condition image \(\), we encode it with the CLIP image encoder  and take the features from the last hidden layer as its encoding \(^{c}\). These features are then injected into the UNet \(_{}\) through its cross-attention layers, similar to [26; 34]. We empirically find that this method achieves better temporal consistency compared to other alternatives, such as using VAE features for either cross-attention or concatenation with other input features. We ablate on this in table 3.

### Training

Training is supervised by a simplified diffusion objective, namely predicting the added noise :

\[=||-_{}(_{}^{i},,^{c})||_{2}^{2}\] (3)

where \(\) is the ground-truth noise added.

**Dataset**. We train on the large-scale dataset compiled and processed by , which we refer to as HiC+. The resulting dataset contains 2.4M videos of human-scene interactions. It also provides foreground segmentation and masks. We refer the reader to the original paper for more details.

Figure 2: **Architecture overview**. During training, we take a randomly sampled frame from the training video to condition the denoising process. At test time, the condition can be either a composited frame of the subject with a novel background, or a background-only image.

**Pre-trained weights**. We initialize the weights of our denoising network \(_{}\) with the pre-trained weights from the Stable Diffusion image inpainting model 1, which is fine-tuned on top of the original Stable Diffusion on the text-conditioned image inpainting task. We initialize the weights of the inserted motion modules with AnimateDiff v24.

For the CLIP image encoder, we use the "clip-vit-large-patch14" variant4 provided by OpenAI, whose features from the last hidden layer have a dimension of 1024, while the pre-trained U-Net takes in features of dimension 768 as the condition, which are also in the text feature space. To account for this, we train an additional two-layer MLP to project the features into the desired space.

During training, we freeze the shared VAE and the CLIP encoder, and fine-tune the entire U-Net.

**Data processing and augmentation**. Obtaining perfect segmentation masks from videos is challenging. The masks may be incomplete, missing some parts of the foreground, or be excessive such that they include leaked background near the boundary. To deal with incomplete segmentation, during training, we apply random rectangular cut-outs to the foreground segmentation and masks. We provide more information on this in section 7.2 of the appendix. To reduce information leak from excessive segmentation, we perform image erosion to the segmentation and masks with a uniform kernel of size 5 \(\) 5, both during training and inference.

**Random condition dropping**. In order to enable classifier-free guidance at test time, we randomly drop the segmentation and the mask, the condition frame, or all of them at 10% probability each during training. In these cases we set them to zeros before passing into the respective encoders.

**Other details**. We use the AdamW  optimizer with a constant learning rate of 3e-5. We train on 8 NVIDIA A100-80GB GPUs with batch size 4, which takes approximately a week to fully converge.

## 4 Experiments

We start by describing the data used for evaluation. We then show diverse samples generated from our method in section 4.1, both using an inpainted frame and a background-only frame as the condition. In section 4.2 and section 4.3, we compare with baselines through qualitative and quantitative evaluations, including a user study. In section 4.4, we provide ablation study results on key design choices. In the appendix, we include additional results on general video inpainting / outpainting, generalization to videos from diverse domains, and robustness to inaccurate segmentation, along with further implementation details and a discussion on limitations and potential ethical impacts.

We strongly encourage the reader to check our webpage, where we show extensive videos on video background generation with diverse generated contents and camera motions, and under various condition scenarios. It also contains the video version of the comparison with baselines.

**Evaluation data**. Following prior works [5; 10; 13; 16; 42], we compare with previous works on videos sampled from the DAVIS  dataset. We select videos with both human and non-human subjects. We also evaluate qualitatively and perform ablation study on held-out samples from the HiC+ dataset following the original data splits . Samples with our method are generated with 50 denoising steps, with a guidance scale  of 5.

### Diverse Generation with ActAnywhere

In fig. 3, we show results on the held-out segmentation sequences from the HiC+ dataset, using an inpainted frame or a background-only frame as condition. ActAnywhere generates highly realistic foreground-background interactions both at coarse and fine levels. At a coarse level, our model synthesizes road structure, pumpkin field, city views, waves, etc. that align with the subject's motion. While at a fine level, our method also generates small moving objects that are in close interaction with the subject, such as the buckets, bed sheets, horses and dune buggies, as well as the dog. Moreover, these generation stay consistent across frames, and tightly follow the guidance in the condition frame. The synthesized backgrounds also exhibit coherent scale, lighting, and shadows (also see fig. 1).

Figure 3: **Diverse results from our method**. The top part shows examples using inpainted frames as condition, while bottom contains examples with background-only conditioning. Foreground sequences are from the held-out set of HiC+.

### Qualitative Comparison

**Baselines**. We first clarify that since we study a _novel problem_, and there is no prior work operating under the exact same setting to the best of our knowledge. We hence compare to closest works and adapt some, _i.e_. AnimateDiff , if necessary. Nonetheless, we emphasize that the formulation and pipeline are the core contribution of this work.

We compare ActAnywhere to a number of baselines, which we classify based on whether they do (fig. 4 middle) or do not (fig. 4 bottom) take a video as input. For the methods taking a video as input, Gen1  takes an additional image as condition, and also leverages a pre-trained depth-estimation network . Given pre-trained neural atlases , Text2LIVE  assumes a text prompt as condition to synthesize the edited video. TokenFlow  also uses text conditioning. Control-A-Video  first extracts Canny edges from the input video, then synthesizes the output video conditioned jointly on the edges and text.

Figure 4: **Comparison with baselines.** We provide results on two videos sampled from the DAVIS  dataset. For each example, we show three representative frames (top) and their corresponding condition signal (left). Note that different methods assume different input, conditioning or pre-trained models, as specified in section 4.2.

For baselines that do not take a video as input, the original AnimateDiff  only uses text conditioning. We use the strategy contributed by a public pull request4 to make it take additional image conditioning. Specifically, at test time, latent features are first extracted from the condition image with the pre-trained SD VAE encoder , which are then merged with the original per-frame Gaussian noises through linear blending. The diffusion process is later conditioned on a text prompt too. VideoCrafter1  provides both a text-to-video and an image-to-video model. We use the latter for a closer comparison setting.

**Results**. The qualitative comparison on two examples from the DAVIS  dataset is shown in fig. 4. Our method generates temporally coherent videos that follow the foreground motion with highly realistic details, _e.g_. falling snow and snow on the car windshield, while strictly following the guidance and constraints given by the condition frame. Baseline methods in the first category generally inherit the structure present in the input video, _e.g_. road direction, horse, etc., and hence they completely fail when fine-grained edits are desired, _e.g_. horse changes to motorcycle in the second case. Methods in the second category generate unconstrained motion due to lack of guidance (VideoCrafter1 in the second example generates backward motion, which is more evident in the video on our webpage).

### Quantitative Comparison

**Baselines**. We compare with Control-A-Video , TokenFlow  and Gen1 . We also conduct a user study for preferences in comparison to Gen1.

**Metrics**. We evaluate the consistency to the condition image, the temporal consistency of the generated videos, as well as the general generation quality. Specifically, we report the average cosine similarity between the CLIP  image embeddings of all generated frames and that of the condition image (_CLIP_cond_), between all pairs of generated frames (_CLIP_temp_), and the FVD score  against a set of real videos.

For the user study, we ask the participants if they prefer results from our model vs. those from Gen1 on: 1) consistency to condition image; 2) visual quality; and 3) temporal consistency. The results are presented as the percentage of our model being preferred over Gen1.

**Results**. Results on 30 videos from DAVIS are reported in table 1. Our model outperforms baselines across all metrics, particularly by a big margin on _CLIP_cond_, suggesting that our model is able to generate videos that tightly follow the guidance of the condition frame. For the user study, we randomly select 20 videos and ask 16 participants for their preference over Gen1 . Results are shown in table 2. Our method is strongly preferred on these key aspects.

### Ablation Study

We study different choices of conditioning by experimenting with three variants. _VAE Concat_ uses the same VAE encoder \(\) to extract features for the condition frame \(\), and concatenates with the input \(_{}^{i}\) along the feature dimension. _VAE Cross-Attn_ leverages the VAE features \(()\) through cross-attention instead of concatenation. _No Mask_ does not concatenate the masks \(\) with the foreground segmentation \(\) and noise \(_{}\) for the input.

The results on the held-out set of HiC+ are shown in table 3. Providing the condition in the CLIP feature space through cross-attention provides global semantic guidance to the entire diffusion process, while VAE features pose stricter spatial constraints, especially through concatenation, hence making it harder to produce coherent videos that conform to the condition. Providing the mask as

  
**Method** & **CLIP\_cond**\(\) & **CLIP\_temp**\(\) & **FVD**\(\) \\  Control-A-Video  & 0.643 & 0.942 & 381.8 \\ TokenFlow  & 0.762 & 0.943 & 323.0 \\ Gen1  & 0.827 & 0.943 & 337.2 \\ Ours & **0.862** & **0.945** & **313.4** \\   

Table 1: **Quantitative comparison** against baselines.

  
**Condition** & **Visual** & **Temporal** \\ 
97.47\% & 72.13\% & 64.36\% \\   

Table 2: **User study**additional input further indicates the region that the model should generate thus also improves the overall generation performance.

## 5 Conclusion

We present ActAnywhere, a video diffusion-based model that generates videos with coherent and vivid foreground-background interactions, given an input foreground segmentation sequence and a condition frame describing the background. Our model synthesizes highly realistic details such as moving or interacting objects and shadows. The generated videos also exhibit consistent camera scales and lighting effects. We believe our work contributes a useful tool for the movie and visual effects community, as well as for the general public to realize novel ideas of situating an acting subject in diverse scenes, in a simple and efficient way that is not previously possible.

  
**Variant** & **CLIP\_cond**\(\) & **CLIP\_temp**\(\) & **FVD**\(\) \\  VAE Concat & 0.743 & 0.939 & 335.2 \\ VAE Cross-Attn & 0.823 & 0.942 & 326.6 \\ No Mask & 0.850 & 0.943 & 321.7 \\ Full Model (Ours) & **0.857** & **0.944** & **315.2** \\   

Table 3: **Ablation study** on key design choices.

Acknowledgment

We thank the authors of  for compiling and processing the dataset HiC+, especially Sumith Kulal for the code and instructions on accessing the data. We also thank Jiahui (Gabriel) Huang from Adobe Research for helping set up the Adobe Firefly GenFill API. Boxiao Pan and Leonidas J. Guibas are supported by a grant from the Stanford Human-Centered AI Institute (HAI) and a Vannevar Bush Faculty Fellowship.