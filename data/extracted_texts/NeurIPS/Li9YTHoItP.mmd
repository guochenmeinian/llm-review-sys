# Perception of Knowledge Boundary for

**Large Language Models through Semi-open-ended**

**Question Answering**

**Zhihua Wen\({}^{1}\)+, Zhiliang Tian\({}^{1}\)+, Zexin Jian\({}^{1}\), Zhen Huang\({}^{1}\), Pei Ke\({}^{2}\), Yifu Gao\({}^{1}\), Minlie Huang\({}^{2}\), Dongsheng Li\({}^{1}\)+

Footnote â€ : Work done during internship at the CoAI Group.

\({}^{1}\) College of Computer, National University of Defense Technology, Hunan, China

\({}^{2}\) Tsinghua University, Beijing, China

{zhwen, tianzhiliang, jianzexin21, gaoyifu, huangzhen, dsli}@nudt.edu.cn

kepei1106@outlook.com, aihuang@tsinghua.edu.cn

###### Abstract

Large Language Models (LLMs) are widely used for knowledge-seeking purposes yet suffer from hallucinations. The knowledge boundary of an LLM limits its factual understanding, beyond which it may begin to hallucinate. Investigating the perception of LLMs' _knowledge boundary_ is crucial for detecting hallucinations and LLMs' reliable generation. Current studies perceive LLMs' knowledge boundary on questions with concrete answers (close-ended questions) while paying limited attention to _semi-open-ended questions_ that correspond to many potential answers. Some researchers achieve it by judging whether the question is answerable or not. However, this paradigm is not so suitable for semi-open-ended questions, which are usually "partially answerable questions" containing both answerable answers and ambiguous (unanswerable) answers. Ambiguous answers are essential for knowledge-seeking, but they may go beyond the knowledge boundary of LLMs. In this paper, we perceive the LLMs' knowledge boundary with semi-open-ended questions by discovering more ambiguous answers. First, we apply an LLM-based approach to construct semi-open-ended questions and obtain answers from a target LLM. Unfortunately, the output probabilities of mainstream black-box LLMs are inaccessible to sample for low-probability ambiguous answers. Therefore, we apply an open-sourced auxiliary model to explore ambiguous answers for the target LLM. We calculate the nearest semantic representation for existing answers to estimate their probabilities, with which we reduce the generation probability of high-probability existing answers to achieve a more effective generation. Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the knowledge boundary of the target LLM. Following our method, we construct a dataset to perceive the knowledge boundary for GPT-4. We find that GPT-4 performs poorly on semi-open-ended questions and is often unaware of its knowledge boundary. Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering many ambiguous answers, including correct answers neglected by GPT-4 and delusive wrong answers GPT-4 struggles to identify.

Introduction

Large language models (LLMs) have revolutionized our interactions with AI, enabling users to acquire knowledge by posing questions in natural language . However, LLMs are prone to hallucination and generate non-factual responses, hindering the development of trustworthy AI.

One main cause of LLM hallucination is its unfamiliarity with the long-tail knowledge that appears less frequently than common-sense knowledge in the training data. To alleviate this issue, many researchers collect more domain-specific training data  or incorporate external information  via retrieval-augmented generation (RAG) during inference. Another line of work investigates the perception of knowledge boundaries for LLMs, which indicates the extent of knowledge that the LLM can grasp well, beyond which it may begin to hallucinate . Studying the perception of knowledge boundaries for LLMs helps alleviate hallucinations in many ways. For example, 1) It helps detect the hallucinations of a target LLM and the extent and scope of its factual knowledge . 2) It helps align LLMs for more honest generation .

Existing studies on the perception of knowledge boundaries are primarily in the form of Question-Answering (QA). Their methods mainly aim to judge whether a question is answerable or unanswerable and regard their border as the knowledge boundary. An answerable question refers to when the LLM is capable of generating a response matching the ground truth, and conversely, an unanswerable question means unable to answer correctly. These studies can be categorized into two groups. Prompt-based perception employs prompt engineering  to assess whether the LLM can answer the question via LLM self-evaluation. They question whether the LLM knows the answer  or needs external knowledge to answer the question . As LLMs tend to be overconfident , more researchers explore representation-based perception. These studies optimize different representations for answers with different answerability  or extract representations from a fixed encoder to train a classifier .

However, directly discriminating questions into answerable and unanswerable ones may not apply to some partially answerable questions. In many scenarios, the questions are relatively open-ended (i.e. having a list of correct answers) that may include (1) a subset of easy answerable answers, and (2) a subset of hard and unpopular answers, which may be unanswerable. These questions (referred to as "semi-open-ended questions") are particularly challenging and knowledge-extensive. Investigating the ambiguous answers to these semi-open-ended questions in various fields benefits knowledge-seeking. Ambiguous answers often go beyond the knowledge boundaries of LLMs and could lead to misinformation (see App. G). Therefore, we argue that investigating these questions with their ambiguous answers can augment the perception of the knowledge boundaries for LLMs.

In this paper, we propose to perceive the knowledge boundary for a target LLM with semi-open-ended questions by discovering pieces of unfamiliar knowledge where the LLM learns badly. Particularly, We first construct a dataset with semi-open-ended questions on the open domain and query the target LLM for their corresponding answers. We define the low-probability correct answers and delusive incorrect answers are the ambiguous answers corresponding to the LLM's unfamiliar knowledge.

A challenge is that obtaining LLMs' low-probability answers needs accessing LLMs' output probabilities (or violently sampling LLMs' outputs many times to approximate the probabilities), which is inaccessible (or expensive) for mainstream black-box LLMs, i.e. GPT-4. Therefore, we approximate the generation probabilities of the target LLM with an open-sourced auxiliary model. We use the Pseudo-inverse of model embedding to estimate the nearest semantic representation for the existing answers. Consequently, we obtain the probability distribution of existing answers and repetitively filter the existing answers (and their semantic-related counterparts) to obtain answers with low-probabilities. Finally, we recognize answers beyond the knowledge boundary of the target LLM by comparing its self-evaluation results against the ground truth answers obtained from RAG-based evaluation.

Empirically, we use our method to construct a dataset of approximately 1k samples and evaluate GPT-4's performance. We find that GPT-4 makes mistakes in 82.90% of questions and 40.15% of its ambiguous answers generated are unqualified. Besides, GPT-4 also makes inaccurate self-evaluation 28.77% of the time, indicating that these are beyond the knowledge boundary of GPT-4. Moreover,we find nearly 50% of the candidate answers discovered by our auxiliary model, LLMaMA-2-13B, are also beyond the knowledge boundary of GPT-4, including both factual answers that GPT-4 fails to produce and delusive wrong answers GPT-4 evaluates incorrectly.

Our contributions are as threefold: (1) We are the first to investigate the importance of semi-open-ended questions to the perception of knowledge boundaries for LLMs. (2) We propose an ambiguous answer discovery strategy that discovers many ambiguous answers with pieces of knowledge that are beyond the LLM's knowledge boundary. (3) Experimental results show the poor performance of an advanced LLM, GPT-4, on semi-open-ended questions and the effectiveness of our ambiguous answer discovery method in finding more pieces of knowledge which the LLMs are unfamiliar with.

## 2 Related Work

### Perception of Knowledge Boundaries for LLMs

Existing studies on the perception of knowledge boundaries for LLMs can be categorized into prompt-based perception and pattern-based perception. Prompt-based perception perceives the knowledge boundary by querying the target LLM. Many researchers instruct the LLM before and after response generation, asking whether it can correctly answer the questions [32; 36; 8; 46] and if the generated answers are accurate [32; 43]. In addition, Yin et al. (2024) seek the optimal prompt for benchmarking LLM knowledge boundaries. Amayuelas et al. (2023) study the LLMs' ability to understand their knowledge and measure their uncertainty. Kadavath et al. (2022) also instruct LLMs to generate their confidence score for their responses. As studies find that LLMs tend to be overconfident [29; 32; 17; 49], many researchers explore representation-based perception. Researchers identify unknown questions  or evaluate correct and incorrect answers  by implicitly learning their different representations. Chen et al. (2023) train LLMs to identify incorrect answers via parameter-efficient tuning. Besides, Wang et al. (2023) extract representations of answerable and unanswerable questions to train a classifier to predict whether a question is answerable and assume questions with similar representations share the same answerability. Si et al. (2023) take token probability as the answer's confidence score during generation. Zhao et al. (2023) detect unanswerable questions by paraphrasing questions and checking the divergence of their answer distribution. The above studies primarily perceive knowledge boundaries for LLMs by distinguishing between answerable and unanswerable questions. This type of binary division does not apply to questions with both common easy answers and unpopular hard answers. Our study is the first to investigate the perception of knowledge boundaries on semi-open-ended questions.

### Questions Answering for LLMs

Existing studies on Question Answering (QA) can be categorized into open-ended QA and close-ended QA based on the type of questions. Close-ended questions correspond to a limited number of correct answers, usually in the form of yes or no, true or false, or multiple-choice options, constraining the answers to a predetermined answer set [31; 15; 40; 33]. In addition, Researchers also study open-ended questions that allow the respondent to provide a more detailed and subjective response such as personal opinions and explanations [21; 40; 4; 3].

Researchers study the performance of LLMs on QA tasks mainly through various prompting strategies. Wei et al. (2023) explore "Chain-of-Thought" prompting (CoT), a simple and broadly applicable method for enhancing question answering ability of LLMs. Yao et al. (2023) and Besta et al. (2024) introduce similar frameworks for more complex QA tasks, namely, "Tree of Thought" and "Graph of Thoughts" prompting. As studies show that relying solely on an LLM's internal knowledge may lead to hallucinations , many researchers have also improved model performance in QA by incorporating external information (RAG systems  and knowledge graph [11; 1]). More recently, researchers have studied adaptive retrieval to avoid misinformation in the retrieved documents . Ni et al. (2024) estimate the answerability of the given question and determines whether to retrieve [10; 29]. Xu et al. (2024) learn to identify the knowledge boundaries of LLMs and refuse to answer certain questions to avoid risks [14; 42].

Perception of Knowledge Boundary for LLM via Semi-open-ended QA

### Overview

Our framework consists of three parts (see Fig. 1). We first exploit the instruction-following ability of a strong LLM to create a dataset consisting of semi-open-ended questions on various domains with LLM's answers. To discover more pieces of unfamiliar knowledge for the target LLM, we apply an open-sourced auxiliary model to incur more ambiguous answers by encouraging more distinctive generations. Finally, we evaluate whether the ambiguous answers to each question are beyond the knowledge boundary of the target LLM by comparing the self-evaluation results against RAG-based evaluation.

### Semi-open-ended QA Dataset

#### 3.2.1 Dataset Construction

To study the performance of semi-open-ended questions across various domains, we employ an LLM-based 2-step approach to obtain semi-open-ended questions and collect answers.

* **Domain selection.** We first prompt the LLM to generate a list of domains, encompassing world knowledge, which includes areas such as biology, geology, music, etc.
* **Question generation.** We prompt the LLM multiple times under each domain to generate a set of semi-open-ended questions \(Q\). To ensure the quality of the generation quality, we provide human-written sample questions as demonstrations and specify the following requirements for the generation of candidate questions: 1. The question should correspond to multiple correct answers, making it challenging to answer. The question should also be relatively easy for non-expert users to understand. 2. The judgment of question answers should be based on objective standards in the real world instead of the subjective standards of the evaluator. 3. The truthfulness of an answer to a question should not change constantly over time. 4. The questions share the same template: _Tell me a list of......_. We use the same vanilla prompt to eliminate the influence of different question styles.
* **Answer collection.** For each question \(q\) in \(Q\), we query the LLM \(I\) times and collect all responses \(=\{a_{0},a_{1},...,a_{I-1}\}\). In the \(i\)-th interaction, we inform the LLM of all previously generated responses \([:i]\) and obtain \(a_{i-1}\) by querying the LLM with question \(q^{}\), which repeats the same criteria specified in \(q\) and highlights the need for more answers.. Finally, we extract all answer entities in \(\) and construct an answer list \(A\).

#### 3.2.2 Dataset Descriptions

We create a dataset to investigate the performance of mainstream LLM, GPT-4, on semi-open-ended questions. In dataset construction, we set \(I\) to 3 to exploit the knowledge of GPT-4 through multi-round conversations. Like humans, when faced with such questions (e.g., _What are the animals unique to Australia?_), LLMs tend first to give answers in which they hold high confidence (like the _red angaroo_). The latter answers are less certain and may have more mistakes (like _echidna_). We define the initial 75% of answer entities from GPT-4 generations as common-sense answers, while the remaining 25% as ambiguous answers. Our dataset comprises 953 questions covering 32 domains, with well-distributed data within each domain. On average, GPT-4 yields 52 answers for each question, including an average of 13 ambiguous answers. See the data samples in App. A. See the full prompts and demonstrations in App. C.

### Ambiguous Knowledge Discovery

We apply an open-sourced auxiliary LLM to effectively discover more ambiguous answers that may be beyond the knowledge boundaries for black-box LLMs. Our intuition is that low-probability ambiguous answers reflect LLM's unfamiliarity with certain pieces of knowledge. However, it is challenging to collect low-probability ambiguous answers as 1) the generation probability of black-box LLMs (e.g. GPT-4) are inaccessible and their hyper-parameters (e.g. temperature) cannot target ambiguous answer-related tokens (see Sec. 4.2). 2) violently prompting the LLM with question \(q\) many times to approximate the generation probability is inefficient as it prioritizes high-probability answers that may already be present in existing answers. Answers that are semantically similar to high-probability answers also tend to have a high probability during generation. Hence, we propose to prevent the generation of high-probability answers with their semantic-related counterparts on an open-sourced auxiliary LLM to incur more low-probability ambiguous answers for the perception of the knowledge boundary for the black-box LLM.

We denote that the open-sourced auxiliary model's final layer is \(E^{|V| d}\), where \(d\) and \(V\) is the dimension size and \(V\) is the output vocabulary. In each generation step, LLM encodes the semantic representation of the input context as \(x^{d 1}\) and uses it to calculate the next token generation probability as \(y_{1}=Ex\). Specifically, we take the following 3 steps to estimate and decrease the probability of high-probability answers.

1. Probability initialization for existing answers. We choose the first token in an answer entity \(a A\) as "anchor token", which is suppose to represent the primary information about \(a\) since the first token is indispensable to its generation. For a given question \(q\), we define a vector \( y^{V 1}\), indicating the existence of unique anchor tokens in all answer entities to the question. In \( y\), we assign the value \(\) to the anchor tokens' position and assign 0 to other positions, where \(n\) is the number of all anchor tokens. As \(_{i} y_{i}=1\), we deem \( y\) as the initialized probability distribution of all existing answers for the given question.
2. Semantic estimation of high-probability answers. We estimate the semantic representation \( x\) for high-probability answer entities from initialized existing answer probability \( y\). We calculate \( x=E^{+} y\), where \(E^{+}E=I\). Here, \(E^{+}\) is the left Moore-Penrose pseudo-inverse, and \(I\) is the identity matrix. Because the pseudo-inverse of a non-square matrix is often used to find the least squares solution of equations, In this way, we obtain \( x\) as the least squares semantic representation that is the nearest to approximate the real semantic representation (which is unknown) of anchor tokens.
3. Probability Reduction. We calculate the probability of high-probability answers as \( y=E x\) and ultimately obtain an adjusted generation probability \(y_{2}\): \[y_{2}=y_{1}- y=y_{1}- E x=y_{1}- EE^{+}_{l}  y,\]

Figure 1: The overview of our framework. For the question \(q\) in the constructed dataset, The open-sourced auxiliary model prevent the high-probability answers based on existing answer entities \(A\) from the black-box LLM (i.e. GPT-4) and generate 4 categories of ambiguous knowledge that are unfamiliar knowledge for the target model.

where \(\) is a scaler that controls the extension of the reduction. Since anchor tokens and their semantic tokens almost share the same semantics, interpreting the estimated semantic representation of anchor tokens propagates the semantic information to their semantic-related ones. Thereby, we obtain an estimated probability distribution of high-probability tokens \( y\) and use it to prevent the generation of high-probability answers.

During generation, the auxiliary LLM samples words on the adjusted probability distribution \(y_{2}\) instead of the original distribution \(y_{1}\). In this way, we only reduce the probability of answers that are semantically related to existing high-probability answers. while preserving the remaining probability distribution almost intact. The black-box model has already exhausted almost all high-probability common answers in \(A\), forcing the auxiliary model to generate new ambiguous answers with low probabilities. These ambiguous answers are either low-probability correct answers neglected by the black-box model or delusive incorrect answers.

### Ambiguous Knowledge Verification

We compare the results of LLM self-evaluation against the ground truth from RAG-based evaluation to verify the truthfulness of ambiguous answers, thereby identifying answers beyond the knowledge boundary for the target LLM. We conduct self-evaluation on the target LLM with well-designed instructions. We craft a prompt template including 1) an incentive statement that encourages better performance: _I'll pay you 5100 for a factually correct answer_; 2) an instruction _think step by step_ which prompts the LLM to analyze before reaching to a conclusion [37; 24]. 3) multiple human-crafted examples as in-context learning demonstrations.

We believe an answer \(a\) in each test case \((q,)\) to be factually correct to \(q\) if verified on public, trustworthy sources. As questions in our dataset correspond to numerous low-probability hard answers, it is cost-prohibitive to annotate the truthfulness of each answer with expert knowledge. Inspired by Web-GPT , we adopt a cost-efficient approach to mimic human behavior when faced with unfamiliar knowledge. We instruct an Internet-connected LLM with the same prompt for self-evaluation and require it to search online for related information before making judgments.

For both evaluations, we evaluate each answer \(a\) to its corresponding question as 1) _incorrect_, which contradicts reliable sources; 2) _correct_, which is supported by reliable sources, and 3) _unverifiable_, which is for cases that cannot be verified based on available information. Finally, for each answer \(a\), we compare the differences between LLM self-evaluation and RAG-based evaluation to categorize different types of ambiguous knowledge.

### Ambiguous Knowledge Categorization

We classify the ambiguous answers into four categories based on the above two evaluation results and posit that they are beyond the knowledge boundary of the target LLM. For a question, we categorize the following types of answers to be the LLM's ambiguous answers:

* Unqualified answers: answers from the target LLM's response that are identified as incorrect or unverifiable according to the ground truth.
* Inaccurate evaluations: answers from the target LLM's response whose self-evaluation results contradict their ground truth.
* Hidden correct answers: answers that are neglected by the target LLM, yet supplemented by the auxiliary model, which are correct according to ground truth.
* Unexpected wrong evaluations: answers that are neglected by the target LLM but generated by the auxiliary model whose self-evaluation results misalign with their ground truth.

The above categorization helps us understand different types of misunderstanding of the target LLM regarding specific pieces of unfamiliar knowledge.

Experiments

### Settings

In our experiment, we investigate the knowledge boundary of GPT-4 on our constructed dataset. We use two LLaMA-2-13b  models, as our auxiliary models in Sec. 3.3. Our method sets \(\) in Sec. 3.3 to 80. See more implementation details in App. B.

We compare our method with several baselines. Following prompt-based approaches ,_Prompt_ instructs the auxiliary model to generate more answers via prompt-engineering. Inspired by Zhang et al. (2023), _MASK_ belongs to the representation-based perception that uses an average initialization for the probability of tokens from existing answers to represent the likelihood of high-probability answers and reduce their generation probabilities in the auxiliary model.

For the evaluation metrics, we use widely adopted Exact Match  (EM) and F1 scores [32; 36] to measure the performance of in discovering ambiguous answers. Different from previous research, our semi-open-ended questions correspond to a large number of correct answers, making it hard to build a comprehensive answer set for evaluation. Instead, we select ambiguous answers identified by the RAG-evaluation and confirmed by our human annotators as their ground truth and compare them with the full response to calculate the **EM** and **F1**. In this way, EM is an entity-level metric that measures the percentage of ambiguous answers within the response. F1 is a word-level metric that quantifies the word overlap between the ambiguous answers and the ground truth. We adopt Bleu  to measure word-level overlap between responses from the GPT-4 response and the auxiliary model. We also use answer overlap rate (AOR) to evaluate the efficiency of generating distinctive ambiguous answers. AOR is an entity-level metric that calculates the proportion of words in a list of answer entities that duplicate the reference response. See more evaluation details in App. B.

### Overall Performance

We analyze the effectiveness of our auxiliary model in exploring the knowledge boundary of GPT-4 by comparing it with multiple baselines. We randomly sample 200 questions from our dataset and discover ambiguous answers with an auxiliary model. Then, we verify the truthfulness of these answers using RAG-based evaluation with human annotation following Sec 3.4 and measure their performance with our evaluation metrics. Tab 1 shows the results of ambiguous answers on different evaluation metrics when using different auxiliary models and strategies. _Prompt_ directly prompts the auxiliary model to generate answers, achieving the worst performance on all metrics. This suggests that directly prompting the auxiliary model may generate many repetitive answers (results in high AOR and Bleu scores) and, therefore inefficient in discovering new ambiguous answers (results in low EM and F1). _MASK_ reduces the generation probabilities of anchor tokens during generation. When employing _MASK_ on the same auxiliary model, its EM and F1 increase to 0.47 and 0.587 respectively while achieving a lower AOR (0.344). It indicates that reducing the probability of the generation of anchor words effectively achieves a more diverse generation. Replacing the auxiliary model with LLaMA-7B results in a slightly lower EM of 0.458. This marginal decrease implies that while a larger model can offer a broader knowledge base, reducing anchor word probabilities is more influential in generating distinctive answers. Our strategy estimates and reduces the generation probability of near-duplicate answers. This approach achieves the best performance on all metrics. It

  
**Method** & **Auxiliary Model** & **EM**\(\) & **F1**\(\) & **AOR**\(\) & **Bleu1\(\)** & **Bleu2\(\)** & **Bleu3\(\)** & **Bleu4\(\)** \\  Prompt & LLaMA-2-13B & 0.300 & 0.461 & 0.490 & 0.252 & 0.118 & 0.052 & 0.023 \\   & LLaMA-2-7B & 0.458 & 0.570 & 0.342 & 0.185 & 0.075 & 0.021 & 0.004 \\   & LLaMA-2-13B & 0.470 & 0.587 & 0.344 & 0.189 & 0.075 & 0.021 & 0.004 \\  Ours & LLaMA-2-13B & **0.481** & **0.587** & **0.326** & **0.181** & **0.071** & **0.018** & **0.004** \\   

Table 1: The performance of different auxiliary models with various strategies in discovering more ambiguous answers.

underscores the effectiveness of our strategy in generating a diverse set of ambiguous answers that are less likely to duplicate existing ones, thus exploring the knowledge boundary for the target LLM. See our case study in App.G.

### Ablation Study

We conduct an ablation study on our proposed method to verify the importance of each component in eliciting more distinctive ambiguous answers (as shown in Tab 2). \(-\)_Auxiliary Model_ abandons the auxiliary model and use existing answers as in-context learning examples to prompt GPT-4 for more answers. It achieves the highest on all metrics, indicating that prompting the black-box model violently for more answers is inefficient as it results in many repetitive answers. We also try to encourage a more diverse generation by increasing the generation temperature of GPT-4. However, we find that GPT-4 starts generating scrambled texts after just a few words and the perplexity of these texts exceeds 1000, while normally GPT-4 generates a low perplexity of around 10. This indicates that increasing the sampling temperature results in the generation of scrambled texts. Although adjusting the generation temperature can change the generation probabilities, it does not alter the original probability relationships, nor can it specifically target tokens related to ambiguous answers. \(-\)_Inverse Matrix_ only reduces the probability of existing answers without considering their near-duplicate answers. It performs better than \(-\)_Auxiliary Model_ while underperforms Ours on all metrics. It shows that estimating and reducing the probability of near-duplicate tokens augment the auxiliary model for higher generation diversity. We increase the intervention on the generation probability by lowering \(\), the probability influence scaler. From row 3 to row 5 in Tab. 2, \(\) increases from 60 to 80, resulting in a decrease on all metrics. This suggests that the extent to which we intervene in the generation probability is positively correlated with the diversity of ambiguous answers produced by the auxiliary model.

### Results of Perceiving the Knowledge Boundary for GPT-4

By analyzing the ambiguous answers in our dataset, we arrive at the following findings: (1) **GPT-4 performs poorly on the semi-open-ended questions and generates many unqualified answers.**

  
**Variants** & **AOR\(\)** & **Bleu1\(\)** & **Bleu2\(\)** & **Bleu3\(\)** & **Bleu4\(\)** \\  \(-\)_Auxiliary Model_ & 0.535 & 0.267 & 0.106 & 0.037 & 0.010 \\ \(-\)_Inverse Matrix_ & 0.344 & 0.189 & 0.075 & 0.021 & 0.004 \\ _Ours(\(\)=60)_ & 0.419 & 0.224 & 0.098 & 0.038 & 0.013 \\ _Ours(\(\)=70)_ & 0.352 & 0.192 & 0.076 & 0.021 & 0.005 \\  _Ours_ & **0.326** & **0.181** & **0.071** & **0.018** & **0.004** \\   

Table 2: Ablation study on the key components in our method. We use the same metrics as in Sec. 4.2, apart from those that require manual annotation. AOR and Bleu measure the entity- and word-level overlap respectively between answers from GPT-4 and different model variants. \(-\) Auxiliary Model prompt GPT-4 with existing answers as examples for more ambiguous answers. \(-\) Inverse Matrix keeps the near-duplicate tokens during generation. Besides, we adjust the probability influence scaler (\(\)) to verify its impact on the generation results.

  
**Ground Truth\(\)** & **Incorrect** & **Correct** & **Unverifiable** \\ 
**Incorrect** & 20.98 & 8.37 & 2.25 \\ 
**Correct** & 9.30 & 47.77 & 2.78 \\ 
**Unverifiable** & 3.30 & 3.73 & 1.52 \\   

Table 3: Percentages of different categories of answers comparing the LLM self-evaluation and ground truth labels. Following the categorization in Sec. 3.5, we calculate that the percentage of unqualified answers is 40.15% by adding up the underlined results that are incorrect or unverifiable according to the ground truth. We also obtain the percentage of inaccurate evaluations as 28.47%, by adding up the results highlighted in red where self-evaluation is inconsistent with the ground truth.

We calculate the percentage of questions where at least one of the GPT-4's answers is unverifiable or incorrect according to the ground truth. We find that GPT-4 generates incorrect or unverifiable answers in 82.90% of questions. By adding up the proportion of incorrect answers (row 1 in Tab.3) and unverifiable answers (row 3 in Tab.3), we identify that 40.15% of ambiguous answers belong to unqualified answers. (2) **GPT-4 makes many inaccurate evaluations regarding the truthfulness of ambiguous answers, indicating that the LLM lacks understanding of the relevant knowledge.** We identify answers whose ground truth misaligns with self-evaluations in Tab.3 and find that 28.47% ( by adding up the results in red color from Tab.3) of ambiguous answers belong to inaccurate evaluations for GPT-4. It indicates that GPT-4 is unfamiliar with these pieces of knowledge, and retrieval is helpful for LLM to draw the correct conclusion. (3) **GPT-4 has limited ability to recognize its knowledge boundary, while in most cases it continues to produce unqualified answers.** We search for keywords in all responses that reflect GPT-4's admission of its knowledge boundary (e.g. _I apologize_ and _I'm afraid_) and calculate the proportion of corresponding questions in the dataset. We find that in about 7% of questions, GPT-4 admits that it has listed all the answers and refuses to provide more answers (it generates a response like _I apologize for any confusion, but to the best of my knowledge, the list I provided includes all the correct answers._). However, it fails to recognize its knowledge boundary in the rest questions and continues to generate unqualified answers. Our findings indicate that advanced LLM (i.e. GPT-4) is easy to hallucinate on semi-open-ended questions, indicating the importance of detecting the LLM knowledge boundary via these questions.

### Results of the Auxiliary Model on Perceiving the Knowledge Boundary for GPT4

Tab.4 shows the fine-grained results of GPT-4 self-evaluation and the ground truth of ambiguous answers discovered by our auxiliary model. By analyzing the results, we conclude some interesting findings: (1) **LLaMA-2-13B effectively supplements GPT-4 by identifying hidden correct answers.** We add up the starred results in Tab.4 and obtain the proportion of LLM neglected hidden correct answers (75.12%). Notably, 23.97% and 13.61% of correct ambiguous answers are both neglected by GPT-4 and deemed to be incorrect or unverifiable under GPT-4 self-evaluation, showcasing the GPT-4's unfamiliarity with the corresponding knowledge. (2) **LLaMA-2-13B is easy to incur unexpected wrong evaluations during GPT-4 self-evaluation.** We add up the results highlighted in orange from Tab.4 and find that 62.43% of the GPT-4 self-evaluations are inconsistent with the ground truth. It implies that GPT-4's self-evaluation mechanism may not be fully aligned with the actual correctness of the ambiguous answers, especially when they are supplemented by the auxiliary model. (3) **LLaMA-2-13B is also able to discover situations where GPT-4 admits for its knowledge boundary.** We add up the results in column 3 where GPT-4 admits that it cannot make a judgment (unanswerable) during self-evaluation and obtain 24.92% of aligned ambiguous answers. It means that GPT-4 aligns well with these ambiguous answers because it neglected these answers during generation and deems them as unknown knowledge during self-evaluation.

### Practical Implications

Perceiving LLMs' knowledge boundaries is important to understand and alleviate hallucination [18; 49]. Ambiguous answers for semi-open-ended questions are highly likely beyond the knowledge boundaries of LLMs (see Sec 4.4). Discovering ambiguous answers benefits many applications,

  
**Ground Truth** & & & \\
**GPT-4-evaluation** & **Incorrect** & **Correct** & **Unverifiable** \\ 
**Incorrect** & 0.04 & 9.53 & 11.31 \\ 
**Correct** & \(23.97^{*}\) & \(37.54^{*}\) & \(13.61^{*}\) \\ 
**Unverifiable** & 3.18 & 0.83 & 0.00 \\   

Table 4: Percentages of different categories of ambiguous answers comparing the GPT-4 self-evaluation results and their ground truth. Following the categorization in Sec.3.5, we calculate that the percentage of hidden correct answers is 75.12% by adding up the starred (*) results that are correct according to the ground truth. We also obtain the percentage of unexpected wrong evaluations as 62.43%, by adding up the results highlighted in orange where GPT-4-evaluation is inconsistent with the ground truth.

including: 1) It helps detect the knowledge scope of LLMs more faithfully. While many close-ended hallucination evaluation benchmarks face the danger of data contamination [25; 13], semi-open-ended questions are easy to design and correspond to a large number of undocumented answers; 2) Flagging ambiguous answers with higher uncertainty enhances the LLM outputs [22; 19]; 3) Identifying ambiguous answers helps achieve selective retrieval that augments LLM with indispensable external information while reducing the distraction of irrelevant data [36; 26; 39]; 4) It helps align LLMs for a more honest generation by teaching the LLM to admit its knowledge limit on the knowledge it is unfamiliar with (ambiguous answers) [43; 8; 42].

## 5 Conclusion

We investigate the perception of knowledge boundary for LLMs with semi-open-ended questions, an important yet underexplored type of question corresponding to a large number of accurate answers. We introduce an LLM-based approach to construct semi-open-ended questions and collect LLM answers from the target LLM. Then, we discover more pieces of unfamiliar knowledge for the target LLM by eliciting ambiguous answers from an auxiliary model that the LLM neglects. To achieve a more effective generation, we estimate and reduce the generation probability of existing answers with their near-duplicate counterparts. With our methods, we construct a dataset to evaluate the performance of GPT-4 and discover many ambiguous answers with our auxiliary model, LLaMA-2-13B. Our findings reveal that GPT-4 produces many unqualified answers and suffers from inaccurate evaluations. Besides, we verify that LLaMA-2-13B is effective in discovering more unpopular correct answers and delusive wrong answers neglected by GPT-4. Our findings underscore the importance of semi-ended questions and the effectiveness of our method in assisting in perceiving knowledge boundaries for LLMs.

## 6 Acknowledgement

This work is supported by the following findings: Young Elite Scientist Sponsorship Program by CAST (2023QNRC001) under Grant No. YESS20230367, the National Natural Science Foundation of China under Grant No. 62306330, No. 62106275, No. 62025208, No. 62421002, and the Grant of No. WDZC20235250103.