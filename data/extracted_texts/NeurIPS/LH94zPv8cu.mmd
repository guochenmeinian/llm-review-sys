# Warped Diffusion: Solving Video Inverse Problems

with Image Diffusion Models

 Giannis Daras

UT Austin

&Weili Nie

NVIDIA

&Karsten Kreis

NVIDIA

&Alexandros G. Dimakis

UT Austin

Morteza Mardani

NVIDIA

&Nikola B. Kovachki

NVIDIA

&Arash Vahdat

NVIDIA

###### Abstract

Using image models naively for solving inverse video problems often suffers from flickering, texture-sticking, and temporal inconsistency in generated videos. To tackle these problems, in this paper, we view frames as continuous functions in the 2D space, and videos as a sequence of continuous warping transformations between different frames. This perspective allows us to train function space diffusion models only on _images_ and utilize them to solve temporally correlated inverse problems. The function space diffusion models need to be equivariant with respect to the underlying spatial transformations. To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions. Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems. We demonstrate the effectiveness of our method for video inpainting and \(8\) video super-resolution, outperforming existing techniques based on noise transformations. We provide generated video results in the following URL: https://giannisdaras.github.io/warped_diffusion.github.io/.

## 1 Introduction

Diffusion models (DMs)  can synthesize photorealistic imagery . They can be conditioned easily, through explicit training or guidance , and have also been widely used to solve inverse problems , in particular for image processing applications like inpainting and super-resolution .

How do these methods extend to video processing and solving inverse problems on videos? Although video DMs are seeing rapid progress , general text-to-video synthesis has not yet reached the level of robustness and expressivity comparable to modern image models. Moreover, no state-of-the-art video generative models are publicly available , and most video DMs are computationally expensive. To circumvent these challenges, a natural research direction is to leverage existing, powerful _image_ generative models to solve _video_ inverse problems.

Figure 1: Inpainting results for “a robot sitting on a bench”. As the input video shifts smoothly, our output frames stay consistent.

Naively applying image DMs to videos in a frame-wise manner violates temporal consistency. Previous works alleviate the problem by fine-tuning on video data or by warping the networks' features, using, for instance, temporal or cross-frame attention layers [88; 54; 13; 51; 61; 90; 92; 34; 33]. However, these methods are usually designed specifically for high-level text-driven editing or stylization and are typically not directly applicable to general inverse problems. Moreover, without training on diverse video data they often cannot maintain high frequency information across frames. For a detailed discussion of the related works, we refer the reader to Section E in the Appendix.

The recent novel work, "How I Warped Your Noise" , proposes _noise warping_ to achieve temporal consistency in generated videos by changing appropriately the input noise to the diffusion model. Videos can be thought of as image frames subject to spatial transformations. An object may move according to a translation; complex and general transformations can be described by motion vectors on the pixels defined through optical flow . It is these transformations that define how the noise maps need to be warped and transformed. In , temporally consistent noise maps are given as input to the DM's denoiser, with the underlying assumption that temporally consistent inputs induce temporally consistent network outputs. In this paper, we argue that this assumption only holds true if the utilized image DM is _equivariant_ with respect to the spatial warping transformations. However, as we show in this work, the network is not necessarily equivariant because i) the conditional expectation modeled by the DM may not be equivariant, and, ii) more importantly, a free-form neural network, as used in typical DMs, will not learn a perfectly equivariant function. When the equivariance assumption is violated, the method proposed in  achieves poor results. This is typically the case for challenging conditional tasks (see Figure 1) or when modeling complex distributions. Particularly,  finds that the proposed method has "limited impact on temporal coherency" when applied to _latent_ diffusion models and that "all the noise schemes produce temporally inconsistent results".

We introduce a new framework, dubbed _Warped Diffusion_, for the rigorous application of image DMs to video inverse problems. We employ a continuous function space perspective to DMs [52; 59; 28; 35] that naturally allows noise warping for arbitrarily complex spatial transformations. Our method generalizes the warping scheme of  and does not require any auxiliary high-resolution noise maps. To achieve equivariance, we propose _equivariance self-guidance_, a novel sampling mechanism that enforces that the generated frames are consistent under the warping transformation. Our inference time approach elegantly circumvents the need for additional training. This unlocks the use of existing large DMs in a fully equivariant manner without further training, which may be prohibitive for a practitioner.

We extensively validate our method on video inpainting and super-resolution. Super-resolution represents a situation with strong conditioning, while inpainting requires large-scale, temporally coherent synthesis of new content. Warped Diffusion outperforms previous methods quantitatively and qualitatively, and shows reduced flickering and texture sticking artifacts. Due to our equivariance guidance, our method can also be used with _latent_ DMs, which is not possible with previous approaches. Virtually all existing state-of-the-art text-to-image generation systems are indeed latent DMs, like Stable Diffusion . Hence, any inverse problem solving method must be readily usable with latent DMs. In fact, all our experiments utilize the state-of-the-art text-to-image latent DM SDXL .

**Contributions:**_(a)_ We propose Warped Diffusion, a novel framework for applying image DMs to video inverse problems. _(b)_ We introduce a principled scheme for noise warping, based on Gaussian processes and a function space DM perspective. _(c)_ We identify the equivariance of the DM as a critical requirement for the seamless application of image DMs to video inverse problems and propose an inference-time guidance method to enforce it. _(d)_ We comprehensively test Warped Diffusion and achieve state-of-the-art video processing performance when considering the use of image DMs. Critically, Warped Diffusion can be used with any image DMs, including large-scale latent DMs.

## 2 Functional Video Generation

The basis of our approach, summarized in Figure 2, is to structure the generative model so that it is equivariant with respect to spatial deformations and apply these deformations successively to the input noise. Each deformation effectively warps the noise and the equivariance guarantees that each output image will be similarly warped. By using an optical flow from a real video to define a sequence of such deformations, a new video can be generated. To introduce our method, we first conceptualize both images and noise as functions on a domain and the generator as a mapping between two function spaces.

[MISSING_PAGE_FAIL:3]

\(f_{1}=f_{0} T_{1}^{-1}\) on \(D D_{1}\). If \(D D_{1}\), it might seem that our generative model is unnecessary. However, proceeding this way generates blurry and unrealistic videos.

The primary issue is that, in practice, we don't have access to \(f_{0}\) at an infinite resolution but only at a fixed, finite set of grid points \(E_{k}=\{x_{1},,x_{k}\} D\). To determine \(f_{1}\) on our grid points, we need the values of \(f_{0}\) at the points \(T_{1}^{-1}(E_{k})=\{T_{1}^{-1}(x_{1}),,T_{1}^{-1}(x_{k})\}\). It's highly unlikely that \(E_{k}=T_{1}^{-1}(E_{k})\) for any realistic deformation.

Thus, we must interpolate \(f_{0}\) to \(T_{1}^{-1}(E_{k})\), which usually leads to blurry results with standard methods. Furthermore, if \(D D_{1}\), there will be regions where \(f_{1}\) is not determined by \(f_{0}\) and will need to be inpainted on the new visible domain. Therefore, for each frame, we must solve an interpolation and an inpainting problem: tasks for which generative models are well-suited.

Suppose we have access to the noise function \(_{0}\) at infinite resolution, and its domain extends to all of \(^{2}\); we discuss both in Section 3.1. We can then define the new frame in our video by applying the generative model to the deformed noise: \(f_{1}=G(_{0} T_{1}^{-1})\). The deformed noise function \(_{0} T_{1}^{-1}\) gets its values from \(_{0}|_{D}\) for points in \(D D_{1}\) and from the extension of \(_{0}\) to \(^{2}\) for all other points where inpainting is needed. To ensure this definition is consistent with (1), \(G\) must be equivariant with respect to \(T_{1}^{-1}\). Specifically, for all \(() H\), we must have

\[G T_{1}^{-1}(x)=GT_{1}^{-1}(x) ,\;x D D_{1}.\] (2)

Assuming (2), it follows from \(G(_{0})=f_{0}\), that \(f_{1}(x)=G(_{1}|_{D})(x)=(f_{0} T_{1}^{-1})(x)\) for all \(x D D_{1}\) hence the pair \((f_{0},f_{1})\) is a valid 2 frame video according to the definition of Section 2.1. To generate a video with any number of frames, we simply iterate on this process with a given sequence of deformation maps. Enforcing (2) can be done directly by the architectural design, through training with various deformation maps, or, through a guidance process; see Section 3.2.

### White Noise

It is common practice to train generative models assuming the reference measure \(\) is Gaussian white noise. Specifically, a draw \(\) on the grid points \(E_{k}=\{x_{1},,x_{k}\} D\) is realized as \((x_{l})=_{l}\) for an i.i.d. sequence \(_{l}(0,1)\) for \(l=1,,k\). However, this approach is incompatible with our goal of having the generative model perform interpolation. For most deformations \(T\) encountered in practice, none of the points in \(T^{-1}(E_{k})\) will match those in \(E_{k}\). Consequently, each new evaluation \(T^{-1}(x_{l})\) will be independent of the sequence \(\{_{l}\}_{l=1}^{k}\), making \(T^{-1}(E_{k})\) appear as a new noise realization unrelated to \((E_{k})\). This incompatibility arises because white noise processes are distributions, not regular functions, meaning realizations are almost surely not members of \(H\).  proposes a stochastic interpolation method to address this issue (see Appendix C for details and comparison). We generalize this idea and propose using generic Gaussian processes on \(H\).

## 3 Method: Warped Diffusion

In Section 1, we formulated the problem of video generation as the computation of a series of functions warped by an optical flow and proposed the use of a generative model for inpainting and interpolating the warped functions. The main challenges which remain are defining a functional noise process which can be evaluated continuously and a generative model which is equivariant with respect to warping. We propose to use Gaussian processes for our functional noise and a guidance procedure within the sampling step of a diffusion model to overcome these challenges.

### Gaussian Processes (GPs)

A Gaussian Process (GP) \(\) is a probability measure on \(H\) completely specified by its mean element and covariance operator. For a mathematical introduction, see Appendix B. We identify Gaussian processes with positive-definite kernel functions \(:^{2}^{2}\). Recall that \(E_{k}=\{x_{1},,x_{k}\}\) denotes the grid points where we know the values of an image \(f H\). To realize a random function \(\) on these points, we sample the finite-dimensional multivariate Gaussian \(N(0,Q)\), where \(Q^{k k}\) is the kernel matrix \(Q_{ij}=(x_{i},x_{j})\) for \(i,j=1,,k\).

Once sampled, given the fixed values \((E_{k})\), \(\) can be evaluated at any new point \(x^{*} D\) by computing the conditional distribution \((x^{*})(E_{k})\). This approach allows us to realize random functional samples at infinite resolution through conditioning, thus resolving the interpolation problem. Furthermore, by ensuring the kernel \(\) is positive definite on a domain larger than \(D\), we can consistently sample \(\) outside of \(D\), addressing the inpainting problem described in Section 2.2.

For high-resolution images when \(k\) is large, working with the matrix \(Q\) can be computationally expensive. Instead, we propose using **R**andom **F**ourier **F**eatures (RFF) to sample \(\), which amounts to a finite-dimensional projection of the function \(\) that converges in the limit of infinite features [63; 87]. We can approximate samples from a GP with a squared exponential kernel with length-scale parameter \(>0\) by \((x)=}_{j=1}^{J}w_{j}(z_{j},x)+b_{j}\) for i.i.d. sequences \(w_{j} N(0,1)\), \(z_{j} N(0,^{-2}I_{2})\), \(b_{j} U(0,2)\) where \(J\) is the number of features. RFF allows us access to \(\) at infinite resolution on the entirety of the plane while also allowing for efficient computation.

### Function Space Diffusion Models and Equivariance Self-Guidance

We will now focus on the generative model that needs to be equivariant to the noise transformations. Specifically, in this section, i) we introduce function space diffusion models, ii) we prove that if every prediction of the diffusion model is equivariant then the whole diffusion model sampling chain is equivariant to the underlying spatial transformations, and, iii) we describe _equivariance self-guidance_, our sampling technique for enforcing the equivariance assumption.

For ease of notation, we will present everything for the case of unconditional video generation. However, our method seamlessly incorporates any addition conditioning information that may be available. If \(c_{0},,c_{n}^{c}\) is a sequence of known conditioning vectors then these can simply be passed into a conditional score model at the appropriate frame without any other change to our method; see Algorithm 1. Conditioning vectors could be, for example, low resolutions versions of a video or an original video with regions masked. In Section 4, we focus on such conditional tasks.

**Function Space Diffusion Models.** Typically, diffusion models are trained with white noise. As explained in Section 2.3, a principled continuous evaluation of the noise requires a functional process. We briefly describe diffusion models in the context of sampling using the Gaussian processes of Section 3.1. We show in Section 4.1 (Table 1) that a model trained with white noise can be fine-tuned to GP noise without any loss in performance.

While it is possible to formulate diffusion models on the infinite-dimensional space \(H\) e.g. , we will proceed in the finite-dimensional case for ease of exposition. In particular, we will define the forward and backward process as a flow on a vector \(u^{k}\), thinking of the entries as the values of a scalar function evaluated on the grid \(E_{k}\) and recall that \(Q\) is the kernel matrix on \(E_{k}\).

We consider forward processes of the form,

\[u_{t}=2(t)(t)Q^{1/2}W_{t},  u(0)=u_{0}\] (3)

where \(W_{t}\) is a standard Wiener process on \(^{k}\) and \(\) is a scalar-valued, once differentiable function. This process results in conditional distributions \(p(u_{t}|u_{0})=N(u_{0},^{2}(t)Q)\), see . Let \(p(u_{t},t)\) denote the density of \(u_{t}\) induced by (3). Then the following backward in time ODE,

\[u_{t}}{t}=-(t)(t)Q_{u} p(u_{ t},t)\] (4)

started at \(u()\) distributed according to (3) has the same marginal distributions \(p(u_{t},t)\) as (3) on the interval \([0,]\); see . Approximating \(N(u_{0},^{2}()Q)\) by \(N(0,^{2}()Q)\), we may then define the generative model \(G\) by the mapping \(u() u(0)\) with reference measure \(=N(0,^{2}()Q)\).

Solving (4) requires knowledge of the score \(_{u} p(u_{t},t)\). Instead of learning the score, we opt for directly learning the weighted score \(Q_{u} p(u_{t},t)\). This design choice leads to faster sampling since we do not need to perform any expensive matrix multiplication with \(Q\) at inference time.

A generalized version of Tweedie's formula (for proof see Appendix A.2) implies:

\[Q_{u} p(u_{t},t)=[u_{0}|u_{t}]-u_{t}}{^{2}(t)}.\] (5)

We approximate \([u_{0}|u_{t}]\) with a neural network \(h_{}\) by minimizing the denoising objective:

\[_{t U(0,)}_{u_{0}}_{u_{t} N(u _{0},^{2}(t)Q)}|h_{}(u_{t},t)-u_{0}|^{2}.\] (6)Having a minimizer \(h_{}\) of (6) gives us access to the weighted score \(Q_{u} p(u_{t},t)\) via (5). We may then obtain an approximate solution to the map \(u() u(0)\) by discretizing (4) in time. We consider Euler scheme updates given by

\[u_{t- t}=u_{t}- t(t)}{(t)}h_{} (u_{t},t)-u_{t}.\] (7)

started with \(u_{} N(0,^{2}()Q)\) for some time step \( t>0\).

**Equivariance for the Probability Flow ODE.** Since the diffusion model works with discrete inputs, we need to introduce a discretization of (2) for the network. For a deformation \(T_{1}\), we define equivariance as

\[h_{}(u_{t} T_{1}^{-1},t) T_{1}=h_{}(u_{t},t),\] (8)

which is obtained from composing both sides of (2) with \(T_{1}\). Note that (8) is valid only for pixels which stay within frame and we compute the l.h.s. with bilinear interpolation on the network output. The input to the network on the l.h.s. is computed with RFFs without any interpolation. Given this discrete equivariance is satisfied for every prediction of the network, it is straightforward to show that the whole diffusion model sampling chain will be equivariant. Indeed, the whole approximation to \(u() u(0)\) is equivariant by the linearity of composition - for a full derivation, see Appendix A.3.

**Equivariance Self-Guidance.** The condition (8) is rarely satisfied for deformations \(T_{1}\) arising in practical settings. This is because either the conditional expectation \([u_{0}|u_{t}]\) is not equivariant with respect to \(T_{1}^{-1}\) or the neural network approximation has not fully captured it. If the underlying equivariance assumption breaks, methods that rely solely on noise warping for temporal consistency, e.g. , will perform poorly. This is evident in challenging conditional tasks (see Figure 1).

A potential solution is to directly train the network by adding (8) as a regularizer. However, this requires large amounts of video data from which to extract optical flows. Furthermore, by satisfying (8) over a large class of \(T_{1}\)(s), the network may become less apt at satisfying (6) and lose its generative abilities. Therefore, we opt for _guiding the model towards equivariant solutions at inference time_.

We first sample noise \(u_{}^{(0)}\) and generate the first frame following (7), keeping the outputs of the network at each time step \(\{h_{}(u_{t}^{(0)},t)\}\). To generative the next frame, we warp our noise \(u_{}^{(1)}=u_{}^{(0)} T_{1}^{-1}\) with RFFs and again follow (7) but this time using (8) as guidance. In particular, we take a gradient steps in the direction of the loss function \(|h_{}(u_{t}^{(1)},t) T_{1}-h_{}(u_{t}^{(0)},t)|^{2}\), computed on the pixels that stay within frame. All frames can be generated by iterating this procedure as summarized in Algorithm 1 (and visualized in Figures 2, 9) which also shows how to use conditioning information. Guidance is typically used to solve inverse problems with diffusion models (e.g. see ), but here the guidance is applied to align the model with its own past predictions. We emphasize that to compute the composition with \(T_{1}\) above, we use bilinear interpolation on the network outputs but we never need to interpolate the network inputs since we can compute the warping via RFFs. Furthermore, since we are matching interpolated outputs to ones that are not interpolated, our output images remain sharp. This in contrast to directly using a discrete version of (2) which would suggest that we match network outputs to interpolated images, producing blurry results.

## 4 Experimental Results

For all our experiments, we use Stable Diffusion XL  (SDXL) as our base image diffusion model. We start by finetuning SDXL on conditional tasks. We choose super-resolution and inpainting as the tasks of interest since they are both commonly used in the inverse problems literature and they represent two distinct scenarios: in super-resolution, the input condition is strong and inpainting, the model needs to generate new content. For super-resolution, we choose a downsampling factor of \(8\). For inpainting, we create masks of different shapes at random, following the work of . During the finetuning, we train the model to predict the uncorrupted image given the following inputs: i) the encoding of the noised image, ii) the noise level, and, iii) the encoding of the corrupted (downsampled/masked) image. To condition on the corrupted observation, we concatenate the measurements across the channel dimension. We train models with and without correlated noise on the COYO dataset  for \(100\)k steps. We show realizations of independent and correlated noise in Figure 6. Additional implementation details are in Section F.2, including the parameters for the GP introduced in Section 3.1.

[MISSING_PAGE_FAIL:7]

**Noise Warping baselines.** As explained in Section 1, to apply image diffusion models to videos, we need to transform the noise as we move from one frame to the next. We consider the following noise-warping baselines that were used in : **Fixed noise** uses the same noise across all the frames.

**Resample noise** samples a new noise for each new frame. **Nearest Neighbor** uses the noise of the nearest location in the grid to evaluate the noise at the location that is not on the regular grid \(E_{k}\). **Bilinear Interpolation** interpolates the values of the noise bilinearly in the neighboring locations that lie on the grid. **How I Warped Your Noise ** is the state-of-the-art method for solving temporally correlated inverse problems with image diffusion models. It warps the noise by using auxiliary high-resolution noise maps (see our intro, related work section, and Section C). **Our GP Noise Warping** warps the input noise by resampling the Gaussian process in the mapped locations. We note that the Fixed Noise, Resample Noise, and Nearest Neighbor noise warping methods can be applied to models that are trained with either independent noise or correlated noise coming from a GP. For all the experiments, we also include our proposed method, _Warped Diffusion_ that uses GP Noise Warping and Equivariance Self-Guidance (see Algorithm 1 for a reference implementation).

**Video Evaluation Metrics.** We follow the evaluation methodology of the "How I Warped Your Noise" paper . Specifically, we want to measure two different aspects of our method: i) average restoration performance across frames, ii) temporal consistency. For i), we measure the average of all the previously reported metrics (FID, Inception, CLIP Image/Text score, SSIM, LPIPS and MSE) across the frames. For ii), we measure the self-warping error, i.e. how consistent are the model's predictions across time. The warping error can be computed in either pixel or latent space and also with respect to the first generated frame or the previously generated frame, totaling \(4\) warping errors.

To warm up, we start with videos that are synthetically generated by 2-D shifting of a single image, as in Figure 1. To further simplify the setup, we consider the easy case of shifting the current frame by an integer amount of pixels with each new frame. For 2-D translations by an integer amount of pixels, the Nearest Neighbor, Bilinear Interpolation, How I Warped Your Noise and GP Noise Warping methods they become essentially the same since we always evaluate the noise distribution on points in the grid \(E_{k}\). Hence, the only difference is whether we apply these methods to white noise or to GPs.

Figure 1 (Row 2) shows that the How I Warped Your Noise baseline produces temporally inconsistent results as we shift the masked input image. Even though all the inpaintings are of high quality, the baseline results are temporally inconsistent. Instead, our _Warped Diffusion_ method produces temporally consistent results since it enforces equivariance by design. Since the How I Warped Your Noise warping mechanism and GP coincide here, the benefit strictly comes from enforcing the equivariance property. In fact, one could get the same results for the How I Warped Your Noise method by penalizing for equivariance at inference time.

We present quantitative results regarding temporal consistency in Figure 3 (and additional results in Figure 4 in the Appendix). As shown in the Figure, the fixed noise and the resample noise baselines perform the worst w.r.t. the temporal consistency both in latent and pixel space. The warping error of the Resample baseline is almost constant across frames as expected, while the warping error of the Fixed Noise increases with time. Both the How I Warped Your Noise method and our GP warping framework significantly improve the baselines. Yet, they still have significant temporal inconsistencies as evidenced by the results in Figure 1 and the supplemental videos. The two methods perform on par on this task since they are essentially the same when it comes to integer shifts: the only difference is that GP Noise Warping is applied to correlated noise coming from a GP. The remaining temporal errors are not an artifact of the noise warping mechanism but they are due to the fact that the model itself is not equivariant w.r.t. the underlying transformation. The warping errors essentially disappear when we apply Equivariance Self Guidance. As shown in Figure 3, our method, Warped Diffusion, achieves almost \(0\) warping error (1e-4 mean pixel error with respect to the first frame to be precise) since it is enforcing equivariance by design.

The only remaining question is whether Warped Diffusion maintains good restoration performance. To answer this, we measure mean restoration performance across frames for the aforementioned metrics. We report our results in Table 2, including the mean warping error with respect to the first frame. As shown, Warped Diffusion maintains high performance across all the considered metrics while being significantly superior in terms of temporal consistency. The conclusion is that all the other noise warping baselines, including the previous state-of-the-art How I Warped Your Noise paper , perform poorly in terms of temporal consistency since they rely on the assumption that the network is equivariant. Even for simple temporal correlations such as integer movement in the 2-D space, thisassumption is false for the challenging inpainting task. Warped Diffusion is the only method that achieves temporal consistency while it still manages to maintain high reconstruction performance.

We finally remark that our sampling algorithm enforces equivariance in the latent space. Yet, the warping errors are negligible in the pixel space as well. Our finding is that improving latent space equivariance translates to improvements in pixel space equivariance. The authors of  also find that "the VAE decoder is translationally equivariant in a discrete way".

### Effect of Sampling Guidance for more general transformations

We proceed to evaluate our method on realistic videos. We measure performance on 600 captioned videos from the FETV  dataset. Since baseline inpainting methods fail even for very simple temporal transformations, we focus on \(8\) super-resolution for our comparisons on FETV.

For our video results, we could not provide comparisons with the How I Warped Your Noise paper. At the time of this writing, there was no available reference implementation as we confirmed with the authors by direct communication. In any case, the authors acknowledge as a limitation of their work that their proposed method has "limited impact on temporal coherency" when applied to latent models and that "all the noise schemes produce temporally inconsistent results" . Once again, we attribute this to the non-equivariance of the denoiser, which we mitigate with our guidance algorithm.

We proceed to evaluate our method and the baselines with respect to temporal consistency and mean restoration performance across frames, as we did for our inpainting experiments. We present our results in Table 3 and additional results in Figures 5, 8 of the Appendix and in the following URL as videos: https://giannisdaras.github.io/warped_diffusion.github.io/. As shown in Table 3, there is a trade-off between temporal consistency and restoration performance. Methods that perform better in terms of temporal consistency often have significantly worse performance across the other metrics. Our Warped Diffusion achieves a sweet spot: it has the lowest warping error by a large margin and it still maintains competitive performance across all the other metrics. On the contrary, methods that are based solely on noise warping, such as GP Warping and the simple interpolation methods, lead to significant performance deterioration for a small improvement in temporal consistency.

Noise Warping Speed.We measure the time needed for a single noise warping. Our GP Warping mechanism takes \(39\)ms per frame Wall Clock time, to produce the warping at \(1024 1024\) resolution. This is \(16\) faster than the reported \(629\)ms number in . If we use batch parallelization, our method generates \(1000\) noise warpings in just \(46\)ms (at the expense of extra memory).

No Warping?A natural question is whether we can omit completely the noise warping scheme since equivariance is forced at inference time. We ran some preliminary experiments for super

   Method & Warping Err \(\) & FID \(\) & Inception \(\) & CLIP Tat \(\) & CLIP Img \(\) & SSIM \(\) & LPIPS \(\) & MSE \(\) \\  Fixed (gp) & 0.129\(\)0.02 & 60.853\(\)0.298 & **12.421\(\)0.018** & **0.280\(\)0.000** & 0.924\(\)0.000 & 0.800\(\)0.001 & **0.182\(\)0.001** & 0.060\(\)0.002 \\ Fixed (indep) & 0.080\(\)0.00 & 67.021\(\)0.268 & 10.301\(\)0.302 & 0.275\(\)0.000 & 0.919\(\)0.000 & 0.780\(\)0.001 & 0.195\(\)0.001 & 0.059\(\)0.002 \\ Resample (indep) & 0.101\(\)0.00 & 71.078\(\)0.145 & 11.740\(\)0.055 & 0.277\(\)0.000 & 0.921\(\)0.000 & 0.781\(\)0.006 & 0.196\(\)0.002 & 0.061\(\)0.003 \\ Resample (gp) & 0.141\(\)0.00 & 60.002\(\)0.187 & 11.381\(\)0.012 & 0.277\(\)0.000 & **0.925\(\)**1.000** & **0.306\(\)**0.000** & **0.182\(\)**0.000** & **0.056\(\)0.003** \\ How I Warped (indep) & 0.046\(\)0.007 & 68.701\(\)0.298 & 10.877\(\)0.012 & 0.267\(\)0.000 & 0.910\(\)0.000 & 0.181\(\)0.001 & 0.197\(\)0.001 & 0.057\(\)0.001 \\ GP Warping & 0.061\(\)0.000 & **59.897\(\)**3.778** & 11.727\(\)0.375 & 0.277\(\)0.000 & 0.924\(\)0.000 & 0.803\(\)0.002 & **0.182\(\)**0.001** & 0.057\(\)0.002 \\
**Warped Diffusion** (Ours) & **0.001\(\)0.001** & 61.249\(\)0.499 & 11.802\(\)0.427 & 0.276\(\)0.000 & 0.917\(\)0.006 & 0.779\(\)0.001 & 0.188\(\)0.006 & 0.058\(\)0.001 \\   

Table 2: Mean-frame evaluation of inpainting models for the translation task.

Figure 3: Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.

resolution on real-videos and we found that omitting the warping significantly deteriorates the results when the number of sampling steps is low. We found that increasing the number of sampling steps makes the effect of the initial noise warping less significant, at the cost of increased sampling time.

## 5 Limitations

Our method has several limitations. First, the guidance term increases the sampling time, as detailed in the Appendix, Section F.3. For reference, processing a 2-second video takes roughly 5 minutes on a single A-100 GPU. Second, even though in our experiments we observed a monotonic relation between the warping error in latent space and warping error in pixel space, it is possible that for some transformations the decoder of a Latent Diffusion Model might not be equivariant. We noticed that this is a common failure for text rendering, e.g. in this latent video the model seems to be equivariant, but in the pixel video it is not. Third, the success of our method depends on the quality of the flow estimation - inconsistent flow estimation between frames will lead to flickering artifacts. For real videos, there might be occlusions and the estimation of the flow map can be noisy. We observed that in such cases our method fails, especially for challenging tasks such as video inpainting. The correlations obtained by following the optical flow field obtained from real videos might lead to a distribution shift compared to the training distribution. For such extreme deformations, our method produces correlation artifacts. This has been observed in prior work (see this video), but it also appears in our setting (e.g. see this video). Finally, our method cannot work in a zero-shot manner since it requires a model that is trained with correlated noise.

## 6 Conclusions

Warped Diffusion is a novel framework for solving temporally correlated inverse problems with image diffusion models. It leverages a noise warping scheme based on Gaussian processes to propagate noise maps and it ensures equivariant generation through an efficient equivariance self-guidance technique. We extensively validated Warped Diffusion on temporally coherent inpainting and superresolution, where our approach outperforms relevant baselines both quantitatively and qualitatively. Importantly, in contrast to previous work , our method can be applied seamlessly also to _latent_ diffusion models, including state-of-the-art text-to-image models like SDXL .

## 7 Acknowledgements

This research has been partially supported by NSF Grants AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, Amazon, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Giannis Daras has been partially supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship.