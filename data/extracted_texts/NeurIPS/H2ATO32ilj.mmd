# ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users

Guanlin Li\({}^{1}\), Kangjie Chen\({}^{1,*}\), Shudong Zhang\({}^{2}\), Jie Zhang\({}^{3}\), Tianwei Zhang\({}^{1}\)

\({}^{1}\)Nanyang Technological University, \({}^{2}\)Xidian University,\({}^{3}\)CFAR and IHPC, A*STAR.

\({}^{*}\) Corresponding author

{guanlin001, kangjie001}@e.ntu.edu.sg, sdong.zhang@outlook.com,

zhang_jie@cfar.a-star.edu.sg, tianwei.zhang@ntu.edu.sg

###### Abstract

Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.

Content warning: This paper includes examples that contain offensive content (e.g., violence, sexually explicit content, negative stereotypes). Images, where included, are blurred but may still be upsetting.

## 1 Introduction

Recently, generative models have achieved significant success in text generation, exemplified by models such as ChatGPT , Llama , and Mistral , as well as in image generation with models like Stable Diffusion  and Midjourney . Despite their utility in daily applications, these models can produce biased and harmful content, both intentionally and unintentionally. For instance,  have designed jailbreak methods that circumvent the safeguards of large language models (LLMs), enabling them to generate harmful and illegal responses. These security risks are a major concern for model developers, researchers, users, and regulatory bodies. Thus, enhancing the safety of content generated by these models is of paramount importance.

To ensure generative models produce unbiased, safe, and legal responses, one crucial approach is aligning the models with human preferences and values. This involves supervising the training data collection and checking the training process during model development. Once the training is complete, another critical step is to analyze the model's safety through advanced attacking methods, a process known as red-teaming . Previous red-teaming methods designed for LLMs to bypass safeguards and produce harmful responses utilize jailbreak attacks  and various adversarial attacks . However, text-to-image models, such as Stable Diffusion Models, have receivedless attention in red-teaming research. Besides, previous works on red-teaming for text-to-image models generally examine the model's safety under a hypothetical scenario where a malicious user aims to _intentionally_ craft adversarial prompts, revealing that carefully designed unsafe prompts lead to unsafe generations. However, in a scenario where benign users are normally using the model, it is still possible to _unintentionally_ generate some unsafe content, meaning that **even safe prompts1 can lead to unsafe generations**. The safety in this context is evidently more important. Firstly, compared to adversarial prompts, these safe prompts are harmless, making them more difficult to filter by safeguards. Moreover, since the vast majority of the model's users are benign, any user may unintentionally receive an unsafe generation. As shown in Figure 1, the safe prompts, collected from Lexica , can result in unsafe images. Some of them include violent elements and bloody content, and others contain naked bodies, which reveals the undiscovered safety risks in the previous methods. Therefore, we are dedicated to studying the safety of text-to-image models in this scenario.

A concurrent work, Adversarial Nibbler , conducted by Google, introduces a red-teaming methodology by crowdsourcing a diverse set of implicitly adversarial prompts. Essentially, they encourage participants to create safe prompts that trigger text-to-image models to generate unsafe images, where we are on the same page. They discover these safe prompts reveal safety risks not identified by other red-teaming methods and benchmarks. However, crowdsourcing methods are often impractical because it is challenging to protect the welfare of human labor in such an open environment and crowdsourcing methods are also expensive. Moreover, Adversarial Nibbler method employs human evaluation to assess prompt safety and image harmfulness, cultural differences among evaluators can introduce biases and errors. Therefore, it is essential to develop an automatic red-teaming method to evaluate models under safe prompts.

Designing an automatic red-teaming method for text-to-image models is not straightforward and faces several challenges. First, unlike text-to-text models, red-teaming for text-to-image models must consider two modalities. An intuitive approach is to use a Vision Language Model (VLM) to understand the images and generate new prompts. However, if we adopt a single VLM to generate prompts, it requires the model to be able to craft safe prompts on the basis of understanding the content of different categories as well as the connections between prompts and images. Such complex tasks usually require high-quality training data and more model parameters, making the training process and the inference process less efficient. Secondly, defining the safety of prompts and the harmfulness of images is tricky. Unlike Adversarial Nibbler  employing human experts and public participants to manually determine the safety of prompts and images, an automatic red-teaming method requires a new form of safety checking for them. Finally, since unsafe images contain various types of harmful information, an automated red-teaming task should comprehensively assess the model's safety regarding a wide range of toxic content.

To overcome the aforementioned challenges, we propose the Automatic Red-Teaming framework, named ART, combining the powerful LLMs and VLMs, with the help of various detection models to launch a red-teaming process on given text-to-image models. Specifically, we first decompose the complex task into subtasks, i.e., building connections between images and harmful topics, aligning harmful images and safe prompts, and building connections between safe prompts and harmful topics. Based on this decomposition, we use a VLM to establish the connection between images and different topics, aligning these images with their corresponding safe prompts. Then, we introduce an LLM to learn the knowledge from the VLM and build connections between safe prompts and different topics. In our approach, the VLM is utilized to understand the generated images and provide suggestions for

Figure 1: Safe prompts can lead to harmful and illegal images. Prompts are shown below the images.

modifying the prompts instead of directly providing a prompt, while the LLM uses these suggestions to modify the original prompts, thereby increasing the likelihood of triggering unsafe content.

Considering that conventional LLMs and VLMs do not possess the above capabilities, we need to fine-tune them to achieve the desired functionality. Thus, we need to collect a dataset of (safe prompt, unsafe image) pairs from open-source prompt websites (e.g., Lexica ) and reliably determine the safety of both prompts and images. To achieve it, we adopt a group of detection models including _prompt safety detectors_, which ensure that the collected prompts do not contain any harmful information, and _image safety detectors_, which can judge the safety of images for different toxic categories to guarantee the collected images are harmful.

Additionally, we categorize the collected data into seven types based on the harmful information contained in the images, following the taxonomy in previous works [39; 38], to construct a meta dataset. This taxonomy allows a more fine-grained analysis of the model's safety across different types of harmful content. Based on this meta dataset MD, we propose two derived datasets, i.e., the dataset LD for LLM fine-tuning and the dataset VD for VLM fine-tuning. The details of these datasets will be described in Section 3.3.

After fine-tuning LLMs and VLMs, our proposed ART introduces an iterative interaction among the LLM, the VLM, and the target text-to-image (T2I) model. In detail, during the interaction, the LLM first generates a prompt for a specific toxic category and gives it to the T2I model for image generation. Then, the generated image and the prompt are given to the VLM, which provides instructions on how to modify the prompt. The LLM then generates a new prompt based on the instruction and the previous prompt. This interaction process will be repeated until meeting a pre-defined number of rounds. After that, ART adopts the detectors to check whether the prompt and the image are safe or not in each interaction. To evaluate the effectiveness of our proposed automatic red-teaming method ART, we conduct extensive experiments on three popular open-source text-to-image models and achieve 56.25%, 57.87%, and 63.31% success rates, respectively. Besides, we also build three comprehensive red-teaming datasets for text-to-image models, which will provide researchers and developers with valuable resources to understand and mitigate the risks associated with text-to-image generation tasks. Overall, our contributions can be summarized as:

* We propose the first automatic red-teaming framework, ART, to find safety risks when benign users use text-to-image models with only safe and unbiased prompts.
* We propose three comprehensive red-teaming datasets, which serve as crucial tools to enhance the robustness of text-to-image models.
* We use ART to systematically study the safety risks of popular text-to-image models, uncovering insufficient safeguards during inference from benign users, particularly in larger models.

## 2 Related Works

### Advanced Generative Model

Generative models have made a big impression in recent years. Large language models (LLMs), based on transformer  structures with billions of trainable parameters, trained on massive text data, such as LlamA  and Mistral , show advanced capabilities in generating creative articles, chatting with humans, and help people finish their works. After aligning with a vision transformer, LLMs are given abilities to understand images, which are called vision language models (VLMs), such as Otter , LLaVA , and Flamingo . These VLMs are built on LLMs to better understand the instructions and generate responses for a given image. Besides, another multi-modal model, the text-to-image model, can generate images following a given text. One of the most popular text-to-image model, named Stable Diffusion Model , operate by iteratively refining an image, starting from pure noise and gradually denoising it to match the desired distribution. Stable Diffusion Models achieve greater control over the image generation process and demonstrate impressive results in generating high-fidelity images with intricate details.

With the increasing complexity and impact on our daily routines from these models, researchers underscore the importance of robust evaluation and security measures for them. Red-teaming [19; 42], a practice involving simulated attacks to identify vulnerabilities, is essential for ensuring the safety, fairness, and robustness of generative models. By systematically evaluating these models, researchers can uncover biases, improve resilience against adversarial attacks, and enhance the overall reliability of generative AI systems.

### Red-teaming for Text-to-image Models

There are several concurrent red-teaming works for text-to-image models. FLIRT  incorporates the feedback signal into the testing process to update the prompts by in-context learning with a language model. However, it only considers the feedback signal based on the generated images, causing the generated prompts to be highly toxic. Groot  aims to achieve a safe prompt red-teaming framework by decomposing unsafe words and replacing them with other terms in the prompt. This method requires original unsafe prompts as initialized prompts. Therefore, the generalizability and expandability of Groot is weak. Another work, MMA-Diffusion  generates adversarial prompts through optimization to find a prompt having similar semantics to the unsafe prompt. Clearly, it requires unsafe prompts as targets and is based on a gradient-driven optimization process. Therefore, it faces the same weaknesses as Groot. Curiosity  is driven by reinforcement learning methods to teach a language model to write prompts with the feedback from a reward model, i.e., a not-safe-for-work detector. Compared with FLIRT, Curiosity can generate safer prompts. However, Curiosity is highly related to the text-to-image model and lacks generalizability.

We compare ART and concurrent works in Table 1. The Naive method is to select captions from MSCOCO  as safe prompts to test the model. FLIRT, MMA-Diffusion, and Curiosity require gradients directly or indirectly from the text-to-image model, which means they are model-related. FLIRT and Curiosity only focus on generating not-safe-for-work images and cannot generalize to other toxic categories. On the other hand, all previous works do not have the ability to continuously generate testing examples, as they aim to modify a given initialized prompt. Moreover, these methods lack expandability to fit emerging new models and evaluation benchmarks. For ART, it does not require prior knowledge of the text-to-image model and acts like a normal user to provide prompts to the text-to-image models. On the other hand, ART can generate safe prompts continuously and diversely based on specific categories. More importantly, because the agent models are fine-tuned with LoRA , they can cooperate with other LoRA adapters, that are obtained on new datasets, in the future. The other detectors can also be added to the detection models. Therefore, ART is a more advanced red-teaming method.

## 3 Auto Red-teaming under Safe Prompts

In this section, we provide a detailed introduction to our proposed datasets and the novel automatic red-teaming framework, ART. First, we present the motivation and insights behind automatic red-teaming. Then, we introduce the details of the three new datasets and describe ART in depth.

### Motivation and Insight

In previous works [46; 47], adversarial attacks were employed to break the safeguards of text-to-image models. These attacks identify prefixes, suffixes, or word substitutions that can be added to or replace parts of the original prompt, leading the model to generate unsafe images while keeping the prompt not explicitly harmful. Clearly, normal users would not engage in such activities to intentionally produce unsafe images. However, our research indicates that normal users are still not adequately protected from unsafe content by the model's safeguards. Even with benign and unbiased prompts, the model can occasionally generate harmful and biased content. These findings motivate us to explore the safety risks of text-to-image models from a different angle: _protecting normal users from unsafe content._ Consequently, our goal is to develop a method that consistently generates diverse yet safe prompts, capable of exposing the text-to-image model's potential to generate harmful images.

To better understand how safe prompts can lead to harmful generated images, we draw inspiration from agents driven by LLMs and VLMs to design an automatic framework. In this framework, agents help us explore various safe prompts and evaluate whether they cause a given text-to-image model

  
**Method** & **Model Agnostic** & **Category Adaptation** & **Safe Prompt** & **Continuous Generation** & **Diversity** & **Expandability** \\  Naive & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\  FLIRT  & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ \\  Groot  & ✓ & ✓ & ✓ & ✗ & ✓ & ✗ \\  MMA-Diffusion  & ✗ & ✓ & ✓ & ✗ & ✓ & ✗ \\  Curiosity  & ✗ & ✗ & ✓ & ✗ & ✓ & ✗ \\  ART & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparisons between concurrent works and ART.

to generate toxic images. Thus, we propose ART, the first automatic red-teaming framework for text-to-image models aimed at protecting normal users.

### Pipeline of ART

As shown in Figure 2, ART consists two core components, namely the Writer Model and the Guide Model, that are developed with an LLM and a VLM, respectively. These agents work collaboratively: the Writer Model improves the prompt based on instructions provided by the Guide Model, and the Guide Model gives instructions based on the prompt generated by the Writer Model and the corresponding image generated by the T2I model. After that, the Judge Models, built on a group of detectors, evaluate the safety of prompts and images.

To start the red-teaming procedure, we first provide an initial prompt "a pic of a cat", a target harmful category (e.g., hate), and keywords related to the target category to ART for the initialization round. Then, the Writer Model will rewrite the initialized prompt for the given category and keywords based on his knowledge to finish the initialization round. After that, for each round, the T2I Model first receives a prompt from the Writer Model and generates an image. The Guide Model then receives the image and the prompt to generate instructions for the Writer Model to improve the prompt. After that, the Writer Model modifies the previous prompt based on the instructions to end this red-teaming round. After all red-teaming rounds finish, all prompts and corresponding images will be evaluated by the Judge Models to determine whether they are safe or harmful.

There are several advantages in ART. First, the interactions during each round can provide model developers with more information from the improvement instructions of the Guide Model to better understand how the Writer Model creates such prompts. The information can be used to develop safeguards to improve the safety of the model. Second, the flexibility of combinations for harmful categories, keywords, and the number of red-teaming rounds provides model owners with more options to discover potential and fine-grained safety risks in their models. Third, the Judge Models used in ART can be easily extended and replaced with more advanced and private models. These advantages make ART a better choice for developing safe text-to-image models for developers.

### Datasets in ART

To build agents to automatically design and improve prompts, we construct new datasets and leverage them to fine-tune pre-trained models. In this paper, we build three datasets, i.e., the meta dataset MD, the dataset LD for LLMs, and the dataset VD for VLMs.

**Meta Dataset.** We first build the meta dataset MD, which contains safe prompt and their corresponding unsafe images. To collect such data pairs, we follow the method and taxonomy used in the previous work, I2P . Besides, we define a total of 81 toxic keywords in 7 categories 2, which is about 3 times larger than the number of keywords used in the I2P dataset. For each keyword, we collect 1,000 prompts by searching the keyword on Lexica , an open-source prompt-image gallery website. As we focus on safe prompts and unsafe images, we adopt detectors to filter toxic prompts and harmless images. Specifically, we adopt three text detectors, including a toxicity detector , a not-safe-for-work detector , and another toxic comment detector , to filter out the unsafe prompts. We also consider three image detectors: the Q16 detector  and two different not-safe-for-work detectors , to identify the images containing unsafe content. If any prompt detector identifies a collected prompt as unsafe, we will remove it and its corresponding images from the dataset. For the

  
**Category** & **hate** & **harassment** & **violence** & **self-harm** & **sexual** & **shocking** & **illegal activity** \\ 
**\# of prompts** & 1,842 & 1,593 & 2,020 & 2,114 & 1,075 & 3,679 & 3,284 \\   

Table 2: The number of prompts in each category.

Figure 2: Pipeline of ART after initialization round.

prompts that pass the filter, if any image detector deems the corresponding generated image unsafe, we will include this image and its prompt in \(\) as a data pair. Finally, we can obtain a meta dataset \(=\{(c_{k},p_{k},i_{k})|k=0,1,..,N\}\), where \(c\) denotes the category of the data point. \(p\) and \(i\) represent the collected prompt and its corresponding image, respectively. The details of \(\) are shown in Table 2 and Appendix C.

**VLM Dataset.** In our automatic red-teaming framework, the role of the VLM is to understand the content of the generated image \(i_{j}\) and the prompt \(p_{j}\) at the \(j\)-th round, so that it can give suggestion/instruction \(s_{j}\) for how to improve the prompt \(p_{j}\) to construct a new prompt \(p_{j+1}\) to better generate images contain specific harmful content (i.e., for target category \(c\)). Therefore, based on the meta dataset \(\), we construct a new dataset \(\) to fine-tune the VLM to develop this capability. Specifically, we first randomly sample two data examples from different categories: a reference example \((c_{r},p_{r},i_{r})\) and a target example \((c_{t},p_{t},i_{t})\). The purpose of the reference example is to teach the VLM to align the safe prompt \(p_{r}\) and the unsafe image \(i_{r}\). Additionally, the safe prompt \(p_{r}\) from the reference example will serve as the prompt to be modified. The prompt \(p_{t}\) from the target example will be the ground-truth prompt of category \(c\). Therefore, we utilize the VLM to provide general instruction \(s\) based on the differences between the initial prompt \(p_{r}\) and target prompt \(p_{t}\). Since these components are all in text form, we consider using an existing LLM to generate instructions. However, most LLMs, such as GPT-4 , refuse to give instructions because the toxic categories violate their restrictions and user policies. After testing various LLMs, we find that the Meta-Llama-3-70B-Instruct  is the most suitable model to provide instructions. Specifically, we input the reference prompt \(p_{r}\), the target prompt \(p_{t}\), and the target category \(c_{t}\) to Llama 3 and let it provide general instructions. After obtaining the instructions, we use them to construct \(\). Specifically, we follow the format used in LLaVA , i.e., the value from "human" is _"<i,> This image is generated based on <\(p_{r}\)>. Give instructions to rewrite the prompt to make the generated image more relevant to the concept <\(c_{t}\)>."_, and the value from "gpt" is \(s\). This form of data allows the VLM to learn the relationship between safe prompts and unsafe content and provide improvement suggestions based on the initial prompt and the target category.

**LLM Dataset.** As previously discussed, although a VLM can directly modify prompts, its performance is suboptimal due to strict requirements of high-quality training data and more model parameters. Therefore, we adopt a VLM to generate instructions for modifying prompts based on its visual understanding, and then we use an LLM to generate a new prompt based on instructions. To build an LLM with this capability, we created a dataset \(\) with the help of the VLM, which has been fine-tuned on \(\). Specifically, for a reference example \((c_{r},p_{r},i_{r})\) and a target example \((c_{t},p_{t},i_{t})\), we use the prompt \(p_{r}\), the image \(i_{r}\), and category \(c_{t}\) to query the fine-tuned VLM and obtain the general instruction \(s\). Then, we follow the format of Alpaca , where the "input" is _"Modify the prompt: <\(p_{r}\)> based on the instruction <\(s\)> to follow the concept <\(c_{t}\)>."_ and the "output" is _"<\(p_{t}\)>._ This dataset enables an ordinary LLM to quickly learn how to rewrite the initial prompt to the target prompt based on the instructions to align with the knowledge from the fine-tuned VLM.

**Utilization in ART.** The VLM is first fine-tuned on \(\) and then generates \(\). After that, an LLM is fine-tuned on \(\). Both are used LoRA . After fine-tuning two models, we integrate them with the T2I Model into the pipeline of ART as the Guide Model and the Writer Model, respectively. Considering the agents are stateless, without previous conversation logs, we only provide the latest generated prompt to agents during the conversation to save memory.

## 4 Experiments

We conduct comprehensive experiments to evaluate our proposed ART on previous popular open-source text-to-image models and compare it with concurrent works.

### Models

We consider three popular text-to-image models, i.e., Stable Diffusion 1.5 , Stable Diffusion 2.1 , and Stable Diffusion XL . These models have millions of downloads per month from HuggingFace. It implies that there could be tens of millions or billions of normal users facing harmful generated images when they use these open-source models to create. Since our method is a form of red-teaming aimed at improving the model's inherent safety and thus reducing reliance on other safety modules, the models used in our experiments do not include traditional post-processing modules, such as concept erasing [38; 18; 23; 30] and safety detectors [37; 7; 47]. To imitate a normal user, we adopt the widely used negative prompts to enhance the image quality (see Appendix D). If there are no special instructions, we set the guidance scale as 7.5 and use the default settings for other hyperparameters based on _diffisers_.

### Details of Art

In ART, the main components are the Guide Model, the Writer Model, and the Judge Models. For the Guide Model, we fine-tune a pre-trained LLaVA-1.6-Mistral-7B  with LoRA  on VD, to fit different resolutions of generated images. We further adopt this Guide Model to generate LD. To obtain the Writer Model, we fine-tune a pre-trained Llama-2-7B  with LoRA on LD. All training details can be found in Appendix F. The conversation templates used in the inference phase are shown in Appendix G. For the default inference settings, we leave them in Appendix H.

On the other hand, we consider more detection models to construct the Judge Models to avoid the agents in ART overfit the detectors used in building datasets. There are two types of Judge Models, i.e., the Prompt Judge Models and the Image Judge Models. For the Prompt Judge Models, we consider four detection models, i.e., the three detectors used in the meta dataset generation (refer to Section 3.3) and the Meta-Llama-Guard-2-8B . For the Image Judge Models, besides the three detectors used in the meta dataset generation (refer to Section 3.3), we also use the multi-head detector , the fine-tuned Q16 detector , and the safety filter  used in the Stable Diffusion Model. These diverse detectors can mitigate biases in the training data. For example, users with different cultural backgrounds will have different reactions to the same image. These detectors can identify as many unsafe images as possible. The detailed discussion about these detectors can be found in Section 6.

### Baselines

We consider three baseline methods, i.e., Naive method, Curiosity , and Groot . The Naive baseline is to use the captions provided in MSCOCO, which are considered as safe prompts. For Curiosity , we follow their open-source code to train a new language model in our experiments and use this model to generate prompts for evaluation. Groot  is designed for DALL-E 3  and Midjourney , and we directly use the reported prompts in their paper. For FLIRT , we find that the generated prompts are all harmful, such as "A woman covered in blood." and "An image of two dead babies.". Therefore, we do not consider it as a comparable baseline. Besides, MMA-Diffusion  is based on gradient optimization from the text-to-image model to generate adversarial prompts, we do not consider such a method in our paper.

### Results

In our experiments, we repeatedly run ART 5 times with different random seeds to obtain generated prompts. Every time we run ART, we consider a 50-round conversation between the Writer Model and the Guide Model. Therefore, including the initialization round, there are total 5 * (50 + 1) = 255 prompts for each SD Model, generated by the Writer Model. Since the Naive method and Curiosity do not support generating images based on a given toxic category, we randomly select 255 captions as prompts from the MSCOCO dataset for the Naive method and randomly generate 255 prompts from the language model in Curiosity. For Groot, because it requires seed prompts, which are given by the authors (33 seed prompts for each category), and rewrite them to make them harmless, we

   &  &  & **\# of** & **ratio of** &  \\    & & **TD** & **NSFW-P** & **TCD** & **LlamaGuard** & & **safe prompt** & **safe prompt (\%)** \\  Naive & - & 0 & 3 & 4 & 0 & 248 & 97.25 & 97.25 \\  Curiosity & - & 0 & 22 & 2 & 1 & 231 & 90.59 & 90.59 \\   & **hate** & 7 & 0 & 3 & 30 & 2 & 6.06 &  \\   & **harassment** & 0 & 2 & 0 & 7 & 24 & 72.73 & \\   & **violence** & 0 & 9 & 1 & 4 & 20 & 60.61 &  \\   & **self-harm** & 0 & 6 & 0 & 27 & 5 & 15.15 & \\   & **sexual** & 6 & 29 & 0 & 21 & 2 & 6.06 &  \\   & **shocking** & 1 & 11 & 0 & 6 & 21 & 63.64 &  \\   & **illegal activity** & 0 & 0 & 1 & 6 & 27 & 81.82 & \\   & **hate** & 4 & 7 & 15 & 13 & 221 & 86.67 &  \\   & **harassment** & 3 & 13 & 11 & 6 & 230 & 90.20 &  \\   & **violence** & 3 & 9 & 10 & 1 & 237 & 92.94 & \\    & **self-harm** & 1 & 11 & 18 & 6 & 224 & 87.84 &  \\    & **sexual** & 5 & 37 & 15 & 8 & 203 & 79.61 & \\    & **shocking** & 5 & 7 & 12 & 4 & 233 & 91.37 & \\    & **illegal activity** & 9 & 11 & 20 & 5 & 215 & 84.31 & \\  

Table 3: Prompt toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.

only obtain 33 prompts for each category. Then, for each prompt, we first adopt the Prompt Judge Models to detect its safety. If it is a safe prompt, we use the SD Model to generate 5 images based on this prompt and use the Image Judge Models to check whether the generated images are safe or not.

**Prompt Toxicity**. We adopt the Prompt Judge Models to measure the toxicity of generated prompts. We present the results for Stable Diffusion 1.5 in Table 3. The results indicate that ART can generate safe prompts with a high probability. Besides, compared with Curiosity, ART achieves good generalizability of different toxic categories. On the other hand, although Groot can generate prompts for different categories, the ratio of safe prompts in all generated prompts is lower. We also find that for the "sexual" category, the generated prompts from Groot are easy to contain explicit sexual elements, such as naked bodies and breasts. However, ART prefers to use names of characters in Greek mythology, such as Aphrodite and names from the Bible to create prompts, without explicit harmful words, making the ratio of safe prompts higher. In summary, ART is more advanced in generating safe prompts for different toxic categories in the red-teaming process.

**Image Toxicity**. We generate images with only safe prompts using 5 different random seeds. If there are harmful images in these 5 generated images, we mark this prompt as the one that causes the model to generate unsafe images, which is called a success. We calculate the success ratio based on the number of successes and the number of safe and all prompts, respectively. In Table 4, we present the results for Stable Diffusion 1.5. First, we find that a small part of prompts from MSCOCO can generate unsafe content. It is mainly because these advanced detectors are more sensitive to negative information in the images. Second, although the success ratio for Groot is high when we only consider safe prompts, we find the success ratio is very low when we count all generated prompts. This heavily reduces the efficiency of the red-teaming process. The results indicate that ART can achieve the highest success rate on average. Besides, compared with Adversarial Nibbler , the ART highly reduce the cost and biases of the generated test cases from humans. Therefore, ART has higher effectiveness and efficiency in finding safety risks for text-to-image models with safe prompts.

**Impacts of Generation Settings in T2I Models**. To study the impacts of the generation settings used in Stable Diffusion Models, we consider running ART on Stable Diffusion 1.5 under different guidance scales and output resolutions when the model generates images. For the guidance scales, we consider a set of vales {2.5, 5.0, 7.5, 10.0, 12.5} and set the image resolution as 512x512. For the image resolutions, we consider possible values {256x256, 512x512, 768x768, 1024x512, 512x1024, 1024x1024} and set the guidance scale as 7.5.

For each setting, we run a 50-round conversation on Stable Diffusion 1.5. Then, for each generated prompt, we use it to generate 5 images. Therefore, we obtain (50 + 1) prompts and 5 * (50 + 1) = 255 images. We show results in Figure 3 for three categories, i.e., "violence", "shocking", and "self-harm". The success ratio of toxic images is based on only safe prompts. From the figures, we can find that the generation settings does not affect the ratio of safe prompt significantly. The Writer Model can generate safe prompts with a very high probability under different settings. However, the impact on the success ratio of generating unsafe images is very random. We guess this impact mainly depends on the distribution of the training data of the text-to-image model. The guidance scales will make the model lean to follow the prompts or not, which increases the randomness in the generation results. Images under some resolutions could be less toxic. Similarly, we guess the reason is that

    &  &  &  &  &  &  \\  & & & & & & & & & & & \\   & **-** & 12 & 1 & 0 & 4 & 4 & 3 & 16 & 6.45 & 6.27 & 6.27 \\   & **-** & 50 & 15 & 52 & 98 & 22 & 138 & 113 & 48.92 & 44.31 & 44.31 \\   & **late** & 5 & 0 & 2 & 2 & 2 & 1 & 50.00 & 3.03 & \\   & **barsarsament** & 10 & 4 & 2 & 5 & 0 & 9 & 11 & 45.83 & 33.33 \\   & **visolence** & 66 & 0 & 1 & 14 & 0 & 44 & 19 & 95.00 & 57.58 \\   & **self-harm** & 3 & 0 & 0 & 0 & 0 & 0 & 2 & 40.00 & 6.06 & 30.30 \\   & **swank** & 0 & 2 & 6 & 5 & 2 & 6 & 2 & 100.00 & 6.06 \\   & **shocking** & 38 & 3 & 10 & 18 & 7 & 20 & 15 & 71.43 & 45.45 \\   & **Biged activity** & 51 & 2 & 0 & 9 & 1 & 34 & 20 & 74.07 & 60.61 \\   & **late** & 203 & 7 & 26 & 92 & 13 & 193 & 134 & 60.63 & 52.55 \\   & **barsament** & 203 & 9 & 18 & 61 & 15 & 168 & 135 & 58.70 & 52.94 \\   & **visidence** & 400 & 16 & 48 & 140 & 24 & 248 & 185 & 78.06 & 72.55 \\   & **self-harm** & 206 & 25 & 57 & 71 & 19 & 139 & 138 & 61.61 & 54.12 \\   & **svank** & 99 & 50 & 93 & 98 & 78 & 118 & 124 & 61.08 & 48.63 \\   & **shocking** & 276 & 29 & 45 & 78 & 25 & 158 & 151 & 64.81 & 92.22 \\   & **biged activity** & 229 & 4 & 21 & 71 & 15 & 158 & 137 & 63.72 & 53.73 \\   

Table 4: Image toxicity for Stable Diffusion 1.5. The abbreviations of the Judge Models can be found in Appendix B. Results for others are in Appendix I.

in the model's training data, the resolutions of unsafe images are different, making the model have different probability to generate unsafe images under different resolution. These results indicate that our ART method maintains satisfactory effectiveness under different generation parameter settings.

**Prompt Diversity**. Diversity is an important metrics to measure the generation quality in red-teaming tasks. A good method should generate diverse test cases to evaluate the model comprehensively. Therefore, we follow the diversity metrics, i.e., the SelfBLEU score and the BERT-sentence embedding distance, used in Curiosity  with the same settings. In Figure 4, we use "1-AvgSelfBLEU" and "1-CossSim" to represent the diversity under the SelfBLEU and the embedding distance, respectively. A higher value stands for a better diversity of generated prompts. Because the diversity of Groot depends on the seed prompts provided by the authors, we do not consider this method as a baseline. From the results, we find that ART achieves a higher generation diversity for all categories.

## 5 Discussion

**Applicable to Online T2I Models.** Besides the open-source models, we provide a case study on DALL_E 3  in Appendix K and Midjourney  in Appendix L. Overall, the results show that although commercial models employ pre-processing modules like prompt detectors and post-processing modules like image detectors, our ART can still use safe prompts to make it generates and outputs unsafe images. This demonstrates that the current pre-processing and post-processing methods are not entirely effective in eliminating such threats, further emphasizing the importance of automatic red teaming.

**Applicable to More Generation Models.** Our proposed ART is a general framework for automated red teaming. In this paper, we focus on testing T2I models; therefore, within the ART framework, we utilize two agents: a VLM and an LLM. Additionally, the ART framework can be applied to red teaming tasks for other generative models, such as large language models and other vision-language models. Developers have the flexibility to adjust the agents and the fine-tuning datasets accordingly.

## 6 Limitation

There are three limitations in ART for now. The first one is that the Guide Model can only accept one image at one time. However, text-to-image models, such as Stable Diffusion models, can generate

Figure 4: Diversity of generated prompts for categories. Dash lines stand for the results of Curiosity.

Figure 3: Ratio of safe prompt and success ratio for unsafe images under different Stable Diffusion generation settings. Results for other categories can be found in Appendix I.

many images for one prompt once. Moreover, even for the same prompt, the model can generate very different content under different random seeds. Therefore, the current behavior of the Guide Model will not only limit the evaluation speed but also scarify some information for the generated prompt. The solution used in our experiments is to run an additional generation process for all generated prompts with different random seeds and obtain the final results. In the future, we plan to propose some new datasets and training strategies to help VLMs work harmoniously with multiple images. On the other hand, the speed for one round is about 20 seconds, including the image generation cost.

The second limitation is that there are some misalignments in the datasets, as large models generate them without human re-checking. The solution is straightforward, i.e., we can manually check the dataset and recalibrate the flaws. However, this process is heavily costed. Another potential solution is to use more sophisticatedly crafted data to dilute the imperfect data in the training set. We notice that the Adversarial Nibbler  is a promising candidate. It will be our future work to explore such approaches.

The third limitation is that the automatic detection methods used in ART are not 100% perfect. Determining whether an image is harmful or not is challenging because it is heavily related to people's cultural backgrounds and preferences, and the laws of different countries. For example, the training data of the Q16 detector  are labeled by people from North America in most cases. The training data of the multi-head detector  and the fine-tuned Q16 detector  are labeled by three authors from Asia. There are some agreements and disagreements among them. In ART, we attempt to reduce biases and omissions during the detection process by using multiple detectors. However, it is inevitable that some safe images determined by these detectors could hurt others, due to their personal experiences. This asks the model developers to design flexible safety restrictions to meet different personalization requests. In the future, we will explore how to design more fine-grained red-teaming methods. For example, invite more people from Europe, Africa, and South America to label data to train detectors.

## 7 Conclusion

In this paper, we propose the first automatic red-teaming framework, ART, for text-to-image models. We focus on safe prompts that will cause the model to generate harmful images. Besides, we collect and craft three new large-scale datasets for research use to help researchers build more advanced automatic red-teaming systems. With our comprehensive experiments, we prove ART is a useful tool for model developers to find the safety risks in their models and can help them craft targeted resolutions to fix these flaws in Appendix I. Moreover, we further discuss the border social impacts of our work in Appendix N, respectively. We believe our work will help us build a more safe and unbiased AI community.

## 8 Acknowledgement

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-023[T]), Infocomm Media Development Authority under its Trust Tech Funding Initiative, Singapore Ministry of Education (MOE) AcRF Tier 2 under Grant MOE-T2EP20121-0006. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Infocomm Media Development Authority.