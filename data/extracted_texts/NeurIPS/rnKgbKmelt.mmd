# AdaPlanner: Adaptive Planning from Feedback with Language Models

Haotian Sun\({}^{1}\)

Yuchen Zhuang\({}^{1}\)

These authors contributed equally to this work.

 Lingkai Kong\({}^{1}\)

Bo Dai\({}^{1}\)

Chao Zhang\({}^{1}\)

\({}^{1}\) Georgia Institute of Technology

{haotian.sun, yczhuang, lkkong, chaozhang}@gatech.edu, bodai@cc.gatech.edu

###### Abstract

Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, _AdaPlanner_, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both _in-plan_ and _out-of-plan_ refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.

## 1 Introduction

Large language models (LLMs) have recently emerged as versatile autonomous agents for sequential decision-making in grounded environments. Traditional decision-making methodologies like Reinforcement Learning (RL) require extensive task-specific training data and often lack the ability to generalize across tasks and environments. In contrast, LLMs are pre-trained on massive and diverse textual data, which gives them extensive world knowledge and the ability to reason over the knowledge. This makes them highly versatile and able to handle complex, real-world scenarios that may involve multiple steps of planning and decision-making.

Existing methods that leverage LLMs as autonomous agents for decision-making can be briefly categorized into two groups (Table 1): open-loop systems and closed-loop systems. Open-loop methods  rely on pre-determined plans to accomplish the desired task without any feedback adaptation mechanism. On the other hand, closed-loop systems  incorporate environment feedback to continuously monitor system behaviors and make refinements and adjustments of the plans accordingly, which therefore is more flexible.

However, both existing open-loop and closed-loop LLM agents have inherent drawbacks. Open-loop systems are computationally cheap and simple; however, they do not consider feedback from the environment and stick to the initial plan, which lack of adaptability, and, thus, can easily generate suboptimal plans. On the other hand, most existing closed-loop methods generate a fixed plan and only update their executing actions upon environment feedback. This causes them to makesub-optimal decisions that adapt to the environment in the short term but could have detrimental implications for future steps. DEPS  is the only exception, a method that modifies its entire plan based on feedback from the environment. However, it requires training a plan selector to choose the most successful plan, which requires a significant amount of task-specific data. As a result, applying this method to different tasks can be challenging.

To address the limitations of existing LLM agents, we propose AdaPlanner, a closed-loop planning method with LLM playing two roles - planner and refiner. The planner decomposes the task into manageable sub-goals and predicts environmental feedback for each. During execution, the refiner distinguishes and responds to two types of environment feedback - _in-plan feedback_ is the environmental observation that aligns with the prediction, and _out-of-plan feedback_ is one that deviates from the prediction. For in-plan feedback, the refiner can dynamically query the LLM to perform reasoning and extract key information from in-plan feedback expressed in natural language. This is achieved through a specific action called ask_LLM(), in which the LLM separately parses the observation and obtains information pertinent to subsequent actions. For out-of-plan feedback, the refiner proactively revises the entire plan and resumes to solve the current task from an intermediate point. AdaPlanner's adaptive closed-loop framework alleviates the need for prior knowledge about the feedback structure and permits the agent to instantly adopt a refined plan rather than restarting from scratch in a reset episode. This leads to a more efficient and adaptive decision-making process.

AdaPlanner operates solely via prompting, eliminating the need for a dedicated training phase and reducing its computational cost. Furthermore, AdaPlanner leverages a code-based prompting for precise planning and refinement. The use of code prompts facilitates task decomposition into sub-goals and mitigates LLM hallucination during the decision-making process. AdaPlanner also features a skill discovery process, which accumulates successful experiences to guide future planning. This feature further enhances its long-term planning ability and sample efficiency.

We formally define the planning problem with LLM, and introduce open-loop vs. closed-loop control system, which will motivate our method, in Section 2. Each component of the proposed AdaPlanner is specified in Section 3, including code-based prompting in Section 3.1, closed-loop adaptation in Section 3.2, and skill discovery in Section 3.3, and empirically justified in Section 4. The superior performance of AdaPlanner on both ALFWorld and MiniWoB++ demonstrates our proposed adaptive closed-loop framework can effectively enhance planning performance, even when faced with a limited number of samples.

  Methods & Feedback Utilization & Instruction Type & Prompting & Decomposition & Experience Refinement \\   \\  CoT  & - & Prompting & Language & - & - \\ Least-To-Most  & - & Prompting & Language & Sub-Goals & - \\ Zero-Shot Planner  & - & Prompting & Language & - & - \\ HuggingGPT  & - & Prompting & Language & Sub-Goals & - \\ Chameleon  & - & Prompting & Language & Sub-Goals & - \\   \\  ReAct  & Taking Action & Prompting & Language & - & - \\ Inner Monologue  & Taking Action & Prompting & Language & - & - \\ RCI  & Taking Action & Prompting & Language & - & - \\ ProgPrompt  & Taking Action & Prompting & Code & - & - \\ Code as Policies  & Taking Action & Prompting & Code & - & - \\ Reflexion  & Taking Action & Prompting & Language & - & Past Failure \\   \\  DEPS  & Modifying Plan & Prompting \& Training & Language & Sub-Goals & Past Failure \\ AdaPlanner & Action \& Plan & Prompting & Code & Sub-Goals & Past Failure \& Success \\   

Table 1: A comparison of methods that leverage LLMs for decision making. Each methodâ€™s features are reported across five categories: 1) Environment Feedback Utilization: The method can use feedback to decide the next action (Taking Action), revise the entire plan (Modifying Plan), or do both (Action & Plan). 2) Instruction Type: The method may require prompting, training, or both. 3) Prompting Style: The method can employ either natural language or code for its planning backend. 4) Task Decomposition: The method might decompose the task into sub-goals or not. 5) Experience Refinement: The method can learn from past failure, past success, or both. The AdaPlanner proposed in this paper is highlighted in gray.

## 2 Preliminaries

Problem Formulation.We consider adopting an LLM as an autonomous agent to solve different tasks in text-based environments. For initialization, the agent is provided with allowed actions \(\) in the environment, as well as a text-grounded task definition \(g\) from the task space \(\). Besides, the initial state of the environment is also observed as \(o_{1}\) from the observation space \(\). With such inputs, the LLM agent needs first to generate an initial planning policy for solving the task \(_{0}(P_{0}|g,o_{1}):(^{T})\), where \(T\) is the total length of steps in the generated plan and \(()\) is probability simplex function. Also, the agent can interact with the environment for feedback: When the agent interacts with the environment at the \(t\)-th step, the agent receives an observation \(o_{t}\) from the environment and generates a trajectory-like context \(c_{t}=(o_{1},a^{}_{1},o_{2},a^{}_{2},,a^{}_{t-1},o_{t})\), where \(a^{}_{1},a^{}_{2},,a^{}_{t-1}\) are the executed actions within the environment. As the agent may modify the actions according to the feedback, the executed actions \(a^{}_{1},a^{}_{2},,a^{}_{t-1}\) can be different from the actions \(a_{1},a_{2},,a_{t-1}\) in the initial plan. We denote \((|g,c_{t},P_{t})\) as the high-level planning policy that generates an entire plan and \((|g,c_{t},P_{t})\) as the action-generation policy conditioned on a given plan \(P_{t}\). Given the context \(c_{t}\) and the entire plan at the last step \(P_{t-1}\), the agent refines future decisions. In the end, the LLM agent should model both the initial planning policy and the environment feedback-conditioned policy to complete the given task successfully.

**Open-Loop System.** An open-loop system is a non-feedback system (Figure 1), where the output is solely dependent on the input, without any consideration of the environmental feedback. Thus, in an open-loop system, the entire initial plan over the time horizon \(T\) is predetermined and static by the initial planning policy \(_{0}(|g,o_{1})\), without any feedback-based refinement. Despite their simplicity, open-loop systems are notably vulnerable to environmental changes, as they lack the capacity to adapt or adjust their plans based on environmental feedback.

**Closed-Loop Systems.** On the contrary, a closed-loop system (Figure 1) refers to a planning process that incorporates environment feedback to adjust and refine future decisions, involving both initial planning \(_{0}(|g,o_{1})\) and two levels of feedback-based refinements, \((|g,c_{t},P_{t-1})\) and \((|g,c_{t},P_{t-1})\), in the system.

_Implicit Closed-Loop Systems._ After each step of interaction with the environment, implicit closed-loop systems will maintain the initial plan (i.e., \(P_{t}=P_{0}\)) and only modify a single action based on the feedback. Therefore, the feedback-based refinement is defined as \((a^{}_{t}|g,c_{t},P_{0})\), where \(a^{}_{t}\) is the modified action from action space, while the remaining actions \(a_{>t}\) for future steps remain the same as the initial plan. Although locally-optimal actions are adopted at each step, inaccuracies in initial planning can result in task failure or non-completion.

_Explicit Closed-Loop Systems._ Explicit closed-loop systems refine the entire plan based on environment feedback following the policy \((P_{t}|g,c_{t},P_{t-1})\), where \(P_{t}(^{T-t})\) is the refined plan at time step \(t\) containing the modified future actions \(a^{}_{ t}\) to execute and \(P_{t-1}\) is the old plan modified in the previous time step. Allowing for constant refinement and improvement of the plan, explicit closed-loop systems can help prevent costly mistakes or missed opportunities that might arise from adhering to outdated plans. Our proposed AdaPlanner is an explicit closed-loop system.

Figure 1: A comparison between open-loop, implicit closed-loop, and explicit closed-loop systems.

## 3 AdaPlanner

Model Architecture.Our AdaPlanner model, shown in Figure 1, consists of two main components:

* an LLM-based agent that functions dually as a planner and a plan refiner, and
* a skill memory module designed to enhance sample efficiency through skill discovery.

The LLM-based agent, in its planner role, generates a comprehensive plan and performs preliminary assessments to determine its feasibility. The initial planning is modeled as \(_{0}(P_{0}|g,o_{1})\). As the plan unfolds, the agent also operates as a refiner, conducting feedback-based refinement in both in-plan and out-of-plan manners. In-plan and out-of-plan refinement processes primarily differ in how they impact future actions. In-plan refinement is a one-step action that integrates useful information into the existing plan for better action grounding. After this in-plan phase, future actions will be generated using the updated context \((a^{}_{>t}|g,c_{>t}\{h_{t}\},P_{0})\), where \(h_{t}\) represents the new information obtained from \(c_{t}\) via in-plan refinement at timestep \(t\). Out-of-plan refinement, on the other hand, leverages environmental feedback to directly revise the entire plan, denoted as \((P_{t}|g,c_{t},P_{t-1})\). This mechanism allows for comprehensive adjustments to be made to the plan in response to unexpected environmental feedback. Skill memory serves as a repository, archiving past successful plans and their respective interactions with the environment. If the agent encounters a task resembling the skills stored in memory, these skills can serve as few-shot exemplars in the LLM agent's prompt. This feature improves not only sample efficiency but also reliability for future planning.

Environment Interaction.AdaPlanner employs adaptive closed-loop planning and active environment interaction for task solving. It can anticipate environmental observations and proactively refine the plan only when there is a discrepancy between expected and actual outcomes. This is achieved by decomposing the planning process into \(N\) manageable sub-goals. During the planning and action-taking process, the agent selects from a set of timestamps, \(\{t_{1},,t_{N}\}\), to evaluate the success of each sub-goal. If the sub-goal does not align with the planned prediction at timestep \(t\{t_{1},,t_{N}\}\), the environment actively sends the previous sub-trajectories \((o_{1},a^{}_{1},,o_{t},a^{}_{t},o_{t+1})\) back to the refiner for plan revision. This process allows the agent to check the success status only at \(N\) crucial points, thereby reducing computational costs (number of API calls) and enhancing efficiency.

### Plan Generation via Code-Based LLM Prompting

AdaPlanner plans and refines by using Pythonic code prompts for LLMs. Consistent with previous observations [3; 2], we have found that using code prompts instead of natural language prompts for LLMs reduces ambiguity and misinterpretation, which significantly reduces LLM hallucination during plan generation and refinement. We design code prompts during different stages of decision-making, including adaptive planning, feedback generation, and in-episode refinement. We provide a detailed description of the prompts used at each stage in Appendix D.

To generate an initial plan for solving a given task, we input a task description, the permissible actions in the environment, and, when available, sample demonstrations of task resolution into LLM. These pieces of information are all formatted into Pythonic code format for LLM prompting. Figure 2 (a) shows an example programming-based plan generated by AdaPlanner for solving a put task in the ALFWorld environment. The generated solution function is provided with two input arguments: the first is the agent object, which encapsulates environmental information to be used by the agent. The second is the variable start_from, which is a parameter indicating the subgoal from which the agent will later resume its execution with a refined plan. By default, the start_from is initialized as 1. The value of this variable can be further reassigned during the refinement.

When prompting LLM to generate the code-based plan, we design the prompt to teach LLM to decompose a complex task into sub-goals. As shown in Figure 2(a), the generated code plan solution(agent, start_from=1) consists of: 1) a general plan at the outset that decomposes the task into subgoals in the form of comments; and 2) a sequence of sub-plans, each consisting of admissible actions corresponding to a specific subgoal. Such a mechanism allows our method to handle complex, long-horizon tasks by hierarchically decomposing them into sequences of subgoals. Furthermore, each subgoal ends with an assertion statement to test its fulfillment, which allows our method to interact actively with the environment and later resume its execution with a refined plan.

### Adaptive Closed-Loop Plan Refinement

Once an initial plan is generated, AdaPlanner then prompts the LLM to correct any syntax errors. After this, the code undergoes execution through the environment interface. The interface is responsible for grounding the actions in the environment, and also for routing environmental observations back to the code as a return value. This bi-directional flow allows AdaPlanner to adapt and refine its plan in response to environmental observations in a closed-loop manner.

**In-Plan Feedback and Refinement via ask_LLM() Action.** When AdaPlanner observes that the environment is aligned with the anticipated plan, it performs in-plan refinement. This allows it to extract useful information from the observation that can be used for upcoming actions. To achieve this, we provide the agent with an additional action called ask_LLM(), which is used to formulate a plan alongside task-specific actions. The ask_LLM() function enables AdaPlanner to self-query and perform reasoning based on specific information parsed from environmental observations. For instance, in [Step 3] in Figure 2 (a), the ask_LLM() action extracts the identifier of the found object lettuce from the natural-language observation. This information can then be fed into later actions. As an additional atomic action, this in-plan refinement is integrated into the plan at any point where the planner deems a reasoning process is necessary. Existing code-generation-based methods [22; 9; 3] face a challenge in this task, especially when there is no prior knowledge of the structure and organization of these feedback sentences. In contrast, our AdaPlanner method leverages LLM to parse critical information from diverse feedback presented in natural-language sentences to streamline plan execution.

Figure 2: An illustrative example from ALFWorld to show the proposed adaptive closed-loop planning through code. The task is to put some clean lettuce on the diningtable. The _in-plan feedback_ in (a) is a sentence like On the countertop 2, you see a knife 1, a lettuce 1, a saltshaker 2, and a soapbottle 1. This feedback is managed by the ask_LLM() action. The execution of the initial plan might yield misaligned observations, triggering an out-of-plan feedback and refinement process. For instance, the agent cannot clean the lettuce if it is not currently located at a sinkbasin. The _out-of-plan feedback_ in (b) assists AdaPlanner in generating a revised plan (c) so that the agent will move to a sinkbasin before cleaning the lettuce. AdaPlanner then determines to resume from step 3 within the same episode. The task can be successfully completed using the refined plan.

**Out-of-Plan Refinement with the Refine-Then-Resume Mechanism.** After each sub-plan execution, AdaPlanner actively checks an assertion condition to ensure that the current plan is proceeding as expected. If the assertion fails, AdaPlanner performs out-of-plan refinement. For example, in Figure 2 (a), after [Step 3], the agent is expected to hold lettuce. If this condition is not met, AdaPlanner generates an error message that details the current progress of execution gathered by the report() function. In ALFWorld tasks, this function provides a report of the agent's location, the object it is holding, and the last three interactions with the environment, as shown in Figure 2 (b). AdaPlanner then utilizes this information to perform out-of-plan refinement.

During the out-of-plan refinement as in Figure 2 (c), AdaPlanner uses a prompt similar to the one used during the initial planning stage, but with an additional feedback message that reflects the current state. Detailed prompts are provided in Appendix D. AdaPlanner then refines the plan based on the newly acquired information and also determines the value of start_from by comparing the plan before and after the refinement. The newly refined solution() is then executed from the breakpoint start_from. This breakpoint contains all variable states that were saved prior to refinement. Consequently, the current episode can continue from an intermediate checkpoint without restarting from scratch. We call this mechanism _refine-then-resume_. It significantly speeds up task completion and reduces the number of LLM calls required.

### Skill Discovery

Acquiring expert demonstrations for task solving can be costly, particularly as the number of tasks increases. To address this issue, we have equipped AdaPlanner with a skill discovery feature. This is a memory scheme that discovers and archives successful trajectories, thereby improving planning performance when dealing with similar tasks. The skill discovery process consists of two stages, which can be conducted alternately over several rounds, based on the interaction costs and computation resources.

**Skill Acquisition.** In the first stage, AdaPlanner attempts to solve unseen tasks, leveraging a limited number of human demonstrations of other simpler tasks, or even no demonstrations. The model capitalizes on adaptive closed-loop planning to iteratively explore and refine solutions via a trial-and-error approach. Upon successful completion of a given task, the latest solution and the corresponding interactions are treated as candidate discovered skills.

**Skill Filtering.** In the second stage, we compare the planning performance with and without the integration of the discovered solution into the prompt. If the inclusion of this solution boosts the success rate, it is archived as a discovered skill. Conversely, if it does not improve performance, it is discarded. This filtering stage is crucial because the iterative closed-loop refinement may integrate episode-specific information into the revised solution, potentially compromising its generalizability.

## 4 Evaluation

We test AdaPlanner on two text-based decision-making environments: 1) **ALFWorld** is a text-based virtual household environment encompassing six distinct task types set. We evaluate AdaPlanner on a total of 134 tasks across these six types. 2) **MiniWoB++** is a simulation environment that covers a large range of computer tasks. We select 9 MiniWoB++ tasks with environmental feedback, and we also adopt and test the 53 tasks evaluated in RCI . Both environments aim to solve complicated challenges with long-horizon solutions and sparse rewards. We also carefully designed ablation studies to justify the significance of each component in AdaPlanner. The Setup details and prompts for AdaPlanner are depicted in Appendix A and D. Detailed introductions to each baseline are presented in Appendix B Note that we evaluate different baselines for these two benchmarks. These methods utilize task-specific samples for prompting or training purposes, thus necessitating separate evaluations for each benchmark.

**Metrics.** Consistent with previous works [20; 25; 19; 7; 4; 11; 8], we use success rate (%) to evaluate the performance of tested methods. The success rate is defined as the number of successful episodes over the total number of episodes. Note that in ALFWorld, failure of an episode occurs when the total number of actions attains 50, with the task still unsolved. In MiniWoB++, failures can occur in two scenarios: either due to the execution of invalid actions or if the task remains unfinished following the execution of the entire plan.

**Main Results.** AdaPlanner consistently outperforms the existing baselines, achieving state-of-the-art performance, _i.e._, an overall success rate of 91.79% in ALFWorld tasks (Table 2) and 91.11% inMiniWoB++ tasks with feedback (Table 3). Specifically, in ALFWorld, AdaPlanner equipped with GPT-3 achieves a remarkable success rate exceeding 95% in the majority of individual tasks. It also surpasses all other baselines in the Pick, Clean, and Examine tasks. Notably, even in the task with the lowest performance (Pick two), AdaPlanner still outperforms BUTLER and ReAct. In the MiniWoB++ environment, AdaPlanner demonstrates superiority over all other methods on tasks that provide feedback. This superior performance suggests that AdaPlanner effectively leverages feedback to refine its plans and enhance its performance. Furthermore, AdaPlanner maintains competitive performance on tasks without feedback, achieving a success rate of 93.22%. Note that AdaPlanner's success rates of tasks without feedback are still comparable to CC-Net, the state-of-the-art model requiring over 23,000 samples per task. This result highlights the efficacy of the programming-based planning strategy employed by AdaPlanner. In both environments, AdaPlanner consistently delivers superior or competitive performance when compared to not only training-based methods but also implicit closed-loop methods under the same LLM models. These results affirm the effectiveness of the proposed explicit closed-loop plan refinement in AdaPlanner.

Furthermore, we summarize the relationship between success rate (%) and the number of samples in Figure 3. In ALFWorld, AdaPlanner yields the highest performance with the fewest number of samples. In MiniWoB++, our method outperforms most baselines. Notably, our method achieves performance comparable to CC-Net but requires 600 times fewer samples. This study highlights that AdaPlanner significantly reduces the need for extensive demonstrations or expert trajectories, thereby offering a more resource-efficient solution.

**Adaptive Closed-Loop Architecture Enhances Planning Performance.** Figure 3(a) shows the performance v.s. the number of closed-loop refinements, under settings with different numbers of demo samples. The detailed example selection for this study is provided in Appendix A. We observe a significant trend of increased success rates corresponding to each subsequent closed-loop plan refinement. This indicates the AdaPlanner's ability to consistently leverage real-time feedback for performance enhancement, regardless of the number of samples used. Remarkably, AdaPlanner

   Method & Pick & Clean & Heat & Cool & Examine & Pick two & All (134 tasks) \\   \\  BUTLER  & 46.00 & 39.00 & 74.00 & **100.00** & 22.00 & 24.00 & 37.00 \\   \\  ReAct  (GPT-3) & 66.67 & 41.94 & 91.03 & 80.95 & 55.56 & 35.29 & 61.94 \\ ReAct  (GPT-3.5) & 37.50 & 64.52 & 69.57 & 42.86 & 38.89 & 17.65 & 47.76 \\ Reflexion  (GPT-3 + 3.5) & 75.00 & 90.32 & 91.30 & 90.48 & 88.89 & **94.12** & 88.06 \\ Reflexion  (GPT-3.5) & 50.00 & 41.94 & 65.22 & 52.38 & 66.67 & 47.06 & 52.99 \\   \\  AdaPlanner (GPT-3) & **100.00** & **96.77** & **95.65** & **100.00** & **100.00** & 47.06 & **91.79** \\ AdaPlanner (GPT-3.5) & 77.78 & 93.55 & 69.57 & 93.65 & 62.96 & 78.43 & 80.60 \\   

Table 2: Success rate (%) of tested methods on six ALFWorld tasks. For ReAct and AdaPlanner, GPT-3.5 refers to gpt-3.5-turbo, while GPT-3 represents text-davinci-002. For Reflexion, GPT-3.5 indicates gpt-3.5-turbo. GPT-3+3.5 is used in the original Reflexion implementation, which utilizes both GPT-3 (text-davinci-002) and GPT-3.5 (text-davinci-003) for action generation and failure reflection, respectively. AdaPlanner is prompted with one specific example per task, making up six demonstrations in total. This is _half the number of samples_ used in React and Reflection. The best-performing results are marked in bold. The results of our method are colored in gray.

Figure 3: Relationship between success rate (%) and the number of expert demonstrations in ALFWorld and MiniWoB++ environments. We adopt the same settings as in Table 2 (GPT-3 version) and Table 3. The top-left corner represents the pinnacle of sample efficiency.

maintains this trend of success rate enhancement even when the total number of demonstrations across all six tasks is as low as two. In addition, as displayed in Table 4, AdaPlanner consistently outperforms both ReAct and Reflexion in producing the shortest trajectory lengths across all tasks in ALFWorld. Moreover, a comparison with Reflexion, depicted in Figure 3(b), shows AdaPlanner's consistently superior performance across all iterations of closed-loop corrections. These observations highlight AdaPlanner's sample efficiency and its potential for real-world applications where the number of available demonstrations is limited.

**Code Interface Mitigates Hallucination.** The latest gpt-3.5-turbo is reported to be the most capable GPT-3.5 model while reducing the cost by a tenth compared to other prevailing GPT-3  and 3.5 models  (_e.g._, text-davinci-002 and text-davinci-003.) However, our findings from Table 2 indicate that gpt-3.5-turbo underperforms in decision-making tasks relative to its predecessors, i.e., text-davinci-002, in all LLM-agents. Upon examination of trajectories from both models, we observed a noticeable hallucination with GPT-3.5 (gpt-3.5-turbo), as shown in Appendix E. We hypothesize that gpt-3.5-turbo might be a smaller-scale model that is more prone to hallucination. We also found a similar hypothesis drawn based on various experiments in . Furthermore, gpt-3.5-turbo is primarily optimized for human conversation tasks, which could potentially compromise its performance on tasks such as code generation and reasoning. Despite this, AdaPlanner demonstrates a remarkable level of resilience against hallucination even with gpt-3.5-turbo (Table 2), while ReAct and Reflexion are more sensitive to the hallucination issue. AdaPlanner's resilience against hallucination can be attributed to its use of code prompts, which provide a more formal and constrained generation space for LLM. For comparison, we implement an ablation version of AdaPlanner without the code interface by translating solution examples directly into plans and actions using natural language. Without the code interface, AdaPlanner's performance substantially drops in both ALFWorld and MiniWoB++ environments (Figure 3(c)), from 81% to 46% and from 93% to 66%, respectively. This significant performance drop underscores the essential role of the code interface in AdaPlanner.

   Method & Pick & Clean & Heat & Cool & Examine & Pick two & All (134 tasks) \\  ReAct & 19.55 & 25.79 & 19.7 & 27.86 & 29.72 & 36.29 & 25.81 \\ Reflexion & 18.77 & 25.25 & 19.13 & 25.76 & 27.11 & 35.76 & 18.90 \\  AdaPlanner & **10.79** & **13.45** & **17.61** & **13.33** & **21.00** & **20.71** & **15.60** \\   

Table 4: Average trajectory length (# steps) per task for ReAct, Reflexion, and AdaPlanner. We adopt the same settings as in Table 2 (GPT-3 version). Following the setting in , each episode will be terminated after reaching 50 steps.

   Method & With feedback (9 tasks) & No feedback (44 tasks) & All[53 tasks) \\   \\  CC-Net  & 87.00 & **95.66** & **94.00** \\ WGE  & 67.60 & 87.93 & 86.00 \\   \\  WebN-T5-3B  & 38.50 & 54.67 & 52.00 \\   \\  RCI  & 81.56 & 92.68 & 91.00 \\   \\   AdaPlanner & **91.11** & **93.22** & **92.87** \\   

Table 3: Success rate (%) of tested methods on two subsets of tasks in the MiniWoB++ environment. RCI and AdaPlanner harness GPT-3.5 (gpt-3.5-turbo and text-davinci-003) as backends. Our AdaPlanner method is provided with 38 human-written demonstrations; then, it automatically discovers 21 additional examples via skill discovery, which makes up the final set of 59 examples for 53 tasks. This is _around half the number of samples_ used in RCI and _over one six hundredths of the number of samples_ used in CC-Net. The best-performing results are marked in bold. The results of our AdaPlanner are colored in gray. Per-task success rates are provided in Appendix F.

**Skill Discovery Improves Sample Efficiency.** The skill discovery in AdaPlanner utilizes a long-term memory mechanism that retains successful solutions, thus boosting planning performance when faced with similar tasks. An ablation study depicted in Figure 3(d) compares the performance of AdaPlanner with and without the implementation of skill discovery. In the skill acquisition stage, we provide a maximum of one demonstration. In ALFWorld, AdaPlanner is prompted with only one expert demonstration of the simplest task (put). We evaluate the average success rate of the method on the remaining five tasks, which are comparatively more challenging and require additional steps for completion. In MiniWoB++, we apply zero-shot prompting, omitting any examples in the skill acquisition phase. For both environments, we operate the method using GPT-3.5 in adaptive closed-loop mode, and one round of skill discovery is conducted. As Figure 3(d) illustrates, the inclusion of skill discovery significantly enhances performance. In the ALFWorld tasks, the success rate of AdaPlanner nearly doubles when skill discovery is employed. Similarly, in the MiniWoB++ tasks, the overall success rate increases by approximately 15% with skill discovery. Moreover, we conducted an ablation study on MiniWoB++ to evaluate the impact of sample numbers, as detailed in Table 5. AdaPlanner with skill discovery requires only 15 samples to outperform the variant without skill discovery, even though the latter used twice as many samples. It is evident that skill discovery enhances sample efficiency.

## 5 Related Work

Many works have studied how to leverage LLMs as autonomous agents to accomplish decision-making tasks within text-based environments. Earlier studies, like Chain-of-Thoughts  and Zero-Shot Planner , utilize prompts to guide LLMs in generating complete action sequences for elementary tasks. For more complex tasks, methods like HuggingGPT  and Chameleon  also generate the initial plan of using different tools and then call the corresponding APIs for execution. Meanwhile, some other works also prompt LLMs to compose plans in Planning Domain Definition Language (PDDL) [10; 15]. However, all these plans are created in an open-loop fashion without adapting to feedback from external environments.

To address the limitations of open-loop systems, recent techniques have emerged that focus on establishing closed-loop systems. These systems are capable of leveraging environmental feedback, thereby facilitating more adaptive decision-making. ReAct  and Inner Monologue  allow LLM agents to take single-step actions according to the environmental feedback. Reflexion , as an extension of ReAct, tries to resolve this issue by enabling the ReAct agent to revise itself from past trials and errors. Moreover, RCI  starts by formulating a comprehensive plan, modifying the immediate action when the agent encounters a failure at the current step. While all the aforementioned

Figure 4: Performance comparison on 134 ALFWorld tasks in different cases. We adopt the same settings as in Table 2. (a) and (b) presents the success rate (%) with different numbers of closed-loop corrections: (a) compares AdaPlanner with different numbers of samples; (b) compares AdaPlanner and Reflexion with two LLMs. (c) shows the success rate (%) of AdaPlanner with and without code interface (CI). (d) shows the success rate (%) of AdaPlanner with and without skill discovery (SD). Note that for (a), the number signifies the total number of samples used across all six tasks.

   \# samples & 38 & 30 & 20 & 15 & 0 \\  With SD & 92.87 & 84.06 & 79.17 & 75.17 & 60.38 \\ Without SD & 82.40 & 73.58 & 68.70 & 64.70 & 45.47 \\   

Table 5: Success rate (%) of AdaPlanner on MiniWob++ with and without skill discovery (SD) across varying numbers of expert samples per task. We adopt the same settings as in Table 2 (GPT-3 version) and Table 3.

methods can adapt their decisions based on environmental feedback, they assume the LLM-generated initial plan is correct and do not adjust it. Rather, they solely modify the immediate action being executed and are easy to fall into local sub-optimal actions without considering the long-term plans.

To further enhance the agents' both capabilities of planning and adapting to environmental feedback, strict closed-loop architectures are proposed that can recursively refine the generated plans. DEPS  is one of the examples that initially proposes an entire plan and then applies real-world feedback to recursively refine it during execution. However, this method requires training a selector to generate a plan that is highly probable to succeed, which makes it difficult to generalize the plans and actions to other tasks. Besides, the required data for training the plan selector are often unavailable in practice and expensive to collect. In contrast, AdaPlanner generates and refines plans via LLM prompting, making it widely applicable to various decision-making problems. Another PDDL-based planning method is proposed in  with the ability to leverage feedback and modify the plans. However, the plan formulated on the training set remains static and lacks task-specific refinement during the execution of the evaluation task. In contrast, AdaPlanner dynamically refines the plan and adapts to various feedback throughout the task-solving process.

## 6 Conclusion and Limitations

We proposed AdaPlanner, a closed-loop approach enabling LLM agents to adaptively refine their generated plans according to environment feedback. We defined two different refinement strategies, in-plan and out-of-plan refinement, to fully leverage environment information. Furthermore, to mitigate the LLMs' hallucination issue and make them learn from the past experience, we proposed code-style prompting and skill discovery mechanisms. Through comprehensive experiments, we demonstrated that AdaPlanner outperforms the state-of-the-art baselines significantly and has better sample efficiency. Our ablation studies also showed the effectiveness of different components in AdaPlanner. One limitation of AdaPlanner is that it still require few-shot expert demonstrations for solving complex tasks. Although AdaPlanner has already achieved better sample efficiency than existing methods, it is interesting to study how to further enhance AdaPlanner to solve complex tasks with no demonstrations in the future.

## 7 Broader Impacts

Our research approach focuses on treating LLMs as autonomous agents and improving their ability to solve complex sequential decision-making tasks. However, this research line carries inherent risks, including security threats, potential misuse, and unintended consequences such as job displacement due to automation. To mitigate these risks, it is essential for researchers and policymakers to collaborate in creating and implementing effective regulations to guide the development and deployment of these technologies towards positive outcomes. Additionally, we believe that the research community should coordinate efforts to design principles and techniques that prioritize safety and human values before LLM agents are deployed in various industries. This will help ensure that LLMs are aligned with ethical and moral standards while promoting their positive impact on society.