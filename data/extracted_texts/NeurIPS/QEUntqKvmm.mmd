# The surprising efficiency of temporal difference learning for rare event prediction

Xiaoou Cheng

Courant Institute of Mathematical Sciences

New York University

New York, NY 10012

chengxo@nyu.edu &Jonathan Weare

Courant Institute of Mathematical Sciences

New York University

New York, NY 10012

weare@nyu.edu

###### Abstract

We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events. Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for _relative accuracy_ in estimates of very small values. Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC. We prove a central limit theorem for the LSTD estimator and upper bound the relative asymptotic variance by simple quantities characterizing the connectivity of states relative to the transition probabilities between them. Using this bound, we show that, even when both the timescale of the rare event and the relative error of the MC estimator are exponentially large in the number of states, LSTD maintains a fixed level of relative accuracy with a total number of observed transitions of the Markov chain that is only _polynomially_ large in the number of states.

## 1 Introduction

Prediction of the future behavior of a dynamical system from time series observations is a fundamental task in science and engineering. One need only remember the weather forecast consulted this morning to appreciate the substantial intersection this problem has with our everyday lives. Beyond its own obvious importance, prediction is also a key component in modern machine learning tasks like reinforcement learning, where the rules governing the dynamical system are adjusted to optimize some reward. Exciting applications in that field, like self-driving cars, have captured public imagination.

Prediction is made particularly difficult when the future behavior of interest involves some rare or extreme event. Examples include extreme weather or climate events, failure of reliable engineering products, and the conformational rearrangements that determine critical functions of biomolecules in our bodies. Precisely because of their out-sized impact on our lives and society, rare and extreme events are the most important to predict. Unfortunately, by definition, data sets often contain relatively few examples of these events. In this article we ask: _can rare events be predicted accurately with time series data sets much shorter than the typical timescale of the event?_ The surprising answer is Yes. Our mathematical results support recent studies demonstrating accurate rare event prediction with limited data using methods related to those studied here, including in state-of-the-art application settings and for very rare events (Thiede et al., 2019; Strahan et al., 2021; Finkel et al., 2021; 2023; Antoszewski et al., 2021; Lucente et al., 2021; Finkel et al., 2023; Strahan et al., 2023; Jacques-Dumas et al., 2023; Strahan et al., 2023; Guo et al., 2024).

Within the context of reinforcement learning, "prediction" refers to policy evaluation, that is, calculation of the value function of states given a policy (Sutton and Barto, 2018). Prediction is thefoundation for policy improvement in control problems. In this work, we focus on prediction for Markov chains on a finite state space. While many dynamical systems of physical interest evolve in continuous spaces, this so-called "tabular" setting is the most common for mathematical analysis. Moreover, some practical algorithms for prediction problems in continuous space begin with a projection onto a finite state Markov process (Thiede et al., 2019; Finkel et al., 2021, 2023; Jacques-Dumas et al., 2023). For those algorithms our analysis can be viewed as addressing estimation (but not approximation) error.

Specifically, we formulate a prediction problem in terms of a Markov reward process (MRP) \(([n],P,R)\), where \([n]=\{1,2,,n\}\) is the state space, \(P\) is the Markov transition kernel with \(P(i,j)=[X_{t+1}=j\,|\,X_{t}=i]\), and \(R\) is a deterministic, non-negative reward function. Time dependence and randomness of \(R\) can be included by an enlargement of the state space. Our goal is to estimate a value function of the general form1

\[u(i)=_{i}[_{t=0}^{T}R(X_{t})],\] (1)

where \(D\) is a subset of \([n]\), \(T=\{t 0:\,X_{t} D\}\) is the escape time from \(D\), and the subscript \(i\) on the expectation indicates that \(X_{0}=i\). By appropriate choice of \(R\), we can consider both cases in which the escape time from \(D\) is very large and cases in which only low probability escape paths accumulate significant reward. These are the typical situations for rare event statistics, where the reward \(R\) and the set \(D\) are specified by the application. In the example of self-driving cars, when investigating the rare event that the car crashes, \(D\) is the set of all states in which the car has not crashed.

### Monte Carlo and temporal difference learning

The prediction problem has been extensively analyzed in the reinforcement learning literature (Sutton and Barto, 2018; Dann et al., 2014). Our problem's main point of departure from that literature is the central role of the escape event from \(D\) in (1). We are specifically interested in cases in which \(_{j}[T]\) can be very large so that the length of a single escape trajectory can be very large, and/or cases in which large values of the reward functions are very unlikely so that \(u\) can be very small.

Our task is to learn an estimate of \(u\) from a set of \(M\) samples \(\{X_{0}^{},X_{1}^{},,X_{ T^{}}^{}\}_{=1}^ {M}\) of \(X_{0},X_{1},,X_{ T}\) with \(\{X_{0}^{}\}_{=1}^{M}\) drawn from some (possibly unknown) initial probability distribution \(\) which is supported in \(D\). Here and below we use the shorthand \(a b=\{a,b\}\). The deterministic positive integer \(\), which we will refer to as a lag time, limits the length of the trajectories in the data set. In practical applications, the sample trajectories are often correlated, for example because multiple length \(\) trajectories can be harvested from a single longer trajectory. Here, to simplify the analysis, we assume that the trajectories are drawn independently.

The most direct estimator of \(u(i)\) is the Monte Carlo (MC) estimator

\[(i)=}_{=1}^{M_{i}}_{t=0}^{T^{}}R(X_ {t}^{}),M_{i}>0,\]

where \(M_{i}\) is the number of samples with \(X_{0}^{}=i\), i.e. \(M_{i}=_{=1}^{M}_{\{i\}}(X_{0}^{})\). The estimator is undefined for \(i D\) with \(M_{i}=0\). The MC estimator does not allow finite values of \(\) (unless \(T\) is bounded by a constant). When the escape event is rare, \(T\) is very large and the MC estimator requires data sets containing very many observed time steps of \(X_{t}\). Moreover, estimators of rare event statistics are evaluated based on the amount of data required to achieve a desired _relative error_(Bucklew, 2004). In the rare event setting, \(u(i)\) can be extremely small because of the low probability of the event to happen. In this case, assuming \((i)>0\), control of the relative variance, \([(i)]/u^{2}(i)\), often requires \(M 1/u(i)\) trajectory samples for the MC estimator. This scaling is fatal when \(u(i)\) is very small.

Temporal difference (TD) schemes, on the other hand, express \(u\) as the solution to a certain Bellman equation (Sutton, 1988). They enforce temporal local consistency of the estimate of the value function.

As such, TD methods can accommodate _any_ positive choice of \(\), while MC methods require that each trajectory be observed until escape from \(D\) (i.e. \(=\)). TD schemes are used heavily in reinforcement learning and they are often observed to out-perform MC (Sutton and Barto, 2018). However, a quantitative understanding of the advantages of TD compared to MC remains elusive. In practice, there are several variants of TD schemes: parametric or tabular, online or batch. In this work, we stick to the simplest setting: the least-squares TD (LSTD) estimator (Bradtke and Barto, 1996) in the tabular setting of a finite state MRP. Our goal is to clearly characterize the benefit of the LSTD estimator over MC for predictions related to rare events in this setting.

TD schemes begin with the observation that, for any choice of \(\), \(u\) in (1) solves the linear system

\[(I-S^{})u(i)=_{t=0}^{-1}S^{t}R_{D}(i)i D u(i)=R(i)i D\] (2)

where

\[S(i,j)=P(i,j)i D S(i,j)=_{ij}i D,\]

is the transition operator of \(X_{t T}\), \(S^{}\) is the \(\)th power of \(S\) and \(R_{D}(i)=R(i)\) for \(i D\) while \(R_{D}(i)=0\) for \(i D\). In the limit \(\), (2) becomes (1). In our theory and examples, we will see that the choice of \(\) can have a dramatic effect on the performance of the estimator.

TD methods use trajectory data to find an approximate solution to (2). Specifically, the LSTD estimator \(\) in this tabular setting is the solution to a linear system where the transition matrix \(S\) in (2) is approximated with trajectory data. We set \(^{0}=I\) and, for \(t=1,2,,\),

\[^{t}(i,j)=}_{=1}^{M}_{\{(i,j)\}} (X_{0}^{},X_{t T^{}}^{})M_{i}>0^{t}(i,j)= _{ij}M_{i}=0.\]

Unlike \(S^{t}\), \(^{t}\) are not powers of a single matrix. When it exists, the \(\)-step LSTD estimator, \(\) solves

\[(I-^{})(i)=_{t=0}^{-1}^{t}R_{D}(i) { for }i D(i)=R(i)i D.\] (3)

(Like the MC estimator, \((i)\) is undefined when \(M_{i}=0\)). In the \(\) limit, the \(\)-step LSTD and MC estimators are equivalent. We therefore occasionally refer to the Monte Carlo estimator as the \(=\) estimator.

### Our contributions

We pause now to consider what classical perturbation theory for linear systems has to say about the feasibility of estimating \(u\), the solution of (2), by \(\), the solution to (3). Clearly (3) only requires access to trajectories of length \(\), but how sensitive is the solution of (2) to perturbations in \(S\)? If it is very sensitive then any reduction in data requirements from shorter trajectories may be offset by a need for many more trajectories. Classical perturbation theory tells us that errors \(S^{t}-^{t}\) can, in the worst case, be amplified in \(u-\) by a factor of the condition number \(^{}=\|(I-S_{D}^{})^{-1}\|\|I-S_{D}^{}\|\), where \(S_{D}^{}\) is the matrix obtained from \(S^{}\) by setting to zero any row or column with index in \(D^{c}\)(Demmel, 1997). As the next lemma shows, when typical values of \(T\) are much larger than \(\), \(\|(I-S_{D}^{})^{-1}\|\) will be very large. The proof of Lemma 1 is in Appendix A.

**Lemma 1**.: _For any consistent matrix norm, if the restriction of \(S_{D}\) to row and column indices in \(D\) is irreducible and aperiodic, then \(\|(I-S_{D}^{})^{-1}\|_{}[T]/\) where \((i)=_{t}[X_{t}=i\,|\,T>t]\) is the quasi-stationary distribution and the subscript on the expectation indicates that \(X_{0}\) is drawn from \(\). (See Collett et al. (2012) for more about the quasi-stationary distribution)._

While it is possible that a small value of \(\|I-S_{D}^{}\|\) can compensate for a large value of \(_{}[T]/\) and result in a moderate condition number, for many rare event problems this is not the case. For example, for the system studied in our numerical experiments in Section 3, \(\|I-S_{D}\|\) is bounded away from zero even as the escape event becomes increasingly rare (see Appendix B).

Thus it would seem that we are forced to choose between two doomed options: choose \(\) small to control the length of trajectories we need to observe but observe a huge number (\(M_{}[T]/\)) of them to drive down the error in \(^{t}\), or observe very long trajectories (\(_{}[T]\)) to control \(^{}\). With either of these choices we would not expect TD to significantly outperform MC.

A primary goal of this article is to explain that, in many (perhaps most) cases, this is a false choice. The worst case analysis that characterizes the classical perturbation theory is, by design, pessimistic. But as we will explain, in our setting it is wildly pessimistic. With a few simple and practically relevant assumptions we will be able to show that \(\)-step LSTD can achieve remarkably accurate estimates with remarkably little data.

Throughout this paper we quantify the relative accuracy of \((i)\) by the _relative asymptotic variance_\(_{i}^{2}/u^{2}(i)\) where \(_{i}^{2}\) is the variance of the limiting normal distribution in a central limit theorem for \((i)\) established in Theorem 1. Also in Theorem 1, we provide an upper bound on the relative asymptotic variance by simple quantities characterizing the connectivity of states relative to the transition probabilities between them. Crucially, neither the condition number \(^{}\), nor any expectation of \(T\), appear explicitly in the bound. Next, we turn our focus to the rare event setting, which we characterize via the large \(n\) behavior of the estimator. In this setting both the typical escape time and the relative variance of the MC estimator can scale exponentially with \(n\). As we show in Theorem 2, however, the relative asymptotic variance of the LSTD estimator scales, at worst like \(n^{3}\).

### Related work

Since it first appeared in (Sutton, 1988), there have been numerous variants of TD (Bradtke and Barto, 1996; Dann et al., 2014; Lillicrap et al., 2019). Early theoretical analysis of TD focused on asymptotic convergence with linear value function approximation (Jaakkola et al., 1993; Dayan and Sejnowski, 1994; Tsitsiklis and Van Roy, 1997), along with examples of divergence (Baird, 1995; Tsitsiklis and Van Roy, 1997). More recently, nonasymptotic convergence analysis of TD has been performed (Bhandari et al., 2018; Dalal et al., 2018; Srikant and Ying, 2019; Cai et al., 2019).

Besides its own convergence, a long-considered question for TD is when and how it outperforms MC. While practical experience suggests that TD is more efficient than MC, a comprehensive theory is lacking. Grunewalder et al. (2007) and Grunewalder and Obermayer (2009) proved that LSTD is at least as statistically efficient as MC, but the improvement is not quantified. Cheikhi and Russo (2023) expresses the relative benefit of TD over MC by the inverse trajectory pooling coefficient, which facilitates interpretation of the ratio of asymptotic variances. But how that translates to quantitative improvements requires further elucidation. As far as we are aware, prediction for rare event problems has not been analyzed in the reinforcement learning literature.

TD enforces temporal consistency for trajectory data. Trajectory data has been utilized in applications more broadly. Markov state models (Husic and Pande, 2018) and dynamic mode decomposition methods (Schmidt, 2022) were developed in the molecular dynamics and fluid dynamics communities respectively and have been successful at estimating eigenvectors and eigenvalues of Markov transition (or Koopman) operators from trajectory data in state-of-the-art applications. The dynamical Galerkin approximation (Thiede et al., 2019) method extends those approaches to the prediction problem studied here and has been used to study rare events in molecular dynamics and climate science (Strahan et al., 2021; Antoszewski et al., 2021; Finkel et al., 2021, 2023; Guo et al., 2024). Despite widespread application in the study of various rare events over several decades, the present article is the first theoretical evidence that trajectory analysis methods can be effective tools specifically for rare events.

### Notation

\(X}(_{0},_{0}^{2})\) indicates that a random variable \(X\) converges in distribution to a normal distribution with mean \(_{0}\) and variance \(_{0}^{2}\). For any subset \(A[n]\), we define the hitting time \(T_{A}=\{t>0:X_{t} A\}\). When \(X_{t}\) never hits \(A\), \(T_{A}=+\). When the set \(A\) contains only one state \(i\), we use \(T_{i}\) for simplicity. Note that in (1), \(T\) counts from \(t=0\) instead. While \(S\) is the transition probability matrix for the Markov chain \(X_{t T}\), we define another Markov chain \(Y_{t}^{}\) whose transition probability matrix is \(S^{}\). For \(Y_{t}^{}\), we define its hitting time to be \(T_{A}^{}=\{t>0:Y_{t}^{} A\}\). We define \(T^{}=\{t 0:Y_{t}^{} D\}\) as a counterpart of \(T\) for \(X_{t}\). We use \(e_{i}\) for the \(i\)th standard basis vector, i.e. \(e_{i}(j)=_{ij}\). For any two quantities \(a\) and \(b\), \(a b\) means that there exists a positive constant \(C\) independent of \(n\) such that \(a Cb\). Similarly, \(a b\) means that \(a Cb\).

An upper bound for relative asymptotic variance

As previous authors have done (Cheikhi and Russo, 2023), we will characterize the error of the LSTD estimator using a central limit theorem. Unlike previous work, we will also provide a simple upper bound on the relative asymptotic variance that clearly distinguishes the LSTD estimator from the MC estimator. Our subsequent analysis of the LSTD estimator in the rare event setting will be derived from this bound.

**Theorem 1**.: _Suppose \((i)>0\) for all \(i D\) and \(P\) is irreducible. Then, as the number of samples \(M\),_

\[((i)-u(i))}(0,_{i}^{2}),\] (4)

_with_

\[_{i}^{2}=_{k D}(e_{i}^{}(I-S_{D}^{ })^{-1}e_{k})^{2}_{k}[_{t=0}^{-1}R_{D}(X_{t  T})+u(X_{ T})-u(k)]^{2}.\] (5)

_Moreover, the relative asymptotic variance \(_{i}^{2}/u^{2}(i)\) satisfies the upper bound_

\[^{2}}{u^{2}(i)}_{k D,\, [n],\\  k}^{}S^{t}(k,)}{(k)\,Q^{ }(k,)^{2}}\] (6)

_where we have introduced the key quantity \(Q^{}(k,)=_{k}[T_{}^{}<T_{k}^{} T_{D ^{}\{\}}^{}].\)_

An important special case that will include one of the examples studied in Section 3, occurs when the reward function \(R\) is zero in \(D\) (but non-zero in \(D^{}\)). In this case we can strengthen the bound in Theorem 1 by only including \(S^{}\) in the numerator.

**Corollary 1**.: _Under the same assumptions as in Theorem 1 but with \(R(i)=0\) for any \(i D\),_

\[^{2}}{u^{2}(i)}_{k D,\, [n],\\  k}(k,)}{(k)\,Q^{}(k,)^{2}}\;.\] (7)

Let us take a moment to parse the bounds in Theorem 1 and Corollary 1. The bounds tell us that if the sum is bounded, our estimator achieves entry-wise _relative_ accuracy, indicating that even when \(u(i)\) is extremely small, the error in \((i)\) can be much smaller (depending only on \(M\)). But the denominator is the real star of the show. It is the sum of the probabilities of all paths connecting state \(k\) and state \(\) that do not return to \(k\) and do not exit \(D\) (except possibly through \(\)). In particular \(Q^{}(k,) S^{}(k,)\). While the probability \(Q^{}(k,)\) can be very small for most pairs \((k,)\), evidently it is the size of \(Q^{}(k,)^{2}\) relative to \(S^{t}(k,)\) for \(t\) (or only \(S^{}(k,)\)) that matters. Fortunately, in many cases, we can expect \(Q^{}(k,)^{2}\) to be _much_ larger than \(S^{}(k,)\) for a good choice of \(\). We will quantify this statement in the rare event setting in Section 4.

A quantity very similar to \(Q^{}(k,)\) appears in the matrix analysis literature (Thiede et al., 2015) as a measure of the sensitivity of the relative accuracy of the invariant distribution of a Markov chain to entry-wise perturbations in its transition probability matrix. The proofs of Theorem 1 and Corollary 1 are given in Appendix C. Lemma 2 in Appendix C, which is used to bound the variation in \(u\) between states, is key to the argument.

## 3 Experiments

We now consider two example problems that illustrate the challenges posed by rare event prediction. For both problems the statistical efficiency of the MC estimator degrades exponentially in the state space size \(n\). Our goal is to see whether or not LSTD's performance degrades similarly as \(n\) increases, and, in so doing, to motivate our final bounds on the relative asymptotic variance of LSTD established later in Section 4. The examples and numerical results presented in this section are exactly consistent with the assumptions and results presented in Section 4.

Both problems are based on a one dimensional nearest neighbor chain \(X_{t}[n]\) with transition rules

\[P(i,i 1)=P(i,i)=1-P(i,i+1)-P(i,i-1) i 1[n]\] (8)with \(P(1,0)=P(n,n+1)=0\) on the boundary. Here \(p\) is the invariant probability vector of \(P\),

\[p(i)[() ],\] (9)

which has modes around \(i=1\), \(i=(n+1)/2\), and \(i=n\), that become more sharply peaked as \(n\) increases. The escape time from any of these modes scales exponentially with \(n\). We will be interested in predictions related to the escape of \(X_{t}\) from \(D=[n]\{1,n\}\), and choose \(\) to be the uniform distribution on \(D\).

Before moving on to the specific problem statements, we point out that despite the exponentially long waiting time to transition between the modes of \(p\), transitions between nearest neighbor states remain stable as \(n\) increases. In fact, for \(i D\), \(P(i,i 1).\) Stable local transition probabilities such as these play a key role in our upper bounds in Section 4, where they are used to lower bound \(Q(k,)\) in (6).

**The mean first passage time.** The mean first passage time \(u(i)=_{i}[T]\) solves (2) with \(R(i)=1\) for \(i D\) and \(R(i)=0\) for \(i D\). We plot the mean first passage time for \(n=20,\ 40,\) and \(80\) in the left panel of Fig. 1. Evidently the largest values of \(_{i}[T]\) scale exponentially with \(n\). In Appendix B, we prove that, near \(i=(n+1)/2\),

\[_{i}[T]().\] (10)

Therefore the trajectories required by the MC estimator will include a number of transitions of \(X_{t}\) that scales exponentially with \(n\). We compare the performance of the MC estimator of the mean first passage time to that of the LSTD estimator, \(\), found by solving (3) with \(=1\). In the middle panel of Fig. 1 we plot the relative asymptotic variance, \(_{i}^{2}/u^{2}(i)\), for the same values of \(n\). In the same panel we plot empirical estimates of the the (non-asymptotic) relative mean squared error (MSE), \(M[(i)]/u^{2}(i)\), with \(M=10n^{3}\). The empirical relative MSE estimates are computed by generating 30000 independent copies of the LSTD estimator. Remarkably, the relative MSE of the LSTD estimator grows _more slowly_ than \(n^{3}\). Meanwhile, near the boundary of \(D\), the relative MSE of the MC estimator (computed exactly) grows exponentially fast with \(n\), as can be seen in the right panel of Fig. 1.

**The committor.** We now consider estimation of the committor function

\[u(i)=_{i}[T_{n}<T_{1}]i[n]\{1,n \} u(i)=_{\{n\}}(i)i\{1,n\}.\]

The committor corresponds to the choice \(R(i)=0\) for \(i n\) and \(R(n)=1\). Because \(_{i}[T]\) can be exponentially large, the MC estimator of the committor again requires a data set containing exponentially long trajectories of \(X_{t}\). Moreover, in Appendix B, we prove that \(u(i)\) can be exponentially small in \(n\), as

\[u(2)(-),\] (11)

as is evident in the plot of the committor for \(n=20,\ 40,\) and \(80\), in the left panel of Fig. 2. As a consequence, the relative MSE of the MC estimator, plotted in the right panel of Fig. 2, grows

Figure 1: Left: the (exact) mean first passage time \(u(i)\) with \(n=20,\ 40,\) and \(80\). Middle: the relative asymptotic variance (solid lines) and the relative empirical MSE (circles) of the LSTD estimator with \(=1\). The relative empirical MSE are obtained with sample sizes \(M=10n^{3}\). Right: the (exact) relative asymptotic MSE of the MC estimator.

exponentially fast with \(n\). As can be seen from the middle panel of Fig. 2, however, the relative asymptotic variance of the LSTD committor estimator with \(=1\) grows _more slowly_ than \(n^{3}\). Again empirical estimates of the relative MSE of the LSTD estimator with \(M=10n^{3}\) agree well with corresponding relative asymptotic variances and show the same trend.

Together, the mean first passage time and the committor estimation problems in this section strongly suggest a significant (polynomial versus exponential in \(n\)) advantage for TD methods over MC. In the next section we will prove that the performance advantage observed on these two examples holds in significant generality.

**Effect of the lag time.** In the above experiments we have examined the error of the LSTD estimator with \(=1\). In practice, the best choice of \(\) is long enough to avoid a nearly diagonal \(S^{}\), but not so long as to lose the efficiency advantage of TD over MC that we have just observed. Our bounds in Theorem 1 also change with \(\). To better illustrate the effect of \(\) on the relative asymptotic variance of the LSTD estimator and on our bounds, we now consider a "lazier" version of the Markov chain in (8). Specifically, we define \(P(i,i 1)=\) for \(i 1 D\), with the same boundary conditions as before. We again consider estimates of the mean first passage time and the committor, now with fixed \(n=40\). We plot the maximum of the relative asymptotic variance, \(_{i}^{2}/u^{2}(i)\), over \(i D\) in Fig. 3, along with the bound (6) for the mean first passage time in the left panel and (7) for the committor in the right panel. Though the plots do not extend to values of \(\) comparable with the largest values of \(_{i}[T]\), the relative variance of the MC estimator corresponds to the asymptote of the true relative variance toward the right hand side of each plot. We observe that the true relative variance of the LSTD estimator is minimized in both cases for a choice of \( 30\) and that, for this choice, the relative variance of the LSTD estimator is significantly smaller than the relative variance of the MC estimator. The bounds in (6) and (7) are designed to capture the high accuracy of LSTD for relatively small values of \(\) and we indeed see that they deteriorate as \(\) becomes very large. However, at least in the committor case, the bound accurately reproduces the initial decrease in error that occurs when \(\) is increased above \(=1\).

**Effect of the initial distribution.** So far, in our experiments, we have chosen \(\) to be the uniform distribution on \(D\). A natural alternative strategy is to harvest many short trajectories from a single, much

Figure 3: The bound and the truth for the maximum relative asymptotic variance of the mean first passage time and the committor with varying lag time \(\). The number of states fixed at \(n=40\). The relative asymptotic variance bounds for the mean first passage time and the committor are from (6) and (7) respectively.

Figure 2: Left: the (exact) committor function \(u(i)\) with \(n=20,\;40\), and \(80\). Middle: the relative asymptotic variance (solid lines) and the relative empirical MSE (circles) of the LSTD estimator with \(=1\). The relative empirical MSE are obtained with sample sizes \(M=10n^{3}\). Right: the (exact) relative asymptotic MSE of the MC estimator.

longer trajectory. Under ergodicity assumptions on \(P\), these short trajectories are then approximately drawn from the invariant distribution of \(P\). In Fig. 4, we present the relative asymptotic variance and the empirical estimates of the relative MSE of the mean first passage time and the committor, with \(P\) being the same as in (8), and \(\) chosen to be the invariant distribution \(p\) in (9) conditioned within \(D\). For \(=1\) and \(n=20,40,\) and \(80\), the maximum relative asymptotic variances of the LSTD estimators grow exponentially with \(n\). With \(M=10n^{3}\), the empirical relative MSE estimators with \(30000\) independent copies of the LSTD estimators agree well with the relative asymptotic variance for \(n=20\) and \(40\). For \(n=80\), the empirical linear system (3) fails to define the LSTD estimators with high probability, and the empirical error is undefined. These results should be contrasted with the much smaller errors shown in the middle panels of Figs. 1 and 2, corresponding to uniformly chosen initial conditions. We expect that the results in Fig. 4 would be somewhat different if, instead of initializing trajectories independently from the conditional invariant distribution, we had harvested correlated initial conditions from a single ergodic trajectory. Nonetheless, the results indicate that the choice of \(\) can have a significant impact on the performance of LSTD for prediction of rare events.

## 4 Rare event assumptions and upper bound

Our final relative asymptotic variance bounds will rigorously establish the dramatic advantage of TD over MC that we observed in the experimental results of Section 3. As in that section, we focus on the behavior of relative variance as \(n\) increases. Our results rely on several basic assumptions. On the one hand, all of these assumptions are satisfied by the Markov chain examined in Section 3. On the other hand, the results in this section cover considerably more general Markov chains. The assumptions concern certain structural properties of the Markov chain as \(n\) increases, but crucially, they allow both the typical escape time and the relative variance of the MC estimator to grow _exponentially_ in \(n\).

To ensure that a unique \(\) will exist for large enough \(M\), we make the following minorization assumption on \(\).

**Assumption 1** (Lower bound on \(\)).: _For some constant \(>0\), independent of \(n\), and all \(i D\), \((i)\)._

Recall that the uniform distribution on \(D\) was used to generate initial conditions for the numerical tests described in Section 3. The invariant distribution \(p\) in (9) conditioned within \(D\), instead, violates this assumption, as its minimum is exponentially small in \(n\).

As mentioned in Section 1.2 the perturbations, \(^{}-S^{}\), relevant for analysis of the LSTD estimator are far from the worst case perturbations characterizing classical perturbation bounds for Eq. (2). Each entry of the matrix \(^{t}\) is the empirical frequency of a specific transition. As a result, the variance of \(^{t}(i,j)\) is proportional to \(S^{t}(i,j)(1-S^{t}(i,j)) S^{t}(i,j)\), and we can characterize the perturbations by characterizing the entries themselves. Different entries of \(S^{t}\) can have very different magnitudes, and many Markov processes exhibit higher transition probabilities between states that are "close" according to some metric and small transition probabilities between states that are "far away". To express this notion in assumptions without resorting to any topology within which the state space may be embedded, we upgrade \([n]\) to an unweighted directed graph \(G\) by including edges

Figure 4: The relative asymptotic variance (solid lines) and the relative empirical MSE (circles) of the LSTD estimators of the mean first passage time and the committor, with \(\) being the invariant distribution \(p\) conditioned within \(D\). Note the scale is logarithmic. The relative empirical MSE are obtained with sample sizes \(M=10n^{3}\). For \(n=80\) the TD estimator fails with high probability and the empirical error is undefined.

only along transitions with significant probability. Our additional assumptions will be stated in terms of properties of this "minorizing graph."

Specifically, we introduce a directed edge from state \(i G\) to another state \(j G\) if \(S^{}(i,j) c\), where \(c\) is a positive constant independent of the number of states \(n\).

**Assumption 2** (Connected minorizing graph).: _For some constant \(c>0\) independent of \(n\), any two states \(i D\) and \(j[n]\) are connected by a path in \(G\)._

When \(P\) is irreducible, as assumed in Theorem 1, and \(n\) is fixed, we can always find a \(c>0\) so that any \(i D\) and \(j[n]\) are connected by a path in \(G\) for \(=1\). Assumption 2 emphasizes the independence of \(n\), asserting that the transition probabilities above the threshold \(c\) can, on their own, preserve the connectivity of the graph while \(n\) increases. Recall that the Markov chain studied in Section 3 satifies \(P(i,i 1) 1/(2(1+e))\), implying that Assumption 2 is satisfied with the choice \(c=1/(2(1+e))\).

Let \(d(i,j)\) be the length of the shortest path from state \(i\) to state \(j\) in \(G\). When the graph is undirected, \(d\) is a distance on \(G\), but it need not be. Finally, we characterize the decay of the transition probabilities of pairs of states that are "far away" according to \(G\).

**Assumption 3** ("Sub-Gaussian" transition probability tails).: _For some constants \(>0\) and \(C\), independent of \(n\), for all \(i j\), and \(t\), \(S^{t}(i,j) Ce^{- d^{2}(i,j)}\)._

Assumption 3 stipulates that transition probabilities decay fast enough for distant states in \(G\). For the Markov chain studied in Section 3, when \(=1\) this assumption is satisfied because transition probabilities are zero for states that are not nearest neighbors. Assumptions 2 and 3 essentially characterize the locality of the transitions. These assumptions are practically reasonable even when, as in the example in Section 3, the global landscape of the transitions exhibits challenging traits such as multi-modality.

**Illustration.**_The left panel of Fig. 5, depicts a general (symmetric) Markov chain on a graph. Darker edges indicate higher transition probabilities. The transition probabilities are chosen to decay with Euclidean distance in the plane, consistent with, for example, a Gaussian transition kernel in the plane. The minorizing graph on the right retains only those edges exceeding the threshold \(c\), which is chosen large enough to remove most distant edges, but small enough to result in a connected graph._

In Section 3, we considered the effect of the choice of \(\) on relative asymptotic variance. In that example, a larger choice of \(\) initially increased the probability of nearest neighbor transitions and would allow for a larger choice of \(c\). As \(\) increases further however, the probabilities of distant transitions would become larger, potentially decreasing the maximum allowed choice of \(c\)_and_ possibly requiring a smaller choice of \(\) and/or a larger choice of \(C\) in Assumption 3. The quantitative interplay among \(c\), \(\) and \(C\) are determined by the specific Markov chain under study, but we expect similar behavior for common transition probabilities such as Gaussian transition kernels.

As the experiments in Section 3 clearly demonstrate, these assumptions allow both the typical escape time and the relative variance of the MC estimator to scale exponentially with \(n\). The failure of MC in these scenarios does not contradict this paper's theoretical results because the MC estimator does not satisfy Assumption 2. Indeed, the MC estimator corresponds to \(=\), for which \(S^{}(i,j)\) can only be non-zero if \(i D\), \(j D^{c}\).

With these assumptions, we can now state our upper bound for the rare event setting, whose proof is very simple but informative. The key idea is that, under these assumptions, \(Q^{}(k,)\) in the denominator is indeed much larger than the transition probabilities \(S^{t}(k,)\) in the numerator.

Figure 5: Left: an undirected graph with edges colored according to the transition probabilities. An edge with darker color corresponds to a transition with higher probability. Right: after pruning edges with low transition probabilities, the minorizing graph stays connected.

**Theorem 2**.: _Under Assumptions 1, 2 and 3, we have the following asymptotic variance bound:_

\[^{2}}{u^{2}(i)} e^{}{ }}\,n^{3}.\] (12)

_When \(R(i)=0\) for all \(i D\),_

\[^{2}}{u^{2}(i)}e^{}{ }}\,n^{3}.\] (13)

Proof of Theorem 2.: Because \(Q^{}(k,)\) is the sum of the probabilities of all paths connecting states \(k\) and state \(\) that do not return to \(k\) and do not exit \(D\) (except through \(\)), Assumption 2 implies that

\[Q^{}(k,) c^{d(k,)}.\]

Plugging this bound and the ones in Assumptions 1 and 3 into (6) we find that

\[^{2}}{u^{2}(i)}\,\,n_{ k D,\,[n],\\  k}e^{-d(k,)(2 c+\,d(k,))}.\]

Optimizing the summand over \(d(k,)\) gives the bound in (12). The bound in (13) follows from the same argument using (7) instead of (6). 

Remarkably, the total amount of data (measured in observed transitions of \(X_{t}\)) required to achieve a fixed relative accuracy has reduced from as bad as exponential in \(n\) for the MC estimator to no worse than \(n^{3}\) for the short trajectory estimator.

## 5 Conclusions and future work

In this article we show that the LSTD method can produce relatively accurate estimators of rare event statistics with a data set of observed Markov chain transitions that is dramatically smaller than would be required by the Monte Carlo estimator. In particular, the LSTD estimator can achieve relative accuracy with a number of observed transitions much smaller than typical timescale of the rare event. This contrasts with the classical worst case perturbation bounds, which predict a large error when the typical timescale of the rare event is large.

Generalization of our basic conclusions beyond the tabular setting is a natural goal for future work. Our proof strategy can be used to establish general, entry-wise perturbation bounds for linear systems of the form in (2). A version of those bounds that applies to Markov processes in continuous spaces would have many interesting consequences, including in the analysis of both approximation and estimation error for TD approaches beyond the tabular setting.

We have not considered online TD approaches (Sutton and Barto, 2018). In the tabular setting considered here, standard online TD corresponds to a variant of classical Richardson iteration in which the residual of (2) is replaced by an independent realization of the residual of (3) at each iteration. Both deterministic Richardson iteration and online TD will converge very slowly when the typical timescale of the rare event is large. This issue has been explored in Strahan et al. (2023), where the authors suggest a batch version of subspace iteration for online policy evaluation and demonstrate improved convergence. The data requirements of that scheme should be studied theoretically.