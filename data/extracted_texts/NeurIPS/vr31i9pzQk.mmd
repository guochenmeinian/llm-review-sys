# GuardFormer \(\bigcirc\): Guardrail Instruction Pretraining for Efficient SafeGuarding This paper contains examples of harmful language. Reader discretion is recommended.

**GuardFormer \(\): Guardrail Instruction Pretraining for Efficient SafeGuarding This paper contains examples of harmful language. Reader discretion is recommended.**

**James O' Neill\({}^{,}\)1**Santhosh Subramanian\({}^{,}\)1**Eric Lin\({}^{1}\)**Abishek Satish\({}^{1}\) james@dynamofl.com

**Vaikkunth Mugunthan\({}^{1}\)**

**Abstract**

Large language models (LLMs) have shown promise in guardrailing against undesired behaviors, but their high inference costs, memory consumption, and unstructured outputs can be prohibitive. In this work we propose guardrail-specific instruction pretraining using a synthetic data generation pipeline. The data generation process is tailored towards generating policies that define the scope of the guardrail, compliant and non-compliant prompts, rationales when non-compliant and the output binary compliant or non-compliant label. From this, we propose a new guardrail model called Guardformer and show when further few-shot fine-tuned it significantly outperforms current state of the art (SoTA) while only requiring 512MB in storage. GuardFormer is orders of magnitude smaller than baselines such as gpt-4, yet significantly outperforms it while having the ability to learn from multiple custom policies at once.

Empirical evaluation across 7 public datasets and 4 novel guardrail benchmarks demonstrates our efficient classifiers' superiority over state-of-the-art LLMs and third-party APIs. Our models achieve average F1 score improvements of **29.64** and **21.07** points compared to Aegis-LlamaGuard and gpt-4o, respectively, in distinguishing safe from unsafe behaviors. Notably, models trained on our synthetic data consistently outperform those trained on real data, even when evaluated against custom-defined guardrailing policies, underscoring the efficacy of our approach.

## 1 Introduction

The widespread use of large language models (LLMs) in both the public and private domains has led to an increasing concern around guardrailing against malicious prompts (Biswas and Talukdar, 2023; Greshake et al., 2023; Manczak et al., 2024). While there has been a concerted effort to defend against misusage of LLMs, current guardrailing and safety alignment approaches can lead to considerable performance degradation on safe and non-malicious prompts, reducing the models general capabilities (Qi et al., 2023; Jain et al., 2023). In contrast, guardrails that are independent of the main LLM being used avoid this issue of safety alignment degrading generalization performance. While 3rd party API services and publicly available models (e.g PromptGuard and LlamaGuard (Inan et al., 2023)) offerdifferent solutions to this issue of guardrailing while not diminishing the LLMs general capabilities, they are limited in generalization performance, inference speed and adaptability (i.e transfer learning is difficult without retraining).

In this paper, we show that through the use of a well crafted synthetic data generation pipeline that our robustly fine-tuned classifiers can significantly outperform current state of the art (SoTA) while being orders of magnitude smaller w.r.t the number of model parameters. This involves describing each task with task definitions that include a concise summary of the task, allowed and disallowed behaviors and examples of safe and unsafe behaviors. We demonstrate the effectiveness of these classifiers on various safety, toxicity and prompt injection public benchmarks and show significant improvements over LLamaGuard--7b [Inan et al., 2023], Nemo Guardrails [Rebedea et al., 2023], Azure Content Safety, GPT-3.5-turbo/4/4o [OpenAI, 2023a], Meta PromptGuard [Inan et al., 2023] and OpenAIs Content Moderation API [OpenAI, 2023b]. One challenge to this initial approach is that we require a fine-tuned classifier for each domain for optimal performance. To address this challenge we further show that it is possible to maintain better performance over the aforementioned baseline using a unified guardrail that performs guardrail instruction pretraining learning by unifying synthetically generated policy-specific datasets. Below we summarize our contributions:

* Guardrail classifiers that are 14 times faster than the best performing LLM (gpt-4) while outperforming it on public datasets by 21.07 F1 and 5.13 F1 on our proposed CustomGuardBenchmark.
* A multi-task learning approach to guardrailing, we refer to as GuardFormer that outperforms a single-task guardrailing model, referred to as PolicyGuard by performing guardrail specific pretraining on synthetic data.
* A synthetic data generation pipeline for guardrailing, for both classifier pretraining for generalizability and policy specific fine-tuning for task specificity. This includes a self-reflection step that improves the label judgements by reflecting on the LLMs initial label prediction.
* An analysis of how guardrail performance varies as a function of 1) the number of training samples used for training, 2) training on synthetic or real data and 3) the number of active fine-tuning parameters required with and without pretraining.

## 2 Related work

Below describes work related to the main aspects of this work.

**Generative Content moderation.** Ensuring safety has been an active area of research for several years. Most recently, LLMs have been used solely for guardrailing. This includes LlamaGuard-7B/2-8B/3-8B [Inan et al., 2023] have defined policy descriptions and unsafe categories that outline what 'Can' and 'Should not' be allowed for input prompts and output responses. Ghosh et al.  have built upon LlamaGuard-2 by using safety-based adapter fine-tuning of LlamaGuard 2 on the Aegis Safety dataset that outlines a broad taxonomy of

Figure 1: **GuardFormer**: Creating Robust Guardrails with guardrail-instruction pretraining and guardrail classification using Synthetic Guardrail Data Generation.

13 critical safety risk categories. Nemo Guardrails Rebedea et al. (2023) have introduced programmable guardrails whereby a specialized modeling language, Colang, can be used to define behaviors by giving either behavioral definitions or examples of such in a programmatic manner. While these LLMs have shown promise, it still remains infeasible to run (7B or larger) models for many latency (or memory) critical applications. Additionally, next token prediction in these generative models are not guaranteed to predict the defined safe or unsafe categories which potentially makes output parsing unreliable and difficult.

**Discriminative Content Moderation** Bert-based classifiers have been used to detect offensive or toxic inputs (Vidgen et al., 2020; Deng et al., 2022). More more recent work has focused on the use of LLMs through APIs such as Perspective API (Lees et al., 2022), OpenAI Content Moderation API (Markov et al., 2023) (categories including toxicity, threat, harassment, and violence) and Azure Content Safety API (Microsoft, 2023) (categories include hate and violence) that provide a severity score between 0-6. While bert-based classifiers have the benefit of being much smaller than current LLMs, to date they have lacked the necessary training data to be robust against guardrail domains and topics of interest. Lee et al. (2024) have also focused on training binary classifiers to guard against unsafe prompts and responses. They use affirmative prefixes to encourage safety-aligned LLMs to generate instructions, responses and rely on LlamaGuard-3 to generate corresponding labels. In contrast, to our work, we are not limited to a single jailbreaking technique to generate text from LLMs, we do not need to rely on an existing seed dataset (a policy definition instead), we only require one LLM generator (they use 3 different LLMs) and our pretraining dataset is an order magnitude larger (\(>\)1 million training samples, compared to their 100k training samples).

Our work addresses shortcomings of these prior works.

## 3 Methodology

Below we describe how we synthetically generate safe and unsafe samples and refine policy definitions for improved generation on various guardrail tasks. We then describe the model pretraining, fine-tuning and model merging process.

### Synthetic Data Generation

For Synthetic Data Generation (SDG), we begin by defining a description of the task, which we refer to as a policy \(\). Here, \(\) includes a policy name \(_{}\), description \(_{}\), allowed behaviors \(_{}\), disallowed behaviors \(_{}\) and an optional \(_{}\) that gives examples of safe and unsafe prompts. Given \(_{}\), a seed dataset \(_{}:=\{(x^{i}_{},r^{i}_{},y^{i}_ {})\}_{i=1}^{N_{}}\{(x^{i}_{},r^{i }_{}y^{j}_{})\}_{j=1}^{M_{}}\) is generated where \(x_{}\), \(r_{}\) and \(y_{}\) are a compliant prompt, a rationale for compliangvely and label and \(x_{}\), \(r_{}\) and \(y_{}\) are a noncompliant prompt, a rationale for noncompliance and label respectively. We can formulate the SDG process as a conditional distribution \(p(|;)\) where \(\) is the LLM data generator. Once \(\) is generated, we refine the policy to improve clarity using a prompt template that prompts \(\) to self-reflect on its own label judgements for all \(y_{}\) and \(y_{}\) with the aim of recorrecting any incorrectly generated prompts. For our public benchmarks that contain training datasets along with test sets that are used for benchmarking (e.g BeaverTails (Ji et al., 2024)), a set of example unsafe inputs in \(_{}\) are used to bias the data generation towards prompts that are within the same domain.

### Custom Policy Guardailing

Given the synthetic data generation process described by \(p(|;)\), we first train policy-specific fine-tuned classifiers, known as PolicyGuard. This method uses the generated dataset \(\) to create highly specialized models capable of accurately identifying policy violations across diverse domains. Let \(f_{}\) denote our base classifier with parameters \(\), which can be instantiated as large pre-trained language models (e.g RoBERTa-large). We fine-tune \(\{_{}\) to create a policy-specific classifier \(f_{_{}}\) that maximizes performance on the task defined by policy \(\). We formulate the fine-tuning objective as \(_{}()=-|}_{(x_{i},y_{i}) }[y_{i}(f_{}(x_{i}))+(1-y_{i})(1-f_{}(x_{i}))]\) where \((x_{i},y_{i})\) are the prompt-label pairs from the synthetic dataset \(\), and \(f_{}(x_{i})\) is the predicted probability of the input being non-compliant with the policy. Byminimizing \(_{}()\) the classifier learns specific nuances of \(\) as represented in the generated data.

### Multi-Policy Guardailing

In contrast to PolicyGuard, in this section we describe GuardFormer, a novel approach designed to create a single, versatile model capable of performing well across all policies or tasks of interest. This approach not only enhances efficiency but also enables cross-task learning, potentially improving performance on individual tasks through shared representations.

To achieve this, we concatenate the SDG training datasets for all policies \(_{1},_{2},...,_{N}\), creating a unified dataset \(_{}\) where each all \(\) have been humanly created by a domain expert. We then use \(_{}\) as a seed dataset to generate more diverse _synthetic_ policies \(^{{}^{}}\) given the prompts and rationales in \(_{}\). Then, with \(^{{}^{}}\) we prompt the generator \(\) (e.g Mixtral 8x22B-Instruct) to generate safe and unsafe prompts and unsafe rationales, where applicable, given these new policies. This results in the full pretraining dataset \(_{*}\) that consists of the original seed \(_{}\) and the new diverse subset \(_{}\).

For each sample, we construct an instruction input that combines the policy description, prompt, and rationale. Formally, for a policy \(_{i}\), a sample in \(_{*}\) is represented as \(_{i}=_{(i,)}\) Query: \(x_{i}\) [SEP] \(r_{i}\) where \(p_{i}\) is the prompt, \(r_{i}\) is the corresponding generated rationale, and [SEP] is a separator token. We then train a multi-task model \(f_{_{}}\) on \(_{*}\) by minimizing a combination of masked language modeling (MLM) loss, \(_{++}\) loss [Pereira et al., 2021] and classification loss:

\[(_{})=_{1}_{}(_ {})+_{2}_{_{++}}(_{})+_{3}_{}(_{})\] (1)

where \(_{1...3}\) are hyperparameters balancing the three loss components. The MLM loss \(_{}\) is defined as \(_{}(_{})=-|}_ {m} p(_{m}|x_{ m};_{})\) where \(\) is the set of masked tokens, \(_{m}\) is a masked token, and \(x_{ m}\) represents the input with masked tokens. The \(_{++}\) loss \(_{_{++}}\) is designed to improve the model's robustness and generalization across tasks.

It is defined as \(_{_{++}}(_{})=_{}+_{}\) where \(_{}\) is the loss computed using gold labels and \(_{}\) is the virtual adversarial training (VAT) loss. The VAT loss is then defined as: \(_{}(_{})=_{x}:||p(y|x;_{ })|p(y|x+;_{})\) where \(\) is a small perturbation bounded by \(\) and KL is the Kullback-Leibler divergence between the model's predictions for the original and perturbed inputs. This encourages consistent predictions under small input perturbations.

During inference, given a new input \(x_{}\) for a specific policy \(_{j}\), we construct the instruction input as described earlier and use the trained model to predict: \(y_{}=_{y\{,\}}f_{_{ }}(x_{})\). This guardrail instruction-based pretraining (GIP) allows the model to distinguish between different policies during both training and inference, effectively learning to handle multiple tasks within a single architecture while benefiting from shared representations across tasks.

## 4 Experimental Setup

### Dataset Details

For our experiments, we split our fine-tuning and evaluation using synthetic data and compare to fine-tuning on real data on the training dataset from the public benchmark or if there is no training dataset for the public test dataset, we train on real related data (i.e the same domain). For our private benchmark, all our results for PolicyGuard and MultiPolicyGuard are fine-tuned on synthetic data. In the appendix we describe each policy description we use for both our public and private training datasets.

Public BenchmarksWe first benchmark against publicly available datasets that are all available on the huggingface dataset hub1, which we now provide their hub names. This includes 2 prompt-injection datasets (deepset/prompt-injections and xTRam1/safe-guard-prompt-injection), 3 toxicity-based datasets ("toxichat0124" from lmsys/toxic-chat Lin et al. (2023) and SetFit/toxic_conversations_50k) and 3 content safety datasets (nvidia/Aegis-AI-Content-Safety-Dataset-1.0, mnathys/openai-moderation-api-evaluation and PKU-Alignment/BeaverTails). Each datasets test set is converted into binary labels (safe/unsafe) where necessary (e.g openai-moderation).

Private BenchmarkingWe also test our proposed guardrails on a private benchmark CustomGuardBench2, which consists of datasets we refer to as Safety, Finance, Tax and Injection. These 4 datasets cover the prohibiting of unsafe discussions, financial advice, tax advice and prompt injection respectively. For all of these datasets, an expert compliance officer and policy informed annotators manually annotate the benchmark dataset given an aforementioned policy definition for each one.

### Model Details

Baseline Models.For 3rd party API services we use 1) OpenAI GPT models such as gpt-3.5-turbo, gpt-4 and gpt-4o (OpenAI, 2023a)) OpenAI Content Moderation [OpenAI, 2023b], 3) Azure Content Safety and 4) Nemo Guardrails using gpt-4o as the generator. For the GPT-models we use batch completion through litellm3 library to optimally reduce API call response time. For our public available SoTA LLMs, we use LlamaGuard-1/2/3 [Inan et al., 2023], Meta-Llama-3.1-8B-Instruct [Dubey et al., 2024], nvidia/Aegis-AI-LlamaGuard [Ghosh et al., 2024] and Prompt-Guard-86M [AI, 2023].

Finetuning Setup.The base models used for our experiments in finetuning and benchmarking PolicyGuard and Guardformer are RoBERTaLarge Liu et al. (2019) and an instruction pretrained version of XLM-RoBERTaLarge Wang et al. (2024). The former is a standard well-established masked monolingual language model (MLM) model, while the latter is a multilingual MLM that has been trained from instructions to produce high quality sentence embeddings.

Figure 2: F1 Score Across DeepSet, SafeGuard, ToxicChat, SetFit, Nvidia-Content-Safety and OpenAI Moderation test datasets.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_EMPTY:7]

[MISSING_PAGE_FAIL:8]

Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktaschel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. _arXiv preprint arXiv:2311.12786_, 2023.
* Ji et al. (2024) Jiaming Ji, Mikkel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llvm via a human-preference dataset. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lee et al. (2024) Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, Yoshua Bengio, Juho Lee, and Sung Ju Hwang. Harmaug: Effective data augmentation for knowledge distillation of safety guard models. _arXiv preprint arXiv:2410.01524_, 2024.
* Lees et al. (2022) Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A new generation of perspective api: Efficient multilingual character-level transformers. In _Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining_, pages 3197-3207, 2022.
* Lin et al. (2023) Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxichat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. _arXiv preprint arXiv:2310.17389_, 2023.
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* Manczak et al. (2024) Blazej Manczak, Eliott Zemour, Eric Lin, and Vaikkunth Musunthan. Primeguard: Safe and helpful lms through tuning-free routing. _arXiv preprint arXiv:2407.16318_, 2024.
* Markov et al. (2023) Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in the real world. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 15009-15018, 2023.
* Microsoft (2023) Microsoft. Azure AI content safety, 2023. URL https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety. Accessed: 29/09/2024.
* OpenAI (2023a) OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023a.
* OpenAI (2023b) OpenAI. Moderation. https://platform.openai.com/docs/guides/moderation, 2023b. Accessed: 07/08/2021.
* Pereira et al. (2021) Lis Pereira, Fei Cheng, Masayuki Asahara, and Ichiro Kobayashi. Alice++: Adversarial training for robust and effective temporal reasoning. In _Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation_, pages 373-382, 2021.
* Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
* Rebedea et al. (2023) Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen. Nemo guardrails: A toolkit for controllable and safe llvm applications with programmable rails. _arXiv preprint arXiv:2310.10501_, 2023.
* Vidgen et al. (2020) Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically generated datasets to improve online hate detection. _arXiv preprint arXiv:2012.15761_, 2020.
* Wang et al. (2024) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: A technical report. _arXiv preprint arXiv:2402.05672_, 2024.
* Wang et al. (2020)