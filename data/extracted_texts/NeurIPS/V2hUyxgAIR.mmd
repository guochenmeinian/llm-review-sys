# Casting hybrid digital-analog training into hierarchical energy-based learning

Timothy Nest1

timothy.nest@mila.quebec

&Maxence Ernoult2

maxence@rain.ai

###### Abstract

Deep learning requires new approaches to combat the rising cost of training large models. The combination of energy-based analog circuits and the Equilibrium Propagation (EP) algorithm offers one compelling alternative to backpropagation (BP) for gradient-based optimization of neural nets. In this work, we introduce a hybrid framework comprising feedforward (FF) and energy-based (EB) blocks housed on digital and analog circuits. We derive a novel algorithm to compute gradients end-to-end via BP _and_ EP, through FF and EB parts respectively, enabling EP to be applied to much more flexible and realistic architectures as analog units are incorporated into digital circuitry over time. We demonstrate the effectiveness of the proposed approach, showing that a standard Hopfield Network can be split into any shape while maintaining automatic differentiation performance. We apply it to ImageNet32 where we establish new SOTA in the EP and BP-alternative literature (46% top-1). An extended version of this paper and code is available here.

## 1 Introduction

Deep learning today relies on three factors: i) GPUs, ii) feedforward (FF) models and iii) backprop (BP). With skyrocketing demands of AI compute, exploration of new compute paradigms has become an economic, and environmental necessity . One path towards this goal is analog in-memory computing , promising constant time complexity as well as reduced energy consumption . By mapping a neural network onto a "self-learning" _energy-based_ (EB) analog circuit  loss gradients can be derived via two physical relaxations to equilibrium . Equilibrium propagation (EP) , is a suitable algorithm for learning in such a setting due to its strong theoretical guarantees, scalability (among BP-alternatives)  and putative(\(10,000\)) increase in energy-efficiency and speed . Still, end-to-end EP-training presents significant challenges when executed on analog hardware, including non-ideal physical behaviors affecting both inference  and parameter optimization , as well as incompatibility with operations including nonlinearities, batchnorm, and attention . One possibility is hybrid systems incorporating both FF and EB components. Indeed, the design of inferential engines made of analog and digital parts is nearing commercial maturity , yet _in-situ_ learning of such systems remains unexplored.

Here, we propose a theoretical framework to extend end-to-end gradient computation to a setting where the system may or may not be _fully_ analog. Our work contends that a combination of digital_and_ analog hardware, with FF _and_ EB parts trained via BP _and_ EP respectively, can leverage advances from both digital and analog hardware in the near-term. Specifically, we introduce _Feedforward-tied Energy-based Models_ (ff-EBMs, Section 3.1) whose inference pathway is a composition of FF and EB modules (Alg. 1). We show that gradients in ff-EBMs can be computed in an end-to-end fashion (Section 3.3), via BP in FF blocks and EP in EB blocks (Theorem 3.1, Alg. 2) (Section 3.2), and that when each analog block comprises a single layer, the ff-EBM is purely FF (Lemma A.1), whose training is equivalent to BP (Corollary A.1). Finally, we demonstrate the effectiveness of our algorithm on ff-EBMs where EBM blocks are DHNs (Section 4). We show that i) gradient estimates computed by our algorithm (Alg. 2) near perfectly match gradients computed by end-to-end automatic differentiation (Fig. 2), ii) a standard DHN model can be split into a ff-EBM with equivalent layers and architecture without compromising performance and remaining on par with automatic differentiation, _with significant reductions in run-time_ due to the use of smaller EB blocks (Section 4.2), iii) the proposed approach scales, yielding 46 % top-1 (70% top-5) validation accuracy on ImageNet32, beating the current SOTA for BP alternatives by a large margin.

## 2 Background

Notations.Given a differentiable mapping \(A:^{n}^{m}\), we denote its _total_ derivative wrt \(s_{j}\) as \(d_{s_{j}}A(s):=dA(s)/ds_{j}^{m}\), its _partial_ derivative wrt \(s_{j}\) as \(_{j}A(s):= A(s)/ s_{j}^{m}\). When \(A\) takes scalar values (\(m=1\)), its _gradient_ wrt \(s_{j}\) is denoted as \(_{j}A(s):=_{j}A(s)^{}\).

### Energy-based models (EBMs)

For a given static input and set of weights, EBMs implicitly yield a prediction through the minimization of an energy function, making them a kind of implicit model. Namely, an EBM is defined by a (scalar) energy function \(E:s,,x E(s,,x)\) where \(x\), \(s\), and \(\) respectively denote a static input, hidden and output neuron states, and model parameters (weights). Each such tuple defines a configuration with an associated scalar energy value. Among all configurations for an input \(x\) and model parameters \(\), the model prediction \(s_{}\) is an equilibrium state which minimizes the energy :

\[s_{}:=_{s}E(s,,x).\] (1)

### Standard bilevel optimization

Assuming that \(_{s}^{2}E(x,s_{},)\) is invertible, note that the equilibrium state \(s_{}\) implicitly depends on \(x\) and \(\) via the implicit function theorem (Dontchev et al., 2009). Thus our goal when training an EBM is to adjust parameters \(\) such that \(s_{}(x,)\) minimizes a cost function \(:s,y(s,y)\) where \(y\) is some ground-truth associated with \(x\). More formally, our objective can be cast as a _bilevel optimization problem_(Zucchet and Sacramento, 2022):

\[_{}(x,,y):=(s_{},y) s _{}=_{s}E(s,,x).\] (2)

To solve Eq. (2) we compute the gradient of its outer objective \((x,,y)\) wrt to \(\) (\(d_{}(x,,y)\)) and perform gradient descent over \(\).

### Equilibrium Propagation (EP)

An algorithm used to train an EBM as Eq. (2) may be called an EBL algorithm (Scellier et al., 2024). EP (Scellier and Bengio, 2017) is an EBL algorithm which computes an estimate of \(d_{}(x,,y)\) in at least two phases. In its first phase, the model evolves freely to \(s_{}=_{s}E(s,,x)\). Then, the model is slightly nudged towards decreasing values of cost \(\) and evolves to an equilibrium state \(s_{}\). In practice, we augment the energy function \(E\) by a term \((s,y)\) where \(^{}\) is a _nudging factor_. Weights are updated to increase the energy of \(s_{}\) and decrease that of \(s_{}\), thereby "contrasting" these two states. More formally, Scellier and Bengio (2017) prescribe in the seminal EP paper:

\[s_{}:=_{s}[E(s,,x)+(s,y)], ^{}:=(_{2}E(s_{},,x)-_{2}E(s_{},,x)),\] (3)

where \(\) denotes some learning rate. EP comes in different flavors depending on the form of \(\) inside Eq. (3). _Centered_ EP (C-EP), where two nudged states of opposite nudging strengths (\(\)) are contrasted, performs best in practice (Laborieux et al., 2021; Scellier et al., 2024) and reads:\[^{ C-EP}:=(_{2}E(s_{-},,x )-_{2}E(s_{},,x)),\] (4)

## 3 Tying energy-based models with ff blocks

Here we introduce our model, its associated optimization problem and learning algorithm. We show how learning amounts to solving a multi-level optimization problem (Section 3.2), and propose a BP-EP gradient chaining algorithm as a solution (Section 3.3, Theorem 3.1, Alg. 2). We highlight that ff-EBMs reduce to standard ff nets (Lemma A.1) and the proposed BP-EP gradient chaining algorithm to standard BP (Corollary A.1) when each EB block comprises a single hidden layer. Finally, we highlight in red and blue the parts of the model and associated algorithms performed inside feedforward (digital) and EB (analog) blocks respectively.

### Feedforward-tied Energy-based Models (ff-EBMs)

Inference procedure.We define _Feedforward-tied Energy-based Models_ (ff-EBMs) as compositions of feedforward and EB transformations. Namely, an data sample \(x\) is fed into the first FF transformation \(F^{1}\) parameterized by some weights \(^{1}\), yielding an output \(x_{}^{1}\). \(x_{}^{1}\) is fed as a static input into the first EB block \(E^{1}\) with parameters \(^{1}\), and relaxes to equilibrium \(s_{}^{1}\). \(s_{}^{1}\) is fed into the next FF transformation \(F^{1}\) with weights \(^{1}\) and so on until reaching the output layer \(\).

```
1:\(s x\)
2:for\(k=1 N-1\)do
3:\(x F^{k}(s,^{k})\)
4:\(s*{Optim}[E^{k}(s,^{k},x)]\)
5:endfor
6:\( F^{N}(s,^{N})\) ```

**Algorithm 1** ff-EBM inference

A formal definition of the inference procedure for ff-EBMs is given inside Definition A.1, and more compactly inside Fig. 1 and Alg. 1.

Form of the energy functions.Given the \(^{ th}\) EB block of a ff-EBM, the associated energy function \(E^{k}\) takes some static input \(x^{k}\) from the output of the preceding FF transformation, has hidden neurons \(s^{k}\) and is parameterized by weights \(^{k}\). More precisely:

\[E^{k}(s^{k},^{k},x^{k}):=G^{k}(s^{k})-s^{k^{}} x^{k}+U^{k}(s^{k },^{k})\] (5)

Eq. (5) reveals three contributions to the energy. The first corresponds to the non-linearity applied inside the EB block, the second to a purely FF contribution from previous FF block \(F^{k}\), and the third to _internal_ interactions within the layers of the EB block. An interesting edge case is when \(U^{k}=0\) for all \(k\)'s, i.e. no intra-block layer interactions, i.e. the EB block comprises a single layer. In this case, \(s_{}^{k}\) is simply a feedforward mapping \(x^{k}\) through \(\) and in turn the ff-EBM is simply a standard feedforward architecture (see Lemma A.1 inside Appendix A.1.1.

### Multi-level optimization of ff-EBMs

In the same way that learning EBMs can be cast as a bilevel optimization problem, learning ff-EBMs is a _multi-level_ optimization problem where variables optimized in the inner subproblems are EB block variables \(s^{1},,s^{N-1}\). To make this clear, we re-write the energy function of the \(^{ th}\) block \(E^{k}\) from Eq. (5) to highlight the dependence between two consecutive EB block states. Namely, by writing \(^{k}(s^{k},^{k},s_{}^{k-1},^{k}):=E^{k}(s ^{k},^{k},F^{k}(s_{}^{k-1},^{k-1}))\), it can be seen that the equilibrium state \(s_{}^{k}\) obtained by minimizing \(E^{k}\) will be dependent upon the equilibrium state \(s_{}^{k-1}\) of the previous 

[MISSING_PAGE_EMPTY:4]

with \((;_{}^{k}_{}^{k})\), \(\) and \(\) the batchnorm, pooling and convolution operations, \(\) the generalized dot product for tensors and \(s^{k}:=(s_{1}^{k^{}}, s_{L}^{k^{}})^{}\) the state of block \(k\) comprising \(L\) layers. Weight matrices \(^{k}\) are symmetric and has a sparse, block-wise structure such that each layer \(s_{}^{k}\) is bidirectionally connected to its neighboring layers \(s_{-1}^{k}\) and \(s_{+1}^{k}\) through connections \(_{-1}^{k}\) and \(_{}^{k^{}}\) respectively (see Appendix A.1.3), either with fully connected (\(U_{}^{k}\)) and convolutional operations (\(U_{}^{k}\)). The non-linearity \(\) applied within EB blocks is \((x):=((,0),1)\). Additional details about equilibrium computation can be found inside Appendix A.1.3.

**Gradient comparison of EP and ID on ff-EBMs.**

We conducted comparisons between gradients obtained via our algorithm and an ID baseline (implemented as _BP through time_, see Alg. 12 inside appendix) showing near perfect alignment between ID and EP weight gradients. In practice _the use of normalization in between blocks and of the GOE weight initialization are instrumental for good gradient estimation_.

### Splitting experiment

For a given EBM and a _fixed_ number layers, we considered two models of depth (\(L=6\) and \(L=12\)) with various block splits maintaining equivalent architecture 4. These experiments employ a tolerance-based (TOL) convergence. We observe that performance achieved by EP on 6-layer EBM is improved and WCT reduced with smaller blocks. Performance is maintained across 4 distinct splits between 89% and 90% and is on par with both the ID baseline for each, and with the literature 5 [Scellier et al., 2024, Laborieux and Zenke, 2022]. The same trend is seen in ff-EBMs with \(L=12\) where an accuracy of \(~{}92\%\) across 3 splits, matching ID, and surpassing EP state-of-the art on CIFAR-10 [Scellier et al., 2024]. The significant reduction in WCT observed is due to the fact that by design training time for ff-EBMs scales linearly with number of blocks rather than supralinearly with number of layers.

### Scaling experiment

We consider ff-EBMs of fixed block size 2 and train them with two different depths (\(L=12\) and \(L=15\)) on CIFAR-100 and ImageNet32 by EP and ID 1. We observe that EP matches ID performance on all models and tasks, and the performance obtained by training the 15-layer models by exceeds ImageNet32 SOTA for EP by 10% [Laborieux and Zenke, 2022] and by around 5% among all BP-alternatives [Hoier et al., 2023].

## 5 Discussion

Figure 3: Validation accuracy and Wall Clock Time (WCT) obtained on CIFAR-10 by EP (Alg. 2) and ID on models with different number of layers (\(L\)) and block sizes (“bs”). 3 seeds are used.

Figure 2: Cosine similarity of EP/ID grads on a random sample x,y.

Related work.There is a growing body of work showing scalability of EP on vision tasks. Most notably, Laborieux and Zenke (2022) introduced holomorphic EP where loss gradients are computed with adiabatic oscillations via nudging in the complex plane Scellier et al. (2022) proposed a black-box version of EP where details about the system may not be known. These advances could be readily applied inside our EP-BP chaining algorithm to EB blocks. The work closest to ours, albeit without clear algorithmic prescriptions, is that of Zach (2021) where FF model learning is cast as a deeply nested optimization whose consecutive layers are tied by pair-wise energy functions, as does (Hoier et al., 2023). Such settings can be construed as a particular case of ff-EBM learning by EP where each EB block comprises a _single_ layer (\(U^{k}=0\) inside Eq. (5).

Limitations and future work.While ff-EBM learning inherits some pitfalls of BP nothing prevents FF modules inside ff-EBMs from being trained via _any_ BP alternative. BP can be parameterized by feedback weights to obviate weight transport from the inference circuit to the gradient computation circuit (Akrout et al., 2019); its gradients approximated as finite differences of feedback operators (Ernoult et al., 2022); or computed via implicit forward-mode differentiation with random weight perturbations in the inference circuit (Hiratani et al., 2022; Fournier et al., 2023; Malladi et al., 2023); local layer-wise loss functions can be used to prevent "backward locking" (Belilovsky et al., 2019; Ren et al., 2022; Hinton, 2022).

One extension of this study is to incorporate _more realism into ff-EBMs_. Beyond DHNs, Deep Resistive Nets (DRNs) Scellier (2024) Kendall et al. (2020) are exact models of idealized analog circuits trainable by EP. As such, using DRNs as EB blocks inside ff-EBMs is an exciting direction, which brings new challenges (Rasch et al., 2023; Lammie et al., 2024). Finally, considerable work is needed to prove ff-EBM further at scale on more difficult tasks (e.g. standard ImageNet), deeper architectures and novel data.

Concluding remarks and broader impact.We show that ff-EBMs constitute a novel framework for deep-learning in heterogeneous hardware settings. We hope that this can help overcome the typical division between digital _versus_ analog or BP _versus_ BP-free algorithms and that the greater energy-efficiency afforded by this framework provides a pragmatic, near-term blueprint to mitigate the dramatic carbon footprint of AI training (Strubell et al., 2020).As fully analog training accelerators remain far from commercial maturity, we believe this work offers an incremental and sustainable plan for gradually integrating analog, energy-based computational primitives as they are integrated into existing digital accelerators.

    & &  &  \\  & & Top-1 (\%) & Top-5 (\%) & WCT & Top-1 (\%) & Top-5 (\%) & WCT \\   & L=12 & 69.3 \({}^{ 0.2}\) & 89.9 \({}^{ 0.5}\) & 4:33 & 69.2\({}^{ 0.1}\) & 90.0 \({}^{ 0.2}\) & 4:16 \\  & L=15 & 71.2\({}^{ 0.2}\) & 90.2\({}^{ 1.2}\) & 2:54 & 71.1\({}^{ 0.3}\) & 90.9 \({}^{ 0.1}\) & 2:44 \\   & L=12 & 44.7 \({}^{ 0.1}\) & 61:00 \({}^{ 0.1}\) & 65:23 & 44.7 \({}^{ 0.6}\) & 68.9\({}^{ 0.6}\) & 57:00 \\  & L=15 & **46.0**\({}^{ 0.1}\) & **70.0**\({}^{ 0.2}\) & 46:00 & 45.5 \({}^{ 0.1}\) & 69.0 \({}^{ 0.1}\) & 40:01 \\   Laborieux and Zenke (2022) & 36.5 & 60.8 & – & – & – & – \\ Höier et al. (2023) & 41.5 & 64.9 & – & – & – & – \\   

Table 1: Validation accuracy and WCT on CIFAR100 and ImageNet32 by EP and AD on models with different number of layers (\(L\)) and a BS of 2. For comparison, we show best published results for ImageNet32 by EP (Laborieux and Zenke, 2022) and all backprop alternatives (Hoier et al., 2023).

Figure 4: ff-EBMs as hierarchical systems implementing EP at chip scale (adapted from (Yi et al., 2023)) using _EB_ analog processors from resistors (green ), diodes (blue ), voltage sources (purple), ADCs and DACs (adapted from (Scellier, 2024)), digital processors, memory buffers, all linked by digital busses (red ).