# Fact or Fiction: Can Truthful Mechanisms

Eliminate Federated Free Riding?

 Marco Bornstein

University of Maryland

marcob@umd.edu

&Amrit Singh Bedi

University of Central Florida

amritbedi@ucf.edu

&Abdirisk Mohamed

University of Maryland

SAP Labs, LLC

amoham70@umd.edu

&Furong Huang

University of Maryland

furongh@umd.edu

###### Abstract

Standard federated learning (FL) approaches are vulnerable to the free-rider dilemma: participating agents can contribute little to nothing yet receive a well-trained aggregated model. While prior mechanisms attempt to solve the free-rider dilemma, none have addressed the issue of truthfulness. In practice, adversarial agents can provide false information to the server in order to cheat its way out of contributing to federated training. In an effort to make free-riding-averse federated mechanisms truthful, and consequently less prone to breaking down in practice, we propose Fact. Fact is the first federated mechanism that: (1) eliminates federated free riding by using a penalty system, (2) ensures agents provide truthful information by creating a competitive environment, and (3) encourages agent participation by offering better performance than training alone. Empirically, Fact avoids free-riding when agents are untruthful, and reduces agent loss by over \(4\)x.

## 1 Introduction

Truth is essential to the functioning of fair systems. In its absence, fraud and corruption lead to negative effects on those left participating. This too is true within machine learning (ML). Adversaries can attack ML algorithms for insidious purposes , and algorithms trained on data that is not well-representative or poisoned can lead to dangerous and unfair results .

Issues of truthfulness have begun to bleed into multi-agent collaborative learning systems. Federated Learning (FL), the preeminent collaborative learning framework, is susceptible to the free-rider dilemma: agents can contribute little to nothing during training while still obtaining a well-trained final aggregated model . Furthermore, Karimireddy et al.  proves that standard FL algorithms naturally yield catastrophic free-riding equilibria, where most rational agents contribute _nothing_. It is important to note that we do not consider the possibility of malicious gradient updates from adversarial agents, covered by recent works . We assume that agents share a common goal of reducing loss, and act selfishly rather than maliciously.

Recent work has aimed to remedy the free-rider dilemma in FL through contract theory , collaborative fairness , and mechanisms that incentivize contributions . The mechanisms presented in Karimireddy et al.  and Bornstein et al.  are notable because they do not require **(i)** carefully optimized contracts (agents will always participate because the mechanism is individually rational) or **(ii)** alterations to standard FL training (simple FedAvg  can be run withno computation of agent reputations). However, these mechanisms assume that agents are honest and truthful with an orchestrating central server. If agents can lie, the proposed mechanisms no longer guarantee elimination of free riding.

In our paper, we propose a mechanism, **F**ederated **A**gent **C**ost **T**ruthfulness (Fact), that provably eliminates free riding _even when agents are untruthful_. Unlike standard FL approaches, where agents are incentivized to use little to none of their own data, agents participating in Fact are incentivized to use as much data for federated training as they would on their own. Furthermore, when agents use their locally-optimal amount of data, Fact is individually rational (IR): agents always receive greater benefit participating in Fact than by training alone. These results hold even if agents try to cheat the mechanism by misreporting individual training costs to the central server. Fact ensures that each agent's best strategy is to be truthful with the central server.

**Summary of Contributions**. In summary, the main contributions of our paper are,

**(No Free Riding):**: Creating a novel penalization scheme that shifts the catastrophic free-riding optimum back to each agent's local optimum (thereby eliminating free riding).
**(Truthfulness):**: Constructing a truthfulness mechanism (competition) that dissuades agents from lying about their individual costs with the central server.
**(Realistic):**: Showcasing the efficacy of Fact empirically in eliminating agent free riding under untruthfulness, consequently improving federated training performance.

## 2 Related Works

**Distributed Learning Mechanisms for Agent Contribution**. An important line of mechanism research in distributed learning regards mechanisms that incentivize agent contributions for federated training [2; 12; 18; 38; 39]. Some of the first papers in this area were application-based, namely in mobile crowdsensing and smart sensors in vehicles. We detail these works in Appendix A. Zhan et al.  is one of the first works to define agent and server utility when data is shared between agents and the server to train a global model (including agent training costs). The server announces a total reward, and agents determine how much data to contribute in order to maximize their reward. A deep reinforcement learning method is used by the server to learn the optimal announced reward.

An accuracy-shaping mechanism is proposed in Karimireddy et al. , which also follows a similar data sharing and iid setting. This mechanism does not require the use of contracts or learning of a reward, and guarantees that an agent will receive improved reward when participating in the mechanism than by not participating. In the accuracy-shaping mechanism, the server degrades model performance for an agent if it does not contribute as much data as locally optimal. Bornstein et al.  proposes a similar accuracy-shaping mechanism. However, unlike Zhan et al.  and Karimireddy et al. , agent utility is modeled in a non-linear and more realistic manner. Additionally, the accuracy-shaping mechanism is generalized to the federated setting (no data sharing) where data can be non-iid. Unlike the majority of the works above, our mechanism seeks not just to incentivize agent contribution, but to eliminate free riders. Moreover, unlike [2; 12], we do not assume that agents are truthful. Our mechanism ensures that agents contribute what is locally optimal, thereby eliminating free-riding, and punishes agents for lying to the server about their costs.

**Truthful Distributed Learning Mechanisms**. Similar to the agent contribution mechanisms above, many existing truthful distributed learning mechanisms exist within the crowdsensing setting [9; 33; 35; 42]. We detail these works in Appendix A. The more recent works [14; 20; 34] examine truthful mechanisms within traditional FL. Lu et al.  explores truthful mechanisms in Vertical FL, where features are split up amongst agents. Specifically, a truthful mechanism is designed (via a novel transfer-payment rule) to maximize social utility, without use of an auction, even when agents can lie about their costs. Within Horizontal FL (the standard approach), Le et al.  designs a randomized auction for resource-constrained wireless agents, which achieves approximate-truthfulness in expectation. The server receives bids from agents to perform training, and selects a winning bid (agent) for each uplink sub-channel. The server pays a reward designed to minimize agent social cost, and does not take into account the server's utility. Wu et al.  builds on the auction idea in Le et al. , instead using a federated auction bandit which learns to select bidding agentsthat maximize the server's utility. The mechanism ensures truthfulness and individual rationality. Like the works above, our mechanism ensures truthfulness and individual rationality of participating agents. In contrast, our mechanism setting is within horizontal FL and does not require an auction or bandit method to ensure truthfulness. As such, any willing agent can participate in training (no bids are rejected), an optimal server reward does not need to be learned, and agents do not need to have a private valuation. Finally, our mechanism requires no direct payment from the server (all reward payments come from other agents) and provably eliminates the free-riding dilemma. Due to space constraints, **we detail works regarding fairness and agent selection mechanisms in Appendix A.**

## 3 The Challenge of Free Riding in Federated Learning

**Federated Setting**. Our work explores the setting of Centralized Federated Learning (CFL), where \(n\) agents collectively train a model \(w\) under orchestration of a central server. Within each iteration \(t\) of CFL, the server sends down the current aggregated model \(w^{t}\) to each agent. Agents then perform \(h\) local gradient updates (with their own local data) to generate an updated model \(w^{t+1}_{i}\)\( i[n]\). Models are aggregated by the server and this process is repeated until convergence. Each agent is assumed to have access to the same data distribution \(\), and thus we assume data is independent and identically distributed (iid). We leave construction of truthful mechanisms in the non-iid setting for future research. Our work aims to bring truthful mechanisms to CFL, as none have existed in either the iid or non-iid settings prior to this work. We now begin by detailing the objective that agents participating in FL seek to minimize.

**Federated Objective**. The goal of FL is to train a global model, leveraging the data and compute power of thousands of agents, that achieves low loss. To start, we utilize the convergence of distributed stochastic gradient descent (D-SGD)  to first-order stationary points in non-convex settings,

\[_{t=0}^{T-1}\| f(w^{t})\|^{2}}{ T}+L}{2_{j=1}^{n}m_{j}}.\] (1)

The parameter \(_{f}\) is the difference between the optimal objective value \(f^{*}\) and starting value \(f(w^{0})\). The global objective function \(f\) is the uniform sum over all device objective functions \(f_{i}\). There are \(T\) total iterations. The step-size is \(\) and \(L\) is the Lipschitz constant, where \( L<2\). The effective gradient variance is \(^{2}/_{j=1}^{n}m_{j}\). This stems from bounded variance assumptions, formally defined as \(\|_{i=1}^{M} f(w;_{i})- f(w)\| }{M}\) (for \(M\) batches of data \(\)).

**Local Agent Loss**. All rational agents seek to minimize their loss. As shown in Equation (1), this can be accomplished by using more data, and thus a larger batch size \(m\), as well as more rounds of gradient updates \(T\). However, all agents incur a cost \(c_{i}\) for collection, sampling, and compute costs for each data sample \(m\) used. The linear cost \(c_{i}m\) stems from our cross-agent setting. Within this setting, IoT devices are prevalent and data is not difficult to collect (_e.g.,_ IoT sensors). Thus, it makes the most sense that, on average, there are constant costs over time to collect and sample data (_e.g.,_ the cost to power a sensor is usually constant on average) .

Overall, agents must balance the benefit of using more data to achieve better model performance with the costs of data collection, sampling, and computations. We formulate this tradeoff for each agent by defining a local loss function \(_{i,l}(m_{i})\), that depends on the data collected \(m_{i}\) by each agent \(i\). Each agent minimizes its loss function, which is a combination of data costs and convergence error terms. When training locally, or alone, an agent loses the benefit of an increased batch size across all agents \(_{j=1}^{n}m_{j}\). Thus, after removing constants, Equation (1) transforms into our local agent loss,

\[_{i,l}(m_{i}):=L}{2m_{i}}+c_{i}m_{i}.\] (2)

The optimal amount of data each agent uses for local training can be determined with calculus.

**Theorem 1** (Optimal Local Data Usage).: _For an agent \(i\) with marginal cost \(c_{i}\), the optimal amount of data \(m^{*}_{i,l}\) used for local training is \(m^{*}_{i,l}:=L}{2c_{i}}}\)._

**Remark 1**.: _Agents' optimal amount of local data is inversely proportional to its cost \(c_{i}\). Larger costs lead to less data used and vice versa. If costs were zero, agents would use infinite data._

**The Free-Rider Dilemma: Shifted Optima in Federated Settings**. When agents engage in federated training, the effective batch size returns to the sum over all agent data \(_{j=1}^{n}m_{j}\). By participating in federated training, agents gain the benefit of an increased batch size. As such, each agent \(i\)'s loss function in the federated setting becomes,

\[_{i,F}(m_{i}):=L}{2(m_{i}+_{-i})}+c_{i}m_{ i}.\] (3)

While slight, this transformation changes the optimal amount of data used by each agent.

**Theorem 2** (Free-Riding: Optimal Federated Data Usage).: _For an agent \(i\) with marginal cost \(c_{i}\), the optimal amount of data \(m^{*}_{i,F}\) used for federated training is \(m^{*}_{i,F}=L}{2c_{i}}}-_{-i}\)._

**Remark 2**.: _By participating in federated training, each agent is incentivized to use fewer data samples. This explicitly demonstrates the free-rider dilemma mathematically: it may be optimal for an agent to use no data \(m^{*}_{i,F}=0\) (if \(_{-i}L}{2c_{i}}}\)) and still possibly achieve lower cost overall (if \(_{-i} m^{*}_{i,l}\))._

Due to space constraints, we leave the proofs of Theorems 1 and 2 in Appendix C.

## 4 Eliminating Free Riding via Penalization

The natural equilibrium of traditional FL is free riding: agents use less data than is locally optimal in proportion to how much data other agents use (Theorem 2). This necessitates the construction of a mechanism to restore the equilibrium of FL back to what is locally optimal for each agent. In this section, we detail how penalization can perform this restoration.

**Penalized Federated Learning (PFL)**. To disincentivize agents from straying from their locally optimal data usage \(m^{*}_{i,l}\), the central server can use contract theory to ensure each agent \(i\) uses its locally optimal amount for a reported cost \(c_{i}\). Namely, when agreeing to participate in FL, each agent \(i\) agrees to pay the following free-riding penalty \(P_{fr}\) if they do not use \(m^{*}_{i,l}=L}{2c_{i}}}\) data samples,

\[P_{fr}(m_{i}):=_{i}(}{2_{i}}-L} {4_{i}(m^{*}_{i,l}+_{-i})^{2}}+m^{*}_{i,l}-m_{i})^{2}.\] (4)

The hyperparameter \(_{i}>0\), chosen by the central server, denotes the harshness of the free-riding penalty. The value of \(_{i}\) is known by each agent \(i\) while deciding whether or not to participate in the mechanism. Since the penalty is leveled on each agent by the server, it is incorporated into their federated loss function. This results in a new amended federated loss function \(_{i,PFL}(m_{i})\) defined as,

\[_{i,PFL}(m_{i})=L}{2(m_{i}+_{ -i})}+c_{i}m_{i}}_{_{i,F}(m_{i})}+P_{fr}(m_{i}).\] (5)

**Theorem 3** (PFL Eliminates Free Riding).: _For an agent \(i\) with marginal cost \(c_{i}\), the optimal amount of data \(m^{*}_{i,PFL}\) used for federated training under Equation (5) is \(m^{*}_{i,PFL}:=L}{2c_{i}}}\)._

**Remark 3**.: _By utilizing the PFL loss in Equation (4), agents participating in FL will use a locally optimal data amount (\(m_{i,PFL}^{*}=m_{i,l}^{*}\)). This eliminates the free-rider dilemma seen in Theorem 2._

Agents participating in PFL (Algorithm 1) follow the amended loss shown in Equation (5). Careful selection of \(_{i}\) is required to ensure that PFL is IR.

**Ensuring Individual Rationality (IR) in PFL.** As shown above in Theorem 3, it is optimal for agents participating in the penalized federated scheme (with a loss function shown in Equation (5)) to contribute what is locally optimal _i.e., \(m_{i,PFL}^{*}=m_{i,l}^{*}\)_. To ensure individual rationality, it must be true that participating in federated training will produce at least as much reward for an agent than by not participating. The selection of \(_{i}\) is crucial in accomplishing this task.

**Lemma 1** (PFL Assurance of IR at Optimum).: _Let \(c_{i}\) be the marginal cost for an agent \(i\), \(m^{*}:=L}{2c_{i}}}=m_{i,l}^{*}=m_{i,PFL}^{*}\) be the agent's locally optimal data usage, and \([0,2)\) be a server-specified hyper-parameter. In order to ensure that the optimal penalized federated loss is at least lower than the optimal local loss, \(_{i,PFL}(m_{i}^{*})_{i,l}(m_{i}^{*})\), one must select \(_{i}\) such that,_

\[_{i}:=()}{(2-)^{2}L_{ -i}}c_{i}-L}{2(_{-i}+m^{*})^{2}} ^{2}.\] (6)

_Selection of such \(_{i}\) results in a loss gap between \(_{i,PFL}(m^{*})\) and \(_{i,l}(m^{*})\) of,_

\[_{i}:=_{i,l}(m^{*})-_{i,PFL}(m^{*})= L_{-i}}{m^{*}(_{-i}+m^{*})}.\] (7)

**Remark 4**.: _Agents provably receive improved loss \(\) when they use their locally optimal data amount \(m_{i,l}^{*}\) participating in PFL (Algorithm 1) versus local training, thereby ensuring IR._

**Remark 5**.: _The parameters \(_{i}\) and \(\) control the amount of benefit received by an agent \(i\). A larger value of \(\), and consequently \(_{i}\), results in a larger gap between \(_{i,PFL}(m^{*})\) and \(_{i,l}(m^{*})\). Inversely, a smaller value of \(\) and \(_{i}\) results in a smaller gap between local and PFL loss._

Due to space constraints, we leave the proofs of Theorem 3 and Lemma 1 in Appendix C. Now, we formally prove that PFL both (**i**) eliminates agent free riding and (**ii**) is individually rational.

**Theorem 4** (Elimination of Federated Free-Riding With Truthful Agents).: _PFL (Algorithm 1) using \(_{i}\) from Lemma 1 is IR and eliminates the free-rider dilemma when agents are truthful._

**Proof.** The result of Theorem 3 is that each agent \(i\)'s optimal strategy within the penalized federated scheme is to use their locally optimal amount of data \(m_{i}^{*}=L}{2c_{i}}}\). Furthermore, Lemma 1 states that using \(m_{i}^{*}\) within the penalized federated scheme results in a reward, or improvement over localtraining, of \(_{i}=L_{-i}}{m_{i }^{*}(m_{i}^{*}+_{-i})}\). Thus, by combining Theorem 3 and Lemma 1, truthful agents which choose to participate in PFL attain a reshaped and lower loss at the same optimum. Individually rational agents would therefore prefer to participate in PFL (Algorithm 1) over local training due to the reshaped optimum's lower loss. This optimum achieves the same data usage as local training, thereby eliminating the free-rider dilemma.

**Remark 6** (Truthfulness Concerns).: _Participating in PFL, rational agents contribute as much as they would locally (Theorem 3) and attain more benefit (Lemma 1). The only issue that remains is truthfulness: agents may lie about their costs in order to contribute less to federated training under the guise of what looks like an "optimal amount" to the server (thereby avoiding large \(P_{fr}\))._

While PFL eliminates free riding when agents are truthful, this assumption is not realistic in practice. Untruthful agents can report inflated costs to the server in order to trick it into expecting smaller data usage. In effect, agents can lie about their costs to free ride. In the next section, we propose a mechanism which incentivizes agents to be truthful (_i.e.,_ their best strategy is not to lie).

## 5 Fact: Eliminating Free-Riding With Untruthful Agents

When agents are truthful, PFL (Algorithm 1) provably eliminates the free-rider dilemma (Theorem 4). However, an adversarial agent \(i\) can still free ride by misreporting, or lying, about their true marginal cost \(c_{i}\). As previously mentioned, we do not consider the situation of malicious gradient updates coming from adversarial agents, covered by recent works . We assume that agents share a common goal of reducing loss, and act selfishly rather than maliciously.

As seen in Theorem 2, an adversarial agent can lie their way to a smaller data optimum by inflating their cost: \(m_{i}^{lie}=L}{2(c_{i}+)}}<m_{i,l}^{*}\). In this case, an adversarial agent will **(i)** avoid server penalty (since it will be contributing what looks like an optimum from the server's view), **(ii)** incur smaller true costs \(c_{i}m_{i}^{lie}<c_{i}m_{i}\), and **(iii)** still reap the benefits of a larger batch size \(_{-i}+m_{i}^{lie}\).

**Definition 1** (Truthful Mechanism).: _A truthful mechanism is one in which no agent can reduce its loss by reporting a cost different from its true cost, regardless of other agents' actions. Overall, each agent's dominant strategy is to report its true cost._

To counteract the possibility of an agent being untruthful with the server about its true cost, we propose a truthful mechanism Fact: Federated Agent Cost Truthfulness (Algorithm 2). Fact ensures that each agent \(i\)'s dominant strategy is to report its true cost \(c_{i}\) (thereby satisfying Definition 1).

```
1:Input: data \(=[m_{1},,m_{n}]\), marginal costs \(=[c_{1},,c_{n}]\), \(h\) local steps, \(T\) iterations, \(E\) epochs, initial parameters \(^{1}\), loss \(f\), step-size \(\), and constants \((L,^{2},)\).
2:Output: model \(^{T}\) and monetary reward \(r_{i}\).
3:Server receives contract payment from each agent: \(_{i}L_{-i}}{m_{i}()}\)
4:Run Penalized Federated Learning: \(^{T}\) PFL(\(,,h,T,E,^{1},f,,L,^{2},\))
5:Compute each agent reward \(r_{i}\) via the cost truthfulness game in Equation (10) ```

**Algorithm 2** FACT: Federated Agent Cost Truthfulness

**Assumption 1** (No Collusion).: _Agents have no knowledge of other agents' costs or the distribution of agent costs \(f_{C}\). In the absence of information, each agent \(i\) believes that its cost \(c_{i}\) is equally likely to be larger or smaller than any other agent's cost._

Our lone assumption about agent knowledge is that agents are unknowledgeable about the distribution of agent costs \(f_{C}\) or any other agent's cost: agent costs are private. Assumption 1 ensures that there is no collusion amongst agents. In the absence of knowledge about other agents' costs, it is reasonable for an agent to believe that their private cost is equally likely to be greater or lesser than any other agent's cost (_i.e.,_ the median cost). Now that agents can report costs \(c\) which differ from their true cost \(c_{i}\), each agent's PFL penalty (Equation 4) and loss (Equation 5) becomes a multivariable function,

\[P_{fr}(m_{i},c) =_{i}(}-L}{4 _{i}(L}{2c}}+_{-i})^{2}}+L}{2c}}-m_{i})^{2},\] (8) \[_{i,PFL}(m_{i},c) =L}{2(m_{i}+_{-i})}+c_{i}m_{i}+P_ {fr}(m_{i},c).\] (9)

Regardless of the reported cost \(c\), each agent \(i\) will incur a true cost \(c_{i}m_{i}\) locally in Equation (9).

Using a Sandwich to Incentivize Truthfulness.Our mechanism is the first to ensure truthfulness while solving the free-rider dilemma in FL. Fact ensures agent truthfulness by fostering a competition amongst agents. The competition begins at the end of federated training (PFL) by randomly grouping reported agent costs into threes. The rules of this competition, or truthfulness mechanism, are simple: each agent \(i\) "wins" the competition, and receives a reward, if its cost \(c_{i}\) is sandwiched between the two other agent costs in its group. If there is a tie, the server randomly selects one of the tied agents as the winner. This mechanism is detailed mathematically below,

\[P_{ct}(c):=_{i}-_{j i[n] }_{j}&C_{a}<c<C_{b},\\ _{i}&\] (10)

**Lemma 2**.: _The probability of an agent "winning" in the truthfulness mechanism in Equation (10), given a reported cost \(c\), is \(v(c):=2F_{C}(c)(1-F_{C}(c))\), where \(F_{C}\) is the CDF of \(f_{C}\)._

The crux of the truthfulness competition is that each agent \(i\)_never receives its own reward_\(_{i}\). Instead, as shown in Equation (10), each agent pays \(_{i}\) to the server before training regardless of the mechanism outcome. After training, if the agent "wins" the competition, by having a cost sandwiched between \(C_{a}\) and \(C_{b}\), then it receives triple the average of all other agent rewards \(_{j i[n]}_{j}\). The average reward is tripled, since two other agents will lose the competition and receive no reward. By making the reward decoupled from an agent's own reward \(_{i}\), each agent can no longer increase or decrease its reward by altering how much data \(m_{i}\) they use for federated training. Now, they can only affect the likelihood of winning the competition by choosing what cost \(c_{i}\) to report to the server.

Incorporating the truthfulness mechanism into the PFL loss function results in the Fact loss function,

\[_{i,Fact}(m_{i},c)=L}{2(m_{i}+ {m}_{-i})}+c_{i}m_{i}+P_{fr}(m_{i},c)}_{_{i,PFL}(m_{i},c)}+P_{cr}(c).\] (11)

Figure 1: **Enforcement of Agent Truthfulness.** Average net improvement in loss over local training is plotted for iid (dotted line) and two non-iid agent distributions (D-0.6: dashed, D-0.3: solid). For both CIFAR10 (left) and MNIST (right), agents maximize their net improvement in loss when they are truthful (0% added) about their true cost. This matches our theory in Theorem 5.

**Theorem 5** (Main Theorem).: _Each agent \(i\)'s best strategy, when participating in Fact (Equation 11), is to report its true cost and use its locally optimal amount of data \((m_{i},c)_{i}^{*}=(L}{2c_{i}}},c_{i})\)._

**Remark 7** (Truthfulness).: Fact _is a truthful mechanism: each agent's best strategy is to report its true cost \(c_{i}\). Agents cannot reduce their loss by reporting a different cost (satisfying Definition 1)._

**Remark 8** (Elimination of Free Riding).: Fact _eliminates agent free riding: each agent's best strategy is to use as much data as is locally optimal \(m_{i}^{*}=L}{2c_{i}}}\) (Theorem 1). Thus, the improved optimum for PFL (Theorem 3) is maintained even under agent untruthfulness._

**Remark 9** (Individually Rational).: _When using their best strategy, agents participating in Fact either (i) receive loss equivalent to local training if they lose the competition or (ii) reduced loss if they win (shown in Equation 10). Thus, Fact is IR as agents using their best strategy never receive worse loss than if they did not participate. In fact, agents receive lower loss in expectation._

## 6 Experimental Results

Below, we analyze the effectiveness of Fact at **(1)** enforcing agent truthfulness, **(2)** eliminating free riding, and **(3)** reducing agent loss compared to local training. We perform true federated experiments (no simulations) using CIFAR10  and MNIST  datasets to train an image classification model.

**Experimental Setup**. Within our experiments, 16 agents train a model individually (locally) as well as in a federated manner. Each agent uses 3,125 and 3,750 data samples each for CIFAR10 and MNIST respectively. We analyze Fact under homogeneous and heterogeneous agent data distributions. For heterogeneous agent data distributions, we use Dirichlet distributions with parameters \(=0.3,0.6\) to determine the label ratio for each agent . We simulate the sandwich truthfulness competition defined in Equation (10) by randomly sampling 2,000 synthetic agent costs from a Gaussian distribution with a mean centered at each agent's true cost (standard deviation of one-tenth of the true cost). We perform 100,000 simulation trials and compute mean performance for each agent over all trials. Further experiment details and hyperparameters are found in Appendix B.

Figure 2: **Reduction in Agent Loss. The average agent loss for baselines on CIFAR10 (top row) and MNIST (bottom row) under iid (left), and two non-iid data distributions (center: D-0.6, right: D-0.3). Traditional FL is an upper bound on agent loss (if agents did not free ride). Fact improves agent loss over local training by up to a factor of 3x for CIFAR10 and 4x for MNIST.**

**Lack of Baselines**. Fact is the first truthful Horizontal FL method that eliminates free riding while simultaneously enforcing truthfulness without the use of an auction or bandit (thereby allowing all willing agents to participate). As such, we compare the results of Fact with the only available option: local training. We showcase below that Fact reduces agent loss compared to local training while simultaneously avoiding free-riding and truthfulness issues.

**Fact Enforces Truthfulness**. The results of Figure 3 back the theoretical result of Theorem 5, which states that agents maximize their expected reward (_i.e.,_ net reduction in loss versus local training) when truthfully reporting their cost. Across both homogeneous (iid) and heterogeneous (n-iid) agent data distributions, agents' net improvement in loss peaks when reporting their true cost (0% added) and monotonically decreases as agents report an inflated or deflated true cost. Thus, the sandwich-style truthfulness competition Fact employs disincentivizes agents from inflating or deflating their true costs. This is important, as agents are no longer incentivized to lie to the server about their true cost in order to free-ride without the free-rider penalty in Equation (4) being imposed. Net improvement in loss grows as data distributions become increasingly non-iid. FL is robust enough to deal with the increasingly non-iid datasets while local training suffers (Figures 5 & 6). Therefore, the gap between FL and local training grows, and thus so too does the reward in Fact.

**Fact Reduces Agent Loss**. Agents participating in Fact on average achieve a reduction in total loss compared to training on their own when using the same amount of data. Consequently, participating in Fact is individually rational: agents at worst receive loss equivalent to local training and on average receive improved loss (Figure 2). Traditional FL achieves a lower average agent loss than Fact when _all agents use as much data as they would locally_. However, as shown in Theorem 2, this is unrealistic. Traditional FL emits a catastrophic free-riding optimum. The results of traditional FL in Figure 2 is the upper bound on achievable loss by agents, only attainable if agents refused to free ride. The gap in loss between Fact and the upper bound of traditional FL is the cost of agent untruthfulness and their inclination to free ride. The gap is the direct result of the penalty terms on free riding \(P_{fr}\) and truthfulness \(P_{ct}\) found in Equation (11). Average local loss increases as data distributions become increasingly non-iid, while traditional FL remains robust to the distribution shift (Figures 5 & 6). Since the agents in Fact who lose the truthfulness competition receive a loss equivalent to local training, the loss of Fact also rises in proportion to the local training loss.

**Fact Eliminates Free Riding**. Within our experiments for CIFAR10 and MNIST, a marginal cost was carefully selected such that it is locally optimal for each agent to use 3,125 and 3,750 data samples for training respectively. As shown in Figure 3, the free-rider penalty \(P_{fr}\) harshly penalizes agents for deviating from this locally-optimal amount. This result accounts for the reduction (gain) in data collection costs for using less (more) data during training. Thus, as proven theoretically in Theorem 3, we confirm that it is suboptimal for an agent \(i\) to use either more or less data than is what is locally optimal (for a given reported cost \(c_{i}\)) when participating in Fact.

Figure 3: **Elimination of Free Riding via Penalty. The penalty term \(P_{fr}(m_{i})\) plus data collection costs \(c_{i}m_{i}\) is plotted for CIFAR10 (left) and MNIST (right) for varying data contributions \(m_{i}\). These combined costs are minimized at the local optimum \(m_{i,l}^{*}\), as predicted by Theorem 3.**

### Real-World Case Study: Inter-Hospital Skin Cancer Diagnosis

We consider a realistic situation where a consortium of hospitals seek collaboration to train a model, privately in a FL manner, that can diagnose skin cancer. One of the hospitals is smaller and resource-constrained. It is difficult, but not impossible, for this hospital to collect more data for training. Thus, in the absence of a truthful FL mechanism, the smaller hospital could over-report its collection costs to the server in order to contribute little to no data towards training while still reaping the rewards of a well-trained global model.

To test how Fact deals with this scenario, we train an image classification model on the HAM10000  dataset. HAM10000 consists of a collection of dermatoscopic images of pigmented lesions. The goal is to train a model which can perform automated diagnosis of pigmented skin lesions, including melanoma (skin cancer). Our setup is similar to the experimental setup detailed in the previous section. One difference is that we fine-tune a ResNet50 model on HAM10000 that is pre-trained on ImageNet (a realistic approach for a hospital). HAM10000 is an imbalanced dataset, and evenly partitioning \(80\%\) of the data amongst 10 devices, as their local training sets, further exacerbates the non-idness of the data. This too is realistic, as data is often non-iid amongst FL agents. We use the Adam optimizer with a learning rate of 1e-3 and batch size of 128.

As expected, Fact provably dissuades any hospital from lying about its cost in order to reduce their data contribution level. Agents are incentivized to be truthful; reduction in loss is maximized when agents truthfully report their costs (see the middle plot in Figure 4). As a result, a smaller, resourced-constrained hospital would still contribute its locally optimal amount of data, or face harsh penalties (see the right plot in Figure 4). Finally, all participating hospitals are still better off participating in Fact than training alone. Participating agent loss is reduced by nearly \(66\%\) compared to local training (see the left plot in Figure 4). The effectiveness of Fact to dissuade free-riding in the real-world setting of training a skin cancer classifier across hospitals indicates that it will be successful in other real-world settings.

## 7 Conclusion

Traditional FL frameworks are susceptible to free riding: it is often an agent's best strategy to contribute little to nothing during training while still obtaining a well-trained final global model. Furthermore, many FL frameworks do not account for untruthful agents which report inaccurate information to a server in order to skip penalties and collect greater rewards. Our proposed mechanism, Fact, is simultaneously individually rational, truthful, and eliminates free riding. Fact leverages a novel penalization and sandwich mechanism to shift each agent's best strategy to report its true cost with the server (**truthfulness**) and use as much data as it would on its own (**no free riding**). Furthermore, agents which participate in Fact will never receive worse loss than by not participating, and receive lower loss on average if they participate (**individually rational**). Empirically, we find that Fact enforces agent truthfulness while reducing agent loss by upwards of a factor of four.

Figure 4: **Fact Eliminates Free Riding in Realistic Settings**. When training an image classifier for diagnosing skin cancer, agents participating in FACT achieve much lower loss (\(66\%\) less) than if they did not participate (left). Agents maximize their improvement in loss over local training when they are truthful; reporting inflated or deflated costs diminishes improvement in loss (middle). Agents minimize penalties when using their locally optimal amount of data (\(m^{*}=801\)) for training (right).