# You Only Look Around: Learning Illumination Invariant Feature for Low-light Object Detection

Mingbo Hong

Megvii Technology, Beijing, China

mingbohong97@gmail.com

&Shen Cheng

Megvii Technology, Beijing, China

chengshen@megvii.com

&Haibin Huang

Kuaishou Technology

jackiehuanghaibin@gmail.com

&Haoqiang Fan

Megvii Technology, Beijing, China

fhq@megvii.com

&Shuaicheng Liu

University of Electronic Science and Technology of China

liushuaicheng@uestc.edu.cn

Equal contribution.Corresponding Author.

###### Abstract

In this paper, we introduce YOLA, a novel framework for object detection in low-light scenarios. Unlike previous works, we propose to tackle this challenging problem from the perspective of feature learning. Specifically, we propose to learn illumination-invariant features through the Lambertian image formation model. We observe that, under the Lambertian assumption, it is feasible to approximate illumination-invariant feature maps by exploiting the interrelationships between neighboring color channels and spatially adjacent pixels. By incorporating additional constraints, these relationships can be characterized in the form of convolutional kernels, which can be trained in a detection-driven manner within a network. Towards this end, we introduce a novel module dedicated to the extraction of illumination-invariant features from low-light images, which can be easily integrated into existing object detection frameworks. Our empirical findings reveal significant improvements in low-light object detection tasks, as well as promising results in both well-lit and over-lit scenarios. Code is available at https://github.com/MingboHong/YOLA.

## 1 Introduction

In the field of computer vision, object detection stands as a cornerstone, driving advancements in numerous applications ranging from autonomous vehicles to security surveillance . The ability to accurately identify and locate objects in digital imagery has seen remarkable progress, largely due to the advent of deep learning techniques . However, despite these advancements, object detection in low-light conditions remains a significant challenge. Low-light environments lead to poor image quality, reduced visibility, and increased misdetections in night-time surveillance and twilight driving .

Traditional methods in tackling low-light object detection have predominantly leaned towards image enhancement techniques . While these methods have demonstrated effectiveness inimproving visual aesthetics and perceptual quality, they often do not directly translate to improved object detection performance. This discrepancy arises because these enhancement techniques are typically optimized for human visual perception, which does not always correlate with the requirements for effective and accurate object detection by machine learning models.

In addition to image enhancement strategies, another research direction involves fine-tuning pre-trained models for low-light conditions. Typically, detectors are initially trained on extensive datasets of well-lit images, such as those from Pascal VOC  and Microsoft COCO , and subsequently fine-tuned on smaller, low-light datasets . To enhance the utilization of cross-domain information, the MAET framework  was developed to learn intrinsic visual structure features by separating object features from those caused by degradation in image quality. Similarly, methods  aim to restore the normal appearances of corrupted images during detector training. However, these techniques often depend heavily on synthetic datasets, which could limit their real-world applicability.

Recent methods in low-light object detection, such as those in , use Laplacian pyramids  for multi-scale edge extraction and image enhancement. FeatEnHancer  further leverages hierarchical features for improved low-light vision. However, these task-specific, loss-driven approaches often grapple with a larger solution space due to varying illumination effects.

In this study, we introduce a novel approach that explicitly leverages illumination-invariant features, utilizing the principles of the Lambertian image formation model . Under the Lambertian assumption, we can express the pixel values in each channel as a discrete combination of three key components: the surface normal, the light direction (both of which are solely related to the pixel's position), the spectral power distribution, and the intrinsic properties of the pixel itself. The illumination-invariant feature can be learned by eliminating the position-related term and spectral power-related term . We introduce this concept of extracting illumination-invariant features into low-light detection tasks and demonstrate that incorporating this feature yields significant performance improvements in low-light detection tasks. We further improve this illumination-invariant feature using task-driven kernels. Our key observation is that by imposing a zero-mean constraint on these kernels, the feature can simultaneously discover richer downstream task-specific patterns and maintain illumination invariance, thereby improving performance.

Towards this end, we propose the Illumination-Invariant Module (IIM), a versatile and adaptive component designed to integrate the information gleaned from these specialized kernels with standard RGB images. The IIM can be seamlessly integrated with a variety of existing object detection frameworks, enhancing their capability to perform accurately in low-light environments, whether through naive edge features or diverse illumination-invariant characteristics, as shown in Fig 1. We further conduct experiments on the ExDark and \(UG^{2}+\)DARK FACE datasets to evaluate our method. Our experimental results demonstrate that the integration of the IIM significantly enhances the detection accuracy of existing methods, leading to substantial improvements in low-light object detection. To summarize, our contributions are as follows:

* We introduce YOLA, a novel framework for object detection in low-light conditions by leveraging illumination-invariant features.

Figure 1: (a): The base detector failed to recognize objects. (b, c) However, when IIM is employed with a simple edge feature, the object is identified. (d, e) Furthermore, the full IIM utilizes a task-driven learnable kernel to extract illumination-invariant features that are richer and more suitable for the detection task than simple edge features.

* We design a novel Illumination-Invariant Module to extract illumination-invariant features without requiring additional paired datasets, and can be seamlessly integrated into existing object detection methods.
* We provide an in-depth analysis of the extracted illumination-invariant paradigm and propose a learning illumination-invariant paradigm.
* Our experiments show YOLA can significantly improve the detection accuracy of existing methods when dealing with low-light images.

## 2 Related work

### General object detection

Current modern object detection methods can be classified as anchor-based and anchor-free. The anchor-based detectors are derived from the sliding-window paradigm, where the dense anchor can be viewed as the sliding-window arranged in spatial space. Subsequently, the anchors are assigned as positive or negative samples based on the matching strategy (i.e., Intersection-over Union (IoU) , Top-K [52; 50]). Common anchor-based methods include R-CNN [16; 15; 40], SSD , YOLOv2 , and RetinaNet , among others. In contrast, the anchor-free detectors liberate the handcraft anchor hyper-parameter setting, enhancing their potential in terms of generalization capability. Prominent methods in anchor-free include YOLOv1 , FCOS , and DETR . Despite the remarkable achievements of both anchor-based and anchor-free detectors in general object detection, they exhibit unsatisfactory performance under low-light conditions.

### Low-light object detection

Object detection in low-light conditions remains a significant challenge. One common line of research involves leveraging image enhancement techniques, such as KIND , SMG , NeRCo , and others [17; 24; 22; 23] to directly improve the quality of the low-light image. The enhanced images are then deployed in the subsequent training and testing stages of detection. However, the objective of image enhancement is inherently different from that of object detection, making this strategy suboptimal. To address this, some researchers [21; 6] explore integrating image enhancement with object detection during the training process. Nevertheless, the task of balancing hyperparameters to equilibrate visual quality and detection performance remains intricate. Recently, Sun et al.  proposed a targeted adversarial attack paradigm aimed at restoring degraded images to ones that are more favorable for object detection. MAET  trained on a low-light synthetic dataset, obtaining the pre-trained model endowed intrinsic structure decomposition ability for downstream lowlight object detection. Further, IA-YOLO  and GDIP  elaborately design the differentiable image processing module to enhance image adaptively for adverse weather object detection. Note that the

Figure 2: The overall pipeline of YOLA.YOLA extracts illumination-invariant features via IIM and integrates them with original images by leveraging a fuse convolution block for the subsequent detector.

aforementioned methods either require a dedicated low-light enhancement dataset or rely heavily on synthetic datasets in training. To mitigate the limitations, a set of methodologies [36; 49; 18] utilize multi-scale hierarchical features and are driven purely by task-specific loss to improve low-light vision. Unlike those methods, we introduce illumination-invariant features to alleviate the effect of illumination on low-light object detection, without requiring additional low-light enhancement datasets or synthetic datasets.

### Illumination invariant representation

Adverse illumination typically degrades the performance on downstream tasks, prompting researchers to explore illumination-invariant techniques to mitigate this impact. For high-level tasks, Wang et al.  proposed an illumination normalization method for Face Recognition. Alshammari et al.  use illumination-invariant image representation to improve automotive scene understanding and segmentation. Lu et al.  convert RGB images to illumination-invariant chromaticity space, preparing for the following feature extraction to achieve traffic object detection in various illumination conditions. For low-level tasks, several physics-based invariants, such as Colour Ratios  (CR) and Cross Colour Ratios  (CCR), are employed to decompose the illumination for intrinsic image decomposition [10; 9; 8]. However, these methods leverage illumination-invariant representations derived from the fixed formulations, which may not adequately capture the diverse and complex illumination scenarios that are specific to downstream applications. In contrast, our method enables the adaptive learning of illumination-invariant features in an end-to-end manner, thereby enhancing compatibility with downstream tasks.

## 3 Method

In this section, we formally introduce YOLA, a novel method for low-light object detection. As illustrated in Fig.2, the key component of YOLA is the Illumination Invariant Module (IIM) focusing on feature learning to derive downstream task-specific illumination-invariant features. These features can be integrated with existing detection modules, enhancing their capability in low-light conditions. Next, we will introduce the derivation of illumination-invariant features and provide a detailed description of IIM's specific implementation.

### Illumination invariant feature

Notation:Let \(I\) represents an image in the standard RGB domain, and let \(C[R,G,B]\) represent the image in the red, green, or blue channel. We define the value in channel \(C\) of a pixel \(p_{i}\) as \(C_{p_{i}}\), where \(i I\) is the pixel index.

Lambertian assumption:According to body reflection term of the dichromatic reflection model, the value of \(C_{p_{i}}\) can be expressed in the discreer form as follows:

\[C_{p_{i}}=m(_{p_{i}},_{p_{i}})e^{C_{p_{i}}}()^{C_{p_{ i}}}(),\] (1)

Here, \(_{p_{i}},_{p_{i}}\) represents surface normal and light direction respectively, and \(m\) denotes the interaction function between them. The term \(e^{C_{p_{i}}}\) represents the spectral power distribution of the illuminant at point \(p_{i}\) in color channel \(C\), and \(^{C_{p_{i}}}\) represents the intrinsic property (reflectance) of the object at point \(p_{i}\) in color channel \(C\).

It becomes apparent that the term \(m\) is determined solely by the positional component, with no impact from the color channels. This observation leads to the strategy of calculating the difference between values of different color channels at the same spatial positions to effectively eliminate the influence of \(m\). To eliminate the term \(e\), we can utilize the assumption that illumination is approximately uniform across adjacent pixels. Consequently, by computing the difference between values of neighboring pixels, we can further further eliminate the influence of \(m\).

Cross color ratio:Taking into consideration two adjacent pixels, denoted as \(p_{1}\) and \(p_{2}\), along with the red (\(R\)) and blue (\(B\)) channels, we can determine the ratio \(M_{rb}\) between the red and blue channels through the following computational procedure:

\[M_{rb}=}B_{p_{2}}}{R_{p_{2}}B_{p_{1}}}.\] (2)Taking the logarithm of \(M_{rb}\) and substituting the pixel values with Eq. 1, we get:

\[ log(M_{rb})=log(m(n_{p_{1}}^{-},l_{p_{1}}^{-}))-log(m(n _{p_{1}}^{-},l_{p_{1}}^{-}))\\ +log(e^{R_{p_{1}}}())-log(e^{R_{p_{2}}}())\\ +log(^{R_{p_{1}}}())-log(^{R_{p_{2}}}())\\ +log(m(n_{p_{2}}^{-},l_{p_{2}}^{-}))-log(m(n_{p_{2}}^{-},l_{p_{2} }^{-}))\\ +log(e^{B_{p_{2}}}())-log(e^{B_{p_{1}}}())\\ +log(^{B_{p_{2}}}())-log(^{B_{p_{1}}}()). \] (3)

With the illumination assumption that \(e^{C_{p_{1}}} e^{C_{p_{2}}}\), the above equation can be further simplified into an illumination-invariant form:

\[ log(M_{rb})=log(^{R_{p_{1}}}())-log( ^{R_{p_{2}}}())\\ +log(^{B_{p_{2}}}())-log(^{B_{p_{1}}}()) \] (4)

By observing the elimination in Eq. 4, we can find that subtraction within the **same channel** eliminates the illumination term (implemented by zero-mean constraint), while **cross-channel** subtraction removes the surface normal and light direction terms, which motivates us to design the learning illumination-invariant paradigms.

In this case, we can use a convolution operation to extract features, as shown in Fig. 2. The extracted features are processed and fused by the IIM before being sent to the detector. When using fixed weights of adjacent pixels with a subtraction value of 1 or \(-1\), we refer to it as IIM-Edge. Next, we will provide a detailed introduction to the IIM.

### Illumination invariant module

While Eq. 4 offers a straightforward and effective method for calculating Illumination Invariant features, its rigidity presents certain limitations. Specifically, the fixed nature of this equation may not adequately capture the diverse and complex variations in illumination that are specific to downstream tasks across different scenarios. To address this, we have evolved the equation into a more adaptable form using convolutional operations. Instead of relying on a single kernel, our approach involves learning a set of convolutional kernels. This strategy not only enhances the robustness of the Illumination Invariant feature extraction but also improves its efficiency. To this end, we propose Illumination Invariant Module comprising two main components, including learnable kernels and a zero-mean constraint. Note that Illumination Invariant Module yield features are inherently illumination invariant at initialization. Subsequent kernel learning is geared towards producing task-specific illumination invariant features for downstream tasks.

Learnable kernel.The goal is to transform the fixed illumination-invariant feature into a learnable form. Specifically, we aim to learn a set of convolutional kernels \(_{1},_{2},_{n}^{ k k}\), where \(n\) represents the number of kernels and \(k\) denotes the kernel size. Here, we extend the fixed feature into a more versatile and generalized form. Let \(p_{i}\) and \(w_{i}\) represent a group pixel position and its corresponding weight within a kernel \(_{n}\), where \(i=0,1, k^{2}\). These parameters enable us to evolve the Cross Color Ratio (CCR) into an adaptable form, enhancing its capability to effectively handle varying illumination conditions. Note that \(w_{i}\) is trainable, rendering the positive or negative polarity inconsequential.

\[M_{rb}=_{i=1}^{k^{2}-1}(}}{B_{p_{i}}})^{w_{i}} (}}{R_{p_{i+1}}})^{w_{i+1}}=_{i=1}^{k^{2}} (}}{B_{p_{i}}})^{w_{i}}\] (5)

To make the extended form still satisfy Illumination Invariant, the logarithm of \(M_{rb}\) should satisfy the following constraints:

\[_{i}^{k^{2}}w_{i}log(e^{R_{p_{i}}}())=0\\ _{i}^{k^{2}}w_{i}log(e^{B_{p_{i}}}())=0\] (6)If the above equation holds true, the \(e\) term and the \(m\) term are eliminated. The final feature can be expressed in a generalized form:

\[log(M_{rb})=_{i}^{k^{2}}w_{i}log(^{R_{p_{i}}}())-_{i}^{k^{2}}w_ {i}log(^{B_{p_{i}}}())\] (7)

Similarly, we can obtain \(log(M_{rg})\) and \(log(M_{gb})\) to form \(f_{_{i}}(I)\).

The resulting features obtained by applying the kernel \(_{i}\) to the image \(I\) denoted as \(f_{_{i}}(I)\), can be expressed as:

(8)

where denotes the convolution.

Zero mean constraint:Drawing from Eq. 6 and the approximation \(e^{R_{p_{1i}}} e^{B_{p_{i}}}\), in the context of convolutional kernels, we simply ensure that the mean of \(_{n}^{ k k}\) to be \(0\), as depicted by:

\[_{n}}=}_{i=1}^{k^{2}}w_{i}=0\] (9)

This constraint is enforced by substituting the mean value from the kernel \(_{n}=_{n}-_{n}}\).

Figure 3: Qualitative comparisons of TOOD detector on both ExDark and \(UG^{2}+\)DARK FACE dataset, where the top 2 rows visualize the detection results from ExDark, and the bottom 2 rows show the results from \(UG^{2}+\)DARK FACE. The images are being replaced with enhanced images generated by LLIE or low-light object methods. Red dash boxes highlight the inconspicuous cases. Zoom in red dash boxes for the best view.

## 4 Experiments

### Implementation details

We evaluate the proposed method using the popular anchor-based detector YOLOv3  and the anchor-free detector TOOD . Both detectors are initially pre-trained on the COCO dataset and subsequently fine-tuned on the target datasets utilizing the SGD  optimizer with an initial learning rate of \(1e\)-\(3\). Specifically, we resize the ExDark dataset images to \(608 608\) and train both detectors for 24 epochs, reducing the learning rate by a factor of 10 at epochs 18 and 23. For the \(UG^{2}+\)DARK FACE dataset, we resize images to \(1500 1000\) for TOOD and maintain the \(608 608\) resolution for YOLOv3 to be consistent with MAET. YOLOv3 is trained for \(20\) epochs, with the learning rate decreased by a factor of \(10\) at \(14\) and \(18\) epochs. TOOD are trained for \(12\) epochs, with the learning rate decreased by a factor of \(10\) at \(8\) and \(11\) epochs. Additionally, we implement a straightforward illumination-invariant model, denoted as **YOLA-Naive**, by removing the IIM and ensuring various illumination features are consistently imposed by an MSE loss. We implement YOLA using the MMDetection toolbox .

### Dataset

We evaluate our proposed method on both real-world scenarios datasets: exclusively dark  (ExDark) and \(UG^{2}+\)DARK FACE . ExDark dataset contains 7363 images ranging from low-light environments to twilight, including 12 categories, 3,000 images for training, 1,800 images for validation, and 2,563 images for testing. We calculate the overall mean average precision (mAP\({}_{50}\)) and mean recall at the IoU threshold of 0.5 as the evaluation metric. \(UG^{2}+\)DARK FACE dataset contains 6,000 labeled face bounding box images, where 5,400 images are allocated for training and 600 images are reserved for testing, and calculating the corresponding recall and mAP\({}_{50}\) as evaluation metrics. Additionally, we also evaluate the generalization ability of our method on the COCO 2017  dataset.

### Low-light object detection

Table 1 presents the quantitative results of YOLOv3 and TOOD detectors on the ExDark dataset, respectively. We report the low-light image enhancement (LLIE) methods, including KIND, SMG, and NeRCo, along with the state-of-the-art low-light object detection methods, DENet, and MAET. Compared to the low-light object detection methods, the LLIE methods fail to achieve satisfactory performance due to inconsistency between human visual and machine perception. The enhancement methodologies prioritize human preferences. However, it is important to note that optimizing for enhanced visual appeal may not align with optimized object detection performance. Despite being the current state-of-the-art in image enhancement techniques, SMG and NeRCo exhibit worse performance compared to KIND when evaluated in the context of object detection tasks. In contrast, end-to-end approaches such as DENet and MAET, which account for machine perception, generally yield superior results in object detection compared to the LLIE methods. Nevertheless, our method remains simple and effective when compared to similar approaches in the same category. Moreover, compared to YOLA-Naive, YOLA exhibits superior performance because its extracted features inherently possess illumination invariance, implying a smaller solution space compared to YOLA-Naive. Specifically, our method achieves the best performance on both anchor-based YOLOv3

  &  &  \\   & recall & mAP\({}_{50}\) & recall & mAP\({}_{50}\) \\  Baseline & 84.6 & 71.0 & 91.9 & 72.5 \\ KIND  & 83.3 & 69.4 & 92.1 & 72.6 \\ SMG  & 82.3 & 68.5 & 91.8 & 71.5 \\ NeRCo  & 83.4 & 68.5 & 91.8 & 71.8 \\ DENet  & 84.2 & 71.3 & 92.6 & 73.5 \\ GDIP  & 84.8 & 72.4 & 92.2 & 72.8 \\ IAT  & 85.0 & 72.6 & 92.9 & 73.0 \\ MAET  & 85.1 & 72.5 & 92.5 & 74.3 \\  YOLA-Naive & 84.8 & 71.6 & 91.8 & 71.6 \\
**YOLA** & **86.1** & **72.7** & **93.8** & **75.2** \\ 

Table 1: Quantitative comparisons of the ExDark dataset based on YOLOv3 and TOOD detectors.

  &  &  \\   & recall & mAP\({}_{50}\) & recall & mAP\({}_{50}\) \\  Baseline & 77.9 & 60.0 & 81.5 & 62.1 \\ KIND  & 76.0 & 58.4 & 82.4 & 63.8 \\ SMG  & 69.3 & 48.9 & 77.1 & 55.8 \\ NeRCo  & 68.9 & 49.1 & 76.8 & 55.6 \\ DENet  & 77.7 & 60.0 & 84.1 & 66.2 \\ GDIP  & 77.8 & 60.4 & 82.1 & 62.9 \\ IAT  & 77.6 & 59.8 & 82.1 & 62.0 \\ MAET  & 77.9 & 59.9 & 83.6 & 64.8 \\  YOLA-Naive & 76.6 & 59.2 & 82.8 & 64.6 \\
**YOLA** & **79.1** & **61.5** & **84.9** & **67.4** \\ 

Table 2: Quantitative comparisons of the \(UG^{2}+\)DARK FACE dataset based on YOLOv3 and TOOD detectors.

and anchor-free detectors TOOD, surpassing the baseline by significant gains of 1.7 and 2.5 mAP, indicative of its superiority and effectiveness. Meanwhile, compared with most LLIE and lowlight object detection techniques, the number of parameters in our YOLA (**0.008M**) is significantly lower, as presented in Table 5. This highlights the potential for our method to be deployed in lightweight practical applications. For a more detailed quantitative comparison, please refer to our appendix.

### Low-light face detection

We have shown the results on the ExDark dataset. Next, we showcase the results on a dataset that includes small-sized objects. Table 2 presents the quantitative results of the detector YOLOv3 and TOOD on \(UG^{2}+\)DARK FACE dataset. Significantly, it is worth noting that most LLIE methods integrated into the YOLOv3 detector fail to achieve satisfactory results. This implies that the utilization of enhancement-based approaches can impair the details of small faces, hindering the learning of useful representations in such images. On the other hand, methods considering the object detection task demonstrate better performance, where YOLA increases the 1.5 mAP, demonstrating its superior performance and generalization capability. For the recently advanced detector TOOD, our method still outperforms these LLIE and low-light object detection methods, achieving a remarkable mAP of 67.4. This underscores YOLA's superior generalization capabilities in improving the performance of both anchor-based and anchor-free detection paradigms.

### Quantitative results

The top 2 rows of Figure 3 show the qualitative results from the ExDark dataset using the TOOD detector, where existing methods exhibit missed detections, highlighted by the red dashed boxes. In contrast, YOLA excels in detecting these challenging cases, demonstrating its superior performance in complex scenarios. The bottom 2 rows exhibit the qualitative results of the \(UG^{2}+\)DARK FACE dataset using the TOOD detector. These faces are typically tiny under low-light conditions, making it difficult for most methods to achieve comprehensive results.

Although our method does not explicitly constrain image brightness, the enhanced images tend to display increased brightness in the final results. The visual results shown in the figures may appear slightly grayish due to the absence of value range constraints on the enhanced images. For image display, we conducted channel-wise normalization.

### Ablation studies

#### 4.6.1 Illumination invariant module

We evaluate the effectiveness of the IIM in detectors TOOD, as presented in Table 3, respectively. The 1st and 5th rows of Table 3 show the baseline detectors evaluated on ExDark and \(UG^{2}+\)DARK FACE dataset. By incorporating the IIM to introduce illumination-invariant features, the detector yields considerable performance gains (2.3 and 4.8 mAP for ExDark and \(UG^{2}+\)DARK FACE, respectively).

#### 4.6.2 Zero mean constraint

By imposing a zero mean constraint on the convolutional kernels, the subtraction formed by the kernels can factor out the illumination items. To evaluate the impact of this constraint, we exclude it from IIM, and the results are shown in Table 3. It is evident that the removal of this constraint leads

   Dataset & Method & AP\({}_{50}\) & AP\({}_{75}\) & mAP \\   & TOOD & 59.0 & 45.3 & 41.7 \\  & + YOLA & **59.4** & **46.0** & **42.3** \\   & TOOD & 57.4 & 43.8 & 40.5 \\  & + YOLA & **58.3** & **44.6** & **41.2** \\   

Table 4: Ablation study for YOLA on COCO 2017val.

   Dataset & IIM & IIM-Edge & \(_{mean}\) & mAP\({}_{50}\) \\   & & & 72.5 \\  & & ✓ & 73.8 \\  & ✓ & & 74.7 \\  & ✓ & & ✓ \\   & & & 62.1 \\  & ✓ & & 64.5 \\   & ✓ & & 66.9 \\   & ✓ & & ✓ \\   

Table 3: The effectiveness of IIM, IIM-Edge and the zero mean constraint \(_{mean}\) based on TOOD. The blank line denotes the baseline.

to a decline in performance, with reductions of 0.3 and 0.5 mAP for TOOD. These results indicate that utilizing the zero mean constraint to mitigate the effects of illumination is beneficial to low-light object detection.

#### 4.6.3 Learnable kernel

The IIM is formed with the learnable kernels, encouraging the illumination-invariant features that are adaptively learned in an end-to-end fashion. In this experiment, we evaluate the fixed kernels (as specified in Eq. 4, also referred to as IIM-Edge), the results of which are shown in Table 3. It outperforms the baseline by 1.3 mAP on ExDark and 2.4 mAP on \(UG^{2}+\)DARK FACE, which demonstrates that the incorporation of illumination-invariant features is beneficial for low-light object detection. Subsequently, we substitute the fixed kernels with the learnable kernels, yielding further gains of 1.4 mAP on ExDark and 2.9 mAP on \(UG^{2}+\)DARK FACE. These results clearly prove the effectiveness of learnable kernels. In addition, we also impose a consistency loss for IIM's output feature to stabilize the kernel learning to prevent trivial solutions within the kernel, mitigating the impact of uneven lighting. (please refer to the appendix A for details).

**Visualization:** Illumination-invariant features exhibit considerable diversity, but the diversity captured by fixed kernels is limited. We visualize and compare the fixed kernel and learnable kernel as shown in Fig. 4. The features yielded by fixed kernels appear relatively uniform, primarily consisting of simple edge features. In contrast, learnable kernels extract more diverse patterns, resulting in visually richer and more informative representations.

### Generalization

In this section, we broaden the application of the YOLA to the general object detection dataset COCO 2017, investigating the YOLA's generalization capability beyond low-light object detection. The metrics mAP (average for IoU [0.5:0.05:0.95]), AP\({}_{50}\), and AP\({}_{75}\) are adopted to evaluate performance on COCO 2017val (also called minival) as presented in Table 4. Specifically, we trained 12 epochs with 8 GPUs and a mini-batch of 1 per GPU in an initial learning rate of \(1e\)-\(2\) by the SGD optimizer on both well-lit and over-lit (generated by brightening the origin image) scenarios. By observing Table 4, we can see that detectors integrated with YOLA in both scenarios exhibit notable improvements in performance.

## 5 Conclusion

In this work, we have revisited the complex challenge of object detection in low-light conditions and demonstrated the effectiveness of illumination-invariant features in improving detection accuracy in such environments. Our key innovation, the Illumination-Invariant Module (IIM), harnesses these features to great effect. By integrating a zero-mean constraint within the framework, we have effectively learned a diverse set of kernels. These kernels are adept at extracting illumination-invariant

Figure 4: Visualization of the features (columns 2 and 4) generated by IIM-Edge and IIM(kernels are normalized for better visibility, we average the features across the channel dimensions and then conduct spatial normalization), along with detection results (columns 1 and 3). Best viewed by zooming in.

features, significantly enhancing detection precision. We believe that our developed IIM module can be instrumental in advancing low-light object detection tasks in future applications.

Acknowledgement:This work was supported in part by National Natural Science Foundation of China (NSFC) under grant No.62372091 and Natural Science Foundation of Sichuan Province under grant Nos. 2023NSFSC0462 and 2023NSFSC1972.