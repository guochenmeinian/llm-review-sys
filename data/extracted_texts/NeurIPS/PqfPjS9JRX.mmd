# The Shaped Transformer: Attention Models

in the Infinite Depth-and-Width Limit

 Lorenzo Noci

Equal contribution. Correspondence to:

lorenzo.noci@inf.ethz.ch, chuning.li@mail.utoronto.ca, mufan.li@mail.utoronto.ca

Chuning Li1

University of Toronto and Vector Institute

Mufan (Bill) Li1

University of Oxford

Bobby He

University of Oxford

Thomas Hofmann

Equal contribution. Correspondence to:

lorenzo.noci@inf.ethz.ch, chuning.li@mail.utoronto.ca, mufan.li@mail.utoronto.ca

Chris Maddison

University of Toronto and Vector Institute

Daniel M. Roy

University of Oxford

###### Abstract

In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and width, thus preventing the notorious issues of rank degeneracy in deep attention models. Finally, we show, through simulations, that the SDE provides a surprisingly good description of the corresponding finite-size model. We coin the name _shaped Transformer_ for these architectural modifications.

## 1 Introduction

Pre-trained large language models have experienced a remarkable increase in popularity due to their eerily human-like ability to puzzle through complex reasoning tasks, solve coding challenges, and produce pages of logically sound text . Arguably, the Transformer is the foundation of these successes . Recent research has found evidence for scaling laws, linking the performance of these architectures to their parameter counts and the quantity of training data, fueling the desire to train deeper and wider models on ever larger datasets in order to unlock new levels of performance .

Bundled with the increased expressivity of deep architectures, however, is increased numerical instability, both in the forward pass and gradients, which hinders training. One of the clearest examples of instability is the so-called rank collapse phenomenon  - the observation that, in Softmax-based attention models, the network's representation of different tokens tend to perfectly align at large depth. The resulting poorly conditioned covariance and correlation between tokens leads to exploding and/or vanishing gradients at initialization, disrupting gradient updates of the affected parameters. This situation violates a well-known guiding principle from the literature of deep signal propagation: a stable covariance is a necessary condition for stable training . In fact,the instability of Transformers is evident when considering the critical role of hyperparameter tuning and the judicious use of normalization layers. In this work, we study Transformers in a novel infinite limit, rectify sources of instability with a novel modification, and derive the SDEs characterizing the covariance and output distribution.

Scaling limits have been used successfully to provide guidance on architecture [16; 17; 18] and tuning hyperparameters settings . Our work represents a contribution in this direction. The ability to use such limits to diagnose instabilities depends on their tractability and faithfulness to real-world (finite) networks. In this regard, not all limits are created equal. In particular, the faithfulness of scaling limits depends critically on how other parameters are scaled with width. One of the simplest (and thus most popular) limits to work with - the "NTK" limit [20; 21; 22; 23; 24] - treats the depth of the network as fixed. As a result, at initialization, this limit does not accumulate sufficient random fluctuations over the depth of the network, leading to deterministic covariance matrices that do not agree with those of standard (finite) networks. Such networks have another defect: they are incapable of learning features in the limit . Various other limits have been studied, towards identifying tractable yet faithful models of initialization and/or training. These include mean field limits [26; 27; 28; 29] and the perturbative regime [30; 31; 32; 33; 34; 35; 36].

This work operates in a relatively new regime - the _proportional_ infinite depth-and-width limit - where depth \(d\) and width \(n\) diverge as the ratio \(d/n\) tends to a positive constant. This limit, first analyzed by Hanin and Nica , has been the recent subject of study in the context of neural network [38; 39; 40; 41; 18]. A related line of work also studied the Lyapunov exponent for products of random matrices [42; 43; 44; 45]. This regime retains the network's stochasticity and, at initialization, has been shown to closely resemble the behaviour of finite architectures, yet still yield a relatively simple limiting description, expressible in terms of stochastic differential equations [40; 18]. In this work, we fully characterize the initial output distributions of a network with skip connections and Softmax-based attention mechanisms, in the proportional infinite-depth-and-width limit.

Inspired by the idea of shaping activation functions [16; 17; 18; 46], our theoretical approach finds an adequately modified attention mechanism via its SDE limit. Our modification involves making the attention matrix closer to the identity, and appropriately choosing the temperature parameter \(\), which re-scales the logits of the Softmax. Similar to shaping activation functions, the temperature scaling we devise linearizes and reduces the saturation of the Softmax, a known source of training instability in Transformers . In order to model the feedforward layer of a Transformer's block, we extend existing results  to derive an SDE for the proportional limit of shaped-ReLU feedforward multi-layer perceptrons (MLPs) with skip connections. Combined, we fully characterize the output distribution of a Transformer with shaped non-linearities (Corollary 4.3).

Figure 1: Our shaped Transformer prevents token representations from becoming perfectly aligned, i.e. rank collapse. Left: mean correlation \(_{}^{}\) of Transformers (Eq. 11) with and without shaped attention (Eq. 9) and Pre-LN . Right: kernel density estimate and histogram of correlations from covariance SDE in Theorem 4.2 and shaped attention NN. Here we note correlation converging to \(1\) implies a poorly conditioned covariance matrix. Simulated with \(n=200,d=150,=1/,_{0}=1,_{0}^{}=0.2\), SDE step size \(0.01\), and \(2^{12}\) samples.

Notably, our modification successfully prevents a poorly conditioned covariance matrix, whereas the vanilla Softmax-based attention model without LayerNorm  fails in this regard, and the corresponding Pre-LN architecture provides only marginal improvements (see Figure 1). Given that our modification is inspired by previous work on shaping activation functions, we coin the terms _shaped attention_ for the proposed attention mechanism and _shaped Transformer_ for the overall architecture that includes the MLP block and residual connections. Through simulations (e.g., Figure 1), we show that the limiting neural covariance SDE approximates the distribution of finite-size Transformers with shaped attention mechanism surprisingly well. We also provide preliminary training experiments for our proposed shaped attention architecture on standard language modeling tasks, demonstrating the feasibility of the new architecture in practice (see Section 5 and Appendix D).

In summary, our contributions are as follows:

1. We study the effect of skip connections in the proportional limit, showing that under a precise relation between the scaling parameters of the shortcut and residual branches, the feature covariance converges to the solution of a weighted version of the neural covariance SDE for MLPs (Theorem 3.2). The dependence on the depth-to-width ratio implies the existence of a stable non-commutative limit for residual networks, complementing the commutative limit studied in Hayou and Yang .
2. We propose _shaped attention_, where we modify the Softmax-based attention mechanism to be a perturbation of the identity. We demonstrate that shaped attention successfully prevents the degeneracy of correlation in contrast to existing Transformer architectures (Figure 1). The enhanced stability in the forward pass is reflected in the gradients, which are also stable with depth, as we empirically show in Figure 2.
3. For the proposed shaped attention architecture, we derive the neural covariance SDE characterizing the initial distribution in the proportional limit (Theorem 4.2). Consequently, we provide the first characterization of Transformer-type architectures, i.e. the shaped Transformer, in the large depth-and-width regime (Corollary 4.3).
4. We provide simulations to validate the theory and to interpret the effects of network hyperparameters on the covariance matrix of the shaped Transformer. Specifically, we study finite time stability of the SDE and provide explicit guidance on hyperparameters to prevent numerical instability.

Figure 2: Comparing gradients norms at initialization for different parameters as a function of depth, with and without shaped attention. The architecture is the same as in Figure 1 but with autoregressive causal masking, and the task is next-token prediction on code data. Left: Value weights \(W_{}^{V}\) for shaped attention, standard Pre-LN, and the original Post-LN block . Right: the same gradient norm plot but for Query weights \(W_{l}^{Q}\). We find that shaping the attention mechanism successfully prevents gradients from vanishing, while unshaped Transformers suffer from rapidly vanishing gradients. Interestingly, only the Post-LN query gradients vanish, but value gradients are stable across depths, which is consistent with the findings of Noci et al. . On the other hand, shaped attention has stable gradients for both parameters inside and outside the Softmax nonlinearity.

The paper is organized as follows: In Section 2, we provide the basic setup and some background on existing results. In Section 3, we generalize the SDE results of M. Li et al.  to include skip connections. This serves as a model to understand the effect of skip connections in isolation from the attention model. In Section 4, we present our main result, first pinpointing the origins of instability in the Softmax, then showing how the modifications underlying _shaped attention_ allow us to derive a non-trivial SDE limit. Finally, in Section 5, we discuss the implications of our results and some future directions. Proofs of all theorems and additional experiments are deferred to the Appendix.

## 2 Background

**Setup.** Let \(X_{}^{m n}\) be the data matrix representing a sequence of \(m\) tokens embedded in \(n\) dimensions at layer \([d]\), where \(d\) is the depth of the network. We elide the explicit dependence on \(\) when it is clear from the context, and use superscript Greek letters to indicate specific tokens' representations, for instance \(x_{}^{}^{n}\) is the \(\)-th row of \(X_{}\). We consider the following attention model with residual connections:

\[X_{+1}= X_{}+ A_{}X_{}\ }W_{}^{V}\] (1)

where \(,\) are parameters that control the strength of the shortcut and residual branch, respectively, \(W_{}^{V}^{n n}\) is the weight matrix of the values, and \(A_{}^{m m}\) is the attention matrix. We consider Softmax-based scaled dot-product attention, where \(A_{}\) has the form:

\[A_{}=(X_{}\ }W_{}^{Q}\ }W_{}^{K,}\ X_{}^{}),\] (2)

where the Softmax is applied row-wise, \(W_{}^{Q},W_{}^{K}^{n n_{k}}\) are additional random weights, and \(\) is a temperature parameter, which controls the entropy of the distribution. Here we let all the weight matrices \(W_{}^{Q},W_{}^{K},W_{}^{V}\) have \((0,1)\)-iid entries. In the case where \(,=1\), with the application of LayerNorm on the residual branch , and with \(=}\), we recover the attention block of the vanilla "Pre-LN" Transformer architecture . Here we note that we pull the conventional \(n^{-1/2}\) factor outside of the weight matrices, which preserves the forward pass, and yields equivalent training dynamics up to a reparameterization of the learning rate . In this work, we consider unnormalized architectures, and control the variance propagation with the condition \(^{2}+^{2}=1\). We are interested in studying the so-called _neural covariance_ for the attention model (Eq. 1) in the proportional limit.

**Neural Covariance.** In deep learning theory, researchers have long sought to understand how networks internally represent different inputs and how different architectural choices affect these representations. The approach followed by work on signal propagation has been to study how the relative alignment of different inputs evolves across the network, as measured by the neural covariance \(V_{}^{}:= x_{}^{},x_{}^{}\) (or \(^{}:=(V_{}^{}V_{}^{})^{-1/2}V_{ }^{}\) if interested only in the correlation). At initialization, characterizations of this covariance structure have been exploited to infer important properties of neural networks [10; 11]. As an example, in the sequential infinite-width-_then_-depth limit, the correlation \(_{d}^{}\) of MLPs is known to converge to a fixed point independent of the input [11; 16]. In this regime, the model is not able to discriminate different data points, which severely hinders training, as the gradient step for the deep layers is taken in the same direction regardless of the input. In the context of Softmax-based attention models, Dong et al.  proved that the feature matrix \(X_{}\) loses rank doubly exponentially fast with depth, and Noci et al.  showed how this leads to vanishing gradients of the queries and keys parameters, thus further highlighting how the stability of forward and backward passes are deeply entangled (see also Figure 2).

**Stabilizing the Effect of Non-Linear Layers.** Central to the issue of degeneracy of the neural covariance are commonly used non-linear activation functions that severely deviate from the identity. The recent line of work of Deep Kernel Shaping (DKS) [17; 16; 18] addresses the issue by considering the cumulative amount of non-linearity throughout layers, and _shaping_ the activation function by making it closer to the identity map. Inspired by this line of work, B. He et al.  devise an initialization for Transformers that avoid the rank collapse problem without the aid of skip connections or LayerNorm.

In an alternative approach, the line of work behind Stable ResNets [50; 51; 52; 53] considers scaling the residual branches by \(=1/}\), and postulates this scaling is sufficient to stabilize the neural 

[MISSING_PAGE_FAIL:5]

Notice how the limiting SDE closely resembles the MLP case (Eq. 3), which is recovered exactly when \(=1\). The only difference is the extra \(2\) factor, which comes from the fact that in our definition each layer has effectively two times the number of weight matrices than the standard formulation for MLPs. As the drift depends solely on the nonlinearity, and the diffusion depends soley on the random weights, only the diffusion variance is doubled. The residual branch parameter \(<1\) dampens both the drift and the variance of the Brownian motion by \(^{2}\), thus it can be interpreted as a time change. In other words, the effect of \(\) at initialization is equivalent to reducing depth-to-width ratio, inline with existing intuitions that ResNets have a lower "effective-depth" . To visualize the stabilizing effect of \(\) on the distribution, in Figure 3 (right) we plot the 95th percentile correlation as a function of \(\). The increasing trend indicates a larger probability of perfect alignment between two tokens. In Figure 3 (left) we plot the densities of both the residual SDE and the corresponding residual network for various values of \(\). Notice how the samples from the SDE well-approximates the histogram of a finite network.

**A Stable Non-Commutative Limit.** Our results complement those of Hayou and Yang , where the authors have shown that for a similar ResNet under the parameter scaling \(=1,=1/\), the depth and width limits _commute_. More precisely, the covariance \(V^{}\) converges to the same limit regardless of the order with respect to which the limit is taken or the depth-to-width ratio. Furthermore, the limit is _deterministic_, and can be described by an ordinary differential equation (ODE). Intuitively, the convergence to a deterministic quantity occurs because \(=1/\) suppresses the random fluctuations enough to vanish in the limit. On the other hand, our results show that for \(,\) constant in \(n,d\), the random fluctuations are on the right order of \(O(n^{-1/2})\) as in the MLP case (Eq. 3), hence they do not vanish in the simultaneous limit. The most notable difference is that our limiting regime is _non-commutative_ as it depends on the depth-to-width ratio of the network. We remark that both regimes prevents degeneracy of covariance for residual architectures, forming two theories that complement each other.

## 4 Neural Covariance SDE for Softmax-Based Attention

### Unshaped Attention and Its Taylor Expansion

A central piece to the neural covariance SDE theory for MLPs  is identifying the exact scaling of shaped activation functions. In particular, the effect of the activations on the covariance Markov chain \(V_{}\) must be on the same order as the random weights in an MLP, thus forming an approximate

Figure 3: Left: Kernel density estimates of correlation \(_{d}^{}\) for various values of the residual strength parameter \(\). In particular, \(=1\) recovers a shaped-ReLU MLP without skip connections, and \(=1/\) is the setting studied in Noci et al.  and Hayou and Yang . Solid lines represent finite networks, while our SDE simulations are presented in dashed lines. Right: 95th percentile of the absolute value of the correlation distribution as a function of \(\). Note reducing \(\) reduces the concentration around \(^{}=1\), and our SDE reliably approximates finite size networks. Simulated with \(n=300,d=100,_{0}^{}=0.2,c_{+}=0,c_{-}=-1\), and \(2^{13}\) samples.

Euler-discretization

\[V_{+1}=V_{}+)}{n}+)^{1/2}_{} }{}+O(n^{-3/2})\,,\] (7)

where \(b,\) are deterministic coefficients, and \(_{}\) are random vectors with zero mean and identity covariance. From here onwards, we use \(O(n^{-p})\) to denote a random variable \(Z\) such that \(n^{p}Z\) has all moments bounded by universal constants (i.e. independent of \(n\)). Since the update can be interpreted as discretization with step size \(n^{-1}\), naturally the Markov chain converges to an SDE. We again note that a stable SDE implies a stable covariance structure for finite size networks.

To achieve the same goal for modified attention mechanisms, we consider a similar approach as M. Li et al.  for smooth activation functions, and Taylor expand the Softmax function in terms of a large temperature parameter \(\). To this end, let \(Y_{}\) to be the matrix of dot-products between queries, and keys, i.e. \(Y_{}:=X_{}\ }W_{}^{Q}\ }W_{}^{K, }\ X_{}^{}\).

More specifically, given a row \(y^{}^{1 m}\) of the logits \(Y_{}^{m m}\), we can Taylor expand the row-wise Softmax in terms of \(^{-1}\):

\[(^{-1}y^{})=^{}+(y^{}-})+m}[(y^{ }-})^{2}-(}^{2}-)^{2}})]+O(^{-3}),\] (8)

where \(}:=_{}y^{}^{}\) and \((y^{})^{2}\) is the vector with squared entries of \(y^{}\), and \(^{m 1}\) is the (column) vector of ones. We note in practice \(\) is often set to \(}\), which is often quite large and allows for asymptotic analysis .

We observe that the zero-th order term \(m^{-1}^{}\) is independent of \(\). Considering the form of the attention block as \(A_{}X_{}}W_{}^{V}\), this yields an update that is no longer a small perturbation of \(V_{}\), regardless of how \(\) is chosen. Therefore, to form a Markov chain like Eq. 7, we actually require \(A_{}\) to be approximately the identity matrix.

### Shaped Attention

To shape the Softmax-attention mechanism as a perturbation of the identity matrix, we propose the following modifications which we call the _shaped attention_5

\[A_{}=I+(^{-1}Y_{})-^ {}\,,=_{0}}\,.\] (9)

The shaped attention presents three modifications to the Softmax attention in Eq. 2. Firstly, the zero-order term \(m^{-1}^{}\) of the Taylor expansion (Eq. 8) is removed as it causes a non-infinitesimal drift in the Markov Chain that ultimately leads to instability in the covariance (see Section 4.1). Secondly, we also observe that when \(\) is very large, the centered Softmax is a perturbation around zero. To recover an approximate Euler-update like in Eq. 7, we simply add back the identity matrix. By biasing the attention matrix towards the identity, we encourage each token to self-attend. This type of modification was also previously considered by B. He et al. . Finally, the Softmax's temperature is chosen to scale as \(=_{0}}\), for some constant \(_{0}>0\), which guarantees a non-degenerate limit as \((d,n)\) (Theorem 4.2). Note that the extra \(\) term is a departure from the standard parameterization.

In Figure 4, we show how removing any of the proposed changes individually alters the neural covariance structure, which becomes degenerate for large depths, while the proposed modifications remain stable. We stress that here for simplicity we focus on attention without masking. Shaped attention can be extended to include masking (e.g. casual masking) by centering each i-th row of the Softmax matrix by a different factor \(1/m_{i}\), where \(m_{i}\) is the number of un-masked tokens in the i-th row.

### Main Result - Neural Covariance SDEs for Shaped Attention Models and Shaped Transformers

Before we state our main results, we will first define a weakened notion of convergence, which is required whenever the drift and covariance coefficients are not Lipschitz. This was also required for the case of shaped MLPs with smooth activations .

**Definition 4.1** (Local Convergence).: _We say the covariance \(V^{(n)}\) converges locally to \(V\) if the stopped process \(\{V^{(n)}_{t T_{r}}\}_{t 0}\) converges to \(\{V_{t T_{r}}\}_{t 0}\) in the sense of Definition 3.1 for all stopping times of the form \(T_{r}=\{t>0:\|V_{t}\| r\}\) with \(r>0\)._

Let the covariance with respect to the average token be defined as \(V^{}:=m^{-1}_{=1}^{m}V^{}\), and the average trace be \(:=m^{-1}_{=1}^{m}V^{}\). We will need to compute a couple of important moments from the Taylor expansion terms of the Softmax (Lemma C.2)

\[ S_{1}^{,}&:=n _{k}^{-1}(Y^{}-})(Y^{}- })=V^{}(V^{}-V^{} -V^{}+V^{})\,,\\ S_{2}^{}&:=n_{k}^{-1}[ (Y^{}-^{})^{2}-()^{2}}- }^{2})]=V^{}(V^{}-2V^ {}+2V^{}-)\,.\] (10)

We are now ready to state our main result.

**Theorem 4.2**.: _Let \(X_{}\) be the hidden layers of a residual attention network defined in Eq. 1 with shaped attention in Eq. 9, parameters \(^{2}+^{2}=1\) and \(=_{0}}\), where \(,,_{0}\) all do not depend on \(d,n\). Then the feature covariance \(V_{}\) converges locally to the solution of the following SDE (in the sense of Definition 4.1)_

\[dV_{t}=b(V_{t})dt+(V_{t})^{1/2}dB_{t}\,, V_{0}=X_{0}X_{0 }^{}\,,\]

_where the drift has the following form_

\[b(V)=}{_{0}^{2}}[}_{,=1}^{ m}V^{}S_{1}^{,}+_{=1}^{m}(V^{ }S_{2}^{}+V^{}S_{2}^{})]_{ }\,,\]

_the diffusion coefficient is defined by \((V)=^{2}(2-^{2})_{}(V)+^{4}_{0}^{-2 }[^{,}]_{,}\), and_

\[^{,}:=}_{,=1}^{m} (V^{}V^{}S_{1}^{,}+V^{ }V^{}S_{1}^{,}+V^{}V^{ }S_{1}^{,}+V^{}V^{}S_{1}^{,})\,.\]

Figure 4: Mean correlation (left) and covariance (right) (in absolute value) under various interventions on the proposed shaped attention. In particular, we remove either one or two of the three modifications from the shaped attention in Eq. 9. For instance “\(^{2}=nn_{k}\), center” indicates that we use the proposed temperature, and we center by \(m^{-1}\), but we do not add the identity matrix, while in “only id” we add the identity matrix but use \(=}\) and do not center. We note in this “only id” case, the covariance remains unstable due to incorrect scaling. Due to exploding covariance, we choose to not include the cases “id, \(^{2}=nn_{k}\)” and “only id” in the correlation plot (but only in the covariance plot). Simulated with \(n=300,d=150,_{0}^{}=0.2\), \(=1/\) and \(2^{13}\) samples.

The drift depends on the shaped attention mechanism through \(S_{1}^{,}\) and \(S_{2}^{}\), the moments of the first and second order terms of the Softmax's Taylor expansion. On the other hand, the diffusion term depends on the attention solely through \(S_{1}\), present in the additional term \(^{,}\). The presence of \(^{,}\) is an intriguing difference compared to shaped ReLU networks, where the diffusion is not affected by the activation function. Both components of the SDE depend on averages over the tokens, reflecting the mixing property of the self-attention mechanism, in which every pair of tokens is compared through dot products to form the attention weights. Finally, notice how the residual branch parameter \(^{2}\) has a dampening effect on the scale of both the drift and the diffusion in a similar way as in fully-connected residual network.

We are now ready to introduce the full shaped Transformer architecture, where we combine the attention and residual layers:

\[Z_{}= X_{}+ A_{}X_{}}W_{}^{ V}\,, X_{+1}= Z_{}+_{s}(Z_{}}W_{}^{})}W_{}^{}\,,\] (11)

where \(A_{}\) is the shaped attention defined by Eq. 9. We note that covariance SDE handle stacking of different layer types very conveniently by simply adding the drift and covariance of the diffusion coefficients, which we summarize in the Corollary below.

**Corollary 4.3** (Shaped Transformer Covariance SDE).: _Let \(X_{}\) be the hidden layers of a shaped transformer defined in Eq. 11 with parameters \(^{2}+^{2}=1\) and \(=_{0}}\), where \(,,_{0}\) all do not depend on \(d,n\). Then the feature covariance \(V_{}\) converges locally to the solution of the following SDE (in the sense of Definition 4.1)_

\[dV_{t}=[b(V_{t})+b_{}(V_{t})]dt+[(V_{t})+ _{}(V_{t})]^{1/2}dB_{t}\,,\] (12)

_where the coefficients are defined in Theorem 3.2 and Theorem 4.2._

### On Finite Time Stability of the SDE and Shaped Attention Networks

Although we did not observe numerical instability in majority of our simulations of the shaped attention networks and the corresponding SDE, we did observe that the drift component \(b(V_{t})\) in Theorem 4.2 is cubic in the entries of \(V_{t}\). Whenever the drift is not Lipschitz as in this case, we do not have general guarantees for the existence of a solution for all time (see the Feller test for explosions (58, Theorem 5.5.29)). In fact, MLPs with smooth activations also yield non-Lipschitz drift coefficients as seen in M. Li et al. (2018).

However, locally Lipschitz coefficients are sufficient to guarantee the existence of local solutions, in the sense of up to a stopping time (59, Proposition 6.9). Not only does this fact help us establish a precise notion of convergence (Definition 4.1), we can also study the practical implications of this for finite sized attention networks. More specifically, we can inspect the effect of architectural changes to a stopping time.

To demonstrate the potential numerical instabilities, we had to choose an _adversarial_ set of parameters: in particular, an unrealistically large norm (approx. \(10\)) for the initial tokens \(X_{0}\), which enlarges the eigenvalues of \(V_{0}\) to the order of \(100\). Given these initial conditions and a large residual connection weight \(\), we were able to consistently generate numerically unstable behaviour in shaped attention networks (see Figure 5 (left)).

That being said, it is very straight forward to stabilize the network by tweaking parameters such as \(,_{0}\) and the depth-to-width ratio of the network. We demonstrate the effect of tuning \(\) on both sample trajectories of the maximum eigenvalue of \(V_{}\) and the stopping time in Figure 5. As we may intuitively expect, tuning \(\) smaller will delay the time scale of numerical instabilities, hence allowing for larger depth networks to remain stable.

## 5 Discussion

**Architecture Design and Hyperparameter Tuning.** Previous work have demonstrated the practical impact scaling limits can have on designing activation functions (16; 17) and tuning hyperparameters (19). We follow this line of motivations and proposed a novel attention mechanism, which successfully stabilizes the covariance structure in arbitrarily deep Transformers (e.g. Figure 1). The natural next step is to investigate the scaling of gradients in the infinite-depth-and-width limit. As Yang et al.  illustrated, the existence of an infinite-width limit for the gradient implies the optimal hyperparameters for the training algorithm will also converge. This type of results allows for tuning of hyperparameters on networks with a much smaller width, yet extends easily to arbitrarily large networks that approximates the same limit, saving massive amounts of computing cost in the process. Given the existence of an infinite-depth-and-width limit for the forward pass, we believe it's possible to extract optimal hyperparameters from networks with not only a much smaller width, but _smaller depth_ as well.

**Preliminary Experiments.** Although this work is primarily theoretical, it is important to consider whether or not the proposed architecture is useful in practice. Given limited computing resources, we chose to only briefly test the feasibility of training the shaped Transformer. Nevertheless, our preliminary experiments show promising results when it comes to training stability. In particular, the shaped Transformer (without LayerNorm) does indeed train at approximately the same speed as well tuned Transformer architectures. Full details of the experiment and results can be found in Appendix D. A more comprehensive set of experiments with different tasks, datasets, and larger networks will be required to confidently determine the practical feasibility of the shaped Transformer, which we defer to future work.

**Training Dynamics and Generalization.** As mentioned in the introduction, the limitations of infinite-width NTK theories motivates our study of the proportional infinite-depth-and-width limit. In particular, to address many of the open problems in deep learning theory, we need a faithful and tractable description of training dynamics. Given the results at initialization, the proportional limit holds the potential for such a theory of training as well. Another promising indicator is that deep networks learn features in the proportional regime , which has been identified as a key advantage of neural networks over kernel methods . A precise theory of training will help us understand other types of instabilities during training and improve existing optimization methods. Furthermore, determining the network which training converges to is a necessary step towards a theory of generalization, as demonstrated by the infinite-width approach . In light of our results, we believe that our theory sets the stage for future work on training and generalization in deep learning.