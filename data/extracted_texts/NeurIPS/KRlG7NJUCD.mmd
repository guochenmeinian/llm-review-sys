# DAW: Exploring the Better Weighting Function for Semi-supervised Semantic Segmentation

Rui Sun\({}^{1}\) &Huayu Mai\({}^{1}\) &Tianzhu Zhang\({}^{1,2}\)&Feng Wu\({}^{1,2}\)

\({}^{1}\)Deep Space Exploration Laboratory/School of Information Science and Technology,

University of Science and Technology of China

\({}^{2}\)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

{issunrui, mai556}@mail.ustc.edu.cn, {tzzhang, fengwu}@ustc.edu.cn

Equal contributionCorresponding author

###### Abstract

The critical challenge of semi-supervised semantic segmentation lies how to fully exploit a large volume of unlabeled data to improve the model's generalization performance for robust segmentation. Existing methods tend to employ certain criteria (weighting function) to select pixel-level pseudo labels. However, the trade-off exists between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels in these methods when handling pseudo-labels without thoughtful consideration of the weighting function, hindering the generalization ability of the model. In this paper, we systematically analyze the trade-off in previous methods when dealing with pseudo-labels. We formally define the trade-off between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels by explicitly modeling the confidence distribution of correct and inaccurate pseudo-labels, equipped with a unified weighting function. To this end, we propose Distribution-Aware Weighting (DAW) to strive to minimize the negative equivalence impact raised by the trade-off. We find an interesting fact that the optimal solution for the weighting function is a hard step function, with the jump point located at the intersection of the two confidence distributions. Besides, we devise distribution alignment to mitigate the issue of the discrepancy between the prediction distributions of labeled and unlabeled data. Extensive experimental results on multiple benchmarks including mitochondria segmentation demonstrate that DAW performs favorably against state-of-the-art methods. Code is available at https://github.com/yuisuen/DAW.

## 1 Introduction

Semantic segmentation is a fundamental task that has achieved conspicuous achievements credited to the recent advances in deep neural networks . However, its data-driven nature makes it heavily dependent on massive pixel-level annotations, which are laborious and time-consuming to gather. To alleviate the data-hunger issue, considerable works  have turned their attention to semi-supervised semantic segmentation, which has demonstrated great potential in practical applications . Since only limited labeled data is accessible, how to fully exploit a large volume of unlabeled data to improve the model's generalization performance for robust segmentation is thus extremely challenging.

In previous literature, pseudo-labeling  and consistency regularization  have emerged as mainstream paradigms to leverage unlabeled data for semi-supervised segmentation. In specific,the pseudo-labeling methods train the model on unlabeled samples with pseudo labels derived from the up-to-date model's own predictions. And the consistency regularization methods encourage the model to produce consistent predictions for the same sample with different perturbation views, following the smoothness assumption . Recently, these two paradigms are often intertwined in the form of a teacher-student scheme [17; 18; 19; 20; 3]. The critical idea involves updating the weights of the teacher model using the exponential moving average (EMA) of the student model, and the teacher model generates corresponding pseudo labels of the perturbed samples to instruct the learning of the student model.

Despite yielding promising results, these methods tend to employ certain criteria (referred to as _weighting function_) to select pixel-level pseudo labels, considering that the quality of the chosen pseudo-labels determines the upper bound of performance. On the one hand, naive pseudo-labeling methods such as Pseudo-Label  recruit all pseudo labels into training, assuming that each pseudo label is equally correct (i.e., weighting function can be regarded as a constant function). However, as training progresses, maximizing the utilization of pseudo-labels tends to lead to confirmation bias , which is a corollary raised by erroneous pseudo-labels (i.e., _inaccurate yet utilized pseudo-labels_). On the other hand, a series of threshold-based pseudo-labeling methods  such as FixMatch  attempt to set a high threshold (e.g., 0.95) to filter out pixel-level pseudo-labels with low confidence (i.e., the weighting function can be considered as a step function that jumps at 0.95). Although tangling the quality of pseudo-labels can alleviate noise, the strict criteria inevitably lead to the content of numerous unconfident yet correct pseudo-labels (i.e., _correct yet discarded pseudo-labels_), hindering the learning process. To make matters worse, the negative impact is inevitably amplified by inbuilt low-data regimes of semi-supervised segmentation, leading to sub-optimal results. As a compromise, AEL  ad hoc defines the weighting function as a power function, which assigns weights conditioned on the confidence of pseudo-labels, that is, convincing pseudo-labels will be allocated more weights. However, the lack of sophisticated consideration and the arbitrary control of hyperparameters (i.e., tunable power) for the weighting function inevitably compromise its capability. In a nutshell, the trade-off exists between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels in these methods when handling pseudo-labels without thoughtful consideration of the weighting function, hindering the generalization ability of the model. Then, the question naturally arises: _How to explore the better weighting function to effectively alleviate the negative impact raised by the trade-off?_

In this work, we systematically analyze the trade-off in previous methods that hinder the model's learning when dealing with pseudo-labels in semi-supervised semantic segmentation. We formally define the trade-off between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels by explicitly modeling the confidence distribution of correct and inaccurate pseudo-labels, equipped with a unified weighting function. In specific, two Gaussian functions excelling at the maximum entropy property are devised to fit the confidence distribution of correct (positive distribution in Figure 1 (a)) and inaccurate (negative distribution in Figure 1 (a)) pseudo-labels using maximum likelihood estimation, respectively. The parameters of the Gaussian functions are updated via the exponential moving average (EMA) in pursuit of perceiving the learning status of the model. Then the trade-off can be naturally derived by calculating the expectations of inaccurate yet utilized pseudo-labels (depicted as \(E_{1}\) in Figure 1 (a)), and correct yet discarded pseudo-labels (displayed as

Figure 1: Illustration of our motivation. (a) shows the trade-off between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels by explicitly modeling the confidence distribution of correct and inaccurate pseudo-labels. (b) illustrates the negative equivalence impact on generalization performance raised by the trade-off. (c) (d) (e) summarize the models inevitably face a trade-off when dealing with pseudo-labels. Our method can guarantee the theoretical optimal solution by minimizing the negative impact.

\(E_{2}\) in Figure 1 (a)) respectively, on top of the weighting function and the corresponding confidence distribution. Now, we are prepared to propose the Distribution-Aware Weighting (_DAW_) function striving to minimize the negative equivalence impact on generalization performance raised by the trade-off, i.e., minimizing \(E1+E2\). By leveraging functional analysis, we find an interesting fact that the optimal solution for the weighting function is a hard step function, with the jump point located at the intersection of the two confidence distributions. Note that the dedicated weighting function is theoretically guaranteed by reconciling the intrinsic tension between \(E1\) and \(E2\) (see Figure 1 (b)), and is free of setting thresholds manually compared to previous methods. Besides, considering the imbalance issue caused by the discrepancy between the prediction distributions of labeled and unlabeled data, we propose distribution alignment to further unlock the potential of the weighting function and enjoy the synergy. In practice, the weighting function generated by DAW determines the criteria for selecting pseudo-labels to minimize the negative equivalence impact of the trade-off, minimizing \(E1+E2\) (see Figure 1 (e)), which is conducive to model training. In this way, the model improves, benefiting from the effective probe of reliable pseudo-labels. And in turn, the positive distribution will be maximally separated from the negative one, leading to a simultaneous decrease in both \(E1\) and \(E2\) (see Figure 1 (c) and Figure 1 (d)), which is conducive to the generation of the weighting function.

Extensive experiments on mainstream benchmarks demonstrate that our method performs favorably against state-of-the-art semi-supervised semantic segmentation methods, proving that it can better exploit unlabeled data. Besides, we further validate the robustness of DAW on the electron microscopy mitochondria segmentation task, which involves images with dense foreground objects and cluttered backgrounds, making it more challenging to discriminate the reliability of the pseudo-labels.

## 2 Distribution-Aware Weighting

In this section, we first formulate the semi-supervised semantic segmentation problem as preliminaries (Section 2.1), and then formally define the trade-off between inaccurate yet utilized pseudo-labels (\(E1\)), and correct yet discarded pseudo-labels (\(E2\)) by explicitly modeling the confidence distribution of correct and inaccurate pseudo-labels, equipped with a unified weighting function (Section 2.2).

   Method & Pseudo-Label & Fixmatch & AEL & Ours \\  \(f(p) g(p)\) & & & & \\  \(f(p) g(p)\) & & & & \\  \(E1\) & 1 & \([(}{^{+}})-(}{^{+}})]\) & \()^{2}+(^{-})^{2}}{^{+}}\) & \([(}{^{+}})-(}{^{+}})]\) \\ \(-(1+p^{-}) g(1)\) & & \(-(1+p^{-}) g(1)\) & \([(}{^{+}})-(}{^{+}})]\) \\ \(E2\) & 0 & \(1-[(}{^{+}})-(}{^{+}})]\) & \(-^{+}(^{+})^{2}^{+}(}{^{+}})\) & \(1-[(}{^{+}})-(}{^{+}})]\) \\ \(+(1-p^{-}) g(1)\) & & \(+(1-^{-})(^{+})^{2}^{+}(1)\) & \(1+[(}{^{+}})-(}{^{+}})]\) & \(1+[(}{^{+}})-(}{^{+}})]\) \\ \(E1+E2\) & \(1+[(}{^{+}})-(}{ ^{+}})]\) \\ -[(}{^{+}})-(}{^{+}})]\) \\ +_{0}^{+}^{}(p^{-})-^{+}(p^{-})\,|\,dp\) & \(1+[(}{^{+}})-(}{ ^{+}})]\\ -[(}{^{+}})-(}{^{+}})]\) \\ -[(}{^{+}})-(}{^{+}})]\) \\ -[(}{^{+}})-(}{^{+}})]\) \\ \(-[(}{^{+}})-(}{^{+}})]\) \\ \(_{0}^{}(}{^{+}})-^{+}(p^{-})\,|\,dp\) & \(1+[(}{^{+}})-(}{ ^{+}})]\\ -[(}{^{+}})-(}{^{+}})]\) \\ -[(}{^{+}})-(}{^{+}})]\) \\ \(-[(}{^{+}})-(}{^{+}})]\) \\ \(_{1}^{}(}{^{+}})-^{+}(p^{-})\,|\,dp\) & Smaller than all of them! \\   

Table 1: We analyze the learning process of the mainstream methods for semi-supervised semantic segmentation systematically and uniformly abstract the criteria they adopt to select pseudo labels as _weighting function_\(f(p)\) conditioned on the _confidence_\(p\) of pseudo-labels. There are _inherent distributions_\(g(p)\) for positive and negative pseudo-labels(Gaussian distribution is employed as an approximation).

Based on the analysis, we propose the distribution-aware weighting function (DAWF) to avoid performance degradation raised by the trade-off (Section 2.3). Finally, distribution alignment (DA) is devised to alleviate the discrepancies between the confidence distributions of labeled and unlabeled data. (Section 2.4).

### Preliminaries

In semi-supervised semantic segmentation, given a set of labeled training images \(_{l}=\{_{i}^{l},_{i}^{l}\}_{i=1}^{N _{l}}\) and a large amount of unlabeled images \(_{u}=\{_{i}^{u}\}_{i=1}^{N_{u}}\), where \(N_{l}\) and \(N_{u}\) denote the number of labeled and unlabeled images, respectively, and \(N_{u} N_{l}\). Let \((_{ij}^{*})^{C}\) denotes the prediction of the \(j\)-th pixel in the \(i\)-th labeled (or unlabeled) image, and \(*\{l,u\}\), \(C\) is the number of categories. Then the supervised loss \(_{s}\) can be formulated as,

\[_{s}=}_{i=1}^{N_{t}}_{j=1}^{WH} _{ce}(_{ij}^{l},(_{ij}^{l})),\] (1)

where \(W\) and \(H\) represent the width and height of the input image, \(_{ce}\) denotes the standard pixel-wise cross-entropy loss, and \(_{ij}^{l}\) denotes the ground-truth label from \(_{l}\). Considering most methods [22; 18; 19; 10; 3; 23] tend to employ certain criteria (weighting function) to attempt to select reliable pseudo labels, we formulate the unsupervised loss \(_{u}\) as weighted cross-entropy for the convenience of introducing the weighting function \(f(p_{ij})\),

\[_{u}=}_{i=1}^{N_{u}}_{j=1}^{WH}f( p_{ij})_{ce}(}_{ij}^{u},(^{s}( ^{w}(_{ij}^{u})))),\] (2)

where \(^{w}\)/\(^{s}\) denotes weak/strong perturbation to encourage the model to produce consistent predictions, \(_{ij}^{u}\) denotes \((^{w}(_{ij}^{u}))\), i.e., prediction under the weak perturbation view. And \(}_{ij}^{u}\) = \((_{ij}^{u})\) is the one-hot pseudo-label, \(f(p_{ij})\) is a weighting function conditioned on \(p_{ij}\), and \(p_{ij}\) =\((_{ij}^{u})\), denotes the maximum confidence of the prediction. Then we define the overall loss function as \(=_{s}+_{u}\).

### E1-E2 Trade-off from Unified Weighting Function

We formally define the trade-off between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels by explicitly modeling the confidence distribution of correct and inaccurate pseudo-labels, equipped with a unified weighting function. We instantiate the different biases inherent in the trade-off of previous methods and reveal their tight connection with the capability of the model. We start by defining the pseudo-label confidence distribution.

Orthogonal to other previous models, we assume that the confidence distribution of correct (\(g^{+}(p)\), positive distribution) and inaccurate (\(g^{-}(p)\), negative distribution) pseudo-labels follows a truncated Gaussian distribution with mean \(^{+}/^{-}\) and standard deviation \(^{+}/^{-}\), formulated as,

\[g^{+}(p)=^{+}}[-)^{2}}{2(^{+})^{2}}],&  p 1\\ 0,&,\] (3)

where 1/\(=(}{^{+}})-(}{^{+}})\) denotes the normalization factor, \(\) is the cumulative distribution function of the standard normal distribution. Note that \(p\) =\(((_{ij}^{l}))\), denotes the maximum confidence of the prediction from labeled data, so it must meet the condition of \(p\). Similarly,

\[g^{-}(p)=^{-}}[-)^{2}}{2(^{-})^{2}}],&  p 1\\ 0,&,\] (4)

where \(1/=(}{^{-}})-(}{^{-}})\). The reason we choose Gaussian is its valuable maximum entropy property, please refer to the supplementary material for more details. Then we estimate the mean and standard deviation of the positive \(g^{+}(p)\) and negative distribution \(g^{-}(p)\), respectively, resorting to maximum likelihood estimation,

\[^{+}=}_{i=1}^{N_{l}}^{+}}_{j=1}^{N_ {l}^{+}}p_{ij}^{+},(^{+})^{2}=}_{i=1}^{N_{l }}^{+}}_{j=1}^{N_{i}^{+}}(p_{ij}^{+}-^{+})^{2},\] (5)

where \(p_{ij}^{+}\) denotes the prediction confidence, where \(*{argmax}((_{ij}^{l}))\) on the labeled data equals the ground truth \(_{ij}^{l}\), and \(N_{i}^{+}\) is the number of \(p_{ij}^{+}\) in the \(i\)-th image. For the negative distribution \(g^{-}(p)\), the parameters are evaluated in the same way, except that the predictions involved in the calculation are not equal to the ground truth. Note that we only consider predictions from labeled data to evaluate the Gaussian distribution equipped with ground truth, rather than estimating biases raised from unlabeled data with noisy pseudo-labels. Then the parameters of the Gaussian functions are updated via the exponential moving average (EMA) in pursuit of perceiving the learning status of the model in a dynamic manner,

\[_{t}^{+}=m_{t-1}^{+}+(1-m)^{+},(_ {t}^{+})^{2}=m(_{t-1}^{+})^{2}+(1-m)N_{i}}{_{i} N_{i}-1}(^{+})^{2},\] (6)

where unbiased variance is adopted for EMA, \(_{0}^{+}\) and \((_{0}^{+})^{2}\) are initialized as \(1/C\) and 1.0 respectively. A similar way also works for the negative distribution \(g^{-}(p)\).

Then the trade-off can be naturally derived by calculating the expectations of inaccurate yet utilized pseudo-labels (\(E1\)), and correct yet discarded pseudo-labels (\(E2\)) respectively, on top of the weighting function \(f(p)\) and the corresponding confidence distribution \(g^{-}(p)\)/\(g^{+}(p)\).

**Definition 3.1** Inaccurate yet utilized pseudo-labels, \(E1\).

\[E1=_{g^{-}}[f(p)]=_{}}^{1}f(p) g^{-}(p)dp.\] (7)

**Definition 3.2** Correct yet discarded pseudo-labels, \(E2\).

\[E2=1-_{g^{+}}[f(p)]=1-_{}}^{1}f(p) g^{+}(p )dp.\] (8)

After formally defining the trade-off between \(E1\) and \(E2\), it is natural to measure the impact of negative equivalence effects (i.e., \(E1\)+\(E2\)), considering the trade-off between \(E1\) and \(E2\), where an increase in one necessitates a decrease in the other.

**Definition 3.3** Negative equivalence effect of the trade-off, \(E1\)+\(E2\).

\[E1+E2=1+_{}^{1}f(p)[g^{-}(p)-g^{+}(p)]dp,\] (9)

Then, we systematically analyze the trade-off in previous methods as tabulated in Table 1. For more detailed derivations, please refer to the supplementary material. (1) For example, naive pseudo-labeling methods such as Pseudo-Label  enroll all pseudo labels (\(E2=0\)) into training. However, as training progresses, maximizing the utilization of pseudo-labels tends to a confirmation bias raised by erroneous pseudo-labels (\(E1=1\)). (2) And for threshold-based pseudo-labeling methods such as FixMatch , which attempts to set a high threshold (0.95) to filter out pixel-level pseudo-labels with low confidence (_small value_ of \(E1\) caused by the proximity of \(t=0.95\) to 1). However, the strict criteria inevitably lead to the contempt of numerous unconfident yet correct pseudo-labels (_large value_ of \(E2\) caused by the proximity of \(t=0.95\) to 1). (3) As a compromise, AEL  ad hoc defines the weighting function as a power function, which assigns weights conditioned on confidence. That is, convincing pseudo-labels will be allocated more weight. However, the lack of sophisticated consideration and the arbitrary control of hyperparameters (tunable power) for the weighting function inevitably compromise its capability (not guaranteeing the lowest negative equivalence effect).

### Distribution-Aware Weighting Function

Then we seek to explore a better weighting function equipped with the formal trade-off definition, aiming at minimizing the negative equivalence impact raised by the trade-off, that is, minimizing\(E1+E2\),

\[_{f(p)}&E1+E2=1+_{}}^{1}f(p) [g^{-}(p)-g^{+}(p)]dp,\\ &0 f(p) 1,\] (10)

By leveraging functional analysis, we find an interesting fact that the optimal solution for the weighting function is a hard step function, with the jump point located at the intersection of the two confidence distributions, formulated as,

\[f^{*}(p)=\{1,&t^{*} p 1\\ 0,&., t^{*}=((_{2}^{2}-4 _{1}_{3})^{}-_{2})/(2_{1} )\,,\] (11)

where \(_{1}=(^{+}^{-})^{2}/(^{-})^{2}, _{2}=2[^{+}(^{-})^{2}-^{-}(^{+})^{2}]\), \(_{3}=(^{+}^{-})^{2}-(^{-}^{+})^ {2}+2(^{+}^{-})^{2}[(^{-})/( ^{+})]\) and \(p=\)max\((_{ij}^{u})\) denotes the confidence of the prediction from unlabeled data. Please refer to the supplementary material for more detailed derivations. Note that the dedicated weighting function \(f^{*}(p)\) is theoretically guaranteed by reconciling the intrinsic tension between \(E1\) and \(E2\) (see Table 1) and is free of setting thresholds manually compared to previous methods.

### Distribution Alignment

Furthermore, considering the imbalance issue caused by the discrepancy between the prediction distributions of labeled and unlabeled data, we propose distribution alignment (DA) to further unlock the potential of the distribution-aware weighting function. In specific, we define the confidence distributions from labeled data and unlabeled data as expectations \(_{_{l}}[(_{ij}^{l})]\) and \(_{_{u}}[(_{ij}^{u})]\), respectively. Both of these are estimated in the form of EMA in each batch as the training progresses, denoted as \(}_{_{l}}[(_{ij}^{l})]\) and \(}_{_{u}}[(_{ij}^{u})]\). Then we use the ratio between the expectations of labeled and unlabeled to normalize the each prediction \(_{ij}^{u}=q(_{ij}^{u})\) on unlabeled data, formulated as,

\[(_{ij}^{u})=(_{ij}^{u} }_{_{l}}[(_{ij}^{u}) ]}{}_{_{u}}[(_{ij}^{ u})]}),\] (12)

where \(()\) denotes the normalization operation used to constrain the probabilities to sum up to 1. Then we bring the normalized probability back to Equation 2 to calculate the loss weight of each pseudo-label after alignment,

\[_{u}=}_{i=1}^{N_{u}}_{j=1}^{WH}f^ {*}(((_{ij}^{u})))_{ee}(}_{ij}^{u},(^{s}(^{w}(_{ij}^{u}))) ),\] (13)

where \(}_{ij}^{u}\) = \(((_{ij}^{u}))\). In this way, the distribution-aware weighting function is rewarded with better generalization, benefiting from more equal learning of labeled and unlabeled data, mitigating the issue of distribution imbalance, and enjoying the synergy. The algorithm flow is shown in the supplementary material.

## 3 Experiments

### Experimental Setup

**Datasets:** (1) PASCAL VOC 2012  is an object-centric semantic segmentation dataset, containing 21 classes with 1,464 and 1,449 finely annotated images for training and validation, respectively. Some researches [30; 19] augment the original training set (e.g., _classic_) by incorporating the coarsely annotated images in SBD , obtaining a training set (e.g., _blender_) with 10,582 labeled samples. (2) Cityscapes is an urban scene understanding dataset with 2,975 images for training and 500 images for validation.

**Implementation Details:** For a fair comparison, we follow the common practice and use ResNet  as our backbone and DeepLabv3+ as the decoder. We set the crop size as \(513 513\) for PASCAL and \(801 801\) for Cityscapes, respectively. For both datasets, we adopt SGD as the optimizer with the same batch size of 16 and different initial learing rate, which is set as 0.001 and 0.005 for PASCAL and Cityscapes. We use the polynomial policy to dynamically decay the learning rate along the whole training and assemble the channel dropout perturbation  to improve the generalization ability of the model. We train the model for 80 epochs on PASCAL and 240 epochs on Cityscapes, using \(8\) NVIDIA GeForce RTX 3090 GPUs.

### Comparison with State-of-the-art Methods

We conduct experiments on two popular benchmarks including PASCAL VOC 2012 and Cityscapes and make a fair comparison with SOTA semi-supervised semantic segmentation methods. We consistently observe that our DAW outperforms all other methods under all partition protocols on all datasets with different backbones, which strongly proves the effectiveness of our method.

**Results on PASCAL VOC 2012 Dataset.** Table 2 shows the comparison of our method with the SOTA methods on PASCAL _classic_ and _blender_ set. Specifically, on the PASCAL _classic_ set, our method outperforms the supervised-only (_Sup.-only_) model by 29.7%, 22.1%, 14.7%, 10.9% under the partition protocols of 1/16, 1/8, 1/4 and 1/2, respectively with ResNet-101. Our method also significantly outperforms the existing semi-supervised SOTA methods under all data partition protocols. Taking the recently proposed method AugSeg  as an example, the performance gain of our approach reaches to +4.3% under 1/16 partition protocol with ResNet-50. The same superiority of our method can also be observed on the PASCAL _blender_ set.

**Results on Cityscapes Dataset.** Table 3 compares DAW with SOTA methods on the Cityscapes dataset. DAW achieves consistent performance gains over the _Sup.-only_ baseline, obtaining improvements of 11.9%, 7.3%, 6.0% and 2.9% under 1/16, 1/8, 1/4 and 1/2 partition protocols with ResNet-50, respectively. We can also see that over all protocols, DAW outperforms the SOTA methods, e.g., DAW excels to iMAS  by 1.1% under the 1/16 partition with ResNet-101.

**Qualitative Results.** We compare qualitative results of our DAW with different SOTA methods. As shown in Figure 3, DAW also shows more powerful segmentation performance in fine-grained details (see the first and second row in Figure 3). With the help of the optimal weighting function, DAW demonstrates superior abilities in most scenarios.

### Ablation Study and Analysis

To look deeper into our method, we perform a series of ablation studies on PASCAL _classic_ set under 92 partition protocol with ResNet-50 to analyze each component of our DAW, including the **D**istribution-**A**ware **W**eighting **F**unction (DAWF) and the **D**istribution **A**lignment (DA). The baseline method is UniMatch .

**Effectiveness of Components.** In Table 4, "None" denotes there is no threshold for pseudo-label during the training (i.e., Pseudo-Label ) while "Fixed" indicates that a fixed threshold is set (i.e., UiMatch ). A certain performance lift compared with the baseline can be observed owing tothe introduction of Distribution-Aware Weighting Function and Distribution Alignment. (1) The utilization of DAWF brings a 1.1% improvement of mIoU, demonstrating that the negative impact raised by the \(E1+E2\) trade-off is effectively alleviated. (2) DA brings further accuracy gains, indicating that the existence of the discrepancy between the distributions of labeled and unlabeled data may cause a bottleneck in learning. For better visualization, we employ the unused annotations of "unlabeled data" to calculate the ground-truth distribution on the unlabeled data. And as shown in Figure 5, there is a relatively large gap between the distributions, and DA can effectively relieve it, further unlocking the potential of the weighting function and enjoy the synergy.

**Hyperparameter Evaluations.** As shown in Table 5, it can be observed that the performance is optimal with \(m=0.99\).

**Scalability for Other Scenarios.** We further conduct extra experiments on Lucchi [37; 38; 39; 40; 41; 42] to evaluate the scalability of our method. Figure 6 shows the image and ground-truth of Lucchi dataset, presenting a common problem in electron microscope images that the instances are very small and scattered. This calls for more reliable supervision in training under a semi-supervised setting. As shown in Table 6, DAW outperforms other competitive methods in the electron microscopy domain, indicating that our method can provide more reliable supervision.

**Comparison of Distribution.** As shown in Figure 4, the distributions of different methods are almost the same. As the learning goes on, the discrepancy between the positive and negative distributions of ours becomes larger (simultaneously shown in Figure 2) while the others almost no change. A large discrepancy between the positive and negative distributions means that we can filter out as many negative samples while recruiting as many positive samples as possible, which is conducive to model training. This is the fundamental reason behind why our method outperforms other methods.

## 4 Related Work

**Semi-Supervised Learning.** Semantic segmentation is a fundamental task that has achieved conspicuous achievements credited to the recent advances in deep neural networks [43; 44; 45; 46; 47; 48]. However, its data-driven nature makes it heavily dependent on massive pixel-level annotations, which are laborious and time-consuming to gather. To alleviate the data-hunger issue, considerable works have turned their attention to semi-supervised learning. Designing appropriate and effective supervision signals for unlabelled data is the core problem of semi-supervised learning. Previous research can be summarized into two learning schemes: self-training and consistency regularization. Self-training based methods [49; 12; 10; 50] aim to train the model based on the pseudo-labels generated by the up-to-date optimized model for the unlabelled data. Consistency regularization-based methods aim to obtain prediction invariance under various perturbations, including input perturbation [51; 52], feature perturbation  and network perturbation [54; 55; 56; 57]. The recently semi-supervised methods, MixMathch  and FixMatch  combine these two techniques together and achieve state-of-the-art performance. Based on the paradigm of existing semi-supervised learning methods, our method explores a better weighting function for the pseudo-label scheme during training.

**Semi-Supervised Semantic Segmentation.** Semi-supervised semantic segmentation aims at pixel-level classification with limited labeled data. Recently, following the paradigm of semi-supervised learning, many semi-supervised semantic segmentation methods also focus on the design of self-training [26; 27; 5] and consistency regularization [59; 60; 53; 61] strategies. U\({}^{2}\)PL  proposes to

   Method & \(1/32(5)\) & \(1/16(10)\) & \(1/8(20)\) \\  _Sup.-only_ & \(45.7\) & \(57.4\) & \(61.8\) \\  MT  & \(71.8\) & \(72.4\) & \(75.4\) \\ CCT  & \(84.7\) & \(85.4\) & \(85.8\) \\ CPS  & \(84.5\) & \(84.6\) & \(85.8\) \\ 
**DAW (Ours)** & **85.9** & **86.6** & **87.6** \\   

Table 6: Quantitative results of different SSL methods on Lucchi dataset. We report mIoU (\(\%\)) under various partition protocols. The **best** is highlighted in bold.

Figure 6: Visualization of Lucchi dataset.

make sufficient use of unreliable pseudo-labeled data. CCT  adopts a feature-level perturbation and enforces consistency among the predictions from different decoders. More recently, SOTA semi-supervised segmentation methods also integrate both technologies for better performance. PseudoSeg , AEL  and UCC  propose to use the pseudo-labels generated from weak augmented images to constrain the predictions of strong augmented images. In this paper, we shed light on semi-supervised semantic segmentation based on pseudo-labeling and strive to explore better strategies for using pseudo-labels.

## 5 Conclusion

In this paper, we propose DAW to systematically analyze the trade-off in previous methods that hinder the model's learning. We formally define the trade-off between inaccurate yet utilized pseudo-labels, and correct yet discarded pseudo-labels by explicitly modeling the confidence distribution of correct and inaccurate pseudo-labels, equipped with a unified weighting function. Experiments show the effectiveness.

## 6 Acknowledgments

This work was partially supported by the National Defense Basic Scientific Research Program (Grant JCKY2020903B002).