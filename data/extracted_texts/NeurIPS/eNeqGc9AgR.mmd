# Flatten Anything:

Unsupervised Neural Surface Parameterization

Qijian Zhang\({}^{1}\), Junhui Hou\({}^{1}\), Wenping Wang\({}^{2}\), Ying He\({}^{3}\)

\({}^{1}\)Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China

\({}^{2}\)Department of Computer Science and Engineering, Texas A&M University, Texas, USA

\({}^{3}\)School of Computer Science and Engineering, Nanyang Technological University, Singapore

qijizhang3-c@my.cityu.edu.hk, jh.hou@cityu.edu.hk

wenping@tamu.edu, yhe@ntu.edu.sg

Corresponding author. This work was supported in part by the National Natural Science Foundation of China Excellent Young Scientists Fund 62422118, and in part by the Hong Kong Research Grants Council under Grants 11219324 and 11219422.

###### Abstract

Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications. Traditional parameterization approaches are designed for high-quality meshes laboriously created by specialized 3D modelers, thus unable to meet the processing demand for the current explosion of ordinary 3D data. Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmentation) for pre-processing. In this paper, we introduce the _Flatten Anything Model (FAM)_, an unsupervised neural architecture to achieve global free-boundary surface parameterization via learning point-wise mappings between 3D points on the target geometric surface and adaptively-deformed UV coordinates within the 2D parameter domain. To mimic the actual physical procedures, we ingeniously construct geometrically-interpretable sub-networks with specific functionalities of surface cutting, UV deforming, unwrapping, and wrapping, which are assembled into a bi-directional cycle mapping framework. Compared with previous methods, our FAM directly operates on discrete surface points without utilizing connectivity information, thus significantly reducing the strict requirements for mesh quality

Figure 1: _**Flatten Anything Model (FAM)** for neural surface parameterization: (a) input 3D models; (b) learned UV coordinates; (c) texture mappings; (d) learned cutting seams.

and even applicable to unstructured point cloud data. More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries. Extensive experiments demonstrate the universality, superiority, and inspiring potential of our proposed neural surface parameterization paradigm. Our code is available at [https://github.com/keeganhk/FlattenAnything](https://github.com/keeganhk/FlattenAnything).

## 1 Introduction

Surface parameterization intuitively refers to the process of flattening a 3D geometric surface onto a 2D plane, which is typically called the parameter domain. For any 3D spatial point \((x,y,z)\) lying on the underlying surface, we explicitly map it to a 2D planar coordinate \((u,v)\) while satisfying certain continuity and distortion constraints. Building such point-to-point mappings is also known as UV unwrapping, which serves as an indispensable component in modern graphics rendering pipelines for texture mapping and is also widely used in many downstream geometry processing applications, such as remeshing, mesh completion/compression, detail transfer, surface fitting, and editing, etc.

Theoretically, for any two geometric surfaces with identical/similar topological structures, there exists a bijective mapping between them. Nevertheless, when the topology of the target 3D surface becomes complicated (e.g., with high genus), one must pre-open the original mesh to a sufficiently-developable disk along appropriate cutting seams. Consequently, the current industrial practice for implementing UV unwrapping is typically composed of two stages: (1) manually specifying some necessary cutting seams on the original mesh; (2) applying mature disk-topology-oriented parameterization algorithms (e.g., LSCM , ABF ) which have been well integrated into popular 3D modeling software (e.g., Blender, Unity, 3Ds Max) to produce per-vertex 2D UV coordinates. In practice, such stage-wise UV unwrapping pipeline still shows the following limitations and inconveniences:

**1)** Commonly-used surface parameterization algorithms are designed for well-behaved meshes, which are typically produced by specialized 3D modelers and technical artists. However, with the advent of user-generated content (UGC) fueled by the rapidly growing 3D sensing, reconstruction , and generation  techniques, there emerges an urgent need for dealing with ordinary 3D data possibly with unruly anomalies, inferior triangulations, and non-ideal geometries.

**2)** Despite the existence of a few early attempts at heuristic seam generation , the process of finding high-quality cutting seams in practical applications still relies on manual efforts and personal experience. Hence, the entire workflow remains subjective and semi-automated, leading to reduced reliability and efficiency, especially when dealing with complex 3D models that users are unfamiliar with. Besides, since cutting is actually achieved via edge selection, one may need to repeatedly adjust the distribution of mesh edges to allow the cutting seams to walk through.

**3)** The procedures of cutting the original mesh into a disk and then flattening the resulting disk onto the parameter domain should have been mutually influenced and jointly optimized; otherwise, the overall surface parameterization results could be sub-optimal.

In recent years, there has emerged a new family of neural parameterization approaches targeted at learning parameterized 3D geometric representations via neural network architectures. The pioneering works of FoldingNet  and AtlasNet  are among the best two representatives, which can build point-wise mappings via deforming a pre-defined 2D lattice grid to reconstruct the target 3D shape. RegGeoNet  and Flattening-Net  tend to achieve fixed-boundary rectangular structurization of irregular 3D point clouds through geometry image  representations. However, these two approaches cannot be regarded as real-sense surface parameterization due to their lack of mapping constraints. DiffSR  and Nuvo  focus on the local parameterization of the original 3D surface but with explicit and stronger constraints on the learned neural mapping process. Their difference lies in that DiffSR aggregates multi-patch parameterizations to reconstruct the original geometric surface, while Nuvo adaptively assigns surface points to different charts in a probabilistic manner.

In this paper, we make the first attempt to investigate neural surface parameterization featured by both global mapping and free boundary. We introduce the Flatten Anything Model (FAM), a universal and fully-automated UV unwrapping approach to build point-to-point mappings between 3D points lying on the target geometric surface and 2D UV coordinates within the adaptively-deformed parameter domain. In general, FAM is designed as an unsupervised learning pipeline (i.e., no ground-truth UVmaps are collected), running in a per-model overfitting manner. To mimic the actual physical process, we ingeniously design a series of geometrically-meaningful sub-networks with specific functionalities of surface cutting, UV deforming, 3D-to-2D unwrapping, and 2D-to-3D wrapping, which are further assembled into a bi-directional cycle mapping framework. By optimizing a combination of different loss functions and auxiliary differential geometric constraints, FAM is able to find reasonable 3D cutting seams and 2D UV boundaries, leading to superior parameterization quality when dealing with different degrees of geometric and topological complexities. Comprehensive experiments demonstrate the advantages of our approach over traditional state-of-the-art approaches. Conclusively, our FAM shows the following several major aspects of characteristics and superiorities:

* Compared with traditional mesh parameterization approaches [15; 33; 29], FAM directly operates on discrete surface points and jointly learns surface cutting, which can be insensitive to mesh triangulations and is able to deal with non-ideal geometries and arbitrarily-complex topologies. Besides, such a pure neural learning architecture can naturally exploit the powerful parallelism of GPUs and is much easier for implementing, tuning, and further extension.
* Another distinct advantage lies in the smoothness of parameterization. Since mesh parameterization computes parameters solely for existing vertices, for any point on the mesh that is not a vertex, these approaches resort to interpolation within its encompassing triangle to determine the corresponding UV coordinate. Such practice fails to guarantee smoothness, particularly across edges, and this issue is further exacerbated in low-resolution meshes. By contrast, FAM exploits the inherent smooth property  of neural networks to learn arbitrary point-to-point mappings, thereby ensuring smoothness for all points on the target surface.
* Different from previous multi-patch local parameterization frameworks [11; 2; 36], FAM focuses on global parameterization, a more valuable yet much harder problem setting.
* Different from previous fixed-boundary structurization frameworks [41; 42], FAM deforms the 2D UV parameter domain adaptively and regularizes the learned neural mapping explicitly, thus significantly reducing discontinuities and distortions.

## 2 Related Works

### Traditional Parameterization Approaches

Early approaches formulate mesh parameterization as a Laplacian problem, where boundary points are anchored to a pre-determined convex 2D curve [7; 9], tackled by sparse linear system solvers. Such a linear strategy is appreciated for its simplicity, efficiency, and the guarantee of bijectivity. However, the rigidity of fixing boundary points in the parameter domain usually leads to significant distortions. In response to these issues, free-boundary parameterization algorithms [33; 19] have been proposed, offering a more flexible setting by relaxing boundary constraints. Although these approaches bring certain improvements, they often struggle to maintain global bijectivity. More recent state-of-the-art approaches [29; 35] shift towards minimizing simpler proxy energies by alternating between local and global optimization phases, which not only accelerate convergence but also enhance the overall parameterization quality. OptCuts  explores joint optimization of surface cutting and mapping distortion. Despite the continuous advancements, the working mechanisms of all these approaches fundamentally rely on the connectivity information inherent in mesh structures, casting doubt on their applicability to unstructured point clouds. Moreover, mesh-oriented parameterization approaches typically assume that the inputs are well-behaved meshes possessing relatively regular triangulations. When faced with input meshes of inferior quality characterized by irregular triangulations and anomalies, remeshing becomes an indispensable procedure. Additionally, how to deal with non-ideal geometries and complex topologies (e.g., non-manifolds, multiply-connected components, thin structures) has always been highly challenging.

Unlike surface meshes, unstructured point clouds lack information regarding the connectivity between points, and thus are much more difficult to parameterize. Hence, only a relatively limited amount of prior works  have focused on the parameterization of surface point clouds. Early studies investigate various specialized parameterization strategies for disk-type [10; 1; 40], genus-0 [44; 17], genus-1 , well-sampled  or low-quality  point clouds. Later approaches achieve fixed-boundary spherical/rectangular parameterization through approximating the Laplace-Beltrami operators  or Teichmuller extremal mappings . More recently, FBCP-PC  further pursues free-boundary conformal parameterization, in which a new point cloud Laplacian approximation scheme is proposed for handling boundary non-convexity.

### Neural Parameterization Approaches

Driven by the remarkable success of deep learning, there is a family of recent works applying neural networks to learn parameterized 3D geometric representations. FoldingNet  proposes to deform a uniform 2D grid to reconstruct the target 3D point cloud for unsupervised geometric feature learning. AtlasNet  applies multi-grid deformation to learn locally parameterized representations. Subsequently, a series of follow-up researches inherit such a "folding-style" parameterization paradigm as pioneered by [39; 11] and investigate different aspects of modifications. GTIF  introduces graph topology inference and filtering mechanisms, empowering the decoder to preserve more representative geometric features in the latent space. EleStruc  proposes to perform shape reconstruction from learnable 3D elementary structures, rather than a pre-defined 2D lattice. Similarly, TearingNet  adaptively breaks the edges of an initial primitive graph for emulating the topology of the target 3D point cloud, which can effectively deal with higher-genus or multi-object inputs. In fact, the ultimate goal of the above line of approaches is to learn expressive shape codewords by means of deformation-driven 3D surface reconstruction. The characteristics of surface parameterization, i.e., the mapping process between 3D surfaces and 2D parameter domains, are barely considered.

In contrast to the extensive research in the fields of deep learning-based 3D geometric reconstruction and feature learning, there only exist a few studies that particularly focus on neural surface parameterization. DiffSR  adopts the basic multi-patch reconstruction framework  and explicitly regularizes multiple differential surface properties. NSM  explores neural encoding of surface maps by overfitting a neural network to an existing UV parameterization pre-computed via standard mesh parameterization algorithms [38; 29]. DA-Wand  constructs a parameterization-oriented mesh segmentation framework. Around a specified triangle, it learns to select a local sub-region, which is supposed to be sufficiently developable to produce low-distortion parameterization. Inheriting the geometry image (GI)  representation paradigm, RegGeoNet  and Flattening-Net  propose to learn deep regular representations of unstructured 3D point clouds. However, these two approaches lack explicit constraints on the parameterization distortions, and their local-to-global assembling procedures are hard to control. More recently, Nuvo  proposes a neural UV mapping framework that operates on oriented 3D points sampled from arbitrary 3D representations, liberating from the stringent quality demands of mesh triangulation. This approach assigns the original surface to multiple charts and ignores the packing procedure, thus essentially differing from our targeted global parameterization setting.

## 3 Proposed Method

Our FAM operates on discrete spatial points lying on the target 3D geometric surface for achieving global free-boundary parameterization in an unsupervised learning manner. Denote by \(^{N 3}\) a set of unstructured points without any connectivity information, we aim at point-wisely parameterizing (i.e., row-wisely mapping) \(\) onto the planar domain to produce UV coordinates \(^{N 2}\).

**Technical Motivation.** The most straightforward way of learning such a surface flattening process is to directly impose certain appropriate regularizers over the resulting 2D UV coordinates \(}\). _However, due to the missing of ground-truths, it is impossible to explicitly supervise the generation of \(\), and it empirically turns out that designing and minimizing such regularizers cannot produce satisfactory results._ Alternatively, another feasible scheme is to choose an opposite mapping direction to wrap the adaptively-deformed and potentially-optimal 2D UV coordinates \(}^{N 2}\) onto the target surface by generating a new set of unstructured points \(}^{N 3}\), which should approximate the input \(\) by minimizing certain point set similarity metrics. Under ideal situations where \(}\) and \(\) are losslessly matched, we can equivalently deduce the desired UV coordinate of each point in \(\) (i.e., for a certain 3D point \(_{i}\) perfectly matched with a generated 3D point \(}_{j}}\), which is row-wisely mapped from \(}_{j}}\), then we know the UV coordinate of \(_{i}\) should be \(}_{j}\)). _However, due to the practical difficulty of reconstructing point sets with high-precision point-wise matching, the generated \(}\) is only able to coarsely recover the target surface, thereby \(}\) cannot be treated as the resulting 2D UV coordinates of the input \(\)_. Still, despite the inaccuracy of such inverse 2D-to-3D wrapping process, it provides valuable cues indicating how the target 3D surface is roughly transformed from the 2D parameter domain, which naturally motivates us to combine the two opposite mapping processes into a bi-directional joint learning architecture. _To build associations between the unwrapping and wrapping processes and promote mapping bijectivity, we further introduce cycle mapping mechanismsand impose consistency constraints_. Thus, our FAM is designed as a bi-directional cycle mapping workflow, in which all sub-networks are _parameter-sharing_ and _jointly-optimized_.

### Bi-directional Cycle Mapping

As illustrated in Figure 2, there are two parallel learning branches comprising a series of geometrically-meaningful sub-networks built upon point-wisely shared multi-layer perceptrons (MLPs):

* The deforming network \(_{d}\) (**Deform-Net**) takes a uniform 2D lattice as input, and deforms the initial grid points to potentially-optimal UV coordinates.
* The wrapping network \(_{w}\) (**Wrap-Net**) wraps the potentially-optimal 2D UV coordinates onto the target 3D geometric surface.
* The surface cutting network \(_{c}\) (**Cut-Net**) finds appropriate cutting seams for transforming the original 3D geometric structure to an open and highly-developable surface manifold.
* The unwrapping network \(_{u}\) (**Unwrap-Net**) assumes that its input surface has already been pre-opened, and flattens the 3D points onto the 2D parameter domain.

#### 3.1.1 2d\(\)3d\(\)2D Cycle Mapping

The upper learning branch begins with a pre-defined 2D lattice with \(N\) grid points uniformly sampled within \([-1,1]^{2}\), denoted as \(^{N 2}\). To perform adaptive UV space deformation, we employ an offset-based coordinate updating strategy by feeding \(\) into the Deform-Net to produce another set of 2D UV coordinates \(}^{N 2}\), which can be formulated as:

\[}=_{d}()=^{}_{d}([^{ }_{d}();])+, \]

where \([*;*]\) denotes channel concatenation, \(^{}_{d}:^{2}^{h}\) and \(^{}_{d}:^{(h+2)}^{2}\) are stacked MLPs. Intuitively, the initial 2D grid points are first embedded through \(^{}_{d}\) into the \(h\)-dimensional latent space, concatenated with itself, and then mapped onto the 2D planar domain through \(^{}_{d}\). The learned offsets are point-wisely added to the initial grid points to produce the resulting \(}\).

After that, we perform 2D-to-3D wrapping by feeding the generated \(}\) into the Wrap-Net to produce \(}^{N 3}\), which is expected to roughly approximate the target 3D geometric structure represented by the input 3D point set \(\). In the meantime, we use three more output channels to produce normals \(^{n}}^{N 3}\). The whole network behavior can be described as:

\[[};^{n}}]=_{w}(})=^ {}_{w}([^{}_{w}(});}]), \]

where \(^{}_{w}:^{2}^{h}\) and \(^{}_{w}:^{(h+2)}^{6}\) are stacked MLPs, and the resulting 6 output channels are respectively split into \(}\) and \(^{n}}\).

Figure 2: Illustration of bi-directional cycle mapping, composed of (a) 2D\(\)3D\(\)2D cycle mapping branch, and (b) 3D\(\)2D\(\)3D cycle mapping branch. Modules with the same color share network parameters. (c) shows the learned cutting seams. (d) shows the checker-image texture mapping.

In order to construct cycle mapping, we further apply the Cut-Net on the reconstructed \(}\), which is transformed into an open 3D surface manifold \(}_{}^{N 3}\), as given by:

\[}_{}=_{c}(})=_{c}^{ }([_{c}^{}(});}])+}, \]

where \(_{c}^{}:^{3}^{h}\) and \(_{c}^{}:^{(h+3)}^{3}\) are stacked MLPs.

Once we obtain the highly-developable 3D surface manifold \(}_{}\), it is straightforward to perform 3D-to-2D flattening through the Unwrap-Net, which can be formulated as:

\[}_{}=_{u}(}_{})=_{u}(}_{}), \]

where \(_{u}:^{3}^{2}\) is simply implemented as stacked MLPs, and the resulting \(}_{}^{N 2}\) is expected to be row-wisely equal to \(}\).

#### 3.1.2 3d\(\)2d\(\)3d Cycle Mapping

The bottom learning branch directly starts by feeding the input 3D point set \(\) into the Cut-Net to produce an open 3D surface manifold \(_{}\), which can be formulated as:

\[_{}=_{c}()=_{c}^{}([ _{c}^{}();])+. \]

After that, we apply the Unwrap-Net on the generated \(_{}\) to produce the desired 2D UV coordinates \(\), as given by:

\[=_{u}(_{})=_{u}(_{ }). \]

In order to construct cycle mapping, we further apply the Wrap-Net on the generated \(\) for recovering the target 3D geometric structure, which can be formulated as:

\[[_{};_{}^{}]= _{w}()=_{w}^{}([_{w}^{}();]), \]

where \(_{}\) is expected to be row-wisely equal to \(\). When the input surface points are accompanied by normals \(^{}^{N 3}\), we also expect that the normal directions of \(^{}\) and the generated \(_{}^{}\) should be row-wisely consistent.

**Remark.** Throughout the overall bi-directional cycle mapping framework, _only \(\) is regarded as the desired 2D UV coordinates that are row-wisely mapped from the input 3D surface points \(\)_, all the others (e.g., \(}\) and \(}_{}\)) just serve as intermediate results. Morevoer, it is also worth emphasizing that, _when the training is finished, any arbitrary surface-sampled 3D points can be fed into our FAM to obtain point-wise UV coordinates_.

**Extraction of Cutting Seams.** The extraction of points located on the learned cutting seams, which we denote as \(=\{_{i}\}\), can be conveniently achieved by comparing the mapping relationships between the input \(\) and the parameterized \(\).

Specifically, since \(\) and \(\) are mapped in a row-wise manner, for each input 3D point \(_{i}\) and its K-nearest neighbors \(\{_{i}^{(k)}\}_{k=1}^{K_{}}\) within \(\), we can directly know their 2D UV coordinates, denoted as \(_{i}\) and \(\{_{i}^{(k)}\}_{k=1}^{K_{}}\). The maximum distance value \(_{i}\) between \(_{i}\) and its neighboring points can be computed as:

\[_{i}=(\{\|_{i}-_{i}^{(k)}\|_{2}\}_{k=1}^{K_{ }}). \]

Then, \(_{i}\) will be determined to locate on the cutting seam if \(_{i}\) is larger than a certain threshold \(T_{}\). Collecting all such points within \(\) can deduce a set of seam points \(\), and it is observed that in this way no seam points would be identified if cutting the original 3D surface is actually unnecessary.

### Training Objectives

As an unsupervised learning framework, FAM is trained by minimizing a series of carefully-designed objective functions and constraints, whose formulations and functionalities are introduced below.

**Unwrapping Loss.** The most fundamental requirement for \(\) is that any two parameterized coordinates cannot overlap. Hence, for each 2D UV coordinate \(_{i}\), we search its K-nearest-neighbors \(\{_{i}^{(k)}\}_{k=1}^{K_{u}}\), and penalize neighboring points that are too close, as given by:

\[_{}=_{i=1}^{N}_{k=1}^{K_{u}}(0,-\| _{i}-_{i}^{(k)}\|_{2}), \]where the minimum distance value allowed between neighboring points is controlled by a threshold \(\).

**Wrapping Loss.** The major supervision of the upper learning branch is that the generated point set \(}\) should roughly approximate the original 3D geometric structure represented by \(\). Since \(}\) and \(\) are not row-wisely corresponded, we use the commonly-used Chamfer distance \((*;*)\) to measure point set similarity, as given by:

\[_{}=(};). \]

Here, there is no need to impose the unwrapping loss over \(}\), since we empirically find that optimizing the CD loss naturally suppresses clustered point distribution. Besides, we do not optimize \((};)\), since we observe degraded performances, possibly because in earlier training iterations \(}\) and \(\) are far from optimal and thus can interfere with each other.

**Cycle Consistency Loss.** Our bi-directional cycle mapping framework aims to promote consistencies within both 3D and 2D domains, which are natural to learn thanks to our symmetric designs of the functionalities of sub-networks. Thus, the cycle consistency loss is formulated as:

\[_{}=\|-_{}\|_{1}+\|}-}_{}\|_{1}+(^{ };^{}_{}), \]

where \((*;*)\) computes point-wise cosine similarity between \(^{}_{}\) and ground-truth normals \(^{}\), which can be removed when ground-truth normals are not provided or hard to compute.

**Mapping Distortion Constraint.** We regularize the distortion of the learned neural mapping function by exploiting differential surface properties, which can be conveniently deduced via the automatic differentiation mechanisms in common deep learning programming frameworks.

Throughout the bi-directional cycle mapping framework, 2D UV coordinates \(\) and \(}\) are respectively mapped to 3D surface points \(_{}\) and \(}\). For convenience, here we denote the two neural mapping functions as \(f:^{2}^{3}\) and \(g:^{2}^{3}\). We compute the derivatives of \(f\) and \(g\) with respect to \(\) and \(}\) at each 2D UV point \((u,v)\) to obtain the Jacobian matrices \(_{f}^{3 2}\) and \(_{g}^{3 2}\):

\[_{f}=(f_{u}\;\;f_{v}),\;_{g}=(g_{u}\;\;g_{v}) \]

where \(f_{u}\), \(f_{v}\), \(g_{u}\), \(g_{v}\) are three-dimensional vectors of partial derivatives. Then we further deduce the eigenvalues of \(_{f}^{T}_{f}\) and \(_{g}^{T}_{g}\), which are denoted as \((_{f}^{1},_{f}^{2})\) and \((_{g}^{1},_{g}^{2})\). Thus, we can promote conformal (i.e., angle-preserving) parameterizations by constraining the following regularizer:

\[_{}=\;_{}\|_{f}^{1}- _{f}^{2}\|_{1}+\;_{}}\|_{g}^{1}- _{g}^{2}\|_{1}. \]

## 4 Experiments

We collected a series of 3D surface models with different types and complexities of geometric and/or topological structures for experimental evaluation. We made qualitative and quantitative comparisons with SLIM , a state-of-the-art and widely-used mesh parameterization algorithm. Additionally, we also compared our FAM with FBCP-PC  for unstructured and unoriented (i.e., without normals) 3D point cloud parameterization. Finally, we conducted ablation studies to demonstrate the effectiveness of the proposed bi-directional cycle mapping framework and the necessity of Cut-Net, together with different aspects of verifications for comprehensively understanding the characteristics of our approach. Due to page limitations, detailed technical implementations and additional experimental results are presented in our Appendix

**Comparison with SLIM .** Considering that SLIM is only able to be applied on disk-topologies or surfaces with open boundaries, we collected \(8\) such testing models for parameterization, using its officially-released code. For the training of our FAM, we uniformly sample \(10,000\) points from the original mesh vertices at each optimization iteration. During testing, the whole vertex set is fed into our FAM to produce per-vertex UV coordinates. Then, we can perform checker-map texture mappings for qualitative evaluation. For quantitative evaluation, we computed conformality metrics (_the lower, the better_) by measuring the average of the absolute angle differences between each corresponding angle of the 3D triangles and the 2D parameterized triangles. As shown in Figure 3 and Table 1, our FAM outperforms SLIM both qualitatively and quantitatively on all testing models.

**Parameterization with Different Geometric and Topological Complexities.** We conducted more comprehensive experimental evaluations, as displayed in Figure 4 and Table 2, showing the universality and robustness of our approach.

**Comparison with FBCP-PC .** Since FAM operates on discrete surface-sampled points without any dependence on connectivity information, applying our approach to parameterize unstructured and unoriented 3D point clouds is straightforward and basically seamless. The only modification is to remove the supervision of point-wise normals, i.e., the last term \((^{};^{}_{})\) in Eqn. (11). As compared in Figure 5 and Table 3, our performances are not better than FBCP-PC. However, we must point out that FBCP-PC requires manually specifying indices of boundary points (arranged in order) as additional inputs. Hence, the comparisons are actually quite unfair to us.

**Ablation Studies.** We evaluated the necessity of our two-branch joint learning architecture. First, we removed the upper 2D\(\)3D\(\)2D branch to show the resulting \(\). Second, we removed the bottom 3D\(\)2D\(\)3D branch to show UV coordinates \(}\) by performing nearest-neighbor matching between \(\) and \(}\). As illustrated in Figure 6, removing any of the two branches will cause different degrees of performance degradation. Furthermore, we verified the necessity of Cut-Net in the whole learning pipeline. As shown in Figure. 7, removing Cut-Net does not lead to obvious performance degradation for models that are simpler to open (e.g., _human-face_ and _mobius-strip_), yet for the other complex models the removal of Cut-Net causes highly-distorted surface flattening. Without learning offsets, the inherent smoothness of neural works can impede "tearing" the originally-continuous areas on the 3D surface, thus hindering the creation of cutting seams.

  Model & _human-face_ & _human-head_ & _car-shell_ & _spiral_ & _human-hand_ & _shirt_ & _three-men_ & _camel-head_ \\   SLIM & 0.635 & 0.254 & 0.411 & 0.114 & 0.609 & 0.443 & 0.645 & 0.349 \\  FAM & **0.074** & **0.094** & **0.037** & **0.087** & **0.145** & **0.166** & **0.162** & **0.088** \\  

Table 1: Quantitative comparisons of our FAM and SLIM in terms of parameterization conformality.

  Model & _brain_ & _cow_ & _fundisk_ & _human-body_ & _lion_ & _mail_ & _nefertiti_ \\  Conf. Metric & 0.263 & 0.210 & 0.105 & 0.198 & 0.192 & 0.162 & 0.117 \\   Model & _bucket_ & _dragon_ & _mobius-strip_ & _rocker-arm_ & _torus-double_ & _three-holes_ & _fertility_ \\  Conf. Metric & 0.179 & 0.311 & 0.062 & 0.143 & 0.126 & 0.142 & 0.166 \\  

Table 2: Quantitative conformality metrics of our parameterization results.

Figure 3: Comparison of UV unwrapping and texture mapping results on different (a) open surface models produced by (b) our FAM and (c) SLIM, where the 2D UV coordinates are color-coded by ground-truth point-wise normals to facilitate visualization.

In addition, we conducted quantitative evaluations of self-intersection. Given a triangular mesh, we check if any pair of triangles overlaps in the UV space, and then measure the proportion of overlapped pairs to the total amount of triangle pairs. As reported in Table 4, self-intersection are inevitable both in FAM and SLIM, but the proportions of self-intersected triangles are very small. In practice, it is not hard to apply post-processing refinement to slightly adjust the distribution of UV coordinates to further relieve or even eliminate self-intersection issues. For point cloud parameterization, we evaluated our robustness to noises in Figure 8, where we can observe that FAM shows stable performances to noisy input conditions (with 1%, 2%, and 4% Gaussian noises for point position perturbation).

Finally, we performed stress tests on a Hilbert-space-filling-shaped cylinder model in Figure 9a. It is observed that our FAM obtains the basically optimal solution. Still, as shown in Figure 9b, processing the highly-complicated ShapeNet-style CAD model with rich interior structures and many multi-layer issues shows inferior UV unwrapping quality. Although our learned cutting seams are generally reasonable and texture mapping looks relatively regular from outside, it is hard to deal with the complicated interior structures.

Figure 4: Display of surface parameterization results produced by our FAM. (a) input 3D models; (b) learned UV coordinates; (c) texture mappings; (d) learned cutting seams.

Figure 5: Point cloud parameterization achieved by our FAM (left) and FBCP-PC (right).

## 5 Conclusion

We proposed FAM, the first neural surface parameterization approach targeted at global free-boundary parameterization. Our approach is universal for dealing with different geometric/topological complexities regardless of mesh triangulation quality and even applicable to unstructured point clouds. More importantly, our approach automatically learns appropriate cutting seams along the target 3D surface, and adaptively deforms the 2D UV parameter domain. As an unsupervised learning framework, our FAM shows significant potentials and practical values. The current technical implementations still have several aspects of limitations. Our working mode of per-model overfitting cannot exploit existing UV unwrapping results (despite their limited amounts) as ground-truths for training generalizable neural models. Besides, a series of more advanced properties, such as shape symmetry, cutting seam visibility, seamless parameterization, have not yet been considered.

   Testing Models & FAM & SLIM \\   Open-Surface Models (as in Figure 3) & 0.156\% & 0.133\% \\  Higher-Genus Models (as in Figure 4) & 0.204\% & N/A \\   

Table 4: Quantitative self-intersection metrics of Figure 8: Applying FAM to point clouds added with different levels of Gaussian _noises_.

Figure 6: Ablation results produced by (a) our complete FAM framework, (b) FAM without the upper 2D\(\)3D\(\)2D learning branch, (c) FAM without the bottom 3D\(\)2D\(\)3D learning branch.