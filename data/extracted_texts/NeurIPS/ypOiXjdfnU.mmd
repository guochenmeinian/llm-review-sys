# Emergent Correspondence from Image Diffusion

Luming Tang &Menglin Jia1&Qianqian Wang1

Cheng Perng Phoo&Bharath Hariharan

Cornell University

Equal contribution.

###### Abstract

Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models _without any explicit supervision_. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPAir-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io.

## 1 Introduction

Drawing correspondences between images is a critical primitive in 3D reconstruction , object tracking , video segmentation , image and video editing . This problem of drawing correspondence is easy for humans: we can match object parts not only across different viewpoints, articulations and lighting changes, but even across drastically different categories (e.g., between cats and horses) or different modalities (e.g., between photos and cartoons). Yet, we rarely if ever get explicit correspondence labels for training. The question is, can computer vision systems similarly learn accurate correspondences without any labeled data at all?

There is indeed some evidence that contrastive self-supervised learning techniques produce good correspondences as a side product of learning on unlabeled data . However, in this paper, we look to a new class of self-supervised models that has been attracting attention: diffusion-based generative models . While diffusion models are primarily models for image synthesis, a key observation is that these models produce good results for image-to-image translation  and image editing . For instance, they can convert a dog to a cat without changing its pose or context . It would appear that to perform such editing, the model must implicitly reason about correspondence between the two categories (e.g., the model needs to know where the dog's eye is in order to replace it with the cat's eye). We therefore ask, do image diffusion models learn correspondences?

We answer the question in the affirmative by construction: we provide a simple way of extracting correspondences on real images using pre-trained diffusion models. These diffusion models  have at the core a U-Net  that takes noisy images as input and produces clean images as output. As such they already extract features from the input image that can be used for correspondence. Unfortunately, the U-Net is trained to _de-noise_, and so has been trained on _noisy_ images. Our strategyfor handling this issue is simple but effective: we _add noise_ to the input image (thus simulating the forward diffusion process) before passing it into the U-Net to extract feature maps. We call these feature maps (and through a slight abuse of notation, our approach) **DIf**fusion **F**ea**T**ures (**DIFT**). DIFT can then be used to find matching pixel locations in the two images by doing simple nearest neighbor lookup using cosine distance. We find the resulting correspondences are surprisingly robust and accurate (Fig. 1), even across multiple categories and image modalities.

We evaluate DIFT with two different types of diffusion models, on three groups of visual correspondence tasks including semantic correspondence, geometric correspondence, and temporal correspondence. We compare DIFT with other baselines, including task-specific methods, and other self-supervised models trained with similar datasets and similar amount of supervision (DINO  and OpenCLIP ). Although simple, DIFT demonstrates strong performance on all tasks without any additional fine-tuning or supervision, outperforms both weakly-supervised methods and other self-supervised features, and even remains on par with the state-of-the-art supervised methods on semantic correspondence.

## 2 Related Work

**Visual Correspondence.** Establishing visual correspondences between different images is crucial for various computer vision tasks such as Structure-from-Motion / 3D reconstruction [2; 73; 60; 74], object tracking [22; 97], image recognition [63; 81; 9] and segmentation [50; 47; 72; 28]. Traditionally, correspondences are established using hand-designed features, such as SIFT  and SURF . With the advent of deep learning, methods that learn to find correspondences in a supervised-learning regime have shown promising results [46; 14; 42; 35]. However, these approaches are difficult to scale due to the reliance on ground-truth correspondence annotations. To overcome difficulties in collecting a large number of image pairs with annotated correspondences, recent works have started looking into how to build visual correspondence models with weak supervision  or self-supervision [92; 37]. Meanwhile, recent works on self-supervised representation learning  has yielded strong per-pixel features that could be used to identify visual correspondence [84; 3; 10; 28]. In particular, recent work has also found that the internal representation of Generative Adversarial Networks (GAN)  could be used for identifying visual correspondence [99; 62; 57] within certain image categories. Our work shares similar spirits with these works: we show that diffusion models could generate features that are useful for identifying visual correspondence on general images. In addition, we show that features generated at different timesteps and different layers of the de-noising process

Figure 1: Given a red source point in an image (far left), we would like to develop a model that automatically finds the corresponding point in the images on the right. Without any fine-tuning or correspondence supervision, our proposed diffusion features (DIFT) could establish semantic correspondence across instances, categories and even domains, e.g., from a duck to a penguin, from a photo to an oil-painting. More results are in Figs. 15 and 16 of Appendix E.

encode different information that could be used for determining correspondences needed for different downstream tasks.

**Diffusion Model**[78; 32; 79; 41] is a powerful family of generative models. Ablated Diffusion Model  first showed that diffusion could surpass GAN's image generation quality on ImageNet . Subsequently, the introduction of classifier-free guidance  and latent diffusion model  made it scale up to billions of text-image pairs , leading to the popular open-sourced text-to-image diffusion model, i.e., Stable Diffusion. With its superior generation ability, recently people also start investigating the internal representation of diffusion models. For example, previous works [85; 31] found that the intermediate-layer features and attention maps of diffusion models are crucial for controllable generations; other works [5; 94; 101] explored adapting pre-trained diffusion models for various downstream visual recognition tasks. Different from these works, we are the first to directly evaluate the efficacy of features inherent to pre-trained diffusion models on various visual correspondence tasks.

## 3 Problem Setup

Given two images \(I_{1},I_{2}\) and a pixel location \(p_{1}\) in \(I_{1}\), we are interested in finding its corresponding pixel location \(p_{2}\) in \(I_{2}\). Relationships between \(p_{1}\) and \(p_{2}\) could be semantic correspondence (i.e., pixels of different objects that share similar semantic meanings), geometric correspondence (i.e., pixels of the same object captured from different viewpoints), or temporal correspondence (i.e., pixels of the same object in a video that may deform over time).

The most straightforward approach to obtaining pixel correspondences is to first extract dense image features in both images and then match them. Specifically, given a feature map \(F_{i}\) for image \(I_{i}\), we can extract a feature vector \(F_{i}(p)\) for pixel location \(p\) through bilinear interpolation. Then given a pixel \(p_{1}\) in image \(I_{1}\), we can obtain the corresponding pixel in image \(I_{2}\) as:

\[p_{2}=*{arg\,min}_{p}d(F_{1}(p_{1}),F_{2}(p))\] (1)

where \(d\) is a distance metric and we use cosine distance by default in this work.

## 4 Diffusion Features (DIFT)

In this section, we first review what diffusion models are and then explain how we extract dense features on real images using pre-trained diffusion models.

### Image Diffusion Model

Diffusion models [32; 79] are generative models that aim to transform a Normal distribution to an arbitrary data distribution. In our case, we use image diffusion models, thus the data distribution and the Gaussian prior are both over the space of 2D images.

During training, Gaussian noise of different magnitudes is added to clean data points to obtain noisy data points. This is typically thought of as a "diffusion" process, where the starting point of the diffusion \(x_{0}\) is a clean image from the training dataset and \(x_{t}\) is a noisy image obtained by "mixing" \(x_{0}\) with noise:

\[x_{t}=}x_{0}+(})\] (2)

where \((0,)\) is the randomly-sampled noise, and \(t[0,T]\) indexes "time" in the diffusion process with larger time steps involving more noise. The amount of noise is determined by \(\{_{t}\}_{1}^{T}\), which is a pre-defined noise schedule. We call this the diffusion _forward_ process.

A neural network \(f_{}\) is trained to take \(x_{t}\) and time step \(t\) as input and predict the input noise \(\). For image generation, \(f_{}\) is usually parametrized as a U-Net [71; 17; 70]. Once trained, \(f_{}\) can be used to "reverse" the diffusion process. Starting from pure noise \(x_{T}\) sampled from a Normal distribution, \(f_{}\) can be iteratively used to estimate noise \(\) from the noisy data \(x_{t}\) and remove this noise to get a cleaner data \(x_{t-1}\), eventually leading to a sample \(x_{0}\) from the original data distribution. We call this the diffusion _backward_ process.

### Extracting Diffusion Features on Real Images

We hypothesize that diffusion models learn correspondence implicitly [85; 61] in Sec. 1, but how can we extract this correspondence? Consider first _generated_ images, where we have access to the complete internal state of the network throughout the entire backward process. Given a generated image from Stable Diffusion , we extract the feature maps of its intermediate layers at a specific time step \(t\) during the backward process, which we then utilize to establish correspondences between two different generated images as described in Sec. 3. As illustrated in Fig. 2, this straightforward approach allows us to find correct correspondences between generated images, even when they belong to different categories or domains.

Replicating this approach for real images is challenging because of the fact that the real image itself does not belong to the training distribution of the U-Net (which was trained on noisy images), and we do not have access to the intermediate noisy images that would have been produced during the generation of this image. Fortunately, we found a simple approximation using the forward diffusion process to be effective enough. Specifically, we first add _noise_ of time step \(t\) to the real image (Eq. (2)) to move it to the \(x_{t}\) distribution, and then feed it to network \(f_{}\) together with \(t\) to extract the intermediate layer activations as our DIffusion Features, namely DIIFT. As shown in Figs. 1 and 3, this approach yields surprisingly good correspondences for real images.

Moving forward, a crucial consideration is the selection of the time step \(t\) and the network layer from which we extract features. Intuitively we find that a larger \(t\) and an earlier network layer tend to yield more semantically-aware features, while a smaller \(t\) and a later layer focus more on low-level details. The optimal choices of \(t\) and layer depend on the specific correspondence task at hand, as different tasks may require varying trade-offs between semantic and low-level features. For example, semantic correspondence likely benefits from more semantic-level features, whereas geometric correspondence between two views of the same instance may perform well with low-level features. We therefore use a 2D grid search to determine these two hyper-parameters for each correspondence task. For a comprehensive list of the hyper-parameter values used in this paper, please refer to Appendix C.

Lastly, to enhance the stability of the representation in the presence of random noise added to the input image, we extract features from multiple noisy versions with different samples of noise, and average them to form the final representation.

## 5 Semantic Correspondence

In this section, we investigate how to use DIFT to identify pixels that share similar semantic meanings across images, e.g., the eyes of two different cats in two different images.

### Model Variants and Baselines

We extract DIFT from two commonly used, open-sourced image diffusion models: Stable Diffusion 2-1 (SD)  and Ablated Diffusion Model (ADM) . SD is trained on the LAION  whereas ADM is trained on ImageNet  without labels. We call these two features DIFT\({}_{sd}\) and DIFT\({}_{adm}\) respectively.

To separate the impact of training data on the performance of DIFT, we also evaluate two other commonly used self-supervised features as baselines that share basically the same training data:

Figure 2: Given a Stable Diffusion generated image, we extract its intermediate layer activations at a certain time step \(t\) during its backward process, and use them as the feature map to predict the corresponding points. Although simple, this method produces correct correspondences on generated images already not only within category, but also cross-category, even in cross-domain situations, e.g., from a photo to an oil painting.

OpenCLIP  with ViT-H/14  trained on LAION, as well as DINO  with ViT-B/8 trained on ImageNet  without labels. Note that for both DIFT and other self-supervised features, we do not fine-tune or re-train the models with any additional data or supervision.

### Benchmark Evaluation

**Datasets.** We conduct evaluation on three popular benchmarks: SPair-71k , PF-WILLOW  and CUB-200-2011 . SPair-71k is the most challenging semantic correspondence dataset, containing diverse variations in viewpoint and scale with 12,234 image pairs on 18 categories for testing. PF-Willow is a subset of PASCAL VOC dataset  with 900 image pairs for testing. For CUB, following , we evaluate 14 different splits of CUB (each containing 25 images) and report the average performance across all splits.

**Evaluation Metric.** Following prior work, we report the percentage of correct keypoints (PCK). The predicted keypoint is considered to be correct if they lie within \((h,w)\) pixels from the ground-truth keypoint for \(\), where \(h\) and \(w\) are the height and width of either the image (\(_{img}\)) or the bounding box (\(_{bbox}\)). To find a suitable time step and layer feature to use for DIFT and other self-supervised features, we grid search the hyper-parameters using SPair-71k and use the same hyper-parameter settings for PF-WILLOW and CUB.

We observed inconsistencies in PCK measurements across prior literature1. Some works  use the total number of correctly-predicted points in the whole dataset (or each category split) divided

Figure 3: Visualization of semantic correspondence prediction on SPair-71k using different features. The leftmost image is the source image with a set of keypoints; the rightmost image contains the ground-truth correspondence for a target image whereas any images in between contain keypoints found using feature matching with various features. Different colors indicate different keypoints. We use circles to indicate correctly-predicted points under the threshold \(_{bbox}=0.1\) and crosses for incorrect matches. DIFT is able to establish correct correspondences under clustered scenes (row 3), viewpoint changes (row 2 and 4), and occlusions (row 5). See Fig. 17 in Appendix E for more results.

[MISSING_PAGE_FAIL:6]

Furthermore, even without any supervision (be it explicit correspondence or in-domain data), DIFT outperforms all the weakly-supervised baselines on all benchmarks by a large margin. It even outperforms the state-of-the-art supervised methods on PF-WILLOW, and for 9 out of 18 categories on SPair-71k.

**Qualitative Results.** To get a better understanding of DIFT's performance, we visualize a few correspondences on SPair-71k using various off-the-shelf features in Fig. 3. We observe that DIFT is able to identify correct correspondences under cluttered scenes, viewpoint changes, and instance-level appearance changes.

In addition to visualizing correspondence within the same categories in SPair-71k, we also visualize the correspondence established using DIFT\({}_{sd}\) across various categories in Fig. 4. Specifically, we select an image patch from a random image and query the image patches with the nearest DIFT embedding in the rest of the test split but from different categories. DIFT is able to identify correct correspondence across various categories.

**Sensitivity to the choice of time step \(t\).** For DIFT\({}_{sd}\), we plot how its PCK per point varies with different choices of \(t\) on SPair-71k in Fig. 5. DIFT is robust to the choice of \(t\) on semantic correspondence, as a wide range of \(t\) outperforms other off-the-shelf self-supervised features. Appendix B includes more discussion on how and why does \(t\) affect the nature of correspondence.

Figure 4: Given image patch specified in the leftmost image (red rectangle), we use DIFT\({}_{sd}\) to retrieve the top-5 nearest patches in images from different categories in the SPair-71k test set. DIFT is able to find correct correspondence for different objects sharing similar semantic parts, e.g., the wheel of an airplane vs. the wheel of a bus. More results are in Fig. 18 of Appendix E.

Figure 5: PCK per point of DIFT\({}_{sd}\) on SPair-71k. It maintains high accuracy with a wide range of \(t\), outperforming other off-the-shelf self-supervised features.

### Application: Edit Propagation

One application of DIFT is image editing: we can propagate edits in one image to others that share semantic correspondences. This capability is demonstrated in Fig. 6, where we showcase DIFT's ability to reliably propagate edits across different instances, categories, and domains, without any correspondence supervision.

To achieve this propagation, we simply compute a homography transformation between the source and target images using only matches found in the regions of the intended edits. By applying this transformation to the source image edits (e.g., an overlaid sticker), we can integrate them into the corresponding regions of the target image. Figure 6 shows the results for both OpenCLIP and DIFT\({}_{sd}\) using the same propagation techniques. OpenCLIP fails to compute reasonable transformation due to the lack of reliable correspondences. In contrast, DIFT\({}_{sd}\) achieves much better results, further justifying the effectiveness of DIFT in finding semantic correspondences.

## 6 Other Correspondence Tasks

We also evaluate DIFT on geometric correspondence and temporal correspondence. As in Sec. 5, we compare DIFT to other off-the-shelf self-supervised features as well as task-specific methods.

### Geometric Correspondence

Intuitively, we find when \(t\) is small, DIFT focuses more on low-level details, which makes it useful as a geometric feature descriptor.

**Setup.** We evaluate DIFT for homography estimation on the HPatches benchmark . It contains 116 sequences, where 57 sequences have illumination changes and 59 have viewpoint changes. Following , we extract a maximum of 1,000 keypoints from each image, and use cv2.findHomography() to estimate the homography from mutual nearest neighbor matches. For DIFT, we use the same set of keypoints detected by SuperPoint , as in CAPS .

For evaluation, we follow the corner correctness metric used in : the four corners of one image are transformed into the other image using the estimated homography and are then compared with those computed using the ground-truth homography. We deem the estimation correct if the average error of the four corners is less than \(\) pixels. Note that we do this evaluation on the original image resolution following , unlike .

**Results.** We report the accuracy comparison between DIFT and other methods in Tab. 4. Visualization of the matched points can be found in Fig. 7. Though not trained using any explicit geometry supervision, DIFT is still on par with the methods that utilize explicit geometric supervision signals

Figure 6: Edit propagation. The first column shows the source image with edits (i.e., the overlaid stickers), and the rest columns are the propagated results on new images from different instances, categories, and domains, respectively. Compared to OpenCLIP, DIFT\({}_{sd}\) propagates edits much more accurately. More results are in Fig. 20 of Appendix E.

designed specifically for this task, such as correspondences obtained from Structure-from-Motion  pipelines. This shows that not only semantic-level correspondence, but also geometric correspondence emerges from image diffusion models.

### Temporal Correspondence

DIFT also demonstrates strong performance on temporal correspondence tasks, including video object segmentation and pose tracking, although never trained or fine-tuned on such video data.

**Setup.** We evaluate DIFT on two challenging video label propagation tasks: (1) DAVIS-2017 video instance segmentation benchmark ; (2) JHMDB keypoint estimation benchmark .

Following evaluation setups in [49; 37; 10; 95], representations are used as a similarity function: we segment scenes with nearest neighbors between consecutive video frames. Note that there is no training involved in this label propagation process. We report region-based similarity \(\) and contour-based accuracy \(\) for DAVIS, and PCK for JHMDB.

**Results.** Table 5 reports the experimental results, comparing DIFT with other self-supervised features (pre-)trained with or without video data. DIFT\({}_{adm}\) outperforms all the other self-supervised learning methods on both benchmarks, even surpassing models specifically trained on video data by a significant margin. DIFT also yields the best results within the same pre-training dataset.

    &  &  &  &  \\  & **Supervision** & \(=1\) & \(=3\) & \(=5\) & \(=1\) & \(=3\) & \(=5\) & \(=1\) & \(=3\) & \(=5\) \\  SIFT  & None & 40.2 & 68.0 & 79.3 & 26.8 & 55.4 & 72.1 & 54.6 & 81.5 & 86.9 \\ LF-Net  & 34.4 & 62.2 & 73.7 & 16.8 & 43.9 & 60.7 & 53.5 & 81.9 & 87.7 \\ SuperPoint  & 36.4 & 72.7 & 82.6 & 22.1 & 56.1 & 68.2 & 51.9 & 90.8 & **98.1** \\ D2-Net  & Strong & 16.7 & 61.0 & 75.9 & 3.7 & 38.0 & 56.6 & 30.2 & 84.9 & 95.8 \\ DISK  & 40.2 & 70.6 & 81.5 & 23.2 & 51.4 & 67.9 & 58.5 & 91.2 & 96.2 \\ ContextDesc  & 40.9 & 73.0 & 82.2 & 29.6 & 60.7 & 72.5 & 53.1 & 86.2 & 92.7 \\ R2D2  & 40.0 & 74.4 & 84.3 & 26.4 & 60.4 & 73.9 & 54.6 & 89.6 & 95.4 \\    &  &  &  &  &  &  &  & \\ CAPS  & Weak & 44.8 & **76.3** & **85.2** & **35.7** & **62.9** & **74.3** & 54.6 & 90.8 & 96.9 \\ DINO  & 38.9 & 70.0 & 81.7 & 21.4 & 50.7 & 67.1 & 57.7 & 90.8 & 97.3 \\ DIFT\({}_{adm}\) (ours) & 43.7 & 73.1 & 84.8 & 26.4 & 57.5 & **74.3** & **62.3** & 90.0 & 96.2 \\ OpenCLIP  & None & 33.3 & 67.2 & 78.0 & 18.6 & 45.0 & 59.6 & 49.2 & 91.2 & 97.7 \\ DIFT\({}_{adm}\) (ours) & **45.6** & 73.9 & 83.1 & 30.4 & 56.8 & 69.3 & 61.9 & **92.3** & **98.1** \\   

Table 4: Homography estimation accuracy [%] at 1, 3, 5 pixels on HPatches. Colors of numbers indicate the **best**, \(\) results. All the DIFT results have gray background for better reference. DIFT with SuperPoint keypoints achieves competitive performance.

Figure 7: Sparse feature matching using DIFT\({}_{sd}\) on HPatches after removing outliers with cv2.findHomography(). Left are image pairs under viewpoint change, and right are ones under illumination change. Although never trained with correspondence labels, it works well under both challenging changes. More results are in Fig. 21 of Appendix E.

We also show qualitative results in Fig. 8, presenting predictions of video instance segmentation in DAVIS, comparing DIFT\({}_{adm}\) with DINO. DIFT\({}_{adm}\) produces masks with clearer boundaries when single or multiple objects are presented in the scene, even attends well to objects in the presence of occlusion.

## 7 Conclusion

This paper demonstrates that correspondence emerges from image diffusion models without explicit supervision. We propose a simple technique to extract this implicit knowledge as a feature extractor named DIFT. Through extensive experiments, we show that although without any explicit supervision, DIFT outperforms both weakly-supervised methods and other off-the-shelf self-supervised features in identifying semantic, geometric and temporal correspondences, and even remains on par with the state-of-the-art supervised methods on semantic correspondence. We hope our work inspires future research on how to better utilize these emergent correspondence from image diffusion, as well as rethinking diffusion models as self-supervised learners.

Figure 8: Video label propagation results on DAVIS-2017. Colors indicate segmentation masks for different instances. Blue rectangles show the first frames. Compared to DINO, DIFT\({}_{adm}\) produces masks with more accurate and sharper boundaries. More results are in Fig. 22 of Appendix E.

    &  &  &  &  \\  & & & \(_{m}\) & \(_{m}}\) & \(_{m}\) & PCK@0.1 & PCK@0.2 \\  \)} &  &  & 66.4 & 63.9 & 68.9 & 58.5 & 80.2 \\  & & MoCo  & & 65.9 & 63.4 & 68.4 & 59.4 & 80.9 \\  & & SimCLR  &  & 66.9 & 64.4 & 69.4 & 59.0 & 80.8 \\  & & BYOL  &  & 66.5 & 64.0 & 69.0 & 58.8 & 80.9 \\  & & SimSiam  & & 67.2 & 64.8 & 68.8 & 59.9 & 81.6 \\  & & DINO  & & 71.4 & 67.9 & 74.9 & 57.2 & 81.2 \\  & & DIFT\({}_{adm}\) (ours) & & **75.7** & **72.7** & **78.6** & **63.4** & **84.3** \\  & & OpenCLIP  &  & 62.5 & 60.6 & 64.4 & 41.7 & 71.7 \\  & & DIFT\({}_{sd}\) (ours) & & 70.0 & 67.4 & 72.5 & 61.1 & 81.8 \\  \)} &  & VINC  & 65.2 & 62.5 & 67.8 & 58.8 & 80.4 \\  & & VFS  &  & 68.9 & 66.5 & 71.3 & 60.9 & 80.7 \\  & & UVC  & & 60.9 & 59.3 & 62.7 & 58.6 & 79.6 \\  & & CRW  & & 67.6 & 64.8 & 70.2 & 58.8 & 80.3 \\  & & Colorization  &  & 34.0 & 34.6 & 32.7 & 45.2 & 69.6 \\   & & CorrFlow  &  & 50.3 & 48.4 & 52.2 & 58.5 & 78.8 \\   & & TimeCycle  &  & 48.7 & 46.4 & 50.0 & 57.3 & 78.1 \\   & & MAST  & & 65.5 & 63.3 & 67.6 & - & - \\   & & SFC  &  & 71.2 & 68.3 & 74.0 & 61.9 & 83.0 \\   

Table 5: Video label propagation results on DAVIS-2017 and JHMDB. Colors of numbers indicate the **best**, \(\) results. All the DIFT results have \(\) background for better reference. DIFT even outperforms other self-supervised learning methods specifically trained with video data.

**Acknowledgement.** This work was partially funded by NSF 2144117 and the DARPA Learning with Less Labels program (HR001118S0044). We would like to thank Zeya Peng for her help on the edit propagation section and the project page, thank Kamal Gupta for sharing the evaluation details in the ASIC paper, thank Aaron Gokaslan, Utkarsh Mall, Jonathan Moon, Boyang Deng, and all the anonymous reviewers for valuable discussion and feedback.