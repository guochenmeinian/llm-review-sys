# 4DBInfer: A 4D Benchmarking Toolbox for

Graph-Centric Predictive Modeling on RDBs

 Minjie Wang\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Quan Gan\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

David Wipf\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Zhenkun Cai\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Ning Li\({}^{2}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Jianheng Tang\({}^{3}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Yanlin Zhang\({}^{4}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Zizhao Zhang\({}^{5}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Zunyao Mao\({}^{6}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Yakun Song\({}^{2}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Yanbo Wang\({}^{7}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Jiahang Li\({}^{8}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Han Zhang\({}^{2}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Guang Yang\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Xiao Qin\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Chuan Lei\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Mahan Zhang\({}^{7}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Weinan Zhang\({}^{2}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Christos Faloutsos\({}^{1,9}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

Zheng Zhang\({}^{1}\)

Equal contribution; correspondence to {minjiw,quagan}@amazon.com.

###### Abstract

Given a relational database (RDB), how can we predict missing column values in some target table of interest? Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above _four dimensions_ (4D) of exploration within a unified, scalable open-source toolbox called _4DBInfer_; please see https://github.com/awslabs/multi-table-benchmark/.

## 1 Introduction

Relational databases (RDBs) can be viewed as storing a collection of interrelated data spread across multiple linked tables. Of vast and steadily growing importance, the market for RDB management systems alone is expected to exceed $133 billion USD by 2028 . Even so, while the machine learning community has devoted considerable attention to predictive tasks involving _single_ tables, or so-called tabular modeling tasks , thus far efforts to widen the scope to handle _multiple_ tables and RDBs still lags behind, despite the seemingly enormous potential of doing so. With respect to the latter, in many real-world scenarios critical features needed for accurately modeling agiven quantity of interest are not constrained to a single table [9; 14], nor can be easily flattened into a single table via reliable/obvious feature engineering .

This disconnect between commercial opportunity and academic research focus can, at least in large part, be traced back to one transparent culprit: Unlike widely-studied computer vision , natural language processing , tabular , and graph  domains, established benchmarks for evaluating predictive ML models of RDB data are much less prevalent. This reality is an unsurprising consequence of privacy concerns and the typical storage of RDBs on servers with heavily restrictive access and/or licensing protections. With few exceptions (that will be discussed in later sections), relevant model development is instead predicated on surrogate benchmarks that branch as follows.

Along the first branch, sophisticated models that explicitly account for relational information are often framed as graph learning problems, addressable by graph neural networks (GNNs) [6; 29; 32; 37; 42; 45; 57; 66] or their precursors [78; 80; 81], and evaluated specifically on graph benchmarks [35; 43; 51]. The limitation here though is that performance is conditional on a fixed, pre-specified graph and attendant node/edge features intrinsic to the benchmark, not an actual RDB or native multi-table format. Hence the inductive biases that might otherwise lead to optimal performance on the original data can be partially masked by whatever process was used to produce the provided graphs and features. As for the second branch, emphasis is placed on tabular model evaluations that preserve the original format of single table data, possibly with augmentations collected from auxiliary tables. But here feature engineering and table flattening are typically prioritized over exploiting rich network effects as with GNNs [9; 14; 47; 48]. Critically though, currently-available head-to-head comparisons involving diverse candidate approaches representative of _both_ branches on un-filtered RDB/multi-table data are insufficient for drawing clear-cut conclusions regarding which might be preferable and under what particular circumstances.

To address the aforementioned limitations and help advance predictive modeling over RDB data, in Section 2 we first introduce a generic supervised learning formulation across both inductive and transductive settings covering dynamic RDBs as commonly-encountered in practice. A given predictive pipeline is then specified by (i) a sampling/distillation operator which extracts information most relevant to each target label, followed by (ii) a trainable prediction model. In Section 3 we present a specific design space for these two components. For the former, we adopt a graph-centric perspective whereby distillation is achieved (either implicitly or explicitly) via graphs and sampled subgraphs extracted from RDBs. Meanwhile, for the latter we incorporate trainable architectures that represent strong exemplars drawn from _both_ tabular and graph ML domains. We emphasize here that until more extensive benchmarking has been conducted, it is advisable not to prematurely exclude candidates from either domain, or hybrid combinations thereof. In this regard, Section 4 introduces a new suite of RDB benchmarks along with discussion of the comprehensive desiderata which leads to them. These include multiple diversity/coverage considerations across both (i) datasets and (ii) predictive tasks, while also resolving limitations of existing alternatives. Our 4DBInfer toolbox for

Figure 1: 4DBInfer exploration dimensions. Unlike prior benchmarking efforts (table columns on right), 4DBInfer considers an evaluation space with diversity across the 4D Cartesian product of (i) datasets, (ii) tasks, (iii) graph extractors, and (iv) predictive baselines. See Sections 3 and 4 (and in particular Section 4.2) for further details of table properties and assumptions.

pairing a so-called \(2D\) design space of baseline models from Section 3 and the \(2D\) benchmark coverage from Section 4 within a neutral combined \(4D\) evaluation setting is introduced in Section 5. And finally, Section 6 culminates with representative experiments conducted using 4DBInfer.

In tracing these endeavors, our paper consolidates the following contributions:

* **2D Space of Baselines:** On the _modeling side_ we describe a \(2D\) design space with considerable variation in (i) graph construction/sampling operators and (ii) trainable predictor designs. The latter covers popular choices drawn from GNN and tabular domains, representative of both early and late feature fusion strategies. This diversity safeguards against siloed comparisons between pipelines of only a single genre, e.g., tabular, GNNs.
* **2D Space of Benchmarks:** On the _data side_, we introduce a \(2D\) suite of RDB benchmarking (i) datasets and (ii) tasks that are devoid of potentially lossy or confounding pre-processing that might otherwise skew performance in favor of one model class or another. These benchmarks also vary across key dimensions of scale (e.g., up to 2B RDB rows), source domain, RDB schema, and temporal structure.
* **4DBInfer Toolbox:** We operationalize the above via a _unified and scalable open-source toolbox_ called 4DBInfer that facilitates direct head-to-head empirical comparisons across each dimension of baseline model and benchmarking task (and is readily extensible to accommodate new additions of either). Figure 1 depicts the combined \(4D\) exploration space of 4DBInfer, along with comparisons relative to existing RDB, tabular, and graph benchmarking work.
* **Empirical Support:** Experiments using 4DBInfer highlight the relevance of each of the proposed four dimensions of exploration to the design of successful RDB predictive models, as well as the limitations of more naive commonplace approaches such as simply joining adjacent tables.

## 2 Predictive Modeling on RDBs

An RDB \(\) can be viewed as a set of \(K\) tables denoted as \(:=\{^{k}\}_{k=1}^{K}\), where \(^{k}\) refers to the \(k\)-th constituent table defined by a particular entity type. Each row of a table then represents an instance of the corresponding entity type (e.g., a user), while the columns contain relevant features of each such instance (e.g., user profile information). Such features are typically heterogeneous and may include real values, integers, categorical variables, text snippets, or time stamps among other things. We adopt \(^{k}_{i:}\) and \(^{k}_{:j}\) to reference the \(i\)-the row and \(j\)-th column of \(^{k}\) respectively. What establishes \(\) as a _relational_ database, as opposed to merely a collection of tables, is that certain table columns are designated as either _primary keys_ (PKs) or _foreign keys_ (FKs). A column \(^{k}_{:j}\) serves as a PK when each element is a unique index referencing a row of \(^{k}\), such as a user ID for example. In contrast, \(^{k}_{:j}\) is defined as a FK column if each \(^{k}_{ij}\) corresponds with a unique PK value referencing a row in _another_ table \(^{k^{}}\) (generally \(k^{} k\), although this need not strictly be the case), with the only restriction being that all such indices within a given FK column must point to rows within the same table. In this way, the domain of any FK column is given by the corresponding PK column it references. Please see l.h.s. of Figure 2 for a simple RDB example.

Figure 2: 4DBInfer overview. _Left_: First a (i) RDB dataset and (ii) task (i.e., predictive target here) are selected from among proposed benchmarks. _Middle_: Then a (iii) graph extractor/sampling operator is chosen which converts the RDB and task into subgraph chunks (_middle_). _Right_: Lastly a (iv) predictive model ingests these chunks, either through early or late feature fusion, to produce an estimate of the target values (_right_).

### Making Predictions over Dynamic RDBs

Generally speaking, RDBs are _dynamic_, with information regularly being added to or removed from \(\). Hence if we are to precisely define a predictive task involving an RDB, and particularly an inductive task, it is critical that we specify the RDB state during which a given prediction is to occur. For this reason, we refine our original RDB definition as \((s):=\{^{k}(s)\}_{k=1}^{K}\), where \(s\) defines the RDB _state_ drawn from some set \(\). Note that \(\) could simply reflect counting indices (versions) such as the set of natural numbers; importantly though, each \(s\) need not necessarily correspond with physical/real-world time per se, even if in some cases it may be convenient to assume so. This then leads to the following core objective:

**Problem Statement:**_Using all relevant information available in \((s)\), predict an unknown RDB quantity of interest \(^{k}_{ij}(s)\) as uniquely specified by the tuple \(\{s,k,i,j\}\), where \(s\) determines the state, \(k\) the table, and \(\{i,j\}\) the table cell we wish to estimate._

To illustrate, the unknown \(^{k}_{ij}(s)\) is represented by '?' on the l.h.s. of Figure 2. Ideally, we would like to closely approximate the distribution \(p(^{k}_{ij}(s)(s)^{k}_{ij}(s))\), meaning all other information in the RDB is fair game as conditioning variables governing our prediction at state \(s\) of missing value \(^{k}_{ij}(s)\). Of course in practice it is neither feasible nor necessary to condition on the _entire_ RDB given limited computational resources and the likely irrelevance of much of the stored data w.r.t. \(^{k}_{ij}(s)\). Hence our revised objective is to incorporate a sampling operator \(\) defined such that

\[p(^{k}_{ij}(s)[(s)^{k}_{ij}( s)]) p(^{k}_{ij}(s)(s)^{k}_ {ij}(s)),\] (1)

where \([(s)^{k}_{ij}(s)]\) represents a distillation of appreciable information in the RDB relevant to \(^{k}_{ij}(s)\). As a simple illustrative example, if

\[[(s)^{k}_{ij}(s)]=^{k}_{:i}(s) ^{k}_{ij}(s),\] (2)

then all information in \((s)\) excluding the features in row \(i\) of table \(k\) are ignored when predicting \(^{k}_{ij}(s)\) and we recover a canonical tabular prediction task involving just a single table [21; 49; 58]. More broadly though, \(\) may be defined to select other rows of \(^{k}(s)\) (i.e., row \(i^{} i\) as used in recent cross-row tabular predictive models [20; 46; 60]), as well as information from other tables \(^{k^{}}(s)\) (with \(k^{} k\)) that are linked to \(^{k}(s)\) through one or more FK relationships. Even other values in column \(^{k}_{:j}(s)\) can be incorporated when available, noting that a special case of this scenario can be used to rederive trainable variants of label propagation predictors .

### High-Level Training and Inference Specs

We now describe training and inference in general terms under an inductive setup; the transductive case will trivially follow as a special case discussed below. We assume target table \(k\) and target column \(j\) are fixed to define a given predictive task. As such, each training instance is specified by only the tuple \(\{s,i\}\), noting that target table row \(i\) will often be a function of \(s\) by design, e.g., as \(s\) increments forward, additional rows with missing values for column \(j\) may be added to \(^{k}(s)\). Let \(_{tr}\) denote the set of states which have known training labels, and \(_{tr}(s)\) the corresponding set of specific indices with labels for each \(s_{tr}\). Then for a given task defined by \(k\) and \(j\), along with a corresponding sampling operator \(\), we seek to minimize the negative log-likelihood objective

\[_{}_{s_{tr}}_{i_{tr}(s)}- p( ^{k}_{ij}(s)[(s)^{k}_{ij}(s) ];)\] (3)

with respect to parameters \(\) that define the predictive distribution, e.g., a model of the conditional mean for regression problems, or logits for classification tasks, etc. The implicit assumption here is that, when conditioned on \([(s)^{k}_{ij}(s)]\), each \(^{k}_{ij}(s)\) is roughly independent of one anotherfor all \(\{\{s,i\}:i_{tr}(s),s_{tr}\}\); this implicit assumption forms the basis of empirical risk minimization . However, it need _not_ be the case that individual rows of \(^{k}(s)\) are unconditionally independent of one another.

Given some \(\) obtained by minimizing (3), at test time we are presented with new tuples \(\{\{s,i\}:i_{te}(s),s_{te}\}\), from which we can compute \(p(^{k}_{ij}(s)[(s)^{k}_{ij}( s)];)\) that ideally approximates the true distribution \(p(^{k}_{ij}(s)[(s)^{k}_{ij}( s)])\). We remark that a transductive reduction of the above procedure naturally emerges when \(s\) is fixed across both training and testing. More generally though, as \(s\) increments \((s)\) may undergo significant changes, such as new rows appended to \(^{k}(s)\) (e.g., the 'Purchase' table in Figure 2), new labels/values added to the target column \(^{k}_{:j}(s)\), as well as arbitrary changes to other tables \(^{k^{}}(s)\) with \(k^{} k\).

## 3 Design Space of (Graph-Centric) Baseline Models

The general inductive learning framework from the previous section relies on two complementary components: (i) a sampling operator \(\), and (ii) a parameterized predictive distribution as expressed in (3). Collectively, these amount to the first so-called 2D of our proposed 4DBInfer. For both scalability and conceptual reasons, we design the former to operate on graphs that can extracted from RDBs through multiple distinct strategies as summarized in Section 3.1. Subsequently, we will introduce the details of \(\) itself in Section 3.2, followed by choices for predictive architectures in Section 3.3. We also discuss the contextualization with respect to prior work in Appendix H.3.

### Converting RDBs to Graphs

A _heterogeneous graph_\(=\{,\}\) is defined by sets of node types \(V\) and edge types \(E\) such that \(=_{v V}^{v}\) and \(}=_{e E}^{e}\), where \(^{v}\) references a set of \(|^{v}|\) nodes of type \(v\), while \(^{e}\) indicates a set of \(|^{e}|\) edges of type \(e\). Both nodes and edges can have associated features. Additionally, any heterogeneous graph can be generalized to depend on a state variable \(s\) as \((s)\) analogous to \((s)\). The goal herein then becomes the establishment of some procedure or mapping \(^{*}\) such that \((s)=^{*}[((s)]\) for any given RDB of interest.

**Row2Node.** Perhaps the most natural and intuitive way to instantiate \(^{*}\) is to simply treat each RDB row as a node, each table as a node type, and each FK-PK pair as a directed edge. Additionally, non-FK/PK column values are converted to node features assigned to the respective rows. Originally proposed in  with ongoing application by others [77; 23; 79], we refer to this approach as _Row2Node_.

**Row2N/E.** Importantly though, unlike prior work we do _not_ limit 4DBInfer to a single selection for \(^{*}\). The motivation for considering alternatives is straightforward: _Even if we believe that graphs are a sensible route for pre-processing RDB data, we should not prematurely commit to only one graph extraction procedure and the coincident downstream inductive biases that will inevitably be introduced_. To this end, as an alternative to Row2Node, we may relax the restriction that every row must be exclusively converted to a node. Instead, rows drawn from tables with more than one FK column can be selectively treated as typed edges, with the remaining non-FK columns designated as edge features. The intuition here is simply that tables with multiple FKs can be viewed as though they were natively a tabular representation of edges. We denote this variant of \(^{*}\) as _Row2N/E_.

For full details, analyses, and extensions of both _Row2Node_ and _Row2N/E_, please see Appendix I.

### Graph-based Sampling Operator \(\)

In principle, the sampling operator \(\) need not be explicitly predicated on an extracted graph. However, provided we do not restrict ourselves to _a particular fixed graph upfront_, we are not beholden to any one graph-specific inductive bias. In this way (with some abuse of notation) we instantiate \(\) as

\[[(s)^{k}_{ij}(s)][ ^{*}[(s)]^{k}_{ij}(s)]= [(s)^{k}_{ij}(s)],\] (4)

where \(^{*}\) is an RDB-to-graph mapping such as described in Section 3.1 and Appendix I, \((s)=^{*}[(s)]\) represents the extracted graph, and the exclusion operator '\(\)' here simply removes the node feature attribute associated with \(^{k}_{ij}(s)\) from \((s)\). We may now select from among the wide variety of scalable graph sampling methods for finalizing \(\)[7; 10; 32; 73; 76; 83] while specifying the effective receptive field, meaning the number of hops (or tables) away from the target associated with \(^{k}_{ij}(s)\) from which information is collected. Whatever the choice though, the output of \(\) will be a subgraph of \((s)\) containing the target node corresponding to row \(^{k}_{i:}(s)\).

### Trainable Predictive Architectures

At a high-level, once granted \(\) we sub-divide candidate architectures for instantiating the predictive distribution from (3) based on what can be loosely referred to as early versus late feature fusion.

**Late Fusion.** In the context of RDB-specific modeling, we reserve _late fusion_ to delineate models whereby parameter-free feature augmentation is adopted to produce a fixed-length, potentially high-dimensional feature vector associated with each target that is, only then, used to train a high-capacity base model with parameters \(\) such as those commonly applied to tabular data (this strategy is also referred to as _propositionalization_[47; 75]). For the initial feature augmentation step, we lean on the Deep Feature Synthesis (DFS) framework  and extensions thereof for the following reasons:

* DFS is a powerful automated method for generating new features for an RDB by recursively combining data from related tables through aggregation, transformation, etc.
* Although motivated differently, DFS can be re-derived and generalized as a form of subgraph sampling from Section 3.2, followed by concatenated aggregations, as applied to graphs extracted via Row2Node or extensions thereof in Appendix I;
* Special cases of DFS include commonly-used multi-table augmentation and flattening schemes , as when paired with sampling limited to 1-hop, or more general multi-hop strategies such as FastProp from the getML package ;
* DFS can be applied with constraints on \(s\) to avoid label leakage;
* DFS is in principle capable of handling large-scale RDBs. Please see Appendices F.1 and G.2 for additional details regarding DFS and our enhanced implementation.

Then for a given target \(^{k}_{ij}\), this so-called late fusion pipeline produces a fixed-length feature vector

\[^{k}_{ij}:=[(s)^{k}_{ij}(s) ]  ([(s)^{k}_{ij}(s )]),\] (5)

where Agg is an aggregation operator; see Appendix F.1 for specific choices. And in conjunction with (4), we can subsequently apply any tabular model to estimate the parameters of

\[p(^{k}_{ij}(s)\,|\,\,[(s)^{k}_{ ij}(s)];)  p(^{k}_{ij}(s)\,|\,\,^{k}_{ij};)\] (6)

by minimizing (3) over training data. For diversity of tabular base predictors, including both tree- and deep-learning-based, we adopt **MLP**, **DeepFM**, **FT-Transformer**, **XGBoost**, and **AutoGluon (AG)**[3; 21], the latter representing a top-performing AutoML ensembling model. We also remark that DFS combined with gradient boosted trees (akin to our DFS+XGBoost early fusion baseline) has previously been shown to outperform various manual feature engineering and graph embedding methods . We defer broader consideration of benchmarking against handcrafted features to future work.

**Early Fusion.** We next adopt _early fusion_ to reference message-passing GNN-like architectures that produce trainable low-dimensional node embeddings (at least relative to late fusion) beginning from the very first model layer. More concretely, for a heterogeneous graph \(\) (e.g., as extracted from an RDB) these embeddings can be computed as

\[^{v}_{i,}=f(\{\,\{(^{v^{}}_{i^{ },-1},vv^{}):i^{}^{vv^{}}_{i} \}:v^{}^{v}\},^{v}_{i,-1}; ),\] (7)

where \(^{v}_{i,}\) denotes the embedding of node \(i\) of type \(v\) at GNN layer \(\). In this expression, \(^{v}\) indicates the set of node types that neighbor nodes of type \(v\), and \(^{vv^{}}_{i}\) is the set of nodes of type \(v^{}\) that neighbor node \(i\) of type \(v\). Moreover, we assume that there is a unique edge type \(e vv^{}\) associated with each pair of node types \((v,v^{})\), as will always be the case for the graphs extracted from RDBs that we focus on here (note also that the edge type \(vv^{}\) is included within the innermost set definition to differentiate each element within the outer-most set construction). Meanwhile, \(f\) is a permutation-invariant function  over sets with parameters \(\), acting to aggregate or fuse information from all neighbors of connected node types at each layer. At the output layer, the embeddings produced via (7) can be applied to making node-wise predictions, which translates into predictions of target values in column \(^{k}_{;j}\).

For implementing \(f\) we adopt the popular heterogeneous architectures **R-GCN**, **R-GAT**, **HGT**, and **R-PNA**. Note that we specifically select R-PNA because its core principal neighbor aggregation (extended to heterogeneous graphs) bears considerable similarities to DFS aggregators. In all cases the resulting output layer embeddings will generally depend on which \(^{*}\) is used for graph construction. See Appendix F.2 for further details regarding the implementations of \(f\).

## 4 A New Suite of RDB Benchmarks

We now introduce and motivate RDB benchmarks that can be applied to evaluating the efficacy of candidate predictive models such as those described in Section 3. This includes a description of our selection desiderata and specific benchmark choices that adhere to them. Please also see Appendix B for a formal definition of an RDB benchmark.

### Why New RDB Benchmarks

On the tabular side, there exist countless benchmarks covering every conceivable scenario; however, these are predominately _single-table_ datasets, e.g., widely-used Kaggle data . In contrast, on the relational side, benchmarks are often predicated on extracted graphs (usually from limited domains such as citation networks) and pre-processed node features that may have already filtered away useful information [35; 43; 51]. As such, relative performance of candidate models is contingent on what information is available in these graphs and any sub-optimality therein, not actually the original data source. As a simple representative example, on the widely-studied Open Graph Benchmark (OGB) , many of the graph datasets were formed from curated citation networks with fixed text embeddings as node features. In this case, researchers have recently found that by reverting back to the original data sources and text features, vastly superior node classification accuracy is possible . Hence the original benchmarks were implicitly imposing an arbitrary constraint relative to the raw data itself, and the same can apply to imposed graph structure.

As for real-world datasets involving actual multi-table RDB data in its native form, available public benchmarks are somewhat limited and narrow in scope. These include RDBench , RelBench , and the CTU Prague Relational Learning Repository (CRLR) . However, as of the time of this writing, RelBench constitutes only two datasets, relies on Row2Node, and presents no experiments of any kind; see Appendix H for further differences between RelBench and our work. As for RDBench and CRLR, these are composed mostly of small datasets, e.g., with less than 1000 labeled instances, which is far surpassed by the size of typical real-world RDBs (see Section 4.2 and Appendix H for further details). Additionally, among the recent model-driven works targeting predictive ML or deep learning on RDBs [4; 9; 24; 33; 50; 75; 77], there exists no consistent set of diverse data and tasks for empirical comparisons, and for most there is no available software allowing others to follow suit. See Figure 1(r.h.s.) for a summary of existing benchmark properties.

### Benchmark Desiderata and Composition

To increase the chances that strong benchmark performance correlates with strong performance on future real-world application data, it is important to assemble RDB benchmarks so as to achieve adequate diversity or coverage across both (i) datasets and (ii) tasks. With this in mind, on the _dataset side_ our selection criteria are as follows: (i) **Availability:** Some otherwise promising public multi-table datasets currently disallow use for research publications [2; 12]; (ii) **Large-scale:** Real-world RDBs can involve billions of rows; (iii) **Domain diversity:** We seek datasets from diverse domains spanning e-commerce, advertising, social networks, etc.; (iv) **Schema diversity:** Variation over schema width, # tables, # of rows; (v) **Temporality**: Realistic RDBs often vary over time.

Meanwhile, on the _task side_ we have: (i) **Loss type:** Regression, classification, or ranking; (ii) **Learning type:** Inductive versus transductive; (iii) **Proximity to real-world :** Tasks are chosen to reflect practical business scenarios; (iv) **Meaningful difficulty**: Poorly chosen tasks where informative features are lacking can lead to meaningless comparisons. Conversely, tasks involving auxiliary features that are simple functions of target labels may be trivially easy. In real-world scenarios, avoiding these extremes may be non-obvious; see Appendix D for representative case studies.

Based on these desiderata covering our proposed 2D dataset and task space underpinning 4DBInfer, we have curated a representative set of RDB benchmarks adhering to Definition 1 in Appendix B. The tasks include: customer retention prediction on **AVS**, click-through-rate prediction on Outbrain (**OB**) , click-through-rate and purchase prediction on Diginetica (**DN**) , conversion prediction on RetailRocket (**RR**) , user churn, rating, and purchase prediction on Amazon Book Reviews (**AB**) , user churn and post popularity prediction on StackExchange (**SE**) , paper venue and citation prediction on **MAG**, and customer charge/prepay type prediction on **Seznam** (**SZ**) . For further details, please see Appendices C and D.

Additionally, general comparisons with existing benchmarks are presented in Figure 1, where 4DBInfer displays a distinct advantage in terms of the four overall dimensions we have proposed warrant coverage. As shown in the figure (r.h.s.), relevant existing benchmarks include single-table tabular (**OpenML**), graph (**OGB**, **HGB**, **TGB**), and RDB (**RDBench**, **CRLR**, **RelBench**). Note that entity attribute and key prediction correspond with node classification and link prediction in the graph ML literature, respectively. For further reference, Appendix H contains a much broader set of candidate benchmarks that were excluded from 4DBInfer because of failure to adhere with one or more of the above selection criteria.

## 5 Benchmark & Baseline Delivery

To facilitate reproducible empirical comparisons using our proposed benchmarks from Section 4 across the baselines from Section 3 (as well as future/improved predictive models informed by initial results), we instantiate 4DBInfer as a unified, scalable open-sourced Python package; please see https://github.com/awslabs/multi-table-benchmark/ and Appendix E. This package offers a _no-code_ user experience to minimize the effort of experimenting with various baselines over built-in or customized RDB datasets. This is achieved via a composable and modularized design whereby each critical data processing and model training step can be launched independently or combined in arbitrary order. Moreover, adding a new RDB dataset simply requires users to describe its metadata and the location to download the tables; the pipeline will automate the rest. Regarding resource requirements for running current 4DBInfer baselines and benchmarks, including details of peak GPU/CPU memory usage, see Appendix G.1.

As for the critical step of graph sampling, 4DBInfer implements \(\) using the GraphBolt open-source APIs from the Deep Graph Library , which facilitates sampling over graphs with billions of nodes, which is roughly tantamount to RDBs with billions rows. Our 4DBInfer toolbox also provides an enhanced implementation of the DFS algorithm to facilitate the large-scale datasets in our benchmark suite. Specifically, the existing open-source implementation FeatureTools  can only leverage a single-thread for cross-table aggregation, which can take weeks on some of our datasets. We substitute its execution backend with an SQL-based engine, which translates the feature metadata into SQLs for execution. The resulting solution shortens the DFS computation time to only several hours. Appendix G.2 contains quantitative timing comparisons spanning all of our 4DBInfer baselines and benchmarks.

## 6 Supporting Experiments Using 4DBInfer

We apply our 4DBInfer toolbox to explore performance across the proposed 4D evaluation space defined by benchmarks (datasets and tasks from Section 4) and baselines applied to them (graph extractors/samplers and base predictors from Section 3). As for these baseline models, we explore early feature fusion (DFS-based, with join-path set to reach the farthest RDB tables, i.e., the schema width) and late feature fusion (GNN-based) as discussed in Section 3.3. For the latter, we evaluate the impact of graphs extracted via either Row2Node (**R2N**) or Row2N/E (**R2N/E**) per Section 3.1. Additionally, we also include a widely-used baseline category that involves simply joining information from tables adjacent to the target table in the schema graph, and then applying tabular models to the resulting feature-augmented table. We prefix baselines in this category as **Join** (as a point of reference, this process is analogous to a rudimentary form of 1-hop DFS included within 4DBInfer).

Figure 3 displays a representative summary of performance results applying 4DBInfer, while fine-grained comparisons between every pair of baseline and benchmark are deferred to Appendix A. See also Appendix J for extensive supporting ablations over different graph extraction methods, stronger GNN base predictors, and label usage. While there is considerable detail and nuance associated with these performance numbers, several key points are worth emphasizing as follows:

1. **Complex vs simple comparisons.** More complex DFS-based and GNN-based models usually outperform simple join base models, indicating that relevant predictive information exists across a wider RDB receptive field (i.e., beyond adjacent tables). These results also highlight the need to consider diverse, relatively large-scale datasets, as prior work  involving much smaller scales has shown that simple joins can actually outperform GNNs.
2. **Early vs late feature fusion.** Early feature fusion as instantiated via GNNs is generally preferable to late fusion through DFS-based models. That being said, DFS nonetheless remains a strong competitor on multiple benchmarks (see Appendix A). Moreover, because of its lean design relative to GNNs, late fusion may be especially favorable in low resource environments even if the accuracy is not necessarily superior.
3. **Graph extraction method matters.** We observe in Figure 3 that both Row2Node and Row2N/E contribute to high-ranking models, with no clear-cut winner. This is because the preferred graph extraction method depends heavily on the dataset and task (see Appendices A and J.1); hence further exploration along this dimension is warranted.
4. **Task specific dependencies.** GNNs are preferable for predicting foreign keys (Appendices A and J.2), which is analogous to link prediction tasks in the graph ML literature. The latter typically benefits from more complex structural signals such as common neighbors that GNNs are arguably better equipped to exploit.

In on way or another, each of the points above highlight the value of considering _all four dimensions_ of our proposed 4D exploration space, namely, the potential consequences of variability across **dataset (a,b,c)**, **task (d)**, **graph extractor (c)**, and **base predictor (a,b,d)**. Even so, our preliminary comparisons so-obtained crown no unequivocal front-runner across all scenarios, showcasing the need for such benchmarking on realistic RDB tasks in the first place. And quite plausibly, future high-performant solutions may actually lie at the boundary between tabular and graph ML worlds. Either way, reliably establishing such trends hinges on native RDB evaluations that do not a priori favor one approach over another, e.g., results conditional on only one specific pre-processed graph or feature engineering technique, etc.

## 7 Broader Impact

Large RDBs and/or data lakes composed of multiple high-dimensional tables are pervasive, across which countless opportunities for predictive modeling exist. And yet to do so, data scientists continue to face an unresolved conundrum: Either adopt standard tabular predictive pipelines and then manually supplement with cumbersome cross-table feature engineering, or else rely on deep graph neural network models but struggle with how exactly to form the required input graph from raw tables in the first place. Our 4DBInfer toolbox provides a sturdy bridge between these domains, allowing users to effortlessly train and benchmark scalable solutions across a broad spectrum of possibilities spanning _both_ graph and tabular camps.

Figure 3: Median performance rank (_lower is better_) of different model combinations (Sec. 3) computed across all 4DBInfer benchmarks (Sec. 4).