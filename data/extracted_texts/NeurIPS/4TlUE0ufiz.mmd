# Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity

Kaiqu Liang  Zixu Zhang  Jaime Fernandez Fisac

Princeton University

{kl2471,zixuz,jfisac}@princeton.edu

###### Abstract

Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning processes. We propose introspective planning, a systematic approach that align LLM's uncertainty with the inherent ambiguity of the task. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a newly introduced safe mobile manipulation benchmark, demonstrate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Furthermore, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests. The webpage and code are accessible at https://introplan.github.io.

## 1 Introduction

Large Language Models (LLMs), when pre-trained on internet-scale text corpora, demonstrate emergent capabilities that extend far beyond mere text comprehension and generation as their scale increases . Through prompting  and in-context learning , these models have shown remarkable adaptability ranging from answering complex questions and solving mathematical problems to generating computer code and engaging in sophisticated reasoning processes during inference . Robots interacting with humans can leverage the capabilities of LLMs to interpret task instructions in natural language, employ common sense reasoning to understand their environment, and devise high-level action plans grounded in the capabilities and affordances of the robot .

The reliability of LLM outputs has direct implications downstream robotics tasks. Language models are prone to hallucinations , which cause models to generate plans that are at odds with common-sense knowledge, not executable by the robot, or incompatible with the environment constraints . For example, if a human user asks a robot to bake some bread in a kitchen containing an oven and varied cookware, the robot's LLM may generate a decision to use a plastic ray without considering the risk of it melting. Furthermore, possible ambiguities in the user's request can also introduce uncertainty into the LLM's reasoning and planning . In our example, while multiple containers are suitable for the stated task, biases inherited from training data may tilt action generation towards certain options. Therefore, the robot needs to calibrate its uncertainty quantification and seek further communication with users when ambiguities are identified.

Human beings assess their internal values and knowledge of their own abilities to guide domain-level reasoning processes: this is referred to as introspective reasoning . In this paper, we observe that LLMs can leverage an analogous reasoning scheme to better assess underlying uncertainty when generating plans. We propose a novel method for constructing a knowledge base that utilizes LLMs to generate human-aligned introspective reasoning examples with minimal human input. During inference, this human-aligned knowledge guides LLMs to produce more reliable and interpretable plans. Unlike traditional Retrieval-Augmented Generation (RAG) approaches [10; 20; 29; 43; 51], which utilize open-source, off-the-shelf knowledge bases to enhance text generation, our approach retrieves few-shot introspective reasoning examples from the knowledge base. This enables LLMs to explicitly reason about uncertainties and formulate plans in a structured format. Additionally, our method augments previous automatic reasoning approaches  by integrating human feedback into the reasoning generation process. We have observed that introspective planning, when integrated with conformal prediction [2; 3], refines the LLM's uncertainty and achieves a tighter guarantee.

**Statement of contributions.** To the best of our knowledge, this is the first work to integrate retrieval-augmented planning with conformal prediction, refining language agents' uncertainty and reducing user queries while maintaining statistical guarantees. Key contributions are summarized as follows:

* We propose a novel _introspective planning_ scheme that prompts language-enabled agents to proactively assess their own confidence regarding task compliance and safety for multiple candidate plans, with a guaranteed probability that the agent will either execute the actions desired by the user or ask an appropriate follow-up question to disambiguate the user's intent.
* We introduce a new, weakly supervised offline knowledge base construction method that guides the LLM to generate human-aligned introspective reasoning examples as post-hoc rationalizations of human-selected safe-and-compliant plans.
* We create a new Safe Mobile Manipulation benchmark, which augments previous mobile manipulation datasets with safety-critical scenarios and introduces new metrics to evaluate a planner's specification compliance, safety, and degree of conservativeness.

## 2 Introspective Planning

The fundamental aim of introspective planning is to guide LLMs to reason about their own uncertainty regarding task compliance and safety for multiple candidate plans. While this guidance can take various forms, our implementation here is based on distilling example reasoning processes from a knowledge base to inform the LLM via in-context learning.

**Problem Formulation.** Similar to , we cast LLM-based planning as multiple-choice question answering (MCQA). Given a task statement \(d_{i}\) and the observation \(o_{i}\), the LLM planner first generates a set of candidate plans \(_{i}\), each assigned a unique letter label from \(\{ A^{},\, B^{},\, C^{},\}\). The planner then predicts \(_{i}\), aiming to match the unknown true user intent \(z_{i}\). For example, consider

Figure 1: Illustration of the introspective planning pipeline. **Knowledge base construction:** The LLM generates knowledge entries based on human-provided instructions and valid options. **Deployment:** Upon receiving an instruction, the LLM formulates possible next steps, consults the knowledge base to retrieve the most relevant examples, and uses them as prompts for prediction.

the stated task \(d_{i}\) "_Bring me that soda_" with the observation \(o_{i}\) that a banana, a pack of chips, and a can of Coke are placed on the counter. The LLM planner will first generate three options \(_{i}\) of bringing each item to the user, and predict the label \(_{i}\) corresponding to the Coke.

**Knowledge Base Construction.** Consider a training set \(=\{(x_{i},_{i})\}_{i=1}^{N}\) comprising \(N\) instances. For each instance, \(x_{i}:=(d_{i},o_{i})\) encompasses a pair of the task \(d_{i}\) and the observation \(o_{i}\), and a set of all valid options \(_{i}\) satisfying the task specification and the observation. To construct the knowledge base, we query LLM and generate a set of candidate plans \(_{i}\) with alphabetic label from \(\), conditioned on the task \(d_{i}\), the observation \(o_{i}\), along with hand-crafted few-shot examples. This is followed by prompting the LLM to produce _rationale_\(k_{i}\) given the ground truth valid options \(_{i}\). Specifically, we use in-context learning with few-shot examples to guide LLM generating explanations of why certain options are valid according to the ground truth. Incorporating ground truth actions directly into the prompt allows LLMs to generate reasoning that more closely aligns with the actual options. To facilitate retrieval during the inference phase, we compute the textual embedding of each instruction \(d_{i}\) as the key to the knowledge and store them in the knowledge base dictionary \(\). We summarize the procedure of knowledge base construction in algorithm 1.

**Planning with Knowledge Retrieval.** At inference time, the planner selects the most pertinent reasoning examples from the knowledge base \(\) to aid the LLM's reasoning. Given a test instance \(x_{}=(d_{},o_{})\), we compute the cosine similarity between the text embedding of \(d_{}\) and all keys of \(\). As shown in Figure 1, we retrieve the most relevant knowledge corresponding to the \(m\) most similar embeddings as prompt and leverage the in-context learning capabilities of the LLM to generate possible plans and reason about their feasibility. To select the desired robot plan \(_{}\) with generated reasoning, we can use two distinctive prediction methods: (1) **Direct Prediction**: We ask the LLM to output the best option \(_{}\) along with all possible plans and explanations. (2) **Conformal Prediction**: Instead of directly predicting \(_{}\), we construct a set of valid candidate plans \(}_{}_{}\) by querying LLM's confidence \((y_{i}|x_{},_{},k_{})\) for each label \(y_{i}\) given the prompt constructed by knowledge retrieval process and generated reasoning. The robot will request human for help if multiple valid options are included in the \(}_{}\). In the following section, we demonstrate how to incorporate introspective planning with conformal prediction.

```
1:\(=\{(x_{1},_{1}),,(x_{N},_{N})\}\)
2:\(\{\}\)\(\) Knowledge Base Initialization
3:for each train example \(x_{i}=(d_{i},o_{i})\)do
4:\(_{i}(x_{i})\)
5:\(k_{i}(x_{i},_{i},_{i})\)
6:\(e_{i}(d_{i})\)
7:\([e_{i}]\{x_{i},_{i},k_{i},_{i}\}\)
8:endfor ```

**Algorithm 1** Knowledge Base Construction

```
1:\(x_{}=(d_{},o_{})\), \(\), \(m\), \(\)
2:\(_{}(e_{}, .(,m)\)
3:\(_{},k_{}(x_{}, _{})\)
4:\(}_{}(x_{}, _{},k_{},)\)
5:if\(}_{}|==1\)then\(y_{}}_{}\)
6:else Request further instructions
7:endif ```

**Algorithm 2** Introspective Conformal Prediction

## 3 Introspective Conformal Prediction

Successful human-centered robot autonomy hinges on accurate comprehension of users' goals--in situations where a user-specified task admits multiple valid interpretations, it is crucial for the robot to detect task ambiguity and solicit further instructions. Directly querying language models for a prediction (even with few-shot in-context learning strategies) falls short of providing clear confidence indicators. This can result in overconfident decisions that clash with user expectations. On the other hand, conformal prediction offers the advantage of providing quantifiable confidence levels for its predictions, enabling a clearer understanding of a model's certainty in its outcomes. However, its effectiveness can be compromised if the underlying model lacks strong reasoning abilities. In extreme cases, to maintain high success rates, it might output an excessively broad range of options, including irrelevant or unsafe ones. In this section, We augment introspective planning with conformal prediction to provide a tighter bound on the statistical guarantee of success. The synergy of these approaches is illustrated in Figure 2.

Conformal Calibration.Consider a calibration dataset \(=\{(x_{i},_{i},k_{i},z_{i})\}_{i=1}^{N}\), comprising tuples that include tasks \(x_{i}\), plans \(_{i}\), rationale \(k_{i}\), and user intents \(z_{i}\). These tuples are drawn independently from an unknown distribution. The goal of conformal prediction is to generate a prediction set \(}_{}_{}\) for new samples, ensuring that the actual user intent \(z_{}\) is likely to be included. Specifically, conformal prediction aims to achieve:

\[(z_{}}_{}) 1-,\] (1)

where \(1-\) represents the desired level of confidence. During the calibration process, we compute nonconformity scores \(S=\{s_{i}:s_{i}=1-(z_{i}|x_{i},_{i},k_{i})\}_{i=1}^{N}\) using the confidence score \(\) from the LLM for all samples of \(\). The critical threshold, \(\), represents the empirical quantile calculated at the \((N+1)(1-)}{N}\) position within these scores, which follows:

\[=(s_{1},...,s_{N}; {N})\] (2)

Conformal Prediction.Utilizing the calibrated threshold \(\), we construct the prediction set for a test instance \(x_{}\) by including all options \(y\) for which the confidence level meets or exceeds \(1-\) as:

\[}_{}=\{y_{}|(y|x_{ },_{},k_{}) 1-\}.\] (3)

This approach ensures the coverage criterion specified in Equation (1), providing a statistically justified guarantee for the comprehensiveness of the prediction set. The proof is shown in Appendix E.

As the marginal guarantee in Equation (1) depends on both the calibration set and the test set, every time we have a new test instance \(z_{test}\), we would ideally sample a new calibration set to maintain the same level of statistical assurance, which could be too resource-intensive. However, we can choose \(N\) large enough to control the fluctuations in coverage by analyzing its distribution. The distribution of coverage has an analytic form as follows :

\[(z_{}}_{}|\{z_{1},,z_ {N}\})_{N+1-l,l}^{-1}(),\] (4)

where \(l=(N+1)\), \(_{N+1-l,l}^{-1}()\) denotes the inverse CDF (quantile) level of \(\) in a Beta distribution with parameters \(N+1-l\) and \(l\), and \(\) is the threshold used for calibration. Additionally, prior research by Sadin (2019)  demonstrates that conformal prediction minimizes the prediction set size, suggesting that robots employing this method require the least human intervention while attaining desired success rates.

Prior work KnowNo  utilizes a similar conformal prediction approach for planning. In this work, we significantly enhance this framework by incorporating introspective planning rationale \(k_{i}\) to improve the likelihood function's effectiveness. This adjustment optimizes the distribution of nonconformity scores, leading to a tighter concentration around smaller values. Such refinement leads to tighter bounds, improving the framework's reliability and reducing its conservativeness, as will be demonstrated in subsequent sections. We summarize the procedure of introspective conformal prediction in Algorithm 2.

Figure 2: Demonstration of using conformal prediction with Introspective Planning. After generating multiple options, we query the LLM for the explanation by introspective planning and then ask the model to predict the most correct option. Based on the likelihood scores of true intents from a calibration dataset, conformal prediction finds the quantile value \(\) (0.85), and includes any options scoring above \( 1-=0.15\) in the prediction set for each test scenario. This method guarantees the correct answer is included among the options, at a confidence level specified by the user.

## 4 Evaluation

### Evaluation Method

**Asking for help is not enough.** Previous work uses success rate and help rate as metrics, but these do not fully capture a planner's performance. For example, if the instruction is to bring the soda and the robot's prediction set includes Coke, Sprite, and apple, the robot will ask for help but ask the wrong question due to an irrelevant option. Neither the success rate nor the help rate captures this issue because'success' is only defined as the prediction set containing the user intent. Additionally, help rate can sometimes be misleading. A low help rate does not necessarily indicate good performance, as an effective predictor should ideally seek help whenever instructions are ambiguous.

To address these, we categorized errors into three types: **(1)** The robot is uncertain, but the task is unambiguous. **(2)** The robot is certain but wrong. **(3)** The robot is uncertain, and the task is ambiguous, but it asks the wrong question. Based on this analysis, we proposed new metrics to capture these errors. Exact set rate and non-compliant contamination rate effectively measure the error type (3). Overask rate captures the error type (1) while the overstep rate captures the error type (2). Additionally, we propose Unsafe Contamination Rate (UCR) and Unsafe Rate to measure the robot's performance in prioritizing safety, which previous metrics do not account for.

**Metrics.** Beyond the success rate and help rate, we introduce additional metrics to more comprehensively evaluate the performance of our planner.

* Success Rate (SR): How often the language model's predictions match the user's intent, calculated as the percentage of cases where the predicted actions include the correct intent.
* Help Rate (HR): Fraction of cases where the prediction set encompasses more than one option, such that robots will require further human instructions, \(=N_{}/N\).
* Exact Set Rate (ESR): Frequency of the LLM's predictions perfectly aligning with all valid actions inferred from instructions. It evaluates the model's ability to generate precise responses.
* Non-compliant Contamination Rate (NCR): Proportion of prediction sets containing options that deviate from the given instructions, measuring the LLM's ability to follow instructions accurately and ask the right questions to clarify uncertainty.
* Unsafe Contamination Rate (UCR): Frequency at which the prediction sets include potentially unsafe options, assessing the model's ability to prioritize safety in responses.
* Overask Rate: Fraction of instances when the planner is uncertain while the task is unambiguous. Count (robot is uncertain while the task is unambiguous)/Count (task is unambiguous)

Figure 3: **Qualitative results on Safe Mobile Manipulation. We compared our approach with KnowNo , both using conformal prediction with an 85% target success rate. Our method generates explanations via introspective planning before applying conformal prediction, whereas KnowNo directly predicts valid options using conformal prediction. We observed that KnowNo _over-step_ in the left case and _over-ask_ in the right case while IntroPlan generates more precise prediction sets.*** Overstep Rate: Fraction of the planner generating over-confident or incorrect options when the planner is certain. Count (robot is certain but wrong)/Count (robot is certain)
* Unsafe Rate: Frequency at which planner is certain to execute unsafe action.

**Baselines.** We benchmarked our proposed introspective planning against various prompt-based methods to gauge its effectiveness in LLM reasoning. The _Prompt Set_ instruct the LLM to directly output the prediction through few-shot in-context learning. _Prompt Set+CoT_ applies a Chain of Thought (CoT) process to simultaneously produce explanations and predictions. _Retrieval-Q-CoT_ utilizes CoT to generate reasoning in a training dataset and retrieves the most relevant prompt during inference. _Auto-CoT_ automates prompts selecting process by using clustering to ensure a broad representation of diverse scenarios.

To further illustrate the effectiveness of introspective planning in enhancing conformal prediction, we compared our method with _KnownNo_, which integrates conformation prediction with _Prompt Set_. Additionally, we employed conformal calibration for _Retrieval-Q-CoT_ serving as an extra baseline due to its use of retrieval augmentation for reasoning. All calibration processes used the same dataset with 400 instances. We set \(=0.01\) to be consistent with KnowNo, ensuring that the empirical coverage exceeds the conditional coverage with probability \(1-=0.99\).

### Datasets

**Mobile Manipulation:** The original calibration or training dataset comprises 400 examples, while the test set includes 200 examples. We also evaluated the robustness to two kinds of distribution shifts: covariate shift and concept shift, using three additional datasets: one with 200 unambiguous instructions, another with 200 ambiguous instructions, and a third with 100 novel scenes and instructions. The original dataset follows the same distribution of different types of examples as in KnowNo , encompassing a range of types such as single-label, multi-label, spatially-ambiguous, unsafe, and Winograd tasks. The experiment results on this dataset are in Appendix A.

**Safe Mobile Manipulation:** We augment the original mobile manipulation dataset to emphasize safety, with 400 examples for calibration and 200 for testing. Additionally, we assembled a dataset of 200 safety-critical scenarios, categorized into three types: (1) ambiguous instructions that become clear when safety is considered, e.g., choosing between a metal and plastic bowl for microwave use, (2) ambiguous instructions considering safety, such as selecting the correct bowl for microwave heating among stainless steel, plastic, or ceramic options, and (3) unambiguous but unsafe instructions, such as 'place a metal bowl in the microwave'.

**Tabletop Rearrangement:** The task involves moving colored blocks and bowls on a table according to specific instructions. These instructions are intentionally designed to include ambiguities in attributes (such as alternative names for objects and colors), numbers (using vague terms for quantities), and spatial relationships (using general terms for directions and orientations). For this dataset, 400 examples were used for calibration and an additional 200 examples for testing. The experiment results on this dataset are in Appendix A andeach dataset are detailed in Appendix B.

## 5 Results

**Implementation details.** We implemented all tasks using OpenAI's GPT-3.5 (text-davinci-003) and GPT-4 Turbo (gpt-4-1106-preview). We only presented GPT-4 Turbo results on Safe Mobile Manipulation in the main paper. All of the other results are in Appendix A. We employed Sentence-BERT  to encode instructions, retrieving the top \(m=3\) text embeddings based on cosine similarity. We used the default temperature of 0 to sample the LLM's response. The knowledge base and the calibration set contain 400 tasks each. Appendix C contains an in-depth experimental exploration of performance variation with knowledge base size, with sizes of 10, 50, 100, and 200, suggesting that modest knowledge bases with around 100 examples still enable satisfactory results.

**Trade-off between direct and conformal prediction:** Our experiments indicate that introspective planning with direct prediction significantly outperforms all other baselines in terms of performance metrics. However, this approach does not guarantee success. On the other hand, introspective planning with conformal prediction guarantees success and surpasses other methods employing conformal prediction. Nevertheless, a noticeable performance gap exists between direct prediction and conformal prediction. This highlights an intriguing trade-off: while conformal prediction provides success guarantees, it tends to be more conservative.

### Direct Prediction

In Tab. 1, our analysis highlights that introspective planning with direct prediction outperforms all baseline methods in both decision and prediction metrics. KnowNo is too conservative with low and Non-compliant contamination rate (NCR) and exact set rate (ESR). As a result, despite guaranteeing a high success rate, it suffers from a very high Over-Ask Rate (OAR), which is not

    &  &  \\  Method & ESR \(\) & NCR \(\) & UCR \(\) & SR \(\) & HR & OAR \(\) & OSR \(\) & UR \(\) \\  KnowNo (Conformal) & 37.5 & 51.0 & 7.0 & 84.5 & 77.5 & 51.3 & 35.3 & 1.0 \\ Prompt Set & 73.5 & 11.5 & 3.5 & 82.5 & 63.0 & 3.8 & 36.2 & 2.5 \\ Prompt Set + CoT & 79.0 & 10.0 & 5.0 & 87.5 & 67.0 & 10.3 & 30.8 & 4.0 \\ Retrieval-Q-CoT & 81.5 & 7.0 & 4.5 & 88.0 & 65.0 & 2.6 & 26.1 & 4.0 \\ Auto-CoT & 77.5 & 10.0 & 5.0 & 85.5 & 62.5 & 1.3 & 37.3 & 4.0 \\ 
**Ours (Conformal)** & 58.0 & 27.5 & 3.0 & 87.5 & 63.0 & 6.4 & 21.6 & 1.5 \\
**Ours (Direct)** & **93.0** & **5.5** & **0.5** & **96.5** & 67.5 & **0.0** & **3.8** & **0.5** \\   

Table 1: **GPT-4 Results for Safe Mobile Manipulation. SR: Success rate, HR: Help rate, OAR: Over-Ask rate, OSR: Over-Step rate, UR: Unsafe rate, ESR: Exact Set Rate, NCR: Noncompliance contamination rate, UCR: Unsafe contamination rate. Conformal means conformal prediction and Direct means direct prediction. All the others use direct prediction. The target success rate for conformal prediction is \(85\%\). All numbers are reported in percentages.**

Figure 4: Variation of different performance metrics with respect to the Target Success Rate (TSR). Each subplot compares KnowNo, Retrieval-Q-CoT, and Ours (Conformal) methods across various metrics. Introspective planning (Ours-Conformal) consistency achieves the best tradeoff between performance metrics and Target Success Rate (TSR) across all comparisons.

ideal. Without conformal prediction, the _Prompt Set_ method generates more accurate prediction sets but sacrifices the guarantee of success, as indicated by the 19.5% increase in ESR. Using Chain of Thought (CoT) further improves performance. Retrieval-Q-CoT and Auto-CoT, which utilize retrieval augmentation, do not show significant improvement compared to simpler prompting approaches. This is because the model frequently generated misleading knowledge during training without grounding in human feedback. Interestingly, Auto-CoT is more overconfident in its predictions, indicated by a low over-ask rate and high over-step Rate.

Compared to other baselines, Introspective planning guides the LLM to generate more precise prediction sets, as evidenced by the highest exact set rate and lowest non-compliant contamination rate. It avoids over-asking, rarely oversteps, and has the lowest unsafe contamination rate and unsafe rate, demonstrating effective reasoning about both uncertainty and safety.

### Conformal Prediction

In Figure 4, we compare introspective planning with two baselines using conformal prediction. The findings (Figure 3(a)) confirm that conformal prediction aligns the empirical success rate with target success rate. Notably, Figure 3(d) shows that our method consistently achieves a higher Exact Set Rate across the full range of target success rate, outperforming both KnowNo and the Retrieval-Q-CoT.

Our analysis indicates that both Retrieve-Q-CoT and KnowNo are more conservative and generate larger prediction sets to achieve desired success rate levels, as indicated by Figure 3(c). Consequently, they more frequently include irrelevant options, resulting in high non-compliant contamination rate (NCR) as shown in Figure 3(e). In contrast, introspective planning can better reason about ambiguity. It excels in unambiguous scenarios, as both Retrieval-Q-CoT and KnowNo over-ask much more frequently than introspective planning across the target success rate, as indicated by Figure 3(g). In ambiguous tasks, Retrieval-Q-CoT oversteps less initially, but the rate does not decrease significantly as the target success rate increases. Conversely, our approach effectively reduces the overstepping rate while maintaining the lowest over-asking rate, as shown in Figure 3(h). Furthermore, we examined whether introspective planning improves reasoning about unsafe actions in robot planning. Results in Figure 3(f) show that introspective planning maintains the lowest Unsafe Contamination Rate (UCR) across all target success rate levels, indicating its effectiveness in reasoning about unsafe options.

From the conformal prediction perspective, we observed that introspective planning has a lower \(\), resulting in a higher calibration threshold \(1-\), compared to the two baselines, as shown in Figure 3(i). This indicates that our method achieves a tighter confidence bound for the statistical guarantee.

## 6 Related Work

**LLMs as reasoning engines.** Through a process known as (zero-shot) chain-of-thought (CoT), LLMs can be prompted to generate multiple reasoning steps by instructing them to "think step by step" at inference time . This method's accuracy can be improved by including manually designed examples (few-shot CoT) . The tree-of-thoughts approach  generalizes CoT by considering multiple reasoning paths. Our work is inspired by the retrieval augmentation mechanism in Auto-CoT  and Retrieval-Q-CoT , which first use zero-shot CoT to generate a diverse set of reasoning chains and then sample them at runtime as few-shot CoT examples. However, these pre-generated reasoning examples are sometimes incorrect due to LLM hallucination, leading to inference-time errors. As demonstrated in Section 5, our new knowledge base generation approach substantially addresses this issue by instead querying the LLM for _post-hoc rationalizations_ conditioned on human-provided valid/invalid labels on candidate solutions.

**Retrieval-augmented generation.** Retrieval-augmented generation (RAG) augments the input space of LLMs with retrieved text passages, significantly improving performance on knowledge-intensive tasks [10; 20; 29]. Traditional RAG methods typically use open-source, off-the-shelf knowledge bases for text generation [16; 33; 25; 47; 43; 51]. Conversely, our approach retrieves few-shot introspective reasoning examples from the knowledge base. This guides LLMs to explicitly reason about uncertainties and safety, formulating plan in a structured format, as shown in Tab. 13. In practice, we observe this strategy results in a more grounded reasoning process compared to conventional RAG, which relies on open-source knowledge bases. While existing RAG literature primarily addresses content hallucination, our approach aims to equip language agents with the capability to introspect and refine their own uncertainties. This emphasis allows for uncertainty-aware planning in robotics and achieves tighter statistical guarantees with conformal prediction.

**LLMs as planners.** Emergent reasoning allows LLMs to break down a task into intermediate subgoals and generate actions as a sequence of plans . Through prompting and in-context learning, LLMs can ground human instructions in natural language into executable robot actions conditioned on scene descriptions [21; 35; 1; 12]. Recent works further enhance the reasoning and planning ability by iteratively refining actions through self-reflection during planning [45; 24; 28; 6; 34; 39; 23]. ReAct  and Reflexion  focus on _multi-step planning_ scenarios, in which robots can execute certain actions, observe the state feedback, and replan for correction. However, in safety-critical robotic applications, certain invalid actions can immediately lead to catastrophic safety failures that cannot be recovered from. Therefore, instead of relying on self-correction by trial and error, our method uses retrieval augmentation to guide the language agent to proactively reason about task compliance and safety at the planning stage. Additionally, recent work  has shown that LLMs cannot plan effectively through self-reflection alone but can do so when integrated with external verifiers, aligning with our method that employs LLMs to support planning by constructing an external knowledge base.

**Quantifying uncertainty in LLMs.** There is a growing interest in the natural language processing community to quantify uncertainty in LLM outputs [27; 8; 42; 4], calibrate this uncertainty in light of empirical accuracy [7; 15; 50; 46], and examine model reliability [14; 22]. Our work is most closely related to the recently proposed KnowNo framework , which casts task-level planning as multiple choice question answering (MCQA) and uses conformal prediction to output a subset of LLM-generated candidate plans with a desired (marginal) probability of containing at least one valid course of action. Unfortunately, the statistical guarantees achieved by KnowNo come at the cost of frequent superfluous user queries (or _overasking_, as defined in Section 4 and empirically quantified in Section 5). In contrast, our method introduces a new introspection-based approach to automatically align the robot's uncertainty with the inherent task specification ambiguity before predicting a high-confidence subset of plans. This uncertainty alignment step mitigates the need for conservativeness in the calibration stage and drastically reduces the resulting rate of overasking while maintaining the desired statistical success guarantees.

## 7 Conclusion

This paper proposes and investigates a novel _introspective planning_ framework that allows language-enabled agents to align their decision-making uncertainty with safety and task ambiguity. We conducted thorough evaluations on three different datasets and found that introspective planning improves upon the state of the art across multiple relevant metrics. In addition, we show that introspection can be integrated with the conformal prediction framework, achieving strong statistical guarantees with fewer superfluous user clarification queries.

**Limitation.** First, there is still a significant performance gap between direct prediction and conformal prediction, which future work should aim to reduce. Second, the current single-label conformal prediction approach assumes that options are mutually exclusive. A more appropriate approach would be multi-label conformal prediction to account for non-mutually exclusive hypotheses and better handle truly ambiguous tasks. However, our initial attempt generated very conservative prediction sets, which were not as effective as the single-label conformal prediction approaches. This limitation highlights an opportunity for future research to develop methods that improve the performance of multi-label prediction sets, making them more effective than their single-label counterparts.

## 8 Broader Impact

In this paper, we propose a robust method to derive and quantify inference confidence in foundation models from the models' intrinsic ability to reason logically and semantically about uncertainties. Our belief is that introspective planning could serve as a general method to extend reasoning in foundation models beyond robotic applications.

However, as stated previously, our method's inability to differentiate between distinct types of uncertainties warrants concern when implementing our model. Specifically, deploying this uncertainty quantification method in safety-critical systems could result in inadequately safe behaviors.