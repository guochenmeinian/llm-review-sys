# Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE

Xun Zhu1 Ying Hu1 Fanbin Mo2 Miao Li1,* Ji Wu1,3,4

1 Department of Electronic Engineering, Tsinghua University
2 Beijing University of Posts and Telecommunications 3 College of AI, Tsinghua University

4 Beijing National Research Center for Information Science and Technology

{zhu-x24, yinghu_y}@mails.tsinghua.edu.cn mofanbin@bupt.edu.cn

{miao-li, wuj_ee}@tsinghua.edu.cn *corresponding author

###### Abstract

Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/MSIIP/Uni-Med.

## 1 Introduction

Driven by the growth of datasets, the increase in model size, and advances in generative language foundation models , multi-modal large language models (MLLMs) now offer unprecedented abilities as general-purpose interfaces. These advancements are spurring innovation across various visual and linguistic tasks . While significant strides have been made in building a unified foundation model for natural scenery , the development of generalist medical artificial intelligence is still in its early stages .

The goal of a unified and generalist medical foundation model is to enable joint training on massive medical datasets. This model aims to handle multiple tasks and modalities within a single architecture with shared parameters . It seeks to eliminate the need for task-specific modules and further fine-tuning, thereby revolutionizing the traditional task-specificapproach to model development [Wu _et al._2023b; Tu _et al._2024]. However, existing open-source efforts have not yet fully achieved these ambitious goals.

A key challenge in creating a unified medical foundation model is the complexity of multi-modal, multi-task learning, often exacerbated by the tug-of-war problem [Hadsell _et al._2020]. Inherent task conflicts and data imbalances can cause interference during the simultaneous learning of different tasks. This problem is particularly acute in the medical field, where tasks and modalities are highly specialized and diverse. As a result, the performance of each task may degrade compared to task-specialized models [Yu _et al._2020; Zhu _et al._2022].

To mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) [Jacobs _et al._1991] into MLLMs. Figure 1 illustrates three distinct hypotheses and their corresponding architectural implementations for multi-task learning in MLLMs. The first "synergy hypothesis" suggests that all tasks benefit from a fully shared backbone comprising a visual encoder, connector, and language model, which is the standard architecture for MLLMs. The second "conflict hypothesis", proposes that each task requires its own specific adaptations, thereby preventing knowledge sharing among tasks. The third "conflict-synergy coexistence hypothesis", posits that all tasks share multi-task adaptations, which reduces interference and promotes more efficient knowledge sharing. However, current research [Zadouri _et al._2023; Gou _et al._2023; Liu _et al._2023b; Lin _et al._2024] mainly tailors the MoE approach to the language model components, overlooking the potential benefits of exploring and enhancing the connector in MLLMs. Furthermore, the optimization of the tug-of-war problem lacks a detailed, explainable analysis.

In this study, we first identify a tug-of-war problem in multi-task learning at the connector level within standard MLLM architectures. This issue indicates that different tasks may emphasize different types of features in multi-modal, multi-task scenarios. Consequently, a fully shared connector may fall short as it cannot accommodate the diverse modal features required by each task. Drawing inspiration from the successful application of MoE in LLMs, we introduce Connector-MoE (CMoE), a novel approach that employs a mixture of projection experts to align visual and language embedding spaces effectively, thus mitigating the tug-of-war problem. As a pioneering effort in constructing a unified generalist foundation model in the medical field, we present Uni-Med. This model comprises a universal visual feature extraction module, a CMoE module, and an LMM. Uni-Med demonstrates impressive performance across six distinct medical tasks, with minimal training computational overhead. It achieves joint training on 12 datasets on a single A800 in under 10 hours. The effectiveness and generalization of CMoE are underscored through ablation experiments. Additionally, an interpretable analysis reveals that Uni-Med provides a superior solution to the tug-of-war problem at the connector level. Overall, Uni-Med delivers competitive or even superior performance compared to open-source, state-of-the-art medical MLLMs on all test sets. Our contributions can be summarized as:

* We present Uni-Med, an open-source medical generalist foundation model with a unified interface and shared parameters, which can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.
* We propose CMoE, a well-designed connector component for MLLMs, which significantly outperforms baselines under any configuration, with up to an average 8% performance gains. To our knowledge, Uni-Med is the first attempt to focus on the connector in MLLMs to mitigate the tug-of-war problem, which is critical but has always been overlooked.

Figure 1: Three hypotheses and corresponding architectural implementations for multi-task learning in MLLMs. (a) Synergy hypothesis. (b)-(c) Conflict hypothesis in LLM and connector, respectively. (d)-(e) Conflict-synergy coexist hypothesis in LLM and connector, respectively.

* Focusing on the question of how the tag-of-war problem is optimized, which has never been quantitatively discussed, we provide detailed interpretability analysis and instructive findings from the perspective of gradient optimization and parameter statistics.
* Uni-Med achieves competitive or superior performance compared to the open-source, state-of-the-art medical MLLMs on test set of diverse tasks and datasets, which demonstrates the huge potential of medical generalist foundation models.

## 2 Related work

Medical foundation modelsThe increasing availability of medical data, as well as advances in multi-modal LLM technologies, have paved the way for the emergence of medical foundational models. Med-Flanning (Moor _et al._, 2023) continues pre-training on paired and interleaved medical image-text data based on OpenFlamingo (Awadalla _et al._, 2023). LLaVA-Med (Li _et al._, 2024) curates a medical multi-modal instruction following dataset and fine-tunes LLaVA (Liu _et al._, 2024) with it. YrayGPT (Thawkar _et al._, 2023) can analyze and answer open-ended questions about chest X-rays. BiomedGPT (Zhang _et al._, 2023) is a multi-task foundation model pretrained on a diverse source of medical images, literature, and clinical notes. However, most of these efforts require further fine-tuning on task-specific data to support downstream applications. One step further, the generalist foundation model uses the same weight to excel at various tasks without fine-tuning. RadFM (Wu _et al._, 2023) is dedicated to build a generalist foundation model for radiology. Med-PaLM M (Tu _et al._, 2024) is directly trained in a unified framework to jointly handle many tasks, which is perhaps most similar to our effort, but it does not provide access for usage. In addition, recent studies (Wu _et al._, 2023; Yan _et al._, 2024; Xia _et al._, 2024) have suggested the the necessity for a more comprehensive and detailed evaluation of the capabilities of medical MLLMs.

MoE in multi-task learningMoE is originally considered to increase the model capacity (Riquelme _et al._, 2021; Fedus _et al._, 2022) and gains popularity in mitigating multi-task interference (Chen _et al._, 2023, 2024). It achieves this by utilizing a router to determine the token set handled by each expert, thus reducing interference between different types of samples. Recent studies have focused on combining MoE with LLM, such as MoE-LLaVA (Lin _et al._, 2024) and Mikrtal 8x7B (Jiang _et al._, 2024), or combining MoE with one of the representative parameter-efficient tuning techniques, i.e., LoRA (Hu _et al._, 2021), such as Octavius (Chen _et al._, 2023), MoCLE (Gou _et al._, 2023), MTLoRA (Agiza _et al._, 2024) and MOELoRA(Liu _et al._, 2024). However, neither of them introduces MoE into the connector component for MLLMs. Furthermore, there is a lack of clear and explicit interpretable analysis on how the multi-task interference is mitigated through the use of MoE.

Cross-modality connector in MLLThe connector between the multi-modal encoder and the LLM is critical in aligning multi-modal features (Song _et al._, 2023). One of the most popular paradigms is to map multi-modal features into a feature space that aligns with language, such as linear projection (Liu _et al._, 2024) and MLP projection (Liu _et al._, 2023; Chen _et al._, 2023). Another paradigm is to transform multi-modal features into multi-modal tokens that are consistent with the embedded representation space of LLM, such as cross-attention (Ye _et al._, 2023; Li _et al._, 2022; Ye _et al._, 2023), perceiver resampler (Alayrac _et al._, 2022; Peng _et al._, 2023) and Q-Former (Li _et al._, 2023; Zhu _et al._, 2023). However, existing paradigms use the same connector when processing the same modal data for different tasks, ignoring the imperative to acquire distinct alignment patterns tailored to the demands of each task.

## 3 Methodology

### Preliminaries

#### 3.1.1 Multi-task interference

To quantify the intricate tag-of-war problem in a unified foundation model, we provide interpretability from the perspective of gradient optimization and parameter statistics.

Perspective of gradient optimizationWhen optimizing the shared parameters \(\) according to task \(j\), the change in the update direction of loss \(L_{i}\) for task \(i\) can be defined as (Zhu _et al._, 2022):\[_{j}L_{i}(x_{i})_{x_{j}}(L_{i}(x_{i}; )-L_{i}(x_{i};-L_{j}(x_ {j})}{\|_{}L_{j}(x_{j})\|_{2}}) )_{x_{j}}(L_{j}(x_ {j})^{T}}{\|_{}L_{j}(x_{j})\|_{2}} _{}L_{i}(x_{i}))\] (1)

where \(x_{i}\) and \(x_{j}\) are the sampled training batches of task \(i\) and \(j\), respectively. The interference of task \(j\) on task \(i\) in the update direction can be quantified as:

\[_{i,j}=_{x_{i}}(L_{i}(x_{i} )}{_{i}L_{i}(x_{i})})\] (2)

The gradient magnitude similarity between task \(i\) and task \(j\) can be defined as:

\[_{i,j}=_{j,i}=_{x_{i}}(\| _{}L_{i}(x_{i})\|_{2})_{x_{j}} (\|_{}L_{j}(x_{j})\|_{2})}{( _{x_{i}}(\|_{}L_{i}(x_{i})\|_ {2}))^{2}+(_{x_{j}}(\|_{}L_{ j}(x_{j})\|_{2}))^{2}}\] (3)

\(_{i,j}\) goes to zero when the difference in gradient magnitudes is large, indicating that some task is dominant (Yu _et al._, 2020). For all \(T\) tasks, we can get \(},}^{T T}\). Then, we define the tug-of-war indexes for each task in multi-task learning through the function \(G\) as follows:

\[=G(},})=[_{j=1}^{T}_{i,j}_{i,j} ]_{i=1}^{T}\] (4)

Perspective of parameter statisticsInspired by the Gradient Positive Sign Purity proposed by Chen _et al._ (2020), we define the statistics score of a single parameter in multi-task learning:

\[=|^{T}_{}L_{i}}{| _{i}^{T}|_{}L_{i}||}|\] (5)

where \(_{}L_{i}\) is the gradient for task \(i\). The range of the statistics score is , and a value close to 1 indicates that this parameter suffers less gradient conflict during multi-task training. Upon collecting the statistics scores of all parameters, we can intuitively demonstrate and analyze the phenomenon of multi-task interference.

To be specific, we sample 100 batches for each datasets and record the gradients to calculate all of the above metrics. Figure 2 shows the dataset-level (more granular than task-level) multi-task interference of the synergy hypothesis model at the connector in the standard MLLM architecture.

#### 3.1.2 Mixture-of-Experts

A Mixture-of-Experts (MoE) contains a set of expert networks \(E_{1},E_{2},...,E_{N}\) along with a routing network \(R\). For each token \(x_{i}\) in the input sequence \(=\{x_{i}\}_{i=1}^{L}\), the output of MoE is the weighted sum of outputs from each expert, where the weight is calculated by the router:

\[y_{i}=_{k=1}^{N}R(x_{i})_{k} E_{k}(x_{i})\] (6)

Figure 2: Dataset-level multi-task interference of the synergy hypothesis model at the connector in MLLMs. (a) Perspective of gradient direction \(}\). (b) Perspective of gradient magnitude \(}\).

The types of \(R\) can mainly be divided into: 1) Constant router, which assigns equal weight to each expert. 2) Hard router, which enforces one-to-one mapping between tasks and experts. 3) Sparse router, which selects Top-K experts with the maximum routing weight. 4) Soft router, which calculates the routing weights for each expert. For more details on the routing networks, see Appendix A.1.

### Model Architecture

With the primary goal of achieving a unified medical generalist foundation model and mitigating the tug-of-war problem of multi-task learning in mind, we design the overall architecture of Uni-Med as illustrated in Figure 3, which contains three components: a universal vision feature extraction module, a connector-MoE module and an LLM. Detailed descriptions are presented in the following sections.

#### 3.2.1 Visual feature extraction module

Taking one of the multi-modal medical images \(^{H W C}\) as input, the visual encoder \(_{en}\) extracts the image tokens \(_{v}^{N_{v} D_{v}}\) for image perception, where \(N_{v}=HW/P^{2}\) is the number of image patches and \(D_{v}\) is the hidden size of visual embeddings.

To alleviate the efficiency issues caused by prolonged visual input tokens during the training and inference, we scheme a resumpler with a compression rate \(\) for visual feature aggregation. Concretely, \(\) adjacent visual tokens are concatenated and projected into one single embedding. Thus we obtain aggregated image tokens \(_{v}^{ag}^{N_{v}/ D_{v}}\) as follows:

\[_{v}^{ag}=(_{en}(),)\] (7)

#### 3.2.2 Connector-MoE module

Aligning the visual space with the language embedding space of the large language model is a critical process, especially in the complex and diverse input of multi-task multi-modal medical image text pairs. Based on the conflict-synergy coexist hypothesis, we propose the Connector-MoE (CMoE) module, which aims to adaptively minimize task conflict and maximize task synergy at the connector. CMoE module has \(N\) projection experts \(E_{1},E_{2},...,E_{N}\), where each expert is a two-layer MLP, and a soft router \(R_{}\) to control the contribution of each expert.

According to Figure 2, we find that: (1) Gradient optimization conflict is common and consistent at the task level. (2) Even for the same task, there are significant differences in conflict and synergy at dataset-level. To alleviate the above problems, we randomly initialize vision-level special task tokens \(\{_{t}^{sp}\}_{t T}\), where \(_{t}^{sp}^{D_{v}}\) and \(T\) is the set of tasks. \(R_{}\) is a lightweight MLP designed to receive the concatenated inputs of \(_{v}^{ag}\) (token level) and \(_{t}^{sp}\) (task level), and calculate the routing weights \(_{}^{D_{v}/ N}\) of each expert for each image token, which can be formulated as:

Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.

\[_{}(_{v}^{ag})= R_{} ([_{v}^{ag},Repeat(_{t}^{sp})])\] (8)

where \([,]\) denotes concatenation operation, \(\) is _SoftMax_ function. Then we can obtain aligned visual tokens \(_{v}^{align}^{N_{v}/ D_{t}}\) through a weighted sum of all experts' output as follows:

\[_{v}^{align}=_{k=1}^{N}_{,k} E_{k}(_{v }^{ag})\] (9)

where \(D_{t}\) is the hidden size of the language embedding space of the large language model and \(_{,k}\) denotes the routing weight of the \(k\)-th projection expert. We discuss and analyze the effects of router type, router strategy, and number of experts in Section 4.2.1.

#### 3.2.3 Large language model

Similar to the vision-level special task tokens, we assign the text-level special task identifiers for question answering (QA), visual question answering (VQA), report generation (RG), referring expression comprehension (REC), referring expression generation (REG) and image classification (CLS) as shown in Table 1, which can help reduce multi-task ambiguity . The text prompt is designed as "<Img> < ImageFeature> </Img> [Task Identifier] Instruction", which merges the converted image features with the textual instructions. See details about our multi-task instruction template in Appendix C.

After word embedding, we can obtain textual tokens \(_{t}^{N_{t} D_{t}}\), where \(N_{t}\) denotes the number of textual tokens. LLM generates the response \(=\{O_{i}\}_{i=1}^{L}\) conditioned on the aligned visual tokens \(_{v}^{align}\) and textual tokens \(_{t}\) inputs in an autoregressive manner, which can be formulated as:

\[p(_{t}_{v}^{align},_{t})=_{i=1}^{L }p(O_{i}_{v}^{align},_{t},O_{<i})\] (10)

where \(L\) is the length of output tokens. We use low-rank adaption (LoRA)  for efficient LLM fine-tuning, which is applied to all the linear layers.

## 4 Experiments

### Experiment settings

Tasks and datasetsText-only data is collected from MedQA  and PubMedQA  for the task of QA. Image-text pairs are collected from Path-VQA  and Slake-VQA  for the task of VQA, MIMIC-CXR  and MPx-Single  for the task of RG, MedMNIST v2  for the task of CLS. For tasks such as REG and REC that require representation of spatial locations, we use the bounding boxes of the format "<\(X_{min}\)><\(Y_{min}\)><\(X_{max}\)><\(Y_{max}\)>", which denotes the coordinates of objects. Then, we respectively process datasets Slake-VQA  and SA-Med2D-20M  to get datasets Slake-REC, Slake-REG, SA-Med2D-REC, and SA-Med2D-REG. For a detailed description, processing and splitting of all datasets, see Appendix B.

Implementation detailsWe adapt the open-sourced ViT-G/14 from EVA-CLIP  and LLaMA2-Chat (7B)  as our visual backbone and LLM, respectively. During the training process, each task is assigned a sample rate that is calculated in proportion to the respective task's data volume. The visual backbone remains frozen with an input image resolution of 224*224 and the LLM is fine-tuned through LoRA  with the rank of 8. The compression rate \(\)=4 and the number of projection experts \(N\)=5. Uni-Med only requires one-stage training on a NVIDIA A800-SXM4-80GB GPU, with the first 10k iterations to warm-up and a total

    & Question & Visual & Report & Referring & Referring & Image \\  & Answering & Question & Generation & Expression & Expression & Generation & Classification \\ 
**Identifier** & [qz] & [vzq] & [caption] & [refer] & [identify] & [cls] \\   

Table 1: Text-level special task identifiers for different tasks.

of 100k iterations with a batch size of 4, which lasts roughly 10 hours. The peak learning rate is set to 1e-6 and it decays to 1e-7 following the cosine strategy. We use AdamW [Loshchilov and Hutter, 2017] optimizer with \(_{1}\)=0.9, \(_{2}\)=0.95 and weight decay of 0.05.

Evaluation metricsFor ablation studies, we report BLEU-1 for the task of VQA, REG, and RG, IoU for the task of REC, Accuracy for the task of CLS. In addition, we use \(=_{i=1}^{S}(M_{m,i}-M_{b,i})/M_{b,i} 100\%\) to evaluate the performance gains, where \(M_{m,i}\) and \(M_{b,i}\) are the metrics of our model and baseline model, \(S\) can be the number of datasets or tasks. For the overall comparison between models, we report more metrics such as F1, ROUGE, METEOR, RadGraph F1 and RadCliQ [Yu _et al._, 2023]. See details at Appendix D.1.

### Ablation study

#### 4.2.1 Ablation on module design

Connector designTaking the connector of a two-layer MLP as baseline setup, we first discuss the performance of different multi-task learning hypothesis. In Table 2 (a), connectors based on conflict-synergy coexist hypothesis (CMoE with sparse / soft router) show a more holistic improvement trend in multi-task learning compared to connectors based on the conflict hypothesis (CMoE with

    &  **Router** \\ **Strategy** \\  } &  **VQA** \\ BLEU-1 \\  } &  &  **REC** \\ IoU \\  } &  &  **REG** \\ BLEU-1 \\  } &  &  **BG** \\ BLEU-1 \\  } &  &  **CLS** \\  } &  &  **Class** \\  } &  &  **Total** \\ \(\) (\(\)) \\  } \\   \\   Limer \\ MLP \\  } & - & - & 77.90 / 56.27 & -1.44\% & 28.44 / 11.59 & -23.98 / 74.98 / 55.61 & -2.18 / 13.00 / 15.58 & -11.69\% & 72.47 / 69.39 & -5.44 / 8.9 \\  & - & - & 79.81 / 56.48 & 35.18 / 16.26 & 74.54 / 55.84 & 18.55 / 15.50 & 76.26 / 73.64 & - & - \\  & Constant & - & 82.74 / 53.23 & 2.66 / 3.94 / 15.49 & -15.49 / 73.58 / 85.61 & -4.65 / 23.16 / 15.88 & 75.91 / 76.50 & 1.78 / 2.76 \\  & Hard & 81.85 / **55.09** & 3.60 / 10.19 / 23.17 & 70.90 / 58.04 & -2.99 / 15.79 / 15.94 & **3.15** / **15.88** / 16.5 & **0.05** \\  & Sparse & Token & 80.68 / 55.02 & 1.09 / 37.01 / 18.41 & 9.36 / 76.86 / 60.08 & 3.09 / 24.02 / 15.73 & 15.56 / 3.74 / 74.93 & -1.09 / 5.66 \\  & & 81.79 / 57.29 & 3.55 / 15.79 & 7.24 / 74.43 / 61.82 & **2.46** / 15.61 & **2.12** / 76.56 / 27.11 & 2.69 / 6.78 \\  & Soft & Task & **82.51** / 57.43 & 2.35 / **33.38** / 16.98 / 15.08 & **77.48** / 60.67 & 45.4 / 32.53 / 15.89 & 14.27hard router) and synergy hypothesis (linear, MLP, CMoE with constant router). Though the hard router has a obvious lead on the CLS task, implying that the CLS task is better suited to a separate connector to avoid conflicts with other tasks. The soft router achieves the best multi-task performance, indicating that it not only alleviates conflicts between tasks, but also promotes collaboration between tasks. We then discuss three types of router strategy. The strategy of combining token-level with task-level information is superior to using each information separately, indicating the effectiveness for considering the tug-of-war problem from both token and task level.

Resampler designWe explore whether aggregating visual features through resampler has unfavorable effects in Table 2 (b). Despite an increase in compression rate \(\) from 1 to 4, the performance of models utilizing projection aggregation is improved. While the performance of average pooling and max pooling approaches is not satisfactory, especially the latter has severe performance degradation, which may be attributed to the excessive loss of feature information. This phenomenon shows that appropriate visual feature compression can bring efficiency to the training process without losing or even improving performance.

Number of projection expertsThe number of projection experts \(N\) is one of the most significant hyperparameters, which is closely related to the number of tasks and modalities that the CMoE module can accommodate. It is a challenging study as the complexity of the scenario can end up overfitting to simpler tasks and modalities or underfitting complex ones. As shown in Table 2 (c), increasing the number of experts \(N\), namely an augmentation in parameters, still brings performance gains on some datasets, but the average gain tends to stabilize across all tasks and datasets. Therefore, CMoE with 5 projection experts is sufficient to handle the tug-of-war problem in the existing medical multi-task learning scenarios and training configuration. A higher value of \(N\) does not bring the desired further improvement in total \(\).

#### 4.2.2 Ablation on module generalization

We demonstrate the generalization capability of the CMoE module in any configuration, especially when the key hyperparameters and strategies for LLM fine-tuning change. We first focus on the rank of LoRA, which directly determines the LLM capacity, i.e., trainable parameters. Our observations in Table 2 (d) reveal that CMoE with soft router can steadily improve multi-task performance when LoRA rank increases from 4 to 64. In Table 2 (e), we introduce MoE to LoRA, namely LoRA-MoE, which is considered a favorable parameter-efficient tuning solution for multi-task applications (Liu _et al._, 2023b; Chen _et al._, 2024). See details of LoRA-MoE at Appendix A.2. We find that separate LoRA-MoE results in significant performance improvement in 3 tasks while degradation in 2 tasks, indicating that it does not achieve the efficient solution to the tug-of-war problem. After combining CMoE with soft router, we achieve a balance of consistent performance gains, further demonstrating the necessity and effectiveness of mitigating the tug-of-war problem at the connector level in MLLMs.

### Interpretation

We conduct interpretation analysis of the tug-of-war problem based on methods mentioned in Section 3.1.1. Specifically, we focus on the changes in the connector using CMoE compared to MLP and show how the tug-of-war problem is optimized: (1) From the perspective of gradient optimization, we use maximum normalization to make the tug-of-war indexes comparable under different architectures. CMoE results in a more consistent tug-of-war indexes, i.e. higher mean and smaller standard deviation, among different tasks or datasets, implying each individual gets a more balanced optimization, as shown in Figure 4 (a). (2) From the perspective of parameter statistics, we discrete the statistics scores into ten intervals and count the ratio of all parameters at connector by interval. CMoE results in an increase in the proportion of high-value intervals in Figure 4 (b). We show the routing weights of projection experts after the warm-up stage and the final model in Figure 4 (c). CMoE adaptively learns different patterns of routing weights for different tasks.

To better reflect the coexistence of conflict and synergy among tasks, as well as the critical role played by the connector, we visualize the distribution of visual features before and after passing through the connector using the t-SNE method (Van der Maaten and Hinton, 2008). From the perspective of multi-task learning, we randomly select 200 samples from each task. It can be observed that CMoE promotes the optimization of the tug of war problem when aligning the visual space with the textual space of the LLM in Figure 5. Specifically, visual features of the same task are more 

[MISSING_PAGE_EMPTY:9]

### Overall comparison

To demonstrate the model capabilities of Uni-Med on multi-task learning, four open source and state-of-the-art medical MLLMs including Med-Flamingo , RadFM , LLAVA-Med , and XrayGPT  are used for performance comparison in Table 3. Any method of fine-tuning will inevitably lead to changes in the initial capability of the model. Therefore, we use readily available model checkpoints for testing, following the prompt template requirements of different models. Under this comparison strategy, if the training datasets of a model and Uni-Med intersect and strictly follow the official partition, it is fair and comparable to Uni-Med on these datasets. Specifically, LLAVA-MED provides dataset-specific fine-tuning checkpoints on Slake-VQA and Path-VQA separately. XrayGPT focuses on the task of report generation and utilizes MIMIC-CXR as training dataset. RadFM provides a model checkpoint for joint fine-tuning on Slake-VQA, MIMIC-CXR and MPx-Single. However, we do not list performance of RadFM on MPx-Single as we have identified the issue of data leakage, see Appendix D.2.

The results in Table 3 show that our Uni-Med achieves leading and competitive evaluation metrics across all tasks, which has the following prominent advantages: (1) Uni-Med is able to handle a greater variety of medical tasks, which is attributed to multi-task learning during training process. Due to the fact that the above MLLMs do not support input and output in coordinate form, we report the performance of Uni-Med on REC and REG tasks at Appendix D.5. Based on the different input and output forms supported by each model, we have also listed the zero-shot results in Table 3 for reference only. (2) Uni-Med achieves better results through joint training fine-tuning rather than dataset-specific fine-tuning like LLAVA-Med, which benefits from efficient optimization of the tug-of-war problem. In addition to directly compare the capability of existing models, we take LLAVA-Med as an example to compare the capability of model architectures in Appendix D.6.

## 5 Conclusion

In this paper, we present a novel open-source medical generalist foundation model Uni-Med, which can handle six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. Benefiting from the proposed CMoE, which combines MoE with the connector, Uni-Med achieves efficient solution to the tug-of-war problem in multi-task learning. Uni-Med not only achieves competitive or superior performance compared to the open-source state-of-the-art medical MLLMs, but also provides interpretability analysis from the perspective of gradient optimization and parameter statistics on how the tug-of-war problem is optimized. We hope Uni-Med can greatly promote the development of medical generalist foundation models and inspire more research toward generalist medical artificial intelligence.

## 6 Limitations

While Uni-Med has demonstrated strong potential as a unified and generalist medical foundation model, it still exhibits several limitations: (1) Limitations in handling genuine 3D medical image inputs. Most commonly used medical image are in 3D. Same as most medical MLLMs, we process 3D images into 2D slices as input, resulting in significant information loss. (2) The potential of performance gains in more complex multi-modal and multi-task learning scenarios has not yet been explored. Uni-Med use 12 datasets of 6 medical tasks, with a total data volume of 140k. (3) The potential of performance gains in different LLM backbones has not yet been explored. Uni-Med utilizes LLAMA2-7B. (4) Deeper theoretical analysis of tug of war problem remains to be explored. We attempt to combine the existing methods to analyze it from the perspective gradient optimization and parameter statistics. (5) Potential negative societal impacts. We cannot prevent potential malicious or unintended uses, such as generating fake profiles or wrong medical diagnoses, and provide necessary safeguards.