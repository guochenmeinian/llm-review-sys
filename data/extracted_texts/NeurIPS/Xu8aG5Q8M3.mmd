# LayoutGPT: Compositional Visual Planning and Generation with Large Language Models

Weixi Feng\({}^{1}\)1 Wanrong Zhu\({}^{1}\)1 Tsu-jui Fu\({}^{1}\) Varun Jampani\({}^{2}\) Arjun Akula\({}^{2}\)

**Xuehai He\({}^{3}\) Sugato Basu\({}^{2}\) Xin Eric Wang\({}^{3}\) William Yang Wang\({}^{1}\)**

\({}^{1}\)University of California, Santa Barbara

\({}^{2}\)Google

\({}^{3}\)University of California, Santa Cruz

https://github.com/weixi-feng/LayoutGPT

###### Abstract

Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly, LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis, demonstrating its effectiveness and potential in multiple visual domains.

Figure 1: Generated layouts from LayoutGPT in 2D images and 3D indoor scenes. LayoutGPT can serve as a visual planner to reflect challenging numerical and spatial concepts in visual spaces.

Introduction

Can Large Language Models (LLMs) comprehend visual concepts and generate plausible arrangments in visual spaces? Recently, LLMs have shown significant advancement in various reasoning skills [50; 49] that remain challenging to visual generative models. For instance, text-to-image generation (T2D) models suffer from generating objects with specified counts, positions, and attributes [10; 24]. 3D scene synthesis models face challenges in preserving furniture within pre-defined room sizes . Addressing these issues necessitates the development of compositional skills that effectively arrange components in a coherent manner, accurately reflecting object specifications and interactions.

Visual layout is an essential symbolic representation that has been widely studied as it reflects the compositions of a visual space [33; 53; 45; 34]. For instance, layout generation models [21; 25; 17; 53; 23] can be combined with region-controlled image generation methods [56; 27] to improve image compositionality . But unlike LLMs, these models are restricted to discrete categories or have limited reasoning skills for complicated text conditions. Recently, LLMs like ChatGPT , are adopted as a centralized module of frameworks or systems where multiple foundational computer vision models are integrated. Through defined action items or API calls, LLMs can interact with visual generative models to extend the systems' capability into image generation tasks. .

Despite the advancement, existing approaches that involve the collaboration between LLMs and image generation models are either limited to executing the latter through program generation or using LLMs for language data augmentation for image editing . Current LLM-based systems fail to improve the compositional faithfulness of a generated image by simply using T2I models through API calls. While one could additionally integrate models that synthesize images with the guidance of layouts [56; 27], keypoints , or sketches [20; 57], users still have to create fine-grained inputs on their own, leading to extra efforts and degraded efficiency compared to pure language instructions.

To address these challenges, we introduce **LayoutGPT**, a training-free approach that injects visual commonsense into LLMs and enables them to generate desirable layouts based on text conditions. Despite being trained without any image data, LLMs can learn visual commonsense through in-context demonstrations and then apply the knowledge to infer visual planning for novel samples. Specifically, we observe that representing image layouts is highly compatible with how style sheet language formats images on a webpage. Therefore, as LLMs are trained with program data, constructing layouts as structured programs may enhance LLMs' ability to "imagine" object locations from merely language tokens. Our programs not only enable stable and consistent output structures but also strengthen LLMs' understanding of the visual concepts behind each individual attribute value. When combined with a region-controlled image generation model , LayoutGPT outperforms existing methods by 20-40% and achieves comparable performance as human users in generating plausible image layouts and obtaining images with the correct object counts or spatial relations.

In addition, we extend LayoutGPT from 2D layout planning to 3D indoor scene synthesis. With a slight expansion of the style attributes, LayoutGPT can understand challenging 3D concepts such as depth, furniture sizes, and practical and coherent furniture arrangements for different types of rooms. We show that LayoutGPT performs comparably to a state-of-the-art (SOTA) supervised method. Our experimental results suggest that LLMs have the potential to handle more complicated visual inputs. Our contribution can be summarized as the following points:

* We propose LayoutGPT, a program-guided method to adopt LLMs for layout-based visual planning in multiple domains. LayoutGPT addresses the _inherent_ multimodal reasoning skills of LLMs and can improve end-user efficiency.
* We propose **N**umerical and **S**patial **R**easoning (NSR-1K) benchmark that includes prompts characterizing counting and positional relations for text-to-image generation.
* Experimental results show that LayoutGPT effectively improves counting and spatial relations faithfulness in 2D image generation and achieves strong performance in 3D indoor scene synthesis. Our experiments suggest that the reasoning power of LLMs can be leveraged for visual generation and handling more complicated visual representations.

Related Work

**Image Layout Generation** Layout generation has been an important task for automatic graphical design for various scenarios, including indoor scenes [40; 46], document layouts [59; 60; 15], and graphical user interface . Previous work has proposed various types of models that need to be trained from scratch before generating layouts. LayoutGAN  is a GAN-based framework to generate both class and geometric labels of wireframe boxes for a fixed number of scene elements. LayoutVAE  generates image layouts conditioned on an input object label set. Transformer-based methods are proposed to enhance flexibility in the layout generation process. For instance, LayoutTransformer  adopts self-attention to learn contextual relations between elements and achieve layout completion based on a partial layout input. BLT  proposes a hierarchical sampling policy so that any coordinate values can be modified at the sampling stage to enable flexible and controlled generation. However, existing methods are restricted to class labels and fail to reason over numerical and spatial concepts in text conditions. In contrast, LayoutGPT can convert challenging textual concepts to 2D layouts and generate free-form, detailed descriptions for each region.

**Compositional Image Generation** Recent studies have shown that text-to-image generation (T2I) models suffer from compositional issues such as missing objects, incorrect spatial relations, and incorrect attributes [24; 2]. StructureDiffusion  proposes to adjust text embeddings by utilizing prior knowledge from linguistic structures. Attend-and-Excite  optimizes attention regions so that objects attend on separate regions. Another line of work strives to introduce extra conditions as inputs. For example, ReCo , GLIGEN , and Layout-Guidance  can generate images based on bounding box inputs and regional captions.  combines a layout generator and a region-controlled method to achieve accurate generation results. While we focus on layout generation, we also employ layout-to-image models to generate final images and show the effectiveness of LayoutGPT.

**Indoor Scene Synthesis** Indoor scene synthesis aims at generating reasonable furniture layouts in a 3D space that satisfies room functionality. Early work adopting autoregressive models requires supervision of 2D bounding boxes and other visual maps . Later, SceneFormer  proposes to apply a set of transformers to add furniture to scenes. While previous work adopts separate models to predict different object attributes, ATISS  demonstrates that a single transformer model can generate more realistic arrangments while being more efficient. In this work, we investigate leveraging LLMs to achieve scene synthesis without any fine-tuning.

**LLMs for Vision** Language inputs have been an essential part of many vision language tasks [43; 11; 28; 14]. With the strong generalization ability of contemporary LLMs, recent work attempts to adapt the power of LLMs on multimodal tasks [31; 55]. For instance, multimodal chain-of-thought  trained a model to incorporate visual inputs as rationales for question answering.  proposes to learn translation parameters to map embeddings between visual and language domains such that an LLM can ground on both modalities. VisProg  and ViperGPT  use LLMs to design modular pseudocode instructions or executable Python programs to achieve visual reasoning. LLMScore  leverages LLMs to evaluate text-to-image models. Visual ChatGPT  proposes a prompt manager that supports the execution of various image generation models. In this work, we directly involve LLMs in the generation process by leveraging LLMs to design visual layouts through in-context learning and structured representations.

## 3 Method

### Overview

Given a condition \(\), the goal of layout generation is to predict a set of tuples \(=\{_{j}|j=1,2,,n\}\) where each tuple \(_{j}\) denotes the layout information of a 2D or 3D bounding box of object \(j\). In image planning, \(\) is the input text prompt, \(_{j}\) consists of a category \(c_{j}\), bounding box location \(_{j}=(x_{j},y_{j})^{2}\) and bounding box size \(_{j}=(w_{j},h_{j})^{2}\), i.e. \(_{j}=(c_{j},_{j},_{j})\). Similarly, in 3D scene synthesis, \(\) specifies the room type and room size, \(_{j}\) consists of category \(c_{j}\), location \(_{j}^{3}\), size \(_{j}^{3}\), and orientation \(_{j}\), i.e. \(_{j}=(c_{j},_{j},_{j},_{j})\). While \(c_{j}\) can be modeled as a discrete value, our method directly predicts the category text.

### LayoutGPT Prompt Construction

As is shown in Fig. 2, LayoutGPT prompts consist of three main components: **task instructions**, and in-context exemplars in **CSS structures** with **normalization**.

**CSS Structures** In autoregressive layout generation, \(_{j}\) is usually modeled as a plain sequence of values, i.e. \((c_{1},x_{1},y_{1},w_{1},h_{1},c_{2},x_{2},)\)[17; 23]. However, such a sequence can be challenging for LLMs to understand due to underspecified meaning of each value. Therefore, we seek a structured format that specifies the physical meaning of each value for LLMs to interpret spatial knowledge. We realize that image layouts are highly similar to how CSS (short for Cascading Style Sheets) formats the layout of a webpage and defines various properties of the img tag in HTML. For instance, \(x_{j},y_{j}\) corresponds to the standard properties left and top, while \(w_{j},h_{j}\) corresponds to width and height in CSS. As LLMs like GPT-3.5/4 are trained with code snippets, formatting image/scene layouts in CSS structures potentially enhances the LLMs' interpretation of the spatial meaning behind each value. Therefore, as is shown in Fig. 2, we place category name \(c_{j}\) as the selector and map other attribute values into the declaration section following standard CSS styles.

**Task Instructions & Normalization** Similar to previous work in improving the prompting ability of LLMs [48; 42; 37], we prepend task instructions to the prompt to specify the task goal, define the standard format, unit for values, etc. Besides, as the common length unit of CSS is pixels (px), we normalize each property value based on a fixed scalar and rescale the value to a maximum of 256px. As will be shown in later sections (Sec. 4.4 & 5.4), all three components play important roles in injecting visual commonsense into LLMs and improving generation accuracy.

### In-Context Exemplars Selection

Following previous work [1; 54], we select supporting demonstration exemplars for in-context learning based on retrieval results. Given a test condition \(_{j}\) and a support set of demonstrations \(=\{(_{k},_{k})|k=1,2,\}\), we define a function \(f(_{k},_{j})\) that measures the distances between two conditions. For 2D text-conditioned image layout generation, we adopt the CLIP  model to extract text features of \(_{j}\) (usually a caption) and the image feature of \(_{k}\) and measure the cosine similarity between them. For the 3D scene synthesis task where each room has length \(rl\) and width \(rw\), we measure distance with \(f(_{k},_{j})=|rl_{k}-rl_{j}\|^{2}+|rw_{k}-rw_{j}\|^{2}\). We select supporting demonstrations with the top-\(k\) least distance measures and construct them as exemplars following the CSS structure in Fig. 2. These supporting examples are provided to GPT-3.5/4 in reverse order, with the most similar example presented last.

Figure 2: The overview process of our LayoutGPT framework performing 2D layout planning for text-conditioned image generation or 3D layout planning for scene synthesis.

### Image and Scene Generation

For text-conditioned image synthesis, we utilize a layout-to-image generation model to generate images based on the generated layouts. As for each object layout in 3D scene synthesis, we retrieve a 3D object based on the predicted category, location, orientation, and size following . We directly render the scene with the retrieved 3D objects. See Sec. 4 & Sec. 5 for more details.

## 4 LayoutGPT for Text-Conditioned Image Synthesis

In this section, we provide an extensive evaluation of LayoutGPT for 2D text-to-image (T2I) synthesis and compare it with SOTA T2I models/systems. An ablation study is conducted to demonstrate the effect of individual components from LayoutGPT. We also showcase qualitative results and application scenarios of our method.

### Experiment Setup

**Datasets & Benchmarks** To evaluate the generations in terms of specified counts and spatial locations, we propose NSR-1K, a benchmark that includes template-based and human-written (natural) prompts from MSCOCO . Table 1 summarizes our dataset statistics with examples. For template-based prompts, we apply a set of filters to obtain images with only 1-2 types of object and then create prompts based on object categories and bounding box information. As for natural prompts, we extract COCO captions with keywords to suit the task of numerical reasoning (e.g. "four") or spatial reasoning (e.g. "on top of") and ensure that all objects from the bounding box annotations are mentioned in the caption to avoid hallucination. Each prompt from NSR-1K is guaranteed to have a corresponding ground truth image and layout annotations. Detailed benchmark construction processes are described in Appendix B.1.

**Evaluation Metrics** To evaluate generated layouts, we report precision, recall, and accuracy based on generated bounding box counts and spatial positions [9; 16]. For spatial reasoning, each prompt falls into one of the four types of relations ({_left, right, top, below_}) and we use the bounding box center for evaluation following PaintSkills . To evaluate generated images, we first obtain bounding boxes from GLIP  detection results and then compute average accuracy based on the bounding box counts or spatial relations. We also report CLIP cosine similarity between text prompts and generated images for reference. Detailed metric descriptions are listed in Appendix B.2.

**Baselines** As we consider both layout evaluation and image evaluation, we compare LayoutGPT with **end-to-end T2I models** (Stable Diffusion , Attend-and-Excite )2 and **two-stage systems** that generate layouts first and then apply GLIGEN  as the layout-to-image model. We also evaluate ground truth layouts and human-drawn layouts as the theoretical upper bounds. The human-drawn layouts are collected through crowdsourcing, in which we specifically ask human annotators to draw layouts given text prompts. We slightly modify LayoutTransformer  as a baseline for supervised conditional layout generation. Detailed descriptions of baseline setups and human annotating are discussed in the Appendix A and E.

  
**Task** & **Type** & **Example Prompt** & **\# Train** & **\# Val** & **\# Test** \\   & Single Category & “_There are two graffies in the photo._” & 14890 & - & 114 \\   & Two Categories & “_Three potted plants with one wine in the picture._” & 7402 & - & 197 \\   & Comparison & “_A picture of three cars with a few fire hydrants, the number of cars is more than that of free hydrants._” & 7402 & - & 100 \\   & Natural & “_A fenced in pasture with four horses standing around eating grass._” & 9004 & - & 351 \\  T2I Spatial Reasoning & Two Categories & “_A dog to the right of a bench._” & 360 & - & 199 \\   & Natural & “_A black cat laying on top of a bed next to pillows._” & 378 & & 84 \\   

Table 1: Dataset statistics and examples of the NSR-1K benchmark for image layout planning and text-to-image (T2I) generation with an emphasis on numerical and spatial reasoning.

### Evaluation Results

**Quantitative Results** As shown in Table 2, among the variants of LayoutGPT (# 6 - # 9), GPT-3.5 achieves the best performance in numerical reasoning while GPT-4 performs the best in generating correct spatial positions. LayoutGPT outperforms LayoutTransformer (#5) by large margins, proving the strong cross-modal reasoning skills of LLMs. As for image-level evaluation, LayoutGPT surpasses end-to-end T2I models (#1-#3) by 20-40% in GLIP-based accuracy and relatively 1-6% in CLIP similarity. Therefore, using layouts as an intermediate representation indeed leads to more reliable and faithful generation outcomes. In addition, LayoutGPT achieves similar layout accuracy as human users (numerical #6 vs. #11 (86.33% v.s. 92.56%); spatial #9 vs. #11 (91.73% v.s. 91.17%)), which implies its potential to spare users from drawing layouts manually. The discrepancy between layout accuracy and GLIP-based accuracy suggests that the bottleneck mainly stems from layout-guided image generation and GLIP grounding results.

In addition, LayoutGPT binds attributes to each object's bounding box with 100% accuracy on HRS  color prompts. We further evaluate the attribute correctness rate (accuracy) on the final generated images when combining LayoutGPT with GLIGEN/ReCo. As shown in Table 3, our system largely improves the color correctness over Stable Diffusion with multiple objects.

**Qualitative results** We show the qualitative results of LayoutGPT and baselines in Fig. 3. LayoutGPT can understand visual commonsense such as the clock sizes at a train station (top left) or complex spatial relations between multiple objects (bottom right), while SD fails to generate correct numbers or positions. Besides, LayoutGPT demonstrates a similar layout design to human users (bottom left). Fig. 11 in the Appendix visualizes the results of attribute binding using LayoutGPT and ReCo .

    &  &  \\   & &  &  &  &  \\   & **Methods** & Precision & Recall & Accuracy & Acc. (GLIP) & CLIP Sim. & Accuracy & Acc. (GLIP) & CLIP Sim. \\    & _Text \(\) Image_ & & & & & & & \\  & Stable Diffusion (v1.4)  & - & - & - & 32.22 & 0.256 & - & 16.89 & 0.252 \\  & Stable Diffusion (v2.1) & - & - & - & 42.44 & 0.256 & - & 17.81 & 0.256 \\  & Attend-and-Excite (SD v1.4)  & - & - & 38.96 & 0.258 & - & 24.38 & 0.263 \\  & Attend-and-Excite (SD v2.1) & - & - & 45.74 & 0.254 & - & 26.86 & 0.264 \\    & _Text \(\) Layout = Image_ & & & & & & \\  & LayoutGPT Transformer  & 75.70 & 61.69 & 22.26 & 40.55 & 0.247 & 6.36 & 28.13 & 0.241 \\  & LayoutGPT (GPT-3.5) & **94.81** & **96.49** & **86.33** & 51.20 & 0.258 & 82.54 & 52.86 & 0.264 \\  & LayoutGPT (GPT-3.5) & 90.19 & 88.29 & 72.02 & 46.64 & 0.254 & 74.63 & 45.58 & 0.262 \\  & LayoutGPT (GPT-3.5, chat) & 81.84 & 85.47 & 75.51 & 54.40 & **0.261** & 85.87 & 56.75 & **0.268** \\  & LayoutGPT (GPT-4) & 78.36 & 86.29 & 78.43 & **55.64** & **0.261** & **91.73** & **60.64** & **0.268** \\   & GT layouts & 100.00 & 100.00 & 100.00 & 53.23 & 0.256 & 100.00 & 62.54 & 0.261 \\  & Human & 99.26 & 96.52 & 92.56 & 56.07 & 0.258 & 91.17 & 51.94 & 0.258 \\   

Table 2: Comparison of our LayoutGPT with baseline methods in terms of counting and spatial correctness. Line 5-11 generates layout and adopts GLIGEN  for layout-guided image generation. “Human” (line 11) denotes layouts collected from human users given text prompts. Text in bold shows the best results of LayoutGPT.

Figure 3: Qualitative comparison between Stable Diffusion, LayoutGPT, and human annotations regarding numerical (top row) and spatial reasoning (bottom row) skills.

### Application Scenarios

By utilizing LLMs as layout generators, LayoutGPT can be applied to a diverse set of scenarios for accurate and creative image generation.

**Dense Layout Planning**: In Fig. 4 (top), we apply random in-context examples from COCO17 panoptic annotations with 6\(\)15 bounding boxes per image. LayoutGPT can be applied to scenarios that imply numerous objects (e.g. different kinds of donuts) or various categories (e.g. bathroom or street view). Though only a few objects are mentioned in the prompts, LayoutGPT predicts layouts for the whole scene and imagines common objects that are usually visible in each scene.

**Text-based Inpainting**: In addition, the inherent language generation ability of LLMs enables our method to generate fine-grained regional descriptions from coarse global prompts (Fig. 4 bottom). LayoutGPT can enrich the description of each object with details that are not mentioned in the prompt, producing suitable outputs for models like ReCo .

**Counterfactual Scenarios**: We test LayoutGPT on counterfactual prompts provided by GPT-4 . The in-context examples are randomly drawn from MSCOCO 2017, which greatly differs from the counterfactual prompts. As shown in Fig. 5, LayoutGPT manages to generate reasonable layouts on these challenging prompts and handles the relationship between objects well.

### Ablation Study

**Component Analysis** Table 4 presents the component analysis of our CSS-style prompt on spatial reasoning prompts. Comparisons between line 1-3 entails that the task instructions (#2) and CSS format (#3) effectively improve layout accuracy. Format in-context exemplars in CSS structures

Figure 4: **Dense layout planning: LayoutGPT can generate rich objects or categories in complex scenes for MSCOCO 2017 Panoptic prompts . Text-based inpainting: LayoutGPT can generate free-form regional descriptions that are not mentioned in the global prompt.**

    &  \\   & Prompts w/ 2 objects & Prompts w/ 3 objects & Prompts w/ 4 objects & Overall \\  SD1.4 & 18.57 & 10.10 & 11.36 & 12.84 \\ Attend-and-Excite & 31.43 & 19.19 & 20.45 & 22.96 \\ LayoutGPT + GLIGEN & 22.86 & 19.19 & 14.77 & 18.68 \\ LayoutGPT + ReCo  & **40.00** & **37.37** & **34.09** & **36.96** \\   

Table 3: Color binding accuracy evaluated on prompts from HRS-Bench . We follow the benchmark and use a hue-based classifier to identify the color of generated objects.

show a more significant effect on accuracy. Pairwise comparisons of line \(5\)-\(7\) support the argument that the CSS style is the most essential component. While solely applying normalization degrades accuracy in line 4, line 5&8 shows that it slightly improves the performance when combined with other components.

**Model-Agnostic Property** We show that LayoutGPT is agnostic to layout-guided image generation models in line 9-10 in Table 4. We feed the same generated layouts from LayoutGPT to LayoutGudance  and compute image-level metrics. Compared to using ground truth layouts (#10), LayoutGPT (#9) shows a minor gap in GLIP-based accuracy and a comparable CLIP similarity score. The discrepancy in GLIP-based accuracy is similar to that in Table 2, implying that the layouts generated by our method are agnostic to the downstream model.

## 5 LayoutGPT for Indoor Scene Synthesis

### Task Setup

**Datasets & Benchmarks** For indoor scene synthesis, we use an updated version of the 3D-FRONT dataset [12; 13] following ATISS . After applying the same pre-processing operations, we end up with 4273 bedroom scenes and 841 scenes for the living room. We only use rectangular floor plans of the test set for evaluation since LayoutGPT is not compatible with irregular ones. Hence, we end up with 3397/453/423 for train/val/test split of bedroom scenes and 690/98/53 for train/val/test split of living room scenes.

**Evaluation Metrics** We follow prior work  to report KL divergence between the furniture category distributions of predicted and ground truth scenes. We also render scene images from four camera angles for each scene and report FID scores . In addition, we report out-of-bound rates, i.e. the percentage of scenes with furniture exceeding the floor plan boundary.

### Evaluation Results

**Quantitative Results** The evaluation results are recorded in Table 5. We provide a random baseline for comparison denoted as "Random Scenes", in which the scene is randomly sampled from the in-context exemplars for each inference run.3

    & w/ & w/ & **w/** & **Layout-to-Image** & **Layout Eval** &  \\  & **Instr.** & **CSS** & **Norm.** & **Model** & Acc. & Acc. (GLIP) & CLIP Sim \\ 
1 & & & & & 55.12 & 34.35 & 0.259 \\
2 & ✓ & & & & 78.23 & 47.92 & 0.263 \\
3 & & ✓ & & & 80.82 & 51.38 & 0.264 \\
4 & & & ✓ & & 44.10 & 26.43 & 0.257 \\
5 & ✓ & ✓ & & & GLIGEN  & 81.84 & 52.08 & 0.264 \\
6 & ✓ & & ✓ & & 73.36 & 44.88 & 0.262 \\
7 & & ✓ & ✓ & & 76.61 & 47.56 & 0.263 \\
8 & ✓ & ✓ & ✓ & & **82.54** & **52.86** & **0.264** \\ 
9 & ✓ & ✓ & ✓ & & 82.54 & 31.02 & 0.258 \\
10 & & GT layouts & & Layout-Guidance  & 100.00 & 33.92 & 0.257 \\   

Table 4: Ablation study of LayoutGPT (GPT-3.5) on spatial reasoning prompts. “w/ Instr.”: with prepended task instructions. “w/ CSS”: format in-context demonstrations in CSS style. “w/ Norm.”: normalizing attribute values to integers by a fixed size.

Figure 5: Qualitative examples of LayoutGPT’s performance on counterfactual prompts.

For both bedrooms and living rooms planning, LayoutGPT attains lower out-of-bound rates than ATISS (bedrooms: 43.26% vs. 49.88%; living rooms: 64.16% vs. 83.02%), which verifies LayoutGPT's spatial reasoning ability in 3D environments. In addition, LayoutGPT has lower FID compared to ATISS (bedrooms: 28.37 vs. 30.02; living rooms: 76.34 vs. 85.40), which indicates that the planned scene has higher quality. Noted here that the living room split contains much more objects on average (11 for living rooms vs. 5 in bedrooms) and is a low-resource split with only 690 training scenes. Therefore, while living rooms are challenging for both methods, LayoutGPT shows more significant improvement over ATISS as supervised methods tend to overfit in early epochs.

Meanwhile, ATISS performs better in terms of KL divergence, which means that the overall furniture distribution predicted by ATISS is closer to the test split. We observe that LayoutGPT tends to avoid furnitures that are extremely rarely seen in each scene (e.g. coffee tables for bedrooms) as these objects appear less frequently in the in-context demonstrations. The limited in-context demonstration size also restricts LayoutGPT to have a universal observation of the furniture distributions.

**Qualitative Results** As shown in Fig. 6, LayoutGPT manages to understand common 3D concepts, such as "the pendant lamp should be suspended from the ceiling" and "nightstands should be placed by the headboard of the bed" (bottom row). When given a floor plan size for both living and dining rooms, LayoutGPT can also generate complicated 3D planning with dining tables and chairs on one side, and a sofa, a coffee table, and a TV stand on the other side (bottom right).

### Application Scenarios

**Text-guided Synthesis**: LayoutGPT can follow text captions to arrange furniture in the scene (see Fig. 7). When the captions enumerate a complete list of furniture, LayoutGPT strictly follows the captions to generate the furniture and achieve a KL Div. value close to zero.

**Partial Scene Completion**: Thanks to the autoregressive decoding mechanism, LayoutGPT can complete a scene with partial arrangments such that the additional furniture remains coherent with the existing ones. Through in-context demonstrations, LayoutGPT learns critical (visual) commonsense such as visual symmetric (e.g. nightstands in Fig. 8 (a)), positional relations (e.g. stool at the end of the bed in Fig. 8 (b)), and room functions (e.g. desks and chairs in the dining area in Fig. 8 (d)).

    &  &  \\   & Out of bounds (\(\)) & KL Div. (\(\)) & FID (\(\)) & Out of bounds (\(\)) & KL Div. (\(\)) & FID (\(\)) \\  Random Scenes & 11.16 & 0.0142 & 23.76 & 9.43 & 0.1239 & 79.61 \\  ATISS* & 49.88 & **0.0113** & 30.02 & 83.02 & **0.1054** & 85.40 \\ LayoutGPT (GPT-3.5) & **43.26** & 0.0995 & **28.37** & 73.58 & 0.1405 & **76.34** \\ LayoutGPT (GPT-3.5, chat) & 57.21 & 0.0846 & 29.66 & 81.13 & 0.2077 & 89.40 \\ LayoutGPT (GPT-4) & 51.06 & 0.1417 & 29.88 & **64.15** & 0.1613 & 78.60 \\   

Table 5: Comparison of LayoutGPT with ATISS on indoor scene synthesis. “Random Scenes” means randomly sampling one training scene from the in-context demonstrations for each inference room sample. (* denotes results reproduced by us)

Figure 6: Visualization of LayoutGPT across different types of rooms with different floor plan sizes.

### Ablation Study

Similar to Sec. 4.4, we study the effect of task instructions, CSS structure, and normalization on indoor scene synthesis (see Table 6). In contrast to our conclusion for 2D planning in Sec. 4.4, comparisons between line 1\(-\)4 show that normalization (#4) is the most critical component for suppressing the out-of-bound rate while the CSS structure is also effective. We observe that LLMs occasionally copy attribute values directly from in-context exemplars even though the room sizes are different. Therefore, normalizing all exemplars to the same scale can reduce the out-of-bound rate. CSS style facilitates LLMs to understand the physical meaning behind each attribute value and hence leads to almost the best result when combined with normalization (#7).

## 6 Conclusion

In this work, we address a new direction of generative model collaborations. Specifically, we are interested in how Large Language Models (LLMs) can collaborate with visual generative models. To this end, we propose LayoutGPT, an approach that turns an LLM into a visual planner through in-context learning and CSS style prompts. LayoutGPT can generate plausible visual arrangements in both image space and 3D indoor scenes. LayoutGPT can effectively improve image compositions by generating accurate layouts and achieves comparable performance in indoor scene synthesis compared to supervised methods. Besides, LayoutGPT can improve user efficiency in image generation and serve as an essential part of a unified system for all types of multimodal tasks.

    & **w/ Instr.** & **w/ CSS** & **w/ Norm.** & **Out of Bound \(\)** & **KL Div. \(\)** & **FID \(\)** \\ 
1 & & & & 55.32 & 0.1070 & 56.83 \\
2 & ✓ & & & 54.85 & 0.1153 & 58.85 \\
3 & & ✓ & & 51.77 & 0.0776 & 55.62 \\
4 & & & ✓ & 46.57 & 0.1276 & 58.24 \\
5 & ✓ & ✓ & & 51.30 & **0.0741** & 57.64 \\
6 & ✓ & & ✓ & 46.81 & 0.0913 & 58.61 \\
7 & & ✓ & ✓ & 43.74 & 0.0848 & 57.70 \\
8 & ✓ & ✓ & ✓ & **43.26** & 0.0995 & **56.66** \\   

Table 6: Ablation studies on LayoutGPT on the bedroom split for 3D indoor scene synthesis.

Figure 8: LayoutGPT can successfully complete a partial scene for different rooms. We provide three starting objects for bedrooms and seven objects for living rooms.

Figure 7: Generation of 3D scenes based on text captions that enumerate the furniture.