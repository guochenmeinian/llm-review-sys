# Distributionally Robust Linear Quadratic Control

Bahar Taskesen

EPFL

bahar.taskesen@epfl.ch &Dan A. Iancu

Stanford University

daniancu@stanford.edu &Cagli Kocyigit

University of Luxembourg

cagli.kocyigit@uni.lu &Daniel Kuhn

EPFL

daniel.kuhn@epfl.ch

###### Abstract

Linear-Quadratic-Gaussian (LQG) control is a fundamental control paradigm that has been studied and applied in various fields such as engineering, computer science, economics, and neuroscience. It involves controlling a system with linear dynamics and imperfect observations, subject to additive noise, with the goal of minimizing a quadratic cost function depending on the state and control variables. In this work, we consider a generalization of the discrete-time, finite-horizon LQG problem, where the noise distributions are unknown and belong to Wasserstein ambiguity sets centered at nominal (Gaussian) distributions. The objective is to minimize a worst-case cost across all distributions in the ambiguity set, including non-Gaussian distributions. Despite the added complexity, we prove that a control policy that is linear in the observations is optimal, as in the classic LQG problem. We propose a numerical solution method that efficiently characterizes this optimal control policy. Our method uses the Frank-Wolfe algorithm to identify the least-favorable distributions within the Wasserstein ambiguity sets and computes the controller's optimal policy using Kalman filter estimation under these distributions.

## 1 Introduction

The Linear Quadratic Regulator (LQR) is a classic control problem that has served as a building block for numerous applications in engineering and computer science , economics , or neuroscience . It involves controlling a system with linear dynamics and imperfect observations affected by additive noise, with the goal of minimizing a quadratic state and control cost. Under the assumption that noise terms are independent and normally distributed (a case referred to as Linear-Quadratic-Gaussian, or LQG), it is well known that the optimal control policy depends linearly on the observations and can be obtained efficiently by using the Kalman filtering procedure and dynamic programming .

Motivated by practical settings where noise distributions may not be readily available or may not be Gaussian, this paper considers a discrete-time, finite-horizon generalization of the LQG setting where noise distributions are unknown and are chosen adversarially from ambiguity sets characterized by a Wasserstein distance and centered around nominal (Gaussian) distributions.

We show that, even under distributional ambiguity, the optimal control policy remains linear in the system's observations. Our proof is novel and does not rely on traditional recursive dynamic programming arguments. Instead, we re-parametrize the control policy in terms of the purified state observations and we derive an upper bound for the resulting minimax formulation by relaxing the ambiguity set (from a Wasserstein ball into a Gelbrich ball) while simultaneously restricting the controller to linear dependencies. We then use convex duality to prove that this upper bound matchesa lower bound obtained by restricting the ambiguity set in the dual of the minimax formulation. This implies the optimality of linear output feedback controllers, thus generalizing the classic results to a distributionally robust setting.

We also find that the worst-case distribution is actually Gaussian, which leads to a very efficient algorithm for finding optimal controllers. Specifically, we propose an algorithm based on the Frank-Wolfe first-order method that at every step solves sub-problems corresponding to classic LQG control problems, using Kalman filtering and dynamic programming. We show that this algorithm enjoys a sublinear convergence rate and is susceptible to parallelization. Lastly, we implement the algorithm leveraging PyTorch's automatic differentiation module and we find that it yields uniformly lower runtimes than a direct method (based on solving semidefinite programs) across all problem horizons.

### Literature Review

This paper is related to the ample literature in control theory and engineering aimed at designing controllers that are robust to noise. The classic LQR/LQG theory, developed in the 1960s, examined linear dynamical systems in either time or frequency domain, seeking to minimize a combination of quadratic state and control costs (in time-domain) or the \(_{2}\) norm of the system's transfer function (in frequency domain). Motivated by findings that LQG controllers do not provide the guaranteed robust stability properties of LQR controllers , much effort has been devoted subsequently to designing controllers that are robust to worst-case perturbations, typically evaluated in terms of the \(_{}\) norm of the system's transfer function (see, e.g.,  for a comprehensive review of \(_{}\) and \(_{2}\) controllers). Because \(_{}\) controllers tend to be overly conservative , various approaches have been proposed to balance the performance of nominal and robust controllers, e.g., by combining \(_{2}\) and \(_{}\) approaches . A parallel stream of literature has considered risk-sensitive control , which minimizes an entropic risk measure instead of the expected quadratic cost. Although risk-sensitive control has a distributionally robust flavor (as the entropic risk measure is equivalent to a distributionally robust quadratic objective penalized via Kullback-Leibler divergence), risk-sensitive control models do not admit a distributionally robust formulation because the entropic risk measure is convex, but not coherent . In contrast, our distributionally robust model provides a direct interpretation of the exact set of noise distributions against which the controller provides safeguards, and leads to a computationally tractable framework for finding the optimal controller.

In this sense, our work is more directly related to the literature on distributionally robust control, which seeks controllers that minimize expected costs under worst-case noise distributions . Closest to our work are .  proves the optimality of linear state-feedback control policies for a related minimax LQR model with a Wasserstein distance but with _perfect_ state observations. With perfect observations, the optimal policies in the classic LQR formulation are independent of the noise distribution and are thus inherently already robust, so considering imperfect observations is what makes the problem significantly more challenging in our case.  studies a minimax formulation based on the Wasserstein distance with both state and observation noise but without any control policy, and focuses solely on the problem of estimating the states. Several papers have considered robust formulations with imperfect observations but for constrained systems , which are more challenging; the common approach is to restrict attention to linear feedback policies for computational tractability, and without proving their optimality.

Also related is the recent literature stream on distributionally robust optimization using the Wasserstein distance . Within this stream, the closest work is , which consider the problem of minimax mean-squared-error estimation when ambiguity is modeled with a Wasserstein distance from a nominal Gaussian distribution. Our proof builds on some ideas from these papers (e.g., relying on the Gelbrich distance in the construction of the upper bound), which it combines with ideas from control theory on purified output-feedback to obtain the overall construction. Also related is , which studies multistage distributionally robust problems with ambiguity sets given by a nested Wasserstein distance for stochastic processes and identifies computationally tractable cases. For a broader overview of developments related to optimal transport and Wasserstein distance with an emphasis on computational tractability and applications in machine learning, we refer to .

Finally, our paper is also related to literature that documents the optimality of linear/affine policies in (distributionally) robust dynamic optimization models.  prove optimality for one-dimensional linear systems affected by additive noise and with perfect state observations, but with general (convex) state and/or control costs,  provide computationally tractable approaches to quantifying the suboptimality of affine controllers in finite or infinite-horizon settings, and  characterize the performance of affine policies in two-stage (distributionally) robust dynamic models.

_Notation._ All random objects are defined on a probability space \((,,)\). Thus, the distribution of any random vector \(:^{d}\) is given by the pushforward distribution \(_{}=^{-1}\) of \(\) with respect to \(\). The expectation under \(\) is denoted by \(_{}[]\). For any \(t_{+}\), we set \([t]=\{0,,t\}\).

## 2 Problem Definition

We consider a discrete-time linear dynamical system

\[x_{t+1}=A_{t}x_{t}+B_{t}u_{t}+w_{t} t[T-1]\] (1)

with states \(x_{t}^{n}\), control inputs \(u_{t}^{m}\), process noise \(w_{t}^{n}\) and system matrices \(A_{t}^{n n}\) and \(B_{t}^{n m}\). The controller only has access to imperfect state measurements

\[y_{t}=C_{t}x_{t}+v_{t} t[T-1]\] (2)

corrupted by observation noise \(v_{t}^{p}\), where \(C_{t}^{p n}\) and usually \(p n\) (so that observing \(y_{t}\) would not allow reconstructing \(x_{t}\) even if there were no observation noise). The control inputs \(u_{t}\) are causal, i.e., depend on the past observations \(y_{0},,y_{t}\) but not on the future observations \(y_{t+1},,y_{T-1}\). More precisely, the set of feasible control inputs \(_{y}\) is the set of random vectors \((u_{0},u_{1},,u_{T-1})\) where for every \(t\) there exists a measurable control policy \(_{t}:^{p(t+1)}^{m}\) such that \(u_{t}=_{t}(y_{0},,y_{t})\). Controlling the system generates costs that depend quadratically on the states and the controls:

\[J=_{t=0}^{T-1}(x_{t}^{}Q_{t}x_{t}+u_{t}^{}R_{t}u_{t})+x_{T}^{} Q_{T}x_{T},\] (3)

where \(Q_{t}_{+}^{n}\) and \(R_{t}_{++}^{m}\) represent the state and input cost matrices, respectively. The exogenous random vectors \(x_{0}\), \(\{w_{t}\}_{t=0}^{T-1}\) and \(\{v_{t}\}_{t=0}^{T-1}\) are mutually independent and follow probability distributions given by \(_{x_{0}},\{_{w_{t}}\}_{t=0}^{T-1}\), and \(\{_{v_{t}}\}_{t=0}^{T-1}\), respectively. As the control inputs are causal, the system equations (2) imply that \(x_{t}\), \(u_{t}\) and \(y_{t}\) can be expressed as measurable functions of the exogenous uncertainties \(x_{0}\) as well as \(w_{s}\) and \(v_{s}\), \(s[t]\), for every \(t\). From now on we may thus assume without loss of generality that \(=^{n}^{n T}^{p T}\) is the space of realizations of the exogenous uncertainties, \(\) is the Borel \(\)-algebra on \(\) and \(=_{x_{0}}(_{t=0}^{T-1}_{w_{t}}) (_{t=0}^{T}_{v_{t}})\), where \(_{1}_{2}\) denotes the independent coupling of the distributions \(_{1}\) and \(_{2}\).

In this context, the classic LQG model assumes that \(\) is known and Gaussian, and seeks \(u_{y}\) that minimizes \(_{}[J]\). Appendix SSA reviews the standard approach for computing optimal control inputs by estimating states through Kalman filtering techniques and using dynamic programming.

In contrast, we assume that \(\) is only known to belong to an ambiguity set \(\), and we formulate a distributionally robust LQG problem that seeks \(u_{y}\) to minimize the worst-case expected cost:

\[_{}_{}[_{t=0}^{T-1}(x _{t}^{}Q_{t}x_{t}+u_{t}^{}R_{t}u_{t})+x_{T}^{}Q_{T}x_{T}].\] (4)

We construct the ambiguity set \(\) as a ball based on the Wasserstein distance. Specifically, we assume that a _nominal_ Gaussian distribution \(}=}_{x_{0}}(_{t=0}^{T-1}}_{w_{t}})(_{t=0}^{T}}_{v_{t}})\) is available so that \(}_{x_{0}}=(0,_{0})\), \(}_{w_{t}}=(0,_{t})\), and \(}_{v_{t}}=(0,_{t})\) for all \(t[T-1]\), and \(\) is given by:

\[=_{x_{0}}(_{t=0}^{T-1}_{w_{t}}) (_{t=0}^{T-1}_{v_{t}}),\]

where

\[_{x_{0}} =\{_{x_{0}}(^{n}):( }_{x_{0}},_{x_{0}})_{x_{0}},\ _{_{x_{0}}}[x_{0}]=0\}\] \[_{w_{t}} =\{_{w_{t}}(^{n}):( }_{w_{t}},_{w_{t}})_{w_{t}},\ _{_{w_{t}}}[w_{t}]=0\}\] \[_{v_{t}} =\{_{v_{t}}(^{m}):( }_{v_{t}},_{v_{t}})_{v_{t}},\ _{_{v_{t}}}[v_{t}]=0\},\]

and \(\) is the \(2\)-Wasserstein distance. Thus, by construction, all exogenous random variables \(x_{0},w_{0},,w_{T-1},v_{0},,v_{T-1}\) are independent under every distribution in \(\).

**Definition 1** (2-Wasserstein distance).: _The 2-Wasserstein distance between two distributions \(_{1}\) and \(_{2}\) on \(^{d}\) with finite second moments is given by_

\[(_{1},_{2})=(_{( _{1},_{2})}_{^{d}^{d}}\|_{ 1}-_{2}\|_{2}^{2}\,(_{1},_{2}))^{},\]

_where \((_{1},_{2})\) denotes the set of all couplings, that is, all joint distributions of the random variables \(_{1}\) and \(_{2}\) with marginal distributions \(_{1}\) and \(_{2}\), respectively._

Our model strictly generalizes the classic LQG setting,1 which can be recovered by choosing \(_{x_{0}}=_{w_{t}}=_{v_{t}}=0\). The parameters \(\) thus allow quantifying the uncertainty about the nominal model and building robustness to mis-specification. We emphasize that the Wasserstein ambiguity set \(\) contains many non-Gaussian distributions and it is not readily obvious that the worst-case distribution in (4) is in fact Gaussian. However, the set \(\) is also non-convex, as it contains only distributions under which the exogenous uncertainties are independent, which makes the distributionally robust LQG problem potentially difficult to solve.

## 3 Nash Equilibrium and Optimality of Linear Output Feedback Controllers

We henceforth view the distributionally robust LQG problem as a zero-sum game between the controller, who chooses causal control inputs, and nature, who chooses a distribution \(\). In this section we show that this game admits a Nash equilibrium, where nature's Nash strategy is a _Gaussian_ distribution \(^{}\) and the controller's Nash strategy is a _linear_ output feedback policy based on the Kalman filter evaluated under \(^{}\).

Purified Observations.Before outlining our proof strategy, we first simplify the problem formulation by re-parametrizing the control inputs in a more convenient form (following ). Note that the control inputs in the LQG formulation are subject to cyclic dependencies, as \(u_{t}\) depends on \(y_{t}\), while \(y_{t}\) depends on \(x_{t}\) through (2), and \(x_{t}\) depends again on \(u_{t}\) through (1), etc. Because these dependencies make the problem hard to analyze, it is preferable to instead consider the controls as functions of a new set of so-called _purified_ observations instead of the actual observations \(y_{t}\).

Specifically, we first introduce a fictitious _noise-free_ system

\[_{t+1}=A_{t}_{t}+B_{t}u_{t} t[T-1]_{t}=C_{t}_{t} t[T-1]\]

with states \(_{t}^{n}\) and outputs \(_{t}^{p}\), which is initialized by \(_{0}=0\) and controlled by the _same_ inputs \(u_{t}\) as the original system (2). We then define the purified observation at time \(t\) as \(_{t}=y_{t}-_{t}\) and we use \(=(_{0},,_{T-1})\) to denote the trajectory of _all_ purified observations.

As the inputs \(u_{t}\) are causal, the controller can compute the fictitious state \(_{t}\) and output \(_{t}\) from the observations \(y_{0},,y_{t}\). Thus, \(_{t}\) is representable as a function of \(y_{0},,y_{t}\). Conversely, one can show by induction that \(y_{t}\) can also be represented as a function of \(_{0},,_{t}\). Moreover, any measurable function of \(y_{0},,y_{t}\) can be expressed as a measurable function of \(_{0},,_{t}\) and vice-versa [27, Proposition II.1]. So if we define \(\,_{}\) as the set of all control inputs \((u_{0},u_{1},,u_{T-1})\) so that \(u_{t}=_{t}(_{0},,_{t})\) for some measurable function \(_{t}:^{p(t+1)}^{m}\) for every \(t[T-1]\), the above reasoning implies that \(_{}=_{y}\).

In view of this, we can rewrite the distributionally robust LQG problem equivalently as

\[p^{} =\{_{x,u,y}&_{p} \ _{}[u^{}Ru+x^{}Qx]\\ &u_{y},\ x=Hu+Gw,\ y=Cx+v\\ .\] (5) \[=\{_{x,u}&_{p}\ _{ }[u^{}Ru+x^{}Qx]\\ &u_{}, x=Hu+Gw,.\]

where \(x=(x_{0},,x_{T})\), \(u=(u_{0},,u_{T-1})\), \(y=(y_{0},,y_{T-1})\), \(w=(x_{0},w_{0},,w_{T-1})\), \(v=(v_{0},,v_{T-1})\), \(=(_{0},,_{T-1})\), and \(R\), \(Q\), \(H\), \(G\) and \(C\) are suitable block matrices(see Appendix SSB for their precise definitions). The latter reformulation involving the purified observations \(\) is useful because these are _independent_ of the inputs. Indeed, by recursively combining the equations of the original and the noise-free systems, one can show that \(=Dw+v\) for some block triangular matrix \(D\) (see Appendix SSB for its construction). This shows that the purified observations depend (linearly) on the exogenous uncertainties but _not_ on the control inputs. Hence, the cyclic dependencies complicating the original system are eliminated in (5).

Subsequently, we also study the dual of (5), defined as

\[d^{}=\{_{}&_{x,u }\ _{}[u^{}Ru+x^{}Qx]\\ &u_{},\ \ x=Hu+Gw..\] (6)

The classic minimax inequality implies that \(p^{} d^{}\). If we can prove that \(p^{}=d^{}\), that (5) has a solution \(u^{}\) and that (6) has a solution \(^{}\), then \((u^{},^{})\) must be a Nash equilibrium of the zero-sum game at hand [43, Theorem 2]. However, because \(_{}\) is an infinite-dimensional function space and \(\) is an infinite-dimensional, non-convex set of non-parametric distributions, the existence of a Nash equilibrium (in pure strategies) is not at all evident. Instead, our proof strategy will rely on constructing an upper bound for \(p^{}\) and a lower bound for \(d^{}\), and showing that these match.

Upper Bound for \(p^{}\).We obtain an upper bound for \(p^{}\) by suitably _enlarging_ the ambiguity set \(\) and _restricting_ the controllers \(u_{t}\) to linear dependencies. We enlarge \(\) by ignoring all information about the distributions in \(\) except for their covariance matrices, and by replacing the Wasserstein distance with the Gelbrich distance. To that end, we first define the Gelbrich distance on the space of covariance matrices.

**Definition 2** (Gelbrich distance).: _The Gelbrich distance between the two covariance matrices \(_{1},_{2}_{+}^{d}\) is given by_

\[(_{1},_{2})=(_{1}+ _{2}-2(_{2}^{}_{1}_{2}^{} )^{})}.\]

We are interested in the Gelbrich distance because of its close connection to the 2-Wasserstein distance. Indeed, it is known that the 2-Wasserstein distance between two distributions with zero means is bounded below by the Gelbrich distance between the respective covariance matrices.

**Proposition 3.1** (Gelbrich bound [24, Theorem 2.1]).: _For any two distributions \(_{1}\) and \(_{2}\) on \(^{d}\) with zero means and covariance matrices \(_{1},_{2}_{+}^{d}\), respectively, we have \((_{1},_{2})(_{1},_{2})\)._

Recalling that \(_{0}\), \(_{t}\) and \(_{t}\) respectively denote the covariance matrices for \(x_{0},w_{t}\) and \(v_{t}\) under the nominal distribution \(}\), we can then define the following Gelbrich ambiguity set for the exogenous uncertainties:

\[=_{x_{0}}(_{t=0}^{T-1}_{w_{t}} )(_{t=0}^{T-1}_{v_{t}}),\]

where

\[_{x_{0}} =\{_{x_{0}}(^{n}):_{ _{x_{0}}}[x_{0}]=0,\ _{}[x_{0}x_{0}^{}]=X_{0},\ (X_{0},_{0})_{x_{0}}\}\] \[_{w_{t}} =\{_{w_{t}}(^{n}):_{ _{w_{t}}}[w_{t}]=0,\ _{}[w_{t}w_{t}^{}]=W_{t},\ (W_{t},_{t})_{w_{t}}\}\] \[_{v_{t}} =\{_{v_{t}}(^{m}):_{ _{v_{t}}}[v_{t}]=0,\ _{}[v_{t}v_{t}^{}]=V_{t},\ (V_{t},_{t}) _{v_{t}}\}.\]

By construction, the random vectors \(x_{0}\), \(\{w_{t}\}_{t=0}^{T-1}\) and \(\{v_{t}\}_{t=0}^{T-1}\) are thus mutually independent under any \(\). In addition and as a direct consequence of Proposition 3.1, \(\) constitutes an outer approximation for the Wasserstein ambiguity set \(\), as summarized in the next result.

**Corollary 1** (Gelbrich hull).: _We have \(\)._

Because \(\) covers \(\), we henceforth refer to it as the _Gelbrich hull_ of the Wasserstein ambiguity set \(\). To finalize our construction of the upper bound on \(p^{}\), we focus on linear policies2 of the form \(u=q+U=q+U(Dw+v)\), where \(q=(q_{0},,q_{T-1})\), and \(U\) is a block lower triangular matrix

\[U=U_{0,0}&&\\ U_{1,0}&U_{1,1}&&\\ &&&\\ U_{T-1,0}&&&U_{T-1,T-1}.\] (7)

The block lower triangularity of \(U\) ensures that the corresponding controller is causal, which in turn ensures that \(u_{}\). In the following, we denote by \(\) the set of all block lower triangular matrices of the form (7). An upper bound on problem (5) can now be obtained by _restricting_ the controller's feasible set to causal controllers that are _linear_ in the purified observations \(\) and by _relaxing_ nature's feasible set to the Gelbrich hull \(\) of \(\). The resulting bounding problem is given by

\[^{}=\{_{U,q,x,u}& _{}\ _{}[u^{}Ru+x^{}Qx]\\ &U,\ \ u=q+U(Dw+v),\ \ x=Hu+Gw..\] (8)

As we obtained (8) by restricting the feasible set of the outer minimization problem and relaxing the feasible set of the inner maximization problem in (5), it is clear that \(^{} p^{}\). Recall also that problem (5) constitutes an infinite-dimensional zero-sum game, where the agents optimize over measurable policies and non-parametric distributions, respectively. In contrast, the next proposition shows that problem (8) is equivalent to a finite-dimensional zero-sum game.

**Proposition 3.2**.: _Problem (8) is equivalent to the optimization problem_

\[^{}=\{_{ Q^{T}\\ U}_{W_{W}\\ V_{V}}((D^{}U^{ }(R\!+\!H^{}QH)UD\!+2G^{}QHUD\!+\!G^{}QG)W)\\ +((U^{}(R+H^{}QH)U)V)\!+\!q^{ }(R\!+\!H^{}QH)q,.\] (9)

_where_

\[_{W} =\{W_{+}^{n(T+1)}:W= {diag}(X_{0},W_{0},,W_{T-1}),\ X_{0}_{+}^{n},\ W_{t} _{+}^{n}\ \  t[T-1]\\ (X_{0},_{0})^{2}_{x_{0}}^{2},\ \ (W_{t},_{t})^{2} _{x_{t}}^{2}\ \  t[T-1]\}\] \[_{V} =\{V_{+}^{pT}:&V= (V_{0},,V_{T-1}),\ V_{t}_{+}^{p},\ \ (V_{t},_{t})^{2}_{v_{t}}^{2}\ \  t[T-1]\}.\]

We emphasize that Proposition 3.2 remains valid even if the nominal distribution \(}\) fails to be normal. Note also that, while nature's feasible set in (8) is non-convex due to the independence conditions, the sets \(_{W}\) and \(_{V}\) are convex and even semidefinite representable thanks to the properties of the squared Gelbrich distance.3 By dualizing the inner maximization problem, one can therefore reformulate the minimax problem (9) as a convex semidefinite program (SDP). Even though this SDP is computationally tractable in theory, it involves \((T(mp+n^{2}+p^{2}))\) decision variables. For practically interesting problem dimensions, it thus quickly exceeds the capabilities of existing solvers.

Lower Bound for \(d^{}\).To derive a tractable lower bound on \(d^{}\), we restrict nature's feasible set to the family \(_{}\) of all _normal_ distributions in the Wasserstein ambiguity set \(\). The resulting bounding problem is thus given by

\[^{}=\{_{ _{}}&_{x,u}&_{ }[u^{}Ru+x^{}Qx]\\ &u_{},\ \ \ x=Hu+Gw..\] (10)

As we obtained (10) by restricting the feasible set of the outer maximization problem in (6), it is clear that \(^{} d^{}\). Next, we show that (10) can be recast as a finite-dimensional zero-sum game. This result critically relies on the following known fact regarding the 2-Wasserstein distance between two _normal_ distributions, which coincides with the Gelbrich distance between their covariance matrices.

**Proposition 3.3** (Tightness for normal distributions [26, Proposition 7]).: _For any two normal distributions \(_{1}=(0,_{1})\) and \(_{2}=(0,_{2})\) with zero means we have \((_{1},_{2})=(_{1},_{2})\)._

With this, we can provide a finite-dimensional reformulation, as summarized in the next result.

**Proposition 3.4**.: _Problem (10) is equivalent to the optimization problem_

\[^{}=\{_{W _{W}\\ V_{V}}&_{q^{p^{}}\\ U}&(D^{}U^{}(R+H^{ }QH)UD+2G^{}QHUD+G^{}QGW)\\ .\] (11)

_where \(_{W}\) and \(_{V}\) are defined exactly as in Proposition 3.2._

Proposition 3.4 relies on Proposition 3.3 and thus fails to hold unless \(}\) is normal. Also, one can again reformulate (11) as a tractable SDP by dualizing the inner minimization problem.

Conclusions.Propositions 3.2 and 3.4 reveal that problems (9) and (11) are dual to each other, that is, they can be transformed into one another by interchanging minimization and maximization. The following main theorem shows that strong duality holds irrespective of the problem data.

**Theorem 3.5** (Strong duality of (9) and (11)).: _We have \(^{}=^{}\)._

Theorem 3.5 follows immediately from Sion's classic minimax theorem , which applies because \(_{W}\) and \(_{V}\) are convex as well as compact thanks to [38, Lemma A.6].

By weak duality and the construction of the bounding problems (9) and (11), we trivially have \(^{} d^{} p^{}^{}\). Theorem 3.5 reveals that all of these inequalities are in fact equalities, each of which gives rise to a non-trivial insight. The first key insight is that (5) and (6) are strong duals.

**Corollary 2** (Strong duality of (5) and (6)).: _We have \(p^{}=d^{}\)._

We stress that, unlike Theorem 3.5, Corollary 2 establishes strong duality between two _infinite-dimensional_ zero-sum games. The second key implication of Theorem 3.5 is that the distributionally robust LQG problem (5) is solved by a linear output-feedback controller.

**Corollary 3** (The controller's Nash strategy is linear in the observations).: _There exist \(U^{}\) and \(q^{}^{m}\) such that the distributionally robust LQG problem (5) is solved by \(u^{}=q^{}+U^{}y\)._

The identity \(p^{}=^{}\) readily implies that (5) is solved by a causal controller that is linear in the _purified_ observations. However, any causal controller that is linear in the purified observations \(\) can be reformulated _exactly_ as a causal controller that is linear in the original observations \(y\) and vice-versa [6, Proposition 3]. Thus, Corollary 3 follows. The third key implication of Theorem 3.5 is that the _dual_ distributionly robust LQG problem is solved by a normal distribution.

**Corollary 4** (Nature's Nash strategy is a normal distribution).: _The dual distributionally robust LQG problem (6) is solved by a distribution \(^{}_{}\)._

Corollary 4 is a direct consequence of the identity \(^{}=d^{}\). Note that the optimal normal distribution \(^{}\) is uniquely determined by the covariance matrices \(W^{}\) and \(V^{}\) of the exogenous uncertain parameters, which can be computed by solving problem (11). That the worst-case distribution is actually Gaussian is not a-priori expected and is surprising given that the Wasserstein ball contains many non-Gaussian distributions.

## 4 Efficient Numerical Solution of Distributionally Robust LQG Problems

Having proven these structural results, we next turn attention to the problem of finding the optimal strategies. Our next result shows that, under a mild regularity condition, the optimal controller \(u^{}\) of the distributionally robust LQG problem (5) can be computed efficiently from \(^{}\).

**Proposition 4.1** (Optimality of Kalman filter-based feedback controllers).: _If \(_{t} 0\) for all \(t[T-1]\), then problem (6) is solved by a Gaussian distribution \(^{}\) under which \(v_{t}\) has a covariance matrix \(V^{}_{t} 0\) for every \(t[T-1]\), and (5) is solved by the optimal LQG controller corresponding to \(^{}\). Additionally, the optimal value of problem (9) and its strong dual (11) does not change if we restrict \(_{W}\) and \(_{V}\) to \(_{W}^{+}\) and \(_{V}^{+}\), respectively, where_

\[_{W}^{+} =\{W_{W}:X_{0}_{}(_ {0})I,\ W_{t}_{}(_{t})I\  t[T-1]\},\] \[_{V}^{+} =\{V_{V}:V_{t}_{}(_ {t})I\  t[T-1]\}.\]This implies that the optimal controller can be computed by solving a classic LQG problem corresponding to nature's optimal strategy \(^{}\), which can be done very efficiently through Kalman filtering and dynamic programming (see Appendix SSA for details). It thus suffices to design an efficient algorithm for computing \(^{}\), which is uniquely determined by the covariance matrices \((W^{},V^{})\) that solve problem (11). To this end, we first reformulate (11) as

\[_{W_{W}^{+},V_{V}^{+}}f(W,V),\] (12)

where we restrict \(_{W}\) and \(_{V}\) to \(_{W}^{+}\) and \(_{V}^{+}\), respectively, due to Proposition 4.1, and where \(f(W,V)\) denotes the optimal value function of the inner minimization problem in (11). As (11) is a reformulation of (10) and as the family of all causal purified output-feedback controllers matches the family of causal output-feedback controllers, \(f(W,V)\) can also be viewed as the optimal value of the classic LQG problem corresponding to the normal distribution \(\) determined by the covariance matrices \(W\) and \(V\). These insights lead to the following structural result.

**Proposition 4.2**.: \(f(W,V)\) _is concave and \(\)-smooth in \((W,V)_{W}^{+}_{V}^{+}\) for some \(>0\)._

By Proposition 4.2, it is possible to address problem (12) with a Frank-Wolfe algorithm . Each iteration of this algorithm solves a direction-finding subproblem, that is, a variant of problem (12) that maximizes the first-order Taylor expansion of \(f(W,V)\) around the current iterates.

\[_{L_{W}_{W}^{+},L_{V}_{V}^{+}}_{W }f(W,V),L_{W}-W+_{V}f(W,V),L_{V}-V\] (13)

The next iterates are then obtained by moving towards a maximizer \((L_{W}^{},L_{V}^{})\) of (13), i.e., we update

\[(W,V)(W,V)+(L_{W}^{}-W,L_{v}^{}-V),\]

where \(\) is an appropriate step size. The proposed Frank-Wolfe algorithm enjoys a very low per-iteration complexity because problem (13) is separable. To see this, we reformulate (13) as

\[_{L_{W},L_{V}} _{X_{0}}f(W,V),L_{X_{0}}-X_{0}+_{t=0}^{T- 1}_{W_{t}}f(W,V),L_{W_{t}}-W_{t}+_{V_{t}}f(W, V),L_{V_{t}}-V_{t}\] \[ (L_{X_{0}},_{0})^{2}_{x_{0}}^{2},\ \ (L_{W_{t}},_{t})^{2}_{w_{t}}^{2},\ \ (L_{V_{t}},_{t})^{2}_{v_{t}}^{2}\ \ \  t[T-1]\] \[L_{X_{0}}_{}(_{0})I,\ \ \ \ \ L_{W_{t}} _{}(_{t})I,\ \ \ \ L_{V_{t}}_{}(_{t})I\ \  t[T-1].\]

Hence, (13) decomposes into \(2T+1\) separate subproblems that can be solved in parallel. That is, for any matrix \(Z\{X_{0},W_{0},,W_{T-1},V_{0},,V_{T-1}\}\) we solve a separate subproblem of the form

\[_{L_{Z}_{}()}\{_{Z}f(W,V),L_{Z} -Z:(L_{Z},)^{2}_{z}^{2}\}.\] (14)

These subproblems can be reformulated as tractable SDPs and are thus amenable to efficient off-the-shelf solvers. By [38, Theorem 6.2], however, one can exploit the structure of the Gelbrich distance in order to reduce (14) to a univariate algebraic equation that can be solved to any desired accuracy \(>0\) by a highly efficient bisection algorithm. We say that \(L_{Z}^{}\) is a \(\)-approximate solution of problem (14) for some \((0,1)\) if \(L_{Z}^{}\) is feasible in (14) and if

\[_{Z}f(W,V),L_{Z}^{}-Z_{Z}f(W, V),L_{Z}^{}-Z,\]

where \(L_{Z}^{}\) is an exact maximizer of (14). Note that, by the concavity of \(f(W,V)\), the inner product on the right-hand side is nonnegative and vanishes if and only if \(Z\) maximizes \(f(W,V)\) over the feasible set of (14). For further details we refer to Appendix SSE in the supplementary material.

**Remark 1** (Automatic differentiation).: _Recall that \(f(W,V)\) coincides with the optimal value of the LQG problem corresponding to the normal distribution \(\) determined by the covariance matrices \(W\) and \(V\). By using the underlying dynamic programming equations, \(f(W,V)\) can thus be expressed in closed form as a serial composition of \((T)\) rational functions (see Appendix SSA for details). Hence, \(_{Z}f(W,V)\) can be calculated symbolically for any \(Z\{X_{0},W_{0},,W_{T-1},V_{0},,V_{T-1}\}\) by repeatedly applying the chain and product rules. However, the resulting formulas are lengthy and cumbersome. We thus compute the gradients numerically using backpropagation. The cost of evaluating \(_{Z}f(W,V)\) is then of the same order of magnitude as the cost of evaluating \(f(W,V)\)._

A detailed description of the proposed Frank-Wolfe method is given in Algorithm 1 below.

By [31, Theorem 1 and Lemma 7], which applies thanks to the structural properties of \(f(W,V)\) established in Proposition 4.2, Algorithm 1 attains a suboptimality gap of \(\) within \((1/)\) iterations.

```
0: initial iterates \(W\), \(V\), nominal covariance matrices \(\), \(\), oracle precision \((0,1)\)
1: set initial iteration counter \(k=0\)
2:while stopping criterion is not met do
3:for\(Z\{X_{0},W_{0},,W_{T-1},V_{0},,V_{T-1}\}\)do in parallel
4: compute \(_{Z}f(W,V)\)
5: find a \(\)-approximate solution \(L^{}_{Z}\) of (14)
6:end
7:\(g_{W}f(W,V),L^{}_{W}-W+_{V}f( W,V),L^{}_{V}-V\)
8:\((W,V)(W,V)+2/(2+k)(L^{}_{W}-W,L^{}_{V}-V)\)
9:endwhile
10:Output: \(W\) and \(V\) ```

**Algorithm 1** Frank-Wolfe algorithm for solving (12)

## 5 Numerical Experiments

All experiments are run on an Intel i7-8700 CPU (3.2 GHz) machine with 16GB RAM. All linear SDP problems are modeled in Python 3.8.6 using CVXPY  and solved with MOSEK . The gradients of \(f(W,V)\) are computed via Pymanopt  with PyTorch's automated differentiation module .

Consider a class of distributionally robust LQG problems with \(n=m=p=10\). We set \(A_{t}=0.1 A\) to have ones on the main diagonal and the superdiagonal and zeroes everywhere else (\(A_{i,j}=1\) if \(i=j\) or \(i=j-1\) and \(A_{i,j}=0\) otherwise), and the other matrices to \(B_{t}=C_{t}=Q_{t}=R_{t}=I_{d}\). The Wasserstein radii are set to \(_{x_{0}}=_{w_{t}}=_{v_{t}}=10^{-1}\). The nominal covariance matrices of the exogenous uncertainties are constructed randomly and with eigenvalues in the interval \(\) (so as to ensure they are positive definite). The code is publicly available in the Github repository https://github.com/RAO-EPFL/DR-Control1.

The optimal value of the distributionally robust LQG problem (5) can be computed by directly solving the SDP reformulation of (11) with MOSEK or by solving the nonlinear SDP (12) with our Frank-Wolfe method detailed in Algorithm 1. We next compare these two approaches in 10 independent simulation runs, where we set a stopping criterion corresponding to an optimality gap below \(10^{-3}\) and we run the Frank-Wolfe method with \(=0.95\). Figure 0(a) illustrates the execution time for both approaches as a function of the planning horizon \(T\); runs where MOSEK exceeds \(100\)s are not reported. Figure 0(b) visualizes the empirical convergence behavior of the Frank-Wolfe algorithm. The results highlight that the Frank-Wolfe algorithm achieves running times that are uniformly lower than MOSEK across all problem horizons and is able to find highly accurate solutions already after a small number of iterations (50 iterations for problem instances of time horizon \(T=10\)).

Figure 1: (a) Execution time for MOSEK and Frank-Wolfe algorithm over 10 simulation runs as a function of the horizon \(T\) (solid lines show the mean and the shaded areas correspond to 1 standard deviation). (b) Convergence of optimality gap for Frank-Wolfe algorithm with horizon \(T=10\).

## 6 Concluding Remarks and Limitations

In view of the popularity of LQG models, the results in this work carry important theoretical and practical implications. Despite considering a generalization of the classic LQG setting where the noise affecting the system dynamics and the observations follows unknown (and potentially non-Gaussian) distributions, our findings suggest that certain classic structural results continue to hold and that highly efficient methods can be adapted to tackle this more realistic (and more challenging) problem. Specifically, that control policies depending linearly on observations continue to be optimal and that the worst-case distribution turns out to be Gaussian is surprising from a theoretical angle and also has direct practical implications, because it allows leveraging the highly efficient Kalman filter in conjunction with dynamic programming and a Frank-Wolfe method to design an efficient computational procedure for solving the problem.

The results also raise several important questions that warrant future exploration. First, it would be highly relevant to consider extensions where the system matrices are also affected by uncertainty, as this captures many applications of practical interest in, e.g., reinforcement learning or revenue management. Second, it would be worth exploring an infinite horizon setting or relaxing the assumption that the nominal distribution is Gaussian, as both assumptions may be limiting the practical appeal of the framework. Third, one could also attempt to prove structural optimality results or design novel algorithms for generating high-quality suboptimal solutions for the more general setting involving constraints on states and/or control inputs. Lastly, one could improve the present algorithmic proposal by exploiting topological properties of the objective so as to guarantee linear convergence rates in the Frank-Wolfe procedure.

Acknowledgements.This research was supported by the Swiss National Science Foundation under the NCCR Automation, grant agreement 51NF40_180545. Dan A. Iancu would like to acknowledge INSEAD for financial support during the duration of the project.