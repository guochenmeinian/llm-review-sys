# Towards the Dynamics of a DNN Learning Symbolic Interactions

Qihan Ren\({}^{1}\)

Equal contribution.

Junpeng Zhang\({}^{1,2}\)1

Yang Xu\({}^{3}\)

Yue Xin\({}^{1}\)

Dongrui Liu\({}^{1,4}\)

Qunashi Zhang\({}^{1}\)

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Beijing Institute for General Artificial Intelligence

\({}^{3}\)Zhejiang University \({}^{4}\)Shanghai Artificial Intelligence Laboratory

{renqihan, zhangjp63, zqs1022}@sjtu.edu.cn

###### Abstract

This study proves the two-phase dynamics of a deep neural network (DNN) learning interactions. Despite the long disappointing view of the faithfulness of post-hoc explanation of a DNN, a series of theorems have been proven  in recent years to show that for a given input sample, a small set of interactions between input variables can be considered as primitive inference patterns that faithfully represent a DNN's detailed inference logic on that sample. Particularly, Zhang et al.  have observed that various DNNs all learn interactions of different complexities in two distinct phases, and this two-phase dynamics well explains how a DNN changes from under-fitting to over-fitting. Therefore, in this study, we mathematically prove the two-phase dynamics of interactions, providing a theoretical mechanism for how the generalization power of a DNN changes during the training process. Experiments show that our theory well predicts the real dynamics of interactions on different DNNs trained for various tasks.

## 1 Introduction

**Background: mathematically guaranteeing that the inference score of a DNN can be faithfully explained as symbolic interactions.** Explaining the detailed inference logic hidden behind the output score of a DNN is considered one of the core issues for the post-hoc explanation of a DNN. However, after a comprehensive survey of various explanation methods, many studies  have unanimously and empirically arrived at a disappointing view of the faithfulness of almost all post-hoc explanation methods. Fortunately, the recent progress  has mathematically proven that given a specific input sample \(=[x_{1},,x_{n}]^{}\), a DNN3 for a classification task usually only encodes a small set of interactions between input variables in the sample. It is proven that these interactions act like primitive inference patterns and can accurately predict all network outputs, no matter how we randomly mask the input sample4. An _interaction_ refers to a non-linear relationship encoded by the DNN between a set of input variables in \(S\). For example, as Figure 1 shows, a DNN may encode a non-linear relationship between the three image patches in \(S=\{x_{1},x_{2},x_{3}\}\) to form a _dog-snout_ pattern, which makes a numerical effect \(I(S)\) on the network output. The _complexity_ (or _order_) of an interaction is defined as the number of input variables in the set \(S\), _i.e._, \((S)}}{{=}}|S|\).

**Our task.** Since Zhou et al.  found that high-order (complex) interactions usually have a much higher risk of over-fitting than low-order (simple) interactions, in this study, we hope to further trackthe change in the complexity of interactions during training, so as to explain the change of the DNN's generalization power during training. In particular, the time when the DNN starts to learn high-order (complex) interactions indicates the starting point of over-fitting.

**Specifically, we focus on the two-phase dynamics of interaction complexity which was empirically observed by , and we aim to mathematically prove this dynamics.** First, before training, a DNN with randomly initialized parameters mainly encodes interactions of medium complexities. As Figure 2 shows, the distribution of interactions appears spindle-shaped. Then, in the first phase, the DNN eliminates interactions of medium and high complexities, thereby mainly encoding interactions of low complexity. In the second phase, the DNN gradually learns interactions of increasing complexities. We have conducted experiments to train DNNs with various architectures for different tasks. It shows that our theory can well predict the learning dynamics of interactions in real DNNs.

**The proven two-phase dynamics explain hidden factors that push the DNN from under-fitting to over-fitting.** (1) In the first phase, the DNN mainly removes noise interactions, (2) In the second phase, the DNN gradually learns more complex and non-generalizable interactions toward over-fitting.

## 2 Related work

**Long-standing disappointment on the faithfulness of existing post-hoc explanation of DNNs.** Many studies [30; 40; 29; 2; 15] have explained the inference score of a DNN, but how to mathematically formulate and guarantee the faithfulness of the explanation is still an open problem. For example, using an interpretable surrogate model to approximate the output of a DNN [3; 11; 35; 34] is a classic explanation technique. However, the good matching between the DNN's output and the surrogate model's output cannot fully guarantee that the two models use exactly the same inference patterns and/or use the same attention. Therefore, many studies [28; 12; 1] have unanimously and empirically arrived at a disappointing view of the faithfulness of current explanation methods. Rudin  pointed out that inaccurate post-hoc explanations of DNNs would be harmful to high-stakes applications. Ghassemi et al.  showed various failure cases of current explanation methods in the healthcare field and argued that using these methods to aid medical decisions was a false hope.

**New progress towards proving the faithfulness of symbolic explanation of a DNN.** Despite the disappointing view of post-hoc explanation methods, we have established a theory system of interactions within three years, which includes more than 30 papers, to quantify the symbolic concepts encoded by a DNN and explain the hidden factors that determine the generalization power and robustness of a DNN. We revisit this theory system as follows.

\(\)_Proving interactions act as faithful primitives inference patterns encoded by the DNN._ Recent achievements in the theory system of interactions have provided a new perspective to formulate primitive inference patterns encoded by a DNN. We discovered  and proved  that a DNN's inference logic on a certain sample can be explained by only a small number of interactions. Furthermore, we discovered that salient interactions usually represented common inference patterns shared by different samples (sample-wise transferability of interactions) , and proposed a method to extract generalizable interactions shared by different DNNs (model-wise transferability of interactions) .

Figure 1: (a) It is proven that the DNNâ€™s inference on a certain sample is equivalent to a logical model that uses a small number of AND-OR interactions for inference. Each interaction corresponds to a non-linear (AND or OR) relationship between a set \(S\) of input variables (_e.g._, image patches). (b) Sparsity of interactions. We show the strength \(|I(S|)|\) of all \(2^{n}\) interactions sorted in descending order. (c) Illustration of the two-phase dynamics of a DNN learning interactions of different orders.

The above studies indicated that salient interactions could be considered primitive inference patterns encoded by a DNN, which served as the theoretical foundation of this study. Based on interactions, we also defined and learned the optimal baseline value for the Shapley value , and explained the encoding of different types of visual patterns in DNNs for image classification [5; 6].

\(\)_Using interactions to explain the representation power of DNNs._ Our recent studies showed that interactions well explained the hidden factors that determine the adversarial robustness , adversarial transferability , and generalization power  of a DNN. We also discovered and proved the representation bottleneck of a DNN in encoding middle-complexity interactions . In addition, we proved that compared to a standard DNN, a Bayesian neural network (BNN) tended to avoid encoding complex interactions , thus explaining the good adversarial robustness of BNNs. We discovered and explained the phenomenon that DNNs tended to learn simple interactions more easily than complex interactions . We found that complex interactions were less generalizable than simple interactions , and further discovered the two-phase dynamics of a DNN learning interactions of different complexities . To this end, this study aims to theoretically prove the discovery in  to better understand the two-phase dynamics of interactions.

\(\)_Using interactions to unify the common mechanism of various empirical deep learning methods._ We proved that fourteen attribution methods could all be explained as a re-allocation of interaction effects . We proved that twelve existing methods to improve adversarial transferability all shared the common utility of suppressing the interactions between adversarial perturbation units .

## 3 Dynamics of interactions

### Preliminary: interactions

Let us consider a DNN \(v\) and an input sample \(=[x_{1},,x_{n}]^{}\) with \(n\) input variables indexed by \(N=\{1,,n\}\). In different tasks, one can define different input variables, _e.g._, each input variable may represent an image patch for image classification or a word/token for text classification. Let us consider a scalar output5 of a DNN, denoted by \(v()\). Previous studies [4; 43] show that _the output score \(v()\) can be decomposed into the sum of AND interactions and OR interactions._

\[v()=v(_{})+_{ S N}I_{ }(S|)+_{ S N}I_{} (S|),\] (1)

where the computation of \(I_{}(S|)\) and \(I_{}(S|)\) will be introduced later in Eq. (2).

**How to understand the physical meaning of AND-OR interactions.** Suppose that we are given an input sample \(\). According to Theorem 2, a non-zero interaction effect \(I_{}(S|)\) indicates that the entire function of the DNN must equivalently encode an AND relationship between input variables in the set \(S N\), although the DNN does not use an explicit neuron to model such an AND relationship. As Figure 1 shows, when the image patchs in the set \(S_{2}\!=\!\{x_{1}\!=\!,x_{2}\!=\!,x_{3}\!=\!\}\) are all present (_i.e._, not masked), the three regions form a _dog-snout_ pattern, and make a numerical effect \(I_{}(S_{2}|)\) to push the output score \(v()\) towards the dog category. Masking any image patch in \(S_{2}\) will deactivate the AND interaction and remove \(I_{}(S_{2}|)\) from \(v()\). This will be shown by Theorem 2. Likewise, \(I_{}(S|)\) can be considered as the numerical effect of the OR relationship encoded by the DNN between input variables in the set \(S\). As Figure 1 shows, when one of the patches in \(S_{1}\!=\!\{x_{4}\!=\!,x_{5}\!=\!\}\) is present, a _speckles_ pattern is used by the DNN to make a numerical effect \(I_{}(S_{1}|)\) on the network output \(v()\).

**Definition and computation.** Given a DNN and an input \(\), the AND-OR interactions between each specific set of input variables \(S N(S)\) are computed as follows [4; 43].

\[I_{}(S|)=_{T S}(-1)^{|S|-|T|}v_{}(_{T}), I_{}(S|)=-_{T  S}(-1)^{|S|-|T|}v_{}(_{N T}),\] (2)

where \(_{T}\) denotes the sample in which input variables in \(N T\) are masked6, while input variables in \(T\) are unchanged. The network output on each masked sample \(v(_{T}),T N\), is decomposed into two components: (1) the component \(v_{}(_{T})=0.5v(_{T})+_{T}\) that exclusively contains AND interactions, and (2) the component \(v_{}(_{T})=0.5v(_{T})-_{T}\) that exclusively contains OR interactions, subject to \(v(_{T})=v_{}(_{T})+v_{}(_{T})\). Appendix F.1 shows that \(v_{}(_{T})=v(_{})+_{ S^{}  T}I_{}(S^{}|)\) and \(v_{}(_{T})=_{S^{} N;S^{} T; 0 }I_{}(S^{}|)\). The sparsest AND-OR interactions are extracted by minimizing the following objective : \(_{(_{T})}_{S N}|I_{}(S|)|+|I_{} (S|)|\). Please see Appendix C for details about the computation and Appendix D for mathematical support of the coefficient in Eq. (2).

**Salient interactions and noisy patterns.** Let us enumerate all \(2^{n}\) combinations of variables \(S N\), and compute the interaction effects \(I_{}(S|)\) and \(I_{}(S|)\). We can identify a few _salient interactions_ from all these interactions, _i.e._, interactions whose absolute value exceeds a threshold (\(|I_{}(S|)|\) or \(|I_{}(S|)|\)). Other interactions have small effects and are termed _noisy patterns_.

**Theorem 1** (Sparsity property, proven by , and discussed in Appendix B).: _Given a DNN \(v\) and an input sample \(\) with \(n\) input variables, let \(}}{{=}}\{S N:|I_{}( S|)|\}\) denote the set of salient AND interactions whose absolute value exceeds a threshold \(\). If the DNN can generate relatively stable inference outputs \(v(_{S})\) on masked samples7, then the size of the set \(||\) has an upper bound of \((n^{}/)\), where \(\) is an intrinsic parameter for the smoothness of the network function \(v()\). Empirically, \(\) is usually within the range of ._

**Theorem 2** (Universal matching property, proven in  and Appendix F.1).: _Given an input sample \(}\), let us construct the following surrogate logical model \(f()\) to use AND-OR interactions for inference, which are extracted from the DNN \(v()\) on the sample \(}\). Then, the output of the surrogate logical model \(f()\) can always match the output of the DNN \(v()\), no matter how the input sample is masked._

\[ S\!\!N,\,f(}_{S})\!=\!v(}_{S}),\,\,f( }_{S})\!=}_{})\!+\!_{T  N}I_{}(T|})\!\!_{( T)}}\!+\!\!I_{}(T|} )\!\!_{(T)}}_{v_{}(_{S})}\] (3)

\[=v(\!=\!}_{})+_{ T S}I_{ }(T|\!=\!})+_{T N:T S }I_{}(T|\!=\!})\] (4)

\[ v(\!=\!}_{})+_{T_{}:  T S}I_{}(T|\!=\!})+ _{T_{}:T S}I_{}(T| \!=\!}),\] (5)

_where \(_{}\) is the set of all salient AND interactions, and \(_{}\) is the set of all salient OR interactions._

**What makes the interaction-based explanation faithful.** The following four properties guarantee that the inference score of a DNN can be faithfully explained by symbolic interactions.

\(\)_Sparsity property._ The sparsity property means that a DNN for a classification task usually only encodes a small number of AND interactions with salient effects, _i.e._, for most of all \(2^{n}\) subsets of input variables \(S N\), \(I_{}(S|)\) has almost zero interaction effect. Specifically, the sparsity property has been widely observed on various DNNs for different tasks , and it is also theoretically proven (see Theorem 1). The number of AND interactions whose absolute value exceeds the threshold \(\) (\(|I_{}(S|)|\)), is \((n^{}/)\), where \(\) is empirically within the range of \([1.9,2.2]\). This indicates that the number of salient interactions is much less than \(2^{n}\). Furthermore, the sparsity property also holds for OR interactions, because an OR interaction can be viewed as a special kind of AND interaction8.

\(\)_Universal matching property._ The universal matching property means that the output of the DNN on a masked sample \(_{S}\) can be well matched by the sum of interaction effects, no matter how we randomly mask the sample and obtain \(_{S}\). This property is proven in Theorem 2.

\(\)_Transferability property._ The transferability property means that salient interactions extracted from one input sample can usually be extracted from other input samples as well. If so, these interactions are considered transferable across different samples. This property has been widely observed by  on various DNNs for different tasks.

[MISSING_PAGE_FAIL:5]

increased. In the second phase (from the 3rd column to the 6th column in the figure), the DNN learned interactions of increasing orders (complexities).

**How to understand the two-phase phenomenon.** Previous studies [44; 26] have observed and partially proved that the complexity/order of an interaction can reflect the generalization ability10 of the interaction. Let us consider an interaction that is frequently extracted by a DNN from training samples (see the transferability property in Section 3.2). If this interaction also frequently appears in testing samples, then this interaction is considered generalizable10; otherwise, non-generalizable. To this end, Zhou et al.  have discovered that high-order (complex) interactions are less generalizable between training and testing samples than low-order (simple) interactions. Furthermore, Ren et al.  have proved that high-order (complex) interactions are more unstable than low-order (simple) interactions when input variables or network parameters are perturbed by random noises.

Therefore, the two-phase dynamics enable us to revisit the change of generalization power of a DNN:

1. Before training, the interactions extracted from an initialized DNN exhibited a spindle-shaped distribution of interaction strength over different orders. These interactions could be considered random patterns irrelevant to the task, and such patterns were mostly of medium orders.
2. In the first phase, the DNN mainly removed the irrelevant patterns caused by the randomly initialized parameters. At the same time, the DNN shifted its attention to low-order interactions between very few input variables. These low-order interactions usually represented relatively simple and generalizable10 inference patterns, without encoding complex inference patterns. 3. In the second phase, the DNN gradually learned interactions of increasing orders (increasing complexities). **Although there was no clear boundary between under-fitting and over-fitting in mathematics, the learning of very complex interactions had been widely considered as a typical sign of over-fitting10**.

### Proving of the two-phase dynamics

#### 3.3.1 Analytic solution to interaction effects

As the foundation of proving the dynamics of the two phases, let us first derive the analytic solution to interaction effects at a specific time point during the training process. Then, Sections 3.3.2 and 3.3.3 will use this analytic solution to further explain detailed dynamics in the second phase and the first phase, respectively. Later experiments show that our theory can well predict the true dynamics of all AND-OR interactions during the learning of real DNNs.

The proof in this subsection can be divided into three steps. (1) We first rewrite a DNN's inference on an input sample as a weighted sum of triggering functions of different interactions. (2) Then, we can reformulate the learning of the DNN on an input sample as a linear regression problem. (3) Thus, the interactions at an intermediate time point during training can be obtained as the optimal solution to the linear regression problem under a certain level of parameter noises.

\(\)**Step 1: Rewriting a DNN's inference on an input sample as a weighted sum of triggering functions of different interactions.** For simplicity, let us only focus on the dynamics of AND interactions, because OR interactions can also be represented as a specific kind of AND interactions8 (see Appendix E for details). In this way, without loss of generality, let us just analyze the learning of AND interactions _w.r.t._\(v_{}}()=v(_{}})+_{  S N}I_{}}(S|)\), and simplify the notation as \(v()=v(_{}})+_{ S N}I(S| )\) in the following proof. Our conclusions can also be extended to OR interactions, as mentioned above.

Given a DNN, we follow [26; 22] to rewrite the inference function of the network \(v()\). This is inspired by the universal matching property of interactions in Theorem 2, _i.e._, given any arbitrarily masked input sample \(}_{S}\)_w.r.t._ a random subset \(S N\), the network output can always be represented as a linear sum of different interaction effects \(v(=}_{S})=_{T S}I(T|=})\). In this way, the following equation rewrites the inference function of the DNN \(v(}=}_{S})\) as the weighted sum of triggering functions of interactions (see Appendix F.2 for proof).

\[\;S N,\;v(\!=\!}_{S})=f(\!=\!}_{S}),\;\;f()}}{{=}}_{T  N}w_{T}\;J_{T}(),\] (6)where the interaction triggering function \(J_{T}()\) is a real-valued approximation of the binary indicator function \((}_{S}\) triggers the AND relation \(T\)) in Eq. (3) and returns the triggering value of the interaction pattern \(T\). In particular, we set \(w_{}=v(=}_{})\), \(J_{}()=1\). \(J_{T}()\) is computed as a sum of compositional terms in the Taylor expansion of \(v()\).

\[J_{T}()=_{ Q_{T}}^{n}_{i}!}++_{n}}v}{ x_{1}^{_{1}} x_{ n}^{_{n}}}_{=_{}}_{i T}(x_{i}-b_{i} )^{_{i}}/w_{T},\] (7)

where the scalar weight \(w_{T}\) should be computed as \(w_{T}=I(T|=})\) to satisfy the equality in Eq. (6), and \(Q_{T}=\{[_{1},,_{n}]^{}: i T,_{i}^{ +}; i T,_{i}=0\}\). See Appendix F.2 for proof.

**Understanding \(J_{T}()\) and \(w_{T}\).** Let us consider a masked sample \(}_{S}\) in which input variables in \(N S\) are masked. If \(T S\), which means all input variables in \(T\) are not masked in \(}_{S}\), then \(J_{T}(}_{S})=1\), indicating the interaction pattern is triggered; otherwise, \(J_{T}(}_{S})=0\), indicating the interaction pattern is not triggered. \(w_{T}\) is a scalar weight. Particularly, let \(I_{f}(T|)\) denote the interaction extracted from the function \(f()=_{T N}w_{T}J_{T}()\), then we have \(I_{f}(T|)=w_{T}\).

\(\)**Step 2: Based on Eq. (6), the learning of the DNN on an input sample can be reformulated as learning the scalar weight \(w_{T}\) for each interaction triggering function \(J_{T}()\), under a linear regression setting.** We can roughly consider the learning problem as a linear regression to a set of _potentially true interactions_, because it has been discovered by  that different DNNs for the same task usually encode similar sets of interactions. Therefore, the learning of a DNN can be considered as training a model to fit a set of pre-defined interactions. _In spite of the above simplifying settings, subsequent experiments in Figure 4 still verify that our theoretical results can well predict the learning dynamics of interactions in real DNNs._

Specifically, let the DNN be trained on a set of samples \(=\{(,y)\}\). According to Theorem 2, given each training sample \(\), output scores of the finally converged DNN on all \(2^{n}\) randomly masked samples \(\{_{S}:S N\}\) can be written in the form of \(y_{S}}}{{=}}y(_{S})=v(_{ })+_{ T N}(_{S}\) triggers interaction \(T) w_{T}^{*}=v(_{})+_{ T S}w_{T}^{*}\), which is determined by parameters \(\{w_{T}^{*}:T N\}\)11. \(\{w_{T}^{*}:T N\}\) can be taken as a set of true interactions that the DNN needs to learn. Therefore, the learning of the converged interactions on the training sample \(\) can be represented as the regression towards the converged function \(y(_{S})\) on all masked samples \(\{(_{S},y_{S}):S N\}\).

\[L()=_{S N}[(y_{S}-^{}(_{S}))^{2}].\] (8)

where we simplify the notation as follows. \(}}{{=}}(\{w_{T}:T  N\})^{2^{n}}\) denotes the weight vector of \(2^{n}\) different interactions, and \((_{S})}}{{=}}( \{\{J_{T}(_{S}):T N\}})^{2^{n}}\) denotes the vector of triggering values of \(2^{n}\) different interactions \(\{T N\}\) on the masked sample \(_{S}\).

\(\)**Step 3: Directly optimizing Eq. (8) gives the interactions of the finally converged DNN \(w_{T} w_{T}^{*}\), but how do we estimate the interactions in an intermediate time point during the training process?** To this end, we assume that the training process of the DNN is subject to parameter noises (see Lemma 1). In fact, this assumption is common. Before training, randomly initialized parameters in the DNN are pure noises without clear meanings. In this way, the DNN's training process can be viewed as a process of gradually reducing the noise on its parameters. This is also supported by the lottery ticket hypothesis , _i.e._, the learning process actually penalizes most noisy parameters and learns a very small number of meaningful parameters. _Therefore, as training proceeds, the noise on the network parameters can be considered to gradually diminish._

**Lemma 1** (Noisy triggering function, proven in Appendix F.3).: _If the inference score of the DNN contains an unlearnable noise, i.e., \( S N,(_{S})=v(_{S})+ v_{S}\), \( v_{S}(0,^{2})\), then the interaction between input variables w.r.t. \( T N\), extracted from inference scores \(\{(_{S})\}\) can be written as \((T|)=I(T|)+ I_{T}\), where \( I_{T}\) denotes the noise in the interaction caused by the noise in the output \( v_{S}\), and we have \([ I_{T}]=0\) and \([ I_{T}]=2^{|T|}^{2}\). In this way, given an input sample \(}\), we can consider the scalar weight \(w_{T}=I(T|=})\), and consider the interaction triggering function \(_{T}()=J_{T}()+_{T}\), where \(J_{T}()\) is defined in Eq. (7). \(_{T}= I_{T}/w_{T}\) represents the noise term on the triggering function. We have \([_{T}]=0\) and \([_{T}] 2^{|T|}^{2}\) w.r.t. noises._Therefore, the learned interactions under unavoidable parameter noises can be represented as minimizing the following loss, where we vectorize the noise \(\!=\!(\{_{T}\!:\!T N\})\!\! ^{2^{n}}\) for simplicity.

\[()=_{}_{S N}[(y_{ S}-^{}}(_{S}))^{2}]=_{} _{S N}[(y_{S}-^{}((_{S})+))^{2}].\] (9)

**Remark.** The minimizer to Eq. (9) **does not** represent the end of training, but represents the _intermediate state_ of interactions after _a certain epoch in the training process_. We formulate the training process as a process of gradually reducing the noise on the DNN's parameters, and the minimizer \(}\) to Eq. (9) represents the optimal interaction state when the training is subject to certain _parameter noises_. We will show later that the minimizer \(}\) computed under different noise levels can accurately predict the dynamics of interactions during the training process (see Figures 4 and 8).

**Assumption 1**.: _To simplify the proof, we assume that different noise terms \(_{T}\) on the triggering function are independent, and uniformly set the variance as \(\,T N\), \([_{T}]=2^{|T|}^{2}\)._

Assumption 1 is made according to two findings in Lemma 1: (1) the interaction triggering function \(_{T}()\) is real-valued subject to the noise on the DNN's parameters, (2) the variance of the interaction triggering function \(_{T}()\) increases exponentially along with the order \(|T|\). More importantly, the assumed exponential increase of the variance in the above finding (2) has been widely observed in various DNNs trained for different tasks in previous experiments [26; 22].

**Theorem 3** (Proven in Appendix F.4).: _Let \(}=_{}()\) denote the optimal solution to the minimization of the loss function \(()\). Then, we have_

\[}=(^{}+2^{n}())^{-1}^{ }=(^{}+2^{n}())^{-1}^{ }^{*}=}^{*},\] (10)

_where \(}}{{=}}[(_{S_{1}}), (_{S_{2}}),,(_{S_{2n}})]^{}^{2^ {n} 2^{n}}\) is a matrix to represent the triggering values of \(2^{n}\) interactions (w.r.t. \(2^{n}\) columns) on \(2^{n}\) masked samples (w.r.t. \(2^{n}\) rows). \(_{S_{1}},_{S_{2}},,_{S_{2^{n}}}\) enumerate all masked samples. \(}}{{=}}[y(_{S_{1}}),y(_{S_{2}}),,y(_{S_{2^{n}}})]^{}^{2^{n}}\) enumerates the finally-converged outputs on \(2^{n}\) masked samples. \(}}{{=}}( \{[_{T}]:T N\})=(\{2^{|T| }^{2}:T N\})^{2^{n}}\) denotes the vector of variances of the triggering values of \(2^{n}\) interactions. The matrix \(}\) is defined as \(}}}{{=}}(^{}+2^{n}())^{-1}^{}\), and \(^{*}}}{{=}}( \{w_{T}^{*}:T N\})\)._

In this way, Theorem 3 provides an analytic solution to the minimization of \(()\) under parameter noises. Experiments in Figure 4 will show that the learning dynamics of interactions derived from our simplifying assumption can still predict the real distribution of interactions over different orders.

#### 3.3.2 Explaining the dynamics in the second phase

Based on the above analytic solution, this subsection aims to prove that in the second phase, the DNN first encodes interactions of low orders and then gradually encodes interactions of higher orders.

\(\)**The second phase can be viewed as a process of gradually reducing the noise level \(^{2}\).** The analytic solution \(}\) in Theorem 3 under different noise levels \(^{2}\) enables us to analyze the dynamics of interactions during the second phase. This is because the noise on the network parameters can be considered to gradually diminish during the training process, as we assume in Section 3.3.1. Then accordingly, the noise level \(^{2}\) of the noise term \(_{T}\) on the interaction triggering function also gradually diminishes during training. At the start of the second phase, the noise level \(^{2}\) is large, and the interaction triggering function \(_{T}()\) is dominated by the noise term \(_{T}\). Later, as training proceeds in the second phase, the noise level \(^{2}\) gradually decreases, making less effect on the interaction triggering function.

\(\)**The change of the analytic solution \(}\) along with the decreasing noises \(^{2}\) explains the dynamics in the second phase.** We prove that as \(^{2}\) decreases, the ratio of low-order interaction strength to high-order interaction strength in the analytic solution \(}\) decreases. This means that the DNN gradually learns higher-order interactions in the second phase, which can be verified by our observation in Figure 2. The detailed results are derived as follows.

**Lemma 2** (Proven in Appendix F.5).: _The compositional term \(J_{T}()\) in the Taylor expansion in Eq. (7) always has fixed values on \(2^{n}\) masked samples \(\{_{S}:S N\}\), i.e., \( S N\), \(J_{T}(_{S})=(T S)\). It means that the matrix \(=[(_{S_{1}}),(_{S_{2}}),,(_{S_{ 2^{n}}})]^{}\{0,1\}^{2^{n} 2^{n}}\) in Eq. (10) is a fixed binary matrix, no matter how we change the DNN \(v()\) or the input sample \(\)._

**Theorem 4** (Proven in Appendix F.6).: _According to Theorem 3, we can write the analytic solution of the interaction effect \(_{T}\) w.r.t. a subset \(T\) as \(_{T}=}_{T}^{}^{*}\), where \(}_{T}^{}^{1 2^{n}}\) denotes a row vector of the matrix \(}=|}_{T_{1}},}_{T_{2}},}_{T_{ 2n}}|^{}\), indexed by \(T\). Combining with Lemma 2, for any two subsets \(T,T^{} N\) of the same order, i.e., \(|T|=|T^{}|\), we have \(\|}_{T}\|_{2}=|}_{T^{}}\|_{2}\)._

**Proposition 1**.: _For any two subsets \(T,T^{} N\) with \(|T|<|T^{}|\), \(\|}_{T}\|_{2}/\|}_{T^{}}\|_{2}\) is greater than 1 and decreases monotonically as \(^{2}\) decreases throughout training. The norm \(\|}_{T}\|_{2}\) is only determined by \(n\), \(^{2}\), and the order \(|T|\), but is agnostic to finally-converged interactions \(\{w_{T}^{}:T N\}\)._

Proposition 1 shows a monotonic decrease of \(\|}_{T}\|_{2}/\|}_{T^{}}\|_{2}\) along with the decrease of \(^{2}\). The physical meaning of \(\|}_{T}\|_{2}/\|}_{T^{}}\|_{2}\) can be understood as follows. According to \(_{T}=}_{T}^{}^{*}\), \(\|}_{T}\|_{2}\) reflects the strength of the DNN encoding the interaction \(T\). In this way, \(\|}_{T}\|_{2}/\|}_{T^{}}\|_{2}\) measures the relative strength of encoding a low-order interaction \(T\)_w.r.t._ that of encoding a high-order interaction \(T^{}\).

_Conclusions from Theorem 4 and Proposition 1:_ Because the second phase is viewed as a process of gradually reducing the noise level \(^{2}\), Theorem 4 and Proposition 1 explain why the DNN mainly encodes low-order interactions and suppresses high-order interactions at the start of the second phase (when \(^{2}\) is large). They also explain why the DNN learns interactions of increasing orders during the second phase (when \(^{2}\) gradually decreases).

_Experimental verification of Proposition 1:_ We measured the relative strength \(r^{(k)}}}{{=}}\|}_{T}\|_{2}/\| }_{T^{}}\|_{2}\) subject to \(|T|=k\) and \(|T^{}|=k+1\), for \(k=1,,n-1\), under different values of \(^{2}\). Figure 3 shows that when \(^{2}\) decreased, \(r^{(k)}\) monotonically decreased for all orders \(k=1,,n-1\), which verified the proposition. The experiment was conducted using different numbers of input variables \(n\).

**Theorem 5** (Proven in Appendix F.7).: _When \(=0\), \(}\) satisfies \(\  T N,\ _{T}=w_{T}^{*}\)._

Theorem 5 shows a special case when there is no noise on the network parameters. Then, the DNN learns the finally converged interactions \(\{w_{T}^{*}:T N\}\). Note that the finally converged DNN probably encodes some interactions of high orders, which correspond to over-fitted patterns.

\(\)**Experiments on real datasets.** We conducted experiments to examine whether our theory could predict the real dynamics of interaction strength of different orders when we trained DNNs in practice. We trained AlexNet and VGG on the MNIST dataset, the CIFAR-10 dataset, the CUB-200-2011

Figure 4: Comparison between the theoretical distribution of interaction strength \(I_{}^{(k)}\) and the real distribution of interaction strength \(I_{}^{(k)}\) in the second phase. Please see Appendix J.3 for the comparison on the other six DNNs trained for 3D point cloud/image/sentiment classification.

Figure 3: Monotonic increase of \(r^{(k)}\) along with \(^{2}\) mentioned in Proposition 1. We show the curves of \(r^{(k)}\) when we set different numbers of input variables \(n\) and different orders \(k=1,,n-1\).

dataset, and the Tiny-ImageNet dataset, trained BERT-Tiny and BERT-Medium on the SST-2 dataset, and trained DGCNN on the ShapeNet dataset. Then, we computed the real distribution of interaction strength over different orders on each DNN, and tracked the change of the distribution throughout the training process. As mentioned in Section 3.2, the real interaction strength of each \(k\)-th order was quantified as \(I_{}^{(k)}=_{}[_{S|S|=k,|I(S|)|} |I(S|)|]\,/\,Z\)12. Accordingly, we defined the metric \(I_{}^{(k)}=_{}[_{S|S|=k,|_{S}|_ {}}|_{S}|]\,/\,Z_{}\) in the same way of \(I_{}^{(k)}\) to measure the theoretical distribution of the interaction strength, where \(Z_{}=_{1 k^{} n}_{}[_{S |S|=k^{},|_{S}|_{}}|_{S}|]\), \(_{}=0.03|v_{}()-_{}|\), and \(v_{}()}}{{=}}_{S N }_{S}\). To compute the theoretical solution \(}=}^{*}\) in Eq. (10), given an input sample \(\), we used the set of salient interactions \(=\{S N:|I(S|)|\})\) extracted from the finally converged DNN to construct the set of true interactions \(^{*}\).

Figure 4 shows that the theoretical distribution \(I_{}^{(k)}\) could well match the real distribution \(I_{}^{(k)}\) at different training epochs. Particularly, we used a sequence of theoretical distributions of \(I_{}^{(k)}\) with decreasing \(^{2}\) values to match the real distribution of \(I_{}^{(k)}\) at different epochs. The \(^{2}\) value was determined to achieve the best match between \(I_{}^{(k)}\) and \(I_{}^{(k)}\).

#### 3.3.3 Explaining the dynamics in the first phase

Because the spindle-shaped distribution of interaction strength in a randomly initialized DNN has already been proven by , in this subsection, let us further explain the DNN's dynamics in the first phase based on Eq. (9). As previously shown in Figure 2, in the first phase, the DNN removes initial interactions of medium and high orders, and mainly encodes low-order interactions.

Therefore, the first phase is explained as the process of removing chaotic initial interactions and converging to the optimal solution to Eq. (9) under large parameter noise (_i.e._, large \(^{2}\)). In sum, the first phase is a process of _pushing initial random interactions to the optimal solution_, while the second phase corresponds to the _change of the optimal solution_ as \(^{2}\) gradually decreases.

## 4 Conclusion and discussion

In this study, we have proven the two-phase dynamics of a DNN learning interactions of different orders. Specifically, we have followed [26; 22] to reformulate the learning of interactions as a linear regression problem on a set of interaction triggering functions. In this way, we have successfully derived an analytic solution to interaction effects when the DNN was learned with unavoidable parameter noises. This analytic solution has successfully predicted a DNN's two-phase dynamics of learning interactions in real experiments. Considering a series of recent theoretical guarantees of taking interactions as faithful primitive inference patterns encoded by the DNN [44; 27], our study has first mathematically explained why and how the learning process gradually shifts attention from generalizable (low-order) inference patterns to probably over-fitted (high-order) inference patterns.

**Practical implications.** A theoretical understanding of the two-phase dynamics of interactions offers a new perspective to monitor the overfitting level of the DNN on different training samples throughout training. The two-phase dynamics enables us to evaluate the overfitting level of each specific sample, making overfitting no longer a problem _w.r.t._ the entire dataset. We can track the change of the interaction complexity for each training sample, and take the time point when high-order interactions increase as a sign of overfitting. In this way, the two-phase dynamics of interactions may help people remove overfitted samples from training and guide the early stopping on a few "hard samples."

**Acknowledgements.** This work is partially supported by the National Science and Technology Major Project (2021ZD0111602), the National Nature Science Foundation of China (92370115, 62276165). This work is also partially supported by Huawei Technologies Inc.