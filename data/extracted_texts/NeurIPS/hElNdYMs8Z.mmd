# A Finite-Sample Analysis of Payoff-Based Independent Learning in Zero-Sum Stochastic Games

Zaiwei Chen\({}^{1,*}\), Kaiqing Zhang\({}^{2}\), Eric Mazumdar\({}^{1,}\), Asuman Ozdaglar\({}^{3}\), Adam Wierman\({}^{1,}\)

\({}^{1}\)CMS, Caltech, \({}^{*}\)zchen458@caltech.edu, \({}^{}\)mazumdar@caltech.edu, \({}^{}\)adamw@caltech.edu

\({}^{2}\)ECE & ISR, University of Maryland, College Park, kaiqing@umd.edu

\({}^{3}\)EECS, MIT, asuman@mit.edu

###### Abstract

In this work, we study two-player zero-sum stochastic games and develop a variant of the smoothed best-response learning dynamics that combines independent learning dynamics for matrix games with the minimax value iteration for stochastic games. The resulting learning dynamics are payoff-based, convergent, rational, and symmetric between the two players. Our theoretical results present to the best of our knowledge the first last-iterate finite-sample analysis of such independent learning dynamics. To establish the results, we develop a coupled Lyapunov drift approach to capture the evolution of multiple sets of coupled and stochastic iterates, which might be of independent interest.

## 1 Introduction

Recent years have seen remarkable successes in reinforcement learning (RL) in a variety of applications, such as board games , autonomous driving , robotics , and city navigation . A common feature of these applications is that there are _multiple_ decision makers interacting with each other in a common environment. Although empirical successes have shown the potential of multi-agent reinforcement learning (MARL) [5; 6], the training of MARL agents is largely based on heuristics and parameter tuning and, therefore, is not always reliable. In particular, many practical MARL algorithms are directly extended from their single-agent counterparts and lack guarantees because of the adaptive strategies of multiple agents.

A growing literature seeks to provide theoretical insights to substantiate the empirical success of MARL and inform the design of efficient and provably convergent algorithms. Work along these lines can be broadly categorized into work on cooperative MARL where agents seek to reach a common goal [7; 8; 9; 10], and work on competitive MARL where agents have individual (and possibly misaligned) objectives [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22]. While some earlier work focused on providing guarantees on the asymptotic convergence, the more recent ones share an increasing interest in understanding the finite-time/sample behavior. This follows from a line of recent advances in establishing finite-sample guarantees of single-agent RL algorithms, see e.g., [23; 24; 25; 26] and many others.

In this paper, we focus on the benchmark setting of two-player1 zero-sum stochastic games, and develop best-response-type learning dynamics with provable finite-sample guarantees. Crucially, our learning dynamics are independent (requiring no coordination between the agents in learning) and rational (each agent will converge to the best response to the opponent if the opponent plays an (asymptotically) stationary policy ), and therefore capture learning in settings with multiple game-theoretic agents. Indeed, learning dynamics with self-interested agents should not enforce information communication or coordination among agents. Furthermore, we focus on the more challenging but practically relevant setting of payoff-based learning, where each agent can onlyobserve their realized payoff at each stage, without observing the policy or even the action taken by the opponent. For learning dynamics with such properties, we establish to the best of our knowledge the first last-iterate finite-sample guarantees. We detail our contributions as follows.

### Contributions

We first consider zero-sum matrix games and provide the last-iterate finite-sample guarantees for the smoothed best-response dynamics proposed in . Then, we extend the algorithmic idea to the setting of stochastic games and develop an algorithm called value iteration with smoothed best-response dynamics (VI-SBR) that also enjoys last-iterate finite-sample convergence.

**Two-Player Zero-Sum Matrix Games.** We start with the smoothed best-response dynamics in  and establish the last-iterate finite-sample bounds when using stepsizes of various decay rates. The result implies a sample complexity of \((^{-1})\) in terms of the last iterate to find the Nash distribution , which is also known as the quantal response equilibrium in the literature . To our knowledge, this is the first last-iterate finite-sample result for best-response learning dynamics that are payoff-based, rational, and symmetric in zero-sum matrix games.

**Two-Player Zero-Sum Stochastic Games.** Building on the algorithmic ideas for matrix games, we develop best-response-type learning dynamics for stochastic games called VI-SBR, which uses a single trajectory of Markovian samples. Our learning dynamics consist of two loops and can be viewed as a combination of the smoothed best-response dynamics for an induced auxiliary matrix game (conducted in the inner loop) and an independent way of performing minimax value iteration (conducted in the outer loop). In particular, in the inner loop, the iterate of the outer loop, i.e., the value function, is fixed, and the players learn the approximate Nash equilibrium of an auxiliary matrix game induced by the value function; then the outer loop is updated by approximating the minimax value iteration updates for the stochastic game, with only local information.

We establish the last-iterate finite-sample bounds for VI-SBR when using both constant stepsizes and diminishing stepsizes of \((1/k)\) decay rate. To the best of our knowledge, this appears to be the first last-iterate finite-sample analysis of best-response-type independent learning dynamics that are convergent and rational for stochastic games. Most existing MARL algorithms are either symmetric across players, but not payoff-based, e.g., , or not symmetric and thus not rational, e.g., , or do not have last-iterate finite-time/sample guarantees, e.g., .

### Challenges & Techniques

The main challenge in analyzing our learning dynamics is that it maintains multiple sets of stochastic iterates and updates them in a coupled manner. To overcome this challenge, we develop a novel _coupled Lyapunov drift approach_. Specifically, we construct a Lyapunov function for each set of the stochastic iterates and establish a Lyapunov drift inequality for each. We then carefully combine the coupled Lyapunov drift inequalities to establish the finite-sample bounds. Although a more detailed analysis is provided in the appendices, we briefly give an overview of the main challenges in analyzing the payoff-based independent learning dynamics in stochastic games, as well as our techniques to overcome them.

**Time-Inhomogeneous Markovian Noise.** The fact that our learning dynamics are payoff-based presents major challenges in handling the stochastic errors in the update. In particular, due to the best-response nature of the dynamics, the behavior policy for sampling becomes time-varying. In fact, the sample trajectory used for learning forms a time-inhomogeneous Markov chain. This makes it challenging to establish finite-sample guarantees, as time-inhomogeneity prevents us from directly exploiting the uniqueness of stationary distributions and the fast mixing of Markov chains. Building on existing work , we overcome this challenge by tuning the algorithm parameters (in particular, the stepsizes) and developing a refined conditioning argument.

**Non-Zero-Sum Payoffs Due to Independent Learning.** As illustrated in Section 1.1, the inner loop of VI-SBR is designed to approximately learn the Nash equilibrium of an auxiliary matrix game induced by the value functions for the two players, which we denote by \(v_{t}^{1}\) and \(v_{t}^{2}\), where \(t\) is the iteration index of the outer loop. Importantly, the value functions \(v_{t}^{1}\) and \(v_{t}^{2}\) are maintained individually by players \(1\) and \(2\), and therefore do not necessarily satisfy \(v_{t}^{1}+v_{t}^{2}=0\) due to independent learning. As a result, the auxiliary matrix game from the inner loop does _not_ necessarily 

[MISSING_PAGE_FAIL:3]

Zero-Sum Matrix Games

We begin by considering zero-sum matrix games. This section introduces algorithmic and technical ideas that are important for the setting of stochastic games. For \(i\{1,2\}\), let \(^{i}\) be the finite action space of player \(i\), and let \(R_{i}^{|^{i}||^{-i}|}\) (where \(-i\) denotes the index of player \(i\)'s opponent) be the payoff matrix of player \(i\). Note that in a zero-sum game we have \(R_{1}+R_{2}^{}=0\). Since there are finitely many actions for each player, we assume without loss of generality that \(_{a^{i},a^{2}}|R_{1}(a^{1},a^{2})| 1\). Furthermore, we denote \(A_{}=(|^{1}|,|^{2}|)\).

The decision variables here are the policies \(^{i}(^{i})\), \(i\{1,2\}\), where \((^{i})\) denotes the probability simplex supported on \(^{i}\). Given a joint policy \((^{1},^{2})\), the expected reward received by player \(i\) is \(_{A^{i}^{i},A^{-i}^{-i}}[R_{i}(A^{i},^{-i}) ]=(^{i})^{}R_{i}^{-i}\), where \(i\{1,2\}\). Both players aim to maximize their rewards against their opponents. Unlike in the single-agent setting, since the performance of player \(i\)'s policy depends on its opponent \(-i\)'s policy, there is, in general, no universal optimal policy. Instead, we use the _Nash gap_ and the _regularized Nash gap_ as measurements of the performance of the learning dynamics, as formally defined below.

**Definition 2.1** (Nash Gap in Matrix Games).: Given a joint policy \(=(^{1},^{2})\), the Nash gap \((^{1},^{2})\) is defined as \((^{1},^{2})=_{i=1,2}_{^{i}(^{i})}(^{i}-^{i})^{}R_{i}^{-i}\).

Note that \((^{1},^{2})=0\) if and only if \((^{1},^{2})\) is in a Nash equilibrium of the matrix game (which may not be unique), in which no player has the incentive to change its policy.

**Definition 2.2** (Regularized Nash Gap in Matrix Games).: Given a joint policy \(=(^{1},^{2})\) and a constant \(>0\), the entropy-regularized Nash gap \(_{}(^{1},^{2})\) is defined as \(_{}(^{1},^{2})=_{i=1,2}_{^{i} (^{i})}(^{i}-^{i})^{}R_{i}^{-i}+( ^{i})-(^{i})}\), where \(()\) is the entropy function defined as \((^{i})=-_{a^{i}^{i}}^{i}(a^{i})(^{i}(a^{i}))\) for \(i\{1,2\}\).

A joint policy \((^{1},^{2})\) satisfying \(_{}(^{1},^{2})=0\) is called the Nash distribution  or the quantal response equilibrium , which, unlike Nash equilibria, is unique in zero-sum matrix games. As \(\) approaches \(0\), the corresponding Nash distribution approximates a Nash equilibrium .

### The Learning Dynamics in Zero-Sum Matrix Games

We start by presenting in Algorithm 1 (from the perspective of player \(i\), where \(i\{1,2\}\)) the independent learning dynamics for zero-sum matrix games, which was first proposed in . Given \(>0\), we use \(_{}:^{|^{i}|}^{|^{ i}|}\) for the softmax function with temperature \(\), that is, \([_{}(q^{i})](a^{i})=(q^{i}(a^{i})/)/_{^{i} ^{i}}(q^{i}(^{i})/)\) for all \(a^{i}^{i}\), \(q^{i}^{|^{i}|}\), and \(i\{1,2\}\).

```
1:Input: Integer \(K\), initializations \(q^{i}_{0}=0^{|^{i}|}\) and \(^{i}_{0}=(^{i})\).
2:for\(k=0,1,,K-1\)do
3:\(^{i}_{k+1}=^{i}_{k}+_{k}(_{}(q^{i}_{k})-^{i}_{k})\)
4: Play \(A^{i}_{k}^{i}_{k+1}()\) (against \(A^{-i}_{k}\)), and receive reward \(R_{i}(A^{i}_{k},A^{-i}_{k})\)
5:\(q^{i}_{k+1}(a^{i})=q^{i}_{k}(a^{i})+_{k}_{\{a^{i}=A^{i}_{k}\}} (R_{i}(A^{i}_{k},A^{-i}_{k})-q^{i}_{k}(A^{i}_{k}))\) for all \(a^{i}^{i}\)
6:endfor ```

**Algorithm 1** Independent Learning Dynamics in Zero-Sum Matrix Games

To make this paper self-contained, we next provide a detailed interpretation of Algorithm 1, which also motivates our algorithm for stochastic games in Section 3. At a high level, Algorithm 1 can be viewed as a discrete and smoothed variant of the best-response dynamics, where each player constructs an approximation of the best response to its opponent's policy using the \(q\)-function. The update for the \(q\)-function is in the spirit of the TD-learning algorithm in RL .

**The Policy Update.** To understand the update equation for the policies (cf. Algorithm 1 Line \(3\)), consider the discrete version of the smoothed best-response dynamics:

\[^{i}_{k+1}=^{i}_{k}+_{k}(_{}(R_{i}^{-i}_{k})-^{i}_{k} ), i\{1,2\}.\] (1)

In Eq. (1), each player updates its policy \(^{i}_{k}\) incrementally towards the smoothed best response to its opponent's current policy. While the dynamics in Eq. (1) provably converge for zero-sum matrix games, see e.g., , implementing it requires player \(i\) to compute \(_{}(R_{i}_{k}^{-i})\). Note that \(_{}(R_{i}_{k}^{-i})\) involves the exact knowledge of the opponent's policy and the reward matrix, both of which cannot be accessed in payoff-based independent learning. This leads to the update equation for the \(q\)-functions, which estimate the quantity \(R_{i}_{k}^{-i}\) that is needed for implementing Eq. (1).

**The \(q\)-Function Update.** Suppose for now that we are given a _stationary_ joint policy \(=(^{1},^{2})\). Fix \(i\{1,2\}\), the problem of player \(i\) estimating \(R_{i}^{-i}\) can be viewed as a _policy evaluation_ problem, which is usually solved with TD-learning in RL . Specifically, the two players repeatedly play the matrix game with the joint policy \(=(^{1},^{2})\) and produce a sequence of joint actions \(\{(A_{k}^{1},A_{k}^{2})\}_{k 0}\). Then, player \(i\) forms an estimate of \(R_{i}^{-i}\) through the following iterative algorithm:

\[q_{k+1}^{i}(a^{i})=q_{k}^{i}(a^{i})+_{k}_{\{a^{i}=A_{k}^{1}\} }(R_{i}(A_{k}^{i},A_{k}^{-i})-q_{k}^{i}(A_{k}^{i})),\ a^{i} ^{i},\] (2)

with an arbitrary initialization \(q_{0}^{i}^{|^{i}|}\), where \(_{k}>0\) is the stepsize. To understand Eq. (2), suppose that \(q_{k}^{i}\) converges to some \(^{i}\). Then Eq. (2) should be "stationary" at the limit point \(^{i}\) in the sense that \(_{A^{i}^{i}(),A^{i-i}^{-i}()}[_{\{ a^{i}=A^{i}\}}(R_{i}(A^{i},A^{-i})-^{i}(A^{i}))]=0\) for all \(a^{i}^{i}\), which implies \(^{i}=R_{i}^{-i}\), as desired. Although Eq. (2) is motivated by the case when the joint policy \((^{1},^{2})\) is stationary, the joint policy \(_{k}=(_{k}^{1},_{k}^{2})\) from Eq. (1) is time-varying. A natural approach to address this issue is to make sure that the policies evolve at a _slower_ time-scale compared to that of the \(q\)-functions, so that \(_{k}\) is close to being _stationary_ from the perspectives of \(q_{k}^{i}\).

_Remark_.: In , where Algorithm 1 was first proposed, the authors require \(_{k}=o(_{k})\) to establish the asymptotic convergence, making Algorithm 1 a _two time-scale_ algorithm. In this work, for finite-sample analysis and easier implementation, we update \(_{k}^{i}\) and \(q_{k}^{i}\) on a _single time scale_ with only a multiplicative constant difference in their stepsizes, i.e., \(_{k}=c_{,}_{k}\) for some \(c_{,}(0,1)\).

### Finite-Sample Analysis

In this section, we present the finite-sample analysis of Algorithm 1 for the convergence to the Nash distribution . We consider using either constant stepsizes, i.e., \(_{k}\) and \(_{k}=c_{,}\), or diminishing stepsizes with \((1/k)\) decay rate, i.e., \(_{k}=/(k+h)\) and \(_{k}=/(k+h)=c_{,}/(k+h)\). Let \(_{}=[(A_{}-1)(2/)+1]^{-1}\) and \(L_{}=/_{}+A_{}^{2}/\). The requirement for choosing the stepsizes is stated in the following condition.

**Condition 2.1**.: When using either constant or diminishing stepsizes, we choose \( 1\), \(_{0}<}\), \(_{0}<(2,^{2}})\), and \(c_{,}=_{k}/_{k}(^{3}} {32},^{3}}{128A_{}^{2}},}{L_{}^{1/2 }})\).

We next state the finite-sample bounds of Algorithm 1. See Appendix C for the proof.

**Theorem 2.1**.: _Suppose that both players follow the learning dynamics presented in Algorithm 1, and the stepsizes \(\{_{k}\}\) and \(\{_{k}\}\) are chosen such that Condition 2.1 is satisfied. Then we have the following results._

1. _When using constant stepsizes, i.e.,_ \(_{k}\) _and_ \(_{k}\)_, we have_ \[[_{}(_{K}^{1},_{K}^{2})] B_{}( 1-)^{K}+8L_{}+},\] _where_ \(B_{}=4+2(A_{})+2A_{}\)_._
2. _When using_ \(_{k}=/(k+h)\) _and_ \(_{k}=/(k+h)\)_, by choosing_ \(>4\)_, we have_ \[[_{}(_{K}^{1},_{K}^{2})] B_{}( )^{/4}+(64eL_{}+}).\]

The convergence bounds in Theorem 2.1 are qualitatively consistent with the existing results on the finite-sample analysis of general stochastic approximation algorithms [44; 46; 24; 65; 23; 42]. Specifically, when using constant stepsizes, the bound consists of a geometrically decaying term (known as the optimization error) and two constant terms (known as the statistical error) that are proportional to the stepsizes. When using diminishing stepsizes with suitable hyperparameters, both the optimization error and the statistical error achieve an \((1/K)\) rate of convergence.

Although Theorem 2.1 is stated in terms of the expectation of the regularized Nash gap, it implies the mean-square convergence of the policy iterates \((^{1}_{k},^{2}_{k})\). To see this, note that the regularized Nash gap \(_{}(^{1},^{2})\) has a unique minimizer, i.e., the Nash distribution and is denoted by \((^{1}_{*,},^{2}_{*,})\). In addition, fixing \(^{1}\) (respectively, \(^{2}\)), the function \(_{}(^{1},)\) (respectively, \(_{}(,^{2})\)) is a \(\)-strongly convex function with respect to \(^{2}\) (respectively, \(^{1}\)). See Lemma D.7 for a proof. Therefore, by the quadratic growth property of strongly convex functions, we have

\[_{}(^{1}_{k},^{2}_{k}) =_{}(^{1}_{k},^{2}_{k})-_{}(^{ 1}_{*,},^{2}_{k})+_{}(^{1}_{*,},^{2}_{k})-_{}(^{1}_{*,},^{2}_{*,})\] \[(\|^{1}_{k}-^{1}_{*,}\|^{2}_{2}+\|^ {2}_{k}-^{2}_{*,}\|^{2}_{2}).\]

As a result, up to a multiplicative constant, the convergence bound for \([_{}(^{1}_{k},^{2}_{k})]\) implies a convergence bound of \([\|^{1}_{k}-^{1}_{*,}\|^{2}_{2}]+[\|^{2}_{k}- ^{2}_{*,}\|^{2}_{2}]\).

Based on Theorem 2.1, we next derive the sample complexity of Algorithm 1 in the following corollary. See Appendix C.5 for the proof.

**Corollary 2.1.1**.: _Given \(>0\), to achieve \([_{}(^{1}_{K},^{2}_{K})]\), the sample complexity is \((^{-1})\)._

To the best of our knowledge, Theorem 2.1 and Corollary 2.1.1 present the first last-iterate finite-sample analysis of Algorithm 1. Importantly, with only feedback in the form of realized payoffs, we achieve a sample complexity of \((^{-1})\) to find the Nash distribution. In general, for smooth and strongly monotone games, the lower bound for the sample complexity of payoff-based or zeroth-order algorithms is \((^{-2})\). We have an improved \((^{-1})\) sample complexity due to the bilinear structure of the game (up to a regularizer). In particular, with bandit feedback, the \(q\)-function is constructed as an efficient estimator for the marginalized payoff \(R_{i}^{-i}_{k}\), which can also be interpreted as the gradient. Therefore, Algorithm 1 enjoys the fast \((^{-1})\) sample complexity that is comparable to the first-order method .

**The Dependence on the Temperature \(\).** Although our finite-sample bound enjoys the \((1/K)\) rate of convergence, the stepsize ratio \(c_{,}\) appears as \(c_{,}^{-1}\) in the bound. Since \(c_{,}=o(_{})\) (cf. Condition 2.1) and \(_{}\) is exponentially small in \(\), the finite-sample bound is actually exponentially large in \(^{-1}\). To illustrate this phenomenon, consider the update equation for the \(q\)-functions (cf. Algorithm 1 Line \(5\)). Observe that the \(q\)-functions are updated asynchronously because only one component (which corresponds to the action taken at time step \(k\)) of the vector-valued \(q^{i}_{k}\) is updated in the \(k\)-th iteration. Suppose that an action \(a^{i}\) is never taken in the algorithm trajectory, which means that \(q^{i}_{k}(a^{i})\) is never updated during learning. Then, in general, we cannot expect the convergence of \(q^{i}_{k}\) or \(^{i}_{k}\). Similarly, suppose that an action is rarely taken in the learning dynamics, we would expect the overall convergence rate to be slow. Therefore, the finite-sample bound should depend on the quantity \(_{i\{1,2\}}_{0 k K}_{a^{i}}^{i}_{k}(a^{i})\), which captures the exploration abilities of Algorithm 1. Due to the exponential nature of softmax functions, the parameter \(_{}\), which we establish in Lemma C.2 as a lower bound of \(_{i\{1,2\}}_{0 k K}_{a^{i}}^{i}_{k}(a^{i})\), is also exponentially small in \(\). This eventually leads to the exponential dependence in \(^{-1}\) in the finite-sample bound.

A consequence of having such an exponential factor of \(^{-1}\) in the sample complexity bound is that, if we want to have convergence to a Nash equilibrium rather than to the Nash distribution, the sample complexity can be exponentially large. To see this, note that the following bound holds regarding the Nash gap and the regularized Nash gap:

\[(^{1},^{2})_{}(^{1},^{2})+2(A_{ }),\;(^{1},^{2}),\] (3)

which, after combining with Theorem 2.1, gives the following corollary. For simplicity of presentation, we only state the result for using constant stepsizes.

**Corollary 2.1.2**.: _Under the same conditions stated in Theorem 2.1 (1), we have_

\[[(^{1}_{K},^{2}_{K})] B_{}(1-)^{K}+8L_{}+}+2 (A_{}).\] (4)

The last term on the RHS of Eq. (4) can be viewed as the bias due to using smoothed best-response. In view of Eq. (4), to achieve \([(^{1}_{K},^{2}_{K})]\), we need \(=()\). Since \(c_{,}\) appears in the denominator of our finite-sample bound and is exponentially small in \(\), the overall sample complexity for the convergence to a Nash equilibrium can be exponentially large in \(^{-1}\). In Appendix F, we conduct numerical experiments to investigate the impact of \(\) on this smoothing bias.

In light of the discussion before Corollary 2.1.2, the reason for such an exponentially large sample complexity for finding a Nash equilibrium is due to the limitation of using the softmax policies in smoothed best-response for exploration. We kept the softmax policy without further modification to preserve the "naturalness" of the learning dynamics, which is part of the motivation for studying independent learning in games . A future direction of this work is to remove such an exponential dependence on \(\) by designing an improved exploration strategy.

## 3 Zero-Sum Stochastic Games

Moving to the setting of stochastic games, we consider an infinite-horizon discounted two-player zero-sum stochastic game \(=(,^{1},^{2},p,R_{1},R_{2},)\), where \(\) is a finite state space, \(^{1}\) (respectively, \(^{2}\)) is a finite action space for player \(1\) (respectively, player \(2\)), \(p\) represents the transition probabilities, in particular, \(p(s^{} s,a^{1},a^{2})\) is the probability of transitioning to state \(s^{}\) after player \(1\) taking action \(a^{1}\) and player \(2\) taking action \(a^{2}\) simultaneously at state \(s\), \(R_{1}:^{1}^{2}\) (respectively, \(R_{2}:^{2}^{1}\)) is player \(1\)'s (respectively, player \(2\)'s) reward function, and \((0,1)\) is the discount factor. Note that we have \(R_{1}(s,a^{1},a^{2})+R_{2}(s,a^{2},a^{1})=0\) for all \((s,a^{1},a^{2})\). We assume without loss of generality that \(_{s,a^{1},a^{2}}|R_{1}(s,a^{1},a^{2})| 1\), and denote \(A_{}=(|^{1}|,|^{2}|)\).

Given a joint policy \(=(^{1},^{2})\), where \(^{i}:(^{i})\), \(i\{1,2\}\), we define the local \(q\)-function \(q_{}^{i}^{|||^{i}|}\) of player \(i\) as \(q_{}^{i}(s,a^{i})=_{}[_{c=0}^{}^{k}R_{i}( S_{k},A_{k}^{i},A_{k}^{-i})\;]\,S_{0}=s,\,A_{0}^{i}=a^{i}]\) for all \((s,a^{i})\), where we use the notation \(_{}[\,\,]\) to indicate that the actions are chosen according to the joint policy \(\). In addition, we define the global value function \(v_{}^{i}^{||}\) as \(v_{}^{i}(s)=_{A^{i}^{i}(|s)}[q_{}^{i}(s,A^{i})]\) for all \(s\), and the expected value function \(U^{i}(^{i},^{-i})\) as \(U^{i}(^{i},^{-i})=_{S p_{o}}[v_{}^{i}(S)]\), where \(p_{o}()\) is an arbitrary initial distribution on the states. The Nash gap in the case of stochastic games is defined in the following.

**Definition 3.1** (Nash Gap in Zero-Sum Stochastic Games).: Given a joint policy \(=(^{1},^{2})\), the Nash gap \((^{1},^{2})\) is defined as \((^{1},^{2})=_{i=1,2}(_{}\,U^{i}( ^{i},^{-i})-U^{i}(^{i},^{-i}))\).

Similar to the matrix-game setting, a joint policy \(=(^{1},^{2})\) satisfying \((^{1},^{2})=0\) is called a Nash equilibrium, which may not be unique.

**Additional Notation.** In what follows, we will frequently work with the real vectors in \(^{|||^{i}|}\), \(^{|||^{-i}|}\), and \(^{|||^{i}||^{-i}|}\), where \(i\{1,2\}\). To simplify the notation, for any \(x^{|||^{i}||^{-i}|}\), we use \(x(s)\) to denote the \(|^{i}||^{-i}|\) matrix with the \((a^{i},a^{-i})\)-th entry being \(x(s,a^{i},a^{-i})\). For any \(y^{|||^{i}|}\), we use \(y(s)\) to denote the \(|^{i}|\)-dimensional vector with its \(a^{i}\)-th entry being \(y(s,a^{i})\).

### Value Iteration with Smoothed Best-Response Dynamics

Our learning dynamics for stochastic games (cf. Algorithm 2) build on the dynamics for matrix games studied in Section 2.1, with the additional incorporation of minimax value iteration, a well-known approach for solving zero-sum stochastic games .

**Algorithmic Ideas.** To motivate the learning dynamics, we first introduce the minimax value iteration. For \(i\{1,2\}\), let \(^{i}:^{||}^{||| ^{i}||^{-i}|}\) be an operator defined as

\[^{i}(v)(s,a^{i},a^{-i})=R_{i}(s,a^{i},a^{-i})+ [v(S_{1})\;|\;S_{0}=s,A_{0}^{i}=a^{i},A_{0}^{-i}=a^{-i}\]

for all \((s,a^{i},a^{-i})\) and \(v^{||}\). Given \(X^{|^{i}||^{-i}|}\), we define \(^{i}:^{|^{i}||^{-i}|} \) as

\[^{i}(X)=_{^{i}(^{i})}_{^{-i} (^{-i})}\{(^{i})^{}X^{-i}\}=_{^{-i} (^{-i})}_{^{i}(^{i})}\{(^{i})^{}X ^{-i}\}.\]

Then, the minimax Bellman operator \(^{i}:^{||}^{||}\) is defined as \([^{i}(v)](s)=^{i}(^{i}(v)(s))\) for all \(s\), where \(^{i}(v)(s)\) is an \(|^{i}||^{-i}|\) matrix according to our notation. It is known that the operator \(^{i}()\) is a \(-\) contraction mapping with respect to the \(_{}\)-norm , hence it admits a unique fixed point, which we denote by \(v_{*}^{i}\).

A common approach for solving zero-sum stochastic games is to first implement the minimax value iteration \(v^{i}_{t+1}=^{i}(v^{i}_{t})\) until (approximate) convergence to \(v^{i}_{*}\), and then solve the matrix game \(_{^{i}(^{i})}_{^{-i}(^{-i })}(^{i})^{}^{i}(v^{i}_{*})(s)^{-i}\) for each state \(s\) to obtain an (approximate) Nash equilibrium policy. However, implementing this algorithm requires complete knowledge of the underlying transition probabilities. Moreover, since it is an off-policy algorithm, the output is independent of the opponent's policy. Thus, it is not rational by the definition in . To develop a model-free and rational learning dynamics, let us first rewrite the minimax value iteration:

\[v^{i}_{t+1}=,\ \ \ (s)=_{^{i}( ^{i})}_{^{-i}(^{-i})}(^{i})^{} ^{i}(v^{i}_{t})(s)^{-i},\ \ s,\] (5)

where \(^{||}\) is a dummy variable. In view of Eq. (5), we need to solve a matrix game with payoff matrix \(^{i}(v^{i}_{t})(s)\) for each state \(s\) and then update the value of the game to \(v^{i}_{t+1}(s)\). In light of Algorithm 1, we already know how to solve matrix games with independent learning. Thus, what remains to do is to combine Algorithm 1 with value iteration. This leads to Algorithm 2, which is presented from player \(i\)'s perspective, where \(i\{1,2\}\).

```
1:Input: Integers \(K\) and \(T\), initializations \(v^{i}_{0}=0^{||}\), \(q^{i}_{0,0}=0^{|||^{i}|}\), and \(^{i}_{0,0}(|s)=(^{i})\) for all \(s\).
2:for\(t=0,1,,T-1\)do
3:for\(k=0,1,,K-1\)do
4:\(^{i}_{t,k+1}(s)=^{i}_{t,k}(s)+_{k}(_{}(q^{i}_{t,k}(s))- ^{i}_{t,k}(s))\) for all \(s\)
5: Play \(A^{i}_{k}^{i}_{t,k+1}(|S_{k})\) (against \(A^{-i}_{k}\)) and observe \(S_{k+1} p( S_{k},A^{i}_{k},A^{-i}_{k})\)
6:\(q^{i}_{t,k+1}(s,a^{i})=q^{i}_{t,k}(s,a^{i})+_{k}_{\{(s,a^{i})= (S_{k},A^{i}_{k})\}}(R_{i}(S_{k},A^{i}_{k},A^{-i}_{k})+ v^{i}_{t}(S_{k+1 })-\)\(q^{i}_{t,k}(S_{k},A^{i}_{k}))\) for all \((s,a^{i})\)
7:endfor
8:\(v^{i}_{t+1}(s)=^{i}_{t,K}(s)^{}q^{i}_{t,K}(s)\) for all \(s\)
9:\(S_{0}=S_{K}\), \(q^{i}_{t+1,0}=q^{i}_{t,K}\), and \(^{i}_{t+1,0}=^{i}_{t,K}\)
10:endfor ```

**Algorithm 2** Value Iteration with Smoothed Best-Response (VI-SBR) Dynamics

**Algorithm Details.** For each state \(s\), the inner loop of Algorithm 2 is designed to solve a matrix game with payoff matrices \(^{1}(v^{1}_{t})(s)\) and \(^{2}(v^{2}_{t})(s)\) for each state \(s\), which reduces to Algorithm 1 when (1) the stochastic game has only one state, and (2) \(v^{1}_{t}=v^{2}_{t}=0\). However, in general, since \(v^{1}_{t}\) and \(v^{2}_{t}\) are _independently_ maintained by players \(1\) and \(2\), the quantity

\[^{1}(v^{1}_{t})(s,a^{1},a^{2})+^{2}(v^{2}_{t})(s,a^{2},a ^{1})=_{s^{}}p(s^{} s,a^{1},a^{2})(v^{1}_{t}(s^{ })+v^{2}_{t}(s^{}))\]

is in general _non-zero_ during learning. As a result, the auxiliary matrix game (with payoff matrices \(^{1}(v^{1}_{t})(s)\) and \(^{2}(v^{2}_{t})(s)\)) at state \(s\) that the inner loop of Algorithm 2 is designed to solve is not necessarily a zero-sum matrix game, which presents a major challenge in the finite-sample analysis, as illustrated previously in Section 1.2.

The outer loop of Algorithm 2 is an "on-policy" variant of minimax value iteration. To see this, note that, ideally, we would synchronize \(v^{i}_{t+1}(s)\) with \(^{i}_{t,K}(s)^{}^{i}(v^{i}_{t})(s)^{-i}_{t,K}(s)\), which is an approximation of \([^{i}(v)](s)=^{i}(^{i}(v^{i}_{t})(s))\) by design of our inner loop. However, player \(i\) has no access to \(^{-i}_{t,K}\) in independent learning. Fortunately, the \(q\)-function \(q^{i}_{t,K}\) is precisely constructed as an _estimate_ of \(^{i}(v^{i}_{t})(s)^{-i}_{t,K}(s)\), as illustrated in Section 2.1, which leads to the outer loop of Algorithm 2. In Algorithm 2 Line \(8\), we set \(S_{0}=S_{K}\) to ensure that the initial state of the next inner loop is the last state of the previous one; hence Algorithm 2 is driven by a single trajectory of Markovian samples.

### Finite-Sample Analysis

We now state our main results, which, to the best of our knowledge, provide the first last-iterate finite-sample bound for best-response-type payoff-based independent learning dynamics in zero-sum stochastic games. Our results are based on the following assumption.

**Assumption 3.1**.: There exists a joint policy \(_{b}=(_{b}^{1},_{b}^{2})\) such that the Markov chain \(\{S_{k}\}_{k 0}\) induced by \(_{b}\) is irreducible and aperiodic.

One challenge in our finite-sample analysis is that the behavior policies used for taking the actions are time-varying, due to the best-response nature of the dynamics. Most, if not all, existing finite-sample guarantees of RL algorithms under time-varying behavior policies assume that the induced Markov chain of any policy, or any policy encountered along the algorithm trajectory, is uniformly geometrically ergodic . Assumption 3.1 is weaker, since it assumes only the existence of one policy that induces an irreducible and aperiodic Markov chain.

We consider using either constant stepsizes, i.e., \(_{k}\) and \(_{k}=c_{,}\), or diminishing stepsizes of \((1/k)\) decay rate, i.e., \(_{k}=/(k+h)\) and \(_{k}=/(k+h)=c_{,}/(k+h)\), where \(c_{,}(0,1)\) is the stepsize ratio. In the stochastic-game setting, we redefine \(_{}=[1+(A_{}-1)(2/[(1-)])]^{-1}\), which, analogous to the matrix-game setting, is a uniform lower bound on the entries of the policies generated by Algorithm 2 (cf. Lemma D.1). We next state our requirement for choosing the stepsizes.

**Condition 3.1**.: When using either constant or diminishing stepsizes, we choose \( 1/(1-)\) and the stepsize ratio \(c_{,}\) to satisfy \(c_{,}||A_{}},(1-)^{2}}{34||A_{}^{2}},_{ }^{2}^{3}(1-)^{2}}{144A_{}^{2}}\), where \(c_{}_{}\) and \(L_{p}>0\) are defined in Appendix B.3. In addition, when using \(_{k}\) and \(_{k}\), we require 2\(<1/c_{}\) and \(<1\), and when using \(_{k}=/(k+h)\) and \(_{k}=/(k+h)\), we require 2\(=4\), \(>1/c_{}\), and \(h>1\) such that \(_{0}<1/c_{}\) and \(_{0}<1\).

We next state the finite-sample bound of Algorithm 2. For simplicity of presentation, we use \(a b\) to mean that there exists an _absolute_ constant \(c>0\) such that \(a bc\).

**Theorem 3.1**.: _Suppose that both players follow Algorithm 2, Assumption 3.1 is satisfied, and the stepsizes \(\{_{k}\}\) and \(\{_{k}\}\) satisfy Condition 3.1. Then, we have the following results._

1. _When using constant stepsizes, there exists_ \(z_{}=((1/))\) _such that the following inequality holds as long as_ \(K z_{}\)_:_ \[[(_{T,K}^{1},_{T,K}^{2})] ^{2}T}{(1-)^{3}}()^{T-1}}_{:=_{1}}+^{2}L_{ }(K-z_{})^{1/2}}{(1-)^{4}}(1- )^{-1}{2}}}_{:=_{2}}\] \[+|A_{}}{(1-)^{4}c_{,}}z_{}^{2}^{1/2}}_{:=_{3}}+)}{(1-)^{2}}}_{:=_{4}},\] _where_ \(L_{}=+2(A_{})+|A _{}}{(1-)^{2}}\)_._
2. _When using_ \(_{k}=/(k+h)\) _and_ \(_{k}=/(k+h)\)_, there exists_ \(k_{0}>0\) _such that the following inequality holds as long as_ \(K k_{0}\)_:_ \[[(_{T,K}^{1},_{T,K}^{2})]^{2}T} {(1-)^{3}}()^{T-1}+}| |A_{}z_{K}^{2}_{K}^{1/2}}{(1-)^{4}_{k_{0}}^{1 /2}c_{,}}+)}{(1-)^{2}},\] _where_ \(z_{K}=((K))\)_._

_Remark_.: Analogous to , our learning dynamics are symmetric between the two players in the sense that there is no time-scale separation between the two players, that is, they both implement the algorithm with the same stepsizes.

A detailed proof sketch of Theorem 3.1 is provided in Appendix B and the complete proof is provided in Appendix D. Next, we discuss the result in Theorem 3.1 (1). The bound in Theorem 3.1 (1) involves a value iteration error term \(_{1}\), an optimization error term \(_{2}\), a statistical error term \(_{3}\), and a smoothing bias term \(_{4}\) due to the use of smoothed best-response in the learning dynamics. Note that \(_{1}\) would be the only error term if we were able to perform minimax value iteration to solvethe game. Since minimax value iteration converges geometrically, the term \(_{1}\) also goes to zero at a geometric rate. Notably, the terms \(_{2}\) and \(_{3}\) are orderwise larger compared to their matrix-game counterparts, see Corollary 2.1.2. Intuitively, the reason is that the induced auxiliary matrix game (with payoff matrices \(^{1}(v_{t}^{1})(s)\) and \(^{2}(v_{t}^{2})(s)\)) that the inner loop of Algorithm 2 aims at solving does not necessarily have a zero-sum structure (see the discussion in Section 3.1 after Algorithm 2). Consequently, the error due to such a "non-zero-sum" structure propagates through the algorithm and eventually undermines the convergence bound.

Recall that in the matrix game setting, we proved convergence to the Nash distribution (or the Nash equilibrium of the entropy-regularized matrix game). In the stochastic-game setting, we do not have convergence to the Nash equilibrium of the entropy-regularized stochastic game. The main reason is that, in order to have such a convergence, our outer loop should be designed to approximate the _entropy-regularized_ minimax value iteration rather than the vanilla minimax value iteration as in Algorithm 2 Line \(8\). However, in the payoff-based setting, since each player does not even observe the actions of their opponent, it is unclear how to construct an estimator of the entropy function of the opponent's policy, which is an interesting future direction to investigate.

Although the transient terms in Theorem 3.1 enjoy a desirable rate of convergence (e.g., geometric in \(T\) and \(}(1/K^{1/2})\) in \(K\)), the stepsize ratio \(c_{,}\) (which is exponentially small in \(\)) appears as \(c_{,}^{-1}\) in the bound; see Theorem 3.1. Therefore, due to the presence of the smoothing bias (i.e., the term \(_{4}\) on the RHS of the bound in Theorem 3.1 (1)), to achieve \([(_{T,K}^{1},_{T,K}^{2})]\), the overall sample complexity can also be exponentially large in \(^{-1}\). This is analogous to Corollary 2.1.2 for zero-sum matrix games. As illustrated in detail in Section 2, the reason here is due to the exploration limitation of using softmax as a means for smoothed best response, which we kept without further modification to preserve the naturalness of the learning dynamics. Removing such exponential factors by developing improved exploration strategies is an immediate future direction.

Finally, we consider the case where the opponent of player \(i\) (where \(i\{1,2\}\)) plays the game with a stationary policy and provide a finite-sample bound for player \(i\) to find the best response.

**Corollary 3.1.1**.: _[Rationality3] Given \(i\{1,2\}\), suppose that player \(i\) follows the learning dynamics presented in Algorithm 2, but its opponent player \(-i\) follows a stationary policy, denoted by \(^{-i}\). Then, we have \(_{^{i}}U^{i}(^{i},^{-i})-[U^{i}(_{T,K }^{i},^{-i})]}_{1}T( {2})^{T}+^{2}}{K^{1/2}}+\), where \(_{1}\) and \(_{2}\) are constants that are exponential in \(^{-1}\), but polynomial in \(||\), \(A_{}\), and \(1/(1-)\)._

Intuitively, the reason that our algorithm is rational is that it performs an _on-policy_ update in RL. In contrast to an off-policy update, where the behavior policy can be arbitrarily different from the policy being generated during learning (such as in \(Q\)-learning ), in the on-policy update for games, each player is actually playing with the policy that is moving towards the best response to its opponent. As a result, when the opponent's policy is stationary, it reduces to a single-agent problem and the player naturally finds the best response (also up to a smoothing bias). This is an advantage of using symmetric and independent learning dynamics. One challenge of analyzing such on-policy learning dynamics is that the behavior policy is time-varying.

## 4 Conclusion and Future Directions

In this work, we consider payoff-based independent learning for zero-sum matrix games and stochastic games. In both settings, we establish the last-iterate finite-sample guarantees. Our approach, i.e., the coupled Lyapunov drift argument, provides a number of tools that are likely to be of interest more broadly for dealing with iterative algorithms with multiple sets of coupled and stochastic iterates.

**Limitations and Future Directions.** As mentioned before Corollary 2.1 and after Theorem 3.1, the convergence bounds involve constants that are exponential in \(^{-1}\), which arise due to the use of the smoothed best response to preserve the naturalness of the learning dynamics. An immediate future direction of this work is to remove such exponential factors by designing better exploration strategies. In the long term, we are interested to see if the algorithmic ideas and the analysis techniques developed in this work can be used to study other classes of games beyond zero-sum stochastic games.