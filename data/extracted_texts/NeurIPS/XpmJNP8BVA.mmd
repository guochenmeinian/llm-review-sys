# Regularized Behavior Cloning for

Blocking the Leakage of Past Action Information

Seokin Seo\({}^{1}\), HyeongJoo Hwang\({}^{1}\), Hongseok Yang\({}^{1,2}\), Kee-Eung Kim\({}^{1,2}\)

\({}^{1}\)Kim Jaechul Graduate School of AI, KAIST

\({}^{2}\)School of Computing, KAIST

siseo@ai.kaist.ac.kr, hjhwang@ai.kaist.ac.kr,

hongseok.yang@kaist.ac.kr, kekim@kaist.ac.kr

###### Abstract

For partially observable environments, imitation learning with observation histories (ILOH) assumes that control-relevant information is sufficiently captured in the observation histories for imitating the expert actions. In the offline setting where the agent is required to learn to imitate without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet effective method for imitation learning. However, when the information about the actions executed in the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own past actions. In this paper, we address this catastrophic failure by proposing a principled regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind our approach is to leverage the classical notion of conditional independence to mitigate the leakage. We compare different instances of our framework with natural choices of conditional independence metric and its estimator. The result of our comparison advocates the use of a particular kernel-based estimator for the conditional independence metric. We conduct an extensive set of experiments on benchmark datasets in order to assess the effectiveness of our regularization method. The experimental results show that our method significantly outperforms prior related approaches, highlighting its potential to successfully imitate expert actions when the past action information leaks into the observation histories.

## 1 Introduction

Imitation learning (IL) aims at learning a policy that recovers an expert's behavior from a demonstration dataset. Leveraging the information about state and expert action available in the dataset, IL has been successful in many real-world applications . Although IL problems can be addressed using either online  or offline algorithms , real-world tasks often impose restrictions on interacting with the environment due to safety, cost, and ethical concerns. Consequently, the practical necessity lies in the development of effective offline IL algorithms. Behavior cloning (BC)  is one of the most prominent offline IL algorithms, which learns to predict an expert's action for each given state using an offline expert dataset via supervised learning. BC has gained widespread recognition  for providing a straightforward and effective solution, especially when full access to state information is available and the dataset is sufficiently extensive. However, when the state information is only partially available in an observation at each timestep, which is closer to the realistic scenario (e.g. an autonomous vehicle with a limited numberof sensors), training with any IL algorithm can be complicated. To enhance the ability of agent to infer missing control-relevant information from observations, incorporating the history of observations from adjacent past timesteps as input can be beneficial [2; 24].

When utilizing observation history for behavior cloning, a notable challenge emerges: the unnecessary dependence of the imitator's actions on the preceding actions, resulting in a suboptimal behavior in test time. In extreme cases, this dependency can lead to undesirable behaviors, such as the imitation policy that merely replicates its own actions from previous time steps 1, which leads to a catastrophic result particularly in safety-critical tasks like autonomous driving. This is often rooted at the inability to differentiate between actual and spurious causal relationship between observation features and expert actions within the collected data . Past action information is a representative instance of such nuisance features, since it is a strongly correlated to the target expert action. This correlation can mislead the IL algorithm into recognizing past action information, which is not causally related to the target expert action, as a crucial feature for prediction. In this work, we refer to this misleading phenomenon as past action information leakage.

A natural way to mitigate this phenomenon is to adopt a mechanism that blocks the leakage of past action information irrelevant to the target expert action. In fact, Wen et al.  proposed an adversarial training approach to discard such redundant information in the representation of the observation history. This is achieved by maximizing the conditional entropy of the expert's previous action given the joint of the representation and target expert action. However, due to the intractability of direct computation of the conditional entropy, this approach relies on unstable adversarial learning. More recently, Chuang et al.  focuses on image-based observations and attempts to mitigate the past action dependency by splitting the policy representation into two parts: a representation of the observation history and a representation of the current observation. This method assumes a particular policy structure and does not provide a systematic general approach to block the leakage of past action information into the representation.

In this paper, we present a regularized behavior cloning framework that effectively mitigates the leakage of past action information. We formally define the problem of the leakage of past action information and establish a metric to quantify the magnitude of the leakage, employing the kernel-based method called HSCIC (Hilbert-Schmidt Conditional Independence Criterion) . Building upon the metric, we devise an objective function that addresses the aforementioned problem, allowing for a more comprehensive understanding and interpretation of existing work . Moreover, we propose a stable and efficient kernel-based regularization method that circumvents challenges such as adversarial learning, nested optimization and reliance on a neural estimator. Lastly, we conduct an extensive set of experiments which empirically show that our regularization method effectively blocks the leakage of past action information across a variety of control task benchmarks.

Our contributions are summarized as follows:

* We formally define the problem of the leakage of past action information based on the concept of conditional independence by quantifying the amount of leaked past action information.
* We introduce a principled framework for a behavior cloning with Past Action Leakage Regularization (PALR), which prevents the imitator from overfitting to leaked past information.
* We provide experimental results on established benchmarks, demonstrating the effectiveness of our method.

## 2 Related Work

Invariant representation learningLearning representation invariant to any unwanted factors has been widely studied in various domains such as fair classification [22; 25; 37; 47], domain adaptation [11; 13; 18; 49], and imitation learning [6; 44]. One of the dominant approaches in invariant representation learning is adversarial learning [11; 13; 44; 47]. Adversarial learning algorithmscommonly train additional networks that predict the unwanted factors from the representations while enforcing representations to make those prediction models fail. Consequently, they require alternating optimization between the main and the additional models [13; 44; 47] or show numerical instability . To bypass those shortcomings of adversarial learning, several information-theoretic approaches [18; 25] have been proposed. Based on Variational Auto-Encoders (VAE) , these methods proposed end-to-end learning algorithms that jointly optimize all of their components with numerical stability. However, these methods assume that the distribution of the representation is Gaussian, which restricts the flexibility of the representation. Wen et al.  concentrated on eliminating shortcuts in supervised learning by incorporating supplementary key information, demonstrating its enhancement of behavior cloning with observation histories. Recent methods have proposed to learn counterfactually or conditionally invariant representation, leveraging on kernel-based conditional independence metric [31; 35]. These methods demonstrated promising empirical results in synthetic domains, effectively mitigating the impact of nuisance correlations.

Information leakage in imitation learningThere is a growing understanding of the importance of addressing the correlation between expert actions and nuisance features, which are not essential for control and may even hinder performance. [8; 29; 39] Especially, termed as causal confusion , imitation learning exhibits a paradoxical phenomenon: having more information can lead to worse performance. Aligning with such observation, recent works have also demonstrated that accessing more information from the observation history leaks past action information so that the imitator may learn an undesirable policy that simply repeats the same action in the past [6; 7; 40; 44; 45]. To avoid learning degenerate solutions from the past action information, Wen et al.  proposed the regularization method based on the conditional entropy of the previous action given the representation of the history and the current action. However, their method involves a nested minimax optimization along with an additional neural network, which complicates the training process. We closely investigate their formulation in Section 4.2.1 to show their limitations as well as their connection to our method. Wen et al.  also introduced the action predictability metric to quantify the dependence between past action histories and imitator actions, relative to expert actions. However, this metric is not based on conditional independence, which is central to our argument concerning the past action leakage. Wen et al.  addressed this problem using a weighted behavior cloning method that upweights "keyframe" samples, which are more likely to be predicted from the action histories. Swamy et al.  showed that online interaction is both necessary and sufficient to resolve repeating behavior. However, in many real-world applications [4; 12; 26; 27; 32; 38; 43], online interaction is often infeasible due to safety, cost, and ethical considerations. In this work, we develop an offline algorithm that can robustly handle the leakage of past action information.

## 3 Preliminaries

### Conditional independence

For random variables \(X,Y\) which taking a value in \(,\) respectively, we write \(P_{X}\), \(P_{XY}\), and \(P_{X|Y}\) for the marginal distribution of \(X\), the joint distribution of \(X\) and \(Y\), and the conditional distribution of \(X\) given \(Y\), respectively. We say that random variables \(X\) and \(Y\) are conditionally independent given a random variable \(Z\) or simply \(Z\)-conditionally independent if \(P_{XY|Z}(x,y)=P_{X|Z}(x)P_{Y|Z}(y)\) for all \(x,y\). Also, we say that \(X\) and \(Y\) are independent if \(P_{XY}(x,y)=P_{X}(x)P_{Y}(y)\) for all \(x,y\). We denote \(Z\)-conditional independence by \(X\!\!\! Y Z\), and independence by \(X\!\!\! Y\).

Conditional Mutual Information (CMI)Mutual information (MI) is an information-theoretic quantity to measure the dependency between two random variables. MI between random variables \(X\) and \(Y\) is defined as \(I(X;Y)=D_{}(P_{XY},P_{X}P_{Y})=_{x,y P_{XY}}[ P_{ XY}(x,y)- P_{X}(x)P_{Y}(y)]\). MI is always non-negative (i.e., \(I(X;Y) 0\)), and it becomes zero if and only if \(X\!\!\! Y\). Conditional mutual information (CMI) is defined similarly, but with conditional distributions. CMI between random variables \(X\) and \(Y\) given a random variable \(Z\) is defined as \(I(X;Y Z)=_{P_{Z}}[D_{}(P_{XY|Z},P_{X|Z}P_{Y|Z})]\). As in the case of MI, \(I(X;Y Z) 0\), and also \(I(X;Y Z)=0\) if and only if \(X\!\!\! Y Z\).

Hilbert-Schmidt Conditional Independence Criterion (HSCIC)Hilbert-Schmidt Independence Criterion (HSIC)  is a kernel-based measure that quantifies the dependency between two random variables. Let \(_{}\) and \(_{}\) be reproducing kernel Hilbert spaces (RKHS) on \(\) and \(\) with the corresponding reproducing kernels \(k_{}\) and \(k_{}\), respectively. The tensor product \(k_{} k_{}\) of the kernels is a binary function on \(\) defined by \((k_{} k_{})((x_{1},y_{1}),(x_{2},y_{2})):=k_{ }(x_{1},x_{2})k_{}(y_{1},y_{2})\). The RKHS on \(\) with the kernel \(k_{} k_{}\) is called the tensor product RKHS of \(_{}\) and \(_{}\), and it is denoted by \(_{}_{}\). HSIC between \(X\) and \(Y\) is defined to be the maximum mean discrepancy (MMD) between \(P_{XY}\) and \(P_{X}P_{Y}\) under \(_{}_{}\), i.e, the distance in the Hilbert space \(_{}_{}\) between the so-called kernel mean embeddings of \(P_{XY}\) and \(P_{X}P_{Y}\) to that space:

\[(X,Y) :=^{2}(P_{XY},P_{X}P_{Y};_{} _{})\] \[=\|_{P_{XY}}-_{P_{X}}_{P_{Y}}\|^{2}_{ _{}_{}}\]

where \(_{P_{XY}}\) is the kernel mean embedding of the distribution \(P_{XY}\) to \(_{}_{}\), i.e. \(_{P_{XY}}(x,y):=_{P_{X}}[(k_{} k_{ })((X,Y),(x,y))]\), and \(_{P_{X}}\), \(_{P_{Y}}\) are defined similarly but using \(_{}\), \(_{}\) instead. The \((_{P_{X}}_{P_{Y}})\) is simply the function in \(_{}_{}\) that maps \((x,y)\) to \(_{P_{X}}(x)_{P_{Y}}(y)\). When \(k_{} k_{}\) satisfies some condition (i.e., characteristic), \((X,Y)=0\) if and only if \(X\!\!\! Y\).

HSCIC (Hilbert-Schmidt Conditional Independence Criterion) is an extension of HSIC that quantifies the amount of conditional dependency between two random variables given another random variable. In this paper, we follow the definition of HSCIC based on conditional mean embedding , and use an estimator of HSCIC that draws samples from the joint distribution \(P_{XYZ}\) and performs vector-valued RKHS regression. The conditional mean embedding of \(X\) given \(Z\) is a function in the RKHS \(_{}\) that is parameterized by the value of \(Z\). Given a value \(z\) of \(Z\), it maps \(x\) to \(_{P_{X Z=x}}(x):=_{P_{X Z=z}}[k_{}(X,x)]\). Then, HSCIC is a mapping from a value of \(Z\) to a non-negative real defined as follows: for all \(z\),

\[(X,Y|Z=z):=\|_{P_{XY Z=z}}-_{P_{X Z=z}}_ {P_{Y Z=z}}\|^{2}_{_{}_{}}\]

When \(k_{} k_{}\) satisfies the condition mentioned from above, \(X\!\!\! Y Z\) if and only if \(_{P_{Z}}[(X,Y|Z)]=0\). We use an empirical estimator of the expectation here, denoted by \(}(X,Y|Z)\). See Section A in the supplementary material for more details for definition of HSCIC and its empirical estimator.

### Imitation learning from observation histories

We consider a Partially Observable Markov Decision Process (POMDP)  without reward, which is defined as a tuple of \(,,,O,P,_{0}\). Here \(\), \(\) and \(\) are an (underlying) state space, an observation space and an action space, respectively. The next \(O:\) specifies the conditional probability \(O(z|s)\) of an observation \(z\) given a state \(s\). Finally, \(_{0} S\) defines the probability \(_{0}(s_{0})\) that the process starts from \(s_{0}\), and \(P:\) defines the probability \(P(s^{}|s,a)\) of transitioning to state \(s^{}\) when action \(a\) is performed in state \(s\).

Assume a (stochastic) expert policy \(^{E}:\) that is defined as a conditional probability \(^{E}(a|s)\) of the expert's performing an action \(a\) given a state \(s\). Also, assume that we have a dataset \(=\{^{(1)},,^{(N)}\}\) of the expert trajectories where each \(=\{(z_{t},a_{t})\}_{t=0}^{T}\) is sampled by

\[s_{0}_{0},\ a_{t}^{E}^{E}(|s_{t}),\ z_{t} O(|s_{t }),\ s_{t+1} P(|s_{t},a_{t})\ \ \ t\{0,1,...,T\}.\]

That is, \(\) in the dataset is drawn from the following joint distribution of all observations and actions:

\[ p_{D}(z_{0:T},a_{0:T}^{E})=_{0}(s_{0})_{t=0}^{T}O(z_{t}|s _{t})^{E}(a_{t}^{E}|s_{t})P(s_{t+1}|s_{t},a_{t}^{E})ds_{0:T+1}.\]

In our study, we consider POMDP scenarios where an ideal imitator policy is able to match the expert policy's performance, even when the imitator's actions are solely determined by observation histories. Specifically, we focus on situations where the observation histories \(z_{t-w-1:t}\) (for some fixed \(1 w T-1\)) encompass all information about the true states \(s_{t}\) utilized by the expert policy.

In this setting, our goal is to learn an imitator policy \(^{I}:^{w}\) that acts as closely to \(^{E}\) as possible on the given dataset \(\). To achieve this goal, we consider the following joint distribution of actions of both \(^{I}\) and \(^{E}\):

\[p(a^{I}_{0:T},a^{E}_{0:T})=\\ _{0}(s_{0})_{t=0}^{T}O(z_{t}|s_{t})^{I}(a^{I}_{t} |z_{t-w+1:t})^{E}(a^{E}_{t}|s_{t})P(s_{t+1}|s_{t},a^{E}_{t})ds_{0:T+1}dz_{0: T+1}.\] (1)

## 4 Behavior Cloning with Past Action Leakage Regularization

In this section, we propose a framework that effectively mitigates the past action leakage problem in IL. We first define the problem by formalizing the absence of leaked past-action information via conditional independence (Section 4.1). Then, we compare several regularization-based approaches that attempt to achieve the absence of such information in the context of offline IL (Section 4.2). These approaches performs regularized BC where the regularizer is derived from a metric for measuring the amount of conditional dependence among random variables. The choice of the metric differentiates these approaches, and our comparison advocates the use of the HSCIC-based approach.

### Past action leakage problem in imitation learning

When a policy takes an observation history as an input, the input history may include the information about past actions unexpectedly. The inclusion of such information can have detrimental effects in the context of offline imitation learning by confusing the imitator and making it fail to predict expert actions accurately. Intuitively, the past action leakage problem in imitation learning refers to this failure of the imitator due to the leakage of such harmful past action information.

To express this intuition formally, for each timestep \(t\), let \(A^{E}_{t}\) and \(A^{I}_{t}\) denote random variables of expert action and imitator action at timestep \(t\). Note that for each \(0<t T\), the joint distribution of the three random variables \(A^{E}_{t-1}\), \(A^{E}_{t}\) and \(A^{I}_{t}\) is

\[p(a^{E}_{t-1},a^{E}_{t},a^{I}_{t})= p(a^{I}_{0:T},a^{E}_{0:T})da^{E}_{0:t- 2}da^{I}_{0:t-1}da^{E}_{t+1:T}da^{I}_{t+1:T}\]

where \(p(a^{I}_{0:T},a^{E}_{0:T})\) is the distribution in Eq. (1).

We formalize the absence of the leakage of harmful past-action information by conditional independence between the imitator's current actions and the expert's previous actions:

\[A^{I}_{t}\!\!\! A^{E}_{t-1} A^{E}_{t}0<t T.\] (2)

This conditional independence says that the imitator's current action never depends on some information that is only about the expert's past action but not about the expert's current action. This past-specific information corresponds to harmful information in our intuitive explanation from above.

Ideally we would like to achieve conditional independence in Eq. (2), which ensures the absence of leaked past-action information that was harmful to the imitator. However, in practice, we can achieve it only approximately, so that we need a quantitative measure for conditional independence or the lack of conditional independence between \(A^{E}_{t-1}\) and \(A^{E}_{t}\) given \(A^{I}_{t}\). Such a measure is also needed to design an offline IL algorithm that does not suffer from such leaked harmful past-action information. In the following subsection, we consider two quantitative measures for the lack of conditional independence, namely, (1) conditional mutual information (CMI) and (2) Hilbert-Schmidt Conditional Independence Criterion (HSCIC).

### Behavior cloning with past action leakage regularization

We aim to learn the representation \(_{t}\) of observation history \(z_{t-w+1:t}\) that removes any unnecessary information on the past action. To simplify the notation, let \(t_{w}\) denote \(t-w+1\). Our method is based on the following observation.

**Theorem 1**.: _Let \(A_{t}^{I}\) be the action from the imitator policy \(^{I}(a_{t}|_{t})\) based on the representation \(_{t}\) of observation history. Then, \(_{t}\!\!\! A_{t-1}^{E} A_{t}^{E} A_{t}^{I} \!\!\! A_{t-1}^{E} A_{t}^{E}\)._

Proof.: See Section B in the supplementary material. 

To this end, we formulate our objective of regularized BC framework as follows:

\[(,;,):=_{ bc}(, ;)+_{ reg}(;),\] (3)

where \(_{ bc}\) is an BC objective such that \(_{ bc}(,;):=_{(z_{t\!\!\! },a_{t}^{E}),_{t}(z_{t\!\!\!})}[- (a_{t}^{E}|_{t})]\) and \(_{ reg}\) is a past action leakage regularization objective. For notational simplicity, we abbreviate the expectation with respect to \((a_{t-1}^{E},z_{t\!\!\!},a_{t}^{E}),_{t} (z_{t\!\!\!},a_{t}^{E})\) to \(_{_{t}}\). In the following subsections, we discuss candidates for \(_{ reg}\).

#### 4.2.1 Information-theoretic regularization

A straightforward way to address the conditional independence we discussed in Section 4.1 is minimizing the CMI, which can be decomposed by its definition into two conditional entropy terms:

\[I(a_{t-1}^{E};_{t} a_{t}^{E})=H(a_{t-1}^{E} a_{t}^{E})-H(a_{t-1} ^{E}_{t},a_{t}^{E}).\]

It is important to note that \(H(a_{t-1}^{E} a_{t}^{E})\) is determined by the data distribution \(\) and thus constant. As a result, we can simply consider the minimization of \(-H(a_{t-1}^{E}_{t},a_{t}^{E})\).

\[_{ reg-Ent}(_{t};a_{t-1}^{E},a_{t}^{E}):=-H(a_{t-1}^{E} _{t},a_{t}^{E})=[ p(a_{t-1}^{E}_{t}, a_{t}^{E})].\] (4)

Interestingly, Eq. (4) coincides with the negative conditional entropy maximization objective in FCA . Since the direct computation of Eq. (4) requires to know the intractable distribution \(p(a_{t-1}^{E}_{t},a_{t}^{E})\), FCA trains an additional prediction model \((_{t-1}_{t},a_{t})\) to estimate the negative entropy with \(-(a_{t-1}^{E}_{t},a_{t}^{E}):=_{_{t}}[ (a_{t-1}^{E}_{t},a_{t}^{E})]\). However, this approach faces the following challenges:

1. The estimated negative entropy \(-\) lower bounds Eq. (4), while an upper bound would be desirable for minimizing the objective. Consequently, it imposes nested (minimax) optimization; minimizing with respect to \(_{t}\) after maximizing with respect to \(\).
2. It is required to train an additional neural network to model \(\), which consumes additional computational cost.
3. Assuming that the family of variational distribution \(\) to be Gaussian, FCA tries to tighten the lower bound estimation \(-\) with respect to \(\) by minimizing reconstruction error of \(a_{t-1}^{E}\). The assumption restricts the flexibility of \(\) and thus the estimation can be inaccurate.

To avoid inaccurate estimation of the entropy, we can also decompose CMI into two MI terms by the chain rule of MI as an alternative:

\[I(a_{t-1}^{E};_{t} a_{t}^{E})=I(a_{t-1}^{E};_{t},a_{t}^{E})- I(a_{t-1}^{E};a_{t}^{E}),\]

Similar to \(H(a_{t-1}^{E} a_{t}^{E})\), \(I(a_{t-1}^{E};a_{t}^{E})\) can be safely ignored in the regularization. As a result, the regularization is about simply minimizing \(I(a_{t-1}^{E};_{t},a_{t}^{E})\) while ignoring the constant MI term.

\[_{ reg-MI}(_{t};a_{t-1}^{E},a_{t}^{E}):=I(a_{t-1}^{E}; _{t},a_{t}^{E}).\] (5)

Since the direct computation of Eq. (5) requires to know densities of \(a_{t-1}^{E},_{t},a_{t}^{E}\), one needs to train sample-based MI estimators . Thanks to those estimators, the estimated MI can be minimized without confining any distribution (to be Gaussian). However, this approach still has issues similar to FCA such that (1) it lower bounds MI and consequently imposes (nested) minimax optimization and (2) it introduces an additional neural network particularly sensitive to hyperparameters.

#### 4.2.2 HSCIC regularization

To bypass those shortcomings in information-theoretic regularization, we consider HSCIC , a kernel-based conditional independence metric for the past action leakage regularization.

\[_{}(_{t};a_{t-1}^{E},a_{t}^{E}):= (_{t},a_{t-1}^{E}|a_{t}^{E}).\] (6)

Let \(_{},_{}\) be RKHSs over \(,\) and \(k_{},k_{}\) be their associated kernels, where \(\) is the representation space induced by the encoder \(\). Given \(n\) samples \(\{(_{t}(i),a_{t-1}^{E}(i),a_{t}^{E}(i))\}_{i=1}^{n}\) from \(\) and \(\), let \(_{_{t}},_{a_{t-1}^{E}},_{a_{t}^{E}}\) be the \(n n\) kernel matrices where \([_{_{t}}]_{i,j}=k_{}(_{t}(i),_{t}(j)),[ _{a_{t-1}^{E}}]_{i,j}=k_{}(a_{t-1}^{E}(i),a_{t-1}^{E}(j)),[_{a_{t}^{E}}]_{i,j}=k_{}(a_{t}^{E}(i),a_{t}^{E}(j))\). Then, HSCIC estimator for past action leakage regularization can be defined as follows:

\[(_{t},a_{t-1}^{E}|a_{t}^{E}):=  _{a_{t}^{E}}^{}(_{_{t} }_{a_{t-1}^{E}})^{}_{a_{t}^{E}}\] \[-2_{a_{t}^{E}}^{}(_{_{t }}^{}_{a_{t}^{E}}_{a_{t-1}^{E}}^{}_{a_{t}^{E}})\] \[+(_{a_{t}^{E}}^{}_{_{t }}^{}_{a_{t}^{E}})(_{a_{t}^{E}}^{} _{a_{t-1}^{E}}^{}_{a_{t}^{E}}) \] (7)

where \(=(_{a_{t}^{E}}+n)^{-1}\), \(>0\) is a ridge regression coefficient, \(\) is the element-wise matrix multiplication.

Since Eq. (7) can be estimated using the samples from the joint distribution \(p(_{t},a_{t-1}^{E},a_{t}^{E})\), we can directly plug-in HSCIC estimates into the regularization objective. By leveraging this estimator, HSCIC regularization offers several advantages compared to the conditional entropy regularization and MI regularization objectives discussed earlier:

1. Direct computation of the closed-form solution in Eq. (7) allows HSCIC regularization to bypass any nested optimization.
2. HSCIC regularization does not employ any additional deep neural networks that require careful hyperparameters.
3. Since HSCIC is a non-parametric measure, it does not impose any parametric assumption on the data distribution and does not require any density estimation.

These advantages strongly imply that promoting conditional independence via the HSCIC estimator will be more desirable compared to other estimators, thereby improving the overall effectiveness of the regularization. Hence, we propose HSCIC regularization to address the past action leakage problem, which we call PALR.

## 5 Experiment

In this section, we present the experimental results of our approach. Initially, we investigate the correlation between the extent of past action information leakage and BC's performance. Subsequently, we compare our approach with several offline ILOH baseline methods across four continuous control tasks from the MuJoCo simulator : hopper, walker2d, halfcheetah, and ant, as well as one pixel-based autonomous driving task from the CARLA simulator : carla-lane2.

DatasetFor tasks from the MuJoCo simulator, we transformed them into POMDP scenarios for ILOH by excluding specific state variables (such as velocity information). The remaining state variables, like positional information and joint angles, were treated as observation variables at individual timesteps3. We organized these observations into fixed-size stacks to configure eachproblem setting, denoted as [envname]-W[stacksize] with stack sizes \(w\{2,4\}\). For a task from the CARLA simulator, we used pixel images as observations for ILOH. To extract features from these observations, we employed a pretrained ResNet , keeping its parameters fixed during training. In carla-lane task, we stacked the extracted features with a fixed stack size of \(w=3\) and used it as input for the policy. All our experiments utilized expert demonstrations from the D4RL benchmark dataset  to ensure the validity and reliability of our results.

Evaluation MetricFor the performance evaluation of the learned policy, we measure the normalized score that ranges from 0 to 100. To evaluate how much the past action information is leaked into a policy \(\), we measure \(}(a_{t}^{I};a_{t-1}^{E}|a_{t}^{E})\) using the estimator (7) in the held-out dataset. To estimate HSCIC estimator, we fix a ridge regression coefficient \(=10^{-5}\) and all kernels are chosen as Gaussian kernels with the bandwidth \(^{2}=1\).

### Relationship between past action information leakage and performance

To see if the problem of the past action information leakage occurs, we conduct an empirical study using BC from observation histories with stack size \(w=2\). The objective of this experiment is to confirm the correlation between the performance and the degree of past action information leakage, similar to FCA .

Experimental setupTo achieve multiple policies at different levels, we train BC policies for 500K steps with 5 different seeds and 7 different training dataset size \(N\{100,50,30,10,5,3,1 {K}\}\) (the number of observation history-action pairs). Using a held-out dataset composed of 2,000 samples, the degree of past action information leakage is measured by \(}(a_{t}^{I};a_{t-1}^{E}|a_{t}^{E})\) of each policy \(^{I}\).

ResultsTo compare \(}\) of fully trained BC in all tasks, we normalize \(}\) of each policy using the maximum and minimum values in each task. Grouping trained policies by the dataset size, we report the mean value of both the normalized score and the normalized HSCIC estimates in Figure 1. It shows that there are clear negative correlations between the normalized HSCIC and the normalized score in all environments. The result implies that BC from observation histories tends to train a policy in which there is a negative correlation between the conditional dependence related to its action and the performance. This insight suggests a potential opportunity for employing regularization methods that enforce conditional independence.

### Performance Evaluation

Experimental setupWe evaluate the performance of our method and offline baseline methods across 5 environments see effectiveness of our method. We train policies using 5 different algorithms, including our method, for 500K, 1M training steps MuJoCo and CARLA tasks respectively with 5 different seeds.

Baseline methodsWe compare our method with 6 offline ILOH baseline methods: BC, KF , PrimeNet , RAP 4, FCA , MINE . While all of these are commonly based on BC, we specify their differences as follows:

Figure 1: Negative correlation between HSCIC estimates and performance of BC with 7 different dataset sizes.

* **BC** : the standard BC algorithm from observation history, which optimizes an objective without any regularization term (\(=0\)).
* **KF** : the weighted BC algorithm that assigns higher weights to keyframes, which contain actions that are highly predictable from their corresponding action histories.
* **PrimeNet** : a supervised learning method designed to prevent undesirable shortcuts by leveraging additional key inputs.
* **RAP** : it employs a dual-stream of policy representation that learns from both observation history and individual observations, maximizing a lower bound that enforces conditional dependence between the representation and expert action, given the past action.
* **FCA** : it maximizes the conditional entropy that corresponds to Eq. (4) with adversarial training, which is an instance of our regularized BC framework.
* **MINE** : it minimizes the MI estimate corresponding to Eq. (5) using MINE estimator , which is one of the representative sample-based neural estimators for MI. It is also an instance of our regularized BC framework.

Our method regularizes HSCIC with its estimator defined as Eq. (7), where Gaussian kernel with fixed bandwidth \(^{2}=1\) are used for all kernels of \(_{t},a_{t-1}^{E},a_{t}^{E}\) and a ridge regression coefficient \(=10^{-5}\). Across all regularization methods, we searched for the optimal \(\) according to the best mean normalized score. Further implementation details can be found in Section C.

ResultsTable 1 summarizes the results of performance evaluation for each problem settings. Our method significantly outperforms other baselines in 7 settings out of 9 and shows competitive performance in the rest 2 settings. In particular, our method shows strong performance in carla-lane, highlighting its effectiveness in enhancing performance within high-dimensional offline ILOH scenarios. We observe that RAP shows the least competitive performance across all tasks. This is because they do not have any penalization of the dependence between the imitator action and expert action in their objective function. However, we also observe that FCA and MINE fail to show consistent improvement over BC in most tasks except walker2d. This is because the lower bound estimators of their regularization objectives are not sufficiently accurate even at the cost of their inefficient alternating optimization. On the other hand, our method consistently outperforms BC, which clearly indicates the effect of promoting the conditional independence as we discussed in Section 4.1.

To better understand the degenerate performance of FCA and MINE, we evaluate \(}(a_{t}^{I};a_{t-1}^{E}|a_{t}^{E})\) of each regularization method during training using the held-out dataset in Figure 1(a). In contrast to our method, the result clearly demonstrates that FCA and MINE commonly

 
**Task** & w & **BC** & **KF** & **PrimeNet** & **RAP** & **FCA** & **MINE** & **PARL (Ours)** \\   hopper & 2 & \(32.5 2.9\) & \(32.0 1.9\) & \(30.0 1.6\) & \(20.2 1.4\) & \(31.9 2.5\) & \(25.0 1.9\) & \(\) \\  & 4 & \(47.7 3.4\) & \(45.7 1.0\) & \(45.3 2.8\) & \(32.6 2.6\) & \(36.9 2.4\) & \(37.6 3.1\) & \(\) \\  walker2d & 2 & \(53.0 2.7\) & \(50.0 2.3\) & \(48.5 3.3\) & \(15.8 2.0\) & \(63.1 2.7\) & \(58.6 5.5\) & \(\) \\  & 4 & \(63.2 6.3\) & \(77.4 2.0\) & \(79.2 3.3\) & \(25.4 2.1\) & \(\) & \(68.7 6.7\) & \(\) \\  halfcheetah & 2 & \(74.1 2.3\) & \(64.3 1.4\) & \(61.5 1.9\) & \(63.9 2.1\) & \(78.2 2.8\) & \(76.3 1.9\) & \(\) \\  & 4 & \(68.4 2.6\) & \(55.7 4.1\) & \(45.5 1.7\) & \(59.0 2.7\) & \(69.9 2.6\) & \(73.4 2.4\) & \(\) \\  ant & 2 & \(56.3 3.5\) & \(54.9 1.7\) & \(51.7 2.4\) & \(44.1 1.2\) & \(51.1 2.2\) & \(53.9 1.9\) & \(\) \\  & 4 & \(\) & \(48.6 3.8\) & \(58.2 1.9\) & \(48.6 2.6\) & \(57.7 1.3\) & \(56.6 1.8\) & \(\) \\   carla-lane & 3 & \(52.5 6.2\) & \(66.6 2.1\) & \(58.2 2.2\) & \(25.3 5.4\) & \(57.1 3.1\) & \(60.1 4.1\) & \(\) \\  

Table 1: Performance evaluation of baseline and regularization methods. The normalized scores averaged over the final 50 evaluations during training and we report mean and standard error over 5 different seeds. The rightmost three algorithms are incorporated into our regularization framework. The method with the highest mean score and its competitive methods (within standard error) are highlighted in bold in each problem setting.

fail to promote conditional independence throughout the learning process, elucidating their underperformance. To further investigate, we measure the HSCIC and conditional MI estimates of the finalized imitator policies across 8 MuJoCo settings (refer to Section D.5). In essence, our method adeptly mitigates the leakage of past action information, surpassing the efficacy of alternative approaches.

Regularization coefficientIn this experiment, we aim to ascertain how the selection of coefficient \(\) influences the performance of our method. We evaluate the asymptotic performance of our method with varying values of \(\{0,10^{0},10^{1},10^{2},10^{3},10^{4},10^{5},10^{6}\}\) on the walker2d-W2 problem setting, which shows the largest performance gap between PALR and BC among our problem settings. As depicted in Figure 2b, the converged regularization loss \(_{}\) progressively decreases as \(\) increases. Notably, the \(\) that minimizes the regularization loss is not necessarily equal to the optimal hyperparameter that maximizes performance. We observe that the selection of \(\) is important, as it adjusts the trade-off between robustness to the leakage and alignment with expert data.

## 6 Conclusion and Future Work

Grounded in the classical notion of conditional independence, we proposed a principled regularization framework for BC that mitigates past action information leakage problem. Within our framework, we have explored multiple choices of conditional independence metric and compared their estimators. Finally, we identified that our method with HSCIC estimator is the most favorable regularization of BC over other choices in terms of robustness to the leaked information of past action. In an extensive set of experiments on D4RL datasets, we empirically showed that our method significantly outperforms baseline methods. We also observed in our experiments that all the comparing methods including ours were sensitive to the choice of \(\). Without assuming any interaction with the environment, it is challenging to find the optimal \(\) only with the given offline dataset. We believe that the discovery of optimal \(\) in offline manner would be an interesting research topic, which we leave as future work.