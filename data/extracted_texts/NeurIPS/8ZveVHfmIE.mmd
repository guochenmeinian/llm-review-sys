# On the Convergence of Encoder-only Shallow Transformers

Yongtao Wu

LIONS, EPFL

yongtao.wu@epfl.ch

&Fanghui Liu

University of Warwick

fanghui.liu@warwick.ac.uk

&Grigorios G Chrysos

LIONS, EPFL

University of Wisconsin-Madison

chrysos@wisc.edu

Work done at LIONS, EPFL.

Volkan Cevher

LIONS, EPFL

volkan.cevher@epfl.ch

###### Abstract

In this paper, we aim to build the global convergence theory of encoder-only shallow Transformers under a _realistic_ setting from the perspective of architectures, initialization, and scaling under a finite width regime. The difficulty lies in how to tackle the softmax in self-attention mechanism, the core ingredient of Transformer. In particular, we diagnose the scaling scheme, carefully tackle the input/output of softmax, and prove that quadratic overparameterization is sufficient for global convergence of our shallow Transformers under commonly-used He/LeCun initialization in practice. Besides, neural tangent kernel (NTK) based analysis is also given, which facilitates a comprehensive comparison. Our theory demonstrates the _separation_ on the importance of different scaling schemes and initialization. We believe our results can pave the way for a better understanding of modern Transformers, particularly on training dynamics.

## 1 Introduction

Transformers (Vaswani et al., 2017) have demonstrated unparalleled success in influential applications (Devlin et al., 2019; Brown et al., 2020; Wang et al., 2018; Dosovitskiy et al., 2021; Liu et al., 2022). A fundamental theoretical topic concerns the global convergence, i.e., the training dynamics of Transformers, which would be helpful for further analysis, e.g., in-context learning (von Oswald et al., 2022; Akyurek et al., 2023), generalization (Li et al., 2023). In fact, even within a simplified Transformer framework under certain specific regimes, the global convergence guarantees still remain an elusive challenge.

To theoretically understand this, let us first recall the exact format of the self-attention mechanism, the core ingredient of the Transformer. Given the input \(^{d_{s} d}\) (\(d_{s}\) is the number of tokens and \(d\) is the feature dimension of each token), a self-attention mechanism is defined as:

\[()_{s}(_{0}( _{Q}^{})(_{K}^{})^{})( _{V}^{})=_{s}(_{0}_{Q}^{} _{K}^{})(_{V}^{})\,\]

where \(_{s}\) is the row-wise softmax function, \(_{0}^{+}\) is the scaling factor, and the learnable weights are \(_{Q},_{K},_{V}^{d_{m} d}\) with the width \(d_{m}\). Given \(\), the input of softmax depends on \(_{0}_{Q}^{}_{K}\), including the scaling factor \(_{0}\) and initialization schemes for learnable parameters, and thus determines the output of softmax and then affects the performance of Transformers in boththeory and practice. There are several scaling schemes in previous literature. For instance, given \(_{Q}\) and \(_{K}\) initialized by standard Gaussian, the scaling factor \(_{0}\) is chosen by

* \(_{0}=d_{m}^{-1/2}\) in the original Transformer (Vaswani et al., 2017): each element in \(_{0}_{Q}^{}_{K}\) is a random variable with mean \(0\) and variance \(1\). This scaling avoids the blow-up of value inside softmax as \(d_{m}\) increases (Hron et al., 2020).
* \(_{0}=d_{m}^{-1}\): This scaling stems from the neural tangent kernel (NTK) analysis (Jacot et al., 2018), a commonly used technical tool for convergence analysis of fully-connected (or convolutional) networks under an infinite-width setting \(d_{m}\). However, for Transformer, if one uses this scaling under the infinite-width setting, then by the law of large numbers, we have \(_{d_{m}}_{0}[_{Q}^{}_{K}]^{(ij)}=0\). As a result, the input of softmax is zero and the softmax degenerates to a pooling layer. That means, the non-linearity is missing, which motivates researchers to carefully rethink this setting.

For instance, under the \(_{0}=d_{m}^{-1}\) setting, Yang (2020) use the same query and key matrices to prevent the softmax from degenerating into a pooling layer. Besides, to avoid the analytic difficulty of softmax due to the fact that each element of the output depends on all inputs, Hron et al. (2020) substitute softmax with ReLU under the \(_{0}=d_{m}^{-1/2}\) and infinite width setting for simplicity.

Clearly, there exists a gap between theoretical analysis and practical architectures on the use of softmax, and accordingly, this leads to the following open question:

_How can we ensure the global convergence of Transformers under a realistic setting?_

The primary contribution of this work is to establish the convergence theory of shallow Transformer under a _realistic_ setting. Despite its shallow and encoder-only architecture, our Transformer model captures all the fundamental components found on typical Transformers, including the self-attention mechanism with the softmax activation function, one feedforward ReLU layer, one average pooling layer, and a linear output layer, _cf._ Eq. (1.2). We adopt the \(_{0}=d_{m}^{-1/2}\) scaling under the finite-width setting and compare the results of LeCun/He initializations, which are commonly used in practical applications. Besides, the convergence result under the \(_{0}=d_{m}^{-1}\) setting (as well as the NTK based analysis) is also studied, which facilitates a comprehensive comparison. Our theoretical results demonstrate _notable separations_ among scaling settings, initializations, and architectures as below:

* _Scaling:_ The global convergence can be achieved under both \(_{0}=d_{m}^{-1/2}\) and \(_{0}=d_{m}^{-1}\). Nevertheless, as suggested by our theory: for a small \(d_{m}\), there is no significant difference for these two scaling schemes on the convergence; but for a large enough \(d_{m}\), the \(_{0}=d_{m}^{-1/2}\) scaling admits a faster convergence rate of Transformers than that with \(_{0}=d_{m}^{-1}\). Interestingly, under this \(_{0}=d_{m}^{-1}\) setting, our theory also demonstrates the _separation_ on the convergence result, depending on whether the input is formed along sequence dimension (\(d=1\)) or embedding dimension (\(d_{s}=1\)).
* _Initialization:_ Under LeCun and He initialization, our shallow Transformer admits a faster convergence rate than the NTK initialization. This could be an explanation for the seldom usage of NTK initialization for Transformer training in practice.
* _Architecture:_ Quadratic over-parameterization is enough to ensure the global convergence of our shallow Transformer. As a comparison, if the self-attention mechanism is substituted by a feed-forward ReLU layer, our shallow Transformer is close to a three-layer fully-connected ReLU neural networks to some extent, requiring cubic over-parameterization for global convergence.

We firmly believe that our theoretical analysis takes a significant step towards unraveling the mysteries behind Transformers from the perspective of global convergence. We hope that our analytical framework and insights on various initialization and scaling techniques would be helpful in training modern, large-scale Transformer-based models (Radford et al., 2018; Brown et al., 2020).

## 2 Related work

**Self-attention, Transformer:** Regarding training dynamics, Snell et al. (2021) explain why single-head attention focuses on salient words by analyzing the evolution throughout training. Hron et al. (2020) show that the output of Transformer converges to Gaussian process kernel and provide the NTK formulation of Transformer. Recently, Li et al. (2023) provide sample complexity of shallow Transformer to study its generalization property under a good initialization from pretrained model. The separation between the Transformer and CNN is recently explored. Jelassi et al. (2022) provably demonstrate that Vision Transformer (ViT) has the ability to learn spatial structure without additional inductive bias such as the spatial locality in CNN. Chen et al. (2022) study the loss landscape of ViT and find that ViT converges at sharper local minima than ResNet. Park and Kim (2022) show that ViT is a low-pass filter while CNN is a high-pass filter, thus, these two models can be complementary.

**NTK, lazy training, Hessian:** The NTK was introduced by Jacot et al. (2018) to connect the infinite-width neural network trained by gradient descent and the kernel regression. The roles of such kernel include analysis of the training dynamics of the neural network in the over-parameterization regime (Allen-Zhu et al., 2019; Chizat et al., 2019; Du et al., 2019, 2020; Zou et al., 2020). The global convergence, generalization bound, and memorization capacity largely depend on the minimum eigenvalue of the NTK (Cao and Gu, 2019; Zhu et al., 2022; Nguyen et al., 2021; Bombari et al., 2022). Even though the NTK is extended from FCNN to several typical networks including Transformer (Tirer et al., 2020; Huang et al., 2020; Arora et al., 2019, 2021; Alemohammad et al., 2021; Nguyen and Mondelli, 2020), it has not been used to analyze the global convergence of Transformer. On the other hand, the stability of the tangent kernel during training is required when connecting to kernel regression, but such stability can not be explained by the phenomenon of lazy training (Chizat et al., 2019), which indicates a small change of the parameters from initialization. The hessian spectral bound is the main reason for the stability of kernel, as mentioned in Liu et al. (2020).

**Over-parameterization for convergence analysis:** Due to over-parameterization, neural networks (NNs) can fit arbitrary labels with zero training loss when trained with (stochastic) gradient descent (SGD), both theoretically Li and Liang (2018); Du et al. (2019) and empirically (Zhang et al., 2017). This leads to an interesting question in theory: _how much overparameterization is enough to ensure global convergence of NNs?_ A common recipe for the proof of global convergence relies on the variant of Polyak-Lojasiewicz condition (Polyak, 1963; Liu et al., 2022), NTK (Du et al., 2019, 2020; Zou and Gu, 2019; Allen-Zhu et al., 2019), or the minimum eigenvalue of the gram matrix (Nguyen, 2021; Bombari et al., 2022). In Appendix B.3, we provide a comprehensive overview of a recent line of work that improves the over-parameterization condition for ensuring the convergence of NNs. However, the over-parameterization condition for Transformer to achieve global convergence remains elusive from existing literature and we make an initial step towards this question.

## 3 Problem setting

This section includes the problem setting with notations and model formulation of the shallow Transformer that is studied in this paper.

### Notation

Vectors (matrices) are symbolized by lowercase (uppercase) boldface letters, e.g., \(\), \(\). We use \(\|\|_{}\) and \(\|\|_{2}\) to represent the Frobenius norm and the spectral norm of a matrix, respectively. The Euclidean norm of a vector is symbolized by \(\|\|_{2}\). The superscript with brackets is used to represent the element of a vector/matrix, e.g., \(w^{(i)}\) is the \(i^{}\) element of \(\). The superscript without brackets symbolizes the parameters at different training step, e.g., \(^{t}\). We denote by \([N]=\{1,,N\}\) for short. We use \(_{}()\) and \(_{}()\) to represent the minimum singular value and minimum eigenvalue of a matrix. The NTK matrix and hessian matrix of the network are denoted by \(\) and \(\), respectively. The order notation, e.g., \(}\), \(\), omits the logarithmic factor. More detailed notation can be found in Table 2 of the appendix.

Let \(X^{d_{s} d}\) be a compact metric space and \(Y\), where \(d\) is the dimension of each token, \(d_{s}\) is the total sequence length of the input. The training set \(\{(_{n},y_{n})\}_{n=1}^{N}\) is assumed to be iid sampled from an unknown probability measure on \(X Y\). In this paper, we focus the regression task by employing the squared loss. The goal of our regression task is to find a hypothesis, i.e., a Transformer \(f:X Y\) in our work, such that \(f(;)\) parameterized by \(\) is a good approximation of the label \(y Y\) corresponding to a new sample \( X\). We use a vector \(\) to denote the collection of all learnable parameters.

### Model formulation of shallow Transformer

Throughout this work, we consider the encoder of Transformer, which can be applied to both regression and classification tasks (Yuksel et al., 2019; Dosovitskiy et al., 2021). Given an input \(^{d_{s} d}\), the model is defined as below:

\[_{1} =()_{s}(_{0}( _{Q}^{})(_{K}^{})^{}) (_{V}^{}),\] (1.1) \[_{2} =_{1}_{r}(_{1}_{H})\,,_{3}= (_{2}), f(;)=_{3}^{}_{O}\,,\] (1.2)

where the output is \(f(;)\), \(_{0}\) and \(_{1}\) are two scaling factors. The ingredients of a Transformer with width \(d_{m}\) are defined as follows:

* A _self-attention_ mechanism (Eq. (1.1)): \(_{s}\) is the row-wise softmax function; the learnable parameters are \(_{Q},_{K},_{V}^{d_{m} d}\). We employ Gaussian initialization \(_{Q}^{(ij)}(0,_{Q})\), \(_{K}^{(ij)}(0,_{K})\), \(_{V}^{(ij)}(0,_{V})\) with \(i[d_{m}]\) and \(j[d]\). Refer to Table 1 for typical initialization examples.
* A _feed-forward ReLU_ layer (in Eq. (1.2)): \(_{r}\) is the ReLU activation function; the learnable parameter is \(_{H}^{d_{m} d_{m}}\). Following Yang et al. (2022), we combine \(_{V}\) and \(_{H}\) together (by setting \(_{H}=\) ) for ease of the analysis. Note that it does not mean its training dynamics are the same as the joint-training of these two adjacent matrices.
* An _average pooling_ layer (in Eq. (1.2)): \(\) indicates the column-wise average pooling. Note that the average pooling layer is applied along the sequence length dimension to ensure the final output is a scalar, which is commonly used in practical Vision Transformer or theoretical analysis (Dosovitskiy et al., 2021; Yang, 2020).
* An _output_ layer (in Eq. (1.2)) with learnable parameter \(_{O}^{d_{m}}\), initialized by \(_{O}^{(i)}(0,_{O})\).

**Remarks:** Proper initialization and scaling are required to ensure the convergence and learnability, as seen in previous work (Jacot et al., 2018; Tirer et al., 2022; Lee et al., 2019). For our convergence analysis, we consider standard Gaussian initialization with different variances and different scaling factor that includes three typical initialization schemes in practice. In Table 1, we detail the formula of LeCun initialization, He initialization, and NTK initialization.

Given \(N\) input samples \(\{_{n}\}_{n=1}^{N}\), the corresponding ground truth label, the final output of network, and the output of the last hidden layer, are denoted by:

\[\{y_{n}\}_{n=1}^{N}^{N},() \{f(_{n};)\}_{n=1}^{N}^{N},_{ pre}()\{_{3}(_{n};)\}_{n=1}^{N} ^{N d_{m}}.\]

We consider standard gradient descent (GD) training of Transformer, as illustrated in Algorithm 1. Here the squared loss is expressed as \(()=\|()-\|_{2}^{2}\).

``` Input: data \((_{n},y_{n})_{n=1}^{N}\), step size \(\).  Initialize weights as follows: \(^{0}:=\{_{Q}^{0},_{K}^{0},_{V}^{0},_{O}^{0}\}\). for\(t=0\) to \(t^{}-1\)do \(_{Q}^{t+1}=_{Q}^{t}-_{_{Q}}(^{t})\), \(_{K}^{t+1}=_{K}^{t}-_{_{K}}(^{ t})\), \(_{K}^{t+1}=_{U}^{t}-_{_{V}}(^{ t})\), \(_{O}^{t+1}=_{O}^{t}-_{_{O}}(^{ t})\). endfor Output: the model based on \(^{t^{}}\). ```

**Algorithm 1**Gradient descent training

## 4 Main results

In this section, we study the convergence guarantee of Transformer training by GD under the squared loss. Firstly, we provide a general analytical framework in Section 4.1 covering different initialization

   Init. & \(_{O}\) & \(_{V}\) & \(_{Q}\) & \(_{K}\) & \(_{1}\) \\  LeCun & \(d_{m}^{-1}\) & \(d^{-1}\) & \(d^{-1}\) & \(d^{-1}\) & \(1\) \\ He & \(2d_{m}^{-1}\) & \(2d^{-1}\) & \(2d^{-1}\) & \(2d^{-1}\) & \(1\) \\ NTK & 1 & 1 & 1 & 1 & \(d_{m}^{-1/2}\) \\   

Table 1: Common initialization methods with their variances of Gaussian distribution and scaling factors. The choice of \(_{1}=d_{m}^{-1/2}\) is based on standard NTK initialization on prior literature (Du et al., 2019).

schemes, where we identify the condition for achieving global convergence. Next, we validate these conditions for several practical initialization schemes under \(_{0}=d_{m}^{-1/2}\) in Section 4.2 and \(_{0}=d_{m}^{-1}\) in Section 4.3, respectively. We include NTK-based results for self-completeness. Discussion on the convergence results with different scalings, initializations, and architectures is present in Section 4.4.

### General framework for convergence analysis

Before presenting our result, we introduce a basic assumption.

**Assumption 1**.: _The input data is bounded \(\|\|_{}}C_{x}\) with some positive constant \(C_{x}\)._

**Remark:** This assumption is standard as we can always scale the input. The embedding of token is usually assumed to have a unit norm (Li et al., 2023), which is unrelated to \(d\).

Now we are ready to present our proposition, with the proof deferred to Appendix C.2. Notice that our proposition holds with high probability under some conditions, which depend on certain initialization schemes and scaling factors. Since our proposition is devoted to a unifying analysis framework under various initialization schemes, we do not include specific probabilities here. The validation of the required conditions and probability is deferred to Sections 4.2 and 4.3, respectively.

**Proposition 1**.: _Consider the Transformer with \(d_{m} N\). Let \(C_{Q},C_{K},C_{V},C_{O}\) be some positive constants, and define the following quantities at initialization for simplification:_

\(\) _The norm of the parameters:_

\[_{Q}\|_{Q}^{0}\|_{2}+C_{Q},\ _{K}\|_{K}^{0}\|_{2}+C_{K},\ _{V}\|_{V}^{0}\|_{2}+C_{V},\ _{O}\|_{O}^{0}\|_{2}+C_{O}.\]

\(\) _Two auxiliary terms: \( N^{1/2}d_{s}^{3/2}_{1}C_{x}, z _{O}^{2}(1+4_{0}^{2}C_{x}^{4}d_{s}^{2}_{V}^{2}( _{Q}^{2}+_{K}^{2})).\)_

_Under Assumption 1, we additionally assume that the minimum singular value of \(_{}^{0}\), i.e., \(_{}(_{}^{0})\) satisfies the following condition at initialization:_

\[^{2}  8(_{V}C_{Q}^{-1},_{O} C_{V}^{-1},2_{0}C_{x}^{2}d_{s}_{K}_{V}_{O}C_{Q}^{-1},2_{0}C_{x}^{2}d_{s}_{Q}_{V }_{O}C_{K}^{-1})^{0})}\,,\] (2) \[^{3}  32^{2}z^{0})}/_{O}\,.\] (3)

_If the step size satisfies \( 1/C\) with a constant \(C\) depending on (\(_{Q},_{K},_{V},_{O},( ^{0}),,_{0}\)), then GD converges to a global minimum as follows:_

\[(^{t})(1-}{2})^{t}\ (^{0})\,, t 0\,.\] (4)

**Remark:** The parameter \(\) in Eqs. (2) and (3) controls the convergence rate of global convergence, and the condition will be verified in the next subsection. The step-size \(\) is inversely proportional to \(N^{1/2}\) due to the construction of \(C\) in Appendix C.2.

**Proof sketch:** The main idea of our convergence analysis is based on the variant of Polyak-Lojasiewicz (PL) inequality (Polyak, 1963, Nguyen, 2021), i.e., \(||()||_{2}^{2} 2_{}()() 2_{}(_{}_{}^{}) ()\). Thus, if the minimum singular value of \(_{}\) is strictly greater than 0, then minimizing the gradient on the LHS will drive the loss to zero. To this end, Proposition 1 can be split into two parts. First, by induction, at every time step, each parameter in the Transformer can be bounded w.h.p; the minimum singular value of \(_{}\) is bounded away for some positive quality at the initialization point. Secondly, we prove that the Lipschitzness of the network gradient, which means the loss function is almost smooth. Combining the above two results, the global convergence can be achieved. Furthermore, based on different initialization and scaling schemes, we are able to validate Eqs. (2) and (3) via the spectral norm estimation of \(_{Q}\), \(_{K}\), \(_{V}\), \(_{O}\) and a positive lower bound for \(_{}\) in the following section.

### LeCun and He initialization under the \(d_{m}^{-1/2}\) setting

Here we aim to show that, under the LeCun/He initialization with the setting of \(d_{m}^{-1/2}\), the conditions in Eqs. (2) and (3) will be satisfied with high probability and scaling schemes in Table 1 and hence lead to global convergence. To derive our result, we need the following assumptions on the input data regarding the rank and dissimilarity of data.

**Assumption 2**.: _We assume that the input data \(\) has full row rank._

**Remark:** This assumption requires that each row \(^{(i,:)}\) is linearly independent for any \(i[d_{s}]\), which is fair and attainable in practice. For example, for language tasks, even though there might be some repeated token in \(\), each row in \(\) can be uncorrelated when added with positional embedding. Similarly, in image tasks with Visual Transformer, the raw image is grouped by patch and mapped via a linear layer to construct \(\), thus each row in \(\) can be uncorrelated.

**Assumption 3**.: _For any data pair \((_{n},_{n^{}})\), with \(n n^{}\) and \(n,n^{}[N]\), then we assume that \((|_{n}^{}_{n},_{n^{}}^ {}_{n^{}}| t)(-t^{})\) with some constant \(>0\)._

**Remark:** We discuss the rationale behind this assumption:

The idea behind Assumption 3 is that different data points admit a small similarity. To be specific, for two data points \(_{n}\) and \(_{n^{}}\) with \(n n^{}\), their inner product reflects the similarity of their respective (empirical) covariance matrix. We expect that this similarity is small with a high probability. The spirit of this assumption also exists in previous literature, e.g., Nguyen et al. (2021). The constant \(\) determines the decay of data dissimilarity. A larger \(\) results in less separable data. Our assumption has no requirement on \(\) such that \(\) can be small enough, which allows for a general data distribution.

**Verification of assumption.** Here we experimentally validate this assumption under a standard language IMDB dataset (Maas et al., 2011). We randomly sample 100 (normalized) sentences with embedding and plot the probability (frequency) of \((|_{n}^{}_{n},_{n^{}}^ {}_{n^{}}| t)\) as \(t\) increases. We repeat it over 10 runs and plot the result in Figure 1. We can observe an exponential decay as \(t\) increases, which implies that our assumption is fair. Besides, the validation of Assumption 3 on image data, e.g., MNIST by ViT, is deferred to Appendix E.2.

Now we are ready to present our main theorem under the \(d_{m}^{-1/2}\) setting. The proof is deferred to Appendix C.3.

**Theorem 1**.: _Under the setting of LeCun/He initialization in Table 1 and Assumptions 1 to 3, suppose \(d_{m} d\), and given \(_{0}=d_{m}^{-1/2}\), \(d_{m}=(N^{3})\), then with probability at least \(1-8e^{-d_{m}/2}--(-((N-1)^{-}d_{s}^{-1}))\) for proper \(\), the GD training of Transformer converges to a global minimum with sufficiently small step size \(\) as in Eq. (4)._

**Remark:** The probability relates to several randomness sources, e.g., data sampling, dissimilarity of data, and parameter initialization. The quantity \((-((N-1)^{-}d_{s}^{-1}))\) can be small for a small enough \(\) as discussed in Assumption 3. Further discussion on our result refers to Section 4.4 for details. Besides, our proof framework is also valid for the \(_{0}=d_{m}^{-1}\) setting. We demonstrate that \(d_{m}=(N^{2})\) suffices to achieve global convergence, see Appendix C.3 for details.

**Proof sketch:** To check whether the conditions in Eqs. (2) and (3) hold, the key idea is to provide the lower bound of \(\). Then we upper bound \(_{Q},_{K},_{V},_{O}\) based on concentration inequalities to upper bound the initial loss, one key step is to utilize Gershgorin circle theorem (Gershgorin, 1931) to provide a lower bound for \(\). Lastly, we plug these bound into the condition Eqs. (2) and (3) in order to obtain the requirement for the width \(d_{m}\).

### NTK initialization under the \(d_{m}^{-1}\) setting

The NTK theory, as a representative application of the \(_{0}=d_{m}^{-1}\) setting, can be also used for analysis of training dynamics. We also include the NTK results in this section for self-completeness. In this section, we first derive the limiting NTK formulation of Transformer under the \(d_{m}^{-1}\) scaling scheme, and then show the global convergence of Transformers.

Figure 1: Validation of Asm. 3.

**Lemma 1**.: _Denote \(^{}:=:[}_{1}^{}_{d_{s}},...,}_{N}^{}_{d_{s}}]^{}^{N d}\), then the limiting NTK matrix \(^{N N}\) of Transformer under the NTK initialization with \(_{0}=d_{m}^{-1}\) has the following form:_

\[=d_{s}^{2}_{(,)}(_ {r}(^{})_{r}(^{} )^{})+d_{s}^{2}_{(,)}(_{r}(^{})_{r}(^{})^{})(^{}^{ }).\]

**Remark:** The formulation of \(^{}\) implies that the self-attention layer degenerates as \(}_{d_{s} d_{s}}_{V}^{}\), i.e., the _dimension missing_ effect as mentioned before.

Now we are ready to present our convergence result under the \(d_{m}^{-1}\) scaling with the proof deferred to Appendix C.4.

**Theorem 2**.: _Under the setting of NTK initialization in Table 1 and Assumptions 1 to 3, suppose \(d_{m}=(N)\), then with probability at least \(1-8e^{-d_{m}/2}--(-((N-1)^{-}d_{s}^{-1}))\), the GD training of Transformer converges to a global minimum with sufficiently small \(\) as in Eq. (4)._

**Proof sketch:** The overall idea is the same as the proof of the previous theorem, i.e., we need to provide the lower bound of \(\). However, in this case, the limit for the output of softmax exists so that we can apply concentration inequality to lower bound the \(\). Lastly, we plug these bound into the condition Eqs. (2) and (3) in order to obtain the requirement for the width \(d_{m}\).

Besides, the stability of NTK during training allows us to build a connection on training dynamics between the Transformer (assuming a squared loss) and the kernel regression predictor. Next, in order to show that the NTK is stable during GD training, below we prove that the spectral norm of Hessian is controlled by the width.

**Theorem 3** (Hessian norm is controlled by the width).: _Under Assumption 1 and scaling \(_{0}=d_{m}^{-1}\), given any fixed \(R>0\), and any \(^{t} B(,R):=\{:\|-^ {0}\|_{2} R\}\), \(^{0}\) as the weight at initialization, then with probability at least \(1-8e^{-d_{m}/2}\), the Hessian spectral norm of Transformer satisfies: \(\|(^{t})\|_{2}d_{m}^{-1/2}\)._

**Remark:** By (Liu et al., 2020, Proposition 2.3), the small Hessian norm is a sufficient condition for small change of NTK. Thus, Theorem 3 can be an indicator for the stability of NTK. Besides, Theorem 3 supplements the result in (Park and Kim, 2022) which exhibits empirically a relationship between the Hessian norm and the width but lacks theoretical proof.

### Discussion on convergence results

We compare the derived results under different scaling schemes, initializations, and architectures.

**Comparison of scaling schemes:** The global convergence can be achieved under both \(_{0}=d_{m}^{-1/2}\) and \(_{0}=d_{m}^{-1}\). Nevertheless, as suggested by our theory, for a small \(d_{m}\), there is no significant difference between these two scaling schemes on the convergence; but for a large enough \(d_{m}\), the \(_{0}=d_{m}^{-1/2}\) scaling admits a faster convergence rate of Transformers than that of \(_{0}=d_{m}^{-1}\) due to a more tight estimation of the lower bound of \(\), see Appendix C.6 for details. The intuition behind the lower convergence rate under the setting of large width and \(_{0}=d_{m}^{-1}\) is that the input of softmax is close to zero such that softmax roughly degenerates as a pooling layer, losing the ability to fit data. This can be also explained by Lemma 1 from the perspective of _dimension missing_: self-attention (\(\)) degenerates as \(}_{d_{s} d_{s}}_{V}^{}\).

The result under the \(_{0}=d_{m}^{-1}\) setting requires weaker over-parameterization than the \(_{0}=d_{m}^{-1/2}\) setting. Nevertheless, we do not claim that \(_{0}=d_{m}^{-1}\) is better than \(_{0}=d_{m}^{-1/2}\). This is because, under the over-parameterization regime, the scaling \(_{0}=d_{m}^{-1}\) makes the self-attention layer close to a pooling layer. This analysis loses the ability to capture the key characteristics of the self-attention mechanism in Transformers.

Note that the lower bound of the minimum eigenvalue is in the constant order, which is tight (since it matches the upper bound). Based on this, by studying the relationship between \(d_{m}\) and \(_{0}\), we can prove that quadratic (cubic) over-parameterization is required for \(d_{m}^{-1}\) (\(d_{m}^{-1/2}\)) scaling. This quadratic over-parameterization requirement could be relaxed if a better relationship is given while it is still unclear and beyond our proof technique.

**Comparison on initializations:** Though our results achieve the same convergence rate under these initialization schemes, we can still show the _separation_ on \(\) that affects the convergence in Eq. (4) among these initialization schemes. To be specific, we verify that under LeCun and He initialization, the lower bound of \(^{2}\) is tighter than that of NTK initialization, and hence admits faster convergence, see Appendix C.5 for further details. This can be an explanation of the seldom usage of NTK initialization in practice. Besides, the NTK initialization scheme allows for a larger step size than LeCun/He initialization for training. That means, if \(\) is the same in these three initialization schemes, we usually choose a large step size under the NTK initialization scheme, see Appendix E.1.

**Comparison on architectures:** Note that the Transformer defined in Eq. (1.2) includes a self-attention layer, a feedforward ReLU layer, and an output layer. Our result proves that a cubic (quadratic) over-parameterization condition is required for \(d_{m}^{-1/2}\) (\(d_{m}^{-1}\)) under LeCun initialization. As a comparison, a three-layer fully-connected ReLU network under LeCun initialization requires \(d_{m}=(N^{3})\)(Nguyen, 2021).

The aforementioned result holds for matrix input. Although not as frequent, some data inputs are naturally in vector form. Two ways to feed the input into Transformer are either formulating along sequence dimension (\(d=1\)) or along embedding dimension (\(d_{s}=1\)). The following result shows the separation under these two settings to understand when the Transformer works well or not regarding the input.

**Corollary 1** (Convergence of vector input).: _Consider LeCun initialization with \(_{0}=d_{m}^{-1}\) scaling, given vector input \(^{}\), if one feeds the input to Transformer by setting \(d_{s}=1,d=\), then training with GD can converge to a global minimum. On the contrary, if one sets \(d_{s}=,d=1\), the conditions in Eqs. (2) and (3) do not hold, the convergence can not be guaranteed by our theory._

**Remark:** Such a result is motivated by considering least squares. Specifically, given input \(^{N 1}\), then the NTK \(^{}\) is a rank-one matrix. As a result, when the augmented matrix \([,]\) is not rank-one, then there is no solution to the linear system so that GD training can not converge to zero loss. The empirical validation can be found in Figure 3.

**Technical difficulty.** The technical difficulty of our analysis includes handling the softmax function and scaling schemes beyond NTK initialization. Previous convergence analysis, e.g., (Du et al., 2019, Nguyen, 2021) cannot be applied to our setting because of the following issues. First, different from classical activation functions, e.g., ReLU, in softmax each element of the output depends on all input. To tackle the interplay between dimensions, we build the connection between the output of softmax and \(^{}\) to disentangle the nonlinear softmax function. By doing so, a lower bound on the minimum singular value of \(_{}\) in Proposition. 1 can be well controlled by \(^{}\) and the output of softmax.

Regarding different initializations and scaling, previous NTK-based analysis is only valid under the \(d_{m}^{-1}\) setting (the softmax degenerates to an all-one vector) but is inapplicable to the realistic \(d_{m}^{-1/2}\) setting, as discussed in the introduction. To tackle this issue, we analyze the input/output of softmax under LeCun/He initialization and identify the optimization properties of the loss function for global convergence under the finite-width setting.

## 5 Experimental validations

Our experiments are organized as follows: In Section 5.1, we conduct experiments with the model Eq. (1.2) on synthetic data and study the training dynamics. Next, we show convergence results on ViT (Dosovitskiy et al., 2021) on the standard MNIST dataset in Section 5.2. Additional results and detail on the experimental setup are deferred to Appendix E.

### Fitting synthetic data

In this section, we corroborate our theoretical findings on the synthetic data. We generate \(100\) data points where the input \(^{10 100}\) is sampled from standard Gaussian distribution. The corresponding label \(y\) is generated from standard Gaussian distribution. Squared loss is selected as the criterion. We apply gradient descent on the shallow Transformer defined in Eq. (1.2) with LeCun initialization and \(_{0}=d_{m}^{-1/2}\) for \(400\) epochs with a fixed step size \(=1\). We test different widths of the network including \(d_{m}=\{10,100,1000,4000\}\) and plot the training progress inFigure 2. The result shows that the training can converge except for the case with sub-linear width, see the linear convergence rate in Figure 1(a) and the small movement of the weight in Figure 1(b). For these cases, the weights do not change much after 50 epochs while the losses are still decreasing. In Figure 1(c), we keep track of the evolution of NTK along each epoch. Specifically, the kernel distance is given by:

\[(,})=1-(}^{})}{(^{})} (}}^{})}}\,,\]

which quantitatively compares two kernels by the relative rotation, as used in Fort et al. (2020). Figure 1(c) shows that the kernel changes rapidly at the beginning of training while being approximately constant at later stages. The experiment with \(_{0}=d_{m}^{-1}\), which is deferred to Appendix E.1, obtains similar results.

Additionally, the experiment with other initialization schemes is deferred to Appendix E.1, where we observe that NTK initialization indeed yields slower convergence than LeCun/He initialization, which is consistent with our theoretical finding.

Next, in order to verify corollary 1, we feed vector input \(_{n}^{100}\) into Transformer by setting either \(d_{s}=1,d=100\) or \(d_{s}=100,d=1\) under LeCun initialization with \(_{0}=d_{m}^{-1}\). Figure 3 shows that the training of Transformer with \(d_{s}=1,d=100\) is similar to that of two-layer FCNN with the same width \(d_{m}=100\). However, the case of \(d_{s}=100,d=1\) fails to converge, which is consistent with our theoretical finding.

### Fitting real-world dataset

Beyond synthetic data, in this section, we examine the convergence performance of Vision Transformer (ViT) on classification task on MNIST dataset (LeCun et al., 1998), which includes 10 classes of images with size \(28 28\). We use a single layer and single head ViT. The dimension of \(d\) is 64. We change the dimension of the query, key, and value from 16 to 1024 and 16384. The network is optimized with SGD with step size \(0.1\), and momentum \(0.9\) for \(50\) epochs. We repeat the experiment for three runs. We present the convergence results over \(3\) runs in Figure 3(a), which shows that when the width is smaller, e.g., \(16\), both \(_{0}=d_{m}^{-1}\) and \(_{0}=d_{m}^{-1/2}\) scaling have similar

Figure 3: Convergence result on synthetic data with vector input.

Figure 2: Visualization of the training process of Transformers with LeCun initialization and \(_{0}=d_{m}^{-1/2}\) scaling on synthetic data. (a) Linear convergence. (b) Rate of change of the weights during training. Observe that the weights change very slowly after the \(50^{}\) epoch. (c) Evolution of the NTK during the training. The result mirrors the plot (b) and demonstrates how the kernel varies significantly at the beginning of the training and remains approximately constant later. As the width increases, the empirical NTK becomes more stable.

convergence results. However, as the width increases to \(1024\) and \(16384\), the \(_{0}=d_{m}^{-1}\) setting admits a slower rate than that of \(_{0}=d_{m}^{-1/2}\), especially a extremely slow rate under \(d_{m}=16384\). This is consistent with our theoretical result on the _dimension missing_ effect such that the \(_{0}=d_{m}^{-1}\) setting makes Transformer difficult to fit data. Additionally, we visualize the attention map in Figure 3(b), i.e., the output of softmax in the self-attention layer before training and after training under \(_{0}=d_{m}^{-1/2}\) setting. We could see that the self-attention layer changes a lot during training.

## 6 Conclusion

We present a comprehensive analysis of the global convergence properties of shallow Transformer under various scaling and initialization schemes. Regarding scaling schemes, for a large width setting, the difference on convergence between \(_{0}=d_{m}^{-1/2}\) and \(_{0}=d_{m}^{-1}\) can be demonstrated both theoretically and empirically. Our theory is able to explain this in a _dimension missing_ view. Regarding initialization schemes, our theory prefers to using LeCun and He initialization in Transformer training, which allows for a faster convergence rate than NTK initialization. Though our analysis is limited to shallow Transformers, we believe our framework can be extended to deep Transformers. We provide further discussion into this extension in Appendix C.7. Exploring the convergence properties of deep Transformers is indeed an intriguing avenue for future research.