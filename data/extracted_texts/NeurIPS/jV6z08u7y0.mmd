# The Implicit Bias of Gradient Descent toward Collaboration between Layers: A Dynamic Analysis of Multilayer Perceptions

The Implicit Bias of Gradient Descent toward Collaboration between Layers: A Dynamic Analysis of Multilayer Perceptions

 Zheng Wang  Geyong Min

Department of Computer Science

University of Exeter

{zw360;G.Min}@exeter.ac.uk

&Wenjie Ruan

School of Computer Science

USTC

rwjie@ustc.edu.cn

Corresponding Author

###### Abstract

The _implicit bias_ of _gradient descent_ has long been considered the primary mechanism explaining the superior _generalization_ of over-parameterized neural networks without overfitting, even when the training error is zero. However, the implicit bias toward _adversarial robustness_ has rarely been considered in the research community, although it is crucial for the _trustworthiness_ of machine learning models. To fill this gap, in this paper, we explore whether layers in _neural networks_ collaborate to strengthen adversarial robustness during gradient descent. By quantifying this collaboration between layers using our proposed concept, _co-correlation_, we demonstrate a monotonically increasing trend in co-correlation, which implies a decreasing trend in adversarial robustness during gradient descent. Additionally, we observe different behaviours between narrow and wide neural networks during gradient descent. We conducted extensive experiments that verified our proposed theorems.

## 1 Introduction

As Artificial Intelligence (AI) has been widely applied in many industrial sectors, understanding the theoretical properties behind modern machine learning models is important, especially for neural networks due to their black-box nature. One such property is _implicit bias_, stemming from the phenomenon where over-parameterized neural networks, trained in a _gradient descent_ manner, often exhibit great generalization without over-fitting. This implicit bias of gradient descent is often explained as steering neural networks towards solutions characterized by max-margin [3; 23; 12].

Another intriguing phenomenon is the existence of adversarial examples -- imperceptible perturbations of inputs that alter classification results. Apart from existing work on _attacks_ -- algorithms for generating adversarial examples [5; 39; 37], _defences_ against attacks, e.g., adversarial training [25; 36] and distillation , and _verification_[40; 34] to identify safe regions guaranteeing the absence of adversarial examples, recently some works are aiming at a theoretical understanding behind adversarial robustness. However, a principled way to comprehend the core contributors to the vulnerability of neural networks, especially a theoretical understanding of their relation to generalization capabilities, remains fragmented. This fragmentation is largely due to the intricate nature of neural networks, where robustness is interconnected with many factors spanning across input data distribution [33; 14], sampling complexity [1; 27], optimization techniques , weight initialization strategies , and model capacity and architectures [32; 2; 18; 35]. Not to mention that only very few works can address both generalization and adversarial robustness in a uniform framework. One such research by Frei et al.  investigates the _implicit bias_ concerning both _generalization_ and _adversarial robustness_,asserting that while implicit bias leads to solutions with improved generalization, it results in weaker adversarial robustness. However, this work ignores the architectural factor of neural networks and is hard to generalize to neural networks with more than two layers.

Inspired by this work and to investigate whether neural network layers collaborate against adversarial examples during training, this paper first adopts the novel concept of Dirichlet energy--originating from Partial Differential Equations (PDEs) to assess the variability of a function --to evaluate the adversarial robustness of neural networks. We then theoretically demonstrate that Dirichlet energy serves as an effective measure of adversarial risk. By decomposing the Dirichlet energy across the entire neural network into its constituent layers, we can quantify the interactions between adjacent layers concerning adversarial robustness. We term this interaction _collaboration correlation (co-correlation)_ and find that this metric reflects _'alignments'_ in feature selection between neighbouring layers. Furthermore, we conduct a dynamic analysis of co-correlation in two-layer MLPs, demonstrating with high probability a monotonic increase under gradient descent, which indicates diminishing adversarial robustness. Additionally, our experiments show that two-layer MLPs with small widths tend to enhance their performance through strengthened co-correlation, a pattern not observed in wide two-layer MLPs. Our key contributions in this paper can be summarized as follows:

1. To the best of our knowledge, this work is the first to study the implicit bias of interaction between layers. We have quantified the interactions between adjacent layers and theoretically demonstrated that co-correlation between layers strengthens during gradient descent in neural networks under mild assumptions, suggesting that it not only fails to collaborate against adversarial perturbations but may even hinder resistance to them during gradient descent.
2. We demonstrate how neural networks with a large width differ in behaviour from the neural network of a small width, showing that MLPs with larger widths exhibit more resistance to increased co-correlation and, therefore, are more adversarial robust, which is complementary for the work by Dohmatob and Bietti .
3. Extensive experiments have been conducted to validate our proposed framework. By controlling the weight initialization, a perspective also suggested by Zhu et al. , we challenge the argument, as proposed by Huang et al. , that a wide neural network does not necessarily lead to better adversarial robustness, through the lens of cross-layer collaboration.

## 2 Related Works

### Implicit Bias of Gradient Descent

The mystery of over-parameterized neural network trained with _gradient descent_ manner hardly over-fitting has long been studied. Chizat and Bach  study the two-layer neural network with infinite width and homogeneous activations, showing that gradient flow can be characterized as a max-margin classifier on exponentially tailed losses. Lyu et al. , Sarussi et al.  study the two-layer Leaky ReLU neural network on linearly separable data and claim that networks converge to a max-margin linear predictor by gradient descent manner. Frei et al.  confirms those claims on high-dimensional nearly orthogonal data.

Lyu and Li , Ji and Telgarsky  claims that the homogeneous neural networks with exponentially-tailed classification losses converge to a _KKT_ point of a maximum-margin problem. Kunin et al.  extend these results to a more boarder family of _quasi-homogeneous_ neural networks. A more recent research  considers both generalization and robustness for two-layer ReLU neural networks, arguing that gradient descent is biased towards solutions that generalise well but are more vulnerable against adversarial examples, even the neural network is highly over-parameterized.

### Theoretical Investigation of Adversarial Robustness

Since the phenomenon of adversarial examples has been discovered , various works have been proposed to understand the theoretical fundamentals behind it, especially for neural networks. Some researchers argue that the source of adversarial vulnerability comes from the input data [33; 8; 31; 26; 14; 7; 30; 1; 27]. The more recent researches investigate the fragility of neural networks from an architectural perspective. Simon-Gabriel et al.  study the vulnerability of feed-forward neural networks measured by \(L_{p}\) norm of the loss function w.r.t. input data, suggesting that the vulnerability increases with input dimension independent of model structures. Daniely and Shacham  examined the ReLU neural network characterized by decreasing dimensions at each layer. They asserted that the manifestation of adversarial robustness is intrinsically tied to the network's architecture, which contrasts with the propositions put forth by Simon-Gabriel et al. . Bubeck et al.  expanded the findings of Daniely and Shacham's work on two-layer neural networks from an "under-complete case" scenario to an "over-complete" one where the number of neurons surpasses the input dimension. They further broadened the conclusions drawn by Daniely and Shacham  and Bubeck et al.  to encompass Deep ReLU networks, hinting at a crucial role played by bottleneck layers in these networks. Zhu et al. , instead of merely considering random weights as the standard configuration, conducted a comprehensive analysis of the effects of weight initialization on adversarial robustness.

Unlike previous studies that focus solely on the overall assessment of neural networks while overlooking layer interactions, our research examines the synergistic involvement between layers within neural networks, taking into account both weight initialization and optimization.

## 3 Preliminary

### General Setting

We follow the binary classification setting where the input data is \(^{d}\), with the label \(\{0,1\}\). Given data set \(=\{(_{i},y_{i})\}_{i=1}^{n}\) drawn from an unknown probability measure \(P\) on \(\) and the neural network \(f:\), where \(\) denotes the set of parameters, our objective is to optimize \(f\) by updating the weights with _gradient descent_ method such that it can predict the label accurately. The prediction result is shown in Equation (1).

\[y_{pred}=1,(f_{W}())>0.5\\ 0,(f_{W}()) 0.5,\] (1)

where \(sig\) is the _sigmoid_ function, i.e., \(sig(x)=1/(1+e^{-x})\). We use _Binary Cross-Entropy (BCE) loss_ in Equation (2) as our loss function. For simplicity, we denote \(u_{i}=f(_{i},W),i[n]\) as the output of the neural network for input \(_{i}\), where \([n]=\{k^{+}|k n\}\).

\[L(f,y)=_{i=1}^{n}L(sig(u_{i}),y_{i})=-_{i=1}^{n} y_{i}(sig(u_{i}))+(1-y_{i})(1-sig(u_{i})).\] (2)

### Neural Networks and Adversarial Risk

Our exploration starts from a basic linear model, then to _Multilayer Perceptrons (MLP)_. We provide the proof for both linear and 2-layer MLPs, which can be extended to MLPs with more layers. They are defined as

\[f_{linear}(,W)=^{T}(W)\] (3a) \[f_{mlp}(,W)=^{T}((W)),\] (3b)

where \(\) is the input data, \(W^{m d}\) denotes the linear transformation, \(m\) is the width of the networks. \(\) denotes the element-wise _activation functions_.

We follow the initialization setting in , where \(\) is randomly initialized and fixed from a binary selection of \(\{-},}\}^{m}\). Additionally, we introduce a slightly different setting for the weights \(W\), which are randomly initialized following the normal distribution \(N(0,})\) with \(q>0\), instead of \(N(0,)\). However, in our experiment, different settings of \(q\) are considered, including the scenario that \(q 0\).

Generalization ability is one of the most important concepts for machine learning models. Classifiers with better generalization power indicate lower _natural risk_ for unseen data. Given data points \((,y) P\) and classifier \(f\), the natural risk is defined as

\[R(f)=,y) P}{}[L(f(),y))].\] (4)

When it comes to \(0\)-\(1\) loss, the natural risk becomes the probability of misclassification for unseen data points.

Similar to natural risk, the _adversarial risk_ is defined as the probability of misclassification under adversarial perturbations as is shown in Definition 3.1.

**Definition 3.1** (Adversarial Risk).: Given data points \((,y) P\), and a perturbation \(\) within a norm-ball, i.e.,

\[B_{r}=\{\|\|_{2} r\},\] (5)

where \(r>0\) indicates the \(L_{2}\)-norm budget for perturbations. The adversarial risk for neural network \(f\) on loss function \(L\) is defined as

\[R^{rob}(f,r)=*{}_{(,y) P}[_{ B_{r}}L(f(+),y))]\] (6)

Since adversarial perturbations are almost invisible to human eyes, we expect \(r\) to be quite small.

### Dirichlet Energy

The concept of _Dirichlet energy_, originating from _Partial Differential Equations (PDEs)_, serves as a tool to assess the variability of a function . However, as argued by Dohmatob and Bietti , it serves as a more effective measure of adversarial robustness than the _Lipschitz constant_. We extend this concept to mappings to make it more suitable for multi-dimensional problems, which is formally defined in Definition 3.2.

**Definition 3.2** (Dirichlet Energy for Mappings).: Let convex set \(_{1}^{m_{1}}\) and \(_{2}^{m_{2}}\). Given a differentiable mapping \(:_{1}_{2}\), the Dirichlet Energy w.r.t. \( P_{}\) is defined as

\[()}()\|_{L^{2}(P_{ })}^{2}}=}_{ P_{}}[ \|J_{}()\|_{2}^{2}]},\] (7)

where \(\|\|_{2}\) refers to the operator norm and \(J_{}()\) denotes the Jacobian matrix of \(\) w.r.t. its input \(\).

To be clear, we use \(\|\|_{2}\) to denote the \(L_{2}\)-norm for vectors and operator norm for matrices throughout our analysis. This concept can be extended to layers with ReLU activation by extending the derivative to \(ReLU^{}(x)=_{\{x 0\}}\), where \(_{\{x 0\}}\) is the indicator function. With the concepts defined, we now establish the relationship between adversarial risk and Dirichlet energy.

## 4 Measure Adversarial Risk by Dirichlet Energy

In this section, we establish the relationship between the _adversarial risks_ and _Dirichlet energy_, showing that Dirichlet Energy is a proper measurement for adversarial risk. Dohmatob and Bietti  only compares the Dirichlet energy with the Lipschitz constant. We, instead, illustrate that Dirichlet energy is a proper representation of the gap between natural risk and adversarial risk.

**Theorem 4.1**.: _Given data points \((,y) P\) and \( P_{}\), the relationship between adversarial risk and Dirichlet energy for classifier \(f\) with differentiable loss function \(L\) is shown as_

\[R^{rob}(f,r) R(f)+r(L(f)),\] (8)

_where \(r>0\) is the largest perturbation budget and \((L(f))=}_{ P_{}} [\|_{}L^{T} J_{}()\|_{2}^{2}]}\) indicating the Dirichlet energy of the classifier on loss \(L\)._

As is shown, the Dirichlet energy is a proper representation of the gap between generalization and adversarial risk which indicates the adversarial robustness. The proof relies on the linear approximation of \(L(f)\). The detailed proof is shown in Appendix A.

We also empirically show that the Dirichlet energy for classifier \(f\), i.e., \((f)\) instead of \((L(f))\), is the part that influences the adversarial robustness, as is shown in Figure 1 where we compare the level of Dirichlet energy for \(f\) with the _robust accuracy_ of the classifier attacked by _Auto-attack_.

Since the Dirichlet energy defined in Equation (7) can be used to measure the _variability_ of mappings, it follows that this metric could be employed to evaluate the adversarial robustness of individual layers or modules within neural networks. Consequently, this allows for an assessment of whether there is collaboration between these components in terms of adversarial robustness.

[MISSING_PAGE_FAIL:5]

_Remark 4.4_ (Linear Correlation for \(L_{2}\)-norm of the Jacobian).: We have \(_{,}=1\) iff there exist \(t\) such that

\[P(t\|J_{}()\|_{2}=\|J_{}()\|_{2})=1,\]

implying that \(_{,}\) certainly can be used to assess the linear correlations. It reduces to _Pearson correlation coefficient_ when the _mean_ of both random variables equals zero.

Now we give the theorem that binds them together.

**Theorem 4.5** (Robustness Decomposition).: _Given the same assumption in Definition 4.2, the measurement for overall adversarial robustness can be decomposed as_

\[() =(_{ P}\|J_{}()\|_{2}^{2})^{}\] \[=_{,}1+, }}{_{,}^{2}}^{}_{ ,}()()\] (15)

The proof is straightforward from the definition. Based on this theorem, we have conducted experiments with various linear models and 2-layer MLPs. These experiments demonstrate that, apart from the co-correlation, all other statistics are negligible, as shown in Figure 5 in the Appendix D. Consequently, our analysis primarily focuses on the co-correlation \(\).

## 5 On Dynamics of Co-Correlation

### Dynamics for Linear Model

Before delving into our analysis, we state our assumptions explicitly.

**Assumption 5.1**.: We assume that each element \(w_{i,j}\) in the weight matrix \(W(0)^{m d}\) at initialization follows the Gaussian distribution \(N(0,})\), with \(q>0\). Additionally, each element \(a_{r},r[m]\) in \(\) is randomly selected from the set \(\{-},}\}\), and fixed during training.

**Assumption 5.2**.: We assume that for each \((_{i},y_{i}) D,i[n]\), \(_{i}\) is \(L_{2}\) norm bounded such that \(\|_{i}\|_{2}=1\) for all \(i[n]\).

Since we only assume bounded inputs and a specific weight initialization method, compared to existing works [23; 19; 21; 12], our approach can be easily extended to MLPs with more than two layers.

Now let us focus on the co-correlation defined in Equation (9) and show the dynamics of \(_{,}\) for each step of gradient descent. We start from the linear model described in Equation (3a). Given the binary classification problem and the linear model described. Our first theorem demonstrates that co-correlation \(_{,}\) gradually accumulates throughout gradient descent optimization. Despite the simplicity of the linear model, it effectively exhibits most of the core properties under consideration.

Figure 1: For all MLPs considered, lower value of Dirichlet Energy (Figure 0(b)) corresponds to larger robust accuracy on test-set attacked by \(L_{2}\)-norm Auto-attack with \(=0.5\). The dynamics of the co-correlation \(\) for linear and MLPs is shown in Figure 0(c) and 0(d). The architectures of neural networks are defined in Equation (3b). The parameters to control the weight initialization in Assumption 5.1 is set to \(q=0.25\)

Since the weights are updated by the gradient descent, the update of the weights at step \(t\) is

\[_{r}(t)= a_{r}_{i=1}^{n}y_{i}- sig(u_{i}(t))_{i},\]

where \(\) denotes the learning rate. Therefore, the dynamics of the weights can be expressed as

\[(t)=}^{T}(t),\] (16)

where \(\) denotes the Kronecker product and \(}(t)\) is the _error weighted input_ such that

\[}(t)=_{i=1}^{n}y_{i}-sig u_{i}(t)_{i}.\] (17)

Given the weight updates, we demonstrate that the dynamics of the co-correlation for the linear model, denoted as \(_{,W}(t)\), exhibit an increasing trend, particularly during the initial steps of training when most predictions are still essentially random.

**Theorem 5.3** (Dynamics of the Co-correlation for Linear Model).: _Given the linear model defined in Equation (3a) and training dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\). Assume that assumptions 5.1 and 5.2 hold for \(W\) and \(\). The gradient descent applied to the weights results in the dynamics of the co-correlation being expressed as:_

\[_{,W}(t)= C(t)_{,W},\] (18)

_and with high probability,_

\[C(t)^{t}}()^{T} }(t)}{\|W(t)\|_{2}^{2}}1-(t)^{T}^{2}+}\] (19)

_where the \(\) is the dominate eigenvector for \(W(t)W(t)^{T}\)._

_When \(m\) is sufficiently large, and during the initial steps of the optimization process, \(}(),[t]\) are quite similar to each other in terms of cosine similarity, implying an acute angle to each other, which leads to \(_{=1}^{t}}()^{T}}(t) 0\). As a result, we can conclude that \(C(t) 0\)._

The detailed proof can be found in the Appendix B. This assertion is also corroborated by the results of our experiments as shown in Figure 0(c). Even though Theorem 5.3 is based on a linear model, the essential properties are universally applicable and can be summarized as follows:

**Property 1**.: The co-correlation \(_{,W}\) develops during the initial stages of training and becomes saturated as training progresses to its later stages.

**Property 2**.: The speed of the accumulation of co-correlation \(_{,W}\) is inversely related to the operator norm of weights \(\|W(t)\|_{2}\).

Under the same weight initialization conditions specified in Assumption 5.1, an increase in network width leads to a decrease in the \(L_{2}\)-norm of the weight, consequently causing a substantial rise in co-correlation.

### Dynamics for MLP Model

For the non-linear case, we make certain assumptions regarding activation functions.

**Assumption 5.4**.: The derivative of the activation function \(^{}(x)\) in non-linear neural networks is bounded by \(M\). In other words, we have \(|^{}(x)| M\).

With Assumption 5.4, Theorem 5.3 can be extended to MLPs, and the two properties still hold. Different from the linear model, the update of weights for non-linear MLP defined in (3b) is

\[_{r}= a_{r}_{i=1}^{n}y_{i}- sig(u_{i})^{}(_{r}^{T}_{i})_{i},\] (20)

where \(^{}(_{r}^{T}_{i})\) is the derivative of activation function w.r.t. its input. Hence,

\[ W=a_{1}}_{1}^{T}\\ \\ a_{m}}_{m}^{T},\]where for each \(r[m]\),

\[}_{r}=_{i=1}^{n}^{}(_{r}^{T} {x}_{i})y_{i}-sig(u_{i})_{i}.\]

As is shown in Equation (20), \(\) is not the dominant eigenvector for \( W W^{T}\) due to the difference of \(}_{r},r[m]\) from \(}\). Thus to average out the difference, we define the weighted sum of inputs for the non-linear model as

\[}_{}(t)_{i=1}^{n}_{i}(t, )y_{i}-sig(u_{i}(t))_{i},\] (21)

where \((_{i},y_{i})\) are realized r.v., and \( P_{}\). Given \(w_{i,j}N(0,})\) for each element in \(W(0)\), we have \((0) N(0,}_{d})\). Furthermore, given that \(W(t)\) is calculated from \(W(0)\), it is evident that each row in \(W(t)\), denoted as \((t)\), constitutes a random variable. As a result, both \(^{}((t)^{T})\) and \(^{}((t)^{T}_{i})\) are bounded random variables that are contingent upon \(W(0)\). Hence, we define \(_{i}(t,)\) as

\[_{i}(t,)_{W(0)}^{}(( t)^{T})^{}((t)^{T}_{i}).\]

Since, \( P_{}\), \(_{i}(t,)\) is still r.v. contingent to \(\) and so does the \(}_{}(t)\). Now we show the dynamics of co-correlation for two-layer MLP defined in Equation (3b).

**Theorem 5.5**.: _(Dynamics of the Co-correlation for MLP) Given the MLP defined in Equation (3) with training dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\), \(\) such that \( P_{}\). Assume that Assumption 5.1 and 5.2 hold for \(W\) and \(\), and Assumption 5.4 holds for the activation function. we have_

\[_{, W}(t)= C(t)_{, W }(t).\]

_With high probability,_

\[C(t)^{t}1-^{T}()^{T}(t)_{ P_{}}}_{ }^{T}()}_{}(t)}{_{ P_{ }}\|D(t)W(t)\|_{2}^{2}}+},}},\]

_where_

\[D(t)=diag(^{}(_{1}(t)^{T}),,^{}(_{m}(t)^{T})),\]

_and \((t)\) denotes the dominant eigenvector for \(W(t)W(t)^{T}\), with \(}_{}^{T}\) is defined in Equation (21). Similar to the Theorem 5.3, when \(m\) is sufficiently large, and during the initial steps of the optimization where the error-weighted inputs \(}_{}^{T}(),[t]\) do not significantly fluctuate, we have that \(C(t) 0\)._

The detailed proof is in Appendix C. In Theorem 5.5, \(}_{}\) serves a similar purpose as \(}\) for the linear model. In addition, it considers the influence of the activation function. Property 1 still holds for the MLPs, and Property 2 extends to \(_{ P_{}}\|D(t)W(t)\|_{2}^{2}=\|J_{ W}( {x})\|_{L(P_{})}^{2}\).

Figure 2: The dynamics of co-correlation for ResNet50 and WRN50 under different way of partition. The way of partition is illustrated in Figure 1(a). A1-A2 and B1-B2 represent the separations that distinguish the head and tail separately.

## 6 Experiments

To estimate the co-correlation more efficiently and in parallel, we employ the _Power Iteration_ algorithm  in conjunction with _Functorch_. The corresponding pseudo-code is presented in the Appendix E.

We verify our proposed theorem on linear and MLP models on the MNIST dataset . The width of hidden layers varied from \(2^{4}\) to \(2^{13}\), with weights initialized via a Gaussian distribution \(N(0,}})\) where \(q\) was set to values ranging from \(0.25\) to \(-0.15\). We train both the linear and MLP for 50 epochs with a batch size of 512 using the SGD optimizer with a learning rate of \(0.003\). In addition, we also conduct the experiment on more complex ResNet50  and WRN50 . For both models, we opted for default random weight initialization and used the Adam optimizer with a learning rate of \(0.0005\) on CIFAR10 . To calculate the co-correlation and other statistics, we cover the entire testset. We consider using \(L_{2}\) Auto-attack  with \(=0.5\) for all MLPs we trained. The experiments were executed on a Nvidia RTX3090 GPU, using Python 3.9.7 and PyTorch 1.9.1. The code for the experiment is available at https://github.com/squarewang2077/co-correlation.

### Empirical Evidence for Proposed Theorem

Figure 1 presents a comparison between the robust accuracy and the Dirichlet energy \((f)\) across all trained MLPs. As observed in Figure 0(a) and 0(b), models with lower levels of Dirichlet energy \((f)\) tend to exhibit higher robust accuracy, suggesting that Dirichlet energy is an effective representation of adversarial robustness. Another noteworthy finding is that wider neural networks, with the same level of weight initialization, demonstrate improved adversarial robustness.

Figure 1 also depicts the dynamic behaviour of shallow neural networks with a weight initialization parameter of \(q=0.25\). As is shown in Figure 0(c) and 0(d), the co-correlation \(q\) increases throughout training. Except for narrow widths like \(2^{4}\) and \(2^{5}\), the majority of networks demonstrate an upward trend. This trend, however, flattens for non-linear models, suggesting potentially stronger adversarial robustness due to the non-linearity of the activation function introduced in MLPs.

Figure 2 shows the dynamics of the co-correlation on ResNet50 and Wide-ResNet50. Both networks are trained on CIFAR10 using the Adam optimizer. We divide them by the pattern of A1-A2 and B1-B2, as shown in Figure 1(a). The co-correlation outcomes for these divisions are displayed in Figure 1(b) and Figure 1(c), it shows that even with the Adam optimizer, without specific weight initialization considerations, there is a noticeable rise in co-correlation.

### The Impact of Width and Weight Initialization

Figure 3 illustrates the co-correlation dynamics under varying \(q\) for both linear and MLPs with widths of 32, 512, 2,048, and 8,192. The figure highlights that our proposed theorems' assumption of \(q>0\) is quite tight, as all trajectories with \(q<0\) remain flat throughout training. We can also observe that the speed of accumulation significantly increases with larger network widths.

Figure 4 displays the accuracy on testset and co-correlation for both linear and MLPs as heat-maps. Each cell in the heat-map represents a trained network. From Figure 3(c) and Figure 3(d), we observe that the best performance and robustness are shown by the MLPs with the largest widths (\(=8192\)) and the smallest weight initializations (\(q=-0.15\)). And when we alter the weight initialization to

Figure 3: The dynamic of co-correlation under different set-up of weight initialization. MLP network defined in Equation (3) with ReLU activation function for width \(32\), \(512\), \(2048\) and \(8192\) are included.

control the co-correlation, making it increase from \(0.25\) to \(0.83\), the accuracy declines accordingly from \(0.93\) to \(0.68\). On the contrary, for another extreme case of models with a width of \(16\), enhanced performance is accompanied by increased co-correlation. Consequently, an interesting conclusion can be drawn about the diverse behaviour of neural networks with small and larger width. Gradient descent tends to enhance the training of neural networks with smaller width by fostering co-correlation among layers, which is intrinsically brittle. However, wide networks are trained with less reliance on interlayer correlation, resulting in inherently more robust models.

## 7 Conclusion and Limitation

Our work investigates the implicit bias of gradient descent toward adversarial robustness from the perspective of collaboration between layers. By adapting Dirichlet energy to estimate the adversarial robustness of neural networks' individual components, we characterized the collaboration behaviour between consecutive layers and identified two fundamental properties for dynamics of the co-correlation. The first property shows that the co-correlation for MLPs will build up during gradient descent under mild assumptions for weight initialization. The second property shows that the speed of accumulation for co-correlation is inversely related to the operator norm of Jacobian for the corresponding sub-modules. In addition, we observed that networks with small widths tend to foster co-correlation among layers to improve performance, whereas wide networks' performance improvement does not heavily rely on establishing such co-correlation. Future research can expand upon this by examining the effects of increased network depth and more sophisticated structures on the observed phenomena.

LimitationOur work can be easily extended to multi-layer neural networks since we only assume that the inputs are bounded by the \(L_{2}\)-norm. However, like many theoretical studies, extending our approach to more complex models is challenging. It remains unknown whether complex models exhibit the same behaviors.