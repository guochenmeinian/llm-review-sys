# Revisiting the Evaluation of Image Synthesis

with GANs

 Mengping Yang\({}^{1,}\)  Ceyuan Yang\({}^{1,}\)  Yichi Zhang\({}^{1}\)  Qingyan Bai\({}^{3}\)  Yujun Shen\({}^{2}\)  Bo Dai\({}^{1}\)

\({}^{1}\)Shanghai AI Laboratory \({}^{2}\)Ant Group \({}^{3}\)Tsinghua University

###### Abstract

A good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground-truth, image synthesis tasks target generating _unseen_ data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a representative of generative models. In particular, we make in-depth analyses of various factors, including how to represent a data point in the representation space, how to calculate a fair distance using selected samples, and how many instances to use from each set. Extensive experiments conducted on multiple datasets and settings reveal several important findings. Firstly, a group of models that include both CNN-based and ViT-based architectures serve as reliable and robust feature extractors for measurement evaluation. Secondly, Centered Kernel Alignment (CKA) provides a better comparison across various extractors and hierarchical layers in one model. Finally, CKA is more sample-efficient and enjoys better agreement with human judgment in characterizing the similarity between two internal data correlations. These findings contribute to the development of a new measurement system, which enables a consistent and reliable re-evaluation of current state-of-the-art generative models. 1

## 1 Introduction

Through reproducing realistic data distribution, generative models  have enabled thrilling opportunities to go beyond existing observations via content recreation. Their technical breakthroughs in recent years also directly lead to the blooming of metaverse, AI Generated Content (AIGC), and various other downstream applications. However, accurately measuring the progress and performance of generative models poses significant challenges, as it requires a consistent and comprehensive evaluation of the divergence between real and synthesized data distributions. Among existing evaluation metrics , Frechet Inception Distance (FID)  is the most popular evaluation paradigm for synthesis comparison. Despite its popularity, recent studies  have identified several flaws in the FID metric that may miscalculate the actual improvements of generative models. Consequently, there is a pressing need for a more systematic and thorough investigation in order to provide a more accurate assessment of synthesis performance.

Therefore, this paper presents an empirical study that rigorously revisits the consistency and comprehensiveness of evaluation paradigms for generative models. Commonly used paradigms including FID typically contain two key components: a feature extractor \(()\) and a distributional distance \(d()\). Through \(()\), _i.e.,_ Inception-V3 , a number of real samples (\(x\)) and synthesizedones (\(y\)) are projected into a pre-defined representation space to approximate the respective data distributions. \(d()\), _i.e.,_ Frechet Distance  is then calculated in the space to deliver the similarity index, indicating the synthesis quality. Following the philosophy, our study revolves around the representation space defined by the feature extractor \(()\), and the distributional distance \(d()\).

The most commonly used feature extractor, _i.e.,_ Inception-V3, has been found to encode limited semantics and possesses a large perceptual null representation space , making it hardly reflect the actual improvement of synthesis. Accordingly, we gather multiple models that vary in _supervision signals_, _architectures_, and _representation similarities_ to investigate the impact of the feature extractor \(()\), which are respectively motivated by 1) representation spaces defined by the extractor \(()\) usually encode different levels of semantics, understanding which extractor or set of extractors can capture rich semantics is crucial yet less explored; 2) and it remains uncertain how the correlations between various representation spaces affect the evaluation results. In addition to studying the feature extractor \(()\), we further delve into _the consistency across spaces_, _the choice of distances_, and _the number of evaluated samples_ for the distributional distance \(d()\). It is imperative that the distributional distance consistently provides a reliable similarity index, even when measured in various representation spaces. Similarly, selecting an appropriate number of samples to represent the synthesis distribution is a critical consideration. Regarding the choice of \(d()\), besides Frechet Distance, we incorporate Centered Kernel Alignment (CKA) [11; 10] to explore alternatives for a more accurate evaluation.

In order to qualitatively and quantitatively compare different choices of the aforementioned aspects of \(()\) and \(d()\), we re-implement the visualization pipeline and the histogram matching technique as in . These techniques allow us to respectively highlight the most relevant semantics of an image _w.r.t_ the similarity indexes and to attack the measurement system through a selected subset. Moreover, we conduct an extensive user study involving 100 participants to investigate the correlation between the synthesis measurement and human judgment. Through these analysis tools, we make _several significant findings_: 1) One specific extractor (_e.g.,_ Inception-V3) tends to capture limited semantics and provide unreliable measurement results. 2) Various extractors naturally focus on different aspects of semantics thus demonstrating the potential generalization across different domains, motivating us to incorporate multiple extractors to deliver a comprehensive and reliable measurement. 3) With respect to features obtained from multiple representation spaces defined by different extractors, CKA proves to be effective in measuring the discrepancy and produces bounded values that facilitate comparison across different spaces. 4) In conjunction with the extensive user study, CKA consistently agrees with human judgment whereas FID failed in some circumstances, further underscoring the advantages of using CKA as the evaluation metric.

After revisiting various factors, we leverage the newly developed measurement system to re-evaluate extensive generative models under various settings. Current state-of-the-art generative models on several domains are first benchmarked through our system. Moreover, the performances and intrinsic properties of diffusion models and GANs are comprehensively compared with the new measurement system. Furthermore, the measurement system is employed to evaluate the performance of image-to-image translation models. It turns out that our system not only delivers a similar assessment with FID and human evaluation in most cases, but also demonstrates a more reliable and consistent correlation with human judgment than FID, demonstrating the robustness and superiority of the proposed metric.

## 2 Preliminary

This section briefly introduces the feature extractor \(()\), distributional distance \(d()\), evaluated datasets and generative models, as well as auxiliary analysis approaches used in our study.

### Representation Spaces

To investigate the effect of feature extractors \(()\), we gather multiple models that are pre-trained on different objectives in fully-supervised/self-supervised manners, and with various architectures (_e.g.,_ ViT, CNN, and MLP).

**Supervision.** Models that are trained in fully-supervised and self-supervised manners are collected due to their potential for generalization. In particular, we include the backbone networks which are well-trained on supervised ImageNet classification tasks [13; 21]. Further, we gather the weights derived from single-modal/multi-modal self-supervised learning approaches [46; 8; 9; 6].

**Architecture.** In addition to considering models trained with different supervisions, we also include models with various architectures. Concretely, models with CNNs [56; 53; 21; 6; 8], ViTs [9; 39; 46], as well as MLPs  architectures are gathered together for our investigation.

### Distributional Distances

**Frechet Inception Distance (FID)** computes the Frechet Distance  between two estimated Gaussian distributions, _i.e.,_\((_{s},_{s})\) and \((_{r},_{r})\), which represent the feature distributions of synthesized and real images extracted by the pre-trained Inception-V3. Formally, Frechet Distance (FD) is calculated by

\[=\|_{s}-_{r}\|^{2}+(_{s }+_{r}-2(_{s}_{r})^{}),\] (1)

where X and Y represent the real distribution and synthesized distribution, respectively. \(\) and \(\) correspond to the mean and variance of Gaussian distribution, and \(()\) is the trace operation.

**Centered Kernel Alignment (CKA)** as a widely used similarity index for quantifying neural network representations [11; 32; 12], could also serve as a metric of similarity between two given data distributions. CKA has been identified to have several advantages: 1) CKA is invariant to orthogonal transformation and isotropic scaling, making is stable under various image transformations; 2) CKA can capture the non-linear correspondence between representations benefit from its kernel mapping; and 3) CKA can determine the correspondence across different features and with different widths, whereas previous metrics fail .

Formally, CKA is normalized from Hilbert-Schmidt Independence Criterion (HSIC)  to ensure invariant to isotropic scaling and is calculated by

\[=(x,y)}{(x,x)(y,y)}}.\] (2)

Here, HSIC determines whether two distributions are independent. Formally, let \(K_{ij}=k(_{i},_{j})\) and \(L_{ij}=l(_{i},_{j})\), where \(k\) and \(l\) are two kernels. HSIC is defined as

\[(K,L)=}\,(KHLH),\] (3)

where \(H\) denotes the centering matrix (_i.e.,_\(H_{n}=I_{n}-^{T}\)). For kernel selections of \(k\) and \(l\), we find that different kernels (RBF, polynomial, and linear) give similar results and rankings, and the RBF kernel contributes to the distinguishability of quantitative results. Therefore, RBF kernel is used for all experiments, and the bandwidth is set as a fraction of the median distance between examples . These metrics are compared in a consistent setting for fair comparison, more implementation details and comparisons on CKA are given in _Supplementary Material_.

### Benchmarks and Analysis Approaches

**Benchmarks.** In order to analyze various factors of measurement, we also collect multiple generators to produce synthesized images. To be more specific, we employ state-of-the-art generative models trained on various datasets for comparison in Tab. 1. We download corresponding publicly available models for comparison. Unless otherwise specified, all of these models are compared in a consistent setting.

**Visualization tool.** To qualitatively compare where these feature extractors "focus on", we employ the visualization technique of  to localize the regions that contribute the most to the similarity index. The highlighted regions reveal the most relevant parts of an image regarding the measurement results. Accordingly, larger highlighted regions indicate that more visual semantics are involved in the evaluation. Note that generating a realistic image requires all parts, even each pixel, to be well-synthesized. Thus, a metric that focuses on more visual regions appears to be more dependable.

**Histogram matching attack.** We employ the histogram matching  to attack the system to investigate the robustness of the measurement results. Concretely, a subset is selected from a superset by matching the class distribution of the synthesized set with that of the real set. As pointed out by , the synthesis performance could be substantially improved using the chosen subset. We thus prepare two distinct sets of synthesized images. One reference set ("Random") is produced by generating images randomly, and the other set ("Chosen\({}_{I}\)") is carefully curated by matching the class distribution histogram of the supervised Inception-V3 . The matched histograms are available in _Supplementary Material_. Since the generator and real data remain unaltered, the evaluation should keep consistent, and any performance gains directly indicate the unreliability of a given extractor.

**Human judgment.** In order to examine the correlation between the evaluation system and human perceptual judgment, we conduct extensive user studies employing two strategies. Firstly, to benchmark the synthesis quality of various generative models, we prepare a substantial number of randomly generated images (_i.e.,_\(5K\)), and ask \(100\) individuals to assess the photorealism of these images. The final scores are averaged across all \(100\) participants. Secondly, to qualitatively compare two paired generative models with similar quantitative performances (_e.g.,_ Projected-GAN  and Aided-GAN  on LSUN Church dataset in Sec. 4.1), we prepare groups of paired images generated by different models and ask \(100\) individuals to assess the perceptual quality of paired images. More details of our user studies can be found in _Supplementary Material_. In this way, we could obtain reliable and consistent human judgments, facilitating better investigation with the evaluation system.

## 3 Analysis on Representation Spaces and Distributional Distances

In this section, we investigate the potential impacts of the representation space and distributional distances with respect to the final similarity index. Concretely, Sec. 3.1 presents the study of extractors that define the representation spaces, followed by the analysis of distributional distances in Sec. 3.2.

### Representation Spaces

Prior works [35; 36; 41; 26] have demonstrated that the most commonly used feature extractor _i.e.,_ Inception-V3 , could hardly reflect the exact improvement of synthesis due to its limited consideration of semantics. We thus conduct a comprehensive study by incorporating various models that differ in _supervision signals_ and _architectures_, to identify which or which set of extractors serve as reliable feature extractors for synthesis comparison.

**Distinct feature extractors with different architectures yield varying semantic areas of focus.** Fig. 1 shows the highlighted regions that contribute most significantly to the measurement results. Obviously, CNN-based extractors consistently emphasize concentrated regions with or without manual labels for pre-training, including limited semantics. Specifically, CNN-based extractors remain to highlight objects (_e.g.,_ microphone, hat, and sunglasses), rather than the main focus of the evaluation domains (_i.e.,_ Human Faces here). In contrast, ViT-based extractors capture larger regions that encompass more synthesis details and semantics. This observation aligns with the finding that ViTs possess a global and expansive receptive field compared to the local receptive field of

   Method & Year & Training Datasets \\  StyleGAN2  & 2020 & FFHQ, LSUN Church \\ BigGAN, BigGAN-deep  & 2019 & ImageNet \\ ADM, ADM-G  & 2021 & ImageNet \\ Projected-GAN  & 2021 & FFHQ, LSUN Church \\ InsGen  & 2021 & FFHQ, FFHQ \\ StyleGAN-XL  & 2022 & FFHQ, ImageNet \\ Aided-GAN  & 2022 & LSUN Church \\ EqGAN  & 2022 & FFHQ, LSUN Church \\ Diffusion-GAN  & 2022 & LSUN Church \\ DiT , BigRoC  & 2022 & ImageNet \\ GigaGAN , DG-Diffusion , MDT  & 2023 & ImageNet \\   

Table 1: **Generative models used in our study**. Publicly available models are gathered for evaluation.

CNN-based extractors [47; 65; 16]. Furthermore, the ViT-based and CNN-based extractors appear to complement each other, with the former capturing broader regions and the latter focusing on specific objects with high density.

**Multiple extractors incorporate more visual semantics in a complementary manner.** When reproducing the whole data distribution, generative models are required to synthesize not only the main objects but also the background, texture, and intricate details. Similarly, the extractors should strive to capture more regions of given images to approximate the data distribution, enabling better visual perception. However, the above observation regarding the heatmaps of different extractors reveals that each individual extractor could only capture partial semantics of the entire image for measurement, inadequately reflecting the overall synthesis performance. Consequently, various extractors with different architectures should be considered since they could involve more semantics, enhancing the reliability of the evaluation.

**Extractors that are vulnerable to the histogram matching attack are not reliable for evaluation.** Prior study  has highlighted that extractors focusing on limited semantics may be susceptible to the histogram matching attack, undermining the trustworthiness of the evaluation. Motivated by this, we investigate the robustness of the above extractors toward the attack as they attach different importance to the visual concepts. Concretely, we obtain the publicly available generator of StyleGAN2 and calculate the Frechet Distance (FD) results on the FFHQ dataset. We compare two evaluated sets: one is randomly generated using the model, while the other is chosen by matching the predicted class distribution of Inception-V3 (the matched histograms are provided in _Supplementary Material_).

Tab. 2 shows the quantitative results. Comparing the performances of the random set and chosen set, we could tell that certain extractors, regardless of their architectures (_e.g.,_ CNN-based Inception-V3 , ViT-based Swin-Transformer , and MLP-based ResMLP ), are susceptible to the histogram matching attack. For instance, the FD score of Inception shows an improvement of \(5.7\)% when the chosen set is used, and ResMLP exhibits a \(3.8\)% improvement (More quantitative

   Model & Inception & ConvNeXt & SWAV & MoCo-R & RepVGG & CLIP-R \\  Random & 2.81\(_{0.01}\) & 78.03\(_{0.10}\) & 0.13\(_{0.002}\) & 0.24\(_{0.003}\) & 129.61\(_{0.41}\) & 10.34\(_{0.06}\) \\ Chosen\({}_{I}\) & 2.65\(_{0.014}\) & 78.19\(_{0.11}\) & 0.13\(_{0.002}\) & 0.24\(_{0.003}\) & 129.67\(_{0.39}\) & 10.36\(_{0.08}\) \\  Model & Swin & ViT & DeiT & CLIP-V & MoCo-V & ResMLP \\  Random & 142.87\(_{0.12}\) & 15.11\(_{0.09}\) & 437.80\(_{0.14}\) & 1.06\(_{0.01}\) & 7.32\(_{0.03}\) & 99.11\(_{0.06}\) \\ Chosen\({}_{I}\) & 140.01\(_{0.12}\) & 15.11\(_{0.10}\) & 430.81\(_{0.16}\) & 1.06\(_{0.01}\) & 7.40\(_{0.03}\) & 95.36\(_{0.06}\) \\   

Table 2: **Quantitative comparison results of Fréchet Distance (FD\({}_{}\)) on FFHQ dataset**. “Random, Chosen\({}_{I}\)” respectively represent the synthesized distribution of randomly generated and matching the class prediction of Inception-V3. Moreover, “-R” and “-V” respectively denote the architecture of ResNet and ViT. (\({}_{}\)) indicates the results are hacked by the histogram matching mechanism. Notably, the values across different rows are not comparable and we report the stds of three testings to better illustrate the numerical fluctuation of various extractors towards the histogram attack. The results of Fréchet Distance (FD) on ImageNet can be found in _Supplementary Material_.

Figure 1: **Heatmaps from extractors with various architectures.** CNN-based extractors (_i.e.,_ Inception , ConvNeXt , SWAV , MoCo-R , CLIP-R , and RepVGG ) focus on objects whereas ViT-based (_i.e.,_ Swin-Transformer , ViT , DeiT , CLIP-V , and MoCo-V ) and MLP-based (_i.e.,_ ResMLP ) ones pour attention on wider areas.

and qualitative results of MLP-based extractors gMLP  and MLP-mixer  are shown in _Supplementary Material_). Namely, the FD score could be improved without making any changes to the generators, aligning with the observations of . We thus filter out the extractors that are vulnerable to the attack since their rankings can be manipulated without actual improvement to the generative models.

**Extractors that define similar representation spaces are redundant.** So far, we have demonstrated the importance of considering multiple extractors for a more comprehensive evaluation and filtered out the extractors that are susceptible to the histogram attack. However, it is also crucial to avoid redundancy among the remaining extractors, as they may define similar representation spaces. To address this, we examine the correlation between representation spaces across different feature extractors, following the approach outlined in . Specifically, a significant number of images (\(10K\) images from ImageNet) are fed into these extractors to compute their correspondence. After calculating the similarity matrix, we can further filter out extractors that define homogeneous representation spaces (the similarity analysis is presented in _Supplementary Material_). The remaining extractors are presented in the table below. These extractors 1) capture rich semantics in a complementary way, 2) are robust toward the histogram matching attack, and 3) define meaningful and distinctive representation spaces. Besides, both CNN-based and ViT-based extractors are considered and all of them have demonstrated strong performance for the pre-defined and downstream tasks, facilitating more comprehensive and reliable evaluation. Notably, the selection of self-supervised extractors SWAV, CLIP-ViT, and MoCo-ViT agrees with previous studies [41; 35; 3].

**The selected extractors can serve as reliable tools for synthesis evaluation.** In order to investigate the reliability of the selected extractors, we employ them to test the synthesis quality of representative generative models on the ImageNet dataset. Tab. 3 presents the quantitative FD scores of various extractors. Although their results differ in numerical scales, they consistently indicate that StyleGAN-XL outperforms both BigGAN and BigGAN-deep by a significant margin, and BigGAN-deep performs slightly better than BigGAN. Such observation also agrees with the human judgment in Tab. 5. Overall, the consistent trends observed across different extractors and the alignment with human judgment confirm that the selected extractors are reliable for comparing the synthesis quality.

### Distributional Distances

After the study of feature extractors, we shift our focus to another essential component of measurement, _i.e.,_ the distributional distance \(d()\). Besides Frechet Distance (FD), we also incorporate Centered Kernel Alignment (CKA) for our investigation.

**CKA provides normalized distances _w.r.t_ numerical scales in variable representation spaces.** Tab. 5 demonstrates the quantitative results of Centered Kernel Alignment (CKA). Unlike the Frechet Distance (FD) scores that exhibit significant fluctuations across various extractors, the CKA scores demonstrate remarkable stability when evaluated in different representation spaces. For instance, the FD score of BigGAN on MoCo-ViT is 238.78 whereas 3.35 on CLIP-ViT, making it challenging to combine the results of different extractors. By contrast, the stability of CKA scores allows the ability to combine results from multiple extractors (_i.e.,_ average) for better comparison.

**CKA demonstrates great potential for quantitative comparison and combination across hierarchical layers.** Here we investigate the distributional distances across various layers of the neural network, as these layers typically extract multi-level features that span from high-level semantics to low-level details. Accordingly, considering hierarchical features can provide a more comprehensive measurement. The left part of Tab. 4 presents the qualitative results of different layers' heatmaps. We can observe that different layers indeed extract different semantics, highlighting the importance of considering hierarchical features in evaluation. Additionally, we provide the quantitative results of FD and CKA in the right part of Tab. 4. Still, the FD scores of various layers fluctuate dramatically, _e.g.,_ the FD results of \(0.60\) in Layer\({}_{1}\) while 104.10 in Layer\({}_{4}\). Differently, the CKA results from hierarchical layers are comparable and the overall score could be derived by averaging multi-level scores. Importantly, the overall score still reflects synthesis quality consistently and reliably.

**CKA shows satisfactory sample-efficiency and stability under different number of samples.** Typically, the synthesis quality are measured between real and synthesized distributions, where the whole training data is used as the real distribution and \(50\) generated images as the synthesized, regardless of how many samples contained in the training data. However, when evaluating on the large-scale datasets (_e.g.,_\(1.28\) million images for ImageNet), \(50\) images may be insufficient to represent the entire distribution. Therefore, we study the impacts of the amount of generated samples. Concretely, we prepare several synthesized sets with different numbers of samples and calculate their distances to the real distribution (more details are presented in _Supplementary Material_).

Fig. 2 demonstrates the curves of FD and CKA scores evaluated under different data regimes on the FFHQ dataset. Obviously, the FD scores can be drastically improved by synthesizing more data regardless of different extractors until sufficient samples (\( 100\)) are used, whereas the CKA scores are stable under different data regimes. Moreover, CKA could measure the distributional distances precisely with only \(5\) synthesized samples, suggesting significant sample efficiency. Such observations demonstrate CKA's impressive adaptability toward the amount of synthesized data.

**Developing a reliable and comprehensive measurement system for synthesis evaluation.** Overall, a set of feature extractors that 1) are robust to the histogram matching attack, 2) capture sufficient semantics, and 3) define distinctive representation spaces could serve as reliable extractors for synthesis comparison. Together with a bounded distance (_i.e.,_ CKA) that is comparable across various representation spaces and hierarchical layers, as well as enjoys satisfactory sample efficiency. These two essential components constitute a reliable system to deliver the distributional discrepancy.

   Model &  &  &  \\  Layer & FD\({}_{}\) & CKA\({}_{}\) & FD\({}_{}\) & CKA\({}_{}\) & FD\({}_{}\) & CKA\({}_{}\) \\  Layer\({}_{1}\) & 0.60 & 99.06 & 0.54 & 98.95 & 0.05 & 99.84 \\ Layer\({}_{2}\) & 7.45 & 86.89 & 5.58 & 90.09 & 0.77 & 91.06 \\ Layer\({}_{3}\) & 30.24 & 82.80 & 23.55 & 83.63 & 6.11 & 85.75 \\ Layer\({}_{4}\) & 104.10 & 80.13 & 81.02 & 81.05 & 35.77 & 83.55 \\  Overall & N/A & 87.22 & N/A & 88.43 & N/A & 90.05 \\   

Table 4: **Heatmaps from various semantic levels on FFHQ dataset (_left_)** and Fréchet Distance (\(_{}\)) and Centered Kernel Alignment (\(_{}\)) scores on ImageNet dataset (_right_)**. CLIP-ViT serves as the feature extractor here, more results can be found in _Supplementary Material_.

Figure 2: **Fréchet Distance (FD) and Centered Kernel Alignment (CKA) scores evaluated under various data regimes on FFHQ dataset.** FID scores are scaled for better visualization. \(\) denotes the results fluctuate downward. The percentages represent the magnitude of the numerical variation. The curve of KID , Precision and Recall  can be found in _Supplementary Material_.

## 4 Benchmark Existing Generative Models

Based on the findings regarding the feature extractors and distributional distances, we construct a new synthesis valuation system. Concretely, our system leverages a set of models with both CNN and ViT architectures as feature extractors, namely, CNN-based ConvNeXt , RepVGG , SWAV  and ViT-based ViT , MoCo-ViT , CLIP-ViT , with which more comprehensive evaluation could be accomplished. Further, Centered Kernel Alignment (CKA) serves as the similarity indicator.

Accordingly, in this part we re-evaluate and compare the progress of existing generative models with our measurement system. Concretely, the latest generative models are re-evaluated with our system in Sec. 4.1, followed by our discussion about the diffusion models and GANs in Sec. 4.2. Notably, user studies are conducted for investigating the correlation between our system and human judgment.

### Comparison on Existing Generative models

In order to investigate the actual improvement of existing generative models, we collect multiple publicly available generators trained on several popular benchmarks (_i.e.,_ FFHQ, LSUN Church, and ImageNet) for comparison. Benefiting from the impressive sample efficiency of CKA, we generate \(50\)_K_ images as the synthesized distribution and use the whole datasets as the real distribution. Results from various selected extractors and their averaged scores are reported for a thorough comparison.

**Our system can measure the synthesis quality in a consistent and reliable way.** Tab. 5, Tab. 7, and Tab. 6 respectively demonstrate the quantitative results of different generative models on the ImageNet, LSUN Church, and FFHQ datasets. In most cases, CKA scores from various extractors and the overall scores provide a consistent ranking with FID scores, as well as agree with human perceptual judgment. These results suggest that our new metric could precisely measure the synthesis quality. However, for the Projected-GAN  and StyleGAN2  (_resp.,_ Aided-GAN ) evaluated on FFHQ (_resp.,_ LSUN Church) dataset in Tab. 6 (_resp.,_ Tab. 7), our evaluation system gives the opposite ranking to the FID. Namely, the quantitative results of StyleGAN2 (_resp.,_ Aided-GAN) are determined better than that of Projected-GAN under our evaluation, whereas the FID scores vote Projected-GAN for the better one. Additionally, the performances of ICGAN  and class-conditional ICGAN on ImageNet in Tab. 5 are identified basically the same by our metric, while FID scores indicate that the class-conditional one significantly surpasses the unconditional one.

   Model & FID\({}^{}\) & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall & User study \\  StyleGAN2  & 3.66 & 92.64 & 62.10 & 99.55 & 99.32 & 98.59 & 97.46 & 91.61 & 45\% \\ Projected-GAN  & 3.39 & 92.34 & 61.62 & 99.37 & 98.99 & 98.81 & 97.31 & 91.41 & 39\% \\ InsGen  & 3.31 & 94.17 & 65.72 & 99.60 & 99.36 & 98.90 & 97.75 & 92.58 & 58\% \\ EqGAN  & 2.89 & 94.54 & 64.36 & 99.69 & 99.49 & 99.10 & 98.61 & 92.63 & 62\% \\ StyleGAN-XL  & 2.19 & 93.78 & 67.59 & 99.68 & 99.49 & 99.25 & 97.33 & 92.85 & 66\% \\   

Table 6: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{}\)) on FFHQ dataset. \({}^{}\) scores are quoted from the original paper and others are tested three times.**

   Model & FID\({}^{}\) & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall & User study \\  ICGAN  & 15.60 & 62.65 & 72.25 & 86.10 & 97.04 & 77.99 & 92.16 & 81.37 & 32\% \\ ADM  & 10.94 & 63.12 & 72.90 & 87.71 & 97.39 & 78.69 & 92.92 & 82.12 & 45\% \\ BigGAN  & 8.70 & 63.89 & 74.62 & 87.94 & 97.60 & 79.60 & 93.25 & 82.82 & 53\% \\ C-ICGAN  & 7.50 & 62.53 & 72.20 & 86.12 & 97.05 & 78.01 & 92.08 & 81.33 & 31\% \\ BigGAN-deep  & 6.95 & 64.97 & 76.45 & 88.31 & 97.77 & 80.27 & 94.11 & 83.65 & 55\% \\ Guided-ADM  & 4.59 & 65.99 & 78.99 & 89.44 & 98.13 & 80.46 & 94.96 & 84.66 & 57\% \\ BigRoc  & 3.69 & 67.86 & 79.48 & 89.93 & 98.23 & 82.25 & 96.07 & 85.64 & 65\% \\ GigaGAN  & 3.45 & 68.01 & 79.93 & 90.15 & 98.34 & 82.40 & 96.52 & 85.89 & 65\% \\ DG-Diffusion  & 3.18 & 68.22 & 80.06 & 90.56 & 98.46 & 82.51 & 96.88 & 86.12 & 66\% \\ StyleGAN-XL  & 2.30 & 68.75 & 80.28 & 91.54 & 98.52 & 82.64 & 97.41 & 86.52 & 67\% \\ DiT  & 2.27 & 68.94 & 80.65 & 91.03 & 99.05 & 82.90 & 97.08 & 86.61 & 67\% \\ MDT  & 1.79 & 69.64 & 81.68 & 91.78 & 99.43 & 83.43 & 98.19 & 87.36 & 69\% \\   

Table 5: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{}\)) on ImageNet dataset. \({}^{}\) scores are quoted from the original paper and others are tested three times. To make our results more trivial to parse, we visualize the correlation between different metrics the human evaluation results in the _Supplementary Material_.**In order to compare the performance of these models in a fine-grained way, we further perform paired-wise human evaluation. Specifically, groups of paired images synthesized by different models (_e.g.,_ StyleGAN2 and Projected-GAN on FFHQ dataset) are randomly picked for visual comparison. Then, presented with two sets of images produced by different models, users are asked to determine which set of images is more plausible.

**Our measurement system provides the right rankings and better correlations with human visual judgment.** Tab. 8 presents the quantitative results of human visual comparison. Observably, the synthesis quality of StyleGAN (_resp._ Aided-GAN) is more preferred by human visual judgment. That is, our measurement system produces the same rankings as the human perceptual evaluation, demonstrating the reliability of our metric. Moreover, our metric's indication that there's no significant gap between ICGAN and class-conditional ICGAN is also verified by the human evaluation. Considering the perceptual null space of Inception-V3, one possible reason for the FID performance gains of Projected-GAN might be the usage of pre-trained models, which is also identified by . By contrast, our measurement produces the right rankings and agrees well with human evaluation, reflecting the actual synthesis quality in a comprehensive and reliable way.

### Comparison between GANs and Diffusion Models

Diffusion models [23; 54; 55; 14; 45; 48; 19; 70; 42; 24] have demonstrated significant advancements in visual synthesis and became the new trend of generative models recently. Benefiting from the reliability of our new measurement system, here we perform a comprehensive comparison between GANs and diffusion models. Specifically, we report the FID and the overall CKA scores, as well as human judgment for quantitative comparison. Additionally, the model parameters and the synthesis speed (tested on an A100 GPU) are also included.

Tab. 9 presents the quantitative results. Obviously, diffusion model (_i.e.,_ DiT) obtains comparable results with GAN (_i.e.,_ StyleGAN-XL), yet with much more parameters (_i.e.,_ 675\(M\)_vs._ 166.3\(M\)). Moreover, diffusion models usually require extra inference time to obtain realistic images. Such comparisons reveal that GANs achieve better trade-offs between efficiency and synthesis quality, and designing computation-efficient diffusion models is essential for the community.

### Comparing the performance of image-to-image translation

In order to testify the compatibility of our metric, here we employ our measurement system to evaluate the performance of image-to-image translation. We collect publicly available image-to-image translation models that are officially released to translate images from one domain to another domain for evaluation. Specifically, three translation benchmarks are involved here, namely Horse-to-Zebra [57; 71; 43], Cat-to-Dog [68; 43], and Dog-to-Cat [68; 25]. For each benchmark, we translate the tested images to the target domain following the original experimental settings. Then we compute the distributional discrepancies between the translated images and the real target images. Tab. 10 presents the quantitative results of the evaluated three image-to-image translation benchmarks. It can

   Model & FID\({}^{}\) & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall & User study \\  StyleGAN2  & 3.86 & 96.99 & 67.43 & 98.70 & 97.68 & 89.18 & 76.25 & 87.71 & 69\% \\ Diffusion-GAN  & 3.17 & 97.15 & 69.28 & 99.22 & 97.70 & 90.25 & 79.06 & 88.78 & 71\% \\ EqGAN  & 3.02 & 97.34 & 71.12 & 99.36 & 98.09 & 90.64 & 80.26 & 89.47 & 73\% \\ Aided-GAN  & 1.72 & 97.91 & 75.63 & 99.42 & 99.62 & 92.56 & 81.69 & 91.14 & 77\% \\ Projected-GAN  & 1.59 & 96.89 & 72.91 & 97.98 & 98.09 & 91.43 & 78.63 & 89.34 & 72\% \\   

Table 7: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{}\)) on LSUN Church dataset. \({}^{}\) scores are quoted from the original paper and others are tested three times.**

   Dataset &  &  &  \\  Model & Projected-GAN & StyleGAN2 & Projected-GAN & Aided-GAN & ICGAN & Conditional-ICGAN \\ User Preference & 45\% & 55\% & 43\% & 57\% & 50\% & 50\% \\   

Table 8: **Fine-grained investigation of human judgment. Percentages indicate the ratio of generated images that are considered to be more plausible.**be seen from these results that CKA provides consistent ranks with FID among various extractors, and the averaged score can reflect the performance of different image translation models. For instance, the performance of CUT  on Horse-to-Zebra is identified better than that of CycleGAN  by both FID and our proposed metric. And the qualitative results in the original paper of CUT  also suggest that the performance of CUT surpasses CycleGAN. That is, our measurement system can provide a reliable evaluation under such settings. This indicates that our measurement system can also be used for evaluating the performance of image translation tasks.

## 5 Conclusion

This work revisits the evaluation of generative models from the perspectives of the feature extractor and the distributional distance. Through extensive investigation regarding the potential contribution of various feature extractors and distributional distances, we identify the impacts of several potential factors that contribute to the final similarity index. With these findings, we construct a new measurement system that provides a more comprehensive and holistic comparison for synthesis evaluation. Importantly, our system could present more consistent measurements with human judgment, enabling more reliable evaluation.

**Discussion.** Despite a comprehensive investigation, our study could still be extended in several aspects. For instance, the impacts of different low-level image processing techniques (_e.g.,_ resizing) could be identified since they also play an important role in synthesis evaluation . Besides, comparing datasets with various resolutions could be further studied. Nonetheless, our study could be considered an empirical revisiting towards the paradigm of evaluating generative models. We hope this work could inspire more fascinating works of synthesis evaluation and provide potential insight to develop more comprehensive evaluation protocols. We will also conduct more investigation on the unexplored factors and compare more generative models with our system.