# Learning List-Level Domain-Invariant Representations for Ranking

Ruicheng Xian\({}^{1}\)\({}^{}\) Honglei Zhuang\({}^{2}\) Zhen Qin\({}^{2}\) Hamed Zamani\({}^{3}\)\({}^{}\) Jing Lu\({}^{2}\)

Ji Ma\({}^{2}\) Kai Hui\({}^{2}\) Han Zhao\({}^{1}\) Xuanhui Wang\({}^{2}\) Michael Bendersky\({}^{2}\)

\({}^{1}\)University of Illinois Urbana-Champaign

{rxian2,hanzhao}@illinois.edu

\({}^{2}\)Google Research

{hlz,zhenqin,ljwinnie,maji,kaihuibj,xuanhui,bemike}@google.com

\({}^{3}\)University of Massachusetts Amherst

zamani@cs.umass.edu

###### Abstract

Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call _item-level alignment_, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose _list-level alignment_--learning domain-invariant representations at the higher level of lists. The benefits are twofold: it leads to the first domain adaptation generalization bound for ranking, in turn providing theoretical support for the proposed method, and it achieves better empirical transfer performance for unsupervised domain adaptation on ranking tasks, including passage reranking.

## 1 Introduction

_Learning to rank_ applies machine learning techniques to solve ranking problems that are at the core of many everyday products and applications, including search engines and recommendation systems . The availability of ever-increasing amounts of training data and advances in modeling are constantly refreshing the state-of-the-art of many ranking tasks . Yet, the training of practical and effective ranking models on tasks with little to no annotated data remains a challenge . A popular transfer learning framework for this problem is _domain adaptation_: given source domains with labeled data that are relevant to the target domain of interest, domain adaptation methods optimize the model on the source domain and make the knowledge transferable by utilizing (unlabeled) target data.

For domain adaptation, there are task-specific and general-purpose (unsupervised) methods. E.g., for text ranking in information retrieval where we only have access to target documents without any training query or relevance annotation, a method is to use generative language models to synthesizerelevant queries for the unannotated documents, then train the ranking model on the synthesized query-document pairs [37; 54; 65]. For general-purpose methods, which are applicable to all kinds of tasks, perhaps the most well-known is _domain-invariant representation learning_[38; 17]. While it is studied extensively on classification and regression problems [71; 73], applications of invariant representation learning on ranking problems are sporadic [12; 58; 68], and the existing implementations lack theoretical justifications, which casts doubt on their applicability to learning to rank.

In this paper, we revisit the method of learning invariant representations for domain adaptation on ranking problems. We found that, from our review of prior work [12; 58; 68], existing implementations of the method all perform what we call _item-level alignment_ (Fig. 1 and Section 3.2.1), which aligns the distributions of the items being ranked (e.g., query-document pairs) aggregated from all lists and discards their _list structure_ (as in, e.g., q-d pairs with the same query all belong to the same list). However, both the data and the metrics for ranking are defined and computed on lists, not the items by themselves, so intuitively the list structure should be leveraged in the feature learning process as it is intrinsic to the problem. To close this discrepancy, we propose and analyze _list-level alignment_, which aligns the distributions of the lists and preserves their list structure (Section 3.2.2).

The conceptual leap from item-level to list-level alignment is significant. First, by analyzing list-level representation invariance, we establish the first domain adaptation generalization bound for ranking (Section 4), in turn providing theoretical support for our proposed method as well as a foundation for future studies. Second, list-level alignment provides empirical benefits to domain adaptation on ranking problems, which we demonstrate on two ranking tasks, passage reranking (Section 5) and Yahoo! LETOR web search ranking (Appendix D), where it outperforms zero-shot learning, item-level alignment, as well as a method specific to text ranking based on query synthesis .

## 2 Problem Setup

We define a ranking problem by a joint distribution \(\) of length-\(\) lists and their nonnegative scores, denoted by \(X=(X_{1},,X_{})\) and \(Y=(Y_{1},,Y_{})_{ 0}^{}\).1 For example, in information retrieval, the items in each list are query-document pairs of the same query, and the scores indicate the relevance of the documents to the query. Note that the lists are _permutation-invariant_, because switching the order of the items (and the corresponding scores) does not change the content of the list; concretely, let \(S_{}\) denotes the set of permutations on \([]\{1,2,,\}\), then permutation invariance means that \(((x_{1},,x_{}),(y_{1},,y_{}))=((x_{_{1}},,x _{_{}}),(y_{_{1}},,y_{_{}}))\) for all \((x,y)\) and \( S_{}\). We assume that the scores are a function of the list, \(Y=y(X)\), so the problem can be equivalently given by the marginal distribution \(^{X}\) of lists and a scoring function \(y:_{ 0}^{}\).

Learning to rank aims to learn a _ranker_ that takes lists \(x\) as input and outputs rank assignments \(r S_{}\), where \(r_{i}[]\) is the predicted rank of item \(i\), such that \(r\) recovers the descending order of the scores \(y\), i.e., \(y_{i}>y_{j} r_{i}<r_{j}\) for all \(i,j\). The more common setup, which we will adopt, is

Figure 1: Item-level and list-level alignment on a ranking problem. Each list contains \(\) items to be ranked. Whereas item-level alignment aligns the distributions of items but ignores the list structure, list-level alignment preserves this structure and aligns the distributions of lists.

to train a _score_, \(f:^{}\), with the rank assignments obtained by first computing the ranking scores \(s=f(x)^{}\), where \(s_{i}\) is the predicted score of item \(i\), then taking the descending order of \(s\) (alternatively, the rank assignments can be generated probabilistically; see Section 4).

To quantitatively measure the quality of the predicted ranks, we use (listwise) ranking metrics, \(u:S_{}_{ 0}^{}_{ 0}\), which computes a utility score via comparing the predicted rank assignments to the ground-truth scores of the list. Common metrics in information retrieval include reciprocal rank and normalized discounted cumulative gain [63; 26]:

**Definition 2.1** (Reciprocal Rank).: If the ground-truth scores are binary, i.e., \(y\{0,1\}^{}\), then the reciprocal rank (RR) of a predicted rank assignments \(r S_{}\) is

\[(r,y)=(\{r_{i}^{-1}:1 i,y_{i}=1\}\{0\}).\]

The average RR of a ranking model \(f^{}: S_{}\) on a dataset, \(_{(X,Y)}[(f^{}(X),Y)]\), is referred to as the mean reciprocal rank (MRR).

**Definition 2.2** (NDCG).: The discounted cumulative gain (DCG) and the normalized DCG (with identity gain function) of a predicted rank assignments \(r S_{}\) are

\[(r,y)=_{i=1}^{}}{(r_{i}+1)}, (r,y)=(r,y)}{(y)},\]

where \((y)=_{r^{} S_{}}(r^{},y)\), called the ideal DCG, is the maximum DCG of a list, which is attained by the descending order of \(y\).

**Domain Adaptation.** In this learning setting, we have a source domain \((_{S}^{X},y_{S})\) and a target domain \((_{T}^{X},y_{T})\) with different data distributions,2 and the goal is to learn a good ranking model for the target domain by leveraging all available resources: whereas access to source domain labeled training data is always assumed, we may only be given unlabeled data for the target domain (this scenario is called _unsupervised_ domain adaptation). A popular method for this adaptation is domain-invariant representation learning, which matches and aligns the source and target domain data distributions on the feature space so that their distributions appear similar to the model.

## 3 Learning Domain-Invariant Representations for Ranking

We begin by reviewing invariant representation learning for domain adaptation and the optimization technique of adversarial training, then describe and compare two instantiations of this framework to ranking problems in Section 3.2--item-level alignment, which is implemented in prior work, and our proposed list-level alignment.

For representation learning, we train composite models of the form \(f=h g\), where \(g:\) is a shared feature map and \(h\) is a task head on the shared features. As an example, if the model \(f\) is end-to-end implemented as an \(m\)-layer multilayer perceptron (MLP), then we could treat the first \((m-1)\) layers as \(g\) and the last as \(h\).

And, recall the definitions of Lipschitz continuity and Wasserstein distance :

**Definition 3.1** (Lipschitz Continuity).: A function \(f:\) from metric space \((,d_{})\) to \((,d_{})\) is \(L\)-Lipschitz, denoted by \(f(L)\), if \(d_{}(f(x),f(x^{})) L\ d_{}(x,x^{})\) for all \(x,x^{}\).

**Definition 3.2** (Wasserstein Distance).: The Wasserstein-\(1\) distance between probability measures \(p,q\) on metric space \(\) is denoted and given by \(W_{1}(p,q)=_{:,(1)} _{}(x)(p(x)-q(x))\,x\).

### Invariant Representation Learning via Adversarial Training

Given a source domain \(_{S}\) and a target domain \(_{T}\), invariant representation learning aims to learn a shared feature map \(g\) s.t. the distributions are aligned on the feature space, i.e., \(D(_{S}^{Z},_{T}^{Z}) 0\), where \(D\) is a divergence measure or probability metric, e.g., Wasserstein distance, and the source feature distribution \(_{S}^{Z}\) (analogously for target \(_{T}^{Z}\)) is defined to be the distribution of \(Z=g(X)\), \(X_{S}^{X}\). The idea is that a composite model with an aligned feature representation is transferable between domains--this is supported by the following generalization bound on classification problems . We will establish a similar result for ranking in Section 4.

**Theorem 3.3**.: _Let binary classification problems on a source and a target domain be given by joint distributions \(_{S},_{T}\) of inputs and labels, \((X,Y)\{0,1\}\). Define the error rate of a predictor \(f:\) on \(\) by \((f)=_{(X,Y)}[f(X)}[Y 1 ]+(1-f(X))}[Y 0]]\)3 where \(}[]\) is the indicator function._

_Let \(^{}\) be an \(L\)-Lipschitz class of prediction heads, and for any \(g\), define the minimum joint error rate by \(_{g}^{*}=_{h^{}}(_{S}(h^{} g)+ _{T}(h^{} g))\), then for all \(h\),_

\[_{T}(h g)_{S}(h g)+2L\ W_{1}(_{S}^{Z}, _{T}^{Z})+_{g}^{*}.\]

This result bounds the target domain risk of the composite model \(h g\) by its source task risk, plus the divergence of the feature distributions and the optimal joint risk. It therefore suggests that a good model for the target could be obtained by learning an informative domain-invariant feature map \(g\) (s.t. \(_{g}^{*}\) and \(W_{1}(_{S}^{Z},_{T}^{Z}) 0\)) and training \(h\) to minimize the source task risk \(_{S}\).

Invariant representation learning algorithms capture the aforementioned objectives with the joint training objective of \(_{h,g}(_{S}(h g)+\,D(_{S}^{Z},_{T}^{Z}))\), where \(_{S}\) is the source task loss, and \(>0\) is a hyperparameter that controls the strength of invariant representation learning. This objective learns domain-invariant features by optimizing \(g\) to minimize the distributional discrepancy, and minimizing source risk at the same time s.t. the learned features are useful for the task. Since it does not require labeled data but only unlabeled ones for estimating \(W_{1}(_{S}^{Z},_{T}^{Z})\), it is applicable to unsupervised domain adaptation. If labeled target data are available, they can be incorporated by, e.g., adding a target loss \(_{T}\), which will help keep \(_{g}^{*}\) low.

Adversarial Training.When the divergence measure \(D\) is simple and admits closed-form expressions, e.g., maximum-mean discrepancy , the objective above can be minimized directly , but they are typically too weak at modeling complex distributions that arise in real-world problems. To achieve feature alignment under stronger measures, e.g., JS divergence or Wasserstein distance, a well-known approach is adversarial training , which reformulates the above objective into

\[_{}(h,g)=_{h,g} _{S}(h g)-_{f_{}_{}}_{}(g,f_{}),\]

and optimizes it using gradient descent-ascent with a gradient reversal layer added on top of \(g\) in the adversarial component.

The adversarial component, \(-_{f_{}}_{}(g,f_{})\), can be shown to upper bound the divergence between \(_{S}^{Z},_{T}^{Z}\). It involves an adversary \(f_{}:\) (parameterized by neural networks) and an adversarial loss function \(_{}:\{0,1\}\), whose inputs are the output of \(f_{}\) and the domain identity \(a\) (we set target domain to \(a=1\)). The adversarial loss is computed over both domains:

\[_{}(g,f_{}):=_{X_{S}^{Z}}[_{ }(f_{} g(X),0)]+_{X_{T}^{Z}}[_{ }(f_{} g(X),1)].\]

If we choose the 0-1 loss, \(_{}(,a)=(1-a)}[ 0]+a }[<0]\), then the adversarial component upper bounds \(W_{1}(_{S}^{Z},_{T}^{Z})\) (see Proposition A.1); here, \(f_{}\) acts as a _domain discriminator_ for distinguishing the domain identity, and \(_{}\) is the balanced classification error of \(f_{}\). For training, the 0-1 loss is replaced by a surrogate loss, e.g., the logistic loss , \(_{}(,a)=(1+e^{(1-2a)})\).

### Invariant Representation Learning for Ranking

This section describes and compares two instantiations of the invariant representation learning framework above for ranking: item-level alignment, and our proposed list-level alignment. The key difference between them is the choice of \(^{Z}\) whose divergence \(W_{1}(_{S}^{Z},_{T}^{Z})\) is to be minimized.

Model Setup.We consider a composite scoring model \(f=h g\) where the feature map \(g\) is s.t. given an input list \(x=(x_{1},,x_{})\), it outputs a list of \(k\)-dimensional feature vectors, \(z=g(x)=(v_{1},,v_{})^{ k}\), where \(v_{i}^{k}\) corresponds to item \(i\). Ranking scores are then obtained by, e.g., projecting each \(v_{i}\) to \(\) with a (shared) linear layer. This setup, which we will adopt in our experiments, is common with listwise ranking models and in many ranking systems : in neural text ranking, each feature vector can be the embedding of the input text computed by a language model .

#### 3.2.1 Item-Level Alignment

In item-level alignment (ItemDA), the distributions of the \(k\)-dimensional feature vectors from all lists in aggregate are aligned , i.e., \(_{S}^{Z,}_{T}^{Z,}\), where

\[^{Z,}(v)_{Z^{Z}}(v Z)=_{ X^{X}}(v g(X)),(^{Z,})^{k}.\]

In other words (when both domain have the same number of lists), it finds a matching between the items of the source and target domains that pairs each target item with a source one (Fig. 2). Note that the list structure is not retained in this definition, because one cannot necessarily tell whether two items drawn from the bag of feature vectors, \(v,v^{}^{Z,}\), belong to the same list.

To implement item-level alignment, the discriminator \(f_{}\) (usually an MLP) is set up to take individual feature vectors \(v^{k}\) as input. In each forward-pass, the feature map \(g\) computes a batch of feature vector lists, \(\{z_{i}\}_{i[b]}=\{(v_{i1},,v_{it})\}_{i[b]}\), then the discriminator takes the items batched from all lists (this step discards the list structure), \(\{v_{ij}\}_{i[b],j[t]}\), and predicts their domain identities.

Item-level alignment is identical to invariant representation learning implementations for classification and regression, e.g., DANN , which also operate on "items", since neither the data nor the metrics in those problem settings have any explicit structural assumptions. However, ranking problems are inherently defined with list structures: the inputs to the model are organized by lists, and ranking metrics are evaluated on lists rather than the items by themselves. This discrepancy casts doubt on the applicability of item-level alignment to ranking problems, and moreover, it is unclear whether domain adaptation for ranking via item-level alignment is justified from a theoretical perspective.

#### 3.2.2 List-Level Alignment

Closing the discrepancy discussed above, we propose list-level alignment (ListDA), which aligns the distributions of the lists of feature vectors on the \(^{ k}\) space, i.e., \(_{S}^{Z,}_{T}^{Z,}\), where

\[^{Z,}(z)_{Z^{Z}}(z)=_{X ^{X}}(z=g(X)),(^{Z,})^{  k}.\]

This means finding a matching that not only pairs all source and target items, but also pairs items from the same target list to items from the same source list. Therefore, list-level alignment is a stronger requirement than item-level alignment, \(_{S}^{Z,}=_{T}^{Z,}_{S}^{Z,}=_{T}^{Z,}\), while the converse is generally not true (see Fig. 2 for a picture, or Example A.2)!

To implement list-level alignment, the discriminator has to make predictions on lists, \(z^{ k}\), i.e., in each forward-pass it would be fed a batch of feature vector lists, \(\{z_{i}\}_{i[b]}=\{(v_{i1},,v_{i})\}_{i[b]}\), and output \(b\) predictions. For this, a possible design choice is to flatten each feature vector list into a long \(( k)\)-dimensional vector and use an MLP as \(f_{}\). But recall from Section 2 that the

Figure 2: List-level alignment is a stronger requirement than item-level alignment, which, in addition to pairing target domain items to source items, it also pairs target lists to source lists.

input lists are permutation-invariant, and so are the feature vector lists. Since the above setup does not incorporate this property and an exponential amount of samples is required to see all possible permutations, this would be inefficient to optimize; yet, the optimality of \(f_{}\) is essential to the success of adversarial training. So instead, we use transformers with mean-pooling less positional encoding as the discriminator , which is permutation-invariant to the input.

Compared to item-level alignment, our list-level alignment is supported by a domain adaptation generalization bound (Theorem 4.7, established in the next section), and achieves better transfer performance in our Section 5 and Appendix D evaluations. These results suggest that the list structure is essential for an effective domain adaptation on ranking problems.

## 4 Generalization Bound for Ranking with List-Level Alignment

Based on the list-level alignment proposed above, we establish a domain adaptation generalization bound for ranking by analyzing list-level representation invariance. The discussions in this section consider learning a composite scoring model, \(f=h g:^{}\), but need not assume that the list feature representation \(z=g(x)\) can be decomposed into item-level feature representations as in the model setup of Section 3.2.2, only that it resides in a metric space \(\).

Our result can be viewed as an instantiation of the framework established originally for classification by Ben-David et al. . But since ranking is a very different task with its unique technical challenges, our analysis differs from those in prior work . We will introduce appropriate assumptions for handling these difficulties leading up to the main Theorem 4.7, which the readers may skip to.

The first hurdle is the discontinuity of the sorting operation. So rather than taking the descending order of the predicted scores to obtain the rank assignments, we generate them probabilistically using a _Plackett-Luce model_ with the exponentiated scores as its parameters . This makes the computation of the utility (Lipschitz) continuous w.r.t. the raw output scores of the model.

**Definition 4.1** (Plackett-Luce Model).: A Plackett-Luce (P-L) model with parameters \(w^{}_{>0}\) specifies a distribution over \(S_{}\), whose probability mass function \(p_{w}\) is

\[p_{w}(r)=_{i=1}^{}}}{_{j=i}^{}w_{I(r)_{j}}},  r S_{},\]

where \(I(r)_{i}\) is the index of the item with rank \(i\), \(r_{I(r)_{i}}=i\).

**Assumption 4.2**.: Given predicted scores \(s=f(x)^{}\), we generate the rank assignments probabilistic from a P-L model by \(R p_{(s)} S_{}\), where \(\) is taken coordinate-wise.

Under this assumption, the utility of a scorer \(f\) on \((^{X},y)\) w.r.t. ranking metric \(u:S_{}^{}_{ 0}_{ 0}\) is computed by (overloading \(u\) for future brevity)

\[_{}[u(f)]_{X^{X}}\ _{R p_{(f(X))}}[u(R,y(X))].\]

We define the risk (or suboptimality) of \(f\) as the difference between its utility to the maximum-attainable utility on the problem (e.g., the maximum is \(1\) when \(u=\)):

\[(f)=_{X^{X}}_{r S_{}}u(r,y(X) )-_{}[u(f)].\]

The next technical challenge is that unlike in classification where the perfect classifier is unique (i.e., achieving zero error), the perfect ranker is generally not: e.g., both rank assignments \((1,2,3)\) and \((2,1,3)\) achieve maximum utility (i.e., \(=0\)) on a list with ground-truth scores \((1,1,0)\). Prior analyses of domain adaptation leverage this uniqueness ; instead, ours does not rely on uniqueness (else the bound would be loose) with the following Lipschitz assumptions:

**Assumption 4.3**.: The ranking metric \(u:S_{}^{}_{ 0}_{ 0}\) is upper bounded by \(B\), and is \(L_{u}\)-Lipschitz w.r.t. the ground-truth scores \(y\) in the second argument (in Euclidean distance).

**Assumption 4.4**.: The input lists \(X\) reside in a metric space \(\), and the ground-truth scoring function \(y:^{}_{ 0}\) is \(L_{y}\)-Lipschitz (in Euclidean distance on the output space).

We will show that Assumption 4.3 is satisfied by both RR and NDCG. Assumption 4.4 says that similar lists (i.e., close in \(\)) should have similar ground-truth scores, and is satisfied, e.g., when \(\) is finite; such is the case with text data, which are one-hot encoded after tokenization.

**Assumption 4.5**.: The list features \(Z\) reside in a metric space \(\), and the class \(\) of scoring functions, \(h:^{}\), is \(L_{h}\)-Lipschitz (in Euclidean distance on the output space).

**Assumption 4.6**.: The class \(\) of feature maps, \(g:\), satisfies that \( g\), the restrictions of \(g\) to the supports of \(_{S}^{X}\) and \(_{T}^{X}\), \(g|_{(_{S}^{X})},g|_{(_{T}^{X})}\) respectively, are both invertible with \(L_{g}\)-Lipschitz inverses.

Assumption 4.5 is standard in generalization and complexity analyses, and could be enforced, e.g., with \(L^{2}\)-regularization . The last assumption is technical, saying that the inputs can be recovered from their feature representations by a Lipschitz inverse \(g^{-1}\). This means that the feature map \(g\) should retain as much information from the inputs (on each domain), which is a desideratum of representation learning. Note that this assumption does not conflict with the goal of invariant representation learning: there is always a sufficiently expressive \(\) s.t. \( g\) satisfying the assumption and \(_{S}^{Z,}=_{T}^{Z,}\).

We are now ready to state our domain adaptation generalization bound for ranking:

**Theorem 4.7**.: _Under Assumptions 4.2 to 4.6, for any \(g\), define the minimum joint risk by \(_{g}^{*}=_{h^{}}(_{S}(h^{} g)+_{T}(h^{} g))\), then for all \(h\),_

\[_{T}(h g)_{S}(h g)+4(L_{u}L_{y}L_{g}+B  L_{h})\ W_{1}(_{S}^{Z,},_{T}^{Z,})+ _{g}^{*},\]

_where \(^{Z,}\) is the marginal distribution of the list features \(Z=g(X)\), \(^{Z,}(z)^{X}(g^{-1}(z))\)._

The key feature of this bound is that it depends on list-level alignment, \(W_{1}(_{S}^{Z,},_{T}^{Z,})\), hence it provides theoretical support for the list-level alignment proposed in Section 3.2.2. It can be instantiated to specific ranking metrics by simply verifying the Lipschitz condition \(L_{u}\) of Assumption 4.3:

**Corollary 4.8** (Bound for MRR).: RR _is \(1\)-Lipschitz in \(y\), thereby_

\[_{_{T}}[(h g)]_{_{S}}[(h g)]-4(L_{y}L_{g}+ L_{h})\ W_{1}(_{S}^{Z,},_{T}^{Z, })-_{g}^{*}.\]

**Corollary 4.9** (Bound for NDCG).: _If \(C^{-1} C\) for some \(C(0,)\) on \((_{S}^{X},y_{S})\) and \((_{T}^{X},y_{T})\) almost surely,4 then NDCG is \((C)\)-Lipschitz in \(y\) almost surely, thereby_

\[_{_{T}}[(h g)]_{_{S}}[ (h g)]-(CL_{y}L_{g}+ L_{h})\ W_{1}(_{S}^{Z, },_{T}^{Z,})-_{g}^{*}.\]

## 5 Experiments on Passage Reranking

To demonstrate the empirical benefits of invariant representation learning with list-level alignment (ListDA) for unsupervised domain adaptation, we evaluate it on the passage reranking task5 and compare to zero-shot transfer, item-level alignment (ItemDA), as well as a recent method based on query synthesis . Note that our method is not specialized to text (re)ranking; in Appendix D, we evaluate ListDA on a web ranking task from the Yahoo! Learning to Rank Challenge.

In passage reranking, given a text query, we are to rank candidate passages based on their relevance to the query from a retrieved set. Reranking is employed in scenarios where the corpus is too large for using accurate but expensive models such as cross-attention neural rankers to rank every (millions of) documents. Rather, a two-stage approach is taken: a simple but efficient model such as sparse or dense retrievers (e.g., BM25  and DPR ) is first used to retrieve a candidate set of (hundreds of) passages, then a more sophisticated (re)ranking model is used to refine and improve their ranks.

We use BM25 to retrieve 1,000 candidate passages in the first-stage, and focus on the adaptation of a RankT5 listwise reranker , a cross-attention model derived from the T5 base language model with 250 million parameters . It takes the concatenation of the query and the document (q-d pair) as input and outputs an embedding. We treat the q-d embeddings computed on the same list as the list feature--consistent with the setting in Section 3.2. For the task training loss function, we use the softmax cross-entropy ranking loss: \((s,y)=-_{i=1}^{}y_{i}((s_{i})/\!_{j=1}^{}(s_{j} ))\).

**Datasets.** In our domain adaptation experiments, we use the MS MARCO dataset for passage ranking  as the source domain, which contains 8 million passages crawled from the web covering a wide range of topics with 532,761 search query and relevant passage pairs. The target domains are biomedical (TREC-COVID , BioASQ ) and news articles (Robust04 ). The data are preprocessed consistently with the BEIR benchmark ; their paper includes dataset statistics.

Since the training split of the target domain datasets does not contain q-d relevance annotations, our setting is unsupervised. It also does not contain any queries, but these are required for performing domain adaptation, because the input lists are defined to consist of q-d pairs. To acquire training queries on the target domains, we synthesize them in a zero-shot manner following , by using a T5 XL query generator (QGen) trained on MS MARCO relevant q-d pairs. QGen synthesizes each query as a seq-to-seq task given a passage, and the generated queries are _expected_ to be relevant to the input passages. See Table 7 for sample QGen q-d pairs.

**Methods.6** In **zero-shot** transfer, the reranker is trained on MS MARCO and evaluated on the target domain without adaptation. In **QGen pseudolabel** (QGen PL), we treat QGen q-d pairs synthesized on the target domain as relevant pairs, and train the reranker on them in addition to MS MARCO. This method is specific to text ranking, and is used in several recent works on domain adaptation of text retrievers and rerankers .

**ItemDA** (item-level alignment) is the prior implementation of invariant representation learning for ranking , and is set up according to Section 3.2.1 with adversarial training described in Section 3.1. The adversarial loss is aggregated from the losses of five three-layer MLP discriminators (no improvements from using wider or more layers), \(_{i=1}^{5}_{}(g,f_{}^{(i)})\), for reducing the sensitivity to the randomness in the initialization and training process . Our **ListDA** (list-level alignment) is set up according to Section 3.2.2, and the discriminator is an ensemble of five stacks of three T5 (transformer) encoding blocks. To predict the domain identity of a list feature \(z=(v_{1},,v_{})\), we feed the vectors into the transformer blocks all at once as a sequence, take the mean-pool of the \(\) output embeddings, then project it to a logit with a linear layer. A block diagram of ListDA is in Fig. 3.

Further details including hyperparameters and the construction of training lists (i.e., sampling negative pairs) are relegated to Appendix C, where we also include additional results such as using the pairwise logistic ranking loss, a hybrid adaptation method combining ListDA and QGen PL, and using transformers as the discriminator for ItemDA.

### Results

The main results are presented in Table 1, where we report metrics that are standard in the literature (e.g., TREC-COVID uses NDCG@20), and evaluate rank assignments by the descending order of the predicted scores. Since TREC-COVID and Robust04 are annotated with 3-level relevancy, the scores are binarized for mean average precision (MAP) and MRR as follows: on TREC-COVID, we map 0 (not relevant) and 1 (partially relevant) to negative, and 2 (fully relevant) to positive; on Robust04, 0 (not relevant) to negative, and 1 (relevant) and 2 (highly relevant) to positive.

Figure 3: Block diagram of the cross-attention RankT5 text ranking model with ListDA.

Across all three datasets, ListDA achieves the best performance, and the fact that it uses the same training resource as QGen PL demonstrates the added benefits of list-level invariant representations for domain adaptation. The favorable comparison of ListDA to ItemDA corroborates the discussion in Sections 3.2 and 4 that for domain adaptation on ranking problems, item-level alignment is insufficient for transferability, sometimes even resulting in negative transfer (vs. zero-shot). Rather, list-level alignment is the more appropriate approach.

### Analyses of ListDA

Quality of QGen.

An explanation for why ListDA outperforms QGen PL despite the same resources is that the sampling procedure (Appendix C.3) of negative q-d pairs could introduce false negatives into the training data. This is supported by the observation in  that QGen synthesized queries lack specificity and could be relevant to many documents. While these false negative labels are used for training in QGen PL, they are discarded in ListDA, which is thereby less likely to be affected by false negatives, or even false positives--when synthesized queries are not relevant to the input passages (see Table 9 for samples). Although out of the scope, better query generation is expected to improve the performance of both QGen PL and ListDA.

Size of Target Data.

Unsupervised domain adaptation requires sufficient unlabeled data, but not all domains have the same amount: BioASQ has 14 million documents (so is the total number of QGen queries, as we synthesize one per document), but Robust04 only has 528,155, and TREC-COVID 171,332.

In Fig. 4, we plot the performance of ListDA under varying numbers of target QGen queries (which is the number of target training lists). Surprisingly, on Robust04 and TREC-COVID, using just ~100 target QGen queries (0.03% and 0.06% of all, respectively) is sufficient for ListDA to achieve full performance! Although the number of queries is small, since we retrieve 1,000 documents per query, the amount of distinct target documents in those 100 lists could be substantial--up to 100,00, or 29.5% and 60.7% of the respective corpora.

The performance begins to drop when reduced to ~10 queries, which caps the amount of distinct documents at 10,000 (2.7% and 5.8%, respectively). The same data efficiency, however, is

   Target domain & Method & MAP & MRR@10 & NDCG@5 & NDCG@10 & NDCG@20 \\   & BM25 & 0.2282 & 0.6801 & 0.4396 & 0.4088 & 0.3781 \\  & Zero-shot & 0.2759 & 0.7977\({}^{}\) & 0.5857\({}^{}\) & 0.5340\({}^{}\) & 0.4856\({}^{}\) \\   & QGen PL & 0.2693 & 0.7644 & 0.5406 & 0.5034 & 0.4694 \\  & ItemDA & 0.2822\({}^{}\) & 0.8037\({}^{}\) & 0.5822\({}^{}\) & 0.5396\({}^{}\) & 0.4922\({}^{}\) \\  & ListDA & **0.2901\({}^{}\)** & **0.8234\({}^{}\)** & **0.5979\({}^{}\)** & **0.5573\({}^{}\)** & **0.5126\({}^{}\)** \\   & BM25 & 0.2485 & 0.8396 & 0.7163 & 0.6559 & 0.6236 \\  & Zero-shot & 0.3083 & 0.9217 & 0.8328 & 0.8200 & 0.7826 \\    & QGen PL & 0.3180\({}^{}\) & 0.8907 & 0.8373 & 0.8118 & 0.7861 \\  & ItemDA & 0.3087\({}^{}\) & 0.9080 & 0.8276 & 0.8142 & 0.7697 \\  & ListDA & **0.3187\({}^{}\)** & **0.9335** & **0.8693\({}^{}\)** & **0.8412\({}^{}\)** & **0.7985\({}^{}\)** \\   & BM25 & 0.4088 & 0.5612 & 0.4580 & 0.4653 & 0.4857 \\  & Zero-shot & 0.5008\({}^{}\) & 0.6465 & 0.5484\({}^{}\) & 0.5542\({}^{}\) & 0.5796\({}^{}\) \\    & QGen PL & 0.5143\({}^{}\) & 0.6551 & 0.5538\({}^{}\) & 0.5643\({}^{}\) & 0.5915\({}^{}\) \\   & ItemDA & 0.4781 & 0.6383 & 0.5315 & 0.5343 & 0.5604 \\   & ListDA & **0.5191\({}^{}\)** & **0.6666\({}^{}\)** & **0.5639\({}^{}\)** & **0.5714\({}^{}\)** & **0.5985\({}^{}\)** \\  Source domain is MS MARCO. Gain function in NDCG is the identity map. \({}^{}\)Improves upon zero-shot with statistical significance (\(p 0.05\)) under the two-tailed Student’s \(t\)-test. \({}^{}\)Improves upon QGen PL. \({}^{}\)Improves upon ItemDA. &  &  &  &  \\ 

Table 1: Reranking performance of RankT5 on top 1000 BM25-retrieved passages.

Figure 4: ListDA performance under different target sizes. Lower grey horizontal line is zero-shot, upper red line is ListDA using all QGen queries.

not observed on BioASQ, likely due to the size and hardness of the dataset, such as the use of specialized biomedical terms (see examples in Tables 7 to 9).

## 6 Related Work

**Learning to Rank and Text Ranking.** Learning to rank historically focuses on tabular data, for which, a wide range of models are developed , from SVMs , gradient boosted decision trees , to neural rankers [8; 42; 45]. Another traditional research direction is the design of ranking losses (surrogate to ranking metrics), which can be categorized into pointwise, pairwise, and listwise approaches [10; 7; 74; 24].

Recent advances in large neural language models have spurred interest in applying them on text ranking tasks [33; 46], leading to cross-attention models [22; 39; 40; 44] and generative models based on query likelihood [14; 77; 78; 50]. A different line of work is neural text retrieval models, which emphasizes efficiency, and has seen the development of dual-encoder [28; 70], late-interaction [29; 23], and models leveraging transformer memory .

**Domain Adaptation in Information Retrieval.** Work on this subject can be categorized into supervised and unsupervised domain adaptation. The former assumes access to labeled source data and (a small amount of) labeled target data (a.k.a. few-shot learning) . This work focuses on the unsupervised setting, where target domain data do not have annotations. Cohen et al.  apply invariant representation learning to unsupervised domain adaptation for text ranking, followed by Tran et al.  for enterprise email search, and Xin et al.  for dense retrieval. However, unlike our proposed list-level alignment method (ListDA), they learn invariant representations at the item-level. A recent family of adaptation methods is based on query generation [37; 65], originally proposed for dense retrieval.

**Invariant Representation Learning.** Domain-invariant representation learning is a popular family of adaptation methods [35; 17; 13], to which our proposed ListDA belongs. Besides ranking, these methods are also applied in fields including vision and language, and on tasks ranging from cross-domain sentiment analysis, question-answering [31; 61], and to cross-lingual learning and machine translation [67; 30].

Zhao et al.  and Tachet des Combes et al.  point out that on classification problems, attaining perfect feature alignment and high source accuracy are insufficient to guarantee good target performance. This occurs when the marginal distributions of labels differ, or the learned features contain domain-specific and nontransferable components. Although their findings do not directly apply to ranking, still, they suggest two potential directions for future work: one is whether Theorem 4.7 would admit a fundamental lower bound under distributional shifts in the ground-truth scores, and another is to modify ListDA to include a component for isolating out nontransferable features, as in domain separation networks .

## 7 Conclusion

We proposed an implementation of invariant representation learning for ranking via list-level feature alignment, and based on which, established a domain adaptation generalization bound for ranking. Our theoretical and empirical results illustrate the significance of preserving the list structure for achieving effective domain adaptation, which is overlooked in prior work.

A broader message is that when working with (feature) representations, either for domain adaptation or other purposes, they should be analyzed at the same level (or structure) at which the data are defined for the task of interest and the metrics are computed. The results of this paper is such an example--by moving from item-level alignment to the more appropriate list-level alignment, we extracted more potentials from invariant representation learning for domain adaptation on ranking problems.