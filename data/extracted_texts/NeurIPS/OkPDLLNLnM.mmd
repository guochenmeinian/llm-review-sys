# A Probabilistic Generative Method for Safe Physical System Control Problems

Peiyan Hu\({}^{2@sectionsign}\)1 Xiaowei Qian\({}^{1@sectionsign}\)1 Wenhao Deng\({}^{1}\) Rui Wang\({}^{3@sectionsign}\)2 Haodong Feng\({}^{1}\)

**Ruiqi Feng\({}^{1}\) Tao Zhang\({}^{1}\) Long Wei\({}^{1}\) Yue Wang\({}^{4}\) Zhi-Ming Ma\({}^{2}\) Tailin Wu\({}^{1}\)2**

\({}^{1}\)School of Engineering, Westlake University,

\({}^{2}\)Academy of Mathematics and Systems Science, Chinese Academy of Sciences,

\({}^{3}\)Fudan University

\({}^{4}\)Microsoft AI4Science

{hupeiyan,wutailin}@westlake.edu.cn, {xiaoweiqian0311}@gmail.com

Equal contribution. \(@sectionsign\)Work done as an intern at Westlake University. \({}^{}\)Corresponding author.

###### Abstract

Controlling complex physical systems is a crucial task in science and engineering, often requiring the balance of control objectives and safety constraints. Recently, diffusion models have demonstrated a strong ability to model high-dimensional state spaces, giving them an advantage over recent deep learning and reinforcement learning-based methods in complex control tasks. However, they do not inherently address safety concerns. In contrast, while safe reinforcement learning methods consider safety, they typically fail to provide guarantees for satisfying safety constraints. To address these limitations, we propose Safe Conformal Physical system control (SafeConPhy), which optimizes the diffusion model with a provable safety bound iteratively to satisfy the safety constraint. We pre-train a diffusion model on the training set. Given the calibration set and the specific control targets, we derive a provable safety bound using conformal prediction. After iteratively enhancing the safety of the diffusion model with the progressively updated bound, the model's output can be certified as safe with a user-defined probability. We evaluate our algorithm on two control tasks: 1D Burgers' equation and 2D incompressible fluid. Our results show that our algorithm satisfies safety constraints, and outperforms prior control methods and safe offline RL algorithms.

## 1 Introduction

The control of complex physical systems is critical and essential in many scientific and engineering fields, including fluid dynamics (Hinze & Kunisch, 2001), nuclear fusion (Edwards et al., 1992), and mathematical finance (Soner, 2004). In real-world scenarios, controlling such systems often requires addressing safety concerns (Barros & des Santos, 1998; Argomedo et al., 2013). For example, in fluid dynamics, small errors in control can lead to turbulence or structural damage, while in controlled nuclear fusion, failure to maintain safety constraints could result in catastrophic consequences. Safety, in this context, involves ensuring that the control sequences guide the system to satisfy pre-defined constraints, thereby mitigating risks and preventing hazardous situations (Dawson et al., 2022; Liu et al., 2023a). Notably, safety remains a bottleneck for applying machine learning to specific scientific and engineering problems, as many machine learning algorithms lack the mechanisms to guarantee safety constraints in their control outputs. This gap between performance and safety has become a critical obstacle in deploying machine learning for high-stake applications.

Despite of its importance, the safe control of complex physical systems is challenging. Firstly, to avoid unacceptable risks, one should prevent algorithms without safety guarantees from interacting with the environment, restricting us to an offline setting with pre-collected data. However, the data is often non-optimal and may contain unsafe samples, resulting in a significant gap between the observed data distribution and the near-optimal, safe distribution (Xu et al., 2022; Liu et al., 2023a). Secondly, the algorithm must balance the need for high performance with adherence to safety constraints (Liu et al., 2023a; Zheng et al., 2024).

After many years of research on traditional control algorithms (Li et al., 2006; Protas, 2008), the advancement of neural networks leads to the emergence of numerous deep learning-based algorithms (Farahmand et al., 2017; Holl et al., 2020; Hwang et al., 2022). For complex physical systems, which are highly nonlinear and high-dimensional, the deep learning-based methods achieve outstanding results (Hwang et al., 2022; Holl et al., 2020; Wei et al., 2024). However, the above deep learning-based methods generally do not account for safety considerations. Regarding safe offline reinforcement learning (RL), on the one hand, RL methods struggle to optimize long-term control sequences under the constraints of system dynamics (Wei et al., 2024). On the other hand, recent TREBI (Lin et al., 2023) and FISOR (Zheng et al., 2024) utilize diffusion models for planning and theoretically analyzing how to satisfy safety constraints, but they fail to compute the probabilistic bound of safety costs concretely. This limitation prevents their capability of certifying safety before testing, which is inconsistent with the goal of satisfying safety constraints using offline data.

To address these problems, we propose Safe Conformal Physical system control (SafeConPhy), an iterative safety improvement method with a certifiable safety bound. Firstly, in offline settings, the training data are often sub-optimal and unsafe, exhibiting a significant deviation from the desired distribution, which is optimal and safe. Inspired by concepts from conformal prediction (Vovk et al., 2005; Tibshirani et al., 2019), we estimate the model prediction error under distribution shift based on a portion of split-out training data (called _calibration set_) and the specific control targets. With the estimated prediction error, we compute a probabilistic upper bound of the safety score, and the safety score for the model's interaction with the environment will be within the upper bound with a user-defined probability. Thus, the model's safety can be certified by verifying whether the upper bound satisfies the safety constraint. Secondly, we implement a process to improve the model safety by leveraging the upper bound through guidance and fine-tuning iteratively. The guidance step directs the model to stochastically generate multiple samples that potentially satisfy the safety constraint, while the fine-tuning step updates the model by incorporating these samples and the safety bound. This safety improvement process is iterative and continues until the safety upper bound meets the safety constraints.

In summary, the advantages of SafeConPhy are highlighted in Table 1. Our main contributions are as follows: **(1)** We introduce safety constraints into the deep learning-based control of complex physical systems, develop two datasets for safe physical system control tasks to evaluate different methods, and propose the offline algorithm SafeConPhy. **(2)** Considering the model's prediction error, we provide a certifiable upper bound of the safety score and design an iterative safety improvement process that uses the upper bound to promote the output distribution becoming more optimal and safer. **(3)** We conduct experiments on 1D Burgers' Equation and 2D incompressible fluid, whose results demonstrate that SafeConPhy can meet the safety constraints and reach better control objectives at the same time.

## 2 Preliminary

### Problem Setup

We consider the following safe control problem of complex physical systems:

\[^{*}=*{arg\,min}_{}(, )(,)=0, s( ) s_{0},\] (1)

where \((t,):[0,T]^{d_{}}\) is the system's state trajectory with dimension \(d_{}\) and \((t,):[0,T]^{d_{}}\) is the external control signal with dimension \(d_{}\). They are both defined on the

  
**Methods** & **Complex Physical System** & **Safety Constraint** & **Certifiable** \\  DiffPhyCon (Wei et al., 2024) & ✓ & ✗ & ✗ \\ CDT (Liu et al., 2023b) & ✗ & ✓ & ✗ \\ TREB1 (Lin et al., 2023) & ✗ & ✓ & ✗ \\ 
**SafeConPhy (ours)** & ✓ & ✓ & ✓ \\   

Table 1: **Comparison between previous deep learning-based control algorithms and our proposed SafeConPhy. SafeConPhy considers the safety constraints in the control of complex physical systems, and its safety is certifiable before interacting with the environment.**time range \([0,T]\) and spatial domain \(^{D}\). \((,)\) is the objective of the control problem, and \((,)=0\) is the physical constraint, such as the partial differential equation. As for the safety constraint, \(s()\) is the safety score and \(s_{0}\) is the bound of the safety score. We need to minimize the control objective while satisfying physical constraints and constraining the safety score to stay below the bound, which requires a careful balance between safety and performance. However, it is important to note that safety and performance are not on equal footing, and the pursuit of a better objective should be built upon ensuring safety.

### Diffusion Models and Diffusion Control

Diffusion models (Ho et al., 2020) learn data distribution from data in a generative way. They present impressive performance in a broad range of generation tasks. Diffusion models involve diffusion/denoising processes: the diffusion process \(q(^{k+1}|^{k})=(^{k+1};}_{k},(1-_{k}))\) corrupts the data distribution \(p(_{0})\) to a prior distribution \((,)\), and the denoising process \(p_{}(^{k-1}|^{k})=(^{k-1}; _{}(^{k},k),_{k})\) makes sampling in a reverse direction. Here \(k\) is the diffusion/denoising step, \(\{_{k}\}_{k=1}^{K}\) and \(\{_{k}\}_{k=1}^{K}\) are the noise and variance schedules. In practice, a denoising network \(_{}\) is trained to estimate the noise to be removed in each step. During inference, the iterative application of \(_{}\) from the prior distribution could generate a new sample that approximately follows the data distribution \(p()\).

Recently, DiffPhyCon (Wei et al., 2024) applies diffusion models to solve the control problem as in Eq. 1 without the safety constraint \(() s_{0}\). For brevity, we only summarize its light version. It transforms the physical constraint to a parameterized energy-based model (EBM) \(E_{}(,)\) with the correspondence \(p(,)(-E_{}(,))\). Then the problem is converted to an unconstrained optimization over \(\) and \(\) for all physical time steps simultaneously:

\[^{*},^{*}=*{arg\,min}_{, }[E_{}(,)+(,)],\] (2)

where \(\) is a hyperparameter. To optimize \(E_{}\), a denoising network \(_{}\) is trained to approximate \(_{,}E_{}(,)\) by the following loss:

\[=_{k U(1,K),(,) p(,),(,)}[\|-_{}(_{k}}[,]+_{k}},k)\|_{2}^{2}],\] (3)

where \(_{k}:=_{k=1}^{k}_{i}\). After \(_{}\) is trained, Eq. 2 can be optimized by sampling from an initial sample \((^{K},^{K})(,)\), and iteratively running the following process

\[(^{k-1},^{k-1})=(^{k},^{k})- (_{}([^{k},^{k}],k)+ _{,}(}^{k},}^{k})+,,_{k}^{2})\] (4)

under the guidance of \(=\) for \(k=K,K-1,...,1\). Here \([}^{k},}^{k}]\) is the noise-free estimation of \([^{0},^{0}]\). The final sampling step yields the solution \(^{0}\) for the optimization problem in Eq. 2.

## 3 Method

In this section, we introduce our proposed method SafeConPhy, with its overall framework outlined in Figure 1. First, in Section 3.1, we briefly outline the overall steps of the algorithm. Next, in Section 3.2, we explain how conformal prediction is applied to estimate the safety score \(s\) under distribution shift in a probabilistic sense, and theoretically derive the formula for the provable safety bound \(s_{+}\). Finally, in Section 3.3, we detail the implementation of the entire algorithm. Specifically, we first describe the detailed implementation of this estimation. Additionally, we describe how the estimated safety score is utilized to modify the distribution of generated control sequences through guidance and fine-tuning.

### Overall Procedures

In this section, we present the workflow of our algorithm as in Figure 1. We set aside a portion of the original training data as the _calibration set_\(D_{}\), which will be used later to estimate the model's prediction errors. The remaining data, which will be used for actual training, is referred to as the training data \(D_{}\). After pre-training with \(D_{}\) as described in Eq. 3, we get the diffusion model \(p_{}\) which models the joint distribution of \([,]\).

Next, we conduct Iterative Safety Improvement, where we apply the provable safety bound \(s_{+}\) under distribution shift to enhance model safety iteratively. First, the calculation of the safety bound \(s_{+}\) runs throughout the entire loop. The safety bound basically takes the calibration set to obtain a corresponding set of model prediction errors (called the _score set_). Furthermore, we take into account the distribution shift between the calibration set and data generated based on control targets, so we apply weighting to the set of model prediction errors to get the _weighted score set_ as Eq. 9. Then, we use the quantile of this set to represent the error in the model's predicted safety score.

Second, 'iterative' refers to the process where we cyclically use the guidance to generate samples, and then combine these samples with model prediction errors to fine-tune the model parameters, thereby improving the model's safety. Specifically, in each iteration, we sample the control sequence \(\) under the guidance as described in Eq. 11 containing \(s_{+}\). After getting \(\), we conditionally sample \(}_{}()\) to get the provable safety bound \(s_{+}(}_{}())\). And we can now take the fine-tuning loss \(_{}\) involving the training data \(D_{}\) and also the progressively updated \(s_{+}(}_{}())\) as in Eq. 12 to fine-tune the model parameters \(\).

Finally, after several iterations, we use the fine-tuned diffusion model, once again under the influence of guidance, to generate the control sequences, which serve as the final output of the algorithm. The complete inference process can be seen in Algorithm 1.

```
1:Require Calibration set \(D_{}\), training set \(D_{}\), confidence level \(\), control objective \(()\), safety score \(s()\), number of iterations \(N\)
2:for\(n=1,,N\)do
3: Compute the weighted score set \(}\) with \(D_{}\)// Eq. 9
4: Get the quantile \(Q(1-;})\)
5: Sample the control sequence \(\) with guidance \(\)// Eq. 11
6: Compute \(s_{+}(}_{}())\) with conditionally sampled \(}_{}()\)// Eq. 10
7: Take gradient descent step on \(_{}_{}\)// Eq. 12
8:endfor
9: Sample the control sequence \(\) with guidance \(\)// Eq. 11
10:return\(\) ```

**Algorithm 1** Inference of SafeConPhy

### Provable Safety Bound with Conformal Prediction

In offline safety control problems, the gap between pre-collected data and the target distribution exacerbates models' prediction errors, which can be critical in ensuring safety. To address this issue, we employ the conformal prediction technique to obtain a provable safety bound, ensuring that the true safety score is included within this estimate with a provable level of confidence, without requiring additional assumptions about the model or the data distribution.

**Calibration set and score set.** To achieve the goal mentioned above, we first set aside a portion of the training dataset, which is not used for training, as the _calibration set_\(D_{}\). After training, we

Figure 1: **Overview of SafeConPhy**. First, we pre-train a diffusion model \(p_{}\) on the training data. Then, we derive a safety bound to certify that the model satisfies the safety constraints. To satisfy the safety constraint, we further design a loss function term based on the safety bound to optimize the diffusion model.

take the calibration set to obtain the _score set_, which is defined as

\[\{|s(}_{}(_{i}))-s(_{i})|:(_{i},_{i}) D_{}\}.\] (5)

Here \(_{i}\) and \(_{i}\) represent different samples, \(}_{}()\) is the system state conditioned on control \(_{i}\) and predicted by the model, and \(\) is the parameters of the model. The score set can be considered as recording the estimation errors of the model with respect to the safety score \(s\).

**Weighted score set under distribution shift.** However, since the data distribution in the calibration set differs from the final model-generated data distribution used for control, it does not satisfy the exchangeability1 condition required by conformal prediction (Vovk et al., 2005; Papadopoulos et al., 2002; Lei et al., 2016). Intuitively, each sample in the calibration set has a different probability of appearing in the final data distribution generated by the model, so we further apply weighting to the elements of the score set.

We define \(p(,)\) as the distribution of the calibration set. And we let \((,)\) be the distribution of the test data, where \(\) is generated by the model based on the control task and safety constraints, and \(\) is the true system state obtained from the interaction between the control sequence \(\) and the environment. According to conformal prediction under covariate shift (Tibshirani et al., 2019), the calculation of the weights is as follows:

\[(_{i},_{i})( _{i},_{i})}{p(_{i},_{i})}= (_{i})(_{i}| _{i})}{p(_{i})p(_{i}| _{i})},\] (6)

where \(p\) denotes the probability density function of distribution \(p\). Since \((_{i}|_{i})\) and \(p(_{i}|_{i})\) both represent the physical constraints of the system itself, the weights can be simplified as \((_{i},_{i})=(_{i })}{p(_{i})}\). We note that as described in Eq. 11, \(_{i}\) is generated by the energy-based model under guidance \(\). In the safe control problems, guidance \(\) encompasses both the control objective and safety constraints and will be detailed in Section 3.3. With the influence of \(\), \((_{i},_{i})(-E_{}(_{ i},_{i})-(_{i},_{i})) p_{ }(_{i},_{i})(-(_{i}, _{i}))\), where \(p_{}\) is distribution learned by the diffusion model. Thus we obtain

\[(_{i},_{i})=Cp_{}(_ {i},_{i})e^{-(_{i},_{i})}}{p(_{i},_{i})},\] (7)

where \(C\) is a constant. Given that the calibration set and the training dataset follow the same distribution, and assuming that the impact of the diffusion model's learning error on the dataset is sufficiently small relative to the second term \(e^{-(_{i},_{i})}\), we can approximate the weight as \((_{i},_{i})=Ce^{-(_{i},_{i})}\). Finally, we normalize the weight and obtain

\[(_{i},_{i})=( _{i},_{i})}}{_{(_{i},_{i}) D_{ }}Ce^{-(_{j},_{j})}}=( _{i},_{i})}}{_{(_{i},_{i}) D_{ }}e^{-(_{j},_{j})}}.\] (8)

The _weighted score set_ is then defined as

\[}\{(_{i},_{i})|s( }_{}(_{i}))-s(_{i})|:(_{ i},_{i}) D_{}\}.\] (9)

**Upper bound \(s_{+}\) on the confidence level \(\).** For a given control sequence \(\), we provide an upper bound \(s_{+}\) below, such that with at least \(1-\) probability, the true \(s\) is smaller than \(s_{+}\). In detail, we exploit the weighted score set to define the \(s_{+}\) as

\[s_{+}(}_{}())=s(}_{}( ))+Q(1-;}),\] (10)

where \(Q(1-;})\) is Quanti(\((1-)(1+}|});})\) and is still a differentiable function with respect to \(\), and \(|D_{}|\) is the cardinality of \(D_{}\). The precise and formal meaning of the probabilistic upper bound is demonstrated through the following lemma.

**Lemma 1**.: _Assume samples \((_{i},_{i}) p\) in the calibration set are independent, and the test set \((,)\) is also independent with the calibration set. Assume \(p\) is absolutely continuous with respect to \(\), \(s_{+}\) is defined as in Eq. 10, then \([s(()) s_{+}(}_{}( ))] 1-\), where \(()\) is the real system state corresponding to the control sequence \(\), and \(}_{}()\) is the system state predicted by the model conditioned on the same control._

### Target Control Generation Based on Estimated Safety

Next, based on the deduced \(s_{+}\), we describe the detailed implementation of modules in SafeConPhy.

**Conditionally sample \(}_{}()\).** In our proposed algorithm, we need to sample from the conditional distribution \(p(|)\) with the model that learns the joint distribution \(p(,)\). To achieve this, at each denoising step of the sampling process, we replace the noisy \(\) in the input of the denoising network with the actual clean or that serves as the condition (Chung et al., 2023). In fact, this situation represents a special case of the data distribution that the denoising network encounters during training, where the \(\) part is noisy, while the \(\) part remains noise-free.

**Guidance \(\)**. Guidance is the first method we adopt to steer the model's output toward satisfying both the control objectives and safety constraints. It plays a role during the sampling process of the diffusion model. When considering safety, the specific form of our guidance is as follows:

\[(,)=(,)+ [s_{+}(())-s_{0},0].\] (11)

Note that although we follow the previous symbol \(s_{+}(())\), during sampling, \(\) and \(\) are actually generated by the diffusion model jointly but not conditionally. The specific denoising step of implementing guidance follows Eq. 4.

**Fine-tuning.** The second method for adjusting the output data distribution of the model is fine-tuning, which achieves the adjustment by optimizing the model parameters \(\). Specifically, both terms in \(s_{+}\), as shown in Eq. 10, are functions of \(\). Therefore, it is both reasonable and effective to compute the gradient of \(s_{+}\) with respect to \(\) to take the gradient descent step. It is worth noting that retaining the computation graph for all denoising steps with respect to \(\) would result in an unmanageable memory overhead. Therefore, when we need to keep the computation graph during denoising for gradient calculation, we only retain the computation graph of the final denoising step.

To optimize the safety score and the diffusion loss (referred to Eq. 3) simultaneously, we form the fine-tune loss \(_{}\) as the weighted sum of both the safety loss \(_{}\) and diffusion loss \(_{}\):

\[_{}=_{}+ _{}\] \[=_{ D_{}}[s_{+}(}_{}())-s_{0},0]+_{(, ) D_{}}\|-_{} (}|,|+_{k}} ,_{0},k)\|_{2}^{2},\] (12)

where \(\) in the first term is from \(D_{}\) sampled according to the guidance described above, \((,)\) in the second term is from the training set \(D_{}\), \(k\) is the denoising step, \((0,)\), \(_{k}:=_{i=1}^{k}_{i}\) is the product of noise schedules and \([,]\) means the concatenation of \(\) and \(\).

## 4 Experiment

To verify our statements that SafeConPhy can both achieve safety and reach lower control objectives than other methods, we conduct experiments on safe offline control problems on 1D Burgers' equation and 2D incompressible fluid. Besides, to evaluate the quality of safety, for different problems, we provide several corresponding metrics.

For comparison, we choose Behavior Cloning (_BC_) (Pomerleau, 1988), and safe reinforcement and imitation learning methods involving _BC_ with safe data filtering (_BC-Safe_), Constrained Decision Transformer (_CDT_) (Liu et al., 2023b) and _CDT_ with safe data filtering (_CDT-Safe_), diffusion-based method _TREBI_(Lin et al., 2023). Note that _CDT_ shows the best performance in the Offline Safe RL benchmark OSRL (Liu et al., 2023a). In addition, we combine the physical system control method Supervised Learning (Hwang et al., 2022) with the Lagrangian approach (Chow et al., 2018) (_SL-Lag_) to enforce safety constraints. We also apply the classical control method _PID_(Li et al., 2006). We provide the anonymous code here.

### 1D Burgers' Equation

**Experiment settings.** 1D Burgers' equation is a fundamental equation that governs various physical systems including fluid dynamics and gas dynamics. Here we follow previous works (Hwang et al., 2022; Mowlavi and Nabi, 2023) and consider the Dirichlet boundary condition along with an externalforce \((t,x)\). This equation is formulated as follows:

\[=-(t,x)+(t,x)}{ x^{ 2}}+(t,x)&[0,T]\\ (t,x)=0&[0,T]\\ (0,x)=_{0}(x)&\{t=0\},\] (13)

where \(\) denotes the viscosity parameter, while \(_{0}\) signifies the initial condition. We set \(=0.01\), \(T=1\) and \(=\). Given a target state \(_{d}(x)\), the primary control objective \(\) is to minimize the control error between the final state \(_{T}\) and the target state \(_{d}\).

\[_{}|(T,x)-_{d}(x)|^{2} x.\] (14)

We define the safety score as:\(s()_{\{t,x\}[0,T]}\{(t,x)^{2}\}\). Considering the safety constraint \(s_{0}\), if \(s()>s_{0}\), the state trajectory \(\) is unsafe, and if \(s() s_{0}\), the state trajectory \(\) is safe. The bound of safety score \(s_{0}\) is set to 0.64 in our experiment. According to this bound, \(89.7\%\) of samples are unsafe among the training set, \(90\%\) of samples are unsafe among the calibration set and all of the samples in the test set are unsafe. More details can be found in Appendix F.1.

To better evaluate whether the set of state trajectories controlled by the model is safe, we define the normalized safety score:

\[s_{}_{1}|}_{i_{1}} _{i})}{s_{0}}+_{2}|}_{i _{2}}_{i})}{s_{0}},\] (15)

where \(_{1}=\{i_{i} s_{0}\}\), \(_{2}=\{i_{i}>s_{0}\}\). Note that \(s()\) and \(s_{0}\) are non-negative. If the state trajectories are all safe, \(s_{}\) is smaller than 1; If _any_ state trajectory is unsafe, \(s_{}\) is greater than 1. Additionally, we compute three unsafe rates to assess the safety levels of different methods' control results. \(_{}\) denotes the proportion of unsafe trajectories among total trajectories'; \(_{}\) denotes the proportion of unsafe timesteps among all timesteps; \(_{}\) denotes the proportion of unsafe spatial lattice points in all spatial lattice points across all time steps.

**Results.** In Table 2, We report the results of the control objective \(\), the safety score \(s_{}\) and other safe metrics of different methods. SafeConPhy can meet the safety constraint and achieve the best control objective at the same time. As shown in Figure 2, given the initial condition and the final state (control target), SafeConPhy can control a state trajectory that satisfies the safety constraint and control target. Other methods either suffer from constraint violations or suboptimal objectives.

### 2D Incompressible Fluid

**Experiment settings.** We then consider the control problems of 2D incompressible fluid, which follows the 2D Navier-Stokes equation. The control task we consider is to maximize the amount of smoke that passes through the target bucket in the fluid flow with obstacles and openings, while constraining the amount of smoke passing through the dangerous region under the safety bound (Wei et al., 2024). Specifically, referring to Figure 3, the control objective \(\) is defined as the negative rate of smoke passing through the target bucket located at the center top, while the safety score

   Methods & \(\) & \(s_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  BC & 0.0001 & 1.9954 & 38\% & 13\% & 1.2\% \\ BC-Safe & 0.0002 & 1.9601 & 14\% & 3\% & 0.2\% \\ PID & **0.0968** & **0.5691** & **0\%** & **0\%** & **0.0\%** \\ SL-Lag & 0.0115 & **0.6817** & **0\%** & **0\%** & **0.0\%** \\ CDT & 0.0026 & 1.9220 & 8\% & 1\% & 0.1\% \\ CDT-Safe & 0.0021 & 0.8570 & **0\%** & **0\%** & **0.0\%** \\ TREBI & 0.0074 & 0.7821 & **0\%** & **0\%** & **0.0\%** \\ 
**SafeConPhy (ours)** & **0.0011** & **0.8388** & **0\%** & **0\%** & **0.0\%** \\   

Table 2: **Results of 1D Burgers’ equation.** Gray: \(s_{}\) is greater than 1 (unsafe). Black: \(s_{}\) is smaller than 1 (safe). **Bold**: Safe trajectories with _lowest_\(\).

\(s\) corresponds to the rate of smoke entering the hazardous red region. It is important to note that there is a trade-off between controlling the flow through the hazardous region and achieving a more optimal control objective, which imposes higher demands on the algorithm. We set the safety score bound to \(s_{0}=0.1\).

Moreover, this control task is particularly challenging: not only does it require indirect control, which means that control can only be applied to the peripheral region, but the spatial control parameters reach as many as 1,792. Among all the training data, 53.1% of the samples are unsafe. The average safety score of the dataset is 0.3215. Other details can be found in Appendix G.1.

**Results.** We report results in Table 3. Here PID is inapplicable and SL-Lag fails to achieve reasonable control results. Due to the challenges of the task, no method can guarantee that all samples meet the safety requirements, so we introduce additional metrics to assess safety. Here we define the normalized safety score \(s_{}\) as \(s/s_{0}\) and define \(\) as the rate of unsafe samples. Additionally, we introduce another metric \([s-s_{0},0]\). When \(s\) does not exceed the bound \(s_{0}\), this metric is 0. If \(s\) exceeds \(s_{0}\), the metric reflects the amount by which it is surpassed. From the results, we can see that our method successfully keeps \(s_{}\) below 1, and other safety metrics are also comparable to other safe baselines (\(s_{} 1\)). Besides, among safe methods, SafeConPhy achieves the lowest \(\), even reaching control performance similar to methods that do not consider safety like BC.

## 5 Conclusion

In this paper, we introduce Safe Conformal Physical system control (SafeConPhy), a probabilistic generative method for safe control physical systems problems. Targeting the meaningful and important offline setting, we provide a provable probabilistic upper bound of the safety score. We then perform guidance and fine-tuning with this provable safety bound iteratively, improving safety and certifying it with a user-defined probability. Experiment results on 1D Burgers' equation and 2D incompressible fluid demonstrate that on the basis of satisfying safety constraints, SafeConPhy is able to achieve better control objectives. We believe that our method is beneficial for making deep learning-based physical control safer, improving the trustworthiness for deploying to the real world.

   Methods & \(\) & \(s_{}\) & \([s-s_{0},0]\) & \(\) \\  BC & -0.7125 & 7.3402 & 0.7160 & 88\% \\ BC-Safe & -0.2520 & **0.3463** & **0.0330** & **8\%** \\ CDT & -0.7133 & 3.0778 & 0.2726 & 34\% \\ CDT-Safe & **-0.6360** & **0.5073** & **0.0292** & **18\%** \\ TREBI & -0.7116 & 0.1273 & 0.1013 & 18\% \\ 
**SafeConPhy (ours)** & **-0.7035** & **0.6092** & **0.0380** & **14\%** \\   

Table 3: **2D incompressible fluid control results.** Gray: \(s_{}\) is greater than 1 (unsafe). Black: \(s_{}\) is smaller than 1 (safe). **Bold**: methods marked in black with lowest \(\).

Figure 3: **Visualization of the 2D incompressible fluid control problem.**

Figure 2: **Visualizations of the 1D Burgers’ equation.** The top row shows the original trajectory corresponding to the control target, and the bottom row is the trajectory controlled by SafeConPhy.