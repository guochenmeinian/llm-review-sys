# Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation

Arvind Murari Vepa

UCLA

amvepa@ucla.edu

&Zukang Yang

UC Berkeley

zukangy@berkeley.edu

&Andrew Choi

Horizon Robotics

asjchoi@ucla.edu

&Jungseock Joo

UCLA

jjoo@comm.ucla.edu

&Fabien Scalzo

UCLA

fab@cs.ucla.edu

&Yizhou Sun

UCLA

yzsun@cs.ucla.edu

###### Abstract

Deep learning has seen remarkable advancements in machine learning, yet it often demands extensive annotated data. Tasks like 3D semantic segmentation impose a substantial annotation burden, especially in domains like medicine, where expert annotations drive up the cost. Active learning (AL) holds great potential to alleviate this annotation burden in 3D medical segmentation. The majority of existing AL methods, however, are not tailored to the medical domain. While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs. Additionally, there is little focus on slice-based AL for 3D segmentation, which can also significantly reduce costs in comparison to conventional volume-based AL. This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models. We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical). Our findings demonstrate that our approach surpasses existing active learning techniques on both weak and full annotations and obtains superior performance with low-annotation budgets which is crucial in medical imaging. Source code for this project is available in the supplementary materials and on GitHub: https://github.com/arvindmvepa/al-seg.

## 1 Introduction

In the field of 3D medical segmentation, manual annotation of entire volumes, despite being laborious and time-consuming, has been the gold standard. Annotating a single 2D image can take minutes to hours depending on the complexity of the image [75; 73; 15; 71; 6], and a 3D medical volume, containing up to 200 slices, can require a significant amount of expert labor. Annotating a full dataset not only imposes a significant time burden on medical experts but also incurs high costs. Therefore, active learning (AL) is urgently needed to optimize annotation efforts.

Surprisingly, the potential of AL in the context of 3D medical segmentation has not been extensively explored. Traditional AL techniques typically focus on either diversity or model uncertainty, often neglecting relevant groupings within the data. For example, slices from the same patient or volume tend to show consistent characteristics. We propose a deep metric learning strategy that identifies and utilizes these similarities to better highlight diversity in the active learning process. Diverse sampleshelp the model learn a wide variety of patterns and features, which can be crucial for generalization. While medical imaging has natural groupings which we can leverage, our approach extends to a wide range of real-world datasets, including video segmentation.

Several other strategies may also help in reducing costs. Most current AL approaches use volume-based AL  rather than slice-based AL for 3D segmentation, which tends to be more costly and less efficient. Alternatively, weakly supervised methods, which require simpler annotations like scribbles [34; 29; 11], bounding boxes [13; 83], points , or semi-automated techniques [5; 44; 62; 56], have been shown to perform comparably to fully supervised methods [72; 40; 54; 13; 26; 59]. However, combining AL with weak supervision, especially in medical settings, remains unexplored. In our research, we explore both slice-based AL and the integration of AL with weak supervision as potential cost-reducing measures.

In this paper, we present our contributions to AL for 3D medical segmentation:

1. The first work to integrate deep metric learning with Coreset during active learning for 3D medical segmentation. Our approach shows superior performance across four datasets (medical and non-medical) with low annotation budgets.
2. The first work to comprehensively compare new and existing algorithms for slice-based active learning for 3D medical segmentation utilizing both weak and full annotations.

## 2 Related work

Active learningAL methods can be broadly classified into 1) uncertainty-based and 2) diversity-based methods . Uncertainty-based methods include deep bayesian methods [21; 45; 38; 65], deep ensembles [3; 14; 52; 32; 20], contrastive learning methods [85; 41; 87; 36], and geometry-based methods . Diversity-based methods include coreset-based methods [64; 7], clustering-based methods , variational adversarial learning methods [66; 37], and random sampling methods . A limitation of previous methods is that they are too general and fail to utilize common data groupings found in real-world datasets. Prior methods [67; 9; 69] have tried to incorporate groupings but do not utilize domain information to generate them, which can be suboptimal. Recent research has started integrating domain-specific data groupings into AL algorithms with some success[28; 79], but these methods are designed for specific uses and lack broad applicability. While other methods have adapted Coreset [31; 30], they are not specifically tailored to 3D medical segmentation as our method is.

Deep metric learningMetric learning is focused on developing methods to measure similarity between data points and used in many applications. Recently, metric learning has focused on deep learning-based feature representations for data points . Contrastive losses are popular for metric learning, including Triplet Loss  and NT-Xent loss . Several non-contrastive approaches have been proposed based on center loss [77; 17; 16], proxy-based methods [47; 70; 27], and LLM guidance . One interesting approach is ensemble deep metric learning which combines embeddings from an ensemble of encoders [1; 43; 53; 60; 80]. Recent work has improved on ensemble-based methods by factorizing the network training based on differerent objectives . However, these approaches can be computationally expensive and narrowly tailored to specific applications. Additionally, non-contrastive approaches often require class supervision to perform well. While some non-contrastive approaches do not [88; 18], they are also narrowly tailored to specific applications.

In contrast, contrastive learning is often used in self-supervised settings in diverse applications [10; 2; 55]. The NT-Xent loss specifically outperformed other contrastive losses in self-supervised zero-shot image classification and outperformed fully-supervised ResNet-50 . However, prior work (including in active learning [85; 23; 39]) does not leverage any inherent data groupings in the contrastive loss, which can be useful weak supervision. Additionally, recent work in active learning can be computationally expensive because they retrain the feature encoder after each active learning round .

Active learning in medical segmentationThere has been some notable research on AL in medical segmentation. Earlier work utilized bootstrapping to estimate sample uncertainty . In another work, the researchers built a mutual information-based metric between the labeled and unlabeled pools to improve diversity . However, both of these approaches are computationally expensive and not scalable for large datasets. There were also inefficiencies in previous studies, such as choosing whole volumes instead of individual slices in 3D segmentation, which increased costs [(39)]. Random sampling proved effective in some cases [(6)] and calculating uncertainty using stochastic batches was also effective [(20)]. None of the prior approaches leverage the data groupings inherent in medical 3D data and also do not focus on active learning with weak annotations (e.g., scribbles). While another method also utilizes groupings in medical data [(86)], their groupings are assumed to be quite large and it's unclear how they would extend their group classification approach to when there are a large number of patients, volumes, and adjacent slice groups like with our datasets.

## 3 Methodology

### Problem Formulation

In this section, we formally describe the problem of active learning for 3D segmentation. Let \(^{h w d}\) be the set of 3D volumes and \(\{0,1\}^{h w d k}\) be the set of 3D masks where \(h,w,d\) correspond to the height, width, and depth (number of slices) of a 3D volume and \(k\) refers to the number of classes. In 3D segmentation, we learn a mapping \(F:\).

Consider a loss function \(:\) where \(\) is the range of model prediction probabilities \((=^{h w d k})\). We consider the dataset \(D\) a large collection of data points which are sampled \(i.i.d.\) over the space \(=\) as \(\{_{i},_{i}\}_{i[n]} p_{Z}\). We additionally consider a partially labeled subset \(s D\). Thus, active learning for 3D segmentation can be formulated as follows:

\[}\;E_{,  p_{Z}}[(},)]\] (1)

where \(}\) are the model predictions \(F(V)\) (where \(V=[_{1},...,_{d}]\) is the volume), \( s\) is the optimal requested labeled set, \(k\) is the active learning budget, and \(F\) is the deep learning method learned from \(s s\). The Coreset approach is one method of solving Equation 1 (64). However, there are two distinguishing factors in our current work: 1) slice-based active learning for 3D segmentation and 2) deep metric learning. In slice-based 3D segmentation, we learn a mapping \(f\) from \(^{}^{}\) where \(^{}^{h w}\), \(^{}\{0,1\}^{h w k}\), and \(F(V)=[f(_{1}),...,f(_{d})]\). Additionally, \(D\) is a collection of slices which are sampled \(i.i.d.\) over the space \(^{}=^{}^{}\) as \(\{^{}_{i},^{}_{i}\}_{i[n]} p_{Z^{}}\).

In the original Coreset paper [(64)], \(s s\) is the set cover of \(D\) with radius \(\). However, the Euclidean metric used to calculate the radius does not utilize task-relevant information and may be sub-optimal. This motivates us to consider deep metric learning to utilize more task-relevant information, where \(d_{}(x_{1},x_{2})\) is some parameterized metric. In the the original paper [(64)], the authors provide a theorem

Figure 1: Overview of our active learning pipeline

which bounds the loss from Equation 1 based on the radius \(\). We show a very similar bound where \(=_{x_{1} D}\ _{x_{2} s}\ d_{}(x_{1},x_{2})\) (we defer the precise bound and proof to the Appendix B). This leads to our Coreset optimization formulation:

\[}\  D}{ }\  s}{}\ d_{}(x_{1},x_{2})\] (2)

The above formulation can be intuitively defined as follows; choose \(k\) additional center points such that the largest distance between a data point and its nearest center is minimized [(64)].

### Metric learning

The parameterized metric \(d_{}(x_{1},x_{2})\) is defined as:

\[d_{}(x_{1},x_{2})=_{2}(g_{}(x_{1}),g_{}(x_{2}))\] (3)

where \(_{2}\) is the Euclidean metric. Our goal is to learn \(g_{}\), or a feature representation, that emphasizes task-relevant similarities and differences in the data for selecting diverse samples for Coreset. We do this by training a contrastive learning-based encoder with a unique Group-based Contrastive Learning (GCL) which utilizes inherent data groupings specific to medical imaging to generate the feature representation. \(d_{}\) is then used by the Coreset algorithm to select the optimal set of slices. A flow chart illustrating our pipeline can be seen in Figure 1. The annotations for the collected slices are then used to train a segmentation model.

#### 3.2.1 Group-based Contrastive Learning for feature representation in metric learning

While there may be several task-relevant groupings in 3D medical imaging, it is not immediately apparent which of these would be useful for feature representation for metric learning for Coreset. For the ACDC dataset, we note the mean pairwise absolute deviation of the normalized training slice pixel values within different groups averaged over the dataset: 0.217 over the entire dataset, 0.159 within patient groups, 0.166 within volume groups, and 0.115 within adjacent slice groups. Note that the volume groups and the adjacent slice groups are the most and least diverse respectively. While intuitively the most diverse group would have the most important features for diversity, this does not necessarily indicate what combinations of groups would be helpful as well.

Instead, we propose a general group contrastive loss based on NT-Xent loss [(67)]. The NT-Xent loss focuses on generating and comparing embeddings for image pairs and their augmentations. It promotes similarity in embeddings for the same image and its augmentation while encouraging dissimilarity for different images. In the same vein, in our group-based loss, we promote similar embeddings for slices from the same group and dissimilar embeddings for slices from different groups, enhancing group-level representation. We define "group" as a set of 2D slices associated with one patient. The formula is as follows:

\[_{}=-_{i=1}^{N}_{j i,g_{j}=g_{ i}}^{N},z_{j})/)}{_{k=1}^{2N}_{[(k  i)((g_{k}=g_{i})(p_{k} p_{i}))]}(sim(z_{i},z_{k})/)}\] (4)

in which \(i\{1,2,...,N\}\) are the indices for the standard batch slices, \(j\{N+1,N+2,...,2N\}\) are the indices for the augmented batch slices, \(g_{i}\) refers to the group associated with slice \(i\) in the batch, \(p_{i}\) refers to the patient associated with slice \(i\) in the batch, \(G\) represents the average number of slices per group (calculated as the total number of unaugmented slices divided by the total number of groups), \(z\) is the embedding for a slice in the batch, \(\) is a temperature parameter, and \(sim\) is a similarity function which was cosine similarity in this study. The formula shares similarities with NT-Xent loss but introduces some modifications:

1. There are two summations which reflects all the group slices for a particular data point.
2. In the numerator of the logarithmic term, all the group slices for particular data point are considered similar, encouraging the development of similar embeddings for these slices.

3. The denominator excludes patient slices for a particular data point that are not part of the same group

Excluding non-group patient slices ensures that the model does not promote dissimilarity between non-group slices from the same patient. This ensures that we can sum multiple group losses together without counteracting their effects. In our study, we considered patient, volume, adjacent slice group contrastive losses in addition to NT-Xent loss. Our overall loss formulation is as follows:

\[_{}=_{}+_{1} _{}+_{2}_{}+ _{3}_{}\] (5)

where \(_{},_{}, _{}\) are the group contrastive losses associated with patient, volume, and adjacent slice groups respectively and \(\{_{1},_{2},_{3}\}\) are constants. The summation of different contrastive losses in our overall loss \(_{}\) is the focus of the ablation study in Section 4.6.

**Batch sampler** We employ \(_{}\) to train a SimCLR network (35) using a ResNet-18 encoder (24), which is our \(g_{}\). In practice, standard random data loading would yield minimal impact from \(_{}\) due to the low probability of randomly selecting two slices from the same group (e.g., same patient, same volume, or adjacent slices) within a batch. To address this, we introduce a batch sampler designed to increase their occurrence. The batch sampling process can be summarized as follows: 1. create a singular slice group for all the dataset slices, 2. for each group, randomly include a slice from the same patient for each of the groupings used (e.g., patient and volume), 3. combine groups from different patients to form a batch. An epoch consists of all the dataset groups: thus, more contrastive loss groups result in larger epochs. The batch sampler would be reset every epoch to ensure randomness during training. Please see Appendix C for more details.

Training the SimCLR network with this batch sampler eliminates the need for complex algorithmic adjustments to accommodate multiple contrastive loss groups, a challenge in other AL contrastive learning methodologies (85). We conduct the training over 100 epochs using an ADAM optimizer, with a learning rate of 3e-4, a weight decay of 1.0e-6, and a batch size of 8 for one or three groups and 9 for two groups.

#### 3.2.2 Segmentation model training

Once we have \(g_{}\), we can calculate \(d_{}\) and collect annotations for unlabeled slices to train our segmentation model. Our AL evaluation consists of several rounds of annotation collection from an oracle. After each round of annotation collection, we train five segmentation models and record the model's test score with the highest validation score. We repeat the AL experiment five times for each algorithm, each time with a different random seed, and report the average model test score per round. For weakly-supervised segmentation, we train a Dual-Branch Network with Dynamically Mixed Pseudo Labels Supervision (DMPLS) (40), which reported strong metrics on the ACDC dataset using weak supervision. For full supervision, we train UNet (58) which is a frequently used fully-supervised segmentation baseline model. We calculate the 3D DICE score on each 3D volume and report the average on all the volumes in the test set. For full supervision, we also provide results using a pre-trained segmentation model with a ResNet-50 backbone (42) (pre-trained on medical images (42) for medical datasets and ImageNetV2 for non-medical datasets). We do not report weakly-supervised results on pre-trained architectures because the weakly-supervised architectures cannot easily utilize pre-trained backbones.

## 4 Experiments

### Datasets

ACDC (Automated Cardiac Diagnosis Challenge)The ACDC dataset (4) consists of 200 short-axis cine-MRI scans from 100 patients in the training set and 100 scans from 50 patients in the test set. Acquired data was fully anonymized and handled within the regulations set by the local ethical committee of the Hospital of Dijon (France). Each patient has two annotated end-diastolic (ED) and end-systolic (ES) phase scans. Annotations are available for three structures: the right ventricle (RV), myocardium (Myo), and left ventricle (LV). Additionally, scribble annotations have been provided for each scan by a previous study (72). The training set size consists of 1448 slices.

CHAOS (Combined Healthy Abdominal Organ Segmentation)The CHAOS dataset (33) comprises of abdominal CT images from 20 subjects in the training set, primarily used for liver and vessel segmentation. The anonymized dataset was collected from the Department of Radiology, Dokuz Eylul University Hospital, Izmir, Turkey and the study was approved by the Institutional Review Board of Dokuz Eylul University. Each patient's CT scans contains approximately 144 slices. Binary segmentation masks for the liver are provided. We resampled, cropped, and normalized the images, following the process described in a previous study (72). We partitioned the training set into training (75%), validation (10%), and test (15%) subsets. The training set size consists of 2351 slices.

MS-CMR (Multi-sequence Cardiac MR Segmentation Challenge)The MS-CMR dataset (22; 89; 78) contains late gadolinium enhancement (LEG) MRI images from 45 patients who underwent cardiomyopathy. The data has been collected from Shanghai Renji hospital with institutional ethics approval and has been anonymized. These images were multi-slice, acquired in the ventricular short-axis views. We obtained realistic and manual scribble annotations from a prior study (84). Similar to this study (84), we split the data such that 25 patients were assigned to train, 5 to validation, and 20 to test, resulting in 382 slices in the training set. For data processing, we resampled the images into a resolution of 1.37x1.37 mm, and then they were cropped or padded to a fixed size of 212 x 212 pixels. During training, the image pixel values were normalized to zero mean and unit variance.

DAVIS (Densely Annotated Video Segmentation)The DAVIS dataset (50; 51) is a densely annotated video dataset associated with the 2016 DAVIS and 2017 DAVIS Challenge. The dataset collection was partially funded by SNF and human subject data was collected ethically, to the best of our knowledge. In our study, we utilized the train and val sets associated with the 2016 DAVIS Challenge which contained 30 and 20 videos respectively and all frames with 480p resolution. While we used the 2016 DAVIS train set, we created the val and test set by further splitting the 2016 DAVIS val set into 5 and 15 videos respectively. Our train set consisted of 2079 densely annotated frames.

For additional generalizability of our approach, we evaluated our methodology on both weak and full supervision, depending on data availability. Because the CHAOS, MS-CMR, and DAVIS dataset do not contain any hierarchical organization of multiple volumes/videos, we did not use the patient group loss. For video segmentation, in our contrastive loss we treated each video as a "volume" and each video frame as a "slice"; thus, we considered both the volume and adjacent slice group loss.

### Experimental settings

When conducting experiments with pre-trained segmentation models, for ACDC, CHAOS, and MS-CMR we collect annotations in cumulative increments of 2%, 3%, 4%, 5%, 10%, 15%, 20%, and 40% in each round. Because segmentation networks require more training data for natural images, for the DAVIS dataset we collect annotations in cumulative increments of 10%, 20%, 30%, and 40% in each round. When conducting experiments with pre-trained segmentation models, because of the benefit of prior pre-training, we collect annotations in cumulative increments of 1%, 2%, 3%, 4%, and 5% in each round.

Because solving the Coreset problem is NP-Hard, we utilized the K-Center Greedy algorithm for our Coreset implementation (48), which is a \(2-OPT\) solution (64) and produces very competitive results in comparison to other more computationally-intensive solutions. We compared our approach to vanilla Coreset (K-Center Greedy) (64), Random Sampling, CoreGCN (7), TypiClust (23; 39), Stochastic Batches (using Deep Ensembles with Entropy) (20), VAAL (66), Deep Ensembles (utilizing Variance Ratio scoring) (3), and Bayesian Deep Learning (utilizing the BALD score) (21). To ensure a fair comparison, all approaches were evaluated using the same experimental settings.

All of our experiments were primarily conducted with a single Tesla V100 GPU on an internal cluster. Our contrastive learning-based encoder and segmentation models consumed approximately 3 GB of GPU memory while training. The contrastive encoder's training speed was approximately 40 slices/second which resulted in a training time of 100 minutes, 200 minutes, and 300 minutes on ACDC for one, two, and three group losses respectively. One single AL experiment for our method on ACDC (eight rounds with five models trained per round) took approximately 24 hours and used about 400 MB of storage.

[MISSING_PAGE_FAIL:7]

consistently achieving the highest or close to the highest performance for a particular annotation level.

Additionally, our algorithm consistently demonstrates superior performance in both weak and full annotation scenarios, unlike other top-performing methods which struggle in one of these settings. Compared to other algorithms, our method shows superior performance on different datasets as well.

We note that clinically acceptable DICE scores for similar medical segmentation tasks range from 0.5-0.9, depending on the task (68; 25; 75; 8). However, even lower DICE scores can be clinically useful, especially for particular volumes with higher scores or in conjunction with semi-automated segmentation methods (57).

Figure 3: Qualitative comparison of our method, CoreGCN, and Coreset. Blue indicates agreement between model predictions and groundtruth masks and red indicates disagreement.

Figure 2: Describes the relationship between model performance and annotation time for our method utilizing weakly and fully-supervised 2D slices and random sampling of fully-supervised 3D volumes on the ACDC dataset. Annotation % is measured as the percentage of the fully-labeled ACDC training data. For weak supervision, we extrapolate the percentage of fully-labeled data based on equivalent annotation time (we follow prior work which assumes that annotators annotate scribbles 15x as fast as the full masks (72)). The dashed green line represents the performance of our method using weakly-supervised 2D slices with 40% of the ACDC training data.

Our fully-supervised experiments with pre-trained models are in Table 3. We saw improvements with our method in comparison to the best performing comparison methods. Please refer to Appendix D for comprehensive results across all datasets, including calculated bootstrapping standard errors.

### Relationship between model performance and annotation time

In Figure 2 we provided a graph which describes the relationship between model performance and annotation time for our method utilizing weakly and fully-supervised 2D slices and random sampling of fully-supervised 3D volumes on the ACDC dataset. To ensure a fair comparison between the different methods, we do not incorporate any of the results from the pre-trained segmentation models. For the 3D results, similar to the 2D U-Net, we train a 3D U-Net from scratch. Given comparable annotation time, our methods trained on both weakly and fully-supervised 2D slices far exceed the performance of random sampling of 3D volumes and achieve 3D volume maximum performance (with the given budget) with much less annotation time.

### Comparison with related work

Of the diversity-based comparison methods (Coreset, VAAL, TypiClust, Random), the performance for these three are generally worse than our method. For example, our method achieves the best performance on 21 out of 27 comparison points (Tables 1, 2, 3). The next closest is Random sampling, which achieves the best performance on 5 out of 27 comparison points. Of the entropy-based methods (BALD, Variance Ratio, Stochastic Batches), Stochastic Batches achieves the best performance on 4 out of 27 comparison points, the best out of the group.

   Coreset & NT-Xent & Patient & Volume & Slice & mDICE \\  ✓ & & & & & 58.5 \\ ✓ & ✓ & & & & 59.5 \\ ✓ & & ✓ & & & 60.0 \\ ✓ & & & ✓ & & **60.7** \\ ✓ & & & & ✓ & 58.1 \\  ✓ & & ✓ & ✓ & & 61.5 \\ ✓ & & & ✓ & ✓ & 60.9 \\ ✓ & ✓ & & ✓ & & **62.6** \\  ✓ & ✓ & ✓ & ✓ & & **65.4** \\ ✓ & & ✓ & ✓ & ✓ & 64.1 \\ ✓ & ✓ & ✓ & ✓ & ✓ & 61.1 \\   

Table 4: Ablation study based on the mean DICE scores for the 2-5% weak annotation datapointCoreset and CoreGCN share similarities with our method. However, on almost all the comparison points, they perform much worse. In Figure 3, there is a qualitative comparison of our method, CoreGCN, and Coreset on a difficult slice in the volume after several weak annotation rounds. With more requested annotations, our method is able to reduce errors even in difficult slices. With 5% weak annotation, while Coreset and CoreGCN still retain large error artifacts, our method has minimally visible errors.

### Ablation study

In Table 4, we see our ablation experiments. The first three sections represent our experiments with one, two, and three or more contrastive losses respectively. We note that all the contrastive loss experiments perform better than vanilla Coreset. Additionally, generally the larger combination of losses tend to outperform the smaller combination of losses. The best loss combination involves the volume group loss, patient group loss, and NT-Xent.

One interesting observation is that the loss associated with the volume group -- the most diverse group -- tends to produce the best additive performance and the loss associated with the the adjacent slice group -- the least diverse group -- tends to have the worst additive performance. We see this trend in the one loss experiments as well in the two loss and three or more loss experiments, where the combinations with the volume group loss tend to produce the best performance and the combinations with the adjacent slice group loss tends to produce the worst performance.

In order to visualize how effective the learned \(g_{}\) trained with different losses are for Coreset, we applied k-means clustering to the generated dataset features with a contrastive encoder trained with NT-Xent and trained with our optimal loss and visualized the quality of cluster labels using a t-SNE plot, which can be seen in Figure 4. We note that the clusters from our loss show good cohesion (tightly grouped), separation between clusters, and are easy to differentiate. However, the clusters from NT-Xent show much less cohesion (points are spread over more space) and the separation is less defined. Higher quality clustering emphasizes points are well separated which leads to better performance when trying to find representative points using Coreset.

## 5 Conclusion

In our research, we introduced a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By leveraging diverse data groups in our feature representation, we were able to learn a metric that promoted diversity and our Coreset implementation was able to outperform all existing methods on medical and non-medical datasets in weak and full annotation scenarios with a low annotation budget. Due to limited computational resources, we restricted the number of experiment runs and models we trained. We also acknowledge that we did not fully consider training set bias which can result in unfair outcomes for underrepresented groups. In future work, we hope to remedy some of these issues and focus more robustly on the applications of our approach in diverse domains.