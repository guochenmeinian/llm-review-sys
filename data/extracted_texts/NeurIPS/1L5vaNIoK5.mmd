# Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies

Yipu Chen*

Georgia Institute of Technology

ychen3302@gatech.edu

&Haotian Xue*

Georgia Institute of Technology

htxue.ai@gatech.edu

&Yongxin Chen

Georgia Institute of Technology

yongchen@gatech.edu

indicates equal contribution. Correspondence to: ychen3302@gatech.edu

###### Abstract

Diffusion models (DMs) have emerged as a promising approach for behavior cloning (BC). Diffusion policies (DP) based on DMs have elevated BC performance to new heights, demonstrating robust efficacy across diverse tasks, coupled with their inherent flexibility and ease of implementation. Despite the increasing adoption of DP as a foundation for policy generation, the critical issue of safety remains largely unexplored. While previous attack attempts have targeted deep policy networks, DP used diffusion models as the policy network, making it ineffective to be attacked using previous methods because of its chained structure and randomness injected. In this paper, we undertake a comprehensive examination of DP safety concerns by introducing adversarial scenarios, encompassing offline and online attacks, and global and patch-based attacks. We propose DP-Attacker, a suite of algorithms that can craft effective adversarial attacks across all aforementioned scenarios. We conduct attacks on pre-trained diffusion policies across various manipulation tasks. Through extensive experiments, we demonstrate that DP-Attacker has the capability to significantly decrease the performance of DP for all scenarios. Particularly in offline scenarios, DP-Attacker can generate highly transferable perturbations applicable to all frames. Furthermore, we illustrate the creation of adversarial physical patches that, when applied to the environment, effectively deceive the model. Video results are put in: https://sites.google.com/view/diffusion-policy-attacker.

## 1 Introduction

Behavior Cloning (BC)  is a pivotal area in robot learning: given an expert demonstration dataset, it aims to train a policy network in a supervised approach. Recently, diffusion models [16; 49] have become dominant in BC, primarily due to their strong capability in modeling multi-modal distribution. The resulting policy learner, termed Diffusion Policy (DP) [9; 18], can generate the action trajectory from a pure Gaussian noise conditioned on the input image(s). An increasing number of works are adopting DP as an action decoder for BC across various domains such as robot manipulation [12; 58; 7], long-horizon planning [35; 26] and autonomous driving .

Adversarial attack [31; 14] has been haunting deep neural networks (DNN) for a long time: a small perturbation on the input image will fool the DNN into making wrong decisions. Despite the remarkable success of diffusion policies in BC, their robustness under adversarial attacks [31; 14]remains largely unexplored, posing a potential barrier and risk to their broader application. While it is straightforward to attack an end-to-end DNN by applying gradient ascent over the loss function [31; 14], it is non-trivial to craft attacks against a DP, due to its concatenated denoising structure and high randomness. Prior research [25; 24; 56; 55; 45] has focused on attacking the diffusion process of the text-to-image (T2I) diffusion models . However, there are distinct differences between attacking a T2I diffusion model and attacking a Diffusion Policy. Firstly, they concentrate on attacking the diffused value while we aim at attacking the conditional image. In addition, they try to fool the editing process over the clean images (e.g. SDEdit ), while we are trying to fool the robot to make wrong actions step by step, each action is generated from the pure gaussian noise. Diffusion policies are also more interactive with the environment. A successful attack not only needs to fool a single inference output but also needs to continuously fool the model to decrease model performance.

In this paper, we focus on crafting adversarial attacks against DP. Specifically, we propose Diffusion Policy Attacker (DP-Attacker), the first suite of white-box-attack algorithms that can effectively deceive the visual-based diffusion policies. We investigate two hacking scenarios as illustrated in Figure 1: (1) digital attack-hacking the scene camera, which means that we can add imperceptible digital perturbations to the visual inputs of DP, and (2) physical attack-hacking the scene by attaching small adversarial patches  to the environments (e.g. table). Furthermore, we consider both offline and online settings, for online settings, we can generate time-variant perturbations based on the current visual inputs, on the opposite, for the offline settings we can only add one fixed perturbation across all the frames.

We conducted extensive experiments on DP pre-trained on six robotic manipulation tasks and demonstrated that DP-Attacker can effectively craft adversarial attacks against DP. For digital attacks, DP-Attacker can generate both online and offline attacks that significantly degrade the DP system's performance. For physical attacks, DP-Attacker is capable of creating adversarial patches tailored for each task, which can be put into the physical environment to disrupt the system. Also, we reveal that the non-robust image encoder makes the DP easy to attack.

## 2 Related Works

Diffusion-based Policy GenerationDiffusion models [49; 16; 48] exhibit superior performance in multiple domains like high-fidelity image generation, video generation and 3D generation [42; 39; 44; 47; 53; 41; 27]. Due to its strong expressiveness in modeling multi-modal distribution, diffusion models have also been successfully applied to robot learning areas such as reinforcement learning [54; 2], imitation learning [9; 58; 20; 38], and motion planning [43; 30; 18]. Among them, Diffusion policy (DP) [9; 58; 23] has gained significant attention due to its straightforward training

Figure 1: **Adversarial Attacks against Diffusion Policy: We aim to attack robots controlled with visual-based DP, unveiling hidden threats to the safe application of diffusion-based policies. (a) By hacking the visual inputs, we can fool the diffusion process into generating wrong actions \(\) (in red). We propose Diffusion Policy Attacker(DP-Attacker), which can effectively attack the DP by (b) hacking the global camera inputs \(I\) using small visual perturbations under both online and offline settings or (c) attaching an adversarial patch into the environment. The online settings use current visual inputs at \(t\)-th timestep \(I^{t}\) to generate time-variant perturbations \(^{t}\), while the offline settings use only offline data \(I^{}\) to generate time-invariant perturbations \(\).**

methodology and consistent, reliable performance. In this paper, we focus on crafting adversarial attacks against visual-based DP, a technology already integrated into various indoor robot prototypes like Mobile Aloha .

Adversarial Examples for Deep SystemsAdversarial attacks have been widely studied for deep neural networks (DNNs): given a small perturbation, the DNN will be fooled to make wrong predictions [51; 14]. For DNN-based visual recognition models, crafting adversarial samples is a relatively easy task using gradient-based budget-limited attacks [31; 57; 14; 5; 10; 3]. However, attacking diffusion models consisting of a cascade of DNNs injected with noise, poses a more complex challenge. Recent studies have demonstrated the feasibility of effectively crafting adversarial samples for latent diffusion models using meticulously designed surrogate losses [25; 59; 24; 46; 45; 56; 6]. However, these efforts have primarily focused on image editing or imitation tasks and are limited to working solely in latent space . Here we hope to explore the adversarial attacks against DP under various settings.

Adversarial Threats against Robot LearningPrevious research has highlighted adversarial attacks as a significant threat to robot learning systems , where small perturbations can cause chaos in applications such as deep reinforcement learning [22; 13; 28; 37; 50; 36], imitation learning , robot navigation , robot manipulation [19; 33], and multi-agent robot swarms . Despite the rising popularity of policies generated by diffusion models, to the best of our knowledge, there have been no prior efforts aimed at attacking these models in the field of robotics.

## 3 Preliminaries

### Diffusion Models for Behaviour Cloning

Diffusion models [49; 16] are one type of generative model that can fit a distribution \(q(x_{0})\), using a diffusion process and a denoising process. Starting from \(x_{K}\), a pure Gaussian noise, the denoising process can generate samples from the target distribution by \(K\) iterations of denoising steps (Here we use \(K,k\) to represent steps in diffusion and \(T,t\) for running timesteps of the robot scenarios):

\[x_{k}=_{k}(x_{k+1}-_{k}_{}(x_{k},k)+(0, _{k}^{2}I)),\;k=0,2,...,K-1\] (1)

where \(_{k},_{k},_{k}\) are hyper-parameters for the noise scheduler. \(_{}\) is a learned denoiser parameterized by \(\), which can be trained by optimizing the denoising loss termed \(=_{x,k}\|_{}(x+_{k},k)-_ {k}\|^{2}\). We define the reverse process in Equation 1 as \(x_{k}=_{}^{k}(x_{k+1})\) for simplicity.

Diffusion policies [18; 9] noted \(_{}\) apply the diffusion models mentioned above, resulting in \(^{t}_{}(s^{t})\), where \(^{t}^{D_{} L_{}}\) is the planned action sequences at timestep \(t\) in the continuous space, \(s^{t}\) is the current states, and \(D_{a}\), \(L_{a}\) are the action dimension and action length respectively. Accordingly, the learnable denoiser becomes \(_{}(_{k},k,s)\), and the denoised diffusion process remains the same. For visual DP, the states \(s^{t}\) are usually images captured by the scene or wrist cameras, so we use \(I^{t}\) throughout to represent the visual inputs at timestep \(t\). Finally, the policy can be formulated as

\[^{t}_{}(I^{t})=_{}^{0}(_{ }^{1}..._{}^{K-2}(_{}^{K-1}(x_{K},I^{t})... I^{t}),I^{t}).\] (2)

The equation above shows that the predicted action \(^{t}\) is the output of chained denoiser models residually conditioned on the current observation \(I^{t}\). In practice, while DP outputs a long sequence of actions \(\), we only execute the first few actions of it in a receding horizon manner to improve temporal action consistency .

### Adversarial Attacks Against Diffusion Models

Adversarial samples [14; 31; 5] have been widely studied as a threat to the AI system: for a DNN-based image classifier \(y=f_{}(x)\), one can easily craft imperceptible perturbations \(\) to fool the classifier to make wrong predictions over \((x)\). In digital attack settings [51; 14], the perturbation should be small and always invisible to humans, which can be formulated by the \(_{}\)-norm as \(|(x)-x|_{}<\) where \(\) is a small value (e.g. \(8/255\) for pixel value). Methods like FGSM  and PGD  can be easily applied to craft such kinds of adversarial attacks. For physical-worldadversarial patches [4; 11; 57; 17], \((x)\) is always crafted as attaching a small adversarial patch to the environments, and the patch should be robust to physical-world transformations such as position, camera view, and lighting condition.

Recent works [25; 45] show that it is also possible to craft such kind of adversarial examples to fool latent diffusion models  with an encoder \(\) and a decoder \(\): adding small perturbation to a clean image, the denoising process will be fooled to generate bad editing or imitation results. The following Monte-Carlo-based adversarial loss to attack a latent diffusion model:

\[_{adv}(x)=_{k}\|_{}((x)+ _{k},k)-_{k}\|_{2}^{2}\] (3)

the mechanism behind attacking latent diffusion models  turns out to be the vulnerability of the autoencoder and works only for the diffusion model in the latent space . Also, the settings above differs from our settings of attacking a DP which targets on attacking the conditional image without the ground-truth clean action to get the diffused input of \(_{}\) in Equation 3. In the following section, we show that we can still effectively craft different kinds of adversarial samples based on Equation 3 with some modification.

## 4 Methods

### Problem Settings

Threat ModelIn this paper, we assume that we have white-box access to some diffusion policy network. That is, we have access to its parameters and also the data used to train it. Given this trained network, we wish to find adversarial perturbations that, when added to the observation \(I\), will cause the trained diffusion policy to generate unwanted actions (either random or targeted) that impede task completion (lower the task score or success rate). We consider two types of perturbations detailed in Sec. 4.2 and Sec. 4.3.

The most straightforward way to measure the quality of the attack is to use the difference between generated actions from the original actions in an end-to-end manner:

\[_{}^{}(I,t)=-||_{}((I) )-^{t,*}||^{2}\] (3)

where \(^{t,*}\) is a known good solution sampled by \(_{}\) given the observation image \(I\), and \(()\) is some perturbation on the observation image. It could be generated either from the trained policy for online attacks or from the training dataset for offline attacks. One can minimize the negative L-2 distance between a generated action and a good action for untargeted attacks. For targeted attacks, the action loss becomes

\[_{}^{}(I,t)=||_{}((I) )-_{}^{t}||^{2}\] (4)

where \(_{}^{t}\) is some target bad action we wish the policy to execute (e.g. always move to left). We can use PGD  to optimize for the best perturbation that minimizes this loss. However, due to the inherent long-denoising chain of the diffusion policy \(_{}\), the calculation of this gradient could be quite costly .

In practice, running the end-to-end attacks above is not effective especially when the model is large and when we need to hack the camera at a high frequency. Instead, borrowing ideas from recent works [25; 24; 56] on adv-samples for diffusion models, we propose to use the following optimization objectives:

\[_{}^{}(I,t)=-_{k}\|_{ }(^{t,*}+_{k},k,(I))-_{k}\|^{2}\] (5)

where \(k\) is the timestep of the diffusion process and \(t\) is the timestep of the action runner. We add noise to the good solution \(^{t,*}\) and then calculate the L-2 distance between the predicted noise of the

Figure 2: **Design Space of DP-Attacker: the tree above shows the design space of DP-Attacker, which can be adapted to various kinds of attack scenarios, including global attacks (hacking and cameras) vs patched attacks (hacking the physical environment); offline vs online; targeted vs untargeted.**

denoise network and the added noise. Minimizing this loss leads to inaccurate noise prediction of the denoising network and, in turn, leads to bad generated action of the diffusion policy. For targeted attacks, the noise prediction loss is:

\[_{}^{}(I,t)=_{k}\|_{}( _{}^{t}+_{k},k,(I))-_{k}\|^{2}\] (6)

Minimizing this loss would allow the denoising net to favor the generation of the target action. The gradient of the noise prediction loss is easier to calculate compared to the action loss because of the short one-step chain. This makes it more favorable for conducting attacks.

### Global Attacks

A global attack injects adversarial perturbation \(\) into the observation image \(I\) by adding it on top of the observation image, _i.e._\((I)=I+\). The adversarial noise \(\) is of the same shape as the original image. To make the attack imperceptible, the adversarial noise's absolute value is limited by some \(\). To find such an adversarial noise, we use PGD , an optimization-based method to search for an adversarial noise. The adversarial noise can be constructed online during inference or offline using the training dataset. The algorithm for conducting an online global attack is shown in Algorithm 1. The algorithm optimizes for loss in Equation 5 or Equation 6. The algorithm can be modified easily to construct an offline attack. Given the training dataset \(D_{T}=\{(^{t},I^{t})|t T\}\) we can optimize for the loss \(_{}^{}(I,t)=-_{k,(^{t},I_{T})}\| _{}(^{t}+_{k},k,(I))-_{k}\|^{2}\) or \(_{}^{}(I,t)=_{k,(^{t},I^{t})}\| _{}(_{}^{t}+_{k},k,(I))- _{k}\|^{2}\). This algorithm is provided in the appendix.

```
0: given observation image \(I\), diffusion policy \(_{}\), noise prediction net \(_{}\), attack budget \(\), step size \(\), number of steps \(N\) adversarial noise \( 0\) for\(i=1\) to \(N\)do\(\) initialize adversarial noise \(_{k},k(0,I)\), \((1,K)\)\(\) sample forward noise and timestep if targeted attack then \(^{t}_{}^{t}+_{k}\)\(\) forward sample, \(_{}^{t}\) should be given \(s 1\) elseif untargeted attackthen \(^{t}_{}(I)\)\(\) use diffusion policy to generate a good solution \(^{t}^{t}+_{k}\)\(\) forward sample \(s-1\) endif \(_{p}_{}(^{t},k,(I+,0,1))\) \( s\|_{k}-_{p}\|^{2}\) \((-(_{I_{}}),-,)\)\(\) Projected-Gradient Descent endfor return\(\) ```

**Algorithm 1** Global Adversarial Attack (Online)

### Patched Attacks

A patched attack directly puts a specifically designed image patch \(x^{c h w}\) into the environment. The camera later captures it and causes undesirable motion from the diffusion policy. The patch should be active under different scales, orientations, and observation views. During training, we apply some random affine transform (shift, rotation, scale, and shear) \(\). The affine transform uses the center of the image as the origin of the coordinate system. The resulting patch replaces the original observation image using the replacement operator: replace\((I,x)\) again using the image's center as the origin of the coordinate system. To search for such a patch, we use the training dataset and optimize for the best patch using PGD. The algorithm is illustrated in Algorithm 2.

## 5 Experiments

We test the effectiveness of DP-Attacker with various strengths and configurations on different diffusion policies. Our target models are vision-based diffusion policy models introduced by Chi et al. . We aim to manipulate the visual input so that the generated trajectory will not lead to task completion. We quantitatively evaluate the effectiveness of our attack methods by recording the result task completion scores/successful rate. We also provide scores without attacks for reference and random noise attacks (adding some Gaussian noise to the observation images) as a baseline attack method. We foucus on the models released by Chi et al. . However, our attack algorithm applies to other variants of diffusion policies as well.

Environment SetupOur benchmark contains \(6\) tasks: PushT, Can, Lift, Square, Transport, and Toolhang. These tasks are illustrated in Figure 7 in the Appendix. Robosuite provides all the simulation of these tasks except for PushT [52; 32; 60]. For evaluation, we attack the released checkpoints of diffusion policies trained by Chi et al. . For tasks Can, Lift, Square, and Transport, each has two demonstration datasets: Multi-Human (MH) and Proficient Human (PH). The other two tasks (PushT and Toolhang) has only one PH dataset, respectively. This gives us a total of \(10\) datasets. In , each dataset is used to train two diffusion policies with different diffusion backbone architectures: CNN-based and Transformer-based. We take the best performing checkpoints for these \(20\) different scenarios released by Chi et. al  as our attack targets. For each attack method, we run 50 rollouts and collect the average score or calculate the success rate of the tasks. The rollout length uses the same length as the demonstration dataset [9; 32]. Besides our attack methods, we also run the rollout using clean images for reference and with random noise added as a baseline attack method. The evaluation is done using a single machine with an RTX 3090 GPU and AMD Ryzen 9 5950X to calculate rollouts and run our attack algorithms.

### Global Attack

We first present the results of global attacks. We evaluate both our online attack algorithm (creating adversarial noise on the fly per inference) and offline algorithm (pre-generating a fixed noise that is used for every inference).

Online AttackFor online attacks, we use attack parameters \(=0.03,=0.001875,N=50\). For targeted attacks, we use a normalized target action vector of all ones. We report the performance of the transformer-based models before and after the attack in Table 1. The results of global attacksFigure 4: **Physical Adversarial Patches:** we show the patches optimized by Algorithm 2, attaching it to the physical scene will effectively lower the success rate of the target diffusion policy.

Figure 3: **Global Attack (Online): We visualize the global attacks in Algorithm 1 within both the PushT and Can environments. Specifically, we present action rollouts for four types of observations: clean observations, observations perturbed with random Gaussian noise, and our optimized perturbations (both untargeted and targeted). While the DPs show robustness to random perturbations, they are vulnerable to adversarial samples generated using DP-Attacker.**

on all models are given in the appendix. Example rollouts and images used in the rollouts are shown in Figure 3.

Offline AttackFor offline global attacks, we train on the training dataset with batch size 64, \(=0.0001,=0.03\) for 10 epochs. The resulting trained adversarial noise is added to the input image for every inference. The results are shown in Table 1. Examples of rollouts and images used in the attack can be found on our website.

We find that diffusion policy is not robust to noises introduced by our DP-Attacker. The performance of diffusion policies is significantly reduced after running global attacks. A disturbance of less than 3% is able to decrease the performance from 100% to 0%. The success of offline global attacks also shows attacks can be cheaply constructed and pose a significant threat to the safety of using diffusion policy in the real world.

### Patched Attack

Training vs. EvaluatingSince patched attacks directly put an attack image into the environment, we only consider offline attacks that pre-generate some patch that is used throughout the rollout. We train the patch using Algorithm 2, where the patch is applied to the training image using some randomized affine transform. This allows the gradient to pass through for successful training. Since we have used random affine transforms during training, the patch should be transferable when used in the simulation environment. For evaluation, we create a thin box object with the trained image patch as its texture and put it randomly onto the table.

   Tasks & PushT &  &  &  &  &  \\ Demonstration Type & PH & PH & MH & PH & MH & PH & MH & PH & MH & PH \\  Clean & 0.75 & 0.92 & 0.92 & 1 & 1 & 0.92 & 0.72 & 0.86 & 0.46 & 0.86 \\ Random Noise & 0.66 & 0.88 & 0.98 & 1 & 1 & 0.82 & 0.74 & 0.84 & 0.48 & 0.82 \\  Targeted-Offline & 0.46 & 0.08 & 0.08 & 0.94 & 0.7 & 0 & 0 & 0 & 0.02 & 0 \\ Untargeted-Offline & 0.39 & 0.1 & 0.46 & 0.8 & 0.62 & 0.04 & 0 & 0 & 0 & 0 \\  Targeted-Online & 0.10 & 0 & 0 & 0.02 & 0 & 0 & 0 & 0 & 0 & 0 \\ Untargeted-Online & 0.19 & 0.02 & 0.02 & 0.62 & 0.62 & 0 & 0 & 0 & 0 & 0 \\   

Table 1: **Quantitative Results on Global Attacks:** The table includes the attack result for all transformer based diffusion policy networks. Our DP-Attack can significantly lower the performance of the diffusion models.

    &  &  &  &  \\  & CNN & Transformer & CNN & Transformer & CNN & Transformer & CNN & Transformer \\  Clean & 0.98 & 0.92 & 1 & 1 & 0.94 & 0.92 & 0.8 & 0.86 \\ Random Noise Patch & 0.9 & 0.94 & 1 & 0.9 & 0.8 & 0.54 & 0.56 & 0.12 \\  Untargeted-Offline & 0.16 & 0.44 & 1 & 0.82 & 0.72 & 0.34 & 0.48 & 0.02 \\   

Table 2: **Quantitative Results on Patched Attacks**

Figure 5: **Difference in Encoded Feature Vector**: we calculate the distance between the clean feature vector and the attacked feature vector. DP-Attacker perturb the feature vector significantly compared to naive random noise attack.

ResultsWe construct a patch of size that covers around 5% of the observation image using Algorithm 2. The details of the training can be found in the appendix. We evaluate the effectiveness of our patch attack algorithm on a total of 8 checkpoints, covering the PH dataset across four tabletop manipulation tasks (Can, Lift, Square, and Toblang) using both CNN and Transformer diffusion backbones. The result success rate (SR) is shown in Table 2. Example rollouts are shown in Table 4.

Simpler tasks such as Can and Lift are quite robust to random noise patch. Our DP-Attacker produces adversarial patches that perform better than random noise in terms of degrading the diffusion policy performance.

### Quantitative Results on Targeted Attacks

We qualitatively evaluate the effectiveness of our targeted attacks. We use our DP-Attacker to run global-online-targeted attacks with varying strength on two model checkpoints: PushT (CNN) and CAN (PH CNN). The target in PushT task is a 2D coordinate around \((323.875,328.75)\) (note the side length of the PushT environment is 1024), and the target in the CAN task is the target end-effector position around \((0.1686,0.1049,1.0848)\) (in meters). In 6, we report how close the actions generated by diffusion policy are to our attack targets during the rollout.

Our DP-Attacker is able to manipulate the generated action to be within 20 units and 5 cm of the attack targets, respectively, for the PushT task and CAN task, with an attack strength of \(=0.06\). Example rollouts of these two attack scenarios can be found in the Sec. D of the appendix.

## 6 Ablation Study

Attack ParametersTo investigate the effectiveness of our attack method, we evaluate how the attack parameter plays a role in DP-Attacker. First, we investigate the effect of the number of PGD steps \(N\). We keep the \(=0.03\), and \(=\). Second, we investigate the effect of the noise scale \(\). We keep \(N=50\), and \(=\). We evaluate all six attacks on the transformer backbone DP trained on the Lift PH dataset. The result is summarized in table 3.

End to End Loss vs. Noise Prediction LossWe perform a comparison with the end-to-end action loss 3. We evaluate both methods with the same attack parameters \((=0.03,=0.001875,N=50)\) on the best-performing transformer backbone trained on the PH dataset of the Lift task. Again, we evaluate 50 randomly initialized environments. The selection of end to end loss with DDPM 

   Parameters (\(=0.03\)) & \(N=10\) & \(N=20\) & \(N=50\) \\  Success Rate & 0.94 & 0.8 & 0.66 \\   Parameters (\(N=50\)) & \(=0.01\) & \(=0.03\) & \(=0.05\) \\  Success Rate & 1 & 0.68 & 0.32 \\   

Table 3: **Different Parameters for DP-Attack:** We did an ablation study on parameters \(\) and \(N\), and we can see that smaller steps and budgets are not enough to fool a DP. Larger budgets will dramatically decrease the Success Rate (SR).

Figure 6: We used DP-Attacker with different attack strengths to run rollouts. We report the average distance between the predicted action sequence and the target action sequence (a sequence of a duplicate target coordinate). The metric is calculated at each inference during the rollout.

scheduler makes it infeasible for online attacks. In addition, we provide results where we replace the loss-calculating noise scheduler with a DDIM-8 step scheduler . This provides speedup for calculating the end to end loss. The result SR after the attack and the average time used to perform the online attacks are shown in 4. The naive end-to-end loss is significantly lower than our attack algorithms and does not provide better results. We suspect that since diffusion models introduce randomness during the sampling of a trajectory, it is better to attack the noise prediction loss rather than the end to end action loss.

What Is Being Attacked Is the EncoderWe try to investigate further what exactly is attacked in our DP-Attacker. Other literature relating to text-to-image diffusion models shows that the encoder is the one being attacked . We suspect the same is happening for diffusion policy. To investigate this, we calculate the L2 distance between the encoded feature vector of clean and attacked images random noise attack, unsuccessful attack parameters, and successful parameters, respectively. The details of the calculation is in the appendix. We do this for 1000 images in the training dataset and plot the distribution of the distances using a violin plot in Figure 5. The significant difference shows that our attack method has drastically changed the representation of the conditional visual feature. This later affects the downstream conditional noise prediction net, causing it to make inaccurate noise predictions. We put details about it in the Appendix.

## 7 Conclusion and limitations

In this paper, we propose DP-Attacker, a suite of algorithms designed to effectively attack diffusion-based policy generation, an emerging approach in behavior cloning. We demonstrate that DP-Attacker can craft adversarial examples across various scenarios, posing a significant threat to systems reliant on DP. Our findings highlight that despite the inherent randomness and cascaded deep structure of diffusion-based policy generation, it remains vulnerable to adversarial attacks. We emphasize the need for future research to focus on enhancing the robustness of DP to ensure its reliability in real-world applications. There are also some limitations for this paper: our experiments were conducted exclusively within a simulation environment, and we did not extend our testing to real-world scenarios. Additionally, we did not develop or implement any defensive strategies for the proposed tasks, which remains an area for future research and exploration.