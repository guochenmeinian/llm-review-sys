# BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping

Taolin Zhang\({}^{1}\)  Jinpeng Wang \({}^{1}\)  Hang Guo \({}^{1}\)

Tao Dai\({}^{*}\)\({}^{2}\)  Bin Chen \({}^{3}\)  Shu-tao Xia \({}^{1,4}\)

\({}^{1}\) Tsinghua University \({}^{2}\) Shenzhen University

\({}^{3}\) Harbin Institute of Technology \({}^{4}\) PengCheng Laboratory

https://github.com/taolinzhang/BoostAdapter

Corresponding author: Tao Dai (daitao.edu@gmail.com)

###### Abstract

Adaptation of pretrained vision-language models such as CLIP to various downstream tasks have raised great interest in recent researches. Previous works have proposed a variety of test-time adaptation (TTA) methods to achieve strong generalization without any knowledge of the target domain. However, existing training-required TTA approaches like TPT necessitate entropy minimization that involves large computational overhead, while training-free methods like TDA overlook the potential for information mining from the test samples themselves. In this paper, we break down the design of existing popular training-required and training-free TTA methods and bridge the gap between them within our framework. Specifically, we maintain a light-weight key-value memory for feature retrieval from instance-agnostic historical samples and instance-aware boosting samples. The historical samples are filtered from the testing data stream and serve to extract useful information from the target distribution, while the boosting samples are drawn from regional bootstrapping and capture the knowledge of the test sample itself. We theoretically justify the rationality behind our method and empirically verify its effectiveness on both the out-of-distribution and the cross-domain datasets, showcasing its applicability in real-world situations.

## 1 Introduction

Vision Language models  have shown incredible performance in downstream vision tasks , such as classification , generation  and recognition . Among these models, CLIP  has been trained with large-scale noisy image-text pairs and can generalize well in zero-shot recognition tasks. The key idea behind CLIP is modality alignment during training and similarity comparison during testing for classification. However, CLIP suffers from domain shift problems during test-time inference. In the presence of out-of-distribution issues  that commonly appear in real-world scenarios, CLIP may fail to effectively align the feature across modalities, leading to performance degradation.

Test-time adaptation (TTA) has been widely explored in recent approaches  to mitigate misalignment issues and improve performance in downstream tasks. Current mainstream TTA methods can be divided into training-required methods and training-free methods, as depicted in Figure. 0(a) and Figure. 0(b). Training-required approaches  adjust model parameters or learnable prompts based on self-supervised objectives like entropy and increase the prediction confidence of model for distribution adaptation. TPT  applies entropy minimization to the vision-language model first. Furthermore, inspired by consistency regularization, TPT performs information miningfrom the test sample itself by random regional cropping in a self-bootstrapping style. However, training-required methods require gradient descent that is time-consuming with large training overhead, which prevents them from being applied in computationally limited situations. Training-free approaches [15; 52; 17] utilize memory networks, cache, or prototypes to store information regarding target samples and distributions, which is then used to adaptively modify the model's prediction. For example, TDA  leverages historical samples from the test data steam to build a dynamic key-value cache. It updates the prior knowledge encoded in CLIP through feature retrieval and output prediction based on the similarity between the test sample and the high-quality data stored in the memory bank. However, existing training-free approaches only consider interaction with other historical samples in the cache and do not effectively exploit the information within the test sample itself. This limitation prevents them from performing well especially in tasks that require fine-grained information.

Both of these approaches demonstrate excellent performance in enhancing the robustness of vision-language models to unknown distributions. However, the connection between them remains unclear. In this paper, we aim to answer three questions: (1) How are training-required methods like TPT and training-free methods like TDA connected? (2) How can we combine these two methods based on their shared nature? (3) Does vision-language models benefit from the combination of these methods?

In order to answer these questions, we first consider that the augmented images of test samples form a regional bootstrapping distribution of the original data. By filtering out the noisy augmentations based on mutual information with the predefined CLIP text embedding clusters, we can obtain a **boosting distribution** from which high-quality samples close to the target clusters can be drawn. Based on this, we delve into the connection between the target operations over the boosting distribution, _i.e._, cross-entropy optimizations and cache classifier, which reveals the shared nature between entropy-based and cache-based methods. Specifically, we pinpoint that with the samples derived from the bootstrapping distribution, entropy minimization over them performs equivalently to feature retrieval from the cache consisting of them. Motivated by this analysis, we propose a brand-new adaptation strategy, dubbed **BoostAdapter**, to improve training-free adapters by incorporating the samples derived from the boosting distribution to the memory bank. Particularly, the cache in BoostAdapter consists of instance-agnostic historical samples filtered from the test data stream, along with instance-aware boosting samples generated through regional bootstrapping from the sample itself. The interactions between intra-sample and cross-sample operations make BoostAdapter effective and efficient by

Figure 1: (a) Existing training-required TTA methods utilize self-supervised objective like entropy minimization for better generalization. (b) Existing training-free TTA methods perform feature retrieval on the historical samples to adjust the model prediction. (c) Performance comparison on the Out-of-Distribution benchmark and Cross-Datasets benchmark.

incorporating the idea of information mining from training-required methods while maintaining the efficiency of training-free methods. Theoretical analyses and empirical results are also provided to validate the effectiveness of BoostAdapter.

To summarize, we make the following contributions in this paper.

* We first discuss the relationship between training-required and training-free methods in test-time adaptation and establish connections between them.
* We propose BoostAdapter, a brand new adaptation strategy in test-time adaptation of vision-language models, which improves training-free adapters by introducing high-quality samples from regional bootstrapping into the memory.
* We theoretically derive target domain error bound of BoostAdapter and shows that BoostAdapter benefit from incorporating self-bootstrapping data.
* Extensive experiments conducted over two benchmark demonstrate the superior performance of BoostAdapter under test-time adaptation settings.

## 2 Related Works

Vision-Language Models have shown remarkable potential in generalization by contrastive pre-training over amounts of text-image pairs [16; 36; 24; 25]. One typical work is CLIP , which benefits from the alignment of 400 million curated image-text pairs and predicts the most relevant text description for a given image based on cosine similarity. Adapting CLIP to the downstream applications has attracted much attention and has been widely explored in recent approaches [55; 54; 52; 26; 56; 30]. CoOp  introduces learnable prompts [22; 51; 50; 28] and CoCoOp  conditions the text prompts on image embedding for better generalization. Maple  performs prompting for both vision and language branches and improves the alignment of the embedding between modalities. These approaches have demonstrated significant performance enhancements, but they still require few training data from the target domain. In contrast, we focus on test-time adaptation where there is no information about the target distribution and aim to generalize the model to any unknown scenarios.

Training-required Test-time Adaptation updates partial wights of the model like prompts [41; 39] or BN layer  with self-supervised objectives that benefit the downstream tasks without requiring additional training data. Tent  reduces generalization error on shifted data by test-time entropy minimization. For vision-language models, Test-time prompt tuning (TPT)  is a method that dynamically optimizes prompts during the testing phase, enhancing the model's zero-shot generalization ability. Specifically, TPT generates multiple augmented views of the test sample and then minimizes the entropy of the model's output logits across them to ensure consistent prediction. Recently, many works built upon TPT have been proposed to further enhance the performance of vision-language models. Particularly, DiffTPT  leverages the power of diffusion models to generate semantically consistent augmented images for entropy minimization. PromptAlign  bridges the gap between the test sample and source distribution by aligning token statistics, including mean and variance. Nevertheless, these approaches require gradient descent over the augmented images, which is computationally expensive and time-consuming.

Training-free Test-time Adaptation applies cache model or prototypes to make prediction of test samples in a non-parametric manner [15; 17; 53]. T3A  utilizes prototypes as downstream classifiers and dynamically adjusts the weights. AdaNPC  leverages the data from the source domain to address the issues of computation overhead and domain forgetting. For vision-language models, TDA  introduces both positive cache and negative cache to obtain high-quality test samples from the target domain. However, these methods only consider inter-sample interactions and may fail to generalize well when the downstream tasks require fine-grained knowledge or there is insufficient similarity across samples.

## 3 Methodology

### Preliminary

Problem setting.We begin by introducing the basic notations in test-time adaptation. We consider binary classification for simplicity and the theory can be easily extended to multi-classifications settings. Let \(p_{t}(x,y)\) denotes the joint distribution of image and labels in the target distribution, and we simply assume that samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) are drawn i.i.d. from the distribution with \(y_{i}\) represents the one-hot label.

**Definition 1**.: _(**Classification error.**) Given \(f\) as a binary classification function. The error incurred by hypothesis \(f:\{0,1\}\) under the distribution \(p_{t}(x,y)\) can be defined as_

\[(f)=_{p_{t}(x,y)}[f(x) y]=_{p_{t}(x,y)}[|f(x)-y |],\] (1)

_the last equality holds in a binary classification setting._

**Definition 2**.: _(**Excess error.**) Given the Bayes classifier under distribution \(p_{t}(x)\): \(f^{*}(x)=\{f(x) 1/2\}\) and the optimal classifier \(f^{*}\), the excess error of \(f\) is defined as_

\[(f)=(f)-(f^{*})=2_{x p_{t}(x)}[ |f(x)-|\{f(x) f^{*}(x)\}]\] (2)

**CLIP classifier** Let \(g\) be the image encoder of CLIP, \(C\) be the feature dimension, \(N\) denotes the number of categories, \(w_{i} R^{C}\) represents the \(i_{th}\) text embedding cluster. Considering normalized embedding \(w\) and \(g(x)\), we can derive a simplified version of the output of CLIP for class \(i\):

\[Z_{i}=w_{i}^{T}g(x).\] (3)

And we denote the output logits as \((x)=[Z_{1},Z_{2},...,Z_{N}] R^{N}\).

**Cache classifier** Given an unseen sample \(x\), encoder \(g\) with dimensional \(C\), cache size \(K\) and number of categories \(N\), the cache classfier conduct feature retrieval based on the similarity with the data \(\{(x_{i},y_{i})\}_{i=1}^{k}\) in the cache. The predictions based on Tip-Adapter  are as follows:

\[}(x)=A(g(x)G_{cache}^{T})Y,\] (4)

where \(A(z)=(-(1-z))\) denotes a scaling function with a weighting factor \(\) and a smoothing scalar \(\), \(G_{cache} R^{K C}\) represents the feature of \(K\) samples \(\{x_{i}\}_{i=1}^{K}\) in the cache and \(Y R^{K N}\) is the corresponding labels \(\{y_{i}\}_{i=1}^{K}\). Considering the number of samples in class \(y_{i}\), We can also derive a simplified version of Eq.(4) as follows, by ignoring the scaling function and adopting an instance-wise computation style:

\[}(x)=_{i=1}^{k}_{i}[g({x_{i}})^{T}g(x) ]y_{i},\] (5)

where \(_{i}=}}\) for class balance or \(_{i}=^{k}[g(x_{j})^{T}g(x)]}\) for normalization across all the samples.

### A Closer Look at Entropy-based and Cache-based Methods

We start with analyzing the filtering operation of augmentated images in TPT. Pseudo-labels tends to be noisy in the test time, and entropy can serve as a confidence metric to identify trustworthy samples

Figure 2: Connection between cross-entropy optimization and cache classifier over well-clustered samples with a frozen feature encoder. With optimization of cross-entropy, samples will pull the classifier weights closer of the same class while pushing them away from different class weights. Since the feature space is well-clustered, the classifier weights will ultimately converge near the feature center of the samples. Finally, the optimal classifier achieved through cross-entropy minimization will exhibit similar behavior with the cache classifier.

among augmented views [43; 41; 33]. These high-quality samples can be considered drawn i.i.d. from the so-called boosting distribution as defined below.

**Definition 3**.: _(Boosting Distribution.) Given a test sample from target distribution \(x p_{t}(x)\), let \(H()\) be the entropy measuring function and \(Aug()\) be the regional augmentation. By filtering noisy samples based on threshould \(\), we have the following property of boosting distribution \(p_{b}(x)\):_

\[ p_{b}(x)\{=Aug(x) H((x))\}\] (6)

We also terms the samples from the boosting distribution as **boosting samples**. Then we can connect entropy-based methods and cache classfier by the following proposition:

**Proposition 1**.: _(Informal) Given \(n\) samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) with a freeze encoder \(g\) that effectively performing feature clustering with respect to labels, the gradient descent optimization direction of the classifier's weights based on cross-entropy generally tends towards making predictions using the cache classifier with class balance weights defined in 5 on these samples._

An intuitive illustration of Proposition 1 is depicted in Figure 2, where the weights of optimal classfier behave like the feature centers across different classes with of the well-clusterd samples. Revisiting the entropy-based method TPT, when provided with high-quality boosting samples with low entropy drawn from the boosting distribution, the objective function of entropy minimization optimizes in a manner similar to conducting cross-entropy optimization over the pseudo-labels. According to Proposition 1, TPT performs similarly to the cache-based methods with a cache comprising the same boosting samples from the boosting distribution.

### Boosting your Training-free Adapters

Existing cache-based methods store historical test samples only as useful information for prediction. In light of the analysis above, we can integrate the idea behind TPT into these training-free adapters by incorporating boosting samples into the memory bank. In particular, each sample can participate in both inter-sample and intra-sample interactions with the instance-agnostic historical samples and the instance-aware boosting samples in the cache, respectively.

Specifically, with \(k_{t}\) selected historical samples and \(k_{b}\) selected boosting samples to comprise the cache, we extend the classifier defined in Eq.(4) and formulate our BoostAdapter as follows:

\[}(x)=A(g(x)_{cache}^{T}),\] (7)

where \(A\) is the same scaling function defined in Eq.(4), \(_{cache} R^{(k_{t}+k_{b}) C}\) denotes the features of the combination of both the historical and boosting samples, and \( R^{(k_{t}+k_{b}) N}\) is the label.

Figure 3: **Overall architecture of BoostAdapter.** BoostAdapter leverages knowledge from the target domain and employs self-bootstrapping with historical and boosting samples in the boosting cache, respectively.

Since we do not have access to the labels of the test samples, we generate one-hot pseudo-labels for them using argmax operations. However, these pseudo-labels tend to be noisy in the target domain. Therefore, we apply filtering based on entropy thresholds on the test data stream following  to obtain trustworthy historical samples. We employ a similar operation to select boosting samples from multiple augmented views of the current sample. In practice, we dynamically adapt the entropy thresholds \(\) for each test sample, with a fixed percentile \(p\). The cache continuously updates with lower entropy historical samples from the test data stream, while the current test sample augments the cache with self-boosting samples and forms an independent cache that only affects its own prediction. Additionally, to maintain diversity while considering the relevance to each test sample, we set a maximum shot capacity for each class \(k\) in the cache. This means that samples in the cache will be replaced by a lower-entropy historical sample or boosting sample when necessary.

An important issue is whether introducing boosting samples brings improvements to the training-free adapters. We will first make some necessary assumptions and then theoretically verify the effectiveness in reducing target error by incorporating samples from the boosting distribution.

**Assumption 1**.: _(Strong Density Condition) For any test sample \(x_{0}\) in the target distribution \(x_{0} p_{t}(x)\) and the boosting distribution \(p_{b}(x_{0})\), given positive lower bound \(m\) and upper bound \(M\), positive scaling constant \(c_{t}\) and \(c_{b}\), the radius bound \(R>0\), and \((x,r)=\{x^{}: x^{}-x r\}\) is the ball centered on \(x\) with radius \(r\). We assume \(p_{t}(x)\) and \(p_{b}(x_{0})\) are absolutely continuous with respect to the Lebesgue measure in \(^{d}\). For \(r(0,R]\), we assume_

\[[p_{t}(x)(x_{0},r)] c_{t}[ (x_{0},r)]\\ [p_{b}(x_{0})(x_{0},r)] c_{b}[(x_ {0},r)]\\ m<(x)}{d}<M;m<(x)}{d}<M,\] (8)

_where \(\) is the Lebesgue measure in Euclidean space._

**Assumption 2**.: _(L-Lipschitz Condition) Let \(f\) be the classification function and \(L\) be a positive constant. For all feasible \(x,x^{}\) we have \(|f(x)-f(x^{})| L x-x^{}\)._

**Assumption 3**.: _(Low Noise Condition). Let \(,C_{}\) be positive constants and we assume \(p_{t}(x)\) satisfies \(P_{x p_{t}(x)}(|f(x)-|<t) C_{}t ^{}\) for all \(t>0\)._

**Remark** Assumption 1 intuitively ensures that for any test sample, there is a surrounding neighborhood with a significant presence of samples from the target domain and the boosting distribution. More importantly, for a specific sample \(x_{0}\), boosting samples \(x p_{b}(x_{0})\) should be closer to \(x_{0}\) than other samples \(x p_{t}(x)\) from the target domain, _i.e._, generally, we have \(c_{t} c_{b}\). Assumption 2 and 3 describe the smoothness of functions and imply a high level of confidence in predictions around the threshold, respectively.

**Proposition 2**.: _(Historical Cache reduce Emperical Risk) Given \(f\) as the training-free classfier consisting of historical samples only defined by Eq.(4). Let \(n_{t}\) to be the number of confident previously predicted samples in the target domain and \(k_{t}\) as the number of historical samples in the cache, with assumptions 1-3, the following results hold with high-probability for large enough \(k_{t}\) and \(n_{t}\)._

\[(f)((})^{1/4}+( }{c_{t}n_{t}})^{1/d})^{1+}\] (9)

**Proposition 3**.: _(Historical Cache benefits from Boosting Samples) Let \(n_{t}\) to be all confident previously predicted samples in the target domain and \(n_{b}\) be the number of boosting samples that are drawn from the boosting distribution. Given \(k_{t}\) and \(k_{b}\) to be the number of historical samples and the number of boosting samples to be selected as the nearest neighbors stored in the cache, respectively. Let \(w_{ti}\) and \(w_{bi}\) be the weights defined in Eq.(5) of the historical samples and boosting samples. We have the following bound for the empirical risk of the cache classifier defined in 7._

\[(f)((+k_{b}})^{1/4}+ _{i=1}^{k_{t}}w_{ti}(}{c_{t}n_{t}})^{1/d}+\;_{i= 1}^{k_{b}}w_{bi}(}{c_{b}n_{b}})^{1/d})^{1+}.\] (10)

**Remark**  Proposition 2 provides a guarantee of the effectiveness of selecting \(k_{t}\) out of \(n_{t}\) historical samples to comprise the cache. The empirical risk is quite small when \(n_{t}\) since the cache captures the full information of the target domain. Proposition 3 demonstrates that the historical cache can further reduce empirical risk by incorporating \(k_{b}\) boosting samples.

## 4 Experiments

### Experimental Setup

**Datasets**  Following the setting in TPT , we conduct experiments on both Out-of-Distribution (OOD) benchmark and Cross-Domain benchmark. The OOD benchmark evaluates the model's robustness to natural distribution shifts on 4 ImageNet  Variants, including ImageNetV2 , ImageNet-Sketch , ImageNet-A  and ImageNet-R . We evaluate the transferring performance on 11 datasets in the Cross-Domain benchmark: Aircraft , Caltech101 , Cars , DTD , EuroSAT , Flower102 , Food101 , Pets , SUN397 ,and UCF101 . We follow the split in  and report the top-1 accuracy. The error bound are also provided.

**Implementation details**  We utilize a pre-trained ViT-B/16 of CLIP as the foundation model. In test-time adaptation, the batch size is set to be 1. We search for the optimal shot capacity to balance diversity and relevance of samples. For boosting samples, we utilize random crop and then random horizontal flip as augmentations. Moreover, we empirically set the entropy threshold percentile to \(p=0.1\) and filter 64 augmented views based on random cropping to obtain the boosting samples. and filter 64 augmented views to obtain the boosting samples. The top-1 accuracy and the error bound is reported on the test sets. All our experiments are conducted with a Nvidia 3090 24GB GPU.

    & Imagenet-V2 & Imagenet-Sketch & Imagenet-A & Imagenet-R & Average \\  CLIP  & 60.86 & 46.09 & 47.87 & 73.98 & 57.20 \\ CLIP+TPT  & 64.35 & 47.94 & 54.77 & 77.06 & 60.81 \\ CoOp  & 64.20 & 47.99 & 49.71 & 75.21 & 59.28 \\ CoOp+TPT  & **66.83** & 49.29 & 57.05 & 77.27 & 62.84 \\ Co-CoOp  & 64.07 & 48.75 & 50.63 & 76.18 & 59.91 \\ Co-CoOp+TPT  & 64.85 & 48.27 & 58.47 & 78.65 & 62.61 \\ Maple  & 64.07 & 49.15 & 50.90 & 76.98 & 60.28 \\ Maple + TPT  & 64.87 & 48.16 & 58.08 & 78.12 & 62.31 \\ PromptAlign  & 65.29 & 50.23 & 59.37 & 79.33 & 63.55 \\ DiffTPT  & 65.10 & 46.80 & 55.68 & 75.00 & 60.52 \\ TDA  & 64.67 & 50.54 & 60.11 & 80.24 & 63.89 \\  BoostAdapter & 65.51 & **51.28** & **64.53** & **80.95** & **65.57** \\   

Table 1: **Full results on the OOD benchmark with ViT-B/16 backbone.**  We report top-1 accuracy and “Average” is calculated by taking the mean accuracy across all four OOD datasets.

    &  &  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  &  \\  CLIP  & 93.35 & 88.25 & 65.48 & 67.44 & 83.65 & 23.67 & 62.59 & 44.27 & 42.01 & 65.13 & 63.58 \\ CLIP+TPT  & 94.16 & 87.79 & 66.87 & 68.98 & 84.67 & 24.78 & 65.50 & **47.75** & 42.44 & 68.04 & 65.10 \\ CoOp  & 93.70 & 89.14 & 64.51 & 68.71 & 85.30 & 18.47 & 64.15 & 41.92 & 46.39 & 66.55 & 63.88 \\ CoCoOp  & 93.79 & 90.46 & 64.90 & 70.85 & 83.97 & 22.29 & 66.89 & 45.45 & 39.23 & 68.44 & 64.63 \\ MaPLe  & 93.53 & 90.49 & 65.57 & 72.23 & 86.20 & 24.74 & 67.01 & 46.49 & 48.06 & 68.69 & 66.30 \\ MaPLe+TPT  & 93.59 & 90.72 & 66.50 & 72.37 & 86.64 & 24.70 & 67.54 & 45.87 & 47.80 & 69.19 & 66.50 \\ DiffTPT  & 92.49 & 88.22 & 67.01 & 70.10 & 87.23 & 25.50 & 65.74 & 47.00 & 43.13 & 62.67 & 65.47 \\ PromptAlign  & 94.01 & **90.76** & 68.50 & **72.39** & 86.65 & 24.80 & 67.54 & 47.24 & 47.86 & 69.47 & 66.92 \\ TDA  & 94.24 & 88.63 & 67.28 & 71.42 & 86.14 & 23.91 & 67.62 & 47.40 & 58.00 & 70.66 & 67.53 \\  BoostAdapter & **94.77** & 89.51 & **69.30** & 71.66 & **87.17** & **27.45** & **68.09** & 45.69 & **61.22** & **71.93** & **68.68** \\   

Table 2: **Full results on the Cross-Domain Benchmark with ViT-B/16 backbone.**  We report top-1 accuracy and “Average” is calculated by taking the mean accuracy across all ten datasets. The error bound is \( 0.17\).

### Out-of-Distribution Generalization

To verify the robustness of BoostAdapter, we evaluate our method on the OOD benchmark, in comparison with existing training-require methods including CoOp , CoCoOp , TPT , DiffTPT , Maple  and PromptAlign , as well as training-free method TDA . As can be seen from Table 8, the most striking observation emerging from the comparison is that BoostAdapter significantly outperforms other baselines on average and improves the generalization ability of the model. For training-free methods such as TPT, DiffTPT and PromptAlign, BoostAdapter achieves superior performance while saving on optimization computation overhead. For training-free methods like TDA, BoostAdapter gains consistent performance improvements with the introduction of the boosting samples. Notably, BoostAdapter surpasses TDA by 4.42% on ImageNet-A and 0.84% on ImageNet-V2, respectively. This enhancement indicates the effectiveness of self-bootstrapping when historical samples may not provide sufficient useful information.

### Cross-Domain Transfer

We further highlight our improvements in the transfer ability of CLIP on the Cross-Domain benchmark and present the results in Table 2. Compared with existing training-required and training-free methods, BoostAdapter achieves state-of-the-art performance on 7 out of 10 tasks, surpassing the strongest baselines by an average of 1.15%. With diverse classes at test time, regional boosting enables BoostAdapter to adaptively extract knowledge that makes classes distinct from each other in a multi-scale manner. Notably, for datasets requiring fine-grained information for classification such as Aircraft, the improvement of BoostAdapter is most significant.

### Ablation Study

Historical Samples and Boosting Samples.To demonstrate the effect of historical and boosting samples, we introduce two variants of BoostAdapter that utilize only historical samples or only boosting samples, respectively. Additionally, we provide the zero-shot results of CLIP for comparison. As shown in Table 3, CLIP significantly benefits from both historical samples and boosting samples, resulting in notable improvements in performance. The consistent improvement of BoostAdapter compared to the variant that utilizes only historical samples further confirms the effectiveness of

    & -V2 & -Sketch & -A & -R & Average \\  CLIP  & 51.41 & 33.37 & 21.83 & 56.15 & 40.69 \\ TPT  & 54.70 & 35.09 & 26.67 & 59.91 & 43.89 \\ CAPT  & 53.70 & 35.61 & 33.96 & 60.81 & 43.52 \\ CoOp  & 55.40 & 34.67 & 23.06 & 56.60 & 42.43 \\ CoCoOp  & 55.72 & 34.48 & 23.23 & 57.74 & 42.82 \\ DiffTPT  & 55.80 & 37.10 & 31.106 & 58.80 & 45.69 \\ TDA  & 55.54 & 38.12 & 30.29 & 62.58 & 46.63 \\   BoostAdapter & **56.14** & **38.87** & **35.12** & **62.66** & **48.20** \\   

Table 4: **Full results on the OOD benchmark with RN-50 backbone. We report top-1 accuracy and the error bound is \( 0.06\).**

Figure 4: Ablation studies of (a) number of augmented views to generate boosting samples (b) different adaptation methods and (c) total shot capacity of the cache.

    & -V2 & -Sketch & -A & -R & Average \\  CLIP & 60.86 & 46.09 & 47.87 & 73.98 & 57.20 \\ Historical Samples & 64.93 & 50.23 & 63.80 & 80.43 & 64.85 \\ Boosting Samples & 65.40 & 50.59 & 64.40 & **80.96** & 65.34 \\  BoostAdapter & **65.51** & **51.28** & **64.53** & **80.95** & **65.57** \\   

Table 3: **Ablation study on historical samples and boosting samples on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and the error bound is \( 0.12\).**

[MISSING_PAGE_FAIL:9]

consistently outperforms TDA across all 15 corruption types, highlighting its practical applicability in real-world situations. The superior performance of BoostAdapter stems from its capability to capture the knowledge of the test sample even under severe corruption. This is achieved with the help of the boosting samples, which effectively filter out noisy parts while retaining useful information.

**Efficiency Analysis** BoostAdapter requires augmentation over the test samples, which may slightly affect the inference speed during testing. We conduct an efficiency analysis of BoostAdapter in comparison with existing Test Time Augmentation (TTA) methods and provide the results in Table 7. BoostAdapter is slightly slower than the cache-based method TDA, yet still significantly faster than training-required methods. The memory cost of BoostAdapter is also comparable to other baselines.

**Unification of Training-required and Training-free Methods.** From the unified perspective, we can also enhance training-free adapters with additional training-required methods. Here we take TSD  and DEYO  as the showcase. Specifically, in the BoostAdapter+DEYO variant, we filter out augmented views with a PLPD lower than 0.2. For the BoostAdapter TSD variant, we discard augmented views that have different cache predictions and CLIP predictions to ensure consistency of the boosting samples. When equipping BoostAdapter with the technique of TSD and DEYO, we observe further improvement and find that training-free adapters can benefit from various boosting techniques of training-required methods.

**Qualitative Results** The qualitative results are provided in Figure. 5. By incorporating samples with low entropy from regional bootstrapping, the model is enhanced to more effectively capture the fine-grained information of the test samples, thereby improving the overall performance.

## 5 Conclusions

In this work, we present an insightful analysis of existing training-required and training-free TTA methods to bridge the gap between them. In particular, we improve training-free adapters by incorporating self-boosting samples into the memory bank inspired by the idea of regional bootstrapping from entropy-based methods. The cache in our method, containing instance-agnostic historical samples and instance-aware boosting samples, is capable of performing knowledge mining on both the target domain and the testing sample itself. We also derive error bounds in the test-time adaptation setting and show that this cache benefits from both historical samples and boosting samples. Extensive experiments on the two benchmarks demonstrate the effectiveness of our method.

Despite the promising performance of our method, it also has some limitations. It requires slightly more computation overhead than existing training-free adapters due to the multiple augmentation of the test samples, as discussed in Appendix. One future direction is to develop a more efficient augmentation method to obtain boosting samples, rather than merely randomly cropping and then filtering over the test samples.

    & Augmentation & Views & Inference Speed (fps) & Memory (GB) & OOD Results & Cross-Domain Results \\  CLIP & - & - & 82.3 & 0.7 & 57.20 & 63.58 \\ TPT & Augmix & 64 & 0.29 & 4.5 & 60.81 & 65.10 \\ DiffTP & Diffusion & 64 & 0.10 & 14.4 & 60.52 & 66.92 \\ TDA & Augmix & 64 & 11.89 & 1.2 & 63.89 & 67.53 \\  BoostAdapter & Rand. Crop \& Rand. Horiz. Flip & 64 & 11.23 & 1.2 & 65.57 & 68.68 \\   

Table 7: **Efficiency analysis. We evaluate different methods on a single NVIDIA 3090 24GB GPU and report the frames per second (fps) and memory cost (GB).**

Figure 5: **Qualitative results. The model predictions are provided below the images. Boosting samples with low entropy improves information extraction from the test sample and helps the model to distinguish better.**

    & -V & -S & -A & -R & Average \\  CLIP-VIT-B/16 & 60.86 & 46.09 & 47.87 & 73.98 & 57.20 \\ TDA & 64.67 & 50.54 & 60.11 & 80.24 & 63.89 \\ BoostAdapter+DFC & 65.51 & 51.28 & 64.53 & 80.95 & 65.57 \\ BoostAdapter+TSD & 65.49 & 51.50 & 64.37 & 81.15 & 65.63 \\ BoostAdapter+ DEYO & 65.71 & 51.52 & 64.65 & 81.43 & 65.83 \\   

Table 8: **Unification of more training-required methods. BoostAdapter benefits from different training-required methods.**