# CoLoR-Filter: Conditional Loss Reduction Filtering

for Targeted Language Model Pre-training

 David Brandfonbrener

Kempner Institute at Harvard University

&Hanlin Zhang

Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Kichard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University
&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University
&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University
&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade
Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Andreas Kirsch

University of Oxford

&Jonathan Richard Schwarz

Harvard University

&Sham Kakade

Kempner Institute at Harvard University

&Sham Kakade

Kempner Institute at Harvard University
maximizes likelihood on the downstream tasks. Then we can also test performance on the tasks under their preferred metrics.

From this objective, we derive an algorithm dubbed CoLoR-Filter (Conditional Loss Reduction Filtering). In Section 2 we derive this method by applying Bayes' rule and approximate empirical Bayes to the downstream likelihood objective. The resulting method is simple and intuitive: each sequence is scored by the difference in likelihood between a "prior" model and a "conditional" model that results from fine-tuning the prior model on the downstream data. Sequences that are more likely under the fine-tuned model are good. We also compare this algorithm to prior work (e.g., (Mindermann et al., 2022)) and discuss computational costs.

To evaluate our method, we consider two tasks. First, in Section 5, we consider a semi-synthetic task where the downstream task is language modeling on Books. Given access to C4 (Raffelt et al., 2020) as potential pre-training data and a small (25 million tokens) sample of data from Books, we use CoLoR-Filter and a variety of baselines to select 3 billion tokens. We find that data selected by CoLoR-Filter can substantially outperform models trained on 8x as much randomly chosen data. Second, in Section 6, we consider a suite of 8 downstream multiple-choice tasks from Groeneveld et al. (2024). As downstream data we take the training sets of the tasks, but we evaluate accuracy on the held-out test sets. We again find that selecting with CoLoR-Filter outperforms training on 8x as much randomly selected data. Moreover, in both tasks, performance scales smoothly with the hyperparameter \(\) that governs how aggressively we select the data, suggesting that further scaling would yield further improvements.

In addition to finding that CoLoR-Filter can select good subsets of data, we also consider the computational cost of the selection procedure itself. CoLoR-Filter only requires running inference of the two auxiliary models to select data. This is computationally beneficial compared to online methods like RHOLoss (Mindermann et al., 2022) since inference is cheaper than training and is entirely parallelizable. To maximize the computational benefits we also show that data selected with a small (150 million parameter) model can be transferred to a larger (1.2 billion parameter) model. Results are shown in Figure 1, showing substantial efficiency improvements.

## 2 Setting and Derivations

Assume that we are given a large pre-training dataset \(D_{}\), a small downstream dataset \(D_{}\) from the downstream task(s) of interest, and a "prior" dataset \(D_{}\) we can use as prior knowledge (in practice we often just sample from \(D_{}\)). We will assume for all practical purposes that

Figure 1: Learning curves for 1.2 billion parameter language models trained on data selected by CoLoR-Filter using smaller 150 million parameter auxiliary models for two different target distributions. (Left) We target and evaluate loss on Books, lower is better. (Right) We target and evaluate accuracy on a suite of 8 downstream tasks from (Groeneveld et al., 2024), higher is better. In both cases, test data is held out from the data used by CoLoR-Filter to guide selection. \(\) is the subset size multiplier denoting the number of examples considered for each selected data point. The CoLoR-Filter line terminates when we run out of data in C4 (\(\)175b possible tokens).

is infinite and training proceeds in the "online" or "single pass" setting where we do not repeat data points. Our goal is to choose a subset \(S D_{}\) of a fixed size \(|S|=n\) that minimizes the downstream loss (maximizes the downstream likelihood).

This section introduces our CoLoR-Filter algorithm, inspired by and building upon the RHODoss approach from prior work (Mindermann et al., 2022; Evans et al., 2023). We also discuss related algorithms applicable to this setting such as DSIR (Xie et al., 2023) and DSDM (Engstrom et al., 2024). Additional related work is discussed further in Section 3.

### Bayesian Data Selection

Our objective can be formulated as a Bayesian optimization problem, where the goal is to select a set \(S\) so as to maximize the posterior probability of \(D_{}\), i.e.

\[_{S D_{},|S|=n}-(D_{}|S),\] (1)

where \((D_{}|S)\) is the posterior probability. Applying Bayes rule we get:

\[_{S D_{},|S|=n}-(S|D_{})+(S)- (D_{})\] (2)

Note that the last term does not depend on \(S\), so it can be ignored when optimizing over \(S\). Introducing a prior over model parameters \(\), we get:

\[_{S D_{},|S|=n}(S| )(|D_{})}_{}+(S|)()}_{}\] (3)

We will refer to the two terms as the conditional and marginal terms, respectively.1 Note that the conditional and marginal terms together make up the negative pointwise mutual information between the selected and downstream data, which has deep connections to prior work on active learning and active sampling (Lindley, 1956; Moore and Lewis, 2010; Houlsby et al., 2011; Bickford Smith et al., 2023; Kirsch, 2023; Rainforth et al., 2024).

### CoLoR-Filter

Given that we have access to prior knowledge from the dataset \(D_{}\), we can replace the uninformed prior over \(\) with an empirical Bayes prior that conditions on \(D_{}\) to obtain:

\[_{S D_{},|S|=n}-_{}(S|)( |D_{},D_{})+_{}(S|)( |D_{})\] (4)

As this integration is still intractable, we now make our main simplifying assumption which is to replace this integration over parameters by a point estimate:

\[_{S D_{},|S|=n}-(S|_{})+(S|_{}),\] (5)

where \(_{}\) is a model trained on \(D_{}\) and \(_{}\) is a model trained on both \(D_{}\) and \(D_{}\) (in practice, we use a model that is pre-trained on \(D_{}\) fine-tuned on \(D_{}\)).

Moreover, this approximation leads to computational benefits by avoiding the full combinatorial optimization of subset selection. In particular, once we condition on a single model \(\), and assuming the distribution over points \(x S\) is independent, i.e. \((S|)=_{x S}(x|)\), we have:

\[_{\{x_{1},,x_{n}\} D_{}}-_{i=1}^{n}( x_{i}|_{})+_{i=1}^{n}(x_{i}|_{ })\] (6)

which simplifies to:

\[_{\{x_{1},,x_{n}\} D_{}}_{i=1}^{n}-(x_ {i}|_{})-(-(x_{i}|_{}))\] (7)This gives our CoLoR-Filter criteria that we use to select data. This optimization selects the points with the largest conditional loss reduction (CoLoR), i.e. the points where the negative log-likelihood loss of the conditional model \(_{}\) is lower than the marginal model \(_{}\). Intuitively, this selects data points that are more likely under the conditional model than the marginal model.

A note on data diversity.While the factorization that results from our point estimate of the parameters is computationally convenient, it makes an important simplifying assumption. In particular, the CoLoR-Filter objective no longer encourages the selection of a diverse dataset, as scores are applied independently to each point. In practice, this is remedied by a few considerations: (1) we can run CoLoR-Filter on a corpus that has already been deduplicated to prevent degenerate duplications, (2) for large \(n\), we must select many different data points, and (3) each datapoint is itself a sequence that may contain diverse signal across tokens. We should also note this is not a unique property of CoLoR-Filter and also happens in other methods that do offline scoring like DSDM and DSIR. We defer a detailed discussion of the nuances of this issue to Appendix C.

### Related Algorithms

Connection to importance sampling.Since the CoLoR-Filter objective is written as a difference of logs, it can also be written as a log of the ratio between probabilities under \(_{}\) and \(_{}\). If data were actually sampled from \(_{}\), then this ratio would be the importance weight needed to reweight samples so that they are from the model defined by \(_{}\). Note that DSIR (Xie et al., 2023) directly attempts to perform importance sampling from \(D_{}\) to \(D_{}\) instead of optimizing performance on the downstream data. Thus, DSIR ends up with a somewhat related algorithm except in DSIR: (1) there is no language model, just features of a full data point (hashed n-grams), and (2) the algorithm samples rather than optimizes.

Connections to DSDM.Another closely related approach is DSDM (Engstrom et al., 2024) which uses a TRAK Datamodel estimator (Ilyas et al., 2022; Park et al., 2023) to score datapoints and then selects the top-\(n\) points. The motivation and setting of DSDM are similar to CoLoR-Filter, but DSDM relies on TRAK which constructs a linear approximation of the influence that data points have on each other. Instead, CoLoR-Filter operates directly in function space by comparing the loss between models directly rather than relying on linear approximations or Datamodels (Ilyas et al., 2022).

Connections to RHO-down.CoLoR-Filter is inspired by and builds on the RHOD loss approach introduced in prior work (Mindermann et al., 2022) with subtle but significant differences in the setting: the original RHO paper focuses on cases where the hold-out data is sampled from the same distribution as \(D_{}\) over multiple epochs of training. In contrast, we focus on selecting data to target downstream distributions that are different from \(D_{}\) and where we only take a single pass over the data. Here, we derive a straightforward adaptation of RHOD loss to our setting, which we call RHO-down.

We now derive RHO-down in our setting, aiming to illustrate the connections between RHO-down and CoLoR-Filter. First, RHO-down approximates the full subset selection problem from Equation (3) by a greedy (sequential) approximation where samples are added to \(S\) one (batch) at a time. Using a batch size of \(1\), the \(i\)th-sample would be ideally added according to the following criterion:

\[_{x_{i} D_{}}-_{}(x_{i}|)( |D_{},x_{<i})+_{}(x_{i}|)(| x_{<i}),\] (8)

where \(i\) ranges from \(1\) to \(n\) sequentially. RHO-down then uses a point estimate of the parameters (as we do in CoLoR-Filter):

\[_{x_{i} D_{}}-(x_{i}|_{}})+(x_{i}|_{x_{<i}})\] (9)

Finally, the RHO-down authors found that updating the conditional term to depend on \(x_{<i}\) was unstable, so they instead approximate this by a fixed model \(_{}\):

\[_{x_{i} D_{}}-(x_{i}|_{})+ (x_{i}|_{x_{<i}}).\] (10)

Note that while both CoLoR-Filter and RHO-down approximate the posterior over parameters with a point estimate, RHO-down makes a few additional approximations. This is largely a result of RHO-down attempting to increase data diversity by using a sequential approach to selection that conditions on the previously selected data \(x_{<i}\). This is an understandable goal, but it introduces more approximations, can cause instability by creating a non-stationary data distribution, and is computationally expensive since the data selection is no longer parallelizable. A continued discussion of the pros and cons of online selection is in Appendix C.

RHO-down + prior.We also consider a version of the algorithm that we call "RHO-down + prior" that replaces \(D_{},_{}\) in the RHO-down algorithm with \(D_{} D_{},_{}\) to incorporate the prior information. This corresponds to conditioning on both \(D_{}\) and \(D_{}\) instead of only \(D_{}\). Intuitively, this method can better leverage stronger features learned on the larger \(D_{}\) to integrate the information from the small \(D_{}\).

## 3 Further Related Work

We now discuss some related work, more broadly, with regards to active learning and data curation.

Active & Curriculum learning.Our formulation of data selection has connections to classic and deep active learning (Houlsby et al., 2011; Bickford Smith et al., 2023; Kirsch, 2023), which are deeply rooted in optimal Bayesian experimental design (Lindley, 1956; Rainforth et al., 2024), whose goal is to select a set of experiments to optimize certain information criteria (Pukelsheim, 2006) such as maximally reducing the uncertainty about model parameters. Various acquisition functions are proposed in deep learning regimes (Sener and Savarese, 2018; Ash et al., 2019, 2021) and most of them focus on label-efficient image classification. Another line of recent techniques share deep methodological connections but emphasize the sub-selection of available data during training (rather than the collection of additional examples typically considered in active learning) and could thus be classified as curriculum learning (e.g. Graves et al., 2017). Among them, RHOLoss (Mindermann et al., 2022) seeks to select data based on the hold-out reference dataset from the same distribution as the training data. It has been later implemented in continual pre-training (Lin et al., 2024) and vision domains (Evans et al., 2023; Tack et al., 2024).

Data curation practices in pre-training.Though large-scale public web-crawled data are common data sources for pre-training models, low-quality, toxic, and uninformative content that can prevent successful pre-training is prevalent (Wenzek et al., 2020; Elazar et al., 2023; Sorscher et al., 2022; Allen-Zhu and Li, 2024). Therefore, practitioners design sophisticated data pre-processing pipelines such as filtering (Brown et al., 2020), deduplication (Lee et al., 2022), and mixing (Touvron et al., 2023a,b) to improve the data quality. Due to the immense scale, state-of-the-art pre-training datasets usually depend on simple heuristic filters (Raffel et al., 2020; Rae et al., 2021; Computer, 2023) (e.g., URL, length, n-gram perplexity, fastest classifiers) that can be parallelized across CPU nodes. Besides the above rule-based filtering, model-based filtering concerns using machine learning models to score and filter data, which has been proven to be effective in vision and vision-text domains (Schuhmann et al., 2022; Abbas et al., 2023; Fang et al., 2023). Such approaches usually leverage a given trustworthy data source like Wikipedia or Books as the reference and contrast the raw data with it. Due to computational cost, models are often designed to be small such as n-gram (Xie et al., 2023), single-layer neural networks (Joulin et al., 2017; Brown et al., 2020), k-means clustering (Tirumala et al., 2024). There is also a growing line of work illustrating that data quality is important in shaping model training from a variety of perspectives, such as increasing data scale (Hoffmann et al., 2022; Meta, 2024) and using synthetic data (Gunasekar et al., 2023).

## 4 Algorithms

### From Derivations to Practical Algorithms

In our experiments, we will consider four algorithms based on the above derivations. In this section we go through each of these in turn.

CoLoR-Filter.Our proposed algorithm is presented formally in Algorithm 1. Compared to the derivation, the main difference is the introduction of \(\), a hyperparameter that acts as a compute-performance trade-off controlling how expensive and aggressive the data selection is. Rather than selecting data from all of \(D_{}\), we take a random subset \(D_{}\) of size \( n\). Thus, larger \(\) subselect more aggressively, but at the cost of more computation. A full discussion of this cost is in Section 4.2.

Conditional only.As an ablation of CoLoR-Filter, we follow prior work [Evans et al., 2023] and include a baseline that only uses the conditional model to select data. Essentially, this is CoLoR-Filter if we always assume that \((x|^{})=0\) in Line 4 of Algorithm 1.

```
1:Downstream data \(D_{}\), train data \(D_{}\), budget \(n\), subset size multiplier \(\), batch size \(b\)
2:Train \(^{}\) on \(D_{}\)
3:Initialize a random \(_{1}^{}\) and \(S=\)
4:for\(t[1,,n/b]\)do
5: Randomly select a batch \(B_{t} D_{}\) of size \( b\)
6: Select data: \[_{t}=b_{x B_{t}}-(x|^{})+ (x|_{t}^{})\]
7:\(S=S_{t}\)
8: Update \(_{t}^{}\) to \(_{t+1}^{}\) by training on \(_{t}\)
9:endfor
10:return Selected dataset \(S\) to train \(\) on. ```

**Algorithm 2** RHO-down

RHO-down.We present a practical variant of RHO-down in Algorithm 2 based on the derivation presented in Section 2. The main changes to make a practical algorithm are (1) the introduction of \(\) as in CoLoR-Filter, and (2) performing the algorithm batch-wise instead of using single data points.

RHO-down + Prior.We can also incorporate the prior data \(D_{}\) into Algorithm 2 by simply replacing Line 1 where \(^{}\) is trained on \(D_{}\) with a procedure where we first pre-train \(^{}\) on \(D_{}\) and then fine-tune it on \(D_{}\).

### Computational Cost

To evaluate the computational cost of the various algorithms, we use units of "model forwards" per token where we assume that a backward pass is twice as expensive as a forward pass [Fleuret, 2023]. Note that our 150m models take about 5e8 FLOPs per model forward of a single token [Hoffmann et al., 2022, Casson, 2023]. The cost of running the selection algorithms depends on \(m,n,\) and \(L\) defined as follows: \(m\) is the size of the prior data \(D_{}\), \(n\) is the size of the selected dataset \(S\), \(\) is the hyperparameter controlling how aggressively we subselect data. Note that we assume that \(|D_{}|\) is so small that the cost of training a model on \(D_{}\) is negligible towards the total cost (and all the methods we consider just fine-tune a model once on \(D_{}\)). We will also be careful to note when computation can be done in parallel before training versus computation that must happen serially during a training run. Offline algorithms like CoLoR-Filter can take advantage of parallelism to improve efficiency. In this section, we go through each method in turn and aggregate the computational costs in table 1.

Scale transfer.We also include another parameter \(L\) to cover the case where we select data using small models and use it to train a larger model [Evans et al., 2023]. Specifically, \(L\) is the ratio of cost of one model forward of the _large_ target model compared to the small auxiliary models used for data selection. For example, in our experiments, when we use 150 million parameter models to select data and then train a 1.2 billion parameter model on the resulting data, then \(L 5.5^{2}\). Training thus costs \(3nL\) across all methods since we run a forward and backward for the large model on all \(n\) sequences.

CoLoR-Filter.The cost of selection is \(2 n\) forward passes. But, this selection process is _entirely_ parallelizable. Training the prior model costs \(3m\) forwards since \(|D_{}|=m\). And training a model on the selected data costs \(3nL\) forward passes. So the total cost is \(3m+2 n+3nL\), but the \(2 n\) scoring computation can be done in parallel.

Conditional Only.The conditional-only method is almost the same as CoLoR-Filter, except we only need \( n\) forward passes for selection since we only run one model over the data. The cost is thus \(3m+ n+3nL\), with \( n\) being parallelizable.

RHO-down.The cost of selection is still \(2 n\) forward passes. Then we need an additional \(2n\) to backward the output model (since the forward is already handled during scoring). Note that we need to evaluate the marginal model online, so it is not parallelizable, but the conditional model is fixed and can be computed offline. So, the cost is \(2 n+2n+3nL\), and the \( n\) conditional model computation can be done in parallel.

RHO-down + Prior.For the version with an added prior, we just add \(3m\) cost for training the prior. Thus, the cost is \(2 n+2n+3nL\) with \( n\) parallelizable.

Overall, the methods all have comparable costs, with Conditional Only being the cheapest and RHO-down + Prior the most expensive. The main difference is that CoLoR-Filter and Conditional Only are easily parallelized while RHO-down and RHO-down + Prior are not. It should also be noted that when doing experimentation, offline methods like CoLoR-Filter also benefit from being able to re-use likelihoods multiple times, while RHO-based methods need to recompute the serial cost any time that some hyperparameter of the algorithm.

## 5 Domain Transfer: a Simple Testbed

### Setup

Training.We train language models with 150 million non-embedding parameters using the OLMo codebase (Groeneveld et al., 2024) and following hyper-parameter choices from (Wortsman et al., 2024). Unless otherwise noted, we use 150m models as the auxiliary models (\(^{},^{}\)) as well as the target model \(\). Full hyperparameters are described in detail in Appendix H.

We take \(D_{}\) to be a small dataset of 25 million tokens sampled from the Project Gutenberg Books data subset of Dolma (Soldaini et al., 2024), \(D_{}\) to be a dataset of 3.1 billion tokens from C4 (Raffel et al., 2020), and \(D_{}\) to be all of C4. We select a dataset \(S\) of 3.1 billion tokens (which is

   Method & Prior cost & Serial cost & Parallel cost & Training cost \\  CoLoR-Filter & \(3m\) & \(0\) & \(2 n\) & \(3nL\) \\ Conditional Only & \(3m\) & \(0\) & \( n\) & \(3nL\) \\ RHO-down & 0 & \( n+2n\) & \( n\) & \(3nL\) \\ RHO-down + Prior & \(3m\) & \( n+2n\) & \( n\) & \(3nL\) \\ Random & 0 & 0 & 0 & \(3nL\) \\   

Table 1: Compute cost of the various algorithms measured in “model forwards”. The total cost of selection and training on the selected data is the sum of all costs across a row. The variables are \(m=|D_{}|\), \(n=|S|\), \(\) is a hyperparameter that controls how aggressively we subselect, and \(L\) is a multiplier of the cost of model forwards between the selection model(s) and the target model (approximately the ratio of parameter counts between the models).

approximately the "chinchilla optimal" amount for models of this size). To get \(_{+}\) or \(_{}\), we fine-tune or train for one epoch on \(D_{}\).

Evaluation.To evaluate the efficacy of our data selection, we report cross-entropy loss of next token prediction on a held-out dataset \(_{}\) from the same distribution as \(D_{}\) (Books).

Baselines.The simplest baseline we consider is **Random** sampling, which has been shown to be a strong baseline for C4 pre-training [Engstrom et al., 2024]. We consider all four algorithms described in Section 4: **CoLoR-Filter**, **Conditional Only**, **RHO-down**, and **RHO-down + prior**. And as one extra baseline, we also include **DSIR**[Xie et al., 2023] which estimates n-gram importance weights between \(D_{}\) and \(D_{}\), and similarly has a parameter like \(\) that controls how aggressively to subselect.

Note that while it is in a similar setting to ours, we do not include DSDM [Engstrom et al., 2024] as a baseline since there is no publicly available code and based on the appendix of that paper, it it much more computationally expensive than the methods we consider.

### Results

We first run the domain transfer experiments on 150m models, sweeping across \(\) that controls the selected subset size. In Figure 2 we plot how the final performance scales with \(\) across methods. We see that CoLoR-Filter has the best scaling performance with increased \(\), with no sign of saturation for \(=16\). We hypothesize that by using strong models to select the data, CoLoR-Filter is able to more effectively scale to larger \(\) than the other methods. In Figure 7 in Appendix A, we plot the learning curves (evaluated on the held-out validation set) for the four methods introduced in Section 4. There, we see especially clean scaling for CoLoR-Filter across the entire learning curve, substantially outperforming random selection with much less data, similar to Figure 1.

Scale generalization.Finally, we also conduct an experiment in scale generalization (partially shown in Figure 1) using the data selected by our 150m auxiliary models to train a 1.2b target model. In Figure 3 we show learning curves for a sweep over \(\). We still see consistent gains as we scale \(\) for a fixed number of training tokens. Interestingly, if we fix the total number of tokens we are _selecting from_ (i.e. where the lines end when we run out of C4), then the final performance with \(=32\) is better than all other values of \(\). This shows how a strict subset of tokens can outperform a superset (e.g. \(=16\)). We should also point out here the computational savings when using CoLoR-Filter. As an example, consider \(=16\) where we match the performance of 25 billion randomly selected tokens with about 1.5 billion filtered tokens. Considering the computational costs discussed above with \(L=5.5\) and measuring \(n\) in billions of tokens, the total cost for training the CoLoR-Filter model is \(3m+2 n+3nL=3*3.1+2*16*1.5+3*1.5*5.5=82\) while the cost for training on 25 billion random tokens is \(3NL=3*25*5.5=412.5\), illustrating a more than 5x total compute savings to achieve the same performance on Books. A full plot visualizing the cost in FLOPs for all \(\) is in Appendix D.

Figure 3: Scaling CoLoR-Filter with \(\) when training 1.2b models with data selected by 150m models. Curves end when we exhaust the data in C4.

Figure 2: Scaling of final performance with \(\) when targeting **Books** with 150m parameter models.

## 6 Downstream Tasks

### Setup

Training.We target the 8 tasks from the OLMo paper (Groeneveld et al., 2024): Hellaswag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-challenge and ARC-easy (Clark et al., 2018), Openbook QA (Mihaylov et al., 2018), SciQ (Welbl et al., 2017), BoolQ (Clark et al., 2019), and Winogrande (Sakaguchi et al., 2021). Each of these datasets has a separate train split. We use these train splits to construct \(D_{}\) as follows: for each question we concatenate the question and the correct answer formatted as a grammatical continuation. Overall, this results in a small \(D_{}\) dataset of 7.4 million tokens. \(D_{}\) and \(D_{}\) are the same as before. And we again get \(_{}\) by fine-tuning \(_{}\) for one epoch on \(D_{}\).

Evaluation.We evaluate on held-out data from each downstream task test or validation sets (using val if test is not publicly available). We use the evaluation procedure from OLMo (Groeneveld et al., 2024) which follows (Gao et al., 2023) for evaluating these multiple-choice tasks using the rank classification approach of Brown et al. (2020). We report aggregat performance across tasks as well as the task-specific performance.

Baselines.Same as in Section 5.

### Results

While the curves themselves are noisier now due to the noisier nature of accuracy evaluation on small datasets compared to cross entropy on a large one, the same trends hold as we saw for domain transfer to Books. CoLoR-Filter in particular is scaling the best as we increase \(\). Other methods do not illustrate the same clean scaling as we increase \(\), which is nearly linear on a log scale for CoLoR-Filter, as seen in Figure 4. Full learning curves are in Appendix A.

We can also look at the performance broken down by task and illustrated relative to training on an equivalent amount (3.1 billion tokens) of randomly selected data for \(=16\) illustrated in Figure 5. We see especially large gains on Hellaswag, ARC easy, Openbook QA and SciQ and actually see performance decreases on BoolQ and Winogrande. However, we should note that at this scale and with all data selected from C4, we actually found BoolQ and Winogrande to be quite noisy and not even correlated with training on 8x as much random data, so it is not clear how much weight to place

Figure 4: Final performance versus \(\) on the suite of downstream tasks for 150m models. CoLoR-Filter scales the best with \(\).

Figure 5: Performance improvement over training on an equivalent amount of random data broken down by task (except for Random 8x, which uses 8x more data). A table of results is in Appendix B.

on those results. Across the other tasks, the gains of CoLoR-Filter over the baselines are clear. It is an interesting direction for future work to probe more deeply into how task-dependent the gains from targeted data selection can be.

Scale generalization.We also consider scale generalization to a 1.2b target model and illustrate the full results of a sweep over \(\) in Figure 6. Again we find significant benefits of CoLoR-Filter across scales. A full table of per-task results is in Appendix B. Again we notice that training on a strict subset of data can outperform a larger dataset.

We can again do out the calculation of computational savings for \(=16\). It now takes about 3 billion tokens for CoLoR-Filter to match the performance of training on 25 billion random tokens. This amounts to a total cost of \(3m+2 n+3nL=3*3.1+2*16*3+3*3*5.5=154.8\), which is still an upwards of 2.5x reduction in compute to achieve the same average performance across the suite of tasks. A full plot visualizing the cost in FLOPs for all \(\) is in Appendix D.

Task generalization.We can also test task generalization beyond the 8 tasks that were used to select the data on a few more tasks that test common sense reasoning (Wang et al., 2019; Socher et al., 2013; Talmor et al., 2018; Sap et al., 2019). Results are presented in Table 2 compared to a random model trained on 10x as much data. The performance indicates that the data selected by CoLoR-Filter are not overfit to the particular evaluation tasks, but captures some general notion of good data for a range of tasks.

Note, we also conduct a few more experiments and ablations in the appendix: Appendix E considers using CoLoR-Filter in-distribution to target C4 loss, Appendix F considers applying CoLoR-Filter batchwise rather than globally, Appendix G considers finetuning on \(D_{}\) after targeted pre-training, Appendix I inspects some of the selected and excluded examples, and Appendix J compared to FineWeb-edu (Penedo et al., 2024).

## 7 Discussion

While fairly simple to derive and implement, we show that CoLoR-Filter is an effective method for data selection on C4, with promising scaling behavior up to 1.2 billion models. In our experiments, CoLoR-Filter continues to improve when only using 1 out of 64 data points considered for selection and generalizes from small auxiliary models to larger target models. This opens many potential lines of research. First, while we have considered targeted pre-training, it is possible that CoLoR-Filter could be extended to fine-tuning, continual pre-training, and more general open-domain pre-training. In particular, it is an interesting open question whether the lack of an explicit consideration of data diversity hinders CoLoR-Filter in any of these settings. Second, CoLoR-Filter could be applied to more challenging domains in language like code generation or even applied beyond the language domain to other modalities. Finally, there is plenty of work to be done to make the algorithm more efficient and to test the limits of scale generalization.

   Method & copa & rte & cb & sst2 & commonsense qa & social iqa \\  Random (25b tokens) & **69.2** & 48.9 & 42.8 & 46.8 & **33.7** & **42.9** \\ CoLoR-Filter (\(=64\), 2.5b tokens) & 65.8 & **52.6** & **46.0** & **55.8** & 32.6 & 42.7 \\   

Table 2: Task generalization for the 1.2b models with \(=64\).

Figure 6: Scaling CoLoR-Filter with \(\) when training 1.2b models with data selected using smaller 150m models. Curves end when we exhaust the data in C4.