# Gradient-Variation Online Learning under

Generalized Smoothness

 Yan-Feng Xie, Peng Zhao, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{xieyf, zhaop, zhouzh}@lamda.nju.edu.cn

###### Abstract

Gradient-variation online learning aims to achieve regret guarantees that scale with variations in the gradients of online functions, which is crucial for attaining fast convergence in games and robustness in stochastic optimization, hence receiving increased attention. Existing results often require the _smoothness_ condition by imposing a fixed bound on gradient Lipschitzness, which may be unrealistic in practice. Recent efforts in neural network optimization suggest a _generalized smoothness_ condition, allowing smoothness to correlate with gradient norms. In this paper, we systematically study gradient-variation online learning under generalized smoothness. We extend the classic optimistic mirror descent algorithm to derive gradient-variation regret by analyzing stability over the optimization trajectory and exploiting smoothness locally. Then, we explore _universal online learning_, designing a single algorithm with the optimal gradient-variation regrets for convex and strongly convex functions simultaneously, without requiring prior knowledge of curvature. This algorithm adopts a two-layer structure with a meta-algorithm running over a group of base-learners. To ensure favorable guarantees, we design a new Lipschitz-adaptive meta-algorithm, capable of handling potentially unbounded gradients while ensuring a second-order bound to effectively ensemble the base-learners. Finally, we provide the applications for fast-rate convergence in games and stochastic extended adversarial optimization.

## 1 Introduction

We consider online convex optimization (OCO) (Hazan, 2016; Orabona, 2019), a flexible framework that models the decision-making problem in an online fashion. At each round \(t[T]\), an online learner is required to submit a decision \(_{t}\) from a convex compact set \(^{d}\) and the environments reveal a convex function \(f_{t}:\). Then the learner suffers a loss \(f_{t}(_{t})\) and updates her decision. The standard performance measure is the _regret_(Zinkevich, 2003) that benchmarks the cumulative loss of the learner against the best decision in hindsight, formally defined as

\[_{T}=_{t=1}^{T}f_{t}(_{t})-_{ }_{t=1}^{T}f_{t}().\] (1)

Regret bounds of \(()\) and \(( T)\) are established for convex and \(\)-strongly convex functions respectively (Zinkevich, 2003; Hazan et al., 2007). While these results are known to be minimax optimal (Abernethy et al., 2008), in this paper we are more interested in obtaining _gradient-variation_ regret guarantees, which replace the dependence of \(T\) in the regret bounds by variations inthe gradients of online functions (Chiang et al., 2012) defined as

\[V_{T}=_{t=2}^{T}_{} f_{t}() - f_{t-1}()_{2}^{2}.\] (2)

This quantity can be as small as a constant in stable environments where online functions remain fixed, and is at most \((T)\) in the worst case under standard OCO assumptions, safeguarding minimax results. Besides this favorable adaptivity, recent studies have shown close relationships of gradient-variation online learning to various fields, including fast convergence in games (Rakhlin and Sridharan, 2013; Syrgkanis et al., 2015; Zhang et al., 2022b) and robust stochastic optimization (Sachs et al., 2022; Chen et al., 2024), hence receiving increased attention (Zhao et al., 2020; Yan et al., 2023; Tsai et al., 2023; Atace Tarzanagh et al., 2024; Zhao et al., 2024).

In online learning, it is proved that the smoothness assumption is necessary for first-order algorithms to achieve gradient-variation regret bounds as discussed in Remark 1 of Yang et al. (2014), which is also restated in Proposition 2 in Appendix B. Previous works typically rely on the _global_\(L\)-smoothness condition, imposing a fixed upper bound on the gradient Lipschitzness, i.e., requiring \(^{2}f_{t}()_{2} L\) for all \(t[T]\) and \(\). However, this global assumption restricts the applicability of theories to loss functions that are quadratically bounded from above. Furthermore, recent studies in neural network optimization have observed phenomena where the global smoothness condition fails to model optimization dynamics effectively, especially for important types of neural networks like LSTM (Zhang et al., 2020) and Transformer (Crawshaw et al., 2022). Therefore, modern optimization has devoted efforts to generalizing the smoothness condition. For example, Zhang et al. (2020) introduce \((L_{0},L_{1})\)-smoothness, which assumes \(^{2}f()_{2} L_{0}+L_{1} f()_{2}\) for an offline objective function \(f()\). A notable generalization is the recent proposal of the \(\)-smoothness condition (Li et al., 2023), which assumes \(^{2}f()_{2}( f() _{2})\) with a link function \(()\), significantly broadening previous assumptions through the flexibility of \(()\). Given this, it is natural to ask _how to design online algorithms to exploit generalized smoothness and obtain favorable gradient-variation regret guarantees._

In this paper, we provide a systematic study of gradient-variation online learning under generalized smoothness. We extend the classic optimistic online mirror descent (optimistic OMD) algorithm (Chiang et al., 2012; Rakhlin and Sridharan, 2013) to derive gradient-variation regret bounds, achieving \((})\) regret and \(( V_{T})\) regret for convex and strongly convex functions under generalized smoothness, respectively. We emphasize the importance of stability analysis across the optimization trajectory, which allows generalized smoothness to be effectively exploited locally. Specifically, optimistic OMD maintains two sequences with submitted decisions \(\{_{t}\}_{t=1}^{T}\) and intermediate decisions \(\{}_{t}\}_{t=1}^{T}\). We need to control algorithmic stability by appropriate step size tuning and optimism design, ensuring that \(_{t}\) is sufficiently close to \(}_{t}\) to exploit local smoothness at \(}_{t}\).

Based on this development, we investigate _universal online learning_(van Erven and Koolen, 2016; Wang et al., 2019; Mhammedi et al., 2019; Zhang et al., 2022; Yan et al., 2023; Yang et al., 2024), where the learner aims to design a single algorithm that simultaneously attains the optimal regret for both convex and strongly functions without the prior knowledge of curvature information. For this scenario, a common wisdom is to adopt an _online ensemble_ consisting of a meta-base two-layer structure to handle the environmental uncertainty (Zhao et al., 2024), i.e., the unknown curvature of loss functions, where a meta-algorithm is running over a set of base-learners with different configurations. The base-learners are basically the instantiations of the developed variants of optimistic OMD, as mentioned earlier. However, designing the meta-algorithm is non-trivial with new challenges. The first challenge is from the potentially unbounded smoothness, which might lead to unbounded Lipschitz constants as well. This challenge requires the meta-algorithm to be _Lipschitz-adaptive_, adapting to Lipschitzness on the fly. Furthermore, we also expect it to provide a _second-order regret_, technically required when analyzing the ensemble errors, and to enable _predictions with optimism_, thereby producing the gradient variation. The second challenge is the complexity introduced by the combination procedure inherent in the ensemble method, which further complicates the smoothness estimation, making it difficult to properly tune the meta-algorithm and exploit smoothness.

To this end, we address both challenges with the _function-variation-to-gradient-variation_ conversion and a newly-designed Lipschitz-adaptive meta-algorithm. The conversion technique, drawing inspiration from Bai et al. (2022), decouples the design between the meta and base levels and derives the gradient variation directly from function values, allowing us to avoid the cancellation-based analysis (Yan et al., 2023) for utilizing smoothness at the meta level. Nevertheless, this conversion requires the meta-algorithm to handle _heterogeneous_ inputs due to certain technical considerations, and we are not aware of available algorithms satisfying all the requirements, motivating us to design a new algorithm. Based on optimistic Adapt-ML-Prod (Wei et al., 2016) and the clipping technique (Cutkosky, 2019), we present a new Lipschitz-adaptive meta-algorithm with a simpler algorithmic design, which can be of independent interest. With this algorithm, we can apply the function-variation-to-gradient-variation conversion to achieve the optimal results for both convex and strongly convex functions, up to doubly logarithmic factors of \(T\), without knowing curvature.

Our findings for gradient-variation online learning are useful for several important applications, including fast-convergence online games (Rakhlin and Sridharan, 2013; Syrgkanis et al., 2015) and stochastic extended adversarial online learning (Sachs et al., 2022), where we establish new results under the generalized smoothness condition.

The rest of paper is organized as follows. Section 2 provides preliminaries and key ideas for exploiting the generalized smoothness throughout the trajectory. In Section 3 we study universal online learning and present our key meta-algorithm. Section 4 discusses our applications. Related work is provided in Appendix A. All proofs can be found in the remaining appendices (Appendix B - D).

## 2 Gradient-Variation Online Learning under Generalized Smoothness

In this section, we first introduce the problem setup, including the formal definition of generalized smoothness and other assumptions used in the paper. We then extend the optimistic online mirror descent framework to achieve gradient-variation regret bounds under generalized smoothness.

### Problem Setup: Generalized Smoothness and Assumptions

Recent studies (Zhang et al., 2020; Chen et al., 2023) extend the global smoothness condition by allowing the smoothness to positively correlate with the gradient norm, where a particular function is required to model this relationship. Zhang et al. (2020) introduce the \((L_{0},L_{1})\)-smoothness condition, where the smoothness is upper bounded by a linear function of the gradient norm, i.e., \(\|^{2}f()\|_{2} L_{0}+L_{1}\| f()\|_{2}\). Li et al. (2023) further generalize this by imposing a weaker assumption on the link function and propose the _generalized smoothness_ defined as follows.

**Definition 1** (\(\)-smoothness).: A twice-differentiable function \(f:\) is called \(\)-smooth for some non-decreasing continuous link function \(:[0,+)(0,+)\) if it satisfies that \(\|^{2}f()\|_{2}(\| f()\|_{2})\) for any \(\).

The mild requirement on the link function \(()\) allows for considerable generality. By selecting a linear link function, \(\)-smoothness immediately recovers \((L_{0},L_{1})\)-smoothness (Zhang et al., 2020). Furthermore, it has been shown that \(\)-smoothness can imply a wide class of functions including rational, logarithmic, and self-concordant functions (Li et al., 2023). Based on this generalized smoothness notion, we now provide the formal assumption on the smoothness of online functions.

**Assumption 1** (generalized smoothness).: The online function \(f_{t}:\) is \(_{t}\)-smooth in an open set containing \(^{d}\) for \(t[T]\), and the learner can query \(_{t}()\) provided any point \(\).

We also require a standard bounded domain assumption in the OCO literature (Hazan, 2016).

**Assumption 2** (bounded domain).: The feasible domain \(^{d}\), which contains the origin \(\), is non-empty and closed with the diameter bounded by \(D\), i.e., \(\|-\|_{2} D\) for any \(,\).

We do not assume the prior knowledge of the Lipschitz constant of online functions. In fact, the unboundedness of smoothness may result in unbounded Lipschitz constants. If a Lipschitz upper bound were known, the generalized smoothness condition would be trivialized, as it would allow us to directly compute the upper bound of the smoothness constant. Furthermore, following the discussion in Jacobsen and Cutkos (2023, Page 2, second paragraph on the right), we assume that there exist finite but _unknown_ upper bounds \(G\) and \(L\) for Lipschitzness and smoothness to ensure the theoretical results are valid. Note that these quantities will only appear in the final regret bounds, and our algorithms does not use them as the inputs. Throughout the paper, we use the \(()\)-notation to hide the constants and use the \(}()\)-notation to omit the poly-logarithmic factors in \(T\).

### Algorithmic Framework

We choose optimistic online mirror descent (optimistic OMD) (Rakhlin and Sridharan, 2013a) as the algorithmic framework, which provides a unified view to design and analyze many online algorithms. Compared to classic OMD (Nemirovskij and Yudin, 1985; Beck and Teboulle, 2003), optimistic OMD predicts with side information, an optimistic vector \(M_{t}^{d}\). This optimistic vector, also known as optimism, serves as a prediction of the incoming function \(f_{t+1}()\), leading to tighter regret bounds when accurate. Optimistic OMD updates the decisions in two steps:

\[_{t}=*{arg\,min}_{}\{  M_{t},+_{_{t}}(,}_{t})\},}_{t+1}=*{arg \,min}_{}\{ f_{t}(_{t}), +_{_{t}}(,}_{t}) \},\] (3)

where \(_{_{t}}(,)=_{t}()-_{t}( )-_{t}(),-\) is the Bregman divergence associated with the regularizer \(_{t}:\). Optimistic OMD maintains two sequences: the sequence of submitted decisions \(\{_{t}\}_{t=1}^{T}\), and the one of intermediate decisions \(\{}_{t}\}_{t=1}^{T}\). Although a simplified optimistic OMD with one-step update per round exists (Joulani et al., 2020), we will demonstrate later that tuning the step size based on intermediate decisions is crucial for adapting to generalized smoothness.

### Gradient-Variation Regret for Convex and Strongly Convex Functions

When minimizing the convex or strongly convex functions, we set the regularizer as \(_{t}()=}\|\|_{2}^{2}\) and optimistic OMD updates with the following steps:

\[_{t}=_{}[}_{t}-_{t}M_{t} ],}_{t+1}=_{}[}_{t}-_{t} f_{t}(_{t})],\] (4)

where \(_{}[]=*{arg\,min}_{}\|-\|_{2}\) denotes the Euclidean projection operator. Next, we briefly review approaches for obtaining the gradient-variation bound under _global smoothness_. This bound typically follows from the regret analysis for optimistic OMD:

\[_{T}}+_{t=1}^{T}_{t}\| f_{ t}(_{t})-M_{t}\|_{2}^{2}-_{t=1}^{T}}(\|_{t }-}_{t}\|_{2}^{2}+\|}_{t}-_{t -1}\|_{2}^{2}).\] (5)

On the right-hand side, the second term is known as the stability term, while the third one is the negative terms that can be further bounded by \((-_{t=1}^{T}}\|_{t}-_{t-1} \|_{2}^{2})\). Previous studies (Chiang et al., 2012; Zhao et al., 2024) for gradient-variation regret under global smoothness often set optimism as \(M_{t}= f_{t-1}(_{t-1})\), such that, the positive stability term can be upper bounded by \(_{t}\| f_{t}(_{t})- f_{t-1}(_{t})\|_{2}^{ 2}+_{t}\| f_{t-1}(_{t})- f_{t-1}(_{t-1}) \|_{2}^{2}\), where the first part can be directly converted to the desired gradient variation and the second part will be at most \(_{t}L^{2}\|_{t}-_{t-1}\|_{2}^{2}\) under global smoothness. Given the smoothness constant \(L\), tuning the step size as \(_{t} 1/(4L)\) ensures \((_{t}L^{2}\|_{t}-_{t-1}\|_{2}^{2}- {_{t}}\|_{t}-_{t-1}\|_{2}^{2}) 0\), thus obtaining the gradient-variation bound.

However, under generalized smoothness, we do not have a global parameter \(L\) for setting the step sizes, and the smoothness constants are related to the decisions. To follow the previous approach, optimistic OMD would require the smoothness constant _before_ generating \(_{t}\) to tune the step size, ensuring that the negative terms are large enough to cancel \(_{t}\| f_{t-1}(_{t})- f_{t-1}(_{t-1})\|_{2} ^{2}\). Nevertheless, the smoothness constant between \(_{t}\) and \(_{t-1}\) can only be evaluated _after_ updating to \(_{t}\), resulting in a contradiction. Unlike offline optimization, where the function is fixed and smoothness constants can be shown to decrease along the trajectory (Li et al., 2023), in online optimization, the online functions change at each round, preventing the reuse of previous smoothness estimations.

To address this challenge, our key idea is to perform a trajectory-wise analysis and configure the algorithm using estimated smoothness so far. The key technical lemma is the local smoothness property of \(_{t}\)-smooth functions (Li et al., 2023), which allows the smoothness constant between two points to be estimated in advance, provided that the two points are close enough.

**Lemma 1** (local smoothness (Li et al., 2023, Lemma 3.3)).: _Suppose \(f:\) is \(\)-smooth. For \(,\) such that \(\|-\|_{2})\|_{2}}{_{t }(2\| f()\|_{2})}\), \(\| f()- f()\|_{2}(2\| f()\|_{2})\|-\|_{2}\)._

Recall that in the update procedures (3) of optimistic OMD, the submitted decision \(_{t}\) is updated based on the intermediate decision \(}_{t}\). Therefore, it is convenient to control their distance and then exploit the local smoothness at point \(}_{t}\). Specifically, we set optimism \(M_{t}= f_{t-1}(}_{t})\) and the step size \(_{t} 1/(4_{t-1})\), where \(_{t-1}=_{t-1}(2\| f_{t-1}(}_{t})\|_{2})\) denotes the locally estimatedsmoothness and is used to tune the step size. This configuration leads to \(_{t}\| f_{t}(_{t})- f_{t-1}(}_{t})\| _{2}^{2}\) for the second term in Eq. (5), which can be further upper bounded as

\[\| f_{t}(_{t})- f_{t-1}(}_{t})\|_{2}^ {2} 2\| f_{t}(_{t})- f_{t-1}(_{t})\|_{2}^ {2}+2\| f_{t-1}(_{t})- f_{t-1}(}_{t}) \|_{2}^{2}.\] (6)

The first part is basically the favorable gradient variation, so it suffices to handle the second part. Performing the stability analysis for OMD and noticing the step size setting, it can be verified that \(\|_{t}-}_{t}\|_{2}_{t}\| f_{t-1}( }_{t})\|_{2}\| f_{t-1}(}_{t}) \|_{2}/(4_{t-1})\). This satisfies the criteria for applying Lemma 1 to the \(_{t-1}\)-smooth function \(f_{t-1}()\), allowing us to upper bound the second term in (6) by \((_{t-1}^{2}\|_{t}-}_ {t}\|_{2}^{2})\). We have clipped \(_{t}\) by \(1/(4_{t-1})\), thereby ensuring the negative term is sufficient to cancel out the positive term. Below, we summarize the result for convex functions.

**Theorem 1**.: _Under Assumptions 1 - 2 and assuming online functions are convex, we set the optimism as \(M_{t}= f_{t-1}(}_{t})\) and \(f_{0}()=0\), with step sizes as \(_{1}=D\) and, for \(t 2\),_

\[_{t}=}{1+_{s=1}^{t-1} f_{s} (_{s})- f_{s-1}(_{s})_{2}^{2}}},\ _{s[t]}(2 f_{s-1}( }_{s})_{2})}},\] (7)

_optimistic OMD in (4) ensures the following regret bound,_

\[_{T}(D}+_{} D^{ 2}),\]

_where \(V_{T}=_{t=2}^{T}_{} f_{t}()- f_{t-1}()_{2}^{2}\) measures the gradient variations and \(_{}=_{t[T]}_{t}\) is the maximum smoothness constant over the optimization trajectory._

This result implies a tighter bound in scenarios where the environments change slowly, i.e., \(V_{T}=(1)\). Meanwhile, it safeguards the worst-case optimal result since \(V_{T}(T)\) holds in all cases. When assuming \(_{t}() L\) for \(t[T]\), the \(_{t}\)-smoothness condition degenerates to the classic global \(L\)-smoothness condition, and our result implies an \((}+LD^{2})\) bound, which matches the best-known gradient-variation regret bounds with the first-order oracle (Chiang et al., 2012; Yan et al., 2023; Zhao et al., 2024) even in terms of the dependence on \(D\) and \(L\). Compared to offline optimization, our result depends on \(_{}\), the maximum smoothness constant along the trajectory. This dependence arises from the adversarial nature of online learning, where the loss functions chosen in consecutive rounds may differ significantly, making it hopeless to leverage the previous estimates of smoothness to improve the dependence.

We further provide an improved gradient-variation regret bound for _strongly convex_ functions, with step size tuning based on recent result under global smoothness (Chen et al., 2024, SS 3.4).

**Theorem 2**.: _Under Assumptions 1 - 2 and assuming online functions are \(\)-strongly convex, we set the optimism as \(M_{t}= f_{t-1}(}_{t})\), \(f_{0}()=0\), and step sizes as \(_{1}=2/\) and, for \(t 2\), \(_{t}=2/( t+16_{s[t]}_{s-1}(2 f_{s-1}( }_{s})_{2}))\), optimistic OMD in (4) ensures the regret bound \(_{T} V_{T}+ _{} D^{2}\), where \(_{}=_{t[T]}_{t}\)._

The above theorem requires the knowledge of curvature information \(\). In Section 3, we design a _universal_ method to remove this requirement and achieve the optimal guarantees for convex and strongly convex functions simultaneously without knowing \(\). In Appendix B.2, we discuss the challenge to obtain a gradient-variation bound for exp-concave functions under Assumption 1.

## 3 Universal Online Learning under Generalized Smoothness

Classic online learning algorithms require the curvature information of online functions as algorithmic parameters to achieve favorable regret guarantees. However, obtaining these curvature parameters can be difficult in practice. This challenge motivates the recent study of _universal online learning_(van Erven and Koolen, 2016; Cutkosky and Boahen, 2017; Wang et al., 2019; Mhammedi et al., 2019; Zhang et al., 2022; Yan et al., 2023; Yang et al., 2024), which aims to design a single algorithm that can achieve optimal regrets without knowing the curvature information. In this section, we study universal online learning with gradient-variation regret under generalized smoothness.

### Reviewing Related Work and Techniques

We review related work on gradient-variation universal online learning under _global_ smoothness (Zhang et al., 2022; Yan et al., 2023). To handle the unknown curvature, universal online learning algorithms utilize a two-layer structure, consisting of a meta-algorithm that ensembles a group of base-learners. Each base-learner optimizes functions with a specific convex curvature, while the meta-algorithm is designed to ensure that ensemble errors do not ruin base-learners' guarantees. Denoted by \(N\) the number of base-learners, the decision \(_{t}=_{i[N]}p_{t,i}_{t,i}\) submitted by a two-layer structure algorithm comprises two key components: \(_{t}_{N}\), the weights provided by the meta-algorithm, and \(_{t,i}\), the decision of the \(i\)-th base-learner. The analysis of a universal algorithm begins by decomposing the regret into two parts against any base-learner. In particular, we choose the base-learner with the best performance (the index \(i_{}\) is unknown) as the benchmark:

\[_{T}=^{T}f_{t}(_{t})-_{t=1}^ {T}f_{t}(_{t,i_{}})}_{}+^{T}f_{t}(_{t,i_{}})-_{}_{t =1}^{T}f_{t}()}_{:},\] (8)

where the first part is the meta-regret, evaluating the meta-algorithm's performance against the best base-learner, and the second part, known as the base-regret, measures the best learner's performance.

Zhang et al. (2022) advocate for a simple approach by employing the meta-algorithms with second-order regret guarantees, which facilitates the analysis at the meta level. In specific, they use Adapt-ML-Prod (Gaillard et al., 2014) as the meta-algorithm, showing that the meta-regret for strongly convex and exp-concave functions are constants by exploiting the negative terms from convexity. Consider \(\)-strongly convex functions as an example and assume the \(i_{}\)-th base-learner ensures the optimal \(( V_{T})\) base-regret. At the meta level, Zhang et al. (2022) pass the linearized regret \(r_{t,i}= f_{t}(_{t}),_{t}-_{t,i}\) to the meta-algorithm for each base-learner. By strong convexity and the guarantees of Adapt-ML-Prod, the meta-regret can be bounded by a constant:

\[_{t=1}^{T}r_{t,i_{}}-_{t= 1}^{T}_{t}-_{t,i_{}}_{2}^{2} ^{T}r_{t,i_{}}^{2}}-_{t=1}^{T} _{t}-_{t,i_{}}_{2}^{2}(1),\]

where the last inequality follows from \( f_{t}(_{t}),_{t}-_{t,i_{}}^{2}}_{}_{t }-_{t,i_{}}_{2}^{2}}\) and is then canceled by the negative terms via the AM-GM inequality. By leveraging the negative terms from strong convexity, the meta-regret can be well-bounded, allowing the overall regret to be dominated by the base-regret, which is then controlled by selecting appropriate base-algorithms.

However, this method is unsuitable for producing the gradient-variation bound for convex functions. To address it, Yan et al. (2023) propose to use a meta-algorithm which ensures an optimistic and second-order regret bound while provides additional negative terms \(-_{t}_{t}-_{t-1}_{1}^{2}\). Besides showing that the meta-regret is a constant for strongly convex and exp-concave functions following the previous approach, with newly designed optimism, Yan et al. (2023) prove that the meta-regret for convex functions can be roughly bounded by:

\[}+_{t=1}^{T}_{t,i_{}}- _{t-1,i_{}}_{2}^{2}+L^{2}_{t=1}^{T}_{t}- _{t-1}_{1}^{2}+L^{2}_{t=1}^{T}_{i=1}^{N}p_{t,i} _{t,i}-_{t-1,i}_{2}^{2}.\]

The first term is the gradient variation, matching the optimal order of convex functions. Yan et al. (2023) demonstrate that the remaining stability terms can be canceled through the collaboration between the base and meta levels (Zhao et al., 2024) with the prior knowledge of the global smoothness constant \(L\), thus obtaining the near-optimal gradient-variation bounds for convex functions as well. Nevertheless, the employed meta-algorithm already has a two-layer structure, resulting in a three-layer structure for the overall algorithm, which is relatively complicated.

### Key Challenges and Main Ideas

We aim to design a universal algorithm with the optimal gradient-variation bounds under generalized smoothness, which exhibits two challenges. First, the Lipschitz condition of online functions is unknown to the meta-algorithm, which requires the meta-algorithm to be _Lipschitz-adaptive_, provide a _second-order regret_, and enable _predictions with optimism_. Second, the combination of theensemble method further complicates the estimation of smoothness constants, making it challenging to tune algorithms properly and to cancel stability terms as Yan et al. (2023) did.

We tackle the second challenge by utilizing a _function-variation-to-gradient-variation_ conversion to derive the gradient-variation bounds at the meta level, drawing inspiration from the development of dynamic regret minimization (Bai et al., 2022). This conversion technique decouples the meta and base levels, allowing us to avoid cancellation-based analysis. To illustrate, suppose a meta-algorithm ensuring \(((_{t,i_{*}}-m_{t,i_{*}})^{2}})\) provided optimism \(_{t}=(m_{t,1},,m_{t,N})\). By setting \(_{t,i_{*}}=f_{t}(_{t,i_{*}})-f_{t}(_{})\) and \(m_{t,i_{*}}=f_{t-1}(_{t,i_{*}})-f_{t-1}(_{})\), where \(_{}\) is a fixed reference point, the meta regret bound becomes \(([(f_{t}(_{t,i})-f_{t-1}(_{t,i}) )-(f_{t}(_{})-f_{t-1}(_{}))]^{2}})\). By the mean value theorem, \([(f_{t}(_{t,i})-f_{t-1}(_{t,i}))-(f_{t}(_{})-f_{t-1}(_{}))]^{2}\) equals the first term below, which can be further upper bounded by the gradient variation:

\[[ f_{t}(_{t,i})- f_{t-1}(_{t,i}),_{t,i}- _{}]^{2} D^{2}_{}  f_{t}()- f_{t-1}()_{2}^{2}.\]

This technique brings hope for minimizing the convex functions. To develop a universal method, our first attempt is to utilize MsMwC-Master (Chen et al., 2021) as the meta-algorithm, which satisfies all the three requirements imposed by the first challenge. Nevertheless, the _heterogeneous_ inputs at the meta level present another challenge to this approach. The heterogeneity arises as we pass \(r_{t,i}=f_{t}(_{t})-f_{t}(_{t,i})\) to the meta-algorithm for base-learner responsible for convex functions, leveraging the conversion technique, while \(r_{t,i}= f_{t}(_{t}),_{t}-_{t,i}\) for base-learners minimizing strongly convex functions. It remains unclear how to adapt MsMwC-Master (Chen et al., 2021) to our heterogeneous inputs, as MsMwC-Master requires an \(_{t}\) as inputs, and the guarantee is for the regret in the form of \(_{t}=_{t},_{t}-_{t}\). However, such an \(_{t}\) cannot be retrieved from our above design. Fortunately, we observe that the Prod algorithms (Cesa-Bianchi et al., 2007; Gaillard et al., 2014; Wei et al., 2016) are friendly to heterogeneous inputs. Technically, the Prod algorithms provide the same guarantees as long as \(_{i[N]}p_{t,i}r_{t,i} 0\), thanks to the potential-based analysis. Therefore, aside from the requirements of the first challenge, we expect that the meta-algorithm can be analyzed similarly to the Prod algorithms, motivating us to design a new meta-algorithm. As a byproduct, we present a universal algorithm with a two-layer structure under global smoothness with the developed techniques. It is more efficient than that by Yan et al. (2023) and attains the optimal gradient-variation guarantees for convex, strongly convex, and exp-concave functions, at a cost of additional function value queries. We defer algorithms and regret bounds to Appendix C.5.

### A New Lipschitz-Adaptive Meta-Algorithm

In Algorithm 1, we present our meta-algorithm, which builds on optimistic Adapt-ML-Prod (Wei et al., 2016) and incorporates the clipping technique introduced by Cutkosky (2019) and further refined by Chen et al. (2021). This algorithm, described in the language of Prediction with Experts' Advice (PEA), may be of independent interest beyond adapting to the generalized smoothness. Apart for satisfying all expected requirements, this algorithm offers a simpler design, which does not require a forced restart as opposed to MsMwC-Master (Chen et al., 2021).

This efficiency improvement is achieved through a simple self-confident learning rate in Line 6 of Algorithm 1, unlike that uses fixed learning rates and thus require restarts. In essence, our approach incorporates the clipping mechanism by adding \(B_{t}^{2}\) to the denominator and removing the threshold on learning rates commonly applied in prior Prod algorithms (Gaillard et al., 2014; Wei et al., 2016). This term \(B_{t}^{2}\) acts as a threshold, ensuring that \(_{t,i}|_{t,i}-m_{t,i}| 1/2\), a critical condition in the analysis (typically satisfied when the Lipschitz constant is provided for prior Prod algorithms). Lipschitz-adaptive algorithms may be sensitive to the choice of \(B_{0}\), while in our case, \(B_{0}=(1/( T))\) is sufficient and does not ruin the guarantees as the factor \(()\) is often treated as a constant (Gaillard et al., 2014; Luo and Schapire, 2015). Theorem 3 summarizes the guarantee of Algorithm 1 and the proof is provided in Appendix C.3.

**Theorem 3**.: _Setting \(m_{t,i}=_{t},_{t-1}-_{t-1,i}\) in Algorithm 1 ensures that, for any \(i_{}[N]\), \(_{t=1}^{T}_{t},_{t}-_{t=1}^{T}_{t,i_{ *}}\) is bounded as follows, where \(B_{T}=\{B_{0},_{t[T]}_{t}-_{t}_{}\}\):_

\[^{T}(r_{t,i_{*}}-m_{t,i_{*}})^{2}}+B _{T}(N)+(B_{T}+ T)}^{T}_{t}-_{t-1} _{}^{2}}.\]```
0: prior information of the scale \(B_{0}\), the number of experts \(N\).
1:Initialization: set \(w_{1,i}=1\), \(m_{1,i}=0\) and \(_{1,i}=1/^{2}}\) for all \(i[N]\).
2:for\(t=1\)to\(T\)do
3: Update the weight for \(i[N]\)\(_{t,i}=w_{t,i}(_{t,i}m_{t,i})\);
4: Calculate decision \(_{t}_{N}\) with \(p_{t,i}=_{t,i}}{_{j[N]}_{t,j} _{t,j}}\) and submit it;
5: Receive \(_{t}\), update \(B_{t}=\{B_{t-1},\|_{t}-_{t}\|_{}\}\), and build \(_{t,i}=m_{t,i}+}{B_{t}}(r_{t,i}-m_{t,i})\);
6: Update the learning rate for \(i[N]\): \(_{t+1,i}=^{t}(_{s,i}-m_{s,i})^{2}+4B_{t }^{2}}}\);
7: Update the weight for \(i[N]\): \(w_{t+1,i}=(w_{t,i}(_{t,i}_{t,i}-_{t,i}^{2}(_{t,i}-m_{t,i})^{2}))^{}{_{t,i}}}\).
8:endfor ```

**Algorithm 1** Lipschitz Adaptive Optimistic Adapt-ML-Prod

Our algorithm improves efficiency at the cost of an additional factor \(()\). This factor is ignorable for universal online learning since we set \(N=( T)\), and the factor \(( T)\) is negligible. Considering other related Lipschitz-adaptive algorithms, Mhammedi et al. (2019) obtain a regret bound of \(((r_{t,i_{*}})^{2}((N)+(B_{T}T))}+B _{T}(N))\), which offers better dependence on the dominant term \((r_{t,i_{*}})^{2}}\) but it is unclear how to include optimism. Chen et al. (2021) achieve a bound of \(((_{t,i_{*}}-m_{t,i_{*}})^{2}(NT) }+B_{T}(NT))\) with a two-layer algorithm; however, the \(()\) term would ruin the desired \(( V_{T})\) bound for strongly convex functions. We remark that the compared methods enjoy other strengths not discussed here, such as the ability to compete with an arbitrary competitor \(_{N}\) and the versatility to handle various learning scenarios, while our method is sufficient for our purpose and the only option to tackle all the challenges as we mention in Section 3.2. Lastly, notice that the optimism \(_{t}\) involves the decision \(_{t}\), which might be improper since \(_{t}\) depends on \(_{t}\) as well. We refer readers to Appendix C.1 for efficiently setting \(_{t}\) through a univariate binary search.

We emphasize that optimism \(_{t}\) in our algorithm is not chosen arbitrarily. In Line 5, we clip the regret with optimism to keep them on the same scale. The performance is then evaluated based on the clipped regret \(_{t,i}\). For the analytical purpose, it is essential that \(_{t},_{t} 0\), and a sufficient condition for this is ensuring \(_{t},_{t} 0\), which imposes an additional requirement on \(_{t}\). In Appendix C.2, we discuss how this requirement introduces challenges for exp-concave functions minimization in universal online learning.

### Overall Algorithm and Regret Guarantees

The function-variation-to-gradient-variation technique decouples the design of universal methods into the base and meta levels, and we are ready to combine the proposed components together. We employ algorithms in Section 2.3 as the base-learners and use Algorithm 1 as the meta-learner, concluding in Algorithm 2. Theorem 4 presents its guarantee with the proof in Appendix C.4.

**Theorem 4**.: _Under Assumptions 1 - 2 and assuming a global lower bound such that \( f_{t}()\) for any \(,t[T]\), setting \(N=_{2}T+1\), defining the curvature coefficient pool \(\{2^{i-1}/T:i[N-1]\}\), and specifying \(B_{0}\), Algorithm 2 simultaneously ensures:_

\[_{T}\{(}  B_{T}),&\\ ( V_{T}+_{}^{2}^{2}(B_ {T})/+B_{T} B_{T}),&,.\]

_where \([1/T,1]\), \(B_{T}=(\{B_{0},D_{t[T]}_{}\| f_{t}( )- f_{t-1}()\|_{2}\})\) and \(_{}\) is the Lipschitz constant on the optimization trajectory._

Without loss of generality, we assume \([1/T,1]\) for strongly convex functions. If \(<1/T\), the optimal \((( V_{T})/)\) bound would imply linear regret, in which case we would treat them as general convex functions. If \(>1\), our result is slightly worse than the optimal one by a negligible constant factor. This simplification is also employed by Zhang et al. (2022); Yan et al. (2023).

**Remark 1**.: This theorem additionally requires the lower bound of loss functions, which is used to perform the binary search when setting the optimism. We defer the details of the binary search to Appendix C.1. This assumption is also employed recently in parameter-free optimizations (Hazan and Kakade, 2019; Attia and Koren, 2024; Khaled and Jin, 2024), and we can simply choose \(=0\) in empirical risk minimization settings (Hazan and Kakade, 2019).

## 4 Applications

In this section, we demonstrate the importance of our results by providing two applications (SEA model and online games), where new results can be directly implied from our findings.

### Stochastically Extended Adversarial (SEA) Model

The stochastically extended adversarial (SEA) model (Sachs et al., 2022) interpolates adversarial and stochastic online optimization. It assumes that the environments select the loss function \(f_{t}()\) from a distribution \(_{t}\). The adversarial nature is characterized by shifts in distribution \(_{t}\), and when \(_{t}\) remains constant, the model captures the environments' stochastic behavior. The following quantities are introduced to measure the levels of adversarial and stochastic behaviors in environments:

\[^{2}_{1:T}=[_{t=2}^{T}_{ } F_{t}()- F_{t-1}()_{2}^{2} ],^{2}_{1:T}=_{t=1}^{T}_{}[ f_{t}()- F_{t}()_{2}^{2}].\] (9)

where we denote by \(F_{t}()=_{f_{t}_{t}}[f_{t}()]\). In above, \(^{2}_{1:T}\) represents the adversarial shift of the distribution, and \(^{2}_{1:T}\) denotes the stochastic variance.

Sachs et al. (2022) prove an \((_{1:T}}+_{1:T}})\) regret for convex functions, and a refined regret bound of \(((^{2}_{}+^{2}_{})(^{2}_{1:T}+^ {2}_{1:T}))\) for strongly convex functions is obtained by Chen et al. (2023); Sachs et al. (2023), where \(^{2}_{}=_{t[T]}_{}[  f_{t}()- F_{t}()_{2}^{2}]\) and \(^{2}_{}=_{t=1}^{T}_{} F _{t}()- F_{t-1}()_{2}^{2}\). Yan et al. (2023) present a universal method which can obtain \(}(_{1:T}}+_{1:T}})\) and \(((^{2}_{}+^{2}_{})(^{2}_{1:T}+ ^{2}_{1:T}))\) bounds for convex and strongly convex functions. However, these results require the global smoothness assumption.

Our result in Section 3 implies a new finding for the SEA model, relaxing the assumption from the global smoothness to the generalized smoothness, while adapting to unknown curvature, summarized in Corollary 1. The proof can be found in Appendix D.1.

**Corollary 1**.: _Under Assumptions 1 - 2 and assuming a global lower bound for the loss functions such that \( f_{t}()\) for any \(,t[T]\), setting \(N=_{2}T+1\), defining the curvature coefficient pool \(=\{2^{i-1}/T:i[N-1]\}\), and specifying \(B_{0}\) with a specific value, then, under the SEA model, Algorithm 2 simultaneously ensures:_

\[[_{T}]\{( ^{2}_{1:T}}+_{1:T}})( {G}_{}D),&\\ ((^{2}_{}+^{2}_{})( ^{2}_{1:T}+^{2}_{1:T})),&,.\]

_where \(_{}\) is the maximum empirical Lipschitz constant, \(^{2}_{1:T}=_{t=1}^{T}[_{ } f_{t}()- F_{t}()_{2}^ {2}]\), and \(^{2}_{}=[_{t[T]}_{ } f_{t}()- F_{t}()_{2}^ {2}]\)._Note that, in real-world streaming learning applications, this corollary can offer a more generalized depiction of data throughput with limited computing resources (Zhou, 2024; Wang et al., 2024), given the connection between these scenarios and the SEA model (Chen et al., 2024, SS 5.6). Our result depends on \(_{1,T}^{2}\), a larger quantity than \(_{1,T}^{2}\) but still can track the stochastic variance. This is because our algorithm utilizes the information afterward \(_{t-1}\) to generate \(_{t}\). We refer the interested readers for this subtle issue to the discussion by Chen et al. (2024). This dependence currently is unknown how to improve even under the global smoothness condition since we need to leverage the function variation to produce gradient variation, inevitably involving the afterward information.

### Fast Rates in Games

Our second application explores the min-max game, aiming to achieve an \(\)-approximate solution to the problem \(_{}_{}f(, )\) within an \((1/T)\) fast convergence rate. Here, we assume that \(f(,)\) is convex for any \(\), and correspondingly, \(f(,)\) is concave given any \(\). Additionally, we assume that both \(^{n}\) and \(^{m}\) are bounded convex sets. Pioneering research (Syrgkanis et al., 2015) demonstrates that optimistic algorithms can reach a convergence rate of \((1/T)\) by leveraging gradient variation. However, these results are limited to the global smoothness condition. In this part, we demonstrate that our findings in Section 2 directly imply a new algorithm that can exploit generalized smoothness.

Following the notations of Nemirovski (2004), we define \(=,=(,) \) and introduce an operator \(F:^{n}^{m}\) with \(F()=(_{}f(,),-_{}f(,))\). We extend the concept of \(\)-smoothness to the min-max optimization setting as follows.

**Definition 2** (\(\)-smoothness for min-max game).: A differentiable convex-concave function \(f:\) is called \(\)-smooth with a non-decreasing link function \(:[0,+)(0,+)\) if it satisfies: for any \(_{1},_{2}\), if \(_{1},_{2},_{1})\|_{2}}{(2\| F()\|_{2})}\), then \(\|F(_{1})-F(_{2})\|_{2}(2\|F()\|)\| _{1}-_{2}\|_{2}\), where \((,r)\) denotes the Euclidean ball centered at point \(\) with radius \(r\).

This definition is a counterpart to Definition 1 in the min-max game, but weaker as it does not require the twice-differentiability requirement. The \(\)-approximate solution \((_{},_{})\) to the min-max game is formally defined by \(f(_{},)- f(_{},_{}) f(,_{})+\) for any \(,\). To achieve the fast convergence rate to the solution, indeed our result in Section 2 can be directly applied to the min-max game and obtains the following tailored algorithm for min-max optimization:

\[_{t}=_{}[}_{t}-_{t}F( }_{t})],}_{t+1}=_{ }[}_{t}-_{t}F(_{t})].\] (10)

In above we set the optimism at the point \(}_{t}\) in order to exploit the smoothness locally. We conclude our result in Corollary 2, with the proof available in Appendix D.2.

**Corollary 2**.: _Assume that the convex-concave function \(f(,)\) is \(\)-smooth, and the domain \(\) is bounded with diameter \(D\). By applying the tuning strategy described in Theorem 1, and defining the final approximated solution as \(}_{T}=_{t=1}^{T}_{t}\), where \(_{t}\) is generated by Eq. (10), we achieve an \(\)-approximate solution with a convergence rate of \((1/T)\)._

## 5 Conclusion

In this paper, we provide a systematic study of gradient-variation online learning under the generalized smoothness condition. We exploit trajectory-wise smoothness to achieve the optimal regret bounds: \((})\) for convex functions and \(( V_{T})\) for strongly convex functions, respectively. We further consider more complicated online learning scenarios, motivating us to design a new Lipschitz-adaptive meta-algorithm, which can be of independent interest. Hinging on this algorithm with the function-variation-to-gradient-variation technique, we design a universal algorithm which guarantees the optimal results for convex functions and strongly convex functions simultaneously without knowing the curvature. In addition, our findings directly imply new results in stochastic extended adversarial online learning and fast-rate games under generalized smoothness.

An important future direction for future research is to explore whether our method can be further extended to accommodate the one-gradient feedback model, where the learner receives only the gradient information of the decision submitted in each round. Another interesting problem is to exploit the exp-concavity in gradient-variation online learning under generalized smoothness.