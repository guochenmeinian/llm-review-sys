# Most Neural Networks Are Almost Learnable

Amit Daniely

Hebrew University and Google

amit.daniely@mail.huji.ac.il

&Nathan Srebro

TTI-Chicago

nati@ttic.edu

&Gal Vardi

TTI-Chicago and Hebrew University

galvardi@ttic.edu

###### Abstract

We present a PTAS for learning random constant-depth networks. We show that for any fixed \(>0\) and depth \(i\), there is a poly-time algorithm that for any distribution on \(^{d-1}\) learns random Xavier networks of depth \(i\), up to an additive error of \(\). The algorithm runs in time and sample complexity of \(()^{(^{-1})}\), where \(\) is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to \(()^{(^{-1})}\), resulting in a quasi-poly-time algorithm for learning constant depth random networks.

## 1 Introduction

One of the greatest mysteries surrounding deep learning is the discrepancy between its phenomenal capabilities in practice and the fact that despite a great deal of research, polynomial-time algorithms for learning deep models are known only for very restrictive cases. Indeed, state of the art results are only capable of dealing with two-layer networks under assumptions on the input distribution and the network's weights. Furthermore, theoretical study shows that even with very naive architectures, learning neural networks is worst-case computationally intractable.

In this paper, we contrast the aforementioned theoretical state of affairs, and show that, perhaps surprisingly, even though constant-depth networks are completely out of reach from a worst-case perspective, _most_ of them are not as hard as one would imagine. That is, they are _distribution-free learnable_ in polynomial time up to any desired constant accuracy. This is the first polynomial-time approximation scheme (PTAS) for learning neural networks of depth greater than \(2\) (see the related work section for more details). Moreover, we show that the standard SGD algorithm on a ReLU network can be used as a PTAS for learning random networks. The question of whether learning random networks can be done efficiently was posed by Daniely et al. , and our work provides a positive result in that respect.

In a bit more detail, we consider constant-depth random networks obtained using the standard Xavier initialization scheme , and any input distribution supported on the sphere \(^{d-1}\). For Lipschitz activation functions, our algorithm runs in time \(()^{(^{-1})}\), where \(\) is the network's size including the \(d\) input components, and \(\) is the desired accuracy. While this complexity is polynomial for constant \(\), we also consider the special cases of sigmoid and ReLU-like activations, where the bound can be improved to \(()^{(^{-1})}\).

The main technical idea in our work is that constant-depth random neural networks with Lipschitz activations can be approximated sufficiently well by low-degree polynomials. This result follows by analyzing the network obtained by replacing each activation function with its polynomial approximation using Hermite polynomials. It implies that efficient algorithms for learning polynomials can beused for learning random neural networks, and specifically that we can use the SGD algorithm on ReLU networks for this task.

### Results

In this work, we show that random fully-connected feedforward neural networks can be well-approximated by low-degree polynomials, which implies a PTAS for learning random networks. We start by defining the network architecture. We will denote by \(:\) the activation function, and will assume that it is \(L\)-Lipschitz. To simplify the presentation, we will also assume that it is normalized in the sense that \(_{X(0,1)}\,^{2}(X)=1\). Define \(_{}(n)=_{(p)=n}_{X(0,1)}(( X)-p(X))^{2}\), namely, the error when approximating \(\) with a degree-\(n\) polynomial, and note that \(_{n}_{}(n)=0\). We will consider fully connected networks of depth \(i\) and will use \(d_{0}=d\) to denote the input dimension and \(d_{1},,d_{i}\) to denote the number of neurons in each layer. Denote also \(=_{j=0}^{i}d_{j}\). Given weight matrices

\[=(W^{1},,W^{i})^{d_{1} d_{0}} ^{d_{i} d_{i-1}}\]

and \(^{d_{0}}\) we define \(^{0}_{}()=\). Then for \(1 j i\) we define recursively

\[^{j}_{}()=W^{j}^{j-1}_{}(), ^{j}_{}()=(^{j}_{}())\]

We will consider random networks in which the weight matrices are random _Xavier matrices_. That is, each entry in \(W^{j}\) is a centered Gaussian of variance \(}\). This choice is motivated by the fact that it is a standard practice to initialize the network's weights with Xavier matrices, and furthermore, it ensures that the scale across the network is the same. That is, for any example \(\) and a neuron \(n\), the second moment of the output of \(n\) (w.r.t. the choice of \(\)) is \(1\).

Our main result shows that \(^{i}_{}\) can be approximated, up to any constant accuracy \(\), via constant degree polynomials (the constant will depend only on \(\), the depth \(i\), and the activation \(\)). We will consider the input space \(}^{d-1}=\{^{d}:\|\|=1\}\). Here, and throughout the paper, \(\|\|\) stands for the _normalized_ Euclidean norm \(\|\|=_{i=1}^{d}x_{i}^{2}}\).

**Theorem 1.1**.: _For every \(i\) and \(n\) such that \(_{}(n)\) there is a constant \(D=D(n,i,)\) such that if \(d_{1},,d_{i-1} D\) the following holds. For any weights \(\), there is a degree \(n^{i-1}\) polynomial \(p_{}\) such that for any distribution \(\) on \(}^{d-1}\)_

\[}_{}}_{} \|^{i}_{}()-p_{}()\| 14 (L+1)^{2}(_{}(n))^{}}}{n^{}}}\.\]

_Furthermore, the coefficients of \(p_{}\) are bounded by \((2)^{4n^{i-1}}\)._

Since constant degree polynomials are learnable in polynomial time, Theorem 1.1 implies a PTAS for learning random networks of constant depth. In fact, as shown in , constant degree polynomials with polynomial coefficients are efficiently learnable via SGD on ReLU networks starting from standard Xavier initialization. Thus, this PTAS can be standard SGD on neural networks. To be more specific, for any constant \(>0\) there is an algorithm with \(()^{O((}{})^{(i-1)2^{i-1}})}\) time and sample complexity that is guaranteed to return a hypothesis whose loss is at most \(\) in expectation. For some specific activations, such as the sigmoid \((x)=(x):=}_{0}^{x}e^{-}{2}}dt\), or the ReLU-like activation \((x)=_{0}^{x}(t)+1dt\) we have that \(_{}(n)\) approaches to \(0\) exponentially fast (see Lemma A.4 in the appendix). In this case, we get get a _quasi-polynomial_ time and sample complexity of \(()^{O(((}{}))^{( i-1)})}\).

**Corollary 1.2**.: _For every constants \(,i\) and \(\) there is a constant \(D\), a univariate-polynomial \(p\) and a polynomial-time algorithm \(\) such that if \(d_{1},,d_{i-1} D\) the following holds. For any distribution \(\) on \(}^{d-1}\), if \(h\) is the output of \(\) upon seeing \(p(d_{0},,d_{i})\) examples from \(\), then1_

\[}_{h}}_{}}_{ }\|_{}^{i}()-h() \|\.\]

_Furthermore, \(\) can be taken to be SGD on a ReLU network starting from a Xavier initialization._

### Related Work

Learning neural networks efficiently.Efficiently learning classes of neural networks has attracted much interest in recent years. Several works established polynomial-time algorithms for learning one-hidden-layer neural networks with certain input distributions (such as the Gaussian distribution) under the assumption that the weight matrix of the hidden layer is non-degenerate [27; 34; 19; 20; 5; 32; 4]. For example, Awasthi et al.  showed such a result for non-degenerate one-hidden-layer ReLU networks with bias terms under Gaussian inputs, and also concluded that one-hidden-layer networks can be learned efficiently under the smoothed-analysis framework. Efficient algorithms for learning one-hidden-layer ReLU networks with Gaussian inputs were also shown in Diakonikolas et al. , Diakonikolas and Kane . These results do not require non-degenerate weight matrices, but they require that the output layer weights are all positive, as well as a sub-linear upper bound on the number of hidden neurons. Chen et al.  recently showed an efficient algorithm for learning one-hidden-layer ReLU networks with Gaussian inputs, under the assumption that the number of hidden neurons is a constant. Note that all of the aforementioned works consider only one-hidden-layer networks. Chen et al.  gave an algorithm for learning deeper ReLU networks, whose complexity is polynomial in the input dimension but exponential in the other parameters (such as the number of hidden units, depth, spectral norm of the weight matrices, and Lipschitz constant of the overall network). Finally, several works established algorithms for learning neural networks, whose complexity is exponential unless we impose strong assumptions on the norms of both the inputs and the weights [23; 30; 33; 24].

Hardness of learning neural networks.As we discussed in the previous paragraph, efficient algorithms for learning ReLU networks are known only for depth-\(2\) networks and under certain assumptions on both the network weights and the input distribution. The limited progress in learning ReLU networks can be partially understood by an abundance of hardness results.

Learning neural networks without any assumptions on the input distribution or the weights is known to be hard (under cryptographic and average-case hardness assumptions) already for depth-\(2\) ReLU networks [28; 3; 11]. For depth-\(3\) networks, hardness results were obtained already when the input distribution is Gaussian [13; 6]. All of the aforementioned hardness results are for improper learning, namely, they do not impose any restrictions on the learning algorithm or on the hypothesis that it returns. For _statistical query (SQ)_ algorithms, unconditional superpolynomial lower bounds were obtained for learning depth-\(3\) networks with Gaussian inputs , and superpolynomial lower bounds for _Correlational SQ (CSQ)_ algorithms were obtained already for learning depth-\(2\) networks with Gaussian inputs [25; 18].

The above negative results suggest that assumptions on the input distribution may not suffice for obtaining efficient learning algorithms. Since in one-hidden-layer networks efficient algorithms exist when imposing assumptions on both the input distribution and the weights, a natural question is whether this approach might also work for deeper networks. Recently, Daniely et al.  gave a hardness result for improperly learning depth-\(3\) ReLU networks under the Gaussian distribution even when the weight matrices are non-degenerate. This result suggests that learning networks of depth larger than \(2\) might require new approaches and new assumptions. Moreover,  showed hardness of learning depth-\(3\) networks under the Gaussian distribution even when a small random perturbation is added to the network's parameters, namely, they proved hardness in the smoothed-analysis framework. While adding a small random perturbation to the parameters does not seem to make the problem computationally easier, they posed the question of whether learning random networks, which roughly correspond to adding a large random perturbation, can be done efficiently. The current work gives a positive result in that respect.

Daniely and Vardi  studied whether there exist some "natural" properties of the network's weights that may suffice to allow efficient distribution-free learning, where a "natural" property is any property that holds w.h.p. in random networks. More precisely, they considered a setting where the target network is random, an adversary chooses some input distribution (that may depend on the target network), and the learning algorithm needs to learn the random target network under this input distribution. They gave a hardness result for improper learning (within constant accuracy) in this setting. Thus, they showed that learning random networks is hard when the input distribution may depend on the random network. Note that in the current work, we give a positive result in a setting where we first fix an input distribution and then draw a random network. Finally, learning deep random networks was studied in Das et al. , Agarwal et al. , where the authors showed hardness of learning networks of depth \(((d))\) in the SQ model.

## 2 Proof of Theorem 1.1

### Notation

We recall that for vectors \(^{d}\) we use the _normalized_ Euclidean norm \(\|\|=^{d}x_{i}^{2}}{d}}\) and take the unit sphere \(}^{d-1}=\{^{d}:\|\|=1\}\) w.r.t. this norm as our instance space. Inner products will also be normalized: for \(,^{d}\) we denote \(,=^{d}x_{i}y_{i}}{d}\). For \(^{d}\) and a closed set \(A^{d}\) we denote \(d(,A):=_{^{} A}\|-^{ }\|\). Unless otherwise specified, a random scalar is assumed to be a standard normal, a random vector in \(^{d}\) is assumed to be a centered Gaussian vector with covariance matrix \(I\), and a random matrix is assumed to be a Xavier matrix. For \(f:\), we denote \(\|f\|^{2}=_{X}\ f^{2}(X)\). We denote the Kronecker delta by \(_{ij}\), i.e. \(_{ij}=1\) if \(i=j\) and \(0\) otherwise.

### Some Preliminaries

We will use the Hermite Polynomials  which are defined via the following recursion formula.

\[h_{n+1}(x)=}h_{n}(x)-}h_{n-1}(x),\ \ h_{0}(x)=1,\ \ h_{1}(x)=x\] (1)

The Hermite polynomials are the sequence of normalized orthogonal polynomials w.r.t. the standard Gaussian measure. That is, it holds that

\[}_{X}h_{i}(X)h_{j}(X)=_{ij}\]

More generally, if \((X,Y)\) is a Gaussian vector with covariance matrix \(1&\\ &1\) then

\[}_{X,Y}h_{i}(X)h_{j}(Y)=_{ij}^{i}\] (2)

We will use the fact that

\[h_{n}^{}=h_{n-1}\] (3)

and that for even \(n\)

\[}_{X}X^{n}=(n-1)!!\] (4)

Let \(=_{i=0}^{}a_{i}h_{i}\) be the representation of the activation function \(\) in the basis of the Hermite polynomials. We will also use the _dual activation_\(()=_{i=0}^{}a_{i}^{2}^{i}\) as defined in . We note that \(\) is defined in \([-1,1]\) and satisfies \((1)=\|\|^{2}=1\).

### Defining a Shadow Network

In order to approximate \(_{}^{i}\) via a polynomial, we will use a "shadow network" that is obtained by replacing the activation \(\) with a polynomial approximation of it. We will show that for random networks we can approximate each activation sufficiently well with low-degree Hermite polynomials. Recall that \(=_{i=0}^{}a_{i}h_{i}\) is the representation of \(\) in the basis of the Hermite polynomials. Define \(_{n}=^{}a_{i}^{2}}}_{i=0}^{}a_ {i}h_{i}\). We have \(_{}(n)=_{i=n+1}^{}a_{i}^{2}\) and hence \(_{n}=(n)}}_{i=0}^{n}a_{i}h_{i}\).

We next define a shadow network. For \(^{d}\) we let \(^{0,n}_{}()=\). For \(1 j i\) we define recursively

\[^{j,n}_{}()=W^{j}^{j-1,n}_{}(), ^{j,n}_{}()=_{n}(^{j,n}_{}( ))\]

for \(1 j i-1\) and \(^{i,n}_{}()=W^{i}^{i-1,n}_{}()\). We will prove the following theorem, which implies Theorem 1.1.

**Theorem 2.1**.: _Fix \(i\) and let \(n\) be large enough so that \(_{}(n)\). There is a constant \(D=D(n,i,)\) such that if \(d_{1},,d_{i-2} D\) then for any \(}^{d-1}\),_

\[}_{W}\|^{i,}_{}()-^{i,n}_{ }()\| 13(L+1)^{2}(_{}(n))^{ }}\]

Since \(_{}(n)\) is the error in the approximation of a single activation \(\) with a degree-\(n\) polynomial, it is natural to expect that the above bound will depend on \(_{}(n)\). To see why Theorem 2.1 (together with Lemma A.3 which bounds \(_{}(n)\)) implies Theorem 1.1, note that \(^{i,n}_{}()\) is a polynomial of degree \(n^{i-1}\). This implies Theorem 1.1, except the requirement that the coefficients of the polynomial are polynomially bounded. To deal with this, define

\[^{i,n}_{}()=^{i,n}_{ }()&$ are at most $_{j=0}^{i}d_{j}$}\\ 0&\]

As we show next \(_{(d_{1},,d_{i-1})}_{}\|^{i,n }_{}()-^{i,n}_{}()\|=0\). Hence, in the theorem we can replace \(^{i,n}\) by \(^{i,n}\) which has polynomially bounded coefficients. See Appendix A.3 and A.4 for the proofs.

**Lemma 2.2**.: _For every \(\) and \(n\) there is a constant \(D\) such that if \(d_{1},,d_{i-1} D\) then for any \(}^{d-1}\), \(_{}\|^{i,n}_{}()-^{i,n}_{}()\|<\)._

**Lemma 2.3**.: \(^{i,n}_{}\) _computes a polynomial whose sum of coefficients is at most \((2)^{4n^{i-1}}\)._

### Proof of Theorem 2.1 for depth-two networks

We will first prove Theorem 2.1 for depth-\(2\) networks (i.e. for \(i=2\)). We will prove Lemma 2.5 below which implies that for every \(\) there is \(n\) such that for any \(}^{d-1}\), \(_{}\|^{1,n}_{}()-^{1}_{ {W}}()\|\). We will then prove Lemma 2.6, that together with Lemma 2.5 will show that \(_{}\|^{2,n}_{}()-^{2}_{ {W}}()\|\), thus proving Theorem 2.1 for \(i=2\). We will start however with the following lemma that will be useful throughout (see Appendix A.5 for the proof).

**Lemma 2.4**.: _Fix \(f,g:\), \(,^{d_{1}}\) and a Xavier matrix \(W^{d_{2} d_{1}}\). Let \((X,Y)\) be a centered Gaussian vector with covariance matrix \(\|\|^{2}&,\\ ,&\|\|^{2}\). Then_

\[}_{W}\|f(W)-g(W)\|}_{W}\|f(W)-g(W)\|^{2}}= }_{X,Y}(f(X)-g(Y))^{2}}\]

**Lemma 2.5**.: _Fix \(}^{d_{1}-1}\). Let \(W^{d_{2} d_{1}}\) be a Xavier matrix. Then_

\[}_{W}\|(W)-_{n}(W)\| (n)}\]

Proof.: By Lemma 2.4 we have

\[}_{W}\|(W)-_{n}(W)\| }_{W}\|(W)-_{n}(W )\|^{2}}=}_{X}((X)-_{n}(X)) ^{2}}\;.\]Now, the above equals to

\[^{n}(1-(n)}} )^{2}a_{i}^{2}+_{i=n+1}^{}a_{i}^{2}} = (n))(1-(n)}})^{2}+_{}(n)}\] \[= (n))((n)}-1}{(n)}})^{2}+_{ }(n)}\] \[= (n)-2(n)}+ _{}(n)}\] \[= (n)})}\] \[ (n)})(1+(n)})}\] \[= (n)}\]

Lemma 2.5 implies that \(_{}\|_{}^{1,n}()-_ {}^{1}()\|(n)}\). Thus, given \(>0\), for sufficiently large \(n\), \(_{}\|_{}^{1,n}()-_ {}^{1}()\|\). The following lemma therefore implies that \(_{}\|_{}^{2,n}()- _{}^{2}()\|(n)}\) and thus implies Theorem 2.1 for depth two networks.

**Lemma 2.6**.: _For any \(}^{d-1}\)_

\[_{W^{i}}\|_{}^{i,n}()-_{ }^{i}()\|\|_{}^{i-1,n} ()-_{}^{i-1}()\|\]

Proof.: By Lemma 2.4 we have

\[_{^{i}}\|_{}^{i,n}( )-_{}^{i}()\| = _{W^{i}}\|W^{i}(_{}^{i-1,n} ()-_{}^{i-1}())\|\] \[ _{^{i-1}}(_{ }\|_{}^{i-1,n}()-_{ }^{i-1}()\|^{2})^{X^{2}}}\] \[= \|_{}^{i-1,n}()-_{ {W}}^{i-1}()\|\]

### Proof of Theorem 2.1 for General Networks

For \(}^{d_{i-1}}\) we denote \(_{W^{i}}()=(W^{i})\) and \(_{W^{i}}^{n}()=_{n}(W^{i})\). Lemma 2.5 can be roughly phrased as

\[(=^{})(\|\|=1) _{W^{i}}()_{W^{i}}^{n}(^{})\]

In order to prove Theorem 2.1 for general networks we will extend it by replacing the strict equality conditions with softer ones. That is, we will show that

\[(^{})(\|\| 1) (\|^{}\| 1)_{W^{i}}() _{W^{i}}^{n}(^{})\] (5)

This will be enough to prove Theorem 2.1 for general networks. Indeed, the conditions \(\|\| 1\) and \(\|^{}\| 1\) are valid w.h.p. via a simple probabilistic argument. Thus, Eq. (5) implies that

\[^{}_{W^{i}}() _{W^{i}}^{n}(^{})\] (6)

Now, for \(}^{d-1}\) Eq. (6) implies that \(_{W^{1}}()_{W^{1}}^{n}(^{})\). Using Eq. (6) again we get that \(_{W^{2}}_{W^{1}}()_{W^{2}}^{n}_{W^{ 1}}^{n}(^{})\). Using it \(i-3\) more times we we get that \(_{W^{i-1}}_{W^{1}}()_{W^{i-1}}^{n} _{W^{1}}^{n}(^{})\), or in other words that \(_{}^{i-1}()_{}^{i-1,n}( )\). As we will show "\(\)" stands for a sufficiently strong approximation, which guarantees that \(_{}\|_{}^{i-1}()-_ {}^{i-1,n}()\|\), and hence Lemma 2.6 implies Theorem 2.1.

To prove Eq. (5) we first prove Lemma 2.7 which softens the requirement that \(=^{}\). That is, it shows that

\[(^{})(\|\|=\|^{ }\|=1)_{W^{i}}()_{W^{i}}^{n}( ^{})\]

The second condition which requires that \(\|\|=\|^{}\|=1\) is softened via Lemmas 2.8 and 2.9. Lemma 2.10 then wraps the two softenings together, and shows that Eq. (5) is valid. Finally, in section 2.5.1 we use Lemma 2.10 to prove Theorem 2.1.

**Lemma 2.7**.: _Fix \(,+}^{d_{1}-1}\) with \(\|\|\). Let \(W^{d_{2} d_{1}}\) be a Xavier matrix. Then_

\[}_{W}\|(W)-_{n}(W(+))\|(n)}+}{1-_{}(n)}}\]

Proof.: We have

\[\|(W)-_{n}(W(+))\|\|(W )-_{n}(W)\|+\|_{n}(W)-_{n}(W( +))\|\]

By Lemma 2.5 we have \(}_{W}\|(W)-_{n}(W)\| (n)}\). It remains to bound \(}_{W}\|_{n}(W)-_{n}(W(+ ))\|.\) By Lemma 2.4 We have

\[}_{W}\|_{n}(W)-_{n}(W(+ ))\|}_{X,Y}(_{n}(X)-_{n}(Y) )^{2}}\]

where \((X,Y)\) is a centered Gaussian vector with correlation matrix \(1&\\ &1\) for \(=,+ 1-\). Finally, we have

\[}_{X,Y}(_{n}(X)-_{n}(Y))^{2} = (n)}}_{X,Y}( _{i=0}^{n}a_{i}(h_{i}(X)-h_{i}(Y)))^{2}\] \[= (n)}_{i=0}^{n}_{j=0}^{n}a_{i} a_{j}}_{X,Y}(h_{i}(X)-h_{i}(Y))(h_{j}(X)-h_{j}(Y))\] \[)}}{{=}} (n)}_{i=0}^{n}a_{i}^{2}(2-2^{i})\] \[ (n)}((1)-())\]

In Lemma A.1 we show that \(\) is \(L^{2}\)-Lipschitz. Hence the above is at most \(}{1-_{}(n)}\). 

We next give a lemma that allows us to "almost jointly project" a pair of points \(_{1},_{2}^{d}\) on a closed set \(A^{d}\), without expanding the distance too much. See Appendix A.6 for the proof.

**Lemma 2.8**.: _Let \(A^{d}\) a closed set and fix \(_{1},_{2}^{d}\). There are \(}_{1},}_{2} A\) such that_

\[\|_{1}-}_{1}\| 2d(_{1},A),\ \ \|_{2}-}_{2}\| 2d( _{2},A)\ \ \ \ \|}_{1}-}_{2}\| 3\|_{1}- _{2}\|\]

**Lemma 2.9**.: _Let \(,+^{d_{1}}\) be vectors such that \(\|\|=1\) and \(\|\| 1\). Let \(W^{d_{2} d_{1}}\) be a Xavier matrix. Then_

\[}_{W}\|(W)-(W(+))\|  L\]

_and_

\[}_{W}\|_{n}(W)-_{n}(W(+ ))\| 2^{2n+1}(9(4n-1)!!)^{1/4}=:(n)\]

Proof.: Fix a centered Gaussain vector \((X,Y)\) with covariance matrix \(1&+,\\ +,&\|+\|^{2} \). Let \(Z=Y-X\). Note that \((Z)^{2}\). By Lemma 2.4 we have

\[}_{W}\|(W)-(W(+))\| }((X)-(X+Z))^{2}} }Z^{2}} L\]We now prove the second part. In Lemma A.2, we show that \(|h_{i}(x)-h_{i}(x+y)| 2^{i}(|x|,|x+y|,1)^{i}|y|\). Therefore,

\[|_{n}(x)-_{n}(x+y)| _{i=0}^{n}|}{(n)}}|h_ {i}(x)-h_{i}(x+y)|\] \[|y|_{i=0}^{n}2^{i}(|x|^{i},|x+y|^{i},1)\] \[|y|2^{n+1}(|x|^{n},|x+y|^{n},1)\]

Hence,

\[(_{n}(X)-_{n}(X+Z))^{2}&&2^{2n+2 }\,\,Z^{2}(|X|^{n},|X+Z|^{n},1)^{2}\\ &&2^{2n+2}\,Z^{4}}(|X|^{4n},|X+Z|^{4n},1)}\\ &&2^{2n+2}\,Z^{4}}[|X|^{4n}+|X+Z|^{4n }+1]}\\ &}}{{=}}&2^{2n+2} \|^{4}}+{ v}\|^{4n}+\|{ x}\|^{4n})}\\ &&2^{2n+2}}(4n-1)!!}\\ &&2^{2n+2}}(4n-1)!!}\]

Lemma 2.4 now implies that

\[\,\|_{n}(W{ x})-_{n}(W({ x}+{  v}))\|&&2^{2n+1}(9(4n-1)!!)^{1/4}\]

**Lemma 2.10**.: _Let \({ x},{ x}+{ v}^{d_{1}}\) be vectors such that \(\|{ v}\|\), \(\|\|{ x}\|-1| 1/2\) and \(\|\|{ x}+{ v}\|-1|\). Let \(W^{d_{2} d_{1}}\) be a Xavier matrix. Then_

\[\,\|(W{ x})-_{n}(W({ x}+{ v }))\| 2L+(n)}+}{1- _{}(n)}}+2(n)\]

Proof.: By Lemma 2.8 there are vectors \({ x}^{},{ v}^{}\) such that \(\|{ x}^{}\|=\|{ x}^{}+{ v}^{}\|=1\) and

\[\|{ x}-{ x}^{}\| 2,\ \|{ x}+{ v}-{ x}^{}-{  v}^{}\| 2,\ \ \|{ v}^{}\| 3\|{ v}\|\]

Now, we have, by Lemmas 2.7 and 2.9,

\[\,\|(W{ x})-_{n}(W({ x}+{ v }))\|&&\,\|(W{ x})-(W{ x}^{})\|+ \,\|(W{ x}^{})-_{n}(W({ x}^{}+{ v }^{}))\|\\ &&+\,\,\|_{n}(W({ x}^{}+{ v}^{}))-_{n} (W({ x}+{ v}))\|\\ &&2L+(n)}+}{1- _{}(n)}}+2(n)\]

#### 2.5.1 Concluding the proof of Theorem 2.1

Define

\[^{i}_{}({ x},)=0&|1-\|^{j}_{ }({ x})\|>\ \ |1-\|^{j,n}_{}({ x})\|>\ \ j<i\\ ^{i}_{}({ x})&\]

and

\[^{i,n}_{}({ x},)=0&|1-\|^{j}_{ }({ x})\|>\ \ |1-\|^{j,n}_{}({ x})\|>\ \ j<i\\ ^{i,n}_{}({ x})&\]We have

\[}_{}\|_{}^{i}( )-_{}^{i,n}()\| }_{}\|_{}^{i}( )-_{}^{i}(,)\|+}_{}\|_{}^{i}(,)- _{}^{i,n}(,)\|\] \[+}_{}\|_{}^{i,n}(,)-_{}^{i,n}()\|\]

Theorem 2.1 now follows from Lemmas 2.11 and 2.12 below, together with Lemma 2.6.

**Lemma 2.11**.: _Let \(n\) be large enough so that \(_{}(n)\) and let \(<(n)}}{2L+2(n)}\). Then,_

\[}_{}\|_{}^{i}(,)-_{}^{i,n}(,)\| 12(L+1)^{2} (_{}(n))^{2^{-i}}\]

Proof.: We will prove the result by induction on \(i\). The case \(i=0\) is clear as \(_{}^{0}(,)=_{}^{0,n}( ,)\). Fix \(i>0\). For every \(<\) and \(n\) we have by Lemma 2.10

\[}_{W^{i}}\|_{}^{i}( ,)-_{}^{i,n}(,)\|  2L+(n)}+}{1- _{}(n)}\|_{}^{i-1}(,)- _{}^{i-1,n}(,)\|+2(n)}\]

Taking expectation over \(W^{1},,W^{i-1}\) we get

\[}_{}\ \|_{}^{i}( ,)-_{}^{i,n}(,)\|\] \[ 2L+(n)}+}_{ }}{1-_{}(n)}\|_{ }^{i-1}(,)-_{}^{i-1,n}(,)\|}+2(n)\] \[}}{{}} 2L+(n)}+}{1- _{}(n)}}_{}\|_{ }^{i-1}(,)-_{}^{i-1,n}(,)\|}+2(n)\] \[(n)}}{2L +2(n)}}}{{}} 4(n)}+}{1-_{ }(n)}}_{}\|_{}^{i -1}(,)-_{}^{i-1,n}(,)\|\] \[(n)}}{{}} 4(n)}+L}_{ }\|_{}^{i-1}(,)-_{ }^{i-1,n}(,)\|}\] \[}}{{}} 4(n)}+L (_{}(n))^{2^{-i+1}}}\] \[ (L+1)(_{}(n))^{2^{-i+1}}}\] \[= 12(L+1)^{2}(_{}(n))^{2^{-i}}\]

**Lemma 2.12**.: _Fix \(i,n,\) and \(>0\). There is a constant \(D\) such that if \(d_{1},,d_{i-1} D\) then_

\[}_{}\|_{}^{i}( )-_{}^{i}(,)\|+}_{ }\|_{}^{i,n}(,)-_{ }^{i,n}()\|\]

Proof sketch (see Appendix A.7 for the formal proof).: Let \(B_{i,}\) be the event that for some \(j<i\), \(|1-\|_{}^{j}()\|>\) or \(|1-\|_{}^{j,n}()\|\|>\). We have

\[}_{}\|_{}^{i}( )-_{}^{i}(,)\|=}_{ }[\|_{}^{i}()\|1_{B_{ i,}}]}_{}[\|_{ }^{i}()\|^{2}]})}\.\]

Similarly,

\[}_{}\|_{}^{i,n}( )-_{}^{i,n}(,)\|}_{}[\|_{}^{i,n}() \|^{2}]})}\.\]

Now, the lemma follows by proving that \(}_{}\|_{}^{i}() \|^{2}\) and \(}_{}\|_{}^{i,n}() \|^{2}\) are bounded by a constant (independent of \(d_{0},,d_{i}\)), and that for every \(,^{},i\) and \(n\), there is a constant \(D\) such that if \(d_{1},,d_{i-1} D\) then \((B_{i,})<^{}\)Conclusion and Future work

One of the prominent approaches for explaining the success of neural networks is trying to show that they are capable of learning complex and "deep" models. So far this approach has relatively limited success. Despite that significant progress has been made to show that neural networks can learn shallow models, so far, neural networks were shown to learn only "toy" deep models (e.g. [21; 2; 10; 31]). Not only that, but there are almost no known rich families of deep models that are efficiently learnable by _some_ algorithm (not necessarily gradient methods on neural networks). Our paper suggests that random neural networks might be candidate models. To take this approach further, a natural next step, and a central open question that arises from our work, is to show the existence of an algorithm that learns random networks in time that is polynomial both in \(\) and the network size. This question is already open for depth-two ReLU networks with two hidden neurons. We note that as implied by , such a result, even for a single neuron, will have to go beyond polynomial approximation of the network, and even more generally, beyond kernel methods.

Our result requires a lower bound \(D\) for the network's width, where \(D\) is a constant. We conjecture that this requirement can be relaxed, and leave it to future work. Additional open directions are: (i) the analysis of random convolutional networks, (ii) achieving time and sample complexity of \(()^{O(^{-2})}\) for random networks of any constant depth (and not only for depth two), and (iii) finding a PTAS for random networks of depth \((1)\).