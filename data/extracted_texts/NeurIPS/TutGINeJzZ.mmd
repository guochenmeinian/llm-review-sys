# A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy

Puning Zhao

Zhejiang Lab

pnzhao@zhejianglab.com

&Lifeng Lai

University of California, Davis

lflai@ucdavis.edu

&Li Shen

Sun Yat-Sen University

mathshenli@gmail.com

&Qingming Li

Zhejiang University

liqn@zju.edu.cn

&Jiafei Wu

Zhejiang Lab

wujiafei@zhejianglab.com

&Zhe Liu

Zhejiang Lab

zhe.liu@zhejianglab.com

Corresponding author.

Zhejiang Lab

###### Abstract

Privacy protection of users' entire contribution of samples is important in distributed systems. The most effective approach is the two-stage scheme, which finds a small interval first and then gets a refined estimate by clipping samples into the interval. However, the clipping operation induces bias, which is serious if the sample distribution is heavy-tailed. Besides, users with large local sample sizes can make the sensitivity much larger, thus the method is not suitable for imbalanced users. Motivated by these challenges, we propose a Huber loss minimization approach to mean estimation under user-level differential privacy. The connecting points of Huber loss can be adaptively adjusted to deal with imbalanced users. Moreover, it avoids the clipping operation, thus significantly reducing the bias compared with the two-stage approach. We provide a theoretical analysis of our approach, which gives the noise strength needed for privacy protection, as well as the bound of mean squared error. The result shows that the new method is much less sensitive to the imbalance of user-wise sample sizes and the tail of sample distributions. Finally, we perform numerical experiments to validate our theoretical analysis.

## 1 Introduction

Privacy is one of the major concerns in modern data analysis. Correspondingly, differential privacy (DP)  has emerged as a standard framework of privacy protection. Various statistical problems have been analyzed with additional DP requirements [2; 3; 4; 5]. Among all these problems, mean estimation is a fundamental one [6; 7; 8; 9], which is not only useful in its own right , but also serves as a building block of many other tasks relying on estimating gradients, such as private stochastic optimization [11; 12; 13; 14; 15; 16] and machine learning [17; 18; 19; 20; 21; 22; 23]. Existing research on DP mean estimation focuses primarily on item-level cases, i.e. each user contributes only one sample. However, in many practical scenarios, especially in recommendation systems [24; 25; 26] and federated learning [27; 28; 29; 30; 31; 32], a user has multiple samples. We hope to regard them as a whole for privacy protection.

In recent years, a flurry of works focus on user-level DP [33; 34; 35; 36]. The most popular one is the Winzorized Mean Estimator (WME) proposed in , which takes a two-stage approach. In the first stage, WME identifies an interval, which is small but contains the ground truth \(\) with high probability. In the second stage, WME clips user-wise averages to control the sensitivity and then calculates the final average with appropriate noise. This method can be extended to high dimensionality by Hadamard transform . The convergence rate has been established in  under some idealassumptions. Despite the merit of the two-stage approach from the theoretical perspective, this method may face challenges in many realistic settings. Firstly,  assumes that users are balanced, which means that users have the same number of items. Nevertheless, in federated learning applications, it is common for clients (each client is regarded as a user here) to possess different numbers of samples [38; 39; 40]. Secondly, this method is not suitable for heavy-tailed distributions, which is also common in reality [41; 42; 43; 44; 45; 46]. For heavy-tailed distributions, the interval generated in the first stage needs to be large enough to prevent clipping bias, which results in large sensitivity. As a result, stronger additive noise is needed for privacy protection, which significantly increases the estimation error. These drawbacks hinder the practical application of user-level DP. We aim to propose new solutions to address these challenges.

Towards this goal, in this paper, we propose a new method, which estimates the mean using Huber loss minimizer , and then adds noise for privacy protection. A challenge is that to determine an appropriate noise strength, it is necessary to conduct a thorough analysis of the local sensitivity that considers all possible datasets. To overcome this challenge, we divide datasets into three types, including those with no outliers, a few outliers, and many outliers, and analyze these cases separately. Based on the sensitivity analysis, we then use the smooth sensitivity framework  to determine the noise strength carefully.

Our method has the following advantages. Firstly, our method adapts well to imbalanced datasets, since the threshold \(T_{i}\) of Huber loss are selected adaptively according to the sample size per user, which leads to a better tradeoff between sensitivity and bias. Secondly, our method performs better for heavy-tailed distributions, since we control sensitivity by penalizing large distances using Huber loss, which yields a smaller bias than the clipping operation. Apart from solving these practical issues, it worths mentioning that our method solves robustness (to model poisoning attacks) and privacy issues simultaneously. In modern data analysis, it is common for a system to suffer from both poisoning and inference attacks at the same time [49; 50; 51]. Consequently, many recent works focus on unified methods for item-level DP and robustness to cope with both attacks simultaneously [52; 53; 54; 55]. To the best of our knowledge, our method is the first attempt to unify robustness and DP at user-level.

The main contributions are summarized as follows.

* We propose the Huber loss minimization approach, which finds the point with minimum Huber distance to all samples. Our method is convenient to implement and only requires linear time complexity.
* For the simplest case with balanced users, we provide a theoretical analysis, which shows that our method makes a slight improvement for bounded distributions and a significant improvement for heavy-tailed distributions over the two-stage approach.
* For imbalanced users, we design an adaptive strategy to select weights and connecting points in Huber loss, which makes our method much less sensitive to the imbalance of local sample sizes of users.
* We conduct experiments using both synthesized and real data, which also verify the effectiveness of the proposed method for imbalanced users and heavy-tailed distributions.

## 2 Related Work

**User-level DP.** applies a brute-force clipping method  for user-level DP in federated learning.  made the first step towards optimal rates under user-level DP, which analyzed discrete distribution estimation problems. The popular method WME was proposed in , which uses the idea of two-stage approaches [55; 56; 57]. The two-stage method for user-level DP has also been extended to stochastic optimization problems [58; 59].  analyzes mean estimation for boolean signals under user-level DP in heterogeneous settings. There are also some works focusing on black-box conversion from item-level DP to user-level counterparts.  analyzed general statistical problems, which shows that a class of algorithms for item-level DP problems having the pseudo-globally stable property can be converted into user-level DP algorithms. Following ,  expanded such transformation for any item-level algorithms.  extends the works to smaller \(m\). It is discussed in  that these black-box methods have suboptimal dependence on \(\). [62; 63; 64] studies user-level DP under local model.

**From robustness to DP.** Robustness and DP have close relationships since they both require the outputs to be insensitive to minor changes in input samples. There are three types of methods for conversion from robust statistics to DP. The first one is propose-test-release (PTR), which was first proposed in , and was extended into high dimensional cases in . The second choice is smooth sensitivity , which calculates the noise based on the "smoothed" local sensitivity. For example,  designed a method to protect trimmed mean with smooth sensitivity. The third solution is inverse sensitivity , which can achieve pure differential privacy (i.e. \(=0\)). All these methods require a detailed analysis of the sensitivity. For some recently proposed high dimensional estimators , the sensitivity is usually large and hard to analyze. As a common method for robust statistics , Huber loss minimization has been widely applied in robust regression , denoising  and robust federated learning . Huber loss has also been used in DP  for linear regression problems.

**Concurrent work.** After the initial submission of this paper, we notice an independent work , which also studies mean estimation under user-level DP (which is called person-level DP in ).  considers _directional_ bound, which requires that the moment is bounded in every direction. However, we consider _non-directional_ bound, which bounds the \(_{2}\) norm of a random vector. We refer to Section 1.3.1 in  for further discussion.

Our work is the first attempt to use the Huber loss minimization method in user-level DP. With an adaptive selection of weights and connecting points between quadratic and linear parts, our method achieves a significantly better performance for imbalanced users and heavy-tailed distributions.

## 3 Preliminaries

In this section, we introduce definitions and notations. To begin with, we recall some concepts of DP and introduce the notion of user-level DP. Denote \(\) as the space of all datasets, and \(\) as the space of possible outputs of an algorithm.

**Definition 1**.: _(Differential Privacy (DP) ) Let \(, 0\). A function \(:\) is \((,)\)-DP if for any measurable subset \(O\) and any two adjacent datasets \(\) and \(^{}\),_

\[(() O) e^{}(( ^{}) O)+,\] (1)

_in which \(\) and \(^{}\) are adjacent if they differ only on a single sample. Moreover, \(\) is \(\)-DP if (1) holds with \(=0\)._

Definition 1 is about item-level DP. In this work, we discuss the case where the dataset contains multiple users, i.e. \(=\{D_{1},,D_{n}\}\), with the \(i\)-th user having \(m_{i}\) samples. Considering that the sample sizes of users are usually much less sensitive , throughout this work, we assume that the local sample sizes \(m_{i}\) are public information. Under this setting, user-level DP is defined as follows.

**Definition 2**.: _(User-level DP ) Two datasets \(\), \(^{}\) are user-level adjacent if they differ in items belonging to only one user. In particular, if \(=\{D_{1},,D_{n}\}\), \(^{}=\{D^{}_{1},,D^{}_{n}\}\), in which \(|D_{i}|=|D^{}_{i}|=m_{i}\) for all \(i\), and there is only one \(i[n]\) such that \(D_{i} D^{}_{i}\), then \(\) and \(^{}\) are user-level adjacent. A function \(\) is user-level \((,)\)-DP if (1) is satisfied for any two user-level adjacent datasets \(\) and \(^{}\)._

Since \(m_{i}\), \(i=1,,N\) are public information, in Definition 2, it is required that \(|D_{i}|=|D^{}_{i}|\), which means that two adjacent datasets need to have the same sample sizes for all users. We then state some concepts related to sensitivity, which describes the maximum change of the output after replacing a user with another one:

**Definition 3**.: _(Sensitivity) Define the local sensitivity of function \(f\) as_

\[LS_{f}()=_{d_{H}(,^{})=1}\|f( )-f(^{})\|,\] (2)

_in which \(d_{H}(,^{})=_{i=1}^{n}(D_{i} D^ {}_{i})\) denotes the Hamming distance. The global sensitivity of \(f\) is \(GS_{f}=_{}LS_{f}()\)._

Adding noise proportional to the global sensitivity can be inefficient, especially for user-level problems. In this work, we use the smooth sensitivity framework .

**Definition 4**.: _(Smooth sensitivity) \(S_{f}\) is a \(\)-smooth sensitivity of \(f\), if (1) for any \(\), \(S_{f}() LS_{f}()\); (2) for any neighboring \(\) and \(^{}\), \(S_{f}() e^{}S_{f}(^{})\)._

The smooth sensitivity can be used to determine the scale of noise. In this work, the noise follows Gaussian distribution. It has been shown in  that if \((0,(S^{2}()/^{2}))\), then the final output \(f()+\) is \((,)\)-DP for the following \((,)\) pair:

\[=\{}}}{}&&d=1\\ }}&&d>1,. =\{}{}}&&d=1\\ )}&&d>1.. \] (3)

**Notations.** Throughout this paper, \(\|\|\) denotes the \(_{2}\) norm by default. \(a b\) means that \(a Cb\) for some absolute constant \(C\), and \(\) is defined conversely. \(a b\) if \(a b\) and \(b a\).

## 4 The Proposed Method

This section introduces the algorithm structures. Details about parameter selection are discussed together with the theoretical analysis in Section 5 and 6, respectively. For a dataset \(=\{D_{1},,D_{n}\}\), in which \(D_{i}^{d}\) is the local dataset of the \(i\)-th user, denote \(m_{i}=|D_{i}|\) as the sample size of the \(i\)-th user. Denote \(N\) as the total number of samples, then \(N=_{i=1}^{n}m_{i}\). We calculate the user-wise mean first, i.e. \(_{i}()=(1/m_{i})_{ D_{i}}\). The new proposed estimator (before adding noise) is

\[_{0}()=*{arg\,min}_{}_{i=1}^{ n}w_{i}_{i}(,_{i}()),\] (4)

in which \(w_{i}\) is the normalized weight of user \(i\), i.e. \(_{i=1}^{n}w_{i}=1\). If users are balanced, then \(w_{i}\) are the same for all \(i\). \(_{i}\) is the Huber loss function:

\[_{i}(,)=\{\| -\|^{2}&&\|- \| T_{i}\\ T_{i}\|-\|-T_{i}^{2}&&\| -\|>T_{i}..\] (5)

\(T_{i}\) is the connecting point between quadratic and linear parts of Huber loss. For balanced users, \(w_{i}\) and \(T_{i}\) are the same for all users. For imbalanced users, \(w_{i}\) and \(T_{i}\) are set differently depending on the per-user sample sizes. The general guideline is that \(w_{i}\) increases with \(m_{i}\), while \(T_{i}\) decreases with \(m_{i}\). The final output needs to satisfy user-level \((,)\)-DP requirement. Hence, we set

\[()=(_{0}(),R_{c})+ ,\] (6)

in which \((,R_{c})=/\|\|)}\) is the function that clips the result into \(B_{d}(,R_{c})\). The clipping operation is used to control the worst case sensitivity. \(\) denotes the noise added to the estimated value. In this work, we use Gaussian noise \((0,^{2})\). The clipping radius \(R_{c}\) is determined by the knowledge of the range of \(\). Given a prior knowledge \(\|\| R\), then we can set \(R_{c}=R\). Actually, similar to , our analysis shows that \(R_{c}\) can grow exponentially with \(n\) without significantly compromising the accuracy. The noise parameter \(^{2}\) needs to be determined carefully through a detailed sensitivity analysis.

Now we comment on the implementation. As has been discussed in , minimizing multi-dimensional Huber loss can be implemented by a modification of an iterative Weiszfeld's algorithm [81; 82]. The overall worst-case time complexity is \(O(nd/)\), in which \(\) is the desired precision. Moreover, for bounded support, with high probability, the algorithm requires only one iteration with time complexity \(O(nd)\). Details can be found in Appendix A.

## 5 Analysis: Balanced Users

In this section, to gain some insights, we focus on the relatively simpler case and assume that all users have the same number of items, i.e. \(m_{i}\) are equal for all \(i\). Therefore, throughout this section, we omit the subscript and use \(m\) to denote the local sample size. Now \(w_{i}=1/n\) for all \(i\). \(T_{i}\) are also the same for all users, denoted as \(T\) in this section. Let \(\) be a random vector, whose statistical mean \(:=[]\) is unknown. Given a dataset \(=\{D_{1},,D_{n}\}\), the goal is to estimate \(\), whilesatisfying user-level \((,)\)-DP. We present the sensitivity analysis for the general dataset \(\) first, and then analyze the estimation error for a randomly generated dataset. To begin with, define

\[Z()=_{i[n]}\|_{i}()- }()\|,\] (7)

in which \(}()=(1/n)_{i=1}^{n}_{i}()\) is the overall average. A small \(Z()\) indicates that the user-wise means \(_{i}\) are concentrated within a small region. If \(Z()\) is large, then there are some outliers. The sensitivity is bounded separately depending on whether the user-wise means are well concentrated. Throughout this section, we use \(LS()\) to denote the local sensitivity of \((_{0}(),R_{c})\), in which the subscript \(f\) in (2) is omitted.

_1) No outliers._ We first bound the local sensitivity for the case with \(Z()<(1-2/n)T\), in which \(T\) represents \(T_{i}\) in (5) for all \(i\). It requires that all \(_{i}\)'s are not far away from their average.

**Lemma 1**.: _If \(Z()<(1-2/n)T\), then_

\[LS())}{n-1}.\] (8)

The proof of Lemma 1 is shown in Appendix B. Here we provide some intuition. From the definition (2), local sensitivity is the maximum change of \(_{0}()\) after replacing some \(_{i}\) with \(_{i}^{}\). To achieve such maximum change, the optimal choice is to move \(_{i}\) sufficiently far away in the direction of \(_{0}()-_{i}\). The impact of \(_{i}\) on \(_{0}()\) is roughly \(Z()/(n-1)\), while the impact of \(_{i}^{}\) is roughly \(T/(n-1)\). Since \(_{i}\) and \(_{i}^{}\) are at opposite direction with respect to \(_{0}()\), the overall effect caused by replacing \(_{i}\) with \(_{i}^{}\) is upper bounded by \((T+Z())/(n-1)\).

_2) A few outliers._ Now we consider a more complex case: \(Z()\) is large, and the dataset is not well concentrated, but the number of outliers is not too large. Formally, assume that there exists another dataset \(^{*}\) whose Hamming distance to \(\) is bounded by \(k\), and \(^{*}\) is well concentrated. Then we have the following lemma to bound the local sensitivity.

**Lemma 2**.: _For a dataset \(\), if there exists a dataset \(^{*}\) such that \(d_{H}(,^{*}) k\), in which \(d_{H}\) is the Hamming distance (see Definition 3), and \(Z(^{*})<(1-2(k+1)/n)\,T\), then \(LS() 2T/(n-k)\)._

The proof of Lemma 2 is shown in Appendix C. The intuition is that since there exists a well concentrated dataset \(^{*}\) with \(d_{H}(,^{*}) k\), \(\) contains no more than \(k\) outliers. At least \(n-k\) other user-wise mean values fall in a small region. To achieve the maximum change of \(_{0}()\), the optimal choice is to replace an outlier \(_{i}\) with \(_{i}^{}\), such that \(_{i}-_{0}()\) and \(_{i}^{}-_{0}()\) have opposite directions. Each of them has an effect of roughly \(T/(n-k)\) on \(_{0}()\), thus the overall change is \(2T/(n-k)\).

_3) Other cases._ For all other cases, since \(\|(_{0},R_{c})\| R_{c}\) always hold, the local sensitivity can be bounded by \(LS() 2R_{c}\).

From the analysis above, we now construct a valid smooth sensitivity. Define

\[()=\{k|^{*},d_{H}(, ^{*}) k,Z(^{*})<T\}.\] (9)

\(()\) can be viewed as the number of outliers. From (9), if \(\) is well concentrated, with \(Z()<T/2\), then \(()=0\). Now we define \(G(,k)\) as follows.

**Definition 5**.: _(a) If \(Z()<(1-2/n)T\), \(k=0\), then \(G(,0)=(T+Z())/(n-1)\);_

_(b) If conditions in (a) are not satisfied, and \(()\) exists, if \(k n/4-1-()\),_

\[G(,k)=)};\] (10)

_(c) If conditions in (a) and (b) are not satisfied, then \(G(,k)=2R_{c}\)._

Based on Definition 5, the smooth sensitivity is given by

\[S()=_{k}e^{- k}G(,k),\] (11)

in which \(\) is determined in (3). Then we show that \(S()\) is a valid smooth sensitivity, and the privacy requirement is satisfied.

**Theorem 1**.: _With \(=S()/\), in which \(\) is determined in (3), \((0,^{2})\), the estimator \(\) defined in (6) is \((,)\)-DP._

To prove Theorem 1, we need to show that \(S\) is a valid smooth sensitivity, i.e. two conditions in Definition 4 are satisfied. The detailed proof is provided in Appendix D.

In the analysis above, all results are derived for a general dataset \(\). In the remainder of this section, we analyze the performance of estimator (6) for randomly generated samples.

### Bounded Support

Let \(\) be a random vector generated from distribution \(P\) with an unknown statistical mean \(:=[]\). We make the following assumption:

**Assumption 1**.: \(\) _is supported on \(B_{d}(0,R)=\{|\|\| R\}^{d}\)._

The mean squared error is analyzed in the following theorem.

**Theorem 2**.: _Let \(R_{c}=R\), and \(T=C_{T}R(mn^{3}(d+1))/\) with \(C_{T}>16\). If \(n>(4/)(nR_{c}/T)\), then_

\[[\|(D)-\|^{2}] }{mn}+}{mn^{2}^{2}}(mnd).\] (12)

The proof of Theorem 2 is shown in Appendix E. With the selection rule of \(T\) in Theorem 2, it is shown that with high probability, \(()=0\), indicating that \(\) is expected to be well concentrated around the population mean \(\). The smooth sensitivity \(S()\) can then be bounded. The first term in the right hand side of (12) is the non-private estimation error, i.e. the error of \(_{0}()\), while the second term is the error caused by noise \(\). The condition \(n>(4/)(nR_{c}/T)\) is necessary, since it ensures that \(G(,k)=2R_{c}\) (Definition 5 (c)) occurs only for sufficiently large \(k\), thus \(e^{- k}\) is small, and does not affect the calculation of \(S()\) in (11). A lower bound on the number of users \(n\) has also been imposed for the two-stage method .

For the simplest case with bounded support and balanced users, the two-stage approach in  is already nearly optimal (Corollary 1 in ). Therefore, improvement in polynomial factors is impossible. Nevertheless, we still improve on the logarithm factor. The main purpose of Theorem 2 is to show that our improvement on heavy-tailed distributions and imbalanced users is not at the cost of hurting the performance under the simplest case with bounded distributions and balanced users.

### Unbounded Support

Now we analyze the heavy-tailed case. Instead of requiring \( B_{d}(0,R)\), we now assume that \(\) has \(p\)-th bounded moment.

**Assumption 2**.: _Suppose that \( B(,R)\), and the \(p\)-th (\(p 2\)) moment of \(-\) is bounded, i.e. \([\|\|^{p}] M_{p}\)._

In Assumption 2, higher \(p\) indicates a lighter tail and vice versa. We then show the convergence rate of mean squared error in Theorem 3.

**Theorem 3**.: _Let \(R_{c}=R\), and_

\[T=C_{T}\{},2(3m)^{-1 }^{-}\},\] (13)

_with \(=/(n)\) and \(C_{T}>8M_{p}^{}\). If \(n>8(1+(1/)(n/2T))\), then under Assumption 2,_

\[[\|(D)-\|^{2}]+[^{2}}+(n^{2}^ {2}})^{1-}^{2}(nd)].\] (14)

The proof of Theorem 3 is shown in Appendix F. Here we provide an intuitive understanding. From the central limit theorem, each user-wise mean \(_{i}()\) is the average of \(m\) i.i.d variables, thus it hasa Gaussian tail around the population \(\). However, since \(\) is only required to have \(p\)-th bounded moment, the tail probability away from \(\) is still polynomial. The formal statement of the tail bound is shown in Lemma 13 in the appendix. Then the threshold \(T\) is designed based on the high probability upper bound of \(Z()\) to ensure that with high probability, \(()\) is small. Regarding the result, we have the following remarks.

**Remark 1**.: _Here we comment on small and large \(m\) limits. If \(m=1\), the right hand side of (14) becomes \(O(1/n+(d/n^{2}^{2})^{1-1/p})\), which matches existing analysis on item-level DP for heavy-tailed random variables . For the opposite limit, with \(m^{1-2/p} n^{2/p}(nd)\), then the convergence rate is the same as the case with bounded support, indicating that the tail of sample distribution does not affect the error more than a constant factor._

**Remark 2**.: _Now we compare (14) with the two-stage approach. Following the analysis in , it can be shown that the bound of mean squared error in  is \(((d/(n^{2}^{2}))(1/m+m^{4/p-2}n^{6/p}))\) (we refer to Appendix G for details). Therefore, we have achieved an improved rate in (14)._

The theoretical results in this section are summarized as follows. If the support is bounded, our method has the same convergence rate as the existing method. For heavy-tailed distributions, our approach significantly reduces the error, since our method avoids the clipping process. In federated learning applications, it is common for gradients to have heavy-tailed distributions , thus our method has the potential of improving the performance of federated learning under DP requirements. Apart from heavy-tailed distributions, another common characteristic in reality is that users are usually imbalanced. We analyze it in the next section.

## 6 Analysis: Imbalanced Users

Now we analyze the general case where \(m_{i}\), \(i=1,,n\) are different. Recall that for balanced users, we have defined \(Z()\) in (7) that finds the maximum distance from \(_{i}()\) to their average \(}()\). For imbalanced users, instead of taking the maximum, we define \(Z_{i}\) separately for each \(i\):

\[Z_{i}()=\|}()-_{i}( )\|,\] (15)

in which \(}()=_{i=1}^{n}w_{i}_{i}()\) is the average of samples all over the dataset.

From now on, without loss of generality, suppose that users are arranged in ascending order of \(m_{i}\), i.e. \(m_{1} m_{n}\). Define

\[h(,k)=^{n}w_{i}(T_{i}+Z_{i}())}{ _{i=1}^{n-k}w_{i}}.\] (16)

Similar to the case with balanced users, we analyze the sensitivity for datasets with no outliers, a few outliers, and other cases separately.

_1) No outliers._ We show the following lemma.

**Lemma 3**.: _If \(h(,1)_{i}(T_{i}-Z_{i}())\), then \(LS() h(,1)\)._

The general idea of the proof is similar to Lemma 1. However, the details become more complex since now the samples are unbalanced. The detailed proof is shown in Appendix H.

_2) A few outliers._ Similar to Lemma 2, we find a neighboring dataset \(^{*}\) that is well concentrated and then bounds the local sensitivity. The formal statement is shown in the following lemma.

**Lemma 4**.: _For a dataset \(\), if there exists another dataset \(^{*}\) such that \(h(^{*},k+1)<_{i}(T_{i}-Z_{i}(^{*}))\), then \(LS() 2_{i}(w_{i}T_{i})/_{i=1}^{n-k-1}w_{i}\)._

The proof of Lemma 4 is shown in Appendix I, which just follows the proof of Lemma 2.

_3) Other cases._ Finally, for all cases not satisfying the conditions in Lemma 3 and 4, we can just bound the local sensitivity with \(2R_{c}\), i.e. \(LS() 2R_{c}\).

Similar to the case with balanced users, now we define

\[()=\{k|^{*},d_{H}(,^{*})=k,h(^{*},k_{0})<_{i}(T_{i}-Z_{i}())\},\] (17)

in which \(k_{0}\) is any integer, and can be viewed as a design parameter. Correspondingly, the smooth sensitivity \(G(,k)\) is defined as follows.

**Definition 6**.: _(a) If \(h(,1)_{i}(T_{i}-Z_{i}())\), then \(G(,0)=h(,1)\); (b) If the conditions in (a) are not satisfied, and \(()\) exists, then for all \(k k_{0}-()-1\),_

\[G(,k)=w_{i}T_{i}}{ _{i=1}^{n-()-k-1}w_{i}};\] (18)

_(c) If the conditions in both (a) and (b) are not satisfied, then \(G(,k)=2R_{c}\)._

We still use the same settings of \(\) and \(\) as in the case with balanced samples. With smooth sensitivity calculated using (11), the privacy requirement is satisfied:

**Theorem 4**.: _Let \((0,(S()^{2}/^{2}))\), in which \(S()=e^{- k}G(,k)\), then the estimator \(\) is \((,)\)-DP._

Now we analyze the convergence of the algorithm. We begin with Assumption 3. Intuitively, this assumption requires that the users can not be too unbalanced. At least half samples belong to users whose sample sizes are not very large.

**Assumption 3**.: _Suppose there exists a constant \( 1\). Let \(k_{c}=\{i|m_{i}> N/n\}\), then \(_{i=k_{c}}^{n}m_{i} N/2\)._

In Assumption 3, \(\) can be viewed as the degree of imbalance. For a better explanation, we provide the following examples:

* If users are balanced, then \(=1\);
* If the \(i\)-th user has \(ki\) samples (which means that the number of items belonging to each user is linear in its order), then for large \(n\), \(\) is approximately \(\).

In general, \(\) is large if users are highly imbalanced. Under Assumption 3, the convergence of mean squared error is shown in Theorem 5.

**Theorem 5**.: _Let the weights of users in (4) be \(w_{i}=m_{i} m_{c}/(_{j=1}^{n}m_{j} m_{c})\), in which \(m_{c}= N/n\). Moreover, let_

\[T_{i}=C_{T}(Nn^{2}(d+1))}{m_{i} m_{c}}},\] (19)

_in which \(C_{T}>16\). In (17), \(k_{0}= n/8\). With the parameters above, if \(n>8(1+(1/2)(Nn))\), then under Assumption 1 and 3,_

\[[\|()-\|^{2}] }{N}+}{Nn^{2}}^{2}(Nnd).\] (20)

The proof of Theorem 5 is shown in Appendix J. If users are balanced, then \(=1\), with \(N=nm\), (20) reduces to (12). From (20) and Assumption 3 it can be observed that our method is much less sensitive to a single large \(m_{i}\). As long as \(m_{i}\) are not very large for most users, the convergence rate of mean squared error is not affected. There are two intuitive reasons. Firstly, \(w_{i}\) is upper bounded by \(m_{c}/(_{j=1}^{n}m_{j} m_{c})\), thus the worst-case sensitivity is controlled. Secondly, \(T_{i}\) are set adaptively to achieve a good tradeoff between sensitivity and bias. With larger \(m_{i}\), smaller \(T_{i}\) is used, and vice versa. We refer to Appendix G for comparison with the two-stage approach.

## 7 Numerical Examples

In this section, we show numerical experiments. We compare the performance of our new Huber loss minimization approach (denoted as HLM) versus the two-stage approach proposed in , called Winsorized Mean Estimator (denoted as WME).

### Balanced Users

Here all users have the same sizes of local datasets. In the following experiments, we fix \(=1\) and \(=10^{-5}\). For a fair comparison, the parameter \(T\) for our method as well as \(\) in  are both tuned optimally for each case. Figure 1 shows the curve of mean squared error. Let the number of users \(n\) be either \(1,000\) and \(10,000\). In each curve, \(n\) is fixed, while the number of samples per user \(m\) varies from \(1\) to \(1,000\). The results are plotted in logarithm scale. Figure 1(a)-(c) show the results of uniform distribution in \([-1,1]\), Gaussian distribution \((0,1)\), and the Lomax distribution, whose pdf is \(f(x)=a/(1+x)^{a+1}\) (we use \(a=4\) in Figure 1(c)). Figure 1 (d)-(f) shows the corresponding experiments with dimensionality \(d=3\). Finally, Figure 1 (g) and (h) show the results using the IPUMS dataset  for total income and salary, respectively, which are typical examples of data following heavy-tailed distributions.

Figure 1 (a) and (b) show that for one-dimensional uniform and Gaussian distribution, the Huber loss minimization approach has nearly the same performance as the two-stage method. Our explanation is that uniform and Gaussian distributions are symmetric, with no tails or light tails, thus the clipping operation does not introduce additional bias. However, for heavy-tailed and skewed distribution, such as Lomax distribution, our new method has a significantly faster convergence rate than the two-stage method. These results agree with the theoretical analysis, which shows that our method reduces the clipping bias. With higher dimensionality, Figure 1(d)-(f) show that the advantage of the practical performance of our method becomes more obvious.

### Imbalanced Users

Now we show the performance with unbalanced users. For some \( 1\), let \(s_{i}= N(i/n)^{}\) for \(i=0,,n\), and \(m_{i}=s_{i}-s_{i-1}\) for \(i=1,,n\). It can be shown that Assumption 3 is satisfied with \(\). Therefore, according to the analysis in Section 6, we let \(w_{i}=m_{i} m_{c}/_{j}m_{j} m_{c}\), with \(m_{c}= N/n\), and \(T_{i}=A/ m_{c}}\), in which \(A\) is tuned optimally for each case. The selection of \(T_{i}\) may be slightly different from (19) in Theorem 5. In Theorem 5, \(T_{i}\) is selected to minimize the theoretical upper bound. To ensure that the analysis is mathematically rigorous, the upper bound of estimation error is larger than the truth. Therefore, the optimal value of \(T_{i}\) in practice is slightly different from that derived in theories. Note that such parameter tuning does not require additional privacy budget since in each experiment, \(T_{i}\) are hyperparameters that is fixed before knowing the value of each sample. They are not determined adaptively based on the data. Figure 2 shows the growth curve of mean squared error with respect to \(\).

From Figure 2, it can be observed that with the increase of \(\), the two-stage method degrades, while the Huber loss minimization approach performs significantly more stable.

Figure 1: Convergence of mean squared error with balanced users.

## 8 Conclusion

In this paper, we have proposed a new approach to mean estimation under user-level DP based on Huber loss minimization. The sensitivity is bounded for all possible datasets. Based on the sensitivity analysis, we use the smooth sensitivity framework to determine the noise added to the result. We have also derived the bound on the mean squared error for various cases. The result shows that our method reduces the error for heavy-tailed distributions, and is more suitable to imbalanced users. It is promising to extend our approach to more learning problems, such as calculating average gradients in federated learning.

**Limitations:** The limitations of our work include: (1) There are some requirements on the minimum number of users \(n\). while entirely removing this condition is impossible, to make our method more practical, we expect that it can be somewhat weakened. (2) The case with local sample sizes \(m_{i}\) also being private has not been analyzed. We will leave these two points as future works.