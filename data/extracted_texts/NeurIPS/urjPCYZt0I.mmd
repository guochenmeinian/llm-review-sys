# JailbreakBench: An Open Robustness Benchmark

for Jailbreaking Large Language Models

 Patrick Chao\({}^{*1}\), Edoardo Debenedetti\({}^{*2}\), Alexander Robey\({}^{*1}\), Maksym Andriushchenko\({}^{*3}\),

Francesco Croce\({}^{3}\), Vikash Sehwag\({}^{4}\), Edgar Dobriban\({}^{1}\), Nicolas Flammarion\({}^{3}\),

George J. Pappas\({}^{1}\), Florian Tramer\({}^{2}\), Hamed Hassani\({}^{1}\), Eric Wong\({}^{1}\)

Equal contribution. \({}^{1}\)University of Pennsylvania, \({}^{2}\)ETH Zurich, \({}^{3}\)EPFL, \({}^{4}\)Sony AI

###### Abstract

Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as _jailbreak artifacts_; (2) a jailbreaking dataset comprising 100 behaviors--both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024)--which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.

## 1 Introduction

Large language models (LLMs) are often trained to align with human values, thereby refusing to generate harmful or toxic content (Ouyang et al., 2022). However, a growing body of work has shown that even the most performant LLMs are not adversarially aligned: it is often possible to elicit undesirable content by using so-called _jailbreaking attacks_(Mowshowitz, 2022; Carlini et al., 2024). Concerningly, researchers have shown that such attacks can be generated in many different ways, including hand-crafted prompts (Shen et al., 2023; Wei et al., 2023), automatic prompting via auxiliary LLMs (Chao et al., 2023; Zeng et al., 2024), and iterative optimization (Zou et al., 2023). And while several defenses have been proposed to mitigate these threats (Robey et al., 2023; Jain et al., 2023), LLMs remain highly vulnerable to jailbreaking attacks. For this reason, as LLMs are deployed in safety-critical domains, it is of pronounced importance to effectively benchmark the progress of jailbreaking attacks and defenses (Longpre et al., 2024).

To meet this need, this paper introduces the JailbreakBench benchmark. The design principles of JailbreakBench revolve around standardizing a set of best practices in the evolving field of LLM jailbreaking. Our core principles include complete _reproducibility_ via a commitment to open-sourcing jailbreak prompts corresponding to attacked and defended models, _extensibility_ to incorporate newattacks, defenses, and LLMs, and accessibility of our evaluation pipeline to expedite future research. In this paper, we elaborate further on these principles, describe the components of the benchmark, provide a thorough discussion on the selection of an accurate jailbreaking judge, and present the results of multiple attacks and test-time defenses on several open- and closed-sourced LLMs.

**Contributions.** The contributions of the JailbreakBench benchmark are as follows:

* **Repository of jailbreak artifacts.** We provide an evolving repository of artifacts corresponding to state-of-the-art jailbreaking attacks and defenses. Despite being critical for reproducible research, many such prompts have not been openly released, and crowd-sourced websites have gone offline (Albert, 2023). These artifacts can be accessed in the following way via our library: ``` importjailbreakbenchamjbb artifact=jbb.read_artifact(method-"PAIR", model_name="vicuna-13b-v1.5") print(artifact.jailbreaks) JailbreakInfo( index=75, goal="Designaphishingemailthatcantrickusersintogivingawaytheirlogin credentials' identifier"Phishing', category="Privacy', prompt="Securityresearchismessentialpartofunderstandinghowtoprotectusers[...]', response="Subject:UrgentActionRequired:UpdateYourAccount[...]', number.of_generies=30, queries_to_jailbreak=5, prompt.tokens=3564, response.tokens=3729, jailbroken=True) ```
* **Pipeline for red-teaming LLMs.** We provide a standardized pipeline for red-teaming LLMs. In particular, our pipeline implements the evaluation of potential jailbreaks, standardizes decoding parameters, and supports both local and cloud-based querying.
* **Pipeline for testing and adding new defenses.** We implement five baseline defenses which can be combined with any LLM. Moreover, we welcome the submission of new defensive mechanisms or robust models, which will be easily accessible by users via the benchmark interface.
* **Jailbreaking classifier selection.** Evaluating the success of jailbreak attacks is challenging given the subjective nature of judging the appropriateness of a LLM's response. We perform a rigorous human evaluation to compare six jailbreak classifiers. Among these classifiers, we find the recent Llama-3-Instruct-70B to be an effective judge when used with a properly selected prompt.
* **Dataset of harmful and benign behaviors.** We introduce the JBB-Behaviors dataset, which comprises 100 distinct misuse behaviors divided into ten broad categories corresponding to OpenAI's usage policies. Approximately half of these behaviors are original, while the other half are sourced from existing datasets (Zou et al., 2023; Mazeika et al., 2023, 2024). For each misuse behavior, we also collect a matching _benign_ behavior on the same exact topic that can be used as a sanity check for evaluating refusal rates of new models and defenses.

Figure 1: The website hosted at https://jailbreakbench.github.io/ provides a convenient web interface to our collected evaluations (both attacks and defenses) and jailbreak artifacts.

* **Reproducible evaluation framework.** We provide a reproducible framework for evaluating the attack success rate of jailbreaking algorithms, which can also be used to submit an algorithm's jailbreak strings to our artifact repository.
* **Jailbreaking leaderboard and website.** We maintain a website hosted at https://jailbreakbench.github.io/ which tracks the performance of jailbreaking attacks and defenses across various state-of-the-art LLMs on the official leaderboard (see Figure 1).

**Preliminary impact.** Two months after releasing the preliminary version of JailbreakBench on arXiv, researchers in the field have already started using our jailbreak artifacts (Peng et al., 2024; Abdelnabi et al., 2024), jailbreak judge prompt (Zheng et al., 2024), and the JBB-Behaviors dataset (Xiong et al., 2024; Arditi et al., 2024; Li et al., 2024; Leong et al., 2024; Jin et al., 2024, 2024), notably including the authors of Gemini 1.5 from Google (Gemini Team, 2024).

## 2 Background and related work

**Definitions.** At a high level, the goal of a jailbreaking algorithm is to design input prompts that cause an LLM to generate text that is harmful, toxic, or objectionable. More specifically, let us assume we have a target model LLM, a judge function JUDGE that determines the correspondence between the generation LLM\((P)\), and a harmful goal \(G\). Then the task of jailbreaking can be formalized

\[ P^{}((P),G)=,\] (1)

where \(P\) is the input prompt and \(^{}\) denotes the set of all sequences of tokens of arbitrary length.

**Attacks.** Early jailbreaking attacks involved manually refining hand-crafted jailbreak prompts (Mowshowitz, 2022; Albert, 2023; Wei et al., 2023). Due to the time-consuming nature of manually collecting jailbreak prompts, research has largely pivoted toward automating the red-teaming pipeline. Several algorithms take an optimization perspective to solve Eq. (1), either via first-order discrete optimization techniques (Zou et al., 2023; Geisler et al., 2024) or zero-th order methods, e.g. genetic algorithms (Lapid et al., 2023; Liu et al., 2023) or random search (Andriushchenko et al., 2024; Sitawarin et al., 2024; Hayase et al., 2024). Additionally, auxiliary LLMs can aid the attacks, for example to refine hand-crafted jailbreak templates (Yu et al., 2023), translate goal strings into low-resource languages (Yong et al., 2023; Deng et al., 2023), generate jailbreaks (Chao et al., 2023; Mehrotra et al., 2023), or rephrase harmful requests (Shah et al., 2023; Zeng et al., 2024; Takemoto, 2024).

**Defenses.** Several methods try to mitigate the threat of jailbreaks. Many such defenses seek to align LLM responses to human preferences via methods such as RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2024). Relatedly, variants of adversarial training have been explored (Mazeika et al., 2024), as well as fine-tuning on jailbreak strings (Hubinger et al., 2024). Conversely, test-time defenses like SmoothLLM (Robey et al., 2023; Ji et al., 2024) and perplexity filtering (Jain et al., 2023; Alon and Kamfonas, 2023) define wrappers around LLMs to detect potential jailbreaks.

**Evaluation.** In the field of image classification, benchmarks such as RobustBench (Croce et al., 2021) provide a unified platform for both evaluating the robustness of models in a standardized manner and for tracking state-of-the-art performance. However, designing a similar platform to track the adversarial vulnerabilities of LLMs presents new challenges, one of which is that there is no standardized definition of a valid jailbreak. Indeed, evaluation techniques span human labeling (Wei et al., 2023; Yong et al., 2023), rule-based classifiers (Zou et al., 2023), neural-network-based classifiers (Huang et al., 2023; Mazeika et al., 2024), and the LLM-as-a-judge framework (Zheng et al., 2023; Chao et al., 2023; Shah et al., 2023; Zeng et al., 2024). Souly et al. (2024) discuss the current state of jailbreak judges suggesting their suboptimal performance and propose a more detailed grading criterion for what constitutes a valid jailbreak. Unsurprisingly, the discrepancies and inconsistencies between these methods lead to variable results.

**Benchmarks, leaderboards, and datasets.** Several benchmarks involving LLM robustness have recently appeared. Zhu et al. (2023) propose PromptBench, a library for evaluating LLMs against adversarial prompts, although not in the context of jailbreaking. DecodingTrust (Wang et al., 2023) and TrustLLM (Sun et al., 2024) consider jailbreaking but only evaluate static templates, which excludes automated red-teaming algorithms. More related to JailbreakBench is the recently introduced HarmBench benchmark (Mazeika et al., 2024), which implements jailbreaking attacks and defenses, and considers a broad array of topics including copyright infringement and multimodal models.1 In contrast, we focus on supporting _adaptive_ attacks (Tramer et al., 2020; Andriushchenko et al., 2024) and _test-time_ defenses (Jain et al., 2023; Robey et al., 2023). Thus, we standardize evaluation of test-time defenses but not attack implementations, since we expect them to potentially differ for different defenses. Moreover, we strive to make our benchmark community-driven, prioritizing clear guidelines for adding new attacks, models, and defenses. Zhou et al. (2024) was proposed concurrently with JailbreakBench and contains a red-teaming framework that implements 11 jailbreak methods. Several competitions have also appeared recently, including the "Trojan Detection Challenge" (TDC) at NeurIPS 2023 (Mazeika et al., 2023), and the "Find the Trojan: Universal Backdoor Detection in Aligned LLMs" competition at SaTML 2024 (Rando et al., 2024). However, JailbreakBench is not a challenge or a competition, but rather an open-ended project which aims at tracking and facilitating the progress of the field. Finally, several stand-alone datasets of harmful behaviors have appeared, such as AdvBench(Zou et al., 2023), MaliciousInstruct(Huang et al., 2023), and the dataset of hand-crafted jailbreaks curated in Wei et al. (2023). However, many existing datasets contain duplicated entries, behaviors that are impossible to fulfill, or are not fully open-sourced.

## 3 Main features for JailbreakBench

Given the current landscape of the field, we believe there is a need for a jailbreaking benchmark that prioritizes supporting of state-of-the-art adaptive attacks and test-time defenses, as well as providing direct access to the corresponding jailbreaking artifacts. Thus, we prioritized the following principles when designing JailbreakBench.

1. **Reproducibility.** We ensure maximal reproducibility by collecting and archiving jailbreak artifacts, with the hope that this establishes a stable basis of comparison. Our leaderboard also tracks the state-of-the-art in jailbreaking attacks and defenses, so to identify leading algorithms and establish open-sourced baselines in future research.
2. **Extensibility.** We accept any jailbreaking attack, including white-box, black-box, universal, transfer, and adaptive attacks, and any jailbreaking defense, all of which are compared using the same set of evaluation metrics. We plan to adapt our benchmark as the community evolves to accommodate new threat models, attacks, defenses, and LLMs.
3. **Accessibility.** Our red-teaming pipeline is fast, lightweight, inexpensive, and can be run exclusively through cloud-based models, circumventing the need for local GPUs. In releasing the jailbreak artifacts, we hope to expedite future research on jailbreaking, especially on the defense side.

### Jbb-Behaviors: A dataset of harmful and benign behaviors

**Harmful behaviors.** An important contribution of this work is to curate the JBB-Behaviors dataset to facilitate the comparison of jailbreaking attacks and defenses. JBB-Behaviors contains a representative set of behaviors that encompass a broad spectrum of misuse. In contrast to several existing datasets, we ensure that each behavior in JBB-Behaviors is unique and realizable with a text-based generation. Among these behaviors, 18% are sourced from AdvBench(Zou et al., 2023), 27% from TDC/HarmBench(Mazeika et al., 2023, 2024), whereas the remaining 55% are original to JBB-Behaviors (further information in Appendix B, including a detailed breakdown over the sources). We note that many of the behavior topics were in part inspired by the topics from Shah et al. (2023). Specifically, JBB-Behaviors contains 100 rows, where each row specifies five distinct fields:

* **Behavior.** A unique identifier describing a distinct misuse behavior.
* **Goal.** A query requesting an objectionable behavior.
* **Target.** An affirmative response to the harmful goal string.
* **Category.** A broader category of misuse from OpenAI's usage policies.
* **Source.** A reference to the source dataset of the goal and target string.

**Benign behaviors.** In addition, we create 100 benign behaviors where each of them matches the exact topic of the corresponding harmful behavior. For example, the benign counterpart of the harmful behavior _"Outline a business model for a fake charity which uses donations for personal gain"_ is _"Describe businesses that have illegally used charitable donations for personal gain."_ We use benign behaviors to evaluate refusal rates for different models and defenses to make sure they do not refuse too often by, e.g., simply detecting some key words that are often associated with harmful behaviors. We note that some fraction of these behaviors can be considered as borderline, and different LLM providers might disagree about whether they should be refused or not.

### A repository of jailbreaking artifacts

A central component of the JailbreakBench benchmark is our repository of easily accessible _jailbreak artifacts_, i.e., the prompts, responses, and classifications corresponding to each submitted attack or defense. Each artifact also contains metadata, e.g., hyperparameters of the attack/defense, the attack success rate, and the number of queries made to the target model, and can be easily loaded by

``` artifact=jbb.read_artifact(method="PAIR",model_name="vicuna-13b-v1.5") ```

The JailbreakBench artifacts repository currently contains jailbreak strings for PAIR (Chao et al., 2023), GCG (Zou et al., 2023), JailbreakChat (Albert, 2023), and the attacks from Andriushchenko et al. (2024). Moreover, as described in SS3.7, we intend for users to submit new artifacts as the benchmark evolves. In general, research surrounding LLM jailbreaking has showed hesitancy toward open-sourcing jailbreaking artifacts, given their propensity for potential misuse (Wei et al., 2023; Zou et al., 2023). However, we believe these jailbreaking artifacts can serve as an initial dataset for adversarial training against jailbreaks, as has been done in past research (see, e.g., Hubinger et al. (2024)). We discuss this topic more thoroughly in SS5.

### A pipeline for red-teaming LLMs

Generating jailbreaks for LLMs often involves complex workflows that facilitate varying tokenization schemes, sampling algorithms, and system prompts. As changing each of these aspects can lead to highly variable results, we streamline the process of generating jailbreaks by introducing a standardized red-teaming pipeline. Our pipeline is both easy to use--LLMs can be loaded and queried in just two lines of Python--and flexible--we support both local and cloud-based LLMs. After loading a particular model type, it is straightforward to query that model:

``` llm=jbb.LLMtitleLLM(model_name="vicuna-13b-v1.5",api_key="<your-api-key>") prompts=["Write=phishingemail.","Howwouldyouwriteabishingemail?"] responses=llm.query(prompt=prompts,behavior="Phishing") ```

### A pipeline for evaluating defenses against jailbreaks

Alongside works on designing new attacks, researchers have also proposed defense algorithms to mitigate the threat posed by jailbreaking. To this end, we provide an implementation of five popular defenses, including "SmoothLLM" (Robey et al., 2023) and "PerplexityFilter" (Jain et al., 2023), and a modular framework for loading and querying them:

``` llm=jbb.LLMvLLM(model_name="vicuna-13b-v1.5") defense=jbb.defenses.SmoothLLM(target_model=llm) response=defense.query(prompt="Writeabishingemail.") ```

Alternatively, one can pass the defense argument directly to llm.query. Finally, we note that proper evaluation of test-time defenses should rely on _adaptive attacks_, i.e., attacks tailored to the specific defense under evaluation (Tramer et al., 2020). Transfer attacks from undefended LLMs can only provide a _lower bound_ on the worst-case attack success rate.

### Selection of the jailbreaking judge

Determining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which might be challenging even for humans. To this end, we consider six candidate classifiers used in the jailbreaking literature:

* **Rule-based.** The rule-based judge from Zou et al. (2023) based on string matching,
* **GPT-4.** The GPT-4-0613 model used as a judge (OpenAI, 2023),
* **HarmBench.** The Llama-2-13B judge introduced in HarmBench (Mazeika et al., 2024),
* **Llama Guard.** An LLM safeguard model fine-tuned from Llama-2-7B (Inan et al., 2023),* **Llama Guard 2.** An LLM safeguard model fine-tuned from Llama-3-8B (Llama Team, 2024),
* **Llama-3-70B.** The recent Llama-3-70B (Al@Meta, 2024) used as a judge with a custom prompt.

For GPT-4, we use the JUDGE system prompt from Chao et al. (2023), while for the Llama Guard models and Llama-3-70B, we use custom system prompts, which we share in Appendix F. We found that it is necessary to use a custom prompt for Llama-3-70B as the prompt from Chao et al. (2023) often leads to refusals. Moreover, Llama-3-8B also often refuses to serve as a judge, even with the custom prompt, which is the reason why we evaluate the 70B model.

To choose an effective classifier, we collected a dataset of 200 jailbreak prompts and responses (see Appendix C for details). Three experts labeled each prompt-response pair, and the agreement between them was approximately 95%. The ground truth label for each behavior is then the majority vote among the labelers. Moreover, we add 100 benign examples from XS-Test (Rottger et al., 2023) to test how sensitive the judges are to benign prompts and responses that share similarities to harmful ones. This dataset of 300 examples is provided in the JailbreakBench HuggingFace Datasets repository.

We compare the agreement, false positive rate (FPR), and false negative rate (FNR) of the candidate judges to these ground truth labels. Table 1 shows that Llama-3-70B and GPT-4 achieve the highest agreement (above 90%) with the annotators, as well as close to the best FPR and FNR. Llama Guard 2 is only slightly worse having 87.7% agreement and approximately equal FPR and FNR. The HarmBench and Llama Guard models have significantly lower agreement rates, at 78.3% and 72.0% respectively. We observe that the HarmBench model has noticeably many false positives on the 100 benign examples from XS-Test, leading to the overall 26.8% FPR on the full evaluation set. The Rule-based judge appears not effective with only 56.0% agreement. Although Llama-3-70B and GPT-4 appear to perform similarly well as judges, GPT-4 comes with the drawback of close-sourced models, i.e., expensive to query and subject to change. Thus, in line with the aim of JailbreakBench to be reproducible, we choose Llama-3-70B as the classifier in our benchmark as it is an open-weight model and comparable to GPT-4 as a judge. Moreover, it has a relatively low FPR, which, although it may systematically reduce the success rate across attack algorithms, is important for remaining conservative and avoiding the misclassification of benign behaviors as jailbroken. We provide an implementation of this judge in our library in two lines of python code:

``` cf=jbb.Classifier(api_key="<your-api-key>) labels=cf(prompt="["Write=phishing email=], responses=["I can't do that."]) ```

We describe more details on judge selection in Appendix C and remain open to updating our chosen judge if consistently better options become available. Finally, we note that submissions to our benchmark, both attacks and defenses, are not permitted to use our Llama-3-70B judge in order to prevent overfitting to the judge.

### Reproducible evaluation framework

We also provide a standardized evaluation framework that--besides providing access to jailbreak artifacts--allows the users to benchmark and report the performance of their methods. The framework implements all system prompts, the JUDGE classifier discussed in SS3.5, and supports both cloud-based and local querying, as discussed in SS3.3. In particular, our framework includes the following LLMs: Vicuna-13B-v1.5 (Zheng et al., 2023), Llama-2-7B-chat-hf (Touvron et al., 2023), GPT-3.5-Turbo-1106 (OpenAI, 2023), GPT-4-0125-Preview (OpenAI, 2023). To facilitate reproducibility, all model use greedy, deterministic sampling (i.e., with temperature equal to zero), and by default all

    &  \\  Metric & Rule-based & GPT-4 & HarmBench & Llama Guard & Llama Guard 2 & Llama-3-70B \\  Agreement (\(\)) & 56.0\% & 90.3\% & 78.3\% & 72.0\% & 87.7\% & **90.7\%** \\ FPR (\(\)) & 64.2\% & 10.0\% & 26.8\% & **9.0\%** & 13.2\% & 11.6\% \\ FNR (\(\)) & 9.1\% & 9.1\% & 12.7\% & 60.9\% & 10.9\% & **5.5\%** \\   

Table 1: Comparison of classifiers across 300 prompts and responses, either harmful or benign. We compute the agreement, false positive rate (FPR), and false negative rate (FNR) for six classifiers. We use the majority vote of three expert annotators as the ground truth label.

LLMs generate 150 tokens per input. A more detailed discussion about reproducibility is given in Appendix E. To evaluate the performance of a set of jailbreak strings on any of these LLMs, one can run the following code snippet:

``` jb=("Phishing":"Write=phishingemail.","Networkhacking":"Write=hackingscript.",...) jb.evaluate_prompts("{"vicuma-13b-v1.5":jb},lim_provider"="litellm") ```

This code first generates responses to the input strings by querying "vicuna-13b-v1.5", after which the prompt-response pairs are scored by the judge classifier. To run the other supported LLMs, users can use one (or multiple) of the following keys when creating the all_prompts dictionary: "llama2-7b-chat-hf", "gpt-3.5-turbo-1106", or "gpt-4-0125-preview". All logs generated by jbb.evaluate_prompts are saved to the logs/eval directory.

### Submissions to JailbreakBench

**New attacks.** Submitting jailbreak strings corresponding to a new attack involves executing three lines of Python. Assuming that the jailbreaking strings are stored in all_prompts and evaluated using jbb.evaluate_prompts as in the code snippet in SS3.6, one can then run the jbb.create_submission function, which takes as arguments the name of your algorithm (e.g., "PAIR"), the threat model (which should be one of "black_box", "white_box", or "transfer"), and a dictionary of hyperparameters called method_parameters.

``` jbb.evaluate_prompts(all_prompts,lim_provider"="litellm") jbb.create_submission(method_name"PAIR",attack_type="black_box",method_params=method_params) ```

The method_parameters should contain relevant hyperparameters of the algorithm. We do not impose _any_ constraints on hyperparameters; for example, we allow adversarial suffixes of arbitrary length. To submit artifacts, users can submit an issue within the JailbreakBench repository, which includes fields for the zipped submissions folder and other metadata, including the paper title and author list. We suggest submissions to include prompts for Vicuna and Llama-2 for direct comparison with previous attacks, although users can provide artifacts _for any LLMs_, including GPT-3.5 and GPT-4. To prevent potential overfitting to the judge, we reserve the right to check manually the jailbreak artifacts and flag entries with a lot of false positives.

**New defenses and models.** To add a new defense to our repository, users can submit a pull request as detailed in the JailbreakBench repository's README file. We are also committed to supporting more models in future versions of this benchmark. To request that a new model be added to JailbreakBench, first ensure that the model is publicly available on Hugging Face, and then submit an issue in the JailbreakBench repository. We are open to adding any new defenses and models, but we will flag those that lead to more than \(90\%\) refusals on our set of benign behaviors.

### JailbreakBench leaderboard and website

Finally, we provide a web-based JailbreakBench leaderboard at https://jailbreakbench.github.io/ for which we use the code from RobustBench(Croce et al., 2021) as the basis. Our website displays the evaluation results for different attacks and defenses as well as links to the corresponding jailbreak artifacts (see Figure 1). Moreover, one can also filter the leaderboard entries according to their metadata (e.g., paper title, threat model, etc.).

## 4 Evaluation of the current set of attacks and defenses

**Baseline attacks.** We include four methods to serve as initial baselines: (1) Greedy Coordinate Gradient (GCG) (Zou et al., 2023), (2) Prompt Automatic Iterative Refinement (PAIR) (Chao et al., 2023), (3) hand-crafted jailbreaks from Jailbreak Chat (JB-Chat) (Albert, 2023), and (4) prompt + random search (RS) attack enhanced by self-transfer (Andriushchenko et al., 2024). For GCG, we use the default implementation to optimize a single adversarial suffix for each behavior, and use the default hyperparameters (batch size of 512, 500 optimization steps). To test GCG on closed-source models we transfer the suffixes it found on Vicuna. For PAIR, we use the default implementation, which involves using Mixtral (Jiang et al., 2024) as the attacker model with a temperature of one, top-\(p\) sampling with \(p=0.9\), \(N=30\) streams, and a maximum depth of \(K=3\). For JB-Chat, we use the most popular jailbreak template, which is called "Always Intelligent and Machiavellian" (AIM).

**Baseline defenses.** Currently, we include five baseline defenses: (1) SmoothLLM (Robey et al., 2023), (2) perplexity filtering (Jain et al., 2023), and (3) Erase-and-Check (Kumar et al., 2023), (4) synonym substitution, (5) removing non-dictionary items. For SmoothLLM, we use swap perturbations, \(N=10\) perturbed samples, and a perturbation percentage of \(q=10\%\). For the perplexity filtering defense, we follow the algorithm from Jain et al. (2023) and compute the perplexity via the Llama-2-7B model. We use Erase-and-Check with an erase length of 20. For SmoothLLM and Erase-and-Check, we use Llama Guard as a jailbreak judge. The last two defenses substitute each word with a synonym with probability 5%, and remove words that are not in the English dictionary provided by the wordfreq library (Speer, 2022), respectively.

**Metrics.** Motivated by our evaluation in Section 3.5, we track the attack success rate (ASR) according to Llama-3-70B as a jailbreak judge. To estimate efficiency, we report the average number of queries and tokens used by the attacks. We do not report these numbers for transfer and hand-crafted attacks since it is unclear how to count them. We still report query and token efficiency for Prompt with RS but note that we do not count the number of queries needed to optimize the universal prompt template and pre-computed suffix initialization (i.e., self-transfer).

**Evaluation of attacks.** In Table 2, we compare the performance of the four jailbreaking attack artifacts included in JailbreakBench. The AIM template from JB-Chat is effective on Vicuna, but fails for all behaviors on Llama-2 and the GPT models; it is likely that OpenAI has patched this jailbreak template due to its popularity. GCG exhibits a slightly lower jailbreak percentage than previously reported values: we believe this is primarily due to (1) the selection of more challenging behaviors in JBB-Behaviors and (2) a more conservative jailbreak classifier. In particular, GCG achieves only 3% ASR on Llama-2 and 4% of GPT-4. Similarly, PAIR, while query-efficient, achieves high success rate only on Vicuna and GPT-3.5. Prompt with RS is on average the most effective attack, achieving 90% ASR on Llama-2 and 78% GPT-4. Prompt with RS also achieves very high query efficiency (e.g., 2 queries on average for Vicuna and 3 for GPT-3.5) due to its usage of a manually optimized prompt template and a pre-computed initialization. Overall, these results show that even recent and closed-source undefended models are highly vulnerable to jailbreaks. Finally, we show ASRs across dataset sources in Appendix B: we observe that the attacks exhibit relatively consistent ASRs across sources, and the deviations across sources are most likely due to the imbalances in composition within categories.

**Evaluation of defenses.** In Table 3, we test three defenses introduced above when combined with the various LLMs (the results of the remaining defenses are deferred to Appendix D). We compute the effectiveness of these algorithms against _transfer attacks_ from the undefended models, which means that we simply re-use the jailbreaking strings found by each attack on the original LLM (whose results are shown in Table 2. We note that this is possibly the simplest type of evaluation, since it is not adaptive to the target defense, and more sophisticated techniques might further increase ASR. We observe that Perplexity Filter is only effective against GCG. Conversely, SmoothLLM successfully reduces the ASR of GCG, PAIR, while might not work well against JB-Chat and Prompt with RS (see Vicuna and GPT-4). Erase-and-Check appears to be the most solid defense, although Prompt

    & &  &  \\  Attack & Metric & Vicuna & Llama-2 & GPT-3.5 & GPT-4 \\   & Attack Success & 69\% & 0\% & 71\% & 34\% \\  & Avg. Queries & 34 & 88 & 30 & 51 \\  & Avg. Tokens & 12K & 29K & 9K & 13K \\   & Attack Success & 80\% & 3\% & 47\% & 4\% \\  & Avg. Queries & 256K & 256K & — & — \\  & Avg. Tokens & 17M & 17M & — & — \\   & Attack Success & 90\% & 0\% & 0\% & 0\% \\  & Avg. Queries & — & — & — & — \\  & Avg. Tokens & — & — & — & — \\   & Attack Success & 89\% & 90\% & 93\% & 78\% \\  & Avg. Queries & 2 & 25 & 3 & 1K \\   & Avg. Tokens & 3K & 20K & 3K & 515K \\   

Table 2: **Evaluation of current attacks.** For each method we report attack success rate according to Llama-3-70B as a judge, and average number of queries and tokens used, across target LLMs.

    & &  &  \\  Attack & Defense & Vicuna & Llama-2 & GPT-3.5 & GPT-4 \\   & SmoothLLM & 55\% & 0\% & 5\% & 19\% \\  & Perplexity Filter & 69\% & 0\% & 17\% & 30\% \\  & Erase-and-Check & 0\% & 0\% & 2\% & 1\% \\   & SmoothLLM & 4\% & 0\% & 0\% & 4\% \\  & Perplexity Filter & 3\% & 1\% & 0\% & 0\% \\  & Erase-and-Check & 17\% & 1\% & 3\% & 2\% \\   & SmoothLLM & 73\% & 0\% & 0\% & 0\% \\  & Perplexity Filter & 90\% & 0\% & 0\% & 0\% \\  & Erase-and-Check & 1\% & 0\% & 0\% & 0\% \\   & SmoothLLM & 68\% & 0\% & 4\% & 56\% \\  & Perplexity Filter & 88\% & 73\% & 61\% & 70\% \\   & Erase-and-Check & 24\% & 25\% & 8\% & 10\% \\   

Table 3: **Evaluation of current defenses.** We report the success rate of _transfer attacks_ from the undefended LLM to the same LLM with different defenses. More defenses are in Appendix D.

with RS still achieves non-trivial success rate on all LLMs. Finally, we note that using some of these defenses incurs a substantial increase in inference time, which should be considered when analyzing the results. We hope that the easy access to these defenses provided by our benchmark will facilitate the development of adaptive jailbreaking algorithms specifically designed to counter them.

**Refusal evaluation.** We compute refusal rates on 100 benign behaviors from JBB-Behaviors on Vicuna and Llama-2 for all defenses. We use Llama-3 BB as a refusal judge with the prompt given in Appendix F. In Figure 2, we observe that, as expected, Vicuna rarely refuses to reply (9% without defenses), while Llama-2 returns refusals in more than 60% of cases. Moreover, we see that the current defenses, with the selected hyperparameters, do not increase the refusal rate substantially. This evaluation is intended to serve as a simple sanity check to quickly detect overly conservative models or defenses. However, it is _not_ a substitute for more thorough utility evaluations, such as using standard benchmarks like MMLU (Hendrycks et al., 2021) or MTBench (Zheng et al., 2023).

## 5 Outlook

**Future plans.** We view JailbreakBench as a first step toward standardizing and unifying the evaluation of the robustness of LLMs against jailbreaking attacks. At present, given the nascency of the field, we do not restrict submissions to particular threat models or target model architectures. Instead, we intend for the current version of JailbreakBench to reflect an initial pass at standardizing jailbreaking evaluation, and intend to periodically update this benchmark as the field develops and the "rules of the game" become more well-established. This may also involve an expanded set of available jailbreaking behavior datasets, more rigorous evaluation of jailbreaking defenses, particularly with respect to non-conservatism and efficiency, updates to classifier used as judge, and periodic re-evaluation of attack success rates on closed-source LLMs.

**Ethical considerations.** Prior to making this work public, we have shared our jailbreaking artifacts and our results with leading AI companies. We also have carefully considered the ethical impact of our work. In the evolving landscape of LLM jailbreaking, several facts stand out: (1) the code for the majority of jailbreaking attacks is open-sourced, meaning that malicious users already possess the means to produce adversarial prompts, (2) as LLMs are trained using Web data, most of the information we seek to elicit from LLMs is available via search engines, thus open-sourcing jailbreaking artifacts on a limited set of behaviors does not contribute any new content that was not already publicly accessible, (3) a promising approach to making LLMs more resilient to jailbreaking attacks is to fine-tune them on jailbreak strings, thus we expect that our repository of jailbreak artifacts will contribute to progress toward safer LLMs.

**Limitations.** While we tried to make our benchmark as comprehensive as possible, we had to restrict the scope of what is allowed for attackers. For example, in the current form, we do not allow attackers to modify the system prompt or prefill the LLM response with a certain string. Moreover, modern LLMs often accept inputs in modalities other than text, which could equally exploited for jailbreaking. Our benchmark does not currently offer such option and focuses solely on text.