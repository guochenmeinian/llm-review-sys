# Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking

Yuwei Zhang\({}^{1}\), Tong Xia\({}^{1}\), Jing Han\({}^{1}\), Yu Yvonne Wu\({}^{1}\), Georgios Rizos\({}^{1}\), Yang Liu\({}^{1}\),

**Mohammed Mosuily\({}^{2}\), Jagmohan Chauhan\({}^{2}\), Cecilia Mascolo\({}^{1}\)**

\({}^{1}\) University of Cambridge, \({}^{2}\) University of Southampton, UK

\({}\) joint first authors, equal contribution

{yz798, tx229}@cam.ac.uk

###### Abstract

Respiratory audio, such as coughing and breathing sounds, has predictive power for a wide range of healthcare applications, yet is currently under-explored. The main problem for those applications arises from the difficulty in collecting large labeled task-specific data for model development. Generalizable respiratory acoustic foundation models pretrained with unlabeled data would offer appealing advantages and possibly unlock this impasse. However, given the safety-critical nature of healthcare applications, it is pivotal to also ensure openness and replicibility for any proposed foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory Acoustic foundation model pretraining and benchmarking system, as the first approach answering this need. We curate large-scale respiratory audio datasets (\(\)136K samples, over 400 hours), pretrain three pioneering generalizable acoustic models, and build a benchmark consisting of 19 downstream respiratory health tasks for evaluation. Our pretrained models demonstrate superior performance (against existing acoustic models pretrained with general audio on 16 out of 19 tasks) and generalizability (to unseen datasets and new respiratory audio modalities). This highlights the great promise of respiratory acoustic foundation models and encourages more studies using OPERA as an open resource to accelerate research on respiratory audio for health.

The OPERA website can be found at opera-benchmark.github.io

Our codebase is open-sourced at github.com/evelyn0414/OPERA

## 1 Introduction

Respiratory audio, such as coughing and breathing sounds generated by the respiratory system's airflow, contains multiple physiological characteristics of individuals and therefore its modeling could be instrumental in health monitoring and disease detection applications . For instance, audio recordings can be used to estimate respiratory rate and lung function , detect snoring and apnea events during sleep , assess the effect of smoking on health  and diagnose diseases like flu and asthma .

To enable the widespread adoption of these applications, high-performing algorithms are needed. Related studies rely on traditional signal processing methods , which require domain knowledge and often exhibit limited performance. Supervised deep acoustic models have been proposed  but their performance heavily depends on the volume and quality of available labels, which might be difficult and expensive to collect. Hence, foundation models pretrained with large unlabeled respiratory audio data have a high potential to improve performance through transfer learning and supervised fine-tuning . However, in contrast with other healthdata modalities like clinical imaging , electronic health records (EHRs) , and medical time series [75; 1; 15], foundation models for respiratory audio are largely under-explored.

**Respiratory audio datasets are available but no comprehensive collection has been curated.** Recent years have seen an ever-increasing accumulation of respiratory audio [70; 48; 12], exhibiting heterogeneous properties such as varying acquisition modalities and sampling rates. These datasets exhibit significant potential for acoustic model development and evaluation. However, no existing effort has curated such data systematically.

**There is no open respiratory acoustic foundation model, impeding the field's growth and understanding.** Existing open-source acoustic models like AudioMAE  and CLAP  are pretrained on general audio event datasets such as YouTube audio, containing very few (around 0.3%) respiratory sounds [38; 24]. These models may not be able to effectively capture the subtle nuances of respiratory sounds, which can vary in abrupt bursts, aperiodic components, and frequency distributions, particularly across different health conditions . Although a model pretrained on respiratory sounds has been recently presented , it is not open-source, making it hard to analyze, replicate, or compare its workings. The insights on how to effectively train generalizable respiratory acoustic models also remain limited.

**There is no ready-to-use benchmark for respiratory audio research.** Current task-specific studies evaluate their models on purposely collected datasets, leaving the models' generalizability to other tasks unclear . A benchmark that combines multiple public datasets across diverse applications to enable fair and comprehensive evaluations of the developed foundation models is essential but currently lacking. This is crucial for safety-critical health applications, where models must be rigorously evaluated before use [68; 65].

To mitigate these gaps, in this paper, we put forward _OPERA_, an **OPEn**R**espiratory **A**coustic foundation model pretraining and benchmarking system (Figure 1). It curates unlabeled respiratory audio datasets, pretrain three pioneering foundational models, and evaluates them against existing pretrained acoustic models across various applications. Specifically, our contributions are:

* We curate a unique large-scale (\(\)136K samples, 400+ hours), multi-source (5 datasets), multi-modal (breathing, coughing, and lung sounds) and publicly available (or available on request) respiratory audio dataset for generalizable model pretraining, orders of magnitude larger than the number of respiratory audio samples in datasets used for training existing open acoustic models.
* We pretrain 3 generalizable acoustic models with the curated unlabeled data using the most common self-supervised approaches (a contrastive learning-based transformer, a contrastive learning-based CNN model, and a generatively pretrained transformer) to study the effect of the training designs.
* We employ 10 labeled datasets (6 not covered by pretraining) to formulate 19 respiratory health tasks (12 in health condition inference and 7 in lung function estimation), ensuring fair, comprehensive and reproducible downstream evaluation.
* We benchmark the performance of our 3 pretrained models, one commonly used acoustic feature set, and 3 open pretrained acoustic models on these tasks as a starting point for future exploration.

Figure 1: System overview of OPERA. After data curation, respiratory audio encoders are pretrained and then evaluated on various downstream health tasks.

Extensive experiments demonstrate that our pretrained models outperform the models pretrained with general audio on 16 out of 19 benchmark tasks, confirming the power and promise of dedicated respiratory acoustic foundation models. Results also show that our models are generalizable across multiple downstream tasks, including new datasets and unseen respiratory audio modalities. This is a critical advancement towards realizing the potential of respiratory sounds as a mainstream technique for health monitoring.

Within our three models, we find that the contrastive pretraining model is better for classification-based downstream tasks, while the generative pretrained model performs better in regression tasks, possibly due to the nature of their training objectives: contrastive learning can capture the nuances of the local patterns to make features distinguishable while generative learning focuses more on global features which are vital for regression. Our transformer models generally outperform the CNN model because they have stronger modeling capability, though requiring more intensive computation. These findings provide insightful guidance to the development and application of such types of models.

In summary, this paper introduces _the first open-source respiratory acoustic foundation model pretraining and benchmarking system_. This represents a critical first step towards comprehensive and reproducible audio foundation models for health: future foundation model research can leverage our system as an experimental resource, and application studies can take advantage of our pretrained models as feature extractors. This can facilitate progress in both machine learning and healthcare. These efforts will extend current machine learning capabilities, now able to _see_ (via vision) and _read_ (via natural languages), to also _listen to_ (via audio) our health.

## 2 Related Work

### Pretraining in Acoustic Modeling

Models pretrained on large-scale datasets have demonstrated great generalizability in diverse downstream tasks, especially when labeled data are limited [8; 16; 25; 35]. For audio-driven health applications, several general audio pretrained models can be used as feature extractors. One widely used model is _VGGish_, trained on 5.24 million hours of audio from YouTube videos to predict 30,871 categories of video labels. Other models have been developed for audio event classification tasks [41; 10; 35]. Among them, _AudioMAE_ is an open model trained via an auto-encoding objective without requiring any audio labels. Inspired by recent advances in large language models, language-supervised pretraining has also been explored. _CLAP_ is an open model pretrained in this manner. We have included these open models in our benchmark.

It is also worth noting that these open models are pretrained on general audio event datasets such as _AudioSet_, _FSD50K_, and _FreeSound_, which contains few samples of respiratory-related audio. For instance, AudioSet's 2 million clips include only 2334 snoring, 871 cough, 834 breathing, and 1200 sneeze clips, making up only 0.3% of the total. In face of this issue, we curate large-scale respiratory audio datasets to pretrain our generalizable respiratory acoustic models for comparison.

In terms of pretraining methods, given the difficulty in collecting large-scale labeled health-related datasets, we consider self-supervised learning (SSL) to leverage unlabelled data for learning meaningful representations [63; 1; 6]. Main SSL methods fall into two categories: contrastive [11; 5; 55] and generative [29; 35; 47]. Contrastive learning trains models to distinguish between similar and dissimilar samples, while generative models are trained to reconstruct original audio data or features from masked or corrupted versions. Since they have been demonstrated to be effective in general audio, We implement both methods in our system.

A recent work, _HeAR_, curated millions of respiratory audio clips from YouTube videos to pretrain a model using a generative SSL approach. However, neither the data nor the model are publicly available, resulting in a lack of transparency and reproducibility. Limited exploration has been conducted on the reasoning behind the chosen SSL method for various downstream tasks. Our work investigates, for the first time, open pretraining generalizable respiratory acoustic models to provide a better understanding of their limits and their potential.

### Benchmarks in Respiratory Audio-based Applications

Current respiratory audio-based health studies typically evaluate their developed models using their self-formulated protocols [6; 71; 73], instead of following a uniform evaluation pipeline. This leads to weak reproducibility due to several challenges : lack of implementation details or releasedcode, absence of reliable training and testing division, and varying implementation frameworks (e.g., some in TensorFlow  while other in PyTorch ) making them difficult to compare.

High-quality benchmarks are essential in machine learning to ensure advancements are reliable and applicable to real-world problems. While several benchmarks exist for pretrained representation models on general audio event detection and speech recognition [64; 57; 26; 74], similar benchmarks are missing in respiratory audio for health, despite their equal importance. The only related benchmark  in this area compares supervised models for breath phase and adventitious sound detection using a single dataset, and is thus not applicable for evaluating foundation models. A comprehensive benchmarking effort of respiratory acoustic foundation models is lacking but has the potential to really shed light on the power of these techniques in the context of respiratory health tasks.

## 3 System Overview

As shown in Figure 1, OPERA comprises three main components: data curation (including unlabeled data for pretraining and labeled data for evaluation), general-purpose pretraining to develop generalizable acoustic models (Encoder), and a benchmark comparing the pretrained models on various downstream tasks.

In OPERA, we employ five datasets for pretraining and ten datasets for benchmarking. Four of the downstream datasets overlap with the pretraining resources, but we ensure the testing data is held out before pretraining and thus is never seen by the models. During the pretraining step, we build two SSL strategies enabling the use of different encoder architectures. We then use the pretrained models to extract features and apply linear probing to report the performance for downstream tasks. Detailed information about data curation and pretraining methods is elaborated on in Section 4, and the benchmark data curation and evaluation results are summarized in Section 5.

## 4 Self-supervised Pretraining

### Pretraining Datasets

Five open data resources are curated in OPERA to enable the training of respiratory acoustic foundation models (Table 1). They were collected by different research institutions using various protocols, and are all publicly available or accessible upon request. Some recordings were made with a microphone near the mouth [70; 12; 48], while others used a digital stethoscope attached to the chest [52; 31]. This allows the pretrained models to see heterogeneous data for better generalizability.

We only include qualified samples (those identified as respiratory audio, not noise) in the pretraining step. Some labeled audio samples from these datasets, which can be used for downstream evaluations, are held out. We then trim the remaining audio recordings by removing the beginning and ending silence to further ensure the quality of the data. The statistics of the data after quality check are summarized in Table 1 (extended description can be found in Appendix A.1). As a result, the entire pretraining dataset consists of 135,944 samples, with a total duration of about 404.1 hours.

Before pretraining, all recordings are resampled to 16 kHz and merged into a mono channel. They are then transformed into spectrograms using 64 Mel filter banks with a 64 ms Hann window that shifts every 32 ms [58; 76]. For example, a 4s recording will be converted into a spectrogram of \(1 126 64\) dimension. Finally, these spectrograms are used to pretrain our respiratory acoustic foundation models.

  
**Data name** & **Collected by** & **SR** & **Modality** & **\#Sample** & **Duration (s)** & **Crop (s)** \\  COVID-19 Sounds  & Microphone & 16\(\)44.1kHz & Induced cough (3 times) & 40866 & 6.1(2.6\(\)11.2) & 2 \\  & & Deep breath (5 times) & 36605 & 20.5(9.7\(\)31.6) & 8 \\  & & 48kHz & Induced cough (3 times) & 19533 & 4.1(2.1\(\)9.2) & 2 \\  & & & & Exhalation (5 times) & 719.7 & 7.4(2.2\(\)15.6) & 4 \\  & & & Induced (ough up to 10s) & 7179 & 6.9(2.4\(\)9.9) & 2 \\  & & 4\(\)44.1kHz & lung sound (several breath cycles) & 538 & 22.2(30.0\(\)65.9) & 8 \\  & & 4kHz & lung sound (several breath cycles) & 10554 & 15.0(15.0\(\)15.0) & 8 \\   

Table 1: Statistics of the data used for model pretraining (SR: sampling rate; Duration: mean [95% quantile range]; Crop: cropped length for pretraining).

### Pretraining Models and Methods

We pre-train our models using a combination of the aforementioned data resources, dividing each dataset into equally-sized batches for consistent processing. We randomly shuffle the batches and reserve 10% for validation. Due to inherent variations in audio length within individual batches, we employ random cropping of spectrograms, with crop lengths specified in Table 1. Considering the unlabeled nature of the pretraining data, we adopt the most representative SSL methods: contrastive learning-based and generative pretraining-based objectives to pretrain our models. The rationale behind this choice is that if an encoder can distinguish the source of audio segments (contrastive) or reconstruct masked spectrograms (generative), it is expected to have encoded useful and generalizable acoustic features. The three foundation models we pretrained are:

* **OPERA-CT**: OPERA-CT is a contrastive learning based  transformer model. Two segments from the same spectrogram are regard as a positive pair, otherwise negative pairs. As shown in Figure 2(a), an encoder network (a transformer ) extracts features from these segments, and a projector maps them into a low-dimensional representation space, where bilinear similarity is calculated. The optimization objective aims to maximize the similarity between positive pairs and minimize it for negative pairs. The encoder has 31M trainable parameters.
* **OPERA-CE**: Similar to OPERA-CT, CE leverages a contrastive pre-training approach. However, it utilizes a more lightweight and efficient CNN encoder (EfficientNet-B0) , which has approximately 4M trainable parameters.
* **OPERA-GT**: OPERA-GT is a generatively pretrained transformer model . As shown in Figure 2(b), the encoder (a vision transformer with 21M trainable parameters) is utilized to extract useful features from masked spectrograms, from which the decoder (a lightweight swin-transformer with 12M trainable parameters) can reconstruct the original spectrograms. To train the encoder and the decoder, spectrograms are cropped to equal lengths and then split into small patches. We randomly mask 70% of patches per spectrogram for reconstruction.

Detailed introduction to these three models can be found in Appendix A.2. We train them for up to 200 epochs and save the best model based on the held-out validation set (i.e., its performance on the pretraining objective). Model checkpoints are also released. More pretraining results and analysis are available in Appendix A.3.

## 5 Benchmarking

### Benchmark Datasets and Tasks Setup

**Tasks**. To facilitate the evaluation of our pretrained models, existing acoustic models, and future emerging respiratory acoustic foundation models, we introduce a new benchmark. A total of 10 labeled respiratory audio datasets, encompassing 6 respiratory audio modalities, are curated for this benchmark. Among these 10 datasets, 6 are new and unseen during the pretraining stage.

Using these 10 datasets, we formulate 19 downstream tasks: 12 for health condition inference and 7 for lung function estimation. The first group covers disease detection such as COVID-19 and COPD (Chronic Obstructive Pulmonary Disease), participant attribute inference like smoker

Figure 2: Self-supervised learning methods used in our system.

and gender, disease severity classification, and body position in sleep monitoring. Tasks 1-10 are binary classification, while Tasks 11-12 involve 5 classes. The second group includes spirometry test performance and respiratory rate estimation, which are regression tasks aimed at predicting continuous values. Data and task statistics are summarized in Table 2, with detailed descriptions and licenses provided in Appendix A.1.

_All data in this benchmark are publicly available or under controlled access procedures_. When available, we follow the official train-test split (Tasks 1-4 and 12-18); otherwise, we implement a random participant-independent split to ensure realistic evaluation (Tasks 5-11 and 19). Due to the limited number of participants in Tasks 13-19, we employ leave-one-subject-out evaluation. For all other tasks, we adopt a fixed random train-validation-test split.

**Baselines**. In addition to our pretrained models, we also include a commonly used acoustic feature set and three open pretrained acoustic models in this benchmark. They are **Opensmile** (_Emobase_ acoustic feature set), **VGGish** (supervised pretrained), **AudioMAE** (self-supervised pretrained) and **CLAP** (language-supervised pretrained). We consider these four methods as baselines to be distinguished from our pretrained models. We also pretrain these architectures with our OPERA data and results can be found in Appendix A.4.

**Evaluation protocol**. All tasks are evaluated using the standard linear probe protocol : training a single fully connected layer on top of the representations extracted from the frozen encoder. Linear evaluation focuses on the quality of learned representations and is applicable to some very small datasets. **AUROC** (area under the receiver operating characteristic) is reported for classification (Task 1-12) and **MAE** (mean absolute error) is reported for regression (Task 13-19). For a comprehensive overall evaluation, we report **MRR** (mean reciprocal rank)  across tasks.

For baselines, both the data pre-processing and feature extraction strictly follow their official implementation. For our pretrained models, the same audio preprocessing is used as in pretraining. We then segment our audio into short frames to feed into our foundation models to extract features, and use the averaged representation over these frames as the input for the linear layer . An extended description of the implementing details can be found in Appendix A.2. _Note that the baselines and our pretrained models are implemented within the same pipeline, making our results easy to reproduce and our benchmark ready to use._

### Experimental Results

We report the MRR of different task groups in Table 3, with the detailed reciprocal ranks of all evaluated methods on each task provided in Appendix A.4. The performance metrics for each task are summarized in Table 4 and Table 5. Our benchmark demonstrates reliability, as our implementation of baselines achieves comparable performance to those reported in the literature (e.g., existing

    & **ID** & **Task** & **Modality** & **\#Sam. (\#Sub.)** & **Data Distribution** \\ 
**UK COVID-19** & T1 & Covid / Non-covid & Exhalation & 2500 (2500) & 840 / 1660 \\  & T2 & Covid / Non-covid & Cough & 2500 (2500) & 840 / 1660 \\
**COVID-19 Sounds** & T3 & Symptomatic / Healthy & Breath & 4138 (3294) & 2029 / 2109 \\  & T4 & Symptomatic / Healthy & Cough & 4138 (3294) & 2029 / 2109 \\
**CoughVID** & T5 & Covid / Non-covid & Cough & 6175 (n/a) & 547 / 5628 \\  & T6 & Female / Male & Cough & 7263 (n/a) & 2468 / 4795 \\
**ICBHI** & T7 & COPD / Healthy & Lung sounds & 828 (90) & 793 / 35 \\
**Coswara** & T8 & Smoker / Non-smoker & Cough & 948 (n/a) & 201 / 1747 \\  & T9 & Female / Male & Cough & 2496 (n/a) & 759 / 1737 \\
**KAUH** & T10 & Obstructive / Healthy & Lung sounds & 234 (79) & 129 / 105 \\
**Respiratory**(rIR) & T11 & COPD severity & Lung sounds & 504 (42) & 72 / 60 / 84 / 84 / 204 \\
**SSBPR** & T12 & Body position recognition & Snoring & 7468 (20) & 1638 / 1454 / 1269 / 1668 / 1439 \\ 
**MMlung** & T13 & FVC & Deep breath & 40 (40) & 3.402 \(\) 1.032 L \\  & T14 & FEV1 & Deep breath & 40 (40) & 2.657 \(\) 0.976 L \\  & T15 & FEV1/FVC & Deep breath & 40 (40) & 0.808 \(\) 0.190 L \\  & T16 & FVC & O Vowels & 40 (40) & 3.402 \(\) 1.032 L \\  & T17 & FEV1 & O Vowels & 40 (40) & 2.657 \(\) 0.976 L \\  & T18 & FEV1/FVC & O Vowels & 40 (40) & 0.808 \(\) 0.190 L \\
**NoseMic** & T19 & Respiratory rate & Breath & 1297 (16) & 13.915 \(\) 3.386 bpm \\   

Table 2: Downstream task characteristics grouped by task category. Datasets in grey are entirely new (not used in pretraining), while others have test sets held out unseen. For T13-T19, FVC denotes forced vital capacity (L), FEV1 is the forced expiratory volume in 1 second, and FEV1/FVC refers to the ratio of the two.

cough-based COVID-19 detection studies report an AUROC of about \(0.65\), aligning with our baseline results in Task 2). Through these extensive experimental results, we now answer the following three main research questions (RQs):

**RQ1. Can pretraining a foundational model with diverse unlabeled respiratory audio data lead to better performance than baselines designed for general audio?**

From results highlighted in Table 3, it is evident that our pretrained respiratory acoustic foundation models outperform both the acoustic feature set and existing general audio pretrained models. Among them, OPERA-CT and OPERA-GT achieve the highest MMR scores of 0.5632 and 0.5298, respectively. Looking at \(\) and * in Table 4 and 5, the best OPERA model outperforms the acoustic feature set on 17 tasks and the baseline pretrained models on 16 tasks out of the 19 evaluated tasks. This provides a clear positive answer to RQ1. This advantage likely stems from their exposure to _large-scale_ and _heterogeneous_ respiratory audio data, showing the power and promise of respiratory audio foundation models for health applications.

Now let us dive into the task performance at a finer granularity. For classification, an AUROC exceeding 0.7 is typically desirable to demonstrate the utility of the extracted features . When examining the AUROC in Table 4, OPERA models achieve an AUROC exceeding 0.7 on 6 of the 12 health condition inference tasks (Task 2, 6-7, 9-10, and 12), whereas the best baseline, CLAP, only surpasses this threshold on 3 tasks (Task 7, 9-10, and 12). This indicates that our models better encode health condition-related information from respiratory audio. Regarding lung function estimations (regression tasks), the model needs to capture the global dynamics from the entire audio sample and lower MAE indicates better performance. In Table 5, our pretrained models reduce the error in FVC estimation using breathing sounds (Task 13), FEV1/FVC estimation using breathing sounds (Task 15), FVC estimation using vowel sounds (Task 16), FEV1/FVC estimation using vowel sounds (Task 18), and respiratory rate estimation (Task 19), with performance close to baselines on other tasks. Furthermore, OPERA-GT also achieves a lower standard deviation across subjects, suggesting better generalizability and robustness to different subjects, which are of great importance for healthcare applications.

**RQ2. Are the pretrained respiratory acoustic models generalizable to new data?**

It is crucial that foundation models can generalize to new and unseen data once developed. In our benchmark, we have 12 tasks formulated from unseen datasets (Task 8-19) and unseen respiratory audio modalities (Task 12, 16-18) not used for pretraining. Notably, our respiratory acoustic foundation models demonstrate good generalization capabilities, achieving the best performance on 5 out of 5 classification tasks and 5 out of 7 regression tasks. They are able to outperform the acoustic feature set and general audio pretrained models which are supposed to exhibit generalizability. Specifically, in

   ID & Task Abate & Opensmile & VGGish & AudioMAE & CLAP & **OPERA-CT** & **OPERA-CE** & **OPERA-GT** & \\  T1 & Covid (Euclidean) & 0.550 \(\) 0.015 & 0.580 \(\) 0.001 & 0.549 \(\) 0.001 & 0.565 \(\) 0.001 & 0.586 \(\) 0.008 & 0.551 \(\) 0.010 & 0.605 \(\) 0.001 & \(\) \\ T2 & Covid (Cough) & 0.649 \(\) 0.008 & 0.557 \(\) 0.005 & 0.616 \(\) 0.001 & 0.648 \(\) 0.003 & 0.701 \(\) 0.002 & 0.629 \(\) 0.008 & 0.673 \(\) 0.001 & \(\) \\ T3 & Symptom (Breath) & 0.571 \(\) 0.008 & 0.571 \(\) 0.003 & 0.583 \(\) 0.003 & 0.611 \(\) 0.006 & 0.603 \(\) 0.005 & 0.610 \(\) 0.004 & 0.613 \(\) 0.002 & \(\) \\ T4 & Symptom (Cough) & 0.633 \(\) 0.012 & 0.608 \(\) 0.004 & 0.609 \(\) 0.001 & 0.669 \(\) 0.002 & 0.608 \(\) 0.006 & 0.668 \(\) 0.001 & 0.673 \(\) 0.001 & \(\) \\ T5 & Covid (Cough) & 0.537 \(\) 0.001 & 0.538 \(\) 0.002 & 0.554 \(\) 0.004 & 0.599 \(\) 0.007 & 0.578 \(\) 0.001 & 0.566 \(\) 0.008 & 0.552 \(\) 0.003 & \(\) \\ T6 & Gender (Cough) & 0.677 \(\) 0.005 & 0.600 \(\) 0.001 & 0.628 \(\) 0.001 & 0.665 \(\) 0.001 & 0.795 \(\) 0.001 & 0.721 \(\) 0.001 & 0.735 \(\) 0.000 & \(\) \\ T7 & COPD (Cough) & 0.579 \(\) 0.003 & 0.605 \(\) 0.007 & 0.886 \(\) 0.007 & 0.386 \(\) 0.003 & 0.085 \(\) 0.002 & 0.672 \(\) 0.011 & 0.741 \(\) 0.011 & \(\) \\ T8 & Smoker (Cough) & 0.544 \(\) 0.006 & 0.507 \(\) 0.002 & 0.549 \(\) 0.002 & 0.680 \(\) 0.005 & 0.685 \(\) 0.002 & 0.674 \(\) 0.013 & 0.650 \(\) 0.005 & \(\) \\ T9 & Gender (Cough) & 0.753 \(\) 0.008 & 0.606 \(\) 0.006 & 0.724 \(\) 0.001 & 0.742 \(\) 0.001 & 0.874 \(\) 0.000 & 0.801 \(\) 0.002 & 0.835 \(\) 0.001 & \(\) \\ T10 & Obtrastive (Janz) & 0.636 \(\) 0.082 & 0.605 \(\) 0.003 & 0.616 \(\) 0.004 & 0.697 \(\) 0.004 & 0.722 \(\) 0.016 & 0.741 \(\) 0.014 & 0.703 \(\) 0.016 & \(\) \\ T11 & COPD severity (Janz) & 0.494 \(\) 0.054 & 0.590 \(\) 0.004 & 0.510 \(\) 0.021 & 0.636 \(\) 0.005 & 0.625 \(\) 0.008 & 0.683 \(\) 0.007 & 0.606 \(\) 0.015 & \(\) \\ T12 & Position (Searning) & 0.772 \(\) 0.005 & 0.657 \(\) 0.002 & 0.649 \(\) 0.001 & 0.702 \(\) 0.001 & 0.781 \(\) 0.000 & 0.769 \(\) 0.008 & 0.742 \(\) 0.010 & \(\) \\   

Table 4: AUROC on health condition inference tasks (higher is better). The best model for each task is highlighted. We report mean and standard deviation from five independent runs. \(\) and * indicates superiority over the opensmile feature set and the other pretrained baselines respectively.

   Task & \# & Opensmile & VGGish & AudioMAE & CLAP & **OPERA-CT** & **OPERA-CE** & **OPERA-GT** \\  All & 19 & 0.2912 & 0.2289 & 0.2489 & 0.3435 & 0.5632 & 0.4412 & 0.5298 \\  Health condition inference & 12 & 0.2190 & 0.1714 & 0.2058 & 0.4319 & 0.6944 & 0.4153 & 0.4569 \\ Lung function estimation & 7 & 0.4150 & 0.3276 & 0.3228 & 0.1918 & 0.3381 & 0.4857 & 0.6548 \\   

Table 3: Mean reciprocal ranks on task groups (higher is better). The best model within each group is highlighted in pink and the second-best is highlighted in blue (p values reported in Appendix A.4).

[MISSING_PAGE_FAIL:8]

OPERA is _not_ intended for clinical use and should not be considered safe for such applications. Care should be taken to prevent potential misuse when utilizing the models.

In addition to the study we have done, OPERA can support a number of future explorations:

**(1) Studying data-efficient fine-tuning**. Section 5 uses linear evaluation with frozen encoders following standard protocols and accommodating limited downstream data (see Table 2). We select some tasks with relatively abundant labeled data to examine fine-tuning performance (details in Appendix A.4). Results for Task 4 are presented in Table 6. Using the same number of labeled data as in linear probing (1749 samples), all models show improved performance and the three OPERA models achieve an AUROC above \(0.7\). With more labeled data for fine-tuning (6648 samples), the best OPERA-GT model achieves an AUROC of \(0.739\). Similarly, OPERA-CT's performance on Task 12 (7468 samples) could be enhanced to \(0.994\) compared to \(0.781\) in linear evaluation.

However, most other tasks have a much smaller training set, and thus data efficient large model fine-tuning approaches are desirable. Methods have been proposed in the machine learning literature such as adapter tuning , prefix tuning , prompt tuning , and low-rank adaptation . Yet, they are not designed for audio (spectrograms) or acoustic foundation models. Considering the

Figure 3: Saliency maps generated by OPERA-CT and OPERA-GT on three example tasks (T2, T13, and T19). The yellow color indicates the largest gradient on the spectrogram.

properties of downstream health-related tasks which often exhibit limited and imbalanced data, novel audio-specific data efficient fine tuning methods need to be explored.

**(2) Investigating scaling law in respiratory acoustic foundation models.** Recent research on foundation models has uncovered their emergent abilities, largely arising from scaling up pretraining data and model size . It is also interesting to study the scaling laws in respiratory acoustic foundation models. While the OPERA dataset is already extensive, further expansion would be valuable for this purpose. Our benchmark can help to quantify how increasing a model's scale and its training data can significantly enhance performance on downstream tasks. Based on the currently 404 hours of respiratory audio, our OPERA-CT (31M parameters) and OPERA-GT (21M) models surpass the lightweight OPERA-CE model (4M). With the rapid accumulation of respiratory audio datasets , more evaluation of scaling laws should be conducted in future.

**(3) Exploring novel pretraining strategies for unlabeled health audio.** We have pretrained three models (OPERA-CT, OPERA-GT, OPERA-CE) and compared their performance. More configurations in terms of model size, architecture, and pretraining methods could be compared in the future. Among the two representative SSL approaches we adapted for pretraining, there exist limitations: For contrastive learning, defining positive and negative pairs is challenging due to downstream task diversity, and our definitions might not be optimal. In generative pretraining, using alternative objectives to reconstruction might improve performance on discriminative tasks. Combining these methods could be beneficial but presents challenges in balancing objectives, and previous studies suggest simple combinations do not improve performance . Audio data also pose unique challenges like heterogeneous sound types, varying sampling rates and durations, and complex temporal-frequency correlations, requiring tailored solutions to better pretrain and apply the foundation models. OPERA provides a framework for exploring these technical challenges.

By introducing this open-source system, we hope to lay the groundwork for responsible, reliable, and sustainable development of foundation models in respiratory healthcare, paving the way for a healthier future for generations to come.