# Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design

**Anonymous Author(s)**

Affiliation

Address

email

###### Abstract

Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration is the first method to jointly address explainability and sample efficiency for molecular design. The code is available at https://figshare.com/s/d0cd53fc14027accd7b0.

## 1 Introduction

Molecular discovery requires identifying candidate molecules possessing desired properties amidst an enormous chemical space . Generative molecular design has become a popular paradigm in drug discovery, offering the potential to navigate chemical space more efficiently with promise for accelerated discovery. Very recently, efforts have come to fruition and a large number of works have reported experimental validation of generated inhibitors, notably for both distribution learning [2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14] and goal-directed generation [15; 16; 17; 18; 19; 20] approaches. Perhaps now more than ever, existing challenges in explainability and sample efficiency offer an avenue to propel generative molecular design towards outcomes that are not yet possible. Specifically, if one can elucidate _why_ certain substructures or molecules satisfy a target objective, the model's _knowledge_ can be made actionable, for example, in an interplay with domain experts. Moreover, sample efficiency concerns with how many experiments, i.e., oracle calls, are required for a model to optimize the target objective. This is a pressing problem as the most informative high-fidelity oracles are computationally expensive, e.g., molecular dynamics (MD) for binding energy prediction [21; 22]. If a generative model can _directly_ optimize these expensive oracles, the capabilities of generative design can be vastly advanced.

In this work, we propose Beam Enumeration to exhaustively enumerate the most probable token subsequences in language-based molecular generative models and show that valid molecular substructures can be extracted from these partial trajectories. We demonstrate that the extracted substructures are informative when coupled with reinforcement learning (RL) and show that this information can be made _actionable_ to self-condition the model's generation by only evaluating sampled molecules containing these substructures with the oracle. The results show significantly enhanced sample efficiency with an expected small trade-off in diversity. Beam Enumeration is the first method to jointly address explainability and sample efficiency. Our contribution is as follows:

1. We propose Beam Enumeration as a task-agnostic method to exhaustively enumerate subsequences and show that molecular substructures can be extracted. When coupled with RL
2. During the course of RL, extracted substructures provide structural insights and are on track to yield high rewards, which, in turn, enables self-conditioned molecular generation.
3. We perform exhaustive hyperparameter investigations (2,224 experiments and 144 with molecular docking) and provide insights on the predictable behavior of Beam Enumeration and recommend default hyperparameters for out-of-the-box applications.
4. We combine Beam Enumeration with the recently reported Augmented Memory  optimization algorithm and show that the sample efficiency becomes sufficient (up to a 29-fold increase on the most challenging task) to find high reward molecules that satisfy a docking objective with only 2,000 oracle calls in three drug discovery case studies.

## 2 Related Work

**Sample Efficiency in Molecular Design.** Tailored molecular generation is vital for practical applications as every use case requires optimizing for a bespoke property profile. Over the past several years, so-called goal-directed generation has been achieved using a variety of architectures, including Simplified molecular-input line-entry system (SMILES) -based recurrent neural networks (RNNs) [25; 26; 27; 28], generative adversarial networks (GANs) [29; 30; 31], variational autoencoders (VAEs) [17; 32; 33], graph-based models [34; 35; 36; 37], GFlowNets , and genetic algorithms . However, while all methods can be successful in optimizing for various properties, the oracle budget, i.e., how many oracle calls (computational calculations) were required to do so, is rarely reported. To address this, Gao et al.  proposed the Practical Molecular Optimization (PMO)  benchmark, which assesses 25 models across 23 tasks and enforces a budget of 10,000 oracle calls. Recently, Guo et al. proposed Augmented Memory , which uses a language-based molecular generative model and achieves the new state-of-the-art on the PMO benchmark.

**Explainability for Molecules.** Explainable AI (XAI)  to interpret and explain model predictions is a vital component for decision-making. Existing methods include Gradient-weighted Class Activation Mapping (Grad-CAM) , which uses gradient-based heat maps for convolutional layers and Local Interpretable Model-agnostic Explanations (LIME) , which uses a locally interpretable model. Other methods include permutation importance  and SHAP values , which are model-agnostic. For molecules, the Molecular Model Agnostic Counterfactual Explanations (MMACE)  method was proposed to search for the most similar counterfactual (model predicts the opposite label) molecule. Recently, the pBRICS  algorithm was proposed to combine functional group decomposition with Grad-CAM to explain matched molecular pairs. While existing XAI methods can work well provided a dataset, making the explanations actionable _during_ a generative design experiment that relies on an interplay between chemical space exploration and oracle feedback is difficult.

To address this limitation, we introduce _Beam Enumeration_, which extracts molecular substructures directly from the model's token sampling probabilities and derives explainability from a generative probabilistic perspective that is modulated by reward feedback. Moreover, when coupled with Augmented Memory , sample efficiency drastically improves.

## 3 Proposed Method: Beam Enumeration

In this section, each component of Beam Enumeration (Fig. 1) is described: the base molecular generative model, the Beam Enumeration algorithm, and how Beam Enumeration harnesses the model's built-in explainability which can be used to improve sample efficiency through self-conditioned generation (further details on Beam Enumeration are presented in Appendix A).

**Autoregressive Language-based Molecular Generative Model.** The starting point of Beam Enumeration is any autoregressive language-based molecular generative model. The specific model used in this work is Augmented Memory  which recently achieved the new state-of-the-art performance on the PMO  benchmark for sample efficiency, outperforming modern graph neural network-based approaches [48; 49] and GFlowNets . Augmented Memory builds on REINVENT [25; 51] which is a SMILES-based  RNN using long-short-term memory (LSTM) cells . The optimization process is cast as an on-policy RL problem. We define the state space, \(S_{t}\), as all intermediate token sequences and the action space, \(A_{t}(s_{t})\), as the token sampling probabilities (conditioned on a given a state). \(A_{t}(s_{t})\) is given by the policy, \(_{}\), which is parameterized by the RNN. The objective is to iteratively update the policy such that token sampling, \(A_{t}(s_{t})\), yields trajectories (SMILES) with increasing reward. Formally, sampling a SMILES, \(x\), is given by the product of conditional state probabilities (Equation 1), and the token sampling is Markovian:

\[P(x)=_{t=1}^{T}P(s_{t} s_{t-1},s_{t-2},,s_{1})\] (1)

Goal-directed generation proceeds by defining the Augmented Likelihood (Equation 2), where the Prior is the pre-trained model and \(S\) is the objective function returning a reward, given a SMILES, \(x\).

\[_{_{}}=_{_{}}+ S (x)\] (2)

The policy is directly optimized by minimizing the squared difference between the Augmented Likelihood and the Agent Likelihood given a sampled batch, \(B\), of SMILES constructed following the actions, \(a A^{*}\) (Equation 3):

Figure 1: Beam Enumeration overview. **a.** The proposed method proceeds via 4 steps: **1.** generate batch of molecules. **2.** filter molecules based on pool to enforce substructure presence, discarding the rest. **3.** compute reward **4.** update the model. After updating the model, if the reward has improved for consecutive epochs, execute Beam Enumeration. **b.** Beam Enumeration sequentially enumerates the top \(k\) tokens by probability for \(N\) beam steps, resulting in an exhaustive set of token sub-sequences. **c.** All valid substructures (either by the _Structure_ or _Scaffold_ criterion) are extracted from the sub-sequences. The most frequent substructures are used for self-conditioned generation.

\[L()=[_{a A^{*}}(_{}{}_{} -_{}{}_{})]^{2}\] (3)

Minimizing \(L()\) is equivalent to maximizing the expected reward as shown previously .

**Beam Enumeration.** Beam Enumeration is proposed based on the fact that on a successful optimization trajectory, it must become increasingly likely to generate high reward molecules. It is therefore reasonable to assume that the highest probability trajectories are more likely to yield high reward. Correspondingly, Beam Enumeration (Fig. 1) exhaustively enumerates the top \(k\) tokens (by probability) sequentially for \(N\) beam steps. Molecular substructures can be extracted from the set of _sub-sequences_, and we show how this information can be made _actionable_.

**Probabilistic Explainability.** Here, we describe how probabilistic explainability can be extracted from the exhaustive set of token sub-sequences. We hypothesized that molecular substructures can be extracted from a given sub-sequence by iteratively considering every (sub)-sub-sequence (Fig. 1). For example, given the sub-sequence "ABC", the set of (sub)-sub-sequences are: "A", "AB", and "ABC". It is expected that not every sub-sequence possesses (sub)-sub-sequences mapping to valid molecular substructures. Still, we show that a sufficient signal can be extracted (Appendix C). We implement two types of substructures: _Scaffold_, which extracts the Bemis-Murcko  scaffold and _Structure_, which extracts _any_ valid substructure. The closest work to ours is the application of Beam Search  for molecular design . Our work differs as the objective is to _exhaustively_ enumerate the highest probability _sub-sequences_ to extract molecular substructures for self-conditioned generation.

**Self-conditioned Generation.** The sub-sequences were enumerated by taking the most probable \(k\) tokens, and generating high reward molecules should be increasingly likely. Correspondingly, it is reasonable to posit that the most frequent molecular substructures are on track to becoming high reward full molecules and that the substructures themselves possess properties aligned with the target objective. The generative process can be self-conditioned to filter sampled batches for the presence of these molecular substructures and discard those that do not (Fig. 1).

**Sample Efficiency Metrics.** We define two metrics to assess sample efficiency: Generative Yield (referred to as Yield from now on) and Oracle Burden. Yield (Equation 4) is defined as the number of _unique_ generated molecules above a reward threshold, where \(g G\) are the molecules in the generated set, \(\) is the indicator function which returns 1 if the reward, \(R(g)\), is above a threshold, \(T\). Yield is a useful metric for drug discovery as the generated set is usually triaged to prioritize molecules, e.g., based on synthetic feasibility, for experimental validation or more expensive computational oracles.

\[Generative\ Yield=_{g=1}^{G}[R(g)>T]\] (4)

Oracle Burden (Equation 5) is defined as the number of oracle calls (\(c\)) required to generate \(N\)_unique_ molecules above a reward threshold. This is a direct measure of sample efficiency as high reward molecules satisfy the target objective, and the metric becomes increasingly important with expensive high-fidelity oracles.

\[Oracle\ Burden=c_{g=1}^{G}[R(g)>T]=N\] (5)

## 4 Results and Discussion

We first design an illustrative experiment to demonstrate the feasibility of Beam Enumeration to extract meaningful substructures and, in turn, enable self-conditioned generation. Next, three drug discovery case studies to design prospective inhibitors were performed to demonstrate real-worldapplication. The key result we convey is that Beam Enumeration can be added directly to existing algorithms, and it both provides structural insights and improves sample efficiency to not only generate more high reward molecules, but also faster, given a fixed oracle budget.

### Illustrative Experiment

**Extracted Substructures are Meaningful.** The illustrative experiment aims to optimize the following multi-parameter optimization (MPO) objective: maximize topological polar surface area (tPSA), molecular weight (MW) < 350 Da, and number of rings \(\) 2. This specific MPO was chosen because satisfying the objective _requires_ generating rings saturated with heteroatoms. Augmented Memory  was used to optimize the MPO objective. The reward trajectory tends towards 1, indicating the model gradually learns to satisfy the target objective, as desired (Fig. 2) Next, we investigate the top \(k\) and \(N\) beam steps parameters for Beam Enumeration and show that while the majority of sub-sequences do not possess valid substructures, a meaningful signal can still be extracted (Appendix C). We hypothesize that the optimal parameters are using a low top \(k\) as we are interested in the most probable sub-sequences and large \(N\) beam steps, which would enable extracting larger (and potentially more meaningful) substructures. Fig. 2 shows the top-4 substructures from Beam Enumeration at varying epochs. The substructures are informative when considering the MPO objective: the most frequent substructures gradually become rings saturated with heteroatoms, which possess a high tPSA.

  
**Metric** &  \\  & **Beam Scaffold 15** & **Beam Structure 15** & **Beam Scaffold** & **Beam Structure** & **Baseline** \\  Generative Yield\({}_{0.0}\) (\(\)) & **1757 \(\) 305** & \(1669 389\) & \(1117 278\) & \(864 202\) & \(496 108\) \\ Generative Yield\({}_{0.08}\) (\(\)) & **819 \(\) 291** & \(700 389\) & \(425 256\) & \(199 122\) & \(85 56\) \\ Oracle Burden\({}_{0.05}\) (\(\)) & **577 \(\) 310** & \(616 230\) & \(1037 414\) & \(897 347\) & \(1085 483\) \\ Oracle Burden\({}_{0.07}\) (\(\)) & 947 \(\) 350 & **926 \(\) 332** & \(1881 259\) & \(1745 292\) & \(2392 216\) \\ Oracle Burden\({}_{0.05}\) (100) (\(\)) & **1530 \(\) 468** & \(1547 513\) & \(2736 335\) & \(2713 402\) & \(3672 197\) \\ Oracle Burden\({}_{0.08}\) (1) (\(\)) & **1311 \(\) 628** & \(1401 695\) & \(2423 487\) & \(2295 482\) & \(3164 492\) \\ Oracle Burden\({}_{0.08}\) (100) (\(\)) & **1794 \(\) 617 (1)** & \(2009 804\) (1) & \(3124 497\) & \(3241 492\) & \(4146 326\) \\ Oracle Burden\({}_{0.08}\) (100) (\(\)) & **2704 \(\) 689 (1)** & \(2943 811\) (6) & \(3973 592\) (6) & \(4415 437\) (20) & \(4827 170\) (69) \\   

Table 1: Illustrative experiment: Beam Enumeration improves the sample efficiency of Augmented Memory. All experiments were run for 100 replicates with an oracle budget of 5,000 calls, and reported values are the mean and standard deviation. _Scaffold_ and _Structure_ indicate the type of substructure, and the number after is the _Structure Minimum Size_. Parentheses after Oracle Burden denote the cut-off number of molecules. Parentheses after values represent the number of unsuccessful replicates (for achieving the metric).

Figure 2: Illustrative experiment with the following multi-parameter optimization objective: maximize tPSA, molecular weight < 350 Da, number of rings \(\) 2. **a**. Augmented Memory  reward trajectory with annotated top-4 (excluding benzene) most frequent molecular substructure scaffolds at varying epochs using Beam Enumeration. **b.** Examples of molecules with high reward.

Self-conditioned Generation Improves Sample Efficiency.Thus far, the results only show that Beam Enumeration can extract meaningful molecular substructures. To enable self-conditioned generation, we consider _when_ extracted substructures would be meaningful and propose to execute Beam Enumeration when the reward improves for _Patience_ number of successive epochs (to mitigate sampling stochasticity). We combine Beam Enumeration with Augmented Memory and perform an exhaustive hyperparameter grid search (with replicates) using Yield and Oracle Burden as the performance metrics (Appendix A). The results elucidate the behavior of Beam Enumeration with three key observations: firstly, _Structure_ extraction is permissive compared to _Scaffold_ and often leads to small functional groups being the most frequent substructures which diminish the sample efficiency benefits (Appendix C). Secondly, enforcing larger substructures to be extracted (_Structure Minimum Size_) improves performance across all hyperparameter combinations. This reinforces that extracted substructures are meaningful as larger substructures heavily bias molecular generation during self-conditioning. If they were not meaningful, sample efficiency would not improve (and would likely be detrimental). Thirdly, _Structure_ extraction while enforcing a higher _Structure Minimum Size_ prevents small functional group extraction which significantly enhances performance. Subsequently, we perform five experiments (N=100 replicates each) based on the optimal hyperparameters identified: Augmented Memory (baseline) and Augmented Memory with Beam Enumeration (_Scaffold_ and _Structure Minimum Size_ = 15). Table 1 shows that Beam Enumeration drastically improves the Yield and Oracle Burden compared to the baseline at both the > 0.7 and > 0.8 reward thresholds, especially when _Structure Minimum Size_ = 15 is enforced. We highlight that the improved sample efficiency is significant as baseline Augmented Memory could not find 100 molecules > 0.8 reward in 69/100 replicates.

### Drug Discovery Case Studies

Next, we apply Beam Enumeration to drug discovery case studies to design inhibitors against DRD2 which is implicated in neurodegenerative diseases, MK2 kinase which is involved in proinflammatory responses, and AChE which is a target of interest against Alzheimer's disease. Following Guo et al., we formulate the following MPO objective: minimize the AutoDock Vina docking score, maximize the QED score, and MW < 500 Da. The QED and MW objectives prevent the generative model from exploiting the weaknesses of docking algorithms to give inflated docking scores to large, lipophilic molecules, which can be promiscuous binders. Moreover, an oracle budget of 5,000 Vina calls was enforced which is almost half the budget of the original Augmented Memory work (9,600). Since the observations made from the previous hyperparameter grid search may not be generalizable to docking tasks, we perform an additional hyperparameter grid search (with replicates). The results (Appendix D) show that the optimal hyperparameters across all drug discovery case studies are the same as the illustrative experiment. We designate these the default hyperparameters and demonstrate the applicability of Beam Enumeration to both Augmented Memory and REINVENT which is the second most (behind Augmented Memory) sample efficient model in the PMO benchmark.

Qualitative Inspection: Explainability.We first show that Augmented Memory with Beam Enumeration generates molecules that satisfy the MPO objective (Fig. 3). We emphasize that results were not cherry-picked and the three generated examples shown are the top 1 (by reward) across triplicate experiments. All molecules possess better Vina scores and higher QED than the reference molecules, as desired. Fig. 3 shows the highlighted substructures extracted using _Structure_ extraction with _Structure Minimum Size_ = 15 with three key observations: firstly, "uncommon" molecular substructures may be extracted such as the bridged cycle against DRD2. The exact substructure extracted was an amide bond with a long carbon chain which implicitly enforces the bridged cycle, and the Vina pose shows that it fits in the binding cavity with no clashes, despite being a bulky group. Secondly, bicylic or double-ring systems are often extracted, forming central scaffolds of the full molecule. Thirdly, scaffolds with branch points, i.e., a central ring with single carbon bond extensions, are often extracted. These substructures are particularly interesting as they heavily bias what can be generated in the remaining portion of the full molecule. An exemplary example of this is in the first generated molecule against MK2, where the branch points are effectively a part of two other ring systems (Fig. 3). Beam Enumeration can provide insights into the tolerability and suitability of certain substructures in the context of the full molecules (see Appendix D for more examples of substructures). Overall, the extracted substructures are meaningful and act both as a source of generative explainability and can self-direct the generative model into specific regions of chemical space with high reward.

**Quantitative Analysis: Sample Efficiency.** Next, we reinforce results from previous work showing that Augmented Memory is significantly more sample efficient than REINVENT (Table 2). Notably, the Yield of Augmented Memory is much greater than REINVENT at both the > 0.7 and > 0.8 reward thresholds, indicating that more high reward molecules are generated. Moreover, Augmented Memory has a lower Oracle Burden than REINVENT in all cases, except for Oracle Burden\({}_{>0.8}\) (1) for DRD2 and AChE where there is essentially no difference. The reason for this is because molecules with > 0.8 reward were already generated at epoch 1, indicating the pre-trained model (trained on ChEMBL) is a good Prior for these case studies. By contrast, the MK2 case study is considerably more challenging as extremely few > 0.8 reward molecules are generated under a 5,000 oracle calls budget. Augmented Memory significantly outperforms REINVENT as the latter could not find 10 molecules with reward > 0.8 (Table 2).

Subsequently, we demonstrate that Beam Enumeration can be applied out-of-the-box on top of Augmented Memory and REINVENT. Firstly, the addition of Beam Enumeration improves the sample efficiency of both base algorithms, as evidenced by the Yield and Oracle Burden metrics in Table 2 with a small trade-off in diversity (Appendix D). The benefits are more pronounced in Augmented Memory as observed by the Yield\({}_{>0.8}\) improving by > 4x in all cases (MK2 improves by 29x) and the Oracle Burden \({}_{>0.8}\) (10 and 100) over halved in most cases. Notably, for MK2

Figure 3: Three drug discovery case studies showing the top generated molecule (triplicate experiments) using Augmented Memory with Beam Enumeration _Structure_ Minimum Structure Size = 15 and the reference ligand. Extracted substructures from Beam Enumeration are highlighted. The multi-parameter optimization objective is: Minimize Vina score, maximize QED, and molecular weight < 500 Da. **a.** Dopamine type 2 receptor. **b.** MK2 kinase. **c.** Acetylcholinesteraseerase.

Oracle Burden >0.8 (100), baseline Augmented Memory could not accomplish the task while Beam Enumeration is successful in almost under 2,000 oracle calls (Table 2). We further verify that a large number of unique scaffolds are generated despite the Beam Enumeration bias (Appendix D), demonstrating that the combined algorithm with Augmented Memory achieves both exploration and exploitation. Overall, the results show that Beam Enumeration is task-agnostic and can be applied on top of existing algorithms to improve sample efficiency. The combined algorithm generates more high reward molecules and faster, even in challenging (MK2) scenarios under a limited oracle budget. Furthermore, in reference to all the Oracle Burden metrics (Table 2), Augmented Memory with Beam Enumeration can identify a small set of _excellent_ (high reward) candidate molecules in under 2,000 oracle calls and in some cases, even under 1,000 oracle calls.

## 5 Conclusion

In this work, we propose Beam Enumeration to exhaustively enumerate sub-sequences from a language-based molecular generative model based on the top \(k\) most probable tokens and for \(N\) beam steps. We show that molecular substructures can be extracted from the sub-sequences, which enables self-conditioned generation by only evaluating (by the oracle) molecules possessing these substructures and discarding the rest. We show that Beam Enumeration can be coupled with existing RL-based algorithms including Augmented Memory  and REINVENT . In three drug discovery case studies involving docking, the addition of Beam Enumeration improves sample efficiency as assessed by the Yield and Oracle Burden metrics with a small trade-off in diversity (which is expected). The extracted substructures themselves provide valuable structural insights, often enforcing the generation of specific cyclic systems and scaffolds with branch points which impose an overall molecular geometry, thus serving as a source of explainability. Beam Enumeration is the first proposed method to jointly address explainability and sample efficiency in molecular generative models. The improvements in the latter will enable more expensive high-fidelity oracles to be explicitly optimized. We note, however, that sparse reward environments  remain a difficult optimization task. Finally, Beam Enumeration is a task-agnostic method and can be combined with recent work integrating active learning with molecular generation to further improve sample efficiency . If the benefits can be synergistic, we may approach sufficient sample efficiency to directly optimize expensive state-of-the-art (in predictive accuracy) physics-based oracles such as MD simulations . Excitingly, this would in turn enhance explainability as high-fidelity oracles are inherently more informative.

  
**Metric** & **Target** &  &  \\   & &  &  &  &  &  &  \\  & &  &  &  &  \\  _{,}\)(1)} & DRD2 & **3474 \(\) 158** & 3412 \(\) 29 & 2513 \(\) 442 & 2392 \(\) 699 & 2686 \(\) 215 & 1879 \(\) 16 \\  & MC2 & **3127 \(\) 138** & 2584 \(\) 443 & 1446 \(\) 173 & 1822 \(\) 444 & 1553 \(\) 391 & 879 \(\) 10 \\  & AChE & 3324 \(\) 162 & **3902 \(\) 189** & 3288 \(\) 85 & 2511 \(\) 369 & 2684 \(\) 242 & 2437 \(\) 53 \\  _{,}\)(1)} & DRD2 & **1780 \(\) 439** & 1607 \(\) 379 & 363 \(\) 195 & 417 \(\) 275 & 687 \(\) 366 & 102 \(\) 6 \\  & MC2 & **987 \(\) 211** & 523 \(\) 438 & 34 \(\) 13 & 179 \(\) 241 & 19 \(\) 7 & 2 \(\) 0 \\  & AChE & 2095 \(\) 377 & **2124 \(\) 326** & 556 \(\) 47 & 323 \(\) 58 & 310 \(\) 207 & 147 \(\) 11 \\  _{,}\)(1)} & DRD2 & 126 \(\) 90 & 832 \(\) 29 & 187 \(\) 51 & 63 \(\) 0 & 127 \(\) 52 & 168 \(\) 149 \\  & MEA & **736 \(\) 166** & 1221 \(\) 564 & 1360 \(\) 543 & 1110 \(\) 268 & 808 \(\) 524 & 1724 \(\) 802 \\  & AChE & 105 \(\) 29 & 63 \(\) 0 & 62 \(\) 0 & 62 \(\) 0 & 84 \(\) 29 & 83 \(\) 29 \\  _{,}\)(1)} & DRD2 & 582 \(\) 83 & **571 \(\) 104** & 711 \(\) 1209 & 1099 \(\) 900 & 604 \(\) 71 & 883 \(\) 105 \\  & MEA & **1122 \(\) 154** & 2426 \(\) 1525 & 383 \(\) 394 & 1778 \(\) 0\({}^{**}\) & 3891 \(\) 631 & Failed \\   & AChE & **466 \(\) 25** & 418 \(\) 27 & **380 \(\)** 441 & 132 \(\) 1241 \(\) 120 & 481 \(\) 108 \\  _{,}\)(1)} & DRD2 & **1120 \(\) 194** & 1065 \(\) 146 & 2558 \(\) 30\({}^{**}\) & 1928 \(\) 117 & 1209 \(\) 1000 & 4595 \(\) 0\({}^{**}\) \\   & MEA & **2189 \(\) 181** & 2676 \(\) 403 & Paired & 3208 \(\) 0\({}^{**}\) & Failed & Failed \\   & AChE & **1110 \(\) 265** & **884 \(\) 162** & 2021 \(\) 89 & 3073 \(\) 427 & 3596 \(\) 678 & 3931 \(\) 236 \\   

Table 2: Drug discovery case studies: Beam Enumeration improves sample efficiency. All experiments were run in triplicate with an oracle budget of 5,000 calls and reported values are the mean and standard deviation. _Scaffold_ and _Structure_ indicate the type of substructure (_Structure Minimum Size_ = 15) extracted. The Generative Yield and Oracle Burden are reported at varying reward thresholds. Parentheses after Oracle Burden denote the cut-off number of molecules. Best performance is bolded with the exception of Oracle Burden (1) (DRD2/AChE) which have essentially identical performance due to the pre-trained model. * and ** denote one and two replicates were unsuccessful, respectively.

The Appendix contains further experiments, ablation studies, experiment hyperparameters, and algorithmic details.

## Appendix A Beam Enumeration

This section contains full details on Beam Enumeration including hyperparameters, design decisions, and pseudo-code.

### Algorithm Overview

Beam Enumeration (Fig. A4) is an algorithm that extracts molecular substructures from a generative model's weights for self-conditioned generation. The problem set-up is any molecular design task to optimize for a target property profile, e.g., high predicted solubility and binding affinity. When molecular generative models are coupled with an optimization algorithm, it should be increasingly likely to generate desirable molecules, i.e., molecules that possess the target property profile.

Beam Enumeration is proposed based on two facts:

1. On a successful optimization trajectory, the model's weights must change such that desirable molecules are more likely to be generated, on average.
2. The act of generating molecules in an autoregressive manner involves sequentially sampling from conditional probability distributions.

In this work, Beam Enumeration is applied to a language-based autoregressive generative model operating on the simplified molecular-input line-entry system (SMILES)  representation. The optimization algorithm is Augmented Memory  which builds on REINVENT [25; 51] and casts the optimization process as an on-policy reinforcement learning (RL) problem. Following RL terminology, sampling from the generative model involves sampling _trajectories_, which in this case, are SMILES, and the desirability of the corresponding molecule is given by the _reward_.

The underlying hypothesis of Beam Enumeration is that during the RL optimization process, _partial trajectories_ provide a source of signal that can be exploited. Usually, full trajectories are sampled which map to a complete SMILES sequence that can be translated to a molecule. Our assumption is that partial trajectories (partial SMILES sequence) can be mapped to molecular substructures (a part of the full molecule). This statement is not guaranteed as SMILES and molecules are discrete and small perturbations often leads to invalid SMILES. We prove this assumption in Section C by showing that although the vast number of partial trajectories do not map to valid SMILES, the raw number is sufficient to extract a meaningful signal. Correspondingly, Beam Enumeration leverages partial trajectories on the assumption that molecular substructures are _on track_ to becoming full molecules that _would_ receive high reward.

### Enumerating Partial Trajectories

In order to extract molecular substructures, a set of partial trajectories must be sampled from the generative model. Recalling the fact that on a successful optimization trajectory, it must become increasingly likely to generate desirable molecules, partial trajectories are sampled by enumerating the top \(k\) tokens, based on the conditional probability. Therefore, the process of enumerating partial trajectories involves sequentially extending each token sequence by their next top \(k\) probable tokens, resulting in the total number of partial trajectories as \(2^{N}\) where \(N\) is the number of beam steps, i.e., how many tokens in the partial trajectory. We note that taking the top \(k\) most probable tokens does not guarantee that the partial trajectories are indeed the most probable, as paying a probability penalty early can lead to higher probabilities later. However, our assumption is that on average, this leads to a set of partial trajectories that are at the very least, amongst the most probable. Moreover, there is a practical limit to how many partial trajectories are sampled due to exponential growth which makes scaling quickly computationally prohibitive. In the later section, we discuss this thoroughly. Finally, from here, partial trajectories will be referred to as token sub-sequences.

### Extracting Molecular Substructures

Given a set of token sub-sequences, the goal is to extract out the most frequent molecular substructures. This is done by taking each sequence, considering every (sub)-sub-sequence, and counting the number of valid substructures (Fig. A). For example, given the sub-sequence "ABCD", the set of (sub)-sub-sequences are: "A", "AB", "ABC", and "ABCD". In practice, we only consider (sub)-sub-sequences with at least three characters ("ABC" and "ABCD") since each character loosely maps to one atom and three is approximately the minimum for meaningful functional groups, e.g., "C=O", a carbonyl. The set of most frequent substructures is assumed to be on track to receive a high reward.

### Defining Molecular Substructures: _Scaffold_ vs. _Structure_

As shown in Fig. A, molecular substructures can be defined on the _Scaffold_ or _Structure_ level. The former extracts the Bemis-Murcko  scaffold while the latter extracts _any_ valid structure. The _any_ valid structure is an important distinction as our experiments find that extracting by _Structure_ leads to the most frequent molecular substructures being small functional groups that do not have corresponding scaffolds. By contrast, extracting the scaffold always leads to ring structures. Moreover, extracting specifically the Bemis-Murcko scaffold is important as heavy atoms, e.g., nitrogen, are important for biological activity. Consequently, extracted substructures are also enforced to contain at least one heavy atom as we find that benzene, perhaps unsurprisingly, is commonly the most frequent substructure. See Section B for more details on the differing behavior of 'Scaffold' vs. 'Structure'.

### Self-conditioned Generation

Self-conditioned generation is achieved by filtering sampled batches of molecules from the generative model to only keep the ones that possess at least one of the most frequent substructures. The effect is that the generative process is self-biased to focus on a narrower chemical space which we show can drastically improve sample efficiency at the expense of some diversity, which is acceptable when expensive high-fidelity oracles are used: we want to identify a small set of _excellent_ candidate molecules under minimal oracle calls.

### Probabilistic Explainability

The set of most frequent molecular substructures should be meaningful as otherwise, the model's weights would not have been updated such that these substructures have become increasingly likely to be generated. We verify this statement in the illustrative experiment in the main text and in Section C. In the drug discovery case studies (Appendix D), the extracted substructures are more subtle in why they satisfy the target objective but certainly must possess meaning, however subtle, as otherwise, they would not receive a high reward. In the main text, we show that extracted substructures form core scaffolds and structural motifs in the generated molecules that complement the protein binding cavity. Finally, we emphasize that the _correctness_ and _usefulness_ of this explainability deeply depends on the oracle(s) being optimized for. The extracted substructures do not explain why the generated molecules satisfy the target objective. Rather, they explain why the generated molecules satisfy the oracle. The assumption in a generative design task is that optimizing the oracle is a good proxy for the target objective, e.g., generating molecules that dock well increases the likelihood of the molecules being true binders. This observation directly provides additional commentary on why sample efficiency is so important: the ability to directly optimize expensive high-fidelity oracles would inherently enhance the _correctness_ of the extracted substructures.

## Appendix B Beam Enumeration: Findings from Hyperparameter Screening

In this section, we introduce all seven hyperparameters of Beam Enumeration and then present results on an exhaustive hyperparameter search which elucidates the behavior and interactions of all the hyperparameters. In the end, we present our analyses and provide hyperparameter recommendations for Beam Enumeration which can serve as default values to promote out-of-the-box application.

### Beam Enumeration Hyperparameters

**Beam k.** This hyperparameter denotes how many tokens to enumerate at each step. Given that our hypothesis is that the most probable sub-sequences yield meaningful substructures, we fix Beam \(k\) to 2. A larger value would also decrease the number of Beam Steps possible as the total number of sub-sequences is \(k^{N}\) and the exponential growth quickly leads to computational infeasibility.

**Beam Steps N.** This hyperparameter denotes how many token enumeration steps to execute and is the final token length of the enumerate sub-sequences. This parameter leads to exponential growth in the number of sequences which can quickly become computationally prohibitive. An important implication of this hyperparameter is that larger Beam Steps means that larger substructures _can_ be extracted. In our experiments, we find that enforcing size in the extracted substructures can drastically improve sample efficiency with decreased diversity as the trade-off. We thoroughly discuss this in a later sub-section. Finally, in our experiments, the upper-limit investigated is 18 Beam Steps.

**Substructure Type.** This hyperparameter has two possible values: _Scaffold_ or _Structure_. _Scaffold_ extracts Bemis-Murcko  scaffolds while _Structure_ extracts _any_ valid substructure.

**Structure Structure Minimum Size.** This hyperparameter enforces the partial SMILES to contain at least a certain number of characters. In effect, this enforces extracted molecular substructures to be larger than a Structure Minimum Size. From the illustrative experiment in the main text and Section C, _Structure_ extraction often leads to small functional groups being the most frequent in thesub-sequences. By enforcing a minimum structure size, _Structure_ extraction leads to partial structures which may carry more meaning. We find that this hyperparameter greatly impacts sample efficiency and we present all our findings in a later sub-section.

**Pool Size.** This hyperparameter controls how many molecular substructures to keep track of. These _pooled_ substructures are what is used to perform self-conditioning. The hypothesis is that the most frequent ones carry the most meaning and thus, a very large pool size may not be desired.

**Patience.** This hyperparameter controls how many successive reward improvements are required before Beam Enumeration executes and molecular substructures are extracted. Recalling the first fact in which Beam Enumeration was proposed on: On a successful optimization trajectory, the model's weights must change such that desirable molecules are more likely to be generated, on average. Patience is effectively an answer to "when would extracted substructures be meaningful?" Too low a patience and stochasticity can lead to negative effects while too high a patience diminishes the benefits of Beam Enumeration on sample efficiency.

**Token Sampling Method.** This hyperparameter has two possible values: "topk" or "sample" and denotes _how_ tokens sub-sequences are enumerated. "topk" takes the top k most probable tokens at each Beam Step while "sample" samples from the distribution just like during batch generation. Our results show interesting observations surrounding this hyperparameter as "sample" can work just as well and _sometimes_ even better than taking the "topk". These results were unexpected as the underlying hypothesis is that the most probable sub-sequences lead to the most useful substructures being extracted. However, our findings are not in contradiction as sampling the conditional probability distributions would still lead to sampling the top k tokens, on average. Moreover, after extensive experiments, we find that "sample" leads to more variance in performance across replicates which is in agreement with the assumption that sampling the distributions can lead to more improbable structures. We thoroughly discuss our findings in a later sub-section where we provide hyperparameter recommendations and analyses to the effects of tuning each hyperparameter.

### Hyperparameters: Grid Search

We performed two exhaustive hyperparameter grid searches on the illustrative experiment which has the following multi-parameter optimization (MPO) objective: maximize topological polar surface area (tPSA), molecular weight < 350 Da, number of rings \(\) 2 with an oracle budget of 5,000. The first grid search investigated the following hyperparameter combinations:

* Beam K = 2
* Beam Steps = 
* Substructure Type = [_Scaffold_, _Structure_]
* Pool Size = 
* Patience = 
* Token Sampling Method = ['topk','sample']

**All hyperparameter combinations (144) were tried and run for 10 replicates each for statistical reproducibility, total of 1,440 experiments.** Next, an additional grid search was performed with the following hyperparameter combinations:

* Beam K = 2
* Beam Steps = 
* Substructure Type = [_Scaffold_, _Structure_]
* Structure Structure Minimum Size = 
* Pool Size = 
* Patience = * Token Sampling Method = ['topk','sample']

We take the general trends from the first grid search and narrow down the most optimal hyperparameters to further investigate Substructure Type and structure Structure Minimum Size. **As from before, all hyperparameter combinations (64) were tried and run for 10 replicates each for statistical reproducibility, total of 640 experiments.**

The following heatmaps performance by the Generative Yield and Oracle Burden (10) metrics at the > 0.8 reward threshold and under a 5,000 oracle budget. The Generative Yield measures how many unique molecules above 0.8 reward were generated. The Oracle Burden (10) measures how few oracle calls were required to generate 10 molecules above 0.8 reward. We note that all Oracle Burden metrics are computed by not allowing more than 10 molecules to possess the same Bemis-Murcko  scaffold, thus also explicitly considering diversity in the generated set.

### Analysis of Grid Search Results

In this section, we summarize our analysis on the grid search experiments. Unless stated, each bullet point means the observation was observed for both Generative Yield and Oracle Burden (10). For example the point: _Scaffold > Structure_ means _Scaffold_ is generally more performant than _Structure_ across all hyperparameters on both the Generative Yield and Oracle Burden (10).

* For _Scaffold_, higher Pool, higher Patience, and higher Beam Steps improves performance
* For _Structure_, lower pool and lower patience improves performance
* _Scaffold > Structure_
* _Scaffold_ and _Structure_ become more performant with increasing Structure Minimum Size
* _Scaffold_ and _Structure_ with Structure Minimum Size: "sample" sampling _can_ be better than "topk" sampling but with more variance

Based on the above analysis, we propose the optimal hyperparameters for the illustrative experiment as:

* _Scaffold_
* "topk" sampling ("sample" sampling can be more performant but exhibits higher variance)
* Patience = 5
* Pool Size = 4
* Beam Steps = 18

Finally, we provide more commentary on interesting observations from the grid search results. _Structure_ without Structure Minimum Size enforcing often leads to small functional groups being the most frequent molecular substructures extracted with Beam Enumeration. Enforcing Structure Minimum Size puts it almost on par with _Scaffold_, suggesting (perhaps not surprisingly) that larger substructures can carry more meaningful information. Moreover, when using "sample" sampling, the generative model undergoes more "filter rounds". Specifically, at each epoch, the sampled batch is filtered to contain the extracted substructures. When using "sample" sampling, the model is more prone to some epochs containing no molecules with the substructures. In practice, this is inconsequential as sampling is computationally inexpensive and a next batch of molecules can easily be sampled. However, specifically in the _Structure_ with "sample" sampling and Structure Minimum Size = 15 experiment, "filter round" can be quite extensive, taking up to 100,000 epochs (maximum observed) for an oracle budget of 5,000 (adding about an hour to the wall time which is minor when the oracle is expensive). This means that many epochs contained molecules without the extracted substructures. There are two observations here: firstly, "sample" sampling can lead to more improbable substructures which are hence less likely to be sampled and secondly, _Structure_ with Structure Minimum Size enforcement leads to extreme biasing (which improves sample efficiency).

Figure B5: illustrative experiment Generative Yield > 0.8. The IntDiv168 is annotated.

Figure 6: illustrative experiment Generative Yield > 0.8 with Structure Minimum Size. The IntDiv168 is annotated.

Figure B7: illustrative experiment Generative Yield > 0.8 with Structure Minimum Size and "Sample" token sampling. The IntDiv1  is annotated.

Figure B8: illustrative experiment Oracle Burden (10) > 0.8

Figure B9: illustrative experiment Oracle Burden (10) > 0.8 with Structure Minimum Size

Figure 10: illustrative experiment Oracle Burden (10) > 0.8 with Structure Minimum Size and ”Sample” token sampling

We believe the remarkable tolerability of the generative model sampling to such bias is an interesting observation. By contrast, _Scaffold_ with Structure Minimum Size enforcement is not as prone to "filter rounds" because _Scaffold_ "truncates" the substructure to its central shape (scaffold). For example, toluene (benzene with a methyl group) has a Bemis-Murcko  scaffold of just benzene. The consequence is that _Structure_ leads to more extreme biasing (it is more likely for a molecule to contain benzene than specifically toluene) which is in agreement with the general observation that the diversity of the generated set decreases when using _Structure_. Overall, both _Scaffold_ and _Structure_ with Structure Minimum Size enforcing exhibits the best performance and "sample" sampling _can_ be more performant than "topk" sampling but exhibits notably higher variance.

The set of optimal hyperparameters found here were used in drug discovery case studies. In order to be rigorous with our investigation, we only fix the following hyperparameters:

* Patience = 5 (lower variance)
* Pool Size = 4 (lower variance, higher Yield, lower Oracle Burden)
* Beam Steps = 18 (lower variance, higher Yield, lower Oracle Burden)

with these hyperparameters, we do a small grid search (on the drug discovery case studies) by changing the Structure Type, Token Sampling Method, and Structure Minimum Size hyperparameters as the optimal hyperparameters in the illustrative experiment are not necessarily the optimal ones in the drug discovery experiments. **The purpose of this is not to necessarily report the best performance on the drug discovery case studies but to gain insights into the optimal general parameters such that Beam Enumeration can be used out-of-the-box. In real-world expensive oracle settings, tuning hyperparameters is infeasible.**

All results from the drug discovery case studies are shown in Section D.

### Beam Enumeration: Recommended default Hyperparameters

Taking into consideration all grid search experiments for the illustrative experiment and Drug Discovery case studies, the following optimal hyperparameters are recommended: Patience = 5, Pool Size = 4, Beam Steps = 18, _Structure_, Structure Minimum Size = 15, "topk" sampling.

Notable differences between the final recommended hyperparameters compared to those found from the illustrative experiment is that _Structure_ and "topk" sampling are more performant than _Scaffold_ and "sample" sampling. In the illustrative experiment, "sample" sampling was sometimes more performant than "topk" sampling. We rationalize these observations as follows: in MPO objectives that include physics-based oracles, structure specificity becomes increasingly important, e.g., specific chemical motifs dock well because they form interactions with the protein. Therefore, "topk" sampling is more robust as there is less variance in the extract substructures compared to "sample" sampling. We empirically observe the increased variance when using "sample" sampling measured by the standard deviation between replicate experiments (Appendix D). In the illustrative experiment where the oracle was more permissive, i.e., _any_ rings saturated with heteroatoms would satisfy the MPO objective, small deviations in the extracted structure do not have as prominent an effect as physics-based oracles which require specificity. Another observation is that _Structure_ sampling often extracts scaffolds with "branch points" which enforces extreme bias that can lead to more focused chemical space exploration. We discuss this in detail in Section D and believe the insights are generally interesting in the context of molecular optimization landscape.

**Finally, we end this section by stating that we cannot try every single hyperparameter combination and the recommended values are from our grid search results which we make an effort to be robust, given that we perform 10 replicates of each experiment. We find that the optimal hyperparameters in the drug discovery case studies are generally the same as in the illustrative experiment.**

### Pseudo-code

The pseudo-code for Beam Enumeration is presented here. The \(\) operator denotes every element on the left is being extended by every element on the right.

``` Input: Generative Agent \(_{_{}}}\), Top \(k\), \(N\) Beam Steps Output: Enumerated Token Sub-sequences \(S\) Initialization:  Hidden State = None;  Sub-sequences = [Top \(k\) <START> Tokens];  Input Vector = top \(k\) number of start tokens; for\(i=1\)to\(N\)do  Logits, New Hidden State \(_{_{}}}\)(Input Vector, Hidden State);  Tokens\({}_{K}\)\(\) top \(k\) tokens from Softmax(Logits); if\(i=1\)then  Sub-sequences \(\) Tokens\({}_{K}\);  Input Vector \(\) Tokens\({}_{K}\);  Hidden State = New Hidden State; else  Create empty list \(temp\); foreach\(seq\) in Sub-sequencesdo \(seq seq\) Tokens\({}_{K}\);  Append \(seq\) to \(temp\);  Sub-sequences \( temp\);  Clear \(temp\);  Input Vector \(\) Flatten Tokens\({}_{K}\);  Hidden State \(\) (New Hidden State\([i]\).repeat_interleave(top \(k\), dim \(=1\)))\({}_{i=0,1}\); returnSub-sequences ```

**Algorithm 1**Beam Enumeration

## Appendix C Illustrative Experiment

This section contains additional results from initial investigations into the feasibility of Beam Enumeration. The illustrative experiment was performed with the following multi-parameter optimization (MPO) objective: maximize topological polar surface area (tPSA), molecular weight (MW) < 350 Da, number of rings \(\) 2.

### Substructure Extraction

The first experiments investigated whether a sufficient substructures signal could be extracted from enumerated sub-sequences. The two parameters of Beam Enumeration (without self-conditioning) are top \(k\) denoting the top \(k\) number of highest probability tokens to enumerate and \(N\) number of beam steps denoting how many steps to perform token expansion for (which is also the length of the final sub-sequence). Our hypothesis is that a lower top \(k\) is desirable as we are interested in the most probably substructures. Thus, the initial experiments were a grid-search with a top \(k\) of 2 and \(N\) beam steps of . The illustrative experiment was run for 100 epochs (6,400 oracle calls which is different from the 5,000 used in the main text experiments as this set of results is only to demonstrate that meaningful substructures can be extracted) and Beam Enumeration was applied at epochs 1, 20, 40, 60, 80, and 100.

Table 3 shows the absolute counts and percentage of sub-sequences containing valid substructures. While the percentage may appear low, we note the absolute counts is more than enough to extract some notion of most probable substructures. We use \(N\) beam steps of 18 for all experiments as we hypothesize that larger substructures can carry more information. The reason the max beam steps investigated was 18 is because of the memory overhead required for sequence expansion.

### Extracted Substructures

To illustrate the capability of Beam Enumeration to extract meaningful substructures, Fig. C11 shows the top 5 most probable substructures at epochs 1, 20, 40, 60, 80, and 100 based on _Structure_ (extract any valid structure) and _Scaffold_ (extract valid Bemis-Murcko  scaffold) using a top \(k\) of 2 and 18 beam steps. We make two crucial observations here. Firstly, _Structure_ often extracts small functional groups which makes the self-conditioned filtering much more permissive as it is more likely for a molecule to possess a specific functional group than a specific scaffold. Secondly, benzene appears often and perhaps unsurprisingly as it is ubiquitous in nature. Based on these observations, we design

   \(N\) **Beam Steps** & **Epoch 1** & **Epoch 20** & **Epoch 40** & **Epoch 60** & **Epoch 80** & **Epoch 100** \\ 
15 & 2294/32768 & 3123/32768 & 584/332768 & 5538/32768 & 5674/32768 & 8004/32768 \\  & (7.00\%) & (9.53\%) & (17.83\%) & (16.90\%) & (17.32\%) & (24.43\%) \\
16 & 4789/65536 & 5890/65536 & 5771/65536 & 1159/65536 & 7657/65536 & 9771/65536 \\  & (7.31\%) & (8.99\%) & (8.81\%) & (17.03\%) & (11.68\%) & (14.91\%) \\
17 & 9998/13107 & 15266/131072 & 2616/131072 & 2432/311072 & 21424/31072 & 3116/131072 \\  & (7.63\%) & (11.65\%) & (19.96\%) & (18.58\%) & (16.36\%) & (23.77\%) \\
18 & 20747/262144 & 33969/262144 & 72126/262144 & 48417/262144 & 45349/262144 & 46994/262144 \\  & (7.91\%) & (12.96\%) & (27.51\%) & (18.47\%) & (17.30\%) & (17.93\%) \\   

Table 3: Feasibility of Beam Enumeration to extract valid substructures. Top-\(k\) = 2.

Beam Enumeration to only extract substructures containing at least one heteroatom on the assumption that heteroatoms are much more informative in forming polar interactions in drug molecules, e.g., a hydrogen-bond cannot form from benzene. Finally, the general observation is that the most probable substructures gradually contain more heteroatoms, as desired.

### Supplementary Main Text Results

In this section, we present the same table as the main text illustrative experiment. The only difference is that the IntDiv1  is also annotated in the table here to show that the sample efficiency improvements of Beam Enumeration come only at a small trade-off in diversity (Table 4). In agreement with our observations in the hyperparameters grid search (Appendix A), _Structure_ extraction with 'Structure Minimum Size' enforcement leads to highly specific substructures which decrease diversity relative to _Scaffold_ extraction but with potential gains in sample efficiency as evidenced in the drug discovery case studies (Appendix D). We further perform statistical testing using Welch's t-test to compare all metrics for _Scaffold_ with 'Structure Minimum Size' = 15 and Baseline Augmented Memory . For the experiments that had unsuccessful replicates, we use the total number of successful experiments, e.g., Oracle Burden\({}_{>10}\) (100), the Baseline was unsuccessful in 69/100 replicates so a 31 sample size was used. Overall, all p-values are significant at the 95% confidence level.

### Beam Enumeration works in Exploitation Scenarios

In the main text illustrative experiment, Augmented Memory  was used with Selective Memory Purge activated which is the mechanism to promote chemical space exploration, as described in the original work. For completeness, we show that Beam Enumeration also works in pure exploitation scenarios where the goal is only to generate high reward molecules even if the same molecule is repeatedly sampled (Table 5). We perform statistical testing using Welch's t-test to compare all metrics for _Scaffold_ with 'Structure Minimum Size' = 15 and Baseline Augmented Memory . For the experiments that had unsuccessful replicates, we use the total number of successful experiments, e.g., Oracle Burden\({}_{>10}\) (100), the Baseline was unsuccessful in 69/100 replicates so a 31 sample size was used. Overall, all p-values are significant at the 95% confidence level.

### Self-conditioned Filtering: _Structure_ vs _Scaffold_

There is a clear discrepancy in the substructures extracted by _Structure_ and _Scaffold_. In particular, _Structure_ substructures contain small functional groups which is much more permissive when used as a filter criterion compared to full scaffolds. Therefore, one would expect that many molecules in the

  
**Metric** &  &  \\  & **Beam Scaffold 15** & **Beam Structure 15** & **Beam Scaffold** & **Beam Structure** & **Baseline** & **p-value (N=100)** \\  Generative Yield\({}_{0,07}\) (\(\)) & **1757 \(\) 305** & 1669 \(\) 389 & 1117 \(\) 278 & 864 \(\) 202 & 496 \(\) 108 & 2.60 \( 10^{-75}\) \\ - Diversity & 0.77 \(\) 0.03 & 0.73 \(\) 0.04 & 0.79 \(\) 0.03 & 0.83 \(\) 0.03 & 0.85 \(\) 0.02 & \\ Generative Yield\({}_{0,08}\) (\(\)) & **819 \(\) 291** & 700 \(\) 389 & 425 \(\) 256 & 199 \(\) 122 & 85 \(\) 56 & 3.70 \( 10^{-48}\) \\ - Diversity & 0.73 \(\) 0.04 & 0.69 \(\) 0.05 & 0.75 \(\) 0.04 & 0.77 \(\) 0.04 & 0.78 \(\) 0.03 & \\ Oracle Burden\({}_{0,07}\) (1 \(\)) & **877 \(\) 390** & **616 \(\) 230** & 1037 \(\) 444 & 877 \(\) 347 & 1085 \(\) 483 & 3.06 \( 10^{-19}\) \\ Oracle Burden\({}_{0,07}\) (10 \(\)) & **947 \(\) 350** & **926 \(\) 332** & 1881 \(\) 259 & 1745 \(\) 292 & 2292 \(\) 126 & 4.99 \( 10^{-87}\) \\ Oracle Burden\({}_{0,07}\) (100 \(\)) & **1530 \(\) 468** & 1547 \(\) 513 & 2736 \(\) 335 & 2713 \(\) 402 & 3672 \(\) 197 & 2.34 \( 10^{-86}\) \\ Oracle Burden\({}_{0,08}\) (1 \(\)) & **1312 \(\) 628** & 1401 \(\) 695 & 2423 \(\) 487 & 2295 \(\) 482 & 3164 \(\) 492 & 6.07 \( 10^{-65}\) \\ Oracle Burden\({}_{0,08}\) (10 \(\)) & **1794 \(\) 417** (1) & 2009 \(\) 304 (1) & 3124 \(\) 497 & 3241 \(\) 492 & 4466 \(\) 326 & 6.48 \( 10^{-79}\) \\ Oracle Burden\({}_{0,08}\) (100 \(\)) & **2704 \(\) 699** (1) & 2943 \(\) 811 (6) & 3973 \(\) 592 (6) & 4415 \(\) 437 (20) & 4827 \(\) 170 (69) & 6.17 \( 10^{-21}\) \\   

Table 4: Illustrative experiment: Beam Enumeration improves the sample efficiency of Augmented Memory. All experiments were run for 100 replicates with an oracle budget of 5,000 calls and reported values are the mean and standard deviation. _Scaffold_ and _Structure_ indicate the type of substructure and the number after is the ’Structure Minimum Size’. Parentheses after Oracle Burden denote the cut-off number of molecules. Parentheses after values represent the number of unsuccessful replicates (for achieving the metric). The IntDiv1  is annotated under each Generative Yield. Welch’s t-test is used to compare the difference between _Scaffold_ with ’Structure Minimum Size’ = 15 and Baseline Augmented Memory . All p-values are significant.

sampled batches would be kept when using _Structure_ Beam Enum. We plot the average number of molecules kept out of 64 (batch size) across the generative run when using Beam Enumeration. Note that the experiments ran for variable epochs due to the stochasticity of Beam Enumeration self-filtering. The number of epochs shown in C12 is the minimum number of epochs out of 100 replicates. Therefore, the average values shown are averaged over 100 replicates. It is evident that _Structure_ is more lenient as many generated molecules make it through the filter compared to _Scaffold_ which maintains a relatively strict filter. One interesting observation is that self-conditioning does not lead to obvious mode collapse. Self-conditioning is inherently biased and one would be concerned that the model gets stuck at generating the same molecules repeatedly. The fact that self-conditioning with _Scaffold_ continues to filter throughout the entire generative run shows that the model is continually moving to new chemical space, supporting findings from the original Augmented Memory  work that Selective Memory Purge (built-in diversity mechanism) is capable of preventing mode collapse.

  
**Metric** &  &  \\  & **Beam Scaffold 15** & **Baseline** & **p-value (N=100)** \\  Generative Yield\({}_{0.07}\) (\(\)) & \(\) & \(496 108\) & 1.54 \( 10^{-29}\) \\ - Diversity & \(0.76 0.04\) & \(0.85 0.02\) & \\ Generative Yield\({}_{0.08}\) (\(\)) & \(\) & \(85 56\) & 1.35 \( 10^{-28}\) \\ - Diversity & \(0.70 0.09\) & \(0.78 0.03\) & \\ Oracle Burden\({}_{0.07}\) (1) (\(\)) & \(\) & \(1085 483\) & 4.52 \( 10^{-15}\) \\ Oracle Burden\({}_{0.07}\) (1) (\(\)) & \(\) & \(2392 216\) & 2.26 \( 10^{-80}\) \\ Oracle Burden\({}_{0.07}\) (100) (\(\)) & \(\) & \(3672 197\) & 4.01 \( 10^{-100}\) \\ Oracle Burden\({}_{0.08}\) (1) (\(\)) & \(\) & \(3164 492\) & 2.21 \( 10^{-53}\) \\ Oracle Burden\({}_{0.08}\) (10) (\(\)) & \(\) (\(\)) & \(4146 326\) & 1.14 \( 10^{-76}\) \\ Oracle Burden\({}_{0.08}\) (100) (\(\)) & \(\) (\(\)) & \(4827 170\) (69) & 1.68 \( 10^{-25}\) \\   

Table 5: Beam Enumeration works in exploitation scenarios. all experiments were run for 100 replicates with an oracle budget of 5,000 calls and reported values are the mean and standard deviation. Parentheses after Oracle Burden denote the cut-off number of molecules. The IntDiv1  is annotated under each Generative Yield. Welch’s t-test is used to compare the difference between _Scaffold_ with ’Structure Minimum Size’ = 15 and Baseline Augmented Memory . All p-values are significant.

Drug Discovery Case Studies

This section contains information on the Autodock Vina docking protocol from receptor grid preparation to docking execution. The Beam Enumeration hyperparameters grid search results are presented for all three drug discovery case studies followed by analysis. Examples of extracted substructures are also shown and commentary provided to their significance and explainability. Finally, the wall times of all experiments are presented.

### AutoDock Vina Receptor Preparation and Docking

All docking grids were prepared using DockStream which uses PDBFixer to refine receptor structures. The search box for all grids was 15A x 15A x 15A. Docking was also performed through DockStream and followed a two step process: conformer generation using the RDKit Universal Force Field (UFF) with the maximum convergence set to 600 iterations and then Vina docking was parallelized over 36 CPU cores (Intel(R) Xeon(R) Platinum 8360Y processors).

**DRD2 - Dopamine Type 2 Receptor.** The PDB ID is 6CM4 and the docking grid was centered at (x, y, z) = (9.93, 5.85, -9.58).

**MK2 - MK2 Kinase.** The PDB ID is 3KC3 and one monomer was extracted. The docking grid for the extracted monomer was centered at (x, y, z) = (-61.62, 30.31, -21.9).

**AChE - Acetylcholinesteraseerase.** The PDB ID is 1EVE and the docking grid was centered at (x, y, z) = (2.78, 64.38, 67.97).

### Beam Enumeration Hyperparameters Grid Search Results

We performed an additional hyperparameter grid search on all three drug discovery case studies based on the insights drawn from the illustrative experiment grid search results. We fix the following hyperparameters:

* Beam K = 2
* Beam Steps = 18
* Pool Size = 4
* Patience = 5

and vary the following:

* Optimization Algorithm = [Augmented Memory, REINVENT]
* Substructure Type = [_Scaffold_, _Structure_]
* Structure Minimum Size = 
* Token Sampling Method = ["topk", "sample"]

**All hyperparameter combinations (8) were tried and run for 3 replicates each for statistical reproducibility, total of 144 experiments.** There are two main results we want to convey: firstly, the optimal hyperparameters are the same for all three drug discovery case studies and only the Substructure Type differs between the optimal hyperparameters here and the illustrative experiment. Secondly, Beam Enumeration is a task-agnostic general method that can be applied to existing algorithms including Augmented Memory and REINVENT. At the end of this section, we present these hyperparameters and designate these the default values. All grid search results are now presented in following tables:

Based on the results from the hyperparameters grid search in the drug discovery case studies, we make two key observations: firstly, _Structure_ extraction with 'Structure Minimum Size' = 15 is now the most performant, on average (for both Augmented Memory and REINVENT). This is in contrast to _Scaffold_ extraction in the illustrative experiment which we rationalize through the permissive nature of the experiment compared to the docking experiments which require structure specificity. Previously, small deviations in the substructures may not have a significant impact on the reward. In physics-based oracles such as Vina docking used here, small substructure differences can have an enormous impact on the outcome since the pose requires specific complementary to the protein binding site. The second observation we make which is in agreement with the illustrative experiment is that "sample" token sampling has more variance and does not perform better than "topk". The rationale is the same in that docking requires specificity and lower probability substructures exhibit more variable performance. Based on all the observations from the illustrative experiment and the drug discovery case studies, we designate the following default hyperparameter values:

* Beam K = 2
* Beam Steps = 18

  
**Experiment** & **Generative** & **Unique** & **Oracle** & **Oracle** & **Oracle** \\
**Augmented Memory DRD2** & **Yield** & **Scaffolds** & **Burden (1)** & **Burden (10)** & **Burden (100)** \\  Baseline & \(363 195\) & \(322 166\) & \(187 51\) & \(711 120\) & \(2558 30^{*}\) \\ - Diversity & \(0.802 0.019\) & & & & \\  Scaffold & \(957 75\) & \(749 62\) & \(82 29\) & \(668 25\) & \(1818 107\) \\ - Diversity & \(0.765 0.006\) & & & & \\  Scaffold Size 15 & \(1607 379\) & \(1023 351\) & \(83 29\) & \(571 104\) & \(1056 146\) \\ - Diversity & \(0.724 0.027\) & & & & \\  Scaffold Sample & \(948 123\) & \(776 128\) & \(126 89\) & \(505 17\) & \(1746 20\) \\ - Diversity & \(0.734 0.018\) & & & & \\  Scaffold Sample Size 15 & \(1552 106\) & \(1274 154\) & \(84 29\) & \(598 110\) & \(1511 416\) \\ - Diversity & \(0.660 0.041\) & & & & \\  Structure & \(887 12\) & \(711 133\) & \(63 0\) & \(595 63\) & \(1862 154\) \\ - Diversity & \(0.764 0.008\) & & & & \\  Structure Size 15 & \(1780 439\) & \(1323 368\) & \(126 90\) & \(582 83\) & \(1120 194\) \\ - Diversity & \(0.699 0.020\) & & & & \\  Structure Sample & \(912 86\) & \(757 30\) & \(63 0\) & \(583 37\) & \(2132 148\) \\ - Diversity & \(0.767 0.015\) & & & & \\  Structure Sample Size 15 & \(752 105\) & \(1352 180\) & \(188 103\) & \(776 129\) & \(1289 193\) \\ - Diversity & \(0.641 0.059\) & & & & \\   

Table 6: DRD2 case study hyperparameters grid search results for Augmented Memory. All experiments were run in triplicate and the reported values are the mean and standard deviation. “Sample” denotes “sample” token sampling. All metrics are for the reward threshold > 0.8. The IntDiv1 is annotated under Generative Yield. * and ** denote one and two replicates were unsuccessful, respectively.

  
**Experiment** & **Generative** & **Unique** & **Oracle** & **Oracle** & **Oracle** \\
**REINVENT DRD2** & **Yield** & **Scaffolds** & **Burden (1)** & **Burden (10)** & **Burden (100)** \\  Baseline & \(102 6\) & \(101 6\) & \(168 149\) & \(883 105\) & \(4959 0^{**}\) \\ - Diversity & \(0.833 0.001\) & & & & \\  Scaffold & \(190 32\) & \(184 32\) & \(63 1\) & \(836 178\) & \(3516 575\) \\ - Diversity & \(0.814 0.007\) & & & & \\  Scaffold Size 15 & \(687 366\) & \(377 204\) & \(127 52\) & \(604 71\) & \(2109 1090\) \\ - Diversity & \(0.730 0.013\) & & & & \\  Scaffold Sample & \(176 86\) & \(149 49\) & \(105 59\) & \(720 121\) & \(3875 883\) \\ - Diversity & \(0.801 0.030\) & & & & \\  Scaffold Sample Size 15 & \(363 249\) & \(225 144\) & \(84 30\) & \(754 183\) & \(3170 1188\) \\ - Diversity & \(0.704 0.044\) & & & & \\  Structure & \(184 14\) & \(183 14\) & \(104 31\) & \(897 100\) & \(3426 282\) \\ - Diversity & \(0.817 0.006\) & & & & \\  Structure Size 15 & \(417 275\) & \(290 178\) & \(63 0\) & \(1099 930\) & \(1928 117^{*}\) \\ - Diversity & \(0.730 0.014\) & & & & \\  Structure Sample & \(169 24\) & \(167 24\) & \(126 52\) & \(711 179\) & \(3568 440\) \\ - Diversity & \(0.826 0.003\) & & & & \\  Structure Sample Size 15 & \(261 225\) & \(182 132\) & \(209 128\) & \(840 107\) & \(3690 1266^{*}\) \\ - Diversity & \(0.734 0.057\) & & & & \\   

Table 7: DRD2 case study hyperparameters grid search results for REINVENT. All experiments were run in triplicate and the reported values are the mean and standard deviation. “Sample” denotes “sample” token sampling. The IntDiv1 is annotated under Generative Yield. All metrics are for the reward threshold > 0.8. * and ** denote one and two replicates were unsuccessful, respectively.

* Pool Size = 4
* Patience = 5
* Substructure Type = _Structure_
* Structure Minimum Size = 15
* Token Sampling Method = "topk"

### Examples of Extracted Substructures: _Structure_ Extraction with 'Structure Minimum Size' = 15

In this section, the top substructures at the end of the generative experiments (using Augmented Memory ) are shown for all three drug discovery case studies (3 replicates). All experiments are for _Structure_ extraction with 'Structure Minimum Size' = 15. The extracted substructures are commonly scaffolds with "branch points", i.e., a central scaffold with single carbon bond extensions outward,

  
**Experiment** & **Generative** & **Unique** & **Oracle** & **Oracle** & **Oracle** \\
**Augmented Memory MK2** & **Yield** & **Scaffolds** & **Burden (1)** & **Burden (10)** & **Burden (100)** \\  Baseline & 34 \(\) 13 & 32 \(\) 12 & 1360 \(\) 543 & 3833 \(\) 394 & Failed \\ - Diversity & 0.794 \(\) 0.008 & & & & \\  Scaffold & 179 \(\) 63 & 131 \(\) 16 & 1163 \(\) 457 & 2550 \(\) 148 & 4421 \(\) 344 \\ - Diversity & 0.743 \(\) 0.038 & & & & \\  Scaffold Size 15 & 523 \(\) 438 & 330 \(\) 269 & 1221 \(\) 564 & 2426 \(\) 1525 & 2676 \(\) 403\({}^{*}\) \\ - Diversity & 0.676 \(\) 0.016 & & & & \\  Scaffold Sample & 106 \(\) 71 & 87 \(\) 58 & 1005 \(\) 573 & 3296 \(\) 1181 & 4592 \(\) 334\({}^{*}\) \\ - Diversity & 0.722 \(\) 0.017 & & & & \\  Scaffold Sample Size 15 & 379 \(\) 357 & 257 \(\) 227 & 983 \(\) 540 & 1846 \(\) 680 & 3244 \(\) 1133\({}^{*}\) \\ - Diversity & 0.653 \(\) 0.026 & & & & \\  Structure & 66 \(\) 18 & 59 \(\) 20 & 1246 \(\) 716 & 2708 \(\) 232 & Failed \\ - Diversity & 0.769 \(\) 0.029 & & & & \\  Structure Size 15 & 987 \(\) 211 & 610 \(\) 117 & 736 \(\) 166 & 1122 \(\) 154 & 2189 \(\) 181 \\ - Diversity & 0.704 \(\) 0.030 & & & & \\  Structure Sample & 40 \(\) 15 & 34 \(\) 11 & 1119 \(\) 1183 & 3516 \(\) 506 & Failed \\ - Diversity & 0.784 \(\) 0.024 & & & & \\  Structure Sample Size 15 & 129 \(\) 52 & 117 \(\) 50 & 1208 \(\) 660 & 2799 \(\) 476 & 4037 \(\) 0\({}^{**}\) \\ - Diversity & 0.671 \(\) 0.073 & & & & \\   

Table 8: MK2  case study hyperparameters grid search results for Augmented Memory . All experiments were run in triplicate and the reported values are the mean and standard deviation. “Sample” denotes “sample” token sampling. All metrics are for the reward threshold > 0.8. The IntDiv1  is annotated under Generative Yield. * and ** denote one and two replicates were unsuccessful, respectively.

  
**Experiment** & **Generative** & **Unique** & **Oracle** & **Oracle** & **Oracle** \\
**RENVENT MK2** & **Yield** & **Scaffolds** & **Burden (1)** & **Burden (10)** & **Burden (100)** \\  Baseline & 2 \(\) 0 & 2 \(\) 0 & 1723 \(\) 802 & Failed & Failed \\ - Diversity & 0.424 \(\) 0.031 & & & & \\  Scaffold & 7 \(\) 2 & 7 \(\) 2 & 1272 \(\) 884 & 4948 \(\) 0\({}^{**}\) & Failed \\ - Diversity & 0.704 \(\) 0.051 & & & & \\  Scaffold Size 15 & 19 \(\) 7 & 18 \(\) 7 & 808 \(\) 524 & 3891 \(\) 631 & Failed \\ - Diversity & 0.674 \(\) 0.065 & & & & \\  Scaffold Sample & 6 \(\) 2 & 6 \(\) 2 & 1427 \(\) 343 & Failed & Failed \\ - Diversity & 0.677 \(\) 0.075 & & & & \\  Scaffold Sample Size 15 & 4 \(\) 2 & 3 \(\) 1 & 2600 \(\) 1455 & Failed & Failed \\ - Diversity & 0.653 \(\) 0.026 & & & & \\  Structure & 3 \(\) 1 & 3 \(\) 1 & 2571 \(\) 1155 & Failed & Failed \\ - Diversity & 0.571 \(\) 0.112 & & & & \\  Structure Size 15 & 179 \(\) 241 & 70 \(\) 87 & 1110 \(\) 268 & 1778 \(\) 0\({}^{**}\) & 3208 \(\) 0\({}^{**}\) \\ - Diversity & 0.670 \(\) 0.020 & & & & \\  Structure Sample & 1 \(\) 0 & 1 \(\) 0 & 1737 \(\) 1595 & Failed & Failed \\ - Diversity & 0.192 \(\) 0.271 & & & & \\  Structure Sample Size 15 & 8 \(\) 5 & 7 \(\) 4 & 1943 \(\) 1153 & 4851 \(\) 0\({}^{**}\) & Failed \\ - Diversity & 0.357 \(\) 0.255 & & & & \\   

Table 9: MK2  case study hyperparameters grid search results for REINVENT . All experiments were run in triplicate and the reported values are the mean and standard deviation. ”Sample” denotes “sample” token sampling. All metrics are for the reward threshold > 0.8. The IntDiv1  is annotated under Generative Yield. * and ** denote one and two replicates were unsuccessful, respectively.

which heavily bias generation. We posit that this may be a reason why _Structure_ extraction can be more performant than _Scaffold_, as observed in the hyperparameters grid search in the previous subsection.

### Wall Times

The wall times for all drug discovery case studies with every algorithm is presented in Table 12. The reported values are averaged over 3 replicates. In general, adding Beam Enumeration to the base Augmented Memory  and REINVENT  algorithms increased wall times but only slightly and it is negligible when considering expensive oracles. An interesting observation is that "sample" token sampling increases wall time variance. This is because less probable substructures lead to more "filter rounds', i.e., epochs where all the sampled molecules are discarded as none of them contain the Beam Enumeration extracted substructures. In addition, REINVENT generally has longer wall times even though the oracle budget is the same. The reason for this is because REINVENT

  
**Experiment** & **Generative** & **Unique** & **Oracle** & **Oracle** & **Oracle** \\
**Augmented Memory AChE** & **Yield** & **Scaffolds** & **Burden (1)** & **Burden (10)** & **Burden (100)** \\  Baseline & \(556 47\) & \(544 50\) & \(62 0\) & \(380 0\) & \(2021 89\) \\ - Diversity & \(0.838 0.002\) & & & & \\  Scaffold & \(1058 102\) & \(1006 113\) & \(62 0\) & \(430 90\) & \(1469 56\) \\ - Diversity & \(0.823 0.005\) & & & & \\  Scaffold Size 15 & \(2124 326\) & \(1523 260\) & \(63 0\) & \(418 27\) & \(884 162\) \\ - Diversity & \(0.752 0.029\) & & & & \\  Scaffold Sample & \(1187 48\) & \(1075 39\) & \(84 29\) & \(409 77\) & \(1519 141\) \\ - Diversity & \(0.806 0.003\) & & & & \\  Scaffold Sample Size 15 & \(1295 126\) & \(1168 143\) & \(188 103\) & \(602 108\) & \(1440 115\) \\ - Diversity & \(0.750 0.021\) & & & & \\  Structure & \(992 64\) & \(946 52\) & \(105 59\) & \(558 94\) & \(1635 81\) \\ - Diversity & \(0.823 0.005\) & & & & \\  Structure Size 15 & \(2059 327\) & \(1552 344\) & \(105 29\) & \(462 25\) & \(1110 265\) \\ - Diversity & \(0.735 0.017\) & & & & \\  Structure Sample & \(831 126\) & \(790 130\) & \(62 1\) & \(357 29\) & \(1617 220\) \\ - Diversity & \(0.841 0.003\) & & & & \\  Structure Sample Size 15 & \(1277 526\) & \(1031 421\) & \(127 52\) & \(800 342\) & \(1879 531\) \\ - Diversity & \(0.657 0.070\) & & & & \\   

Table 10: AChE  case study hyperparameters grid search results for Augmented memory . All experiments were run in triplicate and the reported values are the mean and standard deviation. “Sample” denotes “sample” token sampling. All metrics are for the reward threshold > 0.8. The IntDiv1  is annotated under Generative Yield. * and ** denote one and two replicates were unsuccessful, respectively.

  
**Experiment** & **Generative** & **Unique** & **Oracle** & **Oracle** & **Oracle** \\
**REINVENT AChE** & **Yield** & **Scaffolds** & **Burden (1)** & **Burden (10)** & **Burden (100)** \\  Baseline & \(147 11\) & \(146 11\) & \(83 29\) & \(481 108\) & \(3931 286\) \\ - Diversity & \(0.852 0.004\) & & & & \\  Scaffold & \(245 50\) & \(244 50\) & \(63 0\) & \(566 136\) & \(3360 164\) \\ - Diversity & \(0.844 0.003\) & & & & \\  Scaffold Size 15 & \(310 207\) & \(227 159\) & \(84 29\) & \(421 120\) & \(3596 678\) \\ - Diversity & \(0.744 0.003\) & & & & \\  Scaffold Sample & \(257 77\) & \(252 76\) & \(63 0\) & \(480 60\) & \(2946 460\) \\ - Diversity & \(0.847 0.004\) & & & & \\  Scaffold Sample 15 & \(310 92\) & \(271 70\) & \(148 28\) & \(673 107\) & \(2881 475\) \\ - Diversity & \(0.759 0.039\) & & & & \\  Structure Size 15 & \(323 58\) & \(284 71\) & \(62 0\) & \(441 132\) & \(3073 427\) \\ - Diversity & \(0.795 0.009\) & & & & \\  Structure Sample & \(213 26\) & \(206 22\) & \(84 30\) & \(558 222\) & \(3073 279\) \\ - Diversity & \(0.844 0.005\) & & & & \\  Structure Sample Size 15 & \(316 253\) & \(190 146\) & \(125 50\) & \(561 140\) & \(2683 320\) \\ - Diversity & \(0.721 0.111\) & & & & \\   

Table 11: AChE  case study hyperparameters grid search results for REINVENT . All experiments were run in triplicate and the reported values are the mean and standard deviation. “Sample” denotes “sample” token sampling. The IntDiv1  is annotated under Generative Yield. All metrics are for the reward threshold > 0.8. * and ** denote one and two replicates were unsuccessful, respectively.

Figure D13: Augmented Memory  DRD2  substructures with _Structure_ extraction and ’Structure Minimum Size’ = 15 after 5,000 oracle calls.

optimizes the structure components of the MPO objective: QED and MW constraint to a lesser extent. Consequently, REINVENT generates larger molecules, on average, which take longer to dock with Vina. This observation is in agreement with the original Augmented Memory work which compared to REINVENT.

 
**Target** & **Experiment** & **Augmented Memory Wall Time** & **REINVENT Wall Time** \\  DRD2 & Baseline & 14h 0m \(\) 1h 26m & 16h 30m \(\) 0h 55m \\  & Scaffold & 12h 58m \(\) 1h 11m & 17h 9m \(\) 1h 28m \\  & Scaffold Size 15 & 12h 56m \(\) 0h 46m & 16h 51m \(\) 1h 58m \\  & Scaffold Sample & 12h 11m \(\) 0h 24m & 16h 32m \(\) 1h 3m \\  & Scaffold Sample Size 15 & 13h 32m \(\) 0h 50m & 16h 26m \(\) 2h 58m \\  & Structure & 14h 30m \(\) 0h 51m & 22h 5m \(\) 1h 52m \\  & Structure Size 15 & 14h 54m \(\) 2h 24m & 24h 33m \(\) 5h 8m \\  & Structure Sample & 13h 58m \(\) 0h 51m & 20h 5m \(\) 1h 42m \\  & Structure Sample Size 15 & 14h 52m \(\) 1h 32m & 19h 52m \(\) 3h 22m \\  MK2 & Baseline & 10h 46m \(\) 0h 3m & 15h 19m \(\) 0h 34m \\  & Scaffold & 11h 0m \(\) 0h 28m & 16h 21m \(\) 0h 53m \\  & Scaffold Size 15 & 11h 22m \(\) 1h 30m & 16h 38m \(\) 1h 33m \\  & Scaffold Sample & 12h 56m \(\) 0h 36m & 15h 49m \(\) 0h 36m \\  & Scaffold Sample Size 15 & 11h 52m \(\) 1h 55m & 16h 28m \(\) 0h 33m \\  & Structure & 12h 29m \(\) 0h 19m & 19h 40m \(\) 1h 55m \\  & Structure Size 15 & 11h 22m \(\) 1h 17m & 18h 39m \(\) 1h 33m \\  & Structure Sample & 12h 22m \(\) 0h 28m & 18h 12m \(\) 0h 57m \\  & Structure Sample Size 15 & 12h 37m \(\) 0h 47m & 16h 6m \(\) 1h 37m \\  AChE & Baseline & 10h 6m \(\) 0h 39m & 14h 12m \(\) 0h 59m \\  & Scaffold & 11h 46m \(\) 0h 51m & 13h 10m \(\) 1h 4m \\  & Scaffold Size 15 & 11h 10m \(\) 0h 44m & 15h 52m \(\) 1h 4m \\  & Scaffold Sample & 10h 55m \(\) 0h 44m & 15h 27m \(\) 0h 57m \\  & Scaffold Sample Size 15 & 10h 24m \(\) 0h 17m & 14h 53m \(\) 0h 53m \\  & Structure & 13h 0m \(\) 0h 47m & 19h 10m \(\) 0h 22m \\  & Structure Size 15 & 11h 26m \(\) 0h 51m & 18h 30m \(\) 0h 20m \\  & Structure Sample & 11h 23m \(\) 0h 22m & 15h 36m \(\) 0h 20m \\  & Structure Sample Size 15 & 17h 56m \(\) 4h 27m & 19h 16m \(\) 2h 43m \\  

Table 12: Wall times for all drug discovery case studies hyperparameters grid search using Augmented Memory and REINVENT. ”Sample” denotes “sample” token sampling. All experiments were run in triplicate and the values are the mean and standard deviation.

## Appendix E Augmented Memory and REINVENT Model Hyperparameters

The same pre-trained prior on ChEMBL  was used for Augmented Memory  and REINVENT . All shared hyperparameters (sampling batch size and learning rate) are the same. Default additional hyperparameters for Augmented Memory were used based on the original work : two augmentation rounds and using Selective Memory Purge to prevent mode collapse. Experience replay  was kept default in REINVENT (randomly sample 10 molecules out of 100 from the replay buffer at each epoch).