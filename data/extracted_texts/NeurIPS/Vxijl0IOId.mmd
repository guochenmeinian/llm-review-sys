# Learning Generalized Linear Programming Value Functions

Tu Anh-Nguyen

Google Research and Rice University

Houston, TX

tu.na@rice.edu

&Joey Huchette

Google Research

Cambridge, MA

jhuchette@google.com

Christian Tjandraatmadja

Google Research

Cambridge, MA

ctjandra@google.com

###### Abstract

We develop a theoretically-grounded learning method for the _Generalized Linear Programming Value Function_ (GVF), which models the optimal value of a linear programming (LP) problem as its objective and constraint bounds vary. This function plays a fundamental role in algorithmic techniques for large-scale optimization, particularly in decomposition for two-stage mixed-integer linear programs (MILPs). This paper establishes a structural characterization of the GVF that enables it to be modeled as a particular neural network architecture, which we then use to learn the GVF in a way that benefits from three notable properties. First, our method produces a true under-approximation of the value function with respect to the constraint bounds. Second, the model is input-convex in the constraint bounds, which not only matches the structure of the GVF but also enables the trained model to be efficiently optimized over using LP. Finally, our learning method is unsupervised, meaning that training data generation does not require computing LP optimal values, which can be prohibitively expensive at large scales. We numerically show that our method can approximate the GVF well, even when compared to supervised methods that collect training data by solving an LP for each data point. Furthermore, as an application of our framework, we develop a fast heuristic method for large-scale two-stage MILPs with continuous second-stage variables, via a compact reformulation that can be solved faster than the full model linear relaxation at large scales and orders of magnitude faster than the original model.

## 1 Introduction

The _linear programming (LP) value function_ models the optimal value of an LP as problem data in that problem varies. Value functions are a fundamental abstraction used in many algorithms for large-scale optimization. More concretely, many problems where decisions are made sequentially-e.g., two-stage stochastic programs , facility location problem , multi-commodity problems , or network interdiction problems -can be modeled as two-stage mixed-integer linear programs (MILPs). To pick one common technique as a motivating example: _Benders' decomposition_ is an algorithmic technique that decomposes such a large problem into many smaller ones [21; 42; 51]. At a high level, Benders' decomposition abstracts each LP subproblem away by replacing it with a value function. If we somehow had a good approximation of these value functions that we could efficiently optimize over, this reformulation would be straightforward to approximately solve. In Benders' decomposition however, we do not have such a representation _a priori_, and thus we iterativelyconstruct an approximation of it via cutting planes; this is often the most computationally expensive part of the algorithm . In this paper, we focus on learning such representations.

Separately, it is well-known that neural networks (NNs) are "universal approximators" in theory  and incredibly adept at modeling complex behaviors in practice . Taken together, we can state two natural questions that motivate this work:

* What are the meaningful structural properties of a value function, and what are suitable neural network architectures for encoding these properties?
* How good are these approximations in practice, and how can we leverage them to solve real-world problems?

**Contributions.** Our work studies the _Generalized Linear Programming Value Function_ (GVF), defined as the function that models an LP's optimal value as both its objective and its constraint bounds vary, and shows how machine learning techniques can be used to build practical approximations of this function. In particular, our contributions are as follows.

1. **A GVF Representation theorem.** We study the structure of the GVF and show that it can be exactly modeled as a maximum of bilinear functions, where each function is the dot product of two piecewise linear functions that depends only the objective coefficients or constraint bounds, respectively.
2. **A theoretically-grounded NN architecture for GVF.** We present the _Dual-Stack Model_, a neural network architecture which mimics the structural property of the GVF exposed by our representation theorem.
3. **An unsupervised learning approach.** We show that the GVF can be written as the unique optimal solution of a constrained optimization problem that does not require solving any LPs to write down. We use this as inspiration for an unsupervised learning method that can be implemented using standard NN training constructs and libraries.
4. **Empirical justification.** We present a computational study showing that our unsupervised training approach can perform comparably with supervised training in terms of approximating a GVF, without the expensive data generation phase that supervised training requires.
5. **A fast heuristic for large-scale two-stage MILPs.** Due to the properties of the Dual-Stack Model, we can easily embed it as an LP within a larger optimization problem. As an application of our framework, we leverage this fact to produce a heuristic for two-stage MILPs with continuous second-stage variables, which includes a provable duality gap.

## 2 Preliminaries

A Linear Program (LP) is a mathematical optimization problem of the form:

\[\{c x Ax b,x 0,x^{n}\},\] (1)

where \(x\) are the decision variables, \(c^{n}\) is the vector of objective coefficients, \(A^{m n}\) is the constraint matrix, and \(b^{m}\) is the vector of constraint bounds (often called the "right-hand side" in such a representation).

Duality is a fundamental concept in linear programming that establishes a relationship between the primal (original) linear program and its dual (related) linear program. This relationship provides insights into the optimal solutions, and it is valuable for both theoretical understanding and practical applications. The dual problem of (1) is

\[\{b y A^{T}y c,y 0,y^{m}\}.\] (2)

The typical LP Value Function (LPVF) is \(h_{A,c}()_{x}\{c x Ax,x 0,x ^{n}\}\). As a corollary of strong duality, the LPVF is a piecewise linear convex function, which is the maximum of a finite number of affine functions. Note that the LPVF only considers varying constraint bounds; if we also permit the objective coefficients to vary, we obtain the Generalized LP Value Function (GVF) . Formally, we define a GVF associated with a fixed constraint matrix \(A^{m n}\) as:

\[h_{A}(,)_{x}\{\, x Ax,x  0\}.\] (3)Many typical decomposition methods only need to consider LPVFs of fixed objective vectors \(c\). However, learning the entire GVF at once means that we can reuse the same learned model for many different objectives, potentially saving computation and allowing for a broader generalization.

We use \((,)\) to denote the linear program in (3) for fixed values of \(\) and \(\). Conventionally, when \((,)\) is infeasible, \(h(,)=+\), and when \((,)\) is unbounded from below, we have \(h(,)=-\). Let \(\{^{m} x^{n} Ax,x 0\}\) and \(\{^{n} y^{m} A^{T}y,y 0\}\). By strong duality and the definitions of \(\) and \(\): \(h(,)\) is finite if and only if \(\) and \(\). We define \(X()\{x^{m} Ax,x 0\}\) as the set of feasible solutions of \((,)\) for a fixed \(\).

In this work, we will consider two-stage MILPs with continuous second-stage variables, where fixing the first-stage variables results in the problem decomposing into \(K\) independent LPs. In particular, these have the form:

\[_{x^{1},x^{2}}\{c x^{1}+_{k K }d^{k} x^{2,k} x^{1},\;T^{k}x^{1}+Ax^{2,k} b^{k}, \;x^{2,k} 0\; k K\},\] (4)

where \(x^{1}^{n_{1}}\) are the first-stage variables, \(\) is the first-stage feasible set, \(K^{+}\) is the number of second-stage subproblems, and \( K\) denotes the set \(\{1,,K\}\). For example, in the context of stochastic programming, \(K\) is the number of scenarios, or in a facility location problem, \(K\) is the number of customers. Each subproblem \(k\) is associated with corresponding continuous second-stage variables \(x^{2,k}^{n_{2}}\). We assume that the second-stage constraint matrices are the same, denoted as \(A^{m_{2} n_{2}}\) while the constraint matrices of first-stage and the constraint bounds constraints vector can vary among second-stages, denoted as \(T^{k}\) and \(b^{k}\), respectively. We can rewrite (4) using GVF as

\[_{x^{1}}\{c x^{1}+_{k K}h _{A}(d^{k},b^{k}-T^{k}x^{1}) x^{1} X\}.\] (5)

Within the context of GVFs, we can reformulate equation (3) to elide the requirement for second-stage variables. The large number of these variables can impede computational efficiency , which motivates a compact representation of GVFs.

## 3 Related Work

### Value function learning for multi-stage problems

Neural networks are well-known to be powerful "universal approximators" . This has motivated a line of research focused on learning value functions, particularly those based on constraint bounds such as LPVF and their MILP analogues, with the main goal of improving methods for two-stage or multi-stage optimization problems. Dai et al. , Lee et al. , and Bae et al.  propose various methods to learn the LPVF with the aim to solve multi-stage stochastic programming problems more quickly. Similar to our work, they use models that are convex on constraint bounds to match the structure of the LPVF. Beyond the LP value function, neural networks have also been used to learn IP value functions to improve the integer L-shaped method . Moreover, to tackle difficult mixed-integer problems with a large number of scenarios, Dumouchelle et al. [14; 15] devise NN architectures that learn MILP value functions of constraint bounds and scenarios.

Our method differs from the above in that we directly learn the GVF rather than the LPVF (though  learns the GVF indirectly), we do not require solving optimization subproblems (LPs in our case) to obtain training data. Furthermore, we aim to not only generalize across second-stage subproblems (scenarios), but also across instances. In particular, by allowing the objective coefficients to vary, we learn a single value function that encompasses all subproblems, rather than learning one per subproblem. Of course, learning a single GVF is generally harder than learning a single LPVF, but a core thesis of this work is that there is underlying structure tying together those many related LPVFs that we can exploit when learning the GVF.

### Other learning-based approaches

A related research direction in learning for stochastic optimization is scenario reduction, which seeks a smaller set of "representative scenarios". Many of these approaches perform some form of clustering to reduce the number of scenarios and then solve a smaller surrogate problem with these scenarios [10; 16; 30; 41; 44]. Wu et al.  uses a conditional variational autoencoder to learn scenario embeddings and cluster them. Bengio et al.  predicts a representative scenario for a smaller surrogate problem, but it relies on problem structure to build scenarios for training.

Other learning-based methods for tackling two-stage stochastic problems include reinforcement learning for local search  and Benders cut classification . More generally, ML-based approaches have also been applied for bilevel optimization [29; 47; 48; 56] where value functions are relevant, though unlike in our case, these only have a single inner optimization subproblem. Finally, there is an extensive stream of work focusing on applying ML to support decisions within MILP solvers, such as branching and cutting plane decisions (e.g., [2; 39; 50; 24]).

### Computing value functions

LP value functions are well-studied (e.g., see [45; Chapter 19]), as they play crucial roles in sensitivity analysis and Benders' decomposition. On the other hand, GVFs are considerably less well-studied. While much is known about its structure [22; 27], to the best of our knowledge our method is the first that aims to learn it directly based on its theoretical properties. The computation of value functions has also been studied for ILPs and MILPs using superadditive duality [32; 43; 52; 53]. However, they are less tractable to compute and thus more difficult to leverage into a practical algorithm.

## 4 A Neural Network Representation for Generalized Linear Programming Value Functions

In this section, we will develop a characterization of the GVF that, in the sequel, we will use as inspiration for a neural network architecture that is well-suited to approximate the GVF.

### A Characterization of Generalized Linear Programming Value Function

It is known that \(\) can be partitioned into distinct _inariancy regions_, within each of which the GVF is bilinear with respect to \(\) and \(\). This can be reformulated as the following proposition.

**Proposition 1**.: _Fix a matrix \(A^{m n}\), and define \(S()\) as the set of all bases of \(A\) which are feasible with respect to fixed constraint bounds \(\). Then, \(h_{A}(,)=_{B()}_{B}B^{-1}\). Furthermore, \(h_{A}(,b)\) is piecewise linear concave for every fixed \(b\) and \(h_{A}(c,)\) is piecewise linear convex for every fixed \(c\)._

While Proposition 1 tells us that each invariancy region defined by \(\) can be decomposed into a product of functions that depend only on \(\) and \(\), it does not provide us with a _global_ decomposition that is valid across all invariancy regions. We now show that there does indeed exist a structured, global decomposition of a GVF in terms of piecewise linear functions that consider either \(\) or \(\), but not both.

**Theorem 2**.: _(GVF Representation Theorem) For a fixed matrix \(A^{m n}\), there exists a set of \(p\) piecewise linear functions \(\{F_{p}:^{n}^{K}\}_{p=1}^{p}\) and a piecewise linear convex function \(G:^{m}^{K}\) such that_

\[h_{A}(,)=_{p[P]}\{F_{p}()^{T}G()\} ,.\] (6)

We refer the reader to Appendix A for a proof of this result.

### The Dual-Stack Model

We now use Theorem 2 as inspiration for a neural network architecture that we dub the _Dual-Stack Model_ (DSM). For simplicity, in the remainder we will consider LPs written in the form of (1); see Appendix F for analogous models for other LP representations.

The architecture of a DSM is depicted in Figure 1. It consists of two stacks of feedforward fully-connected neural networks of depth \(N\) and \(M\) corresponding to the objective vector \(\) and the constraint bounds vector \(\); we name them the \(\)-stack and the \(\)-stack, respectively. Each layer has a piecewise linear activation function to ensure that the entire stack itself is piecewise linear; either ReLU or Max-pooling is a suitable choice. We denote the output matrix of the \(\)-stack as \(\) and the output vector of the \(\)-stack as \(\), respectively. To model the outer maximization in (6), the output of the model is the maximum element of the dot product between \(\) and \(\). Finally, the \(\)-stack is constrained so that the first layer has non-positive weights and each subsequent layer has non-negative weights; this enforces the desired properties of a GVP listed in Theorem 3, such as convexity on \(\). In general, the \(\)-stack represents the functions \(\{F_{p}\}_{p=1}^{Q}\) and the \(\)-stack models the function \(G\) from Theorem 2.

We can summarize the properties of DSM as follows. Let \(a^{i}\) the \(i\)-th column of the matrix \(A\). Then we may define \((A)\{_{A}()_{A}_{A}(,a^{i})_{i}\ ,i  n\}\) to be the class of functions that can be represented by a DSM, subject to what we might call dual feasibility constraints on their outputs.

**Theorem 3**.: _Any function \(_{A}(A)\) has the following properties:_

1. \(_{A}(,)\) _is piecewise linear, convex, and monotonically decreasing for every fixed_ \(\)_._
2. \(_{A}(,)\) _is piecewise linear for every fixed_ \(\)_._
3. \(_{A}(,) h_{A}(,)\) _for every fixed_ \(\) _and_ \(\)_._

This result shows that, for fixed inputs, \(h_{A}\) is upper-bounded by the true GVF. In fact, we can show something even stronger.

**Theorem 4**.: _For any fixed \(A^{m n}\), \(h_{A}(A)\), and moreover \(h_{A}\) is pointwise larger than all other elements of \((A)\)._

One way to interpret Theorem 4 is that there exists some DSM architecture whereby we can recover the GVF by setting the weights in such a way that we recover the pointwise maximum across infinitely many points in \(\). We can now sharpen this result to show that it suffices to restrict attention to some finite subset of these points. For any given \(_{ 0}^{M}\) and \(_{ 0}^{N}\), define \((,)\) to be the class of functions represented by a DSM whose \(\)-stack and \(\)-stack have layers with \(\) and \(\) neurons each, respectively.

**Theorem 5**.: _There exists some \(_{+}^{M}\), some \(_{+}^{N}\), a finite set \(}\), a finite set \(}\), and some \(^{}(,)\) such that \(^{}(,)=h_{A}(,)\) for all \(},}\). Moreover, this same \(^{}\) necessarily satisfies \(^{}(,)=h_{A}(,)\) for all \(\) and \(\)._

Finally, we reframe this existential result as an optimization problem; this will form the basis for the unsupervised training framework we develop in Section 5.

**Corollary 6**.: _Take the \(\), \(\), \(}\), and \(}\) that Theorem 5 guarantees must exist. Denote the parameters of a DSM model with \(\). Then, \(h_{A}\) is the unique solution of_

\[_{^{}(,)} _{}_{}^{}( ,)\] (7a) s.t. \[^{}(,a^{i})_{i} },i n.\] (7b)

We refer the reader to Figure 2 for an illustration of how, taken together, the results of this section permit us to learn a good approximation of the GVP. In addition, we highlight that typical "universal approximation theorems"  apply _over bounded input domains_, whereas here \(\) and \(\) may be unbounded. However, the above approach shows that we can attain \(h_{A}\) without this assumption.

## 5 Learning Generalized Linear Programming Value Functions

For context, we begin by describing a standard supervised training method to approximate the function \(h_{A}\) using a neural network with parameters \(\). First, we generate some training data set

Figure 1: Dual-Stack Model (DSM)

\(\{(^{i},^{i};h_{A}(^{i},^{i}))\}_{i=1}^{| |}\). Then, we optimize our parameters \(\) by minimizing a loss function (such as an \(_{2}\)-distance) between the output of the NN with our data. Notably, this is an unconstrained optimization problem. However, data generation may be expensive: in general, we must solve an linear programming problem, \((^{i},^{i})\), for every training data point \(i||\).

### An Unsupervised Learning Approach

Corollary 6 tells that we do not actually need labeled data to learn the GVF. Of course, there is a trade-off here: directly applying the result requires us to somehow compute the sets \(}\) and \(}\), and also requires solving a constrained optimization problem in (7) (as opposed to a typical unconstrained learning problem). To address the first issue, we propose using two subsets \(_{b}\) and \(_{c}\) to use in lieu of \(}\) and \(}\) respectively, which may come from training data or be randomly generated; this is further detailed for a specific application in Section 7. To address the second issue, we introduce a penalty term to the objective to model the constraints (7b) in a "soft" manner. This leaves us with the following unconstrained, unsupervised learning problem:

\[_{^{}(,)}_{ _{c}}_{_{b}}-^{}(,)+ _{_{c}}_{i n}\{^ {}(,a^{i})-_{i},0\},\] (8)

where \(_{+}\) is the penalty coefficient. We can motivate both of the alterations introduced above with the following corollary to Theorem 5. To enforce the constraint \(^{}(,)\), we take the positive (respectively, negative) absolute value of the weights for a nonnegative (resp. nonpositive) weight-sign constraint.

**Corollary 7**.: _Given any nonempty subsets \(_{b}\) and \(_{c}\), there exists a sufficient large \(\) such that \(h_{A}\) is an optimal solution of (8)._

Note that, while (7) has \(h_{A}\) as its unique optimal solution, in general we cannot guarantee that (8) has an unique optimal solution, meaning that we may recover an optimal solution that deviates from \(h_{A}\) on points outside the training set. However, Corollary 7 ensures that any solution of (8) is at least as good as \(h_{A}\) at \((,)_{c}_{b}\)1. Moreover, in Section 5.2 we will provide computational evidence that a near-optimal solution of (8) is empirically a good approximation of \(h_{A}\). Corollary 7 also justifies why we elect to use an \(^{1}\) penalty term in (8), rather than a smooth penalty or Lagrangian

Figure 2: An illustration of how we learn a GVF, looking at slices along the constraint bounds (Top) and objective coefficients (Bottom). Given only data points \(}}\) and no constraints, either (7b) or weight-sign, maximizing \(_{}}_{}^{}( ,)\) will yield a poor initial approximation. Adding constraints (7b) will force the function to be smaller than \(h_{A}\) at certain “anchor” points which are distinct from the input data points, at which the function will still tend to be large. Adding the weight-sign constraints will force the function to be convex in terms of the constraint bounds, and will therefore produce an approximation that lower bounds \(h_{A}\). Theorem 5 then tells us that, with sufficiently many data points to start, we will eventually recover \(h_{A}\) directly.

multipliers. If we were to modify (8) to use an \(^{2}\) penalty term, for example, an analogous version of Corollary 7 need not hold: roughly speaking, \(h_{A}\) is the function that satisfies most of (7b) at equality, whereas an \(^{2}\) penalty term would tend to push optimal solutions away from the boundary of the feasible region of (7). We refer the interested reader to [40, Chapter 17] for a detailed discussion of exact-inexact or smooth-nonsmooth penalty terms.

### Penalty Coefficient Update Strategies

Now, we use standard techniques from nonlinear optimization [40, Chapter 17] to develop a heuristic scheme for updating our penalty term \(\), depicted in Algorithm 1. In practice, the optimization problem (7) might be numerically unstable. For example, if we initialize \(_{0}\) to a small value, then the first term \(_{_{c},_{b}}_{A}^{}( ,)\) is weighed far more than the penalty term, and so the optimal solution will tend towards \(-\). We resolve this by using upper bounds, \(u\), on \(h_{A}\) as "tether" points. For each training data point, we choose a simple upper bound of the optimal objective (see Section 7 and Appendix E for details).

We parameterize our scheme on the penalty update strategy, which is perhaps the most important factor. A common choice would be the linear update strategy, which means \(\) is scaled up by a constant factor \(\) at each step, i.e., \(()=\). However, we propose a more computationally effective "adaptive update" strategy where we update \(\) based on how many constraints (7b) the current solution \(^{t}\) satisfies. In particular, \(()=(2-)\).

```
1:procedureLearningGeneralizedLPVF(\(A,_{c},_{b},u,T,_{0},\))
2: Initialize \(^{0}\)
3:for\(t 0,,T-1\)do
4:\(^{t+1}_{_{c}} _{_{b}}(u_{,}-^{}(, ))^{2}+_{_{c}}_{i[\![n]\!]}_{t }\{^{}(,a^{i})-_{i},0\}\)
5:\(_{t+1}(_{t})\)
6:Return\(^{T}\). ```

**Algorithm 1** Learning GVF with objective upper bounds

Note that, since the minimization problem in line 4 of Algorithm 1 is non-convex, we cannot guarantee a globally optimal solution \(^{t+1}\). Therefore, in practice, we solve the minimization subproblem until some criterion is met, e.g., the gradient is sufficiently small or we reach a prescribed iteration limit.

### Guaranteeing an Under-Approximation

By Theorem 3, any feasible solution for (7) lower bounds the GVF we are attempting to learn. However, our methods laid out in this section do not guarantee this property for two reasons. First, the penalty method treats the constraints (7b) as soft constraints rather than hard constraints, and thus they may not be fully satisfied. Second, we are approximating the set \(}\) from Corollary 6 with a training set \(_{c}\), and therefore our solution may not provide a lower bound at other objective vectors \(\). To resolve this, we can scale the function down as much as needed to guarantee the constraints (7b), with the expectation that our learning method produces a solution that is not too far off from being dual feasible. However, we can do better if we only need to provide a valid lower bound for a single objective \(\), which is often the case (see Section 6 below). In this way, we can train a single DSM approximation and reuse it across many objectives by suitably postprocessing it for each. We describe this procedure in Algorithm 2.

```
1:procedurePost-Processing(\(c,^{A},^{A}\))
2:\(^{c}^{A}(c)^{p N}\)
3:for\(j 1,,p\)do
4:return\(^{c}\) ```

**Algorithm 2** Post-processing to guarantee the lower-bounding property A GVF-Based Heuristic for Two-Stage MILPs

As an application of the learning method we developed in Section 5, we propose a heuristic method for two-stage MILPs with continuous second-stage variables (4). The main idea is to replace each of the second-stage subproblems with the corresponding LPVFs from our learned function: that is, we use our approximation to represent \(h_{A}(d^{k},b^{k}-T^{k}x^{1})\) in (5) for each \(k\). This yields a fast heuristic for two reasons. First, our learned approximation is piecewise linear convex when restricted to a fixed objective, meaning that it can be efficiently modeled inside a larger optimization problem as an LP . Second, the number of variables of this LP scales with the number of neurons in the DSM, typically much smaller than a second-stage subproblem LP. In practice, this enables the heuristic to run faster than solving the LP relaxation of (5), despite maintaining integrality of the first-stage variables.

Given a learned DSM representing the function \(_{A}^{}\), denote by \(W_{}^{0},W_{}^{1},,W_{}^{M}\) the weights of the constraint bounds stack, where \(W_{}^{0} 0\) and \(W_{}^{1},,W_{}^{M} 0\). For a fixed objective coefficient \(c\) and the output of the objective-stack \(^{c}\) (post-processed as in Section 5.3), we can model the set \(\{_{A}^{}(c,)\}\) as

\[_{}(c,)=\{ zz_{1}(W_{}^{0}),\;z_{i+1}(W_{}^{i}z_{i})\;  i M,\;_{i}^{c} z_{M}\; i  p\}.\]

Our heuristic is then to solve the following problem to obtain a solution for the first-stage variables:

\[_{x^{1},^{1},,^{K}}\;\{c x^{1}+_{k  K}^{k}\;\;^{k}_{}(d ^{k},b^{k}-T^{k}x^{1})\; k K,x X\}\] (9)

Note that (9) is simply (5) with the LPVFs replaced by our learned model. Since \(_{A}^{}(c,) h_{A}(c,)\) for all \(c,\), the objective value of (9) is a dual bound for the original problem. Once we compute optimal values of the first-stage variables \(x^{*}\), we can then recover the second-stage variable values by solving each of the second-stage LP subproblems independently with fixed \(x^{*}\), yielding the full solution. Algorithm 3 describes the full method.

```
1:procedureGVFBasedHeuristic(\(A,c,\{T^{k}\}_{k K}\), \(\{b^{k}\}_{k K}\), \(\{d^{k}\}_{k K}\), \(,\))
2:for\(k 1,,K\)do\(^{d^{k}}\)Post-Processing(\(c,,\))
3:\(((x^{*})^{1},\{^{*k}\}_{k K})\) optimal solution of (9) with \(W_{},\{^{d^{k}}\}_{k K}\)
4:for\(k 1,,K\)do
5:\((x^{*})^{2,k}_{x^{2,k}}\{d^{k} x^{2,k} Ax^{2,k}  b^{k}-T^{k}x^{*1},x^{2,k} 0\}\)return\(x^{*}\) ```

**Algorithm 3** GVF-based heuristic for two-stage MILPs with continuous second-stage variables

## 7 Computational Results

In this section, we computationally evaluate2 both the approximation quality of the learning method described in Section 5 and the effectiveness of the heuristic for two-stage problems from Section 6.

We evaluate these methods on the _uncapacitated facility location_ (UFL) . This is a deterministic two-stage problem, in which we first select \(n_{f}\) facilities to open, and allocate each of \(n_{c}\) customers to an open facilities. We consider two classes of instances, _Euclidean_ and _KG_, both with \(n_{c}=n_{f}\). In both cases, we take a set of objective and right-hand side vectors from one instance for training and a second, different, set from five instances for testing (more details are provided in Appendix C).

To produce the training data, we take all or some customer allocation costs from the UFL training instances as our objective coefficient dataset \(_{c}\). Then, for each such cost vector, we generate \( n_{f}/10\) points uniformly at random between \(\) for our constraint bound dataset \(_{b}\). Thus, the total size of the training data is \(|_{c}| n_{f}/10\). If all customer costs are selected for \(_{c}\), this is \(|| n_{c} n_{f}/10\) where \(\) is the set of training instances. Note that we do not use the facility costs at all for training. To improve training stability, we normalize the customer allocation costs by their mean in the training data. For the training dataset, we choose an upper bound of the GVF to be \(2\), which is an upper bound for the largest possible assignment cost in the training dataset after normalization.

To learn each GVF, we run a total of \(T=40\) iterations in Algorithm 1, at each iteration, we solve (8) by performing \(100\) steps of the Adam algorithm . For DSM, we select the model within the \(T\) iterations that satisfies at least \(98\%\) of the constraints (7b) from the training dataset with lowest training objective function (8). Details of hyperparameter tuning for DSMs and DenseNets are provided in Appendix E. In addition to the numerical study for UFL, we include experiments on the Stochastic Capacitated Facility Location (SCFL) in Appendix D.

### Learning Method

Arguably, we would expect that the lack of supervised data would make the Dual-Stack Model more difficult to train than a standard supervised learning approach, especially considering that it also has training constraints and convexity requirements. On the other hand, our theoretical results suggest that these same requirements can help the model take the general shape of a GVF. Indeed, we observe in Table 1 that our approach produces a model that is comparable or better than a standard ReLU network (see Appendix E for DenseNet) and Random Forest Regressors in terms of how well it approximates the GVF. Unlike with DSM, these baselines require solving an LP for each training point to produce labels. The number of LPs can grow large, though they can be solved in an embarasingly parallel manner, and in the case of UFL we can use an efficient greedy algorithm given that the LP reduces to fractional knapsack. We report an _a posteriori_ metric, the True Relative Error, defined as the gap between the model and the GVF, i.e., \(|(,)-h_{A}(,)|/\{(,),h_{A}( ,)\}\), averaged across all \(,\) in the test set. The Lower Bound in Table 1 shows the percentage of constraints (7b) satisfied for either the training or test set.

We see that, on all but one instance family, we are able to train a DSM that attains a lower True Relative Error than both DenseNet and Random Forest. Moreover, the training times between DSM and DenseNet are roughly comparable. We also highlight that the Euclidean instances are relatively harder to learn than the KG instances; this is observable for all models.

Table 1 examines the capacity of the DSM model to learn GVFs. To illustrate the scalability of DSMs, we train the model on LPs of various sizes. We use the allocation cost vectors of all customers for \(_{c}\) in all cases, except for KG 750 where we select 200 customers for \(_{c}\) due to memory limitations. Although training time naturally increases with instance size, both the True Relative Error and Lower Bounds remain stable for KG instances and exhibit only a slight increase in the Euclidean cases. This observation supports the scalability of the DSM approach.

### Heuristic for Two-Stage Problems

We evaluate our heuristic on our generated UFL instances by comparing its performance against that of a state-of-the-art open-source MILP solver, SCIP . We compare with the best feasible solution found by the solver within the time limit specified in the column "MIP Solve Time (s)" in Table 2 for KG, or the optimal solution for Euclidean instances. For KG, even solving the LP relaxation for the full model can take a few minutes. On the other hand, despite being an MILP, solving the model (9) is very fast, often taking less than a second, particularly because we only require binary constraints on the first-stage variables. After solving this MILP, the second-stage solutions are recovered by simply taking the closest open facility. Denoting the objective value of the heuristic feasible solution by \(v^{*}\) and the optimal value of (9) by \(V\)--which is a dual bound to the original problem due to our under-approximation guarantee--the Provable Gap is computed as \((v^{*}-V)/v^{*}\), which is an upper

    &  &  &  \\  & **Train Time (s)** & **True Relative Error** & **Train Time (s)** & **True Relative Error** & **Train Time (s)** & **True Relative Error** & **Train Time (s)** & **True Relative Error** & **Train Time (s)** & **True Relative Error** & **Train Time (s)** & **True Relative Error** \\   & 250 & 157.20 & 15.5\% & 96.5\% & 257.1 \% & 5.41 & 164.1 & **1.80 \%** & 73.4 & 6.05 \% \\  & 500 & 1284.8 & 181.4 \% & 96.7\% & 32.0\% & 29.64 & 140.27 & 1.64 \% & 1092.0 & 6.18 \% \\   **Random Forest** \\ **500** \\ **500** \\ **500** \\  } & 730 & 1028.4 & **68.4\%** & 96.7\% & 33.1\% & 13.27 & 1125.58 & 46.4 \% & 1571.77 & 62.5\% \\   **Random Forest** \\ **500** \\  } & 100 & 7.57 & **33.8\%** & 96.0\% & 85.9\% & 1.17 & 7.26 & 39.2\% & 37.90 & 41.41 \% \\  & 500 & 43.7\% & **37.8\%** & 96.0\% & 67.4\% & 4.98 & 69.6 & 43.1\% & 77.48 & 32.2\% \\   **Random Forest** \\ **500** \\  } & 500 & 161.25 & **32.8\%** & 99.41 \% & 96.5\% & 8.26 & 201.31 & 53.25 \% & 375.60 & 9.2\% \\   

Table 1: Comparison between Dual-Stack Model and DenseNet in Learning GVF.

bound of the true gap. The Gap to MILP is computed as \((v^{*}-)/v^{*}\), where \(\) is the objective value of the MILP baseline. A negative value of Gap to MILP means that the solution returned by our heuristic is better than the solution returned by SCIP within the time limit. We solve 5 instances from each class and report the mean and standard deviation for each metric across these selected instances in each class. When reporting time, we use '\(>\)' to signal the solver reaches a time limit. We also compare our method with a heuristic based on Benders Decomposition, which mimics the DSM heuristic except that we use Benders cuts instead of the GVF. For each instance, we first iteratively generate a number of Benders cuts for all subproblems, and then solve one MILP with the inclusion of all generated optimality cuts. For each UFL instance, we set the maximum number of Benders LP iterations to match the total number of facilities, while the time limit for solving the MILPs with cuts is fixed at 1 minute. In the Euclidean case, the Benders heuristic was able to produce an optimal solution for all instances tested. We observe from Table 2 that our approach produces significantly better solutions than our full model baseline for large KG instances, though not for Euclidean ones. This may be because the Euclidean instances are more difficult to learn as observed in Table 1, both for DSM and the dense model. We also note that the Euclidean instances are much _easier_ to solve to optimality than the KG instances, meaning that a fast heuristic for them is of relatively lesser value.

## 8 Conclusion

JIn this study, we provide a structural characterization of the GVF, inspiring an NN architecture and unsupervised method that approximate it well. Additionally, we utilize this framework to develop a fast heuristic method for two-stage MILPs with continuous second-stage variables, effective for some large-scale instances. We will conclude this paper by highlighting two areas for future work, where we believe that the techniques presented in this paper could be sharpened or otherwise improved.

First, we believe that our training objective could be further improved. During training, we must balance two terms in our loss function: one that rewards fitting the data well, and another one that (softly) constrains the function to be below the true value function. Finding a stable balance between these two terms appears to be one of the most challenging parts of training. We overcome this limitation by proposing an adaptive update method for the penalty, tuning the initial hyperparameter well, and proposing a good stopping criterion. However, even with all these measures, we still observe occasional instability and believe that there is room for improvement, such as developing an adaptive stopping rule.

A second direction for future work is to improve the generalization of our method to objectives not seen in the training set. While our theoretical results guarantee that the constraints in (7) are sufficient to produce a function that lower bounds the GVF, this may require an exponential number of constraints. In practice, we only enforce this constraint for those objective vectors appearing in the training data, which does give us a lower bounding guarantee for arbitrary right-hand sides and objective vectors from outside the training set. We observe empirically that with sufficient samples, our method performs well on out-of-sample objectives, but we do note that our Test Lower Bound column in Table 1 differs from the Train Lower Bound column by a significant amount.

    &  &  &  \\   & & **Skyte Time (s)** & **Prowards Gap (s)** & **Gap to MILP (s)** & **Gap to Random** & **LP Relaxation Solve Time (s)** & **MLP Solve Time (s)** & **Skyte Time (s)** \\   & 250 & 0.0170 \(\) 0.001 & \(\) 5.724 \(\) 0.76 & 12.55 \(\) 0.80 & 11.34 \(\) 0.01 & 3.42 \(\) 0.31 & \(>\) 30 & 613 \(\) 0.2 \\  & 500 & 0.111 \(\) 0.008 & \(\) 14.53 \(\) 0.84 & \(\) 3.75 \(\) 1.65 & 13.14 \(\) 0.01 & 36.07 \(\) 1.52 & \(>\) 90 & 921 \(\) 0.5 \\  & 790 & 0.133 \(\) 0.004 & \(\) 17.66 \(\) 0.40 & \(\) 35.44 \(\) 1.34 & 14.25 \(\) 0.01 & \(>\) 90 & 90 & 198 \(\) 3.6 \\   & 100 & 0.0188 \(\) 0.001 & \(\) 4.40 \(\) 3.70 & 23.21 \(\) 6.25 & 23.12 \(\) 6.25 & 0.57 \(\) 0.22 & 0.20 \(\) 0.02 & 0.031 \(\) 0.001 \\  & 200 & 0.0123 \(\) 0.001 & \(\) 94.52 \(\) 0.09 & 43.59 \(\) 9.10 & 43.59 \(\) 9.10 & 0.60 \(\) 0.04 & 0.67 \(\) 0.04 & 0.683 \(\) 0.016 \\   & 300 & 0.0852 \(\) 0.002 & \(\) 96.22 \(\) 0.56 & 47.98 \(\) 8.60 & 47.98 \(\) 8.60 & 1.41 \(\) 0.04 & 1.55 \(\) 0.04 & 0.174 \(\) 0.019 \\   

Table 2: DSM Heuristic Solver on UFL Instances.