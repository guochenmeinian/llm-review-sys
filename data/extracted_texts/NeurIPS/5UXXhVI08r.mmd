# Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing

Kai Wang\({}^{1,2}\), Fei Yang\({}^{3}\), Shiqi Yang\({}^{1,2}\), Muhammad Atif Butt\({}^{1,2}\), Joost van de Weijer\({}^{1,2}\)

\({}^{1}\)Computer Vision Center, Barcelona, Spain

\({}^{2}\) Universitat Autonoma de Barcelona, Barcelona, Spain

\({}^{3}\) College of Computer Science, Nankai University, Tianjin, China

{kwang,syang,mabutt,joost}@cvc.uab.es, feiyang@nankai.edu.cn

Corresponding author

###### Abstract

Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose _Dynamic Prompt Learning_ (_DPL_) to force cross-attention maps to focus on correct _noun_ words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method _DPL_, based on the publicly available _Stable Diffusion_, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.

## 1 Introduction

Text-to-Image (T2I) is advancing at a revolutionary pace and has demonstrated an unprecedented ability to generate diverse and realistic images [27; 33; 34; 35]. The state-of-the-art T2I models are trained on extremely large language-image datasets, which require huge computational resources. However, these models do not allow for straightforward _text-guided image editing_ and generally lack the capability to control specific regions of a given image.

Recent research on _text-guided image editing_ allows users to easily manipulate an image using only text prompts [5; 9; 14; 15; 24; 42; 43]. In this paper, we focus on prompt based image editing, where we aim to change the visual appearance of a target object (or objects) in the image. Several of the existing methods use DDIM inversion  to attain the initial latent code of the image and then apply their proposed editing techniques along the denoising phase [28; 30; 42]. Nevertheless, current text-guided editing methods are susceptible to unintentional modifications of image areas. We distinguish between two major failure cases: unintended changes of background regions and unintended changes of distractor objects (objects that are semantically or visually related to the target object), as shown in Fig. 1. The functioning of existing editing methods greatly depends onthe accuracy of the cross-attention maps. When visualizing the cross-attention maps (see Fig. 1), we can clearly observe that for DDIM  and Null-Text inversion  these maps do not only correlate with the corresponding objects. We attribute this phenomenon to _cross-attention leakage_ to background and distractor objects. Cross-attention leakage is the main factor to impede these image editing methods to work for complex backgrounds and multi-object images. Similar observations for failure cases are also shown in [9; 14; 19; 22; 28]. For example, Custom Diffusion  fails for _multi-concept composition_ and DiffEdit does not correctly detect the mask region for a given concept.

We conduct a comprehensive analysis of the Stable Diffusion (SD) model and find that the inaccurate attribution of cross-attention (resulting in undesirable changes to background and distractor objects) can be mitigated by exploiting the semantic clusters that automatically emerge from the self-attention maps in the diffusion model. Instead of fine-tuning the diffusion model, which potentially leads to _catastrophic neglect_, we propose replacing prompts related to scene objects with dynamic prompts that are optimized. We propose two main losses for _dynamic prompt learning_ (DPL), one tailored to reduce attention leakage to distractor objects and one designed to prevent background attention leakage. Moreover, better cross-attention maps are obtained when we update the prompts for each timestamp (as implied by _dynamic_). Furthermore, to facilitate meaningful image editing, we use Null-Text Inversion  with classifier-free guidance . In the experiments, we quantitatively evaluate the performance of _DPL_ on images from LAION-5B  dataset. And _DPL_ shows superior evaluation metrics (including CLIP-Score and Structure-Dist) and improved user evaluation. Furthermore, we show prompt-editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting on complex multi-object scenes.

## 2 Related work

For text-guided image editing with diffusion models, various methods [2; 8; 20; 23; 24] leverage CLIP  for image editing based on a pretrained _unconditional_ diffusion model. However, they are limited to the generative prior which is only learned from visual data of a specific domain, whereas the CLIP text-image alignment information is from much broader data. This prior knowledge gap is

Figure 1: Compared with other image editing methods, our _DPL_ achieves more consistent layout when modifying one or more objects in the image and keeping other content frozen. And also the desired editing is correctly delivered to the corresponding area. DiffEdit  method thoroughly fails to detect the editing region in multi-object cases. And Null-Text inversion (NTI ) is unable to perfectly distinguish the objects in the given image since the cross-attention maps are suffering background leakage (yellow circle) and distractor object leakage (red circle). _DPL_ can more successfully localize the objects in the textual prompts, thus benefits for future editing. Here the \(16 16\) cross-attention maps are _interpolated_ to the image size for better view. (image credits: gettyimages)

mitigated by recent progress of text-to-image (T2I) models [6; 10; 18; 33; 34; 38]. Nevertheless, these T2I models offer little control over the generated contents. This creates a great interest in developing methods to adopt such T2I models for controllable image editing. SDEdit , a recent work utilizing diffusion models for image editing, follows a two-step process. It introduces noise to the input image and then applies denoising using the SDE prior to enhance realism and align with user guidance. Imagic  and P2P  attempt structure-preserving editing via Stable Diffusion (SD) models. However, Imagic  requires fine-tuning the entire model for each image and focuses on generating variations for the objects. P2P  has no need to fine-tune the model, it retrains the image structure by assigning _cross-attention_ maps from the original image to the edited one in the corresponding text token. InstructPix2Pix  is an extension of P2P by allowing human-like instructions for image editing. To make the P2P capable of handling real images, Null-Text inversion (NTI)  proposed to update the _null-text_ embeddings for accurate reconstruction to accommodate with the classifier-free guidance . Recently, pix2pix-zero  propose noise regularization and _cross-attention_ guidance to retrain the structure of a given image. However, it only supports image translation from one concept to another one and is not able to deal with multi-concept cases. DiffEdit  automatically generates a mask highlighting regions of the input image by contrasting predictions conditioned on different text prompts. Plug-and-Play (PnP)  demonstrated that the image structure can be preserved by manipulating spatial _features_ and _self-attention_ maps in the T2I models. MnM  clusters self-attention maps and compute their similarity to the cross attention maps to determine the object localization, thus benefits to generate object-level shape variations. StyleDiffusion  adds up a mapping network to modify the value component in cross-attention computation for regularizing the attentions and utilizes P2P for image editing.

These existing methods highly rely on precise _cross-attention_ maps and overlook an important case where the cross-attention maps may not be perfectly connected to the concepts given in the text prompt, especially if there are multiple objects in the given image. Therefore, these imperfect cross-attention maps may lead to undesirable editing leakage to unintended regions. Essentially, our method _DPL_ complements existing methods to have more semantic richer cross-attention maps. There are various diffusion-based image editing methods relying on giving a mask manually [29; 1] or detecting a mask automatically (GSAM-inpaint ) using a pretrained segmentation model. On the contrary, our method only requires textual input and automatically detects the spatial information. This offers a more intuitive editing demand by only manipulating the text prompts.

There are some papers [21; 12] concentrated on object removal or inpainting using user-defined masks [1; 29; 44]. In contrast, the act of placing an object in a suitable position without relying on a mask is an active research domain. This direction usually entails fine-tuning models for optimal outcomes and runs parallel to the research direction explored in this paper.

In this paper, we are also related to the transfer learning for T2I models [22; 37; 11; 13; 25], which aims at adapting a given model to a _new concept_ by given images from the users and bind the new concept with a unique token. However, instead of finetuning the T2I model [25; 13; 37], we learn new concept tokens by personalization in the text embedding space, which is proposed in Textual Inversion . By this means, we avoid updating a T2I model for each concept separately. Furthermore, we step forward to the scenario with complex background or multiple objects, where the previous methods have not considered. In contrast to transfer learning approaches, which aim to ensure that the learned concepts can accurately generate corresponding objects in given images, our proposed approach _DPL_ focuses on enhancing image editing tasks by regularizing cross-attention to disentangle their positions.

## 3 Method

In this section, we first provide a short preliminary and then describe our method to achieve this demand. An illustration of our method is shown in Fig. 2 and Algorithm 1.

### Preliminary

Latent Diffusion Models.We use Stable Diffusion v1.4 which is a Latent Diffusion Model (LDM) . The model is composed of two main components: an autoencoder and a diffusion model. The encoder \(\) from the autoencoder component of the LDMs maps an image \(\) into a latent code \(z_{0}=()\) and the decoder reverses the latent code back to the original image as \((())\). The diffusion model can be conditioned on class labels, segmentation masks or textual input. Let \(_{}(y)\) be the conditioning mechanism which maps a condition \(y\) into a conditional vector for LDMs, the LDM model is updated by the loss:

\[L_{LDM}:=_{z_{0}(x),y,(0,1),t (1,T)}\|-_{}(z_{t},t,_{ }(y))\|_{2}^{2}\] (1)

The neural backbone \(_{}\) is typically a conditional UNet  which predicts the added noise. More specifically, text-guided diffusion models aim to generate an image from a random noise \(z_{T}\) and a conditional input prompt \(\). To distinguish from the general conditional notation in LDMs, we itemize the textual condition as \(=_{}(})\).

**DDIM inversion.** Inversion entails finding an initial noise \(z_{T}\) that reconstructs the input latent code \(z_{0}\) upon sampling. Since we aim at precisely reconstructing a given image for future editing, we employ the deterministic DDIM sampling :

\[z_{t+1}=_{t+1}}f_{}(z_{t},t,)+_{t+1}}_{}(z_{t},t,)\] (2)

where \(_{t+1}\) is noise scaling factor defined in DDIM  and \(f_{}(z_{t},t,)\) predicts the final denoised latent code \(z_{0}\) as \(f_{}(z_{t},t,)=z_{t}-_{t}} _{}(z_{t},t,)/_{t}}\).

**Null-Text inversion.** To amplify the effect of conditional textual input, classifier-free guidance  is proposed to extrapolate the conditioned noise prediction with an unconditional noise prediction. Let \(=_{}()\) denote the null text embedding, then the classifier-free guidance is defined as:

\[_{}(z_{t},t,,)=w_{ }(z_{t},t,)+(1-w)\,_{}(z_{t},t,)\] (3)

where we set the guidance scale \(w=7.5\) as is standard for Stable Diffusion . However, the introduction of amplifier-free guidance complicates the inversion process, and the generated image based on the found initial noise \(z_{T}\) deviates from the original image. Null-text inversion  proposes a novel optimization which updates the null text embedding \(_{t}\) for each DDIM step \(t[1,T]\) to approximate the DDIM trajectory \(\{z_{t}\}_{0}^{T}\) according to:

\[_{_{t}}\|_{t-1}-_{}(_ {t},t,,_{t})\|_{2}^{2}\] (4)

where \(\{_{t}\}_{0}^{T}\) is the backward trace from Null-Text inversion. Finally, this allows to edit real images starting from initial noise \(_{T}=z_{T}\) using the learned null-text \(_{t}\) in combination with P2P .

Figure 2: Dynamic Prompt Learning (_DPL_) first transforms the noun words in the text prompt into dynamic tokens. We register them in the token dictionary and initialize the representations by their original words at time \(\). Using DDIM inversion, we condition the DM-UNet with the original text condition \(\) to get the diffusion trajectory \(\{z_{t}\}_{0}^{T}\) and background mask \(\). In each denoising step \(_{t-1}_{t}\), we first update the dynamic token set \(_{t}\) with a background leakage loss, a disjoint object attention loss and an attention balancing loss, in order to ensure high-quality cross-attention maps. Then we apply Null-Text inversion to approximate the diffusion trajectory.

Note, that the cross-attention maps of Null-Text inversion (\(w=7.5\)) and DDIM inversion (\(w=1.0\)) are similar, since NTI is aiming to approximate the same denoising path.

Prompt2Prompt.P2P  proposed to achieve editing by manipulating the cross-attention \(_{t}^{*}\) from target input \(^{*}\) according to the cross-attention \(_{t}\) from the source input \(\) in each timestamp \(t\). The quality of the estimated attention maps \(_{t}\) is crucial for the good functioning of P2P. This paper addresses the problem of improved \(_{t}\) estimation for complex images.

### _Dpl_: Dynamic Prompt Learning for addressing cross-attention leakage

Prompt-based image editing takes an image \(\) described by an initial prompt \(\), and aims to modify it according to an altered prompt \(^{*}\) in which the user indicates desired changes. The initial prompt is used to compute the cross-attention maps given the conditional text input \(\). As discussed in Sec. 1, the quality of cross-attention maps plays a crucial role for prompt-based image editing methods (see Fig. 1), since these attention maps will be used during the generation process with the altered user prompt . We identify two main failures in the attention map estimation: **(1)** distractor object leakage, which refers to wrong assignment of attention to a semantically or visually similar foreground object and **(2)** background leakage, referring to undesired attention on the background. Both of these problems result in artifacts or undesired modifications in generated images.

The cross-attention maps in the Diffusion Model UNet are obtained from \(_{}(z_{t},t,)\), which is the first component in Eq. 3. They are computed from the deep features of noisy image \((z_{t})\) which are projected to a query matrix \(Q_{t}=l_{Q}((z_{t}))\), and the textual embedding which is projected to a key matrix \(K=l_{K}()\). Then the attention map is computed according to:

\[_{t}=softmax(Q_{t} K^{T}/)\] (5)

where \(d\) is the latent dimension, and the cell \([_{t}]_{ij}\) defines the weight of the \(j\)-th token on the pixel \(i\).

In this paper, we propose to optimize the word embeddings \(v\) corresponding to the initial prompt \(\), in such a way that the resulting cross-attention maps \(_{t}\) do not suffer from the above-mentioned problems, _i.e._ distractor-object and background leakage. We found that optimizing the embedding \(v_{t}\) separately for each timestamp \(t\) leads to better results than merely optimizing a single \(v\) for all timestamps. The initial prompt \(\) contains \(K\) noun words and their corresponding learnable tokens at each timestamp \(_{t}=\{v_{t}^{1},...,v_{t}^{k}...,v_{t}^{K}\}\). We update each specified word in \(_{t}\) for each step \(t\).

The final sentence embedding \(_{t}\) now varies for each timestamp \(t\) and is computed by applying the text transformer on the text embeddings: some of which are learned (those referring to nouns) while the rest remains constant for each timestamp (see Fig. 2). For example, given a prompt "a puppy and a rabbit" and the updatable noun set {puppy, rabbit}, in each timestamp \(t\) we have transformed the textual input into trainable ones as "a <puppy> and a <rabbit>" and we aim to learn word embeddings \(v_{t}^{}\) and \(v_{t}^{}\) in each timestamp \(t\).

We propose several losses to optimize the embedding vectors \(_{t}\): we develop one loss to address cross-attention leakage due to object distractors, and another one to address background leakage. We also found that the attention balancing loss, proposed by Attend-and-Excite , was useful to ensure each specified noun word from \(\) is focusing on its relevant concept.

Disjoint Object Attention Loss.To address the problem of distractor object attention leakage, we propose a loss that minimizes the attention overlap between the attention maps corresponding to the different objects, that is an updatable noun set \(_{t}\). This prevents specific objects related regions to be linked to multiple nouns, which is one of the sources of distractor object attention. We propose to minimize a cosine similarity loss among the \(K\) cross-attention maps (referring to the noun words in the input prompt \(\)) to avoid their overlapping in cross-attention maps through:

\[_{dj}=_{i=1}^{K}_{j=1\\ i j}^{K}(_{t}^{v_{t}^{i}}, _{t}^{v_{t}^{j}})\] (6)

Background Leakage Loss.In order to alleviate the background leakage issue, we propose to utilize background masks to keep the background area unchanged. Details on how to get the background mask are discussed in Sec. 3.3. After obtaining the BG mask \(\) of the given image, for each \(v_{t}^{k}_{t}\)we force its cross-attention map to have minimal overlap with the background region:

\[_{bg}=_{i=1}^{K}(_{t}^{v_{t}^{i}},)\] (7)

#### 3.2.2 Attention Balancing Loss.

For each \(v_{t}^{k}_{t}\), we pass its cross-attention map through a Gaussian filter \(\) and formulate a set of losses \(_{v_{t}^{k}}\) for all words in \(_{t}\). The attention loss is defined as the maximum value in \(_{v_{t}^{k}}\):

\[_{at}=_{v_{t}^{k}_{t}}_{v_{t}^{k}} _{v_{t}^{k}}=1-[(_{t}^{v_{t}^{k}})]\] (8)

The attention loss will strengthen the cross-attention activations for _localizing_ token \(v_{t}^{k}\), thus force \(_{t}^{v}\) to not leak attentions to other irrelevant concepts in the image. In other words, the resulting cross-attention maps will be more concentrated on the corresponding object region. Different from Attend-and-Excite  where the refinement loss is applied to update the latent code to augment the multiple object generation, we are the first to explore its role in learning new conceptual tokens.

With all these three losses, we aim to update the learnable token \(v_{t}^{k}\) as:

\[*{arg\,min}_{_{t}} =_{at}_{at}+_{dj}_{dj }+_{bg}_{bg}\] (9)

Note that, when there is only one updatable noun token in the text input \(\), we only apply the \(_{bg}\) to relieve the background leakage since other losses rely on at least two words. In this paper, we apply _DPL_ over the \(16^{2}\) attention following the common practices [15; 7] for a fair comparison.

#### 3.2.3 Gradual Optimization for Token Updates.

So far, we introduced the losses to learn new dynamic tokens at each timestamp. However, the cross-attention leakage is gradually accumulated in the denoising phase. Hence, we enforce all losses to reach a pre-defined threshold at each timestamp \(t\) to avoid overfitting of the cross-attention maps. We express the gradual threshold by an exponential function. For the losses proposed above, the corresponding thresholds at time \(t\) are defined as:

\[TH_{t}^{at}=_{at}(-t/_{at}); TH_{t}^{dj}=_{dj} (-t/_{dj}); TH_{t}^{bg}=_{bg}(-t/_{bj}).\] (10)

We verify the effectiveness of this mechanism together with other hyperparameters in our ablation experiments (see Fig. 5-(a) and Supplementary Material.).

#### 3.2.4 Null-Text embeddings.

The above described token updating ensures that the cross-attention maps are highly related to the noun words in the text prompt and minimize cross-attention leakage. To

Figure 3: After DDIM inversion, we visualize the average of each component over \(T\) timestamps for the input image in Fig. 1 (with the prompt: “a cat and a dog”). From left to right, they are cross-attention (CA) for each _noun_ word, then PCA&clustering of the self-attention (SA), query, key and value representations. Here, the maps are simply _resized_ without any interpolation to demonstrate their original views.

be able to reconstruct the original image, we use Null-Text inversion  in addition to learn a set of null embeddings \(_{t}\) for each timestamp \(t\). Then at the time \(t\), we have a set of learnable word embeddings \(_{t}\) and null text \(_{t}\) which can accurately localize the objects and also reconstruct the original image.

### Attention-based background estimation

To address the background attention leakage, we investigate the localization information of self-attention and feature representations, which are present in the diffusion model. Besides cross-attention (CA), during the inversion of an image with the DDIM sampler, we can also obtain self-attention maps (SA), or directly analyze the query, key and value in the self-attention maps. In Fig. 3 we visualize these maps. The main observation is that self-attention maps more closely resemble the semantic layout. We therefore exploit these to reduce background leakage.

To acquire the background via self-cross attention matching, we use the agreement score  between cross-attention \(^{n}\) of noun words \(n\) and each of the self-attention clusters \(_{v},v[1,V]\), defined as \(_{nv}=(^{n}_{v})/_{v}\).

However, it should be noted that the agreement score does not accurately segment the objects, since there is still attention leakage between objects and distractor objects (see Fig. 4 for examples). Therefore, other than localizing objects as in , we use the agreement score only for background mask estimation. We label the cluster \(_{v}\) as being background (BG) if for all \(n\) if \(_{nv}<TH\), where \(TH\) is a threshold. Empirically, we choose self-attention with size 32, cross-attention with size 16 and \(TH=0.2,V=5\) to obtain the background mask \(\).

```
1Input: A source prompt \(\), an input image \(\)
2Output: A noise vector \(_{T}\), a set of updated tokens \(\{_{t}\}_{1}^{T}\) and null-text embeddings \(\{_{t}\}_{1}^{T}\)
3 Set \(T=50\) and scale \(w=1\);
4\(\{z_{t}\}_{0}^{T},\)DDIM-inv(\(\));
5 Set guidance scale \(w=7.5\);
6 Initialize \(_{T}\) with original noun tokens;
7 Initialize \(_{T}=z_{T},_{T}=,_{T}=_{}( "")\);
8for\(t=T,T-1,,1\)do
9 Initialize \(_{t}\) by \(_{t}\), then \(_{t}=_{}(_{t})\);
10while\(_{at} TH_{t}^{at}\)or\(_{dj} TH_{t}^{dj}\)or\(_{bg} TH_{t}^{bg}\)do
11\(=_{at}_{at}+_{dj}_{dj} +_{bg}_{bg}\)\(_{t}_{t}-_{_{t}}\)
12 end for
13 Update \(_{t}\) by \(_{t}\), then \(_{t}=_{}(_{t})\);
14\(_{t}=_{}(_{t},t,_{t},_{t})\)\(_{t-1},_{t} NTI(_{t},_{t})\)
15 Initialize \(_{t-1}=_{t},_{t-1}=_{t}\)
16 end for Return \(_{T},\{_{t}\}_{1}^{T},\{_{t}\}_{1}^{T}\) ```

**Algorithm 1**Dynamic Prompt Learning

## 4 Experiments

We demonstrate our method in various experiments based on the open-source T2I model Stable Diffusion  following previous methods [42; 30; 28]. All experiments are done on an A40 GPU.

Figure 4: Although cross-attention maps produce acceptable results in certain cases (up) with minor leakage, the object masks from self-cross attention matching are not always accurate due to the leakage problem (down). However, as the cross-attention maps tend to concentrate on the foreground objects, the background masks remain reliable.

**Dataset.** We focus here on real-image editing, but also provide examples of generated image editing in the Supplementary Material. We use clip retrieval  to obtain experimental multi-object real images from the LALON-5B  dataset by matching CLIP embeddings of the source text description. For the ablation study, we select 100 multi-object images from various search prompts (including concepts as bird, dog, cat, book, clock, pen, pizza, table, horse, sheep, etc.). This Multi-Object set is abbreviated with _MO-Set_. Of the images, 40 images are used for validation and the other 60 are test set. We manually verify their segmentation maps which serve as groundtruth in the ablation study IoU computation. Finally, for user study, we create a _URS-Set_ of 60 multi-object images of semantically related objects from various concepts.

**Comparison methods.** For qualitative comparison, we compare with pix2pix-zero , GSAMI-inpaint , InstructPix2Pix , Plug-and-Play  and NTI+P2P . For quantitative evaluation, we benchmark our approach against NTI+P2P, considering it as a robust baseline for maintaining content stability and achieving superior conceptual transfer.

### Ablation study

The ablation study is conducted on the _MO-Set_ set. To quantitatively evaluate the performance of _DPL_ in localization, we vary the threshold from \(0.0\) to \(1.0\) to obtain the segmentation mask from the cross-attention maps and compute the IoU metric using the segmentation groundtruth for comparison.

In Fig. 5-(a), we compare the localizing performance with Null-Text inversion (NTI), _DPL_ with each proposed loss separately and our final model. We can observe that the disjoint object attention loss and the background leakage loss are not able to work independently to improve the cross-attention map. The attention balancing loss can enhance the cross-attention quality and can be further boosted by the other two proposed losses. We use the validation set to optimize the various hyperparameters (see Eq. 9 and Eq. 10). Using these parameters, we show similar results on the independent test-set (Fig. 5-(a)). These parameters are set for all further results (more details are provided in Supplementary). In Fig. 5-(b), we qualitatively verify our method on some examples of comparison with cross-attention maps. _DPL_ improves the cross-attention quality than NTI as the baseline method.

### Image editing evaluation

For image editing, we combine our proposed _DPL_ with the P2P  image editing method. Here we mainly focus on Word-Swap but also include more results for Prompt Refinement, and Attention Re-weighting in the supplementary material.

**Word-Swap.** In the word swapping scenario, we quantitatively compare NTI and _DPL_ under two scenarios shown in Table 1. For each case, we use the collected 20 images with searching prompts "a cat and a dog" and "a box and a clock" to estimate the evaluation metrics, including CLIP-Score  and Structure Dist . In both cases, _DPL_ is getting better performance than the NTI baseline (more shown in Supplementary).

Furthermore, we conduct a forced-choice user study involving 20 evaluators and the assessment of 60 image editing pairs. Users were asked to evaluate 'which result does better represent the prompt change?' (more protocol choices can be found in Supplementary). We can observe a significantly higher level of satisfaction among the participants with our method when compared to NTI.

Figure 5: (a) IoU curves drawn by varying the threshold from 0.0 to 1.0 for _MO-Set_. (b) We show some images for comparisons between the Null-Text inversion  and our method _DPL_ demonstrated in cross-attention maps.

Qualitatively, we show word-swap examples on multi-object images in Fig. 7. In contrast to other methods, our proposed approach (_DPL_) effectively addresses the issue of inadvertently editing the background or distractor objects in the provided images.

Attention refinement and re-weighting.We also compare NTI and _DPL_ by only tuning a single concept token with additional adjective descriptions in the image in Fig. 6. As can be observed, _DPL_ keeps better details of the "cake" and modifies its appearance according to the new word. NTI suffers from the cross attention leakage to the background, thus it cannot successfully edit only the "cake" region and distorts the "flowers".

## 5 Conclusions

We presented _DPL_ to solve the cross-attention background and distractor object leakage problem in image editing using text-to-image diffusion models. We propose to update the dynamic tokens for each noun word in the prompt in such a way that the resulting cross-attention maps suffer less from attention leakage. The greatly improved cross-attention maps result in considerably better results for text-guided image editing. The experimental results, confirm that _DPL_ obtains superior results, especially on complex multi-object scenes. In the Supplementary Material, we further provide a comprehensive discussion on the limitations of our work, its broader impact, and potential directions for future research.

Limitations and future works.One limitation of our work is that the smaller cross-attention maps, specifically those with a size of 16\(\)16, contain a greater amount of semantic information compared to maps of larger sizes. While this rich information is beneficial, it limits our ability to achieve precise fine-grained structure control. Furthermore, our current approach has not addressed complex scenarios where a single object in the image corresponds to multiple noun words in a long caption. This remains an unexplored area and presents an opportunity for future research. Additionally, the Stable Diffusion model faces challenges when reconstructing the original image with intricate details due to the compression mechanism in the first stage autoencoder model. The editing of high-frequency information remains a significant and ongoing challenge that requires

    &  &  &  \\    & CLIP-Score (\(\)) & Structure Dist (\(\)) & CLIP-Score (\(\)) & Structure Dist (\(\)) & \\  Original Images & 32.50\% & - & 16.46\% & - & - \\ NTI  + P2P & 31.50\% & 0.0800 & 12.91\% & 0.0191 & 12.8\% \\ _DPL_ (ours) + P2P & **32.23\%** & **0.0312** & **14.29\%** & **0.0163** & **87.2\%** \\   

Table 1: Comparison between our method _DPL_ +P2P and the baseline NTI+P2P by modifying multiple objects simultaneously. we examined the quality of editing by transforming an image depicting ”a cat and a dog” into ”a leopard and a tiger”. We evaluated the editing quality of both approaches using objective metrics, such as CLIP-Score and Structure-Dist, across these multi-object collections. Additionally, to capture the subjective opinions of users, we conducted a user study involving 60 pairs of multi-object image translations.

Figure 6: Attention refinement and reweighting. In the given text prompt, only one noun word is learnable. The flowers below the cake are playing as distractors which distort the cross-attention maps while Null-Text Inversion  is applied. As a comparison, our method _DPL_ successfully filters the cross-attention by our background leakage loss.

further investigation and development. Addressing these limitations and advancing our understanding in these areas will contribute to the improvement and refinement of image editing techniques.

**Broader impacts.** The application of text-to-image (T2I) models in image editing offers extensive potential for diverse downstream applications, facilitating the adaptation of images to different contexts. The primary objective of our model is to automate and streamline this process, resulting in significant time and resource savings. It is important to acknowledge that current methods have inherent limitations, as discussed in this paper. However, our model can serve as an intermediary solution, expediting the creation process and offering valuable insights for further advancements. It is crucial to remain mindful of potential risks associated with these models, including the dissemination of misinformation, potential for abuse, and introduction of biases. Broader impacts and ethical considerations should be thoroughly addressed and studied in order to responsibly harness the capabilities of such models.

Acknowledgments.We acknowledge the support from the Spanish Government funding for projects PID2022-143257NB-I00, TED2021-132513B-I00 funded by MCIN/AEI/10.13039/501100011033 and by FSE+ and the European Union NextGenerationEU/PRTR, and the CERCA Programme of Generalitat de Catalunya. We also thank for the insightful discussion with Juan A Rodriguez, Danna Xue, and Marc Paz.

Figure 7: Word-Swap by only modifying the regions corresponding to the target concept. We list the source prompts on the left with _dynamic_ tokens in the curly brackets. Each line is corresponding to modifying one or multiple concepts with various methods.