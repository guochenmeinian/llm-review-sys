# Error Bounds for Learning with

Vector-Valued Random Features

 Samuel Lanthaler

California Institute of Technology

slanth@caltech.edu

&Nicholas H. Nelsen

California Institute of Technology

nnelsen@caltech.edu

Equal Contribution

###### Abstract

This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with Monte Carlo intuition and free from logarithmic factors.

## 1 Introduction

Supervised learning of an unknown mapping \(\) is a core task in machine learning. The _random feature model_ (RFM), proposed in , combines randomization with optimization to accomplish this task. The RFM is based on a linear expansion with respect to a randomized basis, the _random features_ (RF). The coefficients in this RF expansion are optimized to fit the given data of input-output pairs. For popular loss functions, such as the square loss, the RFM leads to a convex optimization problem which can be solved efficiently and reliably.

The RFM provides a scalable approximation of an underlying kernel method . While the former is based on an expansion in \(M\) random features \((\,\,;_{1}),,(\,\,;_{M})\), the corresponding kernel method relies on an expansion in values of a positive definite kernel function \(K(\,\,,u_{1}),,K(\,\,,u_{N})\) on a dataset of size \(N\). Kernel methods are conceptually appealing, theoretically sound, and have attracted considerable interest . However, they require the storage, manipulation, and often inversion of the kernel matrix \(\) with entries \(K(u_{i},u_{j})\). The size of \(\) scales quadratically in the number of samples \(N\), which can be prohibitive for large datasets. When the underlying input-output map is vector-valued with \(()=p\), the often significant computational cost of kernel methods is further exacerbated by the fact that each entry \(K(u_{i},u_{j})\) of \(\) is, in general, a \(p\)-by-\(p\) matrix. Hence, the size of \(\) scales quadratically in both \(N\) and \(p\). This severely limits the applicability of kernel methods to problems with high-dimensional, or indeed infinite-dimensional, output space. In contrast, learning with RF only requires storage of RF matrices whose size is quadratic in the number of features \(M\). When \(M Np\), this implies substantial computational savings, with the most extreme case being the infinite-dimensional setting in which \(p=\).

In the context of operator learning, the underlying target mapping is an operator \(\) with infinite-dimensional input and output spaces. Such operators appear naturally in scientific computingand often arise as solution maps of an underlying partial differential equation. Operator learning has attracted considerable interest, e.g., , and in this context, the RFM serves as an alternative to neural network-based methods with considerable potential for a sound theoretical basis. Indeed, an extension of the RFM to this infinite-dimensional setting has been proposed and implemented in . Although the results show promise, a mathematical analysis of this approach including error bounds and rates of convergence has so far been outstanding.

Related work.Several papers have derived error bounds for learning with RF. Early work on the RFM  proceeded by direct inspection of the risk functional, demonstrating that \(M N\) random features suffice to achieve a squared error \(O(1/)\) for RF ridge regression (RR). This result was considerably improved in , where \( N\) random features were shown to be sufficient to achieve the same squared error. This improvement in parameter complexity is based on the explicit RF RR solution formula, combined with extensive use of matrix analysis and matrix concentration inequalities. Similar analysis in  sharpens the parameter complexity to \( d_{}^{}\) random features. Here \(d_{}^{}\) is the _number of effective degrees of freedom_, with \(\) the RR regularization parameter and \(\) the kernel matrix. In this context, we also mention related analysis in . In all these works, the squared error in terms of sample size \(N\) match the minimax optimal rates for kernel RR derived in . Going beyond the above error bounds,  also derive fast rates under additional assumptions on the underlying data distribution and/or with improved RF sampling schemes.

Many works also study the interpolatory \((M N)\) or overparametrized \((M N)\) regimes in the scalar output setting . However, when \(p=() 1\) or \(p=\), such regimes may no longer be relevant. This is because the kernel matrix \(\) now has size \(Np\)-by-\(Np\), and it is possible that the number of random features \(M\) satisfies \(M Np\) even though \(M N\). In this case, high-dimensional vector-valued learning naturally operates in the underparametrized regime.

In the area of operator learning for scientific problems, approximation results are common  but statistical guarantees are lacking, the main exceptions being  in the linear operator setting and  in the nonlinear setting. The RFM also has potential for such nonlinear problems. Indeed, vector-valued random Fourier features have been studied before . However, theory is only provided for kernel approximation, not generalization guarantees.

To summarize, while previous analyses have provided considerable insight into the generalization properties of the RFM, they have almost exclusively focused on the scalar-valued setting. Given the paucity of theoretical work beyond this setting, it is a priori unclear whether similar estimates continue to hold when the RFM is applied to infinite-dimensional vector-valued mappings.

Contributions.The primary purpose of this paper is to extend earlier results on learning with random features to the vector-valued setting. The theory developed in this work unifies sources of error stemming from approximation, generalization, misspecification, and noisy observations. We focus on training via ridge regression with the square loss. Our results differ from existing work not only in the scope of applicability, but also in the strategy employed to derive our results. Similar to , we do not rely on the explicit random feature ridge regression solution (RF-RR) formula, which is specific to the square loss. One main benefit of this approach is that it entirely avoids the use of matrix concentration inequalities, thereby making the extension to an infinite-dimensional vector-valued setting straightforward. Our main contributions are now listed (see also Table 1).

1. Given \(N\) training samples, we prove that \(M\) random features and regularization strength \( 1/\) is enough to guarantee that the squared error is \(O(1/)\), provided that the target operator belongs to a specific reproducing kernel Hilbert space (Thm. 3.7);
2. we establish that the vector-valued RF-RR estimator is strongly consistent (Thm. 3.10);
3. under additional regularity assumptions, we derive rates of convergence even when the target operator does not belong to the specific reproducing kernel Hilbert space (Thm. 3.12);
4. we demonstrate that the approach of Rahimi and Recht  can be used to derive state-of-the-art rates for the RFM which, for the first time, are free from logarithmic factors.

Outline.The remainder of this paper is organized as follows. We set up the ridge regression problem in Sect. 2. The main results are stated in Sect. 3 and their proofs are sketched in Sect. 4.

Sect. 5 provides a simulation study and Sect. 6 gives concluding remarks. Detailed proofs are deferred to the supplementary material (**SM**).

## 2 Preliminaries

We now set up our vector-valued learning framework by introducing notational conventions, reviewing random features, and formulating the ridge regression problem.

Notation.Let \((,,)\) be a sufficiently rich probability space on which all random variables in this paper are defined. Let \(\) be the input space, \(\) the output space, and \(\) a set. We consistently use \(u\) to denote elements of \(\) and \(\) for RF parameters in \(\). The set of probability measures supported on a set \(\) is denoted by \(()\). We write expectation (in the sense of Bochner integration) with respect to \(u()\) as \(_{u}[\,\,]\) and similarly for \(()\). Independent and identically distributed (iid) samples \(u_{1},,u_{N}\) from \(\) will be denoted by \(\{u_{n}\}^{ N}\) and similarly for \(\{_{m}\}^{ M}\). We write \(a b\) to mean that there exists a constant \(C 1\) such that \(C^{-1}b a Cb\) and similarly for the one-sided inequalities \(a b\) and \(a b\). We define \(a b(a,b)\).

Random features and reproducing kernel Hilbert spaces.Random features are defined by a pair \((,)\), where \(\) and \(()\). Fixing \(\) defines a map \((\,\,;)\). Considering linear combinations of such maps leads to the following definition.

_Definition 2.1_ (Random feature model).: The map \((\,\,;)=(\,\,;,\{_{m}\}) \) given by

\[u(u;)_{m=1}^{M}_{m}(u; _{m})\] (2.1)

is a _random feature model_ (RFM) with coefficients \(^{M}\) and fixed realizations \(\{_{m}\}^{ M}\).

Associated to the pair \((,)\) is a reproducing kernel Hilbert space (RKHS) \(\) of maps from \(\) to \(\)[32, Sect. 2.3]. Under mild assumptions (see **SM** B) assumed in our main results, it holds that

\[= L^{2}_{}(;)\, \,=[()(\,\,;)] { and } L^{2}_{}(;)}\] (2.2)

with RKHS norm \(\|\|_{}=_{}\|\|_{L^{2}_{}}\), where \(\) ranges over all decompositions of \(\) of the form in (2.2). A minimizer \(_{}\) of this problem always exists [2, Sect. 2.2]. We use this fact to identify any \(\) with its minimizing \(_{} L^{2}_{}(;)\) without further comment.

Random feature ridge regression.Let \(()\) be the _joint data distribution_. The goal of RF-RR is to estimate an underlying operator \(\) from finitely many iid input-output pairs \(\{(u_{n},y_{n})\}_{n=1}^{N}^{ N}\), where typically the \(y_{n}\) are noisy transformations of the point values \((u_{n})\). To describe RF-RR, we first make some definitions.

_Definition 2.2_ (Empirical risk).: Writing \(Y=\{y_{n}\}\) for the collection of observed output data and fixing a regularization parameter \(>0\), the _regularized \(Y\)-empirical risk_ of \(^{M}\) is given by

\[^{}_{N}(;Y)_{n=1}^{N}\|y_{n}- (u_{n};)\|_{}^{2}+\|\|_{M}^{2}\,, {where}\|\|_{M}^{2}_{m=1}^{M}|_{m}|^{2}\] (2.3)

is a scaled Euclidean norm on \(^{M}\). The _regularized \(\)-empirical risk_, \(^{}_{N}(;)\), is defined analogously with \((u_{n})\) in place of \(y_{n}\). In the absence of regularization, i.e., \(=0\), these expressions define the \(Y\)_-empirical risk_ and \(\)_-empirical risk_, denoted by \(_{N}(;Y)\) and \(_{N}(;)\), respectively.

   Paper & Approach & \(\) & \(()\) & \(M\) & Squared Error \\  Rahimi \& Recht  & “kitchen sinks” & n/a (\({}^{}\)) & 1 & \(N\) & \(R/\) \\ Rudi \& Rosasco  & matrix concen. & \(1/\) & 1 & \((N)\) & \(1/\) \\ Li et al.  & matrix concen. & \(1/\) & 1 & \((d^{}_{})\) & \(1/\) \\
**This work** & “**kitchen sinks**” & \(/}\) & \(\) & \(}\) & \(/}\) \\   

Table 1: A summary of available error estimates for the RFM, with regularization parameter \(\), output space \(\), and number of random features \(M\). (\({}^{}\)): the truth is assumed to be written as \((u)=_{}[^{}()(u;)]\) with restrictive bound \(|^{}()| R\) to avoid explicit regularization.

RF-RR is the minimization problem \(_{^{M}}_{N}^{}(;Y)\). The minimizer, which we denote by \(\), is referred to as _trained coefficients_ and \((\,\,;)\) the _trained RFM_. For \(M\) and \(N\) sufficiently large and \(>0\) sufficiently small, we expect the trained RFM to well approximate \(\). This intuition is made precise by quantitative error bounds and statistical performance guarantees in the next section.

## 3 Main results

The main result of this paper is an abstract bound on the population squared error (Sect. 3.2). From this widely applicable theorem, several more specialized results are deduced. These include consistency (Sect. 3.3) and convergence rates (Sect. 3.4) of the RF-RR estimator trained on noisy data. The assumptions under which this theory is developed are provided next in Sect. 3.1.

### Assumptions

Throughout this paper, we assume that the input space \(\) is a Polish space and the output space \(\) is a real separable Hilbert space. These are common assumptions in learning theory . We view \(\) and \(\) as measurable spaces equipped with their respective Borel \(\)-algebras.

Next, we make the following minimal assumptions on the random feature pair \((,)\).

**Assumption 3.1** (Random feature regularity).: _Let \(()\) be the input distribution and \((,,)\) be a probability space. The random feature map \(\) and the probability measure \(()\) are such that (i) \(\) is measurable; (ii) \(\) is uniformly bounded; in fact, \(\|\|_{L^{}}_{(u,) }\|(u;)\|_{} 1\); and (iii) the RKHS \(\) corresponding to \((,)\) is separable._

The boundedness assumption on \(\) is shared in general theoretical analyses of RF [26; 37; 38]; the unit bound can always be ensured by a simple rescaling. We work in a general misspecified setting.

**Assumption 3.2** (Misspecification).: _There exist \( L^{}_{}(;)\) and \(_{}\) such that the operator \(\) satisfies the decomposition \(=+_{}\)._

Since Assumption 3.1 implies that \( L^{}_{}(;)\), any \(=_{}+\) as in Assumption 3.2 is automatically bounded in the sense that \( L^{}_{}(;)\). Conversely, any \( L^{}_{}(;)\) allows such a decomposition. We interpret \(\) as a residual from the operator \(_{}\) belonging to the RKHS. It may be prescribed by the problem, as we will see later in the context of discretization errors in operator learning (Ex. 3.9), or be arbitrary, as is customary in learning theory when the only information about \(\) is that it is bounded.

Our main goal is to recover \(\) from iid data \(\{(u_{n},y_{n})\}\) arising from the following statistical model.

**Assumption 3.3** (Joint data distribution).: _The joint distribution \(()\) of the random variable \((u,y)\) is given by \(u\) with \(()\) and \(y=(u)+\). Here, \(\) satisfies Assumption 3.2. The additive noise \(\) is a random variable in \(\) that is conditionally centered, \([\,|\,u]=0\), and is subexponential: \(\|\|_{_{1}()}<\); see (A.7) for the definition of \(\|\|_{_{1}()}\)._

Assumption 3.3 implies that \((u)=[y\,|\,u]\). In contrast to related work [2; 26; 37], we allow for unbounded input-dependent noise. In particular, our results also hold for bounded or subgaussian noise, as well as _multiplicative noise_ (e.g., \(=(u)\) with \([\,|\,u]=0\) and \(\|\|_{_{1}}<\)).

### General error bound

For any \(\), define the \(\)_-population risk functional_ or \(\)_-population squared error_ by

\[(;)_{u}\|(u )-(u;,\{_{m}\})\|_{}^{2} ^{M}\,.\] (3.1)

The main result of this paper establishes an upper bound for this quantity that holds with high probability, provided that the number of random features and number of data pairs are large enough.

**Theorem 3.4** (\(\)-population squared error bound).: _Suppose that \(=+_{}\) satisfies Assumption 3.2. Fix a failure probability \((0,1)\), regularization strength \((0,1)\), and sample size \(N\). Let \(\{_{m}\}^{ M}\) be the \(M\) random feature parameters and \(\{(u_{n},y_{n})\}^{ N}\) be the data according to Assumption 3.3. For \(\) the RFM (2.1) satisfying Assumption 3.1, let \(^{M}\) be the minimizer of the regularized \(Y\)-empirical risk \(^{}_{N}(\,\,;Y)\) given by (2.3). If \(M^{-1}(32/)\) and \(N^{-2}(16/)\), then_

\[_{u}\|(u)-(u;,\{_{m}\}) \|_{}^{2} 79e^{3/2}\|\|_{L^{}_{}}^{2}+2 (,,_{},)\] (3.2)

_with probability at least \(1-\), where_

\[(,,_{},) 328\|_{ }\|_{}^{2}+2023e^{3}\|\|_{_{1}()}^ {2}+8^{-1}\,_{u}\|(u)\|_{}^{2}+18 \|\|_{L^{}_{}}^{2}\] (3.3)

_is a function of \(\), \(\), \(_{}\), and the law of the noise variable \(\)._

The main elements of the proof of Thm. 3.4 will be explained in Sect. 4.

_Remark 3.5_ (Excess risk).: We note that other work  often focuses on bounding the _excess risk_\(}((\,\,;)) -_{_{}}(_{ })\), where \((F)\|y-F(u)\|_{}^{2}= _{u}\,\|(u)-F(u)\|_{}^{2}+ \|\|_{}^{2}\). In particular, this bias-variance decomposition implies that \(}_{u}\|(u)-(u; )\|_{}^{2}\). Thus, Thm. 3.4 also gives a corresponding bound on the excess risk \(}\).

_Remark 3.6_ (The \(\) factor).: In the well-specified setting, that is, \(-_{}= 0\), the factor \(\) in Thm. (3.4) satisfies the uniform bound

\[(,,_{},) B 328\| \|_{}^{2}+2023e^{3}\|\|_{_{1}()}^{2}\,.\] (3.4)

In particular, the constant \(B\) does not depend on \(\) in this case. Otherwise, \(\) in general depends on \(\). We can characterize this dependence precisely if it is known that \( L^{}_{}(;)\). In this case, Assumption 3.2 is satisfied with \(-_{}\)_for any \(_{}\)_. Choosing \(_{}=_{}|_{= }\) as in **SM** B (which is optimal in the sense described there) and a short calculation deliver the bound

\[(,,_{},)^{-1} ^{r 1}=^{-(1-r)_{+}}\] (3.5)

if \(\) additionally satisfies a particular \(r\)-th order regularity condition (see Lem. B.3 for the details). Here, \(a_{+}(a,0)\) for any \(a\). Thus, \(\) is uniformly bounded if \(\)\((r 1)\) and grows algebraically as a power of \(^{-1}\) otherwise \((0 r<1)\).

Consequences.The general error bound (3.2) in Thm. 3.4 has several implications for vector-valued learning with the RFM. First, it immediately implies a rate of convergence if \(\).

**Theorem 3.7** (Well-specified).: _Instantiate the hypotheses and notation of Thm. 3.4. Suppose that \( 0\) so that \(\) (2.2). If \(M^{-1}(32/)\) and \(N^{-2}(16/)\), then_

\[_{u}\|(u)-(u;)\|_{ }^{2} 79e^{3/2}\|\|_{L^{}_{}}^{2}+2B \] (3.6)

_with probability at least \(1-\), where the constant \(B 0\) is defined by (3.4)._

Given a number of samples \(N\), Thm. 3.7 shows that RF-RR with regularization \( 1/\) and number of features \(M\) leads to a population squared error of size \(1/\) with high probability. This result should be compared to the previous state-of-the-art convergence rates in the literature for RF-RR with iid sampled features . See Table 1, which indicates that our analysis gives the lowest parameter complexity to date. We emphasize that such a convergence rate rests on the assumption that \(\). This corresponds to a _compatibility condition_ between \(\) and the pair \((,)\), i.e., the random feature map \(\) and the probability measure \(\), which determine the RKHS \(\). Designing suitable \(\) and \(\) for a given operator \(\) remains an open problem. For an explanation of the poor parameter complexity in Rahimi and Recht's original paper , see [44, Appendix E].

Thm. 3.4 also implies convergence of \((;)\) when \(\), as we will see in Sect. 3.3 and 3.4. But first, the next corollary shows that the same general bound (3.2) also holds for the \(_{}\)-population squared error \((;_{})\), up to enlarged constant factors. The proof is given in **SM** C.

**Corollary 3.8** (\(_{}\)-population squared error bound).: _Instantiate the hypotheses and notation of Thm. 3.4. If \(M^{-1}(32/)\) and \(N^{-2}(16/)\), then there exists an absolute constant \(C>1\) such that with probability at least \(1-\), it holds that_

\[_{u}\|_{}(u)-(u;)\|_{ }^{2} C\|\|_{L^{}_{}}^{2}+2( ,,_{},)\,.\] (3.7)

Although our main goal is to learn \(\) from noisy data, there are settings instead in which the learning of \(_{}\) as in Cor. 3.8 is of primary interest, but only values of some approximation \( L^{}_{}(;)\) are available. The following example illustrates this.

_Example 3.9_ (Numerical discretization error).: One practically relevant setting to which Cor. 3.8 applies arises when training a RFM from functional data generated by a numerical approximation \(=_{}\) of some underlying operator \(_{}\). Here \(>0\) represents a numerical parameter, such as the grid resolution when approximating the solution operator of a partial differential equation. In this setting, \(=_{}-_{}\) is non-zero and it is crucial to include the discretization error in the analysis, which we define as \(_{}\|\|_{L^{}_{}}^{2}=\|_{ }-_{}\|_{L^{}_{}}^{2}\). Assume \( 0\), so that \(\) minimizes \(_{N}^{}(\,\,;Y)=_{N}^{}(\,\,; _{})\). Using Cor. 3.8, it follows that for \(M\) and \(N\) sufficiently large,

\[_{u}\|_{}(u)-(u;) \|_{Y}^{2}\|_{}\|_{}^{2}+ _{}\] (3.8)

with high probability. Thus, as suggested by intuition, in addition to the error contribution that is present when training on perfect data (the first term on the right-hand side), there is an additional discretization error of size \(_{}\). We also see that the performance of RF-RR is stable with respect to such discretization errors stemming from the training data. Actually obtaining a rate of convergence would require problem-specific information about the particular numerical solver that is used.

### Statistical consistency

We now return to the objective of recovering \(\) from data. In particular, suppose that \(\); the RKHS, viewed as a hypothesis class, is misspecified. Our analysis demonstrates that statistical guarantees for RF-RR are still possible in this setting.

To this end, assume that \( L^{}_{}(;)\). It follows that Assumption 3.2 is satisfied with \(-_{}\) and \(_{}\) being _any_ element of the RKHS. Applying Thm. 3.4 and minimizing over \(_{}\) yields

\[_{u}\|(u)-(u;)\|_{Y}^{2} +_{_{}} _{u}\|(u)-_{}(u)\|_{ }^{2}+\|_{}\|_{}^{2}}\] (3.9)

with probability at least \(1-\) if \(M^{-1}(2/)\) and \(N^{-2}(2/)\). To obtain (3.9), we enlarged constants and used the bound \(\|\|_{L^{}_{}}^{2}\|\|_{L^{}_{}}^{2}+ \|}_{}\|_{L^{}_{}}^{2}\) in (3.3).

If \(\) is in the \(L^{2}_{}\)-closure of \(\), then with high probability, the population squared error on the left hand side of (3.9) converges to zero as \( 0\) (by application of Lem. B.2 to the second term on the right). This is a statement about the (weak) _statistical consistency_ of the trained RF-RR estimator; it can be upgraded to an almost sure statement, as expressed precisely in the next main result.

**Theorem 3.10** (Strong consistency).: _Suppose that \( L^{}_{}(;)\) belongs to the \(L^{2}_{}(;)\)-closure of \(\) (2.2). Let \(\{_{k}\}_{k}(0,1)\) be a sequence of positive regularization parameters such that \(_{k}_{k}<\). For \(\) the RFM (2.1) satisfying Assumption 3.1 and for each \(k\), let \(^{(k)}^{M_{k}}\) be the trained RFM coefficients that minimize the regularized \(Y\)-empirical risk \(_{N_{k}}^{_{k}}(\,\,;Y)\) given by (2.3) with \(M_{k}_{k}^{-1}(2/_{k})\) iid random features and \(N_{k}_{k}^{-2}(2/_{k})\) iid data pairs under Assumption 3.3. It holds true that_

\[_{k}_{u}\|(u)-(u; ^{(k)})\|_{Y}^{2}=0\ \ .\] (3.10)

The proof relies on a standard Borel-Cantelli argument. See **SM** C for the details.

_Remark 3.11_ (Universal RKHS).: The assumption that \(\) belongs to the \(L^{2}_{}\)-closure of the RKHS \(\) is automatically satisfied if \(\) is dense in \(L^{2}_{}(;)\). This is equivalent to its kernel being _universal_[7; 9]. In this case, the trained RFM is a strongly consistent estimator of any \( L^{}_{}\). However, we are unaware of any practical characterizations of universality of the kernel in terms of its corresponding random feature pair \((,)\) for the vector-valued setting studied here.

### Convergence rates

The previous subsection establishes convergence guarantees without any rates. We now establish quantitative bounds. Throughout what follows, we denote by \(\,L^{2}_{}(;) L^{2}_{}( ;)\) the integral operator (B.5) corresponding to the operator-valued kernel function of the RKHS \(\) (see **SM** B).

**Theorem 3.12** (Slow rates under misspecification).: _Suppose that \( L^{}_{}(;)\) and that Assumption 3.3 holds. Additionally, assume that \((^{r/2})\) for some \(r>0\), where \(\) is the integral operator corresponding to the kernel of RKHS \(\) (2.2). Fix \((0,1)\) and \((0,1)\). For \(\) the RFM (2.1) satisfying Assumption 3.1, let \(^{M}\) minimize \(_{N}^{}(\,\,;Y)\) given by (2.3). If \(M^{-1}(32/)\) and \(N^{-2}(16/)\), then with probability at least \(1-\) it holds that_

\[_{u}\|(u)-(u;)\|_{}^{2}^{r 1}\,.\] (3.11)

_The implied constant in (3.11) depends only on \(\|\|_{L^{}_{}}\) and \(\|\|_{_{1}()}\)._

Thm. 3.12 provides a quantitative convergence rate as \( 0\). For \(r 1\), i.e., when \(\), we recover the linear convergence rate of order \(\) from Thm. 3.7. The assumption that \((^{r/2})\) can be viewed as a "fractional regularity" assumption on the underlying operator; indeed, in specific settings it corresponds to a fractional (Sobolev) regularity of the underlying function. In general, it appears difficult to check this condition in practice, which is one limitation of our result.

A quantitative analog to the almost sure statement of Thm. 3.10 also holds.

**Corollary 3.13** (Strong convergence rate).: _Instantiate the hypotheses and notation of Thm. 3.10. Assume in addition that \((^{r/2})\) for some \(r>0\). Let \(\{_{k}\}_{k}(0,1)\) be a sequence of positive regularization parameters such that \(_{k}_{k}<\). For each \(k\), let \(^{(k)}^{M_{k}}\) be the trained RFM coefficients with \(M_{k}_{k}^{-1}(2/_{k})\) and \(N_{k}_{k}^{-2}(2/_{k})\). It holds true that_

\[_{k}(_{u}\|(u)-(u; ^{(k)})\|_{}^{2}}{_{k}^{r 1}})< \;\;\] (3.12)

Short proofs of both Thm. 3.12 and Cor. 3.13 may be found in **SM** C.

## 4 Proof outline for the main theorem

Our main results are all derived from Thm. 3.4, whose proof, schematically illustrated in Figure 1, we now outline. Following , we break the proof into several steps that arise from the error decomposition

\[(;)=_{N}(; )+[(;)-_{N} (;)]\,.\] (4.1)

Sect. 4.1 estimates the first term on the right hand side of (4.1) while Sect. 4.2 estimates the second.

### Bounding the regularized empirical risk

The main technical contribution of this work is a tight bound on the \(\)-empirical risk \(_{N}(;)\) for the trained RFM. The analysis involves controlling several sources of error and careful truncation arguments to avoid unnecessarily strong assumptions on the problem. The result is the following.

**Proposition 4.1** (Regularized \(\)-empirical risk bound).: _Let Assumptions 3.1 and 3.3 hold. Suppose that \(=+_{}\) satisfies Assumption 3.2. Fix \((0,1)\), \((0,1)\), \(M\), and \(N\)._

Figure 1: Flow chart illustrating the proof of Theorem 3.4.

_Let \(^{M}\) be the minimizer of the regularized \(Y\)-empirical risk \(^{}_{N}(\,\,;Y)\) given by (2.3). If \(M^{-1}(16/)\) and \(N^{-2}(8/)\), then_

\[^{}_{N}(;)(, ,_{},)\] (4.2)

_with probability at least \(1-\), where the multiplicative factor \((,,_{},)\) is given by (3.3)._

Since \(\|\|_{M}^{2}^{}_{N}(;)\), the next corollary controlling the norm (2.3) of \(\) is immediate. It plays a crucial role in developing an upper bound for the second term on the right side of (4.1).

**Corollary 4.2** (Trained RFM norm bound).: _Instantiate the hypotheses and notation of Prop. 4.1. Fix \((0,1)\) and \((0,1)\). If \(M^{-1}(16/)\) and \(N^{-2}(8/)\), then_

\[_{}\{^{M }\,\,\|\|_{M}^{2}\}\] (4.3)

_with probability at least \(1-\). The radius \((,,_{},)\) of the norm bound is given by (3.3)._

The core elements of the proof of Prop. 4.1 are provided in the next few subsections, with the full argument in **SM** D.1. The main idea is to upper bound the \(\)-empirical risk by its regularized counterpart and then decompose the latter into several (coupled) error contributions.

To do this, first fix any \(^{M}\). It holds that

\[^{}_{N}(;Y)=^{}_{N}(; )+_{n=1}^{N}_{n},(u_{n})- (u_{n};)_{}+_{n=1}^{N}_{n }_{}^{2}\] (4.4)

because \(\) is a Hilbert space and \(y_{n}=(u_{n})+_{n}\). Using this, a short calculation shows that

\[^{}_{N}(;) =^{}_{N}(;Y)- ^{}_{N}(;Y)+^{}_{N}(;)+ _{n=1}^{N}_{n},(u_{n};)-(u_ {n};)_{}\] \[^{}_{N}(;)+ _{n=1}^{N}-_{n},(u_{n};)_{}+_{n=1}^{N}_{n},(u_{n};)_{}\,.\] (4.5)

In the second line, we used the fact that \(\) minimizes \(^{}_{N}(\,\,;Y)\). Since \(^{M}\) is arbitrary, we have the freedom to choose \(\) so that the first term is small (see Sect. 4.1.1 and 4.1.2). With \(\) fixed, the second term averages to zero by our assumptions on the noise, and hence, we expect it to be small with high probability (see Sect. 4.1.3).

The third term in (4.5) exhibits high correlation between the noise \(_{n}\) and the trained RFM coefficients \(\), making it more difficult to estimate. To control this last term, we first note that it is homogeneous in \(\|\|_{M}\), which can be used to derive an upper bound in terms of a supremum over the unit ball with respect to \(_{M}\). The resulting expression is then bounded with empirical process techniques (see **SM** D.1.3). For the complete details of the required argument we refer the reader to **SM** D.1.

In the remainder of this subsection, we estimate the first two terms on the right hand side of (4.5). Using the fact that \(=+_{}\), the first term can be split into two contributions,

\[^{}_{N}(;) 2^{}_{N}( ;_{})+_{n=1}^{N}(u_{n}) _{}^{2}\,.\] (4.6)

These contributions to the first term in (4.5) are bounded in Sect. 4.1.1 and 4.1.2. The second term in (4.5) is controlled in Sect. 4.1.3.

#### 4.1.1 Bounding the approximation error

We begin with the term \(^{}_{N}(;_{})\), which may be viewed as an empirical _approximation error_ due to \(\) being arbitrary. Its only dependence on the data is through \(\{u_{n}\}\) in (2.3). Intuitively, this term should behave like its population counterpart. It is then natural to choose a Monte Carlo approximation \(_{m}=_{}(_{m})\) for \(\), where \(_{} L^{2}_{}(;)\) is identified with \(_{}\) as in (2.2). However, our intuition that \(\|\|_{M}^{2}\) should concentrate around \(\|_{}\|_{L^{2}_{}}^{2}\) fails because it is generally not possible to control the tail of the random variable \(|_{}()|^{2}\). We next show that this problem can be overcome by a carefully tuned truncation argument combined with Bernstein's inequality.

**Lemma 4.3** (Construction of approximator).: _Suppose that \(_{}\). Fix \((0,1)\), \(>0\), \(N\), and \(M\). Let \(\{_{m}\}^{ M}\) and \(\{u_{n}\}^{ N}\). Define \(^{}^{M}\) componentwise by_

\[^{}_{m}_{}(_{m})_{\{| _{}(_{m})| T\}}\,, T \,_{}\,|_{ }()|^{2}}\] (4.7)

_and \(_{}=_{}[_{}( )(\,\,;)]\) with \(\|_{}\|_{}^{2}=_{}| _{}()|^{2}\). If \(M^{-1}(4/)\), then with probability at least \(1-\) in the random feature parameters \(_{1},,_{M}\), it holds that_

\[_{N}^{}(^{};_{}) 81 \|_{}\|_{}^{2}.\] (4.8)

**SM** D.1.1 provides the proof.

_Remark 4.4_ (Well-specified and noise-free).: Lem. 4.3 gives a \(O()\) bound on the regularized \(_{}\)-empirical risk of a RFM trained on well-specified and noise-free iid data \(\{u_{n},_{}(u_{n})\}\).

#### 4.1.2 Bounding the misspecification error

The second contribution to (4.6) is easily bounded by Bernstein's inequality because \( L_{}^{}\). We refer the reader to **SM** D.1.2 for the detailed proof.

**Lemma 4.5** (Concentration of misspecification error).: _Let \(\) be as in Assumption 3.2. Fix \((0,1)\). With probability at least \(1-\), it holds that_

\[\,_{n=1}^{N}(u_{n})_{}^{2} 4\, _{u}(u)_{}^{2}+^{}}^{2}(2/)}{N}\,.\] (4.9)

#### 4.1.3 Bounding the noise error

The second term on the right hand side of (4.5) is a zero mean error contribution due to the noise corrupting the output training data. By the fact that \(\) is subexponential (Assumption 3.3), Bernstein's inequality delivers exponential concentration. The proof details are in **SM** D.1.3.

**Lemma 4.6** (Concentration of noise error cross term).: _Let Assumptions 3.1 and 3.3 hold. Fix \(^{M}\), \(\{_{m}\}^{ M}\), and \((0,1)\). With probability at least \(1-\), it holds that_

\[_{n=1}^{N}-_{n},(u_{n};)_{ } 16e^{3/2}_{1}_{_{1}()} _{M}}\,.\] (4.10)

**SM** D.1.3 also details the techniques used to upper bound the third and final term in (4.5).

### Bounding the generalization gap

Having bounded the empirical risk with approximation arguments, it remains to control the estimation error \((;)-_{N}(; )\) due to finite data in (4.1). We call this the _generalization gap_: the difference between the population test error and its empirical version. If \(\) satisfies \(_{M}^{2} t\) for some \(t>0\), then one can upper bound the generalization gap by its supremum over this set. The main challenge is to show existence of a (sufficiently small) \(t\) that satisfies this inequality. This is handled by Cor. 4.2. As summarized in the following proposition, the resulting supremum of the empirical process defined by the generalization gap is shown to be of size \(N^{-1/2}\) with high probability.

**Proposition 4.7** (Uniform bound on the generalization gap).: _Let Assumption 3.1 hold. Suppose \(\) satisfies Assumption 3.2. Let \(\{_{m}\}^{ M}\) for the RFM \(\) given by (2.1). Fix \((0,1)\). For iid input samples \(\{u_{n}\}^{ N}\), define the random variable_

\[_{}\{u_{n}\},\{_{m}\}_{ _{}}_{n=1}^{N}(u_ {n})-(u_{n};)_{}^{2}-_{u}(u)-(u;)_{}^{2}\,,\] (4.11)

_where \(_{}\{^{}^{M}\,\, ^{}_{M}^{2}\}\) and the deterministic radius \(=(,,_{},)\) is given in (3.3) with \(\) as above. If \(N(1/)\), then with probability at least \(1-\) it holds that_

\[_{}\{u_{n}\},\{_{m}\} 32e^{3/2} _{L_{}^{}}^{2}+(,,_ {},)}\,.\] (4.12)The proof of Prop. 4.7 is given in **SM** D.2. The argument is composed of two steps. The first is to show that \(_{}\,|\,\{_{m}\}\) concentrates around its (conditional) expectation (Lem. D.6). This follows easily using the boundedness of the summands. The second step is to upper bound the expectation of \(_{}\,|\,\{_{m}\}\) (Lem. D.7). This is achieved by exploiting the Hilbert space structure of the \(\)-square loss and the linearity of the RFM with respect to its coefficients.

### Combining the bounds

Since we now have control over the \(\)-empirical risk and the generalization gap, the \(\)-population risk is also under control by (4.1). The proof of Thm. 3.4 follows by putting together the pieces (**SM** C).

## 5 Numerical experiment

To study how our theory holds up in practice, we numerically implement the vector-valued RF-RR algorithm on a benchmark operator learning dataset. The data \(\{(u_{n},(u_{n}))\}_{n=1}^{N}\) is noise-free, the \(\{u_{n}\}\) are iid Gaussian random fields, and \( L^{2}(;) L^{2}(; )\) is a nonlinear operator defined as the time one flow map of the viscous Burgers' equation on the torus \(\). **SM** E provides more details about the problem setting and the choice of random feature pair \((,)\).

Figure 1(a) shows the decay of the relative squared test error as \(M\) increases (with \( 1/M\)) for fixed \(N\). The error closely follows the rate \(O(M^{-1})\) until it begins to saturate at larger \(M\). This is due to either \(\) not belonging to the RKHS of \((,)\) or the finite data error dominating. As implied by our theory, the error does not depend on the discretized output dimension \(p<\). Figure 1(b) displays similar behavior as \(N\) is varied (now with \( 1/\) and fixed \(M\)). Overall, the observed parameter and sample complexity reasonably validate our theoretical insights.

## 6 Conclusion

This paper establishes several fundamental results for learning with infinite-dimensional vector-valued random features; these include strong consistency and explicit convergence rates. When the underlying mapping belongs to the RKHS, the rates obtained in this work match minimax optimal rates in the number of samples \(N\), requiring only a number of random features \(M\). Despite being derived in a very general setting, to the best of our knowledge, this provides the sharpest parameter complexity in \(M\), which is free from logarithmic factors for the first time.

There are several interesting directions for future work. These include deriving fast rates, relaxing the boundedness assumption on the features and the true mapping, and accommodating heavier-tailed or white noise distributions. Obtaining fast rates would require a sharpening of several estimates, and in particular, replacing the global Rademacher complexity-type estimate, implicit in our work, by its local counterpart. As our approach does not make use of an explicit solution formula, which is only available for a square loss, this might pave the way for improved rates for other loss functions, such as a general \(L^{p}\)-loss. We leave such potential extensions of the present approach for future work.

Figure 2: Squared test error of trained RFM for learning the Burgers’ equation solution operator. All shaded bands denote two empirical standard deviations from the empirical mean of the error computed over \(10\) different models, each with iid sampling of the features and training data indices.