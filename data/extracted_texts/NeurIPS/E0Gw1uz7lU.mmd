# Federated Conditional Stochastic Optimization

Xidong Wu

Department of ECE

University of Pittsburgh

Pittsburgh, PA 15213

xidong_wu@outlook.com &Jianhui Sun\({}^{}\)

Computer Science

University of Virginia

Charlottesville, VA 22903

js9gu@virginia.edu &Zhengmian Hu

Computer Science

University of Maryland

College Park, MD 20742

huzhengmian@gmail.com &Junyi Li

Department of ECE

University of Pittsburgh

Pittsburgh, PA 15213

junyili.ai@gmail.com &Aidong Zhang\({}^{@sectionsign}\)

Computer Science

University of Virginia

Charlottesville, VA 22903

aidong@virginia.edu &Heng Huang\({}^{*}\)

Computer Science

University of Maryland

College Park, MD 20742

henghuanghh@gmail.com

Equal contributionThis work was partially supported by NSF CNS 2213700 and CCF 2217071 at UVA.This work was partially supported by NSF IIS 1838627, 1837956, 1956002, 2211492, CNS 2213701, CCF 2217003, DBI 2225775 at Pitt and UMD.

###### Abstract

Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (_i.e._, FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for Meta-Learning in FL, federated conditional stochastic optimization considers the sample of tasks. Extensive experimental results on various tasks validate the efficiency of these algorithms.

## 1 Introduction

The conditional stochastic optimization arises throughout a wide range of machine learning tasks, such as the policy evaluation in reinforcement learning , invariant learning , instrumental variable regression in causal inference , Model-Agnostic Meta-Learning (MAML) , AUPRC maximization  and so on. Recently many efficient conditional stochastic optimization algorithms have been developed [16; 17; 18; 28; 34; 32] to solve the corresponding machine learning problems and applications. However, all existing conditional stochastic optimization algorithms were only designed for centralized learning (_i.e._, model and data both deployed at a single machine) or finite-sum optimization, without considering the large-scale online distributed scenario. Many federated learning algorithms [26; 22; 29; 23; 44; 39; 25; 45] were proposed since FL is a communication-efficient training paradigm for large-scale machine learning training preserving data privacy. In federated learning, clients update the model locally, and the global server aggregates the modelparameters periodically. Although federated learning has been actively applied to numerous real-world applications in the past years, the federated conditional stochastic optimization problem is still underexplored. To bridge this gap, in this paper we study the following federated conditional stochastic optimization problem:

\[_{x}F(x):=_{n=1}^{N}_{^{n}}f_{^ {n}}^{n}(_{^{n}|^{n}}g_{^{n}}^{n}(x,^{n}))\,,\] (1)

where \(^{d}\) is a closed convex set, \(_{^{n}}f_{^{n}}^{n}():^{d^{}}\) is the outer-layer function on the \(n\)-th device with the randomness \(^{n}\), and \(_{^{n}|^{n}}g_{^{n}}^{n}(,^{n}):^{d} ^{d^{}}\) is the inner-layer function on the \(n\)-th device with respect to the conditional distribution of \(^{n}^{n}\). We assume \(f_{}^{n}()\) and \(g_{}^{n}(,)\) are continuously differentiable. The objective subsumes two stochastic functions in (1), where the inner functions rely on the randomnesses of both inner and outer layers, and \(\) and \(\) are not independent, which makes the federated conditional stochastic optimization more challenging compared with the standard federated learning optimization optimization problems.

Federated conditional stochastic optimization contains the standard federated learning optimization as a special situation when the inner-layer function \(g_{^{n}}^{n}(x,^{n})=x\). In addition, federated stochastic compositional optimization is similar to federated conditional stochastic optimization given that both problems contain two-layer nested expectations. However, they are fundamentally different. In federated stochastic compositional optimization, the inner randomness \(\) and the outer randomness \(\) are independent and data samples of the inner layer are available directly from \(\) (instead of a conditional distribution as in Problem (1)). Therefore, when randomnesses \(\) and \(\) are independent and \(g_{^{n}}^{n}(x,)=g_{^{n}}^{n}(x)\), (1) is converted into federated stochastic compositional optimization .

Recently, to solve the conditional stochastic optimization problem efficiently,  studied the sample complexity of the sample average approximation for conditional stochastic optimization. Afterward,  proposed the algorithm called biased stochastic gradient descent (BSGD) and an accelerated algorithm called biased SpiderBoost (BSpiderBoost). The convergence guarantees of BSGD and BSpiderBoost under different assumptions are established. More recently, [28; 34; 37; 32; 14] reformulated the AUC maximization into a finite-sum version of conditional stochastic optimization and introduced algorithms to solve it. In an increasing amount of distributed computing settings, efficient federated learning algorithms are absolutely necessary but still lacking. An important example of conditional stochastic optimization is MAML. In meta-learning, we attempt to train models that can efficiently adapt to unseen tasks via learning with metadata from similar tasks . When the tasks are distributed at different clients, a federated version of MAML would be beneficial to leverage information from all workers . A lot of existing efforts [19; 11] have been made to convert FL MAML into federated compositional optimization. Nonetheless, they ignore the sample of tasks in MAML, and federated conditional stochastic optimization problems have never been studied. Thus, there exists a natural question: Can we design federated algorithms for conditional stochastic optimization while maintaining the fast convergence rate to solve Problem (1)?

In this paper, we give an affirmative answer to the above question. We propose a suite of approaches to solve Problem (1) and establish their corresponding convergence guarantee. To our best knowledge, this is the first work that thoroughly studies federated conditional stochastic optimization problems and provides completed theoretical analysis. Our proposed algorithm matches the lower-bound sample complexity in a single-machine setting and obtains convincing results in empirical experiments. Our main contributions are four-fold:

1. we propose the federated conditional stochastic gradient (FCSG) algorithm to solve Problem (1). We establish the theoretical convergence analysis for FCSG. In the general nonconvex setting, we prove that FCSG has a sample complexity of \(O(^{-6})\) and communication complexity of \(O(^{-3})\) to reach an \(\)-stationary point, and achieves an appealing linear speedup _w.r.t_ the number of clients.
2. To further improve the empirical performance of our algorithm, we introduce a momentum-based FCSG algorithm, called FCSG-M since the momentum-based estimator could reduce noise from samples with history information. FCSG-M algorithm obtains the same theoretical guarantees as FCSG.
3. To reach the lower bound of sample complexity of the single-machine counterpart , we propose an accelerated version of FCSG-M (Acc-FCSG-M) based on the momentum-based variance reduction technique. We prove that Acc-FCSG-M has a sample complexity of \(O(^{-5})\), and communication complexity of \(O(^{-2})\), which matches the best sample complexity attained by single-machine algorithm BSpiderBoost with variance reduction.
4. Experimental results on the robust logistic regression, MAML and AUPRC maximization tasks validate the effectiveness of our proposed algorithms.

## 2 Related Work

### Conditional Stochastic Optimization

 studied the generalization error bound and sample complexity of the sample average approximation (SAA) for conditional stochastic optimization. Subsequently,  proposed a class of efficient stochastic gradient-based methods for general conditional stochastic optimization to reach either a global optimal point in the convex setting or a stationary point in the nonconvex setting, respectively. In the nonconvex setting, BSGD has the sample complexity of \(O(^{-6})\) and a variance reduction algorithm (BSpiderBoost) has the sample complexity of \(O(^{-5})\).  utilized the Monte Carlo method to achieve better results compared with the vanilla stochastic gradient method. Recently,  converted AUPRC maximization optimization into the finite-sum version of the conditional stochastic optimization and propose adaptive and non-adaptive stochastic algorithms to solve it. Similarly, recent work  used moving average techniques to improve the convergence rate of AUPRC maximization optimization and provide theoretical analysis for the adaptive algorithm. Furthermore,  focused on finite-sum coupled compositional stochastic optimization, which limits the outer-layer function to the finite-sum structure. The algorithms proposed in  improved oracle complexity with the parallel speed-up. More recently,  use federated learning to solve AUC maximization. However, algorithms proposed in [28; 34; 32; 14] for AUC maximization have a significant limitation because they maintain an inner state for each data point. As a result, its convergence rate depends on the number of data points and cannot be extended to other tasks and large-scale model training. It is also not applicable to online learning due to the dependence on each local data point.  consider the decentralised online AUPRC maximization but the theoretical analysis cannot be applied into the federated learning.

### Stochastic Compositional Optimization

Recently, a related optimization problem, stochastic compositional optimization, has attracted widely attention [36; 41; 11] and solve the following objective:

\[_{x}F(x):=_{}f_{}(_{}g_{}( x))\,.\] (2)

To address this problem,  developed SCGD, which utilizes the moving average technique to estimate the inner-layer function value.  further developed an accelerated SCGD method with the extrapolation-smoothing scheme. Subsequently, a series of algorithms [20; 43; 15; 41] were presented to improve the complexities using the acceleration or variance reduction techniques.

More recently,  and  studied the stochastic compositional problem in federated learning.  transformed the distributionally robust federated learning problem (_i.e._, a minimax optimization problem) into a simple compositional optimization problem by using KL divergence regularization and proposed the first federated learning compositional algorithm and analysis.  formulated the model personalization problem in federated learning as a model-agnostic meta-learning problem. In personalized federated learning, each client's task assignment is fixed and there is no task sampling on each client in the training procedure. The sampling of the inner layer and outer layer are independent. Therefore, personalized federated learning is formulated as the stochastic compositional optimization .  solves personalized federated learning utilizing SCGD, in contrast to SGD in , to reduce the convergence complexities. However, the algorithm in  has a drawback in that keeping an inner state for each task is necessary, which is prohibitively expensive in large-scale settings. More recently,  proposed a momentum-like method for nonconvex problems with better complexities to solve the stochastic compositional problem in the federated learning setting. Although  claims their algorithm can be used in the MAML problem, it does not consider the two fundamental characteristics in MAML, i.e., task sampling and the dependency of inner data distribution on the sampled task.

Overall, problems (1) and (2) differ in two aspects: i) in Problem (2), the inner randomness \(\) and the outer randomness \(\) are independent, while in Problem (1), \(\) is conditionally dependent on the \(\); and ii) in Problem (1), the inner function depends on both \(\) and \(\). Therefore, Problem (2) can be regarded as a special case of (1). Thus, the conditional stochastic optimization (1) is more general.

## 3 Preliminary

For solving the problem (1), we first consider the local objective \(F^{n}(x)\) and its gradient. We have

\[F^{n}(x)=_{^{n}}f_{^{n}}^{n}(_{^{n}|^{n}}g_{ ^{n}}^{n}(x,^{n}))\]

\[ F^{n}(x)=_{^{n}}[(_{^{n}|^{n}}  g_{^{n}}^{n}(x,^{n}))]^{} f_{^{n}}^{n}( _{^{n}|^{n}}g_{^{n}}^{n}(x,^{n}))]\]

Since there are two layers of stochastic functions, the standard stochastic gradient estimator is not an unbiased estimation for the full gradient. Instead of constructing an unbiased stochastic gradient estimator,  considered a biased estimator of \( F(x)\) using one sample of \(\) and \(m\) samples of \(\):

\[^{n}(x;^{n},_{n})=(_{ _{j}^{n}_{n}} g_{_{j}^{n}}^{n}(x,^{n}))^{}  f_{^{n}}^{n}(_{_{j}^{n}_{n}}g_{ _{j}^{n}}^{n}(x,^{n}))\]

where \(_{n}=\{_{j}^{n}\}_{j=1}^{m}\). And \(^{n}(x;^{n},_{n})\) is the gradient of an empirical objective such that

\[^{n}(x;^{n},_{n}):=f_{^{n}}^{n}(_{_{j}^{n}_{n}}g_{_{j}^{n}}(x,^{n}))\] (3)

### Assumptions

**Assumption 3.1**.: (Smoothness) \( n[N]\), the function \(f_{^{n}}^{n}()\) is \(S_{f}\)-Lipschitz smooth, and the function \(g_{^{n}}^{n}(,^{n})\) is \(S_{g}\)-Lipschitz smooth, _i.e._, for a sample \(^{n}\) and \(m\) samples \(\{_{j}^{n}\}_{j=1}^{m}\) from the conditional distribution \(P(^{n}^{n})\), \( x_{1},x_{2}f^{n}()\), and \( y_{1},y_{2}g^{n}()\), there exist \(S_{f}>0\) and \(S_{g}>0\) such that

\[\| f_{^{n}}^{n}(x_{1})- f_{^{n}}^{n}(x_{2})\| S _{f}\|x_{1}-x_{2}\|\| g_{^{n}}^{n}(y_{1},^{n})-  g_{^{n}}^{n}(y_{2},^{n})\| S_{g}\|y_{1}-y_{2}\|\]

Assumption 3.1 is a widely used assumption in optimization analysis. Many single-machine stochastic algorithms use this assumption, such as BSGD , SPIDER , STORM , ADSGD , and D2SG . In distributed learning, the convergence analysis of distributed learning algorithms, such as DSAL , and many federated learning algorithms such as MIME , Fed-GLOMO , STEM  and FAFED  also depend on it.

**Assumption 3.2**.: (Bounded gradient) \( n[N]\), the function \(f^{n}()\) is \(L_{f}\)-Lipschitz continuous, and the function \(g^{n}()\) is \(L_{g}\)-Lipschitz continuous, _i.e._, \( xf^{n}()\), and \( yg^{n}()\), the second moments of functions are bounded as below:

\[\| f_{^{n}}^{n}(x)\|^{2} L_{f}^{2}\|  g_{^{n}}^{n}(y_{1},^{n})\|^{2} L_{g}^{2}\]

Assumption 3.2 is a typical assumption in the multi-layer problem optimization to constrain the upper bound of the gradient of each layer, as in .

**Assumption 3.3**.: (Bounded variance) \( n[N]\), and \(x\):

\[_{^{n},x}_{^{n}|^{n}}\|g_{^{n}}(x, ^{n})-_{^{n}|^{n}}g_{^{n}}(x,^{n})\|^{2} _{g}^{2}\]

where \(_{g}^{2}<+\). Assumption 3.3 indicates that the random vector \(g_{^{n}}\) has bounded variance.

 
**Algorithm** & **Sample** & **Communication** \\  FCSG & \(O(^{-6})\) & \(O(^{-3})\) \\  FCSG-M & \(O(^{-6})\) & \(O(^{-3})\) \\  Lower Bound  & \(O(^{-5})\) & - \\  Acc-FCSG-M & \(O(^{-5})\) & \(O(^{-2})\) \\  

Table 1: Complexity summary of proposed federated conditional stochastic optimization algorithms to reach an \(\)-stationary point. Sample complexity is defined as the number of calls to the First-order Oracle (IFO) by clients to reach an \(\)-stationary point. Communication complexity denotes the total number of back-and-forth communication rounds between each client and the central server required to reach an \(\)-stationary point.

## 4 Proposed Algorithms

In the section, we propose a class of federated first-order methods to solve the Problem (1). We first design a federated conditional stochastic gradient (FCSG) algorithm with a biased gradient estimator and the momentum-based algorithm FCSG-M. To further accelerate our algorithm and achieve the lower bound of sample complexity of the single-machine algorithm, we design the Acc-FCSG-M with a variance reduction technique. Table 1 summarizes the complex details of each algorithm.

### Federated Conditional Stochastic Gradient (FCSG)

First, we design a federated conditional stochastic gradient (FCSG) algorithm with the biased gradient estimator. We leverage a mini-batch of conditional samples to construct the gradient estimator \(u_{t}\) with controllable bias as (6). At each iteration, clients update their local models \(x_{t}\) with local data, which can be found in Line 9-14 of Algorithm 1. Once every \(q\) local iterations, the server collects local models and returns the averaged models to each client, as Line 5-8 of Algorithm 1. Here, the number of local update steps \(q\) is greater than 1 such that the number of communication rounds is reduced to \(T/q\). The details of the method are summarized in Algorithm 1. Then we study the convergence properties of our new algorithm FCSG. Detailed proofs are provided in the supplementary materials.

```
1:Input: Parameters: \(T\), momentum weight \(\), learning rate \(\), the number of local updates \(q\), inner batch size \(m\) and outer batch size \(b\), as well as the initial outer batch size B ;
2:Initialize:\(x_{0}^{n}=_{0}=_{k=1}^{N}x_{0}^{n}\). Draw \(B\) samples of \(\{_{t,1}^{n},,_{t,B}^{n}\}\) and draw \(m\) samples \(_{0,i}^{n}=\{_{ij}^{n}\}_{j=1}^{m}\) from \(P(^{n}_{0,i}^{n})\) for each \(_{0,i}^{n}\{_{t,1}^{n},,_{t,B}^{n}\};u_{1}^{n}= _{i=1}^{B}^{n}(x_{0}^{n};_{0,i}^{n},_{0,i}^{n})\).
3:for\(t=1,2,,T\)do
4:for\(n=1,2,,N\)do
5:if\((t,q)=0\)then
6:Server Update:
7:\(^{n}}=}=_{i=1}^{N}^{n}}\)
8:\(x_{t}^{n}=_{t}=_{n=1}^{N}(x_{t-1}^{n}- u_{t}^{n})\)
9:else
10:\(x_{t}^{n}=x_{t-1}^{n}- u_{t}^{n}\)
11:endif
12: Draw \(b\) samples of \(\{_{t,1}^{n},,_{t,b}^{n}\}\)
13: Draw \(m\) samples \(_{t,n}^{n}=\{_{ij}^{n}\}_{j=1}^{m}\) from \(P(^{n}_{t,i}^{n})\) for each \(_{t,i}^{n}\{_{t,1}^{n},,_{t,b}^{n}\}\),
14:\(^{n}}=_{i=1}^{b}^{n}(x_{t}^{n}; _{t,i}^{n},_{t,i}^{n})\)
15:\(^{n}}=(1-)u_{t}^{n}+_{i=1}^{b} ^{n}(x_{t}^{n};_{t,i}^{n},_{t,i}^{n})\)
16:endfor
17:endfor
18:Output:\(x\) chosen uniformly random from \(\{_{t}\}_{t=1}^{T}\). ```

**Algorithm 1** FCSG and **FCSG-M** Algorithm

**Theorem 4.1**.: _Suppose Assumptions 3.1, 3.2 and 3.3 hold, if \(_{F}}\), FCSG has the following convergence result_

\[_{t=0}^{T-1}\| F(_{t})\|^{2}_{ 0})-F(_{T})]}{ T}+^{2}S_{f}^{2}_{g}^{2}}{m}+ L_{f}^{2}L_{g}^{2}}{N}+42(q-1)q^{2}L_{f}^{2}L_{g}^{2} S_{F}^{2}\]

**Corollary 4.2**.: _We choose \(=}}\) and \(q=(T/N^{3})^{1/4}\), we have_

\[_{t=0}^{T-1}\| F(_{t})\|^{2}[F( {x}_{0})-F(_{*})]}{(NT)^{1/2}}+^{2}S_{f}^{2}_{g}^{2}}{m }+^{2}L_{g}^{2}}{6(NT)^{1/2}}+^{2}L_{g}^{2}}{9(NT)^{1/2}}\]

[MISSING_PAGE_FAIL:6]

**Theorem 4.7**.: _Suppose Assumptions 3.1, 3.2 and 3.3 hold, \(}\) and \(=5S_{F}\). Acc-FCSG-M has the following_

\[_{t=0}^{T-1}\| F(_{t})\|^{2}_{0 })-F(_{T})]}{T}+^{2}L_{g}^{2}}{ BNT}+^{2}L_{g}^{2}c^{2}}{6S_{F}^{2}}^{2}+^{2}S_{f}^{2}_{g}^ {2}}{m}+^{2}}{Nb}\]

**Corollary 4.8**.: _We choose \(q=(T/N^{2})^{1/3}\). Therefore, \(=}=}{12S_{F}T^{1/3}}\), since \(c=^{2}}{bN}\), we have \(=c^{2}=}{24T^{2/3}b}\). And let \(B=}{N^{1/3}}\). Therefore, we have_

\[_{t=0}^{T-1}\| F(_{t})\|^{2} [F(_{0})-F(_{*})]}{(NT)^{2/3}}\] \[+^{2}L_{g}^{2}b}{5(NT)^{2/3}}+^{2}L_{g }^{2}}{24b^{2}(TN)^{2/3}}+^{2}S_{f}^{2}_{g}^{2}}{m}+^{2}}{4b^{2}(NT)^{2/3}}\]

_Remark 4.9_.: We choose b as \(O(1)(b 1)\) and \(m=O(^{-2})\). According to Corollary 4.8 to make \(_{t=0}^{T-1}\| F(_{t})\|^{2}^{2}\), we get \(T=O(N^{-1}^{-3})\) and \(=(NT)^{2/3}=O(^{-2})\). The sample complexity is \(O(N^{-1}^{-5})\). The communication complexity is \(O(^{-2})\). \(T=O(N^{-1}^{-3})\) indicates the linear speedup of our algorithm.

## 5 Experiments

The experiments are run on CPU machines with AMD EPYC 7513 32-Core Processors as well as NVIDIA RTX A6000. The code is available * and Federated Online AUPRC maximization task follow  *.

Footnote *: https://github.com/xidongwu/Federated-Minimax-and-Conditional-Stochastic-Optimization/tree/main

### Invariant Logistic Regression

Invariant learning has an important role in robust classifier training . In this section, We compare the performance of our algorithms, FCSG and FCSG-M, on the distributed invariant logistic regressionto evaluate the benefit from momentum and the effect of inner batch size, and the problem was formulated by :

\[&_{x}_{n=1}^{N}_{^{n}=( a,b)}[l_{n}(x)+g(x)]\\ &l_{n}(x)=(1+(-b_{^{n}| ^{n}}[(^{n})^{}x])) g(x)=_{i=1}^{d}^{2}}{1+ x_{i}^{2}}\] (4)

where \(l_{n}(x)\) is the logistic loss function and \(g(x)\) is a non-convex regularization. We follow the experimental protocols in  and set the dimension of the model as 10 over 16 clients. We construct the dataset \(^{n}=(a,b)\) and \(\) as follow: We sample \(a N(0,_{1}^{2}I_{d})\), set \(b=\{ 1\}\) according to the sign of \(a^{}x^{*}\), then sample \(_{ij}^{n} N(a,_{2}^{2}I_{d})\). We choose \(_{1}=1\), and consider the \(_{2}/_{1}\) from \(\{1,1.5,2\}\). At each local iteration, we use a fixed mini-batch size \(m\) from \(\{1,10,100\}\). The outer batch size is set as 1. We test the model with 50000 outer samples to report the test accuracy. We carefully tune hyperparameters for both methods. \(\) = 0.001 and \(\) = 10. We run a grid search for the learning rate and choose the learning rate in the set \(\{0.01,0.005,0.001\}\). \(\) in FCSG-M are chosen from the set \(\{0.001,0.01,0.1,0.5,0.9\}\). The local update step is set as 50. The experiments are presented in Figure 1.

Figure 1 shows that when the noise ratio \(_{2}/_{1}\) increases, larger inner samples \(m\) are needed, as suggested by the theory because a large batch size could reduce sample noise. In addition, when \(m=100\), FCSG and FCSG-M have similar performance. However, when batch size is small, compared with FCSG, FCSG-M has a more stable performance, because FCSG-M can use historic information to reduce the effect of stochastic gradient noise.

### Federated Model-Agnostic Meta-Learning

Next, we evaluate our proposed algorithms with the few shot image classification task over the dataset with baselines: Local-SCGD and Local-SCGDM . MOML  is not suitable for this task since MOML requires maintaining an inner state for each task which is not permitted due to the large number of tasks in the Omniglot dataset. Local-SCGD is the federated version of SCGD . This task can be effectively solved via Model-Agnostic Meta-Learning .

Meta-learning aims to train a model on various learning tasks, such that the model can easily adapt to a new task using few training samples. Model-agnostic meta-learning (MAML)  is a popular meta-learning method to learn a good initialization with a gradient-based update, which can be formulated as the following conditional stochastic optimization problem:

\[_{x}_{i_{},a D_{}^{i} }_{i}(_{b D_{}^{i}}(x- _{i}(x,b)),a)\]

where \(_{}\) denotes the learning tasks distribution, \(D_{}^{i}\) and \(D_{}^{i}\) are support (training) dataset and query (testing) dataset of the learning task \(i\), respectively. \(_{i}(,D_{i})\) is the loss function on dataset \(D_{i}\) of task \(i\). And the \(\) is a fixed meta step size. Assume \(=(i,a)\) and \(=b\), the MAML problem is an example of conditional stochastic optimization where the support (training) samples in the inner

Figure 1: Test accuracy vs the number of communication rounds for different inner mini-batch (\(m\) = 1, 10, 100) under different noise ratios (\(_{2}/_{1}=1,1.5,2\)).

layer for the meta-gradient update are drawn from the conditional distribution of \(P()\) based on the sampled task in the outer layer.

Given there are a large number of pre-train tasks in MAML, federated learning is a good training strategy to improve efficiency because we can evenly distribute tasks over various clients and the global server coordinates clients to learn a good initial model collaboratively like MAML. Therefore, in Federated MAML, it is assumed that each device has part of the tasks. The optimization problem is defined as follows:

\[&_{x^{d}}_{n=1}^{N}F^{ n}(x)_{n=1}^{N}f^{n}(g^{n}(x))\\ &g^{n}(x)=_{^{n}_{i,}^{n}}[x-_{i}^{n}(x;^{n}) ],f^{n}(y)=_{i_{}^{n},a^{n} _{i,}^{n}}_{i}^{n}(y;a^{n}).\] (5)

In this part, we apply our methods to few-shot image classification on the Omniglot [24; 10]. The Omniglot dataset contains 1623 different handwritten characters from 50 different alphabets and each of the 1623 characters consists of 20 instances drawn by different persons. We divide the characters to train/validation/test with 1028/172/423 by Torchmeta  and tasks are evenly partitioned into disjoint sets and we distribute tasks randomly among 16 clients. We conduct the fast learning of N-way-K-shot classification following the experimental protocol in . The N-way-K-shot classification denotes we sample N unseen classes, randomly provide the model with K different instances of each class for training, and evaluate the model's ability to classify with new instances from the same N classes. We sample 15 data points for validation. We use a 4-layer convolutional neural model where each layer has 3 x 3 convolutions and 64 filters, followed by a ReLU nonlinearity and batch normalization . The images from Omniglot are downsampled to 28 x 28. For all methods, the model is trained using a single gradient step with a learning rate of 0.4. The model was evaluated using 3 gradient steps . Then we use grid search and carefully tune other hyper-parameters for each method. We choose the learning rate from the set \(\{0.1,0.05,0.01\}\) and \(\) as 1 . We select the inner state momentum coefficient for Local-SCGD and Local-SCGDM from \(\{0.1,0.5,0.9\}\) and outside momentum coefficient for Local-SCGDM, FCSG-M and Acc-FCSG-M from \(\{0.1,0.5,0.9\}\).

Figures 2 and 3 show experimental results in the 5-way-1-shot and 5-way-5-shot cases, respectively. Results show that our algorithms outperform baselines by a large margin. The main reason for Local-SCGD and Local-SCGDM to have bad performance is that converting the MAML optimization to the stochastic compositional optimization is unreasonable. It ignores the effect of task sampling on the training and inner training data distribution changes based on the sampled tasks in the outer layer. The use of the momentum-like inner state to deal with the MAML will slow down the convergence and we have to tune the extra momentum coefficient for the inner state. In addition, the momentum-like inner state also introduces extra communication costs because the server needs to average the inner state as in Local-SCGDM. In addition, comparing the results in Figures 2 and 3, we can see when

Figure 3: Convergence results of the 5-way-5-shot case over Omniglot Dataset.

Figure 2: Convergence results of the 5-way-1-shot case over Omniglot Dataset.

the K increases in the few-shot learning, the training performance is improved, which matches the theoretical analysis that a large inner batch-size \(m\) benefits the model training.

### Federated Online AUPRC maximization

AUROC maximization in FL has been studied in [13; 42; 40] and AUPRC maximization is also used to solve the imbalanced classification. Existing AUPRC algorithms maintain an inner state for each data point.  consider the online AUPRC in the decentralized learning. For the large-scale distributed data over multiple clients, algorithms for online AUPRC maximization in FL is necessary.

Following  and , the surrogate function of average precision (AP) for online AUPRC maximization is:

\[}=_{^{+}^{+}}_{ }(y=1)(x;z^{+},z)}{ _{}\ (x;z^{+},z)}\]

where \((x;z^{+},z)=(\{s-h(x;z^{+})+h(x;z),0\})^{2}\) and \(h(x;z)\) is the prediction score function of input \(z\) with model \(x\). Federated Online AUPRC maximization could be reformulated as:

\[_{x}F(x)=_{x}_{n=1}^{n}_{_{n}_{n}^{+}}f(_{_{n}^{}_{n}}g^{n}(; _{n},_{n}^{}))\] (6)

where \(_{n}=(z_{n},y_{n})_{n}\) and \(_{n}^{+}=(z_{n}^{+},y_{n}^{+})_{n}^{+}\) are samples drawn from the whole datasets and positive datasets, respectively. It is a two-level problem and the inner objective depends on both \(\) and \(^{+}\). Since federated online AUPRC is a special example of a federated conditional stochastic optimization, our algorithms could be directly applied to it.

We choose MNIST dataset and CIFAR-10 datasets. As AUROC maximization in federated settings has been demonstrated in existing works [13; 42], we use CODA+ in  as a baseline. Another baseline is the FedAvg with cross-entropy loss. Since AUPRC is used for binary classification, the first half of the classes in the MNIST and CIFAR10 datasets are designated to be the negative class, and the rest half of the classes are considered to be the positive class. Then, we remove 80% of the positive examples in the training set to make it imbalanced, while keeping the test set unchanged. The results in Table 2 show that our algorithms could be used to solve the online AUPRC maximization in FL and it largely improves the model's performance.

## 6 Conclusion

In this paper, we studied federated conditional stochastic optimization under the general nonconvex setting. To the best of our knowledge, this is the first paper proposing algorithms for the federated conditional stochastic optimization problem. We first used the biased stochastic first-order gradient to design an algorithm called FCSG, which we proved to have a sample complexity of \(O(^{-6})\), and communication complexity of \(O(^{-3})\) to reach an \(\)-stationary point. FCSG enjoys an appealing linear speedup with respect to the number of clients. To improve the empirical performances of FCSG, we also proposed a novel algorithm (_i.e._, FCSG-M), which achieves the same theoretical guarantees as FCSG. To fill the gap from lower-bound complexity, we introduced an accelerated version of FCSG-M, called Acc-FCSG-M, using variance reduction technique, which is optimal for the nonconvex smooth federated conditional stochastic optimization problems as it matches the best possible complexity result achieved by the single-machine method. It has a sample complexity of \(O(^{-5})\), and communication complexity of \(O(^{-2})\). And the communication complexity achieves the best communication complexity of the nonconvex optimization in federated learning. Experimental results on the machine learning tasks validate the effectiveness of our algorithms.

**Limitation** Conditional stochastic optimization has much broader applications and a more comprehensive evaluation of our proposed algorithms on other use cases would be a promising future direction. In addition, FCSG-M and accelerated FCSG-M require higher communication overhead.

 
**Datasets** & FedAvg & CODA+ & FCSG & FCSG-M & Acc-FCSG-M \\  MNIST & 0.9357 & 0.9733 & 0.9868 & 0.9878 & 0.9879 \\  CIFAR-10 & 0.5059 & 0.6039 & 0.7130 & 0.7157 & 0.7184 \\  

Table 2: Final averaged AP scores on the testing data.