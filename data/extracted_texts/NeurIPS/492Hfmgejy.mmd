# Lightweight Vision Transformer with Bidirectional Interaction

Qihang Fan \({}^{1,2}\), Huaibo Huang\({}^{1}\), Xiaoqiang Zhou\({}^{1,3}\), Ran He\({}^{1,2}\)

\({}^{1}\)MAIS & CRIPAC, Institute of Automation, Chinese Academy of Sciences, Beijing, China

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China

\({}^{3}\)University of Science and Technology of China, Hefei, China

fanqihang.159@gmail.com, huaibo.huang@cripac.ia.ac.cn,

xq525@mail.ustc.edu.cn, rhe@nlpr.ia.ac.cn

 Ran He is the corresponding author.

###### Abstract

Recent advancements in vision backbones have significantly improved their performance by simultaneously modeling images' local and global contexts. However, the bidirectional interaction between these two contexts has not been well explored and exploited, which is important in the human visual system. This paper proposes a **F**ully **A**daptive **S**elf-**A**ttention (FASA) mechanism for vision transformer to model the local and global information as well as the bidirectional interaction between them in context-aware ways. Specifically, FASA employs self-modulated convolutions to adaptively extract local representation while utilizing self-attention in down-sampled space to extract global representation. Subsequently, it conducts a bidirectional adaptation process between local and global representation to model their interaction. In addition, we introduce a fine-grained downsampling strategy to enhance the down-sampled self-attention mechanism for finer-grained global perception capability. Based on FASA, we develop a family of lightweight vision backbones, **F**ully **A**daptive **T**ransformer (FAT) family. Extensive experiments on multiple vision tasks demonstrate that FAT achieves impressive performance. Notably, FAT accomplishes a **77.6%** accuracy on ImageNet-1K using only **4.5M** parameters and **0.7G** FLOPs, which surpasses the most advanced ConvNets and Transformers with similar model size and computational costs. Moreover, our model exhibits faster speed on modern GPU compared to other models.

## 1 Introduction

Vision Transformers (ViTs) have recently garnered significant attention in the computer vision community due to their exceptional ability for long-range modeling and context-aware characteristics. However, because of the quadratic complexity of self-attention in ViT , its computational cost is extremely high. As a result, many studies have emerged to improve ViT's computational efficiency and performance in various ways. For instance, some methods restrict tokens that perform self-attention to a specific region and introduce inductive bias to ViT [34; 12; 66; 55]. Further, some methods aim to transform ViT into lightweight backbones with fewer parameters and computational requirements [38; 40; 4; 30; 37], achieving promising results but still not matching the performance of the most advanced ConvNets . How to design an excellent lightweight Vision Transformer remains a challenge.

In current state-of-the-art Vision Transformers, some either excel in creating local feature extraction modules [34; 12; 66] or employing efficient global information aggregation modules [57; 58], while others incorporate both [42; 41]. For instance, LVT  unfolds tokens into separate windows andapplies self-attention within the windows to extract local features, while PVT [57; 58] leverages self-attention with downsampling to extract global features and reduce computational cost. Unlike them, LITv2  relies on window self-attention and spatial reduction attention to capture local and global features, respectively. In terms of the local-global fusion, most methods use simple local-global sequential structures [38; 40; 37; 26; 82], whereas others combine local and global representation with simple linear operations through local-global parallel structures [48; 41; 43]. However, few works have investigated the bidirectional interaction between local and global information. Considering the human visual system where bidirectional local-global interaction plays an important role, these simplistic mixing methods are not fully effective in uncovering the intricate relationship between local and global contexts.

In Fig. 1, we illustrate how humans observe an object and notice its body details. Using the example of a fox, we can observe two types of interaction that occur when humans focus on either the fox's nose or the entire animal. In the first type of interaction, known as Local to Global, our understanding of the local feature transforms into the "Nose of a fox." In the second type of interaction, called Global to Local, the way we comprehend the global feature changes to the "A fox with nose." It can be seen that the bidirectional interaction between local and global features plays an essential role in the human visual system. Based on this fact, we propose that a superior visual model should not only extract good local and global features but also possess adequate modeling capabilities for their interaction.

In this work, our objective is to model the bidirectional interaction between local and global contexts while also improving them separately. To achieve this goal, we introduce three types of Context-Aware Feature Aggregation modules. Specifically, as shown in Fig. 1, we first adaptively aggregate local and global features using context-aware manners to obtain local and global tokens, respectively. Then, we perform point-wise cross-modulation between these two types of tokens to model their

Figure 1: Illustration of the human visual system (top) and our FASA (bottom). The human visual system can perceive both local and global contexts and model the bidirectional interaction between them. Our FASA follows this mechanism and consists of three parts: (a) local adaptive aggregation, (b) global adaptive aggregation, and (c) bidirectional adaptive interaction. Our FASA models local information, global information, and local-global bidirectional interaction in context-aware manners.

Figure 2: Top-1 accuracy v.s. FLOPs on ImageNet-1K of recent SOTA CNN and transformer models. The proposed Fully Adaptive Transformer (FAT) outperforms all the counterparts in all settings.

bidirectional interaction. We streamline all three processes into a simple, concise, and straightforward procedure. Since we use context-aware approaches to adaptively model all local, global, and local-global bidirectional interaction, we name our novel module the Fully Adaptive Self-Attention (FASA). In FASA, we also further utilize a fine-grained downsampling strategy to enhance the self-attention mechanism, which results in its ability to perceive global features with finer granularity. In summary, FASA introduces only a small number of additional parameters and FLOPs, yet it significantly improves the model's performance.

Building upon FASA, we introduce the Fully Adaptive Transformer (FAT) family. The FATs follow the hierarchical design [34; 57] and serve as general-purpose backbones for various computer vision tasks. Through extensive experiments, including image classification, object detection, and semantic segmentation, we validate the performance superiority of the FAT family. Without extra training data or supervision, our FAT-B0 achieves a top-1 accuracy of **77.6%** on ImageNet-1K with only **4.5M** parameters and **0.7G** FLOPs, which is the first model surpasses the most advanced ConvNets with similar model size and computational cost as far as we know. Additionally, as shown in Fig. 2, our FAT-B1, B2, and B3 also achieve state-of-the-art results while maintaining similar model sizes and computational costs.

## 2 Related Work

**Vision Transformer.** Since the earliest version of ViT  appeared, numerous works have been proposed that focus on enhancing the self-attention mechanism. Many methods restrict self-attention to a subset of tokens, sacrificing global receptive fields to reduce the computation of ViT [34; 12; 66; 61; 1]. Despite having only local perception capabilities, the context-aware nature of self-attention enables these methods to achieve excellent performance. A famous among them is the Swin-Transformer , which divides all tokens into windows and performs self-attention within each window, achieving highly competitive performance. In contrast, some methods downsample the \(K\) and \(V\) in self-attention to preserve the global receptive field while reducing computation by minimizing the number of token pairs involved in the calculation [57; 60; 40; 47; 68; 5]. One such method, PVT , leverages large-stride convolution to process \(K\) and \(V\), effectively lowering their spatial resolution. In addition to these methods, numerous efforts have been made to introduce ViT into the design of lightweight vision backbones [26; 40; 37; 15; 38; 4]. For example, MobileViT  concatenates convolution and self-attention to obtain a powerful, lightweight backbone. Nevertheless, a performance gap exists between lightweight vision transformers and state-of-the-art lightweight CNNs such as NAS-based EfficientNet .

**Local-Global Fusion.** A high-performance vision backbone typically possesses exceptional capabilities for both local and global perception. The capabilities are achieved by either connecting local and global perception modules in a serial manner [58; 31; 10; 26; 40; 55; 1], as demonstrated by DaViT , or by simultaneously modeling local and global information within a single module [48; 41; 43], such as the inception transformer . However, current approaches to fusing local and global information are overly simplistic. In serial models, the process of fusing local and global information is not adequately represented. In parallel structures, almost all methods rely on linear modules that depend entirely on trainable parameters to fuse local and global information [41; 48; 43]. These fusing approaches lack the ability to model the interaction between local and global information, which is inconsistent with the human visual system shown in Fig. 1. In contrast, our proposed FASA module models bidirectional interaction between local and global information while separately modeling each one.

**Self-Attention with Downsampling.** Currently, many models utilize self-attention with downsampling, a technique that was earlier used in PVT and so on to improve the computational efficiency [57; 58; 43; 41; 48; 61; 5]. PVT  reduces the spatial resolution of \(K\) and \(V\) using non-overlapping large stride convolutions, which decreases the number of token pairs involved in self-attention and lowers the computational cost while maintaining a global receptive field. Similarly, PVTv2  uses large stride average pooling to downsample \(K\) and \(V\). In contrast, inception transformer  employs large stride average pooling to downsample all three \(Q\), \(K\), and \(V\), then upsamples the tokens to their original size after self-attention is applied. However, using excessively large strides or non-overlapping downsampling as in these methods may lead to significant information loss. In FASA, we propose a fine-grained downsampling strategy to alleviate this issue.

## 3 Method

### Overall Architecture.

The overall architecture of the Fully Adaptive Transformer (FAT) is illustrated in Fig. 3. To process an input image \(x^{3 H W}\), we begin by feeding it into the convolutional stem used in . This produces tokens of size \(\). Following the hierarchical designs seen in previous works [48; 50; 46; 49], we divide FAT into four stages to obtain hierarchical representation. Then we perform average pooling on the feature map containing the richest semantic information. The obtained one-dimensional vector is subsequently classified using a linear classifier for image classification.

A FAT block comprises three key modules: Conditional Positional Encoding (CPE) , Fully Adaptive Self-Attention (FASA), and Convolutional Feed-Forward Network (ConvFFN). The complete FAT block is defined by the following equation (Eq. 1):

\[ X&=(X_{in})+X_{in},\\ Y&=((X))+X,\\ Z&=((Y))+(Y).\] (1)

Initially, the input tensor \(X_{in}^{C H W}\) passes through the CPE to introduce positional information for each token. The subsequent stage employs FASA to extract local and global representation adaptively, while a bidirectional adaptation process enables interaction between these two types of representation. Finally, ConvFFN is applied to enhance local representation further. Two kinds of ConvFFNs are employed in this process. In the in-stage FAT block, ConvFFN's convolutional stride is 1, and \(=\). At the intersection of the two stages, the ConvFFN's convolutional stride is set to 2, and \(=\) is achieved via a Depth-Wise Convolution (DWConv)  with the stride of 2 along with a \(1 1\) convolution. The second type of ConvFFN accomplishes downsampling using only a small number of parameters, avoiding the necessity of patch merging modules between the two stages, thereby saving the parameters.

### Fully Adaptive Self-Attention

In Fig. 1, we present the visual system of humans, which not only captures local and global information but also models their interaction explicitly. Taking inspiration from this, we aim to develop a similar module that can adaptively model local and global information and their interaction. This leads to the proposal of the **F**ully **A**daptive **S**elf-**A**ttention (FASA) module. Our FASA utilizes context-aware manners to model all three types of information adaptively. It comprises three modules:

Figure 3: Illustration of the FAT. FAT is composed of multiple FAT blocks. A FAT block consists of CPE, FASA and ConvFFN.

global adaptive aggregation, local adaptive aggregation, and bidirectional adaptive interaction. Given the input tokens \(X^{C H W}\), each part of the FASA will be elaborated in detail.

#### 3.2.1 Definition of Context-Aware Feature Aggregation

In order to provide readers with a clear comprehension of FASA, we will begin by defining the various methods of context-aware feature aggregation (CAFA). CAFA is a widely used feature aggregation method. Instead of solely relying on shared trainable parameters, CAFA generates token-specific weights based on the target token and its local or global context. These newly generated weights, along with the associated context of the target token, are then used to modulate the target token during feature aggregation, enabling each token to adapt to its related context. Generally, CAFA consists of two processes: aggregation (\(\)) and modulation (\(\)). In the following, the target token is denoted by \(x_{i}\), and \(\) represents a non-linear activation function. Based on the order of \(\) and \(\), CAFA can be classified into various types. Various forms of self-attention [56; 58; 57; 45] can be expressed as Eq. 2. Aggregation over the contexts \(X\) is performed after the attention scores between query and key are computed. The attention scores are obtained by modulating the query with the keys, and then applying a \(\) to the resulting values:

\[y_{i}=(((x_{i},X)),X),\] (2)

In contrast to the approach outlined in Eq. 2, recent state-of-the-art ConvNets [22; 17; 67] utilize a different CAFA technique. Specifically, they employ DWConv to aggregate features, which are then used to modulate the original features. This process can be succinctly described using Eq. 3.

\[y_{i}=((x_{i},X),x_{i}),\] (3)

In our FASA module, global adaptive aggregation improves upon the traditional self-attention method with fine-grained downsampling and can be mathematically represented by Eq. 2. Meanwhile, our local adaptive aggregation, which differs slightly from Eq. 3, can be expressed as Eq. 4:

\[y_{i}=(((x_{i},X)),(x_{i},X)),\] (4)

In terms of the bidirectional adaptive interaction process, it can be formulated by Eq. 5. Compared to the previous CAFA approaches, bidirectional adaptive interaction involves two feature aggregation operators (\(_{1}\) and \(_{2}\)) that are modulated with each other:

\[ y_{i2}&=(( _{1}(x_{i},X)),_{2}(x_{i},X)),\\ y_{i1}&=((_{2}(x_ {i},X)),_{1}(x_{i},X)).\] (5)

#### 3.2.2 Global Adaptive Aggregation

The inherently context-aware nature and capacity to model long-distance dependencies of self-attention make it highly suitable for adaptively extracting global representation. As such, we utilize self-attention with downsampling for global representation extraction. In contrast to other models that downsample tokens [57; 58; 60; 48] by using large stride convolutions or pooling operations, we adopt a fine-grained downsampling strategy to minimize loss of global information to the greatest extent possible. In particular, our fine-grained downsampling module is composed of several basic units. Each unit utilizes a DWConv with a kernel size of \(5 5\) and a stride of 2, followed by a \(1 1\) convolution that subtly downsamples both \(K\) and \(V\). After that, \(Q\), \(K\), and \(V\) are processed through the Multi-Head Self-Attention (MHSA) module. Unlike regular MHSA, we omit the last linear layer. The complete procedure for the global adaptive aggregation process is illustrated in Eq. 6 and Eq. 7. Fisrt, we define our fine-grained downsample strategy and the base unit of it in Eq. 6:

\[(X)_{1 1}(((X))),\\ (X)((^{(n)}(X)),\] (6)

where \(()\) represents the number of base units that are concatenated and \(\) denotes our fine-grained downsample operator. Then, the MHSA is conducted as Eq. 7:

\[ Q,K,V=W_{1} X,\\ X^{}_{global}=(Q,(K),(V)).\] (7)

where the mathematical symbol \(\) denotes the operation of matrix multiplication, \(W_{1}\) is a learnable matrix.

#### 3.2.3 Local Adaptive Aggregation

In contrast to self-attention, convolution possesses an inductive bias that facilitates extracting high-frequency local information. However, convolutional feature aggregation solely relies on parameter-shared convolutional kernels, resulting in a lack of context-awareness. To address this issue, we utilize a self-modulating convolutional operator that embeds context-awareness into convolution, enabling it to extract local representation adaptively. Specifically, we generate context-aware weights through the \(\) and combine them with \(\) to adaptively aggregate local information. This process is summarized in Eq. 8, where \(\) represents the Hadamard product, and \(Q\) is directly derived from Eq. 7 for saving parameters of linear projection:

\[& Q^{}=(Q),\\ & X^{}_{local}=Q^{}(Q^{}) =Q^{}}}.\] (8)

#### 3.2.4 Bidirectional Adaptive Interaction

As illustrated in Fig. 1, when modeling the fox and its nose separately, two types of interactions - "Local to Global" and "Global to Local" - occur between their respective features. Inspired by the human visual system, we design a bidirectional adaptive interaction process that incorporates both types of interactions. To achieve bidirectional interaction, we adopt a method similar to the one described in Sec. 3.2.3 but utilize cross-modulation instead of self-modulation. Specifically, the equation for Local to Global interaction is given by Eq. 9:

\[ X_{local}&=X^{}_{local} (X^{}_{global})=X^{}_{local}_{global}}}\\ &=Q^{}}}_{global}}},\] (9)

Similar to Eq. 9, Global to Local interaction is achieved by Eq. 10:

\[ X_{global}&=X^{}_{global} (X^{}_{local})=X^{}_{global}_{local}}}\\ &=X^{}_{global} {1+e^{-Q^{}}}}},\] (10)

After completing the two interaction processes, the local and global representation contain information about each other. To merge these representation, we use point-wise multiplication and achieve intermingling among channels with a linear projection, as shown in Eq. 11:

\[ Y&=W_{2}(X_{global} X_{ local})\\ &=W_{2}(X^{}_{global} Q^{}}}_{global}}}}}}})\] (11)

It is worth noting that to get a faster and implementable model, we omit the last high-order variable in Eq. 11 and arrive at a concise expression. More details can be found in appendix. Through the combination of local adaptive aggregation, global adaptive aggregation, and bidirectional adaptive interaction, we present the comprehensive FASA module. As depicted in Fig. 3, our model, which draws inspiration from the human visual system, is both straightforward and simple. It adaptively models local, global, and the interaction between the two in context-aware manners. More discussion about bidirectional adaptive interaction can be found in the appendix.

## 4 Experiments

We conducted experiments on a wide range of vision tasks, including image classification on ImageNet-1K , object detection and instance segmentation on COCO 2017 , and semantic segmentation on ADE20K . In addition to them, we also make ablation studies to validate the importance of each component. More details can be found in the appendix.

[MISSING_PAGE_FAIL:7]

during inference. Following , we fine-tune the models by AdamW with the learning rate of \(1 10^{-4}\) and batch size of 16. We train them for 80K iterations on the ADE20K training set.

**Results.** The results can be found in Tab. 2. For B0/B1/B2, the FLOPs are measured with input size of \(512 512\). While them for B3 are measured with \(512 2048\). All our models achieve the best performance in all comparisons. Specifically, our FAT-B1 exceeds EdgeViT-XS for **+1.5** mIoU. Moreover, our FAT-B3 outperforms the recent Shunted-S for **+0.7** mIoU. All the above results demonstrate our model's superiority in dense prediction.

### Object Detection and Instance Segmentation

**Settings.** We conduct object detection and instance segmentation experiments on the COCO 2017 dataset . Following Swin , we use the FAT family as the backbones and take RetinaNet  and Mask-RCNN  as detection and segmentation heads. We pretrain our backbones on ImageNet-1K and fine-tune them on the COCO training set with AdamW .

**Results.** Tab. 3 shows the results with RetinaNet and Mask R-CNN. The results demonstrate that the proposed FAT performs best in all comparisons. For the RetinaNet framework, our FAT-B0 outperforms EdgeViT-XXS by **+0.7** AP, while B1/B2/B3 also perform better than their counterparts. As for the Mask R-CNN, our FAT-B3 outperforms the recent Shunted-S by **+0.5** box AP and **+0.6** mask AP. All the above results tell that our Fully Adaptive Transformer outperforms its counterparts by evident margins. More experiments can be found in appendix.

### Comparison in Efficiency

We compared our approach with the state-of-the-art lightweight vision backbones, as shown in Tab 4. Our method achieved the best trade-off between speed and performance. The CPU is Intel i9Core and the GPU is V100. Inference throughput is measured with batch size 64. Inference latency is measured with batch size 1.

### Larger Model

We scale up our FAT to 50M+ and 85M+ to match the general Vision Transformer backbones. For object detection/instance segmentation, all models use the framework of Mask-RCNN  with the 1x schedule. The results are shown in Tab. 5. It can be found that FAT has great scalability.

    &  &  \\   & Params(M) & \(AP\) & \(AP_{50}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) \\  DFV-T  & - & - & - & - & - & - & - & 25 & 34.8 & 56.9 & 37.0 & 32.6 & 53.7 & 34.5 \\ PVTV-B0  & 13 & 37.2 & 57.2 & 39.5 & 23.1 & 40.4 & 49.7 & 24 & 38.2 & 60.5 & 40.7 & 36.2 & 57.8 & 38.6 \\ QuadTree-B0  & 13 & 38.4 & 58.7 & 41.1 & 22.5 & 41.7 & 51.6 & 24 & 38.8 & 60.7 & 42.1 & 36.5 & 58.0 & 39.1 \\ EdgeViT-XXS  & 13 & 38.7 & 59.0 & 41.0 & 22.4 & 42.0 & 51.6 & 24 & 39.9 & 62.0 & 43.1 & 36.9 & 59.0 & 39.4 \\ 
**FAR-B0** & 14 & 40.4 & 61.6 & 42.7 & 24.0 & 44.3 & 53.1 & 24 & 40.8 & 63.3 & 44.2 & 37.7 & 60.0 & 40.0 \\  DFV-T  & - & - & - & - & - & - & - & 32 & 39.2 & 62.2 & 42.4 & 36.3 & 58.9 & 38.6 \\ EdgeViT-XS  & 16 & 40.6 & 61.3 & 43.3 & 25.2 & 43.9 & 54.6 & 27 & 41.4 & 63.7 & 45.0 & 38.3 & 60.9 & 41.3 \\ VL-Tr2  & 17 & 40.8 & 61.3 & 43.6 & 26.7 & 44.9 & 53.6 & 27 & 41.4 & 63.5 & 45.0 & 38.1 & 60.0 & 40.8 \\ MPViT-T  & 17 & 41.8 & 62.7 & 44.6 & 27.2 & 45.1 & 54.2 & 28 & 42.2 & 64.2 & 45.8 & 39.0 & 61.4 & 41.8 \\ 
**FAR-B1** & 17 & 42.5 & **61.0** & **45.1** & **26.9** & **46.0** & **56.7** & **28 & **43.3** & **55.6** & **47.4** & **39.6** & **61.9** & **42.8** \\  PVTV-T  & 23 & 36.7 & 56.9 & 38.9 & 22.6 & 38.8 & 50.0 & 33 & 36.7 & 59.2 & 39.3 & 35.1 & 56.7 & 37.3 \\ ResT-Small  & 23 & 40.3 & 61.3 & 42.7 & 25.7 & 43.7 & 51.2 & 33 & 39.6 & 62.9 & 42.3 & 37.2 & 59.8 & 39.7 \\ PVTV-B1  & 24 & 41.2 & 61.9 & 43.9 & 25.4 & 44.5 & 54.3 & 34 & 41.8 & 64.3 & 45.9 & 38.8 & 61.2 & 41.6 \\ DFV-B1  & - & - & - & - & - & - & 58 & 43.4 & 63.2 & 48.2 & 39.0 & 51.8 & 42.0 \\ QuadTree-B-B1  & 24 & 42.6 & 63.6 & 45.3 & 26.8 & 46.1 & 57.2 & 34 & 43.5 & 65.6 & 47.6 & 40.1 & 62.6 & 43.3 \\ EdgeViT-S40  & 23 & 43.4 & 64.9 & 46.5 & 26.9 & 47.5 & 58.1 & 33 & 44.8 & 67.4 & 48.9 & 41.0 & 64.4 & 43.8 \\ MPViT-XS  & 20 & 43.8 & 65.0 & 47.1 & 28.1 & 47.6 & 56.5 & 30 & 44.2 & 66.7 & 48.4 & 40.4 & 63.4 & 43.4 \\ 
**FAR-B2** & 24 & 48.0 & 65.2 & 47.2 & 27.5 & 47.9 & 58.8 & 33 & 45.2 & 67.9 & 49.0 & 41.3 & 64.6 & 48.0 \\  Swin-T  & 38 & 41.5 & 62.1 & 44.2 & 25.1 & 44.9 & 55.5 & 48 & 42.2 & 64.6 & 46.2 & 31.9 & 61.6 & 42.0 \\ DAT-T  & 38 & 42.8 & 64.4 & 45.2 & 28.0 & 45.8 & 57.8 & 48 & 44.4 & 67.6 & 48.5 & 40.4 & 64.2 & 43.1 \\ DaViT- Tiny  & 39 & 44.0 & - & - & - & - & 48 & 45.0 & - & 41.1 & - & - \\ CMTS  & 44 & 44.3 & 65.5 & 47.5 & 27.1 & 48.3 & 59.1 & 45 & 44.6 & 66.8 & 48.9 & 40.7 & 63.9 & 43.4 \\ MPViT-S  & 32 & 45.7 & 57.3 & 48.8 & 28.7 & 49.7 & 59.2 & 43 & 46.4 & 68.6 & 51.2 & 42.4 & 65.6 & 45.7 \\ QuadTree-B2  & 35 & 46.2 & 67.2 & 49.5 & 29.0 & 50.1 & 61.8 & 45 & 46.7 & 68.5 & 51.2 & 42.4 & 65.6 & 45.7 \\ CSWu-T  & & - & - & - & - & 42 & 46.7 & 68.6 & 51.3 & 42.2 & 65.6 & 45.4 \\ Shunted-S  & 32 & 45.4 & 65.9 & 49.2 & 28.7 & 49.3 & 60.0 & 42 & 47.1 & 68.8 & 52.1 & 42.5 & 65.8 & 45.7 \\ 
**FAR-B3** & 39

### Ablation Study and Spectral Analysis

In this section, we conduct experiments to understand FAT better. The training settings are the same as in previous experiments. More experiments can be found in the appendix.

**Bidirectional Adaptive Interaction.** Initially, we validate the efficacy of bidirectional adaptive interaction by comparing it with three baselines for fusing local and global information: concatenation, addition and element-wise multiplication. As shown in Tab. 6, it can be observed that bidirectional adaptive interaction outperforms all baselines significantly across various tasks. Specifically, compared to the baseline, which uses the cat+linear to fuse the local and global information, our bidirectional adaptive interaction uses fewer parameters and FLOPs but surpasses the baseline by **1.0%** in Top1-accuracy. Our bidirectional adaptive interaction also performs better than element-wise multiplication.

    &  &  & ADE20K \\ Model & Params(M) & FLOPs(G) & Top-1(\%) & \(AP^{b}\) & \(AP^{m}\) & mIoU \\  add+linear & 4.5 & 0.72 & 76.2 & 39.0 & 35.8 & 39.6 \\ cat+linear & 4.8 & 0.77 & 76.6 & 39.6 & 36.3 & 40.2 \\ mul+linear & 4.5 & 0.72 & 77.1 & 40.3 & 37.1 & 40.9 \\ interaction & 4.5 & 0.72 & 77.6 & 40.8 & 37.7 & 41.5 \\  pool down & 4.4 & 0.71 & 77.2 & 40.2 & 36.9 & 40.6 \\ conv w/o overlap & 4.4 & 0.71 & 77.2 & 40.3 & 36.9 & 40.8 \\ conv w/ overlap & 4.5 & 0.71 & 77.3 & 40.5 & 37.3 & 40.9 \\ refined down & 4.5 & 0.72 & 77.6 & 40.8 & 37.7 & 41.5 \\  w/o conv. pos & 4.4 & 0.70 & 77.4 & 40.5 & 37.3 & 41.2 \\ conv. pos & 4.5 & 0.72 & 77.6 & 40.8 & 37.7 & 41.5 \\   

Table 6: Ablation of FAT

   Model & Params(M) & FLOPs(G) \(\) & CPU(ms) \(\) & GPU(ms) \(\) & Trp(ings/s) \(\) & Top1-acc(\%) \\  EdgeViT-XXS  & 4.1 & 0.6 & 43.0 & 14.2 & 1926 & 74.4 \\ MobileViT-XS  & 2.3 & 1.1 & 100.2 & 15.6 & 1367 & 74.8 \\ tiny-MOAT-0  & 3.4 & 0.8 & 61.1 & 14.7 & 1908 & 75.5 \\ FAT-B0 & 4.5 & 0.7 & 44.3 & 14.4 & 1932 & 77.6 \\  EdgeViT-XS  & 6.7 & 1.1 & 62.7 & 15.7 & 1528 & 77.5 \\ Parc-Net-S  & 5.0 & 1.7 & 112.1 & 15.8 & 1321 & 78.6 \\ EdgeNext-S  & 5.6 & 1.3 & 86.4 & 14.2 & 1243 & 79.4 \\ FAT-B1 & 7.8 & 1.2 & 62.6 & 14.5 & 1452 & 80.1 \\  ParC-ResNet50  & 23.7 & 4.0 & 160.0 & 16.6 & 1039 & 79.6 \\ tiny-MOAT-2  & 9.8 & 2.3 & 122.1 & 15.4 & 1047 & 81.0 \\ EfficientNet-B3  & 12.0 & 1.8 & 124.2 & 25.4 & 624 & 81.6 \\ FAT-B2 & 13.5 & 2.0 & 93.4 & 14.6 & 1064 & 81.9 \\   

Table 4: Comparison with other methods in efficiency.

   Model & Params(M) & FLOPs(G) & Top1-acc(\%) & \(AP^{b}\) & \(AP^{m}\) \\  Swin-S  & 50 & 8.7 & 83.0 & 44.8 & 40.9 \\ Focal-S  & 51 & 9.1 & 83.5 & 47.4 & 42.8 \\ CMT-B  & 46 & 9.3 & 84.5 & – & – \\ FAT-B4 & 52 & 9.3 & 84.8 & 49.7 & 44.8 \\  CSwin-B  & 78 & 15.0 & 84.2 & – & – \\ MOAT-2  & 73 & 17.2 & 84.7 & – & – \\ CMT-L  & 75 & 19.5 & 84.8 & – & – \\ FAT-B5 & 88 & 15.1 & 85.2 & – & – \\   

Table 5: Comparison with general backbones.

**Fine-Grained Downsampling.** We compared our fine-grained downsampling strategy with three other strategies: non-overlapping large stride pooling, non-overlapping large stride convolution, and overlapping large stride convolution. As shown in Tab. 6, experimental results across various tasks have demonstrated the effectiveness of our fine-grained downsampling strategy. Specifically, our fine-grained downsampling strategy surpasses directly pooling downsampling strategy by **0.4%**

**Positional Encoding.** At the end of Tab. 6, we explore the effect of CPE, and the results in the table show that the flexible CPE also contributes to the performance improvement of the model for **0.2%** in image classification.

**Spectral Analysis.** As shown in Fig. 4, we conduct a spectral analysis on FASA. Compared to add+linear fusion, our bidirectional adaptive interaction can more fully fuse high-frequency and low-frequency information.

## 5 Conclusion

In this paper, we propose a new attention mechanism named Fully Adaptive Self-Attention (FASA), which draws inspiration from the human visual system. FASA is designed to model local and global information adaptively while also accounting for their bidirectional interaction using adaptive strategies. Furthermore, we enhance the self-attention mechanism in FASA by incorporating a fine-grained downsampling strategy to promote better global perception at a more detailed level. Using the flexible FASA as our foundation, we develop a lightweight vision backbone called Fully Adaptive Transformer (FAT), which can be applied across various vision tasks. Our extensive experiments on diverse asks, such as image classification, object detection, instance segmentation, and semantic segmentation, provide compelling evidence of the effectiveness and superiority of FAT. We expect to apply FAT to other vision-related pursuits, including video prediction and vision-language models.