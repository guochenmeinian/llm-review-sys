# Constrained Latent Action Policies for

Model-Based Offline Reinforcement Learning

Marvin Alles\({}^{1,2}\) Philip Becker-Ehmck\({}^{1}\) Patrick van der Smagt\({}^{1,3}\) Maximilian Karl\({}^{1}\)

\({}^{1}\)Machine Learning Research Lab, Volkswagen Group \({}^{2}\)Technical University of Munich

\({}^{3}\)Eotvos Lorand University Budapest

{marvin.alles, philip.becker-ehmck, maximilian.karl}@volkswagen.de

Corresponding author.

###### Abstract

In offline reinforcement learning, a policy is learned using a static dataset in the absence of costly feedback from the environment. In contrast to the online setting, only using static datasets poses additional challenges, such as policies generating out-of-distribution samples. Model-based offline reinforcement learning methods try to overcome these by learning a model of the underlying dynamics of the environment and using it to guide policy search. It is beneficial but, with limited datasets, errors in the model and the issue of value overestimation among out-of-distribution states can worsen performance. Current model-based methods apply some notion of conservatism to the Bellman update, often implemented using uncertainty estimation derived from model ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP) which learns a generative model of the joint distribution of observations and actions. We cast policy learning as a constrained objective to always stay within the support of the latent action distribution, and use the generative capabilities of the model to impose an implicit constraint on the generated actions. Thereby eliminating the need to use additional uncertainty penalties on the Bellman update and significantly decreasing the number of gradient steps required to learn a policy. We empirically evaluate C-LAP on the D4RL and V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art methods, especially outperforming on datasets with visual observations.1

## 1 Introduction

Deep-learning methods are widely used in applications around computer vision and natural language processing, related to the fact that datasets are abundant. But when used for control of physical systems, in particular with reinforcement learning, obtaining data involves interaction with an environment. Learning through trial-and-error and extensive exploration of an environment can be done in simulation, but hard to achieve in real world scenarios [1; 2; 3]. Offline reinforcement learning tries to solve this by using pre-collected datasets eliminating costly and unsafe training in real-world environments [4; 5; 6].

Using online reinforcement learning methods in an offline setting often fails. A key issue is the distributional shift: the state-action distribution of the offline dataset, driven by the behavior policy, differs from the distribution generated by a learned policy. This leads to actions being inferred for states outside the training distribution. Therefore, value-based methods are prone to overestimating values due to evaluating policies on out-of-distribution states. This leads to poor performance and unstable training because of bootstrapping [6; 7; 8]. Offline reinforcement learning methods addressthis issue with different approaches and can be categorized into model-free and model-based methods, similar to online reinforcement learning.

Model-free offline reinforcement learning usually follows one of the following paradigms: constrain the learned policy to the behavior policy [9; 10; 11; 7]; or introduce some kind of conservatism to the Bellman update [12; 13; 14; 8]. Model-based reinforcement learning methods transform the offline to an online learning setting: They approximate the system dynamics and try to resolve the evaluation of out-of-distribution states by using the generalization capabilities of the model and generating additional samples. But as the training distribution is fixed, the estimation capabilities of the model are limited. Therefore, these model-based methods also rely on a conservative modification to the Bellman update as a measure to counteract value overestimation which is mostly achieved through uncertainty penalties [15; 16; 17; 18; 19; 20; 21]. Apart from the typical approach of using an auto-regressive model to estimate the dynamics, other model-based methods treat the objective as trajectory modeling [22; 23; 24]. These methods aim to combine decision making and dynamics modeling into one objective. Instead of learning a policy, they sample from the learned trajectory model for planning. We will refer to the first kind of methods, which learn a dynamics model to train a policy, as model-based reinforcement learning.

We aim to solve the problem of value overestimation in model-based reinforcement learning by jointly modeling action and state distributions, without the need for uncertainty penalties or changes to the Bellman update. Instead of learning a conditional dynamics model \(p(s a)\), we estimate the joint state-action distribution \(p(s,a)\). This is similar to methods that frame offline reinforcement learning as trajectory modeling, but we use an auto-regressive model and still learn a policy. By formulating the objective as a generative model of the joint distribution of states and actions, we create an implicit constraint on the generated actions, similar to [10; 25]. The goal of this approach is to address the shift in the entire distribution, rather than looking at out-of-distribution actions and states separately. We achieve this using a recurrent state-space model with a latent action space, which we call the recurrent latent action state-space model. Using a latent action space allows us to learn a policy that uses the latent action prior as an inductive bias. This approach keeps the policy close to the original data while allowing it to change when needed, which makes learning the policy much faster. To achieve this, we treat policy optimization as a constrained optimization problem, similar to enforcing a support constraint . We provide a high level overview of our method in Figure 1.

Figure 1: Overview of C-LAP. (a) The model is trained offline. It is encoding observations \(o_{t}\) and actions \(a_{t}\) to latent states (gray circle) and latent actions \(u_{t}\) (green circle), and decoding them thereafter. Furthermore it is predicting rewards \(_{t}\). (b) The policy is learned in the latent action space, but constrained to the support of the latent action prior, and uses the generative capabilities of the action decoder. Gradients are computed by back-propagating estimated values \(_{t}\) and rewards \(_{t}\) through the imagined trajectories. (c) The policy is used in the real world, again using the generative action decoder.1

Overall, we summarize our contribution as follows:

* We introduce latent action state-space models for model-based offline reinforcement learning, treating it as auto-regressive generative modeling of the joint distribution of states and actions.
* We formulate policy optimization as a constrained optimization problem, using the latent action space to generate actions within the support of the dataset's action distribution and jump-start policy learning by using the generative action decoder.
* We evaluate our approach on one benchmark with image observations (V-D4RL ) and on another one with low-dimensional feature observations (D4RL ).
* We evaluate the effect of our approach on value overestimation.

## 2 Preliminaries

We consider a partial observable Markov decision process (POMDP) defined by \(=(,,,T,R,,)\) with \(\) as state space, \(\) as action space, \(\) as observation space, \(s\) as state, \(a\) as action, \(o\) as observation, \(T:\) as transition function, \(R:\) as reward function, \(:\) as emission function and \((0,1]\) as discount factor. The goal is to find a policy \(:\) that maximizes the expected discounted sum of rewards \([_{t=1}^{T}^{t}r_{t}]\).

In online reinforcement learning, an agent iteratively interacts with the environment \(\) and optimizes its policy \(\). In offline reinforcement learning, however, the agent cannot interact with the environment and must refine the policy using a fixed dataset \(=\{(o_{1:T},a_{1:T},r_{1:T})_{n=1}^{N}\}\). Therefore, the agent must understand the environment using limited data to ensure the policy maximizes the expected discounted sum of rewards when deployed . Auto-regressive model-based offline reinforcement learning tries to learn a parametric function to estimate the transition dynamics \(T\). The transition dynamics model is then used to generate additional trajectories which can be used to train a policy. The majority of these approaches learn a dynamics model directly in observation space \(T_{}(o_{t} o_{t-1},a_{t-1})\)[16; 17; 18; 19; 28], while others use a latent dynamics model \(T_{}(s_{t} s_{t-1},a_{t-1})\)[20; 21].

## 3 Constrained Latent Action Policies

A main issue in offline reinforcement learning is value overestimation, which we address by ensuring the actions generated by the policy stay within the dataset's action distribution. Unlike previous model-based methods, we formulate the learning objective as a generative model of the joint distribution of states and actions. We do this by combining a latent action space with a latent dynamics model. Next, we use the generative properties of the action space to constrain the policy to the dataset's action distribution. A general outline of our method, Constrained Latent Action Policies (C-LAP ), is shown in Appendix B. It starts with learning a generative model, followed by actor-critic agent training on imagined trajectories, similar to the methods in [29; 30; 20; 21].

Generative modelModel-based offline reinforcement learning requires learning a model that is accurate in areas with low data coverage but also generalizes well. Therefore, it's crucial to balance

Figure 2: Recurrent latent action state-space model. The generative process is shown by solid lines and inference by dashed lines. Stochastic variables are denoted by circles and deterministic variables by rectangles.

staying within the dataset's distribution and generalizing to unseen states. We propose a generative model

\[p(o_{1:T},a_{1:T})= p(o_{1:T},a_{1:T} s_{1:T},u_{1:T})p(s_{1:T},u_{1:T}) \ ds\ du.\] (1)

that jointly models the observation and action distribution of a static dataset \(=\{(o_{1:T},a_{1:T},r_{1:T})_{n=1}^{N}\}\) by using latent states \(s_{t}\) along with latent actions \(u_{t}\). Unlike other model-based offline reinforcement learning methods that learn a conditional model \(p(o_{1:T} a_{1:T})\) and rely on ensemble based uncertainty penalties on the Bellman update to generate trajectories within the data distribution , our approach uses a latent action space to impose an additional implicit constraint. By implementing a policy in the latent action space, generated actions will stay within the dataset's action distribution, thus enabling generalization within the limits of the learned model . We empirically validate this claim in Appendix F. To obtain a state space model with Markovian assumptions on the latent states \(s_{t}\) we impose the following structure:

\[p(o_{1:T},a_{1:T} s_{1:T},u_{1:T})=_{t=1}^{T}p(o_{t} s_{t})p(a_{t } s_{t},u_{t}),\] (2)

\[p(s_{1:T},u_{1:T})=_{t=1}^{T}p(u_{t} s_{t})p(s_{t} s_{t-1},u_{t-1}).\] (3)

We implement the probabilistic model modifying the design of a recurrent state-space model . Thus, the latent dynamics model \(p(s_{t} s_{t-1},u_{t-1})\) is based on the deterministic transition \(f(h_{t-1},s_{t-1},a_{t-1})\) using the latent action decoder \(p_{}(a_{t-1} s_{t-1},u_{t-1})\) to generate actions. In the following, we mostly omit deterministic states \(h_{t}\) for notational brevity. The resulting recurrent latent action state-space model is shown in Figure 2 and consists of the following components, specifically

\[&p_{}(s_{t} s_{t-1},u_{t-1}),\\ &p_{}(u_{t} s_{t}),\\ &p_{}(o_{t} s_{t}),\\ &p_{}(a_{t} s_{t},u_{t}).\]

The latent state prior predicts the next latent state \(s_{t}\) given the previous latent state \(s_{t-1}\) and action \(u_{t-1}\) using the deterministic transition and the action decoder. The latent action prior predicts latent actions \(u_{t}\) given latent state \(s_{t}\). Latent states as well and as latent actions are decoded using their respective decoder. Similar to  actions are reconstructed given latent state and latent action.

Directly maximizing the marginal likelihood is intractable, hence we maximize the evidence lower bound (ELBO) on the log-likelihood \( p(o_{1:T},a_{1:T})\) instead. To approximate the true posterior, we introduce

\[&q_{}(s_{t} s_{t-1},a _{t-1},o_{t})\\ &q_{}(u_{t} s_{t},a_{t})\]

as inference models. The latent state posterior encodes observations \(o_{t}\) to latent states \(s_{t}\) by using the deterministic transition. The latent action posterior encodes actions \(a_{t}\) to latent actions \(u_{t}\) conditioned on latent states \(s_{t}\). All parameters of the generative model are indicated by \(\) and parameters of the inference model by \(\).

We derive the ELBO

\[ p(o_{1:T},a_{1:T})&_{t=1}^{T} _{s_{t},u_{t} q_{}}[(o_{t} s_ {t})}_{}+(a_{t} s_{t},u_{t })}_{}]\\ &-(q_{}(s_{t} s_{t-1},a_{t-1},o_{t})  p_{}(s_{t} s_{t-1},u_{t-1}))}_{}\\ &-(q_{}(u_{t} s_{t},a_{t}) p_{ }(u_{t} s_{t},))}_{}=:-L_{ELBO}(o_{1:T},a_{1:T}), \] (4)which can be organized into individual terms for reconstruction and consistency of actions and observations. The derivation can be found in Appendix A.

Maximizing the objective enables us to learn a model which can generate trajectories close to the data distribution \(\) by sampling from both priors. As we want to use the model to learn a policy via latent imagination, we add a reward \(p_{}(r_{t} s_{t})\) and termination \(p_{}(t_{t} s_{t})\) model. Hence, the complete model training objective is

\[L(o_{1:T},a_{1:T})=L_{ELBO}(o_{1:T},a_{1:T})-_{t=1}^{T}_{s_{t},u _{t} q_{}}[log(p_{}(r_{t} s_{t}))+log(p_{}(t_{t}  s_{t}))].\] (5)

Constrained latent action policyWe use the sequence model to generate imagined trajectories and use an actor-critic approach to train the policy. To predict state values we learn a value model \(v_{}(s_{t})\) alongside the policy. Therefore we use the n-step return of a state

\[V_{N}^{k}(s_{t})=_{s_{t} p_{},u_{t}_{}}[ _{n=}^{h-1}^{n-}r_{n}+^{h-}v_{}(s_{h})]  h=(+k,t+H)\] (6)

as regression target for \(v_{}(s_{t})\). Polices trained on trajectories generated by a model are prone to end up with degrading performance if the model only has access to a limited data distribution, as in the case of offline reinforcement learning. Compounding modeling errors and value overestimation of edge-of-reach states  are reasons for the decline. Since we train a generative action model, generated actions are implicitly constrained to the datasets action distribution by sampling from the action decoder \(a_{t} p_{}(a_{t} s_{t},u_{t})\). Hence, states outside the datasets observation distribution are hard to reach and our approach is resilient to value overestimation of edge-of-reach states. Compounding modeling errors are still a source of diminishing performance, but can be counteracted by increasing the representation power of the model or generating only short trajectories. To leverage the generative action model, we learn a policy \(_{}(u_{t} s_{t})\) in the latent action space similar to . But, as both, the latent action prior \(p_{}(u_{t} s_{t})\) and the policy \(_{}(u_{t} s_{t})\) are flexible, it is not ensured that they share the same support. Thus, we formulate policy optimization as a constrained optimization problem

\[_{}_{s_{t} p_{},_{t}_{ }}[_{=t}^{t+H}V_{N}^{k}(s_{})]\] (7) \[\ _{s_{t} p_{},_{t}_{ }}[p_{}(_{t} s_{t})]\]

similar to a support constraint . We implement the constraint explicitly through parametrization to stay within the support, but do not impose any restrictions inside the supported limits (Figure 3). This is different to using a divergence measure which on one hand does not strictly ensure support limits and on the other hand is more restrictive as it also imposes a constraint on the shape of a distribution. Here and in the following \(_{t}\) stands for a latent action sampled from the policy \(_{}(u_{t} s_{t})\).

The policy is trained to maximize the n-step return \(V_{N}^{k}(s_{})\) while staying in support of the latent action prior. Since the latent action prior \(p_{}(u_{t} s_{t})\) is normally distributed as \((_{}(s_{t}),_{}(s_{t}))\), we can express the constraint as

\[p_{}(_{t} s_{t})=p_{}(_{}+_{} s_{t})\] (8)with \(\) as a parameter setting support as multiples of \(_{}\) centered around \(_{}\). From the properties of a normal distributed variable follows that

\[_{}+_{}_{t}_{}- _{}.\] (9)

We implement the constraint explicitly by parameterizing the policy as a linear function \(g\) dependent on \(_{}(u_{t} s_{t})\) and \(p_{}(u_{t} s_{t})\):

\[_{}+_{} g(_{}(u_{t} s_{t} ),p_{}(u_{t} s_{t}))_{}-_{}.\] (10)

The support of the policy distribution is chosen to be bounded

\[_{t}_{}(u_{t} s_{t}),_{t}[-1,1]\] (11)

and \(g(_{}(u_{t} s_{t}),p_{}(u_{t} s_{t}))\) as a linear combination of the latent action predicted by the policy \(_{t}\) and the distribution parameters \(_{}\) and \(_{}\) of the latent action prior:

\[g(_{t},_{},_{})=_{}+_{t} _{}.\] (12)

## 4 Experiments

In the next section, we evaluate the effectiveness of our approach. It is divided into three parts: first, we assess the performance using standard benchmarks; then, we study how different design choices affect value overestimation; lastly, we analyze the influence of the support constraint parameter. We additionally provide the final performances in two tables in Appendix E.

We limit our benchmark evaluation to the most relevant state-of-the-art offline reinforcement learning methods to answer the following questions: 1) How do latent action state-space models compare to state-space models? 2) How comparable are model-free methods focusing on latent action spaces to latent action state-space models? 3) Does C-LAP suffer from value overestimation? 4) How does the support constraint affect the performance? 5) How does the performance differ between visual observations and observations with low-dimensional features? To focus on the latter, we separately evaluate the performance on low-dimensional feature observations using the D4RL benchmark , and on image observations using the V-D4RL benchmark .

### Benchmark results

D4RLSince most offline model-based reinforcement learning methods are designed for observations with low-dimensional feature observations, there exist many options for comparison. We make a selection to include the most relevant methods focusing on latent actions and auto-regressive model-based reinforcement learning. Therefore, we include the following methods: PLAS, which is a model-free method using a latent action space . MOPO, a probabilistic ensemble-based offline model-based reinforcement learning method using a modification to the Bellman update to penalize high variance in next state predictions . And MOBILE, which is similar to MOPO, but penalizes high variance in value estimates instead . We compare the algorithms on three different locomotion environments, namely halfcheetah, walker2d and hopper, with four datasets (medium-replay, medium, medium-expert, expert) each and the antmaze navigation environment with four datasets (umaze, umaze-diverse, medium-play, medium-diverse). The results, shown in Figure 4, display the mean and standard deviation of normalized returns over four seeds during the phase of policy training, with steps denoting gradient steps. The dashed lines indicate the asymptotic performance for MOPO and MOBILE. A detailed summary of all implementation details is provided in the Appendix D.

When comparing C-LAP to PLAS, we find that learning a joint generative model of actions and observations outperforms a generative model of only actions when used with actor-critic reinforcement learning. Both methods can use the generative nature of the model to speed up policy learning, which becomes especially clear in the results on all locomotion expert and medium-expert datasets. Compared to MOPO and MOBILE, C-LAP shows a superior or comparable performance on all datasets except halfcheetah-medium-replay-v2, halfcheetah-medium-v2 and hopper-medium-v2. Especially outperforming on the antmaze environment, where MOPO and MOBILE fail to solve the task for any of the considered datasets. The asymptotic performance of MOBILE on locomotion environments sometimes exceeds the results of C-LAP, but needs three times as many gradient steps.

Overall the results indicate that latent action state-space models with constrained latent action polices not only match the state-of-the-art on observations with low-dimensional features as observations, but also jump-start policy learning by using the action decoder to sample actions that lead to high rewards already after the first gradient steps: If the dataset is narrow, generated actions when sampling from the latent action prior will fall into the same narrow distribution. For instance, in an expert dataset, sampled actions will also be expert-level actions. During policy training, instead of sampling from this prior, we restrict the support of the policy dependent on the latent action prior. Thus, sampled latent actions from the policy will always be decoded to fall into the dataset's action distribution. So even a randomly initialized policy in the beginning of the training can generate a high reward by using the latent action decoder. This effect is especially prominent in narrow datasets such as expert datasets.

V-dtrlThere are currently few auto-regressive model-based reinforcement learning methods that specifically target visual observations, with none emphasizing latent actions. In our evaluation, we include LOMPO  and Offline DV2 . Both methods use a latent state space model and an uncertainty penalized reward. However the specifics of the penalty calculations are different: while LOMPO uses standard deviation of log probabilities as penalty, Offline DV2 uses mean disagreement. Additionaly, LOMPO trains an agent on a mix of real and imagined trajectories with an off-policy actor-critic approach, whereas Offline DV2 exclusively trains on imagined trajectories and back-propagates gradients through the dynamics model. Further implementation details are included in Appendix D.

C-LAP demonstrates superior performance across all datasets, especially significant on cheetah-run-medium_expert, walker-walk-medium_expert and walker-walk_expert. Datasets with a large

Figure 4: Evaluation on low-dimensional feature observations using D4RL benchmark datasets. We plot mean and standard deviation of normalized returns over 4 seeds.

diversity of actions, such as medium-replay datasets, exhibit a weaker inductive bias for a generative action model. Hence, they require more additional policy steps, as can be seen for both the D4RL and V-D4RL benchmarks.

### Value overestimation

Limiting value overestimation plays a central role in offline reinforcement learning. To evaluate the effectiveness of C-LAP, we report value estimates alongside normalized returns on all walker2d datasets in Figure 6. A similar analysis for all considered baselines is provided in Appendix G. To further analyze the influence of different action space design choices, we include the following ablations: a variant _no constraint_, which does not formulate policy optimization as constrained objective, but uses a Gaussian policy distribution to potentially cover the whole Gaussian latent action space; and a variant _no latent action_, which does not emphasize latent actions, but uses a regular state-space model as in Dreamer . Besides that, we added dashed lines to indicate the dataset's average return and average maximum value estimate. The _no latent action_ variant fails to learn

Figure 5: Evaluation on visual observations using V-D4RL benchmark datasets. We plot mean and standard deviation of normalized returns over 4 seeds.

Figure 6: Ablation study, comparing C-LAP to the following variants: no constraint, C-LAP without enforcing the policy constraint dependent on the action prior; no latent action, C-LAP without a latent action space similar to Dreamer . We plot mean and standard deviation of normalized returns and value estimates over 3 seeds. Moreover we add the datasetâ€™s average return and average maximum value estimate indicated by dashed lines.

an effective policy: normalized returns are almost zero and the dataset's reference returns remain unattained; value estimates are significantly exceeding the dataset's reference values, indicating value overestimation. The _no constraint_ variant can use the generative action decoder to limit generated actions to the dataset's action distribution, but the Gaussian policy is free to move to regions which are unlikely under the action prior. Thus, nullifying the implicit constraint imposed by the action decoder, resulting in collapsing returns and value overestimation. Only C-LAP achieves a high return and generates value estimates which are close to the dataset's reference. The value estimates on walker2d-medium-replay-v2 are higher than the dataset's reference, as the agent's performance is also exceeding the reference performance. The results confirm the importance of limiting value overestimation in offline reinforcement learning, and demonstrate that constraining latent action policies can be an effective measure for achieving this.

### Support constraint parameter

To evaluate the influence of the support constraint parameter \(\) on the performance of C-LAP, we perform a sensitivity analysis across all walker2d datasets (Figure 7). Except for the more diverse medium-replay-v2 dataset, adjusting \(\) from \(0.5\) to \(3.0\) only has a minor impact on the achieved return. However, when choosing an unreasonable large value such as \(=10.0\) or removing the constraint altogether (Figure 6), we observe a collapse during training. This highlights a key insight: constraining the policy to the support of the latent action prior is essential. And in many cases, using a smaller support region closer to the mean (small \(\)) proves sufficient.

## 5 Related Work

Offline reinforcement learning methods fall into two groups: model-free and model-based. Both types aim to tackle problems like distribution shift and value overestimation. This happens because the methods use a fixed dataset rather than learning by interacting with the environment.

Model-freeCurrent model-free methods typically work by limiting the learned policy or by regularizing value estimates. TD3+BC  adds a behavior cloning regularization term to the policy update objective to enforce actions generated by the policy to be close to the dataset's action distribution. Similarly, SPOT  includes a regularization term in the policy update, derived from a support constraint perspective. It also uses a conditional variational auto-encoder (CVAE) to estimate the behavior distribution. Following a comparable intention, BEAR  constraints the policy to the support of the behavior policy via maximum mean discrepancy. BCQ  and PLAS  use a CVAE similarly but don't use a regularization term. Instead, they constrain the policy implicitly by making the generative model part of the policy. Beside these methods, many other approaches exist, with CQL  and IQL  being some of the most well-known. CQL uses a conservative policy update by setting a lower bound on value estimates to prevent overestimation, while IQL avoids out-of-distribution values by using a modified SARSA-like objective in combination with expectile regression to only use state-action tuples contained in the dataset.

Model-basedModel-based offline reinforcement learning methods learn a dynamics model to generate samples for policy training. This basically converts offline learning to an online learning problem. Model-based methods mainly address model errors and value overestimation by using

Figure 7: Sensitivity analysis of the support constraint parameter \(\) for the considered D4RL walker2d datasets. We plot mean and standard deviation of normalized returns over 4 seeds.

a probabilistic ensemble and adding an uncertainty penalty in the Bellman update. MOPO  uses a probabilistic ensemble as in  and adds the maximum standard deviation of all ensemble predictions as uncertainty penalty. Similar to that, MOReL  adheres to the same methodology, but uses pairwise maximum difference of the ensemble predictions as penalty instead. Analogously, MOBILE  estimates the values for all by the ensemble predicted states and uses the standard deviation of value estimates as penalty. Edge of reach  comes to the conclusion that value estimation on edge of reach states are the overarching issue compared to model errors. In the end, they come up with a comparable solution to MOBILE, but use an ensemble of value networks alongside the ensemble of dynamic models. COMBO  pursues a different approach, as they integrate the conservatism of CQL into value function updates, removing the need for uncertainty penalties. Besides that, some methods use a different class of models: instead of learning a predictive model in observation space, they use latent state-space models to make predictions on latent states. Among these methods is LOMPO , which builds up on Dreamer , but integrates an ensemble to predict stochastic states and use the standard deviation of the log probability of the ensemble predictions as an uncertainty penalty similar to previous methods. The policy is trained on a mix of imagined and real world samples, hence they use an off-policy actor-critic style approach for policy learning. Offline DV2  uses a similar model, but is based on a different penalty. Namely, they use the difference between the individual ensemble mean predictions and mean over all ensembles as uncertainty penalty. Furthermore, the policy is trained only on imagined trajectories with gradients calculated by back-propagating through the dynamics model. Overall, Offline DV2 is the method most comparable to our approach, but still different in many ways as we propose a latent action state-space model compared to a usual state-space model, and frame policy learning as constrained optimization. So far all discussed models operate in an auto-regressive fashion, but another class of methods exists, which casts offline model-based reinforcement learning as trajectory modeling. Instead of learning a policy, these kind of approaches integrate decision making and modeling of the underlying dynamics into a single objective and use the model for planning. Among them are Diffuser , which employs guided diffusion for planning; TT , which builds on advances in transformers; and TAP , which uses a VQ-VAE with a transformer-based architecture to create a discrete latent action space for planning.

## 6 Conclusion

We present C-LAP, a model-based offline reinforcement learning method. To tackle the issue of value overestimation, we first propose an auto-regressive latent-action state space model to learn a generative model of the joint distribution of observations and actions. Second, we propose a method for policy training to stay within the dataset's action distribution. We explicitly parameterize the policy depending on the latent action prior and formulate policy learning as constrained objective similar to a support constraint. We find that C-LAP significantly speeds-up policy learning, is competitive on the D4RL benchmark and especially outperforms on the V-D4RL benchmark, raising the best average score across all dataset's from previously \(31.5\) to \(58.8\).

LimitationsDepending on the dataset and environment the effectiveness of C-LAP differs: Datasets which only contain random actions are challenging for learning a generative action model, thus we do not include them in our evaluation. The effect of jump-starting policy learning with the latent action decoder to already achieve high rewards in the beginning of policy training is prominent in narrow datasets, but less effective for diverse datasets. While training the model of C-LAP does not require additional gradient steps, it still takes more time compared to LOMPO  and Offline DV2  as the latent action state-space model is more complex than a usual latent state-space model.