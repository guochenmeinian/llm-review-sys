# Towards Understanding Extrapolation: a Causal Lens

Lingjing Kong\({}^{1}\)1 Guangyi Chen\({}^{1,2}\)1 Petar Stojanov\({}^{3}\) Haoxuan Li\({}^{2}\) Eric P. Xing\({}^{1,2}\)

Kun Zhang\({}^{1,2}\)

\({}^{1}\) Carnegie Mellon University

\({}^{2}\) Mohamed bin Zayed University of Artificial Intelligence

\({}^{3}\) Broad Institute of MIT and Harvard, Cancer Program, Eric and Wendy Schmidt Center

###### Abstract

Canonical work handling distribution shifts typically necessitates an entire target distribution that lands inside the training distribution. However, practical scenarios often involve only a handful of target samples, potentially lying outside the training support, which requires the capability of extrapolation. In this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution. To this end, we formulate the extrapolation problem with a latent-variable model that embodies the minimal change principle in causal mechanisms. Under this formulation, we cast the extrapolation problem into a latent-variable identification problem. We provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios. Our theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties. We showcase how our theoretical results inform the design of practical adaptation algorithms. Through experiments on both synthetic and real-world data, we validate our theoretical findings and their practical implications.

## 1 Introduction

Extrapolation necessitates the capability of generalizing beyond the training distribution support, which is essential for the robust deployment of machine learning models in real-world scenarios. Specifically, given access to a source distribution \(_{}:=p(_{},_{})\) with support \(_{}:=(p_{}())\) and one or a few out-of-support samples \(_{}_{}\), the goal of extrapolation is to predict the target label \(_{}\). For example, if the training distribution includes dog images, we aim to accurately classify dogs under unseen camera angles, lighting conditions, and backgrounds. While intuitive for humans, machine learning models can be brittle to minor distribution shifts [1; 2; 3; 4].

Addressing distribution shifts has garnered significant attention from the community. Unsupervised domain adaptation under covariate shifts addresses the shift of the marginal distribution \(p()\) across domains. However, canonical techniques such as importance sampling and re-weighting [5; 6; 7; 8; 9] are predicated on the assumptions of overlapping supports \((p_{}())(p_{}( ))\) and the availability of the entire target marginal distribution \(p_{}()\). Similarly, domain generalization [10; 11; 12] assumes access to multiple source distributions \(p_{}(,)\) whose supports jointly cover the target distribution. In addition to these methods, test-time adaptation (TTA) [13; 14; 15; 16] is particularly relevant to our discussion of extrapolation. TTA addresses out-of-distribution test samples at the individual sample level. Canonical methods include updating the source model with entropy-based or self-supervised losses on target samples. However, most TTA research focuses on empirical aspects, with limited theoretical formalization . Most related to our work, Kong et al.  and Li et al.

 propose theoretical frameworks to characterize distribution shifts and explore conditions for identifying latent changing factors. However, these frameworks assume access to multiple source distributions with overlapping supports, which are not directly applicable to the extrapolation problem considered in this work, where we have potentially only one out-of-support target sample \(_{}\).

Since the target \(_{}\) can be arbitrarily out of the source support (Figure 0(a)), extrapolation is ill-posed without proper assumptions on the relationship between the source and the target. In this work, we formulate a latent-variable model that encodes a _minimal change principle_ to address this ill-posedness. Specifically, we assume that a latent variable \(\) determines \(\) such that \(:=g()\). The minimal change principle entails the following two assumptions on the generating process. 1) The out-of-support nature of \(_{}\) stems from only a _subspace_ of \(\), denoted as \(\), while the complement partition \(\) for the target sample \(_{}\) is within the training support, i.e., \(:=[,]\), \(_{}_{}\), and \(_{}_{}\). 2) The changing variable \(\) only controls non-semantic attributes in \(\) and thus doesn't influence the label \(\), i.e., \(:=g_{y}()\). This formulation attributes the seemingly complex shifts in the pixel space of \(\) to simple intrinsic changes (in the sense that this change involves only \(\)) in the latent space, allowing us to reason about the transfer via the invariant variable \(\). Under our formulation, extrapolation amounts to identifying invariant latent variables \(\), with which a model \(f:\) trained on the labeled source dataset can be directly applied to the target sample.

In light of this formulation, we investigate the identifiability conditions of the invariant variable \(\). We propose two sets of conditions addressing two regimes of the influence from \(\) on \(\). We refer to the case where all dimensions \(_{i}\) (e.g., all pixels in an image) could be influenced by the changing variable \(\) as the dense influence and the case where only a limited subset of dimensions \(x_{i}\) is affected as the sparse influence. Specifically, our first condition (Dense Influence, Theorem 4.2) states that if \(\)'s influence is dense, then assuming that \(\) takes on only finite values, extrapolation requires the manifold associated with each value of \(\) (i.e., \(g(,)\) over \(\)) to be adequately separable, and the target changing variable \(_{}\) to be close to the source support \(_{}\). Intuitively, if images from two classes (two values of \(\)) are sufficiently distinguishable, such as cats and dogs, we can still recognize the class of the target sample \(_{}\), even if it has undergone moderate unseen shifts on all pixels like camera angles and positions controlled by \(\) (Figure 0(b)). Our second condition (Sparse Influence, Theorem 4.4) states that if \(\)'s influence is sufficiently sparse, then extrapolation can occur regardless of the distance of \(_{}\) to the source support \(_{}\). Intuitively, we can recognize the class "cow" even if only the background is changed, regardless of the severity (Figure 0(c)).

Figure 1: **Illustration of extrapolation and our theoretical conditions. The horizontal axis represents the changing variable \(\), ranging from the source support to out-of-support regions. The vertical axis represents the observed data \(\) living on the manifolds indexed by different values of the invariant variable \(\). Figure (a) demonstrates that given a point out of support it is unclear which class manifolds it belongs to. Figure (b) illustrates the dense shift condition (Theorem 4.2) where \(\) potentially changes all pixels in the images, such as the camera angle in the example. In this case, we can identify the invariant variable \(\) under a moderate amount of shift until the shift becomes excessive. For instance, the back view of the cat in the figure could be confused with other animals. Figure (c) illustrates the sparse shift condition (Theorem 4.4) where \(\) influences a limited number of pixels, such as the background in the example. In contrast to the dense shift, we can identify \(\) under the sparse shift regardless of its severity. In the figure, there is no ambiguity of the class “cow” even though the background has changed to the moon.**

Our theory provides insights into the interaction between the underlying manifold's smoothness, the out-of-support distance, and the nature of the shift. We conduct synthetic data experiments to validate our theoretical results. Moreover, we discuss the relationship between our results and TTA approaches. In particular, we apply our theoretical insights to improve autoencoder-based MAE-TTT  and observe noticeable improvements. Additionally, we demonstrate that basic principles (sparsity constraints) from our framework can benefit the state-of-the-art TTA approach TeSLA . Our empirical results not only show the practical viability of our theory but also pave the way for further advancements in the field.

In summary, our contributions are threefold:

* We formulate the extrapolation task as a latent-variable identification problem. Our latent-variable model encodes complex changes in observed variables to a partition of latent variables, allowing us to reason about transferability from the source to the target through latent variable identification.
* We provide identification guarantees for the proposed latent-variable model, including shifts of distinct properties (dense vs. sparse) and corresponding conditions on the generating process. Our theory provides an essential understanding of when latent-variable identification is possible without accessing an entire target distribution and assuming overlapping supports as in prior work [18; 19].
* Inspired by our theory, we propose to add a likelihood maximization term to autoencoder-based MAE-TTT  to facilitate the alignment between the target sample and the source distribution. In addition, we propose sparsity constraints to enhance the state-of-the-art TTA approach TeSLA . We validate our proposals with empirical evidence.

## 2 Related Work

**Extrapolation.** Out-of-distribution generalization has attracted significant attention in recent years. Unlike our work, the bulk of the work is devoted to generalizing to target distributions on the same support as the source distribution [22; 23; 8]. Recent work [24; 25; 26; 27] investigates extrapolation in the form of compositional generalization by resorting to structured generating functions (e.g., additive, slot-wise). Another line of work [28; 29; 30] studies extrapolation in regression problems and does not consider the latent representation. Saengkyongam et al.  leverage a latent variable model and linear relations between the interventional variable and the latent variable to handle extrapolation. In this work, we formulate extrapolation as a latent variable identification problem. Unlike the semi-parametric conditions in prior work, our conditions do not constrain the form of the generating function and are more compatible with deep learning models and tasks.

**Latent-variable identification for transfer learning.** In the latent-variable model literature, one often assumes latent variables \(\) generate the observed data \(\) (e.g., images, text) through a generating function. However, the nonlinearity of deep learning models requires the generating function to be nonlinear, which has posed major technical difficulty in recovering the original latent variable . To overcome this setback, a line of work [33; 34; 35; 36] assumes the availability of an auxiliary label \(\) for each sample \(\) and under different \(\) values, each component \(z_{i}\) of \(\) experiences sufficiently large shift in its distribution. Since this framework assumes all latent components' distributions vary over distributions indexed by \(\), it does not assume the existence of some shared, invariant information across distributions, which is often the case for transfer learning tasks. To address this issue, recent work [18; 19] introduce a partition of \(\) into an invariable variable \(\) and an changing variable \(\) (i.e., \(:=[,]\)) such that \(\)'s distribution remains constant over distributions. They show both \(\) and \(\) can be identified and one can directly utilize the invariant variable \(\) for domain adaptation. However, their techniques crucially rely on the variability of the changing variable \(\), mandating the availability of multiple sufficiently disparate distributions (including the target) and their overlapping supports. These constraints make them unsuitable for the extrapolation problem. In comparison, our theoretical results give identification of the invariant variable \(\) (the on-support variable in the extrapolation context) with only one source distribution \(p_{}()\) and as few as one out-off support target sample \(_{}\) through mild assumptions on the generating function, directly tackling the extrapolation problem.

Please refer to Section A1 for more related work.

## 3 Extrapolation and Latent-Variable Identification

Given the labeled source distribution \(p(_{},_{})\), our goal is to make predictions on a target sample \(_{}\) outside the source support (\(_{}_{}\)). While more target samples would provide better information about the distribution shift, in practice, we often have only a handful of samples to work with. Therefore, we focus on the challenging scenario where only one target sample \(_{}\) is available.

Making reliable predictions on out-of-support samples \(_{}\) is infeasible without additional structure. Real-world problems where humans successfully extrapolate often follow a minimal change principle: they involve sparse, non-semantic intrinsic shifts despite complex raw data changes. For example, a person who has only seen a cow on a pasture can recognize the same cow on a beach, even if the background pixels change significantly. Here, the cow corresponds to the part of the latent variable that remains within the support of the source data, which we call the invariant variable \(\) (\(_{}_{}\)), while the background change corresponds to the complement that drifts off the source support, which we call the changing variable \(\) (\(_{}_{}\)). Clearly, extrapolation is impossible if the intrinsic shift is dense (i.e., all dimensions change, \(=\)) or semantic (i.e., \(\) is a function of \(\)). For instance, if the variable \(\) also alters the cow's appearance drastically, making it unrecognizable, extrapolation fails. We define the data-generating process to encapsulate this minimal change principle, as follows:

\[& p(),\  p(|);\\ &=g(),\ =g_{}(). \] (1)

In this process, the latent space \(^{d_{}}\) comprises two subspaces: the invariant variable \(^{d_{}}\) and the changing variable \(^{d_{}}\). We define \(:=_{}\{_{}\}\) as the source support augmented with the target sample \(_{}\) and similarly \(\) and \(\). The invariant variable \(\) encodes shared information between the source distribution \(p(_{})\) and the out-of-support target sample \(_{}\), while the changing variable \(\) describes the shift from the source support \(_{}\). Hence, \(_{}_{}\) and \(_{}_{}\). The variables \(:=[,]\) jointly generate the observed variable \(^{d_{}}\) through an invertible generating function \(g:^{d_{}}^{d_{}}\). Furthermore, we assume that the label \(\) originates from the invariant variable \(\). This assumption reflects the reality that factors such as camera angles and lighting do not affect the object's class in an image.

Our latent-variable model adheres to the minimal change principle in two key ways: (1) the target sample's out-of-support nature arises from only a subset of latent variables \(\), and (2) these changing variables \(\) are non-semantic, thus not influencing the label \(\).

**Extrapolation and identifiability.** Under this framework, extrapolation is possible if we can identify the true invariant variable \(\) in both the source distribution \(p_{}()\) and the target data \(_{}\). This allows us to learn a classifier \(f_{}:\) on the labeled source distribution \(p_{}(,)\). Since the target sample's invariant variable falls within the source support (\(_{}_{}\)), this classifier \(f_{}\) can be directly applied to the target sample \(_{}\). Thus, the task of extrapolation reduces to identifying the invariant variable \(\) in both the source distribution \(p(_{})\) and the target sample \(_{}\). In Section 4, we explore the conditions for identifying the invariant variable \(\).

Given the above reasoning, we define identifiability in Definition 3.1 (i.e., block-wise identifiability [37; 24]) which suffices for extrapolation.

**Definition 3.1** (Identifiability of the Invariant Variable \(\)).: For any \(_{1}\) and \(_{2}\), their true invariant variables \(_{1}\), \(_{2}\) are equal if and only if the estimates \(}_{1}\), \(}_{2}\) are equal: \(_{1}=_{2}}_{1}=}_{2}\).

## 4 Identification Guarantees for Extrapolation

In this section, we provide two sets of conditions on which one can identify the invariant variable \(\) and discuss the intuition and implications.

As discussed in Section 3, we need to identify the target sample \(_{}\) with source samples \(_{}\) that share the same invariant variable values with the target sample, i.e., \(_{}=_{}\). This enables us to obtain the label of \(_{}\) by assigning the label of such \(_{}\). The shift between the source distribution

Figure 2: **The data-generating process. The invariant latent variable \(c\) and the changing latent variable \(s\) jointly generate the observed variable \(x\). The dashed line indicates potential statistical dependence.**\(p(_{})\) and the target sample \(_{}\) originates from the out-of-support nature of the changing variable \(_{}\), i.e., \(_{}_{}\), it is crucial to impose proper assumptions on the influence of \(\) on \(\) so that \(\) retains sufficient footprints of \(\) for identification beyond the source support.

We denote the Jacobian matrix of the generating function \(g\) as \(_{g}()\) and \(\)'s dimensions under the influence of \(\) as \(_{}():=\{i[d_{}]: j\{d_ {}+1,,d_{}\},,\,[_{g}( )]_{i,j} 0\}\). We note that the set \(_{}()\) is a function of \(\), since the influenced dimensions may vary over \(\). Intuitively, if \(\) influences \(\) in a dense manner, i.e., large \(|_{}()|\) for potentially all dimensions \(x_{i}\), there may not be dimensions of \(\) serving as clear signatures of \(\), thereby increasing the difficulty of identify \(\). Additionally, the degree to which the changing variable \(\) is out-of-support plays a critical role - the further the target changing variable- the further the target changing variable \(_{}\) deviates from the source distribution support \(_{}\), the more severe and unpredictable the shift becomes, making it harder to retrieve \(\). In the following, we formalize conditions on the influence of \(\) from these two perspectives, revealing an interesting trade-off and interaction between these factors.

**Notations.** The true generating process involves \(\), \(\), distributions \(p\), and \(g\) (Equation 1), we define their statistical estimates with \(}\), \(}\), \(\), and \(\) through the objectives we will introduce.2 We assume that the estimation process respects the conditions of the corresponding true-generating process.

### Dense-shift Conditions

We begin by investigating scenarios where there are no constraints on the number of dimensions of \(\) (i.e., the number of pixels) influenced by the changing variable \(\), i.e., potentially large \(|_{}()|\), which we term as dense shifts. For images, these shifts encompass global transformations such as changes in camera angles and lighting conditions that could potentially affect all pixel values (Figure 0(b)).

**Understanding the problem.** As dense shifts could influence all the dimensions of \(\), every dimension could be out of the source support and there might not be dimensions of \(\) that solely contain the information of \(\). Consequently, relying on any subset of \(\) dimensions to infer the original \(\) becomes untenable. For instance, consider a scenario where the source distribution contains frontal-view images of a cat, while the target sample portrays the same cat from a side view (Figure 0(b)). The model cannot recognize these two images as the same cat (the same \(\)) by matching a specific part of the side view, say the cat's nose, to samples in the source distribution because this cat's nose only shows up as a front view and can be vastly different in terms of the pixel region and values. The model cannot match specific features such as the cat's nose, between the side-view target and the source distribution, as the pixel region and values for the nose drastically differ.

**Our approach.** For the reasons above, we need to constrain such dense changes so that even when all dimensions are affected, the target sample adheres to some intrinsic structure determined by the underlying \(_{}\) and remains distinguishable from samples of \(_{}\). In many real-world distributions, we can interpret \(\) as the embedding vector of classes or other categories, with each \(\) value indexing a manifold \(g(,)\) over \(\). If manifolds are smooth and sufficiently separable from each other, they should exhibit limited variations in the adjacent region to the training support, avoiding confusion between distinct categories. For example, there exists a noticeable distinction between cats and lions, such that moderate illumination changes would not cause confusion until illumination significantly obscures distinguishing features. In the following, we formalize these structures by assuming a finite cardinality of \(\) and constraining the distance of \(_{}\) to the support \(_{}\).

**Additional notations.** We denote with \(J_{u}\) an upper bound of the Jacobian spectrum norm: \(\|_{g}()\| J_{u}\) on the support. In Appendix A.2, we show \(J_{u}<\) due to Assumption 4.1-i and Assumption 4.1-ii. We denote with \(D(_{1},_{2})\) the \(_{2}\) distance between two manifolds on the support boundary: \(D(_{1},_{2}):=_{_{1},_{2} (_{})}\|g(_{1},_{1})-g( _{2},_{2})\|\), where we denote the boundary of source support with \((_{})\). We denote with \(D(,_{})\) the minimal \(_{2}\) distance between \(\) and the source support \(_{}\), i.e., \(D(,_{}):=_{_{} _{}}\|-_{}\|\).

**Assumption 4.1** (Identification Conditions under Global Shifts).:
1. _[_Smoothness & Invertibility]:_ _The generating function_ \(g\) _in Equation_ 1 _is a smooth invertible function with a smooth inverse everywhere._
2. _[Compactness]:_ _The source data space_ \(_{}^{d_{x}}\) _is closed and bounded.__;_
3. _[Discreteness]: The invariant variable_ \(\) _takes on values from a finite set:_ \(=\{_{k}\}_{k[K]}\)_._
4. _[Continuity]: The probability density function_ \(p(|)\) _is continuous over_ \(_{}\)_, for all_ \(\)_._
5. _[Out-of-support Distance]: The target sample's out-support components_ \(_{}\)_'s distance from the source support_ \(_{}\) _is constrained:_ \(_{_{}}\|_{}- \|}(_{} )\,D(_{},)}{2J_{}}\)_._

**Discussion on the conditions.** As discussed above, the main conditions revolve two key factors: the discrete structure of the invariant distribution of \(p()\) in Assumption 4.1-iii and the off-support distance of the changing variable \(\) in Assumption 4.1-v. The discrete structure of \(p()\) is applicable to many real-world scenarios, especially classification tasks where the semantic invariant information often manifests as discrete class labels or other categorical distinctions. While this assumption is typically valid, it can be extended to encompass continuous dimensions in the invariant variable \(:=[_{},_{}]\) where \(_{}\) and \(_{}\) stand for the continuous and discrete dimensions, respectively. In such cases, we can group the continuous dimensions \(_{}\) with the changing variable \(\) and the same proof would give rise to the identification of the discrete part \(_{}\), which suffices for classification tasks. The off-support distance condition involves the smoothness of the generating function \(g\), where a smoother generating function allows more leeway for the target changing variable \(_{}\) to deviate. When \(\) controls the camera angle, one may be able to recognize a slightly sided view of cats after seeing front views in the source until the \(\) deviates too far and all images become back views, potentially leading to confusion with other animals (Figure 0(b)). Assumption 4.1-i ensures that the generating process preserves the latent information, which is widely adopted in the literature [18; 19; 35; 36; 33; 38]. Specifically, this guarantees that manifolds indexed by distinct values of \(\) are separate from each other, maintaining strictly positive distances between them. Assumption 4.1-ii,iv are technical conditions mirroring realities that pixels values are bounded and the changing variable s often represent attributes that vary gradually across its support (e.g., lighting and angles).

**Theorem 4.2** (Extrapolation under Dense Shifts).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((,(}),(}))\) with the objective:_

\[(}_{}),( )=p(),\;_{}; }_{}*{arg\,inf}_{}}D(},}_{}).\] (2)

_Under Assumption 4.1, the estimated model can attain the identifiability in Definition 3.1._

**Proof sketch.** We estimate the generative process through maximum likelihood estimation on the source distribution \((})=p()\). Under Assumption 4.1-ii,iii,iv, we can establish the identification of \(\) on the _support_, i.e., \(}_{1}=}_{2}_{1}=_{2}\), for \(_{1},_{2}_{}\). This implies that all samples \(_{}\) on a given source manifold \(g(,)\) share identical values of \(}\). In the objective, we maximize the likelihood \((}_{})\) to drive \(}_{}\) to match one of the discrete values of \(}\) with nontrivial probability mass. Given the identification of \(\) on the support, this equates to assigning \(_{}\) to a manifold \(g(^{*},)\) with \(^{*}\). Our task now switches to ensuring that this is the correct manifold for \(_{}\), i.e., \(^{*}=_{}\). To accomplish this, we select the estimated model that uses the minimal off-support distance on \(}\) (i.e., \(D(},}_{})\)) to explain the off-support nature of \(_{}\). This also embodies the minimal change principle. This and Assumption 4.1-v guarantee that only the correct manifold (\(g(_{},)\)) can effective capture \(_{}\), thereby facilitating the desired identification. In practice, these constraints can be implemented through Lagrange multipliers. Full proof is given in Appendix A2.

### Sparse-shift Conditions

We now examine cases where the changing variable \(\) influences only a subset of dimensions of \(\), i.e., a limited \(|_{}()|\), which we refer to as sparse shifts. For image distributions, these shifts include local corruptions or background changes that do not alter foreground objects (Figure 0(c)).

**Additional notations.** We define the index set \(_{}()\) under the influence of \(\) and the indices under the the exclusive influence of \(\) as \(_{}():=_{ }()_{}()\).

**Understanding the problem.** In contrast to the dense-shift scenario, here we have a non-trivial subset of dimensions \([]_{_{}()}\) that are unaffected by the changing variable \(\). Consequently, if these dimensions carry sufficient information about \(\), we can exploit them to directly recover the true \(\), regardless of the distance \([]_{_{}()}\) deviates from the support. In contrast, in the dense-shift scenario, we need to constrain the out-of-support distance of \(\) and assume the discreteness of \(\). Considera scenario where a fixed \(\) represents a specific cow and \(\) controls only the background. Despite the variation in the target background (e.g., desert or space), we can effectively match the cow in the target image to the correct source images (see Figure 0(c)). While this may seem intuitive for humans, it is nontrivial for machine learning models to automatically recognize the region \([]_{_{,}()}\), especially given its potential variation across \(\).

**Our approach.** For image classification, \([]_{_{,}()}\) corresponds to foreground objects (or a portion) unaffected under sparse changes induced by \(\) (e.g., background changes). Humans can recognize this region because the pixels within it are strongly correlated (e.g., cow features). This observation motivates us to formalize such dependence structures in natural data to enable automatic identification.

**Assumption 4.3** (Identification Conditions under Local Shifts).:
1. _[Smoothness & Invertibility]: The generating function_ \(g\) _in Equation_ 1 _is invertible and differentiable, and its inverse is also differentiable._
2. _[Invariant Variable Informativeness]: The dimensions under_ \(\)_'s exclusive influence is uniquely determined: for a fixed_ \(\)_,_ \([]_{_{,}(,_{1}) }[]_{_{,}(^{*},_{2})}\) _for any_ \(^{*}\)_,_ \(_{1}\)_, and_ \(_{2}\)_._
3. _[Sparse Influence]: At any_ \(\)_, the changing variable_ \(\) _influences at most_ \(d_{}\) _dimensions of_ \(\)_, i.e.,_ \(|_{}()| d_{}\)_. Alternatively, the two variables_ \(\) _and_ \(\) _do not intersect on their influenced dimensions_ \(_{}()_{}()=\)_._
4. _[Mechanistic Dependence]: For all_ \(\)_, any nontrivial partition_ \(_{1},_{2}\) _of the dimensions_ \(_{}()\) _yields dependence between the sub-matrices of the Jacobian_ \(_{g}()\)_: rank_\(([_{g}()]_{_{,} ()})<([_{g}()]_{_{1}}( ))+([_{g}()]_{_{2}}( ))\)_._

**Discussion on the conditions.** Assumption 4.3-iii stipulates that the influence of \(\) is sparse, either in terms of dimension counts or in its intersection with the influence from \(\). It is noteworthy that while the influence is sparse, its location can vary over images, as indicated by the dependence of \(_{}\) on \(\). Consequently, it can capture diverse image corruptions and background changes. Assumption 4.3-ii ensures that \([]_{_{,}}\) is sufficiently informative about \(\). For instance, it precludes scenarios where a sparse corruption alters the top stroke of "7" to resemble "1", rendering the uncorrupted region fundamentally unidentifiable. Assumption 4.3-iv enforces the dependence alluded to in our previous discussion: the unaffected dimensions \([]_{_{,}}\) exhibit mechanistic dependence across them, characterized by the Jacobian rank . Thus, generating separate parts of an object necessitates more capacity than generating the entire object, as the dependence across the two parts can inform each other's generation. This inherent dependence enables the identification of the unaffected region.

**Theorem 4.4** (Extrapolation under Sparse Shifts).: _Assuming a generating process in Equation 1, we estimate the distribution with model \((,(}),(}))\) with the objective:_

\[(}_{}),( )=p(),\,_{}.\] (3)

_Under Assumption 4.3, the estimated model can attain the identifiability in Definition 3.1._

**Proof sketch.** Maximizing the likelihood \((}_{})\) assigns a value \(}^{*}}\) to \(}_{}\). Building on our motivation, we leverage mechanistic dependence (Assumption 4.3-iv) to identify the unaffected dimension indices \(_{}()[d_{}]\) with our estimated model. In other words, we have \(_{}()=}_{ }(})\). Consequently, the unaffected dimensions in the estimated variable equal their counterparts in the true model: \([_{}]_{_{}( }^{*},}^{*})}=[_{}]_{ _{}(_{}, _{})}\). Furthermore, Assumption 4.3-ii stipulates that the dimensions in the target sample \([_{}]_{_{}( _{},_{})}\) cannot be attained by other \(_{}\), so we have established that \(}^{*}\) corresponds to the correct value \(_{}\). Full proof is in Appendix A3.

It's worth noting that unlike the global shift case (Theorem 4.2), here we do not place a constraint on the out-of-support-ness of \(_{}\), a point we empirically verify in Section 6.3.

### Implications for Practical Algorithms

**Generative adaptation.** Our theoretical framework, inherently a generative model, can be implemented through auto-encoding over the source distribution and the target. Akin to our estimation framework, MAE-TTT  trains a masked auto-encoding model (\(f_{}\) and \(f_{}\)) on the source distribution and adapts to target samples through the auto-encoding objective. Consequently, we have \(f_{}(f_{}())\) for \(_{}\{_{}\}\), which approximates the distribution-matching aspect of our estimation objectives Equation 2 and Equation 3.

Despite the resemblance on the reconstruction objective, MAE-TTT does not explicitly perform the representation alignment as our objectives - a classifier \(f_{}\) is only trained on the labeled source distribution, which takes in \(f_{}\)'s output \(}\) and produces logit values. In addition, our objectives entail maximizing the target likelihood \((}_{})\) to align \(}_{}\) to the source support \(}_{}\). As large logit values indicate the sample is close to distribution modes [40; 41] and \(f_{}\) is enforced invertible through auto-encoding, we can interpret minimizing the entropy of \(f_{}(}_{})\) as filtering \(}_{}\) to obtain \(}_{}\) and driving it towards the modes of \((})\). Therefore, we implement the entropy minimization loss \(-_{y}f_{}(}_{})_{y}( f_{}(}_{})_{y})\) as a surrogate for maximizing \(p(})\). We show that this significantly boosts the performance of MAE-TTT in Section 6.1.

**Regularization.** While our objectives simultaneously involve the source distribution and the target sample, the source distribution may not be accessible during adaptation. Aggressive updates on the target sample may distort the source information stored in the model and ultimately impair the performance. To address this, we propose to impose regularization on the source-pretrained backbone during adaptation to enforce minimal changes and preserve the source information. In Section 6.2, we instantiate this with low-rank updates and sparsity constraints, showcasing the resultant benefits.

## 5 Synthetic Data Experiments

In this section, we conduct synthetic data experiments on classification to directly validate the theoretical results in Section 4. We present additional experiments on regression in Section A4.2.

**Experimental setup.** We generated the synthetic data following the generative process in Equation 1, with \(d_{}=4\) and \(d_{}=2\). We focus on binary classification and sample class embeddings \(_{1}\) and \(_{2}\) from \((0,_{c})\) and \((2,_{c})\) respectively. We sample \(_{}\) from a truncated Gaussian centered at the origin and sample \(_{}\) at multiple distances from the origin. For the dense-shift case, we concatenate \(\) and \(\) and feed them to a well-conditioned 4-layer multi-layer perceptron (MLP) with ReLU activation to obtain \(\). For the sparse-shift case, we pass \(\) to a 4-layer MLP to obtain a 4-d vector. We duplicate \(2\) dimensions of this vector and add \(\) to it. The final \(\) is the concatenation of the \(4\)-d vector and the \(2\)-d vector. We sample \(10\)k points for the source distribution and 1 target sample for each run. We perform \(50\) runs for each configuration and compute the accuracy on the target samples. More details can be found in Appendix A4.

**Results and discussions.** We compared our method with iMSDA  and a model trained only on source data. The results in both dense and sparse shift settings are summarized in Table 1. Our method consistently outperforms both baseline methods (nearly random guesses) by a large margin on all sub-settings, validating our theoretical results. The results on iMSDA suggest that directly applying domain-adaptation methods to the extrapolation task may result in negative effects for lack of the target distribution in their training.

## 6 Real-world Data Experiments

We provide real-world experiments to validate our theoretical insights for practical algorithms (Section 4.3) and theoretical results (Section 4.2). More results can be found in Appendix A5. 3

  
**Shifts** &  &  \\ 
**Distance** & 12.0 & 18.0 & 24.0 & 30.0 & 18.0 & 24.0 & 30.0 & 36.0 \\  Only Source & \(0.59\) & \(0.55\) & \(0.45\) & \(0.45\) & \(0.54\) & \(0.54\) & \(0.56\) & \(0.52\) \\ iMSDA  & \(0.46\) & \(0.48\) & \(0.48\) & \(0.50\) & \(0.50\) & \(0.36\) & \(0.40\) & \(0.54\) \\
**Ours** & **0.78** & **0.69** & **0.72** & **0.72** & **0.72** & **0.72** & **0.76** & **0.70** \\   

Table 1: **Synthetic data test accuracy under both dense and sparse shifts across a range of distances.**

### Generative Adaptation with Entropy Minimization

As discussed in the first implication in Section 4.3, we incorporate an entropy-minimization loss to MAE-TTT and compare it with the original MAE-TTT.

**Experimental setup.** We conduct experiments on ImageNet-C  and ImageNet100-C  with 15 different types of corruption. For the baseline, we utilize the publicly available code of MAE-TTT. In our approach, we do not directly integrate the entropy-minimization loss into the MAE-TTT framework. This is because the training process of self-supervised MAE relies on masked images, whereas entropy-minimization requires the classification of the entire image. To address this, we introduce additional training steps with unmasked images and apply the entropy-minimization loss during these steps. Specifically, the training process for each test-time iteration is split into two stages. We first follow the MAE-TTT approach by inputting masked images and training the model using reconstruction loss. In this stage, only the encoder is updated. Then, we input full images (32 in a batch) and optimize the model with the entropy minimization loss following SHOT . In this stage, both the encoder and classifier are optimized. The learning rates for both stages are set the same.

**Comparison with baselines.** In Table 3, we compare our method with the baseline MAE-TTT  and other baselines therein. We can observe that our algorithm largely boosts the performance of the MAE-TTT baseline over most corruption types. This corroborates our theoretical insights and showcases its practical value.

**Understanding entropy-minimization steps.** Table 4 presents the results of entropy-minimization with different training steps. The results indicate that the additional entropy-minimization steps significantly enhance the performance of the MAE-TTT framework, demonstrating the synergy between auto-encoding and entropy-minimization as indicated in our theoretical framework.

    &  &  \\    &  &  &  \\ 
**Acc** & 50.29 & 59.12 \(\) 0.35 & 63.99 \(\) 0.25 & 65.09 \(\) 0.23 & 65.01 \(\) 0.39 \\   

Table 4: **Understanding entropy-minimization steps on ImageNet100-C. Values are classification accuracy (mean and standard deviation) over three random seeds.**

  
**Method** & **CIFAR10-C** & **CIFAR100-C** & **ImageNet-C** \\  Source  & 29.1 & 60.4 & 81.8 \\ BN  & 15.6 & 43.7 & 67.7 \\ TENT  & 14.1 & 39.0 & 57.4 \\ SHOT  & 13.9 & 39.2 & 68.7 \\ TTT++  & 15.8 & 44.4 & 59.3 \\ TTAC  & 13.4 & 41.7 & 58.7 \\  TeSLA-s  & 12.1 & 37.3 & 53.1 \\
**TeSLA-s+SC** & **11.7 \(\) 0.01 \(\)** & **37.0 \(\) 0.06 \(\)** & **50.9 \(\) 0.15 \(\)** \\  TeSLA\({}^{*}\) & \(12.5 0.04\) & \(38.2 0.03\) & \(55.0 0.17\) \\
**TeSLA+SC** & **12.1 \(\) 0.11 \(\)** & **38.0 \(\) 0.13 \(\)** & **54.5 \(\) 0.12 \(\)** \\   

Table 2: **Comparison of SOTA TTA Methods on CIFAR10-C, CIFAR100-C, and ImageNet-C. Average error rates over 15 test corruptions are reported. Baseline results are from Tomar et al. . Values are (means \(\) standard deviations) over three random seeds.** \({}^{*}\) **indicates our reproductions.**

   Acc (\%) & bright & cont & defoc & elast & fog & frost & gauss & glass & impul & jpeg & mnton & pixel & shot & snow & zoom & Avg \\  Joint Train & 62.3 & 4.5 & 26.7 & 39.9 & 25.7 & 30.0 & 5.8 & 16.3 & 5.8 & 45.3 & 30.9 & 45.9 & 7.1 & 25.1 & 31.8 & 26.88 \\ Fine-Tune & 67.5 & 7.8 & 33.9 & 32.4 & 36.4 & 38.2 & 22.0 & 15.7 & 23.9 & 51.2 & 37.4 & 51.9 & 23.7 & 37.6 & 37.1 & 34.45 \\ VIT Probe & 68.3 & 6.4 & 24.2 & 31.6 & 38.6 & 38.4 & 17.4 & 18.4 & 18.2 & 51.2 & 32.2 & 49.7 & 18.2 & 35.9 & 32.2 & 32.06 \\ TTT-MAE & 69.1 & 9.8 & **34.4** & 50.7 & 44.7 & 50.7 & 30.5 & 36.9 & 32.4 & 63.0 & **41.9** & 63.0 & 33.0 & 42.8 & **45.9** & 45.92 \\ 
**Ours** & **73.8** & **14.0** & 33.6 & **69.0** & **47.8** & **64.6** & **38.6** & **42.2** & **36.6** & **68.4** & 32.4 & **67.4** & **41.2** & **51.2** & 35.4 & **47.77** \\   

Table 3: **Test accuracy (%) on ImageNet-C. The baseline results are from Gandelsman et al. **

### Sparsity Regularization

As suggested by the second implication in Section 4.3, we integrate sparsity constraints into the state-of-the-art TTA method, TeSLA/TeSLA-s . Although our theoretical results rely on a generative model, we demonstrate that our implications are also applicable to discriminative models.

**Experimental setup.** We conduct experiments on the CIFAR10-C, CIFAR100-C, and ImageNet-C datasets , following the protocols outlined for TeSLA and TeSLA-s , with and without training data information. In the pre-train stage, we apply the ResNet50  as the backbone network and follow prior work [14; 44] to pre-train it on the clean CIFAR10, CIFAR100, and ImageNet training sets, with joint contrastive and classification losses. In the test-time adaptation process, we adopt the sequential TTA protocol as outlined in TTAC  and TeSLA . This protocol prohibits the change of training objectives throughout the test phase. To encourage sparsity, we add low-rank adaptation (LoRA) modules  to the backbone network, which limits the adaptation to low intrinsic dimensions. Beyond LoRA, we further implement a masking layer with corresponding sparsity constraint (\(_{1}\) loss) to filter out redundant changes. More details can be found in Appendix A5.

**Results analysis.** The average error rates under 15 corruption types for all CIFAR10-C, CIFAR100-C, and ImageNet-C datasets are summarized in Table 2. We can observe that sparsity constraints consistently improve performance over the current SOTA method, TeSLA/TeSLA-s, across all three datasets. The lightweight nature of the sparsity constraint and its consistent performance enhancements make it a valuable addition. This demonstrates the potential of sparsity constraints as a versatile, plug-and-play module for enhancing existing TTA methods.

### Shift Scope and Severity

To investigate the trade-off between the shift scope (dense vs. sparse) and severity, we simulate different levels of corruption severity and corrupted region sizes and evaluate a classical TTA method TENT  on these configurations. Following , we inject impulse noise to the CIFAR10 dataset, with noise levels ranging from 1 to 10 to simulate various severity levels. To control the shift's scope, we crop regions of various sizes and introduce corruption only to this region. Figure 3 displays classification error curves under various shift severity levels and region sizes. We can observe that classification errors rise with increasing noise levels and region sizes. Notably, for large block sizes (dense shifts), the performance dramatically declines and even collapses as the severity level rises, whereas the performance remains almost constant over all severity levels in the sparse shift regime, verifying the theoretical conditions for Theorem 4.2 and Theorem 4.4.

## 7 Conclusion and Limitations

In this work, we characterize extrapolation with a latent-variable model that encodes a minimal change principle. Within this framework, we establish clear conditions under which extrapolation becomes not only feasible but also guaranteed, even for complex nonlinear models in deep learning. Our conditions reveal the intricate interplay among the generating function's smoothness, the out-of-support degree, and the influence of the shift. These theoretical results provide valuable implications for the design of practical test time adaptation methods, which we validate empirically.

**Limitations**: On the theory aspect, the Jacobian norm utilized in Theorem 4.2 only considers the global smoothness of the generating function and thus may be too stringent if the function is much more well-behaved/smooth over the extrapolation region of concern. Therefore, one may consider a refined local condition to relax this condition. On the empirical side, our theoretical framework entails learning an explicit representation space. Existing methods without such a structure may still benefit from our framework but to a lesser extent. Also, our framework involves several loss terms including reconstruction, classification, and the likelihood of the target invariant variable. A careful re-weighting of these terms may be needed during training.

Figure 3: **TTA classification errors under different levels of shift severity levels and scopes.**

**Acknowledgments.** We thank the anonymous reviewers for their valuable insights and recommendations, which have greatly improved our work. The work of L. Kong is supported in part by NSF DMS-2134080 through an award to Y. Chi. This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. P. Stojanov was supported in part by the National Cancer Institute (NCI) grant number: K99CA277583-01, and funding from the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard. This research has been graciously funded by the National Science Foundation (NSF) CNS2414087, NSF BCS2040381, NSF IIS2123952, NSF IIS1955532, NSF IIS2123952; NSF IIS2311990; the National Institutes of Health (NIH) R01GM140467; the National Geospatial Intelligence Agency (NGA) HM04762010002; the Semiconductor Research Corporation (SRC) AIHW award 2024AH3210; the National Institute of General Medical Sciences (NIGMS) R01GM140467; and the Defense Advanced Research Projects Agency (DARPA) ECOLE HR00112390063. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, the National Institutes of Health, the National Geospatial Intelligence Agency, the Semiconductor Research Corporation, the National Institute of General Medical Sciences, and the Defense Advanced Research Projects Agency.