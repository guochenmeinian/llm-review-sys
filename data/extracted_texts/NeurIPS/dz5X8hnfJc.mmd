# Characterizing Out-of-Distribution Error

via Optimal Transport

 Yuzhe Lu\({}^{*}\)\({}^{1}\), Yilong Qin\({}^{*}\)\({}^{1}\), Runtian Zhai\({}^{1}\), Andrew Shen\({}^{1}\), Ketong Chen\({}^{1}\)

Zhenlin Wang\({}^{1}\), Soheil Kolouri\({}^{2}\), Simon Stepputtis\({}^{1}\), Joseph Campbell\({}^{1}\), Katia Sycara\({}^{1}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)Vanderbilt University

{yuzhelu, yilongq, rzhai, andrews, ketongc, zhenlinw}@cs.cmu.edu

soheil.kolouri@vanderbilt.edu, {sstepput, jcampbell, sycara}@cs.cmu.edu

These authors contributed equally to this work.

###### Abstract

Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify _pseudo-label shift_, or the difference between the predicted and true OOD label distributions, as a key indicator of this under-estimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual transport costs and further improves the accuracy of COT's error estimates. We evaluate COT and COTT on a variety of standard benchmarks that induce various types of distribution shift - synthetic, novel subpopulation, and natural - and show that our approaches significantly outperform existing state-of-the-art methods with up to 3x lower prediction errors. Our code can be found at https://github.com/luyuzhe111/COT.

## 1 Introduction

Machine Learning methods are largely based on the assumption that test samples are drawn from the same distribution as training samples, providing a basis for generalization. However, this i.i.d. assumption is often violated in real-world applications where test samples are found to be out-of-distribution (OOD) - sampled from a different distribution than during training. This may result in a significant negative impact on model performance . A common practice for alleviating this issue is to regularly gauge the model's performance on a set of labeled data from the current _target_ data distribution, and update the model if necessary. When labeled data is not available, however, one needs to predict the model's performance on the target distribution with unlabeled data, a task known as _OOD performance prediction_.

Performance prediction on unlabeled data has previously been shown to be impossible without imposing additional constraints over the unknown target distribution , due to the fact that target samples may take any label. Thus, the feasibility of this task is dependent on what assumptions we make regarding the shift between the train and target distributions. Prior works often make the assumption that the conditional density \(P(y|x)\) remains fixed in the presence of covariate shift .

However, this tells us little when \(x\) falls outside the support of the train distribution. Despite the theoretical difficulty, prior works have proposed a number of heuristic methods to estimate the performance of a model based on unlabeled target samples. For instance, Average Confidence (AC) [20; 16] estimates error based on the average maximum softmax score for target samples, assuming the model has been calibrated for the train distribution. This method was further improved with the addition of a learned threshold , for which the error is predicted as the fraction of samples with confidence falling below it. Other approaches estimate model performance based on a disagreement score computed between the predictions of two models trained over the same dataset [22; 3]. Some works have found that applying a transformation over target samples and estimating the effect of the transformation leads to a reliable prediction of model performance in vision tasks [9; 10]. However, many of these prior methods have been shown to underestimate the model's error when it is _miscalibrated_ in the target distribution [13; 22]; that is, the predicted softmax scores differ from the true class likelihoods. We empirically observe that this miscalibration is strongly positively correlated with _pseudo-label shift_, which is the difference between the predicted target label distribution \(P_{T}()\) and the true label distribution \(P_{T}(y)\). Thus, we treat pseudo-label shift as a key indicator of error underestimation.

In this work, we propose an approach which provides robust error estimates in the presence of pseudo-label shift. Our approach, Confidence Optimal Transport (COT), leverages the optimal transport framework to predict the error of a model as the Wasserstein distance between the predicted target class probabilities and the true source label distribution. We theoretically derive lower bounds for COT's predicted error. This results in a more provably conservative error prediction than AC, which is crucial for safety in many real-world machine learning applications. In addition, we introduce a variant of COT, Confidence Optimal Transport with Thresholding (COTT), which introduces a learned threshold over the optimal transportation costs in line with prior work  and empirically improves upon COT's performance. In this work, we mainly target covariate shifts and compare our proposed methods to existing state-of-the-art approaches in extensive empirical experiments over eleven datasets exhibiting distribution shift from both vision and language domains. These distribution shifts include: visual corruptions (e.g., blurred image data), novel subpopulation shifts (e.g., novel appearances of a category), and natural shifts in the wild (e.g., different stain colors for pathology images), all of which are frequently experienced in the real world. We find that COT and COTT consistently avoid the significant error underestimations suffered by previous methods. In particular, COTT achieves significantly lower prediction errors than existing methods for most models and datasets (up to 3x better), establishing new state-of-the-art results.

## 2 Preliminaries and Motivation

In this section, we introduce the problem setup of OOD error prediction and show how a popular baseline, Average Confidence (AC), can be understood as the Wasserstein Distance (WD) between a reference label distribution and the softmax output distribution from an Optimal Transport (OT) perspective. We then demonstrate why the reference label distribution AC uses is problematic by explaining pseudo-label shift and its correlation with miscalibration. After establishing the relation and utility of OT to OOD error prediction, we formally introduce our proposed methods in Sec. 3.

### OOD Performance Prediction

In this work, we address the OOD problem in the domain of classification tasks. Let \(^{d}\) be the input space of size \(d\), and \(=\{1,,k\}\) be the label space, where \(k\) is the number of classes. Let the source distribution over \(\) be \(P_{S}(x,y)\) and the target distribution be \(P_{T}(x,y)\). A classifier \(:^{k-1}\) maps an input to a _confidence vector_ (i.e. the output of the softmax layer), where \(^{k-1}=\{(z_{1},,z_{k}):z_{1}++z_{k}=1,z_{i} 0\}\) is the \(k\)-dimensional unit simplex. Let the training samples be \(\{(x_{}^{(i)},y_{}^{(i)})\} P_{S}(x,y)\), and the validation samples be \(\{(x_{}^{(i)},y_{}^{(i)})\} P_{S}(x,y)\). The validation set is used in some previous methods and will also be used in COT and COTT to perform calibration .

The OOD performance prediction problem is formally stated as follows: Given an unlabeled test set \(\{x_{T}^{(i)}\} P_{T}(x)\), and a classifier \(\) trained over the training samples, predict the accuracy of \(\) over the test set \(=_{i=1}^{n}[y_{T}^{(i)}=_{j}_{j}(x_ {T}^{(i)})]\), where \(y_{T}^{(i)}\) is the ground truth label. Equivalently, we can also predict the error \(=1-\) with an estimate \(\).

In this work, we are further investigating _the distribution of confidence vectors_. While a confidence vector \((x)\) itself is a distribution of labels in the \(k\)-dimensional simplex \(^{k-1}\), the distribution of confidence vectors \(_{\#}P()\) is a distribution of distributions, where \(^{k-1}\). \(_{\#}P()\) is defined to be the _pushforward_ of a covariate distribution \(P(x)\) using \(\): \(_{\#}P()=P(^{-1}())\). Consider the following example: Suppose we have a uniform covariate distribution \(P(x)\) on \(x\{A,B,C\}\) and \(\) maps \(A,B\) to \([0.5,0.5]^{}\) and \(C\) to \([0.7,0.3]^{}\). Then, \(f_{\#}P()\) will assign \(\) mass to \([0.5,0.5]^{}\) and \(\) mass to \([0.7,0.3]^{}\). To facilitate easy comparison between confidence vectors and labels, we denote \(\{0,1\}^{k}^{k-1}\) to be the one-hot representation of \(y\), where the only non-zero element in \(\) is the \(y\)-th element, i.e. \(_{j}=[y=j]\). We denote _the distribution of one-hot labels_ as \(P()\). For a covariate distribution \(P(x)\), we will refer to the distribution of predicted labels from classifier \(\) as the _pseudo-label distribution_\(P_{}()\). We reuse the notation to denote the probability mass of \(\) also as which is given by the mass of the inputs that give this prediction, i.e. \(P_{}()=P(\{x|_{j}_{j}(x)=y\})\).

### Wasserstein Distance and Optimal Transport

In recent years, optimal transport theory has found numerous applications in the field of machine learning [2; 4; 1; 27; 29]. Optimal transport aims to move one distribution of mass to another as efficiently as possible under a given cost function. In the Kantorovich formulation of optimal transport, we are given two distributions \((x)\) over \(\) and \((y)\) over \(\) and a cost function \(c(x,y)\) that tells us the cost of transporting from location \(x\) to location \(y\). Here, we aim to find a transport plan \((x,y)\) that minimizes the total transport cost. The transport plan is a joint distribution with marginal \((,)=()\) and \((,)=()\). The conditional distribution \(\) of the transport plan informs us how much mass is moved from \(x\) to \(y\). Let \((,)\) be the set of all transport plans. More formally, the Wasserstein Distance is defined as:

\[W(,)=_{(,)}_{}c(x,y)d (x,y)\]

The Wasserstein distance satisfies the definition of a metric (non-negativity, symmetry, and sub-additivity), inducing a metric space over a space of probability distributions. Unlike other metrics, such as total variation, the Wasserstein metric induces a weaker topology and provides a robust framework for comparing probability distributions that respect the underlying space geometry .

For discrete distributions \(,\) such as the empirical distributions, the Wasserstein distance simplifies to the following linear programming problem:

\[W(,)=_{(,)}, =_{(,)}_{i,j}_{ij}_{ij}\]

where \(,^{m n}\) are the cost matrix and the plan matrix respectively and \(_{ij}\) is the transport cost from sample \(i\) to sample \(j\). When \(n=m\), the optimal transport problem reduces to the optimal matching problem (Proposition 2.1 in Peyre et al. ), i.e. the optimal transport plan \(^{*}=_{(,)},\) is a permutation matrix. Not only does this constraint enable efficient algorithms like the Hungarian algorithm, but it will also help draw the connection between pseudo-label shift and target error that is central to our Confidence Optimal Transport method.

### Average Confidence as Wasserstein Distance

We use Average Confidence, a popular OOD performance prediction method, as the starting point of our analysis. Average Confidence with Max Confidence (AC-MC) estimates the target accuracy by taking the empirical mean of maximum confidence of the classifier over all target samples \(x^{(i)} P_{T}(x)\), i.e. \(_{i=1}^{n}_{j}_{j}(x^{(i)})\). Its corresponding target error estimate is therefore \(_{}=1-_{i=1}^{n}_{j}_{j}(x^{ (i)})\). By definition, \(|-_{}|\) measures the miscalibration of the model, i.e. how far away the model's confidence is from its actual accuracy. In the following proposition, we connect the AC-MC estimates with distances in the Wasserstein Metric Space:

**Proposition 1** (\(_{}\)-\(W_{}\) Equivalence).: _Let \(((^{k}),W_{})\) be the metric space of all distributions over \(^{k}\), where \(W_{}\) is the Wasserstein distance with \(c(x,y)=\|x-y\|_{}\). Then, the estimated error of AC-MC is given by \(_{}=W_{}(_{\#}P(),P_{}( ))\)._

We defer all proofs to the supplementary material. Here, we appeal to pictorial intuition in Figure 1, where \(_{0}\) (Dirac delta over the zero vector) represents the origin. All one-hot label distributions, including \(P_{T}()\) and \(P_{}()\), are represented as points on the (dashed) unit sphere around \(_{0}\). A distribution of confidence vectors \(_{\#}P()\) is simply a point within the unit ball around \(_{0}\). The AC-MC accuracy estimate measures how far the point is to the origin \(_{0}\). Additionally, we can project the point to the unit sphere, resulting in the projection point \(P_{}()\), which is the pseudo-label distribution. The AC-MC error estimate measures _the length of the projection_.

**Corollary 1** (\(P_{}()\) is closest to \(_{\#}P()\)).: _Let \(P^{}()(\{0,1\}^{k}^{k-1})\) be a one-hot label distribution. Then \(W_{}(_{\#}P(),P^{}()) W_{}( _{\#}P(),P_{}())\)._

This suggests that AC-MC, among all possible reference one-hot label distributions, selects the closest one and reports the distance to be the predicted error. Thus, we can see that AC-MC is a very optimistic prediction strategy, so it is not surprising that AC-MC is widely reported to be over-estimating the performance on real tasks [13; 16; 22; 3].

### Pseudo-Label Shift and its Correlation with Miscalibration

The pseudo-label shift is defined as \(W_{}(P_{}(),P_{T}())\), i.e. the distance from the pseudo label distribution \(P_{}()\) to the ground truth target label distribution \(P_{T}()\), which is also the length of the green line segment in Figure 1. An important property is \(W_{}(P_{}(),P_{T}()) 1\), i.e. the pseudo-label shift is a lower bound of the true target error of the model (as true target error can be interpreted as the average transport cost of a suboptimal matching). We can clearly see how a large pseudo-label shift could potentially destroy the prediction of AC-MC from Figure 1 (right), where the red line segment is much shorter than the green one which should be a lower bound of the true error. This lower bound, however, can be loose when it is small, as shown in Figure 1 (middle).

Most existing prediction methods heavily rely on the model being well-calibrated, as pointed out by Jiang et al. , Garg et al. , yet the underlying difficulty is a lack of precise understanding of _when_ and _by how much_ neural networks become miscalibrated. This is evident in the large-scale empirical study conducted in , showing that the variance of the error becomes much larger among the different types of shift studied, despite some positive correlation between the expected calibration error and the strength of the distribution shift. In this section, we empirically show that there is a strong positive correlation between the pseudo-label shift and \(|-_{}|\), which we define as the model's miscalibration. Because of this correlation, we consider pseudo-label shift as the key signal to why existing methods have undesirable performance.

We evaluate the pseudo label shift and the prediction error of AC-MC \(|-_{}|\) on CIFAR-10 and CIFAR-100 under different distribution shifts, and plot the results in Figure 2 (left). The plots demonstrate a strong positive correlation between these two quantities, which not only means that the performance of AC-MC worsens as the pseudo-label shift gets larger, but also implies that the performance of existing methods [16; 13; 22] that depend on model calibration will drop.

Figure 1: The AC-COT Triangle in the Wasserstein Space \(((^{k}),W_{})\), where \(_{0}\) (Dirac delta over the zero vector) represents the origin. Red line: AC-MC error estimate. Blue line: (our) COT error estimate (assuming \(P_{T}() P_{S}()\)). Green line: Pseudo-label shift. **Left:** AC-MC predicts the target error as the distance between the _distribution of confidence vectors_ and its projection on the unit sphere, the smallest among all label distributions. This makes AC-MC prone to _underestimating_ the target error. **Middle:** Mild pseudo-label shift. **Right:** Severe pseudo-label shift.

Our exploration of miscalibration and pseudo-label shift reveals a previously unexplored tradeoff: When the pseudo-label shift is small, prior work has given us some reassurance to trust the calibration of neural networks, which motivated methods such as AC  and ATC . More critically, as miscalibration worsens, the pseudo-label shift becomes a tighter low bound of the error and thus a more trustworthy estimate of the error.

Motivated by this observation, we leverage the information of the pseudo-label shift to improve the performance of confidence-based prediction methods for miscalibrated models. The problem, as mentioned earlier, is that without any information about the target label distribution, it is theoretically impossible to estimate the pseudo-label shift when the test set contains data from unseen domains. Thus, the only option is to make assumptions on the target label distribution.

In our proposed method, we make a natural assumption: the target label distribution is close to the source label distribution. This assumption aligns with most natural shifts that can be observed in real-world datasets. This extra assumption allows us to develop COT and COTT, which perform much better than existing methods in most cases (See Table 1), especially when the pseudo-label shift is large. Given this assumption, Figure 3 (Left), shows the prediction error of COT versus the pseudo-label shift. We can see that equipped with the extra information, COT is able to maintain a low prediction error even under very large pseudo-label shift, and the correlation between COT performance and the pseudo-label shift is weak. Since the pseudo-label shift is strongly correlated with miscalibration, this implies that COT is much more robust to miscalibration than existing miscalibration-sensitive prediction methods.

## 3 Methods

In this section, we formally introduce our proposed method - Confidence Optimal Transport - and propose an additional variation, COTT, that utilizes a threshold over the optimal transportation costs between two distributions, instead of taking a simple average.

Figure 3: We further compare the sensitivity of COT and COTT’s estimation error to the degree of pseudo-label shift. Compared to Figure 2, we can clearly see that the correlation between the prediction error and pseudo-label shift weakens significantly. Moreover, COTT is even more robust to pseudo-label shift compared to COT.

Figure 2: **Left: Absolute difference between the model’s Average Confidence and its true error under different levels of pseudo-label shift. This difference measures the degree of miscalibration. We see a strong correlation between the miscalibration and pseudo-label shift on the common corruption benchmarks (CIFAR-10-C, CIFAR-100-C). Right: Absolute difference between GDE error estimate and true error under different levels of pseudo-label shift. The strong correlation is also observed.**

### Confidence Optimal Transport

Let \(_{S}()\) denote the empirical source label distribution. The predicted error of COT, which is the length of the blue line segment (assuming \(P_{T}()=P_{S}()\)) in Figure 1, is given by

\[_{}=W_{}(_{\#}_{T}(), _{S}()).\]

By Corollary 1, \(_{}_{}\), which means that COT provably predicts a larger error than AC-MC, which empirically tends to produce more accurate predictions as we find overestimation of the error far less common than underestimation. As mentioned earlier, in the case where the pseudo-label shift is large, such as in Figure 1 (Right), AC-MC can have arbitrarily large prediction error, while COT always has the following guarantee:

**Proposition 2** (Calibration independent lower bound of COT).: _Under the assumption that \(P_{T}()=P_{S}()\), we always have \(_{} 0.5W_{}(P_{}(),P_{T}( ))\)._

Thus, we can see that COT is by nature different from existing methods because it is a _miscalibration-robust method_. Its success is dependent on the difference between \(P_{T}()\) and \(P_{S}()\), which is relatively small in most real-world scenarios, while the dependency on calibration of existing methods can not always be controlled. The geometric explanation of this is that COT measures a fundamentally different length in the Wasserstein Space \(((^{k}),W_{})\), allowing for the above guarantee which does not exist in previous methods. Moreover, as we will empirically demonstrate in the next section, large pseudo-label shift is prevalent in real models and datasets. Consequently, COT performs much better than miscalibration-sensitive methods in most cases.

### Thresholding as a Robust Transport Cost Statistic

Computationally, COT (or Wasserstein Distance in general) is implemented as a two-step process: 1) calculating individual transport costs for all samples; 2) returning the mean across all transport costs as the error estimate. While we have seen that COT has some protection against miscalibration, it is not completely immune. Another outstanding issue lies in the second step - computing the mean of all transport costs. In statistics, it is well known that the mean is less robust to outliers than the median . In the supplemental material, we show that the empirical transport cost distribution is also heavy-tailed and therefore COT is susceptible to outlier costs. A large fluctuation in the outliers impacts the mean much more than it does to the median. Therefore, a more robust statistic is desired to protect COT against these outliers.

To search for such a robust statistic that is suitable for our purpose, we turn to Average Thresholded Confidence (ATC)  for inspiration. ATC tremendously improves the performance of AC precisely by offering such protection against the overconfident outlier predictions (shown in supp material) by returning the percentile above the threshold rather than the mean. In a similar vein, we safeguard COT against outlier costs by introducing COT with Thresholding, COT.

Specifically, to compute the threshold \(t\), we first sample a validation set \(\{(x_{}^{(i)},y_{}^{(i)})\}_{i=1}^{n} P_{S}(x,y)\). Let \(_{}\) denote the empirical validation distribution. We compute the optimal transport plan \(_{}^{*}=_{(_{\#}_{ }(),_{S}())},\), where \(\) is the cost matrix determined by the L-infinity distance. Note that \(_{}^{*}\{0,1\}^{n n}\) is a permutation matrix due to the equal number of confidence vectors and one-hot labels (Proposition 2.1 in ). Then the threshold \(t\) is set such that the validation of error the classifier equals the fraction of samples with transport cost higher than \(t\), i.e.

\[_{}=|\{_{ij} t|_{ij}^{*}=1\}|\]

With the threshold \(t\) learned from the validation set, we can compute the COT target error estimate. Let \(_{S}()\) denote the empirical source label distribution and \(_{\#}_{T}()\) denote the empirical distribution of confidence vectors of samples from the target distribution \(P_{T}(x)\). We compute the optimal transport plan \(^{*}=_{(_{\#}_{T}(), _{S}())},\). Our COT estimate is then defined as

\[_{}=|\{_{ij} t|^{* }_{ij}=1\}|\]Note on Implementation:While solving the linear program of optimal transport for COT and COT is rather straightforward with existing solvers such as POT , its time complexity \((n^{3})\) prohibits large-scale inputs. We bypass this problem by breaking the test dataset into small batches with a maximum size of 10,000. Concretely, given \(n\) test samples, we sample with replacement for \( n/10,000\) batches and compute an error estimate for each batch. We get the final error estimation of COT and COTT by taking an average of batch estimates.

## 4 Experiments

In this section, we empirically compare COT and COTT with existing methods on various benchmark datasets. For all experiments, we trained the model on in-distribution data and froze the model after convergence. To predict the model's performance on the target domain, we only used unlabeled data from the target domain. When we have a test set size greater than 10,000, we show the results of utilizing the batched version of COT and COTT detailed in Section 3.2. For context, solving the OT problem of size 10,000 only takes around 10s, thus adding only negligible computation overhead.

### Datasets and Nature of Shift

In our comprehensive evaluation, we consider more than 10 benchmark datasets across multiple modalities, including vision and language, with a variety of distribution shifts. Each type of shift is introduced in the following paragraphs:

**Synthetic Shift:** First, we consider distribution shifts caused by common visual corruptions, such as brightness, defocusing, and blurriness, which are common in real-world settings. We used the corrupted versions of CIFAR10, CIFAR100, and ImageNet proposed in , which includes 19 types of common visual corruptions across 5 levels of severity.

**Novel Subpopulation Shift:** Next, we consider novel subpopulation shifts, where the subpopulations in the train and test sets differ. For example, models might have only observed golden retrievers for the dog class but not huskies. In our experiments, we used the BREEDS benchmark , which leveraged the ImageNet class hierarchy to create 4 datasets, Living-17, Nonliving-26, Entity13, and Entity-30. With this benchmark, we aim to understand if error estimation methods could capture challenges in an increasingly diverse world.

**Natural Shift:** Finally, we consider non-simulated shifts in which the distribution shifts are induced through differences in the data collection process, such as ImageNet-V2 and CIFAR10-V2 proposed in . We also include ImageNet-Sketch , which consists of sketched images of the original ImageNet classes. Additionally, we consider distribution shifts faced in the wild, such as ones curated in the WILDS benchmark . We consider four WILDS datasets, two for language tasks (Amazon-WILDS, CivilComments-WILDS) and two for vision tasks (Camelyon17, RxRx1). These datasets reflect various discrepancies between data sources happening in the real world. For example, hospitals might use different stain colors for pathological images. Different groups might follow distinct standards assigning star ratings for their reviews.

### Architectures and Evaluations

For vision tasks, We trained ResNet18 (CIFAR10, CIFAR100) and ResNet50  (ImageNet, Living17, Nonliving26, Entity13, Entity30, Camelyon17-WILDS, RxRx1-WILDS); for language tasks (Amazon-WILDS, CivilComments-WILDS), we fine-tuned DistilBERT-base-uncased . We followed training setups from previous works  and provided the full details in the supplemental material. After training, we calibrated models using Temperature Scaling (TS)  on the in-distribution validation data, effectively adjusting the output probabilities of the neural network to match the actual correctness likelihood. This approach has previously demonstrated that TS consistently improves error estimation performance for all methods . To evaluate different methods, we utilized the mean absolute difference between their predicted errors and the true errors, which are obtained using ground truth labels. We refer to this metric as Mean Absolute Error (MAE).

### Baselines

We consider an array of baselines to compare against our methods: COT and COTT.

_Average Confidence (AC)_ estimates target error by taking the average of one minus the maximum softmax confidence of target data. \(_{}=_{x_{}}[1-_{j}_{j}(x)]\).

_Difference of Confidence (DoC a.k.a. DOC-Feat)_ estimates target error through the difference between the confidence of source data and the confidence of target data. \(_{}=_{x_{S}}[[ _{j}_{j}(x) y]]+_{x_{T}}[1- _{j}_{j}(x)]-_{x_{S}}[1-_{j }_{j}(x)]\).

_Importance Re-weighting (IM)_ estimates target error as a re-weighted source error. The weights are calculated as the ratio between the number of data points in each bin in target data and source data. This is equivalent to  using one slice based on the underlying classifier confidence.

_Generalized Disagreement Equality (GDE)_ estimates target error as the disagreement ratio of predictions on the target data using two independently trained models \((x)\) and \(^{}(x)\). \(_{}=_{x_{}}[[_{j}_{j}(x)_{j}_{j}^{}(x)]]\).

_Average Thresholded Confidence (ATC)_ first identifies a threshold \(t\) such that the fraction of source data points that have scores below the threshold matches the source error on in-distribution validation data. Target error is estimated as the expected number of target data points that fall below the identified threshold. \(_{}(s)=_{x_{T}}[[s( {f}(x))<t]]\), where \(s\) is the score function mapping the softmax vector to a scalar. Two different score functions are used, Maximum Confidence (ATC-MC) and Negative Entropy (ATC-NE).

In addition, we also compare our method to _ProjNorm_. ProjNorm cannot provide a direct estimate, instead, the authors demonstrated their metric has the strongest linear correlation to true target error compared to existing baselines. In this case, we followed their setup and performed a correlation analysis to draw a direct comparison. We included results in the supplemental material and showed that our method consistently outperforms ProjNorm.

    &  &  &  &  \\  & & & **DoC** & **IM** & **GDE** & **ATC-MC** & **ATC-NE** & **COT** & **COTT** \\   & Natural & 5.97 & 5.38 & 5.87 & 5.9 & 3.38 & **3.15** & 5.41 & 3.33 \\  & Synthetic & 9.1 & 8.53 & 9.26 & 8.84 & 4.2 & 3.37 & 2.17 & **1.7** \\   & Synthetic & 10.83 & 8.76 & 12.07 & 11.36 & 6.8 & 6.63 & **2.09** & 2.59 \\   & Natural & 8.5 & 7.43 & 8.62 & 5.62 & 3.57 & 2.6 & 3.88 & **2.41** \\  & Synthetic & 10.34 & 9.28 & 12.87 & 6.54 & 1.59 & 3.41 & 3.24 & **1.42** \\   & Same & 19.63 & 19.2 & 17.5 & 15.37 & 8.09 & 7.23 & 8.47 & **2.61** \\  & Novel & 29.61 & 29.18 & 27.22 & 24.48 & 14.54 & 9.49 & 15.9 & **5.46** \\   & Same & 16.97 & 16.21 & 13.56 & 13.98 & 8.19 & 9.08 & 5.9 & **2.46** \\  & Novel & 27.57 & 26.81 & 23.96 & 23.4 & 13.46 & 8.57 & 15.11 & **5.94** \\   & Same & 14.84 & 14.67 & 11.22 & 9.94 & 4.88 & 5.43 & 6.25 & **2.94** \\  & Novel & 29.61 & 29.18 & 27.22 & 24.48 & 14.54 & 9.49 & 15.9 & **5.53** \\   & Same & 19.25 & 18.43 & 16.6 & 12.77 & 11.18 & 9.69 & 7.06 & **3.34** \\  & Novel & 31.37 & 30.54 & 28.79 & 23.37 & 19.93 & 16.56 & 17.8 & **10.46** \\   & Natural & 9.44 & 9.44 & 10.24 & **5.19** & 7.73 & 7.73 & 7.27 & 5.71 \\   & Natural & 5.21 & 8.44 & 8.09 & 7.48 & 6.53 & 6.86 & **3.25** & 5.82 \\  Amazon-WILDS & Natural & 2.62 & 2.35 & 2.34 & 17.04 & 1.63 & **1.54** & 2.43 & 2.01 \\  CivilCom.-WILDS & Natural & 1.54 & 0.96 & **0.86** & 8.7 & 2.3 & 2.3 & 1.23 & 4.68 \\   

Table 1: Mean Absolute Error (MAE) between the estimated error and ground truth error to compare different methods. The ”shift” column denotes the nature of distribution shifts for each dataset. For vision datasets, we reported results for ResNet18 and ResNet50; for language datasets, we reported results for DistilBERT-base-uncased. The results are averaged over 3 random seeds. We highlight the best-performing method. We defer the full table with std to the supplemental material.

### Results

In Table 1, we report the MAE results grouped by datasets and nature of shifts. Across all benchmarks, we observe that COT always obtains lower estimation error than AC, supporting our theoretical analysis that COT additionally leverages pseudo-label shift to fight miscalibration. On synthetic shift benchmarks (CIFAR10-Synthetic, CIFAR100-Synthetic, ImageNet-Synthetic BREEDS-same), COT is \(2-4\) better than AC, drastically reducing the estimation error. On novel subpopulation shift (BREEDS-novel), COT cuts about half of the AC error. On natural shift, COT also improves upon AC. On ImageNet natural shift datasets, COT reduces the error from 8.5 to 3.88 compared to AC. COTT, presents the best overall results, surpassing the best current method (ATC) by a notable margin. On synthetic shift benchmarks, COTT is 2-3\(\) better than ATC-NE, the stronger version of ATC that uses a negative entropy score function. On novel subpopulation shift benchmarks, COTT is at least 4 absolute percent better than ATC-NE. On natural shift benchmarks, however, we observed mixed results. On ImageNet-Natural, COTT is better than ATC-NE while on CIFAR10-Natural, COTT is marginally worse. On WILDS benchmarks, no single method dominantly outperforms others. Note that the WILDS benchmark datasets contain label shift, meaning \(P_{S}(y) P_{T}(y)\). This leads COTT to overestimate the error on the CivilComments-WILDS. Nonetheless, COTT has the smallest worst-case error of 5.82 compared to ATC-NE's 7.73, demonstrating its robustness even on distribution shifts faced in the wild.

In Figure 4, we use scatterplots to visualize estimations given by different methods, notably AC, ATC, COT, and COTT, where we plot the predicted error against the true error. Ideally, these scattered points should demonstrate a strong linear correlation and closely follow the \(y=x\) line. While this is true for COT and COTT, AC and ATC often underestimate, sometimes by a large margin. In Nonliving-26, we can see data points representing shifts whose ATC predicted errors are around 0.1 while true errors are close to 1. These observations corroborate the necessity of leveraging pseudo-label shifts to guard against such catastrophic failures of existing confidence-based methods. We include the scatterplots for all datasets in the supplemental material, where we show that COT and COTT successfully prevent severe underestimation seen in other methods.

## 5 Conclusion

In this work, we proposed COT and COTT, simple yet effective methods leveraging the optimal transport framework to estimate a classifier's performance on out-of-distribution data with only unlabeled samples. Our method is motivated by the observation that existing methods failed to take into account the fact that classifiers often demonstrate pseudo-label shifts when performance drops on an unseen target domain. We formalized this observation by connecting AC and COT under the Wasserstein metric and showing that COT provably provides more conservative estimates by assuming a target marginal distribution. Combining COT with a thresholding strategy that has shown extensive empirical success, we further introduced COTT. On all eleven benchmark tasks, COT and COTT manage to avoid severe underestimation of the model's error seen in current methods. In particular, COTT achieves significantly lower prediction errors than existing methods for most models and datasets (up to 3x better), establishing new state-of-the-art results. We believe our work makes

Figure 4: Qualitative results for AC, ATC, COT, and COTT, comparing error estimates vs. ground truth target error. Accurate estimates should be close to \(y=x\) (dashed black line). Notably, COT and COTT remain accurate even when the shifts are large. By contrast, AC and ATC often severely underestimate the error, which is particularly evident in the Nonliving-26 dataset.

an important step forward in making performance estimation methods more reliable and theoretically grounded and thus more applicable in real-world applications.

Limitations: Despite the strong empirical performance, our method does suffer from several limitations. First, our method is currently only applicable to multi-class classification problems under the assumption that the marginal label distribution must remain constant between the train and test distributions, though as we will show in the supplemental material, our method is still quite robust when the difference between source and target marginal is small. In addition, while we have derived a lower bound for error estimates, we have not currently formulated an upper bound; however, we have not observed overestimating the error to be a common problem empirically. Lastly, the relationship between miscalibration and pseudo-label shift is currently only empirical in nature, not theoretical.