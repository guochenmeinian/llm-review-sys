# Elastic Decision Transformer

Yueh-Hua Wu\({}^{12}\) Xiaolong Wang\({}^{1}\) Masashi Hamaya\({}^{2}\)

\({}^{1}\)UC San Diego \({}^{2}\)OMRON SINIC X

yuw088@ucsd.edu

equal advising

###### Abstract

This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at: https://kristery.github.io/edt/.

## 1 Introduction

Reinforcement Learning (RL) trains agents to interact with an environment and learn from rewards. It has demonstrated impressive results across diverse applications such as game playing , robotics , and recommendation systems . A notable area of RL is Offline RL , which employs pre-collected data for agent training and proves more efficient when real-time interactions are costly or limited. Recently, the conditional policy approach has shown large potentials in Offline RL, where the agent learns a policy based on the observed state and a goal. This approach enhances performance and circumvents stability issues related to long-term credit assignment. Moreover, the successful Transformer architecture , widely used in applications like natural language processing  and computer vision , has been adapted for RL as the Decision Transformer (DT) .

DT utilizes a Transformer architecture to model and reproduce sequences from demonstrations, integrating a goal-conditioned policy to convert Offline RL into a supervised learning task. Despite its competitive performance in Offline RL tasks, the DT falls short in achieving trajectory stitching, a desirable property in Offline RL that refers to creating an optimal trajectory by combining parts of sub-optimal trajectories . This limitation stems from the DT's inability to generate

Figure 1: **Normalized return with medium-replay datasets. The dotted gray lines indicate normalized return with medium datasets. By achieving trajectory stitching, our method benefits from worse trajectories and learns a better policy.**superior sequences, thus curbing its potential to learn optimal policies from sub-optimal trajectories (Figure 1).

We introduce the Elastic Decision Transformer (EDT), which takes a variable length of the traversed trajectory as input. Stitching trajectories, or integrating the current path with a more advantageous future path, poses a challenge for sequence generation-based approaches in offline RL. Stitching a better trajectory appears to contradict one of the core objectives of sequence generation that a sequence generation model is required to reliably reproduce trajectories found within the training dataset. We suggest that in order to'refresh' the prediction model, it should disregard 'negative' or 'unsuccessful' past experiences. This involves dismissing past failures and instead considering a shorter history for input. This allows the sequence generation model to select an action that yields a more favorable outcome. This strategy might initially seem contradictory to the general principle that decisions should be based on as much information as possible. However, our proposed approach aligns with this concept. With a shorter history, the prediction model tends to output with a higher variance, typically considered a weakness in prediction scenarios. Yet, this increased variance offers the sequence prediction model an opportunity to explore and identify improved trajectories. Conversely, when the current trajectory is already optimal, the model should consider the longest possible history for input to enhance stability and consistency. Consequently, a relationship emerges between the quality of the path taken and the length of history used for prediction. This correlation serves as the motivation behind our proposal to employ a variable length of historical data as input.

In practice, we train an approximate value maximizer using expectile regression to estimate the highest achievable value given a certain history length. We then search for the history length associated with the highest value and use it for action inference.

Evidence from our studies indicates that EDT's variable-length input sequence facilitates more effective decision-making and, in turn, superior sequence generation compared to DT and its variants. Furthermore, it is computationally efficient, adding minimal overhead during training. Notably, EDT surpasses state-of-the-art methods, as demonstrated in the D4RL benchmark  and Atari games [7; 17]. Our analysis also suggests that EDT can significantly enhance the performance of DT, establishing it as a promising avenue for future exploration.

**Our Contributions:**

* We introduce the Elastic Decision Transformer, a novel approach to Offline Reinforcement Learning that effectively addresses the challenge of trajectory stitching, a known limitation in Decision Transformer.
* By estimating the optimal history length based on changes in the maximal value function, the EDT enhances decision-making and sequence generation over traditional DT and other Offline RL algorithms.
* Our experimental evaluation highlights EDT's superior performance in a multi-task learning regime, positioning it as a promising approach for future Offline Reinforcement Learning research and applications.

## 2 Preliminaries

In this study, we consider a decision-making agent that operates within the framework of Markov Decision Processes (MDPs) . At every time step \(t\), the agent receives an observation of the world \(o_{t}\), chooses an action \(a_{t}\), and receives a scalar reward \(r_{t}\). Our goal is to learn a single optimal policy distribution \(P_{}^{*}(a^{t}|o^{ t},a^{<t},r^{<t})\) with parameters \(\) that maximizes the agent's total future return \(R_{t}=_{k>t}r^{k}\) on all the environments we consider.

### Offline Reinforcement Learning

Offline RL, also known as batch RL, is a type of RL where an agent learns to make decisions by analyzing a fixed dataset of previously collected experiences, rather than interacting with an environment in real-time. In other words, the agent learns from a batch of offline data rather than actively exploring and collecting new data online.

Offline RL has gained significant attention in recent years due to its potential to leverage large amounts of pre-existing data and to solve RL problems in scenarios where online exploration is impractical or costly. Examples of such scenarios include medical treatment optimization , finance , and recommendation systems .

Despite its potential benefits, offline RL faces several challenges, such as distributional shift, which occurs when the offline data distribution differs significantly from the online data distribution, and the risk of overfitting to the fixed dataset. A number of recent research efforts have addressed these challenges, including methods for importance weighting [35; 37], regularization [53; 28], and model-based learning [25; 29], among others.

### Decision Transformer

The Decision Transformer architecture, introduced by , approaches the offline RL problem as a type of sequence modeling. Unlike many traditional RL methods that estimate value functions or compute policy gradients, DT predicts future actions based on a sequence of past states, actions, and rewards. The input to DT includes a sequence of past states, actions, and rewards, and the output is the next action to be taken. DT uses a Transformer architecture , which is composed of stacked self-attention layers with residual connections. The Transformer architecture has been shown to effectively process long input sequences and produce accurate outputs.

Despite the success of being applied to offline RL tasks, it has a limitation in its ability to perform "stitching." Stitching refers to the ability to combine parts of sub-optimal trajectories to produce an optimal trajectory. This approach can lead to a situation where the agent follows a sub-optimal trajectory that provides an immediate reward, even if a different trajectory leads to a higher cumulative reward over time. This limitation of DT is a significant challenge in many offline RL applications, and addressing it would greatly enhance the effectiveness of DT in solving real-world problems.

## 3 Elastic Decision Transformer

In this section, we present Elastic Decision Transformer (EDT), a model that automatically utilizes a shorter history to predict the next action when the traversed trajectory underperforms compared to those in the training dataset. The mechanism allows the model to switch to a better trajectory by forgetting 'unsuccessful' past experiences, thus opening up more possibilities for future trajectories. We further propose a method to estimate the maximum achieveable return using the truncated history, allowing EDT to determine the optimal history length and corresponding actions.

### Reinforcement Learning as Sequence Modeling

In this paper, we adopt an approach to offline reinforcement learning that is based on a sequence modeling problem. Specifically, we model the probability of the next token in the sequence (denoted as \(\)) based on all the tokens that come before it. The sequences we model can be represented as:

\[=...,o^{t},^{t},a^{t},...,\]

where \(t\) is a time step and \(\) is the return for the remaining sequence. The sequence we consider here is similar to the one used in  whereas we do not include reward as part of the sequence and we predict an additional quantity \(\) that enables us to estimate an optimal input length, which we will cover in the following paragraphs. Figure 2 presents an overview of our model architecture. It should

Figure 2: An overview of our Elastic Decision Transformer architecture. \(\) is the prediction of the maximum return. We also show the environments we used in the experiments on the right. We adopt four tasks for D4RL  and \(20\) tasks for Atari games.

be noted that we also change the way to predict future observation from standard DT , where the next observation is usually directly predicted from \(a^{t}\) through the causal transformer decoder.

### Motivation

We propose a shift in the traditional approach to trajectory stitching. Instead of focusing on training phases, we aim to achieve this stitching during the action inference stage. This concept is illustrated in Figure 3 using a simplified example. In this scenario, we consider a dataset, \(D\), comprising only two trajectories: \(D=(s^{a}_{t-1},s_{t},s^{a}_{t+1}),(s^{b}_{t-1},s_{t},s^{b}_{t+1})\). A sequence model trained with this dataset is likely to predict the next states in a manner consistent with their original trajectories.

To overcome this, we propose a method that enables trajectory stitching, where the model starts from \(s^{b}_{t-1}\) and concludes at \(s^{a}_{t+1}\). This is achieved by adaptively adjusting the history length. We introduce a maximal value estimator, \(\), which calculates the maximum value among all potential outcomes within the dataset. This allows us to determine the optimal history length that maximizes \(\).

In the given example, if the model starts at state \(s^{b}_{t-1}\), it will choose to retain the history \((s_{t})\) upon reaching state \(s_{t}\), as \((s_{t})>(s_{t-1},s_{t})\). Conversely, if the model initiates from state \(s^{a}_{t-1}\), it will preserve the history \((s^{a}_{t-1},s_{t})\) when decision-making at \(s_{t}\), as \((s^{a}_{t-1},s_{t})(s_{t})\). From the above example, we understand that the optimal history length depends on the quality of the current trajectory we've traversed, and it can be a specific length anywhere between a preset maximal length and a single unit.

To estimate the optimal history length in a general scenario, we propose solving the following optimization problem:

\[*{arg\,max}_{T}_{_{T} D}^{t}(_{T}),\] (1)

where \(_{T}\) denotes the history length \(T\). More precisely, \(_{T}\) takes the form:

\[_{T}= o^{t-T+1},^{t-T+1},a^{t-T+1},...,o^{t-1},^{t-1},a^{t-1},o^{t},^{t},a^{t}.\]

### Training objective for Maximum In-Support Return

In the EDT, we adhere to the same training procedure as used in the DT. The key distinction lies in the training objective - we aim to estimate the maximum achievable return for a given history length in EDT. To approximate the maximum operator in \(_{_{T} D}^{t}(_{T})\), we employ expectile regression [38; 3], a technique often used in applied statistics and econometrics. This method has previously been incorporated into offline reinforcement learning; for instance, IQL  used expectile regression to estimate the Q-learning objective implicitly. Here, we leverage it to enhance our estimation of the maximum expected return for a trajectory, even within limited data contexts. The \((0,1)\) expectile of a random variable \(X\) is the solution to an asymmetric least squares problem, as follows:

\[*{arg\,min}_{m_{}}_{x X}[L_{2}^{}(x -m_{})],\]

where \(L_{2}^{}(u)=|-(u<0)|u^{2}\).

Through expectile regression, we can approximate \(_{_{T} D}^{t}(_{T})\):

\[^{t}_{T}=_{_{T} D}^{t}(_{T}) *{arg\,min}_{^{t}(_{T})}_{_{T} D}[L_{2 }^{}(^{t}(_{T})-^{t})].\] (2)

We estimate \(^{t}\) by applying an empirical loss of Equation 2 with a sufficiently large \(\) (we use \(=0.99\) in all experiments). The only difference in training EDT compared to other DT variants is the use of Equation 2, making the training time comparably shorter. We summarize our objective as:

\[_{}=c_{r}_{}+_{}+_{}+_{},\] (3)

Figure 3: A Toy example to illustrate the motivation of EDT. The figure shows an offline RL dataset that contains only two trajectories \((s^{a}_{t-1},s_{t},s^{a}_{t+1}),(s^{b}_{t-1},s_{t},s^{b}_{t+1})\).

where \(_{}\) and \(_{}\) are computed with a mean square error, \(_{}\) is a cross-entropy loss, and \(_{}\) is an empirical estimate of Equation 2. We set \(c_{r}=0.001\) to balance scale differences between mean square error and cross-entropy loss. In tasks with discrete action spaces like Atari, we optimize the action space as the tokenized return objective \(_{}\) using cross-entropy with weight \(10c_{r}\). Our training method extends the work of  by estimating the maximum expected return value for a trajectory using Equation 2. This estimation aids in comparing expected returns of different trajectories over various history lengths. Our proposed method is not only easy to optimize, but can also be conveniently integrated with other DT variants. As such, it marks a significant advance in developing efficient offline reinforcement learning approaches for complex decision-making tasks.

### Action Inference During Test time

During action inference phase in test time, we first (1) estimate the maximum achievable return \(_{i}\) for each history length \(i\). Subsequently, (2) we predict the action by using the truncated traversed trajectory as input. The trajectory is truncated to the history length that corresponds to the highest value of \(_{i}\). These steps are elaborated in Figure 4.

To identify the history length \(i\) that corresponds to the highest \(_{i}^{t}\), we employ a search strategy as detailed in Algorithm 1. As exhaustively searching through all possible lengths from \(1\) to \(T\) may result in slow action inference, we introduce a step size \(\) to accelerate the process. This step size not only enhances inference speed by a factor of \(\), but also empirically improves the quality of the learned policy. An ablation study on the impact of the step size \(\) is provided in Appendix A. For all experiments, we set \(=2\) to eliminate the need for parameter tuning.

To sample from expert return distribution \(P(R^{t},...|^{t})\), we adopt an approach similar to  by applying Bayes' rule \(P(R^{t},...|^{t}) P(^{t}|R^{t},...)P(R^{t},...)\) and approximate the distribu

Figure 4: The figure illustrates the action inference procedure within the proposed Elastic Decision Transformer. Initially, we estimate the value maximizer, \(_{i},\) for each length \(i\) within the search space, as delineated by the green rectangle. Subsequently, we identify the maximal value from all \(_{i}\), which provides the optimal history length \(w\). Utilizing this optimal history length, we estimate the expert value at time step \(t\), denoted as \(_{w,e}^{t}\), by Bayes’ Rule. Finally, the action prediction is accomplished via the causal transformer decoder, which is indicated by the blue rectangle. In practice, we retain the distribution of \(R_{i}^{t}\) during the estimation process for \(_{i}\) and we present the inference here for clarity.

tion of expert-level return with inverse temperature \(\)[24; 48; 47; 43]:

\[P(R^{t}|^{t},...)( R^{t})P(R^{t}).\] (4)

While it may initially appear feasible to directly use the predicted \(\) as the expert return, it's important to note that this remains a conservative maximum operation. Empirically, we have found that Eq. 4 encourages the pursuit of higher returns, which consequently enhances the quality of the actions taken.

## 4 Experiments

Our experiments are designed to address several key questions, each corresponding to a specific section of our study:

* Does EDT significantly outperform DT and its variants? (Sec. 4.2, 4.3)
* Is the EDT effective in a multi-task learning regime, such as Locomotion and Atari games? (Sec. 4.3)
* Does a dynamic history length approach surpass a fixed length one? (Sec. 4.4)
* How does the expectile level \(\) impact the model's performance? (Sec. 4.5)
* How does the quality of datasets affect the predicted history lengths? (Sec. 4.6)

We also provide an additional ablation study in Appendix A due to space constraints.

### Baseline Methods

In the subsequent section, we draw comparisons with two methods based on the Decision Transformer: the original Decision Transformer (DT)  and the Q-learning Decision Transformer (QDT) . Additionally, we include a behavior cloning-based method (TS+BC) , as well as two offline Q-learning methods, namely S4RL  and IQL , in our comparisons.

It is important to note that QDT and TS+BC are specifically designed to achieve trajectory stitching. QDT accomplishes this by substituting collected return values with estimates derived from Conservative Q-Learning . Conversely, TS+BC employs a model-based data augmentation strategy to bring about the stitching of trajectories."

### Single-Task Offline Reinforcement Learning

For locomotion tasks, we train offline RL models on D4RL's "medium" and "medium-replay" datasets. The "medium" dataset comes from a policy reaching about a third of expert performance. The "medium-replay", sourced from this policy's replay buffer, poses a greater challenge for sequence modeling approaches such as DT.

We conclude our locomotion results in Table 1. Since the proposed model estimates the return of the current sequence, reward information is not required during test time.

Our observations indicate that the proposed EDT consistently outperforms the baseline DT and its variants on the majority of the datasets, with a notable performance advantage on the "medium

  Dataset & DT & QDT & TS+BC & S4RL & IQL & EDT (Ours) \\  hopper-medium & \(60.7 4.5\) & \(57.2 5.6\) & \(\) & \(\) & \(\) & \(\) \\ hopper-medium-replay & \(61.9 13.7\) & \(45.8 35.5\) & \(50.2 17.2\) & \(35.4\) & \(\) & \(\) \\ walker-medium & \(71.9 3.9\) & \(67.5 2.0\) & \(78.8 1.2\) & \(\) & \(79.8 3.0\) & \(72.8 6.2\) \\ walker-medium-replay & \(43.3 14.3\) & \(30.3 16.2\) & \(61.5 5.6\) & \(30.3\) & \(\) & \(\) \\ halfcheetah-medium & \(42.5 0.4\) & \(42.3 2.5\) & \(43.2 0.3\) & \(\) & \(\) & \(42.5 0.9\) \\ halfcheetah-medium-replay & \(34.9 1.6\) & \(30.0 11.1\) & \(39.8 0.6\) & \(\) & \(44.1 1.1\) & \(37.8 1.5\) \\  average & \(52.5\) & \(45.5\) & \(56.3\) & \(56.4\) & \(\) & \(63.4\) \\  ant-medium & \(92.5 5.1\) & - & - & - & \(\) & \(\) \\ ant-medium-replay & \(\) & - & - & - & \(\) & \(\) \\  average & \(90.2\) & - & - & - & \(\) & \(\) \\  

Table 1: Baseline comparisons on D4RL  tasks. Mean of 5 random training initialization seeds, 100 evaluations each. Our result is highlighted. The results of QDT, TS+BC, and S4RL are adopted from their reported scores. Following , we emphasize in bold scores within 5 percent of the maximum per task (\( 0.95\)).

replay datasets. These findings provide strong evidence that our approach is highly effective in stitching together sub-optimal trajectories with high return proportion, a task that DT and its variants cannot accomplish. Although EDT doesn't fully outperform IQL in the single-task learning, it does bridge the gap between Q-learning-based methods and DT by performing trajectory stitching with the estimated maximum return.

### Multi-Task Offline Reinforcement Learning

This section aims to evaluate the multi-task learning ability of our model across diverse tasks, focusing on locomotion and Atari tasks. Locomotion tasks utilize vectorized observations, while Atari tasks depend on image observations. To emphasize the role of trajectory stitching, we restrict our datasets to medium-replay datasets for the four locomotion tasks and datasets derived from DQN Replay  for the Atari tasks. Our evaluations span \(20\) different Atari tasks, with further environment setup details available in the Appendix.

**Locomotion.** In the locomotion multi-task experiment, we maintain the same model architecture as in the single-task setting. By confining the dataset to **medium-replay** datasets from four tasks, we increase task complexity and necessitate the offline RL approach to learn and execute these tasks concurrently, while effectively utilizing trajectories generated by random policies. As depicted in Table 2, our proposed EDT successfully accomplishes all four tasks simultaneously without much performance compromise.

**Atari.** For Atari, we adopt a CNN image encoder used in DrQ-v2  to process stacks of four \(8484\) image observations. To ensure fair comparisons, all methods employ the same architecture for the image encoder. Following , we incorporate random cropping and rotation for image augmentation. Additional experiment details are delegated to the Appendix for brevity. Performance on each Atari game is measured by human normalized scores (HNS) , defined as \((-_{})/(_{ }-_{})\), to ensure a consistent scale across each game.

Our experimental results align with those of , highlighting that Q-learning-based offline RL approaches encounter difficulties in learning a multi-task policy on Atari games. Despite IQL achieving the highest score in Table1, it demonstrates relative inadequacy in simultaneous multi-task learning as indicated in Table 2 and Figure 5. We leave the raw scores of the \(20\) Atari games in Appendix B.

  Task & DT-1 & IQL-1 & EDT-1 (Ours) \\  hopper & \(51.2\) & \(59.8\) & **76.9** \\ walker & \(29.8\) & \(52.6\) & **74.1** \\ halfcheetah & \(30.5\) & **40.4** & \(36.8\) \\ ant & \(79.8\) & \(82.3\) & **88.6** \\  sum & \(191.3\) & \(235.1\) & **276.4** \\  

Table 2: Evaluation results on multi-task regime. Mean of 5 random training initialization seeds, 100 evaluations each. The training dataset is a mixture of **medium-replay** datasets from the four locomotion tasks. Our main result is highlighted.

  Dataset & EDT (w=20) & EDT (w=5) & EDT (Ours) \\  hopper-m & \(\) & \(57.8 7.0\) & \(\) \\ hopper-mr & \(67.6 27.7\) & \(66.6 26.9\) & \(\) \\ walker-m & \(65.6 11.7\) & \(62.6 14.3\) & \(\) \\ walker-mr & \(64.5 12.9\) & \(44.3 8.7\) & \(\) \\ halfcheetah-m & \(42.0 0.4\) & \(42.3 0.8\) & \(\) \\ halfcheetah-m & \(33.5 0.8\) & \(36.4 7.4\) & \(\) \\ ant-m & \(90.8 16.0\) & \(95.6 8.0\) & \(\) \\ ant-mr & \(80.0 17.0\) & \(82.3 15.9\) & \(\) \\  sum & \(507.6\) & \(483.9\) & **570.3** \\  

Table 3: Mean of 5 random training initialization seeds, 100 evaluations each. In the Dataset column, “m” indicates medium-replay datasets. The “w” stands for history length. Our main results are highlighted.

Figure 5: The average HNS comparison on \(20\) Atari games. The results are evaluated with three trials.

Figure 6: Ablation study on expectile level \(\). Expectile objective reduces to Mean Square Error when \(=0.5\). Evaluated on Hopper and medium-replay dataset.

### Dynamic History Length vs. Fixed History Length

In Sec. 3.2, we proposed the concept of EDT, which adjusts history length based on the quality of the current trajectory. We illustrated this idea with a toy example.

To validate the benefits of this dynamic history length approach, we tested the EDT using both fixed and variable history lengths. The results, summarized in Table 3, show that the variable history length outperforms the fixed ones, particularly on the "medium-replay" datasets.

These findings suggest that the EDT effectively chooses a history length that yields a higher estimated return. While a shorter history length aids in trajectory stitching, it's also crucial to retain a longer history length to ensure the continuity of optimal trajectories. Therefore, the dynamic adjustment of history length in the EDT is key to its superior performance.

### Ablation Study on Expectile Level \(\)

A key component of EDT is the approximation of the maximal value using expectile learning, as our method depends on accurately estimating these maximal values to choose the optimal history length. Consequently, examining the change in performance relative to the expectile level, \(\), provides insight into the necessity of correct history length selection for performance enhancement.

The results, as displayed in Figure 6, suggest that when expectile regression is able to accurately approximate the maximizer, specifically at higher expectile levels, we observe both a higher average performance and lower standard deviation. This suggests that accurate selection of history length not only stabilizes performance but also enhances scores. Conversely, as the expectile level approaches \(0.5\), the expectile regression's objective shifts towards a mean square error, leading to an estimated value that is more of a mean value than a maximal one. This change makes it a less effective indicator for optimal history length. As a result, we can see a deterioration in EDT's score as the expectile level drops too low, and an increase in standard deviation, indicating inconsistency in the selection of an effective history length.

### Analysis of Optimal History Length Distribution

In our analysis, we examine the history length distributions across various datasets, as depicted in Figure 7. Our findings reveal that the medium-replay dataset, which amalgamates trajectories from multiple policies, yields a distribution closely approximating a uniform distribution. Conversely, the medium dataset, acquired through a singular stochastic policy, exhibits a history length distribution characterized by an increased density at lower history lengths. This observation can be attributed to the prevalence of analogous trajectories within the medium dataset, leading to more frequent occurrences of trajectory stitching than the "medium-replay" dataset. However, it is important to acknowledge that the gains derived from this type of trajectory stitching remain limited, as the trajectories stem from identical distributions. Although performance improvement is observed, as presented in Table 1, it is significantly less pronounced in comparison to the medium-replay dataset.

Contrary to initial expectations, trajectory stitching does not occur as frequently within the medium-replay dataset as within the medium dataset. In fact, the distinct policies within the medium dataset contribute to the reduced instances of trajectory stitching, as their respective state distributions differ from one another. The diversity within the dataset results in a limited number of mutual

Figure 7: The figures illustrate the history length distributions across datasets and training epochs. For each distribution, we collect ten trajectories and derive a histogram of history lengths. The distribution is computed with kernel distribution estimate for better visualization.

instances illustrated in Figure 3. Nevertheless, the proposed EDT method derives substantial benefits from trajectory stitching in this context. The EDT effectively avoids being misled by sub-optimal trajectories within the dataset, demonstrating its capacity to make better decisions regarding history lengths and actions that optimize the current return.

## 5 Related Work

**Offline Reinforcement Learning.** Offline RL has been a promising topics for researchers since sampling from environments during training is usually costly and dangerous in real-world applications and offline reinforcement learning is able to learn a better policy without directly collecting state-action pairs. Several previous works have utilized constrained or regularized dynamic programming to mitigate deviations from the behavior policy [39; 51; 27; 16; 56].

Decision Transformer and its variants [11; 30; 59; 58; 57] have been a promising direction for solving offline RL from the perspective of sequence modeling. Trajectory Transformer (TT)  models distributions over trajectories using transformer architecture. The approach also incorporates beam search as a planning algorithm and demonstrates exceptional flexibility across various applications, such as long-horizon dynamics prediction, imitation learning, goal-conditioned reinforcement learning, and offline reinforcement learning.

Recently, there has been a growing interest in incorporating diffusion models into offline RL methods. This alternative approach to decision-making stems from the success of generative modeling, which offers the potential to address offline RL problems more effectively. For instance,  reinterprets Implicit Q-learning as an actor-critic method, using samples from a diffusion parameterized behavior policy to improve performance. Similarly, other diffusion-based methods [21; 32; 50; 5; 10; 4] utilize diffusion-based generative models to represent policies or model dynamics, achieving competitive or superior performance across various tasks.

**Trajectory Stitching.** A variety of methods have been proposed to tackle the trajectory stitching problem in offline RL. The Q-learning Decision Transformer (QDT)  stands out as it relabels the ground-truth return-to-go with estimated values, a technique expected to foster trajectory recombination. Taking a different approach,  utilizes a model-based data augmentation strategy, stitching together parts of historical demonstrations to create superior trajectories. Similarly, the Best Action Trajectory Stitching (BATS)  algorithm forms a tabular Markov Decision Process over logged data, adding new transitions using short planned trajectories. BATS not only aids in identifying advantageous trajectories but also provides theoretical bounds on the value function. These efforts highlight the breadth of strategies employed to improve offline RL through innovative trajectory stitching techniques.

## 6 Discussion

**Conclusion.** In this paper, we introduced the Elastic Decision Transformer, a significant enhancement to the Decision Transformer that addresses its limitations in offline reinforcement learning. EDT's innovation lies in its ability to determine the optimal history length, promoting trajectory stitching. We proposed a method for estimating this optimal history length by learning an approximate value optimizer through expectile regression.

Our experiments affirmed EDT's superior performance compared to DT and other leading offline RL algorithms, notably in multi-task scenarios. EDT's implementation is computationally efficient and straightforward to incorporate with other DT variants. It outshines existing methods on the D4RL benchmark and Atari games, underscoring its potential to propel offline RL forward.

In summary, EDT offers a promising solution for trajectory stitching, enabling the creation of better sequences from sub-optimal trajectories. This capability can considerably enhance DT variants, leading to improved performance across diverse applications. We are committed to **releasing our code.**

**Limitations.** A potential direction for future improvement involves enhancing the speed at which EDT estimates the optimal history. This could make the method suitable for real-time applications that have strict time constraints. While this adaptation is an exciting avenue for future research, it falls outside the primary scope of this paper.