# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

to downstream tasks. Furthermore, our method exhibits strong orthogonality with traditional methods, allowing for concurrent usage. Our code can be found at https://github.com/ShiLuohe/ReferenceTrustableDecoding

## 1 Introduction

In the rapidly advancing field of artificial intelligence, Large Language Models (LLMs) have demonstrated substantial progress. With their extensive parameter size, LLMs have acquired emergent abilities  and been able to tackle diverse and challenging tasks in fields like education  and medicine . Despite their immense potential, Large Language Models that have just completed pre-training often struggle to effectively adapt to downstream tasks. Moreover, the process of adapting the model is typically costly and requires careful execution by experienced individuals. Otherwise, it could lead to the model generating hallucination [50; 28] at best, or at worst, result in a loss of its language capabilities.

In-Context Learning (ICL), as a category of methods that do not require parameter adjustments, is one of the mainstream methods for adapting models to downstream tasks. ICL embeds domain knowledge, question-answering patterns, etc., into prompts through few-shot learning , prompt engineering , and Retrieval-Augmented Generation (RAG)  methods, leveraging the learning ability of the model itself to provide better answers. As pointed out in Figure 2, ICL focuses on the prompt stage. However, ICL significantly increases the length of the input, consequently increases the space occupied by the KV-Cache required for inference. Further, according to the Roofline model , this part of the KV-Cache cannot be parallelized through batch processing, making memory I/O throughput a system bottleneck, wasting hardware computing power, and increasing token generation time during the entire inference stage.

Fine-tuning is also used to adapt models to downstream tasks. By fine-tuning the pre-trained model based on domain tasks, the model can quickly acquire capabilities within the domain. However, traditional full-parameter fine-tuning often requires a large amount of resources (empirically 8-15 times that of inference), making Parameter-Efficient Fine-Tuning (PEFT) a more popular method. By freezing most parameters and only modifying a few, methods such as Adapters, P-tuning , LoRA  and others [48; 36; 44] have become mainstream methods for quickly adapting models to downstream tasks. However, fine-tuning methods introduce several hyperparameters, which require high experience from the fine-tuners and the effects are unpredictable. Furthermore, due to the need for backpropagation, the computation graph must be saved, meaning that even if only a few parameters need to be updated, there will be a large amount of additional computation and space requirements (several times that of inference), raising the threshold for methods based on fine-tuning.

To address these challenges, we introduce Reference Trustable Decoding (RTD), a novel framework designed to fit LLMs for downstream tasks. Distinct from a conventional LM_Head module, RTD strategically retrieves relevant references from a pre-constructed datastore, guided by the final hidden states of the language model. This approach not only enhances the final output distribution by recalculating it with the similarity score of the retrieved references but also allows for the seamless integration of new knowledge or constraints into the response generation process without increasing the input length or using gradient descent.

RTD, distinctively training-free, emphasizes compact input lengths to expedite inference. RTD's effectiveness was rigorously tested using varied benchmarks focused on different tasks and a Wikipedal-based knowledge injection scenario. On these benchmarks, RTD achieved results comparable to traditional methods like PEFT and ICL, providing significant improvement. Additionally, we combined RTD with traditional methods, further enhancing the model's capabilities and demonstrating the good orthogonality of RTD with other approaches.

Our contribution includes:

* We propose a new paradigm, called RTD, for fitting LLMs for downstream tasks. RTD is a training-free method that focused on the decoding stage of large language models (LLMs), as a alternation of LM_Head. It helps LLMs to adapt to different tasks with different demands and provide trustable response.
* RTD has achieved performance comparable to, or even better than, ICL and PEFT across different benches, while maintaining the desirable properties of training free and not introducing additional input lengths. This demonstrates the potential of RTD as a new paradigm for LLMs to adapt to downstream tasks. Furthermore, RTD can be seamlessly integrated with other existing methods, such as in-context learning (ICL) and fine-tuning. The combination of RTD, ICL, and fine-tuning has the potential to achieve even higher performance.

## 2 Background and Related Work

In the field of NLPs, Transformer models have gained influence rapidly after it get original proposed in 2017. As larger scaled model been introduced, especially giant ones like GPT3 which has 175 Billion of parameters , the training process is getting more and more expensive, hence to fit the LLMs for downstream tasks.

### Fine-tuning

Full Parameter Fine-tuningFull parameter fine-tuning refers to fully optimizing all the parameters of the model during the fine-tuning process. Full parameter fine-tuning has the advantage of allowing the model to adapt more closely to the specific task at hand, as well as injecting more information into the model. However, it also has the disadvantage of being the most computationally expensive and time-consuming, as it requires to manipulate all parameters of the model, with the modern optimizer like Adam , 8 to 15 times of more extra GPU memory is demanded comparing to inference empirically, resulting a must of multi-GPU server or even cross sever training.

Parameter Efficient Fine-tuningParameter Efficient Fine-tune (PEFT), for example, LoRA  and P-tuning , is introduced to make fine-tune more reachable. By freezing most of the model parameters and only let a small amount of them accumulate gradient, the GPU memory and computation resource can be cut down by a large margin .

However, as fine-tuning introduce many tricky hyper-parameters like learning rate, the process is heavily task related and empirical, even experienced fine-tuner need some trials and error when tuning them. Moreover, even if the number of parameters trained is not large, processes such as backpropagation still need to be carried out. The computation graph generated on long sequences will also occupy a large amount of memory, making the threshold for computing power and memory still high, which any method that relies on gradient descent is difficult to avoid.

### In-Context Learning

Few-Shot LearningFew-shot Learning is proved to be a great way for LLMs to gain capability. By appending the true task that LLMs are expecting to response after a couple of existing correct examples, LLMs can gain its reasoning ability [6; 31].

Retrieval Augmented GenerationRetrieval Augmented Generation (RAG)  is an AI framework for retrieving facts from an external knowledge source to LLMs, which helps LLMs correct its hallucination and use latest fact . RAG is to cut external knowledge source into multiple chunks,

Figure 2: The pipeline of LLM inference and the focus of different methods: ICL focuses on the prompt stage, emphasizing the optimization of the modelâ€™s input. Fine-tuning methods optimize the model itself by adjusting its parameters. In contrast, our proposed RTD method targets the decoding stage of the language model. By constructing a reference datastore, RTD optimizes the final output distribution without requiring additional training.

then embed and store them in a database, then retrieve them at the process of generation to let LLMs get the knowledge in it. This technique allowed LLM to use extra information while maintaining their parameters untouched. RAG have been used on multiple fields, like coding  and question answering . And it can be combined with few-shot .

The main drawback of ICL methods lies in their growth of the input sequence. Under the quadratic complexity of the Transformer architecture, this implies a longer KV-Cache, which not only increases the latency during the pre-fill stage but also adds delay each time a token is generated . Moreover, unlike model parameters, each instance needs to save its dedicated portion of KV-Cache, leading to memory I/O bottlenecks and computational power waste. Finally, on some smaller models, the irrelevant information that ICL might contain can confuse the model, resulting in performance loss.

## 3 Reference Trustable Decoding

In this section, we begin by presenting the fundamental formulas and concepts to elucidate the workings of Reference Trustable Decoding, followed by an exploration of the multi-head Reference Trustable Decoding method.

### Preliminary

Given an input sentence \(c=\{t_{1},t_{2},...,t_{n}\}\), where \(t_{i}\) represents the i-th token and \(n\) denotes the sentence length, the last token's output of the last Transformer block in the language model can be represented as:

\[h^{(l)}=(c)\] (1)

In this equation, \(h^{(l)}^{d_{m}}\) is the output of the last token from the final, or the \(l\)-th, Transformer block of the language model, where \(d_{m}\) denotes the hidden size of the model.

Traditionally, a standard decoder-only architecture Transformer usually employs LM_Head, which is, a fully connected layer, usually includes a learnable weight matrix \(W\) and no bias, followed by a softmax function \(()\) to predict the output probability distribution \(\) of the next token from the last hidden states:

\[=(h^{(l)})=(W h^{(l)})\] (2)

where \(v\) is the vocabulary size and \(W^{v d_{m}}\).

However, traditional next token prediction does not support incorporating external information and therefore, we introduce reference trustable decoding where we build a bypass around the LM_Head, showcased in Figure 3, as the entrance of additional knowledge or guidance.

### Reference Trustable Decoding

#### 3.2.1 Generation of Reference Datastore

In reference trustable decoding, we first build the reference datastore \(\), which stores key-value pairs \((k,v)(,)\). Here, the key \(k=(c)\) represents the last hidden states of the token generated by the LMs from the context \(c\), and the value \(v\) is the corresponding label \(y\). Mathematically, we have:

\[=\{(k,v)|(k,v)(,)\}=\{(( c),y)\,(c,y)\}\] (3)

where \(=(,)\) is the task dataset with input context set \(\) and label set \(\), and \(||\) refers the number of possible labels. This process is depicted in Figure 3. It's obvious that **the computational requirement is same as performing a forward pass to every content in the task dataset**, which aligned with the minimal requirement of the inference stage, denotes the superiority of RTD as a gradient-free method.

#### 3.2.2 Decoding Stage

At each decoding round, given the input context \(c\), we first compute \(h^{(l)}=(c)\), which is the input to RTD and LM_Head. Then we use a three stage approach to get the RTD output, **Fetch**, **Normalization**, and **Aggregation**, depicted in Figure 4.

FetchFirst, we calculate the distance \(d^{}\) between \(h^{(l)}\) and all the \(k\) in the reference datastore \(\). Otherwise stated, we use the Euclidean distance \(d^{}_{i}=||h^{(l)}-k_{i}||_{2}\). We then select the top \(K\) instances from \(\) which have the smallest distance, and for the \(j\)-th (\(1 j K\)) closest \((k_{i},v_{i})\), we define \(o_{j}=i\). Then we create a set \(L_{h}\), storing the top \(K\) distances and values:

\[L_{h}=\{(d^{}_{o_{j}},v_{o_{j}})\}=\{(||h^{(l)}-k_{o_{j}}||_{2},v_{o_{j} })\}, o_{j}=i\ \ j{-}\ (k_{i},v_{i})\] (4)

NormalizationWe first scale the \(d^{}\) we got from the previous stage by temperature \(T\), as \(d^{}_{j}=d^{}_{o_{j}}/T\). The scale operation is introduced to prevent overflow in the following Softmax operation. We take the Softmax of \(-d^{}\) as \(d\), guaranteed \(d\) as a valid possibility distribution.

\[d=(-d^{}), d_{j}=_{j}\}}{_{i=1}^{K}\{-d^{}_{i}\}}= _{o_{j}}/T\}}{_{i=1}^{K}\{-d^{}_{o_{i}}/T\}}\] (5)

AggregationWe calculate the final reference possibility distribution \(=[r_{1},r_{2},...,r_{[]}]^{||}\) by aggregating all \(d_{j}\) that satisfies \(v_{o_{j}}=y_{i}\), where \(y_{i}\).

\[r_{i}=_{v_{o_{j}}=y_{i}}d_{j}\] (6)

We denote \((,):^{d_{m}}^{| |}\) as the function represents all three stages of querying the datastore \(\) and building the corresponding reference possibility distribution \(\). Therefore, we have

\[=(h^{(l)},)\] (7)

Additionally, when \(||=v\), we can merge the distribution \(\) given by \(()\) and \(\) given by \((,)\) with a hyper-parameter \(\):

\[d^{}=+(1-)\] (8)

which is a common fusion method for mixing two distributions [34; 23; 12; 10].

Figure 4: Three stages of reference trustable decoding.

Figure 3: Overview of the reference datastore generation and reference trustable decoding process.

### Multi-head Reference Trustable Decoding

Large language models like LLaMA2-70B  or Mistral-7B  utilized MHA and GQA mechanism , implies the potential of splitting a large attention vector into smaller ones. So we adapt this method into our RTD process. We define \(n_{h}\) of the head count of the LM model, and \(d_{h}\) the dimension of the each attention head where \(d_{m}=n_{h} d_{h}\). with this in mind, we split the reference datastore into \(n_{h}\) sub-datastore by head. When decoding, we first split \(h^{(l)}\) in to heads, then query each sub-datastore and merge the result, showcased in Figure 5. Mathematically,

\[ k^{(i)}&=k[d_{h}(i-1):\ d_{h} i ], h^{(l,i)}=h^{(l)}[d_{h}(i-1):\ d_{h} i]\\ ^{(i)}&=\{(k^{(i)},v)|(k,v)( ,)\}\] (9)

And we denote \(_{}(,)\) as the function of the multi-head RTD query process, we have:

\[=_{}(h^{(l)},)=} _{i=1}^{n_{h}}(h^{(l,i)},^{(i)})\] (10)

### Time and Memory Consumption

Time ConsumptionThe time consuming is largely depended on the vector datastore used. For a brute force searching datastore, the time complexity will be \((s_{} d_{m})\) where \(s_{}=||\) is the size of the datastore. However, for those more powerful database like faiss  by Meta, with extra training after the generation of reference datastore, the process which have to be done again if the datastore changes, the time consumption can be cut to \((k d_{m})\), where \(k\) is a constant related the parameters used to train the database.

For multi-head reference trustable decoding, the performance cost remains the same. The time complexity of each attention-head wise query is \((d_{h} s_{})\), the overall query time complexity is \((n d_{h} s_{})=( d s_{})\), which is the time complexity of convention reference trustable decoding processing. The calculation remains the same for a trained database, the overall time complexity is \((n k d_{h})=(k d_{m})\).

Memory ConsumptionThe use of time can be optimized by utilizing vector database, however the memory consumption cannot shrink easily. We further define \(b\) as the bit cost of the models' dtype, where \(b_{}=4,b_{}=b_{}=2,b_{ }=1,b_{}=\). The overall memory cost is \(d_{m} b s_{}\). Due to the lack of lower precision dtype support on CPU, even the base model utilized popular half precision dtype like bfloat16, it still need to be converted into larger ones to be stored. Since all the hidden states have to be saved to calculate precise distanced when rescalled, the memory cost can't be reduced significantly by making it irrelevant with \(s_{}\).

   Method & RTD & MH-RTD \\  MPT-7B & 27.4 & 30.9 \\ LLaMA2-7B & 47.1 & 52.4 \\ LLaMA2-70B & 63.3 & 65.6 \\   

Table 1: Comparison of RTD and MH-RTD on Open Book QA.

Figure 5: Comparison between RTD and multi-head RTD.

On the Multi-head RTD side, the memory cost remains the same as the regular RTD takes. The proof is same as the Section 3.4. For instance, reference datastore and head-wise reference datastore with \(20,480\) entries with \(d_{m}=4096\), \(n=32\) and \(d_{h}=128\), stored in float32, takes \(320\)MB of memory and hard disk space.

MH-RTD for Resource SavingAs MH-RTD splits long vectors into multiple smaller ones, it gives us the opportunity to cut time and memory cost by merging different heads together, or directly evict some of them. If on average, \(p\) heads are merged into one head, then we expect a \(\) resource consumption. The time and memory improvement and corresponding performance impact can be find in the tuning Section 4.3.

## 4 Settings and Experiment

We categorize the common downstream tasks of language models into two types: language understanding and language generation. The former focuses on understanding the input information, based on the context and the information stored within the model, and then outputs the answer in the form of a few tokens, usually in a very simple form. The latter focuses on generating new sentences with complete semantics. We explored the potential of RTD compared to other methods on these two types of tasks. We first compared the effects of RTD and MH-RTD. As shown in Table 1, we found that MH-RTD effectively enhances the capabilities of RTD. Therefore, we default to using the MH-RTD method in the following tests.

### Language Understanding

We tested the language understanding capabilities of RTD on multiple benchmarks. When testing, question without answer be shown to the LLM, then we will gather it's baseline output by LLMs' first output token and our RTD result through searching our reference datastore. \(\) is set to \(1\) in this task. How the reference datastore is generated can be found at appendix B.1.

Models we used are: LLaMA2-7B and 70B , LLaMA3-8B , MPT-7B , GLM3-6B , Yi-34B. Includes model size from 6B to 70B, as most of the major current models are. We use the _base_ version of the model by default. Testing benchmarks are: Massive Multitask Language Understanding (MMLU) , AI2 Reasoning Challenge (ARC, both Easy (E) and Challenge (C) parts) , Reasoning about Physical Commonsense in Natural Language (PIQA) , Open Book Question Answering (OBQA) , and Massive Multitask Language Understanding in Chinese (CMMLU) . C-MMLU is a Chinese benchmark, so only Chinese models, GLM3 and Yi, participated in this benchmark.

The multiple-choice benchmarks we chose is challenging enough in itself and requires strong reasoning ability from the model; moreover, the answer format is fixed, which can simultaneously detect the ability to follow instructions. Since that most tasks in the traditional NLP field can be quickly converted into tasks of choosing one from several categories, even some generative tasks, so the results on the multiple-choice test can also represent many other tasks.

The performance boost can be found both with or without ICL. Results are in table 2. Besides testing scores, we also record the confused rate of baseline, the proportion of the questions that failed to be answered properly, including output irrelevant text or can't give a certain answer, in table 3. Meanwhile RTD is designed to given the LLMs' decision in a trustable and controllable way. In comparison with fine-tuning methods in table 4, we can notice that RTD can achieve approximate performance improvements as using PEFT methods like LoRA. Although it is still insufficient compared to full-parameter fine-tuning, the latter has a higher cost and has undergone knowledge injection (which is not considered in this part of the experiment). The dataset used for full-parameter fine-tuning is MMLU-Recall [32; 33], and the hyper-parameters of LoRA can be found in Appendix D. Moreover, we've tested obqa score with different source of reference library, testing the generalization ability of RTD, as shown in Table 5, RTD yields satisfactory results. We've also tested the performance of RTD with different \(\) for language understanding, shown in Table 6. Lastly, we've tested the iteration speed of these benchmarks, as shown in Table 7, the efficiency impact of RTD is minimized comparing to ICL.

### Language Generation

Reasoning with ContextGenerative tasks are generally subjective and difficult to test. We constructed a benchmark based on Retrieval-Augmented Generation (RAG) and Open Book Question Answering  to test the potential of RTD in areas requires advance reasoning such as knowledge injection. Chain-of-Thought  is a method that encourage the model to provide a step-by-step analysis before giving the final answer, thereby enhancing the model's capabilities. We compared the performance of the model when introducing references through the ICL method and the RTD method, to determine the effectiveness of the RTD method. The extra knowledge source was Wikipedia. The generation of the datastore can be found in detailed in Appendix B.2. With the results of table 8, it can be seen that RTD was indeed helpful in knowledge injection. Besides, the context length is shrunk by a lot, thus saves reasoning GPU time and memory consumption. A detailed exploration of why RAG score is lower than baseline can be find in Appendix C.

Style transferTo explore whether the RTD method can be used to modify the language style of the model, we designed a style transfer experiment. We used a moderately scaled and strongly styled dataset, Tiny-Shakespeare [21; 40], and compared the perplexity (PPL) of the model on the test set after LoRA and RTD, to measure whether our method can help the model change the output style.

   Model &  &  &  \\  Rate & 8.6\% & 11.81\% & 0.44\% & & & \\   

Table 3: Confused rate.

   Model & Benchmark & Baseline & 5-shot ICL & RTD (\(\)) & 5-shot RTD (\(\)) \\  LLaMA2-7B & MMLU & 43.8 & 45.8 & 45.1 (1.3\(\)) & **47.2** (2.1\(\)) \\  & ARC (E \& C) & 30.1 & 65.0 & 41.4 (11.3\(\)) & **67.3** (2.3\(\)) \\  & PIQA & 56.5 & 62.1 & 71.4 (14.9\(\)) & **73.2** (11.1\(\)) \\  & Openbook QA & 27.8 & 51.0 & 30.4 (2.6\(\)) & **53.6** (2.6\(\)) \\  LLaMA2-70B & MMLU & 56.7 & 67.9 & 56.9 (0.2\(\)) & **68.5** (0.6\(\)) \\  & ARC (E \& C) & 67.4 & 91.6 & 86.1 (19.7\(\)) & **91.7** (0.1\(\)) \\  & PIQA & 72.3 & 85.3 & 81.9 (9.6\(\)) & **86.6** (1.3\(\)) \\  & OpenbookQA & 53.7 & 84.4 & 68.2 (14.5\(\)) & **85.4** (1.0\(\)) \\  LLaMA3-8B & MMLU & 47.5 & **63.9** & 57.2 (9.7\(\)) & 61.9 (2.0\(\)) \\  & ARC (E \& C) & 71.2 & **87.3** & 83.7 (12.5\(\)) & 87.1 (0.2\(\)) \\  & PIQA & 69.9 & 78.9 & 76.3 (6.4\(\)) & **80.0** (1.1\(\)) \\  & OpenbookQA & 53.3 & 77.5 & 71.4(18.1\(\)) & **78.6** (1.1\(\)) \\  MPT-7B & MMLU & 27.4 & 29.6 & **30.4** (3.0\(\)) & 29.8 (0.2\(\)) \\  & ARC (E \& C) & 27.5 & failed & 27.6 (0.1\(\)) & **30.1** \\  & OpenbookQA & 29.4 & failed & 27.2 (2.2\(\)) & **30.4** \\  GLM3-6B & MMLU & 41.9 & 48.6 & 47.6 (5.7\(\)) & **49.8** (1.2\(\)) \\  & ARC (E \& C) & 59.1 & 75.3 & 75.0 (15.9\(\)) & **76.5** (1.2\(\)) \\  & PIQA & 66.8 & 73.6 & **75.9** (9.1\(\)) & 74.5 (0.9\(\)) \\  & OpenbookQA & 55.1 & 67.1 & 64.0 (8.9\(\)) & **68.8** (1.7\(\)) \\  & C-MMLU & 48.8 & 54.5 & 53.3 (4.5\(\)) & **54.7** (0.2\(\)) \\  Yi-34B & MMLU & 68.6 & **74.3** & 70.3 (1.7\(\)) & 73.3 (1.0\(\)) \\  & ARC (E \& C) & 93.3 & 94.0 & 90.7 (2.6\(\)) & **94.6** (0.6\(\)) \\  & PIQA & 88.3 & 83.5 & **88.4** (0.1\(\)) & 87.7 (4.2\(\)) \\  & OpenbookQA & 83.5 & **89.8** & 88.4 (0.9\(\)) & 88.8 (1.0\(\)) \\  & C-MMLU & 70.3 & 81.0 & 73.9 (3.6\(\)) & **81.8** (0.8\(\)) \\ 
**Avg** & - & 56.41 & 65.28 & 63.31 & **68.88** \\   

Table 2: RTD on language understanding benches. Baseline refers to zero-shot performance. ICL exceeds MPT-7Bâ€™s \(2048\) context window, with a 0 score result, recorded as failed in the table.

[MISSING_PAGE_FAIL:9]

How to make RTD accomplish tasks with high quality while being space-efficient is our following research direction.