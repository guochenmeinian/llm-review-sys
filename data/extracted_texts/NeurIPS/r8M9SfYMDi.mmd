# Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum

Hadi Pouransari\({}^{1,}\) Chun-Liang Li\({}^{1}\) Jen-Hao Rick Chang\({}^{1}\)

**Pavan Kumar Anassalu Vasu\({}^{1}\) Cem Koc\({}^{1}\) Vaishaal Shankar\({}^{2,}\) Oncel Tuzel\({}^{1}\)**

\({}^{1}\)Apple \({}^{2}\)Anthropic

Corresponding author: mpouransari@apple.com, \({}^{}\)Work is done when at Apple.Code to be available at https://github.com/apple/ml-dataset-decomposition.

###### Abstract

Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce _dataset decomposition_, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to \(6\) faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.1

## 1 Introduction

Large language models (LLMs) are often pretrained autoregressively (i.e., predicting the next token given a context) on large text corpora sourced from the web. Examples include The Pile , RefinedWeb , RedPajama , and DOLMA . Each of these datasets comprises multiple documents, ranging from Wikipedia articles to books and code repositories. While the individual lengths of the documents vary from a few words (e.g., a message) to hundreds of thousands of words (e.g., a book), the training infrastructure often supports only a limited sequence length in a batch. To facilitate efficient training, document chunking is necessary. In this paper, we investigate the influence of document chunking, propose alternative strategies, and evaluate the proposed strategies with careful experiments.

Recent works [43; 37; 59; 60] popularized the _concat-and-chunk_ approach to convert text datasets with variable document lengths into sequences with a fixed target length. In this approach, during a data preparation stage before training, we first randomly shuffle and concatenate all tokenized documents. Consecutive concatenated documents are separated by a special token <EOT>, allowing the model to detect document boundaries. We then chunk the concatenated sequence into subsequences with a _target sequence length_. For example, \(2048\) and \(4096\) for the Llama-1 and Llama-2 models, respectively. The model is then pretrained on batches of sequences with fixed length.

The concat-and-chunk approach has several shortcomings. First, randomly concatenating documents can lead to the model attending to a context from an unrelated document to predict the next token. While well-trained models learn to avoid cross-document attention, this is not explicitly enforced, leading to potential spurious modeling. Second, the cross-document attention spends unnecessary computation on attending to unrelated tokens that do not facilitate learning. This is especially crucial due to the quadratic complexity of the attention mechanism. Even with an implementation of attention that supports cross-document attention masking, the computational cost for each optimization step would be bottlenecked by the longest document in the global batch, leading to significant under-utilization of devices with shorter documents. Third, even if a document is shorter than the target sequence length, it may still be broken into two chunks when they are at the boundary of two sequences. This results in significantly smaller average chunk lengths compared to the original document length average (see Fig. 2(a)), which hinders the model's capability.

Recent and concurrent works on LLM training try to improve the concat-and-chunk approach: document-masking is possible with recent implementation of attention  as adopted in some recent pre-training recipes , best-fit packing  to reduce document chunking, and concatenating semantically related documents instead of randomly . However, none of them address all three issues mentioned above together.

In this work, we introduce _dataset decomposition_ (DD), a novel approach to decompose data based on their length and train with _variable sequence length_ (VSL) and length-based curriculum to address the above issues. We obtain significant both significant accuracy improvement and straining speed-up as shown in Fig. 1. DD decomposes a given dataset containing documents of variable lengths into a union of datasets/buckets, each with sequences of a fixed length. Specifically, a dataset \(\) is decomposed into buckets \(_{i}_{i}\), where each bucket \(_{i}\) contains sequences of length \(2^{i}\), each extracted from a unique document. During training with VSL, at every step of the optimization process, we sample \(i\) (based on a curriculum) to form a batch with \(b/2^{i}\) sequences from the bucket \(_{i}\), which

Figure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb dataset. Note that each point on the figure represents a separate training from scratch (not different checkpoints of a single run). For the largest run, some tokens are seen more than once (the dataset has approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum with 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and the baseline. **(a)** Regular metrics average when training with the baseline method and the proposed method. We observe more than \(4\) data efficiency compared to the baseline. Also, even at 1.1 trillion tokens, DD has a +2.4 accuracy improvement compared to the baseline (which has a plateauing accuracy curve even on a logarithmic x-axis). (b) Comparison of model average accuracy versus training cost (GPU-hours). DD reaches the best accuracy of the baseline more than \(6\) faster. This is the combined effect of DD accuracy and speed gains.

keeps the total number of tokens in a batch constant (\(2^{i} b/2^{i}=b\)), regardless of which \(_{i}\) is sampled.

This approach gives us several advantages and resolves the aforementioned issues of the concat-and-chunk method. First, DD is simple and has negligible computational overhead during the data preparation stage, making it easy to scale to large datasets. Second, tokens in each sequence are ensured to be from the same document by construction, which avoids cross-document attention. Furthermore, we have access to the sequence length distribution (an auxiliary prior knowledge) which can be used to create different mixtures/curricula for training. Finally, our VSL training strategy accelerates training time: the latency for one optimization step is less when sampling from \(_{i}\) with smaller \(i\) (due to attention's quadratic complexity). Following is a summary of our contributions:

* We introduce DD, a method to efficiently decompose a dataset of variable-length documents into a union of buckets with fixed-length sequences. DD enables efficient and robust training via VSL and length-based curriculum.
* We perform large-scale experimentation using different models, datasets, and evaluation tasks to demonstrate the efficacy of the proposed method. We show (see Fig. 1) significant gains in data efficiency (\(>4\)) and compute efficiency (11% to 45%), resulting in combined LLM pretraining acceleration of up to \(6\) (time to reach certain accuracy compared to baseline).
* Through careful experimentation, we study the importance of sequence length distribution and mixture during pretraining for different natural language and long-context tasks. We show the effect of concatenation and chunking operations to synthetically alter sequence length (Section 3.2).

## 2 Method

### Dataset decomposition

Given a dataset \(\) of tokenized documents \(\{d_{1},d_{2},,d_{n}\}\), the goal of dataset decomposition (DD) is to reorganize \(\) as a union of buckets, \(_{i}_{i}\), such that: (1) each bucket \(_{i}\) consists of sequences of tokens with length \(l_{i}\); (2) each sequence \(s_{i}\) is a subsequence of one document \(d\); and (3) each token in \(\) appears in exactly one \(_{i}\). This decomposition produces sequences that each belong to a unique document, ensuring no cross-document attention within a sequence during training. Additionally, all sequences in a given bucket \(_{i}\) have the same length \(l_{i}\), enabling efficient batching.

Dataset decomposition as defined above is not unique. We propose a specific decomposition, with \(l_{i}=2^{i}\), to optimally maintain the original document sequence length distribution while also enabling efficient batch pretraining, as explained in Section 2.2. We apply decomposition at the document level, which makes it very easy to integrate the method into any existing data preparation pipeline (a stage before model training) and is scalable to large datasets. For a tokenized document \(d\) with length \(l\), where \(l=2^{i_{1}}+2^{i_{2}}++2^{i_{k}}\) represents its binary decomposition, we break \(d\) into \(k\) adjacent sequences \(s_{1},,s_{k}\), with lengths of \(2^{i_{1}},,2^{i_{k}}\), respectively. Each sequence \(s_{j}\) of length \(2^{i_{j}}\) is then assigned to bucket \(_{i_{j}}\). Fig. 2 shows a schematic representation of this method.

With our proposed dataset decomposition approach, each bucket \(_{i}\) contains sequences extracted from an original document \(d\) such that the length of \(d\) is at least \(2^{i}\). In Fig. 2(b), we show the distribution

Figure 2: Each cell in the figure represents a token. **Left:** Original documents with variable lengths. **Middle:** Concat-and-chunk baseline to form sequences with a fixed target length (here \(=4\)). **Right:** Dataset decomposition method with \(_{1}\), \(_{2}\), and \(_{3}\) buckets.

of RefinedWeb dataset tokens over different buckets, where \(_{9}\) (corresponding to sequences with length 512) has the maximum tokens. We also highlight the original document lengths from which tokens are extracted. Most tokens in a bucket \(_{i}\) are extracted from documents with length \(l\) such that \(2^{i} l<2^{i+1}\), and some tokens are rolled over from documents with length \(l 2^{i+1}\). This demonstrates the efficacy of the method in retaining original document length, especially for long documents, which are scarce.

In Fig. 2(a), we show the distribution of original document lengths and chunks within 2048 and 8192 target sequence lengths formed by the concat-and-chunk approach. We also present the length distribution using the bin-packing approximate algorithm introduced by a concurrent work . Additionally, in Fig. 2(c), we show the distribution of context length (the number of tokens from the same document a token can attend to during pretraining) when using baselines with a target sequence length of 8192 and DD. See Appendix F for additional discussion on sequence length statistics.

In contrast to the concat-and-chunk approach, which results in a static dataset, DD enables us to use sequence length distribution as prior knowledge and optimize the best mixture for the target task. In Section 3.2, we show the bias of each target evaluation toward a sequence length and the effect of concatenation and chunking on model performance. In Section 3.3, we study the effect of different sequence mixtures for LLM pretraining, a less-studied topic in LLM pretraining.

### Variable sequence length training

Following the setup in Section 2.1, we assume a set of \(k\) buckets such that \(_{i}\), containing sequences with length \(2^{i}\), are available. Let \(b\) be the target batch size - the number of tokens used per optimization step. In variable sequence length (VSL) training, at every step of optimization, we first sample \(i\) from available choices, then pick \(b/2^{i}\) sequences from bucket \(_{i}\). Since \(_{i}\) consists of sequences with length \(2^{i}\), the number of seen tokens per optimization step remains \(b\), independent of the choice of \(i\). Training LLMs with the VSL algorithm comes with several advantages.

First, since the total number of seen tokens per optimization step does not change, VSL does not alter optimization dynamics, and the same hyperparameters as the baseline can be utilized (see Section 3).

Second, in Section 3.1, we show that the time to complete one optimization step (forward+backward) for a fixed \(b\) (tokens per step) varies by sequence length due to the quadratic cost of attention . With VSL training, the cost of every optimization step depends on the bucket \(_{i}\) sampled for that step (and hence the sequence length). Thus, the more expensive steps (corresponding to long sequences) are compensated with less expensive steps (corresponding to short sequences).

Figure 3: For the RefinedWeb dataset : **(a)** Distribution of chunk lengths using different dataset preparation methods. Peaks show the percentage of chunks for each method with the same length as the target sequence length. **(b)** Distribution of tokens over \(_{i}\)’s in DD. Color/pattern shows the \(_{2}l\), where \(l\) is the length of the original document each token is extracted from. **(c)** Probability distribution of context length (number of tokens from the same document a token can attend to) observed during training for the concat-and-chunk baseline with target sequence length \(8192\) and DD with \( 256\) mixture defined in Table 1.

Finally, the sampling component in VSL (which \(_{i}\) to choose at every optimization step) enables different curricula of sequence lengths. In Section 3.4, we show the significance of such curricula on model stability and generalization accuracy.

## 3 Experiments and analysis

In this section, we show the efficacy of the proposed method to train LLMs of different sizes on large-scale datasets and provide additional analyses. For all experiments, except the results in Section 3.5, we use RefinedWeb  filtering of Common Crawl  with a total of \( 525\) billion tokens using the EleutherAI/gpt-neox  tokenizer (vocabulary size is 50,432). Model architectures and training code are based on the OpenLM +. For all experiments, other than model scaling in Section 3.5, we use the OpenLM-1B model with an 8k context length. Please refer to Appendix B for implementation details of all experiments.

Footnote †: https://github.com/mlfoundations/open_lm

Positional encodingWe use Rotary Positional Embedding (RoPE)  to encode positions in queries and keys before the attention module. RoPE rotates the consecutive components of queries and keys with a base frequency \(f_{b}=10,000\). Recent studies  have suggested increasing \(f_{b}\) to better adapt a pretrained model for longer sequences through fine-tuning. We find that using a larger \(f_{b}\) is also beneficial when training LLMs from scratch. In Table 4, we show that increasing \(f_{b}\) to 100,000 improves performance for both the baseline and DD methods.

EvaluationWe evaluate each model on a comprehensive set of standard benchmarks, mainly using LLM Foundry . We report averaged accuracies over each category, as well as the _regular average_, which is the average of 14 regular language modeling benchmarks detailed below:

* **Commonsense Reasoning (CSR)**: PIQA-0-shot , COPA-0-shot , and OpenBookQA-10-shots .
* **Language Understanding (LU)**: Lambada-OpenAI , Hellaswag-0-shot , Winograd-3-shots , and Wino Grande-5-shots .
* **Reading Comprehension (RC)**: SQuAD-3-shots , BoolQ-0-shot , and CoQA-0-shot .
* **World Knowledge (WK)**: Jeopardy-3-shots , ArcEasy-3-shots , ArcChallenge-3-shots , and WikiDataQA-3-shots 

To evaluate model on longer context tasks, we adopt the following real-world benchmarks:

* **Multi-Document Question Answering (MDQA)**: We follow the exact setup as in Liu et al. , where for each question from NaturalQuestions-Open , \(r\) Wikipedia documents are retrieved such that one of them has the answer to the question, and the other \(r-1\) documents are distractors. We report MDQA-10, MDQA-20, and MDQA-30 accuracy corresponding to \(r=10,20\), and 30,

Figure 4: **(a) Average time for one optimization step (\(b=8 8192\) tokens) on an 8\(\)H100 node with FSDP and FlashAttention2 for different context lengths. (b) OpenLM-1B/3B/7B models trained on 137B tokens. Accuracy and training speed gains are shown.**

respectively. For each query, we evaluate the model by changing the location of the target document among distractors and report the averaged accuracy.

* **TOEFL**: This dataset is a multiple-choice question answering dataset from An et al. . The dataset contains QA pairs for 15 longest lectures in Tseng et al. , Chung et al. . Only one of the choices is the correct response. We estimate the correct choice by picking the choice with the lowest mean log probability value.
* **QuALITY**: This dataset is a multiple-choice question answering dataset from An et al. . The dataset contains a long passage for context, followed by a question with multiple choices. Only one of the choices is the correct response. We estimate the correct choice by picking the choice with the lowest mean log probability value.

### Training efficiency

We first verify that VSL training enables a higher throughput than the baseline concat-and-chunk method. We enumerate model sizes (OpenLM-1B/3B/7B) and different context lengths (\(2^{6}\) to \(2^{13}\)) and measure the time to train 100 batches with a fixed global batchsize of \(b=8 8192\) distributed over 8 GPUs in a single node. We repeat this 5 times and report the average time per optimization step in Fig. 3(a) (with STD mostly \(<1\)ms). See Appendix C.1 for additional results with different batchsizes \(b\). For each model, we highlight the training time overhead (due to attention's quadratic complexity with an optimized FlashAttention2 kernel ) when training with 8192 context lengths compared to 64 context lengths: +35%, +88%, and +23% for OpenLM-1B, -3B4, and -7B, respectively. Training overhead grows for longer context lengths (see Fig. 7 for results up to 16k context length).

The concat-and-chunk baseline method always operates at a fixed sequence length. For example, for the OpenLM-1B model, an optimization step with concat-and-chunk takes 243ms and 304ms for target context lengths of 2048 and 8192, respectively. The expected time for VSL, on the other hand, is the weighted average over different sequence lengths depending on the mixture. In Table 1, we report the training step time for different mixtures. For example, with the natural length distribution resulting from DD (Fig. 2(b)), training up to length 8192 sequences takes a similar time (244ms) as baseline training with length 2048 (with 243ms per step) per step--equivalent to a 20% training time reduction compared to baseline training with a fixed length of 8192 (with 304ms per step).

### Sequence length bias

In this section, we study the effect of pretraining data sequence length on model performance in isolation. Using a single bucket \(_{i}\) as the dataset, we train an LLM from scratch on sequences with length \(2^{i}\) for a total of \(2^{34}\) seen tokens. Note that the number of tokens per optimization step is fixed at 256, irrespective of sequence length. We use the same training hyperparameters for all runs. In Appendix C.2, we show that our conclusions do not depend on the choice of hyperparameters. To

Figure 5: **(a)** Performance of OpenLM-1B model trained on \(2^{34}\) tokens from buckets with different sequence lengths. **(b)** distribution of lengths of documents for different benchmarks. **(c)** Effect of chunking (\(_{13 10}\)) and concatenating (\(_{7 13}\)) sequences during pretraining on model performance.

reduce statistical error, we train each model twice from scratch with different random seeds and report the average metric for each benchmark (observing an average standard deviation of \( 0.3\) for regular benchmarks and \( 1.6\) for multi-document QA). Results are demonstrated in Fig. 4(a).

We show a significant correlation between pretraining sequence length and different benchmarks. Specifically, the accuracy of commonsense reasoning, language understanding, and world knowledge shows an inverted U-shape behavior with respect to pretraining sequence length, while reading comprehension benefits from longer sequences. This behavior can be associated with training-test distribution alignment with respect to sequence length. In Fig. 4(b), we show the length distribution for different benchmarks where RC demonstrates a heavier tail compared to CSR, LU, and WK. Multi-document QA benchmarks show a vivid correlation with respect to sequence length: test accuracy is \( 0\) unless pretraining sequence length is greater than the test context length, which is \(\) 2k, 4k, and 6k for MDQA-10, -20, and -30, respectively.

It could be argued that data selection based on sequence lengths could introduce bias since the content (or source) of the documents might change based on the sequence lengths. To better understand the effect of sequence length on common metrics, we created two new buckets, \(_{13 10}\) and \(_{7 10}\), from existing buckets \(_{13}\) and \(_{7}\), respectively. The bucket \(_{13 10}\) contains sequences of length \(2^{10}\) created by _chunking_ sequences from \(_{13}\) into 8 subsequences and then performing a global shuffle. The bucket \(_{7 10}\) also includes sequences of length \(2^{10}\), each formed by _concatenating_ 8 random sequences from \(_{7}\).

In Fig. 4(c), we compare the regular average metric of models pretrained on these buckets; for each bucket, we train two models from scratch using different random seeds and report the averaged results. \(_{13 10}\) gains 2.6 points compared to \(_{13}\) while including the same content. This demonstrates the pure effect of sequence length on model accuracy. Furthermore, training on \(_{13 10}\) underperforms \(_{10}\) by 0.9 points, even though they are of the same length, indicating that long documents (used to construct \(_{13 10}\)) correlate less with our benchmarks than short documents (used to construct \(_{10}\)). Finally, we show that _concatenation_, as opposed to _chunking_, does not mitigate length correlation. This is evident from the fact that \(_{7 10}\) scores the same as \(_{7}\) and still significantly worse than \(_{10}\).

Our analysis suggests that effective base model pretraining requires a mixture of different sequence lengths to perform well on all benchmarks. Next, we systematically study the effect of dataset mixture from the sequence length perspective.

### Data mixture

A key benefit of dataset decomposition is access to and control over sequence length distribution. We form datasets with different mixtures of sequence lengths and explore the performance of a model trained on each mixture. Table 1 shows the results. For all experiments, the total seen tokens and hyperparameters are fixed, and only the distribution over sequence length is changed. First, we observe that mixtures with small average context length (we provide the exact definition in Appendix F) perform poorly on MDQA, which requires long context understanding. For example, as for "1k-only", "\(\)2k", and "Mid" distributions that do not include long sequences from \(_{12}\) and \(_{13}\). Larger average context length (e.g., as in "\(\)1k") also correlates positively with performance on

    & \)**} &  &  &  \\  & \(_{b}\) & \(_{7}\) & \(_{8}\) & \(_{9}\) & \(_{10}\) & \(_{11}\) & \(_{12}\) & \(_{13}\) & **Len** & **Len** & **(ms)** & **CSR** & **LU** & **RC** & **WK** & **Avg.** & **10** & **20** & **30** & **Avg.** \\  Natural & 3 & 6 & 10 & 17 & 21 & 17 & 13 & 9 & 482 & 1018 & 244 & 62.4 & 65.4 & 43.8 & 43.9 & **54.0** & **26.7** & 20.7 & **18.5** & **23.9** \\  Equal & 12 & 12 & 12 & 12 & 12 & 12 & 12 & 12 & 12 & 257 & 1020 & 244 & 61.9 & 64.3 & 43.1 & 43.5 & 53.3 & 25.1 & 21.4 & 17.4 & 21.3 \\ 
1k-only & 0 & 0 & 0 & 0 & 96 & 0 & 0 & 0 & 1024 & 512 & 234 & 60.8 & **66.4** & 43.2 & **44.7** & **54.0** & 0.2 & 0.1 & 0.2 & 0.2 \\  \(\)2k & 16 & 16 & 16 & 16 & 16 & 16 & 0 & 195 & 336 & 231 & **62.8** & 63.7 & 41.8 & 43.5 & 53.1 & 25.5 & 0.4 & 0.4 & 8.1 \\ 
2\(\)256 & 0 & 0 & 16 & 16 & 16 & 16 & 16 & 16 & 780 & 1344 & 250 & 61.5 & 65.6 & 43.4 & 44.1 & 53.8 & 25.0 & 18.4 & 17.2 & 20.1 \\  Mid & 0 & 0 & 24 & 24 & 24 & 24 & 0 & 0 & 546 & 480 & 233 & 61.9 & 65.5 & 42.5 & 43.8 & 53.6 & 19.1 & 0.0 & 0.1 & 6.4 \\  \(\)1k & 0 & 0 & 0 & 0 & 24 & 24 & 24 & 24 & 24 & 2185 & 1920 & 263 & 61.9 & 65.0 & **45.8** & 43.3 & **54.0** & **26.7** & **21.6** & 18.1 & **22.1** \\   

Table 1: Effect of pretraining **dataset mixture** on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of \(96 2^{30}\) tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.

reading comprehension tasks, consistent with our observation in Fig. 4(a), but comes at the cost of a longer training step time.

Furthermore, "1k-only", that is training using only the best sequence length (\(=1024\)) from the study in Section 3.2 results in good performance on regular evaluations, especially for language understanding and world knowledge tasks, but is poor for long context tasks. Finally, we observe that "natural" mixture, that is aligned with the distribution resulting from dataset decomposition (see Fig. 2(b)), obtains near-optimal performance on both regular and MDQA tasks, demonstrating the scalability of the proposed approach to large datasets without a need for intervention on the natural underlying length distribution.

### Length-based curriculum

We can think of short sequences as being "easier" compared to longer ones; hence motivating a curriculum learning [7; 18] that prioritizes short sequences. A similar idea (training with image resolutions from low to high) is explored in vision to train CLIP  models more efficiently . In VSL, we can easily implement curriculum learning through sampling designs. At every optimization step, we sample _without replacement_ a batch with \(b\) tokens from bucket \(_{i}\) with probability \(p_{i}\). If a bucket is empty, we exclude it from sampling. We study different curricula for the "\( 256\)" mixture (with an equal number of tokens in \(_{8},,_{13}\)). Results are shown in Table 2. For each curriculum, we determine the odds of picking a batch from each bucket (\(=p_{i}\)'s when normalized). Details of our length-based sampling and curriculum are provided in Algorithm 1. We consider curricula that shift from short to long sequences at different paces controlled by \(p_{i}\)'s changing linearly, with powers of 2, and with powers of 100 between buckets.

Due to the presence of other hyperparameter schedules during the course of training (e.g., learning rate and weight decay), a curriculum on length may result in a potential implicit bias. For example, if we only see long sequences toward the end of training, long sequence learning occurs only when the learning rate is too small. To address this potential issue, we also explore cyclic curricula, where a curriculum is applied in cycles similar to cyclic learning rate schedules  as shown in Fig. 6. Note that when we train on a sequence of length \(l\), we have \(l\) next-token prediction losses (applied in parallel) with context lengths \(0,1,,l-1\). This already implies some mixing: when training on a "hard" example (i.e., a long sequence), we also include "easy" examples (its shorter sub-sequences). Therefore, even towards the end of each cycle, we still have some losses with short contexts.

Our results show that the cyclic "Grow-P2" curriculum is near optimal with different metrics. An additional benefit of curriculum is training stability. Li et al.  noticed that long sequences contribute to extreme gradient variance, especially at the beginning of training, resulting in instability. We also observe (see Appendix E) that our proposed approach with curriculum results in more stable training dynamics, thus enabling more efficient training with larger batch sizes and learning rates.

### Scaling

Dataset scalingIn Fig. 0(a), we show the performance of models trained with \(2^{34},2^{35},2^{36},2^{37},\) and \(2^{38}\) total tokens using DD and baseline. We use the "\( 256\)" mixture and "Grow-Linear" curriculum with 8 cycles for DD, and a fixed target sequence length 8192 for the baseline. Results show \(>2\) data efficiency: our proposed method reaches the same accuracy as the baseline using less than half the tokens.

    &  &  &  &  \\  & \(_{8}\) & \(_{9}\) & \(_{10}\) & \(_{11}\) & \(_{12}\) & \(_{13}\) & **Cycles** & **CSR** & **LU** & **RC** & **WK** & **Avg.** & **10** & **20** & **30** & **Avg.** \\  Uniform & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 62. & 65. & 43. & 44.0 & 53.8 & 27.3 & 22.0 & 19.6 & 23.0 \\   & 6 & 5 & 4 & 3 & 2 & 1 & 1 & 60.9 & 64.2 & 46.6 & 42.9 & 53.6 & 30.9 & 26.0 & 23.9 & 26.9 \\  & & & & & & 8 & 62.7 & 65.0 & 45.4 & **44.7** & 54.5 & 30.1 & 25.3 & 22.8 & 26.1 \\   & 32 & 16 & 8 & 4 & 2 & 1 & 1 & 60.9 & 64.3 & 46.5 & 44.1 & 54.0 & 29.6 & 25.0 & 23.1 & 25.9 \\  & & & & & & & 8 & 62.8 & 65.2 & 45.3 & 44.2 & 54.4 & **32.3** & **26.9** & **24.6** & **28.0** \\   & \)} & \)} & \)} & \)} & \)} & \)} &  & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & & & & & & & & \\  Shrink-P100 & 1 & 100 & \(100^{2}\) & \(100^{3}\) & \(100^{4}\) & \(100^{5}\) & 1 & 60.0 & 62.2 & 37.6 & 40.7 & 50.3 & 24.5 & 18.7 & 15.6 & 19.6 \\   

Table 2: Effect of **length-based curriculum**. All models are OpenLM-1B and have seen a total of \(96 2^{30}\) tokens, with exactly \(2^{34}\) tokens from each \(_{i}\) for \(i=8,,13\). We use RoPE with a base frequency of 100k and the same default hyperparameters.

Model scalingWe report results on OpenLM-1B, -3B, and -7B trained from scratch for a total of \(2^{37}\) tokens in Fig. 3(b). We compare baseline training with a fixed target sequence length 8192 and VSL training with a \(DD_{ 256}\) mixture and the "Grow-Linear" curriculum with 8 cycles. Training with DD results in significant accuracy gains and reductions in training wall-clock time at different scales.

Alternative datasetWe demonstrate the efficacy of our proposed method on another large-scale dataset, DataComp-LM . We train models with different numbers of parameters: OpenLM-160M, -410M, and -1B, for a total of 137B tokens. We compare the baseline with a \(DD_{ 256}\) mixture trained with the "Grow-P2" curriculum with 8 cycles. Results are reported in Table 3, demonstrating significant accuracy and training efficiency gains.

### Comparison with state-of-the-art

We compare our proposed method, data decomposition, with other approaches for handling various document lengths of pretraining data, including document masking (DM), best-fit sequence packing , and in-context pretraining (ICLM) . We describe the details of our implementation of the best-fit packing in Appendix D. For ICLM, we use the official implementation8 applied to the RefinedWeb dataset. The results are shown in Table 5.

Pre-training context length is an important factor in determining a model's long-context performance. We empirically validate this in the results shown in Fig. 4(a), where models trained on longer sequences perform better on multi-document QA. Our proposed method has an average context length (as defined in Eq. (2)) of 1,344 for the RefinedWeb dataset, compared to 930 for the baseline (see Fig. 4(c)) and 1,064 when packing  is applied. This explains why the dataset decomposition mixture, even without any length-based curriculum (the first row in Table 2), outperforms Baseline-8k-DM and Pack-8k+DM (second and third rows in Table 5). Here, DM refers to applying document masking during training to avoid cross-document attention.

Document masking improves the baseline on regular evaluations from \(51.5\) to \(52.4\) by preventing cross-document attention. However, Xiong et al.  demonstrate that including concatenated unrelated documents can still enhance long-context metrics compared to training solely with shorter sequences. Therefore, DM experiences a slight decline in long-context evaluations, dropping from \(27.5\) to \(27.1\). Baseline-8k multi-document QA performance is even slightly better than our proposed dataset decomposition mixture when used _without_ length-based curriculum (the first row in Table 2).

In-context pre-training LMs (ICLM)  proposes document sorting based on content similarity. Although the benefits of ICLM with large-scale Common Crawl data (used in our experiments) are marginal in regular evaluation, we observe that ICLM results in slightly better multi-document QA performance than Baseline-8k when 30 documents are in the context compared with Baseline-8k (22.0% vs. 20.5%). The average long-context metric boosts from \(27.5\) for Baseline-8k to \(28.7\) for ICLM. However, the similarity finding step proposed by ICLM is resource-intensive at scale9.

Finally, as shown in in Table 2 our proposed cyclic length-based curriculum, for example, Grow-P2 with 8 cycles, results in a significant improvement in the model's long-context capability. Our proposed method avoids cross-document attention to unrelated content, maintains coherent long sequences, and benefits from a length-based curriculum, effectively improving performance in both regular and long-context evaluations compared to all baselines. We further summarize long-context performance of different methods discussed above in Table 6.

    & \)} & **Regular** & **MDQA** \\  & & **Avg.** & **Avg.** \\   & 10k & 51.3 & 19.0 \\  & & 100k & 51.5 & **24.4** \\  _{ 26}\)} & 10k & 53.8 & 20.1 \\  & 100k & 53.8 & **24.9** \\   

Table 4: Effect of RoPE base frequency, \(f_{b}\), in pretraining.

    **Model Size** \\  } &  **Method** \\  } & **Num** & **Time** &  & **Regular** &  & **MDQA** &  \\  & & **Gröt** & **(hours)** & & & & \\   & Baseline-8k & 16 & 18.3 & - & 39.3 & - & - \\  & DD & **15.7** & -14.4 & **40.0** & **+0.7** & **11.4** & **+1.7** \\   & Baseline-8k & 16 & 38.9 & - & 48.3 & - & 14.8 & - \\  & DD & 16 & **29.6** & -24.4 & **+9.4** & **+1.1** & **18.8** & **+4.0** \\   & Baseline-8k & 44.4 & - & 56.7 & - & 25.6 & - \\  & DD & 32 & **35.4** & -20.9 & **58.4** & **+1.7** & 25.6 & - \\   

Table 3: Comparing baseline training with DD on an alternative pretraining dataset and model sizes.

## 4 Related works

Recent works have raised concerns regarding cross-document attention. For example, LLM , ICLM , and , which we discussed in Section 3.6. Similarly,  discuss challenges with the baseline concat-and-chunk approach and propose an approximate bin-packing algorithm.

Related to our study on sequence length bias,  shows the importance of train-vs-test time distribution shift from a sequence length perspective on a string editing task. [66; 25; 36; 6] highlight the challenge of generalizing to lengths beyond what the model has seen during training and discuss the importance of positional encoding. Several works [41; 67; 23; 64; 10; 47; 48; 53; 34] address enabling LLM inference with long context (see  for an overview). These approaches are orthogonal to our contribution and can be applied post-pretraining to adapt to longer lengths. GrowLength  proposes accelerating LLM pretraining by progressively growing context length using the baseline sequence formation method, but does not show results on LLMs. Similarly, increasing sequence length has been shown in BERT model training  to improve compute efficiency.

The idea of dynamic batching has been explored in other domains. In vision, methods like NaViT [16; 38] use images with variable resolutions (a similar concept to context length for LLMs). In seq-to-seq tasks (e.g., automatic speech recognition, text-to-speech, and neural machine translation), the inputs have different lengths. An efficient approach is to sort inputs by their length and form batches of inputs with similar lengths during training (after possible padding). Batchsize is dynamically adjusted inversely proportional to input lengths [20; 21]. Different from these works, in dataset decomposition, we do not simply put documents with similar lengths into the same bucket. Instead, we decompose each document into multiple subsequences and form multiple buckets. We form batches with different lengths during training by sampling from these buckets using a target mixture and curriculum.

## 5 Conclusion and limitations

In this paper, we explore the shortcomings of a popular LLM pretraining approach, concat-and-chunk, and introduce dataset decomposition, a method to decompose a dataset of text documents into buckets containing fixed sequence lengths. We show results of variable sequence training using DD with different mixtures, curricula, datasets, and models, demonstrating significant LLM pretraining speedup and a final model accuracy boost on a wide range of benchmarks. Furthermore, we provide analysis on sequence length bias and attention masking. We compare our proposed method with recent works that also address concat-and-chunk shortcomings in a unified experimental setup and show gains in data preparation cost, training time, and final model accuracy.

**Limitations.** The training speed gains compared to the baseline are significant only when the target sequence length is long enough. Otherwise, the attention cost is not a dominant fraction of training, and hence no significant training speedup is expected.

    & **Dec** & **Average** & **Doss in a** & **Curr.** & **MDQA-** \\  & **Masking** & **Context** & **Sequence** & **30 (\%)** \\  Baseline & ✓ & 930 & Multi-cancon & ✗ & 16.0 \\  Pock- & ✓ & 1064 & Multi-packing & ✗ & 16.9 \\  DD-Uniform & ✗ & 1344 & Single & ✗ & 19.6 \\  Baseline & ✗ & 4096 & Multi-random & ✗ & 20.5 \\  ICLM  & ✗ & 4096 & Multi-sernantic & ✗ & 22.0 \\ 
**DD-Group-P2** & N/A & 1344 & Single & ✓ & **24.6** \\   

Table 6: Summary of long-context performance for different methods from Table 2 and Table 5.

    &  &  &  &  &  \\  & & & & & &  & & & & **Time** & **Pre.** \\  & & & & & **10** & **20** & **30** & & & & **(ms)** & **Cost** \\  Baseline-Sk & 60.6 & 62.5 & 41.5 & 41.3 & 51.5 & 29.0 & 23.8 & 20.5 & 26.2 & 32.0 & 27.5 & 304 & 5 \\ Baseline-Sk+DM & 60.2 & 64.1 & 42.8 & 41.8 & 52.4 & 24.4 & 20.0 & 16.0 & 29.2 & 32.0 & 27.1 & 304 & S \\ Pack-Sk+DM  & 60.3 & 64.0 & 44.6 & 41.8 & 52.7 & 25.6 & 19.8 & 16.9 & 29.2 & 33.1 & 27.7 & 304 & S8 \\ ICLM  & 60.6 & 62.1 & 44.7 & 40.0 & 51.7 & 26.7 & 20.0 & 22.8 & **34.6** & 28.7 & 304 & 55S \\
**DD (ours)** & **62.8** & **65.2** & **45.3** & **44.2** & **54.4** & **32.3** & **26.9** & **24.6** & **30.7** & 34.2 & **30.9** & **244** & S \\   

Table 5: Comparison with baseline and state-of-the-art methods. All models are trained with the same hyperparameters, RoPE with \(f_{b}=100k\), and for \(103\)B tokens. DM denotes training with document masking. DD uses the “Grow-P2” curriculum with 8 cycles. Dataset preparation cost is symbolic to compare methods and does not reflect the wall-clock time.