# BENO: Boundary-embedded Neural

Operators for Elliptic PDEs

 Haixin Wang

Peking University

wang.hx@stu.pku.edu.cn

&Jiaxin Li

Westlake University

lijiaxin@westlake.edu.cn

&Anubhav Dwivedi

Stanford University

anubhavd@stanford.edu

&Kentaro Hara

Stanford University

kenhara@stanford.edu

&Tailin Wu

Westlake University

wuatilin@westlake.edu.cn

Equal contribution. Work done as an intern at Westlake University. Corresponding author.

###### Abstract

Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically neglect complex geometries and inhomogeneous boundary values present in the real world. Here we introduce \(}\)oundary-\(}\)nbedded \(\)Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green's function, BENO consists of two Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model and strong baselines extensively in elliptic PDEs with complex boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%.

## 1 Introduction

Partial differential equations (PDEs), which include elliptic, parabolic, and hyperbolic types, play a fundamental role in diverse fields across science and engineering. For all types of PDEs, but especially for elliptic PDEs, the treatment of boundary conditions plays an important role in the solutions. Elliptic PDEs are one of the three types of PDEs, whose solutions describe steady-state phenomena under interior source and boundary conditions. In particular, the Laplace and Poisson equations constitute prime examples of linear elliptic PDEs, which are used in a wide range of disciplines, including solid mechanics (Riviere, 2008), plasma physics (Chen, 2016), and fluid dynamics (Hirsch, 2007).

Recently, neural operators have emerged as a promising tool for solving elliptic PDEs by directly mapping input to solutions (Li et al., 2020, 2020, 2020, 2020). Lowering the computation efforts makes neural operators more attractive compared with classical approaches like finite element methods (FEM) (Quarteroni and Valli, 2008) and finite difference methods (FDM) (Dimov et al., 2015). However, existing neural operators have not essentially considered the influence of boundaryconditions on solving elliptic PDEs. A distinctive feature of elliptic PDEs is their sensitivity to boundary conditions, which can heavily influence the behavior of solutions.

In fact, boundary conditions pose two major challenges for neural operators in terms of inhomogeneous boundary values and complex boundary geometry. **First**, inhomogeneous boundary conditions can cause severe fluctuations in the solution, and have a distinctive influence on the solution compared to the interior source terms. For example, as shown in Fig. 1, the inhomogeneous boundary values cause high-frequency fluctuations in the solution especially near the boundary, which makes it extremely hard to learn. **Second**, since elliptic PDEs are boundary value problems whose solution describes the steady-state of the system, any variation in the boundary geometry and values would influence the interior solution _globally_(Hirsch, 2007). The above challenges need to be properly addressed to develop a neural operator suitable for more general and realistic settings.

In this paper, we propose **B**oundary-**E**mbedded **N**eural **O**perators (BENO), a novel neural operator architecture to address the above two key challenges. Inspired by classical Green's function, BENO consists of two Graph Neural Networks (GNNs) that model the boundary influence and the interior source terms, respectively, addressing the first challenge. Moreover, to model the global influence of the boundary to the solution, we employ a Transformer (Vaswani et al., 2017) to encode the full boundary information to a latent vector and feed it to each message passing layer of the GNNs. This captures how the global geometry and values of the boundary influence the pairwise interaction between interior points, addressing the second challenge. As a whole, BENO provides a simple architecture for solving elliptic PDEs with complex boundary conditions, incorporating physics intuition into its boundary-embedded architecture. In Table 1, we provide a comparison between BENO and prior deep learning methods for elliptic PDE solving.

To fully validate the effectiveness of our model on inhomogeneous boundary value problems, we construct a novel dataset encompassing various boundary shapes, different boundary values, different types of boundary conditions, and varying resolutions. The experimental results demonstrate that our approach not only outperforms the existing state-of-the-art methods by about an average of 60.96% in solving elliptic PDEs problems but also exhibits excellent generalization capabilities in other scenarios. In contrast, all existing baselines fail to learn solution operators for the above challenging elliptic PDEs.

    & 1. PDE-agnostic & 2. Train/Test space & 3. Evaluation at & 4. Free-form spatial & 5. Inhomogeneous \\  & prediction on re- & & grid independence & unobserved sp- & domain for boundary & boundary condition \\  & w initial condition & & axial locations & shape & value & value \\  GKN (Li et al., 2020) & ✓ & ✓ & ✓ & ✓ & ✗ \\ FNO (Li et al., 2020) & ✓ & ✗ & ✓ & ✗ & ✗ \\ GNN-PDE (Lötzsch et al., 2022) & ✓ & ✓ & ✗ & ✓ & ✗ \\ MP-PDE (Brandstetter et al., 2022) & ✓ & ✗ & ✗ & ✗ & ✗ \\ 
**BENO (ours)** & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of data-driven methods to time-independent elliptic PDE solving.

Figure 1: Examples of different geometries for the elliptic PDEs: (a) forcing terms and (b) solutions. The nodes in red-orange color-map represent the complex, inhomogeneous boundary values. The redder the area, the higher the boundary value it represents, whereas the more orange the area, the lower the boundary value.

Problem Setup

In this work, we consider the solution of elliptic PDEs in a compact domain subject to inhomogeneous boundary conditions along the domain boundary. Let \(u C^{d}()\) be a d-dimnesion-differentiable function of \(N\) interior grid nodes over an open domain \(\). Specifically, we consider the Poisson equation with Dirichlet (and Neumann in Appendix M) boundary conditions in a d-dimensional domain, and we consider \(d=2\) in the following experiments:

\[^{2}u([x_{1},x_{2},,x_{d}])& =\ f([x_{1},x_{2},,x_{d}]),\ ([x_{1},x_{2},,x_{d}]),\\ u([x_{1},x_{2},,x_{d}])&=\ g([ x_{1},x_{2},,x_{d}]),([x_{1},x_{2},,x_{d}] ),\] (1)

where \(f\) and \(g\) are sufficiently smooth function defined on the domain \(=\{(x_{1,i},x_{2,i},,x_{d,i})\}_{i=1}^{N}\), and boundary \(\), respectively. Eq. 1 is utilized in a range of applications in science and engineering to describe the equilibrium state, given by \(f\) in the presence of time-independent boundary constraints specified by \(g\). A distinctive feature of elliptic PDEs is their sensitivity to boundary values \(g\) and shape \(\), which can heavily influence the behavior of their solutions. Appropriate boundary conditions must often be carefully prescribed to ensure well-posedness of elliptic boundary value problems.

## 3 Method

In this section, we detail our method BENO. We first motivate our method using Green's function, a classical approach to solving elliptic boundary value problems in Section 3.1. We then introduce our graph construction method in Section 3.2. Inspired by the Green's function, we introduce BENO's architecture in Section 3.3.

### Motivation

**How to facilitate boundary-interior interaction?** To design the boundary-embedded message passing neural network, we draw inspiration from the traditional Green's function (Stakgold & Holst, 2011) method which is based on a numerical solution. Take the Poisson equation with Dirichlet boundary conditions for example. Suppose the Green's function is \(G:\), which is the solution of the corresponding equation as follows:

\[&^{2}G=(x-x_{0})(y-y_{0})\\ &G|_{}=0\] (2)

Based on the aforementioned equations and the detailed representation of the Green's function formula in the Appendix B, we can derive the solution in the following form:

\[u(x,y)=_{}G(x,y,x_{0},y_{0})f(x_{0},y_{0})d_{0}-_{ }g(x_{0},y_{0}),y_{0})}{ n_{0 }}dl_{0}\] (3)

Motivated by the two terms presented in Eq. 3, our objective is to approach boundary embedding by extending the Green's function. Following the mainstream work of utilizing GNNs as surrogate models (Pfaff et al., 2020; Eliasof et al., 2021; Lotzsch et al., 2022), we exploit the graph network simulator (Sanchez-Gonzalez et al., 2020) as the backbone to mimic the Green's function, and add the boundary embedding to the node update in the message passing. Besides, in order to decouple the learning of the boundary and interior, we adopt a dual-branch network structure, where one branch sets the boundary value \(g\) to 0 to only learn the structural information of interior nodes, and the other branch sets the source term \(f\) of interior nodes to 0 to only learn the structural information of the boundary. The Poisson equation solving can then be disentangled into two parts:

\[^{2}u(x,y)=f(x,y)\\ u(x,y)=g(x,y) ^{2}u(x,y)\ =\ f(x,y)\\ u(x,y)\ =\ 0\\ }_{}+ ^{2}u(x,y)\ =\ 0\\ u(x,y)\ =\ g(x,y)}_{}\] (4)

Therefore, our BENO will use a dual-branch design to build two different types of edges on the same graph separately. Branch 1 considers the effects of interior nodes and Branch 2 focuses solely on how to propagate the relationship between boundary values and interior nodes in the graph. Finally, we aggregate them together to obtain a more accurate solution under complex boundary conditions.

**How to embed boundary?** Since boundary conditions are crucially important for solving PDEs, how to better embed the boundary information into the neural network is key to our design. During a pilot study, we found that directly concatenating the interior node information with boundary information fails to solve for elliptic PDEs, and tends to cause severe over-fitting. Therefore, we propose to embed the boundary to represent its global information for further fusion. In recent years, Transformer (Vaswani et al., 2017) has been widely adopted due to its global receptive field. By leveraging its attention mechanism, the Transformer can effectively capture long-range dependencies and interactions within the boundary nodes. This is particularly advantageous when dealing with complex boundary conditions (i.e., irregular shape and inhomogeneous boundary values), as it allows for the modeling of complex relationships between boundary points and the interior solution.

### Graph Construction

Before designing our method, it is an important step to construct graph \(=\{(,)\}\) with the finite discrete interior nodes as node set \(\) on the PDE's solution domain \(\). In traditional solution methods such as FEM, the solution domain is initially constructed by triangulating the mesh graph (Bern & Eppstein, 1995; Ho-Le, 1988), followed by the subsequent solving process. Therefore, the first step is to implement Delaunay triangulation (Lee & Schachter, 1980) to construct mesh graph with edge set \(_{mesh}\), in which each cell consists of three edges. Then we proceed to construct the edge set \(_{kn}\) by selecting the \(K\)-nearest nodes for each individual node. \(K\) is predetermined to signify the quantity of neighboring nodes that we deem as closely connected based on the Euclidean distance \(_{ij}\) between node \(i\) and \(j\). The final edge set is \(=_{mesh}_{kn}\). Figure 2 shows examples of graph construction.

### Overall Architecture

In this section, we will introduce the detailed architecture of our proposed BENO, as shown in Figure 3. Our overall neural operator is divided into two branches, with each branch receiving different graph information and boundary data. However, the operator architecture remains the same with the _encoder, boundary-embedded message passing neural network_ and _decoder_. Therefore, we will only focus on the common operator architecture.

#### 3.3.1 Encoder & Decoder

**Encoder.** The encoder computes node and edge embeddings. For each node \(i\), the node encoder \(^{v}\) maps the node coordinates \(p_{i}=(x_{i},y_{i})\), forcing term \(f_{i}\), and distances to boundary \(dx_{i},dy_{i}\) to node embedding vector \(v_{i}=^{e}([x_{i},y_{i},f_{i},dx_{i},dy_{i}]) R^{D}\) in a high-dimensional space. The same mapping is implemented on edge attributes with edge encoder \(^{e}\) for edge embedding vector \(_{ij}\). For both node and edge encoders \(\), we exploit a two-layer Multi-Layer Perceptron (MLP) (Murtagh, 1991) with Sigmoid Linear Unit (SiLU) activation (Elfwing et al., 2018).

**Decoder.** We use a two-layer MLP to map the features to their respective solution. Considering our dual-branch architecture, we will add the outputs from each decoder to obtain the final predicted solution \(\).

#### 3.3.2 Boundary-Embedded Message Passing Neural Network (BE-MPNN)

To address the inherent differences in physical properties between boundary and interior nodes, we opt not to directly merge these distinct sources of information into a single network representation. Instead, we first employ the Transformer to specifically embed the boundary nodes. Then, the obtained boundary information is incorporated into the graph message passing processor. We will provide detailed explanations for these two components separately.

Figure 2: Visualization of the graph construction on our train/set samples from 5 different corner elliptic datasets. The interior nodes are in black and the boundary one in purple.

**Embedding Boundary with Transformer.** With the boundary node coordinates \(p^{}=(x^{},y^{})\), the boundary value \(g\), and the distance to the geometric center of solution domain \(dc\) as input features, we first utilize the position embedding to include relative position relationship for initial representation \(H^{}_{0}\), followed by a Transformer encoder with \(L\) layers to embed the boundary information \(H^{}\). The resulting boundary features, denoted as \(\), are obtained by applying global average pooling (Lin et al., 2013) to the encoder outputs \(H^{}\).

Each self-attention layer applies multi-head self-attention and feed-forward neural networks to the input. The output of the \(i\)-th self-attention layer is denoted as \(H^{}_{i}\). The self-attention mechanism calculates the attention weights \(A_{i}\) as follows:

\[A_{i}=(H^{}_{i}(K_{i}H^{}_ {i})^{T}}{}})\] (5)

where \(Q_{i}\), \(K_{i}\), and \(V_{i}\) are linear projections of \(H^{}_{i-1}\) with learnable weight matrices, and \(d_{k}\) is the dimension of the key vectors. The attention output is computed as:

\[H^{}_{i+1}=(A_{i}V_{i}(H^{}_{i} )+H^{}_{i})\] (6)

where LayerNorm denotes layer normalization, which helps to mitigate the problem of internal covariate shift. After passing through the \(L\) self-attention layers, the output \(H^{}\) is subject to global average pooling to obtain the boundary features: \(=(H^{})\).

**Boundary-Embedded Message Passing Processor.** The processor computes \(T\) steps of message passing, with an intermediate graph representation \(^{1}\), \(\), \(^{T}\) and boundary representation \(^{1}\), \(\), \(^{T}\). The specific passing message \(m^{t}_{ij}\) in step \(t\) in our processor is formed by:

\[m^{t}_{ij}=v^{t}_{i},v^{t}_{j},e^{t}_{ij},p_{i}-p_{j}\] (7)

where \(m^{t+1}_{ij}\) represents the message sent from node \(j\) to \(i\). \(p_{i}-p_{j}\) is the relative position which can enhance the equivariance by justifying the symmetry of the PDEs.

Then we update the node feature \(v^{t}_{i}\) and edge feature \(e^{t}_{ij}\) as follows:

\[v^{t+1}_{i}=(v^{t}_{i},^{t},_{j( i)}m^{t}_{ij}),\] (8)

\[e^{t+1}_{ij}=(e^{t}_{ij},m^{t}_{ij})\] (9)

Here, boundary information is embedded into the message passing. \((i)\) represents the gathering of all the neighbors of node \(i\).

**Learning objective.** Given the ground truth solution \(u\) and the predicted solution \(\), we minimize the mean squared error (MSE) of the predicted solution on \(\).

Figure 3: Overall architecture of our proposed BENO. The pink branch corresponds to the first term in Eq. 4, and the **green branch** corresponds to the second term. As the backbone of boundary embedding, Transformer provides boundary information as a supplement for BE-MPNN, thereby enabling better prediction under complex boundary geometry and inhomogeneous boundary values.

## 4 Experiments

Here we aim to answer the following questions: (1) Compared with existing baselines, can BENO learn the solution operator for elliptic PDEs with complex geometry and inhomogeneous boundary values? (2) Can BENO _generalize_ to out-of-distribution boundary geometries and boundary values, and different grid resolutions? (3) Are all components of BENO essential for its performance? We first introduce the setup in Sec. 4.1, then answer questions above in the following three sections.

### Experiment setup

**Datasets.** For elliptic PDEs simulations, we construct five different datasets with inhomogeneous boundary values, including 4/3/2/1-corner squares and squares without corners. Each dataset consists of 1000 samples with randomly initialized boundary shapes and values, with 900 samples used for training and validation, and 100 samples for testing. Each sample covers a grid of 32\(\)32 nodes and 128 boundary nodes. To further assess model performance, higher-resolution versions of each data sample, such as 64\(\)64, are also provided. Details on data generation are provided in Appendix E.

**Baselines.** We adopt two of the most mainstream series of neural PDE solvers as baselines, one is graph-based, including **GKN**(Li et al., 2020b), **GNN-PDE**(Lotzsch et al., 2022), and **MP-PDE**(Brandstetter et al., 2022); the other is operator-based, including **FNO**(Li et al., 2020a). For fair comparison and adaption to irregular boundary shapes in our datasets, all of the baselines are re-implemented with the same input as ours, including all the interior and boundary node features. Please refer to Appendix G for re-implementation details.

**Implementation Details.** All experiments are based on PyTorch (Paszke et al., 2019) on 2\(\) NVIDIA A100 GPUs (80G). Following (Brandstetter et al., 2022), we also apply graph message passing neural network as our backbone for all the datasets. We use Adam (Kingma & Ba, 2014) optimizer with a weight decay of \(5 10^{-4}\) and a learning rate of \(5 10^{-5}\) obtained from grid search for all experiments. Please refer to Appendix F for more implementation details.

### Main Experimental Results

We first test whether our BENO has a strong capability to solve elliptic PDEs with varying shapes. Table 2 and 3 summarize the results for the shape generalization task (more in Appendix J).

From the results, we see that recent neural PDE solving methods (i.e., MP-PDE) overall _fail_ to solve elliptic PDEs with inhomogeneous boundary values, not to mention generalizing to datasets with different boundary shapes. This precisely indicates that existing neural solvers are insufficient for solving this type of boundary value problems.

In contrast, from Table 2, we see that our proposed BENO trained only on 4-Corners dataset consistently achieves a significant improvement and strong generalization capability over the previous methods by a large margin. More precisely, the improvements of BENO over the best baseline

   } &  &  &  &  &  \\  Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\   & 1.1146\(\) & 3.6497\(\) & 1.0692\(\) & 3.7059\(\) & 1.0673\(\) & 3.6822\(\) & 1.1063\(\) & 3.4898\(\) & 1.0728\(\) & 3.9551\(\) \\  & 0.3936 & 1.1874 & 0.2034 & 0.9543 & 0.1393 & 0.9819 & 0.1905 & 0.9469 & 0.2074 & 0.9791 \\   & 1.0947\(\) & 2.2707\(\) & 1.0742\(\) & 2.1657\(\) & 1.0672\(\) & 2.2617\(\) & 1.0921\(\) & 2.3922\(\) & 1.0762\(\) & 2.2281\(\) \\  & 0.3265 & 0.3361 & 0.3418 & 0.3976 & 0.3736 & 0.2449 & 0.2935 & 0.3526 & 0.4420 & 0.4192 \\   & 1.0026\(\) & 3.1410\(\) & 1.0009\(\) & 3.2812\(\) & 1.0015\(\) & 3.3557\(\) & 1.0002\(\) & 3.1421\(\) & 1.0011\(\) & 3.7561\(\) \\  & 0.0093 & 0.8751 & 0.0101 & 0.8839 & 0.0099 & 0.8521 & 0.0153 & 0.8685 & 0.0152 & 1.0274 \\   & 1.0007\(\) & 3.1018\(\) & 1.0003\(\) & 3.2464\(\) & 0.9919\(\) & 3.2765\(\) & 0.9829\(\) & 3.0163\(\) & 0.9882\(\) & 3.6522\(\) \\  & 0.0677 & 0.8431 & 0.0841 & 0.8049 & 0.0699 & 0.8632 & 0.07199 & 0.8272 & 0.0683 & 0.8961 \\   & **0.3523\(\)** & **0.9650\(\)** & **0.4308\(\)** & **1.2206\(\)** & **0.4910\(\)** & **1.4388\(\)** & **0.5416\(\)** & **1.4529\(\)** & **0.5542\(\)** & **1.7481\(\)** \\  & **0.1245** & **0.3131** & **0.1994** & **0.4978** & **0.1888** & **0.5227** & **0.2133** & **0.4626** & **0.1952** & **0.5394** \\   

Table 2: Performances of our proposed BENO and the compared baselines, which are trained on 900 4-corners samples and tested on 5 datasets under relative L2 norm and MAE separately. The unit of the MAE metric is \(1 10^{-3}\). Bold fonts indicate the best performance.

are \(55.17\%\), \(52.18\%\), \(52.43\%\), \(47.38\%\), and \(52.94\%\) in terms of relative L2 norm when testing on 4/3/2/1/No-Corner dataset respectively. We attribute the remarkable performance to two factors: (i) BENO comprehensively leverages boundary information, and fuses them with the interior graph message for solving. (ii) BENO integrates dual-branch architecture to fully learn boundary and interior in a decoupled way and thus improves generalized solving performance.

Similarly, from Table 3, we see that among mixed corner training results, BENO always achieves the best performance among various compared baselines when varying the test sets, which validates the consistent superiority of our BENO with respect to different boundary shapes.

Additionally, we plot the visualization of the best baseline and our proposed BENO trained on 4-Corners dataset in Figure 4. It can be clearly observed that the predicted solution of BENO is closed to the ground truth, while MP-PDE fails to learn any features of the solution. We observe similar behaviors for all other baselines.

### Results on Different Values

To investigate the generalization ability on boundary value, we again train the models on 4-Corners dataset with inhomogeneous boundary value but utilize the test set with zero boundary value, which makes the boundary inhomogeneities totally different. Table 4 compares the best baseline and summarizes the results. From the results, we see that BENO has a significant advantage, successfully reducing the L2 norm to around 0.1. In addition, our method outperforms the best baseline by approximately 60.96% in terms of performance improvement. This not only demonstrates BENO's strong generalization ability regarding boundary values but also provides solid experimental evidence for the successful application of our elliptic PDE solver.

Figure 4: Visualization of two samples’ prediction and prediction error from 4-Corners dataset. We render the solution \(u\) of the baseline MP-PDE, our BENO and the ground truth in \(\).

   } &  &  &  &  &  \\  Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\   & 1.0588\(\) & 3.5051\(\) & 1.0651\(\) & 3.7061\(\) & 1.0386\(\) & 3.6043\(\) & 1.0734\(\) & 3.4048\(\) & 1.0423\(\) & 3.901\(\) \\  & 0.1713 & 0.9401 & 0.1562 & 0.8563 & 0.1271 & 0.9392 & 0.1621 & 0.9519 & 0.2102 & 0.9287 \\   & 1.0834\(\) & 4.6401\(\) & 1.0937\(\) & 4.6902\(\) & 4.5267\(\) & 1.0735\(\) & 4.5027\(\) & 1.0713\(\) & 4.5783\(\) \\  & 0.0462 & 0.5327 & 0.0625 & 0.6713 & 0.0376 & 0.5581 & 0.0528 & 0.5371 & 0.0489 & 0.5565 \\   & 1.0009\(\) & 3.1311\(\) & 1.0003\(\) & 3.2781\(\) & 1.0005\(\) & 3.3518\(\) & 0.9999\(\) & 3.1422\(\) & 1.0002\(\) & 3.7528\(\) \\  & 0.0036 & 0.8664 & 0.0039 & 0.8858 & 0.0038 & 0.8520 & 0.0042 & 0.8609 & 0.0041 & 1.0284 \\   & 1.0063\(\) & 3.1238\(\) & 1.0045\(\) & 3.2537\(\) & 0.9957\(\) & 3.2864\(\) & 0.9822\(\) & 3.0177\(\) & 0.9912\(\) & 3.6658\(\) \\  & 0.0735 & 0.8502 & 0.0923 & 0.7867 & 0.0772 & 0.8607 & 0.0802 & 0.8363 & 0.0781 & 0.8949 \\   & **0.4487\(\)** & **1.2150\(\)** & **0.4783\(\)** & **1.3509\(\)** & **0.4737\(\)** & **1.3516\(\)** & **0.5168\(\)** & **1.3728\(\)** & **0.4665\(\)** & **1.4213\(\)** \\  & **0.1750** & **0.4213** & **0.1938** & **0.5432** & **0.1979** & **0.5374** & **0.1793** & **0.5148** & **0.2001** & **0.5262** \\   

Table 3: Performances of our proposed BENO and the compared baselines, which are trained on 900 mixed samples (180 samples each from 5 datasets) and tested on 5 datasets under relative L2 error and MAE separately. The unit of the MAE metric is \(1 10^{-3}\).

### Ablation Study

To investigate the effectiveness of inner components in BENO, we study four variants of BENO. Table 5 shows the effectiveness of our BENO on ablation experiments, which is implemented based on 4-Corners dataset training. Firstly, **BENO** w. **M** replaces the BE-MPNN with a vanilla message passing neural network (Gilmer et al., 2017) and merely keeps the interior node feature. Secondly, **BENO** w/o. **D** removes the dual-branch structure of BENO and merely utilizes a single Encoder-BE-MPNN-Decoder procedure. Thirdly, **BENO** w. **E** adds the Transformer output for edge message passing. Finally, **BENO** w. **G** replaces the Transformer architecture with a vanilla graph convolution network (Kipf and Welling, 2016).

From the results we can draw conclusions as follows. Firstly, **BENO** w. **M** performs significantly worse than ours, which indicates the importance of fusing interior and boundary in BENO. Secondly, results of **BENO** w/o. **D** indicate that decoupled learning of the interior and boundary proves to be effective. Thirdly, comparing the results of **BENO** w. **E** and ours, we can find that boundary information only helps in node-level message passing. In other words, it is not particularly suitable to directly inject the global information of the boundary into the edges. Finally, comparing results of **BENO** w. **G** with ours validates the design of Transformer for boundary embedding is crucial.

## 5 Conclusion

We propose the Boundary-Embedded Neural Operators (BENO) to address the challenges of solving elliptic PDEs with inhomogeneous and complex boundary conditions. Our BENO incorporates physical intuition through a boundary-embedded architecture consisting of graph neural networks and a Transformer to model the influence of boundary conditions on the solution. Comprehensive experiments demonstrate the effectiveness of our approach in outperforming state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems.

    \\  Test set &  &  &  &  &  \\  Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\   & 0.7092\(\) & 0.1259\(\) & 0.7390\(\) & 0.2351\(\) & 0.7491\(\) & 0.3290\(\) & 0.7593\(\) & 0.4750\(\) & 0.7801\(\) & 0.6808\(\) \\  & 0.0584 & 0.0755 & 0.0483 & 0.1013 & 0.0485 & 0.1371 & 0.05269 & 0.1582 & 0.0371 & 0.1692 \\   & 0.2598\(\) & 0.0459\(\) & 0.3148\(\) & 0.1066\(\) & 0.3729\(\) & 0.1778\(\) & 0.4634\(\) & 0.3049\(\) & 0.5458\(\) & 0.4924\(\) \\  & 0.1098 & 0.0359 & 0.0814 & 0.0618 & 0.0819 & 0.0969 & 0.0649 & 0.1182 & 0.0491 & 0.1310 \\   & **0.0908\(\)** & **0.0142\(\)** & **0.1031\(\)** & **0.0288\(\)** & **0.1652\(\)** & **0.0583\(\)** & **0.1738\(\)** & **0.0862\(\)** & **0.2441\(\)** & **0.1622\(\)** \\  & **0.07381** & **0.0131\(\)** & **0.0728** & **0.0189** & **0.1324** & **0.0362** & **0.1508** & **0.0456** & **0.1665** & **0.0798** \\   

Table 4: Performances of our BENO and the compared baselines, which are trained on 900 4-Corners samples and tested with zero boundary value samples. The unit of the MAE metric is \(1 10^{-3}\).

   Test set &  &  &  &  &  \\  Metric & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE & L2 & MAE \\   & 1.0130\(\) & 3.1436\(\) & 1.0159\(\) & 3.3041\(\) & 0.9999\(\) & 3.3007\(\) & 1.0026\(\) & 3.0842\(\) & 0.9979\(\) & 3.6832\(\) \\  & 0.0858 & 0.8667 & 0.0975 & 0.7906 & 0.0792 & 0.8504 & 0.0840 & 0.8202 & 0.0858 & 0.8970 \\   & 0.4058\(\) & 1.1175\(\) & 0.4850\(\) & 1.3810\(\) & 0.5273\(\) & 1.5439\(\) & 0.5795\(\) & 1.5683\(\) & 0.5835\(\) & 1.8382\(\) \\  & 0.1374 & 0.3660 & 0.2230 & 0.6068 & 0.1750 & 0.4774 & 0.1981 & 0.4670 & 0.2232 & 0.5771 \\   & 0.4113\(\) & 1.2020\(\) & 0.4624\(\) & 1.3569\(\) & 0.5347\(\) & 1.5990\(\) & 0.5891\(\) & 1.6222\(\) & 0.5843\(\) & 1.8790\(\) \\  & 0.1236 & 0.4048 & 0.2102 & 0.5453 & 0.1985 & 0.5604 & 0.2129 & 0.2016 & 0.2016 & 0.5952 \\   & 0.9037\(\) & 2.6795\(\) & 0.8807\(\) & 2.6992\(\) & 0.8928\(\) & 2.8235\(\) & 0.8849\(\) & 2.561\(\) & 0.87212 & 2.9851\(\) \\  & 0.1104 & 0.5332 & 0.1298 & 0.6118 & 0.1208 & 0.5892