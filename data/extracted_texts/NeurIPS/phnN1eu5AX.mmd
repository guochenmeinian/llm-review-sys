# Learning Probabilistic Symmetrization for

Architecture Agnostic Equivariance

 Jinwoo Kim  Tien Dat Nguyen  Ayhan Suleymanzade

Hyeokjun An  Seunghoon Hong

KAIST

###### Abstract

We present a novel framework to overcome the limitations of equivariant architectures in learning functions with group symmetries. In contrary to equivariant architectures, we use an arbitrary base model such as an MLP or a transformer and symmetrize it to be equivariant to the given group by employing a small equivariant network that parameterizes the probabilistic distribution underlying the symmetrization. The distribution is end-to-end trained with the base model which can maximize performance while reducing sample complexity of symmetrization. We show that this approach ensures not only equivariance to given group but also universal approximation capability in expectation. We implement our method on various base models, including patch-based transformers that can be initialized from pretrained vision transformers, and test them for a wide range of symmetry groups including permutation and Euclidean groups and their combinations. Empirical tests show competitive results against tailored equivariant architectures, suggesting the potential for learning equivariant functions for diverse groups using a non-equivariant universal base architecture. We further show evidence of enhanced learning in symmetric modalities, like graphs, when pretrained from non-symmetric modalities, like vision. Code is available at https://github.com/jw9730/lps.

## 1 Introduction

Many perception problems in machine learning involve functions that are invariant or equivariant to certain symmetry group of transformations of data. Examples include learning on sets and graphs, point clouds, molecules, proteins, and physical data, to name a few . Equivariant architecture design has emerged as a successful approach, where every building block of a model is carefully restricted to be equivariant to a symmetry group of interest . However, equivariant architecture design faces fundamental limitations, as individual construction of models for each group can be laborious or computationally expensive , the architectural restrictions often lead to limited expressive power , and the knowledge learned from one problem cannot be easily transferred to others of different symmetries as the architecture would be incompatible.

This motivates us to seek a **symmetrization** solution that can achieve group invariance and equivariance with general-purpose, group-agnostic architectures such as an MLP or a transformer. As a basic form of symmetrization, any parameterized function \(f_{}:\) on vector spaces \(\), \(\) can be made invariant or equivariant by group averaging , _i.e._, averaging over all possible transformations of inputs \(\) and outputs \(\) by a symmetry group \(G=\{g\}\):

\[_{}()=_{g G}g f_{}(g^{-1} ),\] (1)

where \(_{}\) is equivariant or invariant to the group \(G\). An important advantage is that the symmetrized function \(_{}\) can leverage the expressive power of the base function \(f_{}\); it has been shown that \(_{}\) is a universal approximator of invariant or equivariant functions if \(f_{}\) is a universal approximator ,which includes an MLP  or a transformer . On the other hand, an immediate challenge is that for many practical groups involving permutation and rotations, the cardinality of the group \(|G|\) is large or infinite, so the exact averaging is intractable. Due to this, existing symmetrization approaches often focus on small finite groups , manually derive smaller subsets of the entire group _e.g._, a frame  to average over , or implement a relaxed version of equivariance .

An alternative method for tractability is to interpret Eq.1 as an expectation with uniform distribution \((G)\) over the compact group \(G\), and use sampling-based average to estimate it :

\[_{}()=_{g(G)}[g f_{ }(g^{-1})],\] (2)

where \(g f_{}(g^{-1})\) serves as an unbiased estimator of \(_{}()\). While simple and general, this approach has practical issues that the base function \(f_{}\) is burdened to learn all equally possible group transformations, and the expectedly high variance of the estimator can lead to challenges in sampling-based training due to large variance of gradients as well as sample complexity of inference.

Our key idea is to replace the uniform distribution \((G)\) for the expectation in Eq.2 with a parameterized distribution \(p_{}(g|)\) in a way that equivariance and expressive power are always guaranteed, and train it end-to-end with the base function \(f_{}\) to directly minimize task loss. We show that the distribution \(p_{}(g|)\) only needs to satisfy one simple condition to guarantee equivariance and expressive power: it has to be probabilistically equivariant . This allows us to generally implement \(p_{}(g|)\) as a noise-outsourced map \((,) g\) with an invariant noise \(\) and a small equivariant network \(q_{}\), which enables gradient-based training with reparameterization . As \(p_{}\) is trained, it can enhance the learning of \(f_{}\) by producing group transformations of lower variance compared to \((G)\) so that \(f_{}\) is less burdened, and coordinating with \(f_{}\) to maximize task performance. We refer to our approach as probabilistic symmetrization. An overview is provided in Figure1.

We implement and test our method with two general-purpose architectures as the base function \(f_{}\): MLP and transformer. In particular, our transformer backbone is architecturally identical to patch-based vision transformers , which allows us to initialize most of its parameters from ImageNet-21k pretrained weights  and only replace the input and output projections to match task dimensions. We implement the conditional distribution \(p_{}(g|)\) for a wide range of practical symmetry groups including permutation \((_{n})\) and Euclidean groups \((/(d)\) and \(/(d))\) and their product combinations (_e.g._, \(_{n}(3)\)), all of which are combinatorial or infinite groups. Empirical tests on a wide range of invariant and equivariant tasks involving graphs and motion data show competitive results against tailored equivariant architectures as well as existing symmetrization methods . This suggests the potential for learning invariant or equivariant functions for diverse groups with a group-agnostic general-purpose backbone. We further show evidence that pretraining from non-symmetric modality (vision) leads to enhanced learning in symmetric modality (graphs).

Figure 1: Overview of probabilistic symmetrization. We symmetrize an unconstrained base function \(f_{}\) into an equivariant function \(_{,}\) for group \(G\) using a learned equivariant distribution \(p_{}(g|)\).

Probabilistic Symmetrization for Equivariance

We introduce and analyze our approach called probabilistic symmetrization, which involves an equivariant distribution \(p_{}\) and group-agnostic base function \(f_{}\), in Section 2.1. We then describe implementation of \(p_{}\) for practical groups including permutations and rotations in Section 2.2. Then, we describe our choice of base function \(f_{}\) focusing on MLP and, transformers in particular, in Section 2.3. All proofs can be found in Appendix A.1.

Problem SetupIn general, our goal is to construct a function \(:\) on finite vector spaces \(,\) that is invariant or equivariant to symmetry specified by a group \(G=\{g\}\)1. This is formally described by specifying how the group act as transformations on the input and output. A group representation \(:G()\), where \(()\) is the set of all invertible matrices on \(\), associates each group element \(g G\) to an invertible matrix \((g)\) that transforms a given vector \(\) through \( g=(g)\). Given that, a function \(:\) is \(G\) equivariant if:

\[(_{1}(g))=_{2}(g)(),,g G,\] (3)

where the representations \(_{1}\) and \(_{2}\) are on the input and output, respectively. \(G\) invariance is a special case of equivariance when the output representation is trivial, \(_{2}(g)=\).

### Probabilistic Symmetrization

To construct a \(G\) equivariant function \(_{}\), group averaging symmetrizes an arbitrary base function \(f_{}:\) by taking expectation with uniform distribution over the group (Eq. (2)). Instead, we propose to use an input-conditional parameterized distribution \(p_{}(g|)\) and symmetrize \(f_{}\) as follows:

\[_{,}()=_{p_{}(g|)}[ _{2}(g)f_{}(_{1}(g)^{-1})],\] (4)

where the distribution \(p_{}(g|)\) itself satisfies probabilistic \(G\) equivariance:

\[p_{}(g|)=p_{}(g^{}g|_{1}(g^{})),,g,g^{} G.\] (5)

Importantly, we show that probabilistic symmetrization with equivariant \(p_{}\) guarantees equivariance as well as expressive power of the symmetrized \(_{,}\).

**Theorem 1**.: _If \(p_{}\) is \(G\) equivariant, then \(_{,}\) is \(G\) equivariant for arbitrary \(f_{}\)._

Proof.: The proof can be found in Appendix A.1.1. 

**Theorem 2**.: _If \(p_{}\) is \(G\) equivariant and \(f_{}\) is a universal approximator, then \(_{,}\) is a universal approximator of \(G\) equivariant functions._

Proof.: The proof can be found in Appendix A.1.2. 

While the base function \(f_{}\) that guarantees universal approximation can be chosen in a group-agnostic manner _e.g._, an MLP  or a transformer on token sequences , the distribution \(p_{}\) needs to be instantiated group specifically to satisfy \(G\) equivariance. A simplistic choice is using uniform distribution \((G)\) for all inputs \(\) with no parameterization (reducing to group averaging), which is technically equivariant and therefore guarantees equivariance and universality. However, appropriately parameterizing and learning \(p_{}\) can provide distinguished advantages compared to the uniform distribution, as it can **(1)** learn from data to collaborate with (pre-trained) base function \(f_{}\) to maximize task performance and **(2)** learn to produce more consistent samples \(g p_{}(g|)\) that can offer more stable gradients for the base function \(f_{}\) during early training.

We now provide a generic blueprint of \(G\) equivariant distribution \(p_{}(g|)\) for any compact group \(G\). Our goal is to sample \(g p_{}(g|)\) to obtain group representation \((g)\) for symmetrization (Eq. (4)) in a differentiable manner so that \(p_{}\) can be trained end-to-end. Since we only need sampling and there is no need to evaluate likelihoods, we simply implement \(p_{}(g|)\) as a noise-outsourced, differentiable transformation \(q_{}(,)\) of a noise variable \(\) that directly outputs a group representation \((g)\):

\[(g)=q_{}(,), p(),\] (6)where \(q_{}\) is \(G\) equivariant and \(p()\) is \(G\) invariant under an appropriate representation \(^{}\):

\[q_{}(_{1}(g),^{}(g))=(g)q_{ }(,), p()=p(^{}(g) {}),,,g  G.\] (7)

Given above implementation, we can show the \(G\) equivariance of \(p_{}\):

**Theorem 3**.: _If \(q_{}\) is \(G\) equivariant and \(p()\) is \(G\) invariant under representation \(^{}\) that \(|^{}(g)|=1 g G\), the distribution \(p_{}(g|)\) characterized by \(q_{}:(,)(g)\) is \(G\) equivariant._

Proof.: The proof can be found in Appendix A.1.3. 

In practice, one can use any available \(G\) equivariant neural network to implement \(q_{}\), _e.g._, a graph neural network for the symmetric group \(_{n}\), or an equivariant MLP which can be constructed for any matrix group . Since we expect most of the reasoning to be done by the base function \(f_{}\), the equivariant network \(q_{}\) can be small and relatively less expressive. This allows us to get less affected by their known issues in expressiveness and scaling . For the noise \( p()\), simple choices often suffice for \(G\) invariance. For example, standard normal \((0,_{n})\) provides invariance for the symmetric group \(_{n}\) as well as the (special) orthogonal group \((n)\) and \((n)\).

One important detail in designing \(q_{}\) is constraining its output to be a valid group representation \((g)\). For this, we apply appropriate postprocessing to refine neural network features into group representations, _e.g._, Gram-Schmidt orthogonalization to obtain a representation \((g)^{n n}\) of the orthogonal group \(g(n)\). Importantly, to not break the \(G\) equivariance of \(q_{}\), this postprocessing needs to be equivariant itself, _e.g._, Gram-Schmidt process is itself \((n)\) equivariant . Implementations of \(p_{}\) for a range of practical symmetry groups are provided in detail in Section 2.2.

### Equivariant Distribution \(p_{}\)

We present implementations of the \(G\) equivariant distribution \(p_{}(g|)\) for a range of practical groups demonstrated in our experiments (Section 3). Formal proofs of correctness are in Appendix A.1.4.

Symmetric Group \(_{n}\)The symmetric group \(_{n}\) over a finite set of \(n\) elements contains all permutations of the set, which describes symmetry to ordering desired for learning set and graph data. The base representation is given by \((g)=_{g}\) where \(_{g}\{0,1\}^{n n}\) is a permutation matrix for \(g\).

To implement \(_{n}\) equivariant distribution \(p_{}(g|)\) that provides permutation matrices \(_{g}\) from graph data \(\), we use the following design. We first sample invariant noise \(^{n d}\) from i.i.d. uniform \([0,]\) with noise scale \(\). For the \(_{n}\) equivariant map \(q_{}:(,)(g)\), we first use a graph neural network (GNN) as an equivariant map \((,)\) that outputs nodewise scalar \(^{n}\). Then, assuming \(\) is tie-free2, we use below argort operator \(_{g}\) to obtain permutation matrix :

\[_{g}=(^{},( )^{}),\] (8)

where eq denotes elementwise equality indicator. The argsort operator is \(_{n}\) equivariant, _i.e._, it maps \(_{g^{}}_{g^{}}_{g}\) for all \(_{g^{}}_{n}\). To backpropagate through \(_{g}\) during training, we use straight-through gradient estimator  with an approximate permutation matrix \(}_{g}_{g}\) obtained from a differentiable relaxation of argsort operator ; details can be found in Appendix A.3.1.

Orthogonal Group \((n)\), \((n)\)The orthogonal group \((n)\) contains all roto-reflections in \(^{n}\) around origin, and the special orthogonal group \((n)\) contains all rotations without reflections. These groups describe rotation symmetries desirable in learning geometric data. The base group representation for \((n)\) is given by \((g)=_{g}\) where \(_{g}\) is the orthogonal matrix for \(g\). For \((n)\), the representation is \((g)=_{g}^{+}\) where \(_{g}^{+}\) is the orthogonal matrix of \(g\) with determinant \(+1\).

To implement \((n)\)/\((n)\) equivariant distribution \(p_{}(g|)\) that provides orthogonal matrices given input data \(\), we use the following design. We first sample invariant noise \(^{n d}\) from i.i.d. normal \((0,^{2})\) with noise scale \(\). For the \((n)\)/\((n)\) equivariant map \(p_{}:(,)(g)\), we first use an equivariant neural network as a map \((,)\) that outputs \(n\) features \(^{n n}\). Then, assuming \(\) is full-rank3, we use Gram-Schmidt process for orthogonalization, which is differentiable and \((n)\) equivariant . This completes the postprocessing \(_{g}\) for \((n)\). For \((n)\), we can further use a simple scale operator \(_{g}^{+}\) to set the determinant of orthogonalized matrix to \(+1\):

\[:[_{1}&...&_{n}\\ ][() _{1}&...&_{n}\\ ],\] (9)

The scale operator is differentiable and \((n)\) equivariant, _i.e._, it maps \(_{g^{}}^{+}_{g^{}}^{+}_{g}^{+}\) for all \(_{g^{}}^{+}(n)\), thereby completing the postprocessing \(_{g}^{+}\) for \((n)\).

Euclidean Group \((n)\), \((n)\)The Euclidean group \((n)\) contains all roto-translations and reflections in \(^{n}\) and their combinations, and the special Euclidean group \((n)\) contains all roto-translations without reflections. These groups are desired in learning physical systems such as a particle in motion. Formally, the Euclidean group is given as a combination of orthogonal group and translation group \((n)=(n)(n)\), and similarly \((n)=(n)(n)\). As the translation group \((n)\) is non-compact which violates our assumption for symmetrization, we handle it separately. Following prior work [74; 41], we subtract the centroid \(}\) from the input data \(\) as \(-}\), and add it to the rotation symmetrized output as \(}+g f_{}(g^{-1}(-}))\) where \(g\) is sampled from \((n)/(n)\) equivariant \(p_{}(g|-})\). This makes the overall symmetrized function \((n)/(n)\) equivariant.

Product Group \(H K\)While we have described several groups individually, in practice we often encounter product combinations of groups \(G=H K\) that describe joint symmetry to each group \(H\) and \(K\). For example, \(_{n}(3)\) describes joint symmetry to permutations and rotations, which is desired in learning point clouds, molecules, and particle interactions. In general, an element of \(H K\) is given as \(g=(h,k)\) where \(h H\) and \(k K\), and group operations are applied elementwise \(gg^{}=(h,k)(h^{},k^{})=(hh^{},kk^{})\). The base group representation is accordingly given as pair of representations \((g)=((h),(k))\). While a common approach to handling \(H K\) is _partially_ symmetrizing on \(H\) and imposing \(K\) equivariance on the base architecture (_e.g._, rotational symmetrization of a graph neural network for \(_{n}(3)\) equivariance [74; 41]), we extend to _full symmetrization_ on \(H K\) since our goal is not imposing any constraint on the base function \(f_{}\).

To implement \(H K\) equivariant distribution \(p_{}(g|)\) that gives \((g)=((h),(k))\) from data \(\), we use the following design. We first sample invariant noise \(\) from i.i.d. normal \((0,^{2})\) with scale \(\). For the \(H K\) equivariant map \(q_{}:(,)((h),(k))\), we employ a \(H K\) equivariant neural network as a map \((,)(_{H},_{K})\) such that the postprocessing for each group \(H\) and \(K\) provides maps \(_{H}(h)\) and \(_{K}(k)\) respectively, leading to full representation \((g)=((h),(k))\). For this whole procedure to be \(H K\) equivariant, it is sufficient to have \(_{H}\) be \(K\) invariant and \(_{K}\) be \(H\) invariant. These are special cases of \(H K\) equivariance, and is supported by a range of equivariant neural networks especially regarding \(_{n}(3)\) or \(_{n}(3)\) equivariances .

### Base Function \(f_{}\)

We now describe the choice of group-agnostic base function \(f_{}:\). As group symmetry is handled by the equivariant distribution \(p_{}(g|)\), any symmetry concern is _hidden_ from \(f_{}\), allowing the inputs \(\) and outputs \(\) to be treated as plain multidimensional arrays. This allows us to implement \(f_{}\) with powerful general-purpose architectures, namely as an MLP that operates on flattened vectors of inputs and outputs, or a transformer as we describe below.

Let inputs \(=^{n_{1}... n_{n} c}\) and outputs \(=^{n^{}_{1}... n^{}_{ 1} c^{}}\) be multidimensional arrays with \(c\) and \(c^{}\) channels, respectively. Our transformer base function \(f_{}:\) is given as:

\[f_{}=,\] (10)

where \(:^{m d}\) parses input array to a sequence of \(m\) tokens, \(:^{m d}^{m d}\) is a standard transformer encoder on tokens used in language and vision [24; 26], and detokenize \(:^{m d}\) decodes encoded tokens to output array. For the tokenizer and detokenizer, we can use linear projections on flattened chunks of the array, which directly extends flattened patch projections in vision transformers to higher dimensions [26; 85; 86]. This enables mapping between different dimensional inputs and outputs, _e.g_, for graph node classification with \(^{n n c}\) and \(^{n c^{}}\), it is possible to use 2D patch projection for the input and 1D projection for the output.

Above choice of \(f_{}\) offers important advantages including universal approximation  and ability to share and transfer the learned knowledge in \(\) over different domains of different group symmetries. Remarkably, this allows us to directly leverage large-scale pre-trained parameters from data-abundant domains for learning on symmetric domains. In our experiments, we only replace the tokenizer and detokenizer of a vision transformer pre-trained on ImageNet-21k [99; 26] and fine-tune it to perform diverse \(_{n}\) equivariant tasks such as graph classification, node classification, and link prediction.

### Relation to Other Symmetrization Approaches

We discuss relation of our symmetrization method to prior ones, specifically group averaging [102; 4], frame averaging , and canonicalization . An extended discussion on broader related work can be found in Appendix A.2. Since these symmetrization methods share common formalization \(=_{g}[g f_{}(g^{-1})]\), one can expect a close theoretical relationship between them. We observe that probabilistic symmetrization is quite general; based on particular choices of the \(G\) equivariant distribution \(p_{}(g|)\), it can become most of the related symmetrization methods as special cases. This can be easily seen for group averaging , as the distribution \(p_{}\) can reduce to the uniform distribution \((G)\) over the group. Frame averaging  also takes an average, but over a subset of group given by a frame \(F: 2^{G}\); importantly, it is required that the frame itself is \(G\) equivariant \(F((g))=gF()\). We can make the following connection between our method and frame averaging, by adopting the concept of stabilizer subgroup \(G_{}=\{g G:(g)=\}\):

**Proposition 1**.: _Probabilistic symmetrization with \(G\) equivariant distribution \(p_{}(g|)\) can become frame averaging  by assigning uniform density to a set of orbits \(G_{}g\) for some group elements \(g\)._

Proof.: The proof can be found in Appendix A.1.5. 

Canonicalization  uses a _single_ group element for symmetrization, produced by a trainable canonicalizer \(C_{}: G\). Here, it is required that the canonicalizer itself satisfies _relaxed_\(G\) equivariance \(C((g))=gg^{}C()\) up to arbitrary action from the stabilizer \(g^{} G_{}\). We now show:

**Proposition 2**.: _Probabilistic symmetrization with \(G\) equivariant distribution \(p_{}(g|)\) can become canonicalization  by assigning uniform density to a single orbit \(G_{}g\) of some group element \(g\)._

Proof.: The proof can be found in Appendix A.1.5. 

Assuming that stabilizer \(G_{}\) is trivial, this can be implemented with our method by removing random noise \(\), which reduces \(p_{}\) to deterministic map \((g)=q_{}()\). We use this approach to implement canonicalizer for the \(_{n}\) group, while  only provides canonicalizers for Euclidean groups.

## 3 Experiments

We empirically demonstrate and analyze probabilistic symmetrization on a range of symmetry groups \(_{n}\), \((3)\) (\((3)\)), and the product \(_{n}(3)\) (\(_{n}(3)\)), on a variety of invariant and equivariant tasks, with general-purpose base functions \(f_{}\) chosen as MLP and transformer optionally with pretraining from vision domain. Details of the experiments are in Appendix A.3, and supplementary experiments on other base functions and comparisons to other symmetrization approaches are in Appendix A.4.

### Graph Isomorphism Learning with MLP

Building expressive neural networks for graphs (\(_{n}\)) has been considered important and challenging, as simple and efficient GNNs are often limited in expressive power to certain Weisfeiler-Lehman isomorphism tests like 1-WL [101; 56]. Since an MLP equipped with probabilistic symmetrization is in theory universal and \(_{n}\) equivariant, it has potential for graph learning that require high expressive power. To explicitly test this, we adopt the experimental setup of  and use two datasets on graph separation task (\(_{n}\) invariant). GRAPH8c  consists of all non-isomorphic connected graphs with 8 nodes, and EXP  consists of 3-WL distinguishable graphs that are not 2-WL distinguishable. We compare our method to standard GNNs as well as an MLP symmetrized with group averaging , frame averaging , and canonicalization . Our method uses the same MLP architecture to symmetrization baselines, and its \(_{n}\) equivariant distribution \(p_{}\) for symmetrization is implementedusing a 3-layer GIN  which is 1-WL expressive. We use 10 samples for symmetrization during both training and testing. Further details can be found in Appendix A.3.3, and supplementary results on symmetrization of a different base model can be found in Appendix A.4.1.

The results are in Table 1. At random initialization, all symmetrization methods can provide perfect separation of all graphs, similar to PPGN  and GNNML3  that are equivariant neural networks carefully designed to be 3-WL expressive. However, when trained with gradient descent to solve classification problem, naive symmetrization with group averaging fails, presumably because the MLP fails to adjust to equally possible \(64!\) permutations of \(64\) nodes in maximum. On the other hand, our method is able to learn the task, achieving the same accuracy to frame averaging that utilizes costly eigendecomposition of graph Laplacian . What makes our method work while group averaging fails? We conjecture this is since the distribution \(p_{}(g|)\) can learn to provide more consistent permutations during early training, as we illustrate in Figure 2. In the figure, we measured the consistency of samples from \(p_{}(g|)\) over training progress by sampling \(N=50\) permutation matrices \(_{g} p_{}(g|)\) and measuring the row-wise entropy of their average \(}=_{g}/N\) for each input \(\). The more consistent the sampled permutations, the sharper their average, and lesser the entropy. As training progresses, \(p_{}\) learns to produce more consistent samples, which coincides with the initial increase in task performance. Given that, a natural question would be: if we enforce the samples \(g p_{}(g|)\) to be consistent from the first place, would it work? To answer this, we also tested a non-probabilistic version of our model that uses a single permutation per input \((g)=q_{}()\), which is a canonicalizer under relaxed equivariance  as described in Section 2.4. As in Table 1, canonicalization fails, suggesting that probabilistic nature of \(p_{}(g|)\) can be _beneficial_ for learning. For another deterministic version of our model made by fixing the noise \(\) (Eq. (6)) at initialization, the performance drops to 79.5%, further implying that stochasticity of \(p_{}(g|)\) has a role.

  method & arch. & sym. & GRAPHSc \(\) & EXP \(\) & EXP-classify \(\) \\ GCN  & S\({}_{n}\) & - & 4755 & 600 & 50\% \\ GAT  & S\({}_{n}\) & - & 1828 & 600 & 50\% \\ GIN  & S\({}_{n}\) & - & 386 & 600 & 50\% \\ CheNet  & S\({}_{n}\) & - & 44 & 71 & 82\% \\  PPGN  & S\({}_{n}\) & - & 0 & 0 & **100\%** \\ GNNML3  & S\({}_{n}\) & - & 0 & 0 & **100\%** \\  MLP-GA  & - & S\({}_{n}\) & 0 & 0 & 50\% \\ MLP-PA  & - & S\({}_{n}\) & 0 & 0 & **100\%** \\ MLP-Cnnical. & - & S\({}_{n}\) & 0 & 0 & 50\% \\ MLP-PS (Ours), fixed \(\) & - & S\({}_{n}\) & 0 & 0 & 79.5\% \\ MLP-PS (Ours) & - & S\({}_{n}\) & 0 & 0 & **100\%** \\  

Table 1: Results for S\({}_{n}\) invariant graph separation. We use two tasks, one for counting pairs of graphs not separated by a model at random initialization (GRAPH8c and EXP), and one for learning to classify EXP to two classes (EXP-classify). For EXP-classify, we report the test accuracy at best validation accuracy. The columns arch. and sym. denote architectural and symmetrized equivariance, respectively. The results for baselines are from  except for MLP-Canonical. which is tested by us.

### Particle Dynamics Learning with Transformer

Learning sets or graphs attributed with position and velocity in 3D (\(_{n}(3)\)) is practically significant as they universally appear in physics, chemistry, and biology applications. While prior symmetrization methods employ an already \(_{n}\) equivariant base function and partially symmetrize the \((3)\) part, we attempt to symmetrize the entire product group \(_{n}(3)\) and choose the base model \(f_{}\) as a sequence transformer to leverage its expressive power. For empirical demonstration, we adopt the experimental setup of  and use the \(n\)-body dataset [84; 31] where the task is predicting the position of \(n=5\) charged particles after certain time given their initial position and velocity in \(^{3}\) (\(_{n}(3)\) equivariant). We compare our method to \(_{n}(3)\) equivariant neural networks and partial symmetrization methods applying \((3)\) symmetrization to GNNs. We also test prior symmetrization methods on the full group \(_{n}(3)\) along our method, but could not test for frame averaging since equivariant frames for the full group \(_{n}(3)\) was not available in current literature. Our method is implemented using a transformer with sequence positional encodings with around 2.3\(\) parameters of the baselines, and the \(_{n}(3)\) equivariant distribution \(p_{}\) for symmetrization is implemented using a 2-layer Vector Neurons  that has around 0.03\(\) of parameters to the transformer. We use 20 samples for symmetrization during training, and use 10\(\) sample size for testing since the task is regression where appropriate variance reduction is necessary to guarantee a reliable performance. Further details can be found in Appendix A.3.4, and supplementary results on \((3)\) partial symmetrization of GNN base model can be found in Appendix A.4.2.

The results are in Table 2. We observe simple group averaging exhibits a surprisingly strong performance, as it achieves 0.00414 MSE and already outperforms previous state of the art 0.0043 MSE. This is because the permutation component of the symmetry is fairly small, with \(n=5\) particles interacting with each other, such that combining it with an expressive base model \(f_{}\) (a sequence transformer) can adjust to \(5!=120\) equally possible permutations and their rotations in 3D. Nevertheless, our method outperforms group averaging and achieves a new state of the art 0.00401 MSE, presumably as the parameterized distribution \(p_{}\) learns to further maximize task performance. On the other hand, the canonicalization approach, implemented by eliminating noise variable \(\) from our method (Section 2.4), performs relatively poorly. We empirically observe that \(f_{}\) memorizes the per-input canonical orientations provided by \((g)=q_{}()\) that do not generalize to test inputs. This again shows that probabilistic nature of \(p_{}(g|)\) can be beneficial for performance.

### Graph Pattern Recognition with Vision Transformer

One important goal of our approach, and symmetrization in general, is to _decouple_ the symmetry of problem from the base function \(f_{}\), such that we can leverage knowledge learned from other symmetries by transferring the parameters \(\). We demonstrate an extreme case by transferring the parameters of a vision transformer  trained on large-scale image classification (translation invariant) to solve node classification on graphs (\(_{n}\) equivariant) for the first time in literature. For this, we use the PATTERN dataset  that contains 14,000 purely topological random SBM graphs with 44-188 nodes, whose task is finding certain subgraph pattern by binary node classification.

  method & arch. & sym. & Position MSE \(\) \\  SE(3) Transformer  & \(_{n}(3)\) & - & 0.0244 \\ TFN  & \(_{n}(3)\) & - & 0.0155 \\ Radial Field  & \(_{n}(3)\) & - & 0.0104 \\ EGNN  & \(_{n}(3)\) & - & 0.0071 \\  GNN-FA  & \(_{n}\) & \((3)\) & 0.0057 \\ GNN-Canonical.  & \(_{n}\) & \((3)\) & 0.0043 \\  Transformer-Canonical. & - & \(_{n}(3)\) & 0.00508 \\ Transformer-GA & - & \(_{n}(3)\) & 0.00414 \(\) 0.00001 \\ Transformer-PS (Ours) & - & \(_{n}(3)\) & **0.00401 \(\) 0.00001** \\  

Table 2: Results for \(_{n}(3)\) equivariant \(n\)-body problem. The columns arch. and sym. denote architectural and symmetrized equivariance, respectively. We report test MSE at best validation MSE, along with the standard deviation for GA and Ours where predictions are stochastic. The results for baselines are from  except symmetrized transformers which are tested by us.

Based on pre-trained ViT [26; 89], we construct the base model \(f_{}\) by modifying only the input and output layers to take the flattened patches of 2D zero-padded adjacency matrices of size 188\(\)188 and produce output as 1D per-node classification logits of length 188 with 2 channels. In addition to standard GNNs4, we compare group averaging, frame averaging, and canonicalization to our method, and also test whether pre-trained representations from ImageNet-21k is beneficial for the task. For the \(_{n}\) equivariant distribution \(p_{}\) in our method and canonicalization, we use a 3-layer GIN  with only 0.02% of the base model parameters. Further details can be found in Appendix A.3.5.

The results are in Table 3. First, we observe that transferring pre-trained ViT parameters consistently improves node classification for all symmetrization methods. It indicates that some traits of the pre-trained visual representation can benefit learning graph tasks which vastly differ in both the underlying symmetry (translation invariance \(_{n}\) equivariance) and the data generating process (natural images \(\) random process of SBM). In particular, it is somewhat surprising that vision pretraining allows group averaging to achieve 84.351% accuracy, on par with GNN baselines, considering that memorizing all 188! equally possible permutations in this dataset is impossible. We conjecture that group averaged ViT can in some way learn meaningful graph representation internally to solve the task, and vision pretraining helps in acquiring the representation by providing a good initialization point or transferable computation motifs.

On the other hand, frame averaging shows a low performance, 79.637% accuracy with vision pretraining, which is also surprising considering that frames vastly reduce the sample space of symmetrization in general; in fact, the size of frame of each graph in PATTERN is exactly 1. We empirically observe that, unlike group averaging, ViT with frame averaging memorizes frames of training graphs rather than learning generalizable graph representations. In contrast, canonicalization that also uses a single sample per graph successfully learns the task with 86.166% accuracy. We conjecture that the learnable orderings provided by an equivariant neural network \((g)=q_{}()\) is more flexible and generalizable to unseen graphs compared to frames computed from fixed graph Laplacian eigenvectors. Lastly, our method achieves a better performance compared to other symmetrization methods, and the performance consistently improves with vision pretraining and more samples for testing. As a result, our model based on pre-trained ViT and 10 samples for testing achieves 86.285% test accuracy, surpassing all baselines.

   method & pretrain. & Accuracy \(\) \\  GCN , 16 layers & - & 85.614 \\ GAT , 16 layers & - & 78.271 \\ GatedGCN , 16 layers & - & 85.568 \\ GIN , 16 layers & - & 85.387 \\ RingGANN , 2 layers & - & 86.245 \\ RingGNN , 8 layers & - & diverged \\ PPGN , 3 layers & - & 85.661 \\ PPGN , 8 layers & - & diverged \\  ViT-GA, 1-sample & - & 76.776 \(\) 0.137 \\ ViT-GA, 10-sample & - & 83.119 \(\) 0.048 \\ ViT-GA, 1-sample & ImageNet-21k & 81.407 \(\) 0.101 \\ ViT-GA, 10-sample & ImageNet-21k & 84.351 \(\) 0.053 \\  ViT-FA & - & 70.063 \\ ViT-FA & ImageNet-21k & 79.637 \\  ViT-Canonical. & - & 85.460 \\ ViT-Canonical. & ImageNet-21k & 86.166 \\  ViT-PS (Ours), 1-sample & - & 85.542 \(\) 0.012 \\ ViT-PS (Ours), 10-sample & - & 85.635 \(\) 0.021 \\ ViT-PS (Ours), 1-sample & ImageNet-21k & 86.226 \(\) 0.028 \\ ViT-PS (Ours), 10-sample & ImageNet-21k & **86.285 \(\) 0.015** \\   

Table 3: Results for \(_{n}\) equivariant node classification on PATTERN. We report test accuracy at the best validation accuracy, along with the standard deviation for GA and Ours where predictions are stochastic. The results for GNN baselines are from .

### Real-World Graph Learning with Vision Transformer

Having observed that pre-trained ViT can learn graph tasks well when symmetrized with our method, we now provide a preliminary test of it in real-world graph learning. We use three real-world graph datasets from  that involve chemical and biological graphs. PCQM-Contact dataset contains 529,434 molecular graphs with 53 nodes in maximum, and the task is contact map prediction framed as link prediction (S\({}_{n}\) equivariant), on whether two atoms would be proximal when the molecule is in 3D space. Peptides-func and Peptides-struct are based on the same set of 15,535 protein graphs with 444 nodes in maximum and the tasks are property prediction (S\({}_{n}\) invariant), requiring multi-label classification for Peptides-func and regression for Peptides-struct. The tasks require complex understanding of how the amino acids of the proteins would interact in 3D space. We implement our method using a ViT-Base pre-trained on ImageNet-21k as the base model \(f_{}\) and a 3-layer GIN as the equivariant distribution \(p_{}(g|)\), following our model in Section 3.3. We use 10 samples for both training and testing. Further details can be found in Appendix A.3.6.

The results are in Table 4. In Peptides-func and PCQM-Contact, the pre-trained ViT symmetrized with our method achieves a significantly higher performance compared to both GNNs and graph transformers, improving previous best by a large margin5 (0.6527 \(\) 0.8311 for Peptides-func AP, 0.1355 \(\) 0.3268 for PCQM-Contact Hits@1). This demonstrates the scalability of our method as Peptides-func involves 444 maximum nodes, and also its generality as it performs well for both S\({}_{n}\) invariant (Peptides-func) and equivariant (PCQM-Contact) tasks. We also note that, unlike some baselines, our method does not require costly Laplacian eigenvectors to compute positional encoding. On the Peptides-struct, our method achieves a slightly lower performance to SOTA graph transformers while still better than GNNs. We conjecture that regression is harder for the model to learn due to its stochasticity in predictions, and leave improving regression performance as future work.

## 4 Conclusion

We presented probabilistic symmetrization, a general framework that learns a distribution of group transformations conditioned on input data for symmetrization of an arbitrary function. By characterizing that the only condition for such distribution is equivariance to data symmetry, we instantiated models for a wide range of groups, including symmetric, orthogonal, Euclidean groups and their product combinations. Our experiments demonstrated that the proposed framework achieves consistent improvement over other symmetrization methods, and is competitive or outperforms equivariant networks on various datasets. We also showed that transferring pre-trained parameters across data in different symmetries can sometimes be surprisingly beneficial. Our approach has weaknesses such as sampling cost, which we further discuss in Appendix A.5; we plan to address these in future work.

AcknowledgementsThis work was supported in part by the National Research Foundation of Korea (NRF2021R1C1C1012540 and NRF2021R1A4A3032834) and IITP grant (2021-0-00537, 2019-0-00075, and 2021-0-02068) funded by the Korea government (MSIT).

   method & Peptides-func & Peptides-struct &  \\   & AP \(\) & MAE \(\) & Hits@1 \(\) & Hits@3 \(\) & Hits@10 \(\) & MRR \(\) \\  GCN  & 0.5930 & 0.3496 & 0.1321 & 0.3791 & 0.8256 & 0.3234 \\ GCNII  & 0.5543 & 0.3471 & 0.1325 & 0.3607 & 0.8116 & 0.3161 \\ GINE  & 0.5498 & 0.3547 & 0.1337 & 0.3642 & 0.8147 & 0.3180 \\ GatedGCN  & 0.5864 & 0.3420 & 0.1279 & 0.3783 & 0.8433 & 0.3218 \\ GatedGCN+RWSE  & 0.6069 & 0.3357 & 0.1288 & 0.3808 & 0.8517 & 0.3242 \\  Transformer+LapPE  & 0.6326 & 0.2529 & 0.1221 & 0.3679 & 0.8517 & 0.3174 \\ SAN+LapPE  & 0.6384 & 0.2683 & 0.1355 & 0.4004 & 0.8478 & 0.3350 \\ SAN+RWSE  & 0.6439 & 0.2545 & 0.1312 & 0.4030 & 0.8550 & 0.3341 \\ GraphGPS  & 0.6535 & 0.2500 & - & - & - & 0.3337 \\ Explorer  & 0.6527 & **0.2481** & - & - & - & 0.3637 \\  ViT-PS (Ours) & **0.8311** & 0.2662 & **0.3268** & **0.6693** & **0.9524** & **0.5329** \\   

Table 4: Results for real-world graph tasks. We report test performance at best validation performance.