# A Robust Exact Algorithm for the Euclidean Bipartite Matching Problem

Akshaykumar G. Gattani

Department of Computer Science, Virginia Tech

Sharath Raghvendra

Department of Computer Science, Virginia Tech

Pouyan Shirzadian

Department of Computer Science, Virginia Tech

###### Abstract

Algorithms for the minimum-cost bipartite matching can be used to estimate Wasserstein distance between two distributions. Given two sets \(A\) and \(B\) of \(n\) points in a \(2\)-dimensional Euclidean space, one can use a fast implementation of the Hungarian method to compute a minimum-cost bipartite matching of \(A\) and \(B\) in \((n^{2})\) time. Let \(\) be the spread, i.e., the ratio of the distance of the farthest to the closest pair of points in \(A B\). In this paper, we present a new algorithm to compute a minimum-cost bipartite matching of \(A\) and \(B\) with a similar worst-case execution time of \((n^{2})\). However, when \(A\) and \(B\) are drawn independently and identically from a fixed distribution that is not known to the algorithm, the execution time of our algorithm is, in expectation, \((n^{7/4})\).

To the best of our knowledge, our algorithm is the first one to achieve a subquadratic execution time even for stochastic point sets with real-valued coordinates. Our algorithm extends to any dimension \(d\), where it runs in \((n^{2-}(n))\) time for stochastic point sets \(A\) and \(B\); here \((n)\) is the query/update time of a dynamic weighted nearest neighbor data structure. Our algorithm can be seen as a careful adaptation of the Hungarian method in the geometric divide-and-conquer framework.

## 1 Introduction

Given two distributions \(\) and \(\) defined on the sets \(S_{},S_{}^{d}\), respectively, and an integer \(p 1\), the \(p\)-Wasserstein distance between \(\) and \(\) is the minimum cost required to transport mass from one to the other. More formally, let \((,)\) be the set of all probability measures on \(S_{} S_{}\) with marginal distributions \(\) and \(\). The \(p\)-_Wasserstein distance_ between \(\) and \(\) is defined as

\[W_{p}(,)=(_{(,)}_{S_{} S_{ }}\|s_{1}-s_{2}\|^{p}d(s_{1},s_{2}))^{1/p},\]

where \(\|s_{1}-s_{2}\|\) denotes the Euclidean distance of \(s_{1}\) and \(s_{2}\).

In many applications, the distribution \(\) (resp. \(\)) may not be known, but one may have access to \(n\) i.i.d samples \(A\) (resp. \(B\)) from \(\) (resp. \(\)). The _empirical distribution_\(_{n}\) (resp. \(_{n}\)) is a discrete distribution with \(A\) (resp. \(B\)) as support, where each sample has a mass of \(1/n\). The empirical \(p\)-Wasserstein distance is simply the \(p\)-Wasserstein distance between the empirical distributions \(_{n}\) and \(_{n}\). As \(n\), the empirical \(p\)-Wasserstein distance between \(\) and \(\) converges to the real \(p\)-Wasserstein distance between them  with several results showing sharp upper and lower bounds on the empirical \(p\)-Wasserstein distances . See also for a survey on such results. Due to these properties, empirical \(p\)-Wasserstein distance has found applications in training generative adversarial networks , image retrieval , graph predictions , clustering stability validation , and two-sample tests . The convergence of the empirical distribution to the real distribution, however, exhibits the "curse of dimensionality"; that is, the convergence rate decreases as the dimension increases . The empirical \(p\)-Wasserstein distance therefore is most useful in low-dimensional settings.

Computing the empirical \(p\)-Wasserstein distance can be done by solving an instance of the minimum-cost bipartite matching problem as follows. Consider the complete bipartite graph on \(A\) and \(B\) where each edge from \(a A\) to \(b B\) has a cost of \(\|a-b\|^{p}\). A _bipartite matching_ (or simply a _matching_) \(M\) on \(A B\) is a set of vertex-disjoint edges of \((A,B)\). The matching \(M\) is said to be a _perfect matching_ if \(|M|=n\). We define the cost of a matching \(M\), denoted by \(w_{p}(M)\), as \(w_{p}(M)=_{(a,b) M}\|a-b\|^{p}\). The _pth power Euclidean bipartite matching_ is a perfect matching \(M_{p}^{*}\) with the minimum-cost under \(w_{p}()\). The empirical \(p\)-Wasserstein distance between \(\) and \(\) is

\[W_{p}(_{n},_{n})=w_{p}(M_{p}^{*})^{1/p}.\]

The minimum-cost matching \(M_{p}^{*}\) and therefore, the empirical \(p\)-Wasserstein distance can be computed in \(O(n^{3})\) time using the Hungarian algorithm . In this paper, we adapt the Hungarian algorithm in a geometric divide-and-conquer framework. The execution time of our algorithm is similar to that of the Hungarian algorithm (Remark 3.2). However, when the input point sets \(A\) and \(B\) are samples drawn from the same distribution, the algorithm is asymptotically faster than the Hungarian algorithm.

**Related Work.** Computing an optimal minimum-cost bipartite matching can be done in \(O(n^{3})\) time using the classical Hungarian algorithm . In geometric settings, the efficiency of the Hungarian algorithm can be improved to \((n^{2}(n))\), where \((n)\) is the query and update time of a dynamic weighted nearest neighbor data structure . For two dimensions, \((n)=^{O(1)}n\) and therefore, we get a \((n^{2})\) time exact algorithm. For higher dimensions, however, this leads to only slightly sub-cubic execution time.

For graphs with \(n\) vertices and \(m\) edges, there are faster algorithms that compute an optimal minimum-cost matching  provided the edge costs are integers that are bounded by \(C^{1}\). However, in our setting, the coordinates of the input points are real-valued and the Euclidean distances, due to the presence of a square-root, can be irrational. As a result, these algorithms only provide an approximate solution and the Hungarian algorithm and its fast implementations remain the only known exact algorithm for the problem.

For \(2\)-dimensional points with integer coordinates bounded by \(\), Sharathkumar  presented a _weakly polynomial_\((n^{3/2})\) time algorithm to compute the exact \(1\)-Wasserstein distance. The main idea was to use the approximation algorithm by Sharathkumar and Agarwal  to initially find a planar sub-graph that traps all the edges of the optimal matching. Then, they use the planar separator based algorithm by Lipton and Tarjan  to find a minimum-cost matching in this sub-graph. This algorithm, however, is restricted only to two dimensions and for \(p=1\). The design of a faster algorithm to compute the \(p\)th power Euclidean bipartite matching for \(d\)-dimensional point sets remains a major open problem. We would also note the extensive work on the design of approximation algorithms for the \(1\)-Wasserstein distance  as well as the \(2\)-Wasserstein distance .

**Our Results.** For any point set \(P\) in the Euclidean space, the _spread_ of \(P\) is the ratio of the distance of the farthest pair to the closest pair in \(P\). The main result of this paper is presented in Theorem 1.1.

**Theorem 1.1**.: _There exists a randomized algorithm that, given any two point sets \(A\) and \(B\) sampled independently and identically from a distribution \(\) inside the unit \(d\)-dimensional hypercube, where \(|A|=|B|=n\) and \(\) is not known to the algorithm, computes an exact Euclidean bipartite matching between \(A\) and \(B\) and has an expected running time of \((n^{2-}(n))\); here, \(\) is the spread of the points in \(A B\)._

When \(A\) and \(B\) are i.i.d samples from a distribution \(\) in the unit square (\(d=2\)) and \(\) is not known to the algorithm, our algorithm achieves a weakly polynomial sub-quadratic execution time of \((n^{7/4})\). Our algorithm easily extends to any constant \(p>1\) and any fixed \(d\)-dimensionalspace (we present our result for any arbitrary \(p\) as well as the proof of all our claims in the Appendix). For instance, for \(p=2\) and \(d=2\) as well as for \(p=1\) and \(d=3\), the execution time of our algorithm can be bounded by \((n^{11/6}(n))\). However, for simplicity in exposition of the ideas, we restrict the presentation of our algorithm to two-dimensional (\(d=2\)) and Euclidean settings \((p=1)\). To the best of our knowledge, this is the first sub-quadratic weakly polynomial exact algorithm for computing the \(p\)th power Euclidean bipartite matching for \(2\)-dimensional stochastic point sets. Furthermore, for many distributions, such as the uniform distribution on the unit square, one can show that, with high probability, the spread of the point set is bounded by a polynomial in \(n\). For all such distributions, we achieve a strongly polynomial sub-quadratic exact algorithm.

Our algorithm can be seen as an elegant adaptation of the classical Hungarian algorithm in a quadtree based divide-and-conquer framework. The previous speed-ups of the Hungarian algorithm in geometric settings are based on sophisticated data structures that are hard to implement. In contrast, our geometric improvements are relatively simple, which allows us to implement and compare our algorithm with the standard implementation of the Hungarian algorithm. Experiments suggest that our algorithm outperforms the Hungarian algorithm for synthetic data samples drawn from two (possibly different) distributions \(\) and \(\) as long as the \(1\)-Wasserstein distance between \(\) and \(\) is small. Our algorithm also outperforms the Hungarian algorithm on data samples drawn from the New York Taxi dataset, where one sample is drawn from the set of request pick-up locations (\(\)) and the other sample is drawn from the request drop-off locations (\(\)). Experiments also suggest that our analysis may not be tight, at least for the case where \(=\) is the uniform distribution inside the unit square. Obtaining a tighter analysis of our algorithm remains an important open question. Next, we present an overview of our divide-and-conquer algorithm.

**Our Approach.** The classical Hungarian algorithm is based on the primal-dual framework where, along with a matching \(M\), for every point \(v A B\), the algorithm maintains a non-negative _dual weight_\(y(v)\). The matching \(M\) and dual weights \(y()\) are _feasible_ if, for every edge \((a,b) A B\),

\[y(b)-y(a) \|a-b\|, (a,b) M,\] (1) \[y(b)-y(a) =\|a-b\|, (a,b) M.\] (2)

The Hungarian algorithm initializes \(M\) to be an empty matching and the dual weights of every vertex \(v A B\) to be \(0\). It then incrementally builds a perfect matching \(M\) while maintaining its feasibility and returns the constructed feasible perfect matching. It is well-known that a perfect matching that is feasible is also a minimum-cost matching.

In the Euclidean setting, we interpret the dual weight \(y(v)\) of any point \(v\) as the radius of a disc centered around \(v\). We refer to these discs as the _dual discs_. For any edge \((a,b)\), if the dual disc of \(a\) is in the interior of the dual disc of \(b\) (Figure 1(a)), then \(y(b)-y(a)>\|a-b\|\), making the dual assignment infeasible. Therefore, for any feasible dual weight assignment, the dual disc of \(a\) cannot be completely inside the dual disc of \(b\). The condition (2) corresponds to Figure 1(b) where the boundary of the disc of \(a\) touches the boundary of the disc of \(b\) from inside. Figure 1(c) is an example of an edge that only satisfies (1).

In order to implement the Hungarian algorithm in the geometric divide-and-conquer framework, we can use a quadtree \(\) with the unit square as the root node. Each node of this tree represents a square \(\) and if \(\) has more than one input point, then it has four children that are obtained by splitting \(\) into four equal squares. Squares with no more than one point become a leaf node in this tree.

Figure 1: (a) An infeasible edge, (b) a feasible matching edge satisfying constraint (2), (c) a feasible non-matching edge satisfying constraint (1).

Consider the following naive implementation of the Hungarian algorithm in a quadtree based geometric divide-and-conquer framework. Given a square \(\) of the quadtree \(\), we recursively find a feasible matching in each of the four children. Doing so, however, may lead to a dual weight assignment that violates the feasibility condition for _bridge edges_, i.e., edges that cross the boundary of the children squares. See Figure 2(a) that illustrates a dual feasible assignment at two children squares, which when combined causes the edge \((a^{},b)\) to violate the feasibility condition (1). Thus, implementing the conquer step becomes challenging since we cannot simply combine the feasible dual assignments at the children to achieve a feasible dual assignment at the parent.

To overcome this difficulty, we add an additional condition that restricts the dual disc of a point \(v\) inside any child square \(^{}\) to always stay inside \(^{}\), i.e., \(y(v) d(v,^{})\), where \(d(v,^{})\) is the Euclidean distance of \(v\) to the boundary of \(^{}\). Figure 2(b) shows a restricted dual assignment at the two children squares, which remains feasible when combined at the parent. Such a _restricted feasible matching_ inside each of the four children squares of \(\) trivially combine to form a feasible matching at \(\). Thus, one can apply the divide-and-conquer framework to compute such a restricted feasible matching. Unfortunately, however, the restricted feasible matchings computed at the children may be of an extremely low cardinality. This is particularly true when most of the input points are close to the boundaries of the children squares. See, for instance, in Figure 3(a), the restricted feasible matching is an empty one since most of the blue points are close to the boundary. As a result, the divide step may not make any progress in computing a perfect matching and the conquer step may have to do significant amount of work to find a perfect matching, causing the execution time of the conquer step to be the same as that of the standard implementation of the Hungarian algorithm.

We observe that if the quadtree \(\) is constructed using a _random-shift_, then, in expectation, only sub-linearly many points are close to the boundary of any square of \(\). By combining this with the fact that the average length of a matching edge for \(n\) points drawn from an arbitrary distribution \(\) inside unit square is \( 1/\), we are able to bound the expected number of unmatched points in the restricted matchings by \((n^{3/4})\), leading to an execution time of \((n^{7/4})\) for the conquer steps across all cells at each level of the quadtree and an overall execution time of \((n^{7/4})\).

## 2 Preliminaries

We begin by introducing the notations necessary to describe our algorithm. Given \(A B\) and any square \(\), let \(A_{}=A\), \(B_{}=B\), and \(n_{}=|A_{} B_{}|\). Let \(_{}\) denote the side-length of \(\). For any point \(v A_{} B_{}\) inside \(\), let \((v,)\) denote the Euclidean distance of \(v\) to its closest point on the boundary of \(\). We say that \(v\) is \(\)-close to \(\) if \((v,)_{}\). Let \(n_{}^{}\) denote the number of points of \(A_{} B_{}\) that are \(\)-close to \(\).

**Randomly Shifted Quadtree.** Given the input points \(A B\) inside a unit square, a _randomly shifted quadtree_ on the input can be constructed as follows. Let \(\) be a point chosen uniformly at random from the unit square \(^{2}\). Define the square \(^{*}:=[-4,4]^{2}+\) to be the root of the quadtree \(\). Recursively construct \(\) by decomposing any square \(\) with \(n_{}>1\) into four equal squares, each of which become the children of \(\). Any square with exactly one point becomes a leaf square of the quadtree. Given that the spread of the points in \(A B\) is \(\), the height of \(\) is \(O()\).

Figure 2: (a) Feasible matchings for children that is infeasible when combined at the parent (the edge \((b,a^{})\) is infeasible), (b) Restricted feasible matchings inside the children, which is also restricted feasible when combined at parent.

For any square \(\) of the quadtree \(\), in the next lemma, we show that due to the random shift, there are not many points that are very close to the boundary of \(\).

**Lemma 2.1**.: _For any square \(\) of a randomly shifted quadtree and any \((0,1/2)\), \([n_{}^{}]=O([n_{ }])\)._

**Constrained Matching.** For any square \(\), we introduce a variant of the minimum-cost matching problem, which we refer to by the _minimum-cost \(\)-constrained matching_ problem. Given a square \(\), consider a matching \(M_{}\) of the points in \(A_{} B_{}\). Any point \(v A_{} B_{}\) is a _free_ point with respect to \(M_{}\) if \(v\) is not matched in \(M_{}\). Let \(A_{}^{F}\) (resp. \(B_{}^{F}\)) denote the set of free points of \(A_{}\) (resp. \(B_{}\)) with respect to \(M_{}\). We define the _\(\)-constrained cost_ of the matching \(M_{}\), denoted by \(w_{}(M_{})\), as

\[w_{}(M_{}):=_{(a,b) M_{}}\|a-b\|+_{b B_{ }^{F}}(b,).\] (3)

For any square \(\), the _minimum-cost \(\)-constrained matching (\(\)-MCM)_, denoted by \(M_{}^{*}\), is simply a matching with the minimum \(\)-constrained cost. In Lemma 2.2, we show that for the root square \(^{*}\) of \(\), the minimum-cost \(^{*}\)-constrained matching is also a minimum-cost matching on \(A B\).

For any arbitrary square \(\) of \(\), \(M_{}^{*}\) might be of a very small cardinality. See, for instance in Figure 3(a) where \(B\) is the set of blue points, Equation (3) is minimized when all blue points are free points; i.e., \(w_{}(M_{}^{*})=_{b B}(b,)\) and \(M_{}^{*}\) is an empty matching. Nonetheless, by using the bound on the number of \(\)-close points to any square \(\) of a randomly-shifted quadtree, we can bound the expected number of free points with respect to any \(\)-MCM \(M_{}^{*}\). In the example of Figure 3(b), the solid lines show the boundary of the squares of a randomly-shifted quadtree, and as shown in Figure 3(c), only a few points will be free with respect to the optimal matchings constrained to the squares of the quadtree.

**Lemma 2.2**.: _For any square \(\) of a randomly shifted quadtree and any minimum-cost \(\)-constrained matching \(M_{}^{*}\) (i) the expected number of free points of \(B_{}\) with respect to \(M_{}^{*}\) is \((n^{3/4})\) in \(2\)-dimensions (and \((n^{1-})\) in \(d\) dimensions), and (ii) if \(\) is the root square, then \(M_{}^{*}\) is a minimum-cost perfect matching on \(A B\)._

For the rest of the paper, we design a primal-dual method to compute a \(^{*}\)-MCM, which by Lemma 2.2 is also a minimum cost matching on \(A B\).

**Constrained Feasibility.** Similar to the Hungarian algorithm, we devise a primal-dual method to compute a \(\)-MCM. For any square \(\) of \(\), we say that a matching \(M_{}\) on \(A_{} B_{}\) along with a set of non-negative dual weights \(y()\) for the points in \(A_{} B_{}\) is \(\)-_feasible_ if,

\[y(b)-y(a)\|a-b\|, (a,b) A B,\] (4) \[y(b)-y(a)=\|a-b\|, (a,b) M_{}.\] (5) \[y(b)(b,),  b B_{},\] (6) \[y(a)=0,  a A_{}^{F}.\] (7)

Figure 3: (a) A \(\)-MCM where all points of \(B\) (blue points) are unmatched, (b) the boundaries of the cells of the randomly shifted quadtree (solid line) and the \(\)-close points to the cells of the quadtree (points in the shaded area), (c) the matchings constrained to the cells of the quadtree.

Note that (4) and (5) are conditions identical to the ones maintained by the Hungarian algorithm. Condition (6) ensures that the dual discs stay inside the square and condition (7) ensures that the \(\)-MCM is of minimum cost.

Given a \(\)-feasible matching \(M_{},y()\), we say that any point \(b B_{}\) is a _\(\)-free point_ with respect to \(M_{}\) if \(b\) is not matched in \(M_{}\) and \(y(b)<(b,)\). Note that a free point may not be \(\)-free. For instance, in Figure 4(a), the blue point \(b_{2}\) is free but not a \(\)-free point since its dual weight is equal to its distance to \(\). Let \(_{}^{F}\) denote the set of all \(\)-free points in \(B_{}\). We say that a \(\)-feasible matching \(M_{},y()\) is _\(\)-optimal_ if it does not have any \(\)-free points, i.e., \(_{}^{F}=\). The following lemma shows that any \(\)-optimal matching \(M_{},y()\) is a \(\)-MCM. Thus, we focus on computing a \(\)-optimal matching.

**Lemma 2.3**.: _Let \(M_{},y()\) be a \(\)-optimal matching on \(A_{} B_{}\). Then, \(M_{}\) is a minimum-cost \(\)-constrained matching._

Given a square \(\) of \(\) and its four children \(_{1},_{2},_{3},\) and \(_{4}\), we can obtain a \(\)-feasible matching at \(\) by simply combining the \(_{i}\)-optimal matchings from each child \(_{i}\). We use this property to design our divide-and-conquer algorithm for computing \(\)-optimal matching.

**Lemma 2.4**.: _For any square \(\), let \(_{i}\), \(i\) be the set of all children of \(\) and let \(M_{i},y()\) denote a \(_{i}\)-optimal matching. Then, the matching \(_{i=1}^{4}M_{i},y()\) is a \(\)-feasible matching._

Next, we define slack and an augmenting path with respect to a \(\)-feasible matching.

**Slacks.** For any square \(\), any \(\)-feasible matching \(M_{},y()\), and any pair of points \((a,b) A_{} B_{}\), we define the _edge slack_ on the feasibility conditions for edge \((a,b)\), denoted by \(s(a,b)\), as \(s(a,b)=\|a-b\|-y(b)+y(a)\). We say that \((a,b)\) is _admissible_ if \(s(a,b)=0\). Furthermore, for any point \(b B_{}\), we define the _vertex slack_ of \(b\) with respect to condition (6) to be \(s_{}(b):=(b,)-y(b)\). We refer to the edge slack (resp. vertex slack) as slack when the edge (resp. vertex) is obvious from the context.

While the definition of edge slacks are identical to what is used in the Hungarian algorithm, the definition of vertex slacks is new. Since the dual weights associated with any \(\)-feasible matching satisfies (4)-(7), both edge slacks and vertex slacks will be non-negative.

**Admissible Augmenting Path.** For any square \(\), suppose \(M_{},y()\) is a \(\)-feasible matching on \(A_{} B_{}\). An _alternating path_ with respect to \(M_{}\) is a simple path on \(A_{} B_{}\) whose edges alternate between those in \(M_{}\) and those not in \(M_{}\). An _admissible augmenting path_ (or simply _admissible path_) is any alternating path consisting only of zero slack edges that starts at a \(\)-free point \(b_{}^{F}\) and ends at either (i) a free point \(a A_{}^{F}\) or (ii) a point \(b^{} B_{}\) with a slack of \(0\). We can _augment_ a \(\)-feasible matching \(M_{}\) along an admissible path \(P\) by setting \(M_{} M_{} P\), i.e., we remove all matching edges of \(P\) from \(M_{}\) and add all non-matching edges of \(P\) to \(M_{}\) (see Figure 4). The following lemma shows that augmenting a matching along an admissible path does not violate \(\)-feasibility conditions and also reduces the number of \(\)-free points of \(B_{}\) by one.

Figure 4: (a) A \(\)-feasible matching \(M_{}\) where \(b_{1}\) is a \(\)-free point and \(b_{2}\) is a free point that is not \(\)-free, (b) an admissible path \(P\) (dashed line) from \(b_{1}\) to a point \(b_{3}\) whose vertex slack is zero, and (c) the \(\)-optimal matching obtained from augmenting \(M_{}\) along \(P\).

**Lemma 2.5**.: _Suppose \(M_{},y()\) is a \(\)-feasible matching and \(P\) is an admissible path. After augmenting \(M_{}\) along \(P\), the matching \(M_{},y()\) remains \(\)-feasible. Furthermore, the augmentation reduces the number of \(\)-free points of \(B_{}\) with respect to \(M_{}\) by one._

**Residual Network.** In order to assist in computing admissible paths with respect to a \(\)-feasible matching \(M_{},y()\), we define a _residual network_ of \(A_{} B_{}\) with respect to \(M_{}\) as follows. The vertex set of the residual network is the set of points \(A_{} B_{}\) and a source vertex \(s\). For any pair of points \((a,b) A_{} B_{}\), if \((a,b) M_{}\), we add an edge directed from \(a\) to \(b\) with a weight \(s(a,b)\) to the residual network. Otherwise, if \((a,b) M_{}\), we add an edge directed from \(b\) to \(a\) with a weight \(s(a,b)\). In addition, we add zero-weight edges from the source \(s\) to every \(\)-free point in \(^{F}_{}\). Note that any zero-weight directed path from the source vertex \(s\) to a free point \(a A^{F}_{}\) or a zero-slack point \(b B_{}\) in this residual network is an admissible path.

## 3 Algorithm

In this section, we describe our algorithm for computing an optimal matching on a point set \(A B\). Our algorithm builds a randomly shifted quadtree \(\). For any square \(\) of the quadtree \(\), we denote the set of children of \(\) in \(\) by \([]\).

We describe our divide-and-conquer algorithm with respect to an arbitrary square \(\) in \(\). Our algorithm computes a \(\)-optimal matching \(M_{},y()\).

**Base case.** If \(\) is a leaf of \(\), let \(v\) be the only point in \(\). If \(v B\), set \(y(v)(v,)\) and \(M_{}\). If \(v A\), set \(y(v) 0\) and \(M_{}\) and return (see lines 1-4 of Algorithm 1).

**Divide step.** If \(\) is not a leaf of \(\), for each child \(^{}[]\), recursively compute the \(^{}\)-optimal matching \(M_{^{}},y()\) on \(A_{^{}} B_{^{}}\) (see lines 6 and 7 of Algorithm 1).

**Conquer step.** For any child \(^{}[]\), let \(M_{^{}},y()\) denote the \(^{}\)-optimal matching returned by the algorithm. Set \(M_{}:=_{^{}[]}M_{^{ }}\) as the union of all matchings computed inside the children of \(\). Let \(^{F}_{}\) denote the \(\)-free points of \(B_{}\) with respect to \(M_{}\). To obtain a \(\)-optimal matching, our algorithm iteratively executes the ConstrainedHungarianSearch procedure that adjusts the dual weights to find an admissible path \(P\) with respect to \(M_{}\). Then, the Augment procedure updates \(M_{}\) by augmenting the matching along \(P\). The algorithm continues to search and augment until there are no \(\)-free points of \(B_{}\), i.e., until \(M_{},y()\) is a \(\)-optimal matching. See lines 8-11 of Algorithm 1. We describe the details of the ConstrainedHungarianSearch and Augment procedures below.

_1-_ ConstrainedHungarianSearch _procedure:_ Compute the residual network with respect to \(M_{}\) and execute a Dijkstra's shortest path algorithm starting from the source \(s\). Let \(_{v}\) denote the shortest path distance from \(s\) to \(v\) as computed by the Dijkstra's algorithm. Let

\[=\{_{a A^{F}_{}}_{a},_{b B_{}} _{b}+s_{}(b)\},\]

and let \(u\) denote the point that achieves this minimum. Note that \(u A^{F}_{} B_{}\). Let \(P_{u}\) denote the shortest path from \(s\) to \(u\) and \(P\) denote the path obtained by removing \(s\) from \(P_{u}\). For any \(v A_{} B_{}\), if \(_{v}<\), update its dual weight to \(y(v) y(v)+-_{v}\).

_2-_Augment _procedure:_ Augment \(M_{}\) along \(P\).

Our algorithm returns the matching \(M_{^{*}}\) computed at the root \(^{*}\) of \(\) as the minimum-cost matching between \(A\) and \(B\). This completes the description of our algorithm. The pseudo-code of our divide-and-conquer algorithm is provided in Algorithm 1.

### Proof of correctness

Recollect that for the root square \(^{*}\) of \(\), from Lemma 2.2, a \(^{*}\)-optimal matching is a minimum-cost matching of \(A\) and \(B\). Therefore, it suffices to show that, at each square \(\) of \(\), our algorithm computes a \(\)-optimal matching.

We begin by showing this for any leaf square \(\), which by definition contains exactly one point. Since there are no edges inside \(\), conditions (4) and (5) hold trivially. Let \(v\) be the only point inside \(\)If \(v B_{}\), then \(y(v)=(v,)\) and therefore, condition (6) holds. Otherwise, \(v A_{}\) and our algorithm sets \(y(v)=0\); therefore, condition (7) holds. In both cases, it is easy to see that \(_{}^{F}\) is empty and therefore the empty matching \(M_{}\) and dual weight \(y(v)\) is a \(\)-optimal matching.

For any non-leaf square \(\) of \(\), at the beginning of the conquer step, the algorithm simply combines the constrained optimal matchings computed at its non-empty children. From Lemma 2.4, the resulting matching will be a \(\)-feasible matching. From Lemma 3.1 below, after the execution of the ConstrainedHungarianSearch procedure, the matching \(M_{},y()\) remains a \(\)-feasible matching and the path \(P\) returned by the procedure is an admissible path. From Lemma 2.5, the execution of Augment procedure then reduces the number of \(\)-free points by one while maintaining the \(\)-feasibility conditions. Therefore, during the execution of the conquer step, the matching \(M_{},y()\) remains \(\)-feasible and the number of \(\)-free points reduces by one at each iteration; thus, the algorithm terminates with a \(\)-optimal matching, as desired.

**Lemma 3.1**.: _Given any square \(\) of \(\) and any \(\)-feasible matching \(M_{},y()\), after executing the ConstrainedHungarianSearch procedure on \(\), the updated dual weights remain \(\)-feasible. Furthermore, the returned path \(P\) is an admissible path._

### Proof of Efficiency

For any square \(\), let \(T_{}\) denote the execution time of the conquer step of our algorithm when executed on \(\). Additionally, for any \(i\), let \((i)\) denote the set of all squares of the quadtree that are processed at depth \(i\) of recursion and let \(T_{i}\) denote the total execution time of our algorithm across all such squares, i.e., \(T_{i}=_{(i)}T_{}\).

Recall that the conquer step of our algorithm consists of iterations, where in each iteration, it executes the ConstrainedHungarianSearch procedure followed by the Augment procedure. For any square \(\) of \(\), from Lemma 2.5, each iteration of the conquer step reduces the number of \(\)-free points of \(B_{}\) by one. Therefore, the number of iterations is bounded by the number of \(\)-free points of \(B_{}\), which is at most the total number of free points with respect to the \(^{}\)-MCM computed for all children \(^{}\) of \(\). By invoking Lemma 2.2 on all children of \(\), we bound the expected number of iterations of the conquer step on \(\) by \((n^{3/4})\) for \(2\) dimensions (and \((2^{d}n^{1-})\) for \(d\) dimensions).

Using any dynamic weighted nearest neighbor data structure with an update/query time of \((n_{})\), one can execute the ConstrainedHungarianSearch procedure in \((n_{}(n_{}))\) time [44; 48]. For planar point sets, \((n_{})=( n_{})\) and therefore, each execution of the ConstrainedHungarianSearch procedure takes \((n_{})\) time. Furthermore, the Augment procedure simply augments the matching along an admissible path, which can be done in \(O(n_{})\) time. Therefore, the conquer step of our algorithm executes \((n^{3/4})\) iterations for \(2\) dimensions (and \((2^{d}n^{1-})\) iterations for \(d\) dimensions), in expectation, where each iteration takes \((n_{})\) time for \(2\) dimensions (and \((n_{}(n_{}))\) for \(d\) dimensions). Since \(_{(i)}n_{} n\), \([T_{i}]=_{(i)}[T_ {}]=_{(i)}[T_{}]\)\((_{(i)}n^{3/4}n_{})=(n^ {7/4})\) for \(2\) dimensions (and \([T_{i}]=(2^{d}n^{2-}(n))\) for \(d\) dimensions). Summing over all levels of \(\), the expected execution time of our algorithm is \((n^{7/4})\) for \(2\) dimensions (and \((2^{d}n^{2-}(n))\) for \(d\) dimensions). Using a slightly sophisticated argument which is presented in the Appendix, we can remove the \(2^{d}\) from the execution time for the \(d\)-dimensional settings, leading to Theorem 1.1.

**Remark 3.2**.: _When the input points \(A B\) are chosen arbitrarily, the number of iterations of the conquer step can be \((n_{})\), leading to an execution time of \(O(n_{}^{2}(n_{}))\) per square and \(O(n^{2}(n))\) in total._

## 4 Experiments

In this section, we present the results of our experiments comparing the execution time of our algorithm to that of the Hungarian algorithm. Both algorithms are implemented in Java and share the same data structures2. For both algorithms, we use the classical implementation of Dijkstra's shortest path algorithm used in the Hungarian search procedure and do not use any weighted nearest neighbor data structures. All computations are performed using a single calculation thread on a computer with a 2.6 GHz 6-Core Intel Core i7 CPU and 16 GB of 2667 MHz DDR4 RAM.

**Datasets.** We test our algorithm on \(2\)-dimensional synthetic and real-world datasets. For the synthetic dataset, we use two distributions, namely (i) a uniform distribution defined on the unit square (Uniform), and (ii) a Gaussian distribution constrained to the unit square with a randomly chosen mean inside the unit square and a standard deviation of \(0.25\) (Gaussian). For a real-world dataset, we employ the New York Taxi dataset  and obtain two distributions, namely (i) the distribution of pickup locations (Pickup) and (ii) the distribution of drop-off locations (Drop-off) of passengers. We filtered the datasets by considering trips in seven dates in 2014 with (i) a trip duration of at least \(3\) minutes, and (ii) a trip velocity of at most \(110mph\).

**Tests.** In each test, we conducted experiments using two sets of \(n\) i.i.d samples from distributions \(\) and \(\) within the unit square. We executed both our divide-and-conquer algorithm and the Hungarian algorithm to compute the empirical \(1\)-Wasserstein distance between the same set of samples and compared the execution times. In our first experiment, we used Uniform distribution for both \(\) and \(\) to compare the performance of algorithms when the distributions are the same. For the second experiment on synthetic datasets, we compared running times with different distributions - \(\) being Uniform and \(\) being Gaussian, and in the third experiment, which is on the real-world datasets, \(\) was set to the Pickup distribution while \(\) was the Drop-off distribution. In our experiments on the synthetic datasets, we recorded the number of iterations our algorithm executes at each square of the quadtree to find out how tight our analysis are. The results of our experiments are shown in Figures 5 and 6.

**Results.** As depicted in Figure 5, experiments suggest that our algorithm outperforms the Hungarian algorithm, and the improvement is most significant when the distributions are the same (Figure 5(a)). More generally, experiments suggest that our algorithm performs significantly better than the Hungar

Figure 5: The running time of our algorithm (D&C Hungarian Algorithm) and the Hungarian algorithm for computing the \(1\)-Wasserstein distance between \(\) and \(\), where (a) both \(\) and \(\) are Uniform, (b) \(\) is Uniform and \(\) is Gaussian, and (c) \(\) is Pickup and \(\) is Drop-off.

ian algorithm when the matching cost is low (Figures 5(a) and (c)). The reason for this improvement lies in the fact that the Hungarian algorithm performs a 'global Hungarian search' for matching each point, whereas our algorithm isolates shorter edges of the optimal matching within smaller quadtree sub-problems and execute 'local Hungarian search' for them. Therefore, while the asymptotic improvements are shown for samples from the same distribution, we expect our algorithm to perform better when the optimal matching has many edges with small cost, such as when points are drawn from two similar but not identical distributions. This is evident in our third experiment (Figure 5(c)), where we sample \(n\) pick-up locations and \(n\) drop-off locations from the New York Taxi dataset. Notably, pick-up and drop-off locations tend to follow different distributions, with pick-ups having a higher density around Manhattan.

Additionally, our algorithm handles larger point sets efficiently; for example, it computes the optimal solution for \(50\,000\) points from the Uniform distribution in approximately \(700\) seconds. Finally, in Figure 6, our experiments on samples from the same synthetic dataset suggest that for a square with \(n\) points, the number of iterations of the conquer step of our algorithm might be bounded by \(O(n^{})\) for \( 0.55\). This perhaps suggests that our upper-bound of \(O(n^{3/4})\) may be an overestimate, at least for the uniform and Gaussian distributions.

## 5 Conclusion

In this paper, we adapted the classical primal-dual approaches for computing minimum-cost matching within the divide-and-conquer framework. This approach led to a sub-quadratic weakly-polynomial exact algorithm for computing minimum-cost matching on stochastic point sets. Notably, our algorithm incorporates a randomly shifted quadtree, a structure previously used only in approximation algorithms for \(p\)-Wasserstein distance. We conclude by highlighting some open problems for further investigation.

* An interesting question is whether it is possible to further exploit the geometry of the dual weights and give a tighter bound for the number of iterations of our algorithm at each square; thus, improving the analysis of our algorithm.
* Our analysis only requires the fact that the average length of the matching edges in large cells is small. Stochastic point sets have this property and therefore, we can achieve sub-quadratic algorithms for them. Is there a sub-quadratic exact algorithm for the case where the matching edges are long? Understanding this case may shed light into the design of sub-quadratic exact algorithm for arbitrary inputs.
* Finally, our algorithm is a weakly polynomial algorithm as its running time is dependent on the spread of the point set. Can we remove the dependence on spread while maintaining the simplicity of our algorithm?

Figure 6: The number of iterations of the conquer step for a square with \(n\) points when executed on samples from the same distributions ((e) Uniform and (f) Gaussian).