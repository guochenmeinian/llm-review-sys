# 1. Comparison with In-training AT-based methods.** Although SAU is not comparable with the in-training AT-based method due to the underlying assumption of the threat model, we would like to adopt different threat models for different methods to highlight the advantages of SAU compared to the in-training AT-based methods. Therefore, **we compare SAU with the latest SOTA in-processing AT-based method, \(i.e.\), Composite AT (CAT) [13].

[MISSING_PAGE_FAIL:1]

Gao et al.  found that AT can only be effective for mitigating backdoor attacks with certain trigger patterns and may even increase the vulnerability to backdoor attacks when training from scratch on poisoned datasets. Zeng et al.  proposed to unlearn the universal adversarial perturbation (UAP) to remove backdoors from poisoned models. Since directly unlearning UAP leads to highly unstable performance for mitigating backdoors, they employ implicit hyper gradient to solve the min-max problem and achieve remarkable performance. However, their method assumes that the same trigger can activate the backdoor regardless of the samples it is planted on, and therefore lacks a guarantee against more advanced attacks that use sample-specific and/or non-additive triggers.

In this paper, we consider the problem of purifying a poisoned model. After investigating the relationship between adversarial examples and poisoned samples, a new upper bound for backdoor risk to the fine-tuned model is proposed. Specifically, by categorizing adversarial examples into three types, we show that the backdoor risk mainly depends on the shared adversarial examples that mislead both the fine-tuned model and the poisoned model to the same class. Shared adversarial examples suggest a novel upper bound for backdoor risk, which combines the shared adversarial risk and vanilla adversarial risk. Besides, the proposed bound can be extended with minor modifications to universal adversarial perturbation and targeted adversarial perturbation. Based on the new bound, we propose a bi-level formulation to fine-tune the poisoned model and mitigate the backdoor. To solve the bi-level problem, we proposed **S**hared **A**dversarial **U**nlearning (SAU). SAU first identifies the adversarial examples shared by the poisoned model and the fine-tuned model. Then, to break the connection between poisoned samples and the target label, the shared adversarial examples are unlearned such that they are either classified correctly by the fine-tuned model or differently by the two models. Moreover, our method can be naturally extended to defend against backdoor attacks with multiple triggers and/or multiple targets. To evaluate our method, we compare it with six state-of-the-art (SOTA) defense methods on seven SOTA backdoor attacks with different model structures and datasets. Experimental results show that our method achieves comparable and even superior performance to all the baselines.

Our contributions are three folds: **1)** We analyze the relationship between adversarial examples and poisoned samples, and derive a novel upper bound for the backdoor risk that can be generalized to various adversarial training-based methods for backdoor defense; **2)** We formulate a bi-level optimization problem for mitigating backdoor attacks in poisoned models based on the derived bound, and propose an efficient method to solve it; **3)** We conduct extensive experiments to evaluate the effectiveness of our method, and compare it with six state-of-the-art defense methods on seven challenging backdoor attacks with different model structures and datasets.

## 2 Related work

Backdoor attack.Backdoor attack is one of the major challenges to the security of DNNs. The poisoned model behaves normally on clean inputs but produces the target output when the trigger pattern is present. Based on the types of triggers, backdoor attacks can be classified into two types: fixed-pattern backdoor attacks and sample-specific backdoor attacks. BadNets  is the first backdoor attack that uses fixed corner white blocks as triggers. To improve the stealth of the triggers, Blended  is proposed to blend the trigger with the image in a weighted way. Since fixed-pattern triggers can be recognized easily, sample-specific backdoor attacks have been proposed. SSBA , WaNet , LF , IRBA , VSSC  and TAT  use different techniques to inject unique triggers for different samples from different angles. Sleeper-agent  and Lira  optimize the target output to obtain more subtle triggers. Recently, Zhu et al.  proposed a learnable poisoning sample selection strategy to further boost the effect of backdoor attacks. To keep the label of the backdoor image matching the image content, LC  and SIG  use counterfactual and other methods to modify the image to deploy clean label attacks.

Backdoor defense.Backdoor defense aims to reduce the impact of backdoor attacks on deep neural networks (DNNs) through training and other means. There are three types of backdoor defense: pre-processing, in-processing, and post-processing. Pre-processing backdoor defense aims to identify the poisoned samples in the training dataset. For instance, AC  filters out the poisoned samples by the abnormal activation clustering phenomenon in the target class; Confusion Training  identifies the poisoned samples by training a poisoned model that only fits the poisoned samples; VDC  incorporates multimodal large language models to detect the poisoned samples. In-processingbackdoor defense reduces the effect of the backdoor during training. For instance, ABL  exploits the fact that the learning speed of backdoor samples is faster than that of the clean sample, and splits some poisoned samples to eliminate the backdoor by forgetting these poisoned samples; DBD  divides the backdoor training process and directly inhibits the backdoor learning process; D-ST  splits the backdoor samples and uses semi-supervised learning by observing that the clean samples are more robust to image transformation than the backdoor samples. Post-processing backdoor defense mitigates the effect of backdoors for a poisoned model by pruning or fine-tuning. For example, FP  prunes some potential backdoor neurons and fine-tunes the model to eliminate the backdoor effect; ANP  finds the backdoor neurons by adversarial perturbation to model weights; EP  and CLP  distinguish the characteristics of the backdoor neurons from the clean neurons; NAD  uses a mild poisoned model to guide the training of the poisoned model to obtain a cleaner model; I-BAU  finds possible backdoor triggers by the universal adversarial attack and unlearn these triggers to purify the model; FT-SAM  boosts the performance of fine-tuning for backdoor mitigation by incorporating sharpness-aware minimization; NPD  learns a lightweight linear transformation layer by solving a well designed bi-level optimization problem to defend against backdoor attack.

Adversarial training.In adversarial training, models are imposed to learn the adversarial examples in the training stage and therefore, resistant to adversarial attacks in the inference stage. In one of the earliest works , the adversarial examples are generated using Fast Gradient Sign Method. In , PGD-AT is proposed, which generates adversarial examples by running FGSM multiple steps with projection and has become the most widely used baseline for adversarial training. Some further improvements of PGD-AT include initialization improvement [21; 23], attack strategy improvement , and efficiency improvement [53; 62].

## 3 Methodology

In Section 3.1, we first introduce notations, threat model, and defense goal to formulate the problem. By investigating the relationship between adversarial risk and backdoor risk, a new upper bound of backdoor risk is derived in Section 3.2, from which a bi-level formulation is proposed in Section 3.3.

### Preliminary

Notations.We consider a \(K\)-class (\(K 2\)) classification problem that aims to predict the label \(y\) of a given sample \(\), where \(=[1,,K]\) is the set of labels and \(\) is the space of samples. Let \(h_{}:\) be a DNN classifier with model parameter \(\). We use \(\) to denote the set of data \((,y)\). For simplicity, we use \(\) as a abbreviation for \((,y)\). Then, for a sample \(\), its predicted label is

\[h_{}()=*{arg\,max}_{k=1,,K}_{k}( ;),\] (1)

where \(_{k}(;)\) is the (softmax) probability of \(\) belonging to class \(k\).

Threat model.Let \(\) be the set of triggers and define \(g:\) as the generating function for poisoned samples. Then, given a trigger \(\) and a sample \(\), one can generate a poisoned sample \(g(;)\). For simplicity, we only consider all to one case, \(i.e.\), there is only one trigger \(\) and one target label \(\). The all to all case and multi-trigger case are left in **Appendix** A. We assume that the attacker has access to manipulate the dataset and/or control the training process such that the trained model classifies the samples with pre-defined trigger \(\) to the target labels \(\) while classifying clean samples normally. In addition, we define the poisoning ratio as the proportion of poisoned samples in the training dataset.

Defense goal.We consider a scenario where a defender is given a poisoned model with parameter \(_{bd}\) and a small set of _clean_ data \(_{cl}=\{(_{i},y_{i})\}_{i=1}^{N}\). Since we are mainly interested in samples whose labels are not the target label, we further define the set of non-target samples as \(_{-}=\{(,y)|(,y)_{cl},y\}\). Note that the size of \(_{cl}\) is small and the defender cannot train a new model from scratch using only \(_{cl}\). The defender's goal is to purify the model so that the clean performance is maintained and the backdoor effect is removed or mitigated. We assume that the defender cannot access the trigger \(\) or the target label \(\).

Problem formulation.Using the \(0\)-\(1\) loss , the classification risk \(_{cl}\) and the **backdoor risk**\(_{bd}\) with respect to classifier \(h_{}\) on \(_{cl}\) can be defined as:

\[_{cl}(h_{})=_{i=1}^{N}(h_{}(_{i}) y_{i}),_{bd}(h_{})=^{N}(h_{}(g(_{i},))=,_ {i}_{-})}{|_{-}|}\] (2)

where \(\) is the indicator function and \(||\) denotes the cardinality of a set.

In (2), the classification risk concerns whether a clean sample is correctly classified, while the backdoor risk measures the risk of classifying a poisoned sample from \(_{-}\) to target class \(\).

In this paper, we consider the following problem for purifying a poisoned model:

\[_{}_{cl}(h_{})+_{bd}(h _{}),\] (3)

where \(\) is a hyper-parameter to control the tradeoff between classification risk and backdoor risk.

However, directly solving (3) is impractical due to the lack of the trigger \(\) and target label \(\). Therefore, a natural choice is to replace \(_{bd}\) with a surrogate risk irrelevant to \(\) and \(\).

### Connection between backdoor risk and adversarial risk

To tackle the above problem, we propose a novel upper bound of backdoor risk which can be relaxed to a surrogate of \(_{bd}\) that is independent of \(\) and \(\). Before that, we first decompose the adversarial risk and review the connection between backdoor risk and adversarial risk.

Adversarial risk.Given a sample \(\) and an adversarial perturbation \(\), an adversarial example \(_{}}\) can be generated by \(_{}}=+\), where \(\) is the set of perturbations. We define the set of adversarial example-label pair to \(h_{}\) generated by perturbation \(\) and dataset \(_{cl}\) by \(_{,}=\{(_{}},y)| h_{}(_{}}) y,(,y)_{cl}\}\) and abbreviate \((_{}},y)_{,}\) by \(_{}}_{,}\). Then, the vanilla adversarial risk for \(h_{}\) on \(_{-}\) can be defined as

\[_{adv}(h_{})=^{N}_{_{ i}}(_{i}}_{_{i}}_{ _{i},},_{i}_{-})}{| _{-}|}.\] (4)

To bridge the adversarial examples and poison samples, we provide the following assumption:

**Assumption 1**.: _Assume that \(g(;)-\) for \(_{cl}\)._

Assumption 1 ensures that there exists \(\) such that \(+=g(;)\). Note that **the analysis and the proposed method are independent of any specific types of perturbation**. More discussions about the choice of \(\) and its influence on the proposed method are discussed in Appendix B.4.

Then, we reach the first upper bound of backdoor risk:

**Proposition 1**.: _Under Assumption 1, \(_{adv}\) serves as an upper bound of \(_{bd}\), i.e., \(_{bd}_{adv}\)._

Remark.Although \(_{adv}\) seems to be a promising surrogate for \(_{bd}\), it neglects some essential connections between adversarial examples and poisoned samples, resulting in a significant gap between adversarial risk and backdoor risk, as shown in Figure 1. _Such a gap also leads to a fluctuating learning curve for backdoor risk and a significant drop in accuracy in the backdoor mitigation process by adversarial training._ Therefore, we seek to construct a tighter upper bound for \(_{bd}\). A key insight is that not all adversarial examples contribute to backdoor mitigation.

To further identify AEs important for mitigating backdoors, we leverage the information from the poisoned model to categorize the adversarial examples to \(h_{}\) into three types, as shown in Table 1.

Figure 1: Example of purifying poisoned model using adversarial training on Tiny ImageNet . The curves for Accuracy, Backdoor Risk, and Adversarial Risk are indicated by Green, Orange, and Purple, respectively.

Furthermore, we refer adversarial examples of Type I as the shared adversarial examples between \(h_{}\) and \(h_{_{bd}}\), as defined below:

**Definition 1** (Shared Adversarial Example).: _Given two classifiers \(h_{_{1}}\) and \(h_{_{2}}\), an adversarial example \(}_{}\) is shared between \(h_{_{1}}\) and \(h_{_{2}}\) if and only if \(h_{_{1}}(}_{})=h_{_{2}}( {}_{}) y\)._

Let \(_{s}\) be the subset of samples in \(_{-}\) on which planting a trigger can successfully activate the backdoor in \(h_{_{bd}}\), as summarized in Table 2. As illustrated in Figure 2, the following proposition reveals a deeper connection between adversarial examples and poisoned samples.

**Proposition 2**.: _For \((,y)_{s}\), the poisoned sample \(g(;)\) can attack \(h_{}\) if and only if \(h_{}(g(;))=h_{_{bd}}(g(;)) y\). Furthermore, under Assumption 1, the poisoned sample \(g(;)\) is a shared adversarial example between \(h_{}\) and \(h_{_{bd}}\), if it can attack \(h_{}\)._

Then, the adversarial risk \(_{adv}\) can be decomposed to three components as below:

\[_{adv}(h_{})=_{-}|}_{i=1}^{N}_{_{i}}(x_{i}_{s})(h_{}(}_{ i,_{i}})=h_{_{bd}}(}_{i,_{i}}), }_{i,_{i}}_{_{i},})}_{_{s}}\] (5)

Proposition 2 implies that the first component in (5) is essential for backdoor mitigation, as it captures the risk of shared adversarial examples between the poisoned model \(h_{_{bd}}\) and the fine-tuned model \(h_{}\), and the poisoned samples effective for \(h_{}\) belong to SAEs. The second component is irrelevant

   Type & Description & Definition \\  I & Mislead \(h_{}\) and \(h_{_{bd}}\) to the same class & \(h_{_{bd}}(}_{})=h_{_{bd}}( }_{}) y\) \\ II & Mislead \(h_{}\), but not mislead \(h_{_{bd}}\) & \(h_{_{bd}}(}_{}) h_{}( }_{}),h_{_{bd}}(}_{})=y\) \\ III & Mislead \(h_{}\) and \(h_{_{bd}}\) to different classes & \(h_{_{bd}}(}_{}) h_{}( }_{}),h_{_{bd}}(}_{}) y,h_{}(}_{}) y\) \\   

Table 1: Different types of adversarial examples to \(h_{}\).

   Notation & Description/Definition \\  \(h_{_{bd}}\) & Poisoned model \\ \(h_{}\) & Purified model \\ \(\) & Trigger \\ \(\) & Target label \\ \(g(,)\) & Poisoned sample generated from \(\) \\ \(\) & Set of perturbations \\ \(\) & Perturbation \\ \(}_{}\) & Adversarial example, \(+\) \\ \(_{cl}\) & \(\{(_{i},y_{i})\}_{N=1}^{N}\) \\ \(_{-}\) & \(\{(,y)|(|,y)_{cl},y\}\) \\ \(_{s}\) & \(\{(,y)|h_{_{bd}}(g(;))=,_{-}\}\) \\ \(_{,}\) & \(\{(}_{},y)|h_{}(}_{}) y,(,y)_{cl}\}\) \\   

Table 2: Table of notations.

Figure 2: A schematic of the relationship between adversarial examples, shared adversarial examples (SAEs, Type I) and poisoned samples. The adversarial examples for \(h_{_{bd}}\) and \(h_{}\) are shown in the blue and green solid ellipses, respectively. The poisoned samples are in the black dashed circle. Assume that \(h_{_{bd}}\) and \(h_{}\) have \(100\%\) backdoor attack success rates in the left, such that all poisoned samples are contained in SAEs. Thus, by reducing the shared adversarial examples between \(h_{_{bd}}\) and \(h_{}\) (from left to right), the backdoor risk can be mitigated in \(h_{}\).

for backdoor mitigation, since adversarial examples of Type II and III are either not poisoned samples or ineffective to backdoor attack against \(h_{}\). The third component protects the samples that are originally resistant to triggers from being backdoor attacked in the mitigation process.

Thus, by removing the second component in \(()\), we propose the following sub-adversarial risk

\[_{sub}(h_{})=& _{-}|}_{i=1}^{N}_{_{i} }(_{i}_{s})(h_{ }(}_{i,_{i}})=h_{_{bd}}( }_{i,_{i}}),}_{i,_{i}} _{_{i},})\\ &+(_{i}_{-} _{s},}_{i,_{i}}_{_{i},})}.\] (6)

Compared to vanilla adversarial risk \(_{adv}\), the proposed sub-adversarial risk \(_{sub}\) focuses on the shared adversarial examples for samples in \(_{s}\) while considering vanilla adversarial examples for samples in \(_{-}_{s}\). When \(_{s}=_{-}\), \(i.e.\), \(h_{_{bd}}\) has backdoor risk \(100\%\), the sub-adversarial risk measures the shared adversarial risk on \(_{-}\).

Then, we provide the following proposition to establish the relation between backdoor risk and (sub) adversarial risk:

**Proposition 3**.: _Under Assumption 1, for a classifier \(h_{}\), the following inequalities hold_

\[_{bd}(h_{})_{sub}(h_{}) _{adv}(h_{}).\]

Therefore, \(_{sub}\) is a tighter upper bound for \(_{bd}\) compared with \(_{adv}\). After replacing \(_{bd}\) with \(_{sub}\) in (3), we reach the following optimization problem:

\[_{}_{cl}(h_{})+_{sub}( h_{}).\] (7)

Due to the space limit, the proofs of the above propositions, as well as some detailed discussions and extensions will be provided in Appendix A.

### Proposed method

Since the target label \(\) is unavailable and 0-1 loss is non-differentiable, we first discuss the relaxation of \(_{sub}\) and then replace 0-1 loss with suitable surrogate loss functions in this section.

Relaxation.One limitation of \(_{sub}\) is the inaccessibility of \(_{s}\), \(i.e.\), the subset of samples in \(_{-}\) on which planting a trigger can successfully activate the backdoor in \(h_{_{bd}}\). Since a poisoned model usually has a high attack success rate (ASR), the first relaxation is replacing \(_{s}\) with \(_{-}\), \(i.e.\),

\[_{sub}(h_{})_{-}|} _{i=1}^{N}_{_{i}}( _{i}_{-})(h_{}(}_{i, _{i}})=h_{_{bd}}(}_{i,_{i}} ),}_{i,_{i}}_{_{i},})}.\]

Since \(\) is unavailable, we also replace \(_{-}\) by \(_{cl}\) in the sub-adversarial risk. As \(g(;)\) is not malicious for classification if the ground truth label of \(\) is \(\), this relaxation has negligible negative influence for mitigating backdoor risk. After the above two relaxations, we reach the following shared adversarial risk on \(_{cl}\):

\[_{share}(h_{})=_{i=1}^{N}_{_{i}}(h_{}(}_ {i,_{i}})=h_{_{bd}}(}_{i,_{ i}}),}_{i,_{i}}_{_{i},})}.\]

Due to the space limit, we postpone the detailed discussion of the above two relaxations (\(e.g.\), the relaxation gaps) in Appendix A.7.

By replacing \(_{sub}\) with \(_{share}\) in (7), we reach the following problem:

\[_{}_{cl}(h_{})+_{share}(h _{}).\] (8)

By solving (8), we seek \(h_{}\) that either classifies the adversarial examples correctly (\(i.e.\), \(h_{}(}_{})=y\)), or differently with \(h_{_{bd}}\) (\(i.e.\), \(h_{}(}_{}) h_{_{bd}}(}_{})\)), while maintaining a high accuracy on \(_{cl}\).

Approximation.We firstly need to approximate two indicators in (8), including: 1) \((h_{}() y)\); 2) \((}_{}_{,},h_{}(}_{})=h_{_{bd}}( }_{}))\). The former indicator is for the clean accuracy, so we use the widely used cross-entropy (CE) loss as the surrogate loss, \(i.e.\), \(L_{cl}(,y;)=((;),y)\). In terms of the latter indicator that measures the shared adversarial risk, different surrogate losses could be employed for different usages:

* To generate shared adversarial examples, we first decompose the second indicator as below: \[(}_{}_{,}_{,_{bd}})(h_{}(}_{})=h_{_{bd}}(}_ {})),\] (9) which covers two components standing for fooling \(h_{_{bd}}\) and \(h_{}\) simultaneously, and keeping the same predicted label for \(h_{_{bd}}\) and \(h_{}\), respectively. For the first component, we use CE on both \(h_{}\) and \(h_{_{bd}}\) as a surrogate loss, \(i.e.\), \[L_{adv}(}_{},y;)=( {CE}((}_{};),y)+(( }_{};_{bd}),y)).\] (10) For the second component, we measure the distance between \((}_{i,_{i}};)\) and \((}_{};_{bd})\) by Jensen-Shannon divergence  and adopt the following surrogate loss: \[L_{share}(}_{},y;)=-(( }_{};),(}_{};_{bd})).\] (11)
* To unlearn the shared adversarial examples, we adopt the following surrogate loss \[L_{sar}(}_{},y;)=-( {y} y)(1-_{}(}_{} ;)),\] (12) where \(=h_{_{bd}}(}_{})\). By reducing \(L_{sar}\), the prediction of \(}_{}\) by \(h_{}\), \(i.e.\), \(h_{}(}_{})\) is forced to differ from \(h_{_{bd}}(}_{})\) if \(h_{_{bd}}(}_{}) y\), therefore, reducing the shared adversarial risk (SAR).

**Overall objective.** By combining all the above surrogate losses with the linear weighted summation mode, we propose the following bi-level objective:

\[_{}&_{i=1 }^{N}\{_{1}L_{cl}(_{i},y_{i};)+_{2}L_{sar} (}_{i,_{i}^{*}},y_{i};)\}\\ &_{i}^{*}=*{arg \,max}_{_{i}}_{3}L_{adv}(}_{i,_{i}},y_{i};)+_{4}L_{share}(}_{i,_{i}},y_{i};), i=1,,N,\] (13)

where \(_{1},_{2},_{3},_{4} 0\) indicate trade-off weights.

Optimization algorithm.We propose an optimization algorithm called **Shared Adversarial Unlearning** (SAU, Algorithm 1), which solves (13) by alternatively updating \(\), and \(^{*}\). Specifically, the model parameter \(\) is updated using stochastic gradient descent , and the perturbation \(^{*}\) is generated by projected gradient descent .

``` Input: Training set \(_{cl}\), poisoned model \(h_{_{bd}}\), PGD step size \(>0\), number of PGD steps \(T_{adv}\), perturbation set \(\), max iteration number \(T\).  Initialize \(h_{}=h_{_{bd}}\). for\(t=0,...,T-1\)do for Each mini-batch in \(_{cl}\)do  Initialize perturbation \(\). for\(t_{adv}=0,...,T_{adv}-1\)do  Compute gradient \(\) of \(\) w.r.t. the inner maximization objective in (13).  Update \(=_{}(+())\) where \(\) is the projection operation. endfor  Update \(\) w.r.t. the outer minimization objective in (13). endfor endfor ```

**Algorithm 1** Shared Adversarial UnlearningExperiment

### Experiment settings

Backdoor attack.We compare SAU with 7 popular state-of-the-art (SOTA) backdoor attacks, including BadNets , Blended backdoor attack (Blended) , input-aware dynamic backdoor attack (Input-Aware), low frequency attack (LF) , sinusoidal signal backdoor attack (SIG) , sample-specific backdoor attack (SSBA) , and warping-based poisoned networks (WaNet) . To make a fair and trustworthy comparison, we use the implementation and configuration from BackdoorBench , a comprehensive benchmark for backdoor evaluation. By default, the poisoning ratio is set to 10% in all attacks, and the \(0^{th}\) label is set to be the target label. We evaluate all the attacks on 3 benchmark datasets, CIFAR-10 , Tiny ImageNet , and GTSRB  using two networks: PreAct-ResNet18  and VGG19 . Due to space constraints, the results for GTSRB and VGG19 are postponed to **Appendix** D. Note that for clean label attack SIG, the 10% poisoning ratio can only be implemented for CIFAR-10. More attack details are left in **Appendix** C.

Backdoor defense.We compare our method with 6 SOTA backdoor defense methods: ANP , Fine-pruning (FP) , NAD , NC , EP  and i-BAU . By default, all the defense methods can access 5% benign training data. We follow the recommended configurations for SOTA defenses as in BackdoorBench . For our method, we choose to generate the shared adversarial example with Projected Gradient Descent (PGD)  with \(L_{}\) norm. For all experiments, we run PGD 5 steps with norm bound \(0.2\) and we set \(_{1}=_{2}=_{4}=1\) and \(_{3}=0.01\). More details about defense settings and additional experiments can be found in **Appendix** C and D.

Evaluation metric.We use four metrics to evaluate the performance of each defense method: Accuracy on benign data (**ACC**), Attack Success Rate (**ASR**), Robust Accuracy (**R-ACC**) and Defense Effectiveness Rating (**DER**). R-ACC measures the proportion of the poisoned samples classified to their true label, and ASR measures the proportion of poisoned samples misclassified to the target label. Larger R-ACC and lower ASR indicate that the backdoor is effectively mitigated. Note that the samples for the target class are excluded when computing the ASR and R-ACC as done in BackdoorBench. DER \(\) was proposed in  to evaluate the cost of ACC for reducing ASR. It is defined as follows:

\[=[(0,)-(0,)+1]/2,\] (14)

where \(\) denotes the drop in ASR after applying defense, and \(\) represents the drop in ACC after applying defense.

**Note**: Higher ACC, lower ASR, higher R-ACC and higher DER represent better defense performance. We use **boldface** and underline to indicate the best and the second-best results among all defense methods, respectively, in later experimental results.

### Main results

Effectiveness of SAU.To verify the effectiveness of SAU, we first summarize the experimental results on CIFAR-10 and Tiny ImageNet in Tables 3 and 4, respectively. As shown in Tables 3-4, SAU can mitigate backdoor for almost all attacks with a significantly lower average ASR. For the experiments on CIFAR-10, SAU achieves the top-2 lowest ASR in 4 of 7 attacks and very low ASR for the other 3 attacks. Similarly, SAU performs the lowest ASR in 3 attacks for Tiny ImageNet and negligible ASR in two of the other attacks. Notably, SAU fails to mitigate WaNet in Tiny ImageNet, although it can defend against WaNet on CIFAR-10. As a transformation-based attack that applies image transforms to construct poisoned samples, WaNet can generate triggers with a large \(L_{}\) norm. Specifically, WaNet has average \(L_{}\) trigger norm \(0.348\) on TinyImageNet and \(0.172\) on CIFAR-10. Note that for all experiments, we generate adversarial perturbation with \(L_{}\) norm less than \(0.2\). The large trigger norm of WaNet on Tiny ImageNet poses a challenge for AT-based methods such as i-BAU and the proposed one, which reveals an important weakness of such methods, \(i.e.\), their performance may be degraded if the trigger is beyond the perturbation set. A promising approach for this challenge is to consider adversarial examples for the union of multiple perturbation types [34; 45] and we postpone more discussion to **Appendix** B.

As maintaining clean accuracy is also important for an effective backdoor defense method, we also compare SAU with other baselines in terms of ACC, R-ACC and DER in Table 3 and 4. As SAUadopts adversarial examples to mitigate backdoors, it has negative effects on clean accuracy, resulting in a slightly lower clean accuracy compared to the best one. However, SAU achieves the best average R-ACC, which demonstrates its effectiveness in recovering the prediction of poisoned samples, \(i.e.,\) classifying the poisoned samples correctly. Besides, the best average DER indicates that SAU achieves a significantly better tradeoff between clean accuracy and backdoor risk.

Influence of poisoning ratio.To study the influence of the poisoning ratio on the effectiveness of SAU, we test SAU with varying poisoning ratios from 10% to 50%. As shown in Table 5, SAU is still able to achieve remarkable performance even if half of the data is poisoned.

   Defense &  &  &  &  \\  Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\  BadNets  & 56.23 & 100.0 & 0.0 & N/A & 43.45 & **0.00** & 43.13 & 93.61 & 51.73 & 99.99 & 0.01 & 47.76 & 51.52 & 0.10 & 50.82 & 97.59 \\ Blended  & 56.03 & 99.71 & 0.22 & N/A & 43.93 & 61.11 & 17.28 & 90.75 & 51.89 & 59.54 & 2.1 & 49.81 & 52.55 & 83.21 & 3.96 & 51.51 \\ Input-aware  & 57.45 & 98.58 & 1.06 & N/A & 35.90 & **0.10** & 1.82 & 88.00 & 52.82 & 62.92 & 24.67 & 68.88 & 56.20 & 0.09 & **51.29** & 98.26 \\ LF  & 55.97 & 85.97 & 0.97 & N/A & 45.69 & 62.30 & 14.49 & 63.00 & 51.44 & 59.55 & 2.42 & 49.42 & 59.85 & 56.87 & 55.02 \\ SSA  & 55.22 & 97.17 & 1.68 & N/A & 43.36 & 56.53 & 17.24 & 64.66 & 50.74 & 88.87 & 6.26 & 52.04 & 52.47 & 52.47 & 32.17 & 70.75 \\ Wawet  & 56.78 & 99.49 & 0.36 & N/A & 36.16 & **0.01** & 37.97 & 89.04 & 38.34 & 39.3 & 96.33 & 0.23 & 51.63 & 97.90 \\ Average & 56.28 & 99.06 & 7.22 & N/A & 41.33 & 20.83 & 22.66 & 77.12 & 52.44 & 74.48 & 63.81 & 58.88 & **53.18** & 38.78 & 31.64 & 74.50 \\   Defense &  &  &  &  \\  Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\  BadNets  & 46.37 & 0.27 & 45.61 & 94.93 & 52.30 & 0.03 & **52.05** & **98.02** & **52.67** & 98.31 & 1.59 & 49.06 & 51.52 & 0.53 & 51.15 & 97.35 \\ Blended  & 46.89 & 94.99 & 2.63 & 74.79 & 51.86 & 60.63 & 12.31 & 69.75 & **52.73** & 29.66 & 4.15 & 51.72 & 50.30 & **0.06** & **25.27** & **94.96** \\ Input-aware  & 47.91 & 1.86 & 43.13 & 93.57 & **51.23** & 20.44 & 27.19 & **92.00** & 56.35 & 21.26 & 48.47 & 67.96 & 54.11 & 0.33 & **42.29** & 97.59 \\ LF  & 45.45 & 50.49 & 20.21 & 68.78 & **53.33** & 75.41 & 13.06 & 60.26 & 52.77 & 88.04 & 6.82 & 53.67 & 52.65 & **0.97** & **35.87** & **97.14** \\ SABA  & 45.32 & 57.32 & 19.14 & 65.25 & **48.13** & 40.47 & 29.13 & 73.27 & **52.52** & **47.77** & 41.34 & 60.12 & 51.55 & **81.11** & **36.36** & **97.11** \\ WaNet  & 46.98 & 0.43 & 43.99 & 64.53 & **62.1** & 0.23 & **55.13** & **99.34** & 54.04 & 94.86 & 3.92 & 51.12 & 54.65 & 85.75 & 10.35 & 55.80 \\ Average & \(46.49\) & 34.23 & 29.17 & 73.59 & 53.18 & 29.33 & 29.88 & 78.55 & 52.62 & 84.18 & 9.28 & 54.81 & 52.51 & **14.62** & **33.55** & **84.57** \\   

Table 4: Results on Tiny ImageNet with PreAct-ResNet18 and poisoning ratio \(10\%\).

   Defense &  &  &  &  \\  Attack & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER & ACC & ASR & R-ACC & DER \\  BadNets  & 91.32 & 95.03 & 4.67 & N/A & 90.88 & 4.88 & 8.722 & 94.86 & **91.31** & 57.13 & 41.62 & 68.95 & 89.05 & 1.27 & 89.16 & 95.75 \\ Blended  & 93.47 & 99.92 & 0.08 & N/A & 92.97 & 84.88 & 13.36 & 57.22 & 83.37 & 99.26 & 0.73 & 50.18 & **93.47** & 99.92 & 0.08 & 50.00 \\ Input-aware  & 90.67 & 98.26 & 1.66 & N/A & 91.04 & 1.32 & 86.71 & 98.47 & 97.44 & **0.

### Ablation Study

In this section, we conduct ablation study to investigate each component of SAU. Specifically, SAU is composed of two parts: generating shared adversarial examples according to the maximization objective in (13) denoted by **A**, and reducing shared adversarial risk according to the minimization objective in (13), denote by **B**. To study the effect of these parts, we replace them with the corresponding part in vanilla adversarial training, denoted by **C** and **D**, respectively. Then, we run each variant \(20\) epochs on Tiny ImageNet with PreAct-ResNet18 and a poisoning ratio \(10\%\). Each experiment is repeated five times, and the error bar is only shown in Figure 3 for simplicity.

As shown in Table 6, the maximization objective for generating shared adversarial examples (component **A**) is the key to reducing ASR, and component **B** can help to alleviate the hurt to clean accuracy. When replacing the component **B** with the minimization step in vanilla AT (**D**), it suffers from a drop of clean accuracy, although it can also work for mitigating backdoor effectively. Compared to vanilla AT, SAU achieves significantly lower ASR and higher ACC, with a much more stable learning curve, as shown in Figure 3. We remark that the decrease of clean accuracy for **A+D** and vanilla AT demonstrates that imposing a model to learn overwhelming adversarial examples may severely hurt the clean accuracy, a phenomenon widely studied in the area of adversarial training .

## 5 Conclusion

In conclusion, this paper proposes Shared Adversarial Unlearning, a method to defend against backdoor attacks in deep neural networks through adversarial training techniques. By developing a better understanding of the connection between adversarial examples and poisoned samples, we propose a novel upper bound for backdoor risk and a bi-level formulation for mitigating backdoor attacks in poisoned models. Our approach identifies shared adversarial examples and unlearns them to break the connection between the poisoned sample and the target label. We demonstrate the effectiveness of our proposed method through extensive experiments, showing that it achieves comparable to, and even superior, performance than six different state-of-the-art defense methods on seven SOTA backdoor attacks with different model structures and datasets. Our work provides a valuable contribution to the field of backdoor defense in deep neural networks, with potential applications in various domains.

Limitation and future work.One important direction for future work, and a current challenge, is the accessibility of clean samples. A valuable approach to this challenge is to consider the samples from other domains and the generated samples.

Structure of Appendix.The detailed proof, analysis, and extension of propositions are given in Appendix A. Some important discussions such as the computation cost, the scalability, and the generalization ability of the proposed method are provided in Appendix B. The details of the experiments are provided in Appendix C. Additional experiments on different settings such as model structures, poisoning ratios, and sizes of clean datasets are given in Appendix D.

  Attack & BadNets &  &  \\  Defense & ACC & ASR & ACC & ASR & ACC & ASR \\ 
**A+B** (Ours) & \(50.60\) & \(0.27\) & \(51.47\) & \(1.02\) & \(51.31\) & \(0.72\) \\ 
**A+D** & \(37.21\) & \(0.30\) & \(38.35\) & \(1.77\) & \(36.68\) & \(0.17\) \\ 
**C+B** & \(53.05\) & \(28.57\) & \(53.16\) & \(69.22\) & \(52.57\) & \(68.65\) \\ 
**C+D** (Vanilla AT) & \(46.66\) & \(2.48\) & \(46.39\) & \(38.03\) & \(46.69\) & \(13.68\) \\  

Table 6: Results on Tiny ImageNet with different variants of SAU and Vanilla AT.

Figure 3: Learn curves for SAU, Vanilla AT and their variants, averaged over five runs.