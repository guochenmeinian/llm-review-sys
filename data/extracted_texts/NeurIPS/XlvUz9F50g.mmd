# A Simple and Adaptive Learning Rate for FTRL

in Online Learning with Minimax Regret of \((T^{2/3})\)

and its Application to Best-of-Both-Worlds

 Taira Tsuchiya

The University of Tokyo and RIKEN

tsuchiya@mist.i.u-tokyo.ac.jp &Shinji Ito

The University of Tokyo and RIKEN

shinji@mist.i.u-tokyo.ac.jp

###### Abstract

Follow-the-Regularized-Leader (FTRL) is a powerful framework for various online learning problems. By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment. However, most existing adaptive learning rates are for online learning problems with a minimax regret of \(()\) for the number of rounds \(T\), and there are only a few studies on adaptive learning rates for problems with a minimax regret of \((T^{2/3})\), which include several important problems dealing with indirect feedback. To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of \((T^{2/3})\). Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of \((T^{2/3})\). As applications of this framework, we consider three major problems with a minimax regret of \((T^{2/3})\): partial monitoring, graph bandits, and multi-armed bandits with paid observations. We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes. The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of \((T^{2/3})\).

## 1 Introduction

Online learning is a problem setting in which a learner interacts with an environment for \(T\) rounds with the goal of minimizing their cumulative loss. This framework includes many important online decision-making problems, such as expert problems [21; 38; 57], multi-armed bandits [6; 8; 33], linear bandits [1; 14], graph bandits [4; 42], and partial monitoring [9; 11].

For the sake of discussion in a general form, we consider the following _general online learning framework_. In this framework, a learner is initially given a finite action set \(=[k]\{1,,k\}\) and an observation set \(\). At each round \(t[T]\), the environment determines a loss function \(_{t}\), and the learner selects an action \(A_{t}\) based on past observations without knowing \(_{t}\). The learner then suffers a loss \(_{t}(A_{t})\) and observes a feedback \(o_{t}\). The goal of the learner is to minimize the (pseudo-)regret \(_{T}\), which is defined as the expectation of the difference between the cumulative loss of the selected actions \((A_{t})_{t=1}^{T}\) and that of an optimal action \(a^{*}\) fixed in hindsight. That is, \(_{T}=_{t=1}^{T}_{t}(A_{t})-_{t=1}^{T} _{t}(a^{*})\) for \(a^{*}*{arg\,min}_{a}_{t=1 }^{T}_{t}(a)\). For example in the multi-armed bandit problem, the observation is \(o_{t}=_{t}(A_{t})\).

_Follow-the-Regularized-Leader (FTRL)_ is a highly powerful framework for such online learning problems. In FTRL, a probability vector \(q_{t}\) over \(\), which is used for determining action selection probability \(p_{t}\) so that \(A_{t} p_{t}\), is obtained by solving the following convex optimization problem:

\[q_{t}*{arg\,min}_{q_{k}}_{s=1}^{t -1}_{s}(q)+_{t}(q)}\,,\] (1)

where \(_{k}\) is the set of probability distributions over \(=[k]\), \(_{t}_{k}\) is an estimator of loss function \(_{t}\), \(_{t}>0\) is (a reciprocal of) learning rate at round \(t\), and \(\) is a convex regularizer. FTRL is known for its usefulness in various online learning problems . Notably, FTRL can be viewed as a generalization of Online Gradient Descent  and the Hedge algorithm , and is closely related to Online Mirror Descent .

The benefit of FTRL due to its generality is that one can design its regularizer \(\) and learning rate \((_{t})_{t}\) so that it can perform adaptively to various properties of underlying loss functions. The _adaptive learning rate_, which exploits past observations, is often used to obtain such adaptivity. In order to see how it is designed, we consider the following stability-penalty decomposition, well-known in the literature :

\[_{T}_{t=1\\ }^{T}}{_{t}}+_{1}h_{1} +_{t=2}^{T}(_{t}-_{t-1})h_{t}.\] (2)

Intuitively, the _stability_ term arises from the regret when the difference in FTRL outputs, \(x_{t}\) and \(x_{t+1}\), is large, and the _penalty_ term is due to the strength of the regularizer. For example, in the Exp3 algorithm for multi-armed bandits , \(h_{t}\) is the Shannon entropy of \(x_{t}\) or its upper bound, and \(z_{t}\) is the expectation of \((^{2}(x_{t}))^{-1}\)-norm of the importance-weighted estimator \(_{t}\) or its upper bound.

Adaptive learning rates have been designed so that it depends on the stability or penalty. For example, the well-known AdaGrad  and the first-order algorithm  depend on stability components \((z_{s})_{s=1}^{t-1}\) to determine \(_{t}\). More recently, there are learning rates that depend on penalty components \((h_{s})_{s=1}^{t-1}\) and that depend on both stability and penalty components .

However, almost all adaptive learning rates developed so far have been limited to problems with a minimax regret of \(()\), and there has been limited investigation into problems with a minimax regret of \((T^{2/3})\). Such online learning problems are primarily related to indirect feedback and includes many important problems, such as partial monitoring , graph bandits , dueling bandits , online ranking , bandits with switching costs , and multi-armed bandits with paid observations . The \((T^{2/3})\) problem is distinctive also due to the classification theorem in partial monitoring , which is a very general problem that includes a wide range of sequential decision-making problems as special cases. It is known that, the minimax regret of partial monitoring games can be classified into one of four categories: \(0\), \(()\), \((T^{2/3})\), or \((T)\). Among these, the classes with non-trivial difficulties and particular importance are the problems with a minimax regret of \(()\) and \((T^{2/3})\).

ContributionsTo address this limitation, we establish a new learning rate framework for online learning with a minimax regret of \((T^{2/3})\). Henceforth, we will refer to problems with a minimax regret of \((T^{2/3})\) as _hard problems_ to avoid repetition, abusing the terminology of partial monitoring. For hard problems, it is common to combine FTRL with _forced exploration_. In this study, we first observe that the regret of FTRL with forced exploration rate \(_{t}\) is roughly bounded as follows:

\[_{T}_{t=1\\ }^{T}}{_{t}_{t}}+ h_{1}+_{t=2}^{T}(_{t}-_{t-1})h_{t}}_{ }+_{t=1\\ }^{T}_{t}.\] (3)

Here, the third term, called the bias term, represents the regret incurred by forced exploration. In the aim of minimizing the RHS of (3), we will determine the exploration rate \(_{t}\) and learning rate \(_{t}\) so that the above stability, penalty, and bias elements for each \(t[T]\) are matched, where the resulting learning rate is called _Stability-Penalty-Bias matching learning rate (SPB-matching)_. Thiswas inspired by the learning rate designed by matching the stability and penalty terms for problems with a minimax regret of \(()\). Our learning rate is simultaneously adaptive to the stability component \(z_{t}\) and penalty component \(h_{t}\), which have attracted attention in very recent years [26; 28; 55]. The SPB-matching learning rate allows us to upper bound the RHS of (3) as follows:

**Theorem 1** (informal version of Theorem 6).: _There exists learning rate \((_{t})_{t}\) and exploration rate \((_{t})_{t}\) for which the RHS of (3) is bounded by \(O_{t=1}^{T}h_{t}( T)}^{2/3} +h_{}}/^{2/3}\) for any \( 1/T\), where \(z_{}=_{t[T]}z_{t}\) and \(h_{}=_{t[T]}h_{t}\)._

Within the general online learning framework, this theorem allows us to prove the following Best-of-Both-Worlds (BOBW) guarantee [10; 58; 61], which achieves an \(O( T)\) regret in the stochastic regime and an \(O(T^{2/3})\) regret in the adversarial regime simultaneously:

**Theorem 2** (informal version of Theorem 7).: _Under some regularity conditions, an FTRL-based algorithm with SPB-matching achieves \(_{T}(z_{}h_{})^{1/3}T^{2/3}\) in the adversarial regime. In the stochastic regime, if \(h_{t}}}(1-q_{ta^{*}})\) holds for FTRL output \(q_{t}_{k}\) and \(_{1}>0\) for all \(t[T]\), the same algorithm achieves \(_{T}}{_{}^{2}}(T _{}^{3})\) for the minimum suboptimality gap \(_{}\)._

To assess the usefulness of the above result that holds for the general online learning framework, this study focuses on two major hard problems: partial monitoring with global observability, graph

   Setting & Reference & Stochastic & Adversarial & AwSB \\  Partial &  & \(D T\) & – & – \\ monitoring &  & – & \((c_{}T)^{2/3}( k)^{1/3}\) & – \\ (with global &  & \(}^{2} T(kT)}{_{}^{2}}\) & \((c_{}T)^{2/3}( T(kT))^{1/3}\) & ✓ \\  &  & \(}^{2}k T}{_{}^{2}}\) & \((c_{}T)^{2/3}( T)^{1/3}\) & ✓ \\  & \({}^{a}\) & \(}^{2} k T}{_{}^{2}}\) & \((c_{}T)^{2/3}( k)^{1/3}\) & ✓ \\  & **Ours (Cor. 9)** & \(}^{2} k T}{_{}^{2}}\) & \((c_{}T)^{2/3}( k)^{1/3}\) & ✓ & MS-type \\  Graph &  & – & \(( k)^{1/3}T^{2/3}\) & – \\ bandits &  & – & \((^{*} k)^{1/3}T^{2/3}\) & – \\ (with weak &  & \(}^{2}}\) & \(( T(kT))^{1/3}T^{2/3}\) & ✓ \\  & \({}^{a,b}\) & \(}^{2}}\) & \(( k)^{1/3}T^{2/3}\) & ✓ \\  & **Ours (Cor. 11)** & \( k T}{_{}^{2}}\) & \((^{*} k)^{1/3}T^{2/3}\) & ✓ & MS-type \\  MAB with &  & – & \((ck k)^{1/3}T^{2/3}+\) & – \\ paid & observations **Ours (Cor. 17)** & \(k k T}{_{}^{2}}\) & \((ck k)^{1/3}T^{2/3}+\) & ✓ & MS-type \\   

* The framework in  is a hierarchical reduction-based approach, rather than a direct FTRL method, discarding past observations as doubling-trick.
* The bounds in  depend on \(\), but their framework with the algorithm in  can achieve improved bounds replacing \(\) with \(^{*}\).

Table 1: Regret bounds for partial monitoring, graph bandits, and multi-armed bandits (MAB) with paid observations. The number of rounds is denoted as \(T\), the number of actions as \(k\), and the minimum suboptimality gap as \(_{}\). The variable \(c_{}\) is defined in Section 5, \(D\) is a constant dependent on the outcome distribution. The graph complexity measures \(\), \(^{*}\), satisfying \(^{*}\) for graphs with no self-loops, are defined in Section 6, and \(^{*}\) is the fractional weak domination number . The parameter \(c\) is the paid cost for observing a loss of actions. AwSB is the abbreviation of the adversarial regime with a self-bounding constraint. MS-type means that the bound in AdvSB has a form similar to the bound established by Masoudian and Seldin .

bandits with weak observability, and multi-armed bandits with paid observations. We demonstrate that the assumptions in Theorem 2 are indeed satisfied for these problems by appropriately choosing the parameters in SPB-matching, thereby improving the existing BOBW regret upper bounds in several respects. To obtain better bounds in this analysis, we leverage the smallness of stability components \(z_{t}\), which results from the forced exploration. Additionally, SPB-matching is the first unified framework to achieve a BOBW guarantee for hard online learning problems. Our learning rate is based on a surprisingly simple principle, whereas existing learning rates for graph bandits and partial monitoring are extremely complicated (see (25, Eq. (15)) and (54, Eq. (16))). Due to its simplicity, we believe that SPB-matching will serve as a foundation for building new BOBW algorithms for a variety of hard online learning problems.

The SPB-matching framework, though omitted from the main text due to the space constraints, is also applicable to the multi-armed bandits with paid observations , whose minimax regret with costs is \((T^{2/3})\). We can show that the regret with paid costs, \(^{c}_{T}\), is roughly bounded by \(^{c}_{T}=O(ck k)^{1/3}T^{2/3}+\) in the adversarial regime and \(^{c}_{T}=O\{c,1\}k k T/^{2}_{} \) in the stochastic regime for the cost of observation \(c\). The bound for the adversarial regime is of the same order as (53, Theorem 3). The detailed problem setup, regret upper bounds, and regret analysis can be found in Appendix G.

Although omitted in Theorem 2, our approach achieves a refined regret bound devised by Masoudian and Seldin  in the _adversarial regime with a self-bounding constraint_, which includes the stochastic regime, adversarial regime, and the stochastic regime with adversarial corruptions  as special cases. We call the refined bound _MS-type bound_, named after the author. The MS-type bound maintains an ideal form even when \(C=(T)\) or \(_{}=(1/)\) (see  for details), and our bounds are the first MS-type bounds for hard problems. A comparison with existing regret bounds is summarized in Table 1.

## 2 Preliminaries

NotationFor a natural number \(n\), we let \([n]=\{1,,n\}\). For vector \(x\), let \(x_{i}\) denote its \(i\)-th element and \(\|x\|_{p}\) the \(_{p}\)-norm for \(p[1,]\). Let \(_{k}=\{p^{k}\|p\|_{1}=1\}\) be the \((k-1)\)-dimensional probability simplex. The vector \(e_{i}\) is the \(i\)-th standard basis and \(\) is the all-ones vector. Let \(D_{}(x,y)\) denote the Bregman divergence from \(y\) to \(x\) induced by a differentiable convex function \( D_{}(x,y)=(x)-(y)-(y),x-y\). To simplify the notation, we sometimes write \((a_{t})_{t=1}^{T}\) as \(a_{1:T}\) and \(f=O(g)\) as \(f g\). We regard function \(f=[k]\) as a \(k\)-dimensional vector.

General online learning frameworkTo provide results that hold for a wide range of settings, we consider the following general online learning framework introduced in Section 1.

At each round \(t[T]=\{1,,T\}\):

1. The environment determines a loss vector \(_{t}\);
2. The learner selects an action \(A_{t}\) based on \(p_{t}_{k}\) without knowing \(_{t}\);
3. The learner suffers a loss of \(_{t}(A_{t})\) and observes a feedback \(o_{t}\).

This framework includes many problems such as the expert problem, multi-armed bandits, graph bandits, and partial monitoring as special cases.

Stochastic, adversarial, and their intermediate regimesWithin the above general online framework, we study three different regimes for a sequence of loss functions \((_{t})_{t}\). In the stochastic regime, the sequence of loss functions is sampled from an unknown distribution \(\) in an i.i.d. manner. The suboptimality gap for action \(a\) is given by \(_{a}=_{_{t}}[_{t}(a)-_{t}(a^{*})]\) and the minimum suboptimality gap by \(_{}=_{a a^{*}}_{a}\). In the adversarial regime, the loss functions can be selected arbitrarily, possibly based on the past history up to round \(t-1\).

We also investigate, the adversarial regime with a self-bounding constraint , which is an intermediate regime between the stochastic and adversarial regimes.

**Definition 3**.: Let \(^{k}\) and \(C 0\). The environment is in an _adversarial regime with a \((,C,T)\)_ self-bounding constraint if it holds for any algorithm that \(_{T}^{T}_{A_{t}}-C}\)._

[MISSING_PAGE_FAIL:5]

2 differ only in the way indices are shifted.3 For the sake of convenience, we define \(G_{1}\) and \(G_{2}\) by

\[G_{1}(z_{1:T},h_{1:T})=_{t=1}^{T}}}{(_{s=1}^{t} }/h_{s})^{1/3}}\,,\ G_{2}(u_{1:T},h_{1:T})=_{t=1}^{T}}{^{t}u_{s}/h_{s}}}\,.\] (7)

Define \(z_{}=_{t[T]}z_{t}\), \(u_{}=_{t[T]}u_{t}\), and \(h_{}=_{t[T]}h_{t}\). Then, using SPB-matching rules in (6), we can upper-bound \(F\) in terms of \(G_{1}\) and \(G_{2}\) as follows:

**Lemma 4**.: _Consider SPB-matching (6) and suppose that \(h_{t}_{t}\) for all \(t[T]\). Then, Rule 1 achieves \(F(_{1:T},z_{1:T},u_{1:T},h_{1:T}) 3.2G_{1}(z_{1:T},_{1:T})+2 G_{2}(u_{1:T},_{1:T})\) and Rule 2 achieves \(F(_{1:T},z_{1:T},u_{1:T},h_{1:T}) 4G_{1}(z_{1:T},_{2:T+1})+3 G_{2}(u_{1:T},_{2:T+1})+10/_{1}}+5u_{}/_{1}+ _{1}h_{1}\)._

The proof of Lemma 4 can be found in Appendix B.1. One can see from the proof that the effect of using \(_{t}=/_{t}}+u_{t}/_{t}\) instead of \(_{t}=/_{t}}\) only appears in \(G_{2}\), which has a less impact than \(G_{1}\) when bounding \(F\). We can further upper-bound \(G_{1}\) as follows:

**Lemma 5**.: _Let \((z_{t})_{t=1}^{T}_{ 0}\) and \((h_{t})_{t=1}^{T}_{>0}\) be any non-negative and positive sequences, respectively. Let \(_{0}>_{1}>>_{J}>_{J+1}=0\) and \(_{0} h_{}\) and define \(_{j}=\{t[T]:_{j-1} h_{t}>_{j}\}\) for \(j[J]\) and \(_{J+1}=\{t[T]:_{J} h_{t}\}\). Then, \(G_{1}(z_{1:T},h_{1:T})_{j=1}^{J+1}}_{t_{j}}}^{2/3}\). This implies that for all \(J\) it holds that_

Combining Lemmas 4 and 5 and the bound on \(G_{2}\) in [26, Lemma 3], we obtain the following theorem.

**Theorem 6**.: _Let \((z_{t})_{t=1}^{T},(u_{t})_{t=1}^{T}_{ 0}\) and \((h_{t})_{t=1}^{T}_{>0}\). Suppose that \(_{t}\) satisfies \(h_{t}_{t}\) for all \(t[T]\). Then, if \(_{t}\) is given by Rule 1 in (6), then for all \( 1/T\) it holds that_

\[F(_{1:T},z_{1:T},u_{1:T},h_{1:T})\! (_{t=1}^{T}_{t}( T)})^{ \!}\!+\!(_{}} )^{\!}\!,\,(_{t=1}^{T}_{} })^{\!}\!\] \[+\!^{T}u_{t}_{t}(  T)}+_{}/}\,,\,^{T}u_{t}_{}}}\,.\] (8)

_If \(_{t}\) is given by Rule 2 in (6), then for all \( 1/T\) it holds that_

\[F(_{1:T},z_{1:T},u_{1:T},h_{1:T})\! (_{t=1}^{T}_{t+1}( T)})^{ \!}\!+\!(_{}} )^{\!}\!,\,(_{t=1}^{T}_{}} )^{\!}\!}\] \[+\!^{T}u_{t}_{t+1}(  T)}\!+\!_{}/}\,,\,^{T}u_{t}_{}}\!}\!+\!}{_{1} }}\!+\!}{_{1}}\!+\!_{1}h_{1}\,.\] (9)

Note that these bounds are for problems with a minimax regret of \((T^{2/3})\). Roughly speaking, our bounds have an order of \((_{t=1}^{T}_{t+1} T})^{1/3}\) and differ from the existing stability-penalty-adaptive-type bounds of \(^{T}z_{t}_{t+1} T}\) for problems with a minimax regret of \(()\). We will see in the subsequent sections that our bounds are beneficial as they give nearly optimal regret bounds in stochastic and adversarial regimes in partial monitoring, graph bandits, and multi-armed bandits with paid observations.

## 4 Best-of-both-worlds framework for hard online learning problems

Using the SPB-matching learning rate established in Section 3, this section provides a BOBW algorithm framework for hard online learning problems. We consider the following FTRL update:

\[q_{t}=*{arg\,min}_{p_{k}}\!\{_{s=1}^{t-1} _{t},p+_{t}(-H_{}(p))+(-H_{ }(p))\},(0,1)\,,\;=1-\,,\] (10)

where \(H_{}\) is the \(\)-Tsallis entropy defined as \(H_{}(p)=_{i=1}^{k}(p_{i}^{}-p_{i}),\) which satisfies \(H_{}(p) 0\) and \(H_{}(e_{i})=0\). Based on this FTRL output \(q_{t}\), we set \(h_{t}=H_{}(q_{t})\), which satisfies \(h_{1}=h_{}\). Additionally, for \(q_{t}\) and some \(p_{0}_{k}\), we use the action selection probability \(p_{t}_{k}\) defined by

\[p_{t}=(1-_{t})q_{t}+_{t}\,p_{0}_{t}= _{t}^{}+}{_{t}}=}{_{t}}}+ }{_{t}}\,,\] (11)

where \(_{1}\) is chosen so that \(_{t}[0,1/2]\). Let \(=/_{1}}+u_{}/_{1}+_{1}h_{1}+ {h}\) for \(=H_{}(/k)\) and let \(_{t}[\,\,]\) be the expectation given all observations before round \(t\). Then the above procedure with Rule 2 of SPB-matching in (6), summarized in Algorithm 1, achieves the following BOBW bound:

```
1input: action set \(\), observation set \(\), exponent of Tsallis entropy \(\), \(_{1}\), \(\)
2for\(t=1,2,\)do
3 Compute \(q_{t}_{k}\) by (10) with a loss estimator \(_{t}\).
4 Set \(h_{t}=H_{}(q_{t})\) and \(z_{t},u_{t} 0\) defined for each problem.
5 Compute action selection probability \(p_{t}\) from \(q_{t}\) by (11).
6 Choose \(A_{t}\) so that \([A_{t}=i p_{t}]=p_{ti}\) and observe feedback \(o_{t}\).
7 Compute loss estimator \(_{t}\) based on \(p_{t}\) and \(o_{t}\).
8 Compute \(_{t+1}\) by Rule 2 of SPB-matching in (6) with \(_{t+1}=h_{t}\). ```

**Algorithm 1** Best-of-both-worlds framework based on FTRL with SPB-matching learning rate and Tsallis entropy for online learning with minimax regret of \((T^{2/3})\)

**Theorem 7**.: _Consider the general online learning framework in Section 2 with \(\|_{t}\|_{} 1\). Suppose that Algorithm 1 satisfies the following three conditions_ (i)-(ii)_:_

\[&\;_{T}\![ _{t=1}^{T}_{t},q_{t}-e_{a^{*}}+2_{t=1}^{T} _{t}]\!,\\ &\;_{t}\![_{t},q_{ t}-q_{t+1}-_{t}D_{(-H_{})}(q_{t+1},q_{t})]}{ _{t}_{t}^{}}\,,\;h_{t} h_{t-1}\,.\] (12)

_Then, in the adversarial regime, Algorithm 1 achieves_

\[_{T}=O\!((z_{}h_{1})^{1/3}T^{2/3}+h_{1}T}+ ).\] (13)

_In the adversarial regime with a \((,C,T)\)-self-bounding constraint, further suppose that_

\[h_{t}}}(1-q_{ta^{*}}) u _{t}h_{t}_{2}(1-q_{ta^{*}})\] (14)

_are satisfied for some \(_{1},_{2}>0\) for all \(t[T]\). Then, the same algorithm achieves_

\[_{T}=O\!(}^{2}}\!\!( T_{}^{3})+(}{_{}^{2}} \!(}}{C}))^{1/3}+^{ })\] (15)

_for \(=\{_{1},_{2}\}\) and \(^{}=+(z_{}h_{1})^{1/3}+h_{1}} 1/_{}^{3}+C/_{}^{2/3}\) when \(T 1/_{}^{3}+C/_{}=:\), and \(_{T}=O(z_{}h_{1})^{1/3}^{2/3}+h_{1} }\) when \(T<\)._

The proof of Theorem 7 relies on Theorem 6 established in the last section and can be found in Appendix C. Note that the bound (15) becomes the bound for the stochastic regime when \(C=0\).

Case study (1): Partial monitoring with global observability

This section provides a new BOBW algorithm for globally observable partial monitoring games.

### Problem setting and some concepts in partial monitoring

Partial monitoring gamesA Partial Monitoring (PM) game \(=(,)\) consists of a loss matrix \(^{k d}\) and feedback matrix \(^{k d}\), where \(k\) and \(d\) are the number of actions and outcomes, respectively, and \(\) is the set of feedback symbols. The game unfolds over \(T\) rounds between the learner and the environment. Before the game starts, the learner is given \(\) and \(\). At each round \(t[T],\) the environment picks an outcome \(x_{t}[d]\), and then the learner chooses an action \(A_{t}[k]\) without knowing \(x_{t}\). Then the learner incurs an unobserved loss \(_{A_{t}x_{t}}\) and only observes a feedback symbol \(_{t}_{A_{t}x_{t}}\). This framework can be indeed expressed as the general online learning framework in Section 2, by setting \(=\), \(_{t}(a)=_{ax_{t}}=e_{a}^{}e_{x_{t}}\) and \(o_{t}=_{t}=_{A_{t}x_{t}}\).

We next introduce fundamental concepts for PM games. Based on the loss matrix \(\), we can decompose all distributions over outcomes. For each action \(a[k]\), the cell of action \(a\), denoted as \(_{a}\), is the set of probability distributions over \([d]\) for which action \(a\) is optimal. That is, \(_{a}=\{u_{d}_{b[k]}(_{a}-_{b})^{ }u 0\}\), where \(_{a}^{d}\) is the \(a\)-th row of \(\).

To avoid the heavy notions and concepts of PM, we assume that the PM game has no duplicate actions \(a b\) such that \(_{a}=_{b}\) and its all actions are _Pareto optimal_; that is, \((_{a})=d-1\) for all \(a[k]\). The discussion of the effect of this assumption can be found _e.g.,_ in .

Observability and loss estimationTwo Pareto optimal actions \(a\) and \(b\) are _neighbors_ if \((_{a}_{b})=d-2\). Then, this neighborhood relations defines _globally observable games_, for which the minimax regret of \((T^{2/3})\) is known in the literature . Two neighbouring actions \(a\) and \(b\) are _globally observable_ if there exists a function \(w_{e(a,b)}:[k]\) satisfying

\[_{c=1}^{k}w_{e(a,b)}(c,_{cx})=_{ax}-_{bx}\ \ \ x[d]\,,\] (16)

where \(e(a,b)=\{a,b\}\). A PM game is said to be globally observable if all neighboring actions are globally observable. To the end, we assume that \(\) is globally observable.4

Based on the neighborhood relations, we can estimate the loss _difference_ between actions, instead of estimating the loss itself. The _in-tree_ is the edges of a directed tree with vertices \([k]\) and let \([k][k]\) be an in-tree over the set of actions induced by the neighborhood relations with an arbitrarily chosen root \(r[k]\). Then, we can estimate the loss differences between Pareto optimal actions as follows. Let \(G(a,)_{b}=_{c_{}(b)}w_{e}(a,)\) for \(a[k],\) where \(_{}(b)\) is the set of edges from \(b[k]\) to the root \(r\) on \(\). Then, it is known that this \(G\) satisfies that for any Pareto optimal actions \(a\) and \(b\), \(_{c=1}^{k}(G(c,_{cx})_{a}-G(b,_{cx})_{b})=_{ax}- _{bx}\) for all \(x[d]\) (_e.g.,_ [37, Lemma 4]). From this fact, one can see that we can use \(_{t}=G(A_{t},_{A_{t}x_{t}})/p_{tA_{t}}^{k}\) as the loss (difference) estimator, following the standard construction of the importance-weighted estimator . In fact, \(_{t}\) satisfies \(_{A_{t} p_{t}}[_{tta}-_{tb}]=_{c=1}^{k }(G(c,)_{a}-G(c,_{t})_{b})=_{ax}-_{bx}\). We let \(c_{}=\{1,k\|G\|_{}\}\) be a game-dependent constant, where \(\|G\|_{}=_{a[k],}|G(a,)|\).

### Algorithm and regret upper bounds

Here, we present a new BOBW algorithm based on Algorithm 1. We use the following parameters for Algorithm 1. We use the loss (difference) estimator of \(_{t}=_{t}\). We set \(p_{0}\) in (11) to \(p_{0}=/k\). For \(_{t}_{i[k]}q_{ti}\) and \(q_{t*}=\{q_{t_{t}},1-q_{t_{t}}\}\), let

\[_{1}}^{2}}{1-}\,,\,=}}{(1-)^{2}}}\,,\,z_{t}=}^{2}}{1-}_{i_{t}}q_{ti}^{2-}+ q_{t*}^{2-}\,,\,u_{t}=}}{1-}q_{t*}^{1- }\,.\] (17)

Note that \(z_{}=}^{2}}{1-}\), \(u_{}=}}{1-}\), and \(h_{}=h_{1}=k^{1-}\). Then, we can prove the following:

**Theorem 8**.: _In globally observable partial monitoring, for any \((0,1)\), Algorithm 1 with (17) satisfies the assumptions of Theorem 7 with \(_{1}=}^{2}k^{1-}}{(1-)} \) and \(_{2}=}k^{1-}}{(1-)}\)._The proof of Theorem 8 is given in Appendix E. Setting \(=1-1/( k)\) gives the following:

**Corollary 9**.: _In globally observable partial monitoring with \(T\), Algorithm 1 with (17) for \(=1-1/( k)\) achieves_

\[_{T}=O(c_{}T)^{2/3}(  k)^{1/3}+&\\ \@@LTX@noalign{ 6.0pt plus 2.0pt minus 2.0pt}}^{2} k}{_{}^{2}}T_{ }^{3}+c_{}^{2} k}{_{} ^{2}}}}{C}^{1/3}+ ^{}\\ \@@LTX@noalign{ 6.0pt plus 2.0pt minus 2.0pt}&\,.\] (18)

_Here, if we use \(_{1}=64c_{}^{2}/(1-)\), which satisfies (17), \(=O(c_{}^{2} k+k^{3/2}( k)^{5/2})\) and \(^{}=+O((c_{}^{2/3}( k)^{1/3}+} k})(}^{3}}+}})^{2/3})\)._

This regret upper bound is better than the bound based on FTRL in  in both stochastic and adversarial regimes, notably by a factor of \( T\) or \(k\) in the stochastic regime. The bound for the adversarial regime with a \((,C,T)\)-self-bounding constraint is the first MS-type bound in PM. The upper bounds for the adversarial regime and stochastic regime are optimal in terms of \(T\); however, even without considering BOBW guarantees, the optimality with respect to other variables \(k\), \(m\), and \(d\) is unclear (cf. (36, Section 37.9)), and exploring this is an important direction for future work. As discussed in Section 1, employing the black-box reduction approach in  also allows us to achieve an upper bound of the same order as our upper bound. Nevertheless, as previously mentioned, the blackbox approach is a complicated approach involving multi-stage reductions and has the drawback of discarding past observations, similar to the doubling-trick. Hence, demonstrating that using the FTRL framework alone can achieve the same upper bound is a significant theoretical advancement.

## 6 Case study (2): Graph bandits with weak observability

This section presents a new BOBW algorithm for weakly observable graph bandits.

### Problem setting and some concepts in graph bandits

Problem settingIn the graph bandit problem, the learner is given a directed feedback graph \(G=(V,E)\) with \(V=[k]\) and \(E V V\). For each \(i V\), let \(N^{}(i)=\{j V(j,i) E\}\) and \(N^{}(i)=\{j V(i,j) E\}\) be the in-neighborhood and out-neighborhood of vertex \(i V\), respectively. The game proceeds as the general online learning framework provided in Section 2, with action set \(=V\), loss function \(_{t} V\), and observation \(o_{t}=\{_{t}(j) j N^{}(I_{t})\}\).

Observability and domination numberSimilar to partial monitoring, the minimax regret of graph bandits is characterized by the properties of the feedback graph \(G\). A graph \(G\) is _observable_ if it contains no self-loops, \(N^{}(i)\) for all \(i V\). A graph \(G\) is _strongly observable_ if \(i N^{}(i)\) or \(V\{i\} N^{}(i)\) for all \(i V\). Then, a graph \(G\) is _weakly observable_ if it is observable but not strongly observable.5 The minimax regret of the weakly observable is known to be \((T^{2/3})\).

The weak domination number characterizes precisely the minimax regret. The _weakly dominating set_\(D V\) is a set of vertices such that \(\{i V i N^{}(i)\}_{i D}N^{ {out}}(i)\). Then, the _weak domination number_\((G)\) of graph \(G\) is the size of the smallest weakly dominating set. For weakly observable \(G\), the minimax regret of \((^{1/3}T^{2/3})\) is known . Instead, our bound depends on the _fractional domination number_\(^{*}(G)\), defined by the optimal value of the following linear program:

\[\,_{i V}x_{i}_{i  N^{}(j)}x_{i} 1\  j V\,,\,0 x_{i} 1\  i V\,.\] (19)

We use \((x_{i}^{*})_{i V}\) to denote the optimal solution of (19) and define its normalized version \(u_{k}\) by \(u_{i}=x_{i}^{*}/_{j V}x_{j}^{*}\). The advantage of using the fractional domination number mainly lies in its computational complexity; further details are provided in Appendix F.1.

[MISSING_PAGE_EMPTY:10]