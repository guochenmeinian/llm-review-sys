# SNIP: Bridging Mathematical Symbolic and

Numeric Realms with Unified Pre-training

 Kazem Meidani\({}^{*}\)\({}^{1}\), Parshin Shojaee\({}^{*}\)\({}^{2}\),

**Chandan K. Reddy**\({}^{2}\), **Amir Barati Farimani**\({}^{1,3}\)

\({}^{1}\) Department of Mechanical Engineering, Carnegie Mellon University

\({}^{2}\) Department of Computer Science, Virginia Tech

\({}^{3}\) Machine Learning Department, Carnegie Mellon University

Equal contribution. Contact email: mmeidani@andrew.cmu.edu

###### Abstract

In scientific inquiry, symbolic mathematical equations play a fundamental role in modeling complex natural phenomena. Leveraging the power of deep learning, we introduce SNIP, a Multi-Modal Symbolic-Numeric Pre-training framework. By employing joint contrastive learning between symbolic and numeric domains, SNIP enhances their mutual alignment in pre-trained embeddings. Latent space analysis reveals that symbolic supervision significantly enriches the embeddings of numeric data, and vice versa. Evaluations across diverse tasks, including symbolic-to-numeric and numeric-to-symbolic property prediction, demonstrate SNIP's superior performance over fully supervised baselines. This advantage is particularly pronounced in few-shot learning scenarios, making SNIP a valuable asset in situations with limited available data.

## 1 Introduction

Throughout the history of science, symbolic mathematics has been unreasonably effective in representing natural phenomena . Complex patterns of natural systems, represented as numeric data observations, can be elegantly abstracted using mathematical formulas. Mathematical symbolism has given us the language to describe, understand, and predict the natural world. The challenge of bridging the gap between the numeric observations and their mathematical symbolic representations has been a consistent focus in many scientific and engineering domains. Recognizing and exploring this connection is crucial, as it promises to drive advancements in various fields.

In recent years, deep learning has demonstrated promising capabilities in learning from symbolic mathematics language as well as extracting knowledge from numeric data observations. Transformer-based models , in particular, have emerged as frontrunners in this endeavor, effectively capturing patterns within mathematical expressions and solving complex tasks such as differential equations . Efforts have also been made to enhance the mathematical reasoning of language models, improving their performance in general math word problem solving . However, these models, while powerful, are not inherently designed to handle numeric data observations. While some pre-trained symbolic regression models have been introduced to map numeric datasets to their governing mathematical expressions in a supervised manner , a gap still remains in developing a task-agnostic unified pre-training model capable of mutual understanding between the modalities of symbolic mathematical equations and their corresponding numeric counterparts.

Multi-modal pre-training models, exemplified by groundbreaking models like Contrastive Language-Image Pre-training (CLIP) , have found a significant place in the deep learning landscape. CLIP has particularly set new standards in vision-language tasks, bridging the understandingbetween visual content and natural language descriptions. This mutual comprehension across different data modalities has opened up opportunities for more intuitive and context-aware machine learning applications. Expanding beyond traditional vision-language domains, recent studies have broadened multi-modal pre-training to include other modalities, such as audio and tabular data [10; 11; 12]. Additionally, previously untouched scientific domains, like molecular representation, are also benefiting from these advancements [13; 14]. Nevertheless, the symbolic-numeric domain remains relatively unexplored. Considering the foundational role of symbolic mathematics in science and the ubiquity of numeric data, an in-depth exploration of their mutual learning is not only timely but essential. Such an investigation holds the promise of unlocking numerous applications, from improving scientific simulations and modeling to enhancing data-driven decision-making in diverse sectors.

In this work, we present **S**ymbolic-**N**umeric **I**ntegrated **P**re-training (**SNIP**) to connect the two often distinct worlds of symbolic mathematical expressions and their corresponding numeric manifestations. The architecture of SNIP, depicted in Fig. 1, incorporates dual transformer-based encoders, with each encoder dedicated to learning the symbolic or numeric representations of mathematical functions. Subsequently, a task-agnostic joint contrastive objective is employed to enhance the similarity between (symbolic, numeric) pairs of data. The unified multi-modal pre-training of SNIP provides capabilities to understand and generate cross-modal content. Our experiments show that SNIP achieves remarkable performance in cross-modal mathematical property understanding and prediction tasks. Analysis of the latent embeddings further unveils that SNIP's pre-trained representations manifest discernible patterns associated with these cross-modal properties. Moreover, when paired with an equation generation decoder and upon deeper exploration of the latent space, we observe that SNIP's representations are interpolatable. This suggests a significant relationship between the latent vectors and their numeric behaviors. To put it simply, latent space interpolation translates to a symbolic function whose numeric behavior semantically bridges the gap between the original and target functions (check Fig.4 for more details). Such latent space dynamics, ingrained through SNIP's multi-modal pre-training, offer a valuable edge for sophisticated searches and explorations, potentially benefiting various subsequent downstream tasks.

## 2 Related Work

**Large-scale Pre-training.** Our work is built upon an extensive body of research advocating the advantages of pre-training large models on large datasets [15; 16]. Initially, pre-training was single-modal, with self-supervised learning (SSL) as a key paradigm that used data as its own supervision, especially useful where labeled data was limited . This paved the way for the emergence of multi-modal pre-training, where models are trained to understand relationships across different data types . Vision and language have traditionally played the two main characters of pre-training models. For instance, CLIP , ALIGN , and FLAVA  utilize image-caption pairs to construct jointly learned embedding spaces. These models are trained to align the embeddings of corresponding image-caption pairs while distancing unrelated pairs. The success of multi-modal pre-training in vision and language spurred its adoption in other domains. For example, recent works have extended this approach to videos, audio, and even tabular data [10; 21; 12]. Specialized scientific domains have also embraced this paradigm. For instance, different models have emerged to learn joint representations of molecules [13; 14]. Our work introduces a fresh perspective, intertwining symbolic mathematics with numeric observations. To this end, we use multi-modal pre-training's potential to deepen the symbolic-numeric mutual understanding.

**Deep Symbolic Mathematics.** Recently, deep learning models have made significant strides in the field of mathematical reasoning [6; 5]. The Transformer architecture, originally designed for NLP tasks , has been repurposed with remarkable success in the realm of symbolic mathematics. It has powered models that can integrate functions [3; 4], prove mathematical theorems , and perform numerical calculations, such as arithmetic operations [23; 24]. These achievements underscore the flexibility and potential of deep learning models in abstract reasoning. Beyond pure symbolic reasoning, there is also a growing interest in supplementing these models with numerical knowledge for improved mathematical understanding. For example, recent endeavors have sought to enhance language models with numeric representations, aiming to improve their skills in mathematical word problem-solving [25; 26; 27; 28]. Our work contributes a new angle to this growing field by integrating symbolic and numeric understanding in a unified pre-training framework. By doing so, we not only capture the abstract representations of mathematical symbolic concepts but also their tangible numeric behaviors.

## 3 Pre-training

As depicted in Fig. 1, the SNIP architecture comprises two transformer-based encoders, each tailored for learning the symbolic or numeric representations of mathematical functions. These symbolic and numeric encoders are jointly trained with a task-agnostic joint contrastive objective to predict correct pairings within a batch of (symbolic, numeric) examples. During pre-training, SNIP receives synthetically created symbolic equations and their associated numeric data as inputs to the symbolic and numeric heads, respectively.

### Numeric Encoder

The numeric encoder's foundation is rooted in the recent advancements of transformer-based models for encoding numeric observations into latent spaces . In this framework, the numeric encoder--represented as \(_{}^{V}\)--integrates an embedder, a multi-layer Transformer, and an attention pooling approach, to map numeric observations \((,)\) into a condensed latent vector \(_{V}\).

**Tokenization.** Following , numeric inputs are tokenized using base-10 floating-point notation. They are rounded to four significant digits and subsequently represented as sequences of three tokens: sign, mantissa (0-9999 range), and exponent (\(E\)-\(100\) to \(E100\)). For instance, the number \(5.432\) is tokenized as \([+,5432,E\)-\(3]\).

**Encoding.** Given a batch of \(N\) numeric input points \((,)^{D+1}\), each is represented by \(3(D+1)\) tokens. With increasing \(D\) and \(N\), the input sequence length grows, challenging the quadratic complexity of Transformers. To address this, we employ an embedder, as suggested by , before the Transformer encoder. This embedder maps each input point to a unique embedding space. The resulting embeddings, with dimension \(d_{}\), are then fed into the encoder. For the numeric encoder, we utilize a multi-layer Transformer architecture . Notably, due to the permutation invariance of the \(N\) input points for each batch sample, we exclude positional embeddings, aligning with the approach in . This encoder variant is denoted as \(Enc^{V}\). The representation at its \(l\)-th layer is given by \(_{l}=Enc^{V}_{l}(_{l-1})\), where \(l\) ranges from 1 to \(L_{V}\), and \(L_{V}\) signifies the total layer count.

**Attention-based Distillation.** To distill the information from the Transformer's output into a compact representation for the whole sequence of observations, we employ an attention-based pooling mechanism, following . Let \(}_{V}\) denote the attention weights, which are computed as: \(}_{V}=(_{a}_{L_{V}}^{T})\), where \(_{a}^{d_{}}\) is a learnable weight matrix, and we take the transpose of \(_{L_{V}}^{N d_{}}\) to apply softmax along the sequence dimension \(N\). The compact sequence-level representation, \(_{V}\), is then obtained by: \(_{V}=}_{V}_{L_{V}}\). This attention mechanism allows the model to focus on the most informative parts of the data points, effectively compressing the information into a fixed-size embedding.

Figure 1: The SNIP Framework: A schematic representation of the dual-encoder pre-training scheme for mutual learning between symbolic expressions and their numeric data observations. Both symbolic and numeric encoders work in tandem, capturing the paired similarities and essence of their respective modalities.

### Symbolic Encoder

The symbolic encoder in our framework also draws inspiration from recent advancements in transformer-based models for encoding symbolic mathematical functions, as demonstrated in works such as [4; 3]. Here, the symbolic encoder--denoted as \(_{}^{S}\)--is a composite entity parameterized by \(\), encapsulating the embedder, a multi-layer Transformer, and attention-based pooling mechanisms. Given an input symbolic function \(f()\), this encoder outputs a condensed representation \(_{S}\).

**Tokenization.** Mathematical functions are tokenized by enumerating their trees in prefix order, following the principles outlined in . This process employs self-contained tokens to represent operators, variables, and integers, while constants are encoded using the same methodology as discussed in Sec. 3.1, representing each with three tokens. In alignment with , we use special tokens [\( BOS\)] and [\( EOS\)] to mark sequence start and end.

**Encoding.** Given a batch of symbolic functions with \(M\) tokens, each symbolic input is represented as \(_{0}=_{[ BOS]};_{t_{1}};;_{ t_{M}};_{[ EOS]}+^{pos}\), where \(_{0}^{(M+2) d_{}}\). Here, \(\) refers to the embedding matrix, \(t_{i}\) denotes the \(i\)-th token, \(M\) signifies the number of tokens in the symbolic function, \(d_{}\) is the embedding dimension, and \(^{pos}\) represents the positional embedding matrix. In the symbolic encoder, we use a Transformers model with the same architecture settings as in Sec. 3.1. This variant of the encoder, denoted as \(Enc^{S}\), processes the input symbolic data. The \(l\)-th layer representation is described as \(_{l}=Enc^{S}_{l}(_{l-1})\), where \(l\) varies from 1 to \(L_{S}\), and \(L_{S}\) indicates the total number of layers within the symbolic encoder.

**Attention-based Distillation.** The symbolic encoder also employs attention-based pooling, as in Sec. 3.1. This mechanism computes weighted sums to distill information from the symbolic expression into a compact representation \(_{S}=}_{S}_{L_{S}}\), using attention weights \(}_{S}\) through softmax along the symbolic sequence.

### Unified Pre-training Objective

Our work introduces a unified symbolic-numeric pre-training approach, SNIP, which aims to facilitate a mutual understanding of both domains, enabling advanced cross-modal reasoning.

**Training Objective.** SNIP's pre-training objective is inspired by the joint training used in CLIP . Incorporating both a numeric and symbolic encoder, the model optimizes a symmetric cross-entropy loss over similarity scores. It employs a contrastive loss (InfoNCE  objective) to learn the correspondence between numeric and symbolic data pairs. Specifically, this approach learns to align embeddings of corresponding symbolic-numeric pairs while distancing unrelated pairs. The objective function can be defined as:

\[=-_{(v,s) B}(_{S},_{V})+ (_{V},_{S}),\] (1)

where \(B\) represents the batch of (symbolic, numeric) data pairs, \((_{S},_{V})\) and \((_{V},_{S})\) denote the contrastive losses on symbolic-to-numeric and numeric-to-symbolic similarities, respectively. The symbolic-to-numeric contrastive loss, \((_{S},_{V})\), is calculated as follows:

\[(_{S},_{V})=_{S}_{V}^{ +}}{_{\{_{V}^{+},_{V}^{-}\}}_{S}}{}}\] (2)

Here, \(\) is temperature, \(_{V}^{+}\) represents positive SNIP numeric embeddings that overlap with SNIP symbolic embedding \(_{S}\), and \(_{V}^{-}\) are negative numeric embeddings implicitly formed by other numeric embeddings in the batch. A symmetric equivalent, \((_{V},_{S})\), also defines the numeric-to-symbolic contrastive loss. More implementation details are provided in App. B.

### Pre-training Data

In our SNIP approach, pre-training relies on a vast synthetic dataset comprising paired numeric and symbolic data. We follow the data generation mechanism in , where each example consists of \(N\) data points \((x,y)^{D+1}\) and a corresponding mathematical function \(f\), where \(y=f(x)\). Data generation proceeds in several steps, ensuring diverse and informative training examples. More details about each of the following steps are provided in App. A.

**Sampling of functions.** We create random mathematical functions using a process detailed in [8; 3]. This process involves selecting an input dimension \(D\), determining the number of binary operators,constructing binary trees, assigning variables to leaf nodes, inserting unary operators, and applying random affine transformations. This method ensures a diverse set of functions for training.

**Sampling of datapoints.** After generating a function, we sample \(N\) input points and find their corresponding target values. To maintain data quality, we follow guidelines from , discarding samples with inputs outside the function's domain or exceptionally large output values. Our approach includes drawing inputs for each function from various distributions, enhancing training diversity. The generation process of datapoints also involves selecting cluster weights and parameters, sampling input points for each cluster, and normalization along each dimension. To emphasize on the function's numeric behavior rather than the range of values, we also normalize the target values \(\) between \((0,1)\).

## 4 Pre-trained Latent Space Analysis

**Symbolic Encoded Representations.** To evaluate the learned representations of SNIP, we analyze the pre-trained latent space to investigate the mutual understanding that is achieved between the symbolic and numeric representations. We first show that numeric behaviors are learned in the symbolic latent vectors \(_{S}\). To this end, we introduce several mathematical properties that describe different numeric features of the mathematical functions. Specifically, we consider the following properties: (a) _Non-Convexity Ratio (NCR)_ which approximates function convexity with values between NCR=0 (fully convex) and NCR=1 (fully concave); (b) _Upwardness_ which quantifies the function's directionality by assessing the segments where data increases within the training domain, ranging from UP=-1 for strictly decreasing functions to UP=1 for increasing ones; (c) _Average of Normalized_\(y\) can be a measure to distinguish different numeric behaviors, and it can roughly approximate the numerical integral of the normalized function in the defined range of training \(\); and (d) _Log Oscillations_ which quantifies the degree of oscillatory behavior exhibited by the numeric data, represented in logarithmic scale. More details of these properties can be found in App. C.

Fig. 2 illustrate two-dimensional t-SNE  visualizations of SNIP symbolic latent space colored by these properties. We can observe that the latent spaces are shaped by the symbolic-numeric similarities of the functions such that numeric properties can be clustered and/or show visible trends in the symbolic encoded representation space \(_{S}\).

**Numeric Encoded Representations.** Just as numeric behaviors shaped symbolic encoded representations, numeric vectors, denoted as \(_{V}\), are similarly influenced by the symbolic characteristics inherent to the associated governing equations. This relationship is visually depicted in Fig. 3, which presents 2D t-SNE visualizations of the latent space cultivated from SNIP's numeric vectors. These visualizations are color-coded to reflect two specific symbolic attributes: (a) complexity of the function, and (b) a predetermined classification based on

Figure 3: 2D t-SNE plots of the latent space of Numeric encoded vectors \(_{V}\) of pretrained SNIP on 1D datasets, colored by **(a)** Function Complexity, and **(b)** Function Classes based on Operators.

Figure 2: 2D t-SNE visualizations of the latent space of Symbolic encoded vectors \(_{S}\) of pre-trained SNIP, colored by **(a)** Non-Convexity Ratio, **(b)** Upwardness, **(c)** Average of normalized \(y\), and **(d)** Oscillations (Logarithmic scale).

dominant operators in the functions. The _Function Complexity_ refers to the function's length when represented in prefix order notation, effectively counting the number of nodes in its expression tree. On the other hand, _Function Operator Categorization_ is a broader classification that groups functions based on the predominant operators present in their symbolic mathematical expressions. These operators not only affect the function's behavior but also provide insights into the nature of the data they represent. It's crucial to recognize that a function may encompass several operators, adding layers to the intricacy of the data's behavior. Moreover, specific operators within a function might have a pronounced impact, dictating the data's scope and pattern. More details of these symbolic attributes and categorization are provided in App. C.

Latent Space Interpolation.To delve deeper into the interpolation capabilities of SNIP's pre-trained latent representations, we paired the SNIP encoder with an equation generation decoder. This fusion enabled us to explore the latent space in greater depth, unveiling the interpolatability inherent in SNIP's representations. The notion of _interpolatability_, as vividly illustrated in Fig.4, speaks to a profound association between the latent space embeddings and their corresponding numeric behaviors. In the presented figure, we start with a source function, represented by the numeric encoded vector \(_{V}^{s}\) (visualized as the blue curve). We then select a destination function, represented by \(_{V}^{d}\) (depicted as the orange curve). A linear interpolation is carried out between these numeric encoded vectors to derive an intermediate representation, \(_{V}^{int}\). When this interpolated latent vector is decoded, we obtain a symbolic function represented as \(\). Evaluating \(\) over the dataset \(\) reveals a fascinating insight: _the interpolated function manifests behavior that semantically bridges the gap between the behaviors of the source and destination functions_. Such latent space dynamics, ingrained through SNIP's multi-modal pre-training, offer a valuable edge for sophisticated searches and explorations, potentially benefiting various subsequent downstream tasks.

## 5 Using SNIP for Cross-modal Property Prediction

To further evaluate SNIP's capability for cross-modal comprehension between symbolic and numeric domains, we conducted targeted experiments. These tests aimed to assess the model's attitude for predicting specific mathematical properties from one domain based on insights from the other--a non-trivial task requiring mutual understanding of both. Due to space limitations, only results for _NCR_ and _Upwardness_, as described in section 4, are discussed here. More experiments and SNIP's pre-trained representations are provided in App. D.

### Models and Training

To assess property prediction using SNIP's embeddings, we employ a predictor head that passes these embeddings through a single-hidden-layer MLP to yield the predicted values. We adopt a Mean Squared Error (MSE) loss function for training on continuous properties. We consider three key training configurations to probe the efficacy of SNIP's learned representations:

* **Supervised Model**: Utilizes the same encoder architecture as SNIP but initializes randomly.
* **SNIP (frozen)**: Keeps the encoder parameters fixed, training only the predictor head.
* **SNIP (finetuned)**: Initializes encoder from pretrained SNIP, allowing full updates during training.

For a fair comparison, all model variants are trained on identical datasets comprising \(10\)K equations and subsequently tested on a distinct \(1\)K-equation evaluation dataset. These datasets are generated using the technique described in Sec. 3.4, as per .

Figure 4: Interpolatability of SNIP numeric latent space.

### Results

**Quantitative Results.** Table 1 presents the \(R^{2}\) and Normalized Mean Squared Error (NMSE) for all three models across the tasks of predicting _NCR_ and _Upwardness_. Results reveal a significant gap in performance between the purely supervised model and those benefiting from SNIP's prior knowledge. This performance gap can be attributed to SNIP's pre-trained, semantically rich representations, enabling enhanced generalization to unseen functions. Additionally, fine-tuning the SNIP encoder results in marginal performance gains, indicating the model's capability to adapt to specific downstream tasks.

**Qualitative Findings.** To delve deeper into the power of SNIP's representations, we compared its pre-finetuning and post-finetuning latent spaces against that of a supervised model lacking pre-training, using t-distributed Stochastic Neighbor Embedding (t-SNE) . The visualizations are color-coded by the corresponding properties (Fig. 5). Consistent with the quantitative outcomes, the supervised model's latent space, shown in Fig. 5(a), exhibits limited structural coherence. In contrast, SNIP's latent space in Fig. 5(b) shows pronounced clustering and distinct property trends. Notably, further fine-tuning of the encoder for these prediction tasks, depicted in Fig. 5(c), results in a more structured latent space, marked by clearer linear trends in properties. This finding under-scores SNIP's quantitative advantages and its flexibility in adapting to downstream tasks.

**Few-shot Learning Analysis.** We evaluated how training sample size influences the test \(R^{2}\) scores for predicting _NCR_, assessing three model variants on a fixed \(1\)K-sample test set (Fig. 6). In few-shot scenarios with just \(100\) training samples, the supervised model's score fell sharply to \(0.292\), while both SNIP variants maintained scores above \(0.745\). At \(10\)K training samples, SNIP's performance advantage remained consistent. Upon increasing the training sample size to \(1\)M, all models showed improvement; the supervised model notably increased its score to \(0.867\). Yet, both fine-tuned and frozen SNIP variants continued to lead, posting scores of \(0.973\) and \(0.942\), respectively. These results emphasize SNIP's superior generalization from limited data, underscoring the SNIP's rich semantic encodings.

## 6 Discussion and Conclusion

We introduced SNIP, a multi-modal symbolic-numeric pre-training model that learns how to associate the symbolic and numeric aspects of mathematical functions. We showed that SNIP exhibits remarkable few-shot capabilities in estimating cross-modal mathematical properties, outperforming fully-supervised models. While SNIP showcases robustness and versatility in integrating symbolic and numeric learning, it has notable limitations. It struggles with data patterns that cannot be clearly expressed as closed-form mathematical functions. Also, its performance is tied to the

    &  &  \\   & \( R^{2}\) & \(\) NMSE & \( R^{2}\) & \(\) NMSE \\  Supervised & 0.4701 & 0.5299 & 0.4644 & 0.5356 \\ SNIP (frozen) & 0.9269 & 0.0731 & 0.9460 & 0.0540 \\ SNIP (finetuned) & **0.9317** & **0.0683** & **0.9600** & **0.0400** \\   

Table 1: Results of using SNIP for property prediction.

pre-defined data generation protocol, adopted from [3; 8], which sets constraints on factors such as the vocabulary of mathematical operators. Despite these limitations, SNIP has a wide range of capabilities, presenting a powerful tool in the intersection of symbolic and numeric mathematics. Looking ahead, SNIP offers a rich foundation for numerous advancements. Future research can harness numeric guidance to enhance symbolic-to-symbolic tasks like function integration. Conversely, symbolic insights might enhance numeric-to-numeric tasks, such as zero-shot extrapolation and super-resolution. The mutual symbolic and numeric understandings within SNIP could also open doors for complex multi-modal tasks, notably in numeric-to-symbolic equation generation or symbolic regression. Furthermore, the embeddings learned by SNIP present an opportunity to craft novel metrics for evaluating symbolic-numeric proximity and to develop efficient methodologies for data and feature valuation in symbolic-numeric tasks. In essence, SNIP's contributions extend far beyond its current scope, encouraging a future filled with cross-disciplinary innovations.