# Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound

Reuben Adams

Department of Computer Science

University College London

reuben.adams.20@ucl.ac.uk

&John Shawe-Taylor

Department of Computer Science

University College London

j.shawe-taylor@ucl.ac.uk

Benjamin Guedj

Department of Computer Science, University College London and Inria

b.guedj@ucl.ac.uk

###### Abstract

Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of \(M\) error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously.

## 1 Introduction

Generalisation bounds are a core component of the theoretical understanding of machine learning algorithms. For over two decades now, PAC-Bayesian theory has been at the core of studies on generalisation abilities of machine learning algorithms. PAC-Bayes originated in the seminal work of McAllester (1998, 1999) and was further developed by Catoni (2003, 2004, 2007), among other authors--we refer to the surveys Guedj (2019) and Alquier (2021) for an introduction to the field. The outstanding empirical success of deep neural networks in the past decade calls for better theoretical understanding of deep learning, and PAC-Bayes has emerged as one of the few frameworks that can be used to derive meaningful (and non-vacuous) generalisation bounds for neural networks: the pioneering work of Dziugaite and Roy (2017) has been followed by a number of contributions, including Neyshabur et al. (2018); Zhou et al. (2019); Letarte et al. (2019); Perez-Ortiz et al. (2021); Perez-Ortiz et al. (2021); Biggs and Guedj (2022a,b), to name but a few.

Much of the PAC-Bayes literature focuses on the case of binary classification, or of multiclass classification where one only distinguishes whether each classification is correct or incorrect. This is in stark contrast to the complexity of contemporary real-world learning problems, such as medical diagnosis where the severity of Type I and Type II errors may be crucial and context-dependent. This work aims to bridge this gap by deriving a generalisation bound that provides information-rich measures of performance at test time by controlling the probabilities of errors of any finite number of user-specified types. More precisely, we bound the KL-divergence between the empirical and truedistributions over the different error types. From this single bound one can then derive bounds on arbitrary linear combinations of these error probabilities, which will all hold simultaneously with the same probability as the original bound. In addition, these bounds are guaranteed to be non-vacuous (this follows since the KL-divergence blows up on the boundary of the simplex).

As a concrete example, if the severity of Type I and Type II errors of a medical test are context-dependent, one would want to be able to bound arbitrary linear combinations of these error probabilities. Existing bounds could only bound finitely many pre-specified weightings by employing a union bound, which would also degrade the bound. In contrast, by constraining the KL-divergence between the true and empirical error probabilities, our bound constrains all uncountably many weightings of the error probabilities simultaneously.

The usual setting of PAC-Bayes bounds is that of binary classification, namely an input set \(\), output set \(=\{-1,1\}\), hypothesis space \(^{}\) and a sample \(S()^{m}\) drawn i.i.d. from a data-generating distribution \(D\). A number of PAC-Bayes bounds in this setting (e.g. Maurer (2004)) have been unified by a single general bound found in Begin et al. (2016). Briefly, Begin et al. (2016) prove a bound on the discrepancy \(d(R_{S}(Q),R_{D}(Q))\) between the error probability \(R_{D}(Q)\) of a stochastic classifier \(Q\) (a distribution over \(\) which classifies by first drawing \(h Q\) and then classifying according to \(h\)) and its empirical counterpart \(R_{S}(Q)\) (the fraction of the sample \(Q\) misclassifies). The bound holds with high probability for all \(Q\) simultaneously. The bound in Begin et al. (2016) is binary in the sense that \(\) contains two elements, but a more subtle way to look at this is that only two cases are distinguished--correct classification and incorrect classification. While it can be applied to multiclassification provided one maintains the second binary characteristic by only distinguishing correct and incorrect classifications. It is this heavy restriction that our result lifts, by considering the new framework of _error types_.

A new framework of errors typesWe consider a user-specified partition of the space \(\) of prediction-truth label-pairs into a finite partition of error types \(E_{1},,E_{M}\). Our bound then simultaneously constrains the probability with which errors of each type occur. In multiclass classification for example, one can choose the error types to be the set of all different possible mis-classifications, in which case our bound will control the entire confusion matrix, bounding how far the true confusion matrix (i.e. expected over the data-generating distribution) can diverge from the empirical one (i.e. on the training set). From this one can then derive bounds on the probabilities with which each mis-classification may be made, and arbitrary linear combinations of these error probabilities, and all of these will hold simultaneously with the same probability as the original bound. Our bound therefore paints a far richer picture of the performance of the final learned model than can be provided by any existing PAC-Bayes bound.

Formally, we let \(_{j=1}^{M}E_{j}\) be a user-specified disjoint partition of \(^{2}\) into a finite number of \(M\)_error types_, where we say that a hypothesis \(h\) makes an error of type \(j\) on datapoint \((x,y)\) if \((h(x),y) E_{j}\) (by convention, every pair \((,y)^{2}\) is interpreted as a predicted value \(\) followed by a true value \(y\), in that order). It should be stressed that not all of the \(E_{j}\) need correspond to mislabellings--indeed, some of the \(E_{j}\) may distinguish different correct labellings.

Relation to previous resultsOur framework of a finite number of user-specified "error types" includes multiclass classification as a particular case, and it is in this field that one finds the work most closely related to ours. Little is known of multiclass classification from a theoretical perspective and, to the best of our knowledge, only a handful of relevant strategies or generalisation bounds can be compared to the present paper.

The closest is the work of Morvant et al. (2012), which establishes a PAC-Bayes bound on the spectral norm of the difference between the true and empirical confusion matrices. Our bound differs from theirs in two respects. First, they consider the confusion matrix, whereas ours applies to the more general setting of a finite number of error types, which can be the set of all mis-classifications, or some partition thereof. Second, they deal with the spectral norm, whereas we employ the KL-divergence. Since the KL-divergence follows a simple formula, this means we can much more easily infer bounds on the individual error probabilities, which would be very challenging for the spectral norm. The follow-up work Koco and Capponi (2013) shows how a proxy of the spectral norm bound can be used as a training objective that may deal with imbalanced classes. In the present work, we show how our bound can be used as a differentiable training objective directly (without the need of a proxy) and that it can more sensitively deal with imbalanced classes, or errors of different severity, by assigning each error type a user-specified loss value.

Laviolette et al. (2017) extend the celebrated \(\)-bound in PAC-Bayes to ensembles, obtaining a bound on the risk of the majority vote classifier in the case of multiclass classification. In this context, our bound is able to distinguish different mis-classifications and control them, whereas they bound the scalar risk which lumps all mis-classifications together. The \(\)-bound has alternately been generalised by Lacasse et al. (2006) (see also Germain et al. (2015)) to simultaneously control three metrics, namely the so-called _expected disagreement, expected joint success and expected joint error_ of the posterior. While they restricted themselves to the ternary case, some of their proof techniques share similarities with ours. In cases where one has exactly three error types, for example the \(\{-1,0,1\}\)-valued _excess loss_, the work of Wu and Seldin (2022) is applicable; they construct so-called'split-kl' inequalities (both classical and PAC-Bayesian) which deftly handle this specific scenario.

Pires et al. (2013) present a comprehensive analysis of convex surrogate losses in cost-sensitive multiclass classification, providing conditions for consistency, bounding the excess loss of a predictor, and extending the analysis to the "Simplex Coding" scheme. We are considering the generalisation gap rather than the excess loss. Lei et al. (2019) study data-dependent bounds for multiclass classification. Their analysis is restricted to SVMs however, whereas ours applies to arbitrary hypothesis spaces.

Outline.We fix notation in Section 2. Theorem 1 in Section 3 is our main result--a PAC-Bayes bound on the KL-divergence between the true and empirical error distributions. For multiclass classification with a fully refined partition this becomes a bound on the KL-divergence between the true and empirical confusion matrices. Proposition 1 then bounds the individual error probabilities. Our second main result, Theorem 2 in Section 4, allows us to use bounds on _linear combinations_ of error probabilities as training objectives. We prove Theorem 1 in Section 5 via Proposition 4, which bounds the distribution of errors via a general convex function \(d\) and may be of independent interest. Section 6 outlines positive empirical results1 from using our bound as a training objective for neural networks and Section 7 gives perspectives for follow-up work..

## 2 Notation

For any set \(A\), let \((A)\) be the set of probability measures on \(A\). Let \(\) and \(\) be arbitrary input (_e.g._, feature) and output (_e.g._, label) sets respectively, and \(D()\) be a data-generating distribution. For any sample \(S D^{m}\) drawn i.i.d. from \(D\), let \((S)()\) denote the empirical distribution \((S):=_{(x,y) S}_{(x,y)}\). We consider the setting where the user has specified a partition \(\{E_{1},,E_{M}\}\) of \(^{2}\) into \(M\)_error types_.

We are interested in _simple_ hypotheses \(h:\) and _soft_ hypotheses \(H:()\). For example, a neural network outputting scores (logits) in \(^{}\) is converted to a simple or soft hypothesis, respectively, by passing the scores through the argmax or softmax function, respectively. For any \(A\), \(H(x)(A)\) can be interpreted as the probability according to \(H\) that the label of \(x\) is in \(A\). We will see in Section 4 that soft hypotheses permit more flexible training procedures and a more fine-grained analysis. Note that while soft hypotheses output distributions, they do so deterministically, always returning the same distribution for the same input \(x\), and so are distinct from the stochastic classifiers introduced shortly.

For a simple hypothesis \(h:\) and \(j[M]\), define the _\(j\)-risk_ of \(h\) to be \(R^{j}_{D}(h):=_{(x,y) D}((h(x),y) E_{j})\), namely the probability that \(h\) makes an error of type \(E_{j}\) for a randomly sampled \((x,y) D\). For a soft hypothesis \(H:()\) define the _\(j\)-risk_ of \(H\) to be \(R^{j}_{D}(H):=_{(x,y) D, H(x)}((,y) E_{j})\), namely the probability that one would make an error of type \(E_{j}\) on a randomly sampled \((x,y) D\) if one predicted by sampling \(\) from the distribution \(H(x)\). From now until Section 4 it will not matter whether we are dealing with simple or soft hypotheses. So, unless stated explicitly, we will refer to both simply as hypotheses, denote both by lowercase \(h\), and refer to the hypothesis class \(\), whether it is a subset of \(^{}\) or \(()^{}\).

Our goal is to control the _risk vector_\(_{D}(h):=(R^{1}_{D}(h),,R^{M}_{D}(h))\), since controlling this vector controls all linear combinations of \(j\)-risks. Since this is unobservable, we will control it by bounding how far it diverges from its empirical counterpart \(_{S}(h):=_{(S)}(h)\), which we term the _empirical risk vector_. Note that \(_{S D_{m}}_{S}(h)=_{D}(h)\), and that, for a simple hypothesis \(h^{}\), \(_{S}(h)\) is the vector of proportions of the sample on which \(h\) makes an error of type \(E_{j}\)2. Since the \(E_{j}\) partition \(^{2}\), \(_{D}(h)\) and \(_{S}(h)\) are elements of the \(M\)-dimensional simplex \(_{M}:=\{^{M}:u_{1}++u_{M}=1\}\). Thus we can choose our divergence measure to be \((_{S}(h)\|_{D}(Q))\), where for \(,_{M}\) we define \((\|):=_{j=1}^{M}q_{j}}{p_{j}}\).3 When \(M=2\) we abbreviate \(((q,1-q)\|(p,1-p))\) to \((q\|p)\), which is then the conventional definition of \((\|)\) found in the PAC-Bayes literature (as in Seeger, 2002, for example). We define the _risk_ and _empirical risk_ of \(Q\) as \(_{D}(Q):=_{h Q}_{D}(h)\) and \(_{S}(Q):=_{h Q}_{S}(h)\), respectively, and seek a bound on \((_{S}(Q)\|_{D}(Q))\). Note we still have \(_{S}[_{S}(Q)]=_{D}(Q)\), this time using Fubini. Moreover, for a sample \(S\) of size \(m\), we have that \(_{S}(Q)=/m\) where \((m,M,_{D}(Q))\). Recall that for \(m,M\) and \(_{M}\), the multinomial distribution \((m,M,)\) has probability mass function \((;m,M,):=k_{1}&k_{2}&m\\ k_{1}&k_{2}&&k_{M}_{j=1}^{M}r_{j}^{k_{j}}\), where \(k_{1}&k_{2}&m\\ k_{1}&k_{2}&&k_{M}:=^{M}k_ {j}!}\) for \( S_{m,M}:=(k_{1},,k_{M})_{0}^{M}:k_{1}+ +k_{M}=m}\), and zero otherwise. As a final piece of notation, we let \(_{M}^{>0}:=_{M}(0,1)^{M}\) and \(S_{m,M}^{>0}:=S_{m,M}^{M}\) denote the vector elements of \(_{M}\) and \(S_{m,M}\), respectively, that have no zero components.

## 3 Main result

We now state our main result, which bounds the KL-divergence between the true and empirical risk vectors \(_{D}(Q)\) and \(_{S}(Q)\), interpreted as probability distributions. As is conventional in the PAC-Bayes literature, we refer to sample independent and dependent distributions on \(()\) (_i.e._ stochastic hypotheses) as priors (denoted \(P\)) and _posteriors_ (denoted \(Q\)) respectively, even if they are not related by Bayes' theorem.

**Theorem 1**.: _Let \(\) and \(\) be arbitrary sets and \(_{j=1}^{M}E_{j}\) be a disjoint partition of \(^{2}\) into \(M\) error types. Let \(D()\) be a data-generating distribution and \(\) be a simple (\(^{}\)) or soft (\(()^{}\)) hypothesis class. For any prior \(P()\), \((0,1]\) and sample size \(m M\), with probability at least \(1-\) over the random draw \(S D^{m}\), we have that simultaneously for all posteriors \(Q()\), the divergence \(_{S}(Q)\|_{D}(Q)\) is upper bounded by_

\[[(Q\|P)+],\] (1)

\((m,M):=e^{1/(12m)}()^{}_{z=0 }^{M-1}()^{z/2}( )^{-1}((mM)^{M}).\)

The fact that the logarithmic term is of order \((M(mM/))\) means the bound is linear in \(M\) up to logarithmic terms, while this may seem excessive, one should note that the quantity that our theorem bounds also depends on \(M\). Further, the bound has been successfully used in by Biggs and Guedj (2023) to improve on state of the art PAC-Bayes bounds.

To see how our bound compares to existing PAC-Bayes bounds for binary classification, take \(=\{-1,1\}\), \(M=2\), \(E_{1}=\{(-y,y):y\}\) and \(E_{2}=\{(y,y):y\}\). The argument of the logarithm then reduces to \(e^{1/(12m)}(2+}) 1.25\) when \(m\) is large. The corresponding term in Maurer (2004) is \(2\), which is only larger because he relaxes the term for aesthetics. Therefore our bound gracefully reduces to Maurer's in the case of binary classification.

Suppose after a use of Theorem 1 we have a bound of the form \((_{S}(Q)\|_{D}(Q)) B\). We can then derive bounds on the individual \(j\)-risks \(R_{D}^{j}(Q)\) or, more generally, on linear combinations thereof. While one could obtain such bounds perhaps more directly with existing PAC-Bayes bounds, the significance of our bound is that _all_ such derived bounds hold with high probability _simultaneously_. Existing PAC-Bayes bounds would require the use of a union bound in order to bound multiple combinations simultaneously, whereas ours bounds all uncountably many combinations simultaneously, as a package. As for the individual \(j\)-risksks \(R_{D}^{j}(Q)\), the following proposition then yields the bounds \(L_{j} R_{D}^{j}(Q) U_{j}\), where \(L_{j}:=\{p:(R_{S}^{j}(Q)\|p) B\}\) and \(U_{j}:=\{p:(R_{S}^{j}(Q)\|p) B\}\). Moreover, since in the worst case we have \((_{S}(Q)\|_{D}(Q))=B\), the proposition shows that the lower and upper bounds \(L_{j}\) and \(U_{j}\) are the tightest possible, since if \(R_{D}^{j}(Q)[L_{j},U_{j}]\) then \((R_{S}^{j}(Q)\|R_{D}^{j}(Q))>B\) implying \((_{S}(Q)\|_{D}(Q))>B\). For a more precise version of this argument and a proof of Proposition 1, see Appendix C.4.

**Proposition 1**.: _Let \(,_{M}\). Then \((q_{j}\|p_{j})(\|)\) for all \(j[M]\), with equality when \(p_{i}=}{1-q_{j}}q_{i}\). for all \(i j\)._

More generally, suppose we can quantify how costly an error of each type is by means of a loss vector \([0,)^{M}\), where \(_{j}\) is the loss we attribute to an error of type \(E_{j}\). We may then be interested in bounding the _total risk_\(R_{D}^{T}(Q):=_{D}(Q)\). Then, given a bound \((_{S}(Q)\|_{D}(Q)) B\) from Theorem 1, we can deduce

\[R_{D}^{T}(Q)\{:_{M},(_{S}(Q)\|) B\}=_{}^{-1}( _{S}(Q)|B),\] (2)

where we define \(_{}^{-1}(|c)_{M}\) as follows. To see that it is indeed well-defined (at least when \(_{M}^{>0}\)), see the discussion at the beginning of Appendix C.5.

**Definition 1**.: _For \(_{M},c[0,)\) and \([0,)^{M}\), define \(_{}^{-1}(|c)\) to be an element \(_{M}\) solving the constrained optimisation problem_

\[ f_{}():=,\] (3) \[(\|) c.\] (4)

This motivates the following training procedure: search for a posterior \(Q\) for which the bound \(_{}^{-1}(_{S}(Q)|B)\) on the total risk \(R_{D}^{T}(Q)\) is minimised. While this requires a particular choice of loss vector \(\), we emphasise that at the end of training, Theorem 1 bounds \((_{S}(Q)\|_{D}(Q))\) and so can be used to bound _any_ linear combination of the \(j\)-risks, not just the one given the loss vector \(\) chosen for training. It is this flexibility which is the main advantage of our bound; changes in the severity of different error types over time do not require union bounds or retraining.

In the next section we provide a theorem for calculating \(_{}^{-1}(|c)\) and its derivatives so that the training procedure can be executed.

## 4 Construction of a Differentiable Training Objective

We now state and prove Theorem 2, which provides a speedy method for approximating \(_{}^{-1}(|c)\) and its derivatives to arbitrary precision, provided \(c>0\) and \( j\ u_{j}>0\). The only approximation step required is that of approximating the unique root of a continuous and strictly increasing scalar function. Thus, provided the \(u_{j}\) themselves are differentiable, Theorem 1 combined with Theorem 2 shows that the upper bound on the total risk can be used as a tractable and fully differentiable training objective. See Appendix A for more details, including a pseudocode algorithm and an implementation. Since the proof of Theorem 2 is rather long and technical, we defer it to Appendix C.5. The requirement that the \(_{j}\) are not all equal only rules out trivial cases where \(R_{D}^{T}(Q)\) is independent of \(_{D}(Q)\).

**Theorem 2**.: _Fix \([0,)^{M}\) such that not all \(_{j}\) are equal, and define \(f_{}:_{M}[0,)\) by \(f_{}():=_{j=1}^{M}_{j}v_{j}\). For all \(}=(,c)_{M}^{>0}(0,)\), define \(^{*}(}):=_{}^{-1}(|c)_{M}\) and let \(^{*}(})(-,-_{j}_{j})\) be the unique solution to \(c=_{}()\), where \(_{}:(-,-_{j}_{j})\) is given by \(_{}():=(-_{j=1}^{M}}{+_{j}})+_{j= 1}^{M}u_{j}(-(+_{j}))\), which is continuous and strictly increasing. Then \(^{*}(})=_{}^{-1}(|c)\) is given by_

\[^{*}(})_{j}=(})u_{j}}{^{*}( })+_{j}}j[M],\ \ ^{*}(})=(_{j=1}^{M}}{^{* }(})+_{j}})^{-1}.\] (5)

_Further, defining \(f_{}^{*}:_{M}^{>0}(0,)[0,)\) by \(f_{}^{*}(}):=f_{}(^{*}(}))\), we have that_

\[}^{*}}{ u_{j}}(})=^{*}( })(1+}{^{*}(})_{j}}) }^{*}}{ c}( })=-^{*}(}).\] (6)A final wrinkle in evaluating our bound is that while the empirical risk vector \(_{S}(Q)=_{h Q}_{S}(h)\) does not depend on the data-generating distribution \(D\), the expectation over \(Q\) may still be intractable. This would be the default case when \(Q\) is a Gaussian over the weights of a multi-layer perceptron, for example. In such cases, we can estimate \(_{S}(Q)\) via a Monte Carlo sample \(_{S}():=_{n=1}^{N}_{S}(h_{n})\) (where the \(h_{n}\) are drawn i.i.d. from \(Q\)) and use the following two results. Proposition 2 shows that the \((R_{S}^{j}()\|R_{D}^{j}(Q))\) can be simultaneously bounded, whence Proposition 3 can be used to obtain a bound on \((_{S}()\|_{D}(Q))\).

**Proposition 2**.: _Let \((N,M,)\). Then for any \((0,1)\), with probability at least \(1-\) we have that for all \(j[M]\) simultaneously \((X_{j}p_{j})}{N}\)._

Proof.: Each bound holds separately with probability at least \(1-/M\) by Theorem 2.5 in Langford and Caruana (2001). They then hold simultaneously by application of a union bound. 

**Proposition 3**.: _Suppose \(,,}_{M}\) are such that \((\|) B_{1}\) and \((_{j}\|q_{j}) B_{2}\) for all \(j[M]\). For each \(j\), define \(_{j}=\{r:(_{j}\|r) B_{2}\}\). Then_

\[(}\|) MB_{2}-_{j=1}^{M}(1-_{j}) _{j}}{1-_{j}}+B_{1}_{j}_{j}}{ _{j}} B_{1} B_{2} 0.\] (7)

Proof.: Deferred to C.1. 

The fact that the bound on \((}\|) B_{1}\) as \(B_{2} 0\) ensures that as we increase the size of our Monte Carlo sample for estimating \(_{S}(Q)\) the bound on \((_{S}()\|_{D}(Q))\) approaches that of \((_{S}(Q)\|_{D}())\), meaning in the limit we pay an arbitrarily small price in the bound for the approximation.

## 5 Proof of the main bound

We split the proof of Theorem 1 into three parts. First, we prove Proposition 4, a bound on \(d(_{S}(Q),_{D}(Q))\) for an arbitrary convex function \(d\), which may be of independent interest. Second, we prove Corollary 1 by specialising Proposition 4 to the case \(d(,)=(\|)\). Finally, we show that the bound in Theorem 1 is a loosened version of the bound in Corollary 1.

**Proposition 4**.: _Let \(d:_{M}^{2}\) be jointly convex. In the setting of Theorem 1,_

\[d_{S}(Q),_{D}(Q)[ (Q\|P)+_{d}(m,)}{}],\] (8)

\[_{d}(m,):=_{_{M}}[_{ S _{m,M}}(;m,M,)(\! d(}{ m},)\!)].\]

This is a generalisation of the unifying PAC-Bayes bound given in Begin et al. (2016) where we replace the scalar risk quantities \(R_{S}(Q)\) and \(R_{D}(Q)\) with their vector counterparts \(_{S}(Q)\) and \(_{D}(Q)\). To see this, note that we can recover it by setting \(=\{-1,1\}\), \(M=2\), \(E_{1}=\{(-y,y):y\}\) and \(E_{2}=\{(y,y):y\}\). Then, for any convex function \(d:^{2}\), apply Theorem 4 with the convex function \(d^{}:_{M}^{2}\) defined by \(d^{}((u_{1},u_{2}),(v_{1},v_{2})):=d(u_{1},v_{1})\) so that Theorem 4 bounds \(d^{}_{S}(Q),_{D}(Q)=dR_{S}^{1}(Q),R_{D} ^{1}(Q)\) which equals \(d(R_{S}(Q),R_{D}(Q))\) in the notation of Begin et al. (2016). Further, \(_{ S_{m,2}}(;m,2,)( d^{ }}{m},\,)=_{k=0}^{m}(k;m,r_{1})( d(}{m},r_{1}))\), so that the supremum over \(r_{1}\) of the right hand side equals the supremum over \(_{2}\) of the left hand side, which, when substituted into (8), yields the bound given in Begin et al. (2016).

To prove Proposition 4 we require the following two lemmas. The first is the well-known change of measure in equality (Csiszar, 1975; Donsker and Varadhan, 1975). The second is a generalisation from Binomial to Multinomial distributions of a result found in Maurer (2004), the proof of which we defer to Appendix C.2.

**Lemma 1**.: _For any set \(\), any \(P,Q()\) and any measurable function \(:\), \(}(h)(Q\|P)+}((h))\)._

**Lemma 2**.: _Let \(_{1},,_{m}\) be i.i.d \(_{M}\)-valued random vectors with mean \(\) and suppose that \(f:_{M}^{m}\) is convex. If \(^{}_{1},,^{}_{m}\) are i.i.d. \((1,M,)\) random vectors, then \([f(_{1},,_{m})][f(^{}_{ 1},,^{}_{m})]\)._

The consequence of Lemma 2 is that the worst case (in terms of bounding \(d(_{S}(Q),_{D}(Q))\)) occurs when \(_{\{(x,y)\}}(h)\) is a one-hot vector for all \((x,y) S\) and \(h\), namely when \(()^{}\) only contains hypotheses that, when labelling \(S\), put all their mass on elements \(\) that incur the same error type4. In particular, this is the case for hypotheses that put all their mass on a single element of \(\), equivalent to the simpler case \(^{}\) as discussed in Section 2. Thus, Lemma 2 shows that the bound given in Proposition 4 cannot be made tighter only by restricting to such hypotheses.

Proof.: (of Proposition 4) The case \(^{}\) follows directly from the more general case by taking \(^{}:=\{h^{}()^{}:  h\) such that \( x\ \ h^{}(x)=_{h(x)}\}\), where \(_{h(x)}()\) denotes a point mass on \(h(x)\). For the general case \(()^{}\), using Jensen's inequality with the convex function \(d(,)\) and Lemma 1 with \((h)= d_{S}(h),_{D}(h)\), we see that for all \(Q()\)

\[ d_{S}(Q),_{D}(Q) = d(}_{S}(h), }_{D}(h))\] \[} d_{S}(h), _{D}(h)\] \[(Q\|P)+(}  d_{S}(h),_{D}(h))\] \[=(Q\|P)+(Z_{P}(S)),\]

where \(Z_{P}(S):=_{h P} d(_{S}(h),_{D}(h)) \). Note that \(Z_{P}(S)\) is a non-negative random variable, so that by Markov's inequality \(}{}Z_{P}(S)_{S^{ } D}Z_{P}(S^{})}{} 1-\). Thus, since \(()\) is strictly increasing, with probability at least \(1-\) over \(S D^{m}\), we have that simultaneously for all \(Q()\)

\[ d_{S}(Q),_{D}(Q)(Q\|P)+  D^{m}}{}Z_{P}(S^{})}{}.\] (9)

To bound \(_{S^{} D^{m}}Z_{P}(S^{})\), let \(_{i}:=_{\{(x_{i},y_{i})^{}\}}(h)_{M}\) for \(i[m]\), where \((x_{i},y_{i})^{}\) is the \(i\)'th element of the dummy sample \(S^{}\). Noting that each \(_{i}\) has mean \(_{D}(h)\), define the random vectors \(^{}_{i}(1,M,_{D}(h))\) and \(:=_{i=1}^{m}^{}_{i}(m,M,_{D}(h))\). Finally let \(f:_{M}^{m}\) be defined by \(f(x_{1},,x_{m}):= d(_{i=1}^{m}x_{i}, _{D}(h))\), which is convex since the average is linear, \(d\) is convex and the exponential is non-decreasing and convex. Then, by swapping expectations (which is permitted by Fubini's theorem since the argument is non-negative) and applying Lemma 2, we have that \(_{S^{} D^{m}}Z_{P}(S^{})\) can be written as

\[_{S^{} D^{m}}Z_{P}(S^{}) = D^{m}}{}\ } d_{S^{}}(h), _{D}(h)\] \[=}\ _{1},,_{m}}{ }( d(_{i=1}^{m}_{i},_{D}( h)))\] \[}\ ^{}_{1},,^{ }_{m}}{}( d(_{i=1}^{m}^{ }_{i},_{D}(h)))\] \[=}\ }( d (,_{D}(h)))\]\[=}_{h P}\ _{ S_{m,M}} ;m,M,_{D}(h)( d}{m}, _{D}(h))\] \[_{_{M}}[_{ S_{m,M}} ;m,M,( d} {m},)],\]

which is the definition of \(_{d}(m,)\). Inequality (8) then follows by substituting this bound on \(_{S^{} D^{m}}Z_{P}(S^{})\) into (9) and dividing by \(\). 

We now specialise Proposition 4 to the case \(d(,)=(\|)\) to obtain Corollary 1.

**Corollary 1**.: _In the setting of Theorem 1,_

\[_{S}(Q)\|_{D}(Q) [(Q\|P)+ ],\] (10) \[(m,M) :=}_{ S_{m,M}}_{j=1}^{M}^{k_{j}}}{k_{j}!}.\] (11)

Proof.: Applying Proposition 4 with \(d(,)=(\|)\) and \(=m\) gives that with probability at least \(1-\) over \(S D^{m}\), simultaneously for all posteriors \(Q()\),

\[_{S}(Q)\|_{D}(Q)[ {KL}(Q\|P)+_{}(m,m)}{}],\]

where \(_{}(m,m):=_{_{M}}[_{ S_ {m,M}}(;m,M,)(m(}{m}, {r}))]\). Thus it suffices to show that \(_{}(m,m)(m,M)\).

To prove this, for each fixed \(=(r_{1},,r_{M})_{M}\) let \(J_{}=\{j[M]:r_{j}=0\}\). Then \((;m,M,)=0\) for any \( S_{m,M}\) such that \(k_{j} 0\) for some \(j J_{}\). For the other \( S_{m,M}\), namely those such that \(k_{j}=0\) for all \(j J_{}\), the probability term can be written as \((;m,M,)=^{m}k_{j}!}_{j=1}^{M }r_{j}^{k_{j}}=}}^{k_{j}}!}_{j J _{}}r_{j}^{k_{j}},\) and (recalling the convention that \(0=0\)) the term \((m(}{m},))\) can be written as

\[(m_{j=1}^{M}}{m}}{m}}{r_{j}} )=(_{j J_{}}k_{j}}{mr_{j}}) =_{j J_{}}(}{mr_{j}})^{k_{j}}= {m^{m}}_{j J_{}}(}{r_{j}})^{k_{j}},\]

where the last equality is obtained by recalling that the \(k_{j}\) sum to \(m\). Substituting these two expressions into the definition of \(_{}(m,m)\) and only summing over those \( S_{m,M}\) with non-zero probability, we obtain

\[_{ S_{m,M}}(;m,M,) m(}{m},) =_{ S_{m,M}:\\  j J_{}k_{j}=0}(;m,M,) (m(}{m},))\] \[=_{ S_{m,M}:\\  j J_{}k_{j}=0} }}k_{j}!}_{j J_{}}r_{j}^{k_{j}}}_{j J _{}}(}{r_{j}})^{k_{j}}\] \[=}_{ S_{m,M}:\\  j J_{}k_{j}=0}_{j J_{}}^{k_{j}}}{k_{j}!}\] \[=}_{ S_{m,M}:\\  j J_{}k_{j}=0}_{j=1}^{M}^{k_{j}}}{ k_{j}!}}{0!}=1$)}\] \[}_{ S_{m,M}}_{j=1}^{M}^{k_{j}}}{k_{j}!},\]which is \((m,M)\). Since this is independent of \(\), it also holds after taking the supremum over \(_{M}\) of the left hand side, showing that \(_{}(m,m)(m,M)\). 

The final step in obtaining Theorem 1 is to loosen the bound given in Corollary 1 (which is intractable when \(m\) is large) to the tractable form given in Theorem 1. For this we require the following technical lemma, the proof of which we defer to Appendix C.3.

**Lemma 3**.: _For integers \(M 1\) and \(m M\), \(_{ S_{m,M}^{>0}}^{M}}}}m^{}}{()}\)._

Proof.: (Of Theorem 1) It suffices to show that for all \(m M 1\) we have \((m,M)(m,M)\). We achieve this by applying Stirling's approximation \(()^{n}<n!<( )^{n}e^{}\) (valid for \(n 1\)) to the factorials in \((m,M)\) and then using Lemma 3.

Since Stirling's approximation requires that all the \(k_{j}\) are at least one, we partition the sum in \((m,M)\) according to the number of coordinates of \(\) at which \(k_{j}=0\). Let \(z\) index the number of such coordinates. Defining \(f:_{M=2}^{}S_{m,M}\) by \(f()=_{j=1}^{||}k_{j}^{k_{j}}/k_{j}!\) and noting that \(f\) is symmetric under permutations of its arguments, we then have

\[(m,M)=}_{ S_{m,M}}f()=} _{z=0}^{M-1}_{ S_{m,M-z}^{>0}}f().\] (12)

Stirling's approximation can now be applied to each \( S_{m,M}^{>0}\)\(f()_{j=1}^{M}^{k_{j}}}{}(}{e} )^{k_{j}}}=_{j=1}^{M}}}{}}=}{(2)^{M/2}}_{j=1}^{M}}}\). An application of Lemma 3 now gives

\[_{ S_{m,M-z}^{>0}}f()_{ S_{m,M-z}^{>0}} }{(2)^{}}_{j=1}^{M-z}}} }{(2)^{}}}m^{}}{()}=m^{}}{2^ {}()}.\]

Substituting this into equation (12) and bounding \(m!\) using Stirling's approximation, we have \((m,M)e^{1/(12m)}}{e^{m}}_{z=0}^{M-1}m^{}}{2^{}( )}=(m,M),\) which completes the proof of the bound. As for the order of the bound, it is sufficient to bound \((m,M)\) using the crude approximations \( M^{M}\), \((2/m)^{z/2} 1\) and \(((M-z)/2) 1\).

## 6 Numerical experiments

We use binarised versions of MNIST, and HAM10000 Tschandl (2018). In both cases we partition \(^{2}\) into \(E_{0}=\{(0,0),(1,1)\}\), \(E_{1}=\{(0,1)\}\) and \(E_{2}=\{(1,0)\}\), and take \(=(0,1,3)\). Each dataset is split into prior and certification sets. We take \(\) to be two-layer MLPs. As is common in the PAC-Bayes literature, we restrict \(P\) and \(Q\) to be isotropic and diagonal Gaussian distributions over the parameter space, respectively. The means of \(P\) and \(Q\) are set to the parameters of an MLP trained on the prior set. The mean and variances of \(Q\) and the variance of \(P\) are tuned via Theorem 2 to minimize the bound on the total risk \(R_{D}^{T}(Q)\). See Appendix A for pseudocode, Appendix B for full experimental details and https://github.com/reubenadams/PAC-Bayes-Control for code. The results for MNIST can be seen in Figure 1.

We estimate \(_{S}(Q)\) with a Monte Carlo and obtain a PAC-Bayes bound on \(R_{D}^{T}(Q)\) by combining Proposition 2 (with \(=0.01\) and \(N=100000\)) and Proposition 3. We obtain \(R_{D}^{T}(Q) 0.2640\) for MNIST and \(R_{D}^{T}(Q) 0.8379\) for HAM10000, where both bounds hold with probability at least \(1-0.05-0.01=0.94\). While these bounds are far from vacuous--the maximum possible value of \(R_{D}^{T}(Q)\) is \(3\) for our choice of \(\)--one might wonder whether one can do better by bounding each error probability individually using Maurer's inequality Maurer (2004), and then unioning these bounds. As with our Theorem 1, this would also constrain the entire distribution of error types since for any \(\), one could then calculate the maximimum value of \(R_{D}^{T}(Q)\) that satisfies all of these constraints. Bothmethods constrain the region of the simplex in which \(_{D}(Q)\) can lie (with high probability), and a reasonable metric by which to compare them is the volumes of these regions. This can be estimated via a MC sample by uniformly sampling points \(\) from \(_{M}\) and counting how samples are legal values of \(_{D}(Q)\) according to each method. The 95% confidence intervals for the volumes of the two regions are given in Table 1. A more comprehensive table for synthetic values of \(_{S}(Q)\) can be found in Appendix B.

## 7 Perspectives

We introduce the framework of error types, considering the vectors \(_{S}(Q)\) and \(_{D}(Q)\) of empirical and true probabilities of errors of different types. We prove a PAC-Bayes bound (Theorem 1) on \((_{S}(Q)\|_{D}(Q))\) which controls the entire distribution of error probabilities, and hence can be used to derive bounds on arbitrary linear combinations of the error probabilities, all of which hold simultaneously with high probability; this cannot be achieved with any existing PAC-Bayes bound.

We construct a differential training objective based on our bound by introducing the the vectorised kl inverse, providing a recipe for quickly computing its value and derivatives (Theorem 2). Our framework is flexible enough to encompass multiclass classification or discretised regression, but also structured output prediction, multi-task learning and learning-to-learn.

Another potential application of our work is to the excess risk, since under a misclassification loss there are three different error types, corresponding to excess losses of \(\{-1,0,1\}\). Biggs and Guedj (2023) adapted Theorems 1 and 2 to this setting, leading to an empirically tighter PAC-Bayes bound for certain classification tasks.

We require i.i.d. data, which in practice is frequently not the case or is hard to verify. Further, the number of error types \(M\) must be finite. In continuous scenarios it would be preferable to be able to control the entire distribution of loss values without having to discretise into finitely many error types. We leave this direction to future work.

 
**Dataset** & **Volume Our Region** & **Volume Maurer Region** \\  MNIST & **0.0025** (0.002498, 0.002504) & 0.0028 (0.002793, 0.002800) \\  HAM10000 & 0.0012 (0.001207, 0.001211) & **0.0011** (0.001142, 0.001146) \\  

Table 1: Point estimates and 95% confidence intervals for the volumes of the confidence regions for \(_{D}(Q)\) given by Theorem 1 and a union over \(M\) individual Maurer bounds, respectively. Our method is superior for MNIST and inferior for HAM100000.

Figure 1: Experimental results for binarised MNIST. (a) The PAC-Bayes bound on the total risk decreases when tuning the posterior via Theorem 2. (b) This is achieved by a shift in the empirical error probabilities. (c) The bound on \((_{S}(Q)\|_{D}(Q))\) is not substantially increased, meaning we still retain good control of \(_{D}(Q)\) after optimizing \(Q\) for this particular choice of \(\).