# Polynomial-Time Linear-Swap Regret Minimization

in Imperfect-Information Sequential Games

 Gabriele Farina

MIT

gfarina@mit.edu

&Charilaos Pipis

MIT

chpipis@mit.edu

###### Abstract

No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformations considered by the learner determines a natural notion of rationality. As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated equilibria in normal-form games and extensive-form correlated equilibria in extensive-form games. At the extreme, a _no-swap-regret_ agent is one that minimizes regret against the set of _all_ functions from the set of strategies to itself. While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained _efficiently_ in the worst case in _sequential_ (extensive-form) games is a longstanding open problem. In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to _all linear transformations_ of the mixed strategy space, a notion called _no-linear-swap regret_. This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential games--thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call _linear-deviation correlated equilibria_, that can be approached efficiently.

## 1 Introduction

The framework of regret minimization provides algorithms that players can use to gradually improve their strategies in a repeated game, enabling learning strong strategies even when facing unknown and potentially adversarial opponents. One of the appealing properties of no-regret learning algorithms is that they are _uncoupled_, meaning that each player refines their strategy based on their own payoff function, and on other players' strategies, but not on the payoff functions of other players. Nonetheless, despite their uncoupled nature and focus on _local_ optimization of each player's utility, it is one of the most celebrated results in the theory of learning in games that in many cases, when all players are learning using these algorithms, the empirical play recovers appropriate notions of _equilibrium_--a _global_ notion of game-theoretic optimality. Strategies constructed via no-regret learning algorithms (or approximations thereof) have been key components in constructing human-level and even superhuman AI agents in a variety of adversarial games, including Poker (Moravcik et al., 2017; Brown and Sandholm, 2018, 2019), Stratego (Perolat et al., 2022), and Diplomacy (Bakhtin et al., 2023).

In regret minimization, each learning agent seeks to minimize the difference between the loss (opposite of reward) they accumulated through the actions they played, and the loss they would haveaccumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformation functions considered by the learning agent determines a natural notion of rationality of the agent. Already when the agents seeks to learn strategies that cumulate low regret against _constant_ strategy transformations only--a notion of regret called _external_ regret--the average play of the agents converges to a Nash equilibrium in two-player constant-sum games, and to a coarse correlated equilibrium in general-sum multiplayer games. As the sets of transformations the each agent considers grows, more complex equilibria can be achieved, including correlated equilibria in normal-form games (Foster and Vohra (1997); Fudenberg and Levine (1995, 1999); Hart and Mas-Colell (2000, 2001); see also the monograph by Fudenberg and Levine (1998)) and extensive-form correlated equilibria in extensive-form games (Farina et al., 2022). At the extreme, a maximally hindsight-rational agent is one that minimizes regret against the set of _all_ functions from the strategy space to itself (aka. _swap_ regret). While it is known that maximum hindsight rationality can be attained efficiently in nonsequential (normal-form) games (Stoltz and Lugosi, 2007; Blum and Mansour, 2007), it is a major open problem to determine whether the same applies to sequential (_i.e._, extensive-form) games, and more generally what is the strongest notion of rationality that can be attained efficiently in the worst case in the latter setting.

In this paper, we provide a positive result in that direction, by showing that hindsight rationality can be achieved efficiently in general imperfect-information extensive-form games when one restricts to the set of _all linear transformations_ of the mixed strategy space--a notion called _linear-swap regret_, and that coincides with swap regret in normal-form games. In order to establish the result, we introduce several intermediate results related to the geometry of sequence-form strategies in extensive-form games. In particular, a crucial result is given in Theorem 3.1, which shows that the set of linear functions \(_{}\) from the sequence-form strategy set \(\) of a player in an extensive-form game to a generic convex polytope \(\) can be captured using only polynomially many linear constraints in the size of the game tree and the number of linear constraints that define \(\). Applying the result to the special case \(=\), we are then able to conclude that the the polytope of linear transformations \(_{}\) from the sequence-form strategy set to itself can be captured by polynomially many linear constraints in the size of the game tree, and the norm of any element is polynomially bounded. The polynomial characterization and bound for \(_{}\) is used in conjunction with an idea of Gordon et al. (2008) to construct a no-linear-swap-regret minimizer for the set of strategies \(\) starting from two primitives: i) a no-external-regret algorithm for the set of transformations \(_{}\), and ii) an algorithm to compute a fixed point strategy for any transformation in \(_{}\). In both cases, the polynomial representation of \(_{}\) established through Theorem 3.1 plays a fundamental role. It allows, on the one hand, to satisfy requirement ii) using linear programming. On the other hand, it enables us to construct a no-external-regret algorithm that outputs transformations in \(_{}\) with polynomial-time iterations, by leveraging the known properties of online projected gradient descent, exploiting the tractability of projecting onto polynomially-representable polytopes.

Finally, in the last section of the paper we turn our attention away from hindsight rationality to focus instead on the properties of the equilibria that our no-linear-swap-regret dynamics recover in extensive-form games. The average play of no-linear-swap-regret players converges to a set of equilibria that we coin _linear-deviation correlated equilibria (LCEs)_. LCEs form a superset of correlated equilibria and a subset of extensive-form correlated equilibria in extensive-form games. In Section 4 we show that these inclusions are in general strict, and provide additional results about the complexity of computing a welfare-maximizing LCE.

Related workAs mentioned in the introduction, the existence of uncoupled no-regret dynamics leading to correlated equilibrium (CE) in multiplayer normal-form games is a celebrated result dating back to at least the work by Foster and Vohra (1997). That work inspired researchers to seek uncoupled learning procedures in other settings as well. For example, Stoltz and Lugosi (2007) studies learning dynamics leading to CE in games with an infinite (but compact) action set, while Kakade et al. (2003) focuses on graphical games. In more recent years, a growing effort has been spent towards understanding the relationships between no-regret learning and equilibria in imperfect-information extensive-form games, the settings on which we focus. Extensive-form games pose additional challenges when compared to normal-form games, due to their sequential nature and presence of imperfect information. While efficient no-external-regret learning dynamics for extensive-form games are known (including the popular CFR algorithm (Zinkevich et al., 2008)), as of today not much is known about no-swap-regret and the complexity of learning CE in extensive-form games.

In recent work, Anagnostides et al. (2023) construct trigger-regret dynamics that converge to EFCE at a rate of \(O()\), whereas our regret dynamics converge to linear-deviation correlated equilibria at a slower rate of \(O(})\). While the aforementioned paper proposes a general methodology that applies to CEs in normal-form games and EFCE/EFCCE in sequential games, the authors' construction fundamentally relies on being able to express the fixed points computed by the algorithm as (linear combinations of) rational functions with positive coefficients of the deviation matrices. For EFCE and EFCCE this fundamentally follows from the fact that the fixed points can be computed inductively, solving for stationary distributions of local Markov chains at each decision point. The no-linear-swap regret algorithm proposed in our paper does not offer such a local characterization of the fixed point and thus, we cannot immediately transfer the improved regret bounds to our case.

The closest notion to CE that is known to be efficiently computable in extensive-form games is _extensive-form correlated equilibrium_ (EFCE) (von Stengel and Forges, 2008; Huang and von Stengel, 2008). The question of whether the set of EFCE could be approached via uncoupled no-regret dynamics with polynomial-time iterations in the size of the extensive-form games was recently settled in the positive (Farina et al., 2022; Celli et al., 2020). In particular, Farina et al. (2022) show that EFCE arises from the average play of no-trigger-regret algorithms, where trigger deviations are a particular subset of linear transformations of the sequence-form strategy polytope \(\) of each player. Since this paper focuses on learning dynamics that guarantee sublinear regret with respect to _any_ linear transformation of \(\), it follows immediately that the dynamics presented in this paper recover EFCE as a special case.

The concept of linear-swap-regret minimization has been considered before in the context of _Bayesian_ games. Mansour et al. (2022) study a setting where a no-regret _learner_ competes in a two-player Bayesian game with a rational utility _maximizer_, that is a strictly more powerful opponent than a learner. Under this setting, it can be shown that in every round the optimizer is guaranteed to obtain at least the Bayesian Stackelberg value of the game. Then they proceed to prove that minimizing _linear-swap regret_ is necessary if we want to cap the optimizer's performance at the Stackelberg value, while minimizing polytope-swap regret (a generalization of swap regret for Bayesian games, and strictly stronger than linear-swap) is sufficient to cap the optimizer's performance. Hence, these results highlight the importance of developing learning algorithms under stronger notions of _rationality_, as is our aim in this paper. Furthermore, these results provide evidence that constructing a no-linear-swap regret learner, as is our goal here, can present benefits when compared to other less rational learners. In a concurrent paper, Fujii (2023) defines the notion of _untruthful swap regret_ for Bayesian games and proves that, for Bayesian games, it is equivalent to the linear-swap regret which is of interest in our paper.

Bayesian games can be considered as a special case of extensive-form games, where a chance node initially selects one of the possible types \(\) for each player. Thus, our algorithm minimizing linear-swap regret in extensive-form games also minimizes linear-swap regret in Bayesian games. However, we remark that our regret bound depends polynomially on the number of player types \(||\) as they are part of the game tree representation, while Fujii (2023) has devised an algorithm for Bayesian games, whose regret only depends on \(||\).

Finally, we also mention that nonlinear deviations have been explored in extensive-form games, though we are not aware of notable large sets for which polynomial-time no-regret dynamics can be devised. Specifically, we point to the work by Morrill et al. (2021), which defines the notion of "behavioral deviations". These deviations are nonlinear with respect to the sequence-form representation of strategies in extensive-form games. The authors categorize several known or novel types of restricted behavioral deviations into a Deviation Landscape that highlights the relations between them. Even though both the linear deviations, we consider in this paper, and the behavioral deviations seem to constitute rich measures of rationality, none of them contains the other and thus, linear deviations do not fit into the Deviation Landscape of Morrill et al. (2021) (see also Remark E.2).

## 2 Preliminaries

We recall the standard model of extensive-form games, as well as the framework of learning in games.

### Extensive-Form Games

While normal-form games (NFGs) correspond to nonsequential interactions, such as Rock-Paper-Scissors, where players simultaneously pick one action and then receive a payoff based on what others picked, extensive-form games (EFGs) model games that are played on a game tree. They capture both sequential and simultaneous moves, as well as private information and are therefore a very general and expressive model of games, capturing chess, go, poker, sequential auctions, and many other settings as well. We now recall basic properties and notation for EFGs.

Game treeIn an \(n\)-player extensive-form game, each node in the game tree is associated with exactly one player from the set \(\{1,,n\}\{c\}\), where the special player \(c\)--called the _chance_ player--is used to model random stochastic outcomes, such as rolling a die or drawing cards from a deck. Edges leaving from a node represent actions that a player can take at that node. To model private information, the game tree is supplemented with an information partition, defined as a partition of nodes into sets called information sets. Each node belongs to exactly one information set, and each information set is a nonempty set of tree nodes for the same Player \(i\). An information set for Player \(i\) denotes a collection of nodes that Player \(i\) cannot distinguish among, given what she has observed so far. (We remark that all nodes in a same information set must have the same set of available actions, or the player would distinguish the nodes). The set of all information sets of Player \(i\) is denoted \(_{i}\). In this paper, we will only consider _perfect-recall_ games, that is, games in which the information sets are arranged in accordance with the fact that no player forgets what the player knew earlier,.

Sequence-form strategiesSince nodes belonging to the same information set for a player are indistinguishable to that player, the player must play the same strategy at each of the nodes. Hence, a strategy for a player is exactly a mapping from an _information set_ to a distribution over actions. In other words, it is the information sets, and not the game tree nodes, that capture the decision points of the player. We can then represent a strategy for a generic player \(i\) as a vector indexed by each valid information set-action pair \((j,a)\). Any such valid pair is called a _sequence_ of the player; the set of all sequences is denoted as \(_{i}:=\{(j,a):j_{i},a_{j}\}\{\}\), where the special element \(\) is called _empty sequence_. Given an information set \(j_{i}\), we denote by \(p_{j}\) the parent sequence of \(j\), defined as the last pair \((j,a)_{i}\) encountered on the path from the root to any node \(v j\); if no such pair exists we let \(p_{j}=\). Finally, we denote by \(_{}\) the children of sequence \(_{i}\), defined as the information sets \(j_{i}\) for which \(p_{j}=\). Sequences \(\) for which \(_{}\) is an empty set are called _terminal_; the set of all terminal sequences is denoted \(_{i}^{}\).

**Example 2.1**.: _Consider the tree-form decision process faced by Player 1 in the small game of Figure 1 (Left). The decision process has four decision nodes \(_{1}=\{,,,\}\) and nine sequences including the empty sequence \(\). For decision node \(\), the parent sequence is \(p_{}=\); for \(\) and \(\) it is \(p_{}=\); for \(\) it is the empty sequence \(p_{}=\)._

A _reduced-normal-form plan_ for Player \(i\) represents a deterministic strategy for the player as a vector \(\{0,1\}^{_{i}}\) where the entry corresponding to the generic sequence \([ja]\) is equal to \(1\) if the player plays action \(a\) at (the nodes of) information set \(j_{i}\). Information sets that cannot be reached based on the strategy do not have any action select. A crucial property of the reduced-normal-form plan representation of deterministic strategies is the fact that the utility of any player is a multilinear function in the profile of reduced-normal-form plans played by the players. The set of all reduced

Figure 1: (Left) Tree-form decision process considered in the example. Black round nodes belong to Player 1; white round nodes to Player 2. Square white nodes are terminal nodes in the game tree, payoffs are omitted. Gray bags denote information sets. (Right) The constraints that define the sequence-form polytope \(_{1}\) for Player 1 (besides nonnegativity).

normal-form plans of Player \(i\) is denoted with the symbol \(_{i}\). Typically, the cardinality of \(_{i}\) is exponential in the size of the game tree.

The convex hull of the set of reduced-normal-form plans of Player \(i\) is called the _sequence-form polytope_ of the player, and denoted with the symbol \(_{i}(_{i})\). It represents the set of all randomized strategies in the game. An important result by Romanovskii (1962); Koller et al. (1996); von Stengel (1996) shows that \(_{i}\) can be captured by polynomially many constraints in the size of the game tree, as we recall next.

**Definition 2.2**.: _The polytope of sequence-form strategies of Player \(i\) is equal to the convex polytope_

\[_{i}_{ 0}^{ }:(1)&[]=1\\ (2)&_{a_{j}}[ja]=[p_{j}]& \,j}.\]

As an example, the constraints that define the sequence-form polytope for Player 1 in the game of Figure 1 (Left) are shown in Figure 1 (Right). The polytope of sequence-form strategies possesses a strong combinatorial structure that enables speeding up several common optimization procedures and will be crucial in developing efficient algorithms to converge to equilibrium.

### Hindsight Rationality and Learning in Games

Games are one of many situations in which a decision-maker has to act in an online manner. For these situations, the most widely used protocol is that of Online Learning (e.g., see Orabona (2022)). Specifically, each learner has a set of actions or behavior they can employ \(^{d}\) (in extensive-form games, this would typically be the set of reduced-normal-form strategies). At each timestep \(t\) the learner first selects, possibly at random, an element \(\), and then receives a loss (opposite of utility) function \(^{(t)}:\). Since as we observed the above utilities in EFGs are linear in each player's reduced-normal-form plans, for the rest of the paper we focus on the case in which the loss function \(^{(t)}\) is linear, that is, of the form \(^{(t)}:^{(t)},^{(t)}\).

A widely adopted objective for the learner is that of ensuring vanishing average _regret_ with high probability. Regret is defined as the difference between the loss the learner cumulated through their choice of behavior, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. In particular, let \(\) be a desired set of strategy transformations \(:\) that the learner might want to learn not to regret. Then, the learner's \(\)-regret is defined as the quantity

\[^{(T)}_{}_{t=1}^{T} ^{(t)},^{(t)}-^{(t)},(^{(t)})\]

A _no-\(\)-regret algorithm_ (also known as a \(\)-regret minimizer) is one that, at all times \(T\), guarantees with high probability that \(^{(T)}=o(T)\) no matter what is the sequence of losses revealed by the environment. The size of the set \(\) of strategy transformations defines a natural measure of rationality (sometimes called _hindsight rationality_) for players, and several choices have been discussed in the literature. Clearly, as \(\) gets larger, the learner becomes more rational. On the flip side, guaranteeing sublinear regret with respect to all transformations in the chosen set \(\) might be intractable in general. On one end of the spectrum, perhaps the smallest meaningful choice of \(\) is the set of all _constant_ transformations \(^{}=\{_{}}:}\}_{}}\). In this case, \(^{}\)-regret is also called _external regret_ and has been extensively studied in the field of online convex optimization. On the other end of the spectrum, _swap regret_ corresponds to the setting in which \(\) is the set of _all_ transformations \(\). Intermediate, and of central importance in this paper, is the notion of _linear-swap regret_, which corresponds to the case in which

\[\{: ^{d d},\, \}\]

is the set of all linear transformations from \(\) to itself.1

An important observation is that when all considered deviation functions in \(\) are linear, an algorithm guaranteeing sublinear \(\)-regret for the set \(\) can be constructed immediately from a deterministic no-\(\)-regret algorithm for \(^{}=()\) by sampling \(\) from any \(^{}^{}\) so as to guarantee that \([]=^{}\). Since this is exactly the setting we study in this paper, this folklore observation (see also Farina et al. (2022)) enables us to focus on the following problem: does a deterministic no-\(\)-regret algorithm for the set of sequence-form strategies \(=_{i}\) of any player in an extensive-form game, with guaranteed sublinear \(\)-regret in the worst case, exist? In this paper we answer the question for the positive.

From regret to equilibriumThe setting of Learning in Games refers to the situation in which all players employ a learning algorithm, receiving as loss the negative of the gradient of their own utility evaluated in the strategies output by all the other players. A fascinating aspect of no-\(\)-regret learning dynamics is that if each player of a game employs a no-\(\)-regret algorithm, then the empirical frequency of play converges almost surely to the set of \(\)-equilibria, which are notions of correlated equilibria, in which the rationality of players is bounded by the size of the set \(\). Formally, for a set \(\) of strategy deviations, a \(\)-equilibrium is defined as follows.

**Definition 2.3**.: _For a \(n\)-player extensive-form game \(G\) and a set \(_{i}\) of deviations for each player, \(a\)\(\{_{i}\}\)-equilibrium is a joint distribution \((_{1}_{n})\) such that for each player \(i\), and every deviation \(_{i}\) it holds that_

\[_{}[u_{i}()]_{}[u_{i} ((_{i}),_{-i})]\]

_That is, no player \(i\) has an incentive to unilaterally deviate from the recommended joint strategy \(\) using any transformation \(_{i}\)._

This general framework captures several important notions of equilibrium across a variety of game theoretic models. For example, in both NFGs and EFGs, no-external regret dynamics converge to the set of Coarse Correlated Equilibria. In NFGs, no-swap regret dynamics converge to the set of Correlated Equilibria (Blum and Mansour, 2007). In EFGs, Farina et al. (2022) recently proved that a specific subset \(\) of linear transformations called _trigger deviations_ lead to the set of EFCE.

Reducing \(\)-regret to external regretAn elegant construction by Gordon et al. (2008) enables constructing no-\(\)-regret algorithms for a generic set \(\) starting from a no-external-regret algorithm for \(\). We briefly recall the result.

**Theorem 2.4** (Gordon et al. (2008)).: _Let \(\) be an external regret minimizer having the set of transformations \(\) as its action space, and achieving sublinear external regret \(^{(T)}\). Additionally, assume that for all \(\) there exists a fixed point \(()=\). Then, \(a\)\(\)-regret minimizer \(_{}\) can be constructed as follows:_

* _To output a strategy_ \(^{(t)}\) _at iteration_ \(t\) _of_ \(_{}\)_, obtain an output_ \(^{(t)}\) _of the external regret minimizer_ \(\)_, and return one of its fixed points_ \(^{(t)}=^{(t)}(^{(t)})\)_._
* _For every linear loss function_ \(^{(t)}\) _received by_ \(_{}\)_, construct the linear function_ \(L^{(t)}:^{(t)}((^{(t)}))\) _and pass it as loss to_ \(\)_._

_Let \(^{(T)}\) be the \(\)-regret of \(_{}\). Under the previous construction, it holds that_

\[^{(T)}=^{(T)}\,T=1,2,\]

_Thus, if \(\) is an external regret minimizer then \(_{}\) is a \(\)-regret minimizer._

## 3 A No-Linear-Swap Regret Algorithm with Polynomial-Time Iterations

In this section, we describe our no-linear-swap-regret algorithm for the set of sequence-form strategies \(\) of a generic player in any perfect-recall imperfect-information extensive-form game. The algorithm follows the general template for constructing \(\)-regret minimizers given by Gordon et al. (2008) and recalled in Theorem 2.4. For this we need two components:

1. an efficient external regret minimizer for the set \(_{}\) of all matrices inducing linear transformations from \(\) to \(\),
2. an efficiently computable fixed point oracle for matrices \(_{}\), returning \(=\).

The existence of a fixed point, required in ii), is easy to establish by Brouwer's fixed point theorem, since the polytope of sequence-form strategies is compact and convex, and the continuous function \(\) maps \(\) to itself by definition. Furthermore, as it will become apparent later in the section, all elements \(_{}\) have entries in \(^{}\). Hence, requirement ii) can be satisfied directly by solving the linear feasibility program \(\{:=,\}\) using any of the known polynomial-time algorithms for linear programming. Establishing requirement i) is where the heart of the matter is, and it is the focus of much of the paper. Here, we give intuition for the main insights that contribute to the algorithm. All proofs are deferred to the appendix.

### The Structure of Linear Transformations of Sequence-Form Strategy Polytopes

The crucial step in our construction is to establish a series of results shedding light on the fundamental geometry of the set \(_{}\) of _all_ linear transformations from a sequence-form polytope \(\) to itself. In fact, our results extend beyond functions from \(\) to \(\) to more general functions from \(\) to a generic compact polytope \(\{^{d}:=, \}\) for arbitrary \(\) and \(\). We establish the following characterization theorem, which shows that when the functions are expressed in matrix form, the set \(_{}\) can be captured by a polynomial number of constraints. The proof is deferred to Appendix B.

**Theorem 3.1**.: _Let \(\) be a sequence-form strategy space and let \(\) be any bounded polytope of the form \(\{^{d}:=, \}[0,]^{d}\), where \(^{k d}\). Then, for any linear function \(f:\), there exists a matrix \(\) in the polytope_

\[_{}\{(3 )&_{(ja)}=_{j}&\,ja^{}\\ (4)&_{()}=&\,^{ }\\ (5)&_{j^{}_{}}_{j^{}}=\\ (6)&_{j^{}_{ja}}_{j^{}}=_{j}& \,ja^{}\\ (7)&_{()}[0,]^{d}&\,\\ (8)&_{j}^{k}&\,j\}\]

_such that \(f()=\) for all \(\). Conversely, any \(_{}\) defines a linear function \(\) from \(\) to \(\), that is, such that \(\) for all \(\)._

The proof operates by induction in several steps. At its core, it exploits the combinatorial structure of sequence-form strategy polytopes, which can be decomposed into sub-problems using a series of Cartesian products and convex hulls. A high-level description of the induction is as follows:

* The _Base Case_ corresponds to the case of being at a leaf decision point. In this case, the set of deviations corresponds to all linear transformations from a probability \(n\)-simplex into a given polytope \(\). This set is equivalent to all \(d n\) matrices whose columns are points in \(\), which can easily be verified formally. This corresponds to constraint (3) in the characterization of \(_{}\).
* For the _Inductive Step_, we are at an intermediate decision point \(j\), that is, one for which at least one action leads to further decision points.
* In Lemma B.8 we show that any terminal action \(a\) at \(j\) leads to a column in the transformation matrix that is necessarily a valid point in the polytope \(\). This is similar to the base case, and again leads to constraint (3) in the characterization.
* In Lemma B.7, we look at the other case of a non-terminal action \(a\) at \(j\). We prove that there always exists an equivalent transformation matrix whose column corresponding to sequence \(ja\) is identically \(0\) (constraint (4)). This allows for the "crux" of the transformation to happen in the subtrees below \(ja\) or equivalently, the subtrees rooted at the children decision points \(_{ja}\) of \(ja\).
* A key difficulty to conclude the proof is in using the assumption of the inductive step to characterize all such valid transformations. The set of strategies in the subtrees rooted at the children decision points \(_{ja}\) is in general the Cartesian product of the strategies in these subtrees. This explains the need for the fairly technical Proposition B.4, whose goal is to precisely characterize valid transformations of Cartesian products. This leads to constraints (5) and (6) in our final characterization.
We also remark that while the theorem calls for the polytope \(\) to be in the form \(=\{^{d}:=,\}\), with little work the result can also be extended to handle other representations such as \(\{^{d}:\}\). We opted for the form specified in the theorem since it most directly leads to the proof, and since the constraints that define the sequence-form strategy polytope (Definition 2.2) are already in the form of the statement.

In particular, by setting \(=\) in Theorem 3.1 (in this case, the dimensions of \(\) will be \(k=||+1\), and \(d=||\)), we conclude that the set of linear functions from \(\) to itself is a compact and convex polytope \(_{}^{}\), defined by \(O(||^{2})\) linear constraints. As discussed, this polynomial characterization of \(_{}\) is the fundamental insight that enables polynomial-time minimization of linear-swap regret in general extensive-form games.

### Our No-Linear-Swap Regret Algorithm

From here, constructing a no-external-regret algorithm for \(_{}\) is relatively straightforward, using standard tools from the rich literature of online learning. For example, in Algorithm 1, we propose a solution employing online projected gradient descent (Gordon, 1999; Zinkevich, 2003).

``` Data:\(^{(1)}_{}\) and fixed point \(^{(1)}\) of \(^{(1)}\), learning rates \(^{(t)}>0\)
1for\(t=1,2,\)do
2 Output \(^{(t)}\)
3 Receive \(^{(t)}\) and pay \(^{(t)},^{(t)}\)
4 Set \(^{(t)}=^{(t)}(^{(t)})^{}\)
5\(^{(t+1)}=_{_{}}(^{ (t)}-^{(t)}^{(t)})=*{arg\,min}_{ _{}}\|^{(t)}-^{(t)}^{(t)}-\|_{F}^{2}\)
6 Compute a fixed point \(^{(t+1)}=^{(t+1)}^{(t+1)}\) of matrix \(^{(t+1)}\) ```

**Algorithm 1**\(\)-Regret minimizer for the set \(=_{}\)

Combining that no-external-regret algorithm for \(\) with the construction by Gordon et al. (2008), we can then establish the following linear-swap regret and iteration complexity bounds for Algorithm 1.

**Theorem 3.2** (Informal).: _Let \(\) denote the set of sequences of the learning player in the extensive-form game, and let \(^{(t)}=1/\) for all \(t\). Then, for any sequence of loss vectors \(^{(t)}^{}\), Algorithm 1 guarantees linear-swap regret \(O(||^{2})\) after any number \(T\) of iterations, and runs in \(O((||)^{2}t)\) time for each iteration \(t\)._

The formal version of the theorem is given in Theorem D.1. It is worth noting that the polynomial-sized description of \(_{}\) is crucial in establishing the polynomial running time of the algorithm, both in the projection step (5) and in the fixed point computation step (6). We also remark that the choice of online projected gradient descent combined with the ellipsoid method for projections were arbitrary and the useful properties of \(_{}\) are retained when using it with any efficient regret minimizer.

## 4 Linear-Deviation Correlated Equilibrium

As we discussed in the preliminaries, when all players in a game employ no-\(\)-regret learning algorithms, then the empirical frequency of play converges to the set of \(\)-equilibria almost surely. Similarly, when \(=_{}\) the players act based on "no-linear-swap regret" dynamics and converge to a notion of \(\)-equilibrium we call _linear-deviation correlated equilibrium_ (LCE). In this section we present some notable properties of the LCE. In particular, we discuss its relation to other already established equilibria, as well as the computational tractability of optimal equilibrium selection.

### Relation to CE and EFCE

The \(\)-regret minimization framework, offers a natural way to build a hierarchy of the corresponding \(\)-equilibria based on the relationship of the \(\) sets of deviations. In particular, if for the sets \(_{1},_{2}\) it holds that \(_{1}_{2}\), then the set of \(_{2}\)-equilibria is a subset of the set of \(_{1}\)-equilibria. Sincethe Correlated Equilibrium is defined using the set of all swap deviations, we conclude that any \(\)-equilibrium, including the LCE, is a superset of CE. What is the relationship then of LCE with the extensive-form correlated equilibrium (EFCE)? Farina et al. (2022) showed that the set \(^{}\) inducing EFCE is the set of all "trigger deviations", which can be expressed as linear transformations of extensive-form strategies. Consequently, the set \(^{}\) is a subset of all linear transformations and thus, it holds that \(\). In examples E.1 and E.3 of the appendix we show that there exist specific games in which either \(\), or \(\). Hence, we conclude that the previous inclusions are strict and it holds \(\).

For Example E.1 we use a signaling game from von Stengel and Forges (2008) with a known EFCE and we identify a linear transformation that is not captured by the trigger deviations of EFCE. Specifically, it is possible to perform linear transformations on sequences of a subtree based on the strategies on other subtrees of the TFSDP. For Example E.3 we have found a specific game through computational search that has a LCE which is not a normal-form correlated equilibrium. To do that we identify a particular normal-form swap that is non-linear.

Empirical evaluationTo further illustrate the separation between no-linear-swap-regret dynamics and no-trigger-regret dynamics, used for EFCE, we provide experimental evidence that minimizing linear-swap-regret also minimizes trigger-regret (Figure 2, left), while minimizing trigger-regret does _not_ minimize linear-swap regret. Specifically, in Figure 2 we compare our no-linear-swap-regret learning dynamics (given in Algorithm 1) to the no-trigger-regret algorithm introduced by Farina et al. (2022). More details about the implementation of the algorithms is available in Appendix F. In the left plot, we measure on the y-axis the average trigger regret incurred when all players use one or the other dynamics. Since trigger deviations are special cases of linear deviations, as expected, we observe that both dynamics are able to minimize trigger regret. Conversely, in the right plot of Figure 2, the y-axis measures linear-swap-regret. We observe that while our dynamics validate the sublinear regret performance proven in Theorem 3.2, the no-trigger-regret dynamics of Farina et al. (2022) exhibit an erratic behavior that is hardly compatible with a vanishing average regret. This suggests that no-linear-swap-regret is indeed a strictly stronger notion of hindsight rationality.

### Hardness of Maximizing Social Welfare

In many cases we are interested in knowing whether it is possible to select an Equilibrium with maximum Social Welfare. Let MAXPAY-LCE be the problem of finding an LCE in EFGs that maximizes the sum (or any linear combination) of all player's utilities. Below, we prove that we cannot efficiently solve MAXPAY-LCE, unless P=NP, even for 2 players if chance moves are allowed, and even for 3 players otherwise. We follow the structure of the same hardness proof for the problem MAXPAY-CE of finding an optimal CE in EFGs. Specifically, von Stengel and Forges (2008) use a reduction from SAT to prove that deciding whether MAXPAY-CE can attain the maximum value is NP-hard even for 2 players. To do that, they devise a way to map any SAT instance into a polynomially large game tree in which the root is the chance player, the second level corresponds to one player, and

Figure 2: (Left) Average trigger regret per iteration for both a linear-swap-regret minimizer and a trigger-regret minimizer. (Right) Average linear-swap regret per iteration for the same two minimizers.

the third level corresponds to the other player. The utilities for both players are exactly the same, thus the players will have to coordinate to maximize their payoff irrespective of the linear combination of utilities we aim to maximize.

**Theorem 4.1**.: _For two-player, perfect-recall extensive-form games with chance moves, the problem MAXPAY-LCE is not solvable in polynomial time, unless P=NP._

**Remark 4.2**.: _The problem retains its hardness if we remove the chance node and add a third player instead. As showed in von Stengel and Forges (2008), in that case we can always build a polynomially-sized game tree that forces the third player to act as a chance node._

## 5 Conclusions and Future Work

In this paper we have shown the existence of uncoupled no-linear-swap regret dynamics with polynomial-time iteration complexity in the game tree size in any extensive-form game. This significantly extends prior results related to extensive-form correlated equilibria, and begets learning agents that learn not to regret a significantly larger set of strategy transformations than what was known to be possible before. A crucial technical contribution we made to establish our result, and which might be of independent interest, is providing a polynomial characterization of the set of all linear transformations from a sequence-form strategy polytope to itself. Specifically, we showed that such a set of transformations can be expressed as a convex polytope with a polynomial number of linear constraints, by leveraging the rich combinatorial structure of the sequence-form strategies. Moreover, these no-linear-swap regret dynamics converge to linear-deviation correlated equilibria in extensive-form games, which are a novel type of equilibria that lies strictly between normal-form and extensive-form correlated equilibria.

These new results leave open a few interesting future research directions. Even though we know that there exist polynomial-time uncoupled dynamics converging to linear-deviation correlated equilibrium, we conjecture that it is also possible to obtain an efficient centralized algorithm similar to the Ellipsoid Against Hope for computing EFCE in extensive-form games by Huang and von Stengel (2008). Additionally, it is an intriguing question to understand whether a no-linear-swap regret algorithm exists that achieves \(O( T)\) regret per-player, as is the case for no-trigger regret (Anagnostides et al., 2023). Furthermore, it would be interesting to further explore problems of equilibrium selection related to LCE, possibly by devising suitable Fixed-Parameter Algorithms in the spirit of Zhang et al. (2022). Finally, the problem of understanding what is the most hindsight rational type of deviations based on which we can construct _efficient_ regret minimizers in extensive-form games remains a major open question.