# Robust and Faster Zeroth-Order Minimax Optimization: Complexity and Applications

Weixin An\({}^{1}\), Yuanyuan Liu\({}^{1}\)1, Fanhua Shang\({}^{2}\)2, Hongying Liu\({}^{3,4}\)3

\({}^{1}\)Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education,

School of Artificial Intelligence, Xidian University, China

\({}^{2}\)College of Intelligence and Computing, Tianjin University, China

\({}^{3}\)Medical School, Tianjin University, China

\({}^{4}\)Peng Cheng Lab, Shenzhen, China

weixinanut@163.com, yyliu@xidian.edu.cn, fhshang@tju.edu.cn, hyliu2009@tju.edu.cn

###### Abstract

Many zeroth-order (ZO) optimization algorithms have been developed to solve nonconvex minimax problems in machine learning and computer vision areas. However, existing ZO minimax algorithms have high complexity and rely on some strict restrictive conditions for ZO estimations. To address these issues, we design a new unified ZO gradient descent extragradient ascent (ZO-GDEGA) algorithm, which reduces the overall complexity to \((d^{-6})\) to find an \(\)-stationary point of the function \(\) for nonconvex-concave (NC-C) problems, where \(d\) is the variable dimension. To the best of our knowledge, ZO-GDEGA is the first ZO algorithm with complexity guarantees to solve stochastic NC-C problems. Moreover, ZO-GDEGA requires weaker conditions on the ZO estimations and achieves more robust theoretical results. As a by-product, ZO-GDEGA has advantages on the condition number for the NC-strongly concave case. Experimentally, ZO-GDEGA can generate more effective poisoning attack data with an average accuracy reduction of 5%. The improved AUC performance also verifies the robustness of gradient estimations.

## 1 Introduction

We mainly consider a general regularized minimax problem:

\[_{x^{d_{x}}}_{y^{d_{y}}}\{(x,y)=g(x)+f( x,y)-h(y)\},\] (1)

where \(f:^{d_{x} d_{y}}\) is nonconvex in \(x\), concave in \(y\) and \(\)-smooth, \(g:^{d_{x}}\) and \(h:^{d_{y}}\) are convex but maybe nonsmooth functions. The problem (1) appears in many scenarios of machine learning, such as adversarial attack to regularized deep neural networks, regularized fair learning, and robust training . In this paper, we focus on the black-box setting of Problem (1), where the gradients are estimated by only functional values.

In the black-box setting, some evolutionary algorithms such as  have achieved good performance. However, they usually lack complexity analysis and may never converge to a solution due to pathological behavior . Thus, ZO algorithms with convergence guarantees came into being. For example, ZO algorithms provide an alternative to higher-order optimization methods for solving robust network training with gradient or curvature regularization . Besides, ZO algorithms have also made considerable progress in escaping from saddle points .

As for the black-box problem (1), ZO algorithms provide an access for solving it where the gradients are computationally infeasible or expensive, such as the Area Under Curve (AUC) maximization  with the ReLU activation and generating poisoning data to evaluate the black-box models [22; 33; 61]. In these scenarios, ZO algorithms become one of the best choices. For example,  proposed ZO-Min-Max for solving NC-strongly concave (NC-SC) data poisoning attack problems.

More practically, it is desired to solve Problem (1) under weaker conditions such as general concavity and more tolerant smoothing parameters of ZO estimators. But existing ZO algorithms such as  have high overall complexity for NC-C problems, and require picky smoothing parameters to ensure convergence, which weakens the robustness. On the other hand, the stochastic loss is actually more common . But there is no ZO algorithm to solve the stochastic NC-C problem (1), which motivates us to fill this gap. Thus, **we summarize our motivations as follows:**

\(\) Can we design a ZO algorithm to achieve a lower overall complexity with more weaker requirements on ZO estimators for NC-C problems?

\(\) Due to the lack of research on stochastic ZO algorithms in the NC-C setting, can we develop a new stochastic ZO algorithm with theoretical guarantees?

**Our contributions.** In this paper, we propose a unified algorithm to answer the above two questions. Our contributions can be summerized as follows.

\(\) We design a unified single-loop ZO-GDEGA algorithm to solve the NC-C minimax problem (1) faster and more robustly. Specifically, we introduce the idea of continuous-time dynamics to assist in designing the update rules of dual variable \(y\), which plays a key role in complexity analysis. Moreover, we analyze ZO-GDEGA by developing a concise theoretical framework, which reduces the overall complexity of finding a generalized \(\)-stationary point of the function \(\) to \(((d_{x}+d_{y})^{-6})\), even without the Lipschitz continuity assumption, as shown in Table 1.

\(\) To the best of our knowledge, the stochastic ZO-GDEGA is the first ZO stochastic algorithm with theoretical guarantees for solving NC-C problems, which for the first time finds a generalized \(\)-stationary point of \(\) with an overall complexity of \((d_{x}^{-6}+d_{y}^{-8})\), as shown in Table 1.

  Set. & Algs. & \(f/\)  Reg.1 [FOOTNOTE:1]
Table 1: Comparison of the overall ZO oracle complexity of single-loop algorithms to find an \(\)-stationary point of \(f\) (Definition 3) or \(\) (Definition 2). \(=/\) denotes the condition number, \(d=d_{x}+d_{y}\) and \(}()\) hides logarithmic terms. Abbreviation: Settings (Set.), Algorithms (Algs.), Regular (Reg.), Theorem (The.).

\(\) Our ZO-GDEGA algorithm is more tolerant to the ZO estimations. Specifically, ZO-GDEGA allows a larger tolerance \(()\) for smoothness parameters \(_{1}\) and \(_{2}\) compared with existing methods, as shown in Table 1, which enhances the robustness of ZO algorithms.

\(\) As a by-product, ZO-GDEGA applied to NC-SC problems can obtain competitive complexity results \((^{2}(d_{x}+d_{y})^{-2})\) and \((^{2}(d_{x}+ d_{y})^{-4})\) for deterministic and stochastic settings, respectively.

\(\) Finally, the poisoning attack experiment shows that the poisoned data generated by ZO-GDEGA can reduce the accuracy by 2.1%-7.9% compared to baselines. The AUC maximization task shows that our algorithms can improve AUC performance by 0.4-5.4 units under rough ZO estimation.

## 2 Preliminaries and Related Work

This section provides several notations and definitions, and discusses some related works.

### Notations

\(\|\|\) and \(\|\|_{1}\) denote the \(_{2}\)-norm and \(_{1}\)-norm of a vector, respectively. \(\|\|_{}\) denotes the \(_{}\)-norm of a vector, i.e., \(\|x\|_{}=_{1 i d_{x}}|x_{i}|\). We denote \(a=(b)\) if \(a Cb\) for some constant \(C>0\), any subgradient of \(g()\) at \(x\) by \( g(x)\), a ZO estimator of \(f()\) at \(x\) by \(f(x)\) for the gradient \( f(x)\), the regularized coupling function by \((x,y):=f(x,y)-h(y)\), the max function by \((x):=_{y}\{f(x,y)-h(y)\}\) and the regularized max function by \((x):=g(x)+(x)\).

**Definition 1**.: _The function \(f(,)\) is \(\)-smooth, i.e., \(\| f(x,y)- f(x^{},y^{})\|\|(x,y)-(x^{}, y^{})\|\)._

### Related Work

**ZO algorithms for nonconvex minimax optimization.** For solving NC-SC minimax problems, some single-loop ZO algorithms have been presented. For example,  proposed the ZO-Min-Max by integrating alternating stochastic Gradient Descent Ascent (GDA) method and ZO estimators, which finds an \(\)-stationary point with the overall complexity \((^{6}d^{-6})\), when \(g(x)=_{}(x)\) and \(h(y)=_{}(y)\).  proposed the ZO-GDA for solving the deterministic problem (1) with \(g(x)\!\!0\) and \(h(y)\!=\!_{}(y)\), and the ZO overall oracle complexity is bounded by \((^{5}d^{-2})\). Its stochastic variant, ZO-SGDA, can achieve the overall ZO oracle complexity \((^{5}d^{-4})\). Recently,  proposed an Acc-ZOMDA algorithm based on momentum acceleration techniques and further reduced the overall ZO oracle complexity. However, these algorithms have relatively strict restrictions on ZO estimators, as shown in Table 1, which affects their performance, as shown in Table 2.

As for NC-C problems, to the best of our knowledge, the only work proposed in  designs a ZO-AGP algorithm with the ZO oracle complexity \((d_{x}^{-8}+d_{y}^{-10})\), but it only considers the deterministic setting when \(g(x)\!=\!_{}(x)\), \(h(y)\!=\!_{}(y)\), and \(\) and \(\) are two convex compact sets, which limits its application to some scenarios such as regularized robust neural network training . More recently, a new version of ZO-AGP  has been presented during the preparation of our work, which achieves lower complexity under the same constraints, but its theoretical results are still in terms of the coupling function \(f\) instead of the stronger definition in terms of \(\), as shown in Table 1, while our analysis is in terms of \(\) and achieves lower complexity.

**Extragradient methods.** The extragradient (EG) method was first proposed in  for solving convex-concave saddle point problems and it adopts the gradient at the current point \(x_{t}\) to find an intermediate point \(x_{t+1/2}\) and then uses the gradient at \(x_{t+1/2}\) to determine the next iteration point \(x_{t+1}\). Specifically, at iteration \(t\),

\[x_{t+1/2}=^{g}_{_{x}}(x_{t}-_{x}_{x}f(x_{t})), x _{t+1}=^{g}_{_{x}}(x_{t}-_{x}_{x}f(x_{t+1/2})),\] (2)

where \(^{g}_{}(x)_{z}\{g(z)+\|z- x\|^{2}\}\). On the one hand, the extra proximal gradient step guides the optimization process, which allows to escape cycling trajectories of the simultaneous gradient flow  and consider curvature information . Compared with various GDA methods such as , many works have shown the advantages of the first-order (FO) EG structure, such as handling noisy gradients in the convex-concave  and NC-SC(C)  settings, but there is no insight into the ZO setting. On the other hand, in univariate convex optimization, the EG method has made considerable progress inspired by continuous time dynamic theory, and it can be explained as an approximation of the more robust backward-Euler discretization , whereas gradient descent is a variant of the classical forward-Euler discretization . Based on the two facts above, we aim to explore the performance of the EG structure in ZO minimax optimization to improve both theoretical and practical performance for solving the black-box problem (1).

**Lower bounds for minimax optimization.** For minimization problems, the lower bounds on ZO methods justify that the dependence on the dimension is inevitable without additional assumptions . For minimax problems, will there be a similar conclusion? So far, researchers focus on the lower bounds of FO methods. For example, the lower bounds are \((^{-1})\) and \(()\) for convex-concave (C-C) and SC-SC settings, respectively. For NC-SC problems, there is also a lower bound for the FO setting [59; 30]. But to the best of our knowledge, there is no work proving a lower bound for ZO algorithms solving NC minimax problems. This paper focuses on another aspect and provides the upper bound for ZO algorithms solving NC minimax problems.

**Upper bounds for minimax optimization.** For the NC-C setting, GDA , Alternating GDA (AGDA)  and GDmax [25; 25] are analyzed to guarantee convergence. So far, the best complexity bound for FO deterministic NC-C problems is \((^{-6})\). The works [60; 20] have further extended to the stochastic setting. For example, SAPD+  achieved the complexity bound of \((^{-6})\), which matches that of the deterministic case. For the NC-SC setting, the works [34; 49] proved the advanced bound of \((^{3}^{-3})\). SAPD+ achieved another advanced complexity bound of \((^{-4})\).

In minimizing optimization, the complexity of ZO methods is usually \(d\) times that of corresponding FO methods . In this sense, our results match the upper bound in the deterministic NC-C setting. Unfortunately, there is no similar conclusion in the minimax optimization yet. On the other hand, existing ZO methods such as [49; 50] focus on modifying existing FO algorithms, while this paper focuses on designing ZO minimax algorithms to directly improve performance.

## 3 ZO-GDEGA for NC-C Problems

In this section, we design a unified single-loop ZO gradient descent extragradient ascent (ZO-GDEGA) algorithm for solving the NC-C problem (1). The following assumptions are made for our analysis throughout this section.

**Assumption 1**.: _We assume \(f(x,)\) is concave for a given \(x\)._

**Assumption 2**.: _[_4_]_ _The regularizers \(g\) and \(h\) are proper, convex and lower semicontinuous._

_1. Additionally, \(g\) is either \(L_{g}\)-Lipschitz continuous on its domain, which is assumed to be open, or the indicator of a nonempty, convex and closed set. Either of those assumptions guarantees the bound_

\[\|_{}^{g}(x)-x\| L_{g}\] (3)

_holds for any \(>0\) and all \(x dom\ g\) (in the case of the indicator the statement is trivially true)._

_2. Furthermore, \(h\) has a bounded domain \(dom\ h\) such that the diameter of \(dom\ h\) is bounded by \(D_{h}\)._

Similar to , we also use the extension of the classical Danskin's theorem based on Assumptions 1 and 2 to guarantee that the solution set \(Y^{*}(x):=\{y^{*}|y^{*}_{y}\{f(x,y)-h(y)\}\}\) is non-empty for \( x^{d_{x}}\) and \((x)\) is \(\)-weakly convex. Based on these facts, we propose and analyze our ZO-GDEGA algorithm for solving the NC-C problem (1) in deterministic and stochastic settings.

### ZO-GDEGA in the Deterministic Setting

We first approximate the FO gradient by ZO randomized gradient estimators. Then, we propose the ZO-GDEGA algorithm to solve the black-box NC-C problem (1). Our algorithms are single-loop and more easily scalable compared to nested-loop algorithms such as .

_ZO randomized gradient estimators._ We introduce the ZO randomized gradient estimators as follows: \(_{x}f(x,y)=}_{i=1}^{q_{1}}(f(x+_{ i}y,y)-f(x,y))}{_{i}}u_{i}_{y}f(x,y)=}_{i=1}^{q_{2}} (f(x,y+_{2}v_{i})-f(x,y))}{_{2}}v_{i},\) where \(\{u_{i}\}_{i=1}^{q_{1}}^{d_{x}}\) and \(\{v_{i}\}_{i=1}^{q_{2}}^{d_{y}}\) are i.i.d. random direction vectors drawn uniformly from the unit Euclidean spheres, respectively. \(_{1}\) and \(_{2}\) are smoothing parameters, the conditions on which are strict in existing algorithms such as , while our algorithms allow them to be \(()\), as shown in Table 1. The randomness caused by vectors \(u_{i}\) and \(v_{i}\) is coupled in the alternating updates of \(x\) and \(y\) and undoubtedly increases the difficulty of complexity analysis. We introduce two smooth functions \(f_{_{1}}(x,y)=_{u}[f(x+_{1}u,y)]\) and \(f_{_{2}}(x,y)=_{v}[f(x,y+_{2}v)]\) to bridge the ZO estimators and the FO gradient, thereby assisting in complexity analysis as shown in Fig. 1.

```
0:\(x_{0}\), \(z_{0}=y_{0}\), step sizes \(_{x}\) and \(_{y}\).
1:for\(t=0,1,,T-1\)do
2:\(_{x}F(x_{t})=\{_{x}f(x_{t},z_{t}) \\ _{x}f(x_{t},y_{t}).\); \(x_{t+1}=_{_{x}}^{g}(x_{t}-_{x}_{x}F(x_{t}))\);
3:\(z_{t+1}=_{_{y}}^{h}(y_{t}+_{y}_{y}f(x_{t},y_{t}))\); \(y_{t+1}=_{_{y}}^{h}(y_{t}+_{y}_{y}f(x_{t+1},z_{ t+1}))\);
4:endfor
5:Randomly draw \(\) from \(x_{1},,x_{T}\) at uniform. ```

**Output:**\(\). ```

**Algorithm 1** Deterministic Zeroth-Order Gradient Descent Extragradient Ascent Algorithm

To improve performance, we integrate the EG structure and the ZO randomized gradient estimators, and propose the ZO-GDEGA algorithm. Specifically, it contains a proximal gradient descent step and two proximal gradient ascent steps, as shown in Algorithm 1.

**3.1.1 Update rule of \(x\).**

Algorithm 1 updates \(x\) by minimizing the following linearized approximation,

\[x_{t+1}=_{x}\{g(x)+ x-x_{t},_{x}f(x_{t},z_{t}) +}\|x-x_{t}\|^{2}\},\] (4)

where \(z_{t}\) is an auxiliary variable and its update rule is given later. Our design for updating \(x_{t+1}\) relies only on the previous point \(x_{t}\), and does not require \(x_{t+1/2}\) as in the standard EG method (2).

**3.1.2 EG update rule of \(y\).**

The quality of the solution set \(Y^{*}(x)\) plays a key role on complexity. To take full advantage of the EG structure based on the concavity w.r.t. \(y\), we integrate it into the update of \(y\), which guides a new high-level idea from the continuous-time dynamic perspective.

_A high-level idea._ Here we show an intuition for the advantages of our algorithm. If \(h\) is "simple", in the sense that the proximal operator has a closed-form solution . Without loss of generality, we consider the case of \(h(y) 0\). In this case, the proximal operator becomes the identity transformation. In fact, one-step ZO estimation w.r.t. \(y\) can be viewed as the discretization of the stochastic continuous-time dynamic \((t)\!=\!_{y}f(X,Y(t))\) with the limit \(_{y} 0\). To get more robust and accurate solutions, we use the backward-Euler instead of forward-Euler discretization to solve this dynamic. An exact backward-Euler step implies the discretization \(y_{t+1}\!=\!y_{t}+_{y}_{y}f(x_{t+1},y_{t+1})\), but obtaining \(y_{t+1}\) in such a manner could be computationally prohibitive. Instead, we opt for an extra gradient ascent step \(z_{t+1}=y_{t}+_{y}_{y}f(x_{t},y_{t})\) to first approximate \(y_{t+1}\) once as shown in the \(3^{rd}\) line of Algorithm 1, which can be viewed as a "trial" step. Then, we use the ZO oracle at point \((x_{t+1},z_{t+1})\) to approximate one-step backward-Euler discretization as shown in the end of the \(3^{rd}\) line, which is one of the reasons why our ZO-GDEGA algorithm is more robust than .

**3.1.3 Advantages over Existing Methods.**

Compared with existing works, our ZO-GDEGA algorithm has the following three main advantages:

\(\)_Stronger robustness._ About ZO estimators, ZO-GDEGA can tolerate smoothness parameters \(_{1}\) and \(_{2}\) of size \(()\), whereas existing ZO algorithms have stricter restrictions on smoothness parameters such as \((^{4})\) as shown in Table 1, which is the fundamental reason why ZO-GDEGA improves the robustness. Specific proofs and experimental verifications are in subsequent sections.

\(\)_Lower per-iteration complexity._ Compared with the standard EG method , Algorithm 1 only uses the EG structure for the update of \(y\) instead of both \(x\) and \(y\), which reduces per-iteration complexity and maintains the same iteration complexity as the standard EG method. In fact, our ZO-GDEGA algorithm is easily extended to an FO method, which still reduces the computation cost and maintains the corresponding theoretical result. Since this paper mainly focuses on the ZO case, the FO variant of Algorithm 1 is shown in the Appendix.

\(\)_More extensive applications._ The proximal operators generalize the projection operators in existing works such as . Compared with , our ZO-GDEGA does not require additional compactness of the domains, which significantly extend the applicability.

### ZO-GDEGA in the Stochastic Setting

We consider that \(f\) is a stochastic function on a distribution \(\), i.e., \(f(x,y):=_{}f(x,y;)\), and we can only access the stochastic function values \(f(x,y;)\). In this case, we introduce the stochastic version of ZO estimators: \(_{x}f(x,y;_{1})\!=\!}_{j=1}^{b_{1}} _{x}f(x,y;_{j})=}_{j=1}^{b_{1}} _{j},y;_{j})-f(x,y;_{j})}{_{1}/d_{x}}u_{j}\) as well as \(_{y}f(x,y;_{2})\), where \(_{1}\!=\!\{_{j}\}_{j=1}^{b_{1}}\), \(_{2}\!=\!\{_{j}\}_{j=1}^{b_{2}}\) denote mini-batch sets of \(b_{1}\) and \(b_{2}\) i.i.d. samples. According to , \(_{U,_{1}}[_{x}f(x,y;_{1})]\!=\! _{x}f_{_{1}}(x,y)\) and \(_{V,_{2}}[_{y}f(x,y;_{2})]= _{y}f_{_{2}}(x,y)\) with \(U\!=\!\{u_{i}\}_{i=1}^{b_{1}}\) and \(V\!=\!\{v_{i}\}_{i=1}^{b_{2}}\), and we need the common Assumptions 3, 6 and 7 in ZO stochastic optimization. Due to the page limit, we place Assumptions 6 and 7 in the Appendix.

**Assumption 3**.: _The variance of the ZO stochastic estimators are bounded, i.e., \(_{u,}\|_{x}f(x,y;)-_{x}f_{_{1}}(x,y) \|^{2}_{1}^{2}\), \(_{v,}\|_{y}f(x,y;)-_{y}f_{_{2}}(x,y)\|^ {2}_{1}^{2}\)._

Based on the above analysis and Algorithm 1, we design a stochastic variant of ZO-GDEGA, as shown in Algorithm 2 in the Appendix. The main difference between Algorithms 2 and 1 is that the ZO estimators are replaced by their stochastic versions. In fact, for NC-C minimax problems, stochastic ZO-GDEGA is the first stochastic ZO algorithm and we prove its complexity in Section 5.

## 4 ZO-GDEGA for NC-SC Problems

In this section, we provide two by-products of our ZO-GDEGA algorithm for the NC-SC setting, as also shown in Algorithms 1 and 2. We design and analyze them based on Assumption 4.

**Assumption 4**.: _We assume \(f(x,)\) is \(\)-strongly concave in \(y\) for a given \(x\). Moreover, the regularizers \(g\) and \(h\) are proper, convex and lower semicontinuous._

Based on Assumption 4, we know that the max function \((x)\) is \((+1)\)-smooth, where \(=/ 1\) is the condition number, which plays a key role in our analysis. Note that there are two main differences between our ZO-GDEGA for solving NC-SC and NC-C problems as follows:

\(\) Due to the strong concavity, there is no need to use the intermediate point \(z_{t}\) to update \(x_{t+1}\), but directly use the ZO estimators at the point \((x_{t},y_{t})\) to accelerate updating, which contributes to achieve competitive complexity under weaker restrictions on \(_{1}\) and \(_{2}\), thereby enhancing robustness.

\(\) The solution set \(Y^{*}(x)\) is a singleton, which consists of a single element \(y^{*}(x)\), thus we can use the quantity \(_{t}:=\|y_{t}-y^{*}(x_{t})\|^{2}\) to measure the quality of proximal ZO extragradient ascent steps.

In summary, we propose a unified ZO algorithm for solving black-box NC minimax problems, which outperforms existing ZO algorithms. Next, we rigorously analyze its complexity.

## 5 Theoretical Analysis

In this section, we first define two types of _generalized \(\)-stationary points_ as termination criteria. Then, we provide the complexity results of our ZO-GDEGA to find an \(\)-stationary point.

### Termination Criteria

We generalize two classical definitions of \(\)_-stationary point_ in  and  as follows.

**Definition 2**.: _In the NC-C setting, a point \(x\) is an \(\)-stationary point of an \(\)-weakly convex function \(\) if its Moreau envelope2\(_{1/2}(x)\) satisfies \(\|_{1/2}(x)\|\); In the NC-SC setting, a point \(x\) is an \(\)-stationary point of a function \(\) if \(dist(0,(x))\)._

**Definition 3**.: _A pair of points \((x,y)\) is an \(\)-stationary point of the coupling function \(f\) if we have_

\[\|(x,y)\|(x,y)=[x- _{1/}^{3}(x-(1/)_{x}f(x,y))]\\ [y-_{1/}^{h}(y+(1/)_{y}f(x,y))].\]Based on [31, Proposition 4.11 and 4.12], the following two propositions clarify the relationship between the generalized Definitions 2 and 3.

**Proposition 1**.: _Under Assumptions 1 and 2, if a point \((,)\) is an \(^{2}/( D_{h})\)-stationary point in terms of Definition 3, a point \(\) is an \(()\)-stationary point in terms of Definition 2._

**Proposition 2**.: _Under Assumption 4, if a point \((,)\) is an \(/\)-stationary point in terms of Definition 3, a point \(\) is an \(()\)-stationary point in terms of Definition 2._

Thus, we can draw a similar conclusion as in : The \(\)-stationary point definition of \(\) is stronger than that of \(f\). To obtain tighter complexity bounds, we analyze ZO algorithms for the first time based on \(\)-stationary point definition of \(\) (i.e., Definition 2).

### Complexity Analysis for NC-SC Problems

We first analyze the complexity of ZO-GDEGA for the NC-SC setting in deterministic and stochastic cases. Our analysis is a non-trivial extension of the proofs in  due to the following challenges.

_Key technical challenges._ In this case, bounding \(_{t=0}^{T}_{t}\) is a key step. But the proximal operators and the ZO EG structure make it much more difficult to bound this term than existing works. To address this challenge, we propose to establish the upper bound on \(_{t=0}^{T}_{t}\) in terms of \(_{i=0}^{T}\|x_{i}-x_{i+1}\|^{2}\) (see the Appendix for details), _which also reduces the overall complexity's dependence on \(\), and results in the coefficients of \(_{1}^{2}\) and \(_{2}^{2}\) being only \((1)\) and \(()\), thereby weakening their dependence on \(\) and enhancing robustness_. Here we directly give the complexity results.

**Theorem 1**.: _We choose stepsizes \(_{x}}\), \(_{y}=\), \(_{1}=(d_{x}^{-1})\) and \(_{2}=(^{-1/2}d_{y}^{-1})\). Under Assumption 4, our ZO-GDEGA can find an \(\)-stationary point in terms of Definition 2, i.e., \(_{1 t T}dist(- g(x_{t}),(x_{t}))\!\!\), with the overall ZO oracle complexity \((^{2}(d_{x}+d_{y})^{-2})\) for deterministic and \((^{2}(d_{x}+ d_{y})^{-4})\) for stochastic settings._

Theorem 1 shows that ZO-GDEGA allows for larger \(_{1}\) and \(_{2}\) than compared algorithms (see Table 1), which means that our algorithms can tolerate rougher ZO estimations. Besides, our complexity bounds have the weaker \(\) dependence than them. Thus, even our by-products have some advantages. Moreover, this analysis also builds a bridge for analyzing the NC-C setting.

### Complexity Analysis for NC-C Problems

In this part, we analyze ZO-GDEGA in the NC-C setting. We provides two perspectives: continuity-agnostic (more relaxed condition) and continuity-dependent (better bound) analysis.

#### 5.3.1 Continuity-Agnostic Complexity Analysis.

We first analyze the complexity for the NC-C setting based on Theorem 1 and smooth technology in . By adding a smoothing term, we approximate the NC-C problem (1) with the following NC-SC model: \(_{x}_{y}\{(x,y)=g(x)+(x,y)-h(y)\}\), where \((x,y)=f(x,y)-}{2}\|y-\|^{2}\) and given arbitrary \( dom\ h\). Based on Theorem 1 and careful selection of \(\), Theorem 2 ensures that ZO-GDEGA can find an \(\)-stationary point for the NC-C problem (1).

**Theorem 2**.: _Under Assumptions 1 and 2, our ZO-GDEGA applied to the approximate NC-SC model with \(=(^{2}/( D_{h}^{2}))\), can guarantee to generate an \(\)-stationary point \(x_{}\) for the NC-C problem (1), i.e., \(\|_{1/2}(x_{})\|\), with overall complexity \(((d_{x}+d_{y})^{-6})\) and \((d_{x}^{-8}+d_{y}^{-10})\) for the deterministic and stochastic settings, respectively._

Based on stronger Definition 2, Theorem 2 ensures that our ZO-GDEGA can achieve lower complexity for NC-C problems without relying on any extra assumptions, such as continuity and decreasing sequence assumptions, whereas  depend on at least one of them. Besides, Theorem 2 inherits the advantages of our analysis for Theorem 1, i.e., enhancing robustness and reducing complexity.

#### 5.3.2 Better Bounds with Continuity-dependent.

We also analyze our ZO-GDEGA algorithm for solving the NC-C problem (1) under the Lipschitz continuity assumption, which is common in NC-C optimization such as .

**Assumption 5** (Continuity).: \(f(x,y)\) _is \(G\)-Lipschitz continuous in \(x\), i.e., for \( y dom\)\(h\), \( x,x^{}^{d_{x}}\) satisfies that \(\|f(x,y)-f(x^{},y)\| G\|x-x^{}\|\)._

In the case of (ii) in Table 1, Assumption 5 follows immediately by the smoothness and choosing \(G=(D_{}+D_{})+\|_{x}f(x_{},y_{ })\|\), where \(_{x,x^{}}\|x-x^{}\| D_{}\) and \(_{y,y^{}}\|y-y^{}\| D_{}\). Thus, ZO-GDEGA still work without Assumption 5 in this case. For the general problem (1), we use \(G\) for generalization and give a proof sketch. Detailed proofs are provided in the Appendix.

Proof sketch.: We first analyze the recursive relationship between the Moreau envelopes \(_{1/2}(x_{t})\) and \(_{1/2}(x_{t+1})\) based on the ZO gradient descent step. Then, we estimate the tricky error term \(_{t}=(x_{t})-(x_{t},z_{t})\) to measure the upper bound of the ZO EG ascent steps. Based on these results, we obtain the overall complexity for solving the NC-C problem (1). In deterministic and stochastic settings, we provide tighter results than Theorem 2, respectively.

**Deterministic Setting.** Lemma B.13 provides the recursive relationship between \(_{1/2}(x_{t+1})\) and \(_{1/2}(x_{t})\). Our algorithms use more concise ZO estimation gradient descent step instead of EG structure to update \(x\). Thus, the tricky term \(\|_{x}f(x_{t},y_{t})-_{x}f(x_{t-1},y_{t-1})\|^{2}\) can be removed in our analysis compared with the standard EG method in  as shown in Fig. 1. Lemma B.15 proves that \(_{t=0}^{T}[_{t}]\) can be well controlled by carefully choosing \(_{y}\). **More importantly, our analysis makes the coefficients of \(_{1}^{2}\) and \(_{2}^{2}\) independent of \(\), which cases their dependence on \(\) compared with , thereby enhancing the robustness.** Now, we begin to provide tighter results.

**Theorem 3**.: _Under Assumptions 1, 2 and 5, if \(q_{1}=d_{x}\), \(q_{2}=d_{y}\), \(_{y}=1/()\), \(_{x}=(^{4})\), \(_{1}=(}})\) and \(_{2}=(}})\), our deterministic ZO-GDEGA can find an \(\)-stationary point of \(\), i.e., \(_{t=0}^{T}\|_{1/2}(x_{t})\|\), for the NC-C setting with the overall complexity \(((d_{x}+d_{y})^{-6})\)._

Theorem 3 shows that our ZO-GDEGA achieves the same overall complexity as Theorem 2 and further weakens the constraints on smoothing parameters under the continuity assumption. Note that ZO-GDEGA is the first ZO algorithm with convergence guarantees to find an \(\)-stationary point of \(\). Meanwhile, our analysis can be easily extended to the stochastic setting.

**Stochastic Setting.** Due to the page limit, we directly provide results for stochastic ZO-GDEGA.

**Theorem 4**.: _Under assumptions 1, 2, 3 and 5, if we choose the suitable parameters such as \(_{1}=()\) and \(_{2}=()\), our stochastic ZO-GDEGA algorithm can find an \(\)-stationary point in terms of Definition 2 with lower overall complexity \((d_{x}^{-6}+d_{y}^{-8})\) than Theorem 2._

Theorem 4 shows that the overall complexity can be further reduced, and we can still choose \(_{1}\) and \(_{2}\) to be \(()\) to enhance the robustness. Note that we provide theoretical guarantees for the stochastic black-box NC-C setting for the first time. In summary, our ZO-GDEGA shows excellent theoretical properties for solving NC-C problems and competitiveness for solving NC-SC problems.

## 6 Experiments

In this section, we conduct black-box data poisoning attack and AUC maximization experiments. Due to the page limit, more experimental details and results are provided in the Appendix. Our codes are available: https://github.com/Weixin-An/ZO-GDEGA.

### Data Poisoning Attack

Data poisoning attack is one of the most common black-box attack methods [29; 42]. The purpose of the attacker is: when the model parameter \(w\) is well-trained, adding a perturbation \(\) on the training data that makes the loss functions as large as possible, and then such poisoned training data

Figure 1: Comparison of two proof sketches.

\((s_{i}+,t_{i})\) will reduce the model accuracy. Therefore, it is usually required for attackers to optimize the following objective function,

\[_{\|\|_{} r_{x}}_{w}f_{1}(,w):=F_{tr}(,w; _{tr}),\] (5)

where \(_{tr}_{tr,p}_{tr,c}\) denotes the training dataset including \(n\) i.i.d samples, \(_{tr,p}\) and \(_{tr,c}\) denote the poisoned and clean subsets of \(_{tr}\), respectively. We choose the cross-entropy loss to optimize \(\) and \(w\). Note that Problem (5) can be rewritten as the NC-C (1) by setting \(g()=_{\|\|_{} r_{x}}()\), \(h() 0\), and \(f=-f_{1}\). Therefore, it can be solved by our ZO-GDEGA algorithm.

**Datasets.** We validate ZO-GDEGA on a synthetic dataset and the epsilon_test dataset.

\(\) Synthetic datasets: We generate a dataset \(\!=\!\{s_{i},t_{i}\}_{i=1}^{2,000}\) with feature vector \(s_{i}^{100}\) from Gaussian distribution \((0,I)\) and split it into 70% training and 30% test samples. The label \(t_{i}=1\) if \(1/(1+e^{-(s_{i}^{}w^{}+n_{i})})>0.5\), otherwise \(t_{i}=0\), where \(n_{i}(0,10^{-3})\) is random noise, and we set \(w^{}=\) as the ground truth.

\(\) The epsilon_test dataset3: It contains 100,000 samples of 2,000 dimensions, and we also split it into 70% training samples and 30% testing samples.

**Experimental Settings & Baselines.** We set \(r_{x}=2\), \(_{1}=_{2}=2 10^{-5}\) and poisoning ratio \(|_{tr,p}|/|_{tr}|=0.1\), and choose mini-batch size \(b_{1}=b_{2}=100\) and \(b_{1}=b_{2}=10\) for the synthetic and epsilon_test datasets, respectively. The baseline for solving Problem (5) is ZO-AGP. Note that here we replace the ZO estimators in deterministic ZO-AGP with corresponding stochastic versions and choose the same hyperparameter settings for a fair comparison.

**Results.** We use the poisoned data generated by the baselines and our ZO-GDEGA to attack the training procedure of the logistic regression model, and the experimental results on the synthetic

Figure 3: Comparison of the attack results for data poisoning attack on the large-scale epsilon_test dataset.

Figure 2: Comparison of the results for the logistic regression model attacked by poisoned data generated by ZO-AGP and ZO-GDEGA on the synthetic dataset.

and epsilon_test datasets are shown in Figs. 2 and 3, respectively. The experiment in each case was carried out in 10 independent trials with random initialization and the shaded area around the line indicates the standard deviation. From Fig. 2, we find that our ZO-GDEGA can converge faster than baselines in terms of stationary gap and objective value \(-f_{1}\). From the training accuracy subfigure, both our ZO-GDEGA and the baselines (black-box attack) converge, and achieve near-optimal solution compared to the FO-AGP algorithm (white-box attack) . As for testing accuracy, it can be seen that compared with ZO-AGP, the poisoned data generated by our ZO-GDEGA are always more effective in reducing the accuracy of the logistic regression model in terms of both iterations and CPU time, which also confirms that our ZO-GDEGA achieves lower complexity than ZO-AGP. Besides, under different poisoning ratios, ZO-GDEGA can reduce the model accuracy by 2.1%-7.6% than ZO-AGP in terms of attack performance. From Fig. 3, we observe that ZO-GDEGA algorithm also performs about 8.0% lower than ZO-AGP on the epsilon_test dataset, which further verifies the superiority of our algorithm in the large-scale real-world datasets.

### AUC Maximization

AUC is defined as the area under the ROC curve, which is a performance indicator to measure the pros and cons of a machine learning model. However, it is generally difficult to optimize the AUC value directly on the training task, but instead optimize the following minimax problem:

\[_{\|\|,\|a\|,\|b\| r_{x},\;v r_{y}}_{ }[f(,a,b,v;)],\] (6)

where \(r_{x}\) and \(r_{y}\) are the radius of the projection balls, \(=(s,t)\) is drawn independently from the distribution \(\), and \(f(,a,b,v;)\) is defined as in . When we choose the multilayer perception (MLP) model, Problem (6) becomes a NC-SC problem. The non-differentiable point of the nonlinear function such as ReLU causes some non-differentiable modules of the classifier, and the ZO algorithms are one of the techniques to solve this issue . Note that our ZO-GDEGA is the first ZO algorithm to attempt to solve the AUC maximization problem.

Experimental Settings & Baselines.We conduct experiments on the MNIST, Fashion-MNIST and ijcnn1 datasets. Following , we use their training and testing sets but convert the classes of the data into two classes by randomly splitting them into two groups. We implement ZO-SGDA , ZO-Min-Max , Acc-ZOMDA  and our ZO-GDEGA to solve Problem (6) with Leaky ReLU. We choose a two-layer MLP as the classification model, and set mini-batch \(b_{1}=b_{2}=256\), \(q_{1}=q_{2}=10\), \(r_{x}=r_{y}=2\) and step sizes \(_{x}=_{y}=0.1\) to train all the methods for 200 epochs.

**Results.** We show the average AUC performance from 10 independent trials with random initialization in Table 2. It shows that at small \(_{1}\) and \(_{2}\), our ZO-GDEGA algorithms perform competitively compared with the baselines. At larger \(_{1}\) and \(_{2}\), ZO-GDEGA can improve AUC performance by 0.4-5.4 units than baselines, which indicates that our algorithms are more robust to smoothing parameters and robustness may be more important than overall complexity in some cases when using ZO oracle to approximate gradients.

## 7 Conclusions and Future Work

In this paper, we proposed a unified ZO-GDEGA algorithm for solving black-box nonconvex minimax problems, which reduces the overall complexity and improves robustness. The experimental results match our theoretical analysis, which is reflected by the fact that ZO-GDEGA can obtain more effective attacks and improve AUC performance robustly. In summary, ZO-GDEGA achieves better performance than related algorithms and promotes the development of ZO optimization theory. In the future, we will extend our algorithm to non-smooth and federated learning settings as in [45; 44].

  Datasets &  &  \\ \(_{1}(_{2})\) & 0.001 & 0.01 & 0.05 & 0.001 & 0.01 & 0.05 \\  ZO-SGDA & 91.67 & 91.81 & 88.12 & 91.62 & 90.19 & 87.27 \\ ZO-Min-Max & 92.25 & 92.01 & 88.56 & 90.80 & 91.58 & 83.23 \\ Acc-ZOMDA & 92.45 & 92.58 & 89.35 & 92.97 & 91.65 & 87.75 \\ ZO-GDEGA & 91.60 & 92.60 & 89.70 & 91.97 & 92.23 & 88.66 \\  

Table 2: The average AUC performance with different \(_{1}\) and \(_{2}\) on the MNIST and Fashion-MNIST datasets, where \(_{1}=_{2}\). Results on dataset ijcnn1 are provided in the Appendix.

Acknowledgments

We want to thank the anonymous reviewers for their valuable suggestions and comments. This work was supported by the National Key Research and Development Program of China (No. 2023YFF0906204), National Natural Science Foundation of China (No. 62276182), and Peng Cheng Lab Program (No. PCL2023A08).