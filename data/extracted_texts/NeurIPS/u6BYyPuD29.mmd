# MADG: Margin-based Adversarial Learning for Domain Generalization

Aveen Dayal

Indian Institute of Technology Hyderabad

ai21resch11003@iith.ac.in

&Vimal K B

Indian Institute of Technology Hyderabad

vimalkb96@gmail.com

&Linga Reddy Cenkeramaddi

University of Agder

linga.cekeramaddi@uia.no

&C Krishna Mohan

Indian Institute of Technology Hyderabad

ckm@cse.iith.ac.in

&Abhinav Kumar

Indian Institute of Technology Hyderabad

abhinavkumar@ee.iith.ac.in

&Vineeth N Balasubramanian

Indian Institute of Technology Hyderabad

vineethnb@cse.iith.ac.in

###### Abstract

Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based \(\) divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, **MADG**, motivated by a margin loss-based discrepancy metric. The proposed **MADG** model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed **MADG** model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the **MADG** model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.

## 1 Introduction

Over the past decade, Deep Neural Networks (DNNs) have demonstrated exceptional performance across various fields, including robotics , medical imaging , agriculture , and more. However, the effectiveness of DNNs in supervised learning environments relies heavily on the independently and identically distributed (\(i.i.d.\)) assumption of the training and test (target) data. Unfortunately, in reality, this assumption can be compromised due to domain shifts in target data . For example, a model trained on different domains of image data may perform poorly when presented with an image of a known label featuring an unseen background or viewpoint, as explained in . To address this issue, researchers have developed techniques under the framework of domain adaptation (DA) . The key idea behind DA is to adapt a model trained on a source dataset to minimize the generalizationerror on the target dataset . However, the major limitation of DA is that the target data, whether labeled or unlabeled, must be available during training. In contrast, the Domain Generalization (DG) setting aims to leverage knowledge from similar domains to classify previously unseen domains .

Recent years have seen the development of various algorithms addressing the DG setting, among which one popular approach is the adversarial learning-based methodology [9; 10; 11; 12; 13; 14]. Other approaches can be broadly classified into meta-learning techniques [15; 16; 17], data augmentation methods [18; 19; 20], self-supervised learning methods [21; 22; 23], regularization-based methods [24; 25; 26; 27; 28; 29], and so on. Additionally, there have been concerted efforts towards the development of benchmark datasets [30; 31; 32; 33; 34; 35] for the DG setting to study methods. The assumption of distribution shift, i.e., smooth variation between the conditional distribution \((Y|X)\) and the marginal distribution \((X)\) is also common in DG literature [36; 37].

The main objective of DG methods is to model the functional relationship between the input space \(\) and the label space \(\) for all domains. To achieve this, adversarial DG methods learn a domain-invariant representation by minimizing a discrepancy metric among the source domains. However, despite the myriad efforts, most adversarial learning methods use the 0-1 loss based \(\) divergence metric  for domain alignment. In contrast, divergence metrics based on margin loss are more informative , practical, and efficiently optimizable . This work addresses this gap by proposing a novel adversarial DG algorithm, **MADG**, motivated by a margin-based divergence metric. The proposed **MADG** leverages margin-based disparity discrepancy  to estimate source domain discrepancies in the DG setting and uses adversarial training to ensure that **MADG** generalizes well to unseen target domains. We also theoretically analyze the proposed **MADG** algorithm based on bounds for the unseen target error. The proposed generalization bound uses the Rademacher complexity framework , which provides data-dependent estimates of functional class complexities. The effectiveness of the proposed algorithm is demonstrated through extensive experiments on multiple benchmark DG datasets.

The key contributions of this work can be summarized as follows: (i) We introduce the use of margin loss and a corresponding scoring function to formulate the relationship between domains and develop upper bounds for the unseen target error (the first such margin-based effort in the DG setting, to the best of our knowledge); (ii) We subsequently analyze the generalization bound in terms of functional class complexity using the Rademacher complexity framework; (iii) We propose a novel margin-based adversarial DG training algorithm, **MADG**, motivated by our theoretical results; and (iv) We study the proposed method on five well-known benchmark datasets in the DomainBed setup, providing a higher average accuracy and consistency in model performance across these datasets.

## 2 Related Work

In this section, we discuss earlier literature proposed specifically for adversarial DG, as well as theoretical analysis for the DG problem in general. We discuss other DG literature across a broader set of categories in the Appendix. The main idea behind existing adversarial DG methods is to minimize the \(\) divergence by employing a minimax optimization between a domain discriminator and a classifier to learn domain-invariant features. One of the early works  proposed a method that iteratively divided samples into latent domains via clustering and trained the domain-invariant feature extractor via adversarial learning. Other efforts extended such an approach of adversarial learning with different divergence metrics and regularization techniques. Lin et al.  proposed a multi-dataset feature generalization network (MMFA-AAE) based on an adversarial auto-encoder to learn a generalized domain-invariant latent feature representation with the Maximum Mean Discrepancy (MMD) measure to align distributions across multiple domains. Deng et al.  examined adversarial censoring techniques to learn invariant representations from multiple domains. Zhao et al.  proposed an entropy regularization term along with adversarial loss to ensure conditional invariance of learned features. Akuzawa et al.  proposed the notion of accuracy-constrained domain invariance, and then developed an adversarial feature learning method with accuracy constraint (AFLAC), which explicitly provided invariance on adversarial training. Rahman et al.  proposed a correlation-aware adversarial DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning. All these prior works on adversarial DG methods use a 0-1 loss based \(\) discrepancy metric to align source domains. In this work, we instead leverage a margin-based disparity discrepancy and propose a new adversarial learning strategy founded on our theoretical analysis. The margin loss is advantageous compared to the 0-1 loss as it provides informative generalization bounds, tightness, classifier-aware alignment, and efficient optimization. We discuss each of these advantages in detail in Section 4.

Early efforts for theoretical analysis for the DG problem [43; 44] used kernel-based approaches to the problem setting with corresponding generalization error analysis, and showed empirical results using traditional machine learning methods. A few other efforts [45; 46; 47] also followed a similar path, and focused on kernel-based approaches in traditional methods. From a deep learning perspective, an early attempt at such a theoretically motivated method was proposed in  using a convex hull formulation and distribution matching to obtain generalization bounds, which also is an inspiration for parts of our work. In , the authors study theoretical bounds in the DG setting using three new concepts: feature invariance across domains, feature discriminability for the classification task, and a feature expansion function to generalize the first two concepts from the source to the target domain.  identified measures relating to the Fisher information, predictive entropy, and maximum mean discrepancy to be good predictors of out-of-distribution generalization of empirical risk minimization (ERM) models in the DG setting.  proposed generalization bounds in terms of the model's Rademacher complexity and suggested using regularized ERM models for the DG problem. Finally,  introduced an online game in which one model (player) reduces the error on test data provided by an adversarial player, but do not provide any empirical studies. In this work, we draw inspiration from  by considering a convex combination of source domains and leverage this to propose a new margin-based approach to the theoretical analysis of DG, which helps develop an informative generalization bound. Motivated by this analysis, we propose the **MADG** algorithm, a novel margin-based adversarial learning approach for DG, which shows consistent performance over other state-of-the-art methods across benchmarks.

## 3 Preliminaries

We consider a set of source domains \(_{_{i}}\) with \(i\{1,2,,N_{s}\}\) and a set of unseen domains \(_{U_{m}}\) with \(m\{1,2,,N_{u}\}\). We use \(\) to refer to any of these domains when the index is not relevant. We use the terms 'domain' and 'distribution' interchangeably in this work. Each such domain \(\), where \(\) is an input space and \(\) is an output space, which is \(\{0,1\}\) in binary classification and \(\{1,,k\}\) in multi-class classification. We use \(\) to denote a set of samples drawn independently from \(\), i.e. \(=\{(x_{i},y_{i})\}_{i=1}^{n}\) where \(x_{i}\) and \(y_{i}\),\( i\{1,2,,n\}\). We use \((x,y)\) to refer to a labeled sample \((x_{i},y_{i})\), when the index is not relevant.

As in , we consider the multi-class setting with a hypothesis space \(\) of scoring functions \(f:^{||}=^{k}\), where the outputs report the confidence of the prediction on each dimension. Similar to , we use \(f(x,y)\) to denote the component of \(f(x)\) that corresponds to the label \(y\). In order to obtain the final predicted label, we also consider a labeling function space \(\) containing \(h_{f}:\) such that \(h_{f}(x)=_{y}\)\(f(x,y)\), i.e. the predicted label assigned to data sample \(x\) is the one resulting in the largest confidence score.

The expected error rate of a classifier \(h\) with respect to a distribution \(\) is defined as \(err_{}(h)}{{=}}_{(x,y) }h(x) y\) where \(\) is the indicator function. We also define the margin \(_{f}()\) and the corresponding margin loss of hypothesis \(f\) for a labeled example \((x,y)\) as follows:

\[_{f}(x,y)}{{=}}f(x,y)- _{y^{} y}\;f(x,y^{}); err_{ }^{()}(f)}{{=}}_{x} _{}_{f}(x,y)\] (1)

where \(\) denotes the composition function and \(_{}\) is:

\[_{}(t)=0& t\\ 1-&0 t\\ 1&t 0\] (2)

The margin disparity and its empirical version are then defined as:

\[_{}^{()}f^{},f }{{=}}_{}_{} _{f^{}}(.,h_{f})\] (3) \[_{}^{()}f^{},f }{{=}}_{}_{} _{f^{}}(.,h_{f})=_{i=1}^{n}_{} _{f^{}}x_{i},h_{f}(x_{i})\] (4)

where \(f\) and \(f^{}\) are different scoring functions. Margin disparity discrepancy (MDD) is used to quantify the degree of discrepancy/disagreement between decision boundaries of classifiers (using their margins) trained on different domains in our context. Such a measure can be used to evaluate the generalization performance of a model across multiple domains. The MDD and its empirical version are defined in Eqn. (5) and Eqn. (6) below, respectively.

\[^{()}_{f,}_{S_{i}}, _{S_{k}} }{{=}}_{f^{}}^{()}_{ _{S_{k}}}f^{},f-^{()}_{_{S_{i}}}f^{},f\] (5) \[^{()}_{f,}_{S_{i}}, _{S_{k}} }{{=}}_{f^{}}^{( )}_{_{S_{k}}}f^{},f-^{()}_{ {D}_{S_{i}}}f^{},f\] (6)

We use the above terminologies from , which however focused on domain adaptation (with one source domain and one seen target domain). Handling multiple seen domains and an unseen target domain in the DG setting is non-trivial, which we focus on in this work.

## 4 Margin-based Approach to Domain Generalization: Theory

This section presents the theoretical motivation for our margin-based approach to the Domain Generalization (DG) problem. Our approach is based on considering the margin disparity discrepancy (MDD), defined above, between the source domains, thereby obtaining a generalization bound. We believe such a margin-based approach has a few advantages: (i) _Informative:_ Generalization bounds based on margin loss for classification provide more information (than 0-1 loss-based bounds) by establishing a dependency between any margin function satisfying the Lipschitz condition and the upper bounds, as demonstrated in well-known earlier work . (ii) _Tightness:_ In contrast to using a 0-1 loss to identify samples causing disagreement between classifiers, margin loss computes disagreement between scoring functions using a smooth function parameterized by the threshold (\(\)). Consequently, the number of samples causing agreement between classifiers is less, resulting in such an MDD-based bound being tighter, as shown in Fig. 1. (iii) _Classifier-aware Discrepancy:_ MDD considers the classifier function while measuring discrepancy; as shown in Eqns 5 and 6, the supremum is computed over \(f^{}\) while holding \(f\) constant (\(f\) learns the posterior distribution \((y|x)\) discriminatively for the classification task). This provides a classifier-aware approach to computing discrepancy. (iv) _Efficient Optimization and Practicality:_ The definition of MDD, as in Eqn. (5), only requires taking the supremum over one hypothesis. Therefore, compared to other divergence measures such as \(\), MDD can be optimized with ease and is practically useful, as also stated in .

In this section, we derive a generalization bound for an unseen domain based on the margin-based MDD loss in the DG setting. To this end, we first show an upper bound on the unlabeled source domain error given other labeled source domains (Lemma 1 and Theorem 1). We then leverage this to develop the upper bound for the error on an unseen domain that is not necessarily a source domain (Lemma 2, Theorem 2, and Corollary 1). We subsequently analyze the upper bound from Corollary 1 using the Rademacher complexity framework and develop our final generalization bound for the unseen target domain in the DG setting (Lemma 3 and Theorem 3) using our margin-based loss. In Sec 5, we show the formulation of the proposed adversarial learning algorithm, MADG, motivated by the generalization bound in Sec 4, that employs MDD to address the DG problem.

We begin by considering a setting where training data consists of \(N_{s}-1\) labeled source domains \(_{S_{i}},i=\{1,,N_{s}-1\}\) and a single unlabeled source domain \(_{_{T}}\), and show an upper bound on error in this setting. We later use this to show an upper bound for the DG setting. We first establish an upper bound on the MDD between a weighted sum of the labeled source domains and the unlabeled one in Lemma 1 below.

**Lemma 1**: _Consider a weighted sum of \(N_{s}-1\) labeled source distributions defined as \(_{}}:=_{i=1}^{N_{s}-1}_{i}_{S _{i}}\), where \(_{i}\)s are mixture co-efficients s.t. \(_{i=1}^{N_{s}-1}_{i}=1\), and \(f\) is a scoring function. Then_

\[^{()}_{f,}_{}}, _{_{T}}_{i=1}^{N_{s}-1}_{i}d^{() }_{f,}_{_{i}},_{_ {T}}\] (7)

Detailed proofs for all our theoretical results are provided in the Appendix. It follows from Lemma 1 that an effective way to minimize the discrepancy between the unlabeled source domain and the mixture of labeled source domains in the hypothesis space is by minimizing the convex sum of the pairwise MDD between each labeled and unlabeled source domain. Building upon this insight, we provide bounds on the unlabeled source error below in Theorem 1.

Figure 1: Space of intersection (agreement) in MDD (yellow) is reduced as compared to 0-1 loss (blue + yellow) between \(f\) and \(f^{}\) for labels \(\{0,1\}\).

**Theorem 1**: _Consider a scoring function \(f\), unlabeled source domain \(_{S_{T}}\) and a mixture of \(N_{s}-1\) source distributions denoted as \(_{}:=_{i=1}^{N_{s}-1}_{i}_{_{i}}\), where \(_{i}\)s is mixture co-efficients s.t. \(_{i=1}^{N_{s}-1}_{i}=1\). Then the error on the unlabeled source \(_{_{T}}\) is bounded as:_

\[err_{_{S_{T}}}(h_{f})_{i=1}^{N_{s}-1}_{i}err_ {_{_{i}}}^{()}(f)+d_{f,}^{()} _{_{i}},_{_{T}}+ \] (8)

**Remark 1:** From Theorem 1, we observe that the unlabeled source error is upper bounded by the labeled source errors, the pairwise discrepancy between each labeled and unlabeled source domain, and the ideal margin loss described below in Remark 2. We can also interpret Theorem 1 as an upper bound for the multi-source domain adaptation (DA) setting. While this is not our primary focus in this work, we report preliminary empirical results for a multi-source DA algorithm that reduces the first two terms in Theorem 1 in the Appendix.

**Remark 2:** The ideal loss \(\) is defined as \(=_{f^{*}}_{i=1}^{N_{s}-1}_{i }\ err_{_{_{i}}}^{()}(f^{*})+err_{_{ _{T}}}^{()}(f^{*})\) and is a constant that is independent of function \(f\).

Building on Theorem 1, we now develop bounds for the DG setting, where all source domains (\(N_{s}\)) are labeled, and the target domain is unseen during training. To this end, we consider the relationship between the unseen target and multiple source domains. In particular, we derive our error bound on the unseen target domain using the convex hull of the labeled source domains and MDD. We also provide a DG algorithm motivated by our theoretical analysis, which we describe later in Sec 5.

Before stating our main theorem, we present Lemma 2, which states that if the maximum MDD between any two sources is bounded above by \(\), then the MDD be between any two domains that belong to the convex hull of all the sources will also be bounded by \(\). Let \(_{s}\) be the convex hull of source domains defined as \(_{s}=:}=_{i=1}^{N_{s}}_{i} _{_{i}}\) where \(_{i=1}^{N_{s}}_{i}=1}\).

**Lemma 2**: _Let \(d_{f,}^{()}_{_{i}},_{ _{k}}\  i,k1,2,,N_{s}}\) and \(f\) be a scoring function. Then the following inequality holds for the MDD between any pair of domains \(^{},^{}_{s}\):_

\[d_{f,}^{()}^{},^{ }\] (9)

We observe from Lemma 2 that in the hypothesis space, the discrepancy among all the domains (seen or unseen) that belong to the convex hull \(_{s}\) can be reduced by minimizing the maximum MDD between two source domains. With the necessary tools at hand, we state the key theorem, i.e., the unseen target error bound for the DG problem below in Theorem 2.

**Theorem 2**: _Consider a mixture of \(N_{s}\) source distributions, scoring function f, unseen domain \(_{U_{m}}\), and \(=d_{f,}^{()}_{U_{m}},}_ {U}\) where \(}_{U}\) is the projection of \(_{U_{m}}\) onto the convex hull of the sources i.e. \(}_{U}=_{_{1},_{2},,_{N_{s}}}\ d_{f, }^{()}_{U_{m}},_{i=1}^{N_{s}}_{i} _{_{i}},_{i=1}^{N_{s}}_{i}=1\). Then, the unseen target error is bounded as follows:_

\[err_{_{U_{m}}}(h_{f})_{i=1}^{N_{s}}_{i}err_{ _{_{i}}}^{()}(f)+++\] (10)

**Remark 3:** As defined in Theorem 2, \(=_{f,}^{()}_{U_{m}},}_{U}\). Two scenarios therefore arise: \(=0\) when the unseen domain falls in the convex hull, i.e., \(_{U_{m}}=_{i=1}^{N_{s}}_{i}_{_{i}}\), or \(>0\) when the unseen domain cannot be represented by available domains alone. Thus this parameter can be interpreted as the need for diverse source domains. The more diverse the source domains are, the smaller the value of \(\).

**Remark 4:** As seen from Lemma 2, \(\) is defined as the upper bound for the MDD between any two domains that belong to the convex hull \(_{s}\) formed by the source domains. Thus, we can also interpret \(\) as the highest MDD value among the source domains as shown below in Eqn 11.

\[=_{f,}^{()}_{_{i^{ }}},_{_{i^{}}},\ \ _{f,}^{()}_{_{i^{ }}},_{_{k^{}}}_{f, }^{()}_{_{i}},_{_{k}}\  i,k,i^{},k^{}1,,N_{s}}\] (11)

Equipped with this definition for \(\), we can re-state Theorem 2 as Corollary 1 below.

**Corollary 1**: _Consider a mixture of \(N_{s}\) source distributions, scoring function f, unseen domain \(_{U_{m}}\), \(=d_{f,}^{()}_{U_{m}},}_ {U}\), and \(d_{f,}^{()}_{_{i^{}}},_{_{k^{}}}\) where \(d_{f,}^{()}_{_{i^{}}},_{_{k^{}}} d_{f,}^{()} _{_{i}},_{_{k}}\  i,k,i^{},k^{}1,2,,N_{s}}\). Then the unseen target error is bounded as follows:_\[err_{_{U_{m}}}(h_{f})_{i=1}^{N_{s}}_{i}err^{}_{ _{_{i}}}(f)+d^{()}_{f,} _{_{i^{}}},_{_{k^{}}} ++\] (12)

As seen from Corollary 1, the unseen target error is bounded above by the source errors, maximum MDD between two source domains, \(\), and the ideal loss given by \(=_{f^{*}}_{i=1}^{N_{s}}_{i}err^{ ()}_{_{_{i}}}(f^{*})+err^{()}_{_{U_{m }}}(f^{*})\).

Before we derive the generalization bound with the Rademacher complexity framework, we define

\[_{}=x fx,h(x)h ,f}\]  and Rademacher complexity \(_{n,}\) in Definition 1.

**Definition 1**: _Let \(\) be a class of functions such that \(f:[a,b]\) and \(=(x_{1},y_{1}),(x_{n},y_{n})}\) be a fixed sample of size \(n\) drawn from \(\) over \(\). Then the Rademacher complexity of \(\) is defined as:_

\[_{n,}()_{ ^{n}}_{}_{f} _{i=1}^{n}_{i}f(x_{i},y_{i})\] (13)

where \(_{i}\)s are independent Rademacher variables that assume values in \(\{-1,+1\}\). We now leverage Lemma 3.6 in  to derive our Lemma 3.

**Lemma 3**: _For any \(>0\), with probability \(1-2\), the following holds simultaneously for any scoring function \(f\):_

\[d^{()}_{f,}_{_{i}},_{ _{k}}-d^{()}_{f,}_{_{i}},_{_{k}} _{n_{i},_{_{i}}}_{} +_{n_{k},_{ _{k}}}_{}+}{2n_{i}}}+}{2n_{k}}}\] (14)

where \(n_{i}\) and \(n_{k}\) correspond to the sample size of \(_{_{i}}\) and \(_{_{k}}\), respectively. The difference between MDD and its empirical version is bounded by the Rademacher complexity as seen in Eqn 14. With these definitions, we derive our generalization bound based on Rademacher complexity and empirical MDD as below.

**Theorem 3**: _Given the same setting as Corollary 1 and Lemma 3, for any \(>0\), with probability \(1-3\), we obtain the following generalization bound for all \(f\):_

\[err_{_{U_{m}}}(f) _{i=1}^{N_{s}}_{i}err^{}_{_{_{i}}}(f)+d^{()}_{f,}_{_{i^{ }}},_{_{k^{}}}+++ {k}{}_{n_{i},_{_{i}}}_{ }\] (15)

where \(_{1}x f(x,y)y,f }\) as defined in  and \(1\) (in the subscript) represents constant functions mapping all points to the same class (see Appendix for more details). Theorem 3 thus establishes a relationship between the margin value \(\) and the generalization error. The first two terms on the right-hand side exhibit minimal variation with an increase in the margin \(\), especially when \(\) is small and the hypothesis space is rich, thus reducing the overall right-hand side. However, if \(\) exceeds a certain threshold (resulting in a weak classifier), then the first two terms significantly increase, resulting in increase of the overall bound. Choosing a better \(\) value thus allows one to obtain a desired error bound, making it more informative than a 0-1 loss-based bound. Inspired by this theoretical framework, we now propose our margin-based adversarial learning algorithm for DG in the next section.

## 5 MADG: Methodology

This section details the methodology of the proposed **MADG** algorithm, a novel adversarial DG algorithm motivated by the theory in Section 4. As seen in Theorem 3, the unseen target domain error is upper-bounded by the convex sum of the empirical source domain errors, \(\) term defined in Eqn. (16), \(\), \(\) and complexity terms. We hence aim to minimize the two important terms, viz., the convex sum of the empirical source domain errors and the \(\) term (given below).

\[=^{()}_{f,}_{_{i ^{}}},_{_{k^{}}},\ \ ^{()}_{f,}_{_{i^{ }}},_{_{k^{}}}^{()}_{f, }_{_{i}},_{_{k}} \  i,k,i^{},k^{}1,,N_{s}}\] (16)

**Efficient estimation of \(\).** As stated in Eqn. (16), \(\) is defined as the maximum pairwise empirical MDD among all source domains. Similar to Remark 4 and Lemma 2, we can also interpret \(\) as the upper bound of the empirical MDD between any two domains inside the convex hull formed by the source domains. If we minimize the sum of the empirical MDD between different pairs of the source domains, then the size of the convex hull reduces, which in turn minimizes \(\). Thus, one effective way to estimate \(\) is to consider the term below in Eqn. (17).

\[_{i=1}^{N_{s}-1}_{k=i+1}^{N_{s}}_{f,}^{()} _{S_{i}},_{S_{k}}\] (17)

**Minimax optimization.** To find the optimal \(f\) in the hypothesis space \(\), we formulate our objective function as a minimization problem. As mentioned earlier, we minimize the sum of the empirical source errors and the \(\) term estimated using Eqn (17). Thus, the minimization problem can be written as:

\[_{f}_{i=1}^{N_{s}}_{i}err_{_{S_{i}}}^{ ()}(f)+_{i=1}^{N_{s}-1}_{k=i+1}^{N_{s}}_{f,}^{()}_{S_{i}},_{S_{k}}\] (18)

Since empirical MDD is defined as the supremum of the hypothesis space \(\), minimizing it is in turn a minimax game with a strong max player and weaker min player. In order to strengthen the min player, we use a feature extractor, \(\), which further modifies the optimization problem as follows:

\[&_{f,}_{i=1}^{N_{s}}_{i} err_{(_{S_{i}})}^{()}(f)+_{l=1}^{j} _{(_{S_{i}})}^{()}f_{l}^{*},f -_{(_{S_{k}})}^{()}f_{l}^{*},f ,\\ & f_{l}^{*}=_{f_{l}^{}}_{ (_{S_{l}})}^{()}f_{l}^{},f-_{ (_{S_{k}})}^{()}f_{l}^{},f, f,f_{l}^{} l=1,,j}.\] (19)

where \(j=}{2}\). The relationship between \(l,i\) and \(k\) is as follows: if \(l=3\) then we pick the \(i\) and \(k\) value corresponding to the 3rd element of the set \((i,k):i=1,,N_{s}-1},k=i+1,,N_{s} }}\). To solve the minimization problem in Eqn. (19), we design an adversarial learning algorithm whose model architecture is shown in Fig. 2. One classifier \(f\) performs the classification task on all source domains, while \(j\) other classifiers, denoted by \(f^{}\), compute the empirical MDD between different pairs of source domains, which finally sum together as the _transfer loss_. To compute the source errors in Eqn. (19), we use the standard cross-entropy loss \((_{S_{i}})\). For convenience of optimization (MDD can be hard to optimize directly using stochastic gradient descent), in practice, following , we approximate the MDD loss as \(_{(,l)}_{_{i}},_{ _{k}}\) in terms of two loss functions, \(L\) and \(L^{}\), as shown below.

\[_{_{i}} =_{(x,y)_{_{i}}}L f(x),y\] (20) \[_{(,l)}_{_{i}},_{_{k}} =_{(x,y)_{_{k}}}L^{ }f_{l}^{}(x),f(x) -_{(x,y)_{_{i}}} Lf_{l}^{}(x),f( x).\]

We train the feature extractor, \(\), to minimize the above MDD loss term by using a Gradient Reversal Layer (GRL) proposed in , as \(_{(,l)}(_{_{i}},_{_{k}})\) is not differentiable w.r.t the parameters of \(f\). \(L\) and \(L^{}\) are defined as:

Figure 2: Architecture of the proposed MADG methodology\[Lf(x),y-_{y} f(x),\ L^{}f^{} (x),f(x) {=}1-_{_{f}(x)}f^{ }(x)\] (21)

where \(_{w}(z)=}{_{i=1}^{N_{s}}e^{i}}\), \(z^{k}\), for \(w=1,,k\), and \(=e^{}\). The final optimization problem that the **MADG** method solves is shown below.

\[_{f,}_{i=1}^{N_{s}}_{i}(_{ _{i}})+_{l=1}^{j}_{(,l)} _{_{i}},_{_{k}},\ \ _{f_{1}^{ },,f_{j}^{}}_{l=1}^{j}_{(,l) }_{_{i}},_{_{k}}\] (22)

We propose an adversarial DG algorithm to solve the minimization problem, as shown in Algorithm 1. We train the proposed adversarial model in two steps. In the first step, we update the parameters \(f\ \). In the next step, we first do a forward pass to compute the outputs from \(f\) and then update the parameters \(f^{}\)and \(\) as shown in the algorithm. We further show through our ablation studies that this way of updating parameters at different steps outperforms a joint update strategy.

``` INPUT: \(N_{s}\) labeled source domains for epoch \( 1\) to total_epochs do for batch \( 1\) to total_batches do  Update parameters of \(f\), \(\), using Eqn. (20) \(_{_{_{i}}} f((x))\  i\{1,2,,N_{s}\}\)  Compute \(MDD_{l}\) as in Eqn. (20) using \(_{_{_{i}}}\)  Compute Transfer loss \(=_{l=1}^{j}(_{(,l)}(_{_{i}},_{_{k}}))\)  Update parameters of \(f^{}\)and \(\) as in Eqn. (22) endfor endfor ```

**Algorithm 1** Margin-based adversarial learning for Domain Generalization (MADG)

## 6 Experiments

**Datasets and Implementation Details.** We perform an extensive evaluation on five benchmark DG datasets for image classification: VLCS , PACS , OfficeHome (OH) , TerraIncognita (TI)  and DomainNet (DN) .We follow  in using 'Test-domain validation' procedure for hyperparameter selection. We use the Resnet50 architecture  pre-trained on ImageNet dataset  as the feature extractor (\(\)). We train our model using stochastic gradient descent with momentum . We use a mini-batch that contains 32 samples from all source domains. All other implementation details including hyperparameters such as learning rate, margin, and weight decay are provided in the Appendix.

**Baselines and evaluation metrics.** We report the results of all baseline models from , which are represented by \(\) in Table 1. We also present the results from , which are represented by \(\). We run our experiments for three trials and report the average accuracy (with standard deviation). Existing state-of-the-art methods (see ) focus on average accuracy across the benchmarks, with each method doing well on a subset of datasets. In order to reward consistent performance across datasets while improving average accuracy, we additionally include a ranking-based metric (from ) and go beyond ranking with two new metrics (AD and GD), which more precisely capture the consistent performance of a method across datasets: **(i) Median rank (M) :** This measures the median rank of a model's performance across all datasets and is not skewed to best/worst ranks when compared to the mean rank. **(ii) Arithmetic mean of differences (AD):** This measures the difference of a model's performance w.r.t the highest-performing model's accuracy on a given dataset, and then taking its arithmetic mean across datasets. **(iii) Geometric mean of differences (GD):** This similarly measures the geometric mean of the difference between the highest accuracy and the achieved accuracy of a model across all datasets.

**Results.** We report the results on different DG benchmark datasets on the considered evaluation metrics in Table 1. It is observed that most previous methods are far from the best-performing model's accuracy on at least one of the datasets. This is captured well by the metrics AD and GD, which quantify the mean of the differences from the best accuracy across all the datasets. Thus, we observe higher values in the first two columns of Table 1, where our **MADG** model performs the best showing its consistency. The MADG algorithm outperforms all other models on average accuracy, median rank, AD, and GD, thus demonstrating consistent performance.

The results also show that MADG outperform DANN and CDANN models (which are based on the 0-1 loss discrepancy theory) across most datasets and improves the average accuracy by \( 2\%\) with significantly better AD and GD values too - corroborating our margin-based approach to the DG problem. MADG is 1% better on the avg accuracy (and lower AD-GD values) when compared to ERM, which was found to be a strong baseline for DG in . Our model also reports an accuracy improvement of \( 3\%\) on the OfficeHome dataset compared to all other models.

To highlight our contribution, we also provide a grouping of DG methods that are theory-based in Table 1 (also denoted with a \(*\)), some of which do not report results on all benchmark datasets. For the only such method that reports results on all datasets - MTL, MADG achieves \(1.2\%\) more average accuracy, 1.3 units less AD, 1.7 units less GD, and a lower median ranking as compared to MTL. More results, including 'Training-domain' model selection results, are provided in the Appendix.

## 7 More Empirical Analysis and Ablation Studies

**Computational Cost.** The computational cost of a model is usually measured in terms of GPU RAM occupied (GB) and time taken per step (s). In terms of computational cost, following Eqn. (17), while we iteratively compute MDD over all domains, our computational cost during training is similar to other benchmark methods as shown in Table 2. As seen in the table, even simple methods like ERM and Mixup have running times in similar ranges.

**Margin.** As shown in Theorem 3, the margin value \(\) plays an important role, especially with its dependency on the generalization error. In practice, to get the desired margin while training the MADG algorithm, we approximate it as \(}{{=}}\). We experiment with different margin values, \(=1,1.5\), \(2\), and \(3\), and report the average accuracy for PACS and VLCS dataset in Table 3. Table 3 shows that \(=1.5\) outperforms other values on both datasets. As seen from the definition of the margin loss function Eqn. (1) and MDD Eqn. (5), although having a large margin is better to achieve low loss and higher target accuracy, there is a tradeoff between optimal margin and loss value. We observe a similar trend in Table 3.

**Experiments on Colored MNIST.** As stated in Theorem 2, \(\) is defined as the projection of the unseen domain onto the convex hull. It is also defined in Theorem 2 as the MDD between the convex hull and the unseen domain, which can be further approximated with a combination of two different cross-entropy functions as in Eq (20). This equation is equivalent to a balanced Jensen-Shannon (JS) Divergence as shown in Proposition D.1. in . Thus, we can approximate this projection by computing the JS divergence between domains. We experimented on the Colored MNIST dataset, where we noted the \(\) values to be small for all domains, as shown

  
**Algorithm** & **AD(\(\))** & **GD(\(\))** & **M(\(\))** & **VLCS** & **OH** & **PACS** & **TI** & **DN** & **Avg(\(\))** \\  ERM (1998)\({}^{}\) & 2.2 & 1.8 & 8 & 77.6\({}_{0.3}\) & 66.4\({}_{0.3}\) & 86.7\({}_{0.3}\) & 53.0\({}_{0.3}\) & 41.3\({}_{0.1}\) & 65.0 \\ CORAL (2016)\({}^{}\) & 1.9 & 1.1 & 8 & 77.7\({}_{0.2}\) & 65.4\({}_{0.2}\) & 87.1\({}_{0.3}\) & 52.8\({}_{0.2}\) & 41.8\({}_{0.1}\) & 65.6 \\ DANN (2016)\({}^{}\) & 3.4 & 2.1 & 16 & 79.7\({}_{0.3}\) & 65.3\({}_{0.8}\) & 85.2\({}_{0.2}\) & 50.6\({}_{0.4}\) & 38.3\({}_{0.1}\) & 63.8 \\ CDANN (2018)\({}^{}\) & 3.2 & 0.7 & 14 & **79.9\({}_{0.9}\)** & 65.3\({}_{0.5}\) & 85.8\({}_{0.8}\) & 50.8\({}_{0.8}\) & 38.5\({}_{0.2}\) & 64.1 \\ MDDG (2018)\({}^{}\) & 2.3 & 1.8 & 6 & 77.5\({}_{0.2}\) & 66.0\({}_{0.3}\) & 86.0\({}_{0.4}\) & 52.0\({}_{0.1}\) & 41.6\({}_{0.1}\) & 64.9 \\ MMD (2018)\({}^{}\) & 5.9 & 0.9 & 10 & 77.9\({}_{0.1}\) & 66.2\({}_{0.3}\) & 87.2\({}_{0.2}\) & 52.0\({}_{0.4}\) & 23.5\({}_{0.4}\) & 61.4 \\ IRM (2019)\({}^{}\) & 6.7 & 5.3 & 18 & 76.9\({}_{0.6}\) & 63.0\({}_{0.2}\) & 84.5\({}_{0.1}\) & 50.5\({}_{0.7}\) & 28.0\({}_{0.1}\) & 60.6 \\ GroupDRO (2019)\({}^{}\) & 3.9 & 1.9 & 10 & 77.4\({}_{0.6}\) & 66.2\({}_{0.3}\) & 87.1\({}_{0.1}\) & 52.4\({}_{0.3}\) & 33.4\({}_{0.3}\) & 63.3 \\ Mixup (2020)\({}^{}\) & 1.9 & 0.4 & 5 & 78.1\({}_{0.3}\) & 68.0\({}_{0.2}\) & 86.6\({}_{0.3}\) & 54.0\({}_{0.4}\) & 39.6\({}_{0.1}\) & 65.4 \\ ARM (2020)\({}^{}\) & 4.1 & 3.4 & 14 & 77.8\({}_{0.3}\) & 64.8\({}_{0.4}\) & 58.5\({}_{0.2}\) & 51.2\({}_{0.5}\) & 36.0\({}_{0.2}\) & 63.1 \\ RSC (2020)\({}^{}\) & 2.9 & 2.5 & 9 & 77.8\({}_{0.6}\) & 66.5\({}_{0.8}\) & 86.2\({}_{0.5}\) & 52.1\({}_{0.2}\) & 38.9\({}_{0.6}\) & 64.3 \\ SagNet (2021)\({}^{}\) & 2.3 & 2.0 & 6 & 77.6\({}_{0.1}\) & 67.5\({}_{0.2}\) & 86.4\({}_{0.4}\) & 52.5\({}_{0.4}\) & 40.8\({}_{0.2}\) & 65.0 \\ V-REx (2021)\({}^{}\) & 4.7 & 0.8 & 12 & 78.1\({}_{0.2}\) & 65.7\({}_{0.3}\) & 78.2\({}_{0.4}\) & 51.4\({}_{0.5}\) & 30.1\({}_{0.3}\) & 62.5 \\ AND-mask (2021)\({}^{}\) & 3.9 & 3.3 & 13 & 76.4\({}_{0.4}\) & 66.1\({}_{0.2}\) & 86.4\({}_{0.4}\) & 49.8\({}_{0.4}\) & 37.9\({}_{0.6}\) & 63.3 \\ Fish (2021)\({}^{}\) & 2.5 & 0.6 & 13 & 77.8\({}_{0.6}\) & 66.0\({}_{0.2}\) & 85.8\({}_{0.6}\) & 50.8\({}_{0.4}\) & **33.4\({}_{0.3}\)** & 64.8 \\ SAND-mask (2021)\({}^{}\) & 5.2 & 4.2 & 16 & 76.2\({}_{0.5}\) & 65.9\({}_{0.5}\) & 85.9\({}_{0.5}\) & 50.2\({}_{0.1}\) & 32.2\({}_{0.6}\) & 62.1 \\ Fisher (20221)\({}^{}\) & 1.5 & 1.2 & 78.2\({}_{0.2}\) & 68.2\({}_{0.2}\) & 86.2\({}_{0.3}\) & 86.9\({}_{0.2}\) & 53.6\({}_{0.4}\) & 41.8\({}_{0.2}\) & 65.7 \\  G2DM (2019)\({}^{}\) & - & - & - & 75.9\({}_{0.1}\) & - & 73.6\({}_{0.1}\) & - & - & - \\ MTL (2021)\({}^{}\) & - & 2.5 & 2.0 & 8 & 77.7\({}_{0.5}\) & 66.5\({}_{0.1}\) & 86.7\({}_{0.2}\) & 52.2\({}_{0.4}\) & 40.8\({}_{0.1}\) & 64.8 \\ Transfer (2021)\({}^{}\) & - & - & - & - & 64.3\({}_{0.1}\) & 85.3\({}_{0.2}\) & - & - & - \\ OOD (2021)\

[MISSING_PAGE_FAIL:10]