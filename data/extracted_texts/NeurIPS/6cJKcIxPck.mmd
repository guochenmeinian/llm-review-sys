# Strategic Classification under

Unknown Personalized Manipulation

 Han Shao

Toyota Technological Institute Chicago

Chicago, 60637

han@ttic.edu

&Avrim Blum

Toyota Technological Institute Chicago

Chicago, 60637

avrim@ttic.edu

&Omar Montasser

Toyota Technological Institute Chicago

Chicago, 60637

omar@ttic.edu

###### Abstract

We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. _Ball manipulations_ are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work considers manipulations to be _personalized_, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and _unknown_ to the learner.

We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classifier. We investigate various scenarios in terms of the information available to the learner during the interaction, such as observing the original feature vector before or after deployment, observing the manipulated feature vector, or not seeing either the original or the manipulated feature vector. We begin by providing online mistake bounds and PAC sample complexity in these scenarios for ball manipulations. We also explore non-ball manipulations and show that, even in the simplest scenario where both the original and the manipulated feature vectors are revealed, the mistake bounds and sample complexity are lower bounded by \((||)\) when the target function belongs to a known class \(\).

## 1 Introduction

Strategic classification addresses the problem of learning a classifier robust to manipulation and gaming by self-interested agents (Hardt et al., 2016). For example, given a classifier determining loan approval based on credit scores, applicants could open or close credit cards and bank accounts to increase their credit scores. In the case of a college admission classifier, students may try to take easier classes to improve their GPA, retake the SAT or change schools in an effort to be admitted. In both cases, such manipulations do not change their true qualifications. Recently, a collection of papers has studied strategic classification in both the online setting where examples are chosen by an adversary in a sequential manner (Dong et al., 2018; Chen et al., 2020; Ahmadi et al., 2021, 2023), and thedistributional setting where the examples are drawn from an underlying data distribution (Hardt et al., 2016; Zhang and Conitzer, 2021; Sundaram et al., 2021; Lechner and Urner, 2022). Most existing works assume that manipulation ability is uniform across all agents or is known to the learner. However, in reality, this may not always be the case. For instance, low-income students may have a lower ability to manipulate the system compared to their wealthier peers due to factors such as the high costs of retaking the SAT or enrolling in additional classes, as well as facing more barriers to accessing information about college (Milli et al., 2019) and it is impossible for the learner to know the highest achievable GPA or the maximum number of times a student may retake the SAT due to external factors such as socio-economic background and personal circumstances.

We characterize the manipulation of an agent by a set of alternative feature vectors that she can modify her original feature vector to, which we refer to as the _manipulation set_. _Ball manipulations_ are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. For example, Dong et al. (2018); Chen et al. (2020); Sundaram et al. (2021) studied ball manipulations with distance function being some norm and Zhang and Conitzer (2021); Lechner and Urner (2022); Ahmadi et al. (2023) studied a manipulation graph setting, which can be viewed as ball manipulation w.r.t. the graph distance on a predefined known graph.

In the online learning setting, the strategic agents come sequentially and try to game the current classifier. Following previous work, we model the learning process as a repeated Stackelberg game over \(T\) time steps. In round \(t\), the learner proposes a classifier \(f_{t}\) and then the agent, with a manipulation set (unknown to the learner), manipulates her feature in an effort to receive positive prediction from \(f_{t}\). There are several settings based on what and when the information is revealed about the original feature vector and the manipulated feature vector in the game. The simplest setting for the learner is observing the original feature vector before choosing \(f_{t}\) and the manipulated vector after. In a slightly harder setting, the learner observes both the original and manipulated vectors after selecting \(f_{t}\). An even harder setting involves observing only the manipulated feature vector after selecting \(f_{t}\). The hardest and least informative scenario occurs when neither the original nor the manipulated feature vectors are observed.

In the distributional setting, the agents are sampled from an underlying data distribution. Previous work assumes that the learner has full knowledge of the original feature vector and the manipulation set, and then views learning as a one-shot game and solves it by computing the Stackelberg equilibria of it. However, when manipulations are personalized and unknown, we cannot compute an equilibrium and study learning as a one-shot game. In this work, we extend the iterative online interaction model from the online setting to the distributional setting, where the sequence of agents is sampled i.i.d. from the data distribution. After repeated learning for \(T\) (which is equal to the sample size) rounds, the learner has to output a strategy-robust predictor for future use.

In both online and distributional settings, examples are viewed through the lens of the current predictor and the learner does not have the ability to inquire about the strategies the previous examples would have adopted under a different predictor.

Related workOur work is primarily related to strategic classification in online and distributional settings. Strategic classification was first studied in a distributional model by Hardt et al. (2016) and subsequently by Dong et al. (2018) in an online model. Hardt et al. (2016) assumed that agents manipulate by best response with respect to a uniform cost function known to the learner. Building on the framework of (Hardt et al., 2016), Lechner and Urner (2022); Sundaram et al. (2021); Zhang and Conitzer (2021); Hu et al. (2019); Milli et al. (2019) studied the distributional learning problem, and all of them assumed that the manipulations are predefined and known to the learner, either by a cost function or a predefined manipulation graph. For online learning, Dong et al. (2018) considered a similar manipulation setting as in this work, where manipulations are personalized and unknown. However, they studied linear classification with ball manipulations in the online setting and focused on finding appropriate conditions of the cost function to achieve sub-linear Stackelberg regret. Chen et al. (2020) also studied Stackelberg regret in linear classification with uniform ball manipulations. Ahmadi et al. (2021) studied the mistake bound under uniform (possibly unknown) ball manipulations, and Ahmadi et al. (2023) studied regret under a pre-defined and known manipulation. The most relevant work is a recent concurrent study by Lechner et al. (2023), which also explores strategic classification involving unknown personalized manipulations but with a different loss function. In their work, a predictor incurs a loss of \(0\) if and only if the agent refrains from manipulation and the predictor correctly predicts at the unmanipulated feature vector. In our work, the predictor's loss is \(0\)if it correctly predicts at the manipulated feature, even when the agent manipulates. As a result, their loss function serves as an upper bound of our loss function.

There has been a lot of research on various other issues and models in strategic classification. Beyond sample complexity, Hu et al. (2019); Milli et al. (2019) focused on other social objectives, such as social burden and fairness. Recent works also explored different models of agent behavior, including proactive agents Zrnic et al. (2021), non-myopic agents (Haghtalab et al., 2022) and noisy agents (Jagadeesan et al., 2021). Ahmadi et al. (2023) considers two agent models of randomized learners: a randomized algorithm model where the agents respond to the realization, and a fractional classifier model where agents respond to the expectation, and our model corresponds to the randomized algorithm model. Additionally, there is also a line of research on agents interested in improving their qualifications instead of gaming (Kleinberg and Raghavan, 2020; Haghtalab et al., 2020; Ahmadi et al., 2022). Strategic interactions in the regression setting have also been studied (e.g., Bechavod et al. (2021)).

Beyond strategic classification, there is a more general research area of learning using data from strategic sources, such as a single data generation player who manipulates the data distribution (Bruckner and Scheffer, 2011; Dalvi et al., 2004). Adversarial perturbations can be viewed as another type of strategic source (Montasser et al., 2019).

## 2 Model

Strategic classificationThroughout this work, we consider the binary classification task. Let \(\) denote the feature vector space, \(=\{+1,-1\}\) denote the label space, and \(^{}\) denote the hypothesis class. In the strategic setting, instead of an example being a pair \((x,y)\), an example, or _agent_, is a triple \((x,u,y)\) where \(x\) is the original feature vector, \(y\) is the label, and \(u\) is the manipulation set, which is a set of feature vectors that the agent can modify their original feature vector \(x\) to. In particular, given a hypothesis \(h^{}\), the agent will try to manipulate her feature vector \(x\) to another feature vector \(x^{}\) within \(u\) in order to receive a positive prediction from \(h\). The manipulation set \(u\) is _unknown_ to the learner. In this work, we will be considering several settings based on what the information is revealed to the learner, including both the original/manipulated feature vectors, the manipulated feature vector only, or neither, and when the information is revealed.

More formally, for agent \((x,u,y)\), given a predictor \(h\), if \(h(x)=-1\) and her manipulation set overlaps the positive region by \(h\), i.e., \(u_{h,+}\) with \(_{h,+}:=\{x|h(x)=+1\}\), the agent will manipulate \(x\) to \((x,h,u) u_{h,+}\)1 to receive positive prediction by \(h\). Otherwise, the agent will do nothing and maintain her feature vector at \(x\), i.e., \((x,h,u)=x\). We call \((x,h,u)\) the manipulated feature vector of agent \((x,u,y)\) under predictor \(h\).

A general and fundamental type of manipulations is _ball manipulations_, where agents can manipulate their feature within a ball of _personalized_ radius. More specifically, given a metric \(d\) over \(\), the manipulation set is a ball \((x;r)=\{x^{}|d(x,x^{}) r\}\) centered at \(x\) with radius \(r\) for some \(r_{ 0}\). Note that we allow different agents to have different manipulation power and the radius can vary over agents. Let \(Q\) denote the set of allowed pairs \((x,u)\), which we refer to as the feature-manipulation set space. For ball manipulations, we have \(=\{(x,(x;r))|x,r_{ 0}\}\) for some known metric \(d\) over \(\). In the context of ball manipulations, we use \((x,r,y)\) to represent \((x,(x;r),y)\) and \((x,h,r)\) to represent \((x,h,(x;r))\) for notation simplicity.

For any hypothesis \(h\), let the strategic loss \(^{}(h,(x,u,y))\) of \(h\) be defined as the loss at the manipulated feature, i.e., \(^{}(h,(x,u,y)):=(h((x,h,u)) y)\). According to our definition of \(()\), we can write down the strategic loss explicitly as

\[^{}(h,(x,u,y))=1&y=-1,h(x)=+1\\ 1&y=-1,h(x)=-1u_{h,+}\,,\\ 1&y=+1,h(x)=-1u_{h,+}=\,,\\ 0& \]

For any randomized predictor \(p\) (a distribution over hypotheses), the strategic behavior depends on the realization of the predictor and the strategic loss of \(p\) is \(^{}(p,(x,u,y)):=_{h p}[^{}(h,(x,u,y))]\).

Online learningWe consider the task of sequential classification where the learner aims to classify a sequence of agents \((x_{1},u_{1},y_{1}),(x_{2},u_{2},y_{2}),,(x_{T},u_{T},y_{T}) \) that arrives in an online manner. At each round, the learner feeds a predictor to the environment and then observes his prediction \(_{t}\), the true label \(y_{t}\) and possibly along with some additional information about the original/manipulated feature vectors. We say the learner makes a mistake at round \(t\) if \(_{t} y_{t}\) and the learner's goal is to minimize the number of mistakes on the sequence. The interaction protocol (which repeats for \(t=1,,T\)) is described in the following.

```
1:The environment picks an agent \((x_{t},u_{t},y_{t})\) and reveals some context \(C(x_{t})\). In the online setting, the agent is chosen adversarially, while in the distributional setting, the agent is sampled i.i.d.
2:The learner \(\) observes \(C(x_{t})\) and picks a hypothesis \(f_{t}^{}\).
3:The learner \(\) observes the true label \(y_{t}\), the prediction \(_{t}=f_{t}(_{t})\), and some feedback \(F(x_{t},_{t})\), where \(_{t}=(x_{t},f_{t},u_{t})\) is the manipulated feature vector.
```

**Protocol 1** Learner-Agent Interaction at round \(t\)

The context function \(C()\) and feedback function \(F()\) reveals information about the original feature vector \(x_{t}\) and the manipulated feature vector \(_{t}\). \(C()\) reveals the information before the learner picks \(f_{t}\) while \(F()\) does after. We study several different settings based on what and when information is revealed.

* The simplest setting for the learner is observing the original feature vector \(x_{t}\) before choosing \(f_{t}\) and the manipulated vector \(_{t}\) after. Consider a teacher giving students a writing assignment or take-home exam. The teacher might have a good knowledge of the students' abilities (which correspond to the original feature vector \(x_{t}\)) based on their performance in class, but the grade has to be based on how well they do the assignment. The students might manipulate by using the help of ChatGPT / Google / WolframAlpha / their parents, etc. The teacher wants to create an assignment that will work well even in the presence of these manipulation tools. In addition, If we think of each example as representing a subpopulation (e.g., an organization is thinking of offering loans to a certain group), then there might be known statistics about that population, even though the individual classification (loan) decisions have to be made based on responses to the classifier. This setting corresponds to \(C(x_{t})=x_{t}\) and \(F(x_{t},_{t})=_{t}\). We denote a setting by their values of \(C,F\) and thus, we denote this setting by \((x,)\).
* In a slightly harder setting, the learner observes both the original and manipulated vectors after selecting \(f_{t}\) and thus, \(f_{t}\) cannot depend on the original feature vector in this case. For example, if a high-school student takes the SAT test multiple times, most colleges promise to only consider the highest one (or even to "superscore" the test by considering the highest score separately in each section) but they do require the student to submit all of them. Then \(C(x_{t})=\) and \(F(x_{t},_{t})=(x_{t},_{t})\), where \(\) is a token for "no information", and this setting is denoted by \((,(x,))\).
* An even harder setting involves observing only the manipulated feature vector after selecting \(f_{t}\) (which can only be revealed after \(f_{t}\) since \(_{t}\) depends on \(f_{t}\)). Then \(C(x_{t})=\) and \(F(x_{t},_{t})=_{t}\) and this setting is denoted by \((,)\).
* The hardest and least informative scenario occurs when neither the original nor the manipulated feature vectors are observed. Then \(C(x_{t})=\) and \(F(x_{t},_{t})=\) and it is denoted by \((,)\).

Throughout this work, we focus on the _realizable_ setting, where there exists a perfect classifier in \(\) that never makes any mistake at the sequence of strategic agents. More specifically, there exists a hypothesis \(h^{*}\) such that for any \(t[T]\), we have \(y_{t}=h^{*}((x_{t},h^{*},u_{t}))\)2. Then we define the mistake bound as follows.

**Definition 1**.: _For any choice of \((C,F)\), let \(\) be an online learning algorithm under Protocol 1 in the setting of \((C,F)\). Given any realizable sequence \(S=((x_{1},u_{1},h^{*}((x_{1},h^{*},u_{1}))),,(x_{T},u_{T},h^{*}( (x_{T},h^{*},u_{T})))()^{T}\), where \(T\) is any integer and \(h^{*}\), let \(_{}(S)\) be the number of mistakes \(\) makes on the sequence \(S\). The mistake bound of \((,)\), denoted \(_{C,F}\), is the smallest number \(B\) such that there exists an algorithm \(\) such that \(_{}(S) B\) over all realizable sequences \(S\) of the above form._According the rank of difficulty of the four settings with different choices of \((C,F)\), the mistake bounds are ranked in the order of \(_{x,}_{,(x,)}_{, }_{,}\).

PAC learningIn the distributional setting, the agents are sampled from an underlying distribution \(\) over \(\). The learner's goal is to find a hypothesis \(h\) with low population loss \(_{}^{}(h):=_{(x,u,y)} [}(h,(x,u,y))$}]\). One may think of running empirical risk minimizer (ERM) over samples drawn from the underlying data distribution, i.e., returning \(_{h}_{i=1}^{m}^{}(h,(x_{i}, u_{i},y_{i}))\), where \((x_{1},u_{1},y_{1}),,(x_{m},u_{m},y_{m})\) are i.i.d. sampled from \(\). However, ERM is unimplementable because the manipulation sets \(u_{i}\)'s are never revealed to the algorithm, and only the partial feedback in response to the implemented classifier is provided. In particular, in this work we consider using the same interaction protocol as in the online setting, i.e., Protocol 1, with agents \((x_{t},u_{t},y_{t})\) i.i.d. sampled from the data distribution \(\). After \(T\) rounds of interaction (i.e., \(T\) i.i.d. agents), the learner has to output a predictor \(f_{}\) for future use.

Again, we focus on the _realizable_ setting, where the sequence of sampled agents (with manipulation) can be perfectly classified by a target function in \(\). Alternatively, there exists a classifier with zero population loss, i.e., there exists a hypothesis \(h^{*}\) such that \(_{}^{}(h^{*})=0\). Then we formalize the notion of PAC sample complexity under strategic behavior as follows.

**Definition 2**.: _For any choice of \((C,F)\), let \(\) be a learning algorithm that interacts with agents using Protocol 1 in the setting of \((C,F)\) and outputs a predictor \(f_{}\) in the end. For any \(,(0,1)\), the sample complexity of realizable \((,)\)-PAC learning of \((,)\), denoted \(_{C,F}(,)\), is defined as the smallest \(m\) for which there exists a learning algorithm \(\) in the above form such that for any distribution \(\) over \(\) where there exists a predictor \(h^{*}\) with zero loss, \(_{}^{}(h)=0\), with probability at least \(1-\) over \((x_{1},u_{1},y_{1}),,(x_{m},u_{m},y_{m})\), \(_{}^{}(f_{})\)._

Similar to mistake bounds, the sample complexities are ranked in the same order \(_{x,}_{,(x,)}_{,}_{,}\) according to the rank of difficulty of the four settings.

## 3 Overview of Results

In classic (non-strategic) online learning, the Halving algorithm achieves a mistake bound of \((||)\) by employing the majority vote and eliminating inconsistent hypotheses at each round. In classic PAC learning, the sample complexity of \((|)}{})\) is achievable via ERM. Both mistake bound and sample complexity exhibit logarithmic dependency on \(||\). This logarithmic dependency on \(||\) (when there is no further structural assumptions) is tight in both settings, i.e., there exist examples of \(\) with mistake bound of \(((||))\) and with sample complexity of \((|)}{})\). In the setting where manipulation is known beforehand and only \(_{t}\) is observed, Ahmadi et al. (2023) proved a lower bound of \((||)\) for the mistake bound. Since in the strategic setting we can achieve a linear dependency on \(||\) by trying each hypothesis in \(\) one by one and discarding it once it makes a mistake, the question arises:

_Can we achieve a logarithmic dependency on \(||\) in strategic classification?_

In this work, we show that the dependency on \(||\) varies across different settings and that in some settings mistake bound and PAC sample complexity can exhibit different dependencies on \(||\). We start by presenting our results for ball manipulations in the four settings.

* Setting of \((x,)\) (observing \(x_{t}\) before choosing \(f_{t}\) and observing \(_{t}\) after) : For online learning, we propose an variant of the Halving algorithm, called Strategic Halving (Algorithm 1), which can eliminate half of the remaining hypotheses when making a mistake. The algorithm depends on observing \(x_{t}\) before choosing the predictor \(f_{t}\). Then by applying the standard technique of converting mistake bound to PAC bound, we are able to achieve sample complexity of \((|)(||)}{})\).
* Setting of \((,(x,))\) (observing both \(x_{t}\) and \(_{t}\) after selecting \(f_{t}\)) : We prove that, there exists an example of \((,)\) s.t. the mistake bound is lower bounded by \((||)\). This implies that no algorithm can perform significantly better than sequentially trying each hypothesis, which would make at most \(||\) mistakes before finding the correct hypothesis. However, unlike the construction of mistake lower bounds in classic online learning, where all mistakes can be forced to occur in the initial rounds, we demonstrate that we require \((||^{2})\) rounds to ensure that all mistakes occur. Inthe PAC setting, we first show that, any learning algorithm with proper output \(f_{}\), i.e., \(f_{}\), needs a sample size of \((|}{})\). We can achieve a sample complexity of \(O((||)}{})\) by executing Algorithm 2, which is a randomized algorithm with improper output.
* Setting of \((,)\) (observing only \(_{t}\) after selecting \(f_{t}\)) : The mistake bound of \((||)\) also holds in this setting, as it is known to be harder than the previous setting. For the PAC learning, we show that any conservative algorithm, which only depends on the information from the mistake rounds, requires \((|}{})\) samples. The optimal sample complexity is left as an open problem.
* Setting of \((,)\) (observing neither \(x_{t}\) nor \(_{t}\)) : Similarly, the mistake bound of \((||)\) still holds. For the PAC learning, we show that the sample complexity is \((|}{})\) by reducing the problem to a stochastic linear bandit problem.

Then we move on to non-ball manipulations. However, we show that even in the simplest setting of observing \(x_{t}\) before choosing \(f_{t}\) and observing \(_{t}\) after, there is an example of \((,)\) such that the sample complexity is \((|}{})\). This implies that in all four settings of different revealed information, we will have sample complexity of \((|}{})\) and mistake bound of \((||)\). We summarize our results in Table 1.

## 4 Ball manipulations

In ball manipulations, when \((x;r)_{h,+}\) has multiple elements, the agent will always break ties by selecting the one closest to \(x\), i.e., \((x,h,r)=_{x^{}(x;r)_{h,+}}d( x,x^{})\). In round \(t\), the learner deploys predictor \(f_{t}\), and once he knows \(x_{t}\) and \(_{t}\), he can calculate \(_{t}\) himself without needing knowledge of \(r_{t}\) by

\[_{t}=_{x^{}_{f_{t},+}}d(x_{t},x ^{})&_{t}=+1\,,\\ x_{t}&_{t}=-1\,.\]

Thus, for ball manipulations, knowing \(x_{t}\) is equivalent to knowing both \(x_{t}\) and \(_{t}\).

### Setting \((x,)\): Observing \(x_{t}\) Before Choosing \(f_{t}\)

Online learningWe propose a new algorithm with mistake bound of \((||)\) in setting \((x,)\). To achieve a logarithmic mistake bound, we must construct a predictor \(f_{t}\) such that if it makes a mistake, we can reduce a constant fraction of the remaining hypotheses. The primary challenge is that we do not have access to the full information, and predictions of other hypotheses are hidden. To extract the information of predictions of other hypotheses, we take advantage of ball manipulations, which

  & setting & mistake bound & sample complexity \\   & \((x,)\) & \(((||))\) (Thm 1) & \(}(|)}{})^{a}\) (Thm 2), \((|)}{})\) \\   & \((,(x,))\) & \(((|)T},||)) \\ ((|(||)},||)) \\ \) & \((|)}{})\) (Thm 6), \((|)}{})\) \\   & \((,)\) & \((||)\) (implied by Thm 3) & \(^{}=(|}{})\) (Thm 7) \\   & \((,)\) & \((||)\) (implied by Thm 3) & \(}(|}{})\), \((|}{})\) (Thm 8) \\  nonball & all & \((||)\)(Cor 1), \((||)\) & \(}(|}{})\), \((|}{})\) (Cor 1) \\   \({}^{a}\) A factor of \(\)log\((||)\) is neglected.

Table 1: The summary of results. \(}\) and \(\) ignore logarithmic factors on \(||\) and \(\). The superscripts prop stands for proper learning algorithms and csv stands for conservative learning algorithms. All lower bounds in the non-strategic setting also apply to the strategic setting, implying that \(_{C,F}((||))\) and \(_{C,F}(|)}{})\) for all settings of \((C,F)\). In all four settings, a mistake bound of \((||)\) can be achieved by simply trying each hypothesis in \(\) while the sample complexity can be achieved as \(}(|}{})\) by converting the mistake bound of \((||)\) to a PAC bound using standard techniques.

induces an ordering over all hypotheses. Specifically, for any hypothesis \(h\) and feature vector \(x\), we define the distance between \(x\) and \(h\) by the distance between \(x\) and the positive region by \(h\), \(_{h,+}\), i.e.,

\[d(x,h):=\{d(x,x^{})|x^{}_{h,+}\}\,. \]

At each round \(t\), given \(x_{t}\), the learner calculates the distance \(d(x_{t},h)\) for all \(h\) in the version space (meaning hypotheses consistent with history) and selects a hypothesis \(f_{t}\) such that \(d(x_{t},f_{t})\) is the median among all distances \(d(x_{t},h)\) for \(h\) in the version space. We can show that by selecting \(f_{t}\) in this way, the learner can eliminate half of the version space if \(f_{t}\) makes a mistake. We refer to this algorithm as Strategic Halving, and provide a detailed description of it in Algorithm 1.

**Theorem 1**.: _For any feature-ball manipulation set space \(\) and hypothesis class \(\), Strategic Halving achieves mistake bound \(_{x,}(||)\)._

```
1:Initialize the version space \(=\).
2:for\(t=1,,T\)do
3: pick an \(f_{t}\) such that \(d(x_{t},f_{t})\) is the median of \(\{d(x_{t},h)|h\}\).
4:if\(_{t} y_{t}\) and \(y_{t}=+\)then\(\{h|d(x_{t},h) d (x_{t},f_{t})\}\);
5:elseif\(_{t} y_{t}\) and \(y_{t}=-\)then\(\{h|d(x_{t},h) d (x_{t},f_{t})\}\).
6:endfor
```

**Algorithm 1** Strategic Halving

To prove Theorem 1, we only need to show that each mistake reduces the version space by half. Supposing that \(f_{t}\) misclassifies a true positive example \((x_{t},r_{t},+1)\) by negative, then we know that \(d(x_{t},f_{t})>r_{t}\) while the target hypothesis \(h^{*}\) must satisfy that \(d(x_{t},h^{*}) r_{t}\). Hence any \(h\) with \(d(x_{t},h) d(x_{t},f_{t})\) cannot be \(h^{*}\) and should be eliminated. Since \(d(x_{t},f_{t})\) is the median of \(\{d(x_{t},h)|h\}\), we can elimate half of the version space. It is similar when \(f_{t}\) misclassifies a true negative. The detailed proof is deferred to Appendix B.

PAC learningWe can convert Strategic Halving to a PAC learner by the standard technique of converting a mistake bound to a PAC bound (Gallant, 1986). Specifically, the learner runs Strategic Halving until it produces a hypothesis \(f_{t}\) that survives for \((|)}{})\) rounds and outputs this \(f_{t}\). Then we have Theorem 2, and the proof is included in Appendix C.

**Theorem 2**.: _For any feature-ball manipulation set space \(\) and hypothesis class \(\), we can achieve \(_{x,}(,)=(|)}{}(|)}{}))\) by combining Strategic Halving and the standard technique of converting a mistake bound to a PAC bound._

### Setting \((,(x,))\): Observing \(x_{t}\) After Choosing \(f_{t}\)

When \(x_{t}\) is not revealed before the learner choosing \(f_{t}\), the algorithm of Strategic Halving does not work anymore. We demonstrate that it is impossible to reduce constant fraction of version space when making a mistake, and prove that the mistake bound is lower bounded by \((||)\) by constructing a negative example of \((,)\). However, we can still achieve sample complexity with poly-logarithmic dependency on \(||\) in the distributional setting.

#### 4.2.1 Results in the Online Learning Model

To offer readers an intuitive understanding of the distinctions between the strategic setting and standard online learning, we commence by presenting an example in which no deterministic learners, including the Halving algorithm, can make fewer than \(||-1\) mistakes.

**Example 1**.: _Consider a star shape metric space \((,d)\), where \(=\{0,1,,n\}\), \(d(i,j)=2\) and \(d(0,i)=1\) for all \(i,j[n]\) with \(i j\). The hypothesis class is composed of singletons over \([n]\), i.e., \(=\{2_{\{i\}}-1|i[n]\}\). When the learner is deterministic, the environment can pick an agent \((x_{t},r_{t},y_{t})\) dependent on \(f_{t}\). If \(f_{t}\) is all-negative, then the environment picks \((x_{t},r_{t},y_{t})=(0,1,+1)\), and then the learner makes a mistake but no hypothesis can be eliminated. If \(f_{t}\) predicts \(0\) by positive, the environment will pick \((x_{t},r_{t},y_{t})=(0,0,-1)\), and then the learner makes a mistake but no hypothesis can be eliminated. If \(f_{t}\) predicts some \(i[n]\) by positive, the environment will pick \((x_{t},r_{t},y_{t})=(i,0,-1)\), and then the learner makes a mistake with only one hypothesis \(2_{\{i\}}-1\) eliminated. Therefore, the learner will make \(n-1\) mistakes._In this work, we allow the learner to be randomized. When an \((x_{t},r_{t},y_{t})\) is generated by the environment, the learner can randomly pick an \(f_{t}\), and the environment does not know the realization of \(f_{t}\) but knows the distribution where \(f_{t}\) comes from. It turns out that randomization does not help much. We prove that there exists an example in which any (possibly randomized) learner will incur \((||)\) mistakes.

**Theorem 3**.: _There exists a feature-ball manipulation set space \(\) and hypothesis class \(\) s.t. the mistake bound \(_{,(x,)}||-1\). For any (randomized) algorithm \(\) and any \(T\), there exists a realizable sequence of \((x_{t},r_{t},y_{t})_{1:T}\) such that with probability at least \(1-\) (over randomness of \(\)), \(\) makes at least \((|(||/)},||-1)\) mistakes._

Essentially, we design an adversarial environment such that the learner has a probability of \(|}\) of making a mistake at each round before identifying the target function \(h^{*}\). The learner only gains information about the target function when a mistake is made. The detailed proof is deferred to Appendix D. Theorem 3 establishes a lower bound on the mistake bound, which is \(||-1\). However, achieving this bound requires a sufficiently large number of rounds, specifically \(T=(||^{2})\). This raises the question of whether there exists a learning algorithm that can make \(o(T)\) mistakes for any \(T||^{2}\). In Example 1, we observed that the adversary can force any deterministic learner to make \(||-1\) mistakes in \(||-1\) rounds. Consequently, no deterministic algorithm can achieve \(o(T)\) mistakes.

To address this, we propose a randomized algorithm that closely resembles Algorithm 1, with a modification in the selection of \(f_{t}\). Instead of using line 3, we choose \(f_{t}\) randomly from VS since we lack prior knowledge of \(x_{t}\). This algorithm can be viewed as a variation of the well-known multiplicative weights method, applied exclusively during mistake rounds. For improved clarity, we present this algorithm as Algorithm 3 in Appendix E due to space limitations.

**Theorem 4**.: _For any \(T\), Algorithm 3 will make at most \((|)T},||-1)\) mistakes in expectation in \(T\) rounds._

Note that the \(T\)-dependent upper bound in Theorem 4 matches the lower bound in Theorem 3 up to a logarithmic factor when \(T=||^{2}\). This implies that approximately \(||^{2}\) rounds are needed to achieve \(||-1\) mistakes, which is a tight bound up to a logarithmic factor. Proof of Theorem 4 is included in Appendix E.

#### 4.2.2 Results in the PAC Learning Model

In the PAC setting, the goal of the learner is to output a predictor \(f_{}\) after the repeated interactions. A common class of learning algorithms, which outputs a hypothesis \(f_{}\), is called proper. Proper learning algorithms are a common starting point when designing algorithms for new learning problems due to their natural appeal and ability to achieve good performance, such as ERM in classic PAC learning. However, in the current setting, we show that proper learning algorithms do not work well and require a sample size linear in \(||\). The formal theorem is stated as follows and the proof is deferred to Appendix F.

**Theorem 5**.: _There exists a feature-ball manipulation set space \(\) and hypothesis class \(\) s.t. \(^{}_{,}(,)= (|}{})\), where \(^{}_{,}(,)\) is the \((,)\)-PAC sample complexity achievable by proper algorithms._

Theorem 5 implies that any algorithm capable of achieving sample complexity sub-linear in \(||\) must be improper. As a result, we are inspired to devise an improper learning algorithm. Before presenting the algorithm, we introduce some notations. For two hypotheses \(h_{1},h_{2}\), let \(h_{1} h_{2}\) denote the union of them, i.e., \((h_{1} h_{2})(x)=+1\) iff. \(h_{1}(x)=+1\) or \(h_{2}(x)=+1\). Similarly, we can define the union of more than two hypotheses. Then for any union of \(k\) hypotheses, \(f=_{i=1}^{k}h_{i}\), the positive region of \(f\) is the union of positive regions of the \(k\) hypotheses and thus, we have \(d(x,f)=_{i[k]}d(x,h_{i})\). Therefore, we can decrease the distance between \(f\) and any feature vector \(x\) by increasing \(k\). Based on this, we devise a new randomized algorithm with improper output, described in Algorithm 2.

**Theorem 6**.: _For any feature-ball manipulation set space \(\) and hypothesis class \(\), we can achieve \(_{,(x,)}(,)=( (||)+(1/)}{}())\) by combining Algorithm 2 with a standard confidence boosting technique. Note that the algorithm is improper._Now we outline the high-level ideas behind Algorithm 2. In correct rounds where \(f_{t}\) makes no mistake, the predictions of all hypotheses are either correct or unknown, and thus, it is hard to determine how to make updates. In mistake rounds, we can always update the version space similar to what was done in Strategic Halving. To achieve a poly-logarithmic dependency on \(||\), we aim to reduce a significant number of misclassifying hypotheses in mistake rounds. The maximum number we can hope to reduce is a constant fraction of the misclassifying hypotheses. We achieve this by randomly sampling a \(f_{t}\) (lines 3-5) s.t. \(f_{t}\) makes a mistake, and \(d(x_{t},f_{t})\) is greater (smaller) than the median of \(d(x_{t},h)\) for all misclassifying hypotheses \(h\) for true negative (positive) examples. However, due to the asymmetric nature of manipulation, which aims to be predicted as positive, the rate of decreasing misclassifications over true positives is slower than over true negatives. To compensate for this asymmetry, we output a \(f_{}=h_{1} h_{2}\) with two selected hypotheses \(h_{1},h_{2}\) (lines 10-11) instead of a single one to increase the chance of positive prediction.

We prove that Algorithm 2 can achieve small strategic loss in expectation as described in Lemma 1. Then we can achieve the sample complexity in Theorem 6 by boosting Algorithm 2 to a strong learner. This is accomplished by running Algorithm 2 multiple times until we obtain a good predictor. The proofs of Lemma 1 and Theorem 6 are deferred to Appendix G.

**Lemma 1**.: _Let \(S=(x_{t},r_{t},y_{t})_{t=1}^{T}^{T}\) denote the i.i.d. sampled agents in \(T\) rounds and let \((S)\) denote the output of Algorithm 2 interacting with \(S\). For any feature-ball manipulation set space \(\) and hypothesis class \(\), when \(T(||)}{}\), we have \(_{,S}[^{}((S)) ]\)._

```
1:Initialize the version space \(_{0}=\).
2:for\(t=1,,T\)do
3: randomly pick \(k_{t}(\{1,2,2^{2},,2^{_{2}(n_{t})-1}\})\) where \(n_{t}=|_{t-1}|\);
4: sample \(k_{t}\) hypotheses \(h_{1},,h_{k_{t}}\) independently and uniformly at random from \(_{t-1}\);
5: let \(f_{t}=_{i=1}^{k_{t}}h_{i}\).
6:if\(_{t} y_{t}\) and \(y_{t}=+\)then\(_{t}=_{t-1}\{h_{t-1}|d(x_{t},h)  d(x_{t},f_{t})\}\);
7:elseif\(_{t} y_{t}\) and \(y_{t}=-\)then\(_{t}=_{t-1}\{h_{t-1}|d(x_{t},h)  d(x_{t},f_{t})\}\);
8:else\(_{t}=_{t-1}\).
9:endfor
10: randomly pick \(\) from \([T]\) and randomly sample \(h_{1},h_{2}\) from \(_{-1}\) with replacement.
11:output\(h_{1} h_{2}\)
```

**Algorithm 2**

### Settings \((,)\) and \((,)\)

Online learningAs mentioned in Section 2, both the settings of \((,)\) and \((,)\) are harder than the setting of \((,(x,))\), all lower bounds in the setting of \((,(x,))\) also hold in the former two settings. Therefore, by Theorem 3, we have \(_{,}_{,}_{,(x, )}=||-1\).

PAC learningIn the setting of \((,)\), Algorithm 2 is not applicable anymore since the learner lacks observation of \(x_{t}\), making it impossible to replicate the version space update steps in lines 6-7. It is worth noting that both PAC learning algorithms we have discussed so far fall under a general category called conservative algorithms, depend only on information from the mistake rounds. Specifically, an algorithm is said to be conservative if for any \(t\), the predictor \(f_{t}\) only depends on the history of mistake rounds up to \(t\), i.e., \(<t\) with \(_{} y_{}\), and the output \(f_{}\) only depends on the history of mistake rounds, i.e., \((f_{t},_{t},y_{t},_{t})_{t:_{t} y_{t}}\). Any algorithm that goes beyond this category would need to utilize the information in correct rounds. As mentioned earlier, in correct rounds, the predictions of all hypotheses are either correct or unknown, which makes it challenging to determine how to make updates. For conservative algorithms, we present a lower bound on the sample complexity in the following theorem, which is \((|}{})\), and its proof is included in Appendix H. The optimal sample complexity in the setting \((,)\) is left as an open problem.

**Theorem 7**.: _There exists a feature-ball manipulation set space \(\) and hypothesis class \(\) s.t. \(^{}_{,}(,)=(|}{})\), where \(^{}_{,}(,)\) is \((,)\)-PAC the sample complexity achievable by conservative algorithms._In the setting of \((,)\), our problem reduces to a best arm identification problem in stochastic bandits. We prove a lower bound on the sample complexity of \((|}{})\) in Theorem 8 by reduction to stochastic linear bandits and applying the tools from information theory. The proof is deferred to Appendix I.

**Theorem 8**.: _There exists a feature-ball manipulation set space \(\) and hypothesis class \(\) s.t. \(_{,}(,)=(|}{})\)._

## 5 Non-ball Manipulations

In this section, we move on to non-ball manipulations. In ball manipulations, for any feature vector \(x\), we have an ordering of hypotheses according to their distances to \(x\), which helps to infer the predictions of some hypotheses without implementing them. However, in non-ball manipulations, we don't have such structure anymore. Therefore, even in the simplest setting of observing \(x_{t}\) before \(f_{t}\) and \(_{t}\), we have the PAC sample complexity lower bounded by \((|}{})\).

**Theorem 9**.: _There exists a feature-manipulation set space \(\) and hypothesis class \(\) s.t. \(_{x,}(,)=(|}{})\)._

The proof is deferred to Appendix J. It is worth noting that in the construction of the proof, we let all agents to have their original feature vector \(x_{t}=\) such that \(x_{t}\) does not provide any information. Since \((x,)\) is the simplest setting and any mistake bound can be converted to a PAC bound via standard techniques (see Section A.2 for more details), we have the following corollary.

**Corollary 1**.: _There exists a feature-manipulation set space \(\) and hypothesis class \(\) s.t. for all choices of \((C,F)\), \(_{C,F}(,)=(|}{})\) and \(_{C,F}=(||)\)._

## 6 Discussion and Open Problems

In this work, we investigate the mistake bound and sample complexity of strategic classification across multiple settings. Unlike prior work, we assume that the manipulation is personalized and unknown to the learner, which makes the strategic classification problem more challenging. In the case of ball manipulations, when the original feature vector \(x_{t}\) is revealed prior to choosing \(f_{t}\), the problem exhibits a similar level of difficulty as the non-strategic setting (see Table 1 for details). However, when the original feature vector \(x_{t}\) is not revealed beforehand, the problem becomes significantly more challenging. Specifically, any learner will experience a mistake bound that scales linearly with \(||\), and any proper learner will face sample complexity that also scales linearly with \(||\). In the case of non-ball manipulations, the situation worsens. Even in the simplest setting, where the original feature is observed before choosing \(f_{t}\) and the manipulated feature is observed afterward, any learner will encounter a linear mistake bound and sample complexity.

Besides the question of optimal sample complexity in the setting of \((,)\) as mentioned in Sec 4.3, there are some other fundamental open questions.

Combinatorial measureThroughout this work, our main focus is on analyzing the dependency on the size of the hypothesis class \(||\) without assuming any specific structure of \(\). Just as VC dimension provides tight characterization for PAC learnability and Littlestone dimension characterizes online learnability, we are curious if there exists a combinatorial measure that captures the essence of strategic classification in this context. In the proofs of the most lower bounds in this work, we consider hypothesis class to be singletons, in which both the VC dimension and Littlestone dimension are \(1\). Therefore, they cannot be candidates to characterize learnability in the strategic setting.

Agnostic settingWe primarily concentrate on the realizable setting in this work. However, investigating the sample complexity and regret bounds in the agnostic setting would be an interesting avenue for future research.