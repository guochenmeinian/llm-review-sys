# Gold-YOLO: Efficient Object Detector via

Gather-and-Distribute Mechanism

 Chengcheng Wang Wei He Ying Nie Jianyuan Guo Chuanjian Liu Kai Han Yunhe Wang

Huawei Noah's Ark Lab

{wangchengcheng11, hewei142,ying.nie,jianyuan.guo,liuchuanjian,

kai.han,yunhe.wang}@huawei.com

Corresponding Author.

###### Abstract

In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gather-and-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLO-series models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.

## 1 Introduction

Object detection as a fundamental vision task that aims to recognize the categories and locate the positions of objects. It can be widely used in a wide range of applications, such as intelligent security, autonomous driving, robot navigation, and medical diagnosis. High-performance and low-latency object detector is receiving increasing attention for deployment on the edge devices.

Over the past few years, researchers have extensive research on CNN-based detection networks, gradually evolving the object detection framework from two-stage (_e.g._, Faster RCNN  and Mask RCNN ) to one-stage (_e.g._, YOLO ), and from anchor-based (_e.g._, YOLOv3  and YOLOv4 ) to anchor-free (_e.g._, CenterNet , FCOS  and YOLOX ). [12; 7; 17] studied the optimal network structure through NAS for object detection task, and [16; 23; 19] explore another way to improve the performance of the model by distillation. Single-stage detection models, especially YOLO series models, have been widely welcomed in the industry due to their simple structure and balance between speed and accuracy.

Improvement of backbone is also an important research direction in the field of vision. As described in the survey , [26; 27; 60; 21; 62] has achieved a balance between precision and speed, while [9; 36; 22; 18] has shown strong performance in precision. These backbones have improved the performance of the original model in different visual tasks, ranging from high-level tasks like objectdetection to low-level tasks like image restoration. By using the encoder-decoder structure with the transformer, researchers have constructed a series of DETR-like object detection models, such as DETR  and DINO . These models can capture long-range dependency between objects, enabling transformer-based detectors to achieve comparable or superior performance with most refined classical detectors. Despite the notable performance of transformer-based detectors,they fall short when compared to the speed of CNN-based models. Small-scale object detection models based on CNN still dominate the speed-accuracy trade-off, such as YOLOX  and YOLOv6-v3 [33; 49; 14].

We focus on the real-time object detection models, especially YOLO series for mobile deployment. Mainstream real-time object detectors consist of three parts: backbone, neck, and head. The backbone architecture has been widely investigated [42; 44; 9; 36] and the head architecture is typically straight forward, consisting of several convolutional or fully-connected layers. The necks in YOLO series usually use Feature Pyramid Network (FPN) and its variants to fuse multi-level features. These neck modules basically follow the architecture shown in Fig. 3. However, the current approach to information fusion has a notable flaw: when there is a need to integrate information across layers (_e.g.,_ level-1 and level-3 are fused), the conventional FPN-like structure fails to transmit information without loss, which hinders YOLOs from better information fusion.

Built upon the concept of global information fusion, TopFormer  has achieved remarkable results in semantic segmentation tasks. In this paper, we expanding on the foundation of TopFormer's theory, propose a novel Gather-and-Distribute mechanism (GD) for efficient information exchanging in YOLOs by globally fusing multi-level features and injecting the global information into higher levels. This significantly enhances the information fusion capability of the neck without significantly increasing the latency, improving the model's performance across varying object sizes. Specifically, GD mechanism comprises two branches: a shallow gather-and-distribute branch and a deep gather-and-distribute branch, which extract and fuse feature information via a convolution-based block and an attention-based block, respectively. To further facilitate information flow, we introduce a lightweight adjacent-layer fusion module which combines features from neighboring levels on a local scale. Our Gold-YOLO architectures surpasses the existing YOLO series, effectively demonstrating the effectiveness of our proposed approach.

To further improve the accuracy of the model, we also introduce a pre-training method, where we pre-train the backbone on ImageNet 1K using the MAE method, which significantly improves the convergence speed and accuracy of the model. For example, our Gold-YOLO-S with pre-training achieves 46.4% AP, which outperforms the previous SOTA YOLOv6-3.0-S with 45.0% AP at similar speed.

## 2 Related works

### Real-time object detectors

After years of development, the YOLO-series model has become popular in the real-time object detection area. YOLOv1-v3 [40; 41; 42] constructs the initial YOLOs, identifies a single-stage

Figure 1: Comparison of state-of-the-art efficient object detectors in Tesla T4 GPU. Both latency and throughput (batch size of 32) are given for a handy reference. (a) and (b) test with TensorRT 7 and 8, respectively.

detection structure consisting of three parts, backbone-neck-head, predicts objects of different sizes through multi-scale branches, become a representative single-stage object detection model. YOLOv4  optimizes the previously used darknet backbone structure and propose a series of improvements, like the Mish activation function, PANet and data augmentation methods. YOLOv5  inheriting the YOLOv4  scheme with improved data augmentation strategy and a greater variety of model variants. YOLOX  incorporates Multi positives, Anchor-free, and Decoupled Head into the model structure, setting a new paradigm for YOLO-model design. YOLOv6 [33; 32] brings the reparameterization method to YOLO-series models for the first time, proposing EfficientRep Backbone and Rep-PAN Neck. YOLOv7  focuses on analyzing the effect of gradient paths on the model performance and proposes the E-ELAN structure to enhance the model capability without destroying the original gradient paths. The YOLOv8  takes the strengths of previous YOLO models and integrates them to achieve the SOTA of the current YOLO family.

### Transformer-base object detection

Vision Transformer (ViT) emerged as a competitive alternative to convolutional neural networks (CNNs) that are widely used for different image recognition tasks. DETR  applies the transformer structure to the object detection task, reconstructing the detection pipeline and eliminating many hand-designed parts and NMS components to simplify the model design and overall process. Combining the sparse sampling capability of deformable convolution with the global relationship modeling capability of transformer, Deformable DETR  improve convergence speed while improve model speed and accuracy. DINO  first time introduced Contrastive denoising, Mix query selection and a look forward twice scheme. The recent RT-DETR  improved the encoder-decoder structure to solve the slow DETR-like model problem, outperforming YOLO-L/X in both accuracy and speed. However, the limitations of the DETR-like structure prevent it from showing sufficient dominance in the small model region, where YOLOs remain the SOTA of accuracy and velocity balance.

### Multi-scale features for object detection

Traditionally, features at different levels carry positional information about objects of various sizes. Larger features encompass low-dimensional texture details and positions of smaller objects. In contrast, smaller features contain high-dimensional information and positions of larger objects. The original idea behind Feature Pyramid Networks (FPN) proposed by  is that these diverse pieces of information can enhance network performance through mutual assistance. FPN provides an efficient architectural design for fusing multi-scale features through cross-scale connections and information exchange, thereby boosting the detection accuracy of objects of varied sizes.

Based on FPN, the Path Aggregation Network (PANet)  incorporates a bottom-up path to make information fusion between different levels more adequate.Similarly, EfficientDet  presents a new repeatable module (BiFPN) to increase the efficiency of information fusion between different levels. M2Det  introduced an efficient MLFPN architecture with U-shape and Feature Fusion Modules. Ping-Yang Chen  improved interaction between deep and shallow layers using bidirectional fusion modules. Unlike these inter-layer works,  explored individual feature information using the Centralized Feature Pyramid (CFP) method. Additionally,  extended FPN with the Asymptotic Feature Pyramid Network (AFPN) to interact across non-adjacent layers. In response to FPN's limitations in detecting large objects,  proposed a refined FPN structure. YOLO-F  achieved

Figure 2: The architecture of the proposed Gold-YOLO.

state-of-the-art performance with single-level features. SFNet  aligns different level features with semantic flow to improves FPN performance in model. SFANet  introduced Adaptive Feature Fusion and Self-Enhanced Modules.  presented a parallel FPN structure for object detection with bi-directional fusion.However, due to the excessive number of paths and indirect interaction methods in the network, the previous FPN-based fusion structures still have drawbacks in low speed, cross-level information exchange and information loss.

However, due to the excessive number of paths and indirect interaction methods in the network, the previous FPN-based fusion structures still have drawbacks in low speed, cross-level information exchange and information loss.

## 3 Method

### Preliminaries

The YOLO series neck structure, as depicted in Fig.3, employs a traditional FPN structure, which comprises multiple branches for multi-scale feature fusion. However, it only fully fuse features from neighboring levels, for other layers information it can only be obtained indirectly '_recursively_'. In Fig.3, it shows the information fusion structure of the conventional FPN: where existing level-1, 2, and 3 are arranged from top to bottom. FPN is used for fusion between different levels. There are two distinct scenarios when level-1 get information from the other two levels:

1. If level-1 seeks to utilize information from level-2, it can directly access and fuse this information.
2. If level-1 wants to use level-3 information, level-1 should recursively calling the information fusion module of the adjacent layer. Specifically, the level-2 and level-3 information must be fused first, then level-1 can indirectly obtain level-3 information by combining level-2 information.

This transfer mode can result in a significant loss of information during calculation. Information interactions between layers can only exchange information that is selected by intermediate layers, and not selected information is discarded during transmission. This leads to a situation where information at a certain level can only adequately assist neighboring layers and weaken the assistance provided to other global layers. As a result, the overall effectiveness of the information fusion may be limited.

To avoid information loss in the transmission process of traditional FPN structures, we abandon the original recursive approach and construct a novel gather-and-distribute mechanism (GD). By using a unified module to gather and fuse information from all levels and subsequently distribute it to different levels, we not only avoid the loss of information inherent in the traditional FPN structure but also enhance the neck's partial information fusion capabilities without significantly increasing latency. Our approach thus allows for more effective leveraging of the features extracted by the backbone, and can be easily integrated into any existing backbone-neck-head structure.

Figure 3: (a) is example diagram of traditional neck information fusion structure. (b) and (c) is AblationCAM  visualization

In our implementation, the process _gather_ and _distribute_ correspond to three modules: Feature Alignment Module (FAM), Information Fusion Module (IFM), and Information Injection Module (Inject).

* The _gather_ process involves two steps. Firstly, the FAM collects and aligns features from various levels. Secondly, IFM fuses the aligned features to generate global information.
* Upon obtaining the fused global information from the _gather_ process, the inject module _distribute_ this information across each level and injects it using simple attention operations, subsequently enhancing the branch's detection capability.

To enhance the model's ability to detect objects of varying sizes, we developed two branches: low-stage gather-and-distribute branch (Low-GD) and high-stage gather-and-distribute branch (High-GD). These branches extract and fuse large and small size feature maps, respectively. Further details are provided in Sections 4.1 and 4.2. As shown in Fig. 2, the neck's input comprises the feature maps \(B2,B3,B4,B5\) extracted by the backbone, where \(B_{i}^{N C_{Bi} R_{Bi}}\). The batch size is denoted by \(N\), the channels by \(C\), and the dimensions by \(R=H W\). Moreover, the dimensions of \(R_{B2},R_{B3},R_{B4}\), and \(R_{B5}\) are \(R\), \(R\), \(R\), and \(R\), respectively.

### Low-stage gather-and-distribute branch

In this branch, the output \(B2,B3,B4,B5\) features from the backbone are selected for fusion to obtain high resolution features that retain small target information. The structure show in Fig.4(a)

Low-stage feature alignment module.In low-stage feature alignment module (Low-FAM), we employ the average pooling (AvgPool) operation to down-sample input features and achieve a unified size. By resizing the features to the smallest feature size of the group (\(R_{B4}=R\)), we obtain \(F_{align}\). The Low-FAM technique ensures efficient aggregation of information while minimizing the computational complexity for subsequent processing through the transformer module.

The target alignment size is chosen based on two conflicting considerations: (1) To retain more low-level information, larger feature sizes are preferable; however, (2) as the feature size increases, the computational latency of subsequent blocks also increases. To control the latency in the neck part, it is necessary to maintain a smaller feature size.

Therefore, we choose the \(R_{B4}\) as the target size of feature alignment to achieve a balance between speed and accuracy.

Low-stage information fusion module.The low-stage information fusion module (Low-IFM) design comprises multi-layer reparameterized convolutional blocks (RepBlock) and a split operation. Specifically, RepBlock takes \(F_{align}\) (channel = \(sum(C_{B2},C_{B3},C_{B4},C_{B5})\)) as input and produces \(F_{fuse}\) (channel = \(C_{B4}+C_{B5}\)). The middle channel is an adjustable value (_e.g._, 256) to accommodate varying model sizes. The features generated by the RepBlock are subsequently split in the channel dimension into \(F_{inj\_P3}\) and \(F_{inj\_P4}\), which are then fused with the different level's feature.

The formula is as follows:

\[F_{}=Low\_FAM([B2,B3,B4,B5]),\] (1) \[F_{}=RepBlock(F_{}),\] (2) \[F_{},F_{}=Split(F_{}).\] (3)

Information injection module.In order to inject global information more efficiently into the different levels, we draw inspiration from the segmentation experience  and employ attention operations to fuse the information, as illustrated in Fig. 5. Specifically, we input both local information (which refers to the feature of the current level) and global inject information (generated by IFM), denoted as \(F_{local}\) and \(F_{inj}\), respectively. We use two different Convs with \(F_{inj}\) for calculation, resulting in \(F_{global\_embed}\) and \(F_{act}\). While \(F_{local\_embed}\) is calculated with \(F_{local}\) using Conv. The fused feature \(F_{out}\) is then computed through attention. Due to the size differences between \(F_{local}\) and \(F_{global}\), we employ average pooling or bilinear interpolation to scale \(F_{global\_embed}\) and \(F_{act}\) according to the size of \(F_{inj}\), ensuring proper alignment. At the end of each attention fusion, we add the RepBlock to further extract and fuse the information.

In low stage, \(F_{local}\) is equal to \(Bi\), so the formula is as follows:

\[F_{}=resize(Sigmoid(Conv_{}(F_{ }))),\] (4) \[F_{}=resize(Conv_{ }(F_{})),\] (5) \[F_{}=Conv_{}(Bi)*F_{ }+F_{},\] (6) \[Pi=RepBlock(F_{}).\] (7)

### High-stage gather-and-distribute branch

The High-GD fuses the features \(\{P3,P4,P5\}\) that are generated by the Low-GD, as shown in Fig.4(b)

High-stage feature alignment module.The high-stage feature alignment module (High-FAM) consists of \(avgpool\), which is utilized to reduce the dimension of input features to a uniform size. Specifically, when the size of the input feature is \(\{R_{P3},R_{P4},R_{P5}\}\), \(avgpool\) reduces the feature size to the smallest size within the group of features (\(R_{P5}=R\)). Since the transformer module extracts high-level information, the pooling operation facilitates information aggregation while decreasing the computational requirements for the subsequent step in the Transformer module.

High-stage information fusion module.The high-stage information fusion module (High-IFM) comprises the transformer block (explained in greater detail below) and a splitting operation, which involves a three-step process: (1) the \(F_{align}\), derived from the High-FAM, are combined using the transformer block to obtain the \(F_{fuse}\). (2) The \(F_{fuse}\) channel is reduced to \(sum(C_{P4},C_{P5})\) via a \(Conv1 1\) operation. (3) The \(F_{fuse}\) is partitioned into \(F_{inj\_N4}\) and \(F_{inj\_N5}\) along the channel dimension through a splitting operation, which is subsequently employed for fusion with the current level feature.

The formula is as follows:

\[F_{}=High\_FAM([P3,P4,P5]),\] (8) \[F_{}=Transformer(F_{}),\] (9) \[F_{},F_{}=Split(Conv1 1(F_{ })).\] (10)

The transformer fusion module in Eq. 8 comprises several stacked transformers, with the number of transformer blocks denoted by \(L\). Each transformer block includes a multi-head attention block, a Feed-Forward Network (FFN), and residual connections. To configure the multi-head attention block, we adopt the same settings as LeViT , assigning head dimensions of keys \(K\) and queries \(Q\) to \(D\) (e.g., 16) channels, and \(V=2D\) (e.g., 32) channels. In order to accelerate inference, we substitute the velocity-unfriendly operator, Layer Normalization, with Batch Normalization for each convolution, and replace all GELU activations with ReLU. This minimizes the impact of the transformer module on the model's speed. To establish our Feed-Forward Network, we follow the methodologies presented in  for constructing the FFN block. To enhance the local connections of the transformer block, we introduce a depth-wise convolution layer between the two 1x1 convolution layers. We also set the expansion factor of the FFN to 2, aiming to balance speed and computational cost.

Figure 4: Gather-and-Distribute structure. In (a), the Low-FAM and Low-IFM is low-stage feature alignment module and low-stage information fusion module in low-stage branch, respectively. In (b), the High-FAM and High-IFM is high-stage feature alignment module and high-stage information fusion module, respectively.

Information injection module.The information injection module in High-GD is exactly the same as in Low-GD. In high stage, \(F_{local}\) is equal to \(Pi\), so the formula is as follows:

\[F_{}=resize(Sigmoid(Conv_{}(F_{ }))),\] (11) \[F_{}=resize(Conv_{ }(F_{})),\] (12) \[F_{}=Conv_{}(Pi)*F_{ }+F_{},\] (13) \[Ni=RepBlock(F_{}).\] (14)

### Enhanced cross-layer information flow

We have achieved better performance than existing methods using only a global information fusion structure. To further enhance the performance, we drew inspiration from the PAFPN module in YOLOv6  and introduced an Inject-LAF module. This module is an enhancement of the injection module and includes a lightweight adjacent layer fusion (LAF) module that is added to the input position of the injection module.

To achieve a balance between speed and accuracy, we designed two LAF models: LAF low-level model and LAF high-level model, which are respectively used for low-level injection (merging features from adjacent two layers) and high-level injection (merging features from adjacent one layer). There structure is shown in Fig. 5 (b).

To ensure that feature maps from different levels are aligned with the target size, the two LAF models in our implementation utilize only three operators: bilinear interpolation to up-sample features that are too small, average pooling to down-sample features that are too large, and 1x1 convolution to adjust features that differ from the target channel.

The combination of the LAF module with the information injection module in our model effectively balances the between accuracy and speed. By using simplified operations, we are able to increase the number of information flow paths between different levels, resulting in improved performance without significantly increased the latency.

### Masked image modeling pre-training

Recent methods such as BEiT , MAE  and SimMIM , have demonstrated the effectiveness of masked image modeling (MIM) for vision tasks. However, these methods are not specifically tailored for convolutional networks (convnets). Spark  and ConvNeXt-V2  are pioneers in exploring the potential of masked image modeling for convnets.

In this study, we adopt MIM Pre-training following the Spark's  methodology, which successfully identifies and overcomes two key obstacles in extending the success of MAE-style pretraining to

Figure 5: The information injection module and lightweight adjacent layer fusion(LAF) module

[MISSING_PAGE_FAIL:8]

the EfficientRep Backbone, while the head utilized the Efficient Decoupled Head. The optimizer learning schedule and other setting also same as YOLOv6, _i.e._ stochastic gradient descent (SGD) with momentum and cosine decay on learning rate. Warm-up, grouped weight decay strategy and the exponential moving average (EMA) are utilized. Self-distillation and anchor-aided training (AAT) also be used in training. The strong data augmentations we adopt Mosaic [2; 13] and Mixup .

We conducted MIM unsupervised pretraining on the backbone using the 1.28 million ImageNet-1K datasets . Following the experiment settings in Spark , we employed a LAMB optimizer  and cosine-annealing learning rate strategy, with a masking ratio of 60 % and a mask patch size of 32. For the Gold-YOLO-L models, we employed a batch size of 1024, while for the Gold-YOLO-M models, a batch size of 1152 was used. MIM pretraining was not employed for Gold-YOLO-N due to the limited capacity of its small backbone.

All our models are trained on 8 NVIDIA A100 GPUs, and the speed performance is measured on an NVIDIA Tesla T4 GPU with TensorRT.

### Comparisons

Our focus is primarily on evaluating the speed performance of our models after deployment. Specifically, we measure throughput (frames per second at a batch size of 1 or 32) and GPU latency, rather than FLOPs or the number of parameters. To compare our Gold-YOLO with other state-of-the-art detectors in the YOLO series, such as YOLOv5 , YOLOX , PPYOLOE , YOLOv7 , YOLOv8  and YOLOv6-3.0 , we test the speed performance of all official models with FP16-precision on the same Tesla T4 GPU with TensorRT.

Gold-YOLO-N demonstrates notable advancements, achieving an improvement of 2.6%/2.4%/6.6% compared to YOLOv8-N, YOLOv6-3.0-N, and YOLOv7-Tiny (input size=146), respectively, while offering comparable or superior performance in terms of throughput and latency. When compared to YOLOX-S and PPYOLOE-S, Gold-YOLO-S demonstrates a notable increase in AP by 5.9%/3.1%, while operating at a faster speed of 50/27 FPS (with a batch size of 32).

Gold-YOLO-M outperforms YOLOv6-3.0-M, YOLOX-M and PPYOLOE-M by achieving 1.1%, 4.2% and 2.1% higher AP with a comparable speed. Additionally, it achieves 5.7% and 0.9% higher AP than YOLOv5-M and YOLOv8-M, respectively, while achieving a higher speed. Gold-YOLO-M outperforms YOLOv7 with a significant improvement of 98FPS (batch size = 32), while maintaining the same AP. Gold-YOLO-L also achieves a higher accuracy compared to YOLOv8-L and YOLOv6-3.0-L, with a noticeable accuracy advantage of 0.4% and 0.5% respectively, while maintaining similar FPS at a batch size of 32.

### Ablation study

#### 4.3.1 Ablation study on GD structure

To verify the validity of our analysis concerning the FPN and to assess the efficacy of the proposed gather-and-distribute mechanism, we examined each module in GD independently, focusing on AP, number of parameters, and latency on the T4 GPU. The Low-GD predominantly targets small and medium-sized objects, whereas the High-GD primarily detect large-sized objects, and the LAF module bolsters both branches. The experimental results are displayed in Table 2.

#### 4.3.2 Ablation study on LAF

In this ablation study, we conducted experiments to compare the effects of different module designs within the LAF framework and evaluate the influence of varying model sizes on accuracy. The

  
**Low-GD** & **High-GD** & **LAF** & **AP** & **AP-small** & **AP-medium** & **AP-large** & **FPS** & **Params** & **FLOPs** \\  ✓ & & & 44.65\% & 25.13\% & 49.70\% & 60.36\% & 454.4 & 18.7 M & 45.1 G \\  & ✓ & & 42.27\% & 20.79\% & 47.74\% & 61.09\% & 493.7 & 20.9 M & 43.6 G \\  & ✓ & ✓ & 44.36\% & 25.04\% & 49.53\% & 60.64\% & 526.2 & 18.1 M & 43.3 G \\ ✓ & ✓ & & 45.57\% & 24.90\% & 50.38\% & 63.50\% & 461.8 & 21.5 M & 45.8 G \\ ✓ & ✓ & ✓ & 46.11\% & 25.22\% & 51.23\% & 63.42\% & 446.2 & 21.5 M & 46.0 G \\   

Table 2: Ablation study on GD structure. The test model is Gold-YOLO-S on T4 GPU evaluate.

[MISSING_PAGE_FAIL:10]