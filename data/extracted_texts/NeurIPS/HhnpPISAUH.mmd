# Heterogeneity-Guided Client Sampling: Towards Fast and Efficient Non-IID Federated Learning

Huancheng Chen

University of Texas at Austin

huanchengch@utexas.edu

&Haris Vikalo

University of Texas at Austin

hvikalo@ece.utexas.edu

###### Abstract

Statistical heterogeneity of data present at client devices in a federated learning (FL) system renders the training of a global model in such systems difficult. Particularly challenging are the settings where due to communication resource constraints only a small fraction of clients can participate in any given round of FL. Recent approaches to training a global model in FL systems with non-IID data have focused on developing client selection methods that aim to sample clients with more informative updates of the model. However, existing client selection techniques either introduce significant computation overhead or perform well only in the scenarios where clients have data with similar heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning via Hierarchical Clustered Sampling), a novel client selection method in which the server estimates statistical heterogeneity of a client's data using the client's update of the network's output layer and relies on this information to cluster and sample the clients. We analyze the ability of the proposed techniques to compare heterogeneity of different datasets, and characterize convergence of the training process that deploys the introduced client selection method. Extensive experimental results demonstrate that in non-IID settings HiCS-FL achieves faster convergence than state-of-the-art FL client selection schemes. Notably, HiCS-FL drastically reduces computation cost compared to existing selection schemes and is adaptable to different heterogeneity scenarios.

## 1 Introduction

The federated learning (FL) framework enables privacy-preserving collaborative training of machine learning (ML) models across a number of devices (clients) by avoiding the need to collect private data stored at those devices. The participating clients typically experience both the system as well as statistical heterogeneity . The former describes settings where client devices have varying degree of computational resources, communication bandwidth and fault tolerance, while the latter refers to the fact that the data owned by the clients may be drawn from different distributions. In this paper, we focus on FL under statistical heterogeneity and leave studies of system heterogeneity to future work.

An early FL method, FedAvg , performs well in the settings where the devices train on independent and identically distributed (IID) data. However, compared to the IID scenario, training on non-IID data is detrimental to the convergence speed, variance and accuracy of the learned model. This has motivated numerous studies aiming to reduce the variance and improve convergence of FL on non-IID data .

On another note, constraints on communication resources and therefore on the number of clients that may participate in training additionally complicate implementation of FL schemes. It would be particularly unrealistic to require regular contributions to training from all the clients in a large-scale cross-device FL system. Instead, only a fraction of clients participate in any given training round; unfortunately, this further aggravates detrimental effects of statistical heterogeneity. Selecting informative clients in non-IID FL settings is an open problem that has received considerable attentionfrom the research community [8; 11; 12]. Since privacy concerns typically prohibit clients from sharing their local data label distributions, existing studies focus on estimating informativeness of a client's update by analyzing the update itself. This motivated a family of methods that rely on the norms of local updates to assign probabilities of sampling the clients [7; 23]. Aiming to enable efficient use of the available communication and computation resources, another set of methods groups clients with similar data distributions into clusters based on the similarity between clients' model updates [2; 11]. Across the board, the existing methods still struggle to deliver desired performance in an efficient manner and cannot distinguish clients with balanced data from the clients with imbalanced data.

In this paper, we consider training a neural network model for **classification tasks** via federated learning and propose a novel adaptive clustering-based sampling method for identifying and selecting informative clients. The method, referred to as Federated Learning via Hierarchical Clustered Sampling (HiCS-FL), relies on the updates of the (fully connected) output layer in the network to determine how diverse is the clients' data and, based on that, decide which clients to sample. In particular, HiCS-FL enables heterogeneity-guided client selection by utilizing general properties of the gradients of the output layer to distinguish between clients with balanced from those with imbalanced data. Unlike the Clustered Sampling strategies  where the clusters of clients are sampled uniformly, HiCS-FL allocates different probabilities (importance) to the clusters according to their average estimated data heterogeneity. Numerous experiments conducted on vision datasets FMNIST, CIFAR10, Mini-ImageNet and a NLP dataset THUC news demonstrate that HiCS-FL achieves significantly faster training convergence and lower variance than the competing methods. Finally, we conduct convergence analysis of HiCS-FL and discuss implications of the results.

In summary, the contributions of the paper include: (1) Analytical characterization of the correlation between local updates of the output layer and the FL clients' data label distribution, along with an efficient method for estimating data heterogeneity; (2) a novel clustering-based algorithm for heterogeneity-guided client selection; (3) extensive simulation results demonstrating HiCS-FL provides significant improvement in terms of convergence speed and variance over competing approaches; and (4) theoretical analysis of the proposed schemes.

## 2 Background and Related Work

Assume the cross-device federated learning setting with \(N\) clients, where client \(k\) owns private local dataset \(_{k}\) with \(|_{k}|\) samples. The plain vanilla FL considers the objective

\[_{}F()_{k=1}^{N}p_{k}F_{k}(),\] (1)

where \(\) denotes parameters of the global model, \(F_{k}()\) is the loss (empirical risk) of model \(\) on \(_{k}\), and \(p_{k}\) denotes the weight assigned to client \(k\), \(_{k=1}^{N}p_{k}=1\). In FedAvg, the weights are set to \(p_{k}=|_{k}|/_{i=1}^{N}|_{i}|\). In training round \(t\), the server collects clients' model updates \(_{k}^{t}\) formed by training on local data and aggregates them to update global model as \(^{t+1}=_{k=1}^{N}p_{k}_{k}^{t}\).

When an FL system operates under resource constraints, typically only \(K N\) clients are selected to participate in any given round of training; denote the set of clients selected in round \(t\) by \(^{t}\). In departure from FedAvg, FedProx  proposes an alternative strategy for sampling clients based on a multinomial distribution where the probability of selecting a client is proportional to the size of its local dataset; the global model is then formed as the average of the collected local models \(^{t+1}=_{k^{t}}_{k}^{t}\). This sampling strategy is _unbiased_ since the the updated global model is on expectation equal to the one obtained by the framework with full client participation as Eq.1.

AFL  is the first study to utilize local validation loss as a _value_ function for computing client sampling probabilities; Power-of-Choice  takes a step further to propose a greedy approach to sampling clients with the largest local loss. Both of these methods require all clients to compute the local validation loss, which is often unrealistic. To address this problem, FedCor  models the local loss by a Gaussian Process (GP), estimates the GP parameters from experiments, and uses the GP model to predict clients' local losses without requiring them to perform validation. In , Optimal Client Sampling scheme aiming to minimize the variance of local updates by assigning sampling probabilities proportional to the Euclidean norm of the updates is proposed. The study in  models the progression of model's weights by an Ornstein-Uhlenbeck process and proposes a strategy, optimal under that assumption, for selecting clients with significant weight updates.

The clustering-based sampling method proposed in  uses cosine similarity  to group together clients with similar local updates, and proceeds to sample one client per cluster in attempt to avoid redundant gradient information. DivFL  follows the same principle of identifying representative clients but does so by constructing a submodular set and greedily selecting diverse clients. Both of these techniques are computationally expensive due to the high dimension of the gradients that they need to process.

In general, the overviewed methods either: (1) select diverse clients to reduce redundant information; or (2) select clients with a perceived significant contributions to the global model (high loss, large update or low class-imbalance). Efficient and effective client selection in FL remains an open challenge, motivating the heterogeneity-guided adaptive client selection method presented next.

## 3 HiCS-FL: Federated Learning via Hierarchical Clustered Sampling

Existing client sampling methods including Clustered Sampling  and DivFL  aim to select clients such that the resulting model update is an unbiased estimate of the true update (i.e., the update in the case of full client participation) while minimizing the variance

\[\|_{k=1}^{N} F_{k}(^{t})-_{k ^{t}} F_{k}(^{t})\|_{2}^{2}.\] (2)

Clustered Sampling, for instance, groups \(N\) clients into \(K\) clusters based on _representative gradients_, and randomly selects one client from each cluster to contribute to the global model update. Such an approach unfortunately fails to differentiate between model updates formed on data with balanced and those formed on data with imbalanced label distributions - indeed, in either case the updates are treated as being equally important. However, a number of studies in centralized learning has shown that class-imbalanced datasets have significant detrimental effect on the performance of learning classification tasks [3; 4; 26]. This intuition carries over to the FL settings where one expects the updates from clients training on relatively more balanced local data to have a more beneficial impact on the performance of the system. The Federated Learning via Hierarchical Clustered Sampling (HiCS-FL) framework described in this section adapts to the clients' data heterogeneity in the following way: if the levels of heterogeneity (as quantified by the entropy of data label distribution) vary from one cluster to another, HiCS-FL is more likely to sample clusters containing clients with more balanced data; if the clients grouped in different clusters have similar heterogeneity levels, HiCS-FL is more likely to select diverse clients (i.e., sample uniformly across clusters, thus reducing to the conventional clustered sampling strategy).

### Class-imbalance Causes Objective Drift

A number of studies explored detrimental effects of non-IID training data on the performance of a global model learned via FedAvg. An example is SCAFFOLD  which demonstrates _objective drift_ in non-IID FL manifested through large differences between local models \(_{k}^{*}\) trained on substantially different data distributions. The drift is due to FedAvg updating the global model in the direction of the weighted average of local optimal models, which is not necessarily leading towards the optimal global model \(^{*}\). The optimal model \(^{*}\), in principle obtained by solving optimization in Eq. 1, achieves minimal empirical error on the data with uniform label distribution and is intuitively closer to the local optimal models trained on balanced data. Recent work  empirically verified this conjecture through extensive experiments. Let \( F(^{t})\) denote the gradient of \(F(^{t})\) given the global model \(^{t}\) at round \(t\); the difference between \( F(^{t})\) and the local gradient \( F_{k}(^{t})\) computed on client \(k\)'s data is typically assumed to be bounded [7; 11; 31]. To proceed, we formalize the assumption about the relationship between gradients and data label distributions.

**Assumption 3.1** (Bounded Dissimilarity.): _Gradient \( F_{k}(^{t})\) of the \(k\)-th local model at global round \(t\) is such that_

\[\| F_{k}(^{t})- F(^{t})\|^{2}-  e^{(H(^{(k)})-H(_{0}))}=_{k}^ {2},\] (3)

_where \(^{(k)}\) is the data label distribution of client \(k\), \(_{0}\) denotes uniform distribution, \(H()\) is Shannon's entropy of a stochastic vector, and \(>0,>>0\)._

Figure 1: The last two network layers.

The assumption commonly encountered in literature is recovered by setting the right-hand side of (3) to \(_{m}^{2}=_{k}_{k}^{2}\). Intuitively, if the data label distribution of client \(k\) is highly imbalanced (i.e., \(H(^{(k)})\) is small), the local gradient \( F_{k}(^{t})\) may significantly differ from the global gradient \( F(^{t})\) (as reflected by the bound above). Analytically, connecting the gradients to the local data label distributions allows one to characterize the effects of client selection on the variance and the rate of convergence. The results of extensive experiments that empirically verify the above assumption are reported in Appendix A.2.

### Estimating Client's Data Heterogeneity

If the server were given access to clients' data label distributions, selecting clients would be relatively straightforward . However, privacy concerns typically discourage clients from sharing such information. Previous studies have explored the use of multi-arm bandits for inferring clients' data heterogeneity from local model parameters, or have utilized a validation dataset at the server to accomplish the same [27; 34; 36]. In this section, we demonstrate how to efficiently and accurately estimate data heterogeneity using local updates of the output layer of a neural network in a classification task. Figure 1 illustrates the last two layers in a typical neural network. The prediction \(^{C}\) is computed by forming a weighted average of signals \(^{L}\) utilizing the weight matrix \(^{C L}\) and bias \(^{C}\).

#### 3.2.1 Local updates of the output layer

An empirical investigation of the gradients of the output layer's weights while training with FedAvg using mini-batch stochastic gradient descent (SGD) as an optimizer is reported in [5; 29]. There, the focus is on detecting the presence of specific labels in a batch rather than on exploring the effects of class imbalance on the local update. To pursue the latter, we focus on the correlation between local updates of the output layer's bias and the client's data label distribution; we start by analyzing the training via FedAvg that employs SGD and then extend the results to other FL algorithms that utilize optimizers beyond SGD. We assume that the model is trained by minimizing the cross-entropy (CE) loss over one-hot labels - a widely used multi-class classification framework. The gradient is computed by averaging contributions of the samples in mini-batches, i.e., \(_{}_{}=_{j=1}^{l}_{ n=1}^{B}_{}_{}^{(j,n)}(^{(j,n)},y ^{(j,n)})\), where \(B\) denotes the batch size, \(l\) is the number of mini-batches, \(^{(j,n)}\) is the \(n\)-th point in the \(j\)-th mini-batch and \(y^{(j,n)}[C]\) is its label. The contribution of \(^{(j,n)}\) to the \(i\)-th component of the gradient of the output layer's bias \(\) can be found as (details provided in Appendix A.3)

\[_{b_{i}}_{}^{(j,n)}(^{(j,n)},y^{(j,n)} )=\{i=y^{(j,n)}\}(q_{c}^{(j,n)})}{_{c= 1}^{C}(q_{c}^{(j,n)})}+\{i y^{(j,n)}\}^{(j,n )})}{_{c=1}^{C}(q_{c}^{(j,n)})},\] (4)

where \(\{\}\) is an indicator, \(^{(j,n)}=^{(j,n)}+\) is the output logit for signals \(^{(j,n)}^{L}\) corresponding to training point \((^{(j,n)},y^{(j,n)})\) (see Fig. 1), and where \(C\) denotes the number of classes. We make the following observations: (1) the sign of \(y^{(j,n)}\)-th component of \(_{}_{}^{(j,n)}\) is opposite of the sign of other components; and (2) the \(y^{(j,n)}\)-th component of \(_{}_{}^{(j,n)}\) is equal in magnitude to all the other components combined. Note that the above two observations are standard for neural networks using CE loss for supervised multi-class classification tasks.

In each global round \(t\) of FedAvg, the selected client \(k\) starts from the global model \(^{t}\) and proceeds to compute local update in \(R\) local epochs employing an SGD optimizer with learning rate \(\). According to Eq. 4, the \(i\)-th component of local update \(^{(k)}\) is computed as

\[ b_{i}^{(k)}=-_{j=1}^{l}_{n=1}^{B}_{r=1}^{R} _{b_{i}}_{}^{(j,n,r)},\] (5)

where \(_{b_{i}}_{}^{(j,n,r)}\) denotes the gradient of bias at local epoch \(r\). Note that the local update of client \(k\), \(^{(k)}\), is dependent on the label distribution of client \(k\)'s data, \(^{(k)}=[D_{1}^{(k)},,D_{C}^{(k)}]^{T}\) and the label-specific components of \(^{(j,n)}\) which change during training. We proceed by relating expected local updates to the label distributions; for convenience, we first introduce the following definition.

**Definition 3.2**: _Let \(^{-i}\) be the subset of local data \(\) that excludes points with label \(i\). Let \(^{-i}()^{C}\) be the softmax output of a trained neural network for a training point \((,y)^{-i}\). The \(i\)-th component of \(^{-i}()\), \(_{i}^{-i}()\), indicates the level of confidence in (erroneously) classifying \(\) as having label \(i\). For convenience, we define \(_{i}=_{(,y)^{-i}}[_{i}^{-i}()], i[C]\)._

In an untrained/initialized neural network where classifier makes random predictions, \(_{i}=1/C\); as training proceeds, \(_{i}\) decreases. By taking expectation and simplifying, we obtain (details provided in Appendix A.4)

\[[ b_{i}^{(k)}]= R(D_{i}^{(k)}_{c=1}^{ C}_{c}-_{i}),\] (6)

where \(D_{i}^{(k)}\) denotes the true fraction of samples with label \(i\) in client \(k\)'s data, \(_{i=1}^{C}D_{i}^{(k)}=1\).

#### 3.2.2 Estimating local data heterogeneity

We quantify the heterogeneity of clients' data by an entropy-like measure defined below. Let \(^{(k)}\) denote the label distribution of client \(k\)'s data; its entropy is defined as \(H(^{(k)})-_{i=1}^{C}D_{i}^{(k)} D_{i}^{(k)}  C\). Recall that more balanced data results in higher entropy, and that \(H(^{(k)})\) takes the maximal value when \(^{(k)}\) is uniform. The server does not know \(^{(k)}\) and therefore cannot compute \(H(^{(k)})\) directly. We define

\[(^{(k)}) H((^{(k)}, T)),\] (7)

here \(T\) is a scaling hyper-parameter (so-called _temperature_). Note that even though we can compute \((^{(k)})\) to characterize heterogeneity, \(D_{i}^{(k)}\) and \(_{i}\) remain unknown to the server (details in A.5).

**Theorem 3.3**: _Consider an FL system in which clients collaboratively train a model for a classification task over \(C\) classes. Let \(^{(u)}\) and \(^{(k)}\) denote data label distributions of an arbitrary pair of clients \(u\) and \(k\), respectively. Moreover, let \(\) denote the uniform distribution, and let \(\) and \(R\) be the learning rate and the number of local epochs, respectively. Then_

\[[(^{(u)})-(^{(k)})] (_{c=1}^{C}_{c})^{2 }\|^{(k)}-\|_{2}^{2}-\| ^{(u)}-\|_{}-,\] (8)

_where \(=T C)}{C^{2}T^{2}}\) and \(=_{i}|^{C}_{c}}{C}-_{i}|\). The proof is provided in Appendix A.6._

As an illustration, consider the scenario where client \(u\) has a balanced dataset while the dataset of client \(k\) is imbalanced; then \(\|^{(k)}-\|_{2}^{2}\) is relatively large compared to \(\|^{(u)}-\|_{}\). The bound in (8) also depends on \(\), which is reflective of how misleading on average can a class be; small \(\) suggests that no class is universally misleading. As shown in Appendix A.4, during training \(\) gradually decreases to \(0\) as \(_{i=1}^{C}_{i}\) decreases to \(0\).

#### 3.2.3 Generalizing beyond FedAvg and SGD

The proposed method for estimating clients' data heterogeneity relies on the properties of the gradient for the cross-entropy loss objective discussed in Section 3.2.1. However, for FL algorithms other than FedAvg, such as FedProx , FedDyn  and Moon , which add regularization to combat overfitting, the aforementioned properties may not hold. Moreover, optimization algorithms using second-order momentum such as Adam  deploy update rules different from SGD, making the local updates no longer proportional to the gradients. Nevertheless, HiCS-FL remains capable of distinguishing between clients with imbalanced and balanced data, which will be demonstrated in our experiments. Further theoretical discussion of various FL algorithms with optimizers beyond SGD are in appendix A.8 and A.9.

### Heterogeneity-guided Clustering

Clustered Sampling  uses cosine similarity  between gradients to quantify proximity between clients' data distributions and subsequently group them into clusters. However, cosine similarity

[MISSING_PAGE_FAIL:6]

**Theorem 3.4**: _Assume \(F_{k}()\) is \(L\)-smooth for all \(k[N]\). Let \(^{t}\) denote parameters of the global model and let \(F()\) be defined as in Eq. 1. Furthermore, assume the stochastic gradient estimator \(g_{k}(^{t})\) is unbiased and the variance is bounded such that \(\|g_{k}(^{t})- F_{k}(^{t})\|^{2} ^{2}\). Let \(\) and \(R\) be the learning rate and the number of local epochs, respectively. If the learning rate is such that \(,R 2\), then_

\[_{t[]}\| F(^{t})\|^{2}}()-F(^{*})}{_{1}}+ _{2}_{t=0}^{-1}_{k=1}^{N}_{k}^{t}_{ k}^{2})+,\] (11)

_where \(_{1}\), \(_{2}\), \(\) are positive constants, and \(_{k}^{t}\) is the probability of sampling client \(k\) at round \(t\)._

Note that only the second term in the parenthesis on the right-hand side of the bound in Theorem 3.4 is related to the sampling method \(\). Under Assumption 3.1,

\[_{k=1}^{N}_{k}^{t}_{k}^{2}-_{k=1}^{N}_{k} ^{t}^{(k)}))}{( H( _{0}))}=-_{}.\] (12)

If the server samples clients with weights proportional to \(p_{k}\), the statistical heterogeneity of the entire FL system may be characterized by \(_{}=_{k=1}^{N}p_{k}^{( k)}))}{((H(_{0})))}\). If all clients have class-imbalanced data, \(_{}\) is small and thus random sampling leads to unsatisfactory convergence rate (as indicated by Theorem 3.4). On the other hand, since the clients sharing a cluster have similar data entropy, the proposed HiCS-FL leads to \(_{k}^{t}=(^{t}^{t}(^{(k)}))}{ _{j=1}^{N}p_{j}(^{t}^{t}(^{(j)}))}\). When training starts, \(_{}\) is large because the server tends to sample clients with higher \(p_{k}(^{t}H(^{(k)}))\); as \(^{t}\) decreases, \(_{}\) eventually approaches \(_{}\). Further details and the proof of the theorem are in Appendix A.7.

## 4 Experiments

**Setup.** We evaluate the proposed HiCS-FL algorithm on four benchmark datasets (FMNIST, CIFAR10, Mini-ImageNet and THUC news) using different model architectures. We use four baselines: random sampling, pow-d , clustered sampling (CS) , DivFL  and FedCor . To generate non-IID data partitions, we follow the strategy in , utilizing Dirichlet distribution with different concentration parameters \(\) which controls the level of heterogeneity (smaller \(\) leads to generating less balanced data). In a departure from previous works we utilize several different \(\) to generate data partitions for a single experiment, leading to a realistic scenario of varied data heterogeneity across different clients. To quantify the performance of the tested methods, we use two metrics: (1) average training loss, and (2) test accuracy of the learned global model. For better visualization, data points in the results are smoothened by a Savitzky-Golay filter with window length \(13\) and the polynomial order set to \(3\). Further details of the experimental setting and a visualization of data partitions are in Appendix A.1 and A.10.

### Comparison on Test Accuracy and Training Loss

**FMNIST.** We run FedAvg with **SGD** to train a global model which has CNN architecture in an FL system with \(50\) clients, where \(10\%\) of clients are selected to participate in each round of training. The data partitions are generated using one of \(3\) sets of the concentration parameter \(\) values: (1) \(\{0.001,0.002,0.005,0.01,0.5\}\); (2) \(\{0.001,0.002,0.005,0.01,0.2\}\); (3) \(\{0.001\}\). These are used to generate clients' data so as to emulate the following scenarios: (1) \(80\%\) of clients have severely imbalanced data while the remaining \(20\%\) have balanced data; (2) \(80\%\) clients have severely imbalanced data while the remaining \(20\%\) have mildly imbalanced data; (3) all clients have severely imbalanced data. Note that \(_{}\) monotonically decreases as we go through settings (1) to (3). For a fair comparison, pow-d and DivFL are deployed with their ideal settings where the server requires all clients to precompute in each round a metric that is then used for client selection. Figure 2 shows that HiCS-FL outperforms other methods across different settings, exhibiting the fastest convergence rates and the least amount of variance. Particularly significant is the acceleration of convergence in setting (1) where \(20\%\) of the participating clients have balanced data. Figure 3 shows that HiCS-FL is helping achieve significant reduction of training variations (as expected, see Section 3.5) as evident by a smooth loss trajectory.

**CIFAR10**. Here we compare the performance of HiCS-FL to FedProx  running CNN model with **Adam** optimizer on the task of training an FL system with 50 clients, where \(20\%\) of clients are selected to participate in each training round. Similar to the experiments on FMNIST, \(3\) sets of the concentration parameter \(\) are considered: (1) \(\{0.001,0.01,0.1,0.5,1\}\); (2) \(\{0.001,0.002,0.005,\)\(0.01,0.5\}\); (3) \(\{0.001,0.002,0.005,\)\(0.01,0.1\}\). The interpretation of the scenarios emulated by these setting is as same as in the FMNIST experiments. Figure 2 demonstrates improvement of HiCS-FL over all the other methods. HiCS-FL exhibits particularly significant improvements in settings (2) and (3), where \(80\%\) of the clients with extremely imbalanced data benefit from \(20\%\) of the clients with either balanced or mildly imbalanced data. The advantage of HiCS-FL in setting (1) where all clients have relatively high data heterogeneity is relatively modest (see Fig.2.(d)) because the system's \(_{}\) is relatively large (see discussion in Section 3.5).

**Mini-ImageNet.** As in the Mini-ImageNet experiments, we compare HiCS-FL to FedProx running ResNet18 with **Adam** optimizer but now consider training of an FL system with 100 clients, where \(20\%\) of the clients are selected to participate in each round of training. We consider two settings of the concentration parameter \(\): (1) \(\{0.001,0.01,0.1,0.5,1\}\) and (2) \(\{0.001,0.005,0.01,0.1,1\}\). Setting (1) emulates the scenario where clients have a range of heterogeneity profiles, from extremely imbalanced, through mildly imbalanced, to balanced, while setting (2) corresponds to the scenario where \(80\%\) of the clients have extremely imbalanced data while the remaining \(20\%\) have balanced data. The system's \(_{}^{(1)}\) for setting (1) is larger than \(_{}^{(2)}\) for setting (2), which is reflected in a more significant improvements achieved by HiCS-FL in the latter setting, as shown in Figure 4.

**THUC news.** To evaluate our method on data from a different domain, we conduct experiments involving text classification on the THUC news dataset in Chinese language (10 labels). Similar to the aforementioned experiments, we allocate data to \(50\) clients by emulating heterogeneous data distributions scenarios with parameter \(\) set to: (1) \(\{0.001,\)\(0.01,\)\(0.1,\)\(0.2,1\}\); (2) \(\{0.001,0.002,0.01,0.1,0.5\}\); and (3) \(\{0.001,\)\(0.002,\)\(0.005,\)\(0.01,\)\(0.1\}\). We trained TextRNNs  with BiLSTM architecture as

Figure 3: Training loss of HiCS-FL compared to four baselines for setting (1) on the three datasets.

Figure 2: Test accuracy for the global model on 3 groups of data partitions of FMNIST and CIFAR10.

the classifiers using **Adam** optimizer. The test accuracy of the global model trained with different schemes for \(100\) global rounds, reported in Table 1, show that our method outperforms baselines in all the settings, demonstrating efficacy of our proposed algorithm in a simple NLP task.

### Accelerating the Training Convergence

In this section we report the communication costs required to achieve convergence when using HiCS-FL, and compare those results with the competing schemes. For brevity, we select one result from each experiment conducted on the considered four datasets, and display them in Table 2. As can be seen from the table, HiCS-FL significantly reduces the number of communication rounds needed to reach target test accuracy. On FMNIST, HiCS-FL needs \(60\) rounds to reach test accuracy \(0.75\), achieving it \(2.5\) times faster than the random sampling scheme. On CIFAR10, HiCS-FL requires only \(123\) rounds to reach \(0.6\) test accuracy, which is \(7.3\) times faster than random sampling. Significant speedup appears on THUC dataset, in which HiCS-FL only needs \(27\) rounds to achieve \(0.8\) test accuracy, \(3.1\) times faster than the baseline. Acceleration on Mini-ImageNet is relatively modest but HiCS-FL still outperforms other methods, and does so up to \(2.2\) times faster than random sampling.

Table 2 also shows that HiCS-FL provides the reported improvements without introducing major computational and communication overhead. The only additional computation is due to estimating data heterogeneity and performing clustering utilizing bias updates, which scales with the total number of classes but does not increase with the size of the neural network model \(|^{t}|\). Remarkably, HiCS-FL outperforms pow-d, Clustered Sampling, DivFL and FedCor in terms of convergence speed, variance and test accuracy while requiring significantly less computations. More details are provided in Appendix A.11.

### Number of Clustering Groups

As discussed at the end of Section 3.3, the distance function in Equation 9 can be reduced to the conventional cosine similarity when clients exhibit similar levels of statistical heterogeneity, despite potential differences in data distribution. Under these circumstances, our HiCS-FL method can recover the performance of the previously established CS approach . While CS suggests that the number of clusters \(M\) should be greater than or equal to the number of selected clients \(K\), our HiCS-FL does not require \(M>K\) but adheres to the CS settings to ensure a fair comparison. To elucidate the impact of the number of clusters, we conducted supplementary experiments with

  Schemes & Random & Pow-of-Choice & CS & DivFL & FedCor & **HiCS-FL** \\  setting (1) & 78.9 & 80.0 & 80.6 & 73.0 & 81.2 & **83.2** \\ setting (2) & 74.9 & 75.4 & 82.8 & 68.9 & 81.3 & **83.9** \\ setting (3) & 72.7 & 66.5 & 79.4 & 72.1 & 76.4 & **79.7** \\  

Table 1: Test accuracy (%) for the global model on 3 groups of data partitions of THUC news dataset.

    &  &  &  &  \\   & acc = 0.75 & speedup & acc = 0.6 & speedup & acc = 0.5 & speedup & acc = 0.8 & speedup \\  Random & 149 & 1.0\(\) & 898 & 1.0\(\) & 191 & 1.0\(\) & 83 & 1.0\(\) \\ pow-d & 79 & 1.8\(\) & 1037 & 0.9\(\) & 432 & 0.4\(\) & 109 & 0.8\(\) \\ CS & 114 & 1.3\(\) & 748 & 1.2\(\) & 186 & 1.0\(\) & 74 & 1.1\(\) \\ DivFL & 478 & 0.3\(\) & 1417 & 0.6\(\) & 726 & 0.3\(\) & 289 & 0.3\(\) \\ FedCor & 88 & 1.7\(\) & 711 & 1.3\(\) & 229 & 0.8\(\) & 100 & 0.8\(\) \\
**HiCS-FL** & **60** & **2.5\(\)** & **123** & **7.3\(\)** & **86** & **2.2\(\)** & **27** & **3.1\(\)** \\   

Table 2: The number of communication rounds needed to reach a certain test accuracy in the experiments on FMNIST, CIFAR10, Mini-ImageNet and THUC News. All results are for the concentration parameter setting (2).

Figure 4: MiniImageNet acc.

HiCS-FL using varying numbers of clusters \(M\) and compared these results to those obtained with \(M=K\) as presented in the paper. The results of those experiments can be found in Table. 3. As shown there, HiCS-FL can perform well with smaller \(M<K\) as long as \(M\) is not too small, such as \(M=3\).

### Dynamic Availability of Clients

The purpose of the _warm-up_ phase (\(t< N/K\)) shown in Alg. 1 is to collect updates of the output layer from all the available clients in the system in order to facilitate clustering. Although we conduct all the experiments in the setting where clients have fixed availability, our HiCS-FL does not assume all the clients are available in the warm-up phase and can be adapted to more practical scenarios where clients have dynamic availability.

In such a scenario, the warm-up phase can be implemented by the available clients at the beginning of training. The proposed HiCS-FL is then implemented only among the available clients; the available clients with more balanced data are preferred. When new clients join the system at the global round \(t\), the server can obtain the information of availability and selects these new clients at round \(t+1\) to approximate their data heterogeneity. To provide more insights, we conduct additional experiments on CIFAR10 dataset; the results are reported in Table. 4. As can be seen there, HiCS-FL outperforms baselines that consider clients' availability.

## 5 Conclusion

In this paper, we studied federated learning systems where clients that own non-IID data collaboratively train a global model; the system operates under communication constraints and thus only a fraction of clients participates in any given round of training. We developed HiCS-FL, a hierarchical clustered sampling method which estimates clients' data heterogeneity and uses this information to cluster and select clients to participate in training. We analyzed the performance of the proposed heterogeneity estimation method, and the convergence of training a FL system that deploys HiCS-FL. Extensive benchmarking experiments on four datasets demonstrated significant benefits of the proposed method, including improvement in convergence speed, variance and test accuracy, accomplished with only a minor computational overhead.