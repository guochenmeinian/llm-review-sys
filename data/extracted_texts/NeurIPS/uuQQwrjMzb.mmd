# Adaptive Labeling for Efficient Out-of-distribution Model Evaluation

Daksh Mittal, Yuanzhe Ma, Shalmali Joshi, Hongseok Namkoong

Columbia University

{dm3766, ym2865, sj3261, hn2369}@columbia.edu

Equal contribution

###### Abstract

Datasets often suffer severe selection bias; clinical labels are only available on patients for whom doctors ordered medical exams. To assess model performance outside the support of available data, we present a computational framework for adaptive labeling, providing cost-efficient model evaluations under severe distribution shifts. We formulate the problem as a Markov Decision Process over states defined by posterior beliefs on model performance. Each batch of new labels incurs a "state transition" to sharper beliefs, and we choose batches to minimize uncertainty on model performance at the end of the label collection process. Instead of relying on high-variance REINFORCE policy gradient estimators that do not scale, our adaptive labeling policy is optimized using path-wise policy gradients computed by auto-differentiating through simulated roll-outs. Our framework is agnostic to different uncertainty quantification approaches and highlights the virtue of planning in adaptive labeling. On synthetic and real datasets, we empirically demonstrate even a one-step lookahead policy substantially outperforms active learning-inspired heuristics.

## 1 Introduction

Engineering progress in AI is predicated on rigorous empirical evaluation. Evaluating supervised ML models is easy when there is no distribution shift. However, naive offline evaluations on observational data cannot reliably assess safety due to selection bias. When ground-truth outcomes are expensive to collect, existing supervised datasets are often heavily affected by selection bias. For example, AI-based medical diagnosis systems are typically evaluated using past clinical labels, which are only available for patients who were initially screened by doctors. This creates a fundamental limitation: we cannot even assess the model's performance on patients who were never screened in the first place. Moreover, due to the absence of positive pathological patterns in the available data, previously unseen patient types may never get diagnosed by the AI diagnosis system. Evaluating the model's performance on previously unscreened patients is a first step toward mitigating this negative feedback loop.

Figure 1: **Adaptive labeling to reduce epistemic uncertainty over model performance.** Among the two clusters of unlabeled examples (left vs. right), we must learn to prioritize labeling inputs from the left cluster to better evaluate mean squared error.

In this paper, we study model evaluations under severe distribution shifts, where the support of the available supervised data is different from examples encountered during deployment. Randomly selecting inputs \(X\) to collect additional outcomes/labels \(Y\) incurs prohibitive cost, especially when the outcome distribution varies over a high-dimensional feature space. To address this, we must focus our sampling efforts on out-of-distribution (OOD) examples where the model's performance is uncertain. Going beyond the OOD detection literature that focuses on the distribution of input \(X\), we further connect the rarity of \(X\) with how smooth we believe \(Y|X\) to be. For example, in the extreme case where we believe \(Y|X\) is linear, it is easy to extrapolate model performance to regions of \(X\) where no data is available; such extrapolation is more challenging for nonlinear models.

We propose an adaptive sampling framework that focuses sampling effort on inputs \(X\) where there is large _epistemic_ (actionable) uncertainty on the shape of \(Y|X\). We differentiate this with _aleatoric_ (irreducible) uncertainty, which is due to idiosyncratic noise in the outcome measurement process. Since updating the sampling policy requires engineering or organizational effort in practice, we consider a few-horizon setting where at each period, a _batch_ of inputs is selected for labeling.

Our main contribution is the formulation of a _planning problem_ for selecting batches of inputs from a large pool of unlabeled data (Section 3). We develop adaptive policies that incorporate how beliefs on model performance get sharper as more labels are collected. We model our current belief on the data generation process \(Y|X\) as the current state. After observing a new batch of labeled data, we update our belief on the epistemic uncertainty via a posterior update ("state transition"). Our goal is to minimize the uncertainty over model performance at the end of label collection, as measured by the variance under the posterior after the final batch is observed. Modeling each batch as a time period, we arrive at a Markov Decision Process (MDP) where states are posterior beliefs, and actions correspond to selecting subsets (batches) from the pool of examples to be labeled (Figure 2). Notably, our problem is less ambitious than an active learning problem  where the goal is to adaptively label data to _improve_ model performance, over a _large_ number of batches. Narrowing our focus to model _evaluation_ allows us to address a broad class of evaluation problems including regression models, and solve the problem more effectively.

Solving our planning problem is computationally challenging due to the combinatorial action space and continuous states. To address this, we develop a tractable approach by considering a smooth relaxation of the problem, where we optimize over smoothly parameterized policies that select an entire batch of inputs . Although the well-known "score trick" enables the computation of policy gradient estimates based on function evaluations alone ("REINFORCE") [36; 5], this method suffers from high variance and may lead to poor performance in practice.

To compute reliable policy gradients, we develop an _auto-differentiable_ planning framework in Section 4. Since the dynamics of the MDP (posterior updates) are known, we perform "roll-outs" using the current posterior beliefs: we simulate potential outcomes that we may obtain if we label a particular batch, and evaluate the updated uncertainty on model performance based on the imagined pseudo labels of this new batch. By implementing all the operations within an auto-differentiable framework, we can backpropagate the observed reward to the policy parameters and calculate the

Figure 2: **Overview of our adaptive sampling framework. At each period, we select batch of inputs \(X^{t}\) to be labeled, and obtain a new labeled data \(_{t}\). We view posterior beliefs \(_{t}()\) on \(f^{*}(Y|X)\) as “states”, and update it as additional labeled data is collected. Our goal is to minimize uncertainty on the performance of the prediction model \(()\) at the end of \(T\) periods.**

_pathwise_ policy gradient. While our original problem is nonsmooth and discrete, auto-differentiating over this smoothed MDP provides reliable policy optimization methods.

Our approach is agnostic to the uncertainty quantification approach, and allows incorporating latest advances in Bayesian modeling (e.g., [5; 28]). Our "model-based" approach rests on encoding state transitions (posterior updates) in an auto-differentiation framework. We stress that we do not require the posterior updates to have a closed form (conjugacy), and allow approximate posterior inference using gradient-based optimizers. We demonstrate our planning framework over beliefs formed by Gaussian processes, and Deep Ensembles [18; 26] that enable leveraging the inductive biases of neural networks in high dimensions and gradient descent methods to update probabilistic beliefs over \(Y|X\). Empirically, we observe that even a single planning step can yield significant practical benefits (Section 5). On both real and simulated datasets with severe selection bias, we demonstrate that one-step lookahead policies based on our auto-differentiation framework achieve state-of-the-art performance, outperforming REINFORCE policy gradients and active learning-based heuristics.

**Limitations** Our empirical validation highlights several open research directions required to scale our framework. First, although recent advances in Bayesian modeling have achieved substantial progress in one-shot uncertainty quantification, we empirically observe that it is often difficult to maintain accurate beliefs as more data are collected. Second, we detail engineering challenges in implementing our auto-differentiation framework over multi-period roll-outs, highlighting the need for efficient implementations of high-order auto-differentiation.

## 2 Related work

Our work is closely related to active learning [1; 34], where the modeler adaptively collects labels to _improve_ model performance. Active learning methods typically focus on classification models and select inputs to label based on uncertainty sampling techniques such as margin-sampling or entropy, or by leveraging disagreements among different models, exemplified by query-by-committee and Bayesian Active Learning by Disagreement (BALD) . Although there is no notion of planning, some query strategies account for the expected reduction in error or model change . Since it is important to collect labels over a diverse set of inputs, heuristic criteria based on diversity and density are sometimes incorporated to account for the feature space distribution . Prior work addressing distribution shifts in active learning  uses importance weighting, which is not feasible in our setting, where the distribution shift is out of support. Batched settings are a long-standing challenge in active learning . In Section 5, we show that adapting greedy-based heuristics to batched scenarios yields poor performance. Recent extensions of active learning to batched settings (e.g., BatchBALD , BatchMIG ) continue to rely on myopic greedy algorithms for classification. On the other hand, we leverage our limited scope on evaluation to formalize a planning problem and derive a unified computational framework that can handle regression problems. We provide lookahead policies [2; 6; 7] that plan for the future and flexibly handle different objectives and uncertainty quantification methodologies.

Our problem can also be viewed as a version of pure exploration bandits  where we want to minimize some given function of the posterior. In contrast to conventional formulations, our central focus on batching induces combinatorial action spaces and few horizons. Bayesian optimization methods optimize black-box functions that are computationally challenging to evaluate by modeling the function as a draw from a Gaussian process . Notably, the knowledge gradient algorithm  maximizes the single period expected increase in the black-box function value and can be viewed as a one-step lookahead policy. Using auto-differentiation methods, we extend these classical ideas to model evaluation by incorporating batching (combinatorial action spaces).

While Gaussian processes works well in low dimensions, quantifying epistemic uncertainty on \(f^{}(Y|X)\) for high dimensional inputs is an active area of research, including popular techniques such as dropout , Bayes by Backprop , Ensembles/ Ensemble\(+\)[18; 26], and Epistemic Neural Networks . Our framework is compatible with deep learning-based uncertainty quantification models, such as Ensembles, where we can only do approximate posterior updates.

## 3 Markov decision process over posterior beliefs

Our goal is to evaluate the performance of the model \(:\) over the input distribution \(P_{X}\) that we expect to see during deployment. Given features \(X\), outcomes/ labels are generated from some unknown function \(f^{}\): \(Y=f^{}(X)+\) where \( N(0,^{2})\). When ground truth outcomes are costly to obtain, previously collected labeled data \(^{0}\) typically suffers selection bias and covers only a subset of the support of input distribution \(P_{X}\) over which we aim to evaluate the model performance. Assuming we have a pool of data \(_{}\), we design adaptive sampling algorithms that iteratively select inputs in \(_{}\) to be labeled. Since labeling inputs takes time in practice, we model real-world instances by considering _batched_ settings. Our goal is to sequentially label batches of data to accurately estimate model performance over \(P_{X}\) and therefore we assume we have access to a set of inputs \(_{} P_{X}\). We use the squared loss to illustrate our framework, where our goal is to evaluate \(_{X P_{X}}[(Y-(X))^{2}]\). Under the "likelihood" function \(p(y|f,x)=p_{}(y-f(x))\), let \(g(f)\) be the performance of the AI model \(()\) under the data generating function \(f\), which we refer to as our estimand of interest. When we consider the mean squared loss, \(g(f)\) is given by

\[g(f)_{X P_{X}}[_{Y p(|f,X)} (Y-(X))^{2} f].\] (1)

Our framework is general and can be extended to other settings. For example, a clinically useful metric is Recall, defined as the fraction of individuals that the model \(()\) correctly labels as positive among all the individuals who actually have the positive label \(g(f)_{X P_{X}}[_{Y p(|f,X)} \{(X)>0\}|Y=1]\).

Since the true function \(f^{}\) is unknown, we model it from a Bayesian perspective by formulating a posterior given the available supervised data. We refer to uncertainty over the data generating function \(f\) as _epistemic_ uncertainty--since we can resolve it with more data--and that over the measurement noise \(\) as _aleatoric_ uncertainty. Assuming independence given features \(X\), we model the marginal likelihood of the data via the product \(p(|f,)=_{(X,Y)}p(Y|f,X)\) where \(=()\). Our prior belief \(\) over functions \(f\) reflects our uncertainty about how labels are generated given features.

To adaptively label inputs from \(_{}\), we quantify epistemic uncertainty over the labels reliably using an uncertainty quantification (UQ) method. Roughly speaking, a UQ method provides us the ability to formulate a posterior belief \((f)\) given any supervised data \(\). As we detail in Section 4.1, our framework can leverage both classical Bayesian models like Gaussian processes and recent advancements in deep learning-based UQ methods. As new batches are labeled, we update our posterior beliefs about \(f\) over time, which we view as "state transitions" of a dynamical system. Recalling the Markov decision process depicted in Figure 2, we sequentially label a batch of inputs from \(_{}\) (actions), which lead to state transitions (posterior updates). Specifically, our initial state is given by \(_{0}()=(^{0})\) and at each period \(t\), we label a batch of \(K_{t}\) inputs \(^{t+1}_{}\) resulting in labeled data \(^{t+1}=(^{t+1}^{t+1})\). After acquiring the labels at each step \(t\), we update the posterior state to \(_{t+1}()=_{t}(^{t+1})\). Modeling practical instances, we consider a small horizon problem with limited adaptivity \(T\). Formulating an MDP over posterior states has long conceptual roots, dating back to the Gittin's index for multi-armed bandits .

We denote by \(_{t}\) the adaptive labeling policy at period \(t\). We account for randomized policies \(^{t+1}_{t}(_{t})\) with a flexible batch size \(|^{t+1}|=K_{t}\). We assume \(_{t}\) is \(_{t}-\)measurable for all \(t T\), where \(_{t}\) is the filtration generated by the observations up to the end of step \(t\). Observe that \(_{t+1}\) contains randomness in the policy \(_{t}\) as well as randomness in \(^{t+1}(^{t+1},_{t})\). Letting \(=\{_{0},....,_{T-1}\}\), we minimize the uncertainty over \(g(f)\) at the end of data collection

\[H()_{^{1:T}}[G(_{T})] _{^{1:T}}[G((^{0:T}))]=_{^{1:T}}[_{f (|^{0:T})}g(f)],\] (2)

where \(G(_{T})=_{f_{T}}g(f)\). In the above objective (2) we assumed that the modeler pays a fixed and equal cost for each outcome. Our framework can seamlessly accommodate variable labeling cost as well. Specifically, we can define a cost function \(c()\) applied on the selected subsets and update the objective accordingly to have a term \( c(^{1:T})\) where \(\) is the penalty factor that controls the trade-off between minimizing variance and cost of acquiring samples.

In the planning problem (2), states are continuous and the action space is combinatorial. To address these issues, we propose a continuous policy parameterization \(_{}\) (with parameter \(\)) in the next section. We solve this MDP using policy gradients. Let the gradient be defined as \(_{}H()_{}_{A_{ }}[G(A)]\). Here, we slightly abuse notations by letting \(A^{1:T}\) and \(G(A) G((^{0},^{1:T}))\), where the distribution of \(A\) depends on \(_{}\). To approximately solve this MDP using policy gradients, the score function estimator (REINFORCE ) uses the fact that \(_{}_{A_{}}[G(A)]=_{A_{}}[G (A)_{}_{}(A)]\) for any function \(G()\). Despite being unbiased, Monte-Carlo estimation of the above expectation typically suffers from prohibitively large variance  especially when \(_{}(A)\) is small. Although a stream of work strives to provide variance reduction techniques for REINFORCE [19; 29], they require value function estimates that are also challenging to compute in our planning problem (2).

## 4 Planning using pathwise policy gradients

Our main algorithmic insight is that for systems with known dynamics (posterior updates) or where the dynamics can be approximated sufficiently well, we can leverage auto-differentiation to directly estimate approximate pathwise policy gradients (4) instead of relying on high-variance score-based gradients. The policies derived using our efficient differentiable simulator exploits the system structure and achieves significantly improved performance compared to policies that do not rely on gradient information, as detailed in Section 5. Intuitively, this is akin to finding a random variable \(Z p_{Z}\) distributed independent of policy \(_{}\), such that \(A=h(Z,)\) and

\[_{}_{A_{}}[G(A)]=_{} _{Z p_{Z}}[G(h(Z,))]}{{=}}_{Z  p_{Z}}[_{}G(h(Z,))]\] (3)

However, the equality \((a)\) above holds only if \(G(h(Z,))\) is differentiable w.r.t. \(\). In our case, as we will see later, \(h(Z,)\) is non-differentiable w.r.t. \(\) because the actions are discrete and combinatorial. To address this, we will use a smoothed approximation, \(h_{}(Z,)\), such that \(G(h_{}(Z,))\) becomes differentiable. Here, \(\) is a temperature parameter that controls the degree of smoothing. Consequently, we can approximate the gradient as follows -

\[_{}_{A_{}}[G(A)]_{} [G(h_{}(Z,))]_{i=1}^{n}_{ }G(h_{}(Z_{i},)).\] (4)

It is important to note that sometimes even \(G()\) is non-differentiable, and in that case we must also consider a smooth approximation of \(G()\) (e.g., refer to Section D.1 for details on smoothing the Recall objective discussed earlier in Section 3). Our approach is inspired by the line of work on differentiable simulators [4; 15; 21; 40; 35]. Theoretical analysis in stochastic optimization [11; 20] also highlights the benefit of these estimators over zeroth-order gradient estimates of a stochastic objective such as REINFORCE. Suh et al.  notes that first-order gradient estimators typically perform well when the objective is sufficiently smooth and continuous. In what follows, we empirically demonstrate that through a careful smoothing of the planning objective, it is possible to trade-off bias and variance through auto-differentiation.

```
1:Inputs: Labeled batch data \(^{0}\), horizon \(T\), UQ module, pool data \(_{}\), batch size \(K\)
2:Returns: Selected batches \((^{t},^{t})\) for \(1 t T\) and updated estimate of the objective \(G(_{T})\)
3:\(t=0\) : Compute initial posterior state \(_{0}\) based on UQ module and batch of labeled data \(^{0}\).
4:for\(0 t T-1\)do
5:\(_{t}}{{=}}$, pool $(^{t+1}_{})$, batch size $K$)}\)
6:\(^{t+1}=\{X_{j}:X_{j}(^{t+1}_{ };_{t})1 j K\}\)
7: Obtain labels \(^{t+1}\) to create \(^{t+1}\)
8: Update posterior state: \(_{t+1}}{{=}}_{t}(^{t+1})\)
9: Estimate the objective \(G(_{t+1})\)
10:endfor ```

**Algorithm 1**Autodiff 1-lookahead

We showcase the power of _planning using pathwise policy gradients_ by considering the simplest possible planning algorithm: one-step lookaheads. Empirically, we demonstrate that even one-step lookahead achieves significant improvement in sampling efficiency over other heuristic baselines. Additionally, we highlight open engineering directions that can enable efficient multi-step planning methods. At each time step \(t\), our algorithm selects a batch of inputs \(^{t+1}\) to minimize the uncertainty over the one-step target estimand \(G(_{t+1})\) based on current posterior belief \(_{t}\). After selecting the batch \(^{t+1}\), we observe the corresponding labels \(^{t+1}\) and update posterior belief over the data-generating function \(f\) to \(_{t+1}\) (see Algorithm 1). The samples \(^{t+1}\) to query are determined using a policy \(_{t,}}{{=}}_{}\), which we optimize through pathwise policy gradients (Algorithm 2). To optimize the policy \(_{t,}\), we simulate "roll-outs" of posterior beliefs over \(f\) using imagined pseudo labels \(}_{}\) generated from current belief \(_{t}\). Algorithm 1 summarizes each of these steps of the overall procedure. Our conceptual framework is general and can leverage multi-step look-aheads [2; 6; 7].

Our algorithm consists of three major components which we will describe in detail shortly. First, we parameterize a single-batch policy \(_{}\) and use \(K\)-subset sampling  to choose \(K\) samples. Second, we use a UQ module that characterizes posterior beliefs over \(f\). Finally, to reliably optimize \(\), we adopt an auto-differentiable "roll-out" pipeline through which we approximate policy gradients by smoothing the pipeline. The overall pipeline (and its differentiable analogue) is briefly described in Figure 3. We also explain our algorithm graphically in Figure 8 of Section A. Our codebase is available at https://github.com/namkoong-lab/adaptive-labeling.

### Uncertainty Quantification (UQ) Module

The UQ module maintains an estimate \(_{t}\) over posterior beliefs of \(f\) over time, enabling our planning framework. Our method is agnostic to the UQ module, and we illustrate this using two instantiations: i) Gaussian Processes (GP) that are extensively used in the Bayesian Optimization literature and are known to work well in low dimensional data and regression setting, and ii) recently developed neural network-based UQ methods such as Ensembles / Ensemble+.

Gaussian ProcessesGPs \(f()(m(),(,))\) are defined by a mean function \(m()\) and kernel \((,)\), where for any inputs \(X\), \(f(X)\) is Gaussian with mean \(m(X)\) and \((f(X_{i}),f(X_{j}))=(X_{i},X_{j})\). Additionally, the observation consists of \((X,Y)\) with \(Y=f^{*}(X)+\) and \((0,^{2}I)\) for some noise level \(>0\). Posterior updates have a closed form and are differentiable, as we review in Section B.

EnsemblesEnsembles learn an ensemble of neural networks from given data, with each network having independently initialized weights. Ensemble+ extends this approach by combining ensembles of neural networks with randomized prior functions and bootstrap sampling . The prior function is added to each network in the ensemble, trained using L2 regularization. Recently, Epistemic neural networks (ENNs)  have also been shown to be an effective way of quantifying uncertainty. All these deep learning models are parametrized by some parameter \(\). For a given sample \(\{(X_{j},Y_{j})\}_{j}\), the model weights \(\) are update through gradient descent under a loss function \((.)\), with the update rule expressed as: \(_{}=-_{j}_{}(X_{j},Y_{j},)\).

### Sampling Policy

At each time step, a batch of \(K\) samples is queried to improve the objective. The samples are selected from a pool \(_{}\) of size \(n\), based on weights \(()\). Specifically, given weights \(\) and a batch size \(K\), the \(K\)-subset sampling mechanism generates a random vector \(S\{0,1\}^{n}\), whose distribution depends on \(\) such that \(_{i=1}^{n}S_{i}=K\). Let \(^{j}\) denote the \(j\)-th unit vector, and consider a sequence of unit vectors \([^{i_{1}}^{i_{k}}] s\). Using a parametrization known as weighted reservoir sampling (wrs) , the probability of selecting a sequence \(s\) is given by:

\[p_{}([^{i_{1}},,^{i_{k}}]|) {w_{i_{1}}}{_{j=1}^{n}w_{j}}}}{_{j=1}^{n}w_{j}-w_{i_{1}} }...}}{_{j=1}^{n}w_{j}-_{j=1}^{k-1}w_{i_{j}}}.\] (5)

The probability of selecting a batch \(S\) can then be defined as: \(p(S|)_{s(S)}p_{}(s|),\) where \((S)=\{s:S=_{j=1}^{k}s[j]\}\) denote all sequences of unit vectors with sum equal to \(S\).

Figure 3: Differentiable one-step look ahead pipeline for efficient adaptive sampling

### Differentiable pipeline

We propose a fully differentiable pipeline by ensuring that each component of the pipeline: i) \(K\)-subset sampling, ii) posterior updates (UQ module), and iii) the objective are differentiable. We use parametrizations \(\) for the policy function \(\) which yields sampling probabilities \(()\). Posterior beliefs \(\) are parametrized by \(\) (e.g., Ensemble+ weights). In the following, we describe how we smooth the respective components. The one-step objective parametrized by \(\) is given by

\[H()_{}_{}; } p(|())}[G(_{+}^{S} )]\] (6)

where \(G\) was defined in (2). Here, \(p(|())\) is the distribution over subsets governed by \(\) as defined by (5). Further, \(\) is the current posterior state and \(_{+}\) is the updated posterior state after incorporating the the additional batch \(S\) selected from \(_{}\) and the corresponding pseudo-outcomes from \(}_{}\) (drawn based on current posterior state \(\)). We smooth the objective \(H()\) to estimate \(_{}H()\) using a soft \(K\)-subset sampling.

Soft \(K\)-subset samplingWe use the smoothed \(K\)-subset sampling procedure proposed in Xie and Ermon . Briefly, the Algorithm 3 introduced in  produces a random vector \(\) such that \(_{i=1}^{n}a_{i}=K\) (soft version of \(S\)), such that \(() p(|())_{}()\) (5), and is differentiable. We defer the details to Section C.

```
1:Inputs: Posterior belief \(\), pool data \((_{})\), batch size \(K\), training epochs: \(M\) Returns: Policy \(_{}\)
2:Initialize the policy \(_{}\) (parametrized by \(\))
3:for\(1 m M\):do
4: Generate peusdo labels \(}_{}\) for all the \(_{}\), where \(}_{}\) is drawn from the current posterior state \(\).
5: Use Algorithm 3 (Input: \(_{}:=()\)) to generate a random soft vector \(()\) corresponding to policy \(_{}\)
6: Update (pseudo) the posterior state to \(_{+}^{()}\) using the pool data and pseudo labels \((_{},}_{})\)
7: Estimate the objective \(G(_{+}^{()})\)
8: SGD update: \(-_{t}_{}G(_{+}^{()})\)
9:endfor
10:Return: Policy \(_{}\) ```

**Algorithm 2** One-step look-ahead policy gradient

**Smoothing the UQ module** To enable differentiable posterior updates, we approximate the posterior updates using the soft \(K\)-subset samples \(()\). In the case of GPs, the soft \(K\)-subset selection variable \(a\) can be interpreted as a weighting of samples in a GP. Thus, closed form GP updates can be analogously used with appropriate weighting, described in the Appendix D. To perform weighted updates for deep learning based UQ modules (Ensembles/Ensemble+ ), we approximate previous gradient updates by incorporating soft sample weights \(()\):

\[_{1}()_{0}-_{j}a_{j}()_{}(X_ {j},_{j},),\] (7)

where \(\) is the loss function of the concerned model, with gradients evaluated at \(\). We interpret \(_{1}\) as an approximation of the optimal UQ module parameter with one gradient step. Similarly, we define a higher-order approximation \(_{h+1}()\) as

\[_{h+1}()_{h}(())-_{j}a_{j}() _{}(X_{j},_{j},)_{=_{h}(())}.\] (8)

Note here that effectively, the gradient is now of the form \(_{}G(*{argmin}_{}((())))_{}G(_{+}(())).\) To compute this, we use higher  and torchopt which allows us to approximately differentiate through the argmin. In practice, there is a trade-off between the computational time and the accuracy of the gradient approximation depending on the number of training steps we do to estimate the argmin.

## 5 Experiments

We empirically demonstrate effectiveness of our planning framework on both synthetic and real datasets by focusing on the simplest planning algorithm: 1-step lookaheads. Using two uncertainty quantification modules--GPs and Ensembles / Ensemble+ --we show that planning with path-wise gradients is a promising approach to adaptive labeling. Throughout this section, we focus on evaluating the mean squared error of a regression model \(\), and develop adaptive policies that minimize uncertainty on this quantity (\(g(f)\)). When GPs provide a valid model of uncertainty, our experiments show that our auto-differentiable planning framework significantly outperforms other baselines. We further demonstrate that our conceptual framework extends to deep learning-based uncertainty quantification methods like Ensemble+ while highlighting computational challenges that need to be resolved in order to scale our ideas. For simplicity, we assume a naive predictor, i.e., \(() 0\). However, we emphasize that this problem is just as complex as if we were using a sophisticated model \((.)\). The performance gap between the algorithms primarily depends on the level of uncertainty in our prior beliefs.

To evaluate the performance of our algorithm, we benchmark it against several baselines. Our first set of baselines are from active learning :

**Active Learning Heuristics: (1)** Uncertainty Sampling (Static): In this approach, we query the samples for which the model is least certain about. Specifically, we estimate the variance of the latent output \(f(X)\) for each \(X_{}\) using the UQ module and select the top-\(K\) points with the highest uncertainty. **(2)** Uncertainty Sampling (Sequential): This is a greedy heuristic that sequentially selects the points with the highest uncertainty within a batch, while updating the posterior beliefs using pseudo labels from the current posterior state. Unlike Uncertainty Sampling (Static), this method takes into account the information gained from each point within batch, and hence tries to diversify the selected points within a batch.

We also compare our approach to solving the planning problem using **(3)** REINFORCE-based policy gradients, which implements one-step look ahead policy gradient using the score trick. Finally, we study **(4)** Random Sampling, which selects each batch uniformly at random from the pool. We repeat all experiments with 10 random seeds.

### Planning with Gaussian processes

We now briefly describe the data generation process for the GP experiments, deferring a more detailed discussion of the dataset generation to Section E. We use both the synthetic data and the real data to test our methodology. For the _simulated data,_ we construct a setting where the general population is distributed across _51 non-overlapping clusters_ while the initial labeled data \(^{0}\) just comes from one cluster. In contrast, both \(_{}(_{},Y_{}),_{}(_{}, _{})\) are generated from all the clusters. We begin with a low-dimensional scenario, generating a one-dimensional regression setting using a Gaussian Process (GP). Although the data-generating process is not known to the

[MISSING_PAGE_FAIL:9]

observed superior performance . Recall that implementing our framework with deep learning based UQ modules requires us to retrain the model across multiple possible random actions \(()\) sampled from the current policy \(_{}\). This requires significant computational resources, in sharp contrast to the GPs where the posteriors are in closed form and can be readily updated and differentiated.

Due to the computational constraints, we test Ensemble+ on a toy setting to demonstrate the generalizability of our framework. We consider a setting where the general population consists of four clusters, while the initial labeled data only comes from one cluster. Again we generate data using GPs. The task is to select a batch of 2 points in one horizon. We detail the Ensemble+ architecture in the Appendix, and we assume prior uncertainty to be large (depend on the scaling of prior generating functions). The results are summarized in the Table 1.

## 6 Gradient Estimation - Theoretical Insight

In this section, we present theoretical analysis of the statistical properties (bias-variance trade-off) of the REINFORCE and AUTODIFF (smoothed-pathwise (4)) gradient estimators. To compare the two estimators, we consider a simplified setting with two actions \(=\{-1,1\}\), and the policy is parametrized by \(\) with \(_{}(-1)=\) and \(_{}(1)=1-\), Additionally, we are interested in the gradient \(_{}H()\), where \(H()=_{A_{}}G(A)\) with \(G(A)=A\) for simplicity. Given \(N\) i.i.d. samples of \(A_{i}_{}\), the REINFORCE estimator is defined as \(_{N}^{}=_{i=1}^{N}(G(A_{i})_{ }(_{}(A_{i})))\). The pathwise gradient estimator \(_{,N}^{}\) is the \(N\)-sample approximation of \(_{U,0}[_{}G(h_{}(U,))]\), where \(h_{}(U,)=(()-1)/( ()+1)\) (smoothing of \(h(U,):=2(U>)-1A\)). Our result (proof in Section F) highlights the conditions under which \(_{,N}^{}\) achieves a lower mean squared error (mse) compared to \(_{N}^{}\).

**Theorem 1**.: _For \(\) and \(N-1\), there exists \(\) depending on \((N,)\) such that_

\[(_{,N}^{})<4( _{N}^{}).\]

_Additionally, for any \(N\), \(=\), and \(k\), we have the following \((_{N}^{})=(k)\), \((_{,N}^{})<4\). The same statement holds for \(=1-\) as well. This implies that the mse of \(_{N}^{}\) is unbounded, while the mse of gradient estimator is bounded._

## 7 Conclusion and Future Work

Supervised data often suffers severe selection bias when labels are expensive. We propose a new framework for adaptive labeling for model evaluation under out-of-support distribution shifts. Following a Bayesian framework, we formulate an MDP over posterior beliefs on model performance. We show that these planning problems can be efficiently solved with pathwise policy gradients, computed through a carefully designed auto-differentiable pipeline. We also demonstrate that even one-step lookahead policies outperform heuristic active learning algorithms. However, this improvement comes at the cost of additional computational resources. Our formulation is thus appropriate in high-stake settings such as clinical trials where labeling costs far exceed computational costs.

We further highlight some important nuances which we learned from our experiments. There are some important properties that a UQ module should have for it to perform well in our framework. The most important feature is that posteriors should be consistent, that is, early stopping of the training should not cause a significant change across the ranking of the subsets of the pool. Second, the posterior update should be done efficiently, which can be achieved either by doing parallelization or having UQ modules that provide readily available posterior updates, such as GPs. Recent advances in Bayesian transformers [24; 22] will be an interesting direction to explore in this regard.

  Algorithm & Variance of \(_{2}\) loss estimate & Error of \(_{2}\) loss estimate \\  Random sampling & 7129.8 \(\) 1027.0 & 136.2 \(\) 8.28 \\  Uncertainty sampling (Static) & 10852 \(\) 0.0 & 162.156 \(\) 0.0 \\  Uncertainty sampling (Sequential) & 8585.5 \(\) 898.9 & 144 \(\) 6.93 \\  REINFORCE & 1697.1 \(\) 0.0 & 45.27 \(\) 0.0 \\  Autodiff 1-lookahead & 1697.1 \(\) 0.0 & 45.27 \(\) 0.0 \\  

Table 1: Performance under Ensemble+ as UQ module