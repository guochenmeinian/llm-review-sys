# GEX: A flexible method for approximating influence

via Geometric Ensemble

 Sung-Yub Kim

Graduate School of AI, KAIST

sungyub.kim@kaist.ac.kr

&Kyungsu Kim

Massachusetts General Hospital and Harvard Medical School

kshim.doc@gmail.com

&Eunho Yang\({}^{}\)

Graduate School of AI, KAIST and AITRICS

eunhoy@kaist.ac.kr

###### Abstract

Through a deeper understanding of predictions of neural networks, Influence Function (IF) has been applied to various tasks such as detecting and relabeling mislabeled samples, dataset pruning, and separation of data sources in practice. However, we found standard approximations of IF suffer from performance degradation due to oversimplified influence distributions caused by their bilinear approximation, suppressing the expressive power of samples with a relatively strong influence. To address this issue, we propose a new interpretation of existing IF approximations as an average relationship between two linearized losses over parameters sampled from the Laplace approximation (LA). In doing so, we highlight two significant limitations of current IF approximations: the linearity of gradients and the singularity of Hessian. Accordingly, by improving each point, we introduce a new IF approximation method with the following features: i) the removal of linearization to alleviate the bilinear constraint and ii) the utilization of Geometric Ensemble (GE) tailored for non-linear losses. Empirically, our approach outperforms existing IF approximations for downstream tasks with lighter computation, thereby providing new feasibility of low-complexity/nonlinear-based IF design.

## 1 Introduction

In the last decade, neural networks (NNs) have made tremendous advances in various application areas . To make reasonable predictions with NN-based systems, models must be able to explain their predictions. For example, those who doubt the model's prediction can gain insight and foresight by referencing the explanation of the model. Moreover, mission-critical areas like finance and medicine require a high degree of explainability to ensure that the predictions are not biased . Understanding the mechanism of predictions also allows researchers and engineers to improve prediction quality, ensuring that NNs are performing as intended .

To this end, Influence Function (IF) was proposed to explain predictions of pre-trained NNs through training data . Intuitively, IF measures how the leave-one-out (LOO) retraining of a training sample changes the loss of each sample. Therefore, the sign of influence determines whether the training sample is beneficial to others, and the scale of influence measures its impact. Specifically, self-influence, the increase in loss when a sample is excluded, was used to measure how much the sample is memorized : When LOO training is performed on a memorized training sample, its loss will increase substantially since matching its (corrupted) label will be difficult. Therefore, self-influence is used to detect mislabeled samples  where memorization occurs. Furthermore,recent works successfully applied IF to various downstream tasks, including dataset pruning  and data resampling [59; 63].

Despite its broad applicability, we found that IF and its approximations [25; 48; 52] suffer from oversimplified self-influence distributions due to their bilinear form1. Although these approximations are introduced to avoid prohibitive retraining costs of IF, they impose a structural constraint that self-influence becomes quadratic to gradients of pre-trained NNs. Due to this constraint, self-influence follows an unimodal distribution, as gradients of pre-trained NNs typically follow a zero-centered Stable distribution [7; 56]. Unfortunately, unimodal distributions are too restrictive for representing self-influence in real-world datasets containing mislabeled samples. While self-influence distributions estimated by LOO retraining may become bimodal depending on the proportion of (high self-influent) mislabeled samples, unimodal distributions cannot handle this case.

To resolve this problem, we propose a non-linear IF approximation via Geometric Ensemble (GE; ). Our method is motivated by a novel connection between IF approximation and linearized Laplace approximation (LA; ) that we discovered: IF approximations can be translated to an averaged relationship between two linearized losses over parameters sampled from LA. As linearized losses in this connection cause bilinear forms of IF approximations, we consider an IF approximation without linearization. However, we then identify an additional issue of this approximation due to the singularity of Hessian and its solutions (e.g., damping and truncation). To mitigate this issue, we propose a novel approach using GE to manage the relationship of non-linear losses more effectively. As a result, our approach, **Ge**ometric **E**nsemble for sample **eX**planation (GEX), accurately represents the multimodal nature of LOO retraining, leading to improved performance in downstream tasks across various scenarios. Furthermore, \(_{}\) is easy to estimate as it does not require Jacobian-vector products (JVPs) for batch estimation or sub-curvature approximations like LA.

We summarize our contributions as follows:

* We identify a distributional bias in commonly used IF approximations. We demonstrate how this bias results in oversimplified distributions for self-influences.
* We provide a novel connection between IF approximations and LA. By identifying an inherent issue of LA, we provide a non-linear IF approximation via GE, named \(_{}\). Due to its non-linear nature, \(_{}\) can express various influence distributions depending on the characteristic of the datasets.
* We verify that \(_{}\) outperforms standard IF approximations in downstream tasks, including noisy label detection, relabeling, dataset pruning, and data source separation. We also show that \(_{}\) is competitive with well-known baselines of downstream tasks with lighter computation.2

## 2 Background

Consider an independently and identically (i.i.d.) sampled training dataset \(S:=\{z_{n}:(x_{n},y_{n})\}_{n=1}^{N}\) where \(x_{n}^{D}\) is an input vector of \(n\)-th sample and \(y_{n}^{K}\) is its label. To model the relation between inputs and outputs of training samples, we consider a neural network (NN) \(f:^{D}^{P}^{K}\) which maps input \(x^{D}\) and network parameter \(^{P}\) to a prediction \(_{n}:=f(x_{n},)^{K}\). Empirical risk minimization (ERM) solves the following optimization problem

\[^{*}:=*{argmin}_{^{P}}L(S,)\]

where \(L(S,):=_{n=1}^{N}(z_{n},)/N\) for a sample-wise loss \((z_{n},)\) (e.g., cross-entropy between \(_{n}\) and \(y_{n}\)). In general, \(^{*}\) is trained using a stochastic optimization algorithm (e.g., stochastic gradient descent (SGD) with momentum). For the reader's convenience, we provide the notation table in Appendix \(\) for terms used in the paper.

The Leave-One-Out (LOO) retraining effect of \(z S\) on another instance \(z^{}^{D}\) is defined as the difference of sample-loss \(z^{}\) between the \(^{*}\) and the retrained point \(_{z}^{*}\) without \(z\):

\[_{}(z,z^{}):=(z^{},_{z}^{*})-( z^{},^{*})\] (1)where \(_{z}^{*}:=*{argmin}_{^{P}}L(S,)-(z, )/N\). Since retraining for every pair \((z,z^{})\) is computationally intractable where we have a huge number of data points as in practice, Koh and Liang  proposed an efficient approximation of \(_{}\), named Influence Function (IF):

\[(z,z^{}):=g_{z^{}}^{}H^{-1}g_{z}\] (2)

where \(g_{z}:=_{}(z,^{*})^{P}\) and \(H:=_{}^{2}L(S,^{*})^{P P}\) by assuming the strictly convexity of \(\) (i.e., \(H\) is positive definite). Here, \(\) can be understood as a two-step approximation of \(_{}\)

\[_{}(z,z^{}) _{^{*}}^{}(z,_{z}^{*}) -^{}(z^{},^{*})=g_{z^{}}^{}( _{z}^{*}-^{*})\] (3) \[ g_{z^{}}^{}H^{-1}g_{z}=(z,z^{})\] (4)

where \(_{^{*}}^{}(z,):=(z,^{*})+g_{z}^{ }(-^{*})\) and \(^{P}\) is an arbitrary vector in the parameter space \(^{P}\). Here, we use the superscript \(\) to indicate linearization. Note that (3) applies linearization to the sample-loss \(z^{}\) and (4) approximates parameter difference as a Newton ascent term (i.e., \(_{z}^{*}^{*}+H^{-1}g_{z}\)).

While the computation of \(\) is cheaper than \(_{}\), it is still intractable to modern NNs (e.g., ResNet  and Transformer ) because of the prohibitively large Hessian. To alleviate this problem, two additional approximations are commonly used: stochastic approximation methods such as \(_{}\), and sub-curvature approximations such as \(_{}\), which limits the Hessian computation only to the last-layer of NNs . However, both methods also have their own problems: \(_{}\) takes high time complexity since the inverse Hessian-vector product (IHVP) for each training sample needs to be computed separately, as shown in Schioppa et al. . On the other hand, \(_{}\) may cause inaccurate IF approximations, as shown in Feldman and Zhang .

As another alternative, Pruthi et al.  recently proposed to exploit intermediate checkpoints during pre-training, named \(_{}\):

\[_{}(z,z^{}):=_{c=1}^{C}g_{z^{ }}^{c}g_{z}^{c}\] (5)

where \(g_{z}^{c}:=_{}(z,^{c})\) for checkpoints \(^{c}\) (\(c=1,,C\)) sampled from the pre-training trajectory of \(^{*}\). Here, it further simplifies the computation by assuming the expensive \(H^{-1}\) in (2) is an identity matrix. Instead, the performance of \(_{}\) is replenished by averaging over several intermediate checkpoints, which capture various local geometries of loss landscapes. In addition, Pruthi et al.  enhanced the efficiency of \(_{}\) using **random projection**, named \(_{}\),

\[_{}(z,z^{}):=_{c=1}^{C}g_{z^ {}}^{c}Q_{R}Q_{R}^{}g_{z}^{c}\] (6)

where \(Q_{R}^{P R}\) is a random projection matrix whose components are i.i.d. sampled from \((0,1/R)\) for \(R P\). Note that \(_{}\) is an unbiased estimator of \(_{}\) as \([Q_{R}Q_{R}^{}]=_{P}\) where \(_{P}\) is the identity matrix of dimension \(P P\). However, \(_{}\) and \(_{}\) cannot be applied to checkpoints of the open-source community, such as TorchHub  and Huggingface models  since they only provide the final checkpoints without any intermediate results during pre-training.

On the other hand, Schioppa et al.  proposed to approximate IF in a purely post-hoc manner as

\[_{}(z,z^{}):=g_{z^{}}^{}U_{R}_ {R}^{-1}U_{R}^{}g_{z}\] (7)

where \(_{R}^{R R}\) is a diagonal matrix whose elements are top-\(R\) eigenvalues of \(H\) and the columns of \(U_{R}^{P R}\) are corresponding eigenvectors. They use Arnoldi iteration  to estimate \(_{R}\) and \(U_{R}\). Contrary to \(_{}\) with the random projection, \(_{}\) projects \(g_{z},g_{z^{}}\) to the top-\(R\) eigenspace of the Hessian. Due to this difference, Schioppa et al.  argued that \(_{}\) perform comparably to \(_{}\) with less \(R\).

Finally, it is important to note that \(_{}\) and \(_{}\) can perform **batch computations** using Jacobian-vector products (JVPs). To be more precise, computing \(\) and \(_{}\) for multiple samples requires sample-wise gradient computation for each \(g_{z},g_{z}^{c}\), which is difficult to parallelize because of heavy memory complexity. In contrast, \(_{}\) and \(_{}\) can avoid this sample-wise computation by computing JVPs for a batch at once (i.e., parallelize \(g_{z}^{c}Q_{R}\), \(g_{z}^{}U_{R}\) for multiple \(z\)).

## 3 Identifying distributional bias in bilinear influence approximations

Throughout the discussion above, IF approximations ((2), (5), (6), and (7)) are defined as **bilinear forms of gradients**. In this section, we demonstrate a side effect of this form on computing self-influence. Our key empirical observation is that **gradients of pre-trained NNs follow Gaussian distributions centered at zero**. To verify this, we train VGGNet  on MNIST  with 10% label corruption following Schioppa et al. . Then, we project \(\{g_{z}\}_{z S}\) onto each of 5 random vectors \(\{d_{i}\}_{i=1}^{5}\) (with \(d_{i}^{P}\)), uniformly sampled on the unit sphere. If a pre-trained gradient \(g_{z}\) follows Gaussian distribution, its projected component \(g_{z}^{}d_{i}\) will also follow Gaussian distributions for all \(i=1,,5\). We visualize histograms of projected components \(g_{z}^{}d_{i}\) for each \(d_{i}\), and the resulting p-values of the normality test  in Fig. 1(a). In agreement with our hypothesis, the randomly projected components follow Gaussian distributions with significantly low p-values. Moreover, one can observe that Gaussian distributions are centered at zero. This is because gradients at \(^{*}\) satisfy

\[_{z S}[g_{z}^{}d]=_{z S}g_{z}^ {}d=_{P}^{}d=0\]

by the first-order optimality condition at \(^{*}\) (i.e., \(_{}L(S,^{*})=_{P}\)) where \(_{P}\) is the zero vector of dimension \(P\). Note that a similar observation on the normality of gradient was reported in the context of differential privacy in Chen et al. .

Now let us write the eigendecomposition of Hessian as \(H=_{i=1}^{P}_{i}u_{i}u_{i}^{}\) where \(_{1},,_{P}>0\) are eigenvalues of \(H\) in descending order and \(u_{1},,u_{P}^{P}\) are corresponding eigenvectors by positive definite assumption of Koh and Liang . We then arrange self-influence of \(\) as

\[(z,z)=g_{z}^{}H^{-1}g_{z}=_{i=1}^{P})^{2}}{ _{i}}\] (8)

where \(g_{z,i}:=g_{z}^{}u_{i}\) are \(i\)-th component of \(g_{z}\) in the eigenspace of \(H\). Following the above discussion, one can assume that \(g_{z,i}\) follows a Gaussian distribution. Consequently, \((z,z)\) follows a (generalized) \(^{2}\)-distribution due to squared components in (8). The following proposition shows that this phenomenon can be generalized to any stable distribution  and positive definite matrix.

**Proposition 3.1** (Distributional bias in bilinear self-influence).: _Let us assume \(g_{z}\) follows a \(P\)-dimensional stable distribution (e.g., Gaussian, Cauchy, and Levy distribution) and \(M^{P P}\) is a positive (semi-)definite matrix. Then, self-influence in the form of \(_{M}(z,z)=g_{z}^{}Mg_{z}\) follows a unimodal distribution. Furthermore, if \(g_{z}\) follows a Gaussian distribution, then the self-influence follows a generalized \(^{2}\)-distribution._

We refer to Appendix B for the proof. Self-influence distributions approximated by \(_{}\) (Fig. 1(b)) and \(_{}\) (Fig. 1(c)) provide the empirical evidence of Proposition 3.1 in the noisy label setting in Fig. 1(a). In this setting, each mislabeled sample exhibits high self-influence values since predictions by incorrect labels are challenging to recover when removed and retrained. Therefore, mislabeled samples constitute a distinct mode with a greater self-influence than correctly labeled samples. Still, the distributional bias in Proposition 3.1 squeezes correct and mislabeled samples in a unimodal distribution. As a result, two types (clean and corrupted) of samples are indistinguishable in \(_{}\) and \(_{}\). Proposition 3.1 shows that this observation can be generalized to a more diverse setting (e.g., heavy-tailed distributions). In Sec. 5.1, we demonstrate that this bias occurs regardless of the dataset and the architectures of NN.

Figure 1: Since pre-trained gradients follow Gaussian distributions (Fig. 1(a)), quadratic self-influences follow unimodal distributions (See Proposition 3.1), even in noisy label settings (Figs. 1(b)-1(c)). This issue can be solved by removing linearization in the IF approximation (Fig. 1(d)).

## 4 Geometric Ensemble for sample eXplanation

To mitigate the distributional bias in Sec. 3, we propose a flexible IF approximation method using Geometric Ensemble (GE; ), named Geometric Ensemble for sample eXplanataion (GEX). Here is a summary of how GEX is developed.

\[^{}} _{}_{}_{ }_{}\]

In Sec. 4.1, we ensure that the influence approximation is not a bilinear form for the gradient by replacing gradients in IF with sample-loss deviations. The theoretical foundation for this step is provided by our Theorem 4.1 below, which establishes a relationship between the IF and the Laplace approximation (LA; ). Moving on to Sec. 4.2, we modify the parameter distribution to compute the sample-loss deviation from LA to GE. This modification is necessary because GE is based on the local geometry of the loss landscape around \(^{*}\), similar to LA, while avoiding overestimating loss deviations caused by the singularity of the Hessian.

### GEX and its motivation

The proposed method, GEX, comprises three steps. The first step involves the collection of post-hoc checkpoints \(\{^{m}\}_{m=1}^{M}\) through multiple SGD updates on the training loss starting from \(^{*}\). Then, the empirical distribution of Geometric Ensemble (GE) is computed as

\[p_{}():=_{m=1}^{M}_{^{(m)}}().\] (9)

where \(_{^{(m)}}()\) denotes the Dirac delta distribution at \(^{(m)}\). In the final step, GEX is obtained as the expectation of the product of **sample-loss deviations** from the pre-trained parameter as follows

\[_{}(z,z^{})=_{ p_{}}[_{^{*}}(z,)_{^{*}}(z^{ },)]\] (10)

where \(_{^{*}}(z,):=(z,)-(z,^{*})\) means the sample-loss deviations from \(^{*}\). We provide the pseudocode for computing \(_{}\) in Appendix C.

The main motivation behind \(_{}\) in (10) is to establish a new connection between \(\) and LA. This connection is demonstrated in Theorem 4.1, which shows that \(\) is an expectation of the product of **linearized sample-loss deviations** given that parameters are sampled from LA.

**Theorem 4.1** (Connection between IF and LA).: \(\) _in Koh and Liang  can be expressed as_

\[(z,z^{})=_{ p_{}}[ _{^{*}}^{}(z,)_{^{*}}^{}(z^{},)]\] (11)

_where \(_{^{*}}^{}(z,):=_{^{*}}^{}( z,)-_{^{*}}^{}(z,^{*})=g_{z}^{}(- ^{*})\) and \(p_{}\) is the Laplace approximated posterior_

\[p_{}():=(|^{*},H^{-1}).\]

Figure 2: Fig. 2(a): The modified two-circle dataset in Sec. 4. Here, the typical samples are the two outer circle samples with relatively high density, and influential samples correspond to the inner circle, demonstrating relatively low density. The self-influence histogram estimated with \(_{}\), \(\), and \(_{}\) are provided in Fig. 2(b)-2(d). Fig. 2(b): \(_{}\) properly separates the influential samples from the typical samples in the modified two-circle dataset. Fig. 2(c): \(\) mixes typical and influential samples due to the distributional bias in Sec. 3. Fig. 2(d): \(_{}\) accurately represents the bimodal nature of \(_{}\).

The LA was proposed to approximate the posterior distribution with a Gaussian distribution. Recently, it has gained significant attention due to its simplicity and reliable calibration performance [50; 11]. Intuitively, LA is equivalent to the second-order Taylor approximation of log-posterior at \(^{*}\) with Gaussian prior defined as \(p():=(|_{P},^{-1}_{P})\):

\[ p(|S) = p(S|)+ p()- Z\] \[=-L(S,)+ p()- Z\] \[-L(S,^{*})-(-^{*})^{}(H+ _{P})(-^{*})/2\] \[-(-^{*})^{}(H+_{P})( -^{*})/2\]

Here, the training loss represents the negative log-likelihood \(L(S,)=- p(S|)\), and \(Z:= p()p(S|)d\) represents the evidence in Bayesian inference . Similar to the IF, LA becomes computationally intractable when dealing with modern architectures due to the complexity of the Hessian matrix. To address this computational challenge, recent works have proposed various sub-curvature approximations, such as KFAC  and sub-network , which provide computationally efficient alternatives for working with LA.

According to (11), samples with a high degree of self-influence experience significant (linearized) loss changes for the parameters sampled from LA. Furthermore, Theorem 4.1 reveals that the gradients, the origin of distributional bias, arise from the linearizations in (11). Hence, to address the distributional bias of the bilinear IF approximations in Proposition 3.1, we remove the linearizations in (11). This leads us to consider a modified version of \(\), named \(_{}\):

\[_{}(z,z^{}):=_{ p_{} }[_{^{*}}(z,)_{^{*}}(z^{},)].\] (12)

The pseudocode for computing \(_{}\) using Kronecker-Factored Approximate Curvature (KFAC; ) is available in Appendix C.

While \(_{}\) no longer theoretically produces unimodal self-influence, it turns out that it still mixes correct and mislabeled samples, even in toy datasets. To illustrate this problem, we train a fully-connected NN on the two-circle dataset  with a modification as shown in Fig. 2(a): We add ten influential training samples at the center of the two-circle dataset containing 30 train samples per class (circle). These influential samples are highly susceptible (i.e., hard to recover) to leave-one-out retraining as samples of the opposite class are densely located around them. We provide other experimental details in Appendix E.

As discussed in Sec. 3, highly influential samples form a separate mode of high self-influence in the histogram of \(_{}\) (Fig. 2(b)) and \(\) mixes typical and influential samples (Fig. 2(c)) due to the distributional bias of bilinear form. While Proposition 3.1 does not apply to \(_{}\), Fig. 3 shows that \(_{}\) still fails to distinguish between correct and mislabeled samples by severely overestimating the self-influence of typical samples (Fig. 3(a)-3(b)) or squeeze them into a unimodal distribution (Fig. 3(c)), similar to \(\) (Fig. 3(d)).

### Pitfalls of inverse Hessian in non-linear IF approximation

To analyze the limitation of \(_{}\), we reparameterize the sampled parameter from LA as follows

\[=^{*}+H^{-1/2}v=^{*}+_{i=1}^{P}}{}}u_{i}\] (13)

Figure 3: Damping trick for singular Hessian causes severe overestimation of self-influence (Fig. 3(a)). This issue cannot be addressed with a large damping coefficient (Fig. 3(b)). Although truncating small eigenvalues can reduce the overestimation of self-influence (Fig. 3(c)), it introduces another error that even occurs in \(\) (Fig. 3(d)).

From this reparameterization, one can see that for (13) to be valid, all eigenvalues of \(H\) must be positive definite (i.e., all eigenvalues are positive), as Koh and Liang  assumed. However, over-parameterized NNs in practice can contain many zero eigenvalues in their Hessian.

**Proposition 4.2** (Hessian singularity for over-parameterized NNs).: _Let us assume a pre-trained parameter \(^{*}^{P}\) achieves zero training loss \(L(S,^{*})=0\) for squared error. Then, \(H\) has at least \(P-NK\) zero-eigenvalues for NNs such that \(NK<P\). Furthermore, if \(x\) is an input of training sample \(z S\), then the following holds for the eigenvectors \(\{u_{i}\}_{i=NK+1}^{P}\)_

\[g_{z}^{}u_{i}=_{}^{}(z,^{*})^{}f(x,^{*})u_{i}}_{_{K}}=0\] (14)

We refer to Appendix B for the cross-entropy version of Proposition 4.2 with empirical Fisher (EF) matrix, a well-known approximation of the Hessian [31; 22], defined as \(F:=_{n=1}^{N}g_{z}g_{z}^{}\). Note that the over-parameterization assumption (\(NK<P\)) in Proposition 4.2 is prevalent in real-world situations, including the settings of Fig. 1- 3. Also, the zero training loss assumption can be satisfied with sufficient over-parameterization [14; 2]. Accordingly, empirical evidences of Proposition 4.2 have been reported in different scenarios [51; 18].

A simple solution to mitigate this singularity is adding an isotropic matrix to \(H\), known as the **damping technique**: \(H H():=H+_{P}\) for \(>0\). Then, the modified LA sample is

\[_{}:=^{*}+(H())^{-1/2}v=^{*}+_{i=1}^{P}}{+}}u_{i}.\] (15)

Although the damping trick can make all eigenvalues positive in principle, the order of the eigenvalues remains unchanged. Therefore, the null space components (\(g_{z,i}\) for \(i=NK+1,,P\)) in Proposition 4.2 are the most heavily weighted by \(1/\). These heavily weighted null space components do not affect the linearized sample-loss deviations in (11) as follows

\[_{^{*}}^{}(z,_{})=g_{z}^{}(_{ }-^{*})=_{i=1}^{P}}{+}}g_{z }^{}u_{i}=_{i=1}^{NK}}{+}}g_{z}^{ }u_{i}\]

by Proposition 4.2. However, this is not applicable to the sample-loss deviations in (12), since the null space components cause \(_{^{*}}(z,_{})\) to change rapidly (i.e., overestimating the influence of samples). This can be confirmed by observing that setting \(=0.01\) in \(_{}}\) (Fig. 3(a)) leads to significantly overestimating self-influences. To prevent this issue, one can enlarge \(\) (i.e., decrease \(1/\)). However, since the scale of \(\) does not change the order of eigenvalues, the null space components still receive a higher weight than the others. Consequently, \(_{}}\) with \(=100\) (Fig. 3(b)) does not correctly represent the multi-modality of self-influence distribution.

Another way to handle the Hessian singularity is to limit (13) to only the top-\(R\) eigenvalues of Hessian, similar to \(_{}}\). However, this approximation method may result in significant errors due to using only the smallest \(R\) eigenvalues of the inverse Hessian (i.e., \(1/}\)). Consequently, even linearized sample-loss deviations (i.e., \(\)) with \(R=50\) (Fig. 3(d)) suffer from severe performance degradation, compared to full Hessian with damping (Fig. 2(c)).

Compared to damping and truncation, \(_{}}\) (Fig. 2(d)) accurately captures the bimodal self-influence distribution. This effectiveness stems from the differential impact of SGD steps on typical and influential samples : Since typical samples are robust to SGD steps , loss deviation of these samples are small in \(_{}}\). In contrast, influential samples experience significant loss deviations as they are sensitive to SGD steps. As GE makes diverse predictions , the members of GE introduce varying levels of loss deviation for each influential sample. An ablation study exploring the impact of ensemble size is provided in Appendix F.

### Practical advantages of GEX

In addition to the above benefits, \(_{}}\) offers several implementation advantages. First, it can be obtained using **only open-source final checkpoints**, unlike \(_{}}\) and \(_{}}\). This advantage broadens the range of applicable models. Second, batch estimation is easy to implement in \(_{}}\). Projection-based methods like \(_{}}\) and \(_{}}\) require JVP computation, which is only efficient for packages that provide forward-mode auto-differentiation, such as JAX . In contrast, batch computation in \(_{}\) necessitates **only forward computations**, which are efficient in almost all auto-differentiation packages. Since IF is computed for each training sample in most downstream tasks [25; 48; 52], efficient batching is critical for practical applications. As a result, we believe that this distinction will be vital for researchers and practitioners in their work. We provide the complexity analysis of \(_{}\) and other IF approximations in Appendix D and discuss the limitations of our method and broader impacts in Appendix G.

## 5 Experiments

Here we describe experiments demonstrating the usefulness of GEX in downstream tasks. We conducted two experiments for each noisy and clean label setting: Detection (Sec. 5.1) and relabeling (Sec. 5.2) of mislabeled examples for noisy label setting and dataset pruning (Sec. 5.3) and separating data sources (Sec. 5.4) for clean label setting. We refer to Appendix E for experimental details.

### Noisy label detection

In this section, we evaluate the performance of IF approximation methods for detecting noisy labels. We use self-influence as an index for the noisy label of IF approximation methods following Koh and Liang  and Pruthi et al. . A high degree of self-influence indicates mislabeled examples since the removal of mislabeled examples will significantly change prediction or loss.

We train ResNet-18  on CIFAR-10/100  with 10% random label corruption for synthetic noise and CIFAR-N  for real-world noise. We use the "worse label" version of CIFAR-10-N since it corresponds to the highest noise level. We compare \(_{}\) to the following baselines: \(_{}\) with 5 checkpoints and 20 random projections, \(_{}\) with 20 random projections only for the final checkpoint, \(_{}\) with 100 iterations and 20 projections. We assess F-score, and EL2N  as they offer alternative approaches to identifying influential samples. We also evaluate an ablative version of \(_{}\): \(_{}\), which replaces LA with GE for (11) (i.e., \(_{}\) with linear sample-loss deviation). Finally, we report the performance of well-known baselines, Deep-KNN and CL, for comparison. We provide results for cross-influence \((z,z^{})\) in case of \(z z^{}\) in Appendix F.

Table 1 shows that \(_{}\) distinguishes mislabeled samples better than other methods. Furthermore, we find that \(_{}\) is more effective than CL, a state-of-the-art noisy label detection technique. Since CL requires additional \(K\)-fold cross-validation for sample scoring (\(K=2\) used in our experiments), \(_{}\) will be an attractive alternative in terms of both computational complexity and detection performance. Notably, results of \(_{}\) demonstrate that removing linearization is essential for performance improvement of \(_{}\).

One interesting observation in Table 1 is that \(_{}\) did not show improvement compared to simple \(_{}\). Indeed, a similar observation was reported in Table 1 in Schioppa et al. . In contrast, \(_{}\) showed improvements in CIFAR-10 and SVHN compared to \(_{}\), \(_{}\) is computationally expensive since it requires JVP computations for multiple checkpoints. On the other hand, \(_{}\) can be applied in a pure _post-hoc_ style and does not require (framework-dependent)

    &  &  \\  &  &  &  &  \\  Detection method & AUC & AP & AUC & AP & AUC & AP & AUC & AP \\  Deep-KNN & 92.51 \(\) 0.19 & 69.93 \(\) 0.71 & 84.00 \(\) 0.14 & 40.17 \(\) 0.23 & 78.32 \(\) 0.19 & 72.60 \(\) 0.33 & 71.59 \(\) 0.21 & 59.76 \(\) 0.25 \\ CL & 57.60 \(\) 0.30 & 16.27 \(\) 0.20 & 84.16 \(\) 0.10 & 35.76 \(\) 0.50 & 75.94 \(\) 0.02 & 66.50 \(\) 0.09 & 69.49 \(\) 0.15 & 58.69 \(\) 0.23 \\ F-score & 73.34 \(\) 0.07 & 16.27 \(\) 0.09 & 59.18 \(\) 0.21 & 11.04 \(\) 0.05 & 69.39 \(\) 0.06 & 52.89 \(\) 0.06 & 68.95 \(\) 0.11 & 52.29 \(\) 0.14 \\ EL2N & 98.29 \(\) 0.03 & 95.82 \(\) 0.06 & 96.42 \(\) 0.05 & 73.28 \(\) 0.42 & 93.57 \(\) 0.17 & 91.26 \(\) 0.13 & 84.65 \(\) 0.08 & 7.26 \(\) 0.06 \\  \(_{}\) & 62.70 \(\) 0.19 & 17.90 \(\) 0.17 & 79.96 \(\) 0.32 & 26.25 \(\) 0.47 & 56.75 \(\) 0.38 & 45.61 \(\) 0.38 & 67.25 \(\) 0.09 & 54.14 \(\) 0.09 \\ \(_{}\) & 89.59 \(\) 0.14 & 42.62 \(\) 0.37 & 74.99 \(\) 0.25 & 21.62 \(\) 0.26 & 77.24 \(\) 0.45 & 65.17 \(\) 0.68 & 69.04 \(\) 0.28 & 56.41 \(\) 0.31 \\ \(_{}\) & 61.64 \(\) 0.13 & 17.05 \(\) 0.18 & 77.20 \(\) 0.35 & 22.61 \(\) 0.42 & 56.83 \(\) 0.40 & 45.63 \(\) 0.40 & 65.57 \(\) 0.12 & 53.26 \(\) 0.11 \\  \(_{}\) & 64.11 \(\) 0.34 & 18.34 \(\) 0.36 & 76.06 \(\) 0.36 & 22.26 \(\) 0.47 & 56.88 \(\) 0.29 & 45.67 \(\) 0.33 & 65.68 \(\) 0.15 & 52.66 \(\) 0.13 \\ \(_{}\) & **99.74 \(\) 0.02** & **98.31 \(\) 0.06** & **99.33 \(\) 0.03** & **96.08 \(\) 0.12** & **96.20 \(\) 0.03** & **94.89 \(\) 0.04** & **89.76 \(\) 0.01** & **86.30 \(\) 0.01** \\   

Table 1: Area Under Curve (AUC) and Average Precision (AP) for noisy label detection tasks on four datasets. Due to the high time complexity associated with sample-wise gradients, we do not repeatedly measure the self-influence of \(_{}\).

JVP computation. Motivated by \(_{}\), one can propose a purely post-hoc \(_{}\) using checkpoints generated by GE. We provide an ablation study for this setting in Appendix F.

To verify the scalability of results, we train Vision Transformer (ViT; ) and MLP-Mixer  on ImageNet  with 10% label corruption and evaluate the performance of IF approximation methods. Table 2 shows that the performance gap between \(_{}\) and other baselines is still large. Specifically, in this scenario, F-score fails since some samples have never been correctly predicted during pre-training (i.e., no forgetting events for such samples). We provide additional results for other vision datasets (MNIST  and SVHN ) and text classification settings in Appendix F.

### Relabeling mislabeled samples

Following up on the detection task in Sec. 5.1, we improve classification performance by relabeling mislabeled samples following Kong et al. . To this end, Otsu algorithm  was used to find a threshold that distinguishes between noisy and clean labels, given the self-influence of approximation methods. Since the Otsu method has no inputs other than (influence) distribution, practitioners can apply it for noisy label settings without onerous hyperparameter optimization. Following Kong et al. , we relabel training samples as follows:

\[_{k}=\{0,&k=m,\\ _{_{k}}&}{{}}},&.\] (16)

where \(m\) is the training sample's original (corrupted) label and \(_{i}\) is the predicted probability for \(i\)-th class of the training sample. Therefore, the relabel function in (16) masks the original (noisy) label and re-distributes the remaining probabilities. We provide additional results for MNIST and SVHN in Appendix F.

Table 3 shows the relabeled test accuracy with the other settings for comparison. A critical observation in Table 3 is that not all methods achieve performance improvements with relabeling. Otsu algorithm did not find an appropriate threshold for mislabeled examples for these methods. In contrast, since \(_{}\) can separate mislabeled samples from the self-influence distribution, performance improvements were obtained through relabeling. We provide additional results for ImageNet in Appendix F.

### Dataset pruning

We evaluate methods in the previous section on data pruning task  to validate the efficacy of \(_{}\) in clean label settings. We quantify the importance of training samples based on self-influence following Feldman and Zhang : We prune low self-influence samples as they are relatively easy samples that can be generalized by learning other training samples. We use networks and datasets in Sec. 5.1 without label noise. We provide additional results for MNIST and SVHN in Appendix F.

As shown in Fig. 4, \(_{}\) consistently detects samples that can be pruned. It is worth noting that among the IF approximation methods considered, only \(_{}\) is comparable to the well-known state-of-the-art methods, F-score and EL2N. This suggests that existing IF approximation methods may experience a performance decrease in downstream tasks because of the errors caused by distributional bias.

    &  &  \\  Detection method & AUC & AP & AUC & AP \\  F-score & 1.56 & 9.44 & 1.07 & 9.21 \\ EL2N & 87.28 & 38.37 & 88.47 & 41.63 \\  \(_{}\) & 76.09 & 12.08 & 82.36 & 21.86 \\ \(_{}\) & 72.22 & 15.09 & 75.01 & 16.62 \\ \(_{}\) & 73.95 & 16.09 & 82.16 & 21.83 \\  \(_{}\) & **99.39** & **95.22** & **98.73** & **90.65** \\   

Table 2: Noisy label detection performance for ImageNet 

    &  &  \\  & CIFAR-10 & CIFAR-100 & CIFAR-10N & CIFAR-100N \\  Clean label acc. & 95.75 \(\) 0.06 & 79.08 \(\) 0.05 & 95.75 \(\) 0.06 & 79.08 \(\) 0.05 \\ Noisy label acc. & 90.94 \(\) 0.15 & 72.35 \(\) 0.17 & 68.63 \(\) 0.32 & 55.50 \(\) 0.09 \\  Detection method &  \\  Deep-MNIST & 91.58 \(\) 0.10 & 66.12 \(\) 0.27 & 69.12 \(\) 0.25 & 50.03 \(\) 0.19 \\ CL & 91.11 \(\) 0.10 & 75.25 \(\) 0.13 & 30.52 \(\) 0.20 & 31.17 \(\) 0.02 \\ F-score & 78.94 \(\) 0.39 & 55.67 \(\) 0.18 & 53.50 \(\) 0.28 & 44.34 \(\) 0.21 \\ EL2N & 89.40 \(\) 0.10 & 61.72 \(\) 0.18 & 72.01 \(\) 0.51 & 47.58 \(\) 0.22 \\  \(_{}\) & 90.94 \(\) 0.09 & 72.42 \(\) 0.16 & 68.55 \(\) 0.17 & 55.47 \(\) 0.08 \\  \(_{}\) & 91.24 & 72.07 & 68.36 & 54.87 \\ \(_{}\) & 90.82 \(\) 0.06 & 71.79 \(\) 0.15 & 68.12 \(\) 0.23 & 55.20 \(\) 0.06 \\  \(_{}\) & 91.04 \(\) 0.09 & 72.50 \(\) 0.08 & 68.67 \(\) 0.02 & 55.37 \(\) 0.14 \\  \(_{,}\) & 91.04 \(\) 0.16 & 70.08 \(\) 0.12 & 68.44 \(\) 0.08 & 55.51 \(\) 0.21 \\ \(_{}\) & **93.54 \(\) 0.05** & **75.04 \(\) 0.10** & **73.94 \(\) 0.24** & **57.13 \(\) 0.10** \\   

Table 3: Relabeled test accuracy for mislabeled samples

### Separation of data sources

We also evaluate IF approximations on separating heterogeneous data sources following Harutyunyan et al. . One often combines multiple datasets to improve generalization. In general, these datasets differ in their informativeness. Therefore, IF can be used to determine which dataset is the most essential based on the informativeness of each dataset. In this experiment, we train ResNet-18 on a mixed dataset of MNIST and SVHN, with 25K random subsamples for each training dataset. Then, we use self-influence as an index for the informativeness following Harutyunyan et al. .

Table 4 shows detection metrics for MNIST and SVHN separation using various IF approximations (and other baselines in previous sections). In contrast to the noisy label detection experiment in Sec. 5.1, many IF approximations in this experiment fail to distinguish between data sources: MNIST and SVHN can only be significantly separated by \(_{}\), \(_{}\), and F-score. Assuming only the final checkpoint is available, only \(_{}\) can be applied among them.

## 6 Conclusion

In this work, we studied the oversimplification of influence distributions due to their bilinear approximations. To mitigate this bias, we developed a non-linear IF approximation, GEX, with GE. Empirically, GEX consistently outperforms the standard IF approximations in various downstream tasks in practice. Also, GEX is user-friendly as it only requires the final checkpoint of pretraining and excludes JVP computation, often constrained by frameworks, for efficient batch estimation. With these advantages, GEX can be used as a practical tool for researchers and practitioners.