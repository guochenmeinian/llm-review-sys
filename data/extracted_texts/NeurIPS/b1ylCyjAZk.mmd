# Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models

Javier Gonzalez

Gonzalez.Javier@microsoft.com

Microsoft Research, Cambridge

Aditya V. Nori

Aditya.Nori@microsoft.com

Microsoft Research, Cambridge

###### Abstract

Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. However, there is an ongoing debate about the extent to which LLMs are capable of actual _reasoning_. Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS). This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures. By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS. Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples.

## 1 Introduction

Large language models (LLMs) have revolutionized the way we interact with technology, enabling more natural and intuitive communication between humans and computers in applications like writing assistants , sentiment analysis in social media , healthcare  and many others. Despite the surge of interest and recent breakthroughs , the ability of LLMs to _reason_ about real-world problems as humans do continues to be a topic of intense research .

Reasoning is a cognitive process that involves drawing conclusions, making judgments, or forming inferences based on facts or premises. This process has been explored from various perspectives. _Symbolic reasoning_ involves the manipulation of symbols that represent ideas or objects and it is often used in mathematics and logic to represent numerical values or logical propositions. _Causal reasoning_ focuses on discerning the relationship between a cause and its effect, aiming to understand how certain events can impact other. Other forms of reasoning include _inductive reasoning_ (making broad generalisations from specific observations), _deductive reasoning_ (applying general principles to specific cases), and _abductive reasoning_ (forming the best hypothesis based on incomplete information).

In the realm of LLMs, reasoning is typically understood to be the ability of these models to demonstrate _emergent_ capabilities that surpass mere statistical pattern recognition in the training set. It entails systematically breaking down problems into a logical sequence of smaller, manageable steps and then processing these steps internally to arrive at accurate conclusions that are grounded in reality. This concept is the foundation for techniques such as _chain of thoughts prompting_, which aim toeach LLMs how to reason by providing examples where problems are solved through a sequence of smaller steps.

Assessing the reasoning abilities of LLMs involves distinguishing between two aspects: the accuracy with which an LLM solves a problem, and its capacity to understand and process the fundamental elements that lead to that solution. Judea Pearl, in his hierarchy of causality , asserts: _"Only machines that can correctly perform correlations, interventions and counterfactuals will have reasoning abilities comparable to human"_. As demonstrated in , while LLMs are remarkable in using learnt patterns from their training data to generate correct answers (correlations), they latter when faced with hypothetical/imaginary scenarios that were not part of their training (counterfactuals). This is depicted in Figure 5, which presents a straightforward arithmetic problem (this is the **direct prompt** in the figure). Both GPT-35-turbo and GPT-4 can accurately determine the divisibility of numbers by 6, suggesting at first glance that they can reason about divisibility. However, when the questions are framed in a counterfactual manner (this is the **counterfactual prompt** in the figure), only GPT-4 maintains a low error rate, indicating its superior ability to handle such reasoning tasks.

In this paper, we introduce a systematic method to assess the reasoning capabilities of LLMs by examining the concepts _necessity_ and _sufficiency_, which are key elements of logical reasoning and have been studied in multiple fields such as logic, probability, and causality . In propositional logic, a sufficient condition is defined as \(X Y\), indicating that the presence of \(X\) ensures the occurrence of \(Y\). On the other hand, a necessary condition is defined as \(Y X\), signifying that the occurrence of \(Y\) necessitates the prior occurrence of \(X\). We focus on the probabilistic interpretations of necessity and sufficiency . The _probability of necessity_ (PN) between two boolean variables \(X\) and \(Y\) is defined as \((x,y):=(y^{}_{x^{}}|x,y)\). Here, \(y^{}_{x^{}}\) represents the counterfactual value of \(Y=y^{}\), had \(X\) been set to a different value \(x^{}\). By conditioning on both \(X=x\) and \(Y=y\), this measure captures probability of observing a different outcome in the absence of the event \(X=x\). The _probability of sufficiency_ (PS), on the other hand, is defined as \((x,y):=(y_{x}|x^{},y^{})\) and measures the probability that \(X=x\) results in \(Y=y\), for cases where both originally had different values.

We show that when a problem can be solved via a reasoning graph of boolean conditions, denoted by \(\), the PN and PS can be computed using a causal model underlying \(\). As described in , the exact computation of PN and PS requires samples from the (causal) data generative model, counterfactual data (experiments) as well as other monotonicity assumptions. As a _reasoning test_, we statistically compare the true PN and PS measures (computed by sampling from the original and the intervened graph) with those simulated via factual and counterfactual datasets generated by an LLM. Figure 2 presents an informal illustration of the reasoning test advocated in this paper, focusing on the specific problem of determining whether a number \(N\) is divisible by 6. The test relies on the reasoning principle that: _"A natural number \(N\) that is divisible by both \(2\) and \(3\) is also divisible by \(6\)"_. This logic is represented in a reasoning graph \(\) that links the conditions \(C_{2}\) (divisibility by \(2\)) and

Figure 1: Illustration of the actual vs. perceived reasoning abilities of GPT-2, GPT-35-turbo and GPT-4 for a simple arithmetic problem. We posed two distinct types of questions (direct and counterfactual) to the models, each repeated 10 times, for every {number} from 1 to 50. All three models showed an inflated sense of reasoning capability when answering the direct questions. The discrepancy is especially pronounced in GPT-35-turbo, which performed nearly flawlessly on direct questions, but experienced a surge in error rate, exceeding 25%, when handling counterfactual questions.

(divisibility by \(3\)) to the conclusion \(C_{6}\) (divisibility by \(6\)). We test the reasoning abilities of an LLM using natural numbers \(N\) from \(1\) to \(400\). This is shown in Figure 2(A).

As indicated in Figure 2(B), we create two sets of data based on \(\). The first is a factual dataset (\(_{F}\)) which captures whether each number \(N\) satisfies conditions \(C_{2}\) and \(C_{3}\). The second is a counterfactual dataset, (\(_{CF}\)), which assumes condition \(C_{3}\) is always true and then records whether each number \(X\) would satisfy \(C_{6}\) under this assumption/intervention (realised by \(do(C_{3}=True)\) in the figure). For the LLM being evaluated, we also produce two datasets. The first, \(_{F}^{LLM}\), documents the LLM's response for \(C_{6}\) for each number \(N\), when the prompt is based on the reasoning graph \(\). The second, \(_{CF}^{LLM}\), involves a hypothetical scenario where we assume \(C_{3}\) is true and then record the LLM's prediction for \(C_{6}\) given this "counterfactual prompt". This process is repeated multiple times (several answers from the LLMs are collected from each prompt). We assess the LLM's reasoning capability by comparing the estimated (distribution of) PN and PS from the \(_{F}^{LLM}\) and \(_{CF}^{LLM}\) datasets with the actual values derived from \(_{F}\) and \(_{CF}\) datasets. Figure 2(C) displays these comparisons, plotting PN vs. PS. The closer the estimated PN/PS values to the actual PN/PS values, the better it is at reasoning. In this case, LLM 2 demonstrates better reasoning abilities than LLM 1.

**Related work:** Reasoning in LLMs has been studied from multiple perspectives.  presents an overview paper that elucidates key reasoning concepts utilised by LLMs.  examines the similarity between reasoning with a language model and planning with a world model, proposing a novel reasoning framework that redefines the LLM as both a world model and a reasoning agent. Various studies  have focused on assessing the reasoning and problem-solving abilities of LLMs, yet none have used the probabilities of causation as the primary objects of computation as done in our research.  carries out a series of experiments to show that LLMs can indeed derive benefits from reasoning errors, offering potentially cost-effective strategies by using mistakes to bolster reasoning capabilities. Recent research indicates that LLMs like GPT-3.5 and GPT-4 are effective at causal reasoning tasks, including pairwise causal discovery . These models have achieved state-of-the-art performance on multiple causal benchmarks, outperforming existing algorithms. Nevertheless, LLMs also exhibit unpredictable failure modes, and currently, they are not capable of discovering new knowledge or making high-stakes decisions with a high level of precision .

Figure 2: Reasoning test for assessing an LLM’s reasoning abilities. **A)** Divisibility rule and the corresponding reasoning graph. **B)** Dataset generation for computing PN and PS. **C)** Analysis comparing actual values of PN and PS with PN and PS estimates for the LLM-generated data.

**Contributions:** This paper presents two main contributions.

1. A novel, theoretical and practical, framework to evaluate the reasoning capabilities of language models using the probabilistic of causation, specifically the probability of necessity and the probability of sufficiency. Our approach is unique in the sense that it allows to differentiate generalization by reasoning from merely replicating statistical patterns in the training data.
2. Empirical tests on various reasoning problems as well as several insights about the reasoning abilities of language models in the GPT family.

## 2 LLMs as abstract machines

As described by the Hex framework , an LLM functions as an abstract machine that uses natural language as an interface. In this section, we introduce the core elements of this framework, which will subsequently enable us to define an LLM's internal representation of PN and PS.

We define a _problem_ as a query-state pair \((Q,)\). The state \(\) is a mapping defined by \(:\), which assigns values from a specified domain \(\) to a set of variables \(=\{V_{1},,V_{n}\}\). The query \(Q:2^{} 2^{}\) is a mapping that transforms an input state \(\) to a well-defined output state. To _solve a problem_ is to calculate \(_{1}=Q(_{0})\), where \(_{0}\) and \(_{1}\) represent the states before and after the query \(Q\) is applied. To clarify this, we consider the following example:

**Example 1**.: _"Given that a natural number divisible by both 2 and 3 is also divisible by 6, determine whether the number 10 is divisible by 6."_

To solve Example 1, we apply the query \(Q\) to the state \(_{0}=\{N 10,C_{6}\}\), where \(Q=\,.\,((N) 0)((N) 0)\)1. This results in a final state \(_{1}=\{N 10,C_{6} False\}\), thereby resolving the problem with \(_{1}(C_{6})=Q(_{0})=\).

We now turn to the question of how an LLM solves a problem defined by a query-state pair \((Q,_{0})\). This process involves three essential steps as illustrated by Figure 3:

1. First, an abstraction mapping translates the initial state \(_{0}\) into a latent state \(_{0}\) via a _prompt_.
2. Next, the LLM processes (via the query \(Q^{LLM}\)) this latent state \(_{0}\).
3. Finally, the output mapping transforms the LLM output latent state \(_{1}\) back into a concrete state, producing the final _output_\(_{1}\).

Formally, solving a problem \((Q,_{0})\) with an LLM can be described as a sequence of function applications resulting in the output \(_{1}=( Q^{LLM})(_{0})\). To illustrate this, the problem statement is given as a prompt input to GPT-4 . The response from GPT-4 is "False", which matches the result obtained by applying the query \(Q\) directly to the input state \(_{0}\). When both the direct application of \(Q\) and the LLM computation yield the same answer, we say that the diagram, as shown in Figure 3, is commutative-meaning that following either the dotted line or the solid lines lead to the same result. For a more in-depth explanation of this framework, please refer to .

Figure 3: The Hex diagram depicts two approaches for solving the problem \((Q,_{0})\) outlined in Example 1. The dotted path corresponds to the actual process of solving the problem, while the solid path represents the one taken by the LLM.

## 3 Probabilities of causation for an LLM

To assess the reasoning abilities of an LLM, we must link its generated responses to the actual reasoning processes that produced those responses. For a problem \((Q,)\), we postulate the existence of a causal model \(_{}\) defined over variables in \(\), and by a set of structural equations and endogenous variables. For a detailed introduction to causal models, refer to Appendix A. Additionally, the seminal work by Pearl  on causality provides foundational insights on this area. Here, we are particularly interested in causal models that represent the logical steps involved in problem-solving. However, it is important to note that the concept of a causal model is broadly applicable beyond this specific application.

We assume that \(=\{X,Y,Z\}\), which includes \(X\) and \(Y\) as boolean variables, and \(Z\) as a variable (which may be multivariate) that encompasses all necessary factors that are required to understand how an intervention on \(X\) would affect \(Y\). In the context of causality, this means that the distribution \((Y|do(X=x^{}))\), where _do_ denotes the intervention operator defined in , is identifiable. This means we can predict the outcome for \(Y\), and that the counterfactual \(Y_{X=x^{}}\), that can be read as _"the value of \(Y\) had \(X\) been \(x^{}\)"_, is well-defined. For further details, please refer to Appendix A. For ease of exposition in the following text, we will simplify our notation by omitting the explicit reference to \(Z\). Therefore, we will denote \(Y_{X=x}(Z=z)\) more succinctly as \(Y_{X=x}\).

As studied in , if \(Y\) is monotonic with respect to \(X\), then PN and PS can be computed as follows:

\[(x,y)=(y)-(y|do(x^{}))}{ (x,y)}}(x,y)=(y |do(x))-(y)}{(x^{},y^{})}. \]

To estimate PN and PS, we need two different types of datasets. The first is a _factual_ dataset \(_{F}=\{x_{i},y_{i},z_{i}\}_{i=1}^{n}\), which is used to infer \((y)\), \((x,y)\) and \((x^{},y^{})\). The second dataset \(_{CF}=\{x_{i},Y_{X=x_{i}},z_{i}\}_{i=1}^{n}\) is a _counterfactual_ one, and is necessary to determine \((y|do(x))\) and \((y|do(x^{}))\).

There are various methods to acquire the datasets \(_{F}\) (factual) and \(_{CF}\) (counterfactual). For a physical process, the usual method would be through observation and experimentation. However, in this paper, we presume access to a comprehensive reasoning graph that is equivalent to a causal model \(_{}\). This allows us to simulate and generate the \(_{F}\) and \(_{CF}\) datasets. Both \(_{}\) and the sub-model \(_{,do(X=x)}\) define two distinct joint probability distributions \(_{_{}}\) and \(_{_{,do(X=x)}}\) over \(X\), \(Y\) and \(Z\). We obtain the datasets \(_{F}\) and \(_{CF}\) by sampling from these respective probability distributions. These datasets are then used to calculate PS and PN using Eq. (1).

### LLM-based counterfactuals

_Can an LLM reason in a manner that is consistent with \(_{_{}}\)?_ In Example 1, we obtained consistent answers (that is, the corresponding Hex diagram commutes) for a direct divisibility question. However, to evaluate the reasoning abilities of the LLM, it is crucial that this consistency is also observed when the queries are framed in a counterfactual manner. This is necessary to ensure that the LLM can apply its reasoning to imaginary situations that are unlikely to be present in the training set, demonstrating its ability to generalise based on a correct internal representation of the reasoning logic of the problem. Practically, this means employing the LLM as a "counterfactual data simulator", where the data it generates under these hypothetical conditions are used to estimate PN and PS.

**Definition 1** (Counterfactual query).: _Consider a problem \((Q,_{0})\), with \(_{0}=\{X x,Y y,Z z\}\) being an initial state. Let \(_{}\) be a causal model over the variables \(\). We can then define a counterfactual query \(Q^{}\) as follows: \(Q^{}(_{0})=\{X x^{},Y Y_{X=x^{}},Z z\}\)._

In other words, a counterfactual query updates two variables of the state: it sets \(X\) to its new value \(x^{}\), and \(Y\) to the counterfactual \(Y_{X=x^{}}\). An LLM-based counterfactual \(Y_{X=x^{}}^{LLM}\) is computed as follows:

\[Y_{X=x^{}}^{LLM}=( Q^{ LLM})(_{0})(Y)\]

where \(_{0}=\{X x,Y y,Z z\}\), and \(Q^{ LLM}\) is a counterfactual query. This entire process simulates counterfactual reasoning within the LLM, and is facilitated through textual prompts that are structured to elicit the desired counterfactual outcome.

**Definition 2** (Counterfactual prompt).: _A counterfactual prompt is a textual encoding of a counterfactual query for some initial state \(_{0}\)._Figure 1 shows an example of a counterfactual prompt. To create a comprehensive dataset \(^{LLM}_{CF}\) of counterfactuals based on an LLM, we start with the factual dataset \(^{LLM}_{F}\). From this dataset, we generate a set of initial states \(_{0,i}=\{X x_{i},Y y_{i},Z z_{i}\}\), which serve as the basis for deriving counterfactuals using the LLM. To compute PN and PS, we substitute \(_{F}\) with \(^{LLM}_{F}\) and \(_{CF}\) with \(^{LLM}_{CF}\) in Eq. (1).

**Example 1 revisited**. We construct four distinct datasets using every integer in \(\): the factual dataset \(_{F}\), the counterfactual dataset \(_{CF}\), the LLM-based factual dataset \(^{LLM}_{F}\), and the LLM-based counterfactual dataset \(^{LLM}_{CF}\). These datasets, shown in Figure 4 (_Left_) are generated following the causal model shown in Figure 2, its modified version with interventions, and the LLM prompting methods mentioned previously. We obtain \(=1\) and \(=0.50\) for the datasets \(_{F}\) and \(_{CF}\). On the other hand, \(^{}=0.984\) and \(^{}=0.505\), when we use the factual \(^{LLM}_{F}\) and counterfactual \(^{LLM}_{CF}\) datasets generated by GPT-4.

### Counterfactual consistency in LLMs

**Definition 3** (\(\)-counterfactual consistency).: _Consider a structural causal model \(_{}\) with variables \(=\{X,Y,Z\}\). Let \(_{X=x}(Z)\) be a function that generates counterfactuals for \(Y\). We say that \(\) is \(\)-counterfactual consistent with \(_{}\) if the following condition is satisfied: \(_{(X,Y,Z)}[_{X=x}(Z=z) Y_{X=x}(Z=z) ]\), where \( 0\)._

\(\)-counterfactual consistency defines the limit error rate for counterfactuals produced by \(_{X=x}(Z=z)\). This error rate should ideally be zero for an LLM that exhibits flawless reasoning abilities. The following lemma specifies the conditions necessary for this property to hold (the proof can be found in Appendix D).

**Lemma 1**.: _Let \(_{}\), with variables \(=\{X,Y,Z\}\), be a structural causal model for a problem \((Q,_{0})\), and let \(M\) be an LLM that generates counterfactuals for \(Y\). Then \(M\) is \(\)-counterfactual consistent with \(_{}\) if and only if its associated Hex diagram for the problem \((Q^{},_{0})\), where \(Q^{}\) is the counterfactual version of \(Q\), is commutative for all admissible values of \(X\), \(Y\) and \(Z\)._

Lemma 1, provides a theoretical criterion to test the consistency of an LLM with some reasoning graph described by \(_{}\). As we show next in the experimental section, the commutability of the diagram is tested in expectation by sampling over the variables in \(\).

## 4 Empirical illustrations

We focus on three math problems, each with a progressively higher level of difficulty.

_Divisibility by 6_ (Div6): We compute the PN and PS to determine the impact that an integer \(N\)'s divisibility by \(3\) (denoted as \(C_{3}\)) has on its divisibility by \(6\) (denoted as \(C_{6}\)). For our analysis, we consider \(N\).

Figure 4: _Left_: Contingency tables for \(_{F}\), \(_{CF}\) and \(^{}_{CF}\) in Example 1. _Right_: Reasoning graphs for the other math problems in this paper. C-type nodes in the graph represent boolean conditions. See Appendix C for details.

Even sum of integers_ (EvenSum): We examine scenarios where the sum of three integers \(M\), \(N\) and \(T\) is even. This can occur under two conditions: when all three integers are even, or when one is even and the other two are odd. We evaluate PN and PS for impact that \(M\) being odd or even (\(C_{m}\)) has on the resulting sum being odd or even (\(C_{mnt}\)). For our analysis, we consider all possible values for \(M\), \(N\) and \(T\), with each integer ranging from \(1\) and \(8\).

_Candy party_ (CandyParty): In this hypothetical scenario, Rafa is having his birthday party with two guests, Lara and Emma. They have \(20\) candies to distribute among themselves. The party will be considered 'happy' if the candy distribution satisfies at least one of the following conditions: (i) Each person gets the same number of candies, or (ii) Rafa gets more candies than both Lara and Emma, but Lara and Emma each receive an equal number of candies, with both receiving at least one candy each. We compute the PN and PS for the impact that Lara and Emma receiving an equal number of candies (denoted as \(C_{lm}\)) has on the party being 'happy' (denoted as \(C_{h}\)).

A fourth problem (ConPref) is included in Appendix B. The reasoning graphs for the problems EvenSum and CandyParty are shown in Figure 4 (_Right_). The structural equations corresponding to each of these graphs can be found in the Appendix C. We estimate the PN and PS for each of these problems using three difference language models: GPT-2, GPT-3.5-turbo and GPT-4 . Our objective is to investigate whether the ability to reason, as conceptualised in this paper, _emerges_ as the complexity and size of the models grow. While similar evaluations could be conducted using other families of LLMs, such as Llama , Gemini , Phi , etc., we have chosen to limit our analysis to the GPT series here for the sake of a clearer and more straightforward exposition, but more results are available in Appendix J.

To assess the reasoning abilities of various models, we use the following metrics:

1. _Factual Inconsistency Rate_ (FIR): This measures the rate of inconsistencies when the models respond to factual queries.
2. _Counterfactual Inconsistency Rate_ (CIR): Similar to FIR, but this metric measures inconsistencies in responses to counterfactual queries.

For a detailed explanation of these metrics, please refer to Appendix H. We estimate the standard errors of FIR and CIR by examining the variations in outputs across multiple model responses. We take this aspect into account by collecting multiple answers from the models and propagating the stochasticity of the answers to the computation of PN and PS. Additionally, we capitalise on this variability to construct the densities over the inferred PN and PS. This process involves generating \(500\) bootstrap samples from the model's factual and counterfactual responses2. From these densities, we calculate \(\)-PN-overlap, which measures the concentration of the probability distribution within a radius \(\) around the actual PN, and \(\)-PS-overlap does the same for PS3.

Figure 5: _Left_: Heatmaps comparing the consistency of data generated by GPT-2, GPT-3.5-turbo, and GPT-4 for the Div6 problem. Each heatmap cell represents the error rate of the corresponding model for each element of the problem across 10 replicated tests. _Right_: Sensitivity of the simulated PN relative to varying levels of random noise introduced in the true counterfactuals.

### Factual vs. counterfactual predictions

Figure 5 (_Left_) illustrates the alignment between the outputs of GPT-2, GPT-3.5-turbo, and GPT-4, and the factual predictions and counterfactuals for the Div6 problem. The shading within each cell of the heatmap indicates the degree of mismatch between model-generated outputs and the true information, with the colour intensity reflecting the level of disagreement based on the \(10\) answers from the models. As highlighted in Figure1--where the average disagreement across the first 100 columns of these heatmaps informs the results--more sophisticated models like GPT-4 demonstrate a closer match with the counterfactuals derived from the true reasoning graph. For similar comparisons involving other problems, please refer to Appendix I.

One might wonder if the evaluation of reasoning truly requires PN and PS, or if it could be sufficiently assessed by examining only the inconsistency rates in factual/counterfactual data. Figure 5 (_Right_) underscores the importance of PN and PS. It presents the estimated distributions of PN for the Div6 problem, based on 500 replicates under five scenarios where true counterfactuals are randomly altered with probabilities \(0.005\), \(0.001\), \(0.05\), \(0.1\) and \(0.2\). As we might anticipate, the greater the deviation from a dataset free of counterfactual errors, the more significant the discrepancy from the actual PN \(=1\) for this example. Notably, even minor perturbations can lead to substantial shifts in the estimated PN. For example, with a \(0.05\) probability of counterfactual perturbation, the estimated PN varies between 0.5 and 0.9. This suggests that relying solely on counterfactual errors could lead to an overestimation of the models' reasoning abilities, particularly their understanding of the necessary and sufficient conditions within a problem. Furthermore, a counterfactual error rate of \(0.2\) in this example results in entirely inconsistent (negative) probabilities due to the mismatch between the conditional and interventional distributions, as defined in Eq. 1.

### Evaluation of LLMs reasoning

We computed the CIR, FIR, \(\)-PN-overlap, and \(\)-PS-overlap for the problems Div6, EvenSum and CandyParty using GPT-2, GPT-3.5-turbo, and GPT-4.

Figure 6 illustrates the estimated PN and PS for each problem, obtained through bootstrap resampling. Each density is labeled with the model that was used to generate the data for those results. The true values of the PS and PN in each problem is marked with a cross. A model is considered capable of reasoning if the PN-PS density estimates overlap with the true probabilities of causation. Such an overlap was only achieved by GPT-4 for Div6 problem. Other results varied, indicating generally weak reasoning abilities. Negative values of PN and PS in several instances, are due to inconsistencies in \(_{F}^{LLM}\) and \(_{CF}^{LLM}\) as detailed in Section 4.1.

Figure 7 (_Left_, _Centre_) features the \(\)-PN-overlap and \(\)-PN-overlap curves for all models and problems, where ideal reasoning corresponds to the metrics equalling one for any value of \(\). GPT-4 shows this level of reasoning for the Div6 problem. However, GPT-2 had an accurate PN for Even-Sum, but the PS estimates were notably less accurate.

Figure 6: True PN and PS vs. inferred PN and PS using GPT-2, GPT-35-turbo and GPT-4. The densities of the estimated probabilities capture the uncertainty associated with the responses by each model.

Figure 7 (_Right_) presents the values of CIR and PIR (with the standard deviations included brackets). An emerging trend towards reasoning is observed in the GPT family of models, particularly seen with GPT-4 for the Div6 problem. An intriguing question is whether future versions of these models will similarly approach the true PN and PS for other problems as well.

## 5 Discussion

The primary objective of this paper was to explore and understand the reasoning abilities of LLMs, which is essential for their successful deployment in a range of applications. Given the growing dependence on LLMs for complex reasoning tasks, such as mathematics, programming, or strategic planning, understanding this is crucial. To evaluate these reasoning abilities, we introduced a novel framework that employs probabilistic measures of necessity and sufficiency, and find that while various models (GPT-2, GPT-3.5-turbo, and GPT-4) can replicate aspects of reasoning to some degree, they often falter when it comes to counterfactual reasoning. What makes our approach unique is that we test the models with scenarios where generalization by reasoning, rather than replicating statistical patterns in the training data, is required to provide correct answers. Notably, the ability to reason, as defined in this paper, does improve with more complex models, yet it is still far from flawless. This observation leads to the question of whether future versions of these models will achieve perfect reasoning. Our results are significant, as they reveal the limitations of LLMs, and emphasize the need for further research to enhance their reasoning capabilities.

In general, reasoning goes beyond the math examples that we have included in the paper. We believe that the same theory and tools can be used in other domains. The Hex framework, that serves as a mathematical framework to formalize our ideas, requires the definition of a query, state, and an abstract execution machine that is used to predict how the state is modified given the query. In this

Figure 7: _Left_, _Centre_: Reconstruction of the \(\)-PN-overlap and \(\)-PS-overlap curves for GPT-2, GPT-35-turbo and GPT-4. Ideal reasoning is achieved when the overlap is one for all values of \(\). _Right_: Visualization of FIR and PIR. Ideal reasoning is attained when both metrics are zero (denoted by a \(\)).

framework, for instance, one could think about problems in vision, where the elements of the state are the objects in an image, and the query corresponds to a counterfactual query that describes an intervention in the environment. The concepts of necessity and sufficiency still apply in this scenario. In an image where an object is removed or altered, the framework can help determine the impact of this change on a property of the overall scene. We believe that this approach and will be key in other fields such as robotics, and or social sciences, where understanding the necessity and sufficiency between different elements is crucial for accurate reasoning and decision-making.

**Limitations**: Our approach has several limitations that we acknowledge, but did not address within the scope of this research.

1. _Dependence on reasoning graphs_: our method requires access to reasoning graphs. This requirement may hinder our ability to fully understand the reasoning abilities of LLMs in situations where it is challenging to derive relationships, including causal ones.
2. _Boolean variable restriction_: our method is designed to work with boolean valued variables, which is restrictive, particularly for cases involving multiple states or conditions occurring at the same time. However, we believe that this issue can be addresss with further research.
3. _Prompt-dependent results_: The findings we report are based on an LLM's reasoning abilities as determined by two specific types (factual/counterfactual) of prompts that we used. Of course, other techniques like chain-of-thought prompting could be used, but our focus remains on establishing a consistent baseline. Future work could explore the impact of different prompting strategies on reasoning performance, potentially leading to more refined and effective methods for evaluating and enhancing reasoning capabilities in LLMs. Our experiments did not aim to fine-tune these prompts or to 'optimise reasoning'--a separate area of ongoing research. Instead, our goal was to offer valuable insights that can aid the community in developing new benchmarks and employing LLMs responsibly.

**Broader impact**: Evaluating the reasoning capabilities of LLMs is essential as it significantly influences their effectiveness in various domains. In education and research, it is important for the model to be able to provide accurate explanations and to formulate meaningful hypotheses. In the commercial sector, the effectiveness of automated processes/systems relies heavily on how well the model can reason. When it comes to accessibility, the model must be able to understand and meet diverse user needs, which hinges on its reasoning ability. Moreover, identifying and mitigating biases in AI systems--a key aspect of ethical and equitable AI--requires a detailed examination of the models' reasoning processes. Therefore, while LLMs hold immense promise, ensuring their responsible and beneficial use is predicated on a thorough appraisal of their reasoning abilities. We believe that our research is an important step in this direction.