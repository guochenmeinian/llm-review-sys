# FUSE: Fast Unified Simulation and Estimation for PDEs

Levi E. Lingsch

Seminar for Applied Mathematics

& AI Center, ETH Zurich

levi.lingsch@ai.ethz.ch &Dana Grund

Institute for Atmospheric

and Climate Science, ETH Zurich

dana.grund@ethz.ch &Siddhartha Mishra

Seminar for Applied Mathematics

& AI Center, ETH Zurich

siddhartha.mishra@ethz.ch &Georgios Kissas

AI Center

ETH Zurich

gkissas@ai.ethz.ch

###### Abstract

The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems governed by PDEs. Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination. Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness. To this end, we propose a novel and flexible formulation of the operator learning problem that allows jointly predicting infinite-dimensional quantities and inferring distributions of finite-dimensional parameters and thus amortizing the cost of both the inverse and the surrogate models to a joint training step. We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information, and in a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, predicting continuous time-series measurements from inferred system conditions. For both cases, we present comparisons against different baselines to demonstrate significantly increased accuracy in both the inverse and surrogate tasks.

## 1 Introduction

Partial Differential Equations (PDEs) describe the propagation of system conditions for a very wide range of physical systems. Parametric PDEs consider different system conditions as well as an underlying solution operator characterized by a set of finite-dimensional parameters. Traditional numerical methods based on different discretization schemes such as Finite Differences, Finite Volumes, and Finite Elements have been developed along with fast and parallelizable implementations to tackle complex problems, such as atmospheric modeling and cardiovascular biomechanics. For parametric PDEs, these methods define maps from the underlying set of parameters, which describe the dynamics and the boundary/initial conditions and which we assume to be finite-dimensional here, to physical quantities such as velocity or pressure that are continuous in the spatio-temporal domain. Despite their successful application, there still exist well-known drawbacks of traditional solvers. To describe a particular physical phenomenon, PDE parameters and solvers need to be calibrated on precise conditions that are not known a priori and cannot easily be measured in realistic applications. Therefore, iterative and thus expensive calibration procedures are considered in the cases where the parameters and conditions are inferred from data . Even after the solvers are calibrated, anensemble of solutions needs to be generated to account for uncertainties in the model parameters or assess the sensitivity of the solution to different parameters, which are computationally prohibitive downstream tasks .

**Related work** A variety of deep learning algorithms have recently been proposed for scientific application to PDEs, broadly categorized into surrogate and inverse modeling algorithms, to either reduce the computational time of complex forward simulations or infer missing finite-dimensional information from data to calibrate a simulator to precise conditions.

_Surrogate learning_ is a paradigm for accelerating computations. Hence, the cost of evaluating continuous quantities is amortized to an offline training stage. For functional data, the so-called _Neural Operators_[28; 3] generalizes to different conditions and discretizations of a complex system (resolution invariance). A multitude of operator learning approaches has been designed [37; 27; 50; 21; 44; 33; 32; 17], such as the Fourier Neural Operator (FNO) . However, a priori, neural operators are not designed to process maps between finite-dimensional and continuous quantities. A different surrogate modeling strategy is constructed by learning a map between PDE parameters and solutions, as in traditional reduced-order model (ROM) approaches [16; 8; 19], and their deep learning counterparts [23; 10], including Generative Adversarial ROM (GAROM) . In addition, Gaussian processes and kriging have been revisited in the context of PDE emulation [15; 61].

Amortizing the cost of _parameter inference_ has also been widely explored in the literature of generative modeling and Simulation-Based Inference (SBI). In SBI, Invertible Neural Networks are combined with discrete Normalizing Flows to develop Neural Posterior Estimation [11; 12; 24; 45] or continuous Normalizing Flows in Flow Matching Posterior Estimation (FMPE) [60; 35]. However, they commonly rely on an expensive physical solver to sample continuous predictions, and are hindered by the use of approximate physical models, Gaussian distributions, or a limited sample size.

Approaches that attempt to _jointly infer parameters and learn a surrogate_ are much rarer in the literature. Many are built on Variational Autoencoders (VAEs) , in finite  or infinite  dimensions. They inherently assume an underlying bijective relation between the functional and scalar quantities or are restricted to Gaussian latent representations, which limits their applicability to nonlinear problems. Among them, InVAErt  extends a deterministic encoder-decoder pair with a variational latent encoder. However, InVAErt is primarily constructed for investigating identifiability of parameters in systems of PDEs, and may not be suitable for learning uncertainty propagation from parameters to continuous fields. The conditional Variational Neural Operator (cVANO)  makes use of the resolution invariance of neural operators, based on . cVANO relies on a VAE to learn a low-dimensional manifold of the system constrained by finite-dimensional parameters, but is restricted to Gaussian approximations which are not suitable for problems which lack bijectivity. As alternative approaches, Simformer  estimates both finite-dimensional and function-valued parameters using transformers within the SBI framework, and PAGP  leverages the expressive power of Gaussian Processes. However, both are not suited for functional input due to quadratic growth with the input size. Finally, OpFlow  brings together operator learning and normalizing flows for quantifying uncertainty in output functions, but it lacks interpretability and inference on the latent space.

**Our contribution** Based on the experience and limitations of the aforementioned approaches, we formulate the following requirements for a unified forward-inverse framework relating functional quantities to a known space of finite-dimensional parameters: First, the interpretability provided by the given parameterization should be retained in both the forward and inverse tasks as well as their combination. Then, both the forward and inverse model should be discretization invariant with respect to all functional variables. Finally, a general probabilistic representation should be chosen for the latent parameter space, allowing for arbitrary parameter distributions. Following these guidelines, the main contributions of our paper are the following.

* We propose _Fast Unified Simulation and Estimation for PDEs_ (FUSE), a rigorous framework for unified inverse and forward problems for parametric PDEs. The FUSE framework, illustrated in Figure 1, allows to combine different forward and inverse models, and we choose to instantiate it with Fourier Neural Operators (FNOs) and Flow-Matching Posterior Estimation (FMPE) for the experiments in this work.
* We formulate a mathematical framework with a _unified objective_ for the forward and inverse problems, which allows us to assess how uncertainty in the inverse problem propagatesthrough the forward problem (_propagated uncertainty_) and evaluate both models together to avoid nonlinear error amplification at model concatenation.
* We showcase how to _adapt forward surrogate and inverse estimation models_ to fit the FUSE framework, mapping between finite and infinite-dimensional spaces. In particular, we extend FNO with a custom lifting to take finite-dimensional inputs, and we equip FMPE with an FNO-based encoding to allow for functional conditional information.
* Our _experiments_ show that our implementation of FUSE overcomes struggles of baselines (cVANO, GAROM, InVAErt) and an ablation (U-Net) on two complex and realistic PDE examples, pulse wave propagation (PWP) in the human arterial network and an atmospheric cold bubble (ACB). The experiments exemplify its ability for accurate and fast surrogate modeling, parameter inference, out-of-distribution generalization, and the flexibility to handle different levels of input information.

To our knowledge, the extensions we present to FNO and FMPE have not been explored in the literature yet. We would like to emphasize that we chose to base ourselves on these methods to illustrate the FUSE framework, but other forward and inverse methods may be more suitable for particular test cases.

## 2 Methods

Notation and AssumptionsLet \((X,^{d_{u}})\) and \((Y,^{d_{s}})\) be spaces of continuous functions on compact domains \(X^{d}\) and \(Y^{d^{}}\), respectively, and let \(^{m}\) be a space of finite-dimensional parameters. For a map \(F:\), where we take \(\) and \(\) to be among the spaces \(\), \(\), or \(\), and a probability measure \(()\), we denote by \(F_{\#}()\) the push-forward measure that expresses uncertainty on a set \(B\) by the propagation of \(\) through \(F\), defined as \(F_{\#}(B)=(F^{-1}(B))\). Further, we assume all metrics on measures in this paper to be the total variation metric, and denote them by \(d\) both on \(()\) and \(()\). In general, we assume all maps to be Lipschitz continuous, and we omit Lipschitz constants in inequalities (details are provided in the Appendix).

Problem FormulationConsider the setting of a parametric PDE with forward solution mapping \(: s\), where \(\) is a vector of finite-dimensional parameters and \(s\) is a function-valued output. Since it is the goal to calibrate the input parameters, we omit any functional inputs to \(\), such

Figure 1: FUSE models a posterior distribution over finite-dimensional parameters \(\) given infinite-dimensional functions \(u\) with \(d_{u}\) components (channels). It learns other continuous functions \(s\) with \(d_{s}\) channels from the parameters \(\). Band-limited Fourier transforms and a lifting operator act as a bridge between finite and infinite dimensions for the forward problem. Likewise, as inference models such as FMPE or NPE require fixed-size inputs, the inverse operator layers are conjoined with a band-limited Fourier transform to learn a fixed-size representation of the input function.

as initial or boundary conditions, and assume they are kept constant or encoded in the parameters \(\). Given the forward operator \(\), the _forward model uncertainty_ is quantified by propagating a given distribution of parameters \(()\) through the model, i.e. evaluating the push-forward measure \(_{\#}\). In practice, the parameters \(\) are not available and need to be inferred from indirect measurements \(u\), which we assume to be functions in space or time (e.g., time series), and, in general, \(u s\). The extended solution operator \(}:u s\) then maps between functional measurements and functional model output, omitting the intermediate parameter space. Since the measurements \(u\) are impacted by the uncertain parameters \(\), the function inputs \(u\) are as well equipped with uncertainty and represented by the measure \(()\). Finally, the inverse problem consists in estimating the distribution \((|u)\) of parameters given measurements \(u\). The corresponding uncertainty on the predicted outcomes \(s\) is then given by the _propagated uncertainty_\(_{\#(.|u)}\).

Unified ObjectiveGiven an approximate forward operator \(}^{}}\) between function spaces, and an estimated distribution \(^{}\) of input functions \(u\), parameterized by \(\) and \(\), respectively, we use triangle inequality to observe that

\[d(}^{}_{\#^{}},}_{\#}) }^{}_{\#^{}}, }^{}_{\#})}_{}+ }^{}_{\#},}_{\#} )}_{}.\] (1)

Thus, we found a unified objective consisting of two steps corresponding to the two terms in the right hand side (rhs) of Equation (1). In the measure matching step, the objective amounts to learn an approximation \(^{}\) of the measure \(\), whereas in the operator learning step, the objective is to learn a Neural Operator \(}^{}\) that approximates the underlying ground truth operator \(}\).

Our reformulation of operator learning naturally fits into the aim of this paper to propose a method that can act as an operator surrogate (_forward problem_, operator learning objective) as well as performing parameter inference by minimizing distances on measures (_inverse problem_, measure matching objective), unifying the apparently unrelated problems of surrogate modeling and inference.

Forward Problem: Operator LearningThe operator \(}\) resembles the solution operator to a PDE that maps function inputs such as initial and boundary conditions, coefficients, sources etc to (observables of) the solution of the PDE. It is the goal of supervised operator learning  to learn this type of operator as a parameterized Neural Operator (NO) \(}^{}\) on a (training) distribution \(()\) by minimizing the operator learning objective in (1). This objective is bound by _supervised learning_ on the parameter space \(\),

\[d(}^{}_{\#},}_{\#}) d( ^{}_{\#},_{\#}),\] (2)

and further by the term (Appendix A.6),

\[_{1}()=_{}||^{}()-() \|_{L^{1}}d().\] (3)

The loss is approximated by training samples of the form \(\{^{i},(^{i})\}_{i=1}^{N}\), sampled from an underlying data distribution \(()\). In order to use a NO on finite-dimensional inputs, we define the _band-limited lifting_\(h^{_{3}}()\), composed of a lifting increasing the dimension of the parameters and an inverse Fourier transform. We then choose to instantiate \(^{}\) based on Fourier Neural Operator (FNO)  layers \(^{_{2}}\), implemented with discrete spectral evaluations as in  to handle irregularly sampled measurements for the output function,

\[^{}()=^{_{1}}^{_{2} } h^{_{3}}(),\] (4)

where \(^{_{1}}\) is a learnable map that projects the channels to the dimensions of the output function and \(=[_{1},_{2},_{3}]\) are the trainable parameters.

Inverse Problem: Measure MatchingWe would like to approximate the true posterior measure \((|u)\) given measurements \(u\) by \(^{}(|u)\). This task is equivalent to minimizing the measure matching objective in (1) since (Appendix A.7)

\[d(}^{}_{\#^{}},}^{}_{ \#}) d(^{}(|u),(|u)).\] (5)

To minimize the latter distance, we adapt Flow-Matching Posterior Estimation (FMPE) to handle function-valued conditional inputs \(u\). FMPE trains a flow function that maps samples from a standard 

[MISSING_PAGE_FAIL:5]

described by a set of PDEs whose solutions are fully parameterized by finite-dimensional parameters encoding model properties, as well as boundary and initial conditions. In both of these experiments, to replicate a patient or downburst observed in the real world, the solver needs to be calibrated for precise conditions, e.g. a specific patient or atmospheric conditions, and forward uncertainty quantification of the calibrated predictions are of interest. Details on both cases are provided in Appendices A.3 and A.4.

**Atmospheric Cold Bubble (ACB)** In atmospheric modeling, the two-dimensional dry cold bubble  is a well-known test case for numerical simulations resembling downbursts, which cause extreme surface winds in thunderstorms. Within a domain of neutral atmospheric stability without background wind, an elliptic cold air anomaly is prescribed and initiates a turbulent flow as it sinks to the ground, see Figure A.12 for an example. Time series of horizontal and vertical velocity, \(u\) and \(w\), are recorded at eight sensors placed inside the domain. Similar data is obtained by weather stations (\(z 2\) m) or turbulence flux towers (\(z 10-50\) m), experienced by wind turbines or high-rise buildings (\(z 100\) m), or with unmanned aerial vehicles. The PDE model PyCLES  used for simulation is parameterized by the turbulent eddy viscosity \(_{t}\) and diffusivity \(D_{t}\) as model parameters, as well as four parameters describing the amplitude and shape of the initial cold perturbation (\(a,\ x_{r},\ z_{r},\ z_{c}\)). The target task for FUSE here is to calibrate these parameters \(^{6}\) to time series measurements \(u:[0,T]^{20}\), at ten locations for horizontal and vertical velocity each, where the input measurements used for calibration and the model output used for prediction and uncertainty quantification share the same space \(=\) of time series.

**Pulse Wave Propagation (PWP) in the Human Body** The pulse wave propagation in the cardiovascular system contains a great deal of information regarding the health of an individual. For this reason, there are efforts to measure and leverage PWP in both wearable devices, e.g. smart-watches, and clinical medicine. Different systemic parameters \(^{32}\) such as stroke volume (SV), heart rate (HR), and patient age have been shown to affect the morphology of pulse waves . These are used to parameterize pulse wave time series of pressure, velocity, and photoplethysmography (PPG) at 13 locations of systemic arteries of the human cardiovascular system through a reduced order PDE model in the dataset published by . See Tables 7 and 8 in Section A.3 for the full set of parameters and locations. In clinical applications, only measurements \(u\) at easily accessible locations (such as the wrist) are available, while it is the goal to predict the pulse signal \(s\) at other locations of interest from the inferred parameters \(\). The space of input functions \(\) hence differs from the space of output functions \(\) in this test case. In order to account for different clinical scenarios, we train the FUSE model using random masking of locations and evaluate it on different levels of available input information,

**Level 1**: **Perfect information:** Pressure, velocity, and PPG at all locations,
**Level 2**: **Intensive care unit information:** Pressure, velocity, and PPG at the wrist,
**Level 3**: **Minimal information:** PPG at the fingertip.

## 4 Results

Collected results for all experiments are summarized in Table 1. While the main tasks of the FUSE model are evaluated for both test cases, we focus on ACB to exemplify its generalization properties and on PWP for different levels of input information. We would like to emphasize that the goal of uncertainty representation here is to capture parametric uncertainty inherent to the data-generating numerical model and its parameterizations. Our model does not provide a notion of uncertainty in the predictions due to imperfect training of the neural networks. Given that these errors are generally low (Table 1), we are confident to interpret all spread on \(\) and \(s\) given by FUSE as parametric uncertainty in the data-generating model. In particular, we expect the true posterior distributions to have positive spread, hence a prediction with small ensemble spread is not necessarily accurate.

**Inverse Problem** Given time series measurements \(u\), the inverse model samples from the distribution of parameters, \(^{}(|u)\), with more samples in areas of the parameter space that are more likely to match the data-generating parameters \(^{*}\). We find that FUSE outperforms the other methods when sufficient input information is given. We observe that inVAErt experiences posterior collapse for PWP in this setting, which is represented in high CRPS (Figure A.9). Histograms reveal that FUSE captures the expected dependencies of the data on the parameters (Figures A.1, A.2), such as wider distributions for scarcer information in PWP. In the ACB case, sharp Dirac-like estimates are obtained

[MISSING_PAGE_FAIL:7]

**Sensitivity Analysis** Based on the good approximative properties showcased above, the forward operator \(^{}\) of FUSE can be used as a fast surrogate model to assess the sensitivity of the underlying PDE to varying the parameters \(\). Such analysis is usually constrained to a small sample size due to the large computational costs, and the dense sampling enabled by the surrogate helps to explore the parameter space for parameters that optimally fit patient data in PWP, or exhibit extreme winds in ACB. Varying one parameter at a time, while keeping all others fixed at their default value, exhibits the single effect on the data (_fingerprint_). For ACB, an increased amplitude of the cold anomaly mainly results in a speed-up and hence a squeezing of the time series, while other parameters have more nonlinear effects (Figure A.17). As the FUSE model provides no access to the full velocity fields, only numerical simulations reveal that these sharp sensitivities are mainly caused by certain features (eddies) of the flow reaching or missing a sensor depending on the parameter value. A validation of this sensitivity analysis for an estimate of the maximal horizontal velocity over the time series is provided in Figure 4 for a pairwise fingerprint with \(10,000\) FUSE samples, with consistently low errors compared to 100 numerical samples. Only for one sample with a weak and small perturbation, the signal is not captured accurately. An additional evaluation of 60 samples at the margins of the parameter space shows that the peak velocities are consistently represented well (Figures A.20, A.19). For PWP, the sensitivity results (Figure A.10) are consistent with observations reported in the literature .

**Out of Distribution Generalization for ACB** We test the generalization capabilities of the FUSE forward model on test samples with amplitudes larger than seen in training in the pairwise sensitivity setup (Figure 4). The FUSE model is able to capture the shape of the pairwise parameter dependencies but struggles to locate the sharp dependence of the local peak velocity on the eddy diffusivity. Averaged errors in the out of distribution range are shown in Table 2, with FUSE performing clearly superior to all other models.

**Levels of Available Information for PWP** The case of missing input information is modeled by masking certain input components when evaluating the models. While FUSE shows regular performance comparable with the baselines on levels 2 and 3 of missing information, cVANO seems to be better suited for this setting (Table 1), probably due to the assumption that the latent dimensions are uncorrelated. As a sanity check, FUSE samples the prior distribution of the training data when evaluated without any conditional input information \(u\) (Figure A.8). When the available input measurement locations of the PWP experiment are known a priori, it is possible to get more accurate results when training FUSE without masking. For example, training to predict pressures and parameters from only the PPG data at the fingertip only, without masking, results in a CRPS of \(4.28 10^{-2}\) and a relative \(L^{1}\) error of 3.6%, roughly half the error of the masked model. If the measurement data is fixed, predictions without masking are much more suitable for analysis.

## 5 Discussion

**Summary** We propose FUSE, a framework unifying surrogate modeling and parameter identification for parametric PDEs by bridging operator learning with flow-matching posterior estimation. Both are represented in the FUSE objective through a deterministic forward loss and a probabilistic inverse loss, respectively. The joint architecture allows for inverse estimation of scalar parameters \(\) given

Figure 4: ACB, sensitivity analysis and generalization: Validation of the FUSE model against numeric simulations on peak horizontal velocities \(u\) at location 1 (\(x=15\) km, \(z=50\) m). Samples above the dashed line correspond to amplitudes larger than seen during training. The parameter resolution is \(100 100\) for FUSE, and \((10+10) 10\) for the numerical samples. Figure continued in the appendix, Figure A.18.

    & Model & Experimental Setup & CRPS\( 10^{2}\) & Rel. \(L_{1}\) Error & Rel. \(L_{2}\) Error \\   & FUSE (**ours**) & True Parameters & - & **0.13 \(\) 0.05** \% & **0.19 \(\) 0.08** \% \\  & & Level 1 & **1.31 \(\) 0.69** & **0.86 \(\) 0.93** \% & **0.90 \(\) 0.92** \% \\  & & Level 2 & **3.18 \(\) 1.40** & **3.83 \(\) 2.66** \% & **3.87 \(\) 2.64** \% \\  & & Level 3 & \(8.48 2.55\) & **6.56 \(\) 3.36** \% & **7.05 \(\) 3.39** \% \\   & UNet (**ablation**) & True Parameters & - & 1.38 \(\) 0.34 \% & 2.00 \(\) 0.48 \% \\  & & Level 1 & \(3.79 0.97\) & 3.81 \(\) 2.03\% & 4.25 \(\) 1.92 \% \\  & & Level 2 & \(5.75 1.32\) & 6.79 \(\) 3.77 \% & 7.19 \(\) 3.68 \% \\  & & Level 3 & \(12.72 2.65\) & 7.83 \(\) 2.64 \% & 8.78 \(\) 2.87 \% \\   & InVAErt & True Parameters & - & 3.44 \(\) 0.52 \% & 4.21 \(\) 0.60 \% \\  & & Level 1 & \(11.62 2.39\) & 8.28 \(\) 4.78 \% & 8.94 \(\) 4.91 \% \\  & & Level 2 & \(16.88 3.67\) & 7.85 \(\) 2.80 \% & 8.64 \(\) 2.99 \% \\  & & Level 3 & \(21.22 4.35\) & 8.37 \(\) 2.73 \% & 9.48 \(\) 2.98 \% \\   & cVANO & True Parameters & - & 7.49 \(\) 2.60 \% & 8.51 \(\) 2.58 \% \\  & & Level 1 & 5.61 \(\) 0.24 & 7.54 \(\) 2.52 \% & 8.52 \(\) 2.53 \% \\  & & Level 2 & 4.00 \(\) 2.22 & 8.42 \(\) 3.11 \% & 9.72 \(\) 3.23 \% \\  & & Level 3 & **3.66 \(\) 0.76** & 10.79 \(\) 3.60 \% & 12.54 \(\) 4.05 \% \\   & GAROM & True Parameters & - & 0.70 \(\) 0.13 \% & 1.04 \(\) 0.22 \% \\  & Levels 1-3 & - & - & - & - \\   & FUSE (**ours**) & True Parameters & - & **0.41 \(\) 0.45** \% & **0.80 \(\) 0.89** \% \\  & Estimated Parameters & **1.84 \(\) 1.63** & **0.58 \(\) 0.59** \% & **1.13 \(\) 1.11** \% \\   & UNet (**ablation**) & True Parameters & - & 1.70 \(\) 0.90 \% & 2.86 \(\) 1.55 \% \\  & Estimated Parameters & 4.47 \(\) 2.67 & 2.13 \(\) 1.16 \% & 3.69 \(\) 1.94 \% \\   & InVAErt & True Parameters & - & 1.11 \(\) 0.85 \% & 1.94 \(\) 1.31 \% \\  & Estimated Parameters & 6.65 \(\) 9.05 & 1.17 \(\) 1.35 \% & 2.88 \(\) 2.04 \% \\   & cVANO & True Parameters & - & 5.32 \(\) 1.57 \% & 8.26 \(\) 2.44\% \\  & Estimated Parameters & 19.84 \(\) 4.73 & 8.77 \(\) 3.04 \% & 12.33 \(\) 4.00\% \\   & GAROM & True Parameters & - & 3.99 \(\) 1.67 \% & 6.35 \(\) 2.43 \% \\   & Estimated Parameters & - & - & - \\   

Table 1: Performance of FUSE and the baseline models in estimating parameters \(\) from continuous inputs \(u\), quantified by CRPS, and predicting time series data \(s\), quantified by a relative \(L_{1}\) and \(L_{2}\) error. Here, “True parameters” evaluates the forward model part only, and “Estimated parameters” and levels one to three evaluates the sample mean \(\) predicted by the unified model.

    & Model & Experimental Setup & CRPS\( 10^{2}\) & Rel. \(L_{1}\) Error & Rel. \(L_{2}\) Error \\   & FUSE (**ours**) & True Parameters & - & **1.83 \(\) 1.10** \% & **3.15 \(\) 1.80** \% \\  & & Estimated Parameters & **3.13 \(\) 1.13** & **1.63 \(\) 0.78** \% & **2.77 \(\) 1.28** \% \\   & UNet (**ablation**) & True Parameters & - & 2.78 \(\) 0.32 \% & 4.37 \(\) 0.43 \% \\  & & Estimated Parameters & 12.36 \(\) 5.47 & 3.43 \(\) 0.40 \% & 5.40 \(\) 0.58 \% \\   & InVAErt & True Parameters & - & 4.48 \(\) 2.65 \% & 6.49 \(\) 3.26 \% \\  & & Estimated Parameters & 18.80 \(\) 19.09 & 4.31 \(\) 1.13 \% & 6.92 \(\) 1.83 \% \\   & cVANO & True Parameters & - & 9.31 \(\) 1.22 \% & 13.74 \(\) 1.54 \% \\  & & Estimated Parameters & 24.76 \(\) 1.45 & 15.35 \(\) 1.28 \% & 21.20 \(\) 1.49\% \\   & GAROM & True Parameters & - & 7.22 \(\) 1.00 \% & 10.96 \(\) 1.22 \% \\   & Estimated Parameters & - & - & - & - \\   

Table 2: Performance of FUSE on out-of-distribution samples for ACB, as described in Figure 4, following the format of Table 1.

continuous measurements \(u\), as well as forward predictions of (other) continuous measurements \(s\) given \(\). Inheriting from the properties of its components, FUSE is discretization invariant with respect to the continuous inputs and outputs, and can represent arbitrary distributions in the fully interpretable parameter space. As this latent space is defined through a known parameterization of the underlying PDE, FUSE allows for accurate calibration and evaluation of a numerical solver, as well as performing downstream tasks such as parameter uncertainty quantification. On the parametric PDEs of pulse wave propagation (PWP) in the human cardiovascular system and an atmospheric cold bubble (ACB) with time series measurements, FUSE consistently outperforms four baseline methods designed to perform similar tasks. It also shows good out-of-distribution generalization and great flexibility when trained and evaluated on different levels of available input information.

**Connection to Existing Methods** The proposed methodology and the adapted implementation of the FNO and FMPE offer the following advantages over the existing methods for joint parameter estimation and forward emulation mentioned in the introduction. By incorporating Neural Operators in both the forward and inverse parts, we ensure discretization invariance throughout the model . In contrast to Variational Autoencoders, we present a fully probabilistic formulation and require no restriction on the latent data distribution. This allows us to model arbitrary posterior distributions, whereas cVANO is restricted to Gaussian distributions. Likewise, FUSE can leverage powerful, existing neural operators which improves predictions on the output functions. The experiments have shown that FUSE excels over all baselines in both test cases. In particular, cVANO and GAROM fail to capture even rough characteristics of the flow in the forward emulation for the ACB case with nonlinear dynamics, and InVAErt shows a consistent bias towards the mean in the PWP task. Only in the case of missing information in the smoother PWP case, cVANO outperforms FUSE on the inverse problem. In terms of the choice of FNO in both the forward and inverse part of FUSE, the ablation with UNet layers exhibits larger uncertainties and worse performance of the predicted mean. An additional ablation, presented in the Appendix 5, shows that optimal transport probability paths perform better than the diffusion-based paths in a conditional DDPM model. Finally, Gaussian processes are often chosen for their ability to handle unstructured data. We incorporate this property by using the grid-independent FNO implementation by .

**Applicability** FUSE is explicitly formulated for finite-dimensional parameters and function measurements and outputs. This setting naturally arises when calibrating a numerical solver, with training data generated synthetically by the solver itself, and was demonstrated in two test cases. However, it is very common for real measured datasets, e.g. in bioengineering , and more specifically when involving PWP, to contain both time series data and vectors of parameters available for different patients. FUSE naturally extends to these datasets, where an underlying PDE is assumed but not formulated or simulated explicitly, including parameter inference using real data , precision medicine or solver calibration , and fingerprinting to discover parameter-disease correlations . In the case that the parameters are non-physical, and hence not measurable, such as the numerical model parameters in the ACB case, this does of course not apply, and the main aim of solving the forward and inverse problem is to explore the solver itself. Moreover, FUSE may help to refine existing parameterizations either through disentanglement  or by identifying parameters with little influence on the simulated data.

**Limitations and Future Work** The FUSE framework aims to emulate a given parameterization of a PDE. If such a parameterization (or a corresponding dataset) is not available, VAE-based preprocessing  or manifold discovery  may be used. When applying FUSE to real measurements, the parameterization turns to be only an approximation of the dynamical system, whereas it is considered a perfect model in the experiments presented here. This structural model uncertainty has to be added to the interpretation of the unified uncertainties given by the FUSE model, as it has to be for any other inverse algorithm. In practice, measurements are always associated with a measurement error, which is considerable in particular for extreme measurements. The current implementation of FUSE does not allow for errors in the continuous inputs to be taken into account, and it is left for a future extension of the framework to include these into a fully Bayesian formulation of the inverse model. In spatially distributed applications, such as atmospheric modeling, the parameters of interest might be themselves space/time-dependent functions, such as surface properties or initial conditions . Based on the scaling properties of the FMPE model for high-dimensional data such as images , FUSE is expected to scale well in size for high-dimensional parameter spaces, but is yet to be extended to a resolution-invariant function representation in latent space. These limitations will be addressed in future research.