# Game Solving with Online Fine-Tuning

Ti-Rong Wu,1 Hung Guei,1 Ting Han Wei,2 Chung-Chin Shih,1,3 Jui-Te Chin,3 I-Chen Wu3,4

###### Abstract

Game solving is a similar, yet more difficult task than mastering a game. Solving a game typically means to find the game-theoretic value (outcome given optimal play), and optionally a full strategy to follow in order to achieve that outcome. The AlphaZero algorithm has demonstrated super-human level play, and its powerful policy and value predictions have also served as heuristics in game solving. However, to solve a game and obtain a full strategy, a winning response must be found for all possible moves by the losing player. This includes very poor lines of play from the losing side, for which the AlphaZero self-play process will not encounter. AlphaZero-based heuristics can be highly inaccurate when evaluating these out-of-distribution positions, which occur throughout the entire search. To address this issue, this paper investigates applying online fine-tuning while searching and proposes two methods to learn tailor-designed heuristics for game solving. Our experiments show that using online fine-tuning can solve a series of challenging 7x7 Killall-Go problems, using only 23.54% of computation time compared to the baseline without online fine-tuning. Results suggest that the savings scale with problem size. Our method can further be extended to any tree search algorithm for problem solving. Our code is available at https://rlg.iis.sinica.edu.tw/papers/neurips2023-online-fine-tuning-solver.

## 1 Introduction

_Playing_ and _solving_ strategic games have served as drivers and major milestones  in artificial intelligence research. To master such games, the objective is often designed to optimize on the objective of maximizing the probability of winning. In the past several decades, researchers made significant progress in game playing, reaching super-human playing levels in many domains. Successful examples include Chinook (checkers) , Deep Blue (chess) , AlphaGo (Go) , and AlphaStar (StarCraft II) . Furthermore, AlphaZero  and MuZero  even boast generality by mastering a variety of games without requiring expert human knowledge. Although these learning-based agents have progressed dramatically in playing strength, there are no guarantees that their decisions are always correct  in terms of game-theoretic value, which is defined as the outcome of the game given optimal play for both players. Game solving is this pursuit of finding game-theoretic values.

Game solving is a more difficult challenge than game playing. Many seemingly simple games have astronomically large state spaces, with no simple way of exploring this space. Here is whereadvancements in game playing can aid game solving. Strong agents are commonly leveraged to evaluate positions, providing guidance and reducing the search space significantly. For example, the checkers program Chinook claimed to have reached super-human levels as early as 1996 , then about 10 years later, played an instrumental role in the proof that checkers is a drawn game . Similarly, contemporary learning-based approaches such as AlphaZero are widely used to help reduce the search space for game solving. Game solvers that utilized AlphaZero include Hex , Go , Killall-Go , and the Rubik's cube [15; 16]. Such approaches are not limited to applications in games but extend to other non-game fields, like automated theorem proving .

However, a major issue still exists when using learning-based approaches to aid game solving. In the two-player, zero-sum setting, a simple description for a proof involves verifying that there is a winning move for the winner, for all possibilities played by the losing side; i.e. no matter how the loser plays, the winner must be able to respond correctly. However, most learning-based agents are trained along a strong line of play by both players, with some exploration to nearby states for robustness. Using AlphaZero as an example, training samples are generated via self-play by the best version of itself up to that point. Learning-based methods are powerful in that they generalize for previously unseen positions, but accuracy tends to drop the further you stray from training samples. To verify all possibilities on the losing side, the vast majority of positions we must evaluate during the search for a proof are therefore out-of-distribution. To illustrate, AlphaZero-like networks have been shown to make inconsistent or completely incorrect evaluations, simply by adding two meaningless stones to a position . In another example, in the attempt to solve one of the hardest Go life-and-death (L&D) problems from the famous book Igo Hatsuyoron,2 all AlphaZero-like programs failed. It was hypothesized that this was because these highly specific problems are rarely encountered during training .

This paper proposes applying online fine-tuning methods to train learning-based systems while solving games. In our proposed methods, during game solving, an online trainer is added so that the learned heuristic is improved as new data that is relevant to the solving task is generated. This is done by utilizing new information such as _solved_ and _critical_ positions in the current solver search tree. The trainer therefore can learn better heuristics dynamically, that are particularly fine-tuned for the upcoming evaluations. Experiments are conducted on 16 challenging 7x7 Killall-Go three-move openings, shown in Figure 3. We develop a distributed game solver with online fine-tuning, that is built upon the state-of-the-art 7x7 Killall-Go solver . Experiment results show that the online fine-tuning solver can greatly reduce the search space by a factor of 4.61 on average. Namely, it searches only 21.69% of nodes, using 23.54% of the computation time, as compared to the offline solver. Most importantly, for larger problems, the online fine-tuning solver performs significantly faster than that without, which implies that our method scales with problem size.

## 2 Background

### Game solvers

A two-player zero-sum game is considered _solved_ if we know of a winning strategy for either player which guarantees a winning outcome,3 regardless of how the opponent plays; i.e. the player must have at least one action that leads to a win, for all actions by the opponent. A winning strategy is often represented as an AND-OR tree called a _solution tree_, where the game positions with the winner to move are represented by OR-nodes, and those for the opponent by AND-nodes. Leaf nodes in a solution tree are all terminal positions where the outcome is a win.

A _solver_ is a program that does a proof search and can identify a winning strategy or a solution tree, if found. Solvers often rely on heuristic tree search for games with large and complex game state spaces. Search algorithms such as alpha-beta search , proof number search (PNS) , or Monte Carlo tree search (MCTS) [22; 23] have all been shown to be successful. In addition, previous research has shown that none of the algorithms dominates the others [24; 25; 14].

### Distributed game solver

In cases where search spaces are too large for a single instance solver under reasonable time and memory constraints, multiple solvers are often run in parallel, forming a distributed computing system, to scale up the solving process. Examples of games solved by distributed computing include checkers , heads-up limit hold'em poker , breakthrough , Hex , and Connect6 [25; 29].

These distributed game solving/analysis systems, also known as _distributed game solvers_, have been presented commonly with two components, a _manager_ and a set of _workers_. A manager divides the overall problem into smaller sub-problems, keeping only the initial portion of the search tree - the beginning of the game - in memory. As this search tree is expanded, the manager may decide to offload analysis of specific positions to its workers. These offloaded sub-problems are also called _jobs_[25; 29]. A worker computes jobs by taking as input, a specific position and any relevant parameters (e.g. time limits), then outputs either a solved or heuristic value for that position. A worker can be a single solver, a game engine, or even a combination of both.

For example, in the checkers proof, Chinook and another depth-first proof-number search  solver were combined as a heuristic. From the perspective of the manager, a solved job result, such as a _proven_ win, loss, or draw, can be thought of as a terminal node in its solution tree. Unsolved jobs also provide useful information, such as heuristic values to determine _likely win_ or _likely loss_, to guide further tree expansions, with the worker acting as a relatively expensive and accurate heuristic. Similar to the checkers proof, Connect6 openings have also been solved by encapsulating the task of solving and playing a position into a single job, which was then dispatched by a manager to a set of workers [25; 29; 31]. In all examples listed above, a centralized scheme is used where one manager coordinates between dozens to hundreds of workers.

### Proof Cost Network

When using neural networks as heuristics in solving, recent research points out that there is room for improvement when using the value network learned from the AlphaZero algorithm[16; 14]. In a search tree, when several actions can reach a winning outcome, AlphaZero-trained networks have no preference for choosing one that wins fastest. This can increase the amount of computation significantly.

To address this challenge, the Proof Cost Network (PCN)  predicts a proof cost value, rather than a win rate. The cost value represents a logarithmically-normalized estimate of the number of nodes that are required to solve the position. Specifically, PCN adopts the AlphaZero training process and generates self-play games using the cost value to guide the MCTS toward faster winning moves. These self-play games are then used to update PCN's cost values. The resulting network will focus the proof search on actions with minimal cost. Experiments show that the proof cost value is highly correlated to problem difficulty, and can significantly improve solving capability.

## 3 Game solver with online fine-tuning

This section describes our methods for applying online fine-tuning to game solving. We chose an MCTS-based solver due to its popularity when integrating AlphaZero networks as heuristics [16; 13; 14]. However, it is worth noting that the methods presented in this paper are search-independent and can be readily applied to other search algorithms such as alpha-beta search or PNS.

### Distributed game solver

Our distributed game solver consists of a PCN serving as its heuristic, a _manager_, and a set of _workers_. The manager maintains an MCTS rooted at the position to be solved. During the proof search, the manager follows PUCT  selection to traverse from the root to a leaf node. Next, the PCN estimates the cost of the selected leaf node, denoted by \(v_{l}\). \(v_{l}\) is a heuristic value representing the log estimated number of positions that must be examined to solve this node. If the value is larger than a designated threshold, i.e. \(v_{l} v_{thr}\), its proof cost is considered too high to warrant a job. The manager will then continue to follow MCTS, expanding the node and backpropagating \(v_{l}\) to the root. Alternatively, if \(v_{l}<v_{thr}\), the leaf node is highly likely to be solved outright by a worker, at whichpoint a job is created. Job granularity is therefore controlled with \(v_{thr}\). Larger \(v_{thr}\) generates more difficult jobs with higher failure rates, while smaller \(v_{thr}\) leads to easier but more numerous jobs. A balanced \(v_{thr}\) should be set according to the game instance and worker capabilities.

Workers are game solving programs that are limited by specific constraints, say, a given time limit. To keep the heuristic consistent during the proof search, workers use the same PCN weights as the manager. If the job is solved within the given constraint, the worker returns the result, either a win or a loss, back to the manager; otherwise, it returns an unknown. Once the manager receives the job result, it updates the search tree accordingly. For unsolved jobs, the manager expands the nodes that generated the corresponding jobs. The interaction between the manager and the workers is shown between Figure 2(a) and 2(b).

### Online Fine-Tuning Trainer

The Online Fine-Tuning Trainer (OFT) maintains the PCN during the proof search so that the manager and workers have access to ever improving heuristics. Without online fine-tuning, both the manager and workers simply use a fixed PCN, denoted by \(_{0}\), trained via the AlphaZero self-play process. The OFT starts with \(_{0}\), then fine-tunes the weights via updates during the proof search. To do this, the manager picks out _solved_ and/or _critical_ positions in its search tree, adds them to the list of training samples, then the OFT uses them to perform self-play and training. The fine-tuned PCN (\(_{1},_{2},...,_{t},...\)) can then be used to further guide the manager and workers towards a faster proof. The manager and workers update to the most recent \(_{t}\) immediately when a new PCN checkpoint is trained by the OFT. The above iterative process is shown in Figure 2. Details are provided in the following sections.

#### 3.2.1 Online fine-tuning trainer with solved positions

During the proof search, many previously unsolved positions may become _solved_ in the manager's search tree. This new information can be used by the OFT to improve the accuracy of the PCN. Figure 2 provides an example of a manager's AND-OR search tree and six recent selection paths. In this example, positions \(j\) and \(d\) are marked as solved and sent to the OFT after the first and fourth selection, respectively. The OFT maintains a queue that stores these solved positions, as shown in Figure 2(c). Self-play games are generated as in normal PCN training . However, in the optimization phase, the OFT randomly samples training data not only from the generated self-play games, but also from the queue of solved positions. For these solved positions, the cost values are always set to zero (i.e. solved to be a win, from the perspective of the OR-player), since no nodes need to be examined to solve the position. The OFT only samples 10% of training data from the solved queue during optimization to avoid overfitting, where the remaining 90% are sampled from self-play games. In addition, the queue only stores the most recent 1,000 solved positions received from the manager. During self-play, when using \(_{t}\) to evaluate positions that are solved by the manager, it is highly likely to predict costs close to zero. From the AND-player's perspective, it favors moves that lead to larger costs to delay the OR-player's victory. Therefore, self-play naturally explores positions which have not yet been solved in the manager's search tree.

#### 3.2.2 Online fine-tuning trainer with critical positions

Other than solved positions, we can also improve the PCN with specific positions of interest chosen from the manager's current search tree. Positions are considered _critical_ if they are selected in the most recent MCTS iterations in the manager. For example, in the first selection in Figure 2, all positions \(r\), \(a\), \(g\), and \(j\) in the MCTS selection path are considered critical positions. During self-play, the trainer randomly chooses one critical position, and performs self-play starting from that position. Thus, \(_{t}\) can provide more accurate predictions for positions that proof search is currently exploring.

A more selective process can be used to improve the quality of critical positions. First, we can omit \(r\), since self-play from \(r\) is already performed to train \(_{0}\). Ideally, we would prefer to focus on deeper unsolved positions. To achieve this, we only consider the leaf position in the selection path as critical. Also, the OFT maintains a queue in which only the recent 1,000 critical positions are stored. This way, the OFT can focus on the most urgent positions which are likely to be solved soon. As these positions are also usually sent to the workers (if the PCN value \(v v_{thr}\)), the workers can also take advantage of \(_{t}\). Next, we can omit leaf positions solved solely by the manager; i.e. leaf nodes that were solved not as jobs. For example, \(j\) will not be considered critical in the first selection in Figure 2. Since \(j\) is already solved, it is not necessary to perform self-play from that position. Only \(m\), \(n\), \(h\), and \(i\) will be sent to the OFT as critical positions in the second, third, fifth, and sixth selection, respectively. Note that \(r\), \(a\), and \(g\) were critical positions before \(b\) and \(k\) became critical, since the parent nodes are always expanded before their children. Thus, the trainer will gradually fine-tune the PCN by focusing only on deeper critical positions to help avoid redundancy during fine-tuning.

In summary, the pre-trained \(_{0}\) learns general heuristics by exploring from empty games, while the online \(_{t}\) refines its heuristics for specific positions of interest. As a side note, the fine-tuning process is related to the catastrophic forgetting phenomenon , as the focus is shifted from one part of the proof search to another. Interestingly, forgetting is not only acceptable in this context, but probably even preferred, because the heuristic only needs to be accurate for the part of the search space the manager is currently working on. Additionally, the two proposed methods are independent and can be combined. We evaluate these methods in our experiments.

### Manager job assignment improvements

Job assignment refers to the manager's responsibility of dividing the overall problem into distinct jobs. Better job assignment schemes can eliminate redundancy and improve parallelism. We propose three techniques to further improve the efficiency of job assignment, which we call _virtual solving_, _top-\(k\) selection_, and _AND-player job assignment_.

**Virtual solving.** When a job is assigned to workers, we assume that the job result will be solved, even before it is actually returned by a worker. The _virtually solved_ outcome is backpropagated as a normal job outcome. This technique has similar concepts to the _virtual loss_, _virtual win_, and _Young Brothers Wait Concept_ (YBWC) , which were used to avoid repeatedly searching superfluous nodes during the proof search. For example, in Figure 2, assume the manager selects a path from \(r\) to a leaf \(h\) and assigns the job to a worker, at which point \(h\) is immediately marked as virtually solved. Its parent node \(b\), an OR-node, is then also marked as virtually solved. Furthermore, if nodes \(a\), \(c\), and \(e\) (\(d\) is solved already) are all solved or virtually solved, their parent \(r\) will also be marked as virtually solved. When the job result returns, the manager reverts the virtually solved markers and updates the status of all nodes accordingly. The virtual solving technique can provide a highly efficient job assignment scheme, in that the manager search tree can be nearly the same as the solution tree if most virtually solved nodes are indeed winning.

**Top-\(k\) selection.** We exploit the fact that all child nodes must be solved for every AND-node to improve parallelism. At each AND-node, we select uniformly at random among the top \(k\) unsolved children that are likely to be sent off as jobs eventually, i.e. those with the top \(k\) highest PUCT scores. For example, in Figure 2, assume \(k=2\) and nodes \(a\) and \(b\) are the top two children of AND-node \(r\); nodes \(k\) and \(l\) are the top two for \(g\). Note that we omit node \(j\) because it is already solved. At node \(r\), the manager selects between \(a\) and \(b\) with equal probability. Note that selections at OR-nodes remain unchanged. In addition, we only apply top-\(k\) selection when the simulation count of the AND-node is larger than \(k\). Top-\(k\) selection improves parallelism by allowing the manager to assign more jobs simultaneously, when it is combined with virtual solving. We use \(k=4\) in our experiments.

**AND-player job assignment.** We only distribute AND-nodes as jobs, i.e. OR-nodes are never assigned and are directly expanded in the manager. For example, in Figure 2, the OR-node \(b\) is not assigned as a job even if \(v_{b}<v_{thr}\). The manager creates the AND-node \(h\) from the leaf node \(b\), then assigns it to a worker as a job. The underlying intuition is that assuming the PCN policy head output is accurate as a move ordering heuristic, the first guess will often be the move that leads to a solution for OR-nodes. Therefore, by skipping OR-nodes job assignment entirely, the manager gains a 1-ply look ahead. In practice, all three job assignment schemes are applied simultaneously.

## 4 Experiments

We demonstrate our online fine-tuning game solver by solving several three-move 7x7 Killall-Go openings. 7x7 Killall-Go is a variant of Go, where the rules are the same except that: (a) Black places two stones initially, and (b) Black wins if all white stones are killed; otherwise, White wins. Since White aims to live, winning specific openings for this variant is equivalent to solving a L&D problem. Many Go experts believe that 7x7 Killall-Go is a win for White. So far, no proof has been published yet. In this paper, we only focus on weakly solved games  in which White wins. Thus, White is considered the OR-player throughout.

### The 7x7 Killall-Go solver

We build our 7x7 Killall-Go solver upon an AlphaZero training framework . First, we pre-train a PCN \(_{0}\) to serve as heuristics for the game solver (starting from an empty board). We incorporate the Gumbel AlphaZero algorithm  into PCN training, since it performs equivalently well even with a small simulation count. This reduces the computation cost for online fine-tuning without compromising accuracy. The pre-training took around 52 1080Ti GPU-hours. Next, we incorporate several useful techniques into the solver to accelerate solving. This includes relevance zone-based search (RZS) , zone pattern tables , and GHI handling to deal with cycles in Go . This solver is then used as workers in a distributed game solver. The manager is also based on the above solver, with the job assignment techniques added, as described in subsection 3.3. The OFT is similar to the PCN pre-training, but with fine-tuning as described in subsection 3.2.

Two kinds of distributed game solvers are considered for our experiments. The _baseline solver_ uses the manager and worker only, while using a pre-trained, fixed \(_{0}\) as the heuristic throughout the whole proof search. In contrast, the _online fine-tuning solver_ uses the OFT to fine-tune the PCN heuristic dynamically during the proof search. In addition, we consider three variations of online fine-tuning solvers using solved positions (SP), critical positions (CP), and a combination of both (SP+CP). Both solvers use \(v_{thr}=16.5\) for the manager job granularity.4 For fairness, we ran both solvers on 9 1080Ti GPUs. The baseline solver uses one GPU for the manager and eight GPUs shared among workers. For the online fine-tuning solver, the manager and trainer each uses one GPU, while workers share the remaining seven GPUs. Detailed implementations and other machine configuration details are specified in the appendix.

We select a set of three-move openings based on recommendations from experts, including a professional 9-dan player. These openings can be classified into four groups, n

Figure 3: Four 7x7 Killall-Go opening groups, including (a) four openings, JA-JD; (b) six openings, KA-KF; (c) one opening, DA; (d) five openings, SA-SE.

shared first move opening: _jump_ (J), _knight's move_ (K), _diagonal jump_ (D), and _stretch_ (S), shown in Figure 2(a), 2(b), 2(c), and 2(d) respectively. For each opening group, experts also suggest the most likely winning move for White. We split these openings into several three-move openings by exploring Black's possible replies. For each opening group, we select the most difficult replies by Black according to expert recommendations and the PCN policy head output. For simplicity, in the rest of the paper, _JA_ represents the position resulting from Black playing at A in the jump group; _KB_ represents Black playing at B in the knight's move group, etc. The gray solid squares represent moves that are also suggested by the PCN, but cannot be solved by both of the baseline and online fine-tuning solvers in one day. With limited computing resources, we leave these openings for future work. In total, we use 16 three-move openings as shown in Figure 3.

### Baseline versus online fine-tuning

Table 1 lists statistics for solving the 16 three-move openings by the baseline solver and three variants of online fine-tuning solvers. In general, all online fine-tuning solvers outperform the baseline solver in most openings. online-sp, online-cp, and online-sp+cp, require only about 48.53%, 21.69%, and 23.07% of the visited nodes, and 52.67%, 23.54%, and 24.99% of the computing time compared to baseline. This shows that fine-tuning PCNs with critical positions, which are currently being solved by either the manager or workers, provides better heuristics for the current search tree and accelerates the solving process. Furthermore, online-sp+cp has nearly the same performance as online-cp, with both methods outperforming online-sp. This means that training with critical positions is more important than solved positions. To reduce the overhead of sending both solved and critical positions, we simply choose online-cp for further analysis, i.e. all instances of _online fine-tuning solver_ for the rest of this section refers to online-cp. In conclusion, these results indicate that \(_{0}\) provides less accurate heuristics, which impacts the proof search negatively. By performing online fine-tuning with either solved or critical positions, we can fine-tune the PCN dynamically according to the manager's current focus and therefore find faster solutions.

Table 1 also shows another interesting result: the larger the problem, the better the improvement. For better visualization, the solving times are depicted as a bar chart in Figure 4, where the x-axis is sorted according to the solving time of the baseline solver. In Figure 4, the online fine-tuning solver solves all openings within 40,000 seconds, while the baseline solver uses more than one day to solve six openings. Most impressively, for _JA_, the online fine-tuning solver performs about 6.35 times faster than the baseline, reducing the computation time from 142,115 to 22,384 seconds, while the number of visited nodes is reduced from 8.96 billion to 1.29 billion or so nodes.

The online fine-tuning solver does not always perform better than the baseline, especially when the openings are relatively easy to solve for the baseline. In Table 1, the baseline uses less time to solve _SA_ and _SC_. The following reasons may be why this limitation exists for smal

    &  &  &  &  \\   & \# Nodes & Time (s) & \# N Nodes & Time (s) & \# PCN & \# Nodes & Time (s) & \# PCN & \# Nodes & Time (s) & \# PCN \\  JA & 8,964,444,959 & 142,115 & 4,054,562,593 & 69,699 & 359 & **1,288,601,416** & **22,384** & 186 & 1,425,668,707 & 24,865 & 225 \\ JB & 7,137,544,712 & 155,786 & 3,787,627,517 & 83,454 & 424 & **1,576,437** & 319 & 319,577 & 272 & 1,601,479,130 & **31,455** & 283 \\ JC & 721,004,784 & 12,514 & 819,264,890 & 13,963 & 57 & **36,391,324** & **657** & 59 & 41,108,746 & 8,343 & 69 \\ JD & 1,271,426,148 & 30,209 & 8,36,365,092 & 1,936 & 13 & 545,655,175 & 11,083 & 102 & **502,966,563** & **10,896** & 103 \\ KA & 134,881,952 & 2,103 & 143,144,428 & 2,621 & 14 & 111,838,259 & 1,202 & 18 & **194,055,173** & **1,931** & 18 \\ KB & 10,135,035,652 & 165,833 & 3,794,191 & 10,64,935 & 305 & **2,228,799,149** & **384,974** & 32,527,488,112 & 43,200 & 386 \\ KC & 734,217,263 & 747 & 45,217,101 & 1,156 & 6 & 26,441,899 & 758 & 6 & 2,508,784 & **706** & 6 \\ KD & 2,754,21,373,47 & 47,494 & 1,5047,329 & 2,5715 & 126 & 955,271 & 11,743,44 & 145 & **920,090,808** & **10.537** & 148 \\ KE & 1,197,819,407 & 18,771 & 21,614,614,577 & 3,917 & 21 & 181,418,954 & 3.336 & 30 & **168,590,287** & **3.095** & 28 \\ KF & 9,516,440,320 & 147,271 & 6,0880,368,686 & 100,690 & 519 & 2,107,185,353 & 35,418 & 285 & **202,075,585** & **35,197** & 305 \\ DA & 7,324,273,383 & 112,574 & 3,015,438,589 & 50,046 & 248 & 1,761,424,477 & 30,313 & 266 & **1,065,511,013** & **283,337** & 235 \\ SA & 51,27,228,88 & **937** & 54,547,495 & 1,471 & 7 & 41,863,480 & 992 & **41,796,555** & 1,105 & 10 \\ SB & 215,380,130,386 & 360,670,358 & 1,423 & 7, **55,445,185** & **1364** & 12 & 100,591,847 & 2,258 & 20 \\ SC & 97,559,402 & **1,557** & 213,839,777 & 3,553 & 19 & 98,661,355 & 1,715 & 16 & **93,535,813** & 1,655 & 15 \\ SD & 8,187,017,679 & 214,644,384 & 3,821,472,453 & 630,508 & 329 & **1,395,444,427** & **23,751** & 154 & 1485,439,307 & 25,531 & 224 \\ SE & 4,297,808,879 & 64,227 & 2,065,292 & 39,347,347 & 166 & **77,256,934** & **12,465** & 103,200,741,76,02,048 & 182 \\  _sum_ & 62,060,780,290 & 1,021,692 & 30,119,490,145 & 538,092 & - & **13,462,626,704** & **240,556** & - & 14,315,792,186 & 255,359 & - \\   

Table 1: The number of nodes and time to solve 16 7x7 Killall-Go three-move openings by the baseline and three variants of online fine-tuning solvers. “# Nodes” lists the numbers of all nodes visited by the manager and workers together. All the listed times are rounded to the nearest second. The rightmost column lists the number of PCN models produced by the online fine-tuning trainer.

online fine-tuning solver relies on a trainer to fine-tune the PCN. The quicker the problem can be solved, the less time the trainer has to fine-tune specific PCN weights. Consequently, it has a weaker impact on improvement. This is corroborated by the fact that these two openings end up with less than 20 PCN versions,5 as shown in Table 1. Second, when compared to the baseline, the online fine-tuning solver has less computing power for the workers. The trainer overhead takes up a GPU and leaves workers with seven instead of eight GPUs used by the baseline.

It is also worth mentioning that all three-move openings in both _jump_ and _stretch_ are solved. As these openings are considered the most difficult moves, we expect that both _jump_ and _stretch_ (i.e., the two-move openings) can probably be solved completely in the near future, with more computing resources.

### Behavior analysis for the online fine-tuning solver

We further analyze the behavior of two solvers by studying the opening \(J\)A, where the online fine-tuning solver has the largest speedup among all openings, i.e. 6.35 times faster than the baseline. We observe several positions in which the winning moves for White differed between the two solvers. For example, a crucial sub-position in the solution tree is shown in Figure 4(a). The baseline and online fine-tuning solver chose moves A and B to search, respectively. We examine \(_{0}\) and find that the

Figure 4: Solving time comparison for 16 three-move 7x7 Killall-Go openings.

Figure 5: Behavioral analysis of the online fine-tuning solver for the opening JA.

probabilities from the policy network for moves A and B are 0.416 and 0.133, respectively. As a result, the baseline solver has a lower chance to explore B. In contrast, with the help of the OFT, the online fine-tuning solver quickly realizes that solving B is faster, though it still attempted to search A initially, as it starts with the same \(_{0}\). In terms of node counts, the baseline spent a total of 1.63 billion nodes, with approximately 1.48 billion (91.00%) and 35.50 million (2.18%) nodes spent on searching A and B, respectively, while the online fine-tuning solver spent a total of 136.47 million nodes on that position, with approximately 22.13 million (16.21%) and 95.17 million (69.74%) nodes spent on searching A and B, respectively. This example clearly demonstrates the advantage of using the OFT.

Next, we investigate the set of critical positions maintained by the online fine-tuning trainer, as described in subsection 3.2.2. During fine-tuning, the trainer randomly selects positions from the queue, then runs self-play games from these positions. Figure 4(b) shows the average path length of critical positions during training in the y-axis, and the training iteration in the x-axis, where the trainer generates a new PCN version for every iteration. The length starts at around 10 and gradually increases to nearly 25 in the end. This is because as the manager search tree grows through node expansion, the critical positions are chosen from deeper parts of the tree. The curve also fluctuates as the proof search progresses. This is because the manager tends to focus on a sub-problem at a time. When a subtree is solved, the manager may then shift its attention to other unsolved parts of the proof search, which can have a relatively shallower depth. We also analyze similar figures for other openings in the appendix.

### Updating PCNs in online fine-tuning

We investigate the impact of updating PCNs for the manager and workers during online fine-tuning. We select four openings, _JC_, _KE_, _DA_, and _SE_, one from each opening group, for this experiment. Table 2 summarizes results, where baseline denotes the baseline solver, online-cp denotes the online fine-tuning solver that updates PCNs for both the manager and workers (as described in subsection 3.2.2), online-cp-m denotes updating the PCN for the manager only, and online-cp-w for workers only.

First, updating PCNs for both the manager and workers performs the best. By using consistent PCNs, jobs assigned by the manager are efficiently solved by the workers. With inconsistent PCNs, the results can be even worse than the baseline. Generally, online-cp-m outperforms online-cp-w, except for the opening _SE_. We find that the pre-trained PCN causes online-cp-w to divide its computing resources across several OR nodes (white moves), many of which are relatively difficult to solve. In contrast, for online-cp-m, with the updated PCNs, the manager focuses on one white move (or a smaller number of white moves), of which they are much easier to solve. Thus, even if there is a mismatch between the manager and workers in online-cp-m, focusing on one good white move can still result in efficiency.

### Ablation study for job assignment schemes

We conduct an ablation study in the online fine-tuning solver to analyze the impact of job assignment schemes, described in subsection 3.3. We only include ablations for top-\(k\) selection and AND-player job assignment, since virtual solving is required to avoid job redundancy. The ablation study is performed on the same four openings as subsection 4.4, _JC_, _KE_, _DA_, and _SE_. Table 3 summarizes the ablation results, where online-cp denotes the online fine-tuning solver that uses both schemes; the other three versions denote the ablations by removing specific schemes from online-cp. If the top-selection is removed, the manager always selects the best child during selection for both AND-nodes and OR-nodes. If we do not follow the AND-player job assignment scheme (abbreviated as _AND assg._ in the table), the manager assigns both AND-player and OR-player jobs.

From Table 3, online-cp performs the best in general. In particular, online-cp only requires around 48.37% of the computing time on average over all four openings, compared to the solver without both schemes (the last row in the table). When comparing each technique individually, the improvement varies from problem to problem.

## 5 Discussion

This paper demonstrates the potential of using online fine-tuning for game solving. On average across multiple openings, our proposed online fine-tuning solver only uses 23.54% of the computation time compared to the baseline. Our distributed game solver is the first online fine-tuning method for problem solving based on AlphaZero-like algorithms. Although we focus on online fine-tuning throughout this paper, we can also claim that the complete distributed game solver is a life-long learning system. The online trainer continuously refines heuristics for unfamiliar (unsolved) positions, and forgets the previously learned knowledge (solved positions). However, this forgotten knowledge is saved (remembered) in the manager's solution tree. As a result, the worker and trainer will not need to evaluate these positions again.

There are many other topics for future investigation. Our experiments on the four challenging 7x7 Killall-Go opening groups show that two groups are likely to be solved in the near future. However, for the other two, or even 7x7 Killall-Go in its entirety, we expect more novel techniques are needed. As for the standard Go game, the largest solved board to date is only 5x6 in size , with no published progress in 14 years. We expect online fine-tuning to be one of the key improvements that can help push this boundary. As for generality, our method is not limited to Go but can be easily applied to other two-player zero-sum games like Hex or Othello. Moreover, we expect it has the potential to extend to single-player games such as Rubik's Cube, or even to other non-game fields, such as automated theorem proving  or chemical syntheses [42; 43].