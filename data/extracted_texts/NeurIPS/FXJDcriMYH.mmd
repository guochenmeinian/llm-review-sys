# Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training

Wenyu Du\({}^{1}\)1 Tongxu Luo\({}^{2,3}\)2 Zihan Qiu\({}^{4}\) Zeyu Huang\({}^{5}\) Yikang Shen\({}^{6}\)

Reynold Cheng\({}^{1}\) Yike Guo\({}^{2}\) Jie Fu\({}^{2}\)3

\({}^{1}\)School of Computing and Data Science, The University of Hong Kong \({}^{2}\)HKUST

\({}^{3}\)USTB \({}^{4}\)Tsinghua University \({}^{5}\)University of Edinburgh \({}^{6}\)MIT-IBM Watson AI Lab

wydu@cs.hku.hk tongxuluo@gmail.com jiefu@ust.hk

Equal Contributions.Work done during interning at HKUST.Corresponding Author.

###### Abstract

LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical _O_bstacles: (_O_1) lack of comprehensive evaluation, (_O_2) untested viability for scaling, and (_O_3) lack of empirical guidelines. To tackle \(O\)1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depth-wise stacking operator, called \(G_{}\), exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into \(G_{}\) to address \(O\)2 and \(O\)3. For \(O\)2 (untested scalability), our study shows that \(G_{}\) is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our \(G_{}\) model converges to the same loss with 194B tokens, resulting in a 54.6% speedup. We further address \(O\)3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for \(G_{}\), making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of \(G_{}\). Our code and pre-trained model are available at https://llm-stacking.github.io/.

## 1 Introduction

Emergent abilities of Large Language Models (LLMs) rely on scaling-up . Empirical evidence from scaling laws  fuels the development of increasingly larger models, pushing the boundaries of LLMs capabilities. However, pre-training these gigantic models comes at a significant cost in terms of energy consumption and environmental impact  (e.g., pre-training Llama-3  consumes a total of 7.7M GPU hours and generates 2290 tons of carbon dioxide equivalent of carbon emissions). The efficient pre-training of LLMs is thus crucial, both from a scientific and a societal perspective, to ensure the continual growth and adoption of AI .

One promising research direction to accelerate model training involves leveraging trained smaller (base) models to expedite the training of larger (target) models, a technique known as model growth.

Concretely, model growth studies how to leverage the trained smaller model's parameters \(^{(s)}\) to initialize the larger model's parameters \(^{(l)}\). Current popular methods generally focus on expanding the parameters of the base model through techniques like splitting [10; 11; 12], copying [13; 14], or matrix mapping . There are also some approaches that initialize new parameters from scratch [16; 12; 17]. The primary objective is to accelerate the training of large models, and existing methods demonstrate promising speedup results on models such as BERT [11; 14; 18; 15; 12; 13]. Despite such empirical evidence and its alignment with the goal of efficient LLM pre-training, model growth methods are not widely adopted in the context of LLM pre-training [7; 19]. To our best knowledge, the only LLM that utilizes model growth for accelerating is FLM-101B , but it lacks a baseline LLM trained from scratch to compare. We observe three key _O_bstacles that hinder LLM pre-training from using existing model growth techniques, specifically:

\(\)_O_1: Lack of comprehensive assessment. Some existing model growth methods report results on LLM pre-training, but either lack a baseline comparison  or are still in exploratory stages [15; 13]. In contrast, most growth approaches are evaluated in encoder-based BERT models [14; 11; 18; 12; 13; 16; 17], which have different architecture and training configurations compared to prominent decoder-based LLMs such as Llama .

\(\)_O_2: The untested scalability. This scalability has two aspects: the model size and the amount of pre-training data. Regarding the model size, the existing approaches are only evaluated on smaller-scale BERT models or in preliminary experiments with LLMs. It is unclear whether these growth methods will continue accelerating training when applied to large-scale LLMs with more extensive evaluation. As for the amount of pre-training data, there are debates  over whether certain efficient training strategies may initially converge faster but ultimately perform similarly or worse than vanilla training methods when given ample computational resources (i.e., more training data).

\(\)_O_3: Lack of empirical guidelines. Scaling laws [3; 4] give clear empirical guidelines on pre-training computational-optimized LLMs, greatly stimulating and advancing the field. Yet, there is a lack of empirical guidelines on growth techniques, discouraging LLM practitioners from adopting these approaches, especially considering the high costs of LLM pre-training.

These three obstacles are consequential in nature. Hence, in this work, we empirically revisit the concept of model growth as a solution to efficient LLM pre-training by tackling them one by one.

To tackle \(O\)1, we systematically evaluate model growth techniques on practical LLM pre-training. We first categorize existing growth methods and summarize them into four atomic growth operators, each of which can grow along two directions: widthwise (intra-layer) and depthwise (layer-wise). We illustrate them in Figure 2. These operators serve as representative choices for evaluating the performance of model growth techniques. We use these operators to expand 400M base models to 1.1B Llama-like LLMs and continually pre-train them. Next, we evaluate these growth techniques on the training loss and eight standard NLP benchmarks from the Harness toolkit . We found the direct operator that stacks depthwisely \(G_{stack}\) consistently outperforms others across overall evaluation metrics, demonstrating its potential in accelerating LLM pre-training. This motivates us to investigate extensively by addressing \(O\)2 and \(O\)3 on \(G_{stack}\).

To address \(O\)2, we investigate the \(G_{stack}\) operator's scalability to larger model sizes and to more training data. We conduct extensive experiments by scaling model size up to 7B parameters trained with 300B tokens, and pre-training a 410M model with over 750B training tokens. This is in contrast to the previous largest LLM pre-training experiment that uses model growth methods and has baselines for comparison, which is reported in Ligo , where a GPT2-1.5B model is trained for 15k steps (approximately 15B tokens). The

Figure 1: The training loss for two 7B LLMs, trained from scratch and with \(G_{direct}^{}\) (\(G_{stack}\)). At 300B tokens, \(G_{stack}\) accelerates by 54.6% compared to scratch.

results are encouraging, as we consistently observe significant improvements \(G_{}\) offers in both scenarios. For example, we achieve a remarkable 54.6% speedup in pre-training for a 7B model with 300B tokens (Figure 1). Interestingly, the loss improvement in our 750B-token experiment aligns with a logarithmic function. We further extend this logarithmic curve and determine that the improvement continues to be substantial even for the LLM trained with over 8T tokens. Moreover, we summarize all our experiments by estimating the LLM scaling law for LLMs pre-trained with \(G_{}\). Given the same target loss value, our analysis reveals a significantly reduced computational cost compared to the common scaling law .

For \(O\)3, we explore the practical guidelines for using \(G_{}\) in LLM pre-training. Given a computational budget, we determine the optimal strategy for two key factors of \(G_{}\), growth timing \(d\) and growth factor \(g\). Growth timing \(d\) relates to the training tokens used for small models before growing, and growth factor \(g\) refers to the factor between the non-embedding parameter number of the large models and the small models. We formalize our findings into equations that offer concrete suggestions for utilizing \(G_{}\). We believe this work could significantly pique the interest and bolster confidence in future LLM pre-training with model growth techniques, both in academia and industry.

To summarize, our contributions are four-fold: 1) We first systematically investigate model growth techniques and identify four atomic model growth operators, establishing a better understanding of the field in Section 3.1. 2) We then design a standard LLM pre-training testbed and perform comprehensive evaluations on these operators, finding that a simple depthwise stacking \(G_{}\) exhibits significant superiority in Section 3. 3) We further demonstrate the scalability of \(G_{}\) with experiments on LLMs ranging from 410M to 7B parameters and up to 750B training tokens in Section 4.1. 4) We also provide guidelines of equations on determining growth timing and growth factors for optimal use of \(G_{}\) in Section 4.2.

## 2 Related Work - Model Growth for Efficient Pre-training

The idea of growing neural networks dates back to the 1990s [24; 25; 26]. The pioneering work of Net2Net  marks a milestone, for the first attempt to study model growth in deep learning era. Net2Net expands width and depth while keeping original functions (namely function preserving) via randomly splitting old neurons and injecting new identity layers. The widthwise splitting method of Net2Net represents a series of works that aim to "expand" the existing neurons to the desired larger size. Bert2Bert  serves as a BERT-based extension of the widthwise Net2Net. StagedGrow doubles the width by concatenating two identical layers and halves final loss to keep function-preserving. Lemon  suggests integrating a parameter into the splitting of neurons in Bert2Bert, aiming to break weight symmetry. Depthwisely, StackedBert  simply stacks duplicated layers to form a deeper model. In contrast to the above direct copy/split approaches, LiGO presents a learning-based method that initializes the larger model's parameters via learning a linear mapping from the smaller model's parameters.

Alongside the approaches that expand existing parameters, there are works that initialize new ones without relying on existing ones. For instance, MSG  proposes a multi-staged growing strategy that progressively expands transformer components, where the newly grown neurons are randomly initialized using a masking mechanism to ensure function preservation. Besides, some works have assigned specific values, like zero, to the newly initialized neurons to negate their influence [16; 12].

All the above methods are primarily explored in BERT or earlier stages of LLM pre-training. On the other hand, our objective is to present the first systematic review of model growth techniques in the LLMs era. To our knowledge, FLM-101B  is the only existing LLM that uses the growth method  for accelerating billion-scale LLM pre-training. Nonetheless, this work lacks a baseline model trained from scratch, making it difficult to assess the effectiveness of the model growth technique. In contrast, we aim to provide a comprehensive study by establishing a standardized testbed to compare LLMs trained from scratch and with various growth methods in LLM pre-training.

## 3 Systematically Assessing Model Growth for LLM Pre-Training

Existing model growth methods [14; 11; 18; 15; 12; 13; 16; 17] are mainly evaluated on BERT , with limited focus on decoder-only large-scale language models such as Llama . Moreover, these growth methods are often not comparable due to different training settings [14; 11; 17; 12].

Even some growth LLMs experiments are evaluated, their results are often incomplete [20; 15]. To overcome these limitations, we first summarize existing works [14; 11; 18; 15; 12; 13; 16; 17] into four atomic growth operators to represent these growth techniques. Then we build a standardized LLMs training testbed to pre-train LLMs with four growth operators on depthwise and widthwise directions and evaluate the results with both training loss and eight evaluation metrics in Harness .

### Growing LLMs with Growth Operators

Recent years, researchers have focused on enhancing the efficiency of training large models by making use of smaller pre-existing models [10; 11; 14; 18; 15; 12; 13; 16; 17]. These state-of-the-art methods can be categorized into two distinct groups. The first group focuses on deriving new neurons from the existing ones [10; 11; 14; 12; 15], while the second group focuses on initializing new parameters separately [18; 13; 16; 17]. Drawing from these two lines of research, we summarize four **atomic growth operators**. These operators include: **(A)** directly duplicating and stacking old layers in a depthwise manner or splitting neurons in the same layer widthwisely, denoted as \(G_{}\), **(B)** generating expanded parameters using a learnable mapping matrix to the existing parameters, denoted as \(G_{}\), **(C)** setting the new parameters to zero, denoted as \(G_{}\), and **(D)** randomly initializing the new parameters, denoted as \(G_{}\). The illustration of four operators is shown in Figure 2. The \(G_{}\) and \(G_{}\) growth operators produce new neurons from the current ones, in contrast to \(G_{}\) and \(G_{}\) which initialize new parameters independently. _For the formal definitions of the operators and the differences to the existing growth methods in design, please refer to Appendix A_. Complex growth methods, such as those involving auxiliary loss or exploring training dynamics like learning rates [28; 29; 16] are interesting. But considering the high computational cost of LLM pre-training, we focus on simple, universally applicable growth operators for different LLM pre-training settings.

To make a fair comparison of the four growth operators for LLM pre-training, we define a standardized "one-hop" growth process that involves two training phases, small model training before growth and large model training after growth. We first train the small LLMs with \(d\) tokens before growing. Then, we use operator \(G\) to grow them to the target LLMs by a factor of \(g\) for

Figure 2: The simplified illustration of four growth operators \(G_{}\), \(G_{}\), \(G_{}\) and \(G_{}\), each of which can grow along widthwise (intra-layer) \(G^{}\) or depthwise (layer-wise) \(G^{}\). \(}\) is the parameters before growth, while \(}\), \(}\) and \(\) are the growth parameters derived from the old, randomly initialized, and zero-initialized respectively. Except \(G_{}\), other three operators only illustrates the widthwise growth.

and then continual pre-training the large LLMs for \(D\) tokens. Two key factors in the procedure are worth noting: the growth factor \(g\) and the data for base model training \(d\), which can be interpreted as "growth timing". We further evaluate each growth operator by separately examining in _depthwise (intra-layer)_ growth \(G^{}\) and _widthwise (layer-wise)_ growth \(G^{}\). Concretely, we start with base models (400M LLMs) trained on \(d=10B\) tokens, apply the four operators in both directions to scale them up to the target size of 1.1B (approximately a growth factor of \(g=4\)), and then continue training for an additional \(D=97.5B\) tokens. 4 Appendix B contains the LLM's architecture configuration and training details.

### Pre-Training 1.1B LLMs

We report results on training loss, eight standard Harness NLP benchmarks along with the average accuracy and the speedup ratio in Figure 3. Our key discovery reveals that depthwise growth \(G^{}\) exhibits a significant acceleration over both widthwise growth \(G^{}\) and training models from scratch, while surprisingly, \(G^{}\) does not offer any notable advantages. Among the depthwise growth operators, \(G^{}_{}\), \(G^{}_{}\), and \(G^{}_{}\), all outperform the baseline and \(G^{}_{}\). The underperformance of \(G^{}_{}\) in our study may be attributed to its design for gradual "mini-step" growth , whereas our unified approach uses a single step. Most notably, **depthwise stacking \(G^{}_{}\) emerges as the clear winner among growth operators, surpassing its competitors in speedup, training loss and nearly every Harness evaluation metric**. For example, compared to training models from scratch for 100B tokens, \(G^{}_{}\) achieves a significant efficiency gain, increasing training speed by 49.1%. The calculation of speedup please refer to Appendix B.2. The Appendix C presents more experiments on these operators, including their loss training and evaluation figures.

## 4 Delving Deeper Into Depthwise Stacking (\(G_{}\))

The empirical evidence suggests that certain growth operators, most notably \(G^{}_{}\), exhibit an impressive acceleration in LLM pre-training compared to the baseline approach of training models from scratch. We now turn our attention to a more in-depth examination of the \(G^{}_{}\). For ease of reference, **we will henceforth denote this depthwise stacking approach as operator \(G_{}\)

Figure 3: We evaluate operators using training loss and Lambda , ARC-c , ARC-e , Logiqa , PIQA , Sciq , Winogrande  and Wikitext PPL  totaling eight standard NLP benchmarks. After \(8 10^{20}\) FLOPs of training, \(G^{}_{}\) demonstrates a significant speedup.

\(=_{g M}\), where \(M\) is a small base model trained with \(d\) tokens, \(\) is the target model and \(g\) is the growth factor.

This section addresses the two main challenges (_O2_ and _O3_) outlined in the introduction: 1) evaluating the performance of \(G_{}\) in scaling scenarios, i.e. larger model sizes and more training tokens; and 2) determining the hyperparameters when using \(G_{}\), i.e., the growth timing \(d\) and growth factor \(g\).

### Scaling \(G_{}\)

**Scaling Model Sizes for \(G_{}\).** Our scaled-up experiments involve two larger model sizes: 3B and 7B. We initially train smaller models with a layer count that is one-quarter of our target layers (growth factor \(g=4\)), utilizing 10B tokens (\(d=10B\)). Then, we train the stacked models using over 300B tokens (\(D=300B\)) for both sizes. Figures 4 and 5 show the loss, and the NLP benchmarks average accuracy evaluated using the Harness evaluator for training 3B and 7B LLMs with 300B tokens, respectively.5 The acceleration of \(G_{}\) is consistent across two models and all evaluation metrics. For instance, considering the 3B model, Figure 4 demonstrates that \(G_{}\) achieves a 54.5% speedup in pre-training, improvement of 2.1 in NLP benchmarks average accuracy compared to the baseline 3B model trained with 240B tokens.

When comparing the 1B, 3B, and 7B models, it is evident that the benefits of \(G_{}\) are not reduced as the model size increases, implying that its acceleration effect can be leveraged even with larger models. Details of the evaluation results, including evaluation with instruction tuning, can be found in Appendix D. Appendix E compares our baselines with the open-source LLMs Pythia and tinyLlama.

**Scaling Training Tokens for \(G_{}\).** We next evaluate the scalability of the stacking operator on another dimension - training with more tokens. This is especially important in light of recent discussions about the validity of efficient training algorithms, which have sparked debate  over whether certain strategies may initially learn faster but ultimately perform similarly or worse than vanilla training methods when given more training data. Hence, we aim to pre-train a LLM using \(G_{}\) on a substantial amount of training tokens.

Figure 4: Training 3B LLMs with 300B tokens. \(G_{}\) significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, \(G_{}\) accelerates by 48.6% and 54.5% compared to scratch.

Figure 5: Training 7B LLMs with 300B tokens. \(G_{}\) significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 160B, 220B and 280B tokens, \(G_{}\) accelerates by 40.8%, 55.3% and 53.8% compared to scratch.

Concretely, we conduct an experiment on a 410M LLM using 750B tokens. Following the experimental setup in the previous section, we set growth ratio \(g=4\) and growth timing \(d=10B\) and conduct continuous pre-training on the target 410M LLMs for 750B tokens. Compared to the chinchilla-recommended 8B tokens  for the 410M model, our experimental setting also surpasses this value by nearly 100 times, reaching 750B tokens.

The training dynamics on Figure 5(a) indicate that \(G_{}\) remains effective in such cases. Details of the evaluation results with the similar findings can be found in Appendix D.3. Building upon the exceptional stability of LLM pre-training , we estimate loss improvements and plot them in Figure 5(b). The fitting curve indicates \(G_{}\) will continue to exhibit acceleration effects even after 8T tokens, which is over 1000 times longer than the recommended token number . It is also notable that this loss improvement after 8T training is not trivial for LLM pre-training, as previous studies  suggest that even minor improvements in the later phase can have a relatively substantial impact on downstream performance.

From a LLM practitioner's perspective, this is also crucial considering "overtraining", which involves training LLMs with significantly larger amounts of data than recommended by scaling laws , a common practice that has become prevalent. A notable example is the training of LLama 3-8B with 15T tokens, which is nearly 100 times greater than the token count recommended by the chinchilla scaling laws . Hence, this finding provides confidence in the consistent excellent acceleration of \(G_{}\) throughout the entire practical LLM pre-training process.

Estimating Scaling Laws.To further explore our findings, we graph our four models (410M, 1.1B, 3B, and 7B) on the same figure and attempt to uncover our "scaling law" using the \(G_{}\) operator. Following , we define the scaling power law using the equation \(L_{C}=aC^{b}\), where \(a\) and \(b\) are constants we need to fit, \(C\) represents the FLOPs, and \(L_{C}\) denotes the model's final loss under this FLOP. We use the curve_flit function in SciPy  to fit both the scratch model and the \(G_{}\) model and present the estimation scaling law in Figure 7. The figure shows that our \(G_{}\) scaling law exhibits improved efficiency compared to the scaling law estimated from baseline LLMs, achieving the same target loss while requiring much less computational resources. However, in light of the significant computational resources devoted to other scaling law studies , we acknowledge that our \(G_{}\) scaling law is an initial estimate subject to computation constraints, and a comprehensive study is left for future research.

### Determining Growth Timing and Growth Factor for Using \(G_{}\)

We comprehensively validate the effectiveness of the \(G_{}\) compared to training from scratch in Section 4.1. However, to incorporate \(G_{}\) into a LLM's pre-training process, we need to determine two crucial hyperparameters: the growing time (\(d\)) and the growing factor (\(g\)). In our previous experiments, we rely on ad-hoc choices for these parameters, thereby lacking a systematic approach

Figure 6: Training 410M LLMs with 750B tokens. \(G_{}\) significantly outperforms scratch in (a) loss. At 400B tokens, we observe a 53.1% acceleration, and even at 700B tokens, there is still a 31.0% acceleration. (b) We fit the difference between the losses of the scratch and \(G_{}\) and find that the acceleration with \(G_{}\) remain sustainable for longer training.

Figure 7: We plot scaling law lines based on 410M, 1.1B, 3B, 7B LLMs and make two predictions at the same losses of original computational-optimized 13B and 70B LLMs.

to determining them when use \(G_{}\). There exists research on investigating the growth timing , but the settings are quite different from the LLM pre-training. Therefore, this section offers a clear guide for practitioners looking to optimize using the \(G_{}\) operator in LLM pre-training processes.

We begin by offering a formal definition. When given a computational budget \(C\), established scaling power laws  exist to guide the non-embedding parameters \(N\) and the number of training tokens \(D\) to achieve the lowest model loss in the case of training from scratch. However, tuning hyperparameters becomes more complex when the fixed budget \(C\) is allocated to find the optimal model training strategy using the \(G_{}\) operator, which involves two training phases. Consequently, the overall computational budget \(C\) can be expressed as the sum of the two components: \(C=C1+C2\). Here, \(C1\) and \(C2\) represent the flops required to train the initial small models \(C1=FLOPs(n,d)\), and the large model \(C2=FLOPs(N,D)\) respectively, where \(n\) and \(d\) denote the parameters and training tokens of the small model, and \(N\) and \(D\) represent the parameters and training tokens of the large model. Since the large model is grown by a factor of \(g\) such that \(N=gn\), we have \(C=C1+C2=FLOPs(g,N,d)+FLOPs(N,D)=FLOPs(g,N,d,D)\).

So when given a budget \(C\), our objective is to identify the optimized values \(D\), \(N\), \(d\), \(g\) that minimize the loss \((D,N,d,g)\). However, simultaneously optimizing the above four variables can be computationally expensive. Therefore, instead of searching for global optimals, we separately determine two factors closely related to the \(G_{}\): the training tokens for the small model (growth timing) \(d\) and the growth factor \(g\):

\[*{arg\,min}_{f,h}\ (D,N,d,g)d=f(D,N),g=h(D,N)\] (1)

Determining Growth Timing: \(d\).We first explore the effect of growth timing, i.e. the training token \(d\) for the small model. Particularly, we apply the \(G_{}\) operator to a series of small models trained with \(d=0B,1B,5B,10B,20B,50B\) tokens. Subsequently, we stack them to the target layers with growth factor \(g=4\) and train for a fixed set of computational FLOPs. We replicate the above experiments using three target model sizes \(N=410M,1.1B,3B\) and plot each set of IsoFLOP points in Figure (a)a, (b)b and (c)c. Surprisingly, even a small model trained with just \(1B\) tokens exhibits a significant speedup compared to the directly stacking small random initialized models (represented as "0B"). While 0B's performance is similar to models trained from scratch, implying stacking itself does not serve as an effective initialization method. Furthermore, by applying smoothing techniques to model IsoFLOP curves as parabolas, we identify the optimized value of \(d\) that minimizes loss for each FLOP count, leading us to hypothesize the existence of a logarithmic equation involving \(N\), \(C\), and \(d\):

Figure 8: In 410M, 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth timing \(d\) given a set of FLOPs to form IsoFLOP figures. We find a clear valley in loss, indicating that for a given FLOP budget, there exists an optimal growth timing \(d\) for the \(G_{}\) operation.

Figure 9: We fit a contour figure for predicting \(d\) given \(C\) and \(N\). These optimal growth timing \(d\) fit the figure well.

\[log_{10}(d)=a_{10}(N)+(C)}+c\] (2)

After fitting, we obtain \(a=0.88\), \(b=163.27\) and \(c=-5.74\) and we plot the contour figure in Figure 9. It can be observed that our estimated curves align well with the actual optimal points.

#### Determining Growth Factor:

\(g\).Another factor we determine is the growth factor \(g\). As models with 3B and 7B parameters have identical depths, we run experiments using two model sizes: 1.1B (24 layers) and 3B (32 layers). Specifically, we vary the stack factors to \(g=2,4,8,24\) for the 1.1B model and \(g=4,8,16,32\) for the 3B model while keeping the base models trained with \(d=10\)B tokens. The smoothed IsoFLOP curves are plotted in Figure 10. Interestingly, even with a relatively shallow 2-layer base model and a growth factor of \(g=16\), we observe a remarkable improvement compared to the baseline 3B model (\(g=1\)). However, when using a 1-layer base model, \(G_{}\) underperforms compared to the baseline. Our curves indicate that the optimal growth factor \(g\) lies between 2 and 4.

However, unlike determining training token \(d\), we cannot generate sufficient data to estimate the relationship between \(N\), \(C\), and \(g\), due to computational constraints. Thus, this work suggests a constant growth factor of \(g=4\). We also include our preliminary estimated equation and contour figure for \(g\) in the Appendix F. All evaluation results of Section 4.2 are listed in Appendix G.

## 5 Ablation and Discussion

To further give insights into adopting model growth techniques in LLM pre-training, we ablate variances for \(G_{}\) and discuss function preserving in general model growth techniques.

### Ablation: How to Stack?

It is worth noting that \(G_{}\) differs from the algorithm proposed in StackedBERT , which utilizes a gradually stacking strategy. Hence, we compare our "one-hop" \(G_{}\) and their gradual stacking approach. Following the methodology introduced in StackBERT, we employ a two-step stack strategy. Given our target model size of 1.1B with 24 layers, we start with a 6-layer model. Subsequently, we train it on 10B tokens and double the model's depth through stacking, repeating this step twice (train-stack-train-stack) to achieve the desired scale. Our experiments demonstrate that \(G_{}\) outperforms gradual stacking approaches on loss and downstream evaluations. For example, the evaluation results show that \(G_{}\) achieves a 2.4 higher average accuracy and 0.6 better Wikitext PPL than gradual stacking when pre-training large models for 100B tokens. The results can be found in Appendix H.1. We further compare other stacking variations, such as stacking via interpolation and partial stacking of certain layers which are also adopted in LlamaPro  and Solar , and leave our detailed findings in the Appendix H.2 and H.3.

### Discussion: Why Does Function Preserving Fail?

Function preservation (FP) is a key concept that underlies most model growth approaches [10; 11; 12; 17]. The idea is intuitive that a larger model should initialize parameters that can represent the same

Figure 10: In 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth factor \(g\) given a set of FLOPs as IsoFLOP figures. The optimal g falls between 2 and 4.

function as the ones in the smaller model, i.e. \( x,f(x;^{(s)})=f(x;^{(l)}_{init})\), where \(x\) is the input. We give a mathematical definition of FP in the Appendix I.1.

We find it intriguing that our \(G_{}\) approach, which violates FP, emerges as the most effective in our study. To further investigate, we conduct a simple ablation study to break FP by introducing noise on the strict-FP operator \(G_{}^{}\). We initialize the new neurons by a weighted combination of two sets of parameters: those from \(G_{}^{}\) and those from random initialization. The weighting factor is controlled by a noise ratio. Our findings are intriguing. After 40B tokens training, adding 20% noise outperforms original \(G_{}^{}\) by 0.27 on the Wikitext PPL and 0.41 on the average accuracy score.

We also add noise for \(G_{}\). When we add 20% noise, our LLM performs slightly better than the no-noise model. However, when the noise level exceeds 20%, the performance significantly deteriorates. These results indicate that function preservation may not be the sole determining factor for model growth. **In other words, exploring ways to accelerate the training of larger models and strict preserving function during growth might represent two overlapping yet distinct research directions.** The experimental details are provided in the Appendix I.2.

## 6 Conclusion

This work empirically explores model growth approaches for efficient LLM pre-training. We address three key challenges of current model growth research for efficient LLM pre-training. We first comprehensively evaluate model growth techniques into four atomic operators and explore depthwise growth \(G_{}\) beats all other methods and baselines in various evaluations. We next address concerns about the scalability of \(G_{}\) by extending the model and training data scales. Furthermore, we systematically analyze the usage of the \(G_{}\) operator, focusing on growth timing and growth factor. Based on this analysis, we formalize a set of guidelines for effectively utilizing the \(G_{}\) operator. In addition, we provide in-depth discussions and comprehensive ablation studies of \(G_{}\), shedding light on the broader implications of our work.

## 7 Limitations

While our work has demonstrated remarkable potential, four limitations deserve further attention. One limitation is the constraint of computation resources. For example, we only compare two sets of growth factor \(d\) configurations, which limits the capacity to derive a formula for determining the optimal growth factor \(d\). Another limitation of our work is the focus on relatively simple operator choices, where we prioritize simplicity over exploring more sophisticated strategies. For instance, we do not extensively investigate the multi-step growth or dynamic modifications to the training process, such as adjusting the learning rate during continual pre-training. The third limitation involves the incomplete cosine learning rate schedule during training. This also arises from the resource-intensive nature of pre-training LLMs and the constraints on available computational resources. Therefore, we adopt a strategy where we initially set a large number of training tokens and then we pre-train LLMs until the training runs are interrupted by tasks with higher priority. Lastly, although this study's scope is an empirical exploration and the content is self-contained, there is a lack of theoretical insights into the success of \(G_{}\) in LLM pre-training.6 Nonetheless, we will release all LLM checkpoints to facilitate the community's investigation into the theoretical principles behind our observations.

## 8 Acknowledgments

We thank all constructive comments from anonymous reviewers. Reynold Cheng and Wenyu Du were supported by the Hong Kong Jockey Club Charities Trust (Project 260920140), the University of Hong Kong (Project 109000579), the HKU Outstanding Research Student Supervisor Award 2022-23, and the HKU Faculty Exchange Award 2024 (Faculty of Engineering).