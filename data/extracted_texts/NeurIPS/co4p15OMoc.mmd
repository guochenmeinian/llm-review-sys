# Implicit Manifold Gaussian Process Regression

Bernardo Fichera\({}^{1}\) &Viacheslav Borovitskiy\({}^{2}\) &Andreas Krause\({}^{2}\) &Aude Billard\({}^{1}\)

\({}^{1}\) EPFL &\({}^{2}\) ETH Zurich

###### Abstract

Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional _manifold_ upon which the data actually lies, as postulated by the _manifold hypothesis_. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Matern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and improves the predictive performance and calibration of the standard Gaussian process regression in some high-dimensional settings.

## 1 Introduction

Gaussian processes are among the most adopted models for learning unknown functions within the Bayesian framework. Their data efficiency and aptitude for uncertainty quantification make them appealing for modeling and decision-making applications in the fields like robotics (Deisenroth and Rasmussen, 2011), geostatistics (Chiles and Delfiner, 2012), numerics (Hennig et al., 2015), etc.

The most widely used Gaussian process models, like squared exponential and Matern (Rasmussen and Williams, 2006), impose the simple assumption of differentiability of the unknown function while also respecting the geometry of \(^{d}\) by virtue of being stationary or isotropic. Such simple assumptions make uncertainty estimates _reliable_, albeit too conservative at times. The same simplicity makes these models struggle from the _curse of dimensionality_. We hypothesize that it is still possible to leverage these simple priors for real world high-dimensional problems granted that they are adapted to the implicit _low-dimensional submanifolds_ where the data actually lies, as illustrated by Figure 1.

Figure 1: _Euclidean_ (standard Matern-\({}^{5/2}\) kernel) vs _ours_ (implicit manifold) Gaussian process regression for data that lies on a dumbbell-shaped curve (\(1\)-dimensional manifold) assumed unknown. The data contains a small set of labeled points and a large set of unlabeled points. Our technique recognizes that the two lines in the middle are intrinsically far away from each other, giving a much better model on and near the manifold. Far away from the manifold it reverts to the Euclidean model.

Recent works in machine learning generalized Matern Gaussian processes for modeling functions on non-Euclidean domains such as manifolds or graphs (Azangulov et al., 2022; 2023; Borovitskiy et al., 2021; 2020; 2020). Crucially, this line of work assumes _known_ geometry (e.g. a manifold or a graph) beforehand. In this work we aim to widen the applicability of Gaussian processes for higher dimensional problems by _automatically learning_ the implicit low-dimensional manifold upon which the data lies, the existence of which is suggested by the _manifold hypothesis_. We propose a new model which learns this structure and approximates the Matern kernel on the implicit manifold.

Our approach can operate in both supervised and semi-supervised settings, with the emphasis on the latter: uncovering the implicit manifold may require a lot of samples from it, however these samples need not be labeled, and unlabeled data is usually more abundant. Taking inspiration in the manifold learning results of Coifman and Lafon (2006), Dunson et al. (2021) and others we approximate the unknown manifold by an appropriately weighted nearest neighbor graph. Then we use graph Matern kernels thereon as approximations to the manifold Matern kernels of Borovitskiy et al. (2020), extending them to the vicinity of the manifold in the ambient \(^{d}\) in an appropriate way.

### Related Work and Contribution

High-dimensional Gaussian process regression is an area of active research, primarily motivated by decision making applications like Bayesian optimization. There are three main directions in this area: (1) selecting a small subset of input dimensions, (2) learning a small number of new features by linearly projecting the inputs and (3) learning non-linear features. Our technique belongs to the third direction. Further details on the area can be found in the recent review by Binois and Wycoff (2022).

A close relative of our technique in the literature is described in Dunson et al. (2022). It targets the low-dimensional setting where the inputs are densely sampled on the underlying surface. It is based on the heat (diffusion) kernels on graphs as in Kondor and Lafferty (2002) and uses the Nystrom method to extend kernels to \(^{d}\), both of which may incur a high computational cost. Another close relative is the concurrent work by Peach et al. (2023) on modeling vector fields on implicit manifolds.

We are targeting the high-dimensional setting. Here, larger datasets of partly labeled points are often needed to _infer_ geometry. Because of this, we emphasize computational efficiency by leveraging sparse precision matrix structure of Matern kernels (as opposed to the heat kernels) and use KNN for sparsifying the graph and accelerating the Nystrom method. This results in linear computational complexity with respect to the number of data points. Furthermore, the model we propose is fully differentiable, which may be used to find both kernel and geometry hyperparameters by maximizing the marginal likelihood. Finally, to get reasonable predictions on the whole ambient space \(^{d}\), we combine the prediction of the geometric model with the prediction of a classical Euclidean Gaussian process, weighting these by the relative distance to the manifold.

The geometric model is differentiable with respect to its kernel-, likelihood- and geometry-related hyperparameters, with gradient evaluation cost being linear with respect to the number of data points. After training, we can efficiently compute the predictive mean and kernel as well as sample the predictive model, providing the basic computational primitives needed for the downstream applications like Bayesian optimization. We evaluate our technique on a synthetic low-dimensional example and test it in a high-dimensional large dataset setting of predicting rotation angles of rotated MNIST images, improving over the standard Gaussian process regression.

## 2 Gaussian Processes

A Gaussian process \(f(m,k)\) is a distribution over functions on a set \(\). It is determined by the mean function \(m()=\,f()\) and the covariance function (kernel) \(k(,^{})=(f(),f(^{}))\).

Given data \(,\), where \(=(_{1},..,_{n})^{}\) and \(=(y_{1},..,y_{n})^{}\) with \(_{i}^{d},y_{i}\), one usually assumes \(y_{i}=f(_{i})+_{i}\) where \(_{i}(0,_{}^{2})\) is IID noise and \(f(0,k)\) is some _prior_ Gaussian process, whose mean is assumed to be zero in order to simplify notation. The posterior distribution \(f\) is then another Gaussian process \(f(,)\) with (Rasmussen and Williams, 2006)

\[()=_{()}_{ }+_{}^{2}^{-1},\ \ \ \ \ (,^{})=_{(,^{})}-_{( )}_{}+_{ }^{2}^{-1}_{(^{})},\] (1)

where the matrix \(_{}\) has entries \(k(_{i},_{j})\), the vector \(_{()}=_{()}^{}\) has components \(k(_{i},)\). If needed, one can efficiently sample \(f\) using _pathwise conditioning_(Wilson et al., 2020; 2021)Matern Gaussian processes--including the limiting \(\) case, squared exponential Gaussian processes--are the most popular family of models for \(=^{d}\). These have zero mean and kernels

\[k_{,,^{2}}(,^{})=^{2}}{ ()}-^{}\|}{} ^{}K_{}-^{}\|}{} \] (2)

where \(K_{}\) is the modified Bessel function of the second kind (Gradshteyn and Ryzhik, 2014) and \(,,^{2}\) are the hyperparameters responsible for smoothness, length scale and variance, respectively. We proceed to describe how Matern processes can be generalized to inputs \(\) lying on _explicitly given_ manifolds or graphs instead of the Euclidean space \(^{d}\).

### Matern Gaussian Processes on Explicit Manifolds and Graphs

For a domain which is a Riemannian manifold, an obvious and natural idea for generalizing Matern Gaussian processes could be to substitute the Euclidean distances \(\|x-x^{}\|\) in Equation (2) with the geodesic distance. However, this approach results in ill-defined kernels that fail to be positive semi-definite (Feragen et al., 2015; Gneiting, 2013).

Another direction for generalization is based on the stochastic partial differential equation (SPDE) characterization of Matern processes first described by Whittle (1963): \(f(0,k_{,,^{2}})\) solves

\[}-_{^{d}}^{+}f=,\] (3)

where \(_{^{d}}\) is the standard Laplacian operator and \(\) is the Gaussian white noise with variance proportional to \(^{2}\). If taken to be the definition, this characterization can be easily extended to general Riemannian manifolds \(=\) by substituting \(_{^{d}}\) with the Laplace-Beltrami operator \(_{}\), taking \(d=\) and substituting \(\) with the appropriate generalization of the Gaussian white noise (Lindgren et al., 2011). Based on this idea, Borovitskiy et al. (2020) showed that on _compact_ Riemannian manifolds, Matern Gaussian processes are the zero-mean processes with kernels

\[k_{,,^{2}}(x,x^{})=}{C_{,}} _{l=0}^{}}+_{l}^{--d/2 }f_{l}(x)f_{l}(x^{}),\] (4)

where \(-_{l},f_{l}\) are eigenvalues and eigenfunctions of the Laplace-Beltrami operator and \(C_{,}\) is the normalizing constant ensuring that \(}_{}k_{,,^{2}}(x,x)\, x=^{2}\). This, alongside with considerations from Azangulov et al. (2022) allows one to practically compute \(k_{,,^{2}}\) for many compact manifolds.

If the domain \(\) is a weighted undirected graph \(\), we can also use Equation (3) to define Matern Gaussian processes on \(\)(Borovitskiy et al., 2021). In this case, \(_{^{d}}\) is substituted with the minus graph Laplacian \(-_{}\) and \((0,_{}^{2})\) is the vector of IID Gaussians. Here, SPDE transforms into a stochastic linear system, whose solution is of the same form as Equation (4) but with a finite sum instead of the infinite series, with \(d=0\) because there is no canonical notion of dimension for graphs and with \(_{l},f_{l}\) being the eigenvalues and eigenvectors--as functions on the node set--of the matrix \(_{}\). These processes are illustrated on Figure 2.

Figure 2: Kernel values \(k(,)\) and samples for the Matern-\(}{{2}}\) Gaussian processes on the sphere manifold \(_{2}\) and for the approximating Matern-\(}{{2}}\) process on a geodesic polyhedron graph \(\,_{2}\).

## 3 Implicit Manifolds and Gaussian Processes on Them

Consider a dataset \(=(_{1},..,_{N})^{}\), \(_{i}^{d}\) partially labeled with labels \(y_{1},..,y_{n}\), \(n N\). Assume that \(_{i}\) are IID randomly sampled from a compact Riemannian submanifold \(^{d}\). As by Section 2.1, the manifold \(\) is associated to a family of Matern Gaussian processes tailored to its geometry. We do not assume to know \(\), only the fact that it exists, hence the question is: how can we recover the kernels of the aforementioned geometry-aware processes from the observed dataset?

It is clear from Equation (4) that to recover \(k_{,,^{2}}\) we need to get the eigenpairs \(-_{l},f_{l}\) of the Laplace-Beltrami operator on \(\). Naturally, for a finite dataset this can only be done approximately. We proceed to discuss the relevant theory of Laplace-Beltrami eigenpair approximation.

### Background on Approximating the Eigenpairs of the Laplace-Beltrami Operator

There exists a number of theoretical and empirical results on eigenpair approximation. Virtually all of them study approximating the implicit manifold by some kind of a weighted undirected graph1 with node set \(\{_{1},..,_{N}\}\) and weights that are somehow determined by the Euclidean distances \(\|_{i}-_{j}\|\). The eigenvalues of the _graph Laplacian_ on this graph are supposed to approximate the eigenvalues of the Laplace-Beltrami operator, while the eigenvectors--regarded as functions on the node set--approximate the values of the eigenfunctions of the Laplace-Beltrami operator at \(_{i}\). To approximate eigenfunctions elsewhere, any sort of continuous (smooth) interpolation suffices.

There are three popular notions of graph Laplacian. Let us denote the adjacency matrix of the weighted graph by \(\) and define \(\) to be the diagonal degree matrix with \(_{ii}=_{j}_{ij}\). Then

\[_{}=-}_{},_{}= -^{-1/2}^{-1/2}}_{},_{}= -^{-1}}_{}.\] (5)

The first two of these are symmetric positive semi-definite matrices, the third is, generally speaking, non-symmetric. However, from the point of view of linear operators, all of them can be considered symmetric (self-adjoint) positive semi-definite: the first two with respect to the standard Euclidean inner product \(,\), and the third one with respect to the modified inner product \(,_{}= ,\). Thus for each there exists an orthonormal basis of eigenvectors and eigenvalues are non-negative.2

The most common way to define the graph is by setting \(_{ij}=-\|_{i}-_{j}\|^{2}/4^{2}\) for an \(>0\). If \(_{i}\) are IID samples from the _uniform_ distribution on the manifold \(\), then all of the graph Laplacians, each multiplied by an appropriate power of \(\), converge to the Laplace-Beltrami operator, both pointwise (Hein et al., 2007) and _spectrally_(Garcia Trillos et al., 2020), i.e. in the sense of eigenpair convergence, at least at the node set of the graph.3 However, if the inputs \(_{i}\) are sampled non-uniformly, graph Laplacians, at best, converge to different continuous limits, none of which coincides with the Laplace-Beltrami operator (Hein et al., 2007).

Coifman and Lafon (2006) proposed a clever trick to handle non-uniformly sampled data \(_{1},..,_{N}\). Starting with \(}\) and \(}\) defined in the same way as \(\) and \(\) before, they define \(=}^{-1}}}^{-1}\). Intuitively, this corresponds to normalizing by the kernel density estimator to cancel out the unknown density. The corresponding \(_{}\) then converges pointwise to the Laplace-Beltrami operator (Hein et al., 2007), though \(_{}\) and \(_{}\) do not: they converge to different continuous limits. Dunson et al. (2021, Theorem 2) show that, under technical regularity assumptions, eigenvalues \(_{k}\) and renormalized eigenvectors of \(_{}\) converge to the respective eigenvalues and eigenfunctions of the Laplace-Beltrami operator, regardless of the sampling density of \(_{i}\).

Both in the simple case and in the sampling density independent case, the graphs and their respective Laplacians turn out to be dense, requiring a lot of memory to store and being inefficient to operate with. To make computations efficient, sparse graphs such as KNN graphs are much more preferable over the dense graphs. Spectral convergence for KNN graphs is studied, for example, in Calder and Trillos (2022), for \(_{ij}=h(\|_{i}-_{j}\|/)\) with a compactly supported regular function \(h\), and withlimit depending on the sampling density. Unfortunately, we are unaware of any spectral convergence results in the literature that hold for KNN graphs and are independent of the data sampling density.

### Approximating Matern Kernels on Manifolds

Here we incorporate various convergence results, including but not limited to the ones described in Section 3.1, proving that all spectral convergence results imply the convergence of graph Matern kernels to the respective manifold Matern kernels.

**Proposition 1**.: _Denote the eigenpairs by \(_{l},f_{l}\) for a graph Laplacian and by \(_{l}^{},f_{l}^{}\) for the Laplace-Beltrami operator. Fix \(>0\). Assume that, with probability at least \(1-\), for all \(>0\), for \(\) small enough and for \(N\) large enough we have \(|_{l}-_{l}^{}|<\) and \(|f_{l}(_{i})-f_{l}^{}(_{i})|<\). Then, with probability at least \(1-\), we have \(k_{,,^{2}}^{N,,L}(_{i},_{j}) k_{, ,^{2}}(_{i},_{j})\) as \( 0,\,N,L\), where_

\[k_{,,^{2}}^{N,,L}(_{i},_{j})=}{C_{,}}_{l=0}^{L-1}}+_{l} ^{--()/2}f_{l}(_{i})f_{l}(_{j}).\] (6)

Proof.: First prove that the tail of the series in Equation (4) converges uniformly to zero, then combine this with eigenpair bounds. See details in Appendix A. 

**Remark**.: The convergence in \(_{i}\) can be lifted to pointwise convergence for all \(\) if eigenvectors are interpolated Lipschitz-continuously, simply because the eigenfunctions are smooth.

Inspired by this theory, we proceed to present the implicit manifold Gaussian process model.

### Implicit Manifold Gaussian Process

Guided by the theory we described in the previous section we are now ready to formulate the implicit manifold Gaussian process model. Given the dataset \(_{1},..,_{N}^{d}\) and \(y_{1},..,y_{n}\), we put

\[=}^{-1}}}^{-1}, }_{ij}=S_{K}(_{i},_{j})-_{i}-_{j}\|^{2}}{4^{2}},}_{ ij}=_{m}}_{im}&i=j,\\ 0&i j.\] (7)

Here \(S_{K}(_{i},_{j})=1\) if \(_{i}\) is one of the \(K\) nearest neighbors of \(_{j}\) or vice versa and \(S_{K}(_{i},_{j})=0\) otherwise; all matrices are of size \(N N\) and depend on \(\) and \(K\) as hyperparameters. Thanks to the coefficient \(S_{K}(_{i},_{j})\) that performs KNN sparsification, the matrix \(\) is sparse when \(K N\).4

Then we consider the operator \(_{}=-^{-1}\) defined by Equation (5), whose matrix is also sparse. Denoting its eigenvalues--ordered from the smallest to the largest--by \(0=_{0}_{1}_{N-1}\), and its eigenvectors--orthonormal under the modified inner product \(,_{D}\) and regarded as functions on the node set of the graph--by \(f_{0},f_{1},,f_{N-1}\), we define Matern kernel on graph nodes \(_{i}\) by

\[k_{,,^{2}}^{}(_{i},_{j})=}{C_{,}}_{l=0}^{L-1}_{,}(_{l})f_{l}(_{ i})f_{l}(_{j}),_{,}()=}+^{-},\] (8)

where \(L\) does not need to be equal to the actual number \(N\) of eigenpairs. Doing so means truncating the high frequency eigenvectors (\(f_{l}\) for \(l\) large), which always contribute less to the sum because they correspond to smaller values of \(_{,}(_{l})\). This can massively reduce the computational costs.

By Proposition 1, Equation (8) approximates the manifold Matern kernel with smoothness \(^{}=-()/2\). We adopt such a reparametrization because it does not require estimating the a priori unknown \(()\). This, however, makes the typical assumption of \(\{}{{2}},}{{2}},}{{2}}\}\) inadequate. We chose a particular graph Laplacian normalization, namely the random walk normalized graph Laplacian \(_{}\), to approximate the true Laplace-Beltrami operator regardless of the potential non-uniform sampling of \(_{1},..,_{N}\), based on the theoretical insights described in Section 3.1.

The kernel in Equation (8) is only defined on the set of nodes \(_{i}\), next step is to extend it to the whole space \(^{d}\). Extending kernels is usually a difficult problem because one has to worry about positive 

[MISSING_PAGE_EMPTY:6]

After hyperparameters are found--we will return to their search later--we need to compute the eigenpairs \(_{l},_{l}\) of \(_{}\). For this we run Lanczos algorithm (Meurant, 2006) to evaluate the eigenpairs \(_{l}^{},_{l}^{}\) of the symmetric matrix \(_{}\), putting \(_{l}=_{l}^{}\) and \(_{l}=^{-1/2}_{l}^{}\) because the matrices \(_{}\) and \(_{}\) are similar, i.e. \(_{}=^{-1/2}_{}^{1 /2}\). Importantly, Lanczos algorithm only relies on matrix-vector products with \(_{}\). We only compute a few hundred of eigenpairs, asking Lanczos to provide twice or trice as many and disregarding the rest.

When there is a lot of labeled data, we approximate the classical Euclidean (e.g. squared exponential) kernel using random Fourier features approximation (Rahimi and Recht, 2007), this allows linear computational complexity scaling with respect to the number of data points. The number of Fourier features is taken to be equal to \(L\), with the same \(L\) as in Equation (8).

As it was already mentioned, we need to find hyperparameters \(}=(,,^{2},_{}^{2})\) that determine the graph, Gaussian process prior and the noise variance that fit the observations \(\) best. The \(\) parameter we assume manually fixed. To avoid nonsensical parameter values--a common difficulty often occurring when the data is scarce--one might want to assume a prior \(p()\) on \(\). Some specific choices of \(p()\) are discussed in Appendix C. Then \(}\) is a maximum a posteriori (MAP) estimate:

\[}=_{} p(,)+ p().\] (13)

To simplify hyperparameter initialization and align with zero prior mean assumption it makes sense to preprocess \(y_{i}\) to be centered and normalized.

To solve the optimization problem in Equation (13) we use restarted gradient descent. Repeatedly evaluating the gradient of \( p(,)\) is the main computational bottleneck. The key idea for doing this efficiently--viable for integer values of \(\)--is to reduce matrix-vector products with Matern kernels' precision to iterated matrix-vector products with the Laplacian, which is _sparse_. First, we describe this in detail in the noiseless supervised setting, where the idea is most directly applicable.

### Noiseless Supervised Learning

Here we assume that all inputs are labeled, i.e. \(N=n\), and all observations are noiseless, i.e. \(_{}^{2}=0\).

Denoting by \(_{}=_{}^{-1}\) the precision matrix, the log-likelihood \( p(,)\), up to a multiplicative constant and an additive constant irrelevant for optimization, is given by

\[()=-(_{})- ^{}_{}^{-1}=(_{ })-^{}_{}.\] (14)

Its gradient may be given and then subsequently approximated (Gardner et al., 2018) by

\[()}{}= _{}^{-1}_{ }}{}-^{}_{}}{} ^{}_{}^{-1}_{ }}{}-^{}_{}}{},\] (15)

where \(\) is a random vector consisting of IID variables that are either \(1\) or \(-1\) with probability \(1/2\). The first term on the right-hand side is the stochastic estimate of the trace of Hutchinson (1989). Since the kernels from Section 3.3 coincide with graph Matern kernels on the nodes \(x_{i}\), we have

\[_{}=}{C_{,}}_{l=0}^ {L-1}_{,}(_{l})_{l}_{l}^{}, 28.452756pt_{}_{l}=_{l}_{l}, 28.452756pt_{l}^{ }_{m}=_{lm}.\] (16)

However, as the graph bandwidth \(\) is one of the hyperparameters we optimize over, using Equation (16) would entail repeated eigenpair computations and differentiating through this procedure. Because of this, _we use an alternative way_ to compute matrix-vector products \(_{}\) detailed below.6

**Proposition 2**.: _Assuming \(\), the precision matrix \(_{}\) of \(k_{,,^{2}}^{}(_{i},_{j})\) can be given by_

\[_{}=}{C_{,}^{}} }+_{ })(}+_{})}_{}.\] (17)

Proof.: See Appendix A. 

Using Proposition 2 to evaluate matrix-vector products \(_{}\) and conjugate gradients (Meurant, 2006) to solve \(^{}_{}^{-1}\) using only the matrix-vector products, we can efficiently evaluate the right-hand side of Equation (15), with linear costs with respect to \(N\), assuming that the graph is sparse. Preconditioning (Wenger et al., 2022) can be used to further improve the efficiency of the solve.

### Noiseless Semi-Supervised Learning

Here we assume that inputs are partly unlabeled, i.e. \(N n\), while observations are still noiseless, i.e. \(_{}^{2}=0\). Denote \(\) to be the labeled part of \(\). Then the matrix \(_{}\) in the log-likelihood given by Equation (14) should be substituted with \(_{}\). However, while \(_{}\) can be represented using Equation (17), the precision \(_{}=_{}^{-1}\) cannot. We thus compute it as the Schur complement:

\[_{}=-^{-1}, _{}=&\\ &,\] (18)

where partitioning of \(_{}\) corresponds to partitioning \(\) into \(\) and the rest. Evaluating a matrix-vector product \(_{}\) requires a solve of \(^{-1}()\). This solve can also be performed using conjugate gradients, keeping the computational complexity linear in \(N\) but increasing the constants.

### Handling Noisy Observations

Finally, we assume noisy observations, i.e. \(_{}^{2}>0\). The inputs can be partially unlabeled, i.e. \(N n\). In this case, matrix \(_{}\) in the log-likelihood given by Equation (14) should be substituted with \(_{}+_{}^{2}\). To reduce this to the previously considered cases, we use the Taylor expansion

\[(_{}+_{}^{2})^{-1} _{}^{-1}-_{}^{2}_{}^{-2}+_{}^{4}_{}^{-3}-=_{}-_{}^{2}_{}^{2}+ _{}^{4}_{}^{3}-\] (19)

In practice, we only use the first two terms on the right-hand side as an approximation. This allows to retain linear computational complexity scaling with respect to \(N\) but increases the constants.

### Resulting Algorithm

Here we provide a concise summary of the _implicit manifold Gaussian process regression_ algorithm.

**Step 1: KNN-index.** Construct the KNN index on the points \(_{1},..,_{N}\). This allows linear time evaluation of any matrix-vector product with \(}\), and thus also with \(\), \(_{rw}\), \(_{}\) for \(\), etc.

**Step 2: hyperparameter optimization.** Find the hyperparameters \(}\) that solve Equation (13). Assuming \(=\) is manually fixed, this relies only on matrix-vector products with \(_{rw}\).

**Step 3: computing the eigenpairs.** Fixing the graph bandwidth \(\) found on Step 2, compute the eigenpairs \(_{l},f_{l}\) corresponding to the \(L\) smallest eigenvalues \(_{l}\). For large \(N\), use Lanczos algorithm.

After the steps above are finished, Equations (8) and (11) define the geometric kernel \(k_{,,^{2}}^{}(,^{ })\) for arbitrary \(,^{}^{d}\). Then the respective prior \((0,k_{,,^{2}}^{})\) can be conditioned by the labeled data in the standard way, yielding the posterior \(f^{(m)}(m^{(m)},k^{(m)})\). To get sensible predictions far away from the data, the geometric model \(f^{(m)}\) is convexly combined with an independently trained classical Gaussian process model, as given by Equation (12). The resulting predictive model is still a Gaussian process, sum of two appropriately weighted independent Gaussian processes.

**Remark.** The number of neighbors \(K\), the number of eigenpairs \(L\) and the smoothness \(\) are assumed to be manually fixed parameters. Higher values of \(K\) and \(L\) improve the quality of approximation of the manifold kernel, which is often linked to better predictive performance, but requires more computational resources. The parameter \(\) can be picked using cross validation or prior knowledge. Small integer values of \(\) reduce computational costs, but may be inadequate for higher dimensions of the assumed manifold due to the \(^{}=-()/2\) link with the manifold kernel smoothness \(^{}\).

## 5 Experiments

We start in Section 5.1 by examining a simple synthetic example to gain intuition on how noise-sensitive the technique is. Then in Section 5.2 we consider real datasets, showing improvements in higher dimensions. More experiments, results, and additional discussion can be found in Appendix B 

### Synthetic Examples

We consider a one dimensional manifold resembling the shape of a _dumbbell_ which already appeared in Figures 1 and 3. The unknown function \(f_{*}\) is defined by fixing a point \(x^{*}\) in the top left part of the dumbbell, and computing \(((x^{*},))\) where \((,)\) denotes the geodesic (intrinsic) distance between a pair of points on the manifold. This function is illustrated in Figure 3(a).

To measure performance we primarily rely on measuring negative log-likelihood (NLL) on the dense mesh of test locations. We do this because such metric is able to combine accuracy and calibration simultaneously. Additionally, we present the root mean square error (RMSE).

We investigate a semi-supervised setting where the number of unlabeled points is large (\(N-n=1546\)) and the number of labeled points is small (\(n=10\)). We contaminate the inputs with noise, putting \(=_{}+(0,_{}^{2} )\) and do the same with the outputs, putting \(=f_{*}()+(0,_{}^{2})\) for various values of \(_{},_{}>0\). Specifically, we consider \(_{}=_{}=\{0,0.01,0.05\}\) to which we refer to as the noiseless setting, the low noise setting and the high noise setting, respectively.

The results for these are visualized in Figures 3(b) to 3(d) with performance metrics reported in Table 1. The implicit manifold Gaussian process regression is referred to as IMGP (we use \(=1\)) and it is compared with the standard Euclidean Matern-\(}{{2}}\) Gaussian process. IMGP performs much better in the noiseless and the low noise settings. The high noise is enough to damage the calibration of IMGP, as it ties with the baseline model: NLL is slightly worse and RMSE is slightly better.

In Appendix B.1 we show how performance depends on the fraction \(n/N\) of labeled data points, the truncation level \(L\) and we discuss the choice of the \(K\) parameter in KNN. Additionally, in Appendix B.2 we consider noise-sensitivity for a 2D manifold.

### High Dimensional Datasets

For the high-dimensional setting, we considered predicting rotation angles for MNIST-based datasets. Additionally, we examined a high-dimensional dataset from the UCI ML Repository, CT slices.

#### 5.2.1 Setup

**Datasets.** We consider two MNIST-based datasets. The first one is created by extracting a single image per digit from the complete MNIST dataset. By randomly rotating these \(10\) images we obtained \(N=10000\) training samples and \(1000\) testing samples. We call it _Single Rotated MNIST (SR-MNIST)_. For the second dataset, we select \(100\) random samples from MNIST. By randomly rotating these, we generate \(N=100\,000\) training samples, most will be unlabeled, and \(10\,000\) testing samples. We call it _Multiple Rotated MNIST (MR-MNIST)_. The last dataset, _CT slices_, has dimensionality of \(d=385\), we split it to have \(N=24075\) training samples and \(24075\) testing samples. Dataset names can be complemented by the fraction of labeled samples, e.g. MR-MNIST-10% refers to \(n=10\%N\).

    &  &  \\   & \(=0\) & \(=0.01\) & \(=0.05\) & \(=0\) & \(=0.01\) & \(=0.05\) \\  Euclidean Matern-\(}{{2}}\) & \(0.98\) & \(0.99 0.02\) & \(1.02 0.03\) & \(-2.17\) & \(-2.09 0.03\) & \(\) \\ IMGP & \(\) & \(\) & \(\) & \(\) & \(\) & \(-1.91 1.88\) \\   

Table 1: Performance metrics for the dumbbell manifold with varying magnitude of noise \(\).

Figure 4: The ground truth function on the dumbbell manifold and the predictions of the implicit manifold Gaussian process regression (IMGP) under different levels of noise.

**Methods.** We consider implicit manifold Gaussian processes in the supervised regime (_S-IMGP_) and in the semi-supervised regime (_SS-IMGP_). We compare them to the GPTorch implementation of the Euclidean Matern-\(5/2\) Gaussian Process. We refer to it as the _Euclidean Gaussian Process (EGP)_.

**Additional details.** We run 100 iterations of hyperparameter optimization using Adam with a fixed learning rate of \(0.01\). For MNIST, with use IMGP with \(=2\); for CT slices--with \(=3\).

#### 5.2.2 Results

Table 2 shows the negative log-likelihood metric for different datasets and methods on the test set. The respective RMSEs are presented in Appendix B.3. On SR-MNIST, IMGP outperforms EGP in both supervised and semi-supervised scenarios. MR-MNIST is more challenging. In the supervised setting for \(n=1\%N\), S-IMGP is incapable of inferring the underlying manifold structure, performing worse than EGP. However, SS-IMGP, with more data to infer manifold from, performs best. For \(n=10\%N\), IMGP gets a better grip of the dataset's geometry, outperforming EGP in both regimes.

For CT slices, regardless of \(n\), both S-IMGP and SS-IMGP performed poorly. Looking for an explanation, we considered two modifications. First, we fixed the graph bandwidth \(\) found in the algorithm's Step 2 (cf. Section 4.4), and re-optimized the other hyperparameters \(,^{2},_{e}^{2}\) by maximizing the likelihood of the eigenpair-based model (truncated to \(L=2000\) eigenpairs) computed in the algorithm's Step 3. This resulted in limited improvement but did not change the big picture--in fact, values for S-IMGP and SS-IMGP in Table 2 for CT slices already include this modification.

Second, on top of this hyperparameter re-optimization, we tried computing the eigenpairs using torch.linalg.eigh instead of the Lanczos implementation in GP PyTorch, taking the same number \(L=2000\) of eigenpairs. The resulting methods S-IMGP (full) and SS-IMGP (full) showed considerable improvement over the baseline, as shown in Table 2. This indicated an issue with the quality of eigenpairs derived from the Lanczos method which requires further investigation. We discuss this in Appendix B.3, together with the aforementioned hyperparameter re-optimization procedure.

## 6 Conclusion

In this work, we propose the _implicit manifold Gaussian process regression_ technique. It is able to use unlabeled data to improve predictions and uncertainty calibration by learning the implicit manifold upon which the data lies, being inspired by the convergence of graph Matern Gaussian processes to their manifold counterparts. This helps building better probabilistic models in higher dimensional settings where the standard Euclidean Gaussian processes usually struggle. This is supported by our experiments in a synthetic low-dimensional setting and for high-dimensional datasets. Leveraging sparse structure of graph Matern precision matrices and efficient approximate KNN, the technique is able to scale to large datasets of hundreds of thousands points, which is especially important in high dimension, where a large number of unlabeled points is often needed to learn the implicit manifold. The model is fully differentiable, making it possible to infer hyperparameters in the usual way.

Limitations.The quality of the constructed graph significantly influences the technique's performance. When dealing with data from complex manifolds or exhibiting highly non-uniform density, simplistic KNN strategies might fail to capture the manifold structure due to their reliance on a single graph bandwidth. In such scenarios, larger values of parameters \(K\) and \(L\), or in high dimensions, of parameter \(\), may be beneficial but could substantially increase computational costs. Furthermore, larger datasets coupled with high parameter values can lead to numerical stability issues, for instance, in the Lanczos algorithm, calling for further improvements and research. Despite these challenges, our method shows promise for advancing probabilistic modeling in higher dimensions.

    &  &  \\ 
**Method** & **SR - 10\%** & **MR - 1\%** & **MR - 10\%** & **5\%** & **10\%** & **25\%** \\  EGP & \(-0.54 0.01\) & \(-0.20 0.01\) & \(-0.43 0.01\) & \(-0.80 0.02\) & \(-0.96 0.00\) & \(-1.20 0.09\) \\ S-IMGP & \(-1.42 0.01\) & \(2.24 0.20\) & \(-0.68 0.08\) & \(0.47 0.06\) & \(-0.59 0.08\) & \(-0.08 0.01\) \\ SS-IMGP & \(-\) & \(\) & \(\) & \(26.1 12.7\) & \(1.03 0.09\) & \(-0.72 0.68\) \\ S-IMGP (full) & - & - & - & \(0.64 0.83\) & \(0.88 0.29\) & \(-0.42 0.10\) \\ SS-IMGP (full) & - & - & - & \(\) & \(\) & \(\) \\   

Table 2: Negative log likelihood on test samples for real datasets. For RMSE see Tables 4 and 5.