# Gradient Informed Proximal Policy Optimization

Sanghyun Son Laura Yu Zheng Ryan Sullivan Yi-Ling Qiao Ming C. Lin

Department of Computer Science

University of Maryland, College Park

{shh1295,lyzheng,rsulli,yilingq,lin}@umd.edu

###### Abstract

We introduce a novel policy learning method that integrates analytical gradients from differentiable environments with the Proximal Policy Optimization (PPO) algorithm. To incorporate analytical gradients into the PPO framework, we introduce the concept of an \(\)-policy that stands as a locally superior policy. By adaptively modifying the \(\) value, we can effectively manage the influence of analytical policy gradients during learning. To this end, we suggest metrics for assessing the variance and bias of analytical gradients, reducing dependence on these gradients when high variance or bias is detected. Our proposed approach outperforms baseline algorithms in various scenarios, such as function optimization, physics simulations, and traffic control environments. Our code can be found online: https://github.com/SonSang/gippo.

## 1 Introduction

Reinforcement learning (RL) techniques often take gradient-free experiences or trajectories to collect information and learn policies--suitable for many applications such as video games and recommender systems. In contrast, differentiable programming offers analytical gradients that describe the action-result relationship (Equation 2). In particular, recent developments in programming tools (Paszke et al., 2017) have enabled direct access to accurate analytical gradients of the environmental dynamics, as exemplified in physics simulations (de Avila Belbute-Peres et al., 2018; Degrave et al., 2019; Geilinger et al., 2020; Freeman et al., 2021; Qiao et al., 2021), instead of using differentiable models for approximation (Deisenroth and Rasmussen, 2011; Deisenroth et al., 2013). Policy gradients estimated from the integration of these analytical gradients and the reparameterization trick (RP gradient) (Kingma and Welling, 2013) often display less variance than likelihood ratio policy gradients (LR gradient) (Williams and Peng, 1989; Glynn, 1990), derived using the log-derivative trick. Consequently, a subset of research has explored the utilization of these analytical gradients for RL applications, especially in physics domains (Qiao et al., 2021; Mora et al., 2021; Xu et al., 2022).

Nevertheless, in environments with inherent chaotic nature, the RP gradient is found to exhibit high variance (Parmas et al., 2018) and even empirical bias (Suh et al., 2022). Therefore, a strategic approach is necessary for the adoption of analytical gradients in policy learning (Metz et al., 2021). One such strategy involves the interpolation of the LR and RP gradients based on their variance and bias (Parmas et al., 2018; Suh et al., 2022), made possible by the fact that both are different estimators of the same policy gradient. However, this approach requires estimating the LR and RP gradient for each sample trajectory to compute sample variance, a process that can be time-consuming (Appendix 7.7). Additionally, for on-policy RL methods that do not initially estimate the LR gradient, such as Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) or Proximal Policy Optimization (PPO) (Schulman et al., 2017) algorithms, it remains uncertain how to evaluate the variance and bias of analytical gradients. These gradients, however, continue to be desirable when they exhibit low variance and provide insightful information about environmental dynamics.

Given these challenges, our study explores how to incorporate analytical gradients into the PPO framework, taking their variance and bias into account. Our primary contributions are threefold: (1) we propose a novel method to incorporate analytical gradients into the PPO framework _without the necessity of estimating LR gradients_, (2) we introduce an adaptive \(\)-policy that allows us to dynamically adjust the influence of analytical gradients by evaluating the variance and bias of these gradients, and (3) we show that our proposed method, GI-PPO, strikes a balance between analytical gradient-based policy updates and PPO-based updates, yielding superior results compared to baseline algorithms in various environments.

We validate our approach across diverse environments, including classical numerical optimization problems, differentiable physics simulations, and traffic control environments. Among these, traffic environments typify complex real-world scenarios where analytical gradients are biased. We demonstrate that our method surpasses baseline algorithms, even under these challenging conditions.

## 2 Related Work

### Policy Learning Without Analytical Gradients

The policy gradient is described as the gradient of the expected cumulative return in relation to policy parameters (Sutton et al., 1999). For a stochastic policy, as examined in this paper, REINFORCE (Williams, 1992) represents one of the initial methods to employ a statistical estimator for the policy gradient to facilitate policy learning. The policy gradient obtained with this estimator is commonly referred to as a likelihood-ratio (LR) gradient (Glynn, 1990; Fu et al., 2015). Despite the LR gradient's ability to operate without analytical gradients of dynamics and its unbiased nature, it frequently exhibits high variance (Sutton et al., 1998). Furthermore, the nonstationarity of the policy renders stable learning difficult to guarantee (Konda and Tsitsiklis, 1999; Schulman et al., 2015).

Several strategies have been proposed to address the high variance issue inherent to the LR gradient, including the use of additive control variates (Konda and Tsitsiklis, 1999; Greensmith et al., 2004). Among these, the actor-critic method (Barto et al., 1983) is highly favored. However, the introduction of a value function may induce bias. To counteract this, the Generalized Advantage Estimator (GAE) (Schulman et al., 2015) was developed to derive a more dependable advantage function. Concerning the issue of unstable learning, methods such as TRPO (Schulman et al., 2015) and PPO (Schulman et al., 2017) propose optimizing a surrogate loss function that approximates a random policy's expected return. By constraining the policy updates to remain close to the current policy, these methods significantly enhance stability in learning compared to pure policy gradient methods, which makes them more preferred nowadays.

### Policy Learning With Analytical Gradients

In contrast to the LR gradient, the RP gradient is based on the reparameterization trick, which mandates analytical gradients of dynamics yet typically achieves lower variance (Kingma and Welling, 2013; Murphy, 2022). The RP gradient can be directly estimated over the entire time horizon using Backpropagation Through Time (BPTT) (Mozer, 1989) and subsequently used for policy updates. However, BPTT has been found to encounter multiple optimization challenges such as explosion or vanishing of analytical gradients over extended trajectories (Freeman et al., 2021). This is because, surprisingly, the RP gradient, contrary to earlier beliefs, may experience exploding variance due to the chaotic nature of environments (Parmas et al., 2018; Metz et al., 2021). Additionally, Suh et al. (2022) noted that the RP gradient could also be subject to empirical bias.

Several corrective measures exist for the issues concerning the RP gradient (Metz et al., 2021), such as using a truncated time window bootstrapped with a value function (Xu et al., 2022). This approach has demonstrated its efficacy in addressing physics-related problems. Another class of solutions involves interpolating between LR and RP gradients by evaluating their sample variance and bias (Parmas et al., 2018; Suh et al., 2022). Likewise, when we carefully address the problems of RP gradient, it can provide valuable information helpful for policy learning. However, when it comes to PPO, which is one of the most widely used RL methods nowadays, it is unclear how we can leverage the analytical gradients that comprise the RP gradient, as it does not estimate LR gradient which can be used for comparison to the RP gradient. In this paper, we address this issue and show that we can use analytical gradients to enhance the performance of PPO.

Preliminaries

### Goal

We formulate our problems as a Markov decision process (MDP) defined by a tuple \((S,A,P,r,_{0},)\), where \(S\) is a set of states, \(A\) is a set of actions, \(P:S A S\) is a state transition model, \(r:S A\) is a reward model, \(_{0}\) is the probability distribution of the initial states, and \(\) is a discount factor.

Under this MDP model, our goal is to train a parameterized stochastic policy \(_{}:S A^{+}\), where \(\) represents parameters, that maximizes its expected sum of discounted rewards, \((_{})\):

\[(_{})=_{s_{0},a_{0},_{}}[_{ t=0}^{}^{t}r(s_{t},a_{t})].\] (1)

In this paper, we denote the current policy that is used to collect experience as \(_{}\).

### Analytical Gradients

We assume that \(S\) and \(A\) are continuous, differentiable spaces, and \(P\) and \(r\) are also differentiable models. Then, our differentiable environment provides us with following analytical gradients of observation or reward at a later timestep with respect to the action at the previous timestep:

\[}{ a_{t}},}{ a _{t}},\] (2)

where \(k 0\) is an integer. With these basic analytical gradients, we can compute the gradient of certain advantage function, which we denote as \(\). In this paper, we use Generalized Advantage Estimator (GAE) (Schulman et al., 2015b). See Appendix 7.2.1 for details.

### Policy Update

In this section, we discuss how we update policy in two different settings: the policy gradient method based on RP gradient and PPO.

#### 3.3.1 RP Gradient

To compute the RP gradient, we rely on the reparameterization trick (Kingma and Welling, 2013). This trick requires us that we can sample an action \(a\) from \(_{}\) by sampling a random variable \(\) from some other independent distribution \(q\). To that end, we assume that we use a continuous differentiable bijective function \(g_{}(s,):^{n}^{n}\) that maps \(\) to an action \(a\), where \(n\) is the action dimension. Then the following holds because of injectivity,

\[|((s,)}{})| >0,^{n},\] (3)

and we can define the following relationship.

**Definition 3.1**: _We define \(_{} g_{}\) if following holds for an arbitrary open set \(T_{}^{n}\):_

\[_{T_{a}}_{}(s,a)da=_{T_{}}q()d,\]

_where \(T_{a}=g_{}(s,T_{})\)._

**Lemma 3.2**: _If \(_{} g_{}\),_

\[_{}(s,a)=q()|((s, )}{})|^{-1},\]

_where \(a=g_{}(s,)\). The inverse is also true._

**Proof:** See Appendix 7.1.1.

Specifically, we use \(q=(0,I)\) in this paper, and define \(g_{}\) as follows,

\[g_{}(s,)=_{}(s)+_{}(s)(|| _{}(s)||_{2}>0),\]

which satisfies our assumption.

With \(g_{}(s,)\) and the analytical gradients from Equation 2, we obtain RP gradient by directly differentiating the objective function in Equation 1, which we use for gradient ascent. See Appendix 7.2.2 for details.

#### 3.3.2 Ppo

PPO relies on the observation that we can evaluate a policy \(_{}\) with our current policy \(_{}\), using its advantage function \(A_{_{}}\) as

\[(_{})=(_{})+_{s}_{_{}}(s) _{a}_{}(s,a)A_{_{}}(s,a),\]

where \(_{_{}}(s)\) is the discounted visitation frequencies. See (Kakade and Langford, 2002, Schulman et al., 2015a) for the details.

As far as the difference between \(_{}\) and \(_{}\) is sufficiently small, we can approximate \(_{_{}}(s)\) with \(_{_{}}(s)\)(Schulman et al., 2015a), and get following surrogate loss function:

\[L_{_{}}(_{})=(_{})+_{s} _{_{}}(s)_{a}_{}(s,a)A_{_{} }(s,a).\] (4)

Note that we can estimate this loss function with Monte Carlo sampling as described in Appendix 7.2.3. Since this is a local approximation, the difference between \(_{}\) and \(_{}\) must be small enough to get an accurate estimate. TRPO (Schulman et al., 2015a) and PPO (Schulman et al., 2017) constrain the difference to be smaller than a certain threshold, and maximize the right term of (4). In particular, PPO restricts the ratio of probabilities \(_{}(s_{i},a_{i})\) and \(_{}(s_{i},a_{i})\) for every state-action pair \((s_{i},a_{i})\) in the buffer as follows to attain the goal, using a constant \(_{clip}\):

\[1-_{clip}<(s_{i},a_{i})}{_{}(s_{i},a_{i})}<1+_{clip}.\] (5)

## 4 Approach

In this section, we discuss how we can use analytical gradients in the PPO framework while considering their variance and biases. We start our discussion with the definition of \(\)-policy.

### \(\)-Policy

With the analytical gradient of advantage with respect to an action (\(_{a}A_{_{}}(s,a)\)) from our differentiable environments, we can define a new class of policies \(_{}\), parameterized by \(\), as follows.

**Definition 4.1**: _Given current policy \(_{}\), we define its \(\)-policy as follows:_

\[_{}(s,)=\{(s,a)}{ |(I+_{a}^{2}A_{_{}}(s,a))|}&& a=f(a)=a+_{a}A_{_{}}(s,a)\\ 0&&\\ &||<(s,a)|}. .,\] (6)

_Here \(_{1}(s,a)\) represents the minimum eigenvalue of \(_{a}^{2}A_{_{}}(s,a)\)._

**Lemma 4.2**: _Mapping \(f\) is injective, and for an arbitrary open set of action \(T A\), \(_{}(s,)\) selects a set of action \(=f(A)\) with the same probability that \(_{}(s,)\) selects \(A\)._

**Proof:** See Appendix 7.1.2.

In Figure 1, we provide renderings of \(\)-policies for different \(\)s when we use 1-dimensional De Jong's function and Ackley's function  as our target functions. The illustration provides us an intuition that when \(\) is sufficiently small, \(_{}\) can be seen as a policy that selects slightly better action than \(_{}\) with the same probability. In fact, we can prove that \(_{}\) indeed gives us better estimated expected return in Equation 4 when \(>0\) is sufficiently small.

**Proposition 4.3**: _If \(|| 1\),_

\[ L_{_{}}(_{})-(_{ })&=O(||)>0,\\ (_{})-L_{_{}}(_{}) &=O(||)<0,\] (7)

_where \(L_{_{}}\) denotes estimated expected return defined in Equation 4._

**Proof:** See Appendix 7.1.3.

This proposition tells us that \(\)-policy fits into PPO framework seamlessly, and thus if we update our policy towards \(\)-policy, it does not contradict the PPO's objective. However, since there is a second order derivative \(_{a}^{2}A_{_{}}(s,a)\) in the definition, it is unclear how we can update our policy \(_{}\) towards \(_{}\). In the next section, we show how we can do this and how it is related to RP gradient.

### Relationship between RP gradient and \(\)-policy

To show how we can approximate \(\)-policy, for a deterministic function \(g_{}\) such that \(_{} g_{}\), we define a new function \(g_{}\) as follows, and provide a Lemma about it.

\[g_{}(s,)=a+_{a}A_{_{}}(s,a),a=g_{}(s,).\] (8)

**Lemma 4.4**: _When \(a=g_{}(s,)\),_

\[((s,)}{})( }(s,)}{})^{-1}=( I+_{a}^{2}A_{_{}}(s,a)).\] (9)

**Proof:** See Appendix 7.1.4.

Note that we can update \(g_{}\) to approximate \(g_{}\) by minimizing the following loss.

\[L()=_{s_{0},c_{0}, q}[||g_{}(s_{t}, _{t})-g_{}(s_{t},_{t})||^{2}].\] (10)

In fact, it turns out that minimizing Equation 10 leads our policy \(_{}\) to approximate \(_{}\), because of the following Proposition.

**Proposition 4.5**: _If \(_{} g_{}\), for \(\) that satisfies the constraint in Definition 4.1, \(_{} g_{}\)._

Figure 1: \(\)-policies for 1-dimensional (a) De Jong’s function and (b) Ackley’s function. The original policy is rendered in blue, and alpha policies for different \(\) values are rendered in orange and red. Those \(\)-policies are obtained from analytical gradients of the given function \(f(x)\) rendered in black. Note that Ackley’s \(\)-policy becomes invalid when \(=2 10^{-3}\), because of singularities near 0.

**Proof:** See Appendix 7.1.6.

Note that we can use different advantage function formulations for \(A\) in Equation 8, such as GAE [Schulman et al., 2015b] that we use in this work. However, let us consider the following advantage function \(\) to see the relationship between RP gradient and \(\)-policy.

\[_{_{}}(s_{t},a_{t})=_{s_{t},a_{t}, _{}}_{k=t}^{}^{k}r(s_{k},a_{k}),\] (11)

Then, the following Lemma holds.

**Lemma 4.6**: _When \(=1\), if we define \(g_{}\) with \(\), the RP gradient corresponds to \(\) at \(=\)._

**Proof:** See Appendix 7.1.5.

This Lemma tells us that we can understand the RP gradient as the very first gradient we get when we update our policy toward \(_{}\) by minimizing Equation 10. In other words, \(\)-policy is not only a superior policy than the original one in the PPO's point of view (Proposition 4.3), but also a target policy for RP gradient (Lemma 4.6). Therefore, we conclude that \(\)-policy is a bridge that connects these two different policy update regimes.

### Algorithm

Our algorithm is mainly comprised of 3 parts: (1) Update \(_{}\) to \(_{}\) by minimizing Equation 10, (2) Adjust \(\) for next iteration, and (3) Update \(_{}\) by maximizing Equation 4. In this section, we discuss details about Step 2 and 3, and provide the outline of our algorithm.

#### 4.3.1 Step 2: Adjusting \(\)

In Definition 4.1, we can observe that \(\) controls the influence of analytical gradients in policy updates. If \(=0\), it is trivial that \(_{}=_{}\), which means that analytical gradients are ignored. In contrast, as \(\) gets bigger, analytical gradients play bigger roles, and \(_{}\) becomes increasingly different from \(_{}\). Then, our mission is to find a well-balanced \(\) value. Rather than setting it as a fixed hyperparameter, we opt to adjust it adaptively. We first present the following variance and bias criteria to control \(\), as it is commonly used to assess the quality of gradients [Parmas et al., 2018, Suh et al., 2022].

VarianceAfter we approximate \(_{}\) in Step 1), we can estimate \((I+_{a}^{2}A_{_{}}(s,a))\) for every state-action pair in our buffer using Lemma 4.4, and adjust \(\) to bound the estimated values in a certain range, \([1-,1+]\). There are two reasons behind this strategy.

* As shown in Definition 4.1, we have to keep \(\) small enough so that \(_{}\) does not become invalid policy. By keeping \((I+_{a}^{2}A_{_{}}(s,a))\) larger than \(1-\) to keep its positiveness, we guarantee that our policy is not updated towards an invalid policy.
* Note that \((I+_{a}^{2}A_{_{}}(s,a))=_{i=1}^{n}(1+ _{i})\), where \(_{i}\) is the \(i\)-th eigenvalue of \(_{a}^{2}A_{_{}}(s,a)\). By constraining this value near 1 by adjusting \(\), we can guarantee stable policy updates even when some eigenvalues of \(_{a}^{2}A_{_{}}(s,a)\) have huge magnitudes, which is related to a high variance RP gradient. In Appendix 7.3.1, we provide empirical evidence that this estimation is related to the sample variance of analytical gradients.

BiasBy Proposition 4.3, we already know that when we estimate the right term of Equation 4 after Step 1), it should be a positive value. However, if the analytical gradients were biased, this condition could not be met. In runtime, if we detect such cases, we decrease \(\).

In addition, we use one more criterion to adjust \(\), which we call the out-of-range ratio. This criterion was designed to promise the minimum amount of policy update by PPO.

Out-of-range-ratioWe define out-of-range-ratio as follows,

\[=_{i=1}^{N}(|(s_{i},a_{i})}{_{}(s_{i},a_{i})}-1|>_{clip}),\] (12)where \(N\) is the size of the buffer, \(\) is the indicator function, and \(_{clip}\) is the constant from Equation 5. We assess this ratio after the policy update in Step 1), allowing its value to be non-zero when \(>0\). A non-negligible value of this ratio indicates a potential violation of the PPO's restriction outlined in Equation 5, thereby compromising the effectiveness of PPO. Further elaboration can be found in Appendix 7.3.2. Therefore, when this ratio exceeds a predetermined threshold, we subsequently reduce the value of \(\).

To sum it up, our strategy to control \(\) aims at decreasing it when analytical gradients exhibit high variance or bias, or when there is little room for PPO updates. This results in greater dependence on PPO, which can be thought of as a safeguard in our approach. Otherwise, we increase \(\), since we can expect higher expected return with a greater \(\) as shown in Proposition 4.3.

#### 4.3.2 Step 3: PPO Update

Since we do policy updates based on PPO after the update based on \(_{}\), there is a possibility that the PPO update can "revert" the update from Step 1). To avoid such situations, we propose to use the following \(_{h}\) in the place of \(_{}\) in Equation 5:

\[_{h}(s,a)=(_{}(s,a)+_{}(s,a)).\]

By using this virtual policy \(_{h}\), we can restrict the PPO update to be done near both \(_{}\) and \(_{}\). See Appendix 7.3.2 for more details.

#### 4.3.3 Pseudocode

In Algorithm 1, we present pseudocode that illustrates the outline of our algorithm, GI-PPO. There are five hyperparameters in the algorithm, which are \(_{0},,_{det},_{corr},\) and \(()\). We present specific values for these hyperparameters for our experiments in Appendix 7.4.2.

``` \(_{0}\), Initial value \(\) Constant multiplier larger than 1 for \(\) \(_{det},_{corr}\) Constant thresholds \(B\) Experience buffer whileTraining not endeddo  Clear \(B\) whileNot collected enough experiencedo  Collect experience \(\{s_{t},_{t},a_{t},r_{t},s_{t+1}\} B\) end  Estimate advantage \(A\) for every \((s_{i},a_{i})\) in \(B\) using Eq. 14  Estimate advantage gradient \(\) for every \((s_{i},a_{i})\) in \(B\) using Eq. 15  For current \(\), approximate \(\)-policy by minimizing loss in Equation 10  //Variance  For each \(_{i}\) and its corresponding state-action pair \((s_{i},a_{i})\), estimate \((I+_{a}^{2}A_{_{}}(s,a))\) by Lemma 4.4 and get its sample minimum (\(_{min}\)) and maximum (\(_{max}\))  //Bias  Evaluate expected additional return in Equation 16 with our current policy to get \(R_{}\)  //Out-of-range-ratio  Evaluate out-of-range-ratio in Equation 12 with our current policy to get \(R_{corr}\) if\(_{min}<1-_{det}\) or \(_{max}>1+_{det}\) or \(R_{}<0\) or \(R_{corr}>_{corr}\)then \(=/\) end if else \(=\) end if \(=clip(,0,())\)  Do PPO update by maximizing the surrogate loss in Equation 18 end ```

**Algorithm 1** GI-PPO

## 5 Experimental Results

In this section, we present experimental results that show our method's efficacy for various optimization and complex control problems. To validate our approach, we tested various baseline methods on the environments that we use. The methods are as follows:

* LR: Policy gradient method based on LR gradient.
* RP: Policy gradient method based on RP gradient. For physics and traffic problems, we adopted a truncated time window of (Xu et al., 2022) to reduce variance.
* PPO: Proximal Policy Optimization (Schulman et al., 2017).
* LR+RP: Policy gradient method based on interpolation between LR and RP gradient using sample variance (Parmas et al., 2018).
* PE: Policy enhancement scheme of (Qiao et al., 2021), for physics environments only.
* GI-PPO: Our method based on Section 4.3.

Please refer Appendix 7.4 to learn about implementation details of these baseline methods, network architectures, and hyperparameters that we used for all of the experiments.

We have implemented our learning method using PyTorch 1.9 Paszke et al. (2019). As for hardware, all experiments were run with an Intel(r) Xeon(r) W-2255 CPU @ 3.70GHz, one NVIDIA RTX A5000 graphics card, and 16 GB of memory. Experiment results are averaged across 5 random seeds.

### Function Optimization Problems

We start with how different learning methods find the maximum point for a given analytical function. We ran these experiments because RL algorithms can be thought of as function optimizers through function sampling (Williams and Peng, 1989). These problems are \(1\)-step optimization problems, where agents guess an optimal point as an action and get the negative function value as the reward.

As we used for rendering \(\)-policies in Figure 1, we use De Jong's function and Ackley's function for comparison, as they are popular functions for testing numerical optimization algorithms (Molga and Smutnicki, 2005). Please see Appendix 7.5.1 for the details about the function definitions. In this setting, the optimum occurs at the point \(x=0\) for both of the problems. However, their loss landscapes are quite different from one another--De Jong's function has a very smooth landscape that only has one local (global) maximum, while Ackley's function has a very rugged landscape with multiple local maxima. Therefore, De Jong's function represents environments where RP gradients have lower variance, while Ackley's function represents the opposite.

Table 1 shows that **GI-PPO finds far better optimums than the other methods for most of the functions**, except 64-dimensional Dejong's function, where the RP method found slightly better

   Problem & LR & RP & PPO & LR+RP & GI-PPO \\  Dejong (1) & -1.24\( 10^{-6}\) & -1.42\( 10^{-8}\) & -5.21\( 10^{-5}\) & -6.36\( 10^{-8}\) & **-3.84\( 10^{-10}\)** \\ Dejong (64) & -0.0007 & **-9.28\( 10^{-7}\)** & -0.0011 & -3.05\( 10^{-6}\) & -1.04\( 10^{-6}\) \\ Ackley (1) & -1.2772 & -0.4821 & -0.2489 & -1.2255 & **-0.0005** \\ Ackley (64) & -0.6378 & -0.0089 & -0.1376 & -0.0326 & **-0.0036** \\   

Table 1: Average maximum reward (\(\)) for function optimization problems.

Figure 2: Optimization curves for Dejong’s and Ackley’s function of dimension 1 and 64.

optimum than GI-PPO. Optimization curves in Figure 2 also prove that our method converges to the optimum faster than the other methods. Interestingly, the RP method achieved better results than the LR and LR+RP methods for all the functions, which contradicts our assumption that it would not work well for Ackley's function. However, we found out that even though the RP method takes some time to find the optimum than the other methods as shown in Figure 2, when it approaches near-optimum, it converges fast to the optimum based on analytic gradients. Since GI-PPO adaptively changes \(\) to favor RP gradient near optimum, it could achieve better results than the other methods. In Appendix 7.3.3, we provide visual renderings that trace the change of \(\) value during optimization to help understanding, which also shows that higher \(\) is maintained for Dejong's function than Ackley's function, which aligns with our initial assumption. Also note that even though LR+RP also takes advantage of the RP method, and thus achieved a better convergence rate than the two methods when it is far from optimum, it did not work well when it comes to near-optimum.

### Differentiable Physics Simulation

Next, we conducted experiments in differentiable physics simulations. We used Cartpole, Ant, and Hopper environments implemented by (Xu et al., 2022) for comparisons. However, since the current implementation does not support multiple backpropagations through the same computation graph for these environments, we could not test the LP+RP method for them. Instead, we tried to faithfully implement another policy enhancement (PE) strategy of (Qiao et al., 2021) as a comparison, as it showed its efficacy in differentiable physics simulations using analytical gradients.

In Figure 3, we can see that GI-PPO converged to a better policy much faster than the baseline PPO for every environment, which validates our approach. However, the RP method achieved the best results for Ant and Hopper, while GI-PPO converged to the optimal policy slightly faster for Cartpole. To explain this result, we'd like to point out that \(\) is upper bounded by out-of-range-ratio in our approach (Section 4.3.1). Therefore, even if the RP gradient is very effective, we cannot fully utilize it because we have to use PPO to update the policy up to some extent as a safeguard. In the Ant environment, we could observe that it was the major bottleneck in training--in fact, GI-PPO could achieve better results than RP when we increased the ratio from 0.5 to 1.0 (Appendix 7.3.4). However, when the out-of-range ratio equals 1.0, it becomes more likely that we cannot use PPO as a safeguard, which contradicts our design intention. When it comes to the Hopper environment, we found that the variance of proper \(\) values over time was large, so our strategy could not conform to it properly. Overcoming these limitations would be an interesting future work to extend our work.

### Traffic Problems

Here we demonstrate the effectiveness of GI-PPO in an environment where continuous and discrete operations coexist, which leads to highly biased analytical gradients. We suggest a mixed-autonomy traffic environment as a suitable benchmark, which has previously been explored as a benchmark for gradient-free RL algorithms (Wu et al., 2017; Vinitsky et al., 2018). In this paper, we use the pace car problem, where a single autonomous pace car has to control the speed of the other vehicles via interference. The number of lanes, which represent the discontinuities in gradients, and the number of following human vehicles are different for each problem. Please see Appendix 7.5.2 for the details of this environment, and why the analytical gradients are biased.

Figure 3: Learning graphs for 3 problems in differentiable physics simulation: Cartpole, Ant, and Hopper (Xu et al., 2022). In these environments, RP gradients already exhibit lower variance without much bias than LR gradients, because of variance reduction scheme of (Xu et al., 2022).

In Figure 4, we can observe that GI-PPO exhibits a faster convergence rate and converges to better policy overall compared to all the other methods in 2, 4, and 10-lane environments. In a single-lane environment, however, LR+RP showed a better convergence rate than GI-PPO. Note that the RP method still performs well for 2 and 4-lane environments just as other baseline methods, even if there were multiple lanes. We assume this is because the RP method also leverages a state value function, which can provide unbiased analytical gradients in the presence of discontinuities. However, it does not perform well in the 10-lane environment, where discontinuities are excessive compared to the other environments.

This result shows that **GI-PPO can still leverage analytical gradients, even if they were highly biased for policy learning**. Even though LP+RP, which is based on a similar spirit as ours, also exhibited some improvements over LP and RP, _our method showed better results for more difficult problems because baseline PPO offers more stable learning than baseline LR_. Also, the computational cost of GI-PPO was a little more than RP, while that of LR+RP was much more expensive (Appendix 7.7).

## 6 Conclusions

We introduced a novel policy learning method that adopts analytical gradients into the PPO framework. Based on the observation that the RP gradient leads our current policy to its \(\)-policy, we suggested approximating it explicitly, which allows us to indirectly estimate variance and bias of analytical gradients. We suggested several criteria to detect such variance and bias and introduced an algorithm that manipulates \(\) adaptively, which stands for the strength of analytical gradients in policy updates. In our experiments of diverse RL environments, we successfully showed that our method achieves much better results than the baseline PPO by adopting analytical gradients. Even though the interpolated gradient of LR and RP gradient is based on similar motivations, our method performed much better, thanks to the baseline PPO's stable learning.

LimitationsDespite GI-PPO's promising results, there are some remaining issues. First, even if we use a large \(\), we may not fully approximate the corresponding \(\)-policy in a limited number of optimization steps, which may give a false signal for our strategy. We can upper bound the maximum \(\) as we did here, but a more rigorous optimization process may be helpful. This is also related to improving the strategy to control \(\) adaptively, which would be a promising future work.

Also, as discussed in Section 5.2, our method is tightly bound to PPO--that is, even when analytical gradients are much more useful as illustrated in differentiable physics problems, we cannot fully utilize them if they exit the PPO's bound. Even though it is more suitable for stable updates, it could result in a worse convergence rate. If we could adjust PPO's clipping range dynamically, or detect environments where RP gradients are much more reliable, we would be able to overcome this issue.

Finally, computational costs can be further reduced. To utilize analytical gradients, we need backpropagation, which usually requires more time than forward steps. This is the reason why the learning methods based on analytical gradients require longer training time than the others (Appendix 7.7). When the time window gets longer, the cost grows accordingly. However, as we have shown in our traffic experiments, our method works even when gradients are biased. Therefore, it would be an interesting research direction to see if using a very short time window for backpropagation, which produces more biased gradients, would be worthy of exploring for improved computational efficiency.

Figure 4: Learning graphs for traffic pace car problem. Each problem has different number of lanes and following human vehicles, which represent _discontinuities_ and _dimension of the problem_, respectively.

Acknowledgements.This research is supported in part by the ARO DURIP and IARPA HAVSTAC Grants, ARL Cooperative Agreement W911NF2120076, Dr. Barry Mersky and Capital One E-Nnovate Endowed Professorships. Yi-Ling Qiao would like to thank the support of Meta Fellowship.