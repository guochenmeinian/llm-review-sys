# Challenges of Generating Structurally Diverse Graphs

Fedor Velikonivtsev

HSE University, Yandex Research

fvelikon@yandex-team.ru &Mikhail Mironov

Yandex Research

minorov.m.k@gmail.com &Liudmila Prokhorenkova

Yandex Research

ostroumova-la@yandex-team.ru

###### Abstract

For many graph-related problems, it can be essential to have a set of structurally diverse graphs. For instance, such graphs can be used for testing graph algorithms or their neural approximations. However, to the best of our knowledge, the problem of generating structurally diverse graphs has not been explored in the literature. In this paper, we fill this gap. First, we discuss how to define diversity for a set of graphs, why this task is non-trivial, and how one can choose a proper diversity measure. Then, for a given diversity measure, we propose and compare several algorithms optimizing it: we consider approaches based on standard random graph models, local graph optimization, genetic algorithms, and neural generative models. We show that it is possible to significantly improve diversity over basic random graph generators. Additionally, our analysis of generated graphs allows us to better understand the properties of graph distances: depending on which diversity measure is used for optimization, the obtained graphs may possess very different structural properties which gives a better understanding of the graph distance underlying the diversity measure.

## 1 Introduction

Many real-world objects can be naturally represented as graphs: biological and chemical entities (atoms, molecules, proteins, metabolic maps), interaction networks (social and citation networks, financial transactions), road maps, epidemic spreads, and so on. That is why the analysis of graph-structured data is an important and rapidly developing research area.

To generate realistic graph structures, many random graph models have been proposed in the literature (Boccaletti et al., 2006). Such models aim to imitate properties typically observed in natural structures: power-law degree distribution, small diameter, community structure, and others. Each

Figure 1: A sample of generated graphsrandom graph model captures some of these properties; thus, the generated graphs are inevitably similar in certain aspects.

On the other hand, for some applications, it is important to be able to generate a set of graphs that are _structurally diverse_. For instance, if one needs to automatically verify the correctness of a graph algorithm, estimate how well a heuristic algorithm approximates the true solution for a graph problem, or evaluate neural approximations of graph algorithms (Velickovic and Blundell, 2021). In all these cases, algorithms and models should be tested on as diverse graph instances as possible since otherwise the results can be biased towards particular properties of the test set (Georgiev et al., 2023). In other words, we need representative graphs that 'cover' (in some sense) the space of all graphs. Datasets consisting of diverse graphs can also be useful for evaluating graph neural networks and their expressive power. In this direction, Palowitch et al. (2022) propose the GraphWorld benchmark that consists of graphs with various statistical properties. An important part of the benchmark is the relation between graph structure and node labels: graphs with different homophily levels can be constructed. However, generated graph structures are limited to the degree-corrected stochastic block model (Karrer and Newman, 2011) and thus do not cover all complex connection patterns that graphs may have.

To the best of our knowledge, the problem of generating a dataset where graphs are maximally diverse has not been addressed in the literature yet. In this paper, we fill this gap. For this purpose, we first need to define diversity of a set of graphs which is already a challenging task. To measure diversity, we define dissimilarity (distance) for a pair of graphs and then aggregate the pairwise graph distances into the overall diversity measure. In this regard, we show that popular methods of measuring diversity have significant drawbacks and suggest using Energy since it satisfies certain important desirable properties.

After we have defined a performance measure for our problem, several approaches can be used to optimize it. We develop and analyze the following strategies: a greedy method based on diverse random graph generators, a local graph optimization approach, an adaptation of the genetic algorithm to our problem, and a method based on neural generative modeling. For the simplest greedy algorithm, we provide theoretical guarantees on the diversity of the obtained set of graphs relative to the maximal achievable diversity (for a given pre-generated set of graphs to choose from).

We empirically investigate the proposed strategies and show that it is possible to significantly improve the diversity of the generated graphs compared to basic random graph models. In addition to the numerical investigation of diversity measures, we also analyze the distribution of graph structural characteristics and relations between them. Here we also observe significantly improved diversity. Moreover, since we consider diversity measures based on several graph distances, our results shed light on the properties of these graph distances. Indeed, depending on the function we optimize, the structural properties of the generated graphs can vary since graph distances focus on different aspects of graph dissimilarity. Thus, by inspecting the properties of generated graphs, one can better understand what graph characteristics a particular graph distance is sensitive to.

In summary, this work formulates and investigates the problem of generating structurally diverse graphs that can serve as representative instances for various graph-related tasks. Still, many challenges remain to be solved and we hope that our work will encourage further theoretical and practical research in this field.

## 2 Defining diversity for a set of graphs

### Problem setup

As discussed above, for various graph-related problems, it can be essential to have representative graphs that are structurally diverse. Intuitively, such graphs are expected to cover (in some sense) the space of all graphs.1 This section discusses how to define diversity and why it is non-trivial.

Let us start with a motivating example. The most basic random graph generator is the Erdos-Renyi model. In this model, an edge between any two nodes is added with probability \(p\) independently of other edges. If \(n\) is fixed and \(p=0.5\), then every simple graph on \(n\) nodes can be generated equally likely (assuming that the nodes are enumerated), and thus one may think that this model generates representative graphs.

However, it is known that with high probability graphs generated according to the Erdos-Renyi model have typical properties and thus are all very similar to each other with high probability (Erdos et al., 1960). For instance, as illustrated in Figure 2, for the Erdos-Renyi model with \(p=0.5\) (ER-0.5), the average node degree and average clustering coefficient are concentrated near \((n-1)/2\) and 0.5, respectively. Varying \(p\) (ER-mix) allows one to get all possible values of these characteristics, but they are linearly dependent in expectation. Thus, the space of all possible combinations of characteristics cannot be covered by the Erdos-Renyi model.

Intuitively, by _diverse graphs_ we mean those having different structural properties such as degree distribution, pairwise distances, subgraph counts, and so on. However, this intuition is hard to formalize as one may potentially come up with infinitely many properties. On the other hand, defining graph dissimilarity is closely related to _graph distances_. Graph distances have been studied for a long time, and many variants capturing different graph properties exist in the literature (Tantardini et al., 2019). We review some of them in Section 2.2.

Now, assume that we have a multiset of \(N\) graphs \(S=G_{1},,G_{N}\). Throughout the paper, we consider undirected graphs without self-loops and multiple edges. Assume that we are also given a distance measure \(D(G,G^{})\) that evaluates dissimilarity between two graphs. Then, we define diversity as:

\[(S)=F(\{D(G,G^{}):G,G^{} S\})\,,\] (1)

where \(F\) is some function that computes diversity given a set of pairwise distances. The choice of \(F\) is also non-trivial, and we discuss possible approaches in Section 2.3.

After we have defined the distance \(D(,)\) and the measure of diversity, our primary goal is to find a multiset of graphs \(\) of size \(N\) to maximize its diversity:

\[=*{arg\,max}_{G_{1},G_{2},,G_{N}_{n}} (\{G_{1},G_{2},,G_{N}\}),\] (2)

where \(_{n}\) is the set of all graphs with \(n\) nodes.2

### Graph distances

This section discusses how one can define distance between two graphs. As mentioned above, this task is highly non-trivial, and the literature on this topic is abundant (Tantardini et al., 2019; Hartle et al., 2020).

Some graph distance measures are based on the optimal node matching between two graphs: first, the correspondence between nodes is found, and then some distance between two adjacency matrices can be computed (a popular example of this type is _graph edit distance_). However, this approach can only be applied to graphs having the same number of nodes (to achieve this in general case, one may add zero-degree nodes to the smaller graph). Also, finding the optimal matching between nodes is usually computationally expensive.

Another class of distance measures is based on computing some descriptor (vector representation) for a graph and then measuring the distance between two graph representations. Such measures usually violate the positivity axiom of a metric space since the distance between two different graphs can be

Figure 2: Average node degree and average clustering coefficient in the Erdős-Renyi model with \(n=16\) and \(p=0.5\) (ER-0.5) or varying \(p\) (ER-mix)

equal to zero. Indeed, if we guarantee that \(D(G,G^{})=0\) if and only if \(G\) and \(G^{}\) are isomorphic, then computing the distance is at least as hard as graph isomorphism testing, which is infeasible for most applications. Thus, when computing a graph representation, we inevitably lose some information about the graph. Various approaches for creating graph descriptors exist in the literature. Some of them use the spectrum of the graph Laplacian (or its normalized variant) that is known to encode some important structural information (Ipsen and Mikhailov, 2002; Wilson et al., 2005; Tsitsulin et al., 2018). Other approaches are based on local statistics, such as graphlets (Yaveroglu et al., 2014). Each graph distance captures some properties of a graph and can be insensitive to others. We refer to comparative studies of graph distances (Tantardini et al., 2019; Hartle et al., 2020) for a more comprehensive list of known measures and the analysis of their properties. In more recent work, Thompson et al. (2022) suggested graph representations based on an untrained random GNN. Such representations can also be used for computing graph distances.

Our paper does not aim to answer which graph distance is better. Each distance captures particular graph properties and the resulting set of generated diverse graphs may significantly depend on this choice. In our experiments, we consider several representative options. As a result, our analysis of generated graphs gives some additional insights into the properties of graph distances.

### Measuring diversity for a set of elements

In this section, we discuss the problem of measuring diversity for a set of elements given their pairwise distances (or similarity values). This problem was addressed in several recent papers (Xie et al., 2023; Friedman and Dieng, 2023) discussed in more detail in Appendix A.1. However, as we show, none of the proposed approaches are fully suitable for our task.

Probably the most natural and widely-used measure for quantifying diversity is the _average pairwise distance_ between the elements. However, we note that this measure is not suitable in our case since optimizing it may lead to degenerate configurations. For instance, consider a toy experiment with dots distributed on a line segment. As shown in Figure 2(a), optimizing the average pairwise distance forces many points to collapse into one (at the endpoints of the line segment), which is clearly not a desirable behavior. This happens since Average does not take into account whether the elements of a dataset are unique or well isolated from each other.

Another possible measure that does take uniqueness of elements into account is the _minimum pairwise distance_ (often referred to as _Bottleneck_ in the literature). However, this measure is not sensitive to all the distances but the minimal one and thus cannot distinguish vastly different configurations.

Motivated by these examples, we formulate two properties that a good diversity measure is expected to satisfy. We assume that we are given a multiset \(S\) of \(N\) elements and for each pair of elements \(G,G^{} S\) we know the distance \(D(G,G^{})\) between them. Note that we are interested in maximizing diversity for a _fixed_ number of elements \(N\), which simplifies the requirements since we do not have to deal with how diversity changes when the number of elements increases.

MonotonicitySuppose we are given two multisets \(S,S^{}\) both consisting of \(N\) different elements and a bijection \(g:S S^{}\) between them. Assume that for any \(G_{i},G_{j} S\) we have

\[D(G_{i},G_{j}) D(g(G_{i}),g(G_{j}))\] (3)

with strict inequality for at least one pair \(i,j\). Then we require \((S)<(S^{})\).

This property describes the essence of diversity measures: larger pairwise distances should lead to higher diversity values. Thus, a good measure of diversity should be monotone.

UniquenessSuppose \(S\) consists of \(N\) different elements \(G_{1},,G_{N}\). Denote by \(S_{ij}\) the multiset obtained from \(S\) by removing \(G_{i}\) and adding the second copy of \(G_{j}\) for \(j i\), that is \(S_{ij}:=(S\{G_{i}\})\{G_{j}\}\) Then, we require \((S)>(S_{ij})\).

In other words, this property requires that given \(N-1\) elements, adding a unique element results in a higher diversity than duplicating an already existing element. This property is very intuitive since to increase the coverage it is clearly better to add a new element than to duplicate an existing one.

Note that the average pairwise distance does not have the uniqueness property, thus optimizing it may lead to degenerate solutions. Clearly, the minimum pairwise distance does not have monotonicity.

Moreover, it turns out that none of the measures from (Xie et al., 2023; Friedman and Dieng, 2023) has both these properties, see Appendix A.2 for the details.

Thus, we propose an alternative diversity measure motivated by the _energy of a system of equally charged particles_. Namely, given a constant \(>0\), we define the Energy of a set of graphs \(S\) as

\[-_{i j},G_{j})^{}}.\] (4)

The parameter \(\) affects how strongly we penalize small pairwise distances. In an extreme case of \(\), this measure becomes equivalent to Bottleneck. All our theoretical results hold for any \(>0\).3 For our experiments, we use \(=1\), so (4) can be naturally interpreted as the average pairwise energy for a system of equally charged particles (we multiply by -1 to get a measure that is larger for more diverse sets of graphs).

Our toy example in Figure 2(b) shows that when being optimized Energy leads to a diverse configuration, in contrast to Average. Regarding our formal properties, monotonicity is obviously satisfied by Energy (4). To show uniqueness, we note that any multiset with pairwise different elements has some finite negative diversity and any multiset with two copies of one element has diversity \(-\).

**Proposition 2.1**.: _Energy (4) satisfies both monotonicity and uniqueness._

Despite having these desirable properties, Energy still has a shortcoming: it can be unboundedly large when two elements become too close to each other. However, there are currently no better alternatives, as shown in a recent paper by Mironov and Prokhorenkova (2024) that extends the analysis of diversity measures in terms of the desirable properties they satisfy. We refer to Appendix A.4 for a discussion. In our experiments, we use Energy (combined with several graph distances) as our primary measure of diversity and also consider Average as an additional measure.

## 3 Algorithms for diversity optimization

In the previous section, we discussed how to measure diversity and why this task is non-trivial. In this section, we propose several approaches for diversity optimization. Our goal is to investigate diverse algorithms: from a basic approach based on random graph generators to a more advanced one based on neural generative modeling.

Our algorithms can be applied to arbitrary diversity measures. However, for scalability purposes, we restrict ourselves to measures that can be written in the following way. Suppose we are given a set of size \(N\) and any element \(G\) from this set. Denote the subset of all elements excluding \(G\) by \(S=\{G_{1},G_{2},,G_{N-1}\}\). Then, the diversity of the original set can be written as:

\[(\{G\} S)=g(f(G,S),c(S)),\] (5)

where \(g\) is a function that is monotone w.r.t. both arguments, \(f(G,S)\) depends only on the distances \(\{D(G,G_{i}):G_{i} S\}\), and \(c(S)\) is a value that depends only on \(S\) (and does not depend on \(G\)). We call such function \(f(G,S)\) a _fitness_ of a graph \(G\) w.r.t. a set of graphs \(S\). The measures considered in this study satisfy (5). For instance, for Energy, \(g\) can be the sum, \(f(G,S)=_{G_{i} S})^{}}\), and \(c(S)=_{i<j},G_{j})^{}}\). For Bottleneck, \(g\) can be the minimum, \(f(G,S)=_{G_{i} S}D(G,G_{i})\), and \(c(S)=_{i<j}D(G_{i},G_{j})\). Note that computing the fitness \(f(G,S)\) requires \(N-1\) distance computations.

Figure 3: Optimized Average or Energy on a line segment

It is important to note that standard machine learning approaches cannot be directly applied to our task: usually, generative algorithms require a training set that they try to imitate. In our case, there is no training set since the aim is to generate graphs that are maximally dissimilar to each other.

### Greedy algorithm

The main idea of this algorithm is to build a set of diverse graphs iteratively by adding at each step the most suitable graph from a predefined set \(\) of a much larger size. This set can be either user input, the result of another algorithm, or a set of graphs generated by random graph models. The process initiates by randomly choosing a graph from \(\). At each step, the most suitable graph from \(\) is chosen according to the fitness \(f(G,S)\), where \(S\) is the currently selected set of graphs.

A detailed description of the algorithm is given in Appendix C.1. We also provide the analysis of computational complexity and a lower bound on the diversity of graphs returned by the greedy algorithm relative to the diversity of the initial set \(\) (see Theorem C.1).

### Genetic algorithm

The genetic algorithm enhances the diversity of a graph population through evolutionary operations. Starting with an initial set of \(N\) graphs, it iteratively refines this set by selecting pairs of graphs as parents and generating a child through crossover and mutation processes. This child can replace the less-fit graph in the population if it increases the overall diversity; otherwise, the algorithm tries to find a more suitable offspring by repeating the process. To prevent itself from getting stuck in local optima, the algorithm can accept a candidate that decreases the overall diversity if the number of unsuccessful attempts exceeds a certain threshold. The algorithm iterates for a predefined number of iterations, ultimately evolving the population towards greater diversity. This approach adapts principles from genetics to solve optimization problems, as we try to preserve beneficial graph characteristics while at the same time introducing novel configurations to achieve a diverse set of graphs.

The details of this algorithm are given in Section C.3, where we also analyze the complexity of the algorithm.

### Local optimization algorithm

The main idea of the local optimization algorithm is the refinement of the diversity of a graph population by iteratively modifying individual graphs. Starting from an initial set, we randomly sample graphs and make small modifications to their structure (single edge addition/deletion). Then, if the overall diversity improves, we accept the change. As in the other algorithms, we can accept less fit modifications after consecutive failed attempts to prevent stagnation at a local optimum.

The details of this algorithm are given in Section C.4, where we also analyze its computational complexity. Since local optimization makes small modifications at each step, this approach is expected to be most efficient when the input set of graphs is already sufficiently diverse. Thus, when we combine several algorithms, local optimization is always the last step.

### Iterative graph generative modeling

Neural generative models are known to be a powerful tool for generating graphs that imitate a given distribution (You et al., 2018; Martinkus et al., 2022; Vignac et al., 2023). Hence, we aimed to investigate whether such approaches can be used for generating graphs that are structurally diverse. In this case, there is no predefined distribution that needs to be captured. We address this via the following iterative procedure. The process starts from an initial graph set \(S_{0}\) and then iteratively enhances the diversity. At each iteration, the current set of graphs \(S_{i}\) is used to train a generative model. Then, this model is used to generate a significantly larger set of new graphs. From this new set, a smaller subset of diverse graphs \(S_{i+1}\) is selected via the greedy approach. We expect that \(S_{i+1}\) is more diverse than \(S_{i}\). So, we repeat the process by training a neural generative model on the new set \(S_{i+1}\). For the neural network architecture, we use Discrete Denoising Diffusion Model (DiGress) (Vignac et al., 2023). We refer to Appendix C.5 for a detailed description of our approach.

Experiments

In this section, we analyze and compare the algorithms for generating diverse graphs described above. Then, we analyze generated graphs and discuss how the choice of a particular graph distance affects the structures of the obtained graphs.

SetupIn our experiments, we consider four representative distance measures: heat and wave NetLSD (Tsitsulin et al., 2018), Graphlet Correlation Distance (Yaveroglu et al., 2014), and Portrait Divergence (Bagrow and Bollt, 2019). We select these distances to be diverse: NetLSD is based on the Laplacian eigenvalues (we use NetLSD-heat and NetLSD-wave variants), Graphlet Correlation Distance (GCD) uses local structures, while Portrait Divergence (Portrait-div) takes into account both local and global properties. A detailed description of these measures is given in Appendix B.

Following Section 2.3, we choose Energy as the diversity measure. Formally, we optimize and report the following measure:

\[_{i j},G_{j})+},\]

where \(\) is a small constant added for numerical stability. As soon as we fix the diversity measure that we rely on, the goal of each algorithm is to optimize this measure. In other words, in contrast to standard machine learning problems, we do not face the problem of overfitting.

We evaluate the following approaches described in Section 3: Greedy, Genetic, local optimization (LocalOpt), and iterative graph generative modeling (IGGM). Our evaluation also includes the comparisons against simple baseline models, specifically the Erdos-Renyi graphs sampled with various \(p\) (ER-mix) and a sample from diverse random graph generators described in Section C.2. As an additional illustration, we also include a sample of graphs generated by the GraphWorld benchmark (Palowitch et al., 2022), where we vary the model parameters to increase diversity of the obtained graphs. In most of the experiments, we generate \(N=100\) graphs with \(n=16\) nodes. We also conduct experiments with non-neural algorithms on the set of 100 graphs with size \(n=64\).

Let us note that the algorithms introduced in Section 3 can be easily combined: the output of one algorithm can serve as an input to another. Thus, we evaluate the combinations of the algorithms. We use the notation '\(\)' to denote the transition between the consecutive algorithms. Note that Greedy is the only strategy that does not generate any new graphs. Hence, its initial set should be already sufficiently diverse. Thus, we use graphs generated by diverse random graph models described in Section C.2.

We assume that for most algorithms, the most time-consuming operation is computing a graph representation (that is used for distance computations). Therefore, all algorithms except IGGM use the total limit of 3M generated graphs. For IGGM, the number of generated graphs is limited to 1M since training the graph generative model is time-consuming. In the tables, we use the square brackets to denote the number of computed graph representations for an algorithm or sub-algorithm.

Numerical comparisonIn this section, we numerically analyze how well different approaches optimize the chosen diversity measure. Table 1 shows the results for selected algorithms and baselines. For more algorithms, please refer to Table 4 in Appendix, where we also report the standard deviation.

First, we note that all the proposed algorithms significantly improve the performance of the basic algorithms ER-mix and Random Graph Generators. Similarly, the diversity of GraphWorld is far from optimal. This is not surprising since GraphWorld does not directly optimize the diversity of graph structures and relies on the relatively simple stochastic block model.

Among the non-neural algorithms, the best performance is achieved by a combination of Greedy, Genetic, and LocalOpt (applied in this order). Such a combination is natural: Greedy starting from a set generated by different random graph generators is the simplest way to get an initial diverse set of graphs. Then, Genetic uses enough randomness to create all kinds of graph patterns to choose from. After that, LocalOpt is used to make final tuning with small graph modifications. In turn, the neural-network-based method IGGM gives a significant boost in diversity for GCD and Portrait-div distances and exhibits comparative results for NetLSD-heat. Note that it uses less budget for generated graphs but also requires training a graph generative neural model several times.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

In this work, we have only made a first step to analyzing the problem of generating diverse graphs. There are plenty of promising directions for future research, and we hope that our work will encourage researchers to dive deeper into this problem. One particularly important challenge is scalability. If the number of nodes \(n\) becomes large, then the number of possible graphs grows very fast, and for some methods (e.g., LocalOpt that uses single edge modifications) covering the whole space may become infeasible. Secondly, we believe that more advanced algorithms will be developed in the future. Also, further discussions on how to measure diversity and how to choose a proper graph distance seem to be very useful. Finally, it would be great to see practical applications of diverse graphs.