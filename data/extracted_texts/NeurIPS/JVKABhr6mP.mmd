# Meteor: Mamba-based Traversal of Rationale

for Large Language and Vision Models

 Byung-Kwan Lee

KAIST

leebk@kaist.ac.kr &Chae Won Kim

KAIST

chaewonkim@kaist.ac.kr &Beomchan Park

KAIST

bpark0810@kaist.ac.kr &Yong Man Ro

KAIST

ymro@kaist.ac.kr

###### Abstract

The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (_e.g.,_ charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, we present a new efficient LLVM, **M**amba-based **traversal **of** rationales (\(}}}\)**Meteor**), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity. We introduce a new concept of _traversal of rationale_ that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, \(}}}\) Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models. Code is available in https://github.com/ByungKwanLee/Meteor.

## 1 Introduction

Following the successful zero-shot achievements of instruction-tuned large language models (LLMs) , visual instruction tuning  has spurred the rapid development of large language and vision models (LLVMs). The emergence of closed-source LLVMs, such as GPT-4V , Gemini-Pro , and Qwen-VL-Plus , has prompted several studies to create high-quality question-answer visual instruction tuning datasets  and to scale up the model sizes of open-source LLVMs , aiming to compete with their closed-source counterparts by leveraging the scaling law .

Recent research trends focus on enhancing image resolution  and dividing images into smaller sections  to improve image perception capabilities. Additionally, some studies have utilized additional vision encoders  such as EVA-CLIP , DINOv2 , SAM , and SigLIP . Various computer vision models  have also been employedfor tasks such as segmentation, detection, scene graph generation, and optical character recognition (OCR) to enhance LLVMs' answering capabilities with the help of external perception information.

These efforts, along with the curation of high-quality visual instruction datasets, have significantly reduced the performance gap between open- and closed-source LLVMs across numerous evaluation benchmarks and have even led to superior performances on some benchmarks. These successful developments are credited to the multifaceted information necessary for a wide range of capabilities. This encompasses fundamental image understanding, real-world knowledge of common-sense and non-object concepts (_e.g.,_ charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions.

Inspired by the key importance of the multifaceted information, we explore the possibility of designing efficient LLVMs that implicitly embed it as a form of multifaceted rationale (See Appendix A for more details), without significantly increasing model size and without using additional explicit vision encoders and computer vision models during the inference phase. Hence, we present a new efficient LLVM, Mambo-based traversal of rationale (\(}}\)**Meteor**), comprising two core components: the Mambo architecture  and a multimodal language model (MLM) based on a pretrained large language model (LLM). The multifaceted rationale has rich information for achieving diverse capabilities, so its length is inherently long. This is why we employ the Mambo architecture, hereinafter referred to as Meteor-Mambo, which takes advantage of embedding lengthy input. It serves as an embedding module for the rationale, enabling Meteor-MLM, the MLM component, to address questions with the help of these embedded rationales. When conveying the knowledge of embedded rationales from Meteor-Mambo to Meteor-MLM, we introduce a new concept of _traversal of rationale_, which spurs embedding of long sequential rationales.

To ensure that \(}}\) Meteor encompasses diverse capabilities for vision-language tasks (e.g., image understanding, common-sense knowledge, charts, diagrams, documents, signs, symbols, and math problems), we gather 2.1M question-answer pairs from existing visual instruction tuning datasets: ShareGPT4V-Caption/Instruct , MiniGemini-Instruct , Doc-Downstream/Reason , GLLaVA-Align/Instruct , and Math-Vision/Instruct/Plus . Subsequently, we utilize the light and fast Claude Haiku API  to generate detailed and comprehensive rationales tailored for the collected 2.1M question-answer pairs. These rationales are carefully filtered by human reviewers with the aid of GPT-4V, resulting in 1.1M question-rationale-answer triples (Appendix A).

Using the question-rationale pairs in the curated 1.1M triples, the first training step involves training Meteor-Mambo and miscellaneous projectors, _i.e.,_ a vision projector and tor projector. During

Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with \(}}\) Meteor on MME , MMB , MathVista , and AI2D  requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.

[MISSING_PAGE_EMPTY:3]

[MISSING_PAGE_FAIL:4]

broad coverage of math knowledge, we also include 177K GLLaVA-Align/Instruct , 3K Math-Vision , and 566K text-only samples from Math-Instruct/Plus . In summary, we collect 755K real-world images, 627K images for documents, charts, diagrams, signs, and symbols, and 747K math samples (180.5K with images and 566.8K text-only). Overall, the question-answer visual instruction tuning samples sum up to 2.1M.

Curating Rational.Using the gathered 2.1M question-answer pairs, we utilize the light and fast Claude Haiku API  to generate detailed and comprehensive rationales. We use the prompt template: _"Question: \(\{\}\). _Answer: \(\{\}\). Based on the question and answer, carefully provide an explanation about how to answer the question in detail."_ Here, \(\{\}\) represents the placeholder for the corresponding language description. Afterward, we assess the rationale score from GPT-4V  using the following template: _"Question: \(\{\}\). Rationale: \(\{\}\). Answer: \(\{\}\). Based on the question, rationale, and answer, provide a score from 0 to 10, evaluating how well the rationale is described to solve the question. If the given rationale is insufficient, you should rigorously give a score below 5."_ Subsequently, we filter out the generated rationales with a score below 5. The rationales that pass this automated evaluation are then subjected to human review to determine _Yes or No_ on whether they provide a proper description to address the question. Finally, this series of processes yields 1.1M question-rationale-answer triples, which include 338K real-world images covering common-sense knowledge and a few samples for diverse capabilities, 379K images for documents, charts, diagrams, signs, and symbols, and 342K math samples (165K with images and 177K text-only).

Mamba Architecture.To make LLVMs inherently possess rationale when addressing complex questions, we generate comprehensive rationales based on question-answer pairs. Subsequently, we employ the Mamba architecture , leveraging its capability to handle lengthy rationales while maintaining computational efficiency. This approach allows us to effectively incorporate the rationale in an environment where the curated 1.1M question-rationale pairs have an average length of 213 tokens, which is approximately ten times longer than the average length of 22 tokens of ground truth answers in typical visual instruction tuning datasets .

Traversal of Rationale.However, it is crucial to note that we cannot acquire and utilize rationales during the inference phase without API-based models, since only user questions are given. Therefore, we propose a new concept called _traversal of rationale_ to effectively provide the rationale to Meteor-MLM without any help from external APIs in the inference phase. Inspired by retrieval-based knowledge , we introduce a special token, <tor> (stands for traversal **of** rationale), and evenly distribute 10 fixed number of <tor> tokens, as described in Figure 3. The rationale planted with <tor> is propagated into Meteor-Mamba along with image and question tokens, and then the output

Figure 3: Overview of \(\) Meteor architecture and its training steps. Note that, ‘Meteor-Multimodal Language Model (MLM)’ indicates that as training progresses, the pretrained language model evolves into a multimodal language model.

features in Meteor-Mamba are directly propagated into Meteor-MLM. Here, we autoregressively train Meteor to generate the part of the rationale between <tor>, whenever Meteor sees the special token <tor>. This procedure ensures that each <tor> represents the following rationale part until the next <tor> is encountered. Using a single <tor> token to encompass the rationale may not work well when embedding lengthy rationales, and if we do not consider distributing <tor> tokens in the rationale, then the later token does not refer to the earlier ones well due to the common problem of the autoregressive mechanism's forgetting nature . This is why we place multiple <tor> tokens in the rationale instead of one.

Training Strategy.In the first training step, we leverage the question-rationale pairs in the curated 1.1M triples to train Meteor-Mamba and miscellaneous projectors. Throughout this step, the long sequential rationale is embedded into Meteor-Mamba through traversal of rationale by autoregressively generating rationale parts between the special tokens <tor>. By freezing Meteor-MLM, Meteor-Mamba seamlessly incorporates the rationale. In the second training step, we utilize the question-answer pairs in the curated 1.1M triples to jointly train Meteor-Mamba, Meteor-MLM, and the miscellaneous projectors. Here, multiple <tor> special tokens are only propagated to Meteor-Mamba. Then, the rationale-embedded features in Meteor-Mamba corresponding to the special tokens <tor> are only fed into Meteor-MLM, enabling it to adeptly answer complex questions, even in the absence of explicit rationale descriptions. In essence, these steps equip << Meteor with the capability to effectively address complex questions with the aid of the rationale.

## 4 Experiment

Implementation Detail.To ensure successful reproducibility, we outline three crucial technical details of < Meteor: the structure of (a) Meteor-Mamba and Meteor-MLM, (b) vision encoder and miscellaneous projectors, and (c) training and inference details.

(**a**) To build Meteor-Mamba, we use the Mamba architecture with 24 layers and a 768 hidden dimension, resulting in a total of 130M parameters, which is relatively trivial compared to the approximately 7B parameters of the pretrained InternLM2-7B [68; 69]. It is executed under the

   LLVMs & Q-Bench & SQA1 & AI2D & ChartQA & SEED1 & POPE & HallB & MME & MathVista & MMB & MMB\({}^{}\) & MM-Vet & LLaVA\({}^{}\) \\  BLIP2-13B  & - & 61.0 & - & - & 46.4 & 85.3 & - & 1584 & - & - & -2.4 & - \\ Instrubkit-TB  & 56.7 & 60.5 & - & - & 53.4 & - & 53.6 & - & 25.3 & 36.0 & 23.9 & 26.2 & - \\ Instrubkit-TB  & - & 63.1 & - & - & - & 78.9 & - & - & - & 33.9 & - & 25.6 & - \\ IDEFCS-9B  & 51.5 & - & - & - & 74.6 & - & 1353 & 19.8 & 48.2 & 25.2 & 23.7 & - \\ Queen-VL-7B  & 59.4 & 67.1 & - & - & - & - & - & - & 38.2 & 7.4 & - & - \\ Queen-VL-Chat-7B  & 33.8 & 68.2 & - & - & 58.2 & - & 56.4 & 1849 & - & 60.6 & 56.7 & 47.3 & - \\ MinGPT-4-7B  & 51.8 & - & - & - & - & - & - & 23.1 & 23.0 & 11.9 & 22.1 & - \\ Outer-7B  & 47.2 & - & - & - & - & 72.5 & - & 1599 & 17.7 & 48.3 & - & 24.7 & - \\ LLaVA-7B  & - & 38.5 & - & - & - & 80.2 & 44.1 & 1055 & - & 34.1 & 14.1 & 26.7 & - \\ ILaVA1-7B  & 60.1 & 66.8 & - & 58.6 & 58.9 & 1805 & - & 64.3 & 58.3 & 30.5 & 63.4 & - \\ LLaVA1-513B  & 61.4 & 71.6 & 54.8 & 18.2 & 61.6 & 85.9 & 46.7 & 1826 & 27.6 & 67.7 & 63.6 & 35.4 & - \\ mPLUG-OV-7B  & 58.9 & - & - & - & - & - & - & - & 22.2 & 46.6 & - & - & - \\ mPLUG-OV-7B  & 62.9 & 68.7 & - & - & - & - & - & - & 64.5 & 60.3 & 36.2 & - \\ ShareGPT4-7B  & 63.4 & 68.4 & - & 69.7 & - & 49.8 & 1944 & 25.8 & 68.8 & 62.2 & 37.6 & - \\ InernLM-X-CT-7B  & 64.4 & - & - & 66.1 & - & 57.0 & 1919 & 29.5 & 74.4 & 72.4 & 35.2 & - \\ Monkey-10B  & - & 69.4 & - & - & 68.9 & 85.4 & 1924 & 34.8 & 72.4 & 67.5 & 33.0 & - \\ VLA-7B  & - & 68.2 & - & 61.1 & 85.5 & - & - & - & 68.9 & 61.7 & 34.9 & - \\ VLA-13B  & - & 73.7 & - & 62.8 & 84.2 & - & - & - & 70.3 & 64.3 & 38.8 & - \\ SPHINX-TB  & - & 70.6 & - & - & 71.6 & 86.9 & - & 1797 & 27.8 & 65.9 & 57.9 & 40.2 & - \\ SPHINX-Moc-7B-8  & 66.2 & 70.6 & - & - & 73.0 & **89.6** & - & 1852 & 42.7 & 71.3 & - & 40.9 & - \\ SPHINX-Moc-7B-13B  & 66.2 & 70.6 & - & - & 74.8 & 89.1 & 52.1 & 1741 & 36.8 & 71.0 & - & 47.9 & - \\ SPHINX-Moc-7B-12B & - & 70.1 & - & 70.2 & 86.5 & - & 1851 & 34.6 & 69.6 & 63.3 & 43.9 & 72.3 \\ LLaVA-NeX-TB  & - & 71.6 & 69.5 & - & - & - & 1972 & 37.5 & 72.1 & - & - & 80.1 \\ LLaVA-NeXT-13B  & - & 73.6 & 70.0 & 62.2 & 72.2 & 86.7 & - & 1892 & 35.1 & 70.0 & 68.5 & 47.3 & 72.3 \\ MM-17B  & - & 72.6 & - & - & 69.9 & 86.6 & - & 1855 & 35.9 & 72.3 & - & 42.1 & - \\ MM-10B-7B-32  & - & 74.4 & - & 70.9 & 87.8 & 1992 & 40.9 & 72.7 & - & 45.2 & - \\ MinGemini-HD-7B  & - & - & - & - & - & - & - & 1865 & 32.2 & 65.8 & - & 41.3 & - \\ MinGemini-HD-7B  & - & - & - & - & - & - & - & 1917 & 37.0 & 68.6 & - & 50.5 & - \\  Meteor-7B & **69.2** & **88.3** & **77.9** & **74.9** & **75.0** & 88.7 & **60.4** & **2229** & **53.4** & **82.9** & **82.1** & **57.3** & **87.1** \\   

Table 1: Comparison with the current existing open-source LLVMs, evaluating vision language performances of < Meteor on numerous evaluation benchmarks requiring diverse capabilities: Q-Bench , SQA1 , AI2D , ChartQA , SEED1 , POPE , HallB , MME , MathVista , MMB , MMB\({}^{}\), MM-Vet , and LLaVAW . Note that, AI2D and ChartQA performances for LLaVA family models are evaluated under zero-shot conditions, while < Meteor uses training dataset for them.

efficient computation of hardware-aware state expansion , where we borrow the tokenizer  from the backbone MLM to fit the language expression space in the backbone MLM. Meteor-MLM is based on InternLM2-7B  with 32 layers and a 4096 hidden dimension.

**(b)** We use a vision encoder with 428M CLIP-L/14 , which has 24 layers and a 1024 hidden dimension. The resolution of the positional embedding is interpolated from \(24 24\) to \(35 35\) to accommodate a \(490 490\) image resolution. The vision projector involves an MLP that adapts the hidden dimension from 1024 to 4096 to fit that of the backbone MLM. Similarly, we build the tor projector to convey embedded rationales from Meteor-Mamba into Meteor-MLM, employing the same structure as the vision projector but transferring the hidden dimension from 768 to 4096.

**(c)** We train and evaluate \(\) Meteor in the following computing environment: Intel(R) Xeon(R) Gold 6230, 256 GB RAM, and 8\(\)NVIDIA RTX A6000 48GB VRAM. To efficiently train it, we use one epoch of training for each training step under 4-bit quantization and bfloat16 data type  for Meteor-MLM, where double quantization and normalized float 4-bit (nf4)  are used. Meteor-Mamba uses float32 data type because training it with bfloat16 or float16 has been reported to produce an unstable learning process. In addition, QLoRA  is used to train Meteor-MLM, with 64 rank and 64 alpha parameters. We use the AdamW  optimizer and schedule the learning rate by cosine annealing  from 1e-4 to 1e-6 in each training step, with gradient checkpointing  applied to Meteor-MLM for efficient memory management. With a gradient accumulation of 6, we set batch sizes of 192 and 576 for each training step, and each step takes approximately three days. For efficient inference, \(\) Meteor is validated in 4-bit quantization, and we use deterministic beam search (\(n=3\)) for text generation. Note that we implement not only Meteor-MLM but also numerous baselines under the efficient propagation from FlashAttention2.

Evaluation.We have evaluated \(\) Meteor on numerous vision-language benchmarks, the details of which are described in Appendix B. These benchmarks require multifaceted information for diverse capabilities, including fundamental image understanding, real-world knowledge of common-sense knowledge, charts, diagrams, documents, signs, symbols, math problems, and more. Figure 1-2 and Table 1 illustrates vision language performances of various LLVMs, including Meteor-7B, open-, and closed-source LLVMs with various sizes. It is noteworthy that Meteor-7B noticeably outperforms the other models, demonstrating its efficacy and efficiency in using embedded multifaceted rationales from Meteor-Mamba. The detailed generation quality of \(\) Meteor is described in Appendix C. Apart from the results in Table 1, those in Table 2 signify that Meteor-7B also excels at more challenging benchmarks, which require multifaceted information simultaneously. Meteor-7B has outperformed

Table 2: Detailed comparison of \(\) Meteor across more challenging evaluation benchmarks.

other existing models by a large margin, some of which are equipped with additional vision encoders or computer vision models, demonstrating that rationales can provide multifaceted information more effectively than enhanced visual perception.

Ablation Studies.Furthermore, we have conducted several ablation studies to securely corroborate the effectiveness of our proposed method in light of six factors: (a) Meteor-Mamba, (b) Meteor-MLM, (c) the number of <tor> special tokens, (d) the distribution of <tor> special tokens, (e) rationale, and (f) 1.1M question-rationale-answer triples. Table 3 shows the following findings. Note that, Appendix D represents further ablation studies.

(**a**) Once Meteor-Mamba is replaced with other transformer-based models: BERT , GPT2 , and XLNet , we have discovered that the Mamba architecture takes advantage of its efficiency for embedding multifaceted rationales in terms of both computational complexity and model size. As the results suggest, Mamba demonstrates the highest batches-per-second (BPS) value and zero-shot performances on MME and MMB benchmarks among architectures of similar sizes, enabled by its inherent computational efficiency based on linear complexity and strong long sequence modeling capability , which other Transformer-based model architectures lack.

(**b**) We have tried using various pretrained LLMs of comparable sizes for Meteor-MLM, in order to identify the effectiveness of embedding multifaceted rationale together with traversal of rationale. We have observed that InternLM2  has shown the best performances.

(**c**) Varying the number of <tor> special tokens from 2 to 15, we have optimized it based on the vision language performances. The results suggest that using 10 <tor> special tokens shows the best performances for embedding abundant multifaceted rationales, balancing between compression and information preservation.

(**d**) The performances of \(\) Meteor depend on the distribution of <tor> special tokens when Meteor-Mamba is trained to embed multifaceted rationales. Given the observations, evenly distributing the tokens across lengthy rationales has shown the best performances. Prepending them to lengthy rationales may hinder effective embedding due to forgetting nature, and appending them to rationales in the end may not be guaranteed to understand the rationale. Randomly distributing the tokens across rationales may disrupt Meteor-Mamba's ability to stably learn the pattern of embedding rationales. Conversely, evenly distributed <tor> special tokens can segment lengthy rationales into shorter chunks and progressively embed them in a consistent manner, avoiding the issues of other distributions.

(**e**) In order to prove the effectiveness of multifaceted rationales through Meteor-Mamba, we ablated the use of the Mamba architecture and the rationales. The first row represents baseline performances where backbone MLM is only trained. For the second row, we only train backbone MLM with the curated multifaceted rationales without Meteor-Mamba, and for the third row, Mamba has been trained to embed answer instead of the rationales. Compared to the last row where Meteor is evaluated, the second and third rows fall short of performances, clearly showing that using multifaceted rationales through the Mamba architecture has contributed to performance improvement.

(**f**) As another way of showing the significance of multifaceted rationales, we have trained \(\) Meteor with different amounts of question-rationale pairs and evaluated \(\) Meteor trained with each of them.

Table 3: Ablation studies to identify the effectiveness of Meteor-Mamba and rationale through traversal of rationale by controlling the six main factors.

As expected, the more question-rationale pairs used in first training step, the better performances achieves, demonstrating the significance of utilizing multifaceted rationales for diverse capabilities.

Meteor-Mamba's Ability to Embed Rationales.We conduct a thorough analysis to confirm that Meteor-Mamba effectively embeds the rationales. To do this, we perform a retrieval task for multifaceted rationales, where we prepare ten different question-rationale pairs \((_{i},_{i})\) where \(i=0,1,,9\). These pairs are propagated through Meteor-Mamba with or without rationales under <tor> special tokens. This results in two sets of output features: one with rationale \(z_{i}^{}\) and one without rationales \(z_{j}^{}\), with \(j=0,1,,9\). We extract features corresponding to the placement of <tor> tokens, resulting in \(_{i}^{}^{10 768}\) and \(_{j}^{}^{10 768}\), where the dimension 10 corresponds to the number of <tor> tokens. We then compute the cosine similarity between \(_{i}^{}\) and \(_{j}^{}\) to measure the similarity of their representations. As illustrated in Figure 4, the diagonal values in the cosine similarity matrix are much higher than the off-diagonal values. This result indicates that Meteor-Mamba successfully embeds the rationale, and its output features contain multifaceted information even without explicit rationales in natural language. This explains how Meteor-Mamba operates effectively during the inference phase without explicit rationales.

Discussion and Limitation.From the experimental results observed, we gain the insight that equipping LLVMs with a multifaceted rationale is a key factor in building efficient LLVMs that demonstrate impressive vision language performances across numerous evaluation benchmarks requiring diverse capabilities. This rationale, furthermore, naturally reduces hallucination effects in POPE  and HallusionBench  in Table 1. Additionally, Table 2(c)-(d) shows that the need for additional vision encoders and computer vision models can be mitigated by incorporating a multifaceted rationale. However, \(\) Meteor might still be considered inefficient in terms of model size by users without high-end GPU resources, as it requires at least multiple GPUs with 48GB and 32GB VRAM for normal training and inference without (Q)LoRA [106; 105] and 4/8-bit quantization. Although many closed-source LLVMs have demonstrated superior performances following the scaling law , our goal is to reduce the model size while maintaining vision language performances as much as possible. We strongly believe that small language and vision models, even those with about 1\(\)3B parameters, can effectively narrow the performance gap with the closed-source LLVMs by using layer-analyzing approaches such as mixture of depths  and others [117; 118; 119; 120; 121; 122; 123], despite their inherent limitation in layer number and hidden dimension.

## 5 Conclusion

To build efficient LLVMs, we incorporate a multifaceted rationale encompassing various aspects such as image understanding, incorporating external common-sense knowledge, understanding non-object concepts (_e.g.,_ charts, diagrams, symbols, signs, and math), and following systematic step-by-step procedures for how to solve complex questions. \(\) Meteor demonstrates significantly enhanced vision language performances across various evaluation benchmarks without the need to scale up LLVMs, use additional vision encoders, or employ multiple computer vision models. In designing \(\) Meteor, the traversal of rationale combined with Mamba architecture proves highly effective in embedding lengthy rationales. We believe this rationale, facilitated by the traversal of rationale, can pave the way for more efficient models, representing a promising step towards achieving more efficient LLVMs.

Figure 4: Illuminating how the feature correspondences of cosine similarity are computed under the trained Meteor-Mamba, and showing the feature disparity for <tor> with/without rationale.