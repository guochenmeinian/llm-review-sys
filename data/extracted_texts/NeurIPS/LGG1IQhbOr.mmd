# Non-geodesically-convex optimization in the Wasserstein space

Hoang Phuc Hau Luu Hanlin Yu Bernardo Williams Petrus Mikkola Marcelo Hartmann Kai Puolamaki Arto Klami

Department of Computer Science, University of Helsinki

###### Abstract

We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is _nonconvex_ along generalized geodesics. Specifically, the objective exhibits some difference-of-convex structure along these geodesics. The setting also encompasses sampling problems where the logarithm of the target distribution is difference-of-convex. We derive multiple convergence insights for a novel _semi Forward-Backward Euler scheme_ under several nonconvex (and possibly nonsmooth) regimes. Notably, the semi Forward-Backward Euler is just a slight modification of the Forward-Backward Euler whose convergence is--to our knowledge--still unknown in our very general non-geodesically-convex setting.

## 1 Introduction

Sampling and optimization are intertwined. For example, the (overdamped) Langevin dynamics, typically considered a sampling algorithm, can be considered as gradient descent optimization where a suitable amount of Gaussian noise is injected at each step. There are also deeper connections. At the limit of infinitesimal stepsize, the law of the Langevin dynamics is governed by the Fokker-Planck equation describing a diffusion over time of probability measures. In the seminal paper , Jordan, Kinderlehrer, and Otto reinterpreted the Fokker-Planck equation as the gradient flow of the functional relative entropy, a.k.a. Kullback-Leibler (KL) divergence, in the (Wasserstein) space of finite second-moment probability measures equipped with the Wasserstein metric. The discovery connects the two fields and encourages optimization in the Wasserstein space, even conceptually, as it directly gives insight into the sampling context. Studies in continuous-time dynamics  seem natural and enjoy nice theoretical properties without discretization errors. Another line of research studies discretization of Wasserstein gradient flow by either quantifying the discretization error between the continuous-time flow and the discrete-time flow  or viewing discrete-time flows as _iterative optimization schemes_ in the Wasserstein space  where the primary focus is on (geodesically) _convex_ optimization problems.

Nonconvex, nonsmooth optimization is challenging, even in Euclidean space, quoting Rockafellar : _"In fact the great watershed in optimization isn't between linearity and nonlinearity, but convexity and nonconvexity."_ The landscape of nonconvex problems is mostly underexplored in the Wasserstein space. In the sampling language, it amounts to sampling from a _non-log-concave_ and possibly _non-log-Lipschitz-smooth_ target distribution. Recently, Balasubramanian et al.  advocated the need for a sound theory for non-log-concave sampling and provided some guarantees for the unadjusted Langevin algorithm (ULA) in sampling from log-smooth (Lipschitz/Holder smooth) densities. These results are preliminary for the ULA (and its variants) with a specific class of densities (smooth). Theoretical understandings of other classes of algorithms and densities are needed.

We approach the subject through the lens of nonconvex optimization in the space of probability distributions and pose discretized Wasserstein gradient flows as iterative minimization algorithms. This allows us to, on the one hand, use and extend tools from classical nonconvex optimization and, on the other hand, derive more connections between sampling and optimization.

We study the following _non-geodesically-convex_ optimization problem defined over the space \(_{2}(X)\) of probability measures \(\) over \(X=^{d}\) with finite second moment, i.e., \(\|x\|^{2}d(x)<+\),

\[_{_{2}(X)}():=_{F}()+ ():=_{G-H}()+()\] (1)

where \(F:X\) is a _nonconvex_ function which can be represented as a _difference_ of two convex functions \(G\) and \(H\), \(_{F}():= F(x)d(x)\) is the potential energy, and \(:_{2}(X)\{+\}\) plays a role as the regularizer which is assumed to be a convex function along _generalized geodesics_.

Why difference-of-convex structure?Nonconvexity lies at the difference-of-convex (DC) structure \(F=G-H\), where \(G\) and \(H\) are called the first and second DC components, respectively. \(F\) being nonconvex implies \(_{F}\) being non-geodesically-convex in general. First, the class of DC functions is very rich, and DC structures are present everywhere in real-world applications . Weakly convex and Lipschitz smooth (\(L\)-smooth or simply smooth) functions are two subclasses of DC functions. Furthermore, any continuous function can be approximated by a sequence of DC functions over a compact, convex domain . We also remark that many nonconvex functions admit quite natural DC decompositions, for example, an \(L\)-smooth function \(F\) has the following splittings: \(F(x)=\|x\|^{2}-(\|x\|^{2}-F(x))\) and \(F(x)=(F(x)+\|x\|^{2})-\|x\|^{2}\) whenever \( L/2\). Second, DC functions preserve enough structure to extend convex analysis. Such structure is key in classic DC programming  and in our Wasserstein space analysis with optimal transport tools.

ContextMany problems in machine learning and sampling fall into the spectrum of problem (1). For example, refer to a discussion in  that inspired our work. The regularizer \(\) can be the internal energy [4, Sect. 10.4.3]. Under McCann's condition, the internal energy is convex along generalized geodesics [4, Prop. 9.3.9]. In particular, the negative entropy, \(()=((x))d(x)\) if \(\) is absolutely continuous w.r.t. Lebesgue measure, \(+\) otherwise, is a special case of internal energy satisfying McCann's condition. In the latter case, \(()=_{}(\|^{*})+\) where \(^{*}(x)(-F(x))\), the optimization problem reduces to a sampling problem with _log-DC_ target distribution. In Bayesian inference, the posterior structure depends on both the prior and likelihood. If the likelihood is log-smooth, it exhibits the aforementioned DC splittings. Log-priors, often nonsmooth to capture sparsity or low rank, typically also have explicit DC structures . In the context of infinitely wide one-layer neural networks and Maximum Mean Discrepancy , let \(^{*}\) be the optimal distribution over a network's parameters, \(k\) be a given kernel, the regularizer is then the interaction energy \(()= k(x,y)d(x)d(y)\) and \(F(x)=-2 k(x,y)d^{*}(y)\). In general, \(\) is not convex along generalized geodesics and \(F\) is nonconvex but not necessarily DC. When the kernel has Lipschitz gradient, we can adjust both \(\) and \(F\) as \(()= k(x,y)+\|x\|^{2}+\|y\|^{2}d(x)d(y)\) and \(F(x)=-2 k(x,y)d^{*}(y)-2\|x\|^{2}\) for some \(>0\) making \(\) generalized geodesically convex and \(F\) concave (hence DC); Appx. A.2.

Our idea is to minimize (1) in the space of probability distributions by discretization of the gradient flow of \(\), leveraging on the JKO (Jordan, Kinderlehrer, and Otto) operator (2). In the previous work , this has been done with the Forward-Backward (FB) Euler discretization, but it lacks convergence analysis. Recently, Salim et al.  did some study on FB Euler, but their results do not apply here because \(F\) is nonconvex and possibly nonsmooth. Further leveraging on the DC structure of \(F\) and inspired by classical DC programming literature , we subtly modify the FB Euler to give rise to a scheme named semi FB Euler that enjoys major theoretical advantages as we can provide a wide range of convergence analysis. Regarding the name, "semi" addresses the splitting of the potential energy. The scheme can be nevertheless reinterpreted as FB Euler.

ContributionsTo our knowledge, no prior work studies problem (1) when \(F\) is DC. Therefore, most of the derived results in this paper are novel. We propose and analyze the semi FB Euler (4) leveraging on the classic DC optimization proof template  with a substantial accommodation of Wasserstein geometry and derive the following set of new insights:

1. We show that if the \(H\) is continuously differentiable, every cluster point of the sequence of distributions \(\{_{n}\}_{n}\) generated by semi FB Euler is a _critical point_ to \(\). Note that criticality is a notion from the DC programming literature  and it is a necessary condition for local optimality; See Sect. 3.3.
* We provide convergence rate of \(O(N^{-1})\) in terms of Wasserstein (sub)gradient mapping in the general nonsmooth setting. The notion of gradient mapping [33; 38; 55] is from the context of proximal algorithms in Euclidean space that is applicable to nonconvex programs where the notion of distance to global solution is--in general--not possible to work out.
* Under the extra assumption that \(H\) is continuously twice differentiable and has bounded Hessian, we provide a convergence rate of \(O(N^{-})\) in terms of distance of \(0\) to the Frechet subdifferential of \(\). One can think of this as convergence rate to Frechet stationarity, i.e., if \(^{*}\) is a Frechet stationary point of \(\), then, by definition, \(0\) is in the Frechet subdifferential of \(\) at \(^{*}\). Frechet stationarity is a relatively sharp necessary condition for local optimality.
* Under the assumptions of Thm. 3 and additionally \(\) satisfying the Lojasciewicz-type inequality for some Lojasciewicz exponent of \([0,1)\), we show that \(\{_{n}\}_{n}\) is a Cauchy sequence under Wasserstein topology, and thanks to the completeness of the Wasserstein space, the whole sequence \(\{_{n}\}_{n}\) converges to some \(^{*}\). We show that \(^{*}\) is in fact a global minimizer to \(\). Furthermore, we provide convergence rate of \(_{n}^{*}\) in three different regimes (\(W_{2}\) denotes the Wasserstein metric): (1) if \(=0\), \(W_{2}(_{n},^{*})\) converges to \(0\) after a finite number of steps; (2) if \((0,1/2]\), both \((_{n})-(^{*})\) and \(W_{2}(_{n},^{*})\) converges to \(0\) exponentially fast; (3) if \((1/2,1)\), both \((_{n})-(^{*})\) and \(W_{2}(_{n},^{*})\) converges sublinearly to \(0\) with rates \(O(n^{-})\) and \(O(n^{-})\), respectively. When \(\) is the negative entropy, \((_{n})-(^{*})=_{}(_{n}\| ^{*})\); Therefore, in the sampling context, we provide convergence guarantees in both Wasserstein and KL distances. See Sect. 4.3 for additional observations and implications.

## 2 Preliminaries

### Notations and basic results in measure theory and functional analysis

We denote by \(X=^{d}\), \((X)\) the Borel \(\)-algebra over \(X\), and \(^{d}\) the Lebesgue measure on \(X\). \((X)\) is the set of Borel probability measures on \(X\). For \((X)\), we denote its second-order moment by \(_{2}():=_{X}\|x\|^{2}d(x)\), where \(_{2}()\) can be infinity. \(_{2}(X)(X)\) denotes a set of finite second-order moment probability measures. \(_{2,}(X)_{2}(X)\) is the set of measures that are absolutely continuous w.r.t. \(^{d}\). Here \(\)-a.e. stands for almost everywhere w.r.t. \(\).

Let \(C^{p}(X),C_{c}^{}(X),C_{b}(X)\) be the classes of \(p\)-time continuously differentiable functions, infinitely differentiable functions with compact support, bounded and continuous functions, respectively.

From functional analysis , for each \(p 1\), \(L^{p}(X,)\) denotes the Banach space of measurable (where measurable is understood as Borel measurable from now on) functions \(f\) such that \(_{X}|f(x)|^{p}d(x)<+\). We shall consider an element of \(L^{p}(X,)\) as an equivalent class of functions that agree \(\)-a.e. on \(X\) rather than a sole function. The norm of \(f L^{p}(X,)\) is \(\|f\|_{L^{p}(X,)}=(_{X}|f(x)|^{p}d(x))^{1/p}\). When \(p=2\), \(L^{2}(X,)\) is actually a Hilbert space with the inner product \( f,g_{L^{2}(X,)}=_{X}f(x)g(x)d(x)\) which induces the mentioned norm. These results can be extended to vector-valued functions. In particular, we denote by \(L^{2}(X,X,)\) the Hilbert space of \(:X X\) in which \(\|\| L^{2}(X,)\). The norm \(\|\|_{L^{2}(X,X,)}:=(_{X}\|(x)\|^{2}d(x))^{1/2}\).

We say that \(f:X\) has quadratic growth if there exists \(a>0\) such that \(|f(x)| a(\|x\|^{2}+1)\) for all \(x X\). It is clear that if \(f\) has quadratic growth and \(_{2}(X)\), then \(f L^{1}(X,)\).

The pushforward of a measure \((X)\) through a Borel map \(T:X^{m}\), denoted by \(T_{\#}\) is defined by \((T_{\#})(A):=(T^{-1}(A))\) for every Borel sets \(A^{m}\).

### Optimal transport [4; 5; 69; 68]

Given \(,(X)\), the principal problem in optimal transport is to find a transport map \(T\) pushing \(\) to \(\), i.e., \(T_{\#}=\), in the most cost-efficient way, i.e., minimizing \(\|x-T(x)\|^{2}\) on \(\)-average. Monge's formulation for this problem is \(_{T:T_{\#}=}_{X}\|x-T(x)\|^{2}d(x)\), where the optimal solution, if exists, is denoted by \(T^{}_{}\) and called the optimal (Monge) map. Monge's problem can be ill-posed, e.g., no such \(T^{}_{}\) exists when \(\) is a Dirac mass and \(\) is absolutely continuous .

By relaxing Monge's formulation, Kantorovich considers \(_{(,)}_{X X}\|x-y\|^{2}d(x,y)\), where \((,)\) denotes the set of probabilities over \(X X\) whose marginals are \(\) and \(\), i.e, \((,)\) iff \(_{1\#}=,_{2\#}=\) where \(_{1},_{2}\) are the projections onto the first \(X\) space and the second \(X\) space, respectively. Such \(\) is called _a plan_. Kantorovich's formulation is well-posed because \((,)\) is non-empty (at least \((,)\)) and the \(\) element actually exists (see [5, Sect. 2.2]). The set of optimal plans between \(\) and \(\) is denoted by \(_{o}(,)\). In terms of random variables, any pairs \((X,Y)\) where \(X,Y\) is called a coupling of \(\) and \(\) while it is called an optimal coupling if the joint law of \(X\) and \(Y\) is in \(_{o}(,)\).

In \(_{2}(X)\), the \(\) value in Kantorovich's problem specifies a _valid_ metric referred to as Wasserstein distance, \(W_{2}(,)=(_{X X}\|x-y\|^{2}d(x,y))^{1/2}\) for some, and thus all, \(_{o}(,)\). The metric space \((_{2}(X),W_{2})\) is then called the Wasserstein space. In \(_{2}(X)\), beside the convergence notion induced by the Wasserstein metric, there is a weaker notion of convergence called _narrow convergence_: we say a sequence \(\{_{n}\}_{n}_{2}(X)\) converges narrowly to \(_{2}(X)\) if \(_{X}(x)d_{n}(x)_{X}(x)d(x)\) for all \( C_{b}(X)\). Convergence in the Wasserstein metric implies narrow convergence but the converse is not necessarily true. The extra condition to make it true is \(_{2}(_{n})_{2}()\). We denote Wasserstein and narrow convergence by \(}\) and \(}\), respectively.

If \(_{2,}(X),_{2}(X)\), Monge's formulation is well-posed and the unique (\(\)-a.e.) solution exists, and in this case, it is safe to talk about (and use) the optimal transport map \(T^{}_{}\). Moreover, there exists some convex function \(f\) such that \(T^{}_{}= f\)\(\)-a.e. Kantorovich's problem also has a unique solution \(\) and it is given by \(=(I,T^{}_{})\#\) where \(I\) is the identity map. This is known as Brenier theorem or polar factorization theorem .

### Subdifferential calculus in the Wasserstein space

Apart from being a metric space, \((_{2}(X),W_{2})\) also enjoys some pre-Riemannian structure making subdifferential calculus on it possible. Let us have a picture of a _manifold_ in mind. Firstly, the tangent space  of \(_{2}(X)\) at \(\) is \(_{}_{2}(X):=^{}(X)\}}^{L^{2}(X,X,)}\), where the closure is w.r.t. the \(L^{2}(X,X,)\)-topology. Intuitively, for \( C_{c}^{}(X)\), \(I+\) is an optimal transport map if \(>0\) is small enough , so \(\) plays a role as "tangent vector".

Let \(:_{2}(X)\{+\}\), we denote \(()=\{_{2}(X):()<+\}\). Let \(()\), we say that a map \( L^{2}(X,X,)\) belongs to the _Frechet subdifferential_[15; 43]\(_{F}^{-}()\) if \(()-()_{_{o}(,)}_{X X} (x),y-x d(x,y)+o(W_{2}(,))\) for all \(_{2}(X)\), where the little-o notation means \(_{s 0}o(s)/s=0\). If \(_{F}^{-}()\), we say \(\) is Frechet subdifferentiable at \(\). We also denote \((_{F}^{-})=\{_{2}(X):_{F }^{-}()\}\).

Similarly, we say that \( L^{2}(X,X,)\) belongs to the (Frechet) superdifferential \(_{F}^{+}()\) of \(\) at \(\) if \(-_{F}^{-}(-)()\). In other words, \(_{F}^{-}(-)()=-_{F}^{+}()\).

We say \(\) is Wasserstein differentiable [15; 43] at \(()\) if \(_{F}^{-}()_{F}^{+}()\). We call an element of the intersection, denoted by \(_{W}()\), a Wasserstein gradient of \(\) at \(\), and it holds \(()-()=_{X X}_{W}()(x),y-x d (x,y)+o(W_{2}(,))\), for all \(_{2}(X)\) and any \(_{o}(,)\). The Wasserstein gradient is not unique in general, but its parallel component in \(_{}_{2}(X)\) is unique, and this parallel component is again a valid Wasserstein gradient as the orthogonal component plays no role in the above definitions, i.e., if \(^{}_{}_{2}(X)^{}\), it holds \(_{X X}^{}(x),y-x d(x,y)=0\) for any \(_{2}(X)\) and \(_{o}(,)\)[43, Prop. 2.5]. We may refer to this parallel component as the _unique_ Wasserstein gradient of \(\) at \(\).

### Optimization in the Wasserstein space

A function \(:_{2}(X)\{+\}\) is called _proper_ if \(()\), while it is called lower semicontinuous (l.s.c) if for any sequence \(_{n}}\), it holds \(_{n}(_{n})()\).

We next recall (a simplified version of) _generalized_ geodesic convexity.

**Definition 1**.: _[_65_]_ _Let \(:_{2}(X)\{+\}\). We say \(\) is convex along generalized geodesics if \(,_{2}(X),\,_{2, }(X)\), \(((tT^{}_{}+(1-t)T^{}_{})_{\#}) t()+(1-t)()\), \( t\)._

The curve \(t(tT^{}_{}+(1-t)T^{}_{})_{\#}\) (called a generalized geodesic) interpolates from \(\) to \(\) as \(t\) runs from \(0\) to \(1\). The definition says that \(\) is convex along these curves. If \(_{2,}(X)\) and \(=\), the curve is a geodesic in \((_{2}(X),W_{2})\). If the definition is relaxed to the class of geodesics only, we say that \(\) is convex along geodesics.

An important characterization of Frechet subdifferential of a geodesically convex function is that we can drop the little-o notation in its definition in Sect. 2.3 [4, Sect 10.1.1]. As a convention, for a geodesically convex function \(\), the Frechet subdifferential \(_{F}\) will be simply written as \(\).

First-order optimality conditionsLet \(:_{2}(X)\{+\}\) be a proper function. \(^{*}_{2}(X)\) is a global minimizer of \(\) if \((^{*})(),_{2}(X)\). For local optimality, we shall use the Wasserstein metric to define neighborhoods. \(^{*}_{2}(X)\) is a local minimizer if there exists \(r>0\) such that \((^{*})()\) for all \(:W_{2}(,^{*})<r\). We shall denote \(B(^{*},r):=\{_{2}(X):W_{2}(,^{*})<r\}\) the (open) Wasserstein ball centered at \(^{*}\) with radius \(r\). If we replace \(<\) by \(\) we obtain the notion of a closed Wasserstein ball.

We call \(^{*}\) a Frechet stationary point of \(\) if \(0_{F}^{-}(^{*})\). Frechet stationarity is a necessary condition for local optimality. In other words, if \(^{*}\) is a local minimizer, it is a Frechet stationary point (Lem. 5 in Appendix). In addition, if \(\) is Wasserstein differentiable at \(^{*}\), \(_{W}(^{*})(x)=0\)\(^{*}\)-a.e. . When \(\) is geodesically convex, Frechet stationarity is a sufficient condition for global optimality (Lem. 6 in Appendix).

## 3 Semi Forward-Backward Euler for difference-of-convex structures

### Wasserstein gradient flows: different types of discretizations

To neatly present the idea of minimizing \(\) via discretized gradient flow, we first assume for a moment that \(F\) is infinitely differentiable and \(\) is the negative entropy. See also a discussion in .

We wish to minimize (1) in the space of probability distributions. A natural idea is to apply discretizations of the gradient flow of \(\), where the gradient flow is defined (under some technical assumptions ) as the limit \( 0^{+}\) of the following scheme with some simple time-interpolation

\[_{n+1}_{}(_{n}),\;\;\; _{}():=*{arg\,min}_{_{2}(X)}()+W_{2}^{2}(,).\] (2)

Straightforwardly, given a fixed \(>0\), (2) gives back a discretization for this flow known as Backward Euler. On the other hand, if \(\) is Wasserstein differentiable (Sect. 2.2), the Forward Euler discretization reads \(_{n+1}=(I-_{W}(_{n}))_{\#}_{n}\), which is reinterpreted as doing gradient descent in the space of probability distributions. These are optimization methods that work _directly_ on the objective function \(\) itself. However, the composite structure of \(\) (a sum of several terms) can also be exploited. One such scheme is the unadjusted Langevin algorithm (ULA), where it first takes a gradient step w.r.t. the potential part, then follows the heat flow corresponding to the entropy part : \(_{n+1}=(I- F)_{\#}_{n},\;\;_{n+1}=(0,2  I)*_{n+1}\), where \(*\) is the convolution. This ULA is "viewed" in the space of distributions (Eulerian approach), a more familiar and equivalent form of the ULA from the particle perspective (Lagrangian approach) goes like \(x_{n+1}=x_{n}- F(x_{n})+z_{k}\) where \(z_{k}(0,I)\). The ULA is known to be asymptotically biased even for Gaussian target measure (Ornstein-Uhlenbeck process). To correct this bias, the Metropolis-Hasting accept-reject step  is sometimes introduced. Metropolis-Hasting algorithm [52; 36] is a much more general framework that works with quite any proposal (e.g., a random walk) whose convergence analysis is based on the Markov kernel satisfying the detailed balance condition. This convergence framework is different from what is considered in this work: we are more interested in the underlying dynamics of the chain. Metropolis-Hasting algorithm is indeed another story.

In optimization, for composite structure, Forward-Backward (FB) Euler and its variants are methods of choice [59; 10]. The corresponding FB Euler for \(\) will take the gradient step (forward) according to the potential, and JKO step (backward) w.r.t. the negative entropy

\[_{n+1}=(I- F)_{\#}_{n},\;\;_{n +1}_{}(_{n+1}).\] (3)This scheme appears in  without convergence analysis, and later on  derives non-asymptotic convergence guarantees under the assumption \(F\) being convex and Lipschitz smooth.

In this work, as \(F\) is nonconvex and nonsmooth, the theory in  does not apply, and the convergence (if any) of (3) remains mysterious. The DC structure of \(F\) can be further exploited. In DC programming , the forward step should be applied to the concave part, while the backward step should be applied to the convex part. We hence propose the following semi FB Euler

\[_{n+1}=(I+ H)_{\#}_{n},_{n+1}_{(+_{G})}(_{n+1})\] (4)

for which we can provide convergence guarantees. Apparently, the difference between semi FB Euler and FB Euler is subtle: while FB Euler does forward on \(_{G-H}=_{G}-_{H}\) and backward on \(\), semi FB Euler does forward on \(-_{H}\) and backward on \(+_{G}\); recall that \(=_{G}-_{H}+\).

Theoretically, semi FB Euler enjoys some advantages compared to FB Euler. Thanks to Brenier theorem (Sect. 2.2), the pushing step in semi FB Euler is _optimal_ since \(H\) is convex; Meanwhile, the pushing in FB Euler is non-optimal whose optimal Monge map is not identifiable in general. The convergence of FB Euler is still an open question, even when \(F\) is (DC) differentiable. In contrast, we can provide a solid theoretical guarantee for semi FB Euler, especially when \(H\) is differentiable. Additionally, we also offer convergence guarantees when \(H\) is nonsmooth.

### Problem setting

Our goal is to minimize the non-geodesically-convex functional \(()=_{F}()+()\) over \(_{2}(X)\), where \(F=G-H\) is a DC function. We make Assumption 1 throughout the paper:

**Assumption 1**.:
1. _The objective function_ \(\) _is bounded below._
2. \(G,H:X\) _are convex functions and have quadratic growth._
3. \(:_{2}(X)\{+\}\) _is proper,_ _l.s.c, and convex along generalized geodesics in_ \((_{2}(X),W_{2})\)_, and_ \(()_{2,}(X)\)_._
4. _There exists_ \(_{0}>0\) _such that_ \((0,_{0})\)_,_ \(_{(_{G}+)}()\) _for every_ \(_{2}(X)\)_._

Note that Assumption 1(iv) is a commonly-used assumption to simplify technical complication when working with the JKO operator . Assumption 1(ii) implies \(_{G}\) and \(_{H}\) are continuous w.r.t. Wasserstein topology [3, Prop. 2.4] (\(G,H\) are continuous [54, Cor. 2.27] and have quadratic growth).

### Optimality characterizations

First, it follows from Assumption 1(iii), \(()_{2,}(X)\). By analogy to DC programming in Euclidean space, we call \(^{*}()\) a _critical point_ of \(=+_{G}-_{H}\) if \((+_{G})(^{*})_{H}( ^{*})\). Criticality is a necessary condition for local optimality (Lem. 7). Moreover, if \(_{H}\) is Wasserstein differentiable at \(^{*}\), criticality becomes Frechet stationarity (Lem. 8).

### Semi FB Euler: a general setting

We allow \(H\) to be non-differentiable in some derivations, meaning that \( H\) (convex subdifferential ) contains multiple elements in general. We first pick a selector \(S\) of \( H\), i.e., \(S:X X\), such that \(S(x) H(x)\). By the axiom of choice (Zermelo, 1904, see, e.g., ), such selection always exists. However, an arbitrary selector can behave badly, e.g., not measurable. We shall first restrict ourselves to the class of Borel measurable selectors (see Appx. A.1 for an existence discussion).

**Assumption 2** (Measurability).: _The selector \(S\) is Borel measurable._

We recall the semi FB scheme (4) but for nonsmooth \(F\) as follows: start with an initial distribution \(_{0}_{2,}(X)\), given a discretization stepsize \(0<<_{0}\), we repeat the following two steps:

\[_{n+1}=(I+ S)_{\#}_{n}_{n+1}=_{(_{G}+)}(_{n+1}) \]

Well-definiteness and properties: Given \(_{n}_{2}(X)\), it follows from Lem. (4) that \(_{n+1}_{2}(X)\). The two generated sequences are then in \(_{2}(X)\). Moreover, it follows from Assumption 1 that \(\{_{n}\}_{n}\) are in \(_{2,}(X)\), so are \(\{_{n}\}_{n}\) using Lem. 9 by noting that \(I+ S\) is subgradient of a strongly convex function \(x(1/2)\|x\|^{2}+ H(x)\).

Convergence analysis

### Asymptotic analysis

**Lemma 1** (Descent lemma).: _Under Assumptions 1 and 2, let \(\{_{n}\}_{n}\) be the sequence of distributions produced by semi FB Euler starting from some \(_{0}_{2,}(X)\) with \(0<<_{0}\). Then it holds \((_{n+1})(_{n})-_{X}\|T^{_ {n}}_{_{n+1}}(x)-T^{_{n+1}}_{_{n+1}}(x)\|^{2}d_{n+1}(x), n \)._

Lem. 1 shows that the objective does not increase along semi FB Euler's iterates. Proof of Lem. 1 is in Appx. A.3. By using Lem. 1, we establish asymptotic convergence for semi FB Euler as follows.

For the asymptotic convergence analysis, we need the following assumption on \(H\).

**Assumption 3**.: \(H\) _is continuously differentiable._

**Theorem 1** (Asymptotic convergence).: _Under Assumptions 1, 3, let \(\{_{n}\}_{n}\) and \(\{_{n}\}_{n}\) be sequences produced by semi FB Euler starting from some \(_{0}_{2,}(X)\) with \(0<<_{0}\). If \(\{_{n}\}_{n}\) is relatively compact with respect to the Wasserstein topology and \(_{n}(_{n})<+\), then every cluster point of \(\{_{n}\}_{n}\) is a critical point of \(\)._

Proof of Thm.1 is in Appx. A.4. Thm. 1 does not ensure convergence of the whole sequence \(\{_{n}\}_{n}\); Rather, it guarantees subsequential convergence to critical points of \(\).

**Remark 1**.: _In the Euclidean space, the compactness assumption of the generated sequence is usually enforced via the coercivity assumption: \(f(x)+\) whenever \(\|x\|+\). A striking difference in the Wasserstein space is that closed Wasserstein balls are not compact in the Wasserstein topology [43, Prop. 4.2], making coercivity not sufficient to induce (Wasserstein) compactness. For Thm. 1, we simply assume the sequence \(\{_{n}\}_{n}\) to be relatively compact._

### Non asymptotic analysis

To measure how fast the algorithm converges, we need some convergence measurement. First, for proximal-type algorithms in Euclidean space, the notion of _gradient mapping_\(_{}(x_{n})\) is usually used (see, e.g.,  and [38, Eq. (5)]) and we measure the rate \(\|_{}(x_{n})\|^{2} 0\). In analogy as in Euclidean space, we define the _Wasserstein (sub)gradient mapping_ as follows \(_{}():=(I-T^{_{(_{G}+)}((I+ S)_{\#})}_{})\), and we measure the rate of \(\|_{}(_{n})\|^{2}_{L^{2}(X,X,_{n})} 0\).

**Theorem 2** (Convergence rate: Wasserstein (sub)gradient mapping).: _Under Assumptions 1, 2, let \(\{_{n}\}_{n}\) be the sequence of distributions produced by semi FB Euler starting from some \(_{0}_{2,}(X)\) with \(0<<_{0}\). Then it holds \(_{n=}\|_{}(_{n})\|^{2}_{L^{2}(X,X,_{n} )}=O(N^{-1})\)._

Proof of Thm. 2 is in Appx. A.5. This theorem holds without requiring \(G\) and \(H\) to be differentiable.

Next, if \(H\) is twice differentiable with uniformly bounded Hessian, we can derive a stronger convergence guarantee based on Frechet stationarity (see Sect. 2.4). In other words, we evaluate the rate of \((0,_{F}^{-}(_{n})):=_{ _{F}^{-}(_{n})}\|\|_{L^{2}(X,X;_{n})} 0\).

**Assumption 4**.: \(H C^{2}(X)\) _whose Hessian is bounded uniformly (\(H\) is then \(L_{H}\)-smooth)._

**Theorem 3** (Convergence rate: Frechet subdifferentials).: _Under Assumptions 1, 4, let \(\{_{n}\}_{n}\) be the sequence of distributions produced by semi FB Euler starting from some \(_{0}_{2,}(X)\) with \(0<<_{0}\), then \(_{n=}(0,_{F}^{-}(_{n} ))=O(N^{-}).\)_

Proof of Thm. 3 is in Appx. A.6.

### Fast convergence under isoperimetry and beyond

Fast convergence can be obtained under _isoperimetry_, e.g., log-Sobolev inequality (LSI). There are certain connections between LSI in sampling and the Lojasiewicz condition in optimization allowing linear convergence. In nonconvex optimization in Euclidean space, analytic and subanalytic functions are a large class satisfying Lojasiewicz condition . Subanalytic DC programs are studied in . In the infinite-dimensional setting of the Wasserstein space, the Lojasiewicz condition should be regarded as functional inequalities .

**Assumption 5** (Lojasiewicz condition in the Wasserstein space).: _Assume that \(^{*}\) is the optimal value of \(\), and assume there exist \(r_{0}(^{*},+]\), \([0,1)\), and \(c>0\) such that for all \(_{2}(X)\), \(()-^{*}<r_{0} c(()- ^{*})^{}\{\|\|_{L^{2}(X,X,)}: _{F}^{-}()\}\), where the conventions \(0^{0}=0\) and \(=+\) are used. We call \([0,1)\) the Lojasiewicz exponent of \(\) at optimality._

**Remark 2**.: _If \(\) is the is negative entropy, \(F C^{2}(X)\) whose Hessian is bounded uniformly, then \(\) is Wasserstein differentiable at \(_{2,}(X)\) with gradient \(_{W}()=+ F\) provided that all terms are well-defined [43, Prop. 2.12, E.g. 2.3]. We have \(\|_{W}()\|_{L^{2}(X,X,)}^{2}=\|+ F(x)\|^{2}d(x)=(x)\| {(x)}{^{*}(x)}\|^{2}dx\), where \(^{*}(-F)\). On the other hand, \(()-^{*}=_{}(\|^{*})\). The log-Sobolev inequality with parameter \(>0\) inequality reads \(_{}(\|^{*})\ (\|^{*}):=(x)\|(x)}\|^{2}dx\), where \((\|^{*})\) is the relative Fisher information of \(\) w.r.t. \(^{*}\). Therefore, log-Sobolev inequality is a special case of Lojasiewicz condition with \(=1/2\). In another case, when the objective function is the Maximum Mean Discrepancy, under some regularity assumption of the kernel, it holds _

\[2(()-(^{*}))\|^{*}-\|_{^{-1}( )}\|_{W}()\|^{2}d(x)\]

_where \(\|^{*}-\|_{^{-1}()}\) is the weighted negative Sobolev distance. This is "nearly" the Lojasiewicz condition, with a caveat that \(\|^{*}-\|_{^{-1}()}\) may be unbounded. Nevertheless, assuming the boundedness of this term along the algorithm's iterates is sufficient for convergence._

**Theorem 4**.: _Under Assumptions 1, 4 and Assumption 5 with parameters \((r_{0},c,)\). Let \(\{_{n}\}_{n}\) be the sequence of distributions produced by semi FB Euler starting from some sufficiently warm-up \(_{0}_{2,}(X)\) such that \((_{0})<r_{0}\) and with stepsize \(0<<_{0}\), then_

* _if_ \(=0\)_,_ \((_{n})-^{*}\) _converges to_ \(0\) _in a finite number of steps;_
* _if_ \((0,1/2]\)_,_ \((_{n})-^{*}=O(()^{n})\) _where_ \(M=L_{H}^{2}+1)}{c^{2}};\)__
* _if_ \((1/2,1)\)_,_ \((_{n})-^{*}\) _converges sublinearly to_ \(0\)_, i.e.,_ \((_{n})-^{*}=O(n^{-}).\)__

Proof of Thm. 4 is in Appx. A.7.

**Remark 3**.: _In the usual sampling case, i.e., \(\) is the negative entropy, and under log-Sobolev condition, \(r_{0}=+\). Therefore, \(_{0}\) can be arbitrarily in \(_{2,}(X)\). In the general case, however, a good enough starting point (i.e., \((_{0})<r_{0}\)) is needed to guarantee we are in the region where Lojasiewicz condition comes into play. In such a case, \((_{n})-^{*}=_{}(_{n}\|^{*})\) where \(^{*}(x)(-F(x))\) is the target distribution (see Rmk. 2), so Thm. 4 provides convergence rate of \(\{_{n}\}_{n}\) to \(^{*}\) in terms of KL divergence and this convergence is exponentially fast if \((0,1/2]\)._

**Theorem 5**.: _Under the same set of assumptions as in Thm. 4, the sequence \(\{_{n}\}_{n}\) is a Cauchy sequence under Wasserstein topology. Furthermore, as the Wasserstein space \((_{2}(X),W_{2})\) is complete [5, Thm. 2.2], every Cauchy sequence is convergent, i.e., there exists \(^{*}_{2}(X)\) such that \(_{n}}^{*}.\) The limit distribution \(^{*}\) is indeed the global minimizer of \(\). In addition:_

* _if_ \(=0\)_,_ \(W_{2}(_{n},^{*})\) _converges to_ \(0\) _in a finite number of steps;_
* _if_ \((0,1/2]\)_,_ \(W_{2}(_{n},^{*})=O(()^{n}),\) _where_ \(M=1+L_{H}^{2}+1))^{}}{(1-)^{}c^{}}\)_;_
* _if_ \((1/2,1)\)_,_ \(W_{2}(_{n},^{*})=O(n^{-})\)_._

Proof of Thm. 5 is in Appx. A.8. This theorem provides convergence to optimality in terms of Wasserstein distance.

**Remark 4**.: _If \(\) is the negative entropy and \(=1/2\), under some technical assumptions on \(F\) (e.g., continuously twice differentiable), LSI implies Talagrand inequality  (in optimization, known as Lojasiewicz implies quadratic growth ), meaning that KL divergence controls squared Wasserstein distance, so fast convergence under KL divergence implies fast convergence under Wasserstein distance._

## 5 Practical implementations

The push-forward step \(_{n+1}=(I+ H)_{\#}_{n}\) is rather straightforward: if \(Z\) are samples from \(_{n}\) then \(Z+ H(Z)\) are samples from \(_{n+1}\). On the other hand, to move from \(_{n+1}\) to \(_{n+1}\) we have to work out the JKO operator. Recent advances [53; 2] propose using the gradient of an input-convex neural network (ICNN)  to approximate the optimal Monge map pushing \(_{n+1}\) to \(_{n+1}\), which we briefly describe as follows. This approach is inspired by Brenier theorem asserting that an optimal Monge map has to be the (sub)gradient field of some convex function. Therefore, one can "parametrize" \(_{2,}(X)\) as \(=_{\#}_{n+1}\) for some convex function \(\). We then write the JKO objective as

\[(_{\#}_{n+1})+_{X}G((x))d_{n+1}(x)+ _{X}\|x-(x)\|^{2}d_{n+1}(x).\] (5)

While the two last terms (potential energy and squared Wasserstein distance) in (5) can be handled efficiently by the Monte Carlo method using samples from \(_{n+1}\), the first term \(\) might be complicated as it possibly involves the (unavailable) density of \(_{n+1}\). We remark that the easy case would be \(\) being another potential energy or an interaction energy. In such a case, Monte Carlo approximations are again readily applicable. The tricky case would be \(\) being the negative entropy that requires the density of \(_{n+1}\). Fortunately, we have the following change of entropy formula: for any \(T:X X\) diffeomorphic, any \(_{2,}(X)\), it holds \(-(T_{\#})=-()+_{X}| T(x)|d (x)\). Therefore, (5) can be written as (up to a constant that does not depend on \(\))

\[_{X}[-^{2}(x)+G((x))+\|x -(x)\|^{2}]d_{n+1}(x).\]

Note that this entropy formula can be extended naturally to the case of general internal energy , which means we can also handle this general case. Let us now consider the entropy case for simplicity. We can leverage on a class of input convex neural networks \(_{}(x)\) (\(\) is the neural network's parameters, \(x\) is the input) in which \(x_{}(x)\) is convex. Optimizing over \(\) can then be solved effectively by standard deep learning optimizers (e.g. Adam). The complete scheme is given in Alg. 1. We also remark that  further proposes fast approximation for \(^{2}\) and  leverages on the variational formula of the KL to propose an even faster scheme. Nevertheless, these schemes are generally expensive. For illustrative purposes, we adopt the vanilla version of . The iteration complexity is thus cubic in \(d\), linear in the size of the ICNN, and linear in the iteration count \(k\).

``` Input: Initial measure \(_{0}_{2,}(X)\), discretization step size \(>0\), number of steps \(K>0\), batch size \(B\). for\(k=1\) to \(K\)do for\(i=1,2,\)do  Draw a batch of samples \(Z_{0}\) of size \(B\); \((I+ H)_{x}_{_{k}}(I+  H)_{x}_{_{k-1}}(I+ H)(Z)\); \(_{2}^{2}_{}\|_{x}_{ }()-\|^{2}\); \(}_{}G(_{x}_{ }())\); \(}-_{} _{x}^{2}_{}()\). \(}_{2}^{2}+}+}\).  Apply an optimization step (e.g., Adam) over \(\) using \(_{}}\). endfor \(_{k+1}\). endfor ```

**Algorithm 1** Semi FB Euler for sampling

## 6 Numerical illustrations

We perform numerical sampling experiments from non-log-concave distributions: the Gaussian mixture distribution and the distance-to-set-prior  relaxed von Mises-Fisher distribution. Bothare log-DC and the latter has _non-differentiable_ logarithmic probability density (see Appx. C). Fig. 1 presents the sampling results. Experiment details are in Appx. B and Appx. C1.

## 7 Discussion and related work

We first narrow down our discussion on FB Euler and its variants in the Wasserstein space. When \(\) is the negative entropy, Wibisono  provides some insightful discussion on how FB Euler should be consistent (no asymptotic bias) because the backward step is adjoint to the forward step, hence preserves stationarity. However, no convergence theory is presented for FB Euler in the Wasserstein space in . Recently, Salim et al.  provide convergence guarantee for FB Euler within the following setting: \(\) is convex along generalized geodesics, \(F\) is Lipschitz smooth and convex/strongly convex. This setting remains a "convex + convex" structure, while ours has a "convex + concave" structure. A natural extension of our work would be a full-fledged study of DC programming in the Wasserstein space \(}=}-}\) where \(}\) and \(}\) are convex along generalized geodesics and \(}\) is not necessarily potential energy \(_{H}\). This problem possesses another implementation challenge regarding the Wasserstein gradient of \(}\).

Other works that bear a tangential relation to ours involve the use of forward-only Euler and ULA. Arbel et al.  study Wasserstein gradient flows with forward Euler for maximum mean discrepancy. This objective function is also nonconvex (specifically, weakly convex). Durmus et al.  analyze the ULA from the convex optimization perspective. Vempala et al.  show that LSI and Hessian boundedness suffice for fast convergence of the ULA where "fast" is understood as fast to the biased target since ULA is a biased algorithm. Balasubramanian et al.  analyze the ULA under quite mild conditions: log-density is Lipschitz/Holder smooth. Bernton  studies the proximal-ULA also under the convex assumption, where the difference to the ULA is the first step: gradient descent is replaced by the proximal operator. Similar to ULA, proximal-ULA is asymptotically biased. To address nonsmoothness, another line of research utilizes Moreau-Yosida envelopes to create smooth approximations of the ULA dynamics [29; 50]. This approach is also applicable to certain classes of non-log-concave distributions  and is more of a flavour of discretization error quantification.

## 8 Conclusion

We propose a new semi FB Euler scheme as a discretization of Wasserstein gradient flow and show that it has favourably theoretical guarantees that the commonly used FB Euler does not yet have if the objective function is not convex along generalized geodesics. Our theoretical analysis opens up interesting avenues for future work. Given the ubiquity of nonconvexity, we hope that the idea can be reused in various contexts, such as with different optimal transport cost functions, different base spaces in the Wasserstein space , or submanifolds of the Wasserstein space (e.g., Bures-Wasserstein [42; 24]).

Figure 1: **(a) and (b)**: Mixture of Gaussians. **(a)** shows samples obtained from semi FB Euler at iteration \(40\) and **(b)** shows KL divergence along the training process: semi FB Euler with sound theory is as fast as FB Euler. We also show the ULAâ€™s final result as a horizontal line for reference; **(c) and (d)**: Relaxed von Mises-Fisher. **(c)** shows true probability density, and **(d)** shows the sample histogram obtained from semi FB Euler. In this experiment, FB Euler fails to work, attributed to the high curvature of the relaxed von Mises-Fisher.