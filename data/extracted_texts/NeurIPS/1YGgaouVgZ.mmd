# Wide Two-Layer Networks can Learn from

Adversarial Perturbations

 Soichiro Kumano

The University of Tokyo

kumano@cvm.t.u-tokyo.ac.jp

&Hiroshi Kera

Chiba University, Zuse Institute Berlin

kera@chiba-u.jp

&Toshihiko Yamasaki

The University of Tokyo

yamasaki@cvm.t.u-tokyo.ac.jp

###### Abstract

Adversarial examples have raised several open questions, such as why they can deceive classifiers and transfer between different models. A prevailing hypothesis to explain these phenomena suggests that adversarial perturbations appear as random noise but contain class-specific features. This hypothesis is supported by the success of perturbation learning, where classifiers trained solely on adversarial examples and the corresponding _incorrect labels_ generalize well to correctly labeled test data. Although this hypothesis and perturbation learning are effective in explaining intriguing properties of adversarial examples, their solid theoretical foundation is limited. In this study, we theoretically explain the counterintuitive success of perturbation learning. We assume wide two-layer networks and the results hold for any data distribution. We prove that adversarial perturbations contain sufficient class-specific features for networks to generalize from them. Moreover, the predictions of classifiers trained on mislabeled adversarial examples coincide with those of classifiers trained on correctly labeled clean samples. The code is available at https://github.com/s-kumano/perturbation-learning.

## 1 Introduction

Adversarial examples , which are imperceptibly perturbed inputs designed to deceive machine learning models, have raised significant concerns about the robustness and reliability of these models. Despite their importance, the underlying mechanisms of adversarial examples are not yet fully understood. A prevailing hypothesis to explain the intriguing properties of adversarial examples is the "feature hypothesis" . This hypothesis posits that adversarial perturbations, while appearing as imperceptible noise to humans, contain class-specific features. The feature hypothesis provides a unified explanation for several puzzling phenomena associated with adversarial examples, such as their ability to deceive classifiers, transferability across models, and so on (cf. Section2.1).

Perturbation learning  provides empirical evidence supporting the feature hypothesis. In this learning, classifiers are trained _solely_ on adversarial examples that are _mislabeled_ in human perception,1 yet they demonstrate remarkable generalization to clean test data (Fig.1). For example, classifiers achieved 77% accuracy on the correctly labeled clean test dataset of CIFAR-10 , even though theywere trained on entirely mislabeled adversarial examples (e.g., a cat adversarial image labeled as a bird) . This surprising result suggests that adversarial perturbations encode class-relevant features that enable classifiers to learn meaningful representations. However, despite the empirical support, the theoretical foundations of the feature hypothesis and perturbation learning remain limited. While a recent study  provided theoretical justifications, their results rely on stringent assumptions about data distribution, perturbation design, training procedure, and model architectures.

In this study, we theoretically address the understanding and justification of the feature hypothesis and perturbation learning. First, to support the feature hypothesis, we show that adversarial perturbations, while appearing as random noise, are parallel to the weighted sum of all training samples. This result suggests that a single perturbation derived from a classifier and input can potentially contain information about the entire training dataset. In particular, for some specific cases (e.g., when training samples are mutually orthogonal), perturbations include all training data and labels without loss of information. We then reveal that class features within perturbations enable classifiers to generalize from them. Specifically, under three mild conditions, the predictions of a classifier trained on adversarial perturbations are consistent with those of a classifier trained on correctly labeled clean samples. These three conditions can be interpreted from geometric and quantitative perspectives. Finally, we demonstrate that under similar conditions, the prediction agreement is observed between a classifier trained on mislabeled adversarial examples and one trained on correctly labeled clean samples, justifying the empirical success of perturbation learning.

Our analysis assumes two-layer neural networks with sufficient width but does not impose any assumptions on data distribution, which is a substantial progress from prior work  that considered mutually orthogonal training samples. In addition, our perturbation design, training procedure, activation functions, and bias availability are milder. In short, as shown in Tab. 1, except for the wide width assumption, our analysis requires milder conditions than prior work. Our contributions can be summarized as follows:

* We provide a theoretical justification for the feature hypothesis and perturbation learning using wide two-layer neural networks, considering any data distribution and realistic problem settings. Except for the wide width, our assumptions are substantially milder than .
* We demonstrate that adversarial perturbations are parallel to the weighted sum of training samples, suggesting that a single perturbation can potentially contain information about the entire training dataset. This result supports the feature hypothesis.
* We prove that under three mild conditions, the predictions of a classifier trained on perturbations are consistent with those of a classifier trained on correctly labeled clean samples. Moreover, under similar conditions, the prediction agreement between a classifier trained on mislabeled adversarial samples and one trained on clean samples is observed, providing a theoretical justification for the empirical success of perturbation learning.

Figure 1: Counterintuitive generalization of perturbation learning.1 A classifier \(g\) is trained solely on mislabeled adversarial examples \(^{}:=(_{n}^{},y_{n}^{})\}_{n=1}^{N}\). These examples \(_{n}^{}\) are generated to mislead a classifier \(f\), which is trained on correctly labeled clean samples \(:=\{(_{n},y_{n})\}_{n=1}^{N}\), into predicting \(y_{n}^{}\) (\( y_{n}\)). Surprisingly, despite being trained only on mislabeled data, the classifier \(g\) generalizes well to clean test samples. This counterintuitive result suggests that adversarial perturbations contain label-aligned class features, enabling the classifier \(g\) to generalize from them.

[MISSING_PAGE_FAIL:3]

### Problem Setup

In this study, we consider the dynamics of perturbation learning in binary classification problem with a two-layer neural network trained by gradient flow. First, we formally define the perturbation learning framework. The outline of perturbation learning is as follows: (i) train a classifier on correctly labeled clean samples, (ii) create adversarial samples based on the trained classifier, and (iii) train another classifier on the mislabeled adversarial samples.

**Network trained on correctly labeled clean samples.** We consider a two-layer neural network \(f:^{d}\). Let \(:=(_{1},,_{m})^{}^{m d}\) and \(:=(a_{1},,a_{m})^{}^{m}\) be the hidden weight and bias, respectively. We also describe \(:=(V_{ij})_{1 i m,1 j d}\). Let \(:=(_{1},,_{m})^{}^{m}\) be the readout weight. While \(\) and \(\) are trainable, \(\) is fixed during training. Denote the trainable parameters by \(_{,}:=\{,\}\). We initialize \(V_{ij}(0,1/d)\), \(a_{i}(0,1)\), and \(_{i}(0,1/m)\) for each \(i[m]\) and \(j[d]\). The activation function is either ReLU or Leaky-ReLU \((x):=( x,x)\) for \([0,1)\). Finally, the network is given by \(f(;_{,}):=_{i=1}^{m}_{i}( _{i},+a_{i})\).

**Network trained on mislabeled adversarial samples.** Similarly to \(f\), we define a network trained on mislabeled adversarial samples as \(g(;_{,}):=_{i=1}^{m}_{i}(_{i},+b_{i})\). Note that the initializations of \(f\) and \(g\) are independent.

**Loss function.** We consider a differentiable, non-decreasing loss function \(:\), satisfying \(^{}(z) 0\) for any \(z\). Examples of such loss functions include the identity loss \((z):=z\), exponential loss \((z):=(z)\), and logistic loss \((z):=(1+(z))\).

**Training.** We here describe the training process of the network \(f\) on correctly labeled clean samples. The training of \(g\) is similarly defined. Let \(:=\{(_{n},y_{n})\}_{n=1}^{N}^{d}\{ 1\}\) be a correctly labeled training dataset. The loss over \(\) is defined as \((_{,};):=(1/N)_{n=1}^{N}(- y_{n}f(_{n};_{,}))\). The network parameters are updated by gradient flow \(_{,}(t)/t:=-( {}_{,}(t);)/_{,}\), where \(t 0\) is the training time. We consider \(T_{f}>0\) training steps, producing \(f(\,\,;_{,}(T_{f}))\). For notational simplicity, we write \(f(\,\,;t):=f(\,\,;_{,}(t))\).

Note that we do not consider whether \(f(\,\,;T_{f})\) perfectly classify \(\). We discuss whether the classifier \(g\), trained on adversarial examples crafted via \(f(\,\,;T_{f})\), can mimic the predictions of \(f(\,\,;T_{f})\).

**Adversarial perturbations.** We consider a single-step gradient-based perturbation, which is a common perturbation design . An adversarial example \(_{n}^{}^{d}\) and its corresponding adversarial perturbation \(_{n}^{d}\) are defined as follows:

\[_{n}^{}:=_{n}+_{n}, _{n}:=-_{_{n}}(-y_{n}^{ }f(_{n};T_{f}))}{\|_{_{n}}(-y_{n}^{ }f(_{n};T_{f}))\|},\] (1)

where \(>0\) is the perturbation constraint and \(y_{n}^{}\{ 1\}\) is the target label. The adversarial perturbation \(_{n}\) on \(_{n}\) is designed to increase \(y_{n}^{}f(_{n}^{};T_{f})\) under the constraint \(\|_{n}\|\).

    &  & Ours \\  Training samples & Mutually orthogonal & **Any** \\ Perturbation type & Oracle-based & **Standard gradient-based** \\ Perturbation budget & Unrealistically tight & **Any** \\ Training time & Infinite & **Any** \\ Loss function & Exponential or logistic & **Differentiable, non-decreasing** \\ Network bias & Not available & **Available** \\ Activation & Leaky-ReLU & **ReLU and Leaky-ReLU** \\ Network width & **Any** & Sufficiently wide (but finite) \\ Theoretical framework & Feature learning & Lazy training \\  Common & Binary classification, two-layer network, gradient flow \\   

Table 1: Comparison with existing work . With a wide network assumption, we improve the existing results from the perspective of data distribution, perturbation design, training time, loss function, and network architecture. Note that the non-bias and leaky-ReLU assumptions of  are critical for deriving their results. A detailed comparison can be found in Section 3.4.

**Mislabeled dataset.** We consider two configurations of a dataset \(^{}\) for training \(g\). First, we follow the original perturbation learning approach, where classifiers are trained on adversarial perturbations superposed on natural images, i.e., \(^{}:=\{(_{n}^{},y_{n}^{}) \}_{n=1}^{N}\). This setting helps to understand the prior perturbation learning process. Second, we directly consider learning from perturbations rather than adversarial examples, i.e., \(^{}:=\{(_{n},y_{n}^{})\}_{n=1}^{N}\). This setting directly addresses the question of whether classifiers can generalize from class features in perturbations.

**Summary.** The problem setting is summarized as follows:

**Setting 3.1** (Perturbation learning).: Independently initialize \(V_{ij}(0,1/d)\), \(W_{ij}(0,1/d)\), \(a_{i}(0,1)\), \(b_{i}(0,1)\), \(_{i}(0,1/m)\), and \(_{i}(0,1/m)\) for each \(i[m]\) and \(j[d]\). Train a two-layer neural network \(f\) parameterized by \(_{,}\) with (\(\)-scaled Leaky-) ReLU on a dataset \(:=\{(_{n},y_{n})\}_{n=1}^{N}\) using gradient flow with a loss \((_{,};)\) for training time \(T_{f}>0\). Create a dataset \(^{}\) by one of the following procedures with \(\{y_{n}^{}\}_{n=1}^{N}\{ 1\}^{N}\):

\[ ^{}:=\{(_{n},y_{n}^{}) \}_{n=1}^{N},\] (2) \[ ^{}:=\{(_{n}^{},y_{n}^{ })\}_{n=1}^{N}.\] (3)

Train a two-layer neural network \(g\) parameterized by \(_{,}\) on the dataset \(^{}\) using gradient flow with a loss \((_{,};^{})\) for training time \(T_{g}>0\).

Our interests are (i) the relationship between perceptually-noise-like adversarial perturbations \(\{_{n}\}_{n=1}^{N}\) and clean training samples \(\{(_{n},y_{n})\}_{n=1}^{N}\) (cf. Theorem 3.3), and (ii) whether the classifier \(g(\,\,;T_{g})\) trained on the adversarial perturbations or samples \(^{}\) can mimic the predictions of the classifier \(f(\,\,;T_{f})\) trained on the clean samples \(\) (cf. Theorems 3.4 and 3.5).

### Main Results

For \(_{1},_{2}^{d}\), we use \((_{1},_{2})((1+)/2,(1+)/2]\) defined as (cf. Lemma C.4):

\[(_{1},_{2}):=_{(0,/d),a (0,1)}[^{}(,_{1}+a)^{ }(,_{2}+a)],\] (4)

where \(^{}(x):=\,(x)/x\). First, we introduce an assumption on network width.

**Assumption 3.2** (Wide network).: Network width \(m\) satisfies

\[m>}\!(\!d^{2}\!\{_{n=1}^{N}( _{0}^{T_{f}}^{}(-y_{n}f(_{n};t))\,t+_{0}^{T _{g}}^{}(-y_{n}^{}f(_{n}^{};t))\,t )\}^{2}\!),\] (5)

where \(_{n}^{}:=_{n}\) for Scenario (a) and \(_{n}^{}:=_{n}^{}\) for Scenario (b) in Setting 3.1. In particular, \(m>}(d^{2}(T_{f}+T_{g})^{2})\) for \((s)=s\).

This assumption requires sufficiently large width \(m\) that regularizes the variations in parameters and forms the basis of lazy training (cf. Section 3.3). The width is always required to grow with the speed of the squared input dimension \(d^{2}\). The relationship between the width and two training times, \(T_{f}\) and \(T_{g}\), depends on the training set \(\{(_{n},y_{n})\}_{n=1}^{N}\) and loss function \(\). For example, if the training set is easily separable and the loss has an exponential tail, the derivative of the loss function might decrease rapidly with training time \(t\) and small \(m\) is enough to satisfy the assumption. For the identity loss, \(m\) is consistently required to satisfy \((d^{2}(T_{f}+T_{g})^{2})\). Note that the required values of \(T_{f}\) and \(T_{g}\) (and the corresponding \(m\)) for a desirable loss value remain an open question in the community. Our experimental results show that \(m 100\) is sufficient to verify our theorems for high-dimensional Gaussian distributions. Under this assumption, we consider the direction of the adversarial perturbation.

**Theorem 3.3** (Direction of adversarial perturbation).: _Let \(=(1)\) be a small positive number. Under Assumption 3.2, for any \(n[N]\), with probability at least \(1-\), the adversarial perturbation\(_{n}\) is parallel to the weighted sum of training samples as follows:_

\[_{n}\!_{k=1}^{N}y_{k}(_{n},_{k })_{k}_{0}^{T_{f}}^{}(-y_{k}f(_{k};t))\,t+ _{n},\] (6)

_where \(_{n}\) satisfies \(\|_{n}\|=}(1)\). In particular, for \((s)=s\),_

\[_{n}\!}{N}_{k}^{N}y_{k}(_{n}, {x}_{k})_{k}+_{n}.\] (7)

Note that the confidence level \(\) only logarithmically affects the norm of the remainder term \(_{n}\). This theorem indicates that the direction of a single perturbation can be represented as the weighted sum of \(y_{k}_{k}\) and remainder term \(_{n}\). Interestingly, this result suggests that a single perturbation derived from a classifier and sample can potentially contain information about the entire training dataset \(\{(_{n},y_{n})\}_{n=1}^{N}\). Particularly, in some cases (e.g., training samples are mutually orthogonal), \(y_{k}_{k}\) are not cancelled out by each other, and thus the single perturbation \(_{n}\) contains all training data and labels without loss of information.2 These results theoretically support the feature hypothesis. Consider the case with the identity loss. While the norm of the first term is \((T_{f})\), the norm of the remainder is constrained to \(}(1)\), suggesting that larger training time \(T_{f}\) and input dimension \(d\) strengthen the alignment between the perturbation and weighted sum.

Then, we consider the learning solely from these perturbations. The following theorem is a special case of Theorem D.17, which addresses a broader loss class and any sampling of \(y_{n}^{}\{ 1\}\).

**Theorem 3.4** (Perturbation learning, Scenario (a), special case of Theorem D.17).: _Consider Scenario (a) in Setting 3.1. Assume \((s)=s\) and \(y_{n}^{} U(\{ 1\})\) for every \(n[N]\). Let \(=(1)\) be a small positive number and_

\[():=_{n=1}^{N}y_{n}(_{n},) _{n},,_{a}():=}_{n=1}^{ N}(_{n},)_{k=1}^{N}y_{k}(_{n},_{k}) _{k},.\] (8)

_Under Assumption 3.2, for any \(^{d}\), if_

\[ |()|>}1+} ,\] (9) \[ |_{a}()|>}} +}{}}+} ,\] (10) \[ (())=(_{a }()),\] (11)

_then, with probability at least \(1-\), \((f(;T_{f}))=(g(;T_{g}))\) holds._

Note that the confidence level \(\) only logarithmically affects the right terms of Ineqs. (9) and (10), which is why these terms appear independent of \(\). This theorem states that the predictions of a classifier \(g\) trained solely on adversarial perturbations \(\{(_{n},y_{n}^{})\}_{n=1}^{N}\) coincide with those of a classifier \(f\) trained on standard training samples \(\{(_{n},y_{n})\}_{n=1}^{N}\) if the three conditions hold. The two

Figure 2: The regions where Ineqs. (9) and (10) and Eq. (11) hold (colored areas) and their intersection.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

Assuming \(_{i=1}^{m}_{i}^{}(_{i}(0),) {v}_{i}(0),=}(1) 0\) for simplicity, the network prediction \(f(;(T_{f}))\) trained on \(\{(_{n},y_{n})\}_{n=1}^{N}\) can be represented as follows:

\[f(;(T_{f}))= _{i=1}^{m}_{i}^{}(_{i}(0),)_{i}(T_{f}),}{N}_{n=1 }^{N}y_{n}(_{n},)_{n},,\] (22)

and \((f(;(T_{f})))=(_{n=1}^{N}y_{n}( _{n},)_{n},)\). In addition, \(g(;(T_{f}))\) trained on \(\{(_{n},y_{n}^{})\}_{n=1}^{N}\) can be represented as follows:

\[g(;(T_{g}))}{N^{2}}_{n=1}^{N}(_{n},)_{k=1}^{N}y_{k}(_{n}, _{k})_{k},,\] (23)

and \((g(;(T_{g})))=(_{n=1}^{N}(_ {n},)_{k=1}^{N}y_{k}(_{n},_{k})_{k}, {z})\). Thus, if the agreement condition Eq.11 holds, then \((f(;(T_{f})))=(g(;(T_{g})))\).

**Formal proof.** In the above sketch of proof, we have introduced several approximations. Rigorous evaluations are provided in Appendix D. For example, in the sketch, we assumed \(m\), ensuring that the signs of all hidden layer outputs remain unchanged. In contrast, the formal proof derives a bound on the width \(m\) that ensures that the number of hidden neurons with flipped signs is at most \(()\), which makes the discussion (e.g., about Eqs.18 and 19) more complicated. Moreover, in Eq.21, we neglected the first term of Eq.19, but the formal proof carefully considers the impact on the subsequent steps. The functional margin conditions arise from the evaluation of these remainder terms.

### Comparison with Prior Work and Limitations

In this section, we compare our results with  and discuss the limitations of our work. In summary, our results justify the feature hypothesis and perturbation learning under substantially milder conditions than , except for network width (cf. Tab.1). The assumption of wide two-layer networks is our main limitation.

**Goals, results, and tools.** The goals of our work and  are the same: justifying the feature hypothesis and perturbation learning. The conclusions drawn are also equivalent. However, our assumptions are much milder than theirs. This is due to the differences in the analytical approaches. While they leverage research on feature learning [19; 24; 31], we utilize the concept of lazy training [9; 30], which enables us to substantially relax the conditions.

**Data distribution.** Prior work imposes a strong assumption that training samples with/without adversarial perturbations are mutually orthogonal, i.e., \(_{n},_{k} 0\) and \(_{n}^{},_{k}^{} 0\) for any \(n k\). This condition is stringent and is hard to hold for real-world datasets. Moreover, it may not even hold for data sampled from a zero-mean Gaussian distribution in some common situations (e.g., the sample size is sufficiently larger than the dimension). We do not impose any assumptions on the data distribution. This is the first result that theoretically supports the feature hypothesis and the success of perturbation learning on realistic data distribution.

**Perturbation design.** Prior work defined the perturbation form using the decision boundary of a classifier. However, this is not only uncommon but also theoretically computable only in limited problem settings. Additionally, they constrained perturbation size to \(=()\), which becomes unrealistically small for a large sample size and is far from the practical constraint \(=()\). We employ a single-step gradient-based method , which is commonly used in practice, and the perturbation constraint can be set arbitrarily.

**Training time, loss function, network bias, and activation.** First, it should be noted that these constraints are critical for deriving the results in . This is because their theoretical framework [19; 24; 31] substantially requires the above conditions. We consider arbitrary training time, a wide class of loss functions, and (Leaky-) ReLU networks with bias availability. In contrast, they considered infinite training time, loss functions with exponential tails, homogeneous neural networks (thus requiring no bias), and Leaky-ReLU networks (the theorem becomes harder to hold as the negative slope of Leaky-ReLU approaches zero), which are essential for deriving their results (cf. the proofs of Theorem 4.4 in  and the proof of Theorem 3.2 in ).

**Limitations.** Compared to , our main limitation is the requirement for sufficient, though finite, network width. Moreover, our analysis is confined to two-layer networks, a common constraint in previous work. In practice, perturbation learning often employs deeper, and not necessarily wider, networks, which limits the direct applicability of our theoretical insights to more complex architectures. This assumption of a shallow network introduces another limitation. While deep neural networks typically capture high-level features from images and adversarial attacks are considered to exploit them, our framework focuses solely the low-level features (i.e., \(_{n}\) itself) in adversarial perturbations and their extraction through perturbation learning, as shown in Theorems 3.3 to 3.5. Relaxing the shallow network constraint may allow us to capture a broader set of features present in adversarial perturbations. Despite these limitations, our work is the first to rigorously support the feature hypothesis and validate perturbation learning under realistic data distributions, perturbation designs, and training settings, marking a substantial advancement in the theoretical understanding of adversarial examples.

## 4 Experiments

A comprehensive set of experiments conducted to validate our theorems can be found in Appendix B. In this section, we briefly present two results that confirm Theorem 3.4. As a training dataset \(:=\{(_{n},y_{n})\}_{n=1}^{N}\), we employed a synthetic training dataset to easily change the input dimension, which effectively helps perturbation learning in both scenarios, as predicted by our theorems. Note that the perturbation learning on real-world datasets can be found in the literature [22; 28]. We generated synthetic data and labels from the mean-shifted Gaussian distribution as follows: \(\{_{n}\}_{n=1}^{N}\) are independently sampled from \((0.3 y_{n} 1,)\), and \(y_{n}\) is set to one if \(n[N/2]\) and minus one otherwise. The experimental settings are as follows: \(d=100\), \(N=1,000\), \(m=100\), \(=0\), \((s):=s\), \(=0.01\), and the number of training steps is set to 1,000 for both \(f\) and \(g\). The experimental results for perturbation learning under Scenario (a) are shown in Fig. 3. A high input dimension facilitates the alignment between \(f\) and \(g\). Our theoretical results assume a wide network width, and Fig. 3 indicates that a sufficiently large width consistently stabilize the alignment.

## 5 Conclusion

We provided a theoretical justification for perturbation learning and the feature hypothesis. We demonstrated that adversarial perturbations contain class-specific features sufficient for networks to generalize from. Moreover, we revealed that the predictions of a classifier trained solely on these perturbations or mislabeled adversarial examples coincide with those of a classifier trained on correctly labeled training samples under three mild conditions. Except for wide two-layer networks, our assumption is substantially milder than prior work .