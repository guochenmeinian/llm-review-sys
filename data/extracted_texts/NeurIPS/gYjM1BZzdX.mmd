# Diffeomorphic interpolation for efficient persistence-based topological optimization

Mathieu Carriere

DataShape

Centre Inria d'Universite Cote d'Azur

Sophia Antipolis, France

mathieu.carriere@inria.fr

&Marc Theveneau1

Shape Analysis Group

Computer Science department, McGill

Montreal, Quebec, Canada

marc.theveneau@mail.mcgill.ca

&Theo Lacombe

Laboratoire d'Informatique Gaspard Monge,

Univ. Gustave Eiffel, CNRS, LIGM, F-77454

Marne-la-Vallee, France

theo.lacombe@univ-eiffel.fr

Part of this work was done when MT was doing an internship at the Laboratoire d'Informatique Gaspard Monge and student at Universite Paris-Saclay.

###### Abstract

Topological Data Analysis (TDA) provides a pipeline to extract quantitative topological descriptors from structured objects. This enables the definition of topological loss functions, which assert to what extent a given object exhibits some topological properties. These losses can then be used to perform topological optimization via gradient descent routines. While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice.

Focusing on the central case of topological optimization for point clouds, we propose in this work to overcome this limitation using _diffeomorphic interpolation_, turning sparse gradients into smooth vector fields defined on the whole space, with quantifiable Lipschitz constants. In particular, we show that our approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on a subsample can be used to update the coordinates of the full input object, allowing us to perform topological optimization on point clouds at an unprecedented scale. Finally, we also showcase the relevance of our approach for black-box autoencoder (AE) regularization, where we aim at enforcing topological priors on the latent spaces associated to fixed, pre-trained, black-box AE models, and where we show that learning a diffeomorphic flow can be done once and then re-applied to new data in linear time (while vanilla topological optimization has to be re-run from scratch). Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, yielding better interpretability of the model.

## 1 Introduction

Persistent homology (PH) is a central tool of Topological Data Analysis (TDA) that enables the extraction of quantitative topological information (such as, e.g., the number and sizes of loops, connected components, branches, cavities, etc) about structured objects (such as graphs, times series orpoint clouds sampled from, e.g., submanifolds), summarized in compact descriptors called _persistence diagrams_ (PDs). PDs were initially used as features in Machine Learning (ML) pipelines; due to their strong invariance and stability properties, they have been proved to be powerful descriptors in the context of classification of time series , graphs , images , shape registration , or analysis of neural networks , to name a few.

Another active line or research at the crossroad of TDA and ML is _(persistence-based) topological optimization_, where one wants to modify an object \(X\) so that it satisfies some topological constraints as reflected in its persistence diagram \((X)\). The first occurrence of this idea appears in , where one wants to deform a point cloud \(X^{n d}\) so that \((X)\) becomes as close as possible (w.r.t. an appropriate metric denoted by \(W\)) to some target diagram \(D_{}\), hence yielding to the problem of minimizing \(X W((X),D_{})\). This idea has then been revisited with different flavors, for instance by adding topology-based terms in standard losses in order to regularize ML models , improving ML model reconstructions by explicitly accounting for topological features , or improving correspondences between 3D shapes by forcing matched regions to have similar topology . Formally, this goes through the minimization of an objective function

\[L:X((X)),\]

where \(\) is a user-chosen loss function that quantifies to what extent \((X)\) reflects some prescribed topological properties inferred from \(X\). Under mild assumptions (see Section 2), the map \(L\) is differentiable generically and its gradients are obtained as a byproduct of the computation of \((X)\). However, these approaches are limited in practice by two major issues: \((i)\) the computation of \(X(X)\) scales poorly with the size of \(X\) (e.g., number of points \(n\) in a point cloud, number of nodes in a graph, etc), and \((ii)\) the gradient \( L(X)\) tends to be very _sparse_: if \(X=(x_{1},,x_{n})^{n d}\) is a point cloud, \( L(X)_{i} 0\) for only very few indices \(i\{1,,n\}\) (the corresponding points are called the _critical points_ of the topological gradient, see Section 2.1).

Related works.Several articles have studied topological optimization in the TDA literature. The standard, or _vanilla_, framework to define and study gradients obtained from topological losses was described in , where the high sparsity and long computation times were first identified. To mitigate this issue, the authors of  introduced the notion of _critical set_ that extends the usually sparse set of critical points in order to get a gradient-like object that would update more points in \(X\). In , the authors used an average of the vanilla topological gradients of several subsamples to get a denser and faster gradient. On the theoretical side, the authors of  demonstrated that adapting the stratified structure induced by PDs to the gradient definition enables faster convergence.

Limitations.Despite proposing interesting ways to accelerate gradient descent, the approaches mentioned above are still limited in the sense that their proposed gradients are not defined on the whole space, but only on a sparse subset of the current observation \(X\), which still prevents their use in different contexts, that we investigate in our experiments (Section 4). First, when the data has more than tens of thousands of points, the number of subsamples needed to capture relevant topological structures (when using ), as well as the critical set computations (when using ), both become _practically infeasible_. Second, when optimizing the topology of datasets obtained as _latent spaces_ of a _black-box autoencoder model_ (i.e., an autoencoder with forbidden access to its architecture, parameters, and training), then \((a)\) the topological gradients of _cannot_ be reused to process new such datasets, and topological optimization has to be performed from scratch every time that new data comes in, \((b)\) this also impedes their _transferability_, as re-running gradient descent every time makes it very difficult to guarantee some stability for the final output, and finally \((c)\) one _cannot_ generate new data by sampling the optimized latent spaces directly, as it would to apply the sequence of reverted gradients (which are not well-defined everywhere).

Contributions and Outline.In this article, we propose to replace the standard gradient \( L(X)\) of (2.1) derived formally by a _diffeomorphism_\(v:^{d}^{d}\) that _interpolates_\( L(X)\) on its non-zero entries, that is \(v(x_{i})= L(X)_{i}\) for all \(i I\{j\,|\, L(X)_{j} 0\}\) and is, in some sense, as smooth as possible. More precisely, our contribution is three-fold:

* We introduce a _diffeomorphic interpolation_ of the vanilla topological gradient, which extends this gradient to a smooth and denser vector field defined on the whole space \(^{d}\), and which is able to move a lot more points in \(X\) at each iteration,* We prove that its updates indeed decrease topological losses, and we quantify its smoothness by upper bounding its Lipschitz constant (again, in the context of topological losses),
* We showcase its practical efficiency: we show that it compares favorably to the main baseline  in terms of convergence speed, that its combination with subsampling  allows to process datasets whose sizes are currently out of reach in TDA, and that it can successfully be used for the tasks mentioned above concerning black-box autoencoder models.

Section 2 provides necessary background in Topological Data Analysis and diffeomorphic interpolations. Section 3 presents our approach and its corresponding guarantees, and Section 4 showcases our experiments. Limitations and further research directions are discussed in Section 5.

## 2 Background

### Topological Data Analysis

In this section, we recall the basic materials of Topological Data Analysis (TDA), and refer the interested reader to [20; 35] for a thorough overview. We restrict the presentation to our case of interest: extracting topological information from a point cloud using the standard _Vietoris-Rips_ (VR) filtration. A more extensive presentation of the TDA machinery is provided in Appendix A.

Let \(X=(x_{1},,x_{n})^{n d}\). The Vietoris-Rips filtration consists of building an increasing sequence of simplicial complexes \((K_{t})_{t 0}\) over \(X\) by inserting a simplex \(=(x_{i_{1}},,x_{i_{p}})\) whenever \( j,j^{}\{1,,p\}\), \(\|x_{i_{j}}-x_{i_{j^{}}}\| t\). Each time a simplex \(\) is inserted, it either creates a topological feature (e.g., inserting a face creates a cavity, that is a \(2\)-dimensional topological feature) or destroy a pre-existing feature (e.g., the face insertion fills a loop, that is a \(1\)-dimensional feature, making it topologically trivial). The persistent homology machinary tracks the apparition and destruction of such features in the so-called _persistence diagram_ (PD) of \(X\), denoted by \((X)\). Therefore, \((X)\) is a set of points in \(^{2}\) of the form \((t_{b},t_{d})\) with \(t_{d} t_{b}\), where each such point accounts for the presence of a topological feature inferred from \(X\) that appeared at time \(t_{b}\) following the insertion of an edge \((x_{i_{1}},x_{i_{2}})\) with \(\|x_{i_{1}}-x_{i_{2}}\|=t_{b}\) and disappeared at time \(t_{d}\) following the insertion of an edge \((x_{i_{3}},x_{i_{4}})\) with \(\|x_{i_{3}}-x_{i_{4}}\|=t_{d}\). Figure 1 illustrates this construction. From a computational standpoint, computing the VR diagram of \(X^{n d}\) that would reflect topological features of dimension \(d^{} d\) runs in \(O(n^{d^{}+2})\), making the computation quickly unpractical when \(n\) increases, even when restricting to low-dimensional features such as connected components (\(d^{}=0\)), loops (\(d^{}=1\)) or cavities (\(d^{}=2\)).

Topological optimization.PDs are made to be used in downstream pipelines, either as static features (e.g., for classification purposes) or as intermediate representations of \(X\) in optimization schemes. In this work, we focus on the second problem. We formally consider the minimization of objective functions of the form

\[L:X^{n d}((X)).\] (1)

Here, \(\) represents a loss function taking value from the space of PDs, denoted by \(\) in the following. The space \(\) can be equipped with a canonical metric, denoted by \(W\) and whose formal definition is not required in this work, for which a central result is that the map \(X(X)\) is stable (Lipschitz continuous) [17; 18; 38]. Therefore, if \(\) is Lipschitz continuous, so is \(L\), hence it admits a gradient almost everywhere by Rademacher theorem. Building on these theoretical statements, one

Figure 1: Illustration of the Vietoris-Rips filtration on a point cloud in \(^{d}\), focusing on one-dimensional topological features (loops). When the filtration parameter \(t\) increases, loops appear and disappear in the filtration. These values are accounted in the resulting persistence diagram (right).

can consider the "vanilla" gradient descent update \(X_{k+1}:=X_{k}- L(X_{k})\) for a given learning-rate \(>0\) and iterate it in order to minimize (1). Theoretical properties of this seminal scheme (and natural extensions, e.g., stochastic gradient descent) have been studied in [4; 27], where convergence (to a local minimum of \(L\)) is proved.

From a computational perspective, deriving \( L(X)\) comes in two steps. Let \((X)\), written as \(=\{(b_{i},d_{i})\,|\,i I\}\) for some finite set of indices \(I\). To each \(i I\) correspond four (possibly coinciding) points \(x_{i_{1}},x_{i_{2}},x_{i_{3}},x_{i_{4}}\) in the input point cloud \(X\). Intuitively, minimizing \(()\) boils down to prescribe a descent direction \(( b_{i}, d_{i})^{2}\) to each \((b_{i},d_{i})\) for \(i I\), where \( b_{i}=}()\) and \( d_{i}=}()\). Backpropagating this perturbation to \(X\) will move the corresponding points \(x_{i_{1}},x_{i_{2}},x_{i_{3}},x_{i_{4}}\) in order to increase or decrease the distances \(\|x_{i_{1}}-x_{i_{2}}\|=b_{i}\) and \(\|x_{i_{3}}-x_{i_{4}}\|=d_{i}\) accordingly. This yields to the following formula:

\[(X)=_{i,\ x(b_{i},d_{i})}[}}{ x}+}}{ x}](X),\] (2)

where the notation \(x(b_{i},d_{i})\) means that \(x X\) appears in (at least) one of the four points yielding the presence of \((b_{i},d_{i})\) in the diagram \(=(X)\). A fundamental contribution of [28; SS3.3] is to prove that the chain rule formula (2) is indeed valid2. Most of the time, a point \(x X\) will not belong to any critical pair \((_{b},_{d})\) and the above gradient coordinate is \(0\). Therefore, the gradient of \(L\) depends on very few points of \(X\), yielding the sparsity phenomenon discussed in Section 1.

Examples of common topological losses.Let \(X=(x_{1},,x_{n})^{n d}\) be a point cloud and \((X)=\{(b_{i},d_{i})\,|\,i I\}\) be its PD. There are several natural losses that have been introduced in the TDA literature:

* Topological simplification losses: typically of the form \(_{i}(b_{i}-d_{i})^{2}\), where \( I\). Such losses push (some of the) points in \((X)\) toward the diagonal \(=\{b=d\}\), hence destroying the corresponding topological features appearing in \(X\).
* Topological augmentation losses : similar to simplification losses, but typically attempting to push points in \((X)\) away from \(\), i.e., minimizing \(-_{i}(b_{i}-d_{i})^{2}\), to make topological features of \(X\) more salient. As such losses are not coercive, they are usually coupled with regularization terms to prevent points in \(X\) going to infinity.
* Topological registration losses : given a target diagram \(D_{}\), one minimizes \(W((X),D_{})\) where \(W\) denotes a standard metric between persistence diagrams. This loss attempts to modify \(X\) so that it exhibits a prescribed topological structure.

### Diffeomorphic interpolations

In order to overcome the sparsity of gradients appearing in TDA, we rely on diffeomorphic interpolations (see, e.g., [43, Chapter 8]). Let \(X=(x_{1},,x_{n})^{n d}\), let \(I\{1,,n\}\) denote the set of indices on which \( L(X)\) is non-zero and let \(a_{i}( L(X))_{i}^{d}\) for \(i I\). Our goal is to find a smooth vector field \(:^{d}^{d}\) such that, for all \(i I\), \((x_{i})=a_{i}\). To formalize this, we consider a Hilbert space \(H(^{d})^{^{d}}\) for which the map \(_{x}^{}:f,f(x)_{^{d}}= ^{T}f(x)\) is continuous for any \((,x)^{d}^{d}\). Such a space is called a Reproducing Kernel Hilbert Space (RKHS)3. A crucial property is that there exists a matrix-valued kernel operator \(K:^{d}^{d}^{d d}\) whose outputs are symmetric and positive definite, and related to \(H\) through the relation \( k_{x}^{},k_{y}^{}_{H}=^{T}K(x,y)\) for all \(x,y,,^{d}\), where \(k_{x}^{} H\) is the unique vector provided by the Riesz representation theorem such that \( k_{x}^{},f=,f(x)\). Conversely, any such kernel \(K\) induces a (unique) RKHS \(H\) (of which \(K\) is the reproducing kernel). Now, we can consider the following problem:

\[\ \|v\|_{H},\ \ v(x_{i})=a_{i},\  i I,\] (3)

that is, we are seeking for the smoothest (lowest norm) element of \(H\) that solves our interpolation problem. The solution \(\) of this problem is the projection of \(0\) onto the affine set \(\{v H\,|\,v(x_{i})=\ \|v\|_{H},\ \ v(x_{i})=a_{i}\}\).

\(a_{i}, i I\)). Observe that \(\) belongs to the orthogonal of \(\{v H\,|\,v(x_{i})=0, i I\}\), and thus of \(\{v H\,|\,<k^{_{i}}_{x_{i}},v>_{H}=0,\; i I, _{i}^{d}\}\), and therefore \((\{k^{_{i}}_{x_{i}}\,|\,i I\})\). This justifies to search for \(\) in the form of \((x)=_{i I}K(x,x_{i})_{i}\), and the interpolation that it must satisfy yields \((x)=_{i I}K(x,x_{i})(^{-1}a)_{i}\), where \(\) is the block matrix \((K(x_{i},x_{j}))_{i,j I}\) and \(a=(a_{i})_{i I}\). See also [43, Theorem 8.8]. In particular, it is important to note that \(\) inherits from the regularity of \(K\) and will typically be a diffeomorphism in this work. If \(K\) is the Gaussian kernel defined by \(K(x,y)(-}{2^{2}})I_{d}\) for some bandwidth \(>0\), a choice to which we stick to in the rest of this work, the expression of \(\) reduces to

\[(x)=_{i I}_{}(\|x-x_{i}\|)_{i},\] (4)

where \(_{}(u) e^{-}{2^{2}}}\), and \(_{i}(^{-1}a)_{i}\) with \(=(_{}(\|x_{i}-x_{j}\|)I_{d})_{i,j I}\). Note that \(\) can be understood as the convolution of \(a\) with a Gaussian kernel, but involving a correction \(^{-1}\) guaranteeing that after the convolution, the interpolation constraint is satisfied. We will call \(\) the _diffeomorphic interpolation_ associated to the vectors and indices \(\{a_{i}\,|\,i I\}\).

## 3 Diffeomorphic interpolation of the vanilla topological gradient

### Methodology

We aim at minimizing a loss function \(L:X((X))\) as in (1), starting from some initialization \(X_{0}\), and assuming that \(L\) is lower bounded (typically by \(0\)) and locally semi-convex. This assumption is typically satisfied by the topological losses \(\) introduced in Section 2.1. Gradient descents implemented in practice are (explicit) discretization of the _gradient flow_

\[X}{t}- L(X(t)), X(0)=X_{0},\] (5)

where \( L(X)\{v\,|\,L(Y) L(X)+v(Y-X)+o(Y-X)X,Y\}\) denotes the subdifferential of \(L\) at \(X\). Note that a topological loss \(L\) is typically _not_ differentiable everywhere, since the map \(X(X)\) is differentiable almost everywhere but not in \(C^{1,1}\). However, uniqueness of the gradient flow on a maximal interval \([0,+[\) is guaranteed if \(L\) is lower bounded and locally semi-convex [15, SSB.1].

In this work, we propose to use the dynamic described by the diffeomorphism \(_{t}\) introduced in (4) interpolating the current vanilla topological gradient \( L(X_{t})\) at each time \(t\), formally considering solutions \(\) of

\[}{t}=-_{t}((t)), (0)=X_{0}.\] (6)

Here, slightly overloading notation, \(_{t}((t))\) denotes the \(n d\) matrix where the \(i\)-th line is given by \(_{t}((t)_{i})\). The _flow_ at time \(T\) associated to (6) is the map

\[_{T}:x_{0} x_{0}-_{0}^{T}_{t}(x(t))t, (t)=-_{t}(x(t)),\;x(0)=x_{0},\] (7)

which can inverted by simply following the flow backward (i.e., by following \(_{t}\) instead of \(-_{t}\)). We now guarantee that at each time \(t\), following \(_{t}\) instead of the vanilla topological gradient \( L(X_{t})\) still provides a descent direction for the topological loss \(L\).

**Proposition 3.1**.: _For each \(t 0\), it holds that \(L((t))}{t}=-\| L((t))\|^{2}  0\)._

Proof.: One has \(L((t))}{t}=-< L((t)), _{t}((t))>=-_{i=1}^{n}( L((t)))_{i} (_{t}((t)))_{i}\). Since \( L((t))_{i}=0\) for \(i I\), and \(_{t}((t))_{i}=- L((t))_{i}\) for \(i I\), the result follows.

Figure 2: (blue) A point cloud \(X\), and (black) the negative gradient \(- L(X)\) of a simplification loss which aims at destroying the loop by collapsing the circle (reduce the loop’s death time) and tearing it (increase the birth time). While \( L(X)\) only affects four points in \(X\), the diffeomorphic interpolation \((X)\) (orange, \(=0.1\)) is defined on \(^{d}\), hence extends smoothly to other points in \(X\).

Moreover, it is also possible to upper bound the smoothness, i.e., the Lipschitz constant, of \(\):

**Proposition 3.2**.: _Let \(L\) be the simplification or augmentation loss computed with \(k=||\) PD points, as defined at the end of Section 2.1. Let \(=_{t}\) be the diffeomorphic interpolation associated to the vanilla topological gradient at time \(t 0\). Then, one has, \( x,y^{d}\) and \(t 0\):_

\[\|(x)-(y)\|_{2}\|(x)-(y)\|_{1} C_ {d}^{d-1}()_{k}( ((t)))\|x-y\|_{2},\]

_where \(C_{d}= 2^{3+}^{}\), \(()\) is the condition number of \(\), and \(_{k}(((t)))\) is the sum of the \(k\) largest distances to the diagonal in \(((t))\)._

The proof is deferred to Appendix B. This upper bound can be used to quantify how smooth the diffeomorphic interpolation is (as characterized with its Lipschitz constant) based on the parameters it is computed from. In the case of the Gaussian kernel, we found that our upper bound on the Lipschitz constant depends on the kernel bandwidth \(\): indeed, the more spread the Gaussian function is, the more critical points can influence other data points potentially far from them, introducing unwanted distortions and larger Lipschitz constant. Similarly, if the condition number \(()\) is large, inverting the kernel matrix might introduce instabilities in Equation (4), and thus a larger Lipschitz constant as well. Finally, the total persistence also appears, as the more PD points one has to optimize, the more critical pairs will appear, and thus the more constrained Equation (3) is, leading to more complex diffeomorphic interpolation solutions with larger Lipschitz constants.

### Subsampling techniques to scale topological optimization

As a consequence of the limited scaling of the Vietoris-Rips filtration with respect to the number of points \(n\) of the input point cloud \(X\), it often happens in practical applications that computing the VR diagram \((X)\) of a large point set \(X\) (_a fortiori_ its gradient) turns out to be intractable. A natural workaround is to randomly sample \(s\)-points from \(X\), with \(s n\), yielding a smaller point cloud \(X^{} X\). Provided that the Hausdorff distance between \(X^{}\) and \(X\) is small, the stability theorem [18; 17; 8] ensures that \((X^{})\) is close to \((X)\). See [11; 12; 9] for an overview of subsampling methods in TDA.

However, the sparsity of vanilla topological gradients computed from topological losses strikes further when relying on subsampling: only a tiny fraction of the seminal point cloud \(X\) is likely to be updated at each gradient step. In contrast, using the diffeomorphic interpolation \(\) (of the vanilla topological gradient) computed on the subsample \(X^{}\) still provides a vector field defined on the whole input space \(^{d}\), in particular on each point of \(X\) and the update can then be performed in linear time with respect to \(n\). This yields Algorithm 1. Figure 3 illustrates the qualitative benefits offered by the joint use of subsampling and diffeomorphic interpolations when compared to vanilla topological gradients. A larger-scale experiment is provided in Section 4.

``` Input: Initial \(X_{0}^{n d}\), loss function \(\), learning rate \(>0\), subsampling size \(s\{1,,n\}\), max. epoch \(T 1\), stopping criterion.  Set \(L:X((X))\) (+ possibly a regularization term in \(X\)). for\(k=1,,T\)do  Subsample \(X^{}_{k-1}=\{x^{}_{1},,x^{}_{s}\}\) uniformly from \(X_{k-1}\).  Compute \( L(X^{}_{k-1})\) (vanilla topological gradient)  Compute the diffeomorphic interpolation \((X^{}_{k-1})\) from \( L(X^{}_{k-1})\) using (4).  Set \(X_{k}:=X_{k-1}-(X_{k-1})\). if stopping criterion is reached then  Return\(X_{k}\) endif endfor Return\(X_{T}\) ```

**Algorithm 1** Diffeomorphic gradient descent for topological loss functions with subsampling

Stopping criterion.A natural stopping criterion for Algorithm 1 is to assess whether the loss \(L(X_{t})=((X_{t}))\) is smaller than some \(>0\). However, computing \((X_{t})\) can be intractable if \(X_{t}\) is large. Therefore, a tractable loss to consider is \((X_{t})[((X^{}_{t}))]\), where \(X^{}_{t}\) is a uniform \(s\)-sample from \(X_{t}\). Under that perspective, Algorithm 1 can be re-interpreted as a kind of stochastic gradient descent on \(\), for which two standard stopping criteria can be used: \((a)\) compute an exponential moving average of the loss on individual samples \(X^{}_{t}\) over iterations, or \((b)\) compute a validation loss, i.e., sample \(X^{}_{t,(1)},,X^{}_{t,(K)}\) and estimate \(\) by \(K^{-1}_{k=1}^{K}((X^{}_{t,(k)}))\). Empirically, we observe that the latter approach with \(K=n/s\) (more repetitions for smaller sample sizes to mitigate variance) yields the most satisfactory results (faster convergence toward a better objective \(X_{t}\)) overall, and thus stick to this choice in our experiments.

## 4 Numerical experiments

We provide numerical evidence for the strength of our diffeomorphic interpolations. PH-related computations relies on the library Gudhi and automatic differentiation relies on tensorflow. The "big-step gradient" baseline  implementation is based on oineus4. The first two experiments were run on a 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz, the last one on a 2x Xeon SP Gold 5115 @ 2.40GHz. Our code is publicly available at https://github.com/tlacombe/topt.

Convergence speed and running times.We sample uniformly \(n=200\) points on a unit circle in \(^{2}\) with some additional Gaussian noise, and then minimize the simplification loss \(L:X_{(b,d)(X)}|d|^{2}\), which attempts to destroy the underlying topology in \(X\) by reducing the death times of the loops by collapsing the points. The respective gradient descents are iterated over a maximum of 250 epochs, possibly interrupted before if a loss of \(0\) is reached (\((X)\) is empty), with a same learning rate \(=0.1\). The bandwidth of the Gaussian kernel in (4) is set to \(=0.1\). We include the competitor oineus, as--even though relying on a fairly different construction--this method shares a key idea with ours: extending the vanilla gradient to move more points in \(X\). We stress that both approaches can be used in complementarity: compute first the "big-step gradient" of  using oineus, and then extend it by diffeomorphic interpolation. Results are displayed in Figure 45. In terms of loss decrease over _iterations_, both "big-step gradients" and our diffeomorphic interpolations significantly outperform vanilla topological gradients, and their combined use yields the fastest convergence (by a slight margin over our diffeomorphic interpolations alone). However, in terms of raw running times, the use of oineus involves a significant computational overhead, making our approach the fastest to reach convergence by a significant margin.

Subsampling.We now showcase how using our diffeomorphic interpolation jointly with subsampling routines (Algorithm 1) allows to perform topological optimization on point clouds with thousands of points, a new scale in the field. For this, we consider the vertices of the Stanford Bunny, yielding a point cloud \(X_{0}^{n d}\) with \(n=35,947\) and \(d=3\). We consider a topology

Figure 3: Showcase of the usefulness of subsampling combined with diffeomorphic interpolations to minimize a topological simplification loss, with parameters \(=0.1\), \(s=50\), \(n=500\). \((a)\) Initial point cloud \(X\) (blue), subsample \(X^{}\) (red), vanilla topological gradient on the subsample (black) and corresponding diffeomorphic interpolation (orange). \((b)\) and \((c)\), the point cloud \(X_{t}\) after running \(t=100\) and \(t=500\) steps of vanilla gradient descent. \((d)\) the point cloud \(X_{t}\) after running \(t=100\) steps of diffeomorphic gradient descent.

ical augmentation loss (see Section 2.1) for two-dimensional topological features, i.e., we aim at increasing the persistence of the bunny's cavity. The size of \(n\) makes the computation of \((X_{0})\) untractable (recall that it scales in \(O(n^{4})\)); we thus rely on subsampling with sample size \(s=100\) and compare the vanilla gradient descent scheme and our scheme described in Algorithm 1. Results are displayed in Figure 5. Because it only updates a tiny fraction of the initial point cloud at each iteration, the vanilla topological gradient with subsampling barely changes the point cloud (nor decreases the loss) in 1,000 epochs. In sharp contrast, as our diffeomorphic interpolation computed on subsamples is defined on \(^{3}\), it updates the whole point cloud at each iteration, making possible to decrease the objective function where the vanilla gradient descent is completely stuck. Note that a step of diffeomorphic interpolation, in that case, takes about \(10\) times longer than a vanilla step. An additional subsampling experiment can be found in Appendix C.

Black-box autoencoder models.Next, we apply our diffeomorphic interpolations to black-box autoencoder models. In their simplest formulation, autoencoders (AE) can be summarized as two maps \(E:^{d}^{d^{}}\) and \(D:^{d^{}}^{d}\) called encoder and decoder respectively. The intermediate space \(^{d^{}}\) in which the encoder \(E\) is valued is referred to as a _latent space_ (LS), with typically \(d^{} d\). In general, without further care, there is no reason to expect that the LS of a point cloud \(X\), \(E(X)=\{E(x_{1}),,E(x_{n})\}\), reflects any geometric or topological properties of \(X\). While this can be mitigated by adding a topological regularization term to the loss function _during the training_ of the autoencoder [29; 4], this _cannot_ work in the setting where one is given a _black-box, pre-trained_ AE. However, replacing \((E,D)\) by \(( E,D^{-1})\) for any invertible map \(:^{d^{}}^{d^{}}\) yields an AE producing the same outputs yet changing the LS \(E(X)\), without explicit access to the AE's model. Hence, we propose to learn such a \(\) with diffeomorphic interpolations: given some latent space \(E(X)\), we apply \(T\) steps of our diffeomorphic gradient descent algorithm to \(X((X))\) initialized at \(E(X)\). We thus get a sequence of smooth displacements \(-_{1},,-_{T}\) of \(^{d^{}}\) that discretizes the flow (7) via \(:x_{0} x_{0}-_{k=1}^{T}_{k}(x_{k-1})\) where \(x_{k}-x_{k-1}=-_{k}(x_{k-1})\), and such that \(((E(X)))\) is more topologically satisfying. This fixed diffeomorphism \(\) can

Figure 4: (Top) From left to right: initial point cloud, and final point cloud for the different flows. (Bottom) Evolution of the losses with respect to the number of iterations and with respect to running time.

Figure 5: From left to right: initial Stanford bunny \(X_{0}\), the point cloud after \(1,000\) epochs of vanilla topological gradient descent (barely any changes), the point cloud after 200 epochs of diffeomorphic gradient descent, after 1,000 epochs, and eventually the evolution of losses for both methods over iterations.

then be re-applied to any new data coming out of the encoder in a deterministic way. Moreover, any random sample from the topologically-optimized LS can be inverted without further computations by following \(_{T},_{T-1},,_{1}\), which allows to push the new sample back to the initial LS, and then apply the decoder on it. Again, this cannot be achieved with baselines [39; 34].

In order to illustrate these properties, we trained a variational autoencoder (VAE) to project a family of datasets of images representing rotating objects, named COIL, to two-dimensional latent spaces. Given that every dataset in this family is comprised of \(288\) pictures of the same object taken with different angles, one can impose a prior on the topology of the corresponding LSs, namely that they are sampled from circles. However, the VAE architecture is shallow (the encoder has one fully-connected layer (100 neurons), and the decoder has two (50 and 100 neurons), all layers use ReLu activations), and thus the learned latent spaces, although still looking like curves thanks to continuity, do not necessarily display circular patterns. This makes generating new data more difficult, as latent spaces are harder to interpret. To improve on this, we learn a flow \(\) as described above with an augmentation loss associated to the 1-dimensional PD point which is the most far away from the diagonal, in order to force latent spaces to have a significant one-dimensional topological feature, i.e., loop. As the datasets are small, we do not use subsampling, and we use learning rate \(=0.1\), Gaussian kernels with bandwidth \(=0.3\) and an increase of at least \(3\). in the topological loss (from an iteration to the next) to stop the algorithm6.

We provide some qualitative results in Figure 7 (see also Appendix C, Figure 10). In order to quantify the improvement, we also computed the Pearson correlation scores between the ground-truth angles \(_{i}\) and the angles \(_{i}\) computed from the topologically-optimized LSs with

\[_{i}:=( E(x_{i})-}[ E (X)], E(x_{1})-}[ E(X)]),\]

where \(}[ E(X)]:=n^{-1}_{i=1}^{n} E(x_{i})\), and \(\) denotes our flow. In Table 1, we provide an average of these scores over \(100\) test sets obtained by randomly perturbing the training set with uniform noise of amplitude \(0.05\). As expected, correlation becomes better after forcing the latent spaces to have the topology of a circle. This better interpretability is also illustrated in Figure 6, in which four angles are specified, which are mapped to the topologically-optimized LS, then pushed to the initial LS of the black-box VAE by following the reverted flow of our learned diffeomorphism \(\), and finally decoded back into realistic, COIL-like images.

Single-cell data.We also deployed our proposed topological correction of LSs from AEs on a real-world dataset of single cells. Specifically, we designed an experiment on single cell HiC (scHiC) data inspired from . The dataset is comprised of single cells characterized by chromatin folding, that is, each cell is encoded by the spatial distance matrix of its DNA fragments. The dataset we focus on is taken from , in which it was shown that cells are sampled at different phases of the cell cycle. Thus, similar to COIL images, we expect latent embeddings of this dataset to exhibit a circular shape, that we can constrain with diffeomorphic topological optimization.

Figure 6: As the four samples from the topologically-optimized LS (blue) are far from the initial LS (orange), the decoded images are fuzzy. However, reverting \(\) and following the corresponding green trajectories allows to render good-looking images.

Figure 7: COIL images, their corresponding initial LSs in blue and final LSs obtained with diffeomorphic gradient descent in orange, and the corresponding topological losses, for both vase (left) and duck (right).

Specifically, we processed this single cell dataset of \(1,171\) cells with the stratum-adjusted correlation coefficient (SCC) with \(500\)kb and convolution parameter \(h=1\) on chromosome \(10\). Then, we run kernel PCA on the SCC matrix to obtain a preprocessed dataset in \(^{100}\), on which we applied the same VAE architecture than the one described above for COIL images. Finally, we optimized two losses, the first was the same augmentation loss than for the COIL images, the second was the following registration loss:

\[L:X^{n d} W^{2}(^{1}(X),D_{}),\]

where \((X)\) contains the points of the PD of \(X\) with distance-to-diagonal at least \(=1\), \(D_{}\) is a target PD with only one point \(p^{*}=[-3.5,3.5]\), and \(W\) is the 2-Wasserstein distance between PDs. We used \(=0.2\) for the augmentation loss and \(=0.025\) for the registration loss (\(\) is set to a smaller value for the registration loss in order to mitigate the effects of matching instability), \(=0.1\) on \(500\) epochs, with subsampling of size \(s=300\) for computational efficiency (as computing VR diagrams without radius thresholding on \(1,171\) points already takes few minutes on a laptop CPU, which becomes hardly tractable if done repetitively as in gradient descent), and loss increase of \(3\). as a stopping criterion. Qualitative results are displayed in Appendix C, Figure 11, and we also measured quantitative improvement with the correlation scores between latent space angles and repli scores7 in Table 1, in which improvements can be observed. An additional experiment on the influence of the bandwidth parameter \(\) over these correlation scores, as well as over convergence, can also be found in Appendix C.

## 5 Conclusion

In this article, we have presented a way to turn sparse topological gradients into dense diffeomorphisms with quantifiable Lipschitz constants, and showcased practical benefits of this approach in terms of convergence speed, scaling, and applications to black-box AE models on several datasets. Several questions are still open for future work.

In terms of theoretical results, we plan on working on the stability between the diffeomorphic interpolations computed on a dataset and its subsamples. This requires some control over the locations of the critical points, which we expect to be possible in _statistical estimation_; indeed sublevel sets of density functions are know to have stable critical points [10, Lemma 17]. We also plan to look at _adaptive kernels_ whose parameters (like the bandwidth \(\)) depend on the input point cloud \(=(X)\) (instead of using a fixed kernel and parameters at every iteration), and understand the convergence properties of our proposed diffeomorphic gradient descent. Finally, applying our diffeomorphic interpolation to sparse gradients computed with _multiparameter persistent homology_ is another natural research direction, provided that differentiability properties have recently been proved in that setting .

Concerning the AE experiment, we plan to investigate the limitations presented in the figure below: as the initial LSs (colored with the ground-truth angles) have zero (left) or two (right) loops, it is impossible to unfold them with diffeomorphisms; instead the optimized latent spaces either also exhibit no topology, or mixes different angles. In future work, we plan on investigating other losses or gradient descent schemes for diffeomorphic topological optimization, including stratified procedures similar to  that allow for local topological changes during training.

  Dataset & Duck & Cat & Pig & Vase & Teapot \\  No optim. & 0.56 \(\) 7.5e-04 & 0.78 \(\) 1.4e-03 & 0.17 \(\) 5.7e-04 & 0.86 \(\) 7.2e-03 & 0.32 \(\) 1.6e-03 \\ Diffeo & **0.61 \(\) 3.1e-03** & **0.83 \(\) 1.2e-03** & **0.76 \(\) 2.1e-04** & **0.93 \(\) 9.8e-04** & **0.39 \(\) 3.4e-03** \\   
  Dataset & scHiC (augmentation) & scHiC (registration) & \\   & No optim. & 0.79 \(\) 8.1e-03 & 0.792 \(\) 8.1e-03 & \\ Diffeo & **0.84 \(\) 4.3e-03** & **0.794 \(\) 8.4e-03** & \\  

Table 1: Means and variances of correlation scores computed over 100 test sets, for both COIL and scHiC.