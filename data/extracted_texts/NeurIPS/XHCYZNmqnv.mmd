# Detecting Brittle Decisions for Free: Leveraging

Margin Consistency in Deep Robust Classifiers

 Jonas Ngnaw

IID-Universite Laval and Mila

jonas.ngnawe.10ulaval.ca &Sabyasachi Sahoo

IID-Universite Laval and Mila

sabyasachi.sahoo.1@ulaval.ca &Yann Pequignot

IID-Universite Laval

yann.pequignot@iid.ulaval.ca &Frederic Precioso

Universite Cote d'Azur, CNRS, INRIA, I3S, Maasai

frederic.precioso@univ-cotedazur.fr &Christian Gagne

IID-Universite Laval, Mila and Canada CIFAR AI Chair

christian.gagne@gel.ulaval.ca

###### Abstract

Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency - a property that links the input space margins and the logit margins in robust models - for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively and confidently use the logit margin to detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to assess adversarial vulnerability in deployment scenarios efficiently.

## 1 Introduction

Deep neural networks are known to be vulnerable to adversarial perturbations, visually insignificant changes in the input resulting in the so-called adversarial examples that alter the model's prediction (Biggio et al., 2013; Goodfellow et al., 2015). They constitute actual threats in real-world scenarios (Evtimov et al., 2017; Gnanasambandam et al., 2021), jeopardizing their deployment in sensitive and safety-critical systems such as autonomous driving, aeronautics, and health care. Research in the field has been intense and produced various adversarial training strategies to defend against the vulnerability to adversarial perturbations with bounded \(_{p}\) norm (e.g., \(p=2\), \(p=\)) throughaugmentation, regularization, and detection (Xu et al., 2017; Madry et al., 2018; Zhang et al., 2019; Carmon et al., 2019; Wang et al., 2020; Wu et al., 2020; Rice et al., 2020), to cite a few. The empirical robustness (adversarial accuracy) of these adversarially trained models is still far behind their high performance in terms of accuracy. It is typically estimated by assessing the vulnerability of samples of a given test set using adversarial attacks (Carlini and Wagner, 2016; Madry et al., 2018) or an ensemble of attacks such as the standard _AutoAttack_(Croce and Hein, 2020). The objective of that evaluation is to determine if, for a given normal sample, an adversarial instance exists within a given \(\)-ball around it. Yet, this robustness evaluation over a specific test set provides a global property of the model but not a local property specific to a single instance (Seshia et al., 2018; Dreossi et al., 2019). Beyond that specific test set, obtaining this information for each new sample would typically involve rerunning adversarial attacks or performing a formal robustness verification, which in certain contexts may be computationally prohibitive in terms of resources and time. Indeed, the computational cost makes it prohibitive to estimate robust accuracy at scale on large test sets and/or large models, for example, when using _AutoAttack_ in standard mode. Moreover, in high-stakes deployment scenarios, knowing the vulnerability of single instances in real-time (i.e., their susceptibility to adversarial attacks) would be valuable, for example, to reduce risk, prioritize resources, or monitor operations. Therefore, there is a need for efficient and scalable ways to determine the vulnerability of a model's decision on a given sample.

The input space margin (i.e., the distance of the sample to the model's decision boundary in the input space), or input margin in short, can be used as a score to determine whether the sample is non-robust and, as such, likely to be vulnerable to adversarial attacks. Computing the exact input margin is intractable for deep neural networks (Katz et al., 2017; Elsayed et al., 2018; Jordan and Dimakis, 2020). These input margins may not be meaningful for fragile models with zero adversarial accuracies as all samples are vulnerable (close to the decision boundary). However, for robustly trained models, where only certain instances are vulnerable, the input margin is very useful for identifying the critical samples. Previous research studies have explored input margins of deep neural networks during training, focusing on their temporal evolution (Mickisch et al., 2020; Xu et al., 2023), and their exploitation in improving adversarial robustness through instance-reweighting with approximations (Zhang et al., 2020; Liu et al., 2021) and margin maximization (Elsayed et al., 2018; Ding et al., 2020; Xu et al., 2023). However, to the best of our knowledge, no previous research studies the relationship between the input space margin and the logit margin of robustly trained deep classifiers in the context of vulnerability detection.

In this paper, we investigate how the deep representation of robust models can provide information about the vulnerability of any single sample to adversarial attacks. We specifically address whether the logit margin as an approximation of the distance to the decision boundary in the feature space of the deep neural network (penultimate layer) can reliably serve as a proxy of the input margin for vulnerability detection. When this holds, we will refer to the model as being _margin-consistent_. The margin consistency property implies that the model can directly identify instances where its robustness may be compromised simply from a simple forward pass using the logit margin. Fig. 1 illustrates this idea of margin consistency. The following contributions are presented in the paper:

* We introduce the notion of _margin consistency_1, a property to characterize robust models that allow the use of their logit margin as a proxy estimation for the input space margin in the context of non-robust sample detection. We prove that margin consistency is a necessary and sufficient condition to reliably use the logit margin for detecting non-robust samples. * Through an extensive empirical investigation of pre-trained models on CIFAR10 and CIFAR100 with various adversarial training strategies, mainly taken from _RobustBench_(Croce et al., 2021), we provide evidence that almost all the investigated models display high margin consistency, i.e., there is a strong correlation between the input margin and the logit margin.
* We confirm experimentally that models with high margin consistency perform well in detecting samples vulnerable to adversarial attacks based on their logit margin. In contrast, models with weaker margin consistency exhibit poorer performance.
* For models where margin consistency does not hold, exhibiting a weak correlation between the input margins and the logit margins, we simulate margin consistency by learning to map the model's feature representation to a pseudo-margin with a better correlation through a simple learning scheme.

## 2 Methodology

### Notation and Preliminaries

NotationWe consider \(f_{}:^{n}^{K}\) a deep neural network classifier with weights \(\) trained on a dataset of samples drawn iid from a distribution \(\) on a product space \(\). Each sample \(\) in the input space \(^{n}\) has a unique corresponding label \(y=\{1,2,,K\}\). The prediction of \(\) is given by \(()=_{k}f_{}^{k}()\), where \(f_{}^{k}()\) is the \(k\)-th component of \(f_{}()\). We consider that a deep neural network is composed of a feature extractor \(h_{}:^{m}\) and a linear head with \(K\) linear classifiers \(\{_{},b_{k}\}\) such that \(f_{}^{k}()=_{} h_{}()+b_ {k}\). The predictive distribution \(p_{}(y|)\) is obtained by taking the softmax of the output \(f_{}()\). A perturbed sample \(^{}\) can be obtained by adding a perturbation \(\) to \(\) within an \(\)-ball \(B_{p}(,)\), an \(_{p}\)-norm ball of radius \(>0\) centered at \(\), \(\{^{}:\|^{}-\|_{p}=\|\|_{p}<\}\). The distance \(\|^{}-\|_{p}=\|\|_{p}\) represents the perturbation size defined as \((_{i=1}^{n}|_{i}|^{p})^{}\). In this paper, we will focus on \(_{}\) norm (\(\|\|_{}=_{i=1,,n}|x_{i}|\)), which is the most commonly used norm in the literature.

Local robustnessDifferent notions of **local robustness** exist in the literature (Gourdeau et al., 2021; Zhong et al., 2021; Han et al., 2023). In this paper, we equate local robustness to \(}\)**-robustness**, a standard notion corresponding to the invariance of the decision within the \(_{p}\)\(\)-ball around the sample (Bastani et al., 2016; Fawzi et al., 2018) and formalized in terms of \(\)-robustness.

**Definition 1**.: _A model \(f\) is **\(\)-robust** at point \(\) if for any \(^{} B_{p}(,)\) (\(^{}\) in the \(\)-ball around \(\)), we have \((^{})=()\)._

For a given robustness threshold \(\), a data instance is said to be non-robust for the model if this model is not \(\)-robust on it. This means it is possible to construct an adversarial sample from that instance in its vicinity (i.e., within an \(\)-ball distance from the original instance). A vulnerable sample to adversarial attacks is necessarily non-robust. This notion of local robustness can be quantified in the worst-case or, on average, inside the \(\)-ball. We focus here on the worst-case measurement given by the input margin, also referred to as the _minimum distortion_ or the _robust radius_(Szegedy et al., 2014; Carlini and Wagner, 2016; Weng, 2019)

**The input space margin** is the distance to the decision boundary of \(f\) in the input space. It is the norm of a minimal perturbation required to change the model's decision at a test point \(\):

\[d_{in}()=\{\|\|_{p}:^{n}()(+)\}=\{:f\}.\] (1)

An instance \(\) is non-robust for a robustness threshold \(\) if \(d_{in}()\). Evaluating Eq. 1 for deep networks is known to be intractable in the general case. An upper bound approximation can be obtained using a point \(^{}_{0}\), the closest adversarial counterpart of \(\) in \(_{p}\) norm by \(_{in}()=\|-^{}_{0}\|_{p}\) (see Fig. 1).

Figure 1: Illustration of the input space margin, margin in the feature space and margin consistency. The model preserves the relative position of samples to the decision boundary in the input space to the feature space.

**The logit margin** is the difference between the two largest logits. For a sample \(\) classified as \(i=()=*{arg\,max}_{j}f_{}^{j}( )\) the logit margin is defined as \((f_{}^{i}()-_{j,j i}f_{}^{j}() )>0\). It is an approximation of the distance to the decision boundary of \(f_{}\) in the feature space. The decision boundary in the feature space around \(=h_{}()\), the feature representation of \(\), is composed of \((K-1)\) linear decision boundaries (hyperplanes) \(_{ij}=\{^{}^{m}:_{i}^{} ^{}+b_{i}=_{j}^{}^{}+b_{j}\}\) (\(j i\)). The margin in the feature space is therefore the distance to the closest hyperplane \(_{j,j i}d(,_{ij})\), where the distance \(d(,_{ij})\) from \(\) to a hyperplane \(_{ij}\) has a closed-form expression:

\[d(,_{ij})=\{\|\|_{p}:^{m}+_{ij}\}=^{i}()-f_{}^ {j}()}{\|_{i}-_{j}\|_{q}},\] (2)

where \(\|\|_{q}\) is the dual norm of \(p\), \(q=\) for \(p>1\)(Moosavi-Dezfooli et al., 2016; Elsayed et al., 2018).

When the classifiers \(_{j}\) are equidistant, _i.e._\(\|_{i}-_{j}\|_{q}=C\) whenever \(i j\), the margin becomes:

\[_{j,j i}^{i}( )-f_{}^{j}()}{C}=_{j,j i}(f_{}^{i}()-f_{}^{j}( ))=^{i}()- _{j,j i}f_{}^{j}()}_ {}.\] (3)

Under the equidistance assumption, the logit margin is proportional (equal up to a scaling factor) to the margin in the feature space. We will denote the logit margin of \(\) by \(d_{out}()\):

\[d_{out}()=f_{}^{i}()-_{j,j  i}f_{}^{j}().\] (4)

### Margin Consistency

**Definition 2**.: _A model is **margin-consistent** if there is a monotonic relationship between the input space margin and the logit margin, i.e., \(d_{in}(_{1}) d_{in}(_{2}) d_{out}( _{1}) d_{out}(_{2}),\,_{1},_{2}\)._

A margin-consistent model preserves the relative position of samples to the decision boundary from the input space to the feature space. A sample further from (closer to) the decision boundary in the input space remains further from (closer to) the decision boundary in the feature space with respect to other samples, as illustrated in Fig. 1.

We can evaluate margin consistency by computing the **Kendall rank correlation** (\([-1,1]\)) between the logit margins and the input margins over a test set. The Kendall rank correlation tests the existence and strength of a monotonic relationship between two variables. It makes no assumption on the distribution of the variables and is robust to outliers (Chattamvelli, 2024). While a positive value of \(\) indicates samples are ranked similarly (or identically for \(=1\)) according to logit margins and input margins, a negative value of \(\) indicates that one margin's ranking is roughly reversed. Perfect margin consistency corresponds to the situation \(=1\).

### Non-robust Samples Detection

Non-robust detection can be defined as a scored-based binary classification task where non-robust samples constitute the positive class, and the input margin \(d_{in}\) induces a perfect discriminative function \(g\) for that:

\[g(;f_{})=_{[d_{in}()]}= 1&\\ 0&.\]

If a model is margin-consistent, its logit margin can also be a discriminative score to detect non-robust samples. The following theorem establishes that this is a necessary and sufficient condition. Therefore, the degree to which a model is margin-consistent should determine the discriminative power of the logit margin.

**Theorem 1**.: _If a model is margin-consistent, then for any robustness threshold \(\), there exists a threshold \(\) for the logit margin \(d_{out}\) that separates perfectly non-robust samples and robust samples. Conversely, if for any robustness threshold \(\), \(d_{out}\) admits a threshold \(\) that separates perfectly non-robust samples from robust samples, then the model is margin-consistent._Proof sketch.Fig. 2 presents intuition behind the proof of Theorem 1. For the first part of the theorem (see Fig. 1(a)), if there is a monotonic relationship between \(d_{in}\) and \(d_{out}\) (margin consistency), any point \(\) with \(d_{in}\) less than the threshold \(\) (non-robust) will also have \(d_{out}\) less than \(_{0}=d_{out}(})\) (with \(d_{in}(})=\)). For the second part (see Fig. 1(b)), if there are two points \(_{1}\) and \(_{2}\) with non-concordant \(d_{in}\) and \(d_{out}\) (no margin consistency), then for a threshold \(_{0}\) between \(d_{in}(x_{1})\) and \(d_{out}(x_{2})\), they will both have different classes but no threshold of \(d_{out}\) can classify them both correctly. The complete proof of Theorem 1 is deferred to Appendix A. **Common metrics for detection** include (Hendrycks and Gimpel, 2017; Corbier et al., 2019; Zhu et al., 2023): the Area Under the Receiver Operating Curve (**AUROC**), which ensures the ability of a model to distinguish between the positive and negative classes across all possible thresholds; the Area Under the Precision-Recall Curve (**AUPR**), which evaluates the trade-off between precision and recall and is less sensitive to imbalance between positive and negative classes; and the False Positive Rate (FPR) at a 95% True Positive Rate (TPR) (**FPR@95**), that is crucial in systems where missing positive cases can have serious consequences, such as minimizing the number of vulnerable samples missed. The AUROC and AUPR of a perfect classifier is 1, while 0.5 for a random classifier.

## 3 Evaluation

### Experimental Setup

Datasets and modelsWe investigate various pre-trained models on CIFAR10 and CIFAR100 datasets (Krizhevsky, 2009). The majority of models were loaded from the _RobustBench_ model zoo2(Croce et al., 2021), with a few more models that are ResNet-18 (He et al., 2016) models we trained on CIFAR10 with Standard Adversarial Training (Madry et al., 2018), TRADES (Zhang et al., 2019), Logit Pairing (ALP and CLP, Kannan et al. (2018)), and MART (Wang et al., 2020), using the experimental setup of Wang et al. (2020).

Input margin estimationThis is done using FAB attack (Croce and Hein, 2020), which is an attack that minimally perturbs the initial instance. Xu et al. (2023) used it in their adversarial training strategy as a reliable way to compute the closest boundary point given enough iterations. We perform the untargeted FAB attack without restricting the distortion to find the boundary for all the samples in the test set instead of constraining the perturbation inside a given \(\)-ball when evaluating robustness. As a sanity check for the measured distances, we compare the ratio of correct samples \(\) with estimated input margins greater than \(=8/255\) and the robust accuracy in \(_{}\) norm measured with _AutoAttack_(Croce and Hein, 2020) at \(=8/255\). Both quantities estimate the same thing, with a mean absolute difference over the models of \(1.3\) and \(0.48\) for CIFAR10 and CIFAR100, respectively, which are reasonable.

The estimation of the input margins over the \(10,000\) test samples allows us to create for a given threshold \(\) a pool of vulnerable samples that can be successfully attacked at threshold \(\) and non

Figure 2: Illustration of Theorem 1â€™s proof.

vulnerable samples that were not able to be attacked. Training and distance estimations were run on an NVIDIA Titan Xp GPU (1x).

### Results and Analysis

Correlation analysisThe results presented in Fig. 3 show that the logit margin has a strong correlation (up to 0.86) with the input margin, which means that they have a level of margin consistency for those models. The plots are given with standard error for the y-axis values in each interval. However, we also observe that two models (i.e., **DI0**(Ding et al., 2020) and **XU80**(Xu et al., 2023) WideResNets) have a weaker correlation. We show in Sec. 3.3 that we can learn to map the feature representation of these models to a pseudo-margin that reflects the distance to the decision boundary in the input space. Additional results on ImageNet with \(_{}\) norm and on \(_{2}\)-robust models on CIFAR10 are given in Table 5 of appendix E.

Vulnerable samples detectionWe present the results for the robustness threshold \(=8/255\) in Table 1. As expected with the strong correlations, the performance over the non-robust detection task is excellent. We can note that the metrics are lower for the two models with low correlations and particularly very high FPR@95. The performance remains quite good with different values of \(\) (cf. appendix B). Moreover, we show in appendix F.1 that the empirical robust accuracy of margin-consistent models can be accurately estimated by attacking only a small subset of the test set.

Margin Consistency and Lipschitz SmoothnessA neural network \(f\) is said to be \(L\)-Lipschitz if \(\|f(_{1})-f(_{2})\| L\|_{1}-_{2}\|\), \(_{1},_{2}\). Lipschitz smoothness is important for adversarial robustness because a small Lipschitz constant \(L\) guarantees the network's output cannot change more than a factor \(L\) of the change in the input. There are strategies to directly constraint the Lipschitz constant to achieve \(1\)-Lipschitz networks (Cisse et al., 2017; Li et al., 2019; Serrurier et al., 2021; Araujo et al., 2023). Empirical adversarial training strategies only indirectly encourage Lipschitz's smoothness of the model. However, we note that Lipschitz's continuity of the feature extractor \(h_{}\) does not imply margin consistency of the model. Considering two points \(_{1}\) and \(_{2}\) with \(0<d_{in}(_{1})<d_{in}(_{2})\), the \(L\)-Lipschitz condition implies that \(d_{out}(_{i}) Ld_{in}(_{i})\) for \(i=1,2\). However, as long as \(d_{out}(_{1})>0\), it is possible _a priori_ to have \(d_{out}(_{2})<d_{out}(_{1})\), thus violating the margin consistency condition, while still satisfying the previous relations. We also note that the strength of the correlation, _i.e._ the level of margin consistency, does not depend on the robust accuracy (see Fig. 3(a) and 3(b)).

Insight into when margin consistency may hold?We hypothesize that when the feature extractor \(h_{}\) behaves locally as an isometry (preserving distances, up to a scaling factor \(\), at least for directions normal to the decision boundary), i.e., \(\|-^{}\|_{p}=\|h_{}()-h_{}( ^{})\|_{p}\), margin consistency will occur. Given an input sample \(\), by definition \(d_{out}()=\|-^{}\|\) where \(=h_{}()\) and \(^{}\) the closest point to \(\) on the feature space decision boundary. The bijectivity of a local isometry implies that we have \(h_{}(^{})=^{}\), i.e. the representation of the closest point to \(\) in input space matches the closest

Figure 3: Margin consistency of various models: there is a strong correlation between input space margin and logit margin for most \(_{}\) robust models tested, the exceptions being DI0 and XU80 on CIFAR10. See Table 1 for the references on the models. The correlations are given with standard error for the y-axis values in each interval.

point to the representation of \(\) in the feature space. In that case we will have \(\|-^{}\|=\|-^{}\|\) which implies margin consistency. Experimentally, what we observe is that on the one hand, the input margin and the distance between the feature representations of \(\) and \(^{}\) (feature distance) correlate and on the other hand, the feature distance and the logit margin also correlate (Fig. 4(a) and Fig. 4(b) respectively).

### Learning a Pseudo-Margin

For the two models that are weakly margin-consistent, we are proposing to directly learn a mapping that maps the feature representation of a sample to a pseudo-margin that reflects the relative position of the samples to the decision boundary in the input space. We use a learning scheme similar to the one of Corbiere et al. (2019), with a small ad hoc neural network for learning the confidence of the instances. Given some samples with estimations of their input margins, the objective is to learn

    & **Model ID** & **Kendall \(\)** & **AUROC** & **AUPR** & **FPR@95** & **Acc** & **Rob. Acc** & **Architecture** \\   & **D10**(Wu et al., 2020) & 0.28 & 67.49 & 70.91 & 82.56 & 84.36 & 41.44 & WideResNet-28-4 \\  & **XU80**(Xu et al., 2023) & 0.43 & 83.30 & 80.50 & 83.42 & 93.69 & 63.89 & WideResNet-28-10 \\  & **MR0**(Wang et al., 2020) & 0.68 & 92.95 & 94.92 & 29.76 & 96.99 & 39.12 & ResNet-18 \\  & **DM0**(Debenedri et al., 2023) & 0.71 & 94.31 & 93.20 & 32.76 & 91.30 & 57.27 & XCIT-M12 \\  & **ALD**(Kannan et al., 2018) & 0.72 & 94.67 & 95.98 & 24.93 & 80.38 & 40.21 & ResNet-18 \\  & **CUB0**(Cui et al., 2023) & 0.73 & 96.87 & 94.42 & 17.90 & 92.16 & 67.73 & WideResNet-28-10 \\  & **WA80**(Wang et al., 2023) & 0.74 & 96.82 & 94.33 & 17.60 & 92.44 & 67.31 & WideResNet-28-10 \\  & **SEL0**(Schwaya et al., 2021) & 0.74 & 96.03 & 94.66 & 19.13 & 84.59 & 55.54 & ResNet-18 \\  & **EN0**(Ghergstrom et al., 2019) & 0.74 & 95.16 & 95.07 & 24.10 & 87.03 & 49.25 & ResNet-50 \\  & **TR0**(Zhang et al., 2019) & 0.74 & 94.63 & 96.13 & 30.93 & 80.72 & 42.23 & ResNet-18 \\  & **DS0**(Debenedri et al., 2023) & 0.75 & 95.80 & 95.08 & 24.65 & 90.06 & 56.14 & XCIT-S12 \\  & **MD0**(Madry et al., 2018) & 0.75 & 95.36 & 97.00 & 23.23 & 81.85 & 36.91 & ResNet-18 \\  & **ZHO**(Zhang et al., 2019) & 0.75 & 95.86 & 95.65 & 24.91 & 84.92 & 53.08 & WideResNet-34-10 \\  & **HE0**(Hendrycks et al., 2019) & 0.76 & 96.35 & 95.68 & 20.01 & 87.11 & 54.92 & WideResNet-28-10 \\  & **CL0**(Kannan et al., 2018) & 0.77 & 95.93 & 96.98 & 20.01 & 81.12 & 40.08 & ResNet-18 \\  & **RE80**(Rebuffi et al., 2021) & 0.77 & 97.33 & 95.70 & 13.87 & 87.33 & 60.73 & WideResNet-28-10 \\  & **F220**(Wang et al., 2022) & 0.78 & 97.65 & 96.39 & 14.40 & 88.61 & 61.04 & WideResNet-28-10 \\  & **AD20**(Addepalli et al., 2022) & 0.81 & 97.67 & 97.46 & 13.42 & 85.71 & 52.48 & ResNet-18 \\  & **AD10**(Addepalli et al., 2021) & 0.82 & 97.86 & 97.68 & 13.26 & 80.24 & 51.06 & ResNet-18 \\   & **IE1**(Hendrycks et al., 2019) & 0.74 & 94.43 & 97.30 & 30.40 & 59.23 & 28.42 & WideResNet-28-10 \\  & **WU1**(Wu et al., 2020) & 0.78 & 95.81 & 98.00 & 23.34 & 60.38 & 28.86 & WideResNet-34-10 \\  & **RE81**(Rebuffi et al., 2021) & 0.80 & 96.87 & 98.30 & 18.06 & 62.41 & 32.06 & WideResNet-28-10 \\  & **IS1**(Debenedri et al., 2023) & 0.81 & 96.78 & 98.30 & 19.18 & 67.34 & 32.19 & XCIT-S12 \\  & **CU41**(Cui et al., 2023) & 0.82 & 97.07 & 98.48 & 17.21 & 64.08 & 31.65 & WideResNet-34-10 \\  & **CU81**(Cui et al., 2023) & 0.83 & 97.41 & 98.24 & 15.62 & 73.85 & 39.18 & WideResNet-28-10 \\  & **H11**(Rice et al., 2020) & 0.83 & 96.61 & 99.05 & 18.14 & 53.83 & 18.95 & PreResNet-18 \\  & **PA21**(Wang et al., 2022) & 0.83 & 97.66 & 98.28 & 13.83 & 63.66 & 31.08 & WideResNet-28-10 \\  & **WA81**(Wang et al., 2023) & 0.83 & 97.51 & 98.28 & 14.96 & 72.58 & 38.83 & WideResNet-28-10 \\  & **AD21**(Addepalli et al., 2022) & 0.84 & 97.46 & 98.92 & 16.00 & 65.45 & 27.67 & ResNet-18 \\  & **AD1**(Addepalli et al., 2021) & 0.84 & 97.65 & 98.99 & 13.88 & 62.02 & 27.14 & PreResNet-18 \\  & **RE11**(Rebuffi et al., 2021) & 0.85 & 97.97 & 99.05 & 13.21 & 56.87 & 28.50 & PreResNet-18 \\  & **RA11**(Rade & Moosavi-Dezfooli, 2021) & 0.85 & 98.01 & 99.08 & 12.36 & 61.50 & 28.88 & PreResNet-18 \\   

Table 1: Correlations and vulnerable points detection performance at \(=8/255\) on different adversarially trained models.

Figure 4: Distribution of the correlation between input margins and logit margins in \(_{}\) with robust accuracy. The strength of the correlation, which indicates the level of margin consistency, does not depend on the robust accuracy. References on models are given in Table 1.

to map their feature representation to a pseudo-margin that correlates with the input margins. This learning task can be seen as a learning-to-rank problem. We use a simple learning-to-rank algorithm for that purpose, which is a pointwise regression approach (He et al., 2008) relying on the mean squared error as a surrogate loss.

For the experiment, we used a similar architecture and training protocol as (Corbiere et al., 2019) with a fully connected network with five dense layers of 512 neurons, with ReLU activations for the hidden layers and a sigmoid activation at the output layer. We learn using 5000 examples sampled randomly from the training set, with \(20\%\) (1000 examples) held as a validation. Fig. 6 and Table 2 show the improved correlation on the learned score compared to the logit margin for both models. The correlations are given with standard error for the y-axis values in each interval. The network has learned to recover the relative positions of the samples from the feature representation.

## 4 Related Work

Detection tasks in machine learning are found to be of three main types:

* **Adversarial Detection** The goal of adversarial detection (Xu et al., 2017; Carlini and Wagner, 2017) is to discriminate adversarial samples from clean and noisy samples. An adversarial example is a malicious example found by adversarially attacking a sample; it has a different class while being close to the original sample. A vulnerable (non-robust) sample is a normal sample that admits an adversarial example close to it. The two detection tasks are very distinct. Adversarial detection is a defence mechanism like adversarial training; Tramer (2022) has established that both tasks are equivalent problems with the same difficulty.
* **Out-of-Distribution (OOD) detection** In OOD detection (Hendrycks and Gimpel, 2017; Peng et al., 2024; Yang et al., 2021), the objective is to detect instances far from the distribution of the training data. These are often instances with different labels from the training labels or instances with the same label as training labels but with a covariate shift. For example, for a model trained on the CIFAR10 dataset, samples from the SVHN dataset (Netzer et al., 2011) or the corrupted version of CIFAR10-C (Hendrycks and Dietterich, 2019) are OOD samples for such a model.
* **Misclassification Detection (MisD)** It consists in detecting whether the classifier's prediction is incorrect. This is also referred to as Failure Detection or Trustworthiness Detection (Corbiere et al., 2019; Jiang et al., 2018; Luo et al., 2021; Granese et al., 2021; Zhu et al., 2023). MisD is often used for selective classification (classification with a reject option) (Geifman and El-Yaniv, 2017) to abstain from predicting samples on which the model is likely to be wrong. A score for non-robust detection cannot tell if the sample is incorrect, as a vulnerable sample could be from any side of the decision boundary.

Formal robustness verification aims at certifying whether a given sample is \(\)-robust or if it is not an adversarial counter-example can be provided (Brix et al., 2023b). Some complete exact methods

Figure 5: The correlations between the input margin, the distance between the feature representations of samples and their closest adversaries (feature distance \(-\|h_{}(x)-h_{}(x^{})\|\)), and the logit margin may be due to the local isometry of the feature extractor. See Table 1 for the specific references on the model ID. The correlations are given with standard error for the y-axis values in each interval.

based on solving Satisfiability Modulo Theory problems (Katz et al., 2017; Carlini et al., 2017; Huang et al., 2017) or Mixed-Integer Linear Programming (Cheng et al., 2017; Lomuscio & Maganti, 2017; Fischetti & Jo, 2017) provide formal certification given enough time. However, in practice, they are tractable only up to 100,000 activations (Tjeng et al., 2019). Incomplete but effective methods based on linear and convex relaxation methods and Branch-and-Bound methods (Zhang et al., 2018; Salman et al., 2019; Xu et al., 2020, 2021; Zhang et al., 2022; Shi et al., 2023) are faster but conservative, without guaranteed certifications even if given enough time. Scaling them to bigger architectures such as WideResNets and large Transformers is still challenging even with GPU accelartion(Brix et al., 2023a; Konig et al., 2024). Weng et al. (2018) converts the problem of finding the robust radius (input margin) as a local Lipschitz constant estimation problem. Computing the Lipschitz constant of Deep Nets is NP-hard (Virmaux & Scaman, 2018) and Jordan & Dimakis (2020) proved that there is no efficient algorithm to compute the local Lipschitz constant. The estimation provided by Weng et al. (2018) requires random sampling and remains computationally expensive to obtain a good approximation. Vulnerability detection with margin-consistent models does not provide certificates but an empirical estimation of the robustness of a sample as evaluated by adversarial attacks. At scale, it can help filter the samples to undergo formal verification and a more thorough adversarial attack for resource prioritization.

## 5 Limitations and Perspectives

Vulnerability detection scopeThe scope of this work is \(_{p}\) robustness measured by the input space margin; the minimum distortion that changes the model's decision while this does not give a full view of the \(_{p}\) robustness. Samples may be at the same distance to the decision boundary and have unequal unsafe neighbourhoods given by an average estimation over the \(\)-neighbourhood considered. The average estimation of local robustness for a given \(\)-neighborhood remains an open problem, so whether it is possible to extract other notions of robustness from the feature representation efficiently could be a potential avenue for further exploration.

Attack-based verificationThe margin consistency property does not rely on attacks; however, its verification and the learning of a pseudo-margin with an attack-based estimation may not be

  
**Model ID** & **Margin** & **Kendall tau** (\(\)) & **AUROC** (\(\)) & **AUPR** (\(\)) & **FPR@95** (\(\)) & **Acc.** & **Rob. Acc** \\  D10 (Ding et al., 2020) & Logit margin & 0.28 & 67.49 & 70.91 & 82.56 & 84.36 & 41.44 \\  & Learned pseudo-margin & **0.57** & **88.49** & **89.04** & **51.13** & & \\  XU80 (Xu et al., 2023) & Logit margin & 0.43 & 83.30 & 80.50 & 83.42 & 93.69 & 63.89 \\   

Table 2: Comparison of the correlation and detection performance between the actual logit margin and the pseudo-margin learned. The models are initially weakly margin-consistent, but the pseudo-margin learned from feature representations simulates the margin consistency with higher correlation and better discriminative power.

Figure 6: Correlation improvement of the learned pseudo-margin over the logit margin for DI0 (Ding et al., 2020) and XU80 (Xu et al., 2023).

possible if the model cannot be attacked on a sufficient number of samples. The assumption is that we can always successfully provide the closest point to the decision with a sufficient budget. This is a reasonable assumption since the studied models are not perfectly robust, and the empirical evidence so far with adaptive attacks is that no defence is foolproof, which justifies the need to detect the non-robust samples. It might occur that we need to combine with an attack such as _CW-attack_(Carlini and Wagner, 2016) to find the closest adversarial sample.

Influence of terminal phase of trainingThe work of Papyan et al. (2020) shows that when deep neural network classifiers are trained beyond zero training error and beyond zero cross-entropy loss (aka terminal phase of training), they fall into a state known as _neural collapse_. Neural collapse is a state where the within-class variability of the feature representations collapses to their class means, the class means, and the classifiers become self-dual and converge to a specific geometric structure, an equiangular tight frame (ETF) simplex, and the network classifier converges to nearest train class center. This implies that we may lose the margin consistency property. While neural collapse predicts that all representations collapse on their class mean, in practice, perfect collapse is not quite achieved, and it is precisely the divergence of a representation from its class mean (or equivalently its \(}\)) which encodes the information we seek about the distance to the decision boundary in the input space. Exploring the impact of the neural collapse on margin consistency as models tend toward a collapsed state could provide valuable insights into generalization and adversarial robustness.

Adaptaptive attacks and adversarial examplesIn this paper, we study the margin consistency of models on their training distribution by reporting the Kendall rank correlation between the logit margin and the input margin on the test set. The study of this property on inputs from a different distribution or specifically crafted examples is left for future research. However, we observe that the adversarial examples used for the input margin estimation have significantly smaller logit margins than the detection thresholds (see Table 4 in appendix D). This indicates that these specific adversarial examples are indeed identified as non-robust instances, together with clean non-robust samples.

## 6 Conclusion

This work addresses the question of efficiently estimating local robustness in the \(_{p}\) sense at a per-instance level in robust deep neural classifiers in deployment scenarios. We introduce margin consistency as a necessary and sufficient condition to use the logit margin of a deep classifier as a reliable proxy estimation of the input margin for detecting non-robust samples. Our investigation of various robustly trained models shows that they have high margin consistency, which leads to a high performance of the logit margins in detecting vulnerable samples to adversarial attacks. We also find that margin consistency does not always hold, with some models having a weak correlation between the input margin and the logit margin. In such cases, we show that it is possible to learn to map the feature representation to a better-correlated pseudo-margin that simulates the margin consistency and performs better on vulnerability detection. Finally, we present some limitations of this work, mainly the scope of robustness, the attack-based verification, the impact of neural collapse in terminal phases of training, and vulnerability to adaptive attacks. Beyond its highly practical importance, we see this as a motivation to extend the analysis of robust models and the properties of their feature representations in the context of vulnerability detection.