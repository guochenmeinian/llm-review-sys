# Indeterminate Probability Neural Network

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a new general model called **IPNN** - ** Indeterminate **P**robability **N**eural **N**etwork, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. With this new theory, some intractable probability problems have now become tractable (analytical solution). Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are reflected in experimental results.

## 1 Introduction

Humans can distinguish at least 30,000 basic object categories , classification of all these would have two challenges: It requires huge well-labeled images; Model with softmax for large scaled datasets is computationally expensive. Zero-Shot Learning - ZSL [2; 3] method provides an idea for solving the first problem, which is an attribute-based classification method. ZSL performs object detection based on a human-specified high-level description of the target object instead of training images, like shape, color or even geographic information. But labelling of attributes still needs great efforts and expert experience. Hierarchical softmax can solve the computationally expensive problem, but the performance degrades as the number of classes increase .

Probability theory has not only achieved great successes in the classical area, such as Naive Bayesian method , but also in deep neural networks (VAE , ZSL, etc.) over the last years. However, both have their shortages: Classical probability can not extract features from samples; For neural networks, the extracted features are usually abstract and cannot be directly used for numerical probability calculation. What if we combine them?

There are already some combinations of neural network and bayesian approach, such as probability distribution recognition [7; 8], Bayesian approach are used to improve the accuracy of neural modeling , etc. However, current combinations do not take advantages of ZSL method.

We propose an approach to solve the mentioned problems, and our contributions are as follows:

* indeterminate probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. The proposed general tractable Equation (12) is analytical solutions even for some intractable probability calculation problems.
* With this new theory, CIPNN  has found the analytical solution for the posterior calculation of continuous latent variables, which was regarded as intractable [6; 11]. Besides, CIPNN applied our theory and proposed a general auto encoder (CIPAE), the decoder part is not a neural network and uses a fully probabilistic inference model for the first time.
* We propose a novel unified combination of (indeterminate) probability theory and deep neural network. The neural network is used to extract attributes which are defined as discrete random variables, and the inference model for classification task is derived. Besides, these attributes do not need to be labeled in advance.

The rest of this paper is organized as follows: In Section 2, related works are discussed. In Section 3, we first introduce a coin toss game as example of human cognition to explain the core idea of IPNN. In Section 4, the indeterminate probability theory and IPNN is proposed. In Section 5, the training strategy is discussed. In Section 6, we evaluate IPNN and make an impact analysis on its hyper-parameters. Finally, we conclude the paper in Section 7.

## 2 Related Work

Tractable Probabilistic Models.There are a large family of tractable models including probabilistic circuits [12; 13], arithmetic circuits [14; 15], sum-product networks , cutset networks , and-or search spaces , and probabilistic sentential decision diagrams . The analytical solution of a probability calculation is defined as occurrence, \(P(A=a)=(A=a)}{}\), which is however not focused in these models. Our proposed IPNN is fully based on event occurrence and is an analytical solution.

Deep Latent Variable Models.DLVMs are probabilistic models and can refer to the use of neural networks to perform latent variable inference . Currently, the posterior calculation of continuous latent variables is regarded as intractable , VAEs [6; 21; 22; 23] use variational inference method  as approximate solutions. Our proposed IPNN is one DLVM with discrete latent variables and the intractable posterior calculation is now analytically solved with our proposed theory.

## 3 Background

Let's first introduce a small game - coin toss: a child and an adult are observing the outcomes of each coin toss and record the results independently (heads or tails), the child can't always record the results correctly and the adult can record it correctly, in addition, the records of the child are also observed by the adult. After several coin tosses, the question now is, suppose the adult is not allowed to watch the next coin toss, what is the probability of his inference outcome of next coin toss via the child's record?

As shown in Figure 1, random variables X is the random experiment itself, and \(X=x_{k}\) represent the \(k^{th}\) random experiment. Y and A are defined to represent the adult's record and the child's record,

   Experiment & Truth & A & Y \\ \(X=x_{1}\) & \(hd\) & \(A=hd\) & \(Y=hd\) \\ \(X=x_{2}\) & \(hd\) & \(A=hd\) & \(Y=hd\) \\ \(X=x_{3}\) & \(hd\) & \(A=hd\) & \(Y=hd\) \\ \(X=x_{4}\) & \(hd\) & \(A=hd\) & \(Y=hd\) \\ \(X=x_{5}\) & \(hd\) & \(=\) & \(Y=hd\) \\ \(X=x_{6}\) & \(tl\) & \(A=tl\) & \(Y=tl\) \\ \(X=x_{7}\) & \(tl\) & \(A=tl\) & \(Y=tl\) \\ \(X=x_{8}\) & \(tl\) & \(A=tl\) & \(Y=tl\) \\ \(X=x_{9}\) & \(tl\) & \(A=tl\) & \(Y=tl\) \\ \(X=x_{10}\) & \(tl\) & \(A=tl\) & \(Y=tl\) \\ \(X=x_{11}\) & \(hd\) & \(A=?\) & \(Y=?\) \\   

Table 1: Example of 10 times coin toss outcomes

Figure 1: Example of coin toss game.

respectively. And \(hd,tl\) is for heads and tails. For example, after 10 coin tosses, the records are shown in Table 1.

We formulate X compactly with the ground truth, as shown in Table 2.

Through the adult's record Y and the child's records A, we can calculate \(P(Y|A)\), as shown in Table 3. We define this process as observation phase.

For next coin toss (\(X=x_{11}\)), the question of this game is formulated as calculation of the probability \(P^{A}(Y|X)\), superscript A indicates that Y is inferred via record A, not directly observed by the adult. For example, given the next coin toss \(X=hd=x_{11}\), the child's record has then two situations: \(P(A=hd|X=hd=x_{11})=4/5\) and \(P(A=tl|X=hd=x_{11})=1/5\). With the adult's observation of the child's records, we have \(P(Y=hd|A=hd)=4/4\) and \(P(Y=hd|A=tl)=1/6\). Therefore, given next coin toss \(X=hd=x_{11}\), \(P^{A}(Y=hd|X=hd=x_{11})\) is the summation of these two situations: \(+\). Table 3 answers the above mentioned question.

Let's go one step further, we can find that even the child's record is written in unknown language (e.g. \(A\{ZHENG,FAN\}\)), Table 3 can still be calculated by the man. The same is true if the child's record is written from the perspective of attributes, such as color, shape, etc.

Hence, if we substitute the child with a neural network and regard the adult's record as the sample labels, although the representation of the model outputs is unknown, the labels of input samples can still be inferred from these outputs. This is the core idea of IPNN.

## 4 Indeterminate Probability Theory

In this section, we propose a new general probability theory, which is derived from IPNN - a neural network with discrete deep latent variables.

### IPNN Model Architecture

Let \(X\{x_{1},x_{2},,x_{n}\}\) be training samples (\(X=x_{k}\) is understood as \(k^{th}\) random experiment - select one train sample.) and \(Y\{y_{1},y_{2},,y_{m}\}\) consists of \(m\) discrete labels (or classes), \(P(y_{l}|x_{k})=y_{l}(k)\{0,1\}\) describes the label of sample \(x_{k}\). For prediction, we calculate the posterior of the label for a given new input sample \(x_{n+1}\), it is formulated as \(P^{}(y_{l} x_{n+1})\), superscript \(\) stands for the medium - model outputs, via which we can infer label \(y_{l},\;\;l=1,2,,m\). After \(P^{}(y_{l} x_{n+1})\) is calculated, the \(y_{l}\) with maximum posterior is the predicted label.

Figure 1(a) shows IPNN model architecture, the output neurons of a general neural network (FFN, CNN, Resnet , Transformer , Pretrained-Models , etc.) is split into N unequal/equal parts, the split shape is marked as Equation (1), hence, the number of output neurons is the summation of the split shape \(_{j=1}^{N}M_{j}\). Next, each split part is passed to'softmax', so the output neurons can be defined as discrete random variable

\(1,2,,N\), and each neuron in \(A^{j}\) is regarded as an event. After that, all the random variables together form the N-dimensional joint sample space, marked as \(=(A^{1},A^{2},,A^{N})\), and all the joint sample points are fully connected with all labels \(Y\{y_{1},y_{2},,y_{m}\}\) via conditional probability \(P(Y=y_{l}|A^{1}=a_{i_{1}}^{1},A^{2}=a_{i_{2}}^{2},,A^{N}=a_{i_{N}}^{N})\), or more compactly written as \(P(y_{l}|a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})\)1.2

\[:=\{M_{1},M_{2},,M_{N}\}\] (1)

### Definition of Indeterminate Probability

In classical probability theory, perform a random experiment (or given a sample \(x_{k}\)), the event or joint event has only two states: happened or not happened. However, for IPNN, the model only outputs the probability of an event state and its state is indeterminate, that's why this paper is called IPNN. This difference makes the calculation of probability (especially joint probability) also different. Equation (2) and Equation (3) will later formulate this difference.

Given an input sample \(x_{k}\) (perform the \(k^{th}\) random experiment), with Assumption 1 the indeterminate probability (model outputs) is defined as:

\[P(a_{i_{j}}^{j} x_{k})=_{i_{j}}^{j}(k)\] (2)

**Assumption 1**.: _Given an input sample \(X=x_{k}\), **IF**\(_{i_{j}=1}^{M_{j}}_{i_{j}}^{j}(k)=1\) and \(_{i_{j}}^{j}(k),k=1,2,,n\). **THEN**, \(\{a_{1}^{j},a_{2}^{j},,a_{M_{j}}^{j}\}\) can be regarded as collectively exhaustive and exclusive events set, they are partitions of the sample space of random variable \(A^{j},j=1,2,,N\)._

In classical probability, \(_{i_{j}}^{j}(k)\{0,1\}\), which indicates the state of event is 0 or 1.

For joint event, given \(x_{k}\), using Assumption 2 and Equation (2), the joint indeterminate probability is formulated as:

Figure 2: IPNN. (a) \(P(y_{l}|a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})\) is statistically calculated, not model weights. (b, c) Independence illustration with Bayesian network.

\[P(a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N} x_{k})=_{j=1 }^{N}_{i_{j}}^{j}(k)\] (3)

**Assumption 2**.: _Given an input sample \(X=x_{k}\), \(A^{1},A^{2},,A^{N}\) is mutually independent._

Where it can be easily proved,

\[_{}(_{j=1}^{N}_{i_{j}}^{j}(k))=1,k=1,2, ,n.\] (4)

In classical probability, \(_{j=1}^{N}_{i_{j}}^{j}(k)\{0,1\}\), which indicates the state of joint event is 0 or 1.

Equation (2) and Equation (3) describes the uncertainty of the state of event \((A^{j}=a_{i_{j}}^{j})\) and joint event \((A^{1}=a_{i_{1}}^{1},A^{2}=a_{i_{2}}^{2},,A^{N}=a_{i_{N}}^{N})\).

### Observation Phase

In observation phase, the relationship between all random variables \(A^{1},A^{2},,A^{N}\) and \(Y\) is established after the whole observations, it is formulated as:

\[P(y_{l} a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})= ,a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})}{P (a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})}\] (5)

Because the state of joint event is not determinate in IPNN, we cannot count its occurrence like classical probability. Hence, the joint probability is calculated according to total probability theorem over all samples \(X=(x_{1},x_{2},,x_{n})\), and with Equation (3) we have:

\[ P(a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}} ^{N})&=_{k=1}^{n}(P(a_{i_{1}}^{1},a_{i_{2}}^{ 2},,a_{i_{N}}^{N} x_{k}) P(x_{k}))\\ &=_{k=1}^{n}(_{j=1}^{N}P(a_{i_{j}}^{j} x_{ k}) P(x_{k}))=^{n}(_{j=1}^{N}_{i_{ j}}^{j}(k))}{n}\] (6)

Because \(Y=y_{l}\) is sample label and \(A^{j}=a_{i_{j}}^{j}\) comes from model, it means \(A^{j}\) and Y come from different observer, so we can have Assumption 3 (see Figure 2c).

**Assumption 3**.: _Given an input sample \(X=x_{k}\), \(A^{j}\) and Y is mutually independent in observation phase, \(j=1,2,,N\)._

Therefore, according to total probability theorem, Equation (3) and the above assumption, we derive:

\[ P(y_{l},a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i _{N}}^{N})&=_{k=1}^{n}(P(y_{l},a_{i_{1}}^{1},a _{i_{2}}^{2},,a_{i_{N}}^{N} x_{k}) P(x_{k}))\\ &=_{k=1}^{n}(P(y_{l} x_{k})_{j= 1}^{N}P(a_{i_{j}}^{j} x_{k}) P(x_{k}))\\ &=^{n}(y_{l}(k)_{j=1}^{N}_{i_ {j}}^{j}(k))}{n}\] (7)

Substitute Equation (6) and Equation (7) into Equation (5), we have:

\[P(y_{l}|a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})=^{n}(y_{l}(k)_{j=1}^{N}_{i_{j}}^{j}(k))}{ _{k=1}^{n}(_{j=1}^{N}_{i_{j}}^{j}(k))}\] (8)

Where it can be proved,

\[_{l=1}^{m}P(y_{l} a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N} )=1\] (9)

### Inference Phase

Given \(A^{j}\), with Equation (8) (passed experience) label \(y_{l}\) can be inferred, this inferred \(y_{l}\) has no pointing to any specific sample \(x_{k}\), incl. also new input sample \(x_{n+1}\), see Figure 1(b). So we can have following assumption:

**Assumption 4**.: _Given \(A^{j}\), \(X\) and \(Y\) is mutually independent in inference phase, \(j=1,2,,N\)._

Therefore, given a new input sample \(X=x_{n+1}\), according to total probability theorem over joint sample space \((a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})\), with Assumption 4, Equation (3) and Equation (8), we have:

\[ P^{}(y_{l} x_{n+1})& =_{}(P(y_{l},a_{i_{1}}^{1},a_{i_{2}}^{2}, ,a_{i_{N}}^{N} x_{n+1}))\\ &=_{}(P(y_{l} a_{i_{1}}^{1},a_{i_{2}} ^{2},,a_{i_{N}}^{N}) P(a_{i_{1}}^{1},a_{i_{2}}^{2}, ,a_{i_{N}}^{N} x_{n+1}))\\ &=_{}(^{n}(y_{l}(k) _{j=1}^{N}_{i_{j}}^{j}(k))}{_{k=1}^{n}(_{j=1}^{ N}_{i_{j}}^{j}(k))}_{j=1}^{N}_{i_{j}}^{j}(n+1) )\] (10)

And the maximum posterior is the predicted label of an input sample:

\[:=*{arg\,max}_{l\{1,2,,m\}}P^{}(y _{l} x_{n+1})\] (11)

### Summary

Our most important contribution is that we propose a new general **tractable** probability Equation (10), rewritten as:

\[&}}(  X=x_{n+1}})=\\ &}(^{n} ( X=x_{k})_{j=1}^{N}P( A^{j}=a_{i_{j}}^{j} X=x_{k}))}}_{_{k=1}^{n}(_{j=1}^{N}P (A^{j}=a_{i_{j}}^{j} X=x_{k}))}_{j=1}^{N}P (A^{j}=a_{i_{j}}^{j} X=x_{n+1}))}_{}.\] (12)

Where X is random variable and \(X=x_{k}\) denote the \(k^{th}\) random experiment (or model input sample \(x_{k}\)), \(Y\) and \(A^{1:N}\) are different discrete or continuous  random variables. This equation can be applied to any random experiment, as long as the outcomes of random experiments are detected by some observers (neural networks, humans, or others).

Our proposed theory is derived from three our proposed conditional mutual independency assumptions, see Assumption 2 Assumption 3 and Assumption 4. However, in our opinion, these assumptions can neither be proved nor falsified, and we do not find any exceptions until now. Since this theory can not be mathematically proved, we can only validate it through experiment.

Finally, our proposed indeterminate probability theory is an extension of classical probability theory, and classical probability theory is one special case to our theory. More details to understand our theory intuitively, see Appendix B.

## 5 Training

### Training Strategy

Given an input sample \(x_{t}\) from a mini batch, with a minor modification of Equation (10):\[P^{}(y_{l} x_{t})_{}(),)}{(G+g(),)}_{j=1}^{N} _{i_{j}}^{j}(t))\] (13)

\[h()=_{k=b:(-1)+1}^{b}(y_{l}(k)_{j =1}^{N}_{i_{j}}^{j}(k))\] (14)

\[g()=_{k=b:(-1)+1}^{b}(_{j=1}^{N} _{i_{j}}^{j}(k))\] (15)

\[H=_{k=(1,-T)}^{-1}h(k),=2,3,\] (16)

\[G=_{k=(1,-T)}^{-1}g(k),=2,3,\] (17)

Where \(b\) is for batch size, \(=},t=1,2,,n\). Hyper-parameter T is for forgetting use, i.e., \(H\) and \(G\) are calculated from the recent T batches. Hyper-parameter T is introduced because at beginning of training phase the calculated result with Equation (8) is not good yet. And the \(\) on the denominator is to avoid dividing zero, the \(\) on the numerator is to have an initial value of 1. Besides, \(H\) and \(G\) are not needed for gradient updating during back-propagation. The detailed algorithm implementation is shown in Algorithm 1.

We use cross entropy as loss function:

\[=-_{l=1}^{m}(y_{l}(k) P^{}(y_{l}  x_{t}))\] (18)

With Equation (13) we can get that \(P^{}(y_{l} x_{1})=1\) for the first input sample if \(y_{l}\) is the ground truth and batch size is 1. Therefore, for IPNN the loss may increase at the beginning and fall back again while training.

### Multi-degree Classification (Optional)

In IPNN, the model outputs N different random variables \(A^{1},A^{2},,A^{N}\), if we use part of them to form sub-joint sample spaces, we are able of doing sub classification task, the sub-joint spaces are defined as \(^{1},^{2},\) The number of sub-joint sample spaces is:

\[_{j=1}^{N}=_{j=1}^{N}()\] (19)

If the input samples are additionally labeled for part of sub-joint sample spaces3, defined as \(Y^{}\{y_{1}^{},y_{2}^{},,y_{m^{}}^{}\}\). The sub classification task can be represented as \( X,^{1},Y^{1}, X,^{2},Y^{2 },\) With Equation (18) we have,

\[^{}=-_{l=1}^{m^{}}(y_{l}^{}(k) P^{ ^{}}(y_{l}^{} x_{t})),=1,2,\] (20)

Together with the main loss, the overall loss is \(+^{1}+^{2}+\) In this way, we can perform multi-degree classification task. The additional labels can guide the convergence of the joint sample spaces and speed up the training process, as discussed later in Appendix D.1.

### Multi-degree Unsupervised Clustering

If there are no additional labels for the sub-joint sample spaces, the model are actually doing unsupervised clustering while training. And every sub-joint sample space describes one kind of clustering result, we have Equation (19) number of clustering situations in total.

### Designation of Joint Sample Space

As in Appendix C proved, we have following proposition:

**Proposition 1**.: _For \(P(y_{l}|x_{k})=y_{l}(k)\{0,1\}\) hard label case, IPNN converges to global minimum only when \(P(y_{l}|a_{i_{1}}^{1},a_{i_{2}}^{2},,a_{i_{N}}^{N})=1,\) for \(_{j=1}^{N}_{i_{j}}^{j}(t)>0,i_{j}=1,2,,M_{j}\). In other word, each joint sample point corresponds to an unique category. However, a category can correspond to one or more joint sample points._

**Corollary 1**.: _The necessary condition of achieving the global minimum is when the split shape defined in Equation (1) satisfies: \(_{j=1}^{N}M_{j} m\), where \(m\) is the number of classes. That is, for a classification task, the number of all joint sample points is greater than the classification classes._

Theoretically, if model with 100 output nodes are split into 10 equal parts, it can classify 10 billion categories, validation result see Appendix D.1. Besides, the unsupervised clustering (Section 5.3) depends on the input sample distributions, the split shape shall not violate from multi-degree clustering. For example, if the main attributes of one dataset shows three different colors, and your split shape is \(\{2,2,\}\), this will hinder the unsupervised clustering, in this case, the shape of one random variable is better set to 3. And as in Appendix D also analyzed, there are two local minimum situations, improper split shape will make IPNN go to local minimum.

In addition, the latter part from Proposition 1 also implies that IPNN may be able of doing further unsupervised classification task, this is beyond the scope of this discussion.

## 6 Experiments and Results

### Unsupervised Clustering

As in Section 5.3 discussed, IPNN is able of performing unsupervised clustering, we evaluate it on MNIST. The split shape is set to \(\{2,10\}\), it means we have two random variables, and the first random variable is used to divide MNIST labels \(0,1, 9\) into two clusters. The cluster results is shown in Figure 3.

We find only when \(\) in Equation (13) is set to a relative high value that IPNN prefers to put number 1,4,7,9 into one cluster and the rest into another cluster, otherwise, the clustering results is always different for each round training. The reason is unknown, our intuition is that high \(\) makes that each category catch the free joint sample point more harder, categories have similar attributes together will be more possible to catch the free joint sample point.

Figure 3: Unsupervised clustering results on MNIST: test accuracy \(95.1 0.4\), \(=2\), batch size \(b=64\), forget number \(T=5\), epoch is 5 per round. The test was repeated for 876 rounds with same configuration (different random seeds) in order to check the stability of clustering performance, each round clustering result is aligned using Jaccard similarity .

### Hyper-parameter Analysis

IPNN has two import hyper-parameters: split shape and forget number T. In this section, we have analyzed it with test on MNIST, batch size is set to 64, \(=10^{-6}\). As shown in Figure 3(a), if the number of joint sample points is smaller than 10, IPNN is not able of making a full classification and its test accuracy is proportional to number of joint sample points, as number of joint sample points increases over 10, IPNN goes to global minimum for both 3 cases, this result is consistent with our analysis. However, we have exceptions, the accuracy of split shape with \(\{2,5\}\) and \(\{2,6\}\) is not high. From Figure 3 we know that for the first random variable, IPNN sometimes tends to put number 1,4,7,9 into one cluster and the rest into another cluster, so this cluster result request that the split shape need to be set minimums to \(\{2, 6\}\) in order to have enough free joint sample points. That's why the accuracy of split shape with \(\{2,5\}\) is not high. (For \(\{2,6\}\) case, only three numbers are in one cluster.)

Another test in Figure 3(b) shows that IPNN will go to local minimum as forget number T increases and cannot go to global minimum without further actions, hence, a relative small forget number T shall be found with try and error.

### Evaluation on Datasets

Further results on MNIST , Fashion-MNIST , CIFAR10  and STL10  show that our proposed indeterminate probability theory is valid, the backbone between IPNN and 'Simple-Softmax' is the same, the last layer of the latter one is connected to softmax function. Although IPNN does not reach any SOTA, the results are very important evidences to our proposed mutual independence assumptions, see Assumption 2 Assumption 3 and Assumption 4.

## 7 Conclusion

For a classification task, we proposed an approach to extract the attributes of input samples as random variables, and these variables are used to form a large joint sample space. After IPNN converges to global minimum, each joint sample point will correspond to an unique category, as discussed in Proposition 1. As the joint sample space increases exponentially, the classification capability of IPNN will increase accordingly.

We can then use the advantages of classical probability theory, for example, for very large joint sample space, we can use the Bayesian network approach or mutual independence among variables (see Appendix E) to simplify the model and improve the inference efficiency, in this way, a more complex Bayesian network could be built for more complex reasoning task.

   Dataset & IPNN & Simple-Softmax \\  MNIST & \(95.8 0.5\) & \(97.6 0.2\) \\ Fashion- & \(84.5 1.0\) & \(87.8 0.2\) \\ MNIST & \(83.6 0.5\) & \(85.7 0.9\) \\ STL10 & \(91.6 4.0\) & \(94.7 0.7\) \\   

Table 4: Test accuracy: split shape for all these datasets is set to \(\{2,2,5\}\); backbone is FCN for MNIST and Fashion-MNIST, Resnet50  for CIFAR10 and STL10.

Figure 4: (a) Impact Analysis of split shape with MNIST: 1D split shape is for \(\{\},=2,3,,24\). 2D split shape is for \(\{2,\},=2,3,,12\). 3D split shape is for \(\{2,2,\},=2,3,,6\). The x-axis is the number of joint sample points calculated with \(_{j=1}^{N}M_{j}\), see Equation (1). (b) Impact Analysis of forget number T with MNIST: Split shape is \(\{10\}\).