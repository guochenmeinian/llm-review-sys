# Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images

Bahri Batuhan Bilecen

Ahmet Berke Gokmen1

Aysegul Dundar

Bilkent University, Department of Computer Engineering, Ankara, Turkiye

{batuhan.bilecen@, berke.gokmen@ug, adundar@cs}.bilkent.edu.tr

Equal contribution.

###### Abstract

3D GAN inversion aims to project a single image into the latent space of a 3D Generative Adversarial Network (GAN), thereby achieving 3D geometry reconstruction. While there exist encoders that achieve good results in 3D GAN inversion, they are predominantly built on EG3D, which specializes in synthesizing near-frontal views and is limiting in synthesizing comprehensive 3D scenes from diverse viewpoints. In contrast to existing approaches, we propose a novel framework built on PanoHead, which excels in synthesizing images from a 360-degree perspective. To achieve realistic 3D modeling of the input image, we introduce a _dual encoder system_ tailored for high-fidelity reconstruction and realistic generation from different viewpoints. Accompanying this, we propose a _stitching framework on the triplane domain_ to get the best predictions from both. To achieve seamless stitching, both encoders must output consistent results despite being specialized for different tasks. For this reason, we carefully train these encoders using specialized losses, including an adversarial loss based on our novel _occlusion-aware triplane discriminator_. Experiments reveal that our approach surpasses the existing encoder training methods qualitatively and quantitatively. Please visit the **project page**.

Figure 1: From a single input image (first column), our framework reconstructs 3D representation by inverting images into PanoHeadâ€™s latent space, which can be viewed in a 360-degree perspective.

Introduction

In the realm of generative models, 2D GANs have gained renown for their remarkable ability to achieve striking realism through adversarial training, effectively capturing intricate details and textures to produce visually convincing images, especially on face images [18; 19]. However, their inherent limitation lies in their lack of depth perception, which restricts their applicability in three-dimensional contexts. In contrast, 3D GANs mark a groundbreaking advancement by seamlessly integrating Neural Radiance Fields (NeRF) into their architecture [13; 9; 14; 5]. This integration not only allows them to match the realism of their 2D counterparts but also ensures consistency in three-dimensional geometry. While extensive studies have focused on 2D GAN inversion [42; 29; 31; 4; 33; 28], recent efforts have seen the proposal of inversion methods tailored for 3D GANs [39; 7; 21].

2D GAN inversion techniques focus on projecting the images into the GAN's natural latent space to enhance editability and achieve high fidelity to the input image; however, inverting 3D GAN models presents additional challenges. This process requires accurate 3D reconstruction, which means realistic filling of invisible regions, ensuring coherence and completeness in the resulting three-dimensional scenes. Recently, inversion methods for 3D GANs have been developed, firstly, optimization-based and then encoder-based methods. Optimization-based methods [20; 35; 38] employ reconstruction losses to invert images into a latent code specific to the given view. Furthermore, network parameters are optimized by generating pseudo-multi-view images from the optimized latent codes to enhance detail preservation. Such optimization is required for each inference image; it is time-consuming and requires GPUs with large memory. Therefore, researchers focus on encoder-based methods [39; 7; 21]. While successful inversions are achieved with these methods, they rely on EG3D  framework [39; 7; 21], which is constrained to synthesizing near-frontal views. However, our work utilizes PanoHead , a method capable of rendering full-head image synthesis, enabling a comprehensive 360-degree perspective. This advancement introduces additional challenges, particularly concerning the invisibility of many parts in the input image. Despite this, the inversion model is expected to reasonably predict and reconstruct these occluded regions to ensure high-quality 3D reconstruction. Our experiments demonstrate that extending the methods proposed for EG3D is ineffective.

When projecting images onto PanoHead's latent space, we observe a trade-off between achieving high-fidelity reconstruction of the input image and generating realistic representations of the invisible parts of the head. Some models can perfectly reconstruct the image from a given view but produce unrealistic outputs when the camera parameters change. Conversely, other models generate realistic representations under varying camera parameters but fail to achieve high-fidelity reconstruction of the input image. To achieve high-fidelity reconstruction of the input image and realistic representations of the invisible parts of the head simultaneously, we train a **dual encoder**. One encoder specializes in reconstructing the given view, while the other focuses on generating high-quality invisible views. We propose stitching the triplane domain generations to produce the final result. This approach combines the outputs from both encoders to achieve both high-fidelity reconstructions of the given view and high-quality representations of the invisible parts of the head. To achieve seamless stitching, both encoders must output consistent results despite being specialized for different tasks. For this reason, we carefully train these encoders using specialized losses, including an adversarial loss based on our novel **occlusion-aware triplane discriminator**. This ensures that both encoders learn to produce consistent and complementary outputs, enabling seamless stitching of generations for the final result. Our contributions are as follows:

* To achieve high fidelity to the input and realistic generations for different camera views, we train dual encoders and introduce a stitching pipeline that combines the best predictions from both encoders for visible and invisible regions.
* We propose a novel occlusion-aware discriminator that enhances both fidelity and realism.
* We conduct extensive experiments to show the effectiveness of our framework. Quantitative and qualitative results show the superiority of our method compared to the state-of-the-art. Some visual results can be seen in Fig. 1.

## 2 Related works

**3D Generative Models.** Generative Adversarial Networks (GANs), coupled with differentiable renderers, have achieved significant strides in generating 3D-aware multi-view consistent images.

While early efforts, such as HoloGAN , operating on voxel representations, subsequent works shifted towards mesh representations [27; 15], and the latest advancements are built around implicit representations [10; 14; 26; 25]. Among implicit representations, triplane representations have emerged as a popular choice due to their computational efficiency and the high-quality outputs [9; 13; 5]. The architectures of works like EG3D  and PanoHead  bear resemblance to the structure of StyleGAN2 . They consist of mapping and synthesis networks, generating triplanes which are subsequently projected to a 2D image through volumetric rendering operations akin to those used in NeRF . While EG3D is trained on the FFHQ  dataset with limited angle diversity, PanoHead achieves a 360-degree perspective in face generation thanks to their dataset selection and model improvements. In our work, we delve into PanoHead's latent space and construct our inversion encoder based on PanoHead.

**GAN Inversion.** In recent years, GAN inversion, particularly in the context of StyleGAN, has garnered significant attention due to its extensive editing capabilities. The primary objective of these studies is to embed an image into StyleGAN's latent space, enabling subsequent modifications. Initially, this was approached through latent optimization, where latent codes were iteratively adjusted using back-propagation to minimize the reconstruction loss between the generated and target images [11; 1; 2; 19; 30]. For 3D-aware GAN model inversions, supplementary heuristics have been introduced into the optimization process. These include considerations like facial symmetry  and multi-view optimization strategies . However, such methods are computationally intensive as they require optimizing each image's latent codes. Moreover, while minimizing the reconstruction loss can yield visually similar results, it does not guarantee that the image resides within the natural latent space of GANs. This distinction is crucial for effective image editing. Without aligning with StyleGAN's inherent latent space, reconstructed images may not respond correctly to editing techniques, thus limiting their practical utility. This consideration also extends to 3D-aware GAN inversion methods, where encoding geometric information is paramount. Even if an input image can be faithfully reconstructed, its realism may falter when observed from alternative viewpoints, emphasizing the importance of aligning with the GAN's native latent space.

To enhance efficiency, image encoders have been specifically trained for the inversion task, initially targeting StyleGAN [42; 29; 31; 4; 28; 36; 37], and more recently for EG3D [39; 7; 21]. These specialized encoders capitalize on insights gained from training datasets to swiftly project images into latent spaces. Moreover, they can be trained with diverse objectives beyond mere image reconstruction. For instance, some employ discriminators to compare generated and real images and latent space discriminators to ensure inversion aligns with the GAN's natural latent space. As a result, these methods generally offer faster inversion processes. This study focuses on 3D-GAN inversion, specifically targeting PanoHead . The task poses significant challenges due to PanoHead's ability to capture a comprehensive 360-degree perspective, necessitating the prediction of a substantial portion of invisible elements by inversion encoders. Our experiments demonstrate that models trained for EG3D are ineffective in this context.

## 3 Method

### Overview of PanoHead

The overall architecture of PanoHead is given in Fig. 2. Resemblant to EG3D, PanoHead utilizes a mapping network that takes a random vector \(z\) and the camera conditioning \(_{}\). After \(z\) is mapped to a \(w\), StyleGAN-based backbone \(\) generates a tri-grid triplane. Unlike EG3D, PanoHead's triplanes have 3 times the number of channels in comparison, hence the name tri-grid. This approach is stated to ease 360-degree synthesis. The resultant triplane is then rendered via a volumetric neural renderer \(\) with pose \(_{}\) and super-resolved to yield a synthesized image.

Figure 2: Overall architecture of PanoHead.

### Training an encoder

This section introduces the general pipeline of the encoders employed in our dual-encoder framework. Each encoder takes an input image \(\) and predicts the latent code \(w^{+}\), which is then passed to the generator to produce triplane features. These synthesized features are then fed into the renderer to generate a 2D image with a specified camera parameter \(_{cam}\), as described by Eq. (1):

\[ w^{+}=}()\\ ^{sv}}=((w^{+}),_{cam}) \] (1)

Here, \(^{sv}}\) denotes the output rendering for the same view as the input, and \(}\) represents the encoder. While the \(^{+}\) space allows for leveraging priors embedded in the generator, its limited expressive power in reconstructing image details has been noted due to the information bottleneck of its \(14 512\) dimensions. To address this limitation, both 2D inversion techniques [33; 28] and 3D GAN inversion methods  permit higher-rate features to pass to the generators, facilitating the capture of fine details. In 3D GAN inversion methods, these higher-rate features are encoded through a smaller second network and transmitted to the triplane features. We adopt a similar approach in our encoders. We refer to the final output as \(^{sv}}\). Further details of the architecture are provided in the Appendix.

The primary challenge in this setting arises from establishing appropriate training objective losses, as our training dataset consists solely of single images, providing ground truth only for the rendered image from the same view as the input. For these output and ground-truth pairs, we set the usual reconstruction losses, namely, LPIPS perceptual loss , \(_{2}\) reconstruction loss (MSE), and ArcFace  based identity loss as given in Eq. (2):

\[_{}}_{}(^{sv}}, )+_{2}(^{sv}},)+_ {}(^{sv}},)\] (2)

While models trained with the objective given in Eq. (2) learn to reconstruct a given view, they often struggle to generalize and produce realistic features from other camera views. Consequently, while our first encoder is trained with the objective in Eq. (2), we design an adversarial-based loss objective for our second encoder. This second encoder generates realistic predictions for invisible views, as explained in Section 3.3.

### Occlusion-aware triplane discriminator

To achieve a realistic reconstruction of the 3D model, represented in a triplane structure, it is essential to guide the encoder for visible views and overall coherence. Since we lack one-to-one ground truth

Figure 3: Our training methodology for the triplane discriminator involves generating real samples by sampling latent vectors \(^{+}\) and producing in-domain triplanes using PanoHead. Fake samples are generated from encoded images. Despite the effectiveness of adversarial loss in enhancing reconstructions, challenges may persist in achieving high fidelity to the input due to the origin of real samples from the generator \(\). To address this, we propose an **occlusion-aware discriminator**\(\), trained exclusively with features from occluded pixels. This ensures that visible regions, such as frontal views \(_{R}\), have reduced influence during the training of \(\).

to guide the triplane structure, we experiment with various setups incorporating adversarial losses. A naive approach to utilize adversarial loss would be to render estimated triplanes from other views and assess the realism of these 2D images using a discriminator. However, our experiments observe that this setup hinders the model's ability to learn high fidelity to the input image, as will be further detailed in Section 5.2. Moreover, randomly rendering different views can only guide limited parts of the triplane structure rather than the overall.

To overcome this limitation and avoid the computational burden of rendering unnecessary views, we explore the possibility of training a discriminator in the triplane domain. In our training process for the triplane discriminator, we follow a procedure where we sample latent vectors \(^{+}\) and generate in-domain triplanes using PanoHead, serving as our real samples. Meanwhile, the fake samples are triplanes generated from encoded images, as depicted in Fig. 3. Despite the observed improvement in reconstructions facilitated by this adversarial loss, we note a persistent challenge hindering the network's ability to achieve high fidelity to the input. This discrepancy may stem from the real samples originating from the generator, lacking the detailed feature characteristic of real-world images. Therefore, this may lead the encoder to omit to encode realistic facial details if they are absent in the synthesized samples. We propose our **occlusion-aware discriminator** to overcome this limitation. This discriminator is exclusively trained with features corresponding to occluded pixels. This approach ensures that triplane features associated with visible regions, such as a frontal face, are not utilized for discriminator training. Additionally, we introduce a masking mechanism for synthesized triplanes to mitigate any distribution mismatch arising between encoded and synthesized triplanes. This masking process contributes to aligning the distributions of real and fake samples, further enhancing the coherence of the training dynamics.

We find the set of visible points based on the depth map of the given view via inverse rendering. Specifically, the occlusion mask \(_{_{R}}\) is estimated by Eq. (3):

\[_{_{R}}=^{3}\{[x,y,z]\ :\ _{R} ^{-1}[u,v,1]^{T}\}\] (3)

where \([x,y,z]\) is the triplane coordinates, \(_{R}\) is the extrinsic camera parameters of the input view, \(\) is the depth map from the input view, \(\) is the intrinsic camera parameters, \([u,v,1]\) are the homogeneous coordinates of the input image \(\) rendered from the input view. More clearly, from the current camera pose, we map back to the depth values to obtain the mask of visible regions (\(1\)-\(_{_{R}}\)). Then, we invert the visible region mask to obtain \(_{_{R}}\). The utilization of occlusion masks has been previously investigated in 3D methodologies, albeit in different contexts. For instance, they have been used in generating pseudo-ground truth images to facilitate optimization-based 3D reconstruction  and integrated into passing high-rate residual features to the triplane . However, it is the first time used in the discriminator. This allows for a selective focus on regions where the encoder may encounter challenges in faithfully replicating realism.

Compliant with recent advancements in adversarial training, we follow WGAN loss  for \(_{}\) in Eq. (4), where \(^{}_{}\) and \(_{}\) are encoded and \(^{+}\) synthesized triplanes, respectively. Details are given in the Appendix.

\[_{}}_{}_{}( _{_{R}}^{}_{},_{ _{R}}_{})\] (4)

Figure 4: The inference pipeline with dual encoders for full 3D head reconstruction. Given a face portrait with pose \(_{R}\), we can perform 360-degree rendering from any given pose \(_{}\).

### Dual encoder pipeline

In our approach, we train two encoders: the first, as outlined in Section 3.2, and the second, augmented with an additional adversarial loss detailed in Section 3.3. While the initial encoder excels at reconstructing high-fidelity facial images from the input, it often produces unrealistic results for other viewpoints, as depicted in Fig. 5. Conversely, the second encoder yields better overall outcomes, albeit with slightly diminished fidelity to the input face.

Our aim is to devise a dual-encoder pipeline that harnesses the strengths of both encoded features. To achieve this, we leverage the occlusion masks derived in Section 3.3, as illustrated in Fig. 4. By combining the visible portions from Encoder 1 and the occluded segments from Encoder 2, we generate our final output, as demonstrated in the last row of Fig. 5.

While each encoder contributes partially to the ultimate feature, achieving seamless integration necessitates consistency in the output of both encoders despite their distinct specializations. For instance, if Encoder 1 flawlessly renders a given view of the face but fails to capture the correct geometry, artifacts may arise in the combined result. Thus, it remains imperative to train both encoders comprehensively to ensure an overall high-quality outcome.

## 4 Experimental Setup

### Training

We combined images of FFHQ  and LPFF  and split it for training (\(\)140k) and validation (\(\)14k). CelebA-HQ  and multi-view MEAD  are employed for additional evaluation. We removed face portrait backgrounds for training and evaluation datasets, applied camera and image mirroring during training, and performed pose rebalancing proposed in  as data augmentation. We utilized the same dataset to train competitive methods for fair evaluation. The models are trained for 500k iterations with a batch size of 3 on a single RTX 4090 GPU. The learning rate is \(1e^{-4}\) for both encoders and the occlusion-aware discriminator. Ranger is utilized as the optimizer, which is a combination of Rectified Adam  with Lookahead .

### Baselines

The baseline models are provided in Table 1. We note that no encoder pipelines are aimed for full 360-degree head reconstruction. We train the models with the author's released code to invert images into PanoHead's latent space.

### Evaluation metrics

We report \(_{2}\), LPIPS  and ID  scores for original-view reconstruction, which measure the fidelity to the input image. For the novel-view quality, we measure Frechet inception distance (FID) . Since our validation datasets have limited angle variance, we measure the distance between 1k randomly synthesized and 1k encoded real-life image distributions. The images are rendered from varying yaw angles, covering the 360-degree range to include occluded regions. We

Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.

[MISSING_PAGE_FAIL:7]

GURE:S4.F6][ENDFIGURE]

On the other hand, training the model with an additional adversarial objective that operates on novel images generated using randomly sampled camera parameters improves the FID score but significantly harms the fidelity of the input image. Training a discriminator in the triplane domain and applying adversarial losses from this domain improves overall scores compared to training the discriminator in the 2D image domain. However, as seen in Fig. 7 (third row), the face still lacks high fidelity to the input, and other views are unrealistic. Lastly, using the occlusion-aware triplane discriminator improves identity fidelity and FID scores. The hair looks more natural, similar to the ones generated by the model when sampled from \(z\).

In our framework, we chose to embed images into the \(^{+}\) space. Table 4 presents an ablation study that explores utilizing different projection spaces and various combinations of training data. Training an encoder to project images to the \(\) or \(^{+}\) space, where \(\) is sampled 14 times, results in better FID scores. However, this comes at the cost of high-fidelity reconstruction. Similarly, transitioning to a less constrained \(^{+}\) space enhances fidelity to the input but worsens the enhances fidelity to the input but worsens the FID score. Addressing this challenge necessitates additional measures, such as the proposed dual encoder setup with the occlusion-aware discriminator objective. It is important to note that the distinction between the \(^{+}\) and \(^{+}\) space arises from the camera parameters incorporated into the mapping network. While \(^{+}\) employs various samples of \(\), it adheres to the same set of camera parameters assigned to the mapping network. In contrast, the \(^{+}\) space does not impose

   Train data & Proj. & LPIPS \(\) & ID \(\) & FID \(\) \\  Real img. & \(\) & 0.29 & 0.31 & 45.79 \\ Real img. & \(^{+}\) & 0.22 & 0.60 & 76.54 \\ Real img. & \(^{+}\) & 0.10 & 0.86 & 98.97 \\  \(^{+}\) gens. & \(^{+}\) & 0.27 & 0.25 & 46.93 \\
**Real imgs. + \(^{+}\)** & \(^{+}\) & 0.10 & 0.87 & 89.50 \\   

Table 4: Ablation on training data and latent space.

Figure 6: Comparisons of ours and competing methods.

Figure 7: Qualitative results of ablation on occlusion-aware discriminator \(\).

such constraints during encoding. Given that the real image dataset we use primarily consists of limited camera poses, typically front-view faces, we investigate training the encoder with synthetically generated images from PanoHead. However, solely utilizing synthetic images generated from samples of \(^{+}\) to introduce more diversity compared to \(\) leads to poor performance on real image validation sets regarding reconstruction quality. When combining synthetic and real images, we observe an improvement compared to using them individually.

Figure 8: Left to right: input (\(0^{}\)), reconstruction (\(0^{}\)), GT target (\( 60^{}\)), and render on \( 60^{}\) using the reconstruction triplanes of \(0^{}\) on MEAD dataset.

Figure 9: Inputs (first), reconstructions (second), and \(360^{}\) mesh renders (rest) of our method.

Lastly, we show the results of using the dual encoder in Table 5. The visual results were previously presented in Fig. 5. The dual mechanism leverages the strengths of both encoders, achieving the same LPIPS and ID scores as Encoder 1 while also producing FID scores very similar to those of Encoder 2.

### Editing application

We follow the reference-based editing in  in our pipeline. This method encodes input images, and edits are performed in the triplane space. This approach utilizes the fact that triplanes have a canonical space, allowing for the transfer of local parts from one triplane to another. Fig. 10 demonstrates a successful transfer of hairstyle from a reference image to the target human in 3D. Another advantage of encoder-based models over the optimization ones is the feasibility of such applications. For example, this would not be possible with PTI since the generator is fine-tuned for each sample, preventing the copying of features from one image to another in the encoded feature space.

## 6 Conclusion and Broader Impacts

In summary, this study introduces a 3D GAN inversion framework that projects single images into the latent space of a 3D GAN for accurate 3D geometry reconstruction. While prior encoders excel at synthesizing near-frontal views, they struggle with diverse 3D scenes, motivating our exploration of alternatives. Using PanoHead's 360-degree synthesis, we developed a **dual encoder system** for high-fidelity reconstruction and realistic multi-view generation. A stitching mechanism in the triplane domain ensures optimal predictions from both encoders. With specialized losses, including an **occlusion-aware triplane discriminator**, our framework achieves superior qualitative and quantitative performance over existing methods.

**Broader Impacts.** Our framework has the potential to revolutionize the movie industry, AR, and VR, enabling applications like animating portraits and creating realistic game environments. However, it raises ethical concerns, particularly the risk of "deep fakes". We stress the need for safeguards to ensure the ethical use of this technology.

**Limitations.** We acknowledge that there is room for improvements in the fidelity of images, the realism and 3D-consistency of generations (see Fig. 11, row 2), and the smoothness of the meshes (see Fig. 9). Since the projection is made onto the latent space of PanoHead, our method may not handle out-of-domain or tail samples well (such as images with high-frequency details or accessories). For instance, our method struggles with hats, as demonstrated in the first row of Fig. 11. We recognize that, in certain cases, the artifacts are visible in the back middle of the head and are more noticeable in the mesh rendering as shown in Fig. 9. Additional research is required.

**Acknowledgements.** This work was supported by the BAGEP Award of the Science Academy. We acknowledge EuroHPC Joint Undertaking for awarding the project ID EHPC-AI-2024A02-031 access to Leonardo at CINECA, Italy.

Figure 11: Example failure cases. Inputs (first), reconstructions (second), and novel views.

Figure 10: Hair edits from source image (first) to destination image (second) and \(360^{}\) renders (rest).

   Method & LPIPS \(\) & ID \(\) & FID \(\) \\  \(}\) & 0.10 & 0.87 & 89.50 \\ \(}\) & 0.14 & 0.75 & 64.02 \\ 
**Dual** & 0.10 & 0.87 & 65.44 \\   

Table 5: Ablation on dual-encoder.