# Cross-Domain Policy Adaptation via Value-Guided Data Filtering

Kang Xu\({}^{1}\)\({}^{2}\)  Chenjia Bai\({}^{2}\)  Xiaoteng Ma\({}^{3}\)  Dong Wang\({}^{2}\)  Bin Zhao\({}^{2}\)\({}^{4}\)

Zhen Wang\({}^{4}\)  Xuelong Li\({}^{2}\)  Wei Li\({}^{1}\)

\({}^{1}\) Fudan University \({}^{2}\) Shanghai Artificial Intelligence Laboratory \({}^{3}\) Tsinghua University \({}^{4}\) Northwestern Polytechnical University

Part of this work was done during Kang Xu's internship at Shanghai Artificial Intelligence Laboratory.Correspondence to Chenjia Bai <baichenjia@pjlab.org.cn>, Wei Li <fd_liwei@fudan.edu.cn>.

###### Abstract

Generalizing policies across different domains with dynamics mismatch poses a significant challenge in reinforcement learning. For example, a robot learns the policy in a simulator, but when it is deployed in the real world, the dynamics of the environment may be different. Given the source and target domain with dynamics mismatch, we consider the online dynamics adaptation problem, in which case the agent can access sufficient source domain data while online interactions with the target domain are limited. Existing research has attempted to solve the problem from the dynamics discrepancy perspective. In this work, we reveal the limitations of these methods and explore the problem from the value difference perspective via a novel insight on the value consistency across domains. Specifically, we present the Value-Guided Data Filtering (VGDF) algorithm, which selectively shares transitions from the source domain based on the proximity of paired value targets across the two domains. Empirical results on various environments with kinematic and morphology shifts demonstrate that our method achieves superior performance compared to prior approaches.

## 1 Introduction

Reinforcement Learning (RL) has demonstrated the ability to train highly effective policies with complex behaviors through extensive interactions with the environment [62; 59; 2]. However, in many situations, extensive interactions are infeasible due to the data collection costs and the potential safety hazards associated with domains such as robotics  and medical treatments . To address the issue, one approach is to interact with a surrogate environment, such as a simulator, and then transfer the learned policy to the original domain. However, an unbiased simulator may be unavailable due to the complex system dynamics or unexpected disturbances in the target scenario, leading to a dynamics mismatch. Such a mismatch is crucial for the sim-to-real problem in robotics [1; 38; 51] and may cause performance degradation of the learned policy in the target domain. In this work, we focus on the dynamics adaptation problem, where we aim to train a well-performing policy for the target domain, given the source domain with the dynamics mismatch.

Recent research has tackled the adaptation over dynamics mismatch through various techniques, such as domain randomization [56; 53; 45], system identification , or simulator calibration , that require domain knowledge or privileged access to the physical system. Other methods have exploredthe adaptation problem in specific scenarios, such as those with expert demonstrations [41; 32] or offline datasets [42; 49], while the effectiveness of these methods heavily depends on the optimality of demonstrations or the quality of the datasets. In contrast to these works, we consider a more general setting called _online dynamics adaptation_, where the agent can access sufficient source domain data and a limited number of online interactions with the target domain. We compare the settings for the dynamics adaptation problem in Figure 1.

To address the online dynamics adaptation problem, prior works mainly focus on the single-step dynamics discrepancy and practically eliminating the gap via different ways [17; 14]. However, we empirically demonstrate the limitation of the methods through a motivation example, suggesting their effectiveness heavily relies on strong assumptions about the transferability of paired domains. Theoretically, we formulate the performance bound of the learned policy with respect to the dynamics discrepancy term, which provides an explicit interpretation of the results. To address the problem, we focus on the value discrepancy between paired transitions across domains, motivated by the key idea: _the transitions with consistent value targets can be seen as equivalent for policy adaptation_. Based on the insight, we proposed a simple yet efficient algorithm called Value-Guided Data Filtering (VGDF) for online dynamics adaptation via selective data sharing. Specifically, we use a learned target domain dynamics model to obtain paired transitions based on the source domain state-action pair. The transitions are shared from the source to the target domain only if the value targets of the imagined target domain transition and that of the source domain transition are close. Compared to previous methods that utilize the single-step dynamics gap, our method measures value discrepancies to capture long-term differences between two domains for better adaptation.

Our contributions can be summarized as follows: 1) We reveal the limitations of prior dynamics-based methods and propose the value discrepancy perspective with theoretical analysis. 2) To provide a practical instantiation, we propose VGDF for online dynamics adaptation via selective data sharing. 3) We extend VGDF to a more practical setting with an offline source domain dataset and propose a variant algorithm motivated by novel theoretical results. 4) We empirically demonstrate the superior performance of our method given significant dynamics shifts, including kinematics and morphology mismatch, compared to previous methods.

## 2 Related Work

**Domain adaptation in RL.** Different from domain adaptation in supervised learning where different domains correspond to distinct data distributions , different domains in RL can differ in observation space , transition dynamics [56; 77; 17], embodiment [79; 43], or reward functions [16; 81; 57]. In this work, we focus on domain adaptation with dynamics discrepancies. Prior works utilizing meta RL [76; 48; 55], domain randomization [56; 53; 45], and system identification [80; 77; 15; 74] all assume the access to the distribution of training environments and rely on the hypothesis that the source and target domains are drawn from the same distribution. Another line of work has proposed to handle domain adaptation given expert demonstrations from the target domain [41; 32; 27]. These approaches align the state visitation distributions of the trained policy in the source domain to the distribution of the expert demonstrations in the target domain through state-action correspondences  or imitation learning [28; 21; 72]. However, near-optimal demonstrations can be challenging to acquire in some tasks. More recent works have explored the dynamics adaptation given an offline dataset collected in the target domain [42; 49], while the performance of

Figure 1: Semantic illustration of main settings for dynamics adaptation problem. Methods in the first three categories require different assumptions, such as a wide range of source domains, demonstrations from the target domain, or a manipulable simulator. We focus on a more general setting, online dynamics adaptation, only requiring limited online interactions with the target domain.

the trained policy depends on the quality of the dataset . Orthogonal to these settings, we focus on a general paradigm where a relatively small number of online interactions with the target domain are accessible.

**Online dynamics adaptation.** Given limited online interactions with the target domain, several works calibrate the dynamics of the source domain by adjusting the physical parameters of the simulator [8; 58; 15; 47], while they assume the access of a manipulable simulator. Action transformation methods correct the transitions collected in the source domain by learning dynamics models of the two domains [25; 14; 78]. However, the learned model can be inaccurate, which results in model exploitation and performance degradation [30; 31]. Furthermore, the work that compensates the dynamics gap by modifying the reward function  is practical only if the policy that performs well in both domains exists. Instead, we do not assume the dynamics-agnostic policy exists and demonstrate the effectiveness of our method when such an assumption does not hold.

**Knowledge transfer in RL.** Knowledge transfer has been proposed to reuse the knowledge from other tasks to boost the training for the current task [69; 37]. The transferred knowledge can be modules (_e.g._, policy) [52; 9; 4], representations , and experiences [29; 39; 75; 68]. Our method is related to works transferring experiences. However, prior works focus on transferring between tasks with different reward functions instead of dynamics. When the dynamics changes, the direct adoption of commonly used temporal difference error  or advantage function  in previous works [29; 39; 68] would be inappropriate due to the shifted transition probabilities across domains. In contrast, we introduce novel measurements to evaluate the usefulness of the source domain transitions to tackle the dynamics shift problem specifically.

**Theories on learning with dynamics mismatch.** The performance guarantee of a policy trained with imaginary transitions from an inaccurate dynamics model has been analyzed in prior Dyna-style [64; 65; 67] model-based RL algorithms [44; 30; 61]. The theoretical results inspire us to formulate performance guarantees in the context of dynamics adaptation.

## 3 Preliminaries and Problem Statement

We consider two infinite-horizon Markov Decision Processes (MDP) \(_{src}:=(,,P_{src},r,,_{0})\) and \(_{tar}:=(,,P_{tar},r,,_{0})\) for the source domain and the target domain, respectively. The two domains share the same state space \(\), action space \(\), reward function \(r:\) with range \([0,r_{}]\), discount factor \([0,1)\), and the initial state distribution \(_{0}:\). The two domains differ on the transition probabilities, _i.e._, \(P_{src}(s^{}|s,a)\) and \(P_{tar}(s^{}|s,a)\).

We define the probability that a policy \(\) encounters state \(s\) at the time step \(t\) in MDP \(\) as \(_{,t}^{}(s)\). We denote the normalized probability that a policy \(\) encounters state \(s\) in \(\) as \(_{}^{}(s):=(1-)_{t=0}^{}^{t} _{,t}^{}(s)\), and the normalized probability that a policy encounters state-action pair \((s,a)\) in \(\) is \(_{}^{}(s,a):=(1-)_{t=0}^{}^{t}_{,t}^{}(s)(a|s)\). The performance of a policy \(\) in \(\) as is formally defined as \(_{}():=_{s,a_{}^{}}[r(s,a)]\).

We focus on the online dynamics adaptation problem where limited online interactions with the target domain are accessible, which can be defined as follows:

**Definition 3.1**.: **(Online Dynamics Adaptation)** _Given source domain \(_{src}\) and target domain \(_{tar}\) with different dynamics, we assume sufficient data from the source domain (online or offline) and a relatively small number of online interactions with \(_{tar}\) (e.g., \(:==10\)), hoping to obtain a near-optimal policy \(\) concerning the target domain \(_{tar}\)._

The prior work  also focuses on the online dynamics adaptation problem with online source domain interactions. The proposed algorithm DARC estimates the dynamics discrepancy via learned domain classifiers and further introduces a reward correction (_i.e._, \( r(s,a,s^{})(P_{tar}(s^{}|s,a)/P_{src}(s^{ }|s,a))\)) to optimize policy together with the task reward \(r\) (_i.e._, \(r(s,a)+ r(s,a,s^{})\)), discouraging the agent from dynamics-inconsistent behaviors in the source domain.

## 4 Guaranteeing Policy Performance from a Value Discrepancy Perspective

In this section, we will first present an example demonstrating the limitation of the prior method considering the dynamics discrepancy. Following that, we provide a theoretical analysis of the dynamics-based method to provide an interpretation of the experiment results. Finally, we introduce a novel perspective on value discrepancies across domains for the online dynamics adaptation problem.

### Motivation Example

We start with a 2D grid world task shown in Figure 2 (a), where the agent represented by the red dot needs to navigate to the green square representing the goal. We design source and target domains with different layouts and train a policy to reach the goal successfully in the target domain. We investigate the performance of DARC  that trains the policy with dynamics-guided reward correction and our proposed method (Section 5), using tabular \(Q\)-learning  as the backbone for all methods. Detailed environment settings are shown in Appendix D.

As the empirical state visitations and the learned \(Q\) tables show in Figure 2, DARC is stuck in the room and fails to obtain near-optimal \(Q\)-values, leading to poor performance. Specifically, we circle out four positions where specific actions will lead to the states with a dynamics mismatch concerning the two domains. Due to the introduced reward correction on the source domain transitions with dynamics mismatch, DARC learns overly pessimistic value estimations of particular state-action pairs, which hinders the agent from the optimal trajectory concerning the target domain. However, the values of the following inconsistent states, induced by the particular state-action pairs, are not significantly different concerning the target domain. The value difference quantifies the discrepancy of the long-term behaviors rather than single-step dynamics. Motivated by the value discrepancy perspective, our proposed method (Section 5.1) demonstrates superior performance.

### Theoretical Interpretations and Value Discrepancy Perspective

To provide rigorous interpretations for the results, we derive a performance guarantee for the dynamics-guided methods, which mainly build on the theories proposed in prior methods [30; 17].

**Theorem 4.1**.: **(Performance bound controlled by dynamics discrepancy.)** _Denote the source domain and target domain with different dynamics as \(_{src}\) and \(_{tar}\), respectively. We have the performance difference of any policy \(\) evaluated under \(_{src}\) and \(_{tar}\) be bounded as below,_

\[_{_{tar}}()_{_{src}}()-}{(1-)^{2}}_{_{src}^{}}[D_{ }(P_{src}(|s,a)\|P_{tar}(|s,a))]}_{(a)}.\] (1)

The proof of Theorem 4.1 is given in Appendix B. We observe that the derived performance bound in (1) is controlled by the dynamics discrepancy term (a). Intuitively, the performance difference would be minor when the dynamics discrepancy between the two domains is negligible. DARC  applies

Figure 2: The illustrations and results of the motivation experiment. (a) Illustration of the source and target domains in the grid world environment. The red dot and green square represent the agent and goal, respectively. (b) Visualization of the state visitation in both domains. The darker color suggests higher visitation probabilities. Our method guides the agent to reach regions with high target domain values while the agent trained by DARC is stuck in the room. (c) Visualization of the learned Q tables. Four triangles represent four actions; the darker color suggests a higher value estimation. Our method learns the optimal Q table whose greedy policy leads the agent to the goal of the target domain, while DARC fails due to pessimistic values of the crucial state-action pairs with dynamics mismatch.

the Pinsker's inequality  and derives the following form:

\[_{_{tar}}() _{_{src}}()-}{(1- )^{2}}_{_{src}^{p}}[D_{}( P_{src}(|s,a)\|P_{tar}(|s,a))]}\] \[=_{_{src}}()+}{(1- )^{2}}_{_{src}^{p},P_{src}}[( P_{tar}(s^{}|s,a)/P_{src}(s^{}|s,a))]}.\] (2)

Based on the result in (2), DARC optimizes the policy by converting the second term in RHS to a reward correction (_i.e._, \( r:=(P_{tar}(s^{}|s,a)/P_{src}(s^{}|s,a))\)), leading to the dynamics discrepancy-based adaptation. However, given the transition from the source domain (_i.e._, \(P_{src}(s^{}|s,a) 1\)), the reward correction will lead to significant penalty (_i.e._, \((P_{tar}(s^{}|s,a)/P_{src}(s^{}|s,a)) 0\)) if the likelihood estimation of the transition concerning the target domain is low (_i.e._, \(P_{tar}(s^{}|s,a) 0\)). Consequently, the value estimation of the transition with dynamics mismatch tends to be overly pessimistic as shown in Figure 2 (c), which hinders learning an effective policy concerning the target domain.

Instead of myopically considering the single-step dynamics mismatch, we claim that the transitions with significant dynamics mismatch can be equivalent concerning the value estimations that evaluate the long-term behaviors. Due to the dynamics shift across domains, a state-action pair (_i.e._, \((s,a)\)) would lead to two different next-states (_i.e._, \(s^{}_{src},~{}s^{}_{tar}\)), the paired transitions are nearly equivalent for temporal different learning if the induced value estimations are close (_i.e._, \(|V(s^{}_{src})-V(s^{}_{tar})|\)). Motivated by this, we derive a performance guarantee from the value difference perspective.

**Theorem 4.2**.: **(Performance bound controlled by value difference.)** _Denote source domain and target domain as \(_{src}\) and \(_{tar}\), respectively. We have the performance guarantee of any policy \(\) over the two MDPs:_

\[_{_{tar}}()_{_{src}}()-_{_{_{src}}^{}} _{P_{src}}[V^{}_{_{tar}}(s^{}) ]-_{P_{tar}}[V^{}_{_{tar}}(s^{}) ]}_{}.\] (3)

The proof of Theorem 4.2 is given in Appendix B. The value difference term provides a novel perspective: _the performance can be guaranteed if the transitions from the source domain lead to consistent value targets in the target domain_. The result further highlights the _value consistency_ perspective for the online dynamics adaptation problem.

## 5 Value-Guided Data Filtering

In this section, we propose Value-Guided Data Filtering (VGDF), a simple yet efficient algorithm for online domain adaptation via selective data sharing. Then we introduce the setting with offline source domain data and a variant of VGDF based on novel theoretical results. The pseudocodes are shown in Appendix A, and the illustration of VGDF is shown in Figure 3.

### Dynamics Adaptation by Selective Data Sharing

Inspired by the performance bound proposed in Theorem 4.2, we can guarantee the policy performance by controlling the value difference term in (3). As discussed in Section 4.2, the paired transitions concerning two domains, induced by the same state-action pair, can be regarded as equivalent for temporal difference learning when the corresponding values are close. Thus, we propose to select source domain transitions with minor value discrepancies for dynamics adaptation.

To select rational transitions from the source domain, we need to compare the value differences of paired transitions based on the same source domain state-action pair \((s_{src},a_{src})\). Formally, given a state-action pair \((s_{src},a_{src})\) from the source domain, our objective is to estimate whether the value-difference between \(s^{}_{tar}\) and \(s^{}_{src}\) is sufficiently small, _i.e._,

\[(s_{src},a_{src}):=(V^{}_{_{tar}}( s^{}_{tar})-V^{}_{_{tar}}(s^{}_{src}) ),\] (4)

where \(s^{}_{tar} P_{tar}(|s_{src},a_{src}),~{}s^{}_{src} P _{src}(|s_{src},a_{src})\), \(\) denotes the indicator function and \(\) can be a predefined threshold.

To obtain \((s_{src},a_{src})\), we need to perform policy evaluation over the states to obtain the value estimations given the paired next states (_i.e._, \(s^{}_{src},s^{}_{tar}\)), as formulated in Eq. (4). Monte Carlo (MC)evaluation can provide unbiased values by rolling the policy starting from specific states . However, since the environment is not manipulable, we cannot perform MC evaluation from arbitrary states. Thus, we propose to use an estimated value function for policy evaluation. In this work, we adopt the Fitted Q Evaluation (FQE)  that is widely used in off-policy RL algorithms . Specifically, we utilize a learned Q function \(Q_{}:S A\) for evaluation.

Furthermore, one problem is that the corresponding target domain next state \(s^{}_{tar}\) induced by \((s_{src},a_{src})\) is unavailable in practice. To achieve this, we train a dynamics model with the collected data from the target domain. Following prior works , we employ an ensemble of Gaussian dynamics models \(\{T_{_{i}}(s^{}|s,a)\}_{i=1}^{M}\), in an attempt to capture the epistemic uncertainty due to the insufficient target domain samples. Given the source domain state-action pair \((s_{src},a_{src})\), we generate an ensemble of fictitious states and obtain the corresponding values for each state-action pair, which we call fictitious value ensemble (FVE) \(^{}_{tar}(s_{src},a_{src})\):

\[^{}_{tar}(s_{src},a_{src}):=\{Q_{}(s^{}_{i},a^ {}_{i})|_{s^{}_{i} T_{_{i}}(|s_{src},a_{src}),a^{ }_{i}(|s^{}_{i})}\}_{i=1}^{M}.\] (5)

In practice, the choice of \(\) in Eq. (4) is also nontrivial due to task-specific scales of the values and the non-stationary value function during training. We replace the absolute value difference with the likelihood estimation to address the problem. Specifically, we construct a Gaussian distribution with the mean and variance of FVE denoted as \(((^{}_{tar}(s_{src},a_{src})),(^{}_{tar}(s_{src},a_{src})))\). Estimating the value of the source domain state as \(V^{}_{tar}(s^{}_{src}):=Q_{}(s^{}_{src},a^{}_{src })|_{a^{}_{src}(|s^{}_{src})}\), we introduce Fictitious Value Proximity (FVP) representing the likelihood of the source domain state value in the distribution:

\[(s_{src},a_{src},s^{}_{src}):=(V^{}_{tar}(s^{ }_{src})(^{}_{tar}(s_{src},a_{src})), (^{}_{tar}(s_{src},a_{src}))).\] (6)

Based on the likelihood estimation, we utilize the rejection sampling to select fixed percentage data (_i.e._, \(25\%\)) with the highest likelihood from a batch of source domain transitions at each training iteration. Specifically, we train the value function by optimizing the following objective:

\[*{arg\,min}_{}\,\, _{(s,a,r,s^{}) D_{tar}}[(Q_{}-Q _{})^{2}]+\,_{(s,a,r,s^{}) D _{src}}[(s,a,s^{})(Q_{}-Q_{} )^{2}],\] (7)

where

\[(s,a,s^{}):=((s,a,s^{})>_{6\% }).\]

\(_{6\%}\) is the top \(\)-quantile likelihood estimation of the minibatch sampled from source domain data, \(\) represents the Bellman operator, and \(D_{src},D_{tar}\) denote replay buffers of two domains.

Consider the case when the agent can perform online interactions with the source domain, the training data mostly comes from the source domain, while we aim to train a policy for the target domain. Hence, exploring the source domain is essential to collect transitions that might be high-value concerning the target domain. Thus, we introduce an exploration policy \(^{}\) that maximizes the approximate upper confidence bound of the \(Q\)-value, _i.e._, \(^{}*{arg\,max}_{^{}}\,_{s D_{tar}D_{src}}[Q_{}(s,a)|_{a^{}( |s)}]\), where \(Q_{}(s,a):=\{Q_{_{i}}(s,a)\}_{i=1}^{2}\) under the implementation with SAC  backbone. Importantly, the exploration policy \(^{}\) is separate from the main policy \(\) learned via vanilla SAC. \(^{}\) and \(\) are used for data collection in the source domain and target domain, respectively. The optimistic data collection technique has been proposed for advanced exploration  while we utilize the technique in online dynamics adaptation setting.

Figure 3: Semantic illustration of VGDF. We tackle online dynamics adaptation by selectively sharing the source domain data, and the RL denotes any off-the-shelf off-policy RL algorithm.

### Adaptation with Offline Dataset of Source Domain

So far, we have discussed the setting where the agent can interact with the source domain to collect data actively. Nonetheless, simultaneous online access to the source and target domain might sometimes be impractical. In order to address the limitation, we aim to extend our method to the setting we refer to as _Offline Source with Online Target_, in which the agent can access a source domain offline dataset and a relatively small number of online interactions with the target domain.

To adapt VGDF to such a setting, we propose a novel theoretical result of the performance guarantee:

**Theorem 5.1**.: _Under the setting with offline source domain dataset \(D\) whose empirical estimation of the data collection policy is \(_{D}(a|s):=(s,a)}{_{D}(s)}\), let \(_{src}\) and \(_{tar}\) denote the source and target domain, respectively. We have the performance guarantee of any policy \(\) over the two MDPs:_

\[_{_{tar}}()_{_{src}}()-}{(1-)^{2}}_{_{_{src}}^{*D}, P_{src}}[D_{TV}(_{D}||)]}_{}-_{_{_{src}}^{*D}} [|(s,a)|]}_{},\] (8)

_where \((s,a):=_{P_{src},}[Q_{_{tar}}^{}(s^{ },a^{})]-_{P_{tar},}[Q_{_{tar}} ^{}(s^{},a^{})]\)._

The proof of Theorem 5.1 is given in Appendix B. This theorem highlights the importance of policy regularization and value difference for achieving desirable performance. It is worth noting that the policy regularization term can shed light on the impact of behavior cloning, which has been proven effective for offline RL . Additionally, the value difference term has a similar structure to that of Theorem 3. Thus, we propose a variant called _VGDF + BC_ that combines behavior cloning loss with the original selective data sharing scheme. The pseudocode is shown in Algorithm 2, Appendix A.

## 6 Experiments

In this section, we present empirical investigations of our approach. We examine the effectiveness of our method in scenarios with various dynamics shifts, including kinematic change and morphology change. Furthermore, we provide ablation studies and qualitative analysis of our method. Details of environment settings and the implementation are shown in Appendix D and Appendix E, respectively. Additional results are in Appendix F.

### Adaptation Performance Evaluation

To systematically investigate the adaptation performance of the methods, we construct two types of dynamics shift scenarios, including kinematic shift and morphology shifts, for four environments (_HalfCheetah_, _Ant_, _Walker_, _Hopper_) from Gym Mujoco [71; 7]. We use the original environment as the source domain across all experiments. To simulate kinematic shifts, we limit the rotation angle range of specific joints to simulate the broken joint scenario. As for morphology shifts, we modify the size of specific limbs while the number of limbs keeps unchanged to ensure the state/action space consistent across domains. Full details of the environment settings are deferred to Appendix D.

We compare our algorithm with four baselines: (i) _DARC_ trains the domain classifiers to compensate the agent with an extra reward for seeking dynamics-consistent behaviors; (ii) _GARAT_ trains the policy with an adversarial imitation reward in the grounded source domain via action transformation ; (iii) _IW Clip_ (Importance Weighting Clip) performs importance-weighted bellman updates for source domain samples. The importance weights (_i.e._, \(P_{tar}(s^{}|s,a)/P_{src}(s^{}|s,a)\)) are approximated by the domain classifiers proposed in DARC, and we clip the weight to \([10^{-4},1]\) to stabilize training; (iv) _Finetune_ uses the \(10^{5}\) target domain transitions to finetune the policy trained in the source domain with \(1\) samples. Furthermore, _Zero-shot_ shows the performance of directly transferring the learned policy in the source domain to the target domain, and _Oracle_ demonstrates the performance of the policy trained in the target domain from scratch with \(1\) transitions. We run all algorithms with the same five random seeds. The implementation details are given in Appendix E.1.

As the results in Figure 4 show, our method outperforms _GARAT_ and _IW Clip_ in all environments. _DARC_ demonstrates competitive performance only in the first two environments, while it does not work in other environments. We believe that the assumption of _DARC_ does not hold in the failure cases due to the significant dynamics mismatch. _GARAT_ fails in almost all environments, which webelieve is caused by the impractical action transformation from inaccurate dynamics models. The performance of _Zero-shot_ suggests that the policies trained in the source domains barely work in the target domains due to dynamics mismatch. _Finetune_ achieves promising results and outperforms our method in two of eight environments. We believe that the temporally-extended behaviors of the pre-trained policy benefit learning in the downstream tasks with the assistance of efficient exploration. Nonetheless, our method is the only one that outperforms or matches the asymptotic performance of _Oracle_ in four out of eight environments.

### Ablation Studies

To investigate the impact of design components in our method, we perform ablation analysis on the ratio of transitions \(\), data selection ratio \(\%\), and the optimistic exploration.

**Data ratio \(\).** We employ different ratios of transitions from the source domain versus those from the target domain (\(=5,10,20\)) for variants of our algorithm. The results shown in Figure 5 demonstrate that the performance of our algorithm improves with more source domain transitions when the number of target domain transitions is the same. This finding indicates that VGDF can fully exploit the reusable source domain transitions to enhance the training efficiency concerning the target domain.

**Data selection ratio \(\%\).** We employ different data ratios (\(10\%,25\%,50\%,75\%\)) for the variants of our algorithm. Furthermore, we propose a baseline algorithm _Mix_ that learns with all source domain samples without selection (\((s,a,s^{}) 1\) in Eq. (7)). The results, shown in Figure 6, indicate that our algorithm performs robustly under various ratios within a specific range (_e.g._, \(\% 50\%\)). Surprisingly, _Mix_ performs exceptionally well in environments with kinematic mismatches but fails in scenarios with morphology shifts. We attribute this to the less significant dynamics shift induced by kinematic changes compared to morphology changes.

**Optimistic data collection.** To validate the effect of the optimistic exploration \(^{}\), we introduce a variant of our method without \(^{}\). The results are shown in Figure 7. Removing the optimistic exploration technique results in performance degradation in three out of four environments concerning the sample efficiency, validating the effectiveness of the exploration policy.

### Performance under Offline Source with Online Target

In this subsection, we extend our method to the setting with a source domain offline dataset and limited online interactions with the target domain, investigating the performance of our method without online access to the source domain. We use the D4RL medium datasets  of three environments (_i.e._, _HalfCheetah_, _Walker_, _Hopper_) for evaluation. We compare the proposed _VGDF + BC_ (Section 5.2) with the following baselines: _Offline only_ that directly transfers the offline learned policy via CQL  to the target domain; _Symmetric sampling_ that samples 50% of the data from the target domain replay buffer and the remaining 50% from the source domain offline dataset for each training step; _H2O_ that penalizes the Q function learning on source domain transitions with the estimated dynamics gap via learned classifiers. All algorithms have limited interactions with the target domain to \(10^{5}\) steps. The experimental details are shown in Appendix E.2. The results shown in Table 1 demonstrate that our method outperforms the other methods in four out of six environments, indicating that filtering the source domain data with the value consistency paradigm is effective in the offline-online setting.

### Quantifying Dynamics Mismatch via Fictitious Value Proximity

Although the empirical results suggest that our method can adapt the policy in the face of various dynamics shifts, the degree of the dynamics mismatch can only be evaluated via the adaptation performance rather than be quantified directly. Here, we propose quantifying the dynamics shifts via the proposed Fictitious Value Proximity (FVP) (Section 5.1).

We approximate the FVP in Eq. (5) by calculating the average likelihood of a batch of samples from the source domain by \([(s,a,s^{})]_{(s,a,s^{})} (s,a,s^{})\). We show the approximated FVP in Ant environments with kinematic or morphology shifts in Figure 8. We observe a significant gap between the FVP values of the paired domains, which suggests the target domain with the morphology shifts is "closer" to the source domain than the target domain with the kinematic shifts with respect to the value difference. FVP measured by value differences quantifies the long-term effect on the expected return. Such a measurement can be regarded as a way to quantify the domain discrepancies.

    & Offline only & Symmetric sampling & H2O & VGDF + BC \\  halfcheetah - broken back thigh & \(1128\, 156\) & \(2439\, 390\) & \(\, 148\) & \(4834\, 250\) \\ halfcheetah - no thighs & \(361\, 39\) & \(2211\, 77\) & \(3023\,\,77\) & \(\, 160\) \\  hopper - broken hips & \(155\, 19\) & \(2607\, 181\) & \(2435\, 325\) & \(2785\, 75\) \\ hopper - short feet & \(399\, 5\) & \(2144\, 509\) & \(868\,\,73\) & \(\, 60\) \\  walker - broken right thigh & \(1453\, 412\) & \(709\, 128\) & \(\, 50\) & \(3000\, 388\) \\ walker - no right thigh & \(975\, 131\) & \(872\, 301\) & \(2600\, 355\) & \(\, 306\) \\   

Table 1: Results in the _offline source online target_ setting. We evaluate the algorithms via the performance of the learned policy in the target domain and report the mean and std of the results across five runs with different random seeds.

Figure 8: Quantification analysis of the approximated FVP in Ant environments.

Conclusion

This work addresses the online dynamics adaptation problem by proposing VGDF that selectively shares the source domain transitions from a value consistency paradigm. Starting from the motivation example, we reveal the limitation of the prior dynamics-based method. Then we introduce a novel value discrepancy perspective with theoretical analysis, motivated by the insight that paired transitions with consistent value targets can be regarded as equivalent for training. Practically, we propose VGDF and the variant for the offline source domain setting. Empirical studies demonstrate the effectiveness of our method under significant dynamics gaps, including kinematics shifts and morphology shifts.

**Limitation and future directions.** One limitation of our method is the reliance on the ensemble dynamics models. However, the recent work estimating the epistemic uncertainty with a single model  could be applicable. Furthermore, value-aware model learning  may improve our method by training dynamics models with accurate value predictions of the generated samples. Exploring the effectiveness of value consistency for generalizing across reward functions can be another direction for future research. Finally, validating the effectiveness of the data sharing method in the Sim2Real problem would contribute to the robotics community. The online interaction with the reality system could be risky, recent works  can be integrated for safe online interactions.