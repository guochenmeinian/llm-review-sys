# Scalable 3D Captioning with Pretrained Models

Tiange Luo\({}^{1,*}\) Chris Rockwell\({}^{1,*}\) Honglak Lee\({}^{1,2,}\) Justin Johnson\({}^{1,}\)

\({}^{1}\)University of Michigan \({}^{2}\)LG AI Research

 joint first authorship; \({}^{}\) equal advising

###### Abstract

We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 785k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shap-E, and DreamFusion. Our data, code, and finetuned models can be found at https://cap3d-um.github.io/.

Figure 1: Cap3D provides detailed descriptions of 3D objects by leveraging pretrained models in captioning, alignment, and LLM to consolidate multi-view information. Two views of 3D objects are shown here, Cap3D uses eight. Additional examples are available in Appendix B.

## 1 Introduction

Text-conditioned 3D synthesis [1; 2; 3] could revolutionize the creation process of 3D assets, impacting various sectors, including 3D design, virtual reality , film , robotics [6; 7], and autonomous driving . However, challenges persist, namely the high cost of 3D asset creation and the scarcity of high-quality captions for 3D assets. Obijaverse  takes a step towards this as the first public large-scale 3D object dataset. Unfortunately, while objects contain paired metadata, these do not serve as informative captions, as shown in Table 3. In contrast with 3D, a plethora of high-quality text-image paired data is publicly available [10; 11; 12; 13; 14]. This data has led to incredible recent progress in image-text learning [15; 16; 17; 18], text-conditioned image synthesis [19; 20; 21; 22; 23; 24], and image captioning [25; 26; 27; 28; 29].

In this work, we present Cap3D, a method to automate 3D object annotation. Our key insight is to leverage the abundance of knowledge in pretrained image-text models to remedy the lack of existing 3D-text data. The core of our data collection process is to apply an image captioning model (BLIP2 ) to a set of 3D asset renders, use an image-text alignment model (CLIP ) to filter captions, and apply a language model (GPT4 ) to fuse the filtered captions across views. Critically, the models we apply are pretrained on varied and large-scale text-image [11; 12; 13; 31; 32; 33], and text , data; and approach complementary problems. As a result, each model adds additional value to the framework, as we show in Table 3.

Cap3D is agnostic to 3D asset sources and can be effectively scaled to larger extents with increased 3D assets and computational resources. In this paper, we apply it primarily to Obijaverse, gathering a dataset of 785k 3D-text pairs. Through object rendering and captioning, we enable ethical filtering of 3D objects via both image and text, as detailed in SS 3.2. We publicly release all of our collected data including automated and human-annotated captions, along with associated Point Clouds and Rendered Images, at huggingface.co/datasets/tiange/Cap3D. The dataset is released under ODC-By 1.0 license. We also released trained models and code for replicating the benchmark table.

We validate our collection approach by collecting over 50k crowdsourced captions on over 40k objects. We conduct human evaluations and show on Obijaverse that our automated captions are superior to crowdsourced captions in quality, cost, and speed (Table 1, details in Appendix A). Specifically, it is preferred 35% more often by humans, costs more than 10 times less, and is over 40 times faster, assuming only 8A40 GPUs. We also test the limits of automated captioning. We consider a separate task of captioning geometry (as shown in Figure 1 bottom-right) using ABO, a dataset of 3D models with complex geometries . Shown in Table 4, our automated captioning underperforms humans. However, by formulating description as a question answering task (detailed in SS 3.1), we show stronger performance compared to crowdsourced workers. This result shows the ability of our method to adapt beyond traditional captioning and still be highly competitive.

Finally, our high-quality gathered 3D-text dataset enables us to train and validate large-scale text-to-3D models. In SS5.3, we evaluate several state-of-the-art methods on Obijaverse out-of-the box, including Point\(\)E, Shap-E, DreamFields, and DreamFusion. Finetuning on our data typically shows meaningful improvements, demonstrating the value of the collected dataset. In addition, we show our automatically collected captions yield better finetuning performance than human captions - even at the same scale. At full scale, finetuning is further boosted.

## 2 Related Work

Obtaining 3D-text pairs at scale is challenging, and we take inspiration from image-text datasets and methods when approaching this task.

    & A/B Human Testing & Cost per & Annotation \\  & Win \% (Tie \%) & 1k Objects & Speed \\  Human & 37.8\% \(\) 0.5\% (9.5\%) & \$87.18 & 1.4k / day \\ Cap3D & **52.3\% \(\) 0.5\% (9.5\%)** & **\$8.35** & **65k / day** \\    
    \\  BLIP2 & \$3.79 \\ CLIP & \$0.38 \\ GPT4 & \$4.18 \\  Cap3D Total Cost & **\$8.35** \\   

Table 1: Cap3D is _better, cheaper, and faster_ than crowdsourced annotation. Use 36k responses across 22k objects for A/B testing; 8A40s on a cloud platform for speed and cost computations.

[MISSING_PAGE_FAIL:3]

generates one answer to a prompt asking what object is pictured. The answered object is passed into a second prompt, which asks its structure and geometry, and generates 5 answers.

**Caption Selection:** While BLIP2 often generates high-quality captions, it is not uncommon for samples to contain mistakes, particularly in non-forward facing views such as "yellow cup", in Figure 2 view \(1\), caption \(N\). To reduce the frequency of mistakes, we compute CLIP  ViT-B/32  encodings from each of 5 captions and the associated image, and select the caption maximizing cosine similarity. CLIP tends to select good captions for each view, e.g. Figure 2: view \(1\), BLIP2 caption \(1\) and view \(M\), caption \(1\). CLIP is complementary to BLIP2 as not only does it have different training details and architecture, but it trains on different data. While BLIP2 is trained upon COCO , Visual Genome , CC3M , CC12M , SBU  and LAION400M ; CLIP is trained upon a dataset of 400M images based on frequent text occurrence in Wikipedia.

**Caption Consolidation:** Accumulating information across viewpoints to form a complete picture of 3D objects is challenging, but crucial. We find prompting of GPT4  to summarize the \(M\) captions results in good parsing of the details across captions. By applying GPT4 as the final summary step, it can both include significant details and remove unlikely ones. For example, the final caption in Figure 2 filters the incorrect information, from view 2, "toy ball", while keeping key details, including "handle" and "straw". The alternative order of GPT4 followed by CLIP would result in (1) GPT4 having to make sense of more incorrect input details and (2) CLIP simply selecting between aggregate captions instead of being able to error-correct small mistakes. The effectiveness of introducing GPT4 is verified in ablations (Table 3).

### Ethical Filtering

Captions generated and images rendered by Cap3D enhance the identification and mitigation of legal and ethical issues associated with large-scale 3D object datasets, including identifiable information and NSFW content.

We manage two datasets: Objavverse and ABO. In Objavverse, our main responsibility involves dealing with artist-created assets. These can include identifiable elements such as human face scans and NSFW objects. Objavverse contains approximately 800k objects, which makes the manual verification of each asset impractical. The ABO dataset, on the other hand, is smaller and mostly consists of furniture. We manually ensure the ethical integrity of this dataset.

We exclude objects that lack sufficient camera information for rendering, leaving us with 785k objects. These objects encompass a mix of non-commercial and commercial licenses, such as CC BY-NC-SA, CC BY-NC, CC BY, CC BY-SA, and CC0. We then applied ethical filtering exclusively to objects with commercial-friendly licenses (i.e., CC BY, CC BY-SA, and CC0), resulting in a count of 680k objects.

We next follow prior work  and use a face detector  and NSFW classifier [98; 99] on forward-facing object renders and filter detected objects with score \(>=0.9\). The face detector filters out 18.6k

Figure 2: **Overview of Cap3D. Left to Right: (1) Render 3D objects from \(M=8\) camera angles to capture object details (2) Generate \(N=5\) image captions per rendered image using BLIP2; (3) Select one caption for each image based on its similarity to the image encoding using CLIP; (4) Use GPT4 to consolidate all selected captions into a final, summary of the object.**

objects, and the NSFW classifier filters out 217 objects. Text is also carefully processed. Our final captions are the output of GPT4, which has been trained to filter out inappropriate or harmful content .

We run a standard blocklist  on its output, removing any object-caption pairs including blocked words. This filters out 226 objects. After all the filtering, we are left with 661k objects in the Objavverse dataset. We manually estimate detection precision and recall in Table 2. To summarize, our process detects over 19k objects, of which a nontrivial amount is accurately removed. We estimate roughly 1k face and less than 1k NSFW are missed, using a conservative standard (e.g. missed faces are typically sports cards), resulting in a 660k subset.

## 4 Dataset

We collect captions in two distinct settings: Objavverse, a large and varied dataset of artist-created 3D assets; and ABO, a small dataset of real products, typically furniture.

### Objavverse Captions

Objavverse  features roughly 800k 3D object assets across 21k classes designed by over 100k artists. It is of significantly larger scale than prior work; the paper shows this size enables more diversity by generative 3D models trained upon it. It is released under the ODC-By 1.0 license, permitting subsequent researchers to curate new data from it. Metadata is paired with many assets, however as seen in Figure 3 (right), metadata caption length is frequently short or empty. We collect two caption datasets on Objavverse. First, an automated set of one caption for each of 785k objects using Cap3D (a total of 785k captions). Second, a crowdsourced set of 41.4k captions spanning 39.7k objects for evaluating generated captions. Captions are collected using thehive.ai, a crowdsourced platform similar to AMT. Workers are given instructions with gold-standard sample captions, see the same 8 views as models during captioning, and are routinely monitored. Poor captioning performance results in a ban and deletion of the worker's captions. Crowdsourced captions are also filtered using the blocklist in SS 3.2. Figure 3 (left) shows human captions provide more detail than metadata, but automated captions tend to be most descriptive.

### ABO Geometry Captions

ABO  is a collection of 3D models of Amazon products and is primarily furniture. ABO serves as an important contrast to Objavverse as it consists of a small number of classes varying primarily in geometry. Captioning, therefore, needs to focus more on structure as opposed to semantic category. To emphasize this focus, we consider the task of captioning the geometric structure of objects without color or texture (seen in the bottom right of Figure 1). Like Objavverse, ABO contains metadata that is

    & **Detected** &  &  \\   & **(Filtered)** & **5k** & **(\%)** & **10k** & **680k** \\  Faces & 18.6k & 790 & 16\% & 17 & \(\)1k \\ NSFW & 217 & 102 & 47\% & 12 & \(<\)1k \\ Language \(\) 226 & \(-\) & \(-\) & \(-\) & \(-\) \\    \\ 

Table 2: **Ethical Filtering Analysis.** We manually detect faces and NSFW content to validate automated filtering. 16 of 17 missed face detections were sports cards.

Figure 3: **Objavverse Caption Comparison.** Human captions and Internet metadata frequently contain limited detail. Cap3D captions typically have longer length and more detail. Vocabular size comparisons among Cap3D, BLIP2, and human captions are included in Appendix E.

typically quite short (Table 4), resulting in limited detail. We collect three sets of captions on the 6.4k ABO splits of : crowdsourced (a total of 17.2k captions), captions generated by Cap3D (a total of 6.4k captions), and captions generated by Cap3D (QA) which uses the two-stage prompt captioning (a total of 6.4k captions). Crowdsourced captions follow similar detail to Objaverse with the exception instructions and examples are focused on geometric structure. We compare alternatives in Figure 4. In contrast to Objaverse, human geometric descriptions on ABO are more detailed than captioning. With prompting (QA), the Cap3D pipeline can rival human descriptions.

## 5 Experiments

In this section, we first validate the quality of Cap3D captions against metadata and human-authored captions on both Objaverse and ABO. To verify Cap3D captions are helpful in practice, we next compare text-to-3D models finetuned on both human-authored captions and Cap3D (using the same \(>\)30k set as crowdsourced captions). Finally, we evaluate state-of-the-art text-to-3D models on our captions at scale to measure if finetuning on our captions can improve performance.

### 3D Captioning on Objaverse

**Dataset.** We evaluate caption quality on three subsets of Objaverse: (1) a random set of 22k objects containing a human caption, (2) a random split of 5k objects containing a human caption, and (3) a random 5k split across the entire dataset.

**Baselines.** In data splits (1) and (2), we compare the caption generated by Cap3D with human-authored annotations, _Human_, and existing Objaverse metadata, _Metadata_, described in SS 4.1. Split (1) is used for A/B testing of _Cap3D_ vs. _Human_, as shown in Table 1, at scale. Collecting A/B comparison is expensive, so we compute more extensive experiments on the smaller set (2) in Table 3.

In data split (3), we ablate the main components of Cap3D into _BLIP2_ and _+GPT4. BLIP2_ uses only the image captioning component of our method, taking a front-view rendering and producing a single output caption. _+GPT4_ uses the same image captioning process of our method, producing 5 captions for each of 8 views. However, instead of using CLIP to filter 5 captions from each view, it directly summarizes all 40 captions into a final caption.

**Metrics.** Our primary metric is human judgment A/B tests, where we ask workers to select between two captions on a scale of 1-5, where 3 is a tie. Workers are carefully monitored and each comparison has at least 10k observations across 5k objects.We report mean score, along with the percent each method is preferred (i.e. scores a 4 or 5). We use automated metrics CLIPScore [16; 62], the cosine similarity of CLIP encodings with input images; and ViLT Image and Text Retrieval, which ranks likely image-text pairs, from which one computes precision.

We emphasize CLIPScore is not our primary metric since our captioning model utilizes CLIP. BLIP2 utilizes ViT-L/14 and ViT-g/14, while our filtering uses ViT-B/32, so following previous work  we compute CLIP score using a different model to reduce bias (ViT-B/16). However, we report it as it

Figure 4: **ABO Automated Geometric Description. Left: Human descriptions provide more detailed geometry than automated captions. With careful prompting, _Cap3D (QA)_ can match human-level detail. Right: The high peak of Metadata is cropped, which otherwise obscures other curves.**

has shown a higher correlation with human judgments than other automated metrics . ViLT  is trained on different data and is a different architecture than CLIP, providing an orthogonal metric.

**Results.** We report large scale A/B testing (1) against _Human_ in Table 1, which shows Cap3D is better across metrics, with high confidence. The top three rows of Table 3 use the smaller human-captioned split (2), and demonstrate Cap3D's superior performance over Objavverse metadata and human-authored captions across A/B studies and automated metrics. The bottom three rows of Table 3, studied across a random split of the full dataset (3), reveal that while _BLIP2_ is effective, incorporating multiple views with +_GPT4_ enhances performance. As shown in Figure 5, GPT4 adds detail by consolidating view-specific information. Filtering using +_CLIP (Cap3D)_ mitigates false details by purging subpar captions from GPT input. In addition to reducing errors, utilizing CLIP also reduces GPT input captions from 40 to 8, effectively decreasing token numbers and facilitating a cost reduction from \(\$15.33\) to \(\$4.18\).

### Geometry 3D Captioning on ABO

**Dataset.** We evaluate geometric captioning on a 6.4k object split from ABO [35; 78], comparing Cap3D captions for each object against a maximum of two human-authored ones. To emphasize geometric focus, images used for model input and human assessment are texture-free and colorless.

**Baselines and Metrics.** We use two automated variants from SS3.1: _Cap3D_ and _Cap3D (QA)_, which uses a two-stage prompt captioning to ask more about the input 3D geometry; and compare to crowdsourced human descriptions, _Human_, detailed in SS4.1, and ABO metadata, _Meta_.

Our primary metric of comparison is similar human A/B testing to SS5.1, since automated metrics such as CLIPScore do not accurately represent the distance between fine-grained captions and images as shown in .

**Results.** In stark contrast to Objavverse, _Human_ captions beat automated (_Cap3D_) in Table 4. Automated captions alone contain little geometric detail (e.g., Figure 4), making _Cap3D_ unsuited for this setting. However, by using the two-stage prompt engineering, _Cap3D (QA)_ is preferred to _Human_. Shown in Figure 4, _Cap3D (QA)_ produces significant fine-grained geometric detail as well as longer captions in general. In contrast, _Metadata_ is clearly the weakest baseline.

    &  &  &  &  \\  & Score (1-5) & Win \% &  & Score & R@5 & R@10 & R@5 & R@10 \\  Metadata & 1.74\(\)0.026 & \(10.7 0.7\) & \(83.8 0.8\) & 66.8 & 4.3 & 6.3 & 6.1 & 8.5 \\ Human & 2.86\(\)0.026 & 37.0\(\)1.0 & 46.1\(\)1.0 & 72.5 & 21.2 & 29.0 & 18.5 & 24.9 \\ Cap3D & **-** & **-** & **-** & **88.4** & **35.7** & **46.3** & **34.7** & **44.2** \\   BLIP2 & 2.87\(\) 0.019 & 41.0\(\) 0.7 & 50.6\(\) 0.7 & 83.1 & 24.7 & 32.3 & 21.9 & 29.3 \\ + GPT4 & 2.94\(\) 0.015 & 35.2\(\) 0.6 & 40.8\(\) 0.6 & 86.3 & **31.9** & 39.9 & 30.2 & 38.4 \\ + CLIP (Cap3D) & **-** & **-** & **-** & **86.9** & 31.1 & **40.2** & **30.3** & **38.6** \\   

Table 3: **Objavverse Captions Evaluations. _Cap3D_ outperforms _human_ and _Metadata_; _BLIP2_, _GPT4_, and _CLIP_ are all important to performance. We report 95% confidence interval and use 5k objects.

Figure 5: **Objavverse Caption Ablations**. GPT produces longer and more detailed captions than BLIP2; CLIP tends to prune incorrect details and reduces length slightly.

### Large-Scale Text-to-3D Generation

**Dataset.** We evaluate text-to-3D generation on three subsets of Objaverse: (1) a 30k split of objects containing human-authored captions, to measure if finetuning on Cap3D captions outperform human-authored ones; (2) a 350k split of Objaverse objects paired with Cap3D captions, for finetuning state-of-the-art text-to-3D methods - obtaining high-density point cloud and latent codes to finetune Point-E and Shap-E for all 785k objects is prohibitively expensive (20k GPU days); and (3) a 300 object split for optimization-based baselines, which typically take \(>\)30 mins per object to optimize. Pretrained and Finetuned models are evaluated on 8 views across a held-out test set of 2k objects.

**Methods.** We consider several recent SOTA methods in three general categories: text-to-3D diffusion, cascaded text-to-image then image-to-3D diffusion, and optimization-based. We use the direct text-to-3D variant of _Point-E_, as well as two variants of _Shap-E_: _STF_ and _NeRF_. We use _Stable Diffusion_ cascaded with _Point-E (Im-to-3D)_, adapting _ControlNet_ and _LoRA_ for Stable Diffusion finetuning. We use optimization-based baselines _DreamField_, the publicly available implementation of _DreamFusion_, Stable DreamFusion ; and _3DPuse_, using their implementation based on Karlo .

**Metrics.** We use standard metrics from prior work  to evaluate. Primarily, these are CLIP Score and CLIP R-Precision. CLIP R-Precision ranks a rendered image against all text pairs in the test set by CLIP cosine similarity, and computes precision upon true text-image correspondence. Since we have ground truth images, we calculate the FID  of 3D rendered images against ground truth images, as well as assess CLIP Score on these reference images. We also use ViLT Retrieval R-Precision, used in 5.1, which has the same evaluation procedure as CLIP R-Precision with a different model.

**Results.** Table 5 lists the results of finetuning using human-authored and Cap3D captions. Point-E improves after finetuning upon human captions. However, performance is further improved using our captions on the same dataset; and improved most by training upon the full dataset. This result strongly defends Cap3D captioning at scale. Shap-E does not improve on CLIP metrics after finetuning in any dataset, but performs the least bad on the full dataset using our captions; and FID improves most.

Table 6 presents results from several state-of-the-art pretrained and finetuned models using Cap3D-generated captions. The models finetuned on our captions generally outperform pretrained models under the FID metric. For CLIP-related metrics, the finetuned models of _Point-E (Text-to-3D)_ and _StableDiffusion + Point-E (Im-to-3D)_ also beat their pretrained counterparts. Point-E and Stable Diffusion have been trained on massive datasets, so improvement from finetuning is strong evidence Cap3D captions are effective. The observed downturns in _Shap-E_ could be attributed to at least two factors. First, our replication of their privately-available train code is unstable, often resulting in NaN loss during finetuning. We restart from earlier checkpoints upon crashing, but the result alone is concerning. Second, we exclusively finetune the diffusion model in Shap-E's two-stage approach.

Qualitative results in Figure 6 validate quantitative findings. _Point-E_ and _Stable Diffusion_ baselines show large improvements from finetuning, while _Shap-E_ can better fit the Objaverse data distribution (corresponding to improved FID).

    & A/B & A/B & A/B \\  & Score (1-5) & Win \% & Lose \% \\  Human v. Cap3D & 3.09\(\)0.02 & 47.3\(\)1\% & 41.4\(\)1\% \\ Cap3D(QA) V. Human & 3.08\(\)0.02 & 50.2\(\)1\% & 44.0\(\)1\% \\ Cap3D(QA) V. Cray3D & 3.27\(\)0.02 & 56.0\(\)1\% & 37.4\(\)1\% \\ Cap3D(QA) v. Meta & 4.27\(\)0.02 & 88.2\(\)1\% & 10.0\(\)1\% \\   

Table 4: **ABO Fine-Grained Geometry Cap3D (QA) performs best; crowdsourced beats captioning alone.**

    &  A/B \\ Score (1-5) \\  } &  CLIP \\ Score \\  } &   R-1 \\ R-5 \\  } &  R-1 \\ R-1 \\  } \\  Human v. Cap3D & 3.09\(\)0.02 & 47.3\(\)1\% & 41.4\(\)1\% \\ Cap3D(QA) V. Human & 3.08\(\)0.02 & 50.2\(\)1\% & 44.0\(\)1\% \\ Cap3D(QA) V. Cray3D & 3.27\(\)0.02 & 56.0\(\)1\% & 37.4\(\)1\% \\ Cap3D(QA) v. Meta & 4.27\(\)0.02 & 88.2\(\)1\% & 10.0\(\)1\% \\   

Table 5: **Text-to-3D: Human Captions.** Cap3D captions are better than human on the 30k set. Finetuning on Cap3D full set performs best.

Optimization baselines, shown in Table 7, perform very well upon CLIP-based metrics, consistent with prior work . In fact, _DreamField_ outperforms ground truth images in CLIP metrics. This demonstrates _DreamField_ overfits to the CLIP metric, which is the standard protocol for text-to-3D evaluation. We propose to also consider ViLT precision (see SS5.1). This helps mitigate the bias of CLIP, though _DreamField_ performance on this metric is still strong.

## 6 Limitations and Future Works

As described in SS3, Cap3D consists of four steps: (1) 3D objects rendering; (2) captioning via BLIP2; (3) filtering captions via CLIP; (4) consolidate multiview information via GPT4. To effectively capture comprehensive information through 2D renderings, cameras are strategically placed above or below objects. However, this occasionally results in unconventional 2D views, making BLIP2 susceptible to errors that CLIP fails to rectify. This, in turn, hampers GPT4's ability to merge variegated information across views, culminating in vague and verbose descriptions, as illustrated in Figure 7. The system also falters with certain complex indoor 3D scans, as depicted in Figure 8, thus requiring more robust image-captioning models  and potentially benefiting from additional view incorporations beyond the current eight.

Our method's provision of extensive 3D-Caption pairs for Objaverse  could foster the advancement of 3D-LLM models [109; 110], facilitating 3D-caption centric tasks like captioning, dialog, and language-based navigation. Additionally, the geometric descriptions generated for ABO  enable

    &  &  &  \\  & FID\(\) & CLIP & DLP-R-Precision (2k) &  & CLIP & CLIP &  \\  & & Score & R@1 & R@5 & R@10 & & Score & R@1 & R@5 & R@10 \\  Ground Truth Images & - & 81.6 & 32.7 & 55.1 & 64.3 & - & 81.6 & 32.7 & 55.1 & 64.3 \\  Point\(\)E (Text-to-3D)  & 36.1 & 72.4 & 6.0 & 16.2 & 22.4 & **32.8** & **75.6** & **12.4** & **28.1** & **36.9** \\ S. Diff.  (CNeb) + (Im-to-3D) & 54.7 & 73.6 & 11.0 & 23.4 & 30.0 & **53.3** & **74.6** & **12.4** & **26.2** & **33.8** \\ S. Diff.  (LoLoR) + (Im-to-3D) & 54.7 & 73.6 & 11.0 & 23.4 & 30.0 & **53.7** & **74.4** & **11.6** & **24.6** & **31.4** \\ Shap\(\)E  (STF)  & 37.2 & **80.4** & **20.3** & **39.7** & **48.7** & **35.5** & 79.1 & 20.0 & 38.8 & 47.3 \\ Shap-E  (NeRF)  & 48.7 & **79.4** & **19.0** & **37.7** & **46.8** & **48.2** & 78.1 & 18.3 & 35.1 & 43.5 \\   

Table 6: **Text-to-3D on Objaverse**. Finetuning improves FID over pretrained performance across models. CLIP metrics of _Stable Diffusion_ increase; CLIP metrics of _Point-E_ increase significantly.

    & FID\(\) &  &  \\  & Score & R@1 & R@5 & R@1 & R@5 \\  True Images & - & 83.2 & 53.2 & 77.8 & 41.3 & 69.0 \\  D. Field  & 106.1 & **83.7** & **61.8** & **83.6** & **32.3** & **56.0** \\ D. Fusion  & 127.8 & 72.4 & 28.4 & 46.1 & 23.7 & 45.3 \\
3DWeuse  & **91.1** & 77.0 & 38.6 & 58.5 & 26.3 & 53.0 \\   

Table 7: **Text-to-3D: Optimization Baselines**. Overfitting via CLIP leads to higher CLIP-based scores than ground truth; ViLT score is more fair.

Figure 6: **Text-to-3D results.** Finetuning on Cap3D captions can significantly improve results. Additional examples are available in Appendix C.

compositional structure analysis of fine-grained 3D objects [111; 112]. Our developed method assists in scaling up of 3D-text pairs for expansive 3D datasets .

## 7 Conclusion

In this work, we collect (1) 3D object captions at scale, creating the largest publicly available high-quality 3D-text by an order of magnitude. To do so we propose Cap3D, an automated pipeline leveraging several models pretrained on large datasets, and show design choices are important to performance. In addition, we collect (2) a dataset of geometric captions upon fine-grained 3D objects. This helps analyze shortcomings of automated captioning and study the potential of question answering, while yielding geometric descriptions for 3D assets of real objects paired with real images. These datasets serve as benchmarks for text-to-3D tasks (1) at scale and (2) in geometric detail.

Figure 8: An failed case. The caption under each rendered image are generated by BLIP2 + filtered by CLIP. The inaccurate content are highlighted with colors. The various views contain inaccurate information. The associated details, roughly described, fail to accurately depict the indoor scene.

Figure 7: An failed case. The caption under each rendered image are generated by BLIP2 + filtered by CLIP. The inaccurate content are highlighted with colors. GPT4 + CLIP cannot fix the error generated by BLIP2 and result in a fuzzy description.