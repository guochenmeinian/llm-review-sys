# Aligning Diffusion Behaviors with Q-functions

for Efficient Continuous Control

 Huayu Chen\({}^{1,2}\), Kaiwen Zheng\({}^{1,2}\), Hang Su\({}^{1,2,3}\), Jun Zhu\({}^{1,2,3}\)

\({}^{1}\)Department of Computer Science and Technology, Tsinghua University

\({}^{2}\)Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University

\({}^{3}\)Pazhou Lab (Huangpu), Guangzhou, China

The corresponding author.

###### Abstract

Drawing upon recent advances in language model alignment, we formulate offline Reinforcement Learning as a two-stage optimization problem: First pretraining expressive generative policies on reward-free behavior datasets, then fine-tuning these policies to align with task-specific annotations like Q-values. This strategy allows us to leverage abundant and diverse behavior data to enhance generalization and enable rapid adaptation to downstream tasks using minimal annotations. In particular, we introduce Efficient Diffusion Alignment (EDA) for solving continuous control problems. EDA utilizes diffusion models for behavior modeling. However, unlike previous approaches, we represent diffusion policies as the derivative of a scalar neural network with respect to action inputs. This representation is critical because it enables direct density calculation for diffusion models, making them compatible with existing LLM alignment theories. During policy fine-tuning, we extend preference-based alignment methods like Direct Preference Optimization (DPO) to align diffusion behaviors with continuous Q-functions. Our evaluation on the D4RL benchmark shows that EDA exceeds all baseline methods in overall performance. Notably, EDA maintains about 95% of performance and still outperforms several baselines given only 1% of Q-labelled data during fine-tuning. Code: https://github.com/thu-ml/Efficient-Diffusion-Alignment

## 1 Introduction

Learning diverse behaviors is generative modeling; transforming them into optimized policies is reinforcement learning. Recent studies have identified diffusion policies as a powerful tool for representing heterogeneous behavior datasets [21; 38]. However, these behavior policies incorporate suboptimal decisions in datasets, making them unsuitable for direct deployment in downstream tasks. To get optimized policies, typical methods involve either augmenting the behavior policy with an additional guidance/evaluation network [21; 32; 15] or training a new evaluation policy supervised by the behavior policy [13; 4]. While functional, these methods fail to leverage the full potential of pretrained behaviors as they require constructing some new policy models from scratch. This raises the question: _Can we directly fine-tune pretrained diffusion behaviors into optimized policies?_

Recent advances in Large Language Model (LLM) alignment techniques [57; 37; 43] offer valuable insights for fine-tuning diffusion behavior policies due to the fundamental similarity of the issues they aim to address (Fig. 1). While pretrained LLMs accurately imitate language patterns from web-scale corpus, they also capture toxic or unwanted content within the dataset. Alignment algorithms, such as Direct Preference Optimization (DPO, ), are designed to remove harmful or useless content learned during pretraining. They enable quick adaptations of pretrained LLMs to human intentionsby fine-tuning them on a small dataset annotated with human preference labels. These strategies, due to their simplicity and effectiveness, have seen widespread applications in academia and industry.

Despite the high similarity in problem formulation and the immense potential of LLM alignment techniques, they cannot be directly applied to fine-tune diffusion policy in domains like continuous control. This is primarily because LLMs employ Categorical models to deal with discrete actions (tokens). Their alignment relies on computing data probabilities for maximum likelihood training (Sec. 2.2). However, diffusion models lack a tractable probability calculation method in continuous action space . Additionally, the data annotation method differs significantly between two areas: LM alignment uses binary preference labels for comparing responses, while continuous control uses scalar Q-functions for evaluating actions (Fig. 1).

To allow aligning diffusion behavior models with Q-functions for policy optimization, we introduce Efficient Diffusion Alignment (EDA). EDA consists of two stages: behavior pretraining and policy fine-tuning. During the pretraining stage, we learn a conditional diffusion behavior model on reward-free datasets. Different from previous work which constructs diffusion models as an end-to-end network, we represent diffusion policies as the derivative of a scalar neural network with respect to action inputs. This representation is critical because it enables direct density calculation for diffusion policies. We demonstrate that the scalar network exactly outputs the unnormalized density of behavior distributions, making diffusion policies compatible with existing LLM alignment theories.

During the fine-tuning stage, we propose a novel algorithm that directly fine-tunes pretrained behavior models into optimized diffusion policies. The training objective is strictly derived by constructing a classification task to predict the optimal action using log-probability ratios between the policy and the behavior model. Our approach innovatively expands DPO by allowing fine-tuning on an arbitrary number of actions annotated with explicit Q-values, beyond just the typical binary preference data.

One main advantage of EDA is that it enables fast and data-efficient adaptations of behavior models in downstream tasks. Our experiments on the D4RL benchmark  show that EDA maintains 95 % of its performance and still surpasses baselines like IQL  with just 1% of Q-labelled data relative to the pretraining phase. Besides, EDA exhibits rapid convergence during fine-tuning, requiring only about 20K gradient steps (about 2% of the typical 1M policy training steps) to achieve convergence. Finally, EDA outperforms all reference baselines in overall performance with access to the full datasets. We attribute the high efficiency of EDA to its exploitation of the diffusion behavior models' generalization ability acquired during pretraining.

Our key contributions: 1. We represent diffusion policies as the derivative of a scalar value network to allow direct density estimation. This makes diffusion policies compatible with the existing alignment framework. 2. We extend preference-based alignment methods and propose EDA to align diffusion behaviors with scalar Q-functions, showcasing its vast potential in continuous control.

## 2 Background

### Offline Reinforcement Learning

Offline RL aims to tackle decision-making problems by solely utilizing a pre-collected behavior dataset. Consider a typical Markov Decision Process (MDP) described by the tuple \(,,P,r,\). \(\) is the state space, \(\) is the action space, \(P(}|,)\) is the transition function, \(r(,)\) is the reward function and \(\) is the discount factor. Given a static dataset \(^{}:=\{,,r,}\}\) representing

Figure 1: Comparison between alignment strategies for LLMs and diffusion policies (ours).

interaction history between an implicit policy \(\) and the MDP environment, our goal is to learn a new policy that maximizes cumulative rewards in this MDP while staying close to the behavior policy \(\).

Offline RL can be formalized as a constrained policy optimization problem [26; 35; 53]:

\[_{}_{^{},(|)}Q (,)- D_{}[(|)||(|) ],\] (1)

where \(Q(,)\) is an action evaluation network that can be learned from \(^{}\). \(\) is a temperature coefficient. Previous work [40; 39] proves that the optimal solution for solving Eq. 1 is:

\[^{*}(|)=)}(|)e^{Q(, )/}.\] (2)

In this paper, we focus on how to efficiently learn parameterized policies for modeling \(^{*}\).

### Direct Preference Optimization for Language Model Alignment

Direct Preference Optimization (DPO, ) is a fine-tuning technique for aligning pretrained LLMs with human feedback. Suppose we already have a pretrained LLM model \((|)\), where \(\) represents user instructions, and \(\) represents generated responses. The goal is to align \(_{}\) with some implicit evaluation rewards \(r^{}(,)\) that reflect human preference. Our target model is \(^{*}(|)_{}(|)e^{r^{}(,)/}\).

DPO assumes we only have access to some pairwise preference data \(\{(_{w}>_{l})\}\) and the preference probability is influenced by \(r^{}(,)\). Formally,

\[p(_{w}_{l}|):=}(,_{ w})}}{e^{r^{}(,_{l})}+e^{r^{}(,_{ w})}}=(r^{}(,_{w})-r^{}(,_{l})),\] (3)

where \(\) is the sigmoid function.

In order to learn \(_{}^{*}(|)_{}(|)e^{r ^{}(,)/}\), DPO first parameterizes a reward model using the log-probability ratio between \(_{}\) and \(_{}\), and then optimizes this reward model through maximum likelihood training:

\[_{}=-_{\{,_{w}_{l}\}} (r^{}_{}(,_{w})-r^{}_{ }(,_{l})),\] (4)

\[ r^{}_{}(,):=(|)}{_{}(|)}\]

The key insight behind DPO's loss function is the equivalence and mutual convertibility between the policy model and the reward model. This offers a new perspective for solving generative policy optimization problems by applying discriminative classification loss.

### Diffusion Modeling for Estimating Behavior Score Functions

Recent studies show that diffusion models [45; 20; 49] excel at representing heterogeneous behavior policies in continuous control [21; 5; 38]. To train a diffusion behavior model, we first gradually inject Gaussian noise into action points according to the forward diffusion process:

\[_{t}=_{t}+_{t},\] (5)

where \(t\), and \(\) is standard Gaussian noise. \(_{t},_{t}\) are manually defined so that at time \(t=0\), we have \(_{t}\) = \(\) and at time \(t=1\), we have \(_{t}\). When \(\) is sampled from the behavior policy \((|)\), the marginal distribution of \(_{t}\) at various time \(t\) satisfies

\[_{t}(_{t}|,t)=(_{t}|_{t}, _{t}^{2})(|,t).\] (6)

Intuitively, the diffusion training objective predicts the noise added to the original behavior actions:

\[_{}_{t,,,(|)} [\|_{}(_{t}|,t)-\|_{2}^{2} ]_{_{t}=_{t}+_{t}}.\] (7)

More formally, it can be proved that the learned "noise predictor" \(_{}\) actually represents the _score function_\(_{_{t}}_{t}(_{t}|,t)\) of the diffused behavior distribution \(_{t}\):

\[_{_{t}}_{t}(_{t}|,t)=-^{*}(_ {t}|,t)/_{t}-_{}(_{t}|,t)/ _{t}.\] (8)

With such a score-function estimator, we can employ existing numerical solvers [46; 33] to reverse the diffusion process, and sample actions from the learned behavior policy \(_{}\).

## 3 Method

We decompose the policy optimization problem into two stages: behavior pretraining (Sec. 3.1) and policy alignment (Sec. 3.2).

### Bottleneck Diffusion Models for Efficient Behavior Density Estimation

Recent advances in alignment techniques cannot be readily applied to continuous control tasks. Their successful applications in LLM fine-tuning require two essential prerequisites:

1. A powerful foundation model capable of capturing diverse behaviors within datasets.
2. A tractable probability calculation method that allows direct density estimation (Eq. 4).

Language models primarily deal with discrete actions (tokens) defined by a vocabulary set \(\), and thus employ Categorical models. This modeling method enables easy calculation of data probability through a softmax operation and is capable of representing _any_ discrete distribution. In contrast, for continuous action space, direct density estimation is not so feasible. Diffusion policies only estimate the gradient field (a.k.a., score) of data density instead of the density value itself , making it impossible to directly apply LLM alignment techniques [41; 9; 3]. Conventional Gaussian policies have a tractable probability formulation but lack enough expressivity and multimodality needed to accurately model behavior datasets [52; 13; 5], and catastrophically fail in our initial experiments.

To address the above limitation of diffusion models, we propose a new diffusion modeling technique to enable direct density estimation. Normally, a conditional diffusion policy \(_{}(a_{t}|s,t): ^{||}\) maps noisy actions \(a_{t}\) to predicted noises \(^{||}\). In our approach, we redefine \(_{}\) as the derivative of a scalar network \(f_{}(a_{t}|s,t):\) with respect to input \(a_{t}\):

\[_{}(a_{t}|s,t):=-_{t}_{a_{t}}f_{}(a_{t}|s,t).\] (9)

Given that \(f_{}\) is a parameterized network, its gradient computation can be conveniently performed by auto-differential libraries. The new training objective for \(f_{}\) can then be reformulated from Eq. 7:

\[_{}_{}()=_{t,,(, )^{}}[\|_{t}_{a_{t}}f_{}(a_{t}|s, t)+\|_{2}^{2}]_{_{t}=_{t}+_{t} }.\] (10)

As noted by , with unlimited model capacity, the optimal solution for solving Eq. 10 is:

\[^{*}(_{t}|,t)=-_{t}_{_{t}}_ {t}(_{t}|,t) f^{*}(a_{t}|s,t)=_{t}(_{ t}|,t)+C(,t).\] (11)

An illustration is provided in Figure 2 (left). Intuitively, our proposed modeling method first compresses the input action into a scalar value with one single dimension. Then, this bottleneck value is expanded back to \(^{||}\) through back-propagation. We thus refer to \(f_{}\) as _Bottleneck_ Diffusion Models (BDMs).

Figure 2: Algorithm overview. **Left:** In behavior pretraining, the diffusion behavior model is represented as the derivative of a scalar neural network with respect to action inputs. The scalar outputs of the network can later be utilized to estimate behavior density. **Right:** In policy fine-tuning, we predict the optimality of actions in a contrastive manner among \(K\) candidates. The prediction logit for each action is the density gap between the learned policy model and the frozen behavior model. We use cross-entropy loss to align prediction logits \( f_{}:=f_{}^{}-f_{}^{}\) with dataset Q-labels.

The primary advantage of BDMs is their ability to efficiently estimate behavior densities in a single forward pass. Moreover, BDMs are fully compatible with existing diffusion-based codebases regarding training and sampling procedures, inheriting their key benefits such as training stability and model expressivity. BDMs can also be viewed as a diffused version of Energy-Based Models (EBMs, ). We refer interested readers to Appendix A for a detailed discussion.

### Policy Optimization by Aligning Diffusion Behaviors with Q-functions

In this section, our goal is to learn a new policy \(_{}_{}e^{}\) by fine-tuning the previously pretrained behavior policy \(_{}\) on a new dataset annotated by an existing Q-function \(Q(,)\). We show that this policy optimization problem can actually be transformed into a simple classification task for predicting the optimal action among multiple candidates. We elaborate on our method below.

**Dataset construction.** For any state \(\), we draw \(K>1\) independent action samples \(^{1:K}\) from \(_{}(|)\). Assume we already have an \(Q\)-function \(Q(,)\) that evaluates input state-action pairs in scalar values, our dataset is formed as \(^{f}:=\{,^{1:K},Q(,^{k})|_{k 1:K}\}\).

**Action optimality.** We first introduce a formal notion of action optimality. We draw from the control-as-probabilistic-inference framework  and define a random optimality variable \(_{K}\), which is a one-hot vector of length \(K\). The \(k\)-th index of \(_{K}\) being 1 indicates that \(^{k}\) is the optimal action within \(K\) action candidates \(^{1:K}\). We have

\[p(_{K}^{k}=1|,^{1:K})=,^{k})}}{ _{i=1}^{K}e^{Q(,^{i})}}.\]

The optimality probability of a behavioral action \(\) is proportional to the exponential of its Q-value, aligning with the optimal policy definition \(^{*}(|)(|)e^{Q(,)/}\).

**From policy optimization to action classification.** We construct a classification task by predicting the optimal action among \(K\) candidates. This requires learning a Q-model termed \(Q_{}\) first. Drawing inspiration from DPO (Sec. 2.2), we parameterize \(Q_{}\) as the log probability ratio between \(_{}\) and \(_{}\):

\[Q_{}(,):=(|)}{_{ }(|)}+ Z(),\]

Figure 3: Experimental results of EDA in 2D bandit settings at different diffusion times. **Column 1:** Visualization of diversified behavior datasets. Each dot represents a two-dimensional behavioral action. Its color reflects the actionâ€™s Q-value. **Column 2 & 3:** Density maps of the action distribution as estimated by the pretrained or fine-tuned BDM models. The density for low-Q-value actions has been effectively decreased after fine-tuning. **Column 4:** The predicted action Q-values, calculated by Eq. 13, align with dataset Q-values in Column 1. See appendix B for complete results.

This parameterization allows us to directly optimize \(_{}\) during training because \(_{}(|)=)}_{}(|)e^{Q_{ }(,)/}\) constantly holds. Since \(Q\)-values in datasets define the probability of being the optimal action, the training objective can be derived by applying cross-entropy loss:

\[_{}_{}()=_{(,^{1:K}) f}_{k=1}^{K},^{k})}}{ {_{i=1}^{K}e^{Q(,^{i})}}_{}}(^{i}|)}{_{}(^{i}|)} Z ()}}{_{i=1}^{K}e^{(^{i}|)}{ _{}(^{i}|)} Z()}}_{}.\] (12)

The unknown partition function \(Z()\) automatically cancels out during division. \(\) is a hyperparameter that can be tuned to control how far \(_{}\) deviates from \(_{}\).

**Expanding to bottleneck diffusion behavior.** A reliable behavior density estimation of \(_{}(|)\) is critical and is a main challenge for applying Eq. 12. Our initial experiments tested with Gaussian policies drastically failed and even underperformed vanilla behavior cloning. This highlights the necessity of adopting a much more powerful generative policy, such as the BDM model (Sec. 3.1).

Diffusion policies define a series of distributions \(_{t}(_{t}|,t)=(_{t}|_{_{t}}_{ 0},_{t}^{2})_{0}(_{0}|,t)_{0}\) at different timesteps \(t\), rather than just a single distribution \(=_{0}\). Consequently, instead of directly predicting the optimal action given raw actions \(^{1:K}\), we perturb all actions with \(K\) independent Gaussian noises according to the diffusion forward process by applying \(_{t}^{k}=_{t}^{k}+_{t}^{k}\). Then we predict action optimality given \(K\) noisy action \(_{t}^{1:K}\):

\[Q_{}(,_{t},t):= (_{t}|,t)}{_{t,}( _{t}|,t)}+ Z()\] \[= [f_{}^{}(_{t}|,t)-f_{}^{}( _{t}|,t)]+[ Z(,t)-C^{}(,t)+C^{}(,t)],\] (13)

Similar to Eq. 12, all unknown terms above automatically cancel out in the training objective:

\[_{}_{f}()=_{t,^{1:K},(,^{1:K})f}_{k=1}^{K},^{k})}}{_{i=1}^{K}e^{Q(,^{i})}}}_{}^{}(_{t}^{k}|,t) -f_{}^{}(_{t}^{k}|,t)]}}{_{i=1}^{K}e^{[f_{}^ {}(_{t}^{i}|,t)-f_{}^{}(_{t}^{i}|,t)]}}}_{ }.\] (14)

**Proposition 3.1**.: _(Proof in Appendix C) Let \(f_{}^{*}\) be the optimal solution of Problem 14 and \(_{t,}^{*} e^{f_{}^{*}}\) be the optimal diffusion policy. Assuming unlimited model capacity and data samples, we have the following results:_

_(a) Optimality Guarantee._ _At time_ \(t=0\)_, the learned policy_ \(_{}^{*}\) _converges to the optimal target policy._

\[_{}^{*}(|)=_{t=0,}^{*}(|) _{}(|)e^{Q(,)/}\]

_(b) Diffusion Consistency._ _At time_ \(t>0\)_,_ \(_{t>0,}\) _models the diffused distribution of_ \(_{}^{*}\)_:_

\[_{t,}^{*}(|,t)=(_{t}|_{t},_{t}^{2})_{}^{*}(_{0}|)_{0}\]

_asymptotically holds when_ \(K\) _and_ \(=1\)_, satisfying the definition of diffusion process (Eq._ 6_)._

**Fine-tuning Efficiency.** In practice, the policy and the behavior model share the same architecture, so we can initialize \(=\) to fully exploit the generalization capabilities acquired during the pretraining phase. This fine-tuning technique allows us to perform policy optimization with an incredibly small amount of \(Q\)-labelled data (e.g., 10k samples) and optimization steps (e.g., 20K gradient steps). These are less than 5% of previous approaches (Sec. 5.2).

## 4 Related Work

### Diffusion Modeling for Offline Continuous Control

Recent advancements in offline RL have identified diffusion models as an effective approach for behavior modeling [38; 16], which excels at representing complex and multimodal distributionscompared with other modeling methods like Gaussians [54; 55; 24; 35; 53] or VAEs [11; 26; 12]. However, optimizing diffusion models can be a bit more tricky due to the unavailability of a tractable probability calculation method [5; 51]. Existing approaches include learning a separate guidance network to guide the sampling process during evaluation [21; 32], applying classifier-free guidance [1; 7], backpropagating sampled actions through the diffusion policy model to maximize Q-values [52; 22], distilling new policies from diffusion behaviors [13; 4], using rejection sampling to filter out behavioral actions with low Q-values [5; 15] and applying planning-based techniques [30; 36; 16]. Our proposed method differs from all previous work in that it directly fine-tunes the pretrained behavior to align with task-specific annotations, instead of learning a new downstream policy.

### Preference-based Alignment in Offline Reinforcement Learning

Existing alignment algorithms are largely preference-based methods. Preference-based Reinforcement Learning (PbRL) assumes the reward function is unknown, and must be learned from data. Previous work usually applies inverse RL to learn a reward model first and then uses this reward model for standard RL training [19; 57; 28; 44; 34]. This separate learning of a reward model adds complexity to algorithm implementation and thus limits its application. To address this issue, recent work like OPPO , DPO , and CPL  respectively proposes methods to align Gaussian or Categorical policies directly with human preference. Despite their simplicity and effectiveness, these techniques require calculating model probability, and thus cannot be applied to diffusion policies. Existing diffusion alignment strategies [7; 56; 2; 51] are incompatible with mainstream PbRL methods. Our work effectively closes this gap by introducing bottleneck diffusion models. We also extend existing PbRL methods to align with continuous Q-functions instead of just binary preference data.

## 5 Experiments

We conduct experiments to answer the following questions:

* How does EDA perform compared with other baselines in standard benchmarks? (Sec. 5.1)
* Is the alignment stage data-efficient and training-efficient? How many training steps and annotated data does EDA require for aligning pretrained behavior models? (Sec. 5.2)
* Does value-based alignment outperform preference-based alignment? (Sec. 5.3)
* How do contrastive action number \(K\) and other choices affect the performance? (Sec. 5.4)

  
**Environment** & **Dataset** & **BCQ** & **CQL** & **IQL** & **DT** & **D-Diffuser** & **IDQL** & **Diffusion-QL** & **QOPO** & **BBM (Ours)** \\  HalfChereth & Medium-Expert & \(64.7\) & \(91.6\) & \(86.7\) & \(86.8\) & \(90.6\) & \(\) & \(\) & \(\) & \(\) \\ HalfCheath & Medium & \(04.7\) & \(44.0\) & \(47.4\) & \(42.6\) & \(49.1\) & \(51.0\) & \(51.1\) & \(54.1\) & \(\) \\ HalfCheath & Medium-Replay & \(38.2\) & \(45.5\) & \(44.2\) & \(36.6\) & \(39.3\) & \(45.9\) & \(47.8\) & \(47.6\) & \(\) \\  Hopper & Medium-Expert & \(100.9\) & \(105.4\) & \(91.5\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(104.9 7.4\) \\ Hopper & Medium & \(54.5\) & \(58.5\) & \(66.3\) & \(67.6\) & \(79.3\) & \(65.4\) & \(90.5\) & \(\) & \(\) \\ Hopper & Medium-Replay & \(33.1\) & \(95.0\) & \(94.7\) & \(82.7\) & \(\) & \(92.1\) & \(\) & \(\) & \( 10.0\) \\  Walker2d & Medium-Expert & \(57.5\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Walker2d & Medium & \(53.1\) & \(72.5\) & \(78.3\) & \(74.0\) & \(82.5\) & \(82.5\) & \(\) & \(\) & \(\) \\ Walker2d & Medium-Replay & \(15.0\) & \(77.2\) & \(73.9\) & \(66.6\) & \(75.0\) & \(85.1\) & \(\) & \(\) & \(89.2 5.5\) \\   & \(59.9\) & \(77.6\) & \(76.9\) & \(74.7\) & \(81.8\) & \(82.1\) & \(\) & \(\) & \(\) \\  AntMarre & Umaze & \(73.0\) & \(74.0\) & \(87.5\) & \(59.2\) & - & \(\) & \(\) & \(\) & \(\) \\ AntMarre & Umaze-Diverse & \(61.0\) & \(84.0\) & \(62.2\) & \(\) & - & \(\) & \(66.2\) & \(74.4\) & \(\) \\ AntMarre & Medium-Play & \(0.0\) & \(61.2\) & \(71.2\) & \(0.0\) & - & \(\) & \(76.6\) & \(\) & \(\) \\ AntMarre & Medium-Diverse & \(8.0\) & \(53.7\) & \(70.0\) & - & \(\) & \(78.6\) & \(\) & \(\) \\   & \(35.5\) & \(68.2\) & \(72.7\) & \(28.1\) & - & \(\) & \(78.7\) & \(\) & \(\) \\  Kitchen & Complete & \(8.1\) & \(43.8\) & \(62.5\) & - & - & - & \(\) & \(62.8\) & \(\) \\ Kitchen & Partial & \(18.9\) & \(49.8\) & \(46.3\) & - & \(57.0\) & - & \(60.5\) & \(\) & \(\) \\ Kitchen & Mixed & \(8.1\) & \(51.0\) & \(51.0\) & - & \(\) & - & \(\) & \(45.5\) & \(\) \\   & \(11.7\) & \(48.2\) & \(53.3\) & - & - & - & \(\) & \(58.1\) & \(\) \\    & \(39.7\) & \(69.7\) & \(71.4\) & - & - & - & \(\) & \(\) & \(\) \\   

Table 1: Evaluation results of D4RL benchmarks (normalized according to ). We report mean \(\) standard deviation of algorithm performance across 5 random seeds at the end of training. Numbers within 5 % of the maximum are highlighted.

### D4RL Evaluation

**Benchmark.** In Table 1, we evaluate the performance of EDA in D4RL benchmarks . All evaluation tasks have a continuous action space and can be broadly categorized into three types: MuJoCo locomotion are tasks for controlling legged robots to move forward, where datasets are generated by a variety of policies, including expert, medium, and mixed levels. Antmaze is about an ant robot navigating itself in a maze and requires both low-level control and high-level navigation. FrankaKitchen are manipulation tasks containing real-world datasets. It is critical to faithfully imitate human behaviors in these tasks.

**Experimental setup.** Throughout our experiments, we set the contrastive action number \(K=16\). For each task, we train an action evaluation model \(Q_{}(,)\) for annotating behavioral data during the fine-tuning stage. Implicit Q-learning  is employed for training \(Q_{}\) due to its simplicity and orthogonality to policy training. We compare with other critic training methods in Figure 4. The rest of the implementation details are in Appendix D.

**Baselines.** We mainly consider diffusion-based RL methods with various optimization techniques. Decision Diffuser employs classifier-free guidance for optimizing behavior models. QGPO employs energy guidance. IDQL does not optimize the behavior policy and simply uses rejection sampling during evaluation. Diffusion-QL has no explicit behavior model, but instead adopts a diffusion regularization loss. We also reference well-studied conventional algorithms for different classes of generative policies. BCQ features a VAE-based policy. DT has a transformer-based architecture. CQL and IQL targets Gaussian policies.

**Result analysis.** From table 1, we find that EDA surpasses all referenced baselines regarding overall performance and provides a competitive number in each D4RL task. To ascertain whether this improvement stems from the alignment algorithm rather than from a superior critic model or policy class, we conduct additional controlled experiments. As outlined in Figure 4, we evaluate three variants of EDA using different Q-learning approaches and compare these against both diffusion and Gaussian baselines. The experimental results highlight the superiority of diffusion policies and further validate the effectiveness of our proposed method.

### Fine-tuning Efficiency

The success of the pretraining, fine-tuning paradigm in language models is largely due to its high fine-tuning efficiency, allowing pretrained models to adapt quickly to various downstream tasks. Similarly, we aim to explore data efficiency and training efficiency during EDA's fine-tuning phase.

To investigate EDA's data efficiency, we reduce the training data used for aligning with pretrained Q-functions by randomly excluding a portion of the available dataset (Figure 5 (a)). We compare this with IQL, which uses the same Q-model as EDA but extracts a Gaussian policy via weighted regression. We also compare with QGPO, which shares our pretrained diffusion behavior models

Figure 4: Average performance of EDA combined with different Q-learning methods in Locomotion tasks.

Figure 5: Aligning pretrained diffusion behaviors with task Q-functions is fast and data-efficient.

but employs a separate guidance network to augment the behavior model during evaluation, instead of directly fine-tuning the pretrained policy. Experimental results reveal that EDA is significantly more data-efficient than the QGPO and IQL baselines. Notably, EDA maintains about 95% of its performance in locomotion tasks when using only 1% of the Q-labeled data for policy fine-tuning. This even surpasses several baselines that use the full dataset for policy training.

In Figure 5 (b), we plot EDA's performance throughout the fine-tuning phase. EDA rapidly converges in roughly 20K gradient steps, a negligible count compared to the typical 1M steps used for behavior pretraining. Note that behavior modeling and task-oriented Q-definition are largely orthogonal in offline RL. The high fine-tuning efficiency of EDA demonstrates the vast potential of pretraining on large-scale diversified behavior data and quickly adapting to individual downstream tasks.

### Value Optimization v.s. Preference Optimization

A significant difference between EDA and existing preference-based RL methods such as DPO is that EDA is tailored for alignment with scalar Q-values instead of just preference data.

For preference data without explicit Q-labels, we can similarly derive an alignment loss:

\[_{}_{f}^{}()=_{t,^{1:K },(,^{1:K})^{f}}^{}(_{t}^{w}|,t)-f_{}^{}(_{t}^{w}|,t)] }}{_{i=1}^{K}e^{[f_{}^{}(_{t}^{i}|,t)-f_{}^{ }(_{t}^{i}|,t)]}}.\] (15)

Here \(^{w}\) represents the most preferred action among \(^{1:K}\). In practice, we select \(^{w}\) as the action with the highest Q-value but abandon the absolute number. We'd like to note that when \(K=2\), the above objective becomes exactly the DPO objective (Eq. 4) in preference learning:

\[_{}_{f}^{}()=_{t,^{\{w, l\}},,^{w}^{l}}[f_{}^{ }(_{t}^{w}|,t)-f_{}^{}(_{t}^{w}|,t)]-[f _{}^{}(_{t}^{l}|,t)-f_{}^{}(_{t}^{l}|, t)]\] (16)

We ablate different choices of \(K\) and compare our proposed value-based alignment with preference methods in 9 D4RL Locomotion tasks (Figure 6). Results show that EDA generally outperforms preference-based alignment approaches. We attribute this improvement to its ability to exploit the Q-value information provided by the pretrained Q-model. Besides, we notice the performance gap between the two methods becomes larger as \(K\) increases. This is expected because preference-based optimization greedily follows a single action with the highest Q-value. However, as more action candidates are sampled from the behavior model, the final selected action will have a higher probability of being out-of-behavior-distribution data. This further hurts performance. In contrast, EDA is a softer version of preference learning. This leads to greater tolerance for \(K\).

### Ablation Studies

We study the impact of varying temperature \(\) on algorithm performance in Appendix E. We also illustratively compare our proposed diffusion models with other generative models for behavior modeling in 2D settings in Appendix A.

## 6 Conclusion

We propose Efficient Diffusion Alignment (EDA) for solving offline continuous control tasks. EDA allows leveraging abundant and diverse behavior data to enhance generalization through behavior pretraining and enables rapid adaptation to downstream tasks using minimal annotations. Our experimental results show that EDA exceeds numerous baseline methods in D4RL tasks. It also demonstrates high sample efficiency during the fine-tuning stage. This indicates its vast potential in learning from large-scale behavior datasets and efficiently adapting to individual downstream tasks.

Figure 6: Ablation of action numbers \(K\) and optimization methods.