# Efficient and Private Marginal Reconstruction with Local Non-Negativity

Brett Mullins\({}^{1}\) Miguel Fuentes\({}^{1}\) Yingtai Xiao\({}^{2}\) Daniel Kifer\({}^{2}\)

**Cameron Musco\({}^{1}\) Daniel Sheldon\({}^{1}\)**

\({}^{1}\)University of Massachusetts, Amherst \({}^{2}\)Penn State University

{bmullins,mmfuentes,cmusco,sheldon}@cs.umass.edu

{yxx5224,duk17}@psu.edu

###### Abstract

Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people. Many differentially private algorithms for query release and synthetic data contain steps that reconstruct answers to queries from answers to other queries that have been measured privately. Reconstruction is an important subproblem for such mechanisms to economize the privacy budget, minimize error on reconstructed answers, and allow for scalability to high-dimensional datasets. In this paper, we introduce a principled and efficient postprocessing method ReM (Residuals-to-Marginals) for reconstructing answers to marginal queries. Our method builds on recent work on efficient mechanisms for marginal query release, based on making measurements using a _residual query basis_ that admits efficient pseudoinversion, which is an important primitive used in reconstruction. An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers. We demonstrate the utility of ReM and GReM-LNN by applying them to improve existing private query answering mechanisms.

## 1 Introduction

Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people such as the 2020 US Decennial Census . One of the most fundamental problems in differential privacy is answering a workload of linear queries. Linear queries are used for basic descriptive statistics like counts and sums, and as building blocks for more complex tasks. Marginal queries, which describe the frequency distribution of subsets of discrete variables (e.g., income by age and education), are of particular interest as descriptive statistics and for use in downstream tasks like regression analyses.

A key subproblem in linear query answering is _reconstruction_. Given a workload of linear queries, most mechanisms select a different set of queries to measure to make the most efficient use of the privacy budget, and then use the noisy answers to reconstruct answers to workload queries . Effective reconstruction methods can combine information from all noisy measurements to provide mutually consistent answers to workload queries.

Computational complexity is a key challenge for reconstruction methods. These methods answer workload queries by--either explicitly or implicitly--reconstructing a data distribution that has size exponential in the number of variables. To scale to high-dimensional data sets, existing approaches must represent this distribution compactly through some form of parametric representation , which introduces tradeoffs such as a restricted space of data distributions that can be represented [8,, non-convex optimization objectives to find the best representation [9; 10; 11], or complexity that depends on the measured queries and is still exponential in the worst case .

We introduce ReM (residuals-to-marginals), a principled and scalable post-processing method to reconstruct answers to a workload of marginal queries from noisy measurements of _residuals_. Residuals are a class of linear queries that are related to marginals, which were recently introduced in the privacy literature  but previously studied in statistics [13; 14]. ReM uses a compact representation of the data distribution to produce workload answers without exponential complexity in the number of variables. ReM builds on the reconstruction approach of ResidualPlanner , which utilizes Kronecker structure to efficiently perform pseudoinverse operations. ReM is a flexible framework for performing reconstruction in a broad range of settings and it can be used with a variety of existing query-answering mechanisms. ReM also extends to the common setting of reconstructing answers to marginal queries from a set of noisy marginal measurements with isotropic Gaussian noise. In this case, ReM performs the standard pseudoinverse reconstruction and is the first method to do so efficiently. We also develop GReM-LNN (Gaussian ReM with local non-negativity), an extension that reconstructs marginals satisfying non-negativity, which often reduces error on reconstructed answers.

We demonstrate the utility of ReM and GReM-LNN by showing that they significantly reduce error and enhance the scalability of existing private query answering mechanisms including ResidualPlanner  and the multiplicative weights exponential mechanisms (MWEM) . Our code is available at https://github.com/bcmullins/efficient-marginal-reconstruction.

## 2 Preliminaries

We consider a sensitive tabular dataset \(\) of records \(x^{(1)},,x^{(N)}\). Each record \(x=(x_{1},,x_{d})\) consists of \(d\) categorical attributes. The \(i\)th attribute \(x_{i}\) belongs to the finite set \(_{i}\) of size \(n_{i}\). The data universe is \(=_{i=1}^{d}_{i}\) and has size \(n=_{i}n_{i}\). The _data vector_ or _data distribution_\(p^{n}\) is a vector indexed by \(\) that counts the occurrences of each record in \(\); it has entries \(p(x)=_{i=1}^{N}[x^{(i)}=x]\). Since \(n\) is exponential in the data dimension \(d\), it is computationally intractable to work directly with data vectors in high dimensions.

### Linear queries, marginals, and residuals

Linear queries are a rich class of statistics that include counts, sums, and averages and are used as building blocks for more complex tasks. A linear query is the sum of a real-valued function \(q:\) applied to each record in the dataset. We adopt the equivalence that a query is a vector \(q^{n}\) with answer \(q^{}p\). A _query matrix_ or _workload_\(W\) is a collection of \(m\) linear queries arranged row-wise in an \(m n\) matrix. The answer to workload \(W\) for data vector \(p\) is given by \(Wp\).

_Marginal queries_ are a common type of linear query for high-dimensional data. They count the number of records that match certain values for a subset of the attributes - e.g., the number of people in a dataset with education at least a college degree and income $50-$100K. Let \([d]\) be a subset of attributes and \(x_{}=(x_{i})_{i}\) be the corresponding subvector of a record \(x\). Further, let \(_{}=_{i}_{i}\) and \(n_{}=_{i}n_{i}\). The _marginal_\(_{}^{n_{}}\) has entries \(_{}(t)=_{i=1}^{N}[x_{}^{(i)}=t]\) that count the number of occurrences in the dataset for each setting \(t_{}\) of the attributes in \(\). Let \(M_{}^{n_{} n}\) be the _marginal workload_ so that \(_{}=M_{}p\). As shown in Fig. 0(a), \(M_{}\) can be written concisely as a Kronecker product over dimensions, with base matrices equal to the identity \(I_{k}^{n_{k} n_{k}}\) for attributes in \(\) and the all ones vector \(1_{k}^{}^{1 n_{k}}\) for attributes not in \(\). Kronecker product matrices can be understood as applying different linear operations along each dimension of a multi-dimensional array. In this case \(M_{}\) sums over dimensions of the array representation of \(p\) for attributes not in \(\). We provide a brief summary of Kronecker products and their relevant properties in Appendix A.

_Residual queries_ are class of linear queries closely related to marginals. They were recently introduced in the privacy literature  but previously studied in statistics as variable _interactions_[13; 14]. For \([d]\), the \(\)_-residual_ is obtained from the marginal \(_{}\) by applying a differencing operator along each dimension. Let \(D_{(k)}\) be the linear operator that computes successive differences for vectors of length \(n_{k}\), i.e., \((D_{(k)}v)_{i}=v_{i+1}-v_{i}\) for \(i=1,,n_{k}-1\); an example is shown for \(n_{k}=3\) in Fig. 0(b). Let \(D_{}\) be the matrix that applies this operation to all attributes in the \(\)-marginal as shown in Fig. 1c. The residual workload can be written as \(R_{}=D_{}M_{}^{m_{} n}\) where \(m_{}=_{i}(n_{i}-1)\), which has the explicit Kroecker product form shown in Fig. 1d.1 With these definitions, if \(_{}=M_{}p\) is the \(\)-marginal, the \(\)-residual is \(_{}=D_{}_{}=R_{}p\) and can be computed from either \(_{}\) or \(p\).

Residuals and marginals have an intricate structure. The \(\)-marginal is uniquely determined by the \(\)-residuals for \(\), i.e., there is an invertible linear transformation between \(M_{}\) and \((R_{})_{}\) (a vertical block matrix). Intuitively, a \(\)-residual contains information _not_ contained in the \(\)-marginals for \(\). Further, the row spaces of \(R_{}\) and \(R_{^{}}\) are orthogonal for any \(^{}\), and the row spaces of \(M_{}\) and \(R_{}\) are orthogonal when \(\). Along with Kronecker structure, the orthogonality of residuals is the key property we will leverage to perform efficient reconstruction.

A key advantage of residual workloads is that we can work with their pseudoinverses efficiently in certain situations even though they have exponential size. Let \(Q^{+}\) denote the Moore-Penrose pseudoinverse of \(Q\). The following proposition builds on the reconstruction method in  and will be used to reconstruct answers to a marginal query \(M_{}\) from measurements for a collection of residuals.

**Proposition 1**.: _Let \(R_{}=(R_{})_{}\) be a combined workload of residual queries for all \(\) in a collection \( 2^{[d]}\), where the individual matrices \(R_{}\) are stacked vertically. The size of \(R_{}\) is \(m n\) where \(m=_{}m_{}\). Then for any \(z=(z_{})_{}^{m}\) and any \(\), it holds that_

\[M_{}R_{}^{+}z=_{,} A_{,}z_{},A_{,}:=_{k=1}^{d}D_{(k)}^{+}&k\\ }1_{k}&k\\ 1&k.\]

_The matrix \(A_{,}\) has size \(n_{} m_{}\) and maps from the space of \(\)-residuals to the space of \(\)-marginals. The running time to compute \(A_{,}z_{}\) is \((||n_{})\)._

The proof of this result appears in Appendix C. The analysis of time complexity appears in Appendix E.

### Differential Privacy

When releasing the results of any analysis performed on sensitive data, particular care needs to be taken to avoid leaking private information contained in the dataset. Differential privacy is a mathematical criterion that bounds the effect of any individual in the dataset on the output of a mechanism, which is satisfied by adding noise to the computation. This allows for formal quantification of the privacy risk associated with any release of information.

**Definition 1**.: (Differential Privacy; ) Let \(:\) be a randomized mechanism. For any neighboring datasets \(,^{}\) that differ by adding or removing at most one record, denoted \(^{}\), and all measurable subsets \(S\): if \((() S)()(( ^{}) S)+\), then \(\) satisfies \((,)\)-approximate differential privacy, denoted \((,)\)-DP.

A fundamental property of differential privacy relevant to our work is the post-processing property, which states that transformations of differentially private outputs that do not access the sensitive dataset \(\) maintain their privacy guarantees. Formally:

**Proposition 2** (Post-processing; ).: _Let \(_{1}:\) satisfy \((,)\)-DP and \(f:\) be a randomized algorithm. Then \(:=f_{1}\) satisfies \((,)\)-DP._

The reconstruction methods we propose in this paper are post-processing algorithms that take as input a set of noisy linear query answers and, thus, inherit the privacy guarantees from those noisy answers. Note that the present analysis is largely agnostic to the model of differential privacy used.

Figure 1: Kronecker structure of workloads.

We discuss variants of differential privacy and privacy guarantees for query answering in Appendix B.

### Private query answering

In private query answering, we are given a _workload_ of linear queries \(W^{m n}\). We seek to approximate the answers \(Wp\) as accurately as possible while satisfying differential privacy. A general recipe for private query answering is _select-measure-reconstruct_. _Data-independent_ mechanisms following this recipe such as the various matrix mechanisms [2; 3; 4; 5; 6] select and measure a set of queries \(Q\) and reconstruct answers to \(W\). _Data-dependent_ mechanisms following this recipe such as MWEM  and various synthetic data mechanisms [7; 9; 10; 18; 19] typically maintain a model \(\) of the data distribution \(p\) that is improved iteratively by repeating the steps of select-measure-reconstruct and adaptively measuring queries that are poorly approximated by the current model \(\). The key idea is that it is often possible to obtain lower error by measuring a different set of queries \(Q\) than \(W\) and then using answers to \(Q\) to reconstruct answers for \(W\). In this paper, we focus on the reconstruction subproblem and propose methods applicable to both the data-independent and data-dependent settings.

### Query answer reconstruction

Reconstruction is a central subproblem to query answering. Suppose \(y=Qp+\) is the a set of measurements. To reconstruct a data distribution, we seek \(\) such that \(Q y\). One method is to set \(=Q^{+}y\) where \(Q^{+}\) is the Moore-Penrose pseudoinverse. This method is used in the matrix mechanism  and HDMM  but is not tractable in high dimensions. One contribution of our proposed method is to demonstrate that this pseudoinverse reconstruction is tractable when the query matrix \(Q\) is a set of marginal measurements and \(\) is isotropic Gaussian noise. Other reconstruction methods such as Private-PGM  and those used by the mechanisms PrivBayes , GEM , RAP , and RAP++  represent \(\) through a parametric representation. These (usually) ensure tractability in high dimensions by using a compact representation, but introduce different tradeoffs. The parametric assumption typically restricts the space of data distributions that can be represented [8; 9; 10; 11]. Optimizing over the parameteric representation is often non-convex, potentially leading to suboptimal optimization [9; 10; 11]. Private-PGM solves a convex optimization problem and is closest to the methods of this paper. However its complexity depends on the measured queries and is still exponential in the worst case ; our methods will not have exponential complexity.

We note that all of these above reconstruction methods, and the methods presented in this work, only depend on the dataset through the noisy query answers and, thus, satisfy the same degree of privacy as the answers by the post-processing property of differential privacy (Proposition 2).

## 3 Efficient Marginal Reconstruction from Residuals

In this section, we discuss methods for reconstructing answers to a workload of marginal queries given measurements of residuals. These methods utilize the structure of marginals and residuals to make reconstruction tractable and minimize error. Let \( 2^{[d]}\) and \(M_{}=(M_{})_{}\) be the combined workload of marginals for all of the attribute sets in \(\) (e.g., all pairs or triples of attributes). Similarly, let \(R_{}=(R_{})_{}\) represent a set of residual queries for all \(\) in a collection \(\). Our goal is to estimate the marginal query answers \(M_{\!V\!P}\) from noisy measurements \(z=R_{}p+\).

```
0: Marginal workload \(\), \(=^{}\), measurements \(z_{}=R_{}p+(0,_{})\) for \(\)
1: Reconstruct \(_{}=_{}A_{,}z_{}\) for \(\) ```

**Algorithm 1** ResidualPlanner reconstruction

ResidualPlanner.ResidualPlanner  solves this problem elegantly in the matrix mechanism (i.e. data-independent) setting under Gaussian noise. Let \(^{}=\{:\}\) denote the _downward closure_ of \(\). When \(=^{}\), the residual queries for \(\) uniquely determine the marginals for \(\), i.e., there is an invertible linear transformation between \(M_{}\) and \(R_{}\). This yields the reconstruction approach in Alg. 1. We suppose the residual queries \(R_{}\) are measured with Gaussian noise to yield \(z_{}\). In Line 1, the marginals are reconstructed by applying the invertible transformation from residuals to marginals. This reconstruction is equivalent to setting \(_{}=M_{}\) where \(=R_{}^{+}z\) and \(z=(z_{})_{}\) by Proposition 1.

The full ResidualPlanner algorithm additionally chooses each \(_{}=_{}^{2}D_{}D_{}^{}\) such that the resulting algorithm _optimally_ answers the marginal workload indexed by \(\) to minimize error under a natural class of convex loss functions for a given privacy budget . That this can be done efficiently for a broad class of error metrics for marginal workloads is significant given the computational challenges that are often faced when attempting to optimally select measurements and reconstruct workload answers in high dimensions.

A general approach to reconstruction.We propose a reconstruction algorithm that, like the one in ResidualPlanner, is efficient and principled, but that applies in more general settings. Reconstruction in ResidualPlanner uses the invertible transformation from residuals to marginals. This restricts to the case where the measured queries _exactly_ determine the workload queries in the absence of noise. To address the full range of applications, it is important to address the cases where workload queries are overdetermined, underdetermined, or both.

Our proposed algorithm, ReM, is shown in Alg. 2. Compared to ResidualPlanner, the main differences are: (1) the set \(\) of measured residuals is arbitrary, (2) a residual query can be measured any number of times with any noise distribution, (3) an optimization problem is solved for each \(\) to estimate the true residual query answer \(_{} R_{}p\), (4) reconstruction uses the estimated residuals \(_{}\) instead of the noisy measurements \(z_{}\). The loss function \(L_{}(_{})\) in Line 2 captures how well \(_{}\) explains the entire set of noisy measurements \(\{z_{,i}\}_{i=1,,k_{}}\). For example, a typical choice is \(L_{}(_{})=-_{i=1}^{k_{}} p(z_{,i}|R_{}p= _{})\), the negative log-likelihood of the measurements.

The following result shows that solving the optimization problems in Line 1 is equivalent to finding a compact representation of a data distribution \(\) that minimizes a global reconstruction loss and then using \(\) to answer each marginal query.

**Theorem 1**.: _Suppose \(_{}\) minimizes \(L_{}(_{})\) over \(^{m_{}}\) for each \(\) and let \(=(_{})_{}\). Then Alg. 2 outputs \(_{}=M_{}\), where \(=R_{}^{+}\) is a global minimizer of the combined loss function \(_{}L_{}(R_{}p)\) over \(^{n}\)._

This result is proved (in Appendix D) by showing that \(R_{}=_{}\) for all \(\), and thus \(\) optimizes each individual loss function \(L_{}\), and so must be a global minimizer. Proposition 1 then shows that \(_{}=M_{}=M_{}R_{}^{+}\) has the form given in Line 2 of the algorithm.

## 4 Applications of ReM under Gaussian Noise

In this section, we apply ReM to reconstruct answers to marginal queries in various settings: (1) we reconstruct from residuals measured with Gaussian noise, (2) we reconstruct from marginals measured with isotropic Gaussian noise, and (3) we reconstruct non-negative answers from residuals measured with Gaussian noise.

### Reconstruction under Gaussian noise

An instance of ReM that allows for efficient computation is when residuals are measured with Gaussian noise i.e., \(z_{,i}=R_{}p+_{,i}\) where \(_{,i}(0,_{,i})\) and the loss function \(L_{}(_{})\) is the negative log-likelihood of the measurements. In this case, \(=(_{})_{}\) is the maximum likelihood estimate of the residual answers \(=(_{})_{}\). We refer to this setting as GReM-MLE (Gaussian ReM with Maximum Likelihood Estimation), shown in Alg. 3.

The loss function \(L_{}(_{})\) is a sum of quadratic forms given by \(L_{}(_{})=_{i=1}^{k_{}}(_{}-z_{,i})^{} _{,i}^{-1}(_{}-z_{,i})\). In this setting, the optimization problems in Line 1 of Alg. 2 have the closed-form solution \(_{}=_{i}_{,i}^{-1}^{-1}_{i} _{,i}^{-1}z_{,i}\), which is a form of inverse-variance weighting and can be verified by setting the gradient of the loss function to zero.

GReM-MLE improves computational tractability by reducing Alg. 2 to operations on matrices. Moreover, if the covariances among measurements of residual \(R_{}\) differ only by a constant for \(\), i.e., \(_{,i}=_{,i}^{2}K_{}\) where \(_{,i}\), then \(_{}\) can be computed as a weighted average given by \(_{}=(_{i}_{,i}^{-2})^{-1}_{i}_{, i}^{-2}z_{,i}\). All instances of GReM-MLE considered throughout the paper satisfy this assumption of proportional covariances for each \(\).

### Reconstruction from marginals

A common practice in existing mechanisms is to measure marginal queries with isotropic Gaussian noise . In this special case, the measurements can be converted to an equivalent set of residual measurements with independent Gaussian noise, allowing us to apply GReM-MLE.

The key observation is that a marginal query answer \(_{}=M_{}p\) for attribute set \(\) can be used to derive residual answers \(_{}=R_{}p\) for each \(\) via the following Lemma (proved in Appendix D):

**Lemma 1**.: _For \(\), the residual \(R_{}\) can be recovered from the marginal \(M_{}\) as_

\[R_{}=A_{,}^{+}M_{}\;\;A_{,}^{+}= _{k=1}^{d}D_{(k)}&k\\ 1^{T}_{k}&k\\ 1&k.\]

Whereas \(A_{,}\) maps answers from residual \(R_{}\) to answers to marginal \(M_{}\), the matrix \(A_{,}^{+}\) maps answers from marginal \(M_{}\) to residual \(R_{}\). Furthermore, \(_{}\) can be reconstructed from the set of all residuals \((_{})_{}\), so these residuals carry equivalent information to the marginal. Additionally, when the marginal is observed with isotropic noise as \(y_{}=M_{}p+(0,_{}^{2}I)\), the corresponding noisy residuals \(A_{,}^{+}z_{}\) are independent. As a consequence, we can decompose a noisy marginal measurement into a set of equivalent and independent noisy residual measurements.

**Theorem 2**.: _Let \(y_{}(M_{}p,^{2}I)\) be a noisy marginal measurement with isotropic Gaussian noise and let \(z_{}=A_{,}^{+}y_{}\) for each \(\). Then noisy residual \(z_{}\) has distribution \((R_{}p,^{2}D_{}D_{}^{}_{k }n_{k})\) and \(z_{}\) is independent of \(z_{^{}}\) for \(^{}\)._

_Furthermore, let \(H_{}=(A_{,}^{+})_{}\) be the matrix mapping from \(y_{}\) to \((z_{})_{}\). This matrix is invertible, which implies that_

\[(y_{}|M_{}p,^{2}I)=_{} z_{}\,\,R_{}p,\,^{2}D_{}D_{ }^{}_{k}n_{k}+| H_{}|.\] (1)

Given a collection of noisy marginal measurements, we can apply the above decomposition to obtain a set of independent noisy residuals with proportional covariances. To reconstruct marginal answers, we can apply GReM-MLE to the residuals. Alg. 4 shows this decomposition and reconstruction. Equation (1) shows that the noisy residual measurements and noisy marginal measurements are equivalent from the perspective of finding the best data vector \(p\) by maximum likelihood, because the log-likelihood of the residual measurements differs from the log-likelihood of the marginal measurement by a constant \(| H_{}|\) that is independent of \(p\), and measurements of marginals are each independent. A maximum likelihood estimate of \(p\) from the marginal measurements \(y\) is given by using the pseudoinverse of the measured workload to map noisy marginal measurements to a data vector. The following result shows that the method in Alg. 4 is equivalent to answering queries from this maximum likelihood estimate of the data vector given the marginal measurements when the marginals are measured with the same noise scale.

**Theorem 3** (Efficient pseudoinversion of marginal query matrix).: _Let \(M_{}=(M_{})_{}\) be the query matrix for a multiset \(\) of marginals and let \(y=(y_{})_{}\) be corresponding noisy marginal measurements with \(y_{}=M_{}p+(0,^{2})\). Let \(=\{:\}\) and for each \(\) let \(_{,i}\) be the \(i\)th marginal in \(\) containing \(\). Let \(z_{,i}=A^{+}_{_{,i},}y_{_{,i}}\) be the residual measurement obtained from \(_{,i}\) and let \(_{,i}=_{,i}^{2}D_{}D_{}^{}\) be its covariance where \(_{,i}^{2}=^{2}_{k_{,i}}n_{k}\). Then, given any workload of marginal queries \(\), for each \(\), the marginal reconstruction \(_{}\) obtained from Algorithm 3 on these residual measurements is equal to \(M_{}M_{}^{+}y\)._

This result can be generalized to allow for differing noise scales between marginal measurements. We prove this result and discuss the generalized form of Theorem 3 in Appendix D.

### Reconstruction with local non-negativity

It is often possible to improve accuracy of a differentially private mechanism by forcing its outputs to satisfy known constraints [4; 10; 21]. For our problem, true marginals are non-negative, so it is desirable to enforce non-negativity in their private estimates. To enforce non-negativity, instead of solving the separate problems in Line 1 of Alg. 2, we solve the following combined problem over the full vector \(=(_{})_{^{i}}\) of residuals:

\[_{}_{}L_{}(_{})_{}A_{,}_{} 0, .\] (2)

Reconstruction of marginals then proceeds as in Line 2 of Alg. 2. The constraints in Eq. (2) ensure that the reconstructed marginals will be non-negative. We refer to this as _local non-negativity_, since this problem solves for a data distribution \(\) that is non-negative for marginals in \(\) rather than a data distribution with non-negative entries.

A natural setting to apply local non-negativity to ReM is under Gaussian noise with covariance \(_{,i}=_{,i}^{2}D_{}D_{}^{}\) and \(_{,i}^{2}\). Recall that marginals measured with isotropic Gaussian noise decompose into residuals with the above covariance structure. Our proposed application of local non-negativity in the Gaussian noise setting GReM-LNN (Gaussian ReM with local non-negativity) solves Eq. (2) for \(L_{}(_{})=_{i=1}^{k_{}}(_{}-z_{,i})^{} K_{,i}^{-1}(_{}-z_{,i})\) and \(K_{,i}=2^{||}D_{}D_{}^{}\). In the GReM-LNN setting, Eq. (2) is an convex program with linear constraints. Our implementation solves this problem using a scalable dual ascent algorithm (described in Appendix F) but could be solved in principle using standard optimizers, given sufficient resources . With respect to the loss function \(L_{}(_{})\), adopting \(2^{||}\) rather than Gaussian noise scale \(_{,i}^{2}\) is a heuristic that weights lower degree residual queries such as the total query and 1-way residuals more heavily than higher degree queries such as 3-way residuals. In contrast, using the Gaussian noise scale \(_{,i}^{2}\) obtained from both ResidualPlanner and the marginal decomposition in Theorem 2 weights higher degree residualqueries more than lower degree residuals. When enforcing local non-negativity, it is beneficial for reducing reconstruction error to allocate more weight to residuals that affect more marginals through reconstruction. The present choice of weights \(2^{||}\) for GReM-LNN, however, remain a heuristic. We discuss this point further in Section 6.

### Computational Complexity

We summarize the complexity results in Table 1. Formal statements and proofs appear in Appendix E. Let \(\) be the set of measured residuals. To understand the results, suppose that \(R_{}\) is measured once, given by \(z_{}\), for each \(\). Recall from Proposition 1 that computing \(A_{,}z_{}\) takes \((||n_{})\) time. This operation maps from the space of \(\)-residuals to \(\)-marginals. To reconstruct an answer to the marginal query \(M_{}\), we apply the invertible transformation from residuals to marginals by summing over contributions for each \(\) to yield \(_{}=_{:}A_{, }z_{}\). In the worst case, this requires computing \(A_{,}z_{}\) for \(2^{||}\) residuals. Then the running time of reconstructing an answer to marginal \(M_{}\) is \((||n_{}2^{||})\). If \(\) is a workload of marginals, then reconstructing answers to each \(\) is \((_{}||n_{}2^{||})\). The following result shows that the complexity of reconstructing an answer to marginal \(M_{}\) is almost linear with respect to domain size.

**Theorem 4**.: _For \(>0\), reconstructing an answer to \(M_{}\) is \(o(n_{}^{1+})\) as \(n_{i}\) for some \(i\)._

GReM-MLE, given in Alg. 3, consists of two steps: estimating residual answers \(_{}\) from residuals answers \(z_{,i}\) for \(i=1,,k_{}\) and each \( S\), and reconstructing answers to marginal workload \(\). Recall that we suppose that covariance is proportional among measurements of a given residual \(R_{}\), so \(_{}\) can be computed in closed-form as a weighted average in \((n_{})\) time. Then GReM-MLE takes \((_{}||n_{}2^{||})\) time. The efficient marginal pseudoinversion, given in Alg. 4, first decomposes marginals and then applies GReM-MLE. Computing \(A_{,}^{+}y_{}\) takes \((||n_{})\) time, so the running time of decomposing the marginal measurements is \((_{}||n_{}2^{||})\) where \(\) is the set of marginals measured with isotropic Gaussian noise. Then the efficient marginal pseudoinversion is \((_{}||n_{}2^{||})\). Additionally, one round of GReM-LNN, given in Alg. 6, is \((_{}||n_{}2^{||})\).

## 5 Experiments

In this section, we measure the utility of GReM-MLE and GReM-LNN by incorporating them as a post-processing step into two mechanisms for privately answering marginals: (1) ResidualPlanner , and (2) a data-dependent mechanism we call Scalable MWEM. Both mechanisms measure queries with Gaussian noise and reconstruct answers to all three-way marginals for the given data domain. For the ResidualPlanner experiment, we measure residuals for all subsets of three or fewer attributes with Gaussian noise scales determined by ResidualPlanner. For the Scalable MWEM experiment, we measure the total query and a subset of the 3-way marginals in the data domain with isotropic Gaussian noise and reconstruct answers to all 3-ways marginals using the efficient marginal pseudoinversion in Alg. 4. We fully describe Scalable MWEM in Appendix G.

We compare average \(_{1}\) error with respect to the reconstructed marginals of the base mechanism to post-processing with GReM-LNN and two heuristics that enforce non-negativity by truncating negative values to zero (Trunc) and truncating to zero then rescaling (Trunc+Rescale). For the Scalable MWEM experiment, we additionally compare to a well-studied reconstruction mechanism Private-PGM . We run these methods on four datasets of varying size and scale, Titanic , Adult ,

 
**Method** & **Running Time** \\  GReM-MLE(\(,,z\)) & \((_{}||n_{}2^{||})\) \\  EMP(\(,,y\)) & \((_{}||n_{}2^{||})\) \\  One Round of GReM-LNN(\(,,z\)) & \((_{}||n_{}2^{||})\) \\  

Table 1: Summary of Complexity ResultsSalary , and Nist-Taxi , and various practical privacy regimes, \(\{0.1,0.31,1,3.16,10\}\) and \(=1 10^{-9}\). For each setting, we run five trials and report the average error of each method as well as minimum/maximum bands. Additional details are provided in Appendix H.

### ResidualPlanner Results

Fig. 2 displays results for the ResidualPlanner experiment. Across all privacy budgets and datasets considered, GReM-LNN significantly reduces workload error on the reconstructed marginals compared to ResidualPlanner. Averaging over all settings and trials, GReM-LNN reduces ResidualPlanner workload error by a factor of 44.0\(\). With respect to the heuristic methods, GReM-LNN reconstructs marginals with lower error than Trunc across all privacy budgets and datasets. Except at the highest privacy regime considered (\(=0.1\)) on Titanic and Salary, GReM-LNN yields lower error than Trunc+Rescale. Averaging over all settings and trials, GReM-LNN has lower workload error by a factor of 17.6\(\) compared to Trunc and 3.2\(\) compared to Trunc+Rescale. Note that GReM-MLE is omitted from Fig. 2 since ResidualPlanner is the maximum likelihood reconstruction for its measurements. Appendix I reports results for this experiment with respect to \(_{2}\) workload error, which are consistent with the present findings.

### Scalable MWEM Results

Fig. 3 displays results for the Scalable MWEM experiment for 30 rounds of measurements. Observe that Scalable MWEM runs for the settings considered, which would be infeasible for the original MWEM mechanism due to large data domains. Of all methods considered, Private-PGM yields the greatest reduction in workload error in settings where it ran; however, Private-PGM failed due to exceeding memory resources (20 GB) at 30 rounds on Adult, Salary, and Nist-Taxi in all trials. In Appendix I, we report the settings in which Private-PGM successfully ran across 10, 20, and 30 rounds of Scalable MWEM.

With respect to GReM-LNN, the findings from the prior experiment agree with the present results. Across all privacy budgets and datasets considered, GReM-LNN significantly reduces workload error on the reconstructed marginals compared to Scalable MWEM. Averaging over all settings and trials, GReM-LNN reduces Scalable MWEM workload error by a factor of 12.3\(\). Averaging over all settings and trials, GReM-LNN has lower workload error by a factor of 1.1\(\) compared to Trunc+Rescale. Note that we suppress results for Trunc due to space. Appendix I reports results for this experiment with respect to \(_{2}\) workload error, which are consistent with the present findings.

Figure 2: Average \(_{1}\) workload error on all 3-way marginals across five trials and privacy budgets \(\{0.1,0.31,1,3.16,10\}\) and \(=1 10^{-9}\) for ResidualPlanner.

## 6 Discussion

We develop ReM, a method for reconstructing answers to marginal queries that scales to large data domains. We also introduce a tractable method to incorporate local non-negativity that significantly improves reconstruction quality. Finally, we show that ReM can be used to improve the existing query answering mechanisms ResidualPlanner and a scalable version of MWEM.

**Limitations.** Many data-dependent query answering mechanisms also generate synthetic data. In some cases, practitioners utilize these mechanisms primarily in order to use the synthetic data for downstream tasks such as training a machine learning model . For those users, the fact that ReM does not generate synthetic data would be an important limitation. A broader limitation, which is common to many methods in this field, is lack of support for continuous data. Marginal and residual queries are only defined on discrete domains so continuous attributes need to be discretized.

**Future Work and Broader Impacts.** While developing effective algorithms for privacy-preserving data analysis is generally beneficial, it is known that these methods can lead to unfair outcomes . One direction for future work is to further understand the fairness properties of the methods we present and how to mitigate any undesirable outcomes. Another direction for future work is further understanding the weighting scheme used in GeRoM-LNN to apply local non-negativity. Preliminary experiments show that weighting lower-order residual queries more highly in the loss function yields reconstructed answers with lower workload error as well as faster and more reliable convergence of the optimization routine. In general, the relationship between residual weights in the loss function, optimizer convergence, and reconstruction quality is not well understood.