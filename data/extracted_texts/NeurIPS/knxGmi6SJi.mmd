# shapiq: Shapley Interactions for Machine Learning

Maximilian Muschalik

LMU Munich, MCML

&Hubert Baniecki

University of Warsaw,

Warsaw University of Technology &Fabian Fumagalli

Bielefeld University

&Patrick Kolpaczki

Paderborn University &Barbara Hammer

Bielefeld University &Eyke Hullermeier

LMU Munich, MCML

###### Abstract

Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.

## 1 Introduction

Assigning _value_ to entities collectively performing a task is essential in various real-world applications of machine learning (ML) . For instance, when reimbursing data providers based on the _value of data_, or justifying a model's prediction based on _value of feature information_. The _fair_ distribution of value among a group of entities is a central aspect of cooperative game theory, where the Shapley Value (SV)  defines a _unique_ allocation scheme based on intuitive axioms. The SV is applicable to any _game_, i.e. a function that specifies the worth of all possible groups of entities, called coalitions. In ML, application-specific games were introduced , which typically require a definition of the overall worth and a notion of entities' absence . The SV fairly distributes the overall worth among individuals by evaluating the game for all coalitions. However, it does not give insights on _synergies or redundancies_ between entities. For instance, while two features such as _latitude_ and _longitude_ convey separate information, only their joint consideration reveals the synergy of encoding an _exact location_. The value of such a group of entities is known as an _interaction_, or in this context _feature interaction_, and is crucial to understand predictions of complex ML models , as illustrated in Figure 1.

Shapley Interactions (SIs)  distribute the overall worth to all groups of entities up to a maximum _explanation order_. They satisfy axioms similar to the SV, to which they reduce for individuals, i.e. the lowest explanation order. In contrast, for the highest explanation order, which comprises an interaction for every coalition, the SIs yield the Mobius Interaction (MI), or Mobius transform . The MIs are a fundamental concept in cooperative game theory that captures the _isolated joint contribution_, which allows to additively describe every coalition's worth by a sum of the contained MIs. With an increasing explanation order, the SIs comprise more components that finally yield the MIs as the most comprehensive explanation of the game at the cost of highest complexity . While the SV and SIs provide an appealing theoretical concept, computing them without structural assumptions on the game requires exponential complexity . For tree-based models, it was shown that SVs  and SIs  can be efficiently computed by exploiting the architecture. Moreover, game-agnostic stochastic approximators estimate the SV  and SIs  with a limited budget of game evaluations.

Diverse applications of the SV have led to various techniques for its efficient computation . Recently, extensions to any-order SIs addressed limitations of the SV and complemented interpretation of model predictions with higher-order feature interactions . While stochastic approximators are applicable to any game, their evaluation is typically performed in an isolated application , such as feature interactions. Moreover, implementing such algorithms requires a strong mathematical background and specific design choices. Existing Python packages, such as shap, provide a relatively small number of approximators, which are limited to the SV and feature attributions.

Figure 1: The shapiq Python package facilitates research on game theory for machine learning, including state-of-the-art approximation algorithms and pre-computed benchmarks. It provides a simple interface for explaining predictions of machine learning models beyond feature attributions **(1)**, enabling researchers to explore higher-order interactions or domain-independent game theory **(2)**.

Contribution.In this work, we present shapiq, an open-source _Python_ library for computing any-order SIs. shapiq consolidates research on SVs and SIs, making these tools accessible across various ML domains. Our core contributions are summarized in Figure 1 and include,

1. a general approximation and computation interface for state-of-the-art SI algorithms and methods without focus on a specific application like explanations or data valuation,
2. an explanation API for using SIs to explain ML models and visualizing interactions,
3. a benchmarking suite to evaluate SI approximators across several real-world scenarios,
4. and a cross-domain empirical evaluation of approximators guiding practitioners.

Related software tools and benchmarks.shapiq extends the popular shap Python package beyond feature attributions aiming to fully embrace the application of SVs and SIs in ML. While shap implements a single index for 2-order feature interactions to explain the predictions of tree-based models, shapiq implements a dozen approximators for any-order SIs (Table 2) and offers a benchmarking suite for these algorithms across 10 different domains (Table 3). Related software such as aix360, alibi and dalex are general toolboxes offering the implementation and visualization of the most popular ML explanations for end-users. We specify in SIs to provide a comprehensive tool facilitating research in game theory for ML, including the exact computation of 20 interaction indices and game-theoretic concepts (Table 1). Notably, the investigate and captum Python packages offer feature attribution methods specific to (deep) neural networks. Most recently, quantus implements evaluation metrics for these explanation methods.

We build upon recent advances in benchmarking explainable artificial intelligence (XAI) methods such as feature attributions [2; 51; 53; 63] and algorithms for data valuation . XAI-Bench focuses on synthetic tabular data. OpenXAI provides 7 real-world tabular datasets with pre-trained neural network models, 7 feature attribution methods and 8 metrics to compare them. \(^{4}\) extends OpenXAI to benchmark feature attributions of deep neural networks for image and text modalities. In , the authors benchmark several algorithms for approximating SVs based on the conditional feature distribution. OpenDataVal provides 9 real-world datasets, 11 data valuation methods and 4 metrics to compare them. shapiq puts more focus on benchmarking higher-order SI algorithms and provides an interface to state-of-the-art explanation methods that base on SIs, e.g. TreeSHAP-IQ. While open data repositories such as OpenML offer easy access to datasets for ML, we pre-compute and share ground-truth SIs for various games (i.e. dataset-model pairs) that saves considerate time and resources when benchmarking approximation algorithms.

## 2 Theoretical Background

In ML, various concepts are based on synergies of entities to optimize performance in a given task. For example, weak learners construct powerful model ensembles , collected data instances and features are used to train supervised ML models [16; 30], where feature values collectively predict outputs. To better understand such processes, XAI quantifies the contributions of these entities to the task, most prominently for feature values in predictions (local feature attribution [55; 77]), features in models (global feature importance [16; 17; 69]), and data instances in model training (data valuation ). Assigning such contributions is closely related to the field of cooperative game theory , which studies the notion of value for players that collectively obtain a payout. To adequately assess the impact of individual players, it is necessary to analyze the payout for different coalitions. More formally, a cooperative game \(:(N)\) with \(()=0\) is defined by a _value function_ on the power set of \(N:=\{1,,n\}\) entities, which describes such payouts for all possible coalitions of players. We later summarize such prominent examples in the context of ML in Table 3. Here, we summarize existing contribution concepts for individuals and groups of entities, outlined in Table 1.

The **SV** and **Banzhaf Value (BV)** are instances of _semivalues_. Semivalues assign contributions to individual players and adhere to intuitive axioms: _Linearity_ enforces linearly composed contributions for linearly composed games; _Dummy_ requires that players without impact receive zero contribution; _Symmetry_ enforces that entities contributing equally to the payout receive equal value. The SV  is the unique semivalue that additionally satisfies _efficiency_, i.e. the sum of all contributions yields the total payout \((N)\). In contrast, the BV  is the unique semivalue that additionally satisfies _2-Efficiency_, i.e. the contributions of two players sum to the contribution of a joint player in a reduced game, where both players are merged. The SV and BV are represented as a weighted average over _marginal contributions_\(_{i}(T):=(T\{i\})-(T)\) for \(i N\) as

\[^{}(i):=_{T N\{i\}}}_{i}(T)^{}(i):=_{T N\{i\}}} _{i}(T)\,.\]

In ML applications, the SV is typically preferred over the BV due to the efficiency axiom . For instance, in local feature attribution, the SV is utilized to fairly distribute the model's prediction to individual features . However, it was shown that the SV is limited when explaining complex decision systems, and _feature interactions_, i.e. the joint contributions of features' groups, are required to understand such processes .

The **Generalized Value (GV)** and **Interaction Index (II)** are two paradigms to extend the notion of value to groups of entities. The GVs are based on weighted averages over (joint) marginal contributions \((T S)-(T)\) for \(S N\) given \(T N S\). In contrast, IIs are based on discrete derivatives that account for lower-order effects of subsets of \(S\). For instance, for two players \(i,j N\), the discrete derivative \(_{\{i,j\}}(T)\) is defined as the joint marginal contribution \((T\{i,j\})-(T)\) minus the individual marginal contributions \(_{i}(T)\) and \(_{j}(T)\). More generally, the _discrete derivative_\(_{S}(T)\) for \(S N\) in the presence of \(T N S\) is defined as

\[_{S}(T):=_{L S}(-1)^{|S|-|L|}(T L)\ \ \ \ \ _{S}(T)=_{}-_{ L S}(T)}_{}\,.\]

A positive value indicates synergy, whereas a negative value indicates redundancy of \(S\) given \(T\). Lastly, a zero value indicates (additive) independence, i.e. the joint marginal contribution is equal to the sum of all lower-order effects. GVs and IIs are uniquely represented  by

\[^{}(S):=_{T N S}p_{|T|}^{|S|}(n)( (T S)-(T))^{}(S):=_{T N S}p_{|T|}^{|S|}(n) _{S}(T)\,.\]

The most prominent examples are the _Shapley GV (SGV)_ and the _Shapley II (SII)_ with \(p_{t}^{s}(n)=((n-s+1))^{-1}\), which naturally extend the SV (cf. Appendix A.1). While the SGV and SII are natural extensions to the SV, they are not suitable for interpretability, since they are defined on the powerset and comprise an exponential number of components. Moreover, neither GVs nor IIs satisfy the efficiency axiom for higher-orders, which is desirable for ML applications.

Shapley Interactions (SIs) for Machine Learningassign joint contribution \(_{k}\) up to an _explanation order_\(k\), i.e. for all coalitions \(S N\) with \(|S| k\), which satisfy generalized _efficiency_\((N)=_{S N,|S| k}_{k}(S)\,.\) The _\(k\)-SVs (\(k\)-SIIs)_ are the unique SIs that coincide with SII for the highest order. The _Shapley Taylor II (STII)_ puts a stronger emphasis on the top-order interactions, and _Faithful SII (FSII)_ optimizes _Shapley-weighted faithfulness_

\[(,_{k}):=_{T N}(t)((T)- _{S T,|S| k}_{k}(S))^{2}\ \ (t):=_{}&\ t\{0,n\}\\ }&\,.\]

  
**Setting** & **Interaction Index (II)** & **Base Semivalue** & **Generalized Value (GV)** \\   & \(k\)**-Shapley Values (\(k\)-SII)** &  &  \\  & **Shapley Taylor II (STII)** & & \\  & **Faithful Shapley II (FSII)** & & \\  & \(k_{}\)-SHAP  & & \\  & Faithful Banzhaf II (FBII)  & Banzhaf (BV)  & – \\   & **Möbius (MI)** &  &  & Internal GV (IGV)  \\  & Co-Möbius (Co-MI)  & & & External GV (EGV)  \\   & Shapley II (SII)  & & Shapley GV (SGV)  \\   & Chaining II (CHII)  & & & Chaining GV (CHGV)  \\   & Banzhaf II (BII)  & & Banzhaf (BV)  & Banzhaf GV (BGV)  \\   

Table 1: Available concepts in the ExactComputer class in shapiq with **SIs in bold**.

[MISSING_PAGE_FAIL:5]

interface offering approximation performance increases through sampling procedures like the _border_- and _pairing-tricks_ introduced in [17; 29].

Exact computer.A key functionality of shapiq lies in computing the SIs _exactly_, which is feasible for smaller games, but reaches its limit for growing player numbers. The shapiq.ExactComputer class provides an interface for computing 20 interaction indices and game-theoretic concepts, including the MI, BV, SGV, among others (see Table 1).

Games.Approximators and computers require the specification of a _cooperative game_. Games make up a central level of abstraction in shapiq, and specifying a game only requires the implementation of a domain-specific value function. Table 3 describes in detail 11 benchmark games implemented in shapiq. Beyond synthetic games, our benchmark spans the 5 most prominent domains where SIs can be applied for ML. The shapiq.Game class can be easily extended to include future benchmarks in the package. We pre-compute and share exact SIs for \(2\,042\) benchmark game configurations in total (see Appendix B), facilitating future work on improving the approximators, which we elaborate on further in Section 4.

### Explaining Machine Learning Predictions with shapiq

In addition to facilitating theoretical advancements, shapiq also provides practical tools for applying SIs in ML. These tools streamline the process of explaining feature interactions in model predictions, allowing researchers and practitioners to easily compute and visualize interaction effects across a range of models and data types.

Explainer.The shapiq.Explainer class is a simplified interface to explain any-order feature interactions in ML models. Figure 2 goes through exemplary code used to approximate SIs for a single prediction and visualize them on a graph plot. Currently three classes are further distinguished within the API, but we envision extending shapiq.Explainer to include more data modalities

Figure 3: **Left:** Exemplary code for globally explaining multiple model’s predictions with shapiq. **Right:** Global feature interaction importance visualized as a bar plot.

Figure 2: **Left:** Exemplary code for locally explaining a single model’s prediction with shapiq. **Right:** Local feature interactions visualized on a network plot.

and model algorithms. shapiq.TabularExplainer allows for model-agnostic explanation based on feature marginalization with baseline, marginal, or conditional imputation (refer to Appendix C for details). shapiq.TreeExplainer implements TreeSHAP-IQ  for efficient explanations specific to decision tree-based models, e.g. random forest or gradient boosting decision trees, with native support for scikit-learn , xgboost , and lightgbm . Figure 3 goes through exemplary code for explaining a set of predictions and visualizing their aggregation in a bar plot, which represents the global feature interaction importance. Lastly, shapiq.TabPFNLExplainer is a special case of the shapiq.TabularExplainer handling explanations for the TabPFN  model architecture akin to  with a _remove-and-recontextualize_ paradigm.

Visualizing interactions.shapiq supports the plotting and analysis of interaction values with different visualizations techniques. shapiq.plot offers custom visualizations including our custom network and SI graph plot, but also wraps established visualizations from shap  like the force and bar plots. For a detailed guide and summary of the visualizations, we kindly refer to Appendix D.

Utility functions.shapiq offers additional useful tools that are described in detail in the documentation. Interaction values are stored and processed using the shapiq.InteractionValues data class, which is rich in utility functions. Notably, useful set-based operators and generators exist for handling player sets \(S N\) or iterating over power sets \((N)\) of certain sizes with shapiq.powerset. Finally, shapiq.datasets loads datasets used for testing and examples.

## 4 Benchmarking Analysis

The shapiq library enables computation of various SIs for a broad class of application domains. To illustrate its versatility, we conduct benchmarks across a wide variety of traditional ML-based SV application scenarios. The ML benchmark demonstrates how higher-order SIs enable an accuracy-complexity trade-off for model interpretability (Section 4.1) and highlights that different approximation techniques in shapiq achieve the state-of-the-art performance depending on the application domains (Section 4.2). Tables 3, 4 and 5 present an overview of different application domains and associated benchmarks. Depending on the benchmark, it can be instantiated with different datasets, models, player numbers or benchmark-specific configuration parameters, e.g. uncertainty type: _epistemic_ for Uncertainty Explanation or imputer: _conditional_ for Local Explanation. In total, shapiq offers 100 unique benchmark games, i.e. applications times dataset-model pairs.

For all games that include \(n 16\) players, the value functions have been pre-computed by evaluating all coalitions and storing the games to file. Reading a pre-computed game from file, instead of performing up to \(2^{16}=65\,536\) value function calls with each new experiment run, saves valuable computational time and contributes to reproducibility as well as sustainability. This is particularly

  
**Domain** & **Benchmark (Game)** & **Source** & **Players** & **Coalition Worth** \\   & Local Explanation &  & Features & Model Output \\  & Global Explanation &  & Features & Model Loss \\  & Tree Explanation &  & Features & Model Output \\  & Uncertainty Explanation &  & Features & Prediction Entropy \\  & Feature Selection &  & Features & Performance \\  & Ensemble Selection &  & Weak Learners & Performance \\  & RF Ensemble Selection &  & Tree Models & Performance \\  & Data Valuation &  & Data Points & Performance \\  & Dataset Valuation &  & Data Subsets & Performance \\  & Cluster Explanation & – & Features & Cluster Score \\  & Unsupervised Feature Importance &  & Features & Total Correlation \\ 
**Synthetic** & Sum of Unanimity Model &  & Players & Sum of Unanimous Votes \\   

Table 3: Overview of the available benchmark games and domains in shapiq. Each benchmark can be instantiated with different datasets, models, player sizes, or benchmark-specific configuration parameters. This results in \(2\,042\) pre-computed individual configurations (see Tables 4 and 5).

beneficial for tasks that involve remove-and-refit strategies , such as Data Valuation or Feature Selection. For \(n>16\), where pre-computing a game and ground truth values becomes computationally prohibitive, we rely on analytical solutions to compute the ground truth like TreeSHAP-IQ  for tree-based ensembles or the MI representation of Sum of Unanimity Models [28; 29; 81]. For details regarding the experimental setting and reproducibility, we refer to Appendix E.

### Faithfulness and Complexity of Shapley Interactions

In this experiment, we empirically assess how _faithfully_ lower-order SIs representations capture higher-order effects for varying explanation orders \(k\) and choice of interaction index. To this end, we rely on the Shapley-weighted faithfulness \((,_{k})\) introduced in Section 2. The complexity of SIs ranges from SVs (least complex) to MIs (most complex), where SVs minimize \((,_{1})\) for \(k=1\)[12; 55], while MIs perfectly capture all game values with \((,_{n})=0\) for \(k=n\)[10; 81]. To quantify the faithfulness of SIs across different domains, we calculate interaction values for a given index \(_{k}\) and explanation order \(k\). We then approximate the game values for each subset \(T N\) of players as \(_{k}(T):=_{S T:|S| k}_{k}(S)\) and compare them to the ground truths using \((,_{k})\) through a Shapley-weighted \(R^{2}((T),_{k}(T))\) for all \(T N\). For context, a game with no interactions (a 1-additive game) will be perfectly reproduced by a 1-additive explanation, yielding \((,_{1})=0\) and \(R^{2}=1\). Conversely, games with substantial higher-order interactions will result in larger errors for lower-order explanations, with \((,_{k}) 0\) and \(R^{2}<1\).

Figure 4 shows the Shapley-weighted \(R^{2}\) value for \(k=1,,n\) for a synthetic game with a single interaction of varying size (a) and real-world ML applications (b). Here, we used \(_{}=1\) instead of \(_{} 1\), which affects FBII that violates efficiency. The results show that in general SIs become more faithful with higher explanation order. Notably, the difference between pairwise SIs and SVs (SHAP textbox) is remarkable, where pairwise interactions (\(k=2\)) already yield a strong improvement in faithfulness. If higher-order interactions dominate, then SIs require a larger explanation order to maintain faithfulness. While FSII and FBII are optimized for faithfulness, STII and \(k\)-SII do not necessarily yield a strict improvement in this metric. In fact, it was shown that SII and \(k\)-SII optimize a slightly different faithfulness metric, which changes for every order . Yet, we observe a consistent strong improvement of pairwise \(k\)-SII over the SV (SHAP). While FSII and FBII optimize faithfulness, \(k\)-SII and STII adhere to strict structural assumptions, where STII projects all higher-order interactions to the top-order SIs, and \(k\)-SII is consistent with SII. Practitioners may choose SIs tailored to their specific application, where \(k\)-SII is a good default choice for shapiq.

### Comparison of Approximation Methods

Various approximation methods for computing SIs are included in shapiq for a variety of SIs (cf. Table 2). The possibility of attributing (domain-specific) state-of-the-art performance to a single algorithm has been investigated empirically by multiple works [28; 29; 45; 46; 47; 57; 64; 81; 83; 89]. We use the collection of 100 unique benchmark games in shapiq to evaluate the performance of different SV and SI approximation methods on a broad spectrum of ML applications. For each domain and configuration (see Table 4 and 5 in Appendix B), we compute ground truth SVs, \(2\)-SIIs,

Figure 4: Shapley-weighted \(R^{2}\) of interaction indices by explanation order for **(a)** single synthetic interactions and **(b)** ML applications. FSII is optimized for this metric, and increases faithfulness with each order. Interactions improve faithfulness over SHAP and yield an exact decomposition for the highest order. However, increasing interaction size negatively affects faithfulness.

and \(3\)-SIIs and compare them with estimates provided by all approximators from Table 2. The approximators are run with a wide range of budget values and assessed by their achieved mean squared error (MSE) or precision at five (Precision@5). Figure 5 summarizes the approximation results.

Most notably, the ranking of approximators varies strongly between the different applications domains, which is depicted in Figure 5 (a) and (b). This observation holds for both SVs and SIs. In general, two types of approximation methods dominate the application landscape in terms of MSE and Precision@5. First, _kernel-based_ approaches including KernelSHAP, \(k_{}\)-SHAP, KernelSHAP-IQ and Inconsistent KernelSHAP-IQ perform best for Local Explanation, Uncertainty Explanation, and Unsupervised Feature Importance. Second, the two _stratification-based_ estimators SVARM and SVARM-IQ achieve state-of-the-art performance for Data Valuation, Dataset Valuation, or Ensemble Selection. Traditional _mean-estimation_ methods including Permutation Sampling (SV and SII), Unbiased KernelSHAP, SHAPIQ, and Owen Sampling achieve moderate estimation qualities in comparison. Our findings give rise to the conclusion that _stratification-based_ estimators perform superior in settings where the size of a coalition naturally impacts its worth (e.g. training size for Dataset Valuation), which is plausible as these methods group coalitions by size and thus leverage this dependency. Meanwhile, _kernel-based_ estimators achieve state-of-the-art in settings where the dependency between size and worth of a coalition is less pronounced (e.g. sudden jumps of model predictions in Local Explanation).

Interestingly, the settings where _stratified-sampling_ outperforms _kernel-based_ variants exhibit different internal structures in the games' MIs. Generally, MIs disentangle a game into all of its additive components (cf. Section 2) and can be computed exactly with shapiq's pre-computed games. The accuracy of _kernel-based_ estimators drops when higher-order interactions dominate the games' structure instead of lower-order interactions. This is depicted by Figure 5 (c) where the MIs for Local Explanation are of lower order than the Data Valuation games.

Figure 5: Overview of the benchmark results containing **(a)** budget-dependent MSE approximation curves on different benchmark settings, **(b)** a summary of the best performing approximators per setting over all 100 benchmark games measured by MSE (left) and Precision@5 (right), and **(c)** exemplary MIs for ten games of Data Valuation (top) and Local Explanation (bottom).

Conclusion

As SIs are increasingly employed to analyze ML models, it becomes pivotal to ensure that these are accurately and efficiently approximated. To this end, we contributed shapiq, an open-source toolbox that implements state-of-the-art algorithms, defines a dozen of benchmark games, and provides ready-to-use explanations of any-order feature interactions. shapiq contains a comprehensive documentation and is designed to be extendable by contributors.

Limitations and future work.We identify three main limitations of shapiq that provide natural opportunities for future work. First, the TreeSHAP-IQ algorithm is currently implemented in Python, but by-design requires no access to model inference, which allows for a more efficient implementation in C++ alike TreeSHAP . Second, SIs can be misinterpreted based on choosing the wrong index for the application scenario, which we comment on across Sections 2 and 4.1. The selection of a particular SI index, enabled by shapiq, offers great opportunities for application-specific research. We also acknowledge that visualization of higher-order feature interactions is itself challenging and a potential research direction in human-computer interaction. Certainly, a human-centric evaluation of explanations may be required for their broader adoption in practical applications .

Broader impact.A potential negative societal implication of visualizing higher-order feature interactions may be an _information overload_ that leads to users misinterpreting model explanations. Nevertheless, we hope our contribution sparks the advancement of game-theoretical indices motivated by various applications in ML. Specifically in the context of explainability, shapiq may impact the way users interact with ML models when having access to previously inaccessible information, e.g. higher-order feature interactions.