# Scaling White-Box Transformers for Vision

Jinrui Yang\({}^{*1}\) Xianhang Li\({}^{*1}\) Druv Pai\({}^{2}\)

Yuyin Zhou\({}^{1}\) Yi Ma\({}^{2}\) Yaodong Yu\({}^{12}\) Cihang Xie\({}^{1}\)\({}^{1}\)

\({}^{*}\)equal technique contribution, \({}^{}\)equal advising

\({}^{1}\)UC Santa Cruz \({}^{2}\)UC Berkeley

###### Abstract

crate, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of crate remains an open question which this paper aims to address. Specifically, we propose crate-\(\), featuring strategic yet minimal modifications to the sparse coding block in the crate architecture design, and a light training recipe designed to improve the scalability of crate. Through extensive experiments, we demonstrate that crate-\(\) can effectively scale with larger model sizes and datasets. For example, our crate-\(\)-B substantially outperforms the prior best crate-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our crate-\(\)-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned crate models, as we demonstrate through showing that the learned token representations of increasingly larger trained crate-\(\) models yield increasingly higher-quality unsupervised object segmentation of images. The project page is https://rayjryang.github.io/CRATE-alpha/.

## 1 Introduction

Over the past several years, the Transformer architecture  has dominated deep representation learning for natural language processing (NLP), image processing, and visual computing . However, the design of the Transformer architecture and its many variants remains largely empirical and lacks a rigorous mathematical interpretation. This has largely hindered the development of new Transformer variants with improved efficiency or interpretability. The recent white-box Transformer model crate addresses this gap by deriving a simplified Transformer block via unrolled optimization on the so-called _sparse rate reduction_ representation learning objective.

More specifically, layers of the white-box crate architecture are mathematically derived and fully explainable as unrolled gradient descent-like iterations for optimizing the sparse rate reduction. The self-attention blocks of crate explicitly conduct compression via denoising features against learned low-dimensional subspaces, and the MLP block is replaced by an incremental sparsification (via ISTA ) of the features. As shown in previous work , besides mathematical interpretability, the learned crate models and features also have much better semantic interpretability than conventional transformers, i.e., visualizing features of an image naturally forms a zero-shot image segmentation of that image, even when the model is only trained on classification.

Scaling model size is widely regarded as a pathway to improved performance and emergent properties . Until now, the deployment of crate has been limited to relatively modest scales. The most extensive model described to date is the base model size encompasses 77.6M parameters(create-Large) . This contrasts sharply with standard Vision Transformers (ViTs ), which have been effectively scaled to a much larger model size, namely 22B parameters .

To this end, this paper provides the first exploration of training crate at different scales for vision, i.e., Tiny, Small, Base, Large, Huge. Detailed model specifications are given in Table 7 of Appendix A.1. To achieve effective scaling, we make two key changes. First, we identify the vanilla ISTA block within crate as a limiting factor that hinders further scaling. To overcome this, we significantly expand the channels, decouple the association matrix, and add a residual connection, resulting in a new model variant -- crate-\(\). It is worth noting that this architecture change still preserves the mathematical interpretability of the model. Second, we propose an improved training recipe, inspired by previous work , for better coping the training with our new crate-\(\) architecture.

We provide extensive experiments supporting the effective scaling of our crate-\(\) models. For example, we scale the crate-\(\) model from Base to Large size for supervised image classification on ImageNet-21K , achieving _85.1% top-1 accuracy on ImageNet-1K_ at the Large model size. We further scale the model size from Large to Huge, utilizing vision-language pre-training with contrastive learning on DataComp1B , and achieve _a zero-shot top-1 accuracy of 72.3% on ImageNet-1K_ at the Huge model size.1 These results demonstrate the strong scalability of the crate-\(\) model, shedding light on scaling up mathematically interpretable models for future work.

The main contributions of this paper are threefold:

1. We design three strategic yet minimal modifications for the crate model architecture to unleash its potential. In Figure 1, we reproduce the results of the crate model within our training setup, initially pre-training on ImageNet-21K classification and subsequently fine-tuning on ImageNet-1K classification. Compared to the vanilla crate model that achieves 68.5% top-1 classification accuracy on ImageNet-1K, our crate-\(\)-B/32 model significantly improves the vanilla crate model by 8%, which clearly demonstrates the benefits of the three modifications to the existing crate model. Moreover, following the settings of the best crate model and changing the image patch size from 32 to 8, our crate-\(\)-B model attains a top-1 accuracy of 83.2% on ImageNet-1K, exceeding the previous best crate model's score of 79.5% by a significant margin of 3.7%.
2. Through extensive experiments, we show that one can effectively scale crate-\(\) via model size and data simultaneously. In contrast, when increasing the crate model from Base to Large model size, there is a marginal improvement on top-1 classification accuracy (+0.5%, from 70.8% to 71.3%) on ImageNet-1K, indicating diminished returns . Furthermore, by scaling the training dataset, we achieved a substantial 1.9% improvement in top-1 classification accuracy on ImageNet-1K, increasing from 83.2% to 85.1% when going from crate-\(\) Base to Large.
3. We further successfully scale crate-\(\) model from Large to Huge by leveraging vision-language pre-training on DataComp1B. Compared to the Large model, the Huge model (crate-\(\)-H) achieves a zero-shot top-1 classification accuracy of 72.3% on ImageNet-1K, marking a significant

Figure 1: _(Left)_ We demonstrate how modifications to the components enhance the performance of the crate model. The four models are trained using the same setup: first pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. Details are provided in Section 3. _(Right)_. We compare the FLOPs and accuracy on ImageNet-1K of our methods with ViT  and CRATE . The values of crate-\(\) model correspond to those presented in Table 1. A more detailed comparison between crate-\(\) and ViT is included in Appendix A.2.

scaling gain of 2.5% over the Large model. These results indicate that the crate architecture has the potential to serve as an effective backbone for vision-language foundation models.

#### Related Work

**White-box Transformers.**[46; 45] argued that the quality of a learned representation can be assessed through a unified objective function called the _sparse rate reduction_. Based on this framework, [46; 45] developed a family of transformer-like deep network architectures, named crate, which are mathematically fully interpretable. crate models has been demonstrably effective on various tasks, including vision self-supervised learning and language modeling [26; 45]. Nevertheless, it remains unclear whether crate can scale as effectively as widely used black-box transformers. Previous work  suggests that scaling the vanilla crate model can be notably challenging.

**Scaling ViT.** ViT  represents the initial successful applications of Transformers to the image domain on a large scale. Many works [12; 31; 33; 32; 5; 37; 21; 22; 29; 18; 49] have deeply explored various ways of scaling ViTs in terms of model size and data size. From the perspective of self-supervision, MAE  provides a scalable approach to effectively training a ViT-Huge model using only ImageNet-1K. Following the idea of MAE,  further scales both model parameters to billions and data size to billions of images. Additionally, CLIP was the first to successfully scale ViT on a larger data scale (i.e., 400M) using natural language supervision. Based on CLIP, [32; 33] further scale the model size to 18 billion parameters, named EVA-CLIP-18B, achieving consistent performance improvements with the scaling of ViT model size. From the perspective of supervised learning, [49; 5] present a comprehensive analysis of the empirical scaling laws for vision transformers on image classification tasks, sharing some similar conclusions with .  suggests that the performance-compute frontier for ViT models, given sufficient training data, tends to follow a saturating power law. More recently,  scales up ViT to 22 billion parameters. Scaling up different model architectures is non-trivial. [37; 21; 22] have made many efforts to effectively scale up different architectures. In this paper, due to the lack of study on the scalability of white-box models, we explore key architectural modifications to effectively scale up white-box transformers in the image domain.

## 2 Background and Preliminaries

In this section, we present the background on white-box transformers proposed in , including representation learning objectives, unrolled optimization, and model architecture. We first introduce the notation that will be used in the later presentation.

**Notation.** We use notation and problem setup following Yu et al. . We use the matrix-valued random variable \(=[_{1},,_{N}]^{D N}\) to represent the data, where each \(_{i}^{D}\) is a "token", such that each data point is a realization of \(\). For instance, \(\) can represent a collection of image patches for an image, and \(_{i}\) is the \(i\)-th image patch. We use \(f^{D N}^{d N}\) to denote the mapping induced by the transformer, and we let \(=f()=[_{1},,_{N}]^{d N}\) denote the features for input data \(\). Specifically, \(_{i}^{d}\) denotes the feature of the \(i\)-th input token \(_{i}\). The transformer \(f\) consists of multiple, say \(L\), layers, and so can be written as \(f=f^{L} f^{1} f^{}\), where \(f^{}^{d N}^{d N}\) denotes the \(\)-th layer of the transformer, and the pre-processing layer is denoted by \(f^{}=^{D N}^{d N}\). The input to the \(\)-th layer \(f^{}\) of the transformer is denoted by \(^{}=[_{1}^{},,_{N}^{}] ^{d N}\), so that \(f^{}^{}^{+1}\). In particular, \(^{1}=f^{}()^{d N}\) denotes the output of the pre-processing layer and the input to the first layer.

### Sparse Rate Reduction

Following the framework proposed in , we posit that the goal of representation learning is to learn a feature mapping or _representation_\(f^{D N}^{d N}\) that transforms the input data \(\) (which may have a nonlinear, multi-modal, and otherwise complicated distribution) into _structured and compact_ features \(\), such that the token features lie on a union of low-dimensional subspaces, say with orthonormal bases \(_{[K]}=(_{k})_{k[K]}(^{d p})^{K}\).  proposes the _Sparse \(\)ate \(\)duction_ (SRR) _objective_ to measure the goodness of such a learned representation:

\[_{f}\,_{=f()}[L_{}( )]=_{f}\,_{=f()}[R^{ c}(\,|\,_{[K]})-R(\,|\,_{[K]})+\|\|_{1} ],\] (1)where \(=f()\) denotes the token representation, \(\|\|_{1}\) denotes the \(^{1}\) norm, and \(R()\), \(R^{c}(_{[K]})\) are (estimates for) _rate distortions_, defined as:

\[R()(+}^ {}), R^{c}(_{[K]})_{k=1}^{K} R(_{k}^{}).\] (2)

In particular, \(R^{c}(_{[K]})\) (resp. \(R()\)) provide closed-form estimates for the number of bits required to encode the sample \(\) up to precision \(>0\), conditioned (resp. unconditioned) on the samples being drawn from the subspaces with bases \(_{[K]}\). Minimizing the term \(R^{c}\) improves the compression of the features \(\) against the posited model, and maximizing the term \(R\) promotes non-collapsed features. The remaining term \(\|\|_{1}\) promotes sparse features. Refer to  for more details about the desiderata and objective of representation learning via the rate reduction approach.

### crate: Coding RATE Transformer

**Unrolled optimization.** To optimize the learning objective and learn compact and structured representation, one approach is unrolled optimization : each layer of the deep network implements an iteration of an optimization algorithm on the learning objective. For example, one can design the layer \(f^{}\) such that the forward pass is equivalent to a proximal gradient descent step for optimizing learning objective \(L()\), i.e., \(^{+1}=f^{}(^{})=[^{}- _{}L(^{})]\). Here we use \(\) to denote the step size and \([]\) to denote the proximal operator .

**One layer of the crate model.** We now present the design of each layer of the white-box transformer architecture - Coding RATE Transformer (crate) - proposed in . Each layer of crate contains two blocks: the compression block and the sparsification block. These correspond to a two-step alternating optimization procedure for optimizing the sparse rate reduction objective (1). Specifically, the \(\)-th layer of crate is defined as

\[^{+1}=f^{}(^{})=(^{+1/2} ^{}),^{+1/2}=^{}+ (^{}).\] (3)

**Compression block (MSSA).** The compression block in crate, called **M**ulti-head **S**ubspace **S**elf-**A**ttention block (MSSA), is derived for compressing the token set \(=[_{1},,_{N}]\) by optimizing the compression term \(R^{c}\) (defined Eq. (1)), i.e.,

\[^{+1/2}=^{}+(^{}_{[K]}^{ })^{}-_{}R^{c}(^{}\,|\, _{[K]}^{}),\] (4)

where \(_{[K]}^{}\) denotes the (local) signal model at layer \(\), and the MSSA operator is defined as

\[(_{[K]})=}[ {U}_{1}\,\,_{K}](_{1}^{}) ((_{1}^{})^{}(_{1}^{}))\\ \\ (_{K}^{})((_{K}^{})^{ }(_{K}^{})).\] (5)

Compared with the commonly used attention block in transformer , where the \(k\)-th attention head is defined as \((_{k}^{})((_{k}^{})^{ }(_{k}^{}))\), MSSA uses only one matrix to obtain the query, key, and value matrices in the attention: that is, \(_{k}=_{k}=_{k}=_{k}\).

**Sparse coding block (ISTA).** The Iterative Shrinkage-Thresholding Algorithm (ISTA) block is designed to optimize the sparsity term and the global coding rate term, \(\|\|_{0}-R(_{[K]})\) in (1).  shows that an optimization strategy for these terms posits a (complete) incoherent dictionary \(^{}^{d d}\) and takes a proximal gradient descent step towards solving the associated LASSO problem \(_{ 0}[\|^{+1/2}-^{}\|_{2}^{2}+ \|\|_{1}]\), obtaining the iteration

\[^{+1}=(^{+1/2}\,|\,^{})=(^{+1/2}+\,(^{})^{}(^{+1/2}-^{ }^{+1/2})-).\] (6)

In particular, the \(\) block sparsifies the intermediate iterates \(^{+1/2}\) w.r.t. \(^{}\) to obtain \(^{+1}\).

## 3 CRATE-\(\) Model

In this section, we present the crate-\(\) architecture, which is a variant of crate. As shown in Fig. 1 (_Right_), there is a significant performance gap between the white-box transformer crate-B/16(70.8%) and the vision transformer ViT-B/16 (84.0%) . One possible reason is that the ISTA block applies a complete dictionary \(^{d d}\), which may limit its expressiveness. In contrast, the MLP block in the transformer2 applies two linear transformations \(_{1},_{2}^{d d}\), leading to the MLP block having 8 times more parameters than the ISTA block.

Since the ISTA block in CRATE applies a single incremental step to optimize the sparsity objective, applying an orthogonal dictionary can make it ineffective in sparsifying the token representations. Previous work  has theoretically demonstrated that overcomplete dictionary learning enjoys a favorable optimization landscape. In this work, we use an overcomplete dictionary in the sparse coding block to promote sparsity in the features. Specifically, instead of using a complete dictionary \(^{}^{d d}\), we use an overcomplete dictionary \(^{}^{d(Cd)}\), where \(C>1\) (a positive integer) is the overcompleteness parameter. Furthermore, we explore two additional modifications to the sparse coding block that lead to improved performance for crate. We now describe the three variants of the sparse coding block that we use in this paper.

**Modification #1: Overparameterized sparse coding block.** For the output of the \(\)-th crate attention block \(^{+1/2}\), we propose to sparsify the token representations with respect to an overcomplete dictionary \(^{}^{d(Cd)}\) by optimizing the following LASSO problem,

\[^{}*{arg\,min}_{} \|^{+1/2}-^{}\|_{2}^{2}+\| \|_{1}.\] (7)

To approximately solve (7), we apply two steps of proximal gradient descent, i.e.,

\[_{0}^{}=,_{1}^{}=_{, }_{0}^{};^{},^{+1/2},_{2}^{}=_{,}_{1}^{};^{ },^{+1/2},\] (8)

where Prox is the proximal operator of the above non-negative LASSO problem (7) and defined as

\[_{,}[;,]=(- ^{}(-)-).\] (9)

The output of the sparse coding block is defined as

\[^{+1}=^{}^{},^{ }=_{2}^{}(^{+1/2}^{ }).\] (10)

Namely, \(^{}\) is a sparse representation of \(^{+1/2}\) with respect to the overcomplete dictionary \(^{}\). The original crate ISTA tries to learn a complete dictionary \(^{d d}\) to transform and sparsify the features \(\). By leveraging more atoms than the ambient dimension, the overcomplete dictionary \(^{d(Cd)}\) can provide a redundant yet expressive codebook to identify the salient sparse

Figure 2: One layer of the crate-\(\) model architecture. MSSA (**M**ulti-head **S**ubspace **S**elf-**A**ttention, defined in (5)) represents the compression block, and **ODL (**O**vercomplete **D**ictionary **L**earning, defined in (12)) represents the sparse coding block. A more detailed illustration of the modifications is provided in Fig. 6 in the Appendix.

structures underlying \(\). As shown in Fig. 1, the overcomplete dictionary design leads to \(5.3\%\) improvement compared to the vanilla crate model.

**Modification #2: Decoupled dictionary.** We propose to apply a decoupled dictionary \(}^{}\) in the last step (defined in (10) of the sparse coding block, \(^{+1}=}^{}^{}\), where \(}^{}^{d(Cd)}\) is a different dictionary compared to \(^{}\). By introducing the decoupled dictionary, we further improve the model performance by \(2.0\%\), as shown in Fig. 1. We denote this mapping from \(^{+1/2}\) to \(^{+1}\) as the **O**vercomplete **D**ictionary **L**earning block (ODL), defined as follows:

\[(^{+1/2}^{},}^{}) }^{}(^{+1/2}^{})=}^{}^{}.\] (11)

**Modification #3: Residual connection.** Based on the previous two modifications, we further add a residual connection, obtaining the following modified sparse coding block:

\[^{+1}=^{+1/2}+(^{+1/2}^{ },}^{}).\] (12)

An intuitive interpretation of this modified sparse coding block is as follows: instead of directly sparsifying the feature representations \(\), we first identify the potential sparse patterns present in \(\) by encoding it over a learned dictionary. Subsequently, we incrementally refine \(\) by exploiting the sparse codes obtained from the previous encoding step. From Fig. 1, we find that the residual connection leads to a \(0.7\%\) improvement.

To summarize, to effectively scale white-box transformers, we implement three modifications to the vanilla white-box crate model proposed in . Specifically, in our crate-\(\) model, we introduce a decoupling mechanism, quadruple the dimension of the dictionary (4\(\)), and incorporate a residual connection in the sparse coding block.

## 4 Experiments

**Overall.** The experimental section consists of three parts: (1) **Scaling study:** We thoroughly investigate the scaling behaviors of crate-\(\) from Base to Large size and ultimately to Huge size. (2) **Downstream applications:** To further verify the broader benefits of scaling the crate-\(\) model, we conduct additional experiments on real-world downstream tasks and present preliminary exploration results of crate-\(\) on language tasks. (3) **Interpretability:** In addition to scalability, we study the interpretability of crate-\(\) across different model sizes.

### Dataset and Evaluation

**Scaling Study.** For the transition from Base to Large size, we pre-train our model on ImageNet-21K and fine-tune it on ImageNet-1K via supervised learning. When scaling from Large to Huge, we utilize the DataComp1B  dataset within a vision-language pre-training paradigm, allowing us to study the effects of scaling the model to a massive size. For evaluation, we evaluate the zero-shot accuracy of these models on ImageNet-1K.

Figure 3: Training loss curves of crate-\(\) on ImageNet-21K. (_Left_) Comparing training loss curves across crate-\(\) with different model sizes. (_Right_) Comparing training loss curves across crate-\(\)-Large with different patch sizes.

**Downstream Applications.** We include additional experimental results on four downstream datasets (CIFAR-10/100, Oxford Flowers, and Oxford-IIT Pets). We also examine the dense prediction capability of crate-\(\) by training it on segmentation tasks using the ADE20K dataset . For language tasks, we conduct new experiments with crate-\(\) using autoregressive training on OpenWebText, following the setup in nanoGPT .

**Interpretability**. Following the evaluation setup of crate as outlined in , we apply MaskCut  to validate and evaluate the rich semantic information captured by our model in a zero-shot setting, including both qualitative and quantitative results.

### Training and Fine-tuning Procedures

**Scaling Study. (1) Base to Large size:** We initially pre-train the crate-\(\) model on ImageNet-21K and subsequently fine-tune it on ImageNet-1K. During the pre-training phase, we set the learning rate to \(8 10^{-4}\), weight decay to 0.1, and batch size to 4096. We apply data augmentation techniques such as Inception crop  resized to 224 and random horizontal flipping. In the fine-tuning phase, we adjust the base learning rate to \(1.6 10^{-4}\), maintain weight decay at 0.1, and batch size at 4096. We apply label smoothing with a smoothing parameter of 0.1 and apply data augmentation methods including Inception crop, random horizontal flipping, and random augmentation with two transformations (magnitude of 9). For evaluation, we resize the smaller side of an image to 256 while maintaining the original aspect ratio and then crop the central portion to 224\(\)224. In both the pre-training and fine-tuning phases, we use the AdamW optimizer  and incorporate a warm-up strategy, characterized by a linear increase over 10 epochs. Both the pre-training and fine-tuning are conducted for a total of 91 epochs, utilizing a cosine decay schedule.

**(2) Large to Huge size:** In the pre-training stage, we utilize an image size of 84\(\)84, and the maximum token length is 32, with a total of 2.56 billion training samples. During the fine-tuning stage, we increase the image size to 224\(\)224 while maintaining the maximum token length at 32, with a 512 million training samples. Here, the key distinction between the pre-training stage and the fine-tuning stage is the image size. A smaller image size results in a faster training speed. In the configurations of crate-\(\)-CLIPA-B, crate-\(\)-CLIPA-L, and crate-\(\)-CLIPA-H, we use the crate-\(\) model as the vision encoder, and utilize the same pre-trained huge transformer model from CLIPA  as the text encoder. For both the pre-training and fine-tuning stages, we freeze the text encoder and only train the vision encoder, i.e., the crate-\(\) model. As we will show in the later results, this setup effectively demonstrates the scaling behaviors of crate-\(\) models in the image domain. Detailed hyperparameter settings can be found in Appendix A.

**Downstream Applications.** On four downstream datasets, we follow the training setup from . For the segmentation task, we compare the performance of CRATE and crate-\(\) on the ADE20K dataset, mainly following the setup of  with minor modifications. Our batch size is set to 128, and the total number of training steps is 5000. For the language task, we conduct experiments with crate-\(\) using autoregressive training on OpenWebText, following the setup in . We compare crate-\(\) models (57M and 120M) with CRATE and GPT-2, using results from CRATE reported in .

   Models (Base) & ImageNet-1K(\%) & Models (Large) & ImageNet-1K(\%) \\   crate-B/16 w/o IN-21K & 70.8\({}^{}\) & crate-L/16 w/o IN-21K & 71.3\({}^{}\) \\   crate-\(\)-B/32 & 76.5 & crate-\(\)-L/32 & 80.2 \\  crate-\(\)-B/16 & 81.2 & crate-\(\)-L/14 & 83.9 \\  crate-\(\)-B/8 & 83.2 & crate-\(\)-L/8 & 85.1 \\   

Table 1: Top-1 accuracy of crate-\(\) on ImageNet-1K with different model scales when pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. For comparison, we also list the results from the paper  which demonstrate the diminished return from crate base to large, trained only on ImageNet-1K. “IN-21K” refers to ImageNet-21K. (\({}^{}\)Results from .)

### Results and Analysis

**Scaling the crate-\(\) Model from Base to Large.** As shown in Table 1, we compare crate-\(\)-B and crate-\(\)-L at patch sizes 32, 16, and 8. Firstly, we find our proposed crate-\(\)-L consistently achieves significant improvements across all patch sizes. Secondly, compared with the results of the vanilla crate (the first row of Table 1), increasing from crate-B to crate-L results in only a 0.5% improvement on ImageNet-1K. This indicates a case of diminishing returns. These findings compellingly highlight that the scalability of crate-\(\) models significantly outperforms that of the vanilla crate. Meanwhile, the training loss in the pre-training stage is presented in Fig. 3; as the model capacity increases, the trend of the training loss improves predictably. This phenomenon is also described in .

**Scaling the crate-\(\) Model from Large to Huge.** From the results shown in Fig. 4, we find that: (1) crate-\(\)-CLIPA-L/14 significantly outperforms crate-\(\)-CLIPA-B/16 by 11.3% and 9.0% in terms of ImageNet-1K zero-shot accuracy during the pre-training and fine-tuning stages, respectively. The substantial benefit suggests that the quality of learned representation may be constrained by the model size. Therefore, increasing the model size effectively leverages larger amounts of data. (2) When continuing to scale up model size, we also observe that crate-\(\)-CLIP-H/14 continues to benefit from larger training datasets, outperforming crate-\(\)-CLIP-L/14 by 3.1% and 2.5% in terms of ImageNet-1K zero-shot accuracy during the pre-training and fine-tuning stages, respectively. This demonstrates the strong scalability of the crate-\(\) model. To explore the performance ceiling, we train a standard ViT-CLIPA-H/14 from scratch and observe improved performance.

**Downstream Applications.** On four downstream datasets, as shown in Table 2, we find that crate-\(\) consistently outperforms CRATE, with both models pre-trained on IN21K, while crate-\(\) demonstrates improved performance as model size increases. For the segmentation task, results in Table 3 show that crate-\(\) consistently outperforms CRATE across all key metrics, with both models pre-trained on IN21K. These findings indicate significant performance gains in vision tasks beyond classification. For the language task, Table 4 shows that crate-\(\) significantly improves over CRATE in language modeling. Due to limited time and resource constraints, we completed 80% of the total iterations for crate-\(\)-small and 55% for crate-\(\)-base, compared to the 600K total iterations used for CRATE. Nevertheless, crate-\(\) still demonstrated notable improvements.

**Interpretability.** As shown in Fig. 5, we provide the segmentation visualization on COCO val2017  for crate-\(\), crate, and ViT, respectively. We find that our model preserves and even improves the (semantic) interpretability advantages of crate. Moreover, we summarize quantitative evaluation results on COCO val2017 in Table 6. Interestingly, when scaling up model size for crate-\(\), the Large model improves over the Base model in terms of object detection and segmentation.

### Compute-efficient Scaling Strategy

We further explore methods to scale models efficiently in terms of computation. Table 1 demonstrates that the crate-\(\) model scales effectively from the Base model to its larger variants. However, the pre-training computation for the top-performing model, crate-\(\)-L/8, is resource-intensive on

Figure 4: (_Left_) Comparing training loss curves of crate-\(\)-CLIPA with different model sizes on DataComp1B. (_Right_) Comparing zero-shot accuracy of crate-\(\)-B/L/H models and ViT-H on ImageNet-1K.

ImageNet-21K. Inspired by CLIPA , we aim to reduce computational demands by using reduced image token sequence lengths, while maintaining the same training setup during the fine-tuning stage. The results are summarized in Table 5.

**Results and analysis.** (1) When fine-tuning with crate-\(\)-L/14 and using crate-\(\)-L/32 for pre-training on ImageNet-21K, this approach consumes about 35% of the TPU v3 core-hours required by crate-\(\)-L/14, yet achieves a promising 83.7% top-1 accuracy on ImageNet-1K, comparable to the 83.9% achieved by crate-\(\)-L/14; (2) When fine-tuning with crate-\(\)-L/8 and using crate-\(\)-L/32 for pre-training, this approach consumes just 15% of the training time required by crate-\(\)-L/8, yet it still achieves a promising 84.2% top-1 accuracy on ImageNet-1K, compared to 85.1% when using the crate-\(\)-L/8 model in the pre-training stage; (3) While the total computational cost of crate-\(\)-L/32 + crate-\(\)-L/8 is less than that of crate-\(\)-L/14 + crate-\(\)-L/14, the performance of the former is slightly better. In summary, we find that this strategy offers a valuable reference for efficiently scaling crate-\(\) models in the future.

## 5 Discussion

**Limitations.** Although we have used some existing compute-efficient training methods (e.g., CLIPA ) and have initiated an exploration into compute-efficient scaling strategies for white-box transformers in Section 4.4, this work still requires a relatively large amount of computational resources, which may not be easily accessible to many researchers.

**Societal impact.** A possible broader implication of this research is the energy consumption needed to conduct the experiments in our scaling study. However, there is growing interest in developing white-box transformers for better interpretability and transparency across a wide range of tasks and domains, including image segmentation , self-supervised masked autoencoders , and integrated sensing and communications , etc. Moreover, our results on the scalability of white

    & GPT-2-base & CRATE-base & CRATE-\(\)-small & CRATE-\(\)-base \\  Model size & 124M & 60M & 57M & 120M \\ Cross-entropy validation loss & 2.85 & 3.37 & 3.28 & 3.14 \\   

Table 4: The comparison between CRATE and CRATE-\(\) on the NLP task using the OpenWebText dataset.

   Dataset & CRATE-B/32 & CRATE-\(\)-B/32 & CRATE-\(\)-L/32 & CRATE-\(\)-B/16 & CRATE-\(\)-L/14 \\  CIFAR-10 & 97.22 & 98.17 & 98.68 & 98.67 & 99.10 \\ CIFAR-100 & 85.27 & 89.40 & 91.16 & 90.58 & 92.57 \\ Oxford Flowers-102 & 93.90 & 97.77 & 99.01 & 99.27 & 99.56 \\ Oxford-IIIIT-Pets & 80.38 & 88.19 & 90.46 & 92.70 & 93.98 \\   

Table 2: The performance comparison between CRATE and CRATE-\(\) across various datasets.

    & GPT-2-base & CRATE-base & CRATE-\(\)-small & CRATE-\(\)-base \\  Model size & 124M & 60M & 57M & 120M \\ Cross-entropy validation loss & 2.85 & 3.37 & 3.28 & 3.14 \\   

Table 5: Compute-efficient scaling strategy. To reduce the compute requirements of the pre-training stage, we use a model with a larger patch size. This results in a shorter token length for the same input size. The second and fourth columns indicate the compute requirements for the pre-training and fine-tuning stages, respectively, measured in TPU v3 core-hours. Details are provided in Section 4.4.

box transformers could also shed light on scaling up a broader class of white-box deep neural networks, such as white-box ISTA networks and their variants [11; 34; 3; 48; 17], designed via unrolled optimization. In summary, we believe that our findings and insights could be helpful for developing white-box transformers for a wide range of applications and tasks, benefiting a broad audience interested in building more interpretable and performant deep learning models and further amortizing the pre-training compute costs.

## 6 Conclusion

This paper provides the first exploration of training white-box transformer crate at scale for vision tasks. We introduce both principled architectural changes and improved training recipes to unleash the potential scalability of the crate type architectures. With these modifications, we successfully scale up the crate-\(\) model along both the dimensions of model size and data size, while preserving, in most cases even improving, the semantic interpretability of the learned white-box transformer models. We believe this work provides valuable insights into scaling up mathematically interpretable deep neural networks, not limited to transformer-like architectures.