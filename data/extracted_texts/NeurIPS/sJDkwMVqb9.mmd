# Cross-links Matter for Link Prediction: Rethinking the Debiased GNN from a Data Perspective

Zihan Luo\({}^{1}\), Hong Huang\({}^{1}\), Jianxun Lian\({}^{2}\), Xiran Song\({}^{1}\), Xing Xie\({}^{2}\), Hai Jin\({}^{1}\)

\({}^{1}\) National Engineering Research Center for Big Data Technology and System,

Service Computing Technology and Systems Laboratory,

Cluster and Grid Computing Lab,

School of Computer Science and Technology,

Huazhong University of Science and Technology, Wuhan, China

\({}^{2}\) Microsoft Research Asia, Beijing, China

{zihanluo,honghuang,xiransong,hjin}@hust.edu.cn,

{jianxun.lian,xingx}@microsoft.com

 Hong Huang is the corresponding author.

###### Abstract

Recently, the bias-related issues in GNN-based link prediction have raised widely spread concerns. In this paper, we emphasize the bias on links across different node clusters, which we call cross-links, after considering its significance in both easing information cocoons and preserving graph connectivity. Instead of following the objective-oriented mechanism in prior works with compromised utility, we empirically find that existing GNN models face severe data bias between internal-links (links within the same cluster) and cross-links, and this inspires us to rethink the bias issue on cross-links from a data perspective. Specifically, we design a simple yet effective twin-structure framework, which can be easily applied to most GNNs to mitigate the bias as well as boost their utility in an end-to-end manner. The basic idea is to generate debiased node embeddings as demonstrations and fuse them into the embeddings of original GNNs. In particular, we learn debiased node embeddings with the help of augmented supervision signals, and a novel dynamic training strategy is designed to effectively fuse debiased node embeddings with the original node embeddings. Experiments on three datasets with six common GNNs show that our framework can not only alleviate the bias between internal-links and cross-links but also boost the overall accuracy. Comparisons with other state-of-the-art methods also verify the superiority of our method.

## 1 Introduction

Recently, due to the strong capability in learning the latent representation of graph structure data, _Graph Neural Networks_ (GNNs)-based link prediction methods have received increasing research interests and show excellent performance in recommendation systems , bioinformatics , and knowledge graph . However, these link prediction methods often prioritize performance without considering potential bias on the sensitive attributes of nodes, such as genders or regions, thus leading to social risks or the creation of informa

Figure 1: A toy graph for illustrating the concept of topological communities. The proportions of two kinds of links also are provided on the right.

tion cocoons. For instance, in social recommendations, models tend to recommend users with friends belonging to the same region, limiting users' opportunities to connect with the outside world . Similarly, in political news recommendations, recommenders prefer to deliver content that aligns with users' partisan beliefs, filtering out different perspectives and narrowing users' scopes . To tackle these issues, researchers have put forward several solutions. For example, FLIP  employs adversarial learning to encourage the model to predict outcomes that are independent of sensitive attributes. Similarly, CFC  aims to debias the node embeddings with compositional adversarial constraints. FairAdj  learns edge weights to generate a fair adjacency matrix, which is used for subsequent link prediction tasks. UGE  explores unbiased node embeddings from an unobserved graph, and proposes two kinds of methods based on reweighting and regularization constraints.

Although these methods make great progress in mitigating the bias in link prediction, there are two issues remaining unsolved: 1) All these methods generally modify the objective functions, like adding regularization constraints [4; 28] or reweighting  for mitigating the bias. However, the mechanism of simply modifying objective function may influence the optimization trajectory of the model and result in convergence towards a sub-optimal status. The experimental trade-off between debias and utility on multiple prior works [4; 20; 28; 38] also supports this claim. 2) Existing methods simply focus on sensitive attributes of nodes while ignoring the bias based on graph topology. In fact, due to GNNs' heavy dependence on the aggregation of neighborhood messages, for the link prediction tasks, GNNs tend to wire new links inside the local community2 (denoted as **internal-links**), while ignoring the links connecting different communities (denoted as **cross-links**), and this kind of bias may leave the graph in danger of being trapped in information cocoons or disconnected [21; 28], as shown in Figure 1. Empirical evidence can also be provided in Figure 2, in which we illustrate the top 10 commonest labels' distributions in different communities. It can be seen that each community contains its own specific information pattern , making it challenging for one single community to encompass all diverse information in a network. This observation implies the propensity of graphs to form information cocoons with insufficient cross-links as bridges3. Motivated by these findings, we aim to design a GNN model that could boost the link prediction performance on cross-links and mitigate the bias between internal-links and cross-links without sacrificing utility.

By rethinking the problem of bias between internal-links and cross-links with a fresh insight from the data perspective, we statistics the proportion of internal-links and cross-links in three real-world datasets in Figure 1, and we can find that the number of internal-links far exceeds that of cross-links. This kind of data bias may be further enlarged and perpetuated by stacking GNN aggregation operations and finally leads to a biased link prediction. In light of this, we propose a simple yet effective twin-structure framework, which consists of two independent GNN models to mitigate the bias between internal-links and cross-links. Specifically, we first divide the whole graph into multiple communities that share similar topological locations and differentiate links into internal-links and cross-links. After that, in order to alleviate the data bias between internal-links and cross-links, supervision augmentation based on multiple rules is proposed to increase the supervised signals for cross-links, which could help the GNNs to better capture the patterns of cross-links and further generate debiased node embeddings. Subsequently, to avoid utility degradation, we design an embedding fusion module to merge original node embeddings and debiased node embeddings with a dynamic training strategy. In this way, the embedding fusion module could effectively preserve the performance of internal-links while alleviating the bias between internal-links and cross-links. Our main contributions can be summarized as follows:

* We reveal a significant link prediction bias based on the graph topology, i.e. the bias between internal-links and cross-links, and design a model-agnostic framework, which can help most of the GNNs address such bias without compromising utility.

Figure 2: Distributions for top 10 labels in 10 different communities detected by Louvain on LastFM

* We propose three core components in our framework, including supervision augmentation, twins-GNN, and embedding fusion module. The combination of supervision augmentation and twins-GNN can generate debiased embeddings from a data aspect as demonstrations, while the embedding fusion module can effectively filter out the noise and preserve the utility of GNN.
* We evaluate our method on three real-world datasets with six common GNNs as base models and as well as compare them with other state-of-the-art baselines. The experimental results consistently demonstrate that our framework could effectively reduce the bias between internal-links and cross-links with even improved overall performance.

## 2 Related Work

Link PredictionAs one of the most important applications in graph learning, link prediction algorithms have received extensive attention in the last two decades [3; 12; 14; 29]. The traditional link prediction methods are mainly based on some heuristic metrics on the graph structure, such as CN , AA , and Jaccard coefficient. In recent years, shallow embedding-based link prediction methods such as DeepWalk , Node2Vec , and LINE  have also emerged. With the powerful capability in representation learning on graph structure data, GNNs have achieved great progress in link prediction tasks as well [3; 13; 14; 19; 37; 48]. For example, LightGCN discovers the redundancy of non-linear activation and feature transformation and achieves better performance on recommendations . PPRGo  uses a Personalized Page Rank matrix to efficiently approximate the multi-hop aggregation, which breaks the classical message-passing paradigm of GNNs. Following LightGCN, UltraGCN  approximates the infinite-layer aggregation for better recommendations. Besides, instead of learning from the whole graph, another line of research focuses on learning from relatively small subgraphs, such as SEAL  and SUREL+ , which also achieve remarkable performance in link prediction.

Bias Related Issues in GNNsRecently, bias-related issues have been widely concerned in graph neural networks, termed as "debias" or "fairness". Concentrating on node classification tasks, there is a branch of works on achieving fair results that are uncorrelated with sensitive attributes, including FairGNN , FairVGNN , EDITS , and EqGNN . As for bias in link prediction, based on node2vec , Fairwalk  proposes a more fair random walk strategy, but fails to avoid the decrease in models' utility after achieving satisfactory fairness. Both FLIP  and CFC  try to mitigate the dyadic bias by proposing adversarial constraints in the loss function. Li et al.  designed a model named FairAdj, which could generate a fair adjacency matrix with different edge weights to address the dyadic bias with competitive utility. Similar to FairAdj, UGE  derives unbiased embeddings from an unobserved graph that involves no sensitive attribute information. However, all these methods modify the original objective functions and face a trade-off between debias and utility.

## 3 Methodologies

### Problem Formulation

In this work, a graph is denoted by \(=(,)\), and it consists of \(N=||\) vertices and \(||\) links. In the real world, nodes on the graph spontaneously form local communities , such as social circles in social networks, and various bundles of also-buy items on product graphs. We use \((v)\) to denote the community membership of a given node \(v\) and define _cross-links_ and _internal-links_:

**Definition 1**.: _Cross-links and Internal-links. Given a link \( u,v\), it belongs to cross-links \(_{cr}\) if its endpoints' memberships satisfy \((u)(v)\); otherwise, it belongs to internal-links \(_{in}\) as it satisfies \((u)=(v)\). So according to the endpoints' memberships, the links \(\) in a graph could be divided into \(_{in}\) and \(_{cr}\) two sets exclusively, i.e. \(_{in}_{cr}=\) and \(_{in}_{cr}=\)._

Centering on the link prediction based on GNNs, each node will be mapped to an \(D\) dimensional embedding vector \(^{D}\) by a GNN encoder. The dot product score of a pair of nodes' embeddings implies the GNN model's confidence on whether there will be a potential link between two given vertices. Our goal is to mitigate the bias between internal-links and cross-links, i.e.:

**Definition 2**.: _Bias between internal-links and cross-links (Bias). Given a trained GNN model, a confidence matrix \(^{N N}\) can be inferred by calculating the dot product of endpoints'embeddings, and a higher \(_{uv}\) indicates that model predicts that it very likely exists a link between node \(u\) and node \(v\). Let the notation \(\) represent an evaluation metric in link prediction tasks, and we use the difference between two kinds of links' average performance to define the bias:_

\[=|[()|=_{in}]- [()|=_{cr}]|\] (1)

**Task definition.** To address such bias, a GNN recommender should balance the performance on internal-links and cross-links, so that the final model could show equivalent capability on mining true internal-links and cross-links. The goal of this paper is to design a model-agnostic framework, which can enhance the embedding ability of a given GNN model so that the bias metrics are reduced while the accuracy metrics are at least not decreased:

\[((|))<(( |^{})),(|) (|^{})\] (2)

where \(^{}\) represents the original node embeddings given by a GNN, and \(\) represents the node embeddings enhanced by our framework.

### Framework Overview

We propose a twin-structure GNN-enhancement framework, which can reduce the bias between cross-links and internal-links without hurting the overall performance. An overview of our framework is illustrated in Figure 3. Its key components include supervision augmentation, Twins GNNs, and an embedding fusion module. Details will be provided in the following sections. In traditional graph embedding models, existing links on the graph are used as supervision signals. In our framework, we first generate a certain number of pseudo cross-links and form the augmented supervision signals. Then, we use two GNNs with the same architecture to model the two sets of supervision signals (original supervision and augmented supervision) independently, leading to original node embeddings and debiased node embeddings. Lastly, to avoid the impact of noise during supervision augmentation, an embedding fusion module is proposed, which ensures that the final embeddings will retain the merits of both original node embeddings and debiased node embeddings.

### Supervision Augmentation

As revealed in Figure 1, in most datasets, usually the vast majority of links on the graph are internal-links, and this kind of disparity may cause the learned model to be biased to internal-links while neglecting the performance of the cross-links in order to obtain better overall metrics. Therefore, we design an augmentation step to alleviate the supervision signal sparsity issue.

**Jaccard based augmentation.** Inspired from the concept of edge strength , a set of \(K\) pairs of nodes \(\{_{1},_{1},...,_{K},_{K}\}\), which satisfy \((_{i})(_{i}),\ \  i\{1,...,K\}\), and have the top \(K\) highest Jaccard coefficient score \((,)\), are wired as pseudo cross-links. Specifically, the \((,)\) can be formulated as:

\[(,)=()()|}{|()()|}\] (3)

and \(K=k*(|_{in}|-|_{cr}|)\), where \(k\) is a hyper-parameter to control the supervision augmentation size \(K\), and \(()\) represents the neighbor nodes of a given node. \((,)\) measures the proportion of common neighbors between two nodes, and intuitively, a pair of nodes with more common neighbors are more likely to form a potential link, which ensures the high confidence of augmented signals.

**Random walk based augmentation.** However, when a network is extremely dense, adopting Jaccard-based augmentation will consume too much computing resource due to the "neighbor explosion" issue. What's more, the Jaccard-based supervision augmentation may easily only cover nodes around the border of communities, and nodes in the center of a community may still fail to benefit from the

Figure 3: The overview of our framework

augmentation. To solve these problems, we propose a new supervision augmentation method based on random walks. Specifically, we take random walks in the original graph, and each pair of nodes \(,\) that appear in the same walk path will be recorded. Next, cross-community node pairs, _i.e._, \(()()\), with the top \(K\) highest co-occurrence frequency will be screened out as augmented supervision signals. Similarly, \(K=k*(|_{in}|-|_{cr}|)\) represents the supervision augmentation size.

Note that, we only generate pseudo supervision signals, and the original graph structure, which is used for message passing in GNNs, remains unchanged. We believe that the lack of supervision plays a much more important role in causing the bias between internal-links and cross-links and adding supervision signals is a more straightforward way to alleviate this kind of bias. Let \(^{P}\) denote the newly constructed edge set. The original supervision signals, which are the original links on the graph, are denoted by \(^{O}\), and the final augmented supervision signals are denoted by \(^{A}=^{O}^{P}\).

### Twins GNNs

Recently, the retrieval-based learning paradigm, in which the core idea is to provide supporting examples that can be used as references for the model, has extracted increasing research interest in the field of natural language processing . Inspired by this, it would be a promising way to generate debiased node embeddings as references for our framework to mitigate the bias between internal-links and cross-links. Specifically, we let \(^{O}\) and \(^{A}\) to guide the training of two twin GNNs, respectively. The two twin GNNs share the same model architecture but have independent sets of parameters. To this end, the twin-structure GNNs could generate two kinds of embeddings named original node embeddings \(^{O}\) and debiased node embeddings \(^{A}\), respectively. In order to make sure that the twin-structure GNNs could precisely generate original embeddings and debiased embeddings for further embedding fusion, we follow the literature [14; 34] and employ one of the most classic loss functions in recommendation systems - BPRLoss to design our two auxiliary loss functions for the twin-structure GNNs, which can be generalized as:

\[=-|}_{ u,v} (r_{u,v}-r_{u,})\] (4)

where \(\) could be \(^{O}\) or \(^{A}\) depending on the type of generated embeddings, and the auxiliary loss functions, are denoted as \(^{O}\) and \(^{A}\), respectively. \(\) is an activation function like sigmoid. \( u,\) denotes a negative sample, and \(r_{u,v}\) denotes the prediction score of a given node pair \( u,v\). Note that, other alternative loss functions with multiple negative samples can be also easily deployed to our method, which is determined by the practitioners according to their practical applications. Intuitively, with the balanced proportion of two types of links in \(^{A}\) after supervision augmentation, the debiased node embeddings \(^{A}\) could show excellent capability in eliminating the bias between internal-links and cross-links. The debiased node embeddings \(^{A}\) will become a good demonstration for subsequent embedding fusion modules in the further learning of final embeddings.

### Embedding Fusion

Although we set several heuristic rules in Section 3.3 to ensure that the generated signals have high confidence, the supervision augmentation stage will inevitably introduce noise. Thus, \(^{A}\) is trained with impure supervision. If fusing \(^{A}\) into \(^{O}\) in a naive way, such as a simple averaging, although the performance on the cross-links can be improved, the internal-links' will be considerably degraded.

Thus, we adopt an embedding fusion module \(F()\) that merges two kinds of embeddings, i.e. original node embeddings \(^{O}\) and debiased node embeddings \(^{A}\), to refine meaningful information from the debiased embeddings and fuse them into the original embeddings:

\[_{i}=(^{O}_{i},^{A})\] (5) \[_{i}=F(_{i},^{F})=_{2}\ ( _{1}_{i}+_{1})+}\] (6)

where \(_{1}^{D 2D}\), \(_{1}^{D}\), \(_{2}^{D D}\), \(_{2}^{D}\) are the trainable parameters of \(F()\), and can be abbreviated as \(^{F}\) for conciseness. \(\) is the activation function and \(D\) is the embedding dimension. Note that, both \(^{O}_{i}\) and \(^{A}_{i}\) are output embeddings from GNN models supervised by auxiliary loss, and we believe that there is no need to further deploy a complex network during embedding fusion, hence we simply show a 2-layer MLP in Eq.(6).

As the core component of our framework, we argue that the final embeddings \(\) after embedding fusion function \(F()\) should filter signal noise in \(^{A}\) while preserving as much correct augmented information as possible. Therefore, we optimize embedding fusion module \(F()\) and twin-GNNs by minimizing the following objective function :

\[^{F}=-^{O}|}_{ u,v ^{O}}(r_{u,v}-r_{u,})\] (7)

```
0: Graph \(\) with link set \(^{O}\). Hyper-parameters: \(\), \(\), and \(T\), learning rate \(^{O}\), \(^{A}_{^{}}\)
0: Twin GNNs' parameters \(^{O},^{A}\), and embedding fusion module parameters \(^{F}\)
1: Randomly initialize twins GNN models with \(^{O}\), \(^{A}\), embedding fusion module with \(^{F}\).
2: Split \(\) into \(||\) communities and categorize links into internal-links and cross-links.
3: Select augmented supervision signals \(^{A}\) by the Jaccard coefficient or co-occurrence frequency.
4:while not converged do
5: Compute \(^{O}\) and \(^{A}\) by Eq.(4)
6: Update twins GNN models: \(^{O}^{O}+^{O}_{^{O}}^ {O}\), \(^{A}^{A}+^{A}_{^{A}}^ {A}\)
7: Compute learning rate \(^{F}_{t}\) and step size \(S_{t}\) by Eq.(8)
8:for\(step=1\)to\(S_{t}\)do
9: Compute \(^{F}\) by Eq.(7)
10: Update embedding fusion module: \(^{F}^{F}+^{F}_{t}_{^{F}}^{F}_{t}\)
11: Update twins GNN models: \(^{O}^{O}+^{F}_{t}_{^{O}}^{F}\), \(^{A}^{A}+^{F}_{t}_{^{A}}^ {F}\)
12:endfor
13:endwhile
14:return\(^{O},^{A},^{F}\) ```

**Algorithm 1** Proposed training process

### Implementation Algorithms

For a better understanding of our method, here we summarize the detailed training process and inference process, and the pseudo codes are provided in Algorithm 1 and Algorithm 2, respectively.

_Training process_: Firstly, during the preprocessing stage, \(C=||\) communities are detected through Louvain algorithm  and all links are categorized into internal-links or cross-links, respectively (line 2). In addition, newly constructed supervision signals \(^{P}\) are also generated during supervision augmentation (line 3). After that, original node embeddings \(^{O}\) and debiased node embeddings \(^{A}\) are generated by minimizing the auxiliary loss functions in Section 3.4 (lines 5-6) with learning rate \(^{O}\) and \(^{A}\), respectively. The two sets of generated embeddings will be forwarded to the embedding fusion module \(F()\) to get the final embeddings \(=F(^{O},^{A})\), and minimize the final loss function with multiple training steps (lines 8-12.) Essentially, the embedding fusion module \(F()\) is highly related to the quality of original embeddings \(^{O}\) and debiased embeddings \(^{A}\), thus we introduce a dynamic training strategy to avoid excessive training for embedding fusion module when \(^{O}\) and \(^{A}\) have not been stable yet. Specifically, the learning rate \(^{F}_{t}\) and training step \(S_{t}\) for \(F()\) at epoch \(t\) is:

\[^{F}_{t}=*}, S_{t}=*}\] (8)

where \(\), \(\), and \(T\) are hyper-parameters to control the learning rate, training steps, and the epoch that \(F()\) can believe the quality of input embeddings have been stable, respectively.

_Inference process_: During the inference stage, we use the node embeddings \(\) for evaluation (line 4), which are the output of the embedding fusion module. Specifically, we first get node embeddings \(^{O}\) and \(^{A}\) through the trained twins GNN in advance (lines 1-2), and then perform embedding fusion operation (line 3) to get the final node embeddings \(\).

## 4 Experiment

### Experimental Settings

**Datasets.** We conduct our experiments on three real-world networks from SNAP4 and RecBole5 public datasets. Specifically, to verify our framework's extendibility, we choose two kinds of networks according to the types of interaction between nodes, including user-user (Epinions, DBLP), and user-item (LastFM). **Epinions** is a who-trust-whom social network extracted from the online review site Epinions.com. Each node represents a consumer, and each directed link represents a consumer-to-consumer trust connection. **DBLP** is a co-author social network, which is collected from the DBLP computer science bibliography, and two author nodes are connected if they used to publish at least one paper together. **LastFM** dataset contains users' listening information from the Last.fm online music system and each listening event represents a user-artist interaction. The detailed statistics for these three datasets are reported in Table 1, and other experimental settings on datasets are described in Section B in the Appendix.

**Base models.** Because our proposed method is a model-agnostic framework, which is compatible with almost all GNN-based link prediction models, here we choose six common and effective GNNs as our base models, including **GraphSAGE**, **GIN**, **GAT**, **LightGCN**, **PPRGo**, and **UltraGCN**. We verify if our framework could mitigate the bias between internal-links and cross-links without hurting the original overall performance.

**Reproducibility.** All experiments are run on machines with the same configuration: Intel(R) Xeon(R) CPU E5-2680 and Tesla V100 GPU with 32GB memory. How to detect the communities of a network is not the research focus of this paper, and here we apply the Louvain algorithm  to detect the communities in a graph after considering its high speed and effectiveness. Other alternative community detection algorithms and hyper-parameters will be discussed in the Appendix. The source code and data are available at https://github.com/CGCL-codes/Cross-links-Bias.

### Main Results

In this part, we show the performance of our method in terms of both link prediction utility and debias effectiveness after applying it to various GNN models.

The main results on three datasets are reported in Table 2. For Epinions and DBLP, we use Jaccard-based augmentation here, and considering the high density of LastFM, we use random walk-based augmentation. More results with random walk-based augmentation on Epinions and DBLP are listed in Section C in the Appendix for reference. The internal-links performance (Internal.), cross-links performance (Cross.), and overall performance (Overall) are calculated on internal-links in the test set, the cross-links in the test set, and the whole test set respectively. To verify our method's capability of addressing the bias, \(Bias\) in Eq.(1) is also computed to evaluate the performance difference between internal-links and cross-links. Table 2 shows the results of our approach and base models, and we can have the following observations:

* The performance of internal-links outperforms that of cross-links under all settings, which indicates the link prediction bias between internal-links and cross-links is widespread. Note that, after proposing our framework, we can only ease this kind of bias to some extent, which inspires us the reasons for the poor performance on cross-links may not only come from the data perspective, and we leave this problem as a future work.

  
**Datasets** & **Users** & **Items** & **Interactions** & **Density** & **Type** \\  Epinions & 75,879 & - & 508,837 & 0.000088 & User-user \\  DBLP & 317,080 & - & 1,049,866 & 0.000010 & User-user \\  LastFM & 1,892 & 17,632 & 92,834 & 0.002783 & User-item \\   

Table 1: Dataset Statistics* After proposing our method, all base models get improved on both internal-links and cross-links in most cases, which results in an improvement in the overall performance. The reasons for this impressive improvement are two-fold. For one thing, supervision augmentation directly helps boost the performance of cross-links. For another, the augmented supervisions play a regularization role in the learning of node embeddings, thus, the overall quality of representations can be improved.
* The increase of cross-links is much higher than that of internal-links, and this contributes to the decrease of bias. For instance, when our framework is applied to LightGCN on DBLP, the performance of internal-links improves from 85.95% to 93.55%, and the performance of cross-links improves from 47.41% to 65.33%, thus the bias decreases from 38.54% to 28.22%.

### Ablation Study

In this section, we want to explore: 1) the impact of supervision augmentation on our framework; 2) the impact of the embedding fusion module on our framework; 3) the impact of two auxiliary loss functions in Eq.(4); 4) the impact of dynamic training strategy during training embedding fusion module. As an example, we show the results of LightGCN on two datasets in Table 3. All results are based on Jaccard-based augmentation.

We first replace the augmented cross-links signals, which are selected through Jaccard coefficient, with random node pairs across different communities, and denote this variant as "-Augment". There are two main observations: 1) After adding random-augmented signals, the overall performance still shows a significant improvement compared to the base model, which indicates that the embedding fusion module can effectively filter out useful information from the augmented embeddings and fuse it into the original embeddings; 2) Compared with our framework, the random-augmented framework demonstrates unsatisfactory results on reducing the performance disparity between cross-links and internal-links, such as slightly reducing this kind of bias on Epinions dataset from 9.32% (46.43% - 37.11%) to 8.46% (49.43% - 40.97%), and this indicates the importance of our supervision augmentation method on debias.

Secondly, after removing the embedding fusion module (denoted as "-Fusion"), although the cross-links performance of LightGCN gets significant improvement, i.e., from 37.11% to 46.39% on the Epinions dataset and from 47.41% to 66.97% on the DBLP dataset, the internal-links performance and overall performance get severe influence comparing with our framework. Especially on DBLP, the internal-links performance of LightGCN is deceased by 13.03% (from 85.95% to 72.92%), while our framework could even slightly improve the performance of internal-links. It indicates that, after proposing the embedding fusion module, our framework could effectively filter out noisy augmented signals to avoid performance deterioration.

    & &  &  \\   & Internal\(\) & Cross\(\) & Overall\(\) & Bias\(\) & Internal\(\) & Cross\(\) & Overall\(\) & Bias\(\) & Internal\(\) & Cross\(\) & Overall\(\) & Bias\(\) \\   & Orig. & 31.68 & 28.91 & 30.69 & 2.77 & 69.27 & 14.62 & 56.41 & 54.65 & 32.84 & 14.84 & 26.80 & 18.00 \\   & Debias & **36.98** & **34.45** & **36.08** & **1.63** & **77.18** & **31.54** & **66.23** & **45.64** & 32.73 & **15.13** & **26.80** & **17.60** \\   & Orig. & 33.49 & 30.97 & 32.59 & 2.52 & 56.66 & 16.86 & 47.29 & 39.80 & 33.63 & 16.91 & 28.00 & 16.72 \\   & Debias & **40.56** & **39.39** & **40.20** & **1.26** & **60.61** & **34.03** & **54.35** & **26.58** & 32.13 & **19.88** & **28.00** & **12.25** \\   & Orig. & 39.30 & 34.90 & 37.73 & 4.60 & 62.52 & 22.47 & 55.94 & 43.78 & 32.28 & 12.76 & 25.70 & 19.52 \\   & Debias & **39.58** & **36.13** & **38.35** & **34.56** & **37.66** & **46.66** & **48.20** & **34.24** & **16.32** & **28.20** & **17.92** \\   & Orig. & 42.86 & 28.75 & 37.83 & 14.11 & 85.71 & 41.14 & 75.28 & 44.58 & 34.84 & 17.51 & 29.00 & 17.33 \\   & Debias & **47.44** & **42.34** & **45.62** & **5.10** & **95.44** & **55.64** & **82.32** & **34.90** & **35.14** & **19.88** & **30.00** & **15.26** \\   & Orig. & 46.43 & 37.11 & 43.11 & 9.32 & 85.95 & 47.41 & 76.88 & 38.54 & 38.16 & 16.62 & 30.90 & 21.54 \\   & Debias & **51.49** & **45.31** & **49.29** & **6.18** & **93.55** & **65.33** & **86.90** & **28.22** & **38.31** & **17.51** & **31.30** & **20.80** \\   & Orig. & 30.62 & 5.81 & 21.78 & 24.81 & 95.74 & 63.82 & 88.22 & 31.92 & 32.73 & 15.73 & 27.10 & 17.00 \\   & Debias & **44.13** & **43.66** & **43.96** & **0.67** & **97.14** & **71.71** & **91.15** & **25.43** & **35.29** & **18.40** & **29.60** & **16.89** \\   

Table 2: Link prediction performance (Hits@50) of internal-links, cross-links, and the whole link set of our methods and corresponding base models on three real-world datasets. The results are reported in percentage (%). We **bold** the results when our framework improves the base GNN model.

    &  &  \\   & Internal\(\) & Cross\(\) & Overall\(\) & Internal\(\) & Cross\(\) & Overall\(\) \\  Original & 46.43 & 37.11 & 43.11 & 85.95 & 47.41 & 76.88 \\ Ours & **51.49** & 45.31 & **49.29** & **93.55** & 65.33 & **86.90** \\  - Augment & 49.43 & 40.97 & 46.72 & 92.76 & 60.97 & 85.24 \\ - Fusion & 44.31 & **46.39** & 44.87 & 72.92 & **66.97** & 71.53 \\ - Auxiliary Loss & 49.63 & 41.58 & 46.75 & 85.31 & 60.61 & 84.85 \\ - Dynamic & 48.63 & 41.58 & 46.12 & 93.09 & 59.08 & 85.08 \\   

Table 3: Ablation studies with LightGCN as the base model. The results (Hits@50) are reported in percentage (%). The best results are **bold**, and the runner-up is underlined.

Finally, after discarding the two auxiliary loss functions of twin GNNs in Eq.(4) ("-Auxiliary Loss") or setting a fixed learning rate \(^{Fe}\) and training step \(S^{*}\) in the embedding fusion module ("-Dynamic"), all metrics deteriorate slightly compared to our proposed framework. This matches our expectation, because both the lack of guidance from auxiliary loss and a fixed training strategy would inevitably influence the quality of two sets of input embeddings, thus influencing the final embeddings.

### Comparison with Other Competitors

In this section, we aim to compare our methods with several powerful competitors on reducing the bias between internal-links and cross-links. Specifically, we choose five highly related methods as our baselines, including FairWalk , CFC , FairAdj , FLIP , and UGE , and we replace the required sensitive attributes in these methods with community memberships detected by Louvain algorithm  in advance. To be fair, all models' embedding dimensions are set to 64, and other details on hyper-parameters are listed in Section B in the Appendix for saving space. Note that, due to the algorithm's requirement on the multiplication of the adjacency matrix, FairAdj reports "OOM" errors when deployed on Epinions and DBLP. For comparison, we report the results of GraphSAGE  trained with our framework (denoted as Debias-SAGE). The same as Section 4.2, we take Jaccard-based augmentation on Epinions and DBLP and take random walk-based augmentation on LastFM.

As is shown in Table 4, our method achieves state-of-the-art results on overall performance with top-ranking debias results. Especially on the DBLP dataset, our method's overall performance far exceeds that of all other baselines, while achieving the best results in reducing the bias between internal-links and cross-links. We believe the reasons are two-fold: 1) With the model-agnostic design of our framework, we could easily deploy our method on many powerful GNN models. 2) Instead of adding extra constraints in the loss function or modifying the objective functions, our method aims at addressing the bias between internal-links and cross-links from a data perspective, and the supervision augmentation plays a regularization role in boosting the overall performance.

### Empirical Findings on Easing Information Cocoons

With emphasizing the performance on cross-links, one merit of our model is the potential capability to ease information cocoons, which can be empirically verified from the following two aspects.

**Debiased Recommendation.** In this part, we further explicitly reveal whether our framework can provide debiased recommendations for nodes to ease the information cocoons. In detail, we train several powerful GNN models, including LightGCN , PPRGo , and UltraGCN  on Epinions and DBLP. Next, we randomly select 2000 source nodes and observe the average proportion of internal-links in their historical data and their recommendation lists given by GNNs. As is shown in Figure 4, the recommendation lists given by GNN models significantly magnify the nodes' original preference on internal-links. From the perspective of the users (or nodes receiving recommendations), such biased recommendations will distort users' true preference by ignoring their niche interests (cross-links) while emphasizing the mainstream interests (internal-links), and make users feel confined to limited domains. For comparison, we provide the results of our enhanced GNN models as well, and it can be seen that after proposing our method (denoted as "Ours"), the distributions of

    &  &  &  \\   & Overall\({}^{*}\) & Bias\({}_{*}\) & Overall\({}^{*}\) & Bias\({}_{*}\) & Overall\({}^{*}\) & Bias\({}_{*}\) \\  FairWalk & 23.75 \(\) 1.1 & 7.16 \(\) 0.7 & 48.81 \(\) 1.3 & 50.00 \(\) 2.1 & 23.05 \(\) 0.6 & 21.78 \(\) 1.0 \\  CFC & 27.35 \(\) 0.4 & 2.01 \(\) 0.3 & 54.12 \(\) 2.1 & 49.37 \(\) 2.6 & 24.25 \(\) 0.5 & 17.79 \(\) 0.3 \\  FairAdj & & & & & & 23.97 \(\) 1.2 & 19.05 \(\) 0.9 \\  FLIP & 26.55 \(\) 0.6 & 4.05 \(\) 0.5 & 55.23 \(\) 1.3 & 45.91 \(\) 2.1 & 27.34 \(\) 0.4 & 23.25 \(\) 0.5 \\  UGE & 27.22 \(\) 0.9 & **1.29 \(\) 0.1** & 54.66 \(\) 1.3 & 54.66 \(\) 2.3 & 28.50 \(\) 0.9 & **14.00** \(\) 0.7 \\  GraphSAGE & **30.09** \(\) 0.9 & 2.77 \(\) 0.3 & 56.41 \(\) 1.5 & 54.65 \(\) 1.6 & 26.80 \(\) 0.5 & 18.00 \(\) 0.6 \\  Debias-SAGE & **30.08** \(\) 0.7 & 1.63 \(\) 0.2 & **66.23** \(\) 1.9 & **45.64** \(\) 1.7 & **26.80** \(\) 0.4 & 17.60 \(\) 0.3 \\   

Table 4: Comparison with several competitors on both utility and debias. The average results (in Hits@50) are reported in percentage (%) after repeating each method three times. The best results are **bold**, and the runner-up is underlined.

Figure 4: The internal-links proportion of historical distribution and multiple GNNs’ recommendations before and after applying our method. “His.”, “Rec.”, and “Ours.” denote the historical interactions, original recommendations, and our debiased recommendations, respectively.

recommendations become closer to that of historical data, which indicates that our approach could effectively address GNN models' biased recommendation and thus ease the information cocoons.

**Visualization of Breaking Filter Bubbles.** To visualize our proposed framework's effect on alleviating information cocoons, we use Gephi6 to show the graphs reconstructed by node embeddings learned through the base model and our model, respectively. To be specific, we take UltraGCN  as the base model here and only a subset of nodes from different communities are shown for conciseness. Note that, all the hyper-parameters in the Gephi platform are set to be the same, and the visualization results reflect the natural layout of the reconstructed networks. The visualization results are shown in Figure 5, and node colors represent the community tags. To better understand the visualization results, we also provide the modularity value in each subgraph, which measures the strength of the division of a network. It can be seen that after proposing our framework, except for some extreme outliers, the connections between most communities are becoming closer, and the isolation between communities is broken down. The reduced modularity values can also support our claims. These results indicate that the embedding generated by our model can effectively facilitate the generation of cross-links in a network, thus alleviating information cocoons.

## 5 Conclusion

In this paper, we aim to address the bias in GNN-based link prediction, especially the bias based on cross-links (links cross communities) after considering its specific roles in easing information cocoons and connecting different communities. We further break the paradigm of modifying objective functions in prior works while pursuing debiased performance, and turn to address the bias issue on cross-links from a data perspective, which could effectively alleviate the bias by even boosting models' prediction utility. Specifically, by borrowing the concept in retrieval-based learning paradigms, debiased node embeddings are generated as demonstrations. An embedding fusion module with dynamic training strategies is also proposed to ensure that the final embeddings could retain the merits of both original embeddings and debiased embeddings. Experiments on three real-world datasets with six base GNNs indicate that our framework could not only reduce the performance disparity between internal-links and cross-links but also significantly improve the overall performance, and further alleviate the potential information cocoons.

**Limitations & Discussions.** Firstly, although we have greatly reduced the bias between internal-links and cross-links, the experimental results indicate that the bias is not clearly eliminated, which implies that data bias may not be the only reason. What's more, despite the supervision augmentation having a potential regularization effect, and leading to improved performance on internal-links, we currently lack theoretical underpinnings and rigorous analysis for this phenomenon. However, we find some potential connections between our work and counterfactual learning. In our settings, supervision augmentation will introduce plenty of unobserved/counterfactual samples for GNN learning. This kind of counterfactual learning may help GNN to better capture the intrinsic relationships between a pair of nodes, thus giving a more accurate prediction.

Figure 5: The visualization of subgraphs reconstructed by the embeddings learned through the base model (above) and ours (below). Note that, each column shows the visualization of the same subgraph, and subgraphs generated by our approach show minor topological isolation phenomenons.