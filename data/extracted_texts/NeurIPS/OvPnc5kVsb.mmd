# Importance-aware Co-teaching for Offline Model-based Optimization

Ye Yuan1, Can (Sam) Chen1,2\({}^{*}\), Zixuan Liu3, Willie Neiswanger4, Xue Liu1

1McGill University, 2MILA - Quebec AI Institute,

3University of Washington, 4Stanford University

ye.yuan3@mail.mcgill.ca, can.chen@mila.quebec,

zucksliu@cs.washington.edu, neiswanger@cs.stanford.edu,

xueliu@cs.mcgill.ca

Equal contribution with random order.Corresponding author.

###### Abstract

Offline model-based optimization aims to find a design that maximizes a property of interest using only an offline dataset, with applications in robot, protein, and molecule design, among others. A prevalent approach is gradient ascent, where a proxy model is trained on the offline dataset and then used to optimize the design. This method suffers from an out-of-distribution issue, where the proxy is not accurate for unseen designs. To mitigate this issue, we explore using a pseudo-labeler to generate valuable data for fine-tuning the proxy. Specifically, we propose _**I**mportance-aware **Co-Te**aching for Offline Model-based Optimization_ (**ICT**). This method maintains three symmetric proxies with their mean ensemble as the final proxy, and comprises two steps. The first step is _pseudo-label-driven co-teaching_. In this step, one proxy is iteratively selected as the pseudo-labeler for designs near the current optimization point, generating pseudo-labeled data. Subsequently, a co-teaching process identifies small-loss samples as valuable data and exchanges them between the other two proxies for fine-tuning, promoting knowledge transfer. This procedure is repeated three times, with a different proxy chosen as the pseudo-labeler each time, ultimately enhancing the ensemble performance. To further improve accuracy of pseudo-labels, we perform a secondary step of _meta-learning-based sample reweighting_, which assigns importance weights to samples in the pseudo-labeled dataset and updates them via meta-learning. ICT achieves state-of-the-art results across multiple design-bench tasks, achieving the best mean rank of \(3.1\) and median rank of \(2\), among \(15\) methods. Our source code can be found here.

## 1 Introduction

A primary goal in many domains is to design or create new objects with desired properties . Examples include the design of robot morphologies , protein design, and molecule design [3; 4]. Numerous studies obtain new designs by iteratively querying an unknown objective function that maps a design to its corresponding property score. However, in real-world scenarios, evaluating the objective function can be expensive or risky [3; 4; 5; 6; 7]. As a result, it is often more practical to assume access only to an offline dataset of designs and their property scores. This type of problem is referred to as offline model-based optimization (MBO) . The goal of MBO is to find a design that maximizes the unknown objective function using solely the offline dataset.

Gradient ascent is a common approach to address the offline MBO problem. For example, as illustrated in Figure 2 (a), the offline dataset may consist of three robot size and robot speed pairs \(p_{1,2,3}\). A simple DNN model, referred to as the _vanilla proxy_ and represented as \(f_{}()\), is trained to fit the offline dataset as an approximation to the unknown objective function. Gradient ascent is subsequently applied to existing designs with respect to the vanilla proxy \(f_{}()\), aiming to generate a new design with a higher score. However, the gradient ascent method suffers from an out-of-distribution issue, where the vanilla proxy cannot accurately estimate data outside of the training distribution, leading to a significant gap between the vanilla proxy and the ground-truth function, as shown in Figure 2 (a). As a consequence, the scores of new designs obtained via gradient ascent can be erroneously high [8; 9].

To mitigate the out-of-distribution issue, recent studies have suggested applying regularization techniques to either the proxy itself [8; 9; 10] or the design under consideration [11; 12]. These methods improve the proxy's robustness and generalization ability. However, a yet unexplored approach in this domain is using a pseudo-labeler to assign pseudo-labels to designs near the current point. Fine-tuning the proxy on this pseudo-labeled dataset can lead to improvement, provided that we can identify the valuable portion of the pseudo-labeled dataset.

Inspired by this, we propose _Importance-aware Co-Teaching for Offline Model-based Optimization_ (**ICT**). This approach maintains three symmetric proxies, and their mean ensemble acts as the final proxy. ICT consists of two main steps with the **first step** being _pseudo-label-driven co-teaching_ as illustrated in Figure 1. During this step, one proxy is iteratively selected as the pseudo-labeler, followed by a co-teaching process  that facilitates the exchange of valuable data between the other two proxies for fine-tuning. As depicted in Figure 1, there are three symmetric proxies, \(f_{_{1}}()\), \(f_{_{2}}()\), and \(f_{_{3}}()\). The entire learning cycle (the larger triangle) can be divided into three symmetric parts (sub-triangles), with one proxy chosen to be the pseudo-labeler in turn. Taking the top triangle as an example, we select \(f_{_{1}}()\) as the pseudo-labeler to generate pseudo labels for a set of points in the neighborhood of the current optimization point \(_{t}\). The other two proxies, \(f_{_{2}}()\) and \(f_{_{3}}()\), then receive the pseudo-labeled dataset. They compute the sample loss for each entry in the dataset and exchange small-loss samples between them for fine-tuning. This co-teaching process encourages knowledge transfer between the two proxies, as small losses are typically indicative of valuable knowledge. The symmetric nature of the three proxies allows the above process to repeat three times, with each proxy--\(f_{_{1}}()\), \(f_{_{2}}()\), and \(f_{_{3}}()\)--taking turns as the pseudo-label generator. This learning cycle promotes the sharing of valuable knowledge among the three symmetric proxies, allowing them to collaboratively improve the ensemble performance in handling out-of-distribution designs.

Figure 1: Pseudo-label-driven co-teaching.

Figure 2: Meta-learning-based sample reweighting.

Despite the efforts made in the first step, small-loss data may still contain inaccurate labels. During the first step, small-loss data (\(p_{a}\) and \(p_{b}\)) from the pseudo-labeled dataset produced by \(f_{_{1}}()\) are identified based on the predictions of proxy \(f_{_{3}}()\) and fed to proxy \(f_{_{2}}()\). However, as shown in Figure 2 (a), the less accurate point \(p_{b}\) deviates noticeably from the ground-truth, causing the fine-tuned proxy \(f_{_{2}}()\) to diverge from the ground-truth function. To address this, we introduce the **second step** of ICT, _meta-learning-based sample reweighting_, which aims to assign higher weights to more accurate points like \(p_{a}\) and lower weights to less accurate ones like \(p_{b}\). To accomplish this, we assign an importance weight for every sample yielded by the first step (\(_{a}\) for \(p_{a}\) and \(_{b}\) for \(p_{b}\)) and propose a meta-learning framework to update these sample weights (\(_{a}\) and \(_{b}\)) automatically by leveraging the supervision signals from the offline dataset \(p_{1,2,3}\). Specifically, the proxy fine-tuned on the weighted small-loss data (\(p_{a}\) and \(p_{b}\)) is expected to perform well on the offline dataset, provided the weights are accurate, i.e., large \(_{a}\) and small \(_{b}\). We can optimize the sample weights by minimizing the loss on the offline dataset as a function of the sample weights. As illustrated in Figure 2 (b), the weight of \(p_{a}\) is optimized to be high, while the weight of \(p_{b}\) is optimized to be low. Consequently, the proxy \(f^{(b)}_{_{2}}()\) fine-tuned on the weighted samples in Figure 2 (b) is brought closer to the ground-truth objective function \(f()\), compared to the case where the fine-tuned proxy \(f^{(a)}_{_{2}}()\) is far from \(f()\) in Figure 2 (a). Through extensive experiments across various tasks , ICT proves effective at mitigating out-of-distribution issues, delivering state-of-the-art results.

In summary, our paper presents three main contributions:

* We introduce _Importance-aware **Co-Teaching**_ (**ICT**) for offline MBO. ICT consists of two steps. In the _pseudo-label-driven co-teaching_ step, a proxy is iteratively chosen as the pseudo-labeler, initiating a co-teaching process that facilitates knowledge exchange between the other two proxies.
* The second step, _meta-learning-based sample reweighting_, is introduced to alleviate potential inaccuracies in pseudo-labels. In this step, pseudo-labeled samples are assigned importance weights, which are then optimized through meta-learning.
* Extensive experiments demonstrate ICT's effectiveness in addressing out-of-distribution issues, yielding state-of-the-art results in multiple MBO tasks. Specifically, ICT secures the best mean rank of \(3.1\) and median rank of \(2\), among \(15\) methods.

## 2 Preliminaries

Offline model-based optimization (MBO) targets a variety of optimization problems with the goal of maximizing an unknown objective function using an offline dataset. Consider the design space \(=^{d}\), where \(d\) represents the design dimension. Formally, the offline MBO can be expressed as:

\[^{*}=_{}f(),\] (1)

where \(f()\) denotes the unknown objective function, and \(\) denotes a candidate design. In this scenario, an offline dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\) is available, where \(_{i}\) represents a specific design, such as robot size, and \(y_{i}\) represents the corresponding score, like robot speed. In addition to robot design, similar problems also include protein and molecule design.

A common strategy for tackling offline MBO involves approximating the unknown objective function \(f()\) using a proxy function, typically represented by a deep neural network (DNN) \(f_{}()\), which is trained on the offline dataset:

\[^{*}=_{}_{i=1}^{N}(f_{}(_{i})-y_{i})^{2}.\] (2)

With the trained proxy, design optimization is performed using gradient ascent steps:

\[_{t}=_{t-1}+_{}f_{}()_{ =_{i}},t[1,T].\] (3)

Here, \(T\) denotes the number of steps, and \(\) signifies the learning rate. The optimal design \(^{*}\) is acquired as \(_{T}\). This gradient ascent approach is limited by an _out-of-distribution issue_, as the proxy \(f_{}()\) may not accurately predict scores for unseen designs, leading to suboptimal solutions.

Method

In this section, we introduce _Importance-aware **Co-Teaching**_ (**ICT**), which consists of two steps. We maintain three symmetric proxies and compute the mean ensemble as the final proxy. In Sec 3.1, we describe the first step, _pseudo-label-driven co-teaching_. This step involves iteratively selecting one proxy as the pseudo-label generator and implementing a co-teaching process to facilitate the exchange of valuable data between the remaining two proxies. Nevertheless, the samples exchanged during co-teaching might still contain inaccurate labels, which necessitates the second step _meta-learning-based sample reweighting_ in Sec 3.2. During this step, each sample from the previous step is assigned an importance weight and updated via meta-learning. Intuitively, the ICT process can be likened to an enhanced paper peer review procedure between three researchers preparing for submission. Each researcher, acting as an author, presents his/her paper to the other two. These two serve as reviewers and co-teach each other important points to better comprehend the paper, ultimately providing their feedback to the author. A detailed depiction of the entire algorithm can be found in Algorithm 1.

### Pseudo-label-driven Co-teaching

Vanilla gradient ascent, as expressed in Eq. (3), is prone to out-of-distribution issues in offline model-based optimization. One potential yet unexplored solution is using a pseudo-labeler to provide pseudo-labels to designs around the optimization point. By fine-tuning the proxy using the valuable portion of the pseudo-labeled dataset, we can enhance the proxy's performance. To achieve this, we maintain three proxies simultaneously, computing their mean ensemble as the final proxy, and iteratively select one proxy to generate pseudo-labeled data. The other two proxies exchange knowledge estimated to have high value, by sharing small-loss data. Due to the symmetric nature of the three proxies, this process can be repeated three times for sharing valuable knowledge further.

**Pseudo-label.** We initially train three proxies \(f_{_{1}}()\), \(f_{_{2}}()\), and \(f_{_{3}}()\) on the whole offline dataset using Eq. (2) with different initializations, and conduct gradient ascent with their mean ensemble,

\[_{t}=_{t-1}+_{}(f_{1}(_{t-1})+f_ {2}(_{t-1})+f_{3}(_{t-1})),\] (4)

where \(\) is the gradient ascent learning rate. Given the current optimization point \(_{t}\), we sample \(M\) points \(_{t,1},_{t,2},,_{t,M}\) around \(_{t}\) as \(_{t,m}=_{t}+\), where \(\) is the noise coefficient and \(\) is drawn from the standard Gaussian distribution. An alternative way is to directly sample the \(M\) points around the offline dataset, rather than the current optimization point. We detail this option in Appendix A.1. We iteratively choose one proxy, for example \(f_{_{1}}()\), to label these points, creating a pseudo-labeled dataset \(_{1}=\{(_{t,j},f_{_{1}}(_{t,j}))\}_{j=1}^{M}\). Lines \(5\) to \(6\) of Algorithm 1 detail the implementation of this segment.

**Co-teaching.** For each sample in the pseudo-labeled dataset \(_{1}\), we compute the sample loss for \(f_{_{2}}()\) and \(f_{_{3}}()\). Specifically, the losses are calculated as \(_{2,i}=(f_{_{2}}(_{t,i})-f_{_{1}}(_{t,i}))^{2}\) and \(_{3,i}=(f_{_{3}}(_{t,i})-f_{_{1}}(_{t,i}))^{2}\), respectively. Small-loss samples typically contain valuable knowledge, making them ideal for enhancing proxy robustness . Proxies \(f_{_{2}}()\) and \(f_{_{3}}()\) then exchange the top \(K\) small-loss samples as valuable data to teach each other where \(K\) is a hyperparameter. The co-teaching process enables the exchange of valuable knowledge between proxies \(f_{_{2}}()\) and \(f_{_{3}}()\). This part is implemented as described in Lines \(7\) to \(8\) of Algorithm 1. The symmetric design of the three proxies, \(f_{_{1}}()\), \(f_{_{2}}()\), and \(f_{_{3}}()\), enables the entire process to be iterated three times with one proxy chosen as the pseudo-labeler every time.

### Meta-learning-based Sample Reweighting

While the previous step effectively selects samples for fine-tuning, these samples may still contain inaccuracies. To mitigate this, we introduce a _meta-learning-based sample reweighting_ step. In this step, each sample obtained from the prior step is assigned an importance weight, which is then updated using a meta-learning framework. Without loss of generality, we use \(f_{}()\) to represent any of \(f_{_{1}}()\), \(f_{_{2}}()\) and \(f_{_{3}}()\) as this step applies identically to all three proxies. The top \(K\) small-loss samples selected from the previous step for fine-tuning \(f_{}()\) are denoted as \(_{s}=\{(_{i}^{s},_{i}^{s})\}_{i=1}^{K}\).

**Sample Reweighting.** We assign an importance weight \(_{i}\) to the \(i^{th}\) selected sample and initialize these importance weights to ones. We expect smaller importance weights for less accurate samplesand larger importance weights for more accurate samples to improve proxy fine-tuning. With these weights, we can optimize the proxy parameters as follows:

\[^{*}()=_{}_{i=1}^{K}}(f_{}(_{i}^{s})-_{i}^{s})^{2}.\] (5)

Since we only want to perform fine-tuning based on \(_{s}\), we can adopt one step of gradient descent:

\[^{*}()=-_{i=1}^{K}}}(_{i}^{s})-_{i}^{s})^{2 }}{^{}},\] (6)

where \(\) is the learning rate for fine-tuning. This part is presented in Line \(10\) in Algorithm 1.

**Meta-learning.** The challenge now is finding a group of proper weights \(\). We achieve this by leveraging the supervision signals from the offline dataset, which are generally accurate. If the sample weights are accurate, the proxy fine-tuned on the weighted samples is expected to perform well on the offline dataset. This is because the weighted samples aim to reflect the underlying ground-truth function that the offline dataset already captures, and both sets of data share common patterns. We can optimize the sample weights by minimizing the loss of the offline dataset in a meta-learning framework. The loss on the offline dataset can be written as:

\[(^{*}())=_{} _{i=1}^{N}(f_{^{*}()}(_{i})-y_{i})^{2}.\] (7)

The sample weight \(_{i}\) for the \(i^{th}\) sample can be updated by gradient descent:

\[_{i}^{{}^{}}&=_{i}-(^{*}())}{ }^{*}()}{_{i}}\\ &=_{i}+ (^{*}())}{}}(_{i}^{s})-_{i}^{s})^{2}}{^{}}, \] (8)

where \(\) is the learning rate for the meta-learning framework. From Eq. (8), it is worth mentioning that \((^{*}())}{} }(_{i}^{s})-_{i}^{s})^{2}}{ ^{}}\) represents the similarity between the gradient of the offline dataset and the gradient of the \(i^{th}\) sample. This implies that a sample with a gradient similar to the offline dataset will receive a higher weight and vice versa, revealing the inner mechanism of this framework. By applying the updated sample weights to Eq. (6) for fine-tuning, we improve the proxy's performance. This process is iteratively applied to each proxy, yielding a stronger ensemble. Lines \(11\) to \(13\) of Algorithm 1 showcase the execution of this part.

## 4 Experimental Results

### Dataset and Evaluation

**Dataset and Tasks.** In this study, we conduct experiments on four continuous tasks and three discrete tasks. The continuous tasks include: (a) Superconductor (SuperC), where the objective is to develop a superconductor with \(86\) continuous components to maximize critical temperature, using \(17,010\) designs; (b) Ant Morphology (Ant)[1; 14], where the aim is to design a quadrupedal ant with \(60\) continuous components to improve crawling speed, based on \(10,004\) designs; (c) D'Kitty Morphology (D'Kitty)[1; 15], where the focus is on shaping a quadrupedal D'Kitty with \(56\) continuous components to enhance crawling speed, using \(10,004\) designs; (d) Hopper Controller (Hopper), where the aim is to identify a neural network policy with \(5,126\) weights to optimize return, using \(3,200\) designs. Additionally, our discrete tasks include: (e) TF Bind \(8\) (TF8), where the goal is to discover an \(8\)-unit DNA sequence that maximizes binding activity score, utilizing \(32,898\) designs; (f) TF Bind \(10\) (TF10), where the aim is to find a \(10\)-unit DNA sequence that optimizes binding activity score, using \(50,000\) designs; (g) NAS , where the objective is to find the optimal neural network architecture to enhance test accuracy on the CIFAR-\(10\) dataset, using \(1,771\) designs.

**Evaluation and Metrics.** In accordance with the evaluation protocol used in [1; 11], we identify the top \(128\) designs from the offline dataset for each approach and report the \(100^{th}\) percentile normalizedground-truth score. This score is computed as \(y_{n}=}{y_{max}-y_{min}}\), where \(y_{min}\) and \(y_{max}\) represent the minimum and maximum scores within the entire unobserved dataset, respectively. The \(50^{th}\) percentile (median) normalized ground-truth scores are included in Appendix A.2. For a better comparison, we report the best design in the offline dataset, denoted as \(()\). We also provide mean and median rankings across all seven tasks for a broad performance assessment.

### Comparison Methods

We compare our approach with two categories of baselines: (1) those that use generative models for sampling purposes, and (2) those that apply gradient updates derived from existing designs. The generative model-based methods learn and sample from the distribution of high-scoring designs, including: **(i)** MIN , which maps scores to designs and searches this map for optimal designs; **(ii)** CbAS , which uses a VAE model to adapt the design distribution towards high-scoring areas; **(iii)** Auto.CbAS , which employs importance sampling to retrain a regression model based on CbAS.

The second category encompasses: **(i)** Grad: carries out a basic gradient ascent on existing designs to generate new ones; **(ii)** Grad. Min: optimizes the lowest prediction from an ensemble of learned objective functions; **(iii)** Grad. Mean: optimizes the ensemble's mean prediction; **(iv)** ROMA : applies smoothness regularization on the DNN; **(v)** COMs : uses regularization to assign lower scores to designs obtained through gradient ascent; **(vi)** NEMO : constrains the gap between the proxy and the ground-truth function via normalized maximum likelihood before performing gradient ascent; **(vii)** BDI  uses forward and backward mappings to distill knowledge from the offline dataset to the design; **(viii)** IOM : enforces representation invariance between the training dataset and the optimized designs.

We also compare with traditional methods in : **(i)** CMA-ES : gradually adjusts the distribution towards the optimal design by modifying the covariance matrix. **(ii)** BO-qEI : executes Bayesian Optimization to maximize the proxy, suggests designs through the quasi-Expected-Improvement acquisition function, and labels the designs using the proxy function. **(iii)** REINFORCE : optimizes the distribution over the input space using the learned proxy.

### Training Details

We adopt the training settings from  for all comparison methods unless otherwise specified. We use a \(3\)-layer MLP (MultiLayer Perceptron) with ReLU activation for all gradient updating methods, and set the hidden size to \(2048\). Additional hyperparameter details are elaborated in Appendix A.3.

One of the top 128 designs from the offline dataset is iteratively selected as the starting point, as outlined in Line 2 of Algorithm 1. We reference results from  for non-gradient-ascent methods such as BO-qEI, CMA-ES, REINFORCE, CbAS, and Auto.CbAS. For gradient-based methods, we run each setting over \(8\) trials and report the mean and standard error. All experiments are run on a single NVIDIA GeForce RTX \(3090\) GPU.

### Results and Analysis

**Performance in Continuous Tasks.** Table 1 presents the results across different continuous domains. In all four continuous tasks, our ICT method achieves the top performance. Notably, it surpasses the basic gradient ascent, Grad, demonstrating its ability to mitigate the out-of-distribution issue. The superior performance of Grad.mean over Grad can be attributed to the ensemble model's robustness in making predictions . Furthermore, ICT generally outperforms ensemble methods and other gradient-based techniques such as COMs and ROMA, demonstrating the effectiveness of our strategy. Generative model-based methods, such as CbAS and MINs, however, struggle with the high-dimensional task Hopper Controller. Interestingly, ICT necessitates only three standard proxies and avoids the need for training a generative model, which can often be a challenging task. These results indicate that ICT is a simple yet potent baseline for offline MBO.

**Performance in Discrete Tasks.** Table 2 showcases the outcomes across various discrete domains. ICT attains top performances in two out of the three tasks, TF Bind \(8\) and TF Bind \(10\). These results suggest that ICT is a powerful method in the discrete domain. However, in NAS, the performance of ICT is not as strong, which can be attributed to two factors. Firstly, the neural network design in NAS,

  Method & \(8\) & TF Bind \(10\) & NAS & Rank Mean & Rank Median \\  \(()\) & \(0.439\) & \(0.467\) & \(0.436\) & \\ BO-qEI & \(0.798 0.083\) & \(0.652 0.038\) & \(\) & \(9.9/15\) & \(11/15\) \\ CMA-ES & \(\) & \(0.670 0.023\) & \(0.985 0.079\) & \(6.1/15\) & \(3/15\) \\ REINFORCE & \(\) & \(0.663 0.034\) & \(-1.895 0.000\) & \(11.3/15\) & \(15/15\) \\ CbAS & \(0.927 0.051\) & \(0.651 0.060\) & \(0.683 0.079\) & \(9.1/15\) & \(9/15\) \\ Auto.CbAS & \(0.910 0.044\) & \(0.630 0.045\) & \(0.506 0.074\) & \(11.6/15\) & \(12/15\) \\ MIN & \(0.905 0.052\) & \(0.616 0.021\) & \(0.717 0.046\) & \(11.0/15\) & \(12/15\) \\  Grad & \(0.906 0.024\) & \(0.635 0.022\) & \(0.598 0.034\) & \(7.7/15\) & \(9/15\) \\ Mean & \(0.899 0.025\) & \(0.652 0.020\) & \(0.666 0.062\) & \(6.6/15\) & \(6/15\) \\ Min & \(0.939 0.013\) & \(0.638 0.029\) & \(0.705 0.011\) & \(7.3/15\) & \(8/15\) \\ COMs & \(0.452 0.040\) & \(0.624 0.008\) & \(0.810 0.029\) & \(10.3/15\) & \(12/15\) \\ ROMA & \(0.924 0.040\) & \(0.666 0.035\) & \(0.941 0.020\) & \(5.1/15\) & \(5/15\) \\ NEMO & \(0.941 0.000\) & \(\) & \(0.734 0.015\) & \(5.0/15\) & \(4/15\) \\ BDI & \(0.870 0.000\) & \(0.605 0.000\) & \(0.722 0.000\) & \(7.9/15\) & \(8/15\) \\ IOM & \(0.878 0.069\) & \(0.648 0.023\) & \(0.274 0.021\) & \(7.6/15\) & \(6/15\) \\ 
**ICT\({}_{(ours)}\)** & \(\) & \(\) & \(0.667 0.091\) & **3.1/15** & **2/15** \\  

Table 2: Experimental results on discrete tasks, and ranking on all tasks for comparison.

  Method & \(\) & \(\) & \(\) & \(\) \\  \(()\) & \(0.399\) & \(0.565\) & \(0.884\) & \(1.0\) \\ BO-qEI & \(0.402 0.034\) & \(0.819 0.000\) & \(0.896 0.000\) & \(0.550 0.018\) \\ CMA-ES & \(0.465 0.024\) & \(\) & \(0.724 0.001\) & \(0.604 0.215\) \\ REINFORCE & \(0.481 0.013\) & \(0.266 0.032\) & \(0.562 0.196\) & \(-0.020 0.067\) \\ CbAS & \(\) & \(0.876 0.031\) & \(0.892 0.008\) & \(0.141 0.012\) \\ Auto.CbAS & \(0.421 0.045\) & \(0.882 0.045\) & \(0.906 0.006\) & \(0.137 0.005\) \\ MIN & \(0.499 0.017\) & \(0.445 0.080\) & \(0.892 0.011\) & \(0.424 0.166\) \\  Grad & \(0.483 0.025\) & \(0.920 0.044\) & \(\) & **1.791 \(\) 0.182** \\ Mean & \(0.497 0.011\) & \(0.943 0.012\) & \(\) & **1.815 \(\) 0.111** \\ Min & \(\) & \(0.910 0.038\) & \(0.936 0.006\) & \(0.543 0.010\) \\ COMs & \(0.472 0.024\) & \(0.828 0.034\) & \(0.913 0.023\) & \(0.658 0.217\) \\ ROMA & \(\) & \(0.917 0.030\) & \(0.927 0.013\) & \(1.740 0.188\) \\ NEMO & \(0.502 0.002\) & \(0.952 0.002\) & \(\) & \(0.483 0.005\) \\ BDI & \(\) & \(0.906 0.000\) & \(0.919 0.000\) & **1.993 \(\) 0.000** \\ IOM & \(\) & \(0.918 0.031\) & \(0.945 0.012\) & \(1.176 0.452\) \\ 
**ICT\({}_{(ours)}\)** & \(\) & \(\) & **0.968 \(\) 0.020** & **2.104 \(\) 0.357** \\  

Table 1: Experimental results on continuous tasks for comparison.

represented by a \(64\)-length sequence of \(5\)-categorical one-hot vectors, has a higher dimensionality than TF Bind \(8\) and TF Bind \(10\), making the optimization process more complex. Furthermore, the simplistic encoding-decoding strategy in design-bench may not accurately capture the intricacies of the neural network's accuracy, which can only be determined after training on CIFAR10.

**Summary.** ICT attains the highest rankings with a mean of \(3.1/15\) and median of \(2/15\) as shown in Table 2 and Figure 3, and also secures top performances in \(6\) out of the \(7\) tasks. We have further run a Welch's t-test between our method and the second-best method, obtaining p-values of \(0.437\) on SuperC, \(0.004\) on Ant, \(0.009\) on D'Kitty, \(0.014\) on Hopper, \(0.000\) on TF8, \(0.045\) on TF10, \(0.490\) on NAS. This demonstrates statistically significant improvement in \(5\) out of \(7\) tasks, reaffirming the effectiveness of our method.

### Ablation Studies

To better understand the impact of pseudo-label-driven co-teaching (co-teaching) and meta-learning-based sample reweighting (reweighting) on the performance of our proposed ICT method, we conduct ablation studies by removing either co-teaching or reweighting from the full ICT approach. Table 3 presents the results. Beyond just assessing these performance indicators, we also verify the accuracy of the samples chosen by co-teaching, as well as the efficacy of the sample weights we have calculated. We do this by referring to the ground truth, with further details provided in Appendix A.4. Our reweighting module is also compared with the recently proposed RGD method  as detailed in the Appendix A.5.

For two of the discrete tasks (TF\(8\) and TF\(10\)), the ICT method consistently exceeds the performance of both its ablated versions. This highlights the efficacy of the two steps when handling discrete tasks. Conversely, the exclusion of the co-teaching in NAS leads to an increase in performance. This could be attributed to the fact that the encoding-decoding strategy of NAS in design-bench is unable to accurately capture the inherent complexity of neural networks. As such, the co-teaching step, reliant on this strategy, might not be as effective. For the continuous tasks (SuperC, Ant, D'Kitty, and Hopper), we observe that the complete ICT method consistently achieves superior performance. This underlines the effectiveness of the two steps when dealing with continuous tasks. The performance gains are particularly substantial in the Hopper task when the complete ICT method is compared with the ablated versions, illustrating the power of the two steps in managing high-dimensional continuous tasks. Overall, our ablation studies demonstrate that the inclusion of both co-teaching and reweighting in the ICT method generally enhances performance across diverse tasks and input dimensions, underscoring their integral role in our approach.

### Hyperparameter Sensitivity

We first assess the robustness of our ICT method by varying the number of samples (\(K\)) selected during the co-teaching process on the continuous D'Kitty Morphology task. For this analysis, \(K\) is varied among \(K=8,16,32,64\). In Figure 4 (a), we illustrate the \(100^{th}\) percentile normalized ground-truth score as a function of time step \(T\), for each of these \(K\) values. The results demonstrate that the performance of ICT is resilient to variations in \(K\), maintaining performances within a certain range. Additionally, ICT is capable of generating high-scoring designs early on in the process, specifically achieving such designs around the time step \(t=50\), and sustains this performance thereafter, demonstrating its robustness against the number of optimization steps \(T\).

We further evaluate the robustness of our ICT method against the learning rate (\(\)) for the meta-learning framework. As depicted in Figure 4 (b), ICT's performance remains relatively consistent across a variety of \(\) values, further demonstrating ICT's robustness with respect to the hyperparameter \(\). We explore the fine-tuning learning rate \(\) and conduct further experiments and analysis on TF Bind 8. Details can be found in Appendix A.6.

  Task & D & ICT & w/o co-teaching & w/o reweighting \\  TF8 & 8 & **0.958 \(\) 0.008** & \(0.905 0.042\) & \(0.910 0.024\) \\ TF10 & 10 & **0.691 \(\) 0.023** & \(0.653 0.018\) & \(0.654 0.023\) \\ NAS & 64 & \(0.667 0.091\) & **0.779 \(\) 0.071** & \(0.666 0.090\) \\  SuperC & 86 & **0.503 \(\) 0.017** & \(0.500 0.017\) & \(0.501 0.017\) \\ Ant & 60 & **0.961 \(\) 0.007** & \(0.927 0.033\) & \(0.914 0.015\) \\ D’Kitty & 56 & **0.968 \(\) 0.020** & \(0.962 0.021\) & \(0.959 0.013\) \\ Hopper & 5126 & **2.104 \(\) 0.357** & \(1.453 0.734\) & \(1.509 0.166\) \\  

Table 3: Ablation studies on two core steps of ICT.

## 5 Related Works

**Offline Model-based Optimization.** Contemporary offline model-based optimization methods can be generally classified into two primary groups: (i) generating novel designs through generative models, and (ii) conducting gradient ascent on existing designs. The former methods learn and sample from the distribution of high-scoring designs including MIN , CbAS , Auto.CbAS  and BootGen . Recently, gradient-based methods have gained popularity due to their ability to leverage deep neural networks (DNNs) for improved design generation. These methods apply regularization techniques to either the proxy itself [8; 9; 10] or the design under consideration [11; 12], enhancing the proxy's robustness and generalization capabilities. An interesting subfield of offline MBO includes biological sequence design, which has potential applications such as designing drugs for treating diseases [27; 28]. In particular, the work  also adopts a proxy as a pseudo-labeler and aligns the generator with the proxy, a technique that resonates with our method. ICT falls under this category, but adopts a unique approach to improve proxy performance: it incorporates valuable knowledge from a pseudo-labeled dataset into other proxies for fine-tuning, thereby enhancing the ensemble performance. Notably, while the concurrent work of parallel mentoring  also employs pseudo-labeling, it focuses on pairwise comparison labels, potentially sacrificing some information due to its discrete nature.

**Sample Reweighting.** Sample reweighting is commonly utilized to address the issue of label noise [30; 31], where each sample is assigned a larger weight if it is more likely to be accurate, using a carefully designed function. Recent studies [32; 33; 34] suggest using a meta-set to guide the learning of sample weights, which can enhance model training. Such an approach is grounded in a meta-learning framework which can be used to learn hyperparameters [35; 36; 37; 38; 39; 40; 41; 42; 43]. Inspired by distributionally robust optimization, recent work  proposes a re-weighted gradient descent algorithm that provides an efficient and effective means of reweighting. In this paper, the pseudo-labeled dataset generated by co-teaching may still contain some inaccuracies, while the offline dataset is generally accurate. We propose a sample reweighting framework to reduce the inaccuracies in the pseudo-labeled dataset by leveraging the supervision signals from the offline dataset.

**Co-teaching.** Co-teaching  is an effective technique for mitigating label noise by leveraging insights from peer networks. It involves the concurrent training of two proxies where one proxy identifies small-loss samples within a noisy mini-batch for fine-tuning the other. Co-teaching bears similarities to decoupling  and co-training , as they all involve the interaction between two models to enhance the training process. In this study, we adapt co-teaching to work with a pseudo-labeled dataset generated by a trained proxy, instead of relying on a noisy original dataset. Specifically, we employ one proxy to select accurate samples from this pseudo-labeled dataset for fine-tuning the other, and vice versa.

## 6 Conclusion and Discussion

In this study, we introduce the ICT (Importance-aware Co-Teaching) method for mitigating the out-of-distribution issue prevalent in offline model-based optimization. ICT is a two-step approach. The first step is pseudo-label-driven co-teaching, which iteratively selects a proxy to generate pseudo-labeled data. Valuable data are identified by co-teaching to fine-tune other proxies. This process,repeated three times with different pseudo-labelers, facilitates knowledge transfer. In the second step, meta-learning-based sample reweighting assigns and updates importance weights to samples selected by the co-teaching process, further improving the proxy fine-tuning. Our experimental findings demonstrate the success of ICT. We discuss its limitations in Appendix A.7

**Future Work.** Though we initially design ICT with three proxies, the method's inherent scalability and flexibility make it applicable to scenarios involving \(N\) proxies. In such a scenario, we can iteratively select one proxy out of \(N\) as the pseudo-labeler to generate data. Then, each of the remaining \(N-1\) proxies could select small-loss samples from its perspective and provide these samples to the other \(N-2\) proxies for fine-tuning. This process enhances knowledge transfer and facilitates cooperative learning among the proxies. Looking to the future, we plan to conduct further research into the dynamics of such an expanded ensemble of proxies.

**Negative Impact.** It is crucial to recognize that ICT's potential benefits come with possible negative consequences. Advanced optimization techniques can be applied for both constructive and destructive purposes, depending on their use. For example, while drug development and material design can have a positive impact on society, these techniques could also be misused to create harmful substances or products. As researchers, we must remain attentive and strive to ensure that our work is employed for the betterment of society while addressing any potential risks and ethical concerns.

## 7 Acknowledgement

This research was empowered in part by the computational support provided by Compute Canada (www.computecanada.ca).