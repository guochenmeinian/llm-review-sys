# Memory-Efficient Gradient Unrolling for

Large-Scale Bi-level Optimization

 Qianli Shen\({}^{1}\)1  Yezhen Wang\({}^{1}\)  Zhouhao Yang\({}^{1}\)  Xiang Li\({}^{1}\)  Haonan Wang\({}^{1}\)

**Yang Zhang\({}^{1}\)  Jonathan Scarlett\({}^{1}\)  Zhanxing Zhu\({}^{2}\)  Kenji Kawaguchi\({}^{1}\)**

\({}^{1}\)National University of Singapore \({}^{2}\)University of Southampton, UK

###### Abstract

Bi-level optimization (BO) has become a fundamental mathematical framework for addressing hierarchical machine learning problems. As deep learning models continue to grow in size, the demand for scalable bi-level optimization has become increasingly critical. Traditional gradient-based bi-level optimization algorithms, due to their inherent characteristics, are ill-suited to meet the demands of large-scale applications. In this paper, we introduce **F**orward **G**radient **U**nrolling with **F**orward **G**radient, abbreviated as **(FG)\({}^{2}\)U**, which achieves an unbiased stochastic approximation of the meta gradient for bi-level optimization. (FG)\({}^{2}\)U circumvents the memory and approximation issues associated with classical bi-level optimization approaches, and delivers significantly more accurate gradient estimates than existing large-scale bi-level optimization approaches. Additionally, (FG)\({}^{2}\)U is inherently designed to support parallel computing, enabling it to effectively leverage large-scale distributed computing systems to achieve significant computational efficiency. In practice, (FG)\({}^{2}\)U and other methods can be strategically placed at different stages of the training process to achieve a more cost-effective two-phase paradigm. Further, (FG)\({}^{2}\)U is easy to implement within popular deep learning frameworks, and can be conveniently adapted to address more challenging black-box bi-level optimization scenarios. We provide a thorough convergence analysis and a comprehensive practical discussion for (FG)\({}^{2}\)U, complemented by extensive empirical evaluations, showcasing its superior performance in diverse large-scale bi-level optimization tasks. Code is available at https://github.com/ShenQianli/FG2U.

## 1 Introduction

Bi-level optimization is a mathematical framework with a long history of research , dealing with hierarchical optimization problems where one problem is nested within the other. A bi-level optimization problem can be formulated as:

\[_{}\ f(^{*}(),) s.t.\ ^{*}()_{}g(,),\] (1)

where \(^{M}\) denotes the inner parameter, \(^{N}\) denotes the meta parameter, and \(f\), \(g\) are called the meta objective function and inner objective function, respectively.

Recently, with the rise of deep learning, bi-level optimization has regained attention as a theoretical framework covering a wide range of machine learning problems, including hyperparameter optimization , neural architecture search , robust machine learning , meta learning , and physics-informed machine learning . In these scenarios, the inner problem often pertains to the optimization of neural networks, therebyprecipitating challenges associated with gradient-based bi-level optimization. Consequently, various gradient-based bi-level optimization algorithms have been developed . These algorithms typically employ an iterative solution \(_{T}\) obtained by executing multiple inner optimization steps to approximate the meta gradient, and provide different tradeoffs between computational costs and performance for meta gradient approximation.

However, as the scale of deep learning models continues to expand, the requirements for scalability in bi-level optimization correspondingly increase. Existing gradient-based bi-level optimization algorithms, due to their inherent characteristics, are ill-suited to meet the demands of large-scale applications. Concretely, _gradient unrolling_ (GU) methods [17; 16; 40; 60] are bottlenecked by the memory overhead associated with either the dimension of the inner parameter or the number of iterative steps for the inner problem. Implicit Function (IF) approaches [48; 19; 64; 76] are compromised by approximation errors, which stem from the iterative estimation of inner solutions and computations that involve the Hessian matrix. _Value Function_ (VF) based strategies [39; 37; 61; 33], although exhibit commendable theoretical properties  for deterministic bi-level optimization, have yet to gain traction in practical applications, predominantly due to their limitations in addressing large-scale stochastic challenges . Recent advancements in algorithms [60; 9] have been specifically tailored for large-scale bi-level optimization. Although these methodologies facilitate efficient gradient approximation by compromising accuracy, they may result in significantly suboptimal performance due to biased gradient approximations. Additionally, these methods struggle in more complex scenarios, such as when inner problems are addressed through black-box optimization.

In this paper, we propose a novel method called **F**orward **G**radient **U**nrolling with **F**orward **G**radient, abbreviated as **(FG)\({}^{2}\)U**, which achieves an unbiased stochastic approximation of the meta gradient for bi-level optimization. (FG)\({}^{2}\)U circumvents the memory issues associated with GU-based approaches and approximation issues associated with IF-based approaches. Compared to recently developed large-scale bi-level optimization approaches, (FG)\({}^{2}\)U delivers significantly more accurate gradient estimates. Additionally, (FG)\({}^{2}\)U is inherently designed to support parallel computing, enabling it to effectively leverage large-scale distributed computing systems to achieve significant computational efficiency. In practice, a cost-effective two-phase paradigm can be achieved by strategically placing

Figure 1: **Top Left**: A comparison of bi-level optimization methods. (FG)\({}^{2}\)U circumvents the large-scale challenges inherent in classical bi-level optimization techniques. Within large-scale bi-level optimization, (FG)\({}^{2}\)U prioritizes the accuracy of gradient approximation over efficiency. **Top Right**: An overview of the cost-effective two-phase paradigm. (FG)\({}^{2}\)U is ideally positioned in Phase II to enhance performance after an approximate solution has been obtained using other efficient methods. **Bottom Left**: GPU Memory Usage and Performance on _Meta Learning Online Adaptation_ experiment. (FG)\({}^{2}\)U can effectively address the memory issue of RGU when both the inner model and the unrolled depth are large. **Bottom Center**: GPU Memory Usage and Performance on _Data Condensation_ experiments. The performance of (FG)\({}^{2}\)U surpasses that of other large-scale bi-level optimization methods, owing to its accurate gradient approximation, while demonstrating better memory efficiency. **Bottom Right**: Efficiency tradeoff of (FG)\({}^{2}\)U on _Data Condensation_ experiments. The efficiency of (FG)\({}^{2}\)U can be well enhanced via intra/inter-GPU parallelism.

(FG)\({}^{2}\)U and other methods at different stages of the training process to balance efficiency and performance. Further, (FG)\({}^{2}\)U is easy to implement within popular deep learning frameworks, and can be conveniently adapted to address more challenging zeroth-order bi-level optimization scenarios.

We provide an overview of (FG)\({}^{2}\)U in Figure 1 to illustrate its strengths and role in large-scale bi-level optimization. The rest of the paper is organized as follows. Firstly, in Section 2, we provide summaries of existing bi-level optimization algorithms and discuss their limitations in large-scale contexts. Next, in Section 3, we introduce the proposed method, (FG)\({}^{2}\)U, followed by a convergence analysis in Section 3.1 and a detailed discussion of the practical considerations in Section 3.2. Further, in Section 4, we conduct extensive empirical studies covering large-scale bi-level optimization in computer vision, natural language processing, and physics-informed machine learning to demonstrate the efficacy of (FG)\({}^{2}\)U in large-scale bi-level optimization scenarios.

## 2 Background

Gradient-based Bi-level Optimization.Within deep learning applications, the model concerned with optimizing over \(\) as presented in (1) typically constitutes deep neural networks. The optimal parameters of such networks are not explicitly accessible and are estimated through iterative procedures. Consequently, the primal problem of bi-level optimization in (1) is approximately reformulated as follows:

\[_{}\;h():=f(_{T}(),),\] (2) \[\;\;_{0}()=_{0}( ),\;\;_{t}()=_{t}(_{t-1}( ),),\;\;t=1,,T,\]

where \(^{N}\), \(^{M}\) are the parameter spaces; \(T\), commonly called the unrolled depth, denotes the number of inner optimization steps for approximating \(^{*}()\); \(_{0}:^{N}^{M}\) specifies the initialization of the inner optimization, and \(_{t}:\) delineates the transition dynamics of the inner optimization at timestep \(t\). In particular, for gradient descent, \(_{t}(_{t-1}(),)=_{t-1}- _{t}_{}(_{t-1},)\), where \(_{t}\) denotes the step size at timestep \(t\).

To optimize \(\) using a first-order method, it is necessary to estimate the meta gradient \(_{}h\), which can be further decomposed according to the chain rule:

\[}h()}_{}= _{T}(),)}{ _{T}}_{T}()}{d}}_{}+_{T}(),)}{ }}_{}.\] (3)

The computation of meta-gradient poses a significant challenge, primarily due to the need for efficient approximation of the implicit gradient. This task is complicated by the recursive dependency of \(_{T}\) on \(\). To surmount this challenge, a variety of gradient-based bi-level optimization algorithms have been developed, as extensively reviewed recently in . These algorithms can be fundamentally categorized into three types based on their approach to meta-gradient approximation: _Gradient Unrolling_ (GU), _Implicit Function_ (IF), and _Value Function_ (VF). Recent innovations such as truncated RGU (TRGU)  and Hessian-Free approaches [76; 75; 9], which are predicated on GU and IF methodologies respectively, have introduced significant biases in their approximations to accommodate the computational constraints of large-scale scenarios. In the subsequent paragraph, we furnish a concise overview of GU-based approaches, addressing their non-constant memory issues in large-scale applications. Extended discussions on the remaining methods are reserved for Appendix B.

Gradient Unrolling.The core idea behind GU [17; 16; 40; 60] entails unrolling the inner optimization into an expansive computational graph, followed by the employment of automatic differentiation (AD) techniques for the iterative computation of gradients.

_Forward Gradient Unrolling_ (FGU) [17; 16] computes the meta gradient using the following forward recursive formula, starting from \(_{0}=_{0}()}{d}\):

\[_{t}()}{d}}_{_{t}}= _{t}(_{t-1}(),)}{_{t-1}}_{t-1}()}{d}}_{_{t}}+_{t}(_{t-1 }(),)}{}}_{_{t}},\;t=1,,T,\] (4)_Reverse Gradient Unrolling_ (RGU) [46; 16], instead of the employment of explicit recursive formulas of \(_{T}\), focuses on the implicit recursive formulas of \(_{}h\):

\[_{}h()= _{T}(), )}{_{T}}}_{_{T}}_{T}( )}{d}}_{_{T}}+ _{T}(),)}{}}_{_{T}}\] (5) \[= _{T}_{T}+_{T}}_{T}}_{_{T-1}}_{T-1}+_{T}_{T}+_{T}} _{_{T-1}}==_{0}_{0}+_{0}.\]

The corresponding reverse recursive formulas can thus be summarized as

\[_{t-1}=_{t}+_{t}_{t},_{t-1}=_{t}_{t}, t=T,,1.\] (6)

**Weakness (GU): Non-Constant Memory**. Both GU approaches exhibit a non-constant memory overhead, which constrains their utility in large-scale scenarios. The forward recursive formulas in (4) revolve around the Jacobian matrix product, demanding \((MN)\) space consumption. The reverse recursive formulas in (6) necessitate the storage of the entire trajectory of the inner optimization \(_{0:T}\) for backward computation, thereby imposing a memory requirement of \((TM)\). These requirements are often impractical for large-scale bi-level optimization, when \(\) and \(\) are of high dimension and a significant unrolled depth is required.

**Forward Gradient.** Forward-mode automatic differentiation (forward-mode AD) has been applied to a variety of research fields, including the training of recurrent neural networks , the computation of Hessian vector products , etc. However, the computation of the true gradient via forward-mode AD requires the full Jacobian, which is typically too costly to compute.

To solve this, forward gradient learning [69; 4; 63; 4; 56], built upon forward-mode AD, was proposed. Forward gradient methods update parameters based on the directional gradient along a random perturbation direction for backpropagation-free training. More formally, given a differentiable function \(h:^{N}\), the gradient for a given input \(^{N}\) can be approximated as

\[h()= h()^{T},\] (7)

where \( p()\) is a \(N\)-dimensional multivariate random variable, satisfying \([^{T}]=\). Common choices of the distribution of \(\) include Rademacher \((\{-1,1\}^{N})\), Gaussian \((,)\), and uniform distribution over a set of normalized orthogonal coordinates \((\{_{i}\}_{1:N})\). For any given \(\), \(h()\) is an unbiased estimator of \( h()\), as \([h()]=[ h()^{T}]= h()[^{T}]= h() = h()\). Despite the unbiasedness of \(h\), the dimension-dependent variance of the estimated gradient with a single direction impedes the scaling-up to high-dimensional problems. In practice, Monte Carlo gradient estimation can be used via averaged forward gradients over multiple random directions to reduce the variance.

## 3 (FG)\({}^{2}\)U: Forward Gradient Unrolling with Forward Gradient

We aim to circumvent the memory overhead issues associated with forward gradient unrolling (FGU) as discussed in Section 2. We begin by examining the forward gradient of \(h\) at \(\),

\[h()= h()^{T} }}{{=}}(_{T}_{T}+ _{T})^{T},\] (8)

where \( p()\) is a \(N\)-dimensional multivariate random variable, satisfying \([^{T}]=\). We follow the idea of FGU introduced in Section 2 to compute \(_{T}\). By multiplying both sides of (4) by \(\) on the right, we can obtain the recursive formulas for \(_{t}\) as

\[_{0}=_{0};_{t}=_{t}_{t-1} +_{t},\ \ t=1,,T.\] (9)

The revised recursive formulas in (9) facilitate the tracking of a \(M\)-dimensional vector \(_{t}\), rather than full Jacobian \(_{t}\) of size \(M N\), throughout the forward pass. The stochastic estimation in (8) is unbiased, adhering to the properties of forward gradient methods. To reduce the variance, we use Monte Carlo estimate via averaged forward gradients over _b i.i.d._ random directions:

\[h()=_{i=1}^{b} h()_{i} _{i}{}^{T}=_{i=1}^{b}(_{T}_{T}_{i}+_{T}_{i})_{i}{}^{T}.\] (10)

We call this algorithm **(FG)2U**, as an abbreviation of **F**orward **G**radient **U**nrolling with **F**orward **G**radient. The algorithm is summarized in Appendix A as Algorithm 1.

Compared to GU-based methods, as discussed in Section 2, (FG)2U eliminates the dependency on the meta parameter dimension \(N\) and the depth of unrolling \(T\) without introducing bias, significantly enhancing memory efficiency. Unlike IF-based methods, as discussed in Appendix B.2, (FG)2U overcomes the approximation issues associated with them while maintaining a constant memory overhead, thus providing superior gradient approximation. Compared to TRGU and Hessian-Free methods, which compromise approximation accuracy for efficiency, (FG)2U consistently delivers accurate gradient approximations. The computational efficiency of (FG)2U can be further enhanced by leveraging large-scale distributed computing resources, capitalizing on its inherently parallelizable formulation as presented in (10). In practice, a more cost-effective two-phase paradigm can be achieved by strategically placing (FG)2U and other methods at different stages of the training process, as we will discuss in Section 3.2. For an illustration of the role of (FG)2U in large-scale bi-level optimization, please refer to Figure 1.

### Convergence

In this section, we provide a convergence analysis for (FG)2U. The proofs can be found in Appendix C.

First, we establish a bound on the variance of the estimated gradient, when employing random vectors whose entries follow the Rademacher distribution.

**Lemma 3.1**.: _For any \(\), if \(_{i}(\{-1,1\}^{N})\), the gradient estimation in (10), satisfies_

\[\|h()- h()\|^{2}= \| h()\|^{2},\]

_where \(:=(0,1]\) as the sample size \(b\) is selected from \(1,,N-1\)._

The resultant error is bounded by \(O()\), where \(b\) represents the sample size used for computing the forward gradient, and \(N\) is the dimensionality of the gradient itself. This bound demonstrates how the error scales inversely with the sample size while also being influenced by the gradient's dimensionality.

Next, we lay down the following assumptions, on which our main theorems are based. Let \(=(,)\) denote the combination of the lower-level parameter \(\) and the meta parameter \(\). Following existing papers on the theory of bilevel optimization [45; 60; 28], in Assumption 3.2, we adopt some standard assumptions over the smoothness of the objective functions \(f\) and \(g\).

**Assumption 3.2**.: _The meta objective function \(f()\) and the lower-level objective function \(g()\) are both \(C\)-Lipschitz and L-smooth, i.e., for any \(,^{}\),_

\[|f()-f(^{})| C\|-^{ }\|,\ \ \| f()- f(^{})\| L\|- ^{}\|,\] (11) \[|g()-g(^{})| C\|-^{ }\|,\ \ \| g()- g(^{})\| L\|- ^{}\|.\] (12)

The next assumption regulates that the transition functions \(\) satisfy similar smoothness conditions.

**Assumption 3.3**.: _The transition functions \(_{0:T}\) are \(C_{}\)-Lipschitz and \(L_{}\)-smooth, i.e., for any \(,^{}\),_

\[\|_{0}()-_{0}(^{})\| C_{ {}}\|-^{}\|,\ \|_{0}()-_{0}(^{})\|  L_{}\|-^{}\|.\] (13)

_For any \(,^{}\), \(t=1,,T\),_

\[\|_{t}()-_{t}(^{})\| C_{ {}}\|-^{}\|,\ \|_{t}()-_{t}(^{})\|  L_{}\|-^{}\|.\] (14)

Assumption 3.3 is made to ensure the generality of our analysis over different optimizers. Note that \(\) is scheme-dependent w.r.t. the gradient-based optimizer we adopt for lower-level problems. In many cases, such as gradient descent where \(_{t}(_{t-1})=_{t-1}-_{t}_{}g (_{t-1})\), Assumption 3.3 is a direct consequence of Assumption 3.2.

We propose the following theorem and remark for convergence analysis of (FG)\({}^{2}\)U on problem (2). Notice the convergence result can be extended to the primal BO problem (1) with some further assumptions. We place a proof scratch and some discussions in Appendix C.3.

**Theorem 3.4** (Convergence).: _Suppose that Assumption 3.2 and Assumption 3.3 hold. Setting the learning rate \(=}\) for gradient descent over the hyperparameter \(\), then there exists a constant \(L_{h}\) (depending on \(C\), \(L\), \(C_{}\), \(L_{}\), and \(T\), and defined formally in the proof) such that_

\[_{k=0}^{K-1}[\| h(_{k})\|^{2} ]([h(_{0})]-_{}h( ))}{ K}.\] (15)

**Remark 3.5**.: _Theorem 3.4 shows that Algorithm 1 converges to an \(\)-accurate stationary point with a convergence rate of \(=(^{-1}^{-1})\)._

Recall that \(=\), which indicates that the convergence rate is linearly dependent on \(N\), which poses a significant challenge when managing high-dimensional meta-parameters \(\). However, it is important to note that the dimension-dependent convergence rate represents an upper bound, and scalability has been found to be feasible with several practical considerations, as discussed in the following subsection.

### Practical Considerations

**Choice of \(b\)**. According to the convergence analysis in Section 3.1, a sample size of \(b=(N)\) is required to achieve a convergence rate of \((^{-1})\). However, it has been widely observed that forward gradient and zeroth-order optimization, despite having dimension-dependent convergence rates, work well empirically with \(b=(1)\) in large-scale scenarios, such as in LLM fine-tuning [47; 74]. In this paper, we select the largest possible \(b\) that does not exceed the GPU memory limit for our empirical study. Additionally, gradient accumulation is utilized to further control variance and stabilize the training process.

**Cost-Effective Two-Phase Paradigm**. It is important to note that the upper bound delineated in (15) linearly depends on the performance discrepancy between the initialized meta parameter \(_{0}\) and the optimal. This dependence motivates the adoption of a more cost-effective two-phase paradigm for large-scale bi-level optimization. In the initial phase, we utilize efficient yet less accurate gradient approximation methods, such as TRGU  and Hessian-Free , to efficiently establish an initial \(_{0}\) that surpasses random initialization, while keeping computational overhead manageable. Subsequently, in the second phase, (FG)\({}^{2}\)U is utilized for a more accurate, albeit less efficient, gradient approximation to further elevate the performance, leveraging extensive computational resources.

**Implementation**. The technique employed in computing \( h()\) is identified as forward-mode automatic differentiation (forward-mode AD). In advanced automatic differentiation libraries, such as JAX  and PyTorch , forward-mode AD is efficiently implemented as Jacobian-vector product (jvp), without the necessity of explicitly computing the Jacobian matrix. The FLOP cost of jvp is approximately three times that of a standard forward pass, while the memory overhead is doubled. In practice, it is only necessary to define the forward computational graph of inner optimization and invoke forward-mode AD, which simplifies the implementation process significantly. Regarding distributed training, JAX offers the vmap interface for efficient intra-GPU parallelism and the pmap interface for effective inter-GPU parallelism.

**Zeroth-order Bi-level optimization**. In certain applications of bi-level optimization, the inner problem is approached as a black box, where the gradient of \(\) is inaccessible, rendering the analytical gradient unrolling unfeasible. For example, in PDE-constrained optimization [23; 62], in which the inner problem entails solving a Partial Differential Equation (PDE) using a non-differentiable solver. In such scenarios, rather than employing forward-mode Automatic Differentiation (AD), one can resort to Finite Difference methods to approximate the directional gradient \( h()\) by

\[ h()=_{ 0}+)-h()}{}+)-h()}{}\] (16)

with sufficiently small positive \(>0\). We refer to this zeroth-order variant of (FG)\({}^{2}\)U as (FG)\({}^{2}\)U-ZO, noting that the computation solely encompasses two forward passes and does not involve the utilization of any first-order information. The memory complexity is the same as forward-mode ADand the actual computation time will be slightly less than forward-mode AD, at the cost of introducing an approximation bias. We give a more detailed discussion within the context of _zeroth-order optimization_ in Appendix D, and empirically study a corresponding case in Section 4.

## 4 Experiments

We conduct experiments across various contexts, as detailed in the respective subsections. Initially, we engage in an image data condensation task, where we focus on a comprehensive performance comparison between (FG)\({}^{2}\)U and both classical and large-scale bi-level optimization algorithms. Subsequently, we investigate meta-learning for the online adaptation of language models, employing a GPT model as the inner model, to illustrate how (FG)\({}^{2}\)U effectively circumvents the non-constant memory issue associated with RGU. Finally, we address a physics-informed bi-level optimization problem, where gradient-based inner solvers are ineffective, to demonstrate the efficacy of combining (FG)\({}^{2}\)U-ZO, the zeroth-order variant of (FG)\({}^{2}\)U discussed in Section 3.2, with non-differentiable numerical solvers.

**Data Condensation**. To overcome the challenges posed by large-scale datasets, a line of works known as data condensation [68; 72] has been proposed. The main idea is to generate a compact, synthesized dataset, designed to elicit similar behaviors in machine learning models as those trained with the original, massive dataset. The objective of the mainstream principles  designed for data condensation can be naturally formulated as a bi-level optimization problem. We focus on the best-known principle _performance matching_ on classification tasks, which can be formulated as

\[_{_{c}}\ (_{T};_{o}),\ \ _{t}=_{t-1}-(_{t-1};_{c}),\ \ t=1,,T,\] (17)

where \(_{o}\), \(_{c}\) respectively denote the original and condensed dataset, \(\) denotes the model parameter, \(\) denotes the cross-entropy loss function, and \(\) represents the step-size for inner optimization.

We conducted our experiments following the standard data condensation setting established by [68; 77; 67]. A more detailed task description is given in Appendix E.1 and implementation details are given in Appendix F.1.

The condensed datasets are evaluated using 3-layer convolutional networks with randomly initialized parameters, and the average accuracies on test datasets are summarized in Table 1. Compared to large-scale bi-level optimization methods like TRGU and Hessian-Free, which prioritize efficiency at the expense of approximation accuracy, (FG)\({}^{2}\)U exhibits significantly better performance, due to more accurate gradient approximation as explained in Appendix B. Additionally, we assessed Neumann Series (denoted as Neumann in Table 1), an IF-based method that mitigates gradient approximation errors through extended computations, as introduced in Appendix B.2. While it demonstrates performance enhancements over the Hessian-Free method, Neumann still yields suboptimal performance

    &  &  &  &  \\  & & & TRGU & Hessian-Free & Neumann & **(FG)\({}^{2}\)U** & RGU & WHOLE \\   & 1 & 0.017 & 73.76\(\)1.68 & 65.98\(\)1.38 & 68.37\(\)1.44 & **82.44\(\)**0.68 & 92.32\(\)0.33 \\  & 10 & 0.17 & 94.05\(\)0.33 & 94.97\(\)0.34 & 95.75\(\)0.24 & **96.12\(\)**0.28 & 96.79\(\)0.29 & 99.6\(\)0.00 \\  & 50 & 0.83 & 96.63\(\)0.41 & 96.34\(\)0.31 & 96.78\(\)0.22 & **97.01\(\)**0.19 & 97.72\(\)0.23 \\   & 1 & 0.02 & 20.78\(\)1.07 & 19.72\(\)1.28 & 21.33\(\)0.90 & **29.37\(\)**0.75 & 34.08\(\)0.55 \\  & 10 & 0.2 & 44.01\(\)0.57 & 45.32\(\)1.02 & 47.67\(\)0.87 & **50.10\(\)**0.56 & 53.15\(\)0.53 & 84.8\(\)0.10 \\  & 50 & 1 & 49.22\(\)0.45 & 48.73\(\)0.78 & 50.02\(\)0.69 & **51.98\(\)**0.44 & 56.37\(\)0.37 \\   & 1 & 0.2 & 3.96\(\)0.68 & 3.14\(\)0.41 & 4.52\(\)0.56 & **8.22\(\)**0.45 & 15.61\(\)0.32 \\  & 10 & 2 & 20.20\(\)0.66 & 19.01\(\)0.84 & 20.87\(\)0.82 & **23.38\(\)**0.33 & 25.42\(\)0.45 & 56.2\(\)0.30 \\   & 50 & 10 & 22.33\(\)0.93 & 23.59\(\)0.71 & 24.52\(\)0.77 & **25.84\(\)**0.31 & 28.52\(\)0.53 \\   

Table 1: The performance (testing accuracy %) comparison among various bilevel optimization methods on the data condensation task over three datasets. All the datasets are condensed using a 3-layer ConvNet. IPC: image(s) per class. Ratio (%): the ratio of condensed examples to the whole training set.

compared to (FG)\({}^{2}\)U, owing to the inherent bias of the IF-based method. Further discussions and supporting evidence are available in Appendix B.2.

The results of RGU, which represent the upper performance bound for both TRGU and (FG)\({}^{2}\)U, are provided for reference, along with the results from training on the entire dataset (denoted as WHOLE in Table 1), representing the upper performance bound for all approaches. However, it is crucial to acknowledge that RGU is not practical in large-scale bi-level optimization scenarios due to its non-constant memory requirements, as discussed in Section 2. This limitation will be further exemplified in the subsequent, where the inner model is significantly larger. In principle, the performance of (FG)\({}^{2}\)U can be further improved to approach that of RGU by increasing the number of random directions for gradient approximation.

The memory and computational efficiencies of TRGU, Hessian-Free, and (FG)\({}^{2}\)U in the most challenging case (CIFAR-100, IPC=50) are reported in Figure 1 (Bottom Right), demonstrating that the efficiency of (FG)\({}^{2}\)U can be significantly enhanced through intra/inter-GPU parallelism.

Meta Learning Online Adaptation of Language Models.The online adaptation of language models (LM) has been studied recently to keep the knowledge of LM current . However, trivial auto-regressive fine-tuning the LM, which applies uniform weights to all tokens, often results in suboptimal performance in downstream tasks. This issue stems from the default average negative log-likelihood (NLL) loss, which fails to capture the significance of tokens . To overcome this limitation,  proposed Context-aware Meta-learned Loss Scaling (CaMeLS), a strategy that employs meta-learning to adjust token weights for more effective online adaptation. Specifically, they meta train a weight model to reweight the auto-regressive loss during online fine-tuning, aiming to enhance LM performance on downstream question-answering tasks. A comprehensive task description and the mathematical formulation of the objectives are detailed in Appendix E.2.

The trained weight model is subsequently fine-tuned on unseen online documents and evaluated on corresponding question-answering tasks. In , RGU is utilized for meta gradient approximation. To mitigate the non-constant memory issue associated with RGU, a DistilGPT2 model  is chosen as the surrogate base model for training the weight model, instead of larger models typically employed for online adaptation. Additionally, a very limited unrolled depth of 6 is utilized within a 40 GiB GPU memory budget. In our experiments, since (FG)\({}^{2}\)U has circumvented the non-constant memory issue associated with RGU, we are able to increase the unrolled depth and upscale the base model for training the weight model. Empirical evaluations are conducted on two datasets, StreamingQA  and SQuAD-Seq .

Firstly, we increased the unrolled depth while maintaining the base model as a DistilGPT2. We plotted the F1 scores and GPU memory usages for RGU with unrolled depths of \(\{1,2,4,6\}\) and (FG)\({}^{2}\)U with unrolled depths of \(\{24,48\}\) on StreamingQA in Figure 1 (Bottom Left). The performance of the weight model is positively correlated with the unrolled depth, substantiating the benefits of

    &  &  &  \\  & & EM (\(\)) & F1 (\(\)) & EM (\(\)) & F1 (\(\)) \\   & CaMeLS + RGU  & 1.62 & 5.79 & 1.45 & 3.08 \\  & CaMeLS + RGU (impl.) & 2.04 & 5.53 & 1.52 & 3.16 \\  & CaMeLS + **(FG)\({}^{2}\)U** (ours) & **2.22** & **6.37** & **1.72** & **3.50** \\   & CaMeLS + RGU  & 5.35 & 10.60 & 4.97 & 8.63 \\  & CaMeLS + RGU (impl.) & 7.02 & 12.19 & 4.86 & 8.57 \\  & CaMeLS + **(FG)\({}^{2}\)U** (ours) & **7.21** & **12.50** & **5.56** & **8.99** \\   & CaMeLS + RGU  & 6.55 & 11.67 & 6.70 & 10.15 \\  & CaMeLS + RGU (impl.) & 7.93 & 12.94 & 6.71 & 9.65 \\   & CaMeLS + **(FG)\({}^{2}\)U** (ours) & **8.89** & **14.42** & **7.37** & **10.37** \\   

Table 2: Comparison of the online adaptation performance. The reported evaluation metrics include the exact match (EM) and F1 scores. For vanilla CaMeLS , RGU is conducted with unrolled depth \(6\), using DistilGPT2 as the base model. We present both the results reported by  and those from our implementation (denoted as impl.). For CaMeLS + (FG)\({}^{2}\)U, we select unrolled depths from \(\{24,48\}\), and the base model from \(\{,\}\). We report the results for the combination that yields the best F1 score. Additional details and ablation studies are documented in Appendix G.1.

training with larger unrolled depths. The non-constant memory issue associated with RGU can be observed when the unrolled depth increases, while (FG)\({}^{2}\)U maintains constant memory even with large unrolled depth. Subsequently, we endeavored to upscale the base model to GPT2 to reduce the disparity between training and evaluation. The performances are summarized in Table 2, with detailed ablation studies on unrolled depths and base model variants documented in Table G.1 and Table G.2.

**Data-driven Discovery of Partial Differential Equations (PDEs)**. Let us consider the following general forms of parametrized and nonlinear PDEs:

\[u_{t}+[u;]=0,\;x,t[0,T],\] (18)

where \(x\) denotes the space-time coordinate, \(\) denotes a bounded domain with boundary, \(u:[0,T]\) denotes the latent solution, \(u_{t}\) represents the first-order derivative of \(u\) with respect to \(t\), and \(\) is a general differential operator parameterized by \(\), acting on \(\). This setup encompasses a broad spectrum of problems in physics. For example, the one-dimensional Burgers' equation is defined by \([u;]= uu_{x}- u_{xx}\), where \(=(,)^{2}\), and \(u_{x}\), \(u_{xx}\) represent the first and second-order derivatives of \(u\) with respect to \(x\), respectively.

The problem of data-driven discovery of PDEs  can be framed as follows: given a set of scattered observations of the latent solution \(u(x)\), what are the parameters most accurately describing the observed data? The problem can be formulated as a PDE-constrained optimization problem (PDECO):

\[_{}\;_{x,u}\;|u(x;)-u|^{2} s.t. u_{t}+[u(;);]=0,x,\] (19)

where \(=\{(x_{i},u_{i})\}_{1:k}\) denotes the observed data. In cases where the closed-form solutions of the nonlinear PDEs are intractable, parametric solutions \(u_{}\) are used to approximate the latent solution \(u\) for given \(\). The PDECO in (19) is then reformulated into a bi-level optimization problem:

\[_{}\;_{x,u}\;|u_{_{S}()}(x;)-u|^{2} s.t._{s}()=_{s}(_{s-1},),s=1,,S.\] (20)

Employing gradient-based PDE solvers, such as physics-informed neural networks (PINN) , facilitates the direct application of (FG)\({}^{2}\)U. However, as demonstrated in Figure 2 (Left), the accuracy and efficiency of PINNs fall short of the rigorous demands of scientific computing. This limitation has prompted us to integrate faster and more accurate traditional solvers like the spectral method  (see also Appendix E.3.4) to tackle the inner problem. Given these solvers are non-differentiable, we employ (FG)\({}^{2}\)U-ZO, the zeroth-order variant of (FG)\({}^{2}\)U introduced in Section 3.2, to solve the problem.

We conduct experiments on three non-linear PDEs: Burgers, Allen-Cahn, and KdV, with a more detailed task description available in Appendix E.3. The results are summarized in Figure 2 (Right). We can observe that the combination of (FG)\({}^{2}\)U-ZO and the numerical solver significantly outperforms (FG)\({}^{2}\)U and the PINN solver, in terms of both the prediction on \(\) and \(u\). The implementation details are documented in Appendix F.3.

Figure 2: **Left**: Comparison of efficiency between the PINN solver and the numerical solver. We evaluated Adam  and SGD as the inner optimizers for the PINN solver, with steps ranging from 100 to 50,000. The results demonstrate that the numerical solver is significantly more efficient. **Right**: Comparison of relative L2 errors in the prediction of \(\) and \(u\). \(_{}=\|_{pred}-\|_{2}/\|\|_{2}\), \(_{u}=\|u_{pred}-u\|_{2}/\|u\|_{2}\).

Conclusion

In this work, we propose a novel algorithm **F**orward **G**radient **U**nrolling with **F**orward **G**radient, abbreviated as (**FG**)\({}^{2}\)**U**, designed to tackle the challenges associated with large-scale bi-level optimization. We conduct a convergence analysis of (FG)\({}^{2}\)U, perform extensive comparisons with existing methods, and provide detailed discussions on its practical applications. Additionally, we undertake an empirical evaluation across a series of large-scale bi-level optimization tasks. Our findings indicate that (FG)\({}^{2}\)U effectively complements existing bi-level optimization algorithms, addressing gaps in large-scale bi-level optimization scenarios.

**Limitations and future works**. The experiments conducted in this paper are of relatively small scale, with the largest inner model being a GPT-2 model. We look forward to validating its effectiveness on larger-scale bi-level optimization tasks. Additionally, the application of black-box bi-level optimization and the potential of (FG)\({}^{2}\)U-ZO remain underexplored, considering the prevalent black-box interaction between users and models today. We hope our work will inspire further development of large-scale bi-level optimization algorithms and their application in corresponding scenarios. Furthermore, we have not specifically addressed the efficiency issues inherited by (FG)\({}^{2}\)U from the forward gradient method. Enhancing the efficiency of (FG)\({}^{2}\)U while maintaining its gradient estimation accuracy will be an important direction for future research.

## 6 Acknowledgements

This research is supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-TC-2023-010-SGIL) and the Singapore Ministry of Education Academic Research Fund Tier 1 (Award No: T1 251RES2207, T1 251RES2218). The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).