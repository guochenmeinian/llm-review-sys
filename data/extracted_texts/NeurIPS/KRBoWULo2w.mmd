# PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning

Florian Bordes\({}^{1,2,3}\)   Shashank Shekhar\({}^{1}\)   Mark Ibrahim\({}^{1}\)   Diane Bouchacourt\({}^{1}\)

**Pascal Vincent\({}^{1,2,3}\)   Ari S. Morcos\({}^{1}\)**

\({}^{1}\)FAIR, Meta  \({}^{2}\)Mila - Quebec AI Institute  \({}^{3}\)Universite de Montreal, DIRO

###### Abstract

Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited - and often played down - mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of _photorealistic_ synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both _controllability_ and _realism_. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce **PUG (Photorealistic Unreal Graphics)** environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models. The datasets can be downloaded at https://pug.metademolab.com/.

## 1 Introduction

A grand goal of machine learning is to learn representations of data that are useful across many tasks. Essential to measuring and making progress towards this goal is the availability of ample controllable, realistic data for evaluation and training. This is especially true when considering deep neural network models not only in terms of their raw accuracy, but also their robustness and fairness--crucial properties for models deployed in real-world applications. However, collecting such data is challenging, presenting issues with privacy, bias, and copyright. Furthermore, the majority of available image datasets lack fine-grained labels and are challenging to manipulate beyond coarse image augmentations (e.g. with a photograph, it is hard to change the viewpoint or the time of day).

Using synthetic image data where we precisely control all the factors affecting the rendered scene gives easy access to the corresponding rich set of factor labels. This enables evaluating the extent of a trained deep neural network's abilities, most importantly its robustness. Is the network robust to change in pose? Are the predictions similar for different textures? All these questions may be answered systematically by using synthetic data, enabling highly rigorous evaluations of deep neural network models. In addition, training could also benefit from controllable factors1, by increasing the robustness of models with respect to these factors. They may also be used to monitor training, e.g. tracking which factors a model focuses on or becomes most invariant to, and in which order, as training progresses. This potentially enables better understanding of the training and generalizationdynamics in deep neural networks. However the lack of realism typical in many of the currently available synthetic image datasets, and their usually very limited scope greatly limits their usefulness for general image representation learning research.

To address this, we introduce2 a new family of synthetic _Photorealistic Unreal Graphics_ (PUG) _datasets_, designed for ease of use by the representation learning research community, where image realism is significantly improved compared to current public synthetic image datasets. The environments were built using the Unreal Engine [EpicGames], which is widely used in the video game and entertainment industries and praised for its realism. In addition to pre-rendered static image datasets, we also introduce the TorchMultiverse python library, which offers a simple python interface to enable easily controlled dataset creation from any given PUG environment. Using these tools, we contribute 4 new datasets and show their usefulness across several different research domains. To summarize:

* We introduce a new family of environments and image datasets (coined as PUG) for representation learning, based on the Unreal Engine [EpicGames].
* We present _PUG: Animals_ for research on out-of-distribution (OOD) generalization and to study the representational space of foundation models.
* We introduce _PUG: ImageNet_ as an additional robustness test set to ImageNet, containing a rich set of factor changes such as pose, background, size, texture, and lighting.
* We introduce _PUG: SPAR_ for evaluating vision-language models. We use it to demonstrate how synthetic data can be utilized to address known benchmark limitations. In addition, we introduce _PUG: AR4T_ for fine-tuning vision-language models and use it to demonstrate the reliability of PUG: SPAR in contrast to other benchmarks.

## 2 Related work

Synthetic data for representation learningTo address robustness shortcomings, researchers today commonly study representations using lower-fidelity controlled datasets such as CLEVR, Biased Cars, and ShapeNet [Johnson et al., 2017, Madan et al., Chang et al., 2015]. Other datasets also contain precise factor labels useful for probing how well a representation encodes each factor in a structured form [Gondal et al., 2019, Struckmeier et al., 2023, Weng, 2018]. While these datasets offer control in terms of the factors that change as well as the train and evaluation splits enabling controlled scientific experimentation, they lack realism. This gap between the lower-fidelity controlled data and the real world poses a challenge for the broader application of these studies. On the other hand, photorealistic datasets have been explored in various application-specific domains in machine learning (outside

Figure 1: **The PUG Dataset Family** (left) Cartoon illustration of our dataset creation setup, which consists of two steps: environment creation and then data creation. (right) Example images from PUG: Animals, PUG: Image-Net and PUG: SPAR.

of representation learning.) This is especially relevant when trying to evaluate and train models on rare events in which getting real data might be really difficult, such as for autonomous driving. CARLA (Dosovitskiy et al., 2017) is a popular self-driving car simulator which offer highly realistic environment with a significant amount of controllable factors such as environmental conditions, full control of all static and dynamic actors and maps rendering. Another domain where simulated environments are commonly used is reinforcement learning (RL), as RL algorithms often requires the ability to run millions of simulations to learn to master non-trivial tasks, and this cannot be done in a real environment. Data environments based on video games like Atari have been very popular to design and evaluate RL algorithms. Alternatively, platforms like Habitat (Szot et al., 2021) offers indoor scene for training home assistant agents. While these simulators, games or datasets can offer some photo-realism and mimic real world interactions for agents, they are relegated to domain-specific applications making them challenging to use for evaluating the representations of deep neural networks more broadly. Since our focus is not RL, we do not need to embed a fast simulator capable of rendering several thousands frames per second for effective online-training. Instead we can pre-render custom high-quality datasets offline. Photorealistic environments and datasets have also been explored for more general domains with the ThreeDWorld platform (Gan et al., 2021). Based on the Unity game engine, it offers an interactive environment that can be leveraged to create datasets. The environment is presented as a simulator that is generic enough to handle multiple uses cases, and users can customize the setup of a scene and the data smampling through a low level API. One such dataset that utilizes ThreeDWorld is the Synthetic Visual Concepts (SyVIC) dataset (Cascante-Bonilla et al., 2023), which uses the API to create scene images and descriptive captions for training vision-language models. One of the downsides of ThreeDWorld is that the back-end, the simulator itself, is closed source which limits external contributions. In contrast with ThreeDWorld, we do not provide a platform or a generic simulator for people to use. In fact, we believe that tools like the Unreal Engine are simple enough to be used directly by researchers to create the environments they want without the need to use an intermediate platform. In addition, being free of such intermediate platform allows us to leverage most of the content created for video gaming directly into our simulator by using the existing Epic Games marketplace.

Evaluating model robustnessTo study model robustness, there is an inherent trade-off between photo-realism and control. Photo-realism depicts objects as they appear in the real world, but often lacks control to precisely define the factors to describe the object such as pose or background. Prior works either collect natural images with specific factor changes (Xiao et al., 2020; Barbu et al., 2019) or label distinctive factors in existing datasets (Idrissi et al., 2022). Such datasets allow researchers to measure average accuracy on photo-realistic images, but lack granular control necessary for precisely controlled robustness experiments. On the other hand, prior studies (Ibrahim et al., 2022; Abbas and Deny, 2022; Alcorn et al., 2019) examine model robustness with respect to factors such as pose and size by rendering 3D-objects such as buses. These studies precisely control how each object is depicted, but lack in realism. In this work, we advance the photo-realism of these prior works by using the Unreal engine 5.0 (EpicGames), a rendering engine commonly used in high-end cinematic CGI and high-resolution video games which allow us to measure robustness with respect to factors of variation such as lighting.

Benefits and limitations of using generative models as data generatorAnother way to generate realistic datasets is to use generative models(Ho et al., 2020; Goodfellow et al., 2020). However, one limitation of such models, despite impressive improvements in the last few years (Dhariwal and Nichol, 2021), is the lack of quality control on what the model can produce (Gandikota et al., 2023). It's not uncommon to find cases in which the model will ignore parts of the conditioning prompt. Despite such limitation many works have tried to leverage generative model as an additional source of data to train deep neural networks with some success (Astolfi et al., 2023; Bansal and Grover, 2023; Trabucco et al., 2023; Azizi et al., 2023; Zhang et al., 2021; Li et al., 2022; Jahanian et al., 2022; Jain et al., 2023; Sarivildiz et al., 2023; He et al., 2023). Another limitation of using generative models are privacy concerns that arise from such models replicating data from their training datasets Somepalli et al. (2022). Finally, Shumailov et al. (2023) recently demonstrated that training on data recursively generated from such models results in increasing underestimates of the tails and overestimates of the mode, amplifying bias in datasets. In contrast to generative models that might produce unreliable results, we use an entirely controllable environment for which we can have a known and accurate generation with respect to a set of factors.

## 3 Photorealistic Unreal Graphics (PUG) environments and datasets

### Leveraging Unreal Engine to create environments and datasets for representation learning

We introduce the _Photorealistic Unreal Graphics_ (PUG) environments, a family of 3D graphics environments that leverage Unreal Engine for rendering image data for representation learning research. To create a PUG environment, we first obtain a number of assets 3 which can be 3D objects or 3D backgrounds. Then, we import them in the Unreal Engine editor and create blueprints that yield a simple generic 3D environment. Once this generic and controllable environment is created, it is compiled into a Linux binary file, which can be run on standard GPU clusters. This environment is programmed in such a way that when running, it is listening for incoming packets through WebRTC which can specify instructions about how to change a scene. Since most machine learning practitioners are used to python scripting, we wanted to have a very simple approach by which a user can request image data rendered from a packaged PUG environment, through very simple python code and JSON config files. To do so, we developed a python API, _TorchMultiverse_, that allows a user to easily specify a scene configuration in JSON and request rendered images from the PUG by using WebRTC. Once the factors have been set as requested by the user, for a specific environment configuration, the user can send a command to freeze the current environment and receive back an image. It takes around 1 second to render an image at a resolution of 512x512 on a V100 GPU4. We illustrate this setup in Figure 1. It shows how, starting from 3D assets, we design interactive environments that enable us to create different datasets. In the present work, we focus on pre-rendered static image datasets, however our setup also allows dynamic communication between a PUG environment and a pytorch program, meaning that new data could be requested and rendered on the fly while training a deep neural network. We leave the exploration of such active learning setups, as well as the rendering of videos, as future work.5

### PUG: Animals

As the first member of the PUG family we introduce _PUG: Animals_ (Figure 2), which contains 215 040 pre-rendered images using 70 animals assets, 64 backgrounds, 3 object sizes, 4 textures, under 4 different camera orientations. PUG: Animals is designed with the intent to create a dataset with every combination of the factors of variation available. PUG: Animals allows one to precisely control distribution shifts between training and testing which can give researchers better insight on how a deep neural network generalizes on held out factors of variations. Surprisingly, the usage of 3D realistic synthetic data is limited in OOD generalization research - with the exception of Biased-cars[Madan et al., 2022] that has been used to study generalization on new category-viewpoints. Commons OOD datasets are Colored MNIST [Arjovsky et al., 2020] - to study how well a network can generalize to unseen combinations of digits and colors and MNIST-R [Ghifary et al., 2015] - to study generalization on new combination of digits and rotations. However, MNIST-based dataset

Figure 2: We present _PUG: Animals_, a new photorealistic synthetic dataset with annotated factors of variations to evaluate the out-of-distribution (OOD) robustness of models.

might be too tough to evaluate modern architectures. A more _realistic_ dataset based on real images is Nico++[Xingxuan Zhang, 2022] - to study generalization with respect to different domains or environments. However, in Nico++ the objects and backgrounds are never entirely disentangle (the context background is different for each image). Thus, it is never clear if the model is failing because of the context or because of a specific object (since the contexts and the objects are never disentangle). In contrast, in PUG: Animals the animal asset is always the same, in that case the environment factor and the objects are perfectly disentangle such that if the model is able to classify correctly an elephant on a road and is not able to classify the elephant in a museum, we can rigorously say that the failure is caused by the change in context background. In addition of analysis the robustness with respect to the background scene, it is also possible to analyse with PUG: Animal the robustness with respect to the camera position, asset size and texture (as we demonstrated in Appendix 3.2).

Classification with held out setsIn the first experiment, we held out some factors of variation during training (backgrounds, sizes or textures) except for a specific number of animals \(C\) and use the held out data as validation set. Thus, \(C=0\) means that the network never saw the factor during training (this is an OOD scenario with unseen factors) while \(C=10\) imply that the network saw this factor for least 10 different animals (OOD scenario with unseen combinations of factors). In Figure 3, we present our results training a ResNet50 with different held out factors. Every model reached more than 99% accuracy on the training set. First, we trained on 50 backgrounds and used the remaining 14 backgrounds for validation: here, the network reached an accuracy of 80%. However, when using only 30 backgrounds for training and using the remaining 34 as validation, the accuracy drop significantly. Interestingly, showing every background for some of the animals (having unseen combination of factors instead of just unseen factors) decrease performance. In contrast, for texture, we found that having at least 10 animals for which every possible textures are seen during training improves generalization. Interestingly, the network overfits much more to the grass texture relative the default network. Lastly, when looking at the size factor, it seems that training on medium size assets leads to good generalization on small and large assets while training only on small asset leads to worse performance on medium and large assets.

Studying foundation model representational space PUG:Animal can also be to study the equivariance of foundation models' representations. For this, we augment each image in PUG: Animals with a caption that describes it according to its factor values (sizes are converted to three adjectives: "small", "medium" or "big", see Appendix C.1 for details), using the following template6: _"A photo of a [size] sized [character] textured with [texture] on a [background] background"_. Informally, equivariance of a model's representation with respect to a factor means that when the factor changes from one value to another, the embedding of the corresponding image (or caption) changes in a predictable manner. Equivariance is a sought-after property to improve sample efficiency and robustness to transformations [Klee et al., 2023, Tai et al., 2019, Wang et al., 2023]. Similar to previous works on equivariance and compositionality Bouchacourt et al. , Xie et al. , we measure equivariance as the alignment (i.e. parallelism) between embedding differences. First, we feed images and their corresponding captions to 9 pretrained vision-language models including multiple CLIP models Radford et al. , NegCLIP Yukeskegounl et al.  Flava Singh et al. , BLIP Li et al. [2022b] and X-VLM Zeng et al.  and collect their embeddings of PUG: Animals images and created captions. For each model, we compute difference vectors between the embeddings of two images (or captions) of an object undergoing a factor change: e.g. a big penguin textured with grass on a background "village square" modified to the same penguin but

Figure 3: Accuracy on held out factors with PUG: Animals. Each line and value C correspond to the number of animals for which all the factors are seen. The test space is built by taking all the factors minus the training factors. If we train on the Default texture, then the network is evaluate on Grass, Sky and Asphalt. If we train on 50 backgrounds, then we evaluate on 64 (total number of background) - 50 (training background) = 14 backgrounds.

with background "restaurant", see arrows in Figure 4 (left). Specifically, for a sample \(i\), undergoing a change from background \(b_{k}\) for background \(b_{l}\), we denote the difference vector between the embedding of the image of the sample with background \(b_{k}\) and the image of the same sample but with background \(b_{l}\) by \(v^{i}_{b_{k} b_{l}}\). Similarly, we denote by \(u^{i}_{b_{k} b_{l}}\) the difference vector between the embedding of each of the two captions accompanying the images.

Then, we measure the alignment of difference vectors across pairs _undergoing the same factor change_ (here, the penguin and the cat) as their cosine similarity7. We estimate three types of equivariance: (i) Image equivariance: how parallel (measured with cosine similarity) are difference vectors across image pairs? (lined and dashed red arrows) (ii) Text equivariance: same but for caption pairs (parallelism of lined and dashed green arrows) (iii) Across modalities equivariance: for the same object, alignment of difference vectors between pairs of image-caption (i.e. alignment of the two arrows for the penguin). Specifically, for image equivariance between sample \(i\) and \(j\), for background change \(b_{k}\) to \(b_{l}\), we compute:

\[sim(v^{i}_{b_{k} b_{l}},v^{j}_{b_{k} b_{l}})=_{b_{k} b_{l}} \,^{T}v^{j}_{b_{k} b_{l}}}{||v^{i}_{b_{k} b_{l}}||\,||v^{j}_{b_{k} b _{l}}||}\] (1)

For text equivariance, we compute be \(sim(u^{i}_{b_{k} b_{l}},u^{j}_{b_{k} b_{l}})\) while for across equivariance, we compute \(sim(v^{i}_{b_{k} b_{l}},u^{i}_{b_{k} b_{l}})\). We report cosine similarity averaged over pairs and possible changes for each factor (higher value means higher equivariance, 1 is the maximum). Specifically, the image equivariance for background writes as

\[_{b_{k}}_{b_{l}}_{i}_{j}sim(v^ {i}_{b_{k} b_{l}},v^{j}_{b_{k} b_{l}})\] (2)

where \(B\) is the number of possible backgrounds and \(N\) is the number of samples.

We show in Figure 4 (right) results for equivariance with respect to the background. Plots for equivariance to texture and size are in Figure 10. Looking at Figure 4 results (right side, top row), we see that the foundation models' image embeddings present high equivariance to background (\(0.78 0.04\) on average over models). There is also (see Figure 10a) small image equivariance to texture (\(0.15 0.04\)), but almost no equivariance to size (\(0.06 0.02\)). Text equivariance is high with respect to background (average of \(0.87 0.03\)), but is also strong for size and texture (\(0.71 0.11\) for size and \(0.81 0.03\) for texture, see Figure 10b) suggesting that foundation models' caption embeddings can be manipulated with vector arithmetic, similar to word vectors behaviours Ethayarajh et al. (2019). This aligns with the recent work of (Trager et al., 2023) that show linear behavior of VLMs text embedding spaces. Across modalities, small equivariance is present with respect to background (\(0.22 0.03\) and Figure 4 right side, bottom row). However when size or texture change for a given object, its image and caption representations seem to move in non-aligned directions (\(0.07 0.01\) for texture and \(0.04 0.01\) for size, see Figure 10c). While more syntactically complex captions and other equivariance metrics could be designed, our aim here is to provide an initial study to showcase how PUG: Animals can be easily used to study state-of-the-art models representations.

Figure 4: **Measuring foundation models equivariance thanks to PUG: Animals. Left: Illustration of how to use PUG: Animals to compute equivariance. Right: Image and text equivariance is present with respect to background, while across modalities equivariance to background doesn’t hold as much. See main text for detailed results.**

### PUG: ImageNet

As a second member of the PUG family, we introduce _PUG: ImageNet_ (Figure 5), which contains 88,328 pre-rendered images using 724 assets representing 151 ImageNet classes with 64 backgrounds, 7 sizes, 9 textures, 18 different camera orientation, 18 different character orientation and 7 light intensity. In contrast to PUG: Animals, PUG: ImageNet was created by varying only a single factor at a time (which explain the lower number of images than PUG: Animals despite using more factors). The main purpose of this dataset is to provide a novel useful benchmark, paralleling ImageNet, but for _fine-grained evaluation of the robustness_ of image classifiers, along several factors of variation.

An extensive evaluation of the robustness of SOTA modelsOur PUG: ImageNet dataset offers both photo-realism and precise control over how each object is depicted from pose and size to environment and camera-angle. We also provide a collection of objects with mappings to classes in the popular ImageNet dataset, enabling researchers to probe the robustness of SoTA vision models without retraining. We assess a variety of model architectures across several pretraining datasets including ImageNet-1/-21k, LAION (400M and 2B), and JFT300M (Kolesnikov et al., 2020, Liu et al., 2021, Dosovitskiy et al., 2021). We observe in Table 1 that the models that perform the best on the ImageNet validation accuracy are not always the ones which offer the best robustness on PUG: ImageNet. For example, the pretrained ViT-B32 trained on ImageNet-21k is better on the ImageNet validation set compared to a Win-B, but offers worse robustness across all factors. We confirm no statistically significant relationship exists between ImageNet accuracy and robustness by computing Pearson's correlation coefficients (Appendix C.3). This result showcases how PUG: ImageNet can be added as an additional benchmark to evaluate vision models.

### PUG: SPAR for VLMs

As a third member of the PUG family, we introduce _PUG: SPAR (Scene, Position, Attribute and Relation)_ for evaluating vision-language models (VLMs). In contrast to pure vision based models, VLMs should be able to predict the correct caption (from a given set of captions) that describe the content of a given image. Several benchmarks to evaluate VLM models already exist such as Winoground (Thrush et al., 2022) or ARO (Yuksekgonul et al., 2023). However, recent works(Thrush et al., 2022, Diwan et al., 2022) have highlighted an important limitation in these benchmarks: some image-caption pairs in Winoground might be even too difficult to solve for a human whereas ARO

    & &  &  \\   & ImageNet-1/-21k & Cameras (Yuzsa,Puckn,Rull) & Pose (Yuzsa,Puckn,Rull) & Size & Texture & Light & Background \\  ResNet50 & 81.5 & (38.1, 33.1, 26.9) & (38.0, 23.6, 22.9) & 35.7 & 27.0 & 13.6 & 29.5 \\ ResNet101 & 82.3 & (44.3, 35.9, 39.4) & (45.1, 36.7, 25.6) & 39.7 & 31.1 & 14.1 & 32.8 \\ ViTlarge & 58.8 & (52.4, 20.4, 37.1) & (52.4, 34.8, 28.4) & 46.4 & 42.9 & 8.9 & 34.6 \\ ViTlarge & 84.3 & (37.5, 34.3, 37.7) & (38.0, 21.8, 20.5) & 33.0 & 28.5 & 4.1 & 26.6 \\ Swin & 83.6 & (56.4, 45.6, 41.8, 45.9) & (56.9, 33.4, 34.2) & 52.9 & 40.1 & 19.1 & 42.0 \\ BT (DT300M) & 80.3 & (40.3, 32.3, 36.0) & (42.1, 23.6, 22.8) & 37.3 & 23.4 & 6.3 & 20.5 \\ DINO2 (400M) & 84.5 & (45.6, 41.1, 37.4) & (47.5, 28.5, 28.5) & 43.1 & 35.0 & 6.1 & 30.9 \\ Pircus (PMD70M) & 75.5 & (31.2, 32.4, 17.6) & (50.8, 17.6, 15.4) & 30.5 & 24.2 & 7.8 & 21.9 \\ CIFAR (2400M) & 62.9 & (41.7, 30.2, 22.1) & (41.6, 23.8, 20.9) & 40.1 & 34.4 & 5.7 & 24.4 \\ CIFAR1 (210B) & 66.6 & (44.0, 31.5, 24.1) & (43.8, 24.8, 21.3) & 44.2 & 34.7 & 3.3 & 26.0 \\ CLIP/ViT1/L4 (400M) & 72.8 & (52.3, 39.8, 35.7) & (51.8, 29.9, 26.4) & 50.6 & 41.1 & 4.3 & 33.0 \\   

Table 1: Robustness measured by average top-1 accuracy across factors on PUG: ImageNet (We show on the second column the traditional ImageNet validation set accuracy for comparison). Pretraining dataset sizes are indicated in parenthesis with the default being ImageNet-1k. CLIP uses ViT-B32 or ViT-L14. Camera orientation and object pose indicate accuracy along (yaw, pitch, roll) axes.

Figure 5: We present _PUG: ImageNet_, a new photorealistic synthetic dataset with annotated factors of variations as an additional test set for ImageNet pretrained models.

has been shown by Lin et al. (2023) to be mostly solvable without even using the image information at all. Consider that for an image containing a horse eating grass, ARO will propose two captions: _"the horse eating the grass"_ and _"the grass eating the horse"_. The model should predict the correct caption between these two. However, the second caption is impossible, so even without looking at the image, any model can be confident that the first caption is the correct one.

Another shortcoming of current benchmarks is that most of them probe only if the model is correctly able to understand the relations or the attributes between objects. However it is not clear if the failures in finding the correct relations or attributes come from the model not understanding them or come

Figure 6: Setup and zero-shot evaluation of CLIP models on PUG: SPAR with **caption retrieval**. By using synthetic data, we can increase progressively the _difficulty_ of a scene. Our setup is presented in the image above the table in which we show 6 different types of image captioning. 1) caption for background scene recognition for which we have 10 different backgrounds which are easy to distinguish from each other. 2) caption for single animal class prediction, the model should predict the correct categories over the 32 possible animals and 10 backgrounds (for a total of 320 captions). 3) caption for single animal position prediction that increases the number of caption up to 640 and lead to a significant drop in accuracy for every models. 4) caption for two animals class prediction, the model should predict the correct categories of the two animals presented in the images (5120 captions). 5) caption for two animals positions prediction, the model should predict the position of the two animals in the picture (over a total of 10240 captions). 6) caption for two textured animals class prediction, the model should recognize a blue elephant from a red camel. The performances of several VLMs models are presented in the table for which each row corresponds to one of the scenario described previously.

from not understanding which objects are present in the scene. For example, to understand complex relations like: _A photo of an elephant on the left and a camel on the right in a desert background_, the model should first be able to identify whether the background of the picture is an actual desert. Then, the model should identify whether there is an elephant on the picture. It should understand what is _an elephant on the left_. The model could be very effective at identifying individual elephants or camels, but it could unexpectedly fail when a camel and an elephant appear in the same picture. If the model does not fail in recognizing the animals, then we can probe the model to evaluate the position of each of them. We built PUG: SPAR with this goal of having a progressive evaluation scheme in which we can easily determine exactly what are the failure modes of a given VLM. From basic scene understanding to complex relations and attributes, our dataset offer a simple and yet effective way to get a better understanding of the capabilities and limitations of VLMs.

The dataset contains 43,560 images with the associated factors of variations: 10 backgrounds, 32 animals, 4 relations (left/right, bottom/top) and 4 animal texture attributes (blue/red, grass/stone). We have images containing either 1) only the background (for scene recognition) 2) only one animal at different left/right or bottom/top position 3) two animals at different left/right or bottom/top position. And for each of the scenes (either single or multiple animals), we vary the texture of the animals and the background to evaluate the robustness of the model. Our setup and experiments are presented in Figure 6 in which we display some images from the dataset with the corresponding captions used for evaluations. In our benchmark, we used 6 different types of captions to evaluate the following: 1) Scene Recognition (first row) 2) Single animal classification (second row) 3) Single animal position detection (third row) 4) Multiple animals classification (forth row) 5) Multiple animal position detection (fifth row) 6) Multiple animal and textures prediction (sixth row). For each of them, we evaluate the top-1 retrieval accuracy of the correct captions within the set of captions associated to each setup. We evaluate multiple models on these setups: OpenAI CLIP , OpenCLIP , Flava , BLIP , X-VLM  and NegCLIP . Most of the models are correctly able to solve the scene recognition task (which is not surprising since we used only 10 environments which are very different from each other). Concerning the simple object recognition task when using a single animal, the performances across models is highly variable. Our experiments also highlight that the VLM performance in a multiple animals detection setting are much worse than the performance in a single animal detection setting. Those experiments show that despite their successes, VLMs are far from having a good understanding of the world and that improving the robustness of these models is a needed step for real-world robustness.

Inspired by Winoground , we present an experimental setup in which we leverage hard-negative pair of images. Instead of performing caption retrieval within all captions associated to a given setup, we performed caption retrieval between the correct and the _hard negative_ caption. For example, the hard negative caption of _"An elephant on the left of the picture and a camel on the right of the picture"_ will be _"A camel on the left of the picture and an elephant on the right of the picture"_. In addition of switching the relation (left/right and bottom/top), we also provide hard negative captioning for the attributes (blue/red and grass/stone). In Table 2, we present our results using the hard-negative pair. We clearly observe that none of the models are able to predict the correct captions, with many models being close to random performance (50%).

#### 3.4.1 Pug: Ar4t

Lastly, we introduce PUG: AR4T (Attributes and Relations for training). In contrast to PUG: SPAR which is only an evaluation benchmark, PUG: AR4T was created as an additional fine-tuning dataset for VLMs.8 As shown in the previous section, VLMs struggle to understand spatial relations or attributes and thus are good candidates for our fine-tuning scenario. PUG: AR4T contains 249,986 training images with captions and 23,216 test images9. In Table 3, we present CLIP fine-tuning results on the ARO and PUG: SPAR benchmark. We also compare our results against Syn-CLIP, which is CLIP fine-tuned on the SyVIC synthetic dataset. Our results are very similar to Syn-CLIP, but Syn-CLIP training requires several additional tricks to arrive at this performance (Section 3.2 in [Cascante-Bonilla et al., 2023]). On the other hand, the photo-realistic nature of PUG: AR4T enables us to match Syn-CLIP without any of these additional bells and whistles. However, even if we note some improvements on ARO, we are still far from having a model able to understand spatial relations. This is highlighted by the results given on our PUG: SPAR benchmark for which the improvement on single animal position prediction is still only above random chance while there is no improvement on the double animal location prediction task. This confirm the unreliability of the ARO benchmark highlighted by Lin et al. .

## 4 Conclusion

The fine-grained controllability of synthetic rendered image data makes it ideal for designing challenging evaluation benchmarks to better understand the properties and limitations of vision models, as well as for controlled training scenarios - if only it was closer to real data. To this effect, we introduced PUG datasets for representation learning. By leveraging the photorealism of the Unreal Engine[EpicGames], we created 4 new datasets and showcased their utility for robust evaluation. We showed how PUG: Animals could be leveraged for OOD generalization and to study properties of the representation spaces. We developed PUG: ImageNet as a new challenging benchmark that researchers can easily use to assess and compare the robustness of image classifiers. With PUG: SPAR we provide a reliable benchmark for vision-language models while PUG:AR4T offer additional data that could be leveraged to fine-tune VLMs. Together, the PUG family of datasets represents a new standard of photorealism and control for synthetic image data.

   &  &  &  \\   &  VG-Relation \\ (Micro-Accuracy) \\  &  VG-Attention \\ (Micro-Accuracy) \\  &  COCO-Order \\ (Precision@1) \\  &  Fickr-Web \\ (Precision@1) \\  &  Average \\ (Precision@1) \\  & 
 Single \\ (Precision@1) \\  \\ 
**CLIP-VIT-W32 (400M)** & 59.16 15.850 & 62.18 161.52 & 47.96 & 59.98 & 57.09 & 57.09 & 49.84 & 54.42 \\
* _+FT/Syn-CLIP_ & **71.40** & **66.94** & 59.06 & 70.96 & 67.09 (+0.77) & N/A & N/A \\
* _+FT/FPG-ART (2000M)_ & 68.36 175.18 & 65.54 164.44 & 57.80 & 69.74 & 65.36 (+0.86) & 50.78(+0.94) & 54.23(+0.19) \\
* _+FT/FPG-ART (13M)_ & 71.01 176.57 & 65.51 164.32 & **61.07** & **72.84** & **67.52** (+0.3) & 50.16(+0.32) & 54.29(+0.23) \\  

Table 3: Fine-tuning CLIP on PUG: AR4T. For VG-Relation and Attribution, the results (Acc1 | Acc2) indicate macro-accuracy across all relations and attributes (Acc1), and macro-accuracy on the subset of relations and attributes present in both ARO and PUG (Acc2). For PUG: SPAR, we evaluate on images in which there is only one animal (Single) or two animals (Double) with the relation being left or right. We were not able to run SynCLIP on PUG: SPAR because the model was not public at the time of the publication.