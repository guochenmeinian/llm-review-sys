# Thinker: Learning to Plan and Act

Stephen Chung

University of Cambridge

mhc48@cam.ac.uk &Ivan Anokhin

Mila, Universite de Montreal

ivan.anokhin@mila.quebec &David Krueger

University of Cambridge

dsk30@cam.ac.uk

###### Abstract

We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for handcrafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. Thinker is the first work showing that an RL agent can learn to plan with a learned world model in complex environments.

## 1 Introduction

Model-based reinforcement learning (RL) has significantly enhanced sample efficiency and performance by employing world models, or simply models, to generate additional training data [1; 2], provide better estimates of the gradient [3; 4], and facilitate planning . Here, _planning_ refers to the process of interacting with the world model to inform the subsequent selection of actions. Handcrafted planning algorithms, such as Monte Carlo Tree Search (MCTS) , have achieved remarkable success in recent years [7; 8; 5], underscoring the importance of planning.

However, a significant research gap persists in creating methods that enable an RL agent to _learn to plan_, meaning to interact autonomously with a model, eliminating the need for handcrafted planning algorithms. We suggest this shortfall stems from the inherent complexities of mastering several skills concurrently required by planning: searching (exploring new potential plans), evaluating (assessing plan quality), summarizing (contrasting different plans), and executing (implementing the optimal plan). While a handcrafted planning algorithm can handle these tasks, they pose a significant learning challenge for an RL agent that learns to plan. Introducing a learned world model further complicates the issue, as predictions from the model might not be accurate.

To address these challenges, we introduce the _Thinker_ algorithm, a novel approach that enables the agent to interact with a learned model as an integral part of the environment. The Thinker algorithm wraps a Markov Decision Process (MDP) with a learned model and introduces a new set of actions that enable agents to interact with the model.1 The manner in which an agent uses a model is not predetermined. In principle, an agent can learn various common planning algorithms, such as \(n\)-step exhaustive search or MCTS, or ignore the model if planning is not beneficial. Importantly, wehave designed the augmented MDP in a way that simplifies the processes of searching, evaluating, summarizing, and executing, making it easier for agents to learn to use the model.

Drawing from neuroscience, our work is inspired by the influential hypothesis that the brain conducts planning via _internalized actions_--actions processed within our experience-based internal world model rather than externally [11; 12]. According to this hypothesis, both model-free and model-based behaviors utilize a similar neural mechanism. The main distinction lies in whether actions are directed toward the external world or the internal world model. This perspective is consistent with the structural similarities observed in brain regions governing model-free and model-based behaviors . This hypothesis offers insights into how an RL agent might _learn to plan_--by unifying both imaginary and real actions and leveraging existing RL algorithms.

Experimental results show that actor-critic algorithms, when applied to the Thinker-augmented MDP, yield state-of-the-art performance in Sokoban. Notably, this approach attains a solving rate of 94.5% within 5e7 frames, a significant improvement over the 56.7% solving rate achieved when the same algorithm is applied to the raw MDP. On the Atari 2600 benchmark, actor-critic algorithms using the Thinker-augmented MDP show a significant improvement over those using the raw MDP. Visualization results, as shown in Fig 1, reveal the agent's effective use of the model. To summarize, the Thinker algorithm provides the following advantages:

* **Flexibility:** The agent learns to plan on its own, without any handcrafted planning algorithm, allowing it to adapt to different states, environments, and models.
* **Generality:** The Thinker algorithm only dictates how an MDP is transformed, making it compatible with any RL algorithm. Specifically, one can convert any model-free RL algorithm into a model-based RL algorithm by switching to the Thinker-augmented MDP.
* **Interpretability:** We can visualize the agent's plan prior to its execution, as depicted in Fig 1. This provides greater insight compared to scenarios where the neural network internalizes the model and plans in a black box fashion .
* **Aligned objective:** Both the real and imaginary actions are trained using the same rewards from the environment, ensuring that the objectives of planning and acting align.
* **Improved learned model:** We introduce a novel combination of architecture and feature loss for the model, which is designed to prioritize the learning of task-relevant features and enable visualization at the same time.

Figure 1: Illustration of a trained agent during a single stage in Sokoban, where the goal is to push all four boxes to red-bordered squares. From the root state, the agent generates four rollouts using the learned model. Except for the two real states, all frames are generated by the model, showcasing near-perfect frame prediction. Predicted rollout return \(g\) (equivalent to \(g_{}\) in Equation 1) is displayed. The four rollouts are reasonable plans with agents pushing different boxes towards targets. The agent follows the third rollout for the next five real steps (not fully shown in the figure), likely due to its high rollout return. Crucially, plan generation, plan execution, and the model are all _learned_. Videos of the whole game and other Atari games are available at https://youtu.be/0AfZh5SR7Fk.

Related Work

Various approaches exist for incorporating planning into RL algorithms, with the majority relying on handcrafted planning algorithms. In other words, the policy underlying the interaction with the world model, termed here as the _imaginary policy_, is typically handcrafted. For example, MuZero , AlphaGo , and AlphaZero  employ MCTS. VPN , TreeQN and ATreeC  explore all possible action sequences up to a predefined depth. I2A  generates one rollout per action, with the first action iterating over the action set and subsequent actions following the current policy.

Learning to plan is a challenging domain, and a few works exist in this area. MCTSnets  is a supervised learning algorithm that learns to imitate actions from expert trajectories generated from MCTS. VIN  and DRC  are model-free approaches that utilize a special neural network architecture. VIN's architecture aims to facilitate value iteration, while DRC's architecture aims to facilitate general planning. IBP , which is closely related to our work, allows the agent to engage with both the model and the environment. However, several critical differences distinguish IBP from Thinker. First, in IBP, the imaginary policy is trained by backpropagating through the model, a method that is primarily suitable for continuous action sets, whereas our approach focuses on environments with discrete action sets and treats the model as a black-box tool external to the agent. Second, unlike our method, IBP does not predict future values and policies, which we have identified as crucial for complex environments (see Appendix E). Third, we have designed an augmented state representation that significantly simplifies the process of using the model by providing more than just the predicted states to the agent, as is the case with IBP.

Another research direction employs gradient updates to refine rollouts [22; 23; 24] prior to taking each real action. These methods start with a preliminary imaginary policy and use this policy to gather rollouts from the model. Subsequently, these rollouts are used to calculate gradient updates for the imaginary policy by maximizing the imaginary rewards, thus incrementally refining the rollouts' quality. In the final step, a handcrafted function is used to select a real action. For instance, previous works such as , , and  have proposed to maximize the rollout return using REINFORCE , PPO , and backpropagation through the model, respectively. In contrast to these works, Thinker does not employ such gradient updates to refine rollouts, as the agent has to learn how to refine rollouts and select a real action on its own. Moreover, unlike these works, Thinker trains the imaginary policy to maximize the real rewards, thus preventing model exploitation when the model is learned.

Thinker is the first work showing that an RL agent _can learn to plan with a learned world model in complex environments_. Prior related works either do not satisfy (i) learning to plan, i.e., a learned imaginary policy, (ii) using a learned model or (iii) being evaluated in a complex environment. For example, MuZero , VPN , TreeQN , and I2A  do not satisfy (i). VIN , DRC  and [22; 24] do not satisfy (ii). IBP  and  do not satisfy (iii).

We describe the connection between our algorithm and Meta-RL [27; 28], as well as generalized policy iteration [29; 30], in Appendix H.

## 3 Background and Notation

We consider a Markov Decision Process (MDP) defined by a tuple \((,,P,R,,d_{0})\), where \(\) is a set of states, \(\) is a finite set of actions, \(P:\) is a transition function representing the dynamics of the environment, \(R:\) is a reward function, \(\) is a discount factor, and \(d_{0}:\) is an initial state distribution. Denoting the state, action, and reward at time \(t\) by \(s_{t}\), \(a_{t}\), and \(r_{t}\) respectively, \(P(s,a,s^{})=(s_{t+1}=s^{}|s_{t}=s,a_{t}=a)\), \(R(s,a)=[r_{t+1}|s_{t}=s,a_{t}=a]\), and \(d_{0}(s)=(s_{1}=s)\), where \(P\) and \(d_{0}\) are valid probability mass functions. An episode is a sequence of \((s_{t},a_{t},r_{t+1})\), starting from \(t=1\) and continuing until reaching the terminal state, a special state where the environment ends. Letting \(g_{t}=_{k=t}^{}^{k-t}r_{k}\) denote the infinite-horizon discounted return accret after acting at time \(t\), we are interested in finding, or approximating, a _policy_\(:\), such that for any time \(t 1\), selecting actions according to \((s,a)=(a_{t}=a|s_{t}=s)\) maximizes the expected return \([g_{t+1}|]\). The value function for policy \(\) is \(v^{}\) where for all \(s\), \(v^{}(s)=[g_{t+1}|s_{t}=s,]\). We adopt the notation \((s_{t},a_{t},r_{t+1},s_{t+1})\) for representing transitions, as opposed to \((s_{t},a_{t},r_{t},s_{t+1})\), which facilitates a clearer description of the algorithm. In this work, we only consider an MDP with a discrete action set.

## 4 Algorithm

The Thinker algorithm transforms a given MDP, denoted as \(=(,,P,R,,d_{0})\), into an augmented MDP, denoted as \(^{}=(^{},^{},P^ {},R^{},^{},d_{0}^{})\). We call \(\) the real MDP, and \(^{}\) the augmented MDP. Besides the real MDP, we assume that we are given (i) a model in the form of \((,)\) that is an approximate version of \((P,R)\) and is deterministic, and (ii) a _base policy_\(:(,)\) and its value function \(:\). The base policy can be any policy, which helps to simplify the search and evaluation of rollouts for the agent. We will show how both the model and base policy can be learned from scratch in Section 4.4, but for now we assume that both are given.

### Augmented Transitions

To facilitate interaction with the model, we add \(K-1\) steps before each step in the real MDP, where \(K 1\) and denotes the _stage length_. We typically use \(K=20\) in the experiments. The first \(K-1\) steps are called imaginary steps, as actions in these steps are passed to the model instead of the real MDP. The step that follows the imaginary step is called the real step, as the action selected is passed to the real MDP. These \(K\) steps in the augmented MDP, including \(K-1\) imaginary steps and one real step, constitute a single _stage_ in the augmented MDP. An illustration of a stage is shown in Fig 2. Each episode in the augmented MDP is composed of multiple stages, with a single stage corresponding to one step in the real MDP. The augmented MDP terminates if and only if the underlying real MDP terminates. We use \(k\{1,2,...,K\}\) to denote the augmented step within a stage, and \(t\{1,2,...\}\) to denote the current stage. We omit the subscript \(t\) where it is not pertinent.

Our next step is to determine the form of the augmented transition. We can view the model search as tree traversal, with the tree's root node corresponding to the current real state \(s\). We consider a simple method of traversing in a tree. At each imaginary step, the agent decides which child node to visit by imaginary action \(\) and also whether to reset the model to the root node by reset action \(^{r}\{0,1\}\). In other words, the imaginary action unrolls the model by one step while the reset action sets the model back to the root node. We also impose a maximum search depth, \(L\), in the algorithm. This ensures that the agent is forced to reset once its search depth exceeds \(L\), as a learned model may not be accurate for a large search depth. An illustration of the tree traversal is shown in Figure 3.

After the imaginary steps, the agent chooses a real action \(\) in a real step, where this real action is passed to the real MDP. We can merge both the imaginary and real actions, while interpreting the reset action as an additional one, thereby defining the augmented action space by \(^{}:=\{0,1\}\), and an augmented action by the tuple \(a^{}:=(,^{r})\). During imaginary steps, \(\) becomes the imaginary action, while \(^{r}\) decides whether to reset the current node. During real steps, \(\) becomes the real action, while \(^{r}\) is not used. The pseudocode can be found in Algorithm 1.

### Augmented State

We then consider the design of the augmented state that is passed to the agent. One straightforward method involves concatenating all predicted states in the tree. At each real step, the \(K-1\) predicted

Figure 2: Illustration of a stage in the augmented MDP, which is composed of \(K-1\) imaginary steps and one real step. The environment outputs a state \(s_{t}\) that sets the root state of the world model. The agent interacts with the world model \(K-1\) times with augmented actions. In each imaginary step, the agent receives an augmented state \(s_{t,k}^{}\), which contains the latest outputs from the world model. Finally, the agent acts in the real environment for a single step based on \(s_{t,K}^{}\).

states serve as input, informing the selection of the real action. However, this method presents two significant challenges: (i) the high dimensionality of the predicted states makes learning to search and evaluate rollouts difficult, and (ii) the concatenation effectively multiplies the state's dimension by \(K\), further complicating the learning of rollout summarization. To mitigate these issues, we propose a compact way of representing the tree in a low-dimensional vector. First, we consider how to encode a node compactly, followed by how to use these node representations to represent the tree.

To encode a node, we include the reward, the base policy and its value at the node. Moreover, we provide certain _hints_ that summarize the rollouts. Taking inspiration from MCTS, we consider three hints: mean rollout return, maximum rollout return, and visit count.

For a node \(i_{0}\) and its \(n\)-level descendant node \(i_{n}\), we define the _rollout return_ from node \(i_{0}\) to node \(i_{n}\) through the path \((i_{0},i_{1},i_{2},...,i_{n})\) as:

\[g_{i_{0},i_{n}}:=_{i_{0}}+_{i_{1}}+^{2}_{i_{2 }}+...^{n}_{i_{n}}+^{n+1}_{i_{n}},\] (1)

where \(_{i}\) denotes the value function applied on the node \(i\) and \(_{i}\) denotes the predicted reward upon reaching node \(i\). That is, \(_{i}:=(_{i})\), and \(_{i}:=(_{(i)},}_{ {parent\_edge}(i)})\).

For a given node \(i\), we can compute \(g_{i,j}\) for all its descendant nodes \(j\) that are visited in the tree. We also define \(g_{i,i}:=_{i}+_{i}\). We define the mean rollout return \(g_{i}^{}\) and the maximum rollout return \(g_{i}^{}\) as the mean of \(g_{i,j}\) and the maximum of \(g_{i,j}\) over all descendant nodes \(j\) (including itself) that are visited respectively. The mean and maximum rollout returns of a node's children are useful in guiding actions. For example, the mean rollout return of the child \(i\) of a node \(k\) gives a rough estimate of the average estimated return that can be collected following action \(i\) from node \(k\). The mean rollout return is also equivalent to \(Q(s,a)\) used in the UCB formula of MCTS. We use \(g_{(i)}^{}\) and \(g_{(i)}^{}\) to denote the vector of mean and maximum rollout returns for each child of a node \(i\).

The visit count of a node, which represents the number of times the agent traverses that node, may also be useful. For example, a child node that has never been visited may be worth exploring more. We use \(n_{(i)}^{||}\) to denote the vector of visit count for each possible child of a node \(i\).

Altogether, we represent a node \(i\) by a vector \(u_{i}^{5||+2}\), called _node representation_, that contains:

1. \(_{i},_{i}\), \(_{i}^{||}\): The core statistics, composed of the reward, the value, and the base policy of the node \(i\). Here, \(_{i}:=((s_{i},a_{1}),(s_{i},a_{2}),...,(s_ {i},a_{||}))\).
2. one_hot(\(a_{i}\)) \(^{||}\): The one-hot encoded action that leads to node \(i\).
3. \(g_{(i)}^{},g_{(i)}^{}\), \(n_{(i)}^{||}\): The hints, composed of mean rollout return, maximum rollout return and visit counts of the child nodes normalized by \(K\).

We only include the root node's representation \(u_{}\) and the current node's representation \(u_{}\) to represent the tree (we use the subscript \(\) to denote the root node, which is unchanged throughout a

Figure 3: Illustration of tree traversal in imaginary steps, where the stage length \(K=6\) and the maximum search depth \(L=2\). Two actions, \(a\) and \(b\), are available. The root node corresponds to the real state \(s\) underlying the stage, and the child nodes correspond to the predicted states \(_{a_{1},a_{2},}\) following the selection of actions \(a_{1},a_{2},\) from state \(s\). The augmented actions in the \(K-1=5\) imaginary steps are depicted beneath the tree. The first action unrolls the model, whereas the second action determines whether the current node is reset to the root node. The yellow and the orange nodes indicate the current node before and after unrolling (before reset is applied), respectively. A forced reset is triggered on the \(3^{}\) imaginary step, as the maximum search depth has been reached.

stage, and the subscript \({}_{}\) to denote the current node _before reset_). The former guides the selection of real actions, while the latter guides the selection of imaginary actions. Besides \(u_{}\) and \(u_{}\), we also add auxiliary statistics, such as the current step in a stage, in the representation. Further details can be found in Appendix A. Combined, we represent the tree with a fixed-size vector \(u_{}^{10||+K+9}\), called _tree representation_. We may also include the real state \(s\) or the current node's predicted state \(_{}\) in the augmented state, providing a richer context for the agent.

The tree representation is designed to simplify various planning procedures. In searching, the base policy and visit count help the identification of promising, yet unvisited rollouts. In evaluation, rather than simulating until a terminal state, value estimates can be utilized to assess the potential of a given state. In summarization, mean and maximum rollout returns consolidate the outcomes for the current node. Finally, in execution, the hints at the root node provide a compact summary of all rollouts. Compared to the concatenated predicted states, which have a dimension of \(K||\), the tree representation, typically with fewer than 100 dimensions, is much easier to learn from.

The tree representation enables an agent to execute various common planning algorithms, such as \(n\)-step exhaustive search and MCTS, without requiring memory. Employing an agent with memory, such as an actor-critic algorithm with an RNN, may facilitate learning a broader class of planning algorithms, as the agent can access previous node representations. In principle, an agent with memory can learn to compute the hints or other ways of summarizing rollouts by itself.

### Augmented Reward

To align the agent's goals in the augmented MDP with those in the real MDP, we adopt the following augmented reward function: no reward is given for imaginary steps, and the reward for real steps is identical to the real reward from the real MDP. In other words, \(r_{k}^{}=0\) for \(k K\) and \(r_{K+1}^{}=r\), where \(r\) denotes the real reward. We set the augmented discount rate \(^{}:=\) to account for the \(K-1\) imaginary steps preceding one real step, ensuring that the return in the augmented MDP equals the return in the real MDP.

We also experimented with the use of an auxiliary reward, called the _planning reward_, to mitigate the sparse reward in the augmented MDP. The core idea is to encourage the agent to maximize the maximum rollout return \(g_{}^{}\), but we later found that it only provided a minimal increase in initial learning speed, indicating that the real rewards are sufficient for learning to plan. The details of the planning reward can be found in Appendix G.

This completes the description of the augmented MDP. One may view an RL agent acting on this augmented MDP with a fixed base policy as performing one step of policy improvement in generalized policy iteration, but the policy improvement operator is learned instead of handcrafted. Experiments show that RL agents can indeed learn to improve the base policy (see Appendix H). As such, we only need to project the improved base policy back to the base policy space to complete the full cycle of generalized policy iteration, which will be discussed next.

### Learning the World Model and the Base Policy

Until now, we assume that we are provided (i) the model (\(,\)) and (ii) a base policy \(\) and its value \(\). To remove both assumptions, we propose employing a single, unified model that assumes the roles of \(,,\), and \(\), learning directly by fitting the real transitions. Specifically, we introduce a novel model, the _dual network_, designed for this purpose.

The dual network consists of two RNNs which we refer to as the _state-reward network_ and the _value-policy network_. The state-reward network's input consists of the root state \(s_{t}\) and an action sequence \(a_{t+1:t+L}\), and the network predicts future states \(_{t+1:t+L}\), rewards \(_{t+1:t+L}\), and termination probability \(_{t+1:t+L}^{}\). Meanwhile, the value-policy network takes both the state-reward network's input and its predicted states \(_{t+1:t+L}\) as inputs, and the network predicts future policies \(_{t:t+L}\) and values \(_{t:t+L}\). We can also view the dual network as an RNN with \(h=(h^{sr},h^{vp})\) being the hidden state, where \(h^{sr}\) and \(h^{vp}\) denote the hidden states of the two sub-RNNs. The relevant statistics required in the augmented state can be computed by unrolling the dual network using the root node and the action sequence leading to the current node.

To train the model, we first store all real transitions \((s_{t},a_{t},r_{t+1},d_{t+1})\) in a replay buffer, which corresponds to the state, action, reward, and termination indicator. We then sample a sequence of real transitions with length \(L+1\) from the buffer. The two sub-RNNs in the dual network are trained separately. For the state-reward network, the following loss is used:

\[^{sr}:=_{l=1}^{L}(c^{r}(_{t+l}-r_{t+l})^{2}+c^{d}d_{ t+1}_{t+l}^{d}+c^{s}^{}(_{t+l},s_{t+l}) ),\] (2)

where \(c^{r},c^{d},c^{s} 0\) are hyperparameters that modulate the loss strength. While a straightforward choice for the state prediction loss metric, \(^{}\), is the L2 loss - defined as \(^{}(s^{},s)=||s^{}-s||_{2}^{2}-\) this approach could cause the model to allocate capacity towards predicting non-essential features. To address this issue, we suggest using the feature loss instead of the L2 loss. Specifically, \(^{}(s^{},s)=||g(s^{})-g(s)||_{2}^{2}\), where \(g\) represents the encoding function of the value-policy network. As this encoding function concentrates on features relevant to action, the feature loss encourages the model to prioritize predicting the most crucial features.

For the value-policy network, the following loss is used:

\[^{vp}:=_{l=0}^{L}(c^{v}(_{t+l}-_{t+l}^{ {target}})^{2}+c^{}(a_{t+l})^{T}_{t+l}),\] (3)

where \(c^{v},c^{} 0\) are hyperparameters and \(_{t+l}^{}\) denotes the multi-step return. One may also use the real action distribution \(_{t}\) place of one_hot\((a_{t+l})\) for a better learning signal, though this requires passing the real action distribution instead of only the actions to the augmented MDP.

The design choice of the dual network aims to improve data efficiency while also ensuring inaccurately predicted states do not negatively affect the learning of the base policy and values. If the predicted states are inaccurate, the value-policy network, being an RNN, can learn to ignore these states, relying solely on the root state and action sequence. Conversely, if the predicted states are accurate, the value-policy network can effectively learn to make predictions based on these states. Consider, for instance, the task of predicting the value after executing several actions in Sokoban. If the predicted state suggests the level is close to completion, the predicted value should naturally be higher. Although, in theory, a standalone RNN might autonomously learn this intermediary step, in practice, it would likely require much more data. We explore the relationship of this dual network with models from other model-based RL algorithms in Appendix I.

With the dual network, the augmented state can also include the hidden state of the model at either the current node, \(h_{}\), or the root node, \(h_{}\). These hidden states, usually in much lower dimensions, might be easier to process than the real state \(s\) or the current node's predicted state \(_{}\). In our experiments, we use \((u_{},h_{})\) as the augmented state, based on the intuition that the current hidden state is likely the most informative among the others.2 This completes the specification of the Thinker algorithm, which takes an MDP as input and transforms it into another MDP.

Figure 4: Illustration of the dual networkâ€™s architecture, composed of the state-reward and the value-policy network. See Equations 2 and 3 for the loss functions and Appendix B for training details.

## 5 Experiments

For all our experiments, we train a standard actor-critic algorithm, specifically the IMPALA algorithm , on the Thinker-augmented MDP. Although other RL algorithms could be employed, we opted for the IMPALA due to its computational efficiency and simplicity. The actor-critic's network uses an RNN for encoding the tree representation and a convolutional neural network for encoding the model's hidden state. These encoded representations are concatenated, then passed through a linear layer to generate actions and predicted values. We set the stage length, \(K\), to \(20\), and the maximum search depth or model unroll length, \(L\), to \(5\). More experiment details can be found in Appendix D.

### Sokoban

We selected the game of Sokoban [32; 18], a classic puzzle problem, as our primary testing environment due to its inherent complexity and requirement for extensive planning. In Sokoban, the objective is to push all four boxes onto the designated red target squares, as depicted in Figure 1. We used the unfiltered dataset comprising 900,000 Sokoban levels from . We train our algorithm for 5e7 frames, while baselines in the literature typically train for at least 5e8 frames. Despite this discrepancy, we present learning curves plotted against frame count for direct comparison.

The learning curve for the actor-critic algorithm applied to the Thinker-augmented MDP is depicted in Fig. 5, with results averaged across five seeds. We include seven baselines: DRC , Dreamer-v3 , MuZero , I2A , ATreeC , VIN , and IMPALA with ResNet . In Sokoban, DRC stands as the current state-of-the-art algorithm, outperforming others by a significant margin, likely due to its well-suited prior for the game. The 'DRC (original paper)' result is sourced directly from the paper, whereas 'DRC' denotes our replicated results.

Thinker surpasses the other baselines in terms of performance in the first 5e7 frames. At 2e7 frames, Thinker solves 88% of the levels, while DRC (original paper) solves 80% and the other baselines solve at most 21%. At 5e7 frames, Thinker solves 95 % of the levels, while DRC (original paper) solves 93% and the other baselines solve at most 45%. These results underscore the enhanced planning capabilities afforded by the Thinker-augmented MDP to an RL agent.

To understand the benefits of planning, we evaluate a policy trained with \(K=20\) on the Thinker-Augmented MDP, by testing its performance with different \(K\) values in the same augmented environment during the testing phase. This variability allows us to control the degree of planning and the result is shown in Figure 6. The figure also depicts the performance of the base policy, which can be regarded as a distilled policy devoid of planning. We observe a significant performance improvement attributable to planning, even at the end of training.

The agent's behavior, visualized in Figure 1, illustrates that it learns to use the model for selecting better actions. Interestingly, it appears that the agents learn a planning algorithm that diverges from traditional \(n\)-step-exhaustive search and MCTS. For instance, the agent chooses real actions based on

Figure 5: Running average solving rate over the last 200 episodes in Sokoban, compared with other baselines.

both visit counts and rollout returns, and the agent learns to reset upon entering an irreversible state, contrasting the MCTS strategy of resetting at a leaf node. Further analysis on the trained agent's behavior can be found in Appendix F.

We conducted an ablation analysis to examine the impact of planning duration, varying the stage length \(K\) across \(\{1,2,5,10,20\}\) during both training and testing. When \(K=1\), the agent does not interact with the model at all. The outcomes of this ablation analysis are presented in Figure 7. The results suggest that \(K=10\) already gives the optimal performance. This is in stark contrast to the 2000 simulations required by MCTS to achieve good performance in Sokoban .

As a comparison, we train the same actor-critic algorithm on the raw MDP, employing a similar network architecture used by the model to encode the raw state. Surprisingly, the result for \(K=1\) is worse than the raw MDP. A detailed explanation of the reasons behind this phenomenon can be found in Appendix D. However, employing a marginally larger \(K\), such as 2, greatly improves the performance on the Thinker-augmented MDP, surpassing that of the raw MDP.

We use the dual network as the world model, trained with feature loss in state prediction. We consider several variants of the model, including (i) a dual network that uses L2 loss in state prediction, (ii) a dual network that has no loss in state prediction, (iii) a single network that uses L2 loss in state prediction, (iv) a single network that does not predict state. The single network refers to a single RNN that predicts the relevant quantities, and (iv) is equivalent to MuZero's model. The results of these four variants are shown in Figure 8. We observe that both the dual network architecture and the state loss are critical to performance. However, the use of feature loss instead of L2 loss gives marginal performance improvement, likely because the L2 loss' prior that each pixel is equally important suits well in Sokoban. Nonetheless, this prior may not be suitable for other games like Breakout.

Figure 8: Running average solving rate over the last 200 episodes in Sokoban with different models.

Further ablation studies in Appendix E offer insights into the various components of Thinker, including the role of hints in tree representation, the agent's memory, base policy, planning reward, etc. For example, either the hints _or_ an RNN agent is sufficient to achieve good performance, as the hints summarize the rollouts, and an agent with memory can learn to summarize rollouts on its own. However, if both hints and memory are omitted, the agent fails to learn to plan, as it cannot recall past rollouts. In addition, if the base policy and its value are also omitted, then an RNN agent still fails to learn, suggesting the necessity of a heuristic in learning to plan.

### Atari 2600

Finally, we test our algorithm on the Atari 2600 benchmark  using a 200M frames setting. The performance of the actor-critic algorithm on both the Thinker-augmented MDP and raw MDP is evaluated using the same hyperparameters as in the Sokoban experiment, but with a deeper neural network for the model and an increased discount rate from 0.97 to 0.99.

The learning curve, measured in the median human-normalized score across 57 games, is displayed in Figure 9. We evaluate the agent in the no-ops starts regime at the end of the training, and the learning curve is shown in Fig 9. The median (mean) normalized score for the actor-critic algorithm applied on the Thinker-augmented MDP and the raw MDP are 261% (1372%) and 102% (514%), respectively, underscoring the advantages of the Thinker-augmented MDP over the raw MDP. For context, the median (mean) normalized score of Rainbow , a robust baseline, is 223% (868%).

The advantages of the Thinker algorithm are particularly pronounced in certain types of games, especially shooting games such as chopper command, seaquest, and phoenix. It is plausible that the agents benefit from predicting the trajectories of fast-moving objects in these shooting games. However, in some games, particularly those where a single action does not produce significant effects, the Thinker algorithm appears to offer no advantages. For instance, in the game qbert, the decision to move to a different box is made only once every five or more actions. This effectively transforms any 5-step plan into a single-step plan, diminishing the algorithm's benefits. Detailed experiment results on the Atari 2600 are available in Appendix D.

Many interesting behaviors learned by the agent are visualized in https://youtu.be/OAfZh5SR7Fk. We also observe that the predicted frames in the video are of high quality, showing that the feature loss can lead to the emergence of interpretable and visualizable predicted frames, despite the absence of an explicit loss in the state space. We attribute this capability to using a convolutional mapping \(g\) in the feature loss, as convolutions can preserve local features.

## 6 Future Work and Conclusion

While the Thinker algorithm offers many advantages, it also presents several limitations that provide potential areas for future research. Firstly, the Thinker algorithm carries a large computational cost. A single step in the original MDP corresponds to \(K\) steps in the augmented MDP, in addition to the overhead of model training. Secondly, the algorithm currently enforces rigid planning, requiring the agent to roll out from the root state and restricts it to planning for a fixed number of steps. Thirdly, the algorithm employs a deterministic model to facilitate tree traversal, which may not work well for a stochastic environment.

Future work could focus on addressing these limitations, such as enabling more flexible planning steps and integrating uncertainty into the model. Additionally, exploring the application of the algorithm in multi-environment settings is a promising direction. We hypothesize that the advantages of a learned planning algorithm over a handcrafted one will become more pronounced in such scenarios. Exploring how other RL algorithms perform within the Thinker-augmented MDP is an additional direction for future work.

The history of machine learning research tells us that learned approaches often prevail over handcrafted ones. This transition becomes especially pronounced when large amounts of data and computational power are available. In the same vein, we surmise that learned planning algorithms will eventually surpass handcrafted planning algorithms in the future.