# Toward Information Theoretic Active Inverse Reinforcement Learning

Ondrej Bajgar

University of Oxford

&Dewi Sid William Gould

Alan Turing Institute

&Jonathon Liu

Alan Turing Institute

&Oliver Newcombe

Independent

&Oliver Newcombe

University of Oxford

&Rohan Narayan Langford Mitta

Independent

&Jack Golden

University of Oxford

###### Abstract

As AI systems become increasingly autonomous, aligning their decision-making to human preferences is essential. In domains like autonomous driving or robotics, it is impossible to write down the reward function representing these preferences by hand. Inverse reinforcement learning (IRL) offers a promising approach to infer the unknown reward from demonstrations. However, obtaining human demonstrations can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration, reducing the amount of required human effort. Where most prior work allowed querying the human for an action at one state at a time, we motivate and analyse scenarios where we collect longer trajectories. We provide an information-theoretic acquisition function, propose an efficient approximation scheme, and illustrate its performance through a set of gridworld experiments as groundwork for future work expanding to more general settings.

## 1 Introduction

Stuart Russell suggested three principles for the development of beneficial artificial intelligence: its only objective is realizing human preferences, it is initially uncertain about these preferences, and its ultimate source of information about them is human behavior . _Apprenticeship learning_ via Bayesian _inverse reinforcement learning_ (IRL) can be understood as a possible operationalization of these principles: Bayesian IRL starts with a prior distribution over reward functions representing initial uncertainty about human preferences. It then combines this prior with _demonstration_ data from a human expert acting approximately optimally with respect to the unknown reward, to produce a posterior distribution over rewards. In apprenticeship learning, this posterior over rewards is then used to produce a policy that should perform well with respect to the unknown reward function.

However, getting human demonstrations requires scarce human time. Also, many risky situations where we would wish AI systems to behave especially reliably may be rare in these demonstration data. Bayesian active learning can help with both by giving queries to a human demonstrator that are likely to bring the most information about the reward. Most prior methods for active IRL  queried the expert for action annotations of particular isolated states. However, in domains such as autonomous driving with a high frequency of actions, it can be much more natural for the human to provide whole trajectories - say, to drive for a while in a simulator - than to annotate a large collection of unrelated snapshots. There is one previous paper on _active IRL with full trajectories_ suggesting a heuristic acquisition function whose shortcomings can, however, completely prevent learning. We instead suggest using the principled tools of Bayesian active learning for the task.

The article provides the following contributions: we formulate the problem of active IRL with full expert trajectories and adapt the expected information gain (EIG) acquisition function to this setting. We then provide an algorithm approximating the EIG and present experiments showing its superior performance relative to random sampling and two other baselines in gridworlds. We consider this initial investigation in tabular settings a stepping stone toward algorithms for more general settings.

## 2 Task formulation

Let \(()=(,,p_{},r,,t_{},_{})\) be a parameterized Markov decision process (MDP), where \(\) and \(\) are finite state and action spaces respectively, \(p_{}:()\) is the transition function where \(()\) is a set of probability measures over \(\), \(r:\) is an (expected) reward function,1\((0,1)\) is a discount rate, \(t_{}\{\}\) is the time horizon, and \(_{}\) is the initial state distribution. The parameter \(\) will be used to set up the environment in active learning. Due to space limitations here, we present experiments where \(=s_{0}\) deterministically chooses an initial state, but our method can be used also for choosing the transition dynamics.

We assume we are initially uncertain about the reward \(r\), and our initial knowledge is captured by a prior distribution \(p(r)\) over rewards, which is a distribution over \(^{||||}\) - a space of vectors representing the reward associated with each state-action pair. We also have access to an expert that, given an instance \((_{i})\) of the MDP, can produce a trajectory \(_{i}=(s_{0}^{i},a_{0}^{i}),,(s_{n_{i}}^{i},a_{n_{i}}^{i})\), where \(s_{0}^{i}_{_{i}}\), \(s_{t+1} p_{_{i}}(|s_{t},a_{t})\), and

\[_{E}^{}(a_{t}|s_{t})=^{*}(s_{t},a_{t}))}{_{ a^{}}( Q_{}^{*}(s_{t},a^{}))}\,\] (1)

which is called a _Boltzmann-rational_ policy, given the optimal Q function \(Q_{_{i}}^{*}\) and a hyperparameter \(\) expressing how close to optimal the expert behaviour is (where \(=0\) corresponds to fully random behaviour and \(+\) would yield the optimal policy).

Figure 1: (a) shows an illustrative gridworld and its true rewards. The lower left corner has a ”jail” state with negative reward from which an agent cannot leave. The starred green state is the terminal ”goal” state with a large positive reward. The brown, blue, and red states are ”mud”, ”water”, and ”lava” type states respectively, whose rewards are unknown to the IRL agent. The IRL agent tries to learn the rewards of these three state types from expert demonstrations. (b) shows the learned distributions over the rewards of the ”mud”, ”water”, and ”lava” state types respectively, at some particular step of the active learning process. These learned reward distributions are used to calculate the EIG of obtaining another expert demonstration starting from each given state, shown in (c). In this case, a demonstration starting in the bottom right state gives the most information about the unknown reward parameters.

The task of _Bayesian active inverse reinforcement learning_ is to sequentially query the expert to provide demonstrations in environments \(_{1},,_{N}\) to gain maximum information about the unknown reward. We start with a (possibly empty) set of expert trajectories \(_{0}\) and then, at each step of active learning, we choose a parameter \(_{i}\) for the MDP, from which we get the corresponding expert trajectory \(_{i}\). We then update our demonstration dataset to \(_{i}=_{i-1}_{i}\), and the distribution over rewards to \(p(r|_{i})\), which we again use to select the most informative environment setup \(_{i+1}\) in the next step. We repeat until we exhaust our limited demonstration budget \(N\).

Our goal can be operationalized as minimizing the entropy of the posterior distribution over rewards, once all expert demonstrations have been observed. This is equivalent to maximizing the log likelihood of the true parameter value in expectation, or to maximizing the mutual information between the demonstrations and the reward. We call this the _information-theoretic objective_.

For the _apprenticeship-learning objective_, we use the final posterior \(p(r|_{N})\) to produce an _apprentice policy_\(^{A}:=*{argmax}_{}_{r}[_{r}[_{s_{ i},a_{t}}^{t}r(s_{t},a_{t})]]\) maximizing the expected return, where \(\) is a trajectory on a known target setup \(_{}\) with \(s_{0}_{_{}}\), \(s_{t+1} p_{_{}}(|s_{t},a_{t})\) and \(a_{t}=(s_{t})\).

## 3 Method

Our goal at each step is to select an environment setup \(\) that will produce the most information in expectation. In _Bayesian experimental design_ (BED) , especially Bayesian optimization , this is often framed in terms of an _acquisition function_ that for each \(\) estimates how useful it would be to select, i.e. we would like to select \(\) that maximizes the acquisition function.

We use the acquisition function most common in BED, the _expected information gain_ (EIG):

\[EIG_{n}()=_{r|_{n}}_{|r,}[  p(r|,)- p(r)]=_{r|_{n}} _{|r,}[ p(|r,)- p(|)],\]

where the expectation over trajectories is taken with respect to \(_{}\), \(p_{}\), and an expert policy that would correspond to the reward \(r\) from the outer expectation, taken with respect to the current posterior.

In general, the expectations cannot be calculated analytically. A basic way to approximate the EIG would be using the following nested Monte Carlo estimator for each candidate environment setup \(\):

1. Sample \(N_{r}\) reward functions \(r_{i}\) from the current posterior \(p(r|_{n})\). For each \(r_{i}\): 1. Sample \(N_{}\) trajectories \(_{ij}\) from the estimated expert policy \(_{E}^{r_{i},}\) given the environment parameters \(\), where \(_{E}^{r_{i},}\) would be the Boltzmann-rational policy corresponding to \(r_{i}\). 2. Estimate2\(p(_{ij}|r_{i},)=_{s_{t},a_{t}}_{E}^{r_{i},}(a_ {t}|s_{t})\) and \(p(_{ij}|)=}_{k}p(_{ij}|r_{k},)\). 2. Approximate EIG using the Monte Carlo estimate: \[()=}_{i=1}^{N_{r}}}_{j=1} ^{N_{}}[ p(_{ij}|r_{i},)- p(_{ij}|)].\] (2)

While conceptually simple, the computational demands of this grow quickly with the size of the state space. Thus, in the next section, we discuss a method based on Bayesian optimization to allocate any computational budget we may have more efficiently.

### Efficient sampling with Bayesian optimization

We propose to use _Bayesian optimization_, in particular the _upper confidence bound_ (UCB) algorithm, to adaptively choose from which initial states to sample additional hypothetical trajectories to efficiently estimate the EIG. We still use the basic structure of (2), but instead of using the same number of samples in each initial state, we dynamically choose where to add additional samples to best improve our chance of identifying the state maximizing the EIG.

We model the information gain from each hypothetical trajectory \(_{si}\) starting in state \(s\) as a Gaussian noisy observation of the true EIG value:

\[e_{si}(s)(_{s},_{s}^{2})\,,\] (3)where we assume \(_{s}=(s)\). We also assume we have a prior on the mean and noise,

\[_{s}(_{},_{}^{2})\,, _{s} p_{}(_{s})\,.\] (4)

We first collect a fixed initial number of samples for each state. Then, we repeat the following until we have exhausted a budget of trajectories \(T\). Following standard Gaussian updating, after an observation of a new hypothetical trajectory from \(s\), we update the parameters

\[_{s}=}}{_{}^{2}}+}(s)}{_{s}^{2}}}^{2}}+}{_{s}^{2}}^{-1}\,,_{ s}^{2}=}^{2}}+}{_{s}^{2}} ^{-1}\,,\] (5)

where \(n_{s}\) is the number of observed trajectories from \(s\), and \(}(s)=}_{i=1}^{n_{s}}e_{si}\) is the average of the corresponding EIG estimates. We then update \(_{s}\) using maximum a posteriori estimation:

\[_{s}=*{arg\,max}_{_{s}}p_{}(_ {s})(}(s)_{s}(_{s}), _{s}(_{s}))\,.\] (6)

and compute a new EIG estimate for the value \(s^{*}\) maximizing the upper confidence bound:

\[s^{*}=*{arg\,max}_{s}(s):=*{arg\,max}_{s} _{s}+_{s}\,,\] (7)

where \(\) is a UCB hyperparameter (we use \(=3\)).

## 4 Experiments

We evaluated our EIG-based methods with full trajectories on two randomized gridworld setups against several simpler baselines: (1) uniform random sampling, (2) selecting the state with maximum entropy in Q-values, (3) querying just a single state (to measure the benefits of whole trajectories), and (4) selecting the starting state leading to trajectories with maximum posterior predictive entropy over the optimal policy. The last one is an acquisition function from , which is the only previous work on active IRL over trajectories that we are aware of.

We use two main metrics: the entropy of the posterior distribution over reward parameters after a given number of steps of active learning and the expected return (with respect to the initial state distribution and environment dynamics) of an apprentice policy maximizing this expected return (also with respect to the posterior over rewards).

We test on two kinds of gridworld environments: one with fewer state types (and thus reward parameters) than states, which gives the algorithm a known environment structure to exploit, and one with a single random reward per state. Full details on our experiments and additional results (including the efficiency gains from Bayesian optimization) are provided in Appendix C.

Structured gridworldWe begin with the \(6 6\) gridworld shown in Figure 0(a). This environment is deterministic with 5 actions corresponding to moving in the four directions and staying in place. The agent can move freely, except for the bottom-left "jail" state, which is non-terminal, has a negative reward, and traps the agent permanently upon entry. In terms of the state rewards, there are five different state types and both the apprentice and the expert know the type of each state a priori. The rewards associated with two state types are known: "path" type, with a reward of \(-1\), and a "goal" type with reward 100, which is also terminal. There are 3 state types, which we refer to as "water", "mud", and "lava", which have unknown negative reward. We place an independent uniform prior in the interval \([-100,0]\) on the reward of each state type. Our goal is to infer the reward of these three state types.

Fully random gridworldWe also performed experiments on a \(7 7\) gridworld with each state's reward drawn from \((0,3)\). Each state furthermore has a 10% probability of being terminal. States with reward above the \(0.9\) quantile of rewards are also terminal.

ResultsFigure 2 shows results for structured environment (the results for the fully random environment can be found in Appendix C), comparing active methods with randomly choosing trajectories. We observe that the performance of EIG in terms of posterior entropy and in terms of apprentice performance is superior to the baselines. The Q-Entropy does better than random initially, but then starts to do worse due to repeatedly sampling from states with irreducible uncertainty. Notably, on the structured environment, the posterior predictive action entropy acquisition function from  breaks entirely, as it only ever queries for demonstration trajectories that start in the jail state, as this state trivially has a uniform action distribution, and demonstrations starting in the jail state deterministically remain in the jail state. Thus these demonstrations offer no useful information about the expert reward or the policy. Based on these results, we believe that our information-theoretically derived acquisition function is more principled and robust.

## 5 Discussion and conclusion

We have provided a preliminary study of the problem of active IRL with full trajectories in tabular environments. We have shown that an information theoretic acquisition function provides improvements both in terms of achieving lower posterior entropy, and in terms of apprentice performance. It thus allows using the scarce time of demonstrators more efficiently. We see this preliminary study with synthetic gridworlds and demonstrations as a stepping stone toward an extension to continuous state spaces and more realistic settings.