# Boosting Graph Pooling with Persistent Homology

Chaolong Ying,  Xinjian Zhao,  Tianshu Yu

School of Data Science, The Chinese University of Hong Kong, Shenzhen

{chaolongying,xinjianzhao1}@link.cuhk.edu.cn, yutianshu@cuhk.edu.cn

Corresponding author

###### Abstract

Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.

## 1 Introduction

Persistent homology (PH) is a powerful tool in the field of topological data analysis, which is capable of evaluating stable topological invariant properties from unstructured data in a multi-resolution fashion . Concretely, PH derives an increasing sequence of simplicial complex subsets by applying a filtration function (see Fig. 1(a)). According to the fact that PH is at least as expressive as Weisfeiler-Lehman (WL) hierarchy , there recently emerged a series of works seeking to merge PH into graph neural networks (GNNs), delivering competitive performance on specific tasks . Standard schemes of existing works achieve this by employing pre-calculated topological features  or placing learnable filtration functions in the neural architectures . Such integration of PH features is claimed to enable GNNs to emphasize persistent topological sub-structures. However, it is still unclear to what extent the feature-level integration of PH is appropriate and how to empower GNNs with PH other than utilizing features.

Graph pooling (GP) in parallel plays an important role in a series of graph learning methods , which hierarchically aggregates an upper-level graph into a more compact lower-level graph. Typically, GP relies on calculating an assignment matrix taking into account local structural properties such as community  and cuts . Though the pooling paradigm in convolutional neural networks (CNNs) is quite successful , some researchers raise concerns about its effectiveness and applicability in graphs. For example,  challenges the local-preserving usage of GP by demonstrating that random pooling even leads to similar performance. Till now, it remains opaque what property should be preserved for pooled topology to better facilitate the downstream tasks.

From Fig. 1(a), it is readily observed that PH and GP both seek to coarsen/sparsify a given graph in a hierarchical fashion: while PH gradually derives persistent sub-topology (substructures such as cycles) by adjusting the filtering parameter, GP obtains a sub-graph by performing a more aggressive cut-off. In a sense of understanding a graph through a hierarchical lens, PH and GP turn out to align with each other well.

Driven by this observation, in this paper, we investigate the mechanism of aligning PH and GP so as to mutually reinforce each other. To this end, we conduct experiments by running a pioneer GP method DiffPool  to conduct graph classification on several datasets and at the same time use the technique in  to compute PH information. We manually change the pooling ratio and see what proportion of meaningful topological information (characterized by the ratio of non-zero persistence) is naturally preserved at the final training stage. Surprisingly, the correspondence is quite stable regardless of different datasets (see Fig. 1(b)), which implies the monotone trend between the pooling ratio and non-zero persistence is commonly shared by a large range of graph data. As a consequence, we develop a natural way to integrate PH and GP in both feature and topology levels. Concretely, in addition to concatenating vectorized PH diagram as supplementary features, we further enforce the coarsened graph to preserve topological information as much as possible with a specially designed PH-inspired loss function. Hence we term our method Topology-Invariant Pooling (TIP). TIP can be flexibly injected into a variety of existing GP methods, and demonstrates a consistent ability to provide substantial improvement over them. We summarize our contributions as follows:

* We for the first time investigate the way of aligning PH with GP, by investigating the monotone relationship in between.
* We further design an effective mechanism to inject PH information into GP at both feature and topology levels, with a novel topology-preserving loss function.
* Our mechanism can be flexibly integrated with a variety of GP methods, achieving consistent and substantial improvement over multiple datasets.

## 2 Related work

**Graph pooling.** Graph pooling has been used in various applications, which can reduce the graph size while preserving its structural information. Early methods are based on clustering to coarsen graphs, such as the greedy clustering method Graclus , non-negative matrix factorization of the adjacency matrix , and spectral clustering . Recently, learnable graph pooling methods have gained popularity, which learn to select important nodes in an end-to-end manner. DiffPool  follows a hierarchical learning structure by utilizing GNNs to learn clusters and gradually aggregate nodes into a coarser graph. MinCutPool  optimizes a normalized cut objective to partition graphs into clusters. DMoNPool  optimizes the modularity of graphs to ensure high-quality clusters. SEP  generates clusters in different hierarchies simultaneously without compromising local structures. These methods are classified as dense pooling due to the space complexity they incur. Despite their effectiveness, dense pooling methods have been criticized for high memory cost and complexity . Therefore, various sparse pooling methods have been proposed, such as Top-K , ASAPool , and SAGPool . These methods coarsen graphs by selecting a subset of nodes based on a ranking score. As they drop some nodes in the pooling process, these methods are criticized for their limited capacity to retain essential information, with potential effects on the expressiveness of preceding GNN layers .

Figure 1: Illustration of Graph Pooling (GP) and Persistent Homology (PH). (a) GP and PH share a similar hierarchical fashion by coarsening a graph. (b) As a motivating experiment, we gradually change pooling ratio and count how persistence ratio (ratio of non-zero persistence) changes with it. (c) Illustration of persistence diagrams.

**Persistent homology in GNNs.** PH is a technique to calculate topological features of structured data, and many approaches have been proposed to use PH in graph machine learning due to the high expressiveness of topological features on graphs . Since non-isomorphic graphs may exhibit different topological features, the combination of PH and the Weisfeiler-Lehman (WL) algorithm leads to stronger expressive power . This encourages further exploration on equipping GNNs with topological features.  propose that message passing in GNNs can be effectively reweighted using topological features.  and  provide theoretical and practical insights that filtrations in PH can be purely learnable, enabling flexible usage of topological features in GNNs. However, existing methods tend to view PH merely as a tool for providing supplementary information to GNNs, resulting in unsatisfactory improvements and limited interpretability.

## 3 Background

We briefly review the background of this topic in this section, as well as elaborate on the notations.

Let \(=(V,E)\) be an undirected graph with \(n\) nodes and \(m\) edges, where \(V\) and \(E\) are the node and the edge sets, respectively. Nodes in attributed graphs are associated with features, and we denote by \(V=\{(v,_{v})\}_{v 1:n}\) the set of nodes \(v\) with \(d\) dimensional attribute \(_{v}\). It is also practical to represent the graph with an adjacency matrix \(\{0,1\}^{n n}\) and the node feature matrix \(^{n d}\).

Graph Neural Networks.We focus on the general message-passing GNN framework that updates node representations by iteratively aggregating information from neighbors . Concretely, the \(k\)-th layer of such GNNs can be expressed as:

\[^{(k)}=(,^{(k-1)};^{(k)} ),\] (1)

where \(^{(k)}\) is the trainable parameter, and \(\) is the message propagation function. Numbers of \(\) have been proposed in previous research [25; 15]. A complete GNN is typically instantiated by stacking multiple layers of Eq. 1. Hereafter we denote by \(()\) an arbitrary such multi-layer GNN for brevity.

Dense Graph Pooling.GP in GNNs is a special layer designated to produce a coarsened or sparsified sub-graph. Formally, GP can be formulated as \(_{P}=(V_{P},E_{P})\) such that the number of nodes \(|V_{P}| n\). GP layers can be placed into GNNs in a hierarchical fashion to persistently coarsen the graph. Typical GP approaches [50; 2; 34] rely on learning a soft cluster assignment matrix \(^{(l)}^{n_{l-1} n_{l}}\):

\[^{(l)}=(^{(l)}(^{(l -1)},^{(l-1)})).\] (2)

Subsequently, the coarsened adjacency matrix at the \(l\)-th pooling layer is calculated as

\[^{(l)}=^{(l)}^{(l-1)}^{(l)},\] (3)

and the corresponding node representations are calculated as

\[^{(l)}=^{(l)}^{(l)}(^{(l-1 )},^{(l-1)}).\] (4)

These approaches differ from each other in the way to produce \(\), which is used to inject a bias in the formation of clusters. In our work, we select three GP methods, i.e., DiffPool , MinCutPool , and DMoNPool , to cope with. Details of the pooling layers in these methods are summarized in Appendix A.

Topological Features of Graphs.A simplicial complex \(K\) consists of a set of simplices of certain dimensions. Each simplex \( K\) has a set of faces, and each face \(\) has to satisfy \( K\). An element \( K\) with \(||=k+1\) is called a \(k\)-simplex, which we denote by writing \(=k\). Furthermore, if \(k\) is maximal among all simplices in \(K\), then \(K\) is referred to as a \(k\)-dimensional simplicial complex. A graph can be seen as a low-dimensional simplicial complex that only contains 0-simplices (vertices) and 1-simplices (edges) . The simplest kind of topological features describing graphs are Betti numbers, formally denoted as \(_{0}\) for the number of connected components and \(_{1}\) for the number of cycles.

Despite the limited expressive power of these two numbers, it can be improved by evaluating them alongside a filtration. Filtrations are scalar-valued functions of the form \(f:V E\). Changes in the Betti numbers, named as persistent Betti numbers, can subsequently be monitored throughout the progress of the filtration: by considering a threshold (\(a\)), we can analyze the subgraph originating from the pre-image of \(((-,a])\) of \(f\), denoted as \((f^{-1}((-,a]))\). The image of \(f\) leads to a finite set of values \(a_{1}<<a_{n}\) and generates a sequence of nested subgraphs of the form \(_{0}_{k} _{n}=\), where \(_{k}=(V_{k},E_{k})\) is a subgraph of \(\) with \(V_{k}:=\{v V f(_{v}) a_{k}\}\) and \(E_{k}:=\{(v,w) E\{f(x_{v}),f(x_{w}) \} a_{k}\}\). This process is also known as persistent homology (denoted as \(ph()\)) on graphs. Typically, persistent Betti numbers are summarized in a persistence diagram (PD) as \((,f)[i]=_{i}\), where \(i[0,1,...]\) is the dimension of topological features. PD is made up of tuples \((a_{i},a_{j})^{2}\), with \(a_{i}\) and \(a_{j}\) representing the creation and destruction of a topological feature respectively (see Fig. 1(c)). The absolute difference in function values \(|a_{j}-a_{i}|\) is called the persistence of a topological feature, where high persistence corresponds to features of the function, while low persistence is typically considered as noise [19; 39].

## 4 Methodology

### Overview

An overview of our method is shown in Fig. 2, where the shaded part corresponds to one layer of Topology-Invariant Pooling. The upper part is the GP process and the lower part is the injection of PH. Let \((^{(0)},^{(0)})\) be the input graph. We consider to perform a GP at the \((l-1)\)-th layer. After obtaining a coarsened (densely connected) graph \((^{(l)},^{l})\) with a standard GP method, we resample the coarsened graph using Gumbel-softmax trick as \(^{(l)}\) in order to make it adapt to PH. Then, this coarsened graph is further reweighted injecting persistence, and is optimized by minimizing the topological gap \(_{topo}\) from the original graph, yielding \((^{(l)},^{l})\). By stacking multiple TIP layers, hierarchical pooling emphasizing topological information can be achieved. In the following sections, we elaborate on the detailed design of our mechanism.

### Topology-Invariant Pooling

In many real-world applications, the topology of graphs are of utmost importance [44; 49; 16]. However, typical GNNs fail to capture certain topological structures in graphs, such as cycles [4; 51; 21]. Moreover, in dense graph pooling, graphs are pooled without preserving any topology. Even if we manage to make GNN topology-aware, the coarsened graph is nearly fully connected and has no meaningful topology at all, impairing the use of GNNs in these tasks. To overcome these limitations, we propose to inject topological information into GP. We resort to PH to characterize the importance of edges.

The core of PH is the notion of filtration, the selection of which presents a challenging task. As the coarsened graph evolves in each training step, integrating PH into GP demands multiple computations

Figure 2: Overview of our method. The shaded part is one layer of Topology-Invariant Pooling.

of filtrations. To address this, we place recently proposed learnable filtration (LF) functions  to incorporate PH information for flexibility and efficiency. LF relies on node features and graph topology, which are readily available in GP. Consequently, LF can be seamlessly integrated into GP with minimal computational overhead. Specifically, we employ an MLP network \(()\) as the filtration function together with \(()\) to map node features \(^{n d}\) into \(n\) scalar values. Recently, an increasing amount of attention has been devoted to cycles  due to their significant relevance to downstream tasks in various domains such as biology , chemistry , and social network analysis . Recognizing that cycles offer an intuitive representation of graph structure , and preliminary experiments, shown in Appendix E.5, indicate that the additional inclusion of zero-dimensional topological features merely increases runtime, thus we instead focus on the one-dimensional PDs associated with cycles. For those edges do not form cycles, their creation and destruction are the same, leading to zero persistence. Following the standard way in GP (Eq. 2 3 4), we additionally propose the subsequent modules to inject PH into GP at both feature and topology levels.

Resampling.One major limitation of utilizing LF proposed in  is that the computation process is unaware of edge weights, i.e. edges with non-zero weights will be treated equally, so PH cannot directly extract meaningful topology from \(^{(l)}\). Besides, rethinking GP in Eq. 3, the coarsened adjacency matrix has limited expressive power for two reasons. First, although \(^{(l)}\) is a soft assignment matrix obtained by \(()\), each element still has nonzero values, i.e. \(^{(l)}\) is always densely connected. Second, the edge weights may span a wide range by multiplication (refer to Appendix D for empirical evidence). These drawbacks hinder the stability and generalization power of the subsequent message passing layers . None of the existing GP methods can handle these problems properly.

Therefore, we resample the coarsened adjacency \(^{(l)}\) obtained from a normal GP layer (Eq. 3) as:

\[^{(l)}=(^{(l)}- (^{(l)})}{(^{(l)})-(^{(l)})}),\] (5)

where \(^{(l)}\) is first normalized in the range of \(\), and \(()\) is performed independently for each matrix entry using the Gumbel-softmax trick . In practice, only the upper triangular matrix is resampled to make it symmetric and we add self-loops to the graph.

Persistence Injection.Now \(^{(l)}\{0,1\}^{n_{l} n_{l}}\) is a sparse matrix without edge features so we can easily inject topological information into it. For a resampled graph with \(^{(l)}\) and \(^{(l)}\), we formulate the persistence injection as:

\[}_{1}&=(^{(l)},((^{(l)})))\\ ^{(l)}&=^{(l)} (}_{1}-}_{1}[ 0]),\] (6)

where \(\) is the Hadamard product, \(()\) means transforming sparse representations in terms of edges to dense matrix representations, \(}_{1}\) is the augmented 1-dimensional PDs by placing the tuples correspond to self-loop edges on the diagonal part of original PDs \(_{1}\), \(}_{1}[i]\) is the \(i\)-th value in each tuple of \(}_{1}\), and we denote the updated adjacency matrix after persistence injection still as \(^{(l)}\) for notation consistency. Persistence injection can actually be regarded as a reweighting process. Since the filtration values are within \(\), \(^{(l)}\) after persistence injection is guaranteed to have edge weights in the range of \(\) and is passed to the next pooling layer.

Topological Loss Function.The aforementioned mechanism can explicitly inject topological information into graphs, but it relies on the condition that the coarsened graph retains certain essential sub-topology. To this end, we propose an additional loss function to guide the GP process.

Intuitively, the coarsened graph should exhibit similarity to the original graph in terms of topology. Since the computation of PH is differentiable, one possible approach is to directly minimize the differences between the PDs of the original graph and the coarsened graph. However, this implementation would require computing the Wasserstein distance between two PDs through optimal transport , which is intractable in training due to its complexity. Considering that our objective is to estimate the difference, we instead propose vectorizing the PDs and minimizing their high-order statistical features . Specifically, we use several transformations (denoted as \(()\)) and concatenate the output, including triangle point transformation, Gaussian point transformation and line point transformation introduced in  to convert the tuples in PD into vector \(_{t}\) (\(t[1,m]\)). We calculate the mean vector \(\) as well as the second-order statistics as the standard deviation vector \(\) as:

\[_{t}=(}_{1})\] (7) \[=_{t=1}^{m}_{t},=_{t=1}^{m}_{t}_{t}-}\]

In this manner, the difference between PDs can be estimated through the comparison of their statistics in the features, which is the concatenation of the mean and variance vectors. To further regularize the topological difference between layers, we introduce a topological loss term defined as:

\[_{topo}=_{l=1}^{L}_{i=1}^{d}((_{i }^{(l)}\|_{i}^{(l)})-(_{i}^{(0)}\|_{i}^{(0)}) )^{2},\] (8)

where \((||)\) stands for the concatenation operation, \(L\) is the number of pooling layers, and \(d\) is the feature dimension. Note that the intuition behind \(_{topo}\) is different from the loss functions in existing graph pooling methods: the coarsened graph after pooling should be topologically similar to the original graph rather than having exact cluster structures.

### Analysis

In this section, we examine the validity of our proposed method, and in particular, analyze its expressive power and complexity.

**Theorem 1**.: _The self-loop augmented 1-dimensional topological features computed by PH is sufficient enough to be at least as expressive as 1-WL in terms of distinguishing non-isomorphic graphs, i.e. if the 1-WL label sequences for two graphs \(\) and \(^{}\) diverge, there exists an injective filtration \(f\) such that the corresponding 1-dimensional persistence diagrams \(}_{1}\) and \(}_{1}^{}\) are not equal._

**Proof Sketch.** We first assume the existence of a sequence of WL labels and show how to construct a filtration function \(f\) from this. Consider nodes \(u\) and \(u^{}\) are nodes with unique label count in \(\) and \(^{}\), then our filtration is constructed such that their filtration values \(f(u)\) and \(f(u^{})\) are unique and different. Consider all three cases: (1) \(u\) and \(u^{}\) are both in cycles; (2) \(u\) and \(u^{}\) are both not in cycles; (3) one of \(u\) and \(u^{}\) is in cycles and the other is not. For all the cases, \(f(u)\) and \(f(u^{})\) will be revealed in their respective persistence diagrams. Since \(f(u)\) and \(f(u^{})\) are unique and different, we can use the augmented persistence diagrams to distinguish the two graphs.

This result demonstrates that the self-loop augmented 1-dimensional topological features contain sufficient information to potentially perform at least as well as 1-WL when it comes to distinguishing non-isomorphic graphs. We can then obtain the concluding remark that TIP is more expressive than other dense pooling methods by showing that there are pairs of graphs that cannot be distinguished by 1-WL but can be distinguished by TIP. Besides, our proposed simple yet effective self-loop augmentation eliminates the necessity of computing 0-dimensional topological features, thus reducing computational burdens.

**Proposition 1**.: _TIP is invariant under isomorphism._

Detailed proof and illustrations of the theorem and proposition can be found in Appendix C.

**Complexity.** PH can be efficiently computed for dimensions 0 and 1, with a worst-case time complexity of \(O(m(m))\), where \(m\) represents the number of sorted edges in a graph. Here, \(()\) represents the inverse Ackermann function, which is extremely slow-growing and can essentially be considered as a constant for practical purposes. Therefore, the primary factor that affects the calculation of PH is the complexity of sorting all the edges, which is \(O(m m)\). Our resampling and persistence injection mechanism ensures that the coarsened graphs are sparse rather than dense, making our approach both efficient and scalable. We provide running time comparisons in Appendix E.2, which indicates that the inclusion of TIP does not impose a significant computational burden.

Experiments

In the experiments, we evaluate the benefits of persistent homology on several state-of-the-art graph pooling methods, with the goal of answering the following questions:

**Q1.** Is PH capable of preserving topological information during pooling?

**Q2.** How does PH affect graph pooling in preserving task-specific information?

To this end, we showcase the empirical performance of TIP on two tasks, namely, topological similarity (Section 5.2) and graph classification (5.3). Our primary focus is to assess in which scenarios topology can enhance GP.

### Experimental Setup

Models.To investigate the effectiveness of PH in GP, we integrate TIP with DiffPool, MinCutPool, and DMoNPool, which are the pioneering approaches that have inspired many other pooling methods. Additionally, as most pooling methods rely on GNNs as their backbone, we compare the widely used GNN models GCN , GIN , and GraphSAGE . We also look into another two related and State-of-the-Art GNN models, namely TOGL  and GSN , which incorporate topological information and graph substructures into GNNs to enhance the expressive power. Several other GP methods, namely Graclus  and TopK  are also compared. For model selection, we follow the guidelines provided by the original authors or benchmarking papers. Our method acts as an additional plug-in to existing pooling methods (referred to as -TIP) without modifying the remaining model structure and hyperparameters. Appendix B.1 provides detailed configurations of these models.

Datasets.To evaluate the capabilities of our model across diverse domains, we assess its performance on a variety of graph datasets commonly used in graph related tasks. We select several benchmarks from TU datasets , OGB datasets  and ZINC dataset . Specifically, we adopt molecular datasets NCI1, NCI109, and OGBG-MOLHIV, bioinformatics datasets ENZYMES, PROTEINS, and DD, as well as social network datasets IMDB-BINARY and IMDB-MULTI. Furthermore, to investigate the topology-preserving ability of our method, we conduct experiments on several highly structured datasets (ring, torus, grid2d) obtained from the PyGSP library. Appendix B.2 provides detailed statistics of the datasets.

Evaluation.In the graph classification task, all datasets are splitted into train (80%), validation (10%), and test (10%) data. Following the evaluation protocol in [50; 30], we train all models using the Adam optimizer  and implement a learning rate decay mechanism, reducing the learning rate from \(10^{-3}\) to \(10^{-5}\) with a decay ratio of 0.5 and a patience of 10 epochs. Additionally, we use early stopping based on the validation accuracy with patience of 50 epochs. We report statistics of the performance metrics over 20 runs with different seeds.

### Preserving Topological Structure

In this experiment, we study **Q1** about the ability of PH to preserve topological structure during pooling. Specifically, we assess the topological similarity between the original and coarsened graphs \(\) and \(^{}\), by comparing the Wasserstein distance associated with their respective PDs \(}_{1}\) and \(}^{}_{1}\). This evaluation criterion is widely used to compare the topological similarity of graphs [48; 41]. We utilize Forman curvature on each edge of the graph as the filtration, which incorporates edge weights and graph clusters to better capture the topological features of the coarsened graphs [42; 45]. We consider the 1-Wasserstein distance \(W(}_{1},}^{}_{1})=_{ (}_{1},}^{}_{1})} _{(x,y)}[\|x-y\|]\) as the evaluation metric, where \(()\) is the set of joint distributions \((x,y)\) whose marginals are \(}_{1}\) and \(}^{}_{1}\), respectively. Note that we are not learning a new filtration but keep a fixed one. Rather, we use learnable filtrations in training to enhance flexibility, and solely optimize \(L_{topo}\) as the main objective.

We compare TIP with other pooling methods. Table 1 reports the average \(W\) values on three datasets, demonstrating that TIP can improve dense pooling methods to a large margin and have the best topological similarity. We visualize the pooling results in Fig. 3 for better interpretation, where isolated nodes with no links are omitted for clarity. It is evident that DiffPool, MinCutPool, and 

[MISSING_PAGE_FAIL:8]

### Preserving Task-Specific Information

In this experiment, we examine the impact of PH on GP in downstream tasks to answer **Q2**. We have observed in the former experiment that PH can preserve essential topological information during pooling. However, two additional concerns arise: (1) Does TIP continue to generate invariant sub-topology in the downstream task? (2) If so, does this sub-topology contribute to the performance of the downstream task? To address these concerns, we evaluate TIP using various graph classification benchmarks, where the accuracy achieved on these benchmarks serves as a measure of a method's ability to selectively preserve crucial information based on the task at hand.

We begin by visualizing the coarsened graphs in this task, where edges are cut-off by a small value. From Fig. 4, we can clearly observe that our method manage to preserve the essential sub-topology similar to the original graphs, while dense pooling methods cannot preserve any topology. As discussed in , dense pooling methods achieve comparable performance when the assignment matrix \(\) is replaced by a random matrix. Here our visualization reveals that regardless of the value of \(\), the coarsened graph always approaches to a fully connected one. Sparse pooling methods, on the other hand, manage to preserve some local structures through clustering or dropping, but the essential global topological structures are destroyed.

Table 2 presents the average and standard deviation of the graph classification accuracy on benchmark datasets, where the results of GP and several baseline GNNs are provided. Experimental results demonstrate that TIP can consistently enhance the performance of the three dense pooling methods. While the original dense pooling methods sometimes underperform compared to the baselines, they are able to surpass them after integrating TIP.

Moreover, an intriguing observation can be found on ENZYMES dataset, where TOGL surpasses the baseline GNNs. TOGL in practice, incorporates PH into GNNs (GraphSAGE in our implementation), so this results underscores the significance of incorporating topological information for improved performance on ENZYMES. Further, our method demonstrates more significant improvements by augmenting the three dense pooling methods on the ENZYMES dataset. One possible explanation for the observed phenomenon is that the coarsened graphs generated by our methods bear a striking resemblance to numerous frequent subgraphs present in this dataset . Such substructures may serve as indicators of unique characteristics within the graph, rendering them valuable for subsequent tasks. However, it is also worth noting that TOGL only exhibits marginal improvements or even underperforms on the other datasets. This suggests that simply integrating PH features into GNN layers does not fully exploit topological information. Conversely, injecting global topological invariance into pooling layers in our method yields superior performance.

To demonstrate the effectiveness of preserving the invariant sub-topology, we compared DiffPool-TIP with its variant counterpart, DiffPool-TIP-NL (no topological loss), by replacing \(_{topo}\) with the original \(_{r}\) in DiffPool (see Table 4 in Appendix A). The training objective curve and the Wasserstein distance curve are presented in Figure 5, both based on the ENZYMES dataset and a fixed filtration (the same as in Section 5.2). From the figures, it is evident that the objective value decreases as the coarsened graphs become more similar in topology to the original graphs when using DiffPool-TIP. However, when training without \(_{topo}\), the performance is inferior. Additionally, even when the

Figure 4: Graphs pooled with different methods in graph classification experiment.

objective value converges, DiffPool-TIP-NL still exhibits changing topology, whereas DiffPool-TIP maintains a stable topology, possibly benefiting from the stability of PH . This also suggests that multiple suboptimal topologies may contribute equally to the objective. Our topology invariant pooling strategy consistently selects topologies similar to the original graph, which leads to better performance. Additional visualization results and analysis about the coarsened graphs obtained by DiffPool-TIP-NL can be found in Appendix E.3.

Aside from the graph classification task, Table 3 presents the mean and standard deviation of prediction accuracy for the constrained solubility of molecules in the ZINC dataset, where mean square error is used as performance metric. We can observe that TIP can still boost the three pooling methods on regression task, which demonstrates that our proposed method can retain task-related information. Besides, we design an additional set of experiments in Appendix E.4, where the topological structure of the graph is highly task-relevant. **Ablation study** about the contributions of different modules are shown in Appendix E.5. Finally, to empirically demonstrate the expressive power of our proposed method, we provide an experiment on distinguishing non-isomorphic graphs in Appendix E.6.

## 6 Conclusion

In this paper, we developed a method named Topology-Invariant Pooling (TIP) that effectively integrates global topological invariance into graph pooling layers. This approach is inspired by the observation that the filtration operation in PH naturally aligns with the GP process. We theoretically showed that PH is at least as expressive as WL-test, with evident examples demonstrating TIP's expressivity beyond dense pooling methods. Empirically, TIP indeed preserved persistent global topology information, and achieved substantial performance improvement on top of several pooling methods on various datasets, demonstrating strong flexibility and applicability.

The potential limitation of our study is the heavy reliance of the proposed method on circular structures within graphs, potentially hindering its efficacy on tree-like graphs. Besides, our method lacks the ability to discriminate between graphs when the number of connected components is the only distinguishing factor. Our method can be extended to address this limitation by explicitly incorporating this information into the node features during the pooling process.