# Fourier-enhanced Implicit Neural Fusion Network

for Multispectral and Hyperspectral Image Fusion

 Yu-Jie Liang

University of Electronic

Science and Technology of China

yujieliang0219@gmail.com

&Zihan Cao

University of Electronic

Science and Technology of China

iamzihan666@gmail.com

&Shangqi Deng

Xi'an Jiaotong University

shangqideng0124@gmail.com &Hong-Xia Dou

Xihua University

hongxiaodou1991@126.com

&Liang-Jian Deng

University of Electronic Science and Technology of China

liangjian.deng@uestc.edu.cn

Corresponding author.

###### Abstract

Recently, implicit neural representations (INR) have made significant strides in various vision-related domains, providing a novel solution for Multispectral and Hyperspectral Image Fusion (MHIF) tasks. However, INR is prone to losing high-frequency information and is confined to the lack of global perceptual capabilities. To address these issues, this paper introduces a Fourier-enhanced Implicit Neural Fusion Network (FeINFN) specifically designed for MHIF task, targeting the following phenomena: _The Fourier amplitudes of the HR-HSI latent code and LR-HSI are remarkably similar; however, their phases exhibit different patterns._ In FeINFN, we innovatively propose a spatial and frequency implicit fusion function (Spa-Fre IFF), helping INR capture high-frequency information and expanding the receptive field. Besides, a new decoder employing a complex Gabor wavelet activation function, called Spatial-Frequency Interactive Decoder (SFID), is invented to enhance the interaction of INR features. Especially, we further theoretically prove that the Gabor wavelet activation possesses a time-frequency tightness property that favors learning the optimal bandwidths in the decoder. Experiments on two benchmark MHIF datasets verify the state-of-the-art (SOTA) performance of the proposed method, both visually and quantitatively. Also, ablation studies demonstrate the mentioned contributions. The code can be available at [https://github.com/294coder/Efficient-MIF](https://github.com/294coder/Efficient-MIF).

## 1 Introduction

Hyperspectral imaging captures scenes across contiguous spectral bands, offering intricate details compared to traditional single or limited-band images, and improving computer vision application accuracy, such as target recognition, classification , tracking, and segmentation . However, practical optical sensors face challenges in balancing spatial resolution and spectral precision. Images with over 100 bands often exhibit lower spatial resolution, while those with fewerbands display higher spatial resolution. Efforts for MHIF are underway to fuse high spatial-resolution multispectral images (HR-MSI) with low spatial-resolution hyperspectral images (LR-HSI) to finally obtain high spatial-resolution hyperspectral images (HR-HSI). Actually, MHIF technology could fuse hyperspectral images with multispectral images, extracting information not detectable by HR-MSI to enhance richness and precision. Recent MHIF literature explores model-based approaches [8; 9; 45] and deep learning methods [17; 10; 3; 54]. While model-based methods leverage image priors, challenges persist in obtaining high-fidelity, low-distortion HR-HSI due to the lack of large-scale training datasets. Among deep-learning approaches, CNN-based networks for HR-MSI and LR-HSI tend to be limited and lack interpretability for MHIF tasks and Transformer frameworks [15; 7] address the small receptive field of CNN but bring greater computational overhead.

In recent years, implicit representations of 3D scenes have garnered significant attention from researchers. For instance, Neural Radiance Field  models 3D static scenes by mapping coordinates to signals through a neural network. Inspired by this, researchers have revisited image representation for 2D tasks. Recent studies [5; 20; 34; 4] have achieved arbitrary-scale super-resolution (SR) by replacing commonly used upsampling layers with local implicit image functions. Though these methods demonstrate superior performance in 2D tasks, they still have some drawbacks. _Firstly_, INR calculates the RGB values of a queried coordinate based on the relative distances to the surrounding four pixels, treating it as a local operation in space that lacks consideration for global information. _Additionally_, the MLP-ReLU structure used in traditional INR inherent high-frequency information bias  which is challenging to be eliminated during training.

To address these issues, we propose implicit fusion functions tailored for the MHIF task as a novel fusion paradigm. We first employ encoders to extract prior information from LR-HSI and HR-MSI, which is then fed into the implicit fusion functions in the form of latent codes. Unlike traditional INR, we transform latent codes into the Fourier domain and simultaneously perform spatial and frequency fusion in a unified network. This approach not only rectifies the high-frequency insensitivity induced by the MLP but also effectively extends the receptive field, encompassing a more comprehensive scope of global information. To integrate spatial and frequency domain representations efficiently, we design a decoder with time-frequency tightness, mapping features on both domains to pixel space. The contributions of this work are three folds:

* We define a novel fusion framework based on INR, which innovatively extracts information from the spatial and Fourier domains, effectively enhances the representation ability of high-frequency information, and expands the receptive field.
* We propose a new decoder employing a Gabor wavelet activation function to enhance the interaction of INR features. Furthermore, we theoretically prove that the complex Gabor wavelet activation possesses a time-frequency tightness property, which facilitates the decoder in learning the optimal bandwidths.
* The proposed network reaches state-of-the-art (SOTA) performance on the MHIF task across two widely used hyperspectral datasets at various fusion ratios. Fig. 1 provides a fair comparison with other SOTA methods.

Figure 1: Comparison of our method with other methods on the CAVE(\(\) 4, \(\) 8) and Harvard(\(\) 4, \(\) 8) datasets. Closer to the top-right corner indicates better performance and the size of the circle indicates the number of parameters in the model.

## 2 Related works

Implicit Neural RepresentationUnlike traditional discrete representations, neural implicit representation (INR) provides a more elegant and continuous parameterized approach. Initially applied in 3D modeling tasks, NeRF  revolutionized 3D computer vision by representing intricate three-dimensional scenes with just 2D pose images. This line of work extends to the 2D imaging domain, where INR performs a weighted average on adjacent sub-codes to ensure output value continuity. LIIF  recently introduces a local implicit image function for SR, leveraging MLP to sample pixel signals across the spatial domain. Several improvements focus on decoding networks; for example, UltraSR  incorporates residual networks, merging spatial coordinates and depth encoding. DIINN  utilizes a dual-interactive implicit neural network to decouple content and position features, improving decoding capabilities. JIIF  proposes joint implicit image functions for multimodal learning, extracting priors from guided images. Regarding activation functions in the MLP, SIREN  recommends utilizing periodic activation functions for continuous INR to fit complex signals. On the other hand, WIRE  further employs continuous complex Gabor wavelet activation functions to activate non-linearity, focusing more on spatial frequencies. However, there is limited research dedicated to designing INR architectures specifically for the MHIF task. The unique characteristics of hyperspectral images pose challenges for INR networks, in their insensitivity to high-frequency information.

Latent Enhancement by Fourier TransformFourier transform is a commonly used time-frequency analysis technique in signal processing, which converts signals from the time domain to the frequency domain. The Fourier domain has global statistical properties, and in recent years, many works use the Fourier transform to enhance the representation ability of neural networks. For example, FDA  proposes exchanging amplitude and phase components in Fourier space between images to enhance and adjust frequency information. FFC  introduces a novel convolution module that internally fuses cross-scale information to capture global features in Fourier space. Similarly, GFNet  uses 2D discrete Fourier transform to extract features, implements learnable global filtering, and replaces the self-attention layer in Transformer. UHDFour  embeds Fourier transform into the image enhancement network to model global information. Together, these studies demonstrate the utility of frequency domain information in improving performance on visual tasks. We exploit the architecture of FeINFN to transform latent codes into the frequency domain, implicitly integrating representations of amplitude and phase components, and enhancing high-frequency injection.

Motivation finds that most neural networks exhibit a phenomenon of spectral bias through Fourier analysis. This includes neural networks such as MLP, which tend to learn low-frequency information during the early stages of training and are insensitive to high-frequency information. Moreover, we found this issue occurs in the MHIF task according to an experimental analysis as shown in Fig. 2(a), where HR-HSI and LR-HSI were concatenated with HR-MSI and fed into a trained encoder to obtain latent codes. These codes were transformed into the frequency domain to visualize the amplitude and phase. It can be observed that the amplitudes from HR-HSI and LR-HSI are very similar, while the phases differ significantly. The phase of HR-HSI should naturally contain more texture than LR-HSI, a hypothesis validated by the visualized phase maps. Based on this finding, we transformed the latent codes into the Fourier domain to separately process amplitude and phase, to enhance the global learning of high-frequency information in the images.

Figure 2: **(a)** The amplitude of latent code from the encoder fed by HR-HSI and LR-HSI (combined with HR-MSI) share a similarity, but the phases differ from each other. \(E_{^{*}}\) is a trained encoder. **(b)**\(3 3\) convolution would suffer from the issue of spectrum leakage, which can be alleviated by \(1 1\) convolution.

Methodology

In this section, we first present the preliminary of INR and then provide the proposed framework tailored for MHIF task. Subsequently, we elaborate on the implementation details of the composited modules of the proposed FeINFN.

### Preliminary: Implicit Neural Representation

Neural Radiance Fields  is represented by integral construction scenes. The value of a pixel in a certain viewing angle image is regarded as the integral of the characteristics of the sampling point from the proximal end to the far end of the ray. During actual training, the integral needs to be discretized. Extended to 2D image representation , it is sampled pixel by pixel from the vicinity of the query target. Taking the low-resolution (LR) image \(^{h w 3}\) upsampling to the high-resolution (HR) image \(}^{H W 3}\) as an example, the process of generating the RGB values of the target coordinates \(_{q}^{2}\) can be regarded as interpolation form, expressed as:

\[}(_{q})=_{i_{q}}w_{q,i}_{ q,i}, \]

where \(_{q,i}^{4 4 3}\) is the interpolation pixel of \(i\) interpolated by \(q\)'s surrounding pixels \(_{q}^{4}\) and \(w_{q,i}\) signifies the interpolation weight. In the implicit representation of local image features, the weights \(w_{q,i}=S_{i}/S\), where \(S_{i}\) represents the area formed by \(q\) and \(i\) in the diagonal region and \(S\) denotes the total area enclosed by the set \(_{q}\). The interpolation value \(_{q,i}\) is effectively generated by a basis function:

\[_{q,i}=_{}(_{i},_{q}-_{i}), \]

where \(_{}\) is typically an MLP, \(_{i}\) is the latent code generated by an encoder for the coordinates \(_{i}\), and \(_{q}-_{i}\) represents the relative coordinates. From the above equations, it can be inferred that the interpolation features can be represented by a set of local feature vectors in the LR domain. Typically, interpolation-based methods [28; 18] achieve upsampling by querying \(_{q}-_{i}\) in the arbitrary SR task. See more details in .

### Overview of the FeINFN Framework

In this work, we propose the FeINFN, which adopts a novel framework for simultaneously performing neural implicit representation in both the spatial and frequency domains to execute the MHIF task. Fig. 3 provides an overview of the proposed framework, designed to fuse LR-HSI \(^{LR}^{h w S}\) and HR-MSI \(^{HR}^{H W s}\) to generate HR-HSI \(}^{H W S}\) based on a upsampling scale \(r\).

Figure 3: The flowchart of the FeINFN framework which is composed of a spectral encoder \(E_{}\), a spatial encoder \(E_{}\), MHIF task-designed spatial and Fourier domains implicit fusion functions, and a pixel space mapping decoder. Please note that \(^{LR}\) is the LR-HSI, \(^{HR}\) is the HR-MSI, \(^{LR}_{up}\) is the bicubic interpolation LR-HSI, and \(^{HR}\) is the HR normalized 2D coordinate map. \(_{spe}\), \(_{spa}\), \(_{hp}\), \(\) correspond to individual pixel units, \(\) and \(\) represents amplitude and phase, respectively.

Initially, the LR-HSI is fed into encoder \(E_{}\) to extract spectral features \(_{spe}^{h w C}\). Simultaneously, the concatenated bicubic interpolation LR-HSI \(_{up}^{LR}^{H W S}\) and \(^{HR}\), are fed into encoder \(E_{}\) to extract spatial features \(_{spa}^{H W C}\). Additionally, the pixel's central position is represented as the coordinate point. The coordinate map is normalized into a two-dimensional grid \([-1,1][-1,1]\), obtaining a HR normalized 2D coordinate map \(^{HR}^{H W 2}\). The extracted \(_{spe}\) and \(_{spa}\), along with the 2D coordinates of \(^{HR}\), are forwarded to Spatial-Frequency Implicit Fusion Function (Spa-Fre IFF), outputting spatial domain features \(_{s}^{H W S}\) and frequency domain features \(_{f}^{H W S}\). The \(_{s}\) and \(_{f}\) as inputs to a pixel space mapping decoder which generates the residual image \(_{r}^{HR}^{H W S}\). Finally, the residual image \(_{r}^{HR}\) is combined with the bicubicly upsampled image \(_{up}^{LR}\) via element-wise addition, yielding the ultimate fusion image \(}\).

### INR Encoder Networks

Analogous to local implicit representation functions [5; 20; 34; 4], the initial step involves extracting latent code representations. For the MHIF task, we address the challenges of both upsampling and fusion simultaneously, employing implicit neural representations as the solution.

The INR encoders try to extract spatial and spectral latent codes \(_{spa}^{H W C},_{spe}^{h w C}\): one is extracted from \(^{LR}\), serving as the carrier for spectral information; the other is encoded from the concatenation of \(_{up}^{LR}\) and \(^{HR}\), aiding in spatial information during the fusion process. This process can be denoted as:

\[_{spe}=E_{}(^{LR}),_{spa}=E_{}( (_{up}^{LR},^{HR})), \]

where \(E_{}\) is the spectral encoder parameterized by \(\), \(E_{}\) is the spatial encoder parameterized by \(\), and \((_{up}^{LR},^{HR})\) denotes the concatenation along the channel dimension. In practice, we utilize EDSR  as INR encoder networks.

### Spatial-Frequency Implicit Fusion Function

To address the mentioned issues 2, we propose Spatial-Frequency Implicit Fusion Function, dubbed Spa-Fre IFF which is a dual-branch fusion function and utilized for computing the fusion feature of \(_{spe}\) and \(_{spa}\) in the spatial and frequency domains, respectively. Given a queried HR coordinate \(_{q}^{HR}\) of a pixel unit \(q\), Spa-Fre IFF estimates spatial feature vector \(_{s}^{1 1 S}\) (\(_{s}_{s}\)) and frequency feature vector \(_{f}^{1 1 S}\) (\(_{f}_{f}\)) as follows:

\[_{s},_{f}=( _{spe},_{spa},), \]

where \(_{spe}^{1 1 C}\) represents the spectral latent code vector corresponding to \(_{q}\), and \(_{spa}^{4 4 C}\) is the spatial latent code vector. \(\) denotes the set of local relative coordinates, expressed by the following formula:

\[=\{_{q}-_{q,i}\}_{i_{q}}\,, \]

where \(_{q,i}\) refers to the coordinates most proximate to the query coordinate \(_{q}\), representing the four corner pixels closest to \(q\) in the HR space.

Spatial Implicit Fusion FunctionThe Spatial Implicit Fusion Function aims to leverage the powerful representation capabilities of INR to achieve implicit fusion in the spatial domain, as shown in Fig. 3 (see branch "Spatial Domain"). Specifically, we employ high-pass operators \(\) to filter the spectral latent codes, as a complement to the high-frequency information on the spectrum:

\[_{hp}=(_{spe}), \]

where \(_{hp}^{1 1 C}\) represents the high-frequency latent code of \(^{LR}\). Also, we suggest frequency encoding for relative positional coordinates as follows:

\[()=[(2^{0}),(2^{0} ),,(2^{L-1}),(2^{L-1} )], \]

where \(L\) is a hyperparameter, in practice, we set \(L\) to 10. Additionally, leveraging the graph attention mechanism , we parameterize the solution for interpolation weights \(_{q,i}^{1 S}\), and the implicit fusion function simultaneously outputs fusion interpolation values \(_{q,i}^{4 4 S}\) and interpolation weights \(_{q,i}\). The implicit fusion function is specifically expressed as:

\[_{q,i},_{q,i}=_{}(_{spe},_{ spa},_{hp},()), \]

where \(_{}\) is an MLP parameterized by \(\). The weights used for interpolation need to pass through a softmax function, obtaining normalized weights \(}_{q,i}\). The spatial implicit fusion interpolation, as shown in Eq. (1), yields the fused spatial feature \(_{s}^{1 1 S}\) and can be described as follows:

\[_{s}=_{i_{q}}}_{ q,i}*_{q,i}. \]

Frequency Implicit Fusion FunctionFrom Fig. 2(a), we observed characteristics in the frequency features between LR-HSI and HR-HSI. Hence, we design a frequency implicit fusion function to express global features continuously in the Fourier domain. Notably, directly applying static kernel convolution in the frequency domain would only enhance a specific frequency range, which is inappropriate for the fusion task. However, by learning feature content to generate weights, INR can be seen as a dynamic interpolation method in continuous space, adaptively enhancing information in the frequency domain without overly altering the frequency distribution. Therefore, introducing INR into the Fourier domain is reasonable. Since amplitude and phase exhibit different forms, as shown in Fig. 2(a), we handle them separately.

With the considerations mentioned above, as illustrated in Fig. 3 (see branch "Fourier Domain"), we initially employ FFT to transform latent codes \(_{spe}\) and \(_{spa}\) from the spatial domain to the frequency domain, obtaining \(_{spe}^{1 1 C}\) and \(_{spa}^{4 4 C}\). After the transformation, we further obtain amplitude components \((_{spe})\) and \((_{spa})\), as well as phase components \((_{spe})\) and \((_{spa})\).

_For the amplitude_, as shown in Fig. 2(b), the amplitude distribution of LR-HSI and HR-HSI are very similar, and the non-point-wise convolution (_e.g._ Conv \(3 3\)) causes an issue of spectrum leakage, confusing channel information. In contrast, point-wise convolution does not span multiple locations in the frequency domain and has no overlap allowing it to capture information across channels effectively. Thus the fusion function for amplitude components is more suitable when applying point-wise convolution:

\[_{q,i}^{},_{q,i}^{}=_{}^{ }((_{spe}),(_{spa}), ), \]

where \(_{q,i}^{}^{1 S}\) and \(_{q,i}^{}^{4 4 S}\) are the weights and interpolated values for the corresponding amplitude component, and \(_{}^{}\) is a simple network composed of two layers of point convolutions parameterized by \(\). Similar to operations in the spatial domain, implicit fusion interpolation is performed after obtaining interpolated values \(_{q,i}^{}\) and the normalized weights \(}_{q,i}^{}\):

\[_{f}^{}=_{i_{q}}}_{q,i }^{}*_{q,i}^{}, \]

where \(_{f}^{}^{1 1 S}\) is the integrated amplitude component.

_For the phase_, which encapsulates information such as texture details, LR-HSI and HR-HSI often have different phase information. It is known that point convolutions fail to capture sufficient spatial representations. Therefore, we use a \(3 3\) convolution to learn phase information. Additionally, small changes in the frequency domain may result in significant variations in the spatial domain. We still consider using the form of INR interpolation for phase learning. The handling of the phase components \((_{spe})\) and \((_{spa})\) are formally similar to Eqs. (10) and (11):

\[_{q,i}^{},_{q,i}^{}=_{}^{ }((_{spe}),(_{spa}), ()),_{f}^{}=_{i_{q}} }_{q,i}^{}*_{q,i}^{}. \]

The simple network \(_{}^{}\) consists of two layers of \(3 3\) convolutions parameterized by \(\). \(_{f}^{}^{1 1 S}\) represents the integrated phase component.

Finally, IFFT is applied to map the frequency features \(_{f}^{}\) and \(_{f}^{}\) back to the image space, obtaining the frequency domain feature \(_{f}_{f}\). Since in frequency space, one frequency point may correspond to multiple pixels at different positions in the spatial domain, the receptive field of INR in the frequency domain is enlarged in the spatial domain.

### Spatial-Frequency Interactive Decoder

After obtaining the spatial feature map and frequency domain feature map, it is essential to consider how to integrate them seamlessly. Firstly, our decoder needs to have dual input and interactive capabilities. Secondly, it is necessary to focus on representing images in the spatial-frequency domain. With this in mind, we introduce the complex Gabor wavelet activation function with good time-frequency tightness and propose the Spatial-Frequency Interactive Decoder (SFID). Specifically, SFID consists of three layers, taking spatial and frequency domain features as inputs. The outputs \(_{r}^{HR}\) and \(_{up}^{HR}\) contribute to the final fused image \(}\). The decoding process is illustrated in Fig. 4. The complex Gabor wavelet function is defined as:

\[()=e^{j_{0}}e^{-|v_{0}|^{2}}, \]

where \(_{0}\) is the center frequency in the frequency domain, \(v_{0}\) is a constant that is considered as the standard deviation of the Gaussian function, and \(\) is a vector in the time (or spatial) domain. In what follows, we provide a theorem below that this Gabor wavelet activation has time-frequency tightness , which is helpful for the decoder's information interaction.

**Theorem 1**.: _The complex Gabor wavelet activation in Eq. (13) has the time-frequency tightness property (more preliminary can be found in ). Moreover, from the perspective of signal spectrum analysis, this activation helps the decoder learn the optimal bandwidths._

_Proof:_ First, for the time-domas, the function \(|()|\) in the time domain is primarily concentrated around \(=0\) due to the exponential decay term. The Gaussian term \(e^{-|v_{0}|^{2}}\) ensures that \(()\) is bounded and rapidly decreases in the time domain. Second, for the frequency-domain Tightness, the Fourier transform is given by:

\[[()]= e^{j_{0}}e^{-|v_{0} |^{2}}e^{-j}d. \]

The Fourier transform of the Gaussian term \(e^{-|v_{0}|^{2}}\) remains a Gaussian function, and its bandwidth in the frequency domain is influenced by \(v_{0}\). Due to the characteristics of the Gaussian function in the frequency domain, \(()\) is mainly concentrated around \(=_{0}\). Combining the narrow-bandwidth properties in both time and frequency domains, we can apply the uncertainty principle to demonstrate the time-frequency tightness of the complex Gabor function:

\[|_{0}| v_{0}, \]

where \(v_{0}\) is the time-domain bandwidth, and \(|_{0}|\) is the frequency-domain bandwidth. In practical training, we provide an initial set of bandwidths and allow the network to learn the optimal bandwidths, which concludes the proof. \(\)

As depicted in Fig. 5, the frequency response of the decoder with Gabor wavelet activation closely approximates the optimal bandwidth. Moreover, the decoder with Gabor activation achieves consistency with GT in frequency, demonstrating rapid frequency alignment.

Figure 4: Detailed composition of the proposed SFID.

Figure 5: **The complex Gabor wavelet function.** (a) and (b) depict the visualization of the complex Gabor wavelet function. (c), (d), and (e) represent the frequency responses of GT and decoder’s mean feature using Gabor and regular ReLU activations, respectively.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

data is illustrated in Fig. 7. Our FeINFN, which incorporates Fourier domain fusion, leads to faster PSNR convergence and overall higher efficiency. The visual comparison of high-frequency details in "_chart and stuffed toy_" from the cave dataset at 80k iterations further supports the significant improvement achieved with our results.

Decoder with Different NonlinearIn this section, we evaluate the impact of different activation functions in SFID, aiming to match SFIFF. Our dual-input decoder incorporates a complex Gabor wavelet activation function to facilitate the fusion of spatial and frequency domain features.

Through experiments, we replaced the Gabor wavelet activation with other activations, presenting the results in Tab. 4. The findings distinctly demonstrate the enhanced fusion quality achieved with the complex Gabor wavelet activation. This emphasizes the critical role of wavelet activation in promoting robust and reliable learning in SFID.

## 5 Conclusion

Inspired by the distinct behaviors of LR-HSI and HR-HSI in the Fourier domain, we introduce a novel Fourier-enhanced Implicit Neural Fusion Network (FeINFN) based on INR. Through Fourier transformation, latent features are converted into the frequency domain, allowing the modeling of frequency components to enrich high-frequency information in images. Additionally, we propose a spatial-frequency decoding module, achieving a unified representation of both spatial and frequency domains using a time-frequency-tight activation function. Thanks to the unique design of our network, it outperforms state-of-the-art methods in MHIF with appealing efficiency. We desire that our work will inspire future research on frequency fusion-based MHIF methods.

## 6 Acknowledgement

This work is supported by the National Natural Science Foundation of China under Grants 12271083, 12171072 and Natural Science Foundation of Sichuan Province under Grants 2023NSFSC1341.