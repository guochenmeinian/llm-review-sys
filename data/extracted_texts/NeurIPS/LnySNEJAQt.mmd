# Flow Factorized Representation Learning

Yue Song

T. Anderson Keller

Nicu Sebe

Department of Information Engineering and Computer Science, University of Trento, Italy Amsterdam Machine Learning Lab, University of Amsterdam, the Netherlands

Max Welling

###### Abstract

A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both _disentanglement_ and _equivariance_. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.

## 1 Introduction

Developing models which learn useful representations of data has become an increasingly important focus in the machine learning community [5; 55]. For example, Large Language Models such as GPT  rely on an extensive pre-training phase to learn valuable representations, enabling quick finetuning on a diversity of tasks. However, a precise definition of what makes an ideal representation is still debated. One line of work has focused on 'disentanglement' of the underlying ground truth generative factors [5; 35; 13]. In general, the definition of 'disentanglement' often refers to learning and controlling statistically independent factors of variation [5; 36]. Over the years, many disentanglement methods have been proposed, including axis-aligned single-dimensional manipulation [35; 13], linear multi-dimensional traversals [78; 77; 90; 66], and, more recently, dynamic non-linear vector-based traversals [84; 79]. Although these methods have been met with significant success (and even linked to single-neuron brain activity [37; 91]), there are known theoretical limitations which make them ill-specified, including the presence of topological defects . This has limited their deployment beyond toy settings.

Another line of work has focused on developing representations which respect symmetries of the underlying data in their output space [15; 36]. Specifically, equivariant representations are those for which the output transforms in a known predictable way for a given input transformation.

They can be seen to share many similarities with disentangled representations since an object undergoing a transformation which preserves its identity can be called a symmetry transformation . Compared with disentanglement methods, equivariant networks are much more strictly defined, allowing for significantly greater control and theoretical guarantees with respect to the learned transformation [16; 50; 73; 20; 39]. However, this restriction also limits the types of transformations to which they may be applied. For example, currently only group transformations are supported, limiting real-world applicability. To avoid this caveat, some recent attempts propose to learn general but approximate equivariance from disentangled representations [49; 45; 79].

In this work, we provide an alternative viewpoint at the intersection of these two fields of work which we call Flow Factorized Representation Learning. Fig. 1 depicts the high-level illustration of our method. Given \(k\) different transformations \(p_{k}(_{t}|_{0})\) in the input space, we have the corresponding latent probabilistic path \(_{_{0},_{t}}q(_{0}|_{0})q_{k}(_{t}|_{ 0})p(_{t}|_{t})\) for each of the transformations. Each latent flow path \(q_{k}(_{t}|_{0})\) is generated by the gradient field of some learned potentials \( u^{k}\) following fluid mechanical dynamic Optimal Transport (OT) . Our framework allows for novel understandings of both _disentanglement_ and _equivariance_. The definition of disentanglement refers to the distinct set of tangent directions \( u^{k}\) that follow the OT paths to generate latent flows for modeling different factors of variation. The concept of equivariance in our case means that the two probabilistic paths, _i.e.,_\(p_{k}(_{t}|_{0})\) in the image space and \(_{_{0},_{t}}q(_{0}|_{0})q_{k}(_{t}|_{ 0})p(_{t}|_{t})\) in the latent space, would eventually result in the same distribution of transformed data.

We build a formal generative model of sequences and integrate the above latent probability evolution as condition updates of the factorized sequence distribution. Based on the continuity equation, we derive a proper flow of probability density for the time evolution of both the prior and posterior. To perform inference, we approximate the true posterior of latent variables and train the parameters as a Variational Autoencoder (VAE) . When the transformation type \(k\) is not observed (_i.e.,_ available as a label), we treat \(k\) as another latent variable and incorporate its posterior into our framework by learning it from sequences. Extensive experiments and thorough analyses have been conducted to show the effectiveness of our method. For example, we demonstrate empirically that our representations are usefully factorized, allowing flexible composability and generalization to new datasets. Furthermore, we show that our methods are also approximately equivariant by demonstrating that they commute with input transformations through the learned latent flows. Ultimately, we see these factors combine to yield the highest likelihood on the test set in each setting. Code is publicly available at https://github.com/KingJamesSong/latent-flow.

## 2 The generative model

In this section, we first introduce our generative model of sequences and then describe how we perform inference over the latent variables of this model in the next section.

### Flow factorized sequence distributions

The model in this work defines a distribution over sequences of observed variables. We further factorize this distribution into \(k\) distinct components by assuming that each observed sequence is generated by one of the \(k\) separate flows of probability mass in latent space. Since in this work we

Figure 1: Illustration of our flow factorized representation learning: at each point in the latent space we have a distinct set of tangent directions \( u^{k}\) which define different transformations we would like to model in the image space. For each path, the latent sample evolves to the target on the potential landscape following dynamic optimal transport.

model discrete sequences of observations \(}=\{_{0},_{1},_{T}\}\), we aim to define a joint distribution with a similarly discrete sequence of latent variables \(}=\{_{0},_{1},_{T}\}\), and a categorical random variable \(k\) describing the sequence type (observed or unobserved). Explicitly, we assert the following factorization of the joint distribution over \(T\) timesteps:

\[p(},},k)=p(k)p(_{0})p(_{0}|_{0})_{ t=1}^{T}p(_{t}|_{t-1},k)p(_{t}|_{t}).\] (1)

Here \(p(k)\) is a categorical distribution defining the transformation type, \(p(_{t}|_{t})\) asserts a mapping from latents to observations with Gaussian noise, and \(p(_{0})=(0,1)\). A plate diagram of this model is depicted through the solid lines in Fig. 2.

### Prior time evolution

To enforce that the time dynamics of the sequence define a proper flow of probability density, we compute the conditional update \(p(_{t}|_{t-1},k)\) from the continuous form of the continuity equation: \(_{t}p()=-(p()^{k}())\), where \(^{k}()\) is the \(k\)'th potential function which advects the density \(p()\) through the induced velocity field \(^{k}()\). Considering the discrete particle evolution corresponding to this density evolution, \(_{t}=f(_{t-1},k)=_{t-1}+_{z}^{k}(_{t-1})\), we see that we can derive the conditional update from the continuous change of variables formula [69; 11]:

\[p(_{t}|_{t-1},k)=p(_{t-1})_{t-1},k)}{d _{t-1}}^{-1}\] (2)

In this setting, we see that the choice of \(\) ultimately determines the prior on the transition probability in our model. As a minimally informative prior for random trajectories, we use a diffusion equation achieved by simply taking \(^{k}=-D_{k} p(_{t})\). Then according to the continuity equation, the prior evolves as:

\[_{t}p(_{t})=-p(_{t})=D _{k}^{2}p(_{t})\] (3)

where \(D_{k}\) is a constant coefficient that does not change over time. The density evolution of the prior distribution thus follows a constant diffusion process. We set \(D_{k}\) as a learnable parameter which is distinct for each \(k\).

## 3 Flow factorized variational autoencoders

To perform inference over the unobserved variables in our model, we propose to use a variational approximation to the true posterior, and train the parameters of the model as a VAE. To do this, we parameterize an approximate posterior for \(p(_{0}|_{0})\), and additionally parameterize a set of

Figure 2: Depiction of our model in plate notation. (Left) Supervised, (Right) Weakly-supervised. White nodes denote latent variables, shaded nodes denote observed variables, solid lines denote the generative model, and dashed lines denote the approximate posterior. We see, as in a standard VAE framework, our model approximates the initial one-step posterior \(p(_{0}|_{0})\), but additionally approximates the conditional transition distribution \(p(_{t}|_{t-1},k)\) through dynamic optimal transport over a potential landscape.

functions \(u^{k}()\) to approximate the true latent potentials \(^{*}\). First, we will describe how we do this in the setting where the categorical random variable \(k\) is observed (which we call the supervised setting), then we will describe the model when \(k\) is also latent and thus additionally inferred (which we call the weakly supervised setting).

### Inference with observed \(k\) (supervised)

When \(k\) is observed, we define our approximate posterior to factorize as follows:

\[q(}|},k)=q(_{0}|_{0})_{t=1}^{T}q(_ {t}|_{t-1},k)\] (4)

We see that, in effect, our approximate posterior only considers information from element \(_{0}\); however, combined with supervision in the form of \(k\), we find this is sufficient for the posterior to be able to accurately model full latent sequences. In the limitations section we discuss how the posterior could be changed to include all elements \(\{_{t}\}_{0}^{T}\) in future work.

Combing Eq. (4) with Eq. (1), we can derive the following lower bound to model evidence (ELBO):

\[ p(}|k) =_{q_{}(}|},k)}[ },}|k)}{q(}|},k)}}|},k)}{p(}|},k)}]\] \[_{q_{}(}|},k)}[ }|},k)p(}|k)}{q(}| {},k)}]\] (5) \[=_{q_{}(}|},k)}[ p (}|},k)]+_{q_{}(}| {},k)}[}|k)}{q(}|},k)}]\]

Substituting and simplifying, Eq. (5) can be re-written as

\[ p(}|k) _{t=0}^{T}_{q_{}(}|k)}  p(_{t}|_{t},k)-_{q_{}(}|k) }_{}[q_{}(_{0}|_{0})||p(_{0})]\] (6) \[-_{t=1}^{T}_{q_{}(}|k)} _{}[q_{}(_{t}|_{t-1},k)||p(_{t }|_{t-1},k)]\]

We thus see that we have an objective very similar to that of a traditional VAE, except that our posterior and our prior now both have a time evolution defined by the conditional distributions.

### Inference with latent \(k\) (weakly supervised)

When \(k\) is not observed, we can treat it as another latent variable, and simultaneously perform inference over it in addition to the sequential latent \(}\). To achieve this, we define our approximate posterior and instead factorize it as

\[q(},k|})=q(k|})q(_{0}|_{0})_ {t=1}^{T}q(_{t}|_{t-1},k)\] (7)

Following a similar procedure as in the supervised setting, we derive the new ELBO as

\[ p(}) =_{q_{}(},k|})}[ },},k)}{q(},k|})}},k|})}{p(},k|})}]\] \[_{q_{}(},k|})}[ }|},k)p(}|k)}{q(}|},k)}})}]\] (8) \[=_{q_{}(},k|})}[ p (}|},k)]+_{q_{}(},k| })}[}|k)}{q(}|},k )}]+_{q_{}(k|})}[})}]\]

We see that, compared with Eq. (5), only one additional KL divergence term \(_{}[q_{}(k|})||p(k)]\) is added. The prior \(p(k)\) is set to follow a categorical distribution, and we apply the \(\) trick  to allow for categorical re-parameterization and sampling of \(q_{}(k|})\).

### Posterior time evolution

As noted, to approximate the true generative model which has some unknown latent potentials \(^{k}\), we propose to parameterize a set of potentials as \(u^{k}(,t)=([;t])\) and train them through the ELBOs above. Again, we use the continuity equation to define the time evolution of the posterior, and thus we can derive the conditional time update \(q(_{t}|_{t-1},k)\) through the change of variables formula. Given the function of the sample evolution \(_{t}=g(_{t-1},k)=_{t-1}+_{}u^{k}\), we have:

\[q(_{t}|_{t-1},k)=q(_{t-1})_{t-1},k)}{d _{t-1}}^{-1}\] (9)

Converting the above continuous equation to the discrete setting and taking the logarithm of both sides gives the normalizing-flow-like density evolution of our posterior:

\[ q(_{t}|_{t-1},k)= q(_{t-1})-|1+_{}^ {2}u^{k}|\] (10)

The above relation can be equivalently derived from the continuity equation (_i.e.,_\(_{t}q()=-q() u^{k}\)). Notice that we only assume the initial posterior \(q(_{0}|_{0})\) follows a Gaussian distribution. For future timesteps, we do not pose any further assumptions and just let the density evolve according to the sample motion.

### Ensuring optimal transport of the posterior flow

As an inductive bias, we would like each latent posterior flow to follow the OT path. To accomplish this, it is known that when the gradient \( u^{k}\) satisfies certain PDEs, the evolution of the probability density can be seen to minimize the \(L_{2}\) Wasserstein distance between the source distribution and the distribution of the target transformation. Specifically, we have:

**Theorem 1** (Benamou-Brenier Formula ).: _For probability measures \(_{0}\) and \(_{1}\), the \(L_{2}\) Wasserstein distance can be defined as_

\[W_{2}(_{0},_{1})^{2}=_{,v}\{(x,t)|v( x,t)|^{2}\,dx\,dt\}\] (11)

_where the density \(\) and the velocity \(v\) satisfy:_

\[=-(v(x,t)(x,t)),\;v(x,t)= u(x,t)\] (12)

The optimality condition of the velocity is given by the generalized Hamilton-Jacobi (HJ) equation (_i.e.,_\(_{t}u+}{{2}}|| u||^{2} 0\)). The detailed derivation is deferred to the supplementary. We thus encourage our potential to satisfy the HJ equation with an external driving force as

\[u^{k}(,t)+||_{}u^{k} (,t)||^{2}=f(,t)\;\;\;\;\;\;f(,t) 0\] (13)

Here we use another MLP to parameterize the external force \(f(,t)\) and realize the negativity constraint by setting \(f(,t)=-([;t])^{2}\). Notice that here we take the external force as learnable MLPs simply because we would like to obtain a flexible negativity constraint. The MLP architecture is set the same for both \(u(,t)\) and \(f(,t)\). To achieve the PDE constraint, we impose a Physics-Informed Neural Network (PINN)  loss as

\[_{HJ}=_{t=1}^{T}u^ {k}(,t)+||_{}u^{k}(,t)||^{2}-f(,t) ^{2}+|| u^{k}(_{0},0)||^{2}\] (14)

where the first term restricts the potential to obey the HJ equation, and the second term limits \(u(_{t},t)\) to return no update at \(t\)=0, therefore matching the initial condition.

## 4 Experiments

This section starts with the experimental setup, followed by the main qualitative and quantitative results, then proceeds to discussions about the generalization ability to different composability and unseen data, and ends with the results on complex real-world datasets.

[MISSING_PAGE_FAIL:6]

**Metrics.** We use the approximate equivariance error \(_{k}\) and the log-likelihood of transformed data \( p(_{t})\) as the evaluation protocols. The equivariance error is defined as \(_{k}=_{t=1}^{T}|_{t}-(_{t})|\) where \(_{t}=_{0}+_{t=1}^{T}_{}u^{k}\). For TVAE, the latent operator is changed to \((_{0},t)\). For unsupervised disentanglement baselines [35; 46] and SlowVAE , we carefully select the latent dimension and tune the interpolation range to attain the traversal direction and range that correspond to the smallest equivariance error. Since the vanilla VAE does not have the corresponding learned transformation in the latent space, we simply set \(_{}u^{k}=0\) and take it as a lower-bound baseline. For all the methods, the results are reported based on \(5\) runs.

Notice that the above equivariance error is defined in the output space. Another reasonable evaluation metric is instead measuring error in the latent space as \(_{k}=_{t=1}^{T}|(_{t})-_{t}|\). We see the first evaluation method is more comprehensive as it further involves the decoder in the evaluation.

### Main Results

**Qualitative results.** Fig. 3 and 4 display decoded images of the latent evolution on MNIST  and Shapes3D , respectively. On both datasets, our latent flow can perform the target transformation precisely during evolution while leaving other traits of the image unaffected. In particular, for the weakly-supervised setting, the decoded images (_i.e.,_ the bottom rows of Fig. 3 and 4) can still reproduce the given transformations well and it is even hard to visually tell them apart from the generated images under the supervised setting. This demonstrates the effectiveness of the weakly-supervised setting of our method, and implies that qualitatively our latent flow is able to learn the sequence transformations well under both supervised and weakly-supervised settings.

**Quantitative results.** Tables 1 and 2 compare the equivariance error and the log-likelihood on MNIST  and Shapes3D , respectively. Our method learns the latent flows which model the transformations precisely, achieving the best performance across datasets under different supervision settings. Specifically, our method outperforms the previous best baseline by \(69.74\) on average in the equivariance error and by \(32.58\) in the log-likelihood on MNIST. The performance gain is also consistent on Shapes3D: our method surpasses the second-best baseline by \(291.70\) in the average equivariance error and by \(120.42\) in the log-likelihood. In the weakly-supervised setting, our method also achieves very competitive performance, falling behind that of the supervised setting in the average equivariance error slightly by \(6.22\) on MNIST and by \(67.88\) on Shapes3D.

### Discussion

**Extrapolation: switching transformations.** In Fig. 5 we demonstrate that, empowered by our method, it is possible to switch latent transformation categories mid-way through the latent evolution

    &  &  &  \\    & & & **Scaling** & **Rotation** & **Coloring** \\  VAE  & No (✗) & 1275.31\(\)1.89 & 1310.72\(\)2.19 & 1368.92\(\)2.33 & -2206.17\(\)1.83 \\ \(\)-**VAE** & No (✗) & 741.58\(\)4.57 & 751.32\(\)5.22 & 808.16\(\)5.03 & -2224.67\(\)2.35 \\
**FactorVAE** & No (✗) & 695.71\(\)4.89 & 632.44\(\)5.76 & 662.18\(\)5.26 & -2209.33\(\)2.47 \\
**SlowVAE** & Weak (✗) & 461.59\(\)5.37 & 447.46\(\)5.46 & 398.12\(\)4.83 & -2197.68\(\)2.39 \\
**TVAE** & Yes (✗) & 505.19\(\)2.77 & 493.28\(\)3.37 & 451.25\(\)2.76 & -2181.13\(\)1.87 \\
**PoFlow** & Yes (✗) & 234.78\(\)2.91 & 231.42\(\)2.98 & 240.57\(\)2.58 & -2145.03\(\)2.01 \\
**Ours** & Yes (✗) & **185.42\(\)2.35** & **153.54\(\)3.10** & **158.57\(\)2.95** & **-2112.45\(\)1.57** \\
**Ours** & Weak (✗) & 193.84\(\)2.47 & 157.16\(\)3.24 & 165.19\(\)2.78 & -2119.94\(\)1.76 \\   

Table 1: Equivariance error \(_{k}\) and log-likelihood \( p(_{t})\) on MNIST .

    &  &  &  \\    & & **Floor Hue** & & **Wall Due** & **Object Hue** & **Scale** \\ 
**VAE** & No (✗) & 6924.63\(\)8.92 & 7746.37\(\)8.77 & 433.54\(\)9.26 & 2609.59\(\)7.41 & -11784.69\(\)4.87 \\ \(\)-**VAE** & No (✗) & 2243.95\(\)12.48 & 2279.23\(\)13.97 & 2188.73\(\)12.61 & 2037.94\(\)11.72 & -11924.83\(\)5.64 \\
**FactorVAE** & No (✗) & 1985.75\(\)13.26 & 1876.41\(\)11.93 & 1902.83\(\)12.27 & 1657.32\(\)11.05 & -11802.17\(\)5.69 \\
**SlowVAE** & Weak (✗) & 1247.36\(\)12.49 & 1314.86\(\)11.41 & 1102.82\(\)12.17 & 1058.74\(\)10.96 & -11674.89\(\)5.74 \\
**TVAE** & Yes (✗) & 1257.36\(\)9.82 & 1266.32\(\)9.54 & 261.79\(\)9.86 & 1142.40\(\)9.37 & -11475.48\(\)5.18 \\
**PoFlow** & Yes (✗) & 885.46\(\)10.37 & 916.17\(\)10.49 & 912.48\(\)9.86 & 924.39\(\)10.05 & -11335.84\(\)4.95 \\
**Ours** & Yes (✗) & **613.29\(\)8.93** & **65.345\(\)9.48** & **605.79\(\)8.63** & **599.71\(\)9.34** & **41215.42\(\)5.71** \\
**Ours** & Weak (✗) & 690.84\(\)9.57 & 717.74\(\)10.65 & 681.59\(\)9.02 & 665.358\(\)9.57 & -11279.61\(\)5.89 \\   

Table 2: Equivariance error \(_{k}\) and log-likelihood \( p(_{t})\) on Shapes3D .

and maintain coherence. That is, we perform \(_{t}=_{t-1}+_{}u^{k}\) for \(t}{{2}}\) and then change to \(_{t}=_{t-1}+_{}u^{j}\) where \(j k\) for \(t>}{{2}}\). As can be seen, the factor of variation immediately changes after the transformation type is switched. Moreover, the transition phase is smooth and no other attributes of the image are influenced.

**Extrapolation: superposing transformations.** Besides switching transformations, our method also supports applying different transformations simultaneously, _i.e.,_ consistently performing \(_{t}=_{t-1}+_{k}^{K}_{}u^{k}\) during the latent flow process. Fig. 6 presents such exemplary visualizations of superposing two and all transformations simultaneously. In each case, the latent evolution corresponds to simultaneous smooth variations of multiple image attributes. This indicates that our method also generalizes well to superposing different transformations.

Notice that we only apply single and separate transformations in the training stage. Switching or superposing transformations in the test phase can be thus understood as an extrapolation test to measure the generalization ability of the learned equivariance to novel compositions.

**Equivariance generalization to new data.** We also test whether the learned equivariance holds for Out-of-Distribution (OoD) data. To verify this, we validate our method on a test dataset that is different from the training set and therefore unseen to the model. Fig. 7 displays the exemplary visualization results of the VAE trained on MNIST  but evaluated on dSprites . Although the reconstruction quality is poor, the learned equivariance is still clearly effective as each transformation still operates as expected: scaling, rotation, and coloring transformations from top to bottom respectively.

### Results on Complex Real-world and Large-scale Datasets

Table 3 and 4 compare the equivariance error of our methods and the representative baselines on Falcol3D and Isaac3D, respectively. Notice that the values are much larger than previous datasets due to the increased image resolution. Our method still outperforms other baselines by a large margin

Figure 5: Exemplary visualization of switching transformations during the latent sample evolution.

Figure 6: Examples of combining different transformations simultaneously during the latent evolution.

and achieves reasonable equivariance error. Fig. 8 displays the qualitative comparisons of our method against other baselines. Our method precisely can control the image transformations through our latent flows. _Overall, the above results demonstrate that our method can go beyond the toy setting and can be further applied to more complex real-world scenarios._

More visualization results of exemplary latent flows are kindly referred to in the supplementary.

## 5 Related work

**Disentangled representation learning.** The idea of learning disentangled representation dates back to factorizing non-redundant input patterns  but is recently first studied by InfoGAN  and \(\)-VAE . InfoGAN  achieves disentanglement by maximizing the mutual information between a subset of latent dimensions and observations, while \(\)-VAE  induces the factorized posterior \(q()\) by penalizing the Total Correlation (TC) through an extra hyper-parameter \(\)\(>\)\(1\) controlling the strength of the KL divergence. Following infoGAN, many attempts have been made to facilitate the discovery of semantically meaningful traversal directions through regularization [33; 42; 89; 34; 100; 66; 77; 90; 98; 84; 99; 78; 62]. The follow-up research of \(\)-VAE mainly explored different methods to factorize the aggregated posterior [22; 25; 52; 46; 12; 44; 96; 23; 76; 58; 80; 28]. More recently, some works proposed to discover meaningful directions of diffusion models in the bottleneck of denoising networks [53; 64; 95; 41]. The previous literature mainly considers disentanglement as learning different transformations per dimension or per linear direction. Our method generalizes this concept to learning a distinct tangent bundle \( u^{k}\) that moves every latent sample via dynamic OT.

We see the most similar method to ours is the work of . In , the authors also apply the gradient of a potential function to move the latent code. However, their potentials are restricted to obey the wave equations, which do not really correspond to the OT theory. Also, they do not consider the posterior evolution but instead use the loss \(||_{t}-(_{t})||^{2}\) to match the latent codes. By contrast, we propose a unified probabilistic generative model that encompasses the posterior flow that follows dynamic OT, the flow-like time evolution, and different supervision settings.

**Equivariant neural networks.** A function is said to be an equivariant map if it commutes with a given transformation, _i.e.,_\(T^{}[f(x)]=f(T[x])\) where \(T\) and \(T^{}\) represent operators in different domains. Equivariance has been considered a desired inductive bias for deep neural networks as this property can preserve geometric symmetries of the input space [38; 75; 56; 57; 1]. Analytically equivariant

 
**Methods** & **Robot X-move** & **Robot Y-move** & **Camera Height** & **Object Scale** & **Lighting Intensity** & **Lighting Y-dir** & **Object Color** & **Wall Color** \\ 
**TVAE** & 8441.65 & 8348.23 & 8495.31 & 8251.34 & 8291.70 & 8741.67 & 8456.78 & 8512.09 \\
**PoFlow** & 6572.19 & 6489.35 & 6319.82 & 6188.59 & 6517.40 & 6712.06 & 7056.98 & 6343.76 \\
**Ours** & **3695.72** & **3993.33** & **4712.27** & **4398.78** & **4225.34** & **4098.84** & **5814.97** & **3876.801** \\   

Table 4: Equivariance error (\(\)) on Isaac3D .

Figure 7: Equivariance generalization to unseen OoD input data. Here the model is trained on MNIST  but the latent flow is tested on dSprites .

networks typically enforce explicit symmetry to group transformations in neural networks [16; 17; 68; 93; 92; 85; 31; 39]. Another line of research proposed to directly learn approximate equivariance from data [21; 18; 49; 20; 45]. Our framework re-defines approximate equivariance by matching the latent probabilistic flow to the actual path of the given transformation in the image space.

**Optimal transport in deep learning.** There is a vast literature on OT theory and applications in various fields [87; 88]. Here we mainly highlight the relevant applications in deep learning. The pioneering work of  proposed a light-speed implementation of the Sinkhorn algorithm for fast computation of entropy-regularized Wasserstein distances, which opened the way for many differentiable Sinkhorn algorithm-based applications [32; 29; 14; 27; 51]. In generative modeling, the Wasserstein distance is often used to minimize the discrepancy between the data distribution and the model distribution [2; 81; 72; 65]. Inspired by the fluid mechanical interpretation of OT , some normalizing flow methods [69; 24; 48] considered regularizing the velocity fields to satisfy the HJ equation, thus matching the dynamic OT plan [94; 30; 83; 63; 60]. Our method applies PINNs  to directly model generalized HJ equations in the latent space and uses the gradient fields of learned potentials to generate latent flows, which also aligns to the theory of dynamic fluid mechanical OT.

## 6 Conclusion

In this paper, we introduce Flow Factorized Representation Learning which defines a set of latent flow paths that correspond to sequences of different input transformations. The latent evolution is generated by the gradient flow of learned potentials following dynamic optimal transport. Our setup re-interprets the concepts of both _disentanglement_ and _equivariance_. Extensive experiments demonstrate that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously achieving smaller equivariance error. Furthermore, we show that the learned latent transformations generalize well, allowing for flexible composition and extrapolation to new data.

## 7 Limitations

For flexibility and efficiency, we use PINN  constraints to model the HJ equation. However, such PDE constraints are approximate and not strictly enforced. Other PDE modeling approaches include accurate neural PDE solvers [40; 8; 70] or other improved PINN variants such as competitive PINNs  and robust PINNs . Also, when infering with observed \(k\), we change the posterior from \(q(}|},k)\) to \(q(}|_{0},k)\) because we assume \(k\) contains sufficient information of the whole sequence. To keep the posterior definition of \(q(}|},k)\), we need to make \(q(_{t})\) also a function of \(_{t}\). This can be achieved either by changing the potential to \(u(_{t-1},_{t},t-1)\) or modifying the external driving force to \(f(_{t-1},_{t},t-1)\). Nonetheless, we see these modifications would make the model less flexible than our current formulations as the element \(_{t}\) might be needed during inference.