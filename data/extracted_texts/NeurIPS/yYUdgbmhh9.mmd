# Star-Shaped Denoising Diffusion Probabilistic Models

Andrey Okhotin

HSE University, MSU University

Moscow, Russia

andrey.okhotin@gmail.com

&Dmitry Molchanov

BAYESG

Budva, Montenegro

dmolch111@gmail.com

&Vladimir Arkhipkin

Sber AI

Moscow, Russia

arkhipkin.v98@gmail.com

&Grigory Bartosh

AMLab, Informatics Institute

University of Amsterdam

Amsterdam, Netherlands

g.bartosh@uva.nl

&Viktor Ohanesian

Independent Researcher

v.v.oganesyan@gmail.com

&Aibek Alanov

AIRI, HSE University

Moscow, Russia

alanov.aibek@gmail.com

&Dmitry Vetrov

Constructor University

Bremen, Germany

dvetrov@constructor.university

Equal contribution

###### Abstract

Denoising Diffusion Probabilistic Models (DDPMs) provide the foundation for the recent breakthroughs in generative modeling. Their Markovian structure makes it difficult to define DDPMs with distributions other than Gaussian or discrete. In this paper, we introduce Star-Shaped DDPM (SS-DDPM). Its _star-shaped diffusion process_ allows us to bypass the need to define the transition probabilities or compute posteriors. We establish duality between star-shaped and specific Markovian diffusions for the exponential family of distributions and derive efficient algorithms for training and sampling from SS-DDPMs. In the case of Gaussian distributions, SS-DDPM is equivalent to DDPM. However, SS-DDPMs provide a simple recipe for designing diffusion models with distributions such as Beta, von Mises-Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold. We evaluate the model in different settings and find it competitive even on image data, where Beta SS-DDPM achieves results comparable to a Gaussian DDPM. Our implementation is available at https://github.com/andrey-okhotin/star-shaped.

## 1 Introduction

Deep generative models have shown outstanding sample quality in a wide variety of modalities. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Karras et al., 2021), autoregressive models (Ramesh et al., 2021), Variational Autoencoders (Kingma and Welling, 2013; Rezende et al., 2014), Normalizing Flows (Grathwohl et al., 2018; Chen et al., 2019) and energy-based models (Xiao et al., 2020) show impressive abilities to synthesize objects. However, GANs are not robust to the choice of architecture and optimization method (Arjovsky et al., 2017; Gulrajani et al., 2017; Karras et al., 2019; Brock et al., 2018), and they often fail to cover modes in data distribution (Zhao et al., 2018; Thanh-Tung and Tran, 2020). Likelihood-based models avoid mode collapse but may overestimate the probability in low-density regions (Zhang et al., 2021).

Recently, diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have received a lot of attention. These models generate samples using a trained Markov process that starts with white noise and iteratively removes noise from the sample. Recent works have shown that diffusion models can generate samples comparable in quality or even better than GANs (Song et al., 2020; Dhariwal and Nichol, 2021), while they do not suffer from mode collapse by design, and also they have a log-likelihood comparable to autoregressive models (Kingma et al., 2021). Moreover, diffusion models show these results in various modalities such as images (Saharia et al., 2021), sound (Popov et al., 2021; Liu et al., 2022) and shapes (Luo and Hu, 2021; Zhou et al., 2021).

The main principle of diffusion models is to destroy information during the forward process and then restore it during the reverse process. In conventional diffusion models like denoising diffusion probabilistic models (DDPM) destruction of information occurs through the injection of Gaussian noise, which is reasonable for some types of data, such as images. However, for data distributed on manifolds, bounded volumes, or with other features, the injection of Gaussian noise can be unnatural, breaking the data structure. Unfortunately, it is not clear how to replace the noise distribution within traditional diffusion models. The problem is that we have to maintain a connection between the distributions defining the Markov noising process that gradually destroys information and its marginal distributions. While some papers explore other distributions, such as delta functions (Bansal et al., 2022) or Gamma distribution (Nachmani et al., 2021), they provide ad hoc solutions for special cases that are not easily generalized.

In this paper, we present Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), a new approach that generalizes Gaussian DDPM to an exponential family of noise distributions. In SS-DDPM, one only needs to define marginal distributions at each diffusion step (see Figure 1). We provide a derivation of SS-DDPM, design efficient sampling and training algorithms, and show its equivalence to DDPM (Ho et al., 2020) in the case of Gaussian noise. Then, we outline a number of practical considerations that aid in training and applying SS-DDPMs. In Section 5, we demonstrate the ability of SS-DDPM to work with distributions like von Mises-Fisher, Dirichlet and Wishart. Finally, we evaluate SS-DDPM on image and text generation. Categorical SS-DDPM matches the performance of Multinomial Text Diffusion (Hoogeboom et al., 2021) on the text8 dataset, while our Beta diffusion model achieves results, comparable to a Gaussian DDPM on CIFAR-10.

## 2 Theory

### DDPMs

We start with a brief introduction of DDPMs. The Gaussian DDPM (Ho et al., 2020) is defined as a forward (diffusion) process \(q^{}(x_{0:T})\) and a corresponding reverse (denoising) process \(p_{}^{}(x_{0:T})\). The forward process is defined as a Markov chain with Gaussian conditionals:

\[q^{}(x_{0:T}) =q(x_{0}){_{t=1}^{T}}q^{}(x_{t}|x_{t-1}),\] (1) \[q^{}(x_{t}|x_{t-1}) =(x_{t};}x_{t-1},_{t} {I}),\] (2)

where \(q(x_{0})\) is the data distribution. Parameters \(_{t}\) are typically chosen in advance and fixed, defining the noise schedule of the diffusion process. The noise schedule is chosen in such a way that the final \(x_{T}\) no longer depends on \(x_{0}\) and follows a standard Gaussian distribution \(q^{}(x_{T})=(x_{T};0,)\)

Figure 1: Markovian forward processes of DDPM (left) and the star-shaped forward process of SS-DDPM (right).

The reverse process \(p_{}^{}(x_{0:T})\) follows a similar structure and constitutes the generative part of the model:

\[p_{}^{}(x_{0:T}) =q^{}(x_{T})}p_{ }^{}(x_{t-1}|x_{t}),\] (3) \[p_{}^{}(x_{t-1}|x_{t}) =(x_{t-1};_{}(x_{t},t),_{}(x _{t},t)).\] (4)

The forward process \(q^{}(x_{0:T})\) of DDPM is typically fixed, and all the parameters of the model are contained in the generative part of the model \(p_{}^{}(x_{0:T})\). These parameters are tuned to maximize the variational lower bound (VLB) on the likelihood of the training data:

\[^{}() =_{q^{}}[ p_{}^{}(x_{0}|x_{1})-}D_{KL}(q^{} (x_{t-1}|x_{t},x_{0})\,\|\,p_{}^{}(x_{t-1}|x_{t}))]\] (5) \[^{}() \] (6)

One of the main challenges in defining DDPMs is the computation of the posterior \(q^{}(x_{t-1}|x_{t},x_{0})\). Specifically, the transition probabilities \(q^{}(x_{t}|x_{t-1})\) have to be defined in such a way that this posterior is tractable. Specific DDPM-like models are available for Gaussian (Ho et al., 2020), Categorical (Hoogeboom et al., 2021) and Gamma (Kawar et al., 2022) distributions. Defining such models remains challenging in more general cases.

### Star-Shaped DDPMs

As previously discussed, extending the DDPMs to other distributions poses significant challenges. In light of these difficulties, we propose to construct a model that only relies on marginal distributions \(q(x_{t}|x_{0})\) in its definition and the derivation of the loss function.

We define star-shaped diffusion as a _non-Markovian_ forward process \(q^{}(x_{0:T})\) that has the following structure:

\[q^{}(x_{0:T})=q(x_{0})}q^ {}(x_{t}|x_{0}),\] (7)

where \(q(x_{0})\) is the data distribution. We note that in contrast to DDPM all noisy variables \(x_{t}\) are conditionally independent given \(x_{0}\) instead of constituting a Markov chain. This structure of the forward process allows us to utilize other noise distributions, which we discuss in more detail later.

### Defining the reverse model

In DDPMs the true reverse model \(q^{}(x_{0:T})\) has a Markovian structure (Ho et al., 2020), allowing for an efficient sequential generation algorithm:

\[q^{}(x_{0:T})=q^{}(x_{T})}q^{}(x_{t-1}|x_{t}).\] (8)

Figure 2: Model structure of DDPM and SS-DDPM.

For the star-shaped diffusion, however, the Markovian assumption breaks:

\[q^{}(x_{0:T})=q^{}(x_{T})_{t=1}^{T}q^{}(x_{ t-1}|x_{t:T}).\] (9)

Consequently, we now need to approximate the true reverse process by a parametric model which is conditioned on the whole tail \(x_{t:T}\).

\[p^{}_{}(x_{0:T})=p^{}_{}(x_{T})_{t=1}^{ T}p^{}_{}(x_{t-1}|x_{t:T}).\] (10)

It is crucial to use the whole tail \(x_{t:T}\) rather than just one variable \(x_{t}\) when predicting \(x_{t-1}\) in a star-shaped model. As we show in Appendix B, if we try to approximate the true reverse process with a Markov model, we introduce a substantial irreducible gap into the variational lower bound. Such a sampling procedure fails to generate realistic samples, as can be seen in Figure 3.

Intuitively, in DDPMs the information about \(x_{0}\) that is contained in \(x_{t+1}\) is nested into the information about \(x_{0}\) that is contained in \(x_{t}\). That is why knowing \(x_{t}\) allows us to discard \(x_{t+1}\). In star-shaped diffusion, however, all variables contain independent pieces of information about \(x_{0}\) and should all be taken into account when making predictions.

We can write down the variational lower bound as follows:

\[^{}()=_{q^{}}[ p_{ }(x_{0}|x_{1:T})-_{t=2}^{T}\!D_{KL}(q^{}(x_{t-1}|x_{ 0})\,\|\,p^{}_{}(x_{t-1}|x_{t:T})].\] (11)

With this VLB, we only need the marginal distributions \(q(x_{t-1}|x_{0})\) to define and train the model, which allows us to use a wider variety of noising distributions. Since conditioning the predictive model \(p_{}(x_{t-1}|x_{t:T})\) on the whole tail \(x_{t:T}\) is typically impractical, we propose a more efficient way to implement the reverse process next.

### Efficient tail conditioning

Instead of using the full tail \(x_{t:T}\), we would like to define some statistic \(G_{t}=_{t}(x_{t:T})\) that would extract all information about \(x_{0}\) from the tail \(x_{t:T}\). Formally speaking, we call \(G_{t}\) a _sufficient tail statistic_ if the following equality holds:

\[q^{}(x_{t-1}|x_{t:T})=q^{}(x_{t-1}|G_{t}).\] (12)

One way to define \(G_{t}\) is to concatenate all the variables \(x_{t:T}\) into a single vector. This, however, is impractical, as its dimension would grow with the size of the tail \(T-t+1\).

The Pitman-Koopman-Darmois (Pitman, 1936) theorem (PKD) states that exponential families admit a sufficient statistic with constant dimensionality. It also states that no other distribution admits one: if such a statistic were to exist, the distribution has to be a member of the exponential family. Inspired by the PKD, we turn to the exponential family of distributions. In the case of star-shaped diffusion, we cannot apply the PKD directly, as it was formulated for i.i.d. samples and our samples are not identically distributed. However, we can still define a sufficient tail statistic \(G_{t}\) for a specific subset of the exponential family, which we call an _exponential family with linear parameterization_:

**Theorem 1**.: _Assume the forward process of a star-shaped model takes the following form:_

\[q^{}(x_{t}|x_{0}) =h_{t}(x_{t})\{_{t}(x_{0})^{}(x _{t})-_{t}(x_{0})\},\] (13) \[_{t}(x_{0}) =A_{t}f(x_{0})+b_{t}.\] (14)

Figure 3: Markov reverse process fails to recover realistic images from star-shaped diffusion, while a general reverse process produces realistic images. The top row is equivalent to DDIM at \(_{t}^{2}=1-_{t-1}\). A similar effect was also observed by Bansal et al. (2022).

_Let \(G_{t}\) be a tail statistic, defined as follows:_

\[G_{t}=_{t}(x_{t:T})=_{s=t}^{T}A_{s}^{}(x_{s}).\] (15)

_Then, \(G_{t}\) is a sufficient tail statistic:_

\[q^{\,}(x_{t-1}|x_{t:T})=q^{\,}(x_{t-1}|G_{t}).\] (16)

Here definition (13) is the standard definition of the exponential family, where \(h_{t}(x_{t})\) is _the base measure_, \(_{t}(x_{0})\) is the vector of _natural parameters_ with corresponding _sufficient statistics_\((x_{t})\), and \(_{t}(x_{0})\) is _the log-partition function_. The key assumption added is the _linear parameterization_ of the natural parameters (14). We provide the proof in Appendix C. When \(A_{t}\) is scalar, we denote it as \(a_{t}\) instead.

For the most part, the premise of Theorem 1 restricts the parameterization of the distributions rather than the family of the distributions involved. As we discuss in Appendix F, we found it easy to come up with linear parameterization for a wide range of distributions in the exponential family. For example, we can obtain a linear parameterization for the Beta distribution \(q(x_{t}|x_{0})=(x_{t};_{t},_{t})\) using \(x_{0}\) as the mode of the distribution and introducing a new concentration parameter \(_{t}\):

\[_{t} =1+_{t}x_{0},\] (17) \[_{t} =1+_{t}(1-x_{0}).\] (18)

In this case, \(_{t}(x_{0})=_{t}x_{0}\), \((x_{t})=}{1-x_{t}}\), and we can use equation (15) to define the sufficient tail statistic \(G_{t}\). We provide more examples in Appendix F. We also provide an implementation-ready reference sheet for a wide range of distributions in the exponential family in Table 6.

We suspect that, just like in PKD, this trick is only possible for a subset of the exponential family. In the general case, the dimensionality of the sufficient tail statistic \(G_{t}\) would have to grow with the size of the tail \(x_{t:T}\). It is still possible to apply SS-DDPM in this case, however, crafting the (now only approximately) sufficient statistic \(G_{t}\) would require more careful consideration and we leave it for future work.

### Final model definition

To maximize the VLB (11), each step of the reverse process should approximate the true reverse distribution:

\[p_{}^{\,}(x_{t-1}|x_{t:T}) q^{\,}(x_{t-1}| x_{t:T})= q^{\,}(x_{t-1}|x_{0})q^{\,}(x_{0}|x_{t:T})dx _{0}.\] (19)

Similarly to DDPM (Ho et al., 2020), we choose to approximate \(q^{\,}(x_{0}|x_{t:T})\) with a delta function centered at the prediction of some model \(x_{}(_{t}(x_{t:T}),t)\). This results in the following definition of the reverse process of SS-DDPM:

\[p_{}^{\,}(x_{t-1}|x_{t:T})=.q^{\,}(x_{t-1}|x _{0})|_{x_{0}=x_{}(_{t}(x_{t:T}),t)}.\] (20)

The distribution \(p_{}^{\,}(x_{0}|x_{1:T})\) can be fixed to some small-variance distribution \(p_{}^{\,}(x_{0}|_{0})\) centered at the final prediction \(_{0}=x_{}(_{1}(x_{1:T}),1)\), similar to the dequantization term, commonly used in DDPM. If this distribution has no trainable parameters, the corresponding term can be removed from the training objective. This dequantization distribution would then only be used for log-likelihood estimation and, optionally, for sampling.

Together with the forward process (7) and the VLB objective (11), this concludes the general definition of the SS-DDPM model. The model structure is illustrated in Figure 2. The corresponding training and sampling algorithms are provided in Algorithms 1 and 2.

The resulting model is similar to DDPM in spirit. We follow the same principles when designing the forward process: starting from a low-variance distribution, centered at \(x_{0}\) at \(t=1\), we gradually increase the entropy of the distribution \(q^{\,}(x_{t}|x_{0})\) until there is no information shared between \(x_{0}\) and \(x_{t}\) at \(t=T\).

We provide concrete definitions for Beta, Gamma, Dirichlet, von Mises, von Mises-Fisher, Wishart, Gaussian and Categorical distributions in Appendix F.

### Duality between star-shaped and Markovian diffusion

While the variables \(x_{1:T}\) follow a star-shaped diffusion process, the corresponding tail statistics \(G_{1:T}\) form a Markov chain:

\[G_{t}=_{s=t}^{T}A_{s}^{}(x_{s})=G_{t+1}+A_{t}^{}(x_{t}),\] (21)

since \(x_{t}\) is conditionally independent from \(G_{t+2:T}\) given \(G_{t+1}\) (see Appendix E for details). Moreover, we can rewrite the probabilistic model in terms of \(G_{t}\) and see that variables \((x_{0},G_{1:T})\) form a (not necessarily Gaussian) DDPM.

In the case of Gaussian distributions, this duality makes SS-DDPM and DDPM equivalent. This equivalence can be shown explicitly:

**Theorem 2**.: _Let \(_{t}^{}\) define the noising schedule for a DDPM model (1-2) via \(_{t}=(_{t-1}^{}-_{t}^{ })/_{t-1}^{}\). Let \(q^{}(x_{0:T})\) be a Gaussian SS-DDPM forward process with the following noising schedule and sufficient tail statistic:_

\[q^{}(x_{t}|x_{0}) =(x_{t};_{t}^{}}x_{0},1-_{t}^{}),\] (22) \[_{t}(x_{t:T}) =_{t}^{}}{_{t}^{}}}_{s=t}^{T}_{s }^{}}x_{s}}{1-_{s}^{}},\] (23) \[_{t}^{}}{1-_{ t}^{}} =_{t}^{}}{1-_{t}^{}}-_{t+1}^{}}{1-_{t+1}^{ }}.\] (24)

_Then the tail statistic \(G_{t}\) follows a Gaussian DDPM noising process \(q^{}(x_{0:T})|_{x_{1:T}=G_{1:T}}\) defined by the schedule \(_{t}^{}\). Moreover, the corresponding reverse processes and VLB objectives are also equivalent._

We show this equivalence in Appendix D. We make use of this connection when choosing the noising schedule for other distributions.

This equivalence means that SS-DDPM is a direct generalization of Gaussian DDPM. While admitting the Gaussian case, SS-DDPM can also be used to implicitly define a non-Gaussian DDPM in the space of sufficient tail statistics for a wide range of distributions.

## 3 Practical considerations

While the model is properly defined, there are several practical considerations that are important for the efficiency of star-shaped diffusion.

Choosing the right scheduleIt is important to choose the right noising schedule for a SS-DDPM model. It significantly depends on the number of diffusion steps \(T\) and behaves differently given different noising schedules, typical to DDPMs. This is illustrated in Figure 4, where we show the noising schedules for Gaussian SS-DDPMs that are equivalent to DDPMs with the same cosine schedule.

Since the variables \(G_{t}\) follow a DDPM-like process, we would like to somehow reuse those DDPM noising schedules that are already known to work well. For Gaussian distributions, we can transform a DDPM noising schedule into the corresponding SS-DDPM noising schedule analytically by equating \(I(x_{0};G_{t})=I(x_{0};x_{t}^{})\). In general case, we look for schedules that have approximately the same level of mutual information \(I(x_{0};G_{t})\) as the corresponding mutual information \(I(x_{0};x_{t}^{})\) for a DDPM model for all timesteps \(t\). We estimate the mutual information using Kraskov (Kraskov et al., 2004) and DSIVI (Molchanov et al., 2019) estimators and build a look-up table to match the noising schedules. This procedure is described in more detail in Appendix G. The resulting schedule for the Beta SS-DDPM is illustrated in Figure 5. Note how with the right schedule appropriately normalized tail statistics \(G_{t}\) look and function similarly to the samples \(x_{t}\) from the corresponding Gaussian DDPM. We further discuss this in Appendix H.

Implementing the samplerDuring sampling, we can grow the tail statistic \(G_{t}\) without any overhead, as described in Algorithm 2. However, during training, we need to sample the tail statistic for each object to estimate the loss function. For this we need to sample the full tail \(x_{t:T}\) from the forward process \(q^{}(x_{t:T}|x_{0})\), and then compute the tail statistic \(G_{t}\). In practice, this does not add a noticeable overhead and can be computed in parallel to the training process if needed.

Reducing the number of stepsWe can sample from DDPMs more efficiently by skipping some timestamps. This wouldn't work for SS-DDPM, because changing the number of steps would require changing the noising schedule and, consequently, retraining the model.

However, we can still use a similar trick to reduce the number of function evaluations. Instead of skipping some timestamps \(x_{t_{1}+1:t_{2}-1}\), we can draw them from the forward process using the current prediction \(x_{}(G_{t_{2}},t_{2})\), and then use these samples to obtain the tail statistic \(G_{t_{1}}\). For Gaussian SS-DDPM this is equivalent to skipping these timestamps in the corresponding DDPM. In general case, it amounts to approximating the reverse process with a different reverse process:

\[p_{}^{}(x_{t_{1}:t_{2}}|G_{t_{2}})=_{t=t_{1}}^{t_{2}}. q^{}(x_{t}|x_{0})|_{x_{0}=x_{}(G_{t},t)}_{t=t_{1}}^ {t_{2}}.q^{}(x_{t}|x_{0})|_{x_{0}=x_{}(G_{t_{2}},t _{2})}.\] (25)

We observe a similar dependence on the number of function evaluations for SS-DDPMs and DDPMs.

Time-dependent tail normalizationAs defined in Theorem 1, the tail statistics can have vastly different scales for different timestamps. The values of coefficients \(a_{t}\) can range from thousandths when \(t\) approaches \(T\) to thousands when \(t\) approaches zero. To make the tail statistics suitable for use in neural networks, proper normalization is crucial. In most cases, we collect the time-dependent means and variances of the tail statistics across the training dataset and normalize the tail statistics to zero mean and unit variance. We further discuss this issue in Appendix H.

Architectural choicesTo make training the model easier, we make some minor adjustments to the neural network architecture and the loss function.

Figure 4: The noising schedule \(_{t}^{}\) for Gaussian star-shaped diffusion, defined for different numbers of steps \(T\) using eq. (24). All the corresponding equivalent DDPMs have the same cosine schedule \(_{t}^{}\).

Figure 5: Top: samples \(x_{t}\) from a Gaussian DDPM forward process with a cosine noise schedule. Bottom: samples \(G_{t}\) from a Beta SS-DDPM forward process with a noise schedule obtained by matching the mutual information. Middle: corresponding samples \(x_{t}\) from that Beta SS-DDPM forward process. The tail statistics have the same level of noise as \(x_{t}^{}\), while the samples \(x_{t}^{}\) are diffused much faster.

Our neural networks \(x_{}(G_{t},t)\) take the tail statistic \(G_{t}\) as an input and are expected to produce an estimate of \(x_{0}\) as an output. In SS-DDPM the data \(x_{0}\) might lie on some manifold, like the unit sphere or the space of positive definite matrices. Therefore, we need to map the neural network output to that manifold. We do that on a case-by-case basis, as described in Appendices I-L.

Different terms of the VLB can have drastically different scales. For this reason, it is common practice to train DDPMs with a modified loss function like \(L_{simple}\) rather than the VLB to improve the stability of training (Ho et al., 2020). Similarly, we can optimize a reweighted variational lower bound when training SS-DDPMs.

## 4 Related works

Our work builds upon Denoising Diffusion Probabilistic Models (Ho et al., 2020). Interest in diffusion models has increased recently due to their impressive results in image (Ho et al., 2020; Song et al., 2020; Dariwal and Nichol, 2021) and audio (Popov et al., 2021; Liu et al., 2022) generation.

SS-DDPM is most closely related to DDPM. Like DDPM, we only rely on variational inference when defining and working with our model. SS-DDPM can be seen as a direct generalization of DDPM and essentially is a recipe for defining DDPMs with non-Gaussian distributions. The underlying non-Gaussian DDPM is constructed implicitly and can be seen as dual to the star-shaped formulation that we use throughout the paper.

Other ways to construct non-Gaussian DDPMs include Binomial diffusion (Sohl-Dickstein et al., 2015), Multinomial diffusion (Hoogeboom et al., 2021) and Gamma diffusion (Nachmani et al., 2021). Each of these works presents a separate derivation of the resulting objective, and extending them to other distributions is not straightforward. On the other hand, SS-DDPM provides a single recipe for a wide range of distributions.

DDPMs have several important extensions. Song et al. (2020) provide a family of non-Markovian diffusions that all result in the same training objective as DDPM. One of them, denoted Denoising Diffusion Implicit Model (DDIM), results in an efficient deterministic sampling algorithm that requires a much lower number of function evaluations than conventional stochastic sampling. Their derivations also admit a star-shaped forward process (at \(_{t}^{2}=1-_{t-1}\)), however, the model is not studied in the star-shaped regime. Their reverse process remains Markovian, which we show to not be sufficient to invert a star-shaped forward process. Denoising Diffusion Restoration Models (Kawar et al., 2022) provide a way to solve general linear inverse problems using a trained DDPM. They can be used for image restoration, inpainting, colorization and other conditional generation tasks. Both DDIMs and DDRMs rely on the explicit form of the underlying DDPM model and are derived for Gaussian diffusion. Extending these models to SS-DDPMs is a promising direction for future work.

Song et al. (2020) established the connection between DDPMs and models based on score matching. This connection gives rise to continuous-time variants of the models, deterministic solutions and more precise density estimation using ODEs. We suspect that a similar connection might hold for SS-DDPMs as well, and it can be investigated further in future works.

Other works that present diffusion-like models with other types of noise or applied to manifold data, generally stem from score matching rather than variational inference. Flow Matching (Lipman et al., 2022) is an alternative probabilistic framework that works with any differentiable degradation process. De Bortoli et al. (2022) and Huang et al. (2022) extended score matching to Riemannian manifolds, and Chen and Lipman (2023) proposed Riemannian Flow Matching. Bansal et al. (2022) proposed Cold Diffusion, a non-probabilistic approach to reversing general degradation processes.

To the best of our knowledge, for the first time, an approach for constructing diffusion without a consecutive process was proposed by Rissanen et al. (2022) (IHDM) and further expanded on by Daras et al. (2022) and Hoogeboom and Salimans (2022). IHDM uses a similar star-shaped structure that results in a similar variational lower bound. Adding a deterministic process based on the heat equation allows the authors to keep the reverse process Markovian without having to introduce the tail statistics. As IHDM heavily relies on blurring rather than adding noise, the resulting diffusion dynamics become very different. Conceptually our work is much closer to DDPM than IHDM.

Experiments

We evaluate SS-DDPM with different families of noising distributions. The experiment setup, training details and hyperparameters are listed in Appendices I-L.

Synthetic dataWe consider two examples of star-shaped diffusion processes with Dirichlet and Wishart noise to generate data from the probabilistic simplex and from the manifold of p.d. matrices respectively. We compare them to DDPM, where the predictive network \(x_{}(x_{t},t)\) is parameterized to always satisfy the manifold constraints. As seen in Table 1, using the appropriate distribution results in a better approximation. The data and modeled distributions are illustrated in Table 3. This shows the ability of SS-DDPM to work with different distributions and generate data from exotic domains.

Geodesic dataWe apply SS-DDPM to a geodesic dataset of fires on the Earth's surface (EOSDIS, 2020) using a three-dimensional von Mises-Fisher distribution. The resulting samples and the source data are illustrated in Table 3. We find that SS-DDPM is not too sensitive to the distribution family and can fit data in different domains.

Discrete dataCategorical SS-DDPM is similar to Multinomial Text Diffusion (MTD, (Hoogeboom et al., 2021)). However, unlike in the Gaussian case, these models are not strictly equivalent. We follow a similar setup to MTD and apply Categorical SS-DDPM to unconditional text generation on the text8 dataset (Mahoney, 2011). As shown in Table 2, SS-DDPM achieves similar results to MTD, allowing to use different distributions in a unified manner. While D3PM (Austin et al., 2021) provides some improvements, we follow a simpler setup from MTD. We expect the improvements from D3PM to directly apply to Categorical SS-DDPM.

Image dataFinally, we evaluate SS-DDPM on CIFAR-10. Since the training data is constrained to a \(\) segment, we use Beta distributions. We evaluate our model with various numbers of generation steps, as described in equation (25), and report the resulting Frechet Inception Distance (FID, (Heusel et al., 2017)) in Figure 6. Beta SS-DDPM achieves comparable quality with the Improved DDPM (Nichol and Dhariwal, 2021) and is slightly better on lower numbers of steps. As expected, DDIM performs better when the number of diffusion steps is low, but both SS-DDPM and DDPM outperform DDIM on longer runs. The best FID score achieved by Beta SS-DDPM is \(3.17\). Although the FID curves for DDPM and DDIM do not achieve this score in Figure 6, Ho et al. (2020) reported an FID score of \(3.17\) for \(1000\) DDPM steps, meaning that SS-DDPM performs similarly to DDPM in this setting.

    & Dirichlet & Wishart \\  DDPM & \(0.200\) & \(0.096\) \\ SS-DDPM & \(\) & \(\) \\   

Table 1: KL divergence between the real data distribution and the model distribution \(D_{KL}(q(x_{0})\,\|\,p_{}(x_{0}))\) for Gaussian DDPM and SS-DDPM.

   Model & NLL (bits/char) \\  MTD & \( 1.72\) \\ SS-DDPM & \(\) \\   

Table 2: Comparison of Categorical SS-DDPM and Multinomial Text Diffusion on text8. NLL is estimated via ELBO.

Figure 6: Quality of images, generated using Beta SS-DDPM, DDPM and DDIM with different numbers of sampling steps. Models are trained and evaluated on CIFAR-10. DDPM and DDIM results were taken from Nichol and Dhariwal (2021).

## 6 Conclusion

We propose an alternative view on diffusion-like probabilistic models. We reveal the duality between star-shaped and Markov diffusion processes that allows us to go beyond Gaussian noise by switching to a star-shaped formulation. It allows us to define diffusion-like models with arbitrary noising distributions and to establish diffusion processes on specific manifolds. We propose an efficient way to construct a reverse process for such models in the case when the noising process lies in a general subset of the exponential family and show that star-shaped diffusion models can be trained on a variety of domains with different noising distributions. On image data, star-shaped diffusion with Beta distributed noise attains comparable performance to Gaussian DDPM, challenging the optimality of Gaussian noise in this setting. The star-shaped formulation opens new applications of diffusion-like probabilistic models, especially for data from exotic domains where domain-specific non-Gaussian diffusion is more appropriate.