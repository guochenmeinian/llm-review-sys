# Cluster-wise Graph Transformer with

Dual-granularity Kernelized Attention

 Siyuan Huang\({}^{1,2}\)   Yunchong Song\({}^{1}\)   Jiayue Zhou\({}^{2}\)   Zhouhan Lin\({}^{1}\)

\({}^{1}\)LUMIA Lab, Shanghai Jiao Tong University

\({}^{2}\)Paris Elite Institute of Technology, Shanghai Jiao Tong University

siyuan_huang_sjtu@outlook.com   ycsong@sjtu.edu.cn   lin.zhouhan@gmail.com

Zhouhan Lin is the corresponding author

###### Abstract

In the realm of graph learning, there is a category of methods that conceptualize graphs as hierarchical structures, utilizing node clustering to capture broader structural information. While generally effective, these methods often rely on a fixed graph coarsening routine, leading to overly homogeneous cluster representations and loss of node-level information. In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding. To enable effective information transfer among these node sets, we propose the Node-to-Cluster Attention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple Kernel Learning into the kernelized attention framework, effectively capturing information at both node and cluster levels. We then devise an efficient form for N2C-Attn using the cluster-wise message-passing framework, achieving linear time complexity. We further analyze how N2C-Attn combines bi-level feature maps of queries and keys, demonstrating its capability to merge dual-granularity information. The resulting architecture, Cluster-wise Graph Transformer (Cluster-GT), which uses node clusters as tokens and employs our proposed N2C-Attn module, shows superior performance on various graph-level tasks. Code is available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.

## 1 Introduction

Graph learning represents a rapidly evolving field. Techniques like Graph Neural Networks (GNNs) and Graph Transformers (GT) demonstrate impressive performance across a range of tasks , such as social networks , time series , traffic flow  and drug discovery . These methods enhance performance by promoting message propagation at the node level and calculating attention between node pairs, thereby concentrating on node-level interactions.

Recent advancements have extended beyond node-level message propagation, adopting approaches that treat the graph as a hierarchical structure , capturing information at multiple levels of the graph . For instance, node clustering pooling segments the graph into multiple clusters . Each cluster is then independently pooled, preserving the structural information of the hierarchical graph. Drawing inspiration from Vision Transformers , GraphViT  treats subgraphs as tokens and computes attention among them, which enables the model to capture long-distance dependencies and reduces the overall computational complexity compared to node-level Graph Transformers.

However, existing methods based on node clustering rely on a fixed graph coarsening routine . This routine involves partitioning the graph into several clusters and subsequently pooling each cluster into a single node to generate a coarsened version of the original graph. While generallyeffective, research has shown that compressing each cluster into a single embedding can lead to overly uniform cluster representations, which may not accurately reflect the diversity within each cluster . Furthermore, these methods typically simplify the interactions between clusters to basic vertex-level interactions on the coarsened graph. This oversimplification overlooks the rich node-level information contained within each cluster, thereby limiting the potential for richer cluster-wise interactions.

In this work, we propose a different strategy for enhancing cluster-wise interaction. Instead of reducing each cluster to a single node through coarsening, we envision the graph as a network of interconnected node sets. To enable message propagation among these node sets, we develop a method termed Node-to-Cluster Attention (N2C-Attn). N2C-Attn incorporates techniques from Multiple Kernel Learning (MKL)  into the kernelized attention framework . By combining kernels at two different granularity levels, N2C-Attn effectively captures hierarchical graph structural information at the cluster level while also preserving node-level details within each cluster.

We propose treating the graph as interconnected node clusters without coarsening, which inherently increases computational complexity. To mitigate this issue, we employ the technique of kernelized softmax  to reduce the computational complexity to linear. Consequently, the computation process of N2C-Attn can be viewed as a cluster-wise message propagation: each cluster gathers internal keys and values, then propagates them along weighted edges to the queries of other clusters.

We present a further analysis of how N2C-Attn synthesizes new queries and keys by merging inputs from both node and cluster levels. We consider two scenarios: 1) using the product of kernels and 2) using the convex sum of kernels. The former implicitly conducts a tensor product of the feature maps from both the node-level and cluster-level queries (and keys), adopting this product as the new query (or key) for N2C-Attn. The latter concatenates node and cluster-level feature maps with learnable weights, maintaining their independence and allowing the model to adjust their relative significance. We also demonstrate that cluster-level attention can be regarded as a special case of N2C-Attn.

Our resulting architecture, Cluster-wise Graph Transformer (Cluster-GT), leverages our proposed N2C-Attn module in conjunction with a simple graph partitioning algorithm, Metis . We conduct extensive evaluations of Cluster-GT across eight graph-level datasets, varying in size and domain. Cluster-GT outperforms existing Graph Transformers and graph pooling methods that employ more intricate graph partitioning algorithms, which highlights the effectiveness of enhancing inter-cluster interactions and preserving information at both granular levels. We further analyze the relative weights of the combined kernel, finding that Cluster-GT pays more attention to cluster-level information when handling graphs in the social network domain compared to graphs in the biological domain.

## 2 Background

Consider a graph \(\) represented by the multi-tuple \(,,,\). \(\) denotes the set of \(n\) nodes, \(\) denotes the set of \(m\) edges. \(^{n d}\) is the feature matrix and \(^{n n}\) is the adjacency matrix. We use the superscript \(P\) to indicate the cluster-level (coarsened) graph: \((^{P},^{P},^{P},^{P})\), where \(^{P}\) represents clusters of nodes, and \(^{P}\) denotes the edges connecting these clusters.

Node Clustering Pooling and Cluster Assignment MatrixNode clustering pooling captures hierarchical structural information by partitioning and iteratively coarsening the graph to a smaller size [32; 1; 33]. This process involves two main steps. Initially, a Cluster Assignment Matrix (CAM) \(^{n m}\) is generated using a carefully designed strategy, where \(n\) represents the number of original nodes, and \(m\) indicates the number of clusters. Once the Cluster Assignment Matrix is obtained, it is used to perform graph coarsening, i.e., pooling each cluster into a single node:

\[^{P}=^{T};^{P}=^{T}\] (1)

where \(^{P}^{m d}\) and \(^{P}^{m m}\) are the new node features and adjacency matrix, defining the post-coarsening graph structure. \(_{sj}\) thus represents the weight of the \(s\)-th node in the \(j\)-th cluster.

Beyond node clustering pooling, methods exist that leverage node clusters to enhance graph attention [18; 3]. GraphViT  utilizes Metis  to partition the graph into multiple subgraphs. It then applies mean pooling to each subgraph, treating the pooled clusters as tokens for further attention computation. Despite promising results, GraphViT still adheres to the graph coarsening pipeline, which leads to overly similar cluster representations  and the loss of node-level information.

Generalized Self-attention and Kernelized SoftmaxNumerous studies suggest reevaluating the attention mechanism through the lens of kernel methods [46; 24]. The generalized formulation of self-attention utilizes a non-negative kernel function \((,):^{d_{k}}^{d_{k}}_{+}\), which can be represented with a corresponding feature map \(\). The self-attention mechanism can be expressed as:

\[(X)_{i}=_{j=1}^{N},k_{j})}{_{j^{} =1}^{N}(q_{i},k_{j^{}})}v_{j}\] (2)

where \(k_{j}\), \(q_{i}\), and \(v_{j}\) are the corresponding keys, queries, and values. By expressing \(\) with feature map \((q_{i},k_{j})=(q_{i})^{T}(k_{j})\), the computation simplifies to:

\[(X)_{i}=)_{j=1}^{N}(k_{j})^{T}v_{j}}{ (q_{i})_{j=1}^{N}(k_{j})^{T}}\] (3)

where the sums \(_{j=1}^{N}(k_{j})^{T}v_{j}\) and \(_{j=1}^{N}(k_{j})^{T}\) are shared across all nodes and need to be computed only once, thus reducing computational complexity to \((N)\). Various choices of feature maps are shown effective, such as the RBF kernel  and Positive Random Features .

Multiple Kernel LearningThe selection of an optimal kernel function \((,)\) is critical for enhancing the performance of kernel-based learning methods. Multiple Kernel Learning (MKL) methods [56; 44] leverage a combination of kernel functions to integrate various features from different perspectives. The resultant kernel, \(_{}\), is mathematically defined as:

\[_{}(\{^{m}\}_{m=1}^{M},\{^{m}\}_{m=1}^{M})=f_{ }(\{_{m}(^{m},^{m})\}_{m=1}^{M})\] (4)

where \(f_{}:^{M}\) can be either a linear or nonlinear function. Each \(_{m}:^{D_{m}}^{D_{m}}\) is a valid kernel for vectors \(^{m},^{m}^{D_{m}}\), with \(D_{m}\) representing the dimensionality of each feature. There are various strategies for combining kernels, which represent a dynamic area of research. 

In this work, we concentrate on the pairwise scenario, where \(M=2\). We note the two input spaces as \(\) and \(^{}\). For constructing pairwise kernels when elements of each pair belong to different input spaces, we select two fundamental strategies: the tensor product of kernels and the convex linear combination of kernels, which are commonly used on the product space \(^{}\).

Given two kernels \(_{1}:\) and \(_{2}:^{}^{}\), the tensor product method is defined as:

\[_{}((x,x^{}),(y,y^{}))=_{1}(x,y)_{2}(x^{ },y^{})\] (5)

where \((x,x^{})\), \((y,y^{})\) are pairs of objects from \(^{}\). While the convex linear combination method is defined as:

\[_{}((x,x^{}),(y,y^{}))=_{1}(x,y)+_ {2}(x^{},y^{})\] (6)

where \(, 0\) and \(+=1\). \(\) and \(\) are coefficients that balance the contribution of each kernel.

## 3 Node-to-Cluster Attention

In this section, we present the Node-to-Cluster Attention (N2C-Attn) mechanism. We begin in Section 3.1 by defining the concept of N2C-Attn. We then proceed to Section 3.2, where we devise an efficient form of N2C-Attn with the message-passing framework. In Section 3.3, we re-examine N2C-Attn, focusing on the integration of feature maps of queries and keys across two granularities.

### Definition of Node-to-Cluster Attention

Node-to-Cluster Attention marks a departure from the graph coarsening pipeline that typically coarsens each cluster into a single embedding. Instead, as shown in Figure 1, we maintain the clusters uncompressed and use N2C-Attn to propagate messages among the inter-connected node clusters. The definition of N2C-Attn is based on the Cluster Assignment Matrix \(\), which can be obtained through various graph partitioning methods . N2C-Attn focuses on the "post-clustering" phase.

Bi-level Query and KeyA key observation is that after node clustering, each node possesses two tiers of information: 1) its individual node feature and 2) the collective feature of its cluster. An effective attention mechanism needs to accommodate these two distinct levels of information. Thus, the \(t\)-th node in the \(j\)-th cluster is characterized by a bi-level pair of keys: \(\{K_{j},k_{t}\}_{C}_{N}\):

\[k_{t}=_{k}h_{t},\ K_{j}=_{k}^{}(_{s} _{sj}h_{s})\] (7)

where \(h_{t}\) is the feature of the \(t\)-th node. \(k_{t}_{N}\) is the node-level key, which is solely derived from the embedding of \(t\)-th node, and \(K_{j}_{C}\) represents the cluster-level key, which depends on all nodes within the \(j\)-th cluster. \(_{k}\) and \(_{k}^{}\) are two different projections to \(_{N}\) and \(_{C}\), respectively.

Since we are considering the attention between clusters and nodes, each cluster needs to provide a corresponding bi-level query. Thus, the \(i\)-th cluster is characterized by a bi-level pair of queries:

\[q_{i}=_{v}(_{s}_{si}h_{s}),\ Q_{i}= _{v}^{}(_{s}_{si}h_{s})\] (8)

where \(Q_{i}\) denotes the cluster-level query, \(q_{i}\) denotes the node-level query. \(_{v}\) and \(^{}{}_{v}\) are two different projections to \(_{N}\) and \(_{C}\), respectively. The bi-level query is thus \(\{Q_{i},q_{i}\}_{C}_{N}\).

Note that we use uppercase letters to represent cluster-level queries and keys, e.g., \(\{Q_{i},K_{j}\}\), and lowercase letters to represent node-level queries and keys, e.g., \(\{q_{i},k_{t}\}\).

General Definition of Node-to-Cluster AttentionHaving obtained the bi-level queries and keys, we consider how to use kernels to measure their similarity. We denote a valid kernel in the cluster-level space \(_{C}\) as \(_{C}\), and a valid kernel in the node-level space \(_{N}\) as \(_{N}\). We now consider how to construct a kernel \(_{}\) on the tensor product space \(_{C}_{N}\). \(_{}\) stands for \(\)-level kernel.

Given \(\{Q_{i},q_{i}\}\), the bi-level query for the \(i\)-th node cluster, and \(\{K_{j},k_{t}\}\), the bi-level key for the \(t\)-th node in the \(j\)-th node cluster, the general Node-to-Cluster Attention for the \(i\)-th cluster is defined as:

\[Attn}(X)_{i}=_{i,j}^{P}_{t} _{t}_{tj}_{}(\{Q_{i},q_{i}\},\{K_{j},k_{t}\})} {_{j}_{i,j}^{P}_{t}_{tj}_{}(\{Q_{i },q_{i}\},\{K_{j},k_{t}\})}\] (9)

Equation 9 depicts the process of the \(i\)-th cluster gathering information from nodes of all connected clusters. The attention score between the \(i\)-th cluster and the \(t\)-th node in the \(j\)-th cluster is \(_{i}^{P}_{tj}_{}(\{Q_{i},q_{i}\},\{K_ {j},k_{t}\})}{_{j}_{i,j}^{P}_{t}_{tj}_{}(\{Q_{i},q_{i}\},\{K_{j},k_{t}\})}\). \(_{}\) plays a pivotal role in integrating information across cluster and node levels. As described in Section 2, we mainly consider two options for \(_{}\): the tensor product method and the linear combination method. Next, we introduce these two different implementations.

Figure 1: Definition of Node-to-Cluster Attention (N2C-Attn). N2C-Attn perceives the graph as interconnected node sets instead of coarsening each cluster into a single node. It integrates multiple kernel learning methods into the kernelized attention framework to facilitate message propagation among node clusters, simultaneously capturing both the node-level and cluster-level information.

Node-to-Cluster Attention with Tensor Product of Kernels (N2C-Attn-T)With the help of Equation 5, we can define the bi-level kernel \(_{B}\) as:

\[_{}(\{Q_{i},q_{i}\},\{K_{j},k_{t}\})=_{C}(Q_{i},K_{j})_{ N}(q_{i},k_{t})\] (10)

We can thus rewrite the Node-to-Cluster Attention defined in Equation 9 as:

\[}(X)_{i}=_{i,j}^{P}_{t} _{tj}_{C}(Q_{i},K_{j})_{N}(q_{i},k_{t})v_{t}}{_{j} _{i,j}^{P}_{t}_{tj}_{C}(Q_{i},K_{j})_{N}(q_ {i},k_{t})}\] (11)

\(}\) stands for Node-to-Cluster Attention with Tensor Product of Kernels. By performing the product between \(_{C}\) and \(_{N}\), this construction enables interaction across all dimensions of the feature vectors at different granular levels, thereby capturing the dependencies within the combined feature space. We offer a more detailed explanation in subsection 3.3.

Node-to-Cluster Attention with Convex Linear Combination of Kernels (N2C-Attn-L)With the help of Equation 6, we can also define the bi-level kernel \(_{B}\) as:

\[_{}(\{Q_{i},q_{i}\},\{K_{j},k_{t}\})=_{C}(Q_{i},K_{ j})+_{N}(q_{i},k_{t})\] (12)

where \(, 0\) are learnable parameters and \(+=1\). We can thus rewrite Equation 9 as:

\[}(X)_{i}=_{i,j}^{P}_{t} _{tj}(_{C}(Q_{i},K_{j})+_{N}(q_{i},k_{t}))v_ {t}}{_{j}_{i,j}^{P}_{t}_{tj}(_{C}(Q_{ i},K_{j})+_{N}(q_{i},k_{t}))}\] (13)

\(}\) stands for Node-to-Cluster Attention with Convex Linear Combination of Kernels. By combining the kernels \(_{C}\) and \(_{N}\) with coefficients \(\) and \(\), this construction allows for flexible integration of the similarities measured in \(_{C}\) and \(_{N}\), letting the combined kernel adaptively scale the influence of the cluster-level and node-level information on the overall similarity measure.

### Efficient Implementation of Node-to-Cluster Attention

N2C-Attn requires the computation of similarity between queries and keys at two different levels of granularity. Normally, this necessitates a computational complexity of \((|||^{P}|)\), where \(||\) denotes the number of nodes and \(|^{P}|\) denotes the number of clusters. To speed up this process, we devise a linear algorithm using the feature map and the message-passing framework. In this subsection, we focus on the efficient implementation of N2C-Attn-T. Following a similar method, we can also develop an efficient implementation for N2C-Attn-L, which is detailed in Appendix A.

To accelerate Equation 11, a key observation is that \(_{C}\) is correlated to the edges between clusters, serving as the gates on the edges. While \(_{N}\) involves queries of clusters and keys of nodes. Therefore, we propose separating the node-level and cluster-level computation of \(_{N}\), and then turning the computation of N2C-Attn-T into a cluster-wise message propagation. We represent \(_{N}\) using the corresponding feature map: \(_{N}(q_{i},k_{t})=(q_{i})^{T}(k_{t})\). Thus, Equation 11 can be rewritten as:

\[}(X)_{i}=)^{T}_{j}_{i, j}^{P}_{C}(Q_{i},K_{j})_{t}_{tj}(k_{t})v_{t}}{(q_{i})^{T} _{j}_{i,j}^{P}_{C}(Q_{i},K_{j})_{t}_{tj} (k_{t})}\] (14)

Figure 2: An efficient implementation of N2C-Attn-T with the message-passing framework. \(|^{P}|\) denotes the number of clusters and \(|^{P}|\) denotes the number of edges between clusters. The computation can be decomposed into 4 steps: 1) aggregation of node-level keys and values within each cluster, 2) computation of gate on each edge with the cluster-level kernel, 3) message propagation among clusters, 4) dot product of aggregated value with the node-level query of each cluster.

With Equation 14, we observe that the computation of N2C-Attn-T can be encompassed within the message-passing framework. Figure 2 shows the complete process, which contains four steps: 1) aggregating node-level keys and values within each cluster, 2) calculating the gate on each edge using the cluster-level kernel, 3) propagating messages among clusters, and 4) computing the dot product of the aggregated value with the node-level query for each cluster. N2C-Attn-T can thus be seen as a form of cluster-wise message propagation. Each cluster acts as a sender, propagating the packaged keys and values of its internal nodes; it also acts as a receiver, using its own query to interpret the information from the received keys and values. The overall time complexity is thus reduced to linear.

### Equivalent Feature Maps of Bi-level Kernels

In this subsection, we delve into how the Node-to-Cluster Attention mechanism integrates information across cluster and node levels through the lens of feature maps. We note \(_{}\) as the feature map of \(_{}\): \(_{}(\{Q_{i},q_{i}\},\{K_{j},k_{t}\})=_{ }(\{Q_{i},q_{i}\}),_{}(\{K_{j},k_{t}\})\) where \(,\) represents the inner product in the corresponding feature space. Equation 9 can thus be expressed as:

\[(X)_{i}=_{i,j}^{P}_{t} _{tj}_{}(\{Q_{i},q_{i}\}),_{}(\{K_{j},k_{t }\}) v_{t}}{_{j}_{i,j}^{P}_{t}_{tj} _{}(\{Q_{i},q_{i}\}),_{}(\{K_{j},k_{t}\})}\] (15)

\(_{}(Q_{i},q_{i})\) represents the feature vector of the newly formulated bi-level query, while \(_{}(K_{j},k_{t})\) represents the feature vector of the newly formulated bi-level key. We are interested in their relationship with the original queries and keys \(\{Q_{i},q_{i},K_{j},k_{t}\}\). We establish the following relationships:

**Proposition 1**: _If \(_{C}(Q_{i},K_{j})=(Q_{i}),(K_{j})\) and \(_{N}(q_{i},k_{t})=(q_{i}),(k_{t})\), where \(\) and \(\) are feature maps for the respective kernels, then the Node-to-Cluster Attention with the tensor product kernel implies the following equivalent feature map:_

\[_{}(\{Q_{i},q_{i}\})=(Q_{i})(q_{i});_{ }(\{K_{j},k_{t}\})=(K_{j})(k_{t})\] (16)

_where \(\) represents the outer product of the node-level and cluster-level feature maps. Conversely, the Node-to-Cluster Attention with the convex sum implies the following equivalent feature map:_

\[_{}(\{Q_{i},q_{i}\})=(Q_{i}) (q_{i});_{}(\{K_{j},k_{t}\})=(K_{j}) (k_{t})\] (17)

_where \(\) represents the concatenation of the weighted node-level and cluster-level feature maps._

This proposition provides an intuitive understanding of N2C-Attn: by integrating queries and keys from both node-level and cluster-level, N2C-Attn synthesizes new queries and keys enriched with bi-level information. Specifically, using the product of kernels, as detailed in Equation 16, N2C-Attn-T implicitly performs a tensor product between the feature maps of the node-level query (key) and the cluster-level query (key), and finally using the product as the new query (key). This resulting equivalent feature map thus extends into a higher-dimensional space, offering a feature fusion of bi-level information. It's worth noting that we do not need to actually compute the tensor product between the cluster-level and node-level queries or keys, which requires high spatial complexity.

While employing the convex sum of kernels, as detailed in Equation 17, can be regarded as a concatenation of the feature maps of the original node-level and cluster-level queries (keys), appending learnable weights. This approach preserves the independence of queries (keys) at different levels, empowering the model to adjust their relative significance. Besides, we can leverage this point to design an efficient implementation method for N2C-Attn-L. We introduce it in detail in Appendix A.

We offer a further analysis by comparing the assigned attention scores between N2C-Attn and previous cluster-level attention methods. We prove that the attention mechanism used in GraphViT , which is based on the graph coarsening pipeline and serves as a cluster-level attention mechanism, can be seen as a special case of our proposed N2C-Attn. More details can be found in Appendix B.

## 4 Cluster-wise Graph Transformer

In this section, we introduce a simple yet performant architecture named Cluster-wise Graph Transformer (Cluster-GT) which takes node clusters as tokens and utilizes N2C-Attn defined in Section 3 to propagate information among clusters. Cluster-GT can be divided into three main modules: 1) a node-level convolution module, 2) a graph partition module, and 3) a cluster-wise interaction module.

Figure 3 presents the overall architecture of our proposed Cluster-GT. We begin with a node-level convolution module to capture the local structural information. We try two common options, GCN  and GIN , during our implementation. We also utilize two graph positional encoding strategies, random-walk structural encoding (RWSE)  and Laplacian eigenvector encodings , to enhance the perception of the graph structure. More details can be found in Appendix D. For the graph partition module, we use a relatively simple graph partition algorithm, Metis , to assign nodes to different clusters. After node clustering assignment, we introduce our proposed N2C-Attn as the cluster-wise interaction module, which propagates information among clusters. This process is divided into two steps: we first calculate the corresponding bi-level keys and queries, and then execute the efficient algorithm of N2C-Attn introduced in subsection 3.2, which outputs a single embedding for each cluster. We finally perform average pooling to obtain the graph-level embedding.

The choice of kernel and feature map is not the main focus of our work. In our implementation, we use the common exp-dot-product \((K}{}})\) as \(_{C}\). For the feature map of \(_{N}\), we try two basic options: \((x)=(x)+1\) and \((x)=(x)\), which we set as a hyperparameter.

Cluster-GT, in conjunction with N2C-Attn, is designed to enhance information exchange between node clusters after the graph partitioning. This process can be viewed as a "post-partitioning" phase, which is a key distinction from many other node-clustering-based methods that primarily focus on optimizing the graph partition itself. In our implementation, we utilize a non-learnable and rigid graph partitioning algorithm, Metis. Notably, the Graph Partition module in Cluster-GT can be replaced with other learnable or flexible graph partitioning strategies, allowing for potential enhancements.

## 5 Evaluation

To evaluate the performance of Cluster-GT, we compare it against two categories of methods: Graph Pooling and Graph Transformers. We conduct experiments on eight graph classification datasets from different domains, including social networks and biology. We further visualize the weight coefficients of the cluster-level and node-level kernels in N2C-Attn-L to observe how the model focuses on different information granularities across different datasets. Additionally, we perform an ablation study, restricting the attention mechanism to different granularities, to demonstrate the benefits of integrating both levels of information. We finally carry out an efficiency study of Cluster-GT. All experiments are conducted on NVIDIA RTX 3090s with 24GB of RAM. Detailed dataset information is available in Appendix E, and more details of the implementation are provided in Appendix F.

### Comparison with Graph Pooling Methods

Given the close relationship between Cluster-GT and node clustering methods, we compare Cluster-GT with mainstream Graph Pooling methods:two well-known GNN baselines: GCN , GIN , six hierarchical pooling approaches: DiffPool , SAGPool(H) , TopKPool , ASAP , MinCutPool , SEP  and five global pooling techniques: Set2Set , SortPool , SAGPool(G) , StructPool , GMT  We test Cluster-GT on six TU datasets : IMDB-BINARY, IMDB-MULTI, COLLAB, MUTAG, PROTEINS, and D\(\&\)D. The first three datasets are in the field

Figure 3: Architecture of Cluster-wise Graph Transformer (Cluster-GT), which can be decomposed into three main modules: 1) a node-wise convolution module with GNN, 2) a graph partition module with Metis, and 3) a cluster-wise interaction module with N2C-Attn.

of social networks, while the latter three are in the field of biology. For a fair comparison, we strictly follow the experimental setup of . Table 1 shows the results, indicating that Cluster-GT outperforms all baselines on most datasets, even though it employs a relatively simple graph partitioning algorithm compared to other node clustering pooling methods. This result highlights the effectiveness of the N2C-Attn module and shows the importance of the interaction between clusters in the "post-partitioning" phase, which is often oversimplified by other node clustering pooling methods.

### Comparison with Graph Transformers

To assess the effectiveness of Cluster-GT within the context of Graph Transformers, we compare Cluster-GT with a range of existing Graph Transformers, including GT , GraphiT , Graphormer , GPS , SAN+LapPE , SAN+RWSE , Graph MLP-Mixer  and Graph ViT . We conduct the experiment on two datasets: ZINC from Benchmarking GNNs  and Mol-HIV from OGB . For a fair comparison, we strictly follow the experimental setup of . The result shown in Figure 2 demonstrates that Cluster-GT surpasses most existing Graph Transformers, underscoring the importance of integrating information at both the cluster and node levels and showcasing the potential of using node clusters as tokens in attention mechanisms.

### Visualization of \(\) in N2C-Attn-L

In this subsection, we present the dynamic changes in the weight coefficient \(\) during the training process of N2C-Attn-L. \(\) quantifies the contribution of cluster-level information in the combined kernel, whereas \(=1-\) quantifies the node-level information. By enabling the model to

   Model & IMDB-BINARY & IMDB-MULTI & COLLAB & MUTAG & PROTEINS & D\&D \\  GCN & \(73.26_{ 0.46}\) & \(50.39_{ 0.41}\) & \(80.59_{ 0.27}\) & \(69.50_{ 1.78}\) & \(73.24_{ 0.73}\) & \(72.05_{ 0.55}\) \\ GIN & \(72.78_{ 0.86}\) & \(48.13_{ 1.36}\) & \(78.19_{ 0.63}\) & \(81.39_{ 1.53}\) & \(71.46_{ 1.66}\) & \(70.79_{ 1.17}\) \\ Set2Set & \(72.90_{ 0.75}\) & \(50.19_{ 0.39}\) & \(79.55_{ 0.39}\) & \(69.89_{ 1.94}\) & \(73.27_{ 0.85}\) & \(71.94_{ 0.56}\) \\ SortPool & \(72.12_{ 1.12}\) & \(48.18_{ 0.83}\) & \(77.87_{ 0.47}\) & \(71.94_{ 3.55}\) & \(73.17_{ 0.88}\) & \(75.58_{ 0.72}\) \\ SAGPool(G) & \(72.16_{ 0.88}\) & \(49.47_{ 0.56}\) & \(78.85_{ 0.56}\) & \(76.78_{ 2.12}\) & \(72.02_{ 1.01}\) & \(71.54_{ 0.91}\) \\ StructPool & \(72.06_{ 0.64}\) & \(50.23_{ 0.53}\) & \(77.27_{ 0.51}\) & \(79.50_{ 0.75}\) & \(75.16_{ 0.86}\) & \(78.45_{ 0.40}\) \\ GMT & \(73.48_{ 0.76}\) & \(50.66_{ 0.82}\) & \(80.74_{ 0.54}\) & \(83.44_{ 1.33}\) & \(75.09_{ 0.59}\) & \(78.72_{ 0.59}\) \\  DiffPool & \(73.14_{ 0.70}\) & \(51.31_{ 0.72}\) & \(78.68_{ 0.43}\) & \(79.22_{ 1.02}\) & \(73.03_{ 1.00}\) & \(77.56_{ 0.64}\) \\ SAGPool(H) & \(72.55_{ 1.28}\) & \(50.23_{ 0.44}\) & \(78.03_{ 0.31}\) & \(73.67_{ 4.28}\) & \(71.56_{ 1.49}\) & \(74.72_{ 0.82}\) \\ TopKPool & \(71.58_{ 0.95}\) & \(48.59_{ 0.72}\) & \(77.58_{ 0.85}\) & \(67.61_{ 3.36}\) & \(70.48_{ 1.01}\) & \(73.63_{ 0.55}\) \\ ASAP & \(72.81_{ 0.50}\) & \(50.78_{ 0.75}\) & \(78.64_{ 0.50}\) & \(77.83_{ 1.49}\) & \(73.92_{ 0.63}\) & \(76.58_{ 1.04}\) \\ MinCutPool & \(72.65_{ 0.75}\) & \(51.04_{ 0.70}\) & \(80.87_{ 0.34}\) & \(79.17_{ 1.64}\) & \(74.72_{ 0.48}\) & \(78.22_{ 0.54}\) \\ SEP-G & \(74.12_{ 0.56}\) & \(51.53_{ 0.65}\) & \(81.28_{ 0.15}\) & \(85.56_{ 1.09}\) & \(76.42_{ 0.39}\) & \(77.98_{ 0.57}\) \\  Cluster-GT & \(}\) & \(}\) & \(80.43_{ 0.52}\) & \(}\) & \(}\) & \(}\) \\   

Table 1: Comparison with Graph Pooling Methods on six TU datasets. The shown accuracies (%) are mean and standard deviation over 10 different runs. We highlight the best results.

   Model & ZINC (MAE \(\)) & MoHIV (ROCAUC \(\)) \\  GT & \(0.226_{ 0.014}\) & — \\ GraphiT & \(0.202_{ 0.011}\) & — \\ Graphormer & \(0.122_{ 0.066}\) & — \\ GPS & \(}\) & \(0.7880_{ 0.0101}\) \\ SAN+LapPE & \(0.139_{ 0.006}\) & \(0.775_{ 0.0061}\) \\  Graph MLP-Mixer & \(}\) & \(0.797_{ 0.0102}\) \\ Graph ViT & \(0.085_{ 0.005}\) & \(0.7792_{ 0.0149}\) \\  Cluster-GT & \(}\) & \(}\) \\   

Table 2: Comparison with Graph Transformers on ZINC and MolHIV over 4 different runs of 4 different seeds. We highlight the best results. Missing values from literature are indicated as ’-’.

Figure 4: Visualization of \(\) weight of the cluster-level kernel) during the training process. N2C-Attn learns to pay more attention to cluster-level information in social networks than in bioinformatics.

autonomously learn these coefficients, it dynamically adjusts to the varying importance of information at different granularities. We plot the evolution of \(\) across training steps for six diverse datasets, as shown in Figure 4. We observe that the model automatically adjusts the weights assigned to the two levels of granularity. Notably, for social network datasets, Cluster-GT shows a preference for cluster-level information, whereas, for biology datasets, Cluster-GT balances its attention more equally between both granularities. This result indicates that N2C-Attn has a stronger inclination towards cluster-level information in the social networks domain compared to the biology domain.

### Necessity of Combining Cluster-level and Node-level Information

In this subsection, we explore the necessity of fusing kernels of dual granularities within the N2C-Attn module. We analyze four variants: the first two are N2C-Attn-T and N2C-Attn-L, which are the attention schemes utilized in Cluster-GT. N2C-Attn-T deeply integrates cluster-level and node-level information, whereas N2C-Attn-L autonomously adjusts the balance between these two granularities. Then, we create two additional variants that specifically focus on the node level or the cluster level by setting \(\) in N2C-Attn-L to 0 (exclusively focusing on the node-level kernel) and 1 (exclusively focusing on the cluster-level kernel). We provide a detailed description of the methods compared here in subsection F.3. Figure 5 shows the experimental results. We find that the variants that combine attention from both levels significantly surpass those that do not, with N2C-Attn-T leading marginally. This highlights the effectiveness of N2C-Attn's multiple kernel learning approach in integrating diverse levels of information. We reference the performance of GCN from Table 1 as a baseline.

## 6 Other Methods Involving Graph Coarsening

In this section, we will briefly introduce some existing research on GNNs with graph coarsening to capture broader structural information, aside from the node clustering pooling introduced in section 2.  utilizes a dual-graph structure, employing a hierarchical message passing strategy between a molecular graph and its junction tree to facilitate a bidirectional flow of information. This concept of interaction between the coarsened graph (clusters) and the original graph (nodes) is similar to our N2C-Attn. However, the difference lies in 's approach to propagating messages between clusters and nodes, whereas N2C-Attn integrates cluster and node information directly in the attention calculation using a multiple-kernel method.  introduces a novel node sampling strategy as an adversarial bandit problem and implements a hierarchical attention mechanism with graph coarsening to efficiently address long-range dependencies.  uses graph pooling to coarsen nodes into fewer representatives, focusing attention on these pooled nodes to manage scalability and computational efficiency.  introduces the Subgraph-To-Node (S2N) translation method, coarsening subgraphs into nodes to improve subgraph representation learning.  introduces HIGH-PPI, a double-viewed hierarchical graph learning model that uses a hierarchical graph combining protein-protein interaction networks and chemically described protein graphs to accurately predict PPIs and interpret their molecular mechanisms. Despite achieving good results in their respective downstream tasks, these methods still follow the graph coarsening pipeline, whereas our work attempts to break this limitation and has demonstrated effectiveness on various graph-level tasks.

Figure 5: Comparison of different attention strategies. We restrict the attention module in Cluster-GT to focus on different granularities. N2C-Attn-T and N2C-Attn-L represent schemes that integrate information at both the node and cluster granularities. Cluster-Level-Attn focuses solely on cluster-level information, i.e., \(=1\), while Node-Level-Attn focuses solely on node-level information, i.e., \(=0\). We provide a detailed description of the methods compared here in subsection F.3.

Conclusion

Our Node-to-Cluster Attention mechanism leverages the strengths of both node-level and cluster-level information processing without succumbing to the limitations of the graph coarsening pipeline. By conceptualizing the graph as interconnected node sets and integrating kernelized attention with multiple kernel learning, we effectively bridge the gap between cluster-level and node-level spaces, capturing the hierarchical structure of graphs as well as the node-level information. We develop an efficient form of N2C-Attn using the message-passing framework and techniques of kernelized softmax. Our Cluster-wise Graph Transformer, empowered by a straightforward partitioning strategy and the N2C-Attn module, demonstrates robust performance across diverse graph datasets. Extensive experiments have demonstrated the effectiveness of our Cluster-GT and N2C-Attn modules. We offer a further discussion on the current limitation and potential impact in Appendix H and Appendix I.