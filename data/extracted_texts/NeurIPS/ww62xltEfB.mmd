# A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness

A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness

Yuri Kinoshita, Taro Toyoizumi

Department of Mathematical Informatics,

Graduate School of Information Science and Technology,

The University of Tokyo, Tokyo, Japan.

Laboratory for Neural Computation and Adaptation,

RIKEN Center for Brain Science, Wako, Japan.

yuri-kinoshita111@g.ecc.u-tokyo.ac.jp

###### Abstract

While neural networks can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood. To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them. Especially, throughout the past few years, the concept of _bi-Lipschitzness_ has been proved as a beneficial inductive bias in many areas. However, due to its complexity, the design and control of bi-Lipschitz architectures are falling behind, and a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants along with solid theoretical analysis is lacking. In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality. Its desirable properties are illustrated with concrete experiments to illustrate its broad range of applications.

## 1 Introduction

### Background

Nowadays, neural networks have become an indispensable tool in the field of machine learning and artificial intelligence. While they can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood. To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them. Especially, throughout the past few years, the concept of _sensitivity_ has been proved as a beneficial inductive bias in many areas.

Sensitivity can be translated into the concept of _bi-Lipschitzness_, which combines two different properties, namely, _Lipschitzness_ and _inverse Lipschitzness_. The former describes the maximal, and the latter the minimal sensitivity of a function. Bi-Lipschitzness is attracting interests in various fields not only as it proposes a promising solution to avoid unexpected and irregular results caused by a too sensitive or too insensitive behavior of the trained function, but also as it achieves an approximate isometry that preserves geometries of the input dimension (Li et al., 2020). It plays an essential role in generative models such as normalizing flows to guarantee invertibility (Behrmann et al., 2019), in uncertainty estimation to prevent the recurrent problem of _feature collapse_(Liu et al., 2020; Van Amersfoort et al., 2020) and in inverse problems (Kruse et al., 2021) to assure the stability of both the forward and inverse function (Behrmann et al., 2021).

Unfortunately, the appropriate design and effective control of bi-Lipschitz neural networks are far from simple, which hinders their application to prospective areas. First of all, the _estimation_ of bi-Lipschitz constants is an NP-hard problem (Scaman and Virmaux, 2018). Second, the _design_ of bi-Lipschitz models is even harder as we cannot straightforwardly extend existing Lipschitz architectures that exploit some unique properties of the concept to inverse Lipschitzness and keep their advantages, and _vice-versa_. Finally, the _control_ of bi-Lipschitz constants requires particular attention as it is a question of manipulating two distinct concepts in a preferably independent and simple manner.

Currently existing bi-Lipschitz models still present some issues in terms of design or control. On the one hand, some lack theoretical guarantees because they impose soft constraints by adding regularization terms to the loss function (Van Amersfoort et al., 2020), or because their expressive power is not well-understood and may be more limited than expected. On the other hand, others restrict bi-Lipschitzness on a layer-wise basis (Behrmann et al., 2019; Liu et al., 2020). Particularly, this means these approaches can only build in the essence a simple bi-Lipschitz function employed as a layer of a more complex neural network. In practice, this kind of parameterization impacts the generalization ability of the model or leads to loose control (Fazlyab et al., 2019). They can also contain so many parameters affecting sensitivity to the same extent that controlling all of them is unrealistic and fixing some may affect the expressive power. See Section 2 for further details.

Therefore, taking into account both the potential and complexity of this inductive bias, we first and foremost need a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants of the overall function more complex than a single layer neural network equipped with solid theoretical analysis. In this work, we investigate an architecture that can achieve such a clear and tight control and apply it to several problem settings to illustrate its effectiveness.

### Contributions

Our contributions can be summarized as follows. First, we construct a model bi-Lipschitz by design based on _convex neural networks_ and the _Legendre-Fenchel duality_, as well as a _partially_ bi-Lipschitz variant. This architecture provides a simple, direct and tight control of the Lipschitz and inverse Lipschitz constants through only two parameters, the ideal minimum, equipped with theoretical guarantees. These characteristic features are illustrated and supported by several experiments including comparison with prior models. Finally, we show the utility of our model in concrete machine learning applications, namely, uncertainty estimation and monotone problem settings and show that it can improve previous methods.

OrganizationIn Section 2, we will first explain in more detail existing architectures around bi-Lipschitzness. In Section 3, we will develop our model followed by theoretical analyses. The next Section 4 will be devoted to experiments and applications of our proposed method.

NotationThroughout this paper, the Euclidean norm is denoted as \(\|\|\) for vectors unless stated otherwise. Similarly, for matrices, \(\|\|\) corresponds to the matrix norm induced by the Euclidean norm. For a real-valued function \(F:^{m}\), \( F\) is defined as \(( f(x)/ x_{1},, f(x)/ x_{n})^{}\), and \(^{}F\) as its transpose. When the function is a vector \(F:^{m}^{n}\), then its Jacobian is defined as \(^{}F=( f_{i}/ x_{j})_{i,j}\). The subderivative is denoted as \(_{}\).

## 2 Preliminaries

In this section, we explain mathematical backgrounds and existing bi-Lipschitz models to clarify the motivation of this work.

### Definition

Let us first start by the definition of bi-Lipschitzness.

**Definition 2.1** (bi-Lipschitzness).: _Let \(0<L_{1} L_{2}\). \(f:^{l}^{t}\) is \((L_{1},L_{2})\)-bi-Lipschitz if \(L_{1}\|x-y\|\|f(x)-f(y)\| L_{2}\|x-y\|\) holds for all \(x,y^{l}\). The right (left) inequality is the (inverse) Lipschitzness with constant \(L_{2}\) (\(L_{1}\)). \(L_{1}\) and \(L_{2}\) are called together bi-Lispchitz constants._Several interpretations can be attributed to this definition. Bi-Lipschitzness appears when we want to guarantee a high sensitivity and a high smoothness for better generalization such as in uncertainty estimation. This can be also regarded as a quasi-isometry with a distortion of \(L_{2}/L_{1}\), which means that the structure of the input is relatively inherited in the output as well. Moreover, since a bi-Lipschitz function is by definition invertible, the inverse Lipschitz can be interpreted as the Lipschitz constant of the inverse function. Therefore, controlling the bi-Lipschitz constants is beneficial for the stability of both the forward and inverse.

### Desired Features for a Controllable Inductive Bias

In order to gain new insights of a problem and a predictable behavior of the bi-Lipschitz model, it is primordial that bi-Lipschitzness works as an inductive bias with high controllability and theoretical foundation. For this goal, the following points are of particular interest: (a) bi-Lipschitzness guaranteed by design, (b) theoretical bounds on the bi-Lipschitz constants, (c) tight bounds, (d) independent control of the Lipschitz and inverse Lipschitz constants, (e) a minimal number of hyper-parameters to tune, (f) theoretical guarantee on the expressive power and (g) direct parameterization of the optimized variables (in the sense of Definition 2.3 of Wang and Manchester (2023)).

### Related Works and Bi-Lipschitz Models

There are currently three methods that are mainly employed to achieve and control bi-Lipschitz neural networks. See Appendix A for a brief overview on Lipschitz and inverse Lipschitz architectures.

RegularizationThe first one is based on a regularization method (Gulrajani et al., 2017; Van Amersfoort et al., 2020). It adds to the loss a term that incites bi-Lipschitzness. While the method is simple, the resulting function may not be bi-Lipschitz at the end of the training, we have no theoretical bounds on the bi-Lipschitz constants and cannot achieve a real control of them.

i-ResNetThe invertible residual network (i-ResNet) of Behrmann et al. (2019) is based on the composition of several \((1-L_{g_{i}},1+L_{g_{i}})\)-bi-Lipschitz layers of the type \(f_{i}(x)=x+g_{i}(x)\), where \(g_{i}\) is Lipschitz with constant \(L_{g_{i}}<1\). The Lipschitz \(g_{i}\) can be constructed by layer-wise methods such as spectral normalization of weights (SN) (Miyato et al., 2018). However, this kind of layer-wise control of (bi-)Lipschitzness is known to be loose in general (Fazlyab et al., 2019). See Figure 1 for an illustration. The i-ResNet has limited expressive power as it cannot represent the function \(y=-x\)(Zhang et al., 2020), and the construction of \(g_{i}\) risks to introduce more restriction on the expressive power in practice (Anil et al., 2019). A similar model, the Lipschitz monotone network (LMN) of Nolte et al. (2023) combines the GroupSort activation function with SN to create \(g_{i}\). These approaches requires to adjust the Lipschitzness of each weight during the training process, which may be sub-optimal (i.e., no direct parameterization).

BiLipNetThe concurrent work of Wang et al. (2024) also provides a bi-Lipschitz neural network (BiLipNet), which was mainly used for creating a Polyak-Lojasiewicz function useful in surrogate loss learning. It extends the Lipschitz architecture of Wang and Manchester (2023). The BiLipNet is constructed as the composition of monotone Lipschitz layers and orthogonal layers. The former layer is realized based on a direct parameterization of the IQC theory of Megretski and Rantzer

Figure 1: Results of fitting \(y=50x\) with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound \(L\). \(L=50\) (red line) is where an \(L\)-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around \(L=100\) while ours at \(L=50\). See Appendix G.4 for further details.

(1997). While they can achieve a certified control of the bi-Lipschitz constant, theoretical guarantee on its expressive power is lacking. Moreover, BiLipNet composes many layers which may harm the tightness of the bounds.

Therefore, current models fail to satisfy the desired features of Subsection 2.2. Notably, all these points cannot be satisfied as long as we rely on the simple layer-wise control which offers too conservative bounds and a number of tunable hyper-parameters proportional to that of the layers in general. In this work, we will establish a bi-Lipschitz architecture that satisfies almost all these expected attributes mentioned above. In terms of mathematical foundations, it takes inspiration from the inverse Lipschitz architecture of Kinoshita et al. (2023).

## 3 Bi-Lipschitz Neural Network

We introduce our novel bi-Lipschitz model, followed by some theoretical analyses and discussion.

### Additional Definitions

We clarify notions of smoothness, convexity and the Lengendre-Fenchel transformation (LFT) which will be a core process in our construction.

**Definition 3.1**.: _Let \(>0\). \(F:^{m}\) is \(\)-smooth if \(F\) is differentiable and \( F\) is \(\)-Lipschitz._

**Definition 3.2**.: _Let \(>0\). \(F:^{m}\) is \(\)-strongly convex if \(F(x)-\|x\|^{2}\) is convex._

**Definition 3.3**.: _Let \(F:I\) a convex function over \(I^{m}\). Its Legendre-Fenchel transformation \(F^{*}\) is defined as \(F^{*}(x):=_{y I}\{ y,x-F(y)\}\)._

### Construction of Bi-Lipschitz Functions

We extend the approach of Kinoshita et al. (2023) and propose a method that creates bi-Lipschitz functions, where it is sufficient to control two parameters to manipulate the overall bi-Lipschitzness without needing to dissect the neural network and control each layer, a feature that existing methods for (bi-)Lipschitzness cannot deliver. The first step is to notice that the gradient of a real-valued \(\)-strongly convex function becomes \(\)-inverse Lipschitz, and that of a \(\)-smooth function \(\)-Lipschitz by definition. Therefore, we aim to compose a function which is both strongly convex and smooth. Interestingly, the LFT of a \(1/\)-strongly convex function is \(\)-smooth. This leads to our main theorem.

**Theorem 3.4**.: _Let \(F\) be a closed \(1/\)-strongly convex function and \( 0\). Then the following function is \(\)-strongly convex and \(+\)-smooth: \(_{y I}\{ y,x-F(y)\}+\|x\|^{2}\). Thus, its derivative is \((,+)\)-bi-Lipschitz which equals \(f^{*}(x):=*{argmax}_{y}\{ y,x-F( y)\}+ x\)._

See Appendix B.2 for the proof. The term \(/2\|x\|^{2}\) has the effect of turning a convex function into an \(\)-strongly convex function by definition of strong convexity. This \(f^{*}(x)\) constructed as described in the above theorem is precisely the bi-Lipschitz model we propose in this paper.

### Implementation of the Forward Pass

Based on Theorem 3.4, we can create a bi-Lipschitz function parameterized by neural networks, and thus applicable to machine learning problems. Here, we clarify the implementation of a strongly convex function and the LFT. The overall explicit formulation of our bi-Lispchitz neural network (BLNN) is already shown in Algorithm 1 for convenience.

Strongly Convex Neural NetworksA \(\)-strongly convex neural network can be constructed by adding a regularization term to the output of the Input Convex Neural Network (ICNN) from Amos et al. (2017). The resulting structure can be written as \(F_{}(y)=z_{k}+\|y\|^{2}\), where \(z_{i+1}=g_{i}(W_{i}^{(z)}z_{i}+W_{i}^{(y)}y+b_{i})\)\((i=0,,k-1)\). \(\{W_{i}^{(z)}\}_{i}\) are non-negative, \(W_{0}^{(z)}=0\), and all functions \(g_{i}\) are convex and non-decreasing. This architecture is a universal approximator of \(\)-strongly convex functions defined on a compact domain endowed with the sup norm (Chen et al., 2019). Note that any other choice for the convex architecture is possible. This is only an example, also employed in Huang et al. (2021) and Kinoshita et al. (2023). Indeed, we could opt for a convolutional version by using the convolutional ICNN proposed by Amos et al. (2017).

Algorithms for LFTBy Theorem 3.4, we only need to compute the optimal point of the LFT, i.e., \(y_{}^{*}(x):=*{argmin}_{y}\{F_{}(y)- y,x \}\), which is a strongly convex optimization. This computation is thus rather fast, and we can use various algorithms with well-known convergence guarantees such as the steepest gradient descent (Shamir and Zhang, 2013; Bansal and Gupta, 2017). Such convex solvers will generate for a fixed \(x\) a sequence of points \(\{y_{t}(x)\}_{t=0,,T}\), and its last point will be an estimate of \(y_{}^{*}(x)\). However, this kind of discrete finite time approximation could compromise the bi-Lipschitzness of the whole algorithm. The bi-Lipschitz behavior of \(y_{t}(x)\) can be explicitly described for the steepest gradient descent as follows. For other algorithms, see Appendix C.2.1.

**Theorem 3.5**.: _Let the symbols defined as in Algorithm 1. Consider the steepest gradient descent of \(_{y}\{ y,x-F_{}(y)\}\) generating points \(\{y_{t}(x)\}_{t}\) at the \(t\)-th iterations and \(y_{}^{*}(x)\) is the global maximum. If \(F_{}\) is \(\)-strongly convex and \(\)-smooth then the point \(_{t}y_{t}(x)\) is \((1/,1/)\)-bi-Lipschitz without any bias. Moreover, with \(_{t}=1/((t+1))\) as a step size and \(y_{0}(x_{i})=y_{0}\) as initial point, then for all \(x_{i}\), \(x_{j}\), \(\|y_{t+1}(x_{i})-y_{t+1}(x_{j})\| h(t)\|x_{i}-x_{j}\|\) where \(_{t}h(t)=1/\)._

See Appendix C.2.2 for the concrete formulation of \(h(t)\) and the proof. The above theorem guarantees that the optimization scheme will ultimately provide a \(y_{t}(x)\) that is bi-Lipschitz with the right constants. This was not trivial as a bias may have persisted due to the non-zero step size. More importantly, this statement offers a non-asymptotic bound for Lipschitzness. This is greatly useful when we theoretically need to precisely assure a certain degree of Lipschitzness such as in robustness certification against adversarial attacks (Szegedy et al., 2013). The step size \(_{t}=1/((t+1))\) can be precisely calculated as \(=1/\) for an \((,)\)-BLNN. Experimental results suggest a similar behavior for inverse-Lipschitzness. Note the strong convexity of \(F\) is always satisfied in our setting and smoothness can be fulfilled if we use an ICNN with softplus activation functions \((1+^{x})\). Since this simple gradient descent has been thoroughly analysed and assured to properly behave in practice as well, we will employ it as the algorithm of LFT in the remainder of this paper.

### Expressive Power

As we mentioned earlier, layer-wise approaches realize a bi-Lipschitz neural network by restricting the sensitivity of each layer. Nevertheless, this kind of construction may be sub-optimal as it does not take into account the interaction between layers, limiting more than expected the expressive power, which was often not considered in the original papers. Layer-wise bi-Lipschitz approaches cannot inherit the proofs and properties of the original network, which makes the understanding of their behavior more difficult. As for our model, the \((,)\)-BLNN, we can guarantee the following universality theorem.

**Theorem 3.6**.: _For any proper closed \(\)-strongly convex and \(+\)-smooth function on a compact domain, there is a BLNN without taking the gradient at the end, i.e., \(_{y}\{ y,x-(G_{}(y)+\|y\|^{2}/(2))\}+\|x \|^{2}/2\) where \(G_{}\) is an ICNN with ReLU or softplus-type activation function, that approximates it within \(\) in terms of the sup norm._

Thus, after taking the gradient, we can create a sequence of functions that converges point-wise to any function which is \(+\)-Lipschitz, \(\)-strongly monotone (Definition B.12) and the derivative of a real-valued function. See Appendix B.3 for the proofs and further discussion.

### Backward Pass of BLNN

The most straightforward way to compute the gradient of a BLNN is to track the whole computation of the forward pass including the optimization of the LFT and back-propagate through it. However,this engenders a crucial bottleneck since back-propagating over the for-loop of the optimization involves many computations of the Hessian and the memorization of the whole computational graph. Nevertheless, if the activation function of the ICNN is chosen as softplus, the convergence of the LFT is quite fast making this strategy scalable to moderately large data.

Interestingly, when \(F_{} F(;)\) is \(C^{2}\), the backward pass can be computed only with the information of the core ICNN, which means we do not need to track the whole forward pass involving many recurrent computations:

**Theorem 3.7**.: _Suppose a loss function \(L:z L(z)\), and the output of the BLNN is \(f^{*}(x;):= F^{*}_{}(x)+ x\) as defined in Algorithm 1. If \(F\) is \(C^{2}\) and \(F^{*}\) is differentiable, then the gradient \(_{}^{}L(f^{*}(x;))\) can be expressed as \(-_{z}^{}L(z)\{_{y}^{2}F(y^{*}(x;);)\} ^{-1}_{}^{}_{y}F(y^{*}(x;);)\)._

See Appendix B.4 for the proof and formulation with more complex architectures. In practice, coding is simple since we can directly employ the backward algorithm provided in libraries such as Pytorch (Corollary B.19). The advantage of this second method is threefold. First, it can reduce the computational and memory cost of the backward process as it does not depend on the iteration number and involves fewer matrix manipulations. Second, this leaves the freedom to choose the optimizer to calculate the LFT, while the brute force method requires the gradient to be tracked during the optimization process, which is a feature that many current solvers do not provide. Third, this method approximates the gradient by taking the derivative along the true curve at a point \(y^{(t)}_{}(x)\) close to \(y^{*}_{}(x)\), which is a fair approximation. In contrast, the brute force method approximates the gradient based on the recurrent computation of \(y^{(t)}_{}(x)\) but we do not know whether this is really a good estimate.

### Comparison with Deep Equilibrium Models

Interestingly, our BLNN can be regarded as a bi-Lipschitz version of a broader family of models called deep equilibrium models (DEQs) from Bai et al. (2019). A DEQ is an implicit model whose output \(z\) is defined as the fixed point of a neural network \(h_{}\), i.e., \(z=h_{}(z,x)\), where \(x\) is the input. Similarly, the output of our algorithm can be re-formulated as finding the solution \(z\) of \(z=x- F_{}(z)+z\) which corresponds to a DEQ with \(h_{}(z,x)=x- F_{}(z)+z\). The general properties of a DEQ concerning the computational complexity (both time and space) are thus also inherited in our model. One major difference is that the iteration to find this fixed point \(z\) is in our case unique and guaranteed to converge, which addresses one of the main concerns of general DEQs. We believe this interpretation is promising for future work to increase the generality of our model as it enables to converge to the larger flow of work around DEQs and apply the various improvements for DEQs researched so far. However, we will not pursue this direction further since our primary goal is to develop a bi-Lipschitz model with direct and simple control.

### Increased Scalability and Expressive Power: Partially BLNN

Despite all the theoretical guarantees mentioned above and the correspondence of the proposed method to DEQs, its main drawbacks persist in computational efficiency and expressive power. However, these weaknesses can be alleviated by imposing the bi-Lipschitz requirement on a limited number of variables. This can be realized by using the partially input convex neural network (PICNN) instead of the ICNN in our architecture (Amos et al., 2017). A PICNN is convex with respect to a pre-defined limited set of variables. Based on this architecture, we can proceed similarly to Algorithm 1 and obtain a _partially_ bi-Lipschitz neural network (PBLNN). See Appendix F.2 for further details. As a result, all heavy operations such as the LFT and the gradient are applied on this smaller set of variables, which makes the architecture much more scalable to higher dimensions. Moreover, a PICNN with \(k\) layers can represent any ICNN with \(k\) layers and any purely feedforward network with \(k\) layers, as shown in Proposition 2 of Amos et al. (2017). This also enhances the expressive power of our model with larger liberty on the precise construction of the architecture.

### Computational Complexity

Regarding the time and space complexity of our model, it is largely equivalent to that of a DEQ (Bai et al., 2019) except that the core function is the derivative of a neural network, which adds some computational burden. In Figure 2, we present a comparison of the computational complexity in floating point number operations (FLOPs) and memory for a single iteration between a traditional feedforward network and various BLNN variants: BLNN (with brute force backpropagation), BLNN with Theorem 3.7, PBLNN with only one variable constrained to be bi-Lipschitz. Comparing our model with Theorem 3.7 to a traditional feed-forward neural network, we can conclude that their difference is only a factor of order 10. Theorem 3.7 greatly contributes to reducing both computational and space requirements (improvement of order of \(10\) and \(10^{2}\), respectively). This explains the scalability of our model to large datasets. Finally, the PBLNN clearly decreases the complexity of the model by limiting the number of variables we impose bi-Lipschitzness. See Appendix H.1 for details on the experimental setup.

### Discussion

Our bi-Lipschitz model BLNN (and PBLNN) possesses interesting properties. First, it provides by design a theoretical bound of the bi-Lipschitz constants. Some methods cannot necessarily afford this. Furthermore, it enables a direct parameterization of the overall bi-Lipschitzness through the addition of two regularization terms with respective coefficients \(\) and \(\) at different outputs. This translation of bi-Lipschitzness into strong convexity plays an indispensable role as we do not disturb the formulation of the core function, do not track the transformation of the input down to the output and only have two hyper-parameters to monitor, the strict minimum. As a result, we can create bi-Lipschitz functions with known expressive power and more complex than a single layer which others had to compose many times to create a deep bi-Lipschitz neural network. We also avoid the risk of losing tightness in the bounds of the bi-Lipschitz constants caused by this kind of layer-wise parameterization. Therefore, our model satisfies the desired points of Subsection 2.2.

While the PBLNN solves several issues of the BLNN, optimization inside the forward pass is still an expensive procedure. Nevertheless, there are plenty of approaches to accelerate the LFT and backpropagation through approximations as well. Since the objective function of the LFT is always strongly concave, its convergence speed does not depend on the dimension, and the only bottleneck is the computation of the gradient information of the objective function (i.e., the ICNN). Approximations by zeroth-order methods can improve the procedure, for example. Moreover, amortizing the LFT as Amos (2023) did may also be a promising simplification of the forward pass. On the other hand, backpropagation through the whole forward pass can also be simplified based on Theorem 3.7. For instance, the Hessian matrix can be replaced by its diagonal or upper triangular elements, making the inversion operation easier. Other improvements, including those for the backward pass, can be adapted from research on DEQs (Bai et al., 2019).

Another limitation, which is overcome by the PBLNN, is that the BLNN cannot represent a function that is not the gradient of a convex function. However, such a type of function is the core of machine learning problems related to optimal transport (Santambrogio, 2015) and some physical systems (Huang et al., 2021). In the following chapter, we will show that our model can be applied to various problems and its performance is not overshadowed by these limitations. In short, it is a necessary price to pay to gain a bi-Lipschitz model with features such as known expressive power and high controllability so that it can outperform other methods with looser constraint but lower controllability and fewer guarantees. Indeed, if we define the _unit_ of a bi-Lipschitz model as the basic architecture that requires the minimal number of hyperparameters, i.e., one for Lipschitzness and one for inverse-Lipschitzness, most existing models are limited to constructing simple (bi-)Lipschitz units with low expressive power (e.g., only linear) and they have to compose those units to achieve higher expressive power, which leads to looser bounds. In that sense, our model still has a higher expressive

Figure 2: Comparison of the time (left) and space (right) complexity for a single iteration between a traditional feedforward network and various BLNN variants.

power and tighter bounds than other (bi-)Lipschitz units as ours can _with only one unit_ produce complex functions and its parameterization is not layer wise. Now, if we can afford to sacrifice the tightness of the bounds to further increase expressive power, we can proceed like other methods by stacking multiple BLNNs or combining them with other architectures to suit the characteristics of the problem at hand (see Appendix F).

Further substantial extensions of our method can be found in Appendix F, such as generalization to different input and output dimensions, to non-homogeneous functions and to other norms.

## 4 Experiments

Experimental setups can be found in Appendix H and codes in https://github.com/yuri-k111/Bi-Lipschitz-Neural-Network. Further detailed results are summarized in Appendix G.

### Bi-Lipschitz Control

In this subsection, the goal is to empirically verify that (i) our model achieves a tight control of bi-Lipschitz constants, and (ii) it provides new beneficial behaviors different from other methods. We focus on simple problems as they effectively convey key ideas.

Tightness of the BoundsHere, we verify the quality of the bi-Lipschitz bounds when the model undergoes training. Especially, we focus on the Lipschitz bound since it is the most affected by the approximation of LFT, and there exist many works to compare with it. Inspired by the experiment of Wang and Manchester (2023), we aim to learn the function \(f(x)=x\ (x<0),\ x+1\ (x 0)\) that has a discontinuity at \(x=0\). We take as comparison the LMN (Nolte et al., 2023), the BiLipNet (Wang et al., 2024) and the i-ResNet network represented by its substructures: spectral normalization (SN) (Miyato et al., 2018), AOL (Prach and Lampert, 2022), Orthogonal (Trockman and Kolter, 2021), SLL (Araujo et al., 2023), Sandwich (Wang and Manchester, 2023). The Lipschitzness of each model is constrained by a constant \(L\). A model with a tight Lipschitz bound should achieve that upper bound around \(x=0\) in order to reproduce the behavior of \(f\). The percentage between the empirical Lipschitz constant and the imposed upper bound \(L\) can be found in Table 1. Interestingly, our method is the only one that achieves an almost perfect score for all settings, while for others the tightness is decreasing. This can be understood as the result of the direct control of bi-Lipschitzness without relying on the information of the individual layers and the construction which uses only direct parameterizations. See Figure 3 for a visualization and Appendix G.2 for more results.

   Models & \(L=5\) & \(L=10\) & \(L=50\) \\  SN & 80.1\% & 68.9 \% & 32.5\% \\ Orthogonal & 75.1\% & 48.9\% & 14.6\% \\ AOL & 65.9\% & 46.5\% & 15.4\% \\ SLL & 77.6\% & 50.0\% & 15.7\% \\ Sandwich & 84.3\% & 60.2\% & 16.4\% \\ LMN & **100.0\%** & 98.5\% & 26.0\% \\ BiLipNet & 98.0\% & 58.9\% & 6.8\% \\ Ours & **100.0\%** & **99.4\%** & **99.9\%** \\   

Table 1: Tightness of Lipschitz bound when fitting \(f(x)=x\ (x<0),\ x+1\ (x 0)\). Mean over five trials. See Table 5 for further results.

Figure 3: Results of fitting \(f(x)=x\ (x<0),\ x+1\ (x 0)\) with SLL (left), Sandwich (middle) and our method (right) with a specified Lipschitzness of 50. See Figure 16 for further details.

Flexibility of the ModelIf we can underestimate the Lipschitz constant of the target function as the previous experiment, we may also overestimate it. In this case, we observe that previous methods are influenced by the imposed Lipschitz bound presenting high fluctuations in the learned representation or slower learning speed. This is illustrated when learning the identity function \(y=x\) with a Lipschitz constraint of \(L=1000\) in Figure 4. Our model can learn without any problem and the learning speed is hardly affected by \(L\). This may be due to the way we control the Lipschitz constant of the model. In layer-wise models, the Lipschitz constant is adjusted by scaling the input, while in ours we add a regularization term at the end, resulting in different loss landscapes with seemingly better regularization performance for the latter strategy. See Appendix G.3 for more results.

### Uncertainty Estimation

We now illustrate that our model can be efficiently used in areas where bi-Lipschitzness already plays an essential role as an inductive bias. In the estimation of uncertainty based on a single neural network, out-of-distributions and in-distributions points may overlap in the feature space making them indistinguishable. Imposing inverse Lipschitzness on the underlying neural network of the architecture is thus important as it can avoid this phenomenon called _feature collapse_. Lipschitzness is also beneficial to improve the generalization performance. In one of the state-of-the-art approaches called deterministic uncertainty quantification (DUQ) from (Van Amersfoort et al., 2020), bi-Lipschitzness is constrained through a regularization method. Therefore, we can replace the gradient penalty with a hard restriction by changing the neural network to a BLNN, resulting in a method we call here DUQ+BLNN. See Appendix E for a precise formulation of this method and theme.

Two moonsThe first experiment we lead is with the two moons dataset. Here, we compare DUQ+BLNN with the deep ensembles method (Lakshminarayanan et al., 2017), DUQ and a DUQ with no bi-Lipschitz regularization (DUQ with no reg.). Results are plotted in Figure 5. The ideal scenario is for the boundary of the yellow area to be as close as possible to the training data. As we can observe, by adding a bi-Lipschitz constraint, the area of certainty is decreased around the training data. DUQ+BLNN with \((,)=(2,4)\) achieves a tighter area than DUQ. We chose the value of \(\) and \(\) based on a grid search. We took the hyper-parameters with the highest accuracy and \(\) since a higher \(\) is expected to create a tighter yellow area as shown in Figure 5. Note this strategy only relies on the training data, and such strategy is possible thanks to our direct and simple parameterization of the bi-Lipschitz constants. This clearly shows the advantage of the unique features of our model. See Appendix G.6.1 for further results and discussion on this experiment.

Fashion-MNISTNext, we use real world data of FashionMNIST (Xiao et al., 2017), MNIST (Leun and Cortes, 2010) and NotMNIST (Bulatov, 2011). The task is to learn to classify FashionMNIST, but at the end of training we verify whether the uncertainty of the model significantly increases when other types of data such as MNIST or NotMNIST is given to the model. This task to distinguish FashionMNIST from MNIST datasets is known to be a complicated task (Van Amersfoort et al., 2020). We compute the AUROC for the detection performance. The result is shown in Table 2. Our

Figure 4: Results of fitting the linear function \(y=x\) with (from left) AOL, Sandwich, BiLipNet and our method with a specified Lipschitzness of 1000. See Figures 17 and 18 for further results.

Figure 5: Uncertainty estimation with the two moons data set with several models. Blue indicates high uncertainty, and yellow low uncertainty. (d)-(f) are with DUQ+BLNN, where \((,)\) are clarified.

model achieves not only higher performance for FashionMNIST but also better detection of MNIST and NotMNIST dataset. See Appendix G.6.2 for further results and discussion on this experiment.1

### Partially Monotone Settings

Sometimes, it happens that we have preliminary knowledge on some type of monotone behaviors of the dataset (Nolte et al., 2023). For example, COMPAS (Angwin et al., 2016), BlogFeedBack LoanDefaulter (Nolte et al., 2023), HeartDisease (Janosi et al., 1988) and AutoMPG (Quinlan, 1993) are benchmark datasets that possess a monotone inductive bias on some variables. See Nolte et al. (2023) for further details on the dataset. As a result, it is more efficient to tune the architecture of the trained model so that it successfully reflects this inductive bias, and various models have been proposed to address this challenge. This is another field where we can apply our architecture that can create monotone (or inverse Lipschitz) functions. We can also control the Lipschitzness to improve the generalization performance. We compare our PBLNN with two state-of-the-art methods: LMN (Nolte et al., 2023) and SMNN (Kim and Lee, 2024) in Table 3, and with other models in Table 9 as well. As we can observe, our model is competitive with the others.

Generalization and Scalability TestFurthermore, we used the dataset provided by Nolte et al. (2023), CIFAR101, which is a slight augmentation of the original CIFAR100 dataset and designed to exhibit a monotone behavior with respect to one variable. We adopted their training scheme, utilizing the entire dataset for training and intentionally overfitting the data in order to assess both the scalability and expressive power of the model. Successfully, we achieved a 0 loss and 100% accuracy for this experiment, and the convergence was faster than that of Nolte et al. (2023). This illustrates the high scalability and expressive power of the PBLNN.

## 5 Conclusion

We built a model called BLNN based on convex neural networks and the LFT so that bi-Lipschitz functions more complex than a single layer can be constructed and its bi-Lipschitz constants manipulated through the coefficient of two regularization terms added at different outputs. That way, BLNN not only achieves such a tight, direct and simple control but also provides rigorous straightforward analysis on the expressive power and on approximations involved in practice. We illustrated with experiments its distinctive advantageous features compared to prior models. While the primary focus of this paper was to establish a framework suited for solid analyses and for the practical control of bi-Lipschitzness, it is, of course, essential to complement this effort with more varied machine learning applications in future work. We still believe this work on its own contributes to the further exploitation of bi-Lipschitzness in prospective fields and to the deeper understanding of neural networks by delivering a model with unique features for the control of this central inductive bias.

   Models & Accuracy & BCE & AUROC MNIST & AUROC NotMNIST \\  DUQ & 0.889 & 0.064 & 0.862 & 0.900 \\ DUQ+BLNN & **0.899** & **0.060** & **0.896** & **0.964** \\   

Table 2: Out-of-distribution detection task of FashionMNIST vs MNIST and FashionMNIST vs NotMNIST with DUQ and DUQ+\((0,3)\)-BLNN. Means over five trials.

   Models & COMPAS (Acc.) & BF (RMSE) & LD (Acc.) & HD (Acc.) & AutoMPG (MSE) \\  LMN & 69.3 \% & 0.160 & 65.4 \% & 89.6 \% & 7.58 \\ SMNN & 69.3 \% & **0.150** & 65.0 \% & 88.0 \% & 7.44 \\ Ours & **69.4 \%** & 0.157 & **65.5 \%** & **90.2 \%** & **7.13** \\   

Table 3: Comparison of our model with state-of-the-art monotone models in benchmark datasets. Means over three trials. Results of LMN and SMNN are from the original papers. BF = BlogFeedBack, LD = LoanDefaulter, HD = HeartDisease, Acc. = accuracy. See Table 9 for complete results.