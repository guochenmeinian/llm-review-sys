# AlpacaFarm: A Simulation Framework for

Methods that Learn from Human Feedback

 Yann Dubois

Stanford

Xuechen Li

Stanford

Rohan Taori1

Stanford

Tianyi Zhang1

Stanford

Jimmy Ba

University of Toronto

Carlos Guestrin

Stanford

Percy Liang

Stanford

Tatsunori B. Hashimoto

Stanford

###### Abstract

Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowd-workers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-\(n\), expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that the rankings of models trained in AlpacaFarm match the rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a+10% improvement in win-rate against Davinci003. We release AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.

## 1 Introduction

Large language models (LLMs) [10; 13; 48] have demonstrated unprecedented capabilities in following diverse and open-ended instructions [49; 5; 40]. These achievements have often been attributed to the fine-tuning of pretrained LLMs using human feedback, but this process remains poorly understood due to the lack of published information on the training methods from LLM vendors. For example, it was recently revealed that only the Davinci003 model in the instruct series of OpenAI models used reinforcement learning (RL) with the PPO algorithm , leading some to question the importance of RL in the training process. Understanding and improving these methods requires open and transparent replications of the training process, but this remains challenging due to the cost and complexity associated with methods that learn from human feedback.

Our goal is to facilitate research and development on instruction following models and methods that learn from human feedback. We identify three main challenges: the high cost of data annotation, the lack of automated evaluation for model development, and the absence of validated implementationsof existing methods. To address these three challenges, we introduce AlpacaFarm (Figure 1), a simulation sandbox that enables experimentation at a low cost. Using AlpacaFarm, researchers can rapidly iterate on method development in simulation and transfer these insights to build high-performance systems with actual human feedback.

For the first challenge of data annotation costs, AlpacaFarm simulates human annotators with API LLMs that are faster and lower cost. To collect simulated feedback data, we design prompts for API LLMs (e.g. GPT-4) that enable us to simulate human pairwise comparisons at a cost that is 45x cheaper than crowd-workers, and tune these prompts to faithfully capture many aspects of human annotators, such as their quality judgments, inter-annotator variability, and stylistic preferences.

For the second challenge of automated evaluation, we design an automatic evaluation protocol that aims to quantify system performance on simple but realistic real-world human instructions. Improving evaluations of open-ended instruction following has been challenging due to the cost and non-replicability of human evaluation, the lack of real human interaction data, and the diversity of natural human instructions. To address this, we use instructions from user interactions (Alpaca Demo ) as a reference for simple but real human interactions and show that we can combine existing evaluation datasets to mimic this evaluation. Quantitative evaluations of system rankings on our evaluation data show a high correlation with system rankings on the Alpaca Demo instructions.

For the third challenge of missing reference implementations, we implement and test several popular learning algorithms including PPO , expert iteration , and Quark , and release reference implementations. We show that, among the methods we studied, PPO with a surrogate reward model is the most effective training-time method in our leaderboard, improving the win-rate against Davinci003 of an instruction fine-tuned LLaMA 7B model from 44% to 55%. Other baselines that have been validated on simpler tasks fall short in comparison, highlighting the importance of testing these algorithms in a real instruction-following setting.

As an end-to-end evaluation of the AlpacaFarm, we compare eleven methods trained and evaluated in AlpacaFarm with the same methods trained and evaluated on actual human feedback. We show that the method rankings obtained from developing on AlpacaFarm closely agree with the method rankings obtained from training on actual human data (Spearman correlation of 0.98) and that the best method in AlpacaFarm leads to substantial gains with human feedback. Finally, we find that AlpacaFarm can replicate qualitative behaviors of human feedback such as over-optimization of the reward model, suggesting that AlpacaFarm serves as an effective way for researchers to rapidly study and develop methods that learn from human feedback.

Figure 1: AlpacaFarm is a simulation sandbox that enables fast and cheap experimentation on methods that learn from human feedback. It simulates human feedback with API LLMs, provides a validated evaluation protocol, and offers a suite of reference method implementations. Researchers can iterate on model development and transfer their methods to training on human data to maximize performance.

Background & problem statement

To begin, we introduce the instruction following task and the pairwise-comparison-based human feedback setting that we study. With this background, we formally define the goals of developing a low-cost simulator for studying instruction following and learning from human feedback.

### Learning to follow instructions

In the instruction following task [49; 6; 76; 40], we are presented with user instructions \(x\) (e.g. "Tell me something about Alpacas"), and our goal is to develop a model \(p_{}\) that generates high-quality responses \(y p_{}(y x)\) as judged by an unobserved human reward function \(R:\).

While there are a rich set of methods that learn from human feedback to directly optimize \(R\) (see Appendix A), in this work we focus on the setting of _learning from pairwise feedback_ (LPF) due to its central role in recent instruction-following LLMs . The starting point of this process is a model that is fine-tuned on instruction-following demonstrations \((x,y)\), which we denote as \(p_{}^{}(y x)\). The LPF process then involves taking pairs of samples from \(p_{}^{}\), querying humans for which sample within each pair is better, and learning from this pairwise feedback. Since all methods start from the SFT base, we use \(p_{}\) for notational simplicity.

Learning from pairwise feedback (LPF).More formally, we define the pairwise feedback dataset as \(_{}=\{(x^{(j)},y_{0}^{(j)},y_{1}^{(j)},z^{(j)})\}_{j}\). In this notation, a human annotator rates two candidate responses \(y_{0},y_{1} p_{}(y x)\) for the instruction \(x\). These binary ratings \(z\{0,1\}\) are assumed to be generated according to their unobserved reward \(R\) and \(z\) indicates a (potentially stochastic) comparison for the better response \(y_{z}\), where \(R(x,y_{z})>R(x,y_{1-z})\).

Many algorithms have been proposed to learn on \(_{}\). Some algorithms like RLHF [18; 49] learn a surrogate reward function as the learning signal and some operate more directly on \(_{}\). We defer the discussion of different learning algorithms to Section 3.4.

Pairwise evaluation.Once instruction-following models are trained, researchers need to evaluate these models. One common approach for evaluating models is pairwise model evaluation [15; 22; 7], which performs pairwise comparisons on outputs generated by the model \(p_{}\) and a reference model \(p_{}\). Concretely, we collect pairwise preference for two models (\(\{(x^{(j)},y_{}^{(j)},y_{}^{(j)},z^{(j)})\}_{j}\)), which is aggregated by computing the average win-rate - the percentage of times \(p_{}\) is preferred to \(p_{}\). Researchers can then compare LPF models by their win-rates against the same reference model.

### Problem statement

The goal of AlpacaFarm is to provide three key components that enable rapid research and development of instruction following models: low-cost pairwise feedback generators, automated evaluations for methods development, and reference implementations for comparison and modification. With these three components, researchers can develop new methods in simulation and transfer these insights to build high-performance systems on actual human feedback.

For pairwise feedback, we substitute human preference judgements \(z_{} p_{}(z x,y_{0},y_{1})\) with a simulated preference \(z_{} p_{}(z x,y_{0},y_{1})\) using API LLMs. Our goal is to construct a \(p_{}\) that is both low-cost and faithfully captures different aspects of human preference feedback, such as quality judgments, inter-annotator agreement rates, and stylistic preferences.

For evaluations, we evaluate system outputs using the pairwise preference simulator and identify evaluation datasets that reflect natural human-LLM interactions. The goal for our evaluations is to ensure that system rankings on the new evaluation dataset closely match both human rankings and rankings on instructions from real usage of the Alpaca Demo.

For reference methods, we develop and evaluate six LPF methods. Our goal will be to provide simple and working implementations that provide substantial improvements on both simulated and human feedback data. This will allow researchers to build upon and compare to competitive baselines in a complex instruction-following environment.

AlpacaFarm combines these three components into a simulation framework for learning from pairwise feedback. We evaluate the complete system by an end-to-end workflow of developing methods in simulation and transferring the insights to the real world.

Concretely, we will run each method \(M\) on the simulated preferences (called \(M_{}\)) and evaluate with simulated rankings \(p_{}\). In parallel, we will run \(M\) on human preferences (called \(M_{}\)) and evaluate with human rankings \(p_{}\). We consider AlpacaFarm to be successful if the simulated method rankings correlate well with the human method rankings. The rest of this work will present the details of the pairwise feedback and evaluation design (Section 3), evaluate these designs (Section 4), and analyze the different reference methods we implemented in the AlpacaFarm (Section 5).

## 3 Constructing the AlpacaFarm

In this section, we detail how we construct the AlpacaFarm. In Section 4, we then validate our design choices by comparing the LPF workflow with human feedback and evaluation.

### Instruction following data

Before defining the details of how we simulate pairwise feedback, we must first specify a large and diverse set of instructions \(x\) upon which we can build the rest of AlpacaFarm. We opt to use the Alpaca data  as a starting point due to its large size (52k \((x,y)\) examples) and the non-trivial instruction following capabilities of models trained on this data.

We repurpose the Alpaca data into splits suitable for learning from human feedback by following a similar splitting ratio as . We created four splits (42k in total), leaving 10k for the future:

* Supervised finetuning (SFT) split: 10k data for fine-tuning the base instruction-following LLM used in subsequent steps.
* Pairwise preference (PREF) split: 10k instructions on which we collect pairwise feedback data.
* Unlabeled split: 20k unlabeled instructions used in algorithms such as PPO.
* Validation split: 2k data for development and tuning.

### Designing simulated pairwise preference \(p_{}\)

We now describe the design of our simulated annotator for pairwise preferences. Our core proposal is to design the simulator \(p_{}(z x,y_{0},y_{1})\) by prompting OpenAI API LLMs. While using LLMs as a proxy for annotators has become increasingly popular [15; 39], using LLMs as part of a simulation environment poses major additional challenges. Our simulated preferences \(p_{}\) must not only have a high agreement with human preferences \(p_{}\), it must also capture other qualitative aspects of human feedback such as inter- and intra-annotator inconsistencies. Intuitively, the noise and variability in pairwise feedback are key parts of the challenge in the LPF problem and we find that ignoring these factors leads to a simulator that diverges substantially from real-world behavior (Section 4.3).

Basic GPT-4 prompt design.To start with, we design prompts by providing an annotation guideline, feeding in-context examples, and leveraging batch generation to save on costs. As a first baseline, we query GPT-4 with a single prompt (denoted as \(p_{}^{}\)) and we find \(p_{}^{}\) has a high agreement rate with human annotators (65%; see results in Section 4.3). However, we find that this simple baseline of \(p_{}^{}\) fails to capture the variability in human annotation and can lead to qualitatively different results for method development, especially for reward over-optimization (Section 4.3).

Simulating human variability.To more completely emulate human annotators, we modify the basic simulated annotator design to capture annotator variability in two ways. First, we emulate inter-annotator variability in the simulated pairwise preference \(p_{}\) by mimicking a pool of annotators. We design different annotators by querying different API LLMs and varying the prompts with different formats, batch sizes, and in-context examples. In the end, we created 13 simulated annotators which we describe fully in Appendix D. Second, we emulate intra-annotator variability by directly injecting random noise and flipping the simulated preference \(25\%\) of the time.

With these ingredients, we come up with a simulated preference \(p_{}^{}\) that meets our requirement of agreement and variability. Overall, annotating 1000 outputs using simulated preference only costs $6,which is 50x cheaper than human annotation. In Section 4, we collect actual human preference and quantitatively verify the agreement and variability of our simulated preference.

### Designing an automatic evaluation

For researchers to develop LPF methods in the AlpacaFarm, we want to support them with an automatic evaluation so they can quickly iterate while reliably comparing methods. To replace the usual human evaluation, there are two challenges. First, how do we quantify the quality of different models? Second, what instructions can we use that are representative of human interactions?

Evaluation protocol.To quantify the quality of an LLM \(p_{}\), we measure the win-rate of that LLM against a reference model, i.e, the expected number of times that the output from \(p_{}\) is preferred to the output of a reference model \(p_{}\) on the same instruction \(x\). The benefits of using simulated win-rates are that it provides a metric that is easy to understand, is comparable across methods, and can reuse the routine we built for pairwise feedback. We use the 13 simulated annotators described in Section 3.2 without injecting the additional noise (as adding uniform noise does not change model rankings) and denote this preference simulator as \(p_{}^{}\). For the reference model, we use Davinci003 as it is a well-studied system that performs similarly to the models we fine-tune.

Evaluation data.Instruction following requires diverse coverage over realistic interactions. To build an appropriate evaluation protocol, we combine several open-source evaluation datasets and use real-world interactions with a demo instruction-following LM (Alpaca Demo ) as guidance for constructing our data combination. Due to privacy concerns, we do not directly release the demo data and opt to use it to guide how we combine existing open evaluation datasets.

Our final evaluation set consists of 805 instructions, which includes 252 instructions from the self-instruct test set , 188 from the Open Assistant (OASST) evtest setaluation, 129 from Anthropics' helpful test set , 80 from Vicuna test set [80; 17], and 156 from Koala test set . In 1, we show example instructions from our evaluation set (Table 7) and their root verb distribution(Figure 13), which shows the diverse coverage of our data. We find that aggregating across datasets is important for automatic evaluation to match real-world interactions, as discussed in Section 4.4.

### Reference methods in AlpacaFarm

Finally, AlpacaFarm defines a collection of validated LPF methods for instruction following. We describe those methods in detail in Appendix B but provide a brief overview here. In all LPF methods that follow, we begin by first performing an initial fine-tuning step on supervised data of instructions and outputs.We begin by describing two simple baselines that directly operate on pairwise feedback.

* **Binary FeedME.** Binary FeedME  continues supervised fine-tuning on the preferred output in each pairwise comparison.
* **Binary reward conditioning.** Reward conditioning [28; 36; 30] is a simple scheme that incorporates learning from negative examples; a token denoting whether the output was preferred is prepended before fine-tuning, and the positive token is used to condition at inference time.

Many LPF methods do not directly operate on pairwise feedback data, but instead first construct a surrogate reward model by fine-tuning a classifier from the SFT base using pairwise feedback. The following LPF methods maximize the continuous-valued reward defined by the logits of this classifier.

* **Best-of-\(n\) sampling.** Best-of-\(n\) (or re-ranking) [67; 5; 23; 9] is a simple but effective inference-time method that draws \(n\) i.i.d. responses from the SFT model and returns the response with the highest surrogate reward. Unless stated otherwise we use \(n=1024\) in our experiments.
* **Expert iteration.** Expert iteration [2; 64; 74] is the natural training-time extension of best-of-\(n\): it first generates according to best-of-\(n\) on new instructions and then fine-tunes on the best outputs.
* **Proximal Policy Optimization (PPO).** PPO [26; 62] is a reinforcement learning algorithm that maximizes surrogate reward, subject to a KL penalty keeping parameters near SFT initialization.
* **Quark.** We use the top-quantile variant of Quark  which bins sequences by reward and trains on the best bin, along with adding KL and entropy regularization.

## 4 Validating the AlpacaFarm simulator

With the simulator and methods defined, we now evaluate AlpacaFarm. As our main result, in Section 4.2 we analyze the correlation between the final rankings of methods in both the simulated LPF workflow and human-based LPF. Afterward, we will analyze more specifically whether our pairwise feedback accurately mimics human pairwise feedback (Section 4.3) and whether rankings on our evaluation data match rankings on the Alpaca Demo data (Section 4.4).

### Experimental details

Models.As baseline and starting point for LPF methods, we fine-tuned LLaMA 7B on the 10k SFT split. We take SFT 10k as starting point for all LPF methods and collect the simulated preference \(p_{}^{}\) and human preference \(p_{}\) from SFT 10k's outputs (with temp=1.0) on the 10k instruction PREF split. Then, for each of the six reference LPF methods \(M\):

* We trained and tuned \(M\) on simulated preferences \(p_{}^{}\), evaluating the resulting model \(M_{}\) against the Davinci003 reference with the simulated evaluator \(p_{}^{}\).
* We trained a few models \(M\) on human preferences across hyperparameter ranges identified in simulation, evaluating the resulting model \(M_{}\) against Davinci003 with humans \(p_{}\).

In addition to the six methods, we also evaluate existing instruction-following and base models: GPT-4 (gpt-4-0314), ChatGPT (gpt-3.5-turbo-0301), Davinci001 (text-davinci-001), LLaMA 7B , and Alpaca 7B . Alpaca 7B is a LLaMA 7B model finetuned on all data splits, denoted SFT 52k. For these models, we measure both the simulated win-rate \(p_{}^{}\) and human win-rate \(p_{}\).

At inference time, for all systems except best-of-\(n\), we sample with temp=0.7 and set the maximum number of tokens to be 300. For best-of-\(n\) sampling, we found a higher temperature to be helpful in encouraging output diversity, and so we rerank samples from SFT 10k with temp=1.0. We provide more thorough experimental details and hyperparameters for all methods in Appendix C.

Human annotation.We collected reference human annotation by showing crowd-workers two potential outputs \(y_{0}\) or \(y_{1}\) for a given instruction \(x\) and asked them to select the index \(z\{0,1\}\) of their preferred output. Annotators are recruited from Amazon Mechanical Turk using a qualification test of 25 questions. Out of an initial pool of 34 annotators, we selected the 16 whose agreement rate was higher than 70% with the author's annotations. We paid the annotators a median hourly rate of $21, leading to a one-time $3000 cost of annotating our PREF split and a recurring $242 cost for evaluating a single model on the 805 evaluation instructions. See Appendix E for additional details.

### End-to-end validation of AlpacaFarm

We now analyze the correlation between rankings in simulation and on human data. Figure 2 shows the win-rate of methods in AlpacaFarm (x-axis) with the win-rate from the human-based pipeline (y-axis). We see that the rankings have a Spearman correlation of 0.98, which suggests that AlpacaFarm faithfully captures the rankings among different LPF methods. This enables researchers to develop models at low-cost in AlpacaFarm and transfer insights to train models on real-world human interactions.

Figure 2: The ranking of methods trained and evaluated in AlpacaFarm matches that of methods trained and evaluated in the human-based pipeline. Each blue point represents one method \(M\) (e.g. PPO). The x-axis shows the simulated evaluation (win-rates measured by \(p_{}^{}\)) on methods trained in simulation \(M_{}\). The y-axis shows human evaluation (win-rates measured by \(p_{}\)) on methods trained with human feedback \(M_{}\). Gray points show models that we did not train, so their \(x\) and \(y\) values only differ in the evaluation (simulated vs human). Without those points, we have \(R^{2}=0.83\) and a Spearman Correlation of \(0.94\).

Inspecting these results more closely, we point out the two rank mismatches. The first comparison is SFT10k against SFT52k, where human annotators preferred SFT10k (44.3% vs 40.7%) while the simulator had the opposite preference (36.7% vs 39.2%, Table 1). The other mismatch is ChatGPT against PPO, where human annotators preferred PPO (55.1% vs 52.9%) unlike the simulator (46.8% vs 61.4%). In both cases, these are not major mistakes, as we do not expect SFT52k to be much worse than SFT10k or for a 7B LLaMA model to substantially outperform ChatGPT.

### Validating the pairwise preferences component

Having demonstrated that AlpacaFarm succeeds at the end-to-end validation of methods rankings, we now take a closer look at our pairwise preferences, showing that they agree with human annotators and replicate important qualitative features of model training. For additional details see appendix D.

Simulated annotators match human agreement.We begin by computing agreement levels between our simulated annotator and a majority vote of 3 human annotators, comparing this to the agreement level of a held-out human annotator, as shown in Figure 3. We find that our evaluator \(p_{}^{}\) () has a 65% agreement rate with the human majority vote, which is similar to the held-out human agreement rate at 66% (). At the same time, \(p_{}^{}\) is \(25\) cheaper ($300 \(\) $12 per 1000 examples). The training time annotator \(p_{}^{}\) () has lower agreement due to label flip noise but this does not mean that \(p_{}^{}\) is less faithful to human annotations, since this noise is unbiased and both annotators (\(p_{}^{}\), \(p_{}^{}\)) represent the same underlying preference function.

Figure 3 also shows that we identified single prompts performing even better than \(p_{}^{}\), with one prompts achieving 68% agreement. While this high agreement level is impressive, we do not use single

Figure 4: Human and AlpacaFarm preferences result in over-optimization, while a naive single-prompt GPT-4 simulator does not. Left: training and evaluation with human preferences. Middle: training and evaluation with AlpacaFarm preferences. Right: training and evaluation with single-prompt GPT-4 preferences. The x-axis measures the average surrogate rewards on the eval set.

Figure 3: Our simulated annotators are cheap and have a high agreement with human annotators. We show price (x-axis) vs agreement (y-axis) of each annotator with the majority vote among 3 human annotations. Grey points are the simulated annotators in the pool, the green  shows the resulting pool of annotators (used for evaluation), the orange  shows the same pool with random noise added during training. This does not change the implied reward function from, but makes the learning problem more challenging. The blue  shows the average of human annotators, and the red  shows a single low variance GPT-4 annotator analyzed below.

prompts for AlpacaFarm, as single prompts do not replicate the inter-annotator variability important for a simulator. Instead, randomizing over different annotators and injecting additional noise is needed to match the distributional features and learning dynamics of human data, which we discuss next.

Simulated annotators replicate overoptimization.We now show that modeling annotator variability in the simulated annotator (\(p_{}^{}\)) is necessary to capture important qualitative features of LPF model training. To do so, we compare the behavior of three of the best-performing models trained under \(p_{}^{}\) with those trained using the single GPT-4 prompt \(p_{}^{}\), which has higher human agreement but little annotator variability.

Figure 4 shows the learning dynamics of these models for pairwise feedback by \(p_{}\) (left), \(p_{}^{}\) (middle), and \(p_{}^{}\) (right) as the PPO iteration count and rerank sample counts for best-of-\(n\) and expert iteration are increased. For both the human and AlpacaFarm preferences, models that more effectively optimize the surrogate reward (x-axis) improve in win-rate (y-axis) up until a point, where overoptimization of the reward occurs and win-rates decrease. In contrast, simple GPT-4 feedback shows no overoptimization, leading to the false conclusion that the LPF methods can be optimized for much longer than is actually the case. For example, Figure 4 right shows best-of-\(1024\) to be much better than PPO, which disagrees strongly with the results on human data.

Noted in prior work , overoptimization is the result of the proxy reward model \(_{}\) being an imperfect estimate of the true reward \(R\). We hypothesize that the strong overoptimization seen for human annotators is partly due to the (inter- and intra-) variability of their annotations, which degrades the quality of the reward model. To test this hypothesis, we measured the _variance_ of each of the three annotators by calculating the average error of a held-out annotation to the majority vote over 3 random draws. At 0.26 for \(p_{}^{}\) and 0.43 for \(p_{}^{}\), AlpacaFarm is close to the human variance of 0.35; in contrast, the GPT-4 annotator has a much lower variance at 0.1. Finally, in Appendix F.1 we ablate the AlpacaFarm design more finely and find that the added label noise provides the majority of the benefit for inducing overoptimization. Appendix D contains further analyses of bias and variability of annotators.

### Validating the evaluation protocol

Finally, we test our evaluation data that combines existing open-source evaluation datasets. While we have observed that this dataset is diverse (see Figure 13), it is unclear whether it evaluates performance for any type of real-world human usage. To resolve this, we measure method-level correlations against a set of real user interactions recorded on the Alpaca Demo . We manually went through the interactions and identified 200 instructions that do not contain any personal identifying information, toxic or unsafe questions, and those that refer to the chatbot directly (e.g. "who are you developed by?"). The terms of use for the demo do not allow us to publicly release this data, but we use this data to evaluate the proposed evaluation set.

We use the 11 systems displayed in Figure 2, with the LPF methods trained in simulation, and evaluate them using \(p_{}^{}\). Figure 5 plots the simulated win-rates on the Demo instructions against those on the AlpacaFarm evaluation data. The two win-rates are strongly correlated (\(r^{2}=0.97\)), indicating that AlpacaFarm evaluation data can serve as a proxy for evaluating methods on simple demo interactions.

Figure 5: Correlation plot of simulated win-rates computed on AlpacaFarm’s evaluation versus that on real-world interactions with the Alpaca Demo.

## 5 Benchmarking reference methods on the AlpacaFarm

We now study the performance of reference methods in AlpacaFarm. Table 1 contains the details of the main evaluation results (presented in Figure 2). In the rest of this section, we discuss our findings from these results, demonstrating that the conclusions we reach using human feedback could have been derived using AlpacaFarm at a substantially lower cost. See Appendix C for experimental details.

### Comparing LPF methods

Supervised fine-tuning is highly effective.Table 1 shows that the SFT is effective and provides the majority of gains. SFT brings the base LLaMA model up from a simulator win-rate of \(11\%\) to \(37\%\) and a human win-rate of \(7\%\) to \(44\%\). However, we observe no gain from SFT 10k to SFT 52k.

PPO tops the LPF leaderboard.Among the LPF methods we study, PPO performs the best in both the simulator (\(47\%\)) and on human data (\(55\%\)). Notably, with a win-rate of \(55\%\), the PPO trained with human feedback was preferred to ChatGPT by our annotators for single-turn instruction-following. This surprising result is likely because ChatGPT was prompted for responses shorter than 1000 characters and both crowd-workers and simulators annotators preferred longer responses (see appendix D.2). For example, in AlapcaEval  (slightly different simulated annotator) we show that the unconstrained ChatGPT achieves 89% win-rate, much higher than the \(44\%\) win-rate for the PPO model.

Best-of-\(n\) is simple and competitive.Best-of-\(n\) sampling outperforms all LPF methods besides PPO. The performance best-of-\(n\) sampling points to the useful learning signals captured by the reward model and helps us understand how training-time LPF methods benefit from a reward model.

Expert Iteration and Quark lag behind.Even though expert iteration trains with best-of-\(n\) outputs, it trails best-of-\(n\) with a 3-6% win-rate gap in both simulated and human feedback workflows. This result suggests that acquiring the improvement from best-of-\(n\) sampling is not straightforward and points to why more complex learning algorithms like PPO might be useful. For Quark, although the rewards of samples improve during training, the resulting models did not outperform the SFT baseline.

Methods that directly learn from pairwise feedback do not perform well.We observe that binary reward conditioning and binary FeedME do not improve over the SFT 10k baseline. This result suggests that learning a surrogate reward model may be an important ingredient to LPF.

In appendix F.3 we analyze the outputs of different models. We find a major difference is that LPF methods produce longer outputs in both human and simulated feedback.

   Method & Simulated Win-rate (\%) & Human Win-rate (\%) \\  GPT-4*\({}^{}\) & \(79.0 1.4\) & \(69.8 1.6\) \\ ChatGPT*\({}^{}\) & \(61.4 1.7\) & \(52.9 1.7\) \\ PPO & \(46.8 1.8\) & \(55.1 1.7\) \\ Best-of-\(1024\) & \(45.0 1.7\) & \(50.7 1.8\) \\ Expert Iteration & \(41.9 1.7\) & \(45.7 1.7\) \\ SFT 52k & \(39.2 1.7\) & \(40.7 1.7\) \\ SFT 10k & \(36.7 1.7\) & \(44.3 1.7\) \\ Binary FeedME & \(36.6 1.7\) & \(37.9 1.7\) \\ Quark & \(35.6 1.7\) & - \\ Binary Reward Conditioning & \(32.4 1.6\) & - \\ Davinci001* & \(24.4 1.5\) & \(32.5 1.6\) \\ LLaMA 7B* & \(11.3 1.1\) & \(6.5 0.9\) \\   

Table 1: AlpacaFarm evaluation results on baseline and LHF methods. For methods without a *, the left column shows win-rates when we train and evaluate in simulation, while the right column shows win-rates when we train and evaluate with human feedback. For models with a *, those are not trained by us so the left and right columns respectively show simulated and human evaluation. Win-rates are computed against Davinci003. We did not evaluate Quark and Binary Reward Conditioning for human evaluation because they underperformed in development. \({}^{}\) ChatGPT and GPT-4 were prompted for responses shorter than 1000 characters to have length comparable to other methods.

### Using AlpacaFarm to train models directly for human deployment

A natural question is whether models trained in AlpacaFarm can directly perform well on human evaluation Table 2 shows that AlpacaFarm can be repurposed for this goal by using simulated annotators that maximize agreement rather than match human variability, i.e., using preferences from the single low-variance GPT-4 annotator \(p_{}^{}\)rather than our noisy annotators \(p_{}^{}\).

## 6 Limitations

Validation.Section 4 validates the use of AlpacaFarm, but there are nevertheless some limitations with the validation setting. First, the instructions we considered (even those from the real world-demo in section 4.4) are relatively simple and single-turn. Second, all models we fine-tuned use a LLaMA 7B as starting point. Finally, Human validation is based on feedback from 13 crowd-workers, which may not reflect broader human preferences and seem to have biases such as preferences for longer outputs as highlighted in appendix D.2. Although we do not expect the previous limitations to significantly affect the usefulness of AlpacaFarm, we encourage users to be vigilant when using any simulator and hope to further validate AlpacaFarm as more datasets and base models become available.

Assumption of an oracle LLM.AlpacaFarm assumes that we can access an oracle LLM, much more powerful than the investigated models, that can be used to approximate human feedback. While this may be true in research settings, it is not always the case in practice. We believe that investigating the use of the same model for the simulator and for learning is a promising direction for future work.

AlpacaFarm for fine-grained development.Section 4 shows that AlpacaFarm is effective for model selection and can replicate important behaviors such as over-optimization. We have nevertheless found that suitable hyperparameters for learning algorithms can be different for training with simulated feedback compared to human feedback. For example, due to changes in the scale of values of the surrogate reward model, the range of suitable KL regularization coefficients for RLHF is different.

Simulated annotators.A key component of AlpacaFarm is the simulated annotators. In section 4.3 and appendix D we showed that our annotators have high agreement with human annotators and exhibit similar biases, such as preferences for longer outputs and lists. There are nevertheless some biases that are specific to simulated annotators. For example, we found that simulated annotators prefer the first outputs shown in the prompt and outputs that come from the same model as the simulator. Although we controlled for those biases by randomizing the order of output in the prompt, and using the same training data for all considered methods, there may be other biases that we have not identified. For better automatic annotators and further analysis refer to AlpacaEval .

## 7 Related works and outlook

The idea of building a simulator is closely related to recent efforts in automatic model evaluations [16; 55; 15; 53; 37] and constitutional AI . However, our work crucially differs in our goal of building a simulator, which requires us to mimic distributional properties such as inter-annotator disagreement. We discuss these differences and the broader context of our work in Appendix A.

We showed that AlpacaFarm substantially lowers the cost and iteration time for developing methods that learn with pairwise feedback. AlpacaFarm provides a blueprint for constructing simulators for AI research that requires human supervision, and we view it as an exciting opportunity to expand this simulation approach to data from other domains and alternative forms of human feedback.

   Method & Human Win-rate (\%) \\   \(_{}\) & 55\% \\ \(_{}\) & 51\% \\ \(_{}^{}\) & 50\% \\ \(\) \(10\) & 44\% \\ \(_{}^{}\) & 43\% \\   

Table 2: Model transfer results. Table 2 compares the best model \(_{}^{}\) trained on \(p_{}^{}\) with a similar model \(_{}^{}\) trained on the GPT-4 annotator. We find that \(_{}^{}\) only achieves 43% win-rate, while \(_{}^{}\) achieves 50% win-rate. As context, we show win-rates of the initial SFT, the human \(\)\(_{}\), and the best non-PPO human method Best-of-\(16_{}\). Overall, training in AlpacaFarm can provide good models for deployment, but still suffers a 5% performance gap relative to real human annotations.