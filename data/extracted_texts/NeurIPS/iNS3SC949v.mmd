# Sm: enhanced localization in Multiple Instance Learning for medical imaging classification

Francisco M. Castro-Macias

CITIC-UGR

Dept. of Comp. Science and A. I.

University of Granada

&Pablo Morales-Alvarez

Dept. of Statistics and Operations Research

CITIC-UGR

University of Granada

&Yunan Wu

Dept. of Elect. and Comp. Engineering

Northwestern University&Rafael Molina

Dept. of Comp. Science and A. I.

University of Granada

&Aggelos K. Katsaggelos

Dept. of Elect. and Comp. Engineering

Northwestern University

###### Abstract

Multiple Instance Learning (MIL) is widely used in medical imaging classification to reduce the labeling effort. While only bag labels are available for training, one typically seeks predictions at both bag and instance levels (classification and localization tasks, respectively). Early MIL methods treated the instances in a bag independently. Recent methods account for global and local dependencies among instances. Although they have yielded excellent results in classification, their performance in terms of localization is comparatively limited. We argue that these models have been designed to target the classification task, while implications at the instance level have not been deeply investigated. Motivated by a simple observation - that neighboring instances are likely to have the same label - we propose a novel, principled, and flexible mechanism to model local dependencies. It can be used alone or combined with any mechanism to model global dependencies (e.g., transformers). A thorough empirical validation shows that our module leads to state-of-the-art performance in localization while being competitive or superior in classification. Our code is at https://github.com/Franklueee/SmMIL.

## 1 Introduction

Over the last decades, medical imaging classification has benefited from advances in deep learning . However, the performance of these methods drops when the number of labeled samples is low, which is common in real-world medical scenarios . To overcome this, Multiple Instance Learning (MIL) has emerged as a popular weakly supervised approach .

In MIL, instances are arranged in bags. At train time, a label is available for the entire bag, while the instance labels remain unknown. The goal is to train a method that, given a test bag, can predict both at bag and instance levels (classification and localization tasks, respectively). This paradigm is well suited to the medical imaging domain . In cancer detection from Whole Slide Images (WSIs), the WSI represents the bag, and the patches are the instances. In intracranial hemorrhage detection from Computerized Tomographic (CT) scans, the full scan represents the bag, and the slices at different heights are the instances. In these scenarios, making accurate predictions of instancelabels is extremely important from a clinical viewpoint, as it translates into pinpointing the location of the lesion .

Most successful approaches in MIL build on the attention-based pooling , a permutation-invariant operator that assigns an attention value to each instance independently. This method has been extended in different ways while maintaining the permutation-invariant property [21; 25; 39]. The aforementioned works pose a problem: the dependencies between the instances, which are important when making a diagnosis, are ignored. To account for this, TransMIL  proposed to model global dependencies using a transformer encoder. The idea is to use the self-attention mechanism to introduce interactions between each pair of instances. Based on it, other transformer-based approaches have emerged, also focusing on global dependencies [9; 22; 37]. More recently, several works have also incorporated natural local interactions, which are those between neighboring instances [14; 40; 41].

Although these methods accounting for dependencies have resulted in excellent performance at the bag level, the evaluation at the instance level has received less attention and the results are not comparatively good so far, see the very recent . In this work, we argue that recent MIL methods have been designed with the classification task in mind, and we propose a new model that focuses on both the classification and localization tasks. Specifically, we propose a novel and theoretically grounded mechanism to introduce local dependencies, hereafter referred to as _the smooth operator_Sm. This is a flexible module that can be used alone on top of classical MIL approaches, or in combination with transformers to also account for global dependencies. In both cases, we show that the proposed operator achieves state-of-the-art localization results while being competitive in classification. We compare against a total amount of eight methods, including very recent ones [14; 40]. We utilize three different datasets of different nature and size, covering two different medical imaging problems (cancer detection in WSI images and hemorrhage detection in CT scans).

Our main contributions are: (i) we provide a unified view of current deep MIL approaches; (ii) we propose a principled mechanism to introduce local interactions, which is a modular component that can be combined or not with global interactions; and (iii) we evaluate up to eight state-of-the-art MIL methods on three real-world MIL datasets in both classification and localization tasks, showing that the proposed method stands out in localization while being competitive or superior in classification.

## 2 Related work

In this work, we tackle the localization task in deep MIL methods using existing concepts and techniques from deep MIL and Graph Neural Networks (GNNs) theory.

**Deep Multiple Instance Learning.** As explained by Song et al. , deep MIL methods can be divided into two broad categories, namely instance-based or embedding-based, depending on the level at which a specific _aggregation_ operator is applied. In this paper, we focus on the embedding-based category, and in particular attention-based ones.

Ilse et al.  proposed attention-based pooling to weigh each instance in the bag. To improve it, different modifications were proposed, including the use of clustering layers , grouping the instances in pseudo-bags , and using similarity measures to critical instances to compute the attention values . However, these methods ignore the existing dependencies between the instances in a bag. To address this, Shao et al.  proposed to use a transformer-based architecture and the PPEG position encoding module. This has been extended with different transformer variations, including the deformable transformer architecture , hierarchical attention , and regional sampling . Recently, these methods have been improved to include spatial information in different ways, including the use of a Graph Convolutional Network (GCN) before the transformer , a neighbor-constrained attention module , and a spatial-encoding transformer architecture .

In the studies mentioned above, the objective is to obtain increasingly better bag-level results, while the evaluation at the instance level is usually performed qualitatively. In contrast, our work addresses both the instance localization task and the bag classification task, as both are of great importance for making a diagnosis. Moreover, our work is not limited to WSI classification; it is also valid for other medical imaging modalities.

**Graph Neural Networks.** Our motivation -- that neighboring instances are likely to have the same label -- is a well-established assumption within the machine learning community, often referred to as the cluster assumption [10; 31]. Since leveraged in 1984 by Ripley  in the context of spatial statistics, it has been extensively used in spectral clustering , semi-supervised learning on graphs , and recently in GNNs . Our work builds upon seminal works in these areas.

The proposed smooth operator is derived considering a Dirichlet energy minimization problem, similar to the work by Zhou and Scholkopf  and Zhou et al. . This approach has been employed in recent years to obtain new GNN models, including the \(p\)-Laplacian layer , and PPNP layer . Moreover, the Dirichlet energy has been studied in the context of GNNs to analyze the over-smoothing phenomenon [6; 23]. In this regard, our bound on the decrease of the Dirichlet energy is analogous to the result derived by Li et al.  to study over-smoothing for GCNs. Our result, however, holds for the proposed mechanism, of which the graph convolutional layer is a special type.

## 3 Background: A unified view of deep MIL approaches

We first describe the binary MIL problem tackled in this paper. Then, we provide a unified view of the most popular deep MIL methods. As explained in Sec. 2, we focus on embedding-based approaches.

In MIL, the training set consists of pairs of the form \((,Y)\), where \(^{N P}\) is a bag of instances and \(Y\{0,1\}\) is the bag label. We write \(=[_{1},,_{N}]^{T} ^{N P}\), where \(_{n}^{P}\) are the instances. Each instance \(_{n}\) is associated to a label \(y_{n}\{0,1\}\), _not available during training_. It is assumed that \(Y=\{y_{1},,y_{N}\}\), i.e., a bag \(\) is considered positive if and only if there is at least one positive instance in the bag.

Given a previously unseen bag (e.g., a medical image), the goal at test time is to: i) predict the bag label (classification task) and ii) obtain predictions or estimates for the instance labels (localization task). In general, deep MIL models output a bag-level prediction \(\), as well as instance-level scalars \(f_{n}\) that are used for instance-level prediction. This general process is depicted in Fig. 0(a). In many approaches, these \(f_{n}\) are the so-called _attention values_ (e.g., ABMIL , TransMIL , CAMIL ), but they can be obtained in different ways (e.g., through GraphCAM in GTP ). Within the general process in Fig. 0(a), deep MIL models can be categorized into three families, depending on how instances interact with each other, see Fig. 0(b), Fig. 0(c), and Fig. 0(d).

In the first family, shown in Fig. 0(b), the instances are encoded _independently_ and then aggregated. The well-known ABMIL  fits in this paradigm. Subsequent works introduce slight modifications to ABMIL, while still encoding each instance _independently_[21; 25; 39]. ABMIL, on which we will rely to introduce our model, is depicted in Fig. 2(a). First, a bag of embeddings \(=[_{1},,_{N}]^{N  D}\) is obtained by applying a neural network independently to each instance. Then, the attention-based pooling computes the attention values \(\) and the bag embedding \(\) according to

\[=(^{}), =,\] (1) \[=()=^{ }\,(),\] (2)

where \(\!\!^{L D}\), \(\!\!^{L}\) are trainable weights. Last, \(\) is obtained by applying a linear classifier on \(\).

The second family accounts for _global_ interactions between instances, possibly long-range ones, see Fig. 0(c). These works treat instances as tokens that interact through the self-attention mechanism. This way, global interactions between instances are learned. One of the most popular approaches in this family is TransMIL , which was later extended in different directions [9; 22]. The third family complements the previous one with _local_ interactions defined by a fixed neighborhood, see

Figure 1: (a) Unified view of deep MIL models. Depending on how instances interact with each other in (a), we devise three different families of methods: (b), (c), (d).

Fig. (d)d. They differ in how local interactions are represented, e.g., as a graph in CAMIL  and GTP , or using a position-encoded feature map in SETMIL .

In most of these works, the localization task is assessed qualitatively, e.g., by visually comparing the attention maps. This contrasts with the classification task, which is always evaluated quantitatively. As evidenced by Fourkioti et al. , this has translated into comparatively poor performance in terms of localization. We notice that current models have been designed to target the classification task, and they excel at that. However, their model design is not as careful about the instance-level implications. For example, CAMIL  does not leverage any local information to obtain the instance-level attention values. Indeed, from their Eq. (8) one deduces that the \(a_{i}\) values are obtained from the tile representations \(_{i}\), which have not undergone any local interaction. Observe that local interactions take place in Eq. (4) and Eq. (5) in their paper, but these only affect the bag-level predictions, not the instance-level ones. Similarly, GTP  introduces local interactions through an off-the-shelf graph convolutional layer, the effect of which is not investigated at the instance level. In the following section, we propose a principled approach to account for meaningful local interactions based on the Dirichlet energy. The idea is motivated by a natural property often observed in the instance-level labels of medical images: the similarity between neighboring instances.

## 4 Method: Introducing smoothness in the attention values

In medical imaging, instance labels are _a priori_ expected to exhibit local dependencies with their neighboring instances: an instance is likely to be surrounded by instances with the same label, see Fig. 2. Recall that attention values are commonly used as a proxy to estimate these labels, so they should inherit this property. Based on these observations, our intuitive idea is to favor a _smoothness_ property on the attention values. To this end, Sec. 4.1 formalizes the notion of smoothness through the Dirichlet energy. Sec. 4.2 presents the proposed smoothing operator Sm, which encourages smoothness as well as fidelity to the original signal. Sec. 4.3 proposes how to leverage Sm in the context of MIL, both in combination with global interactions (via transformers), and without them. We will build on top of the well-known and simple ABMIL to isolate the effect of Sm and avoid over-sophisticated models.

### Modelling the smoothness

We represent each bag as a graph, where the nodes are the instances and the edges represent the spatial connectivity between instances. Formally, we suppose that each bag \(^{N D}\) has been assigned an adjacency matrix \(=[A_{ij}]^{N K}\), defined by \(A_{ij}>0\) if instances \(_{i}\) and \(_{j}\) are neighbors, and \(A_{ij}=0\) otherwise. We assume that the adjacency matrix is symmetric, i.e. \(A_{ij}=A_{ji}\).

The _Dirichlet energy_ is a well-known functional that measures the variability of a function defined on a graph [42; 43]. In our case, we think of this function as the attention values \(^{N}\), recall Fig. (a)a. As we shall see below, it will be necessary to define the Dirichlet energy for multivariate graph functions. Given a multivariate graph function \(=[_{1},,_{N}]^{}^{N D}\) defined on the bag graph, the Dirichlet energy of \(\) is given by

\[_{D}()=_{i=1}^{N}_{j=1}^{ N}A_{ij}\|_{i}-_{j}\|_{2}^{2}= (^{}),\] (3)

Figure 2: WSIs are divided into patches. CT scans are provided as slices. They often show spatial dependencies: in a WSI, a patch is usually surrounded by patches with the same label, while in a CT scan, a slice is usually surrounded by slices with the same label. The red color indicates malignant/hemorrhage patches/slices.

where \(\|\|_{2}\) denotes the Euclidean norm, \(\) is the graph Laplacian matrix \(=-\), \(^{N N}\) is the degree matrix, \(=(D_{1},,D_{N})\), \(D_{n}=_{i}A_{ni}\). When \(D=1\) we obtain the definition for univariate graph functions, such as the attention values \(\).

**Bounding \(_{D}\) on the attention values.** In most deep MIL approaches, the attention values \(\) are obtained by applying a neural network to instance-level features. One example is ABMIL , which uses a two-layer perceptron defined by Eq. 1. Noting that \(\) is a Lipschitz function with Lipschitz constant equal to \(1\), we arrive at the following chain of inequalities

\[_{D}()\|\|_{2}^{2} _{D}()\|\|_{2}^{2} \|\|_{2}^{2}_{D}(),\] (4)

where \(\|\|_{2}\) denotes the spectral norm. A more general result holds in the general case of an arbitrary multi-layer perceptron, see Appendix A for a proof. The above chain of inequalities tells us that if we want \(_{D}()\) to be low, _we can act on \(\) itself or on previous layers (e.g., on \(\) or on \(\))_, constraining the norm of the trainable weights to remain constant. This constraint can be achieved using spectral normalization , and we study its influence in Sec. B.3. In the next subsection, we propose an operator that can be used on any of these levels (\(\), \(\), \(\)) to reduce the Dirichlet energy of its output.

### The smooth operator

Our goal now turns into finding an operator \(:^{N D}^{N D}\) that, given a bag graph multivariate function \(^{N D}\), returns another bag graph multivariate function \(()^{N D}\) such that its Dirichlet energy is lower without losing the information present in the original \(\). Following seminal works [42; 43], we cast this as an optimization problem,

\[()=_{}( ),\] (5)

\[()=_{D}( )+(1-)\|-\|_{F}^{2},\] (6)

where \([0,1)\) accounts for the trade off between both terms, and \(\|\|_{F}\) denotes the Frobenius norm. The first term in the above equation penalizes functions with too much variability, while the second term penalizes functions that differ too much from the original \(\). Note that this can be interpreted as a maximum a posteriori formulation, where the first term corresponds to the prior distribution and the second to the observation model, see . The objective function \(\) is strictly convex, and therefore admits a unique solution, given by

\[()=(+)^{- 1},\] (7)

where \(=/(1-)\). Unfortunately, the expression in Eq. (7), although elegant, incurs prohibitive computational and memory costs, especially when the number of instances in the bag is large (which is the case of WSIs). Instead, we can take an iterative approach, defining \(()=(T)\), with

\[(0)=;(t)=(- )(t-1)+(1-), t\{1, ,T\}.\] (8)

As demonstrated by Zhou et al. , the sequence \(\{(t)\}\) converges to the optimal solution in Eq. 7. As studied by Gasteiger et al. , it is enough to use a small number of iterations \(T\) to approximate the exact solution. Therefore, in this work, we will adopt the iterative approach described by Eq. 8. Based on previous work , we will use \(T=10\), and \(\) will be set as a trainable parameter initialized at \(=0.5\). See Sec. B.3 for a study on the effects of these hyperparameters and Fig. 12 for a visual comparison of the effect that \(\) has on the attention maps.

**Theoretical guarantees via the normalized Laplacian.** We present a result that informs us about the rate at which the Dirichlet energy decreases when applying \(\). Let us define \(_{}^{*}=\{(1+_{n})^{-2}:_{n} \{0\}\}\), where \(=\{_{1},,_{N}\}\) are the eigenvalues of the bag graph Laplacian matrix. Then, we have the following inequality,

\[_{D}(())_{ }^{*}_{D}().\] (9)

The proof is inspired by Cai and Wang , see Appendix A. If \(_{}^{*}<1\), then the smooth operator effectively decreases the Dirichlet energy. If we replace the Laplacian matrix by the normalized Laplacian matrix, \(}=^{-1/2}^{-1/2}\), it is known that its eigenvalues lie in the interval \([0,2)\), and then \(_{}^{*}<1\) holds. This motivates the use of the normalized Laplacian in our experiments.

The smooth operator \(\) only introduces one parameter to be estimated, \(\). Also, it is differentiable with respect to its input. Therefore, it can be integrated into simple attention-based MIL models, such as ABMIL, to account for local dependencies.

### The proposed model

Here we propose how to leverage the operator Sm in the context of MIL. We build on top of the well-known ABMIL. First, we introduce SmAP, which integrates ABMIL with Sm and only accounts for local interactions. Second, we introduce SmTAP, which equips SmAP with a transformer encoder to account for global dependencies. The proposed models are depicted in Fig. 2(b) and Fig. 2(c). The details about the architecture we have used can be found in Sec. B.2.

SmAP: Smooth Attention Pooling.This is represented in Fig. 2(b). First, the bag of embeddings \(\) is obtained as in ABMIL , i.e. treating the instances independently. Then, the operator Sm is integrated within the attention pooling. Based on Eq. 4, this can be done on the attention values themselves or on previous representations. We consider three different variants: SmAP-late, SmAP-mid, SmAP-early. They act, respectively, on \(\) (the attention values themselves), on \(\) (i.e. before entering the last layer), and on \(\) (i.e. before entering the attention-based pooling). Formally,

\[=((^{})),\] (10) \[=((^{})),\] (11) \[=(()),\] (12)

While SmAP-late and SmAP-mid act on the computation of the attention values, SmAP-early acts on the embedding that is passed to the attention-based pooling, see Fig. 8 in Appendix C. We use SmAP-early by default. Sec. 5.3 shows that results do not differ much among configurations.

SmTAP: Smooth Transformer Attention Pooling.This is represented in Fig. 2(c). The only difference with SmAP is that the neural network acting independently on the instance embeddings is replaced by a transformer encoder to account for global dependencies. Based on the idea that smoothness can be imposed at previous locations, recall Eq. 4, we propose to also apply Sm to the transformer output:

\[=((q( )k()^{})v()),\] (13)

where \(q\), \(k\), and \(v\) are the standard queries, keys, and values in the dot product self-attention . Notice that SmTAP uses Sm in two places: the first after the transformer encoder and the second in the aggregator. Naturally, one could think of other variants that use Sm in only one place or the other. In Sec. 5.3 we ablate these different configurations, leading to similar results.

## 5 Experiments

We validate the proposed Sm in three medical MIL datasets: RSNA , PANDA , and CAME-LYON16 . We evaluate the effectiveness of our approach by a quantitative and qualitative analysis. All experiments have been conducted under fair and reproducible conditions. Details on the datasets and experimental setup can be found in Appendix B. The code is available at https://github.com/Franblueee/SmMIL.

We compare our approaches with state-of-the-art deep MIL methods. We consider two groups of methods, depending on the presence/absence of a transformer block to model global dependencies. In the first group, we include those models that do not use this block: the proposed SmAP, ABMIL , CLAM , DSMIL , and DFTD-MIL . The second group consists of models that do use the transformer encoder: the proposed SmTAP, TransMIL , SETMIL , GTP , and CAMIL . These groups ensure a fair comparison in terms of model capabilities and complexity.

Figure 3: Smooth Attention Multiple Instance Learning. (a) The well-known model in , which we build upon. (b): only local interactions are considered by applying the proposed smooth operator Sm in the aggregation part. (c): both global and local interactions are considered by applying Sm both in the transformer and in the aggregation parts.

In Sec. B.3 we report the results of three more methods: DeepGraphSurv , PathGCN , and IIBMIL . Note that the performance obtained by these methods does not affect the conclusions we will obtain in this section.

In Sec. 5.1 we consider the localization task. In Sec. 5.2 we turn to the classification task. Sec. 5.3 shows an ablation study on how different uses of the smooth operator affect the proposed model.

### Localization: instance level results

In this subsection, we analyze the ability of each model to predict the label of the instances inside a bag. As explained in Sec. 3, deep MIL models assign a scalar value \(f_{n}\) to each instance \(_{n}\), see Fig. 0(a). Although these can be obtained in different ways, for simplicity we will refer to them as _attention values_. Thus, we compare the attention values with the ground truth instance labels, which are available for the test set only for evaluation purposes.

**Quantitative analysis.** We analyze the performance of each method using the area under the ROC curve (AUROC) and the F1 score. Note that a critical hyperparameter for the latter is the threshold used on \(f_{n}\) to determine the label of each instance. To ensure a fair comparison, we compute the optimal threshold for each method using the validation set. As a general summary, we also report the average rank achieved by each model across metrics and datasets.

The results are shown in Table 1. We find that using Sm provides the best performance overall, placing as the best or second-best within each group. Only in RSNA the proposed SmAP is outperformed by ABMIL. We attribute this to the fact that the bag graphs in CT scans are not as complex as in WSIs, and therefore the local interactions are not as meaningful. Note that the performance gain is particularly significant on CAMELYON16, where the bags have a larger number of instances, the graphs are much denser and the imbalance between positive and negative instances is more severe. Notably, SmTAP significantly outperforms SETMIL, GTP, and CAMIL, which also model local dependencies. Contrary to our method, their design is focused on bag-level performance and it does not translate into meaningful instance-level properties.

**Attention histograms.** We examine the attention histograms produced by each model on the CAMELYON16 dataset. The corresponding figures for RSNA and PANDA can be found in Appendix C. In Fig. 4, we represent the frequency with which attention values are assigned to positive and negative instances, separately. An ideal instance classifier would place all the positive instances on the right and all the negative instances on the left. This illustrates why SmTAP and SmAP achieve such a good performance: they concentrate the negative instances to the left of the histogram while succeeding in grouping a large part of the positive instances to the right. TransMIL and GTP assign low attention values to both positive and negative instances. CAMIL is able to identify positive instances, but negative instances are assigned from intermediate to high attention values. CLAM and DSMIL assign low attention values to negative instances, but the distribution of the positive instances resembles a uniform and a normal distribution, respectively.

**Attention maps.** To visualize the localization differences, we show the attention maps generated by four of the transformer-based methods in a WSI from CAMELYON16, see Fig. 5. SmTAP attention

    & &  &  &  \\   & & AUROC (\(\)) & F1 (\(\)) & AUROC (\(\)) & F1 (\(\)) & AUROC (\(\)) & F1 (\(\)) & Rank (\(\)) \\   & SnAP & \(0.798_{0.033}\) & \(0.477_{0.014}\) & \(_{0.005}\) & \(0.635_{0.006}\) & \(_{0.007}\) & \(_{0.053}\) & \(_{0.548}\) \\  & ABMIL & \(_{0.012}\) & \(_{0.03}\) & \(0.768_{0.002}\) & \(0.602_{0.004}\) & \(0.819_{0.074}\) & \(0.766_{0.000}\) & \(2.501_{0.255}\) \\ global & CLAM & \(0.523_{0.069}\) & \(0.076_{0.154}\) & \(0.727_{0.046}\) & \(0.568_{0.038}\) & \(0.849_{0.044}\) & \(0.821_{0.046}\) & \(4.167_{1.229}\) \\  & DSMIL & \(0.554_{0.004}\) & \(0.180_{0.000}\) & \(0.765_{0.003}\) & \(0.598_{0.006}\) & \(0.760_{0.070}\) & \(0.654_{0.183}\) & \(4.333_{0.516}\) \\  & DFTD-MIL & \(0.747_{0.070}\) & \(0.453_{0.194}\) & \(0.795_{0.004}\) & \(_{0.006}\) & \(0.884_{0.002}\) & \(0.742_{0.040}\) & \(2.501_{0.049}\) \\   & SnAP & \(_{0.046}\) & \(_{0.023}\) & \(_{0.007}\) & \(0.622_{0.01}\) & \(_{0.008}\) & \(_{0.067}\) & \(_{1.255}\) \\  & TransMIL & \(0.732_{0.013}\) & \(0.417_{0.014}\) & \(0.751_{0.011}\) & \(0.636_{0.008}\) & \(0.781_{0.024}\) & \(0.127_{0.078}\) & \(3.083_{1.249}\) \\  & SETMIL & \(0.726_{0.025}\) & \(0.438_{0.027}\) & \(0.774_{0.007}\) & \(0.631_{0.010}\) & \(0.615_{0.231}\) & \(0.134_{0.267}\) & \(3.667_{0.816}\) \\    & GTP & \(0.736_{0.017}\) & \(0.425_{0.018}\) & \(0.768_{0.022}\) & \(0.636_{0.011}\) & \(0.442_{0.091}\) & \(0.037_{0.036}\) & \(3.917_{1.229}\) \\    & CAMIL & \(0.769_{0.036}\) & \(0.456_{0.013}\) & \(0.785_{0.011}\) & \(0.621_{0.013}\) & \(0.742_{0.028}\) & \(0.479_{0.175}\) & \(2.833_{1.169}\) \\   

Table 1: Localization results (mean and standard deviation from five independent runs). The best is in bold and the second-best is underlined. \(()/()\) means lower/higher is better. The proposed operator improves the localization results in all three datasets and both with and without global interactions. It ranks first in eight out of twelve dataset-score pairs.

map resembles the most to the ground truth. As noted in Fig. 4, CAMIL assigns high attention values to both positive and negative instances. TransMIL and GTP pinpoint the regions of interest, but the attention is relatively low in those areas, which produces unclear boundaries, especially in the case of TransMIL. The attention maps for the rest of the methods and datasets are in Appendix C.

The results shown in this subsection validate the utility of the smooth operator at the instance level. They suggest that having smooth attention maps is a powerful inductive bias that improves the instance-level performance. In the following, we analyze its impact at the bag level.

### Classification: bag level results.

In this subsection, we show that the use of the smooth operator does not deteriorate the bag classification results. On the contrary, in some cases, it improves them. Again, we focus on the AUROC and F1 scores, measured by comparing the true bag labels with the methods' bag label predictions. The threshold for the F1 score is \(0.5\). We also report the mean rank achieved by each model.

Table 2 shows the results. The proposed models achieve the best performance overall. As in the localization task, ABMIL performs better than SmAP in RSNA. Again, we believe it to be a consequence of the CT scan's low-complexity structure. DFTD-MIL obtains the best result in CAMELYON16, but ranks second or third in the other two datasets. GTP and SETMIL outperform the proposed SmTAP in PANDA, but their performance significantly decreases in CAMELYON16, obtaining the worst results. Overall, our methods provide the most consistent performance, achieving an aggregated mean rank of 1.833.

### Ablation study

The proposed Sm comes with different design choices and hyperparameters: the placement of Sm, the trade-off parameter \(\), the number of approximation steps \(T\), and the use of spectral normalization. We analyze them in the following, showing that Sm leads to enhanced results almost under any choice. This supports that our hypothesis -- that neighboring instances are likely to have the same label -- is a powerful inductive bias worth exploring.

Figure 4: Attention histograms on CAMELYON16. First/second rows show models without/with global interactions. SmAP and SmTAP stand out at separating positive and negative instances.

Figure 5: Attention maps on CAMELYON16. The novel SmTAP produces the most accurate one.

#### 5.3.1 Placement of Sm

Recall that SmAP leverages by default the _early_ variation, but we also described SmAP-mid and SmAP-late. Likewise, we discussed different variants for SmTAP. Table 3 summarizes the impact of these choices on the final performance.

Sm**without the transformer encoder (SmAP).** These variants differ in the place where Sm is located inside the attention pool, recall Eq. 10-Eq. 12. We include ABMIL since we build our model on top of it. We see that using SmAP improves the performance at both instance and bag levels. This improvement is more noticeable in PANDA and CAMELYON. We attribute it to the bag graph structure being more complex in WSIs than in CT scans. Also, the Dirichlet energy is lower when the smooth operator is used, as theoretically expected. We observe that the proposed method is robust to different placement configurations, which is consistent with the theoretical guarantees presented in Sec. 4.1. However, none of the variants consistently outperforms the others.

Sm**with the transformer encoder (SmTAP).** Recall that SmTAP leverages Sm both after the transformer encoder and inside the attention pooling. Here we will refer to it as SmT+SmAP, and will compare it against T+SmAP and SmT+AP (using Sm only in one of the components) and against T+AP (not using Sm). We observe that Sm has no significant effect on bag-level performance. At instance-level we do observe differences: the baseline T+AP is outperformed as long as Sm is used within the attention pooling.

#### 5.3.2 Sm hyperparameters

In the following we study the influence of the trade-off parameter \(\) and of the spectral normalization. Due to space limitations, the analysis for the number of approximation steps \(T\) is in Sec. B.3.

**The trade-off parameter \(\).** From Eq. 6 we see that \([0,1)\) controls the _amount of smoothness_ enforced by Sm. Note that \(=0\) in Eq. 7 produces no smoothness, turning Sm into the identity operator. In Fig. 5(a) we show the performance obtained for different values of \(\) in CAMELYON16. Each choice of this hyperparameter improves upon the baseline ABMIL (\(=0\)). We see that better localization results are obtained when \(\) is lower, while better classification results are obtained when

    &  &  &  \\   & & AUROC (\(\)) & F1 (\(\)) & AUROC (\(\)) & F1 (\(\)) & AUROC (\(\)) & F1 (\(\)) & Rank (\(\)) \\   & SnAP & \(0.885_{0.005}\) & \(0.757_{0.006}\) & \(_{0.001}\) & \(_{0.002}\) & \(0.976_{0.007}\) & \(0.916_{0.016}\) & \(_{0.753}\) \\  & ABMIL & \(0.889_{0.005}\) & \(_{0.011}\) & \(0.933_{0.002}\) & \(0.909_{0.001}\) & \(0.956_{0.011}\) & \(0.914_{0.021}\) & \(2.501_{0.109}\) \\ global & CLAM & \(0.674_{0.157}\) & \(0.161_{0.291}\) & \(0.893_{0.026}\) & \(0.868_{0.034}\) & \(0.960_{0.029}\) & \(0.897_{0.021}\) & \(4.500_{0.837}\) \\ interactions & DSMIL & \(0.689_{0.063}\) & \(0.240_{0.012}\) & \(0.921_{0.008}\) & \(0.904_{0.008}\) & \(0.947_{0.076}\) & \(0.866_{0.123}\) & \(4.167_{0.753}\) \\  & DFTD-MIL & \(_{0.045}\) & \(0.775_{0.282}\) & \(0.940_{0.001}\) & \(0.903_{0.002}\) & \(_{0.01}\) & \(_{0.013}\) & \(2.000_{1.265}\) \\   & SnAP & \(_{0.007}\) & \(_{0.026}\) & \(0.946_{0.003}\) & \(0.917_{0.002}\) & \(0.976_{0.014}\) & \(_{0.022}\) & \(_{0.839}\) \\  & TransML & \(0.883_{0.008}\) & \(0.716_{0.013}\) & \(0.933_{0.010}\) & \(0.895_{0.029}\) & \(0.973_{0.018}\) & \(0.911_{0.028}\) & \(4.083_{0.917}\) \\ global & SETMIL & \(0.869_{0.011}\) & \(0.716_{0.006}\) & \(_{0.003}\) & \(_{0.003}\) & \(0.715_{0.155}\) & \(0.471_{0.341}\) & \(3.583_{0.010}\) \\ interactions & GTP & \(0.901_{0.008}\) & \(0.805_{0.017}\) & \(0.949_{0.004}\) & \(0.920_{0.003}\) & \(0.748_{0.118}\) & \(0.727_{0.143}\) & \(2.750_{0.987}\) \\  & CAMIL & \(0.889_{0.019}\) & \(0.805_{0.028}\) & \(0.938_{0.003}\) & \(0.911_{0.004}\) & \(_{0.007}\) & \(0.918_{0.018}\) & \(2.751_{1.173}\) \\   

Table 2: Classification results (mean and standard deviation from five independent runs). The best is in bold and the second-best is underlined. (\(\))/(\(\)) means lower/higher is better. The models with the proposed operator achieve the best performance overall, ranking first or second in nine out of twelve dataset-score pairs.

    &  &  &  \\   & _{}()\)} & AUROC\({}_{}()\) & \(_{D}()\) & AUROC\({}_{}()\) & AUROC\({}_{}()\) & \(_{D}()\) & AUROC\({}_{}()\) & AUROC\({}_{}()\) & \(_{D}()\) \\   & \(0.798\) & \(0.888\) & \(0.009\) & \(0.799\) & \(0.943\) & \(0.106\) & \(0.960\) & \(0.976\) & \(0.395\) \\  & SmAP-mid & \(0.806\) & \(0.888\) & \(0.012\) & \(0.792\) & \(0.940\) & \(0.135\) & \(0.922\) & \(0.964\) & \(0.384\) \\  & SmAP-late & \(0.811\) & \(0.891\) & \(0.011\) & \(0.802\) & \(0.944\) & \(0.082\) & \(0.819\) & \(0.964\) & \(0.321\) \\  & ABMIL & \(0.806\) & \(0.889\) & \(0.023\) & \(0.768\) & \(0.933\) & \(0.141\) & \(0.819\) & \(0.956\) & \(0.419\) \\   & \(0.791\) & \(0.910\) & \(0.010\) & \(0.813\) & \(0.944\) & \(0.306\) & \(0.841\) & \(0.986\) & \(0.313\) \\  & SnT+P & \(0.791\) & \(0.910\) & \(0.010\) & \(0.754\) & \(0.940\) & \(0.356\) & \(0.754\) & \(0.984\) & \(0.320\) \\  & \(0.792\) & \(0.910\) & \(0.010\) & \(0.787\) & \(0.944\) & \(0.332\) & \(0.915\) & \(0.986\(\) is higher. Fixing \(=0.5\) is a compromise between the two, and produces very similar results as setting it as a trainable parameter initialized at \(=0.5\). Fig. 12 provides a visual comparison of the effect that \(\) has on the attention maps.

**The effect of spectral normalization.** Spectral normalization forces the norm of the multi-layer perceptron weights to remain constant. In this work, this is a key design choice that helps Sm to obtain attention maps with lower Dirichlet energy. In our experiments, we have used spectral normalization in the layers immediately after Sm. Note that the late variant does not require spectral normalization, since it applies Sm directly to the attention values. In Fig. 5(b) we show the results obtained with and without spectral normalization in CAMELYON16. We observe that, even without spectral normalization, Sm improves upon the baseline. The improvement is more significant when Sm is paired with spectral normalization, especially at the instance level.

## 6 Discussion and conclusion

The main goal of this paper is to draw attention to the study of MIL methods at the instance level. To that end, we revised current deep MIL methods and provided a unified perspective on them. We proposed the smooth operator Sm to introduce local interactions in a principled way. By design, it produces smooth attention maps that resemble the ground truth instance labels. We conducted an exhaustive experimental validation with three real-world MIL datasets and up to eight state-of-the-art methods in both classification and localization tasks. This study showed that our method provides the best performance in localization while being highly competitive (best or second best) at classification.

Despite its advantages, our method has some limitations. The first is that, as with every other operator in GNNs, the computational costs of the smooth operator scale with the size of the bag. Fortunately, it can be paired with existing subgraph sampling techniques to mitigate this problem. The second limitation is that we do not have a definite answer for where it is better to use the proposed operator. We have shown that it leads to improvements in almost every place, but the optimal location may be problem-dependent and has to be tailored by the practitioner.

Finally, we hope that our work will draw more attention to the localization problem, which is very important for the deployment of computer-aided systems in the real world. In this sense, safely deploying the proposed methods in clinical practice requires evaluating them in a wider range of medical problems and quantifying their uncertainty. For the latter, we believe that the smooth operator could also benefit from a probabilistic formulation.