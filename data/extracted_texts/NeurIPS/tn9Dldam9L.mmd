# Add and Thin: Diffusion for Temporal Point Processes

David Ludke\({}^{1,\,2}\)   Marin Bilos\({}^{1,\,3}\)   Oleksandr Shchur\({}^{1,\,4}\)

**Marten Lienen\({}^{1,\,2}\)   Stephan Gunnemann\({}^{1,\,2}\)**

\({}^{1}\)School of Computation, Information and Technology, Technical University of Munich, Germany

\({}^{2}\)Munich Data Science Institute, Technical University of Munich, Germany

\({}^{3}\)Machine Learning Research, Morgan Stanley, United States

\({}^{4}\)Amazon Web Services, Germany

{d.luedke,m.bilos,o.shchur,m.lienen,s.guennemann}@tum.de

###### Abstract

Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive Add-Thin, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, Add-Thin naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.

## 1 Introduction

Many machine learning applications involve the analysis of continuous-time data, where the number of events and their times are random variables. This data type arises in various domains, including healthcare, neuroscience, finance, social media, and seismology. Temporal point processes (TPPs) provide a sound mathematical framework to model such event sequences, where the main problem is finding a parametrization that can capture the seasonality and complex interactions (e.g., excitation and inhibition) within point processes.

Traditional TPP models [17; 20] employ simple parametric forms, limiting their flexibility to capture the intricacies of arbitrary TPPs. In recent years, various neural TPPs have been proposed (see  for an overview) that capture complex event interactions in an autoregressive manner,

Figure 1: Proposed noising and denoising process for Add-Thin. _(Left)_ Going from step \(n-1\) to step \(n\), we add and remove some points at random. _(Right)_ Given \(^{(n)}\) and \(^{(0)}\) we know the intensity of points at step \(n-1\). We approximate this intensity with our model, which enables sampling new sequences.

often using recurrent neural networks (RNN) or transformer architectures. While autoregressive models are expressive and have shown good performance for _one-step-ahead_ prediction, their suitability for forecasting remains limited due to the accumulation of errors in sequential sampling.

We propose to take a completely different approach: instead of autoregressive modeling, we apply a generative diffusion model, which iteratively refines entire event sequences from noise. Diffusion models [18; 45] have recently shown impressive results for different data domains, such as images [15; 18; 25], point clouds [31; 32], text  and time-series [1; 3; 24; 46]. But how can we apply diffusion models to TPPs? We cannot straightforwardly apply existing Gaussian diffusion models to learn the mapping between TPPs due to the particular requirements that must be met, namely, the randomness in the number of events and the strictly positive arrival times.

We present a novel diffusion-inspired model for TPPs that allows sampling entire event sequences at once without relying on a specific choice of a parametric distribution. Instead, our model learns the probabilistic mapping from complete noise, i.e., a homogeneous Poisson process (HPP), to data. More specifically, we learn a model to reverse our noising process of adding (superposition) and removing (thinning) points from the event sequence by matching the conditional inhomogeneous denoising intensity \(_{n-1}(t^{(0)},^{(n)})\) as presented in Figure 1. Thereby, we achieve a natural way to generate sequences with varying numbers of events and expressively model arbitrary TPPs. In short, our contributions are as follows:

* We connect diffusion models with TPPs by introducing a novel model that naturally handles the discrete and continuous nature of point processes.
* We propose a model that is flexible and permits parallel and closed-form sampling of entire event sequences, overcoming common limitations of autoregressive models.
* We show that our model matches the performance of state-of-the-art TPP models in density estimation and outperforms them in forecasting.

## 2 Background

### Temporal point processes (TPPs)

Temporal point processes (TPPs) [8; 9] are stochastic processes that define a probability distribution over event sequences whose number of points (events) \(K\) and their locations (arrival times) \(t_{i}\) are random. A realization of a TPP can be represented as a sequence of strictly increasing arrival times: \(=(t_{1},,t_{K})\), \(0<t_{1}<<t_{K} T\). Viewing a TPP as a counting process, we can equivalently represent a TPP realization by a counting measure \(N(t)=_{i}^{K}(t_{i} t)\), for \(t[0,T]\). The intensity characterizing a TPP can be interpreted as the expected number of events per unit of time and is defined as:

\[(t_{t})=_{ t 0}[N(t+  t)-N(t)_{t}]}{ t},\] (1)

where \(_{t}=\{t_{i}:t_{i}<t\}\) is the event history until time \(t\), which acts as a filtration to the process.

TPPs have a number of convenient theoretical properties, two of which will be central to our derivation of a noising process for TPPs later in the paper. The first property is _superposition_: If we combine events generated by TPPs with intensities \(_{1}(t)\) and \(_{2}(t)\), the resulting event sequence will again follow a TPP, but now with intensity \(_{1}(t)+_{2}(t)\). Conversely, randomly removing each event generated by a TPP process with intensity \((t)\) with probability \(p\) is known as independent _thinning_. This is equivalent to sampling from a TPP with intensity \((1-p)(t)\).

Poisson process.A _(in)homogeneous_ Poisson process is the simplest class of TPPs, where the rate of event occurrence is independent of the history. Then the number of points on \([0,T]\) follows a Poisson distribution with rate \((T)=_{0}^{t}\,(t)\,t\). In the context of our model, the Poisson process with a constant intensity on \([0,T]\), called _homogeneous_ Poisson Process (HPP), represents the noise distribution. Even though Poisson processes can model seasonality, i.e., time-varying rates of event occurrence, they assume the events to be independent and do not capture the exciting or inhibiting behavior present in the real world, e.g., a large earthquake increasing the likelihood of observing other earthquakes soon after.

Conditional intensity.Most TPP models leverage the _conditional intensity_ function \((t_{t})\) or equivalently the conditional density \(p(t_{t})\) to overcome the independence of points limitation of an _inhomogeneous_ Poisson process. Historically, these intensity models were parameterized using hand-crafted functions [17; 20], whereas now, it is more common to use neural networks for learning intensities from data [12; 33; 41]. While the conditional intensity provides a general framework to model TPPs, sampling from these models is inherently autoregressive.

### Denoising diffusion probabilistic models

Diffusion models [18; 45] are a class of latent variable models that learn a generative model to reverse a fixed probabilistic noising process \(_{0}_{1}_{N}\), which gradually adds noise to clean data \(_{0}\) until no information remains, i.e., \(_{N} p(_{N})\). For continuous data, the forward (noising) process is usually defined as a fixed Markov chain \(q(_{n}_{n-1})\) with Gaussian transitions. Then the Markov chain of the reverse process is captured by approximating the true posterior \(q(_{n-1}_{0},_{n})\) with a model \(p_{}(_{n-1}_{n})\). Ultimately, sampling new realizations \(_{0}\) from the modeled data distribution \(p_{}(_{0})= p(_{N})_{n=1}^{N}p_{}(_{n-1 }_{n})\,_{1}_{N}\) is performed by starting with a sample from pure noise \(_{N} p(_{N})\) and gradually denoising it with the learned model over \(N\) steps \(_{N}_{N-1}_{0}\).

## 3 Add-Thin

In the following, we derive a diffusion-like model for TPPs--Add-Thin. The two main components of this model are the forward process, which converts data to noise (noising), and the reverse process, which converts noise to data (denoising). We want to emphasize again that existing Gaussian diffusion models [18; 45] are not suited to model entire event sequences, given that the number of events is random and the arrival times are strictly positive. For this reason, we will derive a new noising and denoising process (Sections 3.1 & 3.2), present a learnable parametrization and appropriate loss to approximate the posterior (Section 3.3) and introduce a sampling procedure (Sections 3.4 & 3.5).

### Forward process - Noising

Let \(^{(0)}=(t_{1},,t_{K})\) denote an i.i.d. sample from a TPP (data process) with \(T=1\) specified by the unknown (conditional) intensity \(_{0}\). We define a _forward_ noising process as a sequence of TPPs that start with the true intensity \(_{0}\) and converge to a standard HPP, i.e., \(_{0}_{1}_{N}\):

\[_{n}(t)=_{n-1}(t)}_{}+ )_{}}_{},\] (2)

where \(1>_{1}>_{2}>>_{N}>0\) and \(_{}\) denotes the constant intensity of an HPP. Equation 2 corresponds to a superposition of (i) a process \(_{n-1}\) thinned with probability \(1-_{n}\) (removing old points), and (ii) an HPP with intensity \((1-_{n})_{}\) (adding new points).

**Property** (Stationary intensity).: _For any starting intensity \(_{0}\), the intensity function \(_{N}\) given by Equation 2 converges towards \(_{}\). That is, the noised TPP will be an HPP with \(_{}\)._

Proof.: In Appendix B.1 we show that, given \(_{0}\) and Equation 2, \(_{n}\) is given by:

\[_{n}(t)=_{n}_{0}(t)+(1-_{n})_{ },\] (3)

where \(_{n}=_{j}^{n}_{j}\). Since \(_{j}^{N}_{j} 0\) as \(N\), thus \(_{N}(t)_{}\). 

If all \(_{n}\) are close to \(1\), each consecutive realization will be close to the one before because we do not remove a lot of original points, nor do we add many new points. And if we have enough steps, we will almost surely converge to the HPP. Both of these properties will be very useful in training a generative model that iteratively reverses the forward noising process.

But how can we sample points from \(_{n}\) if we do not have access to \(_{0}\)? Since we know the event sequence \(^{(0)}\) comes from a true process which is specified with \(_{0}\), we can sample from a thinned process \(_{n}_{0}(t)\), by thinning the points \(^{(0)}\) independently with probability \(1-_{n}\). This shows that even though we cannot access \(_{0}\), we can sample from \(_{n}\) by simply thinning \(^{(0)}\) and adding new points from an HPP.

To recap, given a clean sequence \(^{(0)}\), we obtain progressively noisier samples \(^{(n)}\) by both removing original points from \(^{(0)}\) and adding new points at random locations. After \(N\) steps, we reach a sample corresponding to an HPP--containing no information about the original data.

### Reverse process - Denoising

To sample realizations \(_{0}\) starting from \(^{(N)}_{HPP}\), we need to learn to reverse the Markov chain of the forward process, i.e., \(_{N}_{0}\), or equivalently \(^{(N)}^{(0)}\). Conditioned on \(^{(0)}\), the reverse process at step \(n\) is given by the posterior \(q(^{(n-1)}^{(0)},^{(n)})\), which is an inhomogeneous Poisson process for the chosen forward process (Section 3.1). Therefore, the posterior can be represented by a history-independent intensity function \(_{n-1}(t^{(0)},^{(n)})\).

As the forward process is defined by adding and thinning event sequences, the points in the random sequence \(^{(n-1)}\) can be decomposed into disjoint sets of points based on whether they are also in \(^{(0)}\) or \(^{(n)}\). We distinguish the following cases: points in \(^{(n-1)}\) that were kept from \(0\) to \(n\) (**B**), points in \(^{(n-1)}\), that were kept from \(0\) to \(n-1\) but thinned at the \(n\)-th step (**C**), added points in \(^{(n-1)}\) that are thinned in the \(n\)-th step (**D**) and added points in \(^{(n-1)}\) that are kept in the \(n\)-th step (**E**). Since the sets **B**-**E** are disjoint, the posterior intensity is a superposition of the intensities of each subsets of \(^{(n-1)}\): \(_{n-1}(t^{(0)},^{(n)})=^{(B)}(t)+^{(C)}( t)+^{(D)}(t)+^{(E)}(t)\).

To derive the intensity functions for cases **B**-**E**, we additionally define the following helper sets: **A** the points \(^{(0)}^{(n-1)}\) that were thinned until \(n-1\) and **F** the points \(^{(n)}^{(n-1)}\) that have been added at step \(n\). The full case distinction and derived intensities are further illustrated in Figure 2. In the following paragraphs, we derive the intensity functions for cases **B**-**E**:

**Case B:** The set \(^{(B)}\) can be formally defined as \(^{(0)}^{(n)}\) since \((^{(0)}^{(n)})^{(n-1)}=\) almost surely. This is because adding points at any of the locations \(t^{(0)}^{(n)}\) carries zero measure at every noisy step. Hence, given \(^{(0)}^{(n)}\) the intensity can be written as a sum of Dirac measures: \(^{(B)}(t)=_{t_{i}^{(0)}^{(n)}}_{t_{i}}(t)\). Similar to how the forward process generated \(^{(B)}\) by preserving some points from \(^{(0)}\), sampling from the reverse process preserves points from \(^{(n)}\).

**Case C:** Given \(^{(A C)}=^{(0)}^{(n)}\), \(^{(C)}\) can be found by thinning and consists of points that were kept by step \(n-1\) and removed at step \(n\). Using the thinning of Equations 2 and 3, we know the probability of a point from \(^{(0)}\) being in \(^{(C)}\) and \(^{(A C)}\) is \(_{n-1}(1-_{n})\) and \(1-_{n}\), respectively. Since we already know \(^{(B)}\) we can consider the probability of finding a point in \(^{(C)}\), given \(^{(A C)}\), which is equal to \(_{n-1}-_{n}}{1-_{n}}\). Consequently, \(^{(C)}(t)\) is given as a thinned sum of Dirac measures over \(^{(A C)}\) (cf., Figure 2).

**Case D:** The set \(^{(D)}\) contains all points \(t(^{(0)}^{(n)})\) that were added until step \(n-1\) and thinned at step \(n\). Again using Equations 2 and 3, we can see that these points were added with intensity \((1-_{n-1})_{HPP}\) and then removed with probability \(_{n}\) at the next step. Equivalently, we can write down the intensity that governs this process as \(^{(D)}(t)=(1-_{n-1})(1-_{n})_{HPP}\), i.e., sample points from an HPP and thin them to generate a sample \(^{(D)}\).

Figure 2: _(Left) Illustration of all possible disjoint sets that we can reach in our forward process going from \(^{(0)}\) to \(^{(n)}\) through \(^{(n-1)}\). (Right) Posterior intensity describing the distribution of \(^{(n-1)}^{(0)},^{(n)}\), where each subset **B**-**E** can be generated by sampling from the intensity functions.

**Case E:** The set \(^{(E)}\) can be found by thinning \(^{(E F)}=^{(n)}^{(0)}\) and contains the points that were added by step \(n-1\) and then kept at step \(n\). The processes that generated \(^{(E)}\) and \(^{(F)}\) are two independent HPPs with intensities \(^{(E)}=(1-_{n-1})_{n}_{HPP}\) and \(^{(F)}=(1-_{n})_{HPP}\), respectively, where \(^{(E)}(t)\) is derived in a similar way to \(^{(D)}(t)\). Since \(^{(E)}\) and \(^{(F)}\) are independent HPPs and we know \(}^{(E F)}\), the number of points in \(^{(E)}\) follows a Binomial distribution with probability \(p=}{^{(E)}+^{(F)}}\) (see Appendix B.2 for details). That means we can sample \(^{(E)}\) given \(^{(n)}\) and \(^{(0)}\) by simply thinning \(^{(E F)}\) with probability \(1-p\) and express the intensity as a thinned sum of Dirac functions (cf., Figure 2).

For sequences of the training set, where \(^{(0)}\) is known, we can compute these intensities for all samples \(^{(n)}\) and reverse the forward process. However, \(^{(0)}\) is unknown when sampling new sequences. Therefore, similarly to the denoising diffusion approaches , in the next section, we show how to approximate the posterior intensity, given only \(^{(n)}\). Further, in Section 3.4, we demonstrate how the trained neural network can be leveraged to sample new sequences \(_{0}\).

### Parametrization and training

In the previous section we have derived the intensity \(_{n-1}(t^{(0)},^{(n)})\) of the posterior \(q(^{(n-1)}^{(0)},^{(n)})\) for the reverse process, i.e., the intensity of points at step \(n-1\) given \(^{(n)}\) and \(^{(0)}\). Now we want to approximate this posterior using a model \(p_{}(^{(n-1)}^{(n)},n)\) to learn to sample points \(^{(n-1)}\) given only \(^{(n)}\). As we are only missing information about \(^{(0)}\) we will learn to model \(^{(B)}(t)\) and \(^{(A C)}(t)\) to approximate \(}^{(0)}^{(0)}\) (cf., Figure 3) for each \(n\) and then have access to the full posterior intensity from Section 3.2 to reverse the noising process.

Sequence embedding.To condition our model on \(^{(n)}\) and \(n\) we propose the following embeddings. We use a sinusoidal embedding  to embed the diffusion time \(n\). Further, we encode each arrival time \(t_{i}^{(n)}\) and inter-event time \(_{i}=t_{i}-t_{i-1}\), with \(t_{0}=0\), to produce a temporal event embedding \(_{i}^{d}\) by applying a sinusoidal embedding . Then we leverage a three-layered 1D-Convolutional Neural Network (CNN) with circular padding, dilation, and residual connections to compute a context embedding \(_{i}^{d}\). Compared to attention and RNN-based encoders, the CNN is computationally more efficient and scales better with the length of event sequences while allowing us to capture long-range dependencies between the events. Finally, a global sequence embedding \(}\) is generated by a mean aggregation of the context embeddings.

Posterior approximation.The posterior defining \(^{(D)}\) is independent of \(^{(0)}\) and \(^{(n)}\) and can be sampled from directly. The set \(^{(B)}\) corresponds to those points that were kept from \(^{(0)}\) until the \(n\)-th step. Since all of these points are also included in \(^{(n)}\) we specify a classifier \(g_{}(_{i},_{i},n)\) with an MLP that predicts which points from \(^{(n)}\) belong to \(^{(0)}\). As \(^{(0)}\) is known during training, this is a standard classification setting. We use the binary cross entropy (BCE) loss \(_{}\) to train \(g_{}\). Note that classifying \(^{(B)}\) from \(^{(n)}\) simultaneously predicts \(^{(n)}^{(0)}=^{(E F)}\). Therefore we can subsequently attain \(^{(E)}\) by thinning \(^{(E F)}\) as explained in Section 3.2.

To sample points \(^{(C)}\) we have to specify the intensity function \(^{(A C)}(t)\) that will be thinned to attain \(^{(C)}\) (cf., Section 3.2). As \(^{(A C)}(t)\) is a mixture of Dirac functions we use an _unnormalized_ mixture of \(H\) weighted and truncated Gaussian density functions \(f\) on \([0,T]\) to parameterize the

Figure 3: Architecture of our model predicting \(_{0}\) from \(_{n}\).

inhomogeneous intensity:

\[_{}^{(A C)}(t)=K_{j=1}^{H}w_{j}f(t;_{j}, _{j}),\] (4)

where \(w_{j}=(_{w}([n,}]))\), \(_{j}=(_{}([n,}]))\) and \(_{j}=(-|_{}([n,}])|)\) are parameterized by MLPs with two layers, a hidden dimension of \(d\) and a ReLU activation. Note that the Gaussian function is the standard approximation of the Dirac delta function and can, in the limit \( 0\), perfectly approximate it. We include \(K\), the number of points in \(^{(n)}\), in the intensity to more directly model the number of events. Then \(_{}^{(A C)}(t)\) is trained to match samples \(^{(A C)}^{(A C)}\) by minimizing the negative log-likelihood (NLL):

\[_{}=- p(^{(A C)})=-_{t_{i} ^{(A C)}}_{}^{(A C)}(t_{i})+_{0}^{T} _{}^{(A C)}(t)\,t.\] (5)

Thanks to the chosen parametrization, the integral term in \(_{}\) can be efficiently computed in any deep-learning framework using the 'erf' function, without relying on Monte Carlo approximation. We present an overview of our model architecture to predict \(^{(0)}\) from \(^{(n)}\) in Figure 3.

Training objective.The full model is trained to minimize \(=_{}+_{}\). During training, we do not have to evaluate the true posterior or sample events from any of the posterior distributions. Instead, we can simply sample \(n\) and subsequently \(^{(n)}\) and minimize \(\) for \(^{(n)}\). Interestingly, in Appendix A, we show that \(\) is equivalent to the Kullback-Leibler (KL) divergence between the approximate posterior \(p_{}(^{(n-1)}^{(n)},n)\) and the true posterior \(q(^{(n-1)}^{(0)},^{(n)})\). Ultimately, this shows that optimizing the evidence lower bound (ELBO) of the proposed model boils down to simply learning a binary classification and fitting an inhomogeneous intensity.

### Sampling

To sample an event sequence from our model, we start by sampling \(^{(N)}\) from an HPP with \(_{}\). Subsequently, for each \(n[N,,1]\), \(}^{(0)}\) is predicted by classifying \(}^{(R)}\) and sampling \(}^{(A C)}\) from \(_{}^{(A C)}(t)\). Note that the Poisson distribution with intensity \(^{(A C)}(T)=_{0}^{T}_{}^{(A C)}(t)\,t\) parameterizes the number of points in \(A C\). Therefore, \(}^{(A C)}\) can be sampled by first sampling the number of events and then sampling the event times from the normalized intensity \(_{}^{(A C)}(t)/^{(A C)}(T)\). Given our predicted \(}^{(0)}\) we can sample \(}^{(n-1)}\) from the posterior intensity defined in Section 3.2. By repeating this process, we produce a sequence of \(^{(n-1)}\)s. Finally, we obtain a denoised sample \(^{(0)}\) by predicting it from \(}^{(1)}\). We provide an overview of the sampling procedure as pseudo-code in Algorithm 1.

``` \(^{(n=N)}_{}\); for\(n\{N,,1\}\)do  sample \(}^{(B)}_{t_{i}^{(n)}}g_{}(t_{i}_{i},_{i},n)_{t_{i}}(t)\);  sample \(}^{(C)}_{n-1}-_{n}}{1-_{n}}_ {}^{(A C)}(t^{(n)},n)\);  sample \(}^{(D)}(1-_{n-1})(1-_{n})_{}\);  sample \(}^{(E)}_{t_{i}^{(n)}}^{(E)}} -_{n}}{1-_{n}}_{t_{i}}(t)\); \(^{(n-1)}}^{(B)}}^{(C)}}^{(D)}}^{(E)}\);  end for  sample \(}^{(A C)}_{}^{(A C)}(t^{(1)},1)\);  sample \(}^{(B)}_{t_{i}^{(1)}}g_{}(t_{i} _{i},_{i},1)_{t_{i}}(t)\); \(}^{(B)}}^{(A C)}\); return\(\) ```

**Algorithm 1**Sampling

### Conditional sampling

The above-described process defines an _unconditional_ generative model for event sequences on an interval \([0,T]\). For many (multi-step) forecasting applications, such as earthquake forecasting , we need to condition our samples on previous event sequences and turn our model into a conditional one that can generate future event sequences in \([H,H+T]\) given the past observed events in \([0,H]\). To condition our generative model on a history, we apply a simple GRU encoder to encode the history into a \(d\)-dimensional history embedding \(\), which subsequently conditions the classifier and intensity model by being added to the diffusion time embedding.

Related work

Autoregressive neural TPPs.Most neural TPPs model the intensity or density of each event conditional on a history and consequently consist of two parts: a history encoder and an intensity/density decoder. As history encoders, RNNs [12; 41] and attention-based set encoders [50; 52] have been proposed. Attention-based encoders are postulated to better model long-range dependencies in the event sequences, but at the cost of a more complex encoder structure . To decode the intensity \((t|)\), density \(p(t|)\) or the cumulative hazard function from the history, uni-modal distributions , mixture-distributions , a mixture of kernels [36; 44; 51], neural networks  and Gaussian diffusion  have been proposed. Another branch of neural TPPs models the event times conditional on a latent variable that follows a continuous-time evolution [5; 13; 16; 21], where, e.g., Hasan et al.  relate inter-event times of a TPP to the excursion of a stochastic process. In general, most neural TPPs are trained by maximizing the log-likelihood, but other training approaches have been proposed [26; 29; 49]. We want to highlight the difference of our model to two related works. TriTPP  learns a deterministic mapping between a latent HPP and a TPP using normalizing flows, which allows for parallel sampling. However, it models the conditional hazard function, which forces a conditional dependency of later arrival times and can still produce error accumulation. Lin et al.  proposed an autoregressive TPP model leveraging Gaussian diffusion to approximate the conditional density. Besides being autoregressive, the model does not directly model the number of points in the TPP but instead is trained to maximize the ELBO of the next inter-event time.

Non-autoregressive neural TPPs.An alternative to the conditional (autoregressive) modeling of TPPs is to apply a latent variable model that learns to relate entire point processes to latent variables. The class of Cox processes [7; 11; 19] models point processes through a hierarchy of latent processes under the assumption that higher-level latent variables trigger lower-level realizations. Add-Thin can be considered to be a non-autoregressive latent variable model.

Denoising diffusion models.Recently, denoising diffusion models on continuous state spaces [18; 45] established the new state-of-the-art for many image applications [15; 18; 25]. Subsequently, diffusion models for other application domains such as point clouds [31; 32], physical simulations [23; 28; 30] and time-series [1; 3; 24; 46] emerged. While the majority of denoising diffusion models are based on Gaussian transition kernels in continuous state spaces proposed in [18; 45], a variety of diffusion models for discrete state spaces such as graphs , text [2; 27] and images [2; 6] have been presented. Here, we highlight the similarity of our work to the concurrent work of Chen and Zhou , who derived a diffusion process that models pixel values as a count distribution and thins them to noise images. In contrast to the related work on continuous and discrete state space diffusion models, Add-Thin constitutes a novel diffusion model defined on a state space that captures both the discrete and continuous components of point processes.

## 5 Experiments

We evaluate the proposed model in two settings: density estimation and forecasting. In density estimation, the goal is to learn an unconditional model for event sequences. As for forecasting, the objective is to accurately predict the entire future event sequence given the observed past events.

Data.Add-Thin is evaluated on 7 real-world datasets proposed by Shchur et al.  and 6 synthethic datasets from Omi et al. . The synthetic datasets consist of Hawkes1 and Hawkes2 , a self-correcting (SC) , inhomogeneous Poisson process (IPP) and a stationary and a non-stationary renewal process (MRP, RP) [39; 11]. For the real-world datasets, we consider PUBG, Reddit-Comments, Reddit-Submissions, Taxi, Twitter, and restaurant check-ins in Yelp1 and Yelp2. We split each dataset into train, validation, and test set containing 60%, 20%, and 20% of the event sequences, respectively. Further dataset details and statistics are reported in Appendix C.

Baselines.We apply the _RNN_-based intensity-free TPP model from Shchur et al. . Similar to Sharma et al. , we further combine the intensity-free model with an attention-based encoder from Zuo et al.  as a _Transformer_ baseline. Additionally, we compare our model to an autoregressive TPP model with a continuous state Gaussian-diffusion  from Lin et al. , which we abbreviate as _GD_. Lastly, _TriTPP_ is used as a model that provides parallel but autoregressive sampling.

Training and model selection.We train each model by its proposed training loss using Adam . For our model, we set the number of diffusion steps to \(N=100\), apply the cosine beta-schedule proposed in Glide , and set \(_{}=1\) for the noising process. We apply early stopping, hyperparameter tuning, and model selection on the validation set for each model. Further hyperparameter and training details are reported in Appendix D.

### Sampling - Density estimation

A good TPP model should be flexible enough to fit event sequences from various processes. We evaluate the generative quality of the TPP models on 13 synthetic and real-world datasets by drawing 4000 TPP sequences from each model and computing distance metrics between the samples and event sequences from a hold-out test set. In short, the goal of this experiment is to show that our proposed model is a flexible TPP model that can generate event sequences that are 'close' to the samples from the data-generating distribution.

Metrics.In general, diffusion models cannot evaluate the exact likelihood. Instead, we evaluate the quality of samples by comparing the distributions of samples from each model and the test set with the maximum mean discrepancy measure (MMD)  as proposed by Shchur et al. . Furthermore, we compute the Wasserstein distance  between the distribution of sequence lengths of the sampled sequences and the test set. We report the results on the test set averaged over five runs with different seeds.

Results.Table 1 presents the MMD results for all models and datasets. Among them, the RNN baseline demonstrates a strong performance across all datasets and outperforms both _Transformer_ and _GD_. Notably, Add-Thin exhibits competitive results with the autoregressive baseline, surpassing or matching (\( 0.01\)) it on 11/13 datasets. Additionally, Add-Thin consistently outperforms the _Transformer_ and _GD_ model on all datasets except SC and RP. Lastly, _TriTPP_ performs well on most datasets but is outperformed or matched by our model on all but two datasets.

Table 2 shows the result for comparing the count distributions. Overall, the Wasserstein distance results align closely with the MMD results. However, the _GD_ model is an exception, displaying considerably worse performance when focusing on the count distribution. This outcome is expected since the training of the _GD_ model only indirectly models the number of events by maximizing the ELBO of each diffusion step to approximate the conditional density of the next event and not the likelihood of whole event sequences. Again, Add-Thin shows a very strong performance, further emphasizing its expressiveness.

In summary, these results demonstrate the flexibility of our model, which can effectively capture various complex TPP distributions and matches the state-of-the-art performance in density estimation.

  & Hawkes1 & Hawkes2 & SC & IPP & RP & MRP & PUBG & Reddit-C & Reddit-S & Taxi & Twitter & Yelp1 & Yelp2 \\  RNN & **0.02** & **0.01** & **0.08** & 0.05 & **0.01** & **0.03** & 0.04 & **0.01** & **0.02** & **0.04** & **0.03** & 0.07 & **0.03** \\ Transformer & 0.03 & 0.04 & 0.19 & 0.10 & 0.02 & 0.19 & 0.06 & 0.05 & 0.09 & 0.09 & 0.08 & 0.12 & 0.14 \\ GD & 0.06 & 0.06 & 0.13 & 0.08 & 0.05 & 0.14 & 0.11 & 0.03 & 0.03 & 0.10 & 0.15 & 0.12 & 0.10 \\ TriTPP & 0.03 & 0.04 & 0.23 & 0.04 & 0.02 & 0.05 & 0.06 & 0.09 & 0.12 & 0.07 & 0.04 & **0.06** & 0.06 \\ Add-Thin & **0.02** & 0.02 & 0.19 & **0.03** & 0.02 & 0.10 & **0.03** & **0.01** & **0.02** & **0.04** & 0.04 & 0.08 & 0.04 \\ 

Table 1: MMD (\(\)) between the TPP distribution of sampled sequences and hold-out test set (**bold** best, underline second best). The results with standard deviation are reported in Appendix E.

  & Hawkes1 & Hawkes2 & SC & IPP & RP & MRP & PUBG & Reddit-C & Reddit-S & Taxi & Twitter & Yelp1 & Yelp2 \\  RNN & **0.03** & **0.01** & **0.00** & 0.02 & **0.02** & **0.01** & **0.02** & **0.01** & 0.05 & **0.02** & **0.01** & 0.04 & **0.02** \\ Transformer & 0.06 & 0.04 & 0.06 & 0.07 & 0.04 & 0.11 & 0.04 & 0.08 & 0.11 & 0.13 & 0.05 & 0.11 & 0.21 \\ GD & 0.16 & 0.13 & 0.50 & 0.42 & 0.28 & 0.50 & 0.54 & 0.02 & 0.16 & 0.33 & 0.07 & 0.26 & 0.25 \\ TriTPP & **0.03** & 0.03 & 0.01 & **0.01** & **0.02** & 0.03 & 0.03 & 0.09 & 0.09 & 0.04 & **0.01** & **0.03** & 0.04 \\ Add-Thin & 0.04 & 0.02 & 0.08 & **0.01** & **0.02** & 0.04 & **0.02** & 0.03 & **0.04** & 0.03 & **0.01** & 0.04 & **0.02** \\ 

Table 2: Wasserstein distance (\(\)) between the distribution of the number of events of sampled sequences and hold-out test set (**bold** best, underline second best). The results with standard deviation are reported in Appendix E.

### Conditional sampling - Forecasting

Forecasting event sequences from history is an important real-world application of TPP models. In this experiment, we evaluate the forecasting capability of our model on all real-world datasets. We evaluate each model's performance in forecasting the events in a forecasting window \( T\), by randomly drawing a starting point \(T_{s}[ T,T- T]\). Then, the events in \([0,T_{s}]\) are considered the history, and \([T_{s},T_{s}+ T]\) is the forecasting time horizon.

In the experiment, we randomly sample 50 forecasting windows for each sequence from the test set, compute history embedding with each model's encoder, and then conditionally sample the forecast from each model. Note that _TriTPP_ does not allow for conditional sampling and is therefore not part of the forecasting experiment.

Metrics.To evaluate the forecasting experiment, we will not compare distributions of TPPs but rather TPP instances. We measure the distance between two event sequences, i.e., forecast and ground truth data, by computing the distance between the count measures with the Wasserstein distance between two TPPs, as introduced by Xiao et al. . Additionally, we report the mean absolute relative error (MAPE) between the predicted sequence length and ground truth sequence length in the forecasting horizon. We report the results on the test set averaged over five runs with different seeds.

Results.Table 3 presents the average Wasserstein distance between the predicted and ground truth forecast sequences. The results unequivocally demonstrate the superior performance of our model by forecasting entire event sequences, surpassing all autoregressive baselines on all datasets. Notably, the disparity between Add-Thin and the baselines is more pronounced for datasets with a higher number of events per sequence, indicating the accumulation of prediction errors in the autoregressive models. Further, the transformer baseline achieves better forecasting results than the RNN baseline for some datasets with more events. This suggests that long-range attention can improve autoregressive forecasting and mitigate some error accumulation.

Table 4 reports the MAPE between the forecasted and ground truth sequence length. The MAPE results align consistently with the Wasserstein distance across all models and datasets. Figure 4

  & PUBG & Reddit-C & Reddit-S & Taxi & Twitter & Yelp1 & Yelp2 \\  Average Seq. Length & 76.5 & 295.7 & 1129.0 & 98.4 & 14.9 & 30.5 & 55.2 \\  RNN & 1.72 & 5.47 & 0.68 & 0.54 & 0.95 & 0.59 & 0.72 \\ Transformer & 0.65 & 7.38 & 0.55 & 0.46 & 1.18 & 0.63 & 0.99 \\ GD & 1.66 & 10.49 & 1.33 & 0.71 & 1.43 & 0.78 & 1.65 \\ Add-Thin (Ours) & **0.45** & **1.07** & **0.38** & **0.37** & **0.69** & **0.45** & **0.50** \\ 

Table 4: Count MAPE \( 100\%\) between forecasted event sequences and ground truth reported for 50 random forecast windows on the test set (lower is better). The results with standard deviation are reported in Appendix E.

Figure 4: \(5\%\), \(25\%\), \(50\%\), \(75\%\), and \(95\%\) quantile of forecasts generated by Add-Thin for a Taxi event sequence (_blue_: history, _black_ ground truth future).

  & PUBG & Reddit-C & Reddit-S & Taxi & Twitter & Yelp1 & Yelp2 \\  Average Seq. Length & 76.5 & 295.7 & 1129.0 & 98.4 & 14.9 & 30.5 & 55.2 \\  RNN & 6.15 & 35.22 & 39.23 & 4.14 & 2.04 & 1.28 & 2.21 \\ Transformer & 2.45 & 38.77 & 27.52 & 3.12 & 2.09 & 1.29 & 2.64 \\ GD & 5.44 & 44.72 & 64.25 & 4.32 & 2.16 & 1.52 & 4.25 \\ Add-Thin (Ours) & **2.03** & **17.18** & **21.32** & **2.42** & **1.48** & **1.00** & **1.54** \\ 

Table 3: Wasserstein distance between forecasted event sequence and ground truth reported for 50 random forecast windows on the test set (lower is better). The results with standard deviation are reported in Appendix E.

depicts the quantiles for 1000 forecasts generated by Add-Thin for one Taxi event sequence and highlights the predictive capacity of our model. Overall, our model outperforms state-of-the-art TPP models in forecasting real-world event sequences.

## 6 Discussion

Add-Thin vs. autoregressive TPP models.On a conceptual level, Add-Thin presents a different trade-off compared to other TPP models: Instead of being autoregressive in event time, our model gradually refines the entire event sequence in parallel at every diffusion step to produce a sample from the learned data distribution. Thereby, we have found that our model is better suited for forecasting and modeling very long event sequences than autoregressive TPP models. Furthermore, the iterative refinement of the entire sequence allows us to leverage simple and shared layers to accurately model the long-range interaction between events and results in nearly constant sampling times across different sequence lengths (cf., Appendix E.3).

Limitations and future work.With Add-Thin, we have derived a novel diffusion-inspired model for TPPs. Thereby, we focused on modeling the arrival times of the events and did not model continuous and discrete marks. However, we see this as an exciting extension to our framework, which might incorporate Gaussian diffusion  for continuous marks and discrete diffusion  for discrete marks. Further, while generative diffusion is known to produce high-quality samples, it also can be expensive. Besides tuning the number of diffusion steps, future work could focus on alternative and faster sampling routines . Ultimately, we hope that by having connected diffusion models with TPPs, we have opened a new direction to modeling TPPs and broadened the field of diffusion-based models. Here, it would be especially interesting for us to see whether our framework could benefit other application domains in machine learning that involve sets of varying sizes, such as graph generation (molecules), point clouds, and spatial point processes.

## 7 Conclusion

By introducing Add-Thin, we have connected the fields of diffusion models and TPPs and derived a novel model that naturally handles the discrete and continuous nature of point processes. Our model permits parallel and closed-form sampling of entire event sequences, overcoming common limitations of autoregressive TPP models. In our experimental evaluation, we demonstrated the flexibility of Add-Thin, which can effectively capture complex TPP distributions and matches the state-of-the-art performance in density estimation. Additionally, in a long-term forecasting task on real-world data, our model distinctly outperforms the state-of-the-art TPP models by predicting entire forecasting windows non-autoregressively.

## Broader impact

We see the proposed model as a general framework to model continuous-time event data. As such, our method can be applied to many fields, where common application domains include traffic, social networks, and electronic health records. We do not find any use cases mentioned above raise ethical concerns; however, it is essential to exercise caution when dealing with sensitive personal data.