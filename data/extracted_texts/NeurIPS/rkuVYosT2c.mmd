# Distributed Least Squares in Small Space via

Sketching and Bias Reduction

 Sachin Garg

Computer Science & Engineering

University of Michigan

sachg@umich.edu &Kevin Tan

Department of Statistics

University of Pennsylvania

kevatan@umich.edu

&Michal Derezinski

Computer Science & Engineering

University of Michigan

derezin@umich.edu

###### Abstract

Matrix sketching is a powerful tool for reducing the size of large data matrices. Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression. We show that these limitations can be circumvented in the distributed setting by designing sketching methods that minimize the bias of the estimator, rather than its error. In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data. This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches. Our key novelty is a new bias analysis for sketched least squares, giving a sharp characterization of its dependence on the sketch sparsity. The techniques include new higher-moment restricted Bai-Silverstein inequalities, which are of independent interest to the non-asymptotic analysis of deterministic equivalents for random matrices that arise from sketching.

## 1 Introduction

Matrix sketching is a powerful collection of randomized techniques for compressing large data matrices, developed over a long line of works as part of Randomized Numerical Linear Algebra [RandNLA, e.g., 45, 26, 37, 39, 21]. Sketching can be used to reduce the large dimension \(n\) of a data matrix \(^{n d}\) by applying a random sketching matrix (operator) \(^{m n}\) to obtain the sketch \(}=^{m d}\) where \(m n\). For example, sketching can be used to approximate the solution to the least squares problem, \(^{*}=*{argmin}_{}L()\) where \(L()=\|-\|^{2}\), by using a sketched estimator \(}=*{argmin}_{}\|} -\|^{2}\), where \(}=\) and \(=\).

Perhaps the simplest form of sketching is subsampling, where the sketching operator \(\) selects a random sample of the rows of matrix \(\). However, the real advantage of sketching as a framework emerges as we consider more complex operators \(\), such as sub-Gaussian matrices , randomized Hadamard transforms , and sparse random matrices . These approaches ensure higher quality and more robust compression of the data matrix, e.g., leading to provable \(\)-approximation guarantees for the estimate \(}\) in the least squares task, i.e., \(L(})(1+)L(^{*})\). Nevertheless, there are fundamental limitations to how far we can compress a data matrix using sketching while ensuring an \(\)-approximation. These limitations pose a challenge particularly in space-limited computingenvironments, such as for streaming algorithms where we observe the matrix \(\), say, one row at a time, and we have limited space for storing the sketch .

One strategy for overcoming the fundamental limitations of sketching as a compression tool is to look beyond the single approximation guarantee provided by a sketching-based estimator \(}\), and consider how its broader statistical properties can be leveraged in a given computing environment. To that end, many recent works have demonstrated both theoretically and empirically that sketching-based estimators often exhibit not only approximation robustness but also statistical robustness, for instance enjoying sharp confidence intervals, effectiveness of statistical inference tools such as bootstrap and cross-validation, as well as accuracy boosting techniques such as distributed averaging [e.g., 35, 25, 32, 33]. Yet, these results have had limited impact on the traditional computational complexity analysis in RandNLA and sketching literature, as many of them either impose additional assumptions, focus on sharpening the constant factors, or require using more expensive sketching techniques. In this work, we demonstrate that the statistical properties of sketching-based estimators can in fact have a substantial impact on the computational trade-offs that arise in RandNLA.

Our key motivating example is the above mentioned least squares regression task. It is well understood that for an \(n d\) least squares task, to recover an \(\)-approximate solution out of an \(m d\) sketch, we need sketch size at least \(m=(d/)\). This has been formalized in the streaming setting with a lower bound of \((^{-1}d^{2}(nd))\) bits of space required, when all of the input numbers use \(O((nd))\) bits of precision . One setting where this can be circumvented is in the distributed computing model where the bits can be spread out across many machines, so that the per-machine space can be smaller. Here, one could for instance hope that we can maintain small \(O(d) d\) sketches in \(q=O(1/)\) machines and then combine their estimates to recover an \(\)-approximate solution. A simple and attractive approach is to average the estimates \(}_{i}\) produced by the individual machines, returning \(}=_{i=1}^{q}}_{i}\), as this only requires each machine to communicate \(O(d(nd))\) bits of information about its sketch. This approach requires the sketching-based estimates \(}_{i}\) to have sufficiently small bias for the averaging scheme to be effective. While this has been demonstrated empirically in many cases, existing theoretical results still require relatively expensive sketching methods to recover low-bias estimators, leading to an unfortunate trade-off in the distributed averaging scheme between the time and space complexity required.

In this work, we address the time-space trade-off in distributed averaging of sketching-based estimators, by giving a sharp characterization of how their bias depends on the sparsity of the sketching matrix. Remarkably, we show that in the distributed streaming environment one can compress the data down to the minimum size of \(O(d^{2}(nd))\) bits at no extra computational cost, while still being able to recover an \(\)-approximate solution for least squares and related problems. Importantly, our results require the sketching matrix to be slightly denser than is necessary for obtaining approximation guarantees on a single estimate, and thus, cannot be recovered by standard RandNLA sampling methods such as approximate leverage score sampling .

Before we state the full result in the distributed setting, we give our main technical contribution. which is the following efficient construction of a low-bias least squares estimator in a single pass using only \(O(d^{2}(nd))\) bits of space, assuming all numbers use \(O((nd))\) bits of precision. Below, \(>0\) denotes an arbitrarily small constant.

Figure 1: Illustration of the leverage score sparsification algorithm used in Theorem 1. Each row of the sketch mixes \((1/)\) leverage score samples from \(\). Remarkably, the \(\)-error guarantee of the subsampled estimator is retained as \(\)-bias of the sketched estimator.

**Theorem 1**.: _Given streaming access to \(^{n d}\) and \(^{n}\), and direct access to a preconditioner matrix \(^{d d}\) such that \(()\), within a single pass over \((,)\), in \(O(^{-1}()+^{-1} d^{2+}(d))\) time and \(O(d^{2}(nd))\) bits of space, we can construct a randomized estimator \(}\) for the least squares solution \(^{*}=*{argmin}_{}\|- \|^{2}\) such that:_

\[\|[} ]-\|^{2} (1+)\|^{*}-\|^{2},\] \[\|}-\|^{2}  2\,\|^{*}-\|^{2}.\]

**Remark 1**.: _The above construction assumes access to a preconditioner matrix \(\) with \(()\) (where \(\) denotes the condition number). Such matrix can be obtained efficiently with \(=O(1)\) in a separate single pass, leading to a two-pass algorithm described later in Theorem 2._

Our estimator \(}\) is constructed at the end of the data pass from a sketch \((},})\) where \(}=\) and \(}=\), by minimizing \(\|}-}\|^{2}\) using preconditioned conjugate gradient. Here, \(\) is a carefully constructed sparse sketching matrix which is inspired by the so-called leverage score sparsified (LESS) embeddings . Leverage scores represent the relative importances of the rows of \(\) which are commonly used for subsampling in least squares (see Definition 1), and their estimates can be easily obtained in a single pass by using the preconditioner matrix \(\).

Our time complexity bound of \((()+d^{2}/)\) matches the time it would take (for a single machine) to subsample \((d/)\) rows of \(\) according to the approximate leverage scores and produce an estimator \(}\) that achieves the \(\)-error bound \(\|}-\|^{2}(1+)\| ^{*}-\|^{2}\). However, this strategy requires either maintaining \((d^{2}/)\) bits of space for the sketch, or computing \(}\) directly along the way, blowing up the runtime to \((d^{}/)\). Since approximate leverage score sampling leads to significant least squares bias, averaging can only improve this to \((d^{}/)\) (see Table 1). An alternate strategy would be to combine leverage score sampling with a preconditioned mini-batch stochastic gradient descent (Weighted Mb-SGD), with mini-batches chosen so that they fit in \((d^{2})\) space. This achieves the same time and space complexity as our method, but due to the streaming access to \(\) and the sequential nature of SGD, it requires \(O(1/)\) data passes.

Instead, our algorithm essentially mixes an \((d/)\) size leverage score sample into an \(O(d)\) size sketch, merging \((1/)\) rows of \(\) into a single row of the sketch (see Figure 1). This results in better data compression compared to direct leverage score sampling, with only \((d^{2})\) bits of space, while retaining the same \((()+d^{2}/)\) runtime complexity as the above approaches and requiring only a single data pass. The resulting estimator \(}\) can no longer recover the \(\)-error bound, but remarkably, its expectation \([}]\) still does. To turn this into an improved estimator in a distributed model, we can simply average \(q=1/\) such estimators, i.e., \(}=_{i=1}^{q}}_{i}\), obtaining \(\|}-\|^{2}(1+2)\| ^{*}-\|^{2}\). As shown in Table 1, ours is the first result in this model to achieve \((d^{2})\) space in a single pass and faster than current matrix multiplication time \(O(d^{})\).

Finally, by incorporating a preconditioning scheme, we illustrate how our construction can be used to design the first algorithm that solves least squares in current matrix multiplication time, constant parallel passes and \(O(d^{2}(nd))\) bits of space. We note that the \(O(d^{})\) cost comes only from the worst-case complexity of constructing the preconditioner \(\), which can often be accelerated in practice. The computational model used in Theorem 2 is described in detail in Section 3.

 Reference & Method & Total runtime & Parallel passes \\   Folklore & Gaussian sketch & \((nd^{-1})\) & 1 \\
 & Leverage Score Sampling & \(()+(d^{}/)\) & 1 \\
 & Determinantal Point Process & \(()+(d^{})\) & \(^{3}(n/)\) \\
 & Weighted Mb-SGD (sequential) & \(()+(d^{2}/)\) & \(1/\) \\ 
**This work** (Thm. 1) & Leverage Score Sparsification & \(()+(d^{2}/)\) & 1 \\ 

Table 1: Comparison of time complexities and parallel passes over the data required for different methods to obtain a \((1+)\)-approximation in \(O(d^{2}(nd))\) bits of space for an \(n d\) least squares problem \((,)\), given a preconditioner \(\) such that \(()=O(1)\) (see Section 3 for our computational model). We include the fully sequential Weighted Mb-SGD as a reference.

**Theorem 2**.: _Given \(^{n d}\) and \(^{n}\) in the parallel computing model, using two parallel passes with \(q\) machines, we can compute \(}\) such that with probability \(0.9\)_

\[\|}-\|1++O(1/q) \|^{*}-\|\]

_in \(O(^{-1}()+d^{}+^{-1}d^{2+} (d))\) time, \(O(d^{2}(nd))\) bits of space and \(O(d(nd))\) bits of communication. In particular, choosing \(q=1/\) we recover an \(O()\)-approximation._

**Remark 2**.: _While the parallel computing model in Theorem 2 assumes that all machines have streaming access to the entire data matrix, this result can be easily extended to the setting where \(\) has been randomly down-sampled or partitioned into separate size \(x\) chunks \(_{1},...,_{q}\), and each machine constructs an estimate \(}_{i}\) based on a sketch of its own chunk. Then, with the same computational guarantees as in Theorem 2, the averaged estimator \(}=_{i=1}^{q}}_{i}\) with probability \(0.9\) enjoys a guarantee of:_

\[\|}-\|1+O(+1/q+B_{ })\|^{*}-\|,\]

_where \(B_{}\) is the bias that would be incurred if we solved each chunk exactly and averaged those solutions. Using existing guarantees for uniform down-sampling of least squares [Theorem 20, 43], one can bound this bias as \(B_{}=+()^{2}\), where \(\) is the coherence of the data matrix and \(n\) is the chunk size. For sufficiently large chunks, this bias is negligible compared to the error \(\). We prove the above high probability guarantee in Theorem 6 in Appendix E._

Further applications.Our least squares analysis can be extended to other settings where prior works [e.g., 17, 18, 16, 22] have analyzed randomized estimators based on sparse sketching via techniques from asymptotic random matrix theory. The primary and most direct application involves correcting _inversion bias_ in the so-called sketched inverse covariance estimate \((}^{}})^{-1}\), which was the motivating task of , with applications including distributed second-order optimization and statistical uncertainty quantification, where quantities like \((}^{}})^{-1}\) are approximated.

**Theorem 3** (informal Theorem 5).: _Given \(^{n d}\) and its LESS embedding \(\) with sketch size \(m Cd\) and \(s\) non-zeros per row, the inverse covariance sketch \((^{}^{})^{-1}\) is an \((,)\)-unbiased estimator of \((^{})^{-1}\) (see Definition 3) for \(=(1+)}{m}\) and \(=1/(d)\)._

This result should be compared with \(=(1+d/s)}{m}\) obtained by . Thus, we get a direct improvement for very sparse sketches, i.e., \(s=o(d)\). This can be immediately translated into an improved local convergence guarantee for Distributed Newton Sketch which is a second-order convex minimization algorithm used in settings where the Hessian matrix can be expressed as \(^{}\) for a tall matrix \(\), e.g., in generalized linear models like logistic regression. In this method, following Corollary 16 of  as well as related results , we use sketching and averaging to estimate a Newton step:

\[_{t+1}=_{t}-_{i=1}^{q}}_{ i}^{-1}_{t},\]

where \(_{t}\) is the gradient at \(_{t}\) and \(}_{1},...,}_{q}\) are Hessian sketches constructed by independent machines. The following corollary, which is a direct improvement over Corollary 16 of , shows that Distributed Newton Sketch on a generalized linear model task can achieve a fast local convergence rate of the form \(_{t+1})-f(^{*})}{f(_{t})-f(^{* })}=o(1)\) with \(O(d^{})\) time per iteration (see Appendix B.1 for details).

**Corollary 1** (Distributed Newton Sketch).: _Consider \(f()=_{i=1}^{n}_{i}(^{}_{i})+ \|\|^{2}\), where \(_{i}\) are convex twice continuously differentiable functions, such that \(f\) has a Lipschitz Hessian, \(>0\), and \(_{i}^{}\) is the \(i\)th row of an \(n d\) data matrix \(\). Given \(>0\), there is a neighborhood \(U_{}\) around the minimizer \(^{*}=*{argmin}_{}f()\) such that, for any \(_{t} U_{}\), using two parallel passes with \((1/)\) machines, we can compute a Distributed Newton Sketch update \(_{t+1}\) such that_

\[f(_{t+1})-f(^{*})f(_{t})- f(^{*}),\]

_in \(O(^{-1}()+d^{}+^{-1}d^{2+} (d))\) time, \(O(d^{2}(nd))\) bits of space and \(O(d(nd))\) bits of communication._Our Techniques.At the core of our analysis are techniques inspired by asymptotic random matrix theory (RMT) in the proportional limit [e.g., see 6]. Here, in order to establish the limiting spectral distribution (such as the Marchenko-Pastur law) of a random matrix \(}^{}}\) whose dimensions diverge to infinity, one aims to show the convergence of the Stieltjes transform of its resolvent matrix \((}^{}}-z)^{-1}\). Recently,  showed that these techniques can be adapted to sparse sketching matrices (via leverage score sparsification) in order to characterize the bias of the sketched inverse covariance \((}^{}})^{-1}\), where \(}=\).

Our main contribution is two-fold. First, we show that a similar argument can also be applied to analyze the bias of the least squares estimator, \(}=(}^{}})^{-1}}^{}}\). Unlike the inverse covariance, this estimator no longer takes the form of a resolvent matrix, but its bias is also associated with the inverse, which means that we can use a leave-one-out argument to characterize the effect of removing a single row of the sketch on the estimation bias. Our second main contribution is to improve the sharpness of the bounds relative to the sparsity of the sketching matrix by combining a careful application of Holder's inequality with a higher moments analysis of the restricted Bai-Silverstein inequality for quadratic forms. Those improvements are not only applicable to the least squares analysis, but also to all existing RMT-style results for LESS embeddings, including the aforementioned inverse covariance estimation, as well as applications in stochastic optimization, resulting in the sketching cost of LESS embeddings dropping below matrix multiplication time.

## 2 Related Work

Randomized numerical linear algebra.RandNLA sketching techniques have been developed over a long line of works, starting from fast least squares approximations of ; for an overview, see [45; 26; 37; 39; 21] among others. Since then, these methods have been used in designing fast algorithms not only for least squares but also many other fundamental problems in numerical linear algebra and optimization including low-rank approximation [13; 34], \(l_{p}\) regression , solving linear systems [28; 24] and more. Using sparse random matrices for matrix sketching also has a long history, including data-oblivious sketching methods such as CountSketch , OSNAP , and more . Leverage score sparsification (LESS) was introduced by  as a data-dependent sparse sketching method to enable RMT-style analysis for sketching (see below).

Unbiased estimators for least squares.To put our results in a proper context, let us consider other approaches for producing near-unbiased estimators for least squares, see also Table 1. First, a well known folklore result states that the least squares estimator computed from a dense Gaussian sketching matrix is unbiased. The bias of other sketching methods, including leverage score sampling and OSNAP, has been studied by , showing that these methods need a \(\)-error guarantee to achieve an \(\)-bias which leads to little improvement unless \(\) is extremely small and the sketch size is sufficiently large. Another approach of constructing unbiased estimators for least squares, first proposed by , is based on subsampling with a non-i.i.d. importance sampling distribution based on Determinantal Point Processes [DPPs, 31; 20]. However, despite significant efforts [30; 7; 5], sampling from DPPs remains quite expensive: the fastest known algorithm requires running a Markov chain for \((n/)\) many steps, each of which requires a separate data pass and takes \(O(d^{})\) time. Other approaches have also been considered which provide partial bias reduction for i.i.d. RandNLA subsampling schemes in various regimes that are are either much more expensive or not directly comparable to ours [2; 44].

Statistical and RMT analysis of sketching.Recently, there has been significant interest in statistical and random matrix theory (RMT) analysis of matrix sketching; see  for an overview. These approaches include both asymptotic analysis via limiting spectral distributions and deterministic equivalents [35; 25; 32; 33], and non-asymptotic analysis under statistical assumptions [36; 41; 4]. A number of works have shown that the RMT-style techniques based on deterministic equivalents can be made rigorously non-asymptotic for certain sketching methods such as dense sub-Gaussian , LESS matrices [16; 18], and other sparse matrices , which has been applied to low-rank approximation, fast subspace embeddings and stochastic optimization. Our new analysis can be viewed as a general strategy for directly improving the sparsity required by LESS embeddings (and thereby, the sketching time complexity) in many of these applications, specifically those that rely on analysis inspired by the calculus of deterministic equivalents via generalized Stieltjes transforms.

Preliminaries

Notations.In all our results, we use lowercase letters to denote scalars, lowercase boldface for vectors, and uppercase boldface for matrices. The norm \(\|\|\) denotes the spectral norm for matrices and the Euclidean norm for vectors, whereas \(\|\|_{F}\) denotes the Frobenius norm for matrices. We use \(\) to denote the p.s.d. ordering of matrices.

Computational model.We next clarify the computational model that is used in Theorem 2. We consider a central data server storing \((,)\), and \(q\) machines. The \(j\)th machine has a handle \((j)\), which can be used to _open_ a stream and to _read_ the next row/label pair \((_{i},b_{i})\) in the stream. After a full pass, the machine can re-open the handle and begin another pass over the data. The machines can operate their streams entirely asynchronously, and each has its own limited local storage space, e.g., in Theorem 2 we use \(O(d^{2}(nd))\) bits of space per machine. At the end, they can communicate some information back to the server, e.g., in Theorem 2, they communicate their final estimate vectors \(}_{j}\), using \(O(d(nd))\) bits of communication. Then, the server computes the final estimate, in our case via averaging, \(}=_{i=1}^{q}}_{i}\), which can be done either directly or via a map-reduce type architecture.

We define the _parallel passes_ required by such an algorithm as the maximum number of times the stream is opened by any single machine. We analogously define time/space/communication costs by taking a maximum over the costs required by any single machine (for communication, this refers only to the number of bits sent from the machine back to the server).

Definitions and useful lemmas.In our framework, we construct a sparse sketching matrix \(\) where sparsification is achieved using a probability distribution over rows of data matrix \(\), that is proportional to the leverage scores of \(\). The next definition (following, e.g., 9) provides the explicit definition of exact and approximate leverage scores for our setting.

**Definition 1** (\((_{1},_{2})\)-approximate leverage scores).: _Fix a matrix \(^{n d}\) and consider matrix \(^{n d}\) with orthonormal columns spanning the column space of \(\). Then, the leverage scores \(l_{i},1 i n\) are defined as the row norms squared of \(\), i.e., \(l_{i}=\|_{i}\|^{2}\), where \(_{i}^{}\) is the \(i\)th row of \(\). Furthermore, consider fixed \(_{1},_{2}>1\). Then \(_{i}\) are called \((_{1},_{2})\)-approximate leverage scores for \(\) if the following holds for all \(i\)_

\[}{_{1}}_{i}\;\;\;\;_{i=1}^{n} _{i}_{2} d.\]

The approximate leverage scores can be computed by first constructing a preconditioner matrix \(^{d d}\) such that \(()=O(1)\), which takes \(O(()+d^{})\) in a single pass, and then relying on the following norm approximation scheme.

**Lemma 1** (Based on Lemma 7.2 from ).: _Given \(^{n d}\) and \(^{d d}\), using a single pass over \(\) in time \(O(^{-1}(()+d^{2}))\) for small constant \(>0\), we can compute estimates \(_{1},...,_{n}\) such that with probability \( 0.95\):_

\[n^{-}\|e_{i}^{}\|^{2}_{i} O( (n))\|_{i}^{}\|^{2} i _{i}_{i} O(1)\|\|_{ F}^{2}.\]

In the next definition, we give the sparse sketching strategy used in our analysis. This approach is similar to the original leverage score sparsification proposed by , except: 1) we adapted it so that it can be implemented effectively in a single pass, and 2) we use it in a much sparser regime (fewer non-zeros per row).

**Definition 2** (\((s,_{1},_{2})\)-LESS embedding).: _Fix a matrix \(^{n d}\) and some \(s 0\). Let the tuple \((_{1},,_{n})\) denote \((_{1},_{2})\)-approximate leverage scores for \(\). Let \(p_{i}=\{1,_{i}}{d}\}\). We define a \((s,_{1},_{2})\)-approximate leverage score sparsifier \(\) as follows._

\[=(}{}},,}{}}) b_{i}(p_{i}).\]

_Moreover, we define the \((s,_{1},_{2})\)-leverage score sparsified (LESS) embedding of size \(m\) as matrix \(^{m d}\) with i.i.d. rows \(}_{i}\) such that \(_{i}=(_{i})_{i}\) where \(_{i}\) denotes a randomly generated \((_{1},_{2})\)-approximate leverage score sparsifier and \(_{i}^{n}\) consist of random \( 1\) entries._A key property of a sketching matrix is the subspace embedding property, defined below. It was recently shown by  that LESS embeddings require only polylogarithmically many non-zeros per row of \(\) to prove that \(\) is a subspace embedding for the data matrix \(\) with the optimal \(m=O(d)\) sketching dimension. For our main results, it is sufficient to use \(=O(1)\) below.

**Lemma 2** (Subspace embedding for LESS, Theorem 1.3, ).: _Fix \(,>0\). Consider \(_{1},_{2}>1\) and a full rank matrix \(^{n d}\). Then for a \((_{1},_{2})\)-leverage score sparsified embedding \(^{m n}\) with \(s O(^{4}(d/)/^{4})\) and \(m=((d+ 1/)/^{2})\), with probability \(1-\) we have_

\[^{}^{} ^{}(1+)^{} .\] (1)

## 4 Least Squares Bias Analysis

In this section we provide an outline of the bias analysis for the sketched least squares estimator constructed using a LESS embedding, leading to the proofs of our main results, Theorems 1 and 2. In particular, we prove the following main technical result (detailed proof in Appendix C).

**Theorem 4** (Bias of LESS-sketched least squares).: _Fix \(^{n d}\) and let \(\) be an \((s,_{1},_{2})\)-LESS embedding of size \(m\) for \(\). Let \(\) satisfy (1) with \(=\) and probability \(1-\) where \(<}\). Then there exists an event \(\) with probability at least \(1-\) such that_

\[L(_{}[}])-L(^{*})= (}(1+)^{9}(n/)) L (^{*}).\]

**Remark 3**.: _Thus, the bias \(L(_{}[}])-L(^{*})\) of the LESS estimator using \(O(_{1}_{2}s)=(s)\) non-zeros per row of \(\) is of the order \((}{sm^{2}}+}) L(^{*})\). By comparison, the standard expected loss bound which holds for sketched least squares (including this estimator) is \([L(})]-L(^{*})() L(^{*})\), and the best known bound on the bias of most standard sketched estimators (e.g., leverage score sampling) is \((}{m^{2}})L(^{*})\), given by . So, our result recovers the standard bias bound for \(s=1\) and improves on if for \(s 1\) by a factor of \(\{s,d\}\). At the end of the section, we discuss how to deal with the lower order term \((})\) to reduce the bias further._

**Proof sketch.** Using a standard argument, we can replace the matrix \(\) with the matrix \(^{n d}\) consisting of orthonormal columns spanning the column space of \(}\), and assume that \(n=(d)\). Let \(\) be an \((s,_{1},_{2})\)-LESS embedding for \(\). Also, let \(^{n}\) be a vector of responses/labels corresponding to \(n\) rows in \(\). Let \(}=_{}\| -\|^{2}\). Furthermore for any \(^{d}\) we can find the loss at \(\) as \(L()=\|-\|^{2}\). Additionally, we use \(\) to denote the residual \(-^{*}\). We also define \(=(^{}^{}^{})^{-1}\) as the sketched inverse covariance matrix with scaling \(=\) representing the standard correction accounting for inversion bias. We condition on the high probability event \(\) guaranteed in Lemma 2 and consider \(L(_{}(}))-L(^{*})\). By Pythagorean theorem, we have \(L(_{}[}])-L(^{*})=\| (_{}[}])-U^{*} \|^{2}\). Note that by the normal equations we have \(}=(^{}^{})^{-1} ^{}^{}=^{}^{}^{}\), and also \(^{}=_{i=1}^{m}_{i} _{i}^{}\). These two facts lead to writing the bias as follows:

\[L(_{}[}])-L(^{*})=\| _{}[^{}_{i} _{i}^{}]\|^{2}.\]

Using a leave-one-out technique, we replace \(\) with \(_{-i}=(^{}_{-i}^{}_{-i} )^{-1}\), where \(_{-i}\) denotes matrix \(\) without the \(i\)th row, by noting that \(=(^{}_{-i}^{}_{-i} +^{}_{i}_{i} _{i}^{})^{-1}\) and applying the Sherman-Morrison formula. This leads to the following relation:

\[L(_{}[}])-L(^{*}) 2_{}[_{-i}^{}_{i} _{i}^{}]\|^{2}}_{\|_{}[ _{-i}^{}]\|^{2}}+2_{}[(}-1)_{- i}^{}_{i}_{i}^{}]\|^{2}}_{\| _{}[]^{2}}\]

where \(_{i}=1+_{i}^{}_{-i} ^{}_{i}\). Due to the subspace embedding assumption and assuming \(m\) large enough, we have \(\|\|=O(1)\) and also \(\|_{-i}\|=O(1)\). The first term \(\|_{0}\|^{2}\) is quite straightforward to bound since, if not for the conditioning on the high probability event \(\), we would have \([_{-i}^{}_{i}_{i}^{} ]=[_{-i}^{}]=\), which follows from \(^{}(-^{*})=\). We get an upper bound on \(\|_{0}\|^{2}\) as \(O((d/)}{sm^{2}}+})\|\|^{2}\), which is sufficient for us.

The central novelty of our analysis lies in bounding \(\|_{2}\|^{2}\) for \((s,_{1},_{2})\)-LESS embeddings, which is the dominant term. Our key observation is that, when examining a random variable of the form \(_{i}^{}\) for some vector \(\), the dependence on the sparsity of row \(_{i}\) only arises when considering moments higher than \(2+\), because otherwise we can simply rely on the fact that \([_{i}_{i}^{}]=\). Thus, when decomposing \(\|_{2}\|^{2}\), we must carefully separate the contribution of near-second moments vs the contribution of higher moments to the overall bound.

To obtain this separation, we start by applying Holder's inequality on \(\|_{2}\|\) with \(p=O((n))\) and \(q=1+\) to get

\[\|_{2}\|(_{}[ }-1|^{p}])^{1/p}(_{\|\|=1}_{}[^{}_{-i} ^{}_{i}_{i}^{}]^{q} )^{1/q}.\]

Furthermore applying Cauchy-Schwarz inequality on the second term leads to

\[\|_{2}\|(_{}[ }-1|^{p}])^{1/p}(_{ }\|_{-i}^{}_{i}\|^{2q })^{1/2q}(_{}\|_{i}^{ }\|^{2q})^{1/2q}.\]

Unlike , we exploit the fact that \(\|_{i}\|^{1/O((n))}=O(1)\) and get a constant upper bound on \((_{}\|_{-i}^{}_{ i}\|^{2q})\). However, this results in a much more careful argument, requiring now an upper bound on \((_{}[|}-1|^{p}])^{1/p}\) for \(p=O((n))\). First, we observe that

\[(_{}[|}-1|^{p}]) ^{1/p}|-|+(_{}[ (_{i}-)^{p}])^{1/p}\] (2)

where \(=1+_{}(_{i}^{ }_{-i}^{}_{i})\). In particular, for the second term, we have

\[(_{}[(_{i}-)^{p}] )^{1/p}()[(_{ }[((_{-i})-_{i}^{}_{-i}^{}_{i})^{p}])^{1/p}+ (_{}[(_{-i})-_{ }(_{-i})]^{p})^{1/p}].\] (3)

To bound the first of these two terms, we prove a new version of the Restricted Bai-Silverstein inequality (Lemma 3) for \((s,_{1},_{2})\)-LESS embeddings. Unlike , we provide a proof with any \(p\) and any \((_{1},_{2})\) values. Furthermore, utilizing the subspace embedding guarantee from Lemma 2, we prove a much more general result where the number of non-zeros in the approximate leverage score sparsifier \(\) can be much smaller than \(d\) (proof in Appendix D).

**Lemma 3** (Restricted Bai-Silverstein for \((s,_{1},_{2})\)-LESS embeddings).: _Let \(p\) be fixed and \(^{n d}\) be such that \(^{}=\). Let \(_{i}=()_{i}\) where \(_{i}^{n}\) has independent \( 1\) entries and \(\) is an \((s,_{1},_{2})\)-approximate leverage score sparsifier for \(\). Then for any matrix with \(0(1)\) and any \(>0\) we have for an absolute constant \(c>0\)._

\[([()-_{i}^{} ^{}_{i}]^{p})^{1/p}<c p^{3}(1+}).\]

Using Lemma 3, we upper bound the first term squared in (3) as \((}(1+))\). Moreover, also using Lemma 3, we get a matching upper bound on \(|-|\). We then design a martingale concentration argument to prove a high probability upper bound on the last remaining term, \(|(_{-i})-_{}(_ {-i})|\), which implies the desired moment bound (proof in Appendix B), concluding the proof of Theorem 4.

**Lemma 4**.: _For given \(>0\) and matrix \(_{-i}\) we have with probability \(1-\):_

\[|(_{-i})-(_{-i})| c^{ }}^{4.5}(m/).\]

Completing the proof of Theorem 1.First, suppose that \( O((d)/d)\) so that the bias bound can be achieved from Theorem 4. Our implementation is mainly based on the online construction of approximate leverage scores, given the preconditioner \(\), using Lemma 1. Briefly, this construction proceeds by first sketching \(\) using a \(d O(1/)\) Gaussian matrix \(\) to produce the matrix \(}=\), and then, for each observed row \(_{i}\) of \(\), we compute \(_{i}=\|_{i}^{}}\|^{2}\). Assuming without loss of generality that \(d=(n)\) and adjusting \(\), the estimates satisfy \(_{1}_{2}=O( d^{})\).

Next, we sample the non-zero entries of \(\) corresponding to the observed row \(_{i}\), \(i\).e., the \(i\)-th column of \(\). Note that for this we only need to know the single leverage score estimate \(_{i}\). Crucially for our analysis, the entries of this column need to be sampled i.i.d., which can be done in time proportional to the number of non-zeros in that column by first sampling a corresponding Binomial distribution to determine how many non-zeros we need, then picking a random subset of that size, and then sampling the random \( 1\) values. Altogether, the cost of constructing the sketch is \(O(^{-1}()+_{1}_{2}sd^{2})=O(^{-1} ()+^{-1}d^{2+}(d))\) by setting \(s=O((d)/)\). Finally, once we construct the sketch, at the end of the pass we can run conjugate gradient preconditioned with \(\) on the sketched problem, which takes \(( d^{2})\).

We note that in the (somewhat artificial) regime where we require extremely small bias, i.e., \(=o((d)/d)\), the bound claimed in Theorem 1 can still be obtained, since in this case for small enough \(\) we have \(d^{2+}/=O(d^{}/)\) with \(<2.5\), so we can rely on direct leverage score sampling (which corresponds to \(s=1\)), and instead of maintaining the sketch, we compute the estimator \(}=(}^{}})^{-1}}^{}\) directly along the way. This involves performing a separate \(d d\) matrix multiplication after collecting each \(d\) leverage score samples, to gradually compute \(}^{}}\), and then inverting the matrix at the end. From Theorem 4, we see that it suffices to set sketch size \(m=(d/)\), which leads to the desired runtime.

Completing the proof of Theorem 2.Here, we use a slightly modified variant of Lemma 2, given as Theorem 1.4 in , which shows that using a single pass we can compute a sketch \(}\) in time \(O(()+d^{})\), which satisfies the subspace embedding property (1) with \(=\). Then, we can perform the QR decomposition \(}=\) and set \(=^{-1}\) in additional time \(O(d^{})\) to obtain the desired preconditioner. Next, we use Theorem 1 to construct \(q\) i.i.d. estimators \(}_{i}\) in a second parallel pass, and finally, the estimators are aggregated to compute \(}=_{i=1}^{q}}_{i}\), which satisfies \(\|}-\|^{2}1+ +O(1/q)\|^{*}-\|^{2}\). Applying Markov's inequality concludes the proof.

## 5 Experiments

In this section, we illustrate empirically how our results point to a practical free lunch phenomenon in distributed averaging of sketching-based estimators. As mentioned in Section 1, our construction from Theorem 1 essentially works by taking a subsample of the data and then mixing groups of those rows together to produce an even smaller sketch (see Figure 1). According to our theory, while the small sketch does not recover the same \(\)-small error as the larger subsample, it does recover an \(\)-small bias. Moreover, this happens without incurring any additional computational cost, as the cost of the sketching is proportional to the cost of simply reading the subsampled rows. This suggests that we can use sparse sketching to compress a data subsample down to a small size while retaining the least squares performance in a distributed averaging environment.

To verify this, we evaluate the effectiveness of distributed averaging of sketched least squares estimators on several benchmark datasets. Specifically, we visualize the relative error of the averaged sketch-and-solve estimator \(})-L(^{*})}{L(^{*})}\), against the number of machines \(q\) used to generate the estimate \(}=_{i=1}^{q}}_{i}\). Each estimate \(}_{i}\) is constructed with the same sparsification strategy used by LESS, except that instead of sparsifying the sketch with leverage scores, we instead sparsify them with uniform probabilities (which is often sufficient in practice). Following , we call the resulting method LESSUniform. Within each dataset, we perform four simulations, designed so that the total sketching cost stays the same for all four test cases, by simultaneously changing sketch size and sparsity. Concretely, we vary these so that the product (sketch size \(\) nnz per row) stays the same in each case, so as to ensure that the total cost of sketching is fixed in each plot.

In Figure 2, on the X-axis we plot the number \(q\) of estimators being averaged, so that the bias of a single estimator appears on the right-hand side of the plot (large \(q\)), whereas the variance (error) appears on the left-hand side (\(q=1\)). In each plot, the line with nnz per row \(=1\) (and large sketch size) corresponds to uniform subsampling, whereas the remaining ones are sketches produced by compressing that subsample. The plot shows that decreasing the sketch size (i.e., compressing the sample) does increase the error of a single estimator (as expected), however it also shows that the bias of these estimators remains essentially unchanged regardless of the sketch size (since all linesmeet as \(q\)), confirming that suitable sparse sketches that "compress" rows of data can preserve near-unbiasedness without increasing the cost.

This phenomenon may not occur for all sketching methods. Figure 4 within Appendix F showcases a few more interesting results. We first further demonstrate that suitable sketches that "compress" rows of data \(\) into a single row of \(}\) can preserve near-unbiasedness without increasing the cost. In particular, this desirable phenomenon that LESSUniform enjoys also extends to LESS proper, as well as the Gaussian and Subgaussian (Rademacher) sketches. In fact, we observe that LESS enjoys similar desirable performance as the Gaussian and Subgaussian sketches and virtually no least squares bias, while retaining the computational speedups of sparse sketching, suggesting that it attains the best of both worlds.

However, when we decrease the number of subsamples within leverage score subsampling, the bias introduced by subsampling increases as expected. This happens as the number of subsamples is reduced without increasing the amount of "compression" as one would with LESS or LESSUniform.1 We also show that the subsampled randomized Hadamard transform (SRHT) can exhibit some amount of least squares bias as the sketch size decreases. The numerical results shown for the bias introduced by leverage score subsampling and the SRHT complement the lower bounds established in .

## 6 Conclusions

We gave a new sparse sketching method that, using two passes over the data, produces a nearly-unbiased least squares estimator, which can be used to improve upon the space-time trade-offs of solving least squares in parallel or distributed environments via simple averaging. In particular, our algorithm is the first to require only \(O(d^{2}(nd))\) bits of space and current matrix multiplication time \(O(d^{})\) while obtaining an \(=o(1)\) approximation in few passes. Our techniques are of broader interest to sketching-based optimization algorithms, including Distributed Newton Sketch.