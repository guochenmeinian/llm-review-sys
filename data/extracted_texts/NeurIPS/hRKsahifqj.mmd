# Autoregressive Policy Optimization for Constrained Allocation Tasks

David Winkel1 Niklas Strauss1

Maximilian Bernhard Zongyue Li Thomas Seidl Matthias Schubert

Munich Center for Machine Learning, LMU Munich

{winkel,strauss,bernhard,li,seidl,schubert}@dbs.ifi.lmu.de

Both authors contributed equally.

###### Abstract

Allocation tasks represent a class of problems where a limited amount of resources must be allocated to a set of entities at each time step. Prominent examples of this task include portfolio optimization or distributing computational workloads across servers. Allocation tasks are typically bound by linear constraints describing practical requirements that have to be strictly fulfilled at all times. In portfolio optimization, for example, investors may be obligated to allocate less than 30% of the funds into a certain industrial sector in any investment period. Such constraints restrict the action space of allowed allocations in intricate ways, which makes learning a policy that avoids constraint violations difficult. In this paper, we propose a new method for constrained allocation tasks based on an autoregressive process to sequentially sample allocations for each entity. In addition, we introduce a novel de-biasing mechanism to counter the initial bias caused by sequential sampling. We demonstrate the superior performance of our approach compared to a variety of Constrained Reinforcement Learning (CRL) methods on three distinct constrained allocation tasks: portfolio optimization, computational workload distribution, and a synthetic allocation benchmark. Our code is available at: https://github.com/niklasdbs/paspo.

## 1 Introduction

Continuous allocation tasks are a class of problems where an agent needs to distribute a limited amount of resources over a set of entities at each time step. Many complex real-world problems are formulated as allocation tasks, and state-of-the-art solutions rely on using Reinforcement Learning (RL) to learn effective policies . Notable examples include portfolio allocation tasks, where portfolio managers must allocate the available financial resources among various assets , or allocation tasks of computational workloads to a set of compute instances in data centers . In many cases, allocation tasks come with allocation constraints , such as investing at most 30 % of the portfolio into a specific subset of the assets or to restrict the maximum workload to certain servers in a data center. Formally, allocation constraints are expressed as linear constraints and form a system of linear inequalities, geometrically describing a convex polytope. Each point in this polytope describes a possible allocation and each dimension corresponds to one of the entities. Allocation tasks often require hard constraints, i.e., constraints that are explicitly given and must be satisfied at any point in time. However, most of the existing CRL literature focuses on soft constraints that are not explicitly given . These approaches typically cannot guarantee constraint satisfaction and tend to have many constraint violations during training. The majority of these methods approximate the cumulative costs of constraint violations and optimize the cumulative reward while trying to adhere to the maximum cumulative costs. While less explored, there existseveral techniques that ensure the satisfaction of hard constraints [18; 6; 11; 10; 20]. These approaches might generate actions that do not satisfy the constraints but utilize a correction mechanism to map the actions back into the valid action space. In addition, most of these approaches are restricted to off-policy algorithms [11; 20]. In another line of research, solutions tailored for constrained allocation tasks have been proposed [27; 28]. However, these solutions are severely limited since they can only handle a specific subset of linear constraints and cannot handle more than two.

In this paper, we propose Polytope Action Space Policy Optimization (PASPO), a novel RL-based method that, firstly, decomposes the action space into several dependent sub-problems and, secondly, autoregressively computes the allocations step-by-step for each entity individually. In contrast to previous methods for hard constraints, we directly generate an action within the action space. This makes the correction of invalid actions unnecessary and, thus, avoids potential sampling bias introduced by the correction. Our new decomposition approach is implemented in a neural network-based policy function, which can be employed in on-policy and off-policy RL algorithms. We show that initialization bias can prevent proper exploration in early training which leads to premature convergence. Thus, we propose a de-biasing mechanism to stabilize exploration in early training stages.

We evaluate our approach against various baselines on three distinct allocation tasks: portfolio optimization, distributing computational workload in data centers, and a synthetic benchmark. These experiments demonstrate that our approach can outperform existing methods consistently and show the importance of the proposed de-biasing mechanism.

To summarize, the main contributions of our paper are:

* A new autoregressive stochastic policy function applicable to arbitrary convex polytope action spaces of constrained allocation tasks.
* A new de-biasing mechanism to prevent premature convergence to a sub-optimal policy.
* An empirical evaluation that optimizes our new policy function using PPO  and demonstrates improved results compared to state-of-the-art CRL methods.

The remainder of the paper is structured as follows: In Section 2, we provide an overview of the related work in CRL, constrained allocation tasks, and autoregressive policy functions. Afterward, we formalize constrained allocation tasks in Section 3 and present our novel approach in Section 4. Section 5 describes the results of our experimental evaluation, Section 6 briefly discusses limitations and future work before Section 7 concludes the paper.

## 2 Related Work

Resource allocation tasks are a widely researched area with numerous applications spanning logistics, power distribution, computational load balancing, security screening, and finance [6; 26; 3; 20; 27]. We identify three key research directions that are particularly important when discussing resource allocation tasks.

**Safe Reinforcement Learning** The majority of work in CRL addresses soft constraints, a setting often referred to as Safe RL. We will provide a brief overview of the most important methods in this field. For a more comprehensive examination of Safe RL, we direct readers to the survey papers by [15; 12]. A common technique in Safe RL is the use of Lagrangian relaxation [4; 15]. Several works employ primal-dual optimization to leverage the Lagrangian duality, including [9; 23; 13]. Another frequently used approach involves different penalty terms [25; 14; 31]. The authors of IPO  propose to use logarithmic barrier functions. CPO  extends TRPO  to ensure near-constraint satisfaction with each update. Additionally, two-step approaches such as FCOOPS  and CUP  are popular in the field. However, unlike our method, these approaches do not guarantee strict constraint satisfaction, particularly during training.

**Hard Constraints** Although less studied than Safe RL, several works address hard instantaneous constraints on actions to ensure full constraint satisfaction at any time step. Most of these approaches employ mechanisms to correct infeasible actions, i.e., those that violate constraints, into feasible actions [18; 6; 20; 11]. In contrast, our method always generates feasible actions without the need for correction. OptLayer  is one of the most prominent examples in this field, which employs OptNet  to map infeasible actions to the nearest feasible action. Similarly,  propose a more efficient projection on the polytope action space than OptLayer. The authors of  focus on resource allocation with hierarchical allocation constraints by proposing a faster approximate version of OptLayer. In , the authors propose an off-policy algorithm based on the generalized reduced gradient method  to handle non-linear hard constraints by projecting infeasible actions. In contrast, our method is not limited to off-policy algorithms.

In , the action space is decomposed into independent subspaces. However, these approaches can only handle up to two allocation constraints. Furthermore, they are only applicable to binary allocation constraints. In contrast, our approach can handle an arbitrary number of constraints as well as any type of linear allocation constraints.

**Action Space Decomposition/Factorization** The decomposition or factorization of multi-dimensional action spaces has been examined in several works . A notable example is , in which the authors discretize a continuous action space into several independent action branches, each parameterized by individual network branches. In , a variant of DQN  that discretizes a continuous action space into multiple discrete dimensions is proposed. These dimensions are sequentially parameterized, conditional on the previous sub-actions. Similarly,  propose an autoregressive factorization of an unconstrained action space into dependent sub-problems. Unlike our approach, these methods focus either on decomposing continuous action spaces into discrete action spaces or decomposing unconstrained action spaces. However, the decomposition of arbitrary convex polytope action spaces into tractable sub-action spaces remains a non-trivial challenge that our approach addresses.

## 3 Problem Description

An allocation task can be described as a finite-horizon Markov decision process (MDP) \((S,A,T,R,)\), where \(S\) represents the state space, \(A\) the action space, \(T:S A S\) the state transition function, \(R\) the reward function, and \(\) a discount factor. The goal of this task is to find a policy \(\) maximizing the expected cumulative reward \(J^{}_{R}=_{}[_{t=1}^{n}^{t}R(s_{t},(s_ {t}),s_{t+1})]\).

The action \(a\) is an allocation \(a=\{a_{1},,a_{n}\} A\) over a set of \(n\) entities \(E=\{e_{1},,e_{n}\}\) at each time step. Each element \(a_{i}\) of the action vector \(a\) represents the proportion allocated to entity \(e_{i}\). Furthermore, allocation tasks require a complete allocation, i. e., \(_{i=1}^{n}a_{i}=1\) and allocations cannot be negative (\(a_{i} 0\)). Thus, the action space of unconstrained allocation tasks forms an \(n\)-dimensional standard simplex. A visualization of an unconstrained allocation action space is provided in Figure 0(a).

Allocation tasks frequently include constraints, such as allocating at most 30% to a subset of the entities. An example of a constrained action space is visualized in Figure 0(b). Formally, an allocation constraint can be expressed as a linear inequality \(_{i=1}^{n}c_{i}a_{i} b\), where \(c_{i}\) denotes the weighting of the allocation variable \(a_{i}\) of entity \(e_{i}\) and \(b\) denotes the corresponding constraint limit. For the sake of readability and simplicity, we only define \(\) constraints since \(a b\) can be transformed into \(-a-b\) and \(a=b\) can be rewritten as \(a b\) and \(-a-b\).

The action space \(A\) of constrained allocation tasks can be easily expressed by a set of linear inequalities, defining a polytope \(A=\{a^{n}|Ca b\}\), where

\[C^{m n}=c_{11}&&c_{1n}\\ &&\\ c_{m1}&&c_{mn}\] (1)

is a matrix of coefficients for the \(m\) constraints, including those linked to the simplex constraints \(_{i=1}^{n}a_{i}=1\) and \(a_{i} 0\)\( i\{1, n\}\), as well as all coefficients for additional allocation constraints. Let \(a^{n}\) represent an allocation vector and \(b^{m}\) is the vector of constraint limits.

Alternatively, constrained allocation tasks can be defined using the framework of constrained Markov decision processes (CMDPs). A CMDP extends the standard MDP by a number of cost functions to incorporate the constraints. The goal is to maximize the expected cumulative reward while satisfying \(m\) constraints on the expected cumulative costs. The expected cumulative costs for the \(k\)-th cost function \(CF_{k}\) are defined as \(J^{}_{CF_{k}}=[_{t=0}^{T}^{t}CF_{k}(s_{t},a_{t})]\). The \(m\) constraints to be satisfied in the CMDP are then stated as \(J^{}_{CF_{k}} d_{k}\), where \(d_{k}\) denotes the cost limit with \(k\{1,,m\}\). To formulate constrained allocation tasks using CMDPs, the cost functions can be defined as \(CF_{k}(s,a)=\{0,(Ca)_{k}-b_{k}\}\) to measure any allocation constraint violation as a cost. In addition, strict adherence to all allocation constraints at any point in time is required, i.e., \(d_{k}=0\). By formulating constraint allocation tasks using CMDPs, it becomes possible to use existing methods from Safe RL for soft constraints. However, these methods cannot guarantee constraint satisfaction at all times . Let us note that our method does not use cost functions, instead it samples actions directly from the constrained action space.

## 4 Polytope Action Space Policy Optimization (PASPO)

Our approach PASPO autoregressively computes the allocation to every single entity in an iterative process until all allocations are fixed. We will later show that this step-wise decomposition allows for a tractable parametrization of the action space.

### Autoregressive Polytope Decomposition

PASPO starts by determining the feasible interval \([a_{1}^{min},a_{1}^{max}]\) for allocations into the first entity \(e_{1}\). Then, we sample the first allocation \(a_{1}\) from this interval. The details of the sampling process will be further discussed in Section 4.2. Fixing an allocation impacts the shape of the remaining action space. Thus, we have to compute the shape of the polytope \(A^{(2)}\) described by \(C^{(2)}\) and \(b^{(2)}\) before we can sample the next allocation \(a_{2}\).

Each iteration \(i\) starts with determining the interval \([a_{i}^{min},a_{i}^{max}]\) of all feasible values for \(a_{i}\). Geometrically, this interval is bounded by the minimum and the maximum value of the remaining polytope \(A^{(i)}\) in the \(i\)-th dimension associated with the allocation \(a_{i}\). To determine \(a_{i}^{min}\), we solve the following linear program:

\[& a_{i}\\ & C^{(i)}a^{(i)} b^{(i)}\]

where \(C^{(i)}\) are the constraint coefficients for the entities \(e_{i},,e_{n}\), \(b^{(i)}\) are the adjusted constraint limits, and \(a^{(i)}\) describes the unfixed allocations. We determine \(a_{i}^{max}\) by solving the respective maximization problem. For the first iteration \(i=1\), we define \(C^{(1)}=C\), \(b^{(1)}=b\) and \(a^{(1)}=a\). After sampling an allocation \(a_{i}\) from the interval \([a_{i}^{min},a_{i}^{max}]\). The resulting polytope \(A^{(i+1)}\) for the next iteration \(i+1\) is described by the following inequality system:

\[}&&c_{1,n}\\ &&\\ }&&c_{m,n}]}_{C^{(i+1)}}a_{i+1}\\ \\ a_{n}]}_{a^{(i+1)}} b_{1}^{(i)}\\ \\ b_{m}^{(i)}]-a_{i}[}\\ \\ }]}_{b^{(i+1)}}\] (2)

To define the new coefficient matrix \(C^{(i+1)}\) (red), we remove the first column of the coefficient matrix of the previous iteration \(C^{(i)}\). To calculate the new vector \(b^{(i+1)}\) of constraint limits, we subtract

Figure 1: **Examples of 3-dimensional allocation action spaces (a) unconstrained and (b) constrained (valid solutions as red area).**

the removed column (blue) scaled by the fixed allocation \(a_{i}\) from the previous constraint limits \(b^{(i)}\) (yellow). We iterate over all entities until we determine \(a_{n-1}\). Allocation \(a_{n}\) is already determined as soon as the allocations \(a_{1},,a_{n-1}\) are fixed because of the simplex constraint \(_{i=1}^{n}a_{i}=1\). Sampling an allocation using this approach always guarantees constraint satisfaction and it is possible to sample any action in the constrained action space. A formal proof of these guarantees can be found in Appendix D.

Figure 2 displays a visualization of the process for a 3-dimensional case. The set of valid solutions before any allocations have been fixed is shown in Figure 1(a). Figure 1(b) depicts the first iteration after \(a_{1}=0.3\) has been determined, and the resulting new polytope \(A^{(2)}\), i.e., a set of valid solutions, shrinks to the red line is shown. Figure 1(c) shows the second iteration after also \(a_{2}=0.5\) has been determined. It can be seen that the new polytope \(A^{(3)}\) contains only a single valid solution represented as a red dot, making a third iteration unnecessary since the only remaining solution is to allocate \(a_{3}=0.2\), resulting in a final allocation of \(a=(0.3,0.5,0.2)\).

### Parameterizable Policy Process

Our goal is to define a learnable stochastic policy function over the action space. For unconstrained allocation tasks, a Dirichlet distribution can be used to parameterize the action space [26; 28]. Unfortunately, to the best of our knowledge, there is no known parameterizable, closed-form distribution function over arbitrary convex polytopes as in our setting. In fact, even uniform sampling over a convex polytope is an active research problem .

We sequentially constructed an action \(a\) from the polytope action space \(A\) in the previous section. Now, we describe how to utilize this process to define a parameterizable policy function over the action space \(A\). We model the distribution for allocating each individual entity using a beta distribution that is normalized to the range \([a_{i}^{min},a_{i}^{max}]\). This distribution is also known as the four-parameter beta distribution . Its probability density function is defined as:

\[p(x;,,a_{i}^{min},a_{i}^{max})=^{min})^{-1}(a_{ i}^{max}-x)^{-1}}{(a_{i}^{max}-a_{i}^{min})^{+-1}B(, )},\]

where \(B(,)\) is the beta function. It is important to note that any other parameterizable distributions with bounded support in the range \([a_{i}^{min},a_{i}^{max}]\) can be used, such as a squashed Gaussian distribution. However, our preliminary experiments indicated that the beta distribution performs particularly well.

To optimize the policy \(_{}(s)\) over the complete allocations, we follow the approach of  for training an autoregressively dependent series of sub-policies. A fixed but arbitrary order of entities is used for sampling the allocations \(a_{i}\). The sub-policy \(_{}^{i}(a_{i}|a_{1},,a_{i-1})\) is conditional on the previous allocations \(a_{1},,a_{i-1}\). Using this autoregressive dependence structure, the policy is defined as: \(_{}(a|s)=_{}^{1}(a_{1}|s)_{}^{2}(a_{2})|s,a_{ 1})_{}^{n}(a_{n}|s,a_{1},,a_{n-1})\). This policy can be jointly optimized. We parameterize each sub-policy using a neural network that receives an embedding of the state and the previously selected actions as input.

Figure 2: **Example of sampling process of an action \(a=(a_{1},a_{2},a_{3})\) in a 3-dimensional constrained allocation task.**

An entropy term is often used to encourage exploration. However, our policy does not have a closed-form solution for entropy. Therefore, we follow  to empirically estimate the entropy:

\[H_{}(_{}(|s))=_{a_{}(|s)} [_{i=1}^{n}H(_{}^{i}(|s,a_{1},,a_{i-1} ))]\] (3)

Here, \(H\) denotes the entropy of the beta distribution. We compute the expectation within each training batch to estimate the entropy of the complete policy function over the entropies of single actions. Let us note that when using an off-policy algorithm, the actions must be resampled using the current policy. As the current policy might have a significantly different parametrization than the sampling policy, we have to generate actions based on the current policy to estimate the entropy properly.

### Policy Network Architecture

We create an embedding of the state using an MLP, denoted as \(f_{}(s)=MLP(s)\). We parameterize the probability distribution over allocations for each entity using an MLP \(_{_{i}}^{i}(s)=MLP(f_{}(s),a_{1},,a_{i-1})\), which receives the latent encoding of the state and the previously sampled allocations \(a_{1},,a_{i-1}\) as input. Note that each of the MLPs \(_{_{i}}^{i}\) has its own parameters. For further details, we refer to the Appendix.

### De-biasing Mechanism

A drawback of generating actions by an autoregressive process is that a random initialization of the beta distributions leads to a sampling bias towards the entities selected earlier in the process. The

Figure 3: **The impact of initialization in an unconstrained simplex.** (a) Mean allocations \(a_{i}\) to each entity in a seven entity setup when sampling each individual allocation using the uniform distribution (red) vs. our initialization (blue). (b,c) Distribution of 2500 allocations in a three entity setup when sampling each individual allocation uniformly (b) or using beta distributions with parameters set according to our initialization (c).

effect is caused by the autoregressive dependency structure of our process. To sample an allocation of \(80\%\) for entity \(a_{i}\) the cumulated fixed allocations for earlier entities \(_{j=1}^{i-1}a_{j}\) must be at most \(20\%\). However, this is rather unlikely if we initialize the distribution for all entities in a similar way. This effect can be observed in the red bars of Figure 2(a). The red bars correspond to the average allocation for each dimension when uniformly drawing from an unconstrained seven-dimensional simplex with our autoregressive process for each entity \(e_{i}\). As expected, the mean for the first dimension is \(0.5\), which is the mean of a uniform distribution over the interval \(\). Correspondingly, the mean is decreased by half for any successive further entity until entity 6, which has the same mean as entity 7 due to the simplex constraint. Even though the bias is more complex for constrained allocation spaces, a similar effect can be expected.

For policy gradient methods, such a bias in the initialization of the policy function can lead to convergence to poorly performing policies or long training times. As the initial policy is crucial for ensuring sufficient exploration of the state-action space, a biased initial distribution leads to underexplored regions in the state-action space. Consequently, well-performing actions might not be discovered. To counter this effect, we propose a de-biasing mechanism that adjusts the initial parameters of beta distributions to estimate a uniform sampling over the joint action space. During learning, the amount of required exploration decreases, and the parameters of the policy function are optimized to increase the cumulative rewards. Thus, the impact of our de-biasing mechanism should diminish over time. We achieve this effect by adding a de-biasing term to the linear layers' initial bias terms, predicting \(_{i}\) and \(_{i}\) for entity \(i\). As the default initialization of the bias terms has zero means, the first iterations use \(\) and \(\) values close to the de-biasing terms.

To determine suitable initial values for each iteration step, we proceed as described in Algorithm 1. We start by uniformly sampling \(n\) data points from the complete action space \(A\). We do this by rejection sampling, i.e., we sample over the standard simplex and reject the samples outside the action polytope \(A\). To determine the parameters corresponding to the acquired uniform sample, we project any allocation for each entity to a standard interval between \(\). However, for this step, we have to determine the interval \([a_{i}^{min},a_{i}^{max}]\) for each entity following the above process. We determine the relative position in this interval, corresponding to the position in the named standard interval. After collecting relative values for each sample and entity, we employ the standard maximum likelihood estimator to generate an empirical estimate of the \(_{i}\) and \(_{i}\) for each entity \(e_{i}\). The blue bars in Figure 2(a) correspond to the results on the unconstrained seven-dimensional unconstrained simplex. Figure 2(b) shows autoregressive sampling based on uniform distribution, whereas Figure 2(c) displays the result of our initialization for a three-dimensional example. It can be seen that the result of our initialization of the autoregressive process closely resembles a uniform distribution over the complete action space.

## 5 Experiments

In this section, we provide an extensive experimental evaluation of our approach in various scenarios demonstrating its ability to handle various allocation tasks and constraints. We use two real-world tasks: Portfolio optimization  and compute load distribution . Additionally, we create a synthetic benchmark with a reward surface generated by a randomly initialized MLP. Each of these tasks comes with a different set of allocation constraints. We will briefly describe each setting in the following and refer the reader to the Appendix for more details.

**Portfolio Optimization** Portfolio optimization is a prominent constrained allocation task. In this task the agent has to allocate its wealth over 13 assets at each time step. We use the environment of . Each investment period contains 12 months and the investor needs to reallocate the portfolio each month. This environment is highly stochastic since each trajectory is sampled from a hidden Markov model fitted on real-world NASDAQ-100 data. After every 5120 environment steps, we run eight parallel evaluations on 200 fixed trajectories. Constraints in this setting define minimum and maximum allocation to groups of assets. Additionally, we add constraints where the constraint coefficients in \(C\) correspond to portfolio measures like a minimum dividend yield or a maximum on the CO2 intensity.

**Compute Load Distribution** The environment is based on the paper of  and simulates a data center in which computational jobs need to be split into sub-jobs to enable parallel processing across nine servers. Here, we use five constraints that are randomly sampled as follows: First, we sample the number of affected entities for each constraint. We then sample the constraint coefficients from the range \(\).

**Synthetic Environment** In addition to the aforementioned environments, we propose a synthetic benchmark. The reward surface consists of an MLP with random weights. Each episode compromises two states. As it is completely deterministic, it provides a simple yet effective way to benchmark approaches for constrained allocation tasks. In this setting, we create the constraints by randomly sampling 30 points and use their convex hull as the polytope defining the action space. We utilize a seven-dimensional setting with 611 constraints in our experiments.

### Experimental Setup

We train PASPO using PPO  and compare our approach to various baselines, including state-of-the-art approaches for constrained allocation tasks and Safe RL. Specifically, we compare PASPO with five representative approaches from Safe RL: CPO , CUP , IPO , P3O , and PPO with Lagrangian relaxation. Additionally, we compare our method to OptLayer , a popular projection-based method for linear hard constraints. To maintain a consistent and fair comparison across different methods, we use the same hyperparameters across the different methods if possible. Many Safe RL approaches have difficulties handling equality constraints . Therefore, we use a Dirichlet distribution to represent the policy in the baselines, thereby ensuring satisfaction of the simplex equality constraint. We do not share the parameters between the policy and value function. We use a fully-connected MLP with two hidden layers of 32 units and ReLU non-linearities for each policy, cost, and value function. In our approach, the state encoder and each policy head consists of a two-layer MLP. The training process is run for 150,000 steps and the results are averaged over five different seeds. In the portfolio optimization task, we use ten different seeds due to the stochasticity of the financial environment and train for 250,000 steps. Given the relatively small network sizes, training is conducted exclusively on CPUs. We implement our algorithm and the baselines using RLlib and PyTorch. More details regarding the environments, training, and hyperparameters can be found in the Appendix.

#### 5.1.1 Performance of PASPO

We visualize the performance and constraint violations of all methods across our three environments in Figure 4. A tolerance of \(1e^{-3}\) is used for evaluating constraint violations and we report the total number of violations per episode. In all three environments, PASPO converges faster to a higher

Figure 4: **Learning curves of all methods in three environments. The x-axis corresponds to the number of environment steps. The y-axis is the average episode reward (first row), and the number of constraint violations during every epoch (second row). For portfolio optimization (b) we report the performance running eight evaluation on 200 fixed market trajectories. This is because in training, every trajectory is different which makes comparisons hard. Curves smoothed for visualization.**

average return compared to baselines. Additionally, while all compared soft-constraint methods display constraint violations, only the hard constraint approaches PASPO and OptLayer guarantee to permanently satisfy the constraints. Finally, we can observe that the variance of PASPO is rather low compared to other methods. However, in portfolio optimization task (b) our approach displays some variance which we attribute to the stochasticity of the environment. Overall, these results demonstrate that our approach is not only able to consistently outperform other algorithms in terms of rewards but also guarantees no constraint violations.

#### 5.1.2 Importance of de-biased Initialization and Order

We conduct ablation studies to investigate the impact of our de-biased initialization and the order of entity allocation on our synthetic benchmark. No constraints are applied except for the simplex constraint to highlight the effects. The results, shown in Figure 5, indicate that without de-biased initialization (orange in (a)), learning is slower and converges prematurely to a sub-optimal policy. In (b), we explore the impact of allocation order by reversing it (red) and observe no significant performance difference. This indicates that our approach is robust to the allocation order due to the use of the de-biasing initialization.

## 6 Limitations and Future Work

While PASPO guarantees that constraints are always satisfied, it is considerably more computationally expensive than standard neural networks in allocation tasks with many entities, as the sampling of each action requires solving a series of linear programs. RL in high-dimensional continuous action spaces is a very challenging task. Our approach cannot overcome this issue and also struggles in very high-dimensional settings. For future work, we plan to extend PASPO to also incorporate state-dependent constraints. While we evaluate our approach only on benchmarks with hard constraints, it can be applied to settings with both hard and soft cumulative constraints. In these scenarios, our method for handling hard constraints can be easily combined with most Safe RL algorithms to handle soft cumulative constraints.

## 7 Conclusion

In this paper, we examine allocation tasks where a certain amount of a resource has to be distributed over a set of entities at every step. This problem has many applications like logistics tasks, portfolio management, and computational workload processing in distributed environments. In all these applications, the set of feasible allocations might be bound by a set of linear constraints. Formally, these restrict the action space to a convex polytope. To define a stochastic policy function that can be used with policy gradient methods in RL, we propose an autoregressive process that computes allocation sequentially. We employ linear programming to compute the range of feasible allocations for an entity given the already fixed allocations of other entities. Our policy function consists of a sequence of one-dimensional beta distributions where the shape parameters \(\) and \(\) are learned by neural networks. To counter the effect of initialization bias, we utilize a de-biasing mechanism to ensure sufficient exploration and prevent premature convergence to a sub-optimal policy. In our

Figure 5: **Ablations** in (a) show the performance of our approach with (blue) and without (orange) the de-biased initialization. In (b) depicts the impact of the allocation order. We reverse the allocation order (red).

experiments, we demonstrate that our novel method PASPO yields better results than state-of-the-art approaches while not having any constraint violations. Furthermore, we show that our initialization method yields better results than random initializations and counters the impact of the allocation order.