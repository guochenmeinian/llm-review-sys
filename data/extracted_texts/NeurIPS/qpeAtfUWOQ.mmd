# Variational Multi-scale Representation for Estimating Uncertainty in 3D Gaussian Splatting

Ruiqi Li, Yiu-ming Cheung

Department of Computer Science, Hong Kong Baptist University

{csrqli, ymc}@comp.hkbu.edu.hk

Corresponding author is Yiu-ming Cheung (ymc@comp.hkbu.edu.hk).

###### Abstract

Recently, 3D Gaussian Splatting (3DGS) has become popular in reconstructing dense 3D representations of appearance and geometry. However, the learning pipeline in 3DGS inherently lacks the ability to quantify uncertainty, which is an important factor in applications like robotics mapping and navigation. In this paper, we propose an uncertainty estimation method built upon the Bayesian inference framework. Specifically, we propose a method to build variational multi-scale 3D Gaussians, where we leverage explicit scale information in 3DGS parameters to construct diversified parameter space samples. We develop an offset table technique to draw local multi-scale samples efficiently by offsetting selected attributes and sharing other base attributes. Then, the offset table is learned by variational inference with multi-scale prior. The learned offset posterior can quantify the uncertainty of each individual Gaussian component, and be used in the forward pass to infer the predictive uncertainty. Extensive experimental results on various benchmark datasets show that the proposed method provides well-aligned calibration performance on estimated uncertainty and better rendering quality compared with the previous methods that enable uncertainty quantification with view synthesis. Besides, by leveraging the model parameter uncertainty estimated by our method, we can remove noisy Gaussians automatically, thereby obtaining a high-fidelity part of the reconstructed scene, which is of great help in improving the visual quality. 1

## 1 Introduction

The radiance field methods  for view synthesis have received increasing attention in the past few years due to their capability of achieving photorealistic results. Among them, the recently proposed 3D Gaussian Splatting (3DGS) algorithm  has pushed the boundary of real-time view synthesis with prominent quality and efficiency. However, a major functionality deficiency of 3DGS is that it cannot provide uncertainty information regarding the reconstructed model and predictions. Such uncertainty information would be useful in removing the noisy components in the model and providing confidence maps to assess the quality view synthesis results, which is important in applications such as autonomous driving simulation and robotics navigation.

Previous works in view synthesis made attempts to quantify the uncertainty in Neural Radiance Field (NeRF) models via ensemble, variational inference or Laplace's approximation methods . Although these works demonstrate the ability to quantify the uncertainty with NeRF models, they cannot be directly applied to 3DGS models, due to the intrinsic difference between the implicit NeRF representation and explicit 3DGS. Furthermore, some prior works deal with the uncertainty in learning models with methods such as Monte-Carlo dropout , Deep Ensemble  and Subnetwork. A major characteristic of these methods is that they can be seen as the approximation of Bayesian Inference, which estimates the distribution of posterior \(p(|)\) and prediction \(p(y|,x)\), instead of point estimation.

The methods mentioned above can be seen as inferring the posterior distribution using model space samples. As mentioned in previous works [8; 10; 11], the key objectives for good model space samples are diversity and efficiency. To improve the quality of uncertainty estimation by increasing the diversity, the generated model space samples should explore the potential cases of true posterior as much as possible, which was not achieved in previous work on NeRF uncertainty estimation. Furthermore, although naive methods like ensemble provide good approximations, they require large storage and the computational cost which increase linearly with the number of samples. Thus, on top of diversity, efficiency is another important aspect we focus on when estimating the uncertainty for 3DGS, which means that we should use as few samples of parameters as possible.

In this paper, we design an uncertainty estimation method for 3DGS by constructing diversified parameter space samples efficiently. Inspired by the Level-of-Detail (LoD) technique used in computer graphics [12; 13] and multi-scale representation widely used in computer vision [14; 15], we propose to investigate the potential of scale information in fitting the scene representation with diversified samples. Specifically, we design a multi-scale variational inference method for 3DGS, which increases the diversity of parameter samples by enforcing them to model local spatial areas with multiple scales. To reduce the number of extra parameters, we design a mechanism that spawns finer multi-scale Gaussians from the base Gaussian, which are heavily involved in the rendering process. Instead of creating actual new Gaussians, we maintain an offset table parameterizing only a subset of attributes and share the remainder with base Gaussian. This can further reduce the number of parameter samples by maintaining only the attributes that contribute to our multi-scale representation.

Extensive experiments demonstrate the remarkable performance of our method on uncertainty estimation without affecting the rendering quality. We also show that our method provides both accurate posterior and predictive uncertainty estimation. The posterior uncertainty can be utilized directly to remove noisy Gaussians, thanks to the explicit benefit of the 3DGS method. The predictive uncertainty can serve as a confidence map to interpret the synthesized novel views.

The main contributions of our work are summarized as follows:

* We propose a multi-scale variational inference framework for uncertainty-aware view synthesis with the 3D Gaussian Splatting algorithm.
* We develop a spawning strategy to create multi-scale representations for 3DGS, and increase the sample diversity and inference efficiency by maintaining an offset table and sharing parameters.

Figure 1: The results of cleaning up an unbounded scene reconstructed with 3DGS using our uncertainty estimation. We remove the Gaussians with large parameter uncertainty, the majority of which are under-reconstructed background. The desk at the center of the scene remains complete even after removing 90% of the Gaussians.

* We evaluate the accuracy of uncertainty estimation on various benchmarks, and demonstrate the application of the parameter uncertainty in cleaning up noisy components in the scene.

## 2 Related Work

### Uncertainty Quantification

Providing uncertainty together with predictions of learning models is of great use such as interpreting the model output [16; 17] or providing guidance in active data collection [18; 19]. This goal can be achieved by Bayesian learning, which characterizes the predictive distribution and posterior distribution of model parameters with theoretical foundations . One can use Hamiltonian Monte Carlo sampling for Bayesian learning in neural networks , which guarantees asymptotic performance. Previous works proposed other approximation methods such as Laplace's approximation . These methods consider the correlations between parameters, and are known for not requiring further assumption while reducing computations.

More recent works borrow from regularization techniques such as the dropout method as an approximated Bayesian inference, such as Monte-Carlo DropOut (MCDO) , Concrete Dropout  and Variational Dropout . Another line of work builds model ensembles from various subsets of data [8; 16], hyperparameters  or multiple subnetworks [9; 26] to infer the uncertainty. Some methods also introduce network modulation to form a model ensemble, among which BatchEnsemble  learns multiple low-rank weight matrices. Turkoglu et al.  introduced a set of linear modulation parameters. Other works focus on extra network components, such as Variational AutoEncoders (VAEs) methods  or auxiliary network .

The commonality of the above methods is that they introduce randomness into the model and draw model space samples. Furthermore, the importance of the diversity of model space samples in approximated Bayesian learning is demonstrated in [11; 31]. Unlike the implicit characteristics of neural networks, the parameters of 3DGS have physical meanings that we can handle explicitly.

For algorithms in the multi-view geometry in computer vision, the uncertainty of camera pose can be estimated from the covariance matrix of the transformation matrix using Monte Carlo methods . In the Simultaneous Localization and Mapping (SLAM) system, the uncertainty can be estimated from Kalman filter . Our method can be aggregated to a dense SLAM system to provide uncertainty information for optimizing camera pose together with the map.

### View Synthesis and Radiance Field

Recently, the radiance field has become popular in 3D vision, which achieves great success in generating photo-realistic images of novel view directions from a set of calibrated images. Early research

Figure 2: The comparison between our multi-scale variational inference and other methods. (a) Laplaceâ€™s Approximation fits posterior with normal distribution where the mean equals maximum a posteriori solution \(_{MAP}\) and precision equals fisher information \(I()\). (b) The ensemble method learns multiple models simultaneously to form the model space samples. (c) Our method builds a multi-scale representation of the scene, where inference is done by sampling the offset distribution and forming finer Gaussians.

developed light field  and multiplane images based  methods for this task. Building upon previous research on implicit representations [36; 37; 38; 39], NeRF and its variants achieve a successful framework combining volume rendering and positional encoding to synthesize photorealistic novel views [1; 40; 41]. The recently proposed 3DGS  explicitly represents the 3D scene with points associated with the Gaussian function. Some works explore potential solutions to the anti-aliasing rendering problem, for example, Turki et al.  developed a multi-resolution feature grid, and Yan et al.  designed a multi-scale Gaussian with level-of-detail. In our work, we propose a variational multi-scale representation for 3DGS by spreading only local deviations to form multi-scale samples of model parameters.

Some previous works focus on uncertainty estimation for NeRF models. Stochastic NeRF  addresses the uncertainty estimation in NeRF models via standard variational inference technique. NeRF Ensembles  leverages neural network ensemble technique to quantify the predictive uncertainty. CF-NeRF  models the predictive distribution via learning a conditional normalizing flow. ActiveRMAP  focuses on the active reconstruction task, and models the predictive uncertainty via entropy of ray density distribution. NeurAR and ActiveNeRF [45; 19] adopt the neural network to output uncertainty values for each pixel and learning with Negative Log-Likelihood loss. ProbNeRF  designs a learned variational autoencoder that can generate 3D models from 2D images and uses the Monte Carlo method at inference to provide predictive uncertainty. Bayes' Ray  performs a post-hoc Laplace's approximation for NeRF to quantify the uncertainty via applying perturbations to spatial points. Compared to the NeRF models, the uncertainty estimation of 3DGS is less discussed. FisherRF  designs an uncertainty quantification method for 3DGS and applies it to active learning, which adopts Laplace's approximation to compute the uncertainty from Fisher Information. However, the approximation of posterior uncertainty needs extensive computations of gradient aggregating over the whole training views. In our work, we leverage variational inference to approximate the posterior distribution of parameters.

## 3 Proposed Method

### Preliminaries: Uncertainty Quantification for 3DGS

The goal of the view synthesis problem is to generate images from any viewpoint given a set of calibrated input images. NeRF  proposes to solve this problem by representing the geometry and appearance of the scene using a learned radiance field \(f(,)(,)\), where \(\) and \(\) represent a spatial point and view direction, \(\) and \(\) represent the corresponding color and opacity of that point viewing from \(\). Based on this radiance field representation, 3DGS algorithm  further proposes

Figure 3: The pipeline of our variational multi-scale representation. We spawn base Gaussians, which are the major components in the scene, into multi-scale finer Gaussians. We learn an offset table to perform the spawn operation by offsetting a subset of attributes. The offset table is learned with variational inference with multi-scale prior. The predictive and parameter uncertainty can be inferred from the variational parameters stored in the table.

that the values in the field can be explicitly stored in a set of ellipsoids parameterized by the Gaussian function. The radiance value of each point in the scene \(\) is queried from its adjacent ellipsoid from the Gaussian function:

\[()=(-(-) ^{}^{-1}(-)),\] (1)

where \(\) and \(\) are the center and covariance matrix of the ellipsoid. The covariance is further decomposed to rotation \(R\) and scale \(S\) by \(=RSS^{T}R^{T}\). We referred to each ellipsoid as Gaussian \(\). In the rendering process, each Gaussian component is projected to the image space and transformed into its 2D projections. After that, they are accumulated via alpha blending to form pixel values \(\) in the rendered image:

\[=_{n=1}^{N}_{n}_{n}_{n}() _{j=1}^{n-1}(1-_{j}_{j}()).\] (2)

Note that the rendering process is deterministic. For each Gaussian \(\), its learnable parameters \(\) are composed of position, color, density \(\), and covariance: \(=\{,,,S,R\}\). The scale \(S\) and rotation \(R\) are learned respectively and form the covariance matrix \(\) in rendering.

Standard 3DGS algorithm applies a non-Bayesian approach to train the model with \(_{1}\) loss function, which can be seen as Maximum Likelihood Estimation (MLE) with the error following Laplace distribution . However, this only performs point estimation which lacks the ability to quantify predictive uncertainty and provide confidence information.

Previous methods in Bayesian learning provide tools like variational inference and ensemble methods for estimating the predictive uncertainty of models. For example, with ensemble methods, we can train multiple 3DGS models with different data subsets, random seeds or hyperparameters, and compute the variance of their output as the predictive uncertainty. With variational inference methods, the model posterior \(p(|)\) is approximated by a tractable variational distribution \(q()\), where \(\) is the training data. Then, the likelihood of pixel color \(p(|x,)\) at pixel \(x\) together with the discrepancy between \(p(|)\) and \(q()\) is optimized.

### Local Multi-scale 3D Gaussian

As discussed in previous works [8; 10; 11], the effectiveness of approximation methods in Bayesian inference highly depends on the diversity of model parameter space samples. With the explicit attributes of 3DGS parameters, we can manipulate the diversity of parameter samples and explore extensively the model parameter space. With more formations of the model representing the scene explored during learning, we can achieve better approximations in inferring the parameter posterior distribution. In the following, we will introduce a practical approach to building a representation with such diversification ability.

Local Multi-scale Representation.Different from NeRF-based scene representation where the model parameters are the neural network parameters with no explicit meaning, 3D Gaussian models the local area of the spatial scene with attributes describing the geometry and appearance. Therefore, we can perform heterogeneous operations for Gaussian attributes by diversifying the scale of the local Gaussian to increase the performance in approximations. In our local multi-scale 3D Gaussian, we propose to learn scene representations with multiple Gaussian scales to represent a local spatial area. Specifically, we first select Gaussians that contribute more to the representation of the scene, and draw new multi-scale samples that are attached to these Gaussians. The parameters of the new Gaussians are shared or learned with variational inference. Through this strategy, we are able to control the scale variance and achieve diversified model space samples \(\) in our uncertainty estimation.

Spawn from Base Gaussians.The pipeline for spawning local multi-scale 3D Gaussians is illustrated in Figure 3. For every fixed step in training, we perform a spawn operation for Gaussians. Specifically, we would like to select the Gaussians with larger scales and more contributions to the scene representation. We refer to them as _base Gaussians_, which is found by selecting Gaussians whose scale and opacity are above certain thresholds in the meantime. These base Gaussians are of greater contributions in the scene representation, which can be replaced by a set of Gaussians with various scales alternatively. Thresholding the gradient magnitude filters out trivial Gaussians such as those located in the distant background where they can hardly be seen.

Learn Finer Gaussians via Offset Table. After that, we spawn multi-scale finer Gaussians locally on top of base Gaussians. Instead of creating actual new Gaussians, to reduce the computational cost, we create a table \(\) to store the parameters of offset distribution for \(K\) finer Gaussians. We find that some of the attributes of finer Gaussians can be shared as the same as base Gaussians with losing representation ability. We select position \(\), scale \(S\) and opacity \(\) as the attributes maintained in the learned offset table, and the color \(\) and rotation \(R\) are shared by the base Gaussian associated. At inference, we apply the sampled offset to the base Gaussian to get the finer Gaussian. For example, the new position \(}\) and scale \(S^{*}\) after offset are:

\[}=+_{};\ \ \ \ \ S^{*}=S+_{S},\] (3)

where \(_{}\) and \(_{S}\) represent position and scale offset values sampled from the offset distribution parameterized by \(}\). There are total \(K\) entries in the offset table \(\) while only \(M\) of them are selected randomly with equal probability and averaged to get the final offset distribution parameters \(}=_{m}^{M}(i_{m})\), where \(i\) represents the index for the selected entries. By doing so, we create a subset of finer Gaussians with multiple scales and select a random mixture of their attribute distribution parameters each time. Therefore, we build a random scale alternative to the original large and significant base Gaussians. For the base Gaussians, the densification, splitting, and cloning operations are performed identically to the convention in , and these operations are also performed for the offset table. We will introduce how to learn the offset table with variance in scale using variational inference in the following.

### Infer the Posterior of the Offset Table

The offset table to learn contains entries for \(K\) finer Gaussians, and the inference process introduces randomness in selecting the \(M\) entries that form the final offset distribution parameters. After obtaining these parameters, we perform variational inference for the offset distribution that enforces the finer Gaussians to be multi-scaled by assigning the prior distribution for the offset. We can infer the uncertainty from the distribution of \(^{*}\), the parameter after applying the offset. Specifically, to infer the posterior and learn a multi-scale representation, we let \(q()\) be the variational distribution for offset to approximate the true offset posterior distribution \(p(|)\) and minimize the Kullback-Leibler divergence between them:

\[d_{KL}[q()\|p()] :=_{}q())}\] (4) \[=-_{ q()}[ p(,)-  q()]+ p(),\] (5)

where \(p()\) is the prior distribution for offset \(\). For position offset \(_{}\) and scale offset \(_{S}\), we assume normal distribution and uniform distribution respectively as the prior. Particularly, to learn multi-scale samples, we assume the offset prior distribution with the following parameters:

\[q(_{S}) U(-S_{base}+(1-1/K)S_{base},0);\ \ \ \ q(_{}) (0,^{2}),\] (6)

where \(S_{base}\) is the scale of base Gaussian that the finer Gaussian attached, \(^{2}\) is the prior variance for the position offset. The range of scale after offset for a base Gaussian would be \([(1-1/K)S_{base},S_{base}]\), which means that the scale after offset would vary with the associated base Gaussian. The lower bound of the prior is \((1-1/K)\) times the base Gaussian scale. This design choice ensures that the offset applied to the base Gaussian generates finer Gaussian with a larger scale range as the number of spawned Gaussians decreases, which means that larger diversity in scale is applied when there are fewer spawned Gaussians. Additionally, we choose zero mean value for the prior of position offset to enforce that the position lies around the base Gaussian, and the prior of opacity offset is given in the Appendix B. We learn the offset table \(\) and with the reparameterization trick . The total loss function is \(_{total}=_{1}(},)+_{ SSIM}(},)+d_{KL}[q()\|p()]\), where \(}\) is the ground truth of color and \(_{SSIM}\) is structural similarity index measure loss introduced in . At inference, predictive distribution is inferred by marginalizing over the offset distribution:

\[p( x,)=*{}_{ p( )}[p( x,)]= p( x,)p( ),\] (7)

where \(p( x,)\) is the color prediction for pixel \(x\). The pseudo-code of our proposer algorithm is shown in Algorithm 1.

```
0: Images and corresponding camera poses
0: Maximum training step \(T\); Spawn interval \(t\); Threshold \(\)
0: Trained scene representation with parameter \(\); offset table \(\)
1:while step \(<T\)do
2:if step % \(t==0\)then
3: Select \(_{base}=\{_{n}|||||>_{},||S_{n}||>_{S},>_{}\}\)
4: Spawn \(_{base}\), create offset table \(=\{_{S},\ _{},\ _{}\}\)
5: Assign prior \(p()\) for offsets
6:endif
7: Sample offset \(\), render image \(\) and compute image loss \(_{1},_{SSIM}\)
8: Compute KL divergence \(_{KL}=d_{KL}(p(|)||q())\)
9: Optimize \(,\) with total loss \(=_{1}+_{SSIM}+_{KL}\)
10:endwhile ```

**Algorithm 1** The pseudo-code of the training process of our uncertainty-aware 3DGS.

## 4 Experiments

We evaluate our uncertainty-aware view synthesis technique on multiple real-world scenes. We compare the quantitative metrics for predictive uncertainty by estimating its correlation with the prediction error. Furthermore, we also validate the quality of our posterior uncertainty by removing Gaussians using different uncertainty threshold levels, which is quite practical for cleaning up and removing noisy regions of the scene, as known as floaters . We will first introduce the datasets, metrics and baseline methods we used for evaluation, then the implementation details of our multi-scale variational inference algorithm. Finally, we will present quantitative results on uncertainty estimation, view synthesis and qualitative results on floater removal in the following.

### Experimental Details

**Datasets.** We use three datasets for evaluation: **i) LF dataset** contains in total 8 indoor and outdoor scenes, each containing over 100 images from \(360^{}\) view. Following the same setting with CF-NeRF , we use images from selected scenes torch, basket, africa, statue for evaluation. **ii) LLFF dataset** contains 8 forward-facing and outdoor scenes, each containing 20 to 62 images where the camera positions are arranged in a grid pattern. **iii) Mip-NeRF 360 dataset** contains 6 outdoor scenes, in each scene, more than 200 images are captured in \(360^{}\) view. The scenes in this dataset are unbounded and contain detailed regions like grass fields, which is challenging for reconstruction. We use this dataset to demonstrate the noisy Gaussian removal ability of our method.

**Evaluation Metrics.** For evaluating uncertainty estimation quantitatively, we first use the Area Under Sparsification Error (AUSE) with Mean Absolute Error (MAE) error. This metric evaluated the correlation between estimated uncertainty and true MAE error in each rendered view. It reorders the pixels according to the estimated uncertainty and calculates the difference between the reordered MAE array and the original ones. We normalize the error when calculating the difference. Secondly, we use the Negative Log-Likelihood (NLL) as a metric, which measures the likelihood of ground truth in the predictive distribution. This metric can evaluate both uncertainty and image quality at the same time. For image quality evaluation, we use Peak Signal-to-Noise Ratio (PSNR) to estimate the noise level, Structural Similarity Index Measure (SSIM) to measure the structural distortion and Learned Perceptual Image Patch Similarity (LPIPS) to measure the perceptual quality.

**Baseline Methods.** **i) CF-NeRF** enables the uncertainty estimation of NeRF models by modeling latent variables and learning a conditional normalizing flows model. **ii) S-NeRF** applies Bayesian learning to the NeRF model, and is able to quantify both depth and color uncertainty. **iii) Bayes' Ray** constructs a spatial uncertainty field that can add perturbations to the position input of the radiance field and performs Laplace's approximation. **iv) Ensemble GS.** In this method, we train 10 3DGS models with different subsets of initialization points from Structure from motion (SfM) . We also use different random seeds for each model. The variance of the predictions between all models is regarded as the predictive uncertainty.

**Implementation Details.**  We use an AdamW optimizer to update the learnable parameters of our variational multi-scale representation. The learning rate for 3DGS attributes is the same as the original algorithm . We choose to spawn \(K=10\) finer Gaussians in the offset table to perform our multi-scale variational inference. The learning rate of the offset table is 0.1 times the learning rate of each attribute. The experiments are performed on a single NVIDIA A100 GPU.

### Uncertainty Estimation Quality Evaluation

We first evaluate the uncertainty of depth on the LF dataset. The calibration between uncertainty and depth error is quantified by AUSE reported in Table 1. On average, our method achieves the best performance. The performance of the ensemble method approaches our method while being better than other methods. However, our method requires much less computational cost compared to the model ensemble. In the basket scene, our method largely improves the AUSE metric by 0.9 compared to the second-best ensemble method.

Furthermore, we evaluate the uncertainty quality of rendered images on the LF and LLFF datasets. In Table 2, we report the AUSE and NLL metrics for rendered images. The NLL is estimated by the multivariate kernel density estimator used in . On the LF dataset, our method shows the best AUSE, and outperforms S-NeRF in both metrics. In terms of NLL, CF-NeRF shows comparable results in aligning the predictive distribution with the ground truth. A plausible reason is that CF-NeRF models

Figure 4: The visualization of predicted uncertainty map of novel view renderings. Our method demonstrates the best alignment of the uncertainty map with the error map.

[MISSING_PAGE_FAIL:9]

moving testing cameras away from the training trajectory leads to significant drops in visual quality. In order to obtain a high-fidelity radiance field, one can remove these noisy Gaussians manually with editing tools. However, this process requires a large amount of human labor.

Our uncertainty estimation can be used to automatically remove floaters by deleting Gaussians where their parameters have relatively large posterior uncertainty estimated. The results of floater removal are shown in Figure 5. We retain 50% and 30% of the Gaussians with smaller posterior uncertainty in their parameters. From a distant camera view, the novel view of the background scene fails to be synthesized due to the lack of the multi-view supervision signal. With the increasing number of removed Gaussians, the noisy Gaussians with large scale and irregular covariance in the background are removed gradually. After removing 70% of the Gaussians in the scene, the floaters in the background scene are mostly eliminated while leaving the clear Gaussians that capture the complete object of interest. By using our method, we can obtain clear foreground objects for further editing and presenting in VR/AR scenes.

## 5 Conclusion

In this paper, we have proposed a probabilistic framework to address the uncertainty estimation problem in the 3DGS algorithm. Different from previous work, we have identified the benefits of building multi-scale presentations to enhance the diversity of parameter space samples when performing Bayesian inference. Our method improves the efficiency of variational inference by parameter sharing and sampling only a subset of attributes of the model. Experimental results have demonstrated the accuracy of our method in uncertainty estimation and its effectiveness in removing floaters. The potential usages of our method include quality assessment of 3DGS scenes, robotics navigation and guided interactive active data acquisition.

Figure 5: The results of noisy Gaussian removal on Mip-NeRF 360 scenes. By gradually deleting the Gaussians with large posterior uncertainty, our method removes the blurred floaters. The object of interest remains complete after the clean-up.

Acknowledgement

This work was supported in part by the NSFC / Research Grants Council (RGC) Joint Research Scheme under the grant: N_HKBU214/21, and the RGC Senior Research Fellow Scheme under the grant: SRFS2324-2S02.