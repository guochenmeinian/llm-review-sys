# Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning

Frederik Hoppe\({}^{*}\)

RWTH Aachen University

hoppe@mathc.rwth-aachen.de &Claudio Mayrink Verdun\({}^{*}\)

Harvard University

claudioverdun@seas.harvard.edu &Hannah Laus\({}^{*}\)

TU Munich \(\&\) MCML

hannah.laus@tum.de &Felix Krahmer

TU Munich \(\&\) MCML

felix.krahmer@tum.de &Holger Rauhut

LMU Munich \(\&\) MCML

rauhut@math.lmu.de

###### Abstract

Uncertainty quantification (UQ) is a crucial but challenging task in many high-dimensional learning problems to increase the confidence of a given predictor. We develop a new data-driven approach for UQ in regression that applies both to classical optimization approaches such as the LASSO as well as to neural networks. One of the most notable UQ techniques is the debiased LASSO, which modifies the LASSO to allow for the construction of asymptotic confidence intervals by decomposing the estimation error into a Gaussian and an asymptotically vanishing bias component. However, in real-world problems with finite-dimensional data, the bias term is often too significant to disregard, resulting in overly narrow confidence intervals. Our work rigorously addresses this issue and derives a data-driven adjustment that corrects the confidence intervals for a large class of predictors by estimating the means and variances of the bias terms from training data, exploiting high-dimensional concentration phenomena. This gives rise to non-asymptotic confidence intervals, which can help avoid overestimating certainty in critical applications such as MRI diagnosis. Importantly, our analysis extends beyond sparse regression to data-driven predictors like neural networks, enhancing the reliability of model-based deep learning. Our findings bridge the gap between established theory and the practical applicability of such methods.

+
Footnote â€ : \(*\): Equal contribution. Correspondence to hoppe@mathc.rwth-aachen.de.

## 1 Introduction

The past few years have witnessed remarkable advances in high-dimensional statistical models, inverse problems, and learning methods for solving them. In particular, we have seen a surge of new methodologies and algorithms that have revolutionized our ability to extract insights from complex, high-dimensional data . Also, the theoretical underpinnings of the techniques in these fields have achieved tremendous success. However, the development of rigorous methods for quantifying the uncertainty associated with their estimates and underlying parameters, such as constructing confidence intervals for a given solution, has lagged behind, with much of the underlying theory remaining elusive.

In high-dimensional statistics, for example, even for classical regularized estimators such as the LASSO , it was shown that a closed-form characterization of the probability distribution of the estimator in simple terms is not possible, e.g., [7, Theorem 5.1]. This, in turn, implies that it is very challenging to establish rigorous confidence intervals that would quantify the uncertainty ofsuch estimated parameters. To overcome this, a series of papers  proposed and analyzed the _debiased LASSO_, also known as the desparsified LASSO, a procedure to fix the bias introduced by the \(_{1}\) penalty in the LASSO; see [9, Corollary 11] and  for a discussion on the bias induced by the \(_{1}\) regularizer. The debiased estimator derived in the aforementioned works has established a principled framework for obtaining sharp confidence intervals for the LASSO, initiating a statistical inference approach with UQ guarantees for high-dimensional regression problems where the number of predictors significantly exceeds the number of observations. Recently, this estimator was also extended in several directions beyond \(_{1}\)-minimization which include, for example, deep unrolled algorithms  and it has been applied to many fields like magnetic resonance imaging both with classical high-dimensional regression techniques as well as recent learning ones ; see the paragraph _related works_ in Section 2 below.

The idea of the debiased LASSO is that its estimation error, i.e., the difference between the debiased estimator and the ground truth, can be decomposed into a **Gaussian** and a **remainder/bias** component. It has been shown in certain cases that the \(_{}\) norm of the remainder component vanishes with high probability, assuming an _asymptotic_ setting, i.e., when the dimensions of the underlying model grow within a specific rate, see  for details. In this case, the estimator is proven to be _approximately Gaussian_ from which the confidence intervals are derived. However, in practice, one needs to be in a very high-dimensional regime with enough data for these assumptions to kick in. In many applications with a finite set of observations, the _remainder term does not vanish_; it can rather be substantially large, and the confidence intervals constructed solely based on the Gaussian component _fail to account for the entire estimation error_. Consequently, the derived confidence intervals are narrower, resulting in an overestimation of certainty. This issue is particularly problematic in applications where it is

Figure 1: Illustration of the confidence interval correction. Figs. 0(a), 0(b), 0(c) show the construction of CIs with standard debiased techniques (w/o data adjustment) and with our proposed method (w/ Gaussian adjustment - Thm. 3 - in Fig. 0(b) and data adjustment - Thm. 2 - in Fig. 0(c)), respectively. The red points represent the entries that are not captured by the CIs. Additionally, Fig. 0(d) shows box plots of coverage over all components, and Fig. 0(e) shows them on the support, non-zero pixels. In the last two plots, the left box refers to the asymptotic and the right to the non-asymptotic CIs based on Gaussian adjustment of \(500\) feature vectors. We solve a sparse regression problem \(y=Ax+\) via the LASSO, where \(A^{4000 10000}\), \(x^{N}\) is 200-sparse, and the noise level is \( 10\%\). The averaged coverage over \(250\) vectors with significance level \(=0.05\) of the asymptotic confidence intervals is \(h^{W}(0.05)=0.9353\) and on the support \(h^{W}_{S}(0.05)=0.8941\). Confidence intervals built with our proposed method yield for Gaussian adjustment \(h^{G}(0.05)=0.9684\) and on the support \(h^{G}_{S}(0.05)=0.9421\), and for data-driven adjustment \(h(0.05)=h_{S}(0.05)=1\). For more details, cf. Section 5.1 and Appendix D.

crucial to estimate the magnitude of a vector coefficient with a high degree of confidence, such as in medical imaging applications.

Moreover, according to the standard theory of debiased estimators, the estimation of how small the **remainer term** is depends on how well one can quantify the \(_{2}\) and \(_{1}\) bounds for the corresponding _biased estimator_, e.g., the LASSO . Although sharp oracle inequalities exist for such classical regression estimators, cf. _related works_, the same cannot be said about when unrolled algorithms are employed. For the latter, generalization bounds are usually not sharp or do not exist.

In this paper, we tackle the challenge of constructing valid confidence intervals around debiased estimators for the parameters in high-dimensional learning. The key difficulty lies in accounting for the remainder term in the estimation error decomposition - see Equation 3 - which hinders the development of finite-sample confidence intervals. We propose a **novel non-asymptotic theory that explicitly characterizes the remainder term**, enabling us to construct reliable confidence intervals in the finite-sample regime. Furthermore, we extend our framework to quantify uncertainty for the output of model-based neural networks, which, in turn, are used to solve inverse problems. This paves the way for a rigorous theory of data-driven UQ for modern deep learning techniques. We state an informal version of our main result, discussed in detail in Section 3.

**Theorem 1** (Informal Version).: _Let \(x^{(1)},,x^{(l)}^{N}\) be i.i.d. data. Let \(b^{(i)}=Ax^{(i)}+^{(i)}\) be a high-dimensional regression model with noise \(^{(i)}(0,^{2}I_{N N})\). With the data, derive, for a significance level \(\), a confidence radius \(r_{j}()\) for a new sample's component \(x_{j}^{(l+1)}\). Let \((^{u})_{j}^{(l+1)}\) be the debiased estimator based on a (learned) high-dimensional regression estimator \(_{j}^{(i)}\). Then, it holds that_

\[(|(^{u})_{j}^{(l+1)}-x_{j}^{(l+1)}| r_{j }()) 1-.\]

Theorem 1 has far-reaching implications that transcend the classical regularized high-dimensional regression setting. For example, it enables the establishment of rigorous confidence intervals for learning algorithms such as unrolled networks . To our knowledge, obtaining rigorous UQ results for neural networks without relying on non-scalable Monte Carlo methods remains a challenging problem . To address this and quantify uncertainty, our approach combines model-based prior knowledge with data-driven statistical techniques. The model-based component harnesses the Gaussian distribution of the noise to quantify the uncertainty arising from the noisy data itself. We note that the Gaussian assumption for the noise is not a limitation, and extensions to non-Gaussian distributions are also possible via, e.g., a Central Limit Theorem-type argument, as clarified by . We make a Gaussian noise assumption here for the sake of clarity. Complementing this, the data-driven component is imperative for quantifying the uncertainty inherent in the estimator's performance. Moreover, _our approach does not require any assumptions regarding the convergence or quality properties of the estimator_. This flexibility enables the debiased method to apply to a wide range of estimators.

**Contributions.** The key contributions in this work are threefold 1

1. We solve the problem illustrated in Fig. 1 by **developing a non-asymptotic theory for constructing confidence intervals around the debiased LASSO estimator**. Unlike existing approaches that rely on asymptotic arguments and ignore the remainder term, _our finite-sample analysis explicitly accounts for the remainder_, clarifying an important theoretical gap and providing rigorous guarantees without appealing to asymptotic regimes.
2. We establish a general framework that **extends the debiasing techniques to model-based deep learning approaches for high-dimensional regression**. Our results enable the principled measurement of uncertainty for estimators learned by neural networks, a capability crucial for reliable decision-making in safety-critical applications. We test our approach with state-of-the-art unrolled networks such as the It-Net .
3. For real-world medical imaging tasks, we demonstrate that the remainder term in the debiased LASSO estimation error can be accurately modeled as a Gaussian distribution. Leveraging this finding, we derive **Gaussian adjusted CIs that provide sharper uncertainty estimates than previous methods**, enhancing the practical utility of debiased estimators in high-stakes medical domains.

Background and Problem Formulation

In numerous real-world applications, we encounter high-dimensional regression problems where the number of features far exceeds the number of observations. This scenario, known as high-dimensional regression, arises when we aim to estimate \(N\) features, described by \(x^{0}^{N}\) from only a few \(m\) target measurements \(b^{m}\), where \(m N\). Mathematically, this can be expressed as a linear model \(b=Ax^{0}+\), where \(A^{m N}\) is the measurement matrix and \((0,^{2}I_{N N})\) is additive Gaussian noise with variance \(^{2}\). In the presence of sparsity, where the feature vector \(x^{0}\) has only \(s\) non-zero entries (\(s N\)), a popular approach is to solve the LASSO, which gives an estimator \(\) obtained by solving the following \(_{1}\)-regularized optimization problem:

\[_{x^{N}}\|Ax-b\|_{2}^{2}+\|x\|_{1}.\] (1)

However, the LASSO estimator is known to exhibit a systematic bias, and its distribution is intractable, posing challenges for uncertainty quantification . To address this limitation, debiasing techniques have been developed in recent years [8; 9; 10]. The debiased LASSO estimator, \(^{u}\), is defined as:

\[^{u}=+MA^{*}(A-b),\] (2)

where \(M\) is a correction matrix that could be chosen such that \(_{i,j\{1,,N\}}|(M-I_{N N})_{ij}|\) is small. Here, \(=A}{m}\). We refer to  for a more detailed description of how to choose \(M\). Remarkably, the estimation error

\[^{u}-x^{0}=/m}_{=:W}+-I_{N N})(x^{0}-)}_{=:R},\] (3)

can be decomposed into a Gaussian component \(W(0,}{m})\) and a remainder term \(R\) that vanishes asymptotically with high probability [15, Theorem 3.8], assuming a Gaussian measurement matrix \(A\). Such a result was extended to matrices associated to a bounded orthonormal system like a subsampled Fourier matrix, allowing for extending the debiased LASSO to MRI . The decomposition (3) and the asymptotic behavior of \(R\) enable the construction of asymptotically valid CIs for the debiased LASSO estimate, providing principled UQ for high-dimensional sparse regression problems.

However, in real-world applications involving finite data regimes, the remainder term can be significant, rendering the asymptotic confidence intervals imprecise or even misleading, as illustrated in Fig. 1. This issue is particularly pronounced in high-stakes domains like medical imaging, where reliable UQ is crucial for accurate diagnosis and treatment planning. Second, the debiasing techniques have thus far been restricted to estimators whose error is well quantifiable, leaving the **challenge of how they would behave for deep learning architectures open**. In such cases, the behavior of the remainder term is largely unknown, precluding the direct application of existing debiasing methods and hindering the deployment of these methods in risk-sensitive applications.

A prominent example for solving the LASSO problem in (1) with an unrolled algorithm is the ISTA [20; 21]:

\[x^{k+1}=_{}((I_{N N}-A^{T}A)x^{k}+ A^{T}b), k 0.\]

Here, \(>0\) is a step-size parameter, and \(_{}(x)\) is the soft-thresholding operator. The work  interpreted each ISTA iteration as a layer of a recurrent neural network (RNN). The Learned ISTA (LISTA) approach learns the parameters \(W_{1}^{k},W_{2}^{k},^{k}\) instead of using the fixed ISTA updates:

\[x^{k+1}=_{^{k}}(W_{2}^{k}x^{k}+W_{1}^{k}b).\]

In this formulation, LISTA unrolls \(K\) iterations into \(K\) layers, with learnable parameters \((W^{k},^{k})\) per layer. The parameters are learned by minimizing the reconstruction error \(_{,W}}_{i=1}^{l}\|x_{i}^{k}(,W,b^{(i )},x^{(i)})-x^{(i)}\|_{2}^{2}\) on training data \((x^{(i)},b^{(i)})\). Unrolled neural networks like LISTA have shown promise as model-based deep learning solutions for inverse problems, leveraging domain knowledge for improved performance. Such iterative end-to-end network schemes provide state-of-the-art reconstructions for inverse problems . Recently, the work  proposes a framework based on the debiasing step to derive confidence intervals specifically for the unrolled LISTA estimator. However, similar to the previously mentioned debiased LASSO literature, _it only handles the asymptotic setting_. One of the main goals of this paper is to overcome such limitation of the current theory.

**Related Works.**

_High-dimensional regression._ High-dimensional regression and sparse recovery is now a well-established theory, see  and references therein. In this context, several extensions of the LASSO have been proposed such as the elastic net , the group LASSO , the LASSO with a nuclear norm penalization , the Sorted L-One Penalized Estimation (SLOPE)  which adapts the \(_{1}\)-norm to control the false discovery rate. In addition to convex penalty functions, concave penalties have been explored to address some limitations of the LASSO, e.g., the Smoothly Clipped Absolute Deviation (SCAD) penalty  and the Minimax Concave Penalty (MCP) . Non-convex variants of the LASSO for \(_{p}\)-norm (\(p<1\)) minimization were also studied  as well as noise-blind variants such as the square-root LASSO . Scalable and fast algorithms for solving the LASSO and its variants include semi-smooth Newton methods  and IRLS .

_LASSO theory._ Given how ubiquitous and studied such an estimator is, it is difficult to do justice to all the papers that have contributed to such a theory. Several works have established oracle inequalities for the LASSO . Another key theoretical result is the consistency of the LASSO in terms of variable selection.  and  established the consistency of the LASSO while  analyzed the sparsity behavior of the LASSO when the design matrices satisfy the Restricted Isometry Property.

_Debiased estimators._ After the first papers about the debiased LASSO , some works have focused on improving its finite-sample performance and computational efficiency . The size of the confidence intervals derived for the debiased LASSO has been proven to be sharp in the minimax sense . Debiased estimators have been extended in several directions, e.g., . Recently,  established asymptotic normality results for a debiased estimator of convex regularizers beyond the \(_{1}\)-norm. In the context of MR images,  explored a debiased estimator for inverse problems with a total variation regularizer. Debiased estimators have also been recently extended to unrolled estimators - see discussion in the next paragraph - in .

_Algorithm unrolling and model-based deep learning for inverse problems._ The idea of unfolding the iterative steps of classical algorithms into a deep neural network architecture dates back to , which proposed the Learned ISTA (LISTA) to fast approximate the solution of sparse coding problems. Several works have extended and improved upon the original LISTA framework .  proposed the Learned Primal-Dual algorithm, unrolling the primal-dual hybrid gradient method for tomographic reconstruction.  proposed the Deep Cascade of Convolutional Neural Networks (DC-CNN) for dynamic MRI reconstruction.  unfolded proximal gradient descent solvers to learn their parameters for 1D TV regularized problems.  introduced a general framework for algorithm unrolling.  developed MoDL, a model-based deep learning approach for MRI reconstruction that unrolls the ADMM algorithm.  proposed a proximal alternating direction network (PADNet) to unroll nonconvex optimization. See also the surveys for more information about unrolling and also the connection with physics-inspired methods .  developed the It-Net, an unrolled proximal gradient descent scheme where the proximal operator is replaced by a U-Net. This scheme won the AAPM Challenge 2021  whose goal was _to identify the state-of-the-art in solving the CT inverse problem with data-driven techniques_. A generalization of the previous paradigm is the _learning to optimize_ framework that develops an optimization method by training, i.e., learning from its performance on sample problem .

_Uncertainty Quantification._ There have been a few attempts to quantify uncertainty on a pixel level for unrolled networks used in imaging processing, e.g., . However, such approaches are based on Bayesian networks and MC dropout , which requires significant inference time paired with a loss of reconstruction performance since the dropout for UQ is a strong regularizer in the neural network. Unlike prior work, our contribution focuses on a scalable data-driven method that is easily implementable in the data reconstruction pipeline.

Another method that became popular in the last couple of years is conformal prediction , which addresses the problem of constructing prediction bands \(_{m}:X\{B\}\) for a given level \(\) with the property that for a new i.i.d. pair \((a_{i},b_{i})\), we get \((b_{m+1}_{n}(a_{m+1})) 1-\), where the pairs \((a_{i},b_{i}) P,i=1,,m\) are i.i.d. feature and response pairs from a distribution \(P\) on \(A B\) and the probability is over all of available data \((a_{i},b_{i}),i=1,,m+1\). Such a method assumes that the regression coefficient \(x\), in the model \(b_{i}= a_{i},x+\), is fixed. In contrast to that, the debiased LASSO produces confidence intervals for individual pixels \(x_{j}^{(l+1)}\), where \(j=1,,N\) when a new set of data points \(b_{1}^{(l+1)},,b_{m}^{(l+1)}\) is given for a fixed set of measurement vectors \(a_{1},,a_{m}\). Recently, a few papers developed conformal prediction-based uncertainty masks for imaging tasks [71; 72]. Unlike the former, our method provides computationally inexpensive confidence intervals for a new given test image. Unlike the latter, which constructs an interval-valued function for each pixel of a new image that holds in expectation (see Equation 2 in ), our method comes with pixel-wise coverage guarantees for each new test image.

## 3 Data-Driven Confidence Intervals

We now introduce our data-driven approach to correct the CIs. Instead of deriving asymptotic CIs from the decomposition \(^{u}-x^{0}=W+R\), by assuming that \(R\) asymptotically vanishes, we utilize data \((b^{(i)},x^{(i)})_{i=1}^{l}\) along with concentration techniques to estimate the size of the bias component \(R\). We continue to leverage the prior knowledge of the Gaussian component \(W\) while extending the CIs' applicability to a broad class of estimators, including neural networks. Our method is summarized in Algorithm 1, where the data is used to estimate the radii of the CIs, and in Algorithm 2, which constructs the estimator around which the CIs are centered. The following main result proves the validity of our method.

**Theorem 2**.: _Let \(x^{(1)},,x^{(l)}^{N}\) be i.i.d. complex random variables representing ground truth data drawn from an unknown distribution \(\). Suppose, that \(^{(i)}(0,^{2}I_{m m})\) is noise in the high-dimensional models \(b^{(i)}=Ax^{(i)}+^{(i)}\), where \(A^{m N}\), and independent of the \(x^{(i)}\)'s. Let \(:^{m}^{N}\) be a (learned) estimation function that maps the data \(b^{(i)}\) to \(^{(i)}\), which is an estimate for \(x^{(i)}\). Set \(|R_{j}^{(i)}|=|e_{j}^{T}(M-I_{N N})(^{(i)}-x^{(i)})|\) for fixed \(A\) and \(M\). For \(j=1,,N\), we denote the true but unknown mean with \(_{j}=[|R_{j}^{(1)}|]\) and the unknown variance with \((_{R}^{2})_{j}:=[(|R_{j}^{(1)}|-_{j})^{2}]\), respectively. Let \(_{j}=_{i=1}^{l}|\ R_{j}^{(i)}|\) be the unbiased sample mean estimator and \((_{R}^{2})_{j}=_{i=1}^{l}(|R_{j}^{(i)}|- _{j})^{2}\) the unbiased variance estimator. Let \((0,1)\) and \(_{j}(0,1-)\). Furthermore, set the confidence regions for the sample \(x^{(l+1)}\) in the model \(b^{(l+1)}=Ax^{(l+1)}+^{(l+1)}\) as \(C_{j}()=\{z:|(^{u})_{j}^{(l+1)}-z| r_{j}()\}\) with radius_

\[r_{j}()=M^{*})_{jj}^{1/2}}{}})}+c_{l}()( _{R})_{j}+_{j}, c_{l}():=-1}{ l^{2}(1-_{j})-l}}.\] (4)

_Then, it holds that_

\[(x_{j}^{(l+1)} C_{j}()) 1-.\] (5)

Theorem 2 presents a way to achieve conservative confidence intervals that are proven to be valid, i.e., are proven to contain the true parameter with a probability of \(1-\). Its main advantage is that there are _no assumptions_ on the distribution \(\) (except that \(_{R}^{2}\) exists), making it widely applicable. Hence, Theorem 2 includes the worst-case distribution showing a way to quantify uncertainty even in such an ill-posed setting. Especially in medical imaging, such certainty guarantees are crucial for accurate diagnosis. The proof exploits the Gaussianity of the component \(W\) as well as an empirical version of Chebyshev's inequality, which is tight when there is no information on the underlying distribution. The detailed proof can be found in Appendix B. For a thorough discussion on Theorem 2 including practical simplifications and the dependence of \(_{j}\) on the confidence interval length, we refer to Appendix A.

More certainty comes with the price of larger confidence intervals. If there is additional information on the distribution of \(R\), like the ability to be approximated by a Gaussian distribution, then the confidence intervals become tighter. This case, which includes relevant settings such as MRI, is discussed in Section 4.

```
1:Input: Estimation function \(\), dictionary matrix \(A\), correction matrix \(M\), data points \((b^{(i)},x^{(i)})_{i=1}^{l}\), significance level \(\)
2:for\(i=1,,l\)do
3: Compute \(^{(i)},R^{(i)}^{N}\) via \(^{(i)}=(b^{(i)})\) and \(R_{j}^{(i)}=e_{j}^{T}(M-I_{N N})(^{(i)}-x^{(i)})\).
4:endfor
5:for\(j=1,,N\)do
6: Estimate \(_{j}=_{i=1}^{l}|R_{j}^{(i)}|\) and \((_{R}^{2})_{j}=_{i=1}^{l}(|R_{j}^{(i)}|- _{j})^{2}\)
7: Solve \(r_{j}()=_{(0,1-)}M^{*})_{jj}^{1/2}}{})}+c_{l}((1-))(_{R })_{j}+_{j}\)
8:endfor
9:Output: Radii of confidence regions \((r_{j}())_{j=1}^{N}\) ```

**Algorithm 1** Estimation of Confidence Radius

## 4 Confidence Intervals for Gaussian Remainders

Valid confidence intervals, i.e., those with correct coverage probability, can be derived most straightforwardly when the distribution of the remainder term is known and easily characterized. In such cases, more informative distributional assumptions lead to potentially tighter confidence intervals compared to Theorem 2, which makes no assumptions about the remainder component. In this section, we derive non-asymptotic confidence intervals assuming the remainder term to be approximated by a Gaussian distribution.

**Theorem 3**.: _Let \(^{u}^{N}\) be a debiased estimator for \(x^{N}\) with a remainder term \(R(0,_{R}/m)\). Then, \(C_{j}()=\{z|z-_{j}^{u}| r_{j}()\}\) with radius_

\[r_{j}^{G}()=(MM^{*})_{jj}+(_{R})_{jj })^{1/2}}{})}.\] (6)

_is valid, i.e. \((x_{j} C_{j}()) 1-\)._

For the proof, we refer to Appendix B. We note, however, that the theorem can be generalized beyond the Gaussian case. In particular, we present in Appendix C a proof of this theorem for heavy-tailed distributions. In Appendix E, we demonstrate empirically that the Gaussian assumption for the remainder term holds in a wide range of relevant practical settings. This validation enables the application of the proposed confidence intervals derived under this assumption. These confidence intervals strike a careful balance between non-asymptotic reliability, ensuring valid coverage even in finite-sample regimes, and tightness, providing informative and precise uncertainty estimates. By leveraging the Gaussian approximation, which becomes increasingly accurate in higher dimensions as illustrated in Figure 7, our framework offers a principled and computationally efficient approach to quantifying uncertainty in high-dimensional prediction problems. The variance of \(R\) can be estimated with the given data using, e.g., the unbiased estimator for the variance as in Theorem 2.

Numerical Experiments

We evaluate the performance of our non-asymptotic confidence intervals through extensive numerical experiments across two settings: (i.) the _classical debiased LASSO framework_ to contrast our non-asymptotic confidence intervals against the asymptotic ones. (ii.) the _learned framework_ where we employ learned estimators, specifically the U-net  as well as the It-Net , to reconstruct real-world MR images and quantify uncertainty. Our experiments demonstrate the importance of properly accounting for the remainder term in practical, non-asymptotic regimes. Each experiment follows the same structure:

1. Data Generation and Management: We fix the forward operator \(A\) and generate \(n>2\) feature vectors \({x^{(i)}}_{i=1}^{n}\) and noise vectors \({^{(i)}}_{i=1}^{n}\) with \({^{(i)}}(0,^{2}I_{m m})\). We obtain observations \({b^{(i)}}_{i=1}^{n}\) via \({b^{(i)}}=Ax^{(i)}+{^{(i)}}\). We split the data \({(b^{(i)},x^{(i)})}_{i=1}^{n}\) into an _estimation_ dataset of size \(l\) and a test dataset of size \(k\) (\(l+k=n\)). If we learn an estimator, we further split the data into training, estimation, and test sets.
2. Reconstruction: Depending on the experiment, we obtain a reconstruction function \(\) in one of the following ways: for the classical LASSO setting, we use the LASSO; for the learned estimator experiment, we train a U-Net  or It-net  on the training data to serve as the reconstruction function \(\).
3. Estimation of Confidence Radii: We run Algorithm 1 with \(A,,M\) (that is chosen according to ), the estimation data \({(b^{(i)},x^{(i)})}_{i=1}^{l}\), and a predefined significance level \((0,1)\) to obtain radii \({r_{j}()}_{j=1}^{N}\). We optimize over \(\), therefore the \(\) we use is the optimal one which leads to the smallest confidence intervals. To construct the final confidence intervals, the radii need to be centered according to the debiased estimator. For every new measurement \(b\), we run Algorithm 2 to obtain tailored confidence intervals for the feature vector \(x\) corresponding to \(b\). In addition, we compute the CI for the Gaussian adjustment based on Theorem 3 using the estimation set to quantify the variance of \(R\) with the unbiased estimator for the variance as before.
4. Evaluation: We use the test dataset \({(b^{(i)},x^{(i)})}_{i=l+1}^{n}\) to evaluate our adjustments. For each \(b^{(i)}\), we run Algorithm 2 to obtain confidence intervals \({C_{j}^{(i)}()}_{j=1}^{N}\) for \(x^{(i)}\). We estimate \((x_{j}^{(i)} C_{j}())\) by \(h_{j}()=_{i=l+1}^{n}_{\{x_{j}^{(i)} C_{j}( )\}}\) and average over all components \(h()=_{j=1}^{N}h_{j}()\). Since performance on the support \(S\), the non-zero pixels, is crucial, we define the hit rate on \(S\) as \(h_{S}^{(i)}=_{j=1}^{N}_{\{x_{j}^{(i)} C_{j}( )\}}\) and average \(h_{S}()=_{i=l+1}^{n}h_{S}^{(i)}\). Note that the support may change with \(i\). Moreover, we do the same for the CI based on the Gaussian-adjusted radii.

### UQ for Sparse Model-Based Regression

We consider a setting aligned with existing debiased LASSO literature, e.g.,  to demonstrate our approach's extension of current UQ methods. The forward operator is a complex Gaussian matrix \(A^{m N}\) with dimensions \(N=10000\), \(m=0.6N\), and \(A_{ij}(0,1)\). We generate \(n=750\)\(s=0.1N\)-sparse features \(x^{(i)}\) by randomly selecting \(m\) distinct indices from \(1,,N\) and drawing magnitudes from \((0,1)\). With relative noise \(}\|}{\|Ax^{(i)}\|} 0.2\), we split the data \({(b^{(i)},x^{(i)})}_{i=1}^{n}\) into \(l=500\) estimation and \(k=250\) test data. For reconstruction, we solve the LASSO \((b):=*{argmin}_{x^{N}}\|Ax-b\|+ \|x\|_{1}\) with \(=10}(2+)\) following .

With significance level \(=0.05\), we run Algorithm 1 to obtain confidence radii, choosing \(M=I_{N N}\) and exploiting the relaxation (9). Averaged over the \(l\) estimation data points, the \(_{2}\) and \(_{}\) norm ratios are: \(}{\|W\|_{2}}=0.9993\) and \(}{\|W\|_{}}=1.1581\). In existing literature, the \(_{}\) norm is typically measured when the remainder term vanishes, as it is relevant for pixel-wise confidence intervals. Here, the remainder term is of comparable order as the Gaussian term and hence, too significant to neglect in confidence intervals derivation.

Evaluating it on the remaining \(k=250\) data points, the data-driven and Gaussian-adjusted averaged hit rates are \(h(0.05)=1\), \(h_{S}(0.05)=1\) and \(h^{G}(0.05)=0.9691\), \(h^{G}_{S}(0.05)=0.8948\), respectively. Neglecting the remainder term yields \(h^{W}(0.05)=0.8692\) and \(h^{W}_{S}(0.05)=0.6783\), which is substantially lower and violates the specified \(0.05\) significance level. Fig. 2 presents confidence intervals of each type for one data point \(x^{(i)}\). A detailed visualization of \(h_{j}(0.05)\), \(h^{(i)}_{S}(0.05)\), \(h^{G}_{j}(0.05)\), and \((h^{G}_{S})^{(i)}(0.05)\) is illustrated in Fig. 4c and 4i in the appendix. Further experiments with different sparse regression settings, including subsampled Fourier matrices, are also presented in Appendix D.

### UQ for MRI Reconstruction with Unrolled Neural Networks

We extend the debiasing approach to model-based deep learning for MRI reconstruction using the U-Net and It-Net on single-coil knee images from the NYU fastMRI dataset 2[74; 75]. Here, the forward operator is the undersampled Fourier operator \(^{m N}\) with \(N=320 320\), \(m=0.6N\), the Fourier matrix \(\) and a radial mask \(\), see Figure 5b. The noise level \(\) is chosen such that the relative noise is approximately \(0.1\). The data is split into training (33370 slices), validation (5346 slices), estimation (1372 slices), and test (100 slices) datasets.

We then train an It-Net  with \(8\) layers, a combination of MS-SSIM  and \(_{1}\)-losses and Adam optimizer with learning rate \(5e^{-5}\) for \(15\) epochs to obtain our reconstruction function \(\).

With significance level \(=0.1\), we run Algorithm 1 to construct confidence radii, this time for the real part of the image instead of the magnitude, choosing \(M=I_{N N}\) and exploiting the relaxation (9). Averaged over the \(l\) estimation data points, we have \(}{\|W\|_{2}}=0.38\) and \(}{\|W\|_{}}=0.49\), which indicates that the remainder term is significant and cannot be neglected. Evaluating the test data, the averages of the data-driven adjustment hit rates are \(h(0.1)=0.9998\), \(h_{S}(0.1)=0.9995\) and the averages of the Gaussian adjusted hit rates are \(h^{G}(0.1)=0.9485\), \(h^{G}_{S}(0.1)=0.9353\), where the support refers to the largest \(10\%\) of the image pixels as the background cannot be clearly delineated numerically. Neglecting the remainder term, the hit rates of the asymptotic CIs are \(h^{W}(0.1)=0.9306\) and \(h^{W}_{S}(0.1)=0.9152\). As in the sparse regression setting, they are significantly lower. Fig. 3 presents confidence intervals based on the data-driven adjustment and the asymptotic confidence intervals for a region in one image \(x^{(i)}\). In addition, it contains a box plot showing the distribution of the hit rates based on the Gaussian adjustment and the asymptotic hit rates. More experiments for UQ for MRI reconstruction can be found in Appendix D and Tables 2 and 3.

Figure 2: Confidence intervals of asymptotic type 2a, with Gaussian adjustment 2b and data-driven adjustment 2c for one evaluation feature vector in the sparse regression setting described in Section 5.1.

## 6 Final Remarks

In this work, we proposed a data-driven uncertainty quantification method that derives non-asymptotic confidence intervals based on debiased estimators. Our approach corrects asymptotic confidence intervals by incorporating an estimate of the remainder component and has solid theoretical foundations. While the correction can be based on prior knowledge, e.g., a Gaussian distribution of the remainder term, we also derive CI based on a data-driven adjustment without further information. This data-driven nature enhances its applicability to a wide range of estimators, including model-based deep-learning techniques. We conducted experiments that confirm our theoretical findings, demonstrating that even in classical sparse regression settings, the remainder term is too significant to be neglected. Furthermore, we applied the proposed method to MRI, achieving significantly better rates on the image support.

**Limitations and Future Directions.** While our method corrects for the remainder term, larger remainder terms necessitate greater corrections, resulting in wider confidence intervals. The goal is that those intervals should be narrow enough to be informative but wide enough to be realistic, given the sample size and variability in the data. Therefore, it is crucial to achieve a small remainder term to avoid excessively large confidence intervals. Additionally, the accuracy of our method depends on the quality of the estimates for the mean and variance of the remainder term, which improves with more available data. Additionally, the length of the intervals can be minimized over a larger parameter set, provided that more data is available. We leave as a future direction to study the sharpness of the proposed confidence intervals and radii for a given amount of data. Moreover, we would like to investigate how the length of the confidence intervals could be improved when estimating higher moments. We believe that our method is applicable to a wide variety of deep learning architectures, including vision transformers in MRI, e.g., . Testing the generality of the method with state-of-the-art architectures for different problems would demonstrate its broad usefulness.

Figure 3: Reconstruction obtained with the It-Net as described in 5.2. Data-driven adjustment \(90\%\) confidence intervals 3b and asymptotic \(90\%\) confidence intervals 3c for the region (50 pixels) in 320x320 knee image 3a; Boxplots of hit rates 3d for \(95\%\) confidence level for the Gaussian adjusted and asymptotic confidence intervals.