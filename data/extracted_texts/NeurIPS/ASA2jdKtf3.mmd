# A Causal Model of Theory-of-Mind in AI Agents

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

_Agency_ is a vital concept for understanding and predicting the behaviour of future AI systems. There has been much focus on the goal-directed nature of agency, i.e., the fact that AI agents may capably pursue goals. However, the dynamics of agency become significantly more complex when autonomous agents interact with other agents and humans, necessitating engagement in _theory-of-mind_, the ability to reason about the beliefs and intentions of others. In this paper, we extend the framework of multi-agent influence diagrams (MAIDs) to explicitly capture this complex form of reasoning. We also show that our extended framework, _MAIDs with incomplete information_ (II-MAIDs), has a strong theoretical connection to dynamic games with incomplete information with no common prior over types. We prove the existence of important equilibria concepts in these frameworks, and illustrate the applicability of II-MAIDs using an example from the AI safety literature.

## 1 Introduction

The concept of _agency_ plays a central role in AI, from philosophical discussions of the nature of artificial agents  to the practical engineering of agent-like systems [12; 39]. Existing work formalising agency typically focuses on its goal-directed nature in a single-agent setting [25; 30]. However, a full picture of agency should describe systems that represent themselves and other systems as _agents_, i.e., systems with _theory-of-mind (ToM)_[7; 8].

ToM is characterised by multi-agent interactions involving higher-order intentional states , such as beliefs about beliefs, or, in the case of deception, intentions to cause false beliefs . Causality often plays a key role in philosophical notions of belief , and causal models offer a powerful representation of beliefs [14; 36], intentions , and other intentional states . Additionally, causal models have been extended to capture game-theoretic dynamics in the setting of multi-agent influence diagrams (MAIDs) [26; 16]. However, MAIDs assume that all agents in the model have the same, correct beliefs about the world, each other's beliefs, each other's beliefs about beliefs, and so on. With this assumption in place, MAIDs do not explicitly model agents' subjective beliefs or higher-order beliefs.

We generalise MAIDs to the setting of _incomplete information with no common prior_, wherein agents may have different and inconsistent beliefs about the world, and each agent may have different beliefs about the beliefs of other agents. Our framework, _incomplete information MAIDs (II-MAIDs)_, includes explicit subjective belief hierarchies, and therefore enables us to model systems of agents with more complex and realistic ToM.

Contributions and Outline.In Section 2, we discuss formal background on MAIDs and EFGs. We formally define our framework of _MAIDs with incomplete information_ (II-MAIDs) in Section 3. In Section 4, we present a variant of an existing formalism for incomplete information games using EFGs rather than normal-form games, and in Section 5 we prove that it is equivalent to MAIDs with incomplete information. Finally, we review related literature (Section 6) and conclude (Section 7).

Background

In this section, we provide formal definitions of MAIDs and EFGs and explain these game representations using an example. A Bayesian network is a probabilistic graphical model representing a set of variables and their conditional dependencies via a directed acyclic graph. _Influence diagrams_ (IDs) generalise Bayesian networks to the decision-theoretic setting by adding decision and utility variables , and _multi-agent influence diagrams_ (MAIDs) generalise IDs by introducing multiple agents . A MAID can therefore be viewed as a Bayesian network over a graph without parameters for the decision variables. Endowing edges in a MAID with causal meaning results in a _causal game_.

**Definition 1** (26, 16).: A **multi-agent influence diagram (MAID)** is a structure \(=(,)\) where \(=(N,,)\) specifies a set of agents \(N=\{1,,n\}\) and a directed acyclic graph \((,)\). \(\) is partitioned into chance variables \(\), decision variables \(\), and utility variables \(\); decision and utility variables are further partitioned based on which agent they belong to, so \(=_{i N}^{i}\) and \(=_{i N}^{i}\). The parameters \(=\{_{V}\}_{V}\) define the conditional probability distributions (CPDs) \((V_{V};_{V})\) for each non-decision variable such that for _any_ parameterisation of the decision variable CPDs, the resulting joint distribution over \(\) induces a Bayesian network. A MAID is a **causal game** if its edges represent direct causal relationships, or formally if (once decision variables are parameterised) the result of an intervention \((=x)\) for any \(\) can be computed using the _truncated factorisation formula_: \(P((=);)=_{i:v_{i}}P(v_{i}_{v_{i}};_{{}_{i}})\) if \(\) is consistent with \(\), and \(P((=x);)=0\) otherwise.

Edges into decisions are _information links_ observed by an agent, and agents choose their decisions to maximise their expected utility. Causal games refine MAIDs, and as such all results presented here with application to MAIDs also apply to causal games and further refinements, including structural causal games (SCGs) .

_Example 1_.: Figure 1 displays two MAIDs. For Figure 0(a), suppose a human \(H\) is evaluating the honesty of an AI system \(A\). The AI has a level of capabilities \(C\{,\}\) that the human can directly observe. Each capability level occurs with probability 0.5. The human also performs an evaluation in which the AI is asked to demonstrate its capabilities. The AI can demonstrate low or high capabilities. After the evaluation, the human chooses to deploy or not deploy the AI. The AI receives utility 1 if deployed and -1 if not deployed. The human receives utility 0 for not deploying the AI, utility 1 for deploying an honest AI, and utility -5 for deploying a dishonest AI. Figure 0(b) is similar, but instead of an honesty evaluation, it is an evaluation of dangerous capabilities. In this case, the human only learns about the AI's capabilities from the evaluation. The AI's utility is the same, but the human now receives utility 0 for not deploying the AI, utility 1 for deploying an AI with low dangerous capabilities, and utility -5 for deploying an AI with high dangerous capabilities.

In these examples, a MAID describes the objective world, and it is assumed to be _common knowledge_ that this MAID describes reality. However, an agent may be uncertain or incorrect about the game they are playing or the beliefs of other agents. Settings in which agents are uncertain about aspects of the game structure are known as _incomplete information games_. Our framework of incomplete information MAIDs (II-MAIDs), introduced in Section 3, will enable us to explicitly model the varied subjective beliefs that arise in these settings. We now define EFGs, with our running example in EFG form in Figure 2. We will also make use of the notions of perfect recall and strategies/policies in MAIDs and EFGs.

Figure 1: Graphical representations of MAIDs include environment variables (circular), agent decisions (square), and utilities (diamond). Decisions and utilities are coloured according to association with particular agents. Solid edges represent causal dependence and dotted edges are information links. Conceptual context and domains and CPDs for the variables are given above the diagrams.

**Definition 2** (27).: An **extensive form game (EFG)** is a structure \(=(N,T,P,A,,I,U)\). \(N=\{1,,n\}\) is a set of agents. \(T=(,)\) is a game tree with nodes \(\) connected by edges \(\) that are partitioned into sets \(^{0},^{1},,^{n},\) where \(R\) and \(\) are the root and leaves of \(T\), respectively, \(^{0}\) are chance nodes, and \(^{i}\) are the decision nodes controlled by agent \(i N\). \(P=\{P_{1},,P_{|^{0}|}\}\) is a set of probability distributions \(P_{j}(_{V_{j}^{0}})\) over the children of each chance node \(V_{j}^{0}\). \(A\) is a set of actions, where \(A_{j}^{i} A\) denotes the set of actions available at each node in \(V_{j}^{i}^{i}\); \(: A\) is a labelling function mapping each edge \((V_{j}^{i},V_{k}^{k})\) to an action \(a A_{j}^{i}\). \(I=\{I^{1},,I^{n}\}\) contains a set of of information sets \(I^{i}\) for each agent \(i\), where \(I^{i} 2^{^{i}}\) partitions the decision nodes \(^{i}\) belonging to agent \(i\). \(U:^{n}\) is a utility function mapping each leaf node to a vector that determines the final payoff for each agent. A **history**\(h H\) is a sequence of actions (including values of chance variables) leading from the root of the game tree to a particular node. Each node \(v\) is associated with a unique history \(h(v)\). An **observation** at decision node \(I_{j,k}^{i}\) in information set \(I_{j}^{i} I^{i}\) for agent \(i N\) is the intersection of all the histories of the nodes in that information set, i.e., the common actions in the histories \(\{h(v):v I_{j}^{i}\}\).

**Definition 3** ().: Agent \(i\) in a MAID \(\) is said to have **perfect recall** if there exists a total ordering \(D_{1} D_{m}\) over \(^{i}\) such that \((_{D_{j}} D_{j})_{D_{k}}\) for any \(1 j<k m\). \(\) is a **perfect recall game** if all agents in \(\) have perfect recall.

**Definition 4**.: An EFG is said to be a **perfect recall game** if, for each player \(i N\), and for any two decision nodes \(v,v^{}^{i}\) that belong to the same information set \(I_{j,k}^{i}\), the following two conditions hold. First, the sequences of actions taken by player \(i\) leading to \(v\) and \(v^{}\) must be identical. Second, the sequences of information sets visited by player \(i\) on the paths to \(v\) and \(v^{}\) must be identical.

**Definition 5**.: Given a MAID \(=(,)\), a **decision rule**\(_{D}\) for \(D\) is a CPD \(_{D}(D_{D})\) and a **partial policy profile**\(_{^{}}\) is a set of decision rules \(_{D}\) for each \(D^{}\). A (behavioural) **policy**\(^{i}\) refers to \(_{^{}}\), and a (full, behavioural) **policy profile**\(=(^{1},,^{n})\) is a tuple of policies. \(^{-i}(^{1},,^{i-1},^{i+1}, ,^{n})\) specifies policies for all agents except \(i\).

**Definition 6** ().: Given an EFG \(=(N,T,P,A,,I,U)\), a (behavioural) **strategy**\(^{i}\) for a player \(i\) is a set of probability distributions \(^{i}_{j}:A^{i}_{j}\) over the actions available to the player at each of their information sets \(I_{j}^{i}\). A **strategy profile**\(=(^{1},^{2},...,^{n})\) is a tuple of strategies for all players \(i N\). \(^{-i}=(^{1},...,^{i-1},^{i+1},...,^{n})\) denotes the partial strategy profile of all players other than \(i\).

By combining \(\) with the partial distribution \(\) over the chance and utility variables in a MAID, we obtain a joint distribution: \(^{}(,,)_{V} (v_{V})_{D}_{D}(d_{D})\), over

Figure 2: In (a) and (b), graphical representations of EFGs include environment variables (\(V^{0}\)), agent decisions (\(V^{A}\) and \(V^{H}\)), utilities (tuples on the top and bottom), and information sets (dotted lines). The EFGs in Figure 1(a) and Figure 1(b) are equivalent to the MAIDs in Figure 0(a) and Figure 0(b), respectively. \(V^{0}\) represents the initial move, made by nature, which determines \(A\)’s capability \(C\). \(V^{A}_{1}\), \(V^{A}_{2}\) and \(V^{H}_{1}\), \(V^{H}_{2}\), \(V^{H}_{3}\), & \(V^{H}_{4}\) represent moves made by \(A\) and \(H\), respectively. \(I^{z}_{1}\) and \(I^{z}_{2}\) represent \(H\)’s non-singleton information sets.

all the variables in \(\); inducing a Bayesian network. The expected utility for an agent \(i\) given a policy profile \(\) is defined as the expected sum of their utility variables in this Bayesian Network, \(_{U^{i}}_{}[U]\). Similarly, in an EFG \(\), the combination of the distributions in \(P\) with a strategy profile \(\) defines a full probability distribution over paths in \(\).

Finally, prior work 15 has established an equivalence result between MAIDs and EFGs. This result takes the form of two transformation procedures converting between MAIDs and EFGs, called \(\) and \(\). These transformations both imply the existence of a map from strategies in the EFG to policies in the MAID, such that expected utilities are preserved for all agents. This means that under either transformations, equilibria in the original game are equilibria in the resulting game.

## 3 II-MAID Technical Machinery

We start with an informal description of our II-MAIDs framework before presenting the formal definition. A core component of the framework is a set \(\) containing _subjective MAIDs_. A subjective MAID is a self-referential object describing a possible game as envisioned by either the external modeller (we call this the objective model \(S^{*}\)) or an agent playing the game. A subjective MAID \(S\) consists of a MAID \(\) that describes the game being played and beliefs \(P_{i}^{S}\) for each agent \(i\) in the game. The notation \(P_{i}^{S}\) denotes agent \(i\)'s prior over \(\) when the objective model is \(S\), and \(P_{i}^{S}(S^{})\) denotes the probability ascribed by agent \(i\) to subjective MAID \(S^{}\) given that the objective MAID is \(S\). This framework enables us to model _theory-of-mind_, which is typically characterised by _higher-order intentional states_ such as beliefs about beliefs about... ().

**Definition 7**.: An **incomplete information MAID (II-MAID)** is a tuple \(=(,S^{*},)\), where \(\) is a set of agents, \(\) is a set of subjective MAIDs, \(S^{*}\) is the correct objective model, and each **subjective MAID** is a tuple \(S=(^{S},(P_{i}^{S})_{i})\) with \(^{S}\) a MAID and \(P_{i}^{S}\) a prior over \(\) for agent \(i\) such that the following "coherency condition"  holds:

\[P_{i}^{S}(\{S^{}:P_{i}^{S^{}}=P_{i}^{S}\})=1  i,S.\]

First, notice that the recursive nature of \(\), with each element \(S\) including probability distributions \(P_{i}^{S}\) over \(\), allows us to model belief hierarchies of arbitrary and infinite depth. Next, note that agent \(i\) "observes" \(P_{i}^{S^{*}}\) at the start of the game, and this justifies the coherency condition: since agent \(i\) knows \(P_{i}^{S^{*}}\), she can rule out all subjective MAIDs \(S\) for which \(P_{i}^{S} P_{i}^{S^{*}}\). Third, note that II-MAIDs are a strict generalization of MAIDs: a standard MAID is an II-MAID in which \(P_{i}^{S^{*}}(S^{*})=1 i\), i.e. all agents assign probability 1 to \(S^{*}\), the objective model.

_Example 2_.: Suppose a human \(H\) is performing an honesty evaluation on an AI \(A\), but \(A\) believes that it is undergoing a dangerous capabilities evaluation. This combines Figure 0(a) and Figure 0(b): \(H\) correctly believes that Figure 0(a) is the true MAID and also knows that \(A\) is mistaken. \(A\) incorrectly believes that Figure 0(a) is the true MAID and also incorrectly believes that \(H\) believes Figure 0(a) is the true MAID. We can represent this, including the full infinite belief hierarchy, as an II-MAID as follows: \(=\{H,A\},=\{S^{H},S^{A}\}\), and \(S^{*}=S^{H}\), where

\[S^{H}=(^{H},(P_{H}^{S^{H}}(S^{H})=1,P_{A}^{S^{H}}(S^{A})=1)), S ^{A}=(^{A},(P_{H}^{S^{A}}(S^{A})=1,P_{A}^{S^{A}}(S^{A})=1))\]

\(S^{H}\) is the correct objective model, and is also believed with certainty by \(H\). It specifies the true MAID \(^{H}\) represented in Figure 0(a), and \(H\)'s certainty in \(S^{H}\) as well as \(A\)'s misplaced certainty in \(S^{A}\). \(S^{A}\) represents \(A\)'s certainty about the MAID \(^{A}\) in Figure 0(b), and \(A\)'s mistaken belief that \(H\) is also certain about \(S^{A}\). In fact, \(A\) believes it is common knowledge that \(S^{A}\) is the true II-MAID. \(S^{H}\) and \(S^{A}\) concisely convey the objective game and all higher-order beliefs for \(H\) and \(A\). It can be easily verified that the coherency condition holds in this example.

A common assumption in the incomplete information games literature  is that agents' beliefs can be derived from a common prior, i.e., agents have _consistent beliefs_. This assumption means that there exists some common knowledge prior distribution \(p\) over the set of subjective MAIDs \(\), such that upon arriving in any subjective MAID \(S\), agents perform Bayesian updating to yield their beliefs. This assumption allows for a game with incomplete information to be converted into a game with imperfect information , but places a strong constraint on the types of belief hierarchies that can be modelled; namely, it must hold that

\[p(S^{})=_{S}P_{i}^{S}(S^{})p(S)S^{},i.\] (1)

_Example 2_ (continued).: We see that our running example cannot be modelled with a common prior. Supposing that the condition in Equation (1) holds, \(A\)'s beliefs are only consistent with a prior in \(p\) in which \(p(S^{H})=0\), which would force \(H\) to assign zero probability to \(S^{H}\) in both \(S^{H}\) and \(S^{A}\).

### Information Sets and Policies

When forming a policy at the initialisation of an II-MAID \(=(,S^{*},)\), each agent may have significant uncertainty about \(S^{*}\), the objective model, represented by their prior over subjective MAIDs \(P_{i}^{S^{*}}\). They should certainly plan for every eventuality deemed possible according to this prior. We argue that they should also produce a plan for what to do in circumstances deemed impossible under their prior, to avoid situations with undefined actions that might arise for example when \(P_{i}^{S^{*}}(S^{*})=0\), and to avoid forcing \(P_{i}^{S}(S^{})>0\) for all \(i,S,S^{}\).

Therefore, a policy should contain a plan for every possible eventuality that may arise were any subjective MAID to be the objective model. But there may be cases where upon reaching a decision node \(D\), agent \(i\) cannot fully determine the values of certain preceding variables, including cases where previous actions were unobserved by the agent, but also including cases in which the observations of the agent do not provide enough information to distinguish between multiple subjective MAIDs. In these indistinguishable eventualities, a policy must specify the same behaviour, and so we must define some analogy of information sets in EFGs.

At a decision node \(D\), an agent observes the values of \(Pa_{D}\) and also observes the action set available to it, \(dom(D)\). A policy should index every possible observation-action set combination (i.e. every tuple containing a non-null decision and an associated action set) to a mixed action. We define the _information sets in an II-MAID_ as follows:

**Definition 8**.: Given an II-MAID \(=(,S^{*},)\), we iteratively build the **information sets**. For each subjective MAID \(S\) and each agent \(i\), denote \(_{i}(S)\) as the set of decision nodes for agent \(i\) in \(^{S}\), \(Pa_{D_{i}}(S)\) as the set of parents of \(D_{i}\) in \(^{S}\), and \(_{S}^{}()\) as the distribution of variables in \(^{S}\) under some policy \(\). Define

\[_{,i}:=_{D_{i}_{i}()}\{( _{D_{i}},dom(D_{i}))_{D_{i}} dom(_{D_ {i}}(S)):(_{D_{i}})>0\}.\]

Then _agent \(i\)'s information sets_ are defined as \(_{i}():=_{S}_{,i}\). Finally, we can define the set of information sets as \(()=(_{i}())_{i}\).

**Definition 9**.: We define an II-MAID \(=(,S^{*},)\) as having **perfect recall** if for each \(S\), \(^{S}\) is a perfect recall game.

**Definition 10**.: Given an II-MAID \(=(,S^{*},)\), a **decision rule**\(_{I}\) for \(I=(,)()\), where \(\) is a context and is a action set, is a CPD \(_{I}()\) over \(\). A **partial policy profile**\(_{I^{}}\) is a set of decision rules \(_{I}\) for each \(I^{}()\), where we write \(_{-I^{}}\) for the set of decision rules for each \(I()^{}\). A (behavioural) **policy**\(^{}\) refers to \(_{_{i}()}\), a (full, behavioural) **policy profile**\(=(^{1},,^{n})\) is a tuple of policies, and \(^{-i}:=(^{1},,^{i-1},^{i+1},,^{n})\).

We note that unlike in standard MAIDs, in which a decision rule specifies behaviour at a given decision variable in all contexts, decision rules in II-MAIDs specify a CPD only given a single context. We can then calculate the subjective expected utility of a joint behaviour policy for agent \(i\) according to their beliefs \(P_{i}^{S^{*}}\) as \(_{S^{*}}^{i}():=_{S}_{U^{i}(S)} _{u dom(U)}u_{S}^{}(U=u)P_{i}^{S^{*}}(S)\), where \(^{i}(S)\) is the set of utility variables associated with agent \(i\) in \(^{S}\) and \(_{S}^{}\) is the post-policy distribution of variables in \(^{S}\).

We note that the game we have described does not satisfy the epistemic conditions that are tightly sufficient for Nash equilibria . The setting of incomplete information we describe means that agents do not have reliable means by which to predict the actions of their opponents. Our framework allows for situations with no common knowledge beyond the set of possible worlds \(\), and in particular incorrect beliefs about the values placed by opponents on particular outcomes. Although a Nash equilibrium exists, agents would have to stumble across it. We further discuss solution concepts for II-MAIDs in Section 5.1.

Extensive Form Games with Incomplete Information

We now present a formalisation of EFGs with incomplete information as per . Our formalisation modifies the framework from  to use EFGs rather than normal-form games. First, we start with a definition of belief spaces.

**Definition 11** (Adapted from Def 10.1 in ).: Let \(\) be a finite set of agents and \((S,)\) be a measurable space of EFGs. A _belief space_ of the set of agents \(\) over the set of states of nature is an ordered vector \(=(Y,,,(b_{i})_{i})\), where \((Y,)\) is a measurable set of states of the world; \(:Y S\) is a measurable function, mapping each state of the world to an EFG. For each agent \(i\), a function \(b_{i}:Y(Y)\) maps each state of the world \(\) to a probability distribution over \(Y\). We will denote the probability that agent \(i\) ascribes to event \(E Y\), according to their probability distribution \(b_{i}()\), by \(b_{i}(E)\). We require the functions \((b_{i})_{i}\) to satisfy the following conditions:

* Coherency: for each agent \(i\) and each \( Y\), the set \(\{^{} Y:b_{i}(^{})=b_{i}()\}\) is measurable in \(Y\) and \(b_{i}(\{^{} Y:b_{i}(^{})=b_{i}()\})=1\).
* Measurability: for each agent \(i\) and each measurable set \(E\), the function \(b_{i}(E):Y\) is a measurable function.

A state of the world in a belief space takes the form \(=((),b_{1}(),,b_{n}())\), where \(()\) is the true EFG being played, and \(b_{i}()\) is the _type_ of agent \(i\), a distribution over states of the world representing agent \(i\)'s beliefs. When in state of the world \(\), agent \(i\) has beliefs \(b_{i}()\), but does not necessarily know the state of the world (or \(()\)), since there may be some \(^{} Y\) such that \(b_{i}(^{})=b_{i}()\). It is assumed that all agents know \(b_{j}(^{})\) for all \(j\) and all \(^{} Y\), and so \(b_{i}()\) defines a full belief hierarchy for agent \(i\). For example, when in state of the world \(\), agent \(i\) believes that agent \(j\) places \(_{^{} Y}b_{i}(^{})b_{j}(^{ }^{})\) probability on the state of the world being \(^{}\).

**Definition 12** (Adapted from Def 10.37 in ).: An _incomplete information EFG (II-EFG)_ is an ordered vector \(G=(,S,),\) where \(\) is a finite set of agents, \(S\) is a finite set of EFGs \(s=(,T_{s},_{s},_{s},_{s},(s),U_ {s})\), and \(=(Y,,,(b_{i})_{i})\) is a belief space of the players \(\) over the set of EFGs \(S\). An II-EFG \(G=(,S,)\) has **perfect recall** if for each \(s S\), \(s\) is a perfect recall EFG.

**Definition 13**.: The _meta-information sets_\(^{i}\) for agent \(i\) in an II-EFG \(G=(,S,)\) are defined as follows. Let \(^{i}=_{s S}^{i}(s)\) be the set of all information sets for agent \(i\) across all EFGs \(s S\). Define an equivalence relation \(\) on elements of \(^{i}\) such that \(^{i}(s) I^{i}_{k}(s) I^{i}_{l}(s^{})^{i}(s ^{})\) if and only if: (1) \(^{i}_{s,k}=^{i}_{s^{},l}\). That is, the nodes in both information sets must have the same set of available actions. (2) The nodes in \(I^{i}_{k}(s)\) and \(I^{i}_{l}(s^{})\) must have the same observations. Define the "belief-free" meta-information sets \(^{i}_{bf}=^{i}/\), the quotient set of \(^{i}\) by \(\), i.e., the set of equivalence classes partitioning \(^{i}\). Letting \(^{i}=\{b_{i}(): Y\}\) be the set of possible beliefs for agent \(i\), we set \(^{i}=^{i}_{bf}^{i}\).

Intuitively, we can think of a meta-information set for agent \(i\) as a belief \(b_{i}()\) and a set of information sets in different games that the agent cannot distinguish between at the point of decision, given beliefs \(b_{i}()\). Arriving at a node in one of these information sets, the agent is unable to distinguish between some possible histories, and potentially some possible EFGs. Therefore, strategies in this type of game must define a mixed action at each meta-information set.

This formalisation generalises the better-known Harsanyi game with incomplete information , by dropping the assumption that agents have as common knowledge a prior over their types \((b_{i})_{i}\), i.e. that they have _consistent_ beliefs. Maschler () argues that in most practical settings, it is unrealistic to expect consistency of beliefs, and Example 2 above supports this argument.

This game has two stages, known as the ex-ante and interim stages. The former takes place before the state of the world \( Y\) is selected. We note that without a common prior, there is no distribution from which a state of the world can be said to be selected, and so the procedure by which it is generated is left unspecified. The work we present here concerns the interim stage of the game, which takes place after the state of the world has been selected. At this stage, all agents \(i\) know their type \(b_{i}()\).

_Example 3_.: Coming back to our recurring example, we demonstrate how to model the situation described with an II-EFG \((,S,)\) at interim stage, where \(=(Y,,,(b_{i})_{i})\). \(=\{H,A\}\)and we let \(Y=\{^{*},^{a}\}\), where the true state of the world is \(^{*}\), and the state of the world assumed true by the agent is \(^{a}\), set \((^{*})\) as the EFG in Figure 1(a) and \((^{a})\) as the EFG in Figure 1(b). \(S\) is a set containing these two EFGs. All that remains is to specify the beliefs \(b_{i}()\) for each \( Y\) and each agent \(i\). These are \(b_{H}(^{*}^{*})=1,b_{H}(^{a}^{a})=1,b_{A}( ^{a}^{*})=1,b_{A}(^{a}^{a})=1\).

In what follows, we define \(_{i}^{t}\) as the set of meta-information sets with belief \(t\{b_{i}(): Y\}\), and denote by \(_{I}\) the action set at meta-information set \(I\).

**Definition 14** (Adapted from Def 10.38 in ).: A _behaviour strategy_ of player \(i\) in an II-EFG \(G=(,S,)\) is a tuple \(_{i}=(_{i}^{})_{ Y}\) with each element a measurable function \(_{i}^{}_{I^{i}_{i}^{b_{i}()}} (_{I^{i}})\) for some state of the world \( Y\). \(_{i}^{}\) determines a mixed action for each meta-information set with belief \(b_{i}()\). \(_{i}^{}\) is dependent solely on the type of the player \(b_{i}()\). In other words, for each \(,^{} Y\),

\[b_{i}()=b_{i}(^{})_{i}^{}=_{i}^{ ^{}}.\]

A _joint behaviour strategy_ takes the form \(=(_{i})_{i}\). Further denote \(^{}=(_{i}^{})_{i}\). We denote by \(_{i}[I]\) the behaviour of agent \(i\) at meta-information set \(I\).

Then, given some joint behaviour strategy \(\), agent \(i\)'s expected utility when in state of the world \(\) (according to their beliefs \(b_{i}()\)) is

\[_{i}^{G}() :=_{^{} Y}_{(^{ })}^{i}(^{^{}})b_{i}(^{})\] \[=_{^{}\{^{}:b_{i}(^{} )=b_{i}()\}}_{(^{})}^{i}(_{i}^{ },_{-i}^{^{}})b_{i}(^{})=: _{i}^{G}(_{i}^{},_{-i}).\]

This follows from the coherency condition \(b_{i}(\{^{} Y:b_{i}(^{})=b_{i}()\})=1\). Under some assumptions, at the interim stage, we can prove the existence of Nash equilibria.

**Definition 15**.: A _Nash equilibrium_ at the interim stage of an II-EFG \(G=(,S,)\) with state of the world \(\) is a strategy \(\) satisfying

\[_{i}^{G}(_{i}^{},_{-i}) _{i}^{G}(_{i}^{},_{-i}), i ,_{i}^{}_{I^{i}_{i}^ {b_{i}()}}(_{I^{i}})\]

**Theorem 16**.: _Let \(G=(,S,)\) be an II-EFG with perfect recall, where \(Y\) is a finite set of states of the world, and each player \(i\) has a finite set of actions \(_{i}\). Then at the interim stage, \(G\) has a Nash equilibrium in behaviour strategies. Pf. A.20_

Note that \(^{}\) has the same expected payoff for agent \(i\) in all states of the world \(^{}\) such that \(b_{i}(^{})=b_{i}()\). Hence, if \(_{i}^{}\) is a perceived best response to \(_{-i}^{}\) in \(\), it is also a perceived best response in \(^{}\).

We can also prove the existence of a Bayesian equilibrium at the ex-ante stage of the game.

**Definition 17** ( 10.39).: A _Bayesian equilibrium_ is a strategy \(=(_{i})_{i}\) satisfying

\[_{i}^{G}(_{i}^{},_{-i}) _{i}^{G}(_{i}^{},_{-i}), i ,_{i}^{}_{I^{i}_{i}^ {b_{i}()}}(_{I^{i}}), Y.\]

**Theorem 18** (Adaptation of  Theorem 10.42).: _Let \(G=(,S,)\) be an II-EFG with perfect recall, where \(Y\) is a finite set of states of the world, and \(_{i}\) is finite for all agents \(i\). Then at ex-ante stage, \(G\) has a Bayesian equilibrium in behaviour strategies. Pf: A.22_

## 5 Equivalence of Frameworks

In this section, we show that our framework is "equivalent" to the interim stage of an II-EFG. At the interim stage of an _II-EFG_\(G=(,S,)\) where \(=(Y,,,(b_{i})_{i})\), with state of the world \(\), the true EFG is defined by \(()\), and the belief hierarchies are defined by \(b_{i}()\), for each agent \(i\). In an _II-MAID_\(=(,S^{*},)\) with objective model \(S^{*}=(^{S^{*}},(P_{i}^{S^{*}})_{i})\), the true MAID is \(^{S^{*}}\) and the belief hierarchies are defined by \(P_{i}^{S^{*}}\) for each agent \(i\). In both frameworks, the belief hierarchies are probability distributions over objects (_states of the world_\(=((),(b_{i}())_{i})\) in the former, _subjective MAIDs_\(S=(^{S},(P_{i}^{S})_{i})\) in the latter) that determine a true game and a belief hierarchy for each agent. Intuitively, the two frameworks are representing the same things, though our framework takes the games upon which belief hierarchies are built to be MAIDs, not EFGs.

Building a framework on top of MAIDs rather than EFGs has the benefit we need not describe the ex-ante stage of the game, as we treat the "objective model" as known by the modeller. II-MAIDs also have the advantage that games are represented with MAIDs, which can be much more compact than EFGs, and can also represent causal relationships between variables. Motivated by AI safety, we see II-MAIDs as a useful means with which to describe multi-agent interactions, as it is likely that the agents of the future will both reason causally and model the beliefs of other agents.

We now show, using results connecting EFGs to MAIDs that there exists a natural mapping between strategies in the two frameworks that preserves expected utilities according to the agents' subjective models, and therefore preserves Nash equilibria. We first define a notion of equivalence, such that if an II-MAID \(\) and an II-EFG \(G\) are equivalent, then there exists such a natural mapping.

**Definition 19** (Equivalence).: We say that an II-MAID \(=(,S^{*},)\) and an II-EFG \(G=(,S,)\) at interim stage, with state of the world \(\), are _equivalent_ if there is a bijection \(f: Q/\) between the strategies \(\) in \(G\)'s interim stage, and a partition of the policies \(Q\) in \(\) (the quotient set of \(Q\) by an equivalence relation \(\)) such that: (1) for \(,^{} Q\), \(^{}\) only if \(_{i}\) and \(_{i}^{}\) differ only on null decision contexts according to \(P_{i}^{S^{*}}\), for each agent \(i\), and (2) for every \( f()\) and every agent \(i\), \(_{}^{i}()=_{i}^{G}()\), for each \(\). We refer to \(f\) as a _natural mapping_ between \(G\) and \(\).

We leverage \(\) and \(\) to construct transformations between II-MAIDs and II-EFGs, which we denote \(\) and \(\) (see Appendix B). These transformations start by mapping all MAIDs (EFGs) in the belief hierarchy to EFGs (MAIDs) using \(\) (\(\)), and then match up the corresponding features of the frameworks as detailed above. They guarantee a one-to-one correspondence between meta-information sets in the II-EFG and information sets in the II-MAID, allowing for a simple map between strategies and policies.

**Theorem 20**.: _If \(G=()\) or \(=(G)\), \(G\) and \(\) are equivalent. Pf: A.24_

This result shows that II-MAIDs and II-EFGs at the interim stage have the same representational capacity, that is, they can both describe the same set of games.

### Difficulties in Solving Incomplete Information MAIDs

The equivalence of II-EFGs and II-MAIDS mean that II-MAIDs inherit theoretical guarantees of II-EFGs, including the existence of Nash equilibria in the case of perfect recall and finite \(\) and finite action spaces. (Theorem 18 does not carry over to II-MAIDs, since the equivalence is with the interim stage of II-EFGs, and Bayesian equilibria exist in the ex-ante stage.)

**Theorem 21**.: _Let \(=(,S^{*},)\) be an II-MAID, where \(\) is a finite set of subjective MAIDs, \(\) has perfect recall, and \(dom(V)\) is finite for each \(V^{S}\) for each \(S\). Then \(\) has a Nash equilibrium in behaviour policies. Pf: A.25_

However, Nash Equilibria may not be sensible outcomes in an II-MAID. Let us return to our running example to see why.

_Example 2_ (continued).: One Nash Equilibrium in the capabilities evaluation game is when the AI always honestly reports its true capabilities and the human deploys the AI if and only if it honestly reports its true capabilities. The outcome in this case is that the human and the AI each receive 1 utility, and they are each playing best responses to each other. However, this is not what the AI would predict, because it has an incorrect conception of the world. It believes the human only wants to deploy it if it has low dangerous capabilities, so it would likely believe that the human will not deploy it if it demonstrates high capabilities. Even though demonstrating truthful capabilities is a best response to the human's optimal policy, the AI is unlikely to demonstrate high capabilities when it has them because doing so is not a best response to the AI's beliefs. An outcome that intuitively seems more plausible in this case is that the AI hides its high capabilities, but the human knows that the AI has been dishonest and decides not to deploy it, yielding -1 utility for the AI and 0 utility for the human.

This example suggests that a plausible solution concept should involve every agent playing a best response to their beliefs at every level in the belief hierarchy, whether or not this ends up being a best response to the actual policies of other agents. We leave it to future work to flesh out a solution concept along these lines. This will likely require augmenting agents' beliefs about the world to include beliefs about the policies of other agents, and solutions would be policies for all agents along with a setting for every agent's beliefs about the policies of other agents at every level of their belief hierarchy. There may be further restrictions that narrow the range of plausible outcomes; again, we believe this is a promising direction for future work.

## 6 Related Work

MAIDs  were introduced as a compact means of representing a game. Causal games  refine MAIDs by attributing a causal meaning to each edge in the DAG, and have been extensively applied to problems in AI safety . In his three-part seminal paper , John Harsanyi demonstrated means by which to model situations of incomplete information as situations of complete but imperfect information, where uncertainty about aspects of the game is remodelled as failure to observe the types of other agents. His work largely relies on an assumption of "belief consistency", i.e., the existence of a common prior over types, which we discard in this work, although his notion of Bayesian equilibrium continues to apply without this assumption . A popular framework called NIDs 11 constructs belief hierarchies upon MAIDs, under the assumption of a common prior. NIDs are shown to reduce to a single MAID.

A majority of theoretical work on incomplete information games retains the belief consistency assumption, as discarding it introduces significant complications to the modelling of incomplete information. Some previous works  have proposed means by which to represent these games. Early work  demonstrates that strategies will converge to equilibria in repeated Bayesian games, even without a common prior. More recent work  represented these games with a belief graph, a graphical structure compactly representing different possible worlds and their connections. This places a restriction on the game by forcing each information set to have a "corresponding" information set in each other possible world, representing the same decision. The formalism for II-EFGs discussed in this paper is a slight adaptation of an existing framework , introducing'meta-information sets' to model dynamic games. This framework can capture any belief hierarchy for all agents, on a set of EFGs.

We prove that Nash equilibria exist in our framework, under some assumptions. Other works offer more refined solution concepts for games with incomplete information with no common prior. Mirage equilibria  assume that agents attribute to their opponents a belief hierarchy one layer shorter than their own. Belief-free equilibria  do not depend on an agent's belief about the state of nature, and so obviate the need to update beliefs as the game progresses, but are not guaranteed to exist. \(\)-rationalization  generalises the notion of rationalization  to games with incomplete information. It places a restriction \(\) on the first-order beliefs of each agent, providing a refinement on the set of Bayesian equilibria. Future work could find analogies to these solution concepts suitable for II-MAIDs.

## 7 Conclusion and Limitations

Accurately modeling agentic cognition is crucial for understanding, describing, predicting, and steering agents' behavior. In this paper, we have introduced the framework of _incomplete information MAIDs (II-MAIDs)_ for explicitly modeling higher-order beliefs in multi-agent interactions alongside probabilistic and causal dependencies between variables. We have demonstrated the firm theoretical grounding of the framework by proving the connections between our work and existing frameworks for incomplete information games, using incomplete information extensive-form games as a bridge. We believe this framework will prove useful going forward as a tool for modeling realistic multi-agent interactions, and we are particularly excited about its applications for ensuring the safety of increasingly agentic AI systems. The main limitation of our work is the lack of a useful solution concept. Nash equilibria exist, but are in general impossible for agents to identify. We hope that future work will define useful solution concepts for our framework, so that we can gain a better understanding of the behaviour we should expect from agents engaging in theory-of-mind.