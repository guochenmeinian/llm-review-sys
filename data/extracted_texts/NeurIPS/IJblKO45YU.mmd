# Goal-Conditioned Predictive Coding for Offline Reinforcement Learning

Zilai Zeng

Brown University

&Ce Zhang

Brown University

&Shijie Wang

Brown University

&Chen Sun

Brown University

###### Abstract

Recent work has demonstrated the effectiveness of formulating decision making as supervised learning on offline-collected trajectories. Powerful sequence models, such as GPT or BERT, are often employed to encode the trajectories. However, the benefits of performing sequence modeling on trajectory data remain unclear. In this work, we investigate whether sequence modeling has the ability to condense trajectories into useful representations that enhance policy learning. We adopt a two-stage framework that first leverages sequence models to encode trajectory-level representations, and then learns a goal-conditioned policy employing the encoded representations as its input. This formulation allows us to consider many existing supervised offline RL methods as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predictive Coding (GCPC), a sequence modeling objective that yields powerful trajectory representations and leads to performant policies. Through extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, we observe that sequence modeling can have a significant impact on challenging decision making tasks. Furthermore, we demonstrate that GCPC learns a goal-conditioned latent representation encoding the future trajectory, which enables competitive performance on all three benchmarks. Our code is available at https://brown-palm.github.io/GCPC/.

## 1 Introduction

Goal-conditioned imitation learning [13; 15; 20] has recently emerged as a promising approach to solve offline reinforcement learning problems. Instead of relying on value-based methods, they directly learn a policy that maps states and goals (e.g. expected returns, or target states) to the actions. This is achieved by supervised learning on offline collected trajectories (i.e. sequences of state, action, and reward triplets). This learning paradigm enables the resulting framework to avoid bootstrapping for reward propagation, a member of the "Deadly Triad"  of RL which is known to lead to unstable optimization when combining with function approximation and off-policy learning, and also allows the model to leverage large amounts of collected trajectories and demonstrations. The emergence of RL as supervised learning on trajectory data coincides with the recent success of the Transformer architecture [46; 20] and its applications to sequence modeling, such as GPT  for natural language and VPT  for videos. Indeed, several recent works have demonstrated the effectiveness of sequence modeling for offline RL [10; 27; 34; 44; 49; 9; 42]. Most of these approaches apply the Transformer architecture to jointly learn the trajectory representation and the policy. When the sequence models are unrolled into the future, they can also serve as "world models" [21; 22] for many RL applications. By leveraging advanced sequence modeling objectives from language and visual domains [11; 4; 24], such methods have shown competitive performance in various challenging tasks [16; 5].

Despite the enthusiasm and progress, the necessity of sequence modeling for offline reinforcement learning has been questioned by recent work : With a simple but properly tuned neural architecture (e.g. a multilayer perceptron), the trained agents can achieve competitive performance on severalchallenging tasks while taking only the current state and the overall goal as model inputs. In some tasks, these agents even significantly outperform their counterparts based on sequence modeling. These observations naturally motivate the question: Is explicit sequence modeling of trajectory data necessary for offline RL? And if so, how should it be performed and utilized?

To properly study the impact of sequence modeling for decision making, we propose to decouple trajectory representation learning and policy learning. We adopt a two-stage framework, where sequence modeling can be applied to learn the trajectory representation, the policy, or both. The connection between the two stages can be established by leveraging encoded trajectory representations for policy learning, or by transferring the model architecture and weights (i.e. the policy can be initialized with the same pre-trained model used for trajectory representation learning). These stages can be trained separately, or jointly and end-to-end. This design not only facilitates analyzing the impacts of sequence modeling, but is also general such that prior methods can be considered as specific instances, including those that perform joint policy learning and trajectory representation learning [10; 34; 49], and those that learn a policy directly without sequence modeling .

Concretely, we aim at investigating the following questions: (1) Are offline trajectories helpful due to sequence modeling, or simply by providing more data for supervised policy learning? (2) What would be the most effective trajectory representation learning objectives to support policy learning? Should the sequence models learn to encode history experiences , future dynamics , or both ? (3) As the same sequence modeling framework may be employed for both trajectory representation learning and policy learning [34; 9; 49], should they share the same training objectives or not?

We design and conduct experiments on the AntMaze, FrankaKitchen, and Locomotion environments to answer these questions. We observe that sequence modeling, if properly designed, can effectively aid decision-making when its resulting trajectory representation is used as an input for policy learning. We also find that there is a discrepancy between the optimal self-supervised objective for trajectory representation learning, and that for policy learning. These observations motivate us to propose a specific design of the two-stage framework: It compresses trajectory information into compact bottlenecks via sequence modeling pre-training. The condensed representation is then used for policy learning with a simple MLP-based policy network. We observe that goal-conditioned predictive coding (**GCPC**) is the most effective trajectory representation learning objective. It enables competitive performance across all benchmarks, particularly for long-horizon tasks. We attribute the strong empirical performance of GCPC to the acquisition of goal-conditioned latent representations about the future, which provide crucial guidance for decision making.

To summarize, our main contributions include:

* We propose to decouple sequence modeling for decision making into a two-stage framework, namely trajectory representation learning and policy learning. It provides a unified view for several recent reinforcement learning via supervised learning methods.
* We conduct a principled empirical exploration with our framework to understand if and when sequence modeling of offline trajectory data benefits policy learning.
* We discover that goal-conditioned predictive coding (GCPC) serves as the most effective sequence modeling objective to support policy learning. Our overall framework achieves competitive performance on AntMaze, FrankaKitchen and Gym Locomotion benchmarks.

## 2 Related Work

**What is essential for Offline RL?** Offline reinforcement learning aims to obtain effective policies by leveraging previously collected datasets. Prior work usually adopts dynamic programming [31; 29; 18] or supervised behavioral cloning (BC) methods [43; 3]. Recent approaches [10; 27] demonstrate the effectiveness of solving decision making tasks with sequence modeling, whereas RvS  establishes a strong MLP baseline for conditional behavioral cloning. To further understand what are the essential components for policy learning, researchers have investigated the assumptions required to guarantee the optimality of return-conditioned supervised learning , and examined offline RL agents from three fundamental aspects: representations, value functions, and policies . Another concurrent work  compares the preferred conditions (e.g. data, task, and environments) to perform Q-learning and imitation learning. In our work, we seek to understand how sequence modeling may benefit offline RL and study the impacts of different sequence modeling objectives.

**Transformer for sequential decision-making.** Sequential decision making has been the subject of extensive research over the years. The tremendous success of the Transformer model for natural language processing [11; 40] and computer vision [24; 35] has inspired numerous works that seek to apply such architectures for decision making, and similarly motivates our work. Prior work has shown how to model sequential decision making as autoregressive sequence generation problem to produce a desired trajectory , while others have explored the applications of Transformer for model-based RL [27; 47] and multi-task learning [41; 9]. Our work aims to utilize Transformer-based models to learn good trajectory representations that can benefit policy learning.

**Masked Autoencoding.** Recent work in NLP and CV has demonstrated masked autoencoding (MAE) as an effective task for self-supervised representation learning [11; 8; 4; 24]. Inspired by this, Uni[MASK] , MaskDP , and MTM  propose to train unified masked autoencoders on trajectory data by randomly masking the states and actions to be reconstructed. These models can be directly used to solve a range of decision making tasks (e.g. return-conditioned BC, forward dynamics, inverse dynamics, etc.) by varying the masking patterns at inference time, without relying on task-specific fine-tuning. In contrast, we decouple trajectory representation learning and policy learning into two stages. Unlike approaches which perform value-based offline RL (e.g. TD3, Actor-Critic) after pretraining [34; 49], our policy learning stage adopts imitation learning setup where the policy can be learned without maximizing cumulative reward.

**Self-supervised Learning for RL.** Self-supervised learning has emerged as a powerful approach for learning useful representations in various domains, including reinforcement learning (RL). In RL, self-supervised learning techniques aim to leverage unlabeled or partially observed data to pre-train agents or learn representations that facilitate downstream RL tasks. Prior work mainly focuses on using self-supervised learning to get state representations [51; 38; 36; 50; 10; 34; 9; 49; 32] or world models [22; 23; 44; 27; 12; 37]. In , the authors evaluate a broad set of state representation learning objectives on offline datasets and demonstrate the effectiveness of contrastive self-prediction. In this work, we investigate representation learning objectives in trajectory-space with the sequence modeling tool, which enables us to explore the impact of different modalities (e.g. states, actions, goals, etc.) on trajectory representation learning.

## 3 Method

We revisit the role of sequence modeling in offline reinforcement learning, from the perspectives of trajectory representation learning and policy learning. Section 3.1 describes the background knowledge on extracting policies from offline trajectories using supervised learning. Section 3.2 introduces a two-stage framework that decouples trajectory representation learning and policy learning, which serves as the basis for our investigation. In Section 3.3, we propose a specific instantiation of the two-stage framework, in which we leverage a self-supervised learning objective - Goal-Conditioned Predictive Coding (**GCPC**), to acquire performant trajectory representations for policy learning.

### Offline Reinforcement Learning via Supervised Learning

We follow prior work that leverages sequence modeling for offline reinforcement learning, and adopt the Reinforcement learning via Supervised learning (RvS) (e.g. ) setting. RvS aims to solve the offline RL problem as conditional, filtered, or weighted imitation learning. It assumes that a dataset has been collected offline, but the policies used to collect the dataset may be unknown, such as with human experts or a random policy. The dataset contains a set of trajectories: \(=\{^{i}\}_{i=1}^{N}\). Each trajectory \(^{i}\) in the dataset is represented as \(\{(s_{t},a_{t})\}_{t=1}^{H}\), where \(H\) is the length of the trajectory, and \(s_{t}\), \(a_{t}\) refer to the state, action at timestep \(t\), respectively. A trajectory may optionally contain the reward \(r_{t}\) received at timestep \(t\).

As the trajectories are collected with unknown policies, they may not be optimal or have expert-level performance. Prior work [10; 33; 30] has shown that properly utilizing offline trajectories containing suboptimal data might produce better policies. Intuitively, the suboptimal trajectories may still contain sub-trajectories that demonstrate useful "skills", which can be composed to solve new tasks.

Given the above trajectories, we denote a goal-conditioned policy \(_{}\) as a function parameterized by \(\) that maps an observed trajectory \(^{(t)}_{}=\{s_{ t},a_{<t}\}\) and a goal \(g^{(t)}\) to an action \(a_{t}\). The goal \(g^{(t)}\) is computed from \(_{t:H}\), which can be represented as either a target state configuration sampledfrom \(s_{t:H}\) or an expected cumulative reward/return-to-go computed from \(r_{t:H}\) (see Appendix A.6 for details). For simplicity of notation, we write \(_{}^{(t)}\) as \(_{}\) and \(g^{(t)}\) as \(g\). We consider a policy should be able to take any form of state or trajectory information as input and predict the next action: (1) when only the current observed state \(s_{t}\) and the goal \(g\) are used, the policy \(_{t}=_{}(s_{t},g)\) ignores the history observations; (2) when \(_{}\) is a sequence model, it can employ the whole observed trajectory to predict the next action \(_{t}=_{}(_{},g)\). To optimize the policy, a commonly used objective is to find the parameters that fit the mapping of observations to actions using maximum likelihood estimation:

\[^{*}=*{argmax}_{}_{} [_{t=1}^{||}_{}(a_{t}|_{},g)]\] (1)

### Decoupled Trajectory Representation and Policy Learning

Sequence modeling can be used for decision making from two perspectives, namely trajectory representation learning and policy learning. The former aims to acquire useful representations from raw trajectory inputs, often in the form of a condensed latent representation, or the pretrained network weights themselves. The latter aims to map the observations and the goal into actions that accomplish the task. To explicitly express the trajectory representation function, we rewrite the goal-conditioned policy function as follows:

\[_{t}=_{}(f(_{}),g)\] (2)

where \(f()\) is an arbitrary function, such as a neural network, that computes representation from \(_{}\). \(f()\) can also be an identity mapping such that \(_{}()\) directly operates on \(_{}\).

Motivated by the recent success of sequence modeling in NLP [11; 40] and CV [24; 14], both the trajectory learning function and the policy learning function can be implemented with Transformer neural networks as \(f_{}()\) and \(_{}()\). We hypothesize that it is beneficial for \(f_{}\) to condense the trajectories into a compact representation using sequence modeling techniques. We also hypothesize that it is desirable to decouple the trajectory representation learning from policy learning. The decoupling not only offers flexibility on the choice of representation learning objectives, but also allows us to study the impact of sequence modeling for trajectory representation learning and policy learning independently. We thus adopt a two-stage framework with a TrajNet \(f_{}()\) and a PolicyNet \(_{}()\). TrajNet aims to learn trajectory representations with self-supervised sequence modeling objectives, such as masked autoencoding  or next token prediction . PolicyNet aims to obtain a performant policy with the supervised learning objective in Equation 1.

We now describe an overall flow of the two networks, which we will show is general to represent recently proposed methods. In the first stage, we approach trajectory representation learning as masked autoencoding. TrajNet receives a trajectory \(\) and an optional goal \(g\), and is trained to reconstruct \(\) from a masked view of the same. Optionally, TrajNet also generates a condensed trajectory representation \(B\), which can be utilized by PolicyNet for subsequent policy learning:

\[,B=f_{}((),g)\] (3)

During the second stage, TrajNet is applied on the unmasked observed trajectory - \(f_{}(_{},g)\), to obtain \(B_{}\). PolicyNet then predicts the action \(a\) given \(_{}\) (or the current observed state \(s_{t}\)), the goal \(g\), and trajectory representation \(B_{}\):

\[a=_{}(_{},B_{},g)\] (4)

Our framework provides a unified view to compare different design choices (e.g. input information, architecture, training objectives, etc.) for representation learning and policy learning, respectively. Many existing methods can be seen as special cases of our framework. For example, prior works [34; 49] instantiate TrajNet with a bi-directional Transformer pre-trained via masked prediction, and re-use it as the PolicyNet in a zero-shot manner. To implement DT , \(f()\) is set as an identity mapping function of the input trajectory and \(()\) is trained to autoregressively generate actions. These methods learn trajectory representations and the policy jointly, with training objectives inspired by MAE  and GPT . At last, our framework recovers RvS-G/R  by having the output of \(f()\) in Eq. 2 as the last observed state \(s_{t}\) from \(_{}\).

### Goal-Conditioned Predictive Coding

Finally, we introduce a specific design of the two-stage framework, Goal-Conditioned Predictive Coding (GCPC), which is inspired by our hypotheses introduced in Section 3.2. To facilitate the transfer of trajectory representations between the two stages, we compress the trajectory into latent representations using sequence modeling, which we refer to as the bottleneck. With this design, we train the bottleneck to perform goal-conditioned future prediction, so that the latent representations encode future behaviors toward the desired goal, which are subsequently used to guide policy learning.

In the first stage (as shown in Figure 1), we use a bi-directional Transformer as TrajNet. The inputs include \(T\) state tokens, one goal token, and a few learnable slot tokens. The action tokens are ignored. We first embed the goal token and all state tokens with separate linear encoders, then apply sinusoidal positional encoding before all inputs are sent into the Transformer Encoder.

While the full trajectories can be used for trajectory representation learning with TrajNet, only observed states are available during policy learning. For notation consistency, we denote the \(k\) observed states as "history", and the \(p\) states that follow the observed states as "future". The total number of input states for stage 1 is \(T=k+p\), and for stage 2 is \(k\). Both \(k\) and \(p\) are hyperparameters that represent history window length and future window length, respectively.

Figure 1: **Stage 1 – Trajectory Representation Learning. For notation consistency between the two stages, we separate the input into observed (history) states and future states. Left: **TrajNet.** We input randomly masked history state, goal and slot tokens to the transformer encoder. The decoder takes in encoded slot tokens (the bottleneck) and a sequence of masked tokens, and reconstruct the whole trajectory. With this training objective, we encourage the bottleneck to perform predictive coding, which is conditioned on the goal and history states. Right: **All Masking Patterns.** TrajNet can be trained with different masking patterns with their corresponding reconstruction objectives. The illustration of TrajNet on the left uses “MAE-RC”, which is adopted by GCPC.

Figure 2: **Stage 2 – Policy Learning. We implement policy learning with a simple MLP as PolicyNet. We input unmasked history states and retrieve the bottleneck from pre-trained encoder. Then the bottleneck is taken as the input to the policy network. We view the bottleneck generated by pre-trained encoder as goal-conditioned latent representations for the future trajectory.**To perform goal-conditioned predictive coding, the entire future trajectory is masked. The tokens in the observed history are randomly masked (Figure 1 "MAE-RC"). The bottleneck is taken as the encoded slot tokens, which condenses the masked input trajectory into a compact representation. The decoder takes the bottleneck and \(T\) masked tokens as the input, and aims to reconstruct both the history and the future states. Our intuition is that this will encourage the bottleneck to learn representations encoding the future trajectory and provide useful signals for decision making. The training objective of the TrajNet is to minimize MSE loss between the reconstructed and the ground-truth trajectory.

Figure 2 illustrates the interface between the TrajNet and the PolicyNet. PolicyNet is implemented as a simple MLP network. The trained TrajNet has its weights frozen, and only the TrajNet encoder is used to compute the trajectory representation \(B_{}\) from \(k\) unmasked history states. PolicyNet takes the current state \(s_{t}\), goal \(g\), and bottleneck \(B_{}\) as input, and outputs an action. The training objective of PolicyNet is to minimize MSE loss between the predicted and ground-truth actions.

**Discussion**. Decoupling trajectory representation learning and policy learning allows us to explore different model's inputs and sequence modeling objectives. For example, recent work [2; 39] observed that the action sequences could potentially be detrimental to the learned policy in some tasks. In GCPC, we employ state-only trajectories as input and ignore the actions. Different objectives (e.g. masking patterns in the masked autoencoding objective) also have an impact on what is encoded in the bottleneck. Here we introduce five sequence modeling objectives for trajectory representation learning (as shown in Table 1) and study their impacts on policy learning. When using "AE-H" or "MAE-H", the bottleneck is only motivated to summarize the history states. When using "MAE-F" or "MAE-RC", the bottleneck is asked to perform predictive coding, and thus encode the future state sequences to achieve the provided goal. By default, we adopt the "MAE-RC" objective in GCPC.

## 4 Experiments

In this section, we aim to answer the following questions with empirical experiments: (1) Does sequence modeling benefit reinforcement learning via supervised learning on trajectory data, and how? (2) Is it beneficial to decouple trajectory representation learning and policy learning with bottlenecks? (3) What are the most effective trajectory representation learning objectives?

### Experimental Setup

To answer the questions above, we conduct extensive experiments on three domains from D4RL offline benchmark suite : AntMaze, FrankaKitchen and Gym Locomotion. AntMaze is a class of long-horizon navigation tasks, featuring partial observability, sparse reward and datasets that consist primarily of suboptimal trajectories. In this domain, an 8-DoF Ant robot needs to "stitch" parts of subtrajectories and navigates to a particular goal location in partially observed mazes. In addition to three mazes from the original D4RL, we also include a larger maze (AntMaze-Ultra) proposed by . Both large and ultra setup in AntMaze poses significant challenges due to the complex maze

    &  &  \\   & History & Future & History & Future \\  AE-H & Unmasked & – & ✓ & \\ MAE-H & Randomly Masked & – & ✓ & \\ MAE-F & Unmasked & Fully Masked & ✓ & ✓ \\ MAE-RC & Randomly Masked & Fully Masked & ✓ & ✓ \\ MAE-ALL & Randomly Masked & Randomly Masked & ✓ & ✓ \\   

Table 1: Objectives for Trajectory Representation Learning

Figure 3: D4RL environments used in evaluation. From left to right: AntMaze, FrankaKitchen, Walker 2D, Hopper, and Halfcheetah.

layout and long navigation horizon. FrankaKitchen is a long-horizon manipulation task, in which a 9-DoF Franka robot arm is required to perform 4 subtasks in a simulated kitchen environment (e.g. open the microwave, turn on the light). In Gym Locomotion, we evaluate our approach on three continuous control tasks: Walker 2D, Hopper and Halfcheetah. Following , we refer to the "goal" of AntMaze and Kitchen as the target state configuration, and that of Gym as the average return-to-go.

**Experimental details.** For all benchmarks, we use a two-layer transformer encoder and a one-layer transformer decoder as the TrajNet, and a two-layer MLP as the PolicyNet (see detailed hyperparameters in A.3). During policy learning, we use the pre-trained TrajNet that achieves the lowest validation reconstruction loss to generate the bottleneck. For each evaluation run, we follow  to take the success rate over 100 evaluation trajectories for AntMaze tasks. For Kitchen and Gym Locomotion, we average returns over 50 and 10 evaluation trajectories, respectively. To determine the performance with a given random seed, we take the best evaluation result among the last five checkpoints (see discussion in A.2). For performance aggregation, we report the mean performance and standard deviation averaged over five seeds for each experiment.

### Impact of Trajectory Representation Pretraining Objectives

In the two-stage framework, the pre-trained TrajNet generates trajectory representations in the form of the bottleneck, which is taken as an input to PolicyNet. To study the impact of trajectory representation learning objectives on the resulting policy performance, we implement five different sequence modeling objectives (as in Table 1) by varying masking patterns in the first stage pretraining.

Table 2 compares the performance of policies with different settings and pretraining objectives1. We note that MAE-F is the only effective masking pattern to perform zero-shot inference. After decoupling representation learning and policy learning, the MLP policy consistently outperforms the zero-shot transformer policy. This suggests good objectives for two stages could be different - by decoupling we can get the best of both worlds. Also, we observe that removing action sequences from trajectory representation pretraining yields performance gains in this task, whereas previous single-stage Transformer policy (e.g. ) usually requires action inputs to function, which further demonstrates the flexibility of our decoupled framework. With properly designed objectives, sequence modeling can generate powerful trajectory representations that facilitate the acquisition of performant policies. In both Table 2 and Figure 4, we observe that both MAE-F and MAE-RC outperform the other objectives in both AntMaze-Large and Kitchen-Partial, confirming the importance of the predictive coding objective for trajectory representation learning.

### The Role of Goal Conditioning in Trajectory Representation Pretraining

We also investigate whether goal conditioning (i.e. the goal input) in TrajNet is necessary or beneficial for learning trajectory representations. In Table 3, we observe that the objectives without performing

   Large-Play & AE-H & MAE-H & MAE-F & MAE-RC & MAE-ALL \\  Zero-shot & - & 0 & 21.2 \(\) 17.1 & 0 & 0 \\ Two-stage (w/ actions) & 12.6 \(\) 4.7 & 6.4 \(\) 3.8 & 57.2 \(\) 5.5 & 62.0 \(\) 10.7 & 11.6 \(\) 6.3 \\ Two-stage (w/o actions) & 30.2 \(\) 6.6 & 36.6 \(\) 12.6 & **76.2 \(\) 4.0** & **78.2 \(\) 3.2** & 32.0 \(\) 13.2 \\   

Table 2: **Comparison of trajectory representation pretraining objectives**. We evaluate five different objectives under three settings on large AntMaze environment: (1) Zero-shot: When actions are considered as part of the masked tokens, the pretrained TrajNet can be directly utilized as the policy; (2) Two-stage (w/o actions): Two-stage framework employs an MLP as PolicyNet, with state-only trajectories as the input to TrajNet. (3) Two-stage (w/ actions): Two-stage framework employs an MLP as PolicyNet, with state-action trajectories as the input to TrajNet.

Figure 4: Policy learning curves with different pretraining objectives on Kitchen-Partial.

future prediction is insensitive to the goal conditioning. For example, the results of AE-H and MAE-H pretraining objectives align with the intuition that merely summarizing the history should not require the goal information. However, goal conditioning is crucial for predictive coding objectives (e.g. MAE-F and MAE-RC). Removing the goal from TrajNet might result in aimless prediction and misleading future representations, which thus harm the policy. Figure 5 shows the curves of validation loss during pretraining with MAE-F objective. We observe that goal conditioning reduces the prediction error and helps the bottleneck properly encode the expected long-term future, which would benefit the subsequent policy learning. One hypothesis is that specifying the goal enables the bottleneck to perform goal-conditioned implicit planning. The planned future may provide the correct waypoints for the agent to reach the distant goal. Figure 6 illustrates a qualitative result of the latent future in AntMaze, which depicts the future states decoded from the bottleneck using the pre-trained Transformer decoder. It demonstrates that the latent future with goal conditioning helps point out the correct direction towards the target location.

   Large-Play & AE-H & MAE-H & MAE-F & MAE-RC & MAE-ALL \\  w/o GC & 32.8 \(\) 4.3 & 33.2 \(\) 11.9 & 10.0 \(\) 2.2 & 16.0 \(\) 5.3 & 36.2 \(\) 10.4 \\ w/ GC & 30.2 \(\) 6.6 & 36.6 \(\) 12.6 & **76.2 \(\) 4.0** & **78.2 \(\) 3.2** & 32.0 \(\) 13.2 \\   

Table 3: **Ablation study of goal conditioning on AntMaze-Large**. Removing the goal conditioning from TrajNet would seriously affect predictive coding objectives and harm the resulting policy performance, suggesting that the properly encoded future representation provides crucial guidance for performing long-horizon tasks. “GC” refers to goal conditioning in TrajNet.

Figure 5: Reconstruction loss on the validation set during the trajectory representation learning stage, when MAE-F objective is used. “GC” refers to goal conditioning in TrajNet.

Figure 6: **Latent future visualization on AntMaze-Large**. A qualitative result comparing the latent future with and without goal conditioning. Given the goal information, the bottleneck can encode the latent future moving in the desired direction.

### Latent Future versus Explicit Future as Policy Conditioning Variables

Prior work [27; 26] has demonstrated that planning into the future is helpful for solving long-horizon tasks. These work performs planning on the explicit future by sampling desirable future states (or transitions), while GCPC leverages goal-conditioned latent representations that encode the future state sequence. In this experiment, we examine how the implicit encoding of future information affects policy performance when it serves as a conditioning variable of PolicyNet. Specifically, we obtain the bottleneck from a TrajNet, which is pre-trained with \(p\)-length future window. The bottleneck encodes latent representations of \(p\) future states. Correspondingly, we acquire the same number of explicit future states using the pre-trained transformer decoder (see Appendix A.4). Either the bottleneck or explicit future states are taken as auxiliary inputs to the PolicyNet. In Table 4, we evaluate the policy with \(p=70\) in large AntMaze and \(p=30\) in Kitchen. The results illustrate that the bottleneck is a powerful future information carrier that effectively improves the policy performance. Different from previous approaches that estimate explicit future states with step-by-step dynamics models , using latent future representations can mitigate compounding rollout errors. Compared to diffusion-based planning methods , which require iterative refinement to obtain a future sequence, latent future avoids the time-consuming denoising step and reduces the decision latency.

### Effectiveness of GCPC

Finally, we show the effectiveness of GCPC by evaluating it on three different domains: AntMaze, Kitchen and Gym Locomotion.

Baselines and prior methods.We compare our approach to both supervised learning methods and value-based RL methods. For the former, we consider: (1) Behavioral Cloning (BC), (2) RvS-R/G , a conditional imitation learning method that is conditioned on either a target state or an expected return. (3) Decision Transformer (DT) , a return-conditioned model-free method that learns a transformer-based policy, (3) Trajectory Transformer (TT) , a model-based method that performs beam search on a transformer-based trajectory model for planning, and (4) Decision Diffuser (DD) , a diffusion-based planning method that synthesizes future states with a diffusion model and acts by computing the inverse dynamics. For the latter, we select methods based on dynamic programming, including (5) CQL  and (6) IQL . Additionally, we include three goal (state)-conditioned baselines for AntMaze and Kitchen: (7) Goal-conditioned IQL (GCIQL), (8) WGCSL  and (9) DWSL . AntMaze-Ultra is a customized environment proposed by , therefore we include TAP's performance on the v0 version of AntMaze for completeness. When feasible, we re-run the baselines with our evaluation protocol for fair comparison .

    & Large-Play & Large-Diverse & Kitchen-Mixed & Kitchen-Partial \\   &  &  \\  Explicit Future & 67.9 \(\) 9.3 & 70.0 \(\) 4.6 & 72.4 \(\) 4.5 & 73.9 \(\) 9.3 \\ Latent Future & **78.2 \(\) 3.2** & **80.6 \(\) 3.9** & **75.6 \(\) 0.8** & **90.2 \(\) 6.6** \\   

Table 4: **Comparison between latent and explicit future**. Compared to explicit future states, the latent future encoded by the bottleneck is more effective for policy learning. \(p\) is future window size.

   Dataset & BC & CQL & IQL & DT & TAP & WGCSL & GCIQL & DWSL & RvS-G & GCPC \\  Umaze & 63.4 \(\) 9.4 & **88.2 \(\) 2.3** & **92.8 \(\) 3.4** & 55.6 \(\) 6.3 & - & **90.8 \(\) 2.8** & **91.6 \(\) 4.0** & 71.2 \(\) 4.0 & 71.2 \(\) 1.3 \\ Umaze-Diverse & 63.4 \(\) 4.4 & 47.4 \(\) 2.0 & 71.2 \(\) 7.0 & 53.4 \(\) 8.6 & - & 55.6 \(\) 15.7 & **88.8 \(\) 2.1** & 74.6 \(\) 2.8 & 66.2 \(\) 5.6 & 71.2 \(\) 6.6 \\ Medium-Play & 0.6 \(\) 0.5 & 72.8 \(\) 3.7 & 75.8 \(\) 1.3 & 0 & 78.0 & 63.2 \(\) 13.7 & **82.6 \(\) 5.4** & 77.6 \(\) 3.0 & 71.8 \(\) 4.7 & 70.8 \(\) 3.4 \\ Medium-Diverse & 0.4 \(\) 0.5 & 70.8 \(\) 1.3 & 76.6 \(\) 4.2 & 0 & **85.0** & 46.0 \(\) 12.6 & 76.2 \(\) 6.3 & 74.8 \(\) 9.3 & 72.0 \(\) 3.7 & 72.2 \(\) 3.4 \\ Large-Play & 0 & 36.4 \(\) 1.3 & 50.0 \(\) 9.7 & 0 & 74.0 \(\) 1.3 & 40.0 \(\) 16.2 & 15.8 \(\) 27.3 & 55.6 \(\) 7.6 & **78.6 \(\) 3.2** \\ Large-Diverse & 0 & 36.0 \(\) 3.3 & 52.6 \(\) 6.9 & 0 & **82.0** & 2.4 \(\) 4.5 & 29.8 \(\) 6.8 & 19.0 \(\) 2.8 & 25.2 \(\) 4.8 & **80.6 \(\) 3.9** \\ Ultra-Play & 0 & 18.0 \(\) 13.3 & 21.2 \(\) 7.5 & 0 & 22.0 \(\) 0.4 & 20.6 \(\) 7.6 & 25.2 \(\) 3.0 & 25.6 \(\) 6.7 & **56.6 \(\) 9.5** \\ Ultra-Diverse & 0 & 9.6 \(\) 14.6 & 17.8 \(\) 4.0 & 0 & 26.0 & 0 & 28.4 \(\) 11.8 & 25.0 \(\) 8.6 & 26.4 \(\) 7.7 & **54.6 \(\) 10.3** \\  Average & 16.0 & 47.4 & 57.3 & 13.6 & - & 32.4 & 57.3 & 47.8 & 49.2 & **69.4** \\   

Table 5: Average normalized scores of GCPC against other baselines on **AntMaze**. TAP’s performances are taken from the original paper . For other baselines, we obtain their performance by re-running author-provided or our replicated implementations with our evaluation protocol. Following , we bold all scores within 5 percent of the maximum per task (\(\) 0.95 \(\) max).

Our empirical results on AntMaze and Kitchen are presented in Table 5 and Table 6. We find our methods outperform all previous methods on the average performance. In particular, on the most challenging large and ultra AntMaze environments, our method achieves significant improvements over the RvS-G baseline, demonstrating the efficacy of learning good future representations using sequence modeling in long-horizon tasks. Table 7 shows the results on Gym Locomotion tasks. Our approach obtains competitive performance as prior methods. We also notice that compared to Decision Transformer, RvS-R can already achieve strong average performance. This suggests that for some tasks sequence modeling may not be a necessary component for policy improvement. With a large fraction of near-optimal trajectories in the dataset, a simple MLP policy may provide enough capacity to handle most of the locomotion tasks.

## 5 Conclusion

In this work, we aim to investigate the role of sequence modeling in learning trajectory representations and its utility in acquiring performant policies. To accomplish this, we employ a two-stage framework that decouples trajectory representation learning and policy learning. This framework unifies many existing RvS methods, enabling us to study the impacts of different trajectory representation learning objectives for sequential decision making. Within this framework, we introduce a specific design - Goal-conditioned Predictive Coding (GCPC), that incorporates a compact bottleneck to transfer representations between the two stages and learns goal-conditioned latent representations modeling the future trajectory. Through extensive experiments, we observe that the bottleneck generated by GCPC properly encodes the goal-conditioned future information and brings significant improvement over some long-horizon tasks. By empirically evaluating our approach on three benchmarks, we demonstrate GCPC achieves competitive performance on a variety of tasks.

**Limitations and Future work.** GCPC models the future by performing maximum likelihood estimation on offline collected trajectories, which may predict overly optimistic future behaviors and lead to suboptimal actions in stochastic environments. Future work includes discovering policies that are robust to the environment stochasticity by considering multiple possible futures generated by GCPC. Another limitation of our work is that GCPC may not be sufficient to maintain high accuracy for long-term future prediction when high-dimensional states are involved, which may potentially be tackled by leveraging foundation models to acquire representations for high-dimensional inputs.

   Dataset & BC & CQL & IQL & DT & DD & WGCSL & GCIQL & DWSL & RvS-G & GCPC \\  Mixed & 48.9 \(\) 0.7 & 52.4 & 53.2 \(\) 1.6 & 50.7 \(\) 7.1 & 65.0 & **77.8 \(\) 3.6** & **74.6 \(\) 1.9** & **74.6 \(\) 0.6** & 69.4 \(\) 4.2 & **75.6 \(\) 0.8** \\ Partial & 41.3 \(\) 3.7 & 50.1 & 59.7 \(\) 8.3 & 48.6 \(\) 9.5 & 57.0 & 75.2 \(\) 6.4 & 74.7 \(\) 4.1 & 74.0 \(\) 5.8 & 71.7 \(\) 7.9 & **90.2 \(\) 6.6** \\  Average & 45.1 & 51.3 & 56.5 & 49.7 & 61.0 & 76.5 & 74.7 & 74.3 & 70.6 & **82.9** \\   

Table 6: Average normalized scores of GCPC against other baselines on **Kitchen**. CQL and DD’s results are taken from . For other baselines, we obtain their performance by re-running author-provided or our replicated implementations with our evaluation protocol. Following , we bold all scores within 5 percent of the maximum per task (\(\) 0.95 \(\) max).

   Dataset & Environment & BC & CQL & IQL & DT & TT & DD & RvS-R & GCPC \\  Medium-Expert & HalfCheetah & 61.9 \(\) 58 & **87.7 \(\) 7.0** & **92.4 \(\) 0.4** & 88.8 \(\) 2.6 & **95** & **90.6** & **93.4 \(\) 93** & **94.0 \(\) 93** \\ Medium-Expert & Hopper & 55.1 \(\) 28 & **110.5 \(\) 2.9** & 102.2 \(\) 7.3 & **108.4 \(\) 2.0** & **110** & **111.8** & **111.3 \(\) 2.1** & **111.7 \(\) 0.4** \\ Medium-Expert & Walker2d & 100.4 \(\) 13.4 & **110.4 \(\) 6.6** & **110.2 \(\) 6.7** & **108.6 \(\) 6.3** & **101** & **108.8** & **109.6 \(\) 0.4** & **109.0 \(\) 0.2** \\  Medium & HalfCheetah & 43.0 \(\) 0.4 & **47.1 \(\) 6.3** & **47.7 \(\) 0.2** & 42.9 \(\) 0.2 & **46.9** & **49.1** & 44.2 \(\) 0.2 & 44.5 \(\) 0.5 \\ Medium & Hopper & 55.8 \(\) 2.8 & 70.1 \(\) 1.6 & 69.2 \(\) 3.2 & 67.8 \(\) 4.2 & 61.1 & **79.3** & 65.1 \(\) 5.2 & 68.0 \(\) 4.4 \\ Medium & Walker2d & 74.1 \(\) 3.2 & **83.5 \(\) 6.5** & **84.5 \(\) 1.8** & 76.5 \(\) 1.6 & 79 & **82.5** & 78.4 \(\) 2.6 & 78.0 \(\) 2.4 \\  Medium-Replay & HalfCheetah & 37.2 \(\) 1.3 & **45.4 \(\) 4.3** & **44.9 \(\) 0.3** & 37.8 \(\) 0.9 & 41.9 & 39.3 & 40.2 \(\) 0.2 & 40.7 \(\) 1.5 \\ Medium-Replay & Hopper & 33.7 \(\) 8.5 & **96.2 \(\) 1.9** & 93.9 \(\) 9.1 & 78.0 \(\) 1.6 & 91.5 & **100** & 88.5 \(\) 12.9 & **94.2 \(\) 3.5** \\ Medium-Replay & Walker2d & 19.2 \(\) 7.3 & **79.8 \(\) 1.6** & **78.6 \(\) 5.7** & 72.5 \(\) 3.3 & **82.6** & 75 & 71.0 \(\) 5.1 & 77.6 \(\) 9.8 \\  Average & & 53.4 & **81.2** & **80.6** & 75.7 & **78.9** & **81.8** & **78.0** & **79.7** \\   

Table 7: Average normalized scores of our approach against other baselines on **Gym Locomotion**. TT and DD’s results are taken from . For other baselines, we obtain their performance by re-running author-provided or our replicated implementations with our evaluation protocol. Following , we hold all scores within 5 percent of the maximum per task (\(\) 0.95 \(\) max).