# The Karp Dataset

Mason DiCicco

Department of Computer Science

Worcester Polytechnic Institute

Worcester, MA 01609

mtdicicco@wpi.edu

&Eamon Worden

Department of Computer Science

Worcester Polytechnic Institute

Worcester, MA 01609

eaworden@wpi.edu

&Conner Olsen

Department of Computer Science

Worcester Polytechnic Institute

Worcester, MA 01609

caolsen@wpi.edu

&Nikil Gangaram

Department of Computer Science

Worcester Polytechnic Institute

Worcester, MA 01609

nrgangaram@wpi.edu

&Daniel Reichman

Department of Computer Science

Worcester Polytechnic Institute

Worcester, MA 01609

dreichman@wpi.edu

&Neil Heffernan

Department of Computer Science

Worcester Polytechnic Institute

Worcester, MA 01609

nth@wpi.edu

###### Abstract

Understanding the mathematical reasoning capabilities of Large Language Models (LLMs) is a central topic in the study of artificial intelligence. This new domain necessitates the creation of _datasets of reasoning tasks_ for both training and benchmarking the performance of LLMs. To this end, we introduce the _Karp dataset_: The first dataset composed of detailed proofs of NP-completeness reductions. The reductions vary in difficulty, ranging from simple exercises of undergraduate courses to more challenging reductions from academic papers. We compare the performance of state-of-the-art models on this task and demonstrate the effect of fine-tuning with the Karp dataset on reasoning capacity.

## 1 Introduction

Perhaps the concept receiving the most attention in theoretical computer science is that of a _reduction_. Loosely speaking, a reduction between decision problems \(A\) and \(B\) is a mapping \(f\) such that: If \(x\) is an input to \(A\), then \(f(x)\) is an input to \(B\), and the answer to \(x\) is "yes" if and only if the answer to \(f(x)\) is "yes." Efficiently computable reductions can be used to leverage algorithms that solve \(B\) in order to solve \(A\). Furthermore, efficient reductions can establish hardness results: if \(A\) is believed to be intractable, and \(A\) reduces to \(B\) efficiently, then \(B\) is intractable as well, since an efficient algorithm for \(B\) can be used to solve \(A\). This simple observation is at the core of the theory of NP-completeness, which is the topic of thousands of papers and an influential monograph Garey and Johnson (1979).

Our goal is to study the capabilities of Large Language Models (LLMs) and their potential to influence formal mathematics. To that end, we built a new dataset of 90 NP-hardness proofs (reductions) to be used for evaluation and training of language models. We are not aware of the study of LLMs for proving new NP-hardness results (by constructing reductions) or reproving and verifying known results. We believe that aiming language models at reductions _in particular_ has great potential tobenefit our understanding of their reasoning capabilities and applicability to formal mathematics. This is because:

* Finding a reduction between two problems is a high-level reasoning task. Imbuing LLMs with the ability to construct reductions could lead to improved reasoning capabilities.
* It is feasible to construct dozens of examples of reductions that are theoretically interesting, go beyond symbolic manipulations to prove mathematical identities, and have a short (several paragraphs) proof using natural language. The existence of short yet difficult-to-find proofs hints that such proofs can be found automatically with reasonable computing resources (e.g., memory, training time).
* Such datasets are challenging to construct in other mathematical domains. Current datasets of mathematical problems (e.g., Hendrycks et al. (2021)) that are used to evaluate math capabilities of large language models generally focus on a single numerical or symbolic outcome.

### Related work

There has been extensive recent research directed toward using generative AI, neural networks, and Interactive Theorem Provers (ITP) in pushing the boundaries of mathematics (Azerbayev et al., 2021; Buzzard, 2020; Hendrycks et al., 2021; Lample et al., 2022; Polu et al., 2022; Szegedy, 2020) including proving new theorems as well as reproving known theorems. To our knowledge, they do not include proofs of NP completeness using reductions. Very few works seem to have studied automatically constructing reductions toward establishing NP-completeness results. One of the more advanced datasets similar to ours is The CLRS Algorithmic Reasoning Benchmark of Velickovic et al. (2022), which predicts the trajectories of various algorithms using an algorithmic model but explicitly avoids NP-Hard problems. Motivated by the education domain, Creus et al. (2014) study the problem of testing the correctness of reductions using SAT-solvers and designated programming language REDNP to establish NP-completeness. One bottleneck noted in proof verification using SAT solvers is the large size of SAT formulas obtained in the process of verification. Recently, Zhang et al. (2022) introduced Karp, a language for programming and testing reductions, motivated by the educational domain as well. Karp is a Racket-esque framework that can be used to define computational problems as well as reductions between them. In addition to providing a systematic way to construct reductions, Karp automatically tests the correctness of reductions. The Karp dataset contains significantly fewer solved questions compared to most math datasets. It does not use generative AI to find reductions and their proofs.

Related datasets such as MATH (Hendrycks et al., 2020), MathQA (Amini et al., 2019), GSM8K (Cobbe et al., 2021), MGSM (Shi et al., 2022), ProofWriter (Tafjord et al., 2020) and others have offered new ways to evaluate the mathematical reasoning and proof generation capabilities of language models. The MATH dataset consists of challenging problems taken from high school math competitions, testing a model's elementary problem-solving skills across various domains of mathematics. The GSM8K and MGSM (multilingual GSM8K) datasets focus on grade-school math problems, assessing the model's ability to perform arithmetic reasoning and handle multi-step calculations. ProofWriter evaluates a model's proficiency in generating natural language proofs for elementary logical inference tasks, emphasizing multi-hop reasoning. While these datasets are instrumental in testing general mathematical and logical reasoning, they are completely disjoint from the task of constructing reductions for NP-completeness proofs. Reductions in computational complexity involve a unique blend of algorithmic thinking, formal proof techniques, and an understanding of computational problems' intrinsic properties. This gap highlights the need for specialized resources.

### Evaluating large language models on the Karp dataset

Datasets such as MATH and MGSM are valuable because they allow for standardized comparison of the capabilities of language models, but LLMs now excel at scoring highly on them. For instance, GPT-4o (Achiam et al., 2023) scores over 90% on GSM8K, 75% on the MATH dataset, and around 86% on MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2020). While impressive, there are concerns that LLMs have been overfit on the testing datasets due to their availability on the internet. Moreover, achieving a high level of performance on GSM8K, which consists of grade-school math problems, only indicates that LLMs are comparable to highly skilled eighth graders. As more advanced LLMs such as Strawberry (also known as o1) are released, researchers will be aiming towards matching the problem-solving capacity of undergraduate or even PhD level students. This necessitates datasets of complex higher-education-level questions such as reductions.

## 2 The Karp dataset

Our dataset consists of detailed natural language descriptions of dozens of reductions establishing NP-hardness proofs. These proofs are significantly more involved and labor-intensive to generate relative to math problems with a numerical answer (Hendrycks et al., 2021) or a sequence of computational steps as a solution (Cobbe et al., 2021). Every reduction in the dataset is sourced from well-known literature such as Garey and Johnson (1979); Papadimitriou (1994); Dasgupta et al. (2006). The dataset also contains natural language versions of Karp's 21 original NP-complete problems (Karp, 2010). Other sources include academic papers Garey et al. (1974, 1976); Fomin et al. (2013); Aloise et al. (2009) and dedicated surveys of NP-completeness Ausiello et al. (2012) and the references therein.

Many proofs of NP-completeness in the literature compress proofs of claims that are somewhat tedious to prove formally, and it has been observed that some proofs contain inaccuracies (Zhang et al., 2022). In our proofs, we attempted to avoid including unproven claims, emphasizing clarity at the cost of verbosity. Such proofs also often rely on diagrams, which we convert to natural language for LLM comprehension. As a result, the proofs in our dataset are somewhat longer than proofs in other datasets, altogether spanning over 170 pages. We avoided including problems with highly complex proofs that require more than two pages. The reductions in the dataset have lengths between 1000 and 6000 characters and have an average length of approximately 2000 characters. The distribution of lengths is depicted in Figure 2. Some examples of reductions can be found in Appendix D, and the full lists of problems and reductions can be found in Tables 5 and 6, respectively. _We will share the full dataset with interested researchers upon request._

FormattingThe dataset consists of reductions (in the form of LaTeX-typeset theorems) between computational problems whose definitions are also provided. Reductions in the dataset adhere to a highly structured template: A precise definition of the mapping followed by a proof of correctness (See Figure 1). The language is fairly expository and instructive: While all the content of a formal proof is present, we frequently include conceptual justification of non-trivial logical steps.

Omitted detailsIn all of our proofs, we omit a key concept needed to establish NP-completeness: Polynomial-time computability and verification. For example, in a proper NP-completeness proof, the mapping from one decision problem to another must be possible to implement efficiently1, otherwise the reduction is vacuous (e.g., if exponential time is allowed, then one could just brute-force the answer to the original problem.) Efficiency of a reduction is often easy (but tedious) to prove, and we maintain that this holds true for all problems in our dataset. Hence, we choose to mask these details.

Figure 1: Our reduction template (left) compared to MATH (middle) and GSM8k (right)

## 3 Experiments

In contrast to computations and formal logical deductions, natural-language mathematical proofs resist straightforward automatic verification. Due to this limitation, all models are manually evaluated on a small, fixed test set by a human expert (a graduate student in theoretical computer science).

Test setWe initially evaluated our models on a randomly chosen set of 8 reductions from the dataset, at the level of undergraduate homework assignments (test set). After our initial evaluation, Strawberry was released and achieved significantly better results on the test set. To gain a better understanding of the capabilities of Strawberry, we constructed an additional list of eight more challenging reductions (challenge set) that did not belong to the original dataset.

PromptsModels are evaluated on their responses to a highly structured prompt, which asks for a reduction between two decision problems. The prompt provides a LaTeX template for the reduction, which matches the format of the dataset, states the two problems and any necessary definitions, and asks for a detailed reduction. Full examples of prompts can be found in Appendix E.

ScoringCompleted reductions receive a score of \(0\), \(1\), or \(2\), where \(0\) represents a completely incorrect answer, \(1\) reflects a construction that contains significant yet sizable flaws, and \(2\) indicates a fully or nearly correct reduction with only minor errors. If the response contains superficial bugs (such as LaTeX-compilation errors), we repair these and proceed with normal scoring.

ModelsWe compare the performance of OpenAI's recent Strawberry model, the Llama70B-Instruct base model (Touvron et al., 2023) as well as our fine-tuned Llama70B-Instruct model, which we call LlamaReduce. The fine-tuning method we used is described in Appendix B.

ResultsStrawberry achieves impressive averages of 1.5 on the test set, and 0.875 on the challenge set. Interestingly, Strawberry even gave a more compact version of a current well-known reduction in the challenge set (See Appendix E). This outperforms the base Llama model, which scores 0.875 on the test set and 0.375 on the challenge set. The only problem that Llama answered correctly from the challenge set was _NAE4SAT to Set Splitting_, whose difficulty is relatively low. LlamaReduce clearly benefited from fine-tuning on the Karp dataset, as it was able to score 1.25 and 0.5 on the test and challenge sets respectively. The complete breakdown of scores is compiled in Tables 2 and 3 in Appendix C.

These preliminary findings, especially the low scores achieved on the challenge set, suggest that reductions are a challenging task for LLMs, leaving room for potential improvement. For easier reductions (such as those in the test set), fine-tuning was beneficial in improving performance. The impressive performance of Strawberry provides additional evidence that prompt engineering has a significant effect on problem-solving capacity, particularly on problems from the test set (at the level of homework questions from an undergraduate course covering NP-completeness). Both prompt engineering and fine-tuning appear to be less effective for improving performance for the harder reductions such as those in the challenge dataset.

We also evaluate LlamaReduce on the MATH and MGSM datasets. Results are in Appendix C.

  
**Benchmark** & **Strawberry** & **Llama** & **LlamaReduce** \\  Test set & 1.5 & 0.875 & 1.25 \\ Challenge set & 0.875 & 0.375 & 0.5 \\   

Table 1: Average scores achieved by Strawberry, Llama, and LlamaReduce on the two problem sets. In the second row, LlamaReduce has been fine-tuned on the entire Karp dataset, while in the first row, the test set is held out during training.

## 4 Conclusion

We have constructed the Karp dataset consisting of reductions establishing NP-completeness. Future work could examine extending the dataset with additional reductions (e.g., reductions establishing hardness of approximation of NP-hard optimization problems Arora et al. (1998); Feige et al. (1996); Dinur (2007)). Using the Karp dataset as well as generative AI more broadly to discover new reductions and simplify known NP-completeness proofs is an exciting future direction.

The lack of automatic verification for natural language proofs of NP-completeness is a bottleneck in creating a larger dataset. In our experiments, language models failed to judge the correctness of reductions. We suspect that a transformation from natural language to more structured representations (e.g., code, formal math, the Karp language) is a required step to allow automatic verification.

  
**Problem** & **Strawberry** & **Llama** & **LlamaReduce** \\ 
3Coloring to Planar 3Coloring & 1 & 0 & 0 \\
3SAT to Independent Set & 2 & 1 & 1 \\
3SAT to NAE4SAT & 1 & 0 & 2 \\ Hamiltonian Path to K-SpanningTree & 0 & 0 & 0 \\ Independent Set to Set Packing & 2 & 1 & 2 \\ Independent Set to Vertex Cover & 2 & 2 & 1 \\ Partition to Bin Packing & 2 & 2 & 2 \\ Partition to Knapsack & 2 & 1 & 2 \\ 
**Average** & 1.5 & 0.875 & 1.25 \\   

Table 2: Scores achieved by each model on each problem in the test set.