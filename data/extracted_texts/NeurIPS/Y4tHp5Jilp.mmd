# A Simple yet Universal Framework for Depth Completion

Jin-Hwi Park

AI Graduate School

 GIST

jinhwipark@gm.gist.ac.kr &Hae-Gon Jeon

AI Graduate School

 GIST

haegonj@gist.ac.kr

###### Abstract

Consistent depth estimation across diverse scenes and sensors is a crucial challenge in computer vision, especially when deploying machine learning models in the real world. Traditional methods depend heavily on extensive pixel-wise labeled data, which is costly and labor-intensive to acquire, and frequently have difficulty in scale issues on various depth sensors. In response, we define **Un**iversal **D**epth Completion (UniDC) problem. We also present a baseline architecture, a simple yet effective approach tailored to estimate scene depth across a wide range of sensors and environments using minimal labeled data. Our approach addresses two primary challenges: _generalizable knowledge_ of unseen scene configurations and _strong adaptation_ to arbitrary depth sensors with various specifications. To enhance versatility in the wild, we utilize a foundation model for monocular depth estimation that provides a comprehensive understanding of 3D structures in scenes. Additionally, for fast adaptation to off-the-shelf sensors, we generate a pixel-wise affinity map based on the knowledge from the foundation model. We then adjust depth information from arbitrary sensors to the monocular depth along with the constructed affinity. Furthermore, to boost up both the adaptability and generality, we embed the learned features into hyperbolic space, which builds implicit hierarchical structures of 3D data from fewer examples. Extensive experiments demonstrate the proposed method's superior generalization capabilities for UniDC problem over state-of-the-art depth completion. Source code is publicly available at https://github.com/JinhwiPark/UniDC.

## 1 Introduction

Acquiring accurate and dense depth maps is crucial for various computer vision tasks such as scene understanding [1; 2; 3; 4], 3D reconstruction [5; 6; 7; 8], and autonomous driving [9; 10; 11]. Traditional methods like dense stereo matching [12; 13; 14] often face challenges of handling occlusion and varying lighting conditions between viewpoints. Additionally, depth maps obtained from active depth sensors [15; 16] like LiDAR and Time-of-Flight cameras typically exhibit low resolutions. As a solution to the above problems, depth completion has been widely studied. The goal of depth completion is to obtain a depth map from a pair of an image and a low-resolution depth map (often sparse depth map) taken by active sensors. The depth completion aims to convert a sparse depth map into a dense depth prediction by propagating it with an image-based affinity map.

Recent advances in learning-based depth perceptions have markedly improved the performance in this domain; however, most approaches are still tailored to specific settings and struggle to generalize to new environments or sensor types. While generalizable knowledge can be achieved by training huge models with large-scale and diverse datasets, acquiring accurate and dense depth information as ground-truth data is prohibitively expensive and time-consuming, which makes such a generalization model for metric scale 3D depth prediction infeasible in practice. Moreover, there exist numerous types of active depth sensors and complex scenarios in the real world. Unfortunately, only twobenchmark datasets (_e.g._, KITTI  and NYU dataset ) are predominantly utilized in relevant research fields. Considering the accessibility of various industrial scenarios and the extremely high annotation cost, it is desirable to explore a few-shot learning approach capable of universal depth prediction for both arbitrary sensors and environments.

In response to the growing needs of both industry and the research community, in this work, we define a new problem, called **Univ**eral **D**epth **C**ompletion (UniDC), and present a baseline architecture and its advanced version. Our key insight of the baseline model for UniDC is to utilize pre-trained knowledge from a foundation model for monocular depth estimation, which provides depth-aware information enriched with high-resolution contextual information. Previous works typically exploit entangled representations of an image and corresponding depth data by concatenating them in an input layer, which reduces the generality of the foundation model. A contemporary work  proposes a sensor-agnostic depth completion with a depth prompting module, which mitigates the sensor bias problem by disentangling image and depth modalities. Since the depth representation is optimized with respect to a specific scene environment, it has limitations in out-of-domain situations, such as the environmental transition from indoor to outdoor, and vice versa.

To resolve this limitation, we design a simple baseline architecture using the foundation model. By excluding the training procedure for a new encoder to represent depth data, we achieve a high generality of the model across various sensors regardless of scene configurations. The proposed architecture consists of three sequential steps: (1) extraction of depth-aware features from the foundation model; (2) sparse-to-dense conversion based on the depth-aware information; (3) refinement of the converted depth with a pixel-wise affinity map constructed based on high-resolution details of the input image. For more details, the sparse-to-dense conversion aggregates adjacent depth values based on the high-resolution pixel-wise features from the foundation model. In the depth refinement process, we adopt a spatial propagation module with a multi-kernel affinity map.

We next boost up the baseline architecture by taking advantage of hyperbolic embedding. As stated in [20; 21], the natural capacity of hyperbolic spaces encourages capturing the implicit hierarchical structure of 3D data. In particular, this capability alleviates bleeding errors in the spatial propagation process . To ensure adaptability and generality, we also design a multi-curvature approach for producing multiple affinity maps in the refinement stage. The effectiveness of our models is demonstrated across a variety of scenarios and datasets, confirming its superior generalization and robustness in different sensor setups and scene configurations. We also conduct extensive experiments and analyses to validate the efficacy of the proposed model.

## 2 Related Works

_Depth Completion._ Image-guided depth completion aims to predict dense depth maps from an RGB image and its synchronized sparse depth acquired by depth sensors. A work in  introduces a deep regression model that significantly enhances prediction accuracy over the existing monocular depth estimation method , which utilizes only RGB image as input. However, depth maps from the direct regression method often suffer from blurry artifacts and distortions at object boundaries . To address these issues, several works have introduced spatial propagation networks (SPNs) [25; 26; 27; 28; 29] as refinement modules. SPNs iteratively update the output of direct-regression methods by aggregating neighboring pixels over a reference pixel. Nonetheless, these models are typically tailored for specific depth sensors, such as the 64-Line Velodyne LiDAR  in KITTI outdoor dataset  and Kinect  for NYUv2 indoor dataset .

To alleviate this limited usage of SPNs, several studies have explored sensor-/domain-agnostic depth completion. SpAgNet  develops a model agnostic to the sparsity of depth points by incorporating sparse depth representions into a depth decoder. Another work  takes the use of both sparse metric depth and data-driven priors from a monocular depth prediction network for domain-agnostic depth completion. DepthPrompting  solves sensor bias problems with a prompt engineering. Despite these efforts, they still face challenges with a cross-domain generalization  and an issue on a limitation of sensors' scan ranges, which causes an overfitting problem [32; 33].

_Usage of Foundation Model in Downstream Task._ Foundation models, designed for various downstream tasks, have revolutionized both natural language processing and computer vision fields. In particular, in the computer vision field, these foundation models excel in high-level visual perception tasks such as image recognition [34; 35; 36] and image captioning [35; 37; 36]. Those vision foundation models provide benefits for strong adaptation to various tasks via tuning methods [38; 39; 40; 41] and feature adaptation methods [42; 43; 44]. In low-level tasks like depth computation, several works [45; 46; 47] create diverse datasets for zero-shot generalization capabilities, while others [48; 49; 50] fine-tune the text-to-image model  to utilize diffusion priors for better generalization which guides them to keep geometric details.

Hyperbolic Geometry for Visual DataHyperbolic embedding for efficient learning-based approaches [52; 53] has gained interest. The Hyperbolic embedding has validated its ability to effectively represent complex data as hierarchical structures in low-dimensional spaces, offering a distinct advantage over Euclidean embeddings. This unique capability promotes the design of hyperbolic neural networks, and is applicable for a range of applications such as hierarchical recognition [54; 55; 56], retrieval [57; 58; 59], dealing with uncertainty [60; 61; 62], and generative learning on scarce data [63; 64; 65; 66]. Especially, hyperbolic methods have been shown to be effective in addressing low-shot visual problems [67; 68; 69; 60; 70], modeling complex 3D data [20; 21] and measuring pixel-wise similarity . In this work, we devise the hyperbolic version of the proposed architecture to make both the generalizable power and understanding 3D depth data better.

## 3 Baseline Architecture

We present a simple yet effective architecture to achieve a generalizable depth completion model for unseen environments with only minimal data. Firstly, in Sec.3.1, we explain the rationale for adopting a monocular depth foundation model to simultaneously achieve sensor-/domain-agnostic depth completion. We then propose a baseline model architecture for UniDC, which integrates the pre-trained foundation model with both the depth propagation and refinement process in Sec.3.2.

### Rationale: Foundation Model Usage in UniDC

_Difficulties to generalize depth completion._ The two major obstacles to sensor-/domain-agnostic depth completion are the high cost of dense depth data acquisition and the scale variance across different sensors. First, capturing dense depth data on a metric scale is expensive. For example, Velodyne 64-line LIDAR, used in the KITTI dataset, provides high-quality depth information but has less than 6% density relative to the number of pixels in its synchronized image. Second, sensors have their own scanning ranges, hindering the development of a universal solution. As shown in Fig. 1-(a,b), the previous frameworks learn the joint representation of image and depth, and the depth prompting module, respectively. However, the trained encoder is vulnerable to handling different sensors due to a bias towards specific scanning ranges.

_Usage of depth-aware knowledge from depth foundation model._ Although the depth foundation model produces relative depth maps, we can measure pixel-wise similarity using them. For example, we are able to distinguish between foreground and background regions only with the relative depth maps and account for depth boundaries between objects in scenes. Therefore, based on this depth-aware information, it will be the most probable solution that propagates a given sparse metric depth into the remaining pixels in an input image space without any additional learning for the depth.

_Revisiting how to use SPN._ SPN  constitutes a core component in most state-of-the-art (SoTA) depth completion and is typically invoked as a final refinement step. The SPN refinement module takes initial depth and pixel-wise affinity as input and yields refined dense depth by iteratively updating its output. During training, the previous methods (Fig.1-(a,b)) jointly optimize the pixel-wise affinity and initial depth. However, the joint optimization scheme hinders the fast adaptation to new environments because learned weights are asked to have both domain- and depth-specific features. Furthermore,

Figure 1: Illustrations of conventional SPN, sensor agnostic model  and ours. Our approach uses hyperbolic-based depth completion in three stages: generating an initial depth, constructing a pixel-wise affinity, and refining the depth based on the affinity.

DepthPrompting (Fig.1-(b)), which employs a depth foundation model for a relative-scale depth map as initial depth of SPN, struggles to adapt to new environments with a limited data. We want to eliminate the possibility of degeneracy, so we devise a sparse-to-dense conversion with a foundation model to make an initial dense depth. In Fig.1, different from the coarse initial depth seen in traditional SPNs, our method provides promising results even before the SPN refinement step.

### Architecture Design

Considering the facts discussed in Sec.3.1, we devise an effective baseline architecture. We first utilize pre-trained knowledge from a foundation model tailored for monocular depth estimation, which provides pixel-wise relative distances (a.k.a. relative scene depth) from a camera along with high-resolution contextual information. Thanks to the knowledge, our baseline architecture becomes simpler due to no need for an additional encoder to represent depth data from arbitrary sensors. Our model operates in three stages: 1 extraction of the relative depth-aware features from the foundation model, 2 propagation of spare depth from arbitrary sensors based on the depth-aware features, and 3 refinement of it with a pixel-wise affinity map constructed from the depth-aware features. This scheme not only simplifies the architectural complexity, but also enhances the adaptability and performance across diverse sensing scenarios. The overall algorithm scheme is summarized in Alg.1.

_Tuning strategy for foundation model._ Given a single image \(I^{3 H W}\), the pre-trained depth model \(f_{}\) outputs multi-scale intermediate features \(E\) and relative depth \(D_{}\) as below:

\[E,D_{}=f_{}(I,_{f_{}}),\] (1)

where \(_{f_{}}\) denotes parameters of the foundation model.

Since the foundation model is trained to estimate relative depth from single images, they inherently face limitations when handling metric scale depths. To reduce the modality discrepancy, our approach involves an integration of an additional loss term to refine the foundation model by minimizing the difference between \(D_{}\) and its Ground Truth (GT) depth \(D_{gt}\) for valid pixels \(v V\). Let \(_{v}= D_{}(v)- D_{gt}(v)\), the loss \(L_{}\) is defined as below:

\[L_{}(D_{},D_{gt})=_{v V }(_{v})^{2}-}(_{v V} _{v})^{2},\] (2)

where we set \(=0.85\) in all experiments as in . We also implement a bias tuning [38; 39], shown to be more effective for dense prediction tasks than other tuning protocols [72; 38]. The bias tuning updates the bias terms while keeping the rest of the backbone parameters unchanged, thus preserving the high-resolution details and contextual information. These strategic modifications significantly enhance the capability of the foundation model for estimating metric scale depth.

## 4 Advanced Architecture with Hyperbolic Geometry

We also present an advanced version of the baseline architecture that grafts hyperbolic geometry onto the depth foundation model, known for its effectiveness in low-shot problems [67; 68; 69; 60; 70]. We first generate depth-aware features by merging the multi-scale intermediate features \(E\) derived from the foundation model and by embedding them into hyperbolic space with geometry-aware curvature (Sec.4.1). Using the depth-aware features alongside sparse sensor data, we develop a hyperbolic propagation inspired by a traditional bilateral filter mechanism, which yields an initial dense depth at a metric scale (Sec.4.2). We lastly introduce a process for generating multi-curvature hyperbolic space for high-fidelity pixel relations and refinement of the initial depth (Sec.4.3).

### Multi-scale Feature Fusion & Hyperbolic Curvature Generation

The intermediate features from the foundation model \(E_{l} E\), where \(l=0,,L-1\), correspond to scales factors 1/2,..., \(1/2^{L}\) of the original resolution of input images. We aim to synergistically fuse the multi-scale information to learn comprehensive, context-aware features that facilitate depth propagation at a metric scale. We upsample the coarser feature map \(E_{l}^{M}(E_{0}^{M}=E_{0})\) using convolution layers, and then aggregate \(E_{l}^{M}\) with finer feature map \(E_{l+1}\) to obtain better visual contextual features\(E_{l+1}^{M}\). This fusion process is described below:

\[E_{l+1}^{M}=f_{l}^{fusion}(E_{l}^{M},E_{l+1}),\] (3)

where \(f_{l}^{fusion}\) indicates multi-scale feature aggregation blocks consisting of 2D transposed convolution layers with a skip connection.

_Hyperbolic embedding._ To ensure strong adaption to both new environments and any type of sensors, we adopt hyperbolic geometry which enables to capture the inherent hierarchical structures of 3D data [20; 21]. To embed the Euclidean features into hyperbolic space and vice versa, one first needs to define a bijective mapping from \(^{n}\) to \(_{}^{n}\). The exponential and the logarithmic mapping are used as bijective functions that have appealing forms at an origin, namely for \(^{n}\) and \(_{}^{}\):

\[_{0}^{}()=(\|\|/2)}{\|\|}_{ }^{}()=^{-}(\|\|)}{\|\|}.\] (4)

Using hyperbolic geometry for pixel-wise relationships, especially spatial propagation, is demonstrated in  by improving the discriminative power with minimal supervision. Following , we embed the mixed feature \(E_{L}^{M}\) into hyperbolic space using Eq.4 as below:

\[H_{i}=_{0}^{}(E_{L,i}^{M}),\] (5)

where \(i\) is an index of spatial coordinates in the image domain, and \(\) is the hyperbolic curvature.

_Hyperbolic curvature generation._ Using an appropriate curvature value is an important factor in projecting Euclidean features into hyperbolic space well, which is closely related to the construction of the hierarchy structures. Previous methods mainly use a fixed geometric structure regardless of data types and scene configurations by merely adjusting \(\) as a hyperparameter [70; 73; 74; 21]. In our problem definition, according to types of sensors and scene configurations, diverse data measurements and geometrical structures are observed, respectively. That's, our key observation is that a fixed and predetermined curvature may not be universally suitable.

We thus propose a curvature generation that learns a geometry-aware curved embedding space to adaptively match it to new environments and sensors. The curvature generator \(\) is composed of a convolution layer, a multi-layer perceptron (MLP) layer, and a global mean-pooling over spatial dimensions, which yields scene-dependent curvatures based on the fused feature \(E_{L}^{M}\) as below:

\[=(E_{L}^{M}).\] (6)

### Sparse-to-Dense Conversion based on Hyperbolic Features

With both the high-resolution pixel-wise features from the foundation model and the sparse depth data from arbitrary sensors, we perform a sparse-to-dense conversion to obtain an initial dense depth map. Inspired by , we design an initial propagation process based on a bilateral filtering mechanism , which is renowned for its edge-preserving ability by incorporating both radiometric differences and spatial distances into the bilateral weight. Considering a pixel \(x_{i}\) and the corresponding neighborhood pixel \(x_{j}\), the bilateral kernel filter \(w_{ij}\) can be simply defined as:

\[w_{ij}=f_{r}(x_{j},x_{i})g_{s}(x_{j}-x_{i}),\] (7)

where \(f_{r}\) is a range kernel for radiometric differences. \(g_{s}\) is a spatial kernel for physical separations in observed scenes and is developed in Euclidean space by calculating the distance between 3D points. For the range kernel \(f_{r}\), we need to design its hyperbolic version. Here, we utilize the hyperbolic feature \(H\) provided as input from Eq.5. With \(f_{r}\) and \(g_{s}\), we can compute the initial dense depth as:

\[D_{i}^{init}=_{j}w_{ij}S_{j} w_{ij}=(Dist _{hyp}(H_{i},H_{j}),Dist_{euc}(E_{L,i}^{M},E_{L,j}^{M})),\] (8)

where \(Dist_{hyp}\) is the hyperbolic function consisting of hyperbolic MLP, and \(Dist_{euc}\) is the Euclidean distance in the 3-dimension coordinate. \(N(i)\) means the neighborhood sparse depth of the pixel \(i\), and \(S_{j}\) is the corresponding depth from a sensor. \(\) indicates the learnable MLP layer to compute a coefficient for each sparse depth of the neighborhood \(S_{j}\). Through the combination of the distance functions in Eq.8, we effectively take advantage of both hyperbolic and Euclidean geometries to produce more accurate and robust depth maps.

### Depth Refinement in Multi-curvature Hyperbolic Space

_Depth refinement._ To refine the initial depth in Eq.8, we employ a convolutional spatial propagation scheme, CSPN++ . This refinement process leverages a predefined depth map \(D_{i}\), augmented by a sparse valid depth map \(S\), and a multi-kernel affinity map with three different kernel sizes \(\{3,5,7\}\). The use of a multi-kernel approach enables the model to capture a diverse range of features from the input data, thus achieving detailed and comprehensive depth estimations. The propagation process for a kernel size \(k\) at step \(t\) to yield a dense map \(\) is formulated as:

\[_{i}^{t+1}=_{k}_{i,k}D_{i,k}^{t+1} D_{i,k}^{t+1}=A_{i,k} D_{i}^{0}+_{j_{k}(i)}A_ {j,k} D_{j,k}^{t},\] (9)

where \(D^{t}\) is the depth map at each propagation step \(t\). \(D^{0}\) and \(A\) are an initial depth for \(t=0\) and its affinity map, respectively. \(\) is an element-wise product, and \(j_{k}(i)\) denotes a set of neighboring pixels around pixel \(i\) within a \(k k\) window. \(\) is a confidence map computed from \(E_{L}^{M}\) in Sec.4.1.

_Hyperbolic convolution layer (HCL)._ We design the multi-kernel affinity map \(A_{k}\) in hyperbolic space with the proposed curvature generation module described in Sec.4.1. To do this, we formulate HCL with hyperbolic feature vector \(\) for a 2-dimensional image domain:

\[HCL(,):=_{}_{(i,j)} ^{}()_{},\] (10)

where \(^{C_{} C_{} }\) is a convolution weight matrix whose kernel size is \(\), and \(\) is a bias term. \(\{(i,j)}^{2}(-^{},-^{}),...,( ^{},^{}),\ ^{}{=}\}\) is a set of signed distances from a center of the convolution kernel to others in \(\). \(\), \(\) and \(^{}\) are hyperbolic multiplication, addition, and concatenation, respectively, whose details are in Appendix A.1. Note that the hyperbolic MLP (Eq.8) is designed with \(=1\).

_Mutli-curvature affinity generation._ By dynamically adjusting the hyperbolic curvature \(\) for each affinity map, our approach tailors the geometrical representation to better fit the specific depth structure of each scene. We first determine the hyperbolic curvature \(\) with Eq.6 and then compute affinity map \(A_{k}\) using a hyperbolic convolution operation equipped with a kernel of size \(k\), chosen to match the receptive field of the corresponding kernel of the affinity map \(A_{k}\). This alignment optimizes the local receptive fields across the depth map, enabling a more precise aggregation of context and texture information from neighboring pixels. We can calculate the hyperbolic affinity map \(A_{k}^{hyp}\) based on the generated curvature \(_{k}\) from the curvature generation blocks \(_{k}\) as below:

\[A_{k}^{hyp}=HCL(E_{L,i}^{M},_{k})_{k}= _{k}(E_{L}^{M}).\] (11)We can achieve the refined dense depth based on the generated hyperbolic affinity maps \(A_{k}^{hyp}\) by incorporating it into Eq.9. In particular, the employment of hyperbolic space is beneficial for depth perception by implicitly building hierarchical structures , whose roots come from sparse points of an input depth in this work. The hyperbolic space is also advantageous in regions where photometric distances between foreground and background pixels are marginal (see Fig.2). The detailed analysis of multi-curvature hyperbolic affinity is described in Sec. 5.3.

## 5 Experiment and Analysis

In this section, we evaluate the performance of our proposed method for UniDC, focusing on its adaptability using minimal labeled data. Firstly, we outline an overview of the experimental setup (Sec.5.1). Subsequent comparisons with various SoTA methods are then presented using standard benchmark datasets (Sec.5.2). Furthermore, we conduct an ablation study to clarify the impact of each component in our methodology (Sec.5.3). In Appendix.A.2, we introduce details of the training procedure, datasets, and evaluation metrics in this work. Additional experiments, including full dataset training benchmarks, hyperbolic space affinity calculations, an ablation study on foundation models, and varying-density performance, are included in the Appendix A.3.

### Implementation Details

_Loss functions._ We train our method in a supervised manner with a linear combination of two loss terms: scale-invariant loss \(L_{}\) (Eq.2) for bridging the gap between relative and metric scale depths, and a composite loss \(L_{}\) based on \(L_{1}\) and \(L_{2}\) distances for inferring the final dense depth map. In total, our framework is optimized by minimizing the final loss \(\) as below:

\[&=L_{}(,D_{gt})+ L_{ }(D_{},D_{gt}),\\ & L_{}(,D_{gt})=_{i V}(|_{i}-D_{gt,i}|+|_{i} -D_{gt,i}|^{2}).\] (12)

where \(\) is a balance term and is empirically set to 0.1.

_Evaluation protocols._ For fair evaluations, we select a diverse array of SoTA depth from sparse measurements. These include a sensor-agnostic model, DepthPrompting  and series of SPNs such as S2D , CSPN , NLSPN , DySPN , CostDCNet , CompletionFormer , and BPNet . We assess depth quality using common quantitative metrics: root mean square error (RMSE, in meters), mean absolute error (MAE, in meters), and inlier ratio (DELTA1, where \(<1.25\)). We employ the widely-used depth completion datasets: NYU  and KITTI DC , setting up a minimal training dataset for few-shot scenarios. Note that we use their official test sets for all the comparison methods.

We implement the few-shot scenarios with and without dense depth supervision. Our experimental setup includes conducting 1-shot, 10-shot, and 100-shot learning by randomly sampling within the official training split. Additionally, we perform 1-sequence training by randomly selecting one sequence from the training set. To ensure the reliability in our experiments, we randomly select 10 sequences, and report averaged results.

### Experiment

_Few-shot learning with dense GT._ Both Tab.1 and Tab.3 show that existing methods face significant challenges when taking input depths from new sensors with minimal labeled data, whose examples are displayed in Fig.3 and Fig.4, respectively. In the 1-shot scenarios, where a model is optimized using

Figure 2: Depth propagation results according to fixed and multi-curvature values.

only a single pair of an image and its corresponding dense depth, our model demonstrates a substantial performance advantage over the comparison models. This underscores the effectiveness of using the foundation model that does not require any additional learning for new depth representations of unseen data. The models with a large number of parameters to learn, such as CompletionFormer  (83.6M), often struggle to optimize with limited datasets. Since the depth prompting module in  requires training from scratch, it encounters difficulties in the adaptation to new sensors.

### Ablation Study

_Probe for hyperbolic embedding._ We assess the efficacy of hyperbolic embedding and curvature generation, focusing on their performance in zero-shot settings. In Tab.2, the hyperbolic method yields promising results, whereas the Euclidean approach fails. The performance gap implies that hyperbolic space offers discriminative features which guide the sparse depth propagation well. While the influence of initial parameter settings cannot be overlooked, the potential for rapid adaptation can be enhanced through well-devised initialization methods, which are in line with principles from meta-learning strategies [82; 83]. we conduct additional experiments under the few-shot regime. The results, presented in Table.F, show a noticeable improvement when using hyperbolic space, with a performance gain of 5% on average, compared to Euclidean space. This validates the effectiveness of hyperbolic geometry in depth completion tasks, especially when dealing with limited data samples.

Additionally, the analysis of the multi-curvature approach for the refinement process (Tab.5) reveals that the curvature values for multi-size affinity maps in

   &  &  &  &  \\   & RMSE & MAE & DETAI & RMSE & MAE & DETAI & RMSE & MAE & DETAI & RMSE & MAE & DETAI \\  CSPN  & 1.4827 & 1.2058 & 0.3455 & 0.3166 & 0.1961 & 0.7106 & 0.2854 & 0.1307 & 0.9748 & 0.3166 & 0.1961 & 0.7106 \\ NLSPN  & 1.9358 & 1.6132 & 0.2229 & 1.5995 & 0.8261 & 0.5040 & 0.5501 & 0.4150 & 0.7985 & 0.8881 & 0.6421 & 0.6809 \\ DySPN  & 1.5474 & 1.2851 & 0.3149 & 0.4102 & 0.2817 & 0.8595 & 0.2674 & 0.1706 & 0.9341 & 0.2584 & 0.1320 & 0.9615 \\ CompletionFormer  & 1.8218 & 1.5539 & 0.2408 & 1.1583 & 1.0162 & 0.3079 & 0.9914 & 0.8164 & 0.4379 & 0.6779 & 0.5356 & 0.7476 \\ CostDCNet  & 1.2298 & 0.9754 & 0.4693 & 0.2363 & 0.1288 & 0.9719 & 0.1770 & 0.0836 & 0.9826 & 0.2066 & 0.0954 & 0.9788 \\ BPNet  & 0.3573 & 0.2077 & 0.9482 & 0.2392 & 0.1120 & 0.9744 & 0.1757 & 0.0793 & 0.9829 & 0.2220 & 0.1040 & 0.9765 \\ DepthProming  & 0.3583 & 0.2067 & 0.9101 & 0.2195 & 0.1006 & 0.9733 & 0.2101 & 0.1008 & 0.9743 & 0.2335 & 0.1191 & 0.9686 \\ Ours & **0.2099** & **0.1075** & **0.9752** & **0.1657** & **0.0794** & **0.9849** & **0.1473** & **0.0669** & **0.9885** & **0.1632** & **0.0745** & **0.9860** \\  

Table 1: Quantitative results on NYUv2.

   & NYU w/o Training & KITTI w/o Training & NYU 1-shot & NYU 10-shot & NYU 100-shot & KITTI 1-shot & KITTI 10-shot \\  Euclidean & 13.8899 / 11.507 & 77.642 / 63.109 & 0.2177 / 0.1122 & 0.7127 / 0.081 & 0.149 / 0.069 & 1.745 / 0.578 & 1.397 / 0.417 & 1.291 / 0.342 \\ Hyperbolic & **0.3237** / **0.246** & **4.061 / 1.974** & **0.210 / 0.108** & **0.166 / 0.079** & **0.147 / 0.067** & **1.684 / 0.521** & **1.385 / 0.407** & **1.224 / 0.339** \\  

Table 2: Ablation of hyperbolic operations on zero-/few-shot performance for NYU and KITTI.

Figure 3: Results of 1-/100-shot on NYU. (CFFormer: CompletionFormer, DP: DepthPrompting).

   & KITTIUC & 1-shot & 10-shot & 100-shot \\  BPNet (8-Line) & 11.647 / 3.19 & 4.00 / 1.62 & 3.28 / 1.36 \\ DepthPrompting (8-Line) & 8.15 / 5.67 & 6.77 / 3.75 & 8.05 / 2.36 \\ Ours (8-Line) & **4.34 / 1.77** & **3.32 / 1.33** & **2.89 / 1.12** \\  BPNet (32-Line) & 4.76 / 1.54 & 2.56 / 2.56 / 2.08 / 0.28 / 0.72 \\ DepthPrompting (32-Line) & 3.90 / 1.63 & 2.92 / 1.25 & 2.40 / 0.87 \\ Ours (32-Line) & **2.01 / 0.66** & **1.92 / 0.61** & **1.89 / 0.64** \\  

Table 4: Result of few-shot learning without dense GT depth. (RMSE/MAE)

[MISSING_PAGE_EMPTY:9]

[MISSING_PAGE_FAIL:10]