# Ecological data and objectives align deep neural network representations with humans

Akash Nagaraj, Alekh Karkada Ashok, Drew Linsley,

Francis E Lewis, Peisen Zhou, Thomas Serre

Carney Institute for Brain Science,

Department of Cognitive Linguistic & Psychological Sciences

Brown University, Providence, RI 02912

akash_n@brown.edu

###### Abstract

The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Although DNNs have been surprisingly adept at explaining behavioral and neural recordings from humans, a growing number of reports indicate that DNNs are becoming progressively worse models of human vision as they improve on standard computer vision benchmarks. Here, we provide evidence that one path towards improving the alignment of DNNs with human vision is to train them with data and objective functions that more closely resemble those relied upon by brains. We find that DNNs trained to capture the causal structure of large spatiotemporal object datasets learn generalizable object representations that exhibit smooth equivariance to 3-dimensional (out-of-plane) variations in object pose and are predictive of human decisions and reaction times on popular psychophysics stimuli. Our work identifies novel data diets and objective functions that better align DNN vision with humans and can be easily scaled to generate the next generation of DNNs that behave as humans do.

## 1 Introduction

Deep neural networks (DNNs) have achieved remarkable success in object recognition benchmarks through computational and data scale, achieving human-level performance on visual tasks ranging from object classification  to segmentation . However, as the accuracy of DNNs on benchmarks has improved in recent years, the alignment of their representations, behaviors, and strategies with humans has decreased precipitously. For example, the most accurate DNNs today rely on features that have a very low correlation with those that humans find diagnostic for object recognition  and are as (in)accurate at predicting responses to images evoked by neurons in the inferotemporal cortex as AlexNet . This growing gap between human and DNN vision implies that the current deep learning paradigm needs to be revised to have any hope of reverse engineering biological vision and creating artificial vision systems that can see, behave, and process information like humans.

A partial solution to the misalignment of DNNs and human vision systems is the 'neural harmonizer:' a constraint on DNN optimization that forces a model's image representations to align with those used by human observers to classify the same images . Despite the efficacy of the neural harmonizer for improving DNN alignment with humans visual decisions , representations , adversarial robustness  and neural predictions , the method is limited in several fundamental ways. First,the neural harmonizer relies on large behavioral datasets to constrain DNN representations, and these datasets are difficult to collect and scale to the training regimes that have yielded the most accurate models to date. Second, while the neural harmonizer is proof that DNN alignment can be improved with humans without hurting object classification accuracy, it does not get us closer to understanding the developmental principles that shape human vision. Identifying such principles would advance our basic understanding of human vision and offer an eminently scalable way of fixing the alignment problem.

Contributions.In this work, we investigate whether ecological data diets and behavioral objectives can shape DNNs to produce more human-like representations and behavior. We focus our efforts on a basic distinction between how state-of-the-art DNNs and humans learn to recognize objects: While DNNs learn to recognize objects through datasets containing millions or billions of object images and explicit categorical supervision, humans do the same by observing objects as they move through the world and often learning about them without explicit supervision. We hypothesize that this difference between the data diets and objective functions of humans vs. DNNs is a key factor driving the growing misalignment of DNNs.

Figure 1: **A framework for investigating the effects of ecological data and objective functions on deep neural network (DNN) representations. (a) We train Visual Transformers (ViTs) on spatiotemporal data generated with neural radiance field (NeRF) object models. Each input consists of four frames captured by a camera following a radial trajectory around an object (_e.g._, a teddy bear, as shown). The ViT consists of an encoder, with weights shared across frames (_i.e._ encoding 2-Dimensional features), and a decoder, with spatiotemporal weights (_i.e._ decoding across space and time). Models are trained on one of three objectives (denoted by the colors), where a part of the image is masked from the input. (1) Causal Vision Modeling (CVM): predict the next frame. (2) Masked Vision Modeling (MVM): predict the intervening frame. (3) Masked Autoencoder (MAE) : predict the content of masked patches in an image. The decoder is only used for training, and the encoder representations of images are used at test time. (b) DNNs trained with CVM learn to reconstruct a future frame in a sequence accurately.**

* We developed a framework for systematically testing the role of different data diets and objective functions on the representations learned by DNNs. We generate rich, naturalistic, spatiotemporal image sequences and instruct DNNs to learn from these through a variety of well-controlled objective functions that focus models on orthogonal aspects of the data.
* We discover that DNNs best explain human behavior on popular psychophysics stimuli ('Greebles' ) when trained to predict the next state of an object -- an objective which we refer to as 'Causal Vision Modeling' (CVM).
* Underlying the alignment of CVM-trained DNNs are representations that exhibit smooth equivariance to 3-dimensional (out-of-plane) object transformations. This capability is not found in DNNs trained on the same data through any other means.
* It is therefore possible to align the visual behavior of DNNs with humans by construction through ecological data and objective functions.

## 2 Methods

Training datasets.We hypothesized that the internet data diets used to train DNNs today are one of the reasons why these models are growing progressively less aligned with human vision. To address this problem, we devised an approach to generate unbounded amounts of rich spatio-temporal object image data, which we thought might capture similar kinds of experiences that humans have with objects. Specifically, we turned to neural radiance fields  (NeRFs) to build 3-dimensional models of individual objects, then created sequences from images taken by a virtual camera as it revolved around the object.

We used NeRFs trained on the Common Objects in 3D (CO3D ) dataset that was previously released as part of the PeRFception challenge . Unlike that challenge, however, we investigate the performance of models trained on spatio-temporal sequences of images instead of random views of objects. Our dataset contained 18,619 NeRFs of common objects from 50 MS-COCO  categories. We rendered a 50-frame video of each object. Models were trained on randomly selected chunks of four frames from this sequence with a predetermined number of skipped frames in between selected frames.

Figure 2: **A CVM-trained model’s recognition confidence aligns with human reaction time in a psychophysics experiment.** Human participants were tested on their ability to identify ‘Greebles’ in various poses. Their reaction time grew as the objects were rotated further away from their canonical poses . A CVM trained on naturalistic object sequences was able to predict the pose of objects reliably, and its recognition confidence strongly aligned with human reaction time. Neither MVM- nor MAE-trained models exhibited the same behavior.

ModelsWe trained multiple instances of a modified Vision Transformer (ViT)  on the spatiotemporal image data generated from NeRFs. The ViT consisted of two parts: a 12-layer frame encoder operating on \(224 224\) pixel images and an 8-layer spatial-temporal decoder  that operated on the outputs of the encoder and ultimately generated an image-sized output. Each frame passed into the encoder was split into patches, or 'tokens,' of size \(16 16\) pixels (Fig. 1). The decoder was only used for training and discarded for the experiments discussed in Results.

Objective functions.Inspired by the success of masked auto-encoding approaches for images , videos  and language , we realized that it is possible to investigate a multitude of objective functions by asking DNNs to solve different reconstruction tasks. This work focuses on just three that resemble popular objectives used in machine learning today or are speculated to be important for biological learning. (_i_) The popular masked auto-encoding (MAE)  where a proportion of image patches are randomly masked. (_ii_) Masked vision modeling (MVM), inspired by the popular BERT objective  from language modeling, where an entire intermediate frame in a sequence is masked. (_ii_) Causal vision modeling (CVM), inspired by the objective of causal language models popularized by the GPT family of models , where the final frame in a sequence is masked (Fig 1a).

## 3 Results

Human psychophysics.After training models with the same hyperparameters as , we tested the alignment of these models with human behavior on images of 'Greebles:' a popular dataset that has been used to investigate the sensitivity of human recognition capabilities to 3-dimensional (out-of-plane) rotations . In those experiments, it was found that human recognition accuracy worsened and reaction time increased as objects were rotated further away from their canonical, front-facing view.

We tested the same effect in models trained with CVM, MVM, and MAE training objectives. We did this in three steps: First, we generated image sequences from a camera revolving around 15 greeble classes. Next, we stored each model's representation of the canonical view of every greeble as a template. Third, we compared each model's representation of every other view of the Greebles to this stored template. We measured model recognition accuracy by assigning the class to the nearest template and the model reaction time as the cosine similarity of the template to all other views of each greeble (Fig 2). The CVM-trained model's accuracy was unrivaled (Human: 0.88, CVM: 0.64, MVM: 0.51 & MAE: 0.44) and had image representation dissimilarities significantly correlated with human reaction times.

Representational analysisWe next investigated why CVM-trained models were significantly more aligned with humans than any other model tested. To do this, we decomposed CVM-trained model representations of Greebles with UMAP into a 2-dimensional embedding to better interpret the structure it contains. Surprisingly, we found that the model grouped all images from any given Greeble into a manifold, in which camera orientations were ordered and linearly decodable. In other words, CVM-trained models learned equivariance to out-of-plane camera rotations during their

Figure 3: **CVM-trained DNNs learn equivariance to 3-dimensional (out-of-plane) object transformations. UMAP was used to decompose ViT-encoder representations of objects into 2 dimensions, which revealed distinct ring-like manifolds for each object.**

training, and this equivariance transferred to the Greeble stimuli '0-shot' (_i.e._, without additional training). Such structure is non-trivial, and we did not observe it in either MVM- or MAE-trained models.