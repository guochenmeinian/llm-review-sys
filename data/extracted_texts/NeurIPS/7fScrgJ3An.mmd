# DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features

DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features

 Letian Wang\({}^{1,2}\)Seung Wook Kim\({}^{1}\)Jiawei Yang\({}^{3}\)Cunjun Yu\({}^{1,4}\)Boris Ivanovic\({}^{1}\)Steven Waslander\({}^{2}\)Yue Wang\({}^{1,3}\)Sanja Fidler\({}^{1,2}\)Marco Pavone\({}^{1,5}\)Peter Karkus\({}^{1}\)

\({}^{1}\)NVIDIA Research \({}^{2}\)University of Toronto \({}^{3}\)University of Southern California

\({}^{4}\)National University of Singapore \({}^{5}\)Stanford University

###### Abstract

We propose DistillNeRF, a self-supervised learning framework addressing the challenge of understanding 3D environments from limited 2D observations in outdoor autonomous driving scenes. Our method is a generalizable feedforward model that predicts a rich neural scene representation from sparse, single-frame multi-view camera inputs with limited view overlap, and is trained self-supervised with differentiable rendering to reconstruct RGB, depth, or feature images. Our first insight is to exploit per-scene optimized Neural Radiance Fields (NeRFs) by generating dense depth and virtual camera targets from them, which helps our model to learn enhanced 3D geometry from sparse non-overlapping image inputs. Second, to learn a semantically rich 3D representation, we propose distilling features from pre-trained 2D foundation models, such as CLIP or DINov2, thereby enabling various downstream tasks without the need for costly 3D human annotations. To leverage these two insights, we introduce a novel model architecture with a two-stage lift-splat-shoot encoder and a parameterized sparse hierarchical voxel representation. Experimental results on the NuScenes and Waymo NORR datasets demonstrate that DistillNeRF significantly outperforms existing comparable state-of-the-art self-supervised methods for scene reconstruction, novel view synthesis, and depth estimation; and it allows for competitive zero-shot 3D semantic occupancy prediction, as well as open-world scene understanding through distilled foundation model features. Demos and code will be available at https://distillnerf.github.io/.

## 1 Introduction

Understanding and interpreting complex 3D environments from limited 2D observations is a fundamental challenge in autonomous driving and beyond. Many efforts have been made to tackle this challenge by learning from labor-intensive and costly 3D annotations, such as 3D bounding boxes  and semantic occupancy labels . However, these approaches typically struggle with scalability due to their excessive reliance on expensive annotations.

Neural scene representations, such as NeRFs  and 3D Gaussian Splitting (3DGS) , have recently emerged as a compelling paradigm for learning 3D representations from 2D signals in a self-supervised manner. While these methods demonstrated strong capabilities in view synthesis for indoor scenes , and more recently also for challenging dynamic outdoor scenes , they require delicately training a new representation for each new scene, leading to extensive computation budget and time needs, typically in the order of hours or minutes. This falls short of the real-time computational requirements for autonomous driving, which typically demand a processing speed of 2-10 Hz. Additionally, most works focus on view synthesis only , resulting in learned 3D representations that lack semantics, sidestep downstream tasks, and do not fully exploit the potential of neural scene representations to transform various sources of 2D information into 3D, such as features from 2D visual foundation models [15; 16].

To this end, we propose DistillNeRF (Fig. 1), a conceptually simple framework for learning generalizable neural scene representations in driving scenes, by distilling 1) offline optimized NeRFs for enhanced geometry and appearance, and 2) pre-trained 2D foundation models, DINov2 [17; 16] and CLIP , for enriched semantics. Generalizable scene representations with feed-forward models is an active area of research [18; 19; 20], and the autonomous driving domain remains particularly challenging due to sparse camera views with little overlap: prior generalizable NeRFs for driving are shown to be struggling with view synthesis [21; 22; 23]. In contrast, we show that by distilling per-scene optimized NeRFs and visual foundation models, DistillNeRF allows predicting a 3D feature volume with strong geometry, appearance, and semantics, from sparse single-timestep images. The representation is capable of rendering tasks without per-scene optimization (e.g. scene reconstruction, novel view synthesis, foundation model feature prediction), and support downstream tasks, such as open-vocabulary text query and zero-shot 3D semantic occupancy prediction.

DistillNeRF comprises two stages: offline per-scene NeRF training, and distillation into a generalizable model. The first stage trains a NeRF for each scene individually from each scene's driving log, exploiting all available multi-view, multi-timestep information. Specifically, we use EmerNerf , a recent NeRF approach with decomposed static and dynamic fields. The second stage trains a generalizable encoder to directly lift multi-camera 2D images captured at a single timestep to a 3D continuous feature field, from which we render images, and supervise with dense depth and novel-view RGB targets generated from the per-scene optimized NeRFs, and foundation model features. Specifically, we propose a novel model architecture with 1) a two-stage Lift-Splat-Shoot encoder  to lift 2D observations into 3D; 2) a sparse hierarchical 3D voxel for efficient runtime and memory, parameterized to account for unbounded driving scenes; 3) feature image generation via differentiable volumetric rendering, decoded into appearance, and optionally, foundation model features.

Extensive experiments on the NuScenes  and Waymo NORR [26; 14] dataset demonstrate that DistillNeRF allows for high-quality scene reconstruction and novel view synthesis in previously unseen environments without per-scene training, on par with test-time per-scene optimization approaches, and significantly outperforming previous generalizable approaches. We also show strong results for zero-shot 3D semantic occupancy prediction, and promising quantitative results for open-vocabulary scene understanding.

## 2 Related Work

Neural Scene Representations.Neural scene representations, like NeRFs [7; 27] and 3DGS [9; 28], have brought unprecedented success in learning powerful representations of 3D scenes, and have also been successfully applied to challenging driving scenes populated with dynamic objects [29; 30;

Figure 1: DistillNeRF is a generalizable model for 3D scene representation, self-supervised by natural sensor streams along with distillation from offline NeRFs and vision foundation models. It supports rendering RGB, depth, and foundation feature images, without test-time per-scene optimization, and enables zero-shot 3D semantic occupancy prediction and open-vocabulary text queries.

12; 31; 32; 33; 34; 35]. However, these methods typically require expensive training for each scene, typically in the order of hours or minutes.

**Generalizable NeRFs.** Generalizable Neural Radiance Fields, such as [18; 36; 37; 38; 39], adapt the capabilities of conventional NeRFs for 3D scene reconstruction and novel view synthesis into a generalizable feedforward model. They replace the costly per-scene optimization with a single feedforward pass through the models. Recent works have extended such approaches to challenging driving scenes and demonstrated the potential in down-stream tasks [22; 5; 23; 40; 41]. However, due to the challenging sparse-view limited-overlapping camera settings on vehicles, these methods usually fail to show strong rendering performance. To the best of our knowledge, we are the very first work to achieve strong scene reconstruction and reasonable novel-view synthesis on par with offline per-scene optimized NeRFs in driving scenes.

**NeRFs with Feature Fields.** Recent advancements extend NeRFs beyond novel view synthesis by integrating 2D features from foundation models into 3D space, equipping neural fields with semantic understanding . Recent approaches also demonstrate similar capabilities in outdoor driving scenes  by distilling DINO features into the scene representation. However, these approaches suffer from prolonged optimization times when combined with feature distillation, and thus are impractical for online autonomous driving due to their costly per-scene optimization. Closely related to our work, FeatureNeRF  is a generalizable method that distills DINOv2  features into 3D for keypoint transfer, but only investigates simple indoor object-level synthetic datasets like ShapeNet , where a large number of overlapping camera images are placed around the object to scan it from various angles. In contrast, we address the more challenging outdoor scene-level settings of autonomous driving, with sparse limited-overlapping camera image inputs. Our method, DistillNeRF, infers 3D feature fields in a single forward pass, making real-time application possible.

**NeRFs in Driving.** Most NeRF-related works in autonomous driving focus on 1) offline scene reconstruction or sensor simulation [11; 12; 14; 13], that accurately reconstruct 3D or 4D scenes with detailed appearance and geometry; 2) exploring potential in downstream tasks [21; 23; 22; 40], that uses volumetric rendering to learn 3D representations from sensor inputs to enlighten online driving. Our work takes the best of these two lines of research: we distill the precise geometry and novel views from offline per-scene optimized NeRFs and rich semantic features from foundation models into our online model. Consequently, our online model not only excels in scene reconstruction and novel view synthesis, but also shows competitive downstream performance, such as zero-shot semantic occupancy prediction, and open-vocabulary query. To the best of our knowledge, we are the first to do so.

**Distilling NeRFs.** Model distillation is a well-established idea . NeRFs have also been distilled into, e.g., Generative Adversarial Networks in , and feed-forward models for temporal object shape prediction in . However, prior work mainly focuses on static, object-centric, or indoor scenes. To the best of our knowledge, we are the first to propose distilling a per-scene optimized NeRFs with static-dynamic decomposition into a generalizable model for outdoor driving scenes.

## 3 Method

DistillNeRF predicts a generalizable scene representation in the form of sparse hierarchical voxels from single-timestep multi-view RGB image inputs, and is self-supervised by natural sensor streams, through volumetric rendering to output RGB, depth, and feature images.

The method is depicted in Fig. 1, the detailed architecture in Fig. 2, and key capabilities in Fig. 3 and Fig. 4. Inputs are \(N\) posed RGB camera images \(\{I_{i}\}_{i=1}^{N}\). We use a 2D backbone to extract \(N\) feature images \(\{X_{i}\}_{i=1}^{N}\). We then lift the 2D features to a 3D voxel-based neural field \(^{H W D C}\), and apply sparse quantization and convolution to fuse features from multiple views. To account for unbounded scenes we use a parameterized neural field with fixed-scale inner voxels, and varying-scale outer voxels contracting the infinite range. Volumetric rendering is performed to supervise the reconstruction of the scene. For better guidance on scene geometry, we "distill" knowledge from offline optimized NeRFs, using rendered dense depth images from original camera views and virtual camera views. Foundation model features, from CLIP or DINOv2, are set as additional reconstruction objectives and thus are also "distilled" into our model to enrich scene semantics. We introduce design principals in the following section, and refer to Appendix A.7 for implementation details.

### Sparse Hierarchical Voxel Model

**Single-View Lifting.** For each of the \(N\) camera image inputs, we follow a similar procedure as Lift-Splat-Shoot (LSS)  to lift the 2D image features to the 3D neural field. Unlike typical LSS and variants  that predict depth in one shot, we propose a two-stage, coarse-to-fine strategy with two jointly trained predictors to capture more nuanced depth. The first stage, following prior LSS works, predicts categorical depth and aggregates them into a single prediction with ray marching. The second stage then predicts a distribution over a fine-grained set of categorical depth values, which are centered around the first-stage predicted coarse depth.

Specifically, in the first stage, we feed each image to a 2D backbone to generate a depth feature map of size \(H W D\). The depth feature map is regarded as a discrete frustum where \(D\) denotes the number of pre-defined categorical depths. Inspired by the volume rendering equation , each entry in the frustum is a density value. That is, the \(d\)'th channel of the frustum at pixel \((h,w)\) represents the density value \(_{h,w,d}\) of the frustum entry at \((h,w,d)\). The occupancy weight of entry \((h,w,d)\) is then

\[(h,w,d)=exp(-_{j=1}^{d-1}_{j}_{h,w,j})(1-exp(- _{d}_{h,w,d})),\] (1)

where \(_{d}=t_{d+1}-t_{d}\) is the distance between each pre-defined depth \(t\) in the frustum. Coarse depth for pixel \((h,w)\) is obtained by aggregating with ray marching:

\[(h,w)=_{d=1}^{D}(h,w,d)t_{d}.\] (2)

In the second stage, centered around the initial coarse depth prediction, we dynamically sample a set of \(D^{}\) fine-grained depth candidates. This involves uniform sampling, with the sampling range adaptively adjusted based on the coarse depth estimate. We then embed these fine-grained depth candidates, and combine their embeddings with the depth features from the first stage, and feed them to another network to generate the density of each fine-grained depth candidate. The occupancy weights \(^{}\) of the fine-grained depth candidates are predicted similarly by Eq 1, which can also be regarded as probabilities of each fine-grained depth candidate.

With the candidate depths associated with probabilities, we then lift 2D image features to 3D. Specifically, we use a 2D image backbone to get 2D image features \(\), and assign the 2D image features to the 3D frustum according to each pixel's depth. That is, for pixel \((h,w)\), its image feature \(_{h,w}\) is distributed to each fine-grained depth candidates \(t^{}_{d}\) by \([^{}_{h,w,d}_{h,w},^{}_{h,w,d}]\), where we scale the pixel image feature \(_{h,w}\) with occupancy \(^{}_{h,w,d}\) and concatenate it with density \(^{}_{h,w,d}\).

**Multi-View Fusion.** After constructing the frustum for each view, we transform the frustums to the world coordinates using the camera poses, and fuse them into a shared 3D voxel-based neural field \(\)

Figure 2: DistillNeRF model architecture. (left) single-view encoding with two-stage probabilistic depth prediction; (center) multi-view pooling into a sparse hierarchical voxel representation using sparse quantization and convolution; (right) volumetric rendering from sparse hierarchical voxels.

where each voxel represents a region in the world coordinates and carries both densities and features. When lifted frustum entries from different views lie in the same voxel, we fuse them with average pooling.

**Sparse Hierarchical Voxels.** Unlike previous works  using dense voxels, which uniformly quantizes the neural field and potentially wastes computation and memory on large empty regions, we apply sparse quantization on the neural field. Specifically, we follow the octree representation  to recursively divide the neural field into specified levels, according to the 3D positions of the lifted 2D features. While an octree with many levels and thus smaller voxel sizes can capture more accurate 3D positions of lifted features, overly fine-grained octrees can lead to difficulty in querying features during rendering (e.g. missing far-away features due to large gaps between sampled rays). To this end, we generate two octrees with different quantization granularities, one fine octree with more quantization levels capturing details of the lifted features, and one coarse octree to represent general information of a larger range. Sparse convolutions  are then applied to both octrees to encode the relationships and interactions among voxels, during which the features in the fine octree are also downsampled and concatenated with the coarse octree to enhance details.

**Neural Field Parameterization.** Unlike prior works that consider a neural field covering a fixed range , our work aims at accounting for the unbounded-scene settings in the driving scenes by proposing a parameterized neural field. We want to keep the inner-range voxels at the real scale and high resolution due to their importance to various downstream tasks (e.g., occupancy prediction in 50 meters' range), while contracting the scene up to infinite distance in the outer range of the voxels at a lower resolution, so we can render with low memory and computation cost (e.g. sky, far-away buildings). Inspired by , we propose a transform function that maps a 3D point

Figure 3: DistillNeRF Capabilities - Given single-frame multi-view cameras as input and without test-time per-scene optimization, DistillNeRF can reconstruct RGB images (row 2), estimate depth (row 3), render foundation model features (rows 4, 5) which enables open-vocabulary text queries (rows 6, 7, 8), and predict binary and semantic occupancy in zero shot (rows 9, 10).

in the world coordinates \(p=(x,y,z)\) to the coordinates in the parameterized neural field:

\[f(p)=}&|p| p_{inner}\\ (1-}{|p|}(1-))&if|p|>p_{inner}.\] (3)

The transformed coordinates \(f(p)\) will always be within \(\), where \(p_{inner}\) sets the range of the inner voxel (region of interest) and varies in \(x,y,z\) directions, and \(\) denotes the contraction ratio, namely the proportion of the inner range in the parameterized neural field. Consistent parameterizations are enforced for both the single-view lifting process (on the depth space) and the multi-view fusion process (on the 3D coordinate space).

**Volume Rendering from Sparse Hierarchical Voxels.** Finally, we use differentiable volumetric rendering to project the 3D neural field onto 2D feature maps and render images. Specifically, for each pixel of each camera, we shoot a ray originating from the camera to the neural field according to the camera poses, and sample points along the ray. _Feature Querying:_ For each sample point, we query both fine and coarse octree to get the density and features of the corresponding voxel that the sample point lies in. Further, to capture both high-level information and fine-grained details, the features from both octrees are concatenated as the final feature. _Density Querying:_ Regarding the density, while the fine octree captures more accurate 3D positioning, since the fine octree voxel only covers a small region, the sample points could be easily within empty voxels and thus query no information, especially for faraway regions. To this end, for each sample point, we first query the fine octree to get the fine density. If the fine density is empty, we query the coarse octree to complement the density. _Two-Stage Sampling:_ Regarding the sampling strategy, we follow  to sample points for each ray with two phases: first we sample a set of points uniformly, then we sample another set of points with importance sampling given densities for the first set of points, so to enhance surface details in the scene. With the densities and features of the sampled points we do volumetric rendering using Eq 2 to get the 2D feature map for each camera. _Decoding:_ The rendered 2D feature maps are then fed into a CNN decoder to enhance high-frequency details, and upsample the final RGB image without increasing the rendering resolution/cost. Note that from the volume rendering process, we can also get the expected depth for each pixel .

### Self-supervised Training with Distillation

**Distillation from Offline NeRFs.** While our model can be trained by simply reconstructing RGB images, it remains challenging to learn scene geometry from only single time-step camera image inputs. The challenge is especially pronounced with typical autonomous vehicle setups where mounted cameras are facing outwards and have limited view overlap, making multi-view recon

Figure 4: DistillNeRF Generalizability - Trained on the nuScenes dataset, our model demonstrates strong zero-shot transfer performance on the unseen Waymo NOTR dataset, achieving decent reconstruction quality (row 2). This quality can be further enhanced by applying simple color alterations to account for camera-specific coloring discrepancies (row 3). After fine-tuning (row 4), our model surpasses the offline per-scene optimized EmerNeRF, achieving higher PSNR (29.84 vs. 28.87) and SSIM (0.911 vs. 0.814). See Tab 3 for quantitative results.

struction degrade to the monocular setting and aggravating depth ambiguity. A natural idea is to use images from multiple time steps to encourage view overlaps. However, driving scenes typically contain many dynamic objects that move between time steps, introducing noise to the reconstruction objective. Instead, we propose to leverage the high-quality geometry of per-scene optimized NeRFs that aggregate information from a full sensor stream. Specifically, we use EmerNeRF , a recent NeRF approach that handles dynamic objects by decomposing the scene into static and dynamic fields in a self-supervised manner. We propose two different ways to distill knowledge from per-scene optimized NeRFs, which together construct \(L_{NeRF}\), a distillation loss from offline NeRFs:

* **Dense 2D depth.** Depth supervision from LiDAR point clouds, \(L_{depth}\), is commonly used to facilitate 3D geometry learning, however, point clouds are typically sparse and only provide depth labels for a limited horizontal/vertical range. Thus we propose to use offline optimized NeRFs as a depth auto-labeling tool. Specifically, for each training target image we render a dense depth map from the offline NeRF, and use it as additional depth supervision, \(L_{depth^{}}\).
* **Virtual cameras.** In addition to depth distillation from original camera views, we can leverage temporally decomposed NeRFs to render depth from "virtual cameras", i.e., novel views, while keeping the time dimension frozen. In this manner, the virtual depth and RGB images can be used as additional reconstruction targets, thus the number of target images and the view overlap between cameras can be artificially increased, encouraging consistent depth prediction and improving novel-view synthesis performance.

**Distillation from Foundation Models.** In addition to RGB and depth prediction, can we learn 3D representations that contain rich semantics and enable a wider range of downstream tasks? Witnessing the rise of vision foundation models with generalized capabilities across various vision tasks, we propose to distill 2D foundation model features, such as CLIP  and DINOv2 , into our 3D scene representation model. We achieve this by simply introducing an additional MLP to our rendered 2D feature images, and train the model to reconstruct the foundation model feature images with an L1 loss \(L_{found}\). We demonstrate early attempts at utilizing these foundation models on open-vocabulary text query tasks shown in Fig 3, but we leave more comprehensive explorations to future work.

**Training objective.** In summary, we train our model for a linear combination of loss terms:

\[L=+L_{depth}+L_{density}}_{}++L_{found}}_{},\] (4)

where \(L_{rgb}\) and \(L_{depth}\) are rendering losses for RGB and depth; \(L_{density}\) is a density entropy loss encouraging clearer surfaces and structured density values as in ; \(L_{NeRF}\) and \(L_{found}\) denote distillation losses from offline NeRFs and foundation models. Please refer to Appendix A.5 for more details on the losses.

## 4 Experiments

We benchmark DistillNeRF against SOTA generalizable NeRFs, offline NeRFs, as well as comparable methods on the popular NuScenes dataset . We first evaluate the rendering performance, i.e., scene reconstruction, novel view synthesis, and feature reconstruction (Table 1). We further evaluate the learned 3D geometry through depth estimation (Table 2), and 3D semantic occupancy prediction (Table 4). Ablations of DistillNeRF are also analyzed in each task and displayed in each table, additional ablations are available in Tab A.2, Tab 6, and Tab A.6 of the Appendix. Qualitative results, including open vocabulary queries, are in Fig. 3, 6, and 7. Videos are online at https://distillmerf.github.io/. Implementation and training details are in the Appendix A.7.

**Dataset.** The nuScenes dataset  contains 1000 driving scenes from different geographic areas, each scene capturing approx. 20 seconds of driving, resulting in approx 40000 frames in total. Scenes are captured via six cameras mounted on the vehicle heading in different directions along with point clouds from LiDAR. We use the default data split, 700 scenes for training, 150 scenes for validation. We adopt the resolution of input RGB images, rendered RGB, and rendered depth are 114x228, 114x228, and 64x114 respectively. We also evaluate the generalizability of our method on the Waymo NOR dataset , a balanced and diverse benchmark derived from the Waymo Open Dataset . NORT features 120 unique, hand-picked driving sequences, split into 32 static, 32 dynamic, and 56 diverse scenes across seven challenging conditions. We adopt the resolution of input RGB images, rendered RGB, and rendered depth are 144x216, 144x216, and 72x108 respectively.

### Rendering Images and Foundation Features

**Setup.** We evaluate our model on previously unseen scenes from the validation set. For scene reconstruction, we compare the rendered images against GT images for the same time step. For novel-view synthesis, we render the novel-view image from the next timestep's camera pose, and compare it against the next timestep's GT image. We use standard metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). We compare with two SOTA generalizable NeRF methods in driving scenes, SelfOcc  and UniPAD , which do not see the validation set during training, similar to ours. We also compare with the SOTA per-scene optimized method, EmerNeRFs , which is trained on the validation set. Since EmerNerfs are trained on all timesteps in the scene, we cannot evaluate them for novel views. Instead, for novel-view evaluation we adapt _Single-Frame EmerNeRFs_, each of which is trained only on a single timestep and then evaluated for the next timestep. Due to the prohibitive training cost of Single-Frame EmerNerf on all timesteps, for all methods we report mean metrics over only the second frame of each scene.

**RGB Reconstruction and Novel-View Synthesis.** In Table 1, we show the results the image reconstruction and novel-view synthesis on the nuScenes validation set. The results show that our generalizable model is on par with the per-scene optimized NeRFs, and significantly outperforms SOTA generalizable methods, both for RGB reconstruction and novel-view synthesis. Without per-scene optimization, reconstruction PSNR for our best model variant is close to per-scene optimized EmerNerfs (30.11 vs 30.88), and achieves even higher SSIM (0.917 vs 0.879). Similarly, our novel-view PSNR is close to Single-Frame EmerNerf (20.78 vs 20.95), while SSIM is slightly higher (0.590 vs 0.585). Compared prior SOTA generalizable methods, our model outperforms the best-performing method (SelfOcc) in PSNR by 45.6% and 14.0%, and in SSIM by 64.9% and 27.1%, for reconstruction and novel-view synthesis, respectively. Novel-view metrics are generally lower than reconstruction metrics. Note that our novel-view setting is challenging, as the vehicle can travel large distances (in 0.5s) between the input-view and novel-view camera poses, and capture elements

   &  & No Test-Time &  &  &  &  \\  & & & &  &  &  &  &  \\   & & &  &  & SSIM \(\) & PSNR \(\) & SSIM \(\) & CLIP PSNR \(\) &  \\  EmerNerf  & \)} & \)} & \)} & \)} & **30.88** & 0.879 &  & **21.12** \\ Single-Frame EmerNerf & & & & & & & **20.85** & 0.585 & & - \\ SelfOcc  & & & & & & 20.87 & 0.556 & 18.22 & 0.464 & - & - \\ UniPAD  & & & & & & 19.44 & 0.497 & 16.45 & 0.375 & - & - \\  Depth 1 & Param. 1 & Vet. &  &  &  \\  \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  are invisible in the original camera pose. Further, dynamic objects that move between frames act as noise in our novel-view targets. Qualitative results are in Fig. 6. EmerNerf and our approach are close to the ground truth, while UniPAD generates blurry reconstructions with scan patterns, and SelfOcc generates grayish images and struggles to reconstruct the scene precisely.

**Generalization to Unseen Waymo NORTR Dataset** As in Fig 4 and Tab 3, we evaluate the generalizability of DistillNeRF to unseen domains. Trained on the nuScenes dataset, our model demonstrates strong zero-shot transfer performance on the unseen Waymo NORTR dataset. This quality can be further enhanced by applying simple color alterations to account for camera-specific coloring discrepancies. After fine-tuning, our model surpasses the offline per-scene optimized EmerNeRF on the data, achieving higher PSNR (29.84 vs. 28.87) and SSIM (0.911 vs. 0.814). We use the full Waymo NORTR data for evaluation, and quote original EmerNeRF metrics.

**Foundation Feature Reconstruction.** We choose the DistillNeRF variant with the best RGB reconstruction performance, and train it replacing the RGB image targets with feature image targets extracted from CLIP or DINOv2. Following EmerNerf, we reduce the dimensionality of target features to 64 dimensions using Principle Component Analysis (PCA). Results in Table 1 indicate that our method can successfully reconstruct CLIP and DINOv2 features, with a reconstruction performance not far from per-scene optimized EmerNerf. Note that EmerNerf additionally learns a separate positional-encoding head to denoise target features, which could also improve DistillNeRF results in the future. In Fig. 3 we show qualitative examples for foundation feature predictions, as well as results for utilizing the predicted features for open vocabulary scene understanding. Specifically, we obtain CLIP text embeddings for keywords, such "Car", "Building", "Road", and visualize the normalized similarity of the text embedding with rendered pixel-wise CLIP features. The results indicate the ability of DistillNeRF to understand rich semantics of the scene to a remarkable extent.

**Ablations.** Ablation results in Table 1 and Fig. 7 indicate that depth distillation from offline NeRFs increases reconstruction and novel-view synthesis performance, while virtual camera distillation benefits novel-view synthesis. The parameterized space slightly reduces the rendering metrics, but as shown in Fig 7, it is capable of generating unbounded depth.

### Depth Estimation

**Setup.** We evaluate depth up to 80m using common metrics (Abs Rel, Sq Rel, RMSE, RMSE log, and \(<t\)) . We use two different depth targets: _Sparse LiDAR GT_, the common evaluation setting using LiDAR point cloud as ground truth, which is accurate but spare and has limited range (e.g. only 3m height); and _Dense Depth GT_, that uses EmmerNerf to define dense depth targets with large range. We compare against the same baselines as for rendering. For UniPAD we increase the maximum range to 80m and retrain the model. For SelfOcc, we evaluate two model variants, _SelfOcc*_ that supports depth prediction only (used in ), and _SelfOcc_ that also supports rendering (thus more similar to our method). Same as before, we evaluate over the second frame of each scene.

**Depth Comparison.** Results in Table 2 and Fig. 6 show that while EmerNerf has superior depth accuracy by being optimized for each scene, our method outperforms prior SOTA generalizable NeRFs (SelfOcc and UniPAD). Specifically, while SelfOcc which only considers the depth prediction task shows high performance (noted as SelfOcc*), when we evaluate the model that supports both depth and rendering (noted as SelfOcc), the performance drops considerably. Looking at Fig. 6, SelfOcc and UniPAD generate unreasonable depths for higher regions of the image, which is not reflected when evaluated against the sparse LiDAR ground truth. When evaluated on dense depth targets (Table 2b), their performance drops, while our approach shows more consistent performance for the two sources of ground truth.

**Ablations.** Consistently with previous results, distillation from offline NeRFs also improve depth estimation (Table 2). Quantitatively (Fig. 7), without depth distillation, we see inconsistent depth predictions between low and high regions of the image; without parameterized space, the model

  
**Method** & **PSNR** & **SSIM** \\  EmerNeRF & 28.87 & 0.814 \\  Zero-Shot Transfer & 21.03 & 0.841 \\ Zero-Shot Transfer + Recolor & 24.85 & 0.867 \\ Finetune & **29.84** & **0.911** \\   

Table 3: Trained on the nuScenes dataset, DistillNeRF shows strong generalizability to the unseen Waymo NORTR dataset.

can only predict depth in a limited depth range, while parameterized space we can generate reasonable unbounded depth.

### 3D Semantic Occupancy Prediction

**Setup.** To evaluate the zero-shot downstream capabilities of DistillNeRF, we run evaluation on the Occ3D-nuScenes dataset  for 3d semantic occupancy prediction. The dataset comprises semantic occupancy labels with 18 classes in the range [-40m, -40m, -1m, 40m, 40m, 5.4m] with voxel size 0.4m. We evaluate both binary and semantic 3d occupancy prediction. In DistillNeRF we use density thresholding (<0.001) to define whether a voxel is occupied. For semantic occupancy prediction, following , we use a pre-trained open vocabulary model GroundedSAM [56; 57; 58] to generate 2D semantic masks for the input images. Then, we project the center of occupied voxels onto the 2D masks to get the semantic class, following . We found that the resolution of input and output images is important for occupancy prediction with DistillNeRF, so we increased them to 400x228 and 200x114, respectively.

We compare our method with SOTA self-supervised methods that do not use occupancy annotation: SimpleOcc , OccNeRF , and SelfOcc . Following prior work, we report Intersection-over-Union (IoU) for each semantic category, mean IoU over all categories (mIoU), and geometry only IoU (G-IoU) for binary occupancy that ignores the semantic class. Additionally, we also report mean IoU over foreground categories (F-mIoU), that is, for categories excluding, _drivable surface_, _sidewalk_, _terrain_, _other flat_, and _others_.

**Results.** We show the comparison of occupancy prediction in Table 4. Our approach achieves competitive performance and excels on F-mIoU compared to the baselines, presumably because the sparse voxel representation emphasizes and better fits the foreground objects. SelfOcc (TPV) produces the highest mIoU and G-IoU, in part because it takes advantage of the fact that these metrics are dominated by ground-related classes (drive. surf., sidewalk, terrain), and it learns a prior for predicting the ground level as occupied even for non-visible regions (Fig.4 in ). Comparing ablations from DistillNeRF, we observe that distillation from offline NeRFs significantly improves performance (8.93 vs. 4.63 mIoU).

## 5 Conclusion

We proposed a framework for generalizable 3D scene representation prediction from sparse multi-view image inputs, using distillation from per-scene optimized NeRFs and visual foundation models. We also introduced a novel model architecture with spare hierarchical voxels. Our method achieved promising results in various downstream tasks.

Our approach is not without limitations. First, we currently rely on LiDAR to train offline EmerNerfs for distillation. Second, our sparse voxel representation naturally trades off rendering efficiency for dense scene representation, and thus may not be suitable for all downstream tasks. An interesting idea would be to combine a low-resolution dense voxel with a sparse voxel, or explore respresentations similar to Gaussian Splatting instead of voxels. Finally, there are numerous exciting directions for future work, including introducing temporal input, learning static-dynamic decomposition similar to EmerNerf, and utilizing the learned rich 3D scene representation for downstream tasks, such as detection, tracking, mapping, and planning.