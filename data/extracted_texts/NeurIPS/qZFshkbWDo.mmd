# Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense

Rui Min\({}^{1}\), Zeyu Qin\({}^{1}\), Nevin L. Zhang\({}^{1}\), Li Shen, Minhao Cheng\({}^{2}\)

\({}^{1}\)Hong Kong University of Science and Technology, \({}^{2}\)Pennsylvania State University

{rminaa, zeyu.qin}@connect.ust.hk, lzhang@cse.ust.hk

mathshenli@gmail.com, minhaocheng@ust.hk

Equal contribution. Correspondence to: Zeyu Qin (zeyu.qin@connect.ust.hk), Li Shen, Minhao Cheng.

###### Abstract

Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as they allow attackers to manipulate model predictions with backdoor triggers. To address these security vulnerabilities, various backdoor purification methods have been proposed to purify compromised models. Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs. However, _Does achieving a low ASR through current safety purification methods truly eliminate learned backdoor features from the pretraining phase?_ In this paper, we provide an affirmative answer to this question by thoroughly investigating the _Post-Purification Robustness_ of current backdoor purification methods. We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples. Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models. We find the failure to achieve satisfactory post-purification robustness stems from the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. To improve the post-purification robustness, we propose a straightforward tuning defense, Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates. Extensive experiments demonstrate that PAM significantly improves post-purification robustness while maintaining a good clean accuracy and low ASR. Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.

## 1 Introduction

Backdoor attacks [2; 7; 16] have emerged as one of the most significant concerns [4; 5; 20; 25] in deep learning. These attacks involve the insertion of malicious backdoor triggers into the training set, which can be further exploited to manipulate the behavior of the model during the inference stage. To defend against these threats, researchers have proposed various safety tuning methods [20; 27; 32; 44; 49; 50; 55] to purify well-trained backdoored models. These methods can be easily incorporated into the existing model deployment pipeline and have demonstrated state-of-the-art effectiveness in reducing the Attack Success Rate (ASR) of backdoored models [32; 48].

However, a critical question arises: _does achieving a low Attack Success Rate (ASR) through current safety tuning methods genuinely indicate the complete removal of learned backdoor features from the pretraining phase?_ If the answer is no, this means that the adversary may still easily reactivate theimplanted backdoor from the residual backdoor features lurking within the purified model, thereby exerting insidious control over the model's behavior. This represents a significant and previously unacknowledged safety concern, suggesting that current defense methods may only offer _superficial safety_. Moreover, if an adversary can successfully re-trigger the backdoor, it raises another troubling question: how can we assess the model's robustness against such threats? This situation underscores the urgent need for a more comprehensive and faithful evaluation of the model's safety.

In this work, we provide an affirmative answer to these questions by thoroughly investigating the **Post-Purification Robustness** of state-of-the-art backdoor safety tuning methods. Specifically, we employ the _Retuning Attack_ (RA) [36; 42] where we first retune the purified models using an extremely small number of backdoored samples and tuning epochs. Our observations reveal that current safety purification defense methods quickly reacquire backdoor behavior after just a few epochs, resulting in significantly high ASR levels. In contrast, the clean model (which does not have backdoor triggers inserted during the pretraining phase) and _Exact Purification_ (EP)--which fine-tunes models using real backdoored samples with correct labels during safety purification, maintain a low ASR even after the RA. This discrepancy suggests that existing safety tuning methods do not thoroughly eliminate the learned backdoor, creating a _superficial impression of backdoor safety_. Since the vulnerability revealed by the Retuning Attack (RA) relies on the use of retuned models, we further propose the more practical Query-based Reactivation Attack (QRA). This attack is capable of generating sample-specific perturbations that can trigger the backdoor in purified models, which were previously believed to have eliminated such threats, simply by querying these purified models.

To understand the inherent vulnerability of current safety purification methods concerning post-purification robustness, we further investigate the factors contributing to the disparity in post-purification robustness between EP and other methods. To this end, we utilize Linear Mode Connectivity (LMC) [13; 33] as a framework for analysis. We find that _EP not only produces a solution with low ASR like other purification methods but also pushes the purified model further away from the backdoored model along the backdoor-connected path, resulting in a more distantly robust solution._ As a result, it becomes challenging for the retuning attack to revert the EP model back to the basin with high ASR where the compromised model is located. Inspired by our findings, we propose a simple tuning defense method called Path-Aware Minimization (PAM) to enhance post-purification robustness. By using reversed backdoored samples as a proxy to measure the backdoored-connected path, PAM updates the purified model by applying gradients from a model interpolated between the purified and backdoored models. This approach helps identify a robust solution that further deviates our purified model from the backdoored model along the backdoor-connected path. Extensive experiments have demonstrated that PAM achieves improved post-purification robustness, retaining a low ASR after RA across various settings. To summarize, our contributions are:

* Our work first offers a new perspective on understanding the effectiveness of current backdoor safety tuning methods. Instead of merely focusing on the commonly used Attack Success Rate, we investigate the Post-Purification Robustness of the purified model to enhance our comprehensive understanding of backdoor safety in deep learning models.
* We employ the Retuning Attack by retuning purified models on backdoored samples to assess the post-purification robustness. Our primary observations reveal that current safety purification methods are vulnerable to RA, as evidenced by a rapid increase in the ASR. Furthermore, we propose the more practical Query-based Reactivation Attack, which can reactivate the implanted backdoor of purified models solely through model querying.
* We analyze the inherent vulnerability of current safety purification methods to the RA through Linear Mode Connectivity and attribute the reason to the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. Based on our analysis, we propose Path-Aware Minimization, a straightforward tuning-based defense mechanism that promotes deviation by performing extra model updates using interpolated models along the path. Extensive experiments verify the effectiveness of the PAM method.

## 2 Related Work

Backdoor Attacks.Backdoor attacks aim to manipulate the backdoored model to predict the target label on samples containing a specific backdoor trigger while behaving normally on benign samples. They can be roughly divided into two categories : **(1)** Data-poisoning attacks: the attacker insertsa backdoor trigger into the model by manipulating the training sample \((,)(,)\), such as adding a small patch to a clean image \(\) and change the sample's label to an attacker-designated target label \(_{t}\); **(2)** Training-control attacks: the attacker has control over both the training process and the training data simultaneously . Note that data-poisoning attacks are more practical in real-world scenarios as they make fewer assumptions about the attacker's capabilities  and have resulted in increasingly serious security risks . In this work, _we focus on data-poisoning backdoor attacks_.

Backdoor Defense.Existing backdoor defense strategies could be broadly categorized into robust pretraining  and robust fine-tuning methods . Robust pretraining aims to prevent the learning of backdoor triggers during the pretraining phase. However, these methods often suffer from accuracy degradation and can significantly increase model training costs, making them impractical for large-scale applications. In contrast, robust purification methods focus on removing potential backdoor features from a well-trained model. Generally, purification techniques involve reversing potential backdoor triggers  and applying fine-tuning or pruning to address backdoors using a limited amount of clean data . While these purification methods reduce training costs, they also achieve state-of-the-art defense performance . Therefore, in this work, _we mainly focus on evaluations of robust purification methods against backdoor attacks_.

Loss Landscape and Linear Mode Connectivity.Early works  conjectured and empirically verified that different DNN loss minima can be connected by low-loss curves. In the context of the pretrain-fine-tune paradigm, Neyshabur et al.  observe that the pretrained weights guide purified models to the same flat basin of the loss landscape, which is close to the pretrained checkpoint. Frankle et al.  also observe and define the linear case of mode connectivity, _Linear Mode Connectivity_ (LMC). LMC refers to the absence of the loss barrier when interpolating linearly between solutions that are trained from the same initialization. The shared initialization can either be a checkpoint in early training  or a pretrained model . _Our work is inspired by  and also utilizes LMC to investigate the properties of purified models in relation to backdoor safety._

Deceptive AI and Superficial Safety.Nowadays, DNNs are typically pretrained on large-scale datasets, such as web-scraped data, primarily using next-token prediction loss , along with simple contrastive  and classification  objective. While these simplified pretraining objectives can lead to the learning of rich and useful representations, they may also result in deceptive behaviors that can mislead humans . One such deceptive behavior is the presence of backdoors . A compromised model can be indistinguishable from a normal model to human supervisors, as both behave similarly in the absence of the backdoor trigger. To address this critical safety risk, researchers propose post-training alignment procedures, such as safety fine-tuning . However, several studies indicate that the changes from fine-tuning are _superficial_. As a result, these models retain harmful capabilities and knowledge from pretraining, which can be elicited by harmful fine-tuning  or specific out-of-distribution (OOD) inputs . _We study this phenomenon in the context of backdoor threats and offer a deeper understanding along with mitigation strategies._

## 3 Revealing Superficial Safety of Backdoor Defenses by Accessing Post-purification Robustness

While current backdoor purification methods can achieve a very low Attack Success Rate (ASR) against backdoor attacks, this does not necessarily equate to the complete elimination of inserted backdoor features. Adversaries may further exploit these residual backdoor features to reconstruct and reactivate the implanted backdoor, as discussed in Section 3.3. This is particularly important because purified models are often used in various downstream scenarios, such as customized fine-tuning  for critical tasks . Therefore, it is crucial to provide a way to measure the robustness of purified models in defending against backdoor re-triggering, which we define as **"Post-Purification Robustness"**.

In this section, we first introduce a simple and straightforward strategy called the Retuning Attack (RA) to assess post-purification robustness. Building on the RA, we then present a practical threat known as the Query-based Reactivation Attack (QRA), which exploits the vulnerabilities in post-purification robustness to reactivate the implanted backdoor in purified models, using only model querying. First, we will introduce the preliminaries and evaluation setup.

### Problem Setup

Backdoor Purification.In this work, we focus on the poisoning-based attack due to its practicality and stealthiness. We denote the original training dataset as \(_{T}(,)\). A few training examples \((,)_{T}\) have been transformed by attackers into poisoned examples \((_{p},_{t})\), where \(_{p}\) is poisoned example with inserted trigger and a target label \(_{t}\). Following previous works [29; 32; 39; 48; 49], only a limited amount of clean data \(D_{t}\) are used for fine-tuning or pruning. For trigger-inversion methods [44; 50], we denote the reversed backdoored samples obtained through reversing methods as \((_{r},)_{r}\). We evaluate several mainstreamed purification methods, including pruning-based defense _ANP_; robust fine-tuning defense _I-BAU_ (referred to as BAU for short), _FT-SAM_ (referred to as SAM for short), _FST_, as well as the state-of-the-art trigger-reversing defense _BTI-DBF_ (referred to as BTI for short). BTI purifies the backdoored model by using both reversed backdoored samples \(_{r}\) and the clean dataset \(_{t}\) while the others use solely the clean dataset \(_{t}\). We also include _exact purification (EP)_ that assumes that the defender has full knowledge of the exact trigger and fine-tunes the models using real backdoored samples with correct labels \((_{p},)\).

Attack Settings.Following , we evaluate four representative data-poisoning backdoors including three dirty-label attacks (BadNet , Blended , SSBA ), and one clean-label attack (LC ). All experiments are conducted on BackdoorBench , a widely used benchmark for backdoor learning. We employ three poisoning rates, \(10\%,5\%\), and \(1\%\) (in _Appendix_) for backdoor injection and conduct experiments on three widely used image classification datasets, including CIFAR-10 , Tiny-ImageNet , and CIFAR-100 . For model architectures, we following , and adopt the ResNet-18, ResNet-50 , and DenseNet-161  on CIFAR-10. For CIFAR-100 and Tiny-ImageNet, we adopt pretrained ResNet-18 on _ImageNet1K_ to obtain high clean accuracy as suggested by [32; 50]. More details about experimental settings are shown in _Appendix_B.

Evaluation Metrics.Following previous backdoor works, we take two evaluation metrics, including _Clean Accuracy (C-Acc)_ (i.e., the prediction accuracy of clean samples) and _Attack Success Rate (ASR)_ (i.e., the prediction accuracy of poisoned samples to the target class) where a lower ASR indicates a better defense performance. We further adopt _O-ASR_ and _P-ASR_ metrics. The O-ASR metric represents the defense performance of original defense methods, while the P-ASR metric indicates the ASR after applying the RA or QRA.

### Purified Models Are Vulnerable to Retuning Attack

Our objective is to investigate whether purified models with low ASR completely eliminate the inserted backdoor features. To accomplish this, it is essential to develop a method for assessing the degree to which purified models have indeed forgotten these triggers. In this section, _we begin with a white-box investigation where the attacker or evaluator has access to the purified model's parameters._ Here we introduce a simple tuning-based strategy named the **Retuning Attack (RA)**[37; 42] to conduct an initial evaluation. Specifically, we construct a dataset for model retuning, which comprises a few backdoored samples (less than \(1\%\) of backdoored samples used during the training process). To maintain C-Acc, we also include benign samples from the training set, resulting in a total RA dataset with \(1000\) samples. We subsequently retune the purified models using this constructed dataset through a few epochs (\(5\) epochs in our implementation). This approach is adopted because a clean model can not be able to learn a backdoor; thus, if the purified models quickly regain ASR during the retuning process, it indicates that some residual backdoor features still exist in these purified models. Implementation details of the RA can be found in the _Appendix_B.2.

As shown in Figure 1, we observe that _despite achieving very low ASR, all purification methods quickly recover backdoor ASR with Retuning Attack_. Their quickly regained ASR presents a stark contrast to that of clean models and remains consistent across different datasets, model architectures, and poisoning rates. Note that the pruning method (ANP) and the fine-tuning method (FST), which achieve state-of-the-art defense performance, still exhibit vulnerability to RA, with an average recovery of approximately \(82\%\) and \(85\%\) ASR, respectively. In stark contrast, the EP method stands out as _it consistently maintains a low ASR even after applying RA, demonstrating exceptional post-purification robustness_. Although impractical with full knowledge of the backdoor triggers, the EP method validates the possibility of maintaining a low attack success rate to ensure post-purification robustness against RA attacks.

Moreover, this evident contrast highlights the significant security risks associated with current backdoor safety tuning methods. While these methods may initially appear robust due to a significantly reduced ASR, they are fundamentally vulnerable to backdoor reactivation, which can occur with just a few epochs of model tuning. This superficial safety underscores the urgent need for more comprehensive evaluations to ensure lasting protection against backdoor attacks. It is crucial to implement faithful evaluations that thoroughly assess the resilience of purified models, rather than relying solely on superficial metrics, to truly safeguard against the persistent threat of backdoor vulnerabilities.

### Reactivating Backdoor on Purified Models through Queries

Although our previous experiments on RA demonstrate that current purification methods insufficiently eliminate learned backdoor features, it is important to note that the success of this tuning-based method relies on the attackers' capability to change purified models' weights. This is not practical in a real-world threat model. To address this limitation, we propose **Query-based Reactivation Attack (QRA)**, which generates sample-specific perturbations that can reactivate the backdoor using only model querying. Specifically, instead of directly retuning purified models, QRA captures the parameter changes induced by the RA process and translates them into input space as perturbations. These perturbations can then be incorporated into backdoored examples, facilitating the successful reactivation of backdoor behaviors in purified models.

Figure 1: The robustness performance against various attack settings. The title consists of the used dataset, model, and poisoning rate. The _O-ASR_ metric represents the defense performance of original defense methods, while the _P-ASR_ metric indicates the ASR after applying the RA. All metrics are measured in percentage (%) Here we report the average results among backdoor attacks and defer more details in _Appendix_C.1.

Figure 2: Experimental results of QRA on both the _purified_ and _clean_ models against four types of backdoor attacks. We evaluate the QRA on CIFAR-10 with ResNet-18 and the poisoning rate is set to \(5\%\). Additional results of QRA are demonstrated in _Appendix_C.3.

To effectively translate the parameter changes into the input space, it is crucial to ensure that when applying the perturbation generated by QRA, the output of the purified model on perturbed inputs should be aligned with that of the post-RA model on original inputs without perturbations. Formally, we denote the purified model as \((_{p};)\), the model after RA as \((_{ra};)\) and their corresponding logit output as \((_{p};)\) and \((_{ra};)\). Our QRA aims to learn a perturbation generator \((;):R^{d}[-1,1]^{d}\), to produce perturbation \((;)\) for each input \(\). We formulate this process into the following optimization problem:

\[_{}_{()_{c}}[ ((_{ra};),(_{p};*(; )+))]}.\] (1)

Here \(\) is the distance metric between two output logits, \(_{c}\) is a compositional dataset incorporating both benign and backdoored samples, and \(\) controls the strength of perturbation (\(=16/255\) in our implementation). We utilize the Kullback-Leibler (KL) divergence  for \(\) and a Multilayer Perception (MLP) for \((;)\). Specifically, we flatten the input image into a one-dimensional vector before feeding into the \((;)\), and obtain the generated perturbation by reshaping it back to the original size. Details of the MLP architecture and training hyperparameters can be found in the _Appendix_B.2.

However, we observe that directly optimizing Equation 1 not only targets purified models but also successfully attacks the clean model. We conjecture that this may stem from the inaccurate inversion of reactivated backdoor triggers, which can exploit a clean model in a manner similar to adversarial examples . To mitigate such adversarial behavior, we introduce a regularization term aimed at minimizing backdoor reactivation on the clean model. Given that accessing the clean model may not be practical, we utilize the EP model \((_{c};)\) as a surrogate model instead. In sum, we formulate the overall optimization objective as follows:

\[_{}_{(,)_{c}}[ ((_{ra};),(_{p};*( ;)+))+*((_{c};* (;)+),)]},\] (2)

where \(\) represents the balance coefficient and the cross-entropy loss is used for \(\).

We demonstrate our experimental results against five purification methods on CIFAR-10 in Figure 2. Here, we report the _C-ASR_ and _P-ASR_, which represent the ASR when evaluating with perturbed clean and perturbed poisoned images, respectively. Notably, our QRA could effectively reactivate the backdoor behaviors on purified models, resulting in a significant increase of \(66.13\%\) on average in P-ASR. Our experiments also demonstrate a consistently low C-ASR on purified models, which indicates that the perturbations generated by QRA effectively reactivate the backdoored examples without affecting the predictions of benign images. Besides, the perturbation generated with QRA exclusively works on the output of backdoored samples on purified models, leading to both a low C-ASR and P-ASR on clean models. This observation further indicates that _the reversed pattern generated by QRA is not a typical adversarial perturbation but rather an accurate depiction of the parameter changes necessary for backdoor reactivation_.

Furthermore, it is worth noting that attackers may lack knowledge about the specific backdoor defense techniques utilized by the defender in practice. Thus, we embark on an initial investigation to explore the transferability of our QRA method across unknown purification methods. Specifically, we aim to determine whether the reversed perturbations optimized for one particular defense method can effectively attack purified models with other purification techniques. As shown in Figure 3, our QRA demonstrates a degree of successful transferability across various defense techniques, achieving an average P-ASR of \(32.1\%\) against all purification techniques. These results underscore the potential of QRA to attack purified models, even without prior knowledge of the defense methods employed by defenders. It also highlights the practical application of QRA in real-world situations.

Figure 3: The results of the QRA transferability. The defense method used in the attack is represented on the \(x\)-axis, while the \(y\)-axis shows the average P-ASR across other purifications.

## 4 Investigating and Mitigating Superficial Safety

### Investigating the Superficial Safety through Linear Mode Connectivity

While our previous evaluations indicate that only the EP model demonstrates exceptional post-purification robustness compared to current backdoor safety tuning methods, the factors contributing to the effectiveness of EP remain unclear. Motivated by prior studies examining fine-tuned models [13; 14; 33], we propose to investigate this intriguing phenomenon from the perspective of the loss landscape using Linear Mode Connectivity (LMC).

Following [13; 33], let \((;_{l})\) represent the testing error of a model \((;)\) evaluated on a dataset \(_{l}\). For \(_{l}\), we use backdoor testing samples. \(_{t}(_{0},_{1};_{l})=((1-t)_{0}+t_{1};_{l})\) for \(t\) is defined as the error path of model created by linear interpolation between the \((_{0};)\) and \((_{1};)\). We also refer to it as the backdoor-connected path. Here we denote the \((_{0};)\) as backdoored model and \((_{1};)\) as the purified model. We show the LMC results of the backdoor error in Figure 4 and 5. For each attack setting, we report the average results among backdoor attacks. More results on other datasets and models are shown in _Appendix C.2_.

The backdoored model and purified models reside in separate loss basins, linked by a backdoor-connected path.We present the results of LMC between purified and backdoored models in Figure 4. It is clear from the results that all purified models exhibit significant error barriers along the backdoor-connected path to backdoored model. This indicates that backdoored and purified models reside in different loss basins. Additionally, we conduct LMC between purified models with EP and with other defense techniques, as depicted in Figure 5. We observe a consistently high error without barriers, which indicates that these purified models reside within the same loss basin. Based on these two findings, we conclude that backdoored and purified models reside in two distinct loss basins connected through a backdoor-connected path.

EP deviates purified models from the backdoored model along the backdoor-connected path, resulting in a more distantly robust solution.Although introducing a high loss barrier, we observe notable distinctions between the LMC of the EP model (red solid line) and purified models (dotted lines). We observe a stable high backdoor error along the backdoor-connected path of EP until \(t<0.2\), where the interpolated model parameter \(\) has over 80% weight from the backdoored model. In

Figure 4: The evaluation of backdoor-connected path against various attack settings. The x-axis and y-axis denote the interpolation ratio \(t\) and backdoor error (1-ASR) respectively. For each attack setting, we report the average results among backdoor attacks.

contrast, other purification models show a tendency to exhibit significant increases in ASR along the path, recovering more than \(20\%\) ASR when \(t<0.5\), while the ASR for the EP model remains low (\( 2\%\)). This clear contrast suggests: _**1)** the current purification methods prematurely converge to a non-robust solution with low ASR, which is still close to the backdoored model along the backdoor-connected path; **2)** compared with purified models, EP makes the purified model significantly deviate from the backdoored checkpoint along the backdoor-connected path, resulting in a more robust solution against RA._

Accurately specified supervision is crucial for achieving stable backdoor safety.As demonstrated in our observations, the EP method attains stable robustness in the context of the RA, whereas its proxy version, the BTI method, employs reversed backdoor data as a substitute for real backdoor data, resulting in suboptimal post-purification robustness. Furthermore, notable discrepancies are evident in the Backdoor LMC results. These findings underscore that current methods for reversing backdoor triggers are still unable to accurately recover all backdoor features , thereby emphasizing the importance of precisely specified supervision in achieving stable backdoor safety. Although data generated by the BTI method does not accurately recover all backdoor features, it could serve as an effective and usable supervision dataset. In the following section, we propose an improved safety tuning method designed to mitigate superficial safety concerns based on this proxy dataset.

### Enhancing Post-Purification Robustness Through Path-Aware Minimization

Motivated by our analysis, we propose a simple tuning defense method called **Path-Aware Minimization (PAM)**, which aims to enhance post-purification robustness by promoting more deviation from the backdoored model along the backdoor-connected path like the EP method.

Since there are no real backdoor samples \(_{p}\) available, we employ the synthetic backdoored samples \(_{r}\) from the trigger-reversing method BTI  as a substitute to get the backdoor-connected path. Although BTI has a similar LMC path curve with the EP model in Figure 5, as we have discussed, tuning solely with \(_{r}\) would lead to finding a non-robust solution with low ASR.

To avoid converging to such a solution, we propose utilizing the gradients of an interpolated model \(_{d}\) between \(_{0}\) and \(\) to update the current solution \(\). As illustrated in Figure 4, the interpolated model, which lies between \(_{0}\) and \(\), exhibits a higher ASR compared to \(\). By leveraging the gradients from the interpolated model, we can perform additional updates on the \(\) which prevents premature convergence towards local minima and results in a solution that deviates from the backdoored model along this path. Specifically, for \(\), we first take a path-aware step \(_{d}}{\|_{d}\|_{2}}\) (\(_{d}=_{0}-\)) towards to \(_{0}\) and obtain the interpolated model \(+_{d}}{\|_{d}\|_{2}}\). Then we compute its gradient on \(_{r}\) to update \(\). We formulate our objective function as follows:

\[_{}_{(,)_{r} _{1}}[((+_{d}}{\|_{d} \|_{2}};),)]},\ \ \ s.t.\ _{d}=_{0}-,\] (3)

where \(\) represents the size of the path-aware step. Typically, a larger \(\) indicates a larger step towards the backdoored model \(_{0}\) along our backdoor-connected path and also allows us to obtain a larger gradient update for \(\), which results in more deviation from the backdoored model along the backdoor-connected path. The detailed algorithm is summarized in the Algorithm 1.

Figure 5: The LMC path connected from other defense techniques to EP. We evaluate the LMC results on CIFAR-10 with ResNet-18, and set the poisoning rate to \(5\%\).

**Post-Purification Robustness of PAM.** We evaluate the post-purification robustness of PAM against RA and make a comparison with EP. Using the same experimental settings in Section 3.1, we set \(\) to \(0.5\) for Blended and SSBA and \(0.9\) for the BadNet and LC attack on CIFAR-10 and set \(\) to \(0.4\) for both CIFAR-100 and Tiny-ImageNet. The results on CIFAR-10, CIFAR-100 and Tiny-ImageNet are shown in Table 1 and Table 2. We could observe that PAM significantly improves post-purification robustness against RA. It achieves _a comparable robustness performance to the EP_, with an average ASR lower than \(4.5\%\) across all three datasets after RA. In comparison to previous experimental results in Section 3.2, our PAM outperforms existing defense methods by a large margin in terms of post-purification robustness. Our PAM also achieves a stable purification performance (O-ASR), reducing the ASR below \(2\%\) on all three datasets and preserves a high C-Acc as well, yielding only around \(2\%\) drop against the original performance of C-Acc.

    &  &  &  &  &  &  \\   & & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) &  & ASR(\(\)) \\   & O-Backdoor & 94.04 & 99.99 & 94.77 & 100.0 & 94.60 & 96.38 & 94.86 & 99.99 & 94.57 & 99.09 \\  & O-Heubness (EP) & 93.57 & 90.94 & 93.47 & 3.37 & 93.93 & 0.53 & 96.64 & 0.52 & 93.52 & 1.34 \\  & P-Robustness (EP) & 92.27 & 1.08 & 93.36 & 4.93 & 92.05 & 2.47 & 93.69 & 0.41 & 92.84 & 2.22 \\  & O-Robustness (PAM) & 92.11 & 1.14 & 93.34 & 1.67 & 92.96 & 1.24 & 92.32 & 4.92 & 26.8 & 2.24 \\  & P-Robustness (PAM) & 91.66 & 3.90 & 93.38 & 2.69 & 92.20 & 3.31 & 92.15 & 8.31 & 92.35 & 4.55 \\   & O-Backdoor & 93.73 & 100.0 & 94.28 & 100.0 & 94.31 & 98.71 & 85.80 & 100.0 & 92.03 & 99.68 \\  & O-Robustness (EP) & 92.78 & 93.93 & 31.52 & 93.10 & 0.71 & 92.12 & 0.16 & 92.84 & 0.83 \\  & P-Robustness (EP) & 92.17 & 1.58 & 93.06 & 4.78 & 92.40 & 3.70 & 91.79 & 2.03 & 92.27 & 3.02 \\    & O-Robustness (PAM) & 92.43 & 1.73 & 92.63 & 0.22 & 92.89 & 1.37 & 91.06 & 2.31 & 92.25 & 1.41 \\  & P-Robustness (PAM) & 91.77 & 1.47 & 91.85 & 7.44 & 92.01 & 2.63 & 90.98 & 3.37 & 91.65 & 3.73 \\   & O-Backdoor & 93.81 & 99.92 & 94.53 & 100.0 & 93.65 & 97.70 & 94.57 & 99.90 & 94.14 & 99.98 \\  & P-Robustness (EP) & 92.84 & 0.98 & 92.10 & 1.12 & 91.84 & 0.43 & 92.91 & 0.41 & 92.42 & 0.74 \\    & O-Robustness (EP) & 92.33 & 1.48 & 91.96 & 3.51 & 89.26 & 6.28 & 92.33 & 1.71 & 91.32 & 3.25 \\    & O-Robustness (PAM) & 92.98 & 0.94 & 92.59 & 0.24 & 92.36 & 1.62 & 91.99 & 1.24 & 92.38 & 1.23 \\    & P-Robustness (PAM) & 91.49 & 1.46 & 92.75 & 0.64 & 92.32 & 14.23 & 90.60 & 3.54 & 91.79 & 4.97 \\   & O-Backdoor & 89.85 & 100.0 & 89.59 & 98.72 & 88.83 & 86.75 & 90.13 & 99.80 & 89.60 & 96.32 \\  & O-Robustness (EP) & 88.58 & 1.52 & 88.00 & 3.21 & 88.17 & 0.69 & 88.59 & 0.56 & 88.34 & 1.50 \\    & P-Robustness (EP) & 88.03 & 2.77 & 87.13 & 3.54 & 86.33 & 1.47 & 88.94 & 0.95 & 87.61 & 2.18 \\    & O-Robustness (PAM) & 88.70 & 1.22 & 87.02 & 1.08 & 87.62 & 1.61 & 87.70 & 1.81 & 87.76 & 1.43 \\    & P-Robustness (PAM) & 87.03 & 3.04 & 86.49 & 1.94 & 85.89 & 3.72 & 86.44 & 9.39 & 86.42 & 4.52 \\   

Table 1: The post-purification robustness performance of PAM on CIFAR-10. The _O-Backdoor_ indicates the original performance of backdoor attacks, _O-Robustness_ metric represents the purification performance of the defense method, and the _P-Robustness_ metric denotes the post robustness after applying RA. All metrics are measured in percentage (%).

    &  &  &  &  &  \\   & & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) \\   & O-Backdoor & 78.91 & 99.51 & 78.98 & 100.70 & 78.59 & 92.38 & 78.83 & 97.30 \\  & O-Robustness (EP) & 76.89 & 0.04 & 76.95 & 0.04 & 76.49 & 0.05 & 76.78 & 0.43 \\  & P-Robustness (EP) & 76.82 & 0.10 & 76.18 & 0.07 & 76.12 & 1.32 & 76.37 & 0.50 \\    & O-Robustness (PAM) & 74.97 & 0.29 & 76.64 & 0.19 & 75.07 & 0.14 & 75.56 & 0.21 \\  & P-Robustness (PAM) & 75.72 & 0.76 & 76.12 & 0.13 & 74.75 & 1.95 & 75.53 & 0.95 \\   & O-Backdoor & 72.60 & 99.01 & 73.68 & 99.99 & 73.02 & 97.05 & 73.10 & 98.63 \\  & O-Robustness (EP) & 70.90 & 0.02 & 71.11 & 0.01 & 70.24 & 0.01 & 70.75 & 0.13 \\    & P-Robustness (EP) & 70.59 & 0.65 & 70.93 & 0.09To further verify the post-purification with PAM, following the experimental setting in Section 4.1, we also show the LMC results of PAM in Figure 4. It is clearly observed that our PAM significantly deviates purified models from the backdoored model along the backdoor-connected path, leading to a robust solution similar to the EP method. This further confirms our findings about post-purification robustness derived from LMC.

Sensitivity Analysis of \(\).We evaluate the performance of PAM with various values of \(\) and conduct experiments on CIFAR-10 with ResNet-18 against four attacks. The experimental results are shown in Figure 6. Note that as \(\) increases, we increase the error barrier along the connected path, indicating an enhanced deviation of our purified model from the backdoored model. However, simply increasing \(\) would also compromise the competitive accuracy (C-Acc) of the purified model. In practice, it is essential to select an appropriate \(\) to achieve a balance between post-purification robustness and C-Acc. We present the model performance across various \(\) values in Table 10 of the _Appendix_. We can observe that as \(\) rises, there is a slight decrease in clean accuracy alongside a significant enhancement in robustness against the RA. Additionally, we note that performance is relatively insensitive to \(\) when it exceeds 0.3. Given that we primarily monitor C-Acc (with the validation set) in practice, we aim to achieve a favorable trade-off between these two metrics. Therefore, we follow the approach of FST  and select \(\) to ensure that C-Acc remains above a predefined threshold, such as \(92\%\).

## 5 Conclusions and Limitations

In this paper, we seek to address the following question: Do current backdoor safety tuning methods genuinely achieve reliable backdoor safety by merely relying on reduced Attack Success Rates? To investigate this issue, we first employ the Retuning Attack to evaluate the post-purification robustness of purified models. Our primary experiments reveal a significant finding: existing backdoor purification methods consistently exhibit an increased ASR when subjected to the RA, highlighting the superficial safety of these approaches. Building on this insight, we propose a practical Query-based Reactivation Attack, which enables attackers to re-trigger the backdoor from purified models solely through querying. We conduct a deeper analysis of the inherent vulnerabilities against RA using Linear Mode Connectivity, attributing these vulnerabilities to the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. Inspired by our analysis, we introduce a simple tuning defense method, Path-Aware Minimization, which actively promotes deviation from the backdoored model through additional model updates along the interpolated path. Extensive experiments demonstrate the effectiveness of PAM, surpassing existing purification techniques in terms of post-purification robustness.

This study represents an initial attempt to evaluate post-purification robustness via RA. While we propose the practical QRA method, future work is essential to develop more efficient evaluation techniques that can faithfully assess post-purification robustness. The need for such evaluations is critical, as they ensure that the perceived safety of purified models is not merely superficial. Additionally, we recognize the significant potential for enhancing QRA in the context of transfer attacks, which we aim to explore in future research. Furthermore, we plan to broaden our research by incorporating additional backdoor attack strategies and safety tuning methods applicable to generative models, such as LLMs and diffusion models , in future work. We will also apply our existing framework of evaluation, analysis, and safety tuning method to research on unlearning in large models.

Figure 6: Ablation studies of PAM across different values of \(\) against four types of backdoor attacks. We conduct our evaluations on CIFAR-10 with ResNet-18.