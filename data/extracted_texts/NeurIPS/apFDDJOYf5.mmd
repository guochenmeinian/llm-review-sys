# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

We present a method for jointly training feed-forward generalizable 3D neural scene representation and camera trajectory estimation, self-supervised only by re-rendering losses on video frames, completely without ground-truth camera poses or depth maps. We propose to leverage single-image neural scene representations and differentiable rendering to lift frame-to-frame optical flow to 3D scene flow. We then estimate \((3)\) camera poses via a robust, weighted least-squares solver on the scene flow field. Regressed poses are used to re-construct the underlying 3D scene from video frames in a feed-forward pass, where weights are shared with the neural scene representation leveraged in camera pose estimation.

We validate the efficacy of our model for feed-forward novel view synthesis and online camera pose estimation on the real-world RealEstate10K and KITTI datasets, as well as the challenging CO3D dataset. We further demonstrate results on in-the-wild scenes in Ego4D and Walking Tours streamed from YouTube. We demonstrate generalization of camera pose estimation to out-of-distribution scenes and achieve robust performance on trajectories on which a state-of-the-art SLAM approach, ORB-SLAM3 , struggles.

To summarize, the contributions of our work include:

* We present a new formulation of camera pose estimation as a weighted least-squares fit of an \((3)\) pose to a 3D scene flow field obtained via differentiable rendering.
* We combine our camera pose estimator with a multi-frame 3D reconstruction model, unlocking end-to-end, self-supervised training of camera pose estimation and 3D reconstruction.
* We demonstrate that our method performs robustly across diverse real-world video datasets, including indoor, self-driving, and object-centric scenes.

## 2 Related Work

Generalizable Neural Scene Representations.Recent progress in neural fields [23; 24; 25] and differentiable rendering [10; 26; 27; 28; 29; 30; 31] have enabled novel approaches to 3D reconstruction from few or single images [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19], but require camera poses both at training and test time. An exception is recently proposed RUST , which can be trained for novel view synthesis without access to camera poses, but does not reconstruct 3D scenes explicitly and does not yield explicit control over camera poses. We propose a method that is similarly trained self-supervised on real video, but yields explicit camera poses and 3D scenes in the form of radiance fields. We outperform RUST on novel view synthesis and demonstrate strong out-of-distribution generalization by virtue of 3D structure.

SLAM and Structure-from-Motion (SfM).SfM methods [33; 34; 35], and in particular, COLMAP , are considered the de-facto standard approach to obtaining accurate geometry and camera poses from video. Recent progress on differentiable rendering has enabled joint estimation of radiance fields and camera poses via gradient descent [36; 37; 38; 39; 40], enabling subsequent high-quality novel view synthesis. Both approaches require offline per-scene optimization. In contrast, SLAM methods usually run online [41; 22; 42], but are notoriously unreliable on rotation-heavy trajectories or scenes with sparse visual features. Prior work proposes differentiable SLAM to learn priors over camera poses and geometry [43; 44], but requires ground-truth camera poses for training. Recent work has also explored how differentiable rendering may be directly combined with SLAM [45; 46; 47; 48], usually using a conventional SLAM algorithm as a backbone and focusing on the single-scene overfitting case. We propose a fully self-supervised method to train generalizable neural scene representations without camera poses, outperforming prior work on generalizable novel view synthesis without camera poses. We do _not_ claim state-of-the-art camera pose estimation, but provide an analysis of camera pose quality nevertheless, demonstrating robust performance on sequences that are challenging to state-of-the-art SLAM algorithms, ORB-SLAM3  and Droid-SLAM .

Neural Depth and Camera Pose Estimation.Prior work has demonstrated joint self-supervised learning of camera pose and monocular depth [49; 50; 20; 51] or multi-plane images . These approaches leverage a neural network to _directly_ regress camera poses with the primary goal of training high-quality monocular depth predictors. They are empirically limited to sequences with simple camera trajectories, such as self-driving datasets, and do not enable dense, large-baseline novel view synthesis. We ablate our flow-based camera pose estimation with a similar neural network-based approach. Most closely related to our work are approaches that infer per-timestep 3D voxel grids and train a CNN to regress frame-to-frame poses [53; 54]. We benchmark with the most recent approach in this line of work, Video Autoencoder . Lastly, we strongly encourage the reader to peruse impressive concurrent work DBARF , which also regresses camera poses alongside a generalizable NeRF. Key differences are that we leverage a pose solver based on 3D-lifted optical flow for real-time odometry versus predicting iterative updates to pose and depth via a neural network. Further, we extensively demonstrate our method's performance on rotation-dominant video sequences, in contrast to a focus on forward-facing scenes. Lastly, we solely rely on the generalizable scene representation in contrast to leveraging a monocular depth model for pose estimation.

## 3 Learning 3D Scene Representations from Unposed Videos

Our model learns to map a monocular video with frames \(\{_{t}\}_{t=1}^{N}\) as well as off-the-shelf optical flow \(\{_{t}\}_{t=1}^{N-1}\) to per-frame camera poses \(\{_{t}\}_{t=1}^{N}\) and a 3D scene representation \(\) in a single feed-forward pass. We leverage known intrinsic parameters when available, but may predict them if not. We will first introduce the generalizable 3D scene representation \(\). We then discuss how we leverage \(\) for feed-forward camera pose estimation, where we lift optical flow into 3D scene flow and solve for pose via a weighted least-squares \((3)\) solver. Finally, we discuss how we obtain supervision for both the 3D scene representation and pose estimation by re-rendering RGB and optical flow for all frames. An overview of our method is presented in Fig. 1.

**Notation.** It will be convenient to treat images sometimes as discrete tensors, such as \(_{t}^{H W 3}\), and sometimes as functions \(I:^{2}^{3}\) over 2D pixel coordinates \(^{2}\). We will denote functions in italic \(I\), while we denote the corresponding tensors sampled on the pixel grid in bold as \(\).

### Defining Our Image-Conditioned 3D Scene Representation

First, we introduce the generalizable 3D scene representation we aim to train. Our discussion assumes known camera poses; in the subsequent section we will describe how we can use our scene representation to estimate them instead. We parameterize our 3D scene as a Neural Radiance Field (NeRF) , such that \(\) is a function that maps a 3D coordinate \(\) to a color \(\) and density \(\) as \(()=(,)\). To render the color for a ray \(\), points \((_{1},_{2},...,_{n})\) are sampled along \(\) between predefined near and far planes \([t_{1},t_{f}]\), fed into \(\) to produce corresponding color and density tuples \([(_{1},_{1}),(_{2},_{2}),...,(_{n}, _{n})]\), and alpha-composited to produce a final color value \(C()\):

\[C()=_{i=1}^{N}T_{i}(1-(-_{i}_{i}))_{i},T_{i}=(-_{j=1}^{i-1}_{j}_{j}),\] (1)

Figure 1: **Method Overview. Given a set of video frames, our method first computes frame-to-frame camera poses (left) and then re-renders the input video (right). To estimate pose between two frames, we compute off-the-shelf optical flow to establish 2D correspondences. Using single-view pixelNeRF , we obtain a surface point cloud as the expected 3D ray termination point for each pixel, \(\), \(\) respectively. Because \(\) and \(\) are pixel-aligned, optical flow allows us to compute 3D scene flow as the difference of corresponding 3D points. We then find the camera pose \((3)\) that best explains the 3D flow field by solving a weighted least-squares problem with flow confidence weights \(\). Using all frame-to-frame poses, we re-render all frames. We enforce an RGB loss and a flow loss between projected pose-induced 3D scene flow and 2D optical flow. Our method is trained end-to-end, assuming only an off-the-shelf optical flow estimator.**

where \(_{i}=t_{i+1}-t_{i}\) is the distance between adjacent samples. By compositing sample locations instead of colors, we can compute an expected ray-surface intersection point \(S()^{3}\):

\[S()=_{i=1}^{N}T_{i}(1-(-_{i}_{i}))_{i}.\] (2)

We require a _generalizable_ NeRF that is not optimized for each scene separately, but instead predicted in a feed-forward pass by an encoder that takes a set of \(M\) context images and corresponding camera poses \(\{(_{i},_{i})\}_{i}^{M}\) as input. We denote such a generalizable radiance field reconstructed from images \(_{i}\) as \((\{(_{i},_{i})\}_{i}^{M})\). Many such models have been proposed [1-10]. We base our model on pixelNeRF , which we briefly discuss in the following - please find further details in the supplement. pixelNeRF first extracts per-image features \(_{i}\) from each input image \(_{i}\). A given 3D coordinate \(\) is first projected onto the image plane of each context image \(_{i}\) via the known camera pose and intrinsic parameters to yield pixel coordinates \(_{i}\). We then retrieve the features \(F_{i}(_{i})\) at that pixel coordinate. Color and density \((,)\) at \(\) are then predicted by a neural network that takes as input the features \(\{F_{i}(_{i})\}_{i}^{M}\) and the coordinates of \(\) in the coordinate frame of each camera, \(\{_{i}^{-1}\}_{i}^{M}\). Importantly, we can condition pixelNeRF on varying numbers of context images, i.e., we may run pixelNeRF with only a _single_ context image as \(((,))\), or with a set of \(M>1\) context images \(((_{i},_{i})\}_{i}^{M})\).

### Lifting Optical Flow to Scene Flow with Neural Scene Representations

Equipped with our generalizable 3D representation \(\), we now describe how we utilize it to lift optical flow into confidence-weighted 3D scene flow. Later, our pose solver will fit a camera pose to the

  &  &  &  &  \\ Model & LPIPS \(\) & PSNR \(\) & LPIPS \(\) & PSNR \(\) & LPIPS \(\) & PSNR \(\) & LPIPS \(\) & PSNR \(\) \\  Vid-AE  & 0.3427 & 18.56 & 0.3889 & 18.50 & 0.4173 & 18.03 & 0.3272 & 17.65 \\ RUST  & 0.6126 & 17.50 & 0.6145 & 17.71 & 0.5692 & 18.07 & 0.5584 & 15.44 \\ Ours & **0.2250** & **23.94** & **0.2687** & **24.17** & **0.2224** & **24.25** & **0.1928** & **23.39** \\ 

Table 1: **Quantitative Comparison on View Synthesis. On the task of view synthesis, our method outperforms other unposed methods by wide margins.**

Figure 2: **Video Reconstruction Results. Our model reconstructs video frames from sparse context frames with higher fidelity than all baselines. While VidAE’s renderings often appear with convincing texture, they are often not aligned with the ground truth. RUST’s renderings are well aligned but are blurry due to their coarse set latent representation.**

estimated scene flow. Given two sequential frames \(_{t-1},_{t}\) we first use an off-the-shelf method  to estimate backwards optical flow \(V_{t}:^{2}^{2}\). The optical flow \(V_{t}\) maps a 2D pixel coordinate \(\) to a 2D flow vector, such that we can determine the corresponding pixel coordinate in \(_{t-1}\) as \(^{}=+V_{t}()\).

We will now lift pixel coordinates \(_{t}\) and \(_{t-1}\) to an estimate of the 3D points that they observe in the coordinate frame of their respective cameras. To achieve this, we cast rays from the camera centers through the corresponding pixel coordinates \(_{t}\) and \(_{t-1}\) using the intrinsics matrix \(\). Specifically, we compute \(_{t}=^{-1}}_{t}\) and \(_{t-1}=^{-1}}_{t-1}\), where \(}\) represents the homogeneous coordinate \((\\ 1)\). Next, we sample points along the rays \(_{t}\) and \(_{t-1}\) and query our pixelNeRF model in the single-view setting. This involves invoking \((|(_{t},_{4 4}))\) and \((|(_{t-1},_{4 4}))\), i.e., pixelNeRF is run with only the respective frame as the context view and the identity matrix \(\) as the camera pose. Applying the ray-intersection integral defined in Eq. 2 to the pixelNeRF estimates, we obtain a pair of 3D points \((_{t},_{t-1})\) corresponding to the estimated surfaces observed by pixels \(_{t}\) and \(_{t-1}\), respectively. We repeat this estimation for all optical flow correspondences, resulting in two sets of surface point clouds, \(,^{}^{H W 3}\). Equivalently, we may view this as defining the 3D scene flow as \(^{}-\).

**Flow confidence weights.** We further utilize a confidence weight for each flow correspondence. To accomplish this, we employ a neural network \(\), which takes image features \(F_{t}(),F_{t-1}(^{})\) as input for every pixel correspondence pair \((,^{})\). The network maps these features to a weight \(\), denoted as \((F_{t}(),F_{t-1}(^{}))=\). \(\) can importantly overcome several failure modes which lead to faulty pose estimation, including incorrect optical flow, such as in areas of occlusions, dynamic objects, such as pedestrians, or challenging geometry estimates, such as sky regions. We show in Fig. 9 that \(\) indeed learns such content-based rules.

**Predicting Intrinsic Camera Parameters K.** Camera intrinsics are often approximately known, either published by the manufacturer, saved in video metadata, or calibrated once. Nevertheless, for purposes of large-scale training, we leverage a simple scheme to predict the camera field-of-view for a video sequence. We feed the feature map of the first frame \(_{0}\) into a convolutional encoder that directly regresses the field of view. We assume that the optical center is at the sensor center. We find that this approach enables us to train on real-world video from YouTube, though more sophisticated schemes  will no doubt improve performance further.

Table 2: **Quantitative Pose Estimation Comparison. In (a) we compare against VideoAutoencoder  on short-sequence odometry estimation (\(20\) frames), reporting the ATE. In (b) we compare against ORB-SLAM3  and DROID-SLAM  on long sequences (\(\)200 frames) from the CO3D 10-Category dataset. We separately report scores on the top and bottom 50% of sequences (“Top” and “Bot.”) in terms of quality of ground-truth poses as indicated by the dataset. We report ATE and percent of sequences tracked (“Tracked”). ORB-SLAM3 fails to track over half of these challenging sequences.**

Figure 3: **Qualitative Pose Estimation Comparison**. On short sequences, we compare our pose estimation to Video Autoencoder , and on long sequences, we compare our method’s sliding-window estimations against the per-scene optimization BARF . The trajectory for the bicycle sequence was obtained using a model trained on hydrant sequences: despite never having seen a bicycle before, our model predicts accurate poses.

### Camera Pose Estimation as Explaining the Scene Flow Field

We will now estimate the camera pose between frame \(_{t}\) and \(_{t-1}\). In the previous section, we lifted the input optical flow into scene flow, producing 3D correspondences \(,^{}\), or, equivalently, 3D scene flow. We cast camera pose estimation as the problem of finding the rigid-body motion that best explains the observed scene flow field, or the transformation mapping points in \(\) to \(^{}\), while considering confidence weights \(\). Note that below, we will refer to the matrices \(\), \(^{}\), and \(\) as column vectors, with their spatial dimensions flattened.

We use a weighted Procrustes formulation to solve for the rigid transformation that best aligns the set of points \(\) and \(^{}\). The standard orthogonal Procrustes algorithm solves for the \((3)\) pose such that it minimizes the least squares error:

\[*{arg\,min}_{(3)}}-}^{}_{2}^{2},\] (3)

with \(=(&\\ 0&1)\) as a rigid-body pose with rotation \(\) and translation \(\) and homogeneous \(}=(\\ 1)\). In other words, the minimizer of this loss is the rigid-body transformation that best maps \(\) onto \(^{}\), and, in a static scene, is therefore equivalent to the sought-after camera pose.

As noted by Choy et al. , this formulation equally weights all correspondences. As noted in the previous section, however, this would make our pose estimation algorithm susceptible to both incorrect correspondences as well as correspondences that should be down-weighted by nature of belonging to parts of the scene that are specular, dynamic, or have low confidence in their geometry estimate. Following , we thus minimize a _weighted_ least-squares problem:

\[*{arg\,min}_{(3)}^{1/2}( }-}^{})_{2}^{2}\] (4)

with the diagonal weight matrix \(=()\). Conveniently, this least-squares problem admits a closed-form solution, efficiently calculated via Singular Value Decomposition, as derived in :

\[=^{T} =(-^{}), ^{T}= (_{^{}}),\] (5) \[_{^{}}= ^{T},=-} }^{T},=(1,...,()( )).\] (6)

**Composing frame-to-frame poses.** Solving this weighted least-squares problem for each subsequent frame-to-frame pair yields camera transformations \((_{2}^{},_{3}^{},,_{n}^{})\), aligning each \(_{t}\) to its predecessor \(_{t-1}\). We compose frame-to-frame transformations to yield camera poses \((_{1},_{2},,_{n})\) relative to the first frame, such that \(_{1}=_{3 3}\), concluding our camera pose estimation module.

### Supervision via Differentiable Video and Flow Re-Rendering

We have discussed our generalizable neural scene representation \(\) and our camera pose estimation module. We will now discuss how we derive supervision to train both modules end-to-end. We have two primary loss signals: First, a photometric loss \(_{}\) scores the visual fidelity of re-rendered video frames. Second, a pose-induced flow loss \(_{}\) scores how similar the flow induced by thepredicted camera transformations and surface estimations are to optical flow estimated by RAFT . Our model is trained on short (\(\)15 frames) video sequences.

**Photometric Loss.** Our photometric loss \(_{}\) comprises two terms: A multi-context loss and a single-context loss. The multi-context loss ensures that the full 3D scene is reconstructed accurately. Here, we re-render each frame of the input video sequence \(_{t}\), using its estimated camera pose \(_{t}\) and multiple context images \(\{(_{j},_{j})\}_{j}^{J}\). The single-context loss ensures that the single-context pixelNeRF used to estimate surface point clouds \(_{t}\) in the pose estimation module is accurate.

\[_{}=_{i=t}^{N}_{t}-C(_{t}\{(_{j},_{j})\}_{j}^{J})\|_{2}^{2}}_{ }+_{t}-C(_{4 4 }(_{t},_{4 4}))\|_{2}^{2}}_{},\] (7)

where, in a slight abuse of notation, we have overloaded the rendering function \(C(|\{(_{j},_{j})\}_{j}^{J})\) defined in Eq. 1 as rendering out the full image obtained by rendering a pixelNeRF with context images \(\{(_{j},_{j})\}_{j}^{J}\) from camera pose \(\). \(N\) refers to the number of frames in the video sequence and \(J\) refers to the subset of context frames use for re-rendering the entire sequence. We first attempted picking the first frame only, however, found that this does _not_ converge due to the uncertainty of the 3D scene given only the first frame: single-view pixelNeRF will generate blurry estimates for parts of the scene that have high uncertainty, such as occluded regions or previously unobserved background.

**Pose-Induced Flow Loss.** An additional, powerful source of supervision for both the estimated geometry and camera poses can be obtained by comparing the optical flow induced by the predicted surface point clouds and pose with the off-the-shelf optical flow. We define this pose-induced flow loss as

\[_{}=_{t=1}^{N-1}\|_{t}-( (_{t}^{-1}_{t+1}_{t+1})-)\|_{2}^{2},\] (8)

with projection operator \(()\) and grid of pixel coordinates \(^{2}\). Intuitively, this transforms the surface point cloud of frame \(t+1\) into the coordinate frame of frame \(t\) and projects it onto that image plane. For every pixel coordinate \(\) at timestep \(t+1\), this yields a corresponding pixel coordinate \(^{}\) at timestep \(t\), which we compare against the input optical flow.

### Test-time Inference

After training our model on a large dataset of short video sequences, we may infer both camera poses and a radiance field of such a short sequence in a single forward pass, without test-time optimization.

**Sliding Window Inference for Odometry on Longer Trajectories.** Our method estimates poses for short (\(\)15 frames) subsequences in a single feed-forward pass. To handle longer trajectories that exceed the capacity of a single batch, we divide a given video into non-overlapping subsequences. We estimate poses for each subsequence individually and compose them to create an aggregated trajectory estimate. This approach allows us to estimate trajectories for longer video sequences.

**Test-Time Adaptation.** Frame-to-frame camera pose estimation methods, both conventional and the proposed method, accumulate pose estimation error over the course of a sequence. SLAM and SfM methods usually have a mechanism to correct for drift by globally optimizing over all poses and closing loops in the pose graph . We do not have such a mechanism, but propose fine-tuning our model on specific scenes for more accurate feed-forward pose and 3D estimation. For a given video sequence, we may fine-tune our pre-trained model using our standard photometric and flow losses on sub-sequences of the video. Note that this is _not_ equivalent to per-scene optimization or _direct_

Figure 6: **Wide-Baseline View Synthesis. Given an input video without poses, our model first infers camera poses and can then render wide-baseline novel views of the underlying 3D scene, where we use the first, middle, and final frame of the video as context views.**

optimization of camera poses and a radiance field, as performed e.g. in BARF : neither camera poses nor the radiance field are free variables. Instead, we fine-tune the weights of our convolutional inference backbone and MLP renderer for more accurate feed-forward prediction.

## 4 Experiments

We benchmark our method on generalizable novel view synthesis on the RealEstate10k , CO3D , and KITTI  datasets. We provide further analysis on the Tanks & Temples dataset and in-the-wild scenes from Ego4D  and YouTube. Though we do not claim to perform state-of-the-art camera pose estimation, we nevertheless provide an analysis of the accuracy of our estimated camera poses. Please find more results, as well as precise hyperparameters, implementation, and dataset details, in the supplemental document and video. We utilize camera intrinsic parameters where available, predicting them only for the in-the-wild Ego4D and WalkingTours experiments.

Pose Estimation.We first evaluate our method on pose estimation against the closest self-supervised neural network baseline, Video Autoencoder (VidAE) . We then analyze the robustness of our pose estimation with ORB-SLAM3  and DROID-SLAM  as references. Finally, we benchmark with BARF , a single-scene unposed NeRF baseline. Tab. 1(a) compares accuracy of estimated poses of our method and VidAE on all four datasets. The performance gap is most pronounced on the challenging CO3D dataset, but even on simpler, forward-moving datasets, RealEstate10k and KITTI, our method significantly outperforms VidAE. Next, we analyse the robustness of our pose estimation module on CO3D, using SfM methods ORB-SLAM3 and DROID-SLAM as references. See Tab. 1(b) and Fig. 4 for quantitative and qualitative results. To account for inaccuracies in the provided CO3D poses we utilize as ground-truth, we additionally report separate results for the top and bottom 50% of sequences, ranked based on the pose confidence scores provided by the authors. Although we do not employ any secondary pose method as a proxy ground truth for the bottom half of sequences, this division serves as an approximate indication of the level of difficulty each sequence poses from a SfM perspective. On both subsets, our method outperforms both DROID-SLAM and ORB-SLAM3. Also note that ORB-SLAM3 fails to track poses for over half (50.7%) of the sequences. On the sequences where ORB-SLAM3 succeeds, our method predicts poses significantly more accurately. Even on the sequences where ORB-SLAM3 fails, our performance does not degrade (.025 ATE). Lastly, we compare against the single-scene unposed NeRF baseline, BARF. Since BARF requires \(\)one day to reconstruct a single sequence, we evaluate on two representative sequences: a forward-walking sequence on RealEstate10K, and an outside-in trajectory on CO3D. We plot recovered trajectories in Fig. 3. While BARF fails to recover the correct trajectory shape on the CO3D scene, our method produces a trajectory that more accurately reflects the ground-truth looping structure.

Novel View Synthesis.We compare against VidAE  and RUST  on the task of novel view synthesis. Tab. 1 and Fig. 6 report quantitative and qualitative results respectively. Our method outperforms both baselines significantly. Since VidAE fails to capture the pose distribution on the CO3D datasets, its novel view renderings generally do not align with the ground truth. On RealEstate10K and KITTI, their method successfully captures the pose distribution, but still struggles

Figure 7: **Fine-tuned Pose Estimation and View Synthesis on Large-Scale, Out-of-Distribution Scene. We evaluate our RealEstate10K-trained model on a significantly out-of-distribution scene from the Tanks and Temples dataset , first without any scene-adaptation and then with. Even with this significant distribution gap, our method’s estimated trajectory captures the looping structure of the ground truth, albeit with accumulated drift. After a scene-adaptation fine-tuning stage (around 7 hours), our model estimates poses which align closely with the ground truth. We also plot the trajectory estimated by BARF , which fails to capture the correct pose distribution.**

[MISSING_PAGE_FAIL:9]

Discussion

Limitations.While we believe our method makes significant strides, it still has several limitations. As an odometry method, it accumulates drift and has no loop closure mechanism. Our model further does not currently incorporate scene dynamics, but recent advancements in dynamic NeRF papers  present promising perspectives for future research.

Conclusion.We have introduced FlowCam, a model capable of regressing camera poses and reconstructing a 3D scene in a single forward pass from a short video. Our key contribution is to factorize camera pose estimation as first lifting optical flow to pixel-aligned scene flow via differentiable rendering, and then solving for camera pose via a robust least-squares solver. We demonstrate the efficacy of our approach on a variety of challenging real-world datasets, as well as in-the-wild videos. We believe that they represent a significant step towards enabling scene representation learning on uncurated, real-world video.

Acknowledgements.This work was supported by the National Science Foundation under Grant No. 2211259, by the Singapore DSTA under DST000ECI20300823 (New Representations for Vision), and by the Amazon Science Hub. The Toyota Research Institute also partially supported this work. This article solely reflects the opinions and conclusions of its authors and no other entity.