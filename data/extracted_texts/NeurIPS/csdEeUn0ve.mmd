# Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations

Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations

Minshuo Chen\({}^{1}\)   Yu Bai\({}^{2}\)   H. Vincent Poor\({}^{1}\)   Mengdi Wang\({}^{1}\)

\({}^{1}\)Princeton University  \({}^{2}\)Salesforce Research

###### Abstract

In real-world reinforcement learning (RL) systems, various forms of _impaired observability_ can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form \(}((H)SAK})\), for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal value obtained with full observability.

## 1 Introduction

In Reinforcement Learning (RL), an agent engages with an environment in a sequential manner. In an ideal setting, at each time step, the agent would observe the current state of the environment, select an action to perform, and receive a reward Smallwood and Sondik (1973); Bertsekas (2012); Sutton and Barto (2018); Lattimore and Szepesvari (2020). However, real-world engineering systems often introduce impaired observability and latency, where the agent may not have immediate access to the instant state and reward information. In systems with lossy communication channels, certain state observations may even be permanently missing, never reaching the agent. Nevertheless, the agent is still required to make real-time decisions based on the available information.

The presence of impaired observability transforms the system into a complex interactive decision process (Figure 1), presenting challenges for both learning and planning in RL. With limited knowledge about recent states and rewards, the agent's policy must extract information from the observed history and utilize it to make immediate decisions. This introduces significant complexity to the policy class and poses difficulties for RL. Moreover, the loss of information due to permanently missing observations further hampers the efficiency of RL methods. Although a naive approach would involve augmenting the state and action space to create a fully observable Markov Decision Process (MDP), such a method would lead to exponential regret growth in the state-action size.

Why existing methods do not work.One may be tempted to cast the problem of impaired observability into a Partially Observed MDPs (POMDPs). However, this would not solve the problem. In POMDP, the system does not reveal its instant state to the agent but provides an emission state observation conditioned on the latent state. POMDPs are known to suffer from the curse of history Papadimitriou and Tsitsiklis (1987); Bertsekas (2012); Krishnamurthy (2016), unless additional assumptions are imposed. Existing efficient algorithms focus on subclasses of POMDPs with decodable or distinguishable partial observations Jin et al. (2020); Uehara et al. (2022); Zhan et al.

, Chen et al. , Liu et al. , Zhong et al. , Chen et al. , where the unseen instant state can be inferred from recent observations. Unfortunately, MDPs with impaired observability do not fall into these benign subclasses. The reason behind this is that at each time step, a new observation, if any, is in fact a past state. Viewing it as an emission state of the current one leads to a time reversal posterior distribution depending on the underlying transitions, which suffers from the curse of history and makes the POMDP intractable. The problem becomes even harder if some observations get missing.

Empirical evidences suggested that efficient RL is possible even with impaired state observability , Liu et al. , Agarwal and Aggarwal . However, theoretical understanding of this problem is very limited. One notable work Walsh et al.  studied learning with constant-time delayed observations. They identified subclasses of MDPs with nearly deterministic transitions that can be efficiently learned. Beyond this special case, efficient RL with impaired observability in MDPs with fully generality remains largely open.

Some recent works studied delayed feedback in MDPs , Howson et al. . It is a fundamentally different problem where the agent's policy can still access real-time states but learning uses delayed data. Our problem is fundamentally harder because the agent's policy can only access the lossy and delayed history. See Section 1 for more discussions.

Our results.In this paper, we provide algorithms and regret analysis for learning the optimal policy in tabular MDPs with impaired observability. Note that this optimal policy is a different one from the optimal policy with full observability. To approach this problem, we construct an augmented MDP reformulation where the original state space is expanded to include available observations of past state and an action sequence. However, the expanded state space is much larger than the original one and naive application of known methods would lead to exponentially large regret bounds. In our analysis, we exploit structure of the augmented transition model to achieve efficient learning and sharp regret bounds. The main results are summarized as follows.

\(\) For MDPs with stochastic delays, we prove a sharp \((H^{4})\) regret bound (Theorem 4.1) comparing to the best feasible policy, Here \(S\) and \(A\) are the sizes of the original state and action spaces, respectively, \(H\) is the horizon, and \(K\) is the number of episodes. Here we allows the delay to be stochastic and conditionally independent given on current state and action. Moreover, we quantify the performance degradation of optimal value due to impaired observability, compared to optimal value of fully observable MDPs (Proposition B.2). We also showcase in Proposition 4.2 that a short delay does not reduce the optimal value, but slightly longer delay leads to substantial degradation.

\(\) For MDPs with randomly missing observations, we provide an optimistic RL method that provably achieves \(}(S^{2}AK})\) regret (Proposition 5.1). We also provide a sharper \(}(H^{4})\) regret in the case when the missing rate is sufficiently small (Theorem 5.2).

To our best knowledge, these results present a first set of theories for RL with delayed and missing observations. Remarkably, our regret bounds nearly match the minimax-optimal regret of standard MDP in their dependence on \(S,A\) (noting that the target optimal policies are different in the two cases). It implies that RL with impaired observability are provably as efficient as RL with full observability (up to poly factors of \(H\)).

Related work.Efficient algorithms for learning in the standard setting of tabular MDPs without impaired observability has been extensively studied Kearns and Singh , Brafman and Tennenholtz , Jaksch et al. , Dann and Brunskill , Azar et al. , Agrawal and Jia ,

Figure 1: Reinforcement learning with impaired observability. At time \(h\), the agent only observes the past state \(s_{h-d}\) and actions \(a_{h-d},,a_{h-1}\). The policy depends on the observed information.

Jin et al. (2018); Dann et al. (2019); Zanette and Brunskill (2019); Zhang et al. (2020); Domingues et al. (2021), where the minimax optimal regret is \(}(SAK})\)Azar et al. (2017); Domingues et al. (2021).

The delayed observation studied in this paper is related to delayed feedback in Howson et al. (2023); Yang et al. (2023), yet the setup is fundamentally different. In delayed feedback, an agent sends a policy to the environment for execution. The environment executes the policy on behalf of the agent for an episode, but the whole trajectory will be returned to the agent after some episodes. The policy executed by the environment is able to "see" instant state and reward. It is Markov and not played by the agent. Our setting concerns learning executable policies when delayed or missing states appear within an episode. The policy is no longer Markov and can only prescribe action based on history. Therefore, the algorithms and analyses for delayed feedback MDPs are not applicable to our settings.

Despite the distinct settings, there are existing fruitful results in efficiently learning MDPs or bandits with delayed feedback. Stochastic delayed feedback in bandits is studied in Agarwal and Duchi (2011); Dudik et al. (2011); Joulani et al. (2013); Vernade et al. (2017, 2020); Gael et al. (2020); Lancewicki et al. (2021). In the more challenging setting of reinforcement learning, Howson et al. (2023) considers tabular MDPs and Yang et al. (2023) generalizes to MDPs with function approximation and multi-agent settings.

On the other hand, results analyzing MDPs with missing observations are limited in literature, although missing data is a commonly recognized issue in applications Garcia-Laencina et al. (2010); Jerez et al. (2010); Little et al. (2012); Emmanuel et al. (2021). One notable result is Bouneffouf et al. (2020) for bandits with missing rewards.

**Notation**: For real numbers \(a,b\), we denote \(a b=\{a,b\}\). In episodic MDPs, we use the superscript \(k\) to denote the index of episodes, and the subscript \(h\) to denote the index of time. We denote \(_{i:j}=\{a_{i},,a_{j}\}\) as the collection of actions from time \(i\) to \(j\). For two probability distributions \(\) and \(\), we denote their total variation distance as \(\|-\|_{}\).

**MDP preliminary**: An episodic MDP is described by a tuple \((,,H,R,P)\), where \(,\) are state and action spaces, respectively, \(H\) is the horizon, \(R=\{r_{h}\}_{h=1}^{H}\) is the reward function and \(P=\{p_{h}\}_{h=1}^{H}\) is the transition probability. We primarily focus on tabular MDPs, where \(S=||\) and \(A=||\) are both finite. We also assume that the reward is uniformly bounded with \(\|r_{h}\|_{} 1\) for any \(h\). An agent will interact with the environment for \(K\) episodes, hoping to find a good policy to maximize the cumulative reward. Within an episode, at the \(h\)-th step, the agent chooses an action based on the available information of the environment. After taking the action, the underlying environment produces a reward and transits to the next state. With full state observation, a policy \(\) maps instant state \(s\) to an action \(a\) or an action distribution. Given such a policy \(\), the value function is \(V_{h}^{}(s_{1})=^{}[_{h^{}=h}^{H}r_{h}(s_{h^{ }},a_{h^{}})|s_{h}],\) where \(^{}\) is the policy induced expectation.

## 2 Problem formulation

In this work, we study MDPs with impaired observability. We focus on two practical settings: 1) delayed observations and 2) missing observations.

### MDP with delayed observations

In any episode, we denote \(d_{h}\{0,1,\}\) as the observational delay of the state and reward at step \(h\). That is, we receive \(s_{h}\) and \(r_{h}\) at time \(h+d_{h}\). The delay time \(d_{h}\) can be dependent on the state \(s_{h}\) and action \(a_{h}\) at time \(h\). To facilitate analysis, we denote the inter-arrival time between the arrival of observations for step \(h\) and \(h+1\) as \(_{h}=d_{h+1}-d_{h}\). With delays, at time \(h\), the nearest observable state is denoted as \(s_{t_{h}}\), where \(t_{h}=*{argmax}\ \{I:_{i=0}^{I}_{i} h\}\). Then the executable policy class

\[_{}=\{_{h}(|s_{t_{h}},_{t_{h}:h-1})h=1,,H\}\]

chooses actions depending on the nearest visible state and history actions. We impose the following assumption on the interarrival time.

**Assumption 2.1**. The interarrival time \(_{h}\) takes value in \(\{0,1,\}\). The distribution \(_{h}(s_{h},a_{h})\) of \(_{h}\) can depend on \((s_{h},a_{h})\), but is conditionally independent of the MDP transitions given \((s_{h},a_{h})\).

Assumption 2.1 does not impose any specific distributional assumption on \(_{h}\), but only requires that the delayed observations arrive in order and at each time step, there is at most one new visible state and reward pair (\(_{h} 0\)). A widely studied example of delays in literature is that the inter-arrival time is geometrically distributed Winsten (1959). Then the observation sequence \(\{h+d_{h}\}\) is known as a Bernoulli process, which is understood as the discretized version of a Poisson process.

Our delayed observation setting is newly proposed and substantially generalizes the Constant Delayed MDPs (CMDDPs) studied in Brooks and Leondes (1972); Bander and White III (1999); Katsikopoulos and Engelbrecht (2003); Walsh et al. (2007). When \(_{h}=0\) being deterministic for all \(h 1\) and \(k\), our observation delay coincides with CDMDPs. In CDMDPs, a new past observation is guaranteed to arrive at each time step. However, our delayed model can result in no new observation at some time steps.

Observation delay leads to difficulty in planning, as the agent can only infer the current state and then choose an action. Therefore, the policy is naturally history dependent. We summarize the interaction protocol of the agent with the environment in Protocol 1.

```
1:for episode \(k=1,,K\)do
2:for time \(h=1,,H\)do
3: The agent observes a pair of new, if any, state and reward \((s^{k}_{t_{h}},a^{k}_{t_{h}})\). By memory, the agent also has access to past actions \(^{k}_{t_{h}:h-1}\).
4: The agent plays action \(a^{k}_{h}\) according to some executable policy \(^{k}_{h}_{}\).
5: The environment transits to next state \(s^{k}_{h+1} p_{h}(|s^{k}_{h},a^{k}_{h})\), which is unobservable to the agent. The environment also decides the delay at step \(h+1\) as \(d^{k}_{h+1}=d^{k}_{h}+^{k}_{h}\) and \(t^{k}_{h+1}\).
6:endfor
7: The environment sends all unobserved pairs of state and reward as well as their corresponding delay time to the agent.
8:endfor ```

**Protocol 1** Interaction between the agent and the environment with delayed observations

All delayed observations, however, these observations are not used in planning. In reality, the agent can collect these observations by waiting after time \(H\). Protocol 1 is similar to hindsight observability in POMDPs studied in Lee et al. (2023). Yet their analysis for POMDPs is not directly transferable to our settings as mentioned in the introduction.

### MDP with missing observations

In addition to the stochastic delay in observations, we also consider randomly missing observations. In applications, an agent interacts with the environment through some communication channel. The communication channel is often imperfect and thus, observation can be lost during transmission. This type of missing is permanent and we describe in the following assumption.

**Assumption 2.2**.: Any pair of observation (state and reward) is independently observable in the communication channel. The observation rate is \(_{h}\) depending on \(h\), but independent of the MDP transitions. Moreover, there exists a constant \(_{0}\) such that \(_{h}_{0}\) for any \(h\). The agent will be informed when an observation is missing.

Equivalently, the missing observation rate in Assumption 2.2 is \(1-_{h}\) and assumes the upper bound of \(1-_{0}\). We will show later that this missing rate directly influences the learning efficiency in Section 5.

## 3 Construction of augmented MDPs

To tackle the limited observability, we expand the original state space and define an augmented MDP. It will serve as the basis for our subsequent theoretical analysis.

### Augmented MDPs with expected reward

In the remainder of this section, we focus on the delayed observation case and defer the missing case to Section 5. Define \(_{h}=\{s_{t_{h}},_{t_{h},h-1},_{t_{h}}\}\) as the augmented state, where \(_{t_{h}}[0,_{t_{h}}]\) is the delayed steps after observing \((s_{t_{h}},r_{t_{h}})\). Let \(_{}\) denote the augmented state space of all possible \(\)'s. Then the original MDP with delayed observations can be reformulated into a state-augmented one \(_{}=(_{},,H,R_{ },P_{})\). The reward is defined as

\[r_{h,}(_{h},a_{h})=[r_{h}(s_{h},a_{h})|_{h}, a_{h}],\]

which is the expected reward given the nearest past state \(s_{t_{h}}\) and history actions \(_{t_{h}:h}\). We can define belief distribution \(_{h}(s|_{h})=(s_{h}=s|_{h})\). Then \(r_{h,}(_{h},a_{h})=_{s_{h}(| _{h})}[r(s,a_{h})]\). Belief distributions are widely adopted in partially observed MDPs Ross et al. (2007), Poupart and Vlassis (2008). We will frequently use the belief distribution to study the expressivity of \(_{}\) in Section 4.2.

The transition probabilities \(P_{}\) are sparse. For any \(_{h}=\{s_{t_{h}},_{t_{h}:h-1},_{t_{h}}\}\) and \(_{h+1}=\{s_{t_{h+1}},_{t_{h+1}:h},_{t_{h+1}}\}\), we have

\[}(_{h+1}|_{h},a_{h}) }{_{a}(_{h},_{h+1})_{}(s_{t_{h}},a_{t_{ h}},_{t_{h}})p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a_{t_{h}})}&_{t_{h+1}}=0t_{h+1}=t_{h}+1\\ _{a}(_{h},_{h+1})(1-_{}(s_{t_{h}},a_{t_{ h}},_{t_{h}}))&_{t_{h+1}}=_{t_{h}}+1t_{h+1}=t_{h}\\ 0&\]

where \(_{a}(_{h},_{h+1})\) indicates whether the rolling actions are matched, i.e.,

\[_{a}(_{h},_{h+1})=\{_{t_{h}:h-1}= _{t_{h+1}:h-1}\},\]

and \(_{}(s_{t_{h}},a_{t_{h}},_{t_{h}})\) is defined as

\[_{}(s_{t_{h}},a_{t_{h}},_{t_{h}})=(_ {t_{h}}=_{t_{h}}|s_{t_{h}},a_{t_{h}},_{t_{h}})=( _{t_{h}}=_{t_{h}}|s_{t_{h}},a_{t_{h}})}{1-_{<_{t_{h }}}(_{t_{h}}=|s_{t_{h}},a_{t_{h}})}.\]

The factored form of \(_{}(s_{t_{h}},a_{t_{h}},_{t_{h}})p_{t_{h}}(s_{t_{h+1 }}|s_{t_{h}},a_{t_{h}})\) follows from the conditional independence in Assumption 2.1. We define \(Q\)-functions and value functions as follows. For any \(_{h},a_{h}\) and policy \(_{}\), we have

\[Q^{}_{h,}(_{h},a_{h}) =^{}[_{l^{}=h}^{H}r_{h,}( _{h^{}},a_{h^{}})_{h},a_{h}]\] \[V^{}_{h,}(_{h}) = Q^{}_{h,}(_{h},),_{h}( |_{h}).\]

We note that \(V^{}_{h}\) is equivalent to \(V^{}_{h,}\) for the same executable policy \(_{}\). We also denote \(_{h,}\) as the transition operator corresponding to \(P_{}\). It can be checked that

\[Q^{}_{h,}(_{h},a_{h})=r_{h,}(_{h},a_{h})+[ _{h,}V^{}_{h,}](_{h},a_{h}).\]

\(_{}\) also appears in makes all the policies in \(_{}\) executable and Markov. Meanwhile, the reward function keeps track of all the expected reward for \(H\) steps. Although the expanded state space \(_{}\) is much more complicated than the original state space \(\), the sparse structures in the transition probabilities still allow an efficient exploration. We note that \(p_{h,}\) only depends on the delay distribution and one-step Markov transitions. However, there is still one caveat for learning in \(_{}\) - the reward function depends belief distributions, which involve multi-step transitions.

### Augmented MDPs with past reward

To tackle the aforementioned challenge, we further define \(}_{}=(}_{ },,,_{},_{ })\) that shares the optimal policy in \(_{}\) with an enlonged horizon \(=2H\). The state space \(}_{}\) consists of any \(_{h}=\{s_{t_{h}},_{t_{h}:h H},_{t_{h}}\}\). Comparing to \(_{}\), we cut off the action at horizon \(H\), since \(a_{h}\) for \(h>H\) has no influence on the state and reward in time \([1,H]\). The reward function is defined as

\[_{h,}(_{h},a_{h})=r_{t_{h}}(s_{t_{h}},a_{t_{h}}) \{_{t_{h}}=0\}\{t_{h}\{1,,H\}\}.\]By definition, \(_{}(_{h},a_{h})\) is a past reward. More importantly, \(_{h,}(_{h},a_{h})\) zeros out rewards outside the original horizon \(H\). Meanwhile, between the arrival of two consecutive state observations, the reward only counts once. Lastly, the transition probabilities are

  \(_{h,}(_{h+1}|_{h},a_{h})\) & Condition \\  \(_{a}(_{h},_{h+1})_{}(s_{t_{h}},a_{t_{h}}, _{t_{h}})p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a_{t_{h}})\) & if \(_{t_{h+1}}=0,t_{h+1}=t_{h}+1\) and \(h<H\) \\ \(_{a}(_{h},_{h+1})(1-_{}(s_{t_{h}},a_{t_{ h}},_{t_{h}}))\) & if \(_{t_{h+1}}=_{t_{h}}+1,t_{h+1}=t_{h}\) and \(h<H\) \\ \(_{a}(_{h},_{h+1})p_{t_{h}}(s_{t_{h}+1}|s_{t_{h}},a_{t_{h}})\) & if \(_{t_{h+1}}=0,t_{h+1}=t_{h}+1\) and \(h>H\) \\  & otherwise \\  

We interpret the transitions as follows. When \(h H\), the transition is the same as \(_{}\). When \(h>H\), we simply wait for unobserved states and rewards to come. As mentioned, actions taken beyond time \(H\) are irrelevant. We build an equivalence in the expected values of \(_{}\) and \(}_{}\).

**Proposition 3.1**.: Let \(_{}\) and \(}_{}\) be defined as in the previous paragraphs. Then for any initial state \(_{1}\) and any policy \(=\{_{h}\}_{h=1}^{H}_{}\), it holds that

\[^{}[_{h=1}^{H}r_{h,}(_{h},a_{h}) _{1}]=^{}[_{h=1}^{}_{h, }(_{h},a_{h})_{1}],\]

where in the right-hand side, the policy for steps \(H+1\) to \(\) is arbitrary.

The proof is provided in Appendix A.1. Proposition 3.1 implies that learning in \(_{}\) until time \(H\) is equivalent to that in \(}_{}\) for \(\) steps.

## 4 RL with delayed observations and regret bound

In this section, we provide regret analysis of learning in MDPs with stochastic delays. For the sake of simplicity, we assume the reward is known, however, extension to unknown reward causes no real difficulty. Motivated by the augmented MDP reformulation, we introduce our learning algorithm in Algorithm 2. In Line 5, unobserved states and rewards are returned to the agent as described in Protocol 1. Using the data set, we construct bonus functions compensating the uncertainty in _one-step_ transitions of the original MDP. This largely sharpens the confidence region, yet still ensures a valid optimism. We emphasize that in Line 9, we are planning on \(}_{}\) involving the augmented transitions and expanded states of \(}_{}\). Only in this way, we can obtain an executable policy in delayed MDPs. The planning complexity is \(SA^{H}\) though.

### Regret bound

We define regret in delayed MDP as

\[(K)=_{k=1}^{K}_{_{}}V_{1}^{}( s_{1}^{k})-_{k=1}^{K}V_{1}^{_{k}}(s_{1}^{k}),\]

where \(V_{1}^{}\) is the value function of the original MDP. Although the regret here is defined on the original MDP, it is equivalent to the regret of the same policy on \(_{}\) and further \(}_{}\) by Proposition 3.1. Note that we are comparing with the best executable policy. The performance degradation caused by observation delay is discussed in Section 4.2. The following theorem bounds the regret.

**Theorem 4.1** (Regret bound for Delayed MDP).: Suppose Assumption 2.1 holds. Let \((0,1)\) be any failure probability. With probaiblity \(1-\), the regret of Algorithm 2 satisfies

\[(K) c(H^{4}+H^{4}S^{2}At^{2}),\]

where \(=\) and \(c\) is a constant.

The proof is provided in Appendix B.1. We discuss several implications.

Sharp dependence on \(S\) and \(A\)Theorem 4.1 has a sharp dependence on \(S\) and \(A\), although the expanded state space \(}_{}\) has a cardinality bounded by \(SA^{H}\). Naively learning and planning in \(}_{}\) would suffer from the exponential enlargement of \(A^{H}\). However, we identify the sparse structures in the transition probabilities. As can be seen, \(_{h,}\) only involves one-step transitions in the original MDP and some conditionally independent delay distributions. Such structures lead to a rather easy estimation of \(_{h,}\), which can be constructed from the estimators of one-step transitions in the original MDP. Meanwhile, the sparse structures make exploration in \(}_{}\) efficient, due to many unreachable states.

Effect of the delay distribution and delay lengthTheorem 4.1 holds for arbitrary conditionally independent delay distributions, even include heavy-tailed distributions. In the worst case of unbounded delays, Theorem 4.1 gives rise to a \((H^{4})\) regret. The reason to this is that if the delay is larger than \(H\), then the corresponding state will only be observed after an episode ends and won't be used in planning. Therefore, we can truncate the delay at \(H\), regardless of its tail distributions.

When the maximal length of delay is bounded by \(D<H\), e.g., CDMDPs with \(d_{h}=D\) for any \(h\), Theorem 4.1 implies that the regret is bounded by

\[(K) c((D+1)^{5/2}SAK}+H^{4}S^{2}A ^{2})\]

for a constant \(c\). A proof is provided in Appendix B.2. As can be seen, as the length of delay increases, the regret bound enlarges, reflecting the increased difficulty of long delays. Moreover, when \(D=0\), that is, no observation delays, the regret bound recovers that in standard MDPs.

### Performance degradation of policy class \(_{}\)

This section devotes to quantify the performance degradation caused by delayed observations. In particular, we bound the value difference between the best executable policy and the best Markov policy in a no delay environment. Recall that \(V_{1}\) is the value function of the original MDP. We denote

\[^{*}_{}=*{argmax}_{}V_{1}^{}(s_{1}) ^{*}_{}=*{argmax}_{ _{}}V_{1}^{}(s_{1})\]as the best vanilla optimal policy and executable policy, respectively. The values achieved by \(^{*}_{}\) and \(^{*}_{}\) are denoted as \(V^{*}_{1,}(s_{1})\) and \(V^{*}_{1,}(s_{1})\), respectively. The gap between \(V^{*}_{1,}\) and \(V^{*}_{1,}\) quantifies the performance degradation, which is denoted as \((s_{1})=V^{*}_{1,}(s_{1})-V^{*}_{1,}( s_{1})\). We bound \(\) in Proposition B.2 in Appendix due to space limit.

In a nutshell, we show that the performance degradation \(\) is highly relevant to the belief distribution \(_{h}(|)\). When \(_{h}(|)\) is evenly spread, meaning that the entropy of \(_{h}\) is high and inferring the current unseen state is difficult, we potentially suffer from a large \(\). On the contrary, when \(_{h}(|)\) is nearly deterministic, the performance degradation is small. In the special case of deterministic transitions, we have \(=0\).

### The (mysterious) effect of delay on the optimal value

To further understand the effect of the delay on the optimal value, we provide the following dichotomy. On the one hand, we show that there exists an MDP instance, such that a constant delay of \(d\) steps does not hurt the performance. On the other hand, in the same MDP instance, a constant delay of \(d+1\) steps suffers from a constant performance drop.

**Proposition 4.2**.: Consider constant delayed MDPs. Fix a positive integer \(d<H\). Then there exists an MDP instance such that the following two items hold simultaneously.

\(\) When delay is \(d\), it holds that \(_{k=1}^{K}(s_{1}^{k})=0\).

\(\) When delay is \(d+1\), it holds that \(_{k=1}^{K}(s_{1}^{k})-}\), with probability \(1-\).

The proof is provided in Appendix B.4. We remark that Proposition 4.2 says that observation delay can be dangerous, even with the slightest possible number of steps. The idea behind Proposition 4.2 is consistent with the analysis on \(\). In particular, we construct an MDP instance demonstrated in Figure 2. The reward vanishes at all times but \(d+1\). When delay is \(d\), the initial state \(s_{1}\) is revealed and the policy can choose the best action to receive a reward. When delay is \(d+1\), however, there is always a \(1/2\) probability of missing the best action for any policy, which leads to a constant performance degradation.

## 5 RL with missing observations and regret analysis

We now switch our study to MDPs with missing observations. In such an environment, executable policies share the same structures as delayed MDPs, where an action is taken based on available history information. Compared to delayed observations, learning with missing observations is more challenging. Since unobserved states and rewards are never revealed, we are suffering from information loss. Besides, we will frequently deal with multi-step transitions, due to missing observations between two consecutive visible states.

### Optimistic planning with missing observations

Despite the difficulty, we present here algorithms that are efficient in learning and planning for MDPs with missing observations. We begin with an optimistic planning algorithm in Algorithm 3. To unify the notation, we denote \(s_{h}^{k}=\) and \(r_{h}^{k}=\) as missing the observation.

Figure 2: MDP instance on two states with two actions. The transition is lazy until time \(d\). Then the transition is uniform regardless of actions for time \(d+1\). Reward is nonzero only at time \(d+1\). This is an example with a delay of length \(d\) causes no degradation and a delay of \(d+1\) causes a constant performance degradation.

The majority of the algorithm resembles the typical optimistic planning Jaksch et al. (2010) but with some notable differences. In Line 4, the value function \(V_{1,}\) is for the original MDP with transition probabilities parameterized by \(\). Different from the typical optimistic planning, the underlying MDP here obeys the stochastic observable model in Assumption 2.2. Therefore, the value \(V_{1,}\) is the sum of all possible values under missing observations. When counting \(N_{h}^{k}(s,a)\) in Line 6, we exclude data tuples missing the next state, which inevitably slows down the learning curve. Nonetheless, the effect of missing only contributes as a scaling factor in the regret.

**Proposition 5.1**.: Suppose Assumption 2.2 holds with \(_{h}\) known. Given a failure probability \(\), with probability \(1-\), the regret of Algorithm 4 satisfies

\[(K) c(^{2})} S^{2}AK^{3}}+K}),\]

where \(=\) and \(c\) is a constant.

The proof is provided in Appendix C.1. Proposition 5.1 is optimal in the \(K\) dependence and achieves an \(S^{2}A\) dependence on the complexity of the underlying MDP. In the extreme case of \(_{0} 0\), which implies that every state and reward are hardly observable, we have \((K)=}(^{2}} S^{2}AK})\). Here \(_{0}^{2}\) is the probability of observing two consecutive states for estimating the transition probabilities. Proposition 5.1 requires knowledge of observable rate \(_{h}\). This is not a restrictive condition, as estimating \(_{h}\) from Bernoulli random variables is much easier than estimating transition probabilities.

### Model-based planning using augmented MDPs

Proposition 5.1 has a lenient dependence on the missing rate \(1-_{0}^{2}\), nonetheless, is not sharp on the dependence of \(S\). We next show that the augmented MDP approach is effective to tackle missing observations, when the observable rate satisfies additional conditions. Specifically, we assume that the observable rate \(_{h}\) is independent of \((s,a)\). We utilize the \(_{}\) reformulation, except that we redefine the transition probabilities as

\[p_{h,}(_{h+1}|_{h},a_{h})=_{h}p_{h}(s_{ h+1}|s_{t_{h}},_{t_{h}:h})&t_{h+1}=h+1\\ _{}(_{h+1},_{h})(1-_{h})&t_{h+1}=t_{h}\\ 0&.\]

The first case in \(p_{h,}\) corresponds to receiving the state observation at time \(h+1\). In contrast to the delayed MDPs, the transition probabilities here potentially rely on multi-step transitions in the original MDP. The second case of the transition corresponds to missing the observation. We summarize the policy learning procedure in Algorithm 4 in Appendix C.2, which is similar to Algorithm 2, but with a new bonus function. The following theorem shows that Algorithm 4 is asymptotically efficient when the observable rate is relatively high.

**Theorem 5.2**.: Suppose Assumption 2.2 holds with \(_{0} 1-A^{-(1+v)}\) for some positive constant \(v\). Given a failure probability \(\), with probability \(1-\), the regret of Algorithm 4 satisfies

\[(K) c(H^{4}}+S^{2}K^{ {1}{(1+v)}}}^{6}),\]

where \(=\) and \(c\) is a constant.

The proof is provided in Appendix C.2. Some remarks are in order.

\(Sa\) **rate when \(K\) is large**When the number of episodes \(K S^{3(1+v)/v}\), the first term \(H^{4}}\) in the regret bound dominates and attains a sharp dependence on \(S\) and \(A\). However, when the number of episodes are limited, the regret bound has a worse dependence on the state space size \(S\). We also observe that as the missing rate \(\) becomes small (equivalently, \(v\) becomes large), the regret is close to \((H^{4}})\).

Observable rate smaller than \(1-1/A\)Theorem 5.2 holds for an observable rate \(_{0}>1-1/A\). The intuition behind is that to fully explore all the actions when a state observation is missing takes \(A\) trials. Therefore, in expectation, we will encounter a missing observation at least every \(A\) episodes as long as \(_{0}>1-1/A\). Nonetheless, when \(_{0} 1-1/A\), the regret bound remains curiously underexplored. We conjecture that \(_{0}=1-1/A\) is a critical point distinguishes unique strategies for learning and planning in MDPs with missing observations. A detailed analysis goes beyond the scope of the current paper.

Proof sketchThe proof of Theorem 5.2 adapts the analysis of model-based UCBVI algorithms Azar et al. (2017). Let \(m\) denote the maximal length of consecutive missing observations. We denote \(_{m}\) as the event when the maximal length of consecutive missing is less than \(m\). On event \(_{m}\), a naive analysis leads to a \(}((H)SA^{m+1}K})\) regret, in observation to the size of the expanded state space \(_{}\). However, our analysis circumvents the \(A^{m}\) dependence by exploiting the occurrence of consecutive missing observations is rare (Lemma C.3). On the complement of event, the regret is bounded by \(KH(1-(_{m}))\). Summing up the two parts and choosing a proper \(m\) yield our result.

## 6 Conclusion

In this paper, we have studied learning and planning in impaired observability MDPs. We focus on MDPs with delayed and missing observations. Specifically, for delayed observations, we have shown an efficient \((H^{4})\) regret. For missing observations, we have provided an optimistic planning algorithm achieving an \((S^{2}AK})\) regret. If the missing rate is relatively small, we have shown an efficient \((H^{4})\) regret bound. Further, we have characterized the performance drop caused by impaired observability compared to full observability.