# Image Textualization : An Automatic Framework for Creating Accurate and Detailed Image Descriptions

Renjie Pi\({}^{1}\), Jianshu Zhang\({}^{2}\), Jipeng Zhang\({}^{1}\), Rui Pan\({}^{1}\), Zhekai Chen\({}^{3}\), Tong Zhang\({}^{4}\)

\({}^{1}\)The Hong Kong University of Science and Technology

\({}^{2}\)Wuhan University, \({}^{3}\)Zhejiang University

\({}^{4}\)University of Illinois Urbana-Champaign

{rpi,rpan,jzhanggr}@ust.hk, jianshu.zhang@whu.edu.cn, chenzhekai@zju.edu.cn

tongzhang@tongzhang-ml.org

 Equal Contribution. Code and data are available at the following links:

https://github.com/sterzhang/image-textualization/

https://huggingface.co/datasets/Sterzhang/image-textualization/.

The code and data are released under MIT and apache2.0 licenses, respectively.

###### Abstract

Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed **Image Textualization (IT)**, which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text. To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from fine-tuning on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.

## 1 Introduction

In recent years, multi-modal large language models (MLLMs) have witnessed significant progresses. Such models start to reach super-human performances in a variety of areas, such as image understanding [2; 15; 34; 41; 80], text-to-image generation [13; 54; 55; 57; 58] and text-image retrieval [33; 53; 69; 77]. One of the primary reasons for these successes is the training data, which consists of image-description pairs. Recent studies highlight that the quality of image descriptions is crucial for MLLM performance. For example, Yin et al.  note that low-quality descriptions often cause hallucinations in image understanding tasks, while Betker et al.  show that detailed descriptions with richer visual concepts significantly enhance generation models' performance. Thus,curating high-quality image description datasets is essential for improving various downstream applications.

A high-quality image description should convey the same information as the corresponding image, effectively textualizing the visual content. However, current datasets often fall short. They mainly come from two sources: web-scraped image-text pairs [8; 60; 61], which are large-scale but low-quality and noisy, and human-labeled datasets [31; 39; 52], which lack in-depth details and are costly to produce. Consequently, a significant gap remains between the information in an image and its textual description.

To address the shortcomings of existing image description datasets, recent advancements in MLLMs have shown remarkable potential for generating descriptions. Dalle-3  has made an early attempt to train text-to-image diffusion models using descriptions produced via MLLMs, which shows improved quality of the generated images. However, MLLMs possess several weaknesses, such as the well known visual hallucination problem [51; 74; 75] and lack of fine-grained details . Even the most powerful MLLMs, such as GPT4-V , exhibit these weaknesses. Therefore, relying solely on MLLMs to generate datasets still has significant limitations.

Meanwhile, we notice remarkable progress in areas of computer vision, such as object detection [42; 72; 73], dense captioning [43; 67] and instance segmentation . Compared with MLLMs that are trained with low-resolution images (336x336 by default setting of CLIP ) and image-level annotations, these vision expert models are trained with high-resolution images and fine-grained object-level annotations specifically catering for perception tasks, which makes them capable of identifying detailed content. However, such models generally do not possess holistic understanding capabilities, so constructing descriptions solely depending on such models is not practical. Consequently, an intriguing thought arises: Can we combine the understanding capability of MLLMs with the perception power of vision experts to generate high-quality descriptions that are both rich in details and free from hallucinations.

Building upon the above intuition, in this paper, we propose **Image Textualization(IT)**, a framework for automatically creating high-quality image descriptions. Specifically, our framework consists of three phases: 1) _Holistic Textualization_: We leverage the MLLM to create the Reference Description, which, despite lacking details and containing hallucinations, provides the basic structure not only for the visual information but also for the linguistic expression. 2) _Visual Detail Texturalization_: Then, we resort to the powerful perception capabilities of vision expert models to extract fine-grained object-level information that is converted into text format. This phase extracts multiple details from the image-side and identifies hallucinations contained in the Reference Description. 3) _Textualized Recaptioning_. Finally, we leverage LLMs' superior understanding and reasoning capabilities to

Figure 1: Visualization of our Image Textualization. Compared with the MLLM-generated description, our description incorporates more visual details and significantly less hallucinations. The shared details, newly added details, hallucinations, and positional descriptions are all marked with different colors.

produce accurate and detailed descriptions based on the textualized information from the first two phases, allowing the LLMs to describe the image without "seeing" it. This approach avoids the weaknesses of MLLM-based recaptioning. As shown in Figure 1, our IT framework is able to create image descriptions that are richer in details and free from hallucination.

For a comprehensive evaluation of our framework, we first construct three benchmarks, namely DID-Bench, D2I-Bench and LIN-Bench, which evaluate the description quality from multiple aspects. Then, we conduct a series of experiments to validate the quality of IT-generated descriptions on the proposed benchmarks. Afterward, we verify that fine-tuning MLLMs with our generated data enhances their capabilities. Lastly, we perform linguistic evaluations and provide statistical analysis of our released dataset.

To summarize, we make the following contributions in this paper:

* We propose **Image Textualization**, a framework that automatically generates detailed image descriptions without human intervention, leveraging the multimodal understanding of MLLMs, the fine-grained perception of visual experts, and the reasoning power of LLMs.
* We create evaluation benchmarks and conduct extensive experiments to validate the effectiveness of our framework. The results demonstrate that the generated image descriptions accurately capture rich visual details.
* Using our Image Textualization framework, we curate a large-scale high-quality image description dataset termed **IT-170K**. To facilitate future research, we release all the source code and our generated dataset to the community.

## 2 Related Work

Image Description DatasetsImage-text paired description datasets are valuable assets for a variety of downstream tasks, such as image understanding [2; 15; 41; 66; 80], text-to-image generation [13;

Figure 2: The framework of **Image Textualization (IT)**, which consists of three phases: (A) **Holistic Textualization** (Sec. 3.1) utilizes a MLLM to generate a “_Reference Description_” that provides a basic structure; (B) **Visual Detail Textualization** (Sec. 3.2) identifies the hallucinations and captures details in the image via a variety of vision experts, then transforms them to text format. (C) **Textualized Recaptioning** (Sec. 3.3), which leverages LLM and textualized results from (A) and (B) to re-generate the image captions that are both rich in details and free from hallucination.

54, 55, 57, 58] and text-image retrieval . Image description datasets primarily curated from three sources: 1) web-scraped image-text pairs, such as CC3M , CC12M  and LAION , which are large-scale, but often contain low quality and noisy descriptions. 2) human-labeled description datasets, such as COCO , Flickr30k  and Visual Genome , which typically have limited quantity and often feature short and incomplete captions due to the costly annotation process. The lack of high-quality image description datasets may cause unsatisfactory performance or out of domain problem . To address this gap, in this paper, we propose a framework that automatically generates high-quality, detailed image descriptions without human intervention.

Multi-Modal Large Language Model.In recent years, great advancements have been made in the development of large language models (LLMs) . These advancements have greatly elevated the capabilities of language understanding and generation, showcasing superhuman proficiency across diverse tasks. Concurrently, the success of LLMs has inspired explorations into the incorporation of visual modality into LLM, leading to the emergence of multi-modal large language models (MLLMs) . These models have demonstrated remarkable abilities in engaging in dialogue based on visual inputs and generating image descriptions containing rich details. Despite the success of MLLMs, their inherent weaknesses such as low image resolution and insufficient training data, leads to problems such as incomplete description and object hallucination . Therefore, directly generating image descriptions with MLLMs remains problematic.

Vision Expert ModelsThere is a variety of sub-fields in computer vision that specialize on different tasks. Object detection aims at localizing objects in images . Recently, open-vocabulary object detection has achieved great progress at localizing objects based on the semantics of text queries . Dense captioning aims to produce short descriptions for each object present in the input images . Prompt-based segmentation models enable producing segmentation mask for objects in the image based on the input prompts, which could be in the form of point or bounding box . Depth estimation enables the prediction of distance between objects and the camera . In this paper, we harness the capabilities of the vision experts to provide object-level information for constructing high-quality image descriptions.

## 3 Method

Image Textualization automatically produces high-quality image descriptions. Given an image, the powerful MLLM first produces a template description capturing the holistic image content. Then, a variety of vision expert models collaborate to extract detailed object information, which may be missing from the template description. Finally, we harness the powerful LLMs to re-generate the description based on the holistic information and fine-grained details. As shown in figure 2, our framework is divided into three phases, which we will elaborate in the following sections.

### Phase 1: Holistic Textualization

Current state-of-the-art MLLMs  excel at producing image descriptions that contain richer information and contextual understanding compared with those generated by conventional captioning models . Therefore, we first leverage MLLM-generated descriptions to textualize the holistic content of the image, as shown in Figure 2 Phase(A). Despite weaknesses such as hallucinations and lack of details, this description can serve as a basic template with a relatively good structure for describing the image. Hereafter, we refer to this description as the _"Reference Description"_.

This Reference Description serves two key purposes. Firstly, in terms of visual information, it includes the main objects present in the image and the contextual information of the scene. These elements act as "anchors" that guide the incorporation of more details in the subsequent phases. Secondly, from a linguistic expression perspective, the inherent understanding and logical capabilities of MLLMs help to form well-organized descriptions. For example, a Reference Description typically includes an overall description of the image, followed by details about the main objects, then concludes with a summarizing sentence. Compared to traditional captioning models, this kind of descriptions are more logically structured and naturally expressed, which are crucial factors for the quality of descriptions.

### Phase 2: Visual Detail Textualization

Reference descriptions generated in the first phase generally lack in visual details and contain hallucinations. In this phase, we utilize vision expert models to extract information from both the image and reference description. From the image, we capture more visual details, and from the reference description, we identify the hallucinated contents. Finally, we textualize the fine-grained visual information and hallucinated objects, as shown in Figure 2 Phase(B) rightmost grey box.

#### 3.2.1 Hallucination Detection

As outlined in Algorithm 1, to identify hallucinations existed in the reference description, we first extract object entities (object nouns and phrases) from it. Here, we leverage the strong instruction following ability of the Large Language Model (LLM). Specifically, we carefully design an entity-extraction prompt and manually annotate in-context learning examples to improve instruction following of the LLM. Afterward, we utilize an open-set object detector (e.g., Grounding Dino ) to verify each of these extracted entity phrases against objects in the image. Any hallucinated object phrases, which are not found in the image, are tagged as "Hallucination" for removal in the later phase.

#### 3.2.2 Fine-Grained Objects Annotation

Dense Caption Generation.To identify objects that are potentially missing in the original description, we resort to a dense captioner (DC) , which not only provides accurate bounding boxes indicating object locations, but also associate them with basic attributes, such as object type, shape and color. Compared with object detectors that only predict the object category (e.g., cat), DC provides a more detailed description such as "a grey and white cat"; compared with conventional captioning models that only predicts image-level captions, DC is able to predict descriptions for all the visible objects in the image. These appealing properties make DC a suitable choice for maximizing the textualization of objects' information, which is beneficial for the subsequent recaptioning phase.

```
0: An input image \(\) with size \(H W\), dense caption model \(DC\), a segment anything model SAM, a monocular depth estimator parameterized by \(()\).
1: Initialize FinegrainedInfo as Empty
2:\(,()\)# get object boxes and phrases
3:\((,)\)# obtain object masks
4:\(()\)# obtain image depth map
5:for each object mask and phrase \(m_{i},p_{i}[,]\)do
6:\(d_{i})}{m_{i}}\)# obtain object depth
7:\(s_{i}}{H W}\)# obtain object size
8: Append \(\{p_{i},d_{i},s_{i}\}\) to FinegrainedInfo
9:endfor
10:Output: FinegrainedInfo ```

**Algorithm 2** Fine-grained Object Annotation

Spatial Information Collection.Now, we have obtained the dense captions of various objects in the image along with their bounding box coordinates. However, the textualized information still falls short compared to the original image's information. The most critical reasons for this is that the current textualization can only convey the relative left-right relationships of objects on a 2D plane, and can lead to mistakes in recaptioning. For example, consider an image with a car in the background and a person in the foreground. Their bounding boxes might have very close coordinates. If we move to the Recaptioning phase with just this information, the LLM might use its logical capabilities to inaccurately describe the scene as "a person standing next to a car". This happens because the current textualization fails to capture the 3D spatial context, such as depth (which indicates the front-back relationships of objects), which are crucial for an accurate and comprehensive image description.

As shown in Algorithm 2, we derive this depth information by first obtaining a distance map using a monocular depth prediction model, where the value for each pixel indicates its distance from the camera. Then, we generate object segmentation masks with SAM  using the object bounding boxes generated by the dense captioner, which gives the exact pixels of the objects. Finally, the object depth is obtained by averaging the depth values within its corresponding segmentation mask.

In this way, the textualized information can convey the distance between the object and the camera, effectively transforming the 2D dense captions into 3D ones.

In this process, we remark the two important factors: 1) **Utilizing Pixel Masks for Size Calculation**: One naive way to calculate object size is using the bounding boxes. However, bounding boxes only provide two opposite corners, which can be inaccurate for irregularly shaped objects (e.g., a long stick). If we solely rely on bounding box's coverage as the object's size, it can lead to severe overestimation. Hence, it is crucial to use pixel-wise masks to accurately represent the object's size. 2) **Normalization**: We normalize the bounding box coordinates and depth scores to ensure that the values are relative, which facilitates the LLMs to adapt to different images during recaptioning phase.

### Phase 3: Textualized Recaptioning.

In this phase, we utilize the comprehensive information gathered from the previous phases that has been transformed into a textual representation, to reconstruct the image description via an LLM. The inclusion of both a holistic description and extensive object-level information enables the LLM to accurately interpret the entire image and its constituent objects, eliminating the need for direct visual input. We demonstrate the effectiveness of our approach in the appendix, where we carefully design prompts and provide few-shot examples to guide the LLM's generation process. This ensures that the LLM can effectively incorporate novel objects while minimizing the presence of hallucinated content.

## 4 Experiments

OverviewDue to lack of standard evaluation benchmarks for long image descriptions, we first propose DID-Bench, D2I-Bench and LIN-Bench for comprehensively evaluating detailed descriptions(Sec. 4.1). Then, we conduct a series of experiments to validate the effectiveness of our Image Textualization that can generate high-quality descriptions(Sec. 4.2). Afterward, we verify that training MLLMs with the data generated by our framework enhances their capabilities (Sec. 4.3). Lastly, we perform linguistic evaluations and provide statistical analysis of our released dataset (Sec. 4.4).

### Benchmarks and Evaluations

DID-BenchDetailed Image Description Bench (_DID-Bench_) contains 200 samples via the following steps: 1) First, we utilize MLLMs to generate Reference Descriptions. To avoid the bias introduced by different MLLMs' output habits, we employ GPT4-V for 100 samples and LLaVA for the remaining 100 samples when generating the Reference Descriptions. 2) Then, we manually check the correctness of these descriptions, add missing details, and remove hallucinated content to establish the human-labeled ground truth descriptions. We later refer to the partition using LLaVA Reference Descriptions to get ground truth as GT-[LLaVA], and the other partition as GT-{GPT4-V}.

We adopt reference-based metrics for image descriptions including BLEU , ROUGE-L , METEOR , SPICE  and WMD . These metrics evaluate various aspects of the generated descriptions, such as n-gram overlap, recall, precision, semantic content, and overall similarity to human-labeled descriptions. The details of these metrics are introduced in the appendix.

D2I-BenchWe propose Description-to-Image-Bench (D2I-Bench) to evaluate the completeness of image information captured by descriptions. Firstly, we feed the descriptions to a pre-trained text-to-image model (e.g., PixArt ) and obtain the generated images. Then, we extract the image

   GroundTruth & Description & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & CIDEr & METEOR & ROUGE & SPICE & WMD \\   & [LLaVA] & 12.90 & 8.64 & 5.80 & 4.09 & 0.00 & 12.84 & 22.69 & 23.08 & 43.82 \\  & \(^{}\)-[LLaVA] & **25.71** & **17.53** & **12.06** & **8.08** & **2.34** & **17.09** & **26.10** & **26.10** & **46.49** \\   & (GPT4-V) & 29.05 & 15.23 & 7.64 & 4.15 & 1.93 & 15.92 & 20.06 & 19.84 & 42.79 \\  & \(^{}\)-[GPT4-V] & **36.20** & **19.97** & **10.75** & **6.23** & **7.64** & **18.56** & **21.34** & **22.35** & **43.81** \\   & [LLaVA] & 9.80 & 5.16 & 2.54 & 1.35 & 0.00 & 9.83 & 15.93 & 13.75 & 37.93 \\  & \(^{}\)-[LLaVA] & **21.86** & **12.17** & **6.67** & **3.94** & **1.18** & **13.80** & **18.74** & **17.86** & **40.12** \\    & (GPT4-V) & 45.26 & 38.77 & 34.42 & 31.18 & 6.08 & 26.63 & 50.85 & 52.21 & 58.52 \\    & \(^{}\)-[GPT4-V] & **57.38** & **48.73** & **43.02** & **38.89** & **36.67** & **30.89** & **54.36** & **55.20** & **61.23** \\   

Table 1: Evaluation of image descriptions on _DID-Bench_. Descriptions generated by our IT outperform the ones generated by MLLMs by a significant margin across different metrics.

embeddings for both the original image and the generated image using the image encoder from a pre-trained CLIP . Finally, we calculate the cosine similarities between the image embeddings. A higher similarity score indicates that the description effectively captures the image details, resulting in generated images that closely resemble the originals.

LIN-BenchTo fully evaluate the readability and linguistic features of descriptions, we propose Linguistic Bench (LIN-Bench), which adopt metrics such as ARI, FK, and SMOG. ARI tends to be higher if there are more words with many characters in the sentence, while FK and SMOG place more emphasis on the number of multi-syllable words. More detailed descriptions tend to achieve higher values for these metrics.

PopePOPE benchmark  evaluates the level of hallucination suffered by the MLLM, which comprises of questions regarding the existence of objects in the image, and associated with short answers such as "yes" or "no".

### Image Description Quality Evaluation

DID-Bench ResultsTo verify the effectiveness of our Image Textualization annotation framework for generating detailed and accurate image descriptions, we compare the quality of descriptions generated by our Image Textualization with the ones directly produced by the MLLMs. As shown in Table 1, we observe significant gain across all the metrics for different MLLMs and ground-truth annotations. Interestingly, we observe that the evaluation results of IT-generated descriptions also dependent on the MLLM used in the holistic textualization phase. This is because the evaluation metrics not only account for the visual correctness, but also the styles of the descriptions, such as pronouns and prepositions. Therefore, to exclude the impact of language bias, it is crucial to conduct evaluation using GT annotations with different styles.

D2I-Bench ResultsAs shown in table 4, the descriptions generated by our IT framework results in images that have higher similarity scores with the original images than COCO's descriptions and MLLM-generated descriptions. In figure 3, we provide qualitative examples to show the IT-generated

Figure 3: D2I-Bench visualization. IT-generated descriptions capture more fine-grained image details, which leads to generated images more similar to the original images.

descriptions leads to images that bear closer resemblance as the originals. These results demonstrate the effectiveness of our framework for accurately capturing the details of the image content.

### MLLM Tuning Evaluation

DID-Bench ResultsIn Table 3, we compare the quality of image descriptions generated by 1) the original LLaVA-7B model; 2) LLaVA fine-tuned with MLLM-generated descriptions and 3) LLaVA fine-tuned with descriptions produced by Image Textualization. We observe that the MLLM fine-tuned using IT-curated dataset consistently outperforms other baselines across all metrics and GT annotations. We observe the following phenomena: 1) the scores on GT-LLaVA is mostly higher than that of GT-GPT4V, which is response style of LLaVA; 2) For each GT split, IT-tuned LLaVA outperforms the baseline and the MLLM-tuned LLaVA by a large margin; 3) from the evaluation on combined GT, we observe IT-LLaVA's effectiveness approaches that of GPT4-V, while IT-GPT4-V still surpass all counterparts significantly. This indicates that Image Textualization has the potential to close the gap between the capability of different MLLMs.

POPE and LIN-Bench ResultsOn the left side of Table 2, we evaluate the hallucination of MLLMs tuned with different image description data. We observe that tuning with IT-generated descriptions leads to the most significant alleviation of hallucination. On the right side, we also show the results on LIN-Bench, which demonstrates that tuning with IT-generated descriptions results in most gain in producing descriptions containing richer details.

### Linguistic-based Evaluation

Statistical AnalysisWe summarize the statistics of image descriptions generated by Image Textualization and MLLM baselines, respectively. In Table 5, we show that the IT-generated descriptions contain more words and sentences. In the figure on the right, we show the counts for different types of words, which demonstrates that the IT-generated descriptions contain richer words such as nouns, verbs and adjectives, indicating these descriptions contain denser information.

LIN-Bench ResultsAs demonstrated in Table 6. Compared with MLLM-generated descriptions, IT-generated descriptions results in higher scores across all metrics, suggesting that these descriptions contain richer details.

## 5 Prompt Design

Prompt design plays a crucial role in our proposed framework, which enables the collaboration between different expert models and results in more accurate and detailed image descriptions. In figure 4, we compare the recaption results without fine-grained object annotation and in-context examples and observe that object annotation leads to more detailed and accurate description, while in-context

    &  &  &  \\  & & Adv & Rand & Popular & Average & ARI & FK & SMOG & Average \\  / & - & 79.13 & 85.70 & 88.93 & 84.59 & 8.80 & 8.48 & 10.93 & 9.40 \\  \{LLaVA\} &  & 79.60 & 86.16 & 89.56 & 85.11 & 8.77 & 8.45 & 10.91 & 9.38 \\ IT-\{LLaVA\} & & **81.37** & **87.40** & **90.63** & **86.47** & **9.99** & **9.48** & **11.3** & **10.26** \\  \{GPT4-V\} &  & 83.46 & **88.03** & 90.23 & 87.24 & 8.78 & 8.53 & 11.14 & 9.51 \\ IT-\{GPT4-V\} & & **83.60** & 88.00 & **90.47** & **87.36** & **10.03** & **10.89** & **11.89** & **10.94** \\  \{GPT4-V\} &  & 81.96 & 88.03 & 90.13 & 86.71 & 9.57 & 8.89 & 11.08 & 9.85 \\ IT-\{GPT4-V\} & & **83.30** & **88.20** & **90.80** & **87.43** & **10.47** & **9.65** & **11.62** & **10.58** \\   

Table 2: Evaluation on POPE and Lin-bench. LLaVA trained with IT-generated data produces richer image descriptions and demonstrates alleviated hallucination.

[MISSING_PAGE_EMPTY:9]

Limitation

While our framework demonstrates promising robustness against hallucinations and enhances downstream performance, several limitations warrant consideration. Firstly, despite claims of mitigating misinformation, some provided examples exhibit inaccuracies--for instance due to the performance bottleneck of vision expert models.

Additionally, our approach relies exclusively on model-based techniques, which introduces inherent limitations related to model diversity. Specifically, models like GPT-4V exhibit a strong positivity bias, often describing images in overly favorable terms such as "cozy" and "beautiful." This lack of diversity can result in biased data, as the training and evaluation processes predominantly reflect these inherent model tendencies.

Furthermore, the potential misuse of image description generation technologies poses significant risks, including the creation of false news or misleading information. Our current work does not address these ethical concerns, nor does it explore the implications related to privacy and bias in depth. These issues are critical, given the powerful capabilities of image description models and their impact on information dissemination. Incorporation with advanced data selection approaches is promising [5; 19; 46; 76; 79].

Lastly, while we acknowledge the limitations associated with using smaller LLaVa models, we have not thoroughly examined the broader societal impacts of our work. Moreover, our reliance on evaluation metrics such as BLEU and ROUGE, which are known to be noisy and sometimes unreliable, underscores the need for more robust assessment methods. These metrics may not fully capture the qualitative aspects of image descriptions, potentially affecting the validity of our evaluations.

In summary, future work should address these limitations by incorporating diverse model architectures, mitigating biases, exploring ethical implications, and employing more reliable evaluation metrics to enhance the robustness and societal relevance of our framework.

## 7 Conclusion

In conclusion, this paper addresses the limitations of existing image description datasets and proposes an innovative framework, Image Textualization (IT), to generate detailed and accurate image descriptions. The framework leverages the power of multimodal large language models (MLLMs) and multiple vision expert models in a collaborative manner. Through extensive experiments for image understanding and generation tasks, we validate the high quality of the descriptions generated by the framework. We hope our work provides inspiration for the design of more efficient and scalable methods to generate detailed and accurate image descriptions.