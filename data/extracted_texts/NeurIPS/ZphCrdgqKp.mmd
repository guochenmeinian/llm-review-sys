# Channel Vision Transformers:

An Image Is Worth \(C 16 16\) Words

 Yujia Bao, Srinivasan Sivanandan, Theofanis Karaletsos

Instito

{yujia,srinivasan,theofanis}@insitro.com

Equal contribution.

###### Abstract

Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate the performance of ChannelViT on microscopy cell imaging (main paper) as well as satellite imaging and pathology imaging (appendix). Our results show that ChannelViT outperforms ViT on classification tasks and generalizes well, even when a subset of input channels is used during testing. Across our experiments, HCS proves to be a powerful regularizer, independent of the architecture employed, suggesting itself as a straightforward technique for robust ViT training. Lastly, we find that ChannelViT generalizes effectively even when there is limited access to all channels during training, highlighting its potential for multi-channel imaging under real-world conditions with sparse sensors. Our code is available at https://github.com/insitro/ChannelViT.

## 1 Introduction

Vision Transformers (ViT) have emerged as a crucial architecture in contemporary computer vision, significantly enhancing image analysis. However, application to specific imaging domains, such as microscopy and satellite imaging, poses unique challenges. Images in these fields often comprise multiple channels, each carrying semantically distinct and independent information. The complexity is further compounded by the fact that these input channels may not always be densely available during training or testing, necessitating a model capable of handling such sparsity.

In response to these challenges, we propose a modification to the ViT architecture that bolsters reasoning across the input channels. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and incorporates a learnable channel embedding that is added to the patch tokens, akin to positional embeddings. This simple modification enables the model to reason across both locations and channels. Furthermore, by treating the channel dimension as the patch sequence dimension, ChannelViT can seamlessly handle inputs with varying sets of channels.

Despite these advancements, two main challenges persist. While ChannelViT can leverage existing efficient implementations of ViT with minimal modifications, the increase in sequence length introduces additional computational requirements. Moreover, if ChannelViT is consistently trained on the same set of channels, its ability to generalize to unseen channel combinations at test time may be compromised. To address these challenges, we introduce Hierarchical Channel Sampling (HCS), a new regularization technique designed to improve robustness. Unlike channel dropout, which drops out each input channel independently, HCS uses a two-step sampling procedure. It first samples the number of channels and then, based on this, it samples the specific channel configurations. While channel dropout tends to allocate more distribution to combinations with a specific number of channels, HCS assigns a uniform weight to the selection of any number of channels. HCS consistently improves robustness when different channels are utilized during testing in both ViT and ChannelViT. Notably, our evaluation on ImageNet shows that using only the red channel, HCS can increase the validation accuracy from 29.39 to 68.86.

We further evaluate ChannelViT on two real world multi-channel imaging applications: microscopy cell imaging (JUMP-CP) and satellite imaging (So2Sat). In these applications, different channels often correspond to independent information sources. ChannelViT significantly outperforms its ViT counterpart in these datasets, underscoring the importance of reasoning across different channels. Moreover, by treating different channels as distinct input tokens, we demonstrate that ChannelViT can effectively generalize even when there is limited access to all channels in the dataset during training. Lastly, we show that ChannelViT enables additional insights. The learned channel embeddings correspond to meaningful interpretations, and the attention visualization highlights relevant features across spatial and spectral resolution, enhancing interpretability. This highlights the potential of ChannelViT for wide-ranging applications in the field of multi-channel imaging.

## 2 Results

We evaluate ChannelViT across four image classification benchmarks: ImageNet Deng et al. (2009), JUMP-CP Chandrasekaran et al. (2022), Camelyon17-WILDS Koh et al. (2021) and So2Sat Zhu et al. (2019). Due to space limits, we only present the results on JUMP-CP in the main paper and leave the additional results and model details to the Appendix.

JUMP-CP is a microscopy imaging benchmark released by the JUMP-Cell Painting Consortium. The objective is to predict the applied perturbation based on the cell image. The dataset includes a total of 160 perturbations. We focused on a compound perturbation plate 'BR00116991', which contains 127k training images, 45k validation images, and 45k testing images. Each cell image contains 8 channels, comprising both fluorescence information (first five channels) and brightfield information (last three channels).

Figure 1: Illustration of Channel Vision Transformer (ChannelViT). The input for ChannelViT is a cell image from JUMP-CP, which comprises five fluorescence channels (colored differently) and three brightfield channels (colored in B&W). ChannelViT generates patch tokens for each individual channel, utilizing a learnable channel embedding chn to preserve channel-specific information. The positional embeddings pos and the linear projection \(W\) are shared across all channels.

Table 1 shows our result on the 160-way perturbed gene classification task on JUMP-CP. We utilize ViT-S as our representation backbone, and we consider both the standard resolution with a patch size of 16x16 and a high-resolution model with a patch size of 8x8.

In the first part of our analysis, we train all models using only the five fluorescence channels and evaluate them on the test set under various channel combinations. Our observations are as follows: 1) HCS significantly enhances the channel robustness for both ViT and ChannelViT; 2) High-resolution models consistently outperform their low-resolution counterparts; 3) With the exception of the 5-channel evaluation with a patch size of 8x8, ChannelViT consistently outperforms ViT.

In the latter part of our analysis, we utilize all available channels for training, which includes three additional brightfield channels for each image. For ViT, the high-resolution ViT-S/8 model improves from 60.29 to 66.44, demonstrating the importance of the additional brightfield information, while the improvement for ViT-S/16 is marginal (from 55.51 to 56.87). When focusing on ChannelViT, we observe a significant performance boost over its ViT counterpart. ChannelViT-S/16 outperforms ViT-S/16 by 11.22 (68.09 vs 56.87) and ChannelViT-S/8 outperforms ViT-S/8 by 8.33 (74.77 vs.

    &  & ChannelViT-S/16 & ViT-S/16 & ChannelViT-S/16 & ViT-S/8 & ChannelViT-S/8 \\   &  & }\)} & }\)} & }\)} & }\)} & }\)} & }\)} \\  &  & & & & & & & \\   & & & & & & \\  & 48.41 & 53.41 & 55.51 & 56.78 & **60.29** & 60.03 \\  & 4 channels & 0.85 & 15.13 & 43.59 & 45.94 & 48.80 & **49.34** \\  & 3 channels & 1.89 & 5.12 & 33.14 & 35.45 & 37.13 & **38.15** \\  & 2 channels & 1.46 & 1.22 & 25.24 & 26.57 & 27.40 & **27.99** \\  & 1 channel & 0.54 & 1.25 & 20.49 & 21.43 & 21.30 & **21.58** \\   & & & & \\  & 52.06 & 66.22 & 56.87 & 68.09 & 66.44 & **74.77** \\  & 7 channels & 5.91 & 41.03 & 49.35 & 61.02 & 59.01 & **68.42** \\  & 6 channels & 1.81 & 24.57 & 42.38 & 53.45 & 51.29 & **61.26** \\  & 5 channels & 2.46 & 14.20 & 35.78 & 45.50 & 43.39 & **53.05** \\  & 4 channels & 2.38 & 8.56 & 29.84 & 37.37 & 35.60 & **43.87** \\  & 3 channels & 2.70 & 5.65 & 24.94 & 29.68 & 28.59 & **34.19** \\  & 2 channels & 2.63 & 3.24 & 21.54 & 23.77 & 23.32 & **25.73** \\  & 1 channel & 3.00 & 2.08 & 19.92 & 20.84 & 20.41 & **21.20** \\   

Table 1: Test accuracy of 160-way perturbed gene prediction on JUMP-CP. Two training settings are considered: one using only 5 fluorescence channels and the other incorporating all 8 channels, which includes 3 additional brightfield channels. During testing, all possible channel combinations are evaluated and we report the mean accuracies for combinations with the same number of channels (See Appendix G for detailed error analyses). We observe that cross channel reasoning is crucial when the inputs have independent information (fluorescence vs. brightfield).

    &  \\   &  & 100\% & 75\% & 50\% & 25\% & 0\% \\  & 0\% & 25\% & 50\% & 75\% & 100\% \\   & & & & \\  & 55.51 & 52.55 & 51.65 & 49.53 & 45.75 \\  & **56.78** & **58.01** & **58.19** & **58.42** & **57.60** \\   & & & & \\  & — & 50.29 & 52.47 & 54.64 & 56.87 \\  & — & **57.97** & **61.88** & **64.80** & **68.09** \\   

Table 2: ViT vs. ChannelViT when we have varying channel availability during training. Both models are trained using HCS. The accuracy is evaluated using five fluorescence channels (top) and all eight channels (bottom). ChannelViT consistently outperforms ViT across all settings, and the performance gap notably widens as access to more 8-channel data is provided.

66.44). These improvements are consistent across different channel combinations. As we have seen in Figure 4, fluorescence and brightfield channels provide distinct information. ChannelViT effectively reasons across channels, avoiding the need to collapse all information into a single token at the first layer, thereby enhancing performance.

Lastly, we delve into a comparative analysis between input channel dropout and hierarchical channel sampling, as depicted in Figure 2. It is evident from our observations that the ViT model, when trained with HCS, consistently surpasses the performance of those trained with input channel dropout across all channel combinations. Furthermore, we discern a pronounced correlation between the performance of models trained with input channel dropout and the probability distribution of the number of channels sampled during training.

Data EfficiencyIn the realm of microscopy imaging, we often encounter situations where not all channels are available for every cell due to varying experiment guidelines and procedures. Despite this, the goal remains to develop a universal model capable of operating on inputs with differing channels. ChannelViT addresses this issue by treating different channels as distinct input tokens, making it particularly useful in scenarios where not all channels are available for all data. Table 2 presents a scenario where varying proportions (0%, 25%, 50%, 75%, 100%) of the training data

Figure 3: Left: Class-specific relevancy attribution of ChannelViT-S/8 for each cell label (perturbed gene) on JUMP-CP. For each perturbed gene (y-axis) and each channel (x-axis), we calculate the maximum attention score, averaged over 100 cells from that specific cell label. This reveals that ChannelViT focuses on different input channels depending on the perturbed gene. Right: A visualization of the relevancy heatmaps for both ViT-S/8 (8-channel view) and ChannelViT-S/8 (single-channel view). Both models are trained on JUMP-CP using HCS across all 8 channels. ChannelViT offers interpretability by highlighting the contributions made by each individual channel.

Figure 2: HCS vs. input channel dropout on JUMP-CP (trained on all 8 channels). On the left, we present the accuracy of ViT-S/16 and ChannelViT-S/16 under varying input channel dropout rates and HCS. The accuracy is evaluated across all channel combinations, with the mean accuracy reported for combinations with an equal number of channels (represented on the horizontal axis). On the right, we illustrate the probability distribution of the sampled channel combinations during the training process. We observe 1) ViTs trained with input channel dropout tend to favor channel combinations that are sampled the most; 2) ChannelViT with input channel dropout outperforms ViT with input channel dropout; 3) HCS surpasses input channel dropout in terms of channel robustness.

have access to all eight channels, with the remaining data only having access to the five fluorescence channels. The performance of ViT and ChannelViT is evaluated at test time using both the five fluorescence channels (top section) and all eight channels (bottom section).

Our observations are as follows: 1) When only a limited amount of 8-channel data (25%) is available, both ChannelViT and ViT show a decrease in performance when utilizing eight channels at test time compared to five channels; 2) As the availability of 8-channel data increases, the performance of the ViT baseline on the fluorescence evaluation steadily declines (from 55.51 to 45.75), while the performance of ChannelViT sees a slight improvement (from 56.78 to 57.60); 3) When evaluated on all eight channels, ChannelViT significantly outperforms ViT, with an average gap of 9.62.

Channel-specific attention visualizationAttention heatmaps, generated by Vision Transformers (ViTs), have emerged as a valuable tool for interpreting model decisions. For instance, Chefer et al. (2021) introduced a relevancy computation method, which assigns local relevance based on the Deep Taylor Decomposition principle and subsequently propagates these relevance scores through the layers. However, a limitation of ViTs is their tendency to amalgamate information across different channels. In the realm of microscopy imaging, discerning the contribution of each fluorescence channel to the predictions is vital due to their distinct biological implications.

Figure 3 (right) presents the class-specific relevancy visualizations for ViT-S/8 and ChannelViT-S/8. For the top cell labeled KCNH76, ChannelViT appears to utilize information from the Mito channel. For the bottom cell labeled KRAS, ChannelViT seems to utilize information from the ER and RNA channels for its prediction. Compared to ViT, ChannelViT facilitates the examination of contributions made by individual channels.

In Figure 3 (left), we further compute the maximum attention score (averaged over 100 cells) for each cell label (perturbed gene) and each input channel. Our observations indicate that ChannelViT focuses on different channels for different labels (corresponding to perturbed genes), with the Mito channel emerging as the most significant information source. This heatmap, which describes the discriminability of different labels over different channels, can also aid in better understanding the relationships between different gene perturbations.

## 3 Conclusion

In conclusion, our proposed model, ChannelViT, effectively addresses the unique challenges of multi-channel imaging domains. By enhancing reasoning across input channels and seamlessly handling inputs with varying sets of channels, ChannelViT has consistently outperformed its ViT counterpart in our evaluations on ImageNet and diverse applications such as medical, microscopy cell, and satellite imaging. The introduction of Hierarchical Channel Sampling (HCS) further bolsters the model's robustness when testing with different channel combinations. Moreover, ChannelViT not only improves data efficiency but also provides additional interpretability, underscoring its potential for broad applications in the field of multi-channel imaging.