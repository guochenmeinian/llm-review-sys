# CBD: A Certified Backdoor Detector Based on Local Dominant Probability

Zhen Xiang  Zidi Xiong  Bo Li

University of Illinois Urbana-Champaign

{zxiangaa, zidix2, lbo}@illinois.edu

###### Abstract

Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first _certified backdoor detector_ (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic _local dominant probability_. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves comparable or even higher detection accuracy than state-of-the-art detectors, and it in addition provides detection certification. Notably, for backdoor attacks with random perturbation triggers bounded by \(_{2} 0.75\) which achieves more than 90% attack success rate, CBD achieves 100% (98%), 100% (84%), 98% (98%), and 72% (40%) empirical (certified) detection true positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, with low false positive rates.

## 1 Introduction

Despite the success of deep neural networks (DNNs) in many applications, they are vulnerable to adversarial attacks such as backdoor attacks . A DNN being backdoored will learn to predict an adversarial target class for test samples embedded with a backdoor trigger, while maintaining a high accuracy on clean test samples without the trigger .

Backdoor detection is a popular task for backdoor defense. It aims to detect if a given model is backdoored without access to the training set or any real test samples that are possibly embedded with the trigger . The task corresponds to practical scenarios where the user of an app or a legacy system containing a DNN seeks to know if the model is backdoor attacked, where the training set is not available . Various empirical approaches have been proposed for backdoor detection, most of which are based on trigger reverse engineering , or meta classification . However, none of these works quantitatively investigate the conditions under which the backdoor attacks are guaranteed to be detectable. Without a detection guarantee, DNNs are still vulnerable to future attacks (e.g.) with new trigger types .

In this paper, we make the first attempt toward the _certification_ of backdoor detection. Certification is an important concept for studies on the robustness of DNNs against adversarial examples . In particular, a robustness certification refers to a probabilistic or deterministic guarantee for a model to produce desired outputs (e.g. correct label prediction) when the adversarial perturbation applied to the inputs satisfies certain conditions (e.g. with some constrained perturbation magnitude) . As an analogy, we propose a _certified backdoor detector_ (CBD)that is guaranteed to trigger an alarm if the attack for a given domain satisfies certain conditions. In Sec. 3, we introduce the detection procedure of CBD based on conformal prediction (with our proposed, optional adjustment scheme) using a novel (model-level) statistic named _local dominant probability_. The calibration set for conformal prediction is obtained from a small number of benign shadow models trained on a small validation set, which addresses the unavailability of the training set. In Sec. 4, we derive the condition for attacks with detection guarantee, as well as a probabilistic upper bound for the false positive rate of our detector, for any prescribed significance level specifying the aggressiveness of the detection. Notably, our certification is comprehensive - for any domain, more effective attacks with strong _trigger robustness_ (which measures the resilience of a trigger against test-time noises) and more stealthy attacks (against human inspectors) with small trigger perturbation magnitudes are easier to be detected with guarantees. Moreover, both our detector and the certification method do not assume the trigger incorporation mechanism or the training configuration of the model, which allows their potential application to future attacks. Our contributions are summarized below:

* We propose CBD, the first certified backdoor detector, which is based on an adjustable conformal prediction scheme using a novel _local dominant probability_ statistic.
* We propose a certification method and show that for any domain, backdoor attacks with stronger _trigger robustness_ and smaller trigger perturbation magnitudes are more likely to be detected by CBD with guarantee. We also derive a probabilistic upper bound for the false positive rate of CBD.
* We show that CBD achieves comparable or even higher detection accuracy than state-of-the-art detectors against three types of backdoors. We also show that for backdoor attacks with random perturbation triggers bounded by \(_{2} 0.75\), CBD achieves 100% (98%), 100% (84%), 98% (98%), and 72% (40%) empirical (certified) true positive rates on GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, with only 0%, 0%, 6%, and 10% false positive rates, respectively.

## 2 Related Work

**Backdoor Detection Methods** Existing methods for backdoor detection are all empirical without theoretical guarantees. An important type of reverse-engineering-based method estimates putative triggers for anomaly detection [67; 6; 75; 41; 78; 73; 79; 76; 69; 56; 62; 24]. Certification for these methods is hard due to the complexity of trigger reverse engineering. Another type of detector is based on meta-classification that involves a large number of shadow models trained with and without attacks [82; 29]. Differently, our CBD is based on conformal prediction involving a scalar detection statistic (rather than a high dimensional feature vector employed by [82; 29]), such that only a small number of benign shadow models will be required. More importantly, our CBD is certified, i.e. with a detection guarantee, which is different from all the detention methods mentioned above.

**Other Backdoor Defense Tasks** Backdoor defenses during training aim to produce a backdoor-free classifier from the possibly poisoned training set [63; 5; 74; 14; 25; 57; 77; 23; 4; 16]. These defenses require access to the training set, which is unavailable for backdoor detection. Backdoor mitigation aims to "repair" models being backdoor attacked [40; 72; 35; 19; 88; 85], which can be viewed as a downstream task following backdoor detection. Inference-stage trigger detection aims to detect if a test sample contains the trigger [15; 55; 12; 9; 44; 34]. However, backdoor detection is performed before the inference stage, where test samples are not available. These defense tasks will not be further discussed due to their fundamental differences from the backdoor detection task.

**Certified Backdoor Defenses** Existing methods mostly modify the training process to prevent the backdoor from being learned, while manipulating the test sample to destroy any potentially embedded triggers [86; 71; 27; 53]. These methods are not applicable to the backdoor detection problem where both the training set and the test samples are not available. More importantly, all these existing certified defenses are deployed during training, which requires an uncontaminated training process fully controlled by the defender. In contrast, in this paper, we consider a stronger threat model that allows the attacker to have full control of the training process.

## 3 Detection Method

### Problem Definition

**Threat Model** Consider a classification domain with sample space \(\) and label space \(\). A backdoor attack is specified by a trigger with some incorporation function \(:\) and a target class \(t\). For a successful backdoor attack, the victim classifier will predict to the target class \(t\) whenever a test sample is embedded with the trigger, while test samples without the trigger will be correctly classified [18; 8; 42; 38]. In this paper, we do allow advanced attackers with full control of the training process [51; 87; 70; 50; 3]. This is deemed a stronger threat model than all previous works on certified backdoor defense where the defender is assumed with full control of the training process. However, in this paper, we do not consider backdoor attacks with multiple triggers or target classes [84; 83] - even empirical detection of these attacks is a challenging problem .

**Goal of Certified Backdoor Detection** The fundamental goal is backdoor detection, i.e. to infer if a given classifier \(f(;w):\) is backdoored or not [48; 64]. The defender has no access to the training set or any test samples that may contain the trigger. In practice, the defender also has no access to benign classifiers with high accuracy for the same domain as \(f(;w)\) - otherwise, these benign classifiers can be used for the task, and detection will be unnecessary. However, the defender is assumed with a small validation set of clean samples for detection - this is a standard assumption made by most post-training backdoor defenses [67; 82; 75; 56; 41].

Beyond model inference, a _certified_ backdoordoor also needs to provide each classification domain (associated with the model to be inspected) with a condition on \(\), \(t\), and \(w\), such that any successful backdoor attack with a trigger \(\) and a target class \(t\) on a victim classifier \(f(;w)\) (trained on this classification domain) is guaranteed to be detected if the condition is satisfied. The stronger a certification is, the more likely a successful attack on the domain will be detectable with guarantee. Moreover, a certification has to be associated with a guarantee on the false positive rate; otherwise, an arbitrarily strong certification can be achieved by increasing the aggressiveness of the detection rule.

### Overview of CBD Detection

**Key Intuition** For a successful backdoor attack with a trigger \(\) and a target class \(t\), a test instance \(x\) embedded with the trigger (denoted by \((x)\)) will be classified to the target class \(t\) with high probability. Practical backdoor triggers should also be robust (i.e. resilient) to noises either from the environment or introduced by simple defenses based on input-preprocessing such as blurring and/or quantization [81; 39]. Such _trigger robustness_ can be measured by the distribution of the model prediction in the neighborhood of each \((x)\) - the more robust the trigger is, the more samples in the neighborhood of \((x)\) will be predicted to the target class \(t\). Thus, if the perturbation magnitude of a robust trigger, i.e. \(\|(x)-x\|_{2}\), is small (which is usual for backdoor attacks to achieve stealthiness), a significant proportion of samples in the neighborhood of \(x\) will also be classified to class \(t\) due to their closeness to \((x)\). Such an increment in the target class probability in the neighborhood of all instances is captured by our proposed statistic named _local dominant probability_ (LDP) to distinguish backdoored classifiers from benign ones - the former tend to have a larger LDP than the latter.

**Outline of CBD Detection Procedure** In short, CBD performs an _adjustable_ conformal prediction to test if the LDP statistic computed for the model to be inspected is sufficiently large to trigger an alarm. A small number of shadow models (with the same architecture as the model to be inspected) are trained on a relatively small validation set, with an LDP computed for each shadow model to form a calibration set. Then, a small proportion of outliers with large values in the calibration set are optionally removed, e.g., by anomaly detection. Based on this adjusted calibration set, the p-value is computed for the LDP for the model to be inspected. The model is deemed to be backdoored if the p-value is less than some prescribed significance level (e.g. the classical 0.05 for statistical testing).

### Definition of LDP

We first define a _samplewise local probability vector_ (SLPV) to represent the distribution of the model prediction outcomes in the neighborhood of any given sample. Then, based on SLPV, we define the _samplewise trigger robustness_ (STR) that measures the resilience of a trigger to the Gaussian noise when it is embedded in a particular sample. Finally, we define the LDP statistic based on SLPV.

**Definition 3.1**.: _(**Samplewise Local Probability Vector (SLPV)**) For any classifier \(f(;w):\) with parameters \(w\), the SLPV for any input \(x\) is a probability vector \((x|w,)^{K}\) over the \(K=||\) classes, with the \(k\)-th entry defined by \(p_{k}(x|w,)_{(0,^{2}I) }(f(x+;w)=k)\), \( k\), where \((,)\) represents Gaussian distribution with mean \(\) and covariance \(\)._

**Remarks:** If \(f(;w)\) is continuous at \(x\) with \(f(x;w)=k\) for some \(k\), it is trivial to show that \(p_{k}(x|w,) 1\) as \( 0\), i.e. the SLPV becomes a singleton at the predicted class of \(x\) without the Gaussian noise. In practice, SLPV can be estimated by Monte Carlo for any given model and sample.

**Definition 3.2**.: _(**Samplewise Trigger Robustness (STR)**) Consider a backdoor attack with a trigger \(\) and a target class \(t\) against a victim model \(f(;w)\). For any sample \(x\) and any isotropic Gaussian distribution \((0,^{2}I)\), the STR is defined by the \(t\)-th entry of the SLPV for \((x)\) (i.e. sample \(x\) with the trigger \(\) embedded), which is denoted by \(R_{,t}(x|w,) p_{t}((x)|w,)\)._

**Remarks:** STR measures the resilience of a trigger against Gaussian noises. Usually, strong STR can be naturally achieved by embedding the trigger in a large variety of samples during training.

**Definition 3.3**.: _(**Local Dominant Probability (LDP)**) Consider a domain with \(K=||\) classes and an isotropic Gaussian distribution \((0,^{2}I)\). The LDP for a classifier \(f(;w)\) is defined by_

\[s(w)=||_{k=1}^{K}(x_{k}|w,)||_{}\] (1)

_where \(x_{1},,x_{K}\) are \(K\) independent random samples satisfying \(f(X_{k};w)=k\), \( k\{1,,K\}\)._

**Remarks:** By the definition, LDP for a given classifier \(f(;w)\) is computed on \(x_{1},,x_{K}\) independently sampled from the \(K\) classes respectively, satisfying \(f(x_{k};w)=k\) for \( k\). LDP computed on more samples per class yields similar detection and certification performance empirically. Specifically, we first compute the SLPV for each of the \(K\) samples. Then we take the average of the \(K\) SLPVs and pick the maximum entry as the LDP for \(f(;w)\). Note that LDP is always no less than \(\).

As stated in the key intuitions in Sec. 3.2, LDP tends to be larger for backdoored classifiers than for benign ones. This can be understood from the illustration in Fig. 1 for a classification domain with \(K=4\) classes. For the benign classifier on the left, the SLPVs for \(x_{1},,x_{4}\) are almost orthogonal to each other, leading to a small LDP close to \(\). On the right, we consider a backdoor attack with a robust trigger \(\) and a target class 4 (with orange decision region). The strong STRs for \(x_{1}\), \(x_{2}\), and \(x_{3}\) (represented by the large orange regions around \((x_{1})\), \((x_{2})\), and \((x_{3})\)), together with the small trigger perturbation magnitudes (i.e. small \(||(x_{i})-x_{i}||_{2}\) for \(i=1,2,3\)), significantly change the class distribution in the neighborhood of each of \(x_{1}\), \(x_{2}\), and \(x_{3}\). In particular, there will be a clear increment in the 4-th entry of the SLPVs for \(x_{1}\), \(x_{2}\), and \(x_{3}\). Thus, the 4-th entry associated with the backdoor target class will dominate the average SLPV over \(x_{1},,x_{4}\), leading to a large LDP.

### CBD Detection Procedure

Although backdoored and benign classifiers have different LDP distributions, it is still a challenge in practice to set a detection threshold. To solve this problem, we propose to use the conformal prediction, which employs a calibration set for supervision . Here, the calibration set consists of LDP statistics obtained from a small number of benign shadow models trained on the small validation set possessed by the defender. However, the actual benign classifiers to be inspected are usually trained on more abundant data, such that the LDPs for these classifiers will likely follow a different distribution from the LDPs for the shadow models without sufficient training. In particular, the LDPs in the calibration set (obtained from the shadow models) may easily have an overly large sample variance and a heavy tail of large outliers. Directly using this calibration set for conformal prediction may lead to a conservative detection due to an overly large detection threshold.

Thus, we propose an optional adjustment scheme that treats the \(m\) largest LDP statistics in the calibration set as outliers. In practice, prior knowledge may be required to determine the exact value

Figure 1: Illustration of the difference in LDP between benign and backdoored classifiers on a classification domain with \(K=4\) classes. A backdoor attack with a robust trigger \(\) (with a small perturbation magnitude) and a target class 4 (with orange decision region) changes the class distribution in the neighborhood of \(x_{1}\), \(x_{2}\), and \(x_{3}\), resulting in a larger LDP for the backdoored model than for the benign model.

of \(m\), while in our experiments, a small \(m/N\) (\( 0.2\), where \(N\) is the size of the calibration set) may significantly increase the detection or certification power of our CBD with only small increment to the false positive rate. The detection procedure of our CBD consists of the following four steps:

**1)** Given a classifier \(f(;w)\) to be inspected, estimate LDP \(s(w)\) based on Def. 3.3.

**2)** Train (benign) shadow models \(f(;w_{1}),,f(;w_{N})\) on the clean validation dataset, and construct a calibration set \(_{N}=\{s(w_{1}),,s(w_{N})\}\) by computing the LDP for each model.

**3)** Compute the adjusted conformal p-value (assuming \(m\) large outliers) defined by:

\[q_{m}(w)=1-_{N}:s<s(w)\}|,N-m\}}{N-m+1}\] (2)

**4)** Trigger an alarm if \(q_{m}(w)\), where \(\) is a prescribed significance level (e.g. \(\)=0.05).

## 4 CBD Certification

In addition to a detection inference, CBD also provides a certification, which is a condition under which attacks are guaranteed to be detectable. Detailed proofs in this section are shown in App. A.

**Theorem 4.1**.: _(**Backdoor Detection Guarantee**) For an arbitrary classifier \(f(;w):\) to be inspected, let \(x_{1},,x_{K}\) be the \(K\) randomly selected samples and \((0,^{2}I)\) be the isotropic Gaussian distribution used to compute the LDP for \(f(;w)\). Let \(\) be the prescribed significance level of CBD. A backdoor attack with a trigger \(\) and a target class \(t\) is guaranteed to be detected if:_

\[<(^{-1}(1-s_{(N-m-(N-m+1))})-^{-1}(1- ))\] (3)

_where \(\) is the standard Gaussian CDF, \(=_{k=1,,K}R_{,t}(x_{k}|w,)\) is the minimum STR over \(x_{1},,x_{K}\), \(=_{k=1,,K}||(x_{k})-x_{k}||_{2}\) is the maximum perturbation magnitude of the trigger, \(m\) is the number of assumed outliers in the calibration set \(_{N}\) with size \(N\), and \(s_{(n)}\) denotes the \(n\)-th smallest element in \(_{N}\)._

Proof (sketch).: The STR for each sample \(x_{k}\) equals the \(t\)-th entry of the SLPV for \((x_{k})\) by Def. 3.2. We also connect the SLPV for \((x_{k})\) to the SLPV for \(x_{k}\) using the Neyman-Pearson lemma (). Based on this connection, we derive the lower bound for the minimum STR, such that the \(t\)-th entry of the SLPV of each \(x_{k}\) is sufficiently large to result in a large LDP for the attack to be detected. 

**Remarks:** (1) (**Main Results**) For fixed trigger perturbation size (\(\)), detection of attacks with larger STR (\(\)) is more likely to be guaranteed; while for fixed STR, detection of attacks with smaller trigger perturbation size is more likely to be guaranteed. (2) Our backdoor detection guarantee is inspired by the randomized smoothing approach in  for certified robustness against adversarial examples. However, certified backdoor detection and certified robustness against adversarial examples are fundamentally different, as will be detailed in App. I. (3) Certified backdoor detection and certified robustness against backdoors complement each other. The former provides detection guarantees to strong backdoor attacks, while the latter prevents the trigger from being learned during training [86; 53]. The two types of certification may cover the entire attack space together in the future, such that a backdoor attack will be either strong enough to be detected or weak enough to be disabled.

A meaningful certification for backdoor detection should also be along with a guarantee for the false positive rate (FPR). Otherwise, one can easily design a backdoor detector that always triggers an alarm, which provides detection guarantees to all backdoor attacks but is useless in practice.

**Theorem 4.2**.: _(**Probabilistic Upper Bound for FPR**) Consider a random calibration set \(_{N}=\{s_{1},,s_{N}\}\) with \(s_{1},,s_{N}\) i.i.d. following some continuous distribution \(F\). Consider a random benign classifier \(f(;W)\) with LDP \(s(W)\) following some distribution \(\). Assume \(F\) dominates \(\) in the sense of first-order stochastic dominance. Let \(m\) be any number of assumed outliers in \(_{N}\) and let \(\) be any prescribed significance level of CBD. Then, the FPR of CBD on \(f(;W)\) conditioned on \(_{N}\), which is denoted by \(Z_{N}=(q_{m}(W)|_{N})\) based on Eq. (2), will be first-order dominated by a random variable \(B\) following \((m+l+1,N-m-l)\) with \(l=(N-m+1)\), i.e. \(B_{1}Z_{N}\). In other words, \((Z_{N} q)(B q)\) for any real \(q\)._

Proof (sketch).: We first express the false positive rate \(Z_{N}\) in terms of the order statistics on the elements of the random calibration set \(_{N}\). Then, we derive the lower bound of the CDF of \(Z_{N}\) using the distribution of order statistics followed by a binomial expansion.

**Remarks:** The assumption that \(F\) dominates \(\) in Thm. 4.2 generally holds in practice. Again, this is because the actual benign classifiers to be inspected are usually trained on more abundant data than the benign shadow models. Empirical results supporting this assumption are shown in Sec. 5.4. Moreover, an analysis of this phenomenon is presented in App. B, where we show on binary Bayes classifiers that a higher empirical loss can easily lead to a larger expected LDP.

While we have shown that \(Z_{N}\) conditioned on a random calibration set of size \(N\) is upper bounded by a Beta random variable in the sense of first-order stochastic dominance, in Col. 4.3 below, we show that asymptotically, the upper bound of \(Z_{N}\) converges to a constant in probability as \(N\).

**Corollary 4.3**.: _(Asymptotic Property of FPR) Consider the settings in Thm. 4.2. For any \(>0\), \(_{N+}(Z_{N})=1\), where \(=+(1-)+\) with \(=m/N\)._

Proof (sketch).: We show that any random variable \(B\) with the Beta distribution described in Thm. 4.2 satisfies \(_{N+}(B)=1\). Then, the corollary is proved since \(Z_{N}\) is dominated by \(B\). 

**Remarks:** For classical conformal prediction without adjustment where \(m=0\), we will have \(=0\) and \(_{N+}(Z_{N}+)=1\) for any \(>0\). In this case, the upper bound of the false positive rate of our CBD converges to the prescribed significance level \(\) in probability.

## 5 Experiment

There have been many different types of backdoor attacks proposed, each also with a wide range of configurations. Thus, it is infeasible to evaluate our CBD over the entire space of backdoor attacks. Inspired by the evaluation protocol for certified robustness , in Sec. 5.1, we focus on backdoor attacks with random perturbation triggers to comprehensively evaluate the certification capability of our CBD. In Sec. 5.2, we compare CBD with three state-of-the-art backdoor detectors (all "uncertified") against backdoor attacks with three popular trigger types to evaluate the detection capability of CBD. In Sec. 5.3, we present ablation studies (e.g. on the number of shadow models used by CBD). Additional results and other supportive empirical analyses are shown in Sec. 5.4.

### Evaluation of CBD Certification

For certified robustness, the prediction of a test example is unchanged with a guarantee, if the magnitude of the adversarial perturbation is smaller than the _certified radius_. Certified robustness is usually evaluated by the _certified accuracy_ on some random test set as the proportion of the samples that are guaranteed to be correctly classified if the magnitude of the adversarial perturbation is no larger than some prescribed value. As an analog, our certification for backdoor detection is specified by an inequality that involves both the STR and the perturbation magnitude of the trigger, which naturally produces a two-dimensional "_certified region_" (illustrated in Fig. 7 and Fig. 8 in Apdx. D). Our certification method is evaluated on a set of random backdoor attacks, each using a random pattern as the trigger, with the perturbation magnitude satisfying some \(_{2}\) constraint. We are interested in the proportion of the attacks falling into the certified region, i.e. guaranteed to be detected.

#### 5.1.1 Setup

**Dataset:** Our experiments are conducted on four benchmark image datasets, GTSRB , SVHN , CIFAR-10 , and TinyImageNet , following their standard train-test splits. Due to the large number of models that will be trained to evaluate our certification method, except for GTSRB, we use 40% of the training set to train these models. We also reserve 5,000 samples from the test set of GTSRB, SVHN, and CIFAR-10, and 10,000 samples from the test set of TinyImageNet (much smaller than the training size for the models for evaluation) for the defender to train the shadow models. More details about these datasets are deferred to App. C.1.

**Evaluation Metric:** Following the convention, the detection performance of CBD is evaluated by the true positive rate (**TPR**) defined by the proportion of attacks being detected (with a correct inference of the target class) and the false positive rate (**FPR**) defined by the proportion of benign classifiers falsely detected. For certification, we define certified TPR (**CTPR**) as the proportion of a set of attacks that are guaranteed to be detectable, i.e. falling into the certified region.

**Attack Setting:** For each dataset, we create 50 backdoor attacks, each with a randomly selected target class. For each attack, we generate a random trigger, which is a random perturbation embeddedby \((x)=x+v\) with \(||v||_{2} 0.75\). The generation process also involves random nullification of the trigger pattern, which helps the trigger to be learned. More details about trigger generation are deferred to App. C.2 due to space limitations. The poisoning ratios for the attacks on GTSRB, SVHN, CIFAR-10, and TinyImageNet are 7.8%, 15.3%, 11.3%, and 12.4%, respectively. Lower poisoning ratios may largely reduce the attack success rate since many randomly generated triggers are relatively hard to learn. Results for triggers with larger perturbation size are shown in App. D.

**Training:** For model architecture, we use the winning model on the leaderboard  for GTSRB, MobileNetV2  for SVHN, the same architecture in  for CIFAR-10, and ResNet-34  for TinyImageNet. For each dataset, 50 benign models are trained (also on the 40% training set except for GTSRB) to evaluate the FPR. The accuracies for these benign models are roughly 98%, 92%, 78%, and 47% on GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively. For each attack, we train a model with \( 90\%\) attack success rate and \( 2\%\) degradation in the benign accuracy (or re-generate the attack for training until both conditions are satisfied). More details about the training configurations are shown in App. C.3. Finally, we train 100 shadow models for each of GTSRB, SVHN, and CIFAR-10, and 50 shadow models for TinyImageNet using the same architectures and configurations as above - these shadow models are used by our CBD for detection and certification.

#### 5.1.2 Certification Performance

In Fig. 2, we show the CTPR (solid) of our CBD against the attacks with the random perturbation trigger on the four datasets for a range of \(\) and for \(=m/N\{0,0.1,0.2\}\). The TPR (dashed) for each combination of \(\) and \(\) is also plotted for reference. Recall that \(\) here represents the proportion of assumed outliers in the calibration set for the adjustment of the conformal prediction, and \(\) is the standard deviation of the isotropic Gaussian distribution for the LDP computation. In our experiments, 1024 random Gaussian noises are generated for each sample used to compute the LDP. The significance level for conformal prediction is set to the classical \(=0.05\) for statistical testing.

In general, our certification is effective, which covers up to 98%, 84%, and 98% backdoor attacks with the random perturbation trigger (all achieved with \(=0.2\)) on GTSRB, SVHN, and CIFAR-10, respectively. Even for the very challenging TinyImageNet dataset, CBD certifies up to 40% of these attacks. Moreover, for all choices of \(\) and \(\), CTPR is upper-bounded by TPR. For example, for the aforementioned CTPRs on the four datasets, the corresponding TPRs are 100%, 100%, 98%, and 72%, respectively (with FPRs 0%, 0%, 6%, and 10%, respectively, see App. D). In fact, all attacks with the detection guarantee are detected empirically, showing the correctness of our certification.

We also make the following observations regarding the hyperparameters \(\) and \(\): 1) An increment in \(\) may lead to an increment in both CTPR and TPR. This is due to the existence of a heavy tail

Figure 2: Certification performance of CBD against backdoor attacks with random triggers with perturbation magnitude \(_{2} 0.75\) measured by CTPR (solid) for a range of \(\) for \(=0,0.1,0.2\). The CTPRs are all upper-bounded by the TPRs (dashed), showing the correctness of our certification. Notably, CBD achieves up to 98% (100%), 84% (100%), 98% (98%), and 40% (72%) CTPRs (TPRs) on GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, across all choices of \(\) and \(\). An increment in \(\), the assumed ratio of calibration outliers, may lead to further increments in both CTPR and TPR. The hyperparameter \(\) can be determined using the calibration set in practice.

(corresponding to large outliers) in the LDP distribution for the calibration set. While an overly large \(\) may cause a significant increment to FPR, our additional results in App. D show that empirically, this is not the case for \( 0.2\). 2) Each domain has its own proper range for the choice of \(\). In general, the detection power of CBD (reflected by TPR) reduces as \(\) becomes overly small. This phenomenon agrees with our remarks on Del. 3.1 - SLPV converges to a singleton at the labeled class as \(\) approaches zero, regardless of the presence of a backdoor attack. Moreover, the certification power (reflected by CTPR) also reduces for small \(\)'s, which matches the inequality (3) in Thm. 4.1. For overly large \(\)'s, on the other hand, LDP for benign classifiers may also grow and possibly approach one, resulting in a large FPR. However, in this case, LDP is no longer computed in a truly 'local' context, contrary to the intuition implied by its name. In Sec. 5.2, we will introduce a practical scheme to choose a proper \(\) for each domain based on the shadow models.

### Evaluation of CBD Detection

Here, we show the detection performance of CBD against backdoor attacks with various trigger types, including the BadNet square , the "chessboard" (CB) pattern , and the blended pattern . For each of GTSRB, SVHN, and CIFAR-10, we train 20 models (using the full training set) for each trigger type following the same training configurations described in Sec. 5.1.1. TinyImageNet is not considered here due to the high training cost. Details for each trigger type and attack settings are shown in App. C.4. We also compare CBD with three state-of-the-art backdoor detectors without certification, which are Neural Cleanse (NC) , K-Arm , and MNTD . In particular, K-Arm and MNTD require manual selection of the detection threshold. For both of them, we choose the threshold that maximizes the TPR while keeping a 5% FPR for each dataset. Moreover, we devise a "supervised" version of CBD, named CBD\({}_{}\), which still uses LDP as the detection statistic but without the conformal prediction. The detection threshold for CBD\({}_{}\) is determined in the same way as for K-Arm and MNTD, i.e. by maximizing the TPR with a controlled 5% FPR.

In practice, CBD needs to choose a moderately large \(\) for each detection task. To this end, we first initialize a small \(\) such that for each of the \(N\) shadow models, the SLPVs for the \(K\) samples used for computing the LDP all concentrate at the labeled classes. In this case, the LDPs for all the shadow models are close to \(\). Then, we gradually increase \(\) until \(_{n=1}^{N}_{k=1}^{K}p_{k}(x_{k}^{(n)}|w_{n},)<\) for some relatively small \(\), where \(x^{(n)}\) is the \(k\)-th sample for LDP computation for the \(n\)-th model, i.e. the SLPVs are no longer concentrated at the labeled classes. In the left of Fig. 4, we show the choice of \(\) based on the above scheme for a range of \(\), which exhibits a trend of convergence as \(\) decreases. We also notice that \(\) selected for \(<0.2\) roughly matches the \(\) choices in Fig. 2 that yields relatively high CTPR and TPR, showing the effectiveness of our scheme. In our evaluation of CBD detection, we set \(=0.1\), which yields \(=1.15,0.39,1.14\) for GTSRB, SVHN, and CIFAR-10, respectively. Other choices of \(\) for \(\) less than \(0.2\) yield similar detection performance.

As shown in Tab. 1, CBD achieves comparable or even higher TPRs than the SOTA detectors (that benefit from unrealistic supervision) for all trigger types on all datasets. CBD also provides non-trivial detection guarantees to most attack types on these datasets. The relatively low CTPRs, e.g. for BadNet on GTSRB, are due to the large perturbation magnitude of the trigger. The even better TPRs achieved by CBD\({}_{}\), though with the same supervision as for the SOTA detectors, show the effectiveness of the LDP statistic in distinguishing backdoored models from benign ones. Such effectiveness is further verified by the receiver operating characteristic (ROC) curves for CBD\({}_{}\). Compared with the baseline detectors, CBD\({}_{}\) achieves generally higher overall areas under curves (AUCs) across the three datasets.

### Ablation Study

We show that the time efficiency and data efficiency of CBD can be improved by training fewer shadow models and using fewer samples for training the shadow models, respectively, without significant degradation in the detection or certification performance. In particular, we show in Tab. 2 that CBD with \(=0.2\) achieves similar TPRs and CTPRs for the same set of attacks in Sec. 5.2 when we reduce the number of shadow models from 100 to 50, 25, and 10, respectively. Such robustness of CBD to the calibration size further verifies the clear separation between the benign and backdoored classifiers using our proposed LDP. In Tab. 3, we show that with shadow models trained on only 100clean samples per class, CBD requires a larger \(\) to achieve similar TPRs and CTPRs. This is because more shadow models will exhibit an abnormally large LDP due to significantly insufficient training.

### Additional Experiments

**Empirical validation of the stochastic dominance assumption in Thm. 4.2.** In the middle of Fig. 4, we show the histograms (with the associated empirical CDF) of the LDP statistics for the shadow models and the benign models for all four datasets. The statistics for each dataset are obtained using the practically selected \(\). The LDP for the benign models is clearly dominated by the LDP for the shadow models in the sense of first-order stochastic dominance.

    &  &  &  & Average \\   & benign & BadNet & CB & Blend & benign & BadNet & CB & Blend & benign & BadNet & CB & Blend \\   NC & 20 & 50 & 75 & 20 & 40 & 80 & 100 & 95 & 20 & 35 & 95 & 60 & 67.8 \\ K-Arm & 5 & 100 & 100 & 5 & 100 & 70 & 40 & 5 & 100 & 80 & 55 & 82.8 \\ MNTD & 5 & 20 & 0 & 0 & 5 & 10 & 10 & 15 & 5 & 90 & 100 & 75 & 35.6 \\ CBD\({}_{}\) & 5 & 100 & 95 & 100 & 5 & 100 & 100 & 90 & 5 & 65 & 100 & 55 & **89.4** \\ CBD\({}_{0}\) & 0 & 75 (5) & 95 (80) & 80 (20) & 0 & 75 (45) & 100 (100) & 80 (75) & 0 & 50 (5) & 100 (90) & 45(30) & 77.2 \\ CBD\({}_{}\) & 0 & 90 (15) & 95 (85) & 90 (25) & 0 & 90 (55) & 100 (100) & 80 (80) & 20 & 75 (20) & 100 (95) & 55 (35) & 86.1 \\ CBD\({}_{}\) & 0 & 90 (15) & 95 (85) & 95 (35) & 0 & 95 (65) & 100 (100) & 90 (80) & 25 & 75 (25) & 100 (100) & 60 (40) & **88.9** \\   

Table 1: Certified detection of CBD for \(=0,0.1,0.2\) (shaded), with the empirical detection performance (measured by TPR (%)) compared with NC, K-Arm, MNTD, and CBD\({}_{}\) against BadNet, CB, and Blend attacks on GTSRB, SVHN, and CIFAR-10. The parentheses in each shaded cell contain the CTPR (%) associated with the TPR outside the parentheses. CBD achieves comparable or even higher empirical TPRs compared with the SOTA baselines and provides non-trivial (or even tight) certification for different attacks and datasets. FPRs (%) are reported on benign classifiers.

    &  &  &  \\  \# shadow & benign & BadNet & CB & Blend & benign & BadNet & CB & Blend & benign & BadNet & CB & Blend \\  
10 & 0 & 95 (10) & 95 (85) & 95 (35) & 0 & 75 (45) & 100 (100) & 90 (75) & 25 & 85 (25) & 100 (100) & 55 (40) \\
25 & 0 & 95 (5) & 95 (85) & 95 (25) & 0 & 95 (45) & 100 (100) & 80 (75) & 25 & 80 (25) & 100 (100) & 70 (40) \\
50 & 0 & 95 (15) & 95 (85) & 95 (40) & 0 & 95 (65) & 100 (100) & 85 (75) & 25 & 80 (25) & 100 (100) & 60 (40) \\
100 & 0 & 90 (15) & 95 (85) & 95 (35) & 0 & 95 (65) & 100 (100) & 90 (80) & 25 & 75 (25) & 100 (100) & 60 (40) \\   

Table 2: Certified detection of CBD (with CTPR inside the parentheses and TPR outside) for \(=0.2\) using 10, 25, 50, and 100 (the default in Sec. 5.2) shadow models. Both the detection and certification performance of CBD are not significantly affected by reducing the number of shadow models.

Figure 3: Receiver operating characteristic (ROC) curves of CBD\({}_{}\) aggregated over all three trigger types on GTSRB, SVNH, and CIFAR-10, respectively. CBD\({}_{}\) with our proposed LDP statistic achieves higher overall areas under curves (AUCs) than K-Arm and MNTD across the three datasets.

**Class imbalance is not the reason for a large LDP.** In our experiments involving generally balanced datasets, backdoor poisoning (i.e. embedding the trigger in a large variety of samples during training) not only enhances the trigger robustness of the attack, but also introduces an imbalance in the poisoned training set. Thus, it is important to show that the large LDPs observed in our experiments are a result of the robustness of the trigger, rather than the class imbalance. Note that if class imbalance can also cause a large LDP, there will easily be false alarms for benign classifiers trained on imbalanced datasets1. Here, we train two groups of benign models on SVHN, with 20 models per group. For the first group, the models are trained on a balanced dataset with 3,000 images per class. For each model in the second group, the training set contains 4,400 additional images labeled to some randomly selected class. With the same model architecture and training configurations, the two groups have similar LDP distributions, with mean\(\)std being 0.445\(\)0.125 and 0.429\(\)0.119, respectively, and with a 0.698 p-value for the t-test for mean. Thus, LDP will not be affected by class imbalance in general, and our method indeed detects the backdoor attack rather than the class imbalance.

**Advanced attacks.** Based on the key ideas in Sec. 1, CBD requires the attack to have a large STR over the sample distribution. However, this requirement is not always satisfied, especially for some advanced attacks with subtle, sample-specific triggers, such as WaNet . Although not detectable or certifiable by our CBD, these attacks can hardly survive noises either from the environment or simple prepossessing-based defenses in practice. To see this, for each of GTSRB and SVHN, we train 20 models with successful WaNet attacks. We consider a simple test-time defense by applying 1024 randomly sampled Gaussian noises from distribution \((0,^{2}I)\) to each input and then performing a majority vote. As shown in the right of Fig. 4, for both datasets and for small \(\), the average attack success rate (ASR) quickly drops without clear scarifies in the benign accuracy (ACC).

## 6 Conclusion

In this paper, we proposed CBD, the first certified backdoor detector, which is based on an adjustable conformal prediction using a novel LDP statistic. CBD not only performs detection inference but also provides a condition for attacks that are guaranteed to be detectable. Our theoretical results show that backdoor attacks with a trigger more resilient to noises and with a smaller perturbation magnitude are more likely to be detected with a guarantee. Our empirical results show the strong certification and detection performance of CBD on four benchmark datasets. In future research, we aim to enhance the certification bound to encompass backdoor attacks with larger trigger perturbation norms, such as rotational triggers and subject-based triggers.