# Can Graph Learning Improve Planning

in LLM-based Agents?

Xixi Wu\({}^{1,3}\)+ Yifei Shen\({}^{2}\)+\({}^{}\) Caihua Shan\({}^{2}\) Kaitao Song\({}^{2}\) Siwei Wang\({}^{2}\) Bohang Zhang\({}^{4}\) Jiarui Feng\({}^{5}\) Hong Cheng\({}^{3}\) Wei Chen\({}^{2}\) Yun Xiong\({}^{1}\)\({}^{}\) Dongsheng Li\({}^{2}\)

\({}^{1}\)Fudan University\({}^{}\) \({}^{2}\)Microsoft Research Asia \({}^{3}\)The Chinese University of Hong Kong

\({}^{4}\)Peking University \({}^{5}\)Washington University, Saint Louis

denotes equal contributions. Work was done during Xixi Wu's (xxwu@se.cuhk.edu.hk) internship at Microsoft Research Asia. Corresponding authors (yifeishen@microsoft.com, yunx@fudan.edu.cn) Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University

###### Abstract

Task planning in language agents is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests in natural language into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. The performance gain increases with a larger task graph size. 3

## 1 Introduction

LLM-based agents have recently emerged as a rapidly growing field of research and are considered a significant step towards artificial general intelligence (AGI) . These agents have achieved remarkable successes across a variety of domains, as evidenced by their ability to address complex AI challenges (e.g., HuggingGPT ), excel in gaming environments (e.g., Voyager ), and drive innovation in chemical research (e.g., ). Within this burgeoning field, task planning in language agents emerges as a critical area of study. It involves LLMs autonomously interpreting user instructions, breaking user's instructions in natural language into concrete and solvable sub-tasks, and then fulfilling the user's request by executing each sub-task . For instance, in the case of HuggingGPT , task planning involves invoking expert AI models from the HuggingFace website to solve complex AI tasks beyond the capabilities of GPT alone.

Given its practical significance, numerous algorithms have been proposed, with a major focus on prompt design . This paper proposes to explore an orthogonaldirection, i.e., graph-learning-based approaches. In task planning, solvable sub-tasks can be naturally represented as a _task graph_, wherein each node corresponds to a distinct sub-task, and each edge signifies the dependencies between these sub-tasks. The crux of task planning, therefore, involves selecting a connected path or subgraph to satisfy the user's request, which is a decision-making problem on graphs. Adopting this framework, we analyze the task planning capabilities of LLMs, specifically within the context of HuggingGPT . Our empirical investigation uncovers that a considerable portion of planning failures can be ascribed to the LLMs' inefficacy in accurately discerning the structure of the task graph. This finding presents intriguing questions from both theoretical and empirical perspectives. Theoretically, it initiates a discussion on the inherent limitations of LLMs in processing task graphs. Empirically, it highlights the urgent need for developing effective and efficient strategies to mitigate this deficiency and improve task planning performance.

For the theoretical question, we first investigate the expressiveness of Transformer architectures when applied to graph tasks with sequential graph input, such as edge list representations, which is the graph input format for task planning. Our initial hypothesis is that the format of sequential graph input might not align with the inductive bias inherent to graph structures, potentially reducing expressiveness. Contrary to this hypothesis, it is proved that by taking edge lists as the input, a constant-width Transformer can solve graph decision-making problems by simulating dynamic programming algorithms on edge lists. Nevertheless, we find that LLMs' solutions lack invariance under graph isomorphism, an important property for graph decision-making problems. In addition, the expressiveness is weakened if the attention is sparse, which is typically observed in LLMs . Beyond expressiveness, we also examine the influence of auto-regressive loss, demonstrating that it introduces spurious correlations that can be harmful to graph decision-making tasks. These insights expose the inherent limitations of LLMs in task planning and, more broadly, in graph-related problems (e.g., the challenges in [14; 59; 34]).

To tackle the limitations, we take the use of GNNs, which have been shown to adeptly handle graph decision-making problems, both in theory and in practice [24; 68]. Initially, we deploy LLMs to interpret an ambiguous user request, breaking it down into more detailed steps. Subsequently, we utilize a GNN to retrieve the relevant sub-tasks based on these detailed steps and the corresponding sub-task descriptions. Notably, this approach can be implemented without training if we adopt parameter-free GNN models such as SGC . In the case of training-based methods, we apply the Bayesian Personalized Ranking (BPR) loss  to facilitate learning from the implicit sub-task rankings. Extensive experiments demonstrate that the proposed methods achieve better performance than baselines. Specifically, our main contributions are summarized as follows:

1. **Task Planning Formulation:** This study presents a formulation of task planning as a graph decision-making problem. In the realm of task planning, our work initiates the exploration of graph learning methodologies to enhance performance. Concurrently, it introduces task planning as a new application in the graph learning domain.
2. **Theoretical Insights:** We prove that Transformers have expressiveness to solve graph decision-making problems _based on edge list input_, but inductive biases of attention and the auto-regressive loss function may serve as obstacles to their full potential.
3. **Novel Algorithms:** Based on the theoretical analysis, we introduce an additional GNN for sub-task retrieval, available in both training-free and training-based variants. The experiments on diverse LLMs and planning datasets demonstrate that the proposed method outperforms existing solutions with much less computation time. Furthermore, the performance is further enhanced by improved prompts or a fine-tuned model.

## 2 Preliminaries

In this section, we introduce task planning in language agents and the current LLM-based solutions.

### Task Planning in Language Agents

We start with the definition of task planning with a concrete example of HuggingGPT . In task planning, there is a pool of pre-defined tasks. Task planning inputs include this task pool and a user request. The user request is expressed in natural language, which is ambiguous and could encompass multiple complex tasks. The output is a sequence of tasks and the order of their invocation to address the user's request.

Figure 1 features the task planning in HuggingGPT, with the pre-defined tasks corresponding to APIs from the HuggingFace website, such as Translation and Pose-to-Image, accompanied by detailed descriptions. For instance, the detailed description for Translation is "Translation is the task of converting text from one language to another". The user requests is "Please generate an image where a girl is reading a book, and her pose matches the boy in 'example,jpg', then describe the new image with your voice." The ground-truth output is a sequence of four APIs (nodes): {Pose Detection, Pose-to-Image, Image-to-Text, Text-to-Speech}, outlining the order of their invocation (a path). By invoking these APIs on HuggingFace, the user request can be fulfilled.

### Current LLM-based Solution to Task Planning

The current solution of task planning is purely based on LLMs and involves two stages [45; 46]. The first stage involves request decomposition, where a user's ambiguous request is broken down into concrete _steps_ via LLMs. For instance, the request illustrated in Figure 1 is decomposed into the following steps: (1) analyze the pose of the boy; (2) take that pose and generate a new image; (3) generate the caption for newly generated image; (4) convert the generated text into audio. The second stage is task retrieval. For each decomposed step, LLMs are employed to retrieve an appropriate task from the task pool and execute them in sequence. For example, "Analyze the pose of the boy" corresponds to Pose detection. The output tasks should be (1) Pose detection; (2) Pose-to-Image; (3) Image-to-Text; (4) Text-to-Speech. Figure 6 illustrates this procedure.

## 3 Graph Formulation and Insights

### Graph Formulation of Task Planning

In this subsection, we formulate the task planning as a decision-making problem on the task graph. The task graph is a special kind of text-attributed graphs and we define it as \(G=(V,E,T)\). Each node \(v V\) represents a pre-defined task in the task pool, associated with a text \(t_{v} T\) describing its function (e.g., "Translation. Translation is the task of converting text from one language to another."). Each edge \((u,v) E\) indicates a dependency between tasks (e.g., the output format of task \(u\) matches the input format of task \(v\)). Task planning is to select a path or connected sub-graph on the task graph.

Viewed from this angle, task planning bears resemblance to traditional decision-making problems on graphs, such as planning for the shortest path. Compared with traditional planning, task planning in language agents involves diverse and open-ended goals due to the varied users' personal requests. For example, on platforms like HuggingFace, users' intentions span across video, text, and image domains. On the contrary, classic planning has a fixed goal for a given domain, which is often explicitly expressed by mathematical formulas [55; 36].

### Failures of LLMs in Planning: Empirical Findings

With the task graph at hand, we diagnose LLMs in task planning in Figure 2. We adopt the experimental settings as outlined in the work of HuggingGPT , where the prompts are specifically optimized for task planning on HuggingFace. The evaluation metric calculates the F1 score to assess the accuracy of the tasks identified by LLMs against the ground-truth tasks. Additionally, we report

Figure 1: **Illustration of Task Planning in Language Agents (e.g., HuggingGPT )**

two task-graph-related metrics: the node hallucination ratio and the edge hallucination ratio. These metrics measure the frequency of non-existent nodes (i.e., tasks) and edges (i.e., dependencies) outputted by LLMs, respectively, indicative of the models' misinterpretation of the graph input.

Our empirical findings reveal that (1) LLMs exhibit a certain **hallucination** ratio, and (2) there is a strong correlation between the hallucination ratio and planning performance. This suggests that LLMs struggle to accurately interpret the task graph while the task graph is the key to the performance.

We further explore whether the incidence of hallucinations correlates with the number of sub-tasks. The HuggingGPT dataset contains 23 sub-tasks, and our analysis is expanded to incorporate the UltraTool dataset , which consists of 260 sub-tasks. Figure 1(b) illustrates that the hallucination ratio increases with a larger task graph size.

### Failures of LLMs in Planning: Theoretical Insights

In this subsection, we provide theoretical insights into the limitations of LLMs in processing task graphs. In contrast to previous graph learning approaches for graph decision-making problems, LLMs process the graph input by flattening it into a sequence and are trained using an auto-regressive loss. We will then examine the impact of these two factors.

**How does sequential graph input impact the expressiveness?** We consider general graph decision-making problems that can be resolved using dynamic programming (DP) as described in (2). The input comprises the edge list and initial states:

\[\ v_{1}\ c[u_{1}][v_{1}]\ u_{1}\ v_{2}\ c[u_{1}][v_{2}]\ \ }_{}\ [u_{1}]\ \ u_{n}\ [u_{n}]}_{}\] (1)

The intended output format is \(u_{1}\ [k][u_{1}]\ \ u_{n}\ [k][u_{n}]\). In existing studies, task graphs are often presented in (1), where the edge list is described by natural language and the initial states are the task descriptions of each task node, detailed in Appendix A.9 of .

As discussed in the previous subsection, task planning is a decision-making problem on the task graph. The decision-making problems on graphs are often solved by DP  and its general formulation is given by

\[[k][i]=f(_{j(i)}g([k-1][j ],c[i][j])),\] (2)

where \([k][i]\) is the solution to state \(i\) in the \(k\)-th iteration, \(c[i][j]\) is a cost associated with state \(i\) and \(j\), \((i)\) is the set of state can be transited to \(i\), \(_{j(i)}\) is an aggregation function such as MAX or SUM, and \(f,g\) are task-specific update functions. We give the formulation of typical DP algorithms in Appendix D.1 including some NPC problems. For the decision-making problems on the text-attributed graphs (e.g., task planning), one may conceptualize them as DP in the feature space, as discussed in .

To our surprise, although the edge list input does not directly reflect the geometric structures of graphs, it enables Transformers to simulate DP efficiently, in terms of expressiveness, as demonstrated by the following theorem.

Figure 2: **Illustration of (a) LLMs’ planning performance and hallucination in HuggingGPT, and (b) hallucination in relation to task graph size.**

**Theorem 1**.: _(LLMs have enough expressiveness) Assume the input format is given in (1) and \(f,g,\) in DP update (2) satisfy the assumptions 1 and 2 in Appendix. There exists a log-precision constant-depth and constant-width Transformer that simulates one step of DP update in (2). As a consequence, there exists a log-precision \(O(k)\)-depth and constant-width Transformer that simulates \(k\) steps of DP update in (2)._

The proof is presented in Appendix D.2. However, certain aspects of the proof's constructions are challenging to be realized in Transformers that have been pretrained on natural language. First, the embedding process must be carefully filtered to ensure invariance under graph isomorphism. This invariance property does not align with the inductive biases inherent in natural language, making it difficult to achieve. Consequently, if an LLM can accurately produce the correct answer for a specific ordering of nodes, it might not maintain this accuracy after the nodes have been reordered (experiments given in Appendix D.3). Second, each token needs to synchronize its hidden states with all other tokens sharing the same token ID, which is of order \(O(|V|)\). In practice, the attention trained from natural language is typically sparse , leading to intractability issues. The formal lower bound is provided in the following proposition and the proof is given in Appendix D.4.

**Proposition 1**.: _(Inductive bias of language hinders expressiveness) Assume the input format is described (1) and that the attention mechanism is limited to attending to a constant number of tokens. There exists at least one instance of one-step DP update such that no log-precision constant-width constant-depth transformer can simulate._

How does auto-regressive Loss impact the generalization?Our investigation next focuses on the auto-regressive loss and considers the following scenario: given a fixed task graph, user data is collected to perform instruction tuning with next-token-prediction loss. For a tractable theoretical study, we conceptualize this issue as a path-planning problem, since the output of task planning is essentially a path. We consider the training dataset comprises input sequences of the form \(s\ t\ s\ v_{1}\ v_{2}\ \ t\), where \(s\) represents the source node, \(t\) the target node, and the sequence \(s\ v_{1}\ v_{2}\ \ t\) is a path that adheres to specified constraints. During testing, given the initial and target nodes \(s\) and \(t\), the model is expected to generate a path with the same constraint. Our findings indicate that auto-regressive loss can lead to the emergence of a frequency-based spurious correlation, as substantiated by the following theorem and the proof is given in Appendix D.5.

**Theorem 2**.: _(Spurious correlations of auto-regressive loss) Assume (1) the loss employed is a next-token-prediction loss utilizing cross-entropy, applied to the sub-sequence \(v_{1}\ v_{2}\ \ t\) during training; (2) the output logits are determined by target node \(t\) and the current node \(v_{i-1}\). Let \(N_{t,v_{i-1},u}\) be the number of times in the training dataset such that \(t\) is the target node, \(v_{i-1}\) is the current node and \(v_{i}=u\) is the next node. The optimal logits for predicting the next node \(u\) from current node \(v_{i-1}\) towards target node \(t\) is given by \(}_{i}[u]=,u}}{_{u}N_{t,v_{i-1},u}}\) if \(_{u}N_{t,v_{i-1},u}>0\). If \(_{u}N_{t,v_{i-1},u}=0\), \(}_{i}[u]\) can be any non-negative number subject to \(_{u}}_{i}[u]=1\)._

In our setup, \(s\ t\) is the instruction and the third token is a duplicate of the first token. It is reasonable to exclude these tokens in the loss calculation, which is the first assumption. The second assumption assumes that the output only depends on the current node and target node, which is a minimal requirement for path-related problems. For DP problems, the frequency-based prediction contradicts to the value-based ground-truth. We then give an example that auto-regressive loss even cannot find a valid path.

**Example 1**.: Consider a training dataset consisting of a sufficient number of valid paths. Suppose the dataset contains two paths \(a\ b\ c\) and \(b\ c\ d\) and there are no other paths such that \(t=d\) and the current node \(v_{i}=a\) for all \(i\). Then we have \(N_{d,a,u} 0\) for all \(u\) and the logits for the next node can be arbitrary. This results in the model's inability to predict the next node of \(a\) when given \(a\) as the source node and \(d\) as the target node.

To a human, finding a path from \(a\) to \(d\) simply involves concatenating the paths \(a\ b\ c\) and \(b\ c\ d\). However, auto-regressive loss fails under such circumstances. In task planning datasets, we indeed observe that the performance of fine-tuned LLMs is inferior to that of GNNs trained on the same dataset, as shown in Figure 2(b).

Integrating GNNs and LLMs for Planning

### Motivations

In the last section, we find that a considerable portion of planning failures can be ascribed to the LLMs' inefficacy in accurately discerning the structure of the task graph, due to the hallucination, the inductive bias of the attention, and next-token prediction loss. In contrast to LLMs, GNNs can strictly operate on the task graph, thereby avoiding hallucinations. Additionally, they leverage the graph structure as input, rather than flattening the graph into a sequence, thus overcoming the theoretical limitations discussed previously. Furthermore, GNNs have demonstrated proficiency in handling graph decision-making problems, both theoretically and empirically [68; 11; 24]. As a result, the simplest fix is to integrate GNNs into the task-planning algorithm.

In the following subsections, we propose both training-free and training-based approaches to enhance performance. Training-free methods are necessary when the available tasks are continuously changing, or new tasks are emerging constantly. This scenario is common when the task planning module is deployed in a new system. Once the task planning module has been deployed for a period, it becomes possible to collect users' requests and label a small proportion of the data, enabling lightweight training-based methods.

### A Training-free GNN-based Approach

As we discussed in Section 2.2, the current solution to task planning involves two stages. The first stage requires the ability to understand users' requests in natural language and break them down into concrete instructions, which is the unique ability of LLMs. The second stage is to select a path on the task graph, where each node corresponds to a decomposed step. Thus, we can integrate GNNs in this stage. The illustration of our framework is shown in **Figure 7 in Appendix**.

For each decomposed step outputted by the first stage, we use a GNN to select a corresponding node within the task graph. Suppose we are selecting the node for the \(i\)-th decomposed step. First, we utilize a small pre-trained language model, e5-335M , to embed the \(i\)-th decomposed step. The resulting embedding is denoted as \(_{i}^{}\). Second, for the task graph, we first use the same pre-trained language model e5-335M to convert each node's description into embeddings, denoted as the node feature \(_{v}^{0}\), where the superscript indicates the layer and the subscript represents the node. Then we adopt a \(K\)-layer SGC  to compute the final node embeddings, resulting in \(_{v}=_{v}^{(K)}\). Given a sequence of previously selected task nodes \(\{v_{1},,v_{i-1}\}\), the next node \(v_{i}\) is chosen according to \(v_{i}=_{v(v_{i-1})}\ _{v},_{i}^{}\), where \(_{v}\) is the final node embedding, and \((v_{i-1})\) denotes the neighbors of node \(v_{i-1}\) in the task graph. Particularly, \(v_{1}\) can be selected from the whole graph.

As e5-335M is pre-trained and SGC is parameter-free, the proposed method requires no additional training and can be effectively applied in a zero-shot manner.

### A Training-required GNN-based Approach

The inference process in training-required methods mirrors that of the training-free approach, with the difference being the substitution of parameter-free GNNs with parametric counterparts, such as GAT  or GraphSAGE . Here we specify the training process of GNNs.

**Data Preparation:** We assume that each entry in the task planning dataset comprises a user request, a sequence of decomposed steps, and the corresponding ground-truth tasks, denoted as \((,\{s_{1},,s_{n}\},\{v_{1},,v_{n}\})\). If the dataset does not adhere to this format, we reformat it accordingly using GPT-4, with details provided in Appendix C.2. It is important to note that there is a one-to-one correspondence between the steps and tasks in the dataset. Therefore, the training dataset can be represented as \(\{(s_{i},v_{i})\}_{i=1}^{n}\), where \(s_{i}\) is a step described in natural language, and \(v_{i}\) is its corresponding invoked task.

**Training Loss:** The problem in the dataset can be viewed as a binary ranking problem, where the labeled node is \(1\) and the other nodes are \(0\). Therefore, we adopt the Bayesian Personalized Ranking (BPR) loss  designed for recommendation with binary rankings. The loss function is given by \(=_{(^{},v,v^{})}-\ (_{v},^{}-_{v^{}},^{})\), where \(^{}\) represents the embedding of the step's textual description generated by e5-335M, \(v\) is the ground-truth task, and \(v^{}\) is a negative task.

We select negative tasks that are textually similar to the positive task, and for computational efficiency, we limit our selection to \(2\) negative tasks per positive task. The trainable parameters may merely include GNNs or both GNNs and e5-335M with illustrative configurations shown in **Figure 8 in Appendix**.

## 5 Experiments and Analysis

### Experimental Setup

**Datasets:** We utilize four datasets across two task planning benchmarks: HuggingFace tasks, Multimedia tasks, and Daily Life API tasks from **TaskBench**, as well as TMDB API tasks from **RestBench**. The HuggingFace dataset includes AI models on the HuggingFace. The Multimedia dataset provides a wide range of user-centric tasks, such as file downloading and video editing. The Daily Life APIs cater to everyday services like web search and shopping functionalities. TMDB focuses on movie-related search and retrieval tasks. Statistics for each dataset are presented in Table 7 with illustrative examples shown in Figure 4 in Appendix. Other benchmarks, such as ToolBench  and ToolAlpaca , are less suitable for our experiments due to (1) the absence of a well-defined task graph detailing tasks and their dependencies, and (2) a scarcity of samples involving multi-task planning, with a focus on single-task retrieval.

**Evaluation:** For the datasets from TaskBench, we split \(3000\) samples for training and \(500\) samples for testing, each containing an invocation path with at least two tasks. For the TMDB dataset, we first filter to include the samples with two or more invoked tasks, and then randomly select a sample served as the in-context learning example. The remaining \(94\) samples are designated for testing. We adopt the evaluation metric in TaskBench  and HuggingGPT , i.e., Node F1-Score (_n-F1_) and Link F1-Score (_l-F1_), which measure the accuracy of invoked tasks and invoked dependencies, respectively. Besides, the Accuracy (_Acc_) can measure the success rate from task level. We also measure the token consumption (_# tok_) as the efficiency metric.

**Choices of LLMs:** We consider close-sourced LLMs, i.e., GPT-3.5-turbo and GPT-4-turbo, as well as open-sourced LLMs with different parameter scales, including CodeLlama-13B(or 7B)-Instruct-hf , Mistral-7B-Instruct-v0.2 , Vicuna-13B-v1.5 , and Baichuan2-13B-Chat .

**Choices of GNNs:** To comprehensively investigate the effectiveness of different graph learning methods for task planning, we consider a wide range of graph neural networks, including SGC , GCN , GAT , GraphSAGE , GIN , and Graph Transformers .

### Performance of the Training-free Approach

We compare the performance across three training-free methods: **(1) LLM's Direct Inference** is introduced in Section 2.2. **(2) GraphSearch**[33; 52; 32] leverages the classic graph search method to generate the candidate nodes and uses LLMs to give a score for node selection. Given a step, **GreedySearch** consistently selects the node with the highest score and adjacent to the previous task node; **AdaptiveSearch** selects the nodes with scores above a fixed threshold, adjusting the breadth of the search space in an adaptive mode; **BeamSearch** retains the \(k\) nodes with highest scores. **(3) SGC** employs a training-free SGC for task retrieval based on decomposed task steps. The details of baselines are given in Section E.1 and illustrated in Figure 6. Table 1 shows both the overall performance and token consumption costs, with results of Accuracy (_Acc_) moved to Table 9 in Appendix.

Compared with direct inference, integrating an SGC consistently improves performance, underscoring the effectiveness of the proposed method. GraphSearch-type methods rely on beam search to identify paths and employ LLMs for evaluation, where longer processing times generally lead to better outcomes. Notably, our proposed method achieves **comparable or superior performance** (Table 1 and Table 9) to BeamSearch while requiring \(5\)-\(10\) **times fewer tokens** (Table 1) and inference time (Table 10). The case studies are provided in Appendix H. However, we observed only marginal improvements with GPT-4-turbo. A unique feature of GPT-4-turbo is its ability to manage ChatGPT-plugins, and it may have been specially trained on task planning datasets. In addition, the pre-trained language model used for feature extraction in SGC is e5-335M, which may not be sufficiently powerful to effectively analyze GPT-4's output. A detailed diagnostic analysis of cases involving GPT-4 is provided in Figure 11 in Appendix.

### Performance of the Training-based Approaches

**Settings:** We further explore the efficacy of training-based GNNs in three TaskBench datasets. The TMDB dataset is excluded due to its limited sample size. Throughout our experiments, we trained a spectrum of GNN variants, both with and without co-training the small LM (i.e., e5-335M), whose role is to generate node embeddings derived from task names and descriptions. Owing to space constraints, we only show the performance of GraphSAGE in the main text, relegating a detailed comparison of all the situations to Table 11 and Table 12 in Appendix.

**Compared Methods:** Due to the lack of training-based baseline methods specifically for task planning, we adapt two existing approaches that combine LLMs and GNNs for graph-related tasks, including: (1) **TAPE** employs a LLM \(\) LM \(\) GNN architecture for node classification task. In this framework, LLMs generate high-quality explanatory text for each predicted node, which is then fine-tuned by an LM to produce node embeddings. Finally, a GNN performs the downstream classification. We adapt TAPE for task planning by reformulating the problem as classifying user requests into corresponding node labels within the task graph. (2) **GraphToken** uses GNNs to tokenize graph nodes, which are then fed into LLMs to generate textual outputs. In our adaptation for task planning, we treat the user request as the input question and the expected plan as the generated answer. Additional implementation details are provided in the Appendix F.2.

**Observations:** From Table 2, we observe a significant improvement in performance when employing a training-based GraphSAGE approach over the training-free method. However, the co-training of GNNs with e5-335M does not yield a marked improvement, suggesting that message passing is the crucial element for enhancing performance. Further analysis across a broad spectrum of GNNs (as shown in Table 11 and Table 12) reveals that powerful GNNs, such as GNNs, perform similarly to networks perceived as less complex, like GCNs, and even underperform compared to GraphSAGE. This pattern indicates that the task's challenge may not lie in the expressiveness of the models but rather in their ability to generalize. Regarding baselines, TAPE is unsuitable for task planning as its classification approach simplifies task planning, overlooking task dependencies. While GraphToken demonstrates superior performance over LLMs' direct inference, we have noted instances of minor hallucination. This observation suggests that GraphToken's understanding of the task graph is not yet perfect. In addition, GraphToken is limited to open-sourced LLMs. Besides, our proposed approaches also greatly boost the parameter prediction performance, as given in Appendix G (e.g. \(9\%\) improvement to GPT-3.5-turbo and \(3\%\) improvement to GPT-4-turbo).

**Efficiency:** The details of the training time are given in Table 13. The training cost is remarkably low because we use e5-335M  as the text embedding model for GNNs. If the trainable parameters are limited to the GNNs alone, training typically concludes within just 3 minutes. Furthermore, the

    &  &  & } &  \\  & & _n-F1_ & _F1_ & _F2_ & _F4_ & _k-1_ & _n-F1_ & _F1_ & _F2_ & _F4_ & _k-1_ & _n-F1_ & _n-F1_ & _n-F1_ & _n-F1_ & _n-F1_ & _n-F1_ \\   & Direct & 50.46 & 21.27 & 2.50 & 53.57 & 23.19 & 2.64 & 73.70 & 45.80 & 3.82 & 44.66 & 14.01 & 2.02 \\  & GreedySearch & 52.94 & 25.73 & 6.23 & 46.99 & 23.11 & 5.55 & 42.98 & 13.33 & 7.18 & 45.22 & 13.69 & 3.42 \\  & AdaptiveSearch & 54.36 & 25.67 & 8.91 & 51.24 & 24.32 & 11.25 & 62.71 & 31.15 & 13.92 & 41.32 & 7.02 & 6.51 \\  & BeamSearch & 56.46 & 26.93 & 24.11 & 54.09 & 26.19 & 25.42 & 54.55 & 23.02 & 24.86 & 46.61 & 15.41 & 7.79 \\   & **SGC** & **90.92** & **31.98** & **23.11** & **61.78** & **30.20** & **24.33** & **83.33** & **63.77** & **38.23** & **48.79** & **15.99** & **1.89** \\  & Direct & 60.60 & 30.23 & 2.49 & 69.83 & 39.85 & 2.64 & 84.26 & 53.63 & 3.77 & 62.23 & 22.02 & 1.96 \\  & GreedySearch & 65.91 & 38.13 & 6.52 & 58.92 & 34.72 & 6.26 & 75.18 & 49.47 & 8.27 & 60.64 & 23.18 & 4.38 \\  & AdaptiveSearch & 67.30 & 38.90 & 6.78 & 11.59 & 44.84 & 10.66 & 86.39 & 6.65 & 10.92 & 54.04 & 21.35 & 9.99 \\  & BeamSearch & 67.13 & 36.73 & 25.66 & 73.55 & 47.12 & 31.10 & 85.87 & 61.53 & 39.16 & 63.34 & **26.79** & 11.26 \\  & **SGC** & **67.43** & **43.02** & **23.27** & **40.79** & **49.20** & **43.73** & **81.73** & **66.30** & **35.64** & **64.72** & 25.67 & **1.89** \\   & Direct & 57.55 & 28.88 & 2.45 & 68.87 & 41.79 & 2.59 & 91.20 & 76.07 & 3.88 & 68.91 & 43.74 & 2.02 \\  & GreedySearch & 61.67 & 34.02 & 5.95 & 67.98 & 42.04 & 4.95 & 91.50 & 76.56 & 5.54 & 66.67 & 42.16 & 3.81 \\  & AdaptiveSearch & 60.85 & 31.66 & 11.10 & 68.14 & 41.71 & 67.91 & 30.46 & 70.98 & 71.63 & 63.74 & 37.17 & 8.16 \\  & BeamSearch & 62.65 & 34.31 & 20.14 & 69.93 & 43.35 & 19.51 & 91.74 & 76.66 & 19.19 & 68.08 & 42.92 & 8.88 \\  & **SGC** & **65.81** & **39.44** & **23.13** & **73.20** & **53.28** & **42.43** & **92.96** & **79.57** & **36.44** & **71.40** & **47.58** & **1.99** \\   & Direct & 73.85 & 45.73 & 2.41 & 82.85 & 62.07 & 2.26 & 96.09 & 83.65 & 3.66 & 31.70 & 57.52 & 1.67 \\  & GreedySearch & 67.75 & 43.88 & 5.29 & 81.11 & 63.02 & 4.92 & 93.77 & 81.26 & 7.36 & 76.19 & 50.11 & 3.06 \\  & AdaptiveSearch & 72.18 & 47.55 & 7.47 & 81.86 & 62.71 & 5.71 & 5.13 & 93.78 & 81.41 & 8.53 & 77.57 & 53.65 & 5.89 \\  & BeamSearch & 75.51 & 49.62 & 14.22 & 83.57 & **64.50** & 1.291 & 95.66 & 82.72 & 20.58 & 81.24 & 57.98 & 6.42 \\  & **SGC** & **76.37** & **50.04** & **2.02** & **83.65** & 63.65 & **2.09** & **96.38** & **86.19** & **3.16** & **82.63** & **59.15** & **1.61** \\   & Direct & 77.60 & 52.18 & 2.19 & 88.29 & 69.38 & 2.28 & **97.36** & **84.58** & 3.37 & **82.56** & **56.67** & 1.75 \\  & GreedySearch & 74.75 & 50.44 & 7.88 & 86.81 & 69.80 & 5.52 & 97.36 & 85.78 & 7.37 & 75.34 & 49.95 & 3.73 \\   & AdaptiveSearch & 76.17 & 51.30 & 8.94 & 88.02 & 69.99 & 7.14 & 97.30 & **85.89** & 9.04 & 81.78 & 55.15 & 6.35 \\   & BeamSearch & 77.76 & **52.54** & 8.98 & 81.6 & **79.39** & 69training duration extends to only 15 minutes when GNNs are jointly trained with e5-335M model. This efficiency stands in stark contrast to the 10-20 hours required for tuning open-sourced LLMs.

### Scaling to Large Task Graphs

To demonstrate the scalability of our method to larger task graphs, we conducted a supplementary experiment on the newly released planning benchmark **UltraTool**, which features a relatively large task graph with \(260\) nodes. Details on processing this dataset are provided in Appendix C.3. We present a performance comparison of GNN models (training-free SGC and training-required GraphSAGE) against strong baselines like BeamSearch in Table 3. Among the metrics, accuracy (_Acc_) is calculated based on whether the predicted tasks match the ground-truth tasks, measuring the success rate at each case level. In such conditions, integrating a GNN significantly enhances performance and mitigates planning failures, e.g., GPT-4-turbo undergoes a **9.05% accuracy improvement** with the introduction of GraphSAGE.

    &  &  &  &  \\  & & _n-F1_\(\) & _l-F1_\(\) & _Acc_\(\) & _n-F1_\(\) & _l-F1_\(\) & _Acc_\(\) & _n-F1_\(\) & _l-F1_\(\) & _Acc_\(\) \\   & Direct & 50.46 & 21.27 & 8.72 & 53.57 & 23.19 & 11.20 & 73.70 & 45.80 & 24.43 \\  & TAPE & 59.47 & NA & 5.07 & 54.97 & NA & 2.07 & 73.26 & NA & 12.50 \\  & GraphToken & **63.37** & 31.54 & 15.61 & 65.40 & 36.38 & 19.87 & 81.65 & 48.29 & 46.37 \\  & GraphSAGE & 61.86 & 35.68 & **20.08** & 63.71 & 39.88 & 21.37 & **86.07** & **67.63** & **48.64** \\  & GraphSAGE\({}_{}\) & 62.82 & **37.04** & 19.68 & **65.89** & **42.18** & **21.58** & 84.23 & 65.44 & 47.81 \\   & Direct & 60.60 & 30.23 & 16.36 & 69.83 & 39.85 & 25.05 & 84.26 & 53.63 & 44.52 \\  & TAPE & 61.82 & NA & 6.13 & 58.92 & NA & 3.29 & 76.40 & NA & 16.44 \\  & GraphToken & 64.42 & 32.04 & 18.60 & 72.31 & 42.60 & 30.31 & 86.82 & 57.06 & **53.99** \\  & GraphSAGE & **68.12** & 43.09 & 25.77 & 75.51 & 52.94 & **34.29** & 87.51 & 66.57 & 52.74 \\  & GraphSAGE\({}_{}\) & 67.61 & **43.14** & **27.20** & **76.96** & **55.46** & 33.26 & **87.61** & **66.75** & 52.97 \\   & Direct & 57.55 & 28.88 & 14.29 & 68.57 & 41.79 & 24.10 & 91.20 & 76.07 & 66.40 \\  & TAPE & 64.03 & NA & 8.05 & 58.27 & NA & 2.01 & 77.74 & NA & 17.37 \\  & GraphToken & 62.15 & 32.55 & 20.08 & 74.57 & 47.60 & 35.06 & 92.50 & 73.57 & 69.42 \\  & GraphSAGE & 67.30 & 42.41 & 26.56 & 74.93 & 54.52 & **38.55** & **93.84** & **80.38** & 73.60 \\  & GraphSAGE\({}_{}\) & **68.92** & **44.85** & **29.58** & **76.28** & **55.41** & 37.75 & 93.30 & 79.51 & **74.00** \\   & Direct & 73.85 & 45.73 & 28.95 & 82.85 & 62.07 & 47.96 & 96.09 & 83.65 & 81.30 \\  & TAPE & 68.00 & NA & 8.83 & 62.43 & NA & 3.87 & 70.67 & NA & 8.92 \\   & GraphSAGE & **77.90** & 52.68 & 35.11 & 85.29 & 65.80 & 53.55 & **96.43** & **86.26** & **83.13** \\   & GraphSAGE\({}_{}\) & 77.87 & **53.04** & **35.32** & **85.51** & **66.56** & **55.91** & 96.34 & 86.09 & 83.13 \\   & Direct & 77.60 & 52.18 & 33.68 & 88.29 & 69.38 & 60.56 & 97.36 & 84.58 & 86.77 \\   & TAPE & 68.82 & NA & 9.50 & 63.94 & NA & 4.02 & 71.51 & NA & 9.40 \\   & GraphSAGE & **78.76** & 52.53 & **34.09** & 88.63 & 69.65 & 60.36 & 97.34 & 85.67 & **86.97** \\   & GraphSAGE\({}_{}\) & 78.49 & **52.62** & 33.88 & **88.86** & **70.25** & **62.37** & **97.42** & **85.80** & 86.57 \\   

Table 2: **Comparison with Training-based Approaches: Node-F1, Link-F1, and Accuracy are reported in \(\%\). TAPE  is designed for node classification task and cannot predict links, so we report Link-F1 as “NA”. GraphToken  requires finetuning LLMs, which is not compatible with close-sourced LLMs. The performance of other GNNs and LLMs are given in Table 11 and Table 12 in the Appendix.**

    &  &  &  \\  & & _n-F1_\(\) & _l-F1_\(\) & _Acc_\(\) & _n-F1_\(\) & _l-F1_\(\) & _Acc_\(\) & _\# Tok_\(\) \\   & Direct & 38.88 & 16.42 & 13.58 & 10,535 & 57.64 & 30.44 & 26.25 & 10,737 \\  & BeamSearch & 49.71 & 22.51 & 17.08 & 26,008 & 64.93 & 36.23 & 33.47 & 23,023 \\  & SGC & 61.07 & 37.61 & 25.31 & 10,456 & 71.64 & 44.00 & 39.68 & 10,658 \\   & GraphSAGE & **63.78** & **39.91** & **27.98** & **10.456** & **72.81** & **45.26** & **43.49** & **10.658** \\    & Direct & 54.35 & 21.35 & 18.33 & 8,462 & 63.58 & 30.85 & 25.00 & 8,614 \\    & BeamSearch & 55.40 & 28.02 & 19.76 & 21,979 & 63.41 & 34.05 & 26.28 & 20,813 \\    & SGC & 59.80 & 37.82 & 25.87 & 8,352 & 64.96 & 37.96 & 29.70 & 8,504 \\    & GraphSAGE & **63.97** & **42.26** & **30.35** & **8.352** & **70.

The results indicate that (1) LLMs' performance is vulnerable to the scale of task graphs; (2) The performance gain of the proposed method increases with a larger task graph.

### Improved Prompts and Fine-tuned LLMs

In this subsection, we show that the proposed method is orthogonal to two dominant methods, i.e., prompt engineering and fine-tuning.

**Orthogonal to Improved Prompts:** We investigate GNN's effectiveness when applied to improved prompt templates, i.e., strategically designed prompts that enhance the task planning abilities of LLMs. Specifically, we consider two types of prompts: **(1) In-context Learning with Increased Examples** During main experiments, we maintain the consistent 1-shot in-context learning example for LLM's direct inference. To realize further improvements, we increase the number of examples to \(2\), and results under this setting are denoted as "2-shot Prompt"; **(2) Plan like a Graph (PlaG)** We adopt the prompt in  to encourage LLM to think and plan in a graph-like manner. Specifically, we convert the entire task graph into plain text and then integrate PlaG instructions to enhance LLM's planning. Results under this prompt template are denoted as "PlaG Prompt".

From the results shown in Figure 2(a), where we apply three different prompts to CodeLlama-13B and Mistral-7B on HuggingFace, it is clear that applying GraphSAGE to improved prompts, where task steps are more concisely decomposed and predictions are more accurate, can also boost performance.

**Orthogonal to LLMs' Fine-tuning:** To explore whether our framework maintains effectiveness on fine-tuned LLMs, which have acquired dataset-specific task planning capabilities, we conduct further experiments. For each dataset, we use LoRA  to fine-tune two LLMs of different parameter scales, including CodeLlama-7B and Vicuna-13B, based on the same training data as GNNs. Details of fine-tuning process are provided in Appendix F.3. The finetuned model is named as "FT-CodeLlama" and "FT-Vicuna" in Figure 2(b).

The results depicted in Figure 2(b) demonstrate that fine-tuning markedly enhances the task-planning capabilities of LLMs. Furthermore, applying GraphSAGE to the decomposed tasks of LLMs further improves the accuracy of task planning.

## 6 Conclusions

This paper presents an initial exploration into graph-learning-based approaches for task planning in language agents. Through theoretical analysis, we demonstrate the inductive bias of the attention mechanism and the utility of auto-regressive loss impedes their effectiveness in task planning. We propose to integrate GNNs for task graph analysis, which yields performance improvements across a range of LLMs and planning benchmarks.

**Limitations:** Despite the encouraging performance, there are limitations that highlight significant opportunities for enhancement. Firstly, our proposed method, while effective, is straightforward; more sophisticated graph-learning-based decision-making algorithms could potentially offer further improvements. Secondly, the construction of the task graph currently requires manual effort. Investigating automated graph generation techniques for diverse applications is another promising direction for future work.

Figure 3: **Orthogonal Effectiveness to both Improved Prompts and Fine-tuned LLMs**