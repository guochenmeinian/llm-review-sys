# Learning Energy-Based Prior Model with Diffusion-Amortized MCMC

Peiyu Yu\({}^{1}\)

yupeiyu98@g.ucla.edu

&Yaxuan Zhu\({}^{1}\)

yaxuanzhu@g.ucla.edu

&Sirui Xie\({}^{2}\)

srxie@ucla.edu

&Xiaojian Ma\({}^{2,4}\)

xiaojian.ma@ucla.edu

&Ruiqi Gao\({}^{3}\)

ruiqig@google.com

&Song-Chun Zhu\({}^{4}\)

sczhu@stat.ucla.edu

&Ying Nian Wu\({}^{1}\)

ywu@stat.ucla.edu

\({}^{1}\)UCLA Department of Statistics \({}^{2}\)UCLA Department of Computer Science

\({}^{3}\)Google DeepMind \({}^{4}\)Beijing Institute for General Artificial Intelligence (BIGAI)

###### Abstract

Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in the field of generative modeling due to its flexibility in the formulation and strong modeling power of the latent space. However, the common practice of learning latent space EBMs with non-convergent short-run MCMC for prior and posterior sampling is hindering the model from further progress; the degenerate MCMC sampling quality in practice often leads to degraded generation quality and instability in training, especially with highly multi-modal and/or high-dimensional target distributions. To remedy this sampling issue, in this paper we introduce a simple but effective diffusion-based amortization method for long-run MCMC sampling and develop a novel learning algorithm for the latent space EBM based on it. We provide theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler. Experiments on several image modeling benchmark datasets demonstrate the superior performance of our method compared with strong counterparts1.

## 1 Introduction

Generative modeling of data distributions has achieved impressive progress with the fast development of deep generative models in recent years [1; 2; 3; 4; 5; 6; 7; 8; 9]. It provides a powerful framework that allows successful applications in synthesizing data of different modalities [10; 11; 12; 13; 14; 15], extracting semantically meaningful data representation [16; 17; 18] as well as other important domains of unsupervised or semi-supervised learning [19; 20; 21]. A fundamental and powerful branch of generative modeling is the Deep Latent Variable Model (DLVM). Typically, DLVM assumes that the observation (_e.g._, a piece of text or images) is generated by its corresponding low-dimensional latent variables via a top-down generator network [1; 2; 3]. The latent variables are often assumed to follow a non-informative prior distribution, such as a uniform or isotropic Gaussian distribution. While one can directly learn a deep top-down generator network to faithfully map the non-informative prior distribution to the data distribution, learning an informative prior model in the latent space could further improve the expressive power of the DLVM with significantly less parameters . In this paper, we specifically consider learning an EBM in the latent space as an informative prior for the model.

Learning energy-based prior can be challenging, as it typically requires computationally expensive Markov Chain Monte Carlo (MCMC) sampling to estimate learning gradients. The difficulty of MCMC-based sampling is non-negligible when the target distribution is highly multi-modal or high-dimensional. In these situations, MCMC sampling can take a long time to converge and perform poorly on traversing modes with limited iterations . Consequently, training models with samples from non-convergent short-run MCMC , which is a common choice for learning latent space EBMs , often results in indiffender energy landscapes [15; 24; 25] and biased estimation of the model parameter. One possible solution is to follow the variational learning scheme , which however requires non-trivial extra efforts on model design to deal with problems like posterior collapse [26; 27; 28] and limited expressivity induced by model assumptions [1; 29; 30].

To remedy this sampling issue and further unleash the expressive power of the prior model, we propose to shift attention to an economical compromise between unrealistically expensive long-run MCMC and biased short-run MCMC: _we consider learning valid amortization of the potentially long-run MCMC for learning energy-based priors_. Specifically, inspired by the connection between MCMC sampling and denoising diffusion process [7; 8; 31], in this paper we propose a diffusion-based amortization method suitable for long-run MCMC sampling in learning latent space EBMs. The learning algorithm derived from it breaks the long-run chain into consecutive affordable short-run segments that can be iteratively distilled by a diffusion-based sampler. The core idea is simple and can be summarized by a one-liner (Fig. 1). We provide theoretical and empirical evidence that the resulting sampler approximates the long-run chain (see the proof-of-concept toy examples in Appendix E.1), and brings significant performance improvement for learning latent space EBMs on several tasks. We believe that this proposal is a notable attempt to address the learning issues of energy-based priors and is new to the best of our knowledge. We kindly refer to Section 5 for a comprehensive discussion of the related work.

Contributionsi) We propose a diffusion-based amortization method for MCMC sampling and develop a novel learning algorithm for the latent space EBM. ii) We provide a theoretical understanding that the learned amortization of MCMC is a valid long-run MCMC sampler. iii) Our experiments demonstrate empirically that the proposed method brings higher sampling quality, a better-learned model and stronger performance on several image modeling benchmark datasets.

## 2 Background

### Energy-Based Prior Model

We assume that for the observed sample \(^{D}\), there exists \(^{d}\) as its unobserved latent variable vector. The complete-data distribution is

\[p_{}(,):=p_{}()p_{}(| ),\;p_{}():=}}(f_{}())p_{0}(),\] (1)

where \(p_{}()\) is the prior model with parameters \(\), \(p_{}(|)\) is the top-down generation model with parameters \(\), and \(=(,)\). The prior model \(p_{}()\) can be formulated as an energy-based model, which we refer to as the Latent-space Energy-Based Model (LEBM)  throughout the paper. In this formulation, \(f_{}()\) is parameterized by a neural network with scalar output, \(Z_{}\) is the partition function, and \(p_{0}()\) is standard normal as a reference distribution. The prior model in Eq. (1) can be interpreted as an energy-based correction or exponential tilting of the original prior distribution \(p_{0}\). The generation model follows \(p_{}(|)=(g_{}(),^{2} _{D})\), where \(g_{}\) is the generator network and \(^{2}\) takes a pre-specified value as in VAE . This is equivalent to using \(l_{2}\) error for reconstruction.

The parameters of LEBM and the generation model can be learned by Maximum Likelihood Estimation (MLE) . To be specific, given the training data \(\), the gradients for updating \(,\) are,

\[_{}():=_{p_{}(|)} [_{}f_{}()]-_{p_{}()}[_{}f_{}()],\; _{}():=_{p_{}(|)} [_{} p_{}(|)].\] (2)

In practice, one may use the Monte-Carlo average to estimate the expectations in Eq. (2). This involves sampling from the prior \(p_{}()\) and the posterior \(p_{}(|)\) distribution using MCMC, specifically Langevin Dynamics (LD) , to estimate the expectations and hence the gradient. For a target distribution \(()\), the dynamics iterates

\[_{t+1}=_{t}+}{2}_{_{t}}(_{t})+s _{t},\;t=0,1,...,T-1,\;_{t}(,_{d}),\] (3)

where \(s\) is a small step size. One can draw \(_{0}(,_{d})\) to initialize the chain. For sufficiently small step size \(s\), the distribution of \(_{t}\) will converge to \(\) as \(t\). However, it is prohibitively expensive to run LD until convergence in most cases, for which we may resort to limited iterations of LD for sampling in practice. This non-convergent short chain yields a moment-matching distribution close to the true \(()\) but is often biased, which was dubbed as short-run LD [15; 23; 24; 25].

### Denoising Diffusion Probabilistic Model

Closely related to EBMs are the Denoising Diffusion Probabilistic Models (DDPMs) [5; 7; 8]. As pointed out in [5; 8], the sampling procedure of DDPM with \(\)-prediction parametrization resembles LD; \(\) (predicted noise) plays a similar role to the gradient of the log density .

In the formulation proposed by Kingma et al. , the DDPM parameterized by \(\) is specified by a noise schedule built upon \(_{s}=[_{s}^{2}/_{s}^{2}]\), _i.e._, the log signal-to-noise-ratio, that decreases monotonically with \(s\). \(_{s}\) and \(_{s}^{2}\) are strictly positive scalar-valued functions of \(s\). We use \(_{0}\) to denote training data in \(^{d}\). The forward-time diffusion process \(q(|_{0})\) is defined as:

\[q(_{s}|_{0})=(_{s};_{s}_{0},_{s} ^{2}_{d}), q(_{s}^{}|_{s})=(_{s}^{};(_{s}/_{s})_{s},_{s^{}|s}^{2} _{d}),\] (4)

where \(0 s<s^{} S\) and \(_{s^{}|s}^{2}=(1-e^{_{s^{}}-_{s}})_{s} ^{2}\). Noticing that the forward process can be reverted as \(q(_{s}|_{s^{}},_{0})=(_{s};}_{s|s^{}}(_{s^{}},_{0}),_{s^{ }|s}^{2}_{d})\), an ancestral sampler \(q_{}(_{s}|_{s^{}})\) that starts at \(_{S}(,_{d})\) can be derived accordingly :

\[}_{s|s^{}}(_{s^{}},_{0}) =e^{_{s^{}}-_{s}}(_{s}/_{s^{ }})_{s^{}}+(1-e^{_{s^{}}-_{s}})_ {s}_{0},_{s|s^{}}^{2}=(1-e^{_{s^{ }}-_{s}})_{s}^{2},\] (5) \[_{s} =}_{s|s^{}}(_{s^{}},} _{0})+_{s|s^{}}^{2})^{1-}(_{s^{ }|s^{}}^{2})^{})},\]

where \(\) is standard Gaussian noise, \(}_{0}\) is the prediction of \(_{0}\) by the DDPM \(\), and \(\) is a hyperparameter that controls the noise magnitude, following . The goal of DDPM is to recover the distribution of \(_{0}\) from the given Gaussian noise distribution. It can be trained by optimizing \(_{,}[\|(_{}) -\|_{2}^{2}],\) where \((,_{d})\) and \(\) is drawn from a distribution of log noise-to-signal ratio \(p()\) over uniformly sampled times \(s[0,S]\). This loss can be justified as a lower bound on the data log-likelihood [8; 33] or as a variant of denoising score matching [7; 35]. We will exploit in this paper the connection between DDPMs and LD sampling of EBMs, based upon which we achieve better sampling performance for LEBM compared with short-run LD.

## 3 Method

In this section, we introduce the diffusion-based amortization method for long-run MCMC sampling in learning LEBM in Section 3.1. The learning algorithm of LEBM based on the proposed method and details about implementation are then presented in Section 3.2 and Section 3.3, respectively.

Figure 1: **Learning the DAMC sampler.** The training samples for updating the sampler to \(_{k+1}\) is obtained by \(T\)-step short-run LD, initialized with the samples from the current learned sampler \(_{k}\). Best viewed in color.

### Amortizing MCMC with DDPM

Amortized MCMCUsing the same notation as in Section 2, we denote the starting distribution of LD as \(_{0}()\), the distribution after \(t\)-th iteration as \(_{t}()\) and the target distribution as \(()\). The trajectory of LD in Eq. (3) is typically specified by its transition kernel \((|^{})\). The process starts with drawing \(_{0}\) from \(_{0}()\) and iteratively sample \(_{t}\) at the \(t\)-th iteration from the transition kernel conditioned on \(_{t-1}\), _i.e._, \(_{t}()=_{t-1}()\), where \(_{t-1}():=(|^{})_{t-1 }(^{})d^{}\). Recursively, \(_{t}=_{t}_{0}\), where \(_{t}\) denotes the \(t\)-step transition kernel. LD can therefore be viewed as approximating a fixed point update in a non-parametric fashion since the target distribution \(\) is a stationary distribution \(():=(|^{})(^{})d^{},\). This motivates several works for more general approximations of this update [36; 37; 38] with the help of neural samplers.

Inspired by , we propose to use the following framework for amortizing the LD in learning the LEBM. Formally, let \(=\{q_{}\}\) be the set of amortized samplers parameterized by \(\). Given the transition kernel \(\), the goal is to find a sampler \(q_{}\) to closely approximate the target distribution \(\). This can be achieved by iteratively distill \(T\)-step transitions of LD into \(q_{}\):

\[q_{_{k}}*{arg\,min}_{q_{ }}[q_{_{k-1},T}\|q_{}],\;q_{_{k-1},T}:=_{T}q_{_{k-1}},\;q_{_{0}} _{0},\;k=0,...,K-1.\] (6)

where \([\|]\) is the Kullback-Leibler Divergence (KLD) measure between distributions. Concretely, Eq. (6) means that to recover the target distribution \(\), instead of using long-run LD, we can repeat the following steps: i) employ a \(T\)-step short-run LD initialized with the current sampler \(q_{_{k-1}}\) to approximate \(_{T}q_{_{k-1}}\) as the target distribution of the current sampler, and ii) update the current sampler \(q_{_{k-1}}\) to \(q_{_{k}}\). The correct convergence of \(q_{}\) to \(\) with Eq. (6) is supported by the standard theory of Markov chains , which suggests that the update in Eq. (6) is monotonically decreasing in terms of KLD, \([q_{_{k}}\|][q_{_{k-1}}\|]\). We refer to Appendix A.1 for a detailed explanation and discussion of this statement. In practice, one can apply gradient-based methods to minimize \([q_{_{k-1},T}\|q_{}]\) and approximate Eq. (6) for the update from \(q_{_{k-1}}\) to \(q_{_{k}}\). The above formulation provides a generic and flexible framework for amortizing the potentially long MCMC.

Diffusion-based amortizationTo avoid clutter, we simply write \(q_{_{k-1},T}\) as \(q_{T}\). We can see that

\[*{arg\,min}_{q_{}}[q_{T}\|q_{ {}}]=*{arg\,min}_{q_{}}-(q_{T})+ (q_{T},q_{})=*{arg\,min}_{q_{}}- _{q_{T}}[ q_{}],\] (7)

where \(\) represents the entropy of distributions. The selection of the sampler \(q_{}\) is a matter of design. According to Eq. (7), we may expect the following properties from \(q_{}\): i) having analytically tractable expression of the exact value or lower bound of log-likelihood, ii) easy to draw samples from and iii) capable of close approximation to the given distribution \(\{q_{T}\}\). In practice, iii) is important for the convergence of Eq. (6). If \(q_{}\) is far away from \(q_{T}\) in each iteration, then non-increasing KLD property \([q_{_{k}}\|][q_{_{k-1}}\|]\) might not hold, and the resulting amortized sampler would not converge to the true target distribution \(()\).

For the choice of \(q_{}\), let us consider distilling the gradient field of \(q_{T}\) in each iteration, so that the resulting sampler is close to the \(q_{T}\) distribution. This naturally points to the DDPMs . To be specific, learning a DDPM with \(\)-prediction parameterization is equivalent to fitting the finite-time marginal of a sampling chain resembling annealed Langevin dynamics [7; 8; 31]. Moreover, it also fulfills i) and ii) of the desired properties mentioned above. We can plug in the objective of DDPM (Section 2.2), which is a lower bound of \( q_{}\), to obtain the gradient-based update rule for \(q_{}\):

\[_{k-1}^{(i+1)}_{k-1}^{(i)}-_{} _{,}[\|(_{}) -\|_{2}^{2}],\;_{k}^{(0)}_ {k-1}^{(M)},\;i=0,1,...,M-1\] (8)

where \((0,_{d})\). \(\) is drawn from a distribution of log noise-to-signal ratio \(p()\). \(\) is the step size for the update, and \(M\) is the number of iterations needed in Eq. (8). In practice, we find that when amortizing the LD sampling chain, a light-weight DDPM \(q_{}\) updated with very small \(M\), _i.e._, few steps of Eq. (8) iteration, approximates Eq. (6) well. We provide a possible explanation using the Fisher information by scoping the asymptotic behavior of this update rule in the Appendix A.2. We term the resulting sampler as Diffusion-Amortized MCMC (DAMC).

### Approximate MLE with DAMC

In this section, we show how to integrate the DAMC sampler into the learning framework of LEBM and form a symbiosis between these models. Given a set of \(N\) training samples \(\{_{i}\}_{i=1}^{N}\) independently drawn from the unknown data distribution \(p_{}()\), the model \(p_{}\) (Section 2.1) can be trainedby maximizing the log-likelihood over training samples \(()=_{i=1}^{N} p_{}(_ {i})\). Doing so typically requires computing the gradients of \(()\), where for each \(_{i}\) the learning gradient satisfies:

\[_{} p_{}(_{i}) =_{p_{}(_{i}|_{i})} [_{} p_{}(_{i},_{i})]\] (9) \[=(_{p_{}(_{i}| _{i})}[_{}f_{}(_{i})]- _{p_{}(_{i})}[_{} f_{}(_{i})]}_{_{}(_{i})} _{p_{}(_{i}|_{i})} [_{} p_{}(_{i}|_{i})]}_{ _{}(_{i})}).\]

Intuitively, based on the discussion in Section 2.1 and Section 3.1, we can approximate the distributions in Eq. (9) by drawing samples from \([_{i}|_{i}]_{T,_{i}|_{i}}q_{_{ k}}(_{i}|_{i})\), \(_{i}_{T,_{i}}q_{_{k}}(_{i})\), to estimate the expectations and hence the learning gradient. Here we learn the DAMC samplers \(q_{_{k}}(_{i}|_{i})\) and \(q_{_{k}}(_{i})\) for the posterior and prior sampling chain, respectively. \(_{k}\) represents the current sampler as in Section 3.1; \(_{T,_{i}|_{i}}\) and \(_{T,_{i}}\) are the transition kernels for posterior and prior sampling chain. Equivalently, this means to better estimate the learning gradients we can i) first draw approximate posterior and prior MCMC samples from the current \(q_{_{k}}\) model, and ii) update the approximation of the prior \(p_{}()\) and posterior \(p_{}(|)\) distributions with additional \(T\)-step LD initialized with \(q_{_{k}}\) samples. These updated samples are closer to \(p_{}(_{i}|_{i})\) and \(p_{}(_{i})\) compared with short-run LD samples based on our discussion in Section 2.2. Consequently, the diffusion-amortized LD samples provide a generally better estimation of the learning gradients and lead to better performance, as we will show empirically in Section 4. After updating \(=(,)\) based on these approximate samples with Eq. (9), we can update \(q_{_{k}}\) with Eq. (8) to distill the sampling chain into \(q_{_{k+1}}\). As shown in Fig. 1, we can see that the whole learning procedure iterates between the approximate MLE of \(p_{}\) and the amortization of MCMC with \(q_{}\). We refer to Appendix A.3 for an extended discussion of this procedure.

After learning the models, we can use either DAMC or LEBM for prior sampling. For DAMC, we may draw samples from \(q_{}(_{i})\) with Eq. (5). Prior sampling with LEBM still requires short-run LD initialized from \((0,_{d})\). For posterior sampling, we may sample from \(_{T,_{i}|_{i}}q_{_{k}}(_{i}|_{i})\), _i.e._, first draw samples from DAMC and then run few steps of LD to obtain posterior samples.

```
0: initial parameters \((,,)\); learning rate \(=(_{},_{},_{})\); observed examples \(\{^{(i)}\}_{i=1}^{N}\); prob. of uncond. training \(p_{}\) for the DAMC sampler.
0:\((^{(K)}=\{^{(K)},^{(K)}\},^{(K )})\).
1for\(k=0:K-1\)do
2 Sample a minibatch of data \(\{^{(i)}\}_{i=1}^{B}\);
3 Draw DAMC samples: For each \(^{(i)}\), draw \(^{(i)}_{+}\) and \(^{(i)}_{-}\) from \(q_{_{k}}(_{i}|_{i})\).
4Prior LD update: For each \(^{(i)}\), update \(^{(i)}_{-}\) using Eq. (3), with \((_{i})=p_{^{(k)}}(_{i})\);
5 Posterior LD update: For each \(^{(i)}\), update \(^{(i)}_{+}\) using Eq. (3), with \((_{i})=p_{^{(k)}}(_{i}|_{i})\);
6 Update \(^{(k)}\): Update \(^{(k)}\) and \(^{(k)}\) using Monte-Carlo estimates (_i.e._, Monte-Carlo average) of Eq. (9) with \(\{^{(i)}_{+}\}_{i=1}^{B}\) and \(\{^{(i)}_{-}\}_{i=1}^{B}\).
7 Update \(^{(k)}\): Update \(^{(k)}\) using Eq. (8) with \(p_{}\) and \(\{^{(i)}_{+}\}_{i=1}^{B}\) as the target. ```

**Algorithm 1****Learning algorithm of DAMC.**

### Implementation

In order to efficiently model both \(q_{_{k}}(_{i}|_{i})\) and \(q_{_{k}}(_{i})\), we follow the method of  to train a single network to parameterize both models, where \(q_{_{k}}(_{i}|_{i})\) can be viewed as a conditional DDPM with the embedding of \(_{i}\) produced by an encoder network as its condition, and \(q_{_{k}}(_{i})\) an unconditional one. For \(q_{_{k}}(_{i})\), we can input a null token \(\) as its condition when predicting the noise \(\). We jointly train both models by randomly nullifying the inputs with the probability \(p_{}=0.2\). During training, we use samples from \(q_{_{k}}(_{i}|_{i})\) to initialize both prior and posterior updates for training stability. For the posterior and prior DAMC samplers, we set the number of diffusion steps to \(100\). The number of iterations in Eq. (8) is set to \(M=6\) throughout the experiments. The LD runs 30 and 60 iterations for posterior and prior updates during training with a step size of \(s=0.1\). For test time sampling from \(_{T,_{i}|_{i}}q_{_{k}}(_{i}|_{i})\), \(T=10\) for the additional LD. For a fair comparison, we use the same LEBM and generator as in [22; 41] for all the experiments. We summarize the learning algorithm in Algorithm 1. Please see Appendices B and C for network architecture and further training details, as well as the pytorch-style pseudocode of the algorithm.

## 4 Experiments

In this section, we are interested in the following questions: (i) How does the proposed method compare with its previous counterparts (_e.g._, purely MCMC-based or variational methods)? (ii) How is the scalability of this method? (iii) How are the time and parameter efficiencies? (iv) Does the proposed method provide a desirable latent space? To answer these questions, we present a series of experiments on benchmark datasets including MNIST , SVHN , CelebA64 , CIFAR-10 , CelebAMask-HQ , FFHQ  and LSUN-Tower . As to be shown, the proposed method demonstrates consistently better performance in various experimental settings compared with previous methods. We refer to Appendix D for details about the experiments.

### Generation and Inference: Prior and Posterior Sampling

Generation and reconstructionWe evaluate the quality of the generated and reconstructed images to examine the sampling quality of DAMC. Specifically, we would like to check i) how well does DAMC fit the seen data, ii) does DAMC provide better MCMC samples for learning LEBM and iii) the generalizability of DAMC on unseen data. We check the goodness of fit of DAMC by evaluating the quality of the images generated with DAMC prior samples. If DAMC does provide better MCMC samples for learning LEBM, we would expect better fitting of data and hence an improved generation quality of LEBM. We evaluate the performance of posterior sampling given unseen testing images by examining the reconstruction error on testing data. We benchmark our model against a variety of previous methods in two groups. The first group covers competing methods that adopt the variational learning scheme, including VAE , as well as recent two-stage methods such as 2-stage VAE , RAE  and NCP-VAE , whose prior distributions are learned with posterior samples in a second stage after the generator is trained. The second group includes methods that adopt MCMC-based sampling. It includes Alternating Back-Propogation (ABP) , Short-Run Inference (SRI) from  and the vanilla learning method of LEBM , which relies on short-run LD for both posterior and prior sampling. We also compare our method with the recently proposed Adaptive CE . It learns a series of LEBMs adaptively during training, while these LEBMs are sequentially updated by density ratio estimation instead of MLE. To make fair comparisons, we follow the same evaluation protocol as in [22; 41].

    &  &  &  &  \\   & MSE & FID & MSE & FID & MSE & FID & MSE & FID \\  VAE  & 0.019 & 46.78 & 0.021 & 65.75 & 0.057 & 106.37 & 0.031 & 180.49 \\
2s-VAE  & 0.019 & 42.81 & 0.021 & 44.40 & 0.056 & 72.90 & - & - \\ RAE  & 0.014 & 40.02 & 0.018 & 40.95 & 0.027 & 74.16 & - & - \\ NCP-VAE  & 0.020 & 33.23 & 0.021 & 42.07 & 0.054 & 78.06 & - & - \\  Adaptive CE*  & 0.004 & 26.19 & 0.009 & 35.38 & **0.008** & 65.01 & - & - \\  ABP  & - & 49.71 & - & 51.50 & 0.018 & 90.30 & 0.025 & 160.21 \\ SRI  & 0.018 & 44.86 & 0.020 & 61.03 & - & - & - & - \\ SRI (L=5)  & 0.011 & 35.32 & 0.015 & 47.95 & - & - & - & - \\ LEBM  & 0.008 & 29.44 & 0.013 & 37.87 & 0.020 & 70.15 & 0.025 & 133.07 \\  Ours-LEBM & **0.002** & 21.17 & **0.005** & 35.67 & 0.015 & 60.89 & 0.023 & 89.54 \\ Ours-DAMC & **18.76** & **30.83** & **30.83** & **57.72** & **0.023** & **85.88** \\   

Table 1: **MSE(\(\)) and FID(\(\)) obtained from models trained on different datasets. The FID scores are computed based on 50k generated images and training images for the first three datasets and 5k images for the CelebA-HQ dataset. The MSEs are computed based on unseen testing images. We highlight our model results in gray color. The best and second-best performances are marked in bold numbers and underlines, respectively; tables henceforth follow this format. * uses a prior model with 4x parameters compared with  and ours.**For generation, we report the FID scores  in Table 1. We observe that i) the DAMC sampler, denoted as Ours-DAMC, provides superior generation performance compared to baseline models, and ii) the LEBM learned with samples from DAMC, denoted as Ours-LEBM, demonstrates significant performance improvement compared with the LEBM trained with short-run LD, denoted as LEBM. These results confirm that DAMC is a reliable sampler and indeed partly addresses the learning issue of LEBM caused by short-run LD. We would like to point out that the improvement is clearer on the CelebAMask-HQ dataset, where the input data is of higher dimension (\(256 256\)) and contains richer details compared with other datasets. This illustrates the superiority of DAMC sampler over short-run LD when the target distribution is potentially highly multi-modal. We show qualitative results of generated samples in Fig. 2, where we observe that our method can generate diverse, sharp and high-quality samples. For reconstruction, we compare our method with baseline methods in terms of MSE in Table 1. We observe that our method demonstrates competitive reconstruction error, if not better, than competing methods do. Additional qualitative results of generation and reconstruction are presented in Appendices E.2 and E.3.

GAN inversionWe have examined the scalability of our method on the CelebAMask-HQ dataset. Next, we provide more results on high-dimensional and highly multi-modal data by performing GAN inversion  using the proposed method. Indeed, we may regard GAN inversion as an inference problem and a special case of posterior sampling. As a suitable testbed, the StyleGAN structure  is specifically considered as our generator in the experiments:  points out that to effectively infer the latent representation of a given image, the GAN inversion method needs to consider an extended latent space of StyleGAN, consisting of 14 different 512-dimensional latent vectors. We attempt to use the DAMC sampler for GAN inversion. We benchmark our method against i) learning an encoder that maps a given image to the latent space , which relates to the variational methods for posterior inference, ii) optimizing a random initial latent code by minimizing the reconstruction error and perceptual loss , which can be viewed as a variant of LD sampling, and iii) optimizing the latent code by minimizing both the objectives used in ii) and the energy score provided by LEBM. We use the pretrained weights provided by  for the experiments. Both the DAMC sampler and the encoder-based method are augmented with \(100\) post-processing optimization iterations. We refer to Appendix D for more experiment details. We test LEBM-based inversion with different optimization iterations. To be specific, 1x, 2x, and 4x represent 100, 200, and 400 iterations respectively. We can see in Table 2 that DAMC performs better than all the baseline methods on the unseen testing data, which supports the efficacy of our method in high-dimensional settings. We provide qualitative results in Fig. 3.

Parameter efficiency and sampling timeOne potential disadvantage of our method is its parameter inefficiency for introducing an extra DDPM. Fortunately, our models are in the latent space

    &  &  \\   & MSE & FID & MSE & FID \\  Opt.  & 0.055 & 149.39 & 0.080 & 240.11 \\ Enc.  & 0.028 & 62.32 & 0.079 & 132.41 \\ 
 w/ 1x & 0.054 & 149.21 & 0.072 & 239.51 \\
 w/ 2x & 0.039 & 101.59 & 0.066 & 163.20 \\
 w/ 4x & 0.032 & 84.64 & 0.059 & 111.53 \\   Ours & **0.025** & **52.85** & **0.059** & **80.42** \\   

Table 2: **MSE(\(\)) and FID(\(\)) for GAN inversion on different datasets**. Opt. and Enc. denotes the optimization-based and encoder-based methods.

Figure 2: **Samples generated from the DAMC sampler and LEBM trained on SVHN, CelebA, CIFAR-10 and CelebA-HQ datasets. In each sub-figure, the first four rows are generated by the DAMC sampler. The last four rows are generated by LEBM trained with the DAMC sampler.**

so the network is lightweight. To be specific, on CIFAR-10 dataset the number of parameters in the DDPM is only around \(10\%\) (excluding the encoder) of those in the generator. The method has competitive time efficiency. With the batch size of \(64\), the DAMC prior sampling takes \(0.3\)s, while \(100\) steps of short-run LD with LEBM takes \(0.2\)s. The DAMC posterior sampling takes \(1.0\)s, while LEBM takes \(8.0\)s. Further discussions about the limitations can be found in the Appendix G.1.

### Analysis of Latent Space

Long-run langevin transitionIn this section, we examine the energy landscape induced by the learned LEBM. We expect that a well-trained \(p_{}()\) fueled by better prior and posterior samples from the DAMC sampler would lead to energy landscape with regular geometry. In Fig. 4, we visualize the transition of LD initialized from \((0,_{d})\) towards \(p_{}()\) on the model trained on the CelebA dataset. Additional visualization of transitions on SVHN and CIFAR-10 datasets can be found in the Appendix E.4. The LD iterates for \(200\) and \(2500\) steps, which is longer than the LD within each training iteration (\(60\) steps). For the \(200\)-step set-up, we can see that the generation quality quickly improves by exploring the local modes (demonstrating different facial features, _e.g._, hairstyle, facial expression and lighting). For the \(2500\)-step long-run set-up, we can see that the LD produces consistently valid results without the oversaturating issue of the long-run chain samples . These observations provide empirical evidence that the LEBM is well-trained.

Anomaly detectionWe further evaluate how the LEBM learned by our method could benefit the anomaly detection task. With properly learned models, the posterior \(p_{,}(|)\) could form a discriminative latent space that has separated probability densities for in-distribution (normal) and out-of-distribution (anomalous) data. Given the testing sample \(\), we use un-normalized log joint density \(p_{,}(|) p_{,}(,) p_{}(|)p_{}()|_{ _{T,|}q_{}(|)}\) as our decision function. This means that we draw samples from \(_{T,|}q_{}(|)\) and compare the corresponding reconstruction errors and energy scores. A higher value of log joint density indicates a higher probability of the test sample being a normal sample. To make fair comparisons, we follow the experimental settings in [22; 41; 55; 56] and train our model on MNIST with one class held out as an anomalous class. We consider the

Figure 4: **Transition of Markov chains initialized from \((0,_{d})\) towards \(p_{}()\). We present results by running LD for 200 and 2500 steps. In each sub-figure, the top panel displays the trajectory in the data space uniformly sampled along the chain. The bottom panel shows the energy score \(f_{}()\) over the iterations.**

Figure 3: **Qualitative results of StyleGAN inversion using the DAMC sampler. In each sub-figure, the left panel contain samples from the FFHQ dataset, and the right panel contains samples from the LSUN-T dataset.**

baseline models that employ MCMC-based or variational inferential mechanisms. Table 3 shows the results of AUPRC scores averaged over the last 10 trials. We observe significant improvements in our method over the previous counterparts.

### Ablation Study

In this section, we conduct ablation study on several variants of the proposed method. Specifically, we would like to know: i) what is the difference between the proposed method and directly training a DDPM in a fixed latent space? ii) What is the role of LEBM in this learning scheme? iii) Does DAMC effectively amortize the sampling chain? We use CIFAR-10 dataset for the ablative experiments to empirically answer these questions. More ablation studies can be found in Appendix F.

Non-Amortized DDPM vs. DAMCWe term directly training a DDPM in a fixed latent space as the non-amortized DDPM. To analyze the difference between non-amortized DDPM and DAMC, we first train a LEBM model with persistent long-run chain sampling  and use the trained model to obtain persistent samples for learning the non-amortized DDPM. In short, the non-amortized DDPM can be viewed as directly distilling the long-run MCMC sampling process, instead of progressively amortizing the chain. We present the FID and MSE of the non-amortized model (NALR) in Table 4. We observe that directly amortizing the long-run chain leads to degraded performance compared with the proposed method. The results are consistently worse for both posterior and prior sampling and the learned LEBMs, which verify the effectiveness of the proposed iterative learning scheme.

The contribution of LEBMOne may argue that since we have the DAMC as a powerful sampler, it might not be necessary to jointly learn LEBM in the latent space. To demonstrate the necessity of this joint learning scheme, we train a variant of DAMC by replacing the LEBM with a Gaussian prior. The results are presented in Table 4. We observe that models trained with non-informative Gaussian prior obtain significantly worse generation results. It suggests that LEBM involved in the learning iteration provides positive feedback to the DAMC sampler. Therefore, we believe that it is crucial to jointly learn the DAMC and LEBM.

Vanilla sampling vs. DAMCWe compare the vanilla sampling process of each model with DAMC. The vanilla sampling typically refers to short-run or long-run LD initialized with \((0,_{d})\). We also provide results of learning LEBM using variational methods for comparison. We can see in Table 4 that sampling with DAMC shows significantly better scores of the listed models, compared with vanilla sampling. The result is even better than that of the persistent chain sampler (V. of NALR-LEBM). This indicates that DAMC effectively amortizes the sampling chain. Comparing DAMC sampler with the variational sampler also indicates that DAMC is different from general variational approximation: it benefits from its connection with LD and shows better expressive power.

    &  &  &  &  &  \\   & V. & D. & V. & D. & V. & D. & V. & D. & V. & D. \\  MSE & 0.054 & - & 0.020 & - & 0.018 & 0.015 & 0.028 & 0.016 & 0.021 & **0.015** \\ FID & 78.06 & - & 70.15 & - & 90.30 & 66.93 & 68.52 & 64.38 & 60.89 & **57.72** \\   

Table 4: **Ablation study on CIFAR-10 dataset**. VI denotes learning LEBM using variational methods. SR denotes learning LEBM with short-run LD. DAMC-G replaces the LEBM in DAMC-LEBM with a standard Gaussian distribution. NALR denotes the non-amortized DDPM setting. For each set-up, we provide results using the vanilla sampling method, denoted as V, and the ones using the DAMC sampler, denoted as D.

    & 1 & 4 & 5 & 7 & 9 \\  VAE  & 0.063 & 0.337 & 0.325 & 0.148 & 0.104 \\ ABP  & \(0.095 0.03\) & \(0.138 0.04\) & \(0.147 0.03\) & \(0.138 0.02\) & \(0.102 0.03\) \\ MEG  & \(0.281 0.04\) & \(0.401 0.06\) & \(0.402 0.06\) & \(0.290 0.04\) & \(0.342 0.03\) \\ BiGAN-\(\) & \(0.287 0.02\) & \(0.443 0.03\) & \(0.514 0.03\) & \(0.347 0.02\) & \(0.307 0.03\) \\ LEBM  & \(0.336 0.01\) & \(0.630 0.02\) & \(0.619 0.01\) & \(0.463 0.01\) & \(0.413 0.01\) \\ Adaptive CE  & \(0.531 0.02\) & \(0.729 0.02\) & \(0.742 0.01\) & \(0.620 0.02\) & \(0.499 0.01\) \\ 
**Ours** & **0.684 \(\) 0.02** & **0.911 \(\) 0.01** & **0.939 \(\) 0.02** & **0.801 \(\) 0.01** & **0.705 \(\) 0.01** \\   

Table 3: **AUPRC(\(\)) scores for unsupervised anomaly detection on MNIST**. Numbers are taken from . Results of our model are averaged over the last 10 trials to account for variance.

## 5 Related Work

Energy-based prior modelEBMs [23; 58; 59; 60; 24] play an important role in generative modeling. Pang et al.  propose to learn an EBM as a prior model in the latent space of DLVMs; it greatly improves the model expressivity over those with non-informative priors and brings strong performance on downstream tasks, _e.g._, image segmentation, text modeling, molecule generation, and trajectory prediction [61; 12; 15; 62]. However, learning EBMs or latent space EBMs requires MCMC sampling to estimate the learning gradients, which needs numerous iterations to converge when the target distributions are high-dimensional or highly multi-modal. Typical choices of sampling with non-convergent short-run MCMC  in practice can lead to poor generation quality, malformed energy landscapes [25; 15; 24], biased estimation of the model parameter and instability in training [23; 59; 60; 25]. In this work, we consider learning valid amortization of the long-run MCMC for energy-based priors; the proposed model shows reliable sampling quality in practice.

Denoising diffusion probabilistic modelDDPMs [7; 31; 8], originating from , learn the generative process by recovering the observed data from a sequence of noise-perturbed versions of the data. The learning objective can be viewed as a variant of the denoising score matching objective . As pointed out in [5; 8], the sampling procedure of DDPM with \(\)-prediction parametrization resembles LD of an EBM; \(\) (predicted noise) plays a similar role to the gradient of the log density . To be specific, learning a DDPM with \(\)-prediction parameterization is equivalent to fitting the finite-time marginal of a sampling chain resembling annealed Langevin dynamics [7; 8; 31]. Inspired by this connection, we propose to amortize the long-run MCMC in learning energy-based prior by iteratively distilling the short-run chain segments with a DDPM-based sampler. We show empirically and theoretically that the learned sampler is valid for long-run chain sampling.

Amortized MCMCThe amortized MCMC technique is formally brought up by Li et al. , which incorporates feedback from MCMC back to the parameters of the amortizer distribution \(q_{}\). It is concurrently and independently proposed by Xie et al.  as the MCMC teaching framework. Methods under this umbrella term [36; 37; 38; 63; 64; 65] generally learns the amortized by minimizing the divergence (typically the KLD) between the improved distribution and its initialization, i.e., \([_{T}q_{_{k-1}}||q_{}]\), where \(_{T}\) represents \(T\)-step MCMC transition kernel and \(q_{_{k-1}}\) represents the current amortized. The diffusion-based amortization proposed in this work can be viewed as an instantiation of this framework, while our focus is on learning the energy-based prior. Compared with previous methods, our method i) specifically exploits the connection between EBMs and DDPMs and is suitable for amortizing the prior and posterior sampling MCMC of energy-based prior, and ii) resides in the lower-dimensional latent space and enables faster sampling and better convergence.

More methods for learning EBMSeveral techniques other than short-run MCMC have been proposed to learn the EBM. In the seminal work, Hinton  proposes to initialize Markov chains using real data and run several steps of MCMC to obtain samples from the model distribution. Tieleman  proposes to start Markov chains from past samples in the previous sampling iteration, known as Persistent Contrastive Divergence (PCD) or persistent chain sampling, to mimic the long-run sampling chain. Nijkamp et al.  provide comprehensive discussions about tuning choices for LD such as the step size \(s\) and sampling steps \(T\) to obtain stable long-run samples for persistent training. [59; 60] employ a hybrid of persistent chain sampling and short-run sampling by maintaining a buffer of previous samples. The methods draw from the buffer or initialize the short-run chain with noise distribution with some pre-specified probability. Another branch of work, stemmed from , considers discriminative contrastive estimation to avoid MCMC sampling. Gao et al.  use a normalizing flow  as the base distribution for contrastive estimation. Aneja et al.  propose to estimate the energy-based prior model based on the prior of a pre-trained VAE  by noise contrastive estimation. More recently, Xiao and Han  learn a sequence of EBMs in the latent space with adaptive multi-stage NCE to further improve the expressive power of the model.

## 6 Conclusion

In this paper, we propose the DAMC sampler and develop a novel learning algorithm for LEBM based on it. We provide theoretical and empirical evidence for the effectiveness of our method. We notice that our method can be applied to amortizing MCMC sampling of unnormalized continuous densities in general. It can also be applied to sampling posterior distributions of continuous latent variables in general latent variable models. We would like to explore these directions in future work.