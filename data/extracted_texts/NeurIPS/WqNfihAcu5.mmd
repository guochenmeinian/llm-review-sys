# Strategic Multi-Armed Bandit Problems Under Debt-Free Reporting

Ahmed Ben Yahmed

CREST, ENSAE, Palaiseau, France

Criteo AI Lab, Paris, France

FairPlay joint team

a.benyahmed@criteo.com

&Clement Calauzenes

Criteo AI Lab, Paris, France

FairPlay joint team

c.calauzenes@criteo.com

&Vianney Perchet

CREST, ENSAE, Palaiseau, France

Criteo AI Lab, Paris, France

FairPlay joint team

vianney.perchet@normalesup.org

###### Abstract

We consider the classical multi-armed bandit problem, but with strategic arms. In this context, each arm is characterized by a bounded support reward distribution and strategically aims to maximize its own utility by potentially retaining a portion of its reward, and disclosing only a fraction of it to the learning agent. This scenario unfolds as a game over \(T\) rounds, leading to a competition of objectives between the learning agent, aiming to minimize their regret, and the arms, motivated by the desire to maximize their individual utilities. To address these dynamics, we introduce a new mechanism that establishes an equilibrium wherein each arm behaves truthfully and discloses as much of its rewards as possible. With this mechanism, the agent can attain the second-highest average (true) reward among arms, with a cumulative regret bounded by \(((T)/)\) (problem-dependent) or \(()\) (worst-case).

## 1 Introduction

The multi-armed bandit (MAB) problem serves as a fundamental modeling framework for exploring the interplay between a decision-making agent (or player) and a set of arms. Its primary aim is to enable the player to learn the most favorable sequence of decisions over time. In formal terms, we consider a scenario involving \(K\) arms, where, at each time step \(t\), the player selects one arm \(k_{t}\) from the set \(\{1,2,,K\}\) and receives a reward, denoted as \(r_{k_{t},t}\). These rewards can either be generated stochastically or determined adversarially. The player's overarching objective is to strike a balance between two key aspects: exploration and exploitation. Initially, he must explore all arms adequately to gain confidence in identifying the best-performing arm. Once this is accomplished, he focuses on exploiting this knowledge to maximize his cumulative reward over subsequent rounds. Established algorithms for this problem ensure that the player's performance is nearly as good as selecting the best individual arm, thus minimizing his regret . This modeling framework has broad real-world applications, including domains such as clinical trials , recommender systems , and resource allocation problems . However, in many of these scenarios, arms are traditionally viewed as passive entities, faithfully reporting their generated rewards \(r_{k_{t},t}\) to the player.

This perspective leaves out an array of dynamic agency dilemmas, where the player selects one of the \(K\) agents (arms) at each time step to execute a task on his behalf, with the associated cost remaining hidden from the player due to his limited domain or market knowledge. Essentially, the player is uncertain about the precise costs or returns until the task is completed, and the agent has substantial freedom to set these ex-post . In this context, it is reasonable to assume that arms are strategic and may act in their self-interest. Conceptually, arms can report a value, denoted as \(x_{k_{t},t}\), which may differ from the actual reward \(r_{k_{t},t}\), retaining in the process a net utility of \(r_{k_{t},t}-x_{k_{t},t}\). This strategic scenario introduces a game-like dynamic, creating a competition of objectives between the player, aiming to minimize his regret and the arms, wishing to maximize their own utilities.

Motivated by numerous real-world applications where, by design, strategic arms operate under restricted payment conditions, our study focuses on debt-free reporting. In this setting, arms cannot report values higher than the observed reward. An illustrative example pertains to scenarios with binary variables, where declaring a fake failure is possible but creating a fake success is impossible. This situation is evident in e-commerce, where advertisers receive rewards for successful ad campaigns that result in a sale after a click. Notably, an e-commerce platform may choose to conceal a sale, but it cannot fabricate one. This setting is also referred to as budget-balance in the repeated trade literature [9; 10], requiring \(x_{k_{t},t} r_{k_{t},t}\) for all \(t\), or equivalently, \((x_{k_{t},t},r_{k_{t},t})\) belonging to the upper triangle \(=\{(a,b)^{2} a b\}\), where \(\) represents the variables' space.

## 2 Problem Statement

The concept of the strategic multi-armed bandit builds upon the foundation of the classic multi-armed bandit problem, introducing a novel element wherein the arms possess the capability to retain a portion of the reward for themselves. Within this framework, we contemplate a collection of \(K\) stochastic arms denoted by \(k\{1,,K\}\), where each arm possesses a bounded reward distribution \(D_{k}\) supported on \(\). Each arm is distinguished by its reward mean, denoted as \(_{k}\). To maintain generality without loss of clarity, we assume, for the presentation and the analysis, an ordering such that \(_{1}_{2}_{K}\) (unknown to the player). In each round \(t\), the _player_ selects an arm denoted as \(k_{t}\). This arm \(k_{t}\) then observes a reward \(r_{k_{t},t}\). Subsequently, the _arm_ reports a quantity \(x_{k_{t},t}\) to the player and retains \(r_{k_{t},t}-x_{k_{t},t}\) as its own utility. In this scenario, it is important to note that solely arm \(k_{t}\) possesses knowledge of the actually observed reward \(r_{k_{t},t}\), whereas the player is privy only to \(x_{k_{t},t}\) and remains unaware of the withheld portion. Hence the decision of the player is based on the information gathered until time \(t\) which can formally encoded in \(_{P,t}=\{k_{s},x_{k_{s},s}\}_{s<t}\) implying that \(k_{t}\) is \((_{P,t})\) measurable. Conversely, the arm \(k_{t}\)'s available information is succinctly represented by \(_{k_{t},t}=\{k_{s},r_{k_{s},s},x_{k_{s},s},k_{t},r_{k_{t},t}\}_{s<t :k_{s}=k_{t}}\{k_{s}\}_{s<t}\) and \(x_{k_{t},t}\) is \((_{k_{t},t})\) measurable. Specifically, this indicates that the tacit model is being utilized, wherein the arms are informed about the pulled arms at each round and only observe the reward upon being chosen.

Furthermore, we operate under the debt-free reporting assumption: arms are prohibited from incurring negative balances when reporting \(x_{k_{t},t}\), implying the following constraint \(0 x_{k_{t},t} r_{k_{t},t}\  t 1\). In the context of strategic bandits, the player has the option to allocate a bonus \(_{k}\) to each arm \(k\{1,,K\}\) at the end of the game. This bonus serves as an incentive to encourage arms to provide truthful information, for instance. It can take various forms, including additional rounds of pulling as in  or simply a bonus payment awarded to the arms, similar to payments in the usual mechanism design for procurement auctions . In this work, the latter option has been chosen for use, i.e. a bonus payment-based algorithm. Therefore, the interaction between the player and the arms can be encapsulated in the Model 1.

```
1 Player commits to an algorithm \(\), which is public to the arms
2for\(t=1,,T\)do
3 Player selects arms \(k_{t}\{1,,K\}\) according to the chosen algorithm \(\)
4 Arm \(k_{t}\) observes reward \(r_{k_{t},t} D_{k_{t}}\)
5 Arm \(k_{t}\) reports \(x_{k_{t},t}[0,r_{k_{t},t}]\) to the player
6 end for
7 Player assigns bonuses \(_{k}\) to each arm \(k\{1,,K\}\) ```

**Model 1**: The Strategic Multi-Armed Bandit Problem

```
1 Player commits to an algorithm \(\), which is public to the arms
2for\(t=1,,T\)do
3 Player selects arms \(k_{t}\{1,,K\}\) according to the chosen algorithm \(\)
4 Arm \(k_{t}\) observes reward \(r_{k_{t},t} D_{k_{t}}\)
5 Arm \(k_{t}\) reports \(x_{k_{t},t}[0,r_{k_{t},t}]\) to the player
6 end for
7 Player assigns bonuses \(_{k}\) to each arm \(k\{1,,K\}\) ```

**Model 2**: The strategic multi-armed bandit problem

### Arm Utility and Subgame Perfect Equilibrium Among Arms

Let us denote by \(\), the set of all possible arm strategies, by \(_{k}=(_{k,t})_{t:k_{t}=k}\) the strategy of arm \(k\) and by \(=(_{1},,_{k})\) the strategy profile of the arms. Explicitly, \(\) is the set of mappings from the past history \(_{s=1}^{T}^{2s}\{0,1\}^{T}}\) to \(\). Here, \(^{2s}\) denotes the accumulated observed and announced rewards, and \(\{0,1\}^{T}\) indicates the rounds during which this arm has been selected thus far. Let \(x_{k}_{}^{T}\) (and \(r_{k}_{}^{T}\)) be the sequence of the reported values (and rewards, respectively) by arm \(k\) across \(T\) rounds where \(_{}=\{\}\), and the symbol \(`\)' is used for rounds when the arm is not observed. We also refer to \(x_{k}\) as the path of arm \(k\) under strategy \(\). Let \(x_{-k}\) denote the \(K-1\) paths of all arms except \(k\). As a result, the utility associated with arm \(k\) is defined in an ex-post fashion as follows:

\[_{k}(_{k},_{-k})=_{t=1}^{T}(r_ {k_{t},t}-x_{k_{t},t})_{[k_{t}=k]}+_{k}(x_{k},x_{-k}) }\] (1)

which is the cumulative savings over rounds plus the corresponding assigned bonus (we slightly abused notations here, as \(_{k}\) only depends on the observations of the player, and not the full vector \(x_{k}\)). Given that each arm aims to maximize its own utility, we introduce the solution concept of subgame perfect Nash Equilibrium (SPE) among arms. A strategy profile \(^{}\) is a SPE if \(_{k}^{}\) is an optimal strategy for any arm \(k\), considering any history and given any strategy \(_{-k}^{K-1}\) adopted by other arms, at any time \(t\).

### Player's Strategic Regret

In the strategic scenario, the player, under the most unfavorable circumstances, cannot guarantee to achieve gains surpassing \(_{2}\). This limitation arises from the fact that the optimal arm merely requires a marginally higher reported value than the second-best arm to be chosen as the superior option by the player (see Section 2.3 for more details). In addition, the bonus allocated to the arms is deducted from the player's total revenue; hence, these bonuses are factored into the regret definition. Therefore, the regret, arising from the cumulative expected discrepancy between \(_{2}\) and the value reported by the selected arm \(k_{t}\) over \(T\) rounds, along with the bonus values, is defined as follows:

\[_{T}=[_{t=1}^{T}(_{2}-x_{k_{t},t}) +_{k=1}^{K}_{k}(x_{k},x_{-k})]\] (2)

### Related Work

A simplified instance of the strategic bandit problem has been examined within the context of the principal-agent problem in contract theory [12; 13]. In this analogy, a player engages an arm with greater expertise in a specific domain to perform work on his behalf. However, the arm may exploit the player's lack of knowledge to maximize its own utility and accumulate private savings. Consequently, the strategic bandit problem can be viewed as a principal-multiagents problem. Additionally, drawing inspiration from the field of online advertising, several studies have delved into dynamic mechanisms to address similar scenarios [14; 15; 16; 17; 18; 19; 20]. For instance,  examined auctions with reserve prices involving strategic myopic buyer.

In particular,  studied strategic arms but with a distinct utility function that depends solely on the number of pulls. In this scenario, each strategic arm aims to maximize the total number of rounds it is selected rather than cumulative savings. The authors demonstrate that stochastic MAB algorithms are robust to strategic manipulation, allowing the player to achieve \(_{1}T\) with a sublinear regret. Under their utility definition, an arm is content to report the entirety of its reward as long as it is selected, justifying their use of the best mean \(_{1}\) as a comparator for regret.

A comparable study was conducted by , albeit with a variation in the arm's utility function, which incorporates the number of pulls alongside savings. The primary difference lies in their incorporation of a non-empty set of truthful arms, among other considerations. In this context, where arms are informed about competition and with additional assumptions conducive to specific MAB algorithms, they establish the resilience of these algorithms to strategic manipulation. However, when arms lackinformation about competition, they propose a mechanism reliant on the presence of truthful arms, ensuring a substantial revenue for the player.

The most related setting is considered by . They addressed a multi-armed bandit problem with strategic arms, employing a similar utility function, dependent on cumulative savings as outlined in (1). Their work revealed that if the player aims for the highest mean \(_{1}\) - i.e., employs a no-regret strategy - then the arms can form an undesirable equilibrium that leaves the player with a sub-linear cumulative revenue. Consequently, any incentive-unaware learning algorithm that is oblivious to the strategic nature of the arms is not resilient to such a utility definition and will generally fail to achieve low regret. Therefore, it becomes crucial to design algorithms that rely not only on sequential decision-making but also on incorporating incentive mechanisms to enhance truthfulness. In this context, Lemma 14 of  establishes that no mechanism can guarantee more than \(_{2}T\) as revenue for the player in the worst case. This observation justifies the use of \(_{2}\) as a comparator in the regret definition. As a solution, they propose a mechanism referred to here as S-ETC (Strategic Explore Then Commit). When arms have the flexibility to report any value within the range of , potentially incurring negative utility, S-ETC ensures that the player achieves a revenue of \(_{2}T\) with sub-linear cumulative regret. However, under debt-free reporting, the player incurs an additional regret of \((T^{})\) within a given Nash equilibrium among the arms.

### Objectives and Challenges

The central question we address is whether there exist, under debt-free reporting, an algorithm for the player that establishes a Nash equilibrium for the arms, guaranteeing lower regret, akin to classical non-strategic bandit settings , or not.

Dealing with strategic arms in the context of debt-free reporting poses significant challenges, especially when striving to minimize regret. Our overarching goal is to achieve a regret of \(()\). This task proves to be far from straightforward, as it cannot be resolved by merely adapting prior research. In particular, when considering the fixed design (i.e., non adaptive exploration phase) solution proposed by , the minimum bonus required to ensure truthfulness is directly proportional to the length of the exploration phase. This cannot provide a regret guarantee better than \((T^{})\).

Furthermore, this observation underscores that achieving incentive-compatibility cannot be solely contingent on the bonus, as it tends to become prohibitively costly for the player. To address this, we shall rely on the adaptive elimination of arms, introducing a secondary trade-off for the arms that enables a reduction in the bonus requirement. The less faithful an arm behaves, the more swiftly it is removed from consideration. However, this approach grants arms some control over the number of their pulls, necessitating meticulous design in other aspects of the mechanism.

### Contributions

We operate within the framework of debt-free reporting, wherein each arm is constrained to report values no higher than its observed outcomes. In this context, our contributions are:

1. We introduce a novel incentive-aware learning algorithm, denoted as Algorithm 1: S-SE, which integrates mechanism design and online learning techniques. This algorithm adeptly motivates favorable arm strategies, minimizing regret by adjusting a well-calibrated bonus. This introduces a strategic tradeoff for arms, balancing high savings through dishonesty with the risk of swift elimination.
2. We demonstrate that under Algorithm 1, there is a dominant-strategy SPE in our game, where each arm simply reports truthfully, as proven in Theorem 4.1. Under this equilibrium, the player provably obtain an expected revenue of \(_{2}T\), up to a sub-linear regret that is bounded by \((_{k=3}^{K}}+_{k=2}^{K} })\) where gaps are defined as: \(_{ij}=_{i}-_{j}, i,j\{1,,K\}\). In the worst-case scenario with significantly small gaps, the regret bound is of \(()\), as outlined in Theorem 4.2. This result outperforms the regret bound of \((T^{2/3})\) presented in the literature within the examined setting (see Table 1).
3. Under additional technical assumptions, we analyze the player's regret incurred when employing Algorithm 1 within any arms strategy profile. This analysis aims to provide a comprehensive characterization of a broad range of potential equilibria.

## 3 The Player Algorithm

This section is dedicated to the description of the algorithm used by the player. We first introduce some notations.

Let \(\{a_{k,t}\}_{k\{1,,K\}}\) be a set of \(K\) real values evaluated at time \(t\). We then denote by \(_{t}\) the vector obtained by concatenating values \(\{a_{k,t}\}_{k\{1,,K\}}\), i.e., \(_{t}^{}=[a_{1,t},a_{2,t},,a_{K,t}]\). We also denote by \(_{}\) a bijection from \(\{1,,K\}\) to itself that gives the index of the \(k^{}\) highest element in vector \(\). For example, if \(^{}=[1,\ 10,\ 4,\ 6]\), then \(_{}(1)\) represents the index of the highest value in \(\), which is \(2\). Likewise, \(_{}(2)=4\), \(_{}(3)=3\), and \(_{}(4)=1\). The inverse of this mapping provides the rank associated with a given index. When the context is clear, we can simplify the notation and omit the vector \(\), proceeding directly with \(\) and its inverse \(^{-1}\).

Let \(S\) be a set; then we denote the cardinality of \(S\) by \(|S|\).

### Algorithm Presentation

Algorithm 1 closely integrates a strategy that combines successive elimination and bonus allocation, hence it is referred to as **Strategic S**uccessive **E**limination (S-SE). It is structured into two distinct phases:

**The Initial Phase.** It is composed of the first \(\) rounds (a random stopping time defined in (8)), where an adaptive exploration technique is employed, based on round-robins on active arms leading to a series of successive eliminations aimed at identifying the best-performing arm. To achieve this, the player keeps track of the number of times \(n_{k}(t)\) each arm \(k\) is pulled up to time \(t\), defined as:

\[n_{k}(t)=_{s=1}^{t}_{[k_{s}=k]}\] (3)

Additionally, the empirical average of received rewards from arm \(k\) at time \(t\) is computed as:

\[_{k,t}=(t)}_{s=1}^{t}x_{k,s}_{[k_ {s}=k]}\] (4)

This is distinct from the empirical average of observed rewards:

\[_{k,t}=(t)}_{s=1}^{t}r_{k,s}_{[k_{s }=k]}\] (5)

which is observable only by the concerned arm \(k\). The relevant quantity to the player is \(}_{}=(_{k,t})_{k[K]}\) and its ordering mapping \(_{}_{t}}\). However, to alleviate cumbersome notations, we shall use from now on \(_{t}\) instead of \(_{}_{t}}\).

Leveraging a Hoeffding confidence bound and denoting \(_{k,t}=(t)}}\), the algorithm effectively eliminates arms that are likely to perform worse than the best arm with high probability. Hence, an arm \(k\) is eliminated at time \(t\) if the following event is happening:

\[E_{k}^{t}:=\{_{_{1}(1),t}-_{_{t}(1),t} {}_{k,t}+_{k,t}\}\] (6)

which leads to define the stopping time at which arm \(k\) is eliminated, with the convention that \(=+\),

\[_{k}:=\{t[T]:E_{k}^{t}\}\,.\] (7)

   & Tacit Model & Debt-free reporting & Bonus nature & Regret \\  S-ETC  & ✓ & ✓ & Additional rounds & \((T^{2/3})\) \\   & & & & Problem-dependent \\ S-SE & ✓ & ✓ & Payment & \(((T)/)\) \\  & & & & Problem-independent \\  & & & & \(()\) \\  

Table 1: Comparison summary between S-ETC  and S-SE: setting and results.

Consequently, the number of active arms in the set \(_{t}\) progressively reduces over time until, eventually, only the best-performing arm remains. Hence, the duration of this initial phase, represented as \(\), is a random stopping time. It is not fixed from the outset but dynamically adapted to the statistical characteristics of the arms, and it is defined as:

\[=\{t[T]:|_{t}|=1\}=\{t[T]: S_{t} \{1,,K\}\ \ \,|S_{t}|=K-1\ \ _{k S_{t}}E_{k}^{t}\}\] (8)

The Second Phase.It starts after stage \(\) and thus may never happen, as in classical MAB. In such case, for \(t>\), the best arm, i.e. \(k=_{}(1)\), is required to report an average value that surpasses at least the second-highest average achieved at the conclusion of the initial phase, denoted as \(_{_{}(2),}\). If the player confidently detects the best arm is defecting and fails to report as much, it is no longer played. Additionally, its bonus is set to zero, and the game is halted (we assume that the player can choose to end the game when the best arm defects).

At the conclusion of the game, a bonus denoted as \(_{k}\) is assigned to each arm \(k\), based on \(}_{[]}\) and its rank \(_{}^{-1}(k)\). The strategic distribution of bonuses serves as an incentive mechanism to encourage truthfulness. Drawing inspiration from real-world practices in financial exchanges within e-commerce, all bonus payments are deferred until the end of the game to ensure incentives.

**Definition 3.1**.: _Let \(_{k}\) represent the last time when arm \(k\) is active. Let \(\) and \(^{}\) respectively denote the duration of the first and second phases. Then the bonus function \(_{k}\) is defined as following:_

\[_{k}(x_{k},x_{-k}) =(n_{k}(T)_{k,T}-_{_{ _{k} 1}(2),_{k} 1}+x_{k,_{k}})_{[ _{}(1)=k]}_{[+^{} T]}\] \[+(_{_{_{k} 1}(1), _{k} 1}-_{k,_{k} 1}}+x_{k,_{k}}) _{[_{}^{-1}(k) 2]}\] (9)

Hence, contingent on their ranking at the conclusion of the first phase:The highest-performing arm:if it refrains from defecting during the second phase, it will be rewarded with a bonus that compensates for the discrepancy between its reported value and the second-best reported value. This process mirrors the dynamics observed in second-price auctions.

Suboptimal arms:they will receive a bonus inversely proportional to the difference between their means and the best arm mean. Consequently, the bonus increases as the reported values grow larger.

By implementing anytime tests to eliminate arms during the first phase (line 6 of Algorithm 1), the algorithm introduces a strategic tradeoff for arms, balancing the advantages of dishonesty with the risk of expedited elimination. This tradeoff constrains potential gains from dishonest behavior, resulting in smaller bonuses required to ensure truthful reporting and, consequently, providing a more robust guarantee. In contrast to the fixed design with a predetermined exploration phase length introduced in , where arms know from the beginning exactly how long they will be played during the first phase independently of their performances, giving them more freedom and requiring larger bonuses (proportional to the exploration phase) to ensure truthfulness, which is more costly than the bonus given in Definition 3.1.

## 4 Dominant-Strategy SPE

Algorithm 1 combines elements of a bandit algorithm and an incentive mechanism, creating a beneficial tradeoff that encourages truthful reporting as a dominant strategy for each arm in any subgame. Consequently, each arm reporting truthfully forms a dominant-strategy SPE. Specifically, we demonstrate that the utility of arm \(k\) under the truthful strategy \(_{k}^{*}\) dominates its utility under any other strategy \(_{k}\), given any fixed history(2)\(h_{k,t}=\{k_{z},r_{k_{z},z},x_{k_{z},z}\}_{z t:k_{z}=k}\{k_{z}\}_{z t}\). Formally, we define the subgame utility of arm \(k\) under strategy profile \((_{k},_{-k})\), at time \(t\) given any history \(h_{k,t-1}\) as:

\[_{k}(_{k},_{-k})[t:T h_{k,t-1}]=_{s=t}^{T}( r_{k,s}-x_{k,s})_{[k_{s}=k]}+_{k}(x_{k},x_{-k}),\] (10)

where the history \(h_{k,t-1}\) is implicitly considered within \(_{k}(x_{k},x_{-k})\).

**Theorem 4.1** (Incentive-Compatibility).: _Under Algorithm 1 and using the bonus function in Definition 3.1, for any arm \(k\), any strategy \(_{k}\), any strategy profile of other arms \(_{-k}\), at any time \(t\), and given any history \(h_{k,t-1}\), the truthful reporting strategy \(_{k}^{*}\) satisfies:_

\[_{k}(_{k},_{-k})[t:T|h_{k,t-1}]_{k} (_{k}^{*},_{-k})[t:T|h_{k,t-1}]\] (11)

Proof.: See Appendix C. 

Therefore, truthful reporting is a best response to any strategy profile \(_{-k}\). Consequently, each arm playing truthfully forms a dominant-strategy SPE.

Regret Bound.Under the dominant truthful SPE established in Theorem 4.1, we compute the player's regret over \(T\) rounds. As previously discussed, the regret is calculated in relation to \(_{2}\). Notably, a subtlety arises in the strategic scenario as compared to the non-strategic one: we must factor in the bonuses paid when calculating the regret.

**Theorem 4.2** (Regret Bound).: _Let \(T 1\). The regret of Algorithm 1 is upper-bounded as:_

\[_{T}(_{k=2}^{K}} +_{k=3}^{K}})\] (12)

_The instance-independent regret upper-bound is given by:_

\[_{T}()\] (13)

Proof.: See Appendix D.

Theorem 4.2 demonstrates that the regret bound under the considered setting is significantly lower than the regret attained in the Nash equilibrium presented in  under the same conditions. The bonus design compensates for any gains that may occur from being untruthful, establishing the truthful SPE. Under the latter, the algorithm is the classical successive elimination algorithm designed for the non-strategic MAB, whose worst-case regret is known to match that of Theorem 4.2. At the same time, the successive eliminations create a trade-off between high savings from dishonesty and swift elimination, which reduces the bonus values needed in a way that they do not incur an additional harmful cost for the regret.

A second observation is that the instance-dependent regret matches the one of a regular MAB \((_{k=2}^{K}})\) up to a second additive term of the same order \((_{k=3}^{K}})\). This second term comes from the additional need to correctly estimate the value of the second-best arm to make sure to "bill" accurately the best arm during the second phase. The less accurate the estimation of \(_{2}\), the larger the deviation the best arm could have in the second phase without being detected. As the gain from potential deviation is incorporated in the bonus to ensure truthfulness, the accuracy of the estimation of \(_{2}\) impacts the regret.

It's worth noting that the truthful SPE highlighted in Theorem 4.1 might raise questions about the regret comparator \(_{2}\). In other words, if truthfulness exists, why can't we expect to achieve rewards of \(_{1}\)? While it's true that arms are incentivized to be truthful, this motivation is realized through the allocation of bonuses at the conclusion. Bonuses are part of the arm's utility and contribute to making truthful reporting the best response that maximizes utility. Specifically, the best arm is incentivized with a bonus of \((T_{12})\). Since the bonus is integral to the regret calculation, we compute the regret relative to \(_{2}\) in a manner similar to the approach justified in Lemma 14 of .

Tightness of Regret.For simplicity, let's consider three arms with \(_{1}_{2}_{3}\). The optimal arm only needs to report a slightly higher value than the second-best arm to be chosen by the player as the superior option. Consequently, all efficient low-regret mechanisms must ensure truthfulness, at least for the two best arms. Under debt-free reporting, where no arm can falsely claim to be better than it actually is, distinguishing between the two best arms incurs a regret of \(}\). Additionally, distinguishing between the second and the third arm is required to estimate the second-best mean accurately, resulting in a regret of \(}\). However, the third arm does not need to be truthful; we only need to distinguish it from the second-best arm. Therefore, in general, the incurred regret will be of order \(}\), where \(_{23}\) is the difference between the true mean \(_{2}\) and an effective mean \(_{3}\), which can be different from \(_{3}\). Ideally, \(_{3}=0\) to minimize the corresponding regret i.e incentivizing non-truthful strategies for arms \(k 3\), allowing the player to identify the second-best arm more quickly. Nonetheless, in all cases, the regret will be of the same order as the one described in (12). In Appendix F, we conducted an experimental analysis on simulated data, highlighting the tightness of the regret bounds by evaluating several strategies.

Utilities Bounds.Considering the game-like dynamics between the player and the arms, alongside the regret bounds, it is natural to examine bounds on the utilities of the arms, as illustrated in the following corollary.

**Corollary 4.1** (Utilities Upper Bound).: _Under the truthful SPE, the utility of any arm is upper bounded as follows:_

\[ k\{2,,K\}[_{k}] (})[_{1}]=(T_{12})\] (14)

Proof.: See Appendix D.1. 

## 5 Regret Analysis Across Arbitrary Strategy Profiles

In the previous section, we assessed the regret of Algorithm 1 when the arms adhere to the dominant strategy SPE. General profiles and equilibria in extensive games are known to be intractable . Specifically, for an arbitrary strategy profile that is not truthful, the values reported by the arms are not i.i.d., complicating the analysis as tools like Hoeffding's inequality cannot be directly applied. Consequently, determining the algorithm's output becomes non-trivial. To address this, we introduce an additional technical assumption regarding the boundedness of gains through dishonesty. Withthis assumption, we can analyze the algorithm under a wide range of strategy profiles, not just the dominant one previously discussed.

Fix an arbitrary strategy profile \(\). For each arm \(k\), we define \(S_{k}^{T}=_{s=1}^{T}(r_{k_{s},s}-x_{k_{s},s})_{[k_{s}=k]}\) as the cumulative savings of arm \(k\) up to round \(T\). We assume the existence of some upper-bound \(M\) on cumulative saving, i.e., such that for all arms \(k[K]\) and for any strategy \(\), it must hold that \(S_{k}^{T} M\). We define the effective mean under strategy \(\) as \(_{k}^{}\), and let \(^{}=(_{k}^{})_{k[K]}\). Therefore, we define \(_{k}^{}\) as the difference between the highest effective mean under strategy \(\) and the effective mean of arm \(k\), given by : \(_{k}^{}=_{_{^{}}(1)}^{}-_{k} ^{}\). Similarly, \(_{k}^{}\) represents the difference between the second-highest effective mean and the effective mean of arm \(k\) under strategy \(\), defined as: \(_{k}^{}=_{_{^{}}(2)}^{}-_{k}^{}\). We show that when arms use the strategy profile \(\), even under non-i.i.d. reported variables, Algorithm 1 outputs the second highest effective mean, and the regret is provided by the following theorem.

**Theorem 5.1**.: _For any arbitrary strategy profile \(\) with \(M\)-bounded savings, the regret of Algorithm 1 is bounded by:_

\[_{T}^{} =[_{t=1}^{T}(_{_{^{}}(2)}^{}-x_{k_{t},t})+_{k=1}^{K}_{k}(x_{k},x_{-k})]\] \[(_{k:_{^{}}^{-1}(k)  2}\{M,^{}}\}+_{k: _{^{}}^{-1}(k) 3}\{M,_{k}^{}}\})\] (15)

Proof.: See Appendix E.2. 

The upper bound of savings \(M\) determines the regret with regard to the second-highest effective mean of any strategy \(\), as shown by Theorem 5.1. In particular, it is demonstrated to be sublinear in \(T\) provided that \(M=o(T)\). The upper bound exhibits a similar form to the problem-dependent upper bound in Theorem 4.2. Specifically, under the truthful equilibrium \(^{}\) where \(M=0\), \(_{k}^{^{}}=_{k}\), \(_{k}^{^{}}=_{1k}\), and \(_{k}^{^{}}=_{2k}\), we retrieve the result from (12). Additionally, in the debt-free reporting setting, it is observed that for any strategy \(\), \(_{_{^{}}(2)}^{}_{2}\). This suggests that the player's revenue drops when arms diverge from the honest SPE in a way that affects the second-highest mean.

Sensitivity of Algorithm 1 around truthful SPE.For a strategy profile where \(M=o((T))\), the difference between the true second highest mean \(_{2}\) and the effective second highest mean \(_{2}^{}\) is of the same order as the regret. In this scenario, the player's revenue is \(_{2}T\), up to a sublinear regret \(_{T}^{}=(_{T})\). Hence, even though the savings are not zero, we retrieve results comparable to those with truthful reporting.

## 6 Limitations and Future Work

The algorithm S-SE, presented as an efficient solution to the strategic MAB, combines elements of successive elimination with a well-tuned incentive. This raises the question: is it possible to adapt any no-regret MAB algorithm, such as UCB, with a suitable incentive to solve the same problem? A complete and detailed answer to such a question is reserved for future work. However, our intuition is that this would be challenging because, in a similar setting,  argue that a truthful mechanism under the strategic MAB problem needs to be exploration-separated. In other words, at any given time step, either it exploits (action depends on current knowledge) or it explores (action allows to gain knowledge but does not depend on current information), but not both simultaneously. This observation suggests that a tailored version of UCB (at least without additional assumptions) may not be suitable.

Despite the similarity between the algorithm of  and our algorithm S-SE, both being strategic versions of exploration-separated algorithms, there is a major difference in the nature of the games generated by both algorithms. In , all arms are played sequentially for a predetermined number of explorations each before committing to the best arm, creating a simultaneous-like game. In contrast, Algorithm 1 uses an adaptive design permitting lower regret. However, this adaptability involves an extensive game requiring a more complicated analysis based on the concept of SPE.

It's also worth mentioning that the improvement in regret doesn't depend on the type of incentives, whether they are additional bonus rounds or bonus payments. This is because bonuses are subtractedfrom player revenue and considered in the regret calculation, irrespective of their nature. Therefore, replacing the additional rounds in  with payments will not lead to an improvement in regret. However, in this paper, actual payments are chosen to keep the intuition more direct and facilitate the presentation.

## 7 Conclusion

We addressed the challenge of strategic multi-armed bandit problems under debt-free reporting. Arms aim to maximize their own utilities by manipulating the reported values to the player, resulting in a more complex problem compared to the standard setting due to the game-like dynamics involved. We proved that by employing a strategic variant of successive elimination algorithm, it is possible to design a bonus-based incentive structure, resulting in a dominant-strategy SPE for arms where they report truthfully. The dynamics of the successive elimination process, particularly the trade-off between high savings and swift elimination, allow for the implementation of cost-effective bonuses, leading to low regret.