# Differentiable Random Partition Models

Thomas M. Sutter, Alain Ryser, Joram Liebeskind, Julia E. Vogt

Department of Computer Science

ETH Zurich

Equal Contribution. Correspondence to {thomas.sutter,alain.ryser}@inf.ethz.ch

###### Abstract

Partitioning a set of elements into an unknown number of mutually exclusive subsets is essential in many machine learning problems. However, assigning elements, such as samples in a dataset or neurons in a network layer, to an unknown and discrete number of subsets is inherently non-differentiable, prohibiting end-to-end gradient-based optimization of parameters. We overcome this limitation by proposing a novel two-step method for inferring partitions, which allows its usage in variational inference tasks. This new approach enables reparameterized gradients with respect to the parameters of the new random partition model. Our method works by inferring the number of elements per subset and, second, by filling these subsets in a learned order. We highlight the versatility of our general-purpose approach on three different challenging experiments: variational clustering, inference of shared and independent generative factors under weak supervision, and multitask learning.

## 1 Introduction

Partitioning a set of elements into subsets is a classical mathematical problem that attracted much interest over the last few decades (Rota, 1964; Graham et al., 1989). A partition over a given set is a collection of non-overlapping subsets such that their union results in the original set. In machine learning (ML), partitioning a set of elements into different subsets is essential for many applications, such as clustering (Bishop and Svensen, 2004) or classification (De la Cruz-Mesia et al., 2007).

Random partition models (RPM, Hartigan, 1990) define a probability distribution over the space of partitions. RPMs can explicitly leverage the relationship between elements of a set, as they do not necessarily assume _i.i.d._ set elements. On the other hand, most existing RPMs are intractable for large datasets (MacQueen, 1967; Plackett, 1975; Pitman, 1996) and lack a reparameterization scheme, prohibiting their direct use in gradient-based optimization frameworks.

In this work, we propose the differentiable random partition model (DRPM), a fully-differentiable relaxation for RPMs that allows reparametrizable sampling. The DRPM follows a two-stage procedure: first, we model the number of elements per subset, and second, we learn an ordering of the elements with which we fill the elements into the subsets. The DRPM enables the integration of partition models into state-of-the-art ML frameworks and learning RPMs from data using stochastic optimization.

We evaluate our approach in three experiments, demonstrating the proposed DRPM's versatility and advantages. First, we apply the DRPM to a variational clustering task, highlighting how the reparametrizable sampling of partitions allows us to learn a novel kind of Variational Autoencoder (VAE, Kingma and Welling, 2014). By leveraging potential dependencies between samples in a dataset, DRPM-based clustering overcomes the simplified _i.i.d._ assumption of previous works, which used categorical priors (Jiang et al., 2016). In our second experiment, we demonstrate how to retrieve sets of shared and independent generative factors of paired images using the proposed DRPM. Incontrast to previous works (Bouchacourt et al., 2018; Hosoya, 2018; Locatello et al., 2020), which rely on strong assumptions or heuristics, the DRPM enables end-to-end inference of generative factors. Finally, we perform multitask learning (MTL) by using the DRPM as a building block in a deterministic pipeline. We show how the DRPM learns to assign subsets of network neurons to specific tasks. The DRPM can infer the subset size per task based on its difficulty, overcoming the tedious work of finding optimal loss weights (Kurin et al., 2022; Xin et al., 2022).

To summarize, we introduce the DRPM, a novel differentiable and reparametrizable relaxation of RPMs. In extensive experiments, we demonstrate the versatility of the proposed method by applying the DRPM to clustering, inference of generative factors, and multitask learning.

## 2 Related Work

Random Partition ModelsPrevious works on RPMs include product partition models (Hartigan, 1990), species sampling models (Pitman, 1996), and model-based clustering approaches (Bishop and Svensen, 2004). Further, Lee and Sang (2022) investigate the balancedness of subset sizes of RPMs. They all require tedious manual adjustment, are non-differentiable, and are, therefore, unsuitable for modern ML pipelines. A fundamental RPM application is clustering, where the goal is to partition a given dataset into different subsets, the clusters. In contrast to many existing approaches (Yang et al., 2019; Sarfraz et al., 2019; Cai et al., 2022), we consider cluster assignments as random variables, allowing us to treat clustering from a variational perspective. Previous works in variational clustering (Jiang et al., 2016; Dilokthanakul et al., 2016; Manduchi et al., 2021) implicitly define RPMs to perform clustering. They compute partitions in a variational fashion by making _i.i.d._ assumptions about the samples in the dataset and imposing of assignments of the clusters to data points during training. A problem related to set partitioning is the earth mover's distance problem (EMD, Monge, 1781; Rubner et al., 2000). However, EMD aims to assign a set's elements to different subsets based on a cost function and given subset sizes. Iterative solutions to the problem exist (Sinkhorn, 1964), and various methods have recently been proposed, e.g., for document ranking (Adams and Zemel, 2011) or permutation learning (Santa Cruz et al., 2017; Mena et al., 2018; Cuturi et al., 2019).

Differentiable and Reparameterizable Discrete DistributionsFollowing the proposition of the Gumbel-Softmax trick (GST, Jang et al., 2016; Maddison et al., 2017), interest in research around continuous relaxations for discrete distributions and non-differentiable algorithms rose. The GST enabled the reparameterization of categorical distributions and their integration into gradient-based optimization pipelines. Based on the same trick, Sutter et al. (2023) propose a differentiable formulation for the multivariate hypergeometric distribution. Multiple works on differentiable sorting procedures and permutation matrices have been proposed, e.g., Linderman et al. (2018); Prillo and Eisenschlos (2020); Petersen et al. (2021). Further, Grover et al. (2019) described the distribution over permutation matrices \(p()\) for a permutation matrix \(\) using the Plackett-Luce distribution (PL, Luce, 1959; Plackett, 1975). Prillo and Eisenschlos (2020) proposed a computationally simpler variant of Grover et al. (2019). More examples of differentiable relaxations include the top-\(k\) elements selection procedure (Xie and Ermon, 2019), blackbox combinatorial solvers (Pogancic et al., 2019), implicit likelihood estimations (Niepert et al., 2021), and \(k\)-subset sampling (Ahmed et al., 2022).

Figure 1: Illustration of the proposed DRPM method. We first sample a permutation matrix \(\) and a set of subset sizes \(\) separately in two stages. We then use \(\) and \(\) to generate the assignment matrix \(Y\), the matrix representation of a partition \(\).

## 3 Preliminaries

Set PartitionsA partition \(=(_{1},,_{K})\) of a set \([n]=\{1,,n\}\) with \(n\) elements is a collection of \(K\) subsets \(_{k}[n]\) where \(K\) is _a priori_ unknown (Mansour and Schork, 2016). For a partition \(\) to be valid, it must hold that

\[_{1}_{K}=[n]\ \ \ \ \  k l:\ _{k}_{l}=\] (1)

In other words, every element \(i[n]\) has to be assigned to precisely one subset \(_{k}\). We denote the size of the \(k\)-th subset \(_{k}\) as \(n_{k}=|_{k}|\). Alternatively, we can describe a partition \(\) through an assignment matrix \(Y=[_{1},,_{K}]^{T}\{0,1\}^{K n}\). Every row \(_{k}\{0,1\}^{1 n}\) is a multi-hot vector, where \(_{ki}=1\) assigns element \(i\) to subset \(_{k}\).

Within the scope of our work, we view a partition of a set of \(n\) elements as a special case of the urn model. Here, the urn contains marbles with \(n\) different colors, where each color corresponds to a subset in the partition. For each color, there are \(n\) marbles corresponding to the potential elements of their color/subset. To derive a partition, we sample \(n\) marbles without replacement from the urn and register the order in which we draw the colors. The color of the \(i\)-th marble then determines the subset to which element \(i\) corresponds. Furthermore, we can constrain the partition to only \(K\) subsets by taking an urn with only \(K\) different colors.

Probability distribution over subset sizesThe multivariate non-central hypergeometric distribution (MVHG) describes sampling without replacement and allows to skew the importance of groups with an additional importance parameter \(\)(Fisher, 1935; Wallenius, 1963; Chesson, 1976). The MVHG is an urn model and is described by the number of different groups \(K\), the number of elements in the urn of every group \(=[m_{1},,m_{K}]^{K}\), the total number of elements in the urn \(_{k=1}^{K}m_{k}\), the number of samples to draw from the urn \(n_{0}\), and the importance factor for every group \(=[_{1},,_{K}]_{0+}^{K}\)(Johnson, 1987). Then, the probability of sampling \(=\{n_{1},,n_{K}\}\), where \(n_{k}\) describes the number of elements drawn from group \(K\) is

\[p(;,)=}_{k=1}^{K}}{n_{ k}}_{k}^{n_{k}}\] (2)

where \(P_{0}\) is a normalization constant. Hence, the MVHG \(p(;,)\) allows us to model dependencies between different elements of a set since drawing one element from the urn influences the probability of drawing one of the remaining elements, creating interdependence between them. For the rest of the paper, we assume \(\ m_{k}:m_{k}=n\). We thus use the shorthand \(p(;)\) to denote the density of the MVHG. We refer to Appendix A.1 for more details.

Probability distribution over Permutation MatricesLet \(p()\) denote a distribution over permutation matrices \(\{0,1\}^{n n}\). A permutation matrix \(\) is doubly stochastic (Marcus, 1960), meaning that its row and column vectors sum to \(1\). This property allows us to use \(\) to describe an order over a set of \(n\) elements, where \(_{ij}=1\) means that element \(j\) is ranked at position \(i\) in the imposed order. In this work, we assume \(p()\) to be parameterized by scores \(_{+}^{n}\), where each score \(s_{i}\) corresponds to an element \(i\). The order given by sorting \(\) in decreasing order corresponds to the most likely permutation in \(p(;)\). Sampling from \(p(;)\) can be achieved by resampling the scores as \(_{i}= s_{i}+g_{i}\) where \(g_{i}(0,)\) for fixed scale \(\), and sorting them in decreasing order. Hence, resampling scores \(\) enables the resampling of permutation matrices \(\). The probability over orderings \(p(;)\) is then given by (Thurstone, 1927; Luce, 1959; Plackett, 1975; Yellott, 1977)

\[p(;)=p((})_{1}(})_{n})= )_{1}}{Z})_{2}}{Z-()_{1}})_{n}}{Z-_{j=1}^{n-1}()_{j}}\] (3)

where \(\) is a permutation matrix and \(Z=_{i=1}^{n}s_{i}\). The resulting distribution is a Plackett-Luce (PL) distribution (Luce, 1959; Plackett, 1975) if and only if the scores \(\) are perturbed with noise drawn from Gumbel distributions with identical scales (Yellott, 1977). For more details, we refer to Appendix A.2).

## 4 A two-stage Approach to Random Partition Models

We propose the DRPM \(p(Y;,)\), a differentiable and reparameterizable two-stage Random Partition Model (RPM). The proposed formulation separately infers the number of elements \(i\) per subset\(_{0}^{K}\), where \(_{k=1}^{K}n_{k}=n\), and the assignment of elements to subsets \(_{k}\) by inducing an order on the \(n\) elements and filling \(_{1},...,_{K}\) sequentially in this order. To model the order of the elements, we use a permutation matrix \(=[_{1},,_{n}]^{T}\{0,1\}^{n n}\), from which we infer \(Y\) by sequentially summing up rows according to \(\). Note that the doubly-stochastic property of all permutation matrices \(\) ensures that the columns of \(Y\) remain one-hot vectors, assigning every element \(i\) to precisely one of the \(K\) subsets. At the same time, the \(k\)-th row of \(Y\) corresponds to an \(n_{k}\)-hot vector \(_{k}\) and therefore serves as a subset selection vector, i.e.

\[_{k}=_{i=_{k}+1}^{_{k}+n_{k}}_{i},\ \ _{k}=_{=1}^{k-1}n_{}\] (4)

such that \(Y=[_{1},,_{K}]^{T}\). Additionally, Figure 1 provides an illustrative example. Note that \(K\) defines the maximum number of possible subsets, and not the effective number of non-empty subsets, because we allow \(_{k}\) to be the empty set \(\)(Mansour and Schork, 2016). We base the following Proposition 4.1 on the MVHG distribution \(p(;)\) for the subset sizes \(\) and the PL distribution \(p(;)\) for assigning the elements to subsets. However, the proposed two-stage approach to RPMs is not restricted to these two classes of probability distributions.

**Proposition 4.1** (Two-stage Random Partition Model).: _Given a probability distribution over subset sizes \(p(;)\) with \(_{0}^{K}\) and distribution parameters \(_{+}^{K}\) and a PL probability distribution over random orderings \(p(;)\) with \(\{0,1\}^{n n}\) and distribution parameters \(_{+}^{n}\), the probability mass function \(p(Y;,)\) of the two-stage RPM is given by_

\[p(Y;,)=p(_{1},,_{K};,)=p( ;)_{_{Y}}p(;)\] (5)

_where \(_{Y}=\{:_{k}=_{i=_{k}+1}^{_{k}+n_{k}}_{i},k=1, ,K\}\), and \(_{k}\) and \(_{k}\) as in Equation (4)._

In the following, we outline the proof of Proposition 4.1 and refer to Appendix B for a formal derivation. We calculate \(p(Y;,)\) as a probability of subsets \(p(_{1},,_{K};,)\), which we compute sequentially over subsets, i.e.

\[p(_{1},,_{K};,)=p(_{1};, ) p(_{K}_{<K};,),\] (6)

where \(_{<k}=[_{1},,_{k-1}]\) and

\[p(_{k}_{<k};,)=p(n_{k} n_{<k};)_{_{_{k}}}p( n_{k},_{<k};),\] (7)

where \(_{_{k}}\) in Equation (7) is the set of all subset permutations of elements \(i_{k}\). A subset permutation matrix \(\) represents an ordering over only \(n_{k}\) out of the total \(n\) elements. The probability \(p(_{k}_{<k};,)\) describes the probability of a subset of a given size \(n_{k}\) by marginalizing over the probabilities of all subset permutations \(p( n_{k},_{<k};)\). Hence, the sum over all \(p( n_{k},_{<k};)\) makes \(p(_{k}_{<k};,)\) invariant to the ordering of elements \(i_{k}\)(Xie and Ermon, 2019). Note that in a slight abuse of notation, we use \(p( n_{k},_{<k};,)\) as the probability of a subset permutation \(\) given that there are \(n_{k}\) elements in \(_{k}\) and thus \(\{0,1\}^{n_{k} n}\).

The probability of a subset permutation matrix \(p( n_{k},_{<k};)\) describes the probability of drawing the elements \(i_{k}\) in the order defined by the subset permutation matrix \(\) given that the elements in \(_{<k}\) are already determined. Hence, we condition on the subsets \(_{<k}\). This property follows from Luce's choice axiom (LCA, Luce, 1959). Additionally, we condition on \(n_{k}\), the size of the subset \(_{k}\). The probability of a subset permutation is given by

\[p( n_{k},_{<k};)=_{i=1}^{n_{k}} )_{i}}{Z_{k}-_{j=1}^{i-1}()_{j}}\] (8)

In contrast to the distribution over permutations matrices \(p(;)\) in Equation (3), we compute the product over \(n_{k}\) terms and have a different normalization constant \(Z_{k}\), which is the sum over the scores \(s_{i}\) of all elements \(i_{k}\). Although we induce an ordering over all elements \(i\) by using a permutation matrix \(\), the probability \(p(_{k}_{<k};,)\) is invariant to intra-subset orderings of elements \(i_{k}\). Finally, we arrive at Equation (5) by substituting Equation (7) into Equation (6),and applying the definition of the conditional probability \(p(;)=_{k=1}^{K}p(n_{k} n_{<k};)\) and by reshuffling indices \(_{_{Y}}p(;)=_{k=1}^{K}_{_{_{k}}}p(  n_{k},_{<k};)\).

Note that in contrast to previous RPMs, which often need exponentially many distribution parameters (Plackett, 1975), the proposed two-stage approach to RPMs only needs \((n+K)\) parameters to create an RPM for \(n\) elements: the score parameters \(_{+}^{n}\) and the group importance parameters \(_{+}^{K}\).

Finally, to sample from the two-stage RPM of Proposition 4.1 we apply the following procedure: First sample \( p(;)\) and \( p(;)\). From \(\) and \(\), compute partition \(Y\) by summing the rows of \(\) according to \(\) as described in Equation (4) and illustrated in Figure 1.

### Approximating the Probability Mass Function

The number of permutations per subset \(|_{_{k}}|\) scales factorially with the subset size \(n_{k}\), i.e. \(|_{_{k}}|=n_{k}!\). Consequently, the number of valid permutation matrices \(|_{Y}|\) is given as a function of \(\), i.e.

\[|_{Y}|=_{k=1}^{K}|_{_{k}}|=_{k=1}^{K}n_{k}!\] (9)

Although Proposition 4.1 describes a well-defined distribution for \(p(Y;,)\), it is in general computationally intractable due to Equation (9). In practice, we thus approximate \(p(Y;,)\) using the following Lemma.

**Lemma 4.2**.: \(p(Y;,)\) _can be upper and lower bounded as follows_

\[_{Y}:\;p(;)p(;)\ \ p(Y;,)\ \ |_{Y}|p(;)_{}p(;)\] (10)

We provide the proof in Appendix B. Note that from Equation (3) we see that \(_{}p(;)=p(_{};)\), where \(_{}\) is the permutation that results from sorting the unperturbed scores \(\).

### The Differentiable Random Partition Model

To incorporate our two-stage RPM into gradient-based optimization frameworks, we require that efficient computation of gradients is possible for every step of the method. The following Lemma guarantees differentiability, allowing us to train deep neural networks with our method in an end-to-end fashion:

**Lemma 4.3** (DRPM).: _A two-stage RPM is differentiable and reparameterizable if the distribution over subset sizes \(p(;)\) and the distribution over orderings \(p(;)\) are differentiable and reparameterizable._

We provide the proof in Appendix B. Note that Lemma 4.3 enables us to learn variational posterior approximations and priors using Stochastic Gradient Variational Bayes (SGVB, Kingma and Welling, 2014). In our experiments, we apply Lemma 4.3 using the recently proposed differentiable formulations of the MVHG (Sutter et al., 2023) and the PL distribution (Grover et al., 2019), though other choices would also be valid.

## 5 Experiments

We demonstrate the versatility and effectiveness of the proposed DRPM in three different experiments. First, we propose a novel generative clustering method based on the DRPM, which we compare against state-of-the-art variational clustering methods and demonstrate its conditional generation capabilities. Then, we demonstrate how the DRPM can infer shared and independent generative factors under weak supervision. Finally, we apply the DRPM to multitask learning (MTL), where the DRPM enables an adaptive neural network architecture that partitions layers based on task difficulty2.

### Variational Clustering with Random Partition Models

In our first experiment, we introduce a new version of a Variational Autoencoder (VAE, Kingma and Welling, 2014), the DRPM Variational Clustering (DRPM-VC) model. The DRPM-VC enables clustering and unsupervised conditional generation in a variational fashion. To that end, we assume that each sample \(\) of a dataset \(X\) is generated by a latent vector \(^{l}\), where \(l\) is the latent space size. Traditional VAEs would then assume that all latent vectors \(\) are generated by a single Gaussian prior distribution \((,_{l})\). Instead, we assume every \(\) to be sampled from one of \(K\) different latent Gaussian distributions \((_{k},(_{k})),k=1,,K\), with \(_{k},_{k}^{l}\). Further, note that similar to an urn model (Section 3), if we draw a batch from a given finite dataset with samples from different clusters, the cluster assignments within that batch are not entirely independent. Since there is only a finite number of samples per cluster, drawing a sample from a specific cluster decreases the chance of drawing a sample from that cluster again, and the distribution of the number of samples drawn per cluster will follow an MVHG distribution. Previous work on variational clustering proposes to model the cluster assignment \(\{0,1\}^{K}\) of each sample \(\) through independent categorical distributions (Jiang et al., 2016), which might thus be over-restrictive and not correctly reflect reality. Instead, we propose explicitly modeling the dependency between the \(\) of different samples by assuming they are drawn from an RPM. Hence, the generative process leading to \(X\) can be summarized as follows: First, the cluster assignments are represented as a partition matrix \(Y\) and sampled from our DRPM, i.e., \(Y p(Y;,)\). Given an assignment \(\) from \(Y\), we can sample the respective latent variable \(\), where \((_{},(_{}))\), \(^{l}\). Note that we use the notational shorthand \(_{}_{()}\). Like in vanilla VAEs, we infer \(\) by independently passing the corresponding \(\) through a decoder model. Assuming this generative process, we derive the following evidence lower bound (ELBO) for \(p(X)\):

\[_{ELBO}=_{ X}_{q(|)}[ p (|)]-_{ X}_{q(Y|X)}[KL[q( |)||p(|Y)]]-KL[q(Y|X)||p(Y)]\]

Note that computing \(KL[q(Y|X)||p(Y)]\) directly is computationally intractable, and we need to upper bound it according to Lemma 4.2. For an illustration of the generative assumptions and more details on the ELBO, we refer to Appendix C.2.

    &  &  \\   & NMI & ARI & ACC & NMI & ARI & ACC \\  GMM & \(0.32_{ 0.01}\) & \(0.22_{ 0.02}\) & \(0.41_{ 0.01}\) & \(0.49_{ 0.01}\) & \(0.33_{ 0.00}\) & \(0.44_{ 0.01}\) \\ Latent GMM & \(0.86_{ 0.02}\) & \(0.83_{ 0.06}\) & \(0.88_{ 0.07}\) & \(0.60_{ 0.00}\) & \(0.47_{ 0.01}\) & \(0.62_{ 0.01}\) \\ VaDE & \(0.84_{ 0.01}\) & \(0.76_{ 0.05}\) & \(0.82_{ 0.04}\) & \(0.56_{ 0.02}\) & \(0.40_{ 0.04}\) & \(0.56_{ 0.03}\) \\ DRPM-VC & \(_{ 0.01}\) & \(_{ 0.03}\) & \(_{ 0.02}\) & \(_{ 0.00}\) & \(_{ 0.01}\) & \(_{ 0.00}\) \\   

Table 1: We compare the clustering performance of the DRPM-VC on test sets of MNIST and FMNIST between Gaussian Mixture Models (GMM), GMM in latent space (Latent GMM), and Variational Deep Embedding (VaDE). We measure performance in terms of the Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and cluster accuracy (ACC) over five seeds and put the best model in bold.

Figure 2: A sample drawn from a DRPM-VC model trained on FMNIST. On top is the sampled partition with the cluster assignments, and on the bottom are generated images corresponding to the sampled assignment matrix. The DRPM-VC learns consistent clusters for different pieces of clothing and can generate new samples of each cluster with great variability.

To assess the clustering performance, we train our model on two different datasets, namely MNIST (LeCun et al., 1998) and Fashion-MNIST (FMNIST, Xiao et al., 2017), and compare it to three baselines. Two of the baselines are based on a Gaussian Mixture Model, where one is directly trained on the original data space (GMM), whereas the other takes the embeddings from a pretrained encoder as input (Latent GMM). The third baseline is Variational Deep Embedding (VaDE, Jiang et al., 2016), which is similar to the DRPM-VC but assumes _i.i.d._ categorical cluster assignments. For all methods except GMM, we use the weights of a pretrained encoder to initialize the models and priors at the start of training. We present the results of these experiments in Table 1. As can be seen, we outperform all baselines, indicating that modeling the inherent dependencies implied by finite datasets benefits the performance of variational clustering. While achieving decent clustering performance, another benefit of variational clustering methods is that their reconstruction-based nature intrinsically allows unsupervised conditional generation. In Figure 2, we present the result of sampling a partition and the corresponding generations from the respective clusters after training the DRPM-VC on FMNIST. The model produces coherent generations despite not having access to labels, allowing us to investigate the structures learned by the model more closely. We refer to Appendix C.2 for more illustrations of the learned clusters, details on the training procedure, and ablation studies.

### Variational Partitioning of Generative Factors

Data modalities not collected as _i.i.d._ samples, such as consecutive frames in a video, provide a weak-supervision signal for generative models and representation learning (Sutter et al., 2023). Here, on top of learning meaningful representations of the data samples, we are also interested in discovering the relationship between coupled samples. If we assume that the data is generated from underlying generative factors, weak supervision comes from the fact that we know that certain factors are shared between coupled pairs while others are independent. The supervision is weak because we neither know the underlying generative factors nor the number of shared and independent factors. In such a setting, we can use the DRPM to learn a partition of the generative factors and assign them to be either shared or independent.

In this experiment, we use paired frames \(=[_{1},_{2}]\) from the _mpi3d_ dataset (Gondal et al., 2019). Every pair of frames shares a subset of its seven generative factors. We introduce the DRPM-VAE, which models the division of the latent space into shared and independent latent factors as RPM. We add a posterior approximation \(q(Y)\) and additionally a prior distribution of the form \(p(Y)\). The model maximizes the following ELBO on the

    & \(n_{s}=0\) & =1\)} & =3\)} & =5\)} \\   & I & S & I & S & I & S & I \\  Label & \(0.14 0.01\) & \(0.19 0.03\) & \(0.16 0.01\) & \(0.10 0.00\) & \(0.23 0.01\) & \(0.34 0.00\) & \(0.00 0.00\) \\  Ada & \(0.12 0.01\) & \(0.19 0.01\) & \(0.15 0.01\) & \(0.10 0.03\) & \(0.22 0.02\) & \(0.33 0.03\) & \(0.00 0.00\) \\  HG & \(0.18 0.01\) & \(0.22 0.05\) & \(0.19 0.01\) & \(0.08 0.02\) & \(0.28 0.01\) & \(0.28 0.01\) & \(0.01 0.00\) \\  DRPM & \( 0.02\) & \( 0.07\) & \( 0.01\) & \( 0.01\) & \( 0.02\) & \( 0.03\) & \(0.01 0.00\) \\   

Table 2: Partitioning of Generative Factors. We evaluate the learned latent representations of the four methods (Label-VAE, Ada-VAE, HG-VAE, DRPM-VAE) with respect to the shared (S) and independent (I) generative factors. We do this by fitting linear classifiers on the shared and independent dimensions of the representation, predicting the respective generative factors. We report the results in adjusted balanced accuracy (Sutter et al., 2023) across five seeds.

Figure 3: The mean squared errors between the estimated number of shared factors \(_{s}\) and the true number of shared factors \(n_{s}\) across five seeds for the Label-VAE, Ada-VAE, HG-VAE, and DRPM-VAE.

marginal log-likelihood of images through a VAE (Kingma and Welling, 2014):

\[_{ELBO} =_{j=1}^{2}_{q(_{s},_{j},Y|)}[  p(_{j}_{s},_{j})]\] (11) \[-_{q(Y|)}[KL[q(_{s},_{ 1},_{2} Y,)||p(_{s},_{1},_{2})]] -KL[q(Y)||\;p(Y)]\]

Similar to the ELBO for variational clustering in Section 5.1, computing \(KL[q(Y)||\;p(Y)]\) directly is intractable, and we need to bound it according to Lemma 4.2.

We compare the proposed DRPM-VAE to three methods, which only differ in how they infer shared and latent dimensions. While the Label-VAE (Bouchacourt et al., 2018; Hosoya, 2018) assumes that the number of independent factors is known, the Ada-VAE (Locatello et al., 2020) relies on a heuristic-based approach to infer shared and independent latent factors. Like in Locatello et al. (2020) and Sutter et al. (2023), we assume a single known factor for Label-VAE in all experiments. HG-VAE (Sutter et al., 2023) also relies on the MVHG to model the number of shared and independent factors. Unlike the proposed DRPM-VAE approach, HG-VAE must rely on a heuristic to assign latent dimensions to shared factors, as the MVHG only allows to model the number of shared and independent factors but not their position in the latent vector. We use the code from Locatello et al. (2020) and follow the evaluation in Sutter et al. (2023). We refer to Appendix C.3 for details on the ELBO, the setup of the experiment, the implementation, and an illustration of the generative assumptions.

We evaluate all methods according to their ability to estimate the number of shared generative factors (Figure 3) and how well they partition the latent representations into shared and independent factors (Table 2). Because we have access to the data-generating process, we can control the number of shared \(n_{s}\) and independent \(n_{i}\) factors. We compare the methods on four different datasets with \(n_{s}\{0,1,3,5\}\). In Figure 3, we demonstrate that the DRPM-VAE accurately estimates the true number of shared generative factors. It matches the performance of HG-VAE and outperforms the other two baselines, which consistently overestimate the true number of shared factors. In Table 2, we see a considerable performance improvement compared to previous work when assessing the learned latent representations. We attribute this to our ability to not only estimate the subset sizes of latent and shared factors like HG-VAE but also learn to assign specific latent dimensions to the corresponding shared or independent representations. Thus, the DRPM-VAE dynamically learns more meaningful representations and can better separate and infer the shared and independent subspaces for all dataset versions.

The DRPM-VAE provides empirical evidence of how RPMs can leverage weak supervision signals by learning to maximize the data likelihood while also inferring representations that capture the relationship between coupled data samples. Additionally, we can explicitly model the data-generating process in a theoretically grounded fashion instead of relying on heuristics.

### Multitask Learning

Many ML applications aim to solve specific tasks, where we optimize for a single objective while ignoring potentially helpful information from related tasks. Multitask learning (MTL) aims to improve the generalization across all tasks, including the original one, by sharing representations between related tasks (Caruana, 1993; Caruana and de Sa, 1996) Recent works (Kurin et al., 2022; Xin et al., 2022) show that it is difficult to outperform a convex combination of task losses if the task losses are appropriately scaled. I.e., in case of equal difficulty of the two tasks, a classifier with equal weighting of the two classification losses serves as an upper bound in terms of performance. However, finding suitable task weights is a tedious and inefficient approach to MTL. A more automated way of weighting multiple tasks would thus be vastly appreciated.

In this experiment, we demonstrate how the DRPM can learn task difficulty by partitioning a network layer. Intuitively, a task that requires many neurons is more complex than a task that can be solved using a single neuron. Based on this observation, we propose the DRPM-MTL. The DRPM-MTL learns to partition the neurons of the last shared layer such that only a subset of the neurons are used for every task. In contrast to the other experiments (Sections 5.1 and 5.2), we use the DRPM without resampling and infer the partition \(Y\) as a deterministic function. This can be done by applying the two-step procedure of Proposition 4.1 but skipping the resampling step of the MVHG and PL distributions. We compare the DRPM-MTL to the unitary loss scaling method (ULS, Kurin et al.,2022), which has a fixed architecture and scales task losses equally. Both DRPM-MTL and ULS use a network with shared architecture up to some layer, after which the network branches into two task-specific layers that perform the classifications. Note the difference between the methods. While the task-specific branches of the ULS method access all neurons of the last shared layer, the task-specific branches of the DRPM-MTL access only the subset of neurons reserved for the respective task.

We perform experiments on MultiMNIST (Sabour et al., 2017), which overlaps two MNIST digits in one image, and we want to classify both numbers from a single sample. Hence, the two tasks, classification of the left and the right digit (see Appendix C.4 for an example), are approximately equal in difficulty by default. To increase the difficulty of one of the two tasks, we introduce the noisyMultiMNIST dataset. There, we control task difficulty by adding salt and pepper noise to one of the two digits, subsequently increasing the difficulty of that task with increasing noise ratios. Varying the noise, we evaluate how our DRPM-MTL adapts to imbalanced difficulties, where one usually has to tediously search for optimal loss weights to reach good performance. We base our pipeline on (Sener and Koltun, 2018). For more details and additional CelebA MTL experiments we refer to Appendix C.4.

We evaluate the DRPM-MTL concerning its classification accuracy on the two tasks and compare the inferred subset sizes per task for different noise ratios \(\{0.0,,0.9\}\) of the noisyMultiMNIST dataset (see Figure 4). The DRPM-MTL achieves the same or better accuracy on both tasks for most noise levels (upper part of Figure 4). It is interesting to see that, the more we increase \(\), the more the DRPM-MTL tries to overcome the increased difficulty of the right task by assigning more dimensions to it (lower part of Figure 4, noise ratio \(\)\(0.6\)-\(0.8\)). Note that for the maximum noise ratio of \(=0.9\), it seems that the DRPM-MTL basically surrenders and starts neglecting the right task, instead focusing on getting good performance on the left task, which impacts the average accuracy.

## Limitations & Future Work

The proposed two-stage approach to RPMs requires distributions over subset sizes and permutation matrices. The memory usage of the permutation matrix used in the two-stage RPM increases quadratically in the number of elements \(n\). Although we did not experience memory issues in our experiments, this may lead to problems when partitioning vast sets. Furthermore, learning subsets by first inferring an ordering of all elements can be a complex optimization problem. Approaches based on minimizing the earth mover's distance (Monge, 1781) to learn subset assignments could be an alternative to the ordering-based approach in our DRPM and pose an interesting direction for future work. Finally, note that we compute the probability mass function (PMF) \(p(Y;,)\) by approximating it with the bounds in Lemma 4.2. While the upper bound is tight when all scores have similar magnitude, the bound loosens if scores differ a lot, leading Equation (10) to overestimate the value of the PMF. In practice, we thus reweight the respective terms in the loss function, but in the future, we will investigate better estimates for the PMF.

Ultimately, we are interested in exploring how to apply the DRPM to multimodal learning under weak supervision, for instance, in medical applications. Section 5.2 demonstrated the potential of learning from coupled samples, but further research is needed to ensure fairness concerning underlying, hidden attributes when working with sensitive data.

Figure 4: Results for noisyMultiMNIST experiment. In the upper plot, we compare the task accuracy of the two methods ULS and the DRPM-MTL. We see that the DRPM-MTL can reach higher accuracy for most of the different noise ratios \(\) while it assigns the number of dimensions per task according to their difficulty.

## Conclusion

In this work, we proposed the differentiable random partition model, a novel approach to random partition models. Our two-stage method enables learning partitions end-to-end by separately controlling subset sizes and how elements are assigned to subsets. This new approach to partition learning enables the integration of random partition models into probabilistic and deterministic gradient-based optimization frameworks. We show the versatility of the proposed differentiable random partition model by applying it to three vastly different experiments. We demonstrate how learning partitions enables us to explore the modes of the data distribution, infer shared and independent generative factors from coupled samples, and learn task-specific sub-networks in applications where we want to solve multiple tasks on a single data point.