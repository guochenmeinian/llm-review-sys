# TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery

Jialin Chen

Yale University

jialin.chen@yale.edu

&Rex Ying

Yale University

rex.ying@yale.edu

###### Abstract

Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called **Temp**oral **Motifs **E**xplainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to \(8.21\%\) increase in terms of explanation accuracy across six real-world datasets and up to \(22.96\%\) increase in boosting the prediction Average Precision of current TGNNs.1

## 1 Introduction

Temporal Graph Neural Networks (TGNN) are attracting a surge of interest in real-world applications, such as social networks, financial prediction, _etc._ These models exhibit the ability to capture both the topological properties of graphs and the evolving dependencies between interactions over time . Despite their widespread success, these models often lack transparency, functioning as black boxes. The provision of human-intelligible explanations for these models becomes imperative, enabling a better understanding of their decision-making logic and justifying the rationality behind their predictions. Therefore, improving explainability is fundamental in enhancing the trustworthiness of current TGNNs, making them reliable for deployment in real-world scenarios, particularly in high-stakes tasks like fraud detection and healthcare forecasting .

The goal of explainability is to discover what patterns in data have been recognized that trigger certain predictions from the model. Explanation approaches on static graph neural networks have been well-studied recently . These methods identify a small subset of important edges or nodes that contribute the most to the model's prediction. However, the success of these methods on static graphs cannot be easily generalized to the field of temporal graphs, due to the complex and volatile nature of dynamic networks . Firstly, there can be duplicate events occurring at the same timestamp and the same position in temporal graphs. The complicated dependenciesbetween interactions were under-emphasized by existing explanation approaches [20; 21]. Moreover, the important events should be temporally proximate and spatially adjacent to construct a human-intelligible explanation . We refer to explanations that satisfy these requirements as _cohesive_ explanations. As illustrated in Figure 1(a), a non-cohesive explanation typically consists of scattered events (highlighted in purple). For instance, event \(1\) and event \(10\) in the disjointed explanation are neither temporally proximate nor spatially adjacent to other explanatory events, leading to a sub-optimal explanation and degrading the inspiration that explanations could bring us. There have been some recent attempts at TGNN explainability [21; 23]. Unfortunately, they all face the critical challenge of generating _cohesive_ explanations and fall short of providing human-intelligible insights. Moreover, they entail high computational costs, making them impractical for real-world deployment.

To address the aforementioned challenges of temporal explanations, we propose to utilize temporal motifs in the explanation task. Temporal motifs refer to recurring substructures within the graph. Recent studies [24; 25; 26; 27; 28; 29; 30; 31] demonstrate that these temporal motifs are essential factors that control the generative mechanisms of future events in real-world temporal graphs and dynamic systems. For example, preferential attachment (Figure 1(c)) elucidates the influence effect in e-commerce marketing graphs [32; 33]. Triadic closure (Figure 1(d)) explains the common-friend rules in social networks [34; 6; 1; 25]. Therefore, they are plausible and reliable composition units to explain TGNN predictions. Moreover, the intrinsic self-connectivity of temporal motifs guarantees the _cohesive_ property of the generated explanations (Figure 1(b)).

**Proposed work.** In the present work, we propose **TempME**, a novel **Tem**oral **M**otif-based **E**xplainer to identify the most determinant temporal motifs to explain the reasoning logic of temporal GNNs and justify the rationality of the predictions. TempME leverages historical events to train a generative model that captures the underlying distribution of explanatory motifs and thereby improves the explanation efficiency. TempME is theoretically grounded by Information Bottleneck (IB), which finds the best tradeoff between explanation accuracy and compression. To utilize Information Bottleneck in the context of temporal graphs, we incorporate a null model (_i.e.,_ a randomized reference) [22; 35; 36] into the model to better measure the information contained in the generated explanations. Thereby, TempME is capable of telling for each motif how the occurrence frequency difference in empirical networks and randomized reference reflects the importance to the model predictions. Different from previous works that only focus on the effect of singular events [23; 21], TempME is the first to bring additional knowledge about the effect of each temporal motif.

We evaluate TempME with three popular TGNN backbones, TGAT , TGN  and GraphMixer . Extensive experiments demonstrate the superiority and efficiency of TempME in explaining the prediction behavior of these TGNNs and the potential in enhancing the prediction performance of TGNNs, achieving up to \(8.21\%\) increase in terms of explanation accuracy across six real-world datasets and up to \(22.96\%\) increase in boosting the prediction Average Precision of TGNNs.

The **contributions** of this paper are: (1) We are the first to utilize temporal motifs in the field of explanations for TGNNs to provide more insightful explanations; (2) We further consider the null model in the information bottleneck principle for the temporal explanation task; (3) The discovered temporal motifs not only explain the predictions of different TGNNs but also exhibit ability in enhancing their link prediction performance.

## 2 Related Work

GNN ExplainabilityExplainability methods for Graph Neural Networks can be broadly classified into two categories: non-generative and generative methods. Given an input instance with its

Figure 1: (a) and (b): Non-cohesive explanation and cohesive explanation (highlighted in colors). (c) and (d): Temporal motifs govern the generation of future interactions (numbers denote event orders).

prediction, non-generative methods typically utilize gradients [15; 37], perturbations [38; 39], relevant walks , mask optimization , surrogate models , and Monte Carlo Tree Search (MCTS)  to search the explanation subgraph. These methods optimize the explanation one by one during the explanation stage, leading to a longer inference time. On the contrary, generative methods train a generative model across the entire dataset by learning the distribution of the underlying explanatory subgraphs [19; 18; 14; 42; 43; 44; 45], which obtains holistic knowledge of the model behavior over the whole dataset. Compared with static GNN, the explainability of temporal graph neural networks (TGNNs) remain challenging and under-explored. TGNNExplainer  is the first explainer tailored for temporal GNNs, which relies on the MCTS algorithm to search for a combination of the explanatory events. Recent work  utilizes the probabilistic graphical model to generate explanations for discrete time series on the graph, leaving the continuous-time setting under-explored. However, these methods cannot guarantee cohesive explanations and require significant computation costs. There are also some works that have considered intrinsic interpretation in temporal graphs  and seek the self-interpretable models [46; 20]. As ignored by previous works on temporal explanation, we aim for cohesive explanations that are human-understandable and insightful in a generative manner for better efficiency during the explanation stage.

Network MotifsThe concept of network motifs is defined as recurring and significant patterns of interconnections , which are building blocks for complex networks [47; 24]. Kovanen et al.  proposed the first notion of temporal network motifs with edge timestamps, followed by relaxed versions to involve more diverse temporal motifs [48; 24; 49]. Early efforts developed efficient motif discovery algorithms, _e.g.,_ MFinder , MAVisto , Kavosh , _etc._ The standard interpretation of the motif counting is presented in terms of a null model, which is a randomized version of the real-world network [35; 52; 22; 53]. Another research line of network motifs focuses on improving network representation learning with local motifs [54; 55; 56]. These approaches emphasize the advantages of incorporating motifs into representation learning, leading to improved performance on downstream tasks. In this work, we constitute the first attempt to involve temporal motifs in the explanation task and target to uncover the decision-making logic of temporal GNNs.

## 3 Preliminaries and Problem Formulation

**Temporal Graph Neural Network**. We treat the temporal graph as a sequence of continuous timestamped events, following the setting in TGNNExplainer . Formally, a temporal graph can be represented as a function of timestamp \(t\) by \((t)=\{(t),(t)\}\), where \((t)\) and \((t)\) denote the set of nodes and events that occur before timestamp \(t\). Each element \(e_{k}\) in \((t)\) is represented as \(e_{k}=(u_{k},v_{k},t_{k},a_{k})\), denoting that node \(u_{k}\) and node \(v_{k}\) have an interaction event at timestamp \(t_{k}<t\) with the event attribution \(a_{k}\). Without loss of generality, we assume that interaction is undirected [5; 1]. Temporal Graph Neural Networks (TGNN) take as input a temporal graph \((t)\) and learn a time-aware embedding for each node in \((t)\). TGNNs' capability for representation learning on temporal graphs is typically evaluated by their link prediction performance [57; 1; 25], _i.e.,_ predicting the future interaction based on historical events. In this work, we also focus on explaining the link prediction behavior of TGNNs, which can be readily extended to node classification tasks.

**Explanation for Temporal Graph Neural Network**. Let \(f\) denote a well-trained TGNN (_aka._ base model). To predict whether an interaction event \(e\) happens between \(u\) and \(v\) at timestamp \(t\), the base model \(f\) leverages the time-aware node representation \(x_{u}(t)\) and \(x_{v}(t)\) to output the logit/probability. An explainer aims at identifying a subset of important historical events from \((t)\) that trigger the future interaction prediction made by the base model \(f\). The subset of important events is known as an explanation. Formally, let \(Y_{f}[e]\) denote the binary prediction of event \(e\) made by base model \(f\), the explanation task can be formulated as the following problem that optimizes the mutual information between the explanation and the original model prediciton [13; 23]:

\[*{argmax}_{|_{}^{e}| K}I(Y_{f}[e]; _{}^{e})*{argmin}_{| _{}^{e}| K}-_{c=0,1}(Y_{f}[e]=c) (f(_{}^{e})[e])\] (1)

where \(I(,)\) denotes the mutual information function, \(e\) is the interaction event to be explained, \(_{}^{e}\) denotes the explanation constructed by important events from \((t)\) for \(e\). \(f(_{}^{e})[e]\) is the probability output on the event \(e\) predicted by the base model \(f\). \(K\) is the explanation budget on the explanation size (_i.e.,_ the number of events in \(_{}^{e}\)).

Proposed Method: TempME

A simple optimization of Eq. 1 easily results in disjointed explanations . Therefore, we utilize temporal motifs to ensure that the generated explanations are meaningful and understandable.

The pipeline of TempME is shown in Figure 2. Given a temporal graph and a future prediction between node \(u\) and node \(v\) to be explained, TempME first samples surrounding temporal motif instances (Sec. 4.1). Then a Motif Encoder creates expressive Motif Embedding for each extracted motif instance, which consists of three main steps: event anonymization, message passing, and graph pooling (Sec. 4.2). Based on Information-bottleneck (IB) principle, TempME characterizes the importance scores of these temporal motifs, under the constraints of both explanation accuracy and information compression (Sec. 4.3). In the explanation stage, succinct and cohesive explanations are constructed by sampling from the Bernoulli distribution controlled by the importance score \(p\) for the prediction behavior of the base model.

### Temporal Motif Extraction

We first extract a candidate set of motifs whose importance scores are to be derived. Intuitively, event orders encode temporal causality and correlation. Therefore, we constrain events to reverse over the direction of time in each motif and propose the following Retrospective Temporal Motif.

**Definition 1**.: _Given a temporal graph and node \(u_{0}\) at time \(t_{0}\), a sequence of \(l\) events, denotes as \(I=\{(u_{1},v_{1},t_{1}),(u_{2},v_{2},t_{2}),,(u_{l},v_{l},t_{l})\}\) is a \(n\)-node, \(l\)-length, \(\)-duration **Retrospective Temporal Motif** of node \(u_{0}\) if the events are reversely time ordered within a \(\) duration, i.e., \(t_{0}>t_{1}>t_{2}>t_{l}\) and \(t_{0}-t_{l}\), such that \(u_{1}=u_{0}\) and the induced subgraph is connected and contains \(n\) nodes._

Temporal dependencies are typically revealed by the relative order of the event occurrences rather than the absolute time difference. Consequently, we have the following definition of equivalence.

**Definition 2**.: _Two temporal motif instances \(I_{1}\) and \(I_{2}\) are **equivalent** if they have the same topology and their events occur in the same order, denoted as \(I_{1} I_{2}\)._

Temporal Motifs are regarded as important building blocks of complex dynamic systems . Due to the large computational complexity in searching high-order temporal motifs, recent works show the great potential of utilizing lower-order temporal motifs, _e.g.,_ two-length motifs  and three-node motifs , as units to analyze large-scale real-world temporal graphs. A collection of temporal motifs with up to \(3\) nodes and \(3\) events is shown in Appendix B.

Given a temporal graph with historical events \(\) and node \(u_{0}\) of interest at time \(t_{0}\), we sample \(C\) retrospective temporal motifs with at most \(n\) nodes and \(l\) events, starting from \(u_{0}\) (\(\) is usually set as a large for the comprehensiveness of motifs). Alg. 1 shows our Temporal Motif Sampling approach, where \((S,t)\) denotes the set of historical events that occur to any node in \(S\) before time \(t\). At each step, we sample one event from the set of historical events related to the current node set. Alg. 1 adapts Mfinder , a motif mining algorithm on static graphs, to the scenario of temporal graphs. We could also assign different sampling probabilities to historical events in Step 3 in Alg. 1 to obtain temporally biased samples. Since the purpose of our sampling is to collect a candidate set of expressive temporal motifs for the explanation, we implement uniform sampling in Step 3 for algorithmic efficiency.

```  Node set: \(S_{c}\{u_{0}\}\), for \(1 c C\)  Event sequence: \(I_{c}()\), for \(1 c C\) for\(c=1\) to \(C\)do for\(j=1\) to \(l\)do  Sample one event \(e_{j}=(u_{j},v_{j},t_{j})\)  from \((S_{c},t_{j-1})\) if\(|S_{c}|<n\)then \(S_{c}=S_{c}\{u_{j},v_{j}\}\) \(I_{c}=I_{c} e_{j}\) return\(\{I_{c} 1 c C\}\) ```

**Algorithm 1**Temporal Motif Sampling

Figure 2: Framework of TempME. Numbers on the edge denote the event order.

**Relation to Previously Proposed Concepts**. Recent works [1; 6] propose to utilize temporal walks and recurrent neural networks (RNN)  to aggregate sequential information. Conceptually, temporal walks construct a subset of temporal motif instances in this work. In contrast, temporal motifs capture more graph patterns for a holistic view of the governing rules in dynamic systems. For instance, the motif of preferential attachment (Fig. 1(c)) cannot be represented as temporal walks.

### Temporal Motif Embedding

In the present work, we focus on explaining the link prediction of temporal graph neural networks. Given an interaction prediction between node \(u\) and node \(v\) to be explained, we sample \(C\) surrounding temporal motif instances starting from \(u\) and \(v\), respectively, denoted as \(M_{u}\) and \(M_{v}\). Note the proposed framework is also flexible for explaining other graph-related problems. For instance, to explain the node classification on dynamic graphs, we sample \(C\) temporal motif instances around the node of interest. Each temporal motif is represented as \((e_{1},e_{2},,e_{l})\) with \(e_{i}=(u_{i},v_{i},t_{i})\) satisfying Definition 1. We design a Motif Encoder to learn motif-level representations for each surrounding motif in \(M_{u}\) and \(M_{v}\).

**Event Anonymization**. The anonymization technique is at the core of many sequential feature distillation algorithms [1; 6; 60; 61]. Previous works [6; 1] mainly focus on node anonymization, while temporal motifs are constructed by sequences of temporal events. To bridge this gap, we consider the following event anonymization to adapt to temporal motifs. To maintain the inductiveness, we create structural features to anatomize event identities by counting the appearance at certain positions:

\[h(e_{i},u,v)[j]=|\{I I M_{u} M_{v},I[j]=(u_{i},v_{i},t);  t\}|,i\{1,2,,l\}.\] (2)

\(h(e_{i},u,v)\) (abbreviated as \(h(e_{i})\) for simplicity) is a \(l\)-dimensional structural feature of \(e_{i}\) where the \(j\)-th element denotes the number of interactions between \(u_{i}\) and \(v_{i}\) at the \(j\)-th sampling position in \(M_{u} M_{v}\). \(h(e_{i})\) essentially encodes both spatial and temporal roles of event \(e_{i}\).

**Temporal Motif Encoding**. The extracted temporal motif is essentially a subgraph of the original temporal graph. Instead of using sequential encoders, we utilize local message passing to distill motif-level embedding. Given a motif instance \(I\) with node set \(_{I}\) and event set \(_{I}\), let \(X_{p}\) denote the associated feature of node \(p_{I}\). \(E_{pq}=(a_{pq} T(t-t_{pq}) h(e_{pq}))\) denotes the event feature of event \(e_{pq}_{I}\), where \(a_{pq}\) is the associated event attribute and \(h(e_{pq})\) refers to the structural feature of event \(e_{pq}\) (Eq. 2). Note that the impact of motifs varies depending on the time intervals. For instance, motifs occurring within a single day differ from those occurring within a year. Thus, we need a time encoder \(T()\) which maps the time interval into \(2d\)-dimensional vectors via \(T( t)=[(w_{1} t),(w_{1} t),,(w_ {d} t),(w_{d} t)]\) with learnable parameters \(w_{1},,w_{d}\)[52; 1]. To derive the motif-level embedding, we initially perform message passing to aggregate neighboring information and then apply the Readout function to pool node features.

\[}=(X_{p};\{X_{q};E_{pq}|q(p) \})m_{I}=(\{},p_{I}\})\] (3)

Following Eq. 3, one may use GIN  or GAT  in MessagePassing step and simple mean-pooling or learnable adaptive-pooling  as Readout function to further capture powerful motif representations. We refer to Appendix D.4 for more details about the Temporal Motif Encoder.

### Information-Bottleneck-based Generator

**Motivation**. A standard analysis for temporal motif distribution is typically associated with the null model, a randomized version of the empirical network [22; 50; 35]. The temporal motif that behaves statistically differently in the occurrence frequency from that of the null model is considered to be structurally significant. Therefore, we assume the information of temporal motifs can be disentangled into interaction-related and interaction-irrelevant ones. The latter is natural result of the null model. Based on this assumption, we resort to the information bottleneck technique to extract compressed components that are the most interaction-related. We refer to Appendix C for theoretical proofs.

**Sampling from Distribution**. Given an explanation query and a motif embedding \(m_{I}\) with \(I\), where \(\) denotes the set of extracted temporal motifs, we adopt an MLP for mapping \(m_{I}\) to an importance score \(p_{I}\), which measures the significance of this temporal motif instance for the explanation query. We sample a mask \(_{I}(p_{I})\) for each temporal motif instance and then apply the masks to screen for a subset of important temporal motif instances via \(_{}=A\).

\(A\) is the mask vector constructed by \(_{I}\) for each motif \(I\) and \(\) denotes element-wise product. The explanation subgraph for the query can thus be induced by all events that occur in \(_{}\).

To back-propagate the gradients _w.r.t._ the probability \(p_{I}\) during the training stage, we use the Concrete relaxation of the Bernoulli distribution  via \((p)(( p-(1-p)+ u- (1-u)))\), where \(u(0,1)\), \(\) is a temperature for the Concrete distribution and \(\) is the sigmoid function. In the inference stage, we randomly sample discrete masks from the Bernoulli distribution without relaxation. Then we induce a temporal subgraph with \(_{}\) as the explanation. One can also rank all temporal motifs by their importance scores and select the Top \(K\) important motifs to induce more compact explanations if there is a certain explanation budget in practice.

**Information Bottleneck Objective**. Let \(^{e}_{}\) and \((e)\) denote the explanation and the computational graph of event \(e\) (_i.e.,_ historical events that the base model used to predict \(e\)). To distill the most interaction-related while compressed explanation, the IB objective maximizes mutual information with the target prediction while minimizing mutual information with the original temporal graph:

\[-I(^{e}_{},Y_{f}[e])+ I(^{e}_{ },(e)),}\ |^{e}_{}| K\] (4)

where \(Y_{f}[e]\) refers to the original prediction of event \(e\), \(\) is the regularization coefficient and \(K\) is a constraint on the explanation size. We then adjust Eq. 4 to incorporate temporal motifs.

The first term in Eq. 4 can be estimated with the cross-entropy between the original prediction and the output of base model \(f\) given \(^{e}_{}\) as Eq. 1, where \(^{e}_{}\) is induced by \(_{}\). Since temporal motifs are essential building blocks of the surrounding subgraph and we have access to the posterior distribution of \(_{}\) conditioned on \(\) with importance scores, we propose to formulate the second term in Eq. 4 as the mutual information between the original motif set \(\) and the selected motif subset \(_{}\). We utilize a variational approximation \((_{})\) to replace its marginal distribution \((_{})\) and obtain the upper bound of \(I(,_{})\) with Kullback-Leibler divergence:

\[I(,_{})_{}D_{ }(_{}(_{}|);(_{}))\] (5)

where \(\) involve learnable parameters in Motif Encoder (Eq. 3) and the MLP for importance scores.

**Choice of Prior Distribution**. Different choices of \((_{})\) in Eq. 5 may lead to different inductive bias. We consider two practical prior distributions for \((_{})\): _uniform_ and _empirical_. In the _uniform_ setting , \((_{})\) is the product of Bernoulli with probability \(p\), that is, each motif shares the same probability \(p\) being in the explanation. The KL divergence thus becomes \(D_{}(_{}(_{}|); (_{}))=_{I_{i}}p_{I_{i}} }}{p}+(1-p_{I_{i}})}}{1-p}\). Here \(p\) is a hyperparameter that controls both the randomness level in the prior distribution and the prior belief about the explanation volume (_i.e.,_ the proportion of motifs that are important for the prediction).

However, _uniform_ distribution ignores the effect of the null model, which is a better indication of randomness in the field of temporal graphs. To tackle this challenge, we further propose to leverage the null model to define _empirical_ prior distribution for \((_{})\). A null model is essentially a randomized version of the empirical network, generated by shuffling or randomizing certain properties while preserving some structural aspects of the original graph. Following prior works on the null model , we utilize the common null model in this work, where the event order is randomly shuffled. The null model shares the same degree spectrum and time-shuffled event orders with the input graph  (see more details in Appendix D.1). We categorize the motif instances in \(\) by their equivalence relation defined in Definition B. Let \((U_{1},,U_{T})\) denote \(T\) equivalence classes of temporal motifs and \((q_{1},,q_{T})\) is the sequence of normalized class probabilities occurring in \(_{}\) with \(q_{i}=_{I_{j} U_{i}}p_{I_{j}}/_{I_{j}}p_{I_{j}}\), where \(p_{I_{j}}\) is the importance score of the motif instance \(I_{j}\). Correspondingly, we have \((m_{1},,m_{T})\) denoting the sequence of normalized class probabilities in the null model. The prior belief about the average probability of a motif being important for prediction is fixed as \(p\). Thus minimizing Eq. 5 is equivalent to the following equation.

\[_{}D_{}(_{}(_{}| );(_{}))_{} (1-s)+s_{i=1}^{T}q_{i}}{pm_{i}},\] (6)

where \(s\) is computed by \(s=_{I_{j}}p_{I_{j}}/||\), which measures the sparsity of the generated explanation. Combing Eq. 1 and Eq. 6 leads to the following overall optimization objective:

\[_{}_{e(t)}_{c=0,1}-(Y_{f}[e]=c) (f(^{e}_{})[e])+((1-s)+s _{i=1}^{T}q_{i}}{pm_{i}}).\] (7)Eq. 7 aims at optimizing the explanation accuracy with the least amount of information. It learns to identify the most interaction-related temporal motifs and push their importance scores close to 1, leading to deterministic existences of certain motifs in the target explanation. Meanwhile, the interaction-irrelevant components are assigned smaller importance scores to balance the trade-off in Eq. 7. TempME shares spirits with perturbation-based explanations [68; 69], where "interpretable components " corresponds to temporal motifs and the "reference" is the null model.

**Complexity**. A brute-force implementation of the sampling algorithm (Alg. 1) has the time complexity \((Cl)\). Following Liu et al. , we create a \(2l\)-digit to represent a temporal motif with \(l\) events, where each pair of digits is an event between the node represented by the first digit and the node represented by the second digit. We utilize these \(2l\)-digits to classify the temporal motifs by their equivalence relations, thus resulting in a complexity of \((C)\). An acceleration strategy with the tree-structured sampling and detailed complexity analysis are given in Appendix D.3.

## 5 Experiments

### Experimental Setups

**Dataset**. We evaluate the effectiveness of TempME on six real-world temporal graph datasets, Wikipedia, Reddit, Enron, UCI, Can.Parl., and US Legis [70; 71; 72] that cover a wide range of domains. Wikipedia and Reddit are bipartite networks with rich interaction attributes. Enron and UCI are social networks without any interaction attributes. Can.Parl. and US Legis are two political networks with a single attribute. Detailed dataset statistics are given in Appendix E.1.

**Base Model**. The proposed TempME can be employed to explain any temporal graph neural network (TGNN) that augments local message passing. We adopt three state-of-the-art temporal graph neural networks as the base model: TGAT , TGN , and GraphMixer . TGN and GraphMixer achieve high performance with only one layer due to their powerful expressivity or memory module. TGAT typically contains 2-3 layers to achieve the best performance. Following previous training setting [23; 1; 6], we randomly sample an equal amount of negative links and consider event prediction as a binary classification problem. All models are trained in an inductive setting [6; 1].

**Baselines**. Assuming the base model contains \(L\) layers and we aim at explaining the prediction on the event \(e\), we first extract \(L\)-hop neighboring historical events as the computational graph \((e)\). For baselines, we first compare with two self-interpretable techniques, Attention (ATTN) and Gradient-based Explanation (Grad-CAM ). For ATTN, we extract the attention weights in TGAT and TGN and take the average across heads and layers as the importance scores for events. For Grad-CAM, we calculate the gradient of the loss function _w.r.t._ event features and take the norms as the importance scores. Explanation \(^{e}_{}\) is generated by ranking events in \((e)\) and selecting a subset of explanatory events with the highest importance scores. We further compare with learning-based approaches, GNNExplainer , PGExplainer  and TGNNExplainer , following the baseline setting in prior work . The former two are proposed to explain static GNNs while TGNNExplainer is a current state-of-the-art model specifically designed for temporal GNNs.

**Configuration**. Standard fixed splits [73; 72] are applied on each datasets. Following previous studies on network motifs [52; 22; 35; 56], we have empirically found that temporal motifs with at most 3 nodes and 3 events are sufficiently expressive for the explanation task (Fig. 4). We use GINE , a modified version of GIN  that incorporates edge features in the aggregation function, as the MessagePassing function and mean-pooling as the Readout function by default.

### Explanation Performance

**Evaluation Metrics**. To evaluate the explanation performance, we report Fidelity and Sparsity following TGNNExplainer . Let \(^{e}_{}\) and \(\) denote the explanation for event \(e\) and the original temporal graph, respectively. Fidelity measures how valid and faithful the explanations are to the model's original prediction. If the original prediction is positive, then an explanation leading to an increase in the model's prediction logit is considered to be more faithful and valid and vice versa. Fidelity is defined as \((,^{e}_{})=(Y_{f}[e]=1) (f(^{e}_{})[e]-f()[e])+(Y_{f}[e]=0) (f()[e]-f(^{e}_{})[e])\). Sparsity is defined as \(=|^{e}_{}|/|(e)|\), where \((e)\) denotes the computational graph of event \(e\). An ideal explanation should be compact and succinct, therefore,higher fidelity with lower Sparsity denotes a better explanation performance. Besides, we further adopt the ACC-AUC metric, which is the AUC value of the proportion of generated explanations that have the same predicted label by the base model over sparsity levels from \(0\) to \(0.3\).

**Results.** Table 1 shows the explanation performance of TempME and other baselines _w.r.t._ ACC-AUC. TempME outperforms baselines on different datasets and base models in general. Notably, TempME achieves state-of-the-art performance in explaining TGN with strong ACC-AUC results (\( 90\%\)) over all six datasets. Specifically, the effectiveness of TempME is consistent across datasets with and without attributes, whereas the performance of baseline models exhibits considerable variation. For example, ATTN and Grad-CAM work well on datasets with rich attributes, _e.g.,_ Wikipedia and Reddit, while may yield poor performances on unattributed datasets. Therefore, events with large gradients or attention values are not sufficient to explain the decision-making logic of the base model.

Figure 3 demonstrates the Fidelity-Sparsity curves of TempME and compared baselines on Wikipedia with different base models. From Figure 3, we observe that TempME surpasses the baselines in terms of explanation fidelity, especially with a low sparsity level. In addition, it reveals that the optimal sparsity level varies among different base models. For TGAT, increasing sparsity initially diminishes and later enhances the general fidelity. Conversely, for TGN and GraphMixer, increasing sparsity consistently improves fidelity. These findings indicate that TGAT gives priority to a narrow subset (_e.g.,_\(1\%\)) of historical events, while TGN and GraphMixer rely on a wider range of historical events.

**Cohesiveness**. To evaluate the cohesive level of the explanations, we propose the following metric:

\[=_{}^{e}|^{2}-|_{}^{e}|}_{e_{i}_{}^{e}}_{e_{j} _{}^{e}:e_{i} e_{j}}(-t_{j}|}{  T})(e_{i} e_{j}),\] (8)

    & & **Wikipedia** & **Reddit** & **UCI** & **Enron** & **USLegis** & **Can.Parl.** \\   & Random & 70.91\(\)1.03 & 81.97\(\)0.92 & 54.51\(\)0.52 & 48.94\(\)1.28 & 54.24\(\)1.34 & 51.66\(\)2.26 \\  & ATTN & 77.31\(\)0.01 & 86.80\(\)0.01 & 27.25\(\)0.01 & 68.28\(\)0.01 & 62.24\(\)0.00 & 79.92\(\)0.01 \\  & Grad-CAM & 83.11\(\)0.01 & 90.29\(\)0.01 & 26.06\(\)0.01 & 19.93\(\)0.01 & 78.98\(\)0.01 & 50.42\(\)0.01 \\  & GNNExplainer & 84.34\(\)0.16 & 89.44\(\)0.56 & 62.38\(\)0.46 & 77.82\(\)0.88 & 89.42\(\)0.50 & 80.59\(\)0.58 \\  & PGExplainer & 84.26\(\)0.78 & 92.31\(\)0.92 & 59.47\(\)1.68 & 62.37\(\)0.82 & 91.24\(\)0.94 & 75.92\(\)1.12 \\  & TGNNExplainer & 85.74\(\)0.56 & 95.73\(\)0.36 & 86.26\(\)2.62 & **82.02\(\)1.94** & 90.37\(\)0.84 & 80.67\(\)1.49 \\  & **TempME** & **85.81\(\)0.53** & **96.69\(\)0.38** & **76.47\(\)0.80** & 81.85\(\)0.26 & **96.10\(\)0.20** & **84.48\(\)0.97** \\   & Random & 91.90\(\)1.42 & 91.42\(\)1.94 & 87.15\(\)2.23 & 82.72\(\)2.24 & 72.31\(\)2.64 & 76.43\(\)1.65 \\  & ATTN & 93.28\(\)0.01 & 93.81\(\)0.01 & 83.24\(\)0.01 & 83.57\(\)0.01 & 75.62\(\)0.01 & 79.38\(\)0.01 \\  & Grad-CAM & 93.46\(\)0.01 & 92.60\(\)0.01 & 87.51\(\)0.01 & 81.12\(\)0.01 & 81.46\(\)0.01 & 77.19\(\)0.01 \\  & GNNExplainer & 95.62\(\)0.53 & 95.05\(\)0.35 & 94.68\(\)0.42 & 88.61\(\)0.50 & 82.91\(\)0.46 & 83.32\(\)0.64 \\  & PGExplainer & 94.28\(\)0.84 & 94.42\(\)0.36 & 92.39\(\)0.85 & 88.34\(\)1.24 & 90.62\(\)0.75 & 88.84\(\)1.42 \\  & TGNNExplainer & 93.51\(\)0.98 & 96.21\(\)0.47 & 94.24\(\)0.52 & 90.32\(\)0.82 & 90.40\(\)0.83 & 84.70\(\)1.19 \\  & **TempME** & **95.80\(\)0.42** & **98.66\(\)0.80** & **96.34\(\)0.30** & **92.64\(\)0.27** & **94.37\(\)0.88** & **90.63\(\)0.72** \\   & Random & 77.31\(\)2.37 & 85.08\(\)0.72 & 53.56\(\)1.27 & 64.07\(\)0.86 & 85.54\(\)0.93 & 87.79\(\)0.51 \\  & Grad-CAM & 76.63\(\)0.01 & 84.44\(\)0.41 & 82.64\(\)0.01 & 72.50\(\)0.01 & 88.98\(\)0.01 & 85.80\(\)0.01 \\   & GNNExplainer & 89.21\(\)0.63 & 95.10\(\)0.36 & 61.02\(\)0.37 & 74.23\(\)0.13 & 89.67\(\)0.35 & 92.28\(\)0.10 \\   & PGExplainer & 85.19\(\)1.24 & 94.26\(\)0.42 & 63.76\(\)1.06 & 75.39\(\)0.43 & 92.37\(\)0.10 & 90.63\(\)0.32 \\   & TGNNExplainer & 87.69\(\)0.86 & **95.82\(\)0.73** & 80.47\(\)0.87 & **81.87\(\)0.45** & 93.04\(\)0.45 & 93.78\(\)0.74 \\   & **TempME** & **90.15\(\)0.30** & 95.05\(\)0.19 & **87.06\(\)0.12** & 79.69\(\)0.33 & **95.00\(\)0.16** & **95.98\(\)0.21** \\   

Table 1: ACC-AUC of TempME and baselines over six datasets and three base models. The AUC values are computed over 16 sparsity levels between 0 and 0.3 at the interval of 0.02. The best result is in **bold** and second best is underlined.

Figure 3: Fidelity-Sparsity Curves on Wikipedia dataset with different base models

where \( T\) means the time duration in the computational graph \((e)\), \(1(e_{i} e_{j})\) indicates whether \(e_{i}\) is spatially adjacent to \(e_{j}\). Meanwhile, temporally proximate event pairs are assigned with larger weights of \((|t_{i}-t_{j}|/ T)\). A higher level of cohesiveness indicates a more cohesive explanation. From Table 2, we observe that ATTN and Grad-CAM excel in generating cohesive explanations compared to learning-based explainers, _e.g.,_ GNNExplainer, TGNNExplainer. However, TempME still surpasses all baselines and achieves the highest cohesiveness levels, primarily due to its ability to extract and utilize self-connected motifs, allowing it to generate explanations that are both coherent and cohesive.

**Efficiency Evaluation**. We empirically investigate the efficiency of TempME in terms of the inference time for generating one explanation and report the results for Wikipedia and Reddit on TGAT in Table 3, where the averages are calculated across all test events. GNNExplainer and TGNNExplainer optimize explanations individually for each instance, making them less efficient. Notably, TGNNExplainer is particularly time-consuming due to its reliance on the MCTS algorithm. In contrast, TempME trains a generative model using historical events, which allows for generalization to future unseen events. As a result, TempME demonstrates high efficiency and fast inference.

**Motif-enhanced Link Prediction**. The extracted motifs can not only be used to generate explanations but also boost the performance of TGNNs. Let \(m_{I}\) denote the motif embedding generated by the Temporal Motif Encoder (Eq. 3) and \(\) is the temporal motif set around the node of interest. We aggregate all these motif embeddings using \(_{I}m_{I}/||\) and concatenate it with the node representations before the final MLP layer in the base model. The performance of base models on link prediction with and without Motif Embeddings (ME) is shown in Table 4. Motif Embedding provides augmenting information for link prediction and generally improves the performance of base models. Notably, TGAT achieves a substantial boost, with an Average Precision of \(95.31\%\) on USLegis, surpassing the performance of state-of-the-art models on USLEgis . More results are given in Appendix E.3.

**Ablation Studies**. We analyze the hyperparameter sensitivity and the effect of prior distributions used in TempME, including the number of temporal motifs \(C\), the number of events in the motifs \(l\), and the prior belief about the explanation volume \(p\). The results are illustrated in Figure 4.

Firstly, when using smaller motifs (_e.g.,_\(l=2\)), TempME achieves comparable explanation accuracy when a sufficient number of motifs are sampled. However, the accuracy plateaus with fewer temporal motifs when \(l=3\) or \(l=4\). Unfortunately, there are only three equivalence classes for temporal motifs with only two events, limiting the diversity of perspectives in explanations. Following previous analysis on temporal motifs [52, 22,

    & **UCI** & **Enron** & **USLEgis** & **Can.Parl.** \\  TGAT & 76.28 & 65.68 & 72.35 & 65.18 \\ TGAT+ME & **83.65\({}^{(7.37)}\)** & **68.37\({}^{(12.69)}\)** & **95.31\({}^{(122.96)}\)** & **76.35\({}^{(111.37)}\)** \\ TGN & 75.82 & **76.40** & 77.28 & 64.23 \\ TGN+ME & **77.46\({}^{(11.54)}\)** & 75.62\({}^{(10.78)}\)** & **83.90\({}^{(16.62)}\)** & **79.46\({}^{(115.23)}\)** \\ GraphMixer & 89.13 & 69.42 & 66.71 & 76.98 \\ GraphMixer+ME & **90.11\({}^{(10.98)}\)** & **70.13\({}^{(10.71)}\)** & **81.42\({}^{(114.71)}\)** & **79.33\({}^{(22.35)}\)** \\   

Table 4: Link prediction results (Average Precision) of base models with Motif Embedding (ME)

Figure 4: (a) Hyperparameter sensitivity of number of temporal motifs \(C\) and motif length \(l\). (b) Comparison between uniform and empirical prior distribution in terms of ACC-AUC over sparsity levels from 0 to 0.3.

    & **Reddit** & **UCI** \\  ATTN & 0.0502 & 0.0708 \\ Grad-CAM & 0.0422 & 0.0722 \\ GNNExplainer & 0.0270 & 0.0337 \\ PGExplainer & 0.0233 & 0.0332 \\ TGNNExplainer & 0.0397 & 0.0538 \\
**TempME** & **0.0574** & **0.0749** \\   

Table 2: Cohesiveness evaluation on Reddit and UCI with TGAT.

24], we suggest considering temporal motifs with up to 3 events in the explanation task for the sake of algorithmic efficiency. Secondly, TempME achieves the highest ACC-AUC when the prior belief of the explanation volume is in the range of \([0.3,0.5]\). Notably, TempME performs better with _empirical_ prior distribution when \(p\) is relatively small, resulting in sparser and more compact explanations. This improvement can be attributed to the incorporation of the null model, which highlights temporal motifs that differ significantly in frequency from the null model. Figure 4 (b) verifies the rationality and effectiveness of the _empirical_ prior distribution in TempME. Additional insight into the role of the null model in explanation generation can be found in the explanation visualizations in Appendix E.5.

It is worth noting that when \(p\) is close to 1, _uniform_ prior distribution leads to deterministic existences of all temporal motifs while _empirical_ prior distribution pushes the generated explanations towards the null model, which forms the reason for the ACC-AUC difference of _empirical_ and _uniform_ as \(p\) approaches 1.

We further conduct ablation studies on the main components of TempME. We report the explanation ACC-AUC on Wikipedia in Table 5. Specifically, we first replace the GINE convolution with GCN and GAT and replace the mean-pooling with adaptive pooling in the Temporal Motif Encoder. Then we iteratively remove event anonymization and time encoding in the creation of event features before they are fed into the Temporal Motif Encoder (Eq. 3). Results in Table 5 demonstrate that all the above variants lead to performance degradation. Moreover, the Time Encoding results in a more severe performance drop across three base models. We further evaluate the effectiveness of _empirical_ prior distribution by comparing it with _uniform_ prior distribution. In both prior distributions, the prior belief on the explanation size \(p\) is set to \(0.3\). We report the best results in Table 5. We can observe that the _empirical_ prior distribution gains a performance boost across three base models, demonstrating the importance of the null model in identifying the most significant motifs.

## 6 Conclusion and Broader Impacts

In this work, we present TempME, a novel explanation framework for temporal graph neural networks. Utilizing the power tool of temporal motifs and the information bottleneck principle, TempME is capable of identifying the historical events that are the most contributing to the predictions made by TGNNs. The success of TempME bridges the gap in explainability research on temporal GNNs and points out worth-exploring directions for future research. For instance, TempME can be deployed to analyze the predictive behavior of different models, screen effective models that can capture important patterns, and online services to improve the reliability of temporal predictions.

By enabling the generation of explainable predictions and insights, temporal GNNs can enhance decision-making processes in critical domains such as healthcare, finance, and social networks. Improved interpretability can foster trust and accountability, making temporal GNNs more accessible to end-users and policymakers. However, it is crucial to ensure that the explanations provided by the models are fair, unbiased, and transparent. Moreover, ethical considerations, such as privacy preservation, should be addressed to protect individuals' sensitive information during the analysis of temporal graphs.