# Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities

Donghao Ying

IEOR Department

UC Berkeley

donghaoy@berkeley.edu

&Yunkai Zhang

IEOR Department

UC Berkeley

yunkai_zhang@berkeley.edu

&Yuhao Ding

IEOR Department

UC Berkeley

yuhao_ding@berkeley.edu

&Alec Koppel

Artificial Intelligence Research

J.P. Morgan

alec.koppel@jpmchase.com

&Javad Lavaei

IEOR Department

UC Berkeley

lavaei@berkeley.edu

###### Abstract

We investigate safe multi-agent reinforcement learning, where agents seek to collectively maximize an aggregate sum of local objectives while satisfying their own safety constraints. The objective and constraints are described by _general utilities_, i.e., nonlinear functions of the long-term state-action occupancy measure, which encompass broader decision-making goals such as risk, exploration, or imitations. The exponential growth of the state-action space size with the number of agents presents challenges for global observability, further exacerbated by the global coupling arising from agents' safety constraints. To tackle this issue, we propose a primal-dual method utilizing shadow reward and \(\)-hop neighbor truncation under a form of correlation decay property, where \(\) is the communication radius. In the exact setting, our algorithm converges to a first-order stationary point (FOSP) at the rate of \((T^{-2/3})\). In the sample-based setting, we demonstrate that, with high probability, our algorithm requires \(}(^{-3.5})\) samples to achieve an \(\)-FOSP with an approximation error of \((_{0}^{2})\), where \(_{0}(0,1)\). Finally, we demonstrate the effectiveness of our model through extensive numerical experiments.

## 1 Introduction

Cooperative multi-agent reinforcement learning (MARL) involves agents operating within a shared environment, where each agent's decisions influence not only their objectives, but also those of others and the state trajectories . In seeking to bring conceptually sound MARL techniques out of simulation [2; 3] and into real-world environments [4; 5], some key issues emerge: safety and communications overhead implied by a training mechanism. Although experimentally, the centralized training decentralized execution (CTDE) framework has gained traction recently [6; 7], its requirement for centralized data collection can pose issues for large-scale  or privacy-sensitive applications . Therefore, we prioritize decentralized training, where to date most MARL techniques impose global state observability for performance certification . In this work, we extend recent efforts to alleviate this bottleneck  especially in the case of safety critical settings, in a flexible manner that allows agents to incorporate risk, exploration, or prior information.

More specifically, we hypothesize that the multi-agent system consists of a network of agents that interact with each other locally according to an underlying dependence graph . Second, to model safety constraints in reinforcement learning (RL), we adopt a standard approach based on constrainedMarkov Decision Processes (CMDPs) , where one maximizes the expected total reward subject to a safety-related constraint on the expected total utility. Third, since many decision-making problems take a form beyond the classic cumulative reward, such as apprenticeship learning , diverse skill discovery , pure exploration , and state marginal matching , we focus on utility functions defined as nonlinear functions of the induced state-action occupancy measure, which can be abstracted as RL with general utilities [16; 17].

Towards formalizing the approach, we consider an MARL model consisting of \(n\) agents, each with its own local state \(s_{i}\) and action \(a_{i}\), where the multi-agent system is associated with an underlying dependence graph \(\). Each agent is privately associated with two local general utilities \(f_{i}()\) and \(g_{i}()\), where \(f_{i}()\) and \(g_{i}()\) are functions of the local occupancy measure. The objective is to find a safe policy for each agent that maximizes the average of the local objective utilities, namely, \(1/n_{i=1}^{n}f_{i}()\), and satisfies each agent's individual safety constraint described by its local utility \(g_{i}()\). This setting captures a wide range of safety-critical applications, for example, resource allocation for the control of networked epidemic models , influence maximization in social networks , portfolio optimization in interbank network structures , intersection management for connected vehicles , and energy constraints of wireless communication networks .

Despite the significance of safe MARL with general utilities, prior works have either ignored the necessity of safety  or the computational bottleneck associated with global information exchange regarding the state and action per step . In fact, the interaction of these two aspects requires addressing the fact that each agent's own safety constraint requires information from all others. In particular, the existing works in safe MARL allow full access to the global state or unlimited communications among all agents for policy implementation, value estimation, and constraint satisfaction [25; 26; 27]. However, this assumption is impractical due to the "curse of dimensionality" , as well as the limited information exchanges and communications among agents .

Therefore, to our knowledge, there is no methodology to both guarantee safety and incur manageable communications overhead for each agent. Compounding these issues is the fact that standard RL training schemes based on the _policy gradient theorem_ are not applicable in the context of general utilities. This deviation from the cumulative rewards adds to the difficulty of estimating the gradient, since there does not exist a policy-independent reward function. We refer the reader to Appendix A for an extended discussion of related works.

To address these challenges, we focus on the setting of **distributed training without global observability** and aim to develop a scalable algorithm with theoretical guarantees. Our main contributions are summarized below:

* Compared with existing theoretical works on safe MARL [25; 26; 31], we present the first safe MARL formulation that extends beyond cumulative forms in both the objective and constraints. We develop a truncated policy gradient estimator utilizing shadow reward and \(\)-hop policies under a form of correlation decay property, where \(\) represents the communication radius. The approximation errors arising from both policy implementation and value estimation are quantified.
* Despite of the global coupling of agents' local utility functions, we propose a scalable Primal-Dual Actor-Critic method, which allows each agent to update its policy based only on the states and actions of its close neighbors and under limited communications. The effectiveness of the proposed algorithm is verified through numerical experiments.
* From the perspective of optimization, we devise new tools to analyze the convergence of the algorithm. In the exact setting, we establish an \((T^{-2/3})\) convergence rate for finding an FOSP, matching the standard convergence rate for solving nonconcave-convex saddle point problems. In the sample-based setting, we prove that, with high probability, the algorithm requires \(}(^{-3.5})\) samples to obtain an \(\)-FOSP with an approximation error of \((_{0}^{2})\), where \(_{0}(0,1)\).

## 2 Problem formulation

Consider a Constrained Markov Decision Process (CMDP) over a finite state space \(\) and a finite action space \(\) with a discount factor \([0,1)\). A policy \(\) is a function that specifies the decision rule of the agent, i.e., the agent takes action \(a\) with probability \((a|s)\) in state \(s\). When action \(a\) is taken, the transition to the next state \(s^{}\) from state \(s\) follows the probability distribution \(s^{}(|s,a)\). Let \(\) be the initial distribution. For each policy \(\) and state-action pair \((s,a)\), the _discounted state-action occupancy measure_ is defined as

\[^{}(s,a)=_{k=0}^{}^{k}(s^{k}=s,a^{k}=a |,s^{0}).\] (1)

The goal of the agent is to find a policy \(\) that maximizes a general objective described by a (possibly) nonlinear function \(f()\) of \(^{}\), known as the _general utility_, subject to a constraint in the form of another general utility \(g()\), namely

\[_{}f(^{}) g(^{}) 0.\] (2)

When \(f()= r,\) and \(g()= u,\) are linear functions, (2) recovers the standard CMDP problem:

\[_{}V^{}(r)\!=\!\![_{k=0}^{}^{k}r (s^{k},a^{k})|,s^{0}.],V^{}(u)\!=\!\![_{k=0}^{}^{k}u(s^{k},a^ {k})|,s^{0}.] 0,\] (3)

where \(V^{}()\) is usually referred to as the _value function_. In contrast, it has been shown that for some MDPs, there is no standard value function that can be equivalent to the general utility [16, Lemma 1]. In Appendix C, we provide more examples of formulation (2) beyond standard value functions.

In this work, we study the decentralized version of problem (2). Consider the system is composed of a network of agents associated with a graph \(=(,_{})\) (not densely connected in general), where the vertex set \(=\{1,2,,n\}\) denotes the set of \(n\) agents and the edge set \(_{}\) prescribes the communication links among the agents. Let \(d(i,j)\) be the length of the shortest path between agents \(i\) and \(j\) on \(\). For \( 0\), let \(_{i}^{}=\{j|d(i,j)\}\) denote the set of agents in the \(\)-hop neighborhood of agent \(i\), with the shorthand notation \(_{i}^{}_{i}^{}\) and \(-i=\{i\}\). The details of the decentralized nature of the system are summarized below:

Space decompositionThe global state and action spaces are the product of local spaces, i.e., \(=_{1}_{2}_{n}\), \(=_{1}_{2}_ {n}\), meaning that for every \(s\) and \(a\), we can write \(s=(s_{1},s_{2},,s_{n})\) and \(a=(a_{1},a_{2},,a_{n})\). For each subset \(^{}\), we use \((s_{^{}},a_{^{}})\) to denote the state-action pair for the agents in \(^{}\).

Observation and communicationEach agent \(i\) only has direct access to its own state \(s_{i}\) and action \(a_{i}\), while being allowed to communicate with its \(\)-hop neighborhood \(_{i}^{}\) for information exchanges. The communication radius \(\) is a given but tunable parameter.

Transition decompositionGiven the current global state \(s\) and action \(a\), the local states in the next period are independently generated, i.e., \((s^{}|s,a)=_{i}_{i}(s^{}_{ i}|s,a)\), \( s^{}\), where we use \(_{i}\) to denote the local transition probability for agent \(i\).

Policy factorizationThe global policy can be expressed as the product of local policies, such that \((a|s)=_{i}^{i}(a_{i}|s)\), \((s,a)\), i.e., given the global state \(s\), each agent \(i\) acts independently based on its local policy \(^{i}\). We assume that each local policy \(^{i}\) is parameterized by a parameter \(_{i}\) within a convex set \(_{i}\). Thus, we can write \((a|s)=_{}(a|s)=_{i}^{i}_{_{i}}(a_ {i}|s)\), where \(=_{1}_{2}_{n}\) is the concatenation of local parameters.

Localized objective and constraintFor each agent \(i\) and its local state-action pair \((s_{i},a_{i})\), the _local state-action occupancy measure_ under policy \(\) is defined as

\[_{i}^{}(s_{i},a_{i})=_{k=0}^{}^{k}(s_{ i}^{k}=s_{i},a_{i}^{k}=a_{i},s^{0}),\] (4)

which can be viewed as the marginalization of the global occupancy measure, i.e., \(_{i}^{}(s_{i},a_{i})=_{s_{-i},a_{-i}}^{}(s,a)\). Each agent \(i\) is privately associated with two local (general) utilities \(f_{i}()\) and \(g_{i}()\), which are functions of the local occupancy measure \(_{i}^{}\). Agents cooperate with each other aiming at maximizing the global objective \(f()\), defined as the average of local utilities \(\{f_{i}()\}_{i}\), while each agent \(i\) needs to satisfy its own safety constraint described by the local utility \(g_{i}()\). Then, under the parameterization \(_{}\), (2) can be rewritten as

\[_{}\ F()_{i}f_{ i}(_{i}^{_{}}),G_{i}() g_{i}(_{i}^{_{}}) 0,\  i .\] (5)

Note that problem (5) is not separable among agents due to the coupling of occupancy measures. Compared to the formulation where the constraint is modeled as the average of local constraints, e.g.,, (5) is stricter and more interpretable. We emphasize that the method proposed in this paper does not require the relaxation of local constraints in (5) to a joint constraint and it directly generalizes to the case of multiple constraints per agent.

Consider the Lagrangian function associated with (5):

\[(,) F()+_{i} _{i}G_{i}()=_{i}[f_{i}(_{i}^{ _{}})+_{i}g_{i}(_{i}^{_{}})],\] (6)

where \(_{+}^{n}\) is the Lagrangian multiplier. The Lagrangian formulation  of (5) can be written as

\[_{}_{ 0}(,).\] (7)

Since the general utilities \(f_{i}(_{i}^{_{}})\) and \(g_{i}(_{i}^{_{}})\) may not be non-concave w.r.t. \(\) even in the form of cumulative rewards, finding the global optimum to (5) is NP-hard in general . Our goal in this work is to develop a scalable and provably efficient gradient-based primal-dual algorithm that can find the first-order stationary points of (5).

## 3 Scalable primal-dual actor-critic method

For a standard value function with the reward \(r^{||||}\), denoted as \(V^{_{}}(r)= r,^{_{}}\), the policy gradient theorem (see Lemma D.1) yields that

\[_{}V^{_{}}(r)=r^{}_{}^{_{ }}=_{s d^{_{}},a_{ }\{\}}_{}_{}(a|s) Q^{_{ }}(r;s,a),\]

where \(d^{_{}}(s)(1-)_{a}^{_{ }}(s,a)\) is the discounted state occupancy measure, \(_{}_{}(|)\) is the score function, and \(Q^{_{}}(r;,)\) is the Q-function with the reward \(r\), defined as

\[Q^{_{}}(r;s,a)=[_{k=0}^{}^{k}r(s ^{k},a^{k})_{},s^{0}=s,a^{0}=a].\] (8)

Although this elegant result no longer holds for general utilities, we can apply the chain rule:

\[_{}f(^{_{}})=_{}f(^{ _{}})^{}_{}^{_{}}= _{}V^{_{}}_{}f^{_{ }},\] (9)

i.e., the gradient \(_{}f(^{_{}})\) is equal to the policy gradient of a standard value function with the reward \(_{}f^{_{}})\). We introduce the following definitions  for the distributed problem (5).

**Definition 3.1** (Shadow reward and shadow Q-function).: _For each agent \(i\), define \(r_{f_{i}}^{_{}}_{_{i}}f_{i}(_{i}^{_{ }})^{|_{i}||_{i}|}\) as the (local) shadow reward for the utility \(f_{i}()\) under policy \(_{}\). Define \(Q_{f_{i}}^{_{}}(s,a) Q^{_{}}(r_{f_{i}}^{_{ }};s,a)\) as the associated (local) shadow Q-function for \(f_{i}()\). Similarly, let \(r_{g_{i}}^{_{}}\) and \(Q_{g_{i}}^{_{}}(s,a)\) be the shadow reward and the Q function for \(g_{i}()\)._

Combining Definition 3.1 with (9), we can write the local gradient for agent \(i\), i.e., \(_{_{i}}(,)\), as

\[_{_{i}}(,)=_{s d ^{_{}},a_{}\{\}}_{_{i}} _{_{i}}^{i}(a_{i}|s)_{j}Q_{f _{j}}^{_{}}(s,a)+_{j}Q_{g_{j}}^{_{}}(s,a),\] (10)

where we apply the policy factorization to arrive at \(_{_{i}}_{}(a|s)=_{_{i}}_{_{i }}^{i}(a_{i}|s)\). By (10), each agent needs to know the shadow Q functions of all agents, as well as the global state, to evaluate its own gradient. However, especially in large networks, this is both inefficient, due to the communication cost, and impractical because of the limited communication radius. In the remainder of this section, we aim to design a scalable estimator for \(_{_{i}}(,)\) that requires only local communications.

### Spatial correlation decay and \(\)-hop policies

Inspired by , we assume that the transition probability satisfies a form of the spatial correlation decay property [35; 36].

**Assumption 3.2**.: _For a matrix \(M^{n n}\) whose \((i,j)\)-th entry is defined as_

\[M_{ij}=_{s_{j},a_{j},s^{}_{j},a^{}_{j},s_{-j},a_{-j}}\| _{i}(|s_{j},s_{-j},a_{j},a_{-j})-_{i} (|s^{}_{j},s_{-j},a^{}_{j},a_{-j})\|_{1},\] (11)

_assume that there exists \(>0\) such that \(_{i}_{j}e^{ d(i,j)}M_{ij}\) with \(<2/\), where \(\) is the discount factor._The value of \(M_{ij}\) reflects the extent to which agent \(j\)'s state and action influence the local transition probability of agent \(i\). Thus, Assumption 3.2 amounts to requiring this influence to decrease exponentially with the distance between any two agents. Such a decay is often observed in many large-scale real-world systems, e.g., the strength of signals decreases exponentially with distance .

Furthermore, as mentioned earlier, the implementation of the local policy \(^{i}_{_{j}}(|s)\) is still impractical, since it requires access to the global state \(s\), while the allowable communication radius is limited to \(\). To alleviate this issue, we focus on a specific class of policies in which the local policy of agent \(i\) only depends on the states of these agents in its \(\)-hop neighborhood \(_{i}^{}\). This class of policies is also referred to as \(\)-hop policies in the concurrent work .

**Assumption 3.3** (\(\)-hop policies).: _For each agent \(i\) and \(\), the local policy \(^{i}_{_{i}}(|s)\) depends only on the neighbor states \(s_{_{i}^{}}\), i.e.,_

\[^{i}_{_{i}}(|s_{_{i}^{}},s_{ _{i}^{}})=^{i}_{_{i}}(|s_{_{i}^{ }},s_{_{i}^{}}^{}),\  s s _{_{i}^{}}^{}_{_{i}^{}}.\] (12)

For simplicity, we use the notation \(^{i}_{_{i}}(|s)=^{i}_{_{i}}(|s_{_{i}^{ }})\) for \(\)-hop policies when it is clear from context. We note that, for any original policy function \(_{}(|s)\), an induced \(\)-hop policy \(_{}(|s_{_{i}^{}})\) can be defined by fixing the states \(s_{_{i}^{}}\) to some arbitrary values and focusing only on the states of agents in \(_{i}^{}\). When considering only \(\)-hop policies, it is essential to understand how much information is lost compared to the case where agents have access to the global states. The following proposition quantifies the maximum information loss in terms of the occupancy measure under the assumption that the original policy function also satisfies a spatial correlation decay property.

**Proposition 3.4**.: _Suppose that there exist \(c 0\) and \([0,1)\) such that for every \(\), agent \(i\), and states \(s,s^{}\) such that \(s_{_{i}^{}}=s_{_{i}^{}}^{}\), we have \(\|^{i}_{_{i}}(|s)-^{i}_{_{i}}(|s^{}) \|_{1} c^{}\). Let \(_{}\) be an induced \(\)-hop policy of \(_{}\). Then, it holds that_

\[\|_{i}^{_{}}-_{i}^{_{}} \|_{1}}{(1-)^{2}}, i.\] (13)

The condition on the local policy in Proposition 3.4 encodes that every \(^{i}_{_{i}}\) is exponentially less sensitive to the states of agents outside \(_{i}^{}\), which is a common assumption in MARL to alleviate computationally burdensome and practically intractable communication requirements imposed by the global observability [34; 39; 38]. By Proposition 3.4, the difference in occupancy measures under \(_{}\) and \(_{}\) is controlled by \(\|^{i}_{_{i}}-^{i}_{_{i}}\|_{1}\). Therefore, if \(f_{i}(^{})\) and \(g_{i}(^{})\) are Lipschitz continuous w.r.t. \(^{}\), Proposition 3.4 implies an \((^{})\) approximation of the Lagrangian function (6) using \(\)-hop policies. The faster the spatial decay of policy is, the more accurate the approximation of the \(\)-hop policy is. This justifies our focus on learning a \(\)-hop policy.

### Truncated policy gradient estimator

In the absence of global observability, it is critical to find a scalable estimator for the local gradient \(_{_{i}}(,)\) in (10), so that each agent can update its local policy with limited communications.

By leveraging the similar idea in the definition of \(\)-hop policies, we define the \(\)_-hop truncated (shadow) \(Q\)-function_, denoted as \(^{_{}}_{_{i}}:_{_{i}^{ }}_{_{i}^{}}\), to be

\[^{_{}}_{_{i}}(s_{_{i}^{ }},a_{_{i}^{}}) Q^{_{}}_{_{i} }(s_{_{i}^{}},_{_{i}^{}},a_{ _{i}^{}},_{_{i}^{}}),\ (s_{_{i}^{}},a_{ _{i}^{}})_{_{i}^{}} _{_{i}^{}},\{f,g\},\] (14)

where \((_{_{i}^{}},_{_{i}^{}})\) is any fixed state-action pair for the agents in \(_{-i}^{}\). Now, we introduce the following _truncated policy gradient estimator_ for agent \(i\):

\[_{_{i}}(,)\!=\! _{\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!

**Lemma 3.5**.: _Suppose that Assumptions 3.2 and 3.3 hold and there exist \(M_{r},M_{}>0\) such that \(\|_{_{i}}^{_{}}\|_{} M_{r}\) and \(\|_{_{i}}_{_{i}}^{i}\|_{2} M_{}\), for every \(\{f,g\}\), \(\), \(i\). Then, for all \(\), \(i\), we have that_

\[\|_{_{i}}(,)-_{_{i}} (,)\|_{2})M_{}c_{0}_{0 }^{}}{1-}=(_{0}^{}),\] (16)

_where \(c_{0}=2 M_{r}(2-)\) and \(_{0}=e^{-}\)._

Recall that the shadow reward is defined as the gradient of \(f_{i}()\) or \(g_{i}()\) w.r.t. the local occupancy measure. Since the set of all possible occupancy measures is compact (see (43)), the existence of \(M_{r}>0\) in Lemma 3.5 is satisfied if \(f_{i}()\) and \(g_{i}()\) are continuously differentiable. The main advantage of using the estimator \(_{},(,)\) lies in that every agent \(i\) only needs to know the truncated Q-functions of agents in its neighborhood \(_{i}^{}\), which can significantly reduce the communication burden and the storage requirement when graph \(\) is not densely connected. The proof of Lemma 3.5 can be found in Appendix E.2.

### Algorithm design

Using the results of the preceding section, we put together all the pieces and propose the _Primal-Dual Actor-Critic Method with Shadow Reward and \(\)-hop Policy_, as outlined in Algorithm 1. It includes three stages: policy evaluation by the critic, Lagrangian multiplier update, and policy update by the actor. Below, we provide an overview of Algorithm 1, while referring the reader to Appendix D for a flow diagram (Figure 2) of the algorithm, as well as a more detailed discussion.

Stage 1 (policy evaluation by the critic, lines 3-6) In each iteration \(t\), the current policy \(_{^{t}}\) is simulated to generate a batch of trajectories, while each agent \(i\) collects its neighborhood trajectories, i.e., the state-action pairs of the agents in \(_{i}^{}\), as batch \(_{i}^{t}\). Then, the batch is used to estimate the local occupancy measures \(_{i}^{_{^{t}}}\) through (17), which are subsequently applied to compute the empirical values for the constraint function \(g_{i}(_{i}^{_{^{t}}})\) and shadow rewards \(r_{f_{i}}^{_{^{t}}}\) and \(r_{g_{i}}^{_{^{t}}}\), denoted as \(_{i}^{t}\), \(_{f_{i}}^{t}\), and \(_{g_{i}}^{t}\), respectively. It is worth mentioning that, when all utility functions reduce to the form of cumulative rewards, the above operation is unnecessary, since all agents have policy-independent local reward functions.

Next, the agents jointly conduct a distributed evaluation subroutine to estimate their truncated shadow Q-functions \(\{_{_{}}^{_{^{t}}}\}_{i}\) using empirical shadow rewards \(\{_{_{}}^{t}\}_{i}\), where \(\{f,g\}\). During the subroutine, each agent \(i\) communicates with its neighbor in \(_{i}^{}\) to exchange state-action information, but only needs to access its own empirical shadow reward \(_{_{i}}^{t}\). In principle, any existing approach that satisfies the observation and communication requirements can be used for the truncated Q-function estimation, such as . As an example subroutine, we introduce the _Temporal Difference (TD) learning_ method , which is outlined as Algorithm 2 in Appendix D.

Stage 2 (Lagrangian multiplier update, line 7) Instead of employing the projected gradient descent, we propose to update the dual variables by the following formula:

\[^{t+1}=*{argmin}_{}(^{t}, )+}\|\|_{2}^{2}=_{}(- _{}_{}(^{t},^{t})),\] (22)

where weight \(_{}\) can be viewed as the dual "step-size". In practice, we replace the true dual gradient \(_{_{i}}(^{t},^{t})=g_{i}(_{i}^{_{ ^{t}}})/n\) with its empirical estimator \(_{_{i}}(^{t},^{t})\). The feasible region for the dual variable is denoted by \(_{+}^{n}\) and will be specified later.

Stage 3 (policy update by the actor, lines 8-9) To perform the policy update, each agent \(i\) first shares its updated dual variable \(_{i}^{t+1}\) and the values of its estimated truncated Q-functions along the trajectories in batch \(_{i}^{t}\) with the agents in its \(\)-hop neighborhood \(_{i}^{}\). Then, the agent estimates its truncated policy gradient \(_{_{i}}(^{t},^{t+1})\) through a REINFORCE-based mechanism  as described in (20). Finally, each agent \(i\) updates its local policy parameter by a projected gradient ascent.

We emphasize that Algorithm 1 is based on the distributed training regime and does not require full observability of global states and actions.

## 4 Convergence analysis

In this section, we analyze the convergence behavior and the sample complexity of Algorithm 1. We begin by summarizing the technical assumptions, including some mentioned previously in the paper. We direct the reader to Appendices F and G where we provide discussions for each assumption and present proofs for the results in this section.

**Assumption 4.1**.: _There exists \(L_{}>0\) such that \(_{_{i}}f_{i}()\) and \(_{_{i}}g_{i}()\) are \(L_{}\)-Lipschitz continuous w.r.t. \(_{i}\), i.e., \(\|_{_{i}}f_{i}(_{i})-_{_{i}}f_{i}(_{ i}^{t})\|_{} L_{}\|_{i}-_{i}^{t}\|_{2}\) and \(\|_{_{i}}g_{i}(_{i})-_{_{i}}g_{i}(_{ i}^{t})\|_{} L_{}\|_{i}-_{i}^{t}\|_{2}\), \( i\)._

**Assumption 4.2**.: _The parameterized policy \(_{}\) is such that **(I)** the score function is bounded, i.e., \( M_{}>0\) s.t. \(\|_{_{i}}_{_{i}}^{t}(a_{i}|s_{^{*}_{i}}) \|_{2} M_{}\), \((s,a)\), \(\), \(i\). **(II)**\( L_{}>0\) s.t. the utility functions \(F()=f(^{_{}})\) and \(G_{i}()=g_{i}(_{i}^{_{}})\) are \(L_{}\)-smooth w.r.t. \(\), \( i\)._

**Assumption 4.3**.: _There exist an FOSP \((^{*},^{*})\) of (5) and a constant \(>0\) s.t. \(^{*}_{i}<\), \( i\). Let \(=U^{n}=[0,]^{n}\)._

In Lemma F.5, we summarize a few properties that are the direct consequence consequence of Assumptions 4.1-4.3. Due to the non-concavity of problem (5), our focus is to find an approximatefirst-order stationary point (FOSP). A point \((,)\) is said to be an \(\)-FOSP if

\[(,)[(,)]^{2}+[ (,)]^{2},\] (23)

where the metrics \((,)\) and \((,)\) are defined as

\[(,)_{^{},\|^{ }-\|_{2} 1}_{}(,), ^{}-,\ \ (,)-_{^{} ,\|^{}-\|_{2} 1}_{}( ,),^{}-.\] (24)

The definitions of \((,)\) and \((,)\) are based on the first-order optimality condition . Given \(^{*}\) and \(^{*}\), it can be shown that \((^{*},^{*})=0\) implies that \((^{*},^{*})\) is an FOSP of (5) (see Lemma F.6). In the following, we first consider the exact setting where the agents can obtain the true values of their local occupancy measures, shadow Q-functions, and truncated policy gradients. Therefore, the only source of approximation error is the truncation of the policy gradient.

**Theorem 4.4** (Exact setting).: _Let Assumptions 3.2, 3.3, 4.1-4.3 hold and suppose that the agents can accurately estimate their local occupancy measures, shadow Q-functions, and truncated policy gradients. For every \(T>0\), let \(\{(^{t},^{t})\}_{t=0}^{T}\) be the sequence generated by Algorithm 1 with \(_{}=(T^{1/3})\) and \(_{}=1/L_{}+4L_{_{}}^{2}_{}\), where \(L_{},L_{}\) are Lipschitz constants defined in Lemma F.5. Then, there exists \(t^{*}\{0,1,,T-1\}\) such that_

\[(^{t^{*}},^{t^{*}+1})=(T^{-2/3 })+(_{0}^{2}).\] (25)

Next, we delve into the sample complexity of Algorithm 1. For theoretical analysis, we assume that the estimation process for the truncated Q-function offers an approximation to the true function, with the error being associated with the magnitude of the reward function. Let \(_{i}^{_{}}(r_{i};,)^{|_ {i}^{_{i}}||_{i^{_{i}}}|}\) be the truncated Q-function with the reward function \(r_{i}(,)^{|_{i}||_{i}|}\) for agent \(i\).

**Assumption 4.5**.: _For every reward function \(r_{i}(,)\) and \(_{0}>0\), the subroutine computes an approximation \(_{i}^{_{}}(r_{i};,)\) to the truncated Q-function \(_{i}^{_{}}(r_{i};,)\) such that_

\[\|_{i}^{_{}}(r_{i};,)-_{i}^{ _{}}(r_{i};,)\|_{}\|r_{i}\|_{}_ {0}\] (26)

_with \((1/(_{0})^{2})\) samples, for every \(i,\)._

We comment that the sample complexity of the truncated Q-function evaluation described in Assumption 4.5 is not restrictive. It can be achieved with high probability by the TD-learning procedure outlined in Algorithm 2 when the agents have enough exploration . For brevity, we assume that (26) holds almost surely. The only difference in the probabilistic version would be the presence of an additional term for the failure probability, which does not affect the order of the sample complexity.

**Theorem 4.6** (Sample-based setting).: _Suppose that Assumptions 3.2, 3.3, 4.1-4.3, and 4.5 hold. For every \(>0\) and \((0,1)\), let \(\{(^{t},^{t})\}_{t=0}^{T}\) be the sequence generated by Algorithm 1 with \(T=(^{-1.5})\), \(_{}=(^{-0.5})\), \(_{}=1/L_{}+4L_{}^{2}_{}\), \(_{0}=()\), \(_{0}=/(2n(T+1))\), batch size \(B=((1/_{0})^{-2})\), episode length \(H=(1/)\), where \(L_{},L_{}\) are Lipschitz constants defined in Lemma F.5. Then, with probability \(1-\), there exists \(t^{*}\{0,1,,T-1\}\) such that_

\[(^{t^{*}},^{t^{*}+1})=( )+(_{0}^{2}).\] (27)

_The required number of samples is \(}(^{-3.5})\)._

### Technical discussions

Theorem 4.4 implies an \((T^{-2/3})\) iteration complexity of Algorithm 1, matching the fastest convergence rate for solving nonconcave-convex maximin problems in the literature . The approximation error \((_{0}^{2})\) decays at a linear rate w.r.t. the radius of communications. Thus, as long as the underlying network is not densely connected, such as those in wireless communication  and autonomous driving , an approximate FOSP to (5) can be efficiently computed, while each agent \(i\) only needs to communicate with a small number of agents in its neighborhood..

In Theorem 4.4, we have chosen large step-sizes for the dual variable update to achieve the best convergence rate. This aggressive update ensures that the dual metric \((^{t},^{t+1})\) always remains within a small range and also provides a satisfactory ascent direction for the policy update. Then, the average primal metric \(1/T_{t=0}^{T-1}[(^{t},^{t+1})]^ {2}\) is upper-bounded by exploiting a recursive relation between any two consecutive dual updates. Hence, the existence of a point \((^{t^{*}},^{t^{*}+1})\) that satisfies (25) is guaranteed. It is worth noting that the proof of Theorem 4.4 can be easily generalized to the scenario where \(T\) is unspecified, and the same convergence rate can still be achieved with adaptive step-sizes \(_{}^{t}=(t^{1/3})\) and \(_{}^{t}=1/(L_{}+4L_{}^{2}_{}^{t})\).

Theorem 4.6 states that, with high probability, Algorithm 1 has an \(}(^{-3.5})\) sample complexity for finding an \(\)-FOSP of (5) with an approximation error \((_{0}^{2})\). Note that we absorb the logarithmic terms in the notation \(}()\). The proof of Theorem 4.6 can be broken down into two parts. Firstly, we evaluate the approximation errors of the estimators used in Algorithm 1 in relation to the model parameters, as outlined in Proposition G.1. Then, we integrate these errors into the iteration complexity result established in Theorem 4.4 and optimize the selection of parameters.

## 5 Numerical experiment

In this section, we validate Algorithm 1 via numerical experiments, focusing on three key questions1:

* How does Algorithm 1 perform with multiple agents, and does the policy gradient truncation effectively alleviate computational load?
* While Algorithm 1 is the first approach that provably solves the safe MARL problem with general utilities, how does it compare with existing methods for standard Safe MARL?
* What benefits does the use of general utilities offer over standard cumulative rewards?

To answer these questions, we performed multiple experiments in three environments2. The objective functions are based on cumulative rewards, while constraint functions leverage general utilities to incentivize or dissuade agents from exploring the environments.

**Synthetic environment** Analogous to (24, Section 5.1), where agents are linearly arranged as \(1-2--n\). Each agent \(i\) has binary local state and action spaces, i.e., \(_{i}=_{i}=\{0,1\}\), and the local transition matrix \(_{i}\) depends solely on its action \(a_{i}\) and the state of agent \(i+1\). The reward functions are constructed such that the optimal unconstrained policy compels all agents to continuously choose action \(1\), irrespective of their states.

**Pistonball** A physics-based game that emphasizes _cooperations and high-dimensional states_ as illustrated in Figure 0(a). Each piston represents an agent, where its local neighborhood includes adjacent pistons, and the goal is to collectively move the ball from right to left. The agent can move up, down, or remain still. We modify the original game so that the agent can only observe the ball when it enters the local neighborhood, as well as the height of neighboring pistons.

**Wireless communication** An access control problem following a similar setup as in . As illustrated in Figure 0(b), the agents try to transmit packets to common access points, and the transmission fails if the access point receives more than one packet simultaneously. As there are more agents than access points, _some agents need to learn to forego their benefits for the collective good_.

In addition to the objective, we incorporate two types of safety constraints characterized by general utilities that cannot be easily encapsulated by standard value functions based on cumulative rewards.

* **Entropy constraints** that stimulates exploration, formalized as \((_{i}^{_{}}) c,\, i \). The function \((_{i}^{_{}})\) represents the local entropy, defined as \(-_{s}d_{i}^{}(s)(d_{i}^{}(s))\), where \(d_{i}^{_{}}(s_{i})=(1-)_{a_{i}_{i}}_{i }^{_{}}(s_{i},a_{i})\) is the local state occupancy measure.
* \(}\)**-constrains** that deter agents from learning overly randomized policies, formulated as \(\|_{s_{i}_{i}}_{i}^{_{}}\|_{2}^ {2} c,\,\, i\). This constraint is beneficial in applications like autonomous driving and human-AI collaboration, where an agent's policy needs to be predictable for other agents.

In Figure 1, we demonstrate the performance of Algorithm 1 in the 20-agent Pistonball environment under entropy constraints. We observe that, while the truncation with \(=3\) converges in fewer iterations, truncation with \(=1\) also yields comparable performance. This underscores the efficiency of Algorithm 1 as employing a smaller communication radius can significantly reduce the computation.

Finally, we compare Algorithm 1 with three baselines based on the MAPPO-Lagrangian method .

* **MAPPO-L**: the original algorithm introduced in . Note that each agent has access to global information.
* **Decentralized MAPPO-L**: decentralized version of MAPPO-L, where each agent only has access to information in the local neighborhood. However, since each agent is trained to greedily maximize its individual reward, its behaviors might sacrifice the performance of other agents.
* **Decentralized Aggregate MAPPO-L**: decentralized version of MAPPO-L, where we address the aforementioned issue by redefining each agent's reward to be the sum of rewards of all agents in its local neighborhood.

For a fair comparison, we consider two standard safe MARL problems, where both objectives and constraints are shaped by cumulative rewards (see Appendix H.4). The results in Table 1 demonstrate that our method consistently outperforms both the centralized and decentralized variants of MAPPO-Lagrangian. We refer the readers to Appendix H for the comprehensive experimental results that fully answer the three questions raised at the beginning of this section.

## 6 Conclusion

In this work, we study the safe MARL with general utilities, with a focus on the setting of distributed training without global observability. To address the challenge of scalability and incorporating general utilities, we propose a primal-dual actor-critic method with shadow reward and \(\)-hop policy. Taking advantage of the spatial correlation decay property of the transition dynamics, we show that the proposed method achieves an \((T^{-2/3})\) convergence rate to the FOSP of the problem in the exact setting and achieves an \(}(^{-3.5})\) sample complexity, with high probability, in the sample-based setting. Finally, the effectiveness of our model and approach is verified by numerical studies. For future research, it would be interesting to develop scalable safe MARL algorithms with adaptive communication of agents' information  and intelligent sampling of agents' trajectories.

  &  &  \\ 
**Algorithm** & **Episodic return** & **Const. vivo.** & **Episodic return** & **Const. vivo.** \\  Ours & \(\) & \(\) & \(\) & \(\) \\  MAPPO-L & \(50.612 2.118\) & \(0.06884\) & \(3.347 0.131\) & \(0.4000\) \\  Decen. Agg. MAPPO-L & \(48.197 6.188\) & \(0.2179\) & \(3.106 0.673\) & \(1.1890\) \\  Decen. MAPPO-L & \(41.102 18.769\) & \(0.09303\) & \(3.148 0.614\) & \(1.5760\) \\ 

Table 1: Comparison between Scalable Primal-Dual Actor-Critic method in our work with MAPPO-L by  in Pistonball and wireless communication.

Figure 1: (a,b) Environment illustration. (c,d) Performance of Algorithm 1 in Pistonball with 20 agents under entropy constraints.