# Metric Transforms and Low Rank Representations of Kernels for Fast Attention

Timothy Chu

Independent Researcher

timothyzchu@gmail.com

&Josh Alman

Columbia University

josh@cs.columbia.edu

&Gary Miller

Carnegie Mellon University

glmiller@cs.cmu.edu

&Shyam Narayanan

Citadel Securities

shyam.s.narayanan@gmail.com

&Mark Sellke

Harvard University

msellke@fas.harvard.edu

&Zhao Song

Simons Institute for the Theory of Computing, UC Berkeley

magic.linuxkde@gmail.com

###### Abstract

We introduce a new linear-algebraic tool based on group representation theory, and use it to address three key problems in machine learning.

1. Past researchers have proposed fast attention algorithms for LLMs by approximating or replace softmax attention with other functions, such as low-degree polynomials. The key property of these functions is that, when applied entrywise to the matrix \(QK^{}\), the result is a low rank matrix when \(Q\) and \(K\) are \(n d\) matrices and \(n d\). This suggests a natural question: what are all functions \(f\) with this property? If other \(f\) exist and are quickly computable, they can be used in place of softmax for fast subquadratic attention algorithms. It was previously known that low-degree polynomials have this property. We prove that low-degree polynomials are the _only_ piecewise continuous functions with this property. This suggests that the low-rank fast attention only works for functions approximable by polynomials. Our work gives a converse to the polynomial method in algorithm design.
2. We prove the first full classification of all positive definite kernels that are functions of Manhattan or \(_{1}\) distance. Our work generalizes, and also gives a new proof for, an existing theorem at the heart of kernel methods in machine learning: the classification of all positive definite kernels that are functions of Euclidean distance.
3. The key problem in metric transforms, a mathematical theory used in geometry and machine learning, asks what functions transform pairwise distances in metric space \(M\) to metric space \(N\) for specified \(M\) and \(N\). We prove the first full classification of functions that transform Manhattan distances to Manhattan distances. Our work generalizes the foundational work of Schoenberg, which fully classifies functions that transform Euclidean to Euclidean distances. We additionally prove results about stable-rank preserving functions that are potentially useful in algorithmic design, and more. Our core tool for all our results is a new technique called the representation theory of the hyperrectangle.

[MISSING_PAGE_FAIL:2]

metrics could have equally rich algorithmic applications, and question (C) generalizes this beyond just Euclidean metrics. We could think of distance metrics in two semi-metric spaces \(\) and \(\), and the function pair \((f,F)\) can be viewed as a transform from \(\) to \(\).

In all these settings, there is a gap in our current understanding of what kernel functions can be used. For instance, there are a number of functions where it is _not known_ whether they are positive definite Manhattan kernels which could be used in classification, semi-supervised learning, and other similar tasks. We will see that a common suite of mathematical tools can be used to address all these different gaps. Most of our results prove that our understanding is complete, and that, for instance, the functions we know to be positive definite Manhattan kernels are, in fact, the only ones. This finally completes our understanding of these important classes of functions. That said, in the setting of preserving the stable rank of matrices, we will find that there are functions that are not polynomials, but that surprisingly do preserve the stable ranks of important matrices. Before we get into our technique in more detail, we first describe our main results in context.

Roadmap.In Section 2, we prove that the only functions which always yield a low-rank matrix when applied entry-wise to a low-rank matrix are low-degree polynomials, and explain the application to transformers. In Section 3, we give a classification of positive definite kernels with Manhattan distance input. In Section 4, we categorize all functions which transform Manhattan distances to Manhattan distances or squared Euclidean distances. In Section 5, we briefly introduce the core tool of this work. In Section 6, we give a conclusion of our work. In Section 7, we discuss the limitations of our work. In Section 8, we discuss the societal impacts of our work.

## 2 Fast Attention and the Polynomial Method

Fast attention computations in transformers and LLMs  use the _polynomial method_ as a key ingredient. This is a powerful technique for designing algorithms and constructing combinatorial objects. It states that applying a low-degree polynomial entry-wise to a low rank matrix yields another low-rank matrix. Examples of these low rank matrices include \(QK^{}/\) for \(n d\) matrices \(Q\) and \(K\) with \(n d\), which is a key matrix for attention computations in transformers and LLMs.

Fast attention computations rely on a polynomial approximation to the exponential function  combined with the polynomial method. Past researchers  suggested replacing the exponential function in softmax attention with a general kernel function \(f\). When \(f\) is a low-degree polynomial, researchers leveraged the polynomial method to create fast algorithms for polynomial attention in LLM computations . The work of  showed experimentally that polynomial attention has faster training and inference times, with little loss in quality on large language models.

**Fact 2.1** (The polynomial method, folklore; see e.g. ).: _Suppose \(f:\) is a polynomial of degree \(d\). Then, for any matrix \(M^{n n}\) of rank \(r\), letting_

\[k:=2,\]

_the matrix \(M^{f}^{n n}\) given by \(M^{f}_{i,j}:=f(M_{i,j})\) has \((M^{f}) k\)._

_For instance, if \(r=_{2}n\) and \(d<o(_{2}n)\), then_

\[(M^{f})<n.\]

The definition of the polynomial method inspires the following definition:

**Definition 2.2** (Preserve low-rank matrices).: _For a function \(f:\) and positive integer \(n\), we say \(f\) preserves low-rank \(n n\) matrices if, for every matrix \(M^{n n}\) with \((M)_{2}(n)+1\), the entry-wise application \(M^{f}^{n n}\) given by \(M^{f}_{i,j}:=f(M_{i,j})\) has_

\[(M^{f})<n.\]

It follows from the polynomial method that low-degree polynomials preserve low rank. Fast attention computations  rely on this low rank preservation property. For any function \(f\) that preserves

[MISSING_PAGE_FAIL:4]

Our first result plugs these gaps, showing that Fact 2.1 cannot be generalized in the settings left open by . (See Section C.8 below where we state [1, Theorem B] more formally and compare it with our Theorem 2.5 in more detail.)

**Theorem 2.5** (Informal statement of Theorem C.11).: _Suppose the function \(f:\) does not have any essential discontinuities of the first kind1. If \(f\) preserves low-rank \(n n\) matrices, then \(f\) is a polynomial of degree at most \(_{2}(n)\)._

This shows that functions \(f\) without essential discontinuities of the first kind, which are also not polynomials, do not preserve low-rank \(n n\) matrices, and only polynomials of degree less than \(_{2}(n)\) can preserve low-rank \(n n\) matrices. The class of functions without these essential discontinuities of the first kind is very rich, and includes all _piecewise continuous_ functions; it is hard to imagine a reasonable kernel function which is not piecewise continuous. Hence, one cannot hope to improve on the polynomial method by extending it to other functions without essential discontinuities of the first kind.

We conjecture that Theorem 2.5 holds for _all_ functions \(f:\) (i.e., that if \(f\) has an essential discontinuity of the first kind, then it also does not preserve low-rank matrices). Functions \(f\)_with_ an essential discontinuity of the first kind are not interesting in our setting since they cannot be efficiently evaluated.

We note that there is a small constant-factor gap between the degree which Fact 2.1 tells us is sufficient for a polynomial to preserve low-rank \(n n\) matrices, and the degree which Theorem 2.5 says is necessary: for instance, Fact 2.1 says that polynomials of degree at most \(_{2}(n)\) suffice, since

\[_{2}(n)\\ _{2}(n) n,\]

whereas Theorem 2.5 says that degree less than \(_{2}(n)\) is necessary. We leave open the question of closing this gap, although we note that the constant factor in front of the polynomial degree does not play a major role in most of the aforementioned applications of Fact 2.1.2

### Weaker Polynomial Methods

Fact 2.1 being essentially tight rules out one way to try to generalize the polynomial method. It is natural to ask whether we can get around this by weakening our constraint on the function \(f\). There are many properties of matrices which can be taken advantage of in the design of fast algorithms, and if we can show that \(M^{f}\) has any of these properties, it could still lead to improvements in the aforementioned applications.

Approximate Low RankWe first study functions \(f\) which, when applied entry-wise to a low-rank matrix \(M\), always result in an _approximately_ low-rank matrix \(M^{f}\). As we mentioned earlier, approximating a non-polynomial kernel function \(f\) by a polynomial is a common technique for taking advantage of the properties of polynomial kernels; when \(f\) can be well-approximated by a polynomial, then \(M^{f}\) has approximately low rank for this reason. This raises the question: can functions \(f\) which cannot be well-approximated by a polynomial also result in approximately low-rank matrices?

Our next result answers this question in the negative: the only functions which approximately preserve low rank are approximate polynomials. In other words, if \(f\) is not approximately a polynomial, then such an algorithmic approach cannot succeed, as \(f\) applied entry-wise to a matrix is not close to low rank.

We say a matrix \(M\) is approximately low-rank if the ratio of its smallest and largest eigenvalues is small; if \(M\) were not full rank, then this ratio would be \(0\). If \(M\) is approximately low-rank in this sense, then fast algorithms for manipulating it follow by using low-rank approximation or approximate subspace finding algorithms to find low-rank approximations for the matrix. Analogously, we say \(f\) is approximately a polynomial if its finite differences are small3; recall that the \(d\)-th order finite differences are \(0\) for any polynomial of degree \(<d-1\).

**Theorem 2.6** (Main result, informal statement of Theorem D.3).: _Let \(d=_{2}n\), and let \((0,1)\) be sufficiently small. Suppose \(f:\) is a real analytic function which \(\)-approximately preserves low-rank matrices, i.e., \(|_{i}(M^{f})|}{_{i[d]}|_{i}(M^{f})| }/n\) for all rank \(d+1\) matrices \(M\)._

_Then, the \(d^{th}\) order finite difference of \(f\), evaluated at \(a\), for sufficiently small gaps, is bounded above by \( K_{a}\). Here, \(K_{a}>0\) is a scaling factor with the property that if \(f\) is rescaled by a factor of \(c>0\) then \(K_{a}\) is also rescaled by \(c\)._

A dependence on a scaling factor \(K_{a}\) is necessary since, if \(f\) is rescaled by \(c\), this rescales the finite differences of \(f\) by \(c\), but does not change the ratio of any two eigenvalues of any matrix \(M^{f}\). In Theorem D.3 we also prove a similar result if \(f\) is Lipschitz (and not necessarily real analytic).

Stable RankOur first two results, Theorem 2.5 and Theorem 2.6, both _ruled out_ approaches to generalizing the polynomial method. Finally, we find one important property of matrices for which we _can_ strictly generalize the polynomial method: stable rank.

**Definition 2.7**.: _For a matrix \(M^{n n}\), its stable rank is defined as \(}(M):=^{2}}{\|M\|_{2}^{2}},\) where \(\|M\|_{F}\) denotes the Frobenius norm of matrix \(M\) and \(\|M\|_{2}\) denotes the spectral norm of matrix \(M\)._

It is known that \(}(M)}(M)\), but there are example matrices where \(}(M)}(M)\). Moreover, matrices with low stable rank can be manipulated quickly in many applications; for instance, low stable rank matrices are a useful tool in data mining and the study of Banach spaces , and very efficient sketching methods are known for matrices with small stable rank .

**Theorem 2.8** (Informal statement of Theorem J.2).: _Let \(M_{>0}^{n n}\) be a matrix, and suppose \(f:_{>0}_{>0}\) has the property that for any entry \(z\) of \(M\), \(} z f(z) z\) for some \(L_{>0}\). Then, \(}(M^{f}) L^{2}}(M)\)._

Consider, for instance, the matrices which arise in polynomial method applications ; these are matrices \(M^{n n}\) where each entry is in the interval \([1,O( n)]\). For these matrices, functions like \(f(x)=x^{c}\), for any constant \(c>0\), which are _not_ a polynomial when \(c\) is not an integer, still preserve stable rank (they satisfy the condition of Theorem 2.8 with \(L=}(n)\)). By contrast, such bounds on the entries of \(M\) do not impact our earlier results, and so such functions do not preserve rank or approximately preserve rank for these matrices.

Unfortunately, it is not clear how to apply this to speed up the polynomial method applications we discussed earlier. Most known applications of stable rank require one to have access to the entire matrix \(M^{f}\) (in order to, for instance, apply sketching), whereas we are aiming for algorithms whose running time is sublinear in the number of entries of \(M^{f}\). Nonetheless, this is an exciting avenue where one can strictly generalize the polynomial method, and we believe it will have interesting algorithmic applications, and further motivates algorithmic applications of stable rank.

Theorem 2.8 tells us that functions which do not grow too quickly preserve stable rank, although the desired rate of growth depends on the matrix entries. We also prove a complementary result about functions which do grow very quickly: In Theorem J.5 we prove that any super-polynomial function which grows like \(x^{^{c}(x)}\) for any \(c>0\) does not preserve low stable rank, and applying it entry-wise to an \(n n\) matrix of rank \(O( n)\) can result in a matrix of stable rank \(>n-1\). Hence, there is a limit to how much one could improve our Theorem 2.8.

## 3 Kernel Methods

Our second main application of our techniques is to the study of kernel methods in machine learning. Much of the prior work on kernels methods focuses in the Euclidean distance setting. Our new result shows how to classify kernels in the Manhattan distance setting. We start with defining positive definite kernel.

**Definition 3.1** (Positive definite Manhattan/Euclidean kernel).: _A function \(f\) is a positive definite \(_{p}\) kernel if, for any \(x_{1}, x_{n}^{d}\) for any \(n\) and \(d\), the matrix \(M^{n n}\) with_

\[M_{i,j}=f(\|x_{i}-x_{j}\|_{p})\]

_is positive semi-definite._

_For \(p=1\), we also say \(f\) is a positive definite Manhattan kernel, and for \(p=2\) we call it a positive definite Euclidean kernel._

Equivalently, \(f\) is a positive definite \(_{p}\) kernel if and only if there exists a function \(F:^{d}\) such that: \( F(x),F(y)=f(\|x-y\|_{p})\) for all \(x,y^{d}\) for all \(d\). Note that \(\) represents Hilbert space. The proof of the equivalence can be found in . Positive definite kernels are used in machine learning to separate data embedded in \(^{d}\) using linear separator techniques, when the initial data is not linearly separable . In other words, a positive definite kernel can map points in \(^{d}\) which are not linearly separable, to points in potentially higher dimensions which are linearly separable. Finding such an embedding is not an easy task in general, but kernel methods solve this problem.

Many regression algorithms _require_ the kernel to be positive definite . The key idea is to pick a function \(f\) based on the application so that a function \(F\) like the one in Definition 3.1 can be found which maps the data points to vectors of possible higher dimensions, after which linear separation can be performed efficiently on these higher dimensional points.

Interestingly, linear separator algorithms such as the widely used _Support Vector Machines_ (SVMs)  can separate the data efficiently as long as \( F(x),F(y)\) is easily computed for any \(x,y^{d}\), even if \(F\) itself cannot be easily computed. By definition of the positive-definite kernel \(f\), we know that

\[ F(x),F(y)=f(\|x-y\|_{p}),\]

which allows us to compute \( F(x),F(y)\) quickly by instead computing \(f(\|x-y\|_{p})\). In other words, in order to apply linear separator algorithms, it suffices to know that an \(F\)_exists_, and not necessarily know what it is or how to compute it.

The main known result behind kernel methods is a full classification of all positive-definite Euclidean kernels in terms of completely monotone functions, which are defined as follows:

**Definition 3.2** (Completely monotone functions ).: _A function \(f:_{ 0}_{ 0}\) is completely monotone if_

\[(-1)^{k}f^{(k)}(x) 0\]

_for all \(k 0,x>0\), and \(f(0)_{x 0^{+}}f(x)\)._

An example of a completely monotone function is \(f(x)=e^{-x}\). Prior work  shows that function \(f:\) is a positive-definite Euclidean kernel (Definition 3.1) if and only if \(f()\) is a completely monotone function (Definition 3.2). A natural question to ask is

**Question 3.3**.: _Is there a result that classifies all positive definite Manhattan kernels?_

In our paper, we classify all positive-definite Manhattan kernels. These kernels are widely used in machine learning for physical and chemical applications . A notable example of such a kernel is the Laplace kernel \(f_{}(x)=e^{- x}\) which is commonly used in classification tasks . However, a full description of all positive-definite Manhattan kernels was not known before our work. In this work, we answer Question 3.3 positively:

**Theorem 3.4** (Main result, informal statement of Theorem G.2).: \(f\) _is a positive definite Manhattan kernel (Definition 3.1) if only if \(f(x)\) is completely monotone (Definition 3.2)._

Theorem 3.4 classifies all positive-definite kernels when the input distance is Manhattan. It was previously known that completely monotone functions are positive definite Manhattan kernels [10,Ass80, DL09], but it was not known these were the only such functions. Interestingly, our new classification is similar to the classification result for Euclidean kernels, but without a square root applied to the input. Prior to our result, one could have imagined that there are other positive definite Manhattan kernels to use in SVMs than were previously known. However, our result shows that there are no other such kernels.

We note that our proof techniques also give a new proof of the known result classifying all positive definite Euclidean kernels. This known result is a core insight at the heart of kernel methods in machine learning , but traditional proofs tend to use methods related to infinite dimensional harmonic analysis .

## 4 Metric Transforms

Our final application of our techniques is to _metric transforms_, a mathematical notion introduced by Von Neumann and Schoenberg .

**Definition 4.1** (Metric transform).: _Suppose \(\) and \(\) are semi-metric spaces4. Function \(f\)**transforms**\(\) to \(\) if, for any finite set \(S\), there is a function \(F:\) such that \(f(d_{}(x_{1},x_{2}))=d_{}(F(x_{1}),F(x_{2})),\) for all \(x_{1},x_{2} S\)._

As we discussed earlier, metric transforms arise naturally in many settings where one wants to transform a set of points from a metric space while maintaining some of the metric structure between them, and they have proven useful for algorithm design in many areas.

Typically we have particular metric spaces \(\) and \(\) of interest, and would like to determine which functions transform \(\) to \(\). This leads to the key question in metric transforms:

**Question 4.2**.: _For a given semi-metric space \(\) and a given semi-metric space \(\), what is the full classification of functions \(f\) that transform \(\) to \(\)?_

Metric transforms in the special case where \(\) and \(\) are both Euclidean distances5 or close variants are well-studied. Related to Schoenberg and Von Neumann's work , Schoenberg  classified all functions that transform Euclidean distances to Euclidean distances. One natural question arises: what is the theory of metric transforms for non-Euclidean metrics?

In the case when \(\) is Manhattan (or \(_{1}\)) distance, and \(\) is Euclidean distance, Schoenberg  provided a partial categorization of functions that transform Manhattan distance to Euclidean distance. This was followed by Assouad's work in 1980, which provided a partial categorization of functions that transform Manhattan distances to Manhattan distances . This setting is particularly well-motivated in physical applications. For instance, recent work  studied the problem of inferring a force vector given a collection of example configurations via kernel ridge regression; in order to encode certain desired symmetries ('axis reflections') in the problem, Manhattan preserving functions must be used to define the kernel. Our work on metric transforms completes the partial categorizations of Schoenberg and Assouad, and proves their partial categorization is a full categorization.

Our main result about metric transforms is a complete classification of functions that transform Manhattan distances to Manhattan distances. First, we need to define Bernstein functions:

**Definition 4.3** (Bernstein functions ).: _A function \(f:_{ 0}_{ 0}\) is Bernstein if \(f(0)=0\) and its derivative \(f^{}\) is completely monotone (see Definition 3.2) when restricted to \(^{+}\). Equivalently, a function \(f\) is Bernstein if:_

* \((-1)^{k^{k}f(x)}{x^{k}}} 0\) _for all_ \(k 1,x 0\)_;_
* \(f(x) 0\) _for all_ \(x 0\)_; and__._
* _3._ \(f(0)=0\)_._6__ 
Now we are ready to state our main result:

**Theorem 4.4** (Main result, classifying all Manhattan metric transforms, informal version and combination of Theorem E.2 and F.3).: _For a function \(f:_{ 0}_{ 0}\), the following are equivalent:_

1. \(f\) _is Bernstein._
2. \(f\) _transforms Manhattan distances to Manhattan distances._
3. \(f\) _transforms Manhattan distances to squared Euclidean distances._

It was previously known that Bernstein functions transform Manhattan distances to Manhattan distances , and that they transform Manhattan distances to squared Euclidean distances , but in both cases, it was not previously known that these were the only such functions. It was previously conceivable that, in situations where one needs a metric transform involving Manhattan spaces, but Bernstein functions do not suffice, one could find other suitable metric transforms; our Theorem 4.4 rules out such a possibility. This also has a number of simple consequences, for instance: given any \(n\) points \(x_{1}, x_{n}\) in the metric space \((^{d},_{1})\) for any \(d\), one can use our construction in Theorem 4.4 to explicitly calculate a finite dimensional embedding \(F:^{d}^{2^{d}}\) such that \(\|F(x_{i})-F(x_{j})\|_{1}=f(\|x_{i}-x_{j}\|_{1})\).

## 5 Core Tool: Representation Theory of the Real Hyperrectangle

Our core mathematical tool to tackle all three problems is a new technique we call the _representation theory of the hyperrectangle_. Given a \(d\) dimensional hyperrectangle (which is just a high dimensional rectangle), consider the matrix \(D\) where the \(ij^{th}\) entry of the matrix is the Manhattan distance between the \(i^{th}\) and \(j^{th}\) vertex of the hyperrectangle. We prove this matrix has three key properties:

1. It is a \(2^{d} 2^{d}\) matrix whose rank is \(d+1\), and thus it is a low rank matrix.
2. This matrix is filled with Manhattan distances between points.
3. Applying \(f\) entry-wise to this matrix does not change the eigenvectors of this matrix, which are always the columns of the so-called Hadamard matrices .

We note that the last property is particularly useful for us: it allows us to provide a closed formula for the eigenvectors and eigenvalues of \(D^{f}\). This is particularly useful because all of our key questions (on low rank preservation, kernels, and metric transforms) can be viewed as questions about the eigenvalues of certain matrices after a function is applied entrywise.

The last property can be verified by linear algebra computation, but it can also be seen as a consequence of group representation theory. Thus, we call our approach the representation theory of the hyperrectangle. For proofs of all three properties, refer to Appendix B.1 and I.

## 6 Conclusion

We demonstrate that low-degree polynomials are the only functions that consistently result in a low-rank matrix when applied entry-wise to an existing low-rank matrix, and discuss applications to transformers and LLMs. Additionally, we classify all positive definite kernels that utilize Manhattan distance as their input, enhancing the theoretical framework for applications in various machine learning tasks. Furthermore, we provide a complete categorization of functions capable of transforming Manhattan distances into either Manhattan distances or Euclidean distances. We do all three tasks using a new linear algebraic tool called the representation theory of the hyperrectangle. Our findings not only advance the theoretical understanding of attention and kernel methods, but also open up new possibilities for their application in fields such as computational biology and algorithm design. This work completes the theoretical landscape of Manhattan to Manhattan metric transforms, and utilizes a sophisticated blend of mathematical techniques from several domains.

## 7 Limitations

In this paper, we identify a few areas that require further exploration. Firstly, there remains a small constant-factor gap between Fact 2.1 and Theorem 2.5 that has not been fully explored; details can be found in the last paragraph of Section 2.1. Additionally, the application of the stable rank results presents an ongoing challenge, as discussed in the second-last paragraph of Section 2.2. Finally, our analysis primarily focuses on LLMs, kernel methods, and metric transforms, potentially limiting its applicability to other methodologies.

## 8 Societal Impacts

This paper contributes positively by providing a deeper and more comprehensive study of kernel functions and completes the theory of Manhattan to Manhattan metric transforms, a problem that has persisted since 1980 due to Assouad's work. It opens up numerous algorithmic applications, potentially including large language models (LLMs), and offers a new direction for designing faster algorithms using stable rank results. However, the practical application of these results remains an open area, requiring additional time and effort to fully realize their potential.

## 9 Acknowledgments

The authors would like to thank Amol Aggarwal, Ainesh Bakshi, Robi Bhattacharjee, Jerry Yao-Chieh Hu, Rajesh Jayaram, Monique Laurent, Roie Levin, Yingyu Liang, Han Liu, Ryan O'Donnell, Zhenmei Shi, Kevin Tian, Alex Wang, Yu Wang, Yufa Zhou, Lichen Zhang, Di Zhu, and Goran Zuzic for support and helpful comments. The authors would like to thank Yufa Zhou for writing the conclusion and checklist of the paper. This work was mostly done when Timothy Chu was at Carnegie Mellon University, and later at Google, and when Shyam Narayanan was a student at MIT. Josh Alman is supported in part by NSF Grant CCF-2238221 and a Google Research Scholar Award. Gary Miller is supported in part by NSF Grant CCF-1637523. Shyam Narayanan is supported by an NSF Graduate Fellowship and a Google Fellowship. For more information related to the paper and adjacent topics, see https://www.youtube.com/@zhaosong2031 and https://space.bilibili.com/3546587376650961.