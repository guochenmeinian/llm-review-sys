# Generalized Balancing Weights via Deep Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, _Neural Balancing Weights_ (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of \(f\)-divergence. For this, we selected \(\)-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking the balance of the distribution changed by the weights. Finally, we discuss the sample size requirements for the weights as a general problem of a curse of dimensionality when balancing multidimensional data. Our study provides a basic approach for estimating the balancing weights of multidimensional data using variational \(f\)-divergences.

## 1 Introduction

Estimating causal effects from observational data is a central problem in many application domains, including public health, social sciences, clinical pharmacology, and clinical decision-making. One standard approach is balancing covariates with weights that are the same as the density ratios between the source and balanced distributions, such that their distribution mimics randomization. Many methods have been developed to estimate the balancing weights, such as inverse propensity weighting (IPW) Rosenbaum and Rubin , augmented inverse propensity weighting (AIPW) , generalized propensity score (GPS) , covariate balancing propensity score (CBPS) , overlap weighting , and entropy balancing (EB) [8; 30]. However, these methods are limited to categorical or continuous interventions.

In this study, we propose generalized balancing weights to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. To the best of our knowledge, no causal inference method focusing on the balancing weights exists for this problem. We approach this problem by directly estimating the density ratio, more precisely, the Radon-Nikodym derivatives, between the source and balanced distributions using a neural network algorithm by optimizing a variational representation of a \(f\)-divergence. \(f\)-divergences, whose values are greater than or equal to zero and considered zero if the two distributions are equal, are the statistics used to measure the closeness of the two distributions. The optimal functions for the variational representations derived from \(f\)-divergences with the Legendre transform correspond to the density ratio between the distributions. An approach to estimate the density ratio by optimizing a variational representation of a \(f\)-divergence was developed in the domain adaptation region .

However, optimizing the \(f\)-divergences, including estimating the density ratio, is challenging. This is due to the following reasons. First, for KL-divergence, the dominant \(f\)-divergence, the requirements for sample size increase exponentially with the true amount of the divergence [14; 28]. Second, a naive gradient estimate over mini-batch samples leads to a biased estimate of the full gradient . Third, gradients of neural networks often vanish when the estimated probability ratios are close to zero .

To avoid the first problem, we focus on \(\)-divergence, which is a subgroup of \(f\)-divergence. \(\)-divergence has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients. In addition, by selecting \(\) from a particular interval, we avoid vanishing gradients of neural networks when the neural networks reach extreme local minima.

In addition, we provide two techniques for estimating the balancing weights. First, we propose a validation method using test data and an early stopping method to improve the generalization performance of balancing. The generalization performance of the weights worsens as the dimensions of the data increase, and the sample size requirements of the weights increase exponentially with the dimensions. Next, we present a method for measuring the performance of balancing weights by estimating the \(\)-divergence information to check the balance of the distribution,

This study is divided into seven parts. First, we introduce the background of the study. Second, we review related studies. Third, we define the terminology and concepts for causal inferences. Fourth, we present our novel method for estimating balancing weights. Fifth, we provide techniques for estimating the weights. Sixth, we discuss the sample requirements for the weights. Finally, we conclude this paper. All the numerical experiments and proofs are described in the appendix.

## 2 Related Work

**Balancing weight: Balancing weight:** Many methods have been proposed to estimate the balancing weights. The following methods are proposed for binary intervention: IPW , AIPW , CBPS , and overlap weighting . The following methods have been proposed for continuous intervention: GPS  and EB [8; 30]. **Statistical divergences and density ratio estimation:** Despite the abundance of classic studies [15; 29], we focused on studies that directly estimate density ratios or optimize statistical divergences using neural networks. In this review, these studies have beclassified into four groups. First is the estimation of KL-divergence or mutual information [3; 18; 21]; the second is density ratio estimation ; the third is generative adversarial networks (GANs) [17; 31; 6; 32] (statistical divergences were used as discriminators for GANs); and the fourth is domain generation [27; 6; 35; 1]. In addition to these application studies, divergences were improved .

## 3 Terminologies and Definitions

Here, we briefly introduce the terminology and definitions used in this study.

**Notations and Terminologies.** Random variables are denoted by capital letters; for example, \(A\). Small letters are used for the values of random variables of the corresponding capital letters; \(a\) is the value of the random variable \(A\). Bold letters \(\) or \(\) represent a set of variables or random variable values. In particular, \(=\{V_{1},,V_{n}\}\) are used for the observed random variables and \(=\{U_{1},,U_{m}\}\) are used as unobserved random variables. For example, the domain of the variable \(A\) is denoted by \(_{A}\), and \(_{A_{1}}_{A_{n}}\) is denoted by \(_{}\) for \(=A_{1} A_{n}\). \(\) are assumed to be semi-Markovian models and \(G=G_{}\) denotes the causal graph for \(\). \(Pa()_{G}\), \(Ch()_{G}\), \(An()_{G}\), and \(De()_{G}\) represent parents, children, ancestors, and descendants of the observed variables in \(G\), respectively, for \(\). In this study, \(Pa()_{G}\), \(Ch()_{G}\), \(An()_{G}\), and \(De()_{G}\) do not include \(\). \(P\) and \(Q\) are used as the probability measures on \((^{d},)\), where \(\) denotes the \(\)-algebra of subsets of \(^{d}\). \(E_{P}[]\) and \(E_{P}[|]\) denote expectation and conditional expectation under the distribution \(P\), respectively. For example, \(E_{P}[]=_{_{}}dP\) and \(E_{P}[|]=_{_{}}dP(| )\). \(_{P}[]\) denotes the empirical expectation under \(P\); that is, the sample mean of the finite observations drawn from \(P\). \(P\) is called _absolute continuous_ with respect to \(Q\), \(P(A)=0\) whenever \(Q(A)=0\) for any \(A\), which is represented as \(P Q\). \(\) denotes the Radon-Nikodym derivative of \(P\) with respect to \(Q\) for \(P\) and \(Q\) with \(P Q\). In this study, we refer to density ratios as the Radon-Nikodym derivatives. \(\) denotes a probability measure on \(^{d}\) with \(P\) and \(Q\). \(^{(N)}=\{^{1},,^{N}\}\) denotes \(N\) i.i.d. random variables from \(\). \(_{P}^{(N)}=\{_{ P}^{1},,_{  P}^{N}\}\) and \(_{Q}^{(N)}=\{_{ Q}^{1},,_{  Q}^{N}\}\) denote variables defined as \(P(_{ P}^{i})=(^{i} )\) and \(Q(_{ Q}^{i})=(^{i} )\), \(^{d}\), for \(1 i N\). We represent \(f g\) when \(_{n}f(n)/g(n)<\) holds. The notation \(f g\) is defined similarly.

### Definitions

In this study, we considered the causal effects of joint and multidimensional interventions. For clarity, we used different notations, "_do_ " and " \(\)," for single-dimensional and multidimensional interventions, respectively. 1 For a single-dimensional intervention, a _do_ symbol is used, which is the same as Pearl's \(do\)-calculation.

**Definition 3.1** (_do_-calculation, Pearl(2009)).: For the two given disjoint sets of \(,\), the causal effect on \(\) for intervention in \(\) with values \(\), denoted by \(P(|do(=))\), is defined as the probability distribution, such that

\[P(|do(=))=_{^ {}_{^{}}\\ pa_{}_{Po()_{G}}},=,Pa()_{G}=pa_{},^{}=^{})}{P(=|Pa()_{G}= pa_{})},\] (1)

where \(^{}=( Pa()_{G} )\). The causal effect of \(\) on \(\) under the conditions \(\) denoted by \(P(=|do(=),=)\) is defined as the probability distribution, such that

\[P(=|do(=),)==,|do(=))}{P(|do (=))}.\] (2)

Notably, from Definition 3.1, a \(do\)-calculation for a set of variables coincides with the simultaneous interventions for each variable:

\[P(|do())=P(|do(X_{1}),do(X_{2}),,do(X_{n })),\] (3)

where \(=\{X_{1},X_{2},,X_{n}\}\). Here, we refer to each intervention in (3) as a " single-dimensional intervention ".

Furthermore, we use the \(\) symbol for multidimensional intervention. Intuitively, a \(\) symbol represents the intervention of the variables that preserves the functional relationship within the variables.

**Definition 3.2** (\(\) symbol).: \(\) symbol defines the following probability distribution:

\[P(|(}),(}), ,(}))=P(|do()) P( }) P(}) P(}),\] (4)

where \(=_{1}_{2}_{n}\).

\(\) symbols are useful, particularly when we consider interventions in a multivalued discrete variable expressed using one-hot encoding. In this case, we cannot express the causal effect effectively using \(do\) symbols. For example, let us consider the case of an intervention in the ternary variable \(X\), \(_{X}=\{x_{1},x_{2},x_{3}\}\) and let \(X\) be expressed by \(^{}=(X_{1}^{},X_{2}^{},X_{3}^{})\), such that \(X_{i}^{}=1\) if \(X=_{i}\) otherwise \(X_{i}^{}=0\) for \(i=1,2,3\). Then, \(P(|do(X=_{3}))\) is the same as \(P(|(^{}=(0,0,1)))\), which differs from \(P(|do(^{}=(0,0,1)))\). We refer to this type of intervention as a " multidimensional intervention ".

Next, we provide definitions of the \(f\)-divergence and \(f\)-divergence information.

**Definition 3.3** (\(f\)-divergence).: The \(f\)-divergence \(D_{f}\) between the two probability measures \(P\) and \(Q\) with \(Q P\) induced by a convex function \(f\) satisfying \(f(1)=0\) is defined by \(D_{f}(Q||P)=E_{P}[f(dQ/dP)]\).

Many divergences are specific cases obtained by selecting a suitable generator function \(f\). For example, \(f(u)=u u\) corresponds to the KL-divergence. In particular, we focus on \(\)-divergence, which is expressed as follows:

\[D_{}(Q||P)=E_{P}[\{()^{1-}-1\}],\] (5)

where \(\{0,1\}\). From (5), Hellinger divergence is obtained as \(=1/2\), and \(^{2}\) divergence by \(=-1\).

From \(f\)-divergence, the \(f\)-divergence information is defined as the mutual information if we choose the KL-divergence as the \(f\)-divergence. Here, we present a definition of \(f\)-divergence information for multi-variables.

**Definition 3.4** (\(f\)-divergence information).: For disjoint variables \(=\{_{1},_{2},,_{n}\} \), let \(P_{}\) be the joint probability measure for \(\). For each \(i=1,2,,n\), \(P_{_{i}}=_{_{i}_{i}}dP_{}\) is a measure of the marginal distribution of \(P_{}\) for \(_{i}\). The \(f\)-divergence information for \(_{1},_{2},,_{n}\) under \(P_{}\) and a convex function \(f\) satisfying \(f(1)=0\) is defined as the \(f\)-divergence between \(P_{}\) and \(P_{_{1}} P_{_{2}} P_{_{ n}}\):

\[I_{f}(_{1},_{2},,_{n};P_{ }) = E_{P_{}}[f(_{1}} dP _{_{2}} dP_{_{n}}}{dP_{}} )].\] (6)

## 4 Problem Set Up

Before describing the details of the problem, we provide a notation for the probability distribution, which is the goal of balancing. Hereafter, \(P\) denotes the probability distribution of observational data. For the given disjoint sets \(_{1},_{2},,_{n},, \), let \(\) be a probability distribution, as follows:

\[ = P(|(_{1}), {d}(_{2}),,(_{ n}),) P()\] (7) \[= P(|d(),) P( _{1}) P(_{2}) P(_{n})  P(),\]

where \(=_{1}_{2}_{n}\). \(\) is the probability distribution of the counterfactual data from simultaneous (multidimensional) interventions in \(_{1},_{2},,_{}\) under the condition \(\).

Objective.The objective of this study is to obtain the balancing weights that transform \(P(,,)\) into \((,,)\). More precisely, given the i.i.d. observational data \(\{(^{i},^{i})|i=1,2,,N\}\), we aim to estimate the weights \(B(,)\), such that

\[E_{}[f(,)]=E_{P}[f(,)(,)]\] (8)

holds for any measurable function \(f\) on \(^{d}\). If we obtain the weights, we estimate the conditional average causal effect (CACE) for \(P(|(_{1}),(_{2}),,(_{n}),)\), that is \(E_{}[|,]\), using state-of-the-art supervised machine learning algorithms, with the weights assigned as the individual weights for each sample.

Assumptions.We assumed the following to achieve our objective:

* Assumption 1. The causal effect \(P(|do())\) is identifiable, or equivalently, \(\) from (7) can be identified. 23 * Assumption 2. Let \(\) = \(P(_{1},_{2},,_{n},)\) and let \(\) = \(P(_{1}) P(_{2}) P(_{n})  P()\). Subsequently, we assume that \(\).

Assumption 2 is the same as the _overlap_ assumption if we consider this a single-dimensional intervention. Here, we propose overlapped assumptions for joint and multidimensional interventions.

Estimation of Balancing Weights

In this section, we present the way to effectively estimate the probability density ratios by optimizing \(f\)-divergence.

Density Ratios as Balancing Weights.We first note that the density ratios, which are referred to as the Radon-Nikodym derivative in this paper, are equal to the balancing weight of the target. For a density ratio of \(P\) to \(\), that is \(}{dP}\), it holds that

\[E_{}[f]= f}{dP} dP=E_{P}[f }{dP}],\] (9)

for any measurable function \(f\) in \(^{d}\). Then, (8) and (9) are equivalent. As an example of the aforementioned density ratio, let \(X\) be a binary variable with \(_{X}=\{1,0\}\) and let \(\) be covariates. Using propensity score \(e()=P(X=1|=)\), we observe that \(}{dP}(X=1,)=P(X=1)/e()\) and \(}{dP}(X=0,)=P(X=0)/(1-e())\). That is, \(}{dP}\) is the stabilized inverse probability of the treatment weighting .

### Our Approach

Our approach involves obtaining the density ratios as an optimal function for a variational representation of an \(f\)-divergence. This approach is based on the fact that the optimal function is connected to density ratios .

Variational representation.Using the Legendre transform of the convex conjugate of a twice differentiable convex function \(f\), \(f^{*}()=_{r}\{ r-f(r)\}\), we obtain a variational representation of \(f\)-divergence:

\[D_{f}(Q||P)=_{ 0}\{E_{Q}[f^{}()]-E_{P}[f^{*}(f^{} ())]\},\] (10)

where supremum is considered over all measurable functions with \(E_{Q}[f^{}()]<\) and \(E_{P}[f^{*}(f^{}())]<\). The maximum value is achieved at \(=dQ/dP\).

We obtained the optimal function for (10) by replacing \(\) in the equation with a neural network model \(_{}\) and training it through back-propagation with a loss function, such that

\[()=-\{_{Q}[f^{}(_{})]-_{P }[f^{*}(f^{}(_{}))]\}.\] (11)

Selecting \(\)-divergence for Optimization.We select \(\)-divergence for the following reasons. First, the sample size requirements for \(\)-divergence is independent of its ground truth value: second, it has unbiased mini-batch gradients; third, it can avoid a vanishing gradient problem.

The variational representation of \(\)-divergence is as follows (Lemma C.1 in Appendix C):

\[D_{}(Q||P)=_{ 0}\{- {}E_{Q}[^{-}]-E_{P}[^{ 1-}]\}.\] (12)

Sample size requirements for \(\)-divergence.The \(\)-divergence has an estimator with sample complexity \(O(1)\) (Corollary 1 in Birrell et al., 2022, P19; Corollary C.10 in Appendix C). Conversely, the sample complexity of KL-divergence is \(O(e^{KL(Q||P)})\)[14; 28]:

\[_{N}}(Q||P )}{KL(Q||P)^{2}}-1}{KL(Q||P)^{2}},\] (13)

where \(}(Q||P)\) is the KL-divergence estimator for sample size N using a variational representation of the divergence, and \(KL(Q||P)\) is the ground truth value.

Unbiasedness for mini-batch gradients.\(\) in (12) can be expressed in a Gibbs density form (Proposition C.2 in Appendix C). Then, we observe that

\[D_{}(Q||P)=_{T}\{-E_{Q} [e^{ T}]-E_{P}[e^{(-1) T }]\},\] (14)

where supremum is considered over all measurable functions \(T:^{d}\) with \(E_{P}[e^{(-1) T}]<\) and \(E_{Q}[e^{ T}]<\).

From this equation, we obtain our loss function, which has unbiasedness for mini-batch gradients (Proposition C.8 in Appendix C), as follows :

\[_{}()=_{Q}[e^{ T _{}}]+_{P}[e^{(-1) T_{ }}].\] (15)

Advantage in vanishing gradients problem.By setting \(\) within \((0,1)\), we can avoid vanishing gradients of neural networks when they reach the extreme local minima. The vanishing-gradient problem for optimizing divergence is known in GANs . Now, we consider the case where the probability ratio \(e^{T_{}()}\) in (15) is nearly zero or large for some point \(\), corresponding to cases in which the probabilities for \(P\) or \(Q\) at some points are much smaller than those for the other.

To show the relation between \(e^{T_{}()}\) and the learning of the neural networks, we obtain gradient of (15):

\[_{}_{}()=_{Q}_{ }T_{} e^{ T_{}}-_{P}_{ }T_{} e^{(-1) T_{}}.\] (16)

The behavior of \(_{}_{}()\) when \(E_{Q}[e^{T_{}}] 0\) or \(E_{Q}[e^{T_{}}]\), under some regular conditions for \(T_{}\) and an assumption that \(P Q\), can be summarized as follows: Let \(E[\ \ ]\) denote \(E_{P}[E_{Q}[\ \ ]]\), then

\[>1\]

: \[E[_{}_{}()]\]

 (as \[E_{Q}[e^{T_{}}] 0\] ), and \[E[_{}_{}()]-\]

 (as \[E_{Q}[e^{T_{}}] 0\] ), and \[E[_{}_{}()]-\]

 (as \[E_{Q}[e^{T_{}}] 0\] ).

Notably, \(E_{Q}[e^{T_{}}] 0\)\(\)\(E_{P}[e^{T_{}}] 0\)\(E_{Q}[e^{T_{}}]\)\(\)\(E_{P}[e^{T_{}}]\), because \(Q P\) and \(P Q\).

For \(>1\) and \(<0\), cases exist where \(E[_{}_{}()]\). This implies the possibility that the neural networks reach extreme local minima such that their estimations for density ratios are \(0\) or \(\). However, this problem can be avoided by selecting \(\) from interval \((0,1)\). We note that the selecting of \(\) does not cause instability in numerical calculations for cases where \(E[_{}_{}()]-\). In Appendix D.1, we present numerical experimental results for different values of \(\).

## 6 Method

In this section, we first present the main theorem that summarizes the new balancing weight method proposed herein. Next, we present the balancing weight method.

### Main Theorem

Here, we present the main theorem that summarizes the new balancing weight method proposed herein.

**Theorem 6.1**.: _Given disjoint sets of \(=\{_{1},_{2},,_{n}\},, \) satisfying_

\[=\{_{1},_{2},,_{n}\} An( )_{G} De()_{G}=.\] (17)

_Let \(=P(_{1},_{2},,_{n},)\) and \(=P(_{1}) P(_{2}) P( _{n}) P()\), and \(=P(|do(),) P(_{1})  P(_{2}) P(_{n}) P()\). We assume that \(P\) satisfies Assumptions 1 and 2 in the aforementioned setting, and it holds that \(E_{}[(d/d)^{1-}]<\) for some \(0<<1\), then, for the optimal function \(T^{*}\), such that_

\[T^{*}(_{1},_{2},,_{n},)=_ {T^{}}\{E_{}[e^{  T}]+E_{}[e^{(-1) T }]\},\] (18)it holds that_

\[}{dP}=e^{-T^{}(_{1},_{2},, _{n},)}.\] (19)

_Here, \(^{}\) denotes the set of all non-constant functions \(T():^{d}\) with \(E_{}[e^{(-1) T()}]<\)._

Proof.: See Appendix C. 

Here, we mention that the assumption (17) is necessary for the (19) to hold, which is derived from our Theorem C.15 in Appendix C.

### Balancing Weight Method

We present the implementation of training a neural balancing weights (NBW) model in Algorithm 1. It is important to consider the stopping time \(K\) for neural network model \(T_{_{K}}\) in Algorithm 1, which is discussed in the next section. To obtain the sample mean under \(\), that is, the estimator for \(E_{}[e^{ T_{}}]\) in (18), a shuffling operation can be used for the samples. Now, we define neural balancing weights (NBW). 45

**Definition 6.2** (Neural Balancing Weights).: Let \(T_{_{K}}\) be a neural networks obtained from Algorithm 1. Then, the NBW of \(T_{_{K}}\), expressed as \((_{1},_{2},,_{n},;T _{_{K}})\), are defined as

\[(_{1},_{2},,_{n},;T _{_{K}})=e^{-T_{_{K}}(_{1},_{2}, ,_{n},)},\] (20)

where \(Z=_{}[e^{-T_{_{K}}(_{1},_{2}, ,_{n},)}]\).

We estimate \(E_{}[[,]\), that is the CACE for \(P(|}(}),}( }),,}(}),)\), using \((_{1},_{2},,_{n},;T _{_{K}})\) as the sample weights of the supervised algorithm:

\[_{}[|,]= _{P}[_{_{K}}, ].\] (21)

Here, \(_{P}\) corresponds to the model of a supervised machine learning algorithm. As an example, we demonstrate a back-propagation algorithm using balancing weights for the mean squared error (MSE) loss in Algorithm 3 in Appendix E.

## 7 Techniques for NBW

We propose two techniques for estimating balancing weights: (i) improves generalization performance of the balancing weights. (ii) measures the performance of the balancing weights by estimating the \(\)-divergence information.

### Improving the Generalization Performance of the Balancing Weights

In this section, we first present an overfitting problem for balancing distributions. We then present two methods for improving the generalization performance of the weights: a validation method using test data and an early stopping method. Herein, let \(T_{_{t}}\) denote an NBW model at step \(t\) in Algorithm 1. Let \(}_{Q}^{(N)}(t)=e^{-T_{_{t}}}_{P}^{(N)}\), that is, the data balanced by the weights of \(e^{-T_{_{t}}}\). Subsequently, let \(_{t}^{(N)}\) and \(^{(N)}\) denote the probability distributions of \(}_{Q}^{(N)}(t)\) and \(}_{Q}^{(N)}\), respectively, which correspond to the estimated and true distributions for balancing.

An overfitting problem for balancing distributions.From Corollary C.12 in Appendix C, we observe \(}_{Q}^{(N)}(t)}}{{ }}_{Q}^{(N)}\) as \(t\). Then, Theorem 1 in  shows that

\[_{t}W_{1}(Q,_{t}^{(N)})=W_{1}(Q,^{(N)}) N^{- 1/(d-)}(\,>0\,),\] (22)

where \(W_{1}\) is the Wasserstein distance of order \(1\) and \(d\) is the lower Wasserstein dimension defined in . (22) implies that, for balancing finite data, the destination of the balanced distribution is an empirical distribution, and the generalization performance of balancing worsens exponentially when the dimension of the data is larger. In view of optimizations of GANs,  referred to this phenomenon the " memorization " and proposed an early stopping method.

Validation method using test data.We can use a validation method using test data. Because \(^{(N)}\) and \(^{(N)}\) are empirical probability distributions, we observe that \(d^{(N)}/d^{(N)}(x)=dQ/dP(x)\) if \(x^{(N)}\), otherwise \(d^{(N)}/d^{(N)}(x)=0\) (Proposition C.17 in Appendix C). Then, the optimal function of (15) for both distributions, that is \(T_{*}^{(N)}=-log(d^{(N)}/d^{(N)})\), is infinite except for the observations, and the loss of the \(T_{*}^{(N)}\) is infinite for data independent of the observations. This implies that the loss of \(T_{t}^{(N)}\) for the test data turns to increase from the middle of the training period, and we can determine the training step at which the generalization performance of the weights begins to worsen. In Section D.2 in Appendix D, we provide numerical experimental results to confirm the relationship between dimensions of data (\(d\)) and steps in training (\(K\)).

Early stopping method.In addition, we present an early stopping method for estimating the balancing weights as follows, which is inspired by the method developed in  (Corollary C.24 in Appendix C): for some \(>0\), let

\[K_{0}=C N^{2/(d+)},\] (23)

where \(C>0\) is constant. Then, we have \(W_{1}(Q,_{K_{0}}^{(N)}) N^{-1/(d+)}\). Unfortunately, the curse of dimensionality remains in the proposed method. This will be discussed in the next section.

### Measuring the Performance of the Balancing Weights

Let us assume that we obtain an NBW model \(T_{_{0}}\) and let \(_{_{0}}=(_{1},_{2},, _{n},;T_{_{0}})\) be the balancing weights of \(T_{_{0}}\). If \(_{_{0}}\) successfully estimates \(\), then the \(\)-divergence between \(Q\) and \(P_{0}\) will be nearly zero. Conversely, if \(_{_{0}}\) fails to estimate \(\), the \(\)-divergence between \(Q\) and \(P_{0}\) is significantly different from zero. This implies that we can measure the performance of the balancing weights using the \(\)-divergence information for \(P_{0}\).

Next, we present the definition of an \(\)-divergence information estimator using neural networks.

**Definition 7.1** (Neural \(\)-divergence Information Estimator).: For disjoint variables \(_{1},_{2},,\)\(_{n}\), the neural \(\)-divergence information estimator for \(P\) is defined as

\[_{}(_{1},_{2},,_{n};T_{ _{*}})=-_{}\{ {}_{Q}[e^{ T_{}}]+ _{P}[e^{(-1) T_{}}]\}.\] (24)

To measure the performance of balancing the weights from the NBW model, we estimate the \(\)-divergence information for balanced distribution from the weights. That is, we use the sample mean under a balanced distribution, despite the sample mean under \(P\) for (24). For example, we assumethat we have certain weights \(BW^{}=\{bw^{i}:i=1,2,...,N\}\), where \(bw^{i}\) denotes the weight of sample \(i\) of \(N\). The balanced distribution from the weights is

\[dP^{}=BW^{} dP.\] (25)

The \(\)-divergence information for \(P^{}\) is estimated by replacing \(P\) with \(P^{}\) for (24) in the following manner: despite the sample mean \(_{P}[e^{(-1) T_{}}]\) for these equations, we use the weighted sample mean, such that

\[_{P^{}}[e^{(-1) T_{}}]=_{i=1}^ {N}bw^{i} e^{(-1) T_{}(_{1}^{i},_{2 }^{i},,_{n}^{i},^{i})}.\] (26)

Details on the implementation for measuring the performance of balancing weights from an NBW model are provided in Algorithm 2, which includes the validation method for the overfitting problem in Section 7.1.

``` Input: Train Data \(\{(_{1}^{i},,_{n}^{i},^{i})\}_{i=1}^{N}\), Test Data \(\{(}_{1}^{i},,}_{n}^{i},^{i})\}_{i=1}^{N}\), A Neural Balancing Weight Model \(T_{}\) Output: The estimated \(\)-divergence information \(_{}\) for the balanced distribution with the balancing weights from \(T_{}\) \[_{1}^{}(\{1:N\})\] \[\] \[_{n}^{}(\{1:N\})\] \[^{}(\{1:N\})\] \[\{bw^{i}\}_{i}^{N}(_{1}^{i },_{2}^{i},,_{n}^{i},^{i})}}{\{e^{-T_{ }(_{1}^{i},_{2}^{i},,_{n}^{i}, ^{i})}\}}\] \[}_{}\{\}\] ```

**Algorithm 2** Algorithm for checking the balance

## 8 Limitations: Sample Size Requirements.

In Section 7.1, we noted that our method has a curse of dimensionality. The sample size requirement of the proposed method is \(N>()^{d+}\) for \(W_{1}(Q,_{K_{}}^{(N)})<\) (Corollary C.25 in Appendix C). However, the curse of dimensionality is an essential problem when balancing multivariate data owing to the following factors. Because the optimal balancing weights defined as (8) for (finite) observational data are the density ratios of the empirical distributions, the distribution of the data balanced by them is the empirical distribution. Subsequently, owing to the balancing of the weights, the curse of dimensionality of the empirical distribution occurs, which is the same as that described in Section 7.1. Therefore, to achieve high generalization performance, we need to obtain weights that differ from the ideal density ratio between the source and target of the empirical distribution. Further research is required to address this problem. In Appendix D.3, we present the numerical examination results in which the causal effects of joint and multidimensional interventions were estimated with different sample sizes.

## 9 Conclusion

We propose generalized balancing weights to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. Three methods for training the weights were provided: an optimization method to learn the weights, a method to improve the generalization performance of the balancing weights, and a method to measure the performance of the weights. We showed the sample size requirements for the weights and then discussed the curse of dimensionality that occurs as a general problem when balancing multidimensional data. Although the curse of dimensionality remains in our method, we believe that this study provides a basic approach for estimating the balancing weights of multidimensional data using variational \(f\)-divergence.