# Inverse Reinforcement Learning with the Average Reward Criterion

Feiyang Wu

feiyangwu@gatech.edu

&Jingyang Ke

jingyang.ke@gatech.edu

&Anqi Wu

anqiuu@gatech.edu

School of Computational Science and Engineering

College of Computing

Georgia Institute of Technology

Atlanta, Georgia 30332

###### Abstract

We study the problem of Inverse Reinforcement Learning (IRL) with an average-reward criterion. The goal is to recover an unknown policy and a reward function when the agent only has samples of states and actions from an experienced agent. Previous IRL methods assume that the expert is trained in a discounted environment, and the discount factor is known. This work alleviates this assumption by proposing an average-reward framework with efficient learning algorithms. We develop novel stochastic first-order methods to solve the IRL problem under the average-reward setting, which requires solving an Average-reward Markov Decision Process (AMDP) as a subproblem. To solve the subproblem, we develop a Stochastic Policy Mirror Descent (SPMD) method under general state and action spaces that needs \((1/)\) steps of gradient computation. Equipped with SPMD, we propose the Inverse Policy Mirror Descent (IPMD) method for solving the IRL problem with a \((1/^{2})\) complexity. To the best of our knowledge, the aforementioned complexity results are new in IRL literature with the average reward criterion. Finally, we corroborate our analysis with numerical experiments using the MuJoCo benchmark and additional control tasks.

## 1 Introduction

Reinforcement Learning (RL) problems are frequently formulated as Markov Decision Processes (MDPs). The agent learns a policy to maximize the reward gained over time. However, in numerous engineering challenges, we are typically presented with a set of state-action samples from experienced agents, or _experts_, without an explicit reward signal. Inverse Reinforcement Learning (IRL) aspires to recover the expert's policy and reward function from these collected samples.

Methods like Imitation Learning  reduces the IRL problem to predicting an expert's behavior without estimating the reward signal. Such formulation is undesirable in some scenarios when we either wish to continue training or analyze the structure of the agent's reward signal, e.g., in reward discovery for animal behavior study . Furthermore, as a popular methodology in Imitation Learning, Generative Adversarial Network (GAN)  suffers from unstable training due to mode collapse . The sampling-based Bayesian IRL (BIRL) approach  treats the reward function as a posterior distribution from the observation. This approach is conceptually straightforward but suffers from slow convergence of sampling. Another line of research reformulatesthe IRL as RL with general utility functions [36; 39; 12; 1]. While theoretically attractive, such a formulation does not learn reward functions.

Furthermore, inferring rewards poses an additional challenge: without any additional assumption, the same expert behavior can be explained by multiple reward signals . To resolve this ambiguity, the authors propose adding the principle of _Maximum Entropy_ that favors a solution with a higher likelihood of the trajectory. In the following work , the objective is replaced by maximizing the entropy of the policy, which better characterizes the nature of stochastic sequential decision-making. A body of work focuses on alleviating the computational burden of the nested optimization structure of under the Maximum Entropy framework. In , the authors skipped the reward function update stage by representing rewards using action-value functions. \(f\)-IRL  minimize the divergence of state visitation distribution for finite-horizon MDPs. More recently, in , the authors propose a dual formulation of the original Maximum Entropy IRL and a corresponding algorithm to solve IRL in the discounted setting.

Despite the success in practice, theoretical understanding of RL/IRL methods is lacking. For RL, specifically for Discounted Markov Decision Processes (DMDPs), analysis of policy gradient methods for solving DMDPs is catching up only until recently [2; 16; 5; 19; 18]. Meanwhile, understanding of the Average-reward Markov Decision Processes (AMDPs) remains very limited, where the policy aims to maximize the long-run average reward gained. Li et al. propose stochastic first-order methods for solving AMDPs with a rate of convergence \(}((^{-1}))\) for policy optimization, but it is constrained to finite states and actions with policy explicitly represented in tabular forms. Moreover, for IRL, the only known finite time analysis are [35; 37; 38; 21] for either solving DMDPs, or finite-horizon MDPs. There are no well-established convergence analyses for IRL with AMDPs. Moreover, prior works only focus on MDPs with finite state and action spaces, which do not fully address many real-life problems that inherently feature continuous elements (e.g., robotics ). Such settings further require general function approximations, the usage of which lacks theoretical understanding in recent work as well.

In summary, all previous works either do not fundamentally address our problem, which necessitates the learning of reward functions, or they miss key elements such as general state and action spaces, general function approximation, and the analysis incorporating approximation errors. Each of these facets demands substantial effort. In this paper, we focus on convergence analyses for average-award MDPs. We extend the RL algorithm  for AMDPs to general state and action spaces with general function approximation and develop an algorithm to solve the IRL problem with the average-reward criterion. Analyses of both algorithms are new in the literature to the best of our knowledge.

### Main contributions

**Stochastic Policy Mirror Descent for solving AMDPs**: Our research introduces the Stochastic Policy Mirror Descent (SPMD) algorithm, a novel approach designed to tackle Average-reward Markov Decision Processes (AMDPs) that involve general state and action spaces. Leveraging a performance difference lemma, we demonstrate that the SPMD algorithm converges within \(}(^{-1})\) steps for a specific nonlinear function approximation class, and in no more than \((^{-2})\) steps for general function approximation.

**Inverse Policy Mirror Descent for solving Maximum Entropy IRL**: We expound a dual form of the Maximum Entropy IRL problem under the average-reward framework with the principle of Maximum Entropy. This dual problem explicitly targets the minimization of discrepancies between the expert's and agent's expected average reward. Consequently, we propose a first-order method named Inverse Policy Mirror Descent (IPMD) for effectively addressing the dual problem. This algorithm operates by partially resolving an Entropy Regularized RL problem at each iteration, which is systematically solved by employing SPMD. Drawing upon the _two-timescale_ stochastic approximation analysis framework , we present the convergence result and establish a \((^{-2})\) rate of convergence.

**Numerical experiments**: Our RL and IRL methodologies have been tested against the well-known robotics manipulation benchmark, MuJoCo, as a means to substantiate our theoretical analysis. The results indicate that the proposed SPMD and IPMD algorithms generally outperform state-of-the-art algorithms. In addition, we found that the IPMD algorithm notably reduces the error in recovering the reward function.

## 2 Background and problem setting

### Average reward Markov decision processes

An Average-reward Markov decision process is described by a tuple \(:=(,,,c)\), where \(\) denotes the state space, \(\) denotes the action space, \(\) is the transition kernel, and \(c\) is the cost function. At each time step, the agent takes action \(a\) at the current state \(s\) according to its policy \(:\). We use \(\) as the space of all feasible policies. Then the agent moves to the next state \(s^{}\) with probability \((s^{}|s,a)\), while the agent receives an instantaneous cost \(c(s,a)\) (or a reward \(r(s,a)=-c(s,a)\)). The agent's goal is to determine a policy that minimizes the long-term cost

\[^{}(s)_{T}_{}[ _{t=0}^{T-1}(c(s_{t},a_{t})+h^{}(s_{t}))s_{0}=s],\] (1)

where \(h^{}\) is a closed convex function with respect to the policy \(\), i.e., there exists \(_{h} 0\) such that

\[h^{}(s)-[h^{^{}}(s)+ h^{^{}}(s,),( |s)-^{}(|s)]_{h}D((s),^{}(s)),\] (2)

where \( h^{^{}}\) denotes the subgradient of \(h\) at \(^{}\) and \(D(,)\) is the Bregman distance, i.e.,

\[D(a_{2},a_{1})(a_{1})-[(a_{2})+(a_{2})^{ },a_{1}-a_{2}]\|a_{1}-a_{2}\|^{2},a_{1},a_{2}.\] (3)

Here \(:\) is a strongly convex function with the associated norm \(\|\|\) in the action space \(\) and let us denote \(\|\|_{*}\) as its dual norm. In this work we also utilize a span semi-norm : \(\|v\|_{ p,} v- v, v^{n}\). If \(h^{}=0\), Eq. 1 reduces to the classical unregularized AMDP. If \(h^{}(s)=_{}[-]=:((s))\), i.e., the (differential) entropy, Eq. 1 defines the average reward of the so-called entropy-regularized MDPs.

In this work, we consider the _ergodic_ setting, for which we make the following assumption formally:

**Assumption 2.1**.: For any feasible policy \(\), the Markov chain induced by policy \(\) is ergodic. The Markov chain is Harris ergodic in general state and action spaces (see ). The stationary distribution \(^{}\) induced by any feasible policy exists and is unique. There is some constant number \(0<<1\) such that \(^{}(s) 1-\).

As a result of Assumption 2.1, for any feasible policy \(\), the average-reward function does not depend on the initial state (see Section 8 of ). Given that, one can view \(^{}\) as a function of \(\). For a given policy we also define the _basic differential value function_ (also called _bias function_; see, e.g., )

\[^{}(s)[_{t=0}^{}c(s_{t},a_{t })+h^{}(s_{t})-^{}|s_{0}=s,a_{t}(|s_{t}),s_{t+1} (|s_{t},a_{t})],\] (4)

and the _basic differential action-value function (or basic differential Q-function)_ is defined as

\[^{}(s,a):=[_{t=0}^{}c(s_{t},a_{t} )+h^{}(s_{t})-^{}|s_{0}=s,a_{0}=a,a_{t}(|s_{t}),s_ {t+1}(|s_{t},a_{t})].\] (5)

Moreover, we define the sets of _differential value functions_ and _differential action-value functions (differential \(Q\)-functions)_ as the solution sets of the following Bellman equations, respectively,

\[^{}(s) =[^{}(s,a)|a(|s)],\] (6) \[^{}(s,a) =c(s,a)+h^{(s)}(s)-^{}(s)+(ds^{}\ |\ s,a)^{}(s^{}).\] (7)

Under Assumption 2.1, the solution of Eq. (6) (resp., Eq. (7)) is unique up to an additive constant. Finally, our goal in solving an AMDP is to find an optimal policy \(^{*}\) that minimizes the average cost:

\[^{*}=^{^{*}}=_{}^{}(|s) ,\  s.\] (AMDP)

### Inverse Reinforcement Learning

Suppose there is a near-optimal policy, and we are given its demonstrations \(\{(s_{i},a_{i})\}_{i 1}\). IRL aims to recover a reward function such that the estimated reward best explains the demonstrations. We consider solving the IRL problem under the Maximum Entropy framework (MaxEnt-IRL), whichaims to find a reward representation that maximizes the entropy of the corresponding policy and incorporates feature matching as a constraint. Formally, the MaxEnt-IRL problem is described as

\[_{}()_{(s,a) d^{}( ,)}[-(a|s)]\] (MaxEnt-IRL) \[\ _{(s,a) d^{}}[(s,a) ]=_{(s,a) d^{E}}[(s,a)]\]

where \(d^{E}\) and \(d^{}\) denote the state-action distribution induced by the expert and current policy \(^{E}\) and \(\) respectively, and \((s,a)^{n}\) denotes the feature of a given \((s,a)\) pair. If we assume that for a given state-action pair, the cost is a linear function to its feature, i.e., \(c(s,a;)=^{T}(s,a)\) for some parameter \(\) with the same dimension of the feature \((s,a)\), we can show that the parameter \(\) is the dual multiplier of the above optimization problem, as the Lagrangian function can be written as

\[(,)=()+_{(s,a) d^{E}}[c (s,a;)]-_{(s,a) d^{}}[c(s,a;)].\] (8)

Therefore, the dual problem is formulated as

\[_{} L()_{(s,a) d^{E}}[c(s,a; )]-_{(s,a) d^{}}[c(s,a;)]\] (Dual IRL) \[ =_{^{}}_{(s,a) d^{^{}} }[-c(s,a;)]+(^{}).\]

Notice that to find \(\), we need to solve an Entropy Regularized Reinforcement Learning problem, i.e., solving AMDP with the regularizer set to be the negative entropy, \(h^{}=-()\). To this end, we first propose a Stochastic Policy Mirror Descent (SPMD) method for solving AMDP. The solution methods are presented in section 3. Then we introduce an Inverse Policy Mirror Descent (IPMD) algorithm based on SPMD for the inverse RL problem (Dual IRL), introduced in section 4. We will see that we only need to solve the subproblem AMDP partially.

## 3 Stochastic Policy Mirror Descent for AMDPs

This section proposes an SPMD method to solve Regularized Reinforcement Learning problems for AMDPs. SPMD operates in an actor-critic fashion. In each step, the agent evaluates its current policy (critic step) and performs a policy optimization step (actor step). In this work, we assume there is a way to perform the critic step using standard methods, e.g., Temporal Difference learning using neural networks. Further discussion on implementation is included in section 3.1. We will focus on designing a novel actor step and providing its complexity analysis.

Our policy optimization algorithm is motivated by the following performance difference lemma, which characterizes the difference in objective values of two policies \(,^{}\).

**Lemma 3.1**.: _(Performance Difference) Assume that Assumption 2.1 holds. For any \(,^{}\)_

\[^{^{}}-^{}=^{}(s,^{}(s)) ^{^{}}(ds), s,\] (9)

_where_

\[^{}(s,^{}(s))^{}(s,^{ }(s))-^{}(s)+h^{^{}}(s)-h^{}(s).\] (10)

Proof can be found in A.1. The above lemma shows the gradient of the objective function with respect to action \(a\) is \(^{}(s,(s))+h^{}(s)\), i.e.,

\[_{a}^{}=_{a}(^{}(s,(s))+h^{ }(s))^{}(ds).\] (11)

The existence of the above equation requires the differentiability of \(\), \(h\) and locally Lipschitz continuity of \(^{}\)[see 24; 25]. Note that \(\) can be seen as some generalized advantage function. Lemma 3.1 shows that the gradient of the objective relates to _differential Q-functions_. This inspires us to update the policy in the following mirror-descent style:

\[_{k+1}(s)=*{arg\,min}_{a}^{ _{k}}(s,a)+h^{a}(s)+}D(_{k}(s),a),\] (12)

where \(_{k}\) is some predefined step size. The complexity analysis of this type of algorithm has been thoroughly studied in  in tabular forms or linear function approximation settings, i.e., a lookuptable represents the policy, and the _differential Q-functions_ are approximated as linear functions with respect to some feature space. In practice, especially in the Deep Reinforcement Learning community, both policy and the \(Q\)-functions are represented by neural networks. Thus, novel analysis is required for general function approximation. Additionally, the exact value of \(^{}\) and \(()\) in Eq. 19 can only be estimated through approximation. Note that \(()\) rises inside the Bregman distance \(D(_{k}(s),a)\), referring to Eq. 13. We consider their stochastic estimators calculated from state-action sample pairs \(\), denoted as \(}^{,}(s,a;),((s);)\) respectively, where \(\) denotes the parameters of the method of choice, for example, weights and biases in a neural network. For the rest of the paper, we abbreviate \(}^{,}(s,a;)\) as \(}(s,a;)\) when the context is clear. We described the Stochastic Policy Mirror Descent (SPMD) algorithm in Algorithm 1.

```
1:Input: Initialize random policy \(_{0}\) and step size sequence \(\{_{k}\}\)
2:for\(k=0,1,,K\)do
3: Sample collection: \(_{k}=\{(s_{t},a_{t},c_{t})\}_{t 1}\)
4:Critic step: Approximate \(^{_{k}}\), \((_{k}(s))\) with \[^{_{k}}(s,a) }^{_{k},_{k}}(s,a;_{k}),\] (13) \[(_{k}(s)) (_{k}(s);_{k}).\] (14)
5:Actor step: Update the policy \[_{k+1}(s)=_{a}}^{_{k},_{k}}( s,a;_{k})+h^{a}(s)+}((_{k}(s); _{k}),a+(a)).\] (15)
6:endfor ```

**Algorithm 1**The Stochastic Policy Mirror Descent (SPMD) algorithm for AMDPs

The following paragraphs present the complexity analysis for Algorithm 1. Note that we assume the approximated \(Q\) function might not be convex but weakly convex, i.e.,

\[}(s,a;)+_{}}D(_{0}(s),a)\] (16)

is convex w.r.t. \(a\) for some \(_{}} 0\) and \(_{0}\) is the initialized policy. This assumption is general as any differential function with Lipschitz continuous gradients is weakly convex. Moreover, we assume that \(h^{i}(s)\), \(}(s,;)\) and \(^{}(s,)\) are Lipschitz continuous with respect to the action.

**Assumption 3.2**.: For all \(a_{1},a_{2}\) there exist some constants \(M_{h},M_{}},M_{}\) such that

\[|h^{a_{1}}(s)-h^{a_{2}}(s)|  M_{h}\|a_{1}-a_{2}\|,\] (17) \[|}(s,a_{1};)-}(s,a_{2};)|  M_{}}\|a_{1}-a_{2}\|,\] (18) \[|^{}(s,a_{1})-^{}(s,a_{2})|  M_{}\|a_{1}-a_{2}\|,\] (19)

and \(}(s,a;)\) is \(_{}}\)-weakly convex, i.e., Eq. 16 is convex.

Note that the strong convexity modulus of the objective function in Eq. 15 can be very large since \(_{k}\) can be small, in which case the subproblem Eq. 15 is strongly convex, thus the solution of Eq. 15 is unique due to strong convexity .

Our complexity analysis follows the style of convex optimization. We begin the analysis by decomposing the approximation error in the following way

\[_{k}^{Q}(s,a):=}(s,a;_{k})- ^{_{k}}(s,a),\] (20) \[_{k}^{}(s):=} (_{k}(s);_{k})-}(_{k}( s)),\] \[=[}( _{k}(s);_{k})]-}(_{k }(s))+}(_{k}(s);_{k} )-[}(_{k}( s);_{k})].\] (21)

We refer to the first two and last two approximation error terms in \(^{}\) as \(_{k}^{,det}\) and \(_{k}^{,sto}\), respectively, as they represent a deterministic approximation error which we cannot reduce and a stochastic error related to the variance of the estimator. They represent the noise introduced in the critic step. To make 

[MISSING_PAGE_FAIL:6]

**Assumption 3.6**.: When \(<0\), the critic step has bounded errors for any \(s\), i.e., there exist some constants \(^{},^{}\) such that

\[\|_{k}^{,det}(s)\|_{*}^{}\;\;\;\; \|_{k}^{,sto}(s)\|_{*}^{}.\] (29)

**Theorem 3.7**.: _Let the number of iterations \(K\) be fixed. Suppose that 2.1, 3.2, and 3.6 hold. Also assume that \(<0\) and that \(_{k}==\{||/2,1/\},k=0,,K-1\). Then for any \(s\), there exist iteration indices \(k(s)\) found by running \(K\) iterations of the SPMD method such that both the generalized advantage function and the distance between the iterates will converge to \(0\), i.e.,_

\[^{_{k(s)}}(s,_{k(s)+1}(s)) =(K^{-1})+(K^{-1/2})+ ^{}_{}/(1-)\] (30) \[D(_{k(s)},_{k(s)+1}(s))+(+) D(_{k(s)+1}(s),_{k(s)}(s))\] \[=(K^{-1})+(K^{-1/2})+^{ }_{}/(1-)\] (31)

**Remark** This is the most general case, as we don't assume any restrictions on the function approximation class. As expected, the complexity bound becomes worse compared to previous results. To reach a \(\)-precision stationary point, we require at least \((^{-2})\) iterations. Note that \( 0\) indicates that \(^{_{k}}(s,_{k+1}(s))\) has virtually no difference than \(^{_{k}}(s)\). This has a similar implication of \(D(_{k+1}(s),_{k}(s))\) approaching \(0\) which implies that we are reaching a stationary point.

### Practical algorithm for Entropy Regularized AMDP

The above SPMD algorithm computes \((s)\) for every state. In each iteration, we need to solve a subproblem Eq. 15 for every state (or for any state encountered). In practice, we prefer performing each SPMD iteration in a mini-batch style, i.e., approximately solving Eq. 15 from some trajectories collected. If the regularizer \(h\) and \(\) are the negative entropy, we can approximately solve the SPMD actor step Eq. 15 by

\[_{k+1}=*{arg\,min}_{}\,KL((s)\;\|\; )}(}^{_{k}}(s,a;_{k}))),\] (32)

where \(Z(_{k})\) is some normalization constant and \(}^{_{k}}(s,a;_{k})-}{1+_ {k}}}^{_{k}}(s,a;_{k})-}_ {k}(s)\). In practice, we apply the so-called reparameterization trick  to obtain an unbiased gradient estimator. Let the policy be parameterized by

\[(a|s)=(f_{}(;s)|s),(s,a),\] (33)

where \(\) is an input noise sampled from a fixed distribution; \(\) represents the parameters of the policy. Then we can approximately perform the actor step by solving the following problem:

\[_{}_{(s,a),}[((f_{}( ;s)|s))-}^{_{k}}(s,f_{}(;s))].\] (34)

For large action spaces, the reparameterization trick efficiently approximates the solution of Eq. 15. As for the critic step, we can approximate \(\) by minimizing the Temporal Difference (TD) error, i.e., solving the following problem:

\[_{}_{(s,a,c,s^{},a^{})}\!(c(s,a)+ h^{a}(s)-^{}+}^{,}(s^{},a^{ };)-}^{,}(s,a;))^{2},\] (35)

where \(^{}\) is the estimated average cost, e.g., by taking \(^{}= c(s,a)+h^{a}(s)\).

## 4 Inverse Policy Mirror Descent for IRL

Equipped with an efficient solver for AMDPs presented in section 3, we can now solve the IRL problem in the form of (Dual IRL). We suppose \(\) parameterizes the reward function with arbitrary methods, e.g., neural networks. To solve the dual problem, we update the parameter \(\) by performing a gradient descent step \(_{k+1}=_{k}-_{k}_{}L()\) where \(_{k}\) is some predefined step size. The gradient computation of the dual objective function is

\[ L()=_{(s,a) d^{}}[_{}c( s,a;)]-_{(s,a) d^{}}[_{ }c(s,a;)].\] (36)Without accessing the transition kernel \(\), the true gradient is unavailable. So we use a stochastic approximation instead, denoted as \(g_{k} g(_{k};_{k}^{E})-g(_{k};_{k}^{})\) where \(g(;)_{t=1}^{N} c(s_{t},a_{t};)\) is the stochastic estimator of the gradient of the average reward. We describe the proposed Inverse Policy Mirror Descent (IPMD) algorithm in Algorithm 2. In this section, we provide our analysis that captures the algorithm behaviors across iterations since, in each iteration, \(}\) is only an approximation of the true differential \(Q\)-function, which itself alters due to the change of the reward estimation. The idea of the proof is based on the Lipschitz continuity of the iterates, as it controls the difference of these functions across iterations. We make the following formal assumptions for such a purpose.

**Assumption 4.1**.: For any \(s,a\), the gradient of the reward function is bounded and Lipschitz continuous, i.e., there exist some constant real numbers \(L_{r},L_{g}\) so that the following holds:

\[\| c(s,a;)\|_{2} L_{r},\| c(s,a;_{1})- c(s, a;_{2})\|_{2} L_{g}\|_{1}-_{2}\|_{2}.\]

Note that \(\) is also a function of \(\). As shown in section 3, \(}\) can be parameterized but we only denote the estimator as \(}^{}(s,a;_{k})\) since its parameters do not contribute to the analysis.

**Assumption 4.2**.: Suppose that at least one of \(,\) is continuous, we assume that

\[_{}\|_{}}^{}(s,a;)\|_{2} L _{q},\] (40)

where \(L_{q}\) is some positive constant. For convenience we denote \(}^{}(s,a;_{k})\) also as \(}^{}_{_{k}}(s,a)\).

We further assume that the estimation error from using \(}^{}_{}\) is bounded.

**Assumption 4.3**.: For any \(s,a\), there exist some constants \(_{k},,_{k},\) such that the following inequalities hold:

\[\|_{_{k}}[}^{_{k},_{k}}_{_{ k}}]-}^{_{k}}_{_{k}}\|_{sp,}_{k} ,_{_{k}}[\|}^{_ {k},_{k}}_{_{k}}-^{_{k}}_{_{k}}\|_{sp, }^{2}]_{k}^{2}^{2}.\] (41)

Notice that this error bound subsumes both the estimation errors and approximation errors. In general, the Bellman operator is nonexpansive in the average-reward setting, so analysis based on the contraction property of the Bellman operator with the infinity norm, as what is used in , fails in this case. To this end, we assume that the operator is span contractive, i.e.,

**Assumption 4.4**.: In the critic step, there's a way to construct a span contractive Bellman operator \(\) such that there exists \(0<<1\) for any _differential \(Q\)-functions_\(_{1},_{2}\),

\[\|_{1}-_{2}\|_{sp,}\| {Q}_{1}-_{2}\|_{sp,}.\] (42)

In A.6, we provide conditions when the Bellman operator has a J-step span contraction. Now we can discuss the convergence results in the following theorems. The convergence of the _differential Q-function_ is characterized in the following result.

**Theorem 4.5**.: _Suppose that assumptions 2.1, 4.1-4.4 hold. If \(=}{}\), the differential Q-function will converge to the optimal solution for a given reward function parameterized by \(_{k}\), i.e.,_

\[_{k=0}^{K-1}\|_{_{k}}[ _{_{k}}^{_{k},_{k}}]-_{_{k}}^{_{_{k}}} \|_{sp,}=(K^{-1})+(K^{-1/2})+,\] (43)

_where \(_{0}>0\) is some step size chosen, and \(\) is some constant defined in Assumption 4.4._

Proof can be found in A.7. The above theorem shows that the _differential Q-function_ will approach the optimal _differential Q-function_\(_{_{k}}^{_{_{k}}}\) with respect to the current reward estimation \(_{k}\). If the reward estimation is accurate, so is the _differential Q-function_. Finally, in the next theorem, we show that the policy converges to the optimal policy for a given reward function, and the reward function approximation converges to a stationary point. Proof can be found in A.8.

**Theorem 4.6**.: _Suppose that assumptions 2.1, 4.1-4.4 hold. If \(=}{}\) and run the proposed IPMD algorithm \(K\) iterations, the algorithm will produce near stationary solutions for the reward function and the optimal policy for such reward function. Specifically, the following holds,_

\[_{k=0}^{K-1}\|_{_{k}}[ _{k+1}]-_{_{k}}\|_{}=(K^{-1})+ (K^{-1/2})+,\] (44) \[[\| L(_{k})\|_{2}^{2 }]=(K^{-1})+(K^{-1/2})+2 L_{c}L_{r }C_{d}|||},\] (45)

_where \(_{0},,L_{c},L_{r},C_{d}\) are some constants and \(||,||\) some measure of the state and action space._

**Remark** First, the above theorem shows that the policy will converge to the optimal policy given the reward parameterization, although each iteration's reward parameter differs. Second, the reward parameter \(\) will converge to a stationary point of (Dual IRL), as the objective can be highly nonconvex under general function approximation. Third, for general AMDPs without \(1\)-step span contraction, we can use a \(J\)-step Bellman operator (see Appendix A.7) for policy evaluation to maintain a \((^{-2})\) complexity for the entire algorithm.

## 5 Numerical experiments

In this section, we showcase the performance of the proposed SPMD and IPMD algorithms. Our code can be found at https://anonymous.4open.science/r/IPMD-9D60. See more detail about all our implementation in the Appendix A.9.

**MuJoCo Robotics Manipulation Tasks for RL** This experiment tests our RL agent's performance on robotics manipulation tasks. Our SPMD algorithm is based on the stable-baselines3 . We compare the performance of our algorithm with Soft Acrot-Critic (SAC)  implemented in . The policy network employs two fully connected hidden layers of dimension 256 each, taking actions as input and outputting a distribution. Both the \(Q\) network and reward function share the same architecture, with ReLU activation used in hidden layers. A double \(Q\)-learning technique is used to minimize overestimation . During training, we found that setting the entropy coefficient term to 0.01 makes training stable and efficient. The learning rate is \(3e^{-4}\). Each step of the algorithm samples 512 state-action sample pairs. Table 1 reports the numerical results of each model. Our proposed SPMD achieves on-par performance with SAC and exceptionally better performance in the Humanoid environment.

**MuJoCo benchmark for IRL** In this experiment, we compare the proposed IPMD method with IQ-Learn  and \(f\)-IRL . The authors of ML-IRL have not released its implementation at the time we experimented. Nevertheless, the performance of ML-IRL is comparable to \(f\)-IRL. It is observed in the literature that imitation learning algorithms have inferior performance . Hence we omit these methods. Table 2 reports the numerical results of each model. Note that we cannot record a competitive result for IQ-Learn with Humanoid as claimed in the original paper, which we highlight using \({}^{*}\). The result shows that IPMD outperforms in a variety of environments. One possible reason Ant is falling behind is that Ant has more ground contact since it has more legs. This will impact the mixing time of the MDP and our assumption on the \(1\)-step contraction. Compared to Half-Cheetah, or Humanoid, it is harder for the Ant to transition from an arbitrary state to another arbitrary state as it involves multiple legs working together. One remedy is that we construct a \(J\)-step contractive operator for policy evaluation, as done in . The success of humanoid is possibly due to a slightly different policy evaluation scheme compared to the discounted setting, where the entropyterm from the policy no longer plays a part, as the term \(c-\) cancels out the additional regularization and entropy of the policy. We suspect this brings more stable training and thus higher performance.

**Reward Recovery** Finally, we compare the proposed IPMD method against IQ-Learn on recovering the expert's reward function in two environments: Pendulum and LunarLanderContinuous, both implemented in . The result of the experiment is shown in Figure 1. Using state-action pairs from 4 different episodes, we compare the span seminorm of the predicted reward and the true reward (the reward the expert uses). The experts are trained with a discount factor \(0.99\). IQ-Learn's discount factor is set to \(0.99\). The result shows IPMD's superiority in reward recovery.

## 6 Discussion

In this paper, we formulate the Maximum Entropy IRL problem with the average-reward criterion and propose efficient algorithms to solve it. We devise the SPMD method for solving RL problems in general state and action spaces beyond the scope of linear function approximation with strong theoretical guarantees. We integrate this method to develop the IPMD method for solving the entire IRL problem and provide convergence analysis. To reach a \(\) precision stationary point, IPMD requires \((^{-2})\) policy optimization steps.

We also notice there are possible improvements from this work. For example, we impose various assumptions both the function approximation classes and strong reliance on the accuracy of the critic step. It is natural to seek more robust methods when facing inaccurate estimations. Additionally, we impose continuity assumption in various Assumptions (e.g. Assump 4.2). Such assumptions can be violated if neural networks are used as function approximations. In practice, it is possible to alleviate the issue by "clipping" the gradient norm when performing gradient descent, which proves effective in related works . Furthermore, we only consider uniform ergodic chains. It is interesting to see how our method would hold under the unichain or multi-chain setting. Lastly, the analysis based on span contraction might be improved for general MDPs. This requires nontrivial work and thus we leave it for future development.