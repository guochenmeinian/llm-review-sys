# Bridge the Points: Graph-based Few-shot

Segment Anything Semantically

 Anqi Zhang1, Guangyu Gao1, Jianbo Jiao2, Chi Harold Liu1, and Yunchao Wei3

1School of Computer Science, Beijing Institute of Technology
2The MIx group, School of Computer Science, University of Birmingham
3WEI Lab, Institute of Information Science, Beijing Jiaotong University

andy_zaq@outlook.com

###### Abstract

The recent advancements in large-scale pre-training techniques have significantly enhanced the capabilities of vision foundation models, notably the Segment Anything Model (SAM), which can generate precise masks based on point and box prompts. Recent studies extend SAM to Few-shot Semantic Segmentation (FSS), focusing on prompt generation for SAM-based automatic semantic segmentation. However, these methods struggle with selecting suitable prompts, require specific hyperparameter settings for different scenarios, and experience prolonged one-shot inference time due to the overuse of SAM, resulting in low efficiency and limited automation ability. To address these issues, we propose a simple yet effective approach based on graph analysis. In particular, a Positive-Negative Alignment module dynamically selects the point prompts for generating masks, especially uncovering the potential of the background context as the negative reference. Another subsequent Point-Mask Clustering module aligns the granularity of masks and selected points as a directed graph, based on mask coverage over points. These points are then aggregated by decomposing the weakly connected components of the directed graph in an efficient manner, constructing distinct natural clusters. Finally, the positive and overshooting gating, benefiting from graph-based granularity alignment, aggregate high-confident masks and filter out the false-positive masks for final prediction, without relying on additional hyperparameters and redundant mask generation. Extensive experimental analysis across tasks including the standard FSS, One-shot Part Segmentation, and Cross Domain FSS validate the effectiveness and efficiency of the proposed approach, surpassing state-of-the-art generalist models with a mIoU of 58.7% on COCO-201 and 35.2% on LVIS-922. The project page of this work is: https://andyzaq.github.io/GF-SAM/.

## 1 Introduction

Previous semantic segmentation methods [1; 2; 3; 4; 5; 6; 7; 8], which rely on the pixel-level classification, often struggle with generalization and overfitting due to limited labeled data. In addition, recent approaches, such as MaskFormer , have shifted the paradigm to mask-based classification, offering a more flexible approach to improving the segmentation performance by exploiting the consistency and completeness of generated class-agnostic masks. The Segment Anything Model (SAM)  further marks a significant advancement by utilizing extensive pre-training on huge-scale dataset SA-1B to achieve more robust, class-agnostic segmentation capabilities. SAM excels in producing precise masks across various domains using simple prompts such as points, boxes, and coarse masks. While the boundaries of these masks can closely align with object boundaries, the lack of semanticunderstanding and the requirement for manual prompts prevent SAM from being used in automatic semantic segmentation applications.

Recent studies have attempted to automate this process in the Few-shot Semantic Segmentation (FSS), using a few reference images and a fine-grained external backbone network (_e.g._, DINOv2 ) to guide SAM in segmenting target semantic objects. However, these methods face two main challenges: achieving suitable points for precise and full coverage of the target object, and handling the ambiguity of SAM-generated masks, from partial to complete coverage. Specifically, they either utilize the most similar candidate point prompts for iterative mask generation and refinement , or build a restrictively selected set of point prompts for heuristically weighted mask merging based on manually designed metrics , outperforming both previous specialist methods  and generalist methods without SAM . However, these methods overlooked the underlying relationships between points (derived from fine-grained features) and masks (generated by SAM in a coarse-grained manner). This oversight led to low efficiency (as indicated in Fig. 1(a)) and limited automation capabilities. Alignment between these two types of granularity could uncover the potential of simple decision-making on masks, which can eliminate redundant refinement and manual hyperparameter selection for complicated metrics.

In this paper, we explicitly explore the relationship between point prompts and corresponding masks from SAM, and present a simple yet effective parameter-free framework with only one-time mask generation to segment anything semantically, in a graph-based few-shot manner. We first introduce a Positive-Negative Alignment (PNA) module to dynamically select point prompts using foreground and background references from reference images. Unlike existing methods, our approach combines different granularity by constructing a directed graph according to mask coverage over points. Then, we perform connectivity analysis on the constructed graph to obtain several weakly connected components as automatic clustering of point prompts, which bridges points and masks as well as fine-grained and coarse-grained features. To mitigate the inevitable introduction of false positives in the PNA module, we further leverage weakly connected component clusters and limited semantic information in selected points, to more accurately filter and merge masks that mismatch in different granularities. In particular, our proposed method involves two post-gating based on weakly connected clusters: the positive gating retains masks capturing a greater proportion of potential target areas, while the overshooting gating screens out outlier points near object boundary, with coverage self-consistency consideration.

Extensive experimental analysis on Few-shot Semantic Segmentation demonstrates both the efficiency and effectiveness of our approach, as shown in Fig. 1(b). We first conduct the experiments on generalized FSS datasets, including Pascal-5' , COCO-20' , FSS-1000  and LVIS-92' . Our approach surpasses existing state-of-the-art approaches on these datasets, with \(5.8\%\) and \(2.2\%\)

Figure 1: Performance comparisons of our approach against previous state-of-the-art methods regarding efficiency and generalized capabilities in Few-shot Semantic Segmentation. Figure 1(a) illustrates our approachâ€™s superior performance in efficiency and effectiveness across various model sizes. Figure 1(b) demonstrates the generalizability of our approach across different domains.

of improvement respectively on more challenging COCO-20\({}^{}\) and LVIS-92\({}^{}\). As for the challenging One-shot Part Segmentation, our approach still exceeds previous methods with \(1.6\%\) of mIoU on both PACO-Part and PASCAL-Part. Furthermore, to demonstrate the ability of our approach across different domains, we perform an evaluation on several specific datasets, including Deepglobe , ISIC , and iSAID-5\({}^{}\). The proposed approach establishes new state-of-the-art performance on mIoU with \(49.5\%\) on Deepglobe, \(48.7\%\) on ISIC, and \(47.3\%\) on iSAID-5\({}^{}\).

Overall, our contributions are summarized as follows:

* We present, to our knowledge, the first graph-based approach for SAM-based few-shot semantic segmentation, modeling the relationship of SAM-generated masks in an automatic clustering manner.
* We propose a positive-negative alignment module and a post-gating strategy based on the weakly connected graph components, enabling a hyperparameter-free pipeline.
* Extensive experimental comparisons and analysis across several datasets over various settings show the effectiveness and efficiency of the proposed method.

## 2 Related Work

**Few-shot semantic segmentation.** Few-shot Semantic Segmentation (FSS)  aims to segment the target object using only a limited number of annotated reference samples for guidance. Previous FSS methods are mainly categorized into two types, namely the methods based on prototype matching [28; 29; 30; 31; 32; 33; 34] and methods based on pixel-wise matching [35; 36; 37; 38; 39; 40]. The methods based on prototype matching, _e.g._ PFENet , BAM , SSP , use the Mask Average Pooling operation from SGOne  to generate a prototype as a global representation of the reference features, and compare the target features with the prototypes. The methods based on pixel-wise matching compute the correlation of all pixels between target and reference features. Then different methods address the correlations through distinct mechanisms, such as 4D Convolution (_e.g._, HSNet ) and Transformer (_e.g._, HDMNet , AMFormer ). Although these specialist models perform significantly on specific tasks, they are prone to overfitting the training samples and often struggle to adapt to domain shifts.

**SAM-based semantic segmentation.** Recently, Segment Anything Model (SAM)  has shown remarkable zero-shot class-agnostic segmentation capabilities using prompts like points, boxes, and coarse masks. However, the coarse-grained feature representation of SAM limits its effectiveness for fine-grained semantic segmentation tasks. Several approaches have been proposed to extend SAM for semantic segmentation. For example, Semantic-SAM  jointly train the model on SA-1B and other semantic aware segmentation datasets to enhance granularity. OV-SAM  combines SAM and CLIP  for open-vocabulary semantic segmentation. Moreover, some methods introduce SAM into FSS tasks. PerSAM and PerSAM-F  leverage SAM for personalized segmentation with one-shot guidance. Matcher  uses a SAM-based training-free structure, achieving impressive performance in both FSS and One-shot Part Segmentation. VRP-SAM  trains an external Visual Reference Prompt Encoder to automatically generate prompts from reference images using points, scribble, box, or masks. However, previous training-free methods struggled to balance performance and efficiency, often relying on excessive external manual hyperparameters.

## 3 Preliminaries

Few-shot Semantic Segmentation (FSS) aims to segment target objects in an image with a few annotated reference images. Consider a scenario where each group of samples contains a target image \(x^{t}\) and a reference image set \(R=\{x_{k}^{r},y_{k}^{r}\}_{k=1}^{K}\) with the size of \(H W\), where \(x_{k}^{r}\) and \(y_{k}^{r}\) mean the \(k_{th}\) reference image and its corresponding mask. Focusing on the 1-shot case, where \(K=1\), it begins with a feature extraction backbone network \(f_{B}()\), which encodes both \(x^{t}\) and \(x^{r}\) into semantic features \(F^{t}\) and \(F^{r}\) in \(^{hw c}\), where \(h\) and \(w\) denote the height and width of the feature maps, and \(c\) is the feature dimension. Subsequent few-shot processes utilize these feature maps to generate a predicted segmentation \(^{H W}\) for \(x^{t}\). This prediction is then compared to the Ground Truth (GT) \(y^{t}\) for evaluation.

The Segment Anything Model (SAM) is a generalized foundation segmentation model adept at generating precise masks based on varied prompts of points, boxes, and coarse masks. Built around a core architecture that includes an image encoder, a prompt encoder, and a mask decoder, SAM effectively processes input images \(x^{t}\) and prompts \(P\) to produce detailed segmentation masks \(\). These masks accurately delineate specific objects or regions within the images, based on the guidance provided by the prompts.

## 4 Method

Diverging from traditional methods, we use a directed graph to exploit the natural relationships between points and their corresponding masks, representing fine-grained and coarse-grained features, respectively. As shown in Fig. 2, our approach mainly comprises the Positive-Negative Alignment (PNA) module, Point-Mask Clustering (PMC) module, and Post-Gating strategy. The PNA module leverages semantic features from the backbone network to sort pixel-wise correlations into similarity maps, enabling precise point selection. The PMC module then clusters masks based on these selected points, while Post-Gating strategy refines the selection, enhancing the accuracy and reliability of the final prediction.

### Positive-Negative Alignment for Point Selection

The PNA module efficiently selects point prompts to balance the number of points and coverage of target objects. Using the semantic features \(F^{r}\) and \(F^{t}\) from the reference and target images respectively (with _e.g._, DINOv2 ), we get the pixel-wise correlation matrix \(C^{hw hw}\):

\[C(i,j)=ReLU((i) F^{r}(j)}{\|F^{t}(i)\|\|F^{r}(j)\|} ),\] (1)

where \(C(i,j)\) represents the similarity between the \(i\)-th pixel of target features \(F^{t}(i)\) and the \(j\)-th pixel of reference features \(F^{r}(j)\).

To minimize hyperparameter reliance, we leverage background features typically overlooked in FSS, indicated by the negative mask \(y^{}= y^{r}\) of the reference image. According to \(y^{r}\) and \(y^{}\), we divide \(C\) into \(C^{+}\) and \(C^{-}\) in \(^{hw hw}\) for foreground and background features, respectively. We then introduce two positive similarity maps in mean and max aspects respectively:

\[S^{+}_{mean}(i)=^{hw}C^{+}(i,j)}{_{j=1}^{hw}(y ^{r})_{j}}, S^{+}_{max}(i)=max(C^{+}(i)),\] (2)

Figure 2: Overview of our approach, where the Positive-Negative Alignment module recognizes the correlation between target features and reference features for point selection, the Point-Mask Clustering module efficiently clusters the points based on the coverage of corresponding masks, and Post-Gating filters out the false-positive masks for generating final prediction.

where \(\) resizes \(y^{r}\) to the same resolution as \(F^{r}\) and then flatten it into a vector, \(max()\) finds the maximum value in the \(i\)-th row of \(C^{+}\). The mean positive similarity map \(S^{+}_{mean}^{hw}\) captures global similarity towards the reference object but may blur distinct internal features, reducing accuracy for complex objects. In contrast, the max positive similarity map \(S^{+}_{max}^{hw}\) focuses on the most similar regions, enhancing recall but also increasing noise. To maintain distinctiveness while reducing noise, we introduce the mixture similarity map \(S^{+}_{mix}=S^{+}_{mean} S^{+}_{max}\) using the Hadamard product. This method boosts target region distinctiveness by merging the strengths of both maps, while diminishing noise through the more stable global similarity.

To select prompt points, we also use the mean negative similarity map \(S^{-}_{mean}\), which reflects background similarity, noting that similar objects typically share higher background similarity values. We then align \(S^{+}_{mix}\) and \(S^{-}_{mean}\) by min-max normalization \(\) to get:

\[S_{p}(i)=(S^{+}_{mix})(i)_{\{(S^{+}_{mix })(i)>(S^{-}_{mean})(i)\}},\] (3)

where \(S_{p}^{hw}\) is the filtered map for point selection, and \(_{\{\}}\) is 1 if the condition is true and 0 otherwise. Although we minimize false negatives, noise points remain. To select suitable points from \(S_{p}\) without hyperparameters, we define the sum of elements in \(S_{p}\) as the number \(N\) of points to be selected. We then pick the \(N\) highest-value points from \(S_{p}\) as the point prompt set \(=\{P_{l}\}_{l=1}^{N}\).

### Point-Mask Clustering with Graph Connectivity

We utilize point prompts from \(\) to generate masks with SAM. Each point \(P_{l}\) in \(\) corresponds to a unique mask \(_{l}^{H W}\). As our point selection strategy prioritizes the coverage of objects, false-negative masks are unavoidable. Moreover, mask coverage can vary significantly within the same region, ranging from partial to full object coverage. This necessitates understanding the internal relationships among coarse-grained masks and points from fine-grained feature comparison to ensure those covering the same target are accurately gathered.

To address this, we design the Point-Mask Clustering (PMC) module, which clusters points and their corresponding masks based on mask coverage over points. Following the principles of efficiency and automation, the PMC module is based on a directed graph \(G=(V,E)\) with the vertex \(v_{l}\) in \(V\) representing point \(P_{l}\) and its corresponding mask \(_{l}\). Edges in \(E\) are established based on mask coverage over other points; an edge \(e_{l,m}\) exists if mask \(_{l}\) covers points \(P_{m}\) (with \(m l\)). Specifically, we do not establish edges for masks covering their corresponding points to avoid creating loops.

The graph \(G\) is a directed simple graph, allowing us to cluster vertices by identifying weakly connected components. This clustering process is hyperparameter-free, ensuring that every pair of vertices \(u,v V\) within the same component has a directed path between them. Each weakly connected component encompasses a set of points \(_{p}\) (with \(P=\{_{p}\}\)) that are all covered by the union of their masks in \(_{p}\), where \(p\) indexes the clusters.

The advanced SAM plays a crucial role in maintaining the precision of the generated masks. The precision of high-quality masks typically ensures non-overlapping between masks and prompting points of adjacent regions, especially those of different categories. This is the precondition for the efficacy of our PMC module, as even slight errors could significantly impact the clustering accuracy.

### Post-Gating on Weakly Connected Components

Our PNA module, while efficient in selecting points, inadvertently includes false positives, as detailed in Sec. 4.1. To mitigate this, we further develop two gating strategies targeting distinct types of false positives based on clusters formed from weakly connected components.

Positive gating.Despite the method in Sec. 4.1 diminishing the noise points outside the target region, there are still a few remaining noise points. These issues may have minimal impact on traditional segmentation methods, but under the SAM framework, masks derived from these noise points can significantly degrade accuracy. Moreover, some clusters of masks may extend beyond their intended target regions due to inaccuracies in SAM-generated masks or because the targeted object is part of a larger entity. Thus, we propose a Positive Gating strategy to address these issues.

This strategy prioritizes mask effectiveness by assessing whether a mask contains more positive than negative pixels, thereby facilitating a specialized designed mask growth for final prediction. The focus of mask growth is to enhance coverage of the target area rather than multiple objects, while minimizing background inclusion. Firstly, this method utilizes a parameter-free gating mechanism that discriminates between pixel polarities, based on the positive and negative similarity maps, \(S_{mean}^{+}\) and \(S_{mean}^{-}\), as described in Sec. 4.1. To achieve this, we utilize \(S_{mean}^{+}\) and \(S_{mean}^{-}\), along with the median of \(S_{mean}^{+}\) (_i.e._, the midpoint between the maximum and minimum values of \(S_{mean}^{+}\)), to constructs the polarity map \(\) as follows:

\[(i)=1,&S_{mean}^{+}(i) S_{mean}^{+}(i)>s_{mid} S _{mean}^{-}(i),\\ -1,&else.\] (4)

Then, using the polarity map \(\), we calculate the number of positive pixels of the \(l^{th}\) mask as follows:

\[s_{l}^{+}=_{i=1}^{hw}(i)(_{l})(i),\] (5)

where \(\) resizes and flattens \(_{l}\) to the feature map dimensions. Subsequently, for each cluster \(_{p}\) of weakly connected components, we sort the masks according to the ratio of positive pixel numbers to their areas. The indices of these sorted masks are denoted by \(Q\). We then initialize a blank pseudo mask \(_{p}^{H W}\) and a set of positive points \(P^{+}\). Following this, we apply a Mask Growth algorithm as outlined in Sec. A.1 and Alg. 1 for maintaining positive masks. This algorithm iteratively evaluates whether the region of \(_{q}\) outside the pseudo mask \(_{p}\) is positive, updates \(_{p}\) with the identified positive mask, and adds its corresponding point into \(P^{+}\).

Overshooting gating.The fine-grained semantic features from \(f()\) are reliable for locating target objects, yet the point coverage of the target areas varies, leading to both under-coverage and over-coverage. SAM effectively addresses under-coverage; however, over-coverage, which extends beyond target boundaries, often produces false-positive masks. These overshooting points, while semantically similar to the target areas in \(F^{4}\), typically derive masks that cover areas outside the target, resulting in a mismatch of representations between the granularity of points and masks. Thus, these points cannot be clustered with points inside the target areas.

Hence, we devise an overshooting gating strategy with consideration of self-consistency to eliminate overshooting points and their associated masks. As shown in Fig. 3, We assess the similarity between the features of each point \(P_{l}\) and the union mask \(_{p}^{H W}\) from each mask cluster \(_{p}\). The similarity computation for estimating self-consistency is performed as follows:

\[s^{sc}(l,p)=^{hw}Sim(F^{t}(P_{l}),(F^{t}(_{p}))(i))}{_{i=1}^{hw}(_{p}) dist(l,p)},\] (6)

where \(Sim(,)\) refers to the correlation calculation mentioned in Eq. 1. We introduce an external function \(dist(,)\) to measure the distance in \(F^{t}\) between each point \(P_{l}\) and the nearest selected point in \(_{p}\). This measure helps confine comparison to neighboring clusters, minimizing interference from other instances. We then identify the cluster most similar to the points and retain those in the point set \(P^{sc}\) that are more similar to their respective clusters.

Figure 3: Illustration of the Overshooting Gating strategy. The outer ring of points in the second image indicates the most similar cluster of corresponding points, _i.e._, points with different outside and inside colors do not satisfy the self-consistency.

Mask Merging.Finally, we obtain two distinct sets of points, namely \(P^{+}\) and \(P^{sc}\). We then union the masks corresponding to points that are common to both \(P^{+}\) and \(P^{sc}\). The merged masks form the final prediction, denoted as \(\).

## 5 Experimental Results

### Datasets

To illustrate the Few-shot Semantic Segmentation ability and generalization capacity, we conduct three types of sub-tasks, _i.e._ standard Few-shot Semantic Segmentation, One-shot Part Segmentation, and Cross Domain Few-shot Semantic Segmentation. The datasets for these tasks are as follows:

**Pascal-5\({}^{i}\)**, **COCO-20\({}^{i}\)**, **FSS-1000**, and **LVIS-92\({}^{i}\)** are standard FSS datasets. Pascal-5\({}^{i}\) is based on the Pascal VOC 2012  and SDS . The 20 classes are separated into 4 folds of 5 classes. COCO-20\({}^{i}\) is an 80-class dataset from MSCOCO , which has 4 folds with each fold containing 20 classes. FSS-1000  contains 1000 classes. The training, validation, and testing folds contain 520, 240, and 240 classes, respectively. LVIS-92\({}^{i}\) is more challenging for evaluating generalist models, which select 920 classes with more than 2 images and divide these classes into 10 folds.

**PASCAL-Part** and **PACO-Part** are One-shot Part Segmentation datasets. PASCAL-Part  contains 56 different object parts in 4 superclasses. PACO-Part is built based on the PACO dataset , which has 456 object part classes. The 303 classes with at least 2 samples in PACO-Part are divided into four folds following Matcher .

**Deepglobe**, **ISIC2018**, and **iSAID-5\({}^{i}\)** are Cross Domain FSS datasets. The Deepglobe  contains satellite images of geographic categories including urban, agriculture, rangeland, forest, water, and

    & ^{i}\)} & ^{i}\)} &  & ^{i}\)} \\  & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\   & & & & & & & \\ HSNet \({}_{}\) & \(66.2\) & \(70.4\) & \(41.2\) & \(49.5\) & \(86.5\) & \(88.5\) & 17.4 & 22.9 \\ VAT \({}_{}\) & \(67.9\) & \(72.0\) & \(41.3\) & \(47.9\) & \(90.3\) & \(90.8\) & 18.5 & 22.7 \\ HDMNet \({}_{}\) & \(69.4\) & \(71.8\) & \(50.0\) & \(56.0\) & - & - & - & - \\ AMFormer \({}_{}\) & \(70.7\) & \(73.6\) & \(51.0\) & \(57.3\) & - & - & - & - \\   & & & & & & & \\ PerSAM \({}_{}\) & 43.1 & - & 23.0 & - & 71.2 & - & 11.5 & - \\ PerSAM-F \({}_{}\) & 48.5 & - & 23.5 & - & 75.6 & - & 12.3 & - \\ Matcher \({}_{}\) & 68.1 & 74.0 & 52.7 & 60.7 & 87.0 & **89.6** & 33.0 & 40.0 \\ VRP-SAM \({}_{}\) & 71.9 & - & 53.9 & - & - & - & - & - \\ Ours & **72.1** & **82.6** & **58.7** & **66.8** & **88.0** & 88.9 & **35.2** & **44.2** \\   

Table 1: Performance on Few-shot Semantic Segmentation datasets of Pascal-5\({}^{i}\), COCO-20\({}^{i}\), FSS-1000, and LVIS-92\({}^{i}\). Gray means the in-domain trained results. The best results are shown in **bold**.

    &  &  \\  & PASCAL-Part & PACO-Part &  &  & ^{i}\)} \\  & 1-shot & 1-shot & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\   & & & & & & & \\ HSNet \({}_{}\) & 32.4 & 22.6 & 29.7 & 35.1 & 31.2 & 35.1 & 34.1 & 40.4 \\ DRA \({}_{}\) & - & - & 41.3 & 50.1 & 40.8 & 48.9 & - & - \\ FRINet \({}_{}\) & - & - & - & - & - & - & 42.6 & 44.5 \\   & & & & & & & \\ PerSAM \({}_{}\) & 32.5 & 22.5 & 31.4 & - & 23.9 & - & 19.2 & - \\ PerSAM-F \({}_{}\) & 32.9 & 22.7 & 35.0 & - & 23.6 & - & 20.3 & - \\ Matcher \({}_{}\) & 42.9 & 34.7 & 48.1 & 50.9 & 38.6 & 35.0 & 33.3 & 34.3 \\ Ours & **44.5** & **36.3** & **49.5** & **57.7** & **48.7** & **55.2** & **47.1** & **52.4** \\   

Table 2: Performance on One-shot Part Segmentation datasets and Cross Domain Few-shot Semantic Segmentation datasets. The best results are shown in **bold**.

[MISSING_PAGE_FAIL:8]

### Ablation Study

**Point selection.** We evaluate the impact of various similarity maps and the parameter-free selection of top N points on performance, as detailed in Sec. 4.1. As shown in Tab. 3, using either \(S^{+}_{mean}\) or \(S^{+}_{max}\) alone leads to a performance drop of up to \(5.6\%\) compared to using both. This decline is due to the inherent limitations of \(S^{+}_{mean}\) and \(S^{+}_{max}\) discussed in Sec 4.1. Additionally, the evaluation confirms that picking the top-N points based on similarity, which is parameter-free and requires no additional settings, simplifies the process and increases accuracy by \(2.1\%\).

**Clustering method.** We compare our PMC module using weakly connected components with the PMC module using strong connected components, which provides finer clustering results. According to our experiment results in Tab. 4, the clusters from weakly connected components provide better performance on COCO-20\({}^{i}\) and LVIS-92\({}^{i}\) for both gating, as these clusters of masks have ideal coverage of the objects. Simply filtering the masks without clustering-based gating can only achieve 44.0% mIoU on COCO-20\({}^{i}\) and 24.2% mIoU on LVIS-92\({}^{i}\), which is significantly lower than the performance achieved with clustering-based gating. Furthermore, our dynamic hyperparameter-free clustering method outperforms the k-means++ with 1.2% on both datasets. Note that k of k-means++ is set to 10 following Matcher .

**Positive gating.** Our approach compares the number of positive points and negative points in \(^{+}\) (Num) to judge whether the mask is positive. We conduct experiments for the strategy of comparing the sum of positive and negative values (Sum). The results in Tab. 5 demonstrate the Num strategy yields better performance, as comparing the number of pixels mitigates the influence of a few excessively high similarity values. Furthermore, the utilization of the Mask Growth algorithm improves both FSS and Part Segmentation performance by carefully retaining the positive regions. However, it weakens the improvement of Num due to their similar effects.

**Overshooting gating.** Our Overshooting gating aims to filter out the overshooting points closely neighboring to target regions, thus having a less remarkable improvement of 1.6% compared to Positive Gating, as shown in Tab. 6. This performance still surpasses the mean similarity of com

Figure 4: Qualitative analysis of Matcher, Baseline, B+PG, B+PG+OG. B, PG, and OG respectively represent Baseline, Positive Gating, and Overshooting Gating. Masks in ref. image are shown in blue.

paring points with regions of clustered points (Point Sim.) or the prototypes from Masked Average Pooling  with union masks (MAP Sim.). More importantly, the distance function avoids the gating from 9.6% of performance decline. It ensures each cluster only affects its neighboring points.

### Qualitative Analysis

Here we present the qualitative results of Matcher, Baseline (\(1^{}\) row in Tab. 4), Baseline+PG (\(2^{}\) row in Tab. 4) and our approach in Fig. 4. The bipartite matching of Matcher has a negative influence when the areas of the target object in reference and target images have significant differences, as shown in the \(1^{}\) and \(3^{}\) rows. The positive gating with clustering filters out the noise masks in the \(3^{}\) row, while the Overshooting Gating further removes the masks belonging to overshooting points in the \(2^{}\) and \(4^{}\) rows. More qualitative analyses please refer to the appendix.

## 6 Conclusion

In this paper, we proposed an efficient, training-free SAM-based FSS approach that requires no external hyperparameters. As an automatic SAM-based semantic segmentation pipeline, our approach balanced candidate points and object coverage in the Positive-Negative Alignment (PNA) module, then used SAM-generated masks in the Point-Mask Clustering (PMC) module to enhance Post Gating. Extensive experiments validated the superior performance of our approach, advancing semantic segmentation without extensive parameter tuning or training.