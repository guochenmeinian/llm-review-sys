# An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning

Dong Li\({}^{2,3}\), Aijia Zhang\({}^{4}\), Junqi Gao\({}^{4}\), Biqing Qi\({}^{1,2}\)

\({}^{1}\) Department of Electronic Engineering, Tsinghua University,

\({}^{2}\) Shanghai Artificial Intelligence Laboratory,

\({}^{3}\) Institute for Advanced Study in Mathematics, Harbin Institute of Technology,

\({}^{4}\) School of Mathematics, Harbin Institute of Technology

{arvinlee826, zhangaijia065, gjunqi97, qibiqing7}@gmail.com

Corresponding author.

###### Abstract

Incremental graph learning has gained significant attention for its ability to address the catastrophic forgetting problem in graph representation learning. However, traditional methods often rely on a large number of labels for node classification, which is impractical in real-world applications. This makes few-shot incremental learning on graphs a pressing need. Current methods typically require extensive training samples from meta-learning to build memory and perform intensive fine-tuning of GNN parameters, leading to high memory consumption and potential loss of previously learned knowledge. To tackle these challenges, we introduce Mecoin, an efficient method for building and maintaining memory. Mecoin employs Structured Memory Units to cache prototypes of learned categories, as well as Memory Construction Modules to update these prototypes for new categories through interactions between the nodes and the cached prototypes. Additionally, we have designed a Memory Representation Adaptation Module to store probabilities associated with each class prototype, reducing the need for parameter fine-tuning and lowering the forgetting rate. When a sample matches its corresponding class prototype, the relevant probabilities are retrieved from the MRaM. Knowledge is then distilled back into the GNN through a Graph Knowledge Distillation Module, preserving the model's memory. We analyze the effectiveness of Mecoin in terms of generalization error and explore the impact of different distillation strategies on model performance through experiments and VC-dimension analysis. Compared to other related works, Mecoin shows superior performance in accuracy and forgetting rate. Our code is publicly available on the Mecoin-GFSCIL.

## 1 Introduction

In the field of graph learning, conventional methods often assume that graphs are static. However, in the real world, graphs tend to grow over time, with new nodes and edges gradually emerging. For example, in citation networks, new papers are published and cited; in e-commerce, new products are introduced and updated; and in social networks, new social groups form as users join. In these dynamic contexts, simply updating graph representation learning methods with new data often leads to catastrophic forgetting of previously acquired knowledge.

Despite numerous methods proposed to mitigate the catastrophic forgetting problem in Graph Neural Networks(GNNs), a critical and frequently neglected challenge is the scarcity of labels for newly introduced nodes. Most current graph incremental learning methods  combat catastrophic forgetting by retaining a substantial number of nodes from previous graphs to preserve prior knowledge. However, these methods become impractical in graph few-shot class-incremental learning(GFSCIL) scenarios due to the limited labeled node samples. Some methods employ regularization to maintain the stability of parameters critical to the graph's topology. Yet, in GFSCIL, the label scarcity complicates accurate assessment of the relationship between parameter importance and the underlying graph structure, thus increasing the difficulty of designing effective regularization strategies. Consequently, the issue of label scarcity hampers the ability of existing graph continual learning methods to effectively generalize in graph few-shot learning scenarios.

GFSCIL presents two critical challenges: **1)How can we empower models to learn effectively from limited samples? 2)How can we efficiently retain prior knowledge with limited samples?** While the first challenge has been extensively explored in graph few-shot learning contexts, this paper focuses on the second. Currently, discussions on the latter issue within GFSCIL are relatively scarce. Existing methods primarily focus on enhancing models' ability to learn from few-shot graphs and preserve prior knowledge by learning class prototypes through meta-learning and attention mechanisms. However, to strengthen inductive bias towards graph structures and learn more representative class prototypes, these approaches require caching numerous training samples from the meta-learning process for subsequent GFSCIL tasks. This caching not only consumes significant memory but also imposes substantial computational costs. Furthermore, these methods extensively adjust parameters during prototype updates, risking the loss of previously acquired knowledge. These challenges underscore the need for innovative strategies that maintain efficiency without compromising the retention of valuable historical data--achieved through minimal memory footprint, high computational efficiency, and limited parameter adjustments.

To address the aforementioned challenges, we introduce Mecoin, an efficient memory construction and interaction module. Mecoin consists of two core components: the Structured Memory Unit(SMU) for learning and storing class prototypes, and the Memory Representation Adaptive Module(MRaM) for dynamic memory interactions with the GNN. To effectively leverage graph structural information, we design the Memory Construction module(MeCs) within the SMU. MeCs facilitates interaction between features of the input node and prototype representations stored in the SMU through self-attention mechanisms, thereby updating sample representations. Then, it utilizes the local structural information of input nodes to extract local graph structural information and compute a local graph structure information matrix(GraphInfo). The updated sample representations and GraphInfo are used to calculate class prototypes for the input nodes. These newly obtained prototypes are compared with those stored in the SMU using Euclidean distance to determine whether the input samples belong to seen classes. If a sample belongs to a seen class, the corresponding prototype in the SMU remains unchanged. Conversely, if a sample belongs to an unseen class, its calculated prototype is added to the SMU.

To address catastrophic forgetting in class prototype learning caused by model parameter updates, we introduce the MRaM mechanism within Mecoin. MRaM stores probability distributions for each class prototype, allowing direct access via indexing once input nodes are processed through MeCs and corresponding class prototypes are retrieved from SMU. This mechanism separates class prototype learning from node probability distribution learning, effectively mitigating forgetting issues caused by parameter updates. Additionally, to enhance the maintenance and updating of knowledge base, we integrate a Graph Knowledge Interaction Module (GKIM) within MRaM. GKIM transfers information about identified classes from MRaM to GNN and extracts knowledge of new classes from GNN back to MRaM, facilitating continuous knowledge updating and maintenance.

**Contributions.** The main contributions of this paper are as follows: **i)** We design Mecoin, a novel framework that effectively mitigates catastrophic forgetting in GFSCIL by integrating the SMU and MRaM; **ii)** We design the SMU, which efficiently learns class prototypes by facilitating interaction between node features and existing class prototypes, while extracting local graph structures of input nodes; **iii)** We propose the MRaM, which reduces the loss of prior knowledge during parameter fine-tuning by decoupling the learning of class prototypes from node probability distributions; **iv)** We analyze the benefits of separating class prototype learning from node probability distribution learning, considering generalization error bounds and VC dimensions. We also explore how different MRaM-GNN interaction patterns affect model performance; **v)** Through extensive empirical studies, we demonstrate Mecoin's significant advantages over current state-of-the-art methods.

## 2 Notation

Let \(_{0}=(_{0},_{0})\) be the base graph with node set \(_{0}\) and edge set \(_{0}\). We consider \(T\) temporal snapshots of \(_{0}\), each corresponding to a GFSCIL task or session. Denote \(=\{S_{0},S_{1},,S_{T}\}\) as the set of sessions including the pre-training session \(S_{0}\), and \(=\{C_{0},C_{1},,C_{T}\}\) as the family of class sets within each session. The graph under session \(S_{i}(i=1,2,...,T)\) is denoted as \(_{i}=(_{i},_{i})\), with node feature matrix and adjacency matrix represented by \(_{i}=(_{i}^{1},...,_{i}^{|_{i}|})^{ }^{|_{i}| d}\) and \(_{i}^{d d}\) respectively. For session \(S_{i}\), let \(_{i}^{tr}^{|C_{i}| d}\) and \(_{i}^{tr}\) be the features and corresponding labels of the training nodes respectively, where \(K\) is the sample size of each class in \(C_{i}\), thus defining a \(C_{i}\)-way \(K\)-shot GFSCIL task. Let \(_{i}\) be the label space of session \(S_{i}\), and we assume that the label spaces of different sessions are disjoint, i.e., \(_{i}_{j}=\) if \(i j\). Our goal is to learn a model \(f_{}\) across successive sessions that maintains strong performance in the current session and also retains memory of the past sessions.

## 3 Efficient Memory Construction and Interaction Module

In this section, we present a comprehensive overview of our proposed framework, Mecoin. Unlike previous methods that are hampered by inefficient memory construction, low computational efficiency, and extensive parameter tuning--which often lead to the loss of prior knowledge--Mecoin enhances the learning of representative class prototypes. It achieves this by facilitating interaction between input nodes and seen class prototypes stored in the SMU, while integrating local graph structure information of the input nodes. Moreover, Mecoin decouples the learning of class prototypes from their corresponding probability distributions, thereby mitigating the loss of prior knowledge during both the prototype learning and classification processes. Fig. 1 illustrates the architecture of Mecoin.

### Structured Memory Unit

To acquire and store representative class prototypes, we develop SMU within Mecoin. Let \(\) be the set of class prototypes \(\{_{0},_{1},,_{n-1}\}\), where \(n=|C_{T-1}|\) refers to the total number of classes learned from the past \(T-1\) sessions, with each \(_{i}^{k}\)(\( i[n]\)). For current session \(S_{T}\)

Figure 1: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model.

the training node features \(_{T}^{tr}\) are encoded through a pre-trained GNN model \(g_{}\):

\[_{T}=g_{}(_{T},_{T}^{tr}),\] (1)

where \(_{T}^{(|C_{T}|K) h}\). During the training process, the parameters of the pre-trained GNN, denoted as \(\), remain fixed, and the training set used for pre-training is excluded from the subsequent GFSCIL tasks. To mitigate the significant memory usage resulting from caching meta-learning samples and incorporate as much graph structural information as possible, we design MeCs within the SMU. MeCs merges graph structure information from the past sessions with that of the current session by interacting the encoded features of node \(_{T}\) with the class prototypes in \(\). Specifically, MeCs firstly facilitates the interaction between \(_{T}\) and the SMU-stored prototypes \(_{0:T-1}(_{0},_{1},,_{ n-1})^{}\) through a self-attention mechanism:

\[_{T}:=(_{T}_{Q})(_{0:T-1}_{K})^{}}{})_{0:T-1}_{V}\] (2)

where \(_{Q}^{h m},_{K}^{k m },_{V}^{k h}\) are learnable weight matrices and \(_{T}^{(|C_{T}|K) h}\). Subsequently, to reduce memory consumption, we perform dimensionality reduction on \(_{T}\) through Gaussian random projection, resulting in \(}_{T}^{(|C_{T}|K) r}\), where \(0<r<k<h\). MeCs then extracts local graph structure information of \(_{T}\) via the vanilla self-attention on \(_{T}^{tr}\), and preserves this information in the GraphInfo \(_{T}\):

\[_{T}:=(_{T}^{tr}).\] (3)

where \(_{T}^{(|C_{T}|K)(k-r)}\) and is integrated with \(}_{T}\) by concatenation:

\[_{T}=(_{T},}_{T}).\] (4)

To determine the class prototypes under session \(S_{T}\), we perform \(k\)-means clustering on \(_{T}\):

\[_{T}:=(_{T}),\] (5)

where \(_{T}^{|C_{T}| k}\). In addition, to improve the utilization of graph structural information, we optimize \(_{T}\) by the _edge_ loss \(_{edge}\) defined as follows:

\[_{edge}=\|_{T}^{tr}(_{T}^{tr})^{}- _{T}(_{T})^{}\|_{2}^{2}.\] (6)

For each node feature(i.e. each row) in \(_{i}\), We identify the nearest class prototype by minimizing the Euclidean distance:

\[_{j}^{*}=_{m_{j}}\;||_{i}^{l}- _{j}||_{2},\] (7)

where \(_{i}^{l}\) is the \(l\)-th row in \(_{i}\).

### Memory Representation Adaptive Module

In the traditional GFSCIL paradigm, adapting to evolving graph structures requires continuous learning and updating of class prototypes based on the current task's graph structure, alongside classifier retraining. This process, which involves adjusting parameters during class prototype learning, can lead the classifier to forget information about past categories, exacerbating the model's catastrophic forgetting. To address this, we introduce the MRaM within Mecoin. MRaM tackles this challenge by decoupling class prototype learning from class representation learning, caching probability distributions of seen categories. This separation ensures that class prototype updates don't affect the model's memory of probability distributions for nodes in seen categories, thus enhancing the stability of prior knowledge retention [14; 15; 16].

To maintain the model's memory of the prior knowledge, we introduce GKIM into MRaM for information exchange between Mecoin and the model. Specifically, let \(=\{_{0},_{1},,_{n-1}\}\) denote the class representations learned from the GNN and stored in MRaM, where each \(_{i}\) corresponds to its respective class prototype \(_{i}\). For a node feature \(_{s}\) from the seen classes with its class representation \(_{_{s}}\), let \(_{_{s}}^{MLP}\) be the category predicted probabilities learned from GNN and MLP, then GKIM transfers the prior knowledge stored in Mecoin to the model through distillation that based on the _memory_ loss function:

\[_{memory}=}_{i=1}^{N_{s}}(_{i }\|_{_{s}}^{MLP})=}_{i=1}^{N_{s}}_{i}_{i}}{_{_{s}}^{MLP}},\] (8)

[MISSING_PAGE_FAIL:5]

**Theorem 2:** If \(n\) class representations are selected from GKIM, i.e. \(n\) is the input size of GNN, then the VC dimension of GKIM is:

\[VCD=(n+1)&\\ (p^{2}H^{2})&\\ (p^{2}n^{2}H^{2})&\] (12)

where \(H\) is the number of hidden neurons in MLP and \(p\) denotes the number of parameters of GNN.

In the above theorem, we take the common voting classifier as an example for non-parametric methods. It averages the category representations stored in MRaM and updates them through backpropagation. While this method has lower complexity, it is highly sensitive to the initialization of category representations due to its reliance solely on the average while computing the prediction probabilities. When using MLP to update category representations, its parameters require fine-tuning to accommodate new categories that will lead to the forgetting of prior knowledge. In contrast, GKIM exhibits a higher VC-dimension compared to the other two methods, indicating superior expressive power. Additionally, GKIM selectively updates category representations in MRaM locally, preserving the model's memory of prior knowledge.

## 4 Experiments

In this section, we will evaluate Mecoin through experiments and address the following research questions: **Q1**). Does Mecoin have advantages in the scenarios of graph few-shot continual learning? **Q2**). How does MeCs improve the representativeness of class prototypes? **Q3**). What are the advantages of GKIM over other distillation methods?

### Graph Few-Shot Continual Learning (Q1)

**Experimental setting.** We assess Mecoin's performance on three real-world graph datasets: Cora-Full, CS, and Computers, comprising two citation networks and one product network. Datasets are split into a Base set for GNN pretraining and a Novel set for incremental learning. Tab.1 provides the statistics and partitions of the datasets. In our experiments, we freeze the pretrained GNN parameters and utilize them as encoders for subsequent few-shot class-incremental learning on graphs. For CoraFull, the Novel set is divided into 10 sessions, each with two classes, using a 2-way 5-shot GFSCIL setup. CS's Novel set is split into 10 sessions, each with one class, adopting a 1-way 5-shot GFSCIL configuration. Computers' Novel set is segmented into 5 sessions, each with one class, also using a 1-way 5-shot GFSCIL setup. During the training process, the training samples for session 0 are randomly selected from each class of the pre-trained testing set, with 5 samples chosen as the training set and the remaining testing samples used as the test set for training. It is worth noting that for each session, during the testing phase, we construct a new test set using all the testing samples of seen classes to evaluate the model's memory of all prior knowledge after training on the new graph data. Our GNN and GAT models feature two hidden layers. The GNN has a consistent dimension of 128, while GAT varies with 64 for CoraFull and 16 for CS and Computers. Training parameters are set at 2000 epochs and a learning rate of 0.005.

**Baselines.** 1) Elastic Weight Consolidation (EWC) -imposes a quadratic penalty on weights to preserve performance on prior tasks. 2) Learning without Forgetting (LwF) -retains previous knowledge by minimizing the discrepancy between old and new model outputs. 3) Topology-aware Weight Preserving (TWP) -identifies and regularizers parameters critical for graph topology to maintain task performance. 4) Gradient Episodic Memory (GEM) -employs an episodic memory to adjust gradients during learning, preventing loss increase from prior tasks. 5) Experience Replay GNN (ER-GNN) - incorporates memory replay into GNN by storing key nodes from prior tasks. 6) Memory Aware Synapses (MAS)  evaluates parameter importance through sensi

   Dataset & CoraFull & CS & Computers \\  Nodes & 19,793 & 18,333 & 13,752 \\ Edges & 126,842 & 163,788 & 491,722 \\ Features & 8,710 & 6805 & 767 \\ Labels & 70 & 15 & 10 \\  Training set & 50 & 5 & 5 \\ Novel set & 20 & 10 & 5 \\   

Table 1: Information of the experimental datasets.

tivity to predictions, distinct from regularization-based EWC and TWP. 7) HAG-Meta ,a GFS-CIL approach, preserves model memory of prior knowledge via task-level attention and node class prototypes. 8) Geometer - adjusts attention-based prototypes based on geometric criteria, addressing catastrophic forgetting and imbalanced labeling through knowledge distillation and biased sampling in GFSCIL.

**Experimental results.** We execute 10 tests for each method across three datasets, varying the random seed, and detailed the mean test accuracy in Tables 2, 3, and 4. Key findings are as follows:1) Mecoin surpasses other benchmarks on CoraFull, CS, and Computers datasets, exemplified by a 66.72% improvement over the best baseline on the Computers dataset across all sessions. 2) Mecoin maintains a performance edge and exhibits reduced forgetting rates, substantiating its efficacy in mitigating catastrophic forgetting. 3) Geometer and HAG-Meta perform poorly in our task setting, likely because, to ensure a fair comparison, the models do not use any training data from the meta-learning or pre-training processes during training. The computation of class prototypes in these two methods heavily relies on the graph structure information provided by this data. Additionally, in Fig.2, we compare the results of Geometer and HAG-Meta tested under the experimental conditions given in their papers with the results of Mecoin on the CoraFull dataset. The experimental results indicate that our method still achieves better performance and forgetting rates than these two methods under low memory conditions, demonstrating the efficiency of our method in terms of memory.

### MeCs for Memory (Q2)

We conduct relevant ablation experiments on MeCs across the CoraFull, CS, and Computers datasets:1) Comparing the impact of MeCs usage on model performance and memory retention; 2) Analyzing the effect of feature of node interaction with class prototypes stored in SMU on model

    &  Average \\ ACC \(\) \\  }} \\   & backbone & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 &  ACC \(\) \\  }} \\   & GCN & 73.43 & 77.64 & 29.38 & 32.14 & 20.93 & 21.84 & 16.13 & 15.28 & 15.2 & 11.77 & 11.30 & 62.13 & 29.55 \\  & GAT & 73.43 & 77.64 & 29.38 & 32.14 & 20.93 & 21.84 & 16.13 & 15.28 & 15.20 & 11.77 & 11.30 & 62.13 & 29.55 \\   & GCN & 73.43 & 45.92 & 25.20 & 18.40 & 19.44 & 13.69 & 11.70 & 11.47 & 9.86 & 8.33 & 8.27 & 65.16 & 22.38 \\  & GAT & 69.06 & 49.49 & 25.30 & 19.85 & 19.44 & 14.78 & 12.79 & 11.91 & 9.90 & 9.15 & 7.52 & 61.54 & 22.65 \\   & GCN & 73.43 & 46.40 & 25.20 & 18.40 & 19.44 & 13.69 & 11.70 & 11.47 & 9.86 & 8.83 & 8.27 & 65.16 & 22.38 \\  & GAT & 69.06 & 66.44 & 44.45 & 84.13 & 37.08 & 47.04 & 45.88 & 91.46 & 42.69 & 41.36 & 43.92 & 22.67 & 49.99 \\   & GCN & 73.43 & 45.92 & 25.75 & 17.82 & 19.71 & 14.16 & 10.95 & 11.38 & 9.90 & 8.08 & 7.73 & 65.7 & 22.26 \\  & GAT & 73.60 & 48.83 & 24.59 & 24.03 & 19.95 & 13.91 & 13.76 & 13.24 & 10.83 & 14.02 & 7.46 & 66.14 & 24.02 \\   & GCN & 73.43 & 45.92 & 26.48 & 18.52 & 20.27 & 14.59 & 11.41 & 11.59 & 10.04 & 7.79 & 7.75 & 65.68 & 22.53 \\  & GAT & 69.06 & 48.52 & 25.58 & 23.54 & 20.39 & 15.97 & 14.86 & 22.71 & 19.77 & 12.31 & 12.55 & 56.51 & 27.14 \\   & GCN & 70.54 & 44.90 & 26.71 & 20.65 & 24.16 & 14.56 & 18.77 & 14.37 & 19.37 & 12.48 & 14.48 & 56.06 & 25.54 \\   & GAT & 69.06 & 50.27 & 26.53 & 28.27 & 21.42 & 23.29 & 15.04 & 26.74 & 14.86 & 17.03 & 13.77 & 55.29 & 27.84 \\   & HAG-Meta & / & **87.62** & **82.78** & **79.01** & **74.5** & 67.65 & 63.38 & 60.00 & 57.36 & 55.5 & 52.98 & 51.47 & 36.15 & 66.57 \\   & / & 72.23 & 61.73 & 43.34 & 37.61 & 36.24 & 32.79 & 29.97 & 22.11 & 21.01 & 17.34 & 16.32 & 55.91 & 35.52 \\   & GCN & 82.18 & 81.56 & 77.47 & 71.83 & **70.43** & **69.70** & **68.78** & **68.07** & **66.70** & **62.10** & **61.36** & 20.82 & **70.90** \\   & GAT & 75.53 & 73.04 & 70.74 & 67.92 & 66.06 & 64.97 & 64.18 & 63.08 & 62.02 & 60.62 & 60.10 & **15.43** & 66.22 \\   

Table 2: Comparison with SOTA methods on CoraFull dataset for GFSCIL.

Figure 2: The comparative analysis of the mean performance, accuracy curves and memory utilization of HAG-Meta, Geometer and Mecoin across 10 sessions on CoraFull, conducted under the experimental conditions delineated in their respective publications.

performance and memory retention;3)Assessing the impact of using graphInfo on model performance and memory retention. We conduct a randomized selection of ten classes from the test set, extracting 100 nodes per class to form clusters with their corresponding class prototypes within the SMU. Subsequently, we assess the efficacy of MeCs and its constituent elements on model performance and the rate of forgetting. In cases where the MeCs was not utilized, we employ the k-means method to compute class prototypes. In experiment, we use GCN as backbone. The experimental parameters and configurations adhered to the standards established in prior studies.

**Experimental Results.** The results of the ablation experiments on CoraFull and CS are shown in Fig.3 and results on other datasets are shown in Appendix.A. We deduce several key findings from the figure:1) The class prototypes learned by MeCs assist the model in learning more representative prototype representations, demonstrating the effectiveness of the MeCs method. 2) The difference in accuracy between scenarios where graphInfo is not used and where there is no interaction with class prototypes is negligible. However, the scenario without graphInfo exhibits a higher forgetting rate, indicating that local graph structural information provides greater assistance to model memory. This may be because local structural information reduces the intra-class distance of nodes, thereby helping the model learn more discriminative prototype representations.

    &  &  & Average \\   & backbone & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & ACC \(\) \\   & GCN & 100 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 14.29 & 12.5 & 11.11 & 10.00 & 14.02 & 85.98 & 22.90 \\  & GAT & 100 & 50.00 & 33.33 & 46.17 & 33.95 & 36.93 & 24.64 & 23.34 & 18.60 & 25.44 & 30.12 & 69.88 & 38.41 \\   & GCN & 100 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 14.29 & 12.50 & 11.11 & 11.00 & 12.12 & 87.88 & 27.73 \\   & GCN & 100 & 50.00 & 33.33 & 25.00 & 20.00 & 17.03 & 14.85 & 13.98 & 14.17 & 21.52 & 18.09 & 81.91 & 30.30 \\   & GCN & 100 & 50.00 & 33.33 & 25.27 & 20.00 & 16.67 & 14.29 & 12.36 & 11.11 & 18.75 & 9.70 & 90.30 & 28.32 \\   & GAT & 100 & 50.00 & 33.45 & 48.23 & 56.16 & 59.54 & 65.57 & 64.28 & 61.41 & 64.36 & 63.92 & 36.08 & 60.68 \\   & GCN & 100 & 50.00 & 33.33 & 25.00 & 20.00 & 16.73 & 14.29 & 12.36 & 11.11 & 16.36 & 15.44 & 84.56 & 28.60 \\   & GAT & 100 & 50.00 & 33.33 & 24.71 & 36.28 & 36.40 & 28.92 & 28.84 & 11.23 & 29.12 & 23.51 & 67.49 & 38.30 \\   & GCN & 100 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 14.29 & 12.5 & 11.11 & 10.00 & 12.12 & 87.88 & 27.73 \\   & GAT & 100 & 50.00 & 33.33 & 31.65 & 38.60 & 36.62 & 25.64 & 29.04 & 16.19 & 26.44 & 38.63 & 61.37 & 38.74 \\   & GCN & 100 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 14.29 & 12.43 & 11.04 & 16.81 & 15.03 & 84.97 & 28.6 \\   & GAT & 100 & 50.35 & 33.33 & 25.18 & 38.14 & 41.74 & 39.34 & 27.67 & 30.52 & 41.11 & 52.02 & 47.98 & 44.44 \\   & HAG-Meta & / & 200.00 & 16.67 & 14.29 & 12.50 & 11.11 & 10.00 & 9.09 & 8.33 & 7.69 & 7.14 & 6.66 & **13.34** & 11.24 \\   & Gcometer & / & 60.60 & 33.35 & 24.05 & 19.03 & 24.87 & 28.86 & 24.46 & 18.18 & 19.30 & 26.39 & 29.63 & 30.97 & 28.11 \\   & GCN & 98.07 & **94.09** & **99.01** & **73.75** & **79.95** & **82.21** & **74.13** & 12.71 & 69.48 & 68.01 & 59.96 & 38.41 & **77.96** \\   & GAT & 97.83 & **95.13** & 90.75 & 68.02 & 71.79 & 77.88 & **75.35** & **75.06** & **72.52** & **65.92** & **62.21** & 35.62 & 77.50 \\   

Table 4: Comparison with SOTA methods on CS dataset for GFSCIL.

    &  &  & Average \\   & backbone & 0 & 1 & 2 & 3 & 4 & 5 & & ACC \(\) \\   & GCN & 100.00 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 83.33 & 40.83 \\  & GAT & 100.00 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 83.33 & 40.83 \\   & GCN & 100.00 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 83.33 & 40.83 \\   & GCN & 100.00 & 50.00 & 33.33 & 25.00 & 20.00 & 16.67 & 83.33 & 40.83 \\   & GAT & 100.00 & 50.00 & 33.33 & 25.00 & 20.00 & 16

### GKIM for Memory (Q3)

In our experimental investigation across three distinct datasets, we scrutinized the influence of varying interaction modalities between MRaM and GNN models on model performance and the propensity for forgetting. We delineate three distinct scenarios for analysis: 1) Class representations stored in MRaM are classified using non-parametric methods and used as the teacher model to interact with the GNN. \(2)\) Class representations stored in MRaM are classified using MLP and Mecoin is employed as the teacher model to transfer knowledge to the GNN model. 3) GNN models are deployed in isolation for classification tasks, without any interaction with Mecoin. In this experiment, we use GCN as backbone. Throughout the experimental process, model parameters were held constant, and the experimental configurations were aligned with those of preceding studies. Due to space constraints, we include the results on the Computers dataset in the Appendix.A.

**Experimental Results**. It is worth noting that due to the one-to-one matching between MRaM and SMU via non-parametric indexing, there is no gradient back-propagation between these two components. This implies that updates to MRaM during training do not affect the matching between node features and class prototypes. Our experimental findings are depicted in Fig.4, and they yield several key insights: 1) GKIM outperforms all other interaction methods, thereby substantiating its efficacy. 2) The second interaction mode exhibits superior performance compared to the first and third methods. This is attributed to the MLP's higher VC-dimension compared to the non-parametric

Figure 3: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull, Computers and CS datasets. The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs. The experimental results for CoraFull are shown in the above figure, the results for Computers are in the middle and the results for CS are in the figure below.

method, which gives it greater expressive power to handle more complex sample representations. However, the use of MLP results in a higher forgetting rate compared to the non-parametric method. This is because when encountering samples from new classes, the parameters of MLP undergo changes, leading to the loss of prior knowledge. For the third method, extensive parameter fine-tuning leads to significant forgetting of prior knowledge. Method one performs less effectively than GKIM, primarily because GKIM, with its larger VC-dimension, ensures that the process of updating class representations does not affect representations of other classes stored in MRaM.

## 5 Conclusion

Current GFSCIL methods typically require a large number of labeled samples and cache extensive past task data to maintain memory of prior knowledge. Alternatively, they may fine-tune model parameters at the expense of sacrificing the model's adaptability to current tasks. To address these challenges, we propose the Mecoin for building and interacting with memory. Mecoin is made up of two main parts: the Structured Memory Unit (SMU), which learns and keeps class prototypes, and the Memory Representation Adaptive Module (MRaM), which helps the GNN preserve prior knowledge. To leverage the graph structure information for learning representative class prototypes, SMU leverages MeCs to integrate past graph structural information with interactions between samples and the class prototypes stored in SMU. Additionally, Mecoin introduces the MRaM, which separates the learning of class prototypes and category representations to avoid excessive parameter fine-tuning during prototype updates, thereby preventing the loss of prior knowledge. Furthermore, MRaM injects knowledge stored in Mecoin into the GNN model through GKIM, preventing knowledge forgetting. We demonstrate our framework's superiority in graph few-shot continual learning with respect to both generalization error and VC dimension, and we empirically show its advantages in accuracy and forgetting rate compared to other graph continual learning methods.

## 6 Acknowledgement

This work is supported by the National Science and Technology Major Project (2023ZD0121403). We extend our gratitude to the anonymous reviewers for their insightful feedback, which has great-lycontributed to the improvement of this paper.