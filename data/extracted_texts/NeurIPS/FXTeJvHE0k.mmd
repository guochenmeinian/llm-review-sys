# NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking

Daniel Dauner\({}^{1,2}\) Marcel Hallgarten\({}^{1,5}\) Tianyu Li\({}^{3}\) Xinshuo Weng\({}^{4}\) Zhiyu Huang\({}^{4,6}\)

Zetong Yang\({}^{3}\) Hongyang Li\({}^{3}\) Igor Gilitschenski\({}^{7,8}\) Boris Ivanovic\({}^{4}\) Marco Pavone\({}^{4,9}\)

**Andreas Geiger\({}^{1,2}\) Kashyap Chitta\({}^{1,2}\)**

\({}^{1}\)University of Tubingen \({}^{2}\)Tubingen AI Center \({}^{3}\)OpenDriveLab at Shanghai AI Lab \({}^{4}\)NVIDIA Research \({}^{5}\)Robert Bosch GmbH \({}^{6}\)Nanyang Technological University \({}^{7}\)University of Toronto \({}^{8}\)Vector Institute \({}^{9}\)Stanford University

###### Abstract

Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, _i.e._, the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.

## 1 Introduction

Autonomous vehicles (AVs) have gained immense research interest due to their potential to change transportation and improve traffic safety . This has created a large community working on the development of AV algorithms, which map high-dimensional sensor data to desired vehicle control outputs. Therefore, measuring and comparing the performance of AV algorithms is a crucial task.

Unfortunately, it is extremely challenging to evaluate driving performance, and the most widely-used benchmarks today fall short in several respects: (1) the datasets used, such as nuScenes , were created for perception tasks such as object detection. As such, they focus on visual diversity and label quality instead of the relevance of the data for research on planning. Often, most frames have a trivial solution of extrapolating the historical driving behavior, leading to "blind" driving policies that observe only the vehicle's past trajectory obtaining state-of-the-art performance . (2) Due to the fact that driving is an inherently multifaceted task where the algorithm must coordinate several desired properties such as safety, comfort, and progress, the evaluation metric must also balancepotentially conflicting objectives. However, as shown in Fig. 1, existing metrics such as the average displacement error (ADE) between a predicted and recorded human trajectory often misrepresent the relative accuracy of trajectories. (3) Since driving involves interactions among multiple agents, evaluation must ideally be interactive, e.g., in simulation. Unfortunately, existing simulators with synthetic sensor data exhibit a significant domain gap to real-world driving. (4) Besides, the lack of a standardized evaluation setup has led to subtle inconsistencies between metrics in existing work, leading to unfair comparisons and inaccurate conclusions . Collectively, these problems hinder progress in the development of AVs, emphasizing the need for more principled benchmarks.

In this work, we take steps towards alleviating these issues. First, we propose a strategy for sampling interesting driving scenarios and apply it to the largest publicly-available driving dataset . We obtain, for the first time, over 100k challenging real-world driving scenarios for training and evaluating sensor-based driving policies. We show that in these scenarios, "blind" driving policies fail to compete with more principled sensor-based policies. Second, we draw inspiration from the literature of rule-based planning for AVs  to identify a set of diverse, efficient, and principled metrics that cover multiple facets of the autonomous driving task. Third, we circumvent the need for inaccurate sensor simulation with domain gaps by simplifying our simulation to a non-reactive one. Given an observed real-world sensor input, the agent under test commits to a set of actions for a specific time horizon. Further, these actions are assumed to not affect the future behavior of other agents in the scene. Under this setting, it is possible to simulate the expected motion of all agents over this time horizon in a simplified bird's-eye-view (BEV) abstraction of the scene, and incorporate metrics that involve interactions, as we observe in Fig. 1. Empirically, we demonstrate that our selected metrics are well-correlated to the outcomes of closed-loop simulations. Finally, we establish an official evaluation server on the open-source HuggingFace platform, which is free, has a low maintenance overhead, and enables future scaling to more challenging datasets and metrics.

We combine these ideas to propose NAVSIM, a comprehensive tool for AV data curation, simulation, and benchmarking. We instantiate standardized training and evaluation splits for NAVSIM with the OpenScene dataset , though our framework can be extended to other datasets. With these splits, we present a detailed analysis of popular end-to-end driving models previously benchmarked either exclusively on CARLA  or nuScenes , providing the first direct comparison between these families of approaches in an independent evaluation setting. Interestingly, we find that the performances of the best methods developed in both settings are similar, despite a vast difference in computational requirements for their training. Finally, we review the insights gained through the 2024 NAVSIM challenge1, hosted in conjunction with the CVPR 2024 Workshop on Foundation Models for Autonomous Systems. For the challenge, 143 teams from 13 countries developed diverse methods that competed on the proposed benchmark. The top methods ranged from multi-billion parameter vision language models  to more efficient and recently overlooked approaches based on trajectory sampling and scoring , demonstrating the remarkable ability of the broader community to advance AV research when provided with the right tools.

Figure 1: **NAVSIM.** Traditional metrics such as the average displacement error (ADE) overlook the multi-modality of driving. They penalize trajectories that deviate from a recorded human driving log, even if such a trajectory is safe. Our benchmark evaluates trajectory outputs of sensor-based driving policies with simulation-based metrics, considering collisions and map compliance.

**Contributions.** (1) We build NAVSIM, a framework for non-reactive AV simulation, with standardized protocols for training and testing, data curation tools ensuring broad accessibility, and an official public evaluation server used for the inaugural NAVSIM challenge. (2) We develop configurable simulation-based metrics that are well-suited for evaluating sensor-based motion planning. (3) We reimplement a collection of end-to-end approaches for NAVSIM including TransFuser, UniAD, and PARA-Drive, showcasing the surprising potential of simple models in our challenging scenarios.

## 2 Related Work

**End-to-End Driving.** End-to-end driving streamlines the entire stack from perception to planning into a single optimizable network. This eliminates the need for manually designing intermediate representations. Following pioneering work [35; 4; 27], a diverse landscape of end-to-end models has emerged. For instance, an extensive body of end-to-end approaches focuses on closed-loop simulators, utilizing single-frame cameras, LiDAR point clouds, or a combination of both for expert imitation [7; 11; 36; 8; 52; 43; 44; 12; 24; 57; 22]. More recently, developing end-to-end models on open-loop benchmarks has gained traction [20; 21; 25; 54; 32; 50]. Our work introduces a new evaluation scheme with which we compare end-to-end models from both communities.

**Closed-Loop Benchmarking with Simulation.** Driving simulators allow us to evaluate autonomous systems in a closed-loop manner and collect downstream driving statistics, including collision rates, traffic-rule compliance, or comfort. A broad body of research conducts evaluations in simulators, such as CARLA  or Metadrive  with sensor simulation, or nuPlan  and Waymax  for data-driven simulation. Unfortunately, ensuring realism when simulating traffic behavior or sensor data remains a challenging task. To simulate camera or LiDAR sensors, most established simulators rely on graphics-based rendering methods, leading to an inherent domain gap in terms of visual fidelity and sensor characteristics. Data-driven simulators for motion planning incorporate traffic recordings but do not support image or LiDAR-based methods [26; 19; 13]. Data-driven sensor simulation leverages and adapts real-world sensor data to create new simulations where the vehicle may move differently, but the rendering quality of existing tools is subpar [1; 2; 49]. Further, while promising image  or LiDAR  synthesis approaches exist, efficiently simulating sensors entirely from data remains an open problem. In this work, we provide an approach for the evaluation of real sensor data with simulation-based metrics by making a simplifying assumption that the agent and environment do not influence each other over a short simulation horizon. Despite this strong assumption, when benchmarking on real data, NAVSIM better reflects planning performance than established evaluation protocols, as demonstrated through our systematic experimental analysis.

**Open-Loop Benchmarking with Displacement Errors.** Open-loop evaluation protocols commonly measure displacement errors between trajectories of a recorded expert (i.e., of a human driver) and a motion planner. However, several issues concerning evaluation with displacement errors have surfaced recently, particularly on the nuScenes dataset . Given that nuScenes does not provide standardized planning metrics, prior work relied on independent implementations, which led to inconsistencies when reporting or comparing results [50; 32]. Next, most planning models in nuScenes receive the human trajectory endpoint as a discrete direction command [20; 21; 25; 32; 50], thereby leaking ground-truth information into inputs. Moreover, about 75% of the scenarios in nuScenes involve trivial straight driving , leading to simple solutions when extrapolating the ego-motion. For instance, AD-MLP demonstrates that an MLP on the kinematic ego status (ignoring perception completely) can achieve state-of-the-art displacement errors . Such blind agents are undeniably dangerous, which highlights a broader concern: displacement metrics are not correlated to closed-loop driving [14; 17; 3; 16]. In this work, we address prevalent issues of nuScenes and propose a standardized driving benchmark with challenging scenarios and an official evaluation server. We derive a navigation goal from the lane graph instead of the human trajectory to prevent label leakage, and propose principled simulation-based metrics as an alternative to displacement errors.

## 3 NAVSIM: Non-Reactive Autonomous Vehicle Simulation

NAVSIM combines the ease of use of open-loop benchmarks such as nuScenes  with metrics based on closed-loop simulators such as nuPlan . In the following, we give a detailed introduction to the task and metrics that driving agents are challenged with in NAVSIM. Subsequently, we propose a filtering method to obtain standardized train and test splits covering challenging scenes.

**Task description.** Driving agents in NAVSIM must plan a trajectory, defined as a sequence of future poses, over a horizon of \(h\) seconds. Their input contains streams of _past_ frames from onboard sensors, such as cameras, LiDAR, as well as the vehicle's current speed, acceleration, and navigation goal, jointly termed the ego status. For compatibility with prior work , we provide the navigation goal as a one-hot vector with three categories: left, straight, or right.

**Non-Reactive Simulation.** Traditional closed-loop benchmarks normally infer planners at high frequencies, e.g., 10Hz . However, this requires efficient simulation of all input modalities for the driving agent, including high-dimensional sensor streams in the case of sensor-based approaches. To sidestep this, the core idea of NAVSIM is to evaluate driving agents using a non-reactive simulation. This means driving agents are only queried in the initial frame of each scene. Afterwards, the planned trajectory is kept fixed for the entire trajectory duration. Over this short horizon, no environmental feedback is provided to the driving agent, and the NAVSIM evaluation is purely based on the initial real-world sensor sample. This makes the agent's task more challenging, limiting simulations to short horizons. We select a horizon of \(h=4\) seconds, which has been shown in prior work to be adequate for closed-loop planning . Despite this limitation, non-reactive simulation offers a key advantage: unlike traditional open-loop benchmarks, which mainly compare the planned trajectory to the human driver's trajectory in a similar setting, it enables the use of simulation outcomes to compute metrics reflecting safety, comfort, and progress. An LQR controller  is applied at each simulation iteration to calculate steering and acceleration values, and a kinematic bicycle model  propagates the ego vehicle. We execute this pipeline at \(10\)Hz over the \(4\)s trajectory horizon. In Sec. 4.1, we show that despite our simplifying assumption, our evaluation results in a much better alignment with closed-loop metrics than traditional open-loop metrics achieve.

**PDM Score.** NAVSIM scores driving agents in two steps. First, subscores in range \(\) are computed after simulation. Second, these subscores are aggregated into the PDM Score (PDMS) \(\). It is named after the Predictive Driver Model (PDM) , a state-of-the-art rule-based planner which uses this scoring function to evaluate trajectory proposals during closed-loop simulation in nuPlan. The metric is also an efficient reimplementation of the nuPlan closed-loop score metric . In NAVSIM, the PDMS can be adapted by adding or removing subscores, changing aggregation parameters, or making subscores more challenging, e.g., by adapting their internal thresholds. It is calculated per frame and averaged across frames. In this work, we use the following aggregation of subscores:

\[=,\}}_{m})}_{}}}_{w}_{w}}{_{w\{}}_{w}})}_{}.\] (1)

Subscores are categorized by their importance as penalties or terms in a weighted average. A penalty punishes inadmissible behavior such as collisions with a factor \(<1\). The weighted average aggregates subscores for other objectives such as progress and comfort. In the following, we briefly describe each subscore. More details can be found in the supplementary material.

**Penalties.** Avoiding collisions and staying on the road is imperative for motion planning as it ensures traffic rule compliance and the safety of pedestrians and road users. Thus, failing to drive with no collisions (NC) with road users (vehicles, pedestrians, and bicycles) or infractions with regard to drivable area compliance (DAC) result in hard penalties of \(_{}=0\) or \(_{}=0\) respectively. This results in a PDMS of 0 for the current scene. We ignore certain collisions that are not considered "at-fault" in the non-reactive environment, e.g. when the ego vehicle is static. For collisions with static objects, we apply a softer penalty of \(_{}=0.5\).

**Weighted Average.** The weighted average accounts for ego progress (EP), time-to-collision (TTC), and comfort (C). The ego progress subscore \(_{}\) represents the agent progress along the route center as a ratio to an approximated safe upper bound from the PDM-Closed planner . PDM-Closed obtains a possible progress value without collisions or off-road driving with a search-based strategy based on trajectory proposals. The final ratio is clipped to \(\) while discarding low or negative progress scores if the upper bound is below 5 meters. Next, the TTC subscore ensures that driving agents respect the safety margins to other vehicles. Defaulting to a value of \(1\), this subscore is set to \(0\) if for any simulation step within the \(4\)s horizon, the ego-vehicle's time-to-collision, when projected forward with a constant velocity and heading, is less than a certain threshold. Finally, the comfort subscore is obtained by comparing the acceleration and jerk of the trajectory to predetermined thresholds. Following the cost weights used by the PDM-Closed planner and the 2023nuPlan Challenge, we set the coefficients of the weighted average as \(}}=5\), \(}}=5\), and \(}}=2\). We find this selection reasonable and robust to changes. For example, the top 3 ranks of the NAVSIM challenge remain identical when assigning an equal weight to the subscores.

### Generating Standardized and Challenging Train and Test Splits

**Dataset.** The NAVSIM framework is agnostic to the choice of driving dataset. We choose OpenScene , a redistribution of nuPlan , the largest annotated public driving dataset. OpenScene includes 120 hours of driving at a reduced frequency of \(2\)Hz typically considered by end-to-end planning algorithms, resulting in a \(90\%\) reduction of data storage requirements compared to nuPlan from over 20 TB to 2 TB. Our agent input, based on OpenScene, comprises eight cameras, each with a resolution of \(1920 1080\) pixels, and a merged LiDAR point cloud from five sensors. The input includes the current time-step and optionally 3 past frames, totaling \(1.5\)s at 2Hz. In principle, any driving dataset that provides annotated HD maps, object bounding boxes, and sensor data can be converted into this format and thus be used with NAVSIM.

**Filtering for challenging scenes.** A majority of human driving data involves trivial situations such as being stationary or straight driving at a near constant speed. These can be solved efficiently by simple heuristics, e.g., as depicted in Fig. 2 (a), the baseline of maintaining a constant velocity and heading achieves a PDMS of 79% on the OpenScene dataset, where human-level performance corresponds to \(91\)%. In NAVSIM, we propose the use of a filtered dataset to remove frames with (1) near-trivial solutions and (2) significant annotation errors. We remove highly simplistic scenes by detecting if the previously mentioned constant velocity agent exceeds a PDMS of \(0.8\). Similarly, we remove scenes in which the human trajectory results in a PDMS of less than \(0.8\). This ensures that an acceptable solution exists to these difficult scenarios and filters out noisy annotations such as inaccurate bounding boxes. These thresholds can be adjusted based on the desired filtered dataset size. The resulting scenarios are challenging, which is underlined by the score of the constant velocity agent dropping to \(22\)%, whereas the human expert achieves a score of \(95\)%. The higher ratio of non-trivial scenarios, such as turning, also results in endpoints being less distant longitudinally when nonzero, and more evenly distributed laterally, as seen in Fig. 2 (b-c). We employ this filtering strategy to provide standardized splits for training and testing, called \(\) and \(\), with 103k and 12k samples respectively. This curated data serves as a benchmark accessible as a standalone download option with a moderate storage demand given its large scale and diversity (450 GB).

## 4 Experiments

In this section, we present the results of our experiments aimed at answering the following questions: (1) Can non-reactive open-loop simulation provide sufficient correlation to closed-loop metrics? (2) What new conclusions do experiments on NAVSIM provide compared to prior benchmarks?

Figure 2: **Filtering. (a) We consider challenging scenes where maintaining a constant velocity and heading fails compared to the human driver. (b) Our filtering primarily removes scenes with static or fast longitudinal movement and (c) leads to more diversity in lateral movement (log-scale).**

### Alignment Between Open-Loop and Closed-Loop Evaluation

Open-loop metrics should ideally be aligned with closed-loop metrics in their evaluation of different driving algorithms. In this section, we benchmark a large set of planners to analyze the alignment of closed-loop metrics with traditional distance-based open-loop metrics and the proposed PDMS.

**Benchmark.** Studying the relation of closed-loop and open-loop metrics necessitates access to a fully reactive simulator. To stay compatible with the dataset, we use the nuPlan simulator , which enables simulation for privileged planners with access to ground-truth perception and HD map inputs. Similar to PDMS, nuPlan combines weighted averages and multiplied penalties in two official scores: the **open-loop score (OLS)** aggregates displacement and heading errors with a multiplied miss-rate, and the **closed-loop score (CLS)** implements similar metrics from Section 3. Including PDMS, all metrics are in \(\) with higher scores indicating better performance.

Due to the heavy computational requirements of closed-loop simulation, we evaluate on the navmini split. This is a new split we create for rapid testing, with 396 scenarios in total that are independent of both navtrain and navtest but filtered using the same strategy (Section 3.1) and hence similarly distributed. We note that nuPlan offers two kinds of background agents: reactive agents along lane centers based on the Intelligent Driver Model (IDM) , and non-reactive agents replayed from the dataset, which we employ unless otherwise stated. While reactive simulations of longer or dynamic lengths are generally desirable, e.g. to evaluate long-term decisions, enabling this requires dedicated solutions to long-horizon simulation that are not currently available in nuPlan . Therefore, we default to a fixed closed-loop simulation duration of \(d=15\)s, and a planning frequency of \(f=10\)Hz, which are the standard closed-loop simulation settings in nuPlan .

**Motion Planners.** Open-loop metrics favor learned planners while rule-based approaches perform well in closed-loop evaluation in nuPlan . We use a combination of both planner types in this experiment to cover different performance levels. In total, we include 37 rule-based planners with 2 constant velocity and 8 constant acceleration models, 15 IDM planners , and 12 PDM-Closed variants  which differ in hyperparameters for trajectory generation. For learned planning, we evaluate Urban Driver models  of 2 model sizes and 2 training lengths, and PlanCNN  models with 15 input combinations of the BEV raster, ego status, centerline, and navigation goal. We train all models on \(\{25\%,50\%,100\%\}\) of navtrain and an equally sized uniformly sampled subset of OpenScene, giving 114 learned planners. See the supplementary material for additional details.

**Results.** The alignment between metrics is presented in Fig. 3 (a-c). Compared to OLS, we consistently observe better closed-loop correlation for PDMS, in terms of Spearman's (rank) and Pearson's (linear) correlation coefficients. As shown in (a), PDMS can capture the closed-loop

Figure 3: **Closed-Loop Alignment. (a) For each planner, we show open-loop metrics (OLS, PDMS) together with the corresponding closed-loop score (CLS). The trendlines depicting correlations are fit linearly to all (learned and rule-based) planners. Moreover, we analyze different (b) CLS durations \(d\), (c) planning frequencies \(f\), (d) PDMS horizons \(h\), and (e) closed-loop background agent behaviors.**properties of both learned and rule-based planners, whereas distance-based open-loop metrics show a clear misalignment. Decreasing the CLS duration in (b) from \(d=15\)s to \(d=4\)s further raises the correlation of PDMS and OLS, as the simulation horizon more closely matches the open-loop counterparts. Interestingly, we observe a higher correlation of open-loop metrics in (c) when reducing the planning frequency to \(2\)Hz. We expect a lower planning frequency to mitigate cumulative errors and enhance the controller's stability in simulation, leading to more precise trajectory execution. Moreover, we observe an increase in correlation for longer PDMS horizons in (d), ranging from \(h=2\)s to \(h=8\)s. While predicting the future motion over 8s is challenging in uncertain scenarios, our results indicate the value of long horizons when evaluating motion planners. Lastly, replacing the non-reactive background agents with reactive IDM vehicles during closed-loop simulation in (c) has little effect on the correlation, possibly due to the similar difficulty of both tasks .

The imbalanced distribution of different types of planners in our study may introduce biases into the overall correlations presented in Fig. 3. To address this, we visualize the individual correlations of each planner type in Fig. 4. The correlation values vary depending on metric range and variance of each planner type. Nevertheless, when examining each type individually, the PDMS is better correlated to the CLS than the OLS, and is always positively correlated.

### Analysis of the State of the Art in End-to-End Autonomous Driving

In this section, we benchmark a collection of end-to-end architectures, which previously achieved state-of-the-art performance on existing open- or closed-loop benchmarks.

**Methods.** As a lower bound, we consider the **(1) Constant Velocity** baseline detailed in Section 3.1. We include an **(2) Ego Status MLP** as a second "blind" agent, which leverages an MLP for trajectory prediction given only the ego velocity, acceleration and navigation goal. As an established architecture on CARLA, we evaluate our reimplementation of **(3) TransFuser**, which uses three cropped and downscaled forward-facing cameras, concatenated into a \(1024 256\) image, and a rasterized BEV LiDAR input for predicting waypoints. It performs 3D object detection and BEV semantic segmentation as auxiliary tasks. We then consider **(4) Latent TransFuser (LTF)**, which shares

  
**Method** & **Ego Stat.** & **Image** & **LiDAR** & **Video** & **NC \(\)** & **DAC \(\)** & **TTC \(\)** & **Comf. \(\)** & **EP \(\)** & **PDMS \(\)** \\  Constant Velocity & ✓ & & & & 68.0 & 57.8 & 50.0 & 100 & 19.4 & 20.6 \\ Ego Status MLP & ✓ & & & & \(93.0\) & \(77.3\) & \(83.6\) & \(100\) & 62.8 & \(65.6\) \\  LTF  & ✓ & ✓ & & & \(97.4\) & \(\) & \(92.4\) & \(100\) & 79.0 & \(83.8\) \\ TransFuser  & ✓ & ✓ & ✓ & & \(97.7\) & \(\) & \(92.8\) & \(100\) & 79.2 & \(\) \\ UniAD  & ✓ & ✓ & & ✓ & \(97.8\) & \(91.9\) & \(92.9\) & \(100\) & 78.8 & \(83.4\) \\ PARA-Drive  & ✓ & ✓ & & ✓ & \(\) & \(92.4\) & \(\) & \(99.8\) & \(\) & \(\) \\  _Human_ & & & & & _100_ & _100_ & _100_ & _99.9_ & _87.5_ & _94.8_ \\   

Table 1: **Navtest Benchmark.** We show the no at-fault collision (NC), drivable area compliance (DAC), time-to-collision (TTC), comfort (Comf.), and ego progress (EP) subscores, and the PDM Score (PDMS), as percentages. Relying on the ego status is insufficient for competitive results. While sensor agents improve, the gap to human performance highlights our benchmark’s challenges.

Figure 4: **Planner-Level Alignment of Metrics.** We report the correlation coefficients between open-loop metrics (OLS, PDMS) and the closed-loop score (CLS) for the five planner types considered in our study. The PDMS is better correlated to the CLS for every planner type.

[MISSING_PAGE_FAIL:8]

concatenated images with a FOV of \(140^{}\). However, expanding the FOV with additional cameras does not result in substantially improved scores. Interestingly, restricting the LiDAR range to \(16\)m in all directions (D1), results in a score of \(79\), which is lower than dropping LiDAR altogether (see LTF in Table 1). Expanding the LiDAR range to \(64\)m in the forward direction (D2) or all directions (D3) does not provide significant improvements. We suspect that changes in the LiDAR range overly simplify or complicate the auxiliary 3D object detection and BEV semantic segmentation tasks, which operate in the LiDAR coordinate frame, hindering effective imitation learning. We check the impact of the auxiliary tasks by excluding them, where performance drops without BEV Segmentation (E1).

**CVPR 2024 NAVSIM Challenge.** We organized the inaugural NAVSIM challenge which ran from March - May 2024. To ensure integrity, we used a private dataset and only gave participants access to sensor inputs, withholding all annotations. Competitors could submit their agent's trajectories to our leaderboard, where they were simulated and scored to obtain the PDMS. We received 463 submissions from 143 teams, of which 78 submissions were made publicly visible. We summarize their scores in Fig. 5, relative to the constant velocity and TransFuser baselines from Table 1. The winning entry extended TransFuser and learned to predict proxy subscores for trajectory samples , with a sampling strategy inspired by VADv2 . These predicted subscores were weighted alongside a human imitation score to select the output plan. While the idea of sampling and scoring trajectories is well-known [46; 51; 55; 6; 16], it has recently been overlooked in favor of approaches which predict a single trajectory. This result prompts a reassessment of such methods. The team that placed second employed a vision language model (VLM) for driving, which is rapidly emerging as a sub-field in the AV literature [45; 33; 58; 33]. Several submissions attempted to reimplement or extend prior work on nuScenes such as UniAD  and VAD , but were unable to outperform the TransFuser baseline by the challenge submission deadline, given the significant engineering challenge and compute requirements. The diversity of the solutions on the leaderboard shows the potential of NAVSIM as a framework for pushing the frontiers of autonomous driving research. We aim to hold future competitions with more challenging data and metrics. Detailed competition results and statistics are provided in the supplementary material.

**NAVSIM 1.1 Leaderboard.** Due to the lasting interest after the challenge, we re-opened a public evaluation server using navtest as the evaluation split. The leaderboard encourages multi-seed submissions and includes reproducibility requirements for openly releasing code and model weights. We populated the leaderboard with 3 training seeds of our learned baselines, as shown in Table 3. For reference, we also include a single seed of the 2024 challenge winner  and constant velocity baseline. Further information is provided in the supplementary material and leaderboard webpage2.

## 5 Discussion

We present NAVSIM, a framework for non-reactive AV simulation. We address shortcomings of existing driving benchmarks and propose standardized but configurable simulation-based metrics for benchmarking driving policies. For accessibility, we provide challenging scenario splits and simple data curation methods. We show that our evaluation protocol is better aligned to closed-loop driving, benchmark an established set of end-to-end planning baselines from CARLA and nuScenes, and present the results of our inaugural competition. We hope that NAVSIM can serve as an accessible toolkit for AV researchers that bridges the gap between simulated and real-world driving.

**Need for Reactive Simulation.** While we show improvements over displacement errors, several aspects of driving remain unaddressed by evaluation in NAVSIM. A high PDMS does not always imply a high CLS, since our framework does not consider reactiveness or the compounding accumulation

  
**Method** & **PDMS**\(\) \\  TransFuser  & \(83.9 0.4\) \\ LTF  & \(83.5 0.6\) \\ Ego Status MLP & \(66.4 0.9\) \\  Hydra-MDP  & \(91.3\) \\ Constant Velocity & \(20.6\) \\   

Table 3: **Leaderboard 1.1.**

Figure 5: **NAVSIM Challenge.**

of errors in closed-loop simulation. Moreover, as in CLS, rear-end collisions into the ego vehicle are currently not classified as "at-fault", resulting in little importance given to the scene behind the vehicle in NAVSIM. In the future, data-driven sensor or traffic simulation could alleviate these issues, once such methods mature and become computationally tractable. Given these limitations of the current framework, we strongly encourage the use of graphics-based closed-loop simulators, such as CARLA , as complementary benchmarks to NAVSIM when developing planning algorithms.

**Simplicity of Metrics.** As a starting point, NAVSIM offers both interpretable open-loop subscores and a scalarizing function, which lets us provide a final score and ranking for participants in the challenge. In the future, multi-objective evaluation and other aggregation functions might be required. Moreover, closed-loop metrics also face problems, i.e., PDMS inherits several weaknesses of nuPlan's CLS. Both scores do not regard certain traffic rules (e.g., stop-sign or traffic light compliance) or concepts such as transit and fuel efficiency. In the future, we aim to improve the subscore definitions (e.g. the at-fault collision logic) and add more subscores during aggregation.

**Call for Datasets.** Certain limitations of the nuPlan dataset persist in NAVSIM, such as missing classes in the label space, minor errors in camera parameters, or noise in vehicle poses and 3D annotations. Our analysis might favor methods that are robust to such inconsistencies. In addition, the lack of road elevation data in our representation presents a challenge for integrating scenarios based on 3D map annotations. We aim to support more datasets in the future, and advocate for more open dataset releases by the community for accelerating progress in autonomous driving.