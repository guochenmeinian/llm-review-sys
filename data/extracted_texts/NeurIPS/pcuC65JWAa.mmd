# Reducing Blackwell and Average Optimality to Discounted MDPs via the Blackwell Discount Factor

###### Abstract

We introduce the _Blackwell discount factor_ for Markov Decision Processes (MDPs). Classical objectives for MDPs include discounted, average, and Blackwell optimality. Many existing approaches to computing average-optimal policies solve for discount-optimal policies with a discount factor close to \(1\), but they only work under strong or hard-to-verify assumptions such as unichain or ergodicity. We highlight the shortcomings of the classical definition of Blackwell optimality, which does not lead to simple algorithms for computing Blackwell-optimal policies and overlooks the pathological behaviors of optimal value functions with respect to the discount factors. To resolve this issue, we show that when the discount factor is larger than the _Blackwell discount factor_\(_{}\), all discount-optimal policies become Blackwell- and average-optimal, and we derive a general upper bound on \(_{}\). Our upper bound on \(_{}\), parametrized by the _bit-size_ of the rewards and transition probabilities of the MDP instance, provides the first reduction from average and Blackwell optimality to discounted optimality, _without any assumptions_, along with new polynomial-time algorithms. Our work brings new ideas from polynomials and algebraic numbers to the analysis of MDPs. Our results also apply to robust MDPs, enabling the first algorithms to compute robust Blackwell-optimal policies.**Julien Grand-Clement**

ISOM Department

HEC Paris

grand-clement@hec.fr

**Marek Petrik**

Department of Computer Science

University of New Hampshire

mpetrik@cs.unh.edu

## 1 Introduction

Markov Decision Processes (MDPs) provide a widely-used framework for modeling sequential decision-making problems (Puterman, 2014). In a (finite) MDP, the decision maker repeatedly interacts with an environment characterized by a finite set of states and a finite set of available actions. The decision maker follows a _policy_ that prescribes an action at a state at every period. An instantaneous reward is obtained at every period, depending on the current state-action pair, and the system transitions to the next state at the next period. MDPs provide the underlying model for the applications of reinforcement learning (RL), ranging from healthcare (Gottesman et al., 2019) to game solving (Mnih et al., 2013) and finance (Deng et al., 2016).

There are several optimality criteria that measure a decision maker's performance in an MDP. In _discounted optimality_, the decision maker optimizes the discounted return, defined as the sum of the instantaneous rewards over the infinite horizon, where future rewards are discounted with a _discount factor_\([0,1)\). In _average optimality_, the decision maker optimizes the average return, defined as the average of the instantaneous rewards obtained over the infinite horizon. The average return ignores any return gathered in finite time, i.e., it does not reflect the transient performance of a policy and it only focuses on the steady-state behavior. The most selective optimality criterion in MDPs is _Blackwell optimality_(Puterman, 2014). A policy is Blackwell-optimal if it optimizes the discounted return simultaneously for all discount factors sufficiently close to \(1\). Since a discount factor closeto \(1\) can be interpreted as a preference for rewards obtained in later periods, Blackwell-optimal policies are also average-optimal. However, average-optimal policies need not be Blackwell-optimal. Blackwell optimality can be a useful criterion in environments with no natural, or known, discount factor. Also, any algorithm that computes a Blackwell-optimal policy also immediately computes an average-optimal policy. This is one of the reasons why better understanding the Blackwell optimality criterion is mentioned as _"one of the pressing questions in RL"_ in the list of open research problems from a recent survey on RL for average reward optimality (Dewanto et al., 2020).

Average-optimal policies can be computed via linear programming (section 9.3, (Puterman, 2014)). However, virtually all of the recent algorithms for computing average-optimal policies require strong assumptions on the underlying Markov chains associated with the policies in the MDP instance, such as ergodicity (Wang, 2017), the unichain and aperiodicity properties (Schneckenreither, 2020), weakly communicating MDPs (Wang et al., 2022), or assumptions on the mixing time associated with any deterministic policies (Jin and Sidford, 2020, 2021). These assumptions are motivated by technical considerations (e.g., ensuring that the average reward is uniform across all states) and can be restrictive in practice (Puterman, 2014) and NP-hard to verify, such as unichain (Tsitsiklis, 2007). Existing methods for computing Blackwell-optimal policies rely on linear programming over the field of power series including negative coefficients (Hordijk et al., 1985), or on an algorithm based on a nested sequence of optimality equations (O'Sullivan and Veinott Jr, 2017) which requires to solve multiple linear programs sequentially. These algorithms are complex, difficult to implement, and have no complexity guarantees or known implementations.

In summary, existing algorithms for average optimality require restrictive assumptions, and algorithms for Blackwell-optimality are very complex. This is in stark contrast with the vast literature on solving discounted MDPs, where general and well-understood methods exist, including value iteration, policy iteration, and linear programming (chapter 6, (Puterman, 2014)). This is the starting point of this paper, which aims to develop new algorithms for computing average-optimal and Blackwell-optimal policies through a reduction to discounted MDPs. We make the following **three main contributions.**

_1) A new definition of Blackwell optimality via the Blackwell discount factor \(_{}[0,1)\)._ Our first main contribution is to highlight that the standard definition of Blackwell optimality cannot be used to compute Blackwell-optimal policies with simple algorithms. Standard definitions have focused on _necessary_ condition for Blackwell optimal policies to be discount optimal. However, we show that this condition needs to be revised when one seeks to compute a Blackwell-optimal policy. We do so by highlighting the potential pathological behaviors of the value functions: a Blackwell-optimal policy may be optimal on an arbitrary number of arbitrary disjoint intervals, and other non-Blackwell optimal policies may also be discount-optimal for some discount factors very close to \(1\). Demonstrating this issue is important because previous literature has repeatedly overlooked it. To address this issue, we introduce and show the existence of a discount factor \(_{}\) such that discount optimality for \(>_{}\) is _sufficient_ for Blackwell optimality. Knowing the discount factor \(_{}\) is vital because it enables one to compute Blackwell- and average-optimal policies simply by solving a discounted MDP with \((_{},1)\), for which there exist well-studied, simple, and efficient algorithms.

_2) Upper-bound the Blackwell discount factor._ As our second main contribution, we provide a strict upper bound on \(_{}\) given an MDP instance. We show that an upper bound must depend on \(\) and \(\), and we compute a bound that is parametrized by the number of states and the number of bits required to represent the MDP instance. Solving a discounted MDP with a discount factor larger or equal than our strict upper bound returns a Blackwell-optimal policy. Crucially, _our strict upper bound requires no assumptions on the underlying structure of the MDP_, which is a significant improvement on existing literature. Interestingly, the construction of our upper bound relies on novel techniques for analyzing MDPs. We interpret \(_{}[0,1)\) as the root of a polynomial equation \(p()=0\) in \(\), show \(p(1)=0\), and use a lower bound \((p)\) on the distance between any two roots of a polynomial \(p\), known as the _separation of algebraic numbers_. This shows that \(_{}<1-(p)\), where \((p)\) depends on the MDP instance. Since Blackwell optimality implies average optimality, we also obtain the first reduction from average optimality to discounted optimality, _without any assumption_ on the MDP structure. Our upper bound on \(_{}\) is itself of polynomial size in the bit-size of the MDP data. Combining this bound with interior-point methods for solving discounted MDPs, we obtain new weakly-polynomial time algorithms for computing Blackwell-optimal and average-optimal policies.

_3) Blackwell discount factor for robust MDPs._ We consider the case of robust reinforcement learning where the transition probabilities are unknown and, instead, belong to an uncertainty set. As our third main contribution, we show that the robust Blackwell discount factor \(_{,r}\) exists for popular models of uncertainty, such as sa-rectangular robust MDPs with polyhedral uncertainty (Goyal and Grand-Clement, 2023b, Iyengar, 2005). For this setting, we generalize our upper bound on \(_{}\) for MDPs to an upper bound on \(_{,r}\) for robust MDPs. Since robust MDPs with discounted optimality can be solved via value iteration and policy iteration, we provide the very first algorithms to compute Blackwell-optimal policies for robust MDPs.

We conclude this section with a discussion on **related works**. Several papers study the reduction of average optimality policy to discounted optimality under strong assumptions. Early attempts include (Ross, 1968), assuming that all transition probabilities are lower bounded by \(>0\). Recent extensions assume bounded times of first returns (Akian and Gaubert, 2013, Huang, 2016), or weakly-communicating MDPs (Wang et al., 2022). Note that checking that an MDP instance is weakly-communicating can be done in polynomial-time (Kallenberg, 2002), in contrast to the unichain assumption (Tsitsiklis, 2007). The case of deterministic MDPs is treated in (Friedmann, 2011, Perotto and Vercouter, 2018, Zwick and Paterson, 1996). Other reductions require assumptions on the mixing times of the Markov chains induced by deterministic policies (Jin and Sidford, 2021). (Boone and Gaujal, 2022) propose a sampling algorithm to learn a Blackwell-optimal policy, in a special case in which it reduces to bias optimality. Under the condition that the robust MDP is unichain and that there is a unique average optimal policy, (Wang et al., 2023) show the existence of Blackwell-optimal policies for sa-rectangular robust MDPs, which is connected to the existence results in (Tewari and Bartlett, 2007) and (Goyal and Grand-Clement, 2023b) for polyhedral uncertainty. In contrast to the existing literature, one of the core strengths of our results is that we do not need any structural assumption on the Markov chains of the underlying MDP to obtain our reduction from Blackwell optimality and average optimality to discounted optimality.

## 2 Preliminaries on MDPs

An MDP instance is characterized by a tuple \(=(,,,)\), where \(\) is a finite set of states and \(\) is a finite set of actions. The instantaneous rewards are denoted by \(^{}\) and the transition probabilities are denoted by \((())^{}\), where \(()\) is the simplex over \(\). At any time period \(t\), the decision maker is in a state \(s_{t}\), chooses an action \(a_{t}\), obtains an instantaneous reward \(r_{s_{t}a_{t}}\), and transitions to state \(s_{t+1}\) with probability \(P_{s_{t}a_{t}s_{t+1}}\). A _deterministic stationary_ policy \(\) assigns an action to each state. Importantly, there exists an optimal deterministic stationary policy for all the criteria considered in this paper (discounted, Blackwell, and average optimality) (Puterman, 2014), so we simply refer to them as _policies_ and denote them as \(=^{}\). A policy \(\) induces a vector of expected instantaneous reward \(_{}^{}\), defined as \(r_{,s}=r_{s(s)},\ s\), as well as a Markov chain over \(\), evolving via a transition matrix \(_{}^{}\), defined as \(P_{,ss^{}}=P_{s(s)s^{}},\ s,s^{}\). We also write \(r_{}=\{|r_{sa}|\ (s,a) \}\).

Given a discount factor \([0,1)\) and a policy \(\), the _value function_\(_{}^{}^{}\) represents the discounted value obtained starting from each state: \(v_{,s}^{}=^{,}[_{t=0}^{+}^{t }r_{s_{t},a_{t}}\ \ s_{0}=s],\ s\). We start with discounted optimality, the most popular optimality criterion in RL.

**Definition 2.1**.: Given \([0,1)\), a policy \(\) is \(\)-discount-optimal if \(v_{,s}^{} v_{,s}^{^{}},\ ^{},\ s\). We call \(_{}^{}\) the set of \(\)-discount-optimal policies.

The discount factor \([0,1)\) represents the preference for current rewards compared to future rewards. The difficulty of choosing the discount factor \(\) is well recognized in RL (Tang et al., 2021). In some applications, it is reasonable to choose values of \(\) close to \(1\), e.g., in finance (Deng et al., 2016), in healthcare (Garcia et al., 2021, Neumann et al., 2016) or in game solving (Brockman et al., 2016). In other applications, \(\) is merely treated as a parameter introduced for algorithmic purposes, e.g., controlling the variance of the policy gradient estimates (Baxter and Bartlett, 2001), or ensuring convergence of algorithms. A discount-optimal policy can be computed efficiently with value iteration, policy iteration, and linear programming (Puterman, 2014). Notably, these algorithms do not require any assumptions on the MDP instance \(\).

Another fundamental optimality criterion is _average optimality_, where the average reward \(^{}^{}\) of a policy \(\) is \(g_{s}^{}=_{T+}^{,}[_{t= 0}^{T}r_{s_{t},a_{t}}\ \ s_{0}=s],\ s\). This limit always exists for stationary policies (Puterman, 2014). A policy \(\) is average-optimal if \(^{}^{^{}}\), \(~{}^{}\). Average optimality has been extensively studied in the RL literature, as it alleviates the introduction of a potentially artificial discount factor. Classical algorithms include relative value iteration (Dong et al., 2019; Yang et al., 2016), and gradient-based methods (Bhatnagar et al., 2007; Iwaki and Asada, 2019). We refer to (Dewanto et al., 2020) for a survey on average optimality in RL.

Several technical complications arise from considering average optimality instead of discounted optimality. First, the average reward \(^{}\) of a policy is not a continuous function of the policy \(\) (e.g., chapter 4, (Feinberg and Shwartz, 2012)). This can make gradient-based methods inefficient, since a small change in the policy may result in drastic changes in the average reward. Additionally, the Bellman operator associated with the average optimality criterion is not a contraction and may have multiple fixed points. These complications can be circumvented by assuming structural properties on the MDP instance, such as bounded times of first returns and weakly-communicating MDPs (Akian and Gaubert, 2013; Wang et al., 2022). Some of these assumptions may be hard to verify in a simulation environment where only samples are available, or NP-hard to verify even when the MDP instance is fully known, as is the case for the unichain assumption (Tsitsiklis, 2007). One of our goals in this paper is to provide a method to compute average-optimal policies via solving discounted MDPs, _without any restrictive structural assumptions on the MDP instance_. We will do so via the notion of _Blackwell optimality_.

## 3 Classical theory of Blackwell optimality

In this section, we describe the classical definition of Blackwell optimality in MDPs and summarize its main limitations. We first give this definition of a Blackwell-optimal policy and outline the proof of its existence. This proof will serve as a building block of our main result in Section 4. We then highlight the main limitations of the existing definition of Blackwell optimality.

**Existing definition and algorithms.** We start with the following classical definition.

**Definition 3.1**.: A policy \(\) is _Blackwell-optimal_ if there exists \([0,1)\), such that \(_{^{}}^{},~{}~{}^{}[,1)\). We call \(_{^{}}^{}\) the set of Blackwell-optimal policies.

In short, a Blackwell-optimal policy is \(\)-discount-optimal for all discount factors \(\) sufficiently close to \(1\)(Blackwell, 1962). This notion has become popular in the field of reinforcement learning, mainly due to its connection to average optimality (Dewanto and Gallagher, 2021). Blackwell optimality bridges the gap between the different optimality criteria: it is defined in terms of discounted optimality, yet, crucially, Blackwell-optimal policies are average-optimal (theorem 10.1.5, (Puterman, 2014)). Therefore, any advances in computing Blackwell-optimal policies transfer to advances in computing average-optimal policies. A Blackwell-optimal policy is guaranteed to exist for finite MDPs.

**Theorem 3.2** ((Blackwell, 1962)).: _When \(||<+,||<+\), there exists at least one Blackwell-optimal policy: \(_{^{}}^{}\)._

We highlight the proof of Theorem 3.2 based on section 10.1.1 in (Puterman, 2014). Summarizing this proof is important because it is not well-known and serves as a building block for our results.

_Step 1._ Let \(,^{},s\). Through this paper use the notation \(_{s}^{,^{}}\) for \(_{s}^{,^{}}: v_{,s}^{}-v_{,s}^{ ^{}}\). We first show that \(_{s}^{,^{}}\) has finitely many zeros in \([0,1)\). This is a consequence of the next lemma.

**Lemma 3.3**.: _For \(\) and \(s\), \( v_{,s}^{}\) is a rational function on \([0,1)\), i.e., it is the ratio of two polynomials._

Lemma 3.3 follows from the Bellman equation for the value function \(^{}\): \(^{}=_{}+_{}^{}\). Therefore, \(^{}\) is the unique solution to the equation \(=\), for \(=_{}\) and \(=-_{}\). Lemma 3.3 then follows directly from Cramer's rule for the solution of a system of linear equations: since \(\) is invertible, then \(=\) has a unique solution \(\), which satisfies \(x_{s}=(_{s})/(),~{}s\), with \(()\) the determinant of a matrix and \(_{s}\) the matrix formed by replacing the \(s\)-th column of \(\) by the vector \(\). A consequence of Lemma 3.3 is that the function \(_{s}^{,^{}}\) is a rational function, and therefore its zeros are the zeros of a polynomial. This shows that \(_{s}^{,^{}}\) is either identically equal to \(0\), or it has only has finitely many roots in \([0,1)\).

_Step 2._ We now conclude the proof of Theorem 3.2. Let \(,^{},s\). If \(_{s}^{,^{}}\) is not identically equal to \(0\), let \((,^{},s)[0,1)\) be its the largest zero of \(_{s}^{,^{}}\) in \([0,1)\): \((,^{},s)=\{[0,1)|v_{,s}^{}-v_{,s }^{^{}}=0\}\). We let \((,^{},s)=0\) if \(_{s}^{,^{}}\) is identically equal to \(0\) in \([0,1)\). We now let

\[=_{,^{},s}(,^{ },s).\] (3.1)

We have \(<1\) since there is a finite number of (stationary, deterministic) policies and \(||<+\). Let \(\) be \(\)-discount-optimal for a certain \(>\). We have, for any \(s,v_{,s}^{} v_{,s}^{^{}}\), \(\;^{}\). By definition of \(\), the map \(_{s}^{,^{}}\) cannot change a sign on \([,1)\) (because it cannot be equal to \(0\)), for any policy \(^{}\) and any state \(s\), i.e., we have \(v_{,s}^{^{}} v_{,s}^{^{}},\;^{ },\;^{}(,1)\). This shows that \(\) remains \(^{}\)-discount-optimal for all \(^{}>\), and, therefore, \(\) is Blackwell-optimal.

**Remark 3.4**.: At this point, the reader may wonder if some Blackwell optimal policies are "better" than others, e.g., for instance, if we can find a Blackwell optimal policy that is \(\)-discount optimal for \(\) as small as possible. Interestingly, all Blackwell optimal policies are \(\)-discount optimal (or not) for the same discount factors. This follows from the key property that the value functions of Blackwell optimal policies coincide for all \((0,1)\) at all states \(s\). Indeed, these value functions must coincide on an entire interval close enough to \(1\), and they are rational functions. Hence, if they are equal for an infinite number of discount factors, they are equal on the entire interval \((0,1)\).

To the best of our knowledge, there are only two **existing algorithms** to compute a Blackwell-optimal policy. The first algorithm (Hordijk et al., 1985) formulates MDPs with varying discount factors as linear programs (LPs) over the field of power series with potentially negative coefficients, known as Laurent series. The simplex method for solving LPs over power series explores \([0,1)\) and computes the subintervals of \([0,1)\) where an optimal policy can be chosen constant (as a function of \(\)). It returns a Blackwell-optimal policy in a finite number of operations, but there are no complexity guarantees for this algorithm. The second algorithm is based on \(n\)-discount optimality, described with a set of \((||+1)\)-nested equations indexed by \(n=-1,...,||-1\) that need to be solved sequentially by solving three LPs at each stage \(n\)(O'Sullivan and Veinot Jr, 2017). This gives a polynomial-time algorithm for computing Blackwell-optimal policies, requiring solving \(3(||+1)\) linear programs of dimension \(O(||)\). A simpler description is in section 10.3.4 in (Puterman, 2014), but only finite convergence is proved. We are not aware of any available implementations of these algorithms.

**Limitations of existing approaches.** We now highlight the shortcomings of the existing definition of Blackwell optimality. In particular, we demonstrate that the current approach is insufficient to reduce Blackwell optimality to discount optimality, we show that it does not lead to simple algorithms, and we show that it completely overlooks the potential pathological behaviors of the value functions.

First, Definition 3.1 leads to methods that are significantly more involved than solving discounted MDPs. The two existing algorithms for computing Blackwell-optimal policies handle complex objects, e.g., the simplex algorithm over the field of power series and nested optimality equations with multiple subproblems that need to be solved sequentially. The intricacy of both algorithms makes them difficult to implement, and these algorithms are not widely used in practice.

Second, Definition 3.1 implicitly introduces, for each Blackwell-optimal policy \(_{}^{}\), a discount factor \(()[0,1)\), defined as the smallest discount factor after which \(\) remains discount-optimal:

\[()=\{[0,1)_{^{}}^{}, \;^{}[,1)\}.\] (3.2)

We now show that \(()\) provides insufficient information to compute a Blackwell-optimal policy.

**Proposition 3.5**.: _There exists an MDP instance \(\), a Blackwell-optimal policy \(_{}^{}\), and discount factors \(_{1},_{2}[0,1)\) with \(_{1}<()<_{2}\) such that:_

1. _the policy_ \(\) _is_ \(_{1}\)_-discount-optimal, and_
2. _there exists_ \(^{}\) _that is_ \(_{2}\)_-discount-optimal and_ not _Blackwell-optimal._

Proposition 3.5 shows the naive approach of solving a \(\)-discounted MDP for discount factor \(>()\) does not compute a Blackwell-optimal policy. That is, the policy \(^{}\) in Proposition 3.5 is optimal for \(_{2}>()\) but is not Blackwell-optimal. It also shows that \(()\) is not even the smallest discount factor for which \(\) is discount-optimal. Note that we are the first to highlight this shortcoming of the classical definition of Blackwell optimality. We also note that Proposition 3.5 remains true even under the assumption that MDP instance is unichain, as we prove in Appendix A.Overall, we have shown that the discount factor \(()\), appearing in the classical definition of Blackwell optimality, cannot be exploited to compute a Blackwell-optimal policy.

The limitation outlined above calls for the definition of another discount factor that can adequately describe when does the set of discount-optimal policies equals to the set of Blackwell optimal policies. We introduce this _Blackwell discount factor_ in the next section. The proof of Proposition 3.5 is based on the following very simple example, with \(||=8,||=3\), and deterministic transitions.

**Example 3.6**.: _We consider the MDP instance from Figure 1. The decision maker starts in state \(0\) and chooses one of three actions \(\{a_{1},a_{2},a_{3}\}\); there is no choice in other states, all transitions are deterministic, and the rewards are indicated above the transition arcs. The reward for \(a_{1}\) is \(1\) and the process transitions to the absorbing state \(7\), which gives a reward of \(0\). The reward for \(a_{2}\) is \(0\), and the process transitions to states \(1,2,3\) before reaching the absorbing state \(7\). The value functions equal to \(v_{}^{a_{2}}=r_{1}+r_{2}^{2}\), \(v_{}^{a_{3}}=r_{4}+r_{5}^{2}\), \(v_{}^{a_{1}}=1\). Choosing \((r_{1},r_{2})=(6,-8)\) and \((r_{4},r_{5})=(8/3,-16/9)\) gives the value functions shown in Figure 1 (left figure). In particular, \(v_{}^{a_{2}}\) is the parabola that is equal to \(0\) at \(=0\), and equal to \(1\) at \(\{1/4,1/2\}\), and \(v_{}^{a_{3}}\) is the parabola that is equal to \(0\) at \(=0\) and equal to its maximum \(1\) at \(=3/4\). This shows that \(a_{1}\) is Blackwell-optimal with \((a_{1})=1/2\). Additionally, for \(_{1}[0,1/4]\), \(a_{1}\) is \(_{1}\)-discount-optimal. Finally, \(a_{3}\) is \(_{2}\)-discount-optimal for \(_{2}=3/4\), but it is not Blackwell-optimal._

In the next proposition, we show that the subintervals of \([0,1)\) where a policy is discount-optimal may be much more complex than usually alluded to in the literature. In particular, there exists a simple MDP instance with only two policies, but where a Blackwell-optimal policy may be discount-optimal in an _arbitrary_ number of _arbitrary_ disjoint subintervals of \([0,1)\).

**Theorem 3.7**.: _For any odd integer \(N\) and any sequence \(0=_{0}<_{1}<...<_{N-1}<_{N}=1\), there exists an MDP instance \((,,,)\) with \(||=N+1\) and \(||=2\), and two policies \(_{1},_{2}\) such that \(_{1}\) is the unique optimal policy on any of the intervals \((_{2i},_{2i+1})\) for \(i=0,...,(N-1)/2\) and \(_{2}\) is the unique optimal policy on \((_{2i-1},_{2i})\), for \(i=1,...,(N-1)/2\)._

Theorem 3.7 shows that the algorithm that explore the entire interval of \((0,1)\) to compute discount-optimal policies (Hordijk et al., 1985) may visit a number of subintervals that is impractical. We present a detailed proof in Appendix B. The proof relies on interpreting value functions as polynomials and using Lagrange interpolation polynomials to tune the instantaneous rewards to ensure that the value functions intersect at the given discount factors. Overall, our results in this section highlight the pitfalls of the existing approach to Blackwell optimality and the potential pathological behaviors of the value functions, even in simple MDP instances. We ameliorate this issue in the next section.

## 4 Introducing the Blackwell discount factor

This section introduces the notion of the _Blackwell discount factor_, which we use to reduce Blackwell optimality and average optimality to discounted optimality. This reduction leads to algorithms to compute Blackwell-optimal and average policies that are significantly simpler than the state-of-the-art. Intuitively, we need the following condition to reduce Blackwell optimality to discounted optimality: there must exist a discount factor \(_{}[0,1)\) such that any \(\)-discount-optimal policy for \(>_{}\) is also \(^{}\)-discount-optimal for any other \(^{}>_{}\). The following definition formalizes this intuition.

Figure 1: MDP instance (left) and value functions (right) for Example 3.6.

**Definition 4.1**.: The Blackwell discount factor \(_{}[0,1)\) is equal to \(_{}=\{[0,1)^{}_{^{}}=^{ }_{},\ ^{}(,1)\},\) where \(^{}_{}\) is the set of Blackwell-optimal policies.

We establish the existence of a Blackwell discount factor in the next theorem.

**Theorem 4.2**.: _The Blackwell discount factor \(_{}\) in Definition 4.1 exists in any finite MDP._

Proof.: We show that there exists a discount factor \([0,1)\) such that \(^{}_{^{}}=^{}_{},\ ^{}(,1)\). Let \(\) defined as in Equation (3.1). We show \(\ [,1),^{}_{}=^{}_{}\). Let \(^{}(,1)\) and let \(\) be a policy that is \(^{}\)-discount-optimal. By definition, we have \(v^{}_{^{},s} v^{}_{^{},s},\ ^{},\ s\). Since \(^{}>\), the map \(^{,^{}}_{s}\) does not change sign on \([,1)\). This shows that \(\) is \(\)-discount-optimal for all \((,1)\). Therefore, \(\) is Blackwell optimal, and any \(\)-discount-optimal policy is Blackwell optimal, for any \((,1)\), i.e., this shows \(^{}_{}^{}_{}\). The inclusion \(^{}_{}^{}_{}\) follows from the definition of \(\): if \(\) is Blackwell-optimal but not discount-optimal for \(\), then it must become discount-optimal for a larger \(^{}>\), which is impossible since \(\) is the largest discount factors where the value functions of any two stationary policies can intersect. 

**Difference from the existing definition.** It is important to elaborate on the difference between Definition 3.1 (classical definition of Blackwell optimality) and Definition 4.1 (Blackwell discount factor). While the proof for the existence of \(_{}\) is relatively concise, the distinction between \(_{}\) and \(()\) has been utterly overlooked in the literature, where it is common to find statements that suggest that \(>()\) implies Blackwell optimality of all discount-optimal policies, e.g. in Dewanto and Gallagher (2021), Wang et al. (2023). To the best of our knowledge, we are the first to properly introduce the Blackwell discount factor \(_{}\), to show its sufficiency to compute Blackwell-optimal policies, to emphasize the shortcomings of the classical approach to Blackwell optimality, and to clarify the distinction between \(_{}\) and \(()\). In particular, in Definition 3.1, a Blackwell-optimal policy \(\) is optimal for any \([(),1)\). However, for some \([(),1)\), there may be other optimal policies that are not Blackwell-optimal, as shown in Proposition 3.5. We show an MDP instance like this in Example 3.6, where \(_{}=3/4\) but where \((a_{1})=1/2\), and \(a_{1}\) is the only Blackwell-optimal policy. Hence in all generality, we may have \(()<_{}\), and \(()_{}\). Note that the authors in (Dewanto and Gallagher, 2021, Dewanto et al., 2020) also introduce the notation "\(_{}\)" but they use it to denote \(()\).

**Reduction to discounted optimality.** If \(_{}\) is known for a given MDP instance, it is straightforward to compute a Blackwell-optimal policy, by solving a discounted MDP with \(>_{}\). Therefore, the notion of Blackwell discount factor provides a method to reduce the criteria of Blackwell optimality and average optimality to the well-studied criterion of discounted optimality. As we have discussed before, efficient methods for solving discounted MDPs such as value iteration or linear programming have been extensively studied. These algorithms are much simpler than the two existing algorithms for computing Blackwell-optimal policies. Note that it is enough to compute an upper bound on \(_{}\). In particular, if we are able to show that \(_{}<^{}\) for some \(^{}[0,1)\), then following the definition of \(_{}\), we can compute a Blackwell-optimal policy by solving a discounted MDP with a discount factor \(=^{}\). Therefore, in the rest of Section 4, we focus on obtaining an upper bound on \(_{}\).

### Upper bound on \(_{}\)

We now obtain an instance-dependent upper bound on \(_{}\), i.e., we construct a scalar \(()(0,1)\) for each MDP instance \(=(,,,)\), such that \(_{}<1-()\). Our main contribution in this section is Theorem 4.4, which gives a closed-form expression for \(()\) as a function of the _maximum bit-size_ of the data of the MDP instance \(\). We start by showing that it is impossible to obtain a bound on \(_{}\) that is independent of \(\) or \(\).

**Proposition 4.3**.: _For any \(>0\), there exists an MDP instance \(=(,,,)\) with \(||=2,||=2\) and deterministic transitions, such that \(_{}>1-\)._

Proof.: Let \(=\{s_{1},s_{2}\},=\{a_{1},a_{2}\}\). In state \(s_{1}\), action \(a_{1}\) transitions to \(s_{1}\) (with reward \(0\)) and action \(a_{2}\) transitions to \(s_{2}\) (with reward \(-1\)). There is no action to choose in state \(s_{2}\) which is absorbing with a reward \(>0\). It is straightforward to check that \(a_{2}\) is Blackwell optimal, with \(_{}=(1+)^{-1}\), so that \(_{}\) can be chosen arbitrarily close to \(1\) by choosing small values for \(\)We show that Proposition 4.3 still holds even under the assumption that the MDP instances are weakly-communicating in Appendix C. Proposition 4.3 shows that an instance-dependent bound on \(_{}\)_must_ depend on the "coarseness" of \(\) and \(\). This suggests parametrizing our upper bound by the _bit-sizes_ of the MDP instance. MDPs with finite bit-sizes parameters are the MDP instances that can be exactly encoded in a computer and practically solved by existing algorithms. We first recall the definitions pertaining to bit-size, necessary to describe the complexity of classical weakly-polynomial time algorithms like interior-point methods (section 4.6 in (Ben-Tal and Nemirovski, 2001)) and the ellipsoid method (Bland et al., 1981). The bit-size of \(r\) is \(_{2}(r)\), the number of bits necessary to represent \(r\) with standard binary encoding. The bit-size of a rational number is the sum of the bit-size of its numerator and its denominator. The maximum bit-size of an MDP instance is the maximum bit-size of any \(r_{sa}\) and \(P_{sas^{}}\) for \((s,a,s^{})\). Its total bit-size is the sum of the bit-sizes of the components of \(\) and \(\). For instance, in the riverswim instance, the maximum bit-size of the reward is \(14\), since the largest rewards are bounded by \(10^{4}\) in the terminal states. Our main theorem in this section provides a strict upper bound on \(_{}\) as follows.

**Theorem 4.4**.: _Let \(=(,,,)\) be an MDP instance with finite bit-size and let \(m\) be the maximum bit-size of the instance \(\). Then we have \(_{}<1-()\), with \(()(0,1)\) defined as_

\[()=(L+1)^{N}},N=2||-1,L=2|| r_{} m^{2||} 4^{| |}.\]

Our proof uses ideas that are new in the MDP literature, such as the separation of algebraic numbers. We provide an outline of the proof below and defer the full statement to Appendix D.

In the first step of the proof, by carefully inspecting the proofs of Theorem 3.2 and of Theorem 4.2, we note that an upper bound for \(_{}\) is \(\), as defined in (3.1): \(=_{,^{},s}(,^{ },s),\) where for \(,^{}\) and \(s\), \((,^{},s)\) is the largest discount factor \(\) in \([0,1)\) for which \(_{s}^{,^{}}()=0\) when \(_{s}^{,^{}}: v_{,s}^{}-v_{,s}^{}\) is not identically equal to \(0\), and \(0\) otherwise. Therefore, we focus on obtaining an upper bound on \((,^{},s)\) for any two policies \(,^{}\) and any state \(s\).

In the second step, following Lemma 3.3, the value functions \( v_{s}^{}, v_{s}^{^{}}\) are rational functions, i.e., they are ratios of two polynomials. Therefore, we interpret \(_{s}^{,^{}}()=0\) as a polynomial equation in \(\), i.e., as \(p()=0\) for a certain polynomial \(p\). With this notation, \((,^{},s)[0,1)\) is a root of \(p\). We show that \(=1\) is always a root of \(p\), even though value functions are a priori not defined for \(=1\). We then precisely characterize the degree \(N\) and the sum \(L\) of the absolute values of the coefficients of the polynomial \(p\), depending on the MDP instance \(\).

**Theorem 4.5**.: _The polynomial \(p\) has degree \(N=2||-1\). Moreover, \(m^{2||}p\) has integral coefficients. The sum of the absolute values of the coefficients of \(m^{2||}p\) is bounded by \(L=2|| r_{} m^{2||} 4^{| |}\)._

In the third step, we lower-bound the distance between any two distinct roots of \(p\). To do this, we rely on the following _separation bounds of algebraic numbers_.

**Theorem 4.6** ((Rump, 1979)).: _Let \(p\) be a polynomial of degree \(N\) with integer coefficients. Let \(L\) be the sum of the absolute values of its coefficients. The distance between any two distinct roots of \(p\) is strictly larger than \(>0\), with \(=2N^{-N/2+2}(L+1)^{-N}\)._

Recall that \((,^{},s)\) and \(1\) are two always roots of \(p\), with \((,^{},s)<1\). Combining Theorem 4.5 with Theorem 4.6, we conclude that \((,^{},s)<1-()\) for \(()>0\) defined as in Theorem 4.4. Therefore, \(<1-(),\) and \(_{}<1-()\). This concludes our proof of Theorem 4.4.

**Discussion.** Using Theorem 4.4, we obtain the first reduction from Blackwell optimality to discounted optimality: solving a discounted MDP with \( 1-()\) returns a Blackwell-optimal policy. Blackwell optimality implies average optimality, so we also obtain the first reduction from average optimality to discounted optimality _without any assumptions on the structure of the underlying Markov chains of the MDP._ We also discuss the **complexity results** for computing a Blackwell-optimal policy using our reduction. Policy iteration returns a discounted optimal policy in \(O(|^{2}||}{1-}())\) iterations (Scherrer, 2013), but it may be slow to converge when \(=1-()\) as in Theorem 4.4, since \(()\) may be close to \(0\). Various algorithms exist to obtain convergence faster than \(O(1/(1-))\), such as accelerated value iteration (Goyal and Grand-Clement, 2023) and Anderson acceleration (Zhang et al., 2020). However, note that \(_{2}(())\), the bit-size of the scalar \(()\)is polynomial in the bit-size of the MDP instance \(\). Since discounted MDPs can be formulated as linear programs, which can be solved in polynomial-time in the input size of the MDP (Ye, 2011), we obtain a weakly-polynomial time algorithm for computing Blackwell-optimal policies. We present the proof of the following theorem in Appendix E.

**Theorem 4.7**.: _Let \(=(,,,)\) be an MDP instance with total bit-size \(Q(,)\). Then we can compute a Blackwell-optimal policy in \(O(||^{5}||^{2}Q(,))\) arithmetic operations._

Note that with Theorem 4.4 and Theorem 4.7, we have reduced the complex problem of computing a Blackwell optimal policy to a much simpler and well-studied problem: solving a linear program, which can be done in weakly-polynomial time. Potential improvements for our upper bound on \(_{}\) are an important future direction: more precise separation bounds than Theorem 4.6 could be obtained for the specific polynomial \(p\) appearing in the proof of Theorem 4.4, or for a specific MDP instances, e.g. ergodic or unichain MDPs. Going beyond the case of finite sets of states and actions is interesting but this may be difficult, as in both cases there may not exist a Blackwell optimal policy anymore (Mitta, 1965; Chitashvili, 1976).

### The case of robust MDPs

In practice, the value function \(_{}^{}\) may be very sensitive to the values of the transition probabilities \(\). To emphasize this dependence, in this section we note \(_{}^{,}\) for the value function associated with a policy \(\) and a transition probability \(\), defined similarly as in Section 2. Robust MDPs (RMDPs) ameliorate this issue by considering an _uncertainty set_\(\), which can be seen as a plausible region for the transition probabilities \(\). We focus on the case of sa-rectangular MDPs (Iyengar, 2005), where \(=_{(s,a)}_{sa}\) for \(_{sa}()\). The worst-case value function \(_{}^{,}^{}\) of a policy \(\) is defined as \(v_{,s}^{,}=_{}v_{,s}^{, },\,\,s\). In discounted RMDPs, the goal is to compute a _robust discounted optimal_ policy, defined as follows.

**Definition 4.8**.: Given \([0,1)\), a policy \(\) is robust \(\)-discount-optimal if \(v_{,s}^{,} v_{,s}^{^{},}, \,\,^{},\,\,s\). We write \(_{,}^{*}\) the set of robust \(\)-discount-optimal policies.

Robust Blackwell optimality is studied in (Goyal and Grand-Clement, 2023b; Tewari and Bartlett, 2007), to address the sensitivity of the robust value functions as regards the discount factors. Its connection to average reward RMDPs is discussed in (Tewari and Bartlett, 2007; Wang et al., 2023).

**Definition 4.9**.: A policy \(\) is _robust Blackwell-optimal_ if there exists \([0,1)\), such that \(_{^{},}^{*},\,\,^{}[,1)\). We call \(_{,}^{*}\) the set of robust Blackwell-optimal policies.

(Goyal and Grand-Clement, 2023b) shows the existence of a Blackwell-optimal policy for RMDPs, under the condition that \(\) is sa-rectangular and has finitely many extreme points. This is the case for popular polyhedral uncertainty sets, e.g., when \(_{sa}\) is based on the \(_{p}\) distance, for \(p\{1,\}\)(Givan et al., 1997; Ho et al., 2018; Iyengar, 2005), for some estimated kernel \(^{0}\) and some radius \(_{sa}>0\):

\[_{sa}=\{()\|-_{sa}^{0}\| _{p}_{sa}\}.\] (4.1)

**Definition 4.10**.: We define the robust Blackwell discount factor \(_{,}[0,1)\) as \(_{,}=\{[0,1)_{^{},}^{*}=_{,}^{*},^{}(,1)\}\).

We provide a detailed proof of the existence of the robust Blackwell discount factor in Appendix F. The proof strategy is the same as for the existence of the Blackwell discount factor for MDPs. We can obtain the same upper bound on \(_{,}\), by studying the values of \(\) for which \( v_{,s}^{,}-v_{,s}^{^{},^{ }}\) cancels, for any two policies \(,^{}\) and any two extreme points \(,^{}\) of \(\). Writing \((,^{},s,,^{})\) for the largest zero in \([0,1)\) of the function \( v_{,s}^{,}-v_{,s}^{^{},^{ }}\) if it is not identically equal to zero, or \((,^{},s,,^{})=0\) otherwise, an upper bound on \(_{,}\) for RMDPs can be computed as \(_{}\), defined as \(_{}=_{,^{},s}_{ {P},^{}_{}}(,^{},s,,^{})\) with \(_{}\) the set of extreme points of \(\). This leads to the following theorem.

**Theorem 4.11**.: _Let \(=(,,,^{0})\) be an MDP instance with maximum bit-size \(m\). Assume that \(\) is sa-rectangular, where for each \((s,a)\), \(_{sa}\) is constructed as in (4.1) based on \(_{1}\) or \(_{}\) distance, and with the scalars \((_{sa})_{s,a}\) of maximum bit-size \(m\). Then \(_{,} 1-()\), with \(()\) defined as in Theorem 4.4 with \(m^{}=2m\) instead of \(m\)._Based on Theorem 4.11, we obtain the first reduction from robust Blackwell optimality to robust discounted optimality. Since discounted RMDPs can be solved with value iteration or policy iteration, we provide the first algorithms to compute a robust Blackwell-optimal policy for RMDPs with sa-rectangular uncertainty, when the uncertainty set is based on the \(_{1}\) or the \(_{}\) distance. Note that there is no known convex (or linear) formulation for RMDPs (Grand-Clement and Petrik, 2022), so we are not able to provide a complexity statement akin to Theorem 4.7.

## 5 Conclusion

We highlight the shortcomings of the existing approach to Blackwell optimality and we introduce the Blackwell discount factor to ameliorate this issue. We provide an upper bound for MDPs and RMDPs in all generality, parametrized by the bit-sizes of the instances. Any progress in solving discounted MDPs, one of the most active research directions in RL, can be combined with our results to obtain new algorithms for computing average- and Blackwell-optimal policies. Our work also opens new research avenues for MDPs and RMDPs: the proof techniques for our bound on \(_{}\) and \(_{,t}\), based on the separation of algebraic numbers, are novel and they could be tightened for specific instances or different optimality criteria, such as bias optimality or \(n\)-discount optimality. The notion of _approximate_ Blackwell optimality as well as the existence of the robust Blackwell discount factor for other uncertainty sets, e.g., s-rectangular or non-polyhedral sa-rectangular uncertainty sets, or for distributionally robust MDPs, are also interesting directions of research.

Funding.J. Grand-Clement is supported by the Agence Nationale de la Recherche [Grant 11-LABX-0047] and by Hi! Paris. M. Petrik's work was supported, in part, by NSF grants 2144601 and 1815275.