# In-N-Out: Lifting 2D Diffusion Prior for 3D Object Removal via Tuning-Free Latents Alignment

Dongting Hu 1 Huan Fu 3 Jiaxian Guo 4 Liuhua Peng 1 Tingjin Chu 1 Feng Liu 1 Tongliang Liu 2,5 Mingming Gong 1,5

###### Abstract

Neural representations for 3D scenes have made substantial advancements recently, yet object removal remains a challenging yet practical issue, due to the absence of multi-view supervision over occluded areas. Diffusion Models (DMs), trained on extensive 2D images, show diverse and high-fidelity generative capabilities in the 2D domain. However, due to not being specifically trained on 3D data, their application to multi-view data often exacerbates inconsistency, hence impacting the overall quality of the 3D output. To address these issues, we introduce "In-N-Out", a novel approach that begins by inpainting a prior, i.e., the occluded area from a single view using DMs, followed by outstretching it to create multi-view inpaintings via latents alignments. Our analysis identifies that the variability in DMs' outputs mainly arises from initially sampled latents and intermediate latents predicted in the denoising process. We explicitly align of **initial** latents using a Neural Radiance Field (NeRF) to establish a consistent foundational structure in the inpainted area, complemented by an implicit alignment of **intermediate** latents through cross-view attention during the denoising phases, enhancing appearance consistency across views. To further enhance rendering results, we apply a patch-based hybrid loss to optimize NeRF. We demonstrate that our techniques effectively mitigate the challenges posed by inconsistencies in DMs and substantially improve the fidelity and coherence of inpainted 3D representations.

## 1 Introduction

Neural Radiance Fields (NeRFs)  have effectively revolutionized 3D scene reconstruction from multi-view images. These models offer high-fidelity novel-view synthesis, proving beneficial across a variety of domains . Despite the impressive ability to reconstruct highly detailed scenes, these learning-based methods depend on the availability of consistent multi-view training data. This reliance limits their generalizability, particularly in editing 3D representations for tasks like object removal and inpainting occluded areas.

Recently, diffusion models (DMs)  have gained significant attention in the field of generative modelling for 2D images. These models are well-known for their robustness as generative priors, capable of producing diverse and high-fidelity results in 2D inpainting tasks. However, adapting these 2D priors for 3D object removal is not straightforward. While the inherent diversity of DMs benefits the generation of varied outputs, it also poses a significant challenge: high variance in the inpainted results (Fig.1 middle column). Consequently, these models frequently produce outputs that, while visually appealing in isolation, may appear misaligned when incorporated into 3D domain . This misalignment often results in the loss of high-frequency details, crucial for realistic and coherent scene rendering.

Previous studies addressing such 3D inconsistencies can be broadly categorized into two approaches: multi-view and single-view priors. The former tackled inconsistencies across multi-view inpainted images by optimizing NeRFs with modified objectives . While these methods have shown promise in refining inconsistent inputs, they sometimes suffer from a loss of detail fidelity during the training process, as illustrated in Fig. 4. Conversely, other studies have attempted to overcome the multi-view inconsistency bottleneck by anchoring the inpainting process to a single reference image that serves the entire scene . This approach, however, places significant reliance on the selection of an appropriate reference image and the accuracy of depth estimates, which could lead to geometric artifacts during testing, as shown in Fig. 4.

To address these challenges, we aim to overcome 3D inconsistencies by guiding 2D DMs to achieve multi-view consistent inpainting results (Fig. 1 right column). Our analysis reveals that the variance in model outputs primarily comes from the random noise as the **initial** latent sample, and **intermediate** latents inferred by the denoising network. Each frame's initial latents are independently sampled, while intermediate latents are individually predicted, highlighting how view-dependent data impacts the generation process. Therefore, our approach focuses on aligning these two critical elements across multiple inputs. We introduce "In-N-Out", a conditional-sampling-like approach that inpaints a sampled view and \(}\) it to multiple views. Our method contains three key components:

1. Conditional Inpainting Pipeline: We propose a pipeline that first samples an inpainting outcome from a random view as an inpainting prior. This prior then serves as a condition to guide the inpainting process of multiple views, ensuring a consistent inpainting foundation.
2. Explicit Latents Alignment: Leveraging the geometry derived from a pre-trained NeRF and the inpainting prior, we sample multi-view initial latents conditional on the geometry dictated by the inpainting prior. This ensures that the primary components within the inpainted areas are structurally consistent and align with the underlying 3D geometry.
3. Implicit Latents Alignment: We employ a cross-view attention mechanism during the denoising steps to align predicted intermediate latents concerning the inpainting prior. This enhances the appearance consistency across the inpainted images.

To further enhance our method's performance in the 3D domain, we have implemented a patch-based optimization strategy using a hybrid loss on our inpainted multi-view images. This strategy employs perceptual loss to rectify spatial mismatches, and adversarial loss to preserve high-frequency details. By addressing these key challenges, our framework effectively handles multi-view inconsistencies and enhances the fidelity and coherence of 3D representations. The effectiveness of our approach is demonstrated through both qualitative and quantitative evaluations of a challenging object removal dataset. Our results indicate comprehensive improvement compared to existing methods, highlighting our model's ability to achieve greater fidelity and consistency in inpainted scenes.

Figure 1: Inpainting outcomes of multi-view images from original Stable Diffusion  (middle) with those achieved by our approach (right). The inpainted areas are highlighted in red and green boxes.

Related Works

2D Editing with Diffusion ModelsDiffusion models , have revolutionized image generation with their capacity to create highly realistic images. These models facilitate customizable generation via textual prompts , predominantly using pre-trained Stable Diffusion . Several editing methods  allow users to adjust images by moving anchor points to new locations. Editing typically begins by inverting the latent representation of the image to be edited back to its initial noise , with modifications made during the denoising phase. Prompt-to-Prompt (P2P)  edits images by adjusting the cross-attention between the image and text. Null-text inversion  addresses artifacts in DDIM inversion  when using classifier-free guidance . Delta Denoising Score (DDS)  optimizes the latent image representation by aligning the predicted noises of the original and modified texts. Additionally, several studies  have identified a relationship between the appearance of images generated by diffusion models and the key-value pairs. While these advancements represent significant progress preserve some content from the original image in 2D image editing, they do not account for multi-view consistency, thus can not be lifted to 3D editing directly.

Lifting 2D diffusion models for 3D editingRecent advancements in 3D editing and generation have effectively utilized 2D DMs to enhance these processes, as demonstrated in various studies . Pioneering works have used images inferred by DMs for direct supervision. Instruct-NeRF2NeRF (IN2N)  approached the editing task by transforming 3D model editing into a 2D image editing task, utilizing Instruct Pix2Pix (IP2P)  to iteratively update 3D scenes. Similarly, ViCA-NeRF  addressed editing challenges by modifying reference images and integrating these changes into the scene. DreamEditor  opted for a different strategy by converting NeRF into a mesh for direct optimization. GaussianEditor  applies semantic tracing to identify and modify editing targets within 3D Gaussian Splitting (3DGS) . Similarly, Gaussian Grouping  implements Identity Encoding for each Gaussian to create masks for editing. Conversely, Score Distillation Sampling (SDS)  provides an alternative way to guide 3D representations by backpropagating gradients from a diffusion model's denoiser  into the underlying scene representation. This technique has been effectively applied to generate realistic 3D and 4D scenes using NeRFs  and 3DGS .

2D and 3D Inpainting2D inpainting methods reconstruct images by filling missing content in areas defined by a mask . Early techniques, exemplified by , relied on copying textures from known to unknown regions. LaMa  excels in restoring large missing areas using fast Fourier convolutions, extensive receptive fields, and large training masks. Although highly effective at generating plausible background textures within specified masks, LaMa limits the fidelity of its outputs. In contrast, probabilistic diffusion models  have shown impressive results in image generation and offer a wide range of inpainted outputs. DMs can be adapted for inpainting without specific training, and modify known regions during each denoising step to fit the task . Similarly, Stable Diffusion  excels at inpainting by operating within latent space, allowing for efficient and effective image generation. In this work, we adopt it as our 2D inpainter.

3D scene inpainting aims to fill missing areas within a space, such as removing objects and generating coherent geometry and textures to complete the scene. Although 3D generative models have garnered large interest , they are often limited by the scarcity of 3D training data, hence result in poor generalization, particularly in scene inpainting tasks. Therefore, most current 3D inpainting models  enhance their effectiveness by adopting priors from 2D models. SPIn-NeRF  reduces multi-view inconsistencies by first inpainting views and then optimizing NeRF using perceptual loss. NeRFiller  tackles multiple frames simultaneously by tiling images for DMs. GaussianEditor  edit targets within 3DGS , guided by inpainted multi-view images from DMs. While these methods show promise, they can sometimes compromise detail fidelity during training. Alternatively, some studies circumvent multi-view inconsistencies by using a single reference image for the entire scene . Infusion  stands out in the inpainting of 3DGS, leveraging a pre-trained depth completion network to infer point clouds from a single inpainted view, though this method depends heavily on precise depth estimates. Concurrent works  address these challenges using SDS objective  to better align 2D model priors with 3D scene consistency.

Preliminaries

### Neural Radiance Fields

Neural Radiance Fields (NeRFs)  represents a breakthrough in 3D rendering by employing a multilayer perceptron (MLP), denoted as \(\) to represent a scene. This MLP serves as a continuous volumetric function to capture and reconstruct a scene in unprecedented detail. Specifically, NeRFs take as input the view direction \(d\) and a 3D coordinate \(r()\) sampled from a camera ray defined by \(r()=o+ d\). At each position along this ray \(r()\), the network predicts the volume density and view-dependent color, represented as \((,c)\). To render a camera pixel, NeRFs perform an aggregation of the predicted densities and color emissions \((_{i}),c(_{i})\) along the camera ray. This process is mathematically formulated as an approximation of a volume rendering integral , which is used to compute the final color of the pixel:

\[(r)=_{i}(1-(-(_{i})(_{i}) ))c(_{i}),_{i}=(-_{j=1}^{i-1}(_{j})_{j} ),\] (1)

where \((_{i})=_{i+1}-_{i}\) is the distance between adjacent samples along the ray. During the training phase, rays are uniformly sampled from the training images, and the volumetric field is optimized using mean square error (MSE) to enhance the accuracy and realism of the rendered scenes.

### Diffusion Models

Diffusion models  consist of two processes: a forward process that gradually introduces noise to a data sample \(z^{0} p_{}(z)\), and a learned reverse process that iteratively denoises a purely Gaussian noise sample \(z^{T}(0,1)\) back into a clean image \(z^{0}\). The reverse process is parameterized by a conditional noise prediction network \(_{}\), trained to predict the noise using the simplified objective:

\[p_{}(z^{0:T}|c)=p(z^{T})_{t=1}^{T}p_{}(z^{t-1}|z^{t},c),  p_{}(z^{t-1}|z^{t},c)=(z^{t-1};_{}(z^{t},t,c), ^{2}I),\] (2)

where \(t\) is the time step in the diffusion process, \(z^{t}\) is an intermediate noisy sample, and \(c\) represents a condition (e.g., images, masks, or text). Utilizing a deterministic sampler like DDIM , the sample \(z^{t-1}\) can be obtained by \(z^{t-1}=z^{t}-_{}(z^{t},t,c)\); note that scaling is omitted for simplicity. In practice, as we use Stable Diffusion , a latent diffusion as the inpainting backbone, \(z\) is latent and the generated image is obtained with a decoder \((z^{0})\). Hence, the variability of the generated image \(z^{0}\) depends solely on initial **sampled** latent \(z^{T}\) and intermediate **inferred** latents \(\{z^{t-1}\}_{t=1}^{T}\).

## 4 Method

Given a set of multi-view training images \(\{_{i}\}_{i=1}^{N}\) from the scene with corresponding masks \(\{_{i}\}_{i=1}^{N}\) indicate the unwanted object in each frame, our approach seeks to generate consistently inpainted training set \(\{}_{i}\}_{i=1}^{N}\) and use them to supervise NeRF. Our approach is structured into three key stages:

* Stage 1: Pretrain a NeRF \(\) using \(\{_{i}\}_{i=1}^{N}\) and \(\{_{i}\}_{i=1}^{N}\), along with a sampled inpainted prior \(}_{p}\) as a rough hallucination of the inpaint feature. (Sec. 4.1).
* Stage 2: Leverage \(\) to inpaint additional views \(\{}_{i}\ |\ i p,i=1,,N\}\) conditioned on the inpainting prior \(}_{p}\) via explicit and implicit latents alignment. (Sec. 4.2)
* Stage 3: Using the inpainted image set \(\{}_{i}\}_{i=1}^{N}\), we optimize \(\) with a patch-based hybrid loss to distill multi-view supervision. (Sec. 4.3)

An overview of our method is shown in Fig. 2.

### Stage 1: Pre-train NeRF

The initial stage involves training the NeRF on the unmasked region, we follow the original work  where simple MSE loss is applied:

\[_{}()=_{r R_{}}\| _{}(r)-C(r)\|_{2}^{2},\] (3)where \(R_{}\) represent the unmasked pixels across all the training images. Then we sample a prior view \(_{p}\) with its mask \(_{p}\) and regularly inpaint it using Stable Diffusion \(\). For illustration, we replace of condition in Eq. 2 with two components used by Stable Inpainting Diffusion  as \(e\) for the input prompt, and \(_{p}^{}\) for the masked image that fed into the diffusion models. Hence the inpainting process can be formulated as:

\[z_{p}^{t-1}=z_{p}^{t}-_{}(z_{p}^{t},t,_{p}^{},e),t=T,,1,z_{p}^{T}(0,1).\] (4)

The inpainted image then can be obtained by \(}_{p}=(z_{p}^{0})\). We then use a monocular depth estimator on \(}_{p}\) to get a depth map \(_{p}\). We regress the scale and offset parameters to align \(_{p}\) with the depth estimated from field \(\) on the unmasked pixels. Hence, we can introduce the geometry and appearance supervision of the inpainting prior \(}_{p}\) into the NeRF's optimization through:

\[_{}()=_{r R_{}}\|_ {}(r)-C(r)\|_{2}^{2}+\|_{}(r)-_{p}(r) \|_{2}^{2},\] (5)

where \(R_{}\) denoted the masked (inpainted) pixels of \(}_{p}\), and \(_{}\) is the depth estimated by NeRF. This stage is depicted in Fig. 2(a).

### Stage 2: Latents Alignment

In this section, we introduce our key approach to condition the additional inpainted frames to have an inpainting feature based on the prior \(}_{p}\). As discussed before, in deterministic sampling of the diffusion inpainting model \(\), the generation structure and layout highly depend on (1) initial **sampled** noise \(z_{i}^{T}\) and (2) intermediate **predicted** latents \(\{z_{i}^{t-1}\}_{t=1}^{T}\). Hence if we can align the latents from different views with the prior one, the model is likely to generate multi-view consistent latent \(z_{i}^{0}\), hence image \(}_{i}\). In this section, we discuss how to align two terms respectively.

Figure 2: Overview of our method. Our approach begins with (a) pre-training the NeRF \(\) with a sampled inpainting prior \(}_{p}\) from Stable Diffusion \(\), detailed in Sec. 4.1. It then progresses to (b) latent-aligned inpainting \(}_{i}\) for multi-view images through Explicit Latents Alignment (ELA) and Implicit Latents Alignment (ILA), as described in Sec. 4.2. Finally, the NeRF is optimized using a patch-based hybrid loss strategy outlined in Sec. 4.3. Throughout the training process, we fix Stable Diffusion \(\) and update the scene-specific NeRF parameters \(\) only.

Explicit Initial Latent Alignment (ELA)Given the sampled latent of prior view \(z_{p}^{T}\) used in Stage 1 (Sec. 4.1), we could leverage geometric information to explicitly align the initial latent in 3D space. Given that estimated depth \(_{p}\), one possible solution is to warp the \(z_{p}^{T}\) into other views using camera matrices. However, \(_{p}\) is not guaranteed to be accurate hence such hard projection could yield significant errors. Alternatively, we propose to leverage the pre-trained NeRF \(\), as it's a naturally 3D-consistent representation. Specifically, to sample a resolution-grain initial latent \(z^{T}(r)\), we utilize the original formulation of volume rendering (Eq.1) but with the substitution of color \(c\) by latent \(z^{T}\). We query the density \(\) from \(\), and acquire \(z^{T}\) by reprojecting the sampled point to the image plane of the prior latent view \(z_{p}^{T}\):

\[z^{T}(r)=_{i}1--(_{i})(_{i}) z^{T}(_{i}),z^{T}(_{i})=f_{p,i}(z_{p}^{T},_{i}),\] (6)

where \(f_{p,i}\) denote camera perspective projection according to \(p\) and \(i\) camera matrices. Such soft projection could avoid error accumulation in the inpainting process, and reduce the precision burden on the depth estimator. We illustrate this process in Fig. 2(a). There are two key reasons why we propose fine-tuning the NeRF and using it as a geometric prior for ELA: (a) After finentuning the NeRF, the geometric is represented by NeRF as a sharp (low variance) unimodal distribution on the ray. Consequently, the aggregated feature remains sharp, preserving the variations in the initial latents. (b) We empirically found the depth prior inferred by the monocular depth estimator is not perfectly aligned with the NeRF. Fine-tuning the NeRF can also benefit this depth prior. Since NeRF learns relatively certain geometry in the known (unmasked) areas, this geometry constraint can improve the geometry of neighboring inpainted (masked) areas due to their geometric proximity. We compromise the view-dependent effect in NeRF within the ELA module. Due to the heuristic nature of diffusion models, incorporating such view-dependent effects into diffusion models' output remains elusive.

Implicit Intermediate Latents Alignment (ILA)While the initial latent could be aligned using the explicit method, intermediate latents are predicted by denoising network \(_{}\) which is hard to control. We address this issue by exploring the conditioning mechanism of the denoising network in Stable Diffusion . Recall that in Eq. 4, denoising network \(_{}\) relies on the input prompt \(e\) and masked image \(^{}_{i}\) to predict the noise occurrence in the current step. While we can use the unified prompt for all views to align the text condition in cross-attention of \(_{}\), the masked images \(\{^{}_{i}\}_{i=1}^{N}\) are inherently different due to multi-view nature. Note that \(^{}_{i}\) condition is introduced based on spatial self-attention (SA)  in the U-Net:

\[(Q_{i},K_{i},V_{i})=(K_{i}^{T}}{})V_{i},\] (7)

where \(Q_{i}\) obtained from each spatial resolution of the latent, \(K_{i},V_{i}\) are derived from corresponding latent encoded from masked image \(^{}_{i}\). We can impose the coherence of the denoising step by introducing cross-view attention (CVA) of the prior view (Fig. 2(b)):

\[(Q_{i},K_{p},V_{p})=((K_{p})^{T}}{ })V_{p},\] (8)

Figure 3: Illustration of two types of Latent Alignment. This figure depicts the Explicit Latents Alignment (ELA) and Implicit Latents Alignment (ILA) processes, as detailed in Sec. 4.2.

where \(K_{p},V_{p}\) are from masked base image \(_{p}^{}\). We can then implicitly align the denoising step by replacing the original SA with a weighted sum from SA and CVA, i.e. \(_{a}*(Q_{i},K_{i},V_{i})+(1-_{a})*(Q_{i},K_{p},V _{p})\). Through this technique we ensure that the intermediate latents \(\{z_{i}^{t-1}\}_{t=1}^{T}\) are also conditioned on the prior \(\{z_{p}^{t-1}\}_{t=1}^{T}\), while retain its distinctiveness due to the individual viewpoint. The rationale of replacing \(KV\) with "prior" \(p\), but not \(Q\) is that the appearance information (\(V\)) of the prior image should be considered when inpainting the other views, with the amount of information propagation is weighted by its attention key value (\(K\)). The attention query value comes from the current inpainting view \(i\), \(Q_{i}\), representing the information the current inpainting for view \(i\) is searching for. Together with \(K_{p}\), it decides how much attention the view \(i\) inpainter should place on the prior view, and finally incorporates the information of the prior view \(V_{p}\) into view \(i\).

### Stage 3: Joint Optimization

As the original intention of this work, we seek to distill the inpainted views into NeRF \(\) in a way such that the high-fidelity is preserved as much as possible as the unmasked region. While some priors work in 3D editing [25; 97; 93; 96] propose to update the training set iteratively until converge, we empirically find it not suitable for inpainting task since the loss of fidelity is significant and could fall into local optima. Hence we propose to inpaint a subset of training images at once and regard them as supplementary guidance using a patch-based hybrid loss:

\[_{}()=_{p_{}}\| _{}()-}()\|_{1}+_{ }(_{}(),}())+_{ }(_{}(),}()),\] (9)

where \(\) is a patch sample from the masked area of subset views \(_{}\), \(_{}()\) is NeRF predicted patch, and \(_{}\), \(_{}\) are perceptual distance LPIPS  and adversarial loss . Here LPIPS is utilized to address geometry mismatches, while adversarial loss is employed to preserve high-frequency details. As shown in Fig. 2, the final optimization objective is:

\[()=_{}()+_{}( )+_{}_{}().\] (10)

The patches are uniformly sampled within the bounding box of the mask, with a size of 256x256. Therefore, only the inpainted area is being optimized by the patch loss.

## 5 Experiment

### Evaluation Setting

Dataset:Aligning with methodologies employed in prior works [53; 51; 109], our experiments utilize the SPIn-NeRF dataset , selected for its comprehensive ground truth availability. This dataset is specifically designed for object removal evaluations and comprises 10 scenes. Each scene includes 60 images featuring an unwanted object (training views) and 40 images from which the object has been removed (test views). For both the training and test views, human-annotated masks indicating the object region are available. We further collected 9 forward-facing scenes with manually annotated masks to evaluate the effectiveness of our method. This dataset includes 4 indoor scenes and 5 outdoor scenes. In the training set, the masked region contains the unwanted object, while the test set contains the ground truth background in the masked region.

Baselines:In our study, we benchmark our method against a variety of established 3D inpainting approaches to ascertain its relative performance. These include the perceptual-based SPIn-NeRF , tiling-based NeRFiller  (both multi-view guidance) and InFusion  (single-view guidance). To ensure a fair comparison, we employ the same inpainting diffusion models across all methods and maintain consistency in the number of denoising steps and used prompts. We utilized the source code provided by the authors and ran all the methods using one NVIDIA A100 (80G) GPU.

Metrics:To quantitatively evaluate the effectiveness of our approach, we employ two similarity metrics: LPIPS  and FID . Additionally, we use MUSIQ , a sharpness metric that quantifies the clarity and detail retention in the edited images. Following established protocols from previous studies , all metrics are calculated specifically within the bounding boxes defined by the masks, focusing the evaluation precisely on the regions most affected by the object removal task.

Figure 4: Qualitative results on the SPIn-NeRF the self-collected dataset.

### Main Results

We first present the quantitative results in Tab. 1, where our method outperforms all baselines in terms of similarity metrics and sharpness. Our approach also excels in qualitative assessments, as demonstrated in Fig. 4. It is important to note that in simpler scenes with low variability in inpainting results, where the inconsistency issue is less pronounced (first row), most methods perform adequately. In other cases, high-frequency loss is observed in multi-view-based methods (SPIn-NeRF and NeRFiller). NeRFiller , through its use of multiple joint denoising steps, ensures consistency but often produces overly smooth outputs that lack fine details. It is noteworthy that the single-view-based method, InFusion , relies on one view and its depth to represent the entire scene. It performs well when geometry estimation is accurate. However, its performance deteriorates in scenarios where depth accuracy is compromised, leading to geometry artifacts (sixth and seventh rows). This underscores the critical role of multi-view supervision in addressing such challenges. By incorporating consistent multi-view supervision, our method remains effective even when depth or geometry is inaccurate, achieving robust and promising results. This explains why our method shows little difference from InFusion when the geometry is accurate (first and third row), but excels when the depth is inaccurate. Additionally, the exclusive reliance on perceptual loss by SPIn-NeRF  fails to fully address the multi-view inconsistencies introduced by inpainting diffusion models, often resulting in a blurred effect, particularly visible in the third and fourth rows. To further validate our findings, we conducted a user study based on the SPIn-NeRF dataset, focusing on the coherence of the background within the inpainted area, the fidelity of detail preservation in the inpainted region, and overall preference. The results of this study are summarized in Tab. 2. This evaluation clearly demonstrates superior performance across all assessed criteria.

### Ablation Studies

We initially demonstrate the efficacy of our latents alignment approach with an example in Fig. 5. The first column displays the inpainting prior (sampled view), and the subsequent columns show the same training image being inpainted under different conditions. Notably, the variant without ELA (w/o ELA) retains colors similar to the prior but fails to preserve the texture structure. Conversely, the version without ILA (w/o ILA) maintains structural integrity but lacks appearance consistency with the prior. Our method effectively merges the strengths of both mechanisms, resulting in inpaintings that are highly consistent and cohesive across all evaluated aspects.

We conducted further ablation studies to underscore the importance of our key design elements in object removal tasks. The quantitative and qualitative results, showcased in Tab. 3 and Fig. 6, clearly indicate the impact of each component. Notably, removing ELA leads to geometry mismatches in the NeRF outputs (w/o ELA), while deactivating ILA results in blurry coloration (w/o ILA). This observation confirms our initial findings: the initial latents primarily influence the inpainting's structural pattern, whereas the intermediate denoising steps largely affect its appearance, including

   Method & LPIPS \(\) & FID \(\) & MUSIQ \(\) \\  SPIn-NeRF  & 0.54 & 185.63 & 38.69 \\ NeRFiller  & 0.71 & 315.83 & 32.60 \\ InFusion  & 0.62 & 153.77 & 39.29 \\ Ours & **0.49** & **130.92** & **50.97** \\   

Table 1: Quantitative Results

Figure 5: Ablation study on latent aligned onpainting. 2D Inpainting results when key components of our proposed method are omitted. Naive inpainting using Stable Diffusion can refer to Fig. 1.

   Method & Coherence & Fidelity & Overall \\  SPIn-NeRF  & 22.72\% & 20.45\% & 21.82\% \\ NeRFiller  & 2.73\% & 4.33\% & 2.50\% \\ InFusion  & 27.50\% & 24.77\% & 25.00\% \\ Ours & **47.05\%** & **50.45\%** & **50.68\%** \\   

Table 2: User Studycolour nuances. Additionally, our patch-based loss plays a crucial role in the optimization process (w/o \(_{}\)). Specifically, the \(_{}\) loss helps to alleviate geometry mismatches (w/o \(_{}\)), and the \(_{}\) serves as a detail-preserving supervisor (w/o \(_{}\)). These results highlight the effectiveness of our design choices in enhancing the overall quality and coherence of the inpainted outputs.

## 6 Conclusion

In this work, we demonstrate the significant improvement achieved through our novel latents alignment approach in 3D object removal. By integrating both explicit and implicit latent alignment mechanisms, we have successfully addressed key challenges associated with geometry mismatches and color inconsistencies that are prevalent in the baselines, enhancing the fidelity and detail of the inpainted 3D scenes. The improvements achieved through our work offer significant societal benefits, such as enhanced editability of radiance fields. However, it also poses risks, including the potential perpetuation of biases and discrimination. If the data used to train diffusion models is biased, our approach could inadvertently reinforce these biases.

Despite notable advancements, our method has limitations: (1) It struggles with full 3D consistency, especially on high-frequency details, due to the constraints of applying 2D diffusion models to multi-view data. Future work could address this by integrating multi-view training into 2D inpainting diffusion models or leveraging true 3D generative models. (2) It is tailored for forward-facing scenes, limiting its applicability to diverse 360\({}^{}\) views. Further exploration of latent relationships for broader view coverage is needed. (3) Predefined masks are currently required. Integrating advanced 3D perception methods [66; 67] could enhance accuracy and flexibility, enabling precise language-driven interactions and creating a more automated, user-friendly framework for neural 3D scene editing.

## 7 Acknowledgements

This research was mainly undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200. This research was also partially supported by the Research Computing Services NCI Access scheme at the University of Melbourne. DH was supported by the Melbourne Research Scholarship from the University of Melbourne. FL is supported by the Australian Research Council (ARC) with grant numbers DP230101540 and DE240101089, and the NSF&CSIRO Responsible AI program with grant number 2303037.

  Method & LPIPS \(\) & FID \(\) & MUSIQ \(\) \\  Ours w/o ELA & 0.52 & 133.09 & 48.90 \\ Ours w/o ILA & 0.50 & 141.78 & 49.70 \\ Ours w/o \(_{}\) & 0.73 & 293.32 & 33.76 \\ Ours w/o \(_{}\) & 0.55 & 223.31 & 46.07 \\ Ours w/o \(_{}\) & 0.51 & 134.70 & 49.88 \\ Ours full model & **0.49** & **130.92** & **50.97** \\  

Table 3: Quantitative Results of Ablation Study.

Figure 6: Ablation study on design choices based on rendering quality. This figure displays rendering results from NeRF when key components are individually removed from our full model.