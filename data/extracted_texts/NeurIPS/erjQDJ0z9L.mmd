# Discovering Preference Optimization Algorithms

with and for Large Language Models

 Chris Lu

Sakana AI and FLAIR

chrislu@sakana.ai

&Samuel Holt

University of Cambridge

sih31@cam.ac.uk

&Claudio Fanconi

University of Cambridge

caf83@cam.ac.uk

&Alex J. Chan

University of Cambridge

ajc340@cam.ac.uk

&Jakob Foerster

FLAIR, University of Oxford

jakob.foerster@eng.ox.ac.uk

&Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

&Robert Tjarko Lange

Sakana AI

robert@sakana.ai

Equal Contribution.

Work partially done at Spotify.

Equal Advising.

Equal Advising.

###### Abstract

Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs. Typically, preference optimization is approached as an offline supervised learning task using manually crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under-explored. We address this by performing LLM-driven _objective discovery_ to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention. Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously evaluated performance metrics. This process leads to the discovery of previously unknown and performant preference optimization algorithms. The best performing of these we call _Discovered Preference Optimization_ (DiscoPOP)1, a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.

## 1 Introduction

Training Large Language Models (LLMs) usually involves starting with a model pre-trained on large text corpora and then fine-tuning it to match human preferences. Pre-trained, and even instruction fine-tuned LLMs, can generate harmful, dangerous, and unethical completions . To mitigate this and align an LLM with human values, we use human preference alignment through preference-ranked completion data. This approach has become an industry standard, popularized by reinforcement learning with human feedback (RLHF) [13, RLHF], and more recently, by offline preference optimization algorithms like direct preference optimization  and sequence likelihood calibration .

et al., 2023, SLiC], which cast the problem as a supervised learning objective. Many algorithms have been proposed in the literature for offline preference optimization, and it remains an open question which one performs best across tasks. While a strictly dominant algorithm may not exist, some algorithms likely exhibit generally improved performance. To date, all existing state-of-the-art preference optimization algorithms [Rafailov et al., 2023, Azar et al., 2023, Zhao et al., 2023] have been developed by human experts. Despite their advancements, these solutions are inherently constrained by human limitations, including creativity, ingenuity, and expert knowledge.

In this work, we aim to address these limitations by performing LLM-driven discovery to _automatically_ generate new state-of-the-art preference optimization algorithms without continual expert human intervention in the development process. While previous works [Ma et al., 2023, Yu et al., 2023] have used LLMs to design environment-specific RL reward functions, we discover general-purpose objective functions which can be used across various preference optimization tasks. More specifically, we iteratively prompt an LLM to propose new preference optimization loss functions and evaluate them, with the previously proposed loss functions and their task performance metric (in our case, MT-Bench scores [Zheng et al., 2024]) as in-context examples. After performing this automatic discovery process, we catalogue high-performing loss functions and introduce a particularly strong one we call _Discovered Preference Optimization_ (DiscoPOP), a new algorithm. To ensure robustness beyond MT-Bench, we validate DiscoPOP using AlapacaEval 2.0 [Dubois et al., 2024], showing an improvement in win rates against GPT-4 from DPO \((11.23\% 13.21\%)\). Additionally, in separate, held-out, tasks such as summarization and controlled generation, models trained with the DiscoPOP loss outperform or perform competitively with existing preference optimization algorithms.

**Contributions:**1 We propose an LLM-driven objective discovery pipeline to discover novel offline preference optimization algorithms (Section 3). **2 We discover multiple high-performing preference optimization losses. One such loss, which we call _Discovered Preference Optimization_ (DiscoPOP), achieves strong performance across multiple held-out evaluation tasks of multi-turn dialogue (AlpacaEval 2.0), controlled sentiment generation (IMDb) and summarization (TL;DR) tasks. **3** We provide an initial analysis of DiscoPOP, which is a weighted sum of logistic and exponential losses, and discover surprising features. For example, DiscoPOP is non-convex.

## 2 Background

**Preference Optimization**. Consider a pre-trained language model policy \(_{}\) and a dataset \(=\{(x^{i},y^{i}_{w},y^{i}_{l})\}_{i=1}^{N}\) consisting of prompts \(x\) and preference-ranked completions \(y_{w}\) and \(y_{l}\). In this dataset, a human rater prefers \(y_{w}\) over \(y_{l}\), denoted as \(y_{w} y_{l}\). The task is to align \(_{}\) with the human values implicit in these preferences. Canonically, this has been achieved through reinforcement learning from human feedback [Christiano et al., 2017, RLHF], an approach that proceeds in two

Figure 1: **Left**. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses \(_{(y_{w},y_{l},x)}[f()]\) as a function of the policy (\(_{}\)) and reference modelâ€™s (\(_{}}\)) likelihoods of the chosen (\(y_{w}\)) and rejected (\(y_{l}\)) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. **Right**. Performance of discovered objective functions on Alpaca Eval.

phases: First, a _reward modelling_ stage that learns a parameterized reward model \(r_{}\). By assuming a Bradley-Terry model (Bradley and Terry, 1952) of preferences, the probability of the data can be expressed as \(P(y_{w} y_{l})= r_{}(y_{w},x)/( r_{}(y_{w},x)+ r_{} (y_{l},x))\), and subsequently simply optimized over \(\) through the maximum likelihood principle. The second stage of _policy optimization_ employs a reinforcement learning algorithm to train the language model against the learned reward. Usually, a KL penalty is introduced between the model and the pre-RL _reference_ policy \(_{ref}\)(Jaques et al., 2019; Stiennon et al., 2020) to prevent over-optimization and straying too far from the original policy, resulting in the final objective:

\[_{_{}}_{y_{},x} [r_{}(y,x)]}_{}-(_{},_{})}_{}.\] (1)

Despite success in frontier models (Anthropic, 2023; Gemini-Team, 2023), deep RL has many implementations (Engstrom et al., 2019) and training challenges (Sutton, 1984; Razin et al., 2023) that hinder its adoption. To simplify the whole process, _direct preference optimization_(Rafailov et al., 2023, DPO) aims to forego both the reward modelling and online RL procedure. Rewriting (1) with a decomposition of the KL term into:

\[_{_{}}_{y_{},x} (y,x)}_{}+(y|x)}_ {_{ref}}+(_{})}_ {},\] (2)

expresses the problem as an entropy-regularised RL bandit task (Ziebart et al., 2008), for which a known analytical solution exists: \(^{*}(y|x)=Z(x)^{-1}_{ref}(y|x)^{-1}r_{}(y,x)\). By rearranging the reward, we can express the task as a binary classification problem based on the reward difference:

\[_{_{}}_{(y_{w},y_{l},x)}f (y_{w}|x)}{_{}( y_{w}|x)}-(y_{l}|x)}{_{}(y_{l}|x)})}_{r_{ }(y_{w},x)-r_{}(y_{l},x)}.\] (3)

Here, we define the log ratio difference as \(=(y_{w}|x)}{_{}(y_{w}|x)}-(y_{l}|x)}{_{}(y_{l}|x)}\). In DPO, the function \(f=-\) is derived as the negative log of the sigmoid function given the BT model assumptions. However, Tang et al. (2024) highlighted that more generally we can obtain a recipe for offline preference optimization algorithms by letting \(f:\) be any scalar loss function. For example, setting \(f(x)=(x-1)^{2}\), the squared loss function (Rosasco et al., 2004) yields IPO (Azar et al., 2023), while employing the max-margin inspired hinge loss (Boser et al., 1992; Cortes and Vapnik, 1995)\(f(x)=(0,1-x)\) produces SLiC (Zhao et al., 2023).

**Meta-Optimization for Algorithm Discovery**. The goal of meta-optimization (optimizing the optimization process) is to uncover novel learning algorithms using a data-driven process. Suppose that an algorithm uses an objective function \(f^{}\) to train a model for \(K\) iterations, where \(\) denotes a set of meta-parameters. Meta-optimization searches for an objective that maximizes the expected downstream performance \(\), \([(_{K})|(f^{})]\) where \(\) is a downstream performance metric. Unlike previous methods that rely on a predefined parameterization of \(\) (e.g., a neural network (Hospedales et al., 2021) or domain-specific language (Alet et al., 2020)), we leverage LLMs to directly propose code-level objective functions in Python. This approach eliminates the need for a carefully designed search space and utilizes the extensive knowledge embedded in the LLM for flexible selection and mutation.

## 3 LLM-Driven Objective _Discovery_

Choosing an appropriate objective function is crucial for insttilling capabilities into networks. Here, we detail our discovery process facilitated by LLM code-level objective function proposals:

**Initial Context Construction**. In the initial system prompt, we 'burn-in' the LLM using several established objective functions given in code and their corresponding performance. Furthermore, we provide problem details and an example of the output response format as a JSON dictionary.

**LLM Querying, Parsing & Output Validation**. We query the LLM, parse the response JSON, and run a set of unit tests (e.g. for valid output shapes) before starting a training run. If the parsing or unit tests fail, we resample a new solution after providing the error message as feedback to the LLM.

**Performance Evaluation**. The proposed objective function is then evaluated based on its ability to optimize a model for a predefined downstream validation task. We refer to the resulting performance metric as \(\).

**Iterative Refinement**. By using the performance provided as feedback, the LLM iteratively refines its proposals. In each iteration, the model synthesizes a new candidate loss function, exploring both variations of previously successful formulas and entirely new formulations that might improve upon the existing benchmarks. This iterative process is repeated for a specified number of generations or until convergence when a set of optimal loss functions is observed.

We summarise this general objective discovery process in Figure 1 and is shown in Algorithm 1.

```
1:Initialize LLM with established loss functions and their performance in context.
2:repeat for each generation \(i\)
3:LLM proposes a new candidate objective function \(f_{i}\)
4:Run unit tests to check the validity of the candidate and resample if needed.
5:Evaluate the objective function using the performance metric \(\)
6:Update the LLM context with the performance data
7:LLM refines generation strategy based on the feedback
8:until convergence criteria are met or maximum generations are reached ```

**Algorithm 1** LLM-Driven Objective Discovery

**Small case study: Discovering supervised classification loss functions**. Consider the case of supervised classification on the CIFAR-10 dataset as a simple starting example. We train a simple ResNet-18 for 5 epochs using the objectives proposed by GPT-4 . After each training run we provide the LLM with the corresponding validation accuracy and query it for the next PyTorch-based  candidate objective function.

Figure 2 depicts the performance of the proposed objective functions across the discovery process. The different discovered objectives all outperform the standard cross-entropy loss. Interestingly, we observe that the LLM-driven discovery alternates between several different exploration, fine-tuning, and knowledge composition steps: Initially, the LLM proposes a label-smoothed cross-entropy objective. After tuning the smoothing temperature, it explores a squared error loss variant, which improved the observed validation performance. Next, the two conceptually different objectives are combined, leading to another significant performance improvement. Hence, the LLM discovery process does not perform a random search over objectives previously outlined in the literature but instead composes various concepts in a complementary fashion. Furthermore, the discovered objectives also generalize to different architectures and longer training runs. In Appendix D.3 we show that this process of discovery is robust to the choice of sampling temperature and prompt/context construction.

## 4 Discovering Offline Preference Optimization Objectives

In this section, we run our LLM-driven discovery to automatically generate new state-of-the-art preference optimization algorithms.

Figure 2: LLM-driven objective discovery for CIFAR-10 classification. **Left.** Performance across LLM-discovery trials. The proposals alternate between exploring new objective concepts, tuning the components, and combining previous insights. **Right.** The best three discovered objectives transfer to different network architectures and longer training runs (100 epochs).

[MISSING_PAGE_FAIL:5]

## 5 Held-Out Evaluations

We next validate each of our discovered objective functions (shown in Table 1) on held-out tasks. We find that the Performance Adaptive Decay Loss (PADLL) and the Log Ratio Modulated Loss (LRML) consistently perform well. Because of its unconventional properties and performance, we refer to LRML as our discovered preference optimization, or _DiscoPOP_, algorithm.

We consider three different standard (Rafailov et al., 2023) open-ended text generation tasks each designed to evaluate different properties of the fine-tuned LLM policy \(_{}\) where each LLM policy is trained with one of our discovered objective functions \(f\) on a preference dataset \(=\{(x^{i},y_{w}^{i},y_{i}^{i})\}_{i=1}^{N}\).

### Single-turn Dialogue - Alpaca Eval 2.0

We evaluate the trained models on Alpaca Eval 2.0, (Li et al., 2023; Dubois et al., 2023, 2024). This is a single-turn dialogue LLM-based automatic evaluation using GPT-4 to assess the win rate of the trained LLM policy's completion compared to the of the underlying SFT base model. Alpaca Eval 2.05, has been validated against 20K human annotations, and aims to reduce the length bias of Alpaca Eval 1.0; where using length controlled (LC) Alpaca Eval shows a correlation with Chatbot Area of 0.98, making it a popular benchmark with the highest correlation to Chatbot Arena (Dubois et al., 2024). We also detail task training details in Appendix B.1.

We provide the Alpaca Eval 2.0 results in Table 2. As reference policies, we used GPT-4 for absolute comparison and the SFT-trained model for relative comparison. We observe that the discovered LRML (DiscoPOP), PADLL, and AQFL functions outperform the baselines and other discovered losses on the normal and length-controlled win rates. The differences in scores among these top-performing losses are not significant, except for the LC win rate against the SFT reference model, where DiscoPOP performs best.

### Summarization (TL;DR)

We train an LLM policy to, given a forum post on Reddit \(x\), generate a summarization \(y\) of the main points. We finetune 'zephyr-7b-gamma-sft' using 10% of the Reddit TL;DR summarization preference dataset (Volske et al., 2017) on each of the baseline and discovered objective functions. As a reference model, we again use 'zephyr-7b-gamma-sft'. Further details on the training pipeline are outlined in Appendix B.2. To evaluate the quality of the summaries, we make use of the Alpaca Eval 2.0 library with a custom evaluation dataset existing of 694 test samples from the TL;DR dataset and a custom GPT-4 annotator template as described in Rafailov et al. (2023). For additional details regarding the summarization evaluation see Appendix C.3.

  
**Function** & **Win Rate (\%) \(\)** & **Win Rate - LC (\%) \(\)** & **Win Rate (\%) \(\)** & **Win Rate - LC (\%) \(\)** \\  & & vs. GPT-4 & & vs. SFT Checkpoint \\  DPO & \(11.23 0.97\) & \(12.81 0.66\) & \(78.72 1.26\) & \(63.34 0.30\) \\ DPO\({}^{*}\) & \(11.99 1.00\) & \(14.73 0.71\) & \(75.75 1.31\) & \(59.88 0.41\) \\ SLiC & \(10.67 0.94\) & \(13;16 0.69\) & \(75.05 1.34\) & \(59.67 0.42\) \\ KTO & \(12.57 1.00\) & \(13.58 0.67\) & \(78.81 1.25\) & \(62.76 0.31\) \\ DB\(\)QL & \(10.68 0.92\) & \(11.41 0.57\) & \(72.06 1.42\) & \(54.40 0.38\) \\ AQL & \(11.11 0.96\) & \(13.63 0.68\) & \(76.34 1.30\) & \(60.94 0.36\) \\ PADLL & \(\) & \(\) & \(\) & \(64.14 0.28\) \\ AQFL & \(\) & \(\) & \(\) & \(64.41 0.34\) \\ CELL & \(10.27 0.93\) & \(12.26 0.61\) & \(71.75 1.39\) & \(57.48 0.34\) \\ LRML & \(\) & \(\) & \(\) & \(\) \\ PFL & \(8.15 0.83\) & \(10.67 0.57\) & \(68.27 1.44\) & \(56.14 0.43\) \\   

Table 2: **Alpaca Eval 2.0 - Held Out Single Turn Dialogue Task**. Win rate of the discovered objective functions \(f\) evaluated on the Alpaca Eval 2.0 task against either GPT-4 or the SFT base model. Some of the discovered objective functions outperform the baselines, with the best bolded. We detail evaluation and error bars in Appendix C. We have highlighted the best scores with overlapping the standard errors.

In Table 3 the PADLL loss and DPO loss perform best, with little difference from each other, on the summarization task in three out of four metrics. Additionally, the LRML - DiscoPOP function achieves scores slightly below the top performers, especially in the length-controlled win rates. In contrast to the single-turn dialogue task, the AQFL loss does not achieve high scores in the held-out evaluation.

### Positive sentiment generation (IMDb)

In this task, we train an LLM policy to generate movie review completions \(y\) with positive sentiment, where \(x\) is a prompt at the start of a movie review from the IMDb dataset (Maas et al., 2011). We start with a GPT-2 (Radford et al., 2019) model, which had supervised fine-tuning on the IMDb dataset, and we perform preference optimization using the baseline and discovered objective loss functions. Details of the training implementations can be found in Appendix B.3. Inspired by Rafailov et al. (2023)'s experiments, we calculate the model rewards through a pre-trained sentiment classifier, which we use as a proxy for ground truth, as well as the KL-Divergence of the trained model and the reference model. Appendix C.4 provides further details into the evaluation for this task.

We provide results of models with converging \(\) values in Figure 5 for LRML compared against DPO and SLiC, displaying the model rewards against the KL-Divergence to the reference model. In Figure 4(a), the LRML-trained text generator outperforms the DPO model in terms of rewards and KL-divergence with low \(\) values (0.025, 0.05, 0.1). At higher \(\) values (0.5 and 1.0) both methods show trends of increased KL-Divergence and lower rewards, but generally, LRML maintains a higher reward than DPO. In Figure 4(b), we note that LRML slightly outperforms DPO, SLiC, AQFL, and PADLL at \(\{0.05,0.1\}\) in terms of reward. For larger \(\) values (0.5 and 1.0), LRML shows similar trends of increased KL-Divergence and rewards like the other objective functions. A more detailed comparison between the individual discovered losses and the baselines can be found in Appendix Figure 8.

## 6 Analysis of DiscoPOP

We list all our discovered objectives in Table 1, as well as the code and mathematical representations in Appendix E. In this section, we now analyze the Log Ratio Modulated Loss, which we define as the DiscoPOP loss function, as it performs consistently high across the held-out evaluation tasks, and we provide some intuitive understanding of how it outperforms the existing state-of-the-art objectives.

### Log Ratio Modulated Loss (DiscoPOP)

The Log Ratio Modulated Loss is a dynamically weighted sum of the logistic loss (as used in DPO) and the exponential loss. The weight of each is determined through a sigmoid calculation of the

  
**Function** & **Win Rate (\%)**\(\) & **Win Rate - LC (\%)**\(\) & **Win Rate (\%)**\(\) & **Win Rate - LC (\%)**\(\) \\  & & vs. Human Preference & & vs. SFT Checkpoint \\  DPO & \(\) & \(\) & \(\) & \(54.64 0.00\) \\ SLiC & \(83.02 1.29\) & \(63.41 0.00\) & \(53.03 1.52\) & \(54.11 0.00\) \\ KTO & \(85.34 1.18\) & \(80.26 0.00\) & \(51.15 1.54\) & \(50.0 0.00\) \\ DBAGL & \(84.71 1.21\) & \(78.68 0.00\) & \(52.55 1.52\) & \(55.14 0.00\) \\ AQL & \(81.87 1.32\) & \(68.89 0.00\) & \(46.00 1.54\) & \(50.0 0.00\) \\ PADLL & \(\) & \(76.13 0.00\) & \(\) & \(\) \\ AQFL & \(85.03 1.22\) & \(76.23 0.00\) & \(49.56 1.53\) & \(50.38 0.00\) \\ CELL & \(86.33 1.14\) & \(73.72 0.00\) & \(50.35 1.52\) & \(51.90 0.00\) \\ LRML & \(\) & \(81.88 0.00\) & \(53.46 1.52\) & \(55.10 0.00\) \\ PFL & \(79.84 1.35\) & \(69.23 0.00\) & \(44.12 1.52\) & \(44.57 0.00\) \\   

Table 3: **TL;DR - Held Out Summarization Task Win-rate of various preference optimization functions in the summarization task was evaluated with the Alpaca Eval 2.0 calculations, against a subset of the test set (694 samples). The baseline outputs are the human-generated preferences, and the model after SFT (see Appendix C for details). Note that the standard error in the LC win-rate has been rounded down because of values \(<0.001\). We have highlighted the scores with means overlapping the standard error of the best score.**

[MISSING_PAGE_EMPTY:8]

In Figure 9 and Figure 10 of the Appendix, we plot the LRML objective function for \(\{0.01,0.025,0.05,0.1,0.25,0.5,1,2.5,5\}\) against DPO. When \(\) is high, the DiscoPOP objective function takes the form of the DPO log sigmoid loss. During training on \(=0.01\), we observed that DiscoPOP gets stuck in generating predominantly negative reviews (resulting in a reward score of \( 0.15\)). We hypothesize that the loss is stuck in the local minima to the left with a negative difference in log ratios. While training with \(\{2.5,5.0\}\), we observed that the model collapsed after a sharp spike in the loss and subsequently having loss value 0 and NaN outputs. This is potentially due to a large gradient in the non-convex part, which could be amended with gradient clipping.

## 7 Related Work

**Evolution and Search with Large Language Models**. LLMs provide a fast and automated way to create multiple candidate solutions for a problem stated in natural language (Song et al., 2024), which makes them powerful tools for driving population-based search procedures. Various recent works have applied this approach to coding problems (Romera-Paredes et al., 2024), neural architecture search (Chen et al., 2024; Holt et al., 2024), virtual robotic design settings (Lehman et al., 2023), reward functions (Ma et al., 2023; Yu et al., 2023), and algorithm heuristics (Liu et al., 2024). Finally, recently LLMs have shown to be capable of acting as recombination operators for black-box optimization with Evolution Strategies (Lange et al., 2024) and for Quality-Diversity approaches (Lim et al., 2024).

**Automated Discovery for Machine Learning**. There are many other approaches to automating the discovery of generalizable machine learning algorithms. Some prior works explore the space of ML functions using genetic algorithms and a hand-crafted domain-specific language for reinforcement learning algorithms (Co-Reyes et al., 2021), curiosity algorithms (Alet et al., 2020), and optimizers (Chen et al., 2024). Other works instead parameterize a transferrable objective function using neural networks and optimize them with evolution strategies or meta-gradients. For example, Lu et al. (2022), Jackson et al. (2024), Houthooft et al. (2018), Alfano et al. (2024), Kirsch et al. (2019), Oh et al. (2020), Jackson et al. (2024) discover policy optimization objectives, Metz et al. (2022) evolves neural network optimizers, and Lange et al. (2023, 2024) evolve blackbox optimizers. Moreover, automatically discovering closed-form functions (i.e., symbolic regression), works exist that leverage RL (Petersen et al., 2020), gradient descent (Kacprzyk et al., 2024), RL with evolution strategies (Mundhenk et al., 2021), pre-training transformers (Biggio et al., 2021) and hybrid combinations of pre-training transformers, which are further refined with RL and evolution strategies (Holt et al., 2023).

**Preference Optimization Algorithms**. While the reduction to supervised learning makes DPO and alternatives easier to use, other approaches have sought to simplify the RL step, including using variants of REINFORCE (Ahmadian et al., 2024; Gemma-Team et al., 2024) as well as more fine-grained feedback (Wu et al., 2024) through preferences over individual steps in the reasoning process (Uesato et al., 2022; Lightman et al., 2023) or reward redistribution (Chan et al., 2024). Others use iterative offline training interleaved with sampling from the policy model and obtaining a preference ranking from themselves (Xu et al., 2023), another judge LLM (Guo et al., 2024), or an oracle (Swamy et al., 2024).

## 8 Conclusion

**Summary**. In this paper, we proposed and used LLM-driven objective discovery to generate novel offline preference optimization algorithms. Specifically, we were able to discover high-performing preference optimization losses that achieve strong performance across held-out evaluation tasks, with the highest performing providing new insights into what an optimal objective may need to possess, such as being a blend of logistic and exponential losses and possibly being non-convex.

**Limitations & Future work**. There are multiple limitations to our current approach. First, we have only scratched the surface of how to generate LLM objective proposals effectively. Initial exploratory experiments using techniques such as temperature sampling or worst-to-best performance sorting in the context did not yield significant improvements. But one could imagine leveraging more information about the training runs and automatically tuning instruction prompt templates. E.g. by providing entire learning curve plots to a Visual Language Model (see Figure 13) or by meta-meta-optimizing (Lu et al., 2023) the LLM prompt. Second, the highest-performing loss re-purposed \(\)in the traditional sense, making it affect the functional behavior and the KL penalty of the model with respect to the base model. This motivates future work to study different forms, with perhaps multiple floating point parameters in the form, that each could be tuned separately. Although we provided an initial analysis sweep over this one single parameter and observed some instances of the functional behavior leading to instability of training the model, a further multi-parameter analysis, reformulating the objective, would be beneficial for future work. Finally, our work uses closed-source models (GPT-4) to generate code, which limits reproducibility and is costly to run. Future work could use the produced models _themselves_ to generate code, resulting in code-level self-improvement.

**Broader Impact and Ethical Considerations**. This paper presents an LLM-driven discovery in-context learning pipeline that is used to generate better-performing novel offline preference optimization algorithms. However, misuse of the pipeline as a tool or training an LLM to produce undesirable, unethical, or harmful outputs could be possible by a user. Furthermore, due to the use of LLMs and training of LLMs, the outputs are susceptible to hallucinations, motivating all outputs of the LLMs to always have a content filter applied to the outputs. Finally, this work takes a small step towards code-level self-improvement in language models, which could potentially result in unintended behaviors.

This work was supported by Azure sponsorship credits granted by Microsoft's AI for Good Research Lab and by Microsoft's Accelerate Foundation Models Academic Research initiative. The hardware used for training was sponsored by GoodAI. SH is funded by AstraZeneca. AJC is funded by a Microsoft Research and EPSRC ICASE scholarship award. CL and RTL were supported by Sakana AI at the time of this work. The code can also be accessed at https://github.com/samholt/DiscoPOP.