# Parameter-free Clipped Gradient Descent Meets Polyak

Yuki Takezawa\({}^{1,2}\), Han Bao\({}^{1,2}\), Ryoma Sato\({}^{3}\), Kenta Niwa\({}^{4}\), Makoto Yamada\({}^{2}\)

\({}^{1}\)Kyoto University, \({}^{2}\)OIST, \({}^{3}\)NII, \({}^{4}\)NTT Communication Science Laboratories

###### Abstract

Gradient descent and its variants are de facto standard algorithms for training machine learning models. As gradient descent is sensitive to its hyperparameters, we need to tune the hyperparameters carefully using a grid search. However, the method is time-consuming, particularly when multiple hyperparameters exist. Therefore, recent studies have analyzed parameter-free methods that adjust the hyperparameters on the fly. However, the existing work is limited to investigations of parameter-free methods for the stepsize, and parameter-free methods for other hyperparameters have not been explored. For instance, although the gradient clipping threshold is a crucial hyperparameter in addition to the stepsize for preventing gradient explosion issues, none of the existing studies have investigated parameter-free methods for clipped gradient descent. Therefore, in this study, we investigate the parameter-free methods for clipped gradient descent. Specifically, we propose Inexact Polyak Stepsize, which converges to the optimal solution without any hyperparameters tuning, and its convergence rate is asymptotically independent of \(L\) under \(L\)-smooth and \((L_{0},L_{1})\)-smooth assumptions of the loss function, similar to that of clipped gradient descent with well-tuned hyperparameters. We numerically validated our convergence results using a synthetic function and demonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT, and T5.

## 1 Introduction

We consider the convex optimization problem:

\[_{^{d}}f(),\] (1)

where the loss function \(f\) is convex and lower bounded. In this setting, gradient descent and its variants (Duchi et al., 2011; Kingma and Ba, 2015) are the de facto standard algorithms to minimize the loss function. The performance of the algorithm is highly sensitive to the hyperparameter settings, necessitating the careful tuning of the hyperparameters to achieve best performance. More specifically, when the loss function is \(L\)-smooth, gradient descent can achieve the optimal convergence rate \((_{0}-^{}\|_{c}^{2}}{T})\) when we set the stepsize to \(\) where \(_{0}\) is the initial parameter and \(^{}\) is the optimal solution (Nesterov, 2018). Unfortunately, parameter \(L\) is problem-specific and unavailable in practice. Thus, gradient descent must be executed in many times with different hyperparameters to identify the good hyperparameter settings, which is a very time-consuming process. Notably, when multiple hyperparameters are under consideration, this hyperparameter search becomes computationally more demanding.

Several recent studies have examined _parameter-free methods_ for tuning hyperparameters on the fly (Berrada et al., 2020; Defazio and Mishchenko, 2023; Orvieto et al., 2022; Jiang and Stich, 2023; Ivgi et al., 2023; Khaled et al., 2023; Orabona and Tommasi, 2017; Carmon and Hinder, 2022).1These methods automatically adjust the stepsize during the training and are guaranteed to converge to the optimal solution without tuning the stepsize. In other words, the stepsize did not require tuning using the grid search. However, the existing parameter-free methods only focus on the stepsize, and parameter-free methods for other hyperparameters have not been explored. For example, in addition to the stepsize, the gradient clipping threshold is an important hyperparameter for training language models (Pascanu et al., 2013; Zhang et al., 2020, 2020, 2020).

Clipped gradient descent can achieve the convergence rate \((\|_{0}-^{}\|^{2}}{T})\) under the assumption that the loss function is \((L_{0},L_{1})\)-smooth when we use the optimal stepsize and gradient clipping threshold (Koloskova et al., 2023). In many cases, \(L_{0}\) is significantly smaller than \(L\)(Zhang et al., 2020). Thus, by comparing with the convergence rate of gradient descent \((_{0}-^{}\|^{2}}{T})\), gradient clipping often allows gradient descent to converge faster. However, we must carefully tune two hyperparameters, stepsize and gradient clipping threshold, to achieve this convergence rate. If the gradient clipping threshold is too large, the gradient clipping fails to accelerate the convergence. Moreover, if the gradient clipping threshold is too small, gradient clipping deteriorates rather than accelerating the convergence rate. _Can we develop a parameter-free method whose convergence rate is asymptotically independent of \(L\) under \((L_{0},L_{1})\)-smoothness?_

In this study, we investigate a parameter-free method for clipped gradient descent. First, we provide the better convergence rate of Polyak stepsize (Polyak, 1987) under \((L_{0},L_{1})\)-smoothness. We discover that the convergence rate of Polyak stepsize matches that of clipped gradient descent with well-tuned stepsize and gradient clipping threshold. Although the convergence rate of Polyak stepsize is asymptotically independent of \(L\) under \((L_{0},L_{1})\)-smooth assumption as clipped gradient descent, it still requires the minimum loss value, which is a problem-specific value. Thus, we make Polyak stepsize parameter-free without losing this property under \((L_{0},L_{1})\)-smoothness by proposing **Inexact Polyak Stepsize**, which converges to the optimal solution without any problem-specific parameters. We numerically evaluated Inexact Polyak Stepsize using a synthetic function and neural networks, validating our theory and demonstrating the effectiveness of Inexact Polyak Stepsize.

## 2 Preliminary

### Gradient descent & \(L\)-smoothness

One of the most fundamental algorithms for solving Eq. (1) represents the gradient descent:

\[_{t+1}=_{t}-_{t} f(_{t}),\]

where \(_{0}^{d}\) is the initial parameter and \(_{t}>0\) is the stepsize at \(t\)-th iteration. To ensure that gradient descent converges to the optimal solution quickly, we must carefully tune the stepsize \(_{t}\). When the stepsize is too large, the training collapses. By contrast, when the stepsize is too small, the convergence rate becomes too slow. Thus, we must search for a proper stepsize as the following theorem indicates.

**Assumption 1** (\(L\)-smoothness).: _There exists a constant \(L>0\) that satisfies the following for all \(,^{d}\):_

\[\| f()- f()\| L\|-\|.\] (2)

**Theorem 1** (Nesterov (2018, Corollary 2.1.2)).: _Assume that \(f\) is convex and \(L\)-smooth, and there exists an optimal solution \(^{}_{^{d}}f()\). Then, gradient descent with stepsize \(_{t}=\) satisfies_

\[f(})-f(^{})(_{0}-^{}\|^{2}}{T}),\] (3)

_where \(}_{t=0}^{T-1}_{t}\) and \(T\) is the number of iterations._

### Clipped gradient descent & \((L_{0},l_{1})\)-smoothness

Gradient clipping is widely used to stabilize and accelerate the training of gradient descent (Pascanu et al., 2013; Devlin et al., 2019). Let \(c>0\) be the threshold for gradient clipping. Clipped gradient descent is given by:

\[_{t+1}=_{t}-_{t}\{1,_{t}) \|}\} f(_{t}).\] (4)Many prior studies investigated the theoretical benefits of gradient clipping (Koloskova et al., 2023; Zhang et al., 2020a,b,c; Li and Liu, 2022; Sadiev et al., 2023). Zhang et al. (2020b) experimentally found that the gradient Lipschitz constant decreases during the training of various neural networks and is highly correlated with gradient norm \(\| f()\|\). To describe this phenomenon, Zhang et al. (2020a) introduced a novel smoothness assumption called \((L_{0},L_{1})\)-smoothness. Then, it has been experimentally demonstrated that the local gradient Lipschitz constant \(L_{0}\) is thousands of times smaller than the global gradient Lipschitz constant \(L\).

**Assumption 2** (\((L_{0},L_{1})\)-smoothness).: _There exists constants \(L_{0}>0\) and \(L_{1}>0\) that satisfy the following for all \(,^{d}\) with \(\|-\|}\):_

\[\| f()- f()\|(L_{0}+L_{1}\| f()\|)\| -\|.\] (5)

Note that \((L_{0},L_{1})\)-smoothness is strictly weaker than \(L\)-smoothness because \((L_{0},L_{1})\)-smoothness covers \(L\)-smoothness by taking \(L_{1}=0\). Using the \((L_{0},L_{1})\)-smoothness assumption, the convergence rate of clipped gradient descent was established as follows.

**Theorem 2** (Koloskova et al. (2023, Theorem 2.3)).: _Assume that \(f\) is convex, \(L\)-smooth, and \((L_{0},L_{1})\)-smooth, and there exists an optimal solution \(^{}_{^{d}}f()\). Then, clipped gradient descent with \(_{t}=}\) and \(c=}{L_{1}}\) satisfies:_

\[f(})-f(^{})(\|_{0} -^{}\|^{2}}{T}+^{2}\|_{0}-^{}\|^{4}}{T^ {2}}),\] (6)

_where \(}_{t=0}^{T-1}_{t}\) and \(T\) is the number of iterations._

When the number of iterations \(T\) is large, the first term \((\|_{0}-^{}\|^{2}}{T})\) becomes dominant, and the convergence rate of clipped gradient descent is asymptotically independent of \(L\). Gradient clipping allows for the use of a larger stepsize, and thus, gradient descent converges faster because of \(L_{0} L\). We can interpret \(L L_{0}+L_{1}_{}\| f()\|\). The stepsize of gradient descent in Theorem 1 is \(+L_{1}_{}\| f()\|}\), which is typically very small. By comparing with gradient descent, the coefficient multiplied by the gradient of clipped gradient descent in Theorem 2 is \(\{},\| f(_{t})\|}\}\), which is larger than \(+L_{1}_{}\| f()\|}\). Specifically, when parameter \(\) is close to the optimal solution \(^{}\) (i.e., \(\| f()\|\) is small), clipped gradient descent can use a larger stepsize and then reach the optimal solution faster than gradient descent.

### Polyak stepsize

When \(f\) is convex, \(_{t+1}\) and \(_{t}\) generated by gradient descent satisfy \(\|_{t+1}-^{}\|^{2}\|_{t}-^{}\|^{2}-2_ {t}(f(_{t})-f(^{}))+_{t}^{2}\| f(_{t})\|^{2}\). By minimizing the right-hand side, we can derive well-known Polyak stepsize (Polyak, 1987):

\[_{t}=_{t})-f^{}}{\| f(_{t})\|^{2}},\] (7)

where \(f^{} f(^{})\). When \(f\) is \(L\)-smooth, gradient descent with Polyak stepsize converges to the optimal solution as quickly as gradient descent with \(_{t}=\).

**Theorem 3** (Hazan and Kakade (2019, Theorem 1)).: _Assume that \(f\) is convex and \(L\)-smooth, and there exists an optimal solution \(^{}_{^{d}}f()\). Then, gradient descent with Polyak stepsize Eq. (7) satisfies:_

\[f(})-f(^{})(_{0}-^{}\|^{2}}{T}),\] (8)

_where \(}_{t=0}^{T-1}_{t}\) and \(T\) is the number of iterations._

In addition to the \(L\)-smooth setting, Polyak stepsize is known to cause gradient descent to converge to the optimal solution with the optimal rate among various settings, e.g., non-smooth convex, smooth convex, and strongly convex settings (Hazan and Kakade, 2019).

Improved convergence result of Polyak stepsize

Before proposing a parameter-free method for clipped gradient descent, in this section, we present a new convergence analysis of Polyak stepsize under \((L_{0},L_{1})\)-smoothness. Surprisingly, our new analysis reveals that Polyak stepsize achieves exactly the same convergence rate as _clipped_ gradient descent with appropriate hyperparameters. A bunch of prior studies established the convergence rates of Polyak stepsize, and it is well-known that Polyak stepsize allows gradient descent to converge as fast as the optimal stepsize. However, our theorem finds that Polyak stepsize achieves a faster convergence rate than gradient descent with the optimal stepsize as clipped gradient descent, and none of the existing studies have found this favorable property of Polyak stepsize.

### Connection between Polyak stepsize and clipped gradient descent

Under \((L_{0},L_{1})\)-smoothness, we can obtain the following results.

**Proposition 1**.: _Assume that \(f\) is convex and \((L_{0},L_{1})\)-smooth. Then, Polyak stepsize Eq. (7) satisfies:_

\[\{},\| f(_{t}) \|}\}_{t})-f^{}}{\| f(_{t})\|^{2}}.\] (9)

Proof.: Assumption 2 and Lemma 2 imply

\[_{t})-f^{}}{\| f(_{t})\|^{2}} +L_{1}\| f(_{t})\|)}.\]

When \(\| f(_{t})\|<}{L_{1}}\), Polyak stepsize is bounded from below by \(}\). When \(\| f(_{t})\|}{L_{1}}\), we have

\[+L_{1}\| f(_{t})\|)}\| f( _{t})\|}.\]

Therefore, we can conclude the statement. 

Under \(L\)-smoothness, the lower bound of Polyak stepsize was obtained as follows.

**Proposition 2** (Jiang and Stich (2023, Lemma 15)).: _Assume that \(f\) is convex and \(L\)-smooth. Then, Polyak stepsize Eq. (7) satisfies:_

\[_{t})-f^{}}{\| f(_ {t})\|^{2}}.\] (10)

By comparing Propositions 1 and 2, Proposition 1 shows that Polyak stepsize does not become excessively small when the parameter approaches the optimal solution (i.e., \(\| f()\|\) approaches zero), similar to clipped gradient descent. If we choose the stepsize and gradient clipping threshold as in Theorem 2, clipped gradient descent can be written as follows:

\[_{t+1}=_{t}-\{},\| f( _{t})\|}\} f(_{t}).\] (11)

Thus, Proposition 1 implies that Polyak stepsize can be regarded as internally estimating the hyperparameters for clipped gradient descent, as shown in Theorem 2.

### Convergence analysis of Polyak stepsize under \((L_{0},L_{1})\)-smoothness

Based on the relationship between Polyak stepsize and clipped gradient descent in Sec. 3.1, we provide a new convergence result for Polyak stepsize under \((L_{0},L_{1})\)-smoothness. The proof is deferred to Sec. A.

**Theorem 4**.: _Assume that \(f\) is convex, \(L\)-smooth, and \((L_{0},L_{1})\)-smooth, and there exists an optimal solution \(^{}_{^{d}}f()\). Let \(T\) be the number of iterations and define \(_{0 t T-1}f(_{t})\). Then, gradient descent with Polyak stepsize Eq. (7) satisfies:_

\[f(_{})-f(^{})( \|_{0}-^{}\|^{2}}{T}+^{2}\|_{0}-^{ }\|^{4}}{T^{2}}).\] (12)By comparing Theorem 4 with Theorem 2, the convergence rate of Polyak stepsize is the same as that of clipped gradient descent. Thus, Polyak stepsize can converge faster than the optimal stepsize given in Theorem 1 when \(L_{0} L\). Many prior studies analyzed the convergence rate of Polyak stepsize and discussed the relationship between Polyak stepsize and gradient descent with the optimal stepsize (Polyak, 1987; Loizou et al., 2021; Galli et al., 2023; Berrada et al., 2020). However, they only recognized Polyak stepsize as making gradient descent converge with the same convergence rate as the optimal stepsize, and none of the prior studies have found this relationship between Polyak stepsize and clipped gradient descent. Our new convergence result is the first to discover that the Polyak stepsize can achieve the same convergence rate not only as gradient descent with an appropriate stepsize but also as clipped gradient descent with an appropriate stepsize and gradient clipping threshold.

## 4 Making clipped gradient descent parameter-free

In the previous section, we found that the convergence rate of Polyak stepsize is asymptotically independent of \(L\) under \((L_{0},L_{1})\)-smoothness as clipped gradient descent with appropriate hyperparameters. However, Polyak stepsize requires the minimum loss value \(f^{}\), which is a problem-specific parameter. In this section, we propose a method that can remove the prior knowledge of \(f^{}\) from Polyak stepsize without losing the property of asymptotic independence of \(L\) under \((L_{0},L_{1})\)-smoothness.

### Inexact Polyak Stepsize

To make Polyak stepsize parameter-free, several prior studies have proposed the use of lower bound of \(f^{}\) instead of \(f^{}\)(Loizou et al., 2021; Orvieto et al., 2022; Jiang and Stich, 2023). The loss functions commonly used in machine learning models are non-negative. Thus, the lower bound of \(f^{}\) is trivially obtained as zero and is not a problem-specific parameter. By utilizing this lower bound, a straightforward approach to make Polyak stepsize independent of problem-specific parameters is replacing \(f^{}\) in Polyak stepsize with the lower bound \(l^{}\) as follows:

\[_{t}=_{t})-l^{}}{\| f(_{t})\|^{2}}.\] (13)

However, the stepsize in Eq. (13) becomes excessively large as the parameter approaches the optimal solution, and it does not lead to the optimal solution (Loizou et al., 2021). This is because \(\| f(_{t})\|\) approaches zero, while \(f(_{t})-l^{}\) approaches \(l^{}-l^{}(>0)\), which makes the stepsize in Eq. (13) excessively large as the parameter approaches the optimal solution. To mitigate this issue, DecSPS (Orvieto et al., 2022) and AdaSPS (Jiang and Stich, 2023), which are parameter-free methods based on Polyak stepsize that use \(l^{}\) instead of \(f^{}\), make the stepsize monotonically non-increasing to converge to the optimal solution.

However, making the stepsize monotonically non-increasing loses the fruitful property that the convergence rate of Polyak stepsize is asymptotically independent of \(L\) as clipping gradient descent under \((L_{0},L_{1})\)-smoothness. This is because Polyak stepsize and clipped gradient descent make the convergence rate asymptotically independent of \(L\) by increasing the stepsize when the parameter approaches the optimal solution. In fact, we evaluated DecSPS and AdaSPS with a synthetic function in Sec. 6.1, demonstrating that the convergence deteriorates as \(L\) increases.

```
1:Input: The number of iterations \(T\) and lower bound \(l^{}\).
2:\(f^{},^{} f(_{0}),_{0}\).
3:for\(t=0,1,,T-1\)do
4:\(_{t+1}_{t}-_{t})-l^{}}{_{t})\|^{2}}} f(_{t}).\)
5:if\(f(_{t+1}) f^{}\)then
6:\(f^{},^{} f(_{t+1}),_{t+1}\).
7:return\(^{}\). ```

**Algorithm 1** Inexact Polyak StepsizeTo address this issue, we propose **Inexact Polyak Stepsize**, whose details are described in Alg. 1. As discussed above, we cannot make the stepsize decrease to maintain the asymptotic independence of \(L\) under \((L_{0},L_{1})\)-smoothness. Thus, we set the stepsize as follows:

\[_{t}=_{t})-l^{}}{\| f(_{t})\|^{2}},\] (14)

where \(T\) denotes the number of iterations. Instead of making the stepsize decrease, we propose returning the parameter for which the lowest loss is achieved as the final parameter.

### Convergence analysis of Inexact Polyak Stepsize

The following theorem provides the convergence rate of Inexact Polyak Stepsize. The proof is deferred to Sec. B.

**Theorem 5**.: _Assume that \(f\) is convex, \(L\)-smooth, and \((L_{0},L_{1})\)-smooth, and there exists an optimal solution \(^{}_{^{d}}f()\). Let \(T\) be the number of iterations and \(^{2} f^{}-l^{}\). Then, \(\) generated by Alg. 1 satisfies:_

\[f()-f(^{})(\|_{0}-^{}\|^{2}+^{2}}{}+^{2}\|_{0}-^{ }\|^{4}}{T}+^{2}L^{4}}{L_{0}^{2}T}).\] (15)

Asymptotic independence of \(L\):When the number of iterations \(T\) is large, only the first term \((\|_{0}-^{}\|^{2}+^{2}}{})\) becomes dominant in the convergence rate, which does not depend on \(L\). Thus, Theorem 5 shows that Inexact Polyak Stepsize successfully inherits the favorable property of Polyak stepsize under \((L_{0},L_{1})\)-smoothness. In addition to Inexact Polyak Stepsize, DecSPS (Orvieto et al., 2022) and AdaSPS (Jiang and Stich, 2023) have been proposed as parameter-free methods that use \(l^{}\) instead of \(f^{}\) in Polyak stepsize. However, these prior methods fail to inherit the favorable property of Polyak stepsize, and their convergence rates deteriorate when \(L\) is large because these methods decrease the stepsize during the training. In fact, we evaluated DecSPS and AdaSPS with a synthetic function in Sec. 6.1, demonstrating that convergence rates of DecSPS and AdaSPS are degraded when \(L\) becomes large, whereas the convergence rate of Inexact Polyak Stepsize does not depend on \(L\).

Removing dependence on \(D_{T}\):The convergence rates of DecSPS and AdaSPS depend on \(D_{T}(:=_{0 t T}\|_{t}-^{}\|)\). Thus, strictly speaking, these convergence rates cannot show that DecSPS and AdaSPS converge to the optimal solution because \(D_{T}\) may increase as the number of iterations \(T\) increases. For instance, if \(D_{T}\) increase with \((T^{})\), the convergence rate of AdaSPS is \((L+L^{2})\), which does not show that AdaSPS converges to the optimal solution. In contrast, the convergence rate in Eq. (15) depends on only \(\|_{0}-^{}\|\). Theorem 5 indicates that Inexact Polyak Stepsize converges to the optimal solution.

Convergence rate with respect to \(T\):Inexact Polyak Stepsize successfully achieves the asymptotic independence of \(L\), while it slows down the convergence rate with respect to the number of iterations \(T\) by comparing clipped gradient descent with proper hyperparameters. The convergence rate of Inexact Polyak Stepsize \((}{})\) is not optimal in terms of \(T\), and there may be room to improve this rate. For instance, the adaptive methods proposed by Hazan and Kakade (2019) might be used to

   Algorithm & Convergence Rate & Assumption \\  DecSPS (Orvieto et al., 2022)\({}^{(a)}\) & \((^{-1}\}D_{T}^{2}+^{2}}{})\) & 1 \\ AdaSPS (Jiang and Stich, 2023)\({}^{(a)}\) & \((^{2}}{}+D_{T}^{4}}{T})\) & 1 \\ Inexact Polyak Stepsize (This work) & \((\|_{0}-^{}\|^{2}+^{2}}{ }+^{2}\|_{0}-^{}\|^{4}}{T}+^{2} L^{4}}{L_{0}^{2}T})\) & 1, 2\({}^{(b)}\) \\    (a) We present the convergence rates of DecSPS and AdaSPS in the deterministic setting to compare DecSPS, AdaSPS, and Inexact Polyak Stepsize in the same deterministic setting, while Orvieto et al. (2022) and Jiang and Stich (2023) also analyzed the rate rates in the stochastic setting. (b) If \(f\) is \(L\)-smooth, \(f\) is \((L_{0},L_{1})\)-smooth because \((L_{0},L_{1})\)-smoothness assumption is strictly weaker than \(L\)-smoothness assumption. (c) If \(f\) is \(L\)-smooth, \(f\) is \((L_{0},L_{1})\)-smooth because \((L_{0},L_{1})\)-smoothness assumption is strictly weaker than \(L\)-smoothness assumption. (d) If \(f\) is \(L\)-smooth, \(f\) is \((L_{0},L_{1})\)-smooth because \((L_{0},L_{1})\)-smoothness assumption is strictly weaker than \(L\)-smoothness assumption. (e) If \(f\) is \(L\)-smooth, \(f\) is \((L_{0},L_{1})\)-smooth because \((L_{0},L_{1})\)-smoothness assumption is strictly weaker than \(L\)-smoothness assumption.

alleviate this issue. However, the parameter-free methods for clipped gradient descent have not been explored well in the existing studies. We believe that Inexact Polyak Stepsize is the important first step for developing parameter-free clipped gradient descent.

## 5 Related work

Gradient clipping:Gradient clipping was initially proposed to mitigate the gradient explosion problem for training RNN and LSTM (Mikolov et al., 2010; Merity et al., 2018) and is now widely used to accelerate and stabilize the training not only for RNN and LSTM, but also for various machine learning models, especially language models (Devlin et al., 2019; Raffel et al., 2019). Recently, many studies have investigated the theoretical benefits of gradient clipping and analyzed the convergence rate of clipped gradient descent under (1) \((L_{0},L_{1})\)-smoothness assumption (Koloskova et al., 2023; Zhang et al., 2020a,b) and (2) heavy-tailed noise assumption (Zhang et al., 2020c; Li and Liu, 2022; Sadiev et al., 2023). (1) Zhang et al. (2020b) found that the local gradient Lipschitz constant is correlated with the gradient norm. To describe this phenomenon, Zhang et al. (2020b), Zhang et al. (2020a), and Koloskova et al. (2023) introduced the new assumption, \((L_{0},L_{1})\)-smoothness, providing the convergence rate of clipped gradient descent under \((L_{0},L_{1})\)-smoothness. Then, they showed that gradient clipping can improve the convergence rate of gradient descent, as we introduced in Sec. 2.2. (2) Besides \((L_{0},L_{1})\)-smoothness, Zhang et al. (2020c) pointed out that the distribution of stochastic gradient noise is heavy-tailed for language models. Then, it has been shown that gradient clipping can make the stochastic gradient descent robust against the heavy-tailed noise of stochastic gradient (Li and Liu, 2022; Sadiev et al., 2023; Zhang et al., 2020c).

Parameter-free methods:Hyperparameter-tuning is one of the most time-consuming tasks for training machine learning models. To alleviate this issue, many parameter-free methods that adjust the stepsize on the fly have been proposed, e.g., Polyak-based stepsize (Berrada et al., 2020; Hazan and Kakade, 2019; Loizou et al., 2021; Mukherjee et al., 2023; Orvieto et al., 2022; Jiang and Stich, 2023), AdaGrad-based methods (Ivgi et al., 2023; Khaled et al., 2023), and Dual Averaging-based methods (Orabona and Tommasi, 2017; Defazio and Mishchenko, 2023). However, parameter-free methods for hyperparameters, except for stepsizes, have not been studied. In this work, we studied the parameter-free methods for two hyperparameters, the stepsize and gradient clipping threshold, and then proposed Inexact Polyak Stepsize, which converges to the optimal solution without tuning any hyperparameters and its convergence rate is asymptotically independent of \(L\) as clipped gradient descent with well-tuned hyperparameters.

## 6 Numerical evaluation

In this section, we evaluate our theory numerically. In Sec. 6.1, we evaluate Polyak stepsize and Inexact Polyak Stepsize using a synthetic function, varying that their convergence rates are asymptotically independent of \(L\). In Sec. 6.2, we show the results obtained using neural networks.

### Synthetic function

Setting:In this section, we validate our theory for Polyak stepsize and Inexact Polyak Stepsize using a synthetic function. We set the loss function as \(f(x)=L_{1}^{2}}{72}x^{4}+}{4}x^{2}+f^{}\), which is \((L_{0},L_{1})\)-smooth for any \(L_{0}>0\) and \(L_{1}>0\) (See Proposition 3 in Appendix). We set \(L_{0}\) to \(1\), \(_{0}\) to \(5\), \(f^{}=1\), and \(l^{}=0\) and then evaluated various methods when varying \(L_{1}\).

Results:We show the results in Fig. 1. The results indicate that gradient descent converges slowly when \(L_{1}\) is large, whereas Polyak stepsize and clipped gradient descent does not depend on \(L_{1}\). These observations are consistent with those discussed in Sec. 3, which shows that the convergence rate of Polyak stepsize is asymptotically independent of \(L\) as in clipped gradient descent. By comparing DecSPS, AdaSPS, and Inexact Polyak Stepsize, which are parameter-free methods, the convergence rates of DecSPS and AdaSPS degrade as \(L_{1}\) increases. Thus, DecSPS and AdaSPS lose the favorable property of asymptotic independence of \(L\) under \((L_{0},L_{1})\)-smoothness. In contrast, the convergence behavior of Inexact Polyak Stepsize does not depend on \(L_{1}\), which is consistent with Theorem 5, and Inexact Polyak Stepsize successfully inherits the Polyak stepsize under \((L_{0},L_{1})\)-smoothness.

### Neural networks

Setting:Next, we evaluated Inexact Polyak Stepsize using LSTM, Nano-GPT2, and T5 (Nawrot, 2023). For LSTM, Nano-GPT, and T5, we used the Penn Treebank, Shakespeare, and C4 as training datasets, respectively. For SGD and Clipped SGD, we tuned the stepsize and gradient clipping threshold on validation datasets. For Polyak stepsize, we showed the results when we set \(f^{}\) to zero. For Inexact Polyak Stepsize, Theorem 4 requires the selection of the best parameters. However, we do not need to choose this for neural networks because the parameters only reach the stationary point and do not reach the global minima. See Sec. D for the detailed training configuration. For all experiments, we repeated with three different seed values and reported the average.

Results:Figure 4 shows the loss curves, and Fig. 2 shows the final test losses for various hyper-parameters. The results indicate that Inexact Polyak Stepsize consistently outperform DecSPS and AdaSPS for all neural network architectures. Although DoG performed the best for LSTM among the parameter-free methods, the training behavior of DoG was very unstable for Nano-GPT, and the loss values were much higher than those of the other methods. Similar to DoG, Polyak stepsize outperformed all parameter-free methods for T5, but the loss values of Polyak stepsize diverged for LSTM and Nano-GPT. Thus, Inexact Polyak Stepsize can consistently succeed in training models for all neural network architectures.

Figure 1: Convergence behaviors of various methods with the synthetic function.

Figure 2: The final test loss with various hyperparameter settings. For T5, the results of DecSPS and AdaSPS were omitted because their final test loss was much larger than the others, as shown in Fig. 4. Furthermore, the results of SGD were also omitted when the final test loss became nan or infinity.

## 7 Conclusion

In this study, we proposed Inexact Polyak Stepsize, which converges to the optimal solution without hyperparameter tuning at the convergence rate that is asymptotically independent of \(L\) under \((L_{0},L_{1})\)-smoothness. Specifically, we first provided the novel convergence rate of Polyak stepsize under \((L_{0},L_{1})\)-smoothness, revealing that Polyak stepsize can achieve exactly the same convergence rate as clipped gradient descent. Although Polyak stepsize can improve the convergence under \((L_{0},L_{1})\)-smoothness, Polyak stepsize requires the minimum loss value, which is a problem-specific parameter. Then, we proposed Inexact Polyak Stepsize, which removes the problem-specific parameter from Polyak stepsize without losing the property of asymptotic independence of \(L\) under \((L_{0},L_{1})\)-smoothness. We numerically validated our convergence results and demonstrated the effectiveness of Inexact Polyak Stepsize.

Figure 3: Loss curves for LSTM, Nano-GPT, and T5. We plotted the training loss per \(100\), \(10\), and \(10\) iterations for LSTM, Nano-GPT, and T5, respectively. We plotted the test loss per one epoch, \(100\) iterations, and \(200\) iterations, respectively. For LSTM and Nano-GPT, we found that Polyak stepsize does not converge, and its loss was much larger than that of other comparison methods. Thus, to make the figure easier to read, we omit the results of Polyak stepsize and provide the complete results, including Polyak stepsize in Sec. E.