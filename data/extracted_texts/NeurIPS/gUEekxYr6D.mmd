# BiSLS/SPS: Auto-tune Step Sizes for

Stable Bi-level Optimization

Chen Fan

University of British Columbia

Gaspard Chone-Ducasse

Ecole Normale Superieure

Mark Schmidt

Christos Thrampoulidis

University of British Columbia

###### Abstract

The popularity of bi-level optimization (BO) in deep learning has spurred a growing interest in studying gradient-based BO algorithms. However, existing algorithms involve two coupled learning rates that can be affected by approximation errors when computing hypergradients, making careful fine-tuning necessary to ensure fast convergence. To alleviate this issue, we investigate the use of recently proposed adaptive step-size methods, namely stochastic line search (SLS) and stochastic Polyak step size (SPS), for computing both the upper and lower-level learning rates. First, we revisit the use of SLS and SPS in single-level optimization without the additional interpolation condition that is typically assumed in prior works. For such settings, we investigate new variants of SLS and SPS that improve upon existing suggestions in the literature and are simpler to implement. Importantly, these two variants can be seen as special instances of general family of methods with an envelope-type step-size. This unified envelope strategy allows for the extension of the algorithms and their convergence guarantees to BO settings. Finally, our extensive experiments demonstrate that the new algorithms, which are available in both SGD and Adam versions, can find large learning rates with minimal tuning and converge faster than corresponding vanilla SGD or Adam BO algorithms that require fine-tuning.

## 1 Introduction

Bi-level optimization (BO) has found its applications in various fields of machine learning such as hyperparameter optimization [14; 17; 30; 40], adversarial training , data distillation [2; 53], neural architecture search [28; 39], neural-network pruning , and meta-learning [13; 37; 11]. Specifically, BO is used widely for problems that exhibit a hierarchical structure of the following form:

\[_{x X}F(x)=_{}[f(x,y^{*}(x);)] y ^{*}(x)=*{argmin}_{y Y}_{}[g(x,y;)].\] (1)

Here, the solution to the lower-level objective \(g\) becomes the input to the upper-level objective \(f\), and in (1) the upper-level variable \(x\) is fixed when optimizing the lower-level variable \(y\). To solve such bi-level problems using gradient-based methods requires computing the hypergradient of \(F\), which based on the chain rule is given as :

\[ F(x)=_{x}f(x,y^{*}(x))-_{xy}^{2}g(x,y^{*}(x))[_{yy}^{ 2}g(x,y^{*}(x))]^{-1}_{y}f(x,y^{*}(x)).\] (2)

In practice, the closed-form solution \(y^{*}(x)\) can be difficult to obtain, and one strategy is to run a few steps of (stochastic) gradient descent on \(g\) with respect to \(y\) to get an approximation \(\), anduse \(\) in places of \(y^{*}(x)\). We denote the stochastic hypergradient based on \(\) as \(h_{f}(x,)\) and the stochastic gradient of \(g\) with respect to \(y\) as \(h_{g}\). This leads to a general gradient-based framework for solving bi-level optimization [15; 19; 4]. At each iteration \(k\), run T (can be one or more) steps of SGD on \(y\) with a step size \(\), \(y^{k,t+1}=y^{k,t}- h_{g}^{k,t}\), then run one step on \(x\) using the approximated hypergradient:

\[x^{k+1}=x^{k}- h_{f}(x^{k},y^{k+1}), y^{k+1}=y^{k, T}.\] (3)

Based on this framework, a series of stochastic algorithms have been developed to achieve the optimal or near-optimal rate of their deterministic counterparts [7; 8]. These algorithms can be broadly divided into single-loop (\(T=1\)) or double-loop (\(T>1\)) categories .

Unlike minimizing the single-level finite-sum (convex) problem

\[F(x):=_{x}_{i=1}^{N}f_{i}(x),\] (4)

where only one learning rate is involved when using SGD, bi-level optimization involves tuning both the lower and upper-level learning rates (\(\) and \(\) respectively). This poses a significant challenge due to the potential correlation between these learning rates . Thus, as observed in Figure 1, algorithm divergence can occur when either \(\) or \(\) is large. While there is considerable literature on achieving faster rates in bi-level optimization [24; 5; 7; 8], only a few studies have focused on stabilizing its training and automating the tuning of \(\) and \(\).

This work addresses the question: **Is it possible to utilize large \(\) and \(\) without manual tuning?** In doing so, we explore the use of stochastic adaptive-step size methods, namely stochastic Polyak step size (SPS) and stochastic line search (SLS), which utilize gradient information to adjust the learning rate at each iteration [44; 29]. These methods have been demonstrated to perform well in interpolation settings with strong convergence guarantees [44; 29]. However, applying them to bi-level optimization (BO) introduces significant challenges, as follows. BO requires tuning two correlated learning rates (for lower and upper-level). The bias in the stochastic approximation of the hypergradient complicates the practical performance and convergence analysis of SLS and SPS. Other algorithmic challenges arise for both algorithms. For SLS, verifying the stochastic Armijo condition at the upper-level involves evaluating the objective at a new \((x,y^{*}(x))\) pair, while \(y^{*}(x)\) is only approximately known; For SPS, most existing variants guarantee good performance only in interpolating settings, which are typically not satisfied for the upper-level objective in BO . Before presenting our solutions to the challenges above in Sec 2, we first review the most closely related literature.

### Related Work

Gradient-Based Bi-level OptimizationPenalty or gradient-based approaches have been used for solving bi-level optimization problems [10; 45; 21]. Here we focus our discussions on stochastic gradient-based methods as they are closely related to this work. For double-loop algorithms, an early work (BSA) by Ghadimi and Wang  has derived the sample complexity of \(\) in achieving an \(\)-stationary point to be \((^{-2})\), but require the number of lower-level steps to satisfy \(T(^{-1})\). Using a warm start strategy (stobeBiO), Ji et al.  removed this requirement on \(T\). However, to achieve the same sample complexity, the batch size of stocBiO grows as \((^{-1})\). Chen et al.  removed both requirements on \(T\) and batch size by using the smoothness properties of \(y^{*}(x)\) and setting the step sizes \(\) and \(\) at the same scale. For single-loop algorithms, a pioneering work by Hong et al.  gave a sample complexity of \((^{-2.5})\), provided \(\) and \(\) are on two different scales (TTSA). By making corrections to the \(y\) variable update (STABLE), Chen et al.  improved the rate to \((^{-2})\). However, extra matrix projections required by STABLE can incur high computation cost [5; 4]. By incorporating momentum into the updates of \(x\) and \(y\) (SUSTAIN), Khanduri et al.

Figure 1: Results based on hyper-representation learning task (see Sec 4 for details). Validation loss against upper-level iterations for different values of \(\) (left, \(=0.005\)) and \(\) (right, \(=0.01\)). Unless carefully tuned, vanilla SGD-based methods for BO are very unstable.

 further improved the rate to \((^{-1.5})\). Besides these single or double-loop algorithms, a series of works have drawn ideas from variance reduction to achieve faster convergence rates for BO. For example, Yang et al.  designed the VRBO algorithm based on SPIDER . Dagreou et al. [7; 8] designed the SABA and SRBA algorithms based on SAGA and SARAH respectively, and demonstrate that they can achieve the optimal rate of \((^{-1})\)[9; 35]. Huang et al.  proposes to use Adam-type step sizes in BO. However, it introduces three sequences of learning rates \((_{k},_{k},_{k})\) that require tuning, which limits its practical usage. To our knowledge, none of these works have explicitly addressed the fundamental problem of how to select \(\) and \(\) in bi-level optimization. In this work, we focus on the alternating SGD framework (T can be \(1\) or larger), and design efficient algorithms that find large \(\) and \(\) without tuning, while ensuring the stability of training.

Adaptive Step SizeAdaptive step-size such as Adam has found great success in modern machine learning, and different variants have been proposed [25; 38; 47; 31; 32]. Here, we limit our discussions on two adaptive step sizes that are most relevant to this work. The Armijo line search is a classic way for finding step sizes for gradient descent . Vaswani et al.  extends it to the stochastic setting (SLS) and demonstrates that the algorithm works well with minimal tuning required under interpolation, where the model fits the data perfectly. This method is adaptive to local smoothness of the objective, which is typically difficult to predict a priori. However, the theoretical guarantee of SLS in the non-interpolating regime is lacking. In fact, the results in Figure 3 suggest that SLS can perform poorly for convex losses when interpolation is not satisfied. Besides SLS, another adaptive method derived from the Polyak step size is proposed by Loizou et al.  with the name stochastic Polyak step size (SPS). Loizou et al.  further places an upper bound on the step size resulting in the SPS\({}_{}\) variant. Similar to SLS, the algorithm performs well when the model is over-parametrized. Without interpolation, the algorithm converges to a neighborhood of the solution whose size depends on this upper bound.

In a later work, Orvieto et al.  make the SPS converge to the exact solution by ensuring the step size and its upper bound are both non-increasing (DecSPS ). However, enforcing monotonicity may result in the step size being smaller than decaying-step SGD and losing the adaptive features of SPS (see Figure 2, 3). In this work, we propose new versions of SLS and SPS that do not require monotonicity and extend them into the alternating SGD bi-level optimization framework (3).

Figure 2: Experiments on quadratic functions adapted from . The objective is the sum of two-dimensional functions \(f_{i}=(x-x_{i}^{*})^{T}H_{i}(x-x_{i}^{*})\), where \(H_{i}\) is positive definite and \(i=1,2\) (see Appendix B for more details). From left to right, we show: the objective value, distance to optimum, step size, and iterate trajectories.

## 2 Summary of Contributions

We discuss our main contributions in this section, which is organized as follows. First, we discuss our variants of SPS and SLS, and unify them under the umbrella of "envelope-type step-size". Then, we extend the envelope-type step size to the bi-level setting. Finally, we discuss our bi-level line-search algorithms based on Adam and SGD.

Converging SPSB and SLSB by Envelope ApproachWe first propose simple variants of SLS and SPS that converge in the non-interpolating setting while not requiring the step size to be monotonic. To this end, we introduce a new stochastic Polyak step size (SPSB). For comparison, we also recall the step-sizes of SPS\({}_{}\) and DecSPS. For all methods, the iterate updates are given as \(x_{k+1}=x_{k}-_{k} f_{i_{k}}(x^{k})\) where \(i_{k}\) is sampled uniformly from \([n]=\{1,,n\}\) at each iteration \(k\). The step-sizes \(_{k}\) are then defined as follows:

\[_{}\] (29): \[_{k}=\{}(x^{k})-f_{i_{k}}^{*}}{c\| f _{i_{k}}(x^{k})\|^{2}},_{b,0}\}\] (5) \[\] (30): \[_{0}=_{k}=}\{ }(x^{k})-l_{i_{k}}^{*}}{\| f_{i_{k}}(x^{k})\|^{2}},c_{k-1 }_{k-1}\} k 1\] (6) \[_{k}=\{}(x^{k})-l _{i_{k}}^{*}}{c_{k}\| f_{i_{k}}(x^{k})\|^{2}},_{b,k}\},\] (7)

where \(f_{i}^{*}=_{x}f_{i}(x)\), \(=}\{}(x^{0})-l_{i_{0}}^{*}}{\|  f_{i_{0}}(x^{0})\|^{2}},c_{0}_{b,0}\}\), \(c_{k}\) is non-decreasing, \(_{b,k}\) is non-increasing, and \(l_{i}^{*} f_{i}^{*}\) is any lower bound.

Unlike SPS\({}_{}\) in which \(_{b,0}\) is a constant, our upper bound \(_{b,k}\) is non-increasing. Also, unlike DecSPS in which both the step size and the upper bound are non-increasing (this is because \(_{k}}{c_{k}}_{k-1}\) and \(\{},_{b,0}}{c_{k}}\}_{k} _{b,0}}{c_{0}}\)[36, Lemma 1]), we simplify the recursive structure and do not require the step-size to be monotonic. As we empirically observe in Figure 3, the step size of DecSPS is similar to that of decaying SGD and in fact can be much smaller. Interestingly, the resulting performance of DecSPS is worse than SPS\({}_{}\) despite SPS\({}_{}\) eventually becoming unstable once the iterates get closer to the neighborhood of a solution and the step-size naturally behaves erratically. This is not unexpected due to small gradient norms (note the division by gradient-norm in (5)) and dissimilarity between samples in the non-interpolating scenario. Moreover, note that the adaptivity of SPS in the early stage seems to be lost in DecSPS due to the monotonicity of the latter. On the other hand, SPSB not only takes advantage of the large SPS steps that leads to fast convergence, but also stays regularized due to the non-increasing upper bound \(_{b,k}\) in (19). These observations are further supported by the experiments on quadratic functions given in Figure 2, where we observe the fast convergence of SPSB and the instability of SPS\({}_{}\). Furthermore, SPSB is significantly more robust to the choice of \(_{b,0}\) than decaying-step SGD as shown by Figure 2(a). Motivated by the good practical performance of SPSB, we take a similar approach for SLS. The

Figure 3: Binary linear classification on w8a dataset using logistic loss . (a) Minimum loss of decaying-step SGD and SPSB for different \(_{b,0}\)’s. (b)(c) Train loss and step size against iterations, respectively. We choose \(c=1\) and \(=1\) for SPS\({}_{}\) and SLS respectively; \(c_{k}=\) for DecSPS ; \(c_{k}=1\) and \(_{b,k}=}{}\) for SPSB ; \(=0.1\) and \(_{b,k}=}{}\) for SLSB ; \(_{b,k}=}{}\) for decaying-step SGD. For (b) and (c), we set \(_{b,0}=1000\) for all algorithms.

SLS proposed and analyzed by Vaswani et al.  starts with \(_{b,0}\) and in each iteration \(k\) finds the largest \(_{k}_{b,0}\) that satisfies:

\[f_{i_{k}}(x_{k}-_{k} f_{i_{k}}(x_{k})) f_{i_{k}}(x_{k})- _{k}\| f_{i_{k}}(x_{k})\|^{2}, 0<<1.\] (8)

To ensure its convergence without interpolation, we replace \(_{b,0}\) with appropriate non-increasing sequence \(_{b,k}\). We name this variant of SLS as SLSB. Interestingly, the empirical performance and step size of SLSB are similar to those of SPSB (see Figure 3). This can be explained by observing that the step sizes of SPSB and SLSB share similar envelope structures, as follows (see Lemma 1 in Appendix A):

\[: \{},_{b,k}\}_{k}=\{ }(x^{k})-l_{i_{k}}^{*}}{c\| f_{i_{k}}(x^{k})\|^{2}}, _{b,k}\}, 0<c,\] \[: \{)}{L_{}},_{b,k}\}_{k} \{}(x^{k})-l_{i_{k}}^{*}}{c\| f_{i_{k}}(x^{k})\|^{ 2}},_{b,k}\}, 0<<1.\]

Therefore, we unify their analysis based on the following generic _envelope-type step size_:

\[_{k}=\{\{_{l,k},_{k}\},_{b,k}\}, _{l,k}=\{w,_{b,k}\},\] (9)

where \(>0\), \(_{b,k}\) is non-increasing, and \(_{k}\) satisfies \(_{l,k}:=\{,_{b,k}\}_{k}_{b,k}\). We show that this envelope-type step size converges at a rate \((})\) and \(()\) for convex and strongly-convex losses respectively.

Envelope Step Size for Bi-level Optimization (BiSPS)We extend the use of envelope-type step sizes to the bi-level setting. The step sizes for upper and lower-level objectives of our general envelope-type method are:

\[_{k}=\{\{_{l,k},_{k}\},_{b,k}\}_{l,k}_{k}_{b,k}\] (10) \[_{k,t}=\{,y^{k,t};)-g(x^{k},y _{x^{k},}^{*};)}{p\|_{y}g(x^{k},y^{k,t};)\|^{2}},_{b,k} \} t,\] (11)

where \(y_{x^{k},}^{*}\) is the minimizer of the function \(g(x^{k},;)\), and \(_{l,k}\), \(_{b,k}\), and \(_{b,k}\) are three non-increasing sequences. Note that \(_{b,k}\) is fixed over the lower-level iterations for a given \(k\), therefore this is equivalent to running \(T\) steps of \(_{}\) to minimize the function \(g\) at each upper iteration \(k\). However, the decrease in the upper bound \(_{b,k}\) with \(k\) is crucial to guarantee the overall convergence of the algorithm (see Theorem 3). Starting from the general step-size rules in (10) and (11), our bi-level extension of SPS (BiSPS) follows by setting \(_{k}\) in the form of SPS computed using stochastic hypergradient \(h_{f}^{k}\). That is,

\[_{k}=,y^{k+1};)-l_{f(,y^{k+1}; )}^{*}}{p\|h_{f}^{k}\|^{2}},_{l,k}=}{},_{b,k}=}{},\] (12)

where \(_{l,0}_{b,0}\) and \(l_{f(,y^{k+1};)}^{*}\) is a lower bound for \(_{x}f(x,y^{k+1};)\). For computing \(h_{f}^{k}\), we can take a similar approach as previous works  that use a Neumann series approximation

\[h_{f}^{k}=_{x}f(x^{k},y^{k+1};)-_{xy}g(x^{k},y^{k+1};_{0}) }_{j=1}^{}(I-_{yy}^{2}g(x^{k},y^{k+1}; _{j}))_{y}f(x^{k},y^{k+1};),\] (13)

where \(\) is sampled uniformly from \([N]\) and \(N\) is the total number of samples. For BiSPS, we use the same sample for \(f(x^{k},y^{k+1};)\) and \(_{x}f(x^{k},y^{k+1};)\) when evaluating \(_{k}\) in (12). Interestingly, we also empirically observe that using independent samples for computing \(_{k}\) and \(h_{f}^{k}\) results in similar performance as using the same sample. The optimal rate of SGD for non-convex bi-level optimization is \((})\) without a growing batch size . We show that BiSPS can obtain the same rate (see Theorem 3) by taking the envelope-type step-size of the form (10) and (11). We implement BiSPS according to (12) and observe that it has better performances over decaying-step SGD with less variations across different values of \(_{b,0}\) (see Figure 4 and note that decaying-step SGD is of the form \(}{}\)).

Stochastic Line-Search Algorithms for Bi-level OptimizationThe challenge of extending SLS to bi-level optimization is rooted in the term \(y^{*}(x)\). In fact, we realize that some of the bi-level objectives are of the form \(F(x)=f(y^{*}(x))\). That is, \(f\) does not have an explicit dependence on \(x\) as in the data hyper-cleaning task . This implies that when SLS takes a potential step on \(x\), the approximation of \(y^{*}(x)\) (i.e, \((x)\)) also needs to be updated, otherwise there is no change in function values. Moreover, the use of approximation \((x)\) and the stochastic estimation error in hypergradient would not gaurantee a step size can be always found. To this end, we modify the Armijo line-search rule to be:

\[& fx^{k }-_{k}h_{f}^{k},^{k+1}(x^{k}-_{k}h_{f}^{k}) f(x^ {k},y^{k+1})-p_{k}\|h_{f}^{k}\|^{2}+,\\ & fx^{k}-_{k}A_{ k}^{-1}h_{f}^{k},^{k+1}(x^{k}-_{k}A_{k}^{-1}h_{f}^{k}) f(x^ {k},y^{k+1})-p_{k}\|h_{f}^{k}\|^{2}_{A_{k}^{-1}}+,\] (14)

where \(p,>0\) and \(A_{k}\) is a positive definite matrix such that \(A_{k}^{2}=G_{k}\).

Similar to the single-level Adam case, the matrix \(G_{k}\) in the bi-level setting is defined as \(G_{k}=(_{2}G_{k-1}+(1-_{2})(h_{f}^{k}{h_{f}^{k}}^ {T}))/(1-_{2}^{k})\). Moreover, BiSLS-Adam takes the following steps for updating the variable \(x\): \(x^{k+1}=x^{k}-_{k}A_{k}^{-1}m_{k}\) where \(m^{k+1}=_{1}m^{k}-(1-_{1})h_{f}^{k}\). The details are given in Algorithms 1 and 2. We denote the search starting point for the upper-level as \(_{b,k}\) at iteration \(k\), and denote it as \(_{b,k}^{!}\) at step \(t\) within iteration \(k\) for the lower-level. We remark the following key benefits of resetting \(_{b,k}\) and \(_{b,k}^{!}\) (by using Algorithm 2) to larger values with reference to \(_{k}\) and \(_{k}^{!}\) (respectively) at each step: (1) we avoid always searching from \(_{b,0}\) or \(_{b,0}^{0}\), thus reducing computation cost and (2) preserving an overall non-increasing (not necessarily monotonic) trend for \(_{b,k}\) and \(_{b,k}^{!}\) (improving training stability). We found that different values of \(\) all give good empirical performances (see Appendix B). The key algorithmic challenge we are facing is that during the backtracking process, for any candidate \(_{k}\), we need to compute \(^{k}:=x^{k}-_{k}h_{f}^{k}\) and approximate \(y^{*}(^{k})\) with \(^{k+1}\) (see Algorithm 1). To limit the cost induced by this nested loop, we limit the number of steps to obtain \(^{k+1}\) to be 1.

Moreover, \(\) in (14) plays the role of a safeguard that ensures a step size can be found. We set \(\) to be small to avoid finding unrealistically large learning rates while tolerating some error in the

Figure 4: Results on data distillation experiments adapted from Lorraine et al.  (see Sec 4 for details). We compare BiSPS and decaying-step SGD for different values of \(_{b,0}\) where Hessian inverse in (2) is computed based on the Identity matrix (left) or Neumann series (right). The lower-level learning rate is fixed at \(10^{-4}\).

Figure 5: Results on hyper-representation learning task (see Sec 4 for details). (a) Validation loss against upper-level iterations for comparing BiSLS-Adam/SGD to fine-tuned Adam/SGD. (b)(c) Upper (left) and lower-level (right) learning rates found by BiSLS-Adam. For the fine-tuned Adam, the optimal lower and upper-level learning rates are \((1)\) and \((10^{-4})\), respectively. BiSLS-Adam outperforms fine-tuned Adam/SGD with a starting point that is \(5\) orders of magnitude larger than the optimal step size.

hypergradient estimation (see Appendix B for experiments on the sensitivity of \(\)). In practice, we found that simply setting \(=0\) works well. In Figure 4(a), we observe that BiSLS-Adam outperforms fine-tuned Adam or SGD. Surprisingly, its training is stable even when the search starting point \(_{b,0}\) is \(5\) orders of magnitude larger than a fine-tuned learning rate (\((10^{-4})\)). Importantly, BiSLS-Adam finds large upper and lower-level learning rates in early phase (see Figure 4(b), 4(c)) for different values of \(_{b,0}\) and \(_{b,0}\) that span \(3\) orders of magnitudes. Interestingly, the learning rates naturally decay with training (also see Figure 4(c) and 4(d)). In essence, BiSLS is a **truly adaptive (no knowledge of initialization required) and robust (different initialization works) method that finds large \(\) and \(\) without tuning**. In the next section, we give the convergence results of the envelope-type step size.

## 3 Convergence Results

### Envelope-type step size for single-level optimization

We first state the assumptions, which are standard in the literature, that will be used for analyzing single-level problems. Assumption 1 is on the Lipschitz continuity of \(f\) and \(f_{i}\) in Problem 4.

**Assumption 1**.: _The individual function \(f_{i}\) is convex and \(L_{i}\)-smooth such that \(\| f_{i}(x)- f_{i}(x^{})\| L_{i}\|x-x^{}\|, i, xf\) and the overall function \(f\) is \(L\)-smooth. We denote \(L_{}_{i}L_{i}\). Furthermore, we assume there exists \(l_{i}^{*}\) such that \(l_{i}^{*} f_{i}^{*}:=_{x}f_{i}(x), i\), and \(f\) is lower bounded by \(f^{*}\) obtained by some \(x^{*}\) such that \(f^{*}=f(x^{*})\)._

The following bounded gradient assumption is also used in the analysis of convex problems [41; 33].

**Assumption 2**.: _There exists \(G>0\) such that \(\| f_{i}(x)\|^{2} G, i\)._

We first state the theorem for the envelope-type step size defined in (9) for convex functions.

**Theorem 1**.: _Suppose Assumption 1, 2 hold, each \(f_{i}\) is convex, \(C=f\), \(_{k}\) is independent of the sample \( f_{k}(x^{k})\), and choose \(_{b,k}=}{}\). Then, the envelope-type step size in (9) achieves the following rate,_

\[[f(^{K})-f(x^{*})]-x^{*}\|^{2}}{2_{l, K-1}K}+^{2}G^{2}(K)}{2_{l,K-1}K},\]

_where \(_{l,K-1}=\{,}{}\}\) and \(^{K}=_{k=0}^{K}x^{k}\)._

We were not able to give a convergence result that uses the same sample for computing the step size and the gradient. However, we empirically observe that the performance is similar when using either one or two independent samples per iteration (see Figure 2 and Appendix B). When two independent samples \(i_{k}\) and \(j_{k}\) are used per iteration, the first computes the gradient sample \( f_{i_{k}}(x^{k})\), and the other computes the step-size \(_{k}\). For example, for SPSB this gives \(_{k}=\{}(x^{k})-l_{j_{k}}^{*}}{c_{k}\| f_{j_{k}} (x^{k})\|^{2}},_{b,k}\}\). This type of assumption has been used in several other works for analyzing adaptive step sizes [27; 44; 29]. Under this assumption, we specialize the results of Theorem 1 to SPSB and SLSB, where \(_{l,K-1}=\{},}{}\}\) and \(_{l,K-1}=\{)}{L_{}},}{}\}\) respectively. Concretely, for \(K_{b,0}^{2}L_{}^{2}\), SLSB and SPSB with \(_{b,k}=}{}\) and \(c==\) achieve the following rate: \([f(^{K})-f(x^{*})]-x^{*}\|^{2}}{2_{b,0}}+G^{2}(K)}{2}\). Next, we state the result for the envelop-type step size when \(f\) is \(\)-strongly convex.

**Theorem 2**.: _Suppose a \(\)-strongly convex function \(f\) satisfying Assumptions 1 and 2, assume \(\) is a closed and convex set, and \(_{k}\) is independent of the sample \( f_{k}(x^{k})\). Then an envelope-type step size as in (9) with \(_{b,k}=}{k+1}\), \(_{b,0}\), and \(<1\) achieves the following rate_

\[[f(_{K})-f(x^{*})]}{2(K-k_{0})}e^{- k_{0}}\|x_{0}-x^{*}\|^{2}+_{b,0}^{2}G^{2}+G^{ 2} K}{2(K-k_{0})},\]

_where \(_{K}=}_{k=k_{0}}^{K-1}x^{k}\) and \(k_{0}=\{1,_{b,0}/-1\}\)._We can again apply the result of Theorem 2 to SPSB and SLSB with \(_{b,k}=}{k}\), \(_{b,0}\), \(=1/L_{}\), and \(c==\) to get an explicit rate: \([f(_{K})-f(x^{*})]}{2(K-k_{0})}(e^{}{k_{}}}\|x_{0}-x^{*}\|^{2}+_{b,0}^{2}G^{2})+G^{2}  K}{2(K-k_{0})}\), where \(k_{0}=\{1,_{b,0}L_{}-1\}\).

**Remark 1**.: _Under the envelop-type step size framework and the assumption of two independent samples, SLSB and SPSB share the same convergence rates of \((})\) and \(()\) as SGD with decaying step-size for convex and strongly-convex losses respectively. For the latter, a projection operation is required to stay in the closed and convex set \(\). These rates are not surprising because of the structure of the envelope step-size in (9). Indeed, the proof is similar to the standard proof of analogous rate for SGD with decaying step-size. Nonetheless, we include it here for completeness._

### Envelope-type step size for bi-level optimization

We start with recalling standard assumptions in BO [22; 19; 15; 4]. We denote \(z=[x;y]\) and recall the bi-level problem in (1). The first assumption is on the lower-level objective \(g\).

**Assumption 3**.: _The function \(g(x,y)\) is \(_{g}\) strongly convex in \(y\) for any given \(x\). Moreover, \( g\) is Lipschitz continuous: \(\| g(x_{1},y_{1})- g(x_{2},y_{2})\| L_{g}\|z_{1}-z_{2}\|\) (also assume that this holds true for each sampled function \(g(x,y;)\)), and \(^{2}g\) is Lipschitz continuous: \(\|^{2}g(x_{1},y_{1})-^{2}g(x_{2},y_{2})\| L_{G}\|z_{1}-z_{2}\|\). We further assume that \(\|_{xy}^{2}g(x,y)\| C_{g}\), and the condition number is defined as \(=}{_{g}}\)._

Next, we state the assumptions on the upper objective \(f\).

**Assumption 4**.: _The function \(f\) and its gradients are Lipschitz continuous. That is: \(\|f(x_{1},y_{1})-f(x_{2},y_{2})\| L_{1}\|z_{1}-z_{2}\|\) and \(\| f(x_{1},y_{1})- f(x_{2},y_{2})\| L_{f,1}\|z_{1}-z_{2}\|\). We also assume that \(\|_{y}f(x,y)\| C_{f}\)._

Furthermore, we make the following standard assumptions on the estimates of \( f\), \( g\), and \(^{2}g\).

**Assumption 5**.: _The stochastic gradients are unbiased: \(_{}[ f(x,y;)]= f(x,y)\), \(_{}[ g(x,y;)]= g(x,y)\), and \(_{}[^{2}g(x,y;)]=^{2}g(x,y)\). The variances of \( f(x,y;)\) and \(^{2}g(x,y;)]\) are bounded: \(_{}[\| f(x,y;)- f(x,y)\|^{2}]_{f}^{2}\) and \(_{}[\|^{2}g(x,y;)-^{2}g(x,y)\|^{2}]_{G} ^{2}\)._

Finally, we introduce the bounded optimal function value assumption in (15), which is used specifically for analyzing step size of the form (11) in the bi-level setting:

\[_{}[g(x,y^{*}(x);)-g(x,y_{x,}^{*};)] _{g}^{2}, x,\] (15) \[[\|_{y}g(x,y)-_{y}g(x,y;)\|^{2}] _{g}^{2}, x,y,\] (16)

where \(y^{*}(x)=_{y}g(x,y)\) and \(y_{x,}^{*}=_{y}g(x,y;)\) for a given \(x\) (recall that at any iteration \(k\), the lower-level steps in BiSPS are \(_{}\) with an upper bound \(_{b,k}\); furthermore, \(_{b,k}\) is non-increasing w.r.t. upper iteration \(k\)). The one-variable analogous assumption of (15) has been used in the analysis of \(_{}\). Here, we extend it to a two-variable function. Unlike the bounded variance assumption (16), which needs to hold true for all \(x\) and \(y\), we require (15) to hold at \(y^{*}(x)\) for any given \(x\). As mentioned previously, the closed form solution \(y^{*}(x)\) is difficult to obtain. Thus, we define the following expression by replacing \(y^{*}(x)\) with \(y\) in (2):

\[f(x,y)=_{x}f(x,y)-_{xy}^{2}g(x,y)[_{yy}^{2}g(x,y) ]^{-1}_{y}f(x,y).\] (17)

A stochastic Neumann series in (13) approximates (17) with \(x\) and \(y\) being \(x^{k}\) and \(y^{k+1}\) (respectively), also recall that \(y^{k+1}\) is an approximation of \(y^{*}(x^{k})\) by running \(T\) lower-level SGD steps to minimize \(g\) w.r.t. \(y\) for a fixed \(x^{k}\). Based on Assumptions 3, 4, and 5, we have the following results to be used in the analysis : \(\| F(x_{1})- F(x_{2})\| L_{F}\|x_{1}-x_{2}\|\), \(\|y^{*}(x_{1})-y^{*}(x_{2})\| L_{y}\|x_{1}-x_{2}\|\), and \(\|f(x,y^{*}(x))-f(x,y)\| L_{f}\|y^{*}(x)-y\|\). Furthermore, the bias in the stochastic hypergradient in (13) (denoted as \(B\)) decays exponentially with \(N\) and its variance is bounded, i.e. \([\|h_{f}^{k}-[h_{f}^{k}]\|^{2}]_{f}^{2}\) (see Appendix A for details) .

Now, we state our main theorem based on step size of the form (10) and (11).

**Theorem 3**.: _Suppose \(f\) and \(g\) satisfy Assumptions 3, 4, and 5, learning-rate upper bounds \(_{b,k}=}{}\) and \(_{l,k}=}{}\) with \(_{b,0}\) and \(_{l,0}\) satisfying \(+4L_{y}^{2}}^{2}}{_{l,0}}\) and \(_{l,0}_{b,0}\). Further assume that \(_{k}\) is independent of the stochastic hypergradient \(h_{I}^{k}\), and each sampled function \(g(x,y;)\) is convex. Then under the Assumption (15) with \(p\), \(C_{k}=\{},_{b,k}\}\), \(TL_{f}^{2}+2)}{-(1-_{g}C_{K-1})}\), and \(_{b,k}=}{k+1}\). BiSPS achieves the rate:_

\[_{k=0}^{K-1}[\| F(x^{k})\|^{2}]}(}{}+ K}{}).\] (18)

**Remark 2**.: _We further give the convergence result under the bounded variance assumption (16) in Appendix A. Theorem 3 shows that BiSPS matches the optimal rate of SGD up to a logarithmic factor without a growing batch size. We notice that the assumption (15) largely simplifies the expression on \(T\) and does not require an explicit upper bound on \(_{b,0}\). As in the single-level case, whether using one sample or two samples (which makes upper-level step-size independent of gradient) gives similar empirical performances (see Appendix B). Note that the independence assumption is only needed for the upper-level. Thus, the two-sample requirement of theorem does not apply to the lower-level problem. This is useful from computational standpoint as typical bi-level algorithms run multiple lower-level updates for each upper-level iteration._

## 4 Additional Hyper-Representation and Data Distillation Experiments

**Hyper-representation learning:** The experiments are performed on MNIST dataset using LeNet [26; 42]. We use conjugate gradient method for solving system of equations when computing the hypergradient . The upper and lower-level objectives are to optimize the embedding layers and the classifier (i.e. the last layer of the neural net), respectively (see Appendix B for details). For constant-step SGD and Adam, we tune the lower-level learning rate \(\{10.0,5.0,1.0,0.5,0.1,0.05,0.01\}\). For the upper-level learning rate, we tune \(\{0.001,0.0025,0.005,0.01,0.05,0.1\}\) for SGD, and \(\{10^{-5},5 10^{-5},10^{-4},5 10^{-4},0.001,0.01\}\) for Adam (recall that \(\) in (14) is set to \(0\)). Based on the results of Figure 6, we make the following key observations: **1 line-search at the upper-level is essential for achieving the optimal performance (Figure 6a); 2 BiSLS-Adam/SGD not only converges fast but also generalizes well (Figure 6b); 3 BiSLS-Adam/SGD is highly robust to search starting points \(_{b,0}\) and \(_{b,0}\) (Figure 6c, 6d). It addresses the fundamental question of how to tune \(\) and \(\) in bi-level optimization** (see Appendix B for additional results on search cost).

Search cost and run-time comparison:The search cost of reset option 1 in Algorithm 2 for BiSLS-SGD and BiSLS-Adam is \(89 15\) and \(115 16\) per iteration, respectively, measured in the number of condition-checking via (14). These costs are significantly reduced when option 3 is used. Concretely, Figure 7b suggests that choosing \(=2\) in option 3 results in \( 9\) evaluations of (14) per iteration for both BiSLS-Adam and BiSLS-SGD. Choosing \(=1\) in reset option 3 is equivalent to reset option 2, which further cuts down the cost to \( 4\). However, this choice forces the learning rate to be monotonically-decreasing, which leads to a slower convergence compared to other \(\)'s (see Appendix B for more details). Besides this, Figure 7a shows that a single step for

Figure 6: Validation loss (a) and accuracy (b) against iterations. (a) Comparisons between whether to use or not use line-search at the upper or lower level; (b) Generalization performance of BiSLS-Adam/SGD and fine-tuned Adam/SGD; (c) Validation loss against iterations for different values of \(_{b,0}\) (\(_{b,0}\) fixed at \(100\)). (d) Same plot as (c) but for different values of \(_{b,0}\) (\(_{b,0}\) fixed at \(10\)).

approximating the \(y^{*}(x)\) in (14) is sufficient, considering the trade-off between performance gain and extra computation overhead introduced. Most importantly, BiSPS-Adam takes less time to reach a threshold test accuracy than fine-tuned Adam (Figure 6(c)) due to a suitable (and potentially large) learning rate found using (14) (note that Figure 6(c) excludes the cost of tuning the learning rate of Adam).

**Data distillation:** The goal of data distillation is to generate a small set of synthetic data from an original dataset that preserves the performance of a model when trained using the generated data [46; 50]. We adapted the experiment set up from Lorraine et al.  to distill MNIST digits. We present the results in Figure 8, where we observe that BiSLS-SGD converges significantly faster than fine-tuned Adam or SGD, and generate realistic MNIST images (see Appendix B for more results).

## 5 Conclusion

In this work, we have given simple alternatives to SLS and SPS that show good empirical performance in non-interpolating scenario without requiring the step size to be monotonic. We unify their analysis based on a simplified envelope-type step size, and extend the analysis to the bi-level setting while designing a SPS-based bi-level algorithm. In the end, we propose bi-level line-search algorithm BiSLS-Adam/SGD that is empirically truly robust and adaptive to learning rate initialization. Our work opens several possible future directions. Given the superior performance of BiSLS, we prioritize an analysis of its convergence rates. The difficulty stems from: (a) the bias in hypergradient estimation; (b) the dual updates in \(x\) and \(y^{*}(x)\) (incurring nested loop structures); (c) the error in estimating \(y^{*}(x)\). On single-level optimization, we remark as an important direction to relax the two-sample assumption on SPSB /SLSB. Ultimately, we hope to promote further research on bi-level optimization algorithms with minimal tuning.

Figure 8: (a)(b): Comparison between BiSLS-SGD and Adam/SGD for Data Distillation on MNIST dataset. Validation loss plotted against iterations. (a) Hypergradient computed using Neumann series; (b) Inverse Hessian in (2) treated as the Identity  when computing the hypergradient; (c) Distilled MNIST images after 3000 iterations of BiSLS-SGD.

Figure 7: (a) Validation accuracy and run time (in seconds) against different number of gradient steps for approximating \(y^{*}(x)\) in (14). (b) Search cost (i.e. number of evaluations of (14) per iteration) against different \(\)’s for reset option 3 in Algorithm 2. (c) Run time (in seconds) of BiSLS-Adam and fine-tuned Adam to reach \(85\%\) validation accuracy against different number of conjugate gradient steps for computing the hypergradient.

[MISSING_PAGE_FAIL:11]

*  Prashant Khanduri, Sillang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum, 2021.
*  Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
*  Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
*  Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes, 2019.
*  Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search, 2019.
*  Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence, 2021.
*  Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In _International Conference on Artificial Intelligence and Statistics_, pages 1540-1552. PMLR, 2020.
*  Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
*  Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of learning rate, 2019.
*  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
*  Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2003.
*  Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In _International Conference on Machine Learning_, pages 2613-2621. PMLR, 2017.
*  Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution, 2022.
*  Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients, 2019.
*  Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. _arXiv preprint arXiv:1904.09237_, 2019.
*  Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. A comprehensive survey of neural architecture search: Challenges and solutions. _ACM Computing Surveys (CSUR)_, 54(4):1-34, 2021.
*  Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1723-1732. PMLR, 2019.
*  Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In _Proceedings of the 24th international conference on Machine learning_, pages 807-814, 2007.
*  Daouda Sow, Kaiyi Ji, and Yingbin Liang. On the convergence theory for hessian-free bilevel algorithms, 2022.
*  Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-Julien. Adaptive gradient methods converge faster with over-parameterization (but you should do a line-search). _arXiv preprint arXiv:2006.06835_, 2020.
*  Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates, 2021.
*  Luis Vicente, Gilles Savard, and Joaquim Judice. Descent approaches for quadratic bilevel programming. _Journal of optimization theory and applications_, 81(2):379-399, 1994.

*  Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation, 2020.
*  Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes, 2021.
*  Jorge Nocedal Stephen J Wright. Numerical optimization, 2006.
*  Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization, 2021.
*  Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review, 2023.
*  Yihua Zhang, Guanhua Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, and Sijia Liu. Revisiting and advancing fast adversarial training through the lens of bi-level optimization. In _International Conference on Machine Learning_, pages 26693-26712. PMLR, 2022.
*  Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu. Advancing model pruning via bi-level optimization, 2023.
*  Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression. _arXiv preprint arXiv:2206.00719_, 2022.

Proofs of Theorems and Additional Convergence Results

### Useful Lemmas

Lemma 1 provides more details on the envelope structure of SPSB and SLSB given in (10) and (11). The lower bound in (19) will also be used in Lemma 5 (for bounding the term \(\|y^{k+1}-y^{*}(x^{k})\|^{2}\)).

**Lemma 1**.: _Under the Assumption 1, we have the following:_

\[\{},_{b,k}\} _{k}=\{}(x^{k})-l_{i_{k}}^{*}}{c\| f_{i_{k}}(x^{ k})\|^{2}},_{b,k}\}, 0<c,\] (19) \[\{)}{L_{}},_{b,k}\} _{k}\{}(x^{k})-l_{i_{k}}^{*}}{\| f _{i_{k}}(x^{k})\|^{2}},_{b,k}\}, 0<<1.\] (20)

Proof.: The bounds in (19) have been shown in [29; 36]. For (20), the first part of the inequality has been shown in . For the second part, recall the Armijo condition (14):

\[f_{i_{k}}(x_{k}-_{k} f_{i_{k}}(x_{k})) f_{i_{k}}(x_{k})- {c}_{k}\| f_{i_{k}}(x_{k})\|^{2}, 0<<1.\]

We can then rearrange this to obtain

\[_{k}}(x_{k})-f_{i_{k}}(x_{k}-_{k}  f_{i_{k}}(x_{k}))}{\| f_{i_{k}}(x^{k})\|^{2}}}(x_{k})-f_{i_{k}}^{*}}{\| f_{i_{k}}(x^{k})\|^{2}}}(x_{k})-l_{i_{k}}^{*}}{\| f_{i_{k}}(x^{k})\|^{2}},\] (21)

where \(l_{i_{k}}^{*}\) is any lower bound for \(f_{i_{k}}^{*}\). Also recall that \(_{b,k}\) is the search starting point at iteration \(k\), hence (20) holds for SLSB. 

Lemma 2 gives the expressions for the constants \(L_{F}\), \(L_{y}\), and \(L_{f}\). The proof can be found in [15; Lem 2.2].

**Lemma 2**.: _Under Assumptions 3 and 4, we have the following:_

\[\| F(x_{1})- F(x_{2})\| L_{F}\|x_{1}-x_{2}\|,\] \[\|y^{*}(x_{1})-y^{*}(x_{2})\| L_{y}\|x_{1}-x_{2}\|,\] \[\|f(x,y^{*}(x))-f(x,y)\| L_{f}\|y^{*} (x)-y\|,\]

_where_

\[L_{f} =L_{f,1}+L_{g}}{_{g}}+}{_{g}}(L_{G }+L_{G}}{_{g}})(^{2})\] \[L_{y} =}{_{g}}()\] \[L_{F} =L_{f,1}+(L_{f,1}+L_{f})}{_{g}}+}{_{ g}}(L_{G}+L_{G}}{_{g}})(^{3})\]

Lemma 3 is on the bias and variance of the stochastic hypergradient in (13), which has the following form 

\[h_{f}^{k}=_{x}f(x^{k},y^{k+1};)-_{xy}g(x^{k},y^{k+1};_{0} )}_{j=1}^{}(I-_{yy}^{2}g(x^{k},y^{k+1}; _{j}))_{y}f(x^{k},y^{k+1};).\] (22)

Recall that the hypergradient surrogate defined in (17) based on \((x^{k},y^{k+1})\) is

\[f(x^{k},y^{k+1})=_{x}f(x^{k},y^{k+1})+_{xy}^{2}g(x^{k},y^{k+1})[_{yy}^{2}g(x^{k},y^{k+1})]^{-1}_{y}f(x^{k},y^{k+1}).\] (23)

Given a filtration \(_{k}^{}\) up to and including \(x^{k}\) and \(y^{k+1}\), the bias of the stochastic hypergradient is defined as \(B=f(x^{k},y^{k+1})-[h_{f}^{k}|_{k}^{}]\), and the variance is defined as \([\|f(x^{k},y^{k+1})-[h_{f}^{k}|_{k }^{}]\|^{2}]\). Lemma 3 has been proven in [19, Lem 1.] (also see [4, Lem 5.]). Lemmas 1, 2, and 3 will be used in the proofs of Theorems 3 and 4.

**Lemma 3**.: _Under Assumptions 3, 4, and 5, the bias and variance of the stochastic hypergradient \(h_{f}^{k}\) satisfy the following_

\[\|f(x^{k},y^{k+1})-[h_{f}^{k}| _{k}]\|C_{f}}{_{g}}(1-}{L_{g}})^{N},  k\] \[[\|f(x^{k},y^{k+1})- [h_{f}^{k}|_{k}^{{}^{}}\|]^{2}]_{ f}^{2}, k,\]

_where N is the total number of samples, and \(_{f}^{2}=_{f}^{2}+[(_{f}^{2}+L_{1}^{2})(_{G}^ {2}+2L_{g}^{2})+_{f}^{2}L_{g}^{2}]^{2}}( ^{2})\)._

Lemma 4 is on the descent of the quantity \(V^{k}:=F(x^{k})-F(x^{*})+\|y^{k}-y^{*}(x^{k})\|^{2}\). It will be used in the proofs of Theorem 3 and 4.

**Lemma 4**.: _Suppose \(F\) satisfies Assumptions 3, 4, and 5, sequences \(_{b,k}=}{}\) and \(_{l,k}=}{}\) with \(_{b,0}\) and \(_{l,0}\) satisfying \(+4L_{y}^{2}}^{2}}{_{l,0}}\) and \(_{l,0}_{b,0}\). Further assume that \(_{k}\) is independent of \(h_{f}^{k}\). Then step sizes of the form (10) and (11) achieve the following:_

\[E[V^{k+1}][V^{k}]-}{2}[\| F(x ^{k})\|^{2}]+(_{b,k}L_{f}^{2}+2)[\|y^{k+1}-y^{*}(x^{k})\|^{2}]+\]

\[_{b,k}B^{2}+(2L_{y}^{2}_{b,k}^{2}+_{b,k}^{2}}{2}) _{f}^{2}-[\|y^{k}-y^{*}(x^{k})\|^{2}], k,\] (24)

_where \(V^{k}=F(x^{k})-F(x^{*})+\|y^{k+1}-y^{*}(x^{k+1})\|^{2}\) and recall that \(F\) is the upper-level loss defined in (1)._

Proof.: We denote \([h_{f}^{k}|_{k}^{{}^{}}]=_{f}^{k}\). By the \(L_{F}\)-smoothness of the objective \(F\):

\[F(x^{k+1}) F(x^{k})+ F(x^{k}),x^{k+1}-x^{k}+}{2}\|x^{k+1}-x^{k}\|^{2}.\]

Take expectation conditioned on a filtration of past iterates \(_{k}^{{}^{}}\) (up to and include \(x^{k}\), \(y^{k+1}\)):

\[[F(x^{k+1})|_{k}^{{}^{}}]  F(x^{k})+[ F(x^{k}),x^{k+1}-x^{k} |_{k}^{{}^{}}]+}{2}[\|x^{k+1}-x^ {k}\|^{2}|_{k}^{{}^{}}]\] \[=F(x^{k})-[_{k} F(x^{k}),h_{f}^{k} |_{k}^{{}^{}}]+[_{k}^{2}| _{k}^{{}^{}}]}{2}[\|h_{f}^{k}-_{f}^{k}+ _{f}^{k}\|^{2}|_{k}^{{}^{}}]\] \[)}}{{=}}F(x^{k})-[ _{k}|_{k}^{{}^{}}] F(x^{k}),_{f}^{k} +[_{k}^{2}|_{k}^{{}^{}}]}{2 }\|_{f}^{k}\|^{2}+[_{k}^{2}|_{k}^{ {}^{}}]}{2}_{f}^{2}\] \[=F(x^{k})-[_{k}|_{k}^{{}^{ }}]}{2}\| F(x^{k})\|^{2}-[_{k}|_{k }^{{}^{}}]}{2}\|_{f}^{k}\|^{2}+[_{k}| _{k}^{{}^{}}]}{2}\| F(x^{k})-_{f}^{k}\|^{2}+\] \[[_{k}^{2}|_{k}^{{}^{ }}]}{2}\|_{f}^{k}\|^{2}+[_{k}^{2}| _{k}^{{}^{}}]}{2}_{f}^{2},\]

where (a) is by the assumption that \(_{k}\) is independent of \(h_{f}^{k}\). Then expand the term \(\| F(x^{k})-_{f}^{k}\|^{2}\) as follows:

\[\| F(x^{k})-_{f}^{k}\|^{2} =\| F(x^{k})-f(x^{k},y^{k+1})+f(x^{k },y^{k+1})-_{f}^{k}\|^{2}\] \[ 2\| F(x^{k})-f(x^{k},y^{k+1})\|^{2}+2\|f(x^{k},y^{k+1})-_{f}^{k}\|^{2}\] \[)}}{{}}2L_{f}^{2}\|y^{k+1}-y^ {*}(x^{k})\|^{2}+2B^{2},\]

where (b) is by Lemma 2 and 3. Substituting this into the above:

\[[F(x^{k+1})|F_{k}^{{}^{}}]  F(x^{k})-[_{k}|F_{k}^{{}^{}}]}{2} \| F(x^{k})\|^{2}-[_{k}|F_{k}^{{}^{}}]}{2}\| _{f}^{k}\|^{2}+[_{k}^{2}|F_{k}^{{}^{}}]}{2 }\|_{f}^{k}\|^{2}+\] \[ E[_{k}|_{k}^{{}^{}}]L_{f}^{2}\|y^{k+ 1}-y^{*}(x^{k})\|^{2}+E[_{k}|_{k}^{{}^{}}]B^{2}+ [_{k}^{2}|_{k}^{{}^{}}]}{2}_{f}^{2}\] \[)}}{{}}F(x^{k})-}{2}\| F(x^{k})\|^{2}-}{2}\|_{f}^{k}\|^{ 2}+_{b,k}^{2}}{2}\|_{f}^{k}\|^{2}+\] \[_{b,k}L_{f}^{2}\|y^{k+1}-y^{*}(x^{k})\|^{2}+_ {b,k}B^{2}+_{b,k}^{2}}{2}_{f}^{2},\]where (c) is by \(_{l,k}_{k}\) and \(_{b,k}_{k}\). Then take total expectations and subtract \(F(x^{*})\):

\[[F(x^{k+1})-F(x^{*})][F(x^{k})-F(x^{*})]-}{2}[\| F(x^{k})\|^{2}]-}{2}[\| _{f}^{k}\|^{2}]+_{b,k}^{2}}{2}[\|_ {f}^{k}\|^{2}]+\]

\[_{b,k}L_{f}^{2}[\|y^{k+1}-y^{*}(x^{k})\|^{2}]+_{b,k}B^{2 }+_{b,k}^{2}}{2}_{f}^{2}\] (25)

Now define the potential function \(V^{k}:=F(x^{k})-F(x^{*})+\|y^{k+1}-y^{*}(x^{k+1})\|^{2}\) and expand the term \(\|y^{k+1}-y^{*}(x^{k+1})\|^{2}\) as follows:

\[\|y^{k+1}-y^{*}(x^{k+1})\|^{2} =\|y^{k+1}-y^{*}(x^{k})+y^{*}(x^{k})-y^{*}(x^{k+1})\|^{2}\] \[ 2\|y^{k+1}-y^{*}(x^{k})\|^{2}+2\|y^{*}(x^{k})-y^{*}(x^{k+1})\| ^{2}\] \[}{{}}2\|y^{k+1}-y^{*}(x^{k})\|^{2 }+2L_{y}^{2}\|x^{k+1}-x^{k}\|^{2}\] \[=2\|y^{k+1}-y^{*}(x^{k})\|^{2}+2L_{y}^{2}_{k}^{2}\|h_{f}^{k }\|^{2}\] \[=2\|y^{k+1}-y^{*}(x^{k})\|^{2}+2L_{y}^{2}_{k}^{2}\|h_{f}^{k }-_{f}^{k}+_{f}^{k}\|^{2},\]

where (d) is by Lemma 2. Take expectation conditioned on \(_{k}^{{}^{}}\):

\[[\|y^{k+1}-y^{*}(x^{k+1})\|^{2}|_{k}^{{}^{ }}]  2\|y^{k+1}-y^{*}(x^{k})\|^{2}+2L_{y}^{2}_{k}^{2}\|_{f}^{k}\|^{2}+2L_{y}^{2}_{k}^{2}_{f}^{2}\] \[ 2\|y^{k+1}-y^{*}(x^{k})\|^{2}+2L_{y}^{2}_{b,k}^{2}\| _{f}^{k}\|^{2}+2L_{y}^{2}_{b,k}^{2}_{f}^{2}.\]

Then, take total expectations:

\[[\|y^{k+1}-y^{*}(x^{k+1})\|^{2}] 2[\|y^{k+1}-y^{*}(x^{k}) \|^{2}]+2L_{y}^{2}_{b,k}^{2}[\|_{f}^{k}\|^{2}]+2L_{y}^ {2}_{b,k}^{2}_{f}^{2}.\] (26)

Now, based on the definition of \(V^{k}\) and combining (25) and (26):

\[E[V^{k+1}] [F(x^{k})-F(x^{*})]-}{2}[\| F(x^{k})\|^{2}]\] \[-[\|_{f}^{k}\|^{2}]}{2}(_{l,k} -L_{F}_{b,k}^{2}-4L_{y}^{2}_{b,k}^{2})+(_{b,k}L_{f}^{2}+2) [\|y^{k+1}-y^{*}(x^{k})\|^{2}]+\] \[_{b,k}B^{2}+(2L_{y}^{2}_{b,k}^{2}+ _{b,k}^{2}}{2})_{f}^{2}\] \[}{{}}[F(x^{k})-F(x^{*} )]-}{2}[\| F(x^{k})\|^{2}]+(_{b,k}L_{f} ^{2}+2)[\|y^{k+1}-y^{*}(x^{k})\|^{2}]+\] \[_{b,k}B^{2}+(2L_{y}^{2}_{b,k}^{2}+ _{b,k}^{2}}{2})_{f}^{2}\] \[=[V^{k}]-}{2}[\| F(x^ {k})\|^{2}]+(_{b,k}L_{f}^{2}+2)[\|y^{k+1}-y^{*}(x^{k})\|^{2}]+\] \[_{b,k}B^{2}+(2L_{y}^{2}_{b,k}^{2}+ _{b,k}^{2}}{2})_{f}^{2}-[\|y^{k}-y^{*}(x^{k})\| ^{2}],\]

where (e) is because \(+4L_{y}^{2}}^{2}}{_{l,0}}\), which guarantees that \(_{l,k}=}{}(L_{F}+4L_{y}^{2})_{b,k}=(L _{F}+4L_{y}^{2})^{2}}{k+1}\). 

Lemma 5 and Lemma 6 give two alternatives for bounding the term \(\|y^{k+1}-y^{*}(x^{k})\|^{2}\). Lemma 5 is based on the assumption \(_{}[g(x,y^{*}(x);)-g(x,y^{*}_{x,};)]_{g}^{2}, x\). The proof for its one-variable analogous assumption is given in Loizou et al. . Here we follow a similar approach for the two-variable function \(g(x,y)\). Lemma 5 will be used in the proof of Theorem 3.

**Lemma 5**.: _Suppose Assumptions 3, 5, and the bounded optimal function value assumption (15) hold. Further assume that each sampled function \(g(x,y;)\) is convex, then step size of the form 11 achieves the following:_

\[[\|y^{k+1}-y^{*}(x^{k})\|^{2}](1-_{g}C_{k})^{T}[\|y^{k }-y^{*}(x^{k})\|^{2}]+2_{b,k}T_{g}^{2},\]

_where \(C_{k}=\{},_{b,k}\}\)._Proof.: We denote \(h_{g}^{k,t}=_{y}g(x^{k},y^{k,t};)\), and \(_{k,t}\) be a filtration up to and including \(x^{k}\) and \(y^{k,t}\).

We have,

\[\|y^{k,t+1}-y^{*}(x^{k})\|^{2} =\|y^{k,t}-_{k,t}h_{g}^{k,t}-y^{*}(x^{k})\|^{2}\] \[=\|y^{k,t}-y^{*}(x^{k})\|^{2}-2_{k,t} y^{k,t}-y^{*}(x^ {k}),h_{g}^{k,t}+_{k,t}^{2}\|h_{g}^{k,t}\|^{2}\] \[\|y^{k,t}-y^{*}(x^{k})\|^{2}-2_{k,t}  y^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+}{p}[g(x^{k},y ^{k,t};)-g(x^{k},y_{x^{k},}^{*};)]\] \[\|y^{k,t}-y^{*}(x^{k})\|^{2}-2_{k,t}  y^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+2_{k,t}[g(x^{k},y^{k,t}; )-g(x^{k},y_{x^{k},}^{*};)]\] \[=\|y^{k,t}-y^{*}(x^{k})\|^{2}-2_{k,t} y^{k,t}-y^{*}(x ^{k}),h_{g}^{k,t}+\] \[ 2_{k,t}[g(x^{k},y^{k,t};)-g(x^{k},y^{*}(x^{k}); )+g(x^{k},y^{*}(x^{k});)-g(x^{k},y_{x^{k},}^{*};)]\] \[=\|y^{k,t}-y^{*}(x^{k})\|^{2}+2_{k,t}[- y^{k,t}-y^{*} (x^{k}),h_{g}^{k,t}+g(x^{k},y^{k,t};)-g(x^{k},y^{*}(x^{k});)]+\] \[ 2_{k,t}[g(x^{k},y^{*}(x^{k});)-g(x^{k},y_{x^{k}, }^{*};)]\] \[\|y^{k,t}-y^{*}(x^{k})\|^{2}+2C_{k}[- y ^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+g(x^{k},y^{k,t};)-g(x^{k},y^{*}(x^{ k});)]+\] \[ 2_{k,k}[g(x^{k},y^{*}(x^{k});)-g(x^{k},y_{x^{k}, }^{*};)],\]

where (a) is by Lemma 1, (b) is by choosing \(p\), and, (c) is by individual convexity of \(g(x,y;)\) such that \(- y^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+g(x^{k},y^{k,t};)-g(x^{k},y^{*}(x^{k});) 0\) and recalling that \(C_{k}=\{},_{b,k}\}_{k,t}\) by Lemma 1. Take expectation conditioned on \(_{k,t}^{{}^{}}\) and note that \([h_{g}^{k,t}|_{k,t}^{{}^{}}]=_{y}g(x^{k},y^{k +1})\), \([g(x^{k},y^{k,t};)|_{k,t}^{{}^{}}]=g(x^{k},y^{ k,t})\), and \([g(x^{k},y^{*}(x^{k});)]=g(x^{k},y^{*}(x^{k}))\):

\[[\|y^{k,t+1}-y^{*}(x^{k})\|^{2}|_{k,t}^{{}^ {}}]\|y^{k,t}-y^{*}(x^{k})\|^{2}+2C_{k}[- y^{k,t}-y^{*}(x^{k} ),_{y}g(x^{k},y^{k+1})+\] \[g(x^{k},y^{k,t})-g(x^{k},y^{*}(x^{k}))]+2_{b,k}_{g}^{2}.\] (27)

Now, based on the strong convexity of \(g\) w.r.t. \(y\),

\[- y^{k,t}-y^{*}(x^{k}),_{y}g(x^{k},y^{k+1})+g(x^{k},y^{k, t})-g(x^{k},y^{*}(x^{k}))}{2}\|y^{k,t}-y^{*}(x^{k})\|^{2},\]

we can further obtain (by taking total expectations of (27) and using strong-convexity):

\[[\|y^{k,t+1}-y^{*}(x^{k})\|^{2}](1-_{g}C_{k})[\|y^ {k,t}-y^{*}(x^{k})\|^{2}]+2_{b,k}_{g}^{2}.\]

Solve this recursively from \(t=0\) to \(t=T-1\) (recall \(T\) is the total number of lower-level steps, \(y^{k,0}=y^{k}\) and \(y^{k+1}=y^{k,T}\)):

\[[\|y^{k+1}-y^{*}(x^{k})\|^{2}] (1-_{g}C_{k})^{T}[\|y^{k}-y^{*}(x^{k})\|^{2}]+2 _{b,k}_{g}^{2}_{j=0}^{T-1}(1-_{g}C_{k})^{j}\] \[(1-_{g}C_{k})^{T}[\|y^{k}-y^{*}(x^{k})\|^{2}]+2 _{b,k}T_{g}^{2}.\]

Lemma 6 is based on the standard bounded variance assumption \(_{}[\|_{y}g(x,y^{*}(x);)-_{y}g(x,y^{*}(x))\|^{2}] _{g}^{2}\), \( x,y\) in the bi-level optimization literature . Lemma 6 will be used in the proof of Theorem 4.

**Lemma 6**.: _Suppose Assumptions 3, 5 and the bounded variance assumption (16) hold. Suppose that \(p\{}{_{g}+L_{g}},+L_{g}}{4L_{g}}\}\), \(_{b,0}\{+L_{g}},+L_{g}}{2_{g}L_{g}}, -L_{g}}{_{g}+L_{g}}}\}\). Then step size of the form 11 achieves the following:_

\[[\|y^{k+1}-y^{*}(x^{k})\|^{2}](}{C_{k}}-L_{g}}{_{g}+L_{g}}_{b,k})^{T}[\|y^{k}-y^{*}(x^{k})\|^{2}]+T _{b,k}^{2}_{g}^{2},\]

_where \(C_{k}=\{},_{b,k}\}\)._Proof.: Similar to the proof of Lemma 5, we can start with

\[\|y^{k,t+1}-y^{*}(x^{k})\|^{2}=\|y^{k,t}-y^{*}(x^{k})\|^{2}-2_{k,t} y ^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+_{k,t}^{2}\|h_{g}^{k,t}\|^{2}.\]

Divide both sides by \(_{k,t}\)

\[-y^{*}(x^{k})\|^{2}}{_{k,t}}=-y^{*}(x^{k}) \|^{2}}{_{k,t}}-2 y^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+_{k, t}\|h_{g}^{k,t}\|^{2}.\]

Then use the facts that \(_{k,t}_{b,k}\) and \(_{k,t} C_{k}\),

\[-y^{*}(x^{k})\|^{2}}{_{b,k}}-y^{*}(x^ {k})\|^{2}}{C_{k}}-2 y^{k,t}-y^{*}(x^{k}),h_{g}^{k,t}+_{b, k}\|h_{g}^{k,t}\|^{2}\,.\]

Next, take expectation conditioned on the Filtration \(_{k,t}^{{}^{}}\) up to and including \(x^{k}\) and \(y^{k,t}\)

\[}[\|y^{k,t+1}-y^{*}(x^{k})\|^{2}| _{k,t}^{{}^{}}] }[\|y^{k,t}-y^{*}(x^{k})\|^{2}| _{k,t}^{{}^{}}]-2 y^{k,t}-y^{*}(x^{k}), g(x^{k},y^{k,t})\] \[+_{b,k}[\|h_{g}^{k,t}\|^{2}|_{k,t}^{{}^{}}]\] \[=}[\|y^{k,t}-y^{*}(x^{k})\|^{2}|_{k,t}^{{}^{}}]-2 y^{k,t}-y^{*}(x^{k}), g(x^{k},y^{k,t})\] \[+_{b,k}[\|h_{g}^{k,t}- g(x^{k},y ^{k,t})+ g(x^{k},y^{k,t})\|^{2}|_{k,t}^{{}^{}}]\] \[}[\|y^{k,t}-y^{*}(x^{ k})\|^{2}|_{k,t}^{{}^{}}]-2 y^{k,t}-y^{*}(x^{k}), g(x ^{k},y^{k,t})\] \[+_{b,k}_{g}^{2}+_{b,k}\| g(x^{ k},y^{k,t})\|^{2},\]

where (a) is by the bounded variance assumption (16). Based on strong-convexity of \(g\)[34, Theorem 2.1.11], we have

\[}[\|y^{k,t+1}-y^{*}(x^{k})\|^{2}| _{k,t}^{{}^{}}] }\|y^{k,t}-y^{*}(x^{k})\|^{2}-L_{g}}{_{g}+ L_{g}}\|y^{k,t}-y^{*}(x^{k})\|^{2}\] \[-+L_{g}}\| g(x^{k},y^{k,t})\|^{2 }+_{b,k}_{g}^{2}+_{b,k}\| g(x^{k},y^{k,t})\|^{2}\]

Multiply by \(_{b,k}\) in both sides, group terms, and take total expectations to reach

\[[\|y^{k,t+1}-y^{*}(x^{k})\|^{2}] (}{C_{k}}-L_{g}}{_{g}+L_{g} }_{b,k})[\|y^{k,t}-y^{*}(x^{k})\|^{2}]+_{b,k}^{2}_{g}^ {2}\] \[+_{b,k}(_{b,k}-+L_{g}})\|  g(x^{k},y^{k,t})\|^{2}\] \[(}{C_{k}}-L_{g} }{_{g}+L_{g}}_{b,k})[\|y^{k,t}-y^{*}(x^{k})\|^{2}]+_{b, k}^{2}_{g}^{2},\]

where in (b) we have chosen \(_{b,k}_{b,0}+L_{g}}, k\). Solving this recursion similar to Lemma 5, we obtain:

\[[\|y^{k+1}-y^{*}(x^{k})\|^{2}](}{C_{k}}-L_{g}}{_{g}+L_{g}}_{b,k})^{T}[\|y^{k}-y^{*}(x^{k})\|^{2 }]+T_{b,k}^{2}_{g}^{2}.\] (28)

In (28), we require \(}{C_{k}}-L_{g}}{_{g}+L_{g}}_{b,k} 0\) (recall \(C_{k}=\{},\,_{b,k}\}_{k,t}_{b,k}\)), this is equivalent to \(C_{k}+L_{g}}{2_{g}L_{g}}\). In case of \(C_{k}=}\), we choose \(p}{_{g}+L_{g}}\) (to avoid contradictions, we also choose \(p+L_{g}}{4L_{g}}\)). In case of \(C_{k}=_{b,k}\), we choose \(_{b,k}_{b,0}+L_{g}}{2_{g}L_{g}}\).

We also require \(}{C_{k}}-L_{g}}{_{g}+L_{g}}_{b,k} 1\). In case of \(C_{k}=_{b,k}\), this is equivalent to \(L_{g}}{_{g}+L_{g}}_{b,k} 0\), which is satisfied by all \(_{b,k}\). In case of \(C_{k}=}\), we choose \(_{b,k}_{b,0}-L_{g}}{_{g}+L_{g}}}\). Putting everything together, we have \(p\{}{_{g}+L_{g}},+L_{g}}{4L_{g}}\}\), and \(_{b,0}\{+L_{g}},+L_{g}}{2_{g}L_{g}}, -L_{g}}{_{g}+L_{g}}}\}\).

### Single-level Convex Proofs

#### a.2.1 Proof of Theorem 1

The proof of Theorem 1 is similar to the standard proof of decaying-step SGD (GD) that can be found in e.g. Beck . Here, we give the proof for completeness.

\[\|x^{k+1}-x^{*}\|^{2} =\|x^{k}-x^{*}\|^{2}-2_{k} x^{k}-x^{*}, f_{i}( x^{k})+_{k}^{2}\| f_{i}(x^{k})\|^{2}\] \[\|x^{k}-x^{*}\|^{2}-2_{k}(f_{i}(x^{k})- f_{i}(x^{*}))+_{k}^{2}\| f_{i}(x^{k})\|^{2}\] \[\|x^{k}-x^{*}\|^{2}-2_{k}(f_{i}(x^{k})- f_{i}(x^{*}))+_{b,k}^{2}G^{2},\]

where (a) is by individual convexity of \(f_{i}\), and (b) is by Assumption 2. Take conditional expectation and assume that \(_{k}\) is independent of sample \(k\)

\[[\|x^{k+1}-x^{*}\|^{2}|x^{k}] \|x^{k}-x^{*}\|^{2}-2E[_{k}|x^{k}](f(x^ {k})-f(x^{*}))+_{b,k}^{2}G^{2}\] \[\|x^{k}-x^{*}\|^{2}-2_{l,k}(f(x^{k})-f( x^{*}))+_{b,k}^{2}G^{2},\]

where (c) is by independence of \(_{k}\) and \(x^{k}\), and (d) is because \(_{l,k}_{k}, k\). Take total expectation and rearrange

\[2_{l,k}[f(x^{k})-f(x^{*})][\|x^{k}-x^{*}\|^{2}] -E[\|x^{k+1}-x^{*}\|^{2}]+_{b,k}^{2}G^{2}\]

Using the fact that \(_{l,K-1}_{l,k} k[K-1]\) and set \(_{b,k}=}{}\), we obtain

\[2_{l,K-1}[f(x^{k})-f(x^{*})][\|x^{k}-x^{*}\|^{2 }]-E[\|x^{k+1}-x^{*}\|^{2}]+^{2}}{k+1}G^{2}\]

Summing over \(k=0\) to \(k=K-1\) we obtain

\[2_{l,K-1}_{k=0}^{K-1}[f(x^{k})-f( x^{*})] -x^{*}\|^{2}-[\|x^{K}-x^{*}\|^{2}]}{K} +_{0}^{2}G^{2}_{k=0}^{K-1}\] \[-x^{*}\|^{2}}{K}+^{2}G^{2}( K)}{K}\]

Define \(=_{k=0}^{K-1}x^{k}\), apply Jensen's inequality and rearrange

\[[f(^{K})-f(x^{*})]-x^{*}\|^{2}}{2_{l, K-1}K}+^{2}G^{2}(K)}{2_{l,K-1}K}.\]

#### a.3 Proof of Theorem 2

The approach of Theorem 2 is similar to [16, Theorem 3.2]. The crucial difference is that the step size in [16, Theorem 3.2] is constant (\(_{k}=}\)) for \(k 4\), whereas for envelope-type step size it is of the form:

\[_{k}=\{\{_{l,k},_{k}\},_{b,k}\}, _{l,k}=\{w,_{b,k}\},\]

where \(_{k}\) can be (e.g. in the case of SPSB ):

\[_{k}=\{}(x^{k})-l_{i_{k}}^{*}}{c_{k}\| f _{i_{k}}(x^{k})\|^{2}},}{k+1}\}, w=L_{ }}\.\]

The proof of Theorem 2 suggests that the step size can be either \(}(x^{k})-l_{i_{k}}^{*}}{c_{k}\| f_{i_{k}}(x^{k})\|^{2}}\) or \(}{k+1}\) depending on their magnitudes for \(k k_{0}-1\) (\(k_{0}=\{1,_{0}/-1\}\)). After \(k_{0}\) iterations, the step size is \(_{k}=}{k+1}\). This finding is numerically confirmed by the experimental results in Section B.

To proceed with the proof, we have:

\[\|x^{k+1}-x^{*}\|^{2} =\|x^{k}-_{k} f_{i}(x^{k})-x^{*}\|^{2}\] \[=\|x^{k}-x^{*}\|^{2}-2_{k} f_{i}(x^{k}),x^{k}-x ^{*}+_{k}^{2}\| f_{i}(x^{k})\|^{2}\] \[}{{}}\|x^{k}-x^{*}\|^{2}-2_ {k} f_{i}(x^{k}),x^{k}-x^{*}+_{b,k}^{2}\| f_{i }(x^{k})\|^{2},\]

where (a) is because \(_{k}_{b,k}, k\). Take conditional expectations

\[[\|x^{k+1}-x^{*}\|^{2}|x^{k}] \|x^{k}-x^{*}\|^{2}-2[_{k}|x^{k}]  f(x^{k}),x^{k}-x^{*}+_{b,k}^{2}[\| f_{i}( x^{k})\|^{2}|x^{k}]\] \[}{{}}\|x^{k}-x^{*}\|^{2}- [_{k}|x^{k}]\|x^{k}-x^{*}\|^{2}-2[_{k}|x^{k}] [f(x^{k})-f(x^{*})]+_{b,k}^{2}G^{2}\] \[}{{}}\|x^{k}-x^{*}\|^{2}- _{l,k}\|x^{k}-x^{*}\|^{2}-2_{l,k}[f(x^{k})-f(x^{*})]+_{b,k} ^{2}G^{2}\]

where (b) is by bounded gradients assumption and strong convexity of \(f\). Take total expectation and rearrange

\[2[f(x^{k})-f(x^{*})])[\|x^{k} -x^{*}\|^{2}]-[\|x^{k+1}-x^{*}\|^{2}]}{_{l,k}}+^{2}G^{2}}{_{l,k}}\]

Choose \(k_{0}=\{1,_{0}/-1\}\), then for \( k\) s.t. \(k k_{0}\), we have \(_{l,k}=\{,_{b,k}\}=_{b,k}=}{k+1}\), which means \(_{k}=}{k+1}\) after \(k_{0}\) steps. Within the first \(k_{0}\) steps, the step size is \(_{k}=\{,_{b,k}\}\). Hence, for \(k k_{0}\) we have

\[2[f(x^{k})-f(x^{*})])[\|x^{k} -x^{*}\|^{2}]-[\|x^{k+1}-x^{*}\|^{2}]}{_{l,k}}+G^{2}}{k+1}\]

Now, sum from \(k=k_{0}\) to \(K-1\)

\[2_{k=k_{0}}^{K-1}[f(x^{k})-f(x^{*})]_{k=k_{0}}^{K-1} )[\|x^{k}-x^{*}\|^{2}]-[\|x^{k+1}- x^{*}\|^{2}]}{_{l,k}}+_{k=k_{0}}^{K-1}G^{2}}{k+1}\] (29)

For the first term in (29), call it \(A\), we expand it as

\[A =_{k=k_{0}+1}^{K-1}[\|x^{k}-x^{*}\|^{2}](}-}-)+[\|x_{k_{0}}-x^{*}\|^{2 }](}}-)-[\|x_{K}-x^{*}\|^{2}]}{ _{l,K-1}}\] \[_{k=k_{0}+1}^{K-1}[\|x^{k}-x^{*}\|^{2}](}-}-)+[\|x_{k_{0}}-x^{*}\|^{2}]( +1}{_{0}}-)\] \[}{{}}[\|x_{k_{0}}-x^{*} \|^{2}] k_{0}.\]

where (d) is because \(_{0}\), we have \(}-}- 0, k\) and \(+1}{_{0}}- k_{0}\). For the second term in (29), call it B, we have

\[B=_{k=k_{0}}^{K-1}G^{2}}{k+1}_{0}G^{2}_{k_{0 }}^{K-1}dx=_{0}G^{2}[(K)-(k_{0}+1)]_{0}G^ {2} K\]

Putting \(A\) and \(B\) together, we obtain

\[}_{k=k_{0}}^{K-1}[f(x^{k})-f(x^{*})][\|x^{k_{0}}-x^{*}\|^{2}] k_{0}}{2(K-k_{0})}+G^{2 }(K)}{2(K-k_{0})}\] (30)

Within the first \(k_{0}-1\) iterations, similarly to the above, we have

\[[\|x^{k+1}-x^{*}\|] (1-_{l,k})[\|x^{k}-x^{*}\|^{2}]-2_{l,k}[f(x^{k})-f(x^{*})]+_{b,k}^{2}G^{2}\] \[(1-_{l,k})[\|x^{k}-x^{*}\|^{2}]+_{b, k}^{2}G^{2}\,.\]For the first \(k k_{0}-1\) iterations, \(_{l,k}=\) where \(<1\); thus, we obtain the following

\[[\|x^{k+1}-x^{*}\|](1-)E[\|x^{k}-x^{*}\|^{2}]+_{b,k }^{2}G^{2}.\]

Solve this recursively,

\[[\|x^{k_{0}}-x^{*}\|^{2}] (1-)^{k_{0}}\|x^{0}-x^{*}\|^{2}+_{k=0}^{k_{0}-1} (1-)^{k_{0}-k-1}^{2}G^{2}}{(k+1)^{2}}\] \[(1-)^{k_{0}}\|x^{0}-x^{*}\|^{2}+_{k=0}^{k_{0}-1} ^{2}G^{2}}{(k+1)^{2}}\] \[(1-)^{k_{0}}\|x^{0}-x^{*}\|^{2}+_{0}^{2}G^{2} _{0}^{k_{0}-1}}dx\] \[=(1-)^{k_{0}}\|x^{0}-x^{*}\|^{2}+_{0}^{2}G^{2}(1- })\] \[(1-)^{k_{0}}\|x^{0}-x^{*}\|^{2}+_{0}^{2}G^{2}.\] (31)

Putting this into (30), we obtain

\[}_{k=k_{0}}^{K-1}[f(x^{k})-f(x^{*})]}{2(K-k_{0})}\{(-k_{0})\|x_{0}-x^{*}\|^{2}+_{0}^{2} G^{2}\}+G^{2} K}{2(K-k_{0})}.\]

Define \(_{K}=}_{k=k_{0}}^{K-1}x^{k}\), then by Jensen's inequality we have

\[[f(_{K})-f(x^{*})]}{2(K-k_{0})}\{(-k_ {0})\|x_{0}-x^{*}\|^{2}+_{0}^{2}G^{2}\}+G^{2}  K}{2(K-k_{0})},\]

where \(k_{0}=\{1,_{0}/-1\}\) and \(_{0}\).

### Bi-level Proofs

#### a.4.1 Proof of Theorem 3

Start with Lemma 4:

\[E[V^{k+1}] [V^{k}]-}{2}[\| F (x^{k})\|^{2}]+(_{b,k}L_{f}^{2}+2)[\|y^{k+1}-y^{*}(x^{k})\|^{ 2}]+\] \[_{b,k}B^{2}+(2L_{y}^{2}_{b,k}^{2}+_{b,k}^{2}}{2})_{f}^{2}-[\|y^{k}-y^{*}(x^{k})\|^{2}].\]

We substitute the result of Lemma 5 for the expression \([\|y^{k+1}-y^{*}(x^{k})\|^{2}]\),

\[E[V^{k+1}] [V^{k}]-}{2}[\| F (x^{k})\|^{2}]+[(_{b,k}L_{f}^{2}+2)(1-_{g}C_{k})^{T}-1][\| y^{k}-y^{*}(x^{k})\|^{2}]+\] \[}{{}}[V^{k}]-}{2}[\| F(x^{k})\|^{2}]+[(_{b,0}L_{f}^{2}+2) (1-_{g}C_{K-1})^{T}-1][\|y^{k}-y^{*}(x^{k})\|^{2}]+\] \[}{{}}E[V^{k}]- }{2}E[\| F(x^{k})\|^{2}]+2_{b,k}_{b,k}TL_{f}^{2}_{g}^{2} +4_{b,k}T_{g}^{2}+_{b,k}B^{2}+(2L_{y}^{2}+}{2}) _{b,k}^{2}_{f}^{2},\]

where (a) is by \(_{b,0}_{b,k},C_{K-1} C_{k}, k\), hence \((_{b,0}L_{f}^{2})(1-_{g}C_{K-1})^{T}(_{b,k}L_{f}^{2})(1- _{g}C_{k})^{T}\) (recall that \(C_{k}=\{},_{b,k}\}\) in Lemma 1); (b) is by \(TL_{f}^{2}+2]}{-(1- C_{K-1})}\), which implies that \((_{b,0}L_{f}^{2}+2)(1-_{g}C_{K-1})^{T} 1\). Now, rearrange and use the fact that \(_{l,K-1}_{l,k}\),

\[_{l,K-1}[\| F(x^{k})\|^{2}] 2[V^{k}]-2 [V^{k+1}]+4_{b,k}_{b,k}TL_{f}^{2}_{g}^{2}+8_{b, k}T_{g}^{2}+2_{b,k}B^{2}+(4L_{y}^{2}+L_{F})_{b,k}^{2} _{f}^{2}.\]Then sum over \(k=0\) to \(K-1\):

\[_{k=0}^{K-1}[\| F(x^{k})\|^{2}] }{_{l,K-1}K}+^{2}_{g}^{2} }{_{l,K-1}K}_{k=0}^{K-1}_{b,k}_{b,k}+^{2 }}{_{l,K-1}K}_{k=0}^{K-1}_{b,k}+}{_{l,K-1}K} _{k=0}^{K-1}_{b,k}+\] \[^{2}+L_{F})_{f}^{2}}{_{l,K- 1}K}_{k=0}^{K-1}_{b,k}^{2}\] \[}{_{l,K-1}K}+^{2} _{g}^{2}_{b,0}_{b,0}}{_{l,K-1}K}_{k=0}^{K-1}}+^{2}_{b,0}}{_{l,K-1}K}_{k=0} ^{K-1}+\] \[+_{b,0}}{_{l,K-1}K}_{k=0}^{K-1 }}+^{2}+L_{F})_{f}^{2}_{ b,0}^{2}}{_{l,K-1}K}_{k=0}^{K-1}\] \[}{_{l,0}}+^{2}_{g}^{2}_{b,0}_{b,0}}{_{l,0}}+^{2}_{b,0}(K)}{_{l,0}}+\] \[_{b,0}}{_{l,0}}+^{2}+L _{F})_{f}^{2}_{b,0}^{2}(K)}{_{l,0}},\]

where we substituted \(_{b,k}=}{}\) and \(_{b,k}=}{k+1}\) in (c); (d) is based on \(_{k=0}^{K-1}} 2\), \(_{k=0}^{K-1}(K)\), and \(_{k=0}^{K-1}}\). Recall that in Lemma 2, we have \(L_{f}(^{2})\), \(L_{y}(^{3})\), and \(L_{F}(^{3})\). Also recall that \(_{b,k}=}{}\) and \(_{l,k}=}{}\), hence we choose \(_{l,0}_{b,0}(^{-3})\), \(T()\), and \(_{b,0}(^{-2})\). Then we can obtain \(_{k=0}^{K-1}[\| F(x^{k})\|^{2}]( }{}+(K)}{})\).

#### a.4.2 Theorem 4 and its proof

**Theorem 4**.: _Suppose \(f\) and \(g\) satisfy Assumptions 3, 4, and 5, and, learning-rate upper bounds \(_{b,k}=}{}\) and \(_{l,k}=}{}\) with \(_{b,0}\) and \(_{l,0}\) satisfying \(+4L_{F}^{2}}^{2}}{_{l,0}}\) and \(_{l,0}_{b,0}\). Further assume that \(_{k}\) is independent of the stochastic hypergradient \(h_{f}^{k}\). Then, under the Bounded-variance assumption in (16) with \(p\{}{_{g}+L_{g}},+L_{g}}{4L_{g}}\}\), \(_{b,0}\{+L_{g}},+L_{g}}{2_{g}L_{g }},-L_{g}}{_{g}+L_{g}}}\}\), and \(T_{b,0}L_{f}^{2}+2)}{\{-(1-L_{g}}{_{g}+L_{g}}_{b,K-1}),-((2pL_{g}-L_{g}}{ _{g}+L_{g}}_{b,0}))\}}\), BiSPS achieves the following rate:_

\[_{k=0}^{K-1}[\| F(x^{k})\|^{2}]}(}{}+ K}{}).\]

Proof.: Start with Lemma 4:

\[E[V^{k+1}] [V^{k}]-}{2}[\| F (x^{k})\|^{2}]+(_{b,k}L_{f}^{2}+2)[\|y^{k+1}-y^{*}(x^{k})\|^{2}]+\] \[_{b,k}B^{2}+(2L_{y}^{2}_{b,k}^{2}+ _{b,k}^{2}}{2})_{f}^{2}-[\|y^{k}-y^{*}(x^{k})\|^{2 }].\]

We substitute the result of Lemma 6 for the expression \([\|y^{k+1}-y^{*}(x^{k})\|^{2}]\),

\[E[V^{k+1}] [V^{k}]-}{2}[\| F (x^{k})\|^{2}]+[(_{b,k}L_{f}^{2}+2)(}{C_{k}}-L_{g}}{_{g}+L_{g}}_{b,k})^{T}-1][\|y^{k}-y^{*}(x^{k}) \|^{2}]+\] \[+L_{f}^{2}T_{g}^{2}_{b,k}_{b,k}^{2}+2T _{g}^{2}_{b,k}^{2}+_{b,k}B^{2}+[2L_{y}^{2}+}{2}] _{b,k}^{2}_{f}^{2}\] \[E[V^{k}]-}{2}[\|  F(x^{k})\|^{2}]+L_{f}^{2}T_{g}^{2}_{b,k}_{b,k}^{2}+2T _{g}^{2}_{b,k}^{2}+_{b,k}B^{2}+[2L_{y}^{2}+}{2}] _{b,k}^{2}_{f}^{2},\]where in (a) we have chosen \(T\{L_{f}^{2}+2)}{-(1-_{g}L_{ g}}{_{b}+L_{g}}_{b,K-1})},^{2}L_{f}^{2}+2)}{- (2pL_{g}-L_{g}}{_{g}+L_{g}}_{b,0})}\}\). This ensures that \(TL_{f}^{2}+2)}{-(L_{f}^{2}+2 }{_{b}+L_{g}}_{b,k})}, k\), which guarantees that \((_{b,k}L_{f}^{2}+2)(}{C_{k}}-L_{g}}{_{ g}+L_{g}}_{b,k})^{T}-1 0\).

Now, rearrange and use the fact that \(_{l,K-1}_{l,k}\),

\[_{l,K-1}[\| F(x^{k})\|^{2}] 2[V^{k}]-2 [V^{k+1}]+2_{b,k}_{b,k}^{2}TL_{f}^{2}_{g}^{2}+4 _{b,k}^{2}T_{g}^{2}+2_{b,k}B^{2}+(4L_{y}^{2}+L_{F})_{b,k}^{2}_{f}^{2}.\]

Sum over \(k=0\) to \(k=K-1\):

\[_{k=0}^{K-1}[\| F(x^{k})\|^{2}] }{_{l,K-1}K}+^{2}_{g}^{2 }}{_{l,K-1}K}_{k=0}^{K-1}_{b,k}_{b,k}^{2}+^{2}}{_{l,K-1}K}_{k=0}^{K-1}_{b,k}^{2}+\] \[}{_{l,K-1}K}_{k=0}^{K-1}_{b,k}+ ^{2}+L_{F})_{f}^{2}}{_{l,K-1}K}_{k=0}^{ K-1}_{b,k}^{2}\] \[}{_{l,K-1}K}+^{2} _{g}^{2}_{b,0}_{b,0}^{2}}{_{l,K-1}K}_{k=0}^{K-1} }+^{2}_{b,0}^{2}}{_{l,K-1}K} _{k=0}^{K-1}+\] \[_{b,0}}{_{l,K-1}K}_{k=0}^{K-1} }+^{2}+L_{F})_{f}^{2}_{b,0}^{2}}{_{l,K-1}K}_{k=0}^{K-1}\] \[}{_{l,K-1}K}+^ {2}_{g}^{2}_{b,0}_{b,0}^{2}}{_{b,0}}+^{2}_{b,0}^{2}(K)}{_{b,0}}+\] \[+_{b,0}}{_{l,0}}+^{2} +L_{F})_{f}^{2}_{b,0}^{2}(K)}{_{l,0}},\]

where in (b) we have substituted \(_{b,k}=}{}\) and \(_{b,k}=}{}\), and (c) is by \(_{k=0}^{K-1}} 2\), \(_{k=0}^{K-1}(K)\), and \(_{k=0}^{K-1}}\). Similar to the proof of Theorem 3, we choose \(_{l,0}_{b,0}(^{-3})\), \(T()\), and \(_{b,0}(^{-1})\) to obtain \(_{k=0}^{K-1}[\| F(x^{k})\|^{2}]( }{}+(K)}{})\). 

## Appendix B Additional Experiment Results

This section is organized as follows. First, we discuss synthetic quadratics experiments. Second, we provide more details on the sensitivity of the algorithms to the choices of \(\) in (14), on the reset procedure, and on the search cost of BiSLS. Third, we compare the empirical performance of 1-sample vs 2-samples implementations of our algorithms for single-level convex and bi-level optimization. Some additional results for hyper-representation learning and data distillation experiments are also presented. We run \(5\) independent runs for all our experiments.

### Synthetic Quadratics

The experiments on quadratic functions are adapted from Loizou et al. . The training objective is as follows:

\[f(x)=(x-x_{1}^{*})^{T}H_{1}(x-x_{1}^{*})\,+\, (x-x_{2}^{*})^{T}H_{2}(x-x_{2}^{*}),\]

where \(H_{i}\) (\(i=1,2\)) are positive definite. The optimal solutions \(x_{i}^{*}\) (\(i=1,2\)) are generated randomly from a standard normal distribution. Specifically, \(H_{i}\) is defined as follows:

\[H_{i}=O^{T}((1+_{i})) O, i=1,2,\]

where \(O\) and \(_{i}\) are taken from the spectral decomposition of \(P^{T}P\), and \(P\) is generated from the standard normal distribution. Figure 9 shows the convergence of various algorithms with different starting points. Interestingly, both SPSB with either 1 sample or 2 samples (1 sample for computing the gradient and the other for computing the step size) converge to the optimal solution (labelled with a star).

### Sensitivity of \(\), reset, and search cost

In this section, we discuss the effects of \(\) in (14), \(\) in Algorithm 2, and comment on the search cost of the options in the Reset Algorithm 2. Recall that the line-search condition (8) assumes that we can find a largest \(_{k}_{b,0}\) to satisfy it. However, in practice, we apply a backtracking procedure, i.e. \(_{k}=_{k}*w,0<w<1\), until \(_{k}\) satisfies (8). Therefore, the found learning rate is not guaranteed to be the largest. Nonetheless, we assume that \(_{k}\) is the largest to simplify our analysis given above (similar arguments apply to line-search at both upper and lower-level in the bi-level optimization). The experiments in this section are based on hyper-representation learning . In this case, the objective of the induced bilevel-optimization problem can be written as:

\[_{w}F(w)=}}\|(X_{1};w)c^{*}(w)-Y_{1 }\|^{2}\] \[s.t. c^{*}(w)=*{argmin}_{c}} }\|(X_{2};w)c-Y_{2}\|^{2}+\|c\|^{2}\,,\]

where \((X_{1},Y_{1})\) and \((X_{2},Y_{2})\) are validation and training data sets with sizes \(D_{X_{1}}\) and \(D_{X_{2}}\), respectively; \((;w)\) are the embedding layers of the model parameterized by \(w\); and, \(c\) is the classification layer. Moreover, we use conjugate gradient methods (CG) [17; 18] to solve the linear system when computing the hypergradient for hyper-representation learning experiments.

ResetWhile Algorithm 2 (reset) can be applied to both upper and lower-level problems, we focus our discussions here on the upper-level learning rate (\(_{k}\)). This is because we empirically find it to be more critical for the convergence performance (see Figure 5(a)). As shown in Algorithm 2, reset has \(3\) options. Options \(1\), \(2\), and \(3\) search starting from \(_{b,0}\), \(_{k-1}\), and \(_{k-1}\), at iteration \(k\) respectively. Option \(1\) has the highest search cost as it always starts from the same initial upper bound (\(_{b,0}\)). Option \(2\) ensures the monotonicity of the learning rate due to \(_{k}_{b,k}=_{k-1}\). Option \(3\) chooses the search starting point at iteration \(k\) (\(_{b,k}\)) by multiplying the previous learning rate (\(_{k-1}\)) by a factor \( 1\). As in the single-level convex case where monotonicity in the step size can potentially lead to slow convergence (see Figure 3), we again observe that monotonicity in the upper learning rate (i.e. option 2) leads to poorer performance when compared against options 1 or 3 as shown in Figure 10. Finally, we compare the performance of different \(\)s in option \(3\) (note that \(=1\) in option 3 is equivalent to option 2). We observe in Figure 11 that different \(\)s perform equally well. This shows the robustness of our algorithm to the choice of \(\). As mentioned previously, the choice of option \(3\) over option \(1\) are due to 2 reasons: (a) reduced search cost; (b) provides an overall non-increasing and non-monotonic trend of upper bound \(_{b,k}\). We discuss search cost of different \(\)s in option 3 below.

Search CostBased on the results in Figure 6(b), we observe the use of option 3 in reset can significantly reduce the search cost for both BiSLS-Adam and BiSLS-SGD. Figure 10 further suggests that options 1 and 3 have nearly the same performance. Therefore, option 3 is an efficient algorithm that maintains good performance while reducing computation cost. Option 2 has the lowest search cost (\( 4\) rounds per iteration). However, its performance is not as good as option 1 or 3 as

Figure 9: Iterate trajectories of different starting points for the synthetic quadratic experiments.

observed in Figure 10. Moreover, the average lower-level search cost is only \(1\) round per iteration when option 1 is used (see Figure 12).

Sensitivity on \(\)As mentioned in Sec 2, due to the stochastic error in hypergradient computation, further complicated by the approximation error of \(y^{*}(x)\) (see (14)), a learning rate is not guaranteed to be found in the bi-level case. Specifically, this is in contrast to the single-level convex problems. To avoid this, we introduce in (14) a \(\) slack to give some tolerance to such errors. Here, we give a thorough investigation of the effects of \(\) on performance. We vary its magnitude across 6 orders for both reset options \(1\) and \(3\) (see Algorithm 2 and discussions on reset above). We observe that despite a large difference on the magnitudes of \(\), they all share very similar performance for both BiSLS-SGD and BiSLS-Adam: see Figures 13 and 14. We summarize the key fins in this section as follows: 1 **The option 3 in reset has good empirical performance (outperforms option 2) and is an effective way to reduce search cost (Figure 10, 27); 2 BiSLS is highly robust to different choices of \(\) in option 3 and \(\) in (14) (Figure 11, 13, 14).**

Figure 11: Validation loss against iterations for different \(\)s based on reset option 3. Results for BiSLS-SGD are given in (a) and for BiSLS-Adam are given in (b). Note that \(=1\) in reset option 3 is equivalent to reset option 2. For the lower-level search, we fix it option 1 with \(_{b,0}=100\). Results are based on hyper-representation learning.

Figure 10: Validation loss against iterations with search options \(1\), \(2\), and \(3\) for the upper-level learning rate. The results for BiSLS-SGD and BiSLS-Adam are in (a) and (b), respectively. For the lower-level search, we fix it option 1 with \(_{b,0}=100\). Results are based on hyper-representation learning.

### Data distillation objective and additional results

We let \(_{S}(w)\) denote the loss evaluated on dataset \(S\) with model weights \(w\). The objective of _data distillation_ can be expressed as a BO problem as follows:

\[D^{*}\,=\,*{argmin}_{D}_{}(w^{*}(D))  w^{*}(D)\,=\,*{argmin}_{w}_{D}(w),\]

where \(\) is of the same size as \(D\) and subsampled from the entire (original) dataset \(V\). The solution \(D^{*}\) is the distilled data, e.g. 9 MNIST digits each corresponding to a different label. In figure 14(a), we show the performance of BiSPS for different values of \(_{b,0}\) in comparison with BiSLS-SGD, and observe that BiSLS-SGD has better performance. In 14(b), we show the results when we increase the number of lower-level iterations (T) from 20 to 50. As observed for \(T=20\) (in Figure 8), BiSLS-SGD here also outperforms a fine-tuned Adam or SGD.

### 1-sample or 2-samples versions of algorithms for convex and bi-level optimization

We provide additional results to compare the performance of 1-sample and 2-samples (one for computing the gradient and the other for computing the step size) versions of our algorithms for

Figure 12: Lower-level search cost measured in the same way as upper-level against different lower-level search starting points (\(_{b,0}\)). The lower-level search is done with option 1 (see above for discussions about these options).

Figure 13: Validation loss against iterations for different \(\)s based on reset option 1. Results for BiSLS-SGD (a) and for BiSLS-Adam (b). For the lower-level search, we fix it option 1 with \(_{b,0}=100\). Results are based on hyper-representation learning.

SPSB and BiSPS used for single-level and bi-level optimization, respectively. In the single-level case (Figure 16), we observe that \(2\)-samples SPSB performs just as well as 1-sample SPSB. Interestingly, we observe that their step sizes also follow a similar pattern. That is: an initial increase followed by a regime where \(_{k}=}(x^{k})-l_{i_{k}}^{}}{c\| f_{i_{k}}(x^{ k})\|^{2}}\) is frequently used, and eventually changes to decaying-step SGD. This seems to also match with Theorem 2 where a transition point for SPSB (\(k_{0}=\{1,|_{0}/|-1\},w=},_{0} \)) is predicted. At the same time, we also note that (perhaps, unsurprisingly) the 2-samples version seems to have a slightly more oscillatory behavior than the 1-sample version as shown in Figure 16. SLSB with either 1-sample or 2-samples also result in a similar performance and step size. Overall, despite the requirements of Theorems 1 and 2 for a 2-samples assumption, the empirical performance of 1-sample and 2-samples for either SPSB or SLSB appears to be very similar. Moving on to the bi-level case, recall that Theorems 3 and 4 require the 2-samples assumption (i.e., \(_{k}\) independent of \(h_{f}^{k}\)) for the upper-level learning rate. We empirically verify this assumption with both hyper-representation learning and data distillation experiments. For hyper-representation learning experiments in Figure 17, BiSPS with either 1-sample or 2-samples for different values of \(_{b,0}\) show similar performance. In fact, for \(_{b,0}=0.1\) we even observe that the 2-samples variant outperforms the 1-sample BiSPS. For data distillation experiments in Figure 18, the performances of 1-sample and 2-samples BiSPS are similar to each other when

Figure 14: Validation loss against iterations for different \(\)s based on reset option 3 (\(=10\)). Results for BiSLS-SGD (a) and for BiSLS-Adam (b). For the lower-level search, we fix it option 1 with \(_{b,0}=100\). Results are based on hyper-representation learning.

Figure 15: Validation loss against iterations. (a) Comparison between BiSPS with different \(_{b,0}\)s and BiSLS-SGD. (b) Comparison between BiSLS-SGD to fine-tuned Adam/SGD (\(_{k}\) fixed at \(10^{-4}\)). Inverse Hessian in (2) treated as the Identity  when computing the hypergradient. Recall that T is the total number of lower-level iterations and we have shown the results for \(T=20\) in Figure 8b.

\(_{b,0}=10.0\) or \(_{b,0}=50.0\). In general, the performance difference between 1-sample and 2-samples in the single-level or bi-level settings is small.

Figure 16: Binary linear classification on w8a dataset using logistic loss . Train loss (left) and step size (right) against iterations. We choose \(_{b,0}=1000\) for all algorithms. The upper bound for either SPSB or SLSB decays as \(_{b,k}=}{}\). For decaying-step SGD, the learning rate schedule is \(}{}\).

Figure 17: Comparison between BiSPS (2-samples), BiSPS (1-sample) and decaying-step SGD. Experiments are based on hyper-representation learning. For either version of BiSPS, the lower-level learning rate (\(_{k}\)) is fixed at \(10\). The hypergradient is computed using conjugate gradient .

Figure 18: Comparison between BiSPS (2-samples), BiSPS (1-sample) and decaying-step SGD. Experiments are based on data distillation. For either version of BiSPS, the lower-level learning rate (\(_{k}\)) is fixed at \(10^{-4}\). The Inverse Hessian in (2) is treated as the Identity when computing the hypergradient .