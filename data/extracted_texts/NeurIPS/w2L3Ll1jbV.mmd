# Adversarially Robust Multi-task Representation Learning

Austin Watkins

Johns Hopkins University

Baltimore, MD 21218

awatki29@jhu.edu

&Thanh Nguyen-Tang

Johns Hopkins University

Baltimore, MD 21218

nguyent@cs.jhu.edu

&Enayat Ullah

Meta\({}^{*}\)

enayat@meta.com

&Raman Arora

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

Work done while the author was at the Johns Hopkins University.

###### Abstract

We study adversarially robust transfer learning, wherein, given labeled data on multiple (source) tasks, the goal is to train a model with small robust error on a previously unseen (target) task. In particular, we consider a multi-task representation learning (MTRL) setting, i.e., we assume that the source and target tasks admit a simple (linear) predictor on top of a shared representation (e.g., the final hidden layer of a deep neural network). In this general setting, we provide rates on the excess adversarial (transfer) risk for Lipschitz losses and smooth non-negative losses. These rates show that learning a representation using adversarial training on diverse tasks helps protect against inference-time attacks in data-scarce environments. Additionally, we provide novel rates for the single-task setting.

## 1 Introduction

In many real-world applications, we typically have a scarcity of data for the intended task. Consider, for example, settings where the learner's environment is evolving rapidly or where access to high-quality labeled data is expensive or infeasible due to a lack of expertise or computational limitations. These problems are typically studied under the framework of transfer learning [6; 22; 23; 40; 19]. Such approaches aim to leverage plentiful labeled data from the source domain to learn models that can handle distribution shifts and work well on the target domain despite having a small labeled dataset for the target task.

As transfer learning methods have proven successful in various applications [16; 18; 27], there is a growing effort to utilize these approaches in high-risk environments like healthcare, medicine, transportation, and finance. Any vulnerability of these systems provides malicious agents with tempting targets. Consequently, the users of these ML systems may find their health and financial well-being potentially jeopardized. Additionally, institutions that deploy these systems risk public relations crises and lawsuits. A particular concern is attacks that happen after a model is deployed. Such attacks are called "inference-time attacks" where, for example, a malicious agent attacks a large language model (LLM) chatbot, a self-driving car, or a fraud-detection system. Much of the literature focuses on inference-time attacks that add small perturbations to the model's input. Prior work has demonstrated that this can cause ML models to act unpredictably [8; 35].

While several works have focused on imparting ML algorithms with robustness to adversarial attacks , there is little emphasis on adversarially robust transfer learning, i.e., ensuring robustness to tasks with little (or no) supervision by leveraging labeled data from related tasks. In this paper, we study this problem from a theoretical perspective, building on prior work on this topic . We focus on transfer learning via learning a representation that provides a method for sharing knowledge between different, albeit related, tasks . A common approach to achieve this has been termed multi-task representation learning (MTRL) . In practice, the class of representations is a complex model like a deep neural network, and the predictors trained on top of them are simple linear models . Such a paradigm offers hope that we can pool our data, potentially providing a substantial benefit if performed well.

In this work, we are interested in answering the following question: _can multi-task learning be used to learn complex representations that facilitate robust transfer while also benefiting from the diversity of source data?_ We answer in the affirmative. Consider a class of representations \(\) from \(^{d}\) to a lower dimensional space \(^{k}\), and a class of real-valued predictors \(\) trained on top of it. Let \(t\) be the number of source tasks, \(n\) be the number of samples per source task, and \(m\) be the number of samples for the target task. For exposition, let \(C()\) be the complexity of a function class that is independent of \(t,n,m\) and the adversarial attack. For Lipschitz losses, we bound the excess transfer risk for the adversarial loss by

\[)}{n}+)}{nt}}+ )}{m}}.\]

For smooth and nonnegative losses, we bound the excess transfer risk for the adversarial loss by

\[_{}^{}})}{m}}+)}{m}+_{}^{}})}{n}+)}{nt}}+)}{m}+)}{nt} +,\]

where \(\) and \(\) quantify task relatedness, \(_{}^{}\) is the average best possible adversarial risk for the source tasks, and \(_{}^{}\) is the best possible adversarial risk for the target task2. The second bound is called an optimistic rate . Both \(_{}^{}\) and \(_{}^{}\) allow the rate above to interpolate between a slow and fast rate depending on how difficult it is to be adversarially robust within the setting. Both of these rates show the benefit of pooling \(nt\) data to learn a feature function that assists in mitigating adversarial attacks.

In the process of showing the above, we establish several results that we believe are of independent interest. A more complete list of our contributions follows.

1. We show bounds on the excess transfer risk for the adversarial loss class for both Lipschitz losses (Theorem 2) and smooth nonnegative losses (Theorem 5).
2. Foundational to Theorems 2 and 5 are Lemmas 1 and 3, resp. These lemmas are similar to results in prior work [25, Lemma 4.4 and Lemma 6.5.]. However, ours are less restrictive because we remove a Lipschitzness assumption on the adversarial loss class. In Appendix B.2, we provide an example of an attack model for which our lemma applies but the previous lemmas do not.
3. In our general attack model (Assumption 4) and both Lipschitz losses and smooth nonnegative losses, we bound the sample-dependent Rademacher complexity of the adversary loss class by the worst-case Rademacher complexity times a multiplicative factor attributed to the adversarial attack. This latter factor for many common attacks has a dimensional dependence of \( d\). Additionally, when the loss function is smooth and nonnegative, our bound is a sub-root function, which is suitable for optimistic rates.
4. We provide a framework for studying adversarial robustness in MTRL, which consists of several foundational contributions, e.g., Theorems 1 and 4, Definition 1, Assumption 4.A, Algorithm 1.

### Some core difficulties and techniques

* A core part of our arguments is a pair of covering number lemmas that convert from the adversarial loss class into the standard loss class at the expense of inflating the data. However, after applying this result to the integrand of Dudley's integral, the sample complexity depends on the radius of the cover of the function class. This dependency makes bounding the integral difficult.

Prior work either invoked a model or made a parametric assumption to bound this integral . In contrast, we bound the integral in more generality. We elaborate on these difficulties and detail our technique in Proof Sketch 5.
* We use a comparison inequality from a celebrated work  in a novel way, to our knowledge, that allows a decomposition that separates function class complexity and attack complexity. Unfortunately, this is not a full decomposition due to a weak \(\) dependence between the factors. However, we show that this dependence can be handled appropriately.
* We present a reduction from multi-task learning to single-task learning, see the proof of Theorem 3, that we believe is of independent interest. In particular, this observation allows simplification of prior work  due to these works' use of worst-case complexity.

### Prior work

**Adversarial attack.** Prior work has shown that the normalization of model weights, data, and the definition of robustness can significantly impact the dimensional cost of achieving adversarial robustness. Although not directly comparable, it is informative to contrast the generalization bounds for linear classifiers presented by  and . Both works perform Rademacher complexity based analysis with losses that are Lipschitz and satisfy certain monotonic properties. However, they mainly differ in how they normalize the quantities involved. Let \(p\) and \(q\) be Holder-conjugates.  consider \(_{p}\) linear classifiers with \(_{q}\) normalized data being attacked with \(_{}\) perturbations. They show that the Rademacher complexity of suitably transformed version of the linear model, after applying Talagrand's contraction lemma, has a tight \(d^{1/q}\) dimensional dependence. Thus, when \(p=1\), there is no dimensional dependence. On the other hand,  consider \(_{2}\) and \(_{p}\) bounded linear classifiers with data being \(_{2}\) bounded and being attacked with \(_{q}\) perturbations. They show that the Rademacher complexity of similarly transformed function class has no dimensional dependence. Both rates lead to generalization bounds on the robust risk. Taken together, these works demonstrate how prior research has strongly leveraged the relationship between the norm on the attack, which we cannot control, and the model and data norms. Both works also provide rates for neural networks.  provides a lower bound, showing that a certain variational version of a neural network has a \(\) lower bound.  shows that their rate has an \(\) upper bound via a "tree transformation" which is then used to bound the robust risk. Finally, both works consider the optimization of surrogate losses for neural networks.

**Adversarial transfer.** Theoretical analyses of adversarial transfer in MTRL remain relatively scarce, despite many empirical studies . Two works that provide theoretical insights into this problem are  and . First,  is perhaps the first theoretical study of this problem. Specifically, they study a shared linear projection onto a smaller dimensional subspace with linear classifiers trained on top of it, analogous to the regression study in . Under \(_{}\) or \(_{2}\) perturbation attacks, they show that the transfer risk of the adversarial loss decays as \(+d/nt}\) along with multiplicative constants that depend on task diversity and can reduce the rate as more diverse tasks are gathered. In addition, they study the combination of semi-supervised learning and adversarial training and show their complementary qualities. Second,  considers a composite model, with a linear model being trained on top of a neural network. For additive perturbations and Lipschitz losses, they provide two results showing that the robustness of the predictor is bounded by the robustness of the representation it is trained on. Their first result, with high probability, bounds the difference between the adversarial loss of the end-to-end predictor and the standard loss in terms of the average Euclidean difference in the representations over the predictions. Their second result, applied to classification, provides a sufficient condition for robustness in terms of a bound on the aforementioned Euclidean difference. These results are independent of the method used for training the parameters of the representation.

While not strictly in the MTRL setting,  works with a non-composite model and gives a sufficient condition for robust transfer in terms of the discrepancy between the symmetric difference hypothesis space. They give generalization bounds using Rademacher complexity. Also,  studies the connection between domain transfer and adversarial robustness, showing that robustness is neither necessary nor sufficient for domain transferability.

**Optimistic rates.** Optimistic rates are a type of self-normalized inequality that bounds the excess risk that interpolates between two rates of decay. The prototypical example being the use of a smooth nonnegative losses to give a rate where a fast \((1/n)\) is achieved when task is realizable and a standard \((1/)\) when it is not . Optimistic rates have been shown for linear regression with Gaussian data , multi-output prediction , adversarial robustness , and multi-task learning . The typical way to achieve optimistic rates is via local Rademacher complexity machinery .

## 2 Problem setup and preliminaries

Let \(^{d}\) and \(\) denote the input and the label spaces, respectively. Let \(\) be a class of representation maps from \(^{d}\) to \(^{k}\), and \(\) and \(_{0}\) each be a class of predictors from \(^{k}\) to \(\).3 For a loss function \(:\), denote \(_{y}:=(,y)\) so we can represent the functions consistently as a composition. Let \(_{2}\) and \(_{}\) denote the Euclidean norm and the uniform norm, respectively. We use the convention that \(}()\) hides log terms from the usual asymptotic notation.

Each source task is represented by a distribution \(\{P_{j}\}\) over \(\), for \(j=1,,t\), and the target task is represented by probability distribution \(P_{0}\). Following prior work [37; 39], we make the following assumptions for all tasks \(P_{0},,P_{t}\). We assume that (a) the marginal distribution over \(\) is the same; (b) there exists a common representation \(h^{}\) and task-specific predictors \(f_{j}^{}\), \(f_{0}^{}_{0}\) such that \(P_{j}\) can be decomposed as \(P_{j}(x,y)=P_{x}(x)P_{y|x}(y|f_{j}^{} h^{}(x))\); and (c) the predictor \(f_{j}^{} h^{}\) is the optimal4 in-class predictor for its respective task w.r.t. \(\). The assumption above implies that any additional noise in \(y\) is independent of \(x\) because \(y\) depends on \(x\) only via \(f_{j}^{} h^{}(x)\). As noted in , the second assumption above does not imply the third.

We assume that we have access to \(n\) training examples for each source task drawn i.i.d. from the respective distributions \(P_{1},,P_{t}\), and \(m\) examples for the target task \(P_{0}\). We use \((x_{j}^{i},y_{j}^{i})\) to denote the \(i^{}\) training example for the \(j^{}\) task.

**Adversarial attacks and adversarial training.** We formulate our attack with a function class \(\{A:\}\). For example, \(=\{x x+_{}  0.01,x+\}\) for additive \(_{}\) attacks. Our goal is to learn a composite predictor \(_{0}\) which performs well and is robust to these adversarial attacks, i.e., it has a small adversarial risk defined formally as follows. Let \(f_{0}_{0}\) and \(=(f_{1},,f_{t})^{ t}\). Then, the adversarial population risk and empirical risk for the target and the source tasks are defined as follows.

\[R_{}(f_{0},h,) _{(x,y) P_{0}}_{A}(_{y} f_{0} h A)(x),\] \[R_{}(,h,) _{j=1}^{t}_{(x,y) P_{j}} _{A}(_{y} f_{j} h A)(x),\] \[_{}(f_{0},h,) _{i=1}^{m}_{A} _{y_{0}^{i}} f_{0} h A(x_{0}^{i}),\] \[_{}(,h,) _{j=1}^{t}_{i=1}^{n}_{A }_{y_{j}^{i}} f_{j} h A(x_{j}^{i }).\]

We also make natural modifications of the two-stage learning procedure used in [37; 39].

**Algorithm 1** (Two-stage adversarial MTRL).: \[(},) *{arg\,min}_{^{ t},h, }_{}(,h,)\] (Stage 1) \[_{0} *{arg\,min}_{_{0}}\ _{} f_{0},,\] (Stage 2)

In Stage 1, we perform empirical risk minimization over the adversarial loss class for the combined \(t\) tasks. After Stage 1 we have \(t\) compositions \(_{1},,_{t}\) which minimize the average risk above. Now, in Stage 2, we fix the representation \(\) learned from the source tasks and perform empirical risk minimization again to find a new predictor for the target task. The final predictor for the target task is \(_{0}\).

**Adversarial task diversity.** Naturally, if all of our tasks are drastically different we expect Algorithm 1 to perform poorly. Therefore, it is crucial to quantify the relationship between the tasks. Prior work in the linear setting gives sufficient properties between the tasks to provide provable rates . However, these assumptions are in terms of spectral properties and therefore not suitable in a more general setting. A more general notion of task relatedness called _task diversity_ was introduced in . This relationship between tasks was shown to be sufficient for Lipschitz losses  and smooth nonnegative losses . Yet, it was not clear if these guarantees hold in an adversarial setting. To close the gap, we introduce a new notion of _adversarial task diversity.5_

**Definition 1** (Robust \((,,)\)-task diversity ).: _The tasks \(\{P_{j}\}_{i=1}^{t}\) are \((,,)\)-diverse over \(P_{0}\), if for the corresponding \(^{}^{ t},_{0}^{} _{0}\) and representation \(h^{}\), we have that for all \(h^{}\)_

\[_{f^{}_{0}}R_{}(f^{},h^{}, )-R_{}(f_{0}^{},h^{},)^{-1 }(_{^{}^{ t}}R_{} ^{},h^{},-R_{ }(^{},h^{},))+.\]

**Loss class and dataset notation.** Let a function class \(\) be a function class and its \(t\)-fold Cartesian product be \(^{ t}\). We use the following notation for the standard MTRL loss class.

\[^{ t}\{(x_{1},,x_{t })((_{y_{1}} q_{1})(x_{1}),,(_{y_{t}} q_{t})(x _{t})) q^{ t}\}.\]

We define an adversarial counterpart to the MTRL loss class (above) as follows.

\[_{}^{ t}\{(x_{ 1},,x_{t})(_{A}(_{y_{1}} q_{1} A )(x_{1}),,_{A}(_{y_{t}} q_{t} A)(x_{t} )) q^{ t}\}.\]

We define the _function class \(\) restricted by functional \(V:^{t}\) at \(r\)_ as \(|_{r}=\{q^{ t} V(q) r\}\). We will consider \(V\) to be a multiple (by a factor of \(b\)) of the adversarial or standard risk. The Rademacher complexity of this restricted functional class yields local Rademacher complexity. When using local Rademacher complexity it is common to bound it by a sub-root function. A function \(:[0,)[0,)\) is sub-root if it is nonnegative, nondecreasing, and if \(r(r)/\) is nonincreasing for \(r>0\). Sub-root functions are continuous and have unique fixed points . Given \(x\) let \(C_{x}()\) be a proper \(\|\|_{}\)-cover of \((x)\) at scale \(\). Since \(C_{x}()\) is proper, this cover is realized by some subset of \(\). Let \(C_{(x)}()\) be this subset of \(\). For a dataset \(S\{(x_{i},y_{i})\}_{i[n]}\), we rotate \(S_{}()\{(A(x_{i}),y_{i}) i[n],A C_{ (x_{i})}()\}\) to represent the approximate inflation of \(S\) with respect to \(\) at radius \(\). Our convention is to have all covers be minimal when minimality can be achieved.

**Complexities.** Next, we introduce the notions of complexities of functions classes that we will utilize. First, we give the Rademacher based complexities suitable in MTRL on a fixed set of inputs. Let \(\) be a class of vector-valued functions from \(\) to \(^{q}\). Denote the \(p\)-fold Cartesian product of \(\) as \(^{ p}\). For \(=z_{j}^{i}_{j[p],i[n]}\), where \(z_{j}^{i}\), define the data-dependent Rademacher width and data-dependent Rademacher complexity, respectively, as \(}(^{ p},)_{_{i,j,k}}[_{^{ p}}(np)^{-1} _{i,j,k=1}^{n,p,q}_{ijk}(q_{j}(z_{j}^{i}))_{k}]\) and \(|}|(^{ p},)_{_{i,j,k}}[_{^{ p}}|(np)^{-1} _{i,j,k=1}^{n,p,q}_{ijk}(q_{j}(z_{j}^{i}))_{k}]|,\) where \(_{i,j,k}\) are i.i.d. Rademacher random variables. In contrast to the convention we will place a particular emphasis on the dataset, and therefore, it is prominent in the notation. We define the worst-case Rademacher width and the worst-case Rademacher complexities as \(}(^{ p},n)_{} }(^{ p},)\) and \(|}|(^{ p},n)_{| |=n}|}|(^{ p},)\).

Next, using the same notation, we define the empirical covering number of vector-valued function class. With \(>0\), define \(_{2}(^{ t},,)\) be the cardinality of a minimal cover \(C\) such that for all \(^{ t}\) there is a \(} C\) such that \((np)^{-1}_{i,j,k=1}^{n,p,q}((q_{j}(z_{j}^{i}))_{k}-(_{j}(z_{j}^{i} ))_{k})^{2}^{2}\) and \(_{}(^{ t},,)\), similarly, but with \(_{i,j,k}|(q_{j}(z_{j}^{i}))_{k}-(_{j}(z_{j}^{i}))_{k}|\) as the norm instead.

Finally, we define the fat-shattering dimension. Let \(\) be a class of functions from \(\) to \(\) and let \(=\{z_{1},,z_{m}\}\) where \(\). Then, for \(>0\), we say that \(\) is \(\)-shattered by \(\), if there exist \(r_{1},,r_{m}\), such that for all \(b\{0,1\}^{m}\) there is a \(q_{b}\) such that for all \(i[m]\) we have \(f_{b}(x_{i}) r_{i}+\) if \(b_{i}=1\) and \(f_{b}(x_{i}) r_{i}-\) if \(b_{i}=0\). Let \(_{}(,)\) be the cardinality of the largest subset of \(\) that is \(\)-shattered by \(\). If a largest subset does not exists, let \(_{}(,)=\).

**Assumptions.** We make the following assumptions on the loss function, hypothesis classes, and the adversarial attack function. The first two assumptions regarding if the loss is Lipschitz or smooth nonnegative are standard.

**Assumption 1** (Lipschitz loss).: _The loss function \(:\) is \(L_{}\)-Lipschitz and \(b\)-bounded i.e., \(|(y^{},y)| b<,\; y^{},y \) and \(|_{y}(y_{1},y)-_{y}(y_{2},y)| L_{}|y_{1}-y_{2}|\) for all \(y_{1},y_{2},y\)._

**Assumption 2** (Smooth nonnegative loss).: _The loss function \(:\) is nonnegative, \(b\)-bounded, and \(H\)-smooth, i.e., \(0(y^{},y) b<\) for all \(y^{},y\) and \(^{}_{y}(y_{1},y)-^{}_{y}(y_{2},y) H|y_{ 1}-y_{2}|\) for all \(y_{1},y_{2},y\)._

The next assumptions are that the predictor and representation function classes are Lipschitz.

**Assumption 3** (Hypotheses and feature maps are Lipschitz).:
1. _All functions in_ \(\) _and_ \(_{0}\) _are_ \(\|\|\)_-Lipschitz for some_ \(0<L_{}<\)_, i.e.,_ \(|f(z_{1})-f(z_{2})| L_{}\|z_{1}-z_{2}\|\) _for all_ \(z_{1},z_{2}(f)\) _and_ \(f_{0}\)_._
2. _All functions in_ \(\) _are_ \((\|\|_{},\|\|)\)_-Lipschitz for some_ \(0<L_{}<\)_, i.e.,_ \(\|h(z_{1})-h(z_{2})\| L_{}\|z_{1}-z_{2}\|_{}\) _for all_ \(z_{1},z_{2}(h)\) _and_ \(h\)_._

If the norm in Assumption 3A is Euclidean and additionally any \(f h\) is \(D\)-bounded over \(\) w.r.t. \(\|\|_{2}\) then these assumptions are sufficient to apply the so called Gaussian chain rule . This allows more interpretability since the statistical cost of learning can be broken up into two terms, for \(\) and \(\) separately, in a rather intuitive way. We do not apply this theorem primarily due to space limitations. However, for illustrative reasons, we will assume these assumptions hold when we analyse the asymptotics of Theorems 2 and 5.

Finally, we make the following assumptions on the adversary.

**Assumption 4** (Bounded within-domain adversarial attacks).:
1. \((x)\) _is totally bounded w.r.t_ \(\|\|_{}\) _for all_ \(\)_. That is, for all_ \(>0\) _a finite number of_ \(\|\|_{}\) _balls of radius_ \(\) _cover_ \((x)\) _for all_ \(x\)_._
2. _The attack function cannot perturb outside of the input domain_ \(\)_, i.e.,_ \(\{A:\}\) _._

First, Assumption 4A trivially implies that there exists a \(<\) such that \(_{A,x}\|A(x)\|_{}\). Second, Assumption 4B is a reasonable assumption as we discuss in Appendix A.

## 3 Adversarial multi-task representation learning

In this section, we give our main adversarial MTRL results. Given robust task diversity (Definition 1) holds, these theorems show that with high probability, adversarial excess transfer risk decays with the sample size and is bounded by the complexity of the non-adversarial loss class and an additional factor derived from the adversarial attack. First, we show our result for Lipschitz losses, and then for smooth nonnegative losses.

### Lipschitz losses

Lipschitz losses, like ramp loss, are frequently used in machine learning. We will start with a uniform convergence bound that is foundational to our study of Lipschitz losses. The following result bounds the adversarial excess transfer risk by the Rademacher complexity of the adversarial loss class for the source tasks and target task. This result is an adversarially robust version of the main MTRL result in  before utilizing the Lipschitzness of the loss. Alternatively, it can be seen as an MTRL version of Corollary 1 in .

**Theorem 1**.: _Let \(\) and \(_{0}\) be the learned representation and target predictor, as described Algorithm 1. Under Assumption 1 and that \(^{}\) is \((,,)\)-diverse over \(_{0}\) w.r.t. \(h^{}\), then, with probability _at least \(1-2\), we have that \(R_{}_{0},,-R_{}(f _{0}^{},h^{},)\) is bounded by_

\[^{-1}(8}(_{}^{  t}(),n)+8b)+8_{h }}(_{}(_{0}  h),m)+8b+.\]

We now apply the above to Lipschitz losses classes. But, before we do, let us define a special function that features prominently in our work. Let the function \(_{}(,L,n,)\) be mapped to

\[\!S_{}} ++40}{c} }{e}S_{}}}\!} .\]

When needed, we will use \(_{}^{}(,,,)\) or \(_{}^{}(,,,)\) to indicate which data is being inflated.

**Theorem 2**.: _Under the setting of Theorem 1 along with Assumption 3, Assumption 4, \(|S_{}eb/4cL_{3}|,|S_{}(eb/4c {m}L_{1})| e^{},\)\(_{}((),_{1}b), _{}((_{0}),_ {2}b) 1\), then the Rademacher complexities \(}(_{}(^{ t}( )),n)\) and \(}(_{}(_{0} h),m)\) in Theorem 1 are, respectively, bounded by_

\[2|}|^{ t}( ),n_{}^{}(2^{-1},L_{3},nt,_{1})2_{h}|}|( (_{0} h),m)_{}^{} 2^{-1},L_{1},m,_{2},\] (1)

_where \(L_{3}=L_{}L_{}L_{}\), \(L_{1}=L_{}L_{}\), and \(C,c\) are absolute constants._

If we ignore the two factors that come from the adversarial complexity, the result is similar to prior work in the non-adversarial setting of . In fact, if \(\) exclusively contains the identity function, then we recover the non-adversarial version modulo log factors.

The assumptions on the fat-shattering dimension and size of the inflated dataset are in several of our theorems. It should be noted that they are technical and mild. First, \(|S_{}(1/)|\) is usually exponential in \(d\), and, therefore, practically always larger than \(e^{e}<16\). Also, the assumption trivially holds if the dataset is larger than \(16\). Second, \(_{}((), b) 1\) is just a parametric version of the assumption that the fat-shattering dimension is nonzero. Practically speaking, \(\) being \(1\) or \(1/2\) is reasonable and does not impact the bound in any meaningful way.

A dimensionality analysis on the first expression in Equation (1).Recall that \((x)\) is totally bounded for all \(x\) w.r.t. \(\|\|_{}\). So, over the sample \(S\), the attack class perturbs the points only so far. That is, \(_{x S,A^{},A}\!\|A(x)-A^{}(x)\|_{ }<\) for some \(\). This gives us a radius for which we place \(n\) balls of radius \(\) to cover \((S)\). By standard volume arguments, e.g. Lemma A.8 in , each of these balls can be \(eb/4cL_{3}\)-covered by \((12cL_{3}/eb)^{d}\) many points. Thus, \(n(12cL_{3}/eb)^{d}\) bounds the cardinality of the inflated dataset \(S_{}eb/4cL_{3}\). By using this inequality in Equation (1), \(}(_{}(^{ t}( )),S)\) is bounded by \(|}|((^{ t}()), nt)/b)}(d(nL_{3}/b))(n),\) where we set \(_{1}=1\), ignored constants and lower-order terms. Therefore, as a function of \(d\), the adversarial training costs at least a factor of \( d\) more in comparison to the standard non-adversarial loss. The analysis of the second expression in Equation (1) is similar but the dimensional dependence is \( k\) because the inflation happens in the image of the representation space.

The dimensionality and sample size dependencies of Equation (1).Like in the introduction, let \(C()\) be the complexity of a function class which is independent of the sample complexity and the adversarial attack. Generally, \(|}|((^{ t}()),n)\) decays as \((^{ t}())/nt})\) and \(|}|((_{0} h),m)\) as \((_{0})/m})\). If the image of the source tasks predictors is bounded and \(\|\|_{2}\)-Lipschitz, we can decompose \(|}|((^{ t}()),n)\) into \(()/nt}+))/n})\) by using the Gaussian chain rule . Additionally, the adversarial robustness factors contribute \(()\) for the source tasks and \(()\) for the target task because of the dimensionality analysis in the prior paragraph. Taken together, in this setting, the adversarial excess transfer risk decays as \()/n+dC()/nt}+_{0})/m}\). See Appendix B.1 for a detailed comparison of the above rate to the linear setting studied in .

Proof Sketch 1.: _The result follows by bounding the adversarial Rademacher complexities by calling Theorem 3 twice. However, for the Rademacher complexity w.r.t. the target task one must make the observation that we can treat \((x)\) as the attack function class for all \(x\). This function is totally bounded by Assumption 3.B and therefore we can proceed as normal. See Appendix E.1 for details._The proof above depends on the following bound on the Rademacher complexity of the multi-task function class \(_{}(^{ t}())\). To achieve this bound, we reduce the multi-task setting to the single-task setting. Although simple, this reduction simplifies arguments made in prior work [37; 39] these works use worst-case complexity.

**Theorem 3**.: _Under the setting of Theorem 2, \((_{}(^{ t}()),S)\) is bounded by \(2||((^{ t}()),n)_{ }^{}2^{-1},L_{3},nt,_{1},\) where \(L_{3}=L_{}L_{}L_{}\)._

**Proof Sketch 2**.: _The proof reduces the multi-task setting to the single-task setting by observing that the data-dependent multi-task Rademacher complexity is equivalent to the data-dependent single-task Rademacher complexity on a dataset with an additional immutable component indexing the task. See Appendix E.1 for details._

The proof of Theorem 3 immediately implies we can use the machinery in the single-task setting literature. This is also true in the non-adversarial setting. Therefore, the above reduction technique applies to situations outside adversarial robustness. However, the above argument does not work when not using worst-case Rademacher complexity because \(\) would not be a sample of i.i.d. random variables. Yet the above method is a convenient tool as we are using worst-case Rademacher complexity because our analysis goes through the fat-shattering dimension. In Appendix D, we give a short remark on a notion of fat-shattering dimension suitable for vector-valued classes.

### Smooth and nonnegative losses

Like in the case of Theorem 1, we extend prior work to the adversarially robust learning setting. Since much of what we discussed about sample complexity and dimensionality parameters in the prior section also applies here, this section will be more brief.

**Theorem 4**.: _Let \(\) and \(_{0}\) be the learned representation and target predictor, as described Algorithm 1. Let \(_{1}\) and \(_{2}\) be sub-root functions such that \(_{1}(r)(_{}(^{  t}()_{r}),n)\) and \(_{2}(r) b_{h}(_{} (_{0} h_{r}),m)\) with \(r_{1}^{}\) and \(r_{2}^{}\) the fixed points of \(_{1}(r)\) and \(_{2}(r)\), respectively. Under Assumption 2 and that \(^{}\) is \((,,)\)-diverse over \(_{0}\) w.r.t. \(h^{}\), then, with probability at least \(1-2e^{-}\), we have that \(R_{}_{0},,}-R_{ }(f_{0}^{},h^{},)\) is bounded by,_

\[}(f_{0}^{},h^{},)} 9}+219^{}}{b}}++^{}}{2b}\] \[+}(^{},h^{ },)}6}+146^{ }}{b}}++^{}}{b}.+.\]

Theorem 4 requires bounding the local Rademacher complexity of the adversarial loss class by a sub-root function, which is the main challenge in applying this result. We show that such a bound is obtained if the predictor classes are Lipschitz and the loss is smooth and nonnegative.

**Theorem 5**.: _Under the setting of Theorem 4 along with Assumption 3, Assumption 4, \(|S_{}(eb/4cL_{2})|,|S_{}(eb/4c}})| e^{}\), \(_{}((),_{1}b), _{}((_{0}),_ {2}b) 1\), then the fixed points \(^{}/b}\) and \(^{}/b}\) in Theorem 4 are, respectively, bounded by_

\[2||(^{ t}(),n )_{}^{}(24Hb)^{-1/2},L_{2},nt,_{1} \]

_and \(2||(_{0} h,m)_{}^{ }(24Hb)^{-1/2},L_{},m,_{2}\), where \(L_{2}=L_{}L_{}\)._

Comparing Theorems 2 and 5, the latter has twice as many complexity terms: two for the target task and two for the source tasks. Therefore, we will start our asymptotic analysis of Theorem 5 from where the analysis after Theorem 2 left off. For the target task terms, one of these terms is multiplied by the square root of the adversarial risk for the best-in-class predictor and representation, i.e., \(R_{}(f_{0}^{},h^{},)\). This factor is zero when an adversarially robust classifier exists. This leaves only one complexity term remaining for the target task, which is a squared version of the last one. Thus, in this setting, the bound is a fast rate in the number of target samples \(m\). This reasoning shows the value of learning a robust representation because it takes fewer samples to learn a good predictor.

Similarly, the bound is a fast rate in the number of source tasks \(t\) and respective samples per task \(n\) if there exist predictors with zero adversarial risk on the source tasks. Thus, the bound on the excess transfer risk of the adversarial loss class decays as \((dC()/n+dC()/nt+kC(_{0})/m),\) when robust predictors and representations exist in our chosen classes.

**Proof Sketch 3**.: _The proof is like the argument for Theorem 2, but we use Theorem 6 not Theorem 3._

In the setting above, we now give our bound on the local Rademacher complexity of the adversarial loss class. Importantly, the bound is a sub-root function in \(r\).

**Theorem 6**.: _Under the setting of Theorem 5, \(}(_{}(^{ t}( )_{r}),S)\) is bounded by \(2|}|(^{ t}(),n) _{}^{}(24Hb)^{-1/2},L_{2},nt,_{1} ,\) where \(L_{2}=L_{}L_{}.\)_

**Proof Sketch 4**.: _Use the reduction in the proof of Theorem 3, then use Theorem 8._

## 4 Standard adversarial learning

### Lipschitz losses

In this section, we bound the adversarial Rademacher complexity for the standard single-task setting under the assumption of a Lipschitz loss. Such bounds immediately give results for the excess risk of the adversarial loss class via a uniform convergence guarantee like Corollary 1 in . The results and arguments for smooth and nonnegative losses are similar. See Section 4.2 for details.

The following result bounds the sample-dependent Rademacher complexity for the adversarial loss class by two factors: the worst-case Rademacher complexity for the non-adversarial loss class and a function that encodes the power of the attack model.

**Theorem 7**.: _Let \(\) be \(L_{}\)-Lipschitz w.r.t. \(\|\|_{}\). Under Assumption 1, Assumption 4, \(|S_{}(eb/4cL_{1})| e^{e}\), \(_{}((), b) 1\), then \(}(_{}(),S)\) is bounded by_

\[2|}|((),n)_{} 2^{-1},L_{1},n,,\] (2)

_where \(L_{1}=L_{}L_{}\) and \(C,c\) are absolute constants._

Many of the remarks in Section 3 apply to Theorem 7. See Appendices B.2 and B.3 for detailed comparisons to  and , respectively.

**Proof Sketch 5**.: _The difficulty of studying adversarial robustness originates from the variational component of the adversarial loss class. Under certain assumptions, we can remove the \(\) function with an appropriate "inflating" of the dataset by using a covering number argument inspired by . The proof of the result below is in the appendix._

**Lemma 1**.: _Let \(\) be \(L_{}\)-Lipschitz w.r.t. \(\|\|_{}\). Under Assumption 1, and Assumption 4, we have \(_{}(_{}(),,S) _{}((),/2,S_{ }(/2L_{1})),\) where \(L_{1}=L_{}L_{}\)._

_Although we find Lemma 1 intuitive, it does result in a difficulty that we demonstrate below. First, as expected, we can apply Lemma 6, Dudley's integral; then Lemma 1. These steps are shown in the inequality below._

\[}(_{}(),S) 4+ }_{}^{b}_{} (),,S_{}}}\ d\] (3)

_The above removes the variational component of the loss at the expense of "inflating" the data. Yet, notice that now this "inflated" data \(S_{}()\) is a function of \(\). This makes the integral more difficult to study because, after applying other techniques from prior work , one cannot move the sample complexity out of the integral. This difficulty is resolved in prior work by either invoking a model (e.g., \(\) being linear predictors) or making a parametric assumption. Both of these approaches are used in , with the parametric assumption being \(_{}((),/2,S_{} (/2L_{1}))^{-2}\)._

_Alternatively, we provide an approach that overcomes these challenges while retaining the generality of the results. Starting at Equation (3), our approach is to decouple the complexity of the class and the properties of the inflated dataset. We observe that a weak decoupling can be achieved by the use of a comparison inequality from a \(\|\|_{}\) cover to the fat-shattering dimension. In particular, such a comparison inequality is the following special case 6 of a celebrated result in ._

**Lemma 2** (Rudelson and Vershynin (2006)).: _Suppose \(\) is uniformly \(B\) bounded for \(B>0\), then, for all \(>0\), we have \(_{}(,,S) Cv(Bn/v) ^{}(n/v)\) for \(0<<B\) where \(v=_{}(,)\) and \(C,c>0\) are universal constants._

_If we apply Lemma 2 to the integrand of Equation (3), then \(_{}((),/2,S_{}( /2L_{1})) C_{}((), c/2)(2b|S_{}(/2L_{1})|/ )^{}(|S_{}(/2L_{1})|),\) where, for illustrative purposes, we removed the fat-shattering dimension factors from the denominators by assuming that \(_{}((),c/2) 1\). (In general, we must be more nuanced than this by handling cases - see the full proof for more details.) This decomposition allows us to handle each factor in turn in a way that was not possible before. Yet, the picture is more complicated than this, for instance, \(\) can depend weakly on \(S_{}()\). Nevertheless, from here, the complete proof - see Appendix E.2 - proceeds with a more traditional analysis and relies on other standard lemmas. The above weak decomposition illuminates the significance of Assumption 4.B in our analysis. In particular, when the image of all attacks is in \(\), we have that the points fat-shattered are within \(\), not \(()\), i.e., we have \(_{}((),2)\) nor \(_{()}((),)\). If Assumption 4.B does not hold, there is a stronger dependence between the complexity of the attack and the complexity of the predictors._

To our knowledge, this is a novel use of this family of fat-shattering comparison inequalities, and our techniques derive novel rates in the single-task setting while retaining the generality. The authors of (31, p. 607) conjecture that the \(^{}(n/v)\) factor in Lemma 2 can be removed, although it is unclear what the nature of a \(\) like dependence would be. Naturally, their conjecture implies another: that, for our setting, the Rademacher complexity of the adversarial loss class is \((_{}(),S)\) is \(()\).

### Smooth nonnegative losses in the single-task setting

In this section, we bound the local Rademacher complexity of the adversarial loss class for smooth nonnegative losses in the standard single-task setting. Many of the remarks in Section 4.1 apply here too. Thus, we will forgo much of this repetitive commentary for the sake of space. First, we bound the adversarial local Rademacher complexity of the adversarial loss class.

**Theorem 8**.: _Under the setting of Lemma 3 along with \(|S_{}(eb/4cL_{})| e^{}\) and \(_{}(,b) 1\), then we have \((_{}(_{r}),S)\) is bounded by \(||(,n)_{}(24Hb)^{-1/ 2},L_{},n,\), where \(C,c\) are absolute constants._

A non-adversarial version of Theorem 8 was shown in the seminal (34) with a bound of order \(}(||(,n))\). In comparison, in the adversarial setting, the expression in Theorem 8 has the additional factor due to the adversary which is of order \(}()\). Importantly, our bound is also a sub-root function in \(r\) and, thus, suitable for optimistic rates derived from local Rademacher complexity. See Appendix B.2 for a comparison of these results with those in (25).

**Proof Sketch 6**.: _The proof proceeds by using Lemma 3 and analysis similar to the proof of Lemma 1._

Our proof of Theorem 8 depends on the following covering number lemma, which similar to Lemma 6.5. in (25). The proof of this lemma is in Appendix E.2.3.

**Lemma 3**.: _Let \(\) be a class of predictors and let \(_{r}\) be all functions in \(\) with empirical adversarial risk less than \(r\) on \(S\). If \(_{r}\) is \(L_{}\)-Lipschitz w.r.t. \(\|\|_{}\), then under Assumption 2 and Assumption 4, we have \(_{2}(_{}(_{r}),,S) _{},/2,S_{ }/2L_{}\)._

## 5 Conclusion

In this work, we have shown several theorems that demonstrate that a representation derived from adversarial training can assist in defending against adversaries on downstream tasks. Such theorems show how utilizing diverse tasks can assist in learning robust representations in data-scarce or high-stake domains. Some additional questions are how to optimally select source tasks to maximally assist in learning a robust representation and if the assumption of the attack residing within the data domain can be relaxed while retaining our \(}()\) rate in general settings. Our main technical innovation is using a celebrated fat-shattering inequality (31) to carefully control the inflation of the dataset. In doing so we have also shown several novel rates in the single-task setting.