# Spiking Neural Network as

Adaptive Event Stream Slicer

 Jiahang Cao\({}^{1}\) Mingyuan Sun\({}^{2}\) Ziqing Wang\({}^{3}\) Hao Cheng\({}^{1}\)

Qiang Zhang\({}^{1,4}\) Shibo Zhou\({}^{5}\) Renjing Xu\({}^{1}\)

\({}^{1}\) The Hong Kong University of Science and Technology (Guangzhou)

\({}^{2}\) Northeastern University \({}^{3}\) Northwestern University

\({}^{4}\) Beijing Innovation Center of Humanoid Robotics Co. Ltd. \({}^{5}\) Brain Mind Innovation

{jcao248, hcheng046, qzhang749}@connect.hkust-gz.edu.cn

mingyuansun20@gmail.com, ziqingwang2029@u.northwestern.edu

bob@brain-mind.com.cn, renjingxu@hkust-gz.edu.cn

Equal contribution.

###### Abstract

Event-based cameras are attracting significant interest as they provide rich edge information, high dynamic range, and high temporal resolution. Many state-of-the-art event-based algorithms rely on splitting the events into fixed groups, resulting in the omission of crucial temporal information, particularly when dealing with diverse motion scenarios (_e.g._, high/low speed). In this work, we propose **SpikeSlicer**, a novel-designed plug-and-play event processing method capable of splitting events stream adaptively. SpikeSlicer utilizes a low-energy spiking neural network (SNN) to trigger event slicing. To guide the SNN to fire spikes at optimal time steps, we propose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's state. Additionally, we develop a Feedback-Update training strategy that refines the slicing decisions using feedback from the downstream artificial neural network (ANN). Extensive experiments demonstrate that our method yields significant performance improvements in event-based object tracking and recognition. Notably, SpikeSlicer provides a brand-new SNN-ANN cooperation paradigm, where the SNN acts as an efficient, low-energy data processor to assist the ANN in improving downstream performance, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SpikeSlicer.

Figure 1: Comparison of event slicing methods. Traditional methods slice event streams based on prefixed time intervals (a) or event counts (b). In contrast, our approach (c) utilizes SNN as a dynamic event processor for adaptive event slicing. The sliced sub-event streams can be converted into various event representations with robust information and then applied to multiple downstream tasks.

Introduction

Event-based cameras  are bio-inspired sensors that capture event streams in an asynchronous and sparse way. Compared with conventional frame-based cameras, event-based cameras offer numerous outstanding properties: high temporal resolution (with the order of \( s\)), high dynamic range (higher than 120 dB), low latency, and low power consumption. Over recent years, rapid growth has been witnessed in dealing with event data due to the inherent advantages of event-based cameras, such as object tracking [2; 3], depth estimation [4; 5], and recognition [6; 7]. Before applying to various downstream tasks, the event stream must be split by groups and then transformed into different event representations, \(e\)._g_., frame, voxel, or point for deep learning architectures.

In detail, the process of event-to-representation conversion consists of two steps: (1) slicing the raw event stream into multiple sub-event stream groups, and (2) converting these sub-event streams into different event representations. Much of the current research focuses on the second step, aiming to refine event representation  techniques such as time surface  and event spike tensor . Yet, this focus often overlooks the crucial first step of slicing, where issues such as the non-uniformity of the information contained in the fixed-sliced event remain unaddressed.

To address the challenges in the slicing process, we delve into the limitations of traditional slicing techniques. Common methods typically cut the event stream into several fixed groups. For example, slicing event stream with fixed event count  or fixed time intervals [11; 12] as depicted in Fig. 1. However, these fixed-group slicing techniques often lead to problems: they may cause insufficient information capture in low-speed motion scenarios or excessive redundancy in high-speed conditions, thereby failing to accurately capture the dynamic changes in event distribution. Additionally, some hyper-parameters, \(e\)._g_., the length of time interval, are highly-sensitive to the downstream tasks (examples are provided in Appendix C) and must be carefully pre-determined. Although some latest slicing methods [13; 14] propose to adaptively sample the events, there still exists the problem of hyper-parameter tuning which can not achieve a fully learnable and adaptable slicing process.

In order to address the above issues, we propose SpikeSlicer, a novel-design event processing method that can adaptively slice the event streams. To achieve this, SpikeSlicer utilizes an SNN as an event trigger to dynamically determine the optimal moment to split the event stream. Our objectives include: (1) training the SNN to spike at a specific time step for accurate event slicing, and (2) developing a training strategy to identify the best slicing time for a continuous event stream during training. In our paper, we achieve (1) through our newly introduced Spiking Position-aware Loss (SPA-Loss) function, which effectively guides the SNN to spike at the desired time by manipulating the membrane potential. For (2), we implement a Feedback-Update training strategy, where the SNN receives real-time performance feedback from the downstream ANN model for supervision. An overview of our proposed method is depicted in Fig. 2. We evaluate the effectiveness of our proposed SpikeSlicer in two downstream tasks: (i) event-based object tracking, which is strongly sensitive to temporal information and motion dynamics, and (ii) event-based recognition, which is highly related to event density. Extensive experiments validate the effectiveness of the proposed approach.

To sum up, our contributions are as follows:

* We propose SpikeSlicer, a novel plug-and-play event processing method capable of splitting event streams in an adaptive manner.
* We design the SPA-Loss to guide the SNN to trigger spikes at the expected time steps. We then propose a novel Feedback-Update strategy that optimizes the event slicing process based on the ANN feedback.
* Extensive experiments demonstrate that SpikeSlicer significantly improves model performance by up to 11.9% in object tracking and 19.2% in recognition with only a 0.32% increase in energy consumption.

## 2 Background and Related Work

**Event-based Cameras.** They are bio-inspired sensors, which capture the relative intensity changes asynchronously. In contrast to standard cameras that output 2D images, event cameras output sparse event streams. When brightness change exceeds a threshold \(C\), an event \(e_{k}\) is generated containing position \(=(x,y)\), time \(t_{k}\), and polarity \(p_{k}\): \( L(,t_{k})=L(,t_{k})-L(,t_{k}- t_{k} )=p_{k}C\). The polarity of an event reflects the direction of the changes (_i_.\(e\)., brightness increase ("ON") or decrease ("OFF"). In general, the output of an event camera is a sequence of events, which can be described as: \(=\{e_{k}\}_{k=1}^{N}=\{[_{k},t_{k},p_{k}]\}_{k=1}^ {N}\). With the advantages of high temporal resolution, high dynamic range, and low energy consumption, event cameras are gradually attracting attention in the fields of tracking [3; 15], identification , 3D reconstruction [16; 17] and estimation .

**Spiking Neural Network (SNN).** SNNs are potential competitors to artificial neural networks (ANNs) due to their distinguished properties: high biological plausibility, event-driven nature, and low power consumption. In SNNs, all information is represented by binary time series data rather than float representation, leading to energy efficiency gains. Also, SNNs possess powerful abilities to extract spatial-temporal features for various tasks, including recognition , tracking , and image generation . In this paper, we adopt the widely used Leaky Integrate-and-Fire (LIF, [20; 21]) model, which effectively characterizes the dynamic process of spike generation and can be defined as:

\[V[n] = V[n-1]+ I[n],\] (1) \[S[n] =(V[n]-_{}),\] (2)

where \(n\) is the time step and \(\) is the leaky factor that controls the information reserved from the previous time step; \(V[n]\) is the membrane potential; \(S[n]\) denotes the output spike which equals 1 when there is a spike and 0 otherwise; \((x)\) is the Heaviside function. When the membrane potential exceeds the threshold \(_{}\), the neuron will trigger a spike and resets its membrane potential to \(V_{}<_{}\). Meanwhile, when \(==1\), LIF neuron evolves into Integrate-and-Fire (IF) neuron. We also introduce a no-reset membrane potential \(U[n]\), meaning that the membrane potential does not reset, but directly passes the original value to the next time step (_i.e._, \(U[n]=V[n]\) after Eq. 2).

## 3 Our Approach: SpikeSlicer

In this section, we first introduce the concept of event cells for data preparation (Sec. 3.1). We then introduce the adaptive event slicing process by utilizing an SNN as the event trigger (Sec. 3.2). In Sec. 3.3, we introduce a novel Spiking Position-aware Loss (SPA-Loss) to supervise the SNN to slice the event at the precise time. Lastly, we build a feedback-update (Sec. 3.4) strategy that allows the resulting events to be correlated with the feedback from the downstream model, thereby improving overall performance.

### Converting Event Stream to Event Cell

Event streams are asynchronous data that can be represented as a set: \(=\{[x_{i},y_{i},t_{i},p_{i}]\}_{i=1}^{N}\) with a time span of \(T\) (_i.e._, \(t_{i}[t_{0},t_{0}+T]\)). We envision an ideal situation where the SNN is utilized to directly process the raw data stream and slice the event. However, in software simulations, the event stream should be converted into event representations to comply with the input requirements of existing deep learning frameworks. In this paper, following the mainstream research, we adopt the voxel-grid representation  as the input of SNN. We first introduce the definition of event cell:

**Definition 1** (Event cell).: _Consider a small time interval \( t\), event cell is a single-grid event representation in the form of: \(C_{}(x,y,t_{*})=_{}(G_{}(x,y,t,\{t[t_{*},t_ {*}+ t]\}))\), where \(_{}\) denotes the voxel grid  method to process the raw event groups \(G_{}\) (Appendix F) with \(t[t_{*},t_{*}+ t]\) into a grid representation._

Figure 2: Overview of our method. The input events are first fed into an SNN, and the event is determined to be sliced when a spike occurs. To find the accurate slicing time, the neighborhood search method explores other time steps. and feeds event representations to the downstream ANN model (_e.g._, object tracker or recognizer). The ANN model then offers feedback, which guides the SNN in firing spikes at the optimal slicing time by supervising the membrane potential through the Spiking Position-aware Loss \(_{SPA}\).

A whole event stream can be then represented by a list of \(N\) event cells, _i.e_., \(\{C_{}(x,y,t_{0}),C_{}(x,y,t_{0}+ t),...,C_{}(x,y,t_{0}+(N-1)  t)\}\), where \(N=T/ t\) and each cell corresponds to a discrete time index \(n\{0,1,,N-1\}\). The mapping of discrete time to real event time interval is defined as:

\[f_{}(n)=\{t|t[t_{0}+n t,t_{0}+(n+1) t]\}.\] (3)

_e.g_., the time interval of \(C_{}(x,y,t_{0})\) is \(f_{}(0)\). In the following sections, we abbreviate the event cell as \(C[n]\).

**Discussion:** Distinct from the typical voxel grid, an event cell only contains a brief time interval, _i.e_., \( t\) is small. At this point, the entire cell sequence appropriately represents the raw event stream, while simultaneously fulfilling the input requisites for the SNN.

### Adaptive Event Slicing Process

Utilizing SNNs on neuromorphic hardware for processing event streams is low-energy and low-latency [23; 24]. Hence, we adopt the Spiking Neural Network as the event stream slicer, aiming for dynamically slicing the event stream to enhance the downstream performance. Incorporating with the SNN, we now describe the adaptive event slicing process:

Considering an event stream \(\), we first convert \(\) into a list of time-continuous event cells. Event cells are then continuously entered into a SNN (structure details are provided in Appendix L) through a loop operation. Through forward propagation, the features of the last hidden layers (\(h^{L-1}\)) are finally mapped to a single spiking neuron to activate spikes:

\[S_{}=(_{}(h^{L-1})).\] (4)

Once the spiking neuron generates a spike (_i.e_., \(S_{out}=1\)) at current time \(n_{c}\), it is considered a slicing action. This allows us to obtain the time interval from the end of the previous spike to the current spike. Suppose the previous spike happened at time \(n_{p}\), the real event time interval is within \(T_{}=_{n=n_{p}+1}^{n_{c}}f_{}(n)=\{t[t_{ 0}+(n_{p}+1) t,t_{0}+(n_{c}+1) t]\}\). Then, the raw event data within this time interval form an event group, which can be converted to any event representation:

\[_{n_{c}}=(G_{}(x,y,t,T_{})),\] (5)

where \(\) denotes an event representation method (_e.g_., frame , point , graph  and surface -based methods). This representation \(_{n_{c}}\) is then prepared for the downstream tasks.

**Event Slicing Rule:** The slice of the event stream is determined by the state (excited/resting) of the SNN's spiking neuron. Serving as a dynamic event trigger, SNN promptly decides to split events upon spike generation and extracts the precise time interval of the raw event stream. The details of the adaptive event slicing process is shown in Alg. 1.

### Spiking Position-aware Loss

In this section, we propose the Spiking Position-aware Loss (SPA-Loss), which contains two parts: (1) membrane potential-driven loss (Mem-Loss) is used to directly guide the spiking state of the spiking neuron at a specified timestamp, and (2) linear-assuming loss (LA-Loss), which is designed to resolve the dependence phenomenon between neighboring membrane potentials, achieving a more precise spiking time. Moreover, we introduce a (3) dynamic hyperparameter tuning method to avoid the experimental bias caused by the manual setting of hyperparameters.

#### 3.3.1 Membrane Potential-driven Loss

As mentioned in the previous section, the slice position of event is determined upon the spike occurrence. The challenge now lies in directing the SNN to trigger a spike precisely at the optimal position, once the label for this optimal slicing position is provided (in Sec. 3.4).

Consider consecutive event cells as inputs starting from the previous spiking time, suppose we expect SNN to slice the event at \(n^{*}\), _i.e_., a spike \(S_{out}\) is triggered at \(n^{*}\). This corresponds to the membrane potential of the spiking neuron needing to reach the threshold \(V_{th}\) at \(n^{*}\), which inspired us to guide the spike time by directly giving the desired membrane potentials. However, membrane potential returns to the resting state immediately after the occurrence of a spike, which may result in inaccurate guidance at later moments (Appendix G). Thus, we choose to supervise the no-reset membrane potential \(U[n]\) (Eq. 16) to exceed the threshold (_i.e._, \(U[n^{*}] V_{th}\)). The membrane potential-driven loss is defined as:

\[_{Mem}=U[n^{*}]-(1+)V_{th}_{2}^{2},\] (6)

where \( 0\) is a hyperparameter to control the desired membrane potential to exceed the threshold. However, an excessively high \(\) may directly induce a premature spike in the neuron, thereby influencing the membrane potential state at the targeted time step. We provide a proposition to address this problem:

**Proposition 1**.: _Suppose the input event cell sequence has length \(N\), desired spiking time is \(n^{*}\) (\(n^{*}\{0,1,...,N-1\}\)), the membrane potential at time \(n^{*}\) satisfying the constraints:_

\[V_{th} U[n^{*}]( V_{th}+ I[n^{*}],V_{th}),\] (7)

_where \(I[n^{*}]\) is the input synaptic current from Eq.1. Then the spiking neuron fires a spike at time \(n^{*}\) and does not excite spikes at neighboring moments._

The proof is provided in the Appendix H. Based on the proposition, we modify the loss function into:

\[_{Mem}=U[n^{*}]-(1-)U_{ lower}+  U_{ upper}_{2}^{2},\] (8)

where \(U_{ lower}=V_{th}\) and \(U_{ upper}=( V_{th}+ I[n^{*}],V_{th})\) denote the lower and upper bounds of the \(U[n^{*}]\) provided in the proposition, respectively; \(\) balances the desired membrane potential \(U[n^{*}]\) between \(U_{ lower}\) and \(U_{ upper}\). Experiments in Sec. 4.1 demonstrate that Mem-Loss is able to supervise the SNN to determine the slicing of the event flow at a specified timestamp. More details of Mem-Loss are provided in Appendix I.1.

#### 3.3.2 Linear-assuming Loss

However, only using Mem-Loss is unable to guarantee that the spiking neuron can trigger spikes at any expected timestamp. We have the following observations:

**Observation 1** (Hill effect).: Suppose there exists a situation where \(S[n]=1\) and \(U[n] U[n+1]\). If the neuron is expected to activate a spike at time \(n+1\), \(U[n+1]\) will be driven to reach the threshold through the Mem-Loss. Nonetheless, the supervised neuron still exhibits \(U[n] U[n+1]\), causing an early spike at time \(n\).

As illustrated in Fig. 3(a), if \(U[n] U[n+1]\) exists, this membrane potential gap is still inherited after the supervision. In this case, when the later membrane potential is guided to exceed the threshold, the earlier membrane potential reaches the threshold sooner and turns the neuron into the resting state. This poses challenges in obtaining a second spike at the later moment.

Therefore, we expect the later membrane potential to increase monotonically with the time step to reverse the hill effect. Here we use the simplest linear monotonically increasing assumption to construct the loss function:

\[_{LA}=||U[n_{c}]-V_{th}\;}{n^{*}}||_{2}^{2 },&_{n};\\ 0,&,\] (9)

where \(n^{*}\) denotes the expected spike timestep and \(n_{c}\) denotes the current spike timestep, \({ condition}_{n}\) corresponds to the nonlinear monotonically increasing condition that satisfies:

Figure 3: Empirical observations: (a) Hill effect in adaptive slicing process; (b) Impact of hyperparameter \(\) settings on TransT tracker  and (c) DiMP tracker .

\(U[n^{*}]\,\&\,n_{c}<n^{*}\). We expect the membrane potential at \(n_{s}\) to reach \(}{n^{*}}V_{th}\) for the latter membrane potential at the \(n^{*}\) to reach \(V_{th}\) in a linearly increasing form. More explanations are provided in Appendix I.2.

Combined with Mem-Loss and LA-Loss, we defined the SPA-Loss, which guides the adaptive event slicing process in subsequent experiments: \(_{SPA}=_{Mem}+_{LA}\).

#### 3.3.3 Dynamic Hyperparameter Tuning

Although controlling the SNN to spike at a desired location can be achieved through the combination of Mem-Loss and LA-Loss, the utilization of varying \(\) values (Eq. 8) may result in significant fluctuations in experimental results. We have the following observation.

**Observation 2**.: The larger the \(\), the earlier the SNN tends to fire spikes; and vice versa.

A larger \(\) in Eq. 8 implies a higher pre-momentary membrane potential, which results in an earlier spike. Taking the larger-\(\) scenario in Fig. 3(c) as an example, if the SNN is expected to activate a spike at a later timestep, the larger \(\) prevents the actual spike from being delayed. Thus, we need to decrease \(\), causing the expected spike time to shift earlier. This concludes that the update direction of the hyperparameter \(\) should be consistent with the update direction of the desired spiking index.

**Observation 3**.: A fixed \(\) leads to significant variations in performance across different tasks.

As illustrated in Fig. 3(b) and (c), the same \(\) varies significantly on different downstream models, which makes it difficult to set the hyperparameter alpha in advance.

To address the above issues, we design a learning-based hyperparameter tuning method for updating \(\) (in Alg. 2). More details are provided in Appendix I.3.

``` Input: SNN model, input event \(\), the number of event cell \(N\), the previous spike index \(n_{p}\), list \(_{rep}\). Initialize: \(n_{p}=0\) for all\(n=0,1,2,...N-1\)do \(S_{out}=}(C[n])\). if\(S_{out}=1\)then \(n_{c}=n\). \(T_{group}=_{n=n_{p}+1}^{c}f_{time}(n)\). \(_{group}=_{}(x,y,t,T_{group})\). \(_{n_{c}}=(_{group})\).  Append \(_{n_{c}}\). \(_{rep}\). \(n_{p}=n_{c}+1\). endfor endfor Return:\(_{rep}\). ```

**Algorithm 1** Adaptive Event Slicing Process

### Feedback-Update Strategy through SNN-ANN Cooperation

Based on the methods proposed in the previous sections: if the desired trigger time \(n^{*}\) is given, the SNN is able to accurately accomplish the event slicing under the guidance of the SPA-Loss function. In this section, we focus on how to obtain the desired spike time \(n^{*}\) through the downstream ANN feedback. We thus propose a feedback-update strategy that enables the SNN to slice events when the downstream ANN model achieves optimal performance. By receiving real-time feedback from the downstream model and updating \(n^{*}\), this strategy ultimately enhances task performance.

Particularly, when SNN processes the input event and triggers a spike at time \(n_{c}\), it returns a spike output sequence \(S=[0,...0,1,0,...]\), where \(1\) is at \(n_{c}\)-th. We first perform a _neighborhood search_ to obtain \(2d+1\) candidate event representation with the index in \(\{n_{c}-d,...,n_{c}+d\}\): \(\{D_{n_{c}-d},...,D_{n_{c}+d}\}\), where \(D_{n_{c}+i}=(G_{}(x,y,t,\{t[t_{0}+(n_{p}+1) t,t_{0}+(n _{c}+1+i) t]\}))\). We then choose a downstream model \(\) (_e.g._, object tracker or recognizer) and feed the candidate event representations into it to obtain feedback \(y\):

\[y=_{}(C[n_{c}-d])..._{}(C[n_{c}+d]),\] (10)

where \(_{}()\) returns the output loss of \(\) and \(\) concatenates these losses into \(y^{2d+1}\). We choose the model loss as the feedback since it directly reflects the quality of inputs. We can then generate the desired spike index \(n^{*}\) by: \(n^{*}=*{arg\,min}_{i}(y[i])\), where \(*{arg\,min}\) extracts the index with the best (minimal loss) feedback, which in turn guides the dynamic slicing process using SPA-Loss. After training the SNN, the ANN is then updated by feeding the newly-sliced events, thus forming an SNN-ANN cooperation process.

**Feedback-Update Strategy.** This strategy employs a two-stage iterative approach. In the first stage, the ANN offers real-time feedback to the adaptive slicing process for training SNN. In the second stage, the trained SNN provides a newly-sliced event to finetune the ANN. The process then iterates back to the first stage. This strategy provides a novel SNN-ANN cooperation paradigm which establishes a strong connection between raw data and the downstream model. We summarize the feedback-update strategy in Alg. 2.

## 4 Experiments

To evaluate the effectiveness of our proposed method, we set up two-level experiments. In the beginner's arena, we expect the SNN to find the exact slicing time with the simulated event inputs. In the expert's arena, we conduct experiments on event-based object tracking and image recognition. Details of experiment settings and more experimental analyses are presented in the Appendix.

### Beginner's Arena: Event Slicing in Simple Tasks

We first conduct some entry-level tasks to validate the effectiveness of SPA-Loss. We set up the task: _Input \(N\) randomized event cells, expect the SNN to slice at a specified time step \(n^{*}\) and there exists a certain probability of interfering with SNN to slice at other time steps_.

We compare the SPA-Loss function with common MSE-Loss and CE-Loss. The experiment setting is detailed in Appendix K. As depicted in Fig 4(b), the SPA-Loss successfully supervises the SNN to activate spikes at the desired time steps. In particular, SPA-Loss requires only a few iterations (\(<\)400) to supervise the SNN to fire spikes correctly. In contrast, MSE-Loss can only succeed at certain time steps, and CE-Loss cannot even accomplish the task. In addition, using both Mem-Loss and LA-Loss yields smoother results compared to using Mem-Loss alone. To summarize, the beginner's arena preliminarily tests the effectiveness of the SPA-Loss and paves the way for subsequent experiments.

### Expert's Arena: Mastering Adaptive Event Slicing with SNN-ANN Collaboration

After a successful challenge in the beginner's arena, we move on to the expert arena. Here we use a low-energy SNN to adaptively process the event data on complex downstream tasks:

**Event-based Object Tracking.** Since the tracking task is highly sensitive to temporal information, dynamic event slicing is of great importance. We provide two versions for adaptive slicing: SpikeSlicer-Base (B) and SpikeSlicer-Small (S). The detailed consumption of these two versions are introduced in the ablation study. Tab. 1 shows that the tracking performances under the SpikeSlicer have a significant improvement in terms of representative success rate (RSR), representative precision rate (RPR), and overlap precision (OP). For instance, TransT with SpikeSlicer-S's performance on OP\({}_{0.50}\) and OP\({}_{0.75}\) improved by 5.8% and 3.2% compared to its original result under the HDR scenario. When compared to results on the fixed event (the number of the fixed-sliced event is aligned with the number of dynamic-sliced events to ensure a fair comparison), our method achieves favorable gains in the overall RSR, _i.e._, 63.6 _vs._ 51.0.

Figure 4: (a) Experiments on comparing different loss functions on a simple event slicing task. Our proposed Mem-Loss and LA-Loss require only a small number of iterations to supervise the SNN to activate spikes at the desired time steps; (b) Experiments on different hyperparameter settings. Our dynamic tuning method can stably converge towards the optimal spiking time (colored in green). In contrast, using a fixed \(\) results in unstable training and challenges in finding the optimal point.

**Event-based Recognition.** We also conduct experiments in event-based recognition to evaluate the effectiveness of our proposed method. As depicted in Tab. 2, our method has a significant improvement over the fixed-sliced method, with an accuracy improvement of 2.78% and 6.46% by using ResNet-34 in DVS-Gesture and N-Caltech101, respectively. To verify that the results of our adaptive slicing method are not biased due to randomness, we add the random-slice baselines for comparison, in which the event stream is randomly sliced into event groups and fed into the ANN for training. Our method also yields better performance compared with the random-slice results.

**Visualization of Adaptive Event Slicing.** We visualize the tracking results to demonstrate that the dynamic slicing method is able to adapt to various motion scenarios. As shown in Fig. 5, our method obtains better tracking performance compared to fixed event inputs, _i.e_., the position of the prediction box is more accurate. Additionally, our dynamic event slicing method can achieve (1) _edge enhancement_ and (2) _redundancy removal_ to refine the event data under different tracking scenarios. However, the fixed-slice approach adopts the same slicing strategy for each event stream, leading to performance degradation.

### Analysis of the Adaptive Slicing Method

**Analysis of Spike Splitting Time and Event Density.** To evaluate the effectiveness of our proposed method, we conducted a detailed visualization analysis, as depicted in Fig. 6, examining the relationship between the locations of split points and the corresponding event stream densities. The definition of event density is detailed in the Appendix B. The analysis reveals a clear **match** between the positions of cuts made by SNN and the respective event density. Specifically, the SNN tends to perform more frequent cuts in regions of higher event density, while conversely, regions with lower event density experienced fewer cuts. These findings indicate that the dynamic cutting method is effectively adaptive to the varying information density within the event stream.

    &  &  &  &  &  \\   &  & OP\({}_{}\) & OP\({}_{}\) & PRF &  & OP\({}_{}\) & OP\({}_{}\) & PRF &  & OP\({}_{}\) & OP\({}_{}\) & RPF &  & OP\({}_{}\) & OP\({}_{}\) & RPF \\  SiamFC++  & 15.3 & 15.0 & 1.3 & 25.2 & 13.4 & 8.7 & 0.8 & 15.3 & 28.6 & 36.3 & 6.0 & 48.2 & 36.8 & 42.7 & 7.4 & 63.1 & 23.8 & 26.0 & 3.9 & 39.1 \\ KYS  & 15.7 & 14.5 & 5.2 & 23.0 & 12.0 & 8.0 & 1.1 & 18.0 & 47.0 & 63.9 & 14.8 & 73.3 & 36.9 & 44.5 & 15.2 & 57.9 & 26.6 & 30.6 & 9.2 & 41.0 \\ CLNet  & 30.0 & 33.5 & 9.6 & 48.8 & 13.7 & 6.0 & 0.9 & 23.6 & 52.9 & 71.2 & 23.3 & 38.0 & 40.6 & 43.6 & 14.2 & 67.7 & 34.4 & 39.1 & 11.8 & 58.5 \\ DiMP  & 49.1 & 60.3 & 16.3 & 77.1 & 67.3 & 87.4 & 40.4 & 96.9 & 52.5 & 53.9 & 7.8 & 98.5 & 50.0 & 60.1 & 21.4 & 78.2 & 52.1 & 62.4 & 17.9 & 84.3 \\ DiMP (_fixed event_) & 33.3 & 68.2 & 21.4 & 81.6 & 67.6 & 86.3 & 43.1 & 95.0 & 49.7 & 45.4 & 58.8 & 50.5 & 49.4 & 23.7 & 55.3 & 53.8 & 64.3 & 19.5 & 82.4 \\ DiMP+**SpikeSizer-B** & 53.3 & 67.3 & 21.9 & 79.8 & 69.8 & 92.3 & 47.2 & 96.5 & 64.1 & 83.4 & 27.9 & 97.2 & 54.6 & 64.7 & 27.7 & 81.2 & 57.3 & 73.0 & 25.8 & 86.1 \\ DiMP+**SpikeSizer-S** & 56.0 & 72.0 & 27.0 & 78.0 & 77.0 & 92.2 & 50.6 & 60.7 & 86.1 & 38.4 & 96.9 & 56.4 & 70.6 & 31.1 & 81.4 & 59.6 & 76.8 & 30.9 & 96.4 \\  PrDMP  & 50.3 & 62.2 & 19.4 & 77.8 & 68.8 & 90.4 & 41.9 & 97.0 & 56.6 & 68.8 & 11.2 & 98.1 & 53.4 & 64.7 & 23.4 & 82.7 & 54.5 & 67.4 & 20.4 & 85.8 \\ PrDMP (_fixed event_) & 41.2 & 49.8 & 18.4 & 66.1 & 42.7 & 45.2 & 18.7 & 62.4 & 85.5 & 21.3 & 90.4 & 57.4 & 58.3 & 18.5 & 77.0 & 48.6 & 21.2 & 19.2 & 78.6 \\ PrDMP+**SpikeSizer-B** & 53.7 & 67.1 & 22.2 & 80.2 & 70.1 & 41.5 & 41.8 & 97.7 & 20.7 & 89.2 & 54.2 & 93.6 & 54.2 & 93.6 & 71.0 & 25.7 & 83.7 & 59.2 & 75.3 & 29.1 & 86.8 \\ PrDMP+**SpikeSizer-S** & 55.2 & 70.7 & 24.9 & 79.9 & 71.3 & 95.7 & 45.1 & 97.7 & 72.5 & 91.4 & 59.2 & 92.5 & 54.7 & 63.0 & 27.6 & 83.8 & 60.9 & 78.2 & 32.3 & 87.2 \\ TaMoMo  & 37.9 & 43.5 & 15.5 & 66.9 & 46.4 & 30.5 & 0.2 & 87.8 & 50.8 & 54.2 & 10.9 & 96.7 & 40.9 & 36.2 & 7.7 & 17.1 & 42.5 & 41.2 & 1.0 & 78.8 \\ TaMoMo (_fixed event_) & 44.0 & 57.0 & 3.9 & 72.7 & 49.5 & 48.0 & 3.0 & 90.0 & 46.2 & 34.0 & 5.6 & 69.7 & 43.4 & 46.0 & 15.8 & 74.4 & 41.5 & 48.5 & 2.1 & 77.1 \\ TaMoMo+**SpikeSizer-B** & 40.4 & 45.4 & 17.2 & 72.7 & 43.6 & 10.1 & 91.7 & 48.7 & 45.7 & 45.0 & 39.8 & 43.6 & 42.0 & 1.0 & 81.7 & 47.2 & 42.3 & 1.1 & 83.3 \\ TaMoMo+**SpikeSizer-S** & 41.4 & 48.1 & 1.4 & 71.7 & 48.5 & 36.1 & 0.3 & 95.5 & 45.0 & 18.7 & 0.1 & 98.7 & 40.7 & 29.6 & 0.5 & 80.4 & 43.0 & 36.3 & 0.8 & 82.9 \\  TransT  & 58.6 & 69.4 & 27.8 & 70.5 & 70.9 & 49.7 & **93.1** & 98.5 & 55.9 & **59.6** & 73.0 & 87.6 & 73.0 & 92.7 & 87.2 & 61.3 & 78.3 & 33.8 & 89.2 \\ TransT (_fixed event_) & 51.4 & 67.8 & 17.1 & 81.1 & 62.3 & 20.2 & 82.3 & 83.9 & 43.5 & 28.0 & 55.7 & 50.6 & 57.9 & 12.2 & 78.9 & 51.0 & 59.0 & 12.0 & 78.8 \\ TransT+**SpikeSizer-B** & 58.0 & 73.4 & 29.4 & 83.8 & 71.9 & 95.9 & 47.7 & **90.3** & 73.4 & 95.7 & 56.9 & 96.2 & **60.5** & **76.6** & 32.0 & **88.4** & 62.1 & 79.9 & 34.6 & 88.7 \\   

Table 1: Quantitative comparison on FE108. There are four challenging scenarios, including high dynamic range (HDR), low light (LL)

[MISSING_PAGE_FAIL:9]

**Ablation Study on Event Cell Number.** Considering that the number of event cells \(N\) may affect the SNN's decision on event slicing, we examine the stability of the sliced event group by varying the size of \(N\). Tab. 6 shows that the average spike time of the SNN varies for different \(N\), but the time duration (_i.e._, Avg Spike/\(N\)) of the resulting event groups is stable. This verifies that the SNN can effectively make cuts based on event information rather than making decisions based on the number of inputs alone. More details are provided in Appendix P.

**Ablation Study on Different Network Sizes of the SpikeSlicer.** To evaluate the performance of SpikeSlicer under different network sizes, we conduct an additional ablation study (in Tab. 7) with a smaller variant, SpikeSlicer-Small. This lightweight model contains only 0.42M parameters--significantly fewer than the base model's while achieving comparable or better performance across key metrics. This compact design demonstrates its potential for efficient deployment on hardware platforms, providing a strong foundation for real-world applications requiring lightweight neural networks.

## 5 Limitation

We summarize the limitations of this work as follows: Firstly, the SpikeSlicer process involves multi-stage SNN-ANN training, which leaves substantial room for improvement in the adaptive slicing strategy. Secondly, for recognition tasks, we convert the event stream into a single-frame representation to obtain accurate supervisory signals. This approach could be refined in the future to enable SpikeSlicer to slice stream events into multi-frame representations, which are the mainstream format. Thirdly, our experiments are conducted on GPUs; however, the most suitable hardware for SNNs would be brain-inspired chips. In addition, dynamic events need to be generated and processed in real-time during inference, rather than fixed generation in advance. As a result, conducting experiments on GPUs may lead to slower overall inference speeds. Extending this paradigm to brain-inspired chips with asynchronous event input is an interesting direction worth exploring in the future.

## 6 Conclusion

In this work, we proposed SpikeSlicer, a novel event processing method that splits event streams adaptively. SpikeSlicer utilizes a spiking neural network (SNN) as an event trigger, which determines the slicing time according to the generated spikes. To achieve accurate slicing, we designed the Spiking Position-aware Loss (SPA-Loss) which guides the SNN to trigger spikes at the desired time step. In addition, we proposed a Feedback-Update training strategy that allows the SNN to make accurate slicing decisions based on the ANN feedback. Extensive experiments have demonstrated the effectiveness of SpikeSlicer in yielding performance improvement in event-based object tracking and recognition tasks. In the future, we will assess SpikeSlicer's suitability for other event-based tasks, and devise more efficient training strategies for the SNN-ANN cooperative framework to optimize real-time processing in the future.