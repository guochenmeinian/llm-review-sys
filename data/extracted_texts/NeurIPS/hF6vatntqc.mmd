# Transformers are Minimax Optimal Nonparametric In-Context Learners

Juno Kim1,2* Tai Nakamaki1 Taiji Suzuki1,2

1University of Tokyo 2Center for Advanced Intelligence Project, RIKEN

*junokim@g.ecc.u-tokyo.ac.jp

###### Abstract

In-context learning (ICL) of large language models has proven to be a surprisingly effective method of learning a new task from only a few demonstrative examples. In this paper, we study the efficacy of ICL from the viewpoint of statistical learning theory. We develop approximation and generalization error bounds for a transformer composed of a deep neural network and one linear attention layer, pretrained on nonparametric regression tasks sampled from general function spaces including the Besov space and piecewise \(\)-smooth class. We show that sufficiently trained transformers can achieve - and even improve upon - the minimax optimal estimation risk in context by encoding the most relevant basis representations during pretraining. Our analysis extends to high-dimensional or sequential data and distinguishes the _pretraining_ and _in-context_ generalization gaps. Furthermore, we establish information-theoretic lower bounds for meta-learners w.r.t. both the number of tasks and in-context examples. These findings shed light on the roles of task diversity and representation learning for ICL.

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable capabilities in understanding and generating natural language data. In particular, the phenomenon of _in-context learning_ (ICL) has recently garnered widespread attention. ICL refers to the ability of pretrained LLMs to perform a new task by being provided with a few examples within the context of a prompt, without any parameter updates or fine-tuning. It has been empirically observed that few-shot prompting is especially effective in large-scale models (Brown et al., 2020) and requires only a couple of examples to consistently achieve high performance (Garcia et al., 2023). In contrast, Raventos et al. (2023) demonstrate that sufficient pretraining task diversity is required for the emergence of ICL. However, we still lack a comprehensive understanding of the statistical foundations of ICL and few-shot prompting.

A vigorous line of research has been directed towards understanding ICL of single-layer linear attention models pretrained on the query prediction loss of linear regression tasks (Garg et al., 2022; Akyurek et al., 2023; Zhang et al., 2023; Ahn et al., 2023; Mahankali et al., 2023; Wu et al., 2024). It has been shown that the global minimizer of the \(L^{2}\) pretraining loss implements one step of GD on a least-squares linear regression objective (Mahankali et al., 2023) and is nearly Bayes optimal (Wu et al., 2024). Moreover, risk bounds with respect to the context length (Zhang et al., 2023) and number of tasks (Wu et al., 2024) have been obtained.

Other works have examined ICL of more complex multi-layer transformers. Bai et al. (2023); von Oswald et al. (2023) give specific transformer constructions which simulate GD in context, however it is unclear how such meta-algorithms may be learned. Another approach is to study learning _with representations_, where tasks consist of a fixed nonlinear feature map composed with a varying linear function. Guo et al. (2023) empirically found that trained transformers exhibit a separation where lower layers transform the input and upper layers perform linear ICL. Recently, Kim and Suzuki(2024) analyzed a model consisting of a shallow neural network followed by a linear attention layer and proved that the MLP component learns to encode the true features during pretraining. However, they assumed the infinite task and sample size limit and did not study generalization capabilities.

Our contributions.In this paper, we analyze the optimality of ICL from the perspective of statistical learning theory. Our object of study is a transformer consisting of a deep neural network with \(N\)-dimensional output followed by one linear attention layer. The model is pretrained on \(n\) input-output samples from \(T\) nonparametric regression tasks, generated from a suitably decaying distribution on a general function space. Compared to previous works, we take a crucial step towards understanding practical multi-layer transformers by incorporating the representation learning capabilities of the DNN module. From a more abstract perspective, this work can also be situated as a nonlinear extension of meta-learning. Our contributions are highlighted below.

* We develop a general framework for upper bounding the in-context estimation error of the empirical risk minimizer in terms of the approximation error of the neural network and separate _in-context_ and _pretraining_ generalization gaps depending on \(n,T\), respectively.
* In the Besov space setting, we show that **ICL achieves nearly minimax optimal risk**\(n^{-}\) when \(T\) is sufficiently large. Since LLMs are pretrained on vast amounts of data in practice, \(T\) can be taken to be nearly infinite, justifying the emergence of ICL at large scales. We extend the optimality guarantees to nearly dimension-free rates in the anisotropic Besov space and also to learning sequential data with deep transformers in the piecewise \(\)-smooth function class.
* We show that ICL can **improve upon the a priori optimal rate** when the task class basis resides in a coarser Besov space by learning to encode informative basis representations, emphasizing the importance of pretraining on diverse tasks.
* We also derive **information-theoretic lower bounds** for the minimax risk in both \(n,T\), rigorously confirming that ICL is jointly optimal when \(T\) is large (Besov space setting), while any meta-learning method is jointly suboptimal when \(T\) is small (coarser space setting). This separation aligns with empirical observations of a _task diversity threshold_(Raventos et al., 2023).

The paper is structured as follows. In Section 2, the regression tasks and transformer model are defined in an abstract setting. In Section 3, we present the general framework for estimating the ICL approximation and generalization error. In Section 4, we specialize to the Besov-type and piecewise \(\)-smooth class settings and show that transformers can achieve or exceed the minimax optimal rate in context. In Section 5, we derive minimax lower bounds. All proofs are deferred to the appendix; moreover, we provide **numerical experiments** validating our results in Appendix E.

### Other Related Works

Meta-learning.The theoretical setting of ICL is closely related to _meta-learning_, where the goal is to infer a shared representation \(^{}\) with samples from a set of transformed tasks \(_{i}^{}^{}\). When \(^{}\) is linear, fast rates have been established by Tripuraneni et al. (2020); Du et al. (2021), while the nonlinear case has been studied by Meunier et al. (2023) where \(^{}\) is a feature projection into a reproducing kernel Hilbert space. Our results can be viewed as extending this body of work to function spaces of generalized smoothness with a specific deep transformer architecture.

Optimal rates for DNNs.Our analysis extends established optimality results for classes of DNNs in ordinary supervised regression settings to ICL. Suzuki (2019) has shown that deep feedforward networks with the ReLU activation can efficiently approximate functions in the Besov space and thus achieve the minimax optimal rate. This has been extended to the anisotropic Besov space (Suzuki and Nitanda, 2021), convolutional neural networks for infinite-dimensional input (Okumoto and Suzuki, 2022), and transformers for sequence-to-sequence functions (Takakura and Suzuki, 2023). We remark that a work in progress (Imaizumi, 2024) also studies the sample complexity of ICL of transformers in a Sobolev space setting.

Pretraining dynamics for ICL.While how ICL arises from optimization is not fully understood, there are encouraging developments in this direction. Zhang et al. (2023) has shown for one layer of linear attention that running GD on the population risk always converges to the global optimum. This was extended to incorporate a linear output layer by Zhang et al. (2024), and to softmax attention by Huang et al. (2023); Li et al. (2024); Chen et al. (2024). Kim and Suzuki (2024) considered a compound transformer equivalent to ours with a shallow MLP component and proved that the loss landscape becomes benign in the mean-field limit, deriving convergence guarantees for the corresponding gradient dynamics. These analyses indicate that the attention mechanism, while highly nonconvex, may possess structures favorable for gradient-based optimization.

## 2 Problem Setup

### Nonparametric Regression

In this paper, we analyze the ability of a transformer to solve nonparametric regression problems in context when pretrained on examples from a _family_ of regression tasks, which we describe below. Let \(^{d}\) be the input space (\(d\) is allowed to be infinite), \(_{}\) a probability distribution on \(\), and \((_{j}^{})_{j=1}^{}\) a fixed countable subset of \(L^{2}(_{})\). A regression function \(F_{}^{}:\) is randomly generated for each task by sampling the sequence of coefficients \(^{}\) from a distribution \(_{}\) on \((^{})\); the class of tasks \(^{}\) is defined as

\[^{}=\{F_{}^{}=_{j=1}^{}_{j} _{j}^{}|\ _{}.\},\]

endowed with the induced distribution. Each task prompt contains \(n\) example input-response pairs \(\{(x_{k},y_{k})\}_{k=1}^{n}\). The covariates \(x_{k}\) are i.i.d. drawn from \(_{}\) and the responses are generated as

\[y_{k}=F_{}^{}(x_{k})+_{k}, 1 k n,\] (1)

where the noise \(_{k}\) is assumed to be i.i.d. with mean zero and \(|_{k}|\) almost surely.1 In addition, we independently generate a query token \(\) and corresponding output \(\) in the same manner.

We proceed to state our assumptions for the regression model. Informally, we suppose a relaxed version of sparsity and orthonormality of \(_{j}^{}\) and suitable decay rates for the basis expansion. These will be subsequently verified for specific function spaces of interest with their natural decay rate.

**Assumption 1** (relaxed sparsity and orthonormality of basis functions).: _For \(N\), there exist integers \(N< N\) with \(-N+1=N\) such that \(_{}^{},,_{}^{}\) are independent and \(_{1}^{},,_{-1}^{}\) are all contained in the linear span of \(_{}^{},,_{}^{}\). Moreover, there exist \(r,C_{1},C_{2},C_{}>0\) such that satisfies \(C_{1}_{N}_{,N} C_{2}_{N}\) and_

\[\|_{j=}^{}(_{j}^{})^{2}\|_{L^{}( _{})}^{1/2} C_{}N^{r}.\] (2)

Denoting the \(\)-basis approximation of \(F_{}^{}\) as \(F_{,}^{}:=_{j=1}^{}_{j}_{j}^{}\), by Assumption 1 there exist 'aggregated' coefficients \(_{},,_{}\) uniquely determined by \(\) such that \(F_{,}^{}=_{j=}^{}_{j}_{j}^{}\). We define two types of coefficient covariance matrices

\[_{,}:=(_{}[_{j}_{k}])_{ j,k=1}^{N}^{}_{ ,N}:=(_{}[_{j}_{k}])_{ j,k=}^{}^{N N}.\]

**Assumption 2** (decay of \(\)).: _For \(s,B>0\) it holds that \(\|F_{}^{}\|_{L^{}(_{})} B\) for all \(F_{}^{}^{}\) and_

\[\|F_{}^{}-F_{,N}^{}\|_{L^{2}(_{})}^{ 2} N^{-2s}\] (3)

_uniformly over \(_{}\). Furthermore, \((_{,N})\) is bounded for all \(N\) and_

\[0_{,}[(j^{-2s-1}(  j)^{-2})_{j=1}^{}].\] (4)

**Remark 2.1**.: In the simple case where \((_{j}^{})_{j=1}^{}\) is a basis for \(L^{2}(_{})\), we may set \(N=1,=N\) so that the dependency condition of Assumption 1 is trivially satisfied, moreover, \(_{,N}=_{,N}\) and boundedness of \((_{,N})\) automatically follows from (4). However, the assumptions in the stated form also allow for hierarchical bases with dependencies such as wavelet systems. We also note that (3) and (4) entail basically the same rate but are not equivalent: the _uniform_ bound \(|_{j}|^{2} j^{-2s-1}( j)^{-2}\) along with Assumption 1 implies (3). The \(( j)^{-2}\) term can be replaced with any \(g(j)\) such that \(_{j=1}^{}j^{-1}g(j)\) is convergent.

### In-Context Learning

We now describe our transformer model, which takes \(n\) context pairs \(=(x_{1},,x_{n})^{d n}\), \(=(y_{1},,y_{n})^{}^{n}\) and a query token \(\) as input and returns a prediction for the corresponding output. The covariates are first passed through a nonlinear representation or feature mapping \(:^{N}\), which we assume belongs to a sufficiently powerful class of estimators \(_{N}\). Specifically:

**Assumption 3** (expressivity of \(_{N}\)).: \(\|(x)\|_{2} B^{}_{N}\) _for some \(B^{}_{N}>0\) for all \(x\), \(_{N}\). Moreover for some \(_{N}>0\), there exist \(^{*}_{N},,^{*}_{N}_{N}\) satisfying_

\[_{N j}\|^{}_{j}-^{*}_{j}\|_{L^{}( _{})}_{N}.\]

By choosing \(_{N}\) and \(_{N}\) to satisfy the above assumption, we will be able to utilize established approximation and generalization guarantees for families of deep neural networks in Section 4.

The extracted representations \(()=((x_{1}),,(x_{n}))\) are then mapped to a scalar output via a linear attention layer parametrized by a matrix \(_{N}\) for \(_{N}^{N N}\),

\[_{}(,,):=_{k=1}^{n}y_{ k}(x_{k})^{}^{}()=)}{n},(),=( ,)_{N}_{N}\,.\]

Finally, the output is constrained to lie on \([-,]\) by applying \(_{}(u):=\{\{u,\},-\}\), yielding \(f_{}(,,):=_{}( {f}_{}(,,))\). We set \(_{N}=\{^{N N} 0 C_{3} _{N}\}\) for some \(C_{3}>0\) and fix \(=B\) for simplicity.

The above setup is a restricted reparametrization of linear attention widely used in theoretical analyses (see e.g. Zhang et al., 2023; Wu et al., 2024, for more details), where the values only refer to \(\) and the query and key matrices are consolidated into one matrix \(\). The form is equivalent to one step of GD with matrix step size and has been shown to be optimal for a single layer of linear attention for linear regression tasks (Ahn et al., 2023; Mahankali et al., 2023). The placement of the attention layer after the DNN module \(\) is justified by the observation that lower layers of trained transformers act as data representations on top of which upper layers perform ICL (Guo et al., 2023).

During pretraining, the model is presented with \(T\) prompts \(\{(^{(t)},^{(t)},^{(t)})\}_{t=1}^{T}\) where the tasks \(F^{}_{^{(t)}}^{}\), \(^{(t)}_{}\) and tokens \(^{(t)}=(x^{(t)}_{1},,x^{(t)}_{n})\), \(^{(t)}=(y^{(t)}_{1},,y^{(t)}_{n})^{}\), \(^{(t)}\) and \(^{(t)}\) are independently generated as described in Section 2.1, and is trained to minimize the empirical risk

\[=*{arg\,min}_{_{N} _{N}}(),()=_{ t=1}^{T}(^{(t)}-f_{}(^{(t)},^{(t)},^{(t)}) )^{2}.\]

Our goal is to verify the efficiency of ICL as a learning algorithm and show that learning the optimal \(\) allows the transformer to solve new random regression problems \(y=F^{}_{}(x)+\) for \(F^{}_{}^{}\) in context. To this end, we evaluate the convergence of the mean-squared risk or estimation error,

\[():=_{(^{(t)},^{(t)}, ^{(t)},^{(t)})_{t=1}^{T}}[R()], R( ):=_{,,,}[(F^{}_{ }()-f_{}(,,))^{2}].\]

Note that we do not study whether the transformer always converges to \(\); the training dynamics of a DNN is already a very difficult problem. For the attention layer, see the discussion in Section 1.1.

## 3 Risk Bounds for In-Context Learning

In this section, we outline our framework for analyzing the in-context estimation error \(()\). Some additional definitions are in order. The \(\)-covering number \((,,)\) of a metric space \(\) equipped with a metric \(\) for \(>0\) is defined as the minimal number of balls in \(\) with radius \(\) needed to cover \(\)(van der Vaart and Wellner, 1996). The \(\)_-covering entropy_ or _metric entropy_ is given as \((,,):=(,,)\). The \(\)-packing number \((,,)\) is given as the maximal cardinality of a \(\)-separated set \(\{c_{1},,c_{M}\}\) such that \((c_{i},c_{j})\) for all \(i j\). The transformer model class is defined as \(_{N}:=\{f_{}_{N}_{N}\}\).

To bound the overall risk, we first decompose into the approximation and generalization gaps.

**Theorem 3.1** (Schmidt-Hieber (2020), Lemma 4, adapted).: _There exists a universal constant \(C\) such that for any \(>0\) such that \((_{N},\|\|_{L^{}},) 1\),_

\[() 2_{_{N}_{N }}R()+C(+^{2}}{T}\,(_{N},\| \|_{L^{}},)+(B+)).\]

Proof.: The convergence rate of the empirical risk minimizer is established for a fixed regression problem \(y=f^{}(z)+\) in Schmidt-Hieber (2020) when \(\) is Gaussian; we modify the proof to incorporate bounded noise in Appendix B.1. The ICL setup can be reduced to the ordinary case as follows. We consider the entire batch \((,,_{1:n},)\) including the hidden coefficient \(\) as a single datum \(z\) with output \(\). The true function is given as \(f^{}(z)=F^{}_{}()\) and the model class is taken to be \(_{N}\) implicitly concatenated with the generative process \((,,_{1:n},)(,,)\). Then \(R()\), \((_{N},\|\|_{L^{}},)\) and \(T\) agree with the ordinary \(L^{2}\) risk, model class entropy and sample size. 

Here, the second term is the _pretraining_ generalization error dependent on the number of tasks \(T\); the _in-context_ generalization error dependent on the prompt length \(n\) manifests as part of the first term. This separation allows us to compare the relative difficulty of the two types of learning.

Bounding approximation error.In order to bound the first term, we analyze the risk of the choice

\[^{*}=(^{*},^{*}):=((_{,N}+ _{,N}^{-1})^{-1},^{*}_{:})\]

where \(^{*}\) is given as in Assumption 3 for a suitable \(_{N}\) to be determined. The definition of \(^{*}\) approximately generalizes the global optimum \(=((1+)+()_ {d})^{-1}\) for the Gaussian linear regression setup where \(x(0,)\)(Zhang et al., 2023). Since \(_{,N} C_{1}_{N}\) we have \(^{*} C_{1}^{-1}_{N}\) and hence we may assume \(^{*}_{N}\) by replacing \(C_{3}\) with \(C_{3} C_{1}^{-1}\) if necessary.

**Proposition 3.2**.: _Under Assumptions 1-3, it holds that_

\[_{_{N}_{N}}R() R(^{* })}{n} N+}{n^{2}}^{2}N++N^{ -2s}+N^{2}_{N}^{4}+N^{2r+1}_{N}^{2}.\]

The proof is presented throughout Appendix A. The overall scheme is to approximate \(F^{}_{}()\) by its truncation \(F^{}_{,}()\) and the finite basis \(^{}_{:}\) by \(^{*}_{:}\), which incurs errors \(N^{-2s}\) and the terms pertaining to \(_{N}\), respectively. The first three terms arise from the concentration of the \(n\) token representations \(^{}_{:}(x_{k})\). All hidden constants are at most polynomial in problem parameters.

Bounding generalization error.To estimate the metric entropy of \(_{N}\), we first reduce to the metric entropy of the representation class \(_{N}\). Here, \(\|\|_{L^{}}\) refers to the essential supremum over the support of \(_{}\) and also over all \(N\) components for \(_{N}\). The proof is given in Appendix B.2.

**Lemma 3.3**.: _Under Assumptions 1-3, there exists \(D>0\) such that for all \(\) sufficiently small,_

\[(_{N},\|\|_{L^{}},) N^{2} ^{ 2}}{}+\,\,_{N},\|\|_{L^{}}, ^{}}.\]

## 4 Minimax Optimality of In-Context Learning

### Besov Space and DNNs

We now apply our theory to study the sample complexity of ICL when \(_{N}\) consists of (clipped, see (6)) deep neural networks. These can be also seen as simplified transformers with attention layers and skip connections removed. To be precise, we define the set of DNNs with depth \(L\), width \(W\), sparsity \(S\), norm bound \(M\) and ReLU activation \((x)=x 0\) (applied element-wise) as

\[_{}(L,W,S,M)=^{(L)}+b^{(L)} )(^{(1)}x+b^{(1)})^{(1)}^{W d},^{()}^{W W},\]

\[^{(L)}^{W},b^{()}^{W},b^{(L)} ,_{=1}^{L}\|^{()}\|_{0}+\|b^{()}\|_{0}  S,_{1 L}\|^{()}\|_{}\|b^{()}\|_ {} M}.\]The _Besov space_ is a very general class of functions including the Holder and Sobolev spaces which captures spatial inhomogeneity in smoothness, and provides a natural setting in which to study the expressive power of deep neural networks (Suzuki, 2019). Here, we fix \(=^{d}\) for simplicity.

**Definition 4.1** (Besov space).: For \(2 p,0<q\), fractional smoothness \(>0\) and \(r=+1\), the \(r\)th modulus of \(f L^{p}()\) is defined using the difference operator \(_{h}^{r},h^{d}\) as

\[w_{r,p}(f,t):=_{\|h\|_{2} t}\|_{h}^{r}(f)\|_{p},_{h}^ {r}(f)(x)=1_{\{x,x+rh\}}_{j=0}^{r}{r j}(-1)^{r-j}f(x+ jh).\]

Also, the Besov (quasi-)norm is given as \(\|\|_{B^{}_{p,q}}=\|\|_{L^{p}}+||_{B^{}_{p,q}}\) where

\[|f|_{B^{}_{p,q}}:=(_{0}^{}t^{-q}w_{r, p}(f,t)^{q}t}{t})^{1/q}&q<\\ _{t>0}t^{-}w_{r,p}(f,t)&q=\]

and the Besov space is defined as \(B^{}_{p,q}()=\{f L^{p}()\|f\|_{B^{}_ {p,q}}<\}\). We write \((B^{}_{p,q}())\) for the unit ball in \((B^{}_{p,q}(),\|\|_{B^{}_{p,q}})\).

We have that the Holder space \(C^{}()=B^{}_{,}()\) for order \(>0,\) and the Sobolev space \(W^{m}_{2}()=B^{m}_{2,2}()\) for \(m\) as well as the embeddings \(B^{m}_{p,1}() W^{m}_{p}() B ^{m}_{p,}()\); if \(>d/p\), \(B^{}_{p,q}()\) compactly embeds into the space of continuous functions on \(\). See Triebel (1983); Gine and Nickl (2015) for more details. The difficulty of learning a regression function in the Besov is quantified by the minimax risk; the following rate is classical.

**Proposition 4.2** (Donoho and Johnstone (1998)).: _The minimax risk for an estimator \(_{n}\) with \(n\) i.i.d. samples \(_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\) over \((B^{}_{p,q}())\) satisfies_

\[_{_{n}:_{n}}_{f^{}(B^{}_{p,q}())}_{_{n}}[\|f^{}- _{n}\|^{2}_{L^{2}()}] n^{-}.\]

A natural basis system for \(B^{}_{p,q}()\) is formed by the _B-splines_, which can be seen as a type of wavelet decomposition or multiresolution analysis (DeVore and Popov, 1988). As B-splines are piecewise polynomials, they can be efficiently approximated by DNNs with at most log depth (Suzuki, 2019).

**Definition 4.3** (B-spline wavelet basis).: The tensor product B-spline of order \(m\) satisfying \(m>+1-1/p\), at resolution \(k_{ 0}^{d}\) and location \( I^{d}_{k}=_{i=1}^{d}[-m:2^{k_{i}}]\) is

\[^{d}_{k,}(x)=_{i=1}^{d}_{m}(2^{k_{i}}x_{i}-_{i}), _{m}(x)=(_{m+1})(x),(x)=1_{\{x\}}.\]

When \(k_{1}==k_{d}\), we abuse notation and write \(^{d}_{k,}\) for \(k_{ 0}\) in place of \(^{d}_{(k,,k),}\).

### Estimation Error Analysis

To apply our framework, we set the task class as the unit ball \(^{}=(B^{}_{p,q}())\) and take as basis \(\{^{}_{j} j\}=\{2^{kd/2}^{d}_{k,} k _{ 0}, I^{d}_{k}\}\) the set of all B-spline wavelets ordered primarily by increasing \(k\) and scaled to counteract the dilation in \(x\). Abusing notation, we also write \(_{k,}\) to denote the coefficient in \(\) corresponding to \(2^{kd/2}^{d}_{k,}\). The set of B-splines at each resolution are independent, while those of lower resolution can always be decomposed into a linear sum of B-splines of higher resolution satisfying certain decay rates, which we prove in Proposition C.10.

From this setup, in Appendix C.1.1, we verify Assumptions 1 and \(2\) with \(r=1/2,s=/d\) under:

**Assumption 4**.: \(^{}=(B^{}_{p,q}())\)_, \(>d/p\) and \(_{}\) has positive Lebesgue density \(_{}\) bounded above and below on \(\). Also, all coefficients \(_{k,}\) are independent and_

\[_{}[_{k,}]=0, 0<_{}[_{k,}^{2 }] 2^{-k(2+d)}k^{-2}, k 0,\  I^{d}_{k}.\] (5)

We can check that we have not given ourselves an easier learning problem with (5): the assumed variance decay rate is tight (up to the logarithmic factor \(k^{-2}\)) in the sense that any \(f(B^{}_{p,q}())\) can indeed be expanded into a sum of wavelets with the same coefficient decay when averaged over \( I^{d}_{k}\). See Lemma C.1 and the following discussion. We also obtain the following in-context approximation and entropy bounds in Appendix C.1.2.

**Lemma 4.4**.: _For any \(_{N}>0\), Assumption 3 is satisfied by taking_

\[_{N}=\{_{B^{}_{N}}=(_{j})_{j=1}^{N}, _{j}_{}(L,W,S,M)\}\] (6)

_where \(_{B^{}_{N}}\) is the projection in \(^{N}\) to the centered ball of radius \(B^{}_{N}=O()\) and each \(_{j}\) is a ReLU network such that \(L=O( N+_{N}^{-1})\) and \(W,S,M=O(1)\). Also, the metric entropy of \(_{N}\) is bounded as \((_{N},\|\|_{L^{}},) N }\)._

Hence Assumptions 1-3 all follow from Assumption 4, and we conclude in Appendix C.1.3:

**Theorem 4.5** (minimax optimality of ICL in Besov space).: _Under Assumption 4, if \(n N N\),_

\[() N^{-}( \\ \\ )+( \\ \\ )+ N}{T}( {c}\\ \\ ).\]

_Hence if \(T n^{}\) and \(N n^{}\), in-context learning achieves the minimax optimal rate \(n^{-}\) up to a log factor._

The first term arises from the \(N\)-term truncation and oracle approximation error of the DNN module, and is equal to the \(N\)-term optimal error (Dung, 2011). The second and third term each correspond to the in-context and pretraining generalization gap. With regard to \(N\), we see that \(n=(N)\) is enough to learn the basis expansion in context, while \(T=(N^{2})\) is necessary to learn the attention layer. However if \(T/N=o(n)\), the third term dominates and the overall complexity scales suboptimally as \(T^{-}\), illustrating the importance of sufficient pretraining. This also aligns with the task diversity threshold observed by Raventos et al. (2023). Since the amount of training data for LLMs is practically infinite in practice, our result justifies the effectiveness of ICL at large scales with only a small number of in-context samples.

A limitation of ICL.In the regime \(1 p<2\), the approximation error is strictly worse without an _adaptive_ representation scheme and the resulting rate is suboptimal (see Remark C.3). While DNNs can adapt to task smoothness in supervised settings (Suzuki, 2019), ICL and any other meta-learning methods are fundamentally constrained to non-adaptive representations since they cannot update at inference time, and hence are bounded below by the best linear approximation rate or Kolmogorov width, which is strictly worse than the minimax optimal rate when \(p<2\). Indeed, for any \(N\)-dimensional subspace \(_{N} B^{}_{p,q}()\) it holds that (Vybiral, 2008)

\[_{_{N}}_{f^{}(B^{}_{p,q}())}_{_{n}_{N}}\|f^{}-_{n}\|_{L^{2}( _{})} N^{-/d+(1/p-1/2)_{+}}.\]

**Remark 4.6**.: The \(N^{2} N\) term in the pretraining generalization gap is due to the covering bound of the attention matrix \(\), while the entropy of the DNN class is only \(N N\). Hence the task diversity requirement may be lessened to the latter by considering low-rank structure or approximation of attention heads (Bhojanapalli et al., 2020; Chen et al., 2021).

### Avoiding the Curse of Dimensionality

The above rate inevitably suffers from the curse of dimensionality as \(d\) appears in the exponent of the optimal rate. We also consider the _anisotropic Besov space_(Nikol'skii, 1975), a generalization allowing for different degrees of smoothness \((_{1},,_{d})\) in each coordinate. Then the optimal rate is nearly dimension-free in the sense that the rate only depends on \(d\) through the quantity \(:=(_{i}_{i}^{-1})^{-1}\), and becomes independent of dimension if only a few directions are important i.e. have small \(_{i}\). Rigorous definitions, statements and proofs are provided in Appendix C.2.

Extending Theorem 4.5, we show that ICL again attains near-optimal estimation error in the anisotropic Besov space, circumventing the curse of dimensionality and theoretically establishing the efficiacy of in-context learning in high-dimensional settings.

**Theorem 4.7** (informal version of Theorem C.7).: _For the anisotropic Besov space of smoothness \((_{1},,_{d})\), assume variance decay (4) with \(s=>1/p\). If \(T nN\) and \(N n^{}\), in-context learning achieves the minimax optimal rate \(n^{-}\) up to a log factor._

### Learning a Coarser Basis

Thus far, we have demonstrated the importance of sufficient pretraining to achieve optimal risk; as another application of our framework, we illustrate how pretraining can actively _mitigate_ the complexity of in-context learning. Consider the case where \((_{j}^{})_{j=1}^{}\) is no longer the B-spline basis of \(B_{p,q}^{}()\) but instead is chosen from some wider function space, say the unit ball of \(B_{p,q}^{}()\) for a smaller smoothness \(<\). Without knowledge of the basis, the sample complexity of learning any regression function \(F_{}^{}\) is a priori lower bounded by the minimax rate \(n^{-}\) by Proposition 4.2. For ICL, this difficulty manifests as an increase in the metric entropy of the class \(_{N}\) which must be powerful enough to approximate \(_{1:N}^{}\) (Corollary C.12), giving rise to the modified risk bound:

**Corollary 4.8** (ICL for coarser basis).: _Suppose \(>>d/p\), the basis \((_{j}^{})_{j=1}^{}(B_{p,q}^{}())\) and Assumptions 1, 2 hold with \(r=1/2,s=/d\). Then if \(n N N\),_

\[() N^{-}+++d}^{3}N}{T}.\]

_Hence if \(T n^{1+}\) and \(N n^{}\), the risk converges as \(n^{-} n\)._

The pretraining generalization gap is now dominated by the higher complexity \(N^{1++}^{3}N\) of the DNN class and strictly worse compared to \(N^{2} N\) for Theorem 4.5. The required number of tasks also suffers and the exponent is no longer \((1,2)\) but scales as \(O(d)\). Nevertheless, observe that the burden of complexity is entirely carried by \(T\); with sufficient pretraining, the third term can be made arbitrarily small and the ICL risk again attains \(n^{-}\). Hence ICL improves upon the a priori lower bound \(n^{-}\) at inference time by encoding information on the coarser basis during pretraining. We remark that the result is also readily adapted to the anisotropic setting.

### Sequential Input and Transformers

We now consider a more complex setting where the inputs \(x^{d}\) are bidirectional _sequences_ of tokens (e.g. entire documents) and \(\) is itself a transformer network.2 In this infinite-dimensional setting, transformers can still circumvent the curse of dimensionality and in fact achieve near-optimal sample complexity due to their parameter sharing and feature extraction capabilities (Takakura and Suzuki, 2023). Our goal in this section is to extend this guarantee to ICL of trained transformers.

For sequential data, it is natural to suppose the smoothness w.r.t. each coordinate can vary depending on the input. For example, the position of important tokens in a sentence will change if irrelevant strings are inserted. To this end, we adopt the _piecewise \(\)-smooth function class_ introduced by Takakura and Suzuki (2023), which allows for arbitrary bounded permutations among input tokens; see Appendix D.1 for definitions. Also borrowing from their setup, we consider multi-head sliding window self-attention layers with window size \(U\), embedding dimension \(D\), number of heads \(H\) with key, query, value matrices \(K^{(h)},Q^{(h)}^{D d},\ V^{(h)}^{d d}\) and norm bound \(M\) defined as3

\[_{}(U,D,H,M)=g:^{d} ^{d}\ \ _{1 h H} K^{(h)}_{} Q^{(h)}_{ } V^{(h)}_{} M,\]

\[g(x)_{i}=x_{i}+_{h=1}^{H}V^{(h)}x_{i-U:i+U}((K^{(h)}x_{i- U:i+U})^{}Q^{(h)}x_{i})}.\]

We also consider a linear embedding layer \((x)=Ex+P\), \(E^{D d}\) with absolute positional encoding \(P^{D}\) of bounded norm. Then the class of depth \(J\) transformers is defined as

\[_{}(J,U,D,H,L,W,S,M):=f_{J} g_{J}  f_{1} g_{1}|\  E M,\] \[f_{i}_{}(L,W,S,M),\,g_{i}_{ }(U,D,H,M)}.\]

Our result, proved in Appendix D.2, reads:

**Theorem 4.9** (informal version of Theorem D.1).: _Suppose \(^{}\) consists of functions on \(^{d}\) of bounded piecewise \(\)-smooth and \(L^{}\)-norm with smoothness \(_{>0}^{d}\), and let \(\) be mixed or anisotropic smoothness with \(^{}=_{i,j}_{ij}\) or \((_{i,j}_{ij}^{-1})^{-1}\), respectively. Under suitable regularity and decay assumptions, by taking \(_{N}\) to be a class of clipped transformers it holds that_

\[() N^{-2^{}}++ )}(N)}{T}.\]

_Hence if \(T nN^{1 1/^{}}\) and \(N n^{+1}}\), ICL achieves the rate \(n^{-}{2^{}+1}}(n)\)._

This matches the optimal rate in finite dimensions independently of the (possibly infinite) length of the input or context window. The dynamical feature extraction ability of attention layers in the \(_{}\) class is essential in dealing with input-dependent smoothness, further justifying the efficiacy of ICL of sequential data.

## 5 Minimax Lower Bounds

In this section, we provide lower bounds for the minimax rate in both \(n,T\) by extending the theory of Yang and Barron (1999), which can be leveraged to yield results stronger than optimality in merely \(n\). The bound is purely information-theoretic and hence applies to not just ICL but any meta-learning scheme for the regression problem of Section 2.1 from the data \(_{n,T}=\{(^{(t)},^{(t)})\}_{t=1}^{T+1}\), where the index \(T+1\) corresponds to the test task.

For this section we assume that the noise (1) is i.i.d. Gaussian, \(_{k}(0,^{2})\), instead of bounded; while the exact shape of the noise distribution is not important, having restricted support may convey additional information and affect the minimax rate. We also suppose for simplicity that the support of \(_{}\) is included in \(:=\{^{}\ |\ |_{j}|^{2} j^{-2s-1}( j)^{-2},\ j \}\) and that the aggregated coefficients \(_{j}\) for \(N j\) satisfy \(_{}[_{j}^{2}]_{}^{2}\) for some \(_{}\) dependent on \(N\). The proof of the following statement is given in Appendix F.1.

**Proposition 5.1**.: _For \(_{n,1},_{n,2},_{n}>0\), let \(Q_{1}\) and \(Q_{2}\) be the \(_{n,1}\)- and \(_{n,2}\)-covering numbers of \(_{N}\) and \(\) respectively, and \(M\) be the \(_{n}\)-packing number of \(^{}\). Suppose that the following conditions are satisfied:_

\[}(n(T+1)_{}^{2}_{n,1}^{2}+C_{2 }n_{n,2}^{2}) Q_{1}+ Q_{2} M,  4 2 M.\] (7)

_Then the minimax rate is lower bounded as_

\[_{:_{n,T}}_{f^{}^{}}_{_{n,T}}[\|-f^{}\|_{L^{2}( _{X})}^{2}]_{n}^{2}.\]

Finally, Proposition 5.1 is applied to obtain concrete lower bounds for the settings studied in Section 4 throughout Appendices F.2-F.4.

**Corollary 5.2** (minimax lower bound).: _The minimax rates in the previous regression settings are lower bounded as follows._

* _Besov space (Section_ 4.2_):_ \(_{}_{f^{}}_{_{n,T}}[\| -f^{}\|^{2}] n^{-}\)_,_
* _Coarser basis (Section_ 4.4_):_ \(_{}_{f^{}}_{_{n,T}}[\| -f^{}\|^{2}] n^{-}+(nT)^{-}\)_,_
* _Sequential input (Section_ 4.5_):_ \(_{}_{f^{}}_{_{n,T}}[\| -f^{}\|^{2}] n^{-}{2^{}+1}}\)_._

These results match the upper bounds for (i), (iii) and show that **ICL is provably jointly optimal** in \(n,T\) in the 'large \(T\) regime. Moreover, we can check for the coarser basis setting that insufficient pretraining \(T=O(1)\) indeed leads to the worse complexity \(n^{-}\), while the faster rate \(n^{-}\) is retrieved when \(T n^{}\). This aligns with the discussion in Section 4.4, showing that ICL is **provably suboptimal** in the'small \(T\)" regime.

**Remark 5.3**.: The obtained upper and lower bounds in the coarser basis setting are not tight as \(T\) varies, hence it remains to be shown whether there exists a meta-learning algorithm that attains the lower bound (ii). The task diversity threshold for optimal learning suggested by the bounds are also different (\(n^{1+}\) v.s. \(n^{}\)); it would be interesting for future work to resolve this gap.

Conclusion

In this paper, we performed a learning-theoretic analysis of ICL of a transformer consisting of a DNN and a linear attention layer pretrained on nonparametric regression tasks. We developed a general framework for bounding the in-context estimation error of the empirical risk minimizer in terms of both the number of tasks and samples, and proved that ICL can achieve nearly minimax optimal rates in the Besov space, anisotropic Besov space and \(\)-smooth class. We also demonstrated that ICL can improve upon the a priori optimal rate by learning informative representations during pretraining. We supplemented our analyses with corresponding minimax lower bounds jointly in \(n,T\) and also performed numerical experiments validating our findings. Our work opens up interesting approaches of adapting classical learning theory to study emergent phenomena of foundation models.

Limitations.Our transformer model is limited to a single layer of linear self-attention and does not consider more complex in-context learning behavior which may arise in transformers with multiple attention layers. Moreover, the obtained upper and lower bounds are not tight in certain regimes, suggesting future research directions for meta-learning.