# IMPACT: A Large-scale Integrated Multimodal Patent Analysis and Creation Dataset for Design Patents

Homaira Huda Shomee   Zhu Wang   Sourav Medya   Sathya N. Ravi

Department of Computer Science, University of Illinois Chicago

{hshome2,zwang260,medya,sathya}@uic.edu

###### Abstract

In this paper, we introduce Impact (Integrated Multimodal Patent Analysis and CreaTion Dataset for Design Patents), a large-scale multimodal patent dataset with detailed captions for design patent figures. Our dataset includes half a million design patents comprising 3.61 million figures along with captions from patents granted by the United States Patent and Trademark Office (USPTO) over a 16-year period from 2007 to 2022. We incorporate the metadata of each patent application with elaborate captions that are coherent with multiple viewpoints of designs. Even though patents themselves contain a variety of design figures, titles, and descriptions of viewpoints, we find that they lack detailed descriptions that are necessary to perform multimodal tasks such as classification and retrieval. Impact closes this gap thereby providing researchers with necessary ingredients to instantiate a variety of multimodal tasks. Our dataset has a huge potential for novel design inspiration and can be used with advanced computer vision models in tandem. We perform preliminary evaluations on the dataset on the popular patent analysis tasks such as classification and retrieval. Our results indicate that integrating images with generated captions significantly improves the performance of different models on the corresponding tasks. Given that design patents offer various benefits for modeling novel tasks, we propose two standard computer vision tasks that have not been investigated in analyzing patents as future directions using Impact as a benchmark viz., 3D Image Construction and Visual Question Answering (VQA). To facilitate research in these directions, we make our Impact dataset and the code/models used in this work publicly available here.

## 1 Introduction

Design patents are important for intellectual property protection in industries where aesthetics and user experience are predominant, such as consumer electronics, fashion, and automotive design . Unlike utility patents, which protect the functional aspects of an invention, design patents safeguard the ornamental or aesthetic features of a product. This includes elements such as shape, surface ornamentation, and overall visual appearance. Design patents provide inventors with exclusive rights to their creations and prevent others from producing, selling, or using designs that are substantially similar. In the past few years, the number of design patents granted in the US is increasing. Though these are publicly available by law, the data is not well-organized for patent analyses which could help patent offices as well as can serve as a benchmark for the machine learning community.

**Existing works on patent datasets.** A variety of patent data formats, including XML, TSV, TIFF, and PDF, are publicly accessible for bulk download from multiple sources. One notable dataset, USPTO2M , comprises approximately 2 million utility patents specifically curated for classification tasks. Another dataset, BIGPATENT , contains a corpus of 1.3 million utility patents, exclusively in textual format. Additionally, the Harvard University Patent Dataset (HUPD) consists of a broader range of data fields, compiling 4.5 million patents intended for multipurpose patent analysis. _These datasets focus only on utility patents_. Despite the presence of figures in utility patents, these datasets omit the visual data, concentrating solely on text to support research in natural language processing (NLP). Among few works on _design patents_, DeepPatent  has curated a dataset focused on patent drawings, which is designed to enhance the retrieval of technical illustrations. This dataset includes 45,000 unique design patents, covering the year 2018 and the first six months of 2019. DeepPatent2  has introduced a dataset with over 2.7 million technical drawings extracted from 14 years of U.S. design patent documentation spanning 2007 to 2020. This dataset includes brief uninformative captions that mention only the viewpoints of the figures. In contrast, our dataset is larger and has richer information with 11 fields and elaborated captions.

**Limitations of existing datasets.** The limitations of the existing works are as follows.

* **Single modality:** Existing datasets primarily focus on either text-based or image-based data and thus, neglects the integration of both modalities. This singular approach restricts their applicability in a comprehensive patent analysis and other multimodal tasks.
* **Lack of large & organized design patent datasets:** Most datasets consist of utility patents. The only existing two design datasets are either small or do not include data from recent years. They also lack other meta information.
* **Lack of important information:** Since design patents do not include detailed descriptions of the figures, the absence of descriptive captions makes patent analysis difficult. This is particularly impactful for tasks such as searching for prior art or work, which are essential for preventing patent infringement. Current datasets do not include such descriptive information.

**Our contributions, Impact:** To address these limitations, we develop a new large dataset Impact that integrates both textual and visual information from the design patents within a unified framework. Our major contributions are as follows.

* **Multimodality:** We introduce a multimodal patent dataset that includes patent images, metadata, and detailed captions to support a variety of NLP, vision, and multimodal tasks. This dataset is also valuable for patent analysis tasks such as classification, retrieval, prior art searches, and design trend analysis.
* **Comprehensive dataset:** We have compiled a collection of 435,101 patents spanning 16 years from U.S. design patent documents. This extensive collection includes a total of 3,609,805 drawing figures. Additionally, our dataset consists of eleven fields such as the title, patent ID, claims, date of publication, classification code, and extensive image-related information, including the number of images per patent and descriptions of the viewpoints.

Figure 1: The main components of Impact dataset. We collect and pre-process the raw data to construct a comprehensive well-structure and accessible design patent dataset. We integrate multimodal information and design various training and application scenarios.

* **Descriptive captions**: To address the absence of descriptions about the designs, such as features and shapes, we generate elaborated captions by employing a vision-language model. It generates descriptive captions for the design figures, capturing details from the sketch. These captions, coupled with the images, enrich our dataset and becomes a valuable resource for advanced patent analysis and multimodal research applications. **Code and data.** The codebase and data are available at the following link: https://github.com/A14Patents/IMPACT.

## 2 Background and Related Work

### Background on Design Patents

A patent is a legal document granting exclusive rights to an inventor for a specific period and it allows the inventor to exclude others from making, using, or selling their invention1. Specifically, we focus on design patents and it has limited information compared to utility patents. While a utility patent is composed of a title, an abstract, multiple detailed claims, a thorough description, and drawings, along with some metadata; the design patents are more limited in textual content consisting of only a title, a single claim, and small descriptions of the figures, alongside drawings of the invention. The claim in a design patent typically starts with "The ornamental design for..." and does not include the extensive, detailed claims that are rich in textual data found in utility patents. The descriptions of the figures cover different viewpoints such as perspective, top, front, rear, bottom, and enlarged details, as depicted in the drawings (please see examples in Figure 7 in the Appendix). These drawings are presented on drawing sheets, where sometimes one sheet may display multiple figures, thus showing different viewpoints in a composite figure. Beyond this, there is no additional text data to describe the images, their shapes, or functionality, since these figures lack any sort of captions.

**Challenges.** While the utility patents have already elaborated set of information, working with design patents are challenging due to several reasons. _First_, design patents protect the ornamental aspects of a product and they are inherently subjective. Determining whether a new design infringes on an existing design patent often involves interpreting visual as well as aesthetic elements and this process is more difficult than with the utility patents. _Second_, the scope of a design patent is determined by the drawings and these drawings must be meticulously accurate. However, the quality of these drawings are often not suitable for the existing machine learning models to use. _Third_, design patents only protect the appearance of an object, not its functional aspects. This is limited as these patents do not have any other information. We overcome this by incorporating elaborated captions in this dataset (see Section 3). To combat these challenges, we believe curating a unified dataset for patent analyses and other machine learning tasks (e.g., VQA) will be extremely useful.

### Related Work on Patent Analysis

**Patent classification.** One of the major patent analysis tasks is assigning CPC or IPC codes2 (utility patents) or design codes3 (design patents) to the submitted patents, which is time-consuming due to the numerous classification codes and their hierarchical structure. Various models have been proposed in the literature to automate this process and they can be categorized into three major types . _First, the traditional methods_ share a two-step approach: generating initial features followed by using a classifier. For instance,  use a single-layer LSTM with Word2Vec features for IPC subgroup classification. Similarly,  employ LSTM with fixed hierarchy vectors for IPC subclass classification.  and  train fastText embeddings on 5 million patents and used Bi-GRU for classification.  extracts key patent sections, trained Word2Vec, and used parallel LSTMs. _Second, the ensemble models_ use different word embeddings and deep learning techniques.  uses SVM as a baseline, experimenting with various datasets, features, and semi-supervised learning.  and  explore ensemble models with Bi-LSTM, Bi-GRU, LSTM, and GRU, focusing on word embeddings and partitioning techniques, respectively. In contrast, patent images are classified into visualization types using the CLIP model with MLP and various CNN models in . _Third, among the LLM-based approaches_, as the first study,  fine-tunes the BERT model on the USPTO-2Mdataset. Similarly,  fine-tunes BERT, XLNet, and RoBERTa on USPTO-2M, establishing XLNet as the new state-of-the-art with the highest precision, recall, and F1 measure.  use domain-adaptive pre-training with Linguistically Informed Masking, showing that SciBERT, pre-trained on scientific literature, outperforms BERT in patent classification.

**Patent retrieval.** The patent retrieval task  aims to efficiently retrieve relevant patent documents and images based on search queries. This is important for identifying new patents, evaluating novelty, and avoiding infringement. Patent image retrieval can also inspire design. _First, traditional machine learning_ methods for patent retrieval,  outlines five technical requirements for AI feasibility, including query expansion and identifying semantically similar documents.  uses random forest, Support Vector Regression, and Decision Trees to merge search results effectively. _Second, neural network-based methods_ have recently gained popularity for patent retrieval. , , and  use CNN, DUAL-VGG, and ResNet, respectively, for retrieving patent images based on query images. _Third, among LLM-based methods,_ utilizes CLIP for image embedding and RoBERTa for textual features, enhancing searches with visual and textual data.  uses BERT with combinations of title, abstract, and claim.  employs SBERT for text embeddings and TransE for citation and inventor knowledge graph embeddings and finds that mean cosine similarity among patent vectors effectively links multiple existing patents to a target patent.

**Patent quality analysis.** Businesses usually evaluate patent value due to its impact on revenue and investment  and investors aim to predict the future value of technological innovations when making decisions. Consequently, companies hire professional analysts for quality analysis as it requires substantial effort and domain expertise . MLP-based approaches utilizing several indices have been employed in the past . These indices usually include claim counts, forward citations, backward citations etc.  classifies patents based on their maintenance period into four categories using a Bi-LSTM with attention mechanism and Conditional Random Field (CRF) to assess patent quality in its initial stages.  predict forward citations and investor reactions to patent announcements using CNN-LSTM neural networks and various ML models. , and  apply neural networks such as CNN, Bi-LSTM, Attention-based CNN (ACNN), and deep and wide Artificial Neural Networks (ANN), respectively. Additionally, large models such as a variation of BERT (MSABERT) has been also employed to assess patent value based on the textual data .

**Patent generation.** Recently, as generative models are gaining popularity, they have been used to reduce the need for human effort in writing long patent documents.  uses GPT-2 to generate independent patent claims, fine-tuning on patent claims from USPTO. This method generates the first claim given a few words but lacks quantitative metrics for evaluating claim quality.  focuses on personalized claim generation by fine-tuning GPT-2 with inventor-centric data, using BERT to assess the relevance of generated claims to the inventor.

Most of the above approaches are applied to _utility patents_ where the patents have detailed information. One of the major reasons that the _design patents_ are not well-studied is because of the lack of suitable datasets. Our work aims to address this gap by building a comprehensive dataset.

## 3 Our Impact Dataset

Our dataset, Impact includes 435,101 design patents issued by the USPTO between January 2007 and December 2022 with a total of 3,609,805 images. We provide a discussion of the data collection methodology, data format, structure, and key statistical attributes of the dataset. Additionally, we discuss the limitations associated with the dataset. Please see more details on the dataset and its analyses in the Appendix.

**Data Collection & Processing.** US patent data is publicly available, but it is stored in a complex format and not suitable for usual machine learning tasks. Thus, we collect and organize the data into an unified and accessible format for NLP and Computer Vision (CV) tasks. We have obtained the data from the USPTO Bulk Data Storage System (BDSS)4. The patent grant full-text data with embedded TIFF Images is published weekly as tar files, containing data for all three types of patents--utility, design, and plant patents. Each design patent includes one XML file accompanied by multiple TIFF images, with each image describing different angels of the design. More specifically, we have collected the data for design patents to process and construct our dataset.

**Data Format & Fields.** From the XML files, we have collected and created 11 necessary fields and stored them in year-wise CSV files. All the images are kept in their original form, i.e., TIF format, as provided by the USPTO. For each design patent, the dataset has a separate folder named using the format USDID-date. For example, _USD0534331-20070102_, where _D0534331_ is the document number/ID and _20070102_ is the date. We have included 11 fields in the dataset. These are as follows: title, ID, claim, date, classification, US field of classification search5, applicent or inventor country, number of figures for the design, number of drawing sheets, names of image files, and descriptions of figures. Detailed descriptions with examples of all fields are provided in Table 5 ( App. A.1).

**Quantitative Analyses.** Figure 2 illustrates the patent statistics in terms of the number of design patents published annually and the corresponding figures in them. Figure 1(a) shows line charts depicting the trend in the number of patents over time and average number of figures per patent. The data shows a general trend of increase which suggests that not only are more patents being granted each year, but the complexity or detail of these patents, as indicated by the number of figures, is also rising. The data also reveals that the average number of figures per patent ranges between 7 and 9 indicating that patents are incorporating more viewpoints and detailed illustrations to enhance the clarity and understanding of a design. Figure 1(b) details the growth in the total number of figures associated with all patents and the number of drawing sheets used. This increase is consistent with the growing volume of patents. Figure 1(c) shows the number of words in each patent claim. Design patent has only one single line claim and it does not provide detailed information about the drawing. This necessitates generating richer information (e.g., an elaborated caption) for the design. Additional quantitative analyses are provided in Appendix A.1.

**Caption generation.** Design patent images typically lack detailed captions, which can be highly beneficial for various NLP and multimodal tasks . This dataset can contribute as a benchmark for the existing multi-modal tools as well as it can also help in patent analyses (see Sec. 4). To create such a dataset, we add descriptive captions of the images in our dataset. For the caption generation task, we use LLaVA (Language-Image Assisted Visual Analysis) , GPT-4o6, and Qwen-VL  to generate captions for 1000 samples from the data of 2022. We consider three factors for choosing caption models including runtime, cost, and quality of the captions. In terms of runtime, GPT4-o, LLaVA, and Qwen-VL required an average of 4.7, 3.98, and 3.25 seconds to generate each caption, respectively. For costs, GPT-4o is not free, whereas LLaVA and Qwen-VL are free of charge. Regarding caption quality, qualitative examples (See Fig. 8 in Appendix A.2) reveal that Qwen-VL often fails to provide functional descriptions and sometimes includes Chinese words in English captions. We also conduct zero-shot multimodal retrieval tasks to evaluate the quality of captions (See Table 1). GPT-4o performs best and LLaVA outperforms Qwen-VL. Considering all

Figure 2: Patent statistics (2007-2022): (a) Number of patents and average number of figures per patent, (b) Number of figures and drawing sheets. (c) Average number of words in a claim. These show a general increase in both the number of patents and figures.

the above factors including cost, we use LLaVA as our captioning model. This model is a robust multimodal system that combines a vision encoder with a large language model (LLM) to enable a comprehensive understanding of both visual and textual information.

For each patent, we experiment with different prompts to generate suitable captions. We observe that the patent title typically represents the name of the object, but lacks details about the shape or functionality of the design. As the final caption generation prompt, we use the following: _This is the image of [patent_title]. What is the shape of the image? What is the functionality of [patent_title]?_ Figure 3 illustrates examples of several design patents, along with their titles and our generated captions. The examples show that the generated captions reflect the provided instructions. We also utilize these captions and demonstrate that they are beneficial for patent related tasks (see Sec. 4).

**Potential bias and limitations of our Impact dataset.** First, Impact covers design patents filed between 2007 and 2022 within the US and documented in English. These temporal and geographical limitations exclude earlier design trends and patents from other regions. It might potentially impact the applicability of our findings across different temporal and global contexts. Second, the number of figures have a high variance across patents. On average, each patent consists of 7 to 9 figures, while some contain as few as 2-3 and others more than 20. This might become an issue if the downstream tasks depend on all figures (e.g., 3D image construction task) and it may not fully represent the variability in terms of the figures.

  
**Model** &  &  \\   & **R@5** & **R@10** & **R@5** & **R@10** \\  Qwen-VL & 6.6 & 10.8 & 9.4 & 13.3 \\  GPT-4o & 10.5 & 15.3 & 12.4 & 17.5 \\  LLaVA & 8.4 & 12.2 & 9.5 & 13.8 \\   

Table 1: Zero-shot multimodal retrieval results on captions generated using different models. GPT-4o performs the best and LLaVA outperforms Qwen-VL.

Figure 3: It shows examples of three different design objects, titles and the generated captions from our dataset. The generated captions contain shapes and functionalities of the objects.

## 4 Patent Analysis with Impact

Our Impact dataset--on design patents--facilitates multimodal analysis tasks such as classification, retrieval, and similarity assessment. We perform classification and retrieval on our Impact dataset and demonstrate the usefulness of elaborated captions. Additional results are provided in the Appendix.

### Classification

Patent reviewers usually are responsible to classify the patent applications, i.e., they assign the design codes. This is time-consuming due to the numerous classification codes. For instance, the U.S. design patent system has 33 classes which are further divided into subclasses. Given that design patents include both titles and visual content, the goal of the experiment is patent classification by integrating titles, captions, and images. Although a single design patent can be associated with multiple design codes, we focus on the primary classification--solving the task as a multi-class classification problem.

**Setup.** For this patent classification task, we use the recent patents from 2021 and obtain a total of 32,536 patents. We exclude the sparsely populated subclasses that appear only twice or less. Consequently, the final dataset has 25,500 patents with a total of 228084 images and they are categorized into 835 classification codes (subclasses).

We test five different combinations of input features: only title, only caption, title and image, caption and image, and a combination, i.e, concatenation of all three. All text-based features are extracted using RoBERTa , while image features are obtained via the image encoder of CLIP . Afterwards, we employ a range of classification algorithms which include Support Vector Machines (SVM), Multinomial Naive Bayes (MultiNB), Logistic Regression (LR), and Multilayer Perceptrons (MLP). The primary objective here is to assess the power of images and texts (e.g., elaborated caption and title) for patent classification. We also applied BERT on only text data (title and caption).

**Results.** Table 2 presents the classification average accuracy over all labels and F1 scores for all the combinations. BERT can be only applied on text-based features. It produces similar results with title and the generated caption. The best results for other individual classifiers are in bold. The results demonstrate that the best results--both accuracy and F1 scores-- are produced by the combined features with title, caption, and image. The combine features outperform the features only with caption and image quite significantly in many cases and up to 36% in terms of accuracy. This also shows the importance of the generated captions in patent classification.

### Multimodal Retrieval

The patent retrieval task is to identify relevant patent documents and images in response to search queries. This process is often used for discovering new patents and assessing their novelty. However, recent patent retrieval systems mainly work on retrieving images only from query images. Here, we focus on multimodal retrieval, which incorporates both text and images. This integration enhances the ability to cross-reference and verify information, thus improving the overall effectiveness and

  
**Metrics** & **BERT** & **SVM** & **MultiNB** & **LR** & **MLP** \\ 
**Acc. (Title)** & 0.55 & 0.55 & 0.38 & 0.54 & **0.52** \\
**Acc. (Caption)** & 0.52 & 0.46 & 0.38 & 0.45 & 0.41 \\
**Acc. (Title+Image)** & - & 0.58 & 0.44 & 0.57 & 0.51 \\
**Acc. (Caption+Image)** & - & 0.44 & 0.44 & 0.52 & 0.46 \\
**Acc. (Combined)** & - & **0.60** & **0.51** & **0.59** & **0.52** \\ 
**F1 Score (Title)** & 0.53 & 0.53 & 0.33 & 0.52 & 0.49 \\
**F1 Score (Caption)** & 0.50 & 0.37 & 0.33 & 0.44 & 0.39 \\
**F1 Score (Title+Image)** & - & 0.57 & 0.41 & 0.56 & 0.50 \\
**F1 Score (Caption+Image)** & - & 0.51 & 0.41 & 0.50 & 0.45 \\
**F1 Score (Combined)** & - & **0.58** & **0.49** & **0.57** & **0.50** \\   

Table 2: Patent classification using various machine learning models. Combined denotes the combination of the features from the title, caption, and image. The best accuracy and F1-scores are shown in bold for individual methods. The combined features produce the best results consistently.

[MISSING_PAGE_FAIL:8]

and _830%_ respectively at R@1 on T2I and I2T retrievals. Note that, ViT-L obtains the best recall in all settings, which demonstrates that the larger advanced models boost the performance. We also measure the multimodal retrieval performance on the entire dataset. Since Image-Caption pair give us the highest recall (See Table 3), we use this pair for the task shown in Table 3(a). Table 3(b) shows that CLIP, when fine-tuned on our dataset, outperforms other models on image retrieval tasks and achieves the highest mAP. These results indicate that more descriptive training data, such as detailed captions versus simpler titles, can effectively improve the models' performance. Moreover, finetuning allows the model to adjust its parameters better aligned with the patent domain. Thus, we believe that a scaled patent multimodal foundation model is beneficial for analyses of design patents.

### Human Evaluation on Generated captions

We conduct human evaluation to assess the generated captions from LLaVA. In the study, we have a total of 12 participants. All participants are graduate students in STEM with prior research experience. Each participant reviews three sets of titles, captions, and images. We ask the user to read these carefully and determine if the captions can add value to the image as a descriptive caption in their opinion with the following questions: _Did the caption describe the image correctly? Did the caption can describe the shape and functionality of the image logically?_ As a result, in more than 60% of their responses, they agree with the quality of the generated captions. However, one of the examples shows the limitations of general multimodal captioning models and we would consider this as a future direction. Figure 4 shows three examples of design patents, generated captions, and human evaluation results.

### Other Applications of Impact in Computer Vision

#### 4.4.1 3D image reconstruction

3D image reconstruction is the process of creating a three-dimensional model from one or more two-dimensional images. The ability to produce rapid and precise 3D image reconstructions has applications in numerous CV fields, including medical, robotics, entertainment, reverse engineering, augmented reality, human-computer interaction, and animation . Recent studies have focused on the reconstruction of three-dimensional images from two-dimensional images [49; 52; 38]. Most

Figure 4: Examples of three different design objects, titles, generated captions, and human evaluation results on the generated captions. Positive means the caption describes the image, shape and functionality correctly and logically. Negatives means the opposite.

existing research in the field of 3D image reconstruction from 2D images use the standard ImageNet dataset as a benchmark .

_Our Impact dataset._ The generic approach involves using an image generator to produce a tri-plane representation with image depths. Multiple viewpoints helps the process as it provides the information about different angles and accurate depth estimation. The advantage of Impact is that is consists of design sketches from many viewpoints for the same design and that can be used to generate 3D models. Thus, Impact can serve as a benchmark for 3D image construction and as a valuable source of design inspiration for future inventors. It can also help the reviewers to search the prior art to mitigate infringement as 3D constructions are more informative than a 2D model. We provide examples in Appendix A.6.

#### 4.4.2 Visual Question Answering

Visual Question Answering (VQA) asks to predict answer classes or generate a short phrase for the given images and textual questions . VQA enhances machine understanding of visual content and enables various practical applications across different industries, such as Medical VQA .

_Our Impact dataset._ We propose a new task PatentVQA, which offers significant advantages in understanding design patents. It can enhance interpretation by simplifying complex visual data and automates the analysis of visual patent information, thus it can help in speeding up the review process. First, we propose to construct and annotate an Impact-VQA dataset. Then, one can develop a VQA systems using Vision-Language models [35; 31], which makes patent databases more accessible to a broader audience and reduces the need for specialized training and helping users engage more deeply with patent content. We illustrate an example in Fig. 1 and additional examples are in Appendix A.7. Therefore, we believe that PatentVQA is a valuable tool in patent analysis and design innovation.

## 5 Conclusions

In this work, we have developed a large dataset on design patents, Impact that integrates visual and textual information within a unified framework. We have compiled a collection of 435,101 design patents spanning 16 years with 3.6 million drawing figures along with eleven fields. Impact is a multimodal patent dataset with patent images and detailed captions to support various computer vision and multimodal tasks. Impact is particularly valuable for advanced patent analysis tasks such as classification, retrieval, prior art searches, and design trend analysis. We demonstrate the usefulness of Impact in two specific tasks: classification and retrieval. The results demonstrate that the integration of additional textual information along with the existing images help in improving performance in several settings.

**Broader Impact.** Patent data--even for design patents--is publicly available, but there has been a need to organize this large amount of data for the recent deep learning methods and applications. We believe Impact will serve as an important benchmark to both patent and computer vision community. This work is about creating an organized dataset from publicly available data and we do not foresee any potential negative societal impact of it.

**Future Directions.** Besides the applications proposed earlier, another interesting future direction could be to develop efficient foundation models specifically for patent data. These can significantly enhance the accuracy and relevance of patent analyses, as generic vision-language models often underperform in this type of specialized domain.

Acknowledgments

We thank the reviewers for their valuable suggestions and 12 participants for human evaluations. This work was supported by in-kind contributions from University of Illinois Urbana-Champaign (UIUC) HACC Cluster and NSF ACCESS UIUC NCSA Cluster (Ref: ELE230014). Zhu Wang was supported by Sathya N. Ravi's UIC start-up funds.