# A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes

A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes

 Han Zhong

Center for Data Science

Peking University

hanzhong@stu.pku.edu.cn &Tong Zhang

Department of Mathematics

HKUST

tongzhang@tongzhang-ml.org

###### Abstract

The proximal policy optimization (PPO) algorithm stands as one of the most prosperous methods in the field of reinforcement learning (RL). Despite its success, the theoretical understanding of PPO remains deficient. Specifically, it is unclear whether PPO or its optimistic variants can effectively solve linear Markov decision processes (MDPs), which are arguably the simplest models in RL with function approximation. To bridge this gap, we propose an optimistic variant of PPO for episodic adversarial linear MDPs with full-information feedback, and establish a \(}(d^{3/4}H^{2}K^{3/4})\) regret for it. Here \(d\) is the ambient dimension of linear MDPs, \(H\) is the length of each episode, and \(K\) is the number of episodes. Compared with existing policy-based algorithms, we achieve the state-of-the-art regret bound in both stochastic linear MDPs and adversarial linear MDPs with full information. Additionally, our algorithm design features a novel multi-batched updating mechanism and the theoretical analysis utilizes a new covering number argument of value and policy classes, which might be of independent interest.

## 1 Introduction

Reinforcement learning (RL)  is a prominent approach to solving sequential decision making problems. Its tremendous successes [19; 32; 33; 5] can be attributed, in large part, to the advent of deep learning  and the development of powerful deep RL algorithms [24; 28; 29; 11]. Among these algorithms, the proximal policy optimization (PPO)  stands out as a particularly significant approach. Indeed, it continues to play a pivotal role in recent advancements in large language models .

Motivated by the remarkable empirical success of PPO, numerous studies seek to provide theoretical justification for its effectiveness. In particular, Cai et al.  develop an optimistic variant of the PPO algorithm in adversarial linear mixture MDPs with full-information feedback [4; 25], where the transition kernel is a linear combination of several base models. Theoretically, they show that the optimistic variant of PPO is capable of tackling problems with large state spaces by establishing a sublinear regret that is independent of the size of the state space. Building upon this work, He et al.  study the same setting and refine the regret bound derived by Cai et al.  using the weighted regression technique . However, the algorithms in Cai et al. , He et al.  and other algorithms for linear mixture MDPs [4; 44] are implemented in a model-based manner and require an integration of the individual base model, which can be computationally expensive or even intractable in general. Another arguably simplest RL model involving function approximation is linear MDP [38; 17], which assumes that the reward functions and transition kernel enjoy a low-rank representation. For this model, several works [17; 15; 3; 13] propose value-based algorithms that directly approximate the value function and provide regret guarantees. To demonstrate the efficiency of PPO in linear MDPs from a theoretical perspective, one potential approach is to extend the resultsof Cai et al. , He et al.  to linear MDPs. However, this extension poses significant challenges due to certain technical issues that are unique to linear MDPs. See SS1.1 for a detailed description.

In this paper, we address this technical challenge and prove that the optimistic variant of PPO is provably efficient for stochastic linear MDPs and even adversarial linear MDPs with full-information feedback. Our contributions are summarized below.

* In terms of algorithm design, we propose a new algorithm OPPO+ (Algorithm 1), an optimistic variant of PPO, for adversarial linear MDPs with full-information feedback. Our algorithm features two novel algorithm designs including a _multi-batched updating mechanism_ and a _policy evaluation step via average rewards_.
* Theoretically, we establish a \(}(d^{3/4}H^{2}K^{3/4})\) regret for OPPO+, where \(d\) is the ambient dimension of linear MDPs, \(H\) is the horizon, and \(K\) is the number of episodes. To achieve this result, we employ two new techniques. Firstly, we adopt a novel covering number argument for the value and policy classes, as explicated in SSC. Secondly, in Lemma 4.3, we meticulously analyze the drift between adjacent policies to control the error arising from the policy evaluation step using average rewards.
* Compared with existing policy optimization algorithms, our algorithm achieves a better regret guarantee for both stochastic linear MDPs and adversarial linear MDPs with full-information feedback (to our best knowledge). See Table 1 for a detailed comparison.

In summary, our work provides a new theoretical justification for PPO in linear MDPs. To illustrate our theory more, we highlight the challenges and our novelties in SS1.1.

### Challenges and Our Novelties

**Challenge 1: Covering Number of Value Function Class.** In the analysis of linear MDPs (see Lemma B.3 or Lemma B.3 in Jin et al. ), we need to calculate the covering number of \(_{h}^{k}\), which is the function class of the estimated value function at the \(h\)-th step of the \(k\)-th episode and takes the following form:

\[_{h}^{k}=\{V()= Q_{h}^{k}(,),_{h}^{k}( )_{}\},\]

where \(Q_{h}^{k}\) and \(_{h}^{k}\) are estimated Q-function and policy at the \(h\)-th step of the \(k\)-th episode, respectively. For value-based algorithms (e.g., LSVI-UCB in Jin et al. ), \(_{h}^{k}\) is the greedy policy with respect to \(Q_{h}^{k}\). Then we have

\[_{h}^{k}=\{V()=_{a}Q_{h}^{k}(,a)\}.\]

Since \(_{a}\) is a contraction map, it suffices to calculate \((_{h}^{k})\), where \(_{h}^{k}\) is the function class of \(Q_{h}^{k}\) and \(()\) denotes the covering number. By a standard covering number argument (Lemma D.4 or Lemma D.6 in Jin et al. ), we can show that \((_{h}^{k})}(d^{2})\). However, for policy-based algorithms such as PPO, \(_{h}^{k}\) is a stochastic policy, which makes the log-covering number \((_{h}^{k})\) may have a polynomial dependency on the size of action space \(\) (log-covering number

   & Sto. + Bandit & Adv. + Full-infor. & Adv. + Bandit & Regret \\ 
 & ✓ & ✗ & ✗ & \(}(d^{3/4}H^{13/4}K^{3/4})\) \\ 
 & ✓ & ✓ & ✓ & \(}(d^{2/3}A^{1/9}H^{20/9}K^{8/9})\) \\ 
 & ✓ & ✓ & ✓ & \(}(dH^{2}K^{6/7})\) \\  Our Work & ✓ & ✓ & ✗ & \(}(d^{3/4}H^{2}K^{3/4})\) \\  

Table 1: A comparison with closely related works on policy optimization for linear MDPs. Here “Sto.” and “Adv.” represent stochastic rewards and adversarial rewards, respectively. Additionally, “Bandit” and “Full-infor.” signify bandit feedback and full-information feedback. We remark that Zanette et al.  do not consider the regret minimization problem and the regret reported in the table is implied by their sample complexity. We will compare the sample complexity provided in Zanette et al.  and the complexity implied by our regret in Remark 3.2.

of \(||\)-dimensional probability distributions is at the order of \(||\)). We also remark that linear mixture MDPs are more amenable to theoretical analysis compared to linear MDPs, as they do not necessitate the calculation of the covering number of \(_{h}^{k}\). As a result, the proof presented by Cai et al.  is valid for linear mixture MDPs, but cannot be extended to linear MDPs. See SSA for more elaboration of technical challenges.

**Novelty 1: Multi-batched Updating and A New Covering Number Argument.** Our key observation is that if we improve the policy like PPO (see (3.2) or Schulman et al. , Cai et al. ), \(_{h}^{k}\) admits a softmax form, i.e.,

\[_{h}^{k}()_{i=1}^{l}Q_{h}^{k_{i}}( ,).\]

Here \(\{k_{i}\}_{i=1}^{l}\) is a sequence of episodes, where \(k_{i} k\) for all \(i[l]\), denoting the episodes in which our algorithm performs policy optimization prior to the \(k\)-th episode. By a technical lemma (Lemma C.3), we can show that

\[(_{h}^{k})_{i=1}^{l} (_{h}^{k_{i}})}(l d^ {2}).\]

If we perform the policy optimization in each episode like Cai et al. , Shani et al. , \(l\) may linear in \(K\) and the final regret bound becomes vacuous. Motivated by this, we use a multi-batched updating scheme. In specific, OPPO+ divides the whole learning process into several batches and only updates policies at the beginning of each batch. See Figure 1 for visualization. For example, if the number of batches is \(K^{1/2}\) (i.e., each batch consists of consecutive \(K^{1/2}\) episodes), we have \(l K^{1/2}\) and the final regret is at the order of \(}(K^{3/4})\). Here we assume \(K^{1/2}\) is a positive integer for simplicity. See SSC for details.

**Challenge 2: Adversarial Rewards.** Compared with previous value-based algorithms [e.g., 17], one superiority of optimistic PPO is that they can learn adversarial MDPs with full-information feedback . In Cai et al. , Shani et al. , the policy evaluation step is that

\[Q_{h}^{k}=r_{h}^{k}+}_{h}^{k}V_{h+1}^{k}+, h[H], V_{h+1}^{k}=0,\]

where \(r_{h}^{k}\) is the adversarial reward function and \(}_{h}^{k}V_{h+1}^{k}\) is the estimator of the expected next step value of \(V_{h+1}^{k}\). This policy evaluation step is invalid if we use the multi-batched updating. Consider the following case, if the number of batches is \(K^{1/2}\) and

\[r_{h}^{k}(,)=\{0&k\{iK^{1/2}+1\}_{i=0}^{ K^{1/2}}\\ &..\]

Then the algorithm only uses zero rewards to find the optimal policy in hindsight with respect to arbitrary adversarial rewards, which is obviously impossible.

Figure 1: Algorithm diagram. The entire \(K\) episodes are partitioned into \(L=K/B\) batches, with each batch containing \(B\) consecutive episodes. The policy/value updating only occurs at the beginning of each batch. The policy evaluation uses the reward function in the last batch.

**Novelty 2: Policy Evaluation via Average Reward and Smoothness Analysis.** To tackle the above challenge, we adopt the following policy evaluation step at the beginning of each batch

\[Q_{h}^{k}=_{h}^{k}+}_{h}^{k}V_{h+1}^{k}+, h[H], V_{h+1}^{k}=0,\]

where \(_{h}^{k}\) the average reward of the last batch:

\[_{h}^{k}=}{}.\]

Let \(^{k_{i}}\) denote the policy executed in the \(i\)-th batch. Intuitively, \(_{k_{i+1}}\) is the desired policy within \(i\)-th batch since its calculation only uses the rewards in the first \((i-1)\) batches (cf. Figure 1). Hence, compared with Cai et al. , Shani et al. , we need to handle the gap between the performance of \(^{k_{i+1}}\) and \(^{k_{i}}\) in the \(i\)-th batch. Fortunately, this error can be controlled due to the "smoothness" of policies in adjacent batches. See Lemma 4.3 for details.

### Related Works

Policy Optimization Algorithms.The seminal work of Schulman et al.  proposes the PPO algorithm, and a line of following works seeks to provide theoretical guarantees for it. In particular, Cai et al.  proposes the optimistic PPO (OPPO) algorithm for adversarial linear mixture MDPs and establishes regret \(}(dK})\) for it. Then, He et al.  improve the regret to \(}(dK})\) by the weighted regression technique . Besides their works on linear mixture MDPs, Shani et al. , Wu et al.  provide fine-grained analysis of optimistic variants of PPO in the tabular case. The works of Fei et al. , Zhong et al.  show that optimistic variants of PPO can solve non-stationary MDPs. However, none of these works show that the optimistic variant of PPO is provably efficient for linear MDPs.

There is another line of works  proposes optimistic policy optimization algorithms based on the natural policy gradient (NPG) algorithm  and the policy-cover technique. But their works are limited to the stochastic linear MDPs, while our work can tackle adversarial rewards. Compared with their results for stochastic linear MDPs, our work can achieve a better regret and compatible sample complexity. See Table 1 and Remark 3.2 for a detailed comparison. Several recent works  study the more challenging problem of learning adversarial linear MDPs with only bandit feedback, which is beyond the scope of our work. Without access to exploratory policies or even known transitions, their regret is at least \(}(K^{6/7})\), while our work achieves a better \(}(K^{3/4})\) regret with the full-information feedback assumption.

RL with Linear Function Approximation.Our work is related to previous works proposing value-based algorithms for linear MDPs . The work of Yang and Wang  develops the first sample efficient algorithm for linear MDPs with a generative model. Then Jin et al.  proposes the first provably efficient algorithms for linear MDPs in the online setting. The results of Jin et al.  are later improved by . In particular, Agarwal et al. , He et al.  show that the nearly minimax optimal regret \(}(dK})\) is achievable in stochastic linear MDPs. Compared with these value-based algorithms, our work can tackle the more challenging adversarial linear MDPs with full-information feedback.

There is another line of works  studying linear mixture MDPs, which is another model of RL with linear function approximation. It can be shown that linear MDPs and linear mixture MDPs are incompatible in the sense that neither model is a special case of the other. Among these works, Zhou et al. , Zhou and Gu  establishes nearly minimax regret \(}(dK})\) for stochastic linear mixture MDPs. Our work is more related to Cai et al. , He et al.  on adversarial linear mixture MDPs with full-information feedback. We have remarked that it is nontrivial to extend their results to linear MDPs.

## 2 Preliminaries

**Notations.** We use \(^{+}\) to denote the set of positive integers. For any \(H^{+}\), we denote \([H]=\{1,2,,H\}\). For any \(H^{+}\) and \(x\), we use the notation\(\{H,\{0,x\}\}\). Besides, we denote by \(()\) the set of probability distributions on the set \(\). For any two distributions \(P\) and \(Q\) over \(\), we denote \((P\|Q)=_{ P}[P(a)/Q(a)]\).

**Episodic Adversarial MDPs.** We consider an episodic MDP \(\), which is denoted by a tuple

\[(,,H,K,\{r^{k}_{h}\}_{(k,h)[K][H]}, =\{_{h}\}_{h[H]}),\]

where \(\) is the state space, \(\) is the action space, \(H\) is the length of each episode, \(K\) is the number of episodes, \(r^{k}_{h}:\) is the deterministic1 reward function at the \(h\)-th step of \(k\)-th episode, \(_{h}\) is the transition kernel with \(_{h}(s^{} s,a)\) being the transition probability for state \(s\) to transfer to the next state \(s^{}\) given action \(a\) at the \(h\)-th step. We consider the _adversarial_ MDPs with _full-information feedback_, which means that the reward \(\{r^{k}_{h}\}_{h[H]}\) is adversarially chosen by the environment at the beginning of the \(k\)-th episode and revealed to the learner after the \(k\)-th episode.

A policy \(=\{_{h}\}_{h[H]}\) is a collection of \(H\) functions, where \(_{h}:()\) is a function that maps a state to a distribution over action space at step \(h\). For any policy \(\) and reward function \(\{r^{k}_{h}\}_{h[H]}\), we define the value function \(V^{,k}_{h}:\) and Q-function \(Q^{,k}_{h}:\) as

\[V^{,k}_{h}(x) =_{}_{h^{}=h}^{H}r^{k}_{h^{} }(x_{h^{}},a_{h^{}})\,\,x_{h}=x,\] \[Q^{,k}_{h}(x,a) =_{}_{h^{}=h}^{H}r^{k}_{h^{ }}(x_{h^{}},a_{h^{}})\,\,x_{h}=x,a_{h}=a,\]

for any \((x,a,k,h)[K][H]\). Here the expectation \(_{}[]\) is taken with respect to the randomness of the trajectory induced by policy \(\) and transition kernel \(\). It is well-known that the value function and Q-function satisfy the following Bellman equation for any \((x,a)\),

\[V^{,k}_{h}(x)= Q^{,k}_{h}(x,),_{h}( x)_{ }, Q^{,k}_{h}(x,a)=r^{k}_{h}(x,a)+(_{h}V^{,k} _{h+1})(x,a), \]

where \((,)_{}\) denotes the inner product over the action space \(\) and we will omit the subscript when it is clear from the context. Here \(_{h}\) is the operator defined as

\[(_{h}V)(x,a)=_{x^{}_{h}( x, a)}[V(x^{})], V:. \]

**Interaction Process and Learning Objective.** We consider the online setting, where the learner improves her performance by interacting with the environment repeatedly. The learning process consists of \(K\) episodes and each episode starts from a fixed initial state \(x_{1}\)2. At the beginning of the \(k\)-th episode, the environment adversarially chooses reward functions \(\{r^{k}_{h}\}_{h[H]}\), which can depend on previous \((k-1)\) trajectories. Then the agent determines a policy \(^{k}\) and receives the initial state \(x^{k}_{1}=x_{1}\). At each step \(h[H]\), the agent receives the state \(x^{k}_{h}\), chooses an action \(a^{k}_{h}^{k}_{h}( x^{k}_{h})\), receives the reward function \(r^{k}_{h}\), and transits to the next state \(x^{k}_{h+1}\). The \(k\)-th episode ends after \(H\) steps.

We evaluate the performance of an online algorithm by the notion of _regret_, which is defined as the value difference between the executed policies and the optimal policy in hindsight:

\[(K)=_{}_{k=1}^{K}V^{,k}_{1}(x_{1})-V^{ ^{k},k}_{1}(x_{1}).\]

For simplicity, we denote the optimal policy in hindsight by \(^{*}\), i.e., \(^{*}=_{}_{k=1}^{K}V^{,k}_{1}(x_{1})\).

**Linear MDPs.** We focus on the linear MDPs , where the transition kernels are linear in a known feature map.

**Definition 2.1** (Linear MDP).: We say an MDP \((,,H,K,\{r^{k}_{h}\}_{(k,h)[K][H]},= \{_{h}\}_{h[H]})\) is a linear MDP if there exists a known feature \(:^{d}\) such that for any \((x,a,k,h)[K][H]\), we have

\[_{h}(x^{} x,a)=(x,a)^{} we have access to the full-information feedback, we do not assume the reward functions are linear in the feature map \(\) like Jin et al. . We also remark that the adversarial linear MDP with full-information feedback is a more challenging problem than the stochastic linear MDP with bandit feedback studied in Jin et al. . In fact, for stochastic linear MDPs, we can assume the reward functions are known without loss of generality since learning the linear transition kernel is more difficult than the linear reward.

## 3 Algorithm

```
0: Batch size \(B^{+}\), regularization parameter \(>0\), and confidence radius \(>0\).
1: Initialize \(\{Q_{h}^{0}\}\), \(\{r_{h}^{k}\}_{-B k 0}\) as zero functions and \(\{_{h}^{0}\}\) as uniform distributions on \(\), \( h[H]\).
2: Let \(L=K/B\), \(i=1\), and \(k_{i}=(i-1) B+1\) for \(1 i[L]\).
3:for episode \(k=1,2,,K\)do
4: Receive the initial state \(x_{1}^{k}\).
5:if\(k=k_{i}\)then
6:\(V_{H+1}^{k}() 0\).
7:for step \(h=1,2,,H\)do
8: Update the policy by \(_{h}^{k}()_{h}^{k-1}()\{  Q_{h}^{k-1}(,)\}\).
9:endfor
10:for step \(h=H,H-1,,1\)do
11:\(_{h}^{k}_{r=1}^{k-1}(x_{1}^{r},a_{1}^{r})(x_{1}^ {r},a_{n}^{r})^{}+ I_{d}\).
12:\(w_{h}^{k}(_{h}^{k-1})^{}_{=1}^{k-1}(x_{h}^{r},a_{n}^{r}) V_{h+1}^{k}(x_{h+1}^{})\).
13:\(_{h}^{k}(,)(_{j=k_{i-1}}^{k_{i-1}}r_{h}^{j}( ,))/B\).
14:\(_{h}^{k}(,)[(,)^{}( _{h}^{k-1})^{-1}(,)]^{1/2}\).
15:\(}_{h}^{k}V_{h+1}^{k}(,)\{(,)^{}w_{h}^{k}+_{h}^{k}(,),H-h\}^{+}\).
16:\(Q_{h}^{k}(,)_{h}^{k}(,)}_{h}^{k}V_{h+1}^{k}(,)\).
17:\(V_{h}^{k}() Q_{h}^{k}(,),_{h}^{k}( )_{}\).
18:endfor
19:\(i i+1\)
20:else
21:\(Q_{h}^{k} Q_{h}^{k-1}\), \(V_{h}^{k} V_{h}^{k-1}\), \(_{h}^{k}_{h}^{k-1}\), \(_{h}^{k}_{h}^{k-1}\)\( h[H]\).
22:endif
23:for\(h=1,2,,H\)do
24: Take the action following \(a_{h}^{k}_{h}^{k}( x_{h}^{k})\).
25: Observe the reward function \(r_{h}^{k}(,)\) and receive the next state \(x_{h+1}^{k}\).
26:endfor
27:endfor
```

**Algorithm 1** OPPO+

## 4 Algorithm

In this section, we propose a new algorithm OPPO+ (Algorithm 1) to solve adversarial linear MDPs with full-information feedback. In what follows, we highlight the key steps of OPPO+.

**Multi-batched Updating.** Due to the technical issue elaborated in SS1.1, we adopt the multi-batched updating rule. In specific, OPPO+ divides the total \(K\) episodes into \(L=K/B\) batches and each batch consists of \(B\) consecutive episodes. Here we assume \(K/B\) is a positive integer without loss of generality3. For ease of presentation, we use \(k_{i}=(i-1) B+1\) to denote the first episode in the \(i\)-th batch. When the \(k\)-th episode is the beginning of a batch (i.e, \(k=k_{i}\) for some \(i[L]\)), OPPO+ performs the following _policy improvement step_ and _policy evaluation step_.

**Policy Improvement.** In the policy improvement step of the \(k\)-th episode (\(k=k_{i}\) for some \(i[L]\)), OPPO+ calculates \(^{k}\) based on the previous policy \(^{k-1}\) using PPO . In specific, OPPO+ updates\(^{k}\) by solving the following proximal policy optimization problem:

\[^{k}*{argmax}_{}L_{k-1}()- ^{-1}_{^{k-1}}_{h=1}^{H} _{h}( x_{h})\|_{h}^{k-1}( x_{h})}, \]

where \(>0\) is the stepsize that will be specified in Theorem 3.1, and \(L_{k-1}()\) takes form

\[L_{k-1}()=V_{1}^{^{k-1},k-1}(x_{1}^{k})+_{^{k- 1}}_{h=1}^{H} Q_{h}^{k-1}(x_{h},),_{h}( x_ {h})-_{h}^{k-1}( x_{h}),\]

which is proportional to the local linear function of \(V_{1}^{,k-1}(x_{1}^{k})\) at \(^{k-1}\) and replaces the unknown Q-function \(Q_{h}^{^{k-1},k-1}\) by the estimated one \(Q_{h}^{k-1}\) for any \(h[H]\). It is not difficult to show that the updated policy \(^{k}\) obtained in (3.1) admits the following closed form:

\[_{h}^{k}( x)_{h}^{k-1}( x)  Q_{h}^{k-1}(x,),(x,h) [H]. \]

**Policy Evaluation.** In the policy evaluation step of the \(k\)-th episode (\(k=k_{i}\) for some \(i[K]\)), \(\) lets \(V_{H+1}^{k}\) be the zero function and iteratively calculates the estimated Q-function \(\{Q_{h}^{k}\}_{h[H]}\) in the order of \(h=H,H-1,,1\). Now we present the policy evaluation at the \(h\)-th step given estimated value \(V_{h+1}^{k}\). By the definitions of linear MDP in Definition 2.1 and the operator \(_{h}\) in (2.2), we know \(_{h}V_{h+1}^{k}\) is linear in the feature map \(\). Inspired by this, we estimate its linear coefficient by solving the following ridge regression:

\[w_{h}^{k}=*{argmin}_{w^{d}}_{ =1}^{k-1}(x_{h}^{},a_{h}^{})^{}w-V_{h+1}^{k}(x_{h+1}^{ })^{2}+ I_{d}, \]

where \(>0\) is the regularization parameter and \(I_{d}\) is the identify matrix. By solving (3.3), we have

\[w_{h}^{k}=(_{h}^{k})^{-1}_{=1}^{k-1}( x_{h}^{},a_{h}^{}) V_{h+1}^{k}(x_{h+1}^{}), _{h}^{k}=_{=1}^{k-1}(x_{h}^{},a_{h}^{} )(x_{h}^{},a_{h}^{})^{}+ I_{d}.\]

Based on this linear coefficient, we construct the estimator \(}_{h}^{k}V_{h+1}^{k}\) as

\[(}_{h}^{k}V_{h+1}^{k})(,)=\{ (,)^{}w_{h}^{k}+_{h}^{k}(,),H-h\}^{+},\]

where \(_{h}^{k}=((,)^{}(_{h}^{k})^{-1}( ,))^{1/2}\) is the bonus function and \(>0\) is a parameter that will be specified in Theorem 3.1. This form of bonus function also appears in the literature on linear bandits  and linear MDPs . Finally, we update \(Q_{h}^{k}\) and \(V_{h}^{k}\) by

\[Q_{h}^{k}(,)=_{h}^{k}(,)+(}_{h}^{k}V_{h+1}^{k})(,), V_{h}^{k}()= Q _{h}^{k}(,),_{h}^{k}()_{}. \]

Here \(_{h}^{k}\) is the average reward function in the last batch, that is

\[_{h}^{k}(,)=}^{k_{i-1}}r_{ h}^{j}(,)}{B}, \]

where \(k=k_{i}=(i-1) B+1\) and \(k_{i-1}=(i-2) B+1\).

Here we would like to make some comparisons between our algorithm \(\) and other related algorithms. Different from previous value-based algorithms that take simply take the greedy policy with respect to the estimated Q-functions , \(\) involves a policy improvement step like PPO . This step is key to tackling adversarial rewards. The most related algorithm is OPPO proposed by Cai et al. , which performs the policy optimization in linear mixture MDPs. The main difference between \(\) and \(\) is that we introduce a multi-batched updating and an average reward policy evaluation, which are important for solving linear MDPs (cf. SS1.1). Finally, we remark that the multi-batched updating scheme is adopted by previous work on bandits  and RL . But their algorithms are value-based and cannot tackle adversarial rewards.

**Theorem 3.1** (Regret).: Fix \((0,1]\) and \(K d^{3}\). Let \(B=K}\) and \(=|/(KH^{2})}\), \(=1\), \(=(d^{1/4}HK^{1/4}^{1/2})\) with \(=(dHK||/)\), then with probability at least \(1-\), the regret of Algorithm 1 satisfies

\[(K)(d^{3/4}H^{2}K^{3/4}|| )+(d^{5/2}H^{2}K^{1/2}).\]Proof.: By See SS4 for a detailed proof. 

To illustrate our theory more, we make several remarks as follows.

**Remark 3.2** (Sample Complexity).: Since learning adversarial linear MDPs with full-information feedback is more challenging than learning stochastic linear MDPs with bandit feedback, the result in Theorem 3.1 also holds for stochastic linear MDPs. By the standard online-to-batch argument , we have that Algorithm 1 can find an \(\)-optimal policy using at most

\[}H^{8}}{^{4}}+H^{4 }}{^{2}}\]

samples. Compared with the sample complexity \(}(d^{3}H^{13}/^{3})\) in Zanette et al. , we have a better dependency on \(H\) but a worse dependency on \(\). Moreover, by the standard sample complexity to regret argument , their sample complexity only gives a \(}(d^{3/4}H^{13/4}K^{3/4})\), which is worse than our regret in Theorem 3.1. More importantly, the algorithm in Zanette et al.  lacks the ability to handle adversarial rewards, whereas our proposed algorithm overcomes this limitation.

**Remark 3.3** (Optimality of Results).: For stochastic linear MDPs, Agarwal et al. , He et al.  design value-based algorithms with \(}(dK})\) regret, which matches the lower bound \((dK})\) up to logarithmic factors. It remains unclear whether policy-based based algorithms can achieve the nearly minimax optimal regret and we leave this as future work. For the more challenging adversarial linear MDPs with full-information feedback, we achieve the state-of-the-art regret bound. In this setup, a direct lower bound is \((dK})\), and we conjecture this lower bound is tight. It would be interesting to design algorithms with \(\) regret or even optimal regret in this setting.

**Remark 3.4** (Beyond the Linear Function Approximation).: The work of Agarwal et al.  extends their results to the kernel function approximation setting. We conjecture that our results can also be extended to RL with kernel and neural function approximation by the techniques in Yang et al. .

## 4 Proof of Theorem 3.1

Proof.: Recall that \(B^{+}\) is the batch size, \(L=K/B\) is the number of batches, and \(k_{i}=(i-1) B+1\) for any \(i[L]\). For any \(k[K]\), we use \(t_{k}\) to denote the \(k_{i}\) satisfying \(k_{i} k<k_{i+1}\). Moreover, we define the Bellman error as

\[_{h}^{k}=r_{h}^{k}+_{h}V_{h+1}^{k}-Q_{h}^{k},(k,h) [K][H]. \]

Here \(V_{h+1}^{k}\) and \(Q_{h}^{k}\) are the estimated value function and Q-function defined in (3.4), and \(_{h}\) is the operator defined in (2.2). Intuitively, (4.1) quantifies the violation of the Bellman equation in (2.1). With these notations, we have the following regret decomposition lemma.

**Lemma 4.1** (Regret Decomposition).: It holds that

\[(K) =_{k=1}^{K}V_{1}^{^{*},k}(x_{1}^{k})-V_{1}^{^{k},k}(x_{1}^{k})\] \[=^{K}_{h=1}^{H}_{^{*}}  Q_{h}^{k}(x_{h},),_{h}^{*}(\,|\,x_{h})-_{h}^{k} (\,|\,x_{h})}_{}\] \[+^{K}_{h=1}^{H} _{^{*}}[_{h}^{k}(x_{h},a_{h})]-_{^{k}}[_{h}^{k}(x_ {h},a_{h})]}_{}.\]

Proof.: This lemma is similar to the regret decomposition lemma in previous works  on policy optimization. See SSB.1 for a detailed proof.

Lemma 4.1 shows that the total regret consists of the _policy optimization error_ and the _statistical error_ related to the Bellman error defined in (4.1). Notably, different from previous works  that optimize policy in each episode, our algorithm performs policy optimization infrequently. Despite this, we can bound the policy optimization error in Lemma 4.1 by the following lemma.

**Lemma 4.2**.: It holds that

\[_{k=1}^{K}_{h=1}^{H}_{^{*}}Q_{ h}^{k}(x_{h},),_{h}^{*}(\,|\,x_{h})-_{h}^{k}(\,|\,x_{h}) K||}.\]

Proof.: See SSB.2 for a detailed proof. 

For the statistical error in Lemma 4.1, by the definitions of the policy evaluation step in (3.4) and Bellman error in (4.1), we have

\[_{h}^{k}=^{k}-_{h}^{k}}_{}+_{h}V_{h+1}^{k}-}_{h}^{k}V_{h+1}^{k}}_{ }. \]

The following lemma establishes the upper bound of the cumulative reward mismatch error

\[_{k=1}^{K}_{h=1}^{H}_{^{*}}[(r_{h}^{k}-_{h }^{k})(x_{h},a_{h})]-_{^{k}}[(r_{h}^{k}-_{h}^{k})(x_{h},a_ {h})], \]

and thus relates the statistical error in Lemma 4.1 to the transition estimation error in (4.2).

**Lemma 4.3**.: It holds with probability at least \(1-/2\) that

\[_{k=1}^{K}_{h=1}^{H}_{^{*}}[_{h }^{k}(x_{h},a_{h})]-_{^{k}}[_{h}^{k}(x_{h},a_{h})]\] \[_{k=1}^{K}_{h=1}^{H}_{^{*}}[( _{h}V_{h+1}^{t_{k}}-}_{h}^{t_{k}}V_{h+1}^{t_{k}} )(x_{h},a_{h})]+_{k=1}^{K}_{h=1}^{H}(}_{h}^{t_{k}}V_ {h+1}^{t_{k}}-_{h}V_{h+1}^{t_{k}})(x_{h}^{k},a_{h}^{k})\] \[+BH+ H^{3}K+K}.\]

Proof.: By calculation, we can show that the cumulative reward mismatch error in (4.3) is bounded by

\[_{i=1}^{L-1}_{k=k_{i}}^{k_{i+1}-1}V_{1}^{^{k_{i+1}},k}(x_{1} )-V_{1}^{^{k_{i}},k}(x_{1})+,\]

which represents the smoothness of adjacent policies. Our smoothness analysis leverages the value difference lemma (Lemma B.1 or SSB.1 in Cai et al. ) and the closed form of the policy improvement in (3.2). See SSB.3 for a detailed proof. 

Then we introduce the following lemma, which shows that the transition estimation error can be controlled by the bonus function.

**Lemma 4.4**.: It holds with probability at least \(1-/2\) that

\[-2\{H,_{h}^{t_{k}}(x,a)\}(_{h}V_{h+1}^{t_{ k}}-}_{h}^{t_{k}}V_{h+1}^{t_{k}})(x,a) 0\]

for all \((k,h,x,a)[K][H]\).

Proof.: The proof involves the standard analysis of self-normalized process  and a uniform concentration of the function class of \(V_{h+1}^{k}\). As elaborated in SS1.1, calculating the covering number of this function class is challenging and requires some new techniques. See SSB.4 for a detailed proof.

Combining Lemmas 4.3 and 4.4, we have

\[_{k=1}^{K}_{h=1}^{H}_{^{*}}[_{h}^{ k}(x_{h},a_{h})]-_{^{k}}[_{h}^{k}(x_{h},a_{h})]\] \[ 2_{k=1}^{K}_{h=1}^{H}\{H,_{h}^{t_{k}}( x_{h}^{k},a_{h}^{k})\}+BH+ H^{3}K+K}.\]

Hence, it remains to bound the term \(_{k=1}^{K}_{h=1}^{H}\{H,_{h}^{t_{k}}(x_{h}^{k},a_{h}^{k})\}\), which is the purpose of the following lemma.

**Lemma 4.5**.: It holds that

\[_{k=1}^{K}_{h=1}^{H}\{H,_{h}^{t_{k}}(x_{h}^{k},a_{h}^{k})\} (d^{3/4}H^{2}K^{3/4})+(d^{5/2}H^{2}K^{1/2 }).\]

Proof.: For all \(k[K]\), let \(_{h}^{k}=(^{}(_{h}^{k})^{-1})^{1/2}\) with \(_{h}^{k}=_{=1}^{k-1}(x_{h}^{},a_{h}^{})(x_{h}^ {},a_{h}^{})^{}+ I_{d}\). By a doubling trick, we can prove that

\[_{k=1}^{K}_{h=1}^{H}\{H,_{h}^{t_{k}}(x_{h}^{k},a_{h}^{k})\} _{k=1}^{K}_{h=1}^{H}\{H,_{h}^{k}(x_{h}^{k},a_{h}^{k} )\}+,\]

which can be further bounded by elliptical potential lemma (Lemma D.5 or Lemma 11 in Abbasi-Yadkori et al. ). See SSB.5 for a detailed proof. 

Finally, putting Lemmas 4.1-4.5 together, we conclude the proof of Theorem 3.1. 

Note the choice of the number of batches \(L\) determines the batch size \(B=K/L\). We make the following remark to illustrate how the choice of \(L\) affects the final regret, which indicates that our choices of parameters are optimal based on our current analysis.

**Remark 4.6** (Choices of Parameters and Final Regret).: If OPPO+ performs the policy optimization for \(L\) times, then by Lemma C.3, we have the complexity (logarithmic covering number) of policy class is roughly \(}(L)\) (ignoring the dependency of \(d\) and \(H\)). Furthermore, by the new self-normalized analysis in SSC, we have \(=}()\), which further implies that the model estimation error is bounded by \(_{k=1}^{K}_{h=1}^{H}^{k},a_{h}^{k})( _{h}^{k})^{-1}(x_{h}^{k},a_{h}^{k})}}()\) (cf. Lemma 4.5). Meanwhile, by Lemma 4.2, we have the policy optimization error is bounded by \(}(K/)=}(K^{3/4})\). By choosing \(L=()\), we obtain a regret of order \(}(K^{3/4})\).

## 5 Conclusion

In this paper, we advance the theoretical study of PPO in stochastic linear MDPs and even adversarial linear MDPs with full-information feedback. We propose a novel algorithm, namely OPPO+, which exhibits a state-of-the-art regret bound as compared to the prior policy optimization algorithms. Our work paves the way for future research in multiple directions. For instance, a significant open research question is to investigate whether policy-based algorithms can achieve the minimax regret in stochastic linear MDPs like previous value-based algorithms . Additionally, an interesting direction is to derive \(\)-regret bounds for adversarial linear MDPs with full-information or even bandit feedback.