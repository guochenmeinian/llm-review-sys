# MoE Jetpack: From Dense Checkpoints to

Adaptive Mixture of Experts for Vision Tasks

 Xingkui Zhu\({}^{*}\) Yiran Guan\({}^{*}\) Dingkang Liang Yuchao Chen

**Yuliang Liu\({}^{}\) Xiang Bai**

Huazhong University of Science and Technology

{adlith, yiranguan, dkliang, ylliu, xbai}@hust.edu.cn

Equal contribution. \(\) Corresponding author.

###### Abstract

The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency. However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption. To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1) _checkpoint recycling_, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy, minimizing the need for extensive pre-training; (2) the _hyper-spherical adaptive MoE (SpheroMoE) layer_, which optimizes the MoE architecture to enhance fine-tuning performance and efficiency. Experimental results indicate that MoE Jetpack doubles the convergence speed and enhances accuracy by \(2.8\%\) on ImageNet-1K. On smaller datasets, it achieves up to 8-fold faster convergence and over \(30\%\) accuracy gains, highlighting its efficiency. The code is available at https://github.com/Adlith/MoE-Jetpack.

## 1 Introduction

Increasing model scale is a key factor in boosting deep learning performance . However, as models expand in size, their computational demands surge, resulting in considerable slowdowns during both training and inference phases. A promising approach that decouples model size from computational costs is the _sparsely activated mixture of experts_ (MoE) . Unlike _densely activated models_ (referred to as _dense models_ hereafter)  that apply all network parameters to each input, MoE dynamically activates distinct parts of the model based on the input tokens. This allows for model scaling without substantially increasing the FLOPs2, thereby maintaining training and inference speeds during model upscaling. Recent advancements have seen successful implementations of MoE across various domains .

Despite their potential, MoE models face significant adoption challenges primarily due to the lack of pre-trained models. Unlike dense models, which benefit from a vast collection of pre-trained resources available through platforms such as Hugging Face  and Timm , most MoE models must be trained from scratch with randomly initialized weights. This process demands substantial computational power and large datasets, limiting MoE research to a select few teams with the necessary resources. Consequently, our research aims to reduce the training time and datarequirements for MoE models by leveraging the pre-trained knowledge from dense checkpoints. _We will specifically investigate whether utilizing dense checkpoints can enhance the accuracy and convergence speed of MoE models during fine-tuning._

In this paper, we propose MoE Jetpack, a new approach for fine-tuning pre-trained dense checkpoints into MoE models. As illustrated in Fig. 1(a), MoE Jetpack leverages the sunk cost of dense pre-training to boost MoE performance and expedite convergence. It comprises two key techniques. Firstly, **checkpoint recycling** is used to initialize MoE models from dense checkpoints. Unlike sparse upcycling , which simply duplicates the Multilayer Perceptron (MLP) to create experts, checkpoint recycling utilizes diverse dense checkpoints and strategic weight selection, providing flexibility and generating higher-quality initialization weights for MoE models. The second technique is the **hyperspherical adaptive MoE (SpheroMoE) layer**, which presents an optimized MoE architecture that facilitates the seamless integration of dense checkpoints and enhances fine-tuning performance. Existing MoE architectures, such as Switch Transformers  and Soft MoE , are not designed to incorporate pre-trained dense checkpoints, often resulting in optimization inefficiencies and over-specialization during fine-tuning. The SpheroMoE layer mitigates these challenges by normalized token mixing, expert regularization, and adaptive dual-path mechanism, ensuring smoother integration and improved performance.

By equipping dense checkpoints with MoE Jetpack, as illustrated in Fig. 1(b), the fine-tuned MoE models achieve significantly higher accuracy. Comprehensive evaluations across various image classification datasets of different scales further demonstrate the effectiveness of MoE Jetpack. In summary, our contributions are as follows:

* We introduce _checkpoint recycling_, which pioneers the sampling of dense checkpoints to initialize MoE experts, enhancing initialization flexibility, diversifying experts, and eliminating the computational burden of MoE pre-training.
* We develop the _spheroMoE layer_, optimized for fine-tuning dense checkpoints into MoE architectures, alleviating optimization challenges, and preventing the over-specialization of experts.

## 2 Background

In this section, we recap the core concepts needed to understand the MoE Jetpack, specifically focusing on the sparsely activated mixture of experts (MoE) and the routing mechanisms. Existing MoE models are typically derived from dense Vision Transformer (ViT) by replacing certain multilayer perceptron (MLP) layers with MoE layers. Each MoE layer consists of a **Router function**, \((x;_{gate})\), which directs input tokens to a set of "experts", where each expert is parameterized as \((;_{i})\). Although the experts in MoE models are typically similar, the routing mechanisms vary and are critical to performance. Several routing algorithms have been developed, including top-\(k\), BASE and Sinkhorn-BASE layers [18; 19], Hash layers , Expert Choice routing , and soft routing .

Figure 1: (a) MoE Jetpack converts dense checkpoints into initialization weights for MoE models, facilitating faster convergence and improved performance while maintaining equivalent FLOPs. Here, **Exp.** represents individual experts, \(E\) denotes the number of experts, and \(L\) indicates the total number of layers. (b) Performance comparison among ViT trained from scratch, pre-trained and fine-tuned ViT, Soft MoE  trained from scratch, and MoE Jetpack across multiple datasets.

The most commonly used mechanism is top-\(k\) routing, which reduces computational overhead by selectively activating only the top-\(k\) experts most relevant to the input tokens. The routing decision is computed as follows:

\[(x;_{gate})=k(((x;_{gate}))),\] (1)

and the output is aggregated by combining the contributions of the selected experts:

\[y=x+_{i E}(x;_{gate})(x; _{i}),\] (2)

where \(\) denotes the weights, \(E\) is the set of activated experts, and \(|E|=K\). However, top-\(k\) routing presents challenges such as imbalanced expert utilization, token dropping, and scalability issues.

A more balanced and representative approach, Soft MoE , effectively addresses these challenges through implicit soft assignments and serves as our baseline. Instead of assigning tokens to specific experts, Soft MoE computes weighted combinations of all input tokens for each expert. Given the input tokens \(^{m d}\), where \(m\) is the number of tokens and \(d\) is their dimensionality, the learnable matrix \(^{d(e s)}\) project the tokens into \(e s\) slots, where \(e\) is the number of experts and \(s\) is the number of slots per expert. The projection is calculated as \(}=()^{}\). Here, \(}^{(e s) d}\) represents the transformed inputs that are weighted combinations of \(\). Each MoE layer includes \(e\) expert functions \(\{f_{i}:^{d}^{d}\}_{i=1}^{e}\), with each expert handling \(s\) slots. The intermediate outputs \(}^{(e s) d}\) are obtained by applying the experts to the slots: \(}_{i,j}=f_{i}(}_{i,j})\) for \(i\{1,,e\}\) and \(j\{1,,s\}\). Finally, the output tokens \(^{m d}\) are reconstructed by combining the expert outputs: \(=()}\).

## 3 MoE Jetpack

In this section, we present the overarching concept of the MoE Jetpack. It is divided into two phases: checkpoint recycling dense checkpoints to initialize MoE models and fine-tuning MoE models using the hyperspherical adaptive MoE (SpheroMoE) layer.

### Checkpoint Recycling

Checkpoint recycling is a foundational phase in the MoE Jetpack framework, transforming pre-trained dense checkpoints (**predecessors**) into high-quality initialization weights (**successors**) for MoE models. This approach leverages the rich pre-trained knowledge from predecessors through weight reuse, boosting the performance and convergence speed of successors. The recycling procedure involves sampling a portion of the weights from the predecessors' multilayer perceptrons (MLPs) to construct experts, ensuring expert diversity and adaptability in expert size to meet varied needs.

To define the process of checkpoint recycling (as illustrated in Fig. 2(a)), consider predecessors with \(N\) layers, where each layer \(L_{i}\) has a feature dimension of \(d\) and a hidden dimension of \(4d\). The object is to transform this predecessor into a successor MoE model \(S\), which also has \(N\) layers, each denoted as \(L^{}_{i}\), but with a reduced feature dimension \(d^{} d\). Following the Soft MoE architecture , the successor comprises two segments: a dense part with \(N_{1}\) layers and an MoE part with \(N_{2}\) layers, where \(N_{1}=N_{2}=\). Formally, the successor model is represented as:

\[S=(\{L^{}_{i}\}_{i=0}^{-1},\{L^{}_{i}\}_{i=}^{N-1}).\] (3)

Inspired by Weight Selection , our recycling process ensures consistency in feature dimensions. We explore four primary strategies to guide the recycling of checkpoints:

**Importance-Based Weight Sampling (default)**: We select weights across hidden units and feature dimensions to construct initialization weights for diverse experts. For **feature dimension** selection, maintaining consistency across all layers is essential . We achieve this by calculating the _mean output features_ across the \(N\) layers and selecting the top-\(d^{}\) dimensions based on these averages:

\[=_{i=1}^{N}O_{i},d^{}=()[ d^{}]\] (4)where \(O_{i}\) is the output features of layer \(L_{i}\), and \(^{n d}\) is the average output features. For **hidden units**, sampling is performed independently for each layer based on the _magnitude of activations_, as the arrangement of units does not impact the output. Activation values from batches of images are used to form a probability distribution, and units are sampled accordingly:

\[P(h|H)=}{_{h^{} H}A_{h^{}}}, h_{} P(h|H),|h_{}|=4d^{},\] (5)

where \(A_{h}\) is the activation value of hidden units \(h\), and \(H\) is the set of all hidden units. This method selects the most important weights for the successor model while promoting diversity among experts through the sampling process.

**Co-Activation Graph Partitioning**: This strategy groups frequently co-activated hidden units into one expert. We construct a co-activation graph by counting the co-activations of hidden units in the testing procedure. Each unit is a vertex in the co-activation graph, and the edges represent their co-activation frequency. Formally, let \(G=(V,E)\) be the co-activation graph, where \(V\) represents the hidden units and \(E\) represents edges with weights indicating co-activation counts. Using the Metis graph partitioning , we get several subgraphs:

\[G=_{i=1}^{k}G_{i}, G_{i}=(V_{i},E_{i}), V_{i} V_{j}= i j.\] (6)

Experts are formed by the combination of sub-graphs. This method leverages the natural grouping of hidden units, ensuring each expert captures a specific functional subset of the predecessor model.

**Uniform Weight Selection**: Weights are selected uniformly across dimensions and hidden units. For a predecessor with feature dimension \(d\) and a successor with dimension \(d^{}\), weights are chosen as:

\[W_{}^{(i)}=W_{}^{(k)}, k= }, i\{0,,d^{}-1\}.\] (7)

This method ensures an even distribution of the pre-trained weights across the successor MoE.

**Random Weight Sampling**: Weights are randomly selected from the predecessor model. Let \(S\) be a random subset of feature dimension indices:

\[S 0,,d-1,|S|=d^{}.\] (8)

Then, the weights for the successor are chosen as:

\[W_{}^{(i)}=W_{}^{(j)}, j S, i \{0,,d^{}-1\}.\] (9)

Figure 2: (a) **Checkpoint Recycling:** Selects hidden units and dimensions from the dense checkpoint MLP, transforming pre-trained weights into diverse experts to initialize MoE models. (b) **SpheroMoE Layer:** Uses cross-attention to dispatch tokens to slots and combine slots back into tokens. Input tokens are projected as keys, and a random query is initialized to compute token-slot similarity in hyperspherical space. Experts then process their assigned slots.

Through the ablation in Sec. 4.3, Importance-Based Weight Sampling is identified as the default method for recycling dense checkpoints to initialize MoE models.

Notably, the computational overhead introduced by Checkpoint Recycling is virtually negligible. Methods such as Random Sampling and Uniform Selection operate with minimal additional processing, as they directly select experts from the dense checkpoint without further computations. While Graph Partitioning and Importance-Based Sampling involve a preliminary inference step to determine neuron importance or co-activation patterns, the time required is minimal. For instance, performing inference on a subset of \(30,000\) images with an RTX 4090 takes less than 5 minutes. This efficient process ensures that Checkpoint Recycling significantly enhances model initialization while introducing almost no additional overhead.

### SpheroMoE Layer

Following the initialization of MoE weights through Checkpoint Recycling, the next step is fine-tuning on downstream datasets. To enhance performance and stability, we designed the hyperspherical adaptive MoE (SpheroMoE) layer (Fig. 2(b)), introducing three key improvements: SpheroMoE Routing to alleviate optimization challenges, Expert Regularization to prevent over-specialization, and Adaptive Dual-path MoE (Fig. 3) for better performance and efficiency. Additionally, the pseudo-code detailing these features' implementation can be found in Appendix C.

**SpheroMoE Routing**: As shown in Fig. 2(b), the proposed hyperspherical MoE (SpheroMoE) routing mechanism utilizes cross-attention  to distribute inputs across experts. Each expert receives an input slot, a weighted average of all input tokens. To maintain consistency between dense checkpoints and MoE layers \(M=\{L_{i}^{}\}_{i=N/2}^{N-1}\), input tokens \(^{b n d}\) (where \(b\) represents the batch size, \(n\) represents the token length, and \(d\) represents the input dimension) are layer normalized inherited from dense checkpoints, resulting in \(}\). Queries \(^{b(e s) d}\) are randomly initialized and similarly normalized to align with \(}\), producing \(}\). The layer normalization process ensures the consistency of distributions between the MoE model, input queries, and the pre-trained dense model. The normalized \(}\) are projected to form keys \(^{b n d}\) for the cross-attention mechanism.

To reduce numerical instability, \(}\) and \(\) are projected onto a hyperspherical space using L2 normalization, ensuring that the resulting dot products reflect cosine similarities rather than unbounded magnitudes. This confines values within a stable range, preventing softmax saturation and enabling more balanced attention distributions, which improves model generalization. The similarity between \(}\) and \(\) is computed, yielding similarity logits \(^{b(e s) n}\): \(=}^{T}\). Input slots \(}^{b(e s) d}\) for experts are formed by a softmax operation along the \(n\) dimension of \(\):

\[}=_{ijk})}{_{k^{}=1}^{n}( _{ijk^{}})}}.\] (10)

Each expert processes its corresponding input slots \(}_{i}\) independently, generating outputs \(}_{i}\). These outputs are then weighted by \(\) (after applying a softmax operation along the \((e s)\) dimension) to aggregate the experts' contributions, producing the final output \(^{b n d}\) of the MoE layer:

\[=_{ijk})}{_{j^{}=1}^{e s}( _{ij^{}k})}}.\] (11)

Figure 3: The Adaptive Dual-path MoE structure enhances the SpheroMoE Router by adapting it into a dual-branch system, designed to optimize computational efficiency and model performance. This configuration directs high-impact tokens to a core path with fewer but larger experts while routing less critical tokens to a universal path equipped with a greater number of smaller experts.

In summary, SpheroMoE routing leverages layer normalization, hyperspherical projection, and cross-attention to effectively distribute inputs across experts, ensuring numerical stability and consistency with pre-trained dense models for improved optimization.

**Expert Regularization**: To improve generalization, SpheroMoE regularizes routing and expert behavior, preventing experts from over-specializing on specific inputs or outputs from depending excessively on certain experts. For the former, we introduce learnable softmax temperatures \(T_{dispatch}\) and \(T_{combine}\)to precisely control token dispatch and slot combination, enabling smooth transitions between broad and focused attention. Since token dispatch distributes tokens across slots, while slot combination aggregates slot outputs back into tokens, these dual temperatures provide flexible control to each process. Both temperatures are initialized high to promote a broad distribution of attention, preventing early specialization. As the training process, these temperatures adaptively decrease, allowing experts to focus more precisely on relevant features and specialize where advantageous. Additionally, we added a certain level of normal noise to the similarity logits \(\), which improves generalization. For the latter, we utilized stochastic expert dropout, where each expert \(i\) is randomly deactivated with a probability \(p\). It ensures that no single expert becomes a crutch for the entire output, promoting a more balanced utilization of all experts. These techniques form an expert regularization strategy that maintains expert versatility and mitigates over-fitting, ensuring the MoE model performs robustly on downstream datasets.

**Adaptive Dual-path MoE**: To mitigate computational redundancy for less critical tokens and concentrate resources on essential ones, SpheroMoE Routing directs input tokens into core and universal slots based on importance. Building on this, the Adaptive Dual-Path structure is designed to assign each slot type to a distinct pathway for optimized processing, as illustrated in Fig. 3. The core pathway consists of a limited number of core experts with larger parameter counts, specifically configured for processing high-importance tokens. In contrast, the universal pathway comprises a larger set of small experts, each with one-fourth of the parameters of core experts, optimized for handling less critical slots. This dual-path configuration improves resource allocation by focusing computation on key tokens, thereby preserving model accuracy and enhancing processing efficiency.

## 4 Experiments

### Experimental Setups

**Models.** We validate our approach using the Vision Transformer (ViT) and ConvNeXt models. Specifically, we initialize the weight of V-JetMoE-T and C-JetMoE-F by transforming the dense checkpoints of ViT-S and ConvNeXt-T (pre-trained on ImageNet-21K and sourced from timm) through checkpoint recycling. As detailed in Sec. 3.1, V-JetMoE-T retains the dense layer structure of ViT-T in its first half, while the latter half is equipped with SpheroMoE layers. Each SpheroMoE layer consists of \(N/2\) core experts and \(N\) universal experts, where \(N\) is the number of input tokens. Further details are in Appendix A.

**Datasets.** We evaluate MoE Jetpack on 8 image classification datasets, including ImageNet-1K , CIFAR-10, CIFAR-100 , Flowers , Pets , STL-10 , Food-101 , and DTD . These datasets encompass a diverse range of classification challenges, including object classification, fine-grained species recognition, and texture classification.

**Baseline Implementation.** We follow the implementation details outlined by Xu et al.  for comparisons of the dense models. For the MoE models, we employ Soft MoE  as the baseline and have replicated it across all datasets. Our MoE Jetpack and Soft MoE utilize the same training strategies as the dense models to ensure comparison fairness. All implementations were executed using the MMPretrain framework  on RTX\(4090\). More information can be found in Appendix B.

### Main Results

Tab. 1 compares the performance of the MoE Jetpack with Dense ViT models and Soft MoE models on various image datasets using ViT-T (a) and ConvNeXt-F (b) architectures. All models maintain approximately the same number of FLOPs. The columns in Tab. 1 are defined as follows:

* **Dense**: Refers to dense models trained from scratch on each specific dataset.

* **Dense (21k)**: Represents dense models initialized with ImageNet-21K pre-trained weights, followed by fine-tuning on the target datasets.
* **Soft MoE**: Reports the results of Soft MoE models trained from scratch on each dataset.
* **MoE Jetpack**: Shows the performance of SpheroMoE models, initialized using checkpoint recycling with pre-trained dense checkpoints from ImageNet-21K, followed by fine-tuning on downstream datasets.

The MoE Jetpack, benefiting from the pre-trained knowledge embedded in dense checkpoints, consistently surpasses the performance of both Soft MoE models trained from scratch and dense models with ImageNet-21K initialization. These results underscore the effectiveness of MoE Jetpack.

### Ablations

We perform ablation studies to assess the impact of various components and hyper-parameters within the MoE Jetpack. By default, we use a ViT-T model with the SpheroMoE layer integrated from layers 7 to 12, comprising 98 core experts and 196 universal experts (detailed in Appendix A). The Checkpoint Recycling method transforms dense checkpoints of ViT-S and ViT-T, pre-trained on ImageNet-21k, into initial weights for our V-JetMoE-T model.

**Effect of MoE Jetpack Components.** We conducted the ablation of two key components of the MoE Jetpack on three datasets. As shown in Tab. 2, integrating Checkpoint Recycling with the Soft MoE baseline significantly improves performance across all datasets, with a mean accuracy increment of \(10.2\%\). The SpheroMoE layer further enhances performance, achieving a mean accuracy of \(87.9\%\). These results demonstrate the efficacy of both components, especially when used together, highlighting their synergistic effect in boosting performance.

**Checkpoint Recycling vs. Sparse Upcycling.** To compare the four checkpoint recycling strategies mentioned in Sec. 3.1 and the method of using duplicated MLPs to construct experts in Sparse Upcycling , we conducted experiments on ImageNet. For fairness, we also employed our SpheroMoE layer in the Sparse Upcycling. The results, summarized in Tab. 3, show that Importance-Based Sampling achieves the highest performance, demonstrating its

Table 1: Performance comparison on visual recognition tasks with ViT-T and ConvNeXt-F.

Table 2: Ablation Study on MoE Jetpack Components.

Table 3: Checkpoint Recycling vs. Sparse Upcyclingeffectiveness in leveraging critical weights to enhance model performance and convergence speed. Additionally, Checkpoint Recycling is highly flexible, allowing the construction of experts of varying sizes to meet different needs, a feature not provided by sparse upcycling.

**Effect of Dual-Path Structure.** The Tab. 4 presents an ablation study on the effectiveness of the dual-path structure in SpheroMoE. When configured with 197 core experts only (no universal experts), SpheroMoE achieves a higher accuracy on ImageNet-1K compared to Soft MoE with the same number of experts. Introducing the dual-path structure with 98 core experts and 196 universal experts (each with one-fourth of the parameters of core experts) further enhances accuracy to 79.9 while reducing the computational cost to 1.1G FLOPs. This result highlights the efficiency of the dual-path structure, which allows SpheroMoE to allocate resources adaptively and achieve better performance without increasing the overall FLOPs.

**Core Experts Ratio.** To evaluate the effectiveness of the Adaptive Dual-path MoE structure introduced in Sec.3.2 and to identify the optimal ratio between core and universal experts, we conducted ablation on the CIFAR-100 dataset. With a fixed total number of experts, we varied the ratio of core experts to find the ideal balance between performance and resource allocation. As shown in Fig. 4, the highest accuracy is achieved when core experts constitute approximately \(1/3\) of the total experts.

**MoE Jetpack Configurations.** This part evaluates the impact of various MoE Jetpack configurations on model performance, as summarized in Tab. 5. The experiments focus on the placement of SpheroMoE layers, the number of experts per layer, and the base size of converted dense checkpoints. Results indicate that more SpheroMoE layers generally enhance performance, though placing it before layer \(7\) slightly hurt the performance. Consequently, SpheroMoE layers were incorporated into layers \(7\!-\!12\). Additionally, models with more experts exhibit improved accuracy, highlighting the benefits of increased expert specialization and diversity. Models converted from larger dense checkpoints demonstrate superior performance. These findings suggest that MoE network performance can be improved by increasing the number of MoE layers, incorporating more experts, and utilizing larger base models.

   Model & Experts & ImageNet & FLOPs (G) \\  Soft MoE & 197 & 78.4 & 1.2 \\ SpheroMoE & core: 197, univ: 0 & 79.6 & 1.2 \\ SpheroMoE & core: 98, univ: 196 & 79.9 & 1.1 \\   

Table 4: Effectiveness of SpheroMoE with the Dual-Path Structure.

Figure 4: CIFAR-100 accuracy across different ratios of core (dark) to universal (light) experts, highlighting optimal performance at a 1/3 core ratio.

   model & Weight Init. & MoE Layers & Expert Number & Param (M) & FLOPs (G) & CIFAR-100 & ImageNet \\  ViT-T & - & - & - & \(6\) & \(1.1\) & \(72.3\) & \(73.9\) \\ Soft MoE-T  & - & \(7\!\!:\!12\) & \(197\) & \(354\) & \(1.2\) & \(75.9\) & \(77.1\) \\ Soft MoE-S  & - & \(7\!\!:\!12\) & \(197\) & \(1412\) & \(4.5\) & \(77.5\) & \(80.3\) \\  ViT-T & ✓ & - & - & \(6\) & \(1.1\) & \(81.4\) & \(75.5\) \\ V-JeMoE-T & ✓ & \(11\!\!:\!12\) & core: \(98\), univ: \(196\) & \(92\) & \(1.1\) & \(87.4\) & - \\ V-JeMoE-T & ✓ & \(9\!\!:\!12\) & core: \(98\), univ: \(196\) & \(179\) & \(1.1\) & \(87.8\) & - \\ V-JeMoE-T & ✓ & \(5\!\!:\!12\) & core: \(98\), univ: \(196\) & \(352\) & \(1.2\) & \(86.7\) & - \\  V-JeMoE-T & ✓ & \(7\!\!:\!12\) & core: \(32\), univ: \(64\) & \(89\) & \(0.8\) & \(87.8\) & - \\ V-JeMoE-T & ✓ & \(7\!\!:\!12\) & core: \(64\), univ: \(128\) & \(175\) & \(1.0\) & \(88.0\) & - \\  V-JeMoE-T & ✓ & \(7\!\!:\!12\) & core: \(98\), univ: \(196\) & \(265\) & \(1.1\) & \(88.4\) & \(79.9\) \\ V-JeMoE-S & ✓ & \(7\!\!:\!12\) & core: \(98\), univ: \(196\) & \(1058\) & \(4.3\) & \(\) & \(\) \\   

Table 5: Comparison of Model Variants with Different Configurations

### Analysis

We analyze the impact of MoE Jetpack on convergence speed for MoE models fine-tuned on ImageNet-1K and CIFAR-100 dataset. Additionally, we offer insights into expert attention patterns and the contribution of each expert to the output tokens.

**Accelerating MoE Convergence with MoE Jetpack.** The effect of MoE Jetpack on convergence speed is illustrated in Fig. 5 for ImageNet (left) and CIFAR-100 (right). In both cases, models with MoE Jetpack achieve target accuracy significantly faster. For ImageNet, MoE Jetpack enables the model to reach approximately \(77\%\) top-\(1\) accuracy within \(150\) epochs--twice as fast as training from scratch. This acceleration is even more pronounced on smaller datasets like CIFAR-100, where MoE Jetpack achieves \(76\%\) top-\(1\) accuracy by around \(40\) epochs, an eightfold improvement over the baseline. These results underscore MoE Jetpack's efficiency in accelerating convergence, reducing fine-tuning time and computational demands.

**Intuition of Expert Attention Patterns.** We visualize the attention maps of experts in Fig. 6(a), which illustrates that different experts focus on different parts of the input image. This diversity in attention suggests that each expert specializes in capturing unique aspects of the input, enhancing the model's ability to represent features comprehensively. The specialization allows the MoE model to combine multiple perspectives, resulting in a more robust and detailed understanding of the input.

**Contribution of Each Expert to Final Results.** Fig. 6(b) demonstrates the varying contributions of core and universal experts across different layers of the MoE model. Core experts show an increasing influence in the later layers, emphasizing their role in refining specific and highly relevant features. Additionally, the contributions among core experts are markedly uneven, some experts can impact output tokens \(17\) more than others, reflecting greater specialization and diversity in their focus areas.

Figure 5: Comparison of convergence speeds using MoE Jetpack versus training from scratch on ImageNet (left) and CIFAR-100 (right). MoE Jetpack achieves target accuracies significantly faster, demonstrating a 2x speed increase on ImageNet and an 8x increase on CIFAR-100.

Figure 6: (a) The attention maps generated by five experts in response to the input image, highlighting the experts’ specialization. (b) These line charts show varying contributions of core and universal experts, with core experts’ influence peaking in later layers, emphasizing their detailed feature refinement, contrasted with the consistent input of universal experts.

In contrast, universal experts maintain a relatively consistent contribution level, indicating a more uniform integration of broader contextual information throughout the network. This hierarchical structure, balancing the specialized refinement by core experts with the generalized understanding provided by universal experts, enhances the model's overall performance and robustness.

## 5 Related Work

**Sparsely activated Mixture of Experts (MoE).** Scaling Laws  indicate that increasing model parameters can enhance performance. However, traditional densely activated models (dense models) [7; 8] activate all parameters for every input, resulting in high computational costs as models scale. In contrast, MoE models [12; 34; 35; 36] activate only a subset of parameters for specific input tokens, enabling efficient scaling to trillions of parameters with sublinear increases in computational costs [37; 5; 4]. To optimize input token allocation among experts, various routing mechanisms have been developed. BASELayer  formulates token-to-expert allocation as a linear assignment problem, while EC-CF2  propose expert choice routing, soft routing methods like SMEAR , and Soft MoE  implicit soft assignments involving all tokens. However, few studies explore leveraging dense model checkpoints to accelerate MoE training .

**Knowledge transfer with pre-trained models.** Knowledge transfer occurs between **identical** or **distinct** models. Pre-training followed by fine-tuning is well-established for **identical models**, utilizing large datasets through supervised learning (e.g., ImageNet21k , JFT-300M ) or self-supervised methods (e.g., BERT , CLIP , MAE , DINO , EVA [44; 45]). These approaches produce foundation models with broad applicability, and subsequent fine-tuning consistently improves performance. For **distinct models**, knowledge distillation  trains a smaller student model to mimic the larger teacher model, enhancing efficiency. Additional strategies include weight pruning [47; 48; 49; 50; 47], which removes redundant parameters, and weight selection  initializes a smaller model with a subset of weights from a pre-trained larger model.

_Research on transferring knowledge from dense checkpoints to MoE models is limited_. MoEfication  partitions a dense model into MoE components, while Sparse Upcycling  replicates a dense model multiple times to form a MoE model. Our MoE Jetpack recycles important weights from larger dense checkpoints to initialize experts of various sizes, combining the flexibility of knowledge transfer across distinct model types with the efficiency of transfer between identical model types.

## 6 Conclusion

In this paper, we introduced MoE Jetpack, a novel framework for fine-tuning pre-trained dense checkpoints into Mixture of Experts model. Our approach leverages checkpoint recycling, which inherits the knowledge of open-source dense checkpoints and the hyperspherical adaptive MoE (SpheroMoE) layer to enhance fine-tuning performance. These innovations contribute to improved convergence speed and model accuracy. The MoE Jetpack significantly improved various visual tasks while maintaining computational efficiency.

The **limitation** of our approach lies in its reliance on the quality of pre-trained dense checkpoints; inadequately trained or poorly generalized dense models may hinder performance improvements. Additionally, while our experiments focused on visual tasks, further research is needed to validate the generalizability of MoE Jetpack across other domains, such as natural language processing and reinforcement learning. Future work may address these limitations, further enhancing the scalability and robustness of the framework, and broadening MoE applicability across diverse tasks.