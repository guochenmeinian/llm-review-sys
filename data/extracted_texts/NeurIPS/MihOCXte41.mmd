# EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching

Xinwang Chen\({}^{41}\), Ning Liu\({}^{41}\), Yichen Zhu\({}^{1}\), Feifei Feng\({}^{1}\), Jian Tang\({}^{12}\)

\({}^{1}\) Midea Group, \({}^{2}\) Beijing Innovation Center of Humanoid Robotics

chen_xinwang@xs.ustb.edu.cn,ningliu1220@gmail.com

{zhuyc25, feifei.feng}@midea.com, jian.tang@x-humanoid.com

Joint first authorship. Either author can be cited first.Corresponding author.

###### Abstract

Transformer-based Diffusion Probabilistic Models (DPMs) have shown more potential than CNN-based DPMs, yet their extensive computational requirements hinder widespread practical applications. To reduce the computation budget of transformer-based DPMs, this work proposes the **E**fficient **D**iffusion **T**ransformer (EDT) framework. The framework includes a lightweight-design diffusion model architecture, and a training-free Attention Modulation Matrix and its alternation arrangement in EDT inspired by human-like sketching. Additionally, we propose a token relation-enhanced masking training strategy tailored explicitly for EDT to augment its token relation learning capability. Our extensive experiments demonstrate the efficacy of EDT. The EDT framework reduces training and inference costs and surpasses existing transformer-based diffusion models in image synthesis performance, thereby achieving a significant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL attained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training phase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the corresponding sizes of MDTv2. Our code is available at here.

## 1 Introduction

Numerous studies  and practical applications  have validated the effectiveness of Diffusion Probabilistic Models (DPMs), establishing them as a mainstream method in image generation. In past years, predominant works  have advanced diffusion models by incorporating a convolutional UNet-like  architecture as their backbone. On the other hand, transformers  have achieved significant milestones in both natural language processing [14,

Figure 1: Illustration of the alternation process of local and global attention during sketching.

15] and computer vision [16; 17; 18; 19], prompting recent attempts to integrate these powerful transformer-based architectures into diffusion models with considerable success. For instance, U-ViT , an early work in diffusion leveraging ViT-based transformers, surpassed the contemporary CNN-based U-Net DPMs in class-conditional image generation on ImageNet, demonstrating their potential. Similarly, diffusion transformer (DiT) , which employs transformers as its backbone instead of the traditional U-Net backbone in latent diffusion models (LDM) , has shown excellent scalability. Further, masked diffusion transformer (MDT)  observes that DPMs often struggle to learn the relations among object parts in an image. To solve this, MDT introduces a masking training scheme to enhance the DPMs' ability to relation learning among object semantic parts in an image. MDT established a SOTA of class-condition image synthesis on the ImageNet.

While transformer-based DPMs offer scalability and a higher performance ceiling than their CNN counterparts, they also require more computational resources. For instance, in each inference step, DiT-XL-2 consumes 118 GFLOPs and U-ViT-H requires 133 GFLOPs. This computational demand escalates with increasing time steps or token length, limiting their practical application. Despite their computational inefficiency, few studies have explored enhancing the efficiency of transformer-based DPMs. Therefore, the trade-off between computation and performance underscores the importance of designing a lightweight model architecture that maintains excellent performance.

To improve the computational efficiency of transformers in DPMs, we introduced a comprehensive optimization framework named **E**fficient **D**iffusion **T**ransformer (EDT). Specifically, we developed a lightweight diffusion transformer architecture based on a comprehensive computation analysis. Moreover, we devised the Attention Modulation Matrix (AMM) and its alternation arrangement in EDT inspired by human-like sketching. AMM, functioning as a plug-in, can be seamlessly integrated into diffusion transformers to enhance image synthesis performance significantly without requiring additional training. Additionally, we introduced a novel token relation-enhanced masking training strategy tailored for EDT to enhance its relation learning capability.

**Lightweight-design diffusion transformer** Based on the empirical analysis of the number of tokens, token dimensions, and the FLOPs, we propose two principles to design the lightweight diffusion transformer, and redesign and incorporate the down-sampling, up-sampling, and long skip connection modules into diffusion transformers. The utilization of down-sampling module can reduce FLOPs, but harms performance, since the token merging operation in down-sampling and long skip connections modules leads to the loss of token information. To mitigate this loss, we enhance the key features by introducing token information enhancement and positional encoding supplement.

**Attention Modulation Matrix** The mind stores visual structures as a top-down hierarchy passing from general shape to the relationships between parts down to the detailed features of individual parts [23; 24]. Based on this storage structure in the mind, humans tend to follow a coarse-to-fine drawing strategy . The logical structure of sketching of humans tends to first form a general framework (using global attention), then gradually refine local details (using local attention) driven by the global perspective (using global attention) shown in Figure 1. Inspired by the sketching process, we integrate the alternation process of local and global attention to EDT, and propose Attention Modulation Matrix (AMM) to modulate from the default global attention in self-attention mechanisms to local attention. AMM, functioning as a plug-in, which can be seamlessly integrated into diffusion transformers, enhancing image synthesis performance without necessitating additional training.

**Token relation-enhanced masking training strategy** The token compression in down-sampling modules may cause token information loss. Learning the relations among tokens can help token down-sampling modules compress tokens effectively. And it has been confirmed that masking training can enhance the DPMs' ability to learn relations among object parts in images . We propose a novel masking training strategy to enhance the relation learning among tokens. Specifically, the full tokens are fed into EDT and the tokens masking is executed in down-sampling modules. This forces models to learn token relations before some of the tokens are masked. We compare our masking training method to the counterpart in MDT, both implemented on EDT. Our masking training method achieves better generation.

We summarize the contributions of our work: 1. We develop an Efficient Diffusion Transformer (EDT) framework and design a lightweight diffusion transformer architecture based on a comprehensive computation analysis. 2. Inspired by human sketching, we design EDT with an alternation process between global attention and location attention. Moreover, to the best of our knowledge, we introduce Attention Modulation Matrix for the first time, which improves the detail of generated images of pre-trained diffusion transformers without any extra training cost. 3. We propose a novel token masking training strategy to enhance the token relation learning ability of EDT. 4. EDT has reached a new SOTA and achieves faster training and inference speed compared to existing representative works DiT and MDTv2. We conduct a series of exploratory experiments and ablation studies to analyze and summarize the key factors affecting the performance of EDT.

## 2 Method

### Preliminaries

We briefly review several fundamental concepts necessary to understand classifier-free guidance class-condition diffusion models . The primary objective of diffusion models is to learn a diffusion process that constructs a probability distribution for a specific dataset, subsequently enabling the sampling of new images. Given a classifier-free guidance class-condition diffusion model \(_{}(x_{t},c)\), the model can generate images of specific class \(c\) from Gaussian noise over multiple denoising time steps. The model operates through two main processes: the forward and reverse processes. The forward process simulates training data \(x_{t}\) to be denoised at time step \(t\), by adding Gaussian noise \(_{t}(0,)\) to the original data \(x_{0}\). This process is mathematically described by \(q(x_{t}|x_{0})=(x_{t};}x_{0},(1-_{t}))\), where \(_{t}\) denotes a hyperparameter. The reverse process samples noise-reduced data \(x_{t-1}\) based on noise data \(x_{t}\) and class-condition \(c\). The reverse process is represented as \(p_{}(x_{t-1} x_{t},c)=(x_{t-1}|_{}(x_ {t},c),_{}(x_{t},c))\), where \(_{}\) and \(_{}\) are the statistics of \(p_{}\). By optimizing the variational lower-bound of the log-likelihood \(p_{}(x_{0})\) and reparameterizing \(_{}\) as a noise prediction network \(_{}\), the model can be trained using simple mean-squared error between the predicted noise \(_{}(x_{t},c)\) and the ground truth \(_{t}\) sampled Gaussian noise: \(_{}(x_{t},c,_{t})=\|_{}( x_{t},c)-_{t}\|_{2}^{2}\). Additionally, \(_{}(x_{t},c)\) is a standard class-condition model; when \(c=\), it functions as an unconditional model. To allow the controllability of class-condition guidance, the prediction of models is further derived as \(}_{}(x_{t},c)=_{} (x_{t},)+(_{}(x_ {t},c)-_{}(x_{t},))\), where \( 1\) is class-condition guidance intensity.

In this work, we employ a classifier-free guidance class-condition diffusion transformer architecture operating on latent space. The pre-trained variational autoencoder (VAE) model  from LDM  remains frozen and is used to encode/decode the image/latent tokens.

### Lightweight-design diffusion transformer

Transformer-based diffusion probabilistic models (DPMs) have demonstrated greater scalability and superior performance compared to CNN-based DPMs [20; 21; 22]. However, these models also entail significant computational overhead during both the training and inference phases. In response, we design a lightweight diffusion transformer architecture in this section. We undertake a computational complexity analysis of the transform-based diffusion model. Based on the empirical analysis of the number of tokens, token dimensions, FLOPs, and the number of parameters, we establish two design principles: (1) reducing the number of tokens to decrease the FLOPs in the self-attention module through the down-sampling module; (2) ensuring that the FLOPs of each EDT stage post a down-sampling module are significantly reduced compared to the stages prior to the down-sampling module, to effectively lower the overall FLOPs.

Building on the aforementioned design principles, we have redesigned and incorporated the down-sampling, up-sampling, and long skip connection modules into the transformer-based diffusion model, successfully achieving a reduction in FLOPs and increased inference speed. For instance, in comparison to DiT-S , our smaller version model EDT-S achieves an inference speed of 5.5 steps per second, versus 2.7 steps per second for DiT-S, effectively doubling the speed. Figure 2 illustrates the architecture of our lightweight-designed diffusion transformer. The model includes three EDT stages in the down-sampling phase, viewed as an encoding process where tokens are progressively compressed, and two EDT stages in the up-sampling phase, viewed as a decoding process where tokens are gradually reconstructed. These five EDT stages are interconnected through down-sampling, up-sampling, and long skip connection modules. Note that each EDT stage comprises several consecutive transformer blocks. For more details on the computational complexity analysis and model design, please refer to Appendix A.2. It is important to note that _the down-sampling and up-sampling phases can be viewed as encoding and decoding processes, respectively, aligning with the conceptualization of drawing pictures in human sketching._ These phases are crucial and will be further discussed in the following Section 2.3 and Section 2.4.

While we have successfully reduced the FLOPs of the model, the token merging operation in the down-sampling and long skip connection modules inevitably leads to a loss of token information, including the positional encoding and contextual data essential for class-condition generation of images. To mitigate this loss of token information, we propose two improvements, as illustrated in Figure 3: **token information enhancement** and **positional encoding supplement**.

**Token information enhancement** We enhance the contextual information required for class-condition generation by employing Adaptive Layer Normalization (AdaLN) before the token merging process. AdaLN adjusts the output by learning scaling factors \(\) and bias coefficients \(\), which can scale, negate, or shut off the features . By utilizing AdaLN, we modulate the tokens based on class conditions and time steps before merging, thereby preserving more contextual information and minimizing the loss of token information. As depicted in Figure 3, class-condition information is integrated by the \(\) and \(\) of AdaLN in both the down-sampling and long skip connection modules.

**Positional encoding supplement** We restore the absolute positional encoding of tokens following the token merging process. As illustrated in Figure 3, after merging the tokens, we add the absolute positional encoding to the merged tokens at the end of both the down-sampling and long skip connection modules.

### Making EDT "sketch" like a human

The lightweight design of EDT might compromise the quality of image synthesis. To enhance the detail fidelity in generated images, we have refined the decoding process (up-sampling phase) of EDT by imitating the process of human sketching. We begin by examining how attention shifts during the act of sketching by humans. Human cognition stores visual structures as a top-down hierarchy passing from general shape to the relationships between parts down to the detailed features of individual parts [23; 24]. This hierarchical structuring of visual information in the brain makes humans tend to follow a coarse-to-fine strategy in sketching . As shown in Figure 1, the process of human

Figure 3: The design of down-sampling, long skip connection and up-sampling modules.

Figure 2: The architecture of lightweight-design diffusion transformer.

sketching tends to first form a general framework (using global attention), then gradually refine local details [24; 28] (using local attention) hinted by the global perspective (using global attention). Even when concentrating on a local detail, humans do not become completely detached from the overall framework. Therefore, humans periodically shift attention back to a global view to scrutinize the local detail and further fine-tune it [29; 30]. This process reflects the alternation of global and local attention in the human brain when sketching.

Inspired by the sketching process, we aim to integrate the alternation process of local attention and global attention to EDT. In the current series of diffusion transformers [20; 22; 21], only default global attention mechanisms are employed, which may lead to poor generation of local details. Therefore, we introduce the Attention Modulation Matrix (AMM) to enhance focus on local details. Moreover, to mimic the alternation process in the EDT, we alternately incorporate the AMM into the lightweight-design transformer diffusion architecture.

#### 2.3.1 Integrating local attention into the up-sampling phase of EDT

To imitate the alternation between global and local attention like the act of humans drawing, we integrate local attention into the up-sampling phase of EDT by introducing Attention Modulation Matrix (AMM). In this section, we concentrate on imitating the alternation process of attention. A detailed discussion of AMM is deferred to Section 2.3.2. We align the decoding process of EDT with the humans drawing pictures. Consequently, we incorporate local attention (AMM) into the decoding process (up-sampling phase) of EDT. Figure 4 illustrates the placement of AMM (local attention) in an EDT stage. As depicted in Figure 4(a), we alternately configure EDT blocks with and without the AMM, thereby mimicking the alternation between global and local attention observed in drawing activities. The EDT block with AMM is shown in Figure 4(b). As shown in Figure 4(c), the AMM is integrated into the self-attention module. The AMM and the global attention score matrix are combined via a Hadamard product to modulate global attention into local attention.

#### 2.3.2 Attention modulation matrix

We develop the Attention Modulation Matrix (AMM) to modulate the default global attention in self-attention mechanisms into local attention, which imitates the local attention of humans during the act of drawing. Humans typically concentrate on either the actively engaged parts or the most salient aspects of a visual scene . When drawing a specific local region of an image, areas closer to the region of interest tend to exhibit stronger contextual relations and thus warrant increased attention. Conversely, areas further from the region of interest generally show weaker contextual relations and can be allocated less attention. Thus, we articulate the principle: _for a local region, the strength of attention on contextual relations within a specific region is inversely related to the distance between the local region and the specific region._ In the self-attention mechanism, we regard the attention score between tokens as an indicator of the strength of attention on contextual relations between regions. Similarly, we aim to modulate the strength of attention based on the distance among tokens

Figure 4: The position of Attention Modulation Matrix (local attention) in an EDT stage in the up-sampling phase.

on the image. Based on this concept, we have developed the Attention Modulation Matrix (AMM), functioning as a plug-in, which can be seamlessly integrated into diffusion transformers, significantly enhancing image synthesis performance without necessitating additional training.

Formally, given a sequence of \(N N\) tokens and its corresponding attention score matrix \(^{N^{2} N^{2}}\), we take two arbitrary tokens as an example to illustrate the formulation. The two arbitrary tokens are denoted as \(Token_{i}\), \(Token_{r}\) and their attention score \(a_{ir}\), where \(a_{ir}\). The coordinates of \(Token_{i}\) and \(Token_{r}\) correspond to the \((x_{i},y_{i})\) and\((x_{r},y_{r})\) in the original \(N N\) tokens grid, where \(i=Nx_{i}+y_{i}\) and \(r=Nx_{r}+y_{r}\). The distance between these two tokens can be calculated by Euclidean distance \(d_{ir}=-x_{r})^{2}+(y_{i}-y_{r})^{2}}\), and we can derive the token distance matrix \(^{N^{2} N^{2}}\). We aim to modulate the global attention into local attention by multiplying the attention score matrix to the AMM, which is generated based on the token distance matrix \(\). The modulation matrix generation function \(F\) is designed with adherence to two principles: (1) the generation function should be monotonically decreasing within the interval \([0,d_{max}]\), ensuring that the modulation matrix elements are inversely correlated with distance, where \(d_{max}=(N-1)\) is the furthest distance, which is the distance between two diagonal opposite tokens; (2) the output range of this function should be limited to avoid significantly altering the original distribution of the attention score matrix. Based on the two principles, we utilize the monotonically decreasing interval in cosine function, \((fd_{ir})\), \(d_{ir}[0,d_{max}]\), where the monotonically decreasing interval can be flexibly adjusted by adjusting its period \(T\) or frequency \(f\). According to the \(d_{max}\), we set \(T=4d_{max}\) and \(f=\). Further, we employ \((fd_{ir})\) as the exponent of the Euler's number \(e\), thereby smoothing the values of the modulation matrix elements. We obtain the final modulation matrix generation function \(F(d_{ir})=ke^{(fd_{ir})}\), which can flexibly scale the function value to \([k,ke]\) by scaling factor \(k\). We empirically set \(k=0.5\) and the output range within \([,]\), which allows the modulation matrix elements to appropriately adjust the attention scores. In addition, we define an effective radius \(R\) for local attention to exclude the interactions of tokens that occur over tokens with far distances. For each token pair, we only modulate the attention scores with distance \(d_{ir} R\), where \(R\) is the effective radius for local attention. We set \(R=+4}\) based on experiments regarding the hyper-parameters of the AMM, detailed in Appendix A.3.3. And those \(d_{ir}>R\), their attention scores are set to zero, indicating that tokens far from the region of interest exert less influence. Thus, we have the Attention Modulation Matrix \(^{N^{2} N^{2}}\), where \(m_{ir}\) is defined as:

\[m_{ir}=\{F(d_{ir}),&d_{ir} R\\ 0,&d_{ir}>R.\] (1)

The modulated attention score element is \(a^{}_{ir}=a_{ir}*m_{ir}\), where \(a^{}_{ir}^{}\), and \(^{}\) is the modulated attention score matrix. Further details about the entire process and an illustration of AMM can be found in Appendix A.3.1.

### Token relation-enhanced masking training strategy

The ability to learn relations among object parts in images is crucial for image generation, as highlighted in MDT . However, the down-sampling process in EDT inevitably leads to the loss of token information. Establishing relations among tokens can alleviate performance degradation caused by the loss of token information. To enhance the relation-learning ability in EDT, we introduce a relation-enhanced masking training strategy. Before detailing the proposed masking training strategy, we first explore the integration of MDT into EDT. Figure 5 (a) shows the masking training strategy of MDT. In MDT, the training loss \(L\) contains two parts as shown in Eqn. 2.

\[L=L_{full}+L_{masked}=_{}(x_{t},c,_{t})+_ {}(mask*x_{t},c,_{t})\] (2)

\(L_{full}\) is the loss when the input consists of the full token input, the \(L_{masked}\) is the loss when the input consists of the remained tokens after masking, and \(mask\) is a matrix to mask tokens randomly.

However, our analysis reveals that the masking training method used in MDT excessively focuses on masked region reconstruction at the expense of diffusion training, potentially leading to a degradation in image generation performance. Additionally, our evaluation of MDT is observed a conflict between the training objectives of \(L_{full}\) and \(L_{masked}\). Specifically, as \(L_{full}\) decreases, \(L_{masked}\) increases, and vice versa, demonstrating the conflicting nature of these training objectives. To mitigate this conflict and allow the model to focus on the diffusion generation task, as shown in Figure 5 (b), we design the token relation-enhanced masking training strategy, which feeds full tokens into shallow blocks and postpones the token masking operation to occur within the down-sampling modules. This strategy is designed to facilitate learning relationships among tokens and reduce the loss of token information without the issues arising from conflicting training objectives. When training, the masked tokens are unseen to the EDT blocks following the operation of masking, which forces the EDT blocks before the operation of masking to learn the relations among tokens. As the relations among tokens are learned, the key information in each token is dispersed and stored across various tokens. This avoids reliance on certain tokens and reduces the loss of token information from compression in down-sampling. Figure 3(a) shows, with a red arrow in the down-sampling module, the specific point at which token masking is performed. After passing through the down-sampling modules, the EDT stages in up-sampling phase generate images solely relying on the remained tokens. The loss function of EDT with token relation-enhanced masking training strategy is shown in Eqn. 3, where the token masking operation is executed in the down-sampling modules.

\[L=L_{full}+L_{masked}=_{}(x_{t},c,_{t})+_ {}(x_{t},c,mask,_{t})\] (3)

We discover that EDT is particularly well-suited for masking training due to its up-sampling modules, which inherently are used for the reconstruction of tokens. Unlike MDT, which requires an additional interpolator module for reconstructing masked tokens, EDT eliminates the need for such a module, thereby reducing unnecessary training overhead associated with an interpolator compared to MDT. For a more detailed analysis, please refer to Appendix A.4.1.

## 3 Experiment

### Implementation Details

**Models**: We develop three different sizes of EDT including small (EDT-S), base (EDT-B) and extra large (EDT-XL), each using a patch size of two. Details regarding token dimensions, head numbers, and parameter counts are provided in Table 5 of Appendix A.2.2. **Training and evaluation**: The training dataset is ImageNet  with 256x256 and 512x512 resolution. For a fair comparison, we follow the training settings of MDTv2 . EDT uses the Adan  optimizer with a global batch size of 256 and without weight decay. The learning rate linearly decreases from 1e-3 to 5e-5 over 400k iterations. **Masking training strategy**: We set the mask ratio \(0.4 0.5\) in the first down-sampling module, and \(0.1 0.2\) in the second. The investigation of mask ratio refers to Appendix A.4.2. **GPUs**: Training is conducted on eight L40 48GB GPUs, while the speed test for inference is performed on a single L40 48GB GPU. **Evaluation metrics**: Common metrics such as Fre'tchet Inception Distance (FID) , sFID , Inception Score (IS) , Precision, and Recall  are used to assess the model performance. The training speed is evaluated by iterations per second, and inference speed is assessed by steps per second using a batch size of 256 in FP32. For fair comparison, we follow  and employ the TensorFlow evaluation suite from ADM , reporting FID-50K results with 250 DDIM  sampling steps. These metrics are reported by default without the classifier-free guidance.

Figure 5: Token relation-enhanced masking training strategy. MDT is fed the remained tokens after token masking into models. EDT is fed full tokens into shallow EDT blocks, and the operation of token masking is performed in down-sampling modules.

### Comparison with SOTA transformer-based diffusion methods

To validate the enhancements in speed and generation performance of EDT, we conducted comparisons with both classical methods [4; 11; 21] and recent advancements [33; 39; 40].

**Experiment on ImageNet 256x256** The result of image generation without classifier-free guidance is shown in Table 1. Our comparisons across three different sizes demonstrate that EDT consistently achieves the best FID scores: EDT-S scored an FID of 34.2, EDT-B scored 19.1, and EDT-XL scored 7.5. Notably, EDT also showed significant reductions in GFLOPs compared to the second-best MDTv2 across all sizes (2.66 GFLOPs vs. 6.07 GFLOPs, 10.2 GFLOPs vs. 23.02 GFLOPs, 51.83 GFLOPs vs. 118.69 GFLOPs). Moreover, EDT exhibited the lowest memory consumption during inference across all three sizes, underscoring the efficiency of our lightweight design. We further investigated the training speed of EDT. Given that both EDT and MDTv2 incorporate additional training strategies, we specifically compared the training speeds of these two models. Additionally, we assessed the training speed of EDT without the masking training strategy (denoted as EDT*) against other methods. In both scenarios, EDT trained faster than the baseline models. For example, EDT-XL* achieved a training speed of 1.49 iter/s, compared to 0.93 iter/s for DiT-XL. In comparison to MDTv2-XL, which trained at 0.51 iter/s, EDT-XL was nearly twice as fast at 0.98 iter/s. We further perform the experiment on **the image generation with classifier-free guidance**. The result is shown in Table 13 of Appendix A.5.1. Under the same training cost, EDT-S-G achieves the lowest FID score compared to MDTv2-S-G (9.89 vs. 15.62). Overall, these findings confirm that EDT significantly enhances both the speed and performance of image synthesis. Additionally, the training cost of EDT is efficient. We include a training cost analysis of EDT in Appendix A.2.3.

   Model &  Cost\(\) \\ (IterxBS) \\  &  Params. \\ (M) \\  &  T-speed \\ (iter/s) \\  & GFLOPs\(\) &  I-Speed \\ (step/s) \\  & 
 Mem. \\ (MB) \\  & FID\(\) \\  DiT-S & 400K\(\)256 & 32.90 & 12.50 & 6.06 & 2.70 & 4296 & 68.40 \\ SD-DiT-S & 400K\(\)256 & 32.90 & - & - & - & - & 48.39 \\
**EDT-S***(our)** & 400K\(\)256 & 38.30 & **13.20** & **2.66** & **5.50** & **4268** & 38.73 \\ MDTv2-S & 400K\(\)256 & 33.10 & 2.25 & 6.07 & 2.40 & 4902 & 39.50 \\
**EDT-S****(our)** & 400K\(\)256 & 38.30 & 8.86 & **2.66** & **5.50** & **4268** & **34.27** \\  DiT-B & 400K\(\)256 & 130.30 & 4.30 & 23.01 & 1.11 & 8978 & 43.47 \\ SD-DiT-B & 400K\(\)256 & 130.30 & - & - & - & - & 28.62 \\
**EDT-S***(our)** & 400K\(\)256 & 152.00 & **5.80** & **10.20** & **2.20** & **8584** & 23.19 \\ MDTv2-B & 400K\(\)256 & 130.80 & 1.42 & 23.02 & 0.96 & 9212 & 19.55 \\ MDTv2-B & 1600K\(\)256 & 130.80 & 1.42 & 23.02 & 0.96 & 9212 & 13.60 \\ EDT-B(our) & 400K\(\)256 & 152.00 & 4.03 & 10.20 & 2.20 & 8584 & 19.18 \\
**EDT-B****(our)** & 1000K\(\)256 & 152.00 & 4.03 & **10.20** & **2.20** & **8584** & **13.58** \\  ADM & 1980k\(\)256 & 554.00 & - & 1120.00 & - & - & 10.94 \\ LDM-4 & 178k\(\)1200 & 400.00 & - & 104.00 & - & - & 10.56 \\ DiT-XL & 400K\(\)256 & 674.80 & 0.93 & 118.64 & 0.25 & 17538 & 19.47 \\ SD-DiT-XL & 1300K\(\)256 & 740.60 & - & - & - & - & 9.01 \\
**EDT-XL***(our)** & 400K\(\)256 & 698.40 & **1.49** & **51.83** & **0.51** & **14486** & 10.48 \\ MDTv2-XL & 400K\(\)256 & 675.80 & 0.51 & 118.69 & 0.23 & 23436 & 7.70 \\
**EDT-XL****(our)** & 400K\(\)256 & 698.40 & 0.98 & **51.83** & **0.51** & **14486** & **7.52** \\   

Table 1: The comparison with existing SOTA methods on class-conditional image generation without classifier-free guidance on ImageNet 256\(\)256. We report the training speed (T-speed), inference speed (I-speed), and memory consumption (Mem.) of inference. The EDT* denotes the EDT without our proposed token relation-enhanced masking training strategy.

   Model & 
 T-speed \\ (iter/s) \\  & GFLOPs\(\) & FID\(\) & IS\(\) & sFID\(\) \\  DiT-S & **2.26** & 31.42 & 85.21 & 23.68 & 13.53 \\ MDTv2-S & 0.53 & 31.46 & **51.16** & 29.94 & 8.57 \\ EDT-S(our) & 1.63 & **13.25** & 51.84 & 29.92 & **7.86** \\   

Table 2: The comparison with existing transformer-based models on class-conditional image generation without classifier-free guidance on ImageNet 512\(\)512.

**Experiment on ImageNet 512x512** As shown in Table 2, we train DiT-S, MDTv2-S and EDT-S on ImageNet 512x512 for 60 epochs. Under the same training cost, both MDTv2-S and EDT-S achieve better FID scores than DiT-S (51.16 vs. 85.21, 51.84 vs. 85.21). In terms of training speed and inference overhead, EDT-S is 3.07 and 2.37 times faster than MDTv2-S respectively. This indicates that EDT achieves competitive image generation performance with lower resource overhead.

### Ablation Study

#### 3.3.1 Attention Modulation Matrix

**Quantitative analysis** We demonstrate the effectiveness and broad applicability of AMM across various models by comparing the FID scores between models with AMM and without AMM in Table 3. Extensive results show that the pre-trained models with AMM consistently outperform models without AMM, thereby verifying the generality and effectiveness of AMM. For instance, MDTv2-S with AMM achieves a better FID score than MDTv2-S without AMM (31.89 vs. 39.02). Using AMM enhances the FID of DiT-XL from 18.48 to 14.73. EDT-XL also has a lower FID score of 7.52 compared to EDT-XL without AMM of an FID score of 12.8.

**Qualitative analysis** We demonstrate the effectiveness of AMM by comparing the synthesis images from EDT-XL and DiT-XL with and without the AMM. As shown in Figure 6, the red boxes highlight the unrealistic regions in the images generated by EDT-XL without AMM. In the corresponding regions of the images generated by EDT-XL with AMM, the results appear more realistic. Moreover, the parrot image generated by EDT-XL without AMM is realistic and the parrot image generated by EDT-XL with AMM still remains equally realistic. This visual analysis demonstrates the effectiveness of the AMM plugin. Please refer to A.3, and A.5.2 for more analysis about AMM. While AMM is effective, there is potential for improvement. Please refer to A.6 for details regarding its limitations.

#### 3.3.2 Lightweight-design diffusion transformer

We investigate the effectiveness of the key components in our proposed diffusion transformer architecture. We denote the token information enhancement as TIE and the positional encoding supplement

   Model & W/o AMM & W AMM & Model & W/o AMM & W AMM \\  EDT-S* & 50.90 & **38.73** & EDT-S & 46.90 & **34.27** \\ EDT-B* & 33.19 & **23.19** & EDT-B & 26.30 & **19.18** \\ EDT-XL* & 14.92 & **10.48** & EDT-XL & 12.80 & **7.52** \\  DiT-S & 67.16 & **63.11** & MDTv2-S & 39.02 & **31.89** \\ DiT-XL & 18.48 & **14.73** & DiT-XL-7000k & 9.62 & **3.75** \\   

Table 3: Results on various models with (w) AMM and without (w/o) AMM. These models are trained for 400k iterations by default. We evaluate models using FID scores.

Figure 6: EDT-XL with AMM achieves more realistic visual effects. **Area A:** There are some blue stains on the panda’s arm. **Area B:** An unreasonable gray area. **Area C:** Black smoke in the red fog. **Area D:** Unrealistic eyes of the fox. **Area E:** Fish with an odd shape. The parrot image generated by EDT-XL without AMM is realistic. And the parrot image generated by EDT-XL with AMM remains equally realistic. The add of AMM does not negatively affect the original quality.

as PES in Table 4. Model A incorporates TIE without PES, with an FID score of 52.8. Model B integrates PES without TIE, with an FID score of 52.1. Model C represents EDT-S*, and utilizes both components, with an FID score of 50.9. Upon comparing Models A and C, we observed that the usage of token information enhancement leads to an improved FID score, decreasing from 52.8 to 50.9. Similarly, the comparison between Models B and C demonstrates that the addition of a positional encoding supplement also results in a better FID score, reducing from 52.1 to 50.9. The experimental results confirm the effectiveness of both components in enhancing model performance.

#### 3.3.3 Token relation-enhanced masking training strategy

We investigate the effectiveness of the token relation-enhanced masking training strategy and compare it with the training strategy used in MDT in Table 4. Model C does not employ any masking training strategy, with an FID score of 50.9 and an IS score of 31.0. Model D, a small-size EDT trained using the masking strategy of MDT, with an FID score of 49.6 and an IS score of 33.1. Model D shows only a slight improvement compared to Model C. Model E is a small-size EDT trained with the masking strategy of EDT, with an FID score of 46.9 and an IS score of 35.4. Model E achieved the best performance in terms of both FID and IS. This result suggests that the masking training strategy of EDT successfully improves performance by enhancing the learning ability of token relations.

## 4 Conclusions

In this work, we propose the Efficient Diffusion Transformer (EDT) framework, which includes a lightweight-design of diffusion transformer, a training-free Attention Modulation Matrix (AMM) inspired by human-like sketching, and the token relation-enhanced masking training strategy. Our lightweight-design reduces the number of tokens through down-sampling to lower computational costs. We redesigned down-sampling module and masking training strategy to address token information loss caused by the reduction of tokens. During inference, we introduce local attention through AMM, further enhancing image generation performance. Extensive experiments demonstrate that the EDT surpasses existing SOTA methods in both inference speed and image synthesis performance.

## 5 Related Work

**Diffusion Probabilistic Models**: Denoising diffusion probabilistic models (DDPM) , have marked a significant advancement in generative models. DDPM improves image generation by progressively reducing noise. ADM  innovates further by introducing a classifier-guided approach to refine the balance between image diversity and fidelity. Subsequent developments include a classifier-free method , which increases the flexibility of diffusion models by eliminating classifier constraints. DiT  replacing U-Net with transformer in LDM , achieving superior scalability. However, transformers-based models are computationally intensive. **Efficient Diffusion**: Various methods have been developed to enhance the efficiency of diffusion models. DDIM  redefines the diffusion process as non-Markovian, speeding up-sampling by removing dependencies on sequential time steps in DDPM . LDM  reduces computational demands by transforming high-resolution images into a latent space for diffusion, thus balancing complexity with image detail. Current research in lightweight diffusion transformers is limited but offers potential for further efficiency improvements in diffusion model technologies. For more related work, please refer to Appendix A.1.

   Model & TIE & PES &  masking training \\ strategy of MDT \\  &  masking training \\ strategy of EDT \\  &  FID\(\) \\  & 
 IS\(\) \\  \\  Baseline & ✗ & ✗ & ✗ & ✗ & 53.90 & 29.29 \\ A & ✗ & ✓ & ✗ & ✗ & 52.76 & 29.38 \\ B & ✓ & ✗ & ✗ & ✗ & 52.13 & 30.60 \\ C & ✓ & ✓ & ✗ & ✗ & **50.90** & **31.02** \\  D & ✓ & ✓ & ✓ & ✗ & 49.60 & 33.11 \\ E & ✓ & ✓ & ✗ & ✓ & **46.90** & **35.40** \\   

Table 4: The ablation study of the key components of the lightweight-design and masking training strategy of EDT. The experiment is conducted on the small-size EDT model (W/o AMM).