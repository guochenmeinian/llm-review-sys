# Step 1.**: **Deriving the general energy inequality

[MISSING_PAGE_FAIL:1]

Indeed, unlike classical games (where a mixed strategy is a probabilistic mixture of the underlying pure strategies), quantum games utilize mixed states, which represent probabilistic mixtures of quantum projectors. As a consequence, a mixed quantum state can yield payoffs that cannot be expressed as a convex combination of classical pure strategies.

In light of this, quantum learning has drawn significant attention in recent years . In a multi-agent context, the most widely used framework is the so-called _matrix multiplicative weights_ (MMW) algorithm : First introduced by Tsuda et al.  in the context of matrix and dictionary learning, MMW can be viewed as a semidefinite analogue of the standard Hedge / EXP3 methods for multi-armed bandits , and is a special case of the mirror descent family of algorithms . Specifically, in the contrete setting of two-player, zero-sum quantum games, Jain & Watrous  showed that players using the MMW algorithm can learn an \(\)-equilibrium in \((1/^{2})\) iterations - or, in terms of speed of convergence after \(T\) iterations, they converge to equilibrium at a \((1/)\) rate.

To the best of our knowledge, this result remains the tightest known bound for equilibrium learning in quantum games - and the more general class of semidefinite games . At this point, we highlight that we focus on classical computing algorithms for solving quantum games, unlike recent results  that employ quantum algorithms to solve classical games and semidefinite programs. Building on , Jain et al.  studied its continuous-time analogue - the _quantum replicator dynamics_ (QRD) - in quantum min-max games, focusing on the recurrence and volume conservation properties of the players' actual trajectory of play. Going beyond the min-max case,  examined the convergence of the dynamics of _"follow the quantum leader"_ (FTQL), a class of continuous-time dynamics that includes the QRD as a special case. The main result of  was that the only states that are asymptotically stable under the (continuous-time) dynamics of FTQL are those that satisfy a certain first-order stationarity condition known as _variational stability_. In a similar line of work, Lin et al.  studied the continuous-time QRD, and discrete-time MMW in quantum potential games, utilizing a Riemannian metric to obtain a gradient flow in the spirit of .

Our contributions in the context of previous work. All works mentioned above, in both continuous and discrete time, assume full information, i.e., players have access to their individual payoff gradients - which, among others, might imply that they have full knowledge of the game. However, this condition is rarely met in online learning environments where players only observe their in-game payoffs; this is precisely the starting point of our paper which aims to derive a convergent payoff-based, gradient-free variant of MMW algorithm for learning in quantum games.

A major roadblock in this is that standard approaches from learning in finite games fail in the quantum setup for two reasons: First and foremost, there is a _continuum_ of pure states available to every player, unlike classical finite games where there is only a _finite_ set of pure actions. Second, even after the realization of the pure states of the players, there is an inherent uncertainty and randomness due to the payoff-generating quantum process (an aspect that has no classical counterpart). To overcome this hurdle, we employ a continuous-action reformulation of quantum games, and we leverage techniques from bandit convex optimization for estimating the players' payoff gradients.

Our first contribution is a variant of MMW that only requires mixed payoff observations and achieves an \((1/)\) equilibrium convergence rate in two-player zero-sum quantum games, matching the rate of the full information MMW in . Then, to account for information-starved environments where players are only able to observe their in-game, realized payoff observable, we also develop a bandit variant of MMW which utilizes a single-point gradient estimation technique in the spirit of  and achieves an \((T^{-1/4})\) equilibrium convergence rate. Finally, we also examine the behavior of the MMW algorithm with bandit information in general \(N\)-player games, where we show that variationally stable equilibria are locally attracting with high probability.

Importantly, the above results transfer to more general games with a semidefinite structure - such as multi-agent covariance matrix optimization in signal processing, energy efficiency maximization in multi-antenna systems, etc. . While we do not provide a complete theory, we discuss a number of non-quantum applications that showcase how our results can be generalized further.

Notation. Given a (complex) Hilbert space \(\), we will use Dirac's bra-ket notation and write \(\) for an element of \(\) and \(\) for its adjoint; otherwise, when a specific basis is implied by the context, we will use the dagger notation "\(\)" to denote the Hermitian transpose \(^{}\) of \(\). We will also write \(^{d}\) for the space of \(d d\) Hermitian matrices, and \(^{d}_{t}\) for the cone of positive-semidefinite matrices in \(^{d}\). Finally, we denote by \(\|\|_{F}=[^{}]}\) the Frobenius norm of \(\) in \(^{d}\).

## 2 Problem setup and preliminaries

We begin by reviewing some basic notions from the theory of quantum games, mainly intended to set notation and terminology; for a comprehensive introduction, see . To streamline our presentation, we introduce the primitives of quantum games in a 2-player setting before treating the general case.

Quantum games.Following [20; 23], a 2-player _quantum game_ consists of the following:

1. Each player \(i\{1,2\}\) has access to a complex Hilbert space \(_{i}^{d_{i}}\) describing the set of (pure) _quantum states_ available to the player (typically a discrete register of qubits). A quantum state is an element \(_{i}\) of \(_{i}\) with unit norm, so the set of pure states is the unit sphere \(_{i}\{_{i}_{i}:\|_{i}\|_{F}=1\}\) of \(_{i}\). We will write \(_{1}_{2}\) for the space of all ensembles \(=(_{1},_{2})\) of pure states \(_{i}_{i}\) that are independently prepared by each player.
2. The rewards that players receive are based on their individual _payoff functions_\(u_{i}\), and they are derived through a _positive operator-valued measure_ (POVM) quantum measurement process. Following , this unfolds as follows: Given a _finite_ set of _measurement outcomes_\(\) that a referee can observe from the players' quantum states (e.g., measure a player-prepared qubit to be "up" or "down"), each outcome \(\) is associated to a positive semi-definite operator \(_{}\) defined on the tensor product \(_{1}_{2}\) of the players' individual state spaces. We further assume that \(_{}_{}=\) so the probability of observing \(\) at state \(\) is \(P_{}()=_{1}_{2}|_{}|_{1} _{2}\).
3. The payoff of each player is then generated by this measurement process via a _payoff observable_\(U_{i}\): specifically, the measurement \(\) is drawn from \(\) based on the players' state profile \(=(_{1},_{2})\), and each player \(i\) receives as reward the quantity \(U_{i}()\). Accordingly, the player's expected payoff at state \(\) is \(u_{i}() U_{i}_{}P_{}() \,U_{i}()\).

A _quantum game_ is then defined as a tuple \((,,u)\) with players, states, and payoff as above.

Mixed states.Apart from pure states, each player \(i\) may prepare probabilistic mixtures thereof, known as _mixed states_. These mixed states differ from mixed strategies used in classical, finite games as they do not correspond to convex combinations of their pure counterparts; instead, given a family of pure quantum states \(_{i_{i}}_{i}\) indexed by \(_{i}_{i}\), a mixed state is described by a _density matrix_ of the form

\[_{i}=_{_{i}_{i}}x_{i_{i}}| _{i_{i}}_{i_{i}}|\] (1)

where the _mixing weights_\(x_{i_{i}} 0\) of each \(_{i_{i}}\) are normalized so that \(_{i}=1\). By Born's rule, this means that the probability of observing \(\) under \(=(_{1},_{2})\) is

\[P_{}()=_{_{1}_{1}} _{_{2}_{2}}x_{1,_{1}}x_{2,_{2}}P_{ }(_{}).\] (2)

where \(_{}=_{1,_{1}}_{2,_{2}}\). Therefore, in a slight abuse of notation, the expected payoff of player \(i\) under \(\) will be \(u_{i}()=_{}x_{}u_{i}(_{})\), which, equivalently, can be written as:

\[u_{i}()=_{}_{ }x_{}u_{i}(_{})U_{i}().\] (3)

This gives a succint representation of the payoff structure of \(\) - see also Eq. (5) below.

Continuous game reformulation.In view of the above, treating a quantum game as a "tensorial" extension of a finite game can be misleading. For our purposes, it would be more suitable to treat a quantum game as a _continuous game_ where each player \(i\) controls a matrix variable \(_{i}\) drawn from the "spectraplex" defined as \(}_{i}=\{_{i}^{d_{i}}_{t}: _{i}=1\}\). In this interpretation, the players' payoff functions \(u_{i}}}_{1} }_{2}\) are _linear_ in each player's density matrix \(_{i}}_{i}\), \(i\). Since \(u_{1},u_{2}\) are linear in \(_{1}\) and \(_{2}\), the individual payoff gradients of each player will be given by

\[_{1}()_{_{1}^{}}u_{1}()_{2}()_{_{1}^{ }}u_{2}()\] (4)

so we can further write each player's payoff function as

\[u_{1}()=[_{1}_{1}()]  u_{2}()=[_{2}_ {2}()]}.\] (5)Since \(\) is compact and each \(u_{i}\) is multilinear in \(\), the players' payoff functions are automatically bounded, Lipschitz continuous and Lipschitz smooth, i.e., there exist constants \(B_{i}\), \(G_{i}\) and \(L_{i}\), \(i\), such that, for all \(,^{}\), we have:

1. Boundedness: \(|u_{i}()| B_{i}\)
2. Lipschitz continuity: \(|u_{i}()-u_{i}(^{})| G_{i}\|- ^{}\|_{F}\)
3. Lipschitz smoothness: \(\|_{i}()-_{i}(^{})\|_{F} L _{i}\|-^{}\|_{F}\)

**Nash equilibrium.** The most widely used solution concept in game theory is that of a _Nash equilibrium_ (NE). In our context, it is mixed profile \(^{*}\) from which no player has incentive to deviate, i.e., \(u_{1}(^{*}) u_{1}(_{1};_{2}^{*})\) and \(u_{2}(^{*}) u_{2}(_{1}^{*};_{2})\) for all \(_{1}_{1}\), \(_{2}_{2}\). Since \(_{i}\) is convex and \(u_{i}\) linear in \(_{i}\), the existence of Nash equilibria follows from the Debreu's theorem .

Zero-sum quantum games. In the case where \(u_{1}=-u_{2}\), and setting \(_{1}_{2}\), the Nash equilibria of \(\) are the saddle points of \(\), i.e., the solutions of the minimax problem

\[_{_{1}_{1}}_{_{2}_{2 }}(_{1},_{2})=_{_{2}_{2}}_{_{1}_{1}}(_{1}, _{2})\] (6)

By Sion's minimax theorem , the set of Nash equilibria is nonempty. Then, given a Nash equilibrium \(^{*}\), we define the _duality gap_ of \(=(_{1},_{2})\) as

\[_{}()(_{1}^{*}, _{2})-(_{1},_{2}^{*})\] (7)

so \(_{}() 0\) with equality if and only if \(\) is itself a Nash equilibrium. In particular, \(\) is an \(\)-Nash equilibrium of \(\) if and only if \(_{}()\).

Other semidefinite games. In addition to quantum games, our framework can also be used for learning in other classes of games with a semidefinite structure as per [26; 45]. As an example, consider the problem of covariance matrix optimization in vector Gaussian multiple-access channels [9; 43; 57; 62]. In this case, there is a finite set of players indexed by \(i=\{1,,N\}\); each player \(i\) picks a unit-trace semidefinite matrix \(_{i}_{i}\) and their payoff is given by the Shannon-Telatar capacity formula , viz.

\[u_{i}(_{1},,_{N})=+_{j }_{j}_{j}_{j}^{}\] (8)

where each \(_{i}\) is a player-specific gain matrix . Even though \(u_{i}\) is no longer multilinear in \(\), the algorithms we derive later in the paper can be applied to this setting essentially verbatim.

## 3 The matrix multiplicative weights algorithm

Throughout the sequel, we will focus on equilibrium learning in quantum - and semidefinite - games. In the context of two-player, zero-sum quantum games, the state-of-the-art method is based on the so-called _matrix multiplicative weights_ (MMW) algorithm [7; 27; 29; 59] which updates as

\[_{i,t+1}=_{i,t}+_{t}_{i}(_{t}) _{i,t}=_{i,t})}{( _{i,t})}\] (MMW)

In the above, \((a)\)\(_{t}=(_{1,t},_{2,t})\) denotes the players' density matrix profile at each stage \(t=1,2,\) of the process; \((b)\)\(_{i}(_{t})\) is the payoff gradient of player \(i\) under \(_{t}\); \((c)\)\(_{t}\) is an auxiliary state matrix that aggregates gradient steps over time; and \((d)\)\(_{t}>0\), \(t=1,2,\), is a learning rate (or step-size) parameter that can be freely tuned by the players.

Importantly, as stated, (MMW) requires _full information_ at the player end: specifically, at each stage \(t=1,2,\) of the process, each player \(i\) must receive their individual payoff gradient \(_{i}(_{t})\) in order to perform the gradient update step in (MMW). Under this assumption, Jain & Watrous  showed that the induced empirical frequency of play

\[}_{T}=_{t=1}^{T}_{t}\] (9)

converges to equilibrium at a rate of \((1/)\) as per the formal result below:

**Theorem 1** (Jain & Watrous ).: _Suppose that each player of a \(2\)-player zero-sum game \(\) follows (MMW) for \(T\) epochs with learning rate \(=G^{-1}\) where \(H=(d_{1}d_{2})\). Then the players' empirical frequency of play enjoys the bound_

\[_{}(}_{T}) G\] (10)

_In particular, if (MMW) is run for \(T=(1/^{2})\) iterations, \(}_{T}\) will be an \(\)-Nash equilibrium of \(\)._

To the best of our knowledge, this guarantee of Jain & Watrous  remains the tightest known bound for Nash equilibrium learning in \(2\)-player zero-sum quantum games. At the same time, Theorem 1 hinges on the players having perfect access to their individual gradients - which, among others, might entail full knowledge of the game, observing the other player's density matrix, etc. Our goal in the sequel will be to relax precisely this assumption and develop a payoff-based variant of (MMW) that can be employed without stringent information and observability requirements as above.

## 4 Matrix learning without matrix feedback

In an online learning framework, it is more realistic to assume that players observe only the _outcome_ of their actions - i.e., their individual payoffs. In this information-starved, payoff-based setting, our main goal will be to employ a _minimal-information matrix multiplicative weights_ (3MW) algorithm that updates as

\[_{i,t+1}=_{i,t}+_{t}}_{i,t} _{i,t}=_{i,t})}{ (_{i,t})}\] (3MW)

where \(}_{i,t}\) is some payoff-based estimate of the payoff gradient \(_{i}(_{t})\) of player \(i\) at \(_{t}\), and all other quantities are defined as per (MMW). In this regard, the main challenge that arises is how to reconstruct each player's payoff gradient matrices when they are not accessible via an oracle.

### The classical approach: Importance weighted estimators.

In the context of classical, finite games and multi-armed bandits, a standard approach for reconstructing \(}_{i,t}\) is via the so-called _importance weighted estimator_ (IWE) . To state it in the context of finite games, assume that each player has at their disposal a finite set of _pure strategies_\(_{i}_{i}\), and if each player plays \(_{i}_{i}\), then, in obvious notation, their individual payoff will be \(_{i}=u_{i}(_{i};_{-i})\). Then, if each player is using a mixed strategy \(x_{i}(_{i})\) to draw their chosen action \(_{i}\), the _importance weighted estimator_ (IWE) for the payoff of the (possibly unplayed) action \(_{i}_{i}\) of player \(i\) is defined as

\[_{i_{i}}=\{_{i}=_{i }\}}{x_{i_{i}}}u_{i}(_{i};_{-i})_{i}$}\] (IWE)

with the assumption that \(x_{i}\) has full support, i.e., each action \(_{i}_{i}\) has strictly positive probability \(x_{i_{i}}\) of being chosen by the \(i\)-th player.1

This approach has proven extremely fruitful in the context of multi-armed bandits and finite games where (IWE) is an essential ingredient of the optimal algorithms for each context . However, in our case, there are two insurmountable difficulties in extending (IWE) to a quantum context: First and foremost, the quantum regime is characterized by a _continuum_ of pure states with highly correlated payoffs (in the sense that quantum states that are close in the Bloch sphere will have highly correlated POVM payoff observables); this comes in stark contrast to the classical regime of finite normal-form games, where players only have to contend with a finite number of actions (with no prior payoff correlations between them). Secondly, even after the realization of the pure states of the players, there is an inherent uncertainty and randomness due to the quantum measurement process that is involved in the payoff-generating process; as such, the players' payoffs are also affected by an exogenous source of randomness which is altogether absent from (IWE).

Our approach to tackle these issues will be to exploit the reformulation of a quantum game as a continuous game with multilinear payoffs over the spectraplex (or, rather, a product thereof), and use ideas from bandit convex optimization - in the spirit of  - to estimate the players' payoff gradients with minimal, scalar information requirements.

### Gradient estimation via finite-difference quotients on the spectraplex.

To provide some intuition for the analysis to come, consider first a single-variable smooth function \(f\) and a point \(x\). Then, for error tolerance \(>0\), a two-point estimate of the derivative of \(f\) at \(x\) is given by the expression

\[_{x}=\] (11)

Going to higher dimensions, letting \(f:^{d}\) be a smooth function, \(\{e_{1},,e_{d}\}\) be the standard basis of \(^{d}\) and \(s\) drawn from \(\{e_{1},,e_{d}\}\) uniformly at random, the estimator

\[_{x}=f(x+ s)-f(x- s)s\] (12)

is a \(()\)-approximation of the gradient, i.e., \(\|_{s}[_{x}]- f(x)\|_{F}=()\). This idea is the basis of the Kiefer-Wolfowitzs stochastic approximation scheme  and will be the backbone of our work.

Now, to employ this type of estimator for a function over the set of density matrices \(}\) in \(^{d}\), we need to ensure two things: _(i)_ the feasibility of the _sampling direction_, and _(ii)_ the feasibility of the _evaluation point_. The first caveat is due to the fact that the set of the density matrices forms a lower dimensional manifold in the set of Hermitian operators, and therefore, not all directions from a base of \(^{d}\) are feasible. The second one is due to the fact that \(}\) is bounded, thus, even if the sampling direction is feasible, the evaluation point can lie outside the set \(}\). We proceed to ensure all this in a series of concrete steps below.

Sampling Directions.We begin with the issue of defining a proper sampling set for the estimator's finite-difference directions. To that end, we will first construct an orthonormal basis of the tangent hull \(}=\{^{d}:=0\}\) of \(}\), i.e., the subspace of traceless matrices of \(^{d}\). Note that if \(}\) then for any \(^{d}\) it holds \((a)\)\(+^{d}\), and \((b)\)\([+]=[]\).

Denoting by \(_{k}^{d}\) the matrix with \(1\) in the \((k,)\)-position and \(0\)'s everywhere else, it is easy to see that the set \(\{_{jj}\}_{j=1}^{d}\{_{k}\}_{k<},\{ }_{k}\}_{k<}}\) is an orthonormal basis of \(^{d}\), where

\[_{k}=}_{k}+} _{ k}}_{k}=}_{k}-}_{ k}\] (13)

for \(1 k< d\), where \(i\) is the imaginary unit with \(i^{2}=-1\). The next proposition provides a basis for the subspace \(}\), whose proof lies in the appendix.

**Proposition 1**.: _Let \(_{j}\) be defined as \(_{j}=}_{11}++_{jj}-j_{j+1,j+1}\) for \(j=1,,d-1\). Then, the set \(=\{_{j}\}_{j=1}^{d-1},\{_{k}\}_{k< },\{}_{k}\}_{k<}}\) is an orthonormal basis of \(}\)._

In the sequel, we will use this basis as an orthonormal sampler from which to pick the finite-difference directions for the estimation of \(\).

Feasibility Adjustment.After establishing an orthonormal basis for \(}\) as per Proposition 1, we readily get that for any \(}\), any \(^{}\{_{j}\}_{j=1}^{d -1},\{_{k}\}_{k<},\{}_{k}\}_{k< }}\) and \(>0\), the point \(+\,\) belongs to \(}\). However, depending on the value of the exploration parameter \(\) and the distance of \(\) from the boundary of \(}\), the point \(+}^{d}\) may fail to lie in \(}\) due to violation of the positive-semidefinite condition. On that account, we now treat the latter restriction, i.e., the feasibility of the _evaluation point_.

To tackle this, the idea is to transfer the point \(\) toward the interior of \(}\) and move along the sampled direction from there. For this, we need to find a reference point \((})\) and a "safety net" \(r>0\) such that \(+r}\) for any \(^{}\). Then, for \((0,r)\), the point

\[^{()}+(-)\] (14)

lies in \((})\), and moving along \(^{}\), the point \(^{()}+\,=(1-)+(+r)\) remains in \(}\) as a convex combination of two elements in \(}\). The following proposition provides an exact expression for \(\) and \(r\), which we will use next to guarantee the feasibility of the sampled iterates.

**Proposition 2**.: _Let \(=_{j=1}^{d}_{jj}\). Then, for \(r=\{},}{d}\}\), it holds that \(+r}\) for any direction \(^{}\)._Bandit learning in zero-sum quantum games

With all these in hand, we are now ready to proceed to the presentation of the MMW with limited feedback information. To streamline our presentation, before delving into the more difficult "bandit feedback" case - where each player \(i\) only observes the realized payoff observable \(U_{i}()\) - we begin with the simpler case where players observe their mixed payoffs \(u_{i}\) at a given profile \(}\).

### Learning with mixed payoff observations.

Our main idea to exploit the observation of mixed payoffs and the finite-difference sampling to the fullest will be to introduce a "coordination phase" where players take a sampling step before updating their state variables and continue playing. In more detail, we will take an approach similar to Bervoets et al.  and assume that players alternate between an "exploration" and an "exploitation" update that allows them to sample the landscape of \(\) efficiently at each iteration. Concretely, writing \(_{t}\) and \(_{t}\) for the players' state profile and sampling radius \(_{t}\) at stage \(t=1,2,\), the sequence of events that we envision proceeds as follows:

**Step 1.**: Draw a sampling direction \(_{t,t}_{i}\) and \(s_{i,t}\{ 1\}\) uniformly at random.
**Step 2.**: \((a)\) Play \(_{i,t}^{()}+s_{i,t}\,_{t}\,_{i,t}\) and observe \(u_{i}(_{t}^{()}+s_{t}_{t}_{t})\).

\((b)\) Play \(_{i,t}^{()}-s_{i,t}\,_{t}\,_{i,t}\) and observe \(u_{i}(_{t}^{()}-s_{t}_{t}_{t})\).
**Step 3.**: Approximate \(_{i}(_{t})\) via the _two-point estimator_ (2PE):

\[}_{i,t}}{2_{t}}u_{i}(_{t}^{()}+s_{t}_{t}_{t})-u_{i}(_{t}^{()}-s _{t}_{t}_{t})\ s_{i,t}_{i,t}\] (2PE)

where \(D_{i}=d_{i}^{2}-1\) is the dimension of \(^{d_{i}}\), and \(D_{i}D_{i}\).

The main guarantee of the resulting (3MW) + (2PE) algorithm may then be stated as follows:

**Theorem 2**.: _Suppose that each player of a \(2\)-player zero-sum game \(\) follows_ (3MW) _for \(T\) epochs with learning rate \(\), sampling radius \(\), and gradient estimates provided by_ (2PE)_. Then the players' empirical frequency of play enjoys the duality gap guarantee_

\[_{}(}_{T} )+8D^{2}G^{2}+16DL\] (15)

_where \(H=(d_{1}d_{2})\). In particular, for \(=(DG)^{-1}\) and \(=(G/L)\), the players enjoy the equilibrium convergence guarantee_

\[_{}(}_{T} ) 8DG.\] (16)

Compared to Theorem 1, the convergence rate (16) of Theorem 2 is quite significant because it only differs by a factor which is linear in the dimension of the ambient space and otherwise maintains the same \(()\) dependence on the algorithm's runtime. In this regard, Theorem 2 shows that the "explore-exploit" sampler underlying (2PE) is essentially as powerful as the full information framework of Jain & Watrous  - and this, despite the fact that players no longer require access to the gradient matrix \(\) of \(\). This echoes a range of previous findings in stochastic convex optimization for the efficiency of two-point samplers , a similarity we find particularly surprising given the stark differences between the two settings - non-commutativity, min-max versus min-min landscape. The key ingredients for the equilibrium convergence rate of Theorem 2 are the two technical results below. The first is a feedback-agnostic "energy inequality" which is tied to the update structure of (MMW) and is stated in terms of the quantum relative entropy function

\[D(,)=\!(- )\] (17)

for \(,}\) with \( 0\). Concretely, we have the following estimate.

**Lemma 1**.: _Fix some \(}\), and let \(_{t},_{t+1}\) be two successive iterates of_ (3MW)_, without any assumptions for the input sequence \(}_{t}\). We then have_

\[D(,_{t+1})\, D(,_{t})+_{t} \!}_{t}(_{t}-) +^{2}}{2}\|}_{t}\|_{F}^{2}.\] (18)

The proof of Lemma 1 follows established techniques in the theory of (MMW), so we defer a detailed discussion to the appendix. The second result that we will need is tailored to the estimator (2PE) and provides a tight estimate of its moments conditioned on the history \(_{t}=(_{1},,_{t})\) of \(_{t}\).

**Proposition 3**.: _The estimator (2PE) enjoys the conditional bounds_

\[(i)\ \|[}_{t}\,|\,_{t}]-( _{t})\|_{F} 4DL_{t}(ii)\ \ \|}_{t}\|_{F}^{2}\,|\,_{t}  16D^{2}G^{2}\] (19)

The defining element in Proposition 3 is that even though the estimator (2PE) is biased, its second moment is bounded as \((1)\). This is ultimately due to the multilinearity of the players' payoff functions and plays a pivotal role in showing that the duality gap of \(}_{t}\) under (3MW) is of the same order as under (MMW), because the bias can be controlled with affecting the variance of the estimator. We provide a detailed proof of Lemma 1, Proposition 3, and Theorem 2 in the appendix.

### Learning with bandit feedback

Despite its strong convergence guarantees, a major limiting factor in the applicability of Theorem 2 is that, in many cases, the game's players may only be able to observe their realized payoff observables \(U_{i}()\), and their mixed payoffs \(u_{i}()\) could be completely inaccessible. In particular, as we described in Section 2, each outcome \(\) of the POVM occurs with probability \(P_{}(_{t})\) under the strategy profile \(_{t}\). Accordingly, if this is the only information available to the players, they will need to estimate their individual payoff gradients through the single observation of the (random) scalar \(U_{i}(_{t})\). In view of this, and inspired by previous works on payoff-based learning and zeroth-order optimization , we will consider the single-point stochastic approximation approach of  which unfolds as follows:

**Step 1.**: Each player draws a sampling direction \(_{i,t}_{i}^{}\) uniformly at random.
**Step 2.**: Each player plays \(_{i,t}^{()}+_{t}\,_{i,t}\).
**Step 3.**: Each player receives \(U_{i}(_{t})\).
**Step 4.**: Each player approximates \(_{i}(_{t})\) via the the _one-point estimator_ (1PE):

\[}_{i,t}}{_{t}}U_{i}(_{t})\, _{i,t}\] (1PE)

In this case, the players' gradient estimates may be bounded as follows:

**Proposition 4**.: _The estimator (1PE) enjoys the conditional bounds_

\[(i)\ \ \|[}_{t}\,|\,_{t}]-( _{t})\|_{F} 4DL_{t}(ii)\ \ [\|}_{t}\|_{F}^{2}\,|\,_{t}] 4D ^{2}B^{2}/_{t}^{2}.\] (20)

The crucial difference between Propositions 3 and 4 is that the former leads to a gradient estimator with \((1)\) variance and magnitude, whereas the magnitude of the latter is inversely proportional to \(_{t}\); however, since \(_{t}\) in turn controls the _bias_ of the gradient estimator, we must now resolve a bias-variance dilemma, which was absent in the case of (2PE). This leads to the following variant of Theorem 2 with bandit, realization-based feedback:

**Theorem 3**.: _Suppose that each player of a \(2\)-player zero-sum game \(\) follows (3MW) for \(T\) epochs with learning rate \(\), sampling radius \(\), and gradient estimates provided by (1PE). Then the players' empirical frequency of play enjoys the duality gap guarantee_

\[_{}(}_{T}) +B^{2}}{^{2}}+16DL\] (21)

_where \(H=(d_{1}d_{2})\). In particular, for \(=^{3/4}}\) and \(=^{1/4}}\), the players enjoy the equilibrium convergence guarantee:_

\[_{}(}_{T}) \,8H^{1/4}D}{T^{1/4}}.\] (22)An important observation here is that the players' equilibrium convergence rate under \((3)+(1)\) no longer matches the convergence rate of the vanilla MMW algorithm (Theorem 1). The reason for this is the bias-variance trade-off in the estimator (1PE), and is reminiscent of the drop in the rate of regret minimization from \((T^{1/2})\) to \((T^{2/3})\) under (IWE) with bandit feedback and explicit exploration in finite games. A kernel-based approach in the spirit of Bubeck et al.  could possibly be used to fill the \((T^{1/4})\) gap between Theorems 1 and 3, but this would come at the cost of a possibly catastrophic dependence on the dimension (which is already quadratic in our setting). This consideration is beyond the scope of our work, but it would constitute an important future direction.

## 6 Bandit learning in \(N\)-player quantum games

We conclude our paper with an examination of the behavior of the MMW algorithm in general, \(N\)-player quantum games. Here, a major difficulty that arises is that, in stark contrast to the min-max case, the set of the game's equilibria can be disconnected, so any convergence result will have to be, by necessity, local. In addition, because general \(N\)-games do not have the amenable profile of a bilinear min-max problem - they are multilinear, multi-objective problems - it will not be possible to obtain any convergence guarantees for the game's empirical frequency of play (since there is no convex structure to exploit). Instead, we will have to focus squarely on the induced trajectory of play, which carries with it a fair share of complications.

Inspired by the very recent work of , we will not constrain our focus to a specific class of _games_, but to a specific class of _equilibria_. In particular, we will consider the behavior of MMW-based learning with respect to Nash equilibria \(^{*}}\) that satisfy the _variational stability_ condition

\[[()(-^{*})]<0\ }\{^{*}\}.\] (VS)

This condition can be traced back to , and can be seen as a game-theoretic analogue of first-order stationarity in the context of continuous optimization, or as an equilibrium refinement in the spirit of the seminal concept of _evolutionary stability_ in population games [39; 40].2 Importantly, as was shown in , variationally stable equilibria are the only equilibria that are asymptotically stable under the _continuous-time_ dynamics of the "follow the regularized leader" (FTRL) class of learning policies, so it stands to reason to ask whether they enjoy a similar convergence landscape in the context of bona fide, discrete-time learning with minimal, payoff-based feedback.

Our final result provides an unambiguously positive answer to this question:3

**Theorem 4**.: _Fix some tolerance level \((0,1)\) and suppose that the players of an \(N\)-player quantum game follow \((3)\) with bandit, realization-based feedback, and surrogate gradients provided by the estimator \((1)\) with step-size and sampling radius parameters such that_

\[(i)\ _{t=1}^{}_{t}=,(ii)\ _{t=1}^{} _{t}_{t}<,(ii)\ _{t=1}^{}_{t}^{2}/_{t}^{2}<.\] (23)

_If \(^{*}\) is variationally stable, there exists a neighborhood \(\) of \(^{*}\) such that_

\[(_{t}_{t}=^{*}) 1- \ _{1}.\] (24)

It is worth noting that the last-iterate convergence guarantee of Theorem 4 is considerably stronger than the time-averaged variants of Theorems 1-3, and we are not aware of any comparable convergence guarantee for general quantum games. [Trivially, last-iterate convergence implies time-averaged convergence, but the converse, of course, may fail to hold] As such, especially in cases that require to track the trajectory of the system or the players' day-to-day rewards, Theorem 4 provides an important guarantee for the realized sequence of events.

On the other hand, in contrast to Theorem 4, it should be noted that the guarantees of Theorems 1-3 are global. Given that general quantum games may in general possess a large number of disjoint Nash equilibria, this transition from global to local convergence guarantees seems unavoidable. It is, however, an open question whether (VS) could be exploited further in order to deduce the rate of convergence to such equilibria; we leave this as a direction for future research.

## 7 Numerical Experiments

In this last section, we provide numerical simulations to validate and explore the performance of (MMW) with payoff-based feedback. Additional experiments can be found in Appendix E.

Game setup.Our testbed is a two-player zero-sum quantum game, which is the quantum analogue of a \(2 2\) min-max game with actions \(\{_{1},_{2}\}\) and \(\{_{1},_{2}\}\), and payoff matrix

\[P=(4,-4)&(2,-2)\\ (-4,4)&(-2,2)\] (25)

In the quantum regime, the payoff information of the quantum game is encoded in the Hermitian matrices \(_{1}=(4,2,-4,-2)\), and \(_{2}=-_{1}\) as per Eq. (3) in Section 2. By elementary considerations, the action profile \((_{1},_{2})\) is a strict Nash equilibrium of the classical zero-sum game, which corresponds to the pure quantum state with density matrix profile \(^{*}=(_{1}^{*},_{2}^{*})\) where \(_{1}^{*}=e_{1} e_{1}\) and \(_{2}^{*}=e_{2} e_{2}\) in the standard basis in which \(_{1}\) and \(_{2}\) are diagonal.

Convergence speed analysis.In Fig. 1, we evaluate the convergence properties of (3MW) using the estimators (2PE) and (1PE), and compare it with the full information variant (MMW). For each method, we perform 10 different runs, with \(T=10^{5}\) steps each, and compute the mean value of the duality gap as a function of the iteration \(t=1,2,,T\). The solid lines correspond to the mean values of the duality gap of each method, and the shaded regions enclose the area of \( 1\) (sample) standard deviation among the 10 different runs. Note that the red line, which corresponds to the full information (MMW), does not have a shaded region, since there is no randomness in the algorithm. All the runs for the three different methods were initialized for \(=0\) and we used \(=10^{-2}\) for all methods. In particular, for (3MW) with gradient estimates given by (2PE) estimator, we used a sampling radius \(=10^{-2}\), and for (3MW) with (1PE) estimator, we used \(=10^{-1}\) (in tune with our theoretical results which suggest the use of a tighter sampling radius when mixed payoff information is available to the players).

Figure 1 has several important take-aways. First and foremost, as is to be expected, the payoff-based methods lag behind the full-information variant of (MMW); however, what is particularly surprising is that the drop in performance is singularly mild. As we see in the second plot in Fig. 1, the various algorithms achieved a rate of convergence closer to \((1/T)\), which is significantly faster than \((1/)\) and/or \((1/T^{1/4})\). This suggests that, in practice, the bandit variants of (MMW) may yield excellent performance benefits, despite the high degree of uncertainty incurred by the complete lack of information on the game being played.

Figure 1: Performance evaluation of the (3MW) with the (2PE) and (1PE) estimators and comparison with the full information (MMW). The solid lines correspond to the mean values of the duality gap of each method, and the shaded regions enclose the area of \( 1\) (sample) standard deviation among the different runs.