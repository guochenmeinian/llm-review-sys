# Diversifying Spatial-Temporal Perception

for Video Domain Generalization

 Kun-Yu Lin\({}^{1}\)1 &Jia-Run Du\({}^{1}\)1 &Yipeng Gao\({}^{1}\) &Jiaming Zhou\({}^{1}\)

Wei-Shi Zheng\({}^{1,2}\)2

\({}^{1}\)School of Computer Science and Engineering, Sun Yat-sen University, China

\({}^{2}\)Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China

{linky5,dujr6,gaoyp23,zhoujm55}@mail2.sysu.edu.cn

wszheng@ieee.org

Equal contributionsCorresponding author

###### Abstract

Video domain generalization aims to learn generalizable video classification models for unseen target domains by training in a source domain. A critical challenge of video domain generalization is to defend against the heavy reliance on domain-specific cues extracted from the source domain when recognizing target videos. To this end, we propose to perceive diverse spatial-temporal cues in videos, aiming to discover potential domain-invariant cues in addition to domain-specific cues. We contribute a novel model named Spatial-Temporal Diversification Network (STDN), which improves the diversity from both space and time dimensions of video data. First, our STDN proposes to discover various types of spatial cues within individual frames by spatial grouping. Then, our STDN proposes to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales by spatial-temporal relation modeling. Extensive experiments on three benchmarks of different types demonstrate the effectiveness and versatility of our approach.

## 1 Introduction

Recently, advanced deep network architectures have achieved competitive results for video classification [1; 2; 3; 4; 5; 6; 7; 8], leading to wide applications in surveillance systems, sport analysis, health monitoring, etc [9; 10; 11]. However, existing video classification models rely on the i.i.d. assumption, _i.e._, training and test videos are independently and identically distributed. This assumption would be easily violated, since models often face unfamiliar scenarios in real-world applications. For example, a housework robot will work in a new house, and a surveillance system will encounter illumination change caused by camera viewpoint or weather [12; 13; 14]. Holding such an assumption, the performance of video classification models would drop significantly in unfamiliar test scenarios.

To alleviate the above problem, our work studies the video domain generalization task, which aims to learn a video classification model that is generalizable in _unseen_ target domains by training in a source domain [15; 16]. In this task, videos from the source and target domains follow different distributions though with an identical label space. For example, as shown in Figure 1, humans in the source domain play basketball shooting on indoor basketball courts while those in the target domain play outdoors. Different from the video domain adaptation task with available unlabeled target videos for training [17; 18; 19; 20], video domain generalization can only access the source domain during training, which is much more challenging but more practical.

A critical challenge of video domain generalization is to defend against the reliance on domain-specific cues in the source domain that are correlated with class labels. For example, as shown in Figure 1, video classification models prefer to leverage the backboard for recognizing the class "shoot ball" in the source domain, since the static backboard provides a clearer cue compared with the blurred basketball in motion (static patterns are usually easy-to-fit [21; 22; 23; 24]). However, in the target domain, the backboard is occluded due to the viewpoint, thus recognizing the class by the backboard would cause recognition errors. It is challenging to address this problem in lack of any knowledge of the target domain. Typically, existing works explore invariance across domains for learning generalizable video features [25; 15; 16]. For example, Yao et al. propose to learn generalizable temporal features by encoding information of local features into global features, assuming that local temporal features are more invariant across domains compared with global ones .

In this work, we propose a novel approach for video domain generalization, which explore spatial-temporal diversity in videos. Our approach aims to perceive diverse class-correlated cues from abundant video contents, and thus we would leverage not only easy-to-fit domain-specific cues but also other _potential_ domain-invariant cues for recognizing videos in target domains (_e.g._, we expect that our model can capture not only static backboards but also dynamic basketballs in the source domain). As a result, our approach can alleviate the overfitting of domain-specific cues in the source domain and generalize better in target domains by leveraging those potential domain-invariant cues. Specifically, we propose to explore the diversity from both space and time dimensions of video data, leading to a novel architecture named Spatial-Temporal Diversification Network (STDN). Our contributions are summarized as follows:

* We propose Spatial Grouping Module to discover various groups of spatial cues within individual frames by embedding a clustering-like process, enriching the diversity from a spatial modeling perspective.
* We propose Spatial-Temporal Relation Module to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales, enriching the diversity from a spatial-temporal relation modeling perspective.
* Extensive experiments are conducted on three benchmarks of different types, including two newly designed benchmarks, and the results demonstrate the effectiveness and versatility of our proposed method.

## 2 Related Works

**Video Classification** aims to recognize actions or events in videos. Recently, many advanced deep learning architectures have been proposed for video classification. 3D CNNs extend the 2D convolution to 3D convolution for video feature learning [1; 2; 3; 26; 27; 28; 29; 30]. Another

Figure 1: Video classification models suffer from the misguidance of domain-specific cues when generalizing to unseen domains. As shown in the figure, in the source domain, the static backboard provides a clearer cue compared with the blurred basketball in motion, thus prevailing video classification models are prone to recognize the class “Shoot Ball” by the backboard. However, the backboard is invisible in the target domain due to viewpoint change, and thus previous models learned in the source domain would make mistakes in recognition. Videos in the figure are from the UCF-HMDB benchmark. Best viewed in color.

type of models first applies 2D convolution for frame-level spatial modeling and then conducts temporal modeling based on frame features [5; 6; 31; 32; 33]. Some works propose to couple explicit shifts along the time dimension for efficient temporal modeling [7; 34; 35]. More recently, pioneer works have made efforts in adapting Vision Transformer  for video classification [37; 38; 39; 40; 41; 42; 43; 44; 45]. Although these advanced architectures achieve appealing performance, they usually assume an identical test distribution to the training one, which is not practical in real-world applications.

**Video Domain Generalization** aims to train video classification models in a source domain for generalizing to _unseen_ target domains. With target videos inaccessible during training, existing works usually assume different types of invariance across domains to defend against the reliance on domain-specific cues [25; 15; 16]. For example, Yao et al. propose to learn generalizable temporal features according to an assumption from empirical findings, _i.e._, local temporal features are more invariant across domains compared with the global ones ; Planamente et al. propose to constrain a consistency across visual and audio modalities by relative norm alignment for addressing domain generalization of egocentric action recognition . In this work, we propose to perceive diverse class-correlated spatial-temporal cues in source videos, which alleviates the misguidance of domain-specific cues in a way that is orthogonal to previous works.

**Video Domain Adaptation** aims to learn transferable video classification models for a label-free target domain by transferring knowledge from a label-sufficient source domain [17; 18]. Different from video domain generalization, video domain adaptation is oriented to a specific _seen_ unlabeled target domain. Typically, existing works learn invariance between labeled source videos and unlabeled target videos to tackle video domain adaptation. A class of representative works propose to learn domain-invariant temporal features by designing temporal modeling modules [18; 19; 46; 47]. In addition, Choi et al. [20; 48] propose self-supervised methods adaptive to video data. Furthermore, multi-modal works explore information interaction between different modalities (_e.g._, RGB, Flow, Audio) for domain-invariant feature learning [49; 50; 51; 52; 53].

**General Domain Generalization**, also known as out-of-distribution generalization, studies learning models generalizable to out-of-distribution data for the image classification task. In recent years, a plethora of methods have been proposed to address domain generalization [25; 54; 55; 56]. Prevailing methods are mainly based on feature alignment [57; 58; 59], domain adversarial learning [60; 61], invariant risk minimization [62; 63; 64; 65], meta learning [66; 67; 68; 69; 70], data augmentation [71; 72; 73; 74; 75], etc. In addition, ensemble learning is an effective way to learn generalizable models [76; 77; 78]. And recently, Zhu et al. develop a theory showing that ensemble learning can provably improve test accuracy by discovering the "multi-view" structure of data , which partially inspires our approach. Among architecture-based methods [80; 81], Meng et al. propose to redesign attention modules for learning diverse task-related features . Different from existing general domain generalization methods, we propose a domain generalization method specific to video classification, which explores diverse class-correlated information in intrinsic space and time dimensions of video data. There are some works that study the identification of out-of-distribution data of different categories from training data [82; 83; 84; 85; 86; 87; 88], but this topic is not within the scope of our work.

## 3 Spatial-Temporal Diversification Network

In this section, we illustrate our proposed Spatial-Temporal Diversification Network (STDN) in detail, which perceives diverse class-correlated spatial-temporal cues from video contents for generalization in unseen target domains.

### Problem Formulation

In video domain generalization, a set of labeled videos \(=\{(x,y)\}\) from a source domain are given for training, where \(x\) and \(y\) denote a source video and its corresponding class label. Given only the source video set, the goal of video domain generalization is to learn a video classification model that is generalizable in _unseen_ target domains. The source and target domains follow different but related distributions with the same label space \(=\{0,1,,C-1\}\), where \(C\) denotes the number of video classes. Following the standard video domain generalization setting , each video is evenly divided into \(N\) segments, and one frame is sampled from each segment as the model input during training and testing, _i.e._, \(x=\{x_{1},x_{2},,x_{N}\}\) and \(x_{n}\) denotes the \(n\)-th sampled frame from the \(n\)-th segment.

### Model Overview

Aiming at generalization in unseen target domains, our idea is to perceive rich and diverse class-correlated cues in the source domain. In this way, our model would leverage not only easy-to-fit domain-specific cues but also other potential domain-invariant cues for recognizing videos in the target domain, alleviating the misguidance of domain-specific cues. Considering the intrinsic space and time dimensions of video data, we propose to explore the diversity in both spatial and temporal modeling. An overview of our proposed STDN is shown in Figure 2. Firstly, given the video \(x\), our STDN takes \(N\) sampled frames as input and separately extracts \(N\) spatial feature maps \(\{z_{1},z_{2},...,z_{N}\}\) by the backbone (_e.g._, ResNet ), where \(z_{n}^{H W D}\) denotes the feature map of the \(n\)-th frame, \(D\) denotes the feature dimension and \(H W\) denotes the size of feature maps. Then, we extract spatial cues of \(K\) types (groups) from each spatial feature map by our proposed Spatial Grouping Module, aiming to enrich the spatial diversity. In the Spatial Grouping Module, two entropy-based losses are introduced to enhance the distinction between different spatial cues. On top of the Spatial Grouping Module, we propose to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales by our proposed Spatial-Temporal Relation Module. The learning of the Spatial-Temporal Relation Module is guided by a relation discrimination loss, which ensures the diversity of the extracted spatial-temporal relation features. Finally, diverse spatial-temporal features are aggregated for video domain generalization.

### Spatial Grouping Module

Our proposed Spatial Grouping Module aims to discover diverse class-correlated spatial cues from abundant contents of individual frames, which enriches the diversity from a spatial modeling perspective for video domain generalization. Our Spatial Grouping Module extracts various spatial cues of different types by partitioning features from different spatial positions into several groups within individual frames. In this way, our Spatial Grouping Module discovers more diverse spatial cues, compared with prevailing approaches that extract an integrated feature for each frame (_e.g._, by average pooling).

As shown in Figure 3 (a), given the spatial feature map \(z_{n}^{H W D}\) of the \(n\)-th frame, our proposed Spatial Grouping Module learns to extract \(K\) spatial cues by aggregating the \(HW\) spatial features. Specifically, the proposed spatial grouping process is conducted based on \(K\) learnable anchor features \(\{a_{n,1},a_{n,2},,a_{n,K}\}\), where \(a_{n,k}^{D}\) denotes the anchor feature of the \(k\)-th spatial group for the \(n\)-th frame. Then, we calculate the probability of assigning a spatial feature to each spatial group, which is formulated as follows:

\[p_{n,i,k}=(z_{n,i},a_{n,k})/ )}{_{j=1}^{K}(-(z_{n,i},a_{n,j})/ )},\] (1)

Figure 2: An overview of our proposed Spatial-Temporal Diversification Network (STDN). We use a video of \(N=5\) segments with \(K=4\) spatial groups for example. After backbone feature extraction, our STDN extracts spatial cues of \(K\) types for each frame by the Spatial Grouping Module, enriching the diversity in spatial modeling. Then, our STDN explicitly models spatial-temporal dependencies at multiple space-time scales, enriching the diversity in spatial-temporal relation modeling. Best viewed in color.

where \(z_{n,i}^{D}\) denotes the \(i\)-th spatial feature in the feature map \(z_{n}\) (\(i[1,2,,HW]\)), \((,)\) denotes the Euclidean distance metric and \(\) is the temperature factor. According to Eq. (1), if the spatial feature \(z_{n,i}\) is closer to the anchor feature \(a_{n,k}\), then the \(z_{n,i}\) will be assigned to the \(k\)-th spatial group with higher probability. After group partition, our Spatial Grouping Module produces \(K\) integrated features representing \(K\) different spatial cues by aggregating spatial features in each group. The integration process is formulated as follows:

\[z_{n,k}^{s}=^{HW}p_{n,i,k}}_{i=1}^{HW}p_{n,i,k}*z_{n,i},\] (2)

where \(z_{n,k}^{s}\) denotes the spatial cues integrated from the \(k\)-th group within the \(n\)-th frame.

In order to extract spatial cues of diverse types, we introduce two entropy-based losses to enhance the distinction between different spatial groups. The first one is an entropy minimization loss to enhance the confidence of group assignment for each spatial feature. The loss is formulated as follows:

\[L_{}=-_{n=1}^{N}_{i=1}^{HW}_{k=1}^{K}p_ {n,i,k}(p_{n,i,k}).\] (3)

For the assignment probability vector \(p_{n,i}=[p_{n,i,1},p_{n,i,2},,p_{n,i,K}]^{T}^{K D}\), if the entropy is minimized, then the feature \(z_{n,i}\) will be confidently assigned to a specific spatial group. The second loss is an entropy maximization loss for the mean assignment probability vector, which guarantees that those \(HW\) spatial features are assigned to different spatial groups. Specifically, the loss is formulated as follows:

\[L_{}=_{n=1}^{N}_{k=1}^{K}_{n,k} (_{n,k}),\] (4)

where \(_{n,k}=_{i=1}^{HW}p_{n,i,k}\) denotes the mean probability of assigning features to the \(k\)-th group within the \(n\)-th frame. For the mean assignment probability vector \(_{n}=[_{n,1},_{n,2},,_{n,K}]^{T}^{K D}\), if the entropy is maximized, then the spatial features \(\{z_{n,i}\}\) will be uniformly assigned to \(K\) spatial groups. By using the two entropy-based losses, we guarantee that spatial features are different from each other across different spatial groups, enriching the diversity of extracted spatial cues.

In the Spatial Grouping Module, the learnable anchor feature for each group is extracted by weighted combining those \(HW\) spatial features, and the weights are calculated conditioned on the feature map \(z_{n}\) by using a lightweight two-layer convolutional network. In this way, the spatial grouping process can be regarded as conducting clustering over spatial features within individual frames. All involved parameters in the module are end-to-end trained together with the main network, _i.e._, we contribute a parametric clustering module to group spatial features for improving the spatial diversity.

### Spatial-Temporal Relation Module

Our proposed Spatial-Temporal Relation Module aims to discover diverse class-correlated spatial-temporal cues from abundant video contents, which enriches the diversity from a spatial-temporal relation modeling perspective for video domain generalization. As demonstrated by previous works [4; 90; 4], there are rich dependencies between entities over space and time dimensions in videos, which

Figure 3: Overview of our proposed (a) Spatial Grouping Module and (b) Spatial-Temporal Relation Module. Best viewed in color.

is crucial for video classification. Accordingly, we propose to explicitly model spatial-temporal dependencies between video cues at multiple space-time scales. Our proposed Spatial-Temporal Relation Module conducts dependency modeling at space and time dimensions separately, and an overview of the module is shown in Figure 3 (b).

First, based on the spatial cues extracted by our Spatial Grouping Module, we conduct spatial dependency modeling between these spatial cues at multiple space scales. Specifically, given the representations of spatial cues \(z_{n}^{s}=[z_{n,1}^{s},z_{n,2}^{s},,z_{n,K}^{s}]^{T}^{K D}\) for the \(n\)-th frame, we extract the spatial relation feature at the \(l\)-th space scale by the spatial dependency modeling function \(R_{l}^{s}()\) as follows:

\[R_{l}^{s}(z_{n}^{s})=_{k_{1},k_{2},,k_{l}}[H_{l}^{s}(z_{n, k_{1}}^{s},z_{n,k_{2}}^{s},,z_{n,k_{l}}^{s})]^{D_{s}},\] (5)

where \([]\) denotes the expectation calculation and \(H_{l}^{s}(,,,)\) denotes a linear projection function after feature concatenation. The index set \(\{k_{1},k_{2},,k_{l}\}\) denotes the index of spatial features uniformly sampled from the \(K\) spatial features, where the index \(l\{2,3,,K\}\) indicates the space scale, \(k_{1} k_{2} k_{l}\) and \(k_{i}\{1,2,,K\}\). For each frame, we extract \(K-1\) spatial relation features by dependency modeling at \(K-1\) space scales separately. And, we concatenate these spatial relation features and produce an integrated feature for each frame, which is given by \(_{n}=[R_{2}^{s}(z_{n}^{s})^{T},R_{3}^{s}(z_{n}^{s})^{T},,R_{K}^{s} (z_{n}^{s})^{T},G(z_{n})^{T}]^{T}^{KD_{s}}\). In the integrated feature \(_{n}\), \(G(z_{n})^{D_{s}}\) denotes the global feature extracted from the feature map \(z_{n}\) by a convolution layer.

Second, based on the frame-level integrated features, we conduct temporal dependency modeling between frames at multiple time scales. Specifically, given \(N\) frame-level features denoted by \(=[_{1},_{2},,_{N}]\), we extract the temporal relation feature at the \(m\)-th time scale by the temporal dependency modeling function \(R_{m}^{t}()\) as follows:

\[R_{m}^{t}()=_{n_{1}<n_{2}<<n_{m}}[H_{m}^{t}(_{n_{1}},_{n_{2}},,_{n_{m}})]^{D_{t}},\] (6)

where \(H_{m}^{t}(,,,)\) denotes a linear projection function after feature concatenation. The index set \(\{n_{1},n_{2},,n_{m}\}\) denotes the index of frame features randomly sampled from the \(N\) frame features, where the index \(m\{2,3,,N\}\) indicates the time scale and \(n_{i}\{1,2,,N\}\). Note that we keep the relative order of sampled frames for temporal modeling. By using \(N-1\) temporal dependency modeling functions, we extract \(N-1\) temporal relation features at \(N-1\) time scales for each video.

To ensure the diversity of temporal relation features, we propose a relation discrimination loss to constrain that different temporal dependency modeling functions (_i.e._, different time scales) capture different temporal cues. This loss constrains that a relation classifier can distinguish one relation feature from not only relation features of other classes but also relation features of the same class but of other time scales. Thus, it avoids the feature collapse of learned temporal relation features. Specifically, the loss is formulated as follows:

\[L_{}=_{m=2}^{N}(F_{}( {z}_{m}),_{m}),\] (7)

where \(_{m}=R_{m}^{t}()\) denotes the temporal relation feature at the \(m\)-th time scale, \(F_{}()\) denotes a relation classifier that classifying \((N-1)*C\) classes, and \((,)\) denotes the cross-entropy loss. The \(_{m}\) denotes the relation label of the video \(x\) with label \(y\), _i.e._, \(_{m}=y*(N-1)+(m-2)\{0,1,2,,(N-1)*C-1\}\). In this way, the loss forces different temporal dependency modeling functions to capture different class-correlated temporal cues in the video since the captured temporal cues are discriminative across scales. By incorporating the Spatial-Temporal Relation Module with the relation discrimination loss \(L_{}\), we extract rich and diverse spatial-temporal cues.

**Feature Aggregation:** After exploring spatial-temporal diversity by our proposed Spatial Grouping Module and Spatial-Temporal Relation Module, our model discovers diverse class-correlated spatial-temporal cues from abundant video contents. Then, we aggregate these diverse spatial-temporal features for video classification. Specifically, the feature aggregation is formulated as \(=_{m=2}^{N}H_{m}^{a}(_{m})\), where \(H_{m}^{a}()\) denotes a small SE-based block  for modulating the \(m\)-th temporal relation features.

**Overall Training and Test:** We adopt a video classification loss on top of the aggregated feature \(\) given by \(L_{}=(F(),y)\), where \(F()\) is a video classifier. Overall, the training loss of our proposed STDN is given as follows:

\[L=L_{}+_{}L_{}+_{}L _{}+_{}L_{},\] (8)

where \(_{}\) and \(_{}\) are hyperparameters for trade-off. Following the standard protocol , we use source videos for training and test the model on target videos for evaluation.

## 4 Experiments

### Benchmarks and Experimental Setups

To demonstrate the effectiveness and versatility of our proposed Spatial-Temporal Diversification Network (STDN), we adopt three benchmarks of different types for experiments, including two newly designed benchmarks, namely EPIC-Kitchens-DG and Jester-DG. For these two new benchmarks, we select video categories and construct domains following previous video domain adaptation works [49; 19]. We split the source video set into training and validation sets following previous source validation protocols [25; 15], _i.e._, a reasonable in-domain model selection scheme for better generalization ability in unseen target domains. We reproduce general domain generalization methods (cooperated with video classification architectures) and state-of-the-art video domain generalization methods for comparison. We report mean and standard deviation of accuracy over three random trials for all methods.

**UCF-HMDB** is the most widely used benchmark for cross-domain video classification [15; 18], which contains 3,809 videos of 12 overlapping sport categories shared by UCF101  and HMDB51 . The videos in UCF101 are mostly captured from certain scenarios or similar environments, and the videos in HMDB51 are captured from unconstrained environments and different camera viewpoints. This benchmark includes two subtasks, i.e., UCF\(\)HMDB and HMDB\(\)UCF.

**EPIC-Kitchens-DG** is a _cross-scene egocentric action recognition_ benchmark, which consists of 10,094 videos across 8 egocentric action classes from three domains (scenes), following Munro et al. . The three domains of EPIC-Kitchens-DG (_i.e._, D1, D2, D3) correspond to three largest kitchens (_i.e._, P08, P01, P22) from the large-scale egocentric action recognition dataset EPIC-Kitchens-55 . This benchmark includes six subtasks constructed from three domains.

**Jester-DG** is a _cross-domain hand gesture recognition_ benchmark. We select videos from the Jester dataset  and construct two domains following Pan et al. . The source (S) and target (T) domains contain 51,498 and 51,415 video clips across 7 categories, respectively. The videos in EPIC-Kitchens-DG and Jester-DG benchmarks are both hand-centric, but they are captured from different views, namely first-person and third-person views.

**Implementation details:** We use ResNet50  pretrained on ImageNet  as the backbone for frame-level feature extraction following the standard video domain generalization protocol . The backbone takes frames of size \(224 224\) as input and outputs feature maps of size \(7 7 2048\). We take \(N=5\) frames for each video for temporal modeling. We set \(K=4\), \(=0.5\), \(D_{s}=192\) and \(D_{t}=256\). \(F()\) is a linear classifier and \(F_{}()\) is an MLP classifier. All parameters are optimized using mini-batch SGD with a batch size of 32, a momentum of 0.9, a learning rate of

   Arch & DG Method & UCF\(\)HMDB & HMDB\(\)UCF & Arch & DG Method & UCF\(\)HMDB & HMDB\(\)UCF \\   & ERM & 51.4\(\)0.2 & 68.6\(\)0.3 &  & ERM & 52.2\(\)0.3 & 69.2\(\)0.3 \\  & ADA\({}_{sem}\) & 51.1\(\)0.3 & 68.2\(\)0.2 &  & ADA\({}_{sem}\) & 51.3\(\)0.3 & 68.6\(\)0.3 \\  & ADA\({}_{pixel}\) & 49.6\(\)0.3 & 67.4\(\)0.2 &  & ADA\({}_{pixel}\) & 52.7\(\)0.3 & 68.3\(\)0.2 \\  & M-ADA  & 52.4\(\)0.2 & 69.2\(\)0.2 & & M-ADA  & 52.5\(\)0.2 & 69.1\(\)0.3 \\  & Jigsaw  & 51.5\(\)0.3 & 68.5\(\)0.3 &  & 52.5\(\)0.3 & 68.9\(\)0.3 \\   & ERM & 54.3\(\)0.3 & 71.4\(\)0.3 &  & ERM & 52.4\(\)0.3 & 69.8\(\)0.3 \\  & ADA\({}_{sem}\) & 55.2\(\)0.3 & 71.9\(\)0.3 &  & ADA\({}_{sem}\) & 52.8\(\)0.2 & 69.6\(\)0.5 \\  & ADA\({}_{pixel}\) & 56.9\(\)0.2 & 72.2\(\)0.3 &  & ADA\({}_{pixel}\) & 52.1\(\)0.3 & 70.6\(\)0.2 \\  & M-ADA  & 55.6\(\)0.3 & 71.5\(\)0.3 & & M-ADA  & 53.4\(\)0.3 & 69.9\(\)0.3 \\  & Jigsaw  & 55.2\(\)0.4 & 72.4\(\)0.3 &  & 53.3\(\)0.3 & 70.1\(\)0.3 \\    & 59.1\(\)0.3 & 74.9\(\)0.3 & STDN (Ours) & **60.2\(\)**0.5 & **77.1\(\)**0.4 \\   

Table 1: Comparison with state-of-the-art methods on the UCF-HMDB benchmark. Red and blue denotes the best and second best. Results of all compared methods are from VideoDG .

[MISSING_PAGE_FAIL:8]

pendency modeling of our Spatial-Temporal Relation Module (denoted by TRM), we obtain 1.8% and 1.4% improvement on UCF\(\)HMDB and HMDB\(\)UCF, respectively. It should be noted that the relation discrimination loss \(L_{}\) is an important part in temporal dependency space and time dimensions, which enriches the diversity in spatial-temporal relation modeling. Moreover, by introducing the feature augmentation technique MixStyle, we obtain further improvement. Finally, our full model aggregates diverse spatial-temporal features, leading to better generalization performance on both UCF\(\)HMDB and HMDB\(\)UCF.

**Diversity Analysis:** We make a quantitative analysis to the diversity of learned video features for our model. Specifically, we evaluate the difference between temporal relation features of different time scales, measured by the normalized Mean Square Error (MSE) between feature vectors. A higher value of normalized MSE indicates a large difference. As shown in Figure 4, without our relation discrimination loss \(L_{}\), learned temporal relation features at different time scales hold very small difference (implying feature collapse). By introducing \(L_{}\), our TRM improves the diversity, indicated by the higher MSE value. By introducing our Spatial Grouping Module, the diversity is further improved as various spatial cues are extracted from each frame. Moreover, by modeling spatial dependencies, our model further enlarges the difference between features across scales.

**Analysis of Spatial Grouping:** We make a qualitative analysis to the grouping process of our proposed Spatial Grouping Module (SGM). Specifically, we use t-SNE  for visualizing feature distributions of spatial features, and we adopt the model trained without our SGM as the baseline (adopts average pooling to extract an integrated feature for each frame) for comparison. Also, we use the Davies-Bouldin Index1 as a quantitative metric to measure the clustering performance, i.e., a lower value of the Davies-Bouldin Index indicates better separation between clusters. As shown by the qualitative and quantitative results in Figure 5, our SGM extracts spatial features with better cluster separation than the baseline, which is attributed to that our SGM enhances the distinction between features in different spatial groups. These results indicate that our proposed spatial grouping process forces the model to learn features encoding more different information. In the supplemental material, we also show Grad-CAM examples to qualitatively compare our SGM with the baseline.

Figure 4: Diversity analysis on UCF\(\)HMDB. We use the normalized Mean Square Error (MSE) to evaluate the feature diversity of variants of TRM, _i.e._, measuring difference between temporal relation features of different time scales. A higher value of normalized MSE indicates higher diversity.

Figure 5: T-SNE visualization of spatial features. For both the baseline and SGM, we cluster the set of spatial features into \(K=4\) clusters by \(K\)-means before visualization. In the figure, dots stand for spatial feature vectors and stars stand for cluster centers, and different colors denote different clusters.

  Method & UCF\(\)HMDB & HMDB\(\)UCF \\  Backbone & 52.7\(_{0.3}\) & 71.9\(_{0.3}\) \\ +SGM & 54.9\(_{0.3}\) & 73.9\(_{0.4}\) \\ +TRM & 56.7\(_{0.2}\) & 75.3\(_{0.4}\) \\ +STRM & 58.3\(_{0.4}\) & 76.2\(_{0.3}\) \\ +MixStyle & 59.3\(_{0.3}\) & 76.6\(_{0.2}\) \\ Full STDN & 60.2\(_{0.5}\) & 77.1\(_{0.4}\) \\  

Table 3: Ablation study on UCF-HMDB.

**Grad-CAM Visualization:** We compare our proposed STDN with a TRN  model (the baseline) by Grad-CAM . As shown in Figure 6, the baseline prefers to use the domain-specific backboard for recognition, which causes recognition errors in the target domains as backboards are invisible. In contrast to the baseline, our proposed STDN perceives more diverse class-correlated cues from the source domain, including some domain-invariant cues such as basketballs. As a result, our STDN can predict the correct video class by recognizing the basketball in the target video. These results demonstrate that, our proposed diversity-based approach can discover some potential domain-invariant cues, which alleviates the overfitting to domain-specific cues and leads to better generalization in the target domain.

## 5 Conclusion

In this work, we propose to explore spatial-temporal diversity to address the video domain generalization task. Our proposed Spatial-Temporal Diversification Network learns diverse spatial-temporal features in videos, which discovers potential domain-invariant cues and thus alleviates the heavy reliance on domain-specific cues. We conduct extensive quantitative and qualitative experiments on three benchmarks (including two newly designed benchmarks), and the results demonstrate the effectiveness and versatility of our approach.

**Acknowledgements.** This work was supported partially by the NSFC (U21A20471,U1911401), Guangdong NSF Project (No. 2023B1515040025, 2020B1515120085). The authors would like to thank Zhilin Zhao, Yi-Xing Peng, and Yu-Ming Tang for their valuable suggestions on model design or writing.