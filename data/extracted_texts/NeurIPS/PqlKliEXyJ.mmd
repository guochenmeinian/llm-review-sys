# LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural Wireframe Alignment

Juelin Zhu\({}^{1}\)

zhujuelin@nudt.edu.cn

&Shen Yan\({}^{1}\)

yanshen12@nudt.edu.cn

&Long Wang\({}^{2}\)

wanglongzju@gmail.com

&Shengyue Zhang\({}^{1}\)

zhangshengyue23@nudt.edu.cn

&Yu Liu\({}^{1}\)

jasonyuliu@nudt.edu.cn

&Maojun Zhang\({}^{1}\)

mjzhang@nudt.edu.cn

\({}^{1}\)National University of Defense Technology

\({}^{2}\)SenseTime Research

###### Abstract

We propose a new method named LoD-Loc for visual localization in the air. Unlike existing localization algorithms, LoD-Loc does not rely on complex 3D representations and can estimate the pose of an Unmanned Aerial Vehicle (UAV) using a Level-of-Detail (LoD) 3D map. LoD-Loc mainly achieves this goal by aligning the wireframe derived from the LoD projected model with that predicted by the neural network. Specifically, given a coarse pose provided by the UAV sensor, LoD-Loc hierarchically builds a cost volume for uniformly sampled pose hypotheses to describe pose probability distribution and select a pose with maximum probability. Each cost within this volume measures the degree of line alignment between projected and predicted wireframes. LoD-Loc also devises a 6-DoF pose optimization algorithm to refine the previous result with a differentiable Gaussian-Newton method. As no public dataset exists for the studied problem, we collect two datasets with map levels of LoD3.0 and LoD2.0, along with real RGB queries and ground-truth pose annotations. We benchmark our method and demonstrate that LoD-Loc achieves excellent performance, even surpassing current state-of-the-art methods that use textured 3D models for localization. The code and dataset are available at https://victorzoo.github.io/LoD-Loc.github.io/.

## 1 Introduction

Aerial visual localization is the process of determining the global position and orientation for a UAV camera relative to a known map. This process benefits many important applications, ranging from cargo transport , surveillance [19; 72], to search and rescue [15; 63].

Following localization algorithms on the ground [18; 30; 42; 53; 54; 57; 67; 68; 70; 77], current aerial visual localization approaches [22; 72] typically involve matching pixels in a query image with points in a pre-built high-quality 3D map, which is often derived from 3D texture models [43; 76; 72]. Subsequently, a Perspective-n-Point (PnP) RANSAC [32; 37; 14; 23; 27; 24] technique is commonly used to calculate the camera pose. However, building high-quality 3D maps using photogrammetry [12; 28; 64; 58; 33] is expensive on a global scale and requires frequent updates to account for temporal changes in visual appearance. Besides, these 3D maps are costly to store, which poses significant challenges for terminal deployment on drones. Furthermore, high-resolution 3D maps disclose detailed information about the localization area, raising critical concerns regarding homeland security and privacy preservation.

To address these challenges, we propose to leverage the Level of Detail (LoD) 3D city maps  as the cue for localization, as illustrated in Figure 1. Compared with traditional textured 3D models, LoD 3D models enjoy the following advantages: 1) **Ease of acquisition and maintenance**: World-scale LoD city models can be generated with the rapid development of remote sensing . Many commercial companies, such as Google Maps  and Baidu Maps , have already integrated LoD 3D models into their MAP Applications. 2) **Light-weight map size**: LoD maps are extremely compact, up to \(10^{4}\) times smaller in size than textured 3D maps, enabling on-device localization over large areas. 3) **Privacy preservation and policy-friendly**: As LoD city models only reveal the basic 3D outlines of buildings in a highly abstracted and simplified manner, they raise fewer concerns about the disclosure of privacy and land resource secrets.

However, compared with a textured 3D model, using LoD maps for localization is very challenging, primarily due to the lack of texture and detail. This deficiency makes it nearly impossible to establish local feature-based 2D-to-3D correspondences. Inspired by the idea that, when the pose is correctly solved, the network-predicted building wireframes can align with those projected from the LoD 3D model, as shown in Figure 1, we introduce LoD-Loc, a novel approach tailored for visual localization in LoD 3D city maps. Our method takes a query image and its real sensor data (i.e., GPS, gravity, and compass) as input, and estimates the 6-DoF pose of a drone in flight. Specifically, we initially fix the 2-DoF gravity direction and generate pose hypotheses by sampling 4-DoF (comprising position and yaw angle) around the sensor pose, given that the gravity direction provided by the inertial unit exhibits minor error. Following the generation of pose hypotheses, LoD building wireframes are projected onto the query image plane. Each pose hypothesis is then scored based on the alignment between the projected and predicted wireframes, thereby forming a 4D pose cost volume. By applying a _softmax_ operation, we derive a probability density over the pose, which can be used for pose selection through classification. Moreover, after the pose selection stage, a differentiable Gauss-Newton method, with an optimization objective to maximize the wireframe alignment, is employed to refine the overall 6-DoF pose. The pose selection and optimization processes are fully differentiable w.r.t. the network output, which enables the use of ground-truth poses as supervision for training feature extraction and pose estimation in an end-to-end manner.

To achieve high accuracy and low memory usage, we propose a hierarchical scheme for pose selection that utilizes multiple small pose volumes, to progressively compute poses in a coarse-to-fine manner. Throughout the hierarchy, we adopt an adaptive sampling strategy, where the variance-based uncertainty from the previous stage influences the sampling range of the next stage for constructing pose cost volume. This adaptive process enables reasonable and fine-grained spatial partitioning of poses, resulting in a significant improvement in the final pose output.

To facilitate research in this area, and to train and evaluate our method, we release two datasets with map levels of LoD3.0 and LoD2.0, respectively, as shown in Figure 2. For the LoD3.0 dataset, we employ a semi-automatic method to generate LoD model data from a recent large-scale oblique photography scene , covering an area of 2.5 square kilometers. The query images are captured by drones, with sensor data (e.g., GPS, IMU) recorded. For the LOD2.0 dataset, we use LoD model data provided by the Swiss federal authorities, specifically the SwissTOPO  data near Ecole

Figure 1: In this paper, we propose LoD-Loc to tackle visual localization w.r.t a scene represented by a LoD 3D map, characterized by its ease of acquisition, lightweight nature, and built-in privacy-preserving capabilities. Given a query image and its coarse sensor pose, our method utilizes the wireframe alignment of LoD models to recover the camera pose.

Polytechnique Federale de Lausanne (EPFL), covering an area of 8.2 square kilometers. The query images with ground-truth poses are sourced from the CrossLoc  project.

We conduct extensive experiments on these two datasets. The results show that, due to the lack of both color and texture in the LoD 3D model, previous state-of-the-art image retrieval-and-matching methods [44; 53; 54; 29; 41; 25; 66; 46; 47] basically fail. In contrast, our method consistently achieves excellent results, even surpassing current state-of-the-art methods [53; 54; 66; 72] that use textured 3D models for localization.

#### Contributions.

* We propose the use of Level of Detail (LoD) 3D maps for 6-DoF visual localization in the air.
* We introduce a novel localization method that utilizes wireframe alignment for pose estimation.
* Our method is differentiable, allowing the pipeline to be trained end-to-end with pose supervision.
* We release two LoD city datasets, complete with RGB queries and ground-truth pose annotations.

## 2 Related Works

**Localization from SfM or Mesh Map.** SfM maps typically consist of reference images and 3D track points with their associated features . For a given query image, an image retrieval method [13; 29] is initially utilized to identify co-visible reference images. Following this, feature matching algorithms [41; 25; 54; 66] are employed to establish accurate 2D-2D correspondences between the query image and the identified reference images, with track information being used to transform these 2D-2D correspondences into 2D-3D relationships. Finally, the pose is resolved using PnP RANSAC [32; 37; 14; 23; 27; 24; 77].

Mesh maps are typically defined by a textured mesh model. Initially, reference images with depth are rendered at appropriate viewpoints surrounding the model [72; 43; 44; 76]. Similar image retrieval [13; 29] and matching [41; 25; 54; 66] processes are employed to identify co-visible images and to establish 2D-2D correspondences. The depth map is utilized to transform 2D-2D correspondences into 2D-3D relationships, and the pose is subsequently determined by a PnP RANSAC [32; 37; 14; 23; 27; 24; 77].

Despite providing high-accuracy localization results, both SfM and mesh models present significant challenges in terms of reconstruction and maintenance. Additionally, their extensive size complicates deployment, necessitating its existence solely in the cloud. Moreover, these maps raise serious concerns about the privacy of personal and land resource leakage.

**Localization from other types of Map.** To mitigate these issues, researchers have proposed to use alternative types of maps for localization. In addressing the difficulty of map reconstruction and maintenance, some methods opt for overhead imagery such as satellite [59; 60; 73; 56], or leverage OpenStreetMap  as a reference. However, these methods are limited to estimating a 3-DoF (planar position and heading) pose at most. To address the issue of large map size, some methods have made attempts to compress the maps [20; 21; 78], reduce model complexity , or utilize geometry information without features [49; 79; 39; 75]. In the context of privacy, some methods [36; 61; 65] propose to transform 3D point clouds into 3D line clouds, leverage semantic

   Map Type &  SfM \\ SLAM \\  &  Mesh \\ model \\  &  Satellite \\ images \\  & OpenStreetMap & 
 LoD model \\ **(our works)** \\  \\   & 3D points & textured & pixel & polygons, & wireframes, \\  & +features & meshes & intensity & lines, points & faces \\  Explicit geometry? & 3D & 3D & \(\) & 2D & 3D \\ Visual appearance? & \(\) & \(\) & \(\) & \(\) & \(\) \\ x-DoF pose estimation & 6-DoF & 6-DoF & 3-DoF & 3-DoF & 6-DoF \\ Storage per 1km\({}^{2}\) & 42 GB & 9.8 GB & 75 MB & 4.8 MB & 2.84 MB \\ Size reduction v.s. SfM & - & 4.28\(\) & 550\(\) & 8800\(\) & 15100\(\) \\   

Table 1: **Different types of maps for visual localization.**information from point clouds, or utilize semantic 3D maps to enhance privacy . Some other methods apply learning-based pose regression or scene point regression models [35; 62; 40] that do not explicitly store the 3D map. However, the effectiveness and generalizability of these methods are often inferior to those that rely on SfM or texture mesh maps. A detailed comparison of attributes from different maps is provided in Table 1.

## 3 Method

Given a 3D city LoD map \(\), a query image \(\), and its coarse sensor pose \(_{p}\), the goal of the proposed method is to compute the absolute 6-DoF pose \(^{*}\). First, a convolutional neural network is used to extract the wireframe probability map for the query image \(\) at multiple levels (Sec. 3.1). Second, at each level, uniform pose sampling and 3D wireframe projection are employed to build a cost volume for various pose hypotheses, describing the pose probability distribution. The pose with the maximum probability is then selected (Sec. 3.2). Finally, a post-processing network refines the wireframe probability map after the last level, and a Gauss-Newton method is applied to refine the pose chosen in the previous stage (Sec. 3.3). Figure 3 provides an overview of the proposed method.

### Multi-Scale Feature Extractor

We use a standard convolutional architecture with U-Net  to extract multi-level features from the query image \(\). Different from previous works that maintain a high-dimensional feature map to encapsulate rich visual information for each level, we abstract and reduce the feature map dimension to a single channel, where each pixel in this map signifies the likelihood of being a wireframe. The resulting feature maps are denoted by \(_{l}^{H_{l} W_{l} 1}\), where \(l=\{1,2,3\}\) is the level index. More details on the architecture of the proposed network can be found in Appendix D.1.

### Pose Selection from Cost Volume

After feature extraction, we construct a cost volume based on various pose hypotheses sampled around the coarse sensor pose, selecting the pose with the highest probability at each level. To ensure

Figure 2: **Overview of datasets. The left side shows the LoD models of the released data. The LoD2.0 model from Swiss-EPFL includes building height and roof information, while the LoD3.0 model from UAVD4L-LoD contains more detailed structural information such as building height, roof, and side pillars. The right side illustrates samples of query images, which consist of images captured by drones in various scenes.**

efficient sampling, we utilize the uncertainty in pose selection at the current level to determine the pose sampling range for the next level.

**Pose cost volume reconstruction.** This subsection explains how to construct the pose cost volume. For a specific level \(l\), the initial pose is represented by \(_{l}\), and the computed pose is denoted as \(}_{l}\). As part of a progressive process, we keep \(_{1}=_{p}\) and \(_{l+1}=}_{l}\). The pose \(_{l}\) can be decoupled into six degrees, with \(_{l}=(_{l},_{l},_{l},_{l},_ {l},_{l})\), where \((_{l},_{l},_{l})\) represents the translation in 3D space while \((_{l},_{l},_{l})\) refers to the Eular angles (i.e., yaw, pitch, roll). Since the pitch and roll \((_{p},_{p})\) of the gravity direction from the sensor pose data exhibit high accuracy, we fix \((_{l},_{l})=(_{p},_{p})\) and only conduct operations on the remaining 4-DoF (i.e., \((_{l},_{l},_{l},_{l})\) ) in the following steps.

Specifically, we begin by uniformly sampling 4-DoF poses centered on the initial pose \(_{l}\), with the sampling range and number defined as \(_{l}=[_{l}(x),_{l}(y),_{l}(z),_{l}()]\) and \([m_{l}(),m_{l}(),m_{l}(),m_{l}()]\), respectively. The pose hypothesis \(\{_{l}^{hyp}(d)\}\) is generated along \((,,,)\) directions separately, where \(d(,,,)\).

\[\{_{l}^{hyp}(d)\}=\{_{l}(d)/2+d_{l},,d_{l },,d_{l}+_{l}(d)/2}_{m_{l}(d)}\}.\] (1)

Next, for a given pose hypothesis, denoted as \(_{l}^{hyp}=(_{l}^{hyp},_{l}^{hyp})\) and a set of discrete 3D wireframe points denoted as \(\{_{i}\}\), we define a line alignment cost:

\[_{l}(_{l}^{hyp})=_{i=1}^{n}_{ l}[_{i}].\] (2)

In this equation, \(_{i}=(_{l}^{hyp}_{i}+_{l}^{ hyp})\) represents the projection of 3D point \(_{i}\) under pose hypothesis \(_{l}^{hyp}\) and \([]\) denotes a lookup with sub-pixel interpolation. The construction of the 3D wireframe points \(\{_{i}\}\) is provided in the next paragraph. By combining these costs in a grid manner across four distinct dimensions \((,,,)\), we obtain a pose cost volume \(_{l}\) with dimensions \([m_{l}() m_{l}() m_{l}() m_{l }()]\). Finally, a _softmax_ function is applied to \(_{l}\) to yield a probability distribution volume \(_{l}\). For pose inference, we select the pose \(}_{l}\) with maximum probability by _argmax_ operation upon \(_{l}\).

**Discrete 3D wireframe points generation.** For a query image \(\) and its associated sensor pose \(_{p}\), we describe how to sample and identify discrete 3D wireframe points \(\{_{i}\}\) across the entire LoD map \(\). Assume the LoD map \(\) is characterized by a number of faces with vertices \(_{j}=[X_{j},Y_{j},Z_{j}]^{T}^{3}\). We derive each line of the LoD model as \(_{jk}=[(X_{j},X_{k}),(Y_{j},Y_{k}),(Z_{j},Z_{k})]\) by connecting vertices \(_{j}\) and \(_{k}\). To focus on distinct geometric structures such as building edges, we discard lines whose normals of their neighboring faces exhibit a significant difference, larger than \(=10\) degrees. The line simplification process is facilitated with the assistance of Blender .

Figure 3: **Overview of LoD-Loc**. 1. LoD-Loc employs a CNN to extract multi-level features \(_{l}\) for the query image \(\) (Sec. 3.1). 2. A cost volume \(_{l}\) is built for various pose hypotheses sampled around the coarse sensor pose \(_{p}\) to select the pose \(_{l}\) with the highest probability, based on the projected wireframe of the 3D LoD model (Sec. 3.2). 3. A differentiable Gauss-Newton method is used to refine the final selected pose \(_{3}\), to obtain a more accurate pose \(^{*}\) (Sec. 3.3).

Subsequently, we set the sampling density \(\) in meters and uniformly sample points along all simplified lines \(\{_{jk}\}\) to obtain 3D points as \(\{_{i}^{u}\}\), where \(i\) represents the index of the 3D points. To obtain visible 3D points for a query image \(\), we use the pinhole camera projection (with intrinsic matrix \(^{p}\)) and pose prior \(_{p}=(^{p},^{p})\) to identify 3D points, taking factors such as frustum inside and occlusion information into account. In particular, we first project 3D points onto the 2D image plane:

\[_{i}[u_{i},v_{i},1]^{}=^{p}(^{p}[X_{i},Y_ {i},Z_{i}]^{}+^{p})\] (3)

where \([X_{i},Y_{i},Z_{i}]\{_{i}^{u}\}\), and \(\) is the projected depth for point \([X_{i},Y_{i},Z_{i}]\). We then render a depth map from the LoD map \(\) from pose \(_{p}\), denoted as \(\). A boolean mask is calculated as:

\[B_{i}=_{i}<(u_{i},v_{i})\ \ \&\ \ 0<u_{i}<H\ \ \&\ \ 0<v_{i}<W\] (4)

where \(H\) and \(W\) denote the size of the image \(\), \((u_{i},v_{i})\) means the interpolating value on depth map \(\) at \((u_{i},v_{i})\). The final discrete visible 3D wireframe points \(\{_{i}\}\) can be obtained using Eq. 5:

\[\{_{i}\}=\{_{i}^{u}[B_{i}]\}.\] (5)

**Uncertainty sampling range estimation.** During the coarse-to-fine process, we leverage the pose selection uncertainty from the previous level to determine the sampling range of the current level. This strategy allows us to progressively subdivide the pose sampling space, thereby enhancing the precision of pose selection. More specifically, for \(l=1\), we define the pose sampling range by evaluating the error in the coarse sensor pose. The sampling range for \((,,,)\) is defined as \([_{1}(x),_{1}(y),_{1}(z),_{1}( )]=[_{p}(x),_{p}(y),_{p}(z),_{p}( )]\). For \(l=\{2,3\}\), we employ the variance of the probability distribution volume \(_{l-1}\) at \(l-1\) to determine the pose sampling range \(_{l}\).

In particular, since the pose hypothesis \(\{_{l}^{hyp}\}\), pose cost volume \(_{l}\) and probability distribution volume \(_{l}\) share the same data structure, we flatten them and index them by \(t\). The variance \(_{l}\) at level \(l\) is calculated as:

\[_{l}=_{t}{}^{t}_{l-1}\|}_{l-1}_{l-1}^{hyp}\|^{2}.\] (6)

Here, the symbol \(\) represents the subtraction operation separately applied to the \((,,,)\) directions. The corresponding standard deviation is computed as \(_{l}=_{l}}\). We compute the pose sampling range as \(_{l}=2_{l}\), where \(\) is a hyperparameter that adjust the length of the sample range. A visualization of this uncertainty sampling range estimation process can be found in Figure 4.

### Pose Refinement

Based on the selected pose \(}_{3}\) from the previous stage, we use a refined wireframe probability map \(_{rf}\), which is further extracted from the feature map \(_{3}\) via a post-processing convolutional network, we optimize the pose \(^{*}=(^{*},^{*})\) so as to align the 3D wireframe with the 2D predicted wireframe. Specifically, we define the objective function:

\[E(^{*})=-_{i}||f_{i}||^{2}=-_{i}||_{rf}[ (^{*}_{i}+^{*})]||^{2},\] (7)

Figure 4: Toy examples to illustrate the uncertainty sampling range estimation. We show pose distribution (connected blue dots), pose prediction (yellow dash line), the ground truth pose (red dash line), and uncertainty sampling range (gray) in the three levels.

where \(_{i}\) is the 3D wireframe point and \(\) is the projection operation. Minimizing this function aligns the projected 3D wireframe points with the 2D locations that have a higher predicted probability. The pose update formula for \(^{*}\), as derived from the Gauss-Newton method, is given by:

\[= -_{i}{(J_{i}^{T}J_{i})^{-1}_{i}{(J_{i}^{T}f_{i})}}\] (8) \[^{*}= ^{*}_{t}+^{*}\] \[^{*}= ^{*}(_{r}).\]

In this formula, \(^{6}\) represents a six-dimensional transformation vector, where \(_{r}^{3}\) constitutes the rotational component and \(_{t}^{3}\) represents the translational component. We transform the rotational component \(_{r}\) into a 3\(\)3 rotation matrix by the exponential map of the Lie algebra \((3)\). Besides, \(J_{i}\) represents the Jacobian matrix of the residual function \(f_{i}\) with respect to the pose parameters. A comprehensive explanation and detailed implementation of the Jacobian computation are provided in Appendix D.2.

### Supervision

We employ two separate loss functions to facilitate end-to-end training of the pose selection procedure (Sec. 3.2) and the pose refinement module (Sec. 3.3). For the pose selection module, we minimize the negative log-likelihood loss on the probability distribution volume \(_{l}\) at three levels, where \(l\{1,2,3\}\).

\[L_{s}=-_{l}_{l}[}],\] (9)

For the pose refinement process, the training involves minimizing the reprojection errors between 3D wireframe points transformed by the estimated pose \(^{*}\) and the ground truth pose \(}=(},})\):

\[L_{f}=_{i}(||(^{*}_{i}+^{*})- (}_{i}+})||^{2}),\] (10)

where \(\) represents the Huber robust kernel.

## 4 Experiment

Extensive experiments are conducted on the UAVD4L-LoD and Swiss-EPFL datasets to demonstrate the effectiveness of our proposed model as described in Sec. 4.2. Additionally, ablation studies are conducted on the UAVD4L-LoD dataset in Sec. 4.3.

**Datasets.** The released datasets consist of two distinct parts, named UAVD4L-LoD and Swiss-EPFL, providing LoD3.0 and LoD2.0 models, respectively. The UAVD4L-LoD dataset, which spans an area of 2.5 square kilometers, is generated through a semi-automatic process which produces a 3D LoD map from the mesh model of the UAVD4L  dataset. The Swiss-EPFL dataset, which covers an expansive area of 8.18 square kilometers, derives its LoD2.0 models from data made publicly accessible by the Swiss federal authorities . We illustrate the 3D LoD maps and query images of these two datasets in Figure 2. More details can be found in Appendix A, B and C.

**Baseline.** We compared our approach with two visual localization baselines: UAVD4L , predicated on textured mesh models, and CadLoc , predicated on LoD models, employing diverse feature extractors and matchers. Both baselines employ a keypoint-based strategy: 1) SIFT  descriptor with traditional Nearest Neighbor (NN) matching, 2) learning-based extractor SuperPoint (SPP)  with graph-based networks Superglue (SPG) , 3) detector-free matcher LoFTR  and 4) e-LoFTR , 5) dense feature matcher RoMA . Additionally, considering the line structure of the LoD model, we apply three line-based algorithms for the CadLoc: 6) deep neural network SOLD\({}^{2}\) for joint detection and description of line segments, 7) deep line segment detector DeepLSD  with line detector in SOLD\({}^{2}\), 8) DeepLSD with wireframe-based representation and dual-softmax matching method GlueStick . Further details about the implementation of the baseline experiments can be found in Appendix E.

**Metrics.** We follow the standard localization evaluation procedure  and set recall thresholds of \((2m,2^{})\), \((3m,3^{})\), and \((5m,5^{})\).

### Implementation Details

During training, we set a random seed to limit 3D wireframe points \(\{_{i}\}\) to \(2,000\) points, and the pose sampling number \(m_{l}(),m_{l}(),m_{l}(),m_{l}()\) for level \(l=1,2,3\) is uniformly set to \(\) due to constraints related to CUDA memory. The image size is \((512,480)\) for the UAVD4L dataset and \([720,480)\) for the Swiss-EPFL dataset. The pose sampling range at level \(1\) is set as \([10,10,30,7.5]\) which refers to \([_{p}(x),_{p}(y),_{p}(z),_{p}()]\). For the UAVD4L-LoD dataset, we incorporate a subset of synthesized images from UAVD4L , which includes buildings, as training data. For Swiss-EPFL, we train the model by combining synthetic images _LHS_ and real query images from the CrossLoc  project, following its data split pattern.

During inference, experiments are executed on real query images \(\{_{i}\}\) derived from two datasets. We make the following changes, the discrete retrieval points from the 3D wireframe are sampled at an interval of 1 meter. The pose sampling number is increased to \([m_{l}(),m_{l}(),m_{l}(),m_{l}()]=[10,10,3 0,8]\) for all levels. \(\) is set as \(0.8\). The training and inference of the entire network are executed using 2 NVIDIA RTX 4090 GPUs. Additionally, we employ four variations to validate the effectiveness of our method. Specifically, 1) _-no NWE_ means no neural wireframe estimation, which extracts explicit line segments using DeepLSD  and constructs a distance field for each segment. It then replaces the cost function in Equations 2 and 7 with the distance function values, and solves for the pose using coarse-to-fine pose selection followed by Gauss-Newton refinement; 2) _-no USR_ means a model without uncertainty sampling range estimation (Sec. 3.2); 3) _-no Refine_ denotes a model without pose refinement (Sec. 3.3); and 4) full model is our proposed LoD-Loc.

### Evaluation Results

**Evaluation over UAVD4L-LoD dataset.** As described in Table 2, our method shows excellent performance, both in the _in-Traj_. and _out-of-Traj_. queries. Apart from the \(2m-2^{}\) and \(3m-3^{}\) metric in the _in-Traj_ queries, which are marginally lower than UAVD4L with RoMA matcher, all other metrics surpass those of contemporary baselines. Note that this comparison is unfair, as baselines reference on a high-precision texture model that is richer in texture and geometry, while we only employ a LoD model. We further compare with CadLoc, which shares the same 3D reference model as ours. However, we observe that regardless of the choice of descriptors (point-based or line-based), these methods perform poorly. We visualize their retrieval and matching failure cases in Appendix E.3. Furthermore, we analyze why our method performs better in the _out-of-Traj_. scenarios compared to the _in-Traj_. scenarios in Appendix F.3.

    &  &  \\   & \)} & 3m-\(3^{}\) & 5m-\(5^{}\) & 2m-\(2^{}\) & 3m-\(3^{}\) & 5m-\(5^{}\) \\   UAVD4L \\ _Mesh model_ \\  } & SIFT+NN & 73.13 & 78.62 & 80.42 & 82.39 & 85.13 & 86.36 \\  & SPP+SPG & 91.71 & 92.02 & 92.14 & 93.43 & 93.70 & 93.80 \\  & LoFTR & 84.98 & 88.09 & 88.90 & 91.56 & 92.02 & 92.11 \\  & e-LoFTR & 84.47 & 88.21 & 88.96 & 91.06 & 91.93 & 92.02 \\  & RoMA & **93.27** & **93.70** & 93.77 & 95.03 & 95.53 & 95.53 \\   CadLoc \\ _LoD model_ \\  } & SIFT+NN & 0 & 0 & 0 & 0 & 0 & 0 \\  & SPP+SPG & 0 & 0 & 0 & 0 & 0 \\  & LoFTR & 0 & 0 & 0 & 0 & 0 \\  & e-LoFTR & 0.37 & 0.87 & 1.31 & 0.41 & 0.78 & 1.37 \\  & RoMA & 2.18 & 2.87 & 3.68 & 6.93 & 8.76 & 10.40 \\  & SOLD2 & 0 & 0 & 0 & 0 & 0 \\  & DeepLSD+SOLD2 & 0 & 0 & 0 & 0 & 0 \\  & DeepLSD+GlueStick & 0 & 0 & 0 & 0 & 0 \\   **Ours** \\ _LoD model_ \\  } & no \(NWE\) & 10.41 & 16.21 & 24.19 & 6.93 & 12.64 & 21.62 \\  & no \(USR\) & 70.39 & 85.47 & 95.32 & 82.62 & 94.71 & 97.63 \\  & no \(Refine\) & 51.31 & 76.06 & 86.78 & 74.27 & 97.95 & 99.36 \\  & **Full model** & 84.41 & 91.77 & **96.95** & **95.94** & **99.00** & **99.36** \\   

Table 2: **Quantitative comparison results over the UAVD4L-LoD dataset.**

**Evaluation over Swiss-EPFL dataset.** Table 3 presents the inference results on the Swiss-EPFL dataset. CadLoc continues to exhibit widespread failures due to its poor retrieval and matching results across different modalities. We surpass the state-of-the-art UAVD4L method in the \(in\)-\(Place\) queries, but fall behind in the \(out\)-\(of\)-\(Place\) queries. Moreover, the overall results obtained on the Swiss-EPFL dataset are not as strong as those on the UAVD4L-LoD dataset. We attribute this discrepancy to the inferior quality of the images in the training database as explained in Appendix F.1. Besides, the LoD2.0 model provides less structured information, making it harder for pose inferences.

**Analysis of Methodological Advantages.** First, compared to the SOTA texture-based approach, the LoD-Loc employs distinct cues for localization. The texture-based method determines the pose by optimizing the re-projection error of corresponding 2D-3D points. Conversely, the LoD-Loc aligns the 3D wireframe projection to solve the pose. Second, 3D-model-based methods typically employ a two-stage scheme, which involves building 2D-3D matches and then solving the pose with PnP RANSAC. The LoD-Loc method directly solves the pose in an end-to-end manner, potentially leading to better pose accuracy. Third, the LoD-Loc includes several important modules to improve performance, such as coarse-to-fine pose cost volume reconstruction, uncertainty-based sampling range estimation, and differential Gauss-Newton refinement. These factors contribute to the superior performance of our method.

    &  &  &  \\   & & 2m-2\({}^{}\) & 3m-3\({}^{}\) & 5m-5\({}^{}\) & T.e.(m) & R.e.(\({}^{}\)) \\   & Level 1 & 23.88 & 60.35 & 83.85 & 2.58 & 1.41 \\  & Level 2 & 48.57 & 75.06 & 85.10 & 2.03 & 1.27 \\  & Level 3 & 51.31 & 76.06 & 86.78 & 1.97 & 1.25 \\  & Refine & **84.41** & **91.77** & **96.95** & **0.97** & **0.52** \\   & Level 1 & 34.81 & 78.01 & 97.67 & 2.31 & 1.05 \\  & Level 2 & 65.37 & 95.35 & 99.22 & 1.76 & 0.97 \\   & Level 3 & 74.27 & 97.95 & 99.36 & 1.63 & 0.95 \\   & Refine & **95.94** & **99.00** & **99.36** & **1.06** & **0.49** \\   

Table 4: **Ablation study on different stages.** T.e./R.e. means translation/rotation error.

    &  &  \\   & & 2m-2\({}^{}\) & 3m-3\({}^{}\) & 5m-5\({}^{}\) & 2m-2\({}^{}\) & 3m-3\({}^{}\) & 5m-5\({}^{}\) \\   & 0 & 0 & 0.56 & 0 & 0 & 1.06 \\   UAVD4L \\ _Mesh model_ \\  } & SIFT+NN & 14.47 & 23.31 & 36.52 & 32.98 & 54.35 & 71.50 \\  & SPP+SPG & 34.83 & 60.39 & 77.25 & 77.04 & 89.71 & 92.35 \\  & LoFTR & 27.67 & 49.58 & 66.43 & 68.87 & 81.00 & 84.96 \\  & e-LoFTR & 37.64 & 60.96 & 76.40 & 81.53 & 91.03 & 93.93 \\  & RoMA & 45.98 & **66.77** & **80.73** & **89.18** & **98.68** & **98.94** \\   CadLoc \\ _LoD model_ \\  } & SIFT+NN & 0 & 0 & 0 & 0 & 0 & 0 \\  & SPP+SPG & 0 & 0 & 0 & 0 & 0 & 0 \\  & LoFTR & 0 & 0 & 0 & 0 & 0 & 0 \\  & e-LoFTR & 0 & 0.14 & 0.14 & 0 & 0 & 0.53 \\  & RoMA & 0.98 & 1.97 & 2.67 & 2.37 & 5.01 & 6.33 \\  & SOLD2 & 0 & 0 & 0 & 0 & 0 & 0 \\  & DeepLSD+SOLD2 & 0 & 0 & 0 & 0 & 0 & 0 \\  & DeepLSD+GlueStick & 0 & 0 & 0 & 0 & 0 & 0 \\   **Ours** \\ _LoD model_ \\  } & no \(NWE\) & 11.37 & 21.35 & 33.57 & 18.99 & 31.39 & 45.91 \\  & no \(USR\) & 42.42 & 58.29 & 71.21 & 31.40 & 48.81 & 70.45 \\   & no \(Refine\) & 36.10 & 58.01 & 76.97 & 18.21 & 39.31 & 66.23 \\   & **Full model** & **48.60** & 65.31 & 79.78 & 37.73 & 57.26 & 77.57 \\   

Table 3: **Quantitative comparison results over the Swiss-EPFL dataset.**

### Ablation Studies

We perform ablation experiments on the UAVD4L-LoD dataset, focusing on different levels. More ablation studies are provided in Appendix F.2.

**Levels.** As depicted in Table 4, we present the results of the ablation experiments in terms of recall, translation errors, and rotation errors. The localization accuracy shows a gradual improvement as the number of levels increases. This demonstrates the effectiveness of the progressive coarse-to-fine estimation and final pose refinement. Figure 5 visualizes feature maps for each level, illustrating that wireframe features extracted from deeper levels is clearer.

## 5 Conclusion

This paper presents LoD-Loc, a novel approach for localizing aerial images using a LoD 3D map. Compared to large and expensive 3D maps that existing methods rely on, the LoD map provides a simple, accessible, and privacy-friendly scene representation. With the coarse sensor pose, the proposed LoD-Loc uses a unified pipeline to estimate the camera pose, including a multi-scale feature extractor, pose selection from cost volume, and pose refinement. Furthermore, we contribute two datasets with map levels of LoD3.0 and LoD2.0, along with real RGB queries with ground-truth pose annotation. LoD-Loc achieves excellent performance, even surpassing current state-of-the-art methods that use textured 3D models for localization. We believe LoD-Loc opens new possibilities for visual localization with simple and scalable 3D maps.

**Limitation.** LoD-Loc operates under the assumptions of a known gravity direction and a location prior. While these assumptions are reasonable, they restrict the application of LoD-Loc in environments where GPS is denied or unavailable.

**Broader impact.** This work has implications regarding privacy and surveillance. However, the LoD models represent building structures in a highly abstracted form, which alleviates concerns about the disclosure of personal privacy or land resource information.

Figure 5: **Visualization of feature maps from different levels.** The feature maps of different levels reflect different fineness of wireframe extraction.