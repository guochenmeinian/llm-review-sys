# Prompt Pre-Training with Twenty-Thousand Classes

for Open-Vocabulary Visual Recognition

Shuhuai Ren\({}^{}\), Aston Zhang\({}^{}\), Yi Zhu\({}^{}\), Shuai Zhang\({}^{}\), Shuai Zheng\({}^{}\),

**Mu Li\({}^{}\), Alex Smola\({}^{}\), Xu Sun\({}^{}\)**

\({}^{}\)National Key Laboratory for Multimedia Information Processing,

School of Computer Science, Peking University

\({}^{}\)Amazon Web Services

Correspondence to Aston Zhang <az@astonzhang.com>

###### Abstract

This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on \(21\) datasets, e.g., \(67.0\%\) average accuracy on \(10\) classification datasets (\(+3.1\%\) compared to CoOp) and \(84.4\) hIoU on open-vocabulary Pascal VOC segmentation (\(+6.9\) compared to ZSSeg). Our code is available at https://github.com/amazon-science/prompt-pretraining.

## 1 Introduction

It has been a new norm to formulate visual recognition tasks (e.g., image classification, object detection, and semantic segmentation) as language-guided visual recognition or vision-and-language problems . In language-guided visual recognition, categories of images are represented by natural language rather than discrete label IDs, and the semantics between images and their corresponding textual descriptions are often aligned via a constrastive loss during training . Model inference also becomes an image-to-text matching problem, where text prompts like "a photo of a [CLASSNAME]" are curated as text descriptions of images. By varying the [CLASSNAME] placeholder and computing the similarity between text descriptions and images, we can identify the most suitable class name and consider it as the predicted target class. A significant benefit of this language-guided paradigm is that it supports open-vocabulary inference, that is, zero-shot recognition for arbitrary categories that may not even have been seen during training, thanks to the flexibility in modifying the class names in the textual prompt .

As the context for class names, the text prompt plays a critical role in language-guided visual recognition models. A good prompt should holistically express the semantics of visual categories to better elicit the knowledge learned by vision-language models (VLMs) during pre-training. There are two popular types of prompts: hard prompts (e.g., a photo of a [CLASSNAME]) and soft prompts. Soft prompts are learnable token embeddings that can be fine-tuned given some input data, and have been demonstrated to be more effective and stable on downstream tasks than hard prompts . However, traditional prompt tuning methods usually fine-tune the soft prompt on task-specific datasets with a limited number of class labels, making it difficult to generalize to novel classes and across tasks . For example, when transferred between the two downstream datasets of Flowers102 and DTD , the soft prompt fine-tuned on DTD only achieves \(33.4\%\) accuracy on Flowers102, significantly lower than the \(61.8\%\) accuracy of a hand-crafted prompt on Flowers102, demonstrating a severe overfitting issue .

In this work, we aim to learn a universal soft prompt that covers a broad set of visual concepts while being task-agnostic. Specifically, we propose PrOMPt Pre-training (**POMP**), a method for scaling up prompt learning on the ImageNet-21K dataset, which has over twenty-thousand classes organized by the WordNet  hierarchy. The set of classes in ImageNet-21K includes general and long-tail visual categories of various semantic granularities, which have been proven to provide better downstream results for large models [30; 14]. Pre-training on this large-scale dataset helps condense semantic information into the soft prompt for universal visual discrimination. Once pre-trained, this universal prompt (i) can be easily applied to downstream datasets to improve model performance in zero-shot settings; (ii) is compatible with both region-level and pixel-level visual patterns, making it useful for various vision tasks such as object detection and semantic segmentation.

However, pre-training prompt with such a massive class set is challenging due to generally prohibitive computational costs. During prompt pre-training, the activation of the whole text encoder needs to be kept independently for every class and the memory consumption increases proportionally to the number of classes. In short, pre-training prompts on ImageNet-21K requires over \(300\) GB GPU memory with traditional methods like CoOp . In POMP, we solve this issue with a simple class sampling strategy, _local contrast_, which reduces the GPU memory requirement dramatically to less than \(16\) GB. Moreover, we propose a _local correction_ strategy to reduce the bias caused by class sampling and improve the generalization of the pre-trained prompt.

Experimental results in Figure 1 show that POMP outperforms previous state-of-the-art (SOTA) models on a broad range of visual recognition tasks and datasets. Specifically, compared to zeroshot CLIP , POMP improves the accuracy on ImageNet-21K by a gain of \(+2.9\%\). It also achieves an average accuracy of \(67.0\%\) when transferred to 10 downstream image classification datasets, which is \(3.1\%\) higher than CoOp . For semantic segmentation, POMP achieves \(39.1\) hIoU on open-vocab COCO Stuff and \(84.4\) hIoU on open-vocab Pascal VOC, outperforming ZSSeg  by \(+1.3\) and \(+6.9\) hIoU, respectively. For object detection, POMP achieves \(57.9\) and \(22.9\) AP\({}_{50}\) when transferred from LVIS to COCO and Object365, surpassing Detic  by \(+1.9\) and \(+0.8\) AP\({}_{50}\), respectively.

## 2 Related Work

### Language-Guided Visual Recognition

Language-guided visual recognition usually leverages VLMs as foundation models. Representative VLMs like CLIP  consist of an image encoder and a text encoder, which are used to encode image-text pairs into a joint feature space for learning the semantic alignment between vision and language [7; 46]. After being pre-trained on large-scale image-text pairs, CLIP-like models [28; 62; 33] are able to map images to their corresponding language descriptions, allowing visual recognition to generalize in the wild. This language-driven modeling paradigm also facilitates other vision tasks, including semantic segmentation [61; 32; 43; 13] and object detection [19; 15; 67]. These works typically designed a two-stage framework: it first leverages the pre-trained proposal network to extract features of specific visual patterns (e.g., segment mask and region) and then conducts classification in the same matching style as CLIP. The class descriptions for matching are synthesized using prompts, and in this work, we pre-train a soft prompt for VLMs on ImageNet-21K to further enhance their zero-shot generalization ability.

Figure 1: POMP outperforms previous state-of-the-art models on a broad range of visual recognition tasks and datasets.

### Prompt Tuning

In order to adapt VLMs to downstream tasks, recent research proposed a parameter-efficient tuning method named prompt tuning. CoOp  first proposed to replace the hand-crafted prompt with learnable vectors (also known as a soft prompt) for fine-tuning while freezing the entire pre-trained parameters. VPT , on the contrary, moved the learnable vectors from the text side to the image side, and proposed to concatenate the "visual soft prompt" and the patch sequence of an image as the input for fine-tuning. Prompt tuning was also used in other visual recognition tasks, such as object detection , semantic segmentation , and video recognition . Over manual prompt engineering, the soft prompt optimized with few-shot data has achieved significant performance improvements, but only fitting one specific downstream dataset.

To enhance the generalization of the soft prompt to a wider range of unseen classes and datasets, CoCoOp  modeled the context condition on input images. A recent work MaPLe  appended the soft prompt to the hidden representations at each layer in both the text and image encoder. In sharp contrast to previous approaches, we propose to pre-train a universal soft prompt on large-scale datasets with massive visual categories. Such a pre-trained prompt is task-agnostic, allowing for direct transfer to various downstream datasets without fine-tuning.

## 3 Method

We first review the process of classical prompt tuning for VLMs in SS 3.1. To address the training efficiency issue of previous methods, in SS 3.2 we introduce our method of prompt pre-training (POMP) that includes two key components: local contrast and local correction. Our pre-trained prompt can then be transferred to downstream datasets and tasks in a zero-shot manner as discussed in SS 3.3.

### Preliminaries

Language-guided visual recognition models like CLIP formulate image classification as an image-text matching problem, where the goal is to select the correct textual class name from a predefined class set for the image query. Following [65; 29], we consider CLIP as our vision-language foundation model, for its simplicity in design and wide applicability. CLIP consists of an image encoder \(f_{I}\) and a text encoder \(f_{T}\). Given a visual recognition dataset \(\) with a class set of \(N\) class names \(=\{c_{i}\}_{i=1}^{N}\), CLIP manually devises a hard prompt to synthesize textual descriptions \(t_{i}\) for each class name \(c_{i}\), e.g., \(t_{i}=\)"a photo of a \([c_{i}]\)". Then each class description is fed into the text encoder to generate the normalized class feature \(_{i}=f_{T}(t_{i})/\|f_{T}(t_{i})\|_{2}^{d}\), where \(d\) is the dimension of the feature. The concatenation of \(N\) class features \([_{1},,_{N}]^{N d}\) can be considered as the class weight of a linear classifier for classifying an image. Given an input image \(x\), the image encoder is used to extract its visual feature \(=f_{I}(x)/\|f_{I}(x)\|_{2}^{d}\). Finally, CLIP calculates the similarity between \(\) and all the class features, then predicts the class with the highest similarity as the target class.

To address the inefficient expressiveness of the manual prompt, previous research like CoOp  proposed to parameterize the manual prompt as a soft prompt \(\), and fine-tune it to fit downstream datasets. The soft prompt is made up of a sequence of learnable token embeddings \(=[_{1},_{2},, _{M}]^{M e}\), where \(M\) is a hyperparameter specifying the length of the soft prompt and \(e\) is the dimension of the token embedding. The token embeddings of each class name \(c_{i}\) are further appended to the soft prompt \(\) to generate the class feature \(_{i}^{()}\), and the prediction probability for the ground-truth class \(y\) is denotes as

\[P(y;)=^{} _{y}^{()}/)}{_{i=1}^{N}(^{ }_{i}^{()}/)},\] (1)

where \(^{}_{i}\) represents the similarity score and \(\) is a temperature parameter. The parameters of the soft prompt are updated by minimizing the cross-entropy loss :

\[()=}_{(,y)}[- P(y;)].\] (2)

The gradient of \(()\) is represented as:

\[_{}- P(y; )=-_{}(^{ }_{y}^{()})+_{i=1}^{N}P(y_{i} ;)_{}(^{} _{i}^{()}).\] (3)It can be decomposed into positive reinforcement for the ground-truth class and negative reinforcement for every class, which are the first and the second terms inside the square brackets of (3), respectively.

Note that both the image encoder and the text encoder are frozen during the prompt tuning process, which allows adapting the soft prompt efficiently to downstream data with very few learnable parameters. Methods along this direction of soft prompt learning include CoCoOp  and MaPLe . However, most previous works fine-tune _task-specific_ prompts, which limits their versatility and generalization .

### POMP: Prompt Pre-Training

Now we present our task-agnostic prompt pre-training method: POMP. Once pre-trained, the learned prompt can be directly used for downstream tasks without fine-tuning (see Figure 2). As introduced in SS 1, we propose to pre-train the soft prompt on the ImageNet-21K dataset for universal visual discrimination. Although the prompt tuning methods such as COOP and CoCoOp are parameter-efficient, they still incur computationally prohibitive training costs when applied to large-scale datasets with massive number of classes. Recall that the learnable parameters \(\) are embedded in the text input, while the loss is calculated at the output layer of the text encoder. For every class description, we need to allocate nearly \(15\) MB of GPU memory to preserve the state of the entire frozen encoder (Transformer-base  with \(12\) layers), and propagate the gradient back through the last layer to the first layer, to update the soft prompt \(\). Accordingly, the computational and caching cost of prompt tuning is proportional to the number of classes \(N\). As shown in Figure 3, for general large-scale datasets like ImageNet-21K with more than twenty-thousand classes, traditional tuning methods will allocate \(21\)K \( 15\) MB (more than \(300\) GB) of GPU memory, which is generally prohibitive.

To enable prompt tuning on massive classes and acquire the capability of global visual discrimination, we introduce a training-efficient algorithm called POMP, which reduces the GPU memory and training time of prompt tuning dramatically. POMP has two major components: **local contrast** and **local correction**. The former decreases the number of classes for contrastive learning through negative class sampling, and the latter reduces the bias caused by local contrast by adjusting the similarity scores of negative classes. We detail these two components in the following.

#### 3.2.1 Local Contrast

Discriminating all classes during contrastive learning is the source of training inefficiency. In order to alleviate this problem, we propose to narrow the scope of contrastive learning from global to local, and only require the model to identify the ground-truth class of the input image from a subset of the full class set. The class subset is sampled at each training step, allowing the model to discriminate within an ever-changing set of categories, and gradually restoring the relationship among all categories.

Specifically, given an input image, we sample \(K\) classes (\(K\) is much smaller than the total number of classes, \(N\)), including the ground-truth class \(y\) and \(K-1\) negative classes. We use a straightforward yet effective proposal distribution of uniform distribution for negative class sampling, where every negative class has an equal probability, i.e., \(p=1/(N-1)\), of being sampled. We also explore alternative types of proposal distribution, such as frequency-based and similarity-based distributions. However, our experiments reveal that POMP with the simple uniform distribution considers both common and rare classes, as well as easy and difficult classes, resulting to the best performance. Please refer to Appendix E.1 for details.

After sampling, we denote the set of the negative classes as \(\), with \(||=K-1\). By using the local contrast, we can significantly reduce the training overhead to a fraction of \(K/N\) of the original one. Upon completion of training, we can use the full class set to compute the prediction probability of each image. Overall, the motivation behind our local contrast is analogous to that in the NCE-based contrastive learning frameworks . In these frameworks, the contrast is performed by sampling a batch of instances due to computational limitations and computing the loss within the batch as an empirical estimation for the expected contrastive loss .

#### 3.2.2 Local Correction

Given that the local contrast component necessitates a reduced number of negative classes, the negative reinforcement in the vanilla gradient in (3) is diminished to \(K/N\). As a result, the prompt optimization direction is inevitably biased due to the absence of other negative classes. To mitigate this bias and enhance the model performance, we add a local correction term \(m\) to the logits of the sampled negative classes \(^{}_{i}^{()}/\ (i y)\), which serves as a margin  between the positive and the negative logits. Accordingly, the final prediction probability of POMP is denoted as:

\[(y;)=^{}_{y}^{()}/)}{(^{} _{y}^{()}/)+_{i} (^{}_{i}^{()}/+m)}.\] (4)

The local correction term \(m\) encourages the positive logit to be larger than the negative logits by a certain margin, resulting in a more stringent decision boundary:

\[C_{+}:^{}_{y}^{()}/ ^{}_{i}^{()}/+m, i y.\]

Therefore, compared to the prediction probability without local correction, (4) makes the decision boundary more robust against the unsampled negative classes and enforces the learning of more discriminative class features . This will improve model regularization and robustness across datasets and domains (to be shown in SS 4.4). Different from other margin-based losses that use a fixed margin , our margin \(m\) is designed to adaptively adjust itself based on the value of \(K\):

\[m=-(K-1)/(N-1).\] (5)

It is worth noting that \(m\) in (5) is a positive scalar. When \(K=N\), all classes are included during optimization, and \(m\) equals zero. In this case, (4) degenerates to the standard prediction probability in (1). As the value of \(K\) decreases and the number of visible classes is reduced, the margin \(m\) increases to create space for potential class features in the representation space. Our adaptive margin outperforms the fixed margins (which are specified as hyper-parameters) for various \(K\), allowing models to maintain optimal performance under different computing budgets (to be shown in SS 4.4).

### Zero-Shot Transfer Learning

As shown in Figure 2, after pre-training, our POMP prompt can be used to synthesize class features for classification with an arbitrary class set, supporting zero-shot inference on downstream datasetsand tasks. In order to plug the POMP prompt into other visual tasks like semantic segmentation and object detection, we adopt a **two-stage framework**. In stage one, we use a pre-trained proposal network to generate a set of mask or region proposals. In stage two, we classify each proposal with the class features generated by our POMP prompt. Experiments in SS 4.3 will show that the POMP prompt can handle both pixel-level and region-level visual patterns, leading to improved performance in segmentation and detection tasks.

## 4 Experiments

### POMP Prompt Pre-Training

We take CLIP (ViT/B-16)  as the backbone and conduct prompt pre-training on the ImageNet-21K dataset. The number of training samples for each class is \(16\) (\(16\) shots), and the prompt length is \(16\). We sample 1,000 classes at each training step, i.e., \(K=1000\) in (4). See Appendix A for details.

### End Task Setups and Implementation Details

To evaluate the generalization of the pre-trained prompt, we directly transfer it to downstream tasks and datasets. Appendix C lists the details of all the datasets. We follow previous works [58; 61; 19; 67] to designate data belonging to two class sets as **source data** and **target data**, respectively. The proposal networks are pre-trained on the source data with the source class set, while conducting zero-shot evaluation on the target data with the target class set. There are two protocols for the source-target data split. The first is the **open-vocabulary protocol**, where the class set of one dataset is divided into two disjoint groups for the source and target data. The second protocol is the **cross-dataset protocol**, in which the source and target data are from two independent datasets with potentially overlapping class sets. See Appendix B for implementation details.

### Results and Analysis

#### 4.3.1 Prompt Pre-Training on ImageNet-21K

Table 1 shows the results of POMP prompt on the ImageNet-21K test set after pre-training. The traditional prompt learning methods (e.g., CoOp and MaPLe) are trained on the ImageNet-1K dataset due to their prohibitive computational cost if trained on ImageNet-21K (more than \(300\) GB of GPU memory). On the contrary, our POMP prompt, pre-trained on ImageNet-21K using less than \(16\) GB of GPU memory, achieves the highest accuracy of \(25.3\%\) based on the CLIP (ViT-B/16) backbone, which surpasses ZeroshotCLIP by \(3.5\%\) and Linear Probe by \(4.4\%\). The VPT method, which uses visual prompts on the image side, does not require training overhead proportional to the number of classes, making it applicable to the ImageNet-21K dataset. VPT prepends independent learnable vectors to the hidden states of each layer in the visual backbone, surpassing linear probing and the previous prompt tuning methods. However, its performance based on ViT-B/16 is still \(0.5\%\) worse than ours, demonstrating that our POMP prompt can better distinguish a large number of general visual categories. In addition, our method is agnostic to the backbone architectures like ResNet and ViT, and the improvement is consistent.

Cross-dataset and Cross-domain Image Classification.Our POMP prompt, which has been pre-trained on a large number of classes, demonstrates a strong generalization ability. As shown in Table 2, POMP achieves the highest average accuracy of \(67.0\%\) when transferred to \(10\) downstream image classification datasets, outperforming CoOp by \(3.1\%\) and surpassing the previous SOTA in

   Method & ResNet50 & ViT-B/32 & ViT-B/16 \\  ZeroshotCLIP  & 17.5 & 19.8 & 21.8 \\ Prompt Ensemble  & 18.8 & 20.9 & 23.5 \\  CoOp  & 16.6 & 18.1 & 20.8 \\ MaPLe  & - & 21.6 & 24.2 \\  Linear Probing  & 6.5 & 18.2 & 20.9 \\ VPT  & - & 21.8 & 24.8 \\ POMP (Ours) & **20.2** & **22.2** & **25.3** \\   

Table 1: Performance on the ImageNet-21K test set. ZeroshotCLIP and Prompt Ensemble in the top block conduct zero-shot inference. CoOp and MaPLe, indicated in gray in the middle block, are trained on the ImageNet-1K dataset due to prohibitive GPU memory consumption if trained on ImageNet-21K. The remaining methods in the bottom block are trained on ImageNet-21K.

\(7/10\) datasets. This is due to the fact that, after learning with enormous long-tail categories, POMP can provide a more expressive context for fine-grained visual concepts such as specific _objects_ and _scenes_, resulting in improved performance on datasets like StanfordCars (\(+1.2\%\)) and Aircraft (\(+0.9\%\)), as well as SUN397 (\(+0.7\%\)) and EuroSAT (\(+4\%\)). Furthermore, POMP is more robust to domain shift and achieves a new SOTA with \(60.8\%\) accuracy on \(4\) out-of-domain variants of the ImageNet dataset.

Training Efficiency.POMP also achieves comparable accuracy to the classical prompt tuning methods when fine-tuning on specific downstream datasets, but significantly reduces the training cost. Table 3 shows the performance of prompt tuning on ImageNet-1K, using a visual backbone of ViT-B/16 and \(16\) shots. The epoch is \(50\) for CoOp and POMP, and \(10\) for CoCoOp. CoOp generates all the \(1000\) class features at each training step, which consumes \(28\) GB of memory and takes \(5.9\) hours to finish the fine-tuning. The training time of CoCoOp is even longer because it devises instance-specific prompts that require an independent forward pass for each image. Compared to these baselines, POMP (\(K=128\)) achieves competitive accuracy on ImageNet-1K while using less than \(19\%\) of GPU memory and \(50\%\) of training time, demonstrating its superiority.

#### 4.3.2 Open-Vocabulary Semantic Segmentation

Table 4 shows the results of our method on open-vocabulary COCO Stuff and Pascal VOC. POMP outperforms the previous state-of-the-art method, ZSSeg , with a higher hIoU of \(39.1\) and mIoU-unseen of \(38.2\) on COCO Stuff. On Pascal VOC, the improvement of POMP is more significant with \(+6.9\) hIoU and \(+4.3\) mIoU-unseen. Figure 4 illustrates qualitative results on open-vocabulary COCO-Stuff, where POMP demonstrates a stronger ability to distinguish background categories compared to ZSSeg. For example, in case (1), ZSSeg misclassifies the classes of _playingfield_ as _dirt_, while the POMP prompt with richer contextual semantics better expresses the difference between regular land and the playingfield with specific textures, thus facilitating the matching of the visual region with the ground-truth class.

POMP also demonstrates its generalization ability in cross-dataset settings. Taking standard COCO Stuff as the source dataset for mask proposal network pre-training, POMP achieves \(20.7\) mIoU and \(51.1\) mIoU when transferred to the target datasets of ADE20K and PASCAL Context, respectively, outperforming ZSSeg by \(+1.3\) mIoU and \(+0.3\) mIoU. Overall, POMP obtains remarkable gains over previous works in all settings.

#### 4.3.3 Open-Vocabulary Object Detection

    &  &  \\   & & & & & & & & & & & & & & & & & \\  hard prompt & 93.3 & 88.2 & 65.6 & 67.4 & 85.3 & 23.7 & 62.6 & 44.3 & 42.0 & 65.1 & 63.7 & 60.9 & 46.1 & 47.8 & 74.0 & 57.2 \\  CoOp  & 93.7 & 89.1 & 64.5 & 87.8 & 83.5 & 84.2 & 41.9 & 46.4 & 66.6 & 63.9 & 64.2 & 48.0 & 49.7 & 75.2 & 59.3 \\ CoCoOp  & 94.4 & 90.1 & 65.3 & 71.9 & 86.1 & 22.9 & 67.4 & 45.7 & 45.4 & 68.2 & 65.7 & 64.1 & 48.8 & 50.6 & 76.2 & 59.9 \\ LASP  & 94.5 & 89.4 & 64.8 & 70.5 & 86.3 & 23.0 & 67.0 & 45.5 & 48.3 & 68.2 & 65.8 & 63.8 & 49.0 & 50.7 & 77.1 & 60.1 \\ VPT  & 93.7 & **90.6** & 65.0 & 70.9 & 86.3 & 24.9 & 67.5 & 36.1 & 45.9 & 68.7 & 66.0 & **64.2** & 49.2 & 51.3 & 77.0 & 60.4 \\ MaPLe  & 93.5 & 90.5 & 65.6 & 72.2 & 86.2 & 24.7 & 67.0 & **46.5** & 48.1 & **68.7** & 66.3 & 64.1 & 49.2 & 50.9 & 77.0 & 60.3 \\ POMP (Ours) & **95.0** & 89.5 & **66.8** & **72.4** & **86.3** & **25.6** & **67.7** & **46.2** & **52.1** & 68.5 & **67.0** & 63.8 & **49.8** & **51.6** & **77.9** & **60.8** \\   

Table 2: Cross-dataset and cross-domain evaluation for image classification. The backbone is ViT/B-16. Overall, POMP achieves the highest average accuracy, indicating better generalization.

  Method & Acc. (\%) & GPU Mem. (GB) & Training Time (h) \\  CoOp & 71.9 & 28.2 & 5.9 \\ CoCoOp & 70.1 & 28.3 & 27.5 \\  POMP (\(K=128\)) & 71.2 & 5.3 & 2.7 \\ POMP (\(K=256\)) & 71.4 & 8.8 & 3.3 \\ POMP (\(K=512\)) & 71.6 & 15.9 & 4.2 \\  

Table 3: Prompt tuning on ImageNet-1K. POMP (\(K=128\)) achieves comparable accuracy with CoOp and CoCoOp, but using less than 19% GPU memory and 50% training time.

We compare POMP with state-of-the-art methods on the open-vocabulary LVIS benchmarks and report results in Table 7. POMP achieves AP\({}_{r}\) of 26.8 for object detection and \(25.2\) for instance segmentation. See Appendix D for qualitative results. Under the cross-dataset setting, we pre-train the visual backbone on the source dataset of standard LVIS, and evaluate the recognition ability on COCO and Object365. As shown in Table 6, compared to Detic, POMP provides a gain of \(1.9\) AP\({}_{50}\) on COCO and \(0.8\) AP\({}_{50}\) on Object365, respectively.

### Ablation Study

We decouple the two components of local contrast and local correction in POMP, and conduct an ablation study to examine their individual contributions. Since removing the local contrast component will lead to prohibitive training cost, we investigate the impact of this component by varying the number of sampled classes \(K\). As shown in Table 8, the performance of POMP improves as \(K\) increases. As discussed in SS 4.3.1 and Table 3, the local contrast component balances accuracy and cost by adjusting \(K\).

On the other hand, as shown in Table 8, removing the local correction component from POMP (\(K=1000\)) results in a decline of \(1.2\) and \(1.0\) in the average accuracy of cross-dataset and cross-domain transfer, respectively. This indicates that local correction significantly improves the

    &  &  &  \\   & AP & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) & AP & AP\({}_{}\) & AP & AP\({}_{}\) & AP & AP\({}_{}\) & AP & AP\({}_{}\) & AP & AP\({}_{}\) & AP & AP\({}_{}\) \\  ViLD\({}^{*}\) & 27.5 & 41.8 & 29.3 & 20.6 & 35.9 & 43.4 & 34.1 & 52.3 & 36.5 & 21.6 & 38.9 & 46.1 & 11.5 & 17.8 & 12.3 & 4.2 & 11.1 & 17.8 \\ Deftp  & 28.4 & 42.9 & 30.3 & 21.0 & 36.7 & 44.1 & 34.9 & 53.8 & 37.4 & 22.5 & 39.6 & 46.3 & 12.1 & 18.8 & 12.9 & 4.5 & 11.5 & 18.6 \\  Detic  & 36.8 & 50.7 & 38.6 & 26.6 & 1.6 & 47.7 & 51.7 & 38.8 & 56.0 & 41.9 & 25.6 & 42.2 & 50.0 & 15.6 & 22.1 & 16.8 & 6.1 & 15.6 & 23.8 \\ POMP (Ours) & **37.2** & **51.1** & **39.3** & **26.5** & **47.2** & **52.6** & **40.3** & **57.9** & **43.6** & **28.3** & **43.9** & **50.6** & **16.1** & **22.9** & **17.3** & **6.2** & **16.3** & **24.7** \\   

Table 6: Cross-dataset evaluation for object detection. The region proposal network is pre-trained on standard LVIS. POMP and Detic share the same region proposal network and training strategy.

    &  &  \\   &  &  &  \\   &  &  &  &  \\  SPNet  & 16.8 & 20.5 & 14.3 & 21.8 & 73.3 & 15.0 \\ ZS3  & 15.0 & 34.7 & 9.5 & 28.7 & 77.3 & 17.7 \\ CoNet  & 18.2 & 35.5 & 12.2 & 39.7 & 78.4 & 25.6 \\ ZegFormer  & 34.8 & 36.6 & 33.2 & 73.3 & 86.4 & 63.6 \\  ZSSeg  & 37.8 & 39.3 & 36.3 & 77.5 & 83.5 & 72.5 \\ POMP (Ours) & **39.1** & **39.9** & **38.2** & **84.4** & **93.6** & **76.8** \\   

Table 4: Comparison with state-of-the-art methods on COCO Stuff dataset and Pascal VOC dataset. POMP and ZSSeg share the same mask proposal network and training strategy.

    &  &  \\   & AP & AP\({}_{}\) & AP\({}_{}\) & AP & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) \\  ViLD\({}^{*}\) & 16.7 & 26.5 & 34.2 & 27.8 & **16.6** & 24.6 & 30.3 & 25.5 \\ Deftp  & 20.8 & 27.8 & 32.4 & 28.4 & **19.8** & 25.6 & 28.9 & 25.9 \\ Deftp  & - & - & - & - & - & 21.4 & 23.3 & 29.3 & 25.3 \\  Detic  & 26.7 & 36.4 & 40.3 & 36.3 & **24.9** & 32.5 & 35.6 & 32.4 \\ POMP (Ours) & **26.8** & 36.4 & 40.4 & 36.2 & **25.2** & 33.0 & 35.6 & 32.7 \\   

Table 8: Ablation on the local contrast and local correction in POMP based on the CLIP (ViT/B-16).

Figure 4: Qualitative results on open-vocabulary COCO-Stuff. Compared to ZSSeg, POMP correctly identifies the background category of playingfield (left) and river (right).

    &  &  \\   &  &  &  &  \\   & hIoU & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\  SPNet  & 16.8 & 20.5 & 14.3 & 21.8 & 73.3 & 15.0 \\ ZS3  & 15.0 & 34.7 & 9.5 & 28.7 & 77.3 & 17.7 \\ CoNet  & 18.2 & 35.5 & 12.2 & 39.7 & 78.4 & 25.6 \\ ZegFormer  & 34.8 & 36.6 & 33.2 & 73.3 & 86.4 & 63.6 \\  ZSSeg  & 37.8 & 39.3 & 36.3 & 77.5 & 83.5 & 72.5 \\ POMP (Ours) & **39.1** & **39.9** & **38.2** & **84.4** & **93.6** & **76.8** \\   

Table 5: Cross-dataset evaluation for semantic segmentation. The mask proposal network is pre-trained on pre-trained on standard COCO Stuff.

generalization of the pre-trained prompt. Furthermore, we analyze the impact of the adaptive margin (5) in the local correction. We pre-train prompts on ImageNet-21K with varying \(m\) values (\(0,0.5,1,1.5\)) and report the cross-dataset accuracy. Notably, our local correction method dynamically sets \(m\) to \(1.5\) when \(K=319\), and \(m\) to \(1\) when \(K=1000\). When the number of sampled classes is relatively small (\(K=319\)), increasing the margin creates more space for potential negative classes, thereby improves cross-dataset accuracy. Conversely, for large \(K\) values (e.g., \(1000\)), imposing a very large margin (\(m=1.5\)) disrupts the natural class distribution and diminishes generalization ability. Overall, compared to the fixed margins [11; 54], our adaptive margin decreases as \(K\) increases, achieving optimal performance across different computing budgets (controlled by \(K\)) and sparing the time for extensive hyper-parameter search. See Appendix E for more ablation studies on the number of shots and prompt length.

### Understanding the Pre-trained Prompt

To better understand the pre-trained prompt, we analyze the feature space of POMP through the properties of alignment and uniformity . Intuitively, the image feature and its ground-truth class feature are supposed to stay closed (alignment). Besides, all the class features should be uniformly distributed to preserve maximal information and make the categories more distinguishable (uniformity). We use the alignment and uniformity loss in the vision-and-language field [45; 63] for representation probing. The alignment loss calculates the expected distance between features of an image \(\) and its ground truth class \(_{y}^{()}\):

\[_{}}_{(,y)}\|-_{y}^{()}\|^{2},\] (6)

while the uniformity loss measures how well the class features \(^{()}\) are uniformly distributed:

\[_{}}_{1  i,j N,\\ i j}(-2\|_{i}^{()}-_{j}^{()}\|^{2}).\] (7)

We visualize the alignment and uniformity measures of POMP and the previous SOTA, MaPLe, in Figure 5. For both measures, lower numbers are better. The circle of POMP in the figure is located in the lower left with the lightest color, indicating relatively smaller losses and the best performance under the cross-dataset setting. Compared with the method without local correction, POMP significantly reduces the uniformity loss at only a slight expense of alignment. In other words, our pre-trained prompt not only ensures the alignment of the image and the ground-truth class, but

Figure 5: \(_{}\) and \(_{}\) of POMP. For both measures, lower numbers are better. The color of circles and the numbers in the boxes denote the average cross-dataset accuracy over \(10\) datasets (higher is better).

  m & K=319 & K=1000 \\ 
0 & 65.2 & 65.8 \\
0.5 & 65.7 & 66.1 \\
1 & 66.2 & **67.0 (Ours)** \\
1.5 & **66.5 (Ours)** & 66.4 \\  

Table 9: Ablation on the adaptive margin \(m\) in the local correction.

Figure 6: Projection of image features (points), class features of POMP (intersections of solid lines and sphere) and class features of CoOp (intersections of light dash-dot lines and sphere). Each color represents a class. Class features of POMP have better alignment with centroids of the corresponding images, and are distributed with better uniformity.

also disperses the class features in the representation space, thereby improving the generalization and robustness of the model. The visualization of the feature space in Figure 6 also verifies our superiority. The endpoints of the POMP class features are closer to the centroids of the image features, indicating better alignment and reduced \(_{}\) loss (from \(1.39\) to \(1.36\) on Aircraft and from \(1.41\) to \(1.36\) on UCF101). Furthermore, the larger angles between the POMP class features demonstrate better feature uniformity and reduced \(_{}\) loss (from \(-0.66\) to \(-0.81\) on Aircraft and from \(-0.95\) to \(-1.23\) on UCF101) compared to CoOp.

## 5 Conclusion

We present POMP to pre-train a general soft prompt on ImageNet-21K for universal visual discrimination. The learned prompt can be easily plugged into various visual recognition datasets and tasks for zero-shot inference. Experiments on open-vocabulary image classification, semantic segmentation, and object detection show that POMP surpasses previous methods by a considerable margin.

## Limitations

To facilitate future research, we analyze the limitations in our work and propose potential solutions. **(1)** We present the local contrast and use the loss within a subsampled class set as an empirical estimation for the expected contrastive loss within the full class set. However, the theoretical risk of such an estimation is urged to be investigated. **(2)** ImageNet-21K comprises a vast number of classes that are organized based on a semantic structure. By leveraging the hyponym and hypernym relations provided by WordNet synsets, we can derive the parent class and a list of child classes for each class. We believe that utilizing the semantic information holds the potential to further enhance performance. **(3)** Despite the excellent performance exhibited by our pre-trained prompt, its interpretability poses a significant challenge because the context vectors are optimized in a continuous space. We leave it as future work.