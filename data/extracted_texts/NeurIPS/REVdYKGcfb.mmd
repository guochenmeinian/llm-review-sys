# What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration

Libo Qin\({}^{}\)1 Qiguang Chen\({}^{}\)1 Hao Fei\({}^{}\) Zhi Chen\({}^{}\) Min Li\({}^{}\) Wanxiang Che\({}^{}\)

\({}^{}\) School of Computer Science and Engineering, Central South University

\({}^{}\) Research Center for Social Computing and Information Retrieval

\({}^{}\) Harbin Institute of Technology \({}^{}\) Tsinghua University \({}^{}\) ByteDance

lbqin@csu.edu.cn, qgchen@ir.hit.edu.cn

Equal Contribution

###### Abstract

Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: "_What factors affect the performance of MM-ICL?_" To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies. Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts. We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research.

## 1 Introduction

Recently, Large Language Models (LLMs) have demonstrated remarkable advancements, showcasing proficiency in a wide range of tasks . Notably, advanced LLMs exhibit the emergence of novel capabilities such as In-Context Learning (ICL) , which optimize task performance by incorporating demonstrations into input prompts . In particular, multi-modal in-context-learning (MM-ICL) is capable of utilizing multi-modal demonstrations to quickly adapt to the downstream task without parameter tuning .

In the literature, a series of works emerge to enhance MM-ICL. Specifically, Gong et al.  manually create a general template with multiple images and corresponding responses during instruction-tuning (IT) stage to improve MM-ICL. Tsimpoukelli et al. , Li et al. , Doveh et al.  and Zhao et al.  develop task-specific MM-ICL templates during the IT stage, further extending its capabilities across more domains. Li et al.  introduce OtterHD, adapting MM-ICL for high-definition image tasks. Furthermore, Sun et al.  and Tian et al.  explore the potential of MM-ICL in the image generation tasks. Jin et al.  provide compelling evidence for the effectiveness of MM-ICL in comprehending game instructions. Zong et al.  and Shukor et al.  develop fine-grained benchmarks and evaluate the MM-ICL in classification tasks.

While significant progress has been witnessed in MM-ICL, the existing work still mainly focuses on how to optimize MM-ICL, ignoring the underlying factors that influence its effectiveness andperformance. Such gap impedes a comprehensive understanding of the mechanisms and performance determinants of MM-ICL, thereby limiting further exploration and research in this field. Motivated by this, this paper aims to systematically investigate the research question: _What factors affect the performance of MM-ICL?_, hoping to offer a unified view and guideline for researchers to build better MM-ICL. Specifically, as illustrated in Figure 1, the MM-ICL process comprises three steps: demonstration retrieval, demonstration ordering, and prompt construction. Therefore, We systematically investigate the following sub-questions: (a) _how to select multi-modal demonstrations_ (Sec. 3.1); (b) _how to order multi-modal demonstrations_ (Sec. 3.2); and (c) _how to construct MM-ICL prompts_ (Sec. 3.3) to this end. To achieve this, we conduct detailed experiments on MM-ICL using 20 strategies across 4 tasks with 6 representative vision large language models (VLLMs).

Through extensive investigations, the main findings are as follows:

* **Multi-modal alignment is the bottleneck for MM-ICL.** Our analysis confirms that, on average, multi-modal retrieval methods outperform single-modal ones. Furthermore, multi-modal alignment in VLLMs has a greater impact on MM-ICL effectiveness than parameter size, identifying alignment as the key limitation in both backbone structure and demonstration quality.
* **Intra-demonstration ordering holds greater importance than inter-demonstration ordering.** Our investigation first indicates that the intra-demonstration ordering, particularly the ordering of modalities, greatly influences model performance more than demonstration arrangement.
* **Introductory instruction guides better task understanding for MM-IC.** To construct a comprehensive MM-ICL prompt, it is essential to include introductory instructions preceding the demonstrations. This approach consistently enhances the performance of MM-ICL compared with summative instruction placed after demonstrations, and intra-demonstration instruction.

## 2 Background

In this work, we formally present the prompt building process for MM-ICL. As depicted in Figure 1, the process of prompt building for MM-ICL involves three sequential stages:

**(1) Demonstration Retrieval:** The core MM-ICL requires retrieval to obtain demonstrations that can help MM-ICL. Formally, given a validation dataset \(_{n}=\{x_{1},x_{2},,x_{n}\}\), each multi-modal sample \(x_{i}\) includes textual input \(I_{i}^{txt}\), visual input \(I_{i}^{vis}\), and output \(O_{i}\). For a specific test query \(q\), this step aims to identify a subset of relevant demonstrations \(_{k}=\{x_{_{j}}\}_{j=1}^{k}\), where \(x_{_{j}}_{n}\).

**(2) Demonstration Ordering:** Researches (Lu et al., 2022; Wu et al., 2023; Xiang et al., 2024) show that LLMs are highly sensitive to the order of demonstrations. Thus, arranging these demonstrations effectively is crucial for MM-ICL. After retrieving relevant demonstrations, we must rearrange the sequence \(_{k}=[x_{_{j}}]_{j=1}^{k}\), which will be used to construct the prompt.

**(3) Prompt Construction:** Previous research indicates that using delimiters and instructions can significantly enhance textual ICL capabilities (Min et al., 2022; Qin et al., 2023). Therefore, the final core step is to transform the ordered demonstrations into a structured prompt \(\), incorporating delimiters and instructions to optimize MM-ICL.

## 3 What Factors Affect Multi-modal In-Context Learning?

### Exploration of MM-ICL Demonstration Retrieval

The efficacy of ICL heavily depends on the quality of the retrieved demonstrations \(\), which provide essential prior knowledge for MM-ICL. As illustrated in Figure 2, the retrieval process encompasses

Figure 1: The whole process of prompting creation for multi-modal in-context-learning.

three key steps: (1) Sample Representation, (2) Sample Comparison, and (3) Sample Selection. In this section, we conduct a systematic analysis of how various strategies for sample representation, comparison, and selection affect MM-ICL task performance.

**Sample Representation.** It involves defining an encoder (\(()\)) to map each input sample \(x_{j}\) and user query \(q\) into a shared representation space:

\[h_{j}=(x_{j}).\] (1)

Specifically, we evaluate various encoder architectures across modalities, focusing on the impact of visual encoder (\(_{vis}\)), text encoder (\(_{txt}\)), and multi-modal encoder (\(_{multi}\)) on model performance.

**Sample Comparison.** After deriving the representations, we employ a metric \(\) to evaluate the quality \(_{j}\) of the sample \(h_{j}\) in comparison to the query representation \(h_{q}\) and the dataset samples \(h_{j}\):

\[_{j}=(h_{q},h_{j}).\] (2)

Specifically, we explore various comparison metrics, including cosine similarity \(_{cos}\)(Liu et al., 2022), L2 similarity \(_{L2}\)(Liu et al., 2022), and semantic diversity \(_{div}\)(Li and Qiu, 2023), to assess sample quality and understand the correlation with model performance.

**Sample Selection.** After quality assessments, we apply a selection criterion \(\) to identify the \(k\) most advantageous samples \(x_{_{j}}\) for inclusion in the demonstration set \(\):

\[=\{x_{_{j}}|x_{_{j}}(q,_{j}),j k\}.\] (3)

Sample selection is guided by factors such as domain information (He et al., 2023), demonstration style (Agrawal et al., 2023), and token distance (Liu et al., 2022). Specifically, we systematically examine samples from both in-domain and out-of-domain collections. And we also assess the impact of image style on the selected demonstrations. Further, we investigate the token distance between modalities to understand its effects on sample selection for MM-ICL.

### Exploration of MM-ICL Demonstration Ordering

Following Lu et al. (2022) and Wu et al. (2023), the order of the demonstration set \(\) significantly impacts MM-ICL performance. As shown in Figure 3, this section explores two key aspects:

**Intra-demonstration Ordering.** The sequence within a demonstration, especially modalities (e.g., text and image), is an important component that might affect the MM-ICL capabilities. Therefore,

Figure 2: The demonstration retrieval process for MM-ICL.

we introduce a intra-demonstration ordering permutation (IOP) to define this sequence:

\[=[(x_{_{1}}),(x_{_{2}}),,(x_{_{k}})].\] (4)

We conduct a systematic exploration of various IOP configurations, including text-image-text (IOPtvt), text-text-image (IOPtvt), and image-text-text (IOPvtt). These order analyses aim to evaluate the impact of different modal sequences on the model's performance.

**Inter-demonstration Ordering.** The sequence in which demonstrations are organized within \(\) also is the key component that might impact the performance of MM-ICL. Formally, we define a sample ordering permutation \(_{j}\) to specify the arrangement:

\[=[x_{_{1}},x_{_{2}},,x_{_{k}}|x_{_{j }}],\] (5)

where \(x_{_{j}}\) represents the \(j\)-th demonstration in the ordered demonstration list.

### Exploration of MM-ICL Prompt Construction

VLLMs are highly sensitive to input instructions (Kojima et al., 2022; Qin et al., 2023). Inspired by this, to enhance task comprehension, we incorporate different instructions to explore the performance influence for MM-ICL. Formally, we construct instruction methods \(()\) that describe the task and position them within the prompt. The prompt construction process is:

\[=((x_{_{1}}),(x_{_{2}}),, (x_{_{k}})),\] (6)

Specifically, as shown in Figure 4, we explore three instruction categories to bolster MM-ICL process:

* **Introductory Instruction (\(_{intro}\))** refers to the initial guidance that offers an overview of the task prior to any demonstrations. As shown in Figure 4 (a), this instruction, denoted as \(_{intro}\), is positioned at the start of the ordered demonstration sequence, \(\).

Figure 4: The process of instruction injection for MM-ICL prompt construction involves three key elements. The Introductory Instruction provides an overview instruction of the task before demonstrations. The Summative Instruction summarizes after the examples, guiding the model to apply the learned concepts to practical problems. The Intra-demonstration Instruction embeds task-specific guidance within each demonstration, enabling VLLMs to grasp task requirements during learning. Further details and additional prompts are provided in Appendix C.3.

Figure 3: The demonstration ordering process for MM-ICL.

* **Summative Instruction (\(_{sum}\))** offers a summary after the examples, guiding the model to apply the learned concepts to real-world problems. As shown in Figure 4 (b), this instruction \(\) is added at the end of the demonstration list \(\).
* **Intra-demonstration Instruction (\(_{intra}\))** embeds task instructions within each demonstration, helping VLLMs understand the task requirements during the learning process. As shown in Figure 4 (c), this instruction \(\) is included within each demonstration \(x_{i}\) in the list \(\).

## 4 Experimental Setup

Following the setting of Li et al. (2023), we systematically explore 4 tasks, including image-caption, visual question answering (VQA), image classification, and chain-of-thought reasoning, which come from M\({}^{3}\)IT (Li et al., 2023) and M\({}^{3}\)CoT (Chen et al., 2024) (as shown in Tables 2), providing a universal paradigm can help researchers conduct unified and fairer comparisons and studies within a unified framework. In order to evaluate the MM-ICL performance accurately, we use two indicators for each task. Following Zhang et al. (2019), Li et al. (2023), and Zong et al. (2024), we use CIDER (Vedantam et al., 2015) and BertScore (Zhang et al., 2019) as image-caption metrics. Since M\({}^{3}\)IT includes various VQA tasks with free-form answers, inspired by the success of free-form and precise answer hybrid evaluation in machine reading comprehension, following Rajpurkar et al. (2016), Zhang et al. (2019), we adapt Token-F1 (Rajpurkar et al., 2016) and BertScore as visual question answering (VQA) metrics (The correlation analysis of the indicators and accuracy as shown in Table 3). Following Li et al. (2023, 2022), we use accuracy and F1 score as indicators of image classification. Following Lu et al. (2022), Golovneva et al. (2022) and Qin et al. (2023), we use accuracy and reasoning alignment score (Golovneva et al., 2022) (RAS) as indicators of reasoning.

To ensure rigorous experimental control, we established a baseline using a multi-modal encoder for data representation and cosine similarity for sample comparison, limiting retrieval to within the same task. This baseline ranks samples based on similarity, with a delimiter and a 3-shot setting (see Appendix A for details). In addition, all open source models complete inference on 2 A100 80G. For all experiments, we select top-p from \(\{0.95,1\}\) and adjust the temperature parameter within \(\). Among them, temperature is the main error variable in this work.

## 5 Empirical Analysis of Factors Affecting MM-ICL

### Empirical Analysis of MM-ICL Demonstration Retrieval

#### 5.1.1 Sample Representation

**Multi-modal alignment is the bottleneck for MM-ICL in both backbones and demonstrations.** To evaluate the impact of semantic representation in different modalities for MM-ICL, we assessed three distinct encoders: RoBERTa (Liu et al., 2019) as a textual encoder for Textual Retriever, CLIP-Vision Encoder (Radford et al., 2021) for Visual Retriever, and BridgeTower (Xu et al., 2023) as multi-modal encoder for Multi-Modal Retriever. As illustrated in Table 1, multi-modal retrieval consistently outperforms zero-shot, randomly selected, and single-modality methods, highlighting the advantages of multi-modal semantic learning for MM-ICL. What's more, as shown in Table 1, our results reveal that increasing model parameters from 8 billion to over 100 billion does not significantly enhance performance, suggesting that beyond parameter size, multi-modal context understanding and alignment are more crucial for MM-ICL than model scaling. Our analysis demonstrates that multi-modal alignment is the critical factor in both the backbone and demonstrations.

**Current multi-modal encoders still lack modeling of multi-modal logic.** Actually, multi-modal retrieval attains better performance in many scenarios like Image Caption and VQA. However, our experiments show that textual retrieval works well for classification and reasoning tasks. Based on the qualitative analysis, we observe that due to the semantic richness of the labels and rationales, textual retrieval can obtain more similar samples. However, the current multi-modal retrieval struggles with complex text semantics, often favoring image similarity. This aligns with recent work (Tong et al., 2023, 2024; Fei et al., 2024), which is valuable for future exploration.

**Multi-modal context diminishes the necessity of careful demonstration selection.** As shown in Table 1, adding relevant demonstrations slightly improves performance, but the gains are less significant compared to text-only ICL scenarios. Specifically, retrieved demonstrations yield an average performance boost of 3.84%, compared to random demonstrations. In contrast, text-only scenarios show performance increases of over 10% with carefully selected samples . Furthermore, the model remains unaffected by irrelevant samples, and the performance of almost all models is higher than zero-shot. This indicates that multi-modal context significantly reduces the need for careful demonstration selection, unlike in text-only scenarios.

**VLLMs learn semantic representations instead of token pattern representations for MM-ICL.** As depicted by Agrawal et al. , textual ICL primarily learns token patterns (e.g., similar output formats, reasoning paths) among demonstration outputs. To investigate whether VLLMs rely on repetitive token patterns, we utilize the average BLEU score across demonstration outputs as a representation of token repetition. Figure 5 shows that only the image captioning task exhibits a positive correlation. In contrast, other tasks show a decline as BLEU scores exceed 30%. This underscores that MM-ICL primarily learns semantic rather than token pattern representations for effective performance.

#### 5.1.2 Sample Comparison

To further analyze the influencing factors of MM-ICL in sample retrieval, this study employs similarity and diversity metrics, which help assess how MM-ICL processes sample similarities and differences, enhancing our understanding of its mechanisms. See Appendix B for more details and results.

    &  &  &  &  &  \\   & CIDER & BERTScore & Token F1 & BERTScore & Acc & F1 & Acc & RAS & \\   &  \\  Zero-shot & 1.84 & 81.18 & 2.78 & 76.17 & 15.17 & 3.63 & 16.53 & 85.13 & 35.30 \\ Few-shot (Random) & 8.23 & 56.63 & 12.63 & 67.37 & 13.11 & 5.11 & 21.35 & 86.53 & 33.87 \\ - Textual Retriever & 13.39 & 74.22 & **21.80** & 75.74 & 13.67 & **12.04** & **25.63** & 87.71 & 40.52 \\ + Visual Retriever & 6.33 & 53.88 & 12.82 & 68.76 & 13.67 & 10.87 & 23.10 & 87.36 & 34.60 \\ + Multi-Modal Retriever & **13.47** & **85.01** & 7.85 & **79.05** & **19.66** & 10.10 & 24.96 & **88.11** & **41.03** \\    &  \\  Zero-shot & 2.86 & 86.42 & 20.90 & 87.95 & 24.34 & 10.85 & 34.06 & 82.67 & 43.76 \\ Few-shot (Random) & 3.50 & 86.62 & 20.95 & 87.76 & 25.66 & 10.28 & 34.23 & 83.67 & 44.08 \\ + Textual Retriever & 3.89 & 86.62 & 20.40 & 87.89 & 25.28 & 8.47 & 32.88 & 81.93 & 43.42 \\ + Visual Retriever & 3.50 & 86.51 & 18.57 & 87.58 & 26.78 & 12.44 & 32.21 & 84.17 & 43.97 \\ + Multi-Modal Retriever & 3.77 & 86.57 & 18.80 & 87.56 & 28.65 & 11.83 & 35.92 & 83.74 & 44.60 \\   &  \\  Zero-shot & 13.57 & 87.65 & 24.96 & 85.09 & 50.19 & 54.28 & **48.40** & 90.87 & 56.87 \\ Few-shot (Random) & 28.52 & 88.47 & 28.43 & 86.11 & 52.43 & 53.50 & 43.34 & 90.19 & 58.87 \\ + Textual Retriever & 21.58 & 88.07 & 26.99 & 85.62 & 49.42 & 53.04 & 46.04 & 90.72 & 57.69 \\ + Visual Retriever & 30.81 & 88.56 & 28.79 & 86.23 & 59.74 & **54.43** & 46.88 & 91.30 & 60.84 \\ + Multi-Modal Retriever & **41.51** & **89.03** & **30.20** & **86.78** & **59.36** & 53.17 & 46.21 & **91.49** & **62.22** \\   &  \\  Zero-shot & 5.15 & 85.43 & 20.01 & 84.77 & 61.42 & 59.07 & 54.64 & 92.46 & 57.87 \\ Few-shot (Random) & 6.37 & 85.95 & 24.43 & 85.42 & 60.11 & 60.81 & 54.30 & 92.54 & 58.74 \\ + Textual Retriever & 9.48 & 86.02 & 31.81 & **87.02** & 62.55 & 51.40 & 55.99 & 92.26 & 59.57 \\ + Visual Retriever & 9.36 & 86.26 & 32.47 & 86.96 & **63.30** & 57.79 & 59.87 & **93.19** & 61.15 \\ + Multi-Modal Retriever & **16.55** & **86.77** & **32.92** & 86.87 & 62.55 & **59.97** & **60.88** & 93.10 & **62.45** \\   &  \\  Zero-shot & 32.80 & 88.59 & 26.88 & 86.99 & **66.85** & 57.84 & **54.97** & 89.01 & 62.99 \\ Few-shot (Random) & 39.68 & 88.88 & 30.82 & 87.59 & 61.61 & 53.39 & 51.94 & 89.52 & 62.93 \\ + Textual Retriever & 35.95 & 88.45 & 31.66 & 87.58 & 61.99 & **67.13** & 45.03 & 88.98 & 63.34 \\ + Visual Retriever & 46.61 & 89.55 & 32.05 & 87.92 & 64.04 & 62.51 & 51.26 & **89.83** & 65.47 \\ + Multi-Modal Retriever & **52.55** & **89.66** & **33.65** & **88.17** & 65.54 & 63.86 & 51.43 & 89.57 & **66.80** \\   &  \\  Zero-shot & 14.05 & 87.07 & 26.93 & 85.78 & 68.20 & 66.10 & 55.14 & 90.72 & 61.75 \\ Few-shot (Random) & 21.21 & 88.13 & 32.99 & 86.81 & 63.67 & 69.75 & 55.65 & 91.82 & 63.75 \\ + Textual Retriever & 15.79 & 87.75 & 34.96 & 87.18 & **69.10** & **72.31** & 53.29 & 91.57 & 63.99 \\ + Visual Retriever & 21.35 & 87.98 & 44.74 & 89.33 & 65.73 & 64.18 & 53.29 & 91.92 & 64.81 \\ + Multi-Modal Retriever & **35.64** & **88.67** & **45.47** & **89.61** & 65.17 & 70.51 & **58.01** & **92.17** & **68.16** \\   

Table 1: Performance comparison of retrievers utilizing different modal representations, where Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected from the development set.

**Cosine similarity matters for sample comparison.** Following Liu et al. (2022), we compare two representative similarity metrics, cosine similarity and L2 similarity. As shown in Figure 6 (a), cosine similarity, which measures the directional semantic alignment, emerges as the superior metric in MM-ICL than L2 similarity. Supported by Deza et al. (2009) and Steck et al. (2024), it indicates that MM-ICL prioritizes semantic directional consistency over complete semantic alignment.

**Diversity does not show significant influence for sample comparison.** He et al. (2023), Li and Qiu (2023) have shown that demonstrations with better diversity can effectively improve textual ICL. To explore whether it exists in MM-ICL, following Li and Qiu (2023), we utilize the "diversity retriever", which selects the top-10 samples and further chooses the best 3 samples based on semantic diversity to obtain a more diverse MM-ICL. As demonstrated in Figure 6 (b), although diversity significantly enhances performance in text-based ICL, our experiments show limited improvement in MM-ICL tasks. This suggests that diversity may not directly correlate with better MM-ICL.

#### 5.1.3 Sample Selection

**Domain interval matters for sample selection.** Prior research highlights the critical role of domain relevance in enhancing ICL performance. Inspired by this, we employ the multi-modal retriever to select samples from both in-domain and out-of-domain pools. Figure 7 (a) shows a nearly 4% performance drop when out-of-domain demonstrations are included, underscoring the necessity of in-domain demonstrations for optimal MM-ICL.

**Visual style is not a crucial factor in sample selection.** Although stylistic similarity in text samples is known to bolster ICL, its effect on the visual modality remains ambiguous. Utilizing CLIP for image classification, we investigate the impact of stylistic coherence in multi-modal samples on MM-ICL performance. As depicted in Figure 7 (b), significant enhancements are observed solely in the VQA task, while captioning and classification show minimal effects and reasoning tasks decline. This indicates that diverse visual styles are not crucial in general MM-ICL.

**Token distances between modalities need to be considered for different tasks to improve sample selection.** For textual ICL, excessive token distance between samples can impede performance (Liu

Figure 5: The impact of token pattern representation in Gemini-Pro. Figure 6: The impact of different sample comparison methodologies in Gemini-Pro.

Figure 6: The impact of different sample comparison methodologies in Gemini-Pro.

et al., 2022a). We extend this inquiry to MM-ICL, analyzing how token distance across modalities influences results. Specifically, during the sample selection process, we considered the impact of the average token distance between two images on the model within the entire prompt of MM-ICL. As illustrated in Figure 7 (c), the effect of token distance varies by task, typically showing an initial performance increase followed by a decline as distance grows, particularly in non-captioning tasks. This highlights the task-dependent nature of optimal token distance in MM-ICL.

### Empirical Analysis of MM-ICL Demonstration Ordering

**Intra-demonstration ordering significantly impacts performance.** Within the demonstration, organizing the ordering, especially the relationship between modalities is a crucial topic. We investigate this by arranging inputs and outputs across modalities using three methods: _text input\(\)text output\(\)image input_ (Text-Image), _text input\(\)image input\(\)text output_ (Text-Image-Text), and _image input\(\)text output_ (Image-Text). As shown in Figure 8 (a), positioning the image at the start significantly enhances model performance. This suggests that presenting visual information first improves multi-modal comprehension, thereby boosting its learning abilities.

**Inter-demonstration ordering demonstrates minimal impacts.** Following Lu et al. (2022c), we investigate how the order of demonstration presentation influences model efficacy. We explore various strategies: random rearrangement, a "similar-last" approach where samples similar to the query are shown last, and a "similar-first" approach where similar samples are presented first. Figure 8 (b) illustrates that inter-demonstration ordering has a negligible impact on MM-ICL performance. This suggests the order-robustness, with the presentation sequence having minimal effect.

### Empirical Analysis of MM-ICL Prompt Construction

**Introductory Instruction is consistently effective for better MM-ICL.** To investigate the impact of inserting task-related instructions within prompts, we conduct the following experiment on three categories of instruction: _Introductory Instruction_, _Summative Instruction_, and _Intra-demonstration Instruction_. As depicted in Figure 9, our analysis indicates that introductory instructions stably

Figure 8: The impact of demonstration ordering on performance.

Figure 9: The impact of injecting instruction into demonstrations on model average score performance.

enhance model performance. In contrast, other instructions generally decrease performance. This finding suggests that introductory instructions facilitate targeted contextual learning and more effective semantic comprehension in demonstrations. We show more prompts and details in Appendix C.3.

MM-ICL is affected by the number of demonstrations depending on the task.Contrary to traditional text-based ICL, where performance improves with more samples, our findings in Figure 11 (a) suggest that MM-ICL does not experience significant gains from more demonstrations. To further understand the reason behind, we analysis the performance on different tasks. As shown in Figure 11 (b), increasing the number of demonstrations enhances performance in caption and VQA tasks, a trend also reported in prior studies (Alayrac et al., 2022; Laurencon et al., 2024; Shukor et al., 2024). However, performance declines when demonstrations exceed three across all tested VLLMs. In more complex reasoning tasks, such as multi-step multi-modal chain-of-thought reasoning, additional demonstrations do not yield effective improvements, aligning with the findings of Chen et al. (2024), and Fei et al. (2024).

Moreover, we attribute it to the following reasons for this limitation: (1) _Cognitive Overload:_ For complex tasks, understanding numerous demonstrations can overwhelm the model, impeding its ability to process and integrate information effectively (Chen et al., 2024). (1) _Complexity of Reasoning Tasks:_ In reasoning tasks, the performance improvement from more demonstrations is often less pronounced than when using diverse retrievers. This suggests that reasoning tasks require sophisticated integration of information, where quality outweighs quantity. See Appendix C.1 for more detailed description.

The importance of delimiter lessens by text-image interleaved demonstrations.Previous research suggests that specific delimiters for input and output data can demonstrably influence textual ICL capabilities (Min et al., 2022). Therefore, we utilize ablation experiments to omit these delimiters to examine their necessity (see Appendix C.2 for details). As shown in Figure 11, the resulting minor performance decline suggests that while these delimiters are less critical in MM-ICL, the modality switch inherent to MM-ICL may serve as an implicit delimiter, compensating for the absence of explicit delimiters.

## 6 Related Work

Recent advancements in vision large language models (VLLMs) have achieved great success in various vision-language tasks (Yin et al., 2023; Wu et al., 2024; Wu et al., 2024; Wang et al., 2024; Fei et al., 2024). Initially, VLLMs lack Multi-modal In-context Learning (MM-ICL) capabilities. To address this, researchers explore incorporating MM-ICL directly into the training phase. This involves constructing training samples with multi-modal interleaved data by manual and general templates, which unlock the MM-ICL capability (Alayrac et al., 2022; Awadalla et al., 2023). Building on this, Li et al. (2023), Doveh et al. (2024) and Zhao et al. (2024) extend the MM-ICL to construct a series of task-specific templates, which improves generalization for MM-ICL. Further, Li et al. (2023) introduce OtterHD and adapt the former process for high-definition images. The potential of MM-ICL is further explored in scene text recognition, image generation, and game instructions (Zhao et al., 2023; Sun et al., 2023; Jin et al., 2024).

Recognizing the effectiveness of MM-ICL, researches shift towards prompt optimization. These methods focus on directly optimizing multi-modal prompts to understand the task and generate the expected output, without parameter adjustments (Gong et al., 2023; Tsimpoukelli et al., 2021; Li et al., 2023b). This approach has significantly improved performance in visual reasoning tasks (Yang et al., 2022; Zheng et al., 2023). Another approach involves textualizing visual information to enable VLLMs to leverage their background knowledge through in-context learning, further enhancing visual reasoning (Yang et al., 2023; Lu et al., 2024; Gupta and Kembhavi, 2023; Shen et al., 2024). In addition, in order to better explore the MM-ICL, Zong et al. (2024) and Shukor et al. (2024) also provide a dataset to test the MM-ICL capabilities of the multi-modal classification. Furthermore, Shukor et al. (2024) take the first step to conduct an instruction modification exploration for MM-ICL. Meanwhile, Baldassini et al. (2024); Chen et al. (2024c) pioneer the first naive multi-modal retrieval exploration to enhance MM-ICL. Different from the existing work, our study mainly focuses on a **systematic exploration** of the effectiveness of key factors influencing the effectiveness of MM-ICL in a **unified perspective**. To this end, we conduct a detailed analysis and exploration on 6 VLLMs and 20 factors across 4 tasks, aiming to provide systematic and practical guidance for future research.

## 7 Discussion

Broader Impacts.Our work is the first to systematically explore the factors influencing MM-ICL. We aim to enhance the understanding of MM-ICL mechanisms and guide future developments in this field. Additionally, our findings could foster a more comprehensive comprehension of MM-ICL within the community. For social impact, this research may influence the creation of more effective multi-modal large language models and relevant applications.

Limitations & Future Work.Due to time and cost constraints, this work is limited to the exploration of image and text modalities. In future research, we can extend our exploration to video modal ICL and multi-lingual MM-ICL scenarios. Another limitation of this work involves the insufficient consideration of certain image instructions, such as grounding or the inclusion of additional arrows. These aspects often require more complex human input and are not adequately supported by most current models.

## 8 Conclusion

This study is the first to systematically explore MM-ICL by identifying key performance determinants. Our experiments with 6 models and 20 factors across 4 tasks show that multi-modal retrieval significantly outperforms single-modal approaches and the intra-demonstration ordering critically influences learning efficacy. Additionally, incorporating task-specific instructions into prompts enhances model performance. We hope these findings will refine our understanding of MM-ICL mechanisms and guide more effective developments and future research in this evolving field.