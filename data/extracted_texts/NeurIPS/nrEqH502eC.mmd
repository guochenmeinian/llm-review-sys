# BLEND: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages

Junho Myung\({}^{1,*}\), Nayeon Lee\({}^{1,*}\), Yi Zhou\({}^{2,*}\), Jiho Jin\({}^{1}\), Rifki Afina Putri\({}^{1}\),

**Dimosthenis Antypas\({}^{2}\), Hsuvas Borkakoty\({}^{2}\), Eunsu Kim\({}^{1}\), Carla Perez-Almendros\({}^{2}\), Abinew Ali Ayele\({}^{3,4}\), Victor Gutierrez-Basulto\({}^{2}\), Yazmin Ibanez-Garcia\({}^{2}\), Hwaran Lee\({}^{5}\), Shamsudeen Hassan Muhammad\({}^{6}\), Kiwoong Park\({}^{1}\), Anar Sabuhi Rzayev\({}^{1}\), Nina White\({}^{2}\), Seid Muhie Yimam\({}^{3}\), Mohammad Taher Pilehvar\({}^{2}\), Nedjma Ousidhoum\({}^{2}\), Jose Camacho-Collados\({}^{2}\), Alice Oh\({}^{1}\)**

\({}^{1}\)KAIST, \({}^{2}\)Cardiff University, \({}^{3}\)Universitat Hamburg, \({}^{4}\)Bahir Dar University,

\({}^{5}\)NAVER AI Lab, \({}^{6}\)Imperial College London

Equal contribution.Co-first authors: junho00211@kaist.ac.kr, nlee0212@kaist.ac.kr, zhouy131@cardiff.ac.UK

###### Abstract

Large language models (LLMs) often lack culture-specific knowledge of daily life, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are limited to a single language or collected from online sources such as Wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. To address this issue, we introduce **BLEND**, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. BLenD comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijan, Hausa, and Sundanese. We construct the benchmark to include two formats of questions: short-answer and multiple-choice. We show that LLMs perform better for cultures that are highly represented online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format. For cultures represented by mid-to-high-resource languages, LLMs perform better in their local languages, but for cultures represented by low-resource languages, LLMs perform better in English than the local languages. We make our dataset publicly available at: https://github.com/nlee0212/BLenD.

## 1 Introduction

Despite the worldwide usage of large language models (LLMs), capturing cultural everyday knowledge specific to a particular country or region is challenging because such knowledge is often not explicitly documented in online data sources like Wikipedia, which are commonly used to train LLMs. For instance, the answers to mundane everyday questions such as _"What can typically be found in the backyard of houses in your country?"_ are not included in the training data of LLMs, except for a handful of highly represented regions such as North America. Consequently, LLMs may provide incorrect, incomplete, or nonsensical responses to everyday questions in underrepresented cultures,even though these inquiries are frequently encountered in daily lives. This can lead to hallucinations or stereotypical responses, potentially offending a large and diverse user base.

This challenge becomes even more evident in cross-lingual settings, as most LLMs are primarily trained on English data reflecting Western perspectives [8; 20; 15]. They often reflect the stereotypes present in the training data [19; 18; 21; 36; 13], hence these models would often respond based on Western perspectives rather than reflecting actual diverse practices. Ideally, language models would reflect the cultural norms of various regions around the world and generate culturally appropriate content when responding in local languages of the regions, unless otherwise specified. To develop multilingual LLMs with such cultural appropriateness, we first need to evaluate the cultural commonsense knowledge. However, there is no well-crafted multilingual multicultural benchmark that captures the daily lives of people in diverse cultures.

To bridge this gap, we present **BLEnD**, a Benchmark for **LLMs** on **E**veryday **k**nowledge in **D**iverse cultures and languages. The benchmark covers 13 languages spoken in 16 different countries and regions shown in Table 1. Note that we include languages that are spoken in two regions with vastly different cultures, such as South Korea and North Korea, both represented by the Korean language. To effectively capture the cultural diversity of people's daily lives, we recruit annotators who are native speakers from various countries. The final dataset includes 500 socio-cultural question-answer pairs for each country/region in 6 categories: _food_, _sports_, _family_, _education_, _holidays/celebrations/leisure_, and _work-life_. To capture a comprehensive understanding of the cultural sensitivity of LLMs, we create a set of questions and answers in two formats: short-answer and multiple-choice questions. The overall framework for construction and evaluation of BLEnD is shown in Figure 1. The statistics of BLEnD are shown in Table 11. In total, BLEnD features an extensive collection of 52.6k question-and-answer pairs, 15k short-answer and 37.6k multiple-choice.

Our experimental results on BLEnD show that even current state-of-the-art LLMs exhibit unbalanced cultural knowledge and unfair cultural biases across various countries and regions. The average performance of all tested models on short answer questions about United States (US) culture in English is 79.22%. In contrast, when asked about Ethiopian (ET) culture in Amharic, the average performance

Figure 1: The overall framework of dataset construction and LLM evaluation on BLEnD. BLEnD is built through 4 steps: question collection, question filtering & translation, answer annotation, and answer aggregation. The dataset includes the same questions in 13 different languages, answered from 16 different countries/regions. We evaluate LLMs by short-answer and multiple-choice questions.

drops to only 12.18%, highlighting a significant performance gap in relatively underrepresented cultures and languages. A similar trend is observed in the multiple-choice format, where the LLMs are required to choose the correct answer for each target country/region, with answers from other countries/regions presented as wrong options.

The main contributions of our paper are as follows:

* We present BLEnD, a benchmark of carefully crafted 52.5k question-answer pairs that reflect the everyday cultural knowledge across 16 countries/regions in 13 different languages.
* Within BLEnD, we propose two types of questions to automatically measure the cultural knowledge in LLMs: short-answer questions and multiple-choice questions.
* We conduct extensive experiments across 16 LLMs on BLEnD, showing a significant performance gap between highly represented cultures and underrepresented cultures.

## 2 Related Work

Although LLMs generally incorporate extensive parametric knowledge from large text corpora during pretraining , such models frequently display bias due to imbalanced representations in the data sources . Cultural knowledge is critical in enhancing the reasoning capabilities of LLMs, contributing significantly to their success across various downstream applications.

Numerous studies have examined the socio-cultural aspects of LLMs. Previous work on cultural NLP defines culture as the way of life of a specific group of people . Most research on the cultural knowledge of LLMs centers on the culture at a national level. Anacleto et al.  collect commonsense knowledge about eating habits in Brazil, Mexico, and US through the Open Mind Common Sense portal. GeoMLAMA  introduces 16 geo-diverse commonsense concepts and uses crowdsourcing to compile knowledge from 5 different countries, each in its native languages. Nguyen et al.  introduce a methodology to extract large-scale cultural commonsense knowledge from the Common Crawl corpus on geography, religion, and occupations. CREHate  is a cross-cultural English hate speech dataset covering annotations from 5 English-speaking countries. CultureAtlas  includes textual data encapsulating the cultural norms from 193 countries, primarily sourced from Wikipedia documents in English. However, the majority of these studies are conducted exclusively in English and focus on more objective aspects of culture that are written in formal data sources.

    &  &  \\ 
**Country/Region** & **Language** & **Count** & **Language** & **Count** \\  United States (US) & English (en) & 500 & & 1,942 \\ United Kingdom (GB) & English (en) & 500 & & 2,167 \\ China (CN) & English (en), Chinese (zh) & 1,000 & & 1,929 \\ Spain (ES) & English (en), Spanish (es) & 1,000 & & 1,931 \\ Indonesia (ID) & English (en), Indonesian (id) & 1,000 & & 1,995 \\ Mexico (MX) & English (en), Spanish (es) & 1,000 & & 1,899 \\ South Korea (KR) & English (en), Korean (ko) & 1,000 & & 2,512 \\ Greece (GR) & English (en), Greek (el) & 1,000 & & 2,734 \\ Iran (IR) & English (en), Persian (fa) & 1,000 & & 3,699 \\ Algeria (DZ) & English (en), Arabic (ar) & 1,000 & & 2,600 \\ Azerbaijan (AZ) & English (en), Azerbaijan (az) & 1,000 & & 2,297 \\ North Korea (KP) & English (en), Korean (ko) & 1,000 & & 2,185 \\ West Java (JB) & English (en), Sundanese (su) & 1,000 & & 2,345 \\ Assam (AS) & English (en), Assamese (as) & 1,000 & & 2,451 \\ Northern Nigeria (NG) & English (en), Hausa (ha) & 1,000 & & 2,008 \\ Ethiopia (ET) & English (en), Amharic (am) & 1,000 & & 2,863 \\ 
**Subtotal** & & & 15,000 & & 37,557 \\ 
**Total** & & & & 52,557 \\   

Table 1: Statistics of the question samples within BLEnD. BLEnD is composed of two question types: Short Answer Questions (SAQ) and Multiple-Choice Questions (MCQ). The question samples are generated based on the 500 question templates generated by annotators from all countries/regions.

More recent studies have focused on the cultural knowledge of non-English speaking countries and languages. For instance, CLICK  and HAE-RAE Bench  evaluate LLMs' knowledge in Korean, while COPAL-ID , ID-CSQA , and IndoCulture  include culturally nuanced questions in Indonesian. Nonetheless, we do not know of any work that has been done to compare the cultural adaptiveness of LLMs across diverse languages and cultures using the same question set, which would enable a direct comparison.

Other recent work focuses on capturing the everyday cultural nuances of LLMs using social networking platforms. StereoKG  extracts cultural stereotypes of five nationalities and five religious groups from questions posted on X (formerly Twitter) and Reddit. However, this method produces a significant amount of noisy and inappropriate assertions due to insufficient filtering. CAMeL  includes masked prompts from naturally occurring contexts on X, focusing on Arabic content, and CultureBank  is a collection of diverse perspectives and opinions on cultural descriptors, including English comments from TikTok and Reddit. However, these datasets are limited to a single language and rely solely on data available from social media, not able to capture people's everyday behaviors to the full extent .

In contrast to prior work, BLEnD is carefully human-crafted, capturing everyday life cultural knowledge across 13 languages spoken in 16 different countries/regions including underrepresented regions such as West Java and North Korea.

## 3 Construction of BLEnD

Language Coverage.We select languages with varying levels of resource availability using the metrics defined by Joshi et al. . The resource availability of languages included in BLEnD is shown in Table 4 in the Appendix. Additionally, we involve at least one author who is a native speaker of the language and originally from the country/region represented in the dataset to handle the data inspection process 2.

Question Collection and Filtering.BLEnD includes 500 question templates that reflect daily life aspects across six socio-cultural categories: _food_, _sports_, _family_, _education_, _holidays/celebrations/leisure_, and _work-life_. To create these templates, we collect 10-15 questions for each category from at least two native annotators per country/region. These annotators are asked to generate culturally relevant questions about their countries while avoiding stereotypical questions. The question generation guideline is shown in Appendix B.4. The collected questions are filtered

    & **Food** & **Sports** & **Family** & **Education** & **Holidays** & **Work-life** \\ 
**SAQ** & 105 & 88 & 63 & 84 & 92 & 68 \\ 
**MCQ** & & & & & & \\ United States (US) & 642 & 393 & 60 & 173 & 500 & 174 \\ United Kingdom (GB) & 990 & 403 & 50 & 189 & 427 & 108 \\ Spain (ES) & 714 & 476 & 43 & 172 & 425 & 101 \\ Mexico (MX) & 489 & 491 & 39 & 183 & 578 & 119 \\ Indonesia (ID) & 471 & 369 & 60 & 212 & 699 & 184 \\ China (CN) & 475 & 349 & 74 & 200 & 705 & 126 \\ South Korea (KR) & 753 & 792 & 57 & 218 & 539 & 153 \\ Algeria (DZ) & 873 & 569 & 59 & 189 & 819 & 91 \\ Greece (GR) & 1,345 & 516 & 40 & 154 & 500 & 179 \\ Iran (IR) & 666 & 519 & 50 & 173 & 2,135 & 156 \\ North Korea (KP) & 784 & 430 & 78 & 228 & 476 & 189 \\ Azerbaijan (AZ) & 852 & 513 & 65 & 216 & 453 & 198 \\ West Java (JB) & 892 & 461 & 20 & 160 & 680 & 132 \\ Assam (AS) & 862 & 584 & 34 & 198 & 666 & 107 \\ Northern Nigeria (NG) & 647 & 421 & 50 & 207 & 508 & 175 \\ Ethiopia (ET) & 984 & 649 & 46 & 278 & 692 & 214 \\   

Table 2: Detailed statistics of the number of questions per category for each country/region in Short Answer Questions (SAQ) and Multiple-Choice Questions (MCQ).

to eliminate duplicates and country-specific items that can only apply to one country/region. For example, items with proper nouns from a single country/region are excluded. Then the questions are formatted into templates like "_What is a common snack for preschool kids in your country_?" Subsequently, '_your country_' is replaced by the country/region names for localizing the questions. Except for US and GB, the questions are translated into the local languages by the native speakers. This process results in a comprehensive dataset of 15,000 short-answer questions, as shown in Table 1. The specific number of questions per topic is shown in Table 2.

**Answer Annotation.** To obtain the answers to the collected questions, we recruit annotators who are native speakers of the target languages and are originally from the target regions/countries. We ensure that the annotators have lived in these countries for over half of their lifetimes 3. For most countries, we recruit annotators through Prolific 4. However, in cases where it is not possible to find annotators through crowdsourcing platforms (i.e., DZ, KR, KP, AZ, JB, AS, NG, and ET), we directly recruit five annotators who meet our criteria 5.

Annotators are required to give at least one short answer to each question and can offer up to three responses if a single answer is insufficient. If an annotator does not know the answer, they can choose from the following options: _'not applicable to our culture,' 'no specific answer for this question,' 'I don't know the answer,'_ or _'others.'_ By default, responses are collected from five annotators per question. If an annotator chooses '_I don't know the answer'_, we discard the response and collect a new one. This process continues until five valid responses for each question are obtained, or more than five annotators choose '_I don't know'_. Examples of the collected questions with answers from each country are presented in Figure 1. The guideline and the interface for answer annotation provided to annotators are shown in Appendix B.5 and B.6.

**Answer Aggregation.** We request 1-2 annotators from each country to review the annotations and remove invalid answers. These invalid answers appear to be due to some annotators misunderstanding a question, leading to nonsensical answers. Additionally, due to the nature of natural language, there are multiple variations of a single term (e.g., "go to bed" and "sleep"). We instruct the annotators to group these variants into one to ensure the final dataset contains accurate vote counts for each answer. We also ask the annotators to translate all the annotations into English. As a result, our final dataset includes variants in local languages and English, along with a final vote count for answers to the question.

Figure 2: Heatmap showing the average number of common lemmas within each question between all country/region pairs. Pairs from the same countries/regions are shown in white. Higher numbers of shared lemmas indicate that those countries/regions provide more similar answers compared to other countries/regions (e.g., Indonesia and West Java).

Statistical Analysis on Annotations.We analyze the annotations to assess their quality and consistency, as detailed in Table 7 in the Appendix. Despite the subjective nature of the questions, the average level of agreement among annotators, calculated by the average of the maximum votes for each question, is 3.16 out of 5 (63.2%). The balance within the dataset indicates that while there is consensus on certain annotations, there is also a substantial variety in the answers within each country, reflecting a diverse range of perspectives. We also present the average number of annotations per question in Table 8 in the Appendix, to show the level of answer variance.

Table 9 in the Appendix presents the average number of _'I don't know'_ responses per question. On average, there were 1.01 out of 5 such responses per question, with a standard deviation of 0.35 (ranging from a high of 1.912 in Northern Nigeria to a low of 0.42 in South Korea). The frequency of _'I don't know'_ responses was higher in the _sports_ and _holidays/celebrations/leisure_ categories, likely due to questions on sports or holidays that are not widely recognized or celebrated in certain countries or regions.

Furthermore, we measure the overlap of answers between countries/regions by calculating the number of shared lemmas of the English versions of annotations to compare the trend between them and show the result in Figure 2. The result indicates that countries/regions with closely aligned cultural backgrounds exhibit higher overlaps in answers. The top pairs with the most similar responses are Indonesia & West Java (a province in Indonesia), the United States & the United Kingdom, and Spain & Mexico, likely due to shared historical, linguistic, or cultural ties that influence how questions are understood and answered. On the other hand, the pairs with the lowest value are Northern Nigeria & Greece/Ethiopia/South Korea. This could be due to the fact that Northern Nigeria has its own unique regional culture captured in the dataset.

## 4 LLMs Cultural Knowledge Evaluation

We measure how the current LLMs perform on BLEnD on the two task settings: _short answer_ and _multiple-choice_. Details for the experimental settings and the 16 evaluated models can be seen in Appendix C.1.

### Short Answer Questions (SAQ)

Experimental Setting.In this experiment, we measure LLMs' performance on SAQ. The final score for each country is calculated as the average score over two prompts: 1) directly ask LLMs to provide the answer, and 2) add persona to the LLMs to make them act as a person from the target country or region. The detailed prompts are shown in Appendix C.2.1. To compute the score, we first mark the LLM's response as correct if it is included in the human annotators' responses to the same question. Then we compute the percentage of questions to which LLM's answer is correct. More details on calculating the scores can be found in Appendix C.2.2.

We compute the scores for all the countries based on the results obtained for the local language and English, respectively. We use lemmatizers and stemmers to handle highly inflectional languages such as Arabic and variations in words. The details are shown in Appendix C.2.2. In addition, we remove accents from words in languages that contain accents, such as Spanish and Greek, to ensure that the annotations from human annotators match the responses of LLMs. When computing the scores, we ignore questions for which three or more annotators do not know the answer.

#### 4.1.1 LLM Performance on SAQ

Figure 2(a) presents the performance of five LLMs on short answer questions in the local languages of target countries/regions. Table 10 shows the performance of all 16 LLMs evaluated. The results indicate a consistent trend of lower performance for lower resource languages .

Highlighting just a few results, the average LLM performance for US, Spain, Iran, North Korea, Northern Nigeria, and Ethiopia are 79.22%, 69.08%, 50.78%, 41.92%, 21.18%, and 12.18%, respectively, indicating a significant drop in performance for underrepresented cultures. Countries that share a common language but differ culturally show significant differences, for example, GPT-4, the highest-performing model, shows a substantial performance disparity of 31.63% between South Korea and North Korea. Similarly, between Spain and Mexico, GPT-4 exhibits a performance gap of 4.35%. Our findings highlight the critical need for LLMs to be trained on more diverse datasets, including low-resource languages and underrepresented cultures.

**Performance of Region-Centric LLMs.** Models built from non-Western countries tend to show higher performance on that specific country/region. For example, as seen in Figure 2(a), Qwen1.5-72B , made by the Qwen Team in Alibaba 6 Group, shows highest performance on Chinese among all models. HyperCLOVA-X , built from the NAVER 7 HyperCLOVA Team, also shows comparable results on Korean, even exceeding GPT-4 performance in North Korean cultural questions. These language/region-specific models often benefit from customized datasets richer in local cultural content and nuances, typically underrepresented in the more universally used datasets, leading to higher performances in their regions.

**Local language vs. English.** We compare the average LLM performance when prompted in local languages versus English, as shown in Figure 2(b)8. For cultures represented by high-resource languages like Spanish and Chinese, the local languages show better performance across all models. In contrast, in cultures represented by low-resource languages such as Azerbaijan, Sundanese, and Amharic, English results in better performance (full results are shown in Table 11). This implies that the models' proficiency in a particular language significantly influences its performance and that models tend to show better cultural sensitivity in the local language when they possess sufficient

Figure 3: (a) LLMs’ performance on short answer questions for each country/region in the local language. Models constructed from a Western country are shown in shades of blue, whereas those built from a non-Western country are shown in shades of red. (b) Average performance of all LLMs in local language and English on short answer questions. The grey error bars indicate the standard deviations among all models.

linguistic capability. Note for North Korean (KP) cultural questions, both English and Korean show poor performance as expected, but Korean performs slightly better, as it is a relatively high-resource language.

**Performance by Question Category.** In our analysis of six socio-cultural categories, models generally exhibit lower performance on questions related to _food_ and _holidays/celebrations/leisure_ than those concerning _work-life_ or _education_. This disparity, significant with a \(p\) < 0.05 using one-way ANOVA, is detailed in Figure 15. This pattern indicates that more subjective topics like food and leisure are more challenging for LLMs to show cultural adaptiveness.

### Multiple-Choice Questions (MCQ)

While SAQ is effective for multilingual evaluation, LLMs often generate responses that deviate from the annotators' one- or few-word answers, for example, generating long sentences, especially in languages that do not follow the instructions well. Hence we make the MCQ to enable simpler evaluation of LLMs. One limitation of our MCQ is that it is only available in English, as the incorrect options were chosen from different cultures' responses to the same questions, and translating all of those requires additional work. We plan to release a multilingual version of MCQ soon.

#### 4.2.1 MCQ Construction

We make the multiple-choice questions about each target country/region in English, with other answer options from other countries/regions. For fair comparison across all countries, we remove questions for which at least one country has an annotation of _'not applicable to our culture,'_ or more than three annotators don't know the answer. We also remove questions where all annotations have one vote each, indicating no typical answer from that country for that question. We determine the correct answer for each question by selecting the annotation with the highest votes from each country. We provide four answer options for each question, with no more than one option from any of the other countries. The detailed process of choosing plausible incorrect answer options can be seen in Appendix C.3.1. The final multiple-choice question prompt is shown in Appendix C.3.3.

#### 4.2.2 LLM Performance on MCQ

In general, models show higher performance in MCQ than in SAQ as shown in Figure 4. This improvement is due to using questions with well-defined answers for multiple-choice questions. However, the pattern of displaying higher performance in high-resource cultures remains consistent. When considering the tendencies of all countries/regions for each model, the average Pearson correlation between the average performance in SAQ in the local languages and English across all countries/regions and the MCQ performance across all countries/regions is notably strong at 0.93. Furthermore, the Pearson correlation between the average model performance in English SAQ for all countries and that in MCQ exhibits a considerably high value of 0.98. This indicates a strong alignment between the two evaluation formats.

## 5 Human Evaluation

We conduct a human evaluation for short-answer responses from LLMs to understand the source of errors. We use responses from GPT-4, the best-performing model, for short-answer questions. We define the following categories: _stereotypical_, _partially correct_, _refusal_, _nonsensical_, _unnatural language_, and _different country's view_ to analyze 120 wrong answers based on the automated evaluation. The detailed instructions and the definitions of each category can be found in Appendix D.3.1. Also, the summary of the human evaluation results can be found in Table 13.

The most stereotypical responses came from answers generated for underrepresented languages/cultures such as Ethiopia, West Java, and Assam, with 48.33% of responses from Ethiopia being stereotypical. Most stereotypical questions were related to food or festivals, where the LLM attempted to provide traditional information about the country or the region without fully understanding the context. For instance, for West Java, the LLM frequently answered any food-related questions with 'Seblak,' one of the most famous dishes originating from the region.

Notably, countries with a high percentage of partially correct answers or refusals were all from underrepresented cultures, such as Azerbaijan, North Korea, Northern Nigeria, and Ethiopia. Thisindicates that the LLMs tend to provide a long list of multiple answers or even refuse to answer when there is insufficient information about the topic/question. The same trend was observed for nonsensical answers, indicating that the capability of LLMs to comprehend questions is limited for low-resource languages. There were also many hallucinations for low-resource languages, such as providing 'Ruslan Cfrov' as the most famous basketball player in Azerbaijan, despite the non-existence of a famous player with that name.

GPT-4 also tends to provide answers from the perspective of other countries when responding to queries about Azerbaijan and North Korea. For Azerbaijan, many answers were from the perspectives of other countries in the Caucasus region, and for North Korea, most responses were from the perspective of South Korea. This aligns with the annotations for unnatural language, as the same two countries had the highest ratio of unnatural language. In the case of Azerbaijan, there were instances where the LLM even responded in Turkish. For North Korea, a surprising 18.33% of the responses were marked as unnatural because they were phrased in the words used exclusively in South Korea.

## 6 Conclusion

In this paper, we present BLenD, a benchmark to evaluate the cultural knowledge about everyday life within 16 current LLMs in 16 countries/regions and 13 distinct languages.

Our experimental findings indicate that current LLMs demonstrate a high level of competence in highly represented cultures such as the United States and the United Kingdom. However, their performance is significantly lower in the case of less-represented and underrepresented cultures and languages, especially when prompted in the local language. This outcome is observed in both short-answer questions and multiple-choice questions. Furthermore, our study reveals the performance gap between two countries using the same language, highlighting a cultural bias among those regions. Moreover, the study shows that the performance of LLMs varies depending on the language used in prompting: LLMs generally perform better in local languages for mid-to-highly represented cultures, while for underrepresented cultures, they perform better in English.

## 7 Limitations and Future Work

One limitation of our approach is the relatively small number of annotators, typically five per question, sometimes from the same locality within one country. This might not fully represent the countries/regions we include in our dataset. Extending efforts to increase the number of annotators per country, especially from diverse regional bases within each of the countries/regions, will be the most immediate future work of this research. Moreover, most language experts involved in the benchmark creation were academics proficient in English, the reference language for communication and translation. This may bias part of the construction process as they may not be fully representative

Figure 4: LLMs’ performance on multiple-choice questions. Models constructed from a Western country are shown in shades of blue, whereas those built from a non-Western country are shown in shades of red. Similar to the results from short-answer questions, models tend to show lower performance in underrepresented countries/regions.

of the population of each country. We do not claim that our data fully represents all the speakers of any language/region, but our dataset remains a good starting point for researchers interested in the topic.

Additionally, evaluating short-answer questions poses noticeable challenges. Despite the extensive human effort and using lemmatiers/stemmers, accounting for all word variations is difficult, leading to correct answers not being evaluated accurately. Our dataset also faces challenges in evaluating long-form responses from LLMs, as the annotated data is based on short answers. Future work should focus on accurately evaluating the cultural adaptiveness of LLMs in long-form natural contexts, as limitations exist within prompt-based evaluations.

This project was funded by the KAIST-NAVER hypercreative AI center. Alice Oh is funded by Institute of Information communications Technology Planning Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2022-000184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics). Moreover, this research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program through which leading foundation models hosted by Microsoft Azure along with access to Azure credits were provided to conduct the research. Jose Camacho-Collados, Dimosthenis Antypas, and Yi Zhou are supported by a UKRI Future Leaders Fellowship.

We also thank the following annotators for their invaluable help in building the dataset: Angela Collados Ais, Kiamehr Rezaee, Nurul Ariyani, Sabrina Borrelli, Trangilo Bumi, Helia Taheri, Chao Tan, Guanqun Cao, Dimitra Mavridou, Abderrahmane Samir Lazouni, Noufel Bouslama, Lyes Taher Khalfi, Nitumoni Neog, Bhagyashree Deka, Sikha Swarnakar, Sangeeta Neog, Nitashree Neog, Hailegnaw Tilaye, Amare Lakew, Wasihun Lakew, Yohannes Bogale, Addis Alemayehu, Yeon Su Park, Hee Su Park, Jeong Min Young, Hyewon Im, Geunsoo Lee, David Chong, Dea Adhista, Sarah Oktavianti, Muhammad Syahrul Kurniawan, Taufik Muhammad Yusup, Miguel Ramirez, Thomas Welsch, annotators from Prolific, and all other annotators who preferred to remain unnamed.