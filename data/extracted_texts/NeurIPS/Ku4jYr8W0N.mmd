# Computationally Efficient Laplace Approximations

for Neural Networks

 Swarnali Raha

Department of Statistics

University of Florida

Gainsville, FL

swarnali.raha@ufl.edu

&Kshitij Khare

Department of Statistics

University of Florida

Gainsville, FL

kdkhare@ufl.edu

&Rohit K Patra

Linkedin Inc

New York, NY

ropatra@linkedin.com

###### Abstract

Laplace approximation is arguably the simplest approach for uncertainty quantification using intractable posteriors associated with deep neural networks. While Laplace approximation based methods are widely studied, they are not computationally feasible due to the involved cost of inverting a (large) Hessian matrix. This has led to an emerging line of work which develops lower dimensional or sparse approximations for the Hessian. In this paper, we build upon this work by proposing two novel sparse approximations of the Hessian: (1) greedy subset selection, and (2) gradient based thresholding. We show via simulations that these methods perform well when compared to current benchmarks over a broad range of experimental settings.

## 1 Introduction

Uncertainty quantification for neural networks is critical in understanding the quality of output/estimates provided by the network, and is a crucial component for many standard frameworks (such as contextual bandits) to study exploration-exploitation trade-offs. Such frameworks are useful in many domains, such as recommender systems (Su et al., 2024; Joachims et al., 2018; Li et al., 2010), content moderation (Avadhanula et al., 2022), healthcare (Durand et al., 2018; Mintz et al., 2020; Esteva et al., 2017), dynamic pricing (Misra et al., 2019), dialogue systems (Liu et al., 2018), self-driving cars (Bojarski et al., 2016), detecting hallucinations in LLMs (Chen and Mueller, 2023; Felicioni et al., 2024; Dwaracherla et al., 2024). It is well known that accurate uncertainty quantification can significantly improve the performance of downstream tasks (Ovadia et al., 2019; Riquelme et al., 2018).

Despite these benefits, uncertainty quantification is not yet widely used in deep learning due to excessively high computational cost in cutting edge applications. This is in particular true for the Laplace approximation based approach, which approximates the posterior distribution of the parameters in a neural network by a multivariate Gaussian centered at the posterior mode \(}_{MAP}\) (with \(\) denoting the parameters in the network), and with the (negative) observed Fisher information matrix as its precision (inverse-covariance) matrix, see (MacKay, 1992; Foong et al., 2019). This approach is technically well-grounded and is also attractive due to its conceptual simplicity. However, any application of this approach for uncertainty quantification requires inversion of the observed Fisher information matrix whose computational complexity \(O(p^{3})\) can be prohibitive in many settings (here \(p\) is the number of parameters in the neural network).

To overcome this computational challenge, a nascent strand of the literature has focused on replacing the Laplace precision matrix \(\) by a lower dimensional approximation which retains the most relevant components or entries. In (Nilsen et al., 2022), the authors propose replacing the Laplace precision matrix by an approximation which uses only the top \(k\) eigenpairs of \(\) (identified using the Lanczos algorithm (Cullum and Willoughby, 2002)). However, the iterative Lanczos algorithm can in general suffer from numerical instability issues when using finite precision arithmetic, especially for large matrices (Chen and Trogdon, 2024). The _PrecisionDiag_ approach in (Riquelmeet al., 2018; Zhang et al., 2020) approximates \(\) by setting all off-diagonal entries to zero. This leads to extremely efficient computation, but can suffer from poor statistical performance given that it ignores _all_ cross-parameter correlations.

Our particular focus in this paper is a class of methods, termed _sub-network LA_ in (Daxberger et al., 2021), that focuses on identifying a small subset \(S\) of parameters capturing the most variability. The matrix \([_{S,S}]^{0}\), constructed by setting all entries of \(\) outside of the principal submatrix \(_{S,S}:=(_{rs})_{r S,s S}\) to be zero, is then used as an approximation for \(\). Since the approximation is singular, its Moore-Penrose inverse is utilized for subsequent variance computations. This amounts to ignoring the variability in all other parameters except those in \(S\). To the best of our knowledge, two methods for identifying \(S\) have been proposed in the current literature. The _Neural-Linear_(Snoek et al., 2015; Riquelme et al., 2018) method chooses \(S\) as the subset of parameters in the last layer of the neural network. However, the last layer of the network may not always contain the most relevant parameters in terms of capturing variability (see Section 4). The marginal variance based approach in (Daxberger et al., 2021, Section 5) relies on an independence assumption (ignoring cross-correlations) for the posterior distribution of the parameters. For a given subset size \(k\), this approach reduces to choosing the parameters corresponding to the \(k\) smallest diagonal entries of \(\).

In this paper, we propose two methods for selecting the parameter subset \(S\) for subnetwork LA which build on these promising initial efforts while aiming to address their deficiencies through a more refined treatment. The first method, _Greedy-Laplace_ pursues a greedy step-wise approach for selecting \(S\). The method starts with the Laplace precision matrix \(\), and at each step the parameter corresponding to the smallest diagonal entry in the current precision matrix is chosen and added to \(S\), and the precision matrix is shrunk and updated by'removing the effect' of this parameter. The second method, _Gradient-Laplace_, recognizes that the posterior predictive variance for a future observation also depends on the corresponding gradient vector (see (1)), and uses parameters with the highest average absolute gradient evaluated over a reference dataset to construct \(S\). Detailed empirical evaluation based on accuracy of subnetwork choice (Section 4) and coverage of predictive credible intervals (Appendix D) is provided.

## 2 Setup

In this article, our central focus will be on the **linearized Laplace Approximation** (LLA) (Foong et al., 2019). We consider a neural network with output \(f_{}()\) for given input \(\) and parameter vector \(^{p}\). The network is trained using a given dataset \(_{train}=\{(_{n},y_{n}):1 n N_{train}\}\), where we assume, \(y_{n} N(f_{}(_{n}),_{0}^{2})\). We place independent Gaussian priors on the weights and biases of the network. As in (Foong et al., 2019), under this setup, the LLA provides the following approximation of the posterior predictive distribution for a new observation \((^{*},y^{*})\),

\[p(y^{*}|^{*},_{train}) N(f_{_{MAP}}( ^{*}),_{0}^{2}+g(^{*})^{T}^{-1}g(^{ *}))\] (1)

where, \(g(^{*})=_{}f_{}(^{*})|_{=_{MAP}}\), and \(\) is the negative Hessian matrix, which in practice, is replaced by the Gauss-Newton matrix, as the Gauss-Newton matrix is guaranteed to be positive semi-definite. In particular, letting \(\) denote the vector containing all prior variances, we set

\[=^{2}}_{n=1}^{N}\,g(_{n})g( _{n})^{T}+(diag())^{-1}.\] (2)

The posterior approximation for a classification problem will be similar but with the Gauss-Newton matrix replaced by the Generalized Gauss-Newton matrix, see Appendix C for more details.

## 3 Proposed Algorithms

Our goal is to develop novel and principled methods for choosing the subset \(S\) for subnetwork LA, so that the corresponding approximation \([_{S,S}]^{0}\) serves as an effective surrogate for \(\) for the variance computation in (1). We now describe two strategies for this purpose.

**The Greedy-Laplace Approach**. Note that the inverse of the \(r^{th}\) diagonal entry of a precision matrix is equal to the conditional variance of the \(r^{th}\) variable given all the other variables. Givena user-specified subset-size \(k\), we leverage this fact to sequentially identify the 'best' subset of \(k\) parameters (i.e., the subset capturing the most variability) in a greedy fashion. We start with \(^{(1)}=\) (as in (2) and (C.1)). In the \(i^{th}\) step, the parameter with the minimum diagonal entry in \(^{(i)}\) is chosen, and the marginal precision matrix of the remaining variables is calculated and set as \(^{(i+1)}\) (needs \(O((p-i)^{2})\) computations, see Step 5 of Algorithm 1). In particular, \(^{(i+1)}\) is a \((p-i+1)(p-i+1)\) matrix. We stop after the \(k^{th}\) step. We refer to this procedure as the _Greedy-Laplace algorithm_ (Algorithm 1), which has an overall computational complexity of \(O(kp^{2})\). The output of Algorithm 1 is an index set \(S\) (of size \(k\)), and the surrogate \([_{S,S}]^{0}\) is obtained from \(\) by setting all elements outside the principal sub-matrix corresponding to \(S\) to zero. Recall that since \([_{S,S}]^{0}\) is singular, its Moore-Penrose inverse is used in (1) instead of \(^{-1}\).

```
0: Laplace Precision matrix \(\) as in (2); user specified dimension \(k\)
1: Initialize \(^{(1)}=\), collection of indices of selected parameters \(S=\)
2:for\(t=1,2,,k\)do
3: Compute \(j= S}{argmin}(^{(t)})_{i,i}\)
4: Update \(S=S\{j\}\)
5: Construct \(^{(t+1)}\) by removing row & column for \(j^{th}\) variable in \(^{(t)}\), and updating remaining entries: \(^{(t+1)}=(^{(t)})_{-j,-j}-)_{-j,j}(^{(t) })_{j,-j}}{(^{(t)})_{j,j}}\)
6:endfor
7: Return S ```

**Algorithm 1** Greedy-Laplace Algorithm

**The Gradient-Laplace Approach**. Note that the variance term in (1) depends on both \(\) and \(g(^{*})\). While Algorithm 1 exclusively uses the matrix \(\) to identify the subset \(S\) of the most relevant parameters, an alternative is to focus instead on the gradient \(g\). In particular, this gradient-based approach, considers a reference data set \(\) (either the training data \(_{train}\) or a separate test data set \(_{test}\)), and computes the average absolute gradient \(=|}_{}|g()|\). Here, for any vector \(^{p}\), \(||:=(|u_{j}|)_{j=1}^{p}\). We now choose \(S\) to be the set of indices corresponding to the largest \(k\) entries of \(\). We refer to this procedure as the Gradient-Laplace algorithm (Algorithm 2). Again, the Moore-Penrose inverse of \([_{S,S}]^{0}\) is used as a surrogate for \(^{-1}\) in (1). Note that this methods requires an additional summary statistic of the gradients, which in the case of \(_{train}\) has negligible overhead but in the case of \(_{test}\) requires a backprop over a large enough sub-sample.

```
0:\(}_{MAP}\) from fitted model using \(_{train}\); Reference data \(\); user-specified dimension \(k\)
1: Compute the gradient \(g()\) for all \(\)
2: Compute the average of the absolute gradients over all data points in \(\), denote by \(\)
3: Select the \(k\) parameters corresponding to the \(k\) highest values in \(\)
4: Collect the respective indices in \(S\). Return \(S\). ```

**Algorithm 2** Gradient-Laplace Algorithm

Note here that, computing \(}_{MAP}\) and the gradients \(g()\) above is required irrespective of the approximation. Now, for a reference dataset \(\) of size \(N\), computing \(\) and selecting \(k\) indices corresponding to the \(k\) highest values in \(\) using the sorting approach requires \(O(max\{Np,p\,logp\})\) computations. Selecting the \(k k\) surrogate submatrix from \(\) and obtaining the Moore-Penrose inverse requires another \(O(k^{3})\) computations, making the overall complexity \(O(max\{Np,p\,logp,\,k^{3}\})\).

## 4 Empirical Evaluation

### Experiment 1: Accuracy of Subnetwork Choice

**Experimental Setup**.  propose using the Wasserstein distance between the linearized Laplace posterior distribution of the parameters \(\) (multivariate normal with covariance matrix \(^{-1}\)) and its subnetwork LA based surrogate (singular multivariate normal with covariance matrix given by the Moore-Penrose inverse of \([_{S,S}]^{0}\)) as a measure of accuracy/quality of the subnetwork based approximation using the subset \(S\). We use a derived Wasserstein metric to compare the performance of the proposed methods (_Greedy-Laplace_ and _Gradient-Laplace_) for choosing \(S\) to existing subnetwork LA methods - the _Subnet diagonal_ approach [Daxberger et al., 2021b, Section 5]), the _NeuralLinear_ approach [Snoek et al., 2015, Riquelme et al., 2018] and the _Last \(k\)_ approach (a natural extension of _NeuralLinear; see Appendix B_). For thoroughness, we also include the (non subnetwork LA based) _PresicionDiag_ approach [Riquelme et al., 2018, Zhang et al., 2020] for obtaining a surrogate of \(\). For further details on these benchmark methods see Appendix B.

For this experiment. two working model settings: mis-specified and well-specified, and three kinds of test data: in-distribution, out-of-distribution (OOD) and mixed domain are considered. For further details about the experimental settings and hyper-parameter choices see Appendix A. For each method, working model and test data combination, the following procedure is used to compute a Wasserstein distance based metric. For each test data observation, say \((^{*},y^{*})\), we compute the Wasserstein distance between the normal distribution in (1) and the normal distribution when \(\) is replaced by its relevant surrogate. The average of these Wasserstein distances over the test dataset is then used as a measure of accuracy for the method in the given setting.

**Results and plots.** The plots in Figure 1 depict the (Wasserstein-based) accuracy measures for various methods under the six experimental settings discussed above. The plots show that both the proposed methods significantly and consistently provide superior approximations to the Laplace posterior compared to other subnetwork LA methods - the _Subnet diagonal_ approach and the _Last \(k\)_ approach - for each subset size \(k\) and across all settings. The improvement in accuracy is remarkably more pronounced in the mis-specified working model setting compared to the well-specified working model setting. The accuracy of the _NeuralLinear_ and _PrecisionDiag_ approaches does not change with \(k\), but we see that even for small or moderate values of \(k\), the proposed methods start providing comparatively better performance. Finally, the performance of the two proposed methods is similar, with Gradient-Laplace providing a marginally better approximation overall. To conclude, these experiments strongly demonstrate that the proposed methods provide more effective and refined choices for computationally efficient approximations of the Laplace posterior.

### Experiment 2: Coverage of Confidence Intervals

In this section, we compare the performance of our proposed methods with several benchmark methods using the coverage of posterior predictive credible intervals as a measure of performance. In particular, we consider two different settings for this experiment: _Regression_ and _Classification_.

Figure 1: Wasserstein distances between the posterior predictive distribution using the full Laplace posterior and the relevant surrogate Laplace posterior, averaged over the entire test data set. The x-axis corresponds to the subset size \(k\). The top row corresponds to the mis-specified working model setting, and the bottom row corresponds to the well-specified working model setting.

Two working models - a mis-specified working model and a well-specified working model are considered. For further details about the experimental settings see Appendix D. For each method (with each setting and working model), we compute the empirical coverage of \(95\%\) credible intervals constructed from the posterior predictive distribution for new observations over a test data set. In particular, for each future (test data) observation with predictor vector \(^{*}\), we check if the corresponding mean response value \(f_{_{0}}(^{*})\) lies in the \(95\%\) posterior predictive credible interval. The empirical coverage is the proportion of such inclusions in the test dataset. We compare our methods with the following Laplace-based algorithms (details in Appendix B): _NeuralLinear_, _PresicionDiag_, _Subnet diagonal_. For thoroughness, we include in our comparison, two standard (non-Laplace) Bayesian methods, the _Split_ method  and the _ensemble_ method .

The _linearized Laplace approximation (LLA)_ provides overall competitive coverage to the two benchmark methods, _Split_ and _Ensemble_. Among the methods that use a low-dimensional surrogate of the Hessian to further approximate LLA, _NeuralLinear_ clearly provides poor coverage. Its immediate extension, _Last k_, also provides much lower coverage compared _Subnet diagonal_, and both the proposed methods, _Greedy-Laplace_ and _Gradient-Laplace_, especially in the well-specified working model settings. The proposed _Gradient-Laplace_ approach clearly outperforms the _Subnet diagonal_ approach in all but the well-specified working model setting under classification, where the _Subnet diagonal_ provides marginally better coverage. The proposed _Greedy-Laplace_ approach provides similar coverage to the _Subnet diagonal_ approach in mis-specified working model settings, and significantly outperforms it in the well-specified working model setting for the regression setup, while the _Subnet diagonal_ approach provides marginally better coverage in the well-specified setting for the classification setup. Although _PrecisionDiag_ provides better coverage than _NeuralLinear_, the coverage is still quite low, and is attained by the proposed methods for very small values of \(k\). To summarize, the proposed methods for subnetwork LA overall provide significantly better coverage than existing methods for constructing surrogates for the Laplace precision matrix.

Figure 2: For various methods, coverages of the credible intervals using the full Laplace posterior, the relevant surrogate Laplace posterior, the split and the ensemble methods averaged over the entire test data set, and then over \(200\) replications are depicted. The x-axis corresponds to the subset size \(k\) (for relevant subnetwork LA methods), and the y-axis corresponds to the empirical coverage described above. For both regression and classification, two working model settings are considered. The top row corresponds to the **regression** setup, and the bottom row corresponds to the **classification** setup. The plots in left column are corresponding to the mis-specified working model setting, and those in the right column are corresponding to the well-specified working model setting.