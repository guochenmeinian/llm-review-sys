# Mitigating Test-Time Bias for Fair Image Retrieval

Fanjie Kong

Duke University

fanjie.kong@duke.edu

&Shuai Yuan

Duke University

shuai@cs.duke.edu

&Weituo Hao

TikTok Inc.

weituohao@tiktok.com

Ricardo Henao

Duke University

KAUST

ricardo.henao@duke.edu

###### Abstract

We address the challenge of generating fair and unbiased image retrieval results given neutral textual queries (with no explicit gender or race connotations), while maintaining the utility (performance) of the underlying vision-language (VL) model. Previous methods aim to disentangle learned representations of images and text queries from gender and racial characteristics. However, we show these are inadequate at alleviating bias for the desired equal representation result, as there usually exists test-time bias in the target retrieval set. So motivated, we introduce a straightforward technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from the pre-trained vision-language model. We evaluate our algorithm on real-world image search datasets, Occupation 1 and 2, as well as two large-scale image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest bias, compared with various existing bias-mitigation methods, in text-based image retrieval result while maintaining satisfactory retrieval performance. The source code is publicly available at https://github.com/timqqt/Fair_Text_based_Image_Retrieval.

## 1 Introduction

Image search on the web based on text-based image retrieval (TBIR) (Lew et al., 2006) involves interpreting a user's (text) query and returning corresponding images that are considered relevant in terms of semantic meaning (Chen et al., 2015). With recent advancements in multi-modal representation learning, Vision-Language (VL) models such as CLIP have been widely used to enhance the efficacy of text-based image retrieval (Radford et al., 2021; Cao et al., 2022). These models are usually trained on vast datasets that consist of millions of text-image pairs scrapped from the web, which inevitably manifest societal biases especially for neutral queries (Wang et al., 2021; Hall et al., 2023; Wang et al., 2021b), _i.e._, queries without explicit demographic (gender or race) connotations. In Figure 1, we show image retrieval results for the neutral query "Bus Driver" using CLIP and an unbiased alternative delivered by the proposed approach.

In our work, we adhere to _equal representation_ as our fairness objective (Mehrotra and Celis, 2021; Kay et al., 2015), as an alternative to _proportional representation_(Jalal et al., 2021; Berg et al., 2022). The latter, which aligns the demographic proportions in the retrieval results with those in the dataset, which is susceptible to biases during collection (Kay et al., 2015). Instead, equal representation for all demographic groups of interest attempts to mitigate (obscure) the influence of any inherent biases. Previous works have been dedicated to developing unbiased VL models that promote fairness in TBIR tasks (Wang et al., 2021a; Berg et al., 2022; Wang et al., 2022; Kim et al., 2023; Chuanget al., 2023; Parraga et al., 2022). Berg et al. (2022) leveraged adversarial training for debiasing in which a learnable prompt is prepended to the input text queries, while an adversary seeks to discourage the image and text encoders from capturing gender or race information for retrieval. Alternatively, Wang et al. (2021) discarded the components of the visual and text representations that are highly correlated with gender, by estimating the mutual information between these components and gender attributes. Further, Wang et al. (2022) neutralized the gender contributions in image representations by enforcing equal contributions from gender (male and female) features using a bias contrast loss, while maximizing the contribution of gender-irrelevant features. These studies largely rely on demographic attributes that can be somewhat visually perceived. For instance, gender attributes are derived from perceived characteristics of masculinity or femininity, leading to labels such as "Male" or "Female". Similarly, race-related attributes are often characterized based on skin tones and typically categorized into groups such as "Fair Skin" and "Dark Skin" (Kay et al., 2015; Celis and Keswani, 2020). However, these methods primarily focus on debiasing the gender or race encoding within VL models, which may be insufficient due to bias or imbalance in the image retrieval candidate pool (test set). In contrast, though we also mitigate bias in TBIR using the same demographic attribute annotations as other works, the proposed solution is simpler in the sense that it forgoes the need for access to gradients or retraining the VL model.

In this paper, we start Section 3.1 by defining the text-based image retrieval task in the context of a fairness objective focused on equal representation. We then analyze the effectiveness of existing bias reduction methods in Section 3.4 and demonstrate how they fall short in achieving equal representation, mainly due to imbalances in the test-time image retrieval set. Based on this observation, in Section 3.5 we propose a simple yet effective post-processing debiasing method called post-hoc bias mitigation (PBM) that creates fair retrieval subsets guided by predicted gender (or race) attributes obtained from either an off-the-shelf gender (or race) classifier or zero-shot inference using a pre-trained VL model. In Section 4, we evaluate PBM on real-world web image search (Kay et al., 2015; Celis and Keswani, 2020) and large-scale image-text datasets such as MS-COCO (Chen et al., 2015) and Flickr30K (Plummer et al., 2015). By comparing our approach to various existing bias-mitigation techniques, we show that our method achieves the lowest bias while maintaining satisfactory retrieval performance, as evidenced by both quantitative and qualitative results. Importantly, PBM strives to provide a more unbiased, fair and diverse visual representation of different groups in search results, ultimately promoting social welfare by challenging existing social perceptions of gender and race.

The summarized contributions of this work are:

* We present an analysis of the effect of existing debiasing methods on VL models, and highlight their insufficiency in achieving equal representation for text-based image retrieval.
* We propose PBM, a straightforward and efficient test-time post-processing debiasing algorithm that generates fair retrieval subsets, guided by predicted gender/race information obtained from an off-the-shelf classifier or inferred via zero-shot using the VL model itself.
* We evaluate PBM on two real-world web image search and two large-scale image-text datasets for text-based image retrieval, and compare with existing bias-mitigation techniques, demonstrating its effectiveness in achieving the lowest bias among all tested methods.

Figure 1: Text-based image retrieval (TBIR) results of a neutral query. Right: all returned images are male bus drivers, possibly pushing the message that only men (can) perform this job. Left: desirable unbiased result with equal gender representation obtained by the proposed PBM approach.

Related work

Text-based image retrievalText-based image retrievalThe most common of the most common image retrieval methods is the process of searching and retrieving images from a large database using textual descriptions or keywords as queries. The task is usually tackled by image-text feature alignment (Cao et al., 2022), which embeds image and text inputs into a shared feature space such that relevant image and text features are close to each other. In recent years, substantial progress has been made due to the emergence of large-scale datasets, as well as the development of effective deep learning-based image-text models. Pioneering works include DeViSE (Frome et al., 2013), which bridges the _semantic gap_ between image content and textual descriptors by aligning CNN-based features of images with textual embeddings from ImageNet labels. Subsequent approaches, such as VSE++ (Faghri et al., 2017) and SCAN (Lee et al., 2018), further refine these joint embeddings to improve retrieval performance. Recently, OpenAI's CLIP (Radford et al., 2021) has emerged as a powerful approach for image retrieval based on similarity matching between extracted image and text features. CLIP leverages a pre-trained transformer model, jointly optimized for both image and text understanding, which allows it to effectively match images and textual descriptions.

Fairness in machine learningRecent studies have highlighted numerous unfair behaviors in machine learning models (Angwin et al., 2016; Buolamwini and Gebru, 2018). For example, a risk assessment algorithm used in the United States criminal justice system predicted that Black defendants were more likely to commit future crimes than white defendants, even after controlling for criminal record (Angwin et al., 2016). Moreover, individuals with different gender and skin-tones are likely to receive disparate treatment in commercial classification systems such as Face++, Microsoft, and IBM systems (Buolamwini and Gebru, 2018). Consequently, there has been a surge in demand and interest for developing methods to mitigate bias, such as regularizing disparate impact (Zafar et al., 2015) and disparate treatment (Hardt et al., 2016), promoting fairness through causal inference (Kusner et al., 2017; Kusner et al., 2019), and incorporating fairness guarantees in recommendations and information retrieval (Beutel et al., 2019; Morik et al., 2020).

Fairness in vision-language modelsAfter some studies revealed the bias of using VL models in downstream tasks (Wang et al., 2021; Hall et al., 2023; Wang et al., 2021), efforts to address and mitigate these biases in VL models have gained increasing attention. Existing solutions for fair vision-language models can be generally classified into pre-processing, in-processing, and post-processing methods. Pre-processing techniques usually involve re-weighting or adjusting the training data to counter imbalances across demographic attributes, while preserving the utility of the dataset for the target task (Friedler et al., 2014; Calmon et al., 2017). In-processing methods focus on altering the training objective by incorporating fairness constraints, regularization terms or leveraging adversarial learning to obtain representations invariant to gender/race (Berg et al., 2022; Wang et al., 2023; Xu et al., 2021; Cotter et al., 2019). Post-processing approaches achieve fairness by applying _post-hoc_ corrections to a (pre-)trained model (Cheng et al., 2021; Calmon et al., 2017) or via feature clipping (Wang et al., 2021) on the output of image-text encoders based on mutual information.

Our work lies in the post-processing category of debiasing methods that encourages equal representation of diverse demographics. We also identified the fair subset selection approach used in Mehrotra and Celis (2021) as a potential post-hoc debiasing method for TBIR. While Mehrotra and Celis (2021) shares our goal of ensuring equality of gender/race attributes in the set of results, their focus did not extend to the TBIR scenario with an underlying VL model nor detail an effective method for obtaining accurate demographic attributes for debiasing. More importantly, they assumed demographic attributes seen by their algorithm to be available ground truth labels subject to noise. This assumption creates difficulties when attempting to adapt their method to a real-world problem such as TBIR. Complementary, our approach is meant to address these deficiencies by providing a practical debiasing procedure, that includes acquiring demographic attributes.

Gender/Racial Bias in Web Image SearchOur research is closely related to studies in the Human Computer Interaction community that demonstrated gender inequality issues in current online image search systems (Kay et al., 2015; Noble, 2018; Celis and Keswani, 2020). These studies revealed how gender bias in occupational image search results influences people's perceptions about the presence of men and women in various professions. Our work builds upon these findings by examining the gender and racial bias in image search algorithm and offering innovative solutions for reducing bias in popular image retrieval framework using pre-trained VL model, such as CLIP (Radford et al., 2021).

## 3 Method

### Problem formulation

Image retrievalSuppose \(\) is the set of all text queries of interest (gender-neutral defined below), and \(=\{v_{i}\}_{i=1}^{N}\) is a database of \(N\) images, from which we retrieve images given query inputs. Each query input \(c\) is relevant to a ground-truth set of images \(V_{c}^{*}\) provided by human annotators.

The text-based image retrieval task aims at finding images from the database so they best match the text query inputs. Specifically, given any text query \(c\) and a fixed retrieval size \(K\) (\(K N\)) as inputs, the goal is to design an algorithm that returns a bag of \(K\) images \(V_{c,K}\), where \(|V_{c,K}|=K\), such that \(V_{c,K}\) contains as many relevant images from \(V_{c}^{*}\) as possible.

For evaluation purposes, a top-\(K\) recall score ("Recall@\(K\)") is usually computed to quantify whether the most relevant images are included in the retrieval output. Specifically, we write

\[K=}|V_{c,K} V_{c}^{*}|}{_{c }|V_{c}^{*}|} 100\%,\]

where \(K\) is selected specifically for each dataset so that \(|V_{c}^{*}| K\) for most queries \(c\); this way, the recall can be realistically close to 100%.

Debiasing in image retrievalUsing gender as a motivating scenario, we are interested in gender debiasing in cases where queries relate to human characteristics with no gender connotations, which we refer to as _gender-neutral queries_. For example, queries could be occupations that are widely open to all individuals, irrespective of gender (Organization, 2019), such as chef, nurse, social worker, _etc._

We anticipate that gender-neutral queries should yield image retrieval sets that comprise equal representation of both male and female associated images, _i.e._, in a 1:1 ratio, which is consistent with the equal representation goal discussed by Wang et al. (2021, 2023); Mehrotra and Celis (2021); Mehrotra and Vishnoi (2022). Note that we use gender debiasing as an example, but the same setting can be generalized to other debiasing issues such as racial discrimination, as we will demonstrate in the experiments.

Specifically, assume each image \(v\) has a gender attribute \(g(v)\{+1,-1\}\), which corresponds to the two genders manifested in the image, male and female, respectively. For each gender-neutral query \(c\), the gender bias of the resulting retrieved bag of images \(V_{c,K}\) is defined as the normalized absolute difference between the numbers of images of each gender, _i.e._,

\[B(V_{c,K})=|_{v V_{c,K}}\{g(v)=+1\}-_{v  V_{c,K}}\{g(v)=-1\}|=|_{v V_{c,K} }g(v)|,\] (1)

which ranges from \((K,2)\) (minimum bias) to 1 (maximum bias). In our retrieval approach, we average \(B(V_{c,K})\) across all gender-neutral queries \(c\) as an evaluation metric for fairness, _i.e._,

\[K=|}_{c}B(V_{c,K})= |}_{c}|_{v V_{K} ^{*}}g(v)|.\]

The goal is to minimize \(K\) while maintaining a satisfactory retrieval \(K\).

### Similarity-based image-text matching

Overall frameworkAs in previous works (Singh et al., 2003, 2014, 2019, 2022, 2022), we use a similarity matching approach to tackle the image retrieval task. Such an approach involves aligning image and text features from largely pre-trained vision and language models during training time. At inference time, we rank images based on whether their features are similar to the input query text features and use the top-ranked images as our retrieval output. Specifically, we use an image encoder network \(f_{}()\) and a text encoder network \(f_{}()\) to embed both image \(v\) and text \(c\) inputs into a shared \(d\)-dimensional feature space as \(f_{}(v),f_{}(c)^{d}\). A cosine similarity score \(S(v,c)\) is then computed to quantify the relevance between \(v\) and \(c\)(Singh et al., 2003, 2014, 2021a).

TrainingAs in previous works (Chen et al., 2020; Radford et al., 2021), the training of the image and text encoders is achieved through optimizing an NT-Xent (Normalized Temperature-scaled Cross Entropy) loss with stochastic gradient descent on mini-batches.

InferenceGiven a new image database \(^{}\), for each new query input \(c^{}\), we compute its similarity score \(S(v_{i},c)\) with every image \(v_{i}^{}\) and then pick the top-\(K\) images that have the maximum similarity scores to form our retrieved set \(V_{c,K}\).

### Fairness criterion for image retrieval

Fairness can be defined in numerous ways in favor of different situation (Saxena et al., 2020). In the context of fair image retrieval (Wang et al., 2021; Berg et al., 2022), we define our fairness objective as _equal representation_, implying that the retrieved image set through the retrieval algorithm should encompass an equal number of samples from each demographic group. We formally define our criterion as follows:

**Definition 3.3.1** (Equal Representation): _An image retrieval algorithm satisfies equal representation with respect to binary demographic attributes \(g(v)\) if,_

\[_{V_{c}-P}[_{v}[(g(v)=+1)] ]=_{V_{c}-P}[_{v}[(g(v)=-1) ]]\]

_where \(P\) represents the distribution of retrieved image set \(V_{c}\) corresponding to neutral queries \(c\)._

The above definition implies \([B(V_{c})]_{V_{c}-P}=0\), _i.e._, the expected bias is zero for any retrieved image sets. Our definition of _equal representation_ is equivalent to "Equal opportunity for ranking distributions" introduced by Singh and Joachims (2017) when ranking position bias is constant.

### Bias analysis

Using the image retrieval framework defined above as a basis, it is important to incorporate strategies to reduce gender (and race) biases of the retrieval output \(B(V_{c,K})\). Below, we briefly analyze existing methods and discuss their limitations.

Existing methodsMost methods address the gender fairness issue by enforcing model features to be less dependent on gender information. This is achieved by mainly two types of approaches, namely adversarial training (Edwards and Storkey, 2015; Berg et al., 2022; Xu et al., 2021) and mutual information (MI) minimization (Wang et al., 2021, 2023). Specifically, adversarial training involves training a separate (adversarial) classifier network by adding an adversarial loss so that the adversarial network cannot distinguish gender given the encoded image features (Edwards and Storkey, 2015; Berg et al., 2022; Xu et al., 2021). Alternatively, MI minimization aims at reducing the MI between feature distribution and their gender by clipping feature dimensions highly correlated with gender (Wang et al., 2021). Both methods encourage the model to extract features that are _independent_ of gender. However, we argue below that enforcing this kind of independence between image features and gender is not sufficient to effectively eliminate gender bias in image retrieval.

Illustrative exampleWe start by showing debiasing results using mutual information minimization. Similar insights can be obtained via adversarial training, which is shown in the Supplementary Material (SM). Figure 2 shows an example for the query "engineer" comparing three methods, namely, CLIP (no debiasing), MI-based debiasing and the proposed PBM (described below). For each method, we first compute, sort and group the similarity scores of all images into 1% quantiles (\(\) 30 images each). Then, for each quantile, we compute the gender bias as defined in (1). This analysis shows the relationship between gender bias and similarity scores. Note that for retrieval, we use samples with the largest similarity scores to form our retrieved bag of images, thus only the right-most data points (highlighted in the figure) are part of the retrieval set. We show percentiles in the figure to emphasize similarity rank rather than the similarity scores _per se_.

As shown in Figure 2(a), the original image retrieval algorithm with no debiasing based on CLIP tends to assign higher scores (at larger percentiles) to samples associated with male attributes, as evidenced by the high correlation between gender and the similarity scores. This makes the final retrieval output largely biased towards male samples. In contrast, Figure 2(b) shows the result of debiasing via MI minimization (Wang et al., 2021). Using the regression line as a visual guide, we see that the correlation between gender and similarity score is close to zero. However, the gender bias is consistently larger than zero across the range of similarity scores, so the final retrieval is still biased. A desirable debiasing outcome is that for which the regression line aligns with the dotted line (zero gender bias across the similarity range), as shown in Figure 2(c). This is accomplished by the proposed PBM (described below).

Example with multiple queriesIn order to get a more generalizable understanding of the behavior shown in Figure 2, we repeat the analysis with all 45 occupations from the dataset provided by Kay et al. (2015) and visualize the results in Figure 3. Specifically, for each query (occupation), we calculate the Spearman's rank correlation between all (test-set) similarity scores and gender bias similar to Figure 2. We use Spearman's rank correlation because it is more appropriate to quantify associations between (variable) rankings. Moreover, we also calculate the gender bias of the final retrieval bag (\(K=100\)) for each occupation. Consistent with the illustrative findings on the "engineer" query, in Figure 3(a), we see a large correlation between similarity scores and gender bias as well as a large retrieval-set gender bias. MI minimization in Figure 3(b) manages to reduce the correlation between similarity score and gender but suffers from considerable gender bias; mostly clustered around 0.4. The proposed PBM not only reduces the correlation between similarity scores and gender bias but also significantly pushes the bias of each retrieval result to zero, which is the sought target debiasing outcome.

InsightsOur observations can be explained and understood from a theoretical perspective. Previous methods are mostly concerned with making their image features independent of gender information. For a fixed query \(c\), suppose we sample a random image from the test image data distribution \(v^{}\), its feature distribution \(f_{}(v)\) is independent of gender distribution \(g(v)\). Since the query feature \(f_{}(c)\) is constant, the similarity score \(S(v,c)\), defined in Section 3.2, is a deterministic function of \(f_{}(v)\) and is thus also independent of gender \(g(v)\). This is why the results for MI-based unbiasing shown in Figure 2(b) and 3(b) show little correlation between similarity scores and gender bias. Similar results for debiasing with adversarial learning can be found in the SM.

However, as shown in Figure 2(b), there is still a consistent gender bias across similarity score values. This means that the gender distribution in each window tends to manifest the gender distribution of

Figure 3: Comparing top-100 retrieval gender bias with full set similarity gender bias Spearman’s correlation. For each query (occupation), we visualize the correlation between similarity score and gender bias against the top-100 retrieval gender bias. Two typical examples: “engineer” and “administrative assistant” are highlighted for illustration.

Figure 2: Gender bias distribution for different methods using “engineer” as query. We compute similarity scores for all images from the test image set and plot them against gender bias in 1% quantile increments. The red circle marks the top-\(K\) window covering the final retrieval output \(V_{c,K}\).

the whole test image set \(^{}\), which may not be balanced. This is also confirmed in Figure 3(b), as the final output bias of most occupations is clustered around 0.4, which is precisely the gender bias of the entire image set \(^{}\). Therefore, ensuring independence, quantified here in terms of correlation, between image features and gender may not guarantee zero gender bias if the test image set itself is biased due to imbalance.

Two types of biasFrom the examples above, we can differentiate two types of bias, namely, the model bias from training and the bias from the test-time image distribution. The former can be quantified based on the correlation between similarity score and gender and only depends on the training data distribution and the way in which the model is trained. This has been previously described and studied (Caliskan et al., 2017; Zhao et al., 2017). In comparison, the latter type of bias manifests in the test phase because the test image set (the database from which we retrieve images) does not necessarily have commensurate numbers of male and female samples. We refer this type of bias as test-time bias. In fact, such proportions are usually unknown for image databases.

These two sources of bias coexist and can be addressed separately during model training and inference. Previous methods (Edwards and Storkey, 2015; Wang et al., 2021; Berg et al., 2022) have been fairly successful at addressing the first type of bias on the training side, but neglect the test-time bias that is specific to the test set. To tackle the test-time bias, it is necessary to find a strategy targeting the test image set, so that the retrieved image genders are balanced despite the gender imbalance in the training (source) set. This very insight motivates our approach called PBM, which we describe below. As previously shown in Figures 2(c) and 3(c), PBM achieves substantially smaller retrieval gender bias than MI debiasing. Other existing approaches will be considered in the experiments.

### Post-hoc Bias Mitigation (PBM)

To address the second type of bias induced by the imbalanced image test set, one simple idea is sub-sampling. Specifically, we could first sub-sample the image set to make sure its gender ratio is balanced _before_ doing retrieval. However, a clear limitation of such an approach is that some highly relevant images may be dropped during sub-sampling, which may negatively affect retrieval quality. This problem is especially exacerbated if the test set has a very large gender bias. Alternatively, our intuition is to sub-sample _after_ computing and ranking similarity scores of all images using a post-hoc method to control gender bias while sampling from the image source set, which we call Post-hoc Bias Mitigation (PBM).

A general version of the PBM algorithm, which is straightforward, is presented in Algorithm 1. For each image \(v_{i}^{}\), we first predict its gender \((v_{i})\{+1,-1\}\), which splits the images into two subsets, \(^{}_{+1}\) and \(^{}_{-1}\). We then rank images from the two subsets based on their similarity scores separately. While forming the retrieval bag, we sample the top of both subsets together in pairs. Specifically, we select the top \( K/2\) samples from each subset. If \(K\) is odd, we randomly pick one of the subsets and select one more top sample from it. This method ensures low gender bias of the retrieval output as long as gender predictions \((v_{i})\) are accurate.

One extension of our method considers the case where gender may not be applicable to some images. For instance, our image dataset may contain cartoon characters that are not easily associated with any gender, or maybe the gender cannot be determined from the image due to body or facial coverings. In this case, we allow our gender predictions to take N/A values, yielding a \(_{}^{}\) subset. Images from \(_{}^{}\) do not inherently exhibit visually perceptible bias. Thus, they could be selected in favor of other male-female pairs if their similarity scores are higher and are exempt from bias measurement.

Gender predictionTo predict gender \((v)\), we consider the following two methods for different scenarios. Note that all options are considered in the experiments.

Supervised gender classificationFor scenarios where ground-truth gender attributes are available, such as MS-COCO (Lin et al., 2014; Zhao et al., 2021), we can train a complementary small gender classification network. Using our encoded image feature \(f_{}(v)\) as input, we train a 3-layer MLP to classify gender. For datasets where gender may not be applicable to some samples, we add a N/A class to the set of labels for predictions.

Zero-shot inference using word embeddings or promptFor scenarios where supervised training of gender is not possible, we can infer gender in a zero-shot manner (Radford et al., 2021; Li et al., 2022) using the implicit knowledge embedded in the text features of large pre-trained vision-language models (Radford et al., 2021; Li et al., 2022). Given that the semantics of gender and race attributes are already incorporated into the text encoder, we can extract them from word embeddings using words such as "Man" and "Woman". Alternatively, we could use prompts prepended to the occupation search query. For example, we add gender-specific adjectives like "Male" or "Female" in front of a query. Finally, we compute the (cosine) similarity score of each image \(v\) with them, _i. e._, \(S(v\), "Male " + c), \(S(v\), "Female " + c). The plus operator (+) here refers to string concatenation. The gender prediction \((v)\) is then generated by comparing these two similarity scores.

## 4 Experiments

We first evaluate our image retrieval algorithm on two real-world web image search datasets, Occupation 1 (Kay et al., 2015) and Occupation 2 (Celis and Keswani, 2020). We also test on two large-scale image-text datasets, MS-COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) to further validate the effectiveness of PBM in handling more complex text-based image retrieval scenarios.

For comparison, we consider adversarial training (Edwards and Storkey, 2015; Berg et al., 2022) and mutual information minimization (Wang et al., 2021) as baseline methods. We also include other types of debiasing methods such as FairSample (Wang et al., 2021), which balances the gender distribution of image-text pairs within a training batch, and FairExpec (Mehrotra and Celis, 2021), a denoised selection algorithm designed to select fair subset based on noisy demographic attributes. We use AbsBias@KK and Recall@KK as evaluation metrics as described in Section 3.1.

Real-world web image searchThe first dataset, which we refer to as Occupation 1 (Kay et al., 2015), comprises the top 100 Google image search results for 45 gender-neutral occupation terms, such as "chef", "librarian", "primary school teacher", _etc_. Each image within this dataset is annotated with a crowd-sourced gender attribute (either "male" or "female") that characterizes the person depicted in the image. Occupation 2 (Celis and Keswani, 2020), the second dataset, includes the top 100 Google image search results for 96 occupations, where both gender and race (represented as skin-tone: fair and dark skin) annotations are provided. Notably, the gender and race attributes also include a N/A category in Occupation 2, where the annotators have chosen the option of "Not applicable" or "Cannot determine" for the gender or skin-tone represented in the image. Consequently, we treat the images labeled with N/A as neutral examples that do not contribute to the bias of retrieval, since in principle, the users cannot perceive gender or racial information from the image.

For these two datasets, we consider OpenAI's CLIP ViT-B/16 (Radford et al., 2021) as the VL model for all debiasing methods. The baselines for comparison are MI-_clip_ from Wang et al. (2021), adversarial training adapted from Edwards and Storkey (2015), and Debias Prompt from Berg et al. (2022). We have also tailored FairExpec (not originally proposed for TBIR) to our task by integrating it with CLIP and our proposed gender predictor \(()\). We refer to this adapted model as CLIP-FairExpec in our experiments. We test our PBM method with four variants of gender predictors \(()\), as discussed in Section 3.5. These variants include a supervised classifier, zero-shot inference, and ground truth from annotators. The implementation details are provided in the SM.

We summarize the experimental results in Table 1. Compared with other methods, PBM variants achieve significantly lower AbsBias@100 while maintaining comparable Recall@100 scores. The male:female ratio of images in Occupation 1 is approximately 61:49, thereby mitigating the first type of bias by MI-_clip_, adversarial training, debias prompt and random selection is not enough. The CLIP-FairExpec cannot achieve better results since the FairExpec algorithm relies on the independence assumption, while the samples in our test dataset are correlated. This happens because images for occupations are collected from the same Google image search, which is biased. Further, FairExpec requires reliable probabilistic predictions for gender, however, in our setting the gender attributes are provided by an off-the-shelf MLP predictor based on visual features. In such a scenario, the label estimates yielded by the off-the-shelf MLP may not be always trustworthy, as the domain has shifted during inference. Consequently, these estimates could include misleading information, resulting in undesirable debiasing outcomes for CLIP-FairExpec. Moreover, it should be noted that Debias Prompt (Berg et al., 2022) always achieves the highest Recall@100, since we utilize their publicly accessible model, which has been fine-tuned on the Flickr30k dataset to enhance the performance of text-based image retrieval.

Large-scale text-based image retrievalWe consider MS-COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015). Our setup aligns with Wang et al. (2021), where the gender attributes are directly inferred from the text captions of images. We consider the same baseline models as Wang et al. (2021), which are SCAN (Lee et al., 2018) and CLIP, as well as their proposed approach, FairSample and MI-_clip_. It should be noted that FairSample is specifically designed to debias SCAN, a specialized in-domain VL model. We choose the best performing PBM model, which leverages the pre-trained classifier for gender attributes.

The performance metrics in Table 2 are consistent with Wang et al. (2021), which resemble our AbsBias@\(K\) metric, with the omission of using absolute values in the sum. This bias metric is computed as Bias@\(K=_{c C}_{v V_{K}^{C}}g(v)\). From the Table 2, we see that PBM maintains the bias to a minimum even when dealing with intricate text queries or images of complex scenes.

Bias-performance trade-off analysisIn Figure 4, we show the trade-off between retrieval performance and bias for MI-_clip_, adversarial training and the four PBM variants. We choose MI-_clip_ and adversarial training for comparison, as these models offer implementation simplicity of adjusting trade-off between AbsBias@100 and Recall@100. For PBM, we introduce a trade-off parameter through a stochastic variable representing the probability of opting for a fair subset at any given time, as opposed to merely selecting the image with the highest similarity score. Additional details about this experiment are provided in the SM. Our results indicate that in a range of relatively high

   &  &  \\  & AbsBias@100 (L) & Recall@100 (L) & AbsBias@100 (L) & Recall@100 (L) & AbsBias@100 (L) & Recall@100 (L) \\  Random Selection & 6.730 & 3155 & - & - & -4171 & - \\ CLIP Original (Radford et al., 2011) & 6231 & 58.3 & 3566 & 46.2 & 5002 & 46.2 \\ ML-_clip_ (Wang et al., 2021) & 3769 & 47.0 & 2539 & 42.2 & 4099 & 42.3 \\ Adversarial Training (Ghwards and Storkey, 2015) & 3216 & 44.0 & 2603 & 373 & 4880 & 43.3 \\ Debias Prompt (Berg et al., 2022) & 6.737 & **59.3** & 3564 & **46.2** & **59.6** & **59.2** \\ CLIP-FairExpec (Mehert and Celtijn, 2021) & 2408 & 47.0 & 2619 & 44.2 & 2788 & 34.7 \\ 
**PBM** - Zero-shot Embedding & 0969 & 49.8 & 1150 & 42.1 & 3113 & 40.2 \\
**PBM** - Zero-shot Prompt & **08560** & 46.1 & **49483** & 42.5 & 2571 & 36.0 \\
**PBM** - Supervised Classifier & 1.404 & 50.3 & 1171 & 42.1 & **0955** & 37.9 \\
**PBM** - Ground-path Gender and Skin-cone & 0.000 & 49.1 & 0000 & 42.4 & 0000 & 41.3 \\  

Table 1: Results for debiased image retrieval from Occupation 1 and 2 datasets.

   &  &  \\  & Bias@1 (\(\)) & Bias@100 (\(\)) & Recall@100 (\(\)) & Recall@100 (\(\)) & Recall@100 (\(\)) \\   & SCAN (Lee et al., 2018) & 1.250 & 2.064 & 2.506 & 47.7 & 82.0 & **91.0** \\  & FairSample (Wang et al., 2021) &.1140 &.1951 &.2347 & **49.7** & **82.5** & 90.9 \\  & CLIP (Radford et al., 2011) &.0900 &.2042 &.2648 & 48.2 & 77.9 & 88.0 \\  & MI-_clip_ (Wang et al., 2021) & 0.670 &.1541 &.2057 & 46.1 & 75.2 & 86.0 \\  & **Our PBM** & **0.402** & **.0961** & **.1082** & 37.3 & 73.6 & 84.8 \\   & SCAN (Lee et al., 2018) &.1379 &.2133 &.2484 & 25.4 & 54.1 & 67.8 \\  & FairSample (Wang et al., 2021) &.1133 &.1916 &.2288 & 26.8 & **55.3** & **68.5** \\   & CLIP (Radford et al., 2021) &.0770 &.1750 &.2131 & **28.7** & 53.9 & 64.7 \\   & ML-_clip_ (Wang et al., 2021) &.0672 &.1474 &.1611 & 27.3 & 50.8 & 62.0 \\   & **Our PBM** & **0.402** & **.1006** & **.1212** & 22.3 & 50.5 & 61.9 \\   & SCAN(Lee et al., 2018) &.1098 &.3341 &.3960 & 41.4 & 69.9 & 79.1 \\   & FairSample (Wang et al., 2021) &.0744 & 2699 &.3573 & 35.8 & 67.5 & 77.7 \\   & CLIP (Radford et al., 2021) &.1150 &.3150 &.3586 & **67.2** & **89.1** & **93.6** \\   & ML-_clip_ (Wang et al., 2021) &.0960 &.2746 &.2951 & 63.9 & 85.4 & 91.3 \\   & **Our PBM** & **0.360** & **.1527** & **.1640** & 41.2 & 85.3 & 92.6 \\  

Table 2: Results for debiased image retrieval from MS-COCO and Flickr30k datasets.

AbsBias@\(K\) values, MI-_clip_ and adversarial training are still able to reduce bias while preserving Recall@\(K\). However, in terms of lowering AbsBias@\(K\), they struggle to maintain satisfactory TBIR performance. In contrast, the proposed PBM consistently succeeds in sufficiently reducing bias and maintaining performance, provided that predictions of attributes are reasonably accurate. Details of implementing this trade-off can be found in the SM.

## 5 Discussion

In this study, we examined gender and racial bias in text-based image retrieval (TBIR) for neutral text queries. In an attempt to identify bias in the test-time (inference) phase, we conducted an in-depth quantitative analysis on bias reduction, alongside existing debiasing methods and the proposed PBM. We concluded that solely addressing training-time model-encoded bias is not sufficient for obtaining equal representation results, because test-time bias also exists due to imbalance in the test image set used during retrieval. So motivated, we proposed Post-hoc Bias Mitigation (PBM), a straightforward post-processing method that aims to directly alleviate test-time bias. Experiments on multiple datasets show that our method can significantly reduce bias while maintaining satisfactory retrieval accuracy at the same time.

Moreover, the potential impact of PBM extends far beyond the initial scope of text-based image retrieval systems. The core concept of our methodology can be seamlessly adapted to a wide variety of information retrieval systems, such as image-based text retrieval or query-by-example image retrieval, as long as the demographic information of the test set is accessible or can be estimated, _e.g._, via zero-shot inference. Overall, our approach is not limited to enhancing fairness in text-based image retrieval, thus can be extended to a broad range of VL model applications.

## Limitations

Some limitations of the proposed method are duly acknowledged. Firstly, the efficacy of our approach is dependent upon the availability of a sufficient number of examples for each category (gender or racial attribute) within the test image set. Our work currently does not consider any techniques, such as using synthetic samples, to mitigate the issues arising from insufficient representations of certain demographic groups. Secondly, the debiasing effect of PBM pertains to the predictability and accessibility of demographic attributes. Attempts at debiasing religious representation, or other socio-cultural factors or identities, in images or speech present significant challenges, because the predicting or securing annotations regarding religious information, can be exceptionally difficult. Thirdly, as we prioritize the "equal representation", our retrieval results sacrifice recall performance to ensure a retrieval bag that contains equal representations of each demographic group. This compromises fairness towards content providers in the image retrieval process. From the standpoint of content providers, fairness should imply that similar samples are treated similarly, regardless of the demographic group membership of their provided samples. Lastly, our work assumes all queries are neutral. We do not develop a technique to identify if a query is neutral or not, thereby limiting the applicability of our method in hybrid text query retrieval where text query can be biased like in "Male doctor". It is important to note, however, that the above challenges are not unique to our method but are a common issue encountered in other debiasing approaches. A more comprehensive discussion on the limitations of our work is available in the SM.

Figure 4: Trade-off between Recall@K and AbsBias@K for debiasing gender attributes within Occupation 1 (Middle) and race attributes using Occupation 2 (Right).

[MISSING_PAGE_FAIL:11]

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_ (2020).
* Edwards and Storkey (2015) Harrison Edwards and Amos Storkey. 2015. Censoring representations with an adversary. _arXiv preprint arXiv:1511.05897_ (2015).
* Faghri et al. (2017) Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2017. Vse++: Improving visual-semantic embeddings with hard negatives. _arXiv preprint arXiv:1707.05612_ (2017).
* Friedler et al. (2014) Sorelle Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2014. Certifying and removing disparate impact. _arXiv preprint arXiv:1412.3756_ (2014).
* Frome et al. (2013) Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visual-semantic embedding model. _Advances in neural information processing systems_ 26 (2013).
* Optimization (2023) Gurobi Optimization, LLC. 2023. Gurobi Optimizer Reference Manual. https://www.gurobi.com
* Hall et al. (2023) Melissa Hall, Laura Gustafson, Aaron Adcock, Ishan Misra, and Candace Ross. 2023. Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities. _arXiv preprint arXiv:2301.11100_ (2023).
* Hardt et al. (2016) Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. _Advances in neural information processing systems_ 29 (2016).
* Hendricks et al. (2018) Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. 2018. Women also snowboard: Overcoming bias in captioning models. In _Proceedings of the European conference on computer vision (ECCV)_. 771-787.
* Jalal et al. (2021) Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. 2021. Fairness for image generation with uncertain sensitive attributes. In _International Conference on Machine Learning_. PMLR, 4721-4732.
* Kay et al. (2015) Matthew Kay, Cynthia Matuszek, and Sean A Munson. 2015. Unequal representation and gender stereotypes in image search results for occupations. In _Proceedings of the 33rd annual acm conference on human factors in computing systems_. 3819-3828.
* Kim et al. (2023) Jae Myung Kim, A Koepke, Cordelia Schmid, and Zeynep Akata. 2023. Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval. _arXiv preprint arXiv:2304.03391_ (2023).
* Kusner et al. (2017) Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual fairness. _Advances in neural information processing systems_ 30 (2017).
* Lee et al. (2018) Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention for image-text matching. In _Proceedings of the European conference on computer vision (ECCV)_. 201-216.
* Lew et al. (2006) Michael S Lew, Nicu Sebe, Chabane Djeraba, and Ramesh Jain. 2006. Content-based multimedia information retrieval: State of the art and challenges. _ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)_ 2, 1 (2006), 1-19.
* Li et al. (2022) Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. 2022. Elevater: A benchmark and toolkit for evaluating language-augmented visual models. _Advances in Neural Information Processing Systems_ 35 (2022), 9287-9301.
* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_. Springer, 740-755.
* Liu et al. (2016)* Mehrotra and Celisa Celis (2021) Anay Mehrotra and L Elisa Celis. 2021. Mitigating bias in set selection with noisy protected attributes. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_. 237-248.
* Mehrotra and Vishnoi (2022) Anay Mehrotra and Nisheeth Vishnoi. 2022. Fair Ranking with Noisy Protected Attributes. _Advances in Neural Information Processing Systems_ 35 (2022), 31711-31725.
* Morik et al. (2020) Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. Controlling fairness and bias in dynamic learning-to-rank. In _Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval_. 429-438.
* Mukhoti et al. (2022) Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, and Ser-Nam Lim. 2022. Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning. _arXiv preprint arXiv:2212.04994_ (2022).
* Noble (2018) Safiya Umoja Noble. 2018. Algorithms of oppression. In _Algorithms of oppression_. New York University Press.
* Organization (2019) International Labour Organization. 2019. A quantum leap for gender equality: For a better future of work for all. (2019).
* Parraga et al. (2022) Otavio Parraga, Martin D More, Christian M Oliveira, Nathan S Gavenski, Lucas S Kupssinsku, Adilson Medronha, Luis V Moura, Gabriel S Simoes, and Rodrigo C Barros. 2022. Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey. _arXiv preprint arXiv:2211.05617_ (2022).
* Plummer et al. (2015) Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_. 2641-2649.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In _International conference on machine learning_. PMLR, 8748-8763.
* Saxena et al. (2020) Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C Parkes, and Yang Liu. 2020. How do fairness definitions fare? Testing public attitudes towards three algorithmic definitions of fairness in loan allocations. _Artificial Intelligence_ 283 (2020), 103238.
* Singh and Joachims (2017) Ashudeep Singh and Thorsten Joachims. 2017. Equality of opportunity in rankings. In _Workshop on Prioritizing Online Content (WPOC) at NIPS_, Vol. 31.
* Singh et al. (2003) Kulwinder Singh, Ming Ma, and Dong-Won Park. 2003. A Content-based Image Retrieval using FFT & Cosine Similarity Coefficient.. In _SIP_. 315-319.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in neural information processing systems_. 6000--6010.
* Wang et al. (2021a) Jialu Wang, Yang Liu, and Xin Eric Wang. 2021a. Are gender-neutral queries really gender-neutral? mitigating gender bias in image search. _arXiv preprint arXiv:2109.05433_ (2021).
* Wang et al. (2021b) Jialu Wang, Yang Liu, and Xin Eric Wang. 2021b. Assessing multilingual fairness in pre-trained multimodal representations. _arXiv preprint arXiv:2106.06683_ (2021).
* Wang et al. (2022) Junyang Wang, Yi Zhang, and Jitao Sang. 2022. FairCLIP: Social Bias Elimination based on Attribute Prototype Learning and Representation Neutralization. _arXiv preprint arXiv:2210.14562_ (2022).
* Wang et al. (2023) Rui Wang, Pengyu Cheng, and Ricardo Henao. 2023. Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling. _arXiv preprint arXiv:2302.13136_ (2023).
* Wang et al. (2021)Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang. 2021. To be robust or to be fair: Towards fairness in adversarial training. In _International Conference on Machine Learning_. PMLR, 11492-11501.
* Zafar et al. (2015) Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. 2015. Learning fair classifiers. _arXiv preprint arXiv:1507.05259_ 1, 2 (2015).
* Zaidi et al. (2019) Syed Ali Jafar Zaidi, Attaullah Buriro, Mohammad Riaz, Athar Mahboob, and Mohammad Noman Riaz. 2019. Implementation and comparison of text-based image retrieval schemes. _International Journal of Advanced Computer Science and Applications_ 10, 1 (2019), 611-618.
* Zhao et al. (2021) Dora Zhao, Angelina Wang, and Olga Russakovsky. 2021. Understanding and evaluating racial biases in image captioning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 14830-14840.
* Zhao et al. (2017) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. _arXiv preprint arXiv:1707.09457_ (2017).

## Appendix A Impact of Demographic Group Classifier on Debiased Results

The demographic group classifier is an important module of our proposed method PBM. The debiasing result is intricately linked to the demographic group classifier's accuracy and prediction bias towards different demographic groups. Figure 6 showcases the relationship between the demographic group classifier's performance and the ensuing retrieval bias, by artificially introducing noise to the demographic group (logit) predictions via Gaussian noise with a standard deviation ranging from 0 (no noise) to 1. These results underscore that better group classifier performance yields lower bias, that bias converges to that of the original CLIP as the group classifier gets worse, and importantly, that the bias after PBM will be no worse than that of the original CLIP.

Further, Table 3 shows the individual demographic group sensitivities under three different scenarios, from which we can see that the group classifier is i) able to achieve good classification sensitivity (no lower than 0.81 and 0.90 in average), likely because demographic image attributes (gender and skin tone) are typically captured in images, and ii) that different scenarios exhibit different degrees of bias as measured by the group sensitivity ratio, which must be close to 1 for the model to be unbiased. Table 3 reveals sensitivity discrepancies among different attributes. To delve deeper into the influence of classifier bias on PBM outcomes, we present the retrieval bias as a function of the sensitivity ratio in Figure 6. This is achieved by altering the gender classification threshold from 0 (maximizing male sensitivity) to 1 (minimizing male sensitivity). From Figure 6, we can conclude that the classifier bias does affect retrieval bias, however, only severely for more extreme sensitivity ratios, which is fortunately not the case in our results as shown in Table 3.

## Appendix B Bias-recall Trade-off Strategies

In Figure 4, we exhibit the bias-recall trade-off curves for MI-clip, Adversarial Training, and various PBM methods. Here, we outline the missing details to achieve these trade-offs.

For adversarial learning, the trade-off is controlled by adjusting the adversarial loss weights between 0 and 1.0. In MI-clip, we modify the clipped dimensions from 10 to 500 (CLIP output dimension is 512). Regarding PBM methods, a trade-off parameter is introduced via a stochastic variable \(\), which denotes the likelihood of choosing a fair subset at any given time, instead of simply opting for the image with the top similarity score. Each curve is plotted by interpolating 10 points of the corresponding trade-off parameters.

Datasets

Occupation 1 (Kay et al., 2015)Occupation 1 comprises the top 100 Google Image Search results for 45 gender-neutral occupation terms, such as "chef", "librarian", "primary school teacher", _etc_. Each image within this dataset is annotated with a crowd-sourced gender attribute (either Male or Female) that characterizes the person depicted in the image. The entire Occupation 1 dataset is exclusively utilized for evaluating gender debiasing effect, as shown in Table 1.

Occupation 2 (Celis and Keswani, 2020)Occupation 2 includes the top 100 Google Image Search results for 96 occupations. Each image in the dataset comes annotated with a gender attribute and a race attribute (represented by skin-tone, namely, Fair Skin and Dark Skin). Notably, the gender attribute and race attribute also include a N/A category, where the annotators have chosen the option of "Not applicable" or "Cannot determine" for the gender or skin-tone depicted in the image. Consequently, we treat the image labeled with N/A as a neutral example that does not contribute to the bias of retrieval, since the user cannot perceive gender or racial information from the image. Different from Occupation 1, gender attributes in Occupation 2 are categorized as {Male, Female, N/A}, while race attributes are classified as {Fair Skin, Dark Skin, N/A}. The entire Occupation 2 dataset is only used for evaluation on mitigating gender and race bias, as the results shown in Table 1.

MS-COCO (Lin et al., 2014)The first large-scale image-text dataset is MS-COCO captions dataset, which is partitioned into 113,287 training images, 5,000 validation images, and 5,000 test images. Each image is accompanied by five corresponding captions. Our experimental setup aligns with the methodology detailed by Wang et al. (2021). Only the first caption of each image is used for evaluation. Further, they ensure all captions are gender-neutral by identifying and replacing or removing gender-specific words with corresponding neutral terms, with the help of predefined word banks (Zhao et al., 2017; Hendricks et al., 2018).

Flickr30k (Plummer et al., 2015)The second large-scale image-text dataset employed in our experiment is Flickr30K, which contains 31,000 images obtained from Flickr. Adhering to the partitioning scheme presented in Plummer et al. (2015), we allocate 1,000 images each for validation and testing, with the remaining images designated for training. We obtain the ground truth of gender attributes of images in Flickr30k in the same way as MS-COCO (Wang et al., 2021), as we detect the gender-specific words in the caption to determine the gender attributes of its corresponding image.

## Appendix D Baseline Models

Random SelectTo simulate an ideal scenario, where image features bear no dependency to gender (and race) attributes, for a neutral query \(c\), we randomly select \(K\) candidates from the true relevant image set \(V_{c}^{*}\), with replacement. As for each query \(c\), the size of the relevant image set \(|V_{c}^{*}|\) is at most 100. Using sampling with replacement simulate the situation that the gender attribute distribution is fixed, and irrelevant to retrieval algorithm. We report AbsBias@\(K\) for reference. The Recall@\(K\) is omitted since the value is meaningless, as we only sample from the true relevant image set \(V_{c}^{*}\).

CLIP (Radford et al., 2021)We consider OpenAI's CLIP ViT-B/16 (Radford et al., 2021) as the VL model for all debiasing methods. Specifically, the image encoder \(f_{}()\) is a Vision Transformer (ViT) (Dosovitskiy et al., 2020) comprising 12 transformer blocks of width 768, with 12 self-attention heads in each block. ViT processes images of size \(224 224\) by dividing them into \(16 16\) patches and outputs 512-dimensional image features by linear projection. The text encoder \(f_{}()\) is a standard text transformer (Vaswani et al., 2017) with masked self-attention, consisting of 12 transformer blocks of width 512 and 8 self-attention heads in each block, with a linear projection layer at the end as well. The CLIP ViT-B/16 model is loaded with pre-trained weights provided by OpenAI (Radford et al., 2021). All the following debiasing methods use this pre-trained CLIP ViT-B/16.

MI-clip (Wang et al., 2021)MI-clip (Wang et al., 2021) clips the fixed number of output dimensions of the image encoder in CLIP to reduce the mutual information between image features and demographic attribute distribution. For MI-clip in Table 1, we clip 312 dimensions of output image features. These 312 dimensions were chosen by examining the reduction in bias and while maintaining retrieval performance. We also show a trade-off between bias reduction and retrieval performance of MI-_clip_ with the number of clipped dimensions from 100 to 500 on the Occupation 1 dataset in Figure 4.

Adversarial Training (Edwards and Storkey, 2015)For adversarial training, we use the same minimax problem setup as in (Edwards and Storkey, 2015). The encoder is the original CLIP image encoder. The decoder is realized by a ViT with 8 vision transformer blocks, and the adversarial predictor is a 3-layer MLP. Training employs the same loss function in Edwards and Storkey (2015), where the final loss is the sum of the cost of reconstructing \(v\) from \(f_{}(v)\), a measure of dependence between \(f_{}(v)\) and \(g(v)\) and the error of target task (_i.e.,_ image-text aligning loss \(_{}\)(Radford et al., 2021)). We assign different weights to the loss of dependence measuring loss, in order to demonstrate the trade-off between bias reduction and retrieval performance. We report the adversarial learning results when the weight of measuring dependence loss is 0.7, and the weights for the other two losses are both 0.15. Additional weight combinations were considered and shown in Figure 4.

Debias Prompt (Berg et al., 2022)The Debias Prompt method (Berg et al., 2022) also leverages an adversarial learning framework. However, instead of just fine-tuning the image and text encoders, they also prepended zero-initialized learnable prompts before inputting query tokens. Considering that their debias-prompt model is already debiased for gender and race attributes, we directly evaluate their pre-trained model (sourced from their github repository https://github.com/oxai/debias-vision-lang) on the Occupation 1 and Occupation 2 datasets.

CLIP-FairExpec (Mehrotra and Celis, 2021)We tailor FairExpec (_not originally proposed_ for TBIR) to our task by integrating it with CLIP and our proposed gender predictor \(()\). We refer to this adaptation as CLIP-FairExpec in our experiments.

Using binary gender as an example for simplicity, our CLIP-FairExpec treats the image-text similarity output as the utility score for each image. Then, the objective of the optimization is to maximize the total similarity scores for selecting \(K\) images corresponding to a query \(c\). The noise estimate \(q\) in the original FairExpec is derived from the probability output of our attribute predictor \((v)\). We use the probability output from the attribute predictor \((v)\) as the noise estimate \(q\). Also, there is a constraint on the sum of the noise estimates \(q\) such that the sum is at least \(L- K\) and at most \(U- K\), where \(L\) and \(U\) is the lower bound and upper bound for the sum of the noise estimate, respectively. \((0,1)\) is a noise tolerance level, that controls how much the constraints can be violated due to the presence of noise. Since our fairness objective is equal representation for each gender attribute class, we wish the sum of noise estimate for each class of gender attribute is equal. Hence, we set the \(L=U=K/2\). In order to force our model to prioritize minimizing bias over maintaining performance, we choose a very small \(=0.001\). We select \(K\) images from \(\) with respect to a neutral query \(c\) based on the above constrained optimization problem. Further, each selection is solved by the GUROBI solver (Gurobi Optimization, LLC, 2023). Upon solving for all selections for queries in \(\), we compute the AbsBias@\(K\) and Recall@\(K\) presented in Table 1.

SCAN (Lee et al., 2018)We consider the Stacked Cross Attention Network (SCAN) (Lee et al., 2018) as an alternative VL model to CLIP. SCAN is a specialized in-domain training model, so it is trained on the MS-COCO training dataset and tested on the MS-COCO test dataset. Similarly, for the experiments with Flickr30k, the model is trained on the Flickr30k training set and then tested on the Flickr30k test set. We use official implementation of SCAN from https://github.com/kuanghuei/SCAN.

FairSample (Wang et al., 2021a)To mitigate bias during the training of SCAN, we implement the FairSample approach as recommended by Wang et al. (2021a). We maintain the same hyperparameters settings as Lee et al. (2018). To address the bias arising from the unbalanced gender distribution within training batches, FairSample is proposed in the following way: for every positive image-text pair \((v,c)\) within a training batch, we first identify if the query \(c\) is gender-neutral or gender-specific. If the training query \(c\) is gender-neutral, a negative image is sampled from either the male or female image sets, each with a probability of 1/2. However, if the query is gender-specific, we maintain the original negative sampling strategy, thereby preserving the model's ability to generalize effectively on such queries.

Post-hoc Bias Mitigation (PBM)

### Engineering Details

PBM - Supervised ClassifierWe can determine the gender attributes with a pre-trained image classifier. Here, the image classifier is pre-trained on MS-COCO training set with gender attribute annotations from Zhao et al. (2021). The image classifier is a 3-layer multi-layer perceptron (MLP) as shown in Table 5, that takes the image representation from the original CLIP as input. We empirically show that the image classifier can be highly accurate even using a light-weight classification MLP. The F1-score for gender attribute prediction is 92.8%.

PBM - Zero-Shot EmbeddingWe describe the first of the two types of zero-shot inference described in Section 3.5. For zero-shot inference based on the embedding approach, we choose the text embeddings for {"Unknown Gender", "Man", "Woman"} tokens to classify the gender attributes of images, and {"Unknown Skin", "Fair Skin", "Dark Skin"} for categorizing race attributes. The gender or race attribute of an image \(v\) is determined by which text embedding has the maximum similarity score to the image representation \(f_{}(v)\).

PBM - Zero-Shot PromptFor the second zero-shot inference described in Section 3.5, namely, the prompt method, we prepend adjectives to the text query \(c\). We use {","Male", "Female"} for gender attributes and {"*", "Fair-skinned", "Dark-skinned"} for race attributes. The gender attributes for each image retrieved by the query \(c\) is determined by which prompted query has the maximum similarity score to the image representation \(f_{}(v)\).

PBM - Ground-Truth Attribute (Gender or Skin-tone)We use the annotations in the dataset as the predicted attributes \((v)\) for reference. This shows the upper-bound performance of our method if all gender predictions are correct (known).

### Additional PBM Results

In Table 4, we showcase the results of applying PBM to CLIP models that has been debiased by other approaches, such as MI-clip, Adversarial Learning, and Debias Prompt. When PBM is utilized in conjunction with other debiasing strategies, it exhibits a unique bias-recall trade-off, thus catering to a variety of application scenarios.

## Appendix F Neural Network Architectures

We summarize the details of the neural networks employed in our experiments in Table 5. For the Image Encoder, the Patch Extraction (dimensions: 16,16) extracts 196 non-overlapping \(16 16\) patches from the 224\(\)224 image. These extracted patches are subsequently flattened. The subsequent Positional and Linear Embedding (768) maps these patch vectors onto a 768-dimensional space and adds 2D positional embeddings of patches to the 768-dimensional vectors. Next, 12 Vision Transformer Blocks (768, 12) processes the 768-dimensional embeddings. Each of these blocks features 12 self-attention heads. Lastly, the output embedding is obtained from a unique classification token ([CLS]) that we add to the input sequence of patch embeddings. The output from [CLS] Token \(1 768\) is then reduced from 768 dimensions to 512 dimensions using a Linear Projection (512).

Similarly in the Text Encoder, the initial phase involves Positional and Token Embedding (512). This step maps each token in the input text onto a 512-dimensional vector space and integrates positional embeddings into these vectors. Following this, the text encoder employs 12 Transformer Blocks

   &  &  \\  & AbsBias@100 (\(\)) & Recall@100(\(\)) & AbsBias@100(\(\)) & Recall@100(\(\)) \\ 
**PBM** &.1404 & 50.3 &.0955 & 37.9 \\  MI-_clip_ **- PBM** & **.0780** & 42.1 & **.0737** & 29.1 \\ Adversarial Training - **PBM** &.1000 & 39.6 &.0997 & 35.7 \\ Debias Prompt - **PBM** &.1711 & **52.1** &.1035 & **40.6** \\  

Table 4: Results of applying PBM - Supervised Learning on modified or fine-tuned CLIP.

(512, 8) to process these 512-dimensional embeddings. Each of these blocks contains 8 self-attention heads. Finally, the output embedding is derived from [CLS] Token \(1 512\). The subsequent Linear Projection (512) then maps the extracted text representation onto the multi-modal embedding space that aligns with the image embeddings.

## Appendix G Computation Resources

All of our experiments ran on one NVIDIA TITAN Xp 12GB GPU with CUDA version 11.5.

## Appendix H Code and Data Availability

Occupation 1 dataset is available at https://github.com/mjskay/gender-in-image-search.

Occupation 2 dataset can be downloaded from https://drive.google.com/drive/folders/1j915ESc-7NRCZ-zSD0C6LHjeNp42RjkJ.

MS-COCO dataset can be access through https://cocodataset.org/#home, and its crowd-sourced gender/racial annotations from https://princetonvisualai.github.io/imagecaptioning-bias/.

Flickr30k dataset can be access via https://shannon.cs.illinois.edu/DenotationGraph/. And the gender word banks to identify the gender attributes of Flickr30k's images is avaliable in the Appendix of the paper by Wang et al. (2021).

## Appendix I Broader Impact

The recent years constituting what can be called the model architecture unifying era, witnessed a seismic shift from small task-specific models to foundation models containing billions of parameters, with numerous applications deployed based on such large models. However, as artificial intelligence (AI) systems become more prevalent, the challenging question of fairness becomes more urgent. The concept of fairness in machine learning revolves around creating algorithms and models that DO NOT discriminate against certain groups based on gender, race, socioeconomic status, or any other potentially biasing factors. As machine learning algorithms are increasingly used in decision-making

  \\ 
**Layer** & **Type** \\ 
1 & Patch Extraction(16, 16) \\
2 & Positional and Linear Embedding(768) \\
4 - 15 & Vision Transformer Blocks(768, 12) \\
16 & [CLS] Token \(1 768\) \\
17 & Linear Projection (512) \\   \\ 
**Layer** & **Type** \\ 
1 & Positional and Token Embedding (512) \\
2 - 13 & Transformer Blocks (512, 8) \\
14 & [CLS] Token \(1 512\) \\
15 & Linear Projection (512) \\   \\  Layer & Type \\ 
1 & fc-512 + BatchNorm + ReLU() \\
2 & fc-512 + BatchNorm + ReLU() \\
3 & fc-512 + BatchNorm + ReLU() \\
4 & fc-n\_class + Softmax() \\  

Table 5: The architecture of each component of CLIP and the MLP used in our experiments.

processes, from job applications, college admissions, to criminal justice and healthcare, subsets of the population who represent minorities may see unfavoring model performance compared to individuals in majority groups. Therefore it is imperative to develop unbiased machine learning systems such that decisions are made fairly and equitably. Our study concerning fair image retrieval, among many other fairness research works, can be used to inform policymakers about the potential risks and benefits of AI systems, potentially enacting new laws and regulations to ensure that these systems are utilized responsibly and ethically.

Specifically, the biased performance of a model is possibly caused by statistical skewness both in the training and testing sets. Existing methods mainly focus on enforcing independence between the model's output and sensitive attributes during training. However, much less effort has been made to mitigate bias during test-time, a potentially vital component of the debiasing procedure. Many machine learning systems are deployed in a setting where the biased testing set is almost guaranteed, and as such, may suffer from fairness concerns. Importantly, PBM is able to dissociate the ranking similarity from sensitive/protected attributes (_e.g._, gender) thus reducing bias, meaning that image candidates share an equal chance to be retrieved even in an unbalanced testing set. We do not claim that PBM guarantees fairness, and there is always the risk that it may be misinterpreted or exploited, but we hope that PBM encourages a more inclusive approach to AI development.