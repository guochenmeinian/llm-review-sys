# Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory

Sokhna Diarra Mbacke

Universite Laval

sokhna-diarra.mbacke.1@ulaval.ca &Florence Clerc

McGill University

florence.clerc@mail.mcgill.ca &Pascal Germain

Universite Laval

pascal.germain@ift.ulaval.ca

###### Abstract

Since their inception, Variational Autoencoders (VAEs) have become central in machine learning. Despite their widespread use, numerous questions regarding their theoretical properties remain open. Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the data-generating distribution. Then, we utilize this result to develop generalization guarantees for the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance between the input distribution and the distribution defined by the VAE's generative model.

## 1 Introduction

In recent years, deep generative models have exhibited tremendous empirical success. Two of the most important families of generative models are Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational Autoencoders (Kingma and Welling, 2014; Rezende et al., 2014). GANs take an adversarial approach, whereas VAEs are based on maximum likelihood estimation and variational inference. VAEs comprise two main components: an encoder which parameterizes an approximation of the posterior distribution over the latent variables, and a decoder which parameterizes the likelihood. In addition to generative modelling tasks such as image generation (Vahdat and Kautz, 2020) and text generation (Bowman et al., 2016), VAEs have been successfully applied to other topics such as semi-supervised learning (Kingma et al., 2014), anomaly detection (An and Cho, 2015), and dimensionality reduction (Kaur et al., 2021). However, despite their empirical success, the question of statistical guarantees for the performance of VAEs remains largely open. Namely, how can one certify that VAEs generalize well, both in terms of reconstruction and generation?

PAC-Bayesian theory (McAllester, 1999; Catoni, 2003) is an influential tool of statistical learning theory dedicated to providing generalization bounds for machine learning models. PAC-Bayes has been applied to a wide variety of problems such as classification (Germain et al., 2009; Parrado-Hernandez et al., 2012), meta-learning (Amit and Meir, 2018), co-clustering (Seldin and Tishby, 2010), domain adaptation (Germain et al., 2020), and online learning (Haddouche and Guedj, 2022). In recent years, PAC-Bayes has been used to derive non-vacuous generalization bounds for supervised learning algorithms based on neural networks (Dziugaite and Roy, 2018; Perez-Ortiz et al., 2021). See Guedj (2019) and Alquier (2021) for excellent surveys.

The objective of this work is to utilize PAC-Bayesian theory to derive statistical guarantees for VAEs. Our generalization bounds investigate the reconstruction, regeneration, as well as the generation properties of VAEs.

### Related Works

In order to explain the empirical success of deep generative models, a lot of attention has been put into deriving theoretical guarantees for these models. Most of the results, however, have been dedicated to GANs and their variants (Arora et al., 2017; Zhang et al., 2018; Liang, 2021; Singh et al., 2018; Schreuder et al., 2021; Biau et al., 2021; Mbacke et al., 2023). A possible explanation for this plethora of theoretical results is the adversarial loss function, which directly offers an estimation of the discrepancy between the input distribution and the generator's distribution. Despite being central tools in modern machine learning, VAEs have not benefited from such a thorough theoretical analysis (Chakrabarty and Das, 2021).

The work of Chakrabarty and Das (2021) studies the regeneration properties of Wasserstein autoencoders (WAEs) (Tolstikhin et al., 2018), which come from the same family as VAEs. Using VC theory, Chakrabarty and Das (2021) derive rates of convergence for the Wasserstein distance between the input distribution and the distribution regenerated by the WAE, as well as the total variation distance between the empirical latent distribution and the latent prior. Taking a more empirical approach, Cherif-Abdellit et al. (2022) use PAC-Bayes to study the generalization properties of stochastic reconstruction models. They define a \(\)-bounded reconstruction loss function, then utilize McAllester's bound (McAllester, 2003) to formulate a generalization bound for models with probabilistic neural networks (Langford and Caruana, 2001). Then, they re-scale their loss and compare the empirical results to the reconstruction of standard VAEs on benchmark datasets.

We also mention the work of Mbacke et al. (2023), who developed PAC-Bayesian bounds for the analysis of adversarial generative models. Using McDiarmid's inequality, they proved upper bounds on the distance between the input distribution and the generator's distribution, for WGANs (Arjovsky et al., 2017) and EBGANs (Zhao et al., 2017).

### Our Contributions

In this work, we derive theoretical guarantees for variational autoencoders using PAC-Bayesian theory. We provide three types of guarantees: reconstruction guarantees showing that VAEs can successfully reconstruct unseen samples from the input distribution; regeneration guarantees proving upper bounds on the Wasserstein distance between the input distribution and the distribution regenerated by the VAE, given the training set as input; and finally, generation guarantees showing upper bounds on the Wasserstein distance between the data-generating distribution and the VAE's generated distribution defined by the latent prior and the decoder. To the best of our knowledge, these are the first generalization bounds for the standard VAE's reconstruction and regeneration properties, as well as the first statistical guarantees for the VAE's generative model.

In our analysis, the PAC-Bayesian posterior coincides with the variational posterior, which requires the PAC-Bayesian posterior to be conditional. Since, to the best of our knowledge, such PAC-Bayes bounds do not exist in the literature, we start by developing the first PAC-Bayesian bound for conditional posterior distributions. Then, we provide upper bounds for the VAE's performance under two main assumptions: we start by assuming the instance space is bounded, then we take advantage of the manifold hypothesis. Our bounds are functions of the optimization objective of the VAE, namely, the empirical reconstruction loss, and the empirical KL-loss.

The remainder of this paper is organized as follows. In Section 2, we define some preliminary concepts, then briefly introduce VAEs and PAC-Bayesian theory. Section 3 presents our general PAC-Bayesian theorem for conditional posteriors. Then, in Sections 4 and 5, we present our generalization bounds for the reconstruction loss, and the regeneration and generation guarantees.

## 2 Preliminaries

### Definitions and Notations

Given metric spaces \((,d)\) and \((,d^{})\), and a real number \(K>0\), a function \(f:\) is \(K\)-Lipschitz continuous if for any \(,\), we have

\[d^{}(f(),f()) Kd(,).\]

The smallest \(K\) such that this condition is satisfied is called the _Lipschitz norm_ or _Lipschitz constant_ of \(f\) and is denoted \(\|f\|_{}\). Moreover, the set of \(K\)-Lipschitz continuous functions \(f:\) is denoted \(_{K}(,)\) (the underlying metrics will be clear from the context).

Throughout the paper, we use lower case letters \(p,q\) to denote both probability distributions and their densities w.r.t. the Lebesgue measure. We may add variables between parentheses to improve readability (e.g. \(p()\) to emphasize that \(p\) is a distribution on the space of variables \(\), and \(q(|)\) to indicate that \(q\) is a conditional distribution). The set of probability measures on a space \(\) is denoted \(^{1}_{+}()\). The Kullback-Leibler (KL) divergence between \(p,q^{1}_{+}()\) is denoted \((p\,||\,q)\). We omit the absolute continuity condition \(p q\) in the statements of the results below, since if it is not satisfied, then one may assume the KL divergence is infinite and the bounds hold trivially.

Integral Probability Metrics (IPMs, see Muller (1997)) are a class of pseudo-metrics defined on the space of probability measures. Given a family \(\) of real-valued functions defined on \(\), the IPM defined by \(\) is denoted \(d_{}\) and defined as

\[d_{}(p,q)=_{f}| f\,dp- f\,dq|,  p,q^{1}_{+}().\] (1)

Stemming from the theory of optimal transportation (Villani, 2009), the Wasserstein distances (see Definition A.2) are a class of metrics between probability measures. The Wasserstein distance of order \(1\), also referred to simply as _the Wasserstein distance_, is the IPM defined by the set \(=\{f:\|f\|_{} 1\}\).

Finally, we recall the definition of a _pushforward measure_. Let \(p\) be a probability distribution on a space \(\) and \(g:\) be a measurable function. The pushforward measure defined by \(g\) and \(p\) and denoted \(g^{}\!p\) is a probability distribution on \(\) defined as \(g^{}\!p(A)=p(g^{-1}(A))\), for any measurable set \(A\). In other words, sampling \( g^{}\!p\) means sampling \( p\) first, then setting \(=g()\).

### Variational Autoencoders

We consider a Euclidean observation space \(\), a data-generating distribution \(^{1}_{+}()\), and a latent space \(=^{d_{}}\). VAEs comprise two main components: the encoder network whose parameters are denoted \(\), and the decoder network whose parameters are denoted \(\). For simplicity, we may refer to \(\) and \(\) as the encoder and decoder respectively. The encoder parameterizes a distribution \(q_{}(|)\) over the latent space \(\), which is a variational approximation of the Bayesian posterior \(p_{}(|)\). The likelihood \(p_{}(|)\) is parameterized by the decoder network. In this work, we consider the standard VAE, with a standard Gaussian prior \(p()=(,)\) on \(\) and Gaussian latent distributions \(q_{}(|)\). More precisely, for any \(\), the distribution \(q_{}(|)\) is a Gaussian distribution with a diagonal covariance matrix \((_{}(),(_{}^{2} ()))\), where

\[_{}:=^{d_{}} _{}:^{d_{}}_{  0}.\]

Note that \(()\) denotes the diagonal matrix whose main diagonal is the vector \(\). In order to simplify some of the expressions below, it may be useful to express the encoder network as a function

\[Q_{}:^{2d_{}},Q_{}()=_{}()\\ _{}().\] (2)

We express the decoder as a parametric function \(g_{}:\). For any \(\), upon receiving \( q_{}(|)\), the decoder's output \(g_{}()\) is a reconstruction of \(\). Given a training set \(S=\{_{1},,_{n}\}\), the encoder and decoder networks are jointly trained by minimizing the following objective:

\[_{}(,)=_{i=1}^{n}_{ q_{}(|_{i})}\,[- p_{}( _{i}|)]+(q_{}(|_{i })\,||\,p()),\] (3)where the first part of (3) is the _reconstruction loss_ and the second part is the KL-divergence between the latent distributions (associated to the training samples) and the prior over the latent space, weighted by a hyperparameter \(>0\)(Higgins et al., 2017). The reconstruction loss measures the similarity between \(\) and its reconstruction \(g_{}()\), and can be defined in many ways. With a Gaussian likelihood, the reconstruction loss is the squared \(L_{2}\) norm \(\|-g_{}()\|^{2}\).

After training, the VAE defines a generative model using the prior \(p()\) and the decoder \(g_{}\)(Kingma and Welling, 2014). The distribution \(g_{} p()^{1}_{+}()\) allows one to generate new samples by first sampling a latent vector from the prior, then passing it through the decoder. We refer to \(g_{} p()\) as the VAE's generated distribution.

### A Brief Introduction to PAC-Bayesian Theory

Dating back to McAllester (1999), PAC-Bayesian theory develops high-probability generalization bounds for machine learning algorithms. In essence, PAC-Bayes frames the output of such an algorithm as a posterior distribution over a class of hypotheses, and provides an upper bound on the discrepancy between a model's empirical risk and its population risk.

PAC-Bayes considers the following concepts: a hypothesis class \(\), a training set \(S=\{_{1},,_{n}\}\) iid sampled from an unknown distribution \(\) over an instance space \(^{1}\), and a real-valued loss function \(:[0,)\). Moreover, the primary goal of PAC-Bayes is to provide generalization bounds uniformly valid for any posterior \(q^{1}_{+}()\). These bounds are dependent on the empirical performance of \(q\) and its closeness to a chosen _prior distribution_\(p^{1}_{+}()\), as measured by the KL-divergence. The empirical and true risks of a posterior distribution \(q^{1}_{+}()\) are defined as

\[}_{S}(q)=*{}_{h q(h)}[_{i=1}^{n}(h,_{i})] (q)=*{}_{h q(h)}[*{ }_{}(h,)].\]

As an illustration, consider the following PAC-Bayesian bound for bounded loss functions developed by Catoni (2003).

**Theorem 2.1**.: _Given a probability measure \(\) on \(\), a hypothesis class \(\), a prior distribution \(p\) on \(\), a loss function \(:\), real numbers \((0,1)\) and \(>0\), with probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q^{1}_{+}()\):_

\[(q)}_{S}(q)++(q\,||\,p)+}{}.\]

The connection between PAC-Bayesian theory and Bayesian inference was highlighted by Grunwald (2012) and Germain et al. (2016), who showed that with a proper choice of \(\) and the negative log-likelihood as the loss function \(\), the optimal posterior minimizing the right-hand side of Catoni's bound is the Bayesian posterior. Note that although the Bayesian posterior is unique (for a given prior and likelihood), a "PAC-Bayesian posterior" could be, in principle, any distribution over \(\).

In our PAC-Bayesian analysis of VAEs, we will use the latent space \(\) as our hypothesis class, so that the VAE's prior will coincide with the PAC-Bayesian prior and the variational posterior \(q_{}(|)\) will stand for our PAC-Bayesian posterior. An immediate concern with this approach is that the encoder's distributions are conditioned on individual samples \(\), whereas the usual PAC-Bayesian bounds hold for unconditional posteriors \(q(h)\). We address this issue in the next section, by developing a novel PAC-Bayesian bound for posterior distributions \(q(\,|)\). This general result will be later utilized to analyze VAEs.

## 3 A General PAC-Bayesian Bound with a Conditional Posterior

In this section, we present our general PAC-Bayesian bound with a conditional posterior distribution. Note that the novelty of this result is not the conditioning on observations, since this can be achieved by exploiting the existing PAC-Bayesian bounds. Indeed, Haddouche and Guedj (2022) utilized the general theorem of Rivasplata et al. (2020) to derive bounds for the online learning framework.

[MISSING_PAGE_FAIL:5]

Applied to supervised learning, Theorem 3.1 bounds the expected risk of a Gibbs posterior \(q\) which, upon receiving a previously unseen datapoint \(\), samples a predictor \(h\)_dependent_ on \(\), and uses it to make a prediction. Note that the family \(\) from Assumption 1 does not appear in the bound, which has nice consequences in practice. Indeed one may pick a loss function \(\) that fits the problem, and then find a family \(\) for which the continuity assumption is satisfied with constant \(K\) that is as small as possible.

Note also that, in the tradition of PAC-Bayesian bounds, Theorem 3.1 does not make any assumptions on the nature of the elements of \(\) (e.g. \(\) could be a class of functions, a set of neural network's parameters, etc). Therefore, the theorem is very general and could be applied to different domains and models. In the following sections, we will use a specific kind of hypothesis class \(=\), in order to capture the VAE's latent space.

## 4 Generalization bounds for the Reconstruction Loss

For the remainder of this work, \(\|\|\) denotes the \(L_{2}\) norm, and we assume the instance space \(\) is Euclidean, and the latent space \(=^{d_{}}\), where \(d_{}>0\). Both \(\) and \(\) are equipped with the Euclidean distance as the underlying metric. Therefore, if \(,^{}\), \(d(,^{})=\|-^{}\|\).

The following assumption states that the encoder and decoder networks have finite Lipschitz norms.

**Assumption 2**.: The encoder and decoder are Lipschitz-continuous w.r.t. their inputs, meaning there exist real numbers \(K_{},K_{}>0\) such that for any \(_{1},_{2}\) and \(_{1},_{2}\),

\[\|Q_{}(_{1})-Q_{}(_{2}) \| K_{}\|_{1}-_{2}\|\] (4)

and

\[\|g_{}(_{1})-g_{}(_{2})\| K_{ }\|_{1}-_{2}\|.\] (5)

Recall the definition of \(Q_{}\) from Equation (2). Note that in practice, one can estimate the Lipschitz constant of trained networks (Fazlyab et al., 2019; Latorre et al., 2020) or train the VAE with preset Lipschitz constants (Barrett et al., 2022).

Moreover, we define the reconstruction loss \(^{}_{}\) with the \(L_{2}\) norm, instead of the squared \(L_{2}\) norm, which enables us to exploit the properties of a metric. We discuss this choice in Section 6. In order to be consistent with the PAC-Bayesian framework, we define the loss function as follows: \(^{}_{}:[0,)\),

\[^{}_{}(,)=\|-g_{ }()\|.\] (6)

Our goal is to apply the general bound of Theorem 3.1 to the VAE model. But first, since Theorem 3.1 requires Assumption 1 to be satisfied, we start by showing that if the encoder and decoder networks have finite Lipschitz norms, then Assumption 1 holds.

**Proposition 4.1**.: _Consider a VAE with parameters \(\) and \(\) and let \(K_{},K_{}\) be the Lipschitz norms of the encoder and decoder respectively. Then the variational distribution \(q_{}(|)\) satisfies Assumption 1, with \(=\{f:\;s.t.\|f\|_{} K_{}\}\), \(=^{}_{}\), and \(K=K_{}K_{}\)._

Proof idea.: The proof of Proposition 4.1 is in Appendix C, we provide a brief summary here. To prove the first part of Assumption 1, we first notice that if \(\) is the set of real-valued \(K_{}\)-Lipschitz continuous functions, then \(d_{}\) is a scaling of the Wasserstein distance. In addition, since \(W_{1} W_{2}\), using the closed form of the Wasserstein-2 distance between Gaussian distributions, one can show that \(d_{}(q_{}(|_{1}),q_{}(|_{2} )) K_{}K_{}\|_{1}-_{2}\|\). Finally, the second part of the assumption is a consequence of the definition of the loss function and the Lipschitz continuity of the decoder. 

Proposition 4.1 tells us that Assumption 1 holds for VAEs. Consequently, we can utilize our general bound of Theorem 3.1 to obtain generalization guarantees. This leads to the following general PAC-Bayesian bound for the VAE's reconstruction loss.

**Theorem 4.2**.: _Let \(\) be the instance space, \(^{1}_{+}()\) the data-generating distribution, \(\) the latent space, \(p()^{1}_{+}()\) the prior distribution on the latent space, \(\) the decoder's parameters,\((0,1),>0\) be real numbers. With probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\):_

\[}_{}}_{q_{ }(|)}_{}^{}(,) _{i=1}^{n}}_{q_{}(|_{i})}_{}^{}(,_{i})+[_{i=1}^{n}(q_{}(|_{i })\,||\,p())+.\] \[.K_{}}{n}_{i=1}^{n}}_{}d(,_{i})++n}_{()} }_{}e^{(}_{^{}}_{}^{}(,^{})-_{}^{}(,) )}],\]

_where \(K_{}\) and \(K_{}\) are the Lipschitz norms of the encoder and the decoder (see (4) and (5)) and \(}_{q_{}(|)}\) is a shorthand for \(}_{ q_{}(|)}\)._

Note that the choice of the hyperparameter \(\) in the VAE's optimization objective (3) correlates with the choice of the hyperparameter \(\) in Theorem 4.2 (e.g. \(=n\) corresponds to \(=1\)). Note also that the encoder and decoder are not treated the same way in Theorem 4.2. Indeed, the inequality holds for a given decoder, but uniformly for any encoder. We discuss this subtle difference and its practical consequences in Section 6.

Theorem 4.2 can be seen as a general framework. In order to obtain a useful upper bound, one needs to bound the average distance and the exponential moment on the right-hand side. In the sections below, we provide upper bounds for these terms under various assumptions on the instance space.

### Reconstruction Guarantees for Bounded Instance Spaces

In the following theorem, we provide a special case of Theorem 4.2 when the instance space's diameter \(}}{{=}}_{,^ {}}d(,^{})\) is finite (see Section C.2 for the proof).

**Theorem 4.3**.: _Let \(\) be the instance space, \(<\) its diameter, \(^{1}_{+}()\) the data-generating distribution, \(\) the latent space, \(p()^{1}_{+}()\) the prior on the latent space, \(\) the decoder's parameters, \((0,1),>0\) be real numbers. With probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\):_

\[}_{}}_{q_{ }(|)}_{}^{}(,) _{i=1}^{n}\{}_{q_{}(| _{i})}_{}^{}(,_{i})\} +(_{i=1}^{n}(q_{}( |_{i})\,||\,p())+.\] \[. K_{}K_{}+. +^{2}}{8n})..\]

The left-hand side of this inequality is the expected reconstruction loss for samples \(\), while the right-hand side is the empirical reconstruction and KL losses, plus an additional term depending on the Lipschitz constants of the VAE and the model's diameter.

Note that for real-life datasets, the diameter of the instance space might be very large and non-representative of the structure and complexity of the data. Indeed, it is common to scale image datasets in order to utilize a specific architecture (Radford et al., 2016). In the following section, we provide a special case of Theorem 4.2 under the manifold hypothesis on the data-generating process.

### Reconstruction Guarantees under the Manifold Assumption

The manifold assumption (Fodor, 2002; Narayanan and Mitter, 2010; Fefferman et al., 2016) states that most high-dimensional datasets encountered in practice lie close to low-dimensional manifolds. This assumption is exploited by latent variable generative models such as GANs and VAEs, which approximate high-dimensional datasets using transformations of distributions on a low-dimensional space. The works of Schreuder et al. (2021) and Mbacke et al. (2023) provide generalization bounds for GANs, by assuming that the data-generating distribution is a smooth transformation of the uniform distribution on \(^{d^{*}}\), where \(d^{*}\) is the intrinsic dimension. However, since the standard VAE calls for a standard Gaussian prior, in the following theorem, we assume \(\) is a smooth transformation of the standard Gaussian distribution \(p^{*}\) on \(^{d^{*}}\). We consider the case when \(p^{*}\) is the uniform distribution on \(^{d^{*}}\) in the supplementary material.

**Theorem 4.4**.: _Let \(\) be the instance space, \(^{1}_{+}()\) the data-generating distribution, \(\) the latent space, \(p()^{1}_{+}()\) the prior distribution on the latent space, \(\) the decoder's parameters, \((0,1),>0,a>0\) real numbers. Assume the data-generating distribution \(=g^{*} p^{*}\), where \(p^{*}\) is the standard Gaussian distribution on \(^{d^{*}}\) and \(g^{*}_{K_{*}}(^{d^{*}},)\). With probability at least \(1--}{2}e^{-a^{2}/2}\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\):_

\[*{}_{}*{ }_{q_{}(|)}_{}^{}( ,)_{i=1}^{n}\{*{ }_{q_{}(|_{i})}_{}^{}( ,_{i})\}+(_{i=1}^{n} (q_{}(|_{i})\,||\,p())+.\] \[. K_{}K_{}K_{*})d^{*}}+. +K_{*}^{2}}{2n})..\]

Let us clarify the role of the new parameter \(a>0\). Each training sample \(_{i} S\) can be expressed as \(_{i}=g^{*}(_{i})\), where \(_{i} p^{*}\). Since \(p^{*}\) is the standard Gaussian distribution on \(^{d^{*}}\), all samples \(_{i}\) will be inside a hypercube \([-a,a]^{d^{*}}\), with high probability. This uncertainty is reflected in the lowered confidence (from \(1-\) in Theorem 4.2 to \(1--}{2}e^{-a^{2}/2}\) in Theorem 4.4), and can be controlled by choosing a large enough value of \(a\). The proof of Theorem 4.4 is in the supplementary material (Section C.3), we provide a short summary below.

Proof idea.: The proof starts with Theorem 4.2, and uses the assumptions of Theorem 4.4 to obtain upper bounds on the exponential moment and the average distance. To derive the upper bound on the exponential moment, we observe that the function \(_{}^{}(,)\) is \(K_{*}\)-Lipschitz continuous, then we use a dimension-free upper bound on the MGF of Lipschitz-continuous functions of Gaussian random variables. Furthermore, we obtain the upper bound on the average distance \(_{i=1}^{n}*{}_{}\| -_{i}\|\), by using Holder's inequality and the expectation of a non-central \(^{2}\) distribution. Then, we upper-bound the probability that \(_{i}[-a,a]^{d^{*}}\) for all \(1 i n\) using the error function and Bernoulli's inequality. Finally, we use the union bound to update the overall confidence. 

## 5 Generalization Bounds for Regeneration and Generation

Let \(_{,}\) be the _empirical regenerated distribution_, meaning

\[_{,}=_{i=1}^{n}g_{} q_{}( |_{i}).\] (7)

In other words, sampling \(_{,}\) is done by sampling \( q_{}(|_{i})\) where \(i\) is uniformly sampled from \(\{1,,n\}\), then passing \(\) through the decoder: \(=g_{}()\). It is therefore the distribution regenerated by the VAE, given the training set \(S=\{_{1},,_{n}\}\) as input.

In this section, we provide statistical guarantees on the regenerative and generative properties of VAEs. More precisely, we derive upper bounds for the quantities \(W_{1}(,_{,})\) and \(W_{1}(,g_{} p())\). Note that the average distance term does not appear in the bounds of this section. This is because instead of relying on Theorem 3.1, the results of this section depend upon a preliminary lemma (Lemma B.1), which does not necessitate Assumption 1.

### Regeneration and Generation Guarantees for Bounded Instance Spaces

The following theorem presents our first upper bound on the distance between the input distribution and the empirical regenerated distribution.

**Theorem 5.1**.: _Under the definitions and assumptions of Theorem 4.3, we have that with probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\):_

\[W_{1}(,_{,})_{i=1}^{n}\{ *{}_{q_{}(|_{i})}_{}^{}(,_{i})\}+( _{i=1}^{n}(q_{}(|_{i})\,||\,p( ))++^{2}}{8n}).\]

As we can see, the right-hand side of Theorem 5.1 depends on the empirical reconstruction loss and KL-divergence. This guarantees that as the VAE's empirical risk decreases, the regenerated distribution gets closer to the data-generating distribution. The proof of Theorem 5.1 exploits the factthat the underlying metric on \(\) is the Euclidean distance \(d(,^{})=-^{}\), which is also used to define the reconstruction loss \(^{}_{}\) (see Equation 6). The full proof can be found in Appendix D.

The following theorem provides an upper bound of the distance between the input distribution and the VAE's generated distribution.

**Theorem 5.2**.: _Under the definitions and assumptions of Theorem 4.3, we have that with probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\):_

\[W_{1}(,g_{} p())_{i= 1}^{n}}_{q_{}(|_{i})}^ {}_{}(,_{i})}+ (_{i=1}^{n}(q_{}(|_{i}) |\,|p())+.\] \[.+^{2}}{8n} )+}{n}_{i=1}^{n}( _{i})^{2}+_{}(_{i})-^{2}},\]

_where \(^{d_{Z}}\) denotes the vector whose entries are all \(1\)._

The right-hand side of Theorem 5.2 is equal to the right-hand side of Theorem 5.1, plus an additional term depending on the Wasserstein-2 distance \(W_{2}(q_{}(|_{i}),p())\), which is used in the proof because of its closed form for Gaussian distributions. Hence, the right-hand side of Theorem 5.2 augments the VAE's optimization objective with \(W_{2}(q_{}(|_{i}),p())\), suggesting that a good generative performance may require the latent codes to be even closer to the prior. This is consistent with the findings of Zhao et al. (2019), who showed that in order to improve generative performance, the latent codes need to be much closer to the prior, which may disrupt the balance between reconstruction loss and KL-loss.

### Regeneration and Generation Guarantees under the Manifold Assumption

Similar to what we did in Section 4.2, we assume that the data-generating distribution is a smooth transformation of the standard Gaussian distribution on \(^{d^{*}}\), where \(d^{*}\) is the intrinsic dimension of the dataset. This yields the following results.

**Theorem 5.3**.: _Under the definitions and assumptions of Theorem 4.4, with probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\):_

\[W_{1}(,_{,})_{i=1}^{n} \{}_{q_{}(|_{i})}^{ }_{}(,_{i})\}+ (_{i=1}^{n}(q_{}(|_{i}) |\,|p())++K_{*}^{2}} {2n}).\]

Note that the intrinsic and extrinsic dimensions do not explicitly appear in this inequality, although they may affect the reconstruction and KL loss.

We now present our last result, an upper bound on the Wasserstein distance between the input distribution and the VAE's generated distribution, under the manifold assumption.

**Theorem 5.4**.: _Under the definitions and assumptions of Theorem 4.4, with probability at least \(1-\) over the random draw of \(S^{ n}\), the following holds for any posterior \(q_{}(|)\) :_

\[W_{1}(,g_{} p())_{i= 1}^{n}\{}_{q_{}(|_{i})}^ {}_{}(,_{i})\}+ (_{i=1}^{n}(q_{}(|_{i}) |\,|p())+..\] \[..+K_{*}^{2}}{2n} )+}{n}_{i=1}^{n}( _{i})^{2}+_{}(_{i})-^{2}},\]

_where \(^{d_{Z}}\) denotes the vector whose entries are all \(1\)._

Theorem 5.2 and Theorem 5.4 show that by minimizing the VAE's objective, one is also minimizing the Wasserstein distance between the input distribution and the VAE's generated distribution.

From the upper bounds given by Theorems 5.1 and 5.3, one can deduce rates of convergence of \(O(n^{-1/2})\) (when \(\)) for the empirical regenerated distribution. Note that \( n\) leads to the much faster rate of \(n^{-1}\), but then the bounds do not converge to the empirical risk, but to a larger positive number, dependent on the input distribution. Similarly, Theorems 5.2 and 5.4 provide rates of convergence of \(O(n^{-1/2})\) for the VAE's generated distribution.

Discussion and Conclusion

The different treatments of \(\) and \(\).The bounds we've presented in this work hold for a given decoder \(\), but uniformly for all encoders. In practice, this means that the risk certificate has to be computed using samples different from the ones used to train the VAE. This is different from the usual PAC-Bayesian trick (Germain et al., 2009; Parrado-Hernandez et al., 2012; Perez-Ortiz et al., 2021, see also Remark F.2) of splitting the training set to learn the prior, then training the model on the whole training set, because the decoder and encoder are jointly optimized. Instead, one has to make sure that the model is only trained on samples distinct from the ones used to compute the bound. The same method would be necessary when computing the risk certificates given by the recent PAC-Bayesian bounds of Rivasplata et al. (2020) and Haddouche and Guedj (2022), since those bounds are not uniformly valid for any posterior.

The reconstruction loss.In our bounds, the reconstruction loss is the \(L_{2}\) norm (RMSE), instead of the squared \(L_{2}\) norm (MSE). In practice, one can still optimize a VAE with the MSE (or any other reconstruction loss, e.g. the cross entropy loss), and then compute the bounds using the RMSE. However, if the reconstruction loss is not the RMSE, then the optima of the chosen optimization objective might differ from the ones minimizing the right-hand side of the bounds. Therefore, if the goal is to minimize the bounds, one should utilize the RMSE as the reconstruction loss.

Conclusion.It is common, when applying PAC-Bayesian theory to new problems, to add additional stochasticity in order to account for the PAC-Bayesian distributions on the hypothesis class. For instance, Mbacke et al. (2023) added distributions on the parameters of a WGAN's generator, in order to perform a PAC-Bayesian analysis. However, because of the seamless integration of the PAC-Bayesian and VAE frameworks, such modification to the original problem has been avoided in this work. We matched the prior and posterior distributions on the VAE's latent space to the PAC-Bayesian prior and posterior, which allowed us to recover the VAE's optimization objective. We provide preliminary experiments on synthetic datasets in the supplementary material.

This work is a humble contribution to the theoretical understanding of VAEs. We developed novel PAC-Bayesian bounds suited to the analysis of VAEs and provided generalizations bounds for the VAE's reconstruction loss. In addition, we also derived upper bounds on the Wasserstein distance between the input distribution and the VAE's generative model's distribution. These bounds depend on the VAE's empirical optimization objective and the data-generating process. By integrating the VAE and PAC-Bayesian frameworks, we hope to establish PAC-Bayesian theory as a prime tool for the theoretical analysis of VAEs.