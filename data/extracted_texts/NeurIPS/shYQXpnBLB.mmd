# Association of Objects May Engender Stereotypes:

Mitigating Association-Engendered Stereotypes in Text-to-Image Generation

 Junlei Zhou\({}^{1}\), Jiashi Gao\({}^{1}\), Xiangyu Zhao\({}^{2}\), Xin Yao\({}^{3}\), Xuetao Wei\({}^{1}\)

\({}^{1}\)Southern University of Science and Technology, \({}^{2}\)City University of Hong Kong

\({}^{3}\) Lingnan University

{zhouj12023,12131101}@mail.sustech.edu.cn, xy.zhao@cityu.edu.hk

xinyao@ln.edu.hk, weixt@sustech.edu.cn

Corresponding author.

###### Abstract

Text-to-Image (T2I) has witnessed significant advancements, demonstrating superior performance for various generative tasks. However, the presence of stereotypes in T2I introduces harmful biases that require urgent attention as the T2I technology becomes more prominent. Previous work for stereotype mitigation mainly concentrated on mitigating stereotypes engendered with individual objects within images, which failed to address stereotypes engendered by the association of multiple objects, referred to as _Association-Engendered Stereotypes_. For example, mentioning "black people" and "houses" separately in prompts may not exhibit stereotypes. Nevertheless, when these two objects are associated in prompts, the association of "black people" with "poorer houses" becomes more pronounced. To tackle this issue, we propose a novel framework, MAS, to Mitigate Association-engendered Stereotypes. This framework models the stereotype problem as a probability distribution alignment problem, aiming to align the stereotype probability distribution of the generated image with the stereotype-free distribution. The MAS framework primarily consists of the _Prompt-Image-Stereotype CLIP_ (_PIS CLIP_) and _Sensitive Transformer_. The _PIS CLIP_ learns the association between prompts, images, and stereotypes, which can establish the mapping of prompts to stereotypes. The _Sensitive Transformer_ produces the sensitive constraints, which guide the stereotyped image distribution to align with the stereotype-free probability distribution. Moreover, recognizing that existing metrics are insufficient for accurately evaluating association-engendered stereotypes, we propose a novel metric, _Stereotype-Distribution-Total-Variation_ (_SDTV_), to evaluate stereotypes in T2I. Comprehensive experiments demonstrate that our framework effectively mitigates association-engendered stereotypes.

## 1 Introduction

Text-to-Image (T2I) (Nichol et al. (2021); Rombach et al. (2022); Ramesh et al. (2022); Saharia et al. (2022)), based on the diffusion model, has achieved significant breakthroughs in image generation, showing potential for various downstream tasks such as image creation, editing (Brooks et al. (2023)), etc. In addition, the scale of T2I applications has expanded impressively. For example, the open-source stable diffusion project (Rombach et al. (2022)) has gained the favor of more than \(10\) million users. As of August \(2023\), the number of images created through T2I has surpassed \(15\) billion+.

As images generated by diffusion models become more prevalent, mitigating the stereotypes they engender is increasingly critical (Seshadri et al. (2023); Luccioni et al. (2024)).

The stereotype in social psychology is a mental construct of a particular type of person, object, or thing, often based on observing individuals who fit that type. Previous work explored sensitive attributes (such as gender, race, region, religion, and age) and corresponding stereotypes present in T2I diffusion models (Wang et al. (2023); Esposito et al. (2023); Bianchi et al. (2023)). For instance, when the prompt is _"a photo of a politician"_, the object ( politician) in the image generated by the T2I diffusion model always appears _male_ in the sensitive attribute of gender. Previous works (Shen et al. (2024); Kim et al. (2023); Esposito et al. (2023)) on stereotype mitigation have been limited to a single object, referred to as _Non-Association-Engendered stereotypes_ (e.g., only mitigating the stereotype problem in the occupation or gender dimension), and cannot effectively address stereotypes involving the association of multiple objects, referred to as _Association-Engendered Stereotypes_. For example, as shown in Figure 1, when the prompt is _"a photo of a white people and a house"_ versus _"a photo of a black people and a house"_, T2I diffusion models often generate images where the houses associated with white people appear better than those associated with black people, reflecting an association-engendered stereotype. Therefore, mitigating such association-engendered stereotypes is highly desired for T2I models.

In this paper, our contributions are as follows: \(\) To the best of our knowledge, we take the first step towards addressing association-engendered stereotypes in T2I models. We define the stereotype problem in the T2I diffusion model as a probability distribution alignment problem, aiming to align the stereotype probability distribution of the generated image with the stereotype-free distribution. \(\) We present a novel framework \(\) to mitigate the association-engendered stereotypes, which primarily consists of the _Prompt-Image-Stereotype CLIP_ (_PIS CLIP_) and _Sensitive Transformer_. The _PIS CLIP_ learns the association between prompts, images, and stereotypes, which can construct the mapping of prompts to stereotypes. The _Sensitive Transformer_ produces the sensitive constraints, which guide the stereotyped image distribution to align with the stereotype-free probability distribution. \(\) Given the insufficiency of existing metrics for accurately evaluating association-engendered stereotypes, we propose a novel metric for evaluating stereotypes in T2I: _Stereotype-Distribution-Total-Variation_ (_SDTV_). \(\) We conduct a comparative evaluation of our stereotype mitigation approach on five popular T2I diffusion models and six advanced stereotype mitigation approaches in T2I. Extensive experiments demonstrate the superiority of our approach in effectively mitigating both non-association-engendered and association-engendered stereotypes.

Figure 1: Original T2I models can generate stereotypes when given prompts involving multiple objects; when the prompts are “_a photo of black/white people_” or “_a photo of a house_”, they do not engender stereotypes. However, if the prompt is “_a photo of black/white people and a house_”, the model may engender stereotypes that the _white people’s_ house is better than the _black people’s_ house. It is noted that previous mitigation approaches have yet to mitigate these association-engendered stereotypes. Our approach, \(\), demonstrates the capability to effectively mitigate these stereotypes.

## 2 Related work

**Stereotypes in T2I models.** Although the current image quality and capabilities generated by T2I are gradually approaching maturity, it is worth noting that various biases and stereotypes may still arise from neutral prompt words (Zhou et al. (2024)). Fraser et al. (2023) and Wan and Chang (2024) found that text-generated image models often engender stereotypes of gender, race, and demographics in sociology. These biases can manifest in multiple aspects, such as occupations (Bianchi et al. (2023); Cho et al. (2023); Luccioni et al. (2023)), objects (Mannering (2023)) and adjectives (Luccioni et al. (2023); Naik and Nushi (2023)). Additionally, T2I diffusion models may exhibit gender bias when depicting interactions between two or more people. Based on the above findings, previous researchers proposed stereotype detection frameworks such as Language Agent (Wang et al. (2023)) and T2IAT detection (Wang et al. (2023)). These research results provide practical tools for identifying stereotypes and lay a solid foundation for further mitigating stereotypes.

**Stereotypes mitigation in T2I models.** Existing approaches to mitigate stereotypes in T2I diffusion models have effectively mitigated some of the stereotypes but still have some limitations. Friedrich et al. (2023) randomly introduced additional text clues by identifying known occupations in the prompts to ensure a fair distribution of the generated images. However, this approach may ineffectively address biases related to occupations that are not predefined. Kim et al. (2023) developed a desereotyping loss function and adjusted specific parameters of the soft prompts to balance the sensitive attributes of the generated images but only achieved de-stereotyping in the T2I model containing occupational prompts. Chuang et al. (2023) proposed a technique that involves projecting biased directions in text embeddings and debiasing by utilizing text data with a calibrated projection matrix. Fine-tuning the diffusion model by modifying the capabilities of the pre-trained model has also proven to be an effective strategy. Inspired by Gal et al. (2023); Zhang et al. (2023); Brooks et al. (2023); Dai et al. (2023); Shen et al. (2024) fine-tuned the sampling process of the diffusion model and used an optimized loss function to align the T2I model with fairness principles directly. Moreover, He et al. (2024) utilized an iterative distribution alignment approach to align biased distributions with a unified distribution. Schramowski et al. (2023) built a Safe Latent Diffusion (SLD) capable of suppressing sensitive image portions during diffusion. Previous researchers adjusted the original T2I model and had to consider the image quality and mitigate stereotypes in the original T2I model. They mainly focused on mitigating non-association-engendered stereotypes in the T2I model (such as those engendered by occupation/gender), so it is unable to mitigate the stereotypes generated when multiple objects are associated effectively.

## 3 Our framework: mitigate association-engendered stereotypes (\(\))

In this section, we construct a mathematical model of stereotypes in T2I. Based on this model, we propose a novel metric, _Stereotype Distribution Total Variation_ (_SDTV_), for evaluating stereotypes. Then, we model the stereotype mitigation problem as a probability distribution alignment problem and build a new stereotype mitigation framework, Mitigate Association-engendered Stereotypes (\(\)). Finally, we train _PIS CLIP_ and _Sensitive Transformer_ models to construct sensitive constraints, which guide the stereotype probability distribution to align with the stereotype-free probability distribution.

### Modeling stereotypes in T2I

Inspired by Shen et al. (2024), we denote the object of the image as \(x\) and the object's sensitive attributes as \(s\) (such as _gender, race, and adjective_). We introduce the probability distribution function (PDF) \(P(s=v(s)|x)\) to quantify the likelihood of the image's object exhibiting specific sensitive attributes, where \(v(s)\) represents the value of the sensitive attribute. For instance, when

  
**Abbreviation** & **Stereotype Categories** & **Prompt** \\  S-O \& S-SA & Single Object with a Single Sensitive Attribute & A photo of [OBJECT]. \\ S-O \& M-SA & Single Object with Multiple Sensitive Attributes & A photo of [SA] [OBJECT]. \\ M-O \& S-SA & Multiple Objects with a Single Sensitive Attribute & A photo of [OBJECT] and [OBJECT]. \\ M-O \& M-SA & Multiple Objects with Multiple Sensitive Attributes & A photo of [OBJECT] and [OBJECT] \\   

Table 1: Stereotype categories and prompt word templatesthe generated image's object \(x\) is _person_ and \(s\) is _gender_, \(v(s)\) could be _male_ or _female_. The PDF represents the probability of a person appearing as male or female in the sensitive attribute of gender. Furthermore, to distinguish the difference between association-engendered stereotypes and stereotypes explored by previous work (Kim et al. (2023); He et al. (2024); Shen et al. (2024)), we categorize stereotypes into four distinct categories (Table 1) based on the association between objects and sensitive attributes: a single object with a single sensitive attribute (S-O \(\&\) S-SA, non-association-engendered stereotypes), a single object with multiple sensitive attributes (S-O \(\&\) M-SA, non-association-engendered stereotypes), multiple objects with a single sensitive attribute (M-O \(\&\) S-SA, association-engendered stereotypes), multiple objects with multiple sensitive attributes (M-O \(\&\) M-SA, association-engendered stereotypes). Here, we utilize a joint PDF to define categories with multiple objects with multiple sensitive attributes.

\[f(x_{1},x_{2},)=Ps_{x_{1}}=v(s_{x_{1}}),s_{x_{2}}=v(s_{x_{2}}), |x_{1},x_{2},,\] (1)

where \(x_{1}\) and \(x_{2}\) represent the two objects in the image. For instance, in Figure 1, \(x_{1}\) corresponds to a _people_, and \(x_{2}\) corresponds to a _house_. The variables \(s_{x_{1}}\) and \(s_{x_{2}}\) denote the _race_ and _house's description_. See Appendix A for more PDF definitions of other stereotype categories.

### A new metric to evaluate stereotypes: _Sdstv_

Stereotypes engender when a sensitive attribute in an image consistently presents one or a few states in most instances (Naik and Nushi (2023)). Therefore, Saravanan et al. (2023) and Wang et al. (2023) have utilized various metrics to evaluate the stereotype from the perspective of the overall proportion of different subgroups. However, when evaluating association-engendered stereotypes, the proportion of subgroups in the population cannot accurately evaluate association-engendered stereotypes. To address this shortcoming, we propose a new stereotype evaluation metric using total variation distance based on the probability distribution model of stereotypes in T2I.

Consider a T2I model, denoted as \(\), which generates an image, and \(x\) is an object of the image. We define a set \(S_{x}\) denotes that object \(x\) contains multiple sensitive attributes such that \(S_{x}=\{\}\). We assign the sensitive attributes as \(v(s_{x})^{},v(s_{x})^{2},,v(s_{x})^{}\) for each sensitive attribute \(s_{x},\,\,s_{x} S_{x}\) (e.g. \(v(s_{x})=,v(s_{x})=,\)). For an object (i.e., \(x=\)"_doctor_" or \(x=\)"_nurse_"), the stereotype-free T2I model should generate images with equal probability across different sensitive attribute values and should avoid significant probability

Figure 2: Overview of our proposed MAS. (1) In the PIS model pre-training stage, we adjust the traditional CLIP structure from the original “text-image” two-dimensional mapping to a “text-image-text” three-dimensional mapping, thereby obtaining a mapping of prompts to stereotypes. (2) In constructing sensitive constraints, we propose a _Sensitive Transformer_ based on the transformer structure to construct sensitive constraints for each prompt. (3) In the stage of adding sensitive constraints to the T2I, we embed the sensitive constraints into the T2I diffusion model to guide the generation of stereotype-free images.

distribution disparities caused by the dependence of sensitive attributes on the object generating. To evaluate the extent of stereotypes, we calculate the difference in probability distributions for each sensitive attribute value using the Total Variation distance Chung et al. (1989): \(D_{TV}(p_{}(s_{x}=v(s_{x})^{i}|x),p_{}(s_{x}=v(s_{x})^{j}|x))\). Therefore, based on the PDF definition of stereotype types in Section 3.1, the _SDTV_ value of model \(\) under single-object \(x\) with single-sensitive attribute \(v(s_{x})\) is:

\[SDTV()=_{\{i,j\}}D_{TV}p_ {}s_{x}=v(s_{x})^{i}|x,p_{}s_{x}=v(s_{x})^{j} |x}.\] (2)

In practice, for a given prompt text \(\) in T2I models, the sensitive attribute's probability of the generated images under this prompt \(\) is represented as \(p_{}(s_{x}=v(s_{x})^{i}|x,)\). For instance, consider the sensitive attribute of gender (\(s_{x}=\)"_gender_") when the input prompt \(\) is set to "_a rich person_", the gender-sensitive attributes' probability of the corresponding generated images are considered and set to \(p_{}(s_{x}=v(s_{x})^{i})|x,)\) (where \(v(s_{x})^{i}\) represents "_male_") and \(p_{}(s_{x}=v(s_{x})^{j})|x,)\) (where \(v(s_{x})^{j}\) represents "_female_") respectively. We could incorporate the prompt and sensitive attribute and obtain the following:

\[SDTV()=_{\{i,j\}}p_{ }s_{x}=v(s_{x})^{i}|x,-p_{}s_{x}=v(s_{x}) ^{j}|x,,\] (3)

where \(\) is the parameter of model \(\). Given an input prompt \(\), \(p_{}(s_{x}=v(s_{x})^{i}|x,)\), and \(p_{}(s_{x}=v(s_{x})^{j}|x,)\) represent the probabilities that the sensitive attribute appears as \(v(s_{x})^{i}\) and \(v(s_{x})^{j}\) in the output, where \(\{i,j\}\) and \(i j\). If the value of \(SDTV()\) is considerable, it suggests that the image exhibits very severe stereotypes. Conversely, suppose its value is close to 0. In that case, it implies that, under the given prompt conditions, the sensitive attributes displayed in the generated image are relatively evenly distributed, indicating no apparent stereotypes in this sensitive attribute dimension. When calculating the _SDTV_ of association-engendered stereotypes, we deal with multiple objects and sensitive attributes simultaneously, so we need to adjust Equation (3). Let's define a set of objects \(X=\{X_{h},X^{}_{h}\}\), where \(X_{h}\) and \(X^{}_{h}\) represent the sets of human and non-human objects, respectively. For a human object \(x_{h} X_{h}\) with sensitive attributes \(S_{h}\), and a non-human object \(x^{}_{h} X^{}_{h}\) with descriptions \(S^{}_{h}\), we can adjust Equation (3) accordingly to obtain:

\[SDTV()=}_{s_{h} S_{h}\\ s_{h} S^{}_{h}}(_{\{i,j\}}p_{}(s_{h}=v(s_{h})^{i}|s^{}_{h}=v(s^{}_{h})^{m}, )-p_{}(s_{h}=v(s_{h})^{j}|s^{}_{h}=v(s^{}_{h})^{m}, )),\] (4)

The sensitive attribute \(s_{h}\) and describe \(s^{}_{h}\) have \(\) and \(\) corresponding values, respectively. See Appendix B for the detailed proof process of the _SDTV_.

### Mitigating association-engendered stereotypes

Our framework aims to alter the stereotype probability distribution in T2I to align it with a stereotype-free probability distribution. As shown in Figure 2, we construct MAS to implement this alignment process, which consists of two main network structures, _PIS CLIP_ and _Sensitive Transformer_. _PIS CLIP_ learns prompts, images, and stereotypes in T2I and constructs mapping of prompts to stereotypes; _Sensitive Transformer_ builds sensitive constraints based on prompts and mapping of prompts to stereotypes. Finally, we embed sensitive constraints into the T2I model by utilizing the _Sensitive Transformer_, which guides the probability distribution associated with stereotypes toward alignment with the stereotype-free distribution.

```
1:Input: Labels \(\)\(L\): Image encoding\( I[n,h,w,c]\): Stereotype texts encoding \( S[n,l]\): Prompt texts encoding \( P[n,l]\).
2:\(L_{c}_{2}\)-norm(\(I W\)) // \(W_{l}\)- learned proj of image to embed
3:\(P_{c}_{2}\)-norm(\(P W\)) // \(W_{l}\)- learned proj of text to embed
4:\(S_{c}\)-norm(\(S W_{l}\))
5:\(I_{loss}\) = CrossEntropyLoss(exp(\(I_{L} P_{L}^{}\)), \(L\))
6:\(I_{loss}\) = CrossEntropyLoss(exp(\(I_{L} P_{L}^{}\)), \(L\))
7:\(S_{loss}\) = CrossEntropyLoss(exp(\(I_{L} P_{L}^{}\)), \(L\))
8:\(S_{loss}\) = CrossEntropyLoss(exp(\(I_{L} P_{L}^{}\)), \(L\))
9:Return\(I_{c}\), \(P_{c}\), \(S_{c}\) ```

**Algorithm 1** Prompt-Image-Sterotype CLIP Algorithmprompts, images, and stereotype descriptions. Inspired by CLIP (Radford et al. (2021)), we construct a novel three-dimensional mapping approach to learn the features and relationships of prompt-image stereotypes. We use a T2I-generated image, which contains a stereotype, as a bridge to create three pairs: <prompt, image>, <image, stereotype>, and <prompt, stereotype> for training. _PIS CLIP_ learns the potential mapping relationship between the prompt and the stereotype by maximizing the cosine similarity of these three pairs. We optimize a symmetric cross-entropy loss based on these similarity scores. Algorithm 1 presents the process of the _PIS CLIP_ (See Appendix C.1 for more detailed experimental settings).

_Sensitive Transformer._ In _PIS CLIP_, we learn and establish a mapping of prompts to stereotypes. Then, based on this learned mapping, we can create sensitive constraints for prompts. When given a prompt, we aim to provide specific sensitive constraints tailored to that prompt. To achieve this, we approach this as a query problem: the aim is to construct the value (V: sensitive constraints) by matching a query (Q: prompts) with keys (K: the mapping of prompts to stereotypes). We implement this task based on the transformer architecture (Vaswani et al. (2017)).

\[}{}})V.}\] (5)

In Equation (5), the input consists of queries and keys of dimension \(d_{k}\). We compute the dot products of the query with all keys, divide each by \(}\), and apply a softmax function to obtain the weights on the values.

**Sensitive Constraints Guide Distribution Alignment.** Consider a T2I diffusion model \(\), which generates images \(x\) with a sensitive attribute \(s\). The attribute \(s\) can have \(\) different categories and needs to align with a target distribution \(\) that is free of stereotypes. We use a prompt to generate a batch of images, denoted as \(=\{x_{i}^{v(s)}\}_{i}\). For the PDF of the sensitive attribute of image \(x\), \(h(x^{v(s)^{i}})\), let \(h(x^{v(s)^{i}})=p_{x}^{v(s)^{i}}=[p_{x}^{v(s)^{1}},p_{x}^{v(s)^{2}},,p_{ x}^{v(s)^{}}],i||\), where \(p_{x}^{v(s)}\) represents the estimated probability of generating images with stereotypes using model \(\). Assume another set of images, \(}=\{x_{i}^{u(s)}\}_{i}\), generated with the same prompt P but without stereotypes. The probability distribution of its sensitive attribute is \(h(x^{u(s)^{i}})=p_{x}^{u_{i}(s)^{i}}=[p_{x}^{u(s)^{1}},p_{x}^{u(s)^{2}},, p_{x}^{u(s)^{}}],i||\), and the corresponding probability distribution without stereotypes is denoted as \(p_{x}^{u(s)}\). We compute the distribution distance between \(p_{x}^{v(s)}\) and \(p_{x}^{u(s)}\).

\[^{*}=}}{}\ |(p_{x}^{v(s)})-p_{x}^{u(s)}|,\] (6)

where \(S_{}\) is the constraint space, \(S_{}=\{V(s_{1}),V(s_{2}),,V(s_{n})\}_{n}\), \(V(s)=[v(s)^{1},v(s)^{2},,\)\(v(s)^{}]\), and \(\) is a set of constraints within this space. The optimal constraint for minimizing the distance between two probability distributions in the constraint space is \(^{*}\). The \(\) contains all possible sensitive attributes \(S=\{s_{1},s_{2},,s_{n}\}_{n}\) and their value corresponding to the prompts. When a sensitive constraint \(\) is added to the input of the T2I diffusion model, it is combined with the original prompt embedding to form a new input for the T2I sampling process. Since the new input includes the sensitive constraint information, it guides the T2I process to produce a new output \((p_{x}^{v_{i}(s)})=[p_{x}^{v(s)^{1}_{}},p_{x}^{v(s)^{2}_{}}, ,p_{x}^{v(s)^{}_{}}]\). \((p_{x}^{v_{i}(s)})\) is a distribution that closely approximates the stereotype-free distribution \(p_{x}^{u(s)}\).

## 4 Experiment

We conduct experiments on popular T2I diffusion pipelines to evaluate the generalizability of our approach across various T2I diffusion pipelines, which include \(\)_runwayml/stable-diffusion-v1-5_(SD-1.5) (Rombach et al. (2022)), \(\)_stabilityai/stable-diffusion-xl-base-1.0_ (SDXL) (Meng et al. (2021)), \(\)_ByteDance/SDXL-Lightning_ (Lightning) (Lin et al. (2024)), \(\)_stabilityai/sdd-turbo_ (Turbo) (Sauer et al. (2023)), and \(\)_stabilityai/stable-cascade_ (Cascade) (Permis et al. (2024)). All these models are openly accessible from Hugging Face*. As mentioned above, we apply our stereotype mitigation approach to the five mainstream T2I diffusion pipelines. We use the optimal sampler and keep the same hyperparameters setting to generate images for all T2I diffusion pipelines (See Appendix C.2 for detail experiment setting). We create ten prompts for each of the four categoriesof stereotypes discussed in Section 3.1, generate \(100\) images per prompt, and calculate attribute probability distributions by utilizing \(100 10=1000\) images. In addition, for comparison, we test each pipeline using two sets of data, one from the original pipeline and the other from the pipeline with MAS. We utilize the _SDTV_ value to evaluate the severity of stereotypes in T2I models. For four types of stereotypes, we apply the corresponding _SDTV_ to evaluate the severity of each type (See Appendix B for detailed calculation methods for the _SDTV_s corresponding to different types of stereotypes).

### Mitigation effects

**Evaluation of stereotype mitigation effectiveness across different T2I diffusion models.** Table 2 reports the performance of our stereotype mitigation approach. Our approach effectively mitigates the association-engendered stereotypes in T2I and maintains excellent performance in mitigating the non-association-engendered stereotypes. Besides, Table 2 shows that the latest T2I models (Lightning, Turbo, and Cascade) display more pronounced stereotypes than their predecessors, SD-1.5 and SD XL. Nevertheless, our approach performs robust generalizability, demonstrating superior performance in mitigating stereotypes across various scenarios, including traditional and the latest T2I models involving non-association-engendered/association-engendered stereotypes (Figure 3).

**Comparative evaluation with different stereotype mitigation approaches.** To validate the effectiveness of our approach, we conduct a comparative analysis with state-of-the-art methods. Table 3 shows that our approach outperforms six other solutions, demonstrating fewer stereotypes for all eight single-object and multi-object scenarios. Our approach offers a distinct advantage compared to the prompt fine-tuning approach. We directly learn the relationship between prompts, images, and stereotypes, which enables us to construct more targeted constraints for the stereotypes in T2I rather than relying on broadly sensitive attributes as supplements. As a result, our approach can maintain an effective mitigation strategy even when faced with more complex and concealed stereotypes. Compared to the model fine-tuning approach (Shen et al. ), our approach only requires integration within the T2I workflow to perform mitigation, eliminating the need to retrain the original T2I model. During the training stage, the fine-tuning mitigation approach must ensure that it cannot affect

    approach \\  }} &  &  &  &  \\    & Gender & Race & Region & G\(\)R. & Gender & Race & Region & &  \\  SD 1.5 & 68\(\).27 & 82\(\).14 & 81\(\).10 & 75\(\).20 & 49\(\).25 & 47\(\).23 & 49\(\).19 & 53\(\).17 & 40\(\).03 \\  Kim. 2023 & 43\(\).17 & 39\(\).08 & - & - & - & - & - & 39\(\).03 \\ Chuang. 2023 & 38\(\).10 & 49\(\).04 & - & 24\(\).02 & - & - & - & - & 37\(\).04 \\ Gandikota. 2024 & 49\(\).33 & 43\(\).06 & - & 21\(\).03 & - & - & - & - & 38\(\).04 \\ Bansal.2022 & 46\(\).32 & 37\(\).08 & - & 19\(\).04 & - & - & - & - & 36\(\).04 \\ Wang. 2023 & 47\(\).23 & 40\(\).05 & - & 20\(\).02 & - & - & - & - & 39\(\).04 \\ Shen. 2024 & 22\(\).13 & 42\(\).05 & - & 20\(\).03 & 1.8\(\).13 & 1.9\(\).06 & - & - & 39\(\).04 \\ MAS(Ours) & 17\(\).14 & 21\(\).09 & 23\(\).13 & 21\(\).02 & 17\(\).11 & 20\(\).09 & 20\(\).02 & 16\(\).10 & 39\(\).04 \\   

Table 3: Comparison with Kim. 2023 (Kim et al. ), Chuang. 2023 (Chuang et al. ), Gandikota. 2024 (Gandikota et al. ), Bansal. 2022 (Bansal et al. ), Wang. 2023 (Wang et al. [2023c]), Shen. 2024 (Shen et al. ). “-” denotes that this approach is unable to mitigate stereotypes. \(\)\(\) indicate that the approach is more outstanding with higher/lower scores.

    &  &  &  &  \\    & Gender & Race & Region & G\(\)R. & Gender & Race & Region & &  \\  SD-1.5 & 68\(\).27 & 82\(\).14 & 81\(\).10 & 75\(\).20 & 49\(\).25 & 47\(\).23 & 49\(\).19 & 53\(\).17 & 40\(\).03 \\  MS(Ours) &.17\(\).14 &.21\(\).09 &.23\(\).13 &.21\(\).02 &.17\(\).11 &.20\(\).09 &.20\(\).02 &.16\(\).10 \\  SD XL &.84\(\).14 &.40\(\).29 &.59\(\).20 &.61\(\).12 &.74\(\).13 &.83\(\).11 &.87\(\).08 &.73\(\).15 \\ MAS(Ours) &.15\(\).12 &.16\(\).05 &.13\(\).04 &.19\(\).09 &.16\(\).10 &.20\(\).11 &.21\(\).07 &.15\(\).05 \\  Lightning &.81\(\).19 &.96\(\).02 &.94\(\).02 &.88\(\).09 &.86\(\).04 &.82\(\).09 &.90\(\).04 &.78\(\).09 \\ MAS(Ours) &.18\(\).12 &.16\(\).09 &.17\(\).04 &.15\(\).12 &.17\(\).10 &.19\(\).05 &.22\(\).11 &.17\(\).08 \\  Turbo &.92\(\).08 &.89\(\).10 &.80\(\).16 &.89\(\).11 &.82\(\).11 &.88\(\).07 &.85\(\).07 &.72\(\).08 \\ MAS(Ours) &.16\(\).13 &.15\(\).10 &.16\(\).10 &.20\(\).13 &.17\(\).11 &.19\(\).10 &.20\(\).10 &.15\(\).10 \\  Cascade &.96\(\).02 &.90\(\).07 &.87\(\).09 &.93\(\).05 &.90\(\).05 &.88\(\).07 &.89\(\).06 &.81\(\).04 \\ MAS(Ours) &.17\(\).15 &.17\(\).09 &.19\(\).08 &.17\(\).08 &.17\(\).08 &.21\(\).07 &.23\(\).09 &.16\(\).04 \\   

Table 2: The _SDTV_ value of five mainstream T2I models in four stereotype types. \(\) indicates that smaller _SDTV_ values correspond to less severe stereotypes. \(\)XX\(\).XX represents the optimal result.

the original T2I diffusion model's alignment of text to image. Therefore, not all resources can be utilized to learn stereotypes, significantly limiting the learning of more types of stereotypes. However, our model can fully use computing resources to learn more types of stereotypes. This enables our approach to demonstrate a practical mitigation effect in covert stereotype scenarios involving multiple objects.

### Generalization

**Semantics preservation experiment.** The primary fundamental of T2I is to ensure that the generated image is consistent with the provided prompt. To achieve this, we conduct semantics preservation (S.P.) (Pezone et al. (2024); Radford et al. (2021)) experiments where we encode prompt, text, and images using CLIP's (_Vit-L1/4_) text-image encoder. In Table 4, we report (1) CLIP-T2I: the CLIP score between generated images and prompts; (2) CLIP-I2I: the similarity between stereotypes-mitigated T2I and original T2I-generated images for the same prompts and hyperparameters. The stereotype-mitigated T2I diffusion model can remain consistent with the original T2I diffusion model in semantics preservation.

**Generalization to non-template prompts.** We summarize more than \(300\) relevant template prompt words from the work of Chuang et al. (2023) and Li et al. (2024), listed in Appendix D, which contains prompt words related to occupation, adjectives, region, race, gender, etc. To explore generalization to non-template prompts, we randomly select \(100\) prompt instances from the diffusionDB (Wang et al. (2022)) dataset--data that might inadvertently perpetuate stereotypes. Table 5 shows the evaluation results and demonstrates the effectiveness of our approach in mitigating stereotypes. Although we only implement stereotype mitigation for template prompts, the stereotype-mitigation effect also generalizes to more complex non-templated prompts. We list some images generated with non-template prompts in Appendix F.

**Impact on T2I diffusion models.** Since our approach integrates a _Sensitive Transformer_ model into the original T2I workflow, it could introduce additional overhead to the T2I model or impact the quality of the generated images. To evaluate these potential effects, we conduct a series of experiments focusing on the quality of image generation and the efficiency of the generation process. We set four different batch sizes (\(10,20,50,100\)) for the five different T2I diffusion models. We evaluate the impact of using MAS on the quality and efficiency of generated images of original T2I by measuring the Frechet Inception Distance (FID) (Heusel et al. (2017)) value of generated images and the time taken to generate images in different batch sizes. Table 6 demonstrates the evaluation results. Our mitigation approach maintains comparable image quality and generation efficiency to the original T2I diffusion models. Compared to other approaches, although our solution adds sensitive constraints to the original T2I diffusion model, it only introduces low additional overhead.

    &  &  \\   & 10 & 20 & 50 & 100 \\  SD-1.5 & 25.8\(\)3.00 & 51.2\(\)3.00 & 125.9\(\)0.00 & 252\(\)19.0 & 15.5\(\)13.0 \\ MAS(Ours) & 29.4\(\)2.80 & 58.4\(\)4.90 & 133\(\)11.0 & 270\(\)23.0 & 17.2\(\)17.0 \\  SD-1.5 & 43.9\(\)2.50 & 87.6\(\)4.50 & 219\(\)12.0 & 429\(\)25.0 & 16.1\(\)60.90 \\ MAS(Ours) & 46.7\(\)3.40 & 95.4\(\)5.70 & 230\(\)13.0 & 443\(\)27.0 & 16.7\(\)80.0 \\  Lighting & 6.99\(\)0.70 & 12.9\(\)1.93 & 34.0\(\)3.40 & 64.5\(\)5.10 & 22.6\(\)1.20 \\ MAS(Ours) & 8.21\(\)0.94 & 14.7\(\)1.87 & 39.0\(\)3.21 & 72.9\(\)5.50 & 23.1\(\)1.51 \\  Thirty & 10.51\(\)1.50 & 43.5\(\)3.09 & 35.9\(\)4.20 & 71.9\(\)5.00 & 20.6\(\)2.10 \\ MAS(Ours) & 10.5\(\)2.10 & 19.6\(\)3.40 & 43.1\(\)4.00 & 88.2\(\)4.90 & 20.9\(\)3.00 \\  Cascade & 25.9\(\)1.30 & 49.7\(\)3.60 & 112\(\)7.20 & 245\(\)17.0 & 23.6\(\)1.70 \\ MAS(Ours) & 29.3\(\)1.90 & 57.3\(\)3.90 & 126\(\)8.30 & 267\(\)17.0 & 240.2\(\)2.20 \\   

Table 6: Evaluate the impact of MAS on image quality and efficiency generated by the original T2I model.

    & & SD-1.5 & SD XL & Lightning & Turbo & Cascade \\   & Original &.39\(\).03 &.33\(\).02 &.32\(\).03 &.32\(\).02 & **.43\(\)**.04 \\  & Ours &.38\(\).05 &.33\(\).04 &.32\(\).05 &.31\(\).04 & **.42\(\)**.05 \\  CLIP-I2I \(\) & Ours &.80\(\).11 &.78\(\).13 &.84\(\).07 &.76\(\).12 & **.89\(\)**.02 \\   

Table 4: Semantic preservation experiment.

    & _{i}\)} & _{i}\)} & _{i}\)} &  \\   & Gender & Race & Region & G\(\)R & & & CLIP-T2I \\  SD-1.5 & 69\(\)1.24 & 84\(\)1.01 & 52\(\)1.10 & 73\(\)1.16 & 48\(\).21 & 52\(\)17 & 40\(\).05 \\  Kim. 2023 & 44\(\)1.16 & 38\(\)0.09 & - & - & - & - & 39.0\(\)0 \\ Chung. 2024 & 36\(\)1.11 & 47\(\)0.05 & - & 24\(\)0.04 & - & - & 37\(\).03 \\ Gandukota. 2024 & 50\(\)3.04 & 44\(\)1.00 & - & 22\(\)0.04 & - & - & 40\(\)0.04 \\ Baas1.2022 & 49\(\)2.70 & 40\(\)1.0 & - &.18\(\)0.04 & - & - & 40\(\)0.04 \\ Wang. 2023 & 49\(\)1.18 & 43\(\)1.0 & - & 21\(\)0.0 & - & - & 39\(\)0.03 \\ Shen. 2024 & 25\(\)1.15 & 44\(\)0.09 & - &.47\(\)0.05 & - & - & 40\(\)0.04 \\ RAS(Ours) & 20\(\)1.11 & 23\(\)10.20 & 23\(\)1.15 & 20\(\)0.05 & 21\(\)13 & 18\(\)0.04 & 40\(\)0.04 \\   

Table 5: Non-template prompts evaluation experiment.

**Stereotype mitigation experiments in more T2I scenarios.** This section aims to evaluate the effectiveness of MAS in mitigating stereotypes in more complex T2I scenarios. In practical T2I generation processes, it is necessary to incorporate models such as LoRA and ControlNet to control the image's style and structure. Consequently, our mitigation approach must be capable of reducing stereotypes in multi-model collaboration T2I scenarios. We conduct stereotype mitigation experiments by combining several mainstream models, including the diffusion-based retraining model Realistic+ (R-SD), LoRA models, and ControlNet models (Con). Table 7 demonstrates that our stereotype mitigation approach maintains outstanding performance even in T2I generation scenarios involving multiple model combinations. See Appendix G.1 and Appendix G.2 for detailed descriptions of the experiment and examples of images.

    &  &  &  &  \\   & Gender & Race & Region & G\(\)R & & & & & & & & & & & & & & & & & & \\  RSD & 84\(\) & 0.07 & 87\(\) 0.5 & 81\(\) 0.09 & 78\(\) 1.1 & 81\(\) 0.06 & 77\(\) 1.13 & 38\(\) 0.03 &  &  &  &  &  \\ MAS(Ours) & 23\(\) & 19\(\) & 22\(\) 13 & 22\(\) 0.70 & 78\(\) 0.03 & 21\(\) 0.09 & 20\(\) 0.06 & 38\(\) 0.03 &  &  &  \\ RSD + D-1 LoRA & 85\(\) & 0.06 & 85\(\) 0.70 & 89\(\) 1.00 & 79\(\) 1.13 & 83\(\) 0.08 & 75\(\) 1.12 & 39\(\) 0.04 &  &  &  \\ MAS(Ours) & 21\(\) & 11\(\) & 21\(\) & 23\(\) 0.06 & 19\(\) 0.09 & 21\(\) 1.00 & 21\(\) 0.08 & 38\(\) 0.04 &  &  &  \\  RSD + LoRA + Con & 84\(\) & 0.06 & 86\(\) 0.28 & 0.79 & 1.10 & 81\(\) 0.08 & 78\(\) 1.11 & 38\(\) 0.05 &  &  &  \\ MS(Ours) & 29\(\) & 16\(\) & 12\(\) & 20\(\) 0.70 & 19\(\) 1.00 & 21\(\) 1.00 & 20\(\) 0.04 & 38\(\) 0.03 &  &  \\  R-SD + LoRA + Con & 87\(\) & 0.04 & 86\(\) 0.7 & 81\(\) 1.00 & 80\(\) 1.00 & 83\(\) 0.09 & 78\(\) 1.12 & 38\(\) 0.05 &  &  &  \\ MS(Ours) & 22\(\) & 10\(\) & 22\(\) & 13\(\) & 21\(\) 0.09 & 20\(\) 0.07 & 21\(\) 1.4 & 21\(\) 0.09 & 38\(\) 0.05 &  \\   

Table 7: Stereotype mitigation experiment in complex T2I scenarios.

Figure 3: Images generated from the original SD-1.5 (left) and the SD-1.5 with MAS for mitigating stereotypes (right). Use the same prompt and T2I parameter settings for each category to generate 100 batch images and calculate the _SDTV_ value. Compare the changes in _SDTV_ values before and after mitigation. After applying MAS to the original model, the stereotypes are significantly mitigated. More images in Appendix H.

Conclusion

In this paper, we took the first step toward mitigating association-engendered stereotypes in Text-to-Image (T2I) diffusion models. We innovatively modeled stereotypes as a probability distribution alignment problem and constructed a probability distribution model for both non-association-engendered and association-engendered stereotypes. Then, we proposed the Mitigate Association-engendered Stereotypes (MAS) framework for the first time. MAS learned the mapping of prompts, images, and stereotypes and constructed sensitive constraints to guide the T2I diffusion model in generating stereotype-free images by embedding these sensitive constraints into the T2I diffusion process. Additionally, we proposed a novel metric to evaluate stereotypes, _Stereotype Distribution Total Variation_ (_SDTV_). Finally, comprehensive experiments demonstrated that we contribute an effective mitigation approach for association-engendered stereotypes in T2I, establishing a more ethical and reliable foundation for future text-to-image generation development.

## 6 Limitations

This research effectively mitigated both non-association-engendered and association-engendered stereotypes. While it successfully addressed the identified stereotypes, exploring other subtle stereotypes will be an interesting direction for future research. To our knowledge, modeling the problem as aligning existing distributions with target distributions holds significant potential for further extension into research areas such as debiasing and detoxification in large models. Investigating these avenues could provide deeper insights and more comprehensive solutions to ethical issues in generative models.