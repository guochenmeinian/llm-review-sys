# Discovering Sparsity Allocation for Layer-wise

Pruning of Large Language Models

 Lujun Li\({}^{1}\), Peijie Dong\({}^{2}\), Zhenheng Tang\({}^{2,3}\), Xiang Liu\({}^{2}\), Qiang Wang\({}^{4}\),

Wenhan Luo\({}^{1}\), Wei Xue\({}^{1}\), Qifeng Liu\({}^{1}\), Xiaowen Chu\({}^{2*}\), Yike Guo\({}^{1}\)

\({}^{1}\)Hong Kong University of Science and Technology

\({}^{2}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)Hong Kong Baptist University \({}^{4}\)Harbin Institute of Technology (Shenzhen)

iliulujuani@gmail.com,pdong212@connect.hkust-gz.edu.cn, zhtang@comp.hkbu.edu.hk, xliu886@connect.hkust-gz.edu.cn, qiang.wang@hit.edu.cn, {whluo,weixue,liuqifeng,kwchu,yikeguo}@ust.hk

###### Abstract

In this paper, we present DSA, the first automated framework for discovering sparsity allocation schemes for layer-wise pruning in Large Language Models (LLMs). LLMs have become increasingly powerful, but their large parameter counts make them computationally expensive. Existing pruning methods for compressing LLMs primarily focus on evaluating redundancies and removing element-wise weights. However, these methods fail to allocate adaptive layer-wise sparsities, leading to performance degradation in challenging tasks. We observe that per-layer importance statistics can serve as allocation indications, but their effectiveness depends on the allocation function between layers. To address this issue, we develop an expression discovery framework to explore potential allocation strategies. Our allocation functions involve two steps: reducing element-wise metrics to per-layer importance scores, and modelling layer importance to sparsity ratios. To search for the most effective allocation function, we construct a search space consisting of pre-process, reduction, transform, and post-process operations. We leverage an evolutionary algorithm to perform crossover and mutation on superior candidates within the population, guided by performance evaluation. Finally, we seamlessly integrate our discovered functions into various uniform methods, resulting in significant performance improvements. We conduct extensive experiments on multiple challenging tasks such as arithmetic, knowledge reasoning, and multimodal benchmarks spanning GSM8K, MMLU, SQA, and VQA, demonstrating that our DSA method achieves significant performance gains on the LLaMA-1l2l3, Mistral, and OPT models. Notably, the LLaMA-1l2l3 model pruned by our DSA reaches 4.73%16.18%10.65% gain over the state-of-the-art techniques (_e.g._, Wanda and SparseGPT).

## 1 Introduction

Large language models (LLMs) [63; 51; 4] have ushered in a new era of natural language processing (NLP) , demonstrating remarkable capabilities in understanding and generating human-like text . However, recent LLMs have an incredibly large number of parameters, which contributes to their high computational resource consumption. For example, OpenAI's GPT-3 model has 175 billion parameters and consumed 284,000 kWh of energy during its training . The exponential growth in model size and complexity presents challenges, especially for deployment on resource-constraineddevices. As a result, there is a pressing need to develop effective compression techniques [3; 67] that can reduce the size of LLMs while preserving their performance. One promising approach is pruning, which involves removing redundant or less important parameters from the model.

Conventional pruning methods  propose extensive pruning metrics  and sparse training strategies . However, these traditional methods often involve performance drops on small-scale models and require extra fine-tuning, making them difficult to transfer to LLMs due to differences in model structure and the high cost of the extra fine-tuning. To address this, recent approaches like SparseGPT  and Wanda  have been developed specifically for pruning LLMs. SparseGPT prunes insignificant weights and reconstructs layer-wise outputs based on an importance metric obtained from the Hessian matrix. Wanda proposes a streamlined approach that simplifies the computations by using only the product of weight and activation magnitudes. Despite these advancements, these pruning methods share a common limitation: they uniformly set sparsity ratios for different layers in LLMs, failing to account for the varying importance of each layer in the model's overall performance. Intuitively, the front layers of LLMs are considered more important, as they establish the fundamental language understanding upon which the rest of the model relies. The limitations of uniform pruning contradict this intuition and lead to performance degradation of pruned LLMs with high sparsity ratios or on difficult language understanding tasks. However, achieving non-uniform pruning is also difficult, mainly due to two challenges: **(1) Additional costly computations:** Most non-uniform methods are trial-and-error paradigms requiring many computations and evaluation overheads. For example, BESA  performs differentiable iterative optimization for block-wise sparsity allocation based on evaluation results. However, this method primarily focuses on intra-layer sparsity configuration and takes at least 5 hours, which is considerably slower compared to other training-free approaches. Additionally, the overheads of these methods grow with both the number of layers and the sparse granularity of LLMs. **(2) Fixed and empirical allocations:** Recent methods like OWL  assign different sparsity ratios based on the outlier ratio within each layer. This empirical method requires tuning hyperparameters such as the outlier threshold and sparsity upper/lower bounds to obtain optimal values, and it heavily relies on empirical analysis and handcrafted design without providing a solid theoretical foundation for its effectiveness, making it difficult to scale to various models and datasets (see Figure 1 _(right)_). These dilemmas naturally raise the question: _How can we efficiently obtain adaptive allocation strategies capable of handling different models and scenarios?_

To answer this question, we analyze the distribution of element-wise scores of different layers in LLMs based on existing sparse methods. As shown in Figure 1 _(left)_, we observe that the mean values of the per-layer element importance scores of the front layers enjoy larger values. This observation aligns with OWL (see Figure 1 _(middle)_) and the understanding that the initial layers of LLMs are more important . Furthermore, we find that other reduction operations, such as variance, entropy, etc., also yield distributions with similar trends. Motivated by these findings, we explore the possibility of directly utilizing these layer-wise importance scores as a guiding principle for allocating sparsity ratios across layers. Thus, we transform the reduction values of element-wise scores using various functions (e.g., sine, exponential) and employ the transformed values as layer-wise sparsity ratios. Such non-uniform manners bring promising gains, which are sensitive to specific transform operations. These observations inspire us to combine advanced reduction operations and transform functions to obtain an effective sparsity allocation based on the original importance scores from uniform pruning methods. However, how to obtain the most promising combinations? Fortunately, the recent advancements in AutoML [71; 45; 34] provide potential answers by enabling the automatic search for optimal solutions within a defined search space. By formulating this problem as an

Figure 1: Sparse ratios by our method _(left)_ and OWL _(middle)_, WikiText-2 perplexity results _(right)_.

AutoML task, we can leverage search algorithms to efficiently explore the search space and identify the effective combinations for non-uniform pruning in LLMs.

Based on our observations, we introduce DSA, an innovative framework that leverages expression discovery and evolutionary algorithms to tailor sparsity allocation schemes for LLMs. DSA seeks to find the best functions for mapping _element-wise scores_\(\)_per-layer importances_\(\)_sparsity ratios_. To identify the most effective allocation function, we construct a vast search space encompassing a diverse set of pre-process, reduction, transformation, and post-process operations. The pre-process operators, such as Frobenius norm and softmax, normalize the original importance values, enabling fair cross-layer comparison. Reduction operations, including variance, standard deviation, entropy, and geometric mean, extract the element-wise importance scores of each layer into a single representative value. Recognizing the potential for complex nonlinear relationships, we incorporate a wide range of mathematical functions as transformation operations, including sine and cosine. These functions provide flexibility to model intricate patterns and amplify or attenuate the importance scores as needed. Additionally, we introduce post-process operations to further increase the upper bound of the function fit. Within this rich search space, DSA employs an evolutionary algorithm to explore and discover promising allocation function candidates. The evolutionary process begins by initializing a population of diverse allocation functions, which are then iteratively evaluated and evolved through crossover and mutation operations. The crossover operation exchanges beneficial components between high-performing parent candidates, while mutation introduces random perturbations to promote exploration. Once the evolutionary process converges, DSA selects the top-performing allocation function candidates and seamlessly integrates them into existing pruning methods, such as Wanda and SparseGPT, through a plug-and-play mechanism. By leveraging the discovered allocation functions, these pruning methods can achieve significantly improved performance, maximizing compression while minimizing accuracy degradation. By automating the search process, DSA eliminates the need for manual tuning and expert intervention, reducing the time and effort required to find allocation strategies. The expressiveness of the search space and the ability to combine diverse operations enable the discovery of intricate, nonlinear allocation functions tailored to the unique characteristics of each LLM.

We conduct extensive experiments on publicly available language processing datasets and benchmarks. The experimental results demonstrate our method achieves significant performance gains on multiple challenging tasks such as arithmetic, knowledge reasoning, and multimodal tasks spanning GSM8K, MMLU, VQAv2, SQA, and VQA benchmarks across multiple model architectures including LLaMA-112l3, Mistral, Vicuna and OPT. Notably, our DSA method yields substantial improvements across all evaluated models, with peak gains of 14.58% in LLaMA-3 8B under magnitude pruning and 10.65% when integrated with SparseGPT. Even under high sparsity ratios of 60-70%, our method maintains robust performance, achieving improvements of 7.68% for LLaMA-2-13B at 60% sparsity. In multimodal tasks, DSA demonstrates exceptional capability by surpassing conventional pruning methods across all benchmarks, achieving superior scores of 76.08% on VQAv2, 65.57% on SQA, and 54.36% on VQA for LLaVA-1.5 with Vicuna-7B at 50% sparsity. The method's effectiveness is particularly evident in challenging scenarios, maintaining strong performance even under aggressive pruning conditions while consistently outperforming existing sparsity allocation approaches across model scales and architectures.

## 2 Related Work

Model compression techniques [24; 57], such as quantization [15; 40; 33] and sparsification [16; 65; 38], are practical approaches to reduce size of large language models (LLMs). Sparsification, or network pruning, increases the number of zero-valued weights and can be categorized into structured [39; 23] and unstructured [15; 66; 64] pruning. While determining pruning criteria and ratios is crucial, the massive scale of LLMs presents challenges for efficient pruning. Conventional retraining-based methods [65; 23] are often impractical due to high computational demands. Researchers develop LLM-specific pruning techniques that prioritize training-free and time-efficient approaches to address these challenges. SparseGPT  introduces an efficient Hessian matrix estimation technique to large-scale models. Wanda  further simplifies the approach, reducing the computational overhead and memory requirements. LLM-Pruner  examines model dependencies by incorporating first-order and approximated Hessian information, providing a comprehensive pruning approach. LLM Surgeon  adapts Kronecker-factored curvature approximations specifically for LLMs. Despite these advancements, most existing methods apply a uniform pruning rate across all layers, which may result in suboptimal performance. To address this, we present the first allocation function search for layer-adaptive sparsity, effectively minimizing performance degradation while achieving high compression ratios. Our method differs significantly from traditional layer-wise sparsity approaches for neural networks [12; 5; 26]. These methods often lead to the accumulation of errors across layers, as the pruning decisions for each layer are made independently without considering the global impact on the model's performance. Recent BESA  has shifted focus to intra-block sparsity allocation, employing various techniques to optimize the sparsity distribution within individual blocks or layers. FLAP  applies sparsity ratios process updating for performance compensation. In contrast, our method is layer-wise and training-free, with finer-grained allocation and an efficient process. OWL  requires experts' empirical design and tuning of hyperparameters. By automating the allocation process, our approach eliminates the need for manual intervention of OWL . Our method differs from approaches like Pruner-Zero  in both the search object and the technique type. Pruner-Zero is a uniformly sparse method that employs a metric-optimized strategy. In contrast, our method explores non-uniform sparsity allocations, searching for an optimal allocation strategy. Additionally, our DSA method deviates from layer-dropping techniques [18; 14], which involve directly removing entire layers from the model architecture. Our approach preserves the model's overall architecture while strategically distributing sparsity across layers, allowing for high compression ratios without sacrificing significant performance or relying on extensive fine-tuning. More discussion are in Appendix A.

## 3 Methodology

### Recap of Sparsity Methods for Large Language Models

Sparse methods introduce sparsity into the model weights by identifying and pruning redundant or less important weights for a given pre-trained dense weight matrix \(\). To determine which weights should be pruned, sparse methods employ pruning metrics or importance scores, denoted as \((_{l},_{l},_{l})\), where \(_{l}\) represents the weights of layer \(l\), and \(_{l}\) and \(_{l}\) are any layer-specific activations, gradient statistics, or the respective. The pruning metric ranks the weights based on their importance, and ranking results with the sparsity ratio \(\) serves as a threshold to select the most significant weights. The pruned weights are typically represented as a sparse mask \(_{l}\), which is a binary tensor of the same shape as \(_{l}\). The mask is obtained by applying a threshold function \(f\) to the pruning metric \(\) and the sparsity ratio \(_{l}\):

\[_{l}=f((_{l},_{l},_{l}), _{l}).\] (1)

Figure 2: Overview of our DSA framework. We search for allocation functions to map element-wise scores to sparse ratios. We build pre-process, reduction, transform, and post-process operations as the search space for the allocation function, and then we perform evolutionary search.

The pruned weights \(_{l}^{}\) are then obtained by element-wise multiplication of the original weights \(_{l}\) and the sparse mask \(_{l}\):

\[_{l}^{}=_{l}_{l},\] (2)

where \(\) denotes element-wise multiplication. The choice of the pruning metric \(\) and the sparsity ratio \(\) significantly impacts the effectiveness of the sparse method. The basic pruning metric is the magnitude-based approach , where \((_{l})=|_{l}|\) employs the element-wise absolute value to assess weight significance. The sparsity ratio \(\) plays a crucial role in determining the level of sparsity introduced into the model. Higher values of \(\) correspond to higher levels of sparsity, resulting in greater reductions in model size and computational requirements. However, excessive pruning may lead to significant performance degradation if important weights are removed. In contrast to traditional methods that use a fixed sparsity ratio \(\) for all layers, our adaptive sparsity allocation scheme allows the sparsity ratio \(_{l}\) to vary across layers based on their importance. This approach is motivated by the observation that different layers in a deep neural network contribute differently to the overall model performance, and a uniform sparsity ratio may not be optimal. The advance of our adaptive sparsity allocation scheme is its ability to identify and selectively prune the less important layers, achieving better compression while preserving the model's performance.

## 4 Allocation Function Search Space

**Allocation Function Representation.** Our allocation function \(\) aims to map element-wise scores \(\) to per-layer importance values \(\), and subsequently map these importance scores to sparsity ratios \(\). The allocation functions are represented as computation graphs consisting of various pre-process \(_{}\), reduction \(_{}\), transformation \(_{}\), and post-process \(_{}\) operations, as follows:

\[=_{}(_{}()),=_{}(_{}()), =(()).\] (3)

**Motivation of Allocation Function Design.** As discussed in introduction and Figure 1 (left), our design is motivated by analyzing element-wise score distributions: (1) We notice that mean, variance, and entropy values of per-layer element-wise scores can serve as allocation indicators, inspiring reduction operations. (2) While basic reduction of element-wise scores showed modest improvements, applying transform functions yielded more promising results, prompting the introduction of transform operations. (3) We include pre-process to normalize scores for fair comparison and post-process to further enhance function fit's upper bound. These observations naturally encourage us to employ the four cascading operations for search space.

**Primary Operators.** Table 1 presents a subset of the primary operators considered in our search space, which is organized into four main categories:

* **Pre-process operations \(_{}\)** are applied to the element-wise scores \(\) to prepare them for the subsequent reduction step. \(_{}\) standardizes inputs by normalizing scores across layers, ensuring consistent performance metrics by addressing scale variations. These operations can include clipping, normalization, or applying non-linear transformations.
* **Reduction operation \(_{}\)** aggregates the pre-processed scores into a single per-layer importance score \(\). \(_{}\) condenses element-wise information by extracting representative values and reduces computational complexity. They use statistical measures like mean, standard deviation, variance, and entropy to provide insights into the distribution of the input data.

  OP ID & OP Name & Expression \\  OP00 & Mean & \((x)\) \\ OP01 & std & \(}(x)\) \\ OP02 & var & var(\(x\)) \\ OP03 & sqrt & \(\) \\ OP04 & geometric & \(^{m}x_{i}}\) \\ OP05 & corref & \(x}{|x|^{2}}\) \\ OP06 & l2\_norm & \((x)}{(x)}\) \\ OP07 & l1\_norm & \(|x|_{1}\) \\ OP08 & entropy & \(-_{i,j}x_{ij} x_{ij}\) \\   
  OP ID & OP Name & Expression \\  OP09 & sigmoid & \(}}\) \\ OP10 & softmax & \(}{_{i=1}^{e^{2}}e^{x}}\) \\ OP11 & exp & \(e^{x}\) \\ OP12 & abslog & \(| x|\) \\ OP13 & cosine & \(cos(x)\) \\ OP14 & sine & \(sin(x)\) \\ OP15 & log & \(lnx\) \\ OP16 & no\_op & \(x\) \\ OP17 & rank & \((x)\) \\  

Table 1: Some operations in our search space. Full operations are in Appendix D.

* **Transform operation \(_{}\)** models the distribution of per-layer scores \(\) and transforms this into sparsity ratios \(\), enabling the representation of intricate patterns in layer importance. This can involve non-linear transformations like sigmoid, softmax, exponential, and logarithmic functions, which capture complex relationships, while trigonometric functions like sine and cosine capture periodic patterns or cyclical behaviors.
* **Post-process operation \(_{}\)** plays the role of augmenting the fitting power and flexibility on transform operation. \(_{}\) ensures that the sparsity ratios \(\) satisfy any required constraints, such as being between \(0\) and \(1\) across all layers. By combining these diverse operators, our framework constructs tailored allocation functions that capture the unique characteristics of each LLM.

## 5 Allocation Function Evolution

**Search Objectives.** Our search goal is to find the optimal combination of operations \(\) that makes the sparse model perform optimally on the validation set given the sparsity metric \(\) and the overall model size constraints \(C\). This can be formulated as an optimization problem

\[}{}( ,X,Y),()<C,\] (4)

where \(\) represents the weights of the LLMs, \(X\) and \(Y\) are the input and target data of the verification set, respectively, and \(\) is the performance metric (_e.g._, perplexity). The mask \(\) is determined by the sparsity ratios \(\) through combination of operations \(_{}\), \(_{}\), \(_{}\), and \(_{}\), as

\[=f(,)=f(,_{}( _{}(_{}(_{}( )))))).\] (5)

To solve this problem, we need to search a combination of \(\) in 4 levels with around 10 options in each level, resulting in a rather large space (_i.e._, \((10^{4})\)). In contrast to simple random search, we develop an evolutionary search for optimal allocation function.

**Evolution Search Procedure.** Our search process begins by generating an initial population of allocation function candidates, which can be created randomly or using heuristic techniques. Each candidate in this population corresponds to a unique combination of operations. Next, the performance of each candidate allocation function is evaluated. This involves computing the sparsity ratios by applying the candidate function to the sparsity metric, evaluating the pruned model on a validation set using a performance metric, and checking if the pruned model's size satisfies the given constraint. Based on this performance evaluation, the fittest candidates are selected for the next generation, considering criteria such as the performance metric, model size constraint, or a combination thereof. These selected candidates then undergo evolutionary operations like mutation and crossover to generate a new population of candidates for the subsequent iteration. The search process continues iterating until a stopping criterion is met, such as a maximum number of iterations or a satisfactory performance level. To accelerate the search, we employ various techniques: **(1) Program checking** uses static analysis to discard invalid candidates early, reducing computational overhead. **(2) Memoization and caching store** and reuse results from previous evaluations, avoiding redundant computations. **(3) Parallel evaluation** distributes the performance evaluation of different candidates across multiple computing resources. **(4) Surrogate models** approximate computationally expensive evaluations using techniques like neural networks trained on a subset of data. After each iteration, the performance of the best candidates is verified on a held-out validation set or a separate test set. These acceleration settings allow at least 100 times faster searches. In this way, we search our allocation function in **only 0.5 day on a 1\(\) NVIDIA GPU H800 server** based on Wanda using perplexity results from the validation set of LLaMA-1-7B on WikiText2 . We confirm that no search was performed on the test set, ensuring the comparisons are completely fair. In addition, the discovered allocation functions are transferable to other tasks without massive costs. Thus, the search cost can be spread across multiple pruning runs.

## 6 Discovered Allocation Function Analysis

One of the top-performing allocation functions discovered through the evolutionary search process is:

\[^{*}=^{*}_{}(^{*}_{}( ))=[n]{_{i=1}^{n}|(())|_{i}}, ^{*}=^{*}_{}(^{*}_{}( ))=((^{*})),\] (6)where \(^{*}_{}\) consists of two steps: log and abslog. The log step applies the natural logarithm operation \(()\) to the input importance values \(\), compressing the range of values and potentially highlighting differences in smaller values. The abslog step computes the absolute value of the natural logarithm, \(|()|\), ensuring that negative values are treated symmetrically with positive values, preventing potential cancellations or sign changes. \(^{*}_{}\) applies the geometric mean operator \([n]{_{i=1}^{n}x_{i}}\) to the result of abslog. This operation further compresses the range of values and introduces a nonlinear transformation. \(^{*}_{}\) is the cosine function \((x)\), applied to the output of the geometric mean. This periodic function introduces oscillatory behavior, which can capture potential cyclical patterns or dependencies in the importance values. Finally, \(^{*}_{}\) aplys the exponential function \((x)\) to the result of the cosine operation. This step reintroduces nonlinearity and expands the range of values, potentially amplifying or attenuating the importance scores as needed.

**Stability Analyses.** To show that the function \(=[n]{_{i=1}^{n}|(()) |_{i}}\) is stable under small perturbations in the input \(\), we can derive an expression for the difference \((+)-()\) and analyze its behavior for small \(\). The difference is approximately:

\[(+)-() ()^{1-n}(_{i=1}^{n}( (())))}}_{ j i}k_{j}())+(()^{2}).\] (7)

For small \(\), the second-order term \((()^{2})\) becomes negligible, and the leading term is linear in \(\). The coefficient of \(\) in this leading term is a product of bounded functions of \(\). Therefore, for small perturbations \(\) around any positive value of \(\), the difference is also small, and the function \(()\) is stable under such perturbations. More analyses are in the Appendix B.

## 7 Experiments

In this section, we conduct detailed evaluation experiments on multiple tasks and models. For pruning and evaluation, we follow the settings of Wanda, SparseGPT and ensure using the same database version, GPU model, and random seed across all experiments to maintain consistent conditions. More experimental results are in Appendix C.

### Experiments on Zero-shot Tasks

**Implementation.** To verify the effectiveness and generalizability, we perform extensive evaluation of our models on 7 zero-shot tasks. We employ a set of seven tasks sourced from the EleutherAI LM Harness . These tasks include Winogrande , OpenBookQA , HellaSwag , BoolQ , ARC , and RTE . To assess the performance of our Dynamic Sparse Allocation (DSA) method, we evaluate its effectiveness on several models. These LLMs include LLaMA-1 (7B/13B/30B/65B) , LLaMA-2 (7B/13B/70B) , LLaMA-3 (8B) , and OPT (6.7B/13B) . Our allocation function is applied to different pruning methods, namely Wanda , Magnitude-based pruning , and SparseGPT . For fair comparisons, we follow the same configurations of SparseGPT and Wanda methods. We select data from the C4 dataset and ensure that all test data used in the evaluation are from zero-shot settings.

   Model &  &  & LLaMA-3 & OPT \\ Method & 7B & 13B & 30B & 65B & 7B & 13B & 70B & 8B & 6.7B \\  Dense & 64.32 & 66.84 & 69.80 & 71.21 & 64.36 & 67.08 & 71.52 & 68.28 & 55.50 \\  Magnitude & 50.83 & 51.26 & 58.22 & 67.14 & 54.69 & 57.37 & 64.90 & 40.91 & 37.90 \\
**+ DSA (Ours)** & **53.73** & **59.05** & **60.55** & **67.86** & **57.90** & **61.38** & **68.76** & **55.50** & **40.25** \\
**Gain** & **2.90\(\)** & **7.78\(\)** & **2.33\(\)** & **0.72\(\)** & **3.21\(\)** & **4.01\(\)** & **3.85\(\)** & **14.58\(\)** & **2.35\(\)** \\  Wanda & 56.60 & 62.86 & 66.96 & 69.42 & 59.72 & 62.53 & 70.14 & 55.93 & 45.19 \\
**+ DSA (Ours)** & **59.22** & **63.03** & **67.80** & **70.98** & **60.80** & **64.87** & **70.54** & **60.70** & **45.45** \\
**Gain** & **2.62\(\)** & **0.17\(\)** & **0.84\(\)** & **1.56\(\)** & **1.09\(\)** & **2.34\(\)** & **0.40\(\)** & **4.76\(\)** & **0.26\(\)** \\  SparseGPT & 53.60 & 62.08 & 63.97 & 67.20 & 54.26 & 57.92 & 68.02 & 51.77 & 52.38 \\
**+ DSA (Ours)** & **58.33** & **62.49** & **67.63** & **67.32** & **59.22** & **64.10** & **68.70** & **62.41** & **55.15** \\
**Gain** & **4.73\(\)** & **0.41\(\)** & **3.66\(\)** & **0.12\(\)** & **4.95\(\)** & **6.18\(\)** & **0.68\(\)** & **10.65\(\)** & **2.77\(\)** \\   

Table 2: Mean accuracies (%) of our DSA at 50% sparsity rate on 7 zero-shot tasks.

[MISSING_PAGE_FAIL:8]

### Experiments on Arithmetric & Knowledge Reasoning Tasks

**Implementation.** We apply our allocation function to Wanda and evaluate the performance on arithmetic and knowledge reasoning tasks, specifically on the GSM8K  and MMLU  datasets using LLaMA-1 7B/13B, LLaMA-2 7B/13B, and Mistral 7B models .

**Comparison Results.** On GSM8K (Table 6), our method consistently outperforms baselines like magnitude pruning, SparseGPT and Wanda across all evaluated LLaMA-1, LLaMA-2, and Mistral models. The gains are most notable for smaller models like LLaMA-1 7B. Similarly, on MMLU (Table 7), our DSA achieves the highest accuracy among all methods, outperforming Wanda by up to 1.24% on LLaMA-1 7B and showing consistent improvements across larger LLaMA and Mistral models. The results highlight the effectiveness of our allocation strategy in optimizing sparse patterns across architectures, even on challenging reasoning tasks.

### Experiments on Multimodal Tasks

**Implementation.** To explore the applicability of our method towards a more diverse task, we evaluate our method for pruning language models on various visual question-answering and reasoning benchmarks, including VQAv2 , SQA , and VQA . In particular, our method is applied with Wanda to LLaVA-1.5 , where the Vicuna-7B and Vicuna-13B language models are pruned. In addition, we also transfer some pruning methods and make comparisons on these multimodal tasks.

**Comparison Results.** Table 8 and Table 9 showcase the performance of different pruning methods such as Magnitude, SparseGPT, and Wanda on the Vicuna-7B and Vicuna-13B models. For LLaVA-1.5 with Vicuna-7B, at 50% sparsity, our method surpasses conventional pruning methods across all benchmarks, achieving top scores of 76.08% on VQAv2, 65.57% on SQA, and 54.36% on VQA. Under the 4:8 structured sparsity pattern, our method consistently outperforms Wanda across all metrics, showing improvements of 0.84%, 0.92%, and 1.54% on VQAv2, SQA, and VQA respectively for the 7B model. The performance gap becomes even more pronounced with 2:4 sparsity, where

   Method &  &  &  \\  & 7B & 13B & 7B & 13B & 7B \\  Dense & 11.07 & 17.82 & 14.59 & 19.86 & 40.11 \\ Magnitude & 1.52 & 5.99 & 2.05 & 6.22 & 15.53 \\ SparseGPT & 8.19 & 15.60 & 8.11 & 13.42 & 25.40 \\ Wanda & 7.96 & 11.52 & 7.43 & 9.10 & 22.74 \\
**Ours** & **8.22** & **15.64** & **8.47** & **14.27** & **25.78** \\   

Table 6: 50% Sparsity results (%) on GSM8K.

   Method &  &  &  \\  & 7B & 13B & 7B & 13B & 7B \\  Dense & 35.28 & 46.98 & 41.97 & 51.47 & 58.92 \\ Magnitude & 26.24 & 30.12 & 26.04 & 43.83 & 50.83 \\ SparseGPT & 29.48 & 38.29 & 33.06 & 47.14 & 50.95 \\ Wanda & 29.81 & 37.84 & 32.09 & 48.06 & 53.05 \\
**Ours** & **31.05** & **39.76** & **33.08** & **48.38** & **53.87** \\   

Table 7: 50% Sparsity results (%) on MMLU.

   Method &  &  &  \\  & 7B & 13B & 7B & 13B & 7B \\  Dense & 35.28 & 46.98 & 41.97 & 51.47 & 58.92 \\ Magnitude & 26.24 & 30.12 & 26.04 & 43.83 & 50.83 \\ SparseGPT & 29.48 & 38.29 & 33.06 & 47.14 & 50.95 \\ Wanda & 29.81 & 37.84 & 32.09 & 48.06 & 53.05 \\
**Ours** & **31.05** & **39.76** & **33.08** & **48.38** & **53.87** \\   

Table 7: 50% Sparsity results (%) on MMLU.

   Method &  &  &  \\  & 7B & 13B & 7B & 13B & 7B \\  Dense & 35.28 & 46.98 & 41.97 & 51.47 & 58.92 \\ Magnitude & 26.24 & 30.12 & 26.04 & 43.83 & 50.83 \\ SparseGPT & 29.48 & 38.29 & 33.06 & 47.14 & 50.95 \\ Wanda & 29.81 & 37.84 & 32.09 & 48.06 & 53.05 \\
**Ours** & **31.05** & **39.76** & **33.08** & **48.38** & **53.87** \\   

Table 8: Results (%) on 7B LLaVA-1.5.

   Vicuna-7B &  &  &  \\  Dense & 78.50 & 66.80 & 58.20 &  & Dense & 80.00 & 74.94 & 61.30 \\ Magnitude (50\%) & 63.50 & 31.24 & 38.39 & Magnitude (50\%) & 75.79 & 70.95 & 52.16 \\ SparseGPT (50\%) & 75.86 & 63.92 & 53.69 & SparseGPT (50\%) & 78.62 & 71.19 & 58.23 \\ Wanda (50\%) & 75.72 & 63.99 & 53.05 & Wanda (**50\%)** & 78.58 & 70.97 & 58.03 \\
**Ours** (**50\%) & **76.08** & **65.57** & **54.36** & **Ours (50\%)** & **79.10** & **73.17** & **58.70** \\  Wanda (4:8) & 72.70 & 58.92 & 50.20 & Wanda (4:8) & 77.57 & 69.79 & 56.15 \\
**Ours** (**4:8) & **73.54** & **59.84** & **51.74** \\  Wanda (2:4) & 68.92 & 55.06 & 45.42 & Wanda (2:4) & 75.39 & 64.89 & 52.52 \\
**Ours** (**2:4) & **71.18** & **57.44** & **48.25** & **Ours (2:4)** & **76.75** & **67.13** & **54.05** \\   

Table 8: Results (%) on 7B LLaVA-1.5.

   Vicuna-13B &  &  &  \\  Dense & 80.00 & 74.94 & 61.30 \\ Magnitude (50\%) & 75.79 & 70.95 & 52.16 \\ SparseGPT (50\%) & 78.62 & 71.19 & 58.23 \\ Wanda (50\%) & 78.58 & 70.97 & 58.03 \\
**Ours (50\%)** & **79.10** & **73.17** & **58.70** \\  Wanda (4:8) & 77.57 & 69.79 & 56.15 \\
**Ours (4:8)** & **77.79** & **70.50** & **56.33** \\  Wanda (2:4) & 75.39 & 64.89 & 52.52 \\
**Ours (2:4)** & **76.75** & **67.13** & **54.05** \\   

Table 9: Results (%) on 13B LLaVA-1.5.

our method achieves substantial gains of 2.26%, 2.38%, and 2.83% over Wanda on the same metrics. For the 13B model, while the improvements under 4:8 sparsity are modest (0.22%, 0.71%, and 0.18%), the 2:4 pattern shows more significant gains of 1.36%, 2.24%, and 1.53% on VQAv2, SQA, and VQA respectively. The consistent superiority of our method across different model sizes and sparsity patterns demonstrates its robustness and effectiveness in maintaining model performance under aggressive compression settings.

### Analysis

**Search Algorithm Analysis.** Figure 3 compares random search with our evolutionary search in the function search tasks. Our advanced evolutionary search has faster convergence and final results, _e.g._, our search algorithm exceeds in 700 generations over 1500 generations of the random algorithm.

**Sparse Allocation Results Analysis.** Figure 1 illustrates the per-layer importance values and the final sparsity ratios with our allocation functions. These distributions are nicely tailored to the specific and explanatory nature of the LLMs.

## 8 Conclusion

In this paper, we introduce the DSA framework, which offers a powerful and automated approach to discovering tailored sparsity allocation schemes for LLMs. By leveraging expression discovery and evolutionary algorithms, DSA can effectively explore a vast search space of operations and uncover intricate, nonlinear allocation functions that map importance metrics to optimal layer-wise sparsity ratios. This automated process eliminates manual tuning and expert intervention, reducing the time and effort required for effective sparsity allocation. Our DSA demonstrates promising results on the LLaMA, Mistral, and OPT models. We hope the DSA framework and its underlying principles will provide valuable insights to the research community, inspiring new avenues for efficient and effective compression of LLMs and enabling their wider deployment in resource-constrained environments.

**Limitations.** Following the AutoML technical route [29; 11; 28], we also need some cost in search process. We will develop more efficient search algorithms and incorporate domain knowledge to guide and constrain the search process in future work.