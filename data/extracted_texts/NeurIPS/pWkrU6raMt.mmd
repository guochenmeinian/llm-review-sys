# SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking

Soukayna Mouatadid1, Paulo Orenstein2, Genevieve Flaspohler3, Miruna Oprescu4,

**Judah Cohen3, Franklyn Wang5, Sean Knight3, Maria Geogdzhayeva3, Sam Levang6, Ernest Fraenkel3, Lester Mackey4**

1University of Toronto, 2IMPA, 3MIT, 4Microsoft Research, 5Harvard, 6Salient Predictions

###### Abstract

Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and advance disaster notice but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce \(\), a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten state-of-the-art machine learning and deep learning-based methods from the literature. Overall, our benchmarks suggest simple and effective ways to extend the accuracy of current operational models. \(\) is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ Python package.

## 1 Introduction

Weather and climate forecasting are fundamental scientific problems with many applications, including agriculture, energy grids, transportation and disaster prevention . Indeed, short-term and long-term operational forecasts are critically employed in many sectors of our society and the world economy. However, skillful forecasts for the subseasonal regime--that is, 2 to 6 weeks ahead--still present operational challenges due to the chaotic nature of the weather  and to the interaction of weather and climate variables operating at different spatial and temporal scales .

In recent years, these challenges have spurred intense activity from both the meteorological and machine learning communities. On the one hand, steady advances have extended the reach of physics-based dynamical models of the atmosphere and oceans into the subseasonal realm . On the other, parallel efforts from the machine learning community have led to improved predictive skill through new models trained on historical observational data and dynamical model forecasts .

Nevertheless, developing and benchmarking new subseasonal models remains challenging due to a lack of standardized, curated datasets targeting this forecast horizon. The data necessary for subseasonal predictions are often collected from multiple data sources, each with its own data processing pipeline, and then standardized into a common spatial and temporal resolution. Because there is uncertainty regarding the main drivers of subseasonal phenomena, domain experts are typically needed to determine which features are likely to carry signal, and different developersemploy different aggregation techniques over time and space. Finally, the changing nature of weather makes the forecasting task hard to compare across different regions and years. This, in turn, has spurred several subseasonal forecasting challenges to benchmark existing solutions .

In this paper, we introduce \(\), a diverse collection of ground-truth measurements and dynamical forecasts for subseasonal prediction over the contiguous United States (U.S.). We include spatiotemporal measurements with known subseasonal impact (including, e.g., temperature, precipitation, sea surface temperature, and geopotential height); the states of known subseasonal drivers such as El Nino-Southern Oscillation (ENSO) and the Madden-Julian oscillation (MJO); and dynamical predictions for temperature and precipitation from eight operational models including the U.S. Climate Forecast System version 2 (CFSv2) and the leading subseasonal model from the European Centre for Medium-Range Weather Forecasts (ECMWF). The dataset is regularly updated and accessible via the open-source \(\) Python package for easy retrieval.

We then use the \(\) data to benchmark a wide range of subseasonal models, highlighting their strengths and weaknesses. These models include traditional meteorological benchmarks (e.g., Persistence and Climatology), operational dynamical models (e.g., CFSv2 and ECMWF), and ten state-of-the-art deep learning (e.g., N-BEATS  and Informer ) and machine learning (e.g., CFSv2++  and Prophet ) forecasters. Two of these models (Salient 2.0, the best-performing deep learning method, and LocalBoosting) were new creations of this work, and nine required new subseasonal forecasting implementations now available via the \(\) Python package. Model performance is measured for four standard subseasonal tasks: predicting average temperature 3-4 and 5-6 weeks ahead and predicting accumulated precipitation 3-4 and 5-6 weeks ahead. They are evaluated in terms of accuracy (measured by spatial root mean squared error) and skill (measured by uncentered anomaly correlation), over the years 2011-2020. Overall, we find that the simplest learned models typically outperform the meteorological baselines, the leading operational models, and the remaining learning methods. Additionally, we show that ensembling different methods through online learning leads to further gains in terms of both accuracy and skill.

Our aims in releasing \(\) are twofold. First, we aim to facilitate the development of skillful learning-based subseasonal forecasting models by providing a comprehensive, standardized, and machine-learning-friendly dataset. Second, we aim to provide standardized benchmarks for subseasonal forecasting progress. To this end, we define four core subseasonal prediction tasks that users can use as benchmarking targets: forecasting (i) temperature in weeks 3-4, (ii) temperature in weeks 5-6, (iii) precipitation in weeks 3-4, and (iv) precipitation in weeks 5-6. Significant advances in any of these tasks would have significant implications for the allocation of water resources, agricultural production, and disaster relief .

**Related Work** There are several datasets available for benchmarking weather models. For example, both the National Oceanic and Atmospheric Administration (NOAA) and the ECMWF provide reanalysis datasets tracking weather variables from the whole globe from the 1940s until today , and global model simulations can be found in datasets provided by the World Climate Research Programme  and ECMWF . More recently, several new datasets have been made available targeting specific AI applications in weather. For instance, classifying clouds , studying storm morphology  and nowcasting , predicting tropical cyclone intensity  and air quality metrics , and analyzing watershed-scale hydrometeorological time series  and river flows . There are also more general-purpose datasets, such as WeatherBench , which provides a benchmark for forecasting different medium-range weather variables 3 to 5 days out. For a general overview of weather datasets for machine learning, see .

While these datasets have helped advance weather prediction in different tasks, there are no general datasets specifically targeting the subseasonal scale for the U.S. There have been instead several competitions targeting this lead time including the U.S. Bureau of Reclamation (USBR) Sub-Seasonal Climate Forecast Rodeos  and the World Meteorological Organization Seasonal-to-Subseasonal (S2S) Artificial Intelligence (AI) Challenge . Furthermore, the SubX Experiment  also makes a series of subseasonal models available for benchmarking (which are included in the \(\) dataset). Finally, the precursor of this work, the SubseasonalRodeo dataset of , targets only the Western U.S., offers only a static data snapshot ending in 2018, provides no forecasts from the leading subseasonal dynamical model (ECMWF), and includes only coarse-grained (monthly) forecasts from the North American Multi-Model Ensemble, with limited utility for weekly or biweekly forecasting.

In contrast, SubseasonalClimateUSA is a modern, regularly updated resource targeting the contiguous U.S. with granular (daily and subweekly) forecasts from ECMWF and seven other operational dynamical models in the SubX consortium . Notably, both the present work and past studies have found complementary predictive signals in physics-based dynamical model forecasts and pure observational data that can lead to better forecasts than either data source alone [see, e.g., 23, 39]. In fact, recent work has demonstrated that even the least skill operational dynamical models can produce forecasts with skill comparable to the best when corrected suitably with observational data . As a result, we have endeavored to include both granular measurements and granular model forecasts in the SubseasonalClimateUSA dataset to best equip future model developers, researchers, and forecasters.

## 2 The SubseasonalClimateUSA dataset

The SubseasonalClimateUSA dataset houses a diverse collection of ground-truth measurements and dynamical model forecasts relevant to forecasting at subseasonal timescales. The dataset is regularly updated, CC BY 4.0 licensed, accessible via the open-source subseasonal_data Python package, and documented at the URL https://github.com/microsoft/subseasonal_data/blob/main/DATA.md. We summarize dataset contents, sources, and processing steps below and provide supplementary details in Appendix A.

**Data Collection and Processing** Figure 1 summarizes the SubseasonalClimateUSA data collection and processing pipeline. The pipeline collects raw data from seven meteorological data sources (contributing different variables, resolutions, and file formats), passes all data through a common preprocessing pipeline, and outputs a standardized collection of machine-learning-ready Python Pandas DataFrames and Series objects stored in HDF5 format. Each file contributes data variables falling into one of three categories: (i) spatial (varying with the target grid point but not the target date); (ii) temporal (varying with the target date but not the target grid point); (iii) spatiotemporal (varying with both the target grid point and the target date). Data representing ground-truth measurements are typically downloaded daily on \(0.25^{}\) or \(0.5^{}\) latitude-longitude grids. However, subseasonal forecasts are typically issued on coarser \(1^{}\) or \(1.5^{}\) grids and averaged over two-week periods . As a result, unless otherwise noted below, temporal and spatiotemporal variables arising from daily data sources were derived by averaging input values over a \(14\)-day rolling window, and spatial and spatiotemporal variables were derived by interpolating input data to a \(1^{}\) latitude-longitude grid and retaining only the grid points belonging to the contiguous U.S. To accommodate ECMWF forecasts, which were only made available on a \(1.5^{}\) latitude-longitude grid , we additionally download SubX forecasts at \(1.5^{}\) resolution and interpolate temperature and precipitation onto the same grid.

Figure 1: Schematic of the SubseasonalClimateUSA data collection and processing pipeline.

Following the protocol adopted by the USBR Sub-Seasonal Climate Forecast Rodeos , our temperature and precipitation variables are interpolated onto fixed \(1^{} 1^{}\) (NUM_LAT=181, NUM_LON=360) and \(1.5^{} 1.5^{}\) (NUM_LAT=121, NUM_LON=240) grids using the NCAR Command Language function area_hi2lores_Wrap with arguments new_lat = latGlobeF(NUM_LAT, "lat", "latitude", "degrees_north"): new_lon = lonGlobeF(NUM_LON, "1on", "longitude", "degrees_east"); wgt = cos(lat*pi/180.0) (so that points are weighted by the cosine of the latitude in radians); opt@critpc = 50 (to require only 50% of the values to be present to interpolate); and fiCyclic = True (indicating global data with longitude values that do not quite wrap around the globe). All remaining spatial and spatiotemporal variables are interpolated using the Climate Data Operators operator remapdis (distance-weighted average interpolation) with target grid r360x181.

**Data Features** The variables comprising the Subseasonal ClimateUSA dataset include:

* **Temperature** (global and U.S., 1979-present): daily mean of maximum and minimum temperature at 2 meters in \({}^{}\)C .
* **Precipitation** (global and U.S., 1948-present): daily accumulated precipitation in mm, aggregated by summing over a rolling two-week window instead of averaging .
* **Sea surface temperature and sea ice concentration** (global, 1981-present): daily variables that track variability in the oceans; the top three principal components for each variable were extracted using global 1981-2010 loadings .
* **Stratospheric geopotential height, zonal winds, and longitudinal winds** (global, 1948-present): daily geopotential height at 10, 100, 500, 850 millibars and zonal and longitudinal winds at 250 and 925 millibars as indicators of polar vortex variability; the top three principal components of each feature were extracted from global 1948-2010 loadings .
* **Surface pressure and relative humidity** (U.S., 1948-present): daily pressure and relative humidity near the surface (sigma level 0.995) .
* **Sea level pressure, precipitable water for entire atmosphere, and potential evaporation** (U.S., 1948-present): daily mean of pressure in millibars, amount of water in the atmosphere available for precipitation in kg/m\({}^{2}\), and potential evaporation rate at surface .
* **Elevation** and **Koppen-Geiger climate classification** (global): multi-resolution terrain elevation data  and Koppen-Geiger climate classification  for each grid point.
* **Madden-Julian Oscillation** (MJO, 1974-present): daily measure of tropical convection known to impact subseasonal climate; phase and amplitude were extracted (but not aggregated) .
* **Multivariate ENSO index** (MEI.v2, 1979-present): bimonthly scalar summary of the state of the El Nino-Southern Oscillation, an ocean-atmosphere coupled climate mode .
* **CFSv2** (U.S., 1999-present): daily 32-member ensemble mean forecasts of temperature and precipitation from the coupled atmosphere-ocean-land dynamical model with 0.5-29.5 day lead times .
* **SubX** (U.S., 1999-present): subweekly forecasts and hindcasts from seven dynamical models (GMAO-GEOS, NRL-NESM, RSMAS-CCSM4, ESRL-FIM, EMC-GEFS, ECCC-GEM, NCEP-CFSV2) and their multi-model mean for temperature and precipitation on a \(1.5^{} 1.5^{}\) latitude-longitude grid .
* **ECMWF** (U.S., 1995-present): control and perturbed forecasts and reforecasts of precipitation and temperature on a \(1.5^{} 1.5^{}\) latitude-longitude grid .

An example of Subseasonal ClimateUSA observations and dynamical model forecasts is displayed in Figure 2.

**Dataset Limitations** Here, we highlight several limitations of the Subseasonal ClimateUSA dataset. First, the dataset was designed for forecasting in the contiguous U.S., and hence several variables are only available in that region. In future work, we aim to develop an analogous dataset for global subseasonal forecasting. Second, subseasonal forecasts are commonly made at the biweekly temporal resolution and \(1^{}\) or \(1.5^{}\) spatial resolution provided in the Subseasonal ClimateUSA dataset ; however, these resolutions alone are insufficient for more localized forecasting problems without additional downscaling. Finally, many of our variables have undergone regridding via interpolation, which, while standard, can still introduce inaccuracies.

## 3 Subseasonal Forecasting Tasks

We study model performance through four canonical subseasonal forecasting tasks: predicting two variables--average temperature (\({}^{}\)C) and accumulated precipitation (mm) over a two-week period--each over two time horizons: 15-28 days ahead (weeks 3-4) and 29-42 days ahead (weeks 5-6). We forecast each variable at \(G=862\) locations on a \(1^{} 1^{}\) latitude-longitude grid covering the contiguous U.S. These prediction targets and time horizons were the focus of the Sub-Seasonal Climate Forecast Rodeos [47; 46], two yearlong real-time forecasting competitions sponsored by USBR and NOAA to advance the state of subseasonal climate prediction. The same targets are used by water managers to apportion water resources, control wildfires, and anticipate droughs and other extreme weather [47; 71].

We evaluate each forecast according to two metrics recommended by the USBR [47; 46]: root mean squared error (RMSE) and _skill_ (also known as uncentered anomaly correlation ). For a two-week period starting on date \(t\), let \(_{t}^{G}\) denote the vector of ground-truth measurements \(y_{t,g}\) for each grid point \(g\) and \(}_{t}^{G}\) denote a corresponding vector of forecasts. In addition, define climatology \(_{t}\) as the average ground-truth values for a given month and day over the years 1981-2010. Then the RMSE is given by

\[(}_{t},_{t})=_{g= 1}^{G}(_{t,g}-y_{t,g})^{2}}_{+}\]

with a smaller value indicating a more accurate forecast, and skill is defined by

\[(}_{t},_{t})=_{ t}-_{t},_{t}-_{t}}{\|}_{t}- _{t}\|_{2}\|_{t}-_{T}\|_{2}}[-1,1]\]

with a larger value indicating higher quality. For a collection of dates, we report average RMSE and average percentage skill, which is \(100\) times the average skill.

Training and Validation RecommendationsWe recommend adopting the Wednesdays of each complete year from 2011 onward as a standard test set when evaluating performance year by year (as in Figure 3) and the Wednesdays from the ten year period 2011-2020 as a standard test set when comparing with the overall (Table 1) or seasonal (Figure 3) performance reported in this work. We also recommend a progressive training and validation protocol in which, to produce a forecast for a given target date, a model can be (re)trained and (re)tuned using any data fully observable on the associated forecast issuance date (i.e., \(14\) days prior for weeks 3-4 and \(28\) days prior for weeks 5-6). All of the models evaluated in Section 5 respect this protocol. In addition, the Subseasonal ClimateUSA dataset provides convenient combination dataframes containing target variables associated with predictive features lagged by an appropriate amount to ensure that each predictive feature was fully observed on the issuance date associated with the target date.

Figure 2: Example of Subseasonal ClimateUSA observations and dynamical model forecasts.

Benchmark Models

Our experiments will evaluate three classes of forecasting methods: three standard meteorological baselines, the adaptive bias correction (ABC) models introduced in , and seven other state-of-the-art machine learning and deep learning methods drawn from the literature. We will also evaluate ensemble forecasts derived from these models. Open-source model implementations are available via the subseasonal_toolkit Python package. Most models train on all available data (subject to the progressive training and validation constraint of Section 3) and tune hyperparameters using a second round of progressive evaluation over the prior three years. Appendix B contains supplementary implementation details for each model, including training, hyperparameter tuning, and testing details.

Meteorological BaselinesWe first consider three standard subseasonal forecasting baselines.

Climatology. Climatology is a standard subseasonal benchmark for the expected temperature or precipitation at a location. For a given grid point and target date, it forecasts the average value of the target variable on the same day and month over 1981-2010 .

Debiased CFSv2. CFSv2 is the U.S. operational dynamical model commonly used for subseasonal forecasting . Debiased CFSv2 is a corrected ensemble forecast used as a benchmark in the two Subseasonal Climate Forecast Rodeo competitions . First, a CFSv2 ensemble forecast is formed by averaging 32 forecasts for the target period based on 4 different model initializations produced at 8 different lead times. The ensemble is then _debiased_ by adding the mean value of the target variable on the target month and day over the period 1999-2010 and subtracting the mean ensemble CFSv2 reforecast over the same period.

Persistence. This baseline  forecasts the most recently observed two-week target value.

ABC ModelsWe next evaluate the ABC models introduced in . ABC is a hybrid physics-plus-learning approach that takes as input a dynamical model forecast (here, CFSv2) and uses the historical record of prior forecasts and observations to correct the model's output and improve predictive skill. While the three learning models contributing to ABC described below are simple and computationally inexpensive, Section 5 shows that each enhancement improves over both operational practice and state-of-the-art learning techniques.

Climatology++. Climatology++ is as an adaptive form of Climatology that learns how many prior years and how many dates in a window around the target date to include in a smoothed historical mean or geometric median estimate for a given grid point, target date, and target variable; importantly, unlike a static climatology, Climatology++ allows these learned window sizes to vary over time to adapt to noise levels and variability.

CFSv2++. CFSv2++ is a learned correction for raw CFSv2 forecasts. After averaging CFSv2 forecasts over a range of issuance dates and lead times, CFSv2++ debiases the ensemble forecast by adding the mean value of the target variable and subtracting the mean forecast over a learned window of observations around the target day of year. The range of ensembled lead times, the number of averaged issuance dates, and the size of the observation window employed are selected adaptively.

Persistence++. For each grid point and target date, Persistence++ predicts a target variable as a function of lagged measurements, Climatology, and CFSv2 forecasts observable for the same grid point on the forecast issuance date. These features are combined using a linear least squares regression trained on all available historical data available as of the forecast issuance date.

State-of-the-art Learning MethodsWe also consider seven state-of-the-art learning methods.

AutoKNN. AutoKNN  was part of a winning solution in the Subseasonal Climate Forecast Rodeo I . Our implementation adapts it to target RMSE as an error metric.

Informer. The Informer  is a transformer-based deep learning model for time series shown to have state-of-the-art performance on a number of short term weather forecasting tasks.

LocalBoosting. LocalBoosting is a decision tree model using CatBoost  over features in a bounding box around the target to extract meaningful spatial information.

MultiLLR. The MultiLLR  model is a customized backward stepwise procedure to select Subseasonal ClimateUSA features relevant for prediction and local linear regression to combine those features into a forecast for each grid point.

N-BEATS. N-BEATS  is a neural network time series forecaster that obtained state-of-the-art results on the Makridakis M3  and M4  benchmarks for time-series forecasting.

ProphetThe Prophet model of  is an additive regression model for time-series and a winning solution in the Subseasonal Forecast Rodeo II .

Salient 2.0.: Salient 2.0 is an ensemble of fully-connected neural networks, trained on historical sea surface temperature (SST) data. It is based on Salient , a winning solution for the Subseasonal Forecast Rodeo I .

**Ensembles** Ensemble forecasts that combine the predictions of multiple models have been shown to improve the performance of long-, mid-, and short-range operational forecasting [10; 49; 23]. Here, we evaluate two ensembling strategies: Uniform ABC, which forms an equal-weighted average of the ABC model forecasts , and Online ABC, which uses the AdaHedgeD algorithm of  to choose weights adaptively to reflect relative model performance. See Appendices B.11 and B.12 for details.

## 5 Benchmark Results

We now turn to evaluating the models of Section 4 on the four subseasonal forecasting tasks of Section 3. We generate forecasts for each Wednesday in the years 2011-2020 and, for each reported period, we assess both mean RMSE relative to a baseline model and average percentage skill.

**Overall Performance** Table 1 summarizes model performance across the entire ten-year period 2011-2020. On each task, we find that the ABC models provide both the best RMSE and the best skill performance. For example, on the two precipitation tasks, Climatology++ alone improves upon debiased CFSv2 RMSE by 9% and skill by 161-250%, outperforming each of the meteorological baselines and state-of-the-art learning methods. On the two temperature tasks, CFSv2++ and Persistence++ each outperform all meteorological baselines and state-of-the-art learning methods, with CFSv2++ improving debiased CFSv2 RMSE by 6-7% and skill by 30-53%. On every task, we observe further improvements in both RMSE and skill by ensembling the predictions of the three ABC models.

One might wonder how the simple ABC models are able to outperform both the standard meteorological baselines and the state-of-the-art learning methods. We believe the answer to this question is multifaceted. First, even the leading physics-based dynamical models are subject to inaccuracies due to inexact measurement, incomplete representation of the environment, imperfect simulation, and chaos . Second, many of the more elaborate machine learning models studied in this work appear to be prone to overfitting in the presence of the relatively high noise levels of subseasonal forecasting. Third, the best-performing models, while relatively simple, are hybrid physics-plus-learning models designed to leverage the strengths of an underlying dynamical model while simultaneously enhancing its predictive skill by reducing its systemic bias.

Amongst the state-of-the-art learning methods, we find that Prophet performs the best for temperature weeks 5-6 and the two precipitation tasks, while MultiLLR performs the best for temperature weeks 3-4. Amongst the neural network methods (Informer, N-BEATS, and Salient 2.0), Salient 2.0 is the top performer with skill that rivals the other learning methods and precipitation RMSE that outpaces debiased CFSv2. In the more detailed analyses to follow, we omit Informer and N-BEATS due to space constraints and their relatively poor performance overall.

**Performance by Season and by Year** We observe the same trends when performance is disaggregated by season or by year (Figure 3). For example, in every season, Climatology++ outperforms debiased CFSv2 and each state-of-the-art learner for the two precipitation tasks, while CFSv2++ and Persistence++ outperform debiased CFSv2 and each state-of-the-art learner each season for the two temperature tasks. Similarly and despite significant heterogeneity in all models' performances from year to year, the ABC models provide the best RMSE performance in 9 out of 10 years for temperature weeks 3-4 and in 10 out of 10 years for the two precipitation tasks. Indeed, Persistence++ alone dominates the temperature weeks 3-4 baselines and learners every year save 2019, and the more detailed RMSE summary of Appendix C.2 shows that the Uniform and Online ABC ensembles dominate the precipitation baselines and learners every year. In Figure 3, we observe nearly identical improvement patterns for skill.

[MISSING_PAGE_FAIL:8]

**Western U.S. Competition Results** Finally, we evaluate our models on the exact geographic region and target dates of the recent Subseasonal Climate Forecast Rodeo II competition. Specifically, we produce forecasts for the Western U.S. region, delimited by latitudes 25N to 50N and longitudes 125W to 93W, at a 1\({}^{}\)\(\) 1\({}^{}\) resolution for a total of \(G=514\) grid points. Forecasts were issued every two weeks for a yearlong period with initial issuance date October 29, 2019 and final issuance date October 27, 2020, leading to a noisier evaluation with only 26 observations.

Table 8 in Appendix C.7 compares the predictive accuracy of the models studied in this work with the accuracy of the contest baselines (debiased CFSv2, Climatology, and, for precipitation only, the Rodeo I Salient model of ) and the performance of the top competitors for each task. For temperature weeks 3-4, Persistence++ provides a 16.59% improvement over the mean debiased CFSv2 RMSE, outperforming the contest baselines, the state-of-the-art learning methods, and all but two of the competitors (the top three competitors improved by 17.12%, 16.67%, and 15.47%). For temperature weeks 5-6, CFSv2++ provides a 9.26% improvement over debiased CFSv2 and, despite its simplicity, outperforms the contest baselines, the state-of-the-art learning methods, and all of the

Figure 3: Per season and per year average skill and improvement over mean debiased CFSv2 RMSE across the contiguous U.S. and the years 2011–2020. Despite their simplicity, the ABC models (solid lines) consistently outperform debiased CFSv2 and the state-of-the-art learners (dotted lines).

competitors in the subseasonal forecasting competition (the top competitor improved by 8.47%). On this task, the Uniform and Online ABC ensembles also outperform all competitors.

On the precipitation tasks, the Salient baseline performed strongly and ultimately placed second and fourth respectively for the weeks 3-4 and weeks 5-6 tasks. Our Salient 2.0 model also performs remarkably well, outscoring all contestants and baselines with 12.65% improvement for weeks 3-4. For comparison, the top competitors for weeks 3-4 and weeks 5-6 improved by 11.54% and 8.63% respectively. Our Uniform ABC ensemble outperforms the remaining baselines and state-of-the-art learning methods but falls short of the exceptional Salient performance. In this setting, applying the adaptive online learning ensemble to the union of the ABC models and the state-of-the-art learners (denoted by Online ABC + Learning in Table 8) allows the user to exploit the irregular complementary benefits of the learning methods yielding 12.52% and 8.18% improvements in weeks 3-4 and 5-6.

## 6 Conclusion

In this work, we release Subseasonal ClimateUSA, a dataset for subseasonal forecasting in the U.S. It is routinely updated and can be accessed as a Python package. The dataset includes a variety of features that are relevant at the subseasonal timescale, including precipitation, temperature, surface pressure, relative humidity, geopotential height, sea surface temperature, sea ice concentration, MJO, and MEI. We use this dataset to train and benchmark multifarious models, including deep learning solutions, dynamical models, and simple learned corrections, as well ensembling strategies. Our experiments with temperature and precipitation forecasting in the contiguous U.S. show that simple learning-based corrections to operational dynamical models yield low-cost strategies that are 10% more accurate and 329% more skillful than the U.S. operational CFSv2 and outperform state-of-the-art machine and deep learning methods, as well as the leading ECMWF dynamical model. Overall, we find that the Subseasonal ClimateUSA dataset facilitates both the training and benchmarking of subseasonal forecasting models and hope that it will stimulate new advances in extended range forecasting.

Figure 4: Percentage improvement over mean debiased CFSv2 RMSE in the contiguous U.S. over 2011–2020. White grid points indicate negative or 0% improvement.