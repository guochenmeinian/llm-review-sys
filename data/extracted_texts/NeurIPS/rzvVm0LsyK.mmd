# ADOPT: Modified Adam Can Converge with Any \(\beta_{2}\) with the Optimal Rate

# ADOPT: Modified Adam Can Converge with Any \(_{2}\) with the Optimal Rate

 Shohei Taniguchi

The University of Tokyo

taniguchi@weblab.t.u-tokyo.ac.jp

&Keno Harada

The University of Tokyo

keno.harada@weblab.t.u-tokyo.ac.jp

&Gouki Minegishi

The University of Tokyo

minegishi@weblab.t.u-tokyo.ac.jp

&Yuta Oshima

The University of Tokyo

yuta.oshima@weblab.t.u-tokyo.ac.jp

&Seong Cheol Jeong

The University of Tokyo

jeong@weblab.t.u-tokyo.ac.jp

&Go Nagahara

The University of Tokyo

nagaharago@weblab.t.u-tokyo.ac.jp

&Tomoshi Iiyama

The University of Tokyo

iiyama@weblab.t.u-tokyo.ac.jp

&Masahiro Suzuki

The University of Tokyo

masa@weblab.t.u-tokyo.ac.jp

&Yusuke Iwasawa

The University of Tokyo

iwasawa@weblab.t.u-tokyo.ac.jp

&Yutaka Matsuo

The University of Tokyo

matsuo@weblab.t.u-tokyo.ac.jp

###### Abstract

Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., \(_{2}\), in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of \((1/)\) with any choice of \(_{2}\) without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt.

## 1 Introduction

Stochastic optimization algorithms, such as stochastic gradient descent (SGD), play a central role in deep learning. In particular, adaptive gradient methods based on exponential moving averages, such as Adam [Kingma and Ba, 2014], are widely used in practice. Despite the empirical success, it isknown that Adam does not converge in theory in general cases. For example, Reddi et al. (2018) show that Adam fails to converge to a correct solution in a simple example where the objective function at time \(t\) is given as:

\[f_{t}()=C,&t 3=1\\ -,&,\] (1)

where \(C>2\) and \([-1,1]\). In this online optimization setting, Adam converges to a wrong solution (i.e., \(=1\)) instead of the true solution (i.e., \(=-1\)) especially when the hyperparameter \(_{2}\) is set to a small value. There have been several attempts to fix the non-convergent behavior of Adam (Reddi et al., 2018; Zou et al., 2019). For example, AMSGrad (Reddi et al., 2018) ensures the convergence for online convex optimization by making slight modifications to the Adam algorithm. Subsequent studies (Chen et al., 2019; Zhou et al., 2018) show that AMSGrad also converges to a stationary point for smooth nonconvex stochastic optimization problems. However, the convergence proofs rely on the assumption that the gradient noise is uniformly bounded. This assumption is stronger than the one used for the analysis of vanilla SGD (Ghadimi and Lan, 2013; Bertsekas and Tsitsiklis, 2000; Khaled and Richtarik, 2023), where the gradient _variance_ is assumed to be uniformly bounded. In fact, the bounded noise assumption is often violated in practice. For example, when Gaussian noise is used in the gradient estimation (e.g., variational autoencoders (Kingma and Welling, 2014) and diffusion models (Ho et al., 2020; Song et al., 2021)), the stochastic gradient is no longer bounded.

Concurrently, Zhou et al. (2019) analyze the non-convergence of Adam in the problem described in Eq. (1) from the perspective of the correlation between the current gradient and the second moment estimate based on the exponential moving average. Specifically, they show that the non-convergence problem can be resolved by excluding the gradient of some recent steps from the calculation of the second moment estimate. Based on the analysis, they propose AdaShift, another variant of Adam. However, their theoretical analysis is limited to a single online convex problem described in Eq. (1), and the convergence of AdaShift for general nonconvex problems is unclear.

More recently, some works have demonstrated that Adam can converge by choosing \(_{2}\) in a problem-dependent manner (Shi et al., 2020; Zhang et al., 2022; Wang et al., 2022; Li et al., 2023; Wang et al., 2023). However, tuning \(_{2}\) for each specific problem is troublesome; hence developing algorithms with the problem-independent convergence guarantee is still important to safely apply adaptive gradient methods to a wide range of machine learning problems.

In this paper, we propose an alternative approach to addressing the non-convergence problem of Adam without relying on the choice of \(_{2}\) or strong assumptions such as the bounded noise assumption. To derive our algorithm, we first examine the case without momentum, analyzing the convergence bound of RMSprop for general smooth nonconvex optimization problems. Through the analysis, we uncover the fundamental cause of non-convergence, which stems from the correlation between the second moment estimate and the current gradient. This finding aligns with the results demonstrated by Zhou et al. (2019) for online convex optimization. This correlation can be easily eliminated by excluding the current gradient from the second moment estimate.

Subsequently, we extend our findings to the case where momentum is incorporated, as in Adam, and discover that the Adam-style momentum also contributes to non-convergence. To address it, we propose to change the order of the momentum update and the normalization by the second moment estimate. With this small adjustment, we successfully eliminate the non-convergence problem of Adam without relying on a specific hyperparameter choice and the bounded noise assumption. We provide theoretical evidence demonstrating that our derived algorithm, named ADOPT, can achieve convergence with the optimal rate of \((1/)\) for smooth nonconvex optimization.

In our experiments, we begin by assessing the performance of ADOPT in a toy example where Adam typically fails to converge depending on the choice of \(_{2}\). This toy example is an extension of the one presented in Eq. (1) by Reddi et al. (2018), but we consider a scenario where AMSGrad is also hard to converge due to the dependence on the bounded noise assumption. Our results demonstrate that ADOPT rapidly converges to the solution, while Adam fails to converge, and AMSGrad exhibits extremely slow convergence. Next, we conduct an experiment using a simple multi-layer perceptron on the MNIST classification task to evaluate the performance of ADOPT in nonconvex optimization. Our findings indicate that ADOPT outperforms existing adaptive gradient methods, including Adam, AMSGrad, and AdaShift. Finally, we evaluate the performance of ADOPT in various practical applications, such as image classification of CIFAR-10 and ImageNet using ResNet (He et al., 2016) and SwinTransformer (Liu et al., 2021), training of deep generative models (NVAE), fine-tuning of language models (LLaMA), and deep reinforcement learning for continuous control. Our empirical results demonstrate that ADOPT achieves superior results over existing algorithms (e.g., Adam) in these practical applications.

## 2 Preliminary

### Problem Definition

We consider the minimization of the objective function \(f:^{D}\) with respect to the parameter \(^{D}\). In this context, we focus on first-order stochastic optimization methods, where only the stochastic gradient \(\) is accessible. As the objective \(f\) can be nonconvex, the goal is to find a stationary point where \( f()=0\)(Blair, 1985; Vavasis, 1995). In order to analyze the convergence behavior of stochastic optimization algorithms, the following assumptions are commonly employed in the literature:

**Assumption 2.1**.: _The objective function \(f()\) is lower-bounded, i.e., \(f() f_{}>-\) for all \(\)._

**Assumption 2.2**.: _The stochastic gradient \(_{t}\) is an unbiased estimator of the objective \(f(_{t-1})\), i.e., \([_{t}]= f(_{t-1})\) for all \(t 1\)._

**Assumption 2.3**.: _The objective function is \(L\)-smooth on \(^{D}\), i.e., there exists a constant \(L>0\) such that \(\| f()- f()\| L\|-\|\) for all \(,^{D}\)._

**Assumption 2.4**.: _Variance of the stochastic gradient is uniformly bounded, i.e., there exists a constant \(>0\) such that \([\|_{t}- f(_{t-1})\|^{2}] ^{2}\)._

For the analysis of adaptive gradient methods (e.g., Adam and AdaGrad), many of previous works (Defossez et al., 2022; Li and Orabona, 2019; Ward et al., 2020; Zou et al., 2018) use a little stronger assumption instead of Assumption 2.4 for ease of proofs:

**Assumption 2.5**.: _The stochastic gradient has a finite second moment, i.e., there exists a constant \(G>0\) such that \([\|_{t}\|^{2}] G^{2}\)._

Assumption 2.5 requires that the true gradient \( f\) is also uniformly bounded in addition to the variance of the stochastic gradient \(\). Moreover, the convergence proof of AMSGrad tends to rely on an even stronger assumption as follows (Chen et al., 2019; Zhou et al., 2018).

**Assumption 2.6**.: _The stochastic gradient is uniformly upper-bounded, i.e., there exists a constant \(G>0\) such that \(\|_{t}\| G\)._

In Assumption 2.6, the gradient noise \(_{t}_{t}- f\) is assumed to be bounded almost surely in addition to the true gradient \( f\). Note that when Assumption 2.6 holds, Assumption 2.5 is automatically satisfied; hence, Assumption 2.6 is a stronger assumption compared to Assumption 2.5. In this paper, we adopt Assumptions 2.1, 2.2, 2.3 and 2.5 for analysis, because one of our motivations is to address the omission of Assumption 2.6. In the analysis, we derive the upper bound of \(_{t}\{[\| f(_{t}))\|^{4/3}]^{3/2}\}\) to investigate the convergence rate of the stochastic optimization algorithms, which is commonly performed in the literature (Defossez et al., 2022; Zou et al., 2019).

### Review of Stochastic Optimization Algorithms for Nonconvex Objectives

The convergence of the vanilla SGD have been studied extensively in previous works. For smooth nonconvex functions, Ghadimi and Lan (2013) showed that SGD with a constant learning rate converges with an \((1/)\) rate under Assumptions 2.1-2.4 by setting \(_{t}==(1/)\), where \(_{t}\) is a learning rate at the \(t\)-th step, and \(T\) is a total number of parameter updates. This convergence rate is known to be minimax optimal up to a constant (Drori and Shamir, 2020). For the diminishing learning rate scheme, the convergence bound of \(( T/)\) is well-known for \(_{t}=/\)(Ghadimi and Lan, 2013). Recently, Wang et al. (2021) have proved that SGD with \(_{t}=/\) can also achieve the optimal rate \((1/)\) by additionally assuming that the objective \(f\) is upper-bounded.

While the vanilla SGD is still one of the most popular choices for stochastic optimization, adaptive gradient methods are dominantly used especially for deep learning. In adaptive gradient methods, the parameter \(\) is updated additionally using the second moment estimate \(_{t}\) in the following form:

\[_{t}=_{t-1}-_{t}_{t}}{_{t}+ ^{2}}},\] (2)

where \(\) is a small positive constant. The division between vectors is applied in an element-wise manner, and the addition between a vector \(\) and a scalar \(b\) is defined as \((+b)_{i} a_{i}+b\). In AdaGrad (Duchi et al., 2011), \(_{t}\) is defined as \(_{0}=\) and \(_{t}=_{t-1}+_{t}_{t}\). In RMSprop (Hinton et al., 2012), an exponential moving average is substituted for the simple summation, i.e., \(_{t}=_{2}_{t-1}+(1-_{2})_{t}_{t}\), where \(0_{2}<1\). Adam (Kingma and Ba, 2014) uses momentum in addition to the second moment estimate to accelerate the convergence as follows:

\[_{t}=_{1}_{t-1}+(1-_{1})\,_{t},\] (3) \[_{t}=_{t-1}-_{t}_{t}}{ _{t}+^{2}}},\] (4)

where \(_{0}=\). Here, we omit the bias correction technique used in the original paper for clarity. Unfortunately, RMSprop and Adam are not guaranteed to converge even in a simple convex optimization problem as demonstrated by Reddi et al. (2018), whereas AdaGrad with a constant learning rate is known to converge with an \(( T/)\) rate under Assuppmtions 2.1-2.3 and 2.5 for smooth nonconvex cases (Li and Orabona, 2019; Ward et al., 2020; Zou et al., 2018; Chen et al., 2019; Defossez et al., 2022). Although the convergence of Adam can be assured by choosing \(_{2}\) in a problem-dependent manner (Shi et al., 2020; Zhang et al., 2022; Wang et al., 2022; Li et al., 2023; Wang et al., 2023), it is difficult to know the proper choice of \(_{2}\) for each problem before training.

To fix the non-convergence of Adam without depending on \(_{2}\), some researchers have proposed variants of Adam. Reddi et al. (2018) proposed AMSGrad, which substitute \(}_{t}\) for \(\) in Eq. (3), where \(}_{0}=\) and \(}_{t}=\{}_{t-1},_{t}\}\). The idea behind AMSGrad is that the scaling factor \(}_{t}+^{2}}\) should be non-decreasing to ensure the convergence. After Reddi et al. (2018) originally proved the convergence of AMSGrad for online convex optimization, Chen et al. (2019) showed that AMSGrad with \(_{t}=/\) converges with \(( T/)\) for nonconvex settings. Zhou et al. (2018) also analyzed the convergence of AMSGrad for nonconvex optimization, and derived the convergence rate of \((1/)\) for a constant learning rate of \(_{t}==(1/)\). However, their results depend on Assumption 2.6, which is often violated in practice. For example, variational autoencoders (Kingma and Welling, 2014) and diffusion models (Ho et al., 2020; Song et al., 2021) are typical examples in which Assumption 2.6 does not hold because they utilize unbounded Gaussian noise in the gradient estimation. The cause of requirement for Assumption 2.6 is the max operation in the definition of \(}_{t}\). Since the max operation is convex, \([}_{t}]_{t}\{[_{t}]\}\) does not hold; hence Assumption 2.6 is required to upper-bound \([}_{t}]\) in their proofs.

Zhou et al. (2019) also tried to fix the non-convergent behavior of Adam. Their proposed AdaShift uses \(_{t-n}\) instead of \(_{t}\) for the second moment estimate, and calculate the momentum using the latest \(n\) gradients as follows:

\[_{t}=^{n-1}_{1}^{k}_{t-k}}{_ {k=0}^{n-1}_{1}^{k}},\] (5) \[_{t}=_{t-1}-_{t}_{t}}{ _{t-n}+^{2}}}.\] (6)

In the original paper, some additional techniques (e.g., the block-wise adaptive learning rate) are used, but we omit them for clarity here. Though they give theoretical analysis for a single online convex example, any convergence bounds are not provided for nonconvex cases. More detailed discussion on existing analyses is provided in Appendix A.

## 3 Analysis: Cause of Non-convergence of Adam and How to Fix It

In this section, to derive an algorithm that can converge with any \(_{2}\) without Assumption 2.6, we analyze the cause of non-convergence of Adam, and discuss how it can be eliminated. To start from a simple case, we first analyze the case without momentum. Subsequently, we extend it to the case with momentum and provide a way to fix the convergence issue of Adam.

### Case without Momentum

We first analyze the convergence of RMSprop, which corresponds to the no-momentum case of Adam when we omit the bias correction. For RMSprop, we derive the following convergence bound.

**Theorem 3.1**.: _Under Assumptions 2.1-2.3 and 2.5, the following holds for the RMSprop with a constant learning rate \(_{t}=\):_

\[_{t=1,,T}\{[\| f(_{t-1}) \|]^{4/3}]^{3/2}\} C_{1}(-f_{} }{ T}+}{T}(1+}{^{2}})-C_{2 }_{2}),\] (7)

_where \(C_{1}=2+^{2}}\), \(C_{2}=)}+}}\), and \(f_{0}=f(_{0})\)._

Sketch of proof.: By Assumption 2.3, the following holds:

\[[f(_{t})][f (_{t-1})+L}{2}\|_{t}}{ _{t}+^{2}}}\|^{2}- f(_{ t-1})^{}(_{t}}{_{t}+^{2}}} )]\] (8)

Applying Lemmas G.4 and G.6 in the appendix to this, the following inequality is derived:

\[[f(_{t})]\] (9) \[[f(_{t-1})+(L}{2}+2 G})\|_{t}}{ _{t}+^{2}}}\|^{2}]-[\| f(_{t-1})\|^{4/3} ]^{3/2}}{^{T})G^{2}+^{2}}},\] (10)

where \(}_{t}=_{2}_{t-1}+(1-_{2})[_{t} _{t}]\). Telescoping this for \(t=1,,T\) and rearranging the terms, we have

\[_{t=1}^{T}[\| f(_{t-1}) \|^{4/3}]^{3/2} C_{1}(_{0} )-f_{}}{}+C_{2}(+^{2}}{_{2}^{ T}^{2}})),\] (11)

where the last inequality holds due to Assumption 2.1 and Lemma G.5. Therefore, the bound in Eq. (7) is derived using \(_{t=1,,T}\{[\| f(_{t-1}))\|^{4/3}\}^{3/2} _{t=1}^{T}[\| f(_{t-1})\|^{4/3} ]^{3/2}/T\). 

A detailed proof is provided in the appendix. When the learning rate \(\) is chosen so that \(=(1/)\), the first and second terms on the right hand side of Eq. (7) converge with \((1/)\) and \((1/T)\) rates, respectively. However, the last term includes a constant factor in terms of \(T\), which represents the non-convergent behavior of RMSprop in the smooth nonconvex setting. More precisely, RMSprop is guaranteed to converge only to a bounded region around a stationary point, and the size of the bounded region depends on the hyperparameter \(_{2}\) and the problem-dependent factors \(D\), \(G\), and \(L\). Therefore, we need to choose \(_{2}\) dependently on each problem to make the bounded region adequately small. Since \(_{_{2} 1}_{2}/}=0\), the size of the bounded region can be made small by setting \(_{2}\) to a value close to 1, which aligns with practical observations. However, how close to \(1\) it should be relies on the problem-dependent factors, which cannot be observed in advance. This result is consistent with recent results of convergence analyses of Adam and RMSprop (Shi et al., 2020; Zhang et al., 2022).

As can be seen from Eqs. (8) and (9), the constant term in Eq. (7) is derived from the last term of Eq. (8). Because \(_{t}\) and \(_{t}\) are not statistically independent, this term is first decomposed as in Eq. (9). After the decomposition, \(_{t}\) and \(}_{t}\) is now conditionally independent given \(_{0},,_{t-1}\), so Eq. (10) is derived using the following fact:

\[[_{t}}{}_{t}+^{2}}} ]=[_{t-1})}{}_{t}+^{2}}}].\] (12)

This indicates that, if the second moment estimate \(_{t}\) is designed to be conditionally independent to \(_{t}\), the constant term in the convergence bound will be removed, because the second term of Eq. (8)can be directly lower-bounded without the decomposition. A simple way to achieve the conditional independence is to substitute \(_{t-1}\) for \(_{t}\) as a second moment estimate, because \(_{t-1}\) does not have information about \(_{t}\). This solution is similar to AdaShift, in which \(_{t-n}\) is substituted for \(_{t}\) as described in Eq. (5). In fact, the modified version of RMSprop is identical to AdaShift with \(n=1\) and \(_{1}=0\) except for the additional techniques (e.g., the block-wise adaptive learning rate).

### Case with Momentum

As we have described, RMSprop can be modified to be convergent by removing the current gradient \(_{t}\) from the second moment estimate \(_{t}\). However, when we combine adaptive gradient methods with momentum like Adam, the convergence analysis becomes more complicated. Unfortunately, when Adam-style momentum in Eq. (3) is applied, the algorithm does not converge in general even when using \(_{t-1}\) as a second moment estimate instead of \(_{t}\). This is because the momentum \(_{t}\) contains all history of the past gradients \(_{0},,_{t}\); hence the second moment estimate always correlates with \(_{t}\). AdaShift prevents this problem by calculating the momentum \(_{t}\) only using the latest \(n\) gradients as described in Eq. (5). In that case, the momentum \(_{t}\) and the second moment estimate \(_{t-n}\) are conditionally independent, so the convergence can be retained. However, this approach has a trade-off in the choice of \(n\). When \(n\) is small, \(_{t}\) has little information about the past gradients; when \(n\) is large, \(_{t-n}\) only has access to the gradient information in the distant past.

To remove this trade-off, instead of truncating the momentum to the latest \(n\) steps, we propose to use momentum of the following form:

\[_{t}=_{1}_{t-1}+(1-_{1})_{t}}{_{t-1}+^{2}}},\] (13) \[_{t}=_{t-1}-_{t}_{t}.\] (14)

The main difference to the Adam-style momentum in Eq. (3) is the order of update of \(_{t}\) and the normalization by \(_{t-1}+^{2}}\). In Eq. (3), the normalization is performed after the update of \(_{t}\), whereas in Eq. (13), the normalization is first applied to the current gradient \(_{t}\) in advance to the update of \(_{t}\). In this case, the second moment estimate \(_{t-1}\) is only used to normalize the current gradient \(_{t}\), so the convergence can be guaranteed. A more detailed convergence analysis is provided in Section 4.

## 4 Method: Adaptive Gradient Method with the Optimal Convergence Rate

Based on the analysis in the previous section, we propose a new adaptive gradient method named ADOPT (_ADaptive gradient method with the OPTimal convergence rate_). The entire procedure is summarized in Algorithm 4. For a simple discription, we place the update of \(\) after the parameter update in Algorithm 4, but it is equivalent to Eqs. (13) and (14) except that \(\{},\}\) is substituted for \(+^{2}}\). The substition is applied because we find that it contributes to slightly better performance in practice. We provide an equivalent expression of Algorithm 4 in Algorithm C in the appendix, which is closer to a practical implementation. By this modification, ADOPT can converge with the optimal rate for smooth nonconvex optimization as follows:

**Theorem 4.1**.: _Under Assumptions 2.1-2.3 and 2.5, the following holds for the ADOPT algorithm with a constant learning rate \(_{t}==(1/)\):_

\[_{t=1,,T}\{[\| f(_{t-1}))\|^{4/3 }]^{3/2}\}(1/),\] (15)

The detailed proof and related lemmas are provided in the appendix. We also provide the convergence bound for the case of diminishing learning rate (i.e., \(_{t}=/\)) in the appendix, which is closer to practical situations. In that case, ADOPT also converges with the optimal rate.

## 5 Experiments

In the experiments, we first validate our ADOPT algorithm using a simple toy example in which Adam is known to fail to converge, and confirm our theoretical findings through numerical simulation. Secondly, we run an experiment of training a simple multi-layer perceptron (MLP) for the MNIST dataset to verify the effectiveness of our ADOPT for nonconvex optimization problems. Finally, we evaluate our ADOPT in a wide range of practical applications, including image classification, natural language processing (NLP) tasks, generative modeling, and deep reinforcement learning. Detailed experimental settings are described in the appendix.

**Toy problem:** We consider a convex optimization problem with an objective \(f()=\) for \([-1,1]\). It is obvious that a solution for the problem is \(=-1\). Through the optimization, we only have access to the stochastic objective \(f_{t}\) as follows:

\[f_{t}()=k^{2},&1/k\\ -k,&1-1/k,\] (16)

where \(k 1\). Because \([f_{t}()]=f()\) holds, the stochastic gradient \(g_{t}= f_{t}()\) is an unbiased estimator of the true gradient \( f\) regardless of the choice of \(k\), satisfying Assumption 2.2. This problem is equivalent, except for scaling, to the stochastic optimization version of Eq. (1) provided by Reddi et al. (2018) as a case where Adam fails to converge. In this setting, the constant \(k\) controls the magnitude of gradient noise. When \(k=1\), it corresponds to the noiseless case where \(f_{t}=f\) with probability \(1\). As \(k\) gets large, stochastic gradient becomes noisy, making \(G\) in Assumptions

Figure 1: Performance comparison between Adam, AMSGrad and ADOPT in a simple univariate convex optimization problem. The plots show transitions of the parameter value, which should converge to the solution \(=-1\).

2.5 and 2.6 large. Therefore, the optimization will be more difficult when \(k\) becomes larger. In the experiment, we set \(k=10\) or \(50\), and compare the robustness of Adam, AMSGrad, and ADOPT for various hyperparameter settings by changing \(_{2}\) from \(0.1 0.999\). We set \(_{1}=0.9\) for all the algorithms, which is a common choice in practice. We set the learning rate to \(_{t}=0.01/\).

The result is shown in Figure 1. It can be seen that, when \(k=10\), Adam fails to converge except for \(_{2}=0.999\) while AMSGrad and ADOPT rapidly converge to the correct solution, i.e., \(=-1\), with any \(_{2}\). In a more extreme case where \(k=50\), Adam fails to converge even with \(_{2}=0.999\). This aligns with Theorem 3.1, since, when the gradient noise is large (i.e., \(G\) is large), the bounded region of the convergence bound also gets large, leading to divergence of Adam. Moreover, when \(k=50\), it is observed that the convergence of AMSGrad also becomes much slower than ADOPT. In fact, this phenomenon is also consistent with theory. In this problem setting, the second moment \([g_{t}^{2}]\) is \((k^{3})\), while the squared norm of the stochastic gradient \(g_{t}^{2}\) is \((k^{4})\). Since the convergence bound of AMSGrad depends on the uniform bound of the stochastic gradient in Assumption 2.6, instead of the second moment in Assumption 2.5, its convergence also deteriorates with the order of \(g_{t}^{2}\). Compared to AMSGrad, ADOPT only depends on the second moment bound for its convergence, so it converges much faster than AMSGrad even in such an extreme setting.

We also perform ablation study on how the two algorithmic changes from Adam to ADOPT affect the convergence. The differences between Adam and ADOPT are (1) decorrelation between the second moment estimate and the current gradient, and (2) change of order of momentum update and normalization by the second moment estimate. In this experiment, we remove each algorithmic change from ADOPT, and compare the result in the toy example. We set \(k=50\), and \((_{1},_{2})=(0.9,0.999)\), since it is a common hyperparameter choice. The result is shown in Figure 3. It can be observed that ADOPT fails to converge with the exception of either algorithmic change. Therefore, applying both changes is essential to overcome the non-convergence of Adam, which also aligns with theory. These results correspond to the theoretical findings, showing the superiority of ADOPT to Adam and AMSGrad in terms of the convergence speed and its robustness to hyperparameter choices.

**MNIST classification:** To investigate the effectiveness of ADOPT on nonconvex optimization, we train nonlinear neural networks for MNIST classification tasks, and compare the performance between ADOPT and existing optimization algorithms, such as Adam, AMSGrad and AdaShift. In this experiment, we use a simple MLP with a single hidden layer, and the number of hidden units is set to 784. We set the learning rate to \(_{t}=/\), and \(\) is tuned in the range of \(\{1,10^{-1},10^{-2},10^{-3}\}\). We apply weight decay of \(1 10^{-4}\) to prevent over-fitting, and run 10K iterations of parameter updates. Figure 2 shows the learning curves of training and test accuracy. We observe our ADOPT performs slightly better than the others in terms of the convergence speed and the final performance.

**Image classification:** As a more practical application, we conduct experiments of image classification using real-world image datasets. We first compare ADOPT and Adam in the classification task of the CIFAR-10 dataset using ResNet-18 [He et al., 2016], a widely-used convolutional neural network. We conduct a similar hyperparameter search to the case of MNIST classification. A detailed experimental setting is provided in the appendix. The learning curves of test accuracy are visualized in Figure 4. It can be observed that ADOPT converges a little faster than Adam.

Figure 2: Accuracy for training data (left) and test data(right) in MNIST classification. The error bars show the 95% confidence intervals of three trials.

To confirm that our ADOPT works well for modern neural network architectures based on Transformers (Vaswani et al., 2017), we perform an experiment of ImageNet classification using SwinTransformer (Liu et al., 2021). We follow the official training recipe of Swin Transformer-tiny provided by Torchvision (Paszke et al., 2019), and fix the training settings except for the optimizer choice. We use AdamW (Loshchilov and Hutter, 2019) as a baseline because it is set as the default official optimizer. We also compare with AMSGrad as another way to fix the non-convergence issue of Adam. Since AdamW uses decoupled weight decay, we also apply it to the other optimizers for fair comparison. We report the top-1 accuracy at \(200\) and \(300\) epochs in Tables 1. We observe that ADOPT outperforms AdamW and AMSGrad throughout the training in terms of the test accuracy, demonstrating the effectiveness of ADOPT for this setting.

**Generative modeling:** We train NVAE (Vahdat and Kautz, 2020) for MNIST using our ADOPT. In the official implementation of NVAE, Adamax (Kingma and Ba, 2014), an infinite-norm variant of Adam, is used as an optimizer, so we use Adamax as a baseline method. We use the exactly the same setting of the official implementation except that the learning rate for ADOPT is set to \(2 10^{-4}\) since the default value \(0.01\) is too large for ADOPT. We report the negative log-likelihood for test data on Table 2. It is observed that the model trained with ADOPT shows the better likelihood.

**Pretraining of large language models:** We run a pre-training of GPT-2 (Radford et al., 2019) using the nanoGPT (Karpathy, 2022) code base to compare Adam and ADOPT. We use OpenWebText (Gokaslan and Cohen, 2019) as the training data. Experimental setup conforms to the default settings of nanoGPT except for the selection of the optimizer. We also test a case in which the total batch size was changed from 480 to 96, as a setting where the gradient noise becomes larger. The results are summarized in Figure 5. The most notable finding is that in the small batch size case, Adam causes loss spikes in the early stages of training and fails to converge, while ADOPT is always able to train stably. This is consistent with Adam's theory of non-convergence. As the gradient noise increases, \(G\) in Theorem 3.1 also increases, and the constant term in Adam's convergence bounds becomes non-negligible especially when using a large-scale dataset like OpenWebText. As a result, Adam is more likely to fail to train in such cases. Our ADOPT, on the other hand, does not suffer from this problem because it can always guarantee convergence. We also observed that both Adam and ADOPT work well when the batch size is large, but even in this case, ADOPT performs slightly better.

   Epoch & \(200\) & \(300\) \\  AdamW & \(79.29 0.05\) & \(81.26 0.04\) \\ AMSGrad & \(78.91 0.03\) & \(81.17 0.03\) \\ ADOPT & \( 0.03\) & \( 0.04\) \\   

Table 1: Top-1 accuracy (%) for ImageNet classification by SwinTransformer.

   Epoch & \(200\) & \(300\) \\  AdamW & \(79.29 0.05\) & \(81.26 0.04\) \\ AMSGrad & \(78.91 0.03\) & \(81.17 0.03\) \\ ADOPT & \( 0.03\) & \( 0.04\) \\   

Table 2: Negative log-likelihood of NVAEs for MNIST density estimation. Lower is better.

Figure 4: Learning curves of test accuracy for CIFAR-10 classification by ResNet-18 trained with Adam and ADOPT.

Figure 3: Ablation study of algorithmic changes between Adam and ADOPT. ”DE” and CO denote ”decorrelation” and ”change of order”, respectively.

**Finetuning of large language models:** We finetune the pretrained LLaMA-7B on 52K instruction-following data provided by Stanford Alpaca and compare the performance between the default optimizer (Adam) and our ADOPT under the exactly same experimental setting. For evaluation, we use Multi-task Language Understanding (MMLU) Benchmark (Hendrycks et al., 2021), which is widely used to assess the performance of large language models. The MMLU score for LLaMA-7B without finetuning is \(35.1\). After fine-tuned via instruction-following using the baseline implementation with Adam, the score improves to \(41.2\). When we substitute ADOPT for Adam, the score even improves to \(42.13\). The detailed score comparison for each task is summarized in Figure 7 in the appendix. Other experimental results, including deep RL experiments, and detailed experimental settings are also provided in the appendix.

## 6 Conclusion

In this paper, we demystified the fundamental cause of divergence of adaptive gradient methods based on the exponential moving average, such as Adam and RMSprop, in general smooth nonconvex optimization problems, and demonstrate a way to fix the issue, proposing a new optimizer named ADOPT. Not only does ADOPT converge with the optimal rate without depending on a hyperparameter choice in theory, but ADOPT demonstrates better performance in a wide range of pracital applications.

We expect that this work will serve as a bridge between theory and practice in the research of adaptive gradient methods. Since ADOPT can be safely applied to many machine learning problems without careful tuning of hyperparameters, it can be expected to improve the training stability and the model performance in practice by substituting it for the existing adaptive gradient methods (e.g., Adam).

One of the limitations of our analysis is that it still relies on the assumption that the second moment of stochastic gradient is uniformly bounded (i.e., Assumption 2.5). Although this assumption is weaker than the bounded stochastic gradient assumption (i.e., Assumption 2.6), it would be more desirable to relax it to the bounded _variance_ assumption (i.e., Assumption 2.4), which is often adopted in the analysis of the vanilla SGD (Ghadimi and Lan, 2013). For Adam, a recent work by Wang et al. (2023) have derived a problem-dependent convergence bound which achieves the \((1/)\) rate without Assumption 2.5. Their proof techniques may help to relax our assumptions in the proof of Theorem 4.1, which we leave as future work.

From a broader perspective, adaptive gradient methods like Adam have been widely used even for the training of large-scale foundation models (e.g., large language models). Although such models can be useful for people, their negative aspects, such as concerns about copyright infringement, are not negligible. Researchers needs to deeply recognize and understand such social impacts of machine learning algorithms.

Figure 5: Learning curves of GPT-2 pretraining for training set (left) and validation set (right).