# Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection

Geng Yu\({}^{1}\)   Jianing Zhu\({}^{2}\)   Jiangchao Yao\({}^{1,3}\)1  Bo Han\({}^{2,4}\)

\({}^{1}\)CMIC, Shanghai Jiao Tong University  \({}^{2}\)TMLR Group, Hong Kong Baptist University

\({}^{3}\)Shanghai A I Laboratory  \({}^{4}\)RIKEN Center for Advanced Intelligence Project

{warriors30, Sunarker}@sjtu.edu.cn

{csjnzhu, bhanml}@comp.hkbu.edu.hk

###### Abstract

Out-of-distribution (OOD) detection is crucial for deploying reliable machine learning models in open-world applications. Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features extracted from ID data. However, the irrelevant context mined from ID data can be spurious due to the inaccurate foreground-background decomposition, thus limiting the OOD detection performance. In this work, we propose a novel framework, namely, _Self-Calibrated Tuning (SCT)_, to mitigate this problem for effective OOD detection with only the given few-shot ID data. Specifically, SCT introduces modulating factors respectively on the two components of the original learning objective. It adaptively directs the optimization process between the two tasks during training on data with different prediction uncertainty to calibrate the influence of OOD regularization, which is compatible with many prompt tuning based OOD detection methods. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed SCT. The code is publicly available at: https://github.com/tmlr-group/SCT.

## 1 Introduction

The deep neural networks (DNNs) are demonstrated to be overconfident on the OOD data out of the pre-defined label space , which can induce severe problems in those safety-critical applications like autonomous driving or medical intelligence. Various explorations  thus have been conducted in designing scoring functions or fine-tuning methods with auxiliary outliers to improve the OOD distinguishability. Specially, with the emergence of the powerful pretrained vision-language models (VLMs) , a series of prompt tuning based methods  show impressive performance in current OOD detection benchmarks, with the regularization given only few-shot in-distribution (ID) data.

Generally, these regularizations  are built upon the ID-irrelevant local context as the surrogate OOD source, which is extracted by VLMs (refer to Figure 1) based on its alignment with ID-class text features. Although this saves the costly collection of auxiliary outliers from the open world, the quality of the ID-irrelevant local context also becomes the bottleneck, which can be greatly affected by the foreground-background decomposition with VLMs. Specifically, as revealed in previous studies , the prevalent VLMs struggle with poor calibration, which means that the decomposition performance on downstream data might not be well guaranteed. Thus, it naturally motivates the following question:_Can we flexibly leverage the imperfect OOD features extracted by the VLM itself, to facilitate the few-shot prompt tuning for effective OOD detection?_

As illustrated in Figure 1, a significant portion of extracted local context from ID data are not valid OOD features due to the inevitable imperfect decomposition. Consequently, OOD regularization based on such unreliable OOD features may potentially constrain the improvement of OOD detection. To investigate this problem, we conduct a proof-of-concept experiment on CLIP with ImageNet as the ID dataset and prompt-tune the model with different groups of data divided by their overall prediction uncertainty. Specifically, we find that the performance of prompt tuning based methods significantly deteriorates as the uncertainty of the given ID data rises, as presented in Figure 2, which motivates us to leverage such clues to overcome the current issue. Intuitively, as the model prediction on ID samples is less certain, the OOD features extracted from these data are less reliable. Performing OOD regularization on such unreliable surrogate OOD features can degrade the OOD detection performance of CLIP. Therefore, a potential idea is to adaptively adjust the importance of OOD features extracted from ID data according to their prediction uncertainty during model training.

Based on the previous observation, we propose a new learning framework, i.e., _Self-Calibrated Tuning_ (SCT), to alleviate the problem induced by spurious OOD features. At the high level, we aim to dynamically adjust the weight of OOD regularization from different training samples based on their prediction uncertainty to calibrate their influence on model training. In detail, we introduce modulating factors based on the sample uncertainty estimation respectively on the two parts of the original learning objective of prompt tuning for OOD detection (refer to Eq (4)). Under this new learning framework, the model's attention is directed towards the classification task to better generalize to the downstream ID dataset when training with low-confidence data. OOD features extracted from high-confidence ID data are attached more importance to achieve more effective OOD regularization. The redirection effect of these two modulating factors facilitates VLMs learning from imperfect OOD features to ultimately improve the OOD detection of prompt tuning. Our main contributions can be summarized as follows,

* Conceptually, we investigate the problem of imperfect OOD features extracted in prompt tuning based OOD detection methods and observe that ID data with different prediction uncertainty exhibits a distinctive influence on the OOD regularization. (in Section 3.2)
* Technically, we propose a novel learning framework, namely _Self-Calibrated Tuning_ (SCT), for facilitating prompt tuning for effective OOD detection given only few-shot ID samples, which conducts adaptive redirection of model optimization process between the two tasks to calibrate the influence of OOD features mined from different ID data. (in Sections 3.3)
* Empirically, extensive experiments from different perspectives are conducted to verify the effectiveness of SCT in improving OOD detection performance. To be specific, SCT improves the false positive rate (FPR95) by 3% compared to the previous best method on the

Figure 1: Imperfect foreground background decomposition. The top row shows the original images from ImageNet-1k and the bottom row shows the ID-irrelevant context extracted from the original images (shown as the colored patches of images on the second row), using CLIP fine-tuned with CoOp on 16-shot data. Due to the imperfect decomposition of fine-tuned vision-language models, large portions of the extracted local features from ID data belong to ID-related regions, thus harming the performance of OOD detection. More illustrations are presented in the Appendix A.3.3.

large-scale ImageNet-1k (Deng et al., 2009) benchmark. Furthermore, we perform various ablations and further discussions to provide a thorough understanding. (in Section 4)

## 2 Related works

**Prompt tuning for VLMs.** The concept of prompt tuning was originally applied in the field of natural language processing (Radford et al., 2018). To eliminate the need for manual prompt crafting, prompt tuning exploits supervision signals from downstream tasks to automate the process of prompt generation. Autoprompt (Shin et al., 2020) searches for tokens that cause the greatest changes in gradients based on the label likelihood. Prefix-Tuning (Li and Liang, 2021) introduces a sequence of continuous vectors that can be end-to-end optimized in the token embedding space. Incorporating prompt tuning into computer vision. CoOp (Zhou et al., 2022) adapts pretrained vision-language models by optimizing a set of learnable continuous prompt vectors. Various prompt tuning methods (Zhou et al., 2022, 2022; Schattak et al., 2023; Sun et al., 2022) have been subsequently proposed to address different vision tasks. However, since these methods are not developed for OOD detection, they face challenges in identifying unknown OOD samples encountered at inference stages.

**Out-of-distribution detection with VLMs.** Large pretrained vision-language models have enriched the landscape of OOD detection through their remarkable generalization capability in both visual and textual domains. MCM (Ming et al., 2022) employs the concept of maximum softmax probability (Hendrycks and Gimpel, 2017) into the inference process of CLIP for OOD detection, while CLIPN (Wang et al., 2023) trains an additional text encoder using and set of prompts with a large external dataset to improve its negative semantic understanding. Compared with zero-shot methods, prompt tuning based approaches achieve better OOD detection with access to few-shot ID training data. LoCoOp (Miyai et al., 2024) adopts prompt tuning and extracts ID-irrelevant background from CLIP's local features as surrogate OOD data to regularize the learned prompts. (Bai et al., 2023) discovers ID-like outliers from ID samples via random cropping to learn a set of negative prompts. However, these two representative prompt learning-based methods suffer from spurious OOD features extracted from ID data due to the imperfect foreground-background decomposition of VLMs. In addition, LSN (Nie et al., 2023) introduce negative prompts to empower VLMs to learn negative semantics from ID samples while NegPrompt (Li et al., 2024) leverages negative prompts to investigate the novel setting of open-vocabulary OOD detection.

## 3 Method

In this section, we introduce our new framework, i.e., _Self-Calibrated Tuning_ (SCT), which conducts adaptive redirection of model learning far away from OOD data region during prompt tuning with only the few-shot ID data. Firstly, we provide preliminaries and notations about prompt tuning based OOD detection (Section 3.1). Secondly, we present and discuss the critical motivation that inspires our method (Section 3.2). Thirdly, we introduce its newly derived learning objective with the explanation and analysis of the underlying intuition and present its algorithmic realization (Section 3.3).

### Preliminaries

VLM-based OOD detection aims to identify test samples that do not belong to any ID class designated by the downstream tasks (Miyai et al., 2024). Therefore, the ID distribution is defined by the ID classes from the downstream tasks, which are different from those of the upstream pretraining. Formally, we consider multi-class classification as the original training task (Nguyen et al., 2015), where \(^{d}\) denotes the input space and \(=\{1,,M\}\) denotes the label space. A reliable classifier should be able to detect the OOD input, which can be considered a binary classification problem. We consider \(_{}\) as the distribution of ID data over pairs of examples \(^{d}\) and corresponding labels \(y\). At test time, the environment can present a distribution \(_{}\) over \(\) of OOD data. In general, the OOD distribution \(_{}\) is defined as an irrelevant distribution of which the label set has no intersection with \(\)(Zhu et al., 2023) and thus should not be predicted by the model parameterized by \(\). A decision model \(()\) can be made with the threshold \(\):

\[_{}(;)=&S(;)\\ &S(;)<^{}\] (1)

**Vanilla prompt tuning**. Given an ID image \(\) and its corresponding label \(y\), a global visual feature \(=f()\) is obtained by the visual encoder \(f\) of CLIP. Then, the textual prompt vectors can be formulated as \(_{m}=\{_{1},_{2},...,_{N},_{m}\}\), where \(_{m}\) denotes the word embedding of the ID class name and \(=\{_{n}|_{n=1}^{N}\}\) are \(N\) learnable context vectors, each of which has the same dimension as the word embedding. The text encoder \(g\) takes prompt \(_{m}\) as the input and outputs the textual feature as \(_{m}=g(_{m})\). The final prediction probability of the CLIP model is computed as follows:

\[p(y=m;)=,_{m}) /)}{_{m^{}=1}^{M}((,_{m^{ }})/)},\] (2)

where \((,)\) denotes cosine similarity, and \(\) represents the temperature of Softmax. We use \((;)\) to represent the probability vector \([p(y=1|;),p(y=2|;),,p(y=M|;)]\), denoting the prediction probability of ID image \(\) for every ID class.

**Prompt tuning for OOD detection**. Compared with vanilla prompt tuning methods like CoOp (Zhou et al., 2022), advanced prompt tuning based OOD detection methods extract surrogate OOD features from ID data via various methods to perform OOD regularization. LoCoOp (Miyai et al., 2024) can further improve the detection performance by regularizing the learnable prompts \(\) on OOD features \(}\) extracted from ID local features using a ranking-based method, and its corresponding learning objective is defined as follows,

\[_{}=_{(,y)_{m}}[ _{}(p(y|;),y)+_{}(( };))],\] (3)

where \(\) is the balancing parameter, \(_{}()\) is the Cross-Entropy (CE) loss, \((})\) is the prediction probability vector for \(}\) and \(_{}()\) is the negative entropy of the given probability vector. IDLike (Bai et al., 2023) conducts multiple random cropping on ID data and chooses cropped regions as OOD features based on their feature similarity with textual features of ID classes.

### Motivation

Given the only ID data, previous studies propose to extract the ID-irrelevant local context as the surrogate OOD source, which depends on the foreground-background decomposition using CLIP. However, since the model itself has uncertainty on the prediction results, the correctness of the decomposition cannot always be guaranteed. Thus, as illustrated in Figure 1, the extracted local regions based on the prediction of CLIP may result in spurious OOD features, which then limits the OOD detection performance. Thus, it naturally motivates the following critical research question:

_How could we better utilize the surrogate OOD features extracted by imperfect foreground-background decomposition of CLIP for effective OOD regularization?_

Figure 2: Empirical demonstration about invalid OOD features extracted from ID data in LoCoOp and the influence of sample uncertainty on OOD detection performance. In the left and middle panels, we illustrate the extracted OOD features at different levels of uncertainty and find that they become unreliable as the uncertainty increases. The numbers at the bottom denote the prediction probability for the ground-truth labels from fine-tuned CLIP. In the right panel, we collect ID samples of different uncertainty levels based on prompt-tuned CLIP and divide them into 2 groups. The result demonstrates that the OOD detection performance of LoCoOp significantly degrades as the uncertainty level of ID data rises. We leave the experimental details in Appendix A.3.1 for reference.

In this work, we conduct a proof-of-concept experiment to investigate the relationship between sample uncertainty and OOD detection performance of prompt tuning based methods. We use ViT-B/16 CLIP as the model and large-scale ImageNet-1k OOD benchmarks. First, we observe that the quality of OOD features extracted by CLIP is highly correlated with the uncertainty estimation of ID data. The extracted OOD features become more spurious as the uncertainty escalates, as illustrated in the left and middle panel of Figure 2. Furthermore, as empirically shown in the right panel of Figure 2, the performance of prompt tuning based methods is heavily affected by the overall uncertainty level of the given ID data. Intuitively, as the predictions of the model on ID samples are less confident, the OOD features extracted from these data are less reliable. Regularizing on such invalid OOD features can undermine the calibration ability and OOD detection performance of CLIP, which can be reflected by the higher FPR95 score (indicating a higher error on OOD detection). Therefore, a new mechanism is required to take the sample uncertainty into consideration to assist the model in learning from these imperfect OOD features for more effective OOD detection.

### Self-calibrated tuning

As aforementioned, the LoCoOp-based OOD detection paradigm relies on the extracted ID-background local features for OOD regularization. Given the imperfect foreground-background decomposition, the model is expected to effectively learn from the inaccurate OOD features for better OOD detection. Inspired by the previous observation as shown in Section 3.2, one conceptual idea to mitigate this problem is to adaptively adjust the importance of OOD regularization generated from different ID samples based on uncertainty estimation to alleviate the wrong guidance of invalid OOD features. Under this learning paradigm, the model can be regularized by more valid OOD features and simultaneously prevent itself from overconfidence to improve OOD detection. To this intuition, we consider reformulating the learning objective under the framework of prompt tuning as follows,

\[_{}=_{(,y)_{}} [_{}(p(y|;),y)*(p(y|;)) +_{}(();)*(p(y|;))],\] (4)

where \(:^{M}\) and \(:^{M}\) indicate the newly-introduced modulating functions that calculate adaptive factors for the two components of the original loss function (i.e., Eq (3)) of LoCoOp based on the uncertainty estimation. In this loss function, the left part is for the ID classification task, and the right part is for the OOD regularization. Specifically, \(\) should be monotonically decreasing and \(\) should be monotonically increasing with respect to \(p(y|;)\) so that the modulating factors shift the focus of prompt learning between the two tasks during the training process.

In detail, when the model outputs low-confidence prediction for the ground-truth label, the importance of the ID classification task is highlighted in order to better generalize to the downstream task and simultaneously reduce the effect of regularization from invalid OOD features extracted from ID data. When the model can accurately and confidently classify the ID samples, its attention is redirected towards OOD regularization to strengthen the positive effect of useful ID-irrelevant features for better OOD detection. In the meantime, the loss contribution of the classification task is reduced to avoid the model overfitting to the downstream dataset, which benefits the calibration of the model [Mukhoti et al., 2020] and further enhances the validity of extracted OOD features.

Under this learning framework, the goal of confidence calibration and OOD detection can benefit from each other. This method calibrates the influence of OOD features mined from different ID data based on model prediction confidence during the training process to facilitate capturing more reliable OOD features from ID data. Among a wide range of functions that satisfy the simple requirements as discussed above, we choose the linear function due to its simple design. Concretely, we formulate the loss function of SCT as follows,

\[_{}=_{(,y)_{}} [_{}(p(y|;),y)*(1-p(y|;))+ _{}((;))*p(y|;)],\] (5)

This implementation of \(_{}\) introduces no extra hyperparameters and we empirically demonstrate its effectiveness in the following experiment section 4. In the Appendix A.3.2, we consider other instantiations of the modulating function and demonstrate that these can be comparably effective.

Extraction of OOD local features.We adopt the ranking-based method as suggested by LoCoOp [Miyai et al., 2024b] to extract OOD local features from ID samples for OOD regularization.

To be specific, we select the region indices of ID-irrelevant regions from a set of all region indices \(I=\{0,1,2,...,H W-1\}\), where \(H\) and \(W\) denote the height and width of the feature map. We calculate the classification prediction probabilities for each region \(i\) during training by computing the similarity between the image features \(^{(i)}\) of each region \(i\) and the text features of the ID classes. Then we choose regions that do not include their ground truth class in the top-\(K\) predicted classes as ID-irrelevant regions \(J\). The corresponding formulation is presented as follows:

\[J=\{i I:(p^{(i)}(y|;))>K\},\] (6)

where \(p^{(i)}(y|;)\) denotes the prediction probability of region \(i\) for the ground-truth label and \((p^{(i)}(y|;))\) denotes the rank of the ground-truth label among all ID classes. We summarize the whole procedure of the proposed SCT in Algorithm 1.

Test-time OOD detection.At the testing stage, we use the GL-MCM score proposed by (Miyai et al., 2023) since it has been empirically proved to outperform the conventional MCM score. It combines the maximum softmax probability scores for both global and local image features. The detailed formulation is presented as follows:

\[S_{}=_{m},_{m}) /)}{_{m^{}=1}^{M}((,_{m^{ }})/)}+_{m,i}^{(i)},_{m})/)}{_{m^{}=1}^{M}(( ^{(i)},_{m^{}})/)}.\] (7)

where \(^{(i)}\) denotes the local image feature of ID samples \(\) for region \(i\) and we set \(=1\).

Comparison and compatibility.Compared with the previous prompt tuning based OOD detection algorithm (Miyai et al., 2024), the critical idea behind SCT is trying to adaptively adjust the contribution of OOD features extracted from ID data with different uncertainty during training. It provides a general framework (under the guidance of the learning objective in Eq. (4)) to assist VLMs in learning from spurious OOD features for effective OOD detection. The discriminative feature regularized by our SCT can be utilized by those advanced post-hoc scoring functions (Sun et al., 2021; Huang et al., 2021; Sun and Li, 2022). For prompt tuning based methods, the adaptive modulation introduced in SCT is orthogonal to current tuning objectives (Liu et al., 2020) and also compatible with different augmentation (Lu et al., 2023) or mining strategies (Bai et al., 2023; Zhu et al., 2023).

## 4 Experiment

In this section, we present the comprehensive verification of the proposed SCT in the CLIP-based OOD detection scenario. First, we provide the experimental setups in detail (in Section 4.1). Secondly, we provide the performance comparison of our approach with a series of CLIP-based post-hoc methods and prompt tuning based methods (in Section 4.2). Thirdly, we conduct various ablation studies and further discussions to understand our method (in Section 4.3).

### Experimental setup

**Datasets.** Following the common benchmarks used in previous works, we adopt the ImageNet-1K dataset (Deng et al., 2009) as the ID data. For OOD datasets, we adopt the same ones as in (Huangand Li, 2021), including subsets of iNaturalist [Van Horn et al., 2018], SUN [Xiao et al., 2010], Places [Zhou et al., 2017], and TEXTURE [Cimpoi et al., 2014]. For the few-shot training, we use 1, 2, 4, and 16 shots ID data for training, respectively, and evaluate models in the full test set. We also present the comparison results on conventional CIFAR benchmarks [Hendrycks et al., 2019, Liu et al., 2020], which adopt CIFAR-10 and CIFAR-100 as ID datasets [Krizhevsky, 2009], in Appendix A.3.2.

**Evaluation metrics.** We employ the following two common metrics to evaluate the performance of OOD detection: (i) Area Under the Receiver Operating Characteristic curve (AUROC) [Davis and Goadrich, 2006] can be interpreted as the probability for a positive sample to have a higher discriminating score than a negative sample [Fawcett, 2006]; (ii) False Positive Rate (FPR) at \(95\%\) True Positive Rate (TPR) [Liang et al., 2018] indicates the probability for a negative sample to be misclassified as positive when the true positive rate is at \(95\%\). We also adopt in-distribution testing accuracy (ID-ACC) to measure the preservation level of the performance for the original classification task on ID data and use Expected Calibration Error (ECE) [Naeini et al., 2015] to measure the performance of SCT on confidence calibration, which are both presented in Appendix A.3.2.

**Implementation details.** Following previous works [Tao et al., 2023, Ming et al., 2022a, Miyai et al., 2023], we use ViT-B/16 [Dosovitskiy et al., 2021] as the backbone model for the main experiment. For the hyperparameter \(K\) in the surrogate OOD features extraction, we use 200 in all experiments as recommended by [Miyai et al., 2024b]. For SCT, we adopt \(=0.4\) under the 1-shot setting and \(=0.2\) under the 16-shot setting. We train the CLIP for 25 epochs with a learning rate of 0.002 and other hyperparameters (_e.g._ batch size=32, SGD optimizer and token lengths \(N\)=16) are the same as those of CoOp [Zhou et al., 2022a]. We use two Nvidia 3090 GPUs for all experiments.

**OOD detection baselines.** We compare SCT with several competitive CLIP-based OOD detection methods in the two directions, including post-hoc methods and prompt tuning based methods. For post-hoc methods, we compare with MCM [Ming et al., 2022a] and GL-MCM [Miyai et al., 2024b] as zero-shot baselines. We also adopt Maximum Softmax Probability (MSP) [Hendrycks and Gimpel, 2017], ODIN [Liang et al., 2018], ReAct [Sun et al., 2021], MaxLogit [Hendrycks et al., 2022], and Energy score [Liu et al., 2020] as conventional scoring function baselines. In addition, we provide more discussions about post-hoc methods and our methods in Appendix A.3.2. For prompt tuning based methods, we adopt CoOp [Zhou et al., 2022a], LoCoOp [Miyai et al., 2024b], IDLike [Bai et al., 2023], NegPrompt [Li et al., 2024] and LSN [Nie et al., 2023] as baselines. For all prompt tuning based methods, we constrain all major experiments to few-shot learning scenarios, which is more practical in real cases. Note that LSN is a general learning framework that can be combined with various prompt tuning methods and we report the result of LSN incorporated with LoCoOp in Table 1. We leave more definitions and implementation details in the Appendix A.1.

### Main results

In this part, we present the major performance comparison with some representative baseline methods for OOD detection to demonstrate the effectiveness of the proposed SCT. Specifically, we consider several zero-shot methods as the performance reference based on the pretrained CLIP and some prompt tuning based methods for specific comparison on fine-tuning with few-shot ID data. Note that we leave the experiment results on 2-shot and 4-shot settings in Appendix A.3.2.

**Comparisons on conventional OOD detection** In Table 1, we present the overall results of the comparison between different baseline methods and SCT for OOD detection. Since the prompt tuning based methods engage the ID data during training, the model will generally gain better empirical performance on OOD detection, reflected by evaluation metrics like FPR95 and AUROC. IDLike, NegPrompt, and LSN all introduce a set of negative prompts for each ID class to learn negative semantics of ID objects using different strategies, which obtain different levels of detection performance gains. Without sacrificing much classification performance (i.e., ID classification accuracy) on ID data, as shown in Table 6, our SCT can consistently achieve better OOD detection performance on the large-scale ImageNet-1k benchmark, which verifies the effectiveness of our methods with the newly proposed modulation factors.

**Compatibility with other baselines.** In Table 2, we report the results of compatibility experiments, in which we compare those prompt tuning based methods with their variants, incorporating our SCT to dynamically adjust the importance of OOD regularization from ID samples with different uncertainty levels. We can find that our SCT can consistently help them gain better or comparable OOD detection performance across two evaluation metrics while keeping the classification accuracy comparable with the vanilla prompt-tuned model, as shown in Appendix A.3.2.

**Comparisons on hard OOD detection.** Following the setup in MCM , we also explore the performance of SCT on hard OOD detection tasks, as shown in Table 3. SCT significantly outperforms LoCoOp in all four experimental settings, demonstrating that SCT has strong discriminative power for semantically hard OOD data.

### Ablation study

In this part, we conduct various ablation experiments and further explorations to provide a thorough understanding of the characteristics of our proposed SCT. For the extra results and discussions (e.g., computational cost and social impact), we leave more details in Appendix A.3.2.

    &  &  &  &  &  \\  & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) \\   \\ MCM & 31.86 & 94.17 & 37.28 & 92.55 & 42.94 & 90.09 & 58.37 & 85.83 & 42.61 & 90.66 \\ GL-MCM & 15.16 & 96.71 & 29.16 & 93.41 & 37.07 & 90.37 & 58.85 & 83.11 & 35.06 & 90.90 \\   \\ MSP\({}^{1}\) & 74.57 & 77.74 & 76.95 & 73.79 & 79.72 & 72.18 & 73.66 & 74.84 & 74.98 & 76.22 \\ ODN 1 & 98.93 & 57.73 & 88.72 & 78.42 & 87.80 & 76.88 & 85.47 & 71.49 & 90.23 & 71.13 \\ Energy\({}^{1}\) & 64.98 & 87.18 & 46.42 & 91.17 & 57.40 & 87.33 & 50.39 & 88.22 & 54.80 & 88.48 \\ ReAct\({}^{1}\) & 65.57 & 86.87 & 46.17 & 91.04 & 56.85 & 87.42 & 49.88 & 88.13 & 54.62 & 88.37 \\ MaxLogit\({}^{1}\) & 60.88 & 88.03 & 48.93 & 91.16 & 55.54 & 87.45 & 48.72 & 88.63 & 52.49 & 88.82 \\   \\ CoOp & 43.80\({}^{1.43}\) & 91.40\({}^{1.78}\) & 35.42\({}^{1.56}\) & 29.65\({}^{1.03}\) & 30.70\({}^{1.24}\) & 59.49\({}^{1.06}\) & 49.91\({}^{1.05}\) & 87.95\({}^{1.07}\) & 42.38\({}^{1.02}\) & 90.63\({}^{1.03}\) \\ LeCoOp & 28.81\({}^{1.278}\) & 94.05\({}^{1.07}\) & 27.56\({}^{1.03}\) & 94.51\({}^{1.01}\) & 33.68\({}^{1.04}\) & 51.95\({}^{1.02}\) & 53.13\({}^{1.02}\) & 86.85\({}^{1.04}\) & 34.91\({}^{1.06}\) & 91.75\({}^{1.02}\) \\ DLike & 12.07\({}^{1.04}\) & 98.76\({}^{1.04}\) & 10.55\({}^{1.54}\) & 94.11\({}^{1.01}\) & 47.14\({}^{1.54}\) & 83.81\({}^{1.02}\) & 33.56\({}^{1.04}\) & 89.67\({}^{1.04}\) & 32.47\({}^{1.08}\) & 91.67\({}^{1.07}\) \\ NegPrompt & 65.03\({}^{1.48}\) & 84.56\({}^{1.22}\) & 44.91\({}^{1.06}\) & 86.93\({}^{1.01}\) & 53.14\({}^{1.01}\) & 86.55\({}^{1.07}\) & 58.61\({}^{1.04}\) & 61.37\({}^{1.02}\)Importance of the modulation factors in SCT.As an important aspect of the learning objective of SCT in Eq. (4), the newly introduced modulating factors in the learning objective of SCT conduct adaptive redirection of prompt tuning towards suitable tasks. Although each of modulating factors can seemingly achieve the redirection effect alone, we empirically find that both modulating factors play a significant role in enhancing OOD detection performance of VLMs, as shown in Table 4.

Influence of regularization weight in OOD regularization.The regularization weight \(\) controls the contribution of OOD regularization to the prompt learning process like the role of the two modulation factors. In Figure 3(a), we show the performance by varying the ratio \(=1\). It is worth noting that setting high regularization weights such as \(=1\) may even degrade the performance, indicating that the ID classification task should be attached more importance for better OOD detection.

Generality of using different OOD regularization functions.Since SCT introduces a general learning framework of prompt tuning for OOD detection, the specific realization for the OOD regularization function can have multiple choices (e.g., MSP (Hendrycks and Gimpel, 2017) or Energy score function (Liu et al., 2020)). Here we report the performance using different OOD regularization functions in Figure 3(b), where they have different performance improvements compared with the original LoCoOp baseline. Specifically, the energy regularization needs tuning the two energy threshold hyperparameters \(m_{in}\) and \(m_{out}\), limiting its advantages over other regularization functions.

Implementation with different CLIP architectures.We evaluate SCT with different VLM architectures and the results are shown in Figure 3(c). The result illustrates that the larger backbone boosts the performance of OOD detection and also shows SCT can outperform LoCoOp across various VLM architectures. It is important to note that we take the same hyperparameters across all architectures, demonstrating the robustness of SCT hyperparameters on different VLM architectures.

Comparison between different methods for extracting OOD features.In Figure 3(d), we perform the comparison of our method adopting different methods for extracting OOD features, including the probability-based and entropy-based methods. The results verify the superiority of SCT on OOD detection across all the OOD feature extraction methods. Specifically, the probability-based method and entropy-based method both have a threshold hyperparameter to discriminate between ID and OOD features. The sensitivity to hyperparameters of different methods might be the reason behind the different OOD detection performance. For the entropy-based method, the performance is significantly poor since it is challenging to determine the appropriate threshold (Miyai et al., 2024).

## 5 Discussions and limitations

Comparisons with advanced post-hoc methods.Recently, advanced post-hoc approaches (Djurisic et al., 2022; Xu et al., 2024) exhibit comparable OOD detection performance to tuning based methods, despite the lack of training data. However, prompt tuning based methods can leverage the generalization ability of VLMs to better fit the domains of the downstream tasks given only few-shot ID training data. What's more, post-hoc methods and prompt tuning based methods are compatible with each other, further boosting the OOD detection performance. We provide more detailed discussions, including empirical experiments, on the compatibility of SCT with advanced post-hoc methods in Appendix A.3.2. Furthermore, future research efforts into post-hoc calibration methods for prompt tuning based OOD detection could also contribute to the community.

   Shot & \(\) & \(\) & FPR95\(\) & AUROC\(\) & ID-ACC\(\) \\   & ✗ & ✗ & 34.94 & 91.75 & **69.03** \\  & ✓ & ✗ & 31.14 & **92.35** & 65.60 \\  & ✓ & ✗ & 31.90 & 91.74 & 68.70 \\  & ✓ & ✓ & **31.09** & 92.04 & 68.80 \\   & ✗ & ✗ & 29.47 & 93.10 & 71.43 \\  & ✓ & ✗ & 29.30 & 92.66 & 71.50 \\   & ✗ & ✓ & 28.94 & 92.62 & **71.90** \\   & ✓ & ✓ & **26.47** & **93.37** & 71.77 \\   

Table 4: Ablation study on the modulation factors for the classification task and regularization term. Detailed results can be found in Appendix A.3.2.

   ID Dataset & OOD Dataset & Method & FPR95\(\) & AUROC\(\) \\  ImageNet-10 & ImageNet-10 &  LeCoOp \\  & 28.20 & 92.75 \\  &  SCT \\  & **25.10** & **94.33** \\  ImageNet-20 & ImageNet-10 &  LeCoOp \\  & 34.40 & 92.34 \\  &  SCT \\  & **25.00** & **94.95** \\   & ImageNet-100 &  LeCoOp \\  & 30.08 & 93.00 \\  & SCT & **26.64** & **93.90** \\   & ImageNet-100 & 
 LeCoOp \\  & 61.40 & 81.97 \\  & SCT & **57.80** & **82.60** \\   

Table 3: OOD detection performance comparison with LoCoOp on hard OOD detection tasks. Bold numbers represents superior results.

Performance improvement on the AUROC metric.The experiment results in Table 1 and 2 indicate that the improvements of SCT over LoCoOp on AUROC are less notable than the FPR95 metric. However, as shown in the comparison of different baselines in Table 1, the improvement space for FPR95 is significantly larger than AUROC. Therefore, the progress on these two metrics should not be treated equally. Nevertheless, further investigation is necessary to enhance the OOD detection performance specifically targeting improvement on the AUROC metric.

The sensitivity to training data under the few-shot setting.VLMs are exposed to limited ID data samples under the few-shot scenarios, which means that the OOD detection performance of prompt tuning based methods, including SCT, can be susceptible to the quality of limited ID training data in practice. Exploration of selection strategies of suitable training data or overcoming the data sensitivity inherent in the few-shot setting could further facilitate the practical application of prompt tuning based OOD detection methods in real-world scenarios.

Theoretical analysis of the proposed method.Although we propose a novel framework to mitigate the problem of invalid extracted OOD features, we have not yet provided sufficient theoretical analysis to prove the effectiveness of our method. We choose the linear function as the modulation function for the sake of simplicity, instead of based on theoretical justification. Conducting theoretical analyses on the relationship between sample uncertainty and OOD detection performance of prompt tuning based methods under few-shot settings is also a potential direction for future work..

## 6 Conclusion

In this paper, we propose a novel learning framework, i.e., _Self-Calibrated Tuning (SCT)_, that improves the OOD detection capability of VLMs with only the given ID training data. To mitigate the problem caused by invalid OOD features mined from ID data, SCT introduces two modulating factors to the original learning objective to conduct adaptive redirection of prompt tuning process between the tasks of ID classification and OOD regularization. Through the redirection effect, our method calibrates the impact of OOD features extracted from different ID samples based on the sample uncertainty estimation during the training process, which facilitates the model learning from imperfect surrogate OOD features for OOD regularization. We have conducted extensive experiments to demonstrate the effectiveness of SCT and its compatibility with a range of prompt tuning based methods, along with various ablation studies and further explorations to characterize the framework.