# CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset

Akshatha Arodi\({}^{1*}\)  Margaux Luck\({}^{1*}\)  Jean-Luc Bedwani\({}^{2}\)

**Aldo Zaimi\({}^{1}\)  Ge Li\({}^{1}\)  Nicolas Pouliot\({}^{2}\)  Julien Beaudry\({}^{2}\)  Gaetan Marceau Caron\({}^{1}\) \({}^{1}\)**Mila - Quebec AI Institute \({}^{2}\)IREQ - Institut de recherche d'Hydro-Quebec

\({}^{*}\)equal contributions

###### Abstract

Machine learning models are increasingly being deployed in real-world contexts. However, systematic studies on their transferability to specific and critical applications are underrepresented in the research literature. An important example is visual anomaly detection (VAD) for robotic power line inspection. While existing VAD methods perform well in controlled environments, real-world scenarios present diverse and unexpected anomalies that current datasets fail to capture. To address this gap, we introduce _CableInspect-AD_, a high-quality, publicly available dataset created and annotated by domain experts from Hydro-Quebec, a Canadian public utility. This dataset includes high-resolution images with challenging real-world anomalies, covering defects with varying severity levels. To address the challenges of collecting diverse anomalous and nominal examples for setting a detection threshold, we propose an enhancement to the celebrated PatchCore algorithm. This enhancement enables its use in scenarios with limited labeled data. We also present a comprehensive evaluation protocol based on cross-validation to assess models' performances. We evaluate our _Enhanced-PatchCore_ for few-shot and many-shot detection, and Vision-Language Models for zero-shot detection. While promising, these models struggle to detect all anomalies, highlighting the dataset's value as a challenging benchmark for the broader research community. Project page: [https://mila-iqia.github.io/cableinspect-ad/](https://mila-iqia.github.io/cableinspect-ad/)

## 1 Introduction

Machine learning is increasingly applied across diverse industrial fields such as robotics, genomics, climate and materials science due to the impressive performance of large pre-trained models. As the community looks towards deploying these models in specialized domains where their effectiveness remains uncertain, there is a pressing need to improve their transferability in these contexts. This underscores the necessity for tailored datasets by domain experts. Visual anomaly detection (VAD) in a specific industrial context, exemplifies a critical application, promising cost reduction, time savings, and enhanced safety measures by enabling preventive maintenance. While existing VAD methods perform well in controlled environments, real-world scenarios present diverse and unexpected anomalies that current datasets fail to capture. Public VAD datasets, such as MvTec AD , VisA , and MVTec LOCO AD , focus mainly on objects and textures in a controlled manufacturing context, thus limiting the scope of potential anomalies. Moreover, these datasets do not account for scenarios with significant variations of the same object, further complicating AD in real-world applications. For instance, objects may exhibit substantial differences when viewed indoors versus outdoors due to varying operational conditions and environmental factors such as lighting and weather. Additionally, wear and tear over time can introduce anomalies that evolve, creating multiple views and states of the same object. Compounding the complexity, images may contain more than one anomaly, requiring models to discern and identify multiple issues simultaneously.

[MISSING_PAGE_FAIL:2]

[MISSING_PAGE_EMPTY:3]

real-world data. It addresses the challenge of detecting rare multi-scale anomalies on power line cables, which vary in wear, color, texture, and braiding. It also facilitates the extension of these techniques to other infrastructure-monitoring areas, such as railways and pipelines, fostering the evaluation of VAD models and the creation of predictive maintenance systems to advance VAD technologies across various sectors.

Creation and annotationThe creation and annotation of _CableInspect-AD_ is highly challenging and requires domain expertise. To achieve this, experts selected three cables used in the field. The cables are suspended for image acquisition, and an apparatus with a moving camera is used to capture the images to ensure a uniform background and mimic real-world robotic scenarios. The uniform background was intentionally chosen to minimize distractions and external factors, allowing models to focus solely on detecting anomalies within the object, a practice commonly seen in other VAD benchmarks. Importantly, capturing images while the apparatus is in motion introduces slight disturbances, making the images less perfect compared to datasets like MVTec AD, thereby adding to the dataset's uniqueness and realism. To maximize the use of each cable, both sides (referred to as sides A and B) are utilized.

For each cable side, three videos are recorded at a frame rate of 30 frames per second, consisting of RGBA images at a resolution of 1920\(\)1080 pixels. A total of 18 videos are captured by manually moving a camera along the cables at different speeds, slow enough to capture a defect in several frames. Each pass includes minor rotational variations, up to 20 degrees, and can be taken forward or backward, slightly changing the perspective. The videos are then processed to keep one frame out of three for anomaly annotations, reducing the frame rate to 10 frames per second.

Annotations include image-level labels and bounding boxes, assigned based on expert assessment of the anomaly's appearance in the image. Additionally, per-pixel labels for the first recorded video on each cable are obtained using SAM  prompted with the bounding boxes and then manually corrected. Depending on the point of view, a defect can be associated with different grades. An image containing at least one bounding box is considered anomalous. Examples of anomalies are shown in Figure 1 illustrating their varying appearance and complexity. The dataset was annotated by at least four experts who first developed and agreed on guidelines to establish a clear annotation framework. The process was repeated five times until an agreement was achieved. The acquisition process, annotation guide, and details on the annotation process are in Supplementary Material.

StatisticsThe dataset contains 4,798 annotated images (2,639 anomalous and 2,159 nominal). Among the anomalous images, there are 193 unique anomalies, comprising 110 manually created and 83 pre-existing real-world anomalies. The total number of anomalies annotated is 6,023. The distribution of defects among the three cables is shown in Figure 2

Evaluation protocolTo estimate variance in model performance, we use a k-fold cross-validation strategy tailored to our dataset. This approach addresses the high anomaly ratio resulting from the

Figure 2: Anomaly types and grades per cable. The grades are (I)Important, (L)ight, (C)omplete, (E)xtracted, (P)Partial, (D)eep and (S)uperficial. The anomalies are not distributed uniformly across all the cables.

deliberate creation of diverse anomalies, the non-uniform distribution of anomalies, and possible data leakage due to overlapping video frames. Specifically, we split the power line cable dataset into train and test sets using a k-fold sampling strategy based on defect identifiers. For each fold, defect identifiers are randomly selected, and 100 subsequent nominal images are selected for training while preventing overlap between training and test sets using buffers. This process is repeated k times, ensuring a consistent training size but varying test images and anomaly ratios across folds as shown in Figure 3. More details can be found in the Supplementary Material.

## 4 Enhanced-PatchCore

_Enhanced-PatchCore_, built on PatchCore , is an instance-based approach that stores feature embeddings of nominal images in a _memory bank_\(\) to establish a context during training. This memory bank is then coreset-subsampled  to reduce its size.

At test time, the abnormality of a test image \(X\) is determined by measuring its distance to the nearest neighbor in the memory bank within the embedding space. This distance, referred to as anomaly score, is defined as:

\[S(X):=_{e(X)}d(e,)=_{e(X)}_{ e^{}}d(e,e^{}) \]

where \((X)\) is the set of patch embeddings generated by an image encoder and \(d\) is the Euclidean distance.

To decide if an image contains an anomaly from this score, a threshold must be set using a validation set. However, creating a robust validation set with a diverse range of anomalies is prohibitively expensive. Many VAD methods overlook this crucial aspect, either manually setting thresholds or reporting the best F1 score. This is impractical in real-world applications, where thresholds must be carefully calibrated to specific operational requirements and constraints. Therefore, we introduce _Enhanced-PatchCore_, which addresses this challenge by setting a threshold using only the train set. Specifically, it computes anomaly scores of images within the memory bank to estimate the empirical distribution of scores of nominal images. The score \(S(X)\) is calculated as follows:

\[(X):=_{e(X)}_{e^{} (X)}d(e,e^{}) \]

Similarly, a segmentation map can be computed by realigning the patch anomaly scores to match the original input resolution by upscaling the scores using bi-linear interpolation. Specifically, the

Figure 3: The three cables have different numbers of images with varying anomaly ratios in the test set. The cables have 40, 46, and 30 folds, respectively. (a) shows the number of images in the test set over all the folds for each cable (x-axis), and (b) shows the ratio in the test set of the cables. Each point corresponds to the anomaly ratio in a fold. The identity line shows where a balanced dataset would be.

anomaly score at the pixel level for a pixel at coordinates \((i,j)\) in the image, with embedding \(e_{i,j}\) is computed using the following equation:

\[(X_{i,j}):=_{e^{}(X)}d(e_{i,j },e^{}), \]

Experimentally, the distribution of \((X)\) closely matches the one from a validation set. We evaluate four thresholding strategies on this estimated empirical distribution: _max_, outliers from a boxplot (_whisker_), percentile estimation from parametric distribution at \(95th\) percentile (_beta-prime_-\(95\)), and percentile estimation from empirical distribution at \(95th\) percentile (_empirical-\(95\)_). Additional details can be found in Supplementary Material.

## 5 Experimental setting

Our experimental setup assumes the unavailability of a validation set, reflecting real-world challenges. Furthermore, many VAD methods assume that the training data contains only nominal images, but the presence of contaminated training data with anomalies can significantly reduce performance .

Given the difficulty of avoiding such contamination in specialized domains due to annotation challenges, our setup transitions from many-shot to few-shot and finally to zero-shot settings by gradually reducing the number of examples in the training set until it is completely removed.

To adhere to our setup constraints, we employed pre-trained models without fine-tuning that operate effectively in low-data regimes as baselines. Specifically, we propose _Enhanced-Patchcore_ for few-shot and many-shot settings. For the zero-shot setting, we use conversational VLMs including LLaVA 1.5-7B/13B and BakLLaVA-7B, , CogVLM-17B and CogVLM2-19B , and a VLM tailored for VAD, WinCLIP . The prompt used to get VLMs' predictions is _"Is there any anomaly or defect in the image. Please answer by Yes or No."_. For WinCLIP, we use "cable" as the object to fill the templates. For the many-shot and few-shot tasks, \(N\) images were randomly sampled from the training sections within the k-fold cross-validation. For the zero-shot task, the training sections were entirely discarded. The test sections remain constant within the k-fold across all tasks.

To evaluate our models' performance, we consider threshold-independent metrics Area Under the Precision-Recall curve (AUPR) and Area Under the Receiver Operating Characteristic Curve (AUROC), and threshold-dependent metrics: precision, recall, false positive rate (FPR), false negative rate (FNR) and F1-score at the image level. To compute AUROC and AUPR for conversational VLMs, we adapt the VQAScore  to obtain anomaly scores. Specifically, VQAScore computes the probability of the output token _"Yes"_ when prompting VLMs with the fixed template _"Does this figure show [caption]? Please answer yes or no."_. We use _"an anomalous or defective cable"_ as _"[caption]"_. For per-pixel evaluation we use AUPRO . Additional implementation details are in Supplementary Material.

## 6 Results and discussion

Table  summarizes the overall performance of the baseline models and _Enhanced-PatchCore_ on our _CableInspect-AD_ dataset at image-level. First, we can observe that CogVLM-17B has the best F1 Score, whereas CogVLM2-19B has the lowest FPR. They both outperform WinCLIP, for which threshold-dependent metrics cannot be computed without a validation set. Overall, VLMs show high AUROC and AUPR, highlighting their potential as effective anomaly detectors. _Enhanced-PatchCore_ has a better F1 score than all VLMs except CogVLM-17B. There are large variations across VLMs, indicating the need for careful selection. CogVLM2-19B's higher AUROC and AUPR but worse F1 score suggest suboptimal thresholding, underscoring the challenge of effective threshold control in zero-shot VLMs. _Enhanced-PatchCore_, even with limited nominal images, maintains competitiveness while offering the added advantage of pixel-level evaluation.

Performance variability in same category objectsFigure 4 compares the threshold-dependent metrics on the _CableInspect-AD_ dataset for each of the three cables. While all models achieve relatively high mean F1-score values, their performance can significantly vary (Figure 4a) across folds and cables. These variations are particularly notable for cables 2 and 3, which, being older, contain artifacts like scratches and discoloration from natural wear. These artifacts were not considered as anomalies by the experts, posing a greater challenge. This underscores the uniqueness of our dataset, where objects of the same category can have a significantly variable appearance. Additionally, the performance varies across the folds because the test sets of each fold can differ in terms of anomaly types and grades (see Figure 2). Consequently, folds containing a higher proportion of harder-to-detect anomalies (e.g., long scratches) compared to easier ones might show lower performance. Furthermore, our analysis suggests that VLMs are more robust compared to other methods, showing more consistent performance across different folds and cables.

Enhanced-PatchCore - thresholding without a validation setFrom Figure 4a we observe that the model performs well despite thresholding on the training set. Specifically, the performances of _Enhanced-PatchCore_ in the few and many-shot settings employing various thresholding strategies show that the mean F1-score improves in most cases as the number of training images increases. Among the thresholding strategies, _max_--which is the most sensitive to outliers in the memory bank--appears brittle, while _whisker_, _empirical_-\(95\) and _beta-prime_-\(95\) seem to be more robust across the cables. Additionally, if we examine the precision-recall and FPR-FNR trade-offs, using the _beta-prime_-\(95\) strategy as an example (Figures 4b and 4c), we observe that, overall, for cables 1 and 2, an increase in recall is accompanied by a decrease in precision, usually at the expense of an increase in FPR, accompanied by a decrease in FNR (i.e., 1 - Recall), as the number of training images increases. Moreover, increasing the number of images in the training set does not seem beneficial, as it increases the risk of including outliers in the memory bank. On the other hand, reducing the number of instances might result in a less diverse training set compared to the distribution of real-world nominal images.

Analysis of conversational VLMsTable  shows that the VLMs achieve promising results despite not using any training examples (zero-shot). Specifically, the CogVLM variants outperform the other baselines. In Figure 4, CogVLM-17B shows the highest mean F1-score with the lowest variance across folds, outperforming other baselines across all cables (Figure 4a), whereas CogVLM-19B shows the lowest FPR. Despite these encouraging results, VLMs are challenged by many limitations. Notably, VLMs can exhibit limitations in instruction following , be prone to object hallucinations , generate factual errors about objects, attributes, and relations , and be vulnerable to deceptive prompts . Moreover, while conversational VLMs show promise in anomaly detection, their ability to accurately localize anomalies remains a challenge. To highlight some of these limitations, we present examples in Supplementary Material.

Evaluating the impact of background removalOne possible reason for the high variability of the performances of _Enhanced-PatchCore_ is its sensitivity to variations in the background. Therefore, we evaluate the baseline models on a cropped version of _CableInspect-AD_, namely _CableInspect-AD_cropped_, in which we retain only the central part of the cables. In Figure 5, _Enhanced-PatchCore_ shows lower variance in the F1-score across the different thresholding strategies while maintaining good performances on all cables. All thresholding strategies perform similarly, except for the _max

  
**Model** & **F1 Score \(\)** & **FPR \(\)** & **AUPR \(\)** & **AUROC \(\)** \\  LLaVA 1.5-7B & 0.59 \(\) 0.07 & 0.32 \(\) 0.19 & 0.75 \(\) 0.05 & 0.68 \(\) 0.04 \\ LLaVA 1.5-13B & 0.69 \(\) 0.02 & 0.66 \(\) 0.21 & 0.74 \(\) 0.04 & 0.66 \(\) 0.03 \\ BakLLaVA-7B & 0.69 \(\) 0.02 & 0.53 \(\) 0.19 & 0.77 \(\) 0.04 & 0.71 \(\) 0.03 \\ CogVLM-17B & **0.77 \(\) 0.02** & 0.34 \(\) 0.21 & 0.83 \(\) 0.03 & 0.79 \(\) 0.04 \\ CogVLM2-19B & 0.66 \(\) 0.04 & **0.04 \(\) 0.01** & **0.91 \(\) 0.02** & **0.86 \(\) 0.03** \\ WinCLIP & - & - & 0.76 \(\) 0.06 & 0.70 \(\) 0.04 \\  _Enhanced-PatchCore_ & 0.75 \(\) 0.03 & 0.55 \(\) 0.19 & 0.84 \(\) 0.06 & 0.78 \(\) 0.05 \\   

Table 1: Performance metrics at image-level. Mean and standard deviation are calculated across all cables after averaging over all folds. VLMs and WinCLIP are evaluated in a zero-shot setting, while _Enhanced-PatchCore_ is evaluated in a 100-shot setting using the _beta-prime_-\(95\) thresholding strategy. Thresholded-metrics are not reported for WinCLIP since it necessitates a validation set.

strategy on cable 3. Thus, the extraction of the region of interest seems beneficial. Surprisingly, the performance of the conversational VLMs drop significantly. This could be attributed to the reduced view in the cropped version of the image, potentially making it more challenging for them.

In Figure 4(b), we observe an increase in mean AUROC and a decrease in its variance as the number of training images increases, indicating that the choice of the training image in the few-shot setting can greatly influence the performance. However, the AUROC variance does not decrease when the background is retained. WinCLIP demonstrates enhancements in AUROC when excluding the background. Similar findings apply to AUPR. More details on metrics and visualizations are in Supplementary Material.

Visual anomaly detection across different anomaly types and gradesDespite the promising performances demonstrated by the baseline models, all the models fail to detect all types/grades of anomalies. For instance, Figure 5 shows the recall of anomalies based on type and grade by CogVLM-17B on the whole _CableInspect-AD_ dataset. More pronounced anomaly types and grades such as _bent strand (important)_ and _broken strand (complete)_ are readily detected, whereas light and smaller anomalies such as _spaced strands_ and _long scratches (light)_ are prone to be overlooked. This highlights the importance of including multi-grade anomalies in the evaluation benchmark.

Anomaly Segmentation_Enhanced-Patchcore_ outperforms WinCLIP in the segmentation task on _CableInspect-AD_cropped_, with an AUPRO of \(0.53 0.08\) compared to \(0.27 0.06\) for WinCLIP. We apply thresholding strategies on anomaly maps generated by Enhanced-Patchcore to generate pixel-level predictions. We use a _max_ thresholding strategy for the segmentation results shown in Figure 5()(more details are in the Supplementary material). The corresponding pixel-level metric, the

Figure 4: Image-level results of _Enhanced-PatchCore_ (few-/many-shot) with the thresholding strategies and conversational VLMs (zero-shot). (a) and (b) show the mean and standard deviation over all folds for F1-score and FPR for the three cables. The x-axis indicates the number of images in the training set. (c) shows mean precision vs mean recall over all folds.

Pixel-wise Overlap (PRO) score, averaged across all cables and folds, is \(0.28 0.09\). Figure  displays example outputs from _Enhanced-Patchcore_, illustrating that the model effectively identifies larger anomalies but struggles with subtler ones. The rightmost image shows a nominal image where texture changes from wear are visible. These texture variations can distract the model adding complexity to the task.

Figure 5: Image-level results in zero-shot setting using conversational VLMs and WinCLIP, and, few-shot and many-shot using _Enhanced-PatchCore_ on _CableInspect-AD_cropped_. Mean and standard deviation over all folds are reported for the three cables. On the figures, the x-axis indicates the number of images in the training set. (a) shows F1-score. For _Enhanced-PatchCore_, the metrics are computed using different thresholding strategies. (b) AUROC for _Enhanced-PatchCore_ and WinCLIP.

Figure 6: CogVLM-17B image-level recall per anomaly types/grades (sample counts on bars).

Figure 7: _Enhanced-PatchCore_ qualitative results for anomaly segmentation. The rightmost image is nominal (green); the rest show anomalies (red). The images (top row) and pixel-level prediction heatmaps with contours of detected anomalies using the _max_ thresholding strategy (middle row) are shown against ground truth masks (bottom row) from different cables. The bottom row shows the segmentation masks coloured based on the anomaly type. Some anomalies are easily detected (left column) whereas the others are difficult and are missed (middle column).

ContributionOur dataset demonstrates its unique strength through the comprehensive diversity of anomaly types and severity levels it captures. Specifically, it includes seven distinct types of anomalies, each with up to three levels of severity. This allows for a more in-depth evaluation within the targeted domain. Broader datasets, with lower anomaly diversity per category, may not fully capture the intricacies persistent in real-world applications. In addition, given the accelerating electrification of transportation, there is a growing need for reliable transmission facilities. Therefore, it is critical to develop VAD models that can specialize in such high-stakes applications. Our dataset meets this need by offering a focused evaluation framework that complements broader datasets.

Broad impactThe methodologies and insights derived from our focused study are adaptable to a wide range of anomaly detection scenarios. For instance, our experiments demonstrate that Vision-Language Models (VLMs) can be effectively utilized for zero-shot VAD tasks. However, we also find that no current model performs well across all anomaly types, particularly when detecting light-grade anomalies. This finding reveals the limitations of current models and provides a valuable direction for future research aimed at enhancing model performance in specialized applications.

LimitationsWe acknowledge that this work has the following limitations. First, we aimed to create a dataset containing a comprehensive range of real-world anomalies. However, this resulted in a higher anomaly ratio than typically observed in real-world scenarios, where anomalies rarely occur. This can be addressed by analyzing the results with this variation in mind or, when necessary, by employing stratified sampling to adjust the anomaly ratio within the folds. Second, despite our efforts to provide a rich and diverse set of examples for effective model learning and evaluation, the dataset does not encompass every possible anomaly found on a cable in real-world settings, because the methodology for data creation may not fully capture all complexities encountered in real-world scenarios, such as the deposition of snow or bird droppings on the cable.

Ethical concernsWe do not anticipate significant risks of security threats or human rights violations in our work or its potential applications. However, while our work aims to improve system reliability, we remind researchers that deploying machine learning models for VAD in robotic power line inspection may miss anomalies, potentially compromising safety and public utility operations.

## 7 Conclusion

In this work, we introduce _CableInspect-AD_, a novel anomaly detection dataset created and annotated by domain experts. We employ a k-fold evaluation to assess _Enhanced-PatchCore_ with multiple thresholding strategies, WinCLIP and open VLMs on the proposed dataset. We find that, in general, the baselines show promising results in detecting anomalies on the cables, but struggle to detect anomalies of certain types and grades. This presents an important challenge for the development of new models on this task and highlights the potential value of _CableInspect-AD_ as a resource for the broader AD community. Furthermore, we highlight the potential of recent open VLMs in zero-shot anomaly detection, requiring minimal prompt engineering and no image preprocessing. Future work will aim to assess VLM's zero-shot capabilities to other anomaly tasks such as type/grade classification, localization, and segmentation.

[MISSING_PAGE_FAIL:11]

*  H. Choi, G. Koo, B. J. Kim, and S. W. Kim. Real-time power line detection network using visible light and infrared images. In _2019 International Conference on Image and Vision Computing New Zealand (IVCNZ)_, pages 1-6. IEEE, 2019.
*  F. S. de Oliveira, M. de Carvalho, P. H. T. Campos, A. D. S. Soares, A. C. Junior, and A. C. R. D. S. Quirino. Ptl-ai furnas dataset: A public dataset for fault detection in power transmission lines using aerial images. In _2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)_, volume 1, pages 7-12. IEEE, 2022.
*  H. Deng, Z. Zhang, J. Bao, and X. Li. Anovl: Adapting vision-language models for unified zero-shot anomaly localization. _arXiv preprint arXiv:2308.15939_, 2023.
*  T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford. Datasheets for datasets. _Communications of the ACM_, 64(12):86-92, 2021.
*  Z. Gu, B. Zhu, G. Zhu, Y. Chen, M. Tang, and J. Wang. Anomalygpt: Detecting industrial anomalies using large vision-language models. _arXiv preprint arXiv:2308.15366_, 2023.
*  P. Hamelin, F. Miralles, G. Lambert, S. Lavoie, N. Pouliot, M. Montfroond, and S. Montambault. Discrete-time control of lindrone: An assisted tracking and landing uav for live power line inspection and maintenance. In _2019 International Conference on Unmanned Aircraft Systems (ICUAS)_, pages 292-298. IEEE, 2019.
*  S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, et al. Language is not all you need: Aligning perception with language models. _Advances in Neural Information Processing Systems_, 36, 2024.
*  J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and O. Dabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19606-19616, 2023.
*  A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
*  S. J. Lee, J. P. Yun, H. Choi, W. Kwon, G. Koo, and S. W. Kim. Weakly supervised learning with convolutional neural networks for power line localization. In _2017 IEEE Symposium Series on Computational Intelligence (SSCI)_, pages 1-8. IEEE, 2017.
*  Y. Lee and P. Kang. Anovit: Unsupervised anomaly detection and localization with vision transformer-based encoder-decoder. _IEEE Access_, 10:46717-46724, 2022.
*  C.-L. Li, K. Sohn, J. Yoon, and T. Pfister. Cutpaste: Self-supervised learning for anomaly detection and localization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9664-9674, 2021.
*  X. Li, Z. Huang, F. Xue, and Y. Zhou. Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. _arXiv preprint arXiv:2401.16753_, 2024.
*  Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating text-to-visual generation with image-to-text generation. 2024.
*  H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
*  H. Liu, W. Xue, Y. Chen, D. Chen, X. Zhao, K. Wang, L. Hou, R. Li, and W. Peng. A survey on hallucination in large vision-language models. 2024.
*  J. Liu, G. Xie, J. Wang, S. Li, C. Wang, F. Zheng, and Y. Jin. Deep industrial image anomaly detection: A survey. _arXiv e-prints_, pages arXiv-2301, 2023.
*  R. Madaan, D. Maturana, and S. Scherer. Wire detection using synthetic data and dilated convolutional networks for unmanned aerial vehicles. In _2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3487-3494. IEEE, 2017.

*  OpenAI. Gpt-4v(ision) system card. OpenAI, September 25 2023. Available at: [https://openai.com/index/gpt-4v-system-card/](https://openai.com/index/gpt-4v-system-card/)[Accessed: 30 November 2023].
*  R. M. Prates, R. Cruz, A. P. Marotta, R. P. Ramos, E. F. Simas Filho, and J. S. Cardoso. Insulator visual non-conformity detection in overhead power distribution lines using deep learning. _Computers & Electrical Engineering_, 78:343-355, 2019.
*  Y. Qian, H. Zhang, Y. Yang, and Z. Gan. How easy is it to fool your multimodal lms? an empirical analysis on deceptive prompts. 2024.
*  P.-L. Richard, N. Pouliot, F. Morin, M. Lepage, P. Hamelin, M. Lagac, A. Sartor, G. Lambert, and S. Montambault. Lineranger: Analysis and field testing of an innovative robot for efficient assessment of bundled high-voltage powerlines. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 9130-9136. IEEE, 2019.
*  K. Roth, L. Pemula, J. Zepeda, B. Scholkopf, T. Brox, and P. Gehler. Towards total recall in industrial anomaly detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14318-14328, 2022.
*  M. Rudolph, T. Wehrbein, B. Rosenhahn, and B. Wandt. Fully convolutional cross-scale-flows for image-based defect detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1088-1097, 2022.
*  J. Santos, T. Tran, and O. Rippel. Optimizing patchcore for few/many-shot anomaly detection. _arXiv preprint arXiv:2307.10792_, 2023.
*  X. Shi, B. Cui, G. Dobbie, and B. C. Ooi. Uniad: A unified ad hoc data processing system. _ACM Transactions on Database Systems (TODS)_, 42(1):1-42, 2016.
*  J. Song, K. Kong, Y.-I. Park, S.-G. Kim, and S.-J. Kang. Anoseg: anomaly segmentation network using self-supervised learning. _arXiv preprint arXiv:2110.03396_, 2021.
*  X. Tao, D. Zhang, Z. Wang, X. Liu, H. Zhang, and D. Xu. Detection of power line insulator defects using aerial images analyzed with convolutional neural networks. _IEEE transactions on systems, man, and cybernetics: systems_, 50(4):1486-1498, 2018.
*  M. Tomaszewski, B. Ruszczak, and P. Michalski. The collection of images of an insulator taken outdoors in varying lighting conditions with additional laser spots. _Data in brief_, 18:765-768, 2018.
*  C.-C. Tsai, T.-H. Wu, and S.-H. Lai. Multi-scale patch-based representation learning for image anomaly detection and segmentation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3992-4000, 2022.
*  A. L. B. Vieira-e Silva, H. de Castro Felix, T. de Menezes Chaves, F. P. M. Simoes, V. Teichrieb, M. M. dos Santos, H. da Cunha Santiago, V. A. C. Sgotti, and H. B. D. T. L. Neto. Stn plad: A dataset for multi-size power line assets detection in high-resolution uav images. In _2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)_, pages 215-222. IEEE, 2021.
*  A. L. B. Vieira e Silva, H. de Castro Felix, F. P. M. Simoes, V. Teichrieb, M. dos Santos, H. Santiago, V. Sgotti, and H. Lott Neto. Insplad: A dataset and benchmark for power line asset inspection in uav images. _International journal of remote sensing_, 44(23):7294-7320, 2023.
*  G. Wang, S. Han, E. Ding, and D. Huang. Student-teacher feature pyramid matching for anomaly detection. _arXiv preprint arXiv:2103.04257_, 2021.
*  W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, J. Xu, B. Xu, J. Li, Y. Dong, M. Ding, and J. Tang. Cogvlm: Visual expert for pretrained language models. 2023.
*  J. Wyatt, A. Leach, S. M. Schmon, and C. G. Willcocks. Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 650-656, 2022.

*  J. Yang, Y. Shi, and Z. Qi. Dfr: Deep feature reconstruction for unsupervised anomaly segmentation. _arXiv preprint arXiv:2012.07122_, 2020.
*  O. E. Yetgin, O. N. Gerek, and O. Nezih. Ground truth of powerline dataset (infrared-ir and visible light-vl). _Mendeley Data_, 8(9), 2017.
*  J. Yoon, K. Sohn, C.-L. Li, S. O. Arik, C.-Y. Lee, and T. Pfister. Self-supervise, refine, repeat: Improving unsupervised anomaly detection. _arXiv preprint arXiv:2106.06115_, 2021.
*  Z. You, K. Yang, W. Luo, L. Cui, Y. Zheng, and X. Le. Adtr: Anomaly detection transformer with feature reconstruction. In _International Conference on Neural Information Processing_, pages 298-310. Springer, 2022.
*  V. Zavrtanik, M. Kristan, and D. Skocaj. Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8330-8339, 2021.
*  J. Zhang, X. Chen, Z. Xue, Y. Wang, C. Wang, and Y. Liu. Exploring grounding potential of vqa-oriented gpt-4v for zero-shot anomaly detection. _arXiv preprint arXiv:2311.02612_, 2023.
*  Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao. Analyzing and mitigating object hallucination in large vision-language models. 2023.
*  Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In _European Conference on Computer Vision_, pages 392-408. Springer, 2022.