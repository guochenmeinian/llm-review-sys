# StoryDiffusion: Consistent Self-Attention for

Long-Range Image and Video Generation

 Yupeng Zhou\({}^{1}\) Daquan Zhou\({}^{2}\) Ming-Ming Cheng\({}^{1,3}\) Jiashi Feng\({}^{2}\) Qibin Hou\({}^{1,3}\)\({}^{}\)

\({}^{1}\) VCIP & TMCC, CS, Nankai University \({}^{2}\) ByteDance Inc. \({}^{3}\) NKIARI, Futian, Shenzhen

https://StoryDiffusion.github.io

During the internship at ByteDance Inc. \({}^{}\)Project lead. \({}^{}\)Corresponding authors.

###### Abstract

For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a simple but effective self-attention mechanism, termed Consistent Self-Attention, that boosts the consistency between the generated images. It can be used to augment pre-trained diffusion-based text-to-image models in a zero-shot manner. Based on the images with consistent content, we further show that our method can be extended to long-range video generation by introducing a semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications.

## 1 Introduction

With extensive pre-training and advanced architectures, diffusion models have shown superior performance in generating very high-quality images and videos over previous generative-adversarial network (GAN) based methods . However, generating subject-consistent (e.g. characters with consistent identity and attire) images and videos to describe a story is still challenging for existing models. The commonly used IP-Adapter  taking an image as a reference could be used to guide the diffusion process to generate images similar to it. However, due to the strong guidance, the controllability over the generated content of the text prompts is reduced. On the other hand, recent state-of-the-art identity preservation methods, such as InstantID  and PhotoMaker , focus on identity controllability but the consistency of the attires and the scenarios cannot be guaranteed. Hence, in this paper, we aim to find a method that can generate images and videos with consistent characters in terms of both identity and attire while maximizing the controllability of the user via text prompts.

A common approach to preserve the consistency between different images (or frames in the context of video generation) is to use a temporal module . However, this requires extensive computational resources and data. Differently, we target to explore a lightweight method with minimum data and computational cost, or even in a zero-shot manner.

As evidenced by previous works [45; 19; 6], self-attention is one of the most important modules for modeling the overall structure of the generated visual content. Our main motivation is that we could use some shared reference image information to guide the self-attention calculation, the consistency between generated images is supposed to be improved clearly. As the self-attention weights are input-dependent, model training or fine-tuning might not be required. Following this idea, we propose Consistent Self-Attention, the core of our StoryDiffusion, which can be inserted into the diffusion backbone to replace the original self-attention in a zero-shot manner.

Different from the standard self-attention that operates on the tokens representing a single image, Consistent Self-Attention incorporates reference tokens sampled from reference images during the token similarity matrix calculation and token merging. The sampled tokens use the same set of \(Q\)-\(K\)-\(V\) weights and thus no extra training is required. As shown in Fig. 1, the generated images using Consistent Self-Attention successfully preserve the consistency in both identity and attire, which is vital for storytelling. Intuitively, Consistent Self-Attention builds correlations across images in the batch, generating consistent character images in terms of identity and attire, such as clothes. This enables us to generate subject-consistent images for storytelling.

Figure 1: Images and videos generated by our StoryDiffusion. (a) Comic generated by StoryDiffusion telling the story of a man who discovers a treasure while exploring the jungle. (b) Comic generated by StoryDiffusion describing the expedition to the moon by Lecun, with a reference image control  same as Fig. 7(b). (c) Videos generated by our StoryDiffusion. Click the image to play the video. Best viewed with _Acrobat Reader_. More generated videos can be found in the uploaded supplementary file.

For any given story text, we begin by dividing it into several prompts, with each prompt corresponding to an individual image. Then our method could generate highly consistent images that effectively narrate a story. To support long story generation, we also implement Consistent Self-Attention together with a sliding window across the generated consistent images. This removes the peak memory consumption's dependency on the input text length, making it possible to generate long stories. To stream the generated story frames into videos, we further propose Semantic Motion Predictor that can predict transitions between two images in the semantic spaces. We empirically found that predicting motions in the semantic space generates more stable results than the predictions in the image latent spaces. Combined with the pre-trained motion module , Semantic Motion Predictor can generate smooth video frames that are notably better than recent conditional video generation methods, such as SEINE  and SparseCtrl .

Our contributions are summarized below:

* We propose a training-free and hot-pluggable attention module, termed Consistent Self-Attention. It can maintain the consistency of characters in a sequence of generated images for storytelling with high text controllability.
* We propose a new motion prediction module that can predict transitions between two images in the semantic space, termed Semantic Motion Predictor. It can generate more stable long-range video frames that can be easily upscaled to minutes than recent popular image conditioning methods, such as SEINE  and SparseCtrl .
* We demonstrate that our approach could generate long image sequences or videos based on a pre-defined text-based story with the proposed Consistent Self-Attention and Semantic Motion Predictor with motions specified by text prompts. We term the new framework as StoryDiffusion.

## 2 Related work

### Controllable text-to-image diffusion generation

As an important sub-field of diffusion model applications [40; 17; 36; 38; 27; 52; 41], text-to-image generation [37; 33; 34], has attracted considerable attention recently. In addition, to enhance the controllability of text-to-image generation, a multitude of methods emerged as well. Among them, ControlNet  and T2I-Adapter  introduce control conditions, such as depth maps, pose images, or sketches, to direct the generation of images. MaskDiffusion  and StructureDiffusion  focus on enhancing the text controllability. There are also some works [30; 28] controlling the layout of generated images.

ID-Preservation, which is expected to generate images with a specified ID, is also a hot topic. According to whether test-time fine-tuning is required, these works can be divided into two major categories. The first one only requires fine-tuning a part of the model with a given image, such as Textual Inversion , DreamBooth , and Custom Diffusion . The other one, exemplified by IPAdapter  and PhotoMaker , leverages models that have undergone pre-training on large datasets, allowing the direct use of a given image to control image generation. Different from both of the two types, we focus on maintaining the subject consistency in multiple images, to narrate a story. Our Consistent Self-Attention is training-free and pluggable and can build connections across images within a batch to generate multiple subject-consistent images.

### Video generation

Due to the success of diffusion models in the field of image generation [37; 17], the exploration in the domain of video generation [13; 23; 42; 49; 54; 56] is also becoming popular. VDM  is among the first that extends the 2D U-Net from image diffusion models to a 3D U-Net to achieve text-based generation. Later works, such as MagicVideo  and Mindscope , introduce 1D temporal attention mechanisms, reducing computations by building upon latent diffusion models. Following Imagen, Imagen Video  employs a cascaded sampling pipeline that generates videos through multiple stages.

In addition to traditional end-to-end text-to-video (T2V) generation, video generation using other conditions is also an important direction. This type of methods generates videos with other auxiliary controls, such as depth maps [12; 14], pose maps [53; 21; 48; 29], RGB images [3; 7; 32], or other guided motion videos [59; 51].

Our video generation method focuses on transition video generation, which is expected to generate videos with a given start frame and an end frame. Typical related works are SEINE  and SparseC-trl . SEINE randomly masks video sequences as the initial input of the video diffusion models in training to enable the predictions of the transition between two frames. SparseCtrl introduces a sparse control network to synthesize the corresponding control information for each frame using sparse control data, thereby directing the generation of videos. However, the aforementioned transition video generation methods rely solely on temporal networks in image latent space for the predictions of intermediate content. Thus, these methods often perform poorly on complex transitions, such as large-scale movements of characters. Our StoryDiffusion aims to perform predictions in image semantic spaces to achieve better performance and can handle larger movements, which we will show in our experiment section.

## 3 Method

Our method can be divided into two stages, as shown in Fig. 2 and Fig. 3. In the first stage, StoryDiffusion utilizes Consistent Self-Attention to generate subject-consistent images in a training-free manner. These consistent images can be directly applied to storytelling and can also serve as input for the second stage. In the second stage, our StoryDiffusion create consistent transition videos based on these consistent images.

### Training-free consistent images generation

The key to addressing the above issues lies in how to maintain consistency of characters within a batch of images. This means we need to establish connections between images within a batch during generation. The previous image and video editing methods [6; 50] use DDIM inversion and insert additional keys and values in attention calculation  to keep similarity. Unlike existing methods [24; 20; 11] applied to single images or highly similar video clips, we aim to generate a set of images where the character remains consistent, yet each image portrays different scenes and actions, for use in anime production or story-boarding. Therefore, we aim to share intermediary tokens within

Figure 2: The Pipeline of StoryDiffusion to generating subject-consistent images. To create subject-consistent images to describe a story, we incorporate our Consistent Self-Attention into the pre-trained text-to-image diffusion model. We split a story text into several prompts and generate images using these prompts in a batch. Consistent Self-Attention builds connections among multiple images in a batch for subject consistency.

a batch of images to enable mutual interaction through self-attention computation, thereby preserving consistency. We obtain these intermediary tokens by randomly sampling some pixels from the image batch before attention calculation, enabling plug-and-play capability and eliminating the need for training. We name this operation as Consistent Self-Attention and insert it into the location of the original self-attention in the existing U-Net architecture of image generation models and reuse the original self-attention weights.

Formally, given a batch of image features \(^{B N C}\), where \(B\), \(N\), and \(C\) are the batch size, number of tokens in each image, and channel number, respectively, we define a function \((X_{k},X_{q},X_{v})\) to calculate self-attention. \(X_{k},X_{q}\), and \(X_{v}\) stand for the query, key, and value used in attention calculation, respectively. The original self-attention is performed within each image feature \(I_{i}\) in \(\) independently. The feature \(I_{i}\) is projected to \(Q_{i}\), \(K_{i}\), \(V_{i}\) and sent into the attention function, yielding:

\[O_{i}=(Q_{i},K_{i},V_{i}).\] (1)

To build interactions among the images within a batch to keep subject consistency, our Consistent Self-Attention samples some tokens \(S_{i}\) from other image features in the batch:

\[S_{i}=(I_{1},I_{2},..,I_{i-1},I_{i+1},...,I_{B-1},I_{B }),\] (2)

where \(\) denotes the random sampling function. After sampling, we pair the sampled tokens \(S_{i}\) and the image feature \(I_{i}\) to form a new set of tokens \(P_{i}\). We then perform linear projections on \(P_{i}\) to generate the new key \({K_{P}}_{i}\) and value \(V_{P_{i}}\) for Consistent Self-Attention. Here, the original query \(Q_{i}\) is not changed. Finally, we compute the self-attention as follows:

\[O_{i}=(Q_{i},K_{Pi},V_{Pi}).\] (3)

Given the paired tokens, our method performs the self-attention across a batch of images, facilitating interactions among features of different images. This type of interaction promotes the model to the convergence of characters, faces, and attires during the generation process. Despite the simple and training-free manner, our Consistent Self-Attention can efficiently generate subject-consistent images, which we will demonstrate in detail in our experiments. These images serve as illustrations to narrate a complex story as shown in Fig. 2. To make it clearer, we also show the pseudo code in Algorithm 1 in the Appendix.

### Semantic Motion Predictor for video generation

Illustrated in Fig. 3, the sequence of the generated character-consistent images can be further refined to videos by inserting frames between each pair of adjacent images. This can be regarded as a video generation task with known start and end frames as conditions. However, we empirically observed

Figure 3: The pipeline of our method for generating transition videos for obtaining subject-consistent images, as described in Sec. 3.1. To effectively model the characterâ€™s large motions, we encode the conditional images into the image semantic space for encoding spatial information and predict the transition embeddings. These predicted embeddings are then decoded using the video generation model, with the embeddings serving as control signals in cross-attention to guide the generation of each frame.

that recent methods, such as SparseCtrl  and SEINE , cannot join two condition images stably when the difference between the two images is large. We argue that this limitation stems from their sole reliance on temporal modules to predict intermediate frames, which may be not enough to handle the large state gap between the image pair. The temporal module operates within pixels on each spatial location independently, therefore, there may be insufficient consideration of spatial information when inferring intermediate frames. This makes it difficult to model the long-distance and physically meaningful motion.

To address this issue, we propose Semantic Motion Predictor, which encodes the image into the image semantic space to capture the spatial information, achieving more accurate motion prediction from a given start frame and an end frame. More specifically, in our Semantic Motion Predictor, we first use a function \(E\) to establish a mapping from the RGB images to vectors in the image semantic space, encoding the spatial information. Instead of directly using linear layers as \(E\), we utilize a pre-trained CLIP image encoder as \(E\) to leverage its zero-shot capabilities for enhancing performance. Using \(E\), the given start frame \(F_{s}\) and end frame \(F_{e}\) are compressed to image semantic space vectors \(K_{s},K_{e}\).

\[K_{s},K_{e}=E(F_{s},F_{e}).\] (4)

Subsequently, in the image semantic space, we train a transformer-based structure predictor to perform predictions of each intermediate frame. The predictor first performs linear interpolation to expand the two frames \(K_{s}\) and \(K_{e}\) into sequence \(K_{1},K_{2},...,K_{L}\), where \(L\) is the required video length. Then, the sequence \(K_{1},K_{2},...,K_{L}\) is sent into a series of transformer blocks \(B\) to predict the transition frames:

\[P_{1},P_{2},...,P_{l}=B(K_{1},K_{2},...,K_{l}).\] (5)

Next, we need to decode these predicted frames in the image semantic space into the final transition video. Inspired by the image prompt methods , we position these image semantic embeddings \(P_{1},P_{2},...,P_{L}\) as control signals, and the video diffusion model as the decoder to leverage the generative ability of the video diffusion model. We also insert additional linear layers to project these embeddings into keys and values, involving into cross-attention of U-Net.

Formally, during the diffusion process, for each video frame feature \(V_{i}\), we concatenate the text embeddings \(T\) and the predicted image semantic embeddings \(P_{i}\). The cross-attention is computed as follows:

\[V_{i}=(V_{i},(T,P_{i}),(T,P_{i})).\] (6)

Similar to previous video generation approaches, we optimize our model by calculating the MSE loss between \(L\) frames predicted transition video \(O=(O_{1},O_{2},...,O_{L})\) and \(L\) frame ground truth \(G=(G_{1},G_{2},...,G_{L})\):

\[Loss=(G,O).\] (7)

By encoding images into an image semantic space for integrating spatial positional relationships, our Semantic Motion Predictor could better model motion information, enabling the generation of smooth transition videos with large motion. The results and comparisons that showcase the notable improvements can be observed in Fig. 1 and Fig. 6.

## 4 Experiments

### Implementation details

For the generation of subject-consistent images, due to the training-free and pluggable property, we implement our method on both Stable Diffusion XL  and Stable Diffusion 1.5 . To align with the comparison models, we conduct comparisons on the Stable-XL model using the same pre-trained weights. All comparison models utilize 50-step DDIM sampling, and the classifier-free guidance score  is consistently set to 5.

For the generation of consistent videos, we implement our method based on the Stable Diffusion 1.5 pertained model and incorporate a pretrained temporal module  to enable video generation. All comparison models adopt a 7.5 classifier-free guidance score and 50-step DDIM sampling. Following the previous methods [12; 7], we use the Webvid10M  dataset to train our transition video model. For training our transition video model, we utilize the AnimateDiff V2 motion module  as our initial weights of the temporal module and and fine-tune the module. We then set our learning rate at 1e-4 and conduct training 100k iterations for our Semantic Motion Predictor on 8 A100GPUs. To encode the conditional images into the image semantic space, we utilize the OpenCLIP ViT-H-14 [35; 8] pre-trained model. Our Semantic Motion Predictor incorporates 8 transformer layers, with a hidden dimension of 1024 and 12 attention heads.

### Comparisons of consistent image generation

We mainly evaluate our method for generating subject-consistent images by comparing it with the two most recent ID preservation methods, IP-Adapter  and Photo Maker . To test the performance, we use GPT-4 to generate twenty character prompts and one hundred activity prompts to describe specific activities. The format of our character prompts is "[adjective] [group or profession] [wearing clothing]" and the format of activity prompts is "[action] [location or object]". We combine character prompts with activity prompts to obtain groups of test prompts. For each test case, we use the three comparison methods to generate a group of images that depict a person engaging in different activities to test the model's consistency. Since IP-Adapter and PhotoMaker require an additional image to control the ID of the generated images, we first generate an image of a character to serve as the control image. We conduct both qualitative and quantitative comparisons to comprehensively evaluate the performance of these methods on consistent image generation.

Figure 4: Comparison of consistent image generation with recent methods.

Figure 5: Additional comparison of our StoryDiffusion with recent storybook generation methods, The Chosen One , ConsiStory  and Zero-shot coherent storybook .

**Qualitative comparisons.** The qualitative result is shown in Fig. 4. Our StoryDiffusion can generate highly consistent images, whereas other methods, IP-Adapter and PhotoMaker, may produce images with inconsistent attire or diminished text controllability. For the first example, the IP-Adapter method generates an image lost "telescope" with the text prompt "Stargazing with a telescope". PhotoMaker generates images matching the text prompt, but there are notable discrepancies in the attire across the three generated images. The third-row images generated by our StoryDiffusion exhibit consistent faces and attire with better text controllability. For the last example "A focused gamer wearing oversized headphones", IP-Adapter loses the "dog" in the second image and the "cards" in the third image. The images generated by PhotoMaker could not maintain the attire. Our StoryDiffusion still generates subject-consistent images, with the same face, and same attire, and conforms to the description in the prompt. To further demonstrate the effectiveness of our method, we compare our method with concurrent or recent storybook generation works, including The Chosen One , ConsiStory , and Zero-shot Coherent Storybook  in Fig. 5. Our approach not only outperforms these methods but also offers greater flexibility and faster inference times. By contrast, The Chosen One requires time-consuming LoRA self-training for each sample; Zero-shot Coherent Storybook necessitates a two-step process, first generating images and then embedding with Iterative Coherent Identity Injection; and ConsiStory involves iterative segmentation mask calculations during diffusion to maintain consistency.

**Quantitative comparisons.** We evaluate the quantitative comparison and show the results in Tab. 1. We evaluate two metrics, the first one is text-image similarity, which calculates the CLIP Score between the text prompts and the corresponding images. The second aspect is character similarity, measured by the CLIP Scores of character images after using the background removal method RMBG-1.4. Our StoryDiffusion achieves the best performance on both quantitative metrics, which shows our method's robustness in maintaining character meanwhile conforming to prompt descriptions.

### Comparisons of transition videos generation

In transition video generation, we conduct comparisons with the two state-of-the-art methods, SparseCtrl  and SEINE , to evaluate our performance. We randomly sample around 1000 videos as the test dataset. We employ the three comparison models to predict the intermediate frames of a transition video, given the start and end frames, in order to assess their performance.

**Qualitative comparisons.** We conduct the qualitative comparison of transition video generation and show the results in Fig. 6. Our StoryDiffusion notably outperforms SEINE  and SparseCtrl , generating transition videos that are smooth and physically plausible. For the first example, two

   Metric & IP-Adapter  & Photo Maker  & StoryDiffusion (ours) \\  Text-Image Similarity & 0.6129 & 0.6541 & **0.6586** \\ Character Similarity & 0.8802 & 0.8924 & **0.8950** \\   

Table 1: Quantitative comparisons of consistent image generation. Our StoryDiffusion achieves better text similarity and subject similarity even without any training.

Figure 6: Comparisons of transition video generation with the recent state-of-the-art methods.

people kissing underwater, the intermediate frames generated by SEINE are corrupted, and there is a direct jump to the final frame. SparseCtrl generates results with slightly better continuity, but the intermediate frames still contain corrupted images, with numerous hands appearing. However, our StoryDiffusion succeeds in generating videos with very smooth motion without corrupted intermediate frames. For the second example, the intermediate frames generated by SEINE have corrupted arms. SparseCtrl, on the other hand, fails to maintain consistency in appearance. Our StoryDiffusion generates consistent videos with excellent continuity. For the last example, the video we generate adheres to physical spatial relationships, unlike SEINE and SparseCtrl, which only change the appearance in the transition. More visual examples can be found in the Sec. A.

**Quantitative comparisons.** Following previous works [12; 58], we compare our method with SEINE and SparseCtrl with four quantitative metrics, including LPIPS-_f_, LPIPS-_a_, CLIPSIM-_f_, and CLIPSIM-_a_, as shown in Tab. 2. LPIPS-_f_ and CLIPSIM-_f_ measure the similarities between the first frame and other frames, which reflect the overall continuity of the video. LPIPS-_a_ and CLIPSIM-_a_ measure the average similarities between consecutive frames, which reflect the continuity between frames. FVD and FID are also computed to evaluate generation quality. Our model outperforms the other two methods across all four quantitative metrics. These quantitative experimental results demonstrate the strong performance of our method in generating consistent and seamless transition videos.

### Ablation study

**User-specified ID generation.** We conduct an ablation study to test the performance of generating consistent images with a user-specified ID. Since our Consistent Self-Attention is pluggable and training-free, we combine our Consistent Self-Attention with PhotoMaker, giving images to control

   Methods & LPIPS-_f_ (\(\)) & LPIPS-_a_ (\(\)) & CLIPSIM-_f_ (\(\)) & CLIPSIM-_a_ (\(\)) & FVD (\(\)) & FID(\(\)) \\  SEINE & 0.4332 & 0.2220 & 0.9259 & 0.9736 & 321 & 140 \\ SparseCtrl & 0.4913 & 0.1768 & 0.9032 & 0.9756 & 429 & 181 \\  Ours & **0.3794** & **0.1635** & **0.9606** & **0.9870** & **271** & **109** \\   

Table 2: Quantitative comparisons with state-of-the-art transition video generation models.

Figure 7: Ablation study. (a) Evaluations of the impact of different sampling rates in Consistent Self-Attention. (b) We explore the introduction of external control IDs to govern the generation of characters. Our StoryDiffusion can generate consistent images that conform to the ID images.

the characters for consistent image generation. The results are shown in Fig. 7. With the control of the ID image, our StoryDiffusion can still generate consistent images conformed to the given control ID, which strongly indicates the scalability and plug-and-play capability of our method.

**Sampling Rate of Consistent Self-Attention.** Our Consistent Self-Attention approach samples tokens from other images within a batch, incorporating them into keys and values during self-attention. The original intent of random sampling is to maintain consistency while avoiding excessive structural information, thereby preventing the weakening of text control and maintaining diversity in poses. We conducted an ablation study to find the optimal sampling rate (results in Fig. 7). A sampling rate of 0.3 does not maintain subject consistency, as seen in the third column of images on the left in Fig. 7, whereas higher rates do. Quantitatively, higher sampling rates can over-correlate images and reduce text control, while lower rates weaken character consistency. We also added a quantitative comparison with the grid sampling method (Tab. 3). While grid sampling better preserves character consistency, it sacrifices text prompt controllability, making our adjustable sampling ratio ideal for balancing both factors. In practice, we set the sampling rate to 0.5, balancing consistency with minimal diffusion process impact.

### User study

We conduct a user study with 79 people. Each user is assigned 50 questions to evaluate the effectiveness of our subject-consistent image generation method and transition video generation method. For subject-consistent image generation, we compare with the recent state-of-the-art methods IP-Adapter and PhotoMaker. In transition video generation, we compare with recent state-of-the-art methods SparseCtrl and SEINE. For fairness, the order of the results is randomized, and users are not informed about which generation model corresponds to each result. As shown in Tab. 4, whether for subject-consistent image generation or transition video generation, our model demonstrates an overwhelming advantage.

## 5 Conclusions

In this paper, we propose StoryDiffusion, a novel method that can generate consistent images in a training-free manner for storytelling and transition these consistent images into videos. Our Consistent Self-Attention builds connections among multiple images to efficiently generate images with consistent faces and clothing. We further propose the Semantic Motion Predictor to transition these images into videos and better narrate the story. We hope that our StoryDiffusion can inspire future controllable image and video generation endeavors.

**Broader impact.** Our StoryDiffusion can generate high-quality character-consistent pictures and videos. Certainly, similar to the previous image and video generation methods, our method may encounter some ethical issues. The generated portraits and videos may be used improperly, such as for fabricating false information. We strongly hope that the use of relevant technologies has clear responsibilities and strengthens legal and technical supervision to promote proper use.

   Consistent Images Generation & IP-Adapter & PhotoMaker & StoryDiffusion (ours) \\  User Preference & 20.8 \% & 10.9 \% & **68.3 \%** \\   Transition Video Generation & SEINE & SparseCtrl & StoryDiffusion (ours) \\  User Preference & 5.9 \% & 9.6 \% & **84.5 \%** \\   

Table 4: **User study on subject-consistent image generation and transition video generation.**

   Sampling Method & Rand 0.3 & Rand 0.5 & Rand 0.7 & Grid 0.5 \\  Character Similarity & 86.39\% & 88.37\% & 89.26\% & **89.29**\% \\ CLIP Score & **57.14**\% & 57.11\% & 56.96\% & 56.53\% \\   

Table 3: Ablation study on different random sampling ratios for both random sampling and grid sampling.

**Acknowledgement.** This work is supported by the National Science Fund of China (No. 62225604) and the Fundamental Research Funds for the Central Universities (Nankai University, 070-63223049). We also acknowledge "Science and Technology Yongjiang 2035" key technology breakthrough plan project (2024Z120).