# TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives

Maitreya Patel\({}^{}\)\({}^{}\)Abhiram Kusumba\({}^{}\)\({}^{\)Sheng Cheng\({}^{}\)Changhoon Kim\({}^{}\)

**Tejas Gokhale\({}^{}\)Chitta Baral\({}^{}\)Yezhou Yang\({}^{}\)**

\({}^{}\)Arizona State University University of Maryland, Baltimore County

tripletclip.github.io

Corresponding author: maitreya.patel@asu.edu. \(\) indicates the equal contribution.

###### Abstract

Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between textual and visual modalities to learn representations. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating "hard" negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over \(9\%\) on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our code, models, and data are available at: tripletclip.github.io.

## 1 Introduction

Large-scale vision-language models, such as CLIP , have significantly advanced multi-modal learning by employing contrastive learning to acquire shared semantic representations from paired datasets. This approach has resulted in improved performance in vision-language tasks as well as zero-shot image classification  and segmentation . Beyond vision-language tasks, the individual components of these models, such as the vision encoder and the language encoder, are integral to several multimodal architectures and generative models such as multimodal large language models (MLLMs)  and text-to-image (T2I) diffusion models . Yet, compositional reasoning remains challenging and multimodal models continue to exhibit naive "bag of words" behavior, frequently failing to distinguish between expressions like "bulb in the grass" and "grass in the bulb" . Addressing this challenge remains critical for enhancing vision-language models and their downstream applications.

Contrastive learning of representations benefits from "hard negative samples" (i.e., points that are difficult to distinguish from an anchor point) . However, at each optimization step for training CLIP, image-text pairs are _randomly_ sampled from the training dataset - this random sampling seldom exposes the model to highly similar negative pairs. We hypothesize that the limited compositional understanding of CLIP may stem from such issues in the optimization objective and sampling from training datasets. A straightforward solution could involve iteratively identifying hard negative pairs for each training iteration. However, due to the noisy captions and the scarcity of such pairs in existing datasets, prior work generates hard negative captions as a form of augmentation using rule-based strategies . For instance, given an image-text pair labeled "a brown horse", an additionalnegative caption "a blue horse" might be introduced. However in prior work, image data is not subjected to similar hard negative semantic augmentation during training; this is mainly because of the difficulty of making semantic perturbations at the pixel levels compared to sentence perturbation. While the text-only augmentation strategies have improved the models' compositional understanding to a certain extent, it raises an intriguing question: _could incorporating hard negative augmentation for both text and image modality further enhance the compositional reasoning capabilities of vision-language models?_

Motivated by this, in this paper, we introduce a novel, simple, and yet highly effective strategy for integrating hard negative images as well as hard negative text to enhance the compositional understanding of vision-language models. Recent developments in text-to-image diffusion models have opened up possibilities for performing semantic perturbations within images . Existing works have evaluated the impact of creating synthetic data for text-to-image generative models [3; 6]. However, it remains less explored how these generative models can benefit the CLIP-like models. To tackle this challenge, our approach leverages the in-context learning capabilities of LLMs to produce realistic, linguistically accurate negative captions . We then employ a pre-trained text-to-image diffusion model to create images corresponding to these captions, thereby enriching any given image-text dataset with valuable hard negatives that foster improved reasoning. This resulting TripletData comprises 13M image-text pairs to complement the CC3M and CC12M datasets .

We developed TripletCLIP, which incorporates hard negative image-text pairs effectively by using them to optimize a novel triplet contrastive loss function. Extensive experiments on the CC3M and CC12M datasets and various downstream tasks with an equal compute budget demonstrate that TripletCLIP significantly enhances compositional reasoning. Notably, TripletCLIP results in more than 9% and 6% absolute improvement on the SugarCrepe benchmark compared to LaCLIP and NegCLIP, respectively. TripletCLIP also improves zero-shot classification and image-text retrieval performance with similar training-time concept diversity. An investigation into the effects of increasing training-time concept diversity revealed that baseline models consistently under-performed in compositional tasks despite an increase in integrated knowledge, while TripletCLIP demonstrated significant improvements. In summary, our **key contributions** are as follows:

* We introduce a novel CLIP pre-training strategy that employs hard negative images in conjunction with triplet contrastive learning to enhance compositionality.
* TripletCLIP consistently improves across downstream tasks, demonstrating the effectiveness of synthesizing hard negative image-text pairs.
* Our extensive ablations on the choice of the loss function, modality-specific pre-training, the increase in concept diversity, and filtering high-quality TripletData provide deeper insights into the utility of hard negative image-text pairs for CLIP pre-training.
* Ultimately, we present a promising avenue where synthetic contrastive datasets significantly improve reasoning capabilities, leading to the creation and release of the TripletData -- a 13M contrastive image-text dataset.

## 2 Related Work

**Vision-Language Models.** Recent advancements, including ALIGN  and CLIP , have gained significant interest due to their capability to learn transferable semantic representations across multiple modalities through contrastive learning. These models facilitate downstream tasks such as zero-shot classification , image-text retrieval [60; 2], visual grounding/reasoning , text-to-image generation [52; 37; 38; 24], semantic segmentation [62; 23], and various evaluations [19; 47]. Subsequent research has sought to enhance various aspects of these models, including data efficiency , hierarchical representation learning , and the quantization of latent spaces for more stable pre-training . LiT  employs a pre-trained frozen CLIP vision encoder to fine-tune a BERT-like text encoder , achieving notable improvements in zero-shot transfer performance. Similarly, BLIP-2  combines contrastive pre-training with the next-token prediction for image captioning during training. However, these approaches generally presume the availability of high-quality data. In contrast, TripletCLIP focuses on leveraging the proposed hard negative contrastive dataset and incorporating triplet contrastive pre-training for compositional data. This approach is orthogonal to prior works.

**Data for Contrastive Pre-training.** The effectiveness of maximizing mutual information between modalities heavily relies on the quality of extensive, web-scraped datasets that ideally encompass all possible concepts and knowledge. For instance, despite its noise, the LAION dataset [48; 16], which includes more than 5 billion internet images paired with alt-text captions, is a primary resource. Studies show that over 1 billion data points are necessary to match the performance of the original CLIP model [16; 56]. Recent works like DataComp  and MetaCLIP  have focused on creating smaller, high-quality datasets by applying stringent filters and ensuring wordnet  synset-level concept diversity. Nevertheless, the inherently noisy nature of internet-scraped datasets can degrade model performance. Studies such as SynthCLIP  demonstrate that tripling the volume of fully synthetic data is required to equal the efficacy of real data. Other efforts like VeCLIP  and LaCLIP  enhance dataset quality by using generative language models to re-caption existing images, significantly boosting performance.

**Compositionality for vision-language.** Despite the increased emphasis on data quality and modeling techniques, mastering compositionality remains a significant challenge for vision-language models. Benchmarks like ARO , VALSE , and CREPE  have been developed to assess models' abilities to handle compositional data. SugarCrepe , in particular, offers a large-scale, systematic framework for such evaluations. Previous methods primarily focused on identifying hard negatives within existing datasets or generating synthetic negative captions [58; 61; 12; 11; 57; 49]. However, these rule-based generated captions are often unrealistic and linguistically flawed, leading to suboptimal model performance on complex datasets like SugarCrepe. A handful of works focus on finding negative images.  propose utilizing the video data.  focuses on object-centric image-editing to synthesize the negative images.  utilizes the simulation-based data negative data.

Contrary to prior approaches that predominantly add unrealistic negative captions or very constrained negative images that are either very synthetic or object-focused, this work introduces TripletCLIP, which centers on generating naturally occurring _hard negative image-text pairs_. We propose a novel triplet contrastive learning strategy that effectively utilizes these challenging data pairs. Additionally, while our method is distinct, integrating advancements that refine contrastive learning could potentially boost TripletCLIP's efficacy further.

## 3 Method

This section begins with an overview of the contrastive learning algorithm used by CLIP and NegCLIP. We then describe the synthetic data generation pipeline for generating hard negatives using LLMs and T2I models and introduce triplet contrastive learning which forms the basis of TripletCLIP. A high-level comparison between prior work and TripletCLIP can be found in Figure 1.

### Preliminaries

The goal for self-supervised contrastive learning , when dealing with inputs from a single modality, is to use a feature extractor (\(F\)) to encode inputs and their augmentations and minimize

Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. \((x,y)\) represents the positive a image-text pair, and \((x^{},y^{})\) represents the corresponding negative image-text pair.

the InfoNCE loss  between the two encodings. CLIP is designed for multimodal settings (for example, vision and language inputs) - this entails using two encoders (one for each modality).

Let \(\) and \(\) represent two modalities and \(=\{(x_{i},y_{i})\}_{i=1,,M}\), be the training dataset, where \(x_{i}\) and \(y_{i}\). The goal is to train two modality-specific encoders, \(F_{}\) and \(F_{}\), by minimizing the InfoNCE loss between the normalized features extracted from the encoders.

\[^{CL}_{}=_{i=1}^{ N}}(x_{i}),F_{}(y_{i})/ )}{_{k=1}^{N}( F_{}(x_{i}),F_{}(y_{k })/)},\] (1)

where \(\) represents cosine similarity and \(\) is the trainable temperature parameter. For simplicity, we do not show feature normalization in the InfoNCE loss. Similarly, we can define the \(^{CL}_{}\) training loss. The combined CLIP total training objective is given as, \(_{CLIP}=^{CL}_{}+ ^{CL}_{}\). By minimizing this training loss, both encoders learn representations that maximize the mutual information between two modalities.

NegCLIP introduces synthetic augmentations to generate "hard" negative captions (\(y^{}_{i}^{}\)) by performing semantic inverting perturbations to the reference captions (\(y_{i}\)). Therefore, the single modality-specific hard negative augmentation-based training loss can be formulated as:

\[^{NegCL}_{;^{}}= _{i=1}^{N}}(x_{i}),F_{ }(y_{i})/)}{_{k=1}^{N}((F_{}(x_{i}), F_{}(y_{k}))/)+_{m=1}^{N}((F_{}(x_{i}),F_{ }(y^{}_{m})))/)}.\] (2)

The total loss for NegCLIP for image modality (\(\)) and text modality (\(\)) is given by:

\[_{NegCLIP}(,,^{})=^{CL}_{}+^{NegCL}_{ ;^{}}.\] (3)

In Eq. 2, the negative samples are generated only for language modality as it is easy to make semantic-level perturbations. Existing methods have not explored performing semantic perturbations in the image modality to create hard negatives. In this work, we demonstrate how hard negatives can be created in the image modality by leveraging the semantic language grounding and photorealism of text-to-image diffusion models. Our novel hard negative generation pipeline and refined training objective seeks to bridge the significant gap identified in literature.

Figure 2: Examples image-text pairs from TripletData. In each block, a positive pair from CC3M is on the left and corresponding negatives from TripletDataare shown on the right.

### TripletData: Image-text hard negative data augmentations

To generate high-quality hard negative image-text pairs, we follow a two-step procedure. The first stage is to generate hard negative captions from the ground truth positive caption. Second, to generate images corresponding to the hard negative captions as negative images. The AltText captions from the existing web-scrapped datasets are very noisy, leading to the noisy and unreliable generation of hard negatives. Therefore, we build upon the existing work LaCLIP, which first rewrites the captions using LLM from the existing data that are linguistically accurate. Figure 2 illustrates several examples of positive and corresponding negative image-text pairs.

**Generating hard negative captions.** Existing works perform random swapping, replacing, and adding actions between the nouns, attributes, and relations of the positive caption . This method results in nonsensical and grammatically incorrect artifacts, such as "a person riding on four slope," which impedes the generation of negative images, ultimately leading to diminishing performance on harder benchmarks . Therefore, we utilize the in-context learning ability of LLMs to generate negative captions. The choice of LLM is a trivial task as long as they provide hard negative captions. We find that Mistral-7B-Instruct-v0.22 performs reasonably better on our goal, and the output is easy to parse. We generate the negative captions in batches to speed up the generation process. Generating the 13M negative captions takes only 3 days on 8xRTX A6000. Instead of generating multiple hard negative captions, we find that a single high-quality hard negative caption is enough to improve the performance compared to the traditional NegCLIP style caption generation (see NegCLIP++ results in Table 4). We provide examples of various types of negative captions in the appendix. Specifically, we provide the following prompt to LLM:

**Generating hard negative images.** Typically, semantic perturbations within images require tools like image editing, which are resource-intensive and cannot be scaled. Remember that we want to provide additional ground truth references for the negative caption. Therefore, we propose to utilize the negative captions from the previous stage to generate the respective reference images directly for pre-training. As the previous stage generates negative captions that are linguistically correct, it becomes easier for image-generative models to synthesize the respective images precisely. We utilize pre-trained text-to-image diffusion models to generate the corresponding images. Specifically, we select SDXL-turbo  due to its relatively faster generation speed. After applying various inference time optimizations, we can generate 13M negative images within 2 days using 30 v100 GPUs. We provide various examples of the hard negative image-text pairs in the appendix.

**Analyzing difficulty of the hard TripletData.** Let's assume we have positive and negative image-text pairs from the TripletData, \((x_{i},y_{i})\) and \((x^{}_{i},y^{}_{i})\), respectively. If the data is truly hard negative, existing pre-trained models should struggle to find the correct image-text pairs (i.e., \(cos(x_{i},y_{i})>cos(x_{i},y^{}_{i})\)). Following winoground, we measure the text-score, image-score, and group-score to evaluate the popular pretrained CLIP models. Table 1 shows that even CLIP models trained on billions of data struggle to get near human performance on TripletData, which is less difficult than winoground. Importantly, the goal of generating hard negative samples isn't to add more diversity

   & **Text Score** & **Grp Score** \\   VI-B32 & 40.29 & 68.17 & 36.53 \\ ViL-14 & 44.84 & 69.21 & 40.91 \\ ViT-bigG & 42.94 & 77.61 & 40.98 \\ Sigig-v0400m & 44.24 & 71.27 & 26.10 \\   & **88.50** & **89.50** & **85.50** \\  

Table 1: Winoground-style evaluation of pretrained CLIP models on TripletData.

   \\   **CCM** \\ **(Negative Only)** \\  } & **TripletData** & **Intersection** \\  \# unique & 59094 & 59616 & 62741 & **55969** \\ \# total synsets & 231M & 215M & 446M & - \\  

Table 2: Wordnet synset analysis of captions from CC3M and TripletData.

in terms of unique concepts during the training but to add diversity in semantic meanings. Therefore, we measure the unique wordnet synsets in CC3M _vs._ TripletData. From Table 2, it can be observed that TripletData does not add any new concepts but uses existing concepts to provide negative samples that are semantically different. To summarize, TripletData contains the relatively hard negative image-text pairs that current models find difficult to differentiate.

### TripletCLIP

Prior works have demonstrated the value of hard negative captions for enhancing the compositionality of CLIP models via \(_{NegCLIP}\) as the key training objective (Eq. 2) [58; 61]. However, it remains elusive if negative images alone can benefit or not. We conduct modality-specific ablations, reporting the average performance across the diverse set of benchmarks in Table 3 (we provide more details about experiments in Section 4). Our findings indicate that both "hard" negative captions and images individually boost performance when compared to LaCLIP. However, this initial empirical experiments to train the CLIP model on hard negative images (i.e., NegImage) by minimizing \(_{NegCLIP}(,,^{})\) reveal that negative images alone cannot improve the compositionality significantly (see Table 3). We hypothesize that images contain low-level information, making it difficult to train the model using images as negative examples. Aligning with our initial motivation and building upon this crucial insight, we propose to utilize the negative images to regularize the effect of negative captions and to stabilize the pre-training. Therefore, to utilize these hard negative image-text pairs from the previous stage more effectively, we propose to focus on two triplets (\(,,^{}\)) and (\(^{},^{},\)), hence, the final triplet contrastive learning training objective is defined as:

\[_{TCL}=_{NegCLIP}(,,^{ })+_{NegCLIP}(^{},^{}, ).\] (4)

Intuitively, the second term introduces the additional form of supervision that hard negative images are closer to the corresponding negative captions than positive captions. This allows the system to understand that if the positive image does not represent the negative caption "blue horse," then what does this caption entail? Through this strategic alternation of hard negative image-text pairs for the TripletCLIP, we improve compositionality and image-text understanding of the vision-language model (see Table 3). We provide the pseudo-code in the appendix and the code in supplementary materials. This simple yet effective strategy elevates the training of the CLIP, offering a scalable framework to improve overall performance.

## 4 Experiments & Results

### Experiment Setup

**Pretraining Datasets.** We utilize the CC3M and CC12M datasets, which comprise 2.6M and 8.6M image-text pairs, respectively. Following the approach demonstrated by LaCLIP, we use LLM-rewritten captions to replace noisy original captions. For NegCLIP, we introduce four negative captions per positive image-text pair, focusing on semantic inverting perturbations across four categories: attribute, relation, object, and action . This generates approximately 10.4M and 34.4M text-only augmentations for CC3M and CC12M, respectively. To train the TripletCLIP, we create augmentations (TripletData) for both datasets to integrate hard negatives effectively. We produce one augmentation per image-text pair, adding 2.6M and 8.6M image-text augmented pairs for CC3M and CC12M, respectively. Finally, we perform all the ablations on the CC3M dataset.

  
**Models** & **Negative Captions** & **Negative Images** & **SugarCrepe** & **Retrieval** & **ImageNet1k** \\ 
**LaCLIP** & \(\) & \(\) & 54.09 & 8.19 & 3.79 \\
**NegImage** & \(\) & ✓ & 56.28 & 9.20 & 4.48 \\
**NegCLIP++** & ✓ & \(\) & 61.69 & 8.36 & 3.84 \\
**TripletCLIP** & ✓ & & **63.49** & **16.42** & **7.31** \\   

Table 3: **Importance of image-text hard negatives.** We measure the importance of various modality-specific hard negatives on SugarCrepe, image-text retrieval, and ImageNet1k. We find that TripletCLIP results into the most optimal solution. **Bold** number indicates the best performance.

**Baselines.** We train LaCLIP, LaCLIP with real hard negatives (LaCLIP+HN), and NegCLIP from scratch to ensure consistency and fairness in our comparisons. As NegCLIP's rule-based augmentations closely resemble some compositional benchmarks, so we introduce NegCLIP++ as an improved baseline. NegCLIP++ incorporates hard negative captions generated using LLM from TripletData, enhancing the language comprehension compared to standard NegCLIP.

**Implementation Details.** Our experiments employ the ViT-B/32  model architecture. To guarantee fair comparisons, we retrain all baseline models using identical hyperparameters. Since the overall training data for NegCLIP and TripletData is more than the baseline datasets, we align the number of iterations across all models to equalize the number of image-text pairs seen during training, similar to the strategy used in DataComp. The batch size is fixed to 1024 with the AdamW optimizer at a maximum learning rate of 0.0005, employing cosine decay. Training durations are set at approximately 100k iterations for CC3M and 200k iterations for CC12M. All models are trained on a single A100 (80GB) GPU using bf16 precision. The final training-related experiments and ablations will cost about 1200 A100 GPU hours. We leave the experiments on increasing the data and model size as future works for the community, as scaling further is not viable in the academic budget.

**Downstream Datasets.** The primary objective of this study is to enhance the compositional capabilities of CLIP models. We mainly evaluate TripletCLIP and the baseline models using the challenging SugarCrepe composition benchmark, with additional performance assessments provided in the appendix for older benchmarks. Models are also tested on image-text retrieval tasks for broader evaluation using the Flickr30k  and MSCOCO  datasets. Zero-shot classification performance

    &  &  &  & **Overall** \\   & **Object** & **Attribute** & **Relation** & **Object** & **Attribute** & **Object** & **Attribute** & **Avg.** \\   & **LaCLIP** & 59.44 & 53.17 & 51.42 & 54.69 & 49.25 & 55.29 & 55.35 & 54.09 \\  & **LaCLIP + HN** & 63.44 & 55.96 & 50.71 & 50.60 & 48.57 & 56.98 & 51.16 & 53.92 \\  & NegCLIP & 62.71 & 58.12 & 54.48 & **56.33** & 51.20 & 56.26 & 61.13 & 57.18 \\  & NegCLIP++ _(ours)_ & 64.77 & 66.12 & **66.93** & 55.51 & 55.41 & 59.65 & **64.45** & 61.69 \\  & **TripletCLIP _(ours)_ & **69.92** & **69.03** & 64.72 & **56.33** & **57.96** & **62.61** & 63.87 & **63.49** \\   & **Performance Gain w.r.L** **LaCLIP** & **16.48\%** & **18.56\%** & **13.30\%** & **1.64\%** & **8.71\%** & **7.32\%** & **8.52\%** & **9.40\%** \\   & **LaCLIP** & 75.06 & 65.48 & 58.68 & 53.47 & 57.66 & 67.65 & 66.76 & 63.54 \\  & **SpecCLIP** & 77.84 & 69.29 & 63.23 & **66.53** & 62.31 & 61.71 & 69.65 & 68.00 \\  & NegCLIP++ _(ours)_ & 82.99 & 78.68 & 75.75 & 61.63 & **65.47** & 70.08 & **76.01** & 72.94 \\  & **TripletCLIP _(ours)_ & **83.66** & **81.22** & **79.02** & 64.49 & 63.66 & **73.67** & 75.43 & **74.45** \\   & **Performance Gain w.r.L** **LaCLIP** & **8.60\%** & **15.75\%** & **20.34\%** & **11.02\%** & **6.00\%** & **8.67\%** & **7.35\%** & **10.91\%** \\    & **small-ViT-B/32\({}^{1}\) (13M)** & 56.90 & 56.85 & 51.99 & 50.81 & 50.00 & 53.93 & 60.55 & 54.43 \\  & medium:**ViT-B/32\({}^{1}\) (128M)** & 77.00 & 69.54 & 57.68 & 57.72 & 57.06 & 66.73 & 64.88 & 64.37 \\   & large:**ViT-B/16\({}^{1}\) (1B)** & **92.68** & 79.82 & 63.94 & 56.10 & 57.66 & **84.34** & **78.61** & 73.31 \\   & **large:ViT-L/14\({}^{1}\)** (13B) & **95.52** & **84.52** & 69.99 & **65.04** & **66.82** & **91.03** & **84.97** & **79.70** \\   

Table 4: **Composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance. \(\) represents the results taken from SugarCrepe benchmark.**

    &  &  \\   &  &  &  &  \\   & **MSCOCO** & **Flickr30k** & **MSCOCO** & **Flickr30k** & **top-1** & **top-5** & **top-1** & **top-5** \\   & **LaCLIP** & 5.06 & 10.90 & 5.97 & 10.84 & 11.56 & 34.72 & 3.79 & 10.49 \\  & **LaCLIP + HN** & 8.08 & 16.10 & 8.64 & 16.64 & **12.31** & 37.14 & 5.75 & 15.22 \\  & NegCLIP & 6.32 & 13.80 & 6.61 & 12.96 & 12.25 & 36.38 & 4.67 & 12.69 \\  & NegCLIP++ _(ours)_ & 5.8 & 11.20 & 6.19 & 10.24 & 11.65 & 35.47 & 3.84 & 10.52 \\  & TripletCLIP _(ours)_ & **10.38** & **22.00** & **11.28** & **22.00** & **12.31** & **41.45** & **7.32** & **18.34** \\   & **Performance Gain** & **53.2\%** & **11.7\%** & **53.1\%** & **11.66\%** & **0.75\%** & **63.73\%** & **3.85\%** & **3.85\%** \\   & **LaCLIP** & 25.86 & 42.70 & 19.78 & 36.30 & 10.98 & 49.06 & 19.72 & 41.39 \\  & NegCLIP & 30.16 & 46.60 & 23.11 & 41.70 & 19.12 & 50.56 & 20.22 & 42.63 \\   & NegCLIP++ _(ours)_ & 26.96 & 43.90 & 22.69 & 42.86 & 18.48 & 50.38 & 19.06 & 40.91 \\   & TripletCLIP _(ours)_ & **33.00** & **55.90** & **28.50** & **52.38** & **20.81** & **53.40** & **23.31** & **47.33** \\   & **Performance Gain** & **7.14\%** & **13.2\%** & **8.72\%** & **16.08\%** & **17.3\%** & **4.34\%** & **3.59\%** & **5.94\%** \\   

Table 5: **Zero-shot image-text retrieval and classification results. Bold number indicates the best performance and underlined number denotes the second-best performance.**is assessed across approximately 18 different datasets. Evaluations adhere to the methodologies outlined in the CLIP-Benchmark3 or the official benchmark implementations.

### Compositional reasoning

We comprehensively analyze the compositional understanding of models on the SugarCrepe benchmark, as detailed in Table 4. Notably, TripletCLIP consistently outperforms all baseline models across all sub-categories of SugarCrepe on both the CC12M/CC3M training datasets. Specifically, TripletCLIP surpasses LaCLIP and NegCLIP by **10.91%/9.4%** and **6.45%/6.31%** on the CC12M and CC3M datasets, respectively. Our enhanced baseline, NegCLIP++, also shows improvement over standard NegCLIP, highlighting the benefits of LLM-generated negatives. Nevertheless, TripletCLIP further advances performance, underscoring the critical role of hard negative image-text pairs, not just text. Additional comparisons on older composition benchmarks (Valse , Cola , and Winoground ) in the appendix reveal TripletCLIP's consistent performance. Table 4 also contrasts TripletCLIP with models trained using the DataComp approach, which involves more parameters and training data, demonstrating that TripletCLIP achieves comparable performance to a ViT-B/16 model trained on 1 billion image-text pairs.

### Zero-shot evaluations

**Image-Text Retrieval.** In Table 5, we summarize the performance of models on text-to-image (T2I) and image-to-text (I2T) retrieval tasks on MSCOCO and Flickr30k datasets, where we report R@5 scores. Remarkably, TripletCLIP significantly outperforms baseline models by an average of **8%/10%** and **8%/12.5%** on I2T and T2I tasks, respectively, on the CC3M and CC12M datasets. Intriguingly, while LaCLIP+HN performs better than NegCLIP, TripletCLIP outstrips both.

**Zero-shot Classification.** Table 5 also presents the average zero-shot classification performance on 18 standard datasets, including ImageNet1k. TripletCLIP consistently enhances top-1 accuracy by an average of **3%** and top-5 accuracy by **5-7%** compared to LaCLIP. Like the retrieval performance, LaCLIP+HN exceeds NegCLIP, yet TripletCLIP maintains the highest performance. Dataset-specific results are in the appendix.

### Finetuning performance

In this paper, we focus on pretraining-based experiments as they allow greater flexibility in learning better representations. To complement this, we also performed additional fine-tuning experiments using hyperparameters similar to the baselines (without LoRA) and compared them against various publicly available baselines [44; 50; 12; 11]. As reported in Table 7, TripletCLIP improves compositionality and outperforms nearly all baselines. Furthermore, the observed drop in retrieval and zero-shot classification performance (Table 16) is attributed to limitations in the vision encoder, highlighting the challenges of existing pre-trained vision encoders in capturing semantic representations. This is further demonstrated in Table 8.

  
**Models** & **Filtering Strategy** & **Data Size** & **Augmentations** & **SugarCrepe** & **Retrieval** & **ImageNet1k** \\  ^{}\)**} & No filtering & 12.8 & - & 55.61 & 6.49 & 2.7 \\  & CLIP Score & 3.8 & - & 57.31 & 9.08 & 5.1 \\  & Image-based \(\) CLIP Score & 1.4 & - & 54.75 & 5.63 & 3.9 \\  & No filtering (CC3M) & 2.6 & - & 54.09 & 8.19 & 3.79 \\  & No filtering (CC3M) & 2.6 & - & 63.49 & 16.42 & 7.31 \\  & TripletCLIP++ & CLIP Score (from CC12M) & 1.4 & 1.4 & **66.09** & **19.85** & **8.85** \\   

Table 6: **Ablation on filtering high-quality image-text pairs from TripletData. We evaluate the TripletCLIP after applying the filters to ensure the quality similar to DataComp and compare the baselines on three benchmarks. We find that TripletCLIP results in the most optimal solution. Bold number indicates the best performance. \(\) represents that results are borrowed from DataComp.**

### Ablations

**Can a high-quality filtered dataset improve the performance?** Given that negative images in TripletData are generated using SDXL-turbo, these may not always be precise. Inspired by DataComp, we employ a pre-trained CLIP-L/14 to filter the image-text pairs, selecting the highest average similarity pairs (positive and negative) individually (i.e., score = \((s(x_{i},y_{i})+s(x^{}_{i},y^{}_{i}))/2\)). The top 1.4M positive image-text pairs and their corresponding negatives from TripletData are selected. Table 6 details this comparison against DataComp pre-trained models. Remarkably, TripletCLIP already surpasses baselines without filtered data; however, with the filtered dataset, despite being trained on 50% smaller dataset, TripletCLIP++ shows further performance improvements. This underlines the significant benefits of carefully selected TripletData in enhancing the performance.

**Which modality-specific encoder plays the key role in improving compositionality?** To address this open question, we designed an ablation study similar to LiT, freezing either the pre-trained CLIP vision or text encoder while training the opposite modality-specific encoder from scratch. We observe the performance of LaCLIP and TripletCLIP on CC3M, as shown in Table 8. Freezing the vision model results in no performance gain on the SugarCrepe for TripletCLIP. However, significant improvements are noted when the vision encoder is actively trained, suggesting that the vision modality may be the bottleneck in compositionality. Notably, TripletCLIP outperforms LaCLIP in all settings, further demonstrating its robustness to different pre-training approaches.

**Concept coverage analysis.** Improving performance on zero-shot transfer learning tasks such as retrieval involves two key components: adding more concept diversity during training and enhancing image-text alignment/compositionality. We create subsets of CC12M data with increasing concept diversity based on unique WordNet synsets. Specifically, we select 3M, 4M, 5M, and 6M subsets for training LaCLIP, while TripletCLIP training involves only half of these training data as positive pairs, and the rest are corresponding augmentations. Evaluations across SugarCrepe, retrieval tasks, and ImageNet1k (see Figure 3) indicate that TripletCLIP not only enhances SugarCrepe performance even at lower concept coverage levels but also significantly outperforms similar concept coverage in retrieval tasks, matching LaCLIP's performance on zero-shot classification tasks that do not require compositionality at all. This bolsters our argument that incorporating hard negatives from both modalities markedly improves compositional understanding in CLIP, while baseline struggles to do so even with more concept diversity.

**What if TripletData is used for large-scale compositional evaluations?** We evaluated the CC12M pre-trained models on a 50,000 random subset of the CC3M dataset using a Winoground-style

    &  &  &  & **Overall** \\   & **Object** & **Attribute** & **Relation** & **Object** & **Attribute** & **Object** & **Attribute** & **Avg.** \\ 
**CLIP** & 90.92 & 80.08 & 69.13 & 61.22 & 64.26 & 77.16 & 68.64 & 73.06 \\
**CLIP (finetuned)** & 90.92 & 79.69 & 64.01 & 60.82 & 64.26 & 84.67 & 78.76 & 74.73 \\
**NegCLIP** & 91.53 & 83.25 & 73.97 & 72.24 & 67.72 & 86.95 & 88.44 & 80.59 \\
**Baseline ** & 93.22 & 84.39 & 67.35 & 62.04 & 70.12 & 88.31 & 79.48 & 77.84 \\
**CoN-CLIP ** & 93.58 & 80.96 & 63.3 & **87.29** & **79.62** & 59.18 & 65.16 & 75.58 \\
**TSVLC (RB) ** & 91.34 & 81.34 & 64.15 & 68.16 & 69.07 & 79.49 & 91.33 & 77.84 \\
**TSVLC (LLM+RB) ** & 88.13 & 76.78 & 62.73 & 64.08 & 66.67 & 75.80 & 81.07 & 73.61 \\
**DAC ** & **94.43** & **89.48** & **84.35** & 75.10 & 74.17 & 89.67 & **97.69** & **86.41** \\
**TripletCLIP (_ours_)** & **94.43** & 85.53 & 80.94 & 69.80 & 69.82 & **90.40** & 86.27 & **82.46** \\   

Table 7: **Finetuning-based composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance.**

  
**Models** & **Train Text** & **Train Vision** & **SugarCrepe** & **Retrieval** & **ImageNetlik** \\ 
**LaCLIP** & ✓ & \(\) & **0.6373** & 0.5345 & 31.21\% \\
**TripletCLIP (_ours_)** & ✓ & \(\) & 0.6227 & **0.6817** & **34.25\%** \\ 
**LaCLIP** & \(\) & ✓ & 0.5886 & 0.1134 & 5.51\% \\
**TripletCLIP (_ours_)** & \(\) & ✓ & **0.6923** & **0.2626** & **12.51\%** \\   

Table 8: **Frozen encoder ablation. LiT style fine-tuning ablations on SugarCrepe, image-text retrieval, and ImageNet1k. Bold number indicates the best performance.**approach . As shown in Table 9, TripletCLIP significantly improves performance compared to the baselines. However, we partially attribute this improvement to spurious correlations learned from the data. At the same time, we note that the models have not fully converged, suggesting minimal risk of overfitting to these spurious correlations.

## 5 Conclusion

In this work, we introduce TripletCLIP, a novel approach to enhancing compositional reasoning in vision-language models through the strategic incorporation of hard negative image-text pairs. Our comprehensive experiments across a suite of benchmarks demonstrate that TripletCLIP significantly outperforms existing methodologies such as LaCLIP and NegCLIP, achieving notable gains not only in compositionality but also in zero-shot classification and retrieval tasks as well. Further, our ablation studies highlight the critical role of modality-specific training and the careful curation of training data, underscoring the importance of both hard negative image and text components in the learning process. TripletCLIP's effectiveness with a smaller, refined dataset suggests a promising direction for future research--maximizing performance without the need for extensive data collection, thereby reducing computational costs and enhancing model efficiency. To this end, we provide an intriguing application of synthetic datasets via hard negative image-text pairs for vision-language tasks that could be easily extended to improve Multimodal Large Language Models and Text-to-Image generative models.

Limitations.Due to constraints inherent in academic settings and limited computational resources, we were unable to scale TripletCLIP to handle hundreds of millions of image-text pairs or employ larger models within the scope of this study. Nevertheless, our results indicate a promising direction for future research within a consistent experimental framework, and we encourage subsequent work to explore scaling both the TripletData and TripletCLIP. Our experimental focus was primarily on the CLIP and LiT methodologies. With additional resources, however, extending our methodologies to more advanced contrastive learning techniques, such as SigLIP, would be feasible. In conclusion, our work introduces a compelling strategy for integrating open-ended hard negatives (both text and image) during the pre-training phase, providing a methodology and large-scale data that could benefit a variety of research domains.

Figure 3: Average Results of LaCLIP and TripletCLIP for SugarCrepe Compositions, Image-Text Retrieval, and ImageNet1k over increasing concept diversity.

  
**Methods** & **Text-Score** & **Image-Score** & **Group-Score** \\ 
**CLIP** & 52.69 & 29.66 & 24.64 \\
**NegCLIP** & 54.84 & 30.42 & 25.82 \\
**NegCLIP++** & 36.50 & 30.67 & 20.11 \\
**TripletCLIP (ours)** & **92.25** & **66.82** & **64.30** \\   

Table 9: TripletData as large-scale composition evaluation dataset after .