# Explaining Datasets in Words:

Statistical Models with Natural Language Parameters

Ruiqi Zhong

ruiqi-zhong@berkeley.edu, corresponding author. All authors affiliated with UC Berkeley.

Heng Wang

Dan Klein

Jacob Steinhardt

###### Abstract

To make sense of massive data, we often first fit simplified models and then interpret the parameters; for example, we cluster the text embeddings and then interpret the mean parameters of each cluster. However, these parameters are often high-dimensional and hard to interpret. To make model parameters directly interpretable, we introduce a family of statistical models--including clustering, time series, and classification models--parameterized by _natural language predicates_. For example, a cluster of text about COVID could be parameterized by the predicate "_discusses COVID_". To learn these statistical models effectively, we develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and discretizes them by prompting language models (LMs). Finally, we apply our framework to a wide range of problems: taxonomizing user chat dialogues, characterizing how they evolve across time, finding categories where one language model is better than the other, clustering math problems based on subareas, and explaining visual features in memorable images. Our framework is highly versatile, applicable to both textual and visual domains, can be easily steered to focus on specific properties (e.g. subareas), and explains sophisticated concepts that classical methods (e.g. n-gram analysis) struggle to produce.2

## 1 Introduction

To analyze massive datasets, we often fit simplified statistical models and interpret the learned parameters. For example, to categorize a set of user queries, we might cluster their embeddings, look at samples from each cluster, and hopefully each cluster corresponds to an explainable category, e.g. "_asks about COVID symptoms_" or "_discusses the U.S. Election_". Unfortunately, each cluster might contain an interpretable group of queries, thus failing to explain the categories.

Such a failure is not an isolated incident: many models explain datasets by learning high dimensional parameters, but these parameters might require significant human effort to interpret. For example, BERTopic  learns uninterpretable cluster centers over high-dimensional neural embeddings. LDA , Dynamic Topic Modeling  (time series), and Naive Bayes (classification) learn weights over a large set of words/phrases, which do not directly explain abstract concepts [9; 52; 63]. We want model parameters that are more interpretable, since explaining datasets is important in machine learning , business , political discussion , and science [17; 34].

To make model parameters directly interpretable, we introduce a family of models where some of their parameters are represented as natural language predicates, which are inherently interpretable. Our core insight is that we can use a predicate to extract a 0/1 feature by checking whether it is true on a sample. For instance, given the predicate \(=\)"_discusses the U.S. Election_", its denotation [\(\)] is a binary function that evaluates to 1 on texts \(x\) discussing the U.S. Election and 0 otherwise:

\[[\![:]\!](x:)=1.\]Using these 0/1 feature values, we define a wide variety of models, including clustering, classification, and time series modeling, all parameterized by natural langauage predicates (Figure 1).

Learning these predicates \(\) requires optimizing them to maximize the log-likelihood of the data. This is challenging because \(\) are discrete and thus do not admit gradient-based optimization. We propose a general method to effectively optimize \(\): we create a continuous relaxation \(\) of \(\) and optimize \(\) with gradient descent; then we prompt an LLM to explain the behavior of \(\), thus converting it back to discrete predicates (Section 4). We repeat this process to iteratively improve performance.

To evaluate our optimization algorithm, we create statistical modeling problems where the optimal predicate parameters are known, so we can use them as the ground truth. We evaluated on three different statistical models (clustering, multilabel classification, and time series modeling, as illustrated in Figure 1) and used five different datasets (NYT articles, AG-News, DBPedia, Bills, and Wiki [40; 58; 32; 23]). We found that both continuous relaxation and iterative refinement improve performance; additionally, our model-agnostic algorithm matches the performance (\(2\%\) increase in F1 score) of the previous algorithm specialized for explainable text clustering .

Finally, we show that our framework is highly versatile by applying it to a wide range of tasks: taxonomizing user chat dialogues , characterizing how they evolve, finding categories where one language model is better than the other, clustering math problems  based on their subareas, and explaining what visual features make an image memorable . Our framework applies to both text and visual domains, can be easily steered to explain specific abstract properties, and explains complicated concepts that classical methods (e.g. n-gram regression/topic model) struggle to produce. Combining LLM's ability to generate explanations along with traditional statistical models' ability to process sophisticated data patterns, our framework holds the promise to help humans better understand the complex world.

## 2 Related Work

**Statistical Modeling in Text.** Statistical models based on n-gram features or neural embeddings are broadly used to analyze text datasets. For example, logistic regression or naive Bayes models are frequently used to explain differences between text distributions ; Gaussian mixture models on pre-trained embeddings can create text clusters ; topic models can mine major topics across a large collection of documents  and across time . However, since these models usually rely on high-dimensional parameters, they are difficult to interpret: for example, human studies from  show that the most probable words for a topic might not form a semantically coherent category. To interpret these models, prior works proposed to explain each topic or cluster by extracting candidate phrases either from the corpus or from Wikipedia [8; 49; 57]. Our work complements these approaches to explain models with natural language predicates, which are potentially more flexible.

Figure 1: Our framework can use **natural language predicates** to parameterize a wide range of statistical models. **Left.** A clustering model that categorizes user queries. **Middle.** A time series model that characterizes how discussion changes across time. **Right.** A classification model that summarizes user traits. Once we define the model, we learn \(\) and \(w\) based on \(x\) (and \(y\)).

Prompting Language Model to Explain Dataset Patterns.Our algorithm heavily relies on the ability of LLMs to explain distributional patterns in data when prompted with datasets [39; 46]. [61; 62; 15] have prompted LLMs to explain differences between two text distributions; [53; 38; 50; 27] prompted LLMs to generate topic descriptions over unstructured texts; [44; 22; 64] prompted LLMs to explain the function that maps from an input to an output; [45; 5] prompted LLMs to explain what inputs activate a direction in the neural embedding space. However, these works focused on individual applications or models in isolation; in contrast, our work creates a unifying framework to define and learn more complex models (e.g. time series) with natural language parameters.

**Concept Bottleneck Models (CBM).** CBMs aim to achieve explainability by learning a simple model over a set of interpretable features , and recent works have proposed to extract these features using natural language phrases/predicates [3; 55; 30; 12; 41]. While most of these works focus on classification tasks, our work formalizes a broad family of models--including clustering and time series --and proposes a model-agnostic algorithm to learn them. Additionally, these prior works focus on downstream task performance (e.g. classification accuracy), thus implicitly assuming that the model grounds the feature explanations in the same way as humans; in contrast, since our focus is on explanations, we focus on our algorithm's ability to recover ground truth explainable features.

We discuss more related work on discrete prompt optimization, exploratory analysis, and learning with latent language in Appendix A.

## 3 Mathematical Formulation

### Predicate-Conditioned Distribution

In order to model text distributions with natural language parameters, we introduce a new family of distributions, _predicate-conditioned distributions_; these distributions will serve as building blocks for the models introduced later, just like normal distributions are building blocks for many classical models like Gaussian Mixture or Kalman Filter. Predicate-conditioned distributions \(p\) are supported on the set \(X\) of all the text samples we observe from the dataset, and they are parameterized by (1) a list of \(K\) predicates \(^{K}\), and (2) real-valued weights \(w^{K}\) on those predicates. Formally,

\[p(x,w) e^{w^{T}(x)}.\] (1)

We now explain how to (1) extract a feature vector from \(x\) using \(\), (2) linearly combine \(\) by re-weighting with \(w\), and (3) use the reweighted values to define \(p(x w,)\).

**Natural Language Parameters \(\).** Each predicate \(\) is a natural language string and its denotation \(:X\{0,1\}\) maps samples to their value under the predicate. For example, if \(=\), then \(\)("_I love soccer_.")\(=1\). Since a model typically requires multiple features to explain the data, we consider vectors \(_{K}\) of \(K\) predicates, where now \(\) maps \(X\) to \(\{0,1\}^{K}\):

\[(x):=_{1}(x), _{2}(x),,_{K}(x).\] (2)

To instantiate \(\) computationally, we prompt a language model to check whether \(\) is true on the input \(x\), following the practice from prior works [61; 62]. See Figure 2 (left) for the prompt we used.

**Reweighting with \(w\)**. Consider the following example:

\[w=[-5,3];=[,].\] (3)

Then \(w^{T}\) has a value of \(-5 1+3 0=-5\) for an English, non-sports related sample \(x\). More generally, \(w^{T}(x)\) is larger for non-English sports-related samples.

**Defining \(p(x,w)\)**. According to Equation 1, \(p(x,w)\) is a distribution over \(X\), all the text samples we observe, but it puts more weights on \(x\) with higher values of \(w^{T}(x)\). Using the example \(w\) and \(\) above, \(p(x,w)\) has higher probability on non-English sports-related texts.

Finally, we define \(U(x)\) as the uniform distribution over \(X\) for later use.

### Example Models Parameterized by Natural Language Predicates

We introduce three models parameterized by predicates: clustering, time series, and multi-label classification. For each model, we explain its input, the learned parameters \(\) and \(w\), the log-likelihood loss \(\), and its relation to classical models.

**Clustering.** This model aims to help humans explore a large corpus by creating clusters, each explained by a predicate. Such a model may help humans obtain a quick overview for a large set of machine learning inputs , policy discussions , or business reviews . Given a set of text \(X\), our model produces a set of \(K\) clusters, each parameterized by a learned predicate \(_{k}\); for example, if the predicate is "_discusses the U.S. Election_", then the corresponding cluster is a uniform distribution over all samples in \(X\) that discuss the U.S. Election.

Similar to K-means clustering, each sample \(x\) is assigned to a unique cluster. We use a one-hot basis vector \(b_{x}^{K}\) to indicate the cluster assignment of \(x\), and set \(w_{x}= b_{x}\), where \(\) has a large value (e.g. 10). We maximize the total log-likelihood:

\[(,w)=-_{x X}(p(x,w_{x})); w _{x}= b_{x},b_{x}.\]

However, some samples might not belong to any cluster and thus have 0 probability; to prevent infinite loss, we add another "background cluster" \(U(x)\) that is uniform over all samples in \(X\); therefore, each sample \(x\) can back off to this cluster and incur a loss of at most \(- U(x)=(|X|)\).

**Time Series Modeling.** This model aims to explain latent variations in texts that change across time; for example, finding that an increasing number of people "search about flu symptoms" (\(\)) can help us forecast a potential outbreak . Formally, the input is a sequence of \(T\) text samples \(X=\{x_{t}\}_{t=1}^{T}\). Our model produces \(K\) predicates \(_{k}\) that capture the principle axes of variation in \(x\) across time. We model \(w_{1} w_{T}\) as being drawn from a Brownian motion, i.e.,

\[p(x_{t},w_{t})(w_{t}^{} (x)); w_{t}:=w_{t-1}+(0,^{-1}I),\] (4)

where \(\) is a real-valued hyper-parameter that regularizes how fast \(w\) can change. The loss \(\) is hence

\[(,w)=_{t=1}^{T}-(p(x_{t},w_{t}))+ _{t=1}^{T-1}||w_{t}-w_{t+1}||_{2}^{2}.\] (5)

**Multiclass Classification with Learned Feature Predicates.** This model aims to explain the decision boundary between groups of texts, e.g. explaining what features are more correlated with the fake news class  compared to other news, or explaining what activates a neuron . Suppose there are \(C\) classes in total; the dataset is a set of samples \(x_{i}\) each associated with a class \(y_{i}[C]\). Our model is hence a linear logistic regression model on the feature vectors extracted by \(\), i.e.

\[(x_{i})=W(x_{i});(,W)=-_{i}((x_{i})_{v_{i}}}}{_{c=1}^{C}e^{(x_{i})_{c}}}),\] (6)

where \(W^{C K}\) is the weight matrix for logistic regression.

Figure 2: **Left. The prompt to compute \((x)\). Right. The prompt to Discretize \(_{k}\), which generates a set of candidate predicates based on samples \(x\) from \(U\) and their scores \((e_{x},_{k})\).**Method

We can now learn the parameters for each model above by minimizing the loss function \(\). Formally,

\[},=_{^{K},w}( ,w).\] (7)

However, optimizing \(\) is challenging, since it is discrete and therefore cannot be directly optimized by gradient-based methods. To address this challenge, we develop a general optimization method, which we describe at a high level in Section 4.1, introduce its individual components in Section 4.2, and explain our full algorithm in Section 4.3.

### High-Level Overview

Our framework pieces together three core functions that require minimal model-specific design:

1. OptW, which optimizes \(w\).
2. OptRelaxedPhi, which optimizes a continuous relaxation \(_{k}\) for each predicate \(_{k}\).
3. Discretize, which maps from continuous predicate \(_{k}\) to a list of candidate predicates.

Using these three components, our overall method initializes the set of predicates by first optimizing \(w\) and \(\) using OptW and OptRelaxedPhi and then discretizing \(\) with Discretize. To further improve the loss, it then iteratively removes the least useful predicate, re-optimizes its continuous representation, and discretizes it back to a natural language predicate.

To provide more intuition for these three components, we explain what they should achieve in the context of clustering. OptW should optimize the 1-hot choice vectors \(w_{x}\) by assigning each text sample to the cluster with maximum likelihood. OptRelaxedPhi should find a continuous cluster representation \(_{k}\) similar to the sample embeddings assigned to this cluster, and Discretize generates candidate predicates that explain which samples' embeddings are similar to \(_{k}\). Next, we introduce these three components formally for general models with predicate parameters.

### Three Components of our framework

OptW optimizes \(w\) while fixing the values of \(\). Formally, \(():=_{w}(,w)\).

This function needs to be designed by the user for every new model, but it is generally straightforward: in the clustering model, it corresponds to finding the cluster that assigns the highest probability for each sample; in classification, it corresponds to learning a logistic regression model; in the time series model, the loss is convex with respect to \(w\) and hence can be optimized via gradient descent.

For later use, we define the fitness of a list of predicates \(\) as the negative loss after \(w\) is optimized:

\[():=-(,()).\] (8)

Next, we discuss OptRelaxedPhi. The parameters \(\) are discrete strings, so the loss function is not differentiable with respect to \(\). To address this, we approximate \((x)\) with the dot product of two continuous vectors, \(_{k} e_{x}\), where \(e_{x}^{d}\) is a feature embedding of \(x\) normalized to unit length (e.g. the last-layer activations of some neural network), and \(_{k}^{d}\) is a unit-length, continuous relaxation of \(_{k}\). Intuitively, if the optimal \(=\)"_is sports-related_" and \(x\) is a sports-related sample with \((x)=1\), then we hope that \(\) would correspond to the latent direction encoding the sports topic and it has high similarity with the embedding \(e_{x}\) of \(x\). Under this relaxation, \(\) becomes differentiable with respect to \(_{k}\) and can be optimized with gradient descent.

Formally, OptRelaxedPhi optimizes all continuous predicates \(_{1 K}\) given a fixed value of \(w\):

\[(w)=_{_{1K}}( {} w).\] (9)

We sometimes also use it to optimize a single continuous predicate \(_{k}\) given a fixed \(w\) and all discrete predicate variables other than \(_{k}\) (denoted as \(_{-k}\)):

\[(_{-k},w)=_{_{k}}(_{k}|_{-k},w).\] (10)Finally, Discretize converts \(_{k}\) into a list of \(M\) discrete candidate predicates to update the variable \(_{k}\). Our goal is to find \(\) whose denotation is highly correlated with the dot product simulation \(_{k} e_{x}\).

To discretize \(_{k}\), we prompt a language model to generate several candidate predicates and then re-rank them. Concretely, we draw samples \(x(x)\)3and sort them based on their dot product \(} e_{x}\). We then prompt a language model with these sorted samples and ask it to generate candidate predicates that can explain what types of samples are more likely to appear later in the sorted list (Figure 2 bottom). To filter out unpromising predicates, we re-rank them based on the pearson-r correlations between \([\![]\!]\) and \(} e_{x}\) on \(U\) if \(w\) cannot be negative (e.g. clustering), and the absolute value of pearson-r correlation otherwise. We then keep the top-\(M\) predicates.

### Piecing the Three Components Together

Our algorithm has two stages: we first initialize all the predicate variables and then iteratively refine each of them. During initialization, we

1. randomly initialize continuous predicates \(\) to be the embedding of random samples from \(X\)
2. optimize \((,w)\) by alternatively optimizing \(w\) and all the continuous predicates \(\) with OptW and OptRelaxedPhii, and
3. set \(_{k}\) as the first candidate from Discretize(\(_{k}\))

During refinement, we repeat the following steps for \(S\) iterations:

1. find the least useful predicate \(_{k}\); we define the usefulness of \(_{k}\) as how much the fitness would decrease if we zero it out, i.e. \((_{-k},0)\).
2. optimize \(}\) using OptRelaxedPhii and choose the fittest predicate from Discretize(\(_{k}\))

We include a formal description of our algorithm in Appendix Algorithm H.

## 5 Experiments

In this section, we benchmark our algorithm from Section 4; we later apply it to open-ended applications in Section 6. We run our algorithm on datasets where we know the ground truth predicates \(\) and evaluate whether it can recover them. On five datasets and three statistical models, continuous relaxation and iterative refinement consistently improve performance. Our general method also matches a previous specialized method for explainable clustering .

   Reference & Size & Learned & Size & Surface & F1 \\  _“artist”_ & 0.07 & _“music”_ & 0.12 & 0.50 & 0.37 \\ _“animal”_ & 0.07 & _“a specific species of plant or animal”_ & 0.14 & 0.50 & 0.65 \\ _“book”_ & 0.08 & _“literary works”_ & 0.07 & 0.50 & 0.64 \\ _“politics”_ & 0.06 & _“a political figure”_ & 0.06 & 0.50 & 0.96 \\ _“plant”_ & 0.07 & _“a specific species of plant or animal”_ & 0.14 & 0.50 & 0.68 \\ _“company”_ & 0.08 & _“business and industry”_ & 0.07 & 0.50 & 0.83 \\ _“school”_ & 0.06 & _“schools”_ & 0.07 & 1.00 & 0.97 \\ _“athlete”_ & 0.07 & _“sports”_ & 0.07 & 0.50 & 0.98 \\ _“building”_ & 0.08 & _“historical buildings”_ & 0.08 & 0.50 & 0.92 \\ _“film”_ & 0.06 & _“film”_ & 0.07 & 1.00 & 0.91 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: We compare the reference predicates and our learned predicates when clustering the DBPedia dataset. We abbreviate the predicates, e.g. _“art”_ = _“has a topic of art”_. For each reference, we match it with the learned predicate that achieves the highest F1-score at predicting the reference denotation. We also report the surface similarity (defined in Section 5.2) between the learned predicate and the reference. Our learning algorithm mostly recovers the underlying reference predicates, though it sometimes learns larger/correlated cluster that disagrees with the reference but is still meaningful.

### Datasets

We design a suite of datasets for each of the three statistical models mentioned from Section 3.2. Each dataset has a set of reference predicates, and we evaluate our algorithm's ability to recover them.

**Clustering.** We consider five datasets, AGNews, DBPedia, NYT, Bills, and Wiki . The datasets have 4/14/9/21/15 topic classes each described in a predicate, and we sample 2048 examples from each for evaluation.

**Multiclass Classification.** We design a classification dataset with 5,000 articles and 20 classes; its goal is to evaluate a method's ability to recover the latent interpretable features useful for classification. Therefore, we design each class to be a set of articles that satisfy three predicates about its topic, location, and language; for example, one of the classes can be described by the predicates "_has a topic of sports_", "_is in Japan_", and "_is written in English_". We create this dataset by adapting the New York Times Articles dataset , where each article is associated with a topic and a location predicate; we then translate them into Spanish, French, and Deutsch. We consider in total \(4+4+4=12\) different predicates for each of the topic/location/language attributes and subsample 20 classes from all \(4 4 4=64\) combinations.

**Time Series modeling.** We synthesize a time series problem by further adapting the translated NYT dataset above. We set the total time \(T=2048\) and sample \(x_{1} x_{T}\) according to the time series model in Section 3.2 to create the benchmark. We set \(\) to be the 12 predicates mentioned above and the weight \(w_{,k}\) for each predicate \(_{k}\) to be a cosine function with a period of \(T\) to simulate how each attribute evolves throughout time. In addition, we included three simpler datasets where there is only variation on one attribute (i.e. varies only on one of topic/location/language). We name these four time series modeling all, topic, locat, and lang, respectively. See Appendix B for a more detailed explanation.

### Metrics

To evaluate our algorithm, we match each learned predicate \(_{k}\) with a reference \(^{*}_{k^{}}\), compute the F1-score and surface similarity for each pair, and then report the average across all pairs. To create the matching, we match \(_{k}\) to the \(^{*}_{k^{}}\) with the highest overlap (number of samples where both are true); formally, we define a bi-partite matching problem to match each predicate in \(\) with one in \(^{*}\), define the weight of matching \(^{*}_{k^{}}\) and \(^{*}_{k^{}}\) to be their overlap, and then find the maximum weight matching via the Hungarian algorithm. We now explain the F1-score and surface similarity metric.

**F1-score Similarity.** We compute the F1-score of using \((x)\) to predict \(^{*}(x)\) on \(X\), the set of samples we observe. This is similar to the standard protocol for evaluating cluster quality .

**Surface Form Similiarity**. We can also directly evaluate the similarity between two predicates based on their string values, e.g. "_is about sports_" is similar in meaning to "_has a topic of sports_", a metric previously used by . For a pair of predicates, we ask gpt-4 to evaluate whether they are similar in meaning, related, or irrelevant, with each option associated with a surface-similarity score of 1/0.5/0. We display the prompt in Figure 5 and example ratings in Table 1.

### Experiments on Our Benchmark

We now use these metrics and datasets to evaluate the optimization algorithm proposed in Section 4 and run ablations to investigate whether continuous relaxation and iterative refinement are effective. We will first introduce the overall experimental setup, and then discuss individual takeaways supported by experimental results in each paragraph.

**Experimental Setup.** When running the algorithm, we generate candidate predicates in Discretize with gpt-3.5-turbo; to perform the denotation operation \([](x)\), we use flan-t5-xl; we create the embedding for each sample \(x\) with the Instructor-xl model  and then normalize it with \(_{2}\) norm. We set the number of candidates \(M\) returned by Discretize to be 5 and the number of optimization iteration \(S\) to be 10. To reduce noises due to randomness, we average the performance of five random seeds for each experiment.

Table 2 reports the results of clustering and Table 3 reports other results. For each dataset, we perform several ablation experiments and present the takeaways from these results.

**Takeaway 0: Is our method better than naively prompting language model to generate predicates?** How does our approach compare to a naive baseline approach, which directly prompts the language model to generate predicates based on dataset samples? For this baseline, we repeatedly prompt a language model to generate more predicates until we obtain \(K\) predicates, compute their denotation, evaluate them using the metrics in Section 5.2, and report the performance in Table 2 and 5, the Prompting row. Across all entries, our approach significantly outperforms this baseline.

**Takeaway 1: Relax + discretize is better than exploring randomly generated predicates.** Our optimization algorithm explores the top-5 LLM-generated predicates that have the highest correlations with \(} e_{x}\). Would choosing a random predicate be equally effective? To investigate this question, we experimented with a variant of our algorithm that randomly chooses five predicates without utilizing the continuous representation \(}\) (No-Relax). In Table 2 and 3, No-Relax underperforms our full algorithm (Ours) in all cases. In Appendix Figure 6, we plot the loss after each iteration averaged across all tasks, and we find that Ours converges much faster than No-Relax.

**Takeaway 2: Iterative refinement improves the performance.** We considered a variant of our algorithm that only discretizes the initial continuous representations and does not iteratively refine the predicates (No-Refine). In Table 2 and 3, No-Refine underperforms the full algorithm in all cases.

**Takeaway 3: Our model-agnostic method is competitive with previous methods specialized for explainable clustering.** We compare our method to GoalEx from , which designs a specialized method for explainable clustering based on integer linear programming. Even though our method is model-agnostic, it matches or outperforms GoalEx on four out of five datasets and improves F1 by \(0.02\) on average.

**Takeaway 4: Our method accounts for information beyond the set of text samples (e.g. temporal correlations in the time series).** We investigate this claim using the time series datasets, where we shuffle the text order and hence destroy the time-dependent information a model could use to extract informative predicates (Shuffled). Table 3 finds that Ours is better than Shuffled in all cases, indicating that our method does make use of temporal correlations.

Appendix D includes additional results: 1) compared to topic modeling and K-means, our method achieves similar or better performance while being explainable; 2) we ran ablations on the effect of neural embeddings and show that informative embeddings are crucial to good performance; 3) Takeaways 1, 2, and 4, are significant with \(p<1\%\) under paired t-tests.

   F1/Surface & AGNews & DBPedia & NYT & Bills & Wiki & Average \\  Prompting & 0.43/0.60 & 0.31/0.44 & 0.21/0.40 & 0.16/0.47 & 0.22/0.34 & 0.27/0.45 \\ No-Refine & 0.72/0.57 & 0.57/0.52 & 0.54/0.58 & 0.34/0.49 & 0.47/0.51 & 0.53/0.54 \\ No-Relax & 0.86/0.60 & 0.59/0.53 & 0.58/0.53 & 0.31/0.51 & 0.46/0.50 & 0.56/0.54 \\ Ours & **0.86/0.62** & 0.68/0.54 & **0.70/0.63** & **0.45/0.52** & **0.51/0.53** & **0.64/0.57** \\  GoalEx (Specialized) & **0.86/0.62** & **0.75/0.64** & 0.68/**0.63** & 0.33/0.50 & 0.49/0.48 & 0.62/**0.57** \\   

Table 2: Results on clustering. Ours always outperforms No-Refine and No-Relax, indicating that both continuous relaxation and iterative refinement are helpful. Compared to GoalEx , our method is slightly better on all datasets except DBPedia, which we analyze in Table 1.

   F1/Surface & topic & lang & locat & all & time-avg & classification \\  Prompting & 0.40/0.35 & 0.39/0.38 & 0.26/0.30 & 0.54/0.57 & 0.40/0.40 & 0.51/0.42 \\ No-Refine & 0.53/0.53 & 0.39/0.50 & 0.37/0.55 & 0.58/0.44 & 0.47/0.50 & 0.58/0.44 \\ No-Relax & 0.65/0.50 & 0.52/0.65 & 0.48/**0.68** & 0.61/0.56 & 0.56/0.60 & 0.68/0.62 \\ Shuffled & 0.46/0.33 & 0.52/0.45 & 0.33/0.28 & 0.60/0.39 & 0.47/0.35 & N/A \\ Ours & **0.67/0.57** & **0.62/0.70** & **0.55/0.68** & **0.72/0.64** & **0.64/0.65** & **0.73/0.70** \\   

Table 3: Our performance on time series (left) and classification (right). Both continuous relaxation and iterative refinement improve the performance (comparing Ours to No-Refine and No-Relax).

## 6 Open-Ended Applications

We apply our framework to a broad range of applications to show that it is highly versatile. Our framework can monitor data streams (Section 6.1), apply to the visual domain (Section F.1), and be easily steered to explain specific abstract properties (Section F.2). Across all applications, our framework is able to explain complex concepts that classical methods struggle to produce.

### Running Our Models Out of the Box: Monitoring Complex Data Streams of LLM Usages

We apply our models from Section 3.2 to monitor complex data streams of LLM usages. In particular, we recursively apply our clustering model to taxonomize user queries into application categories, apply our time series model to characterize trends in use cases across time, and apply our classification model to find categories where one LLM is better than the other. Due to space constraints, we present the key results in the main paper and the full results in Appendix G.

**Taxonomizing User Applications via Clustering.** LLMs are general-purpose systems, and users might applyLLMs in ways unanticipated by the developers. If the developers can better understand how theLLMs are used, they could collect training data correspondingly, ban unforeseen harmful applications, or develop application-specific methods. However, the amount of user queries is too large for individual developers to process, so an automatically constructed taxonomy could be helpful.

We recursively apply our clustering model to user queries to the ChatGPT language model. We obtain the queries by extracting the first turns from the dialogues in WildChat , a corpus of 1M real-world user-ChatGPT dialogues. We use gpt-4o to discretize and claude-3.5-sonnet to compute denotations. We first generate \(K=6\) clusters on a subset of 2048 queries; then we generate \(K=4\) subclusters for each cluster with \(>32\) samples.

We present part of the taxonomy in Figure 3 (left) and contrast it with the taxonomy constructed by directly applying LDA recursively (right). Although some LDA topics are plausibly related to certain applications, they are still ambiguous; for example, it is unclear what topic 1 "_ar prompt description detailed_" means. After manually inspecting the samples associated with this topic, we found that they were related to the application of writing prompts for an image-generation model. In contrast, our framework can explain complicated concepts that are difficult to infer from individual words; for example, it generates "_requesting graphic design prompts_" for the above application, which is much clearer in its meaning when explained in natural language.

**Characterizing Temporal Trends via Time Series Modeling.** Understanding temporal trends in user queries can help forecast flu outbreaks , prevent self-reinforcing trends , or identify new application opportunities. We run our time series model on 1000 queries from WildChat with \(K=4\) to identify temporal trends in user applications, and report part of the results in Figure 4. Based on the blue curve, we find that an increasing number of users "_requests writing or content creation... creating stories based on given prompts._". This helps motivate systems like Coauthor  to assist with this use case.

**Finding Categories where One Language Model is Better than the Other.** One popular method to evaluateLLMs is crowd-sourcing: an evaluation platform (e.g. ChatBotArena ) / or a company (e.g. OpenAI) accepts prompts from users, shows users responses from two different LLM systems,

Figure 3: **Left.** We generate a taxonomy with sophisticated explanations by recursively applying our clustering model. **Right.** We cluster with topic models and present the top words for each topic. Although some topics are plausibly related to certain applications, they are still ambiguous.

and the users indicate which one they like better. The ranking among the LLM systems is then determined by Elo-rating, i.e. how often they win against each other.

However, aggregate Elo-rating omits subtle differences between LLM systems. For example, LLama-3-70B achieved a similar rating as Claude-3-Opus, and the LLM community was excited that open-weight models were catching up. However, is LLama-3-70B similarly capable across all categories, or is it significantly better/worse under some categories? Such information is important for downstream developers, since some capabilities are more commercially valuable than others: e.g. a programmer usually does not care about LLM's capability to write jokes. We need a more fine-grained comparison.

We directly apply the classification model from our framework to solve this task. To understand the categories where LLama-3-70B is better/worse than Claude-3-Opus, we gather user queries \(x\) from the ChatBot Arena maintainers (personal communication), set \(y=1\) if the LLama-3-70B's response to \(x\) is preferred and \(y=0\) otherwise. We set \(K=3\).

Our model finds that LLama-3-70B is better when the query "_asks an open-ended or thought-provoking question_" but worse when it "_presents a technical question_" or "_contains code snippets_". These findings are corroborated by manual analysis by the ChatBot Arena maintainers, who also found that Llama-3 is better at open-ended and creative tasks while worse at technical problems4. We hope that our model can automatically generate similar analysis in the future when a new LLM is released, thus saving researchers' efforts.

To summarize, our framework 1) enables us to define a time series model to explain temporal trends in natural language, and 2) outputs sophisticated explanations that LDA fails to generate. However, it is far from perfect: it is slow to compute denotations for all pairs of \(x\) and candidates \(\) since it involves many LLM API calls, and the predicates themselves are sometimes redundant. We describe these limitations and potential ways to improve them in Appendix G.

Due to space constraints, we present applications in explaining visual features to make images memorable to humans and clustering math problems based on subareas in Appendix F.1 and F.2.

## 7 Conclusion

In this work, we formalize a broad family of models parameterized by natural language predicates. We design a learning algorithm based on continuous relaxation and iterative refinement, both of them effective based on our ablation studies. Finally, we apply our framework to a wide range of applications, showing that it is highly versatile, practically useful, applicable to both text and vision domains, and explains sophisticated concepts that classical methods struggle to produce. We hope future works can make our method more computationally efficient and apply it to more realistic applications, thus assisting humans to discover and understand complex patterns in the world.

Figure 4: We analyze WildChat queries with our time series model. For each learned predicate, we plot how its frequency evolves and the 99% confidence interval of the average frequency (shaded).