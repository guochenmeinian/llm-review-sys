# einspace: Searching for Neural Architectures

from Fundamental Operations

 Linus Ericsson\({}^{1}\)1 Miguel Espinosa\({}^{1}\) Chenhongyi Yang\({}^{1}\) Antreas Antoniou\({}^{2}\)

**Amos Storkey\({}^{2}\) Shay B. Cohen\({}^{2}\) Steven McDonagh\({}^{1}\) Elliot J. Crowley\({}^{1}\)**

\({}^{1}\) School of Engineering \({}^{2}\) School of Informatics

University of Edinburgh

Project page: https://linusericsson.github.io/einspace

Code: https://github.com/linusericsson/einspace

###### Abstract

Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren't diverse enough to include such transformations _a priori_. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.

## 1 Introduction

The goal of neural architecture search (NAS) [14; 42] is to automatically find a network architecture for a given task, removing the need for expensive human expertise. NAS uses (i) a defined search space of all possible architectures that can be chosen, and (ii) a search algorithm e.g. [68; 58; 40] to navigate through the space, selecting the most suitable architecture with respect to search objectives. Despite significant research investment in NAS, with over 1000 papers released since 2020 , manually designed architectures still dominate the landscape. If someone looks through recent deep learning papers, they will most likely come across a (manually designed) transformer , or perhaps a (manually designed) MLP-Mixer , or even a (manually designed) ResNet . Why isn't NAS being used instead?Part of the problem is that most NAS search spaces are not expressive enough, relying heavily on high-level operations and rigid structures. For example in the DARTS  search space, each architecture consists of repeating cells; each cell is a directed acyclic graph where nodes are hidden states, and edges are operations drawn from a fixed set of mostly convolutions. This encodes a very specific prior--_architectures contain convolutions with multi-scale, filter-like structures_--making it impossible to discover anything beyond ConvNet characteristics. Indeed, random search  is often a strong baseline in NAS; networks sampled from unexpressive spaces behave very similarly  which makes it hard to justify an (often expensive) search.

One solution is to take the _ex nihilo_ approach to search space construction. In AutoML-Zero  the authors create a very expressive search space that composes basic mathematical operations without any additional priors. However, searching through this space is far too expensive for mainstream use, requiring several thousand CPUs across several days to (re)discover simple operations like linear layers and ReLUs. Recent interest in hierarchical search spaces  has enabled the study of search across differing architectural granularities which naturally allows for greater flexibility. However, attempts so far have been limited to single architecture families like ConvNets  or transformers . The hybrid search spaces that do exist have limited options both on the operation-level and macro structure .

For NAS to be widely used we need the best of both worlds: a search space that is both highly expressive, and in which we can straightforwardly use existing tried-and-tested architectures as powerful priors for search. To this end, we propose einspace: a neural architecture search space based on a parameterised probabilistic context-free grammar (CFG). It is _highly expressive_, able to represent diverse network widths and depths as well as macro and micro structures. With its expressivity, the space contains disparate state-of-the-art architectures such as ResNets , transformers  and the MLP-Mixer , as shown in Figure 1. Other notable architectures contained in einspace are DenseNet , WideResNet (WRN) , ResMLP  and the Vision Permutator .

We realise our proposed search space through the creation of function-mapping groups that define a broad class of fundamental network operations and further describe how such elements can be composed into full architectures under the natural recursive capabilities of our CFG. To guarantee the validity of all architectures generated within the expressive space, we first extend our base CFG with parameters that ensure diverse components can be combined into complex structures. Next, we balance the contention between search space _flexibility_ and search space _complexity_ by introducing mild constraints on our search space via branching and symmetry-based priors. Finally, we integrate probabilities into our production rules to further control the complexity of architectures sampled from our space.

To demonstrate the effectiveness of einspace, we perform experiments on the Unseen NAS datasets --eight diverse classification tasks including vision, language, audio, and chess problems--using simple random and evolutionary search strategies. We find that in such an expressive search space, the choice of search strategy is important and random search underperforms. When using the powerful priors of human-designed architectures to initialise the search, we consistently find

Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.

both large performance gains and significant architectural changes. Code to reproduce our experiments is available at https://github.com/linuxicsson/einspace.

Using only simple search strategies, we can still identify competitive architectures, indicating that further refining these strategies in einspace could lead to significant advancements. We hope that this novel perspective on search spaces--focusing on expressiveness and incorporating the priors of existing state-of-the-art architectures--has the potential to drive NAS research towards a new paradigm.

## 2 Background

**Neural architecture search**

The search space used in NAS has a significant impact on results [63; 65]. This has facilitated the need to investigate search space design alongside the actual search algorithms . Early macro design spaces [24; 68] made use of naive building blocks while accounting for skip connections and branching layers. Further design strategies have looked at chain-structured [3; 4; 6; 43], cell-based [69; 66; 30; 15] and hierarchical approaches. Hierarchical search spaces have been shown to be expressive and effective in reducing search complexity and methods include factorised approaches , \(n\)-level hierarchical assembly [28; 29; 48], parameterisation of hierarchical random graph generators  and topological evolutionary strategies . Additional work on search spaces have proposed new candidate operations and module designs such as hand-crafted multi-branch cells , tree-structures , shuffle operations , dynamic modules , activation functions  and evolutionary operators . In AutoML-Zero , the authors try to remove human bias from search space construction by defining a space of basic mathematical operations as building blocks.

The pioneering work of  constructs search spaces using CFGs. We take this direction further and construct einspace as a probabilistic CFG allowing for unbounded derivations, balanced by careful tuning of the branching rate. We aim to strike a balance between the level of complexity in the search space and incorporating components from diverse state-of-the-art architectures. Crucially, our space enables flexibility in both macro structure and at the individual operation level. While previous search spaces can be instantiated for specific architecture classes[30; 12], our single space incorporates multiple classes in one, ConvNets, transformers and MLP-only architectures. Such hybrid spaces have been explored before , but they have been limited in their flexibility, offering only direct choices between convolution and attention operations and disallowing the construction of novel components.

Prominent search strategies employed for NAS include Bayesian optimisation [34; 58], reinforcement learning [66; 68; 69] and genetic algorithms [5; 39; 40]. A popular thread of work, towards improving computational efficiency via amortising training cost, involves the sharing of weights between different architectures via a supernet [1; 3; 9; 18; 30; 31]. Efficiency has been further improved by sampling only a subset of supernet channels , thus reducing both space exploration redundancies and memory consumption. Alternative routes to mitigating space requirements have considered both architecture and operation-choice pruning [7; 15]. We however highlight that random search often proves to be a very strong baseline [27; 63]; a consequence of searching within narrow spaces. This is commonly the case for highly engineered search spaces that contain a high fraction of strong architectures . Contrasting this, in our einspace we observe that random search across many tasks performs poorly, underpinning the value of a good search strategy for large, diverse search spaces [2; 41].

**Context-free grammars**

A context-free grammar (CFG; ) is a tuple \((N,,R,S)\), where \(N\) is a finite set of _non-terminal_ symbols, \(\) is a finite set of _terminal_ symbols, \(R\) is the set of _production rules_--where each rule maps a non-terminal \(A\!\!N\) to a string of non-terminal or terminals \(A\!\!(N)^{+}\)--and \(S\) is the _starting symbol_. A CFG describes a context-free language, containing all the strings that the CFG can generate. By recursively selecting a production, starting with the rules containing the starting symbol, we can generate strings within the grammar. CFGs can be _parameterised_: each non-terminal, in each rule in \(R\), is annotated with parameters \(p_{1},,p_{n}\) that influence the production. These parameters can condition production, based on an external state or contextual information, thus extending the power of the grammar.

A _probabilistic_ context-free grammar (PCFG) associates each production rule with a probability . These define the likelihood of selecting a particular rule given a parent non-terminal. The assigned probabilities allow for stochastic string generation.

einspace: A Search Space of Fundamental Operations

Our neural architecture search space, einspace2, is introduced here. Based on a parameterised PCFG, it provides an expressive space containing many state-of-the-art neural architectures. We first describe the groups of operations we include in the space, then how macro structures are represented. We then present the CFG that defines the search space and its parameterised and probabilistic extensions.

As a running example we will be constructing a simple convolutional block with a skip connection within einspace, explaining at each stage how it relates to the architecture. The block will consist of a convolution, a normalisation and an activation, wrapped inside a skip connection.

### Fundamental Operations

Each fundamental operation within einspace takes as input a tensor, either passed as input to the whole network or an intermediate tensor from a previous operation, and operates on it further. An operation can be thought of as a _layer_ in a processing pipeline that defines the overall network. The fundamental operations can be separated into four distinct groups of functions that define their role in a network architecture. The terms _one-to-one_, _one-to-many_ and _many-to-one_ below refer to the number of input and output tensors of the functions within that group. For full details of the operations, see Appendix A.2

**Branching**. _One-to-many_ functions that direct the flow of information through the network by cloning or splitting tensors. Examples include the branching within self-attention modules into queries, keys and values. In our visualisations, these are coloured yellow.

**Aggregation**. _Many-to-one_ functions that merge the information from multiple tensors into one. Examples include matrix multiplication, summation and concatenation. In our visualisations, these are coloured purple.

**Routing**. _One-to-one_ functions that change the shape or the order of the content in a tensor without altering its information. Examples include axis permutations as well as the im2col and col2im operations. In our visualisations, these are coloured green.

**Computation**. _One-to-one_ functions that alter the information of the tensor, either by parameterised operations, normalisation or non-linearities. Examples include linear layers, batch norm and activations like ReLU and softmax. In visualisations, these are coloured blue.

In our example, the skip connection will be handled by a combination of branching and aggregation functions, the convolution is decomposed into the routing functions im2col and col2im, with a linear layer from the computation group between them. The normalisation and activation come from the computation group. In the next subsection, we discuss the larger structures of the architecture.

### Macro Structure

The groups of functions above describe the fundamental operations that make up an architecture. We now describe how these functions are composed in different ways to form larger components.

A _module_ is defined as a composition of functions from above that takes one input tensor and produces one output tensor, with potential branching inside. A module may contain multiple _computation_ and _routing_ operations, but each _branching_ must be paired with a subsequent _aggregation_ operation. Thus, the whole network can be seen as a module that takes a single tensor as input and outputs a single prediction. A network module may itself contain multiple modules, directly pertaining to the hierarchical phrase nature of CFG structures. We divide modules into four types, visualised in Figure 2.

Figure 2: Visualisation of example modules with their CFG derivations in bracket notation. From top to bottom; sequential, branching, routing and computation modules.

**Sequential module**. A pair of modules and/or functions that are applied to the input tensor sequentially. Using our grammar, defined in Section3.3, this can be produced using the rule (M\(\)MM), or equivalently from the starting symbol S. This also applies to the rules below.

**Branching module**. A branching function first splits the input into multiple branches. Each branch is processed by some inner set of modules and/or functions. The outputs of all branches are subsequently merged in an aggregation function. In the grammar below this can be produced by the rule (M\(\)B M A).

**Routing module**. A routing function is applied, followed by a module and/or function. A final routing function then processes the output tensor. In the grammar below this is produced by the rule (M\(\)P MR). For more details on the role of the routing module, see Appendix A.3.

**Computation module**. This module only contains a single function, selected from the one-to-one computation functions described above. While this module is trivial, we will see later how its inclusion is helpful when designing our CFG and its probabilistic extension. In the grammar below this is produced by the rule (M\(\)C ).

To construct our example, we will be using all four modules. The branching module combines the clone and add functions from before to create a 2-branch structure. One branch is a simple skip connection by using the identity function inside a computation module. The other branch is the more complex sequence. The convolutional layer is created by combining im2col, linear and col2im in a routing module. The norm and activation are each wrapped in a computation module and these are all composed in sequential modules. Figure 2 shows similar module instantiations in action.

### Search Space as a Context-Free Grammar

The following CFG defines our einspace, where uppercase symbols represent non-terminals and lowercase represent terminals. The colours refer to the function groups.

Our networks are all constructed according to the high-level blueprint: backbone\(\)head where head is a predefined module that takes an output feature from the backbone and processes it into a prediction (see Appendix B for more details). The backbone is thus the section of the network that is generated by the above CFG. When searching for architectures we search across different backbones.

Completing our running example, we present the full derivation of the architecture in the CFG in Figure 3.

Figure 3: Example derivation tree of a traditional convolutional block with a skip connection.

### Prior Choices

When designing a search space, we must balance the need for flexibility--which allows more valid architectures to be included--and constraints - which reduce the size of the search space. We can view constraints as imposing priors on which architectures we believe are worth including. As discussed, many previous frameworks are too restrictive; therefore, we aim to impose minimal priors, listed below.

**Convolutional prior**. We design our routing module to enable convolutions to be easily constructed, while also allowing components like patch embeddings and transpose operations. We thus enforce that a routing function is followed by another routing function later in the module. Moreover, im2col only appears in the production rule of the first routing function (P) and col2im in the last (R). As shown in Figure 2, to construct a convolution, we start from the rule (M\(\)PMR) and derive the following (P\(\)im2col), (M\(\)C\(\)linear) and (R\(\)col2im).

**Branching prior**. We also impose a prior on the types of branching that can occur in a network. The branching functions clone and group can each have a branching factor of 2, 4 or 8. For a factor of 2, we allow each inner function to be unique, processing the two branches in potentially different ways. For branching factors of 4 or 8, the inner function M is repeated as is, processing all branches identically (though all inner functions are always initialised with separate parameters). Symbolically, given a branching factor of 2 we have (BM\({}_{1}\)M\({}_{2}\)A ) but with a branching factor of 4 we have (BM\({}_{1}\)M\({}_{1}\)M\({}_{1}\)A ). Examples of components instantiated by a branching factor of 2 include skip connections, and for 4, or 8, multi-head attention.

### Feature Mode

Different neural architectures operate on different feature shapes. ConvNets maintain 3D features throughout most of the network while transformers have 2D features. To enable such different types of computations in the same network, we introduce the concept of a _mode_3 that affects the shape of our features and which operations are available at that point in the network. Before and after each module, we fix the feature tensor to be of one of two specific shapes, depending on which mode we are in.

**Im mode**. Maintains a 3D tensor of shape (C, H, W), where C is the number of channels, H is the height and W is the width. Most convolutional architectures operate in this mode.

**Col mode**. Maintains a 2D tensor of shape (S, D), where S is the sequence length and D is the token dimensionality. This is the mode in which most transformer architectures operate.

The mode is changed by the routing functions im2col and col2im. Most image datasets will provide inputs in the Im mode, while most tasks that use a language modality will provide it in Col mode.

Our example architecture maintains the Im mode at almost all stages, apart from inside the routing modules where the im2col function briefly puts us in the Col mode before col2im brings us back.

### Parameterising the Grammar

Due to the relatively weak priors we impose on the search space, sampling a new architecture naively will often lead to invalid networks. For example, the shape of the output tensor of one operation may not match the expected input shape of the next. Alternatively, the branching factor of a branching function may not match the branching factor of its corresponding aggregation function.

We therefore extend the grammar with parameters. Each rule \(r\) now has an associated set of parameters \((s,m,b)\) that defines in which situations this rule can occur. When we sample an architecture from the grammar, we start by assigning parameter values based on the expected input to the architecture. For example, they might be the input tensor shape, feature mode and branching factor:

\[(s\!=\![3,\!224,\!224],m\!=\!,\!b\!=\!1).\] (1)

Given this, we can continuously infer the current parameters during each stage of sampling by knowing how each operation changes them. When we expand a production rule, we must choose a rule which has matching parameters. If at some point, the sampling algorithm has no available valid options, itwill backtrack and change the latest decision until a full valid architecture is found. Hence, we ensure that we can sample architectures without risk of obtaining invalid ones.

As an example of this, the CFG rule for \(\) was previously

\[|| .\] (2)

Enhanced with parameters, this now becomes two rules

\[(m\!=\!) || ,\] (3) \[(m\!=\!) |.\] (4)

This signifies that an \(\) operation is not available in the \(\) mode. Similarly, the available aggregation options depend on the branching factor of the current branching module

\[(b\!=\!2) || ,\] (5) \[(b\!=\!4) |,\] (6) \[(b\!=\!8) |.\] (7)

### Balancing Architecture Complexity

When sampling an architecture, we construct a decision tree where non-leaf nodes represent decision points and leaf nodes represent architecture operations. In each iteration, we either select a non-terminal module to expand the architecture and continue sampling, or choose a terminal function to conclude the search at that depth. Continuously selecting modules results in a deeper, more complex network, whereas selecting functions leads to a shallower, simpler network. We can balance this complexity by assigning probabilities to our production rules, thereby making a PCFG. Recall our CFG rule

\[(||| ).\] (8)

If we choose one of the first three options we are delving deeper in the search tree since there is yet another \(\) to be expanded, but if we choose \((\!\!)\), the _computation-module_, then we will reach a terminal function. Thus, to balance the depth of our traversal and therefore expected architecture complexities, we can set probabilities for each of these rules:

\[p(\!\!\,|), p(\! \!\,\,\,|), p(\!\!\,\,\,|), p( \!\!\,|).\] (9)

The value of \(p(\!\!\,|\,)\) is especially important as it can be interpreted as the probability that we will stop extending the search tree at the current location.

We could set these probabilities to match what we wish the expected depth of architectures to be (for empirical results on the architecture complexity, see Table 10 in the Appendix). However, we can actually ensure that the CFG avoids generating infinitely long architecture strings by setting the probabilities such that the branching rate of the CFG is less than one . For details of how, see Appendix A.4. So, as shown in Figure 4, we set the computation module probability to \(p(\!\!\,|\,)\!=\!0.32\) and the probabilities of the other modules to \(\). For simplicity, all other rule probabilities are uniform.

For a thorough example of how sampling is performed in \(\), please see Appendix A.1.

## 4 Experiments

### Experimental Details

**Datasets**

As our search space strives for expressivity and diverse architectures, we adopt a diverse benchmark suite from the recent paper on Unseen NAS , containing datasets at different difficulties across

Figure 4: To ensure our CFG is consistent and does not generate infinite architectures, we make sure the branching rate is in the sub-critical region by setting \(p(\!\!\,|\,)\!>\!0.31\).

vision, language, audio and further modalities. We run individual searches on these datasets, that are each split into train, validation and test sets. See Appendix B.2 for the detailed dataset descriptions.

While Unseen NAS forms the basis of this section, we run additional experiments on the diverse NASBench360 benchmark  in Appendix C.1, where we beat competing NAS methods on CIFAR100, FSD50K and Darcy Flow, and to the best of our knowledge set a new state-of-the-art on NinaPro.

**Search strategy**

We explore three search strategies within einspace: random sampling, random search, and regularised evolution (RE). _Random sampling_ estimates the average expected test performance from \(K\) random architecture samples. _Random search_ samples \(K\) architectures and selects the best performer on a validation set. In _regularised evolution_, we start by constructing an initial population of 100 individuals, which are either randomly sampled from the search space or seeded with existing architectures. For \((K-100)\) iterations, the algorithm then randomly samples 10 individuals and selects the one with the highest fitness as the parent. This parent is mutated to produce a new child. This child is evaluated and enters the population while the oldest individual in the population is removed, following a first-in-first-out queue structure. An architecture is mutated in three straightforward steps:

1. _Sample a Node_: Uniformly at random sample a node in the architecture derivation tree.
2. _Resample the Subtree_: Replace the subtree rooted at the sampled node by regenerating it based on the grammar rules. This step allows the exploration of new configurations, potentially altering a whole subtree if a non-leaf node is chosen.
3. _Validate Architecture_: Check if the new architecture can produce a valid output in the forward pass, given an input of the expected shape, and that it fits within resource constraints, e.g. GPU memory. If it does, accept it; otherwise, discard and retry the mutation.

Note that these are very simple search strategies, and that there is huge potential to design more intelligent approaches, e.g. including crossover operations in the evolutionary search, using hierarchical Bayesian optimisation  or directly learning the probabilities of the CFG . In this work, we focus on the properties of our search space and investigate whether simple search strategies are able to find good architectures, and leave investigations on more complex search strategies for future work.

**Baselines**

We compare these search strategies to PC-DARTS , DrNAS  and Bonsai-Net  with results transcribed from . We also compare to the performance of a trained ResNet18 (RN18). More details on the baselines, training recipes and network instantiations can be found in Appendix B.1

### Random Sampling and Search

In previous NAS search spaces e.g. [30; 62; 12], complex search methods often perform very similarly to random search [27; 63]. Indeed, we can see this in Table 1 comparing the PC-DARTS strategy to DARTS random search.

However for einspace, this is not the case for most datasets. Random sampling improves on pure random guessing (not shown), but is far from the baseline performance of a ResNet18. The random search baseline is also far behind, but intriguingly outperforms baseline NAS approaches on Chesseract.

### Evolutionary Search from Scratch

We now turn to a more sophisticated search strategy. We perform regularised evolution in einspace for 1000 iterations across all datasets, initialising the population with 100 random samples. In Table 1 the results are shown in the column named RE(Scratch). The performance of this strategy is significantly higher than random search on several datasets, indicating that the search strategy is more important in an expressive search space like einspace compared to DARTS. Compared to the top performing NAS methods, however, it is significantly behind on some datasets.

Figure 5: The top RE(Mix) architecture on AddNIST, found in einspace.

### Evolutionary Search from Existing SOTA Architectures

To fully utilise the powerful priors of existing human-designed structures, we now invoke search where the initial population of our evolutionary search is seeded with a collection of existing state-of-the-art architectures.

We first seed the entire population with the ResNet18 architecture. The search applies mutations to these networks for 500 iterations. In Table 1, these results can be found in the RE(RN18) column.

To further highlight the expressivity of einspace, we perform experiments searching from an initial population seeded with a mix of ResNet18, WRN16-4, ViT and MLP-Mixer architectures. To our knowledge, no other NAS space is able to represent such a diverse set of architectures in a single space. These results are shown in the RE(Mix) column.

Overall, we find that on **every single task**, we can find an improved version of the initial architecture using RE(RN18) and on all but one using RE(Mix). Moreover, in some cases we can beat the existing state-of-the-art, especially on tasks further from the traditional computer vision setting. In particular, where previous NAS methods fail--i.e. the Language dataset--the architecture in Figure 6 has a direct improvement over the ResNet18 by 5.76%. See also the architecture in Figure 5 and the collection in Figure 8 in the Appendix for the breadth of structures that are found in einspace.

We further compare einspace to the previous, CFG-based, hNASBench201 from Schrodi et al. . This allows for an initial study on the effects of our search space design choices and, in particular, the increased expressiveness compared to hNASBench201. These results show how einspace compares favourably to a different search space under the same evolutionary search. Overall, we highlight that our search results on einspace are competitive, even with far weaker search space priors.

One dataset where our searches struggle is CIFARTile, a more complex version of the CIFAR10 dataset. While large improvements are made to the baseline networks, they still lag behind other NAS methods. This shows how the strong and restricted focus on ConvNets within the DARTS search space is highly successful for traditional computer vision style tasks that have been common in the literature.

    &  &  &  & Rand. \\  &  &  &  &  \\  Dataset & RN18 & PC & Dr & Bonsai & RE & RE & RE & RE & RE &  &  &  \\  & RN18 & DARTSâ€™ & NAS* & Net* & & & & & & & & & & \\  AddNIST & 93.36 & 96.60 & 97.06 & **97.91** & 93.82 & 97.54 & 97.72 & 83.87 & 97.07 & 34.17 & 67.00 & 10.13 \\ Language & 92.16 & 90.12 & 88.55 & 87.65 & 92.43 & 96.84 & **97.92** & 88.12 & 90.12 & 76.83 & 87.01 & 35.26 \\ MultiNIST & 91.36 & 96.68 & **98.10** & 97.17 & 93.44 & 96.37 & 92.25 & 93.72 & 96.55 & 39.76 & 66.09 & 18.87 \\ CIFARTile & 47.13 & **92.28** & 81.08 & 91.47 & 58.31 & 60.65 & 62.76 & 30.89 & 90.74 & 24.76 & 30.90 & 25.25 \\ Guthenberg & 43.32 & 49.12 & 46.62 & 48.57 & 43.70 & **54.02** & 50.16 & 36.70 & 47.72 & 29.00 & 39.58 & 19.69 \\ Isabella & 63.65 & 65.77 & 64.53 & 64.08 & 59.79 & 64.30 & 62.72 & 56.33 & **66.35** & 58.53 & 56.90 & 32.24 \\ GeoClassing & 90.08 & 94.61 & **96.03** & 95.66 & 92.33 & 95.31 & 95.13 & 60.43 & 95.54 & 63.56 & 69.13 & 24.35 \\ Chesseract & 59.35 & 57.20 & 58.24 & 60.76 & 63.92 & 60.31 & 61.86 & 59.50 & 59.16 & **68.83** & 61.46 & 44.83 \\  Average acc. \(\) & 72.55 & 80.30 & 78.78 & **80.41** & 74.72 & 78.17 & 77.56 & 63.70 & **80.41** & 49.43 & 59.76 & 26.33 \\ Average rank \(\) & 7.38 & 4.69 & 4.62 & **3.75** & 6.00 & 3.88 & 4.12 & 9.00 & 4.31 & 9.50 & 8.88 & 11.88 \\   

Table 1: Accuracies resulting from the combination of einspace with the simple search strategies of random sampling, random search, and regularised evolution (RE). See text for further detail. We evaluate performance across multiple datasets and modalities from Unseen NAS . Results transcribed from  are denoted *, where DARTS  and Bonsai  search spaces are employed. The expressiveness of einspace enables performance that remains competitive with significantly more elaborate search strategies, as well as outperforming the CFG-based space hNASBench201  when using evolutionary search in both spaces. **Best** and **second** best performance per dataset.

Figure 6: The best model on the Language dataset, found by RE(Mix) in einspace.

## 5 Limitations

Our search space, designed for diversity, is extremely large and uniquely unbounded in terms of depth and width. This complexity makes formulating one-shot methods like ENAS  or DARTS  challenging. Instead, developing an algorithm to learn the probabilities of the PCFG may be more feasible. This approach, however, must address the grammar's context-free nature where sampling probabilities do not consider network depth, feature shape, or previous decisions, although this could be mitigated by using the parameters outlined in Section 3.6. Due to the relatively slow nature of our evolutionary search strategy (see Table 9 in Appendix C.5), we believe that finding more efficient search strategies for expressive spaces like ours is an important and exciting direction for future work.

Another issue is ambiguity arising from the possibility of multiple derivation trees for a single architecture, primarily due to the multiple ways of stacking sequential modules. Moreover, we have found that through sampling and mutation, some architectures' components reduce to the identity operation, from e.g. stacked identity and permute operations within sequential and routing modules. Finding ways to represent the equivalence classes of derivation trees can thus be powerful for reducing effective search space size.

Finally, we designed einspace to be diverse, but some key structures cannot be represented in its current form. There are no options for recurrent computation, as found in RNNs and the new wave of state-space models like Mamba . We believe this can be integrated via the inclusion of a recurrent module that repeats the computation of the components within it--however we leave more detailed exploration of this direction to future work. We also chose to keep the options for activations and normalisation layers as small as possible since in practice the benefit from changing these is minor.

## 6 Conclusion

We have introduced einspace: an expressive NAS search space based on a parameterised PCFG. We show that our work enables the construction of a comprehensive and diverse range of existing state-of-the-art architectures and can further facilitate discovery of novel architectures directly from fundamental operations. With only simple search strategies, we report competitive resulting architectures across a diverse set of tasks, highlighting the potential value of defining highly expressive search spaces. We further demonstrate the utility of initialising search with existing architectures as priors. We believe that future work on developing intelligent search strategies within einspace can lead to exciting advancements in neural architectures.