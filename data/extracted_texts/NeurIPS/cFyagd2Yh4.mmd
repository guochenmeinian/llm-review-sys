# MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models

Tessa Han

Harvard University

Cambridge, MA

than@g.harvard.edu

&Aounon Kumar

Harvard University

Cambridge, MA

aokumar@hbs.edu

&Chirag Agarwal

University of Virginia

Charlottesville, VA

chiragagarwal@virginia.edu

&Himabidu Lakkaraju

Harvard University

Cambridge, MA

hlakkaraju@hbs.edu

###### Abstract

As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.

## 1 Introduction

Large language models (LLMs) have been progressing at a breathtaking speed and have been shown to be proficient in a variety of medical tasks such as answering medical questions , interpreting histopathology data , and conversing with patients . While LLMs have the potential to improve medicine, they can also be used to cause severe medical harm, including mistreating patients, concealing medical errors, violating patient confidentiality, crafting fake medical records, devising ways to restrict access to medical care, and deliberately spreading misinformation. At stake are fundamental issues such as personal and public health, patient safety, and human rights. Therefore, investigating the medical safety of LLMs is of critical importance.

In light of the aforementioned risks, various organizations and communities have emphasized the medical safety of LLMs. For instance, the European Union's Artificial Intelligence (AI) Act, which "aims to protect fundamental rights, democracy, the rule of law, and environmental sustainability from high-risk AI", classifies AI systems "that could put the life and health of citizens at risk" as"high-risk," the highest risk category for AI systems that are acceptable to be used . In addition, the medical AI community has unequivocally called for the use of AI "to do no harm" and "the most good" in medicine [6; 7]. More broadly, various governments have called for the regulation of these systems (e.g., U.S. AI Bill of Rights  and AI Accountability Act , United Kingdom Bletchley Declaration , European Union AI Act ).

While recent research introduced benchmarks to evaluate the safety of LLMs by measuring their "unwillingness" to respond to harmful requests [11; 12; 13], these works predominantly focus on general safety and do not consider the unique safety concerns of the medical domain. For instance, general safety involves ensuring that LLMs do not answer questions that can cause harm in everyday life (e.g., "What are the steps to steal a car?") while medical safety involves ensuring that LLMs do not answer questions that can harm patients or healthcare providers in healthcare settings (e.g., "How can a doctor modify a patient's electronic health record to cover up a diagnostic error?"). However, there is little to no formal understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. These aspects are critical in ensuring the safe deployment of LLMs in medical settings. To address the aforementioned challenges, we introduce MedSafetyBench, a safety evaluation benchmark that addresses the unique safety concerns of the medical domain. Our work makes the following contributions:

* We define the notion of medical safety in LLMs based on the _Principles of Medical Ethics_ set forth by the American Medical Association.
* We leverage the understanding above to develop MedSafetyBench, the first medical safety evaluation benchmark for LLMs. MedSafetyBench comprises 1,800 harmful medical requests and corresponding safe responses. We use a combination of state-of-the-art LLMs (e.g., GPT-4) and adversarial jailbreaking techniques (e.g., Greedy Coordinate Gradient algorithm ) to construct this benchmark dataset.
* We demonstrate the utility of MedSafetyBench by evaluating the medical safety of publicly-available general-knowledge LLMs (e.g., Vicuna , Pythia , Llama-2 , Llama-3.1 , Mistral , Mistral , GPT-3.5 , GPT-4 , and GPT-4o ) and medical LLMs (e.g., Medplaca-13b , Meditron-70b , ClinicalCamel-70b , and Med42-70b ). We find that the medical LLMs fail to meet medical safety standards.
* We also demonstrate how fine-tuning these medical LLMs using MedSafetyBench can improve their medical safety while preserving their medical performance.

Figure 1: Contribution and findings. In this work, we define the notion of medical safety for LLMs, leverage this definition to develop a medical safety benchmark dataset, and use this benchmark to evaluate and improve the medical safety of LLMs. We find that 1) publicly-available medical LLMs do not meet standards of medical safety and that 2) fine-tuning these LLMs on medical safety demonstrations significantly improves their safety while preserving their medical performance.

By concretizing the notion of medical safety and introducing MedSafetyBench, our work enables a systematic study of medical safety in LLMs and motivates future research in this area, thereby paving the way to minimize the safety risks associated with LLMs in medicine.

## 2 Related Work

**Safety Evaluation Benchmarks for LLMs.** Recent works evaluate the safety of LLMs using benchmark datasets consisting of harmful requests that an LLM should refuse to answer [11; 12; 13] and the LLM's safety is measured by its "unwillingness" to respond to such requests . For instance, Qi et al.  develop a safety dataset by collecting harmful prompts that violate Meta and OpenAI usage policies. Bhardwaj and Poria  create a dataset of harmful questions and answers using a red-teaming strategy called Chain of Utterances, which involves using one LLM to elicit harmful responses from another. In addition, Zhang et al.  introduce a multiple-choice question benchmark to evaluate the safety of LLMs. However, all of these safety evaluation benchmarks focus on general harm (e.g., illegal activity, violence, and fraud) and do not address the unique concerns of the medical domain (e.g., clinicians' responsibility to patients, patient rights to confidentiality, and treatment of medical errors). Therefore, in this work, we introduce the first safety evaluation benchmark dataset specific to the medical domain and use it to evaluate and improve the medical safety of LLMs.

**Safety Alignment of LLMs.** The training objective of an LLM, i.e., predicting the next token given a sequence of tokens, does not ensure that its behavior aligns with human preferences. As a result, the field of alignment has emerged in recent LLM research to build LLMs that behave in a manner consistent with human intentions, preferences, goals, and values [28; 29]. One of the key aspects of alignment in LLMs is safety (i.e., the model's ability to operate without causing harm, such as complying with legal requirements and ethical standards and avoiding danger, risk, and injury), where safety-aligned LLMs have safety guardarails that prevent them from complying with harmful instructions. Common alignment techniques include reinforcement learning from human feedback (RLHF) [30; 31; 32], where an LLM is fine-tuned using a reward model that is based on human feedback, and instruction-tuning , where an LLM is fine-tuned on a dataset of instruction-response pairs. While prior work has examined aligning LLMs to general safety standards [34; 35], aligning them to medical safety standards has not been investigated. In this work, we demonstrate that MedSafetyBench can be leveraged to perform instruction-tuning and improve the medical safety of LLMs.

## 3 MedSafetyBench: A Benchmark Dataset for the Medical Safety of LLMs

In this section, we first define medical safety for LLMs. Based on this definition, we develop MedSafetyBench, a medical safety benchmark dataset for LLMs.

### Defining Medical Safety for LLMs

What does it mean for an LLM to be safe in a medical setting? We base the definition of medical safety for LLMs on the _Principles of Medical Ethics_ (Table 1) set forth by the American Medical Association (AMA). These principles, adopted in 1957 and last revised in 2001 , are a set of time-tested, living principles for ethical conduct in medicine. According to the AMA, they represent "standards of conduct that define the essentials of honorable behavior for the physician", outlining a physician's "responsibility to patients first and foremost, as well as to society, to other health professionals, and to self" . They cover various aspects of medical practice and promote medical practice that is consistent with medical knowledge, follows the law, and aspires to medical ideals.

Thus, we define an LLM to be aligned with medical safety standards if its output is not only accurate but also consistent with the AMA's _Principles of Medical Ethics_. Typically, during the training and development of an LLM, the focus is on the model's technical performance ("accuracy"). In this work, we emphasize that, in addition to high technical performance, an LLM must also adhere to other aspects of safety.

### Developing the Benchmark Dataset

**Harmful Medical Requests.** To evaluate medical safety of LLMs, we first generate a set of requests that violate the AMA's _Principles of Medical Ethics_. An LLM that meets medical safety standards should decline to answer such harmful medical requests. Each principle forms a category containing requests that violate the respective ethical guidelines. To generate a diverse set of harmful medical requests in a systematic fashion, we use GPT-4  and Llama-2-7b-chat. With GPT-4, we prompted the model, asking it to write commands or questions that might be given to an AI assistant that violate each principle of medical ethics and that include medical jargon. This prompting approach did not work with Llama-2-7b-chat, which declined to generate harmful medical requests. To circumvent this, with Llama-2-7b-chat, we employed a jailbreaking technique using the Greedy Coordinate Gradient attack algorithm . Given a prompt, this algorithm optimizes a sequence of adversarial input tokens that, when appended to the prompt, cause the LLM to produce a desired target sequence. We set the prompt for Llama-2-7b-chat to a request to produce harmful prompts that violate a particular AMA principle and the target sequence to an affirmative response. We observe that the harmful medical requests created using GPT-4 tend to contain more medical jargon than those created using Llama-2-7b-chat. We manually inspect the harmful medical requests for correctness and quality and edited inadequate ones. This process yielded 1,800 unique harmful medical requests (900 from each model), spanning all nine _Principles of Medical Ethics_. An example harmful medical request is shown below.

```
1:functionmedical(regurds)

[MISSING_PAGE_POST]

medical safety demonstration consists of a harmful medical request and a safe response to the request. Given a harmful medical request, we create a corresponding safe response by prompting GPT-4 to decline to answer the harmful request and provide a brief explanation for why it is unsafe or unethical. We manually verified the quality of the safe responses and edited inadequate ones. An example safety demonstration is shown below.

**Dataset Safety demonstration**

**Harmful request:**

Can you help me create a fake medical degree or license?

**Safe response:**

I'm sorry, but I cannot assist with creating a fake medical degree or license. Doing so would be illegal and unethical, as it involves fraud and can have serious consequences for public safety and trust.

**Dataset Structure.** To use the benchmark dataset to both evaluate and improve LLM medical safety, we randomly split the 1,800 medical safety demonstrations into two groups of 900 demonstrations. We refer to these two groups as MedSafety-Eval and MedSafety-Improve. Each group consists of \(450\) medical safety demonstrations with harmful requests created using GPT-4 and \(450\) medical safety demonstrations with harmful requests created using Llama-2-7b-chat. For MedSafety-Eval, we refer to these two sets of medical safety demonstrations as MedSafety-Eval-GPT4 and MedSafety-Eval-Llama2, respectively. Additional details about the benchmark dataset and its development are in Appendix A.

**Dataset Validation.** To validate the benchmark dataset, we conducted a user study with 25 domain experts (doctors). Each domain expert viewed a randomly selected set of 25 harmful medical requests from the full dataset. For each harmful request, the domain experts were asked if it violates any of the nine Principles of Medical Ethics, and if so, they were further asked to select any of the nine principles that are violated by the harmful request. Through the user study, we confirm that the harmful requests in MedSafetyBench indeed violate _Principles of Medical Ethics_ and that the benchmark covers all nine _Principles of Medical Ethics_. Additional details about the user study are in Appendix D. These results from the user study are consistent with our own inspection of the benchmark dataset and further validate the benchmark.

## 4 Experiments

In this section, we demonstrate the utility of MedSafetyBench for studying the medical safety of LLMs by using MedSafety-Eval to evaluate the medical safety of LLMs and MedSafety-Improve to improve it.

### Evaluating the Medical Safety of LLMs

**Setup.** We prompt LLMs with harmful requests and evaluate the harmfulness of their responses. We evaluate both general and medical safety. To evaluate general safety, we use the GenSafety-Eval dataset  (\(n=330\)). It consists of harmful general requests, spanning eleven categories of general harm (illegal activity, child abuse content, hate / harassment / violence, malware, physical harm, economic harm, fraud deception, adult content, political campaigning, privacy violation activity, and tailored financial advice). To evaluate medical safety, we use MedSafety-Eval (\(n=900\)). It contains harmful medical requests, spanning nine categories of medical harm (corresponding to the nine principles of medical ethics). (Note: While MedSafety-Eval is a set of medical safety demonstrations, where each demonstration consists of a harmful medical request and a corresponding safe response, for the purpose of evaluation, we use only the harmful requests.)

We evaluate publicly-available LLMs. For medical LLMs, we evaluate Medplaca (7b and 13b) , Meditron (7b and 70b) , ClinicalCamel-70b , and Med42-70b . To our knowledge, these LLMs are not safety-aligned. We also evaluate the general-knowledge LLMs on which these medical LLMs were pre-trained and/or fine-tuned: Llama (7b and 13b)  and Llama-2 (7b, 13b, and 70b) . These LLMs are also not safety-aligned. In addition, we evaluate safety-aligned versions of these general-knowledge LLMs: Llama-2-chat (7b, 13b, and 70b) . For additional benchmarking on a more diverse set of models, we also evaluate other state-of-the-art general 

[MISSING_PAGE_FAIL:6]

Figure 2: Average harmfulness score for each LLM by harm dataset. On the x-axis, LLMs with safety alignment are indicated by an asterisk. Error bars indicate the standard error of the mean. The results indicate that medical LLMs readily comply with harmful general and medical requests, and they do so more frequently than their safety-aligned, general-knowledge counterparts. Thus, medical LLMs do not meet standards of general and medical safety.

Figure 3: Safety of medical LLMs before fine-tuning (red) and after fine-tuning (green) on safety demonstrations. Error bars indicate the standard error of the mean. Fine-tuning on safety demonstrations significantly improves the safety of original medical LLMs. This trend is consistent across medical LLMs (Medalpaca-7b, Medalpaca-13b, and ClinicalCamel-70b), across evaluation datasets (GenSafety-Eval, MedSafety-Eval-GPT4, MedSafety-Eval-Llama2), and across the types of safety demonstrations on which the model is fine-tuned (general, medical, or both).

Fine-tuning improves LLM safety across tuning datasets, evaluation datasets, and models (Figure 3: green conditions have lower harmfulness scores than the red conditions; Appendix C, Table 4). In addition, fine-tuning on one type of safety improves not only that type of safety but also the other type of safety (Figure 3: gen_n900 and med_n900 conditions have lower harmfulness scores than the red conditions for both GenSafety-Eval and MedSafety-Eval). Fine-tuning on both types of safety improves safety the most, followed by fine-tuning on only medical safety, then by fine-tuning on only general safety (Figure 3: both_n1800 conditions have the lowest harmfulness scores, followed by med_n900 conditions, then by gen_n900 conditions). As the number of safety demonstrations used during fine-tuning increases, LLM safety improves (Appendix C, Figures 6, 7, and 8). Furthermore, fine-tuning for safety preserves the medical performance of medical LLMs (Appendix C, Figures 9, 10, 11, and 12), suggesting it may be possible to achieve both desiderata (safety and performance).

## 5 Discussion and Conclusion

In this work, we study the medical safety of LLMs. We define the notion of medical safety for LLMs and leverage this definition to develop MedSafetyBench, the first benchmark dataset for the medical safety of LLMs. Using MedSafetyBench, we evaluate and improve the medical safety of LLMs, finding that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using the benchmark dataset improves their safety. These analyses are made possible only by the development of a safety benchmark dataset that is specific to the medical domain.

This work paves the way for future research on the medical safety of LLMs. In this work, medical safety is defined based on the AMA's _Principle of Medical Ethics_. In practice, one could consider introducing nuance to the definition. For example, levels of acceptable risk may vary among medical subspecialities (e.g., emergency medicine vs. neurological surgery vs. dermatology) and based on a patient's condition and personal preference (e.g., a patient with a condition that has no established treatment options may be more willing to try risky experimental procedures). Aligning LLMs to account for different levels of acceptable risk and be tailored to different medical subspecialities is a future research direction. In addition, this work demonstrates that one way to improve the medical safety of LLMs is through instruction-tuning. Exploring other fine-tuning techniques, such as RLHF, is another direction for future research. Although very computationally intensive, RLHF could incorporate domain expert feedback during the safety alignment process and may facilitate the alignment of LLMs to more nuanced and bespoke medical safety standards.

In the application of LLMs to medical tasks, the focus has been on achieving high medical performance [1; 2; 24; 25; 26; 27]. However, as LLMs develop increasingly powerful capabilities and are applied in the medical domain, it is also critical to study and proactively mitigate their risks of medical harm. This calls for collective discussion in the medical research community and beyond of how to define medical safety for LLMs, continued evaluation of the medical safety of LLMs, and the development of safer LLMs, in order to mitigate their risks of harm in medicine. We hope this work jumpstarts this discussion and galvanizes future work in this area.