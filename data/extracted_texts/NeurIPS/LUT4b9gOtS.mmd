# Learning Visual Prior via Generative Pre-Training

Jinheng Xie\({}^{1}\) Kai Ye\({}^{2*}\) Yudong Li\({}^{2*}\) Yuexiang Li\({}^{3}\) Kevin Qinghong Lin\({}^{1}\)

**Yefeng Zheng\({}^{3}\) Linlin Shen\({}^{2}\) Mike Zheng Shou\({}^{1}\)**

\({}^{1}\) Show Lab, National University of Singapore \({}^{2}\) Shenzhen University

\({}^{3}\) Jarvis Research Center, Tencent YouTu Lab

{sierkinhane,mike.zheng.shou}@gmail.com

https://sierkinhane.github.io/visor-gpt

 Equal Contribution \(\) Corresponding Author

###### Abstract

Various stuff and things in visual data possess specific traits, which can be learned by deep neural networks and are implicitly represented as the visual prior, _e.g.,_ object location and shape, in the model. Such prior potentially impacts many vision tasks. For example, in conditional image synthesis, spatial conditions failing to adhere to the prior can result in visually inaccurate synthetic results. This work aims to explicitly learn the visual prior and enable the customization of sampling. Inspired by advances in language modeling, we propose to learn **Visual prior** via **G**enerative **P**re-**T**raining, dubbed VisorGPT. By discretizing visual locations, _e.g.,_ bounding boxes, human pose, and instance masks, into sequences, VisorGPT can model visual prior through likelihood maximization. Besides, prompt engineering is investigated to unify various visual locations and enable customized sampling of sequential outputs from the learned prior. Experimental results demonstrate the effectiveness of VisorGPT in modeling visual prior and extrapolating to novel scenes, _potentially motivating that discrete visual locations can be integrated into the learning paradigm of current language models to further perceive visual world_.

## 1 Introduction

The digital camera can continuously capture photographs of the visual world, such that tremendous photos and videos are currently shared on the Internet. In our world, various stuff and things possess specific _traits_, which have been correspondingly embedded in such visual data. In the current era of deep learning, deep neural networks  have demonstrated remarkable proficiency in learning from vast amounts of data, leading to the development of visual foundation models (VFMs) . Such _traits_ have been accordingly learned and implicitly represented as the _visual prior_ in VFMs, which has the potential to impact real-world applications. An example that highlights its importance can be seen in the field of image synthesis. To present high-quality and natural-looking images, the synthetic stuff and things must adhere to the visual prior such as the **spatial location, shape, and interaction of objects** (Fig. 1 (a)). A vivid example of layout-to-image is provided in Fig. 1 (b). When the spatial conditions do not adhere to the visual prior, such as the shape of 'donut' not being square, the size of 'person' being similar to that of 'donut', and 'donut' being floated in the air instead of being placed on 'dining table', the resulting synthetic contents may be inaccurate and visually inconsistent with the desired outcome. Despite recent advances in conditional image synthesis such as ControlNet  and GLIGEN , the challenge of continuously sampling customized spatial conditions that adhere to the visual prior remains a difficult problem, particularly for automatic synthesis of massive images with corresponding fine-grained annotations.

In this paper, we study the problem of how to explicitly learn visual prior from the real world and enable customization of sampling. If we would like to paint a series of instances on a canvas, weshould decide what to paint and also their shapes, locations, interactions, _etc_. It seems that these elements share a joint probabilistic prior, in which any stuff or things can be accordingly sampled to construct a scene. As there may be many potential variables in the prior, it is extremely hard to be comprehensively formulated. Over the past few years, significant advances have been made in language modeling , demonstrating their remarkable capacity for modeling the probabilistic distribution of sentences. Our focus is on learning the visual prior of location, shape, and relationships among categories, rather than raw pixels. It is possible to convert such visual information into a series of sequences, such that the visual prior can be learned by language modeling. To this end, as presented in Fig. 1 (d), we propose to learn **Vis**ual **prior** via **G**enerative **P**re-**T**raining, dubbed **Vis**G**PT. Thanks to the development of deep learning, many high-quality annotated data such as bounding-box , human pose , instance mask  are publicly available. This provides sufficient location, shape, and relation information of stuff and things in the visual world. Since they are all encoded using 2D or 3D coordinates, we can simply convert them into a corpus of sequences. In this way, the visual prior can be learned by a pretext objective, _e.g.,_ maximizing the likelihood of each sequence. Beyond this, prompt engineering is investigated to unify various visual locations and enable the customized sampling of sequential outputs from the learned prior.

As shown in Fig. 1 (e), according to the user's prompt, VisrGPT can correspondingly sample a sequence from the learned prior, which can be spatially decoded for image synthesis (Fig. 1 (c)). Since the decoded conditions adhere to the prior, the synthetic 'cup', 'dining table', and 'donut' are realistic and consistent with the desired semantics. This finding confirms that we can continuously customize spatial conditions from many aspects, _e.g.,_**data type, object size, number of instances, and classes**, using VisrGPT. With the advance of conditional image synthesis, it is feasible to generate an endless supply of synthetic images with their corresponding fine-grained annotations, potentially providing ample resources to train more robust and generalized visual intelligence models.

## 2 Related Works

**Language Modeling.** Language modeling aims to estimate the probability of a given sequence of words occurring in a sentence. In recent years, Large language models (LLMs) such as GPT series  and BERT family  have revolutionized the field of natural language processing. In particular, BERT family adopts the encoder(-decoder) architecture and employs masked language modeling techniques to model each given sentence bi-directionally in context. In contrast, GPT series employs the decoder-only architecture to sequentially model the probability of

Figure 1: An overview of the problem of visual prior (top) and VisrGPT (bottom). (a) refers to visual prior, _e.g.,_ location, shape, and relations of objects. (b) provides a _failure_ case of image synthesis from spatial conditions that do not adhere to the prior. Specifically, the shape of the ‘donut’ not being square and ‘donut’ being floated in the air instead of being placed on ‘dining table’. (c) displays a _success_ case that conditions sampled from VisrGPT leads to a more accurate synthetic results. (d) illustrates that VisrGPT learns visual prior through sequence corpus converted from the visual world. (e) gives an example that a user customizes a sampling from VisrGPT by prompting.

[MISSING_PAGE_FAIL:3]

The provided prompts can be summarized in Tab. 1, which provides standardized templates to unify commonly used 2D and 3D visual location information into 1D textual sequences. Each prompt begins with the flags [Annotation type] and [Data type], which are the flags indicating the type of annotation and scene, _e.g.,_ box and multiple instances. The following flags of [Size] and [#Instances] represent the average area and the number of instances in the current image, while [#Keypoints] indicates the number of keypoints annotated for each person, _i.e.,_ 14 or 18. The following two flags are the [Category name] of each instance and their corresponding [Coordinate]. At last, [Natural Language Input] is extended for free-form language input and [Instruction] is used for iterative refinement of the current layout. Examples of these prompts are presented in the following sections. By employing our defined templates, we transform commonly used visual annotations into a large-scale sequential corpus. The corpus can be seamlessly ingested by language models, facilitating better learning of visual commonsense prior.

### Learning Visual Prior via Generative Pre-Training

**Model Architecture**. In the past few years, many large language models have been successively proposed, such as GPT [28; 29; 1] and BERT [6; 18; 26] family, and recently introduced LLAMA . We employ the GPT decoder-style transformer as our model to learn the visual probabilistic prior.

**Pretext Objective**. After processing the visual locations \(\) as textual sequences \(\) in SS 3.2, we tokenize each sequence by byte-pair encoding (BPE) algorithm  to obtain a sequence with \(n\) tokens \(=\{u_{1},u_{2},,u_{n}\}\) such that a standard language modeling objective can be directly employed to learn visual prior by maximizing the following likelihood:

\[=_{i}p(u_{i}|u_{i-k},,u_{i-1};),\] (1)

where \(k\) is the size of context window, and \(p(|)\) indicates the conditional probability which is modeled by the neural network \(\). Stochastic gradient descent is used to train the neural network.

### Customizing Sequential Output

In addition to offering formatted visual annotations for learning a probabilistic prior, the standardized templates enable to _personalize sequential output for various applications through prompting_. For example, the customized sequential output can be employed as spatial conditions in image synthesis models (_e.g.,_ ControlNet  and GLIGEN ). This opens up the possibility of synthesizing a broad range of data types to address diverse problems and challenges in computer vision. Here are a few representative scenarios:

**(a) Object Bounding-Box.** As we use a flag to distinguish different types of visual annotations, we can control the type of data and scene to be sampled from the learned probabilistic prior by setting the beginning tokens in the input prompt. Accordingly, we can set the beginning prompt as "box;" to generate sequential output with instances and corresponding bounding-box information. Besides, with flags like [Size], [#Instances], and [#Keypoints], we can sample a scene that adheres to multiple conditions. As depicted in Fig. 2 (a), we can input a prompt "box; multiple instances; small; 16; 0; kite, kite, person," as a prefix to require the VisorGPT to conditionally infer the remaining tokens. In this example, VisorGPT outputs the categories and their locations, specifically fulfilling the requirement of objects being in small size. In particular, (xmin, ymin) and (xmax, ymax) are special tokens indicating the top-left and bottom-right corners of the target object.

**(b) Human Pose.** With flags of [#Instances] and [#Keypoints], VisorGPT is capable of customizing sequential outputs involving instances with keypoints in a crowd scene. We give an example in Fig. 2 (b). Numbers (10 and 14) are added to the beginning of prompt as conditions to infer a scene consisting of 10 people with 14 keypoints. Note that, we use "a, b, c, d, \(\)" and "m0, m1, m2, m3, \(\)" as special tokens to distinguish each human keypoint and object boundary coordinate, respectively.

**(c) Instance Mask.** Beyond sparse coordinates as shown in (a) and (b), VisorGPT can deal with dense spatial annotations, _i.e.,_ instance masks. Typically, pixel-level information can be represented

   Annotation type & box; keypoint; mask; multimodal; \(\) \\ Data type & object centric; multiple instances \\ Size & small; medium; large \\ \#Instances & 1; 2; 3; \(\) \\ \#Keypoints & 14; 18 \\ Category name & cup; person; dog; \(\) \\   

Table 1: Candidate choices of prompt template.

using a mask matrix or a set of boundary coordinates. For convenient sequentialization, we uniformly sample \(n\) points along the angle in the polar space from object boundary coordinates to represent the pixel-level location, which is similar to . We provide an example in Fig. 2 (c).

**(d) Multimodal Inference.**VisorGPT exhibits the capability of multimodal inference. In Fig. 2 (d), VisorGPT can deduce the presence of two individuals and a frisbee along with their bounding boxes and keypoints of these two people are also predicted at the same time.

**(e) Natural Language Input and Instruction.** Apart from the above prompts, VisorGPT can receive free-form language as input to generate corresponding layouts. We present an example in Fig. 2 (d). In addition, the proposed VisorGPT supports iterative refinement by instructions like "make the boy and girl closer".

## 4 Experiments

### Experimental Setup

**Datasets.** We collect around 4 million sequences from the publicly available datasets for VisorGPT. In particular, we consider three types of commonly used visual annotations, _i.e.,_ object bounding-box, human pose, and instance mask. In the MS-COCO dataset , we collect

   Datasets (type) & \#Categories & \#Images \\  Open Images (Box)  & 600 & 1,743,042 \\ Objects365 (Box)  & 365 & 1,728,775 \\ COCO (Box)  & 80 & 117,266 \\ ImageNet (Box)  & 1,000 & 38,285 \\ COCO (Keypoint)  & 1 & 53,473 \\ CrowdPose (Keypoint)  & 1 & 9,981 \\ COCO (Mask)  & 80 & 117,266 \\ Flickr30K  & 44,518 & 31,783 \\ Rico  & 25 & 35,851 \\ PubLayNet  & 5 & 315,757 \\   

Table 2: Training corpus for VisorGPT.

   Datasets (type) & \#Categories & \#Images \\  Open Images (Box)  & 600 & 1,743,042 \\ Objects365 (Box)  & 365 & 1,728,775 \\ COCO (Box)  & 80 & 117,266 \\ ImageNet (Box)  & 1,000 & 38,285 \\ COCO (Keypoint)  & 1 & 53,473 \\ CrowdPose (Keypoint)  & 1 & 9,981 \\ COCO (Mask)  & 80 & 117,266 \\ Flickr30K  & 44,518 & 31,783 \\ Rico  & 25 & 35,851 \\ PubLayNet  & 5 & 315,757 \\   

Table 2: Training corpus for VisorGPT.

Figure 2: Examples of customizing sequential outputs from the proposed VisorGPT.

-118K images annotated with 80 categories and their object bounding-boxes and instance masks. For each image, all object bounding-boxes and instance masks with their category information are formatted to a sequence, respectively. Beyond that, ~3.5 million bounding-box annotations of Objects365  and Open Images  are also converted to sequences. Other types of annotations (_i.e.,_ human keypoint) of MS-COCO (~54K) and CrowdPose (~10K) are also formatted to sequential data. For the object-centric scenario, we collect ~4K sequences from ImageNet-1K . Besides, Flick30K  is employed to support free-form natural language as input. For the scenarios of mobile interface and document layout, we use the Rico  and PubLayNet  datasets. A summary is presented in Tab. 2.

**Evaluation Metrics**. We propose to evaluate VisorGPT from three aspects: **(i)** Evaluating the quality of sequences generated by VisorGPT. In the inference stage, as VisorGPT predicts sequences in the format given in SS 3.2, it is necessary to examine whether the generated sequences can be decoded into visual locations. In particular, we generate a series of sequences using VisorGPT and calculate the accuracy whether it can be successfully decoded (termed **Format** in Table 5) and the number of categories matches the number of locations (termed **Matching** in Table 5). **(ii)** As discussed in SS 3.2, we use flags, _i.e.,_ [Size] and [#Instances], to indicate the average size and number of instances in the current sequence. Hence, we can control the average object size and the number of instances in the generated sequences via setting flags [Size] and [#Instances]. Then, we can calculate the accuracy whether the object size and the number of instances in the generated sequences are consistent with the given flags to validate the performance of controllability (termed **Size** and **#Instances**, respectively). **(iii)** Evaluating the learned probabilistic prior, _i.e.,_ object location, shape, and relation among categories, on the _val_ set of COCO, Objects365, and Open Images datasets. In this work, we propose to compare the discrete distribution of every visual prior. Specifically, to compute the **location** prior of a category, we initialize an empty canvas and convert the bounding-box of each instance of the category to a binary mask. Then, each mask is accumulated on the canvas and normalized as 2D location distribution. To compute the **shape** prior of a category, we calculate the ratio of width to height of each instance of the category, and estimate a discrete distribution as the shape prior. To establish the **relation** prior of a category to other categories, we count the number of co-occurrences between the category and other categories and estimate a discrete distribution. In this way, discrete prior of each category can be computed on COCO, Objects365, and OpenImages _val_ sets as real one. During evaluation, we infer a series of sequences to compute the learned visual prior. Then we measure the similarity between learned and the real prior using the **Kullback-Leibler divergence**. In addition, FID  is adopted to compare with layout generation methods .

### Quantitative Results

**Evaluation on Learned Visual Prior**. In Tab. 4, we present the measured similarity between real probabilistic prior and the one learned by V1-sorGPT on the validation sets of COCO, Open Images, and Objects365, using KL divergence. The prompt template T\({}_{a}\) and T\({}_{a}\)+T\({}_{b}\) in SS 3.2, are

    &  &  &  &  \\   & & Location & Shape & Relation & Location & Shape & Relation & Location & Shape & Relation \\  VisorGPT\({}^{}\) & T\({}_{a}\) & 1.133 & 1.483 & 0.452 & - & - & - & - & - \\ VisorGPT\({}^{}\) & T\({}_{a}\)+T\({}_{b}\) & 1.032 & 1.446 & 0.445 & - & - & - & - & - \\ VisorGPT & T\({}_{a}\) & 1.212 & 1.813 & 0.561 & 0.890 & 2.775 & 3.715 & 1.969 & 1.345 & 2.790 \\ VisorGPT & T\({}_{a}\)+T\({}_{b}\) & 1.583 & 1.710 & 0.581 & 1.007 & 2.782 & 3.888 & 1.995 & 1.377 & 2.765 \\   

Table 4: Evaluation on training corpus scale and prompt templates of VisorGPT. The similarity between real probabilistic prior and the learned one is measured by KL divergence (KL Div).

   Models & \#Parameters & \#Training data & Annotation type & Batch size & Iterations & Learning rate & Sequence length \(n\) \\  VisorGPT & 117M & 4M & box \& keypoint \& mask & 128 & 200K & \(5.0c^{-5}\) & 1024 \\ VisorGPT\({}^{}\) & 117M & 34K & box \& keypoint \& mask & 128 & 200K & \(5.0c^{-5}\) & 1024 \\   

Table 3: Model card of VisorGPT.

    &  &  \\   & Format & Matching & Size & \#Instances \\  COCO & 100.0 & 100.0 & 92.02 & 100.0 \\ Open Images & 99.97 & 99.40 & 89.35 & 98.71 \\ Objects365 & 99.99 & 99.94 & 91.52 & 99.78 \\   

Table 5: Evaluation on customized outputs (%).

used for comparison. Overall, VisorGPT T\({}_{a}\) and T\({}_{a}\)+T\({}_{b}\) exhibit comparable performance, indicating both prompt templates have comparable capability for learning visual prior.

**Evaluation on Customized Sequences**. We present the quality of generated sequences and the performance of VisorGPT's controllability in Tab. 5. It is obvious that nearly all predicted sequences can be decoded successfully in three datasets. Additionally, in over 99% of sequences, all instances can match their respective locations. Besides, the table shows that VisorGPT achieves accuracies of 92.02%, 89.35%, and 91.52% in controlling the average object size on COCO, Open Images, and Objects365 datasets, respectively. Furthermore, VisorGPT can achieve an accuracy of over 98% in controlling the number of instances across all three datasets. These findings demonstrate the strong capacity of VisorGPT in reasoning high-quality sequences and control the object size and number of instances in the scene.

**Comparison with Layout Generation Methods.** We also conduct experiments to compare with layout generation methods and the corresponding results are presented in Table 6. The experimental results include FID and Align. score (referred from LayoutDM ) with lower values indicates better performance. Notably, VisorGPT significantly surpasses these state-of-the-art methods with better FID and Align. score on Rico and PubLayNet datasets, showcasing the superior capability of VisorGPT to model and generate layouts.

### Visualization Results

**Generation Diversity**. One of the concerns is that VisorGPT would memorize the overall data distribution and cannot generate diverse and novel layouts. We present examples in Fig. 3 to address the concern. Firstly, we specify target classes (_e.g.,_ "bottle, dining table, person, knife, bowl, oven, person, cup, cup, bowl, bowl, broccoli, spoon") and then select layouts satisfying these conditions from COCO _train_ set (left in Fig. 3, only one satisfied sample over 110K samples). In contrast, we use the same target classes as prompts for VisorGPT to infer their bounding boxes. As depicted in Fig. 3, the bounding boxes generated by VisorGPT exhibit diversity and considerable differences from those selected ones from COCO _train_ set.

**Scene Completion**. VisorGPT can receive user conditions and reasonably complete the corresponding missing classes in an interactive way, which can also demonstrate the capability of novel/unseen layout generation. We present examples in Fig. 4. One can observe that users can roughly draw two instances of "person" across the canvas (above) and prompt VisorGPT to deduce the remaining "skis" (below). To avoid cherry-picking, we provide the distribution of generated "skis" using 100 samples when given various scenes.

As illustrated, when the two "person" vary from left-top to middle-bottom, the distribution of "skis"

    &  &  \\   & FID (\(\)) & Align. (\(\)) & FID (\(\)) & Align. (\(\)) \\  MaskGIT  & 52.1 & 0.015 & 27.1 & 0.101 \\ BIT  & 88.2 & 1.030 & 116 & 0.153 \\ BART  & 11.9 & 0.090 & 16.6 & 0.116 \\ VQDiffusion  & 7.46 & 0.178 & 15.4 & 0.193 \\ LayoutDM  & 6.65 & 0.162 & 13.9 & 0.195 \\  VisorGPT (Ours) & **5.85** & **0.109** & **9.18** & **0.103** \\   

Table 6: Comparison to previous methods.

Figure 4: Scene completion.

Figure 3: Generation diversity of VisorGPT.

exhibits consistent variations. This validates that VisorGPT does not rely on memorization. Instead, it learns intrinsic visual priors among categories, empowering it to infer novel/unseen layouts, accommodating the diverse requirements of users.

**Relation Prior.** Fig. 5 illustrates the comparison between the real-world relation matrix among 30 categories and the one estimated by VisorGPT. Each row depicts the relation prior of one category to others. For instance, it can be observed from the real world matrix that the 'person' (the first row) frequently interacts with other categories such as 'dog' and 'cat'. Similarly, in the third row, the co-occurrence between 'car' and 'bus', 'truck', and'stop sign' is larger than that of other categories. Notably, it is clear that the relation prior learned by VisorGPT is very close to that of the real-world one. This indicates that VisorGPT can capture the real relationships among categories and generate sequential output that aligns with these visual prior.

**Location Prior.** In addition to the quantitative results presented above, we visualize the comparison between the location prior learned by VisorGPT and the real one across various categories. Fig. 7 displays the location prior of three categories, including'surfboard', 'tie', and 'train'. It is noticeable that, in each column, the location prior learned by VisorGPT is similar to the real one. For instance, from the first column, one can observe that the real distribution of 'tie' is mainly located in the lower-middle region, and the shape prior learned by VisorGPT exhibits a similar pattern.

**Shape Prior.** Fig. 6 shows the shape prior of four categories, such as 'person' and'motorcycle'. To facilitate comparison, we employ kernel density estimation to estimate a continuous distribution from the discrete one. We observe that the shape prior learned by VisorGPT is close to those of the real visual world. For example, in the real world, the ratio of width to height of a car is almost always larger than 1, and the estimated shape prior of 'car' is mainly distributed around 1.8. It is evident that the learned probabilistic prior by VisorGPT, represented by the blue line, closely approximates the real one, represented by the red line. Overall, the shape priors of other categories learned by VisorGPT well match that of the real world.

### Ablation Studies

Tab. 8 presents the impact of Special Words (SW), Textual Knowledge (TK, _i.e._, with model weights initialized from the official pre-trained GPT-2), the number of sequences (#Seq), and model size (#Param). **(a)** Results on Tab. 8(a) are measured by the average KL divergence of location and shape prior. This confirms that the special words can potentially improve VisorGPT's performance in learning the visual prior. Notably, we found that the NLP textual knowledge deteriorated the performance of VisorGPT. We attribute this to the fact that the association between visual coordinates

Figure 5: Relation matrix among 30 categories on COCO.

Figure 6: Shape prior of the categories of ‘person’, ‘car’, ‘motorcycle’, and ‘teddy bear’.

and natural language is relatively weak, thus it becomes inessential to learn visual prior from visual annotations. **(b)** In Tab. (b)b, we find that increasing the number of sampled sequences leads to a more precise estimation of the visual prior by VisorGPT. **(c)** In Tab. (c)c, we investigate the impact of model size on learning visual prior. For simplicity and efficiency, we replace VisorGPT architecture by three GPT versions and train it using only COCO (box) data. The results demonstrate the scalability of VisorGPT, _i.e.,_ modeling the visual prior better with increased learnable parameters.

### Applications

**Conditional Image Synthesis**. VisorGPT's remarkable ability to infer visual categories and their locations based on user-customized prompts shows promising potential for generating customized images that still maintain a sense of realism. Here, we utilize ControlNet  and GLIGEN  to synthesize images from keypoints and bounding-boxes, respectively. We showcase some examples in Fig. 7 and 8. The first and fourth columns in Fig. 7 present the customized spatial conditions sampled from VisorGPT and the conditions not adhering to the visual prior. The second, third, and fifth columns provide synthetic results by GLIGEN conditioned on the corresponding spatial conditions. For example, on the first three columns, it is evident that the spatial conditions sampled from VisorGPT are more natural, such that the synthetic images are realistic and natural-looking. However, when the conditions (the last two columns) do not adhere to the prior, such as 'person' not being on a similar scale to 'dining table', the width of 'pizza' being too long, and the width of 'chair' being too short, the synthetic contents like 'person', 'chair', and 'dining table' appear abnormal, also impacting the authenticity of other objects like the two cups (circled in red dotted line).

Moreover, VisorGPT is capable of inferring sequences that include instances with keypoint information. For example, as shown in Fig. 8, we can provide a prompt like "key point; multiple instances; large; 13; 18; person, " to VisorGPT. This allows it to conditionally imagine a scene involving 13 people with their keypoint coordinates. Decoded results can be used as spatial conditions for image synthesis by ControlNet (shown in the last two columns). More examples can be found in supp.

## 5 Conclusion

This work proposed a novel approach, VisorGPT, to explicitly learning the probabilistic prior of the visual world through generative pre-training. This was achieved by transforming the continuous visual locations into discrete tokens by prompting and training a transformer decoder to maximize

Table 8: Impact of Special Words (SW), Textual Knowledge (TK), Number of Sequences (#Seq), and Model size (#Param). We use KL Div (\(\)) as evaluation metric.

Figure 7: Comparison of synthetic images from _object boxes_ adhering to the prior (left) or not (right).

the likelihood of training sequences. As a result, VisorGPT exhibits significant potential in comprehending real-world visual prior and leveraging this knowledge to create plausible scenes under a variety of customized prompts. This ability can facilitate the automatic synthesis of a vast number of images, along with their corresponding fine-grained annotations, using ControlNet and GLIGEN. This could potentially yield ample resources to train more robust visual intelligence models.

**Broader Impact**. This work demonstrates the substantial capacity of Large Language Models (LLMs) to effectively model spatial visual locations through language modeling. In the subsequent generations of LLMs, the visual priors discussed here can be seamlessly integrated into large-scale training, endowing LLMs with the ability to possess not only textual knowledge but also the capability to perceive and understand the visual world. While current datasets may have limited annotations for object categories, the vast knowledge present in extensive text corpora allows for the association of known objects with novel ones through language-based knowledge, such as synonyms. This presents a promising direction for the next generation of LLMs to expand their understanding and modeling of our visual world.

**Limitation**. We encountered limitations regarding the number of instances that could be included in each sequence due to the maximum token length, despite converting each mask annotation to a fixed length. In the future, we plan to incorporate large-scale natural language corpora into training and extend the maximum sequence length.