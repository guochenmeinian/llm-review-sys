# Make-it-Real: Unleashing Large Multimodal Model

for Painting 3D Objects with Realistic Materials

Ye Fang

Fudan University

Shanghai AI Laboratory

yefang23@m.fudan.edu.cn

&Zeyi Sun

Shanghai Jiao Tong University

Shanghai AI Laboratory

szy2023@sjtu.edu.cn

&Tong Wu

The Chinese University of Hong Kong

Stanford University

wutong16@stanford.edu

&Jiaqi Wang

Shanghai AI Laboratory

wangjiaqi@pjlab.org.cn

&Ziwei Liu

S-Lab, NTU

ziwei.liu@ntu.edu.sg

&Gordon Wetzstein

Stanford University

gordon.wetzstein@stanford.edu

&Dahua Lin

The Chinese University of Hong Kong

Shanghai AI Laboratory

CPII under InnoHK

dhlin@ie.cuhk.edu.hk

Equal ContributionCorresponding Author

###### Abstract

Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, **Make-it-Real**: **1)**

Figure 1: **Usage of Make-it-Real**. Our method can refine a wide range of albedo-map-only 3D objects from both CAD design and generative models. Our method enhances the realism of objects, enables part-specific material assignment to objects and generate PBR maps that are compatible with downstream engines.

We demonstrate that GPT-4V can effectively recognize and describe materials. **2)** Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. **3)** The correctly matched materials are thenticulously applied as reference for the new SVBRDF material generation according to the original albedo map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets. Our project website is at: https://SunzeY.github.io/Make-it-Real/.

## 1 Introduction

High-quality materials are important for the nuanced inclusion of view-dependent and lighting-dependent effects in 3D assets for traditional graphics pipelines, critical for achieving realism in gaming, online product showcasing, and virtual/augmented reality. However, many existing assets and generated objects often lack realistic material properties, limiting their application in downstream tasks. Furthermore, creating hand-designed realistic textures necessitates specialized graphic software and involves a laborious and time-consuming process, compounded by significant creative challenges .

Traditional computer graphics methods have been either manually creating materials or reconstructing them from physical measurements. Emerging text-to-3D generative models [7; 34; 55; 10; 73; 50; 19; 51] and image-to-3D generative models [69; 58; 25; 63; 22; 62] are successful in creating complex geometries and detailed appearances, but they struggle to generate physically realistic materials, hampering their practical applicability. Recent studies have also explored advanced aspects of appearance generation [11; 75; 70; 8; 40]. However, they often rely on simplified material models. For instance,  lacks the ability to produce metallic maps. All these approaches do not generate corresponding displacement and height maps, which restricts the diversity and realism of the generated materials, especially regarding depth and tactile qualities. Furthermore, these methods typically require relatively long training and inference time. Considering the abundance of high-quality 3D assets online [16; 17] that lack material attributes, and the maturity of 3D generative models in geometry and albedo modeling, we aim to recover materials from high-quality geometry and base-colored 3D meshes.

However, extracting and recovering material representations for 3D meshes is challenging. Unlike previous material recognition methods [40; 4; 20], this difficulty is heightened when identifying and separating different material regions within 3D objects in constricted albedo textures. These maps only reflect the base color and can be distorted, as shown by the globe in the upper right corner of Figure 1. Additionally, shadows and lighting can affect judgment. Thus, the model must have strong material recognition capabilities and prior knowledge of object types and materials.

The emergence of Multimodal Large Language Models (MLLMs) [5; 46; 12; 2; 23; 59] provides novel approaches to problem-solving. These models have powerful visual understanding and recognition capabilities, along with a vast repository of prior knowledge, which covers the task of material estimation. Specifically, we are using GPT-4V(ision) for matching of materials. We first create highly detailed descriptions for materials to build a comprehensive library. Next, we use GPT-4V to retrieve materials for each segmented part of the object, utilizing visual prompts  and hierarchical text prompts. Finally, we meticulously designed algorithms to generate SVBRDF maps with consistent albedo, achieving realistic visual quality.

Notably, our work differs from the aforementioned studies by leveraging prior knowledge from foundation models like GPT-4V to extract and infer materials in albedo-only constrained scenarios. Additionally, we utilize existing material libraries as references to generate corresponding SVBRDF maps on a designed region-to-pixel algorithm. As illustrated in Figure 1, our approach features: **1)** Enhanced 3D mesh realism: Leveraging GPT-4V's visual perception and external knowledge, our method improves the realism, depth, and visual quality of a wide range of mature 3D content generation models. **2)** Part-specific material matching: Ensuring material consistency with a segmentation network and refinement process, enabling precise material property retrieval for each segment. **3)** Rendering engine compatibility: Generating six comprehensive material maps(roughness, metallic, specular, normal, displacement, height), which are compatible with downstream rendering engines. Developers only need to paint albedo textures; material properties are then automatically generated, saving extensive time on detailed ambient occlusion masks and material map creation.

In summary, our contributions are as follows:

* We present the first exploration of leveraging multimodal large language models (MLLMs), e.g.,GPT-4V for material recognition and unleashing their potential in applying real-world materials to extensive 3D objects with albedo-only.
* We create a material library containing thousands of materials with highly detailed descriptions readily for MLLMs to look up and assign.
* We develope an effective pipeline for texture segmentation, material matching and SVBRDF maps generation, enabling the high-quality application of materials to 3D assets.

## 2 Related Work

**3D Object Generation.** The generation of 3D models using deep learning methods has experienced rapid development in recent times. The mainstream research can be primarily divided into two categories. The first category relies on techniques that optimize a Neural Radiance Field (NeRF)  or 3D Gaussian  guided by 2D diffusion model through score-distillation-sampling(SDS) loss [48; 42; 39; 8; 64; 53; 56; 67]. The second category aims at obtaining 3D representation via direct inference model, e.g., Point-E , Shap-E , and LRM , proven fast with high quality through large-scale pretraining [24; 22; 57; 35; 55; 71; 58; 63; 65]. Although the capabilities of these methods are continuously improving, they still lack a high degree of realism in textures. More importantly, since the textures of 3D objects obtained by these methods are shaded, they cannot directly respond to lighting changes under different lighting conditions, leading to less realism. Although some works attempt to generate PBR textures, their results are considerably limited to generate physically realistic materials due to the low robustness of SDS [75; 8]. Our work is the first to introduce the prior knowledge of MLLMs into the texture synthesis process. We verifies that our method can be seamlessly and effectively applied to generated 3D objects, facilitating their downstream applications under different lighting condition.

**Material Capture and Generation.** Recent studies such as Material Palette , MatSim , TwoshotBRDF  have made progress in the recognition and extraction of 3D object materials, allowing for the retrieval of SVBRDF information from images of real materials  and combining object shape and illumination , but they fail to extract and infer the materials of 3D objects with only albedo. On the other hand, works like Paint-it , Matlaber , Collaborative , Fantasia3D , and TANGO  focus on generating text-controllable 3D meshes with physically-based rendering material properties. However, these methods require either extensive training time on BRDF datasets or long inference time for generating materials. Additionally, they are limited to synthesizing only a subset of PBR textures and cannot generate the full range of material properties, such as height and displacement, which are essential for the fine tactile perception of object surfaces.

**Multimodal Large Language Models.** In the wake of the advancements achieved by large language models (LLMs) [5; 46; 12; 2; 23; 59], domain of research has increasingly turned its attention towards multimodal large language models (MLLMs). Recent advances in this field focus on the integration of vision understanding capabilities with LLMs [76; 1; 37; 36; 26; 21; 3; 18; 54; 49; 6].The advent of GPT-4V  has marked a significant milestone in the evolution of MLLMs, demonstrating groundbreaking 2D comprehending capabilities and open-world knowledge. Although GPT-4V cannot directly process 3D data, a pioneering work, GPTEval3D , first exploited GPT-4V's ability in evaluating the quality of generated 3D objects, and found that GPT-4V's judgement was in line with human evaluation. In this work, we delve into a new application of GPT-4V for material assignment of 3D objects.

## 3 Methodology

### Preliminary

Physically Based Rendering (PBR) materials are a compact representation of the bi-directional reflectance distribution function (BRDF), which describes how light is reflected from a surface. PBR material maps primarily encompass seven attributes: Albedo, **R**oughness, **M**etallic, **N**ormal, **S**pecular, **H**eight, and **D**isplacement. Based on the rendering equation , given a location \(x\) and the surface normal \(n\), the incident light intensity at this point is denoted as \(L_{i}(_{i};x)\) along the direction \(_{i}\); BRDF \(f_{r}(_{o},_{i};x)\) denotes the reflectance coefficient of the material viewing from direction \(_{o}\). The observed light intensity \(L_{o}(_{o};x)\) is calculated over the hemisphere \(=\{_{i}:_{i} n>0\}\):

\[L_{o}(_{o};x)=_{}L_{i}(_{i};x)f_{r}(_{o},_{i };x)(_{i} n)d_{i}.\] (1)

Given the advancements in generating high-quality 3D shapes with albedo maps, the restoration of realistic material properties remains a challenge. We highlight a novel problem: given a 3D mesh (\(\)) and known Albedo (\(\)) map, which reflect the object's intrinsic appearance, the goal is to extract and restore all other SVBRDF attributes of the object, i.e. _Make-it-Real_(\(,\)) = \(\{R,M,N,S,H,D\}\). Our setup supports the popular Cook-Torrance analytical BRDF model . In this parameterization, the BRDF includes components for albedo \(b_{a}^{3}\), metallic \(b_{m}\), and roughness \(b_{r}\). For more complex surface simulations, such as displacement and height modeling, we use the Blender rendering engine to simulate the BRDF function \(f_{r}(_{o},_{i};x)\).

### Make-it-Real: A Framework for Material Matching and Generataion

In this section, we outline our material matching and generation pipeline, illustrated in Figure 2, which encompasses three stages: rendering and segmenting 3D meshes, retrieving matching materials using MLLM, and generating spatially varying BRDF maps from coarse to fine.

#### 3.2.1 Rendering and Material Segmentation

To accurately segment different material regions on 3D meshes with albedo maps, we propose an innovative segmentation strategy based on 2D image rendering in Figure 2 (a). Initially, we use rasterization to render the input albedo mesh from various viewpoints to obtain a series of images:

\[(x,y)=(_{}((,v_{t},x,y)),).\] (2)

where \((x,y)\) is the pixel value at image coordinates \((x,y)\), rasterize\((,v_{t},)\) maps the 3D mesh \(\) from viewpoint \(v_{t}\) to 2D screen coordinates, UV\({}_{}()\) converts rasterized coordinates to UV coordinates, \(\) is the albedo map, and \((,)\) samples color from the \(\) using the UV coordinates.

For the rendered images, we employ the Semantic-SAM  to perform preliminary semantic segmentation. Empirically, we select the main viewpoint with the largest projected area of the mesh, as it is more likely to contain more details. To address potential over-segmentation, drawing inspiration from , we extract non-overlapping segments from the masks to form distinct patches,

Figure 2: **Overall pipeline. This pipeline of Make-it-Real is composed of image rendering and material segmentation, MLLM-based material retrieval, and SVBRDF Maps Generation. We finally use blender engine to conduct physically-based rendering.**

as detailed in our approach in Figure 4 (a). These patches are then merged based on similar colors to obtain the final material grouping. We incorporate Set-of-Mark  method to annotate each material segment with a unique identifier, sorted by area size from largest to smallest. This annotation acts as a visual cue, enhancing the visual comprehension capabilities of large multimodal language models.

#### 3.2.2 MLLM-based Material Retrieval

Material library with fine-grained annotations.To enable large multimodal language models to accurately retrieve and match materials, we construct a finely annotated material library, as shown in Figure 3. It is composed of three main components: comprehensive PBR texture maps, highly-detailed records, and a category tree. It comprises 1,400 unique, tileable materials spanning 13 primary categories and 80 subtypes. The data primarily derives from the , which offers comprehensive PBR material textures under a CC0 license with 4K resolution. Each material is represented by seven maps: albedo, roughness, metallic, specular, normal, displacement and height. Accompanying each material are highly-detailed annotations by GPT-4V, offering thorough descriptions of the material's visual characteristics and rich semantic information for the subsequent retrieval process. Created by crawling material sphere images and constructing prompts, these annotations capture subtle differences between materials, facilitating precise retrieval by GPT-4V, as detailed in Appendix B.4.

Hierarchical prompting for material retrieval.Due to the vast size of our material library, feeding all prompts to GPT-4V simultaneously proves inefficient and challenging for memory retention. To ensure efficient and accurate material allocation in segmented areas of 3D meshes, we adopt a hierarchical text prompting approach. The schematic of the designed prompt is shown on the left side of Figure 3, and a complete querying process unfolds in Appendix B.4. This method starts by identifying the primary material types corresponding to each labeled region. Subsequently, hierarchical prompts guide GPT-4V to distinguish specific subclasses within the main material categories. We retrieve all descriptions under these subclasses to ascertain the most fitting descriptions for the segmented blocks. This hierarchical processing enables a more granular search of our material library, identifying the optimal description for each material segment. This approach aids in assigning the most suitable materials to each region and reduces memory and time consumption.

Figure 3: **The process of MLLM retrieving materials from the Material Library. Utilizing GPT-4V model, we develop a material library, meticulously generating and cataloging comprehensive descriptions for each material. This structured repository facilitates hierarchical querying for material allocation in subsequent looking up processes.**

#### 3.2.3 SVBRDF Maps Generation

We propose a method to generate SVBRDF maps on a region-to-pixel scale in Figure 2 (c). Initially, we segment texture map in uv space based on queried material regions on 2D image space. We then estimate BRDF values in pixel space using the object's original albedo map for reference, ensuring consistency with the albedo map. This approach effectively enhances the realism of rendered surfaces.

Region-level texture map partitioning.Upon acquiring segmentation masks for material regions within the 2D rendered space of a 3D mesh, our objective transitions to transposing these segmentations from the 2D image space to the corresponding UV space. As described in Section 3.2.1,we extract 2D image features from 3D mesh points via rasterization, and then we apply the material masks to these features, facilitating the accurate transfer of segmentation to UV space. To project the image feature \(I_{t}\) back to the texture atlas \(_{t}\) with segmented image mask \(m_{t}\) at the viewpoint \(v_{t}\), we apply gradient-based optimization for \(L_{t}\) over the values of \(_{t}\) when rendered through the differential renderer \(\), as presented in Equation (3). In Equation (4), we then compute the difference between \(_{t}^{mask}\) and the initialized \(_{t}\) to transfer the mask image feature \(m_{t}\) into the texture space, represented by \(m_{uv}\). The term \(\) denotes the difference coefficient. Due to the limited perspectives available in rendering, we avoid using a naive median-filling approach to ensure that there are no missing areas on the texture map. Instead, we employ a block-centric clustering based on albedo, as illustrated in Figure 4 (b), to obtain cohesive and refined region masks. The process is shown in Appendix B.1

\[_{_{t}}L_{t}=((, _{t},v_{t})-I_{t}) m_{t}}{ _{t}}.\] (3) \[m_{uv}=(_{i=1}^{3}|_{t}^{mask}- _{t}|>).\] (4)

Pixel-level albedo-referenced estimation.To achieve precise estimation of spatially varying BRDF (SVBRDF) at the pixel level, we draw inspiration from techniques commonly utilized by artists in creating texture maps. Artists typically use albedo maps as a reference for constructing ambient occlusion masks and further generating SVBRDF maps for material properties, which occupies a significant time portion of appearance modeling. Our method involves using the albedo map of the original object as a reference to refine the retrieved materials. We enhance the querying process using a KD-Tree algorithm, which searches for the nearest neighbor pixel index in the key albedo(retrieved map) for each RGB value of the queried albedo(input map) pixel, detailed in Appendix B.2. This process ensures that areas with similar colors exhibit similar BRDF values, avoiding abrupt changes

Figure 4: **Illustrations of mask refinement in 2D image space and UV texture space.****(a)** We effectively cluster concise material-aware masks compared to original segmented parts from . **(b)** We fix missing parts on the uv texture space to get a complete texture partition map.

in material properties; For regions with greater color differences, the distribution of differences aligns consistently with the input albedo, to simulate variations such as embossments or scratches. We retrieves SVBRDF values at pixel level, maintaining texture consistency with the albedo and producing appropriate concave or smooth surface effects. We further analyze the effects in Section 4.2.

Figure 5: **Qualitative results of Make-it-Real refining 3D asserts without PBR maps**. Objects are selected from Objavverse  with albedo maps only.

Figure 6: **Visualization of generated texture maps**. We visualize some SVBRDF maps, where the material maps are well aligned with the albedo maps.

## 4 Experiments

### Experiment settings

To verify the effectiveness of Make-it-Real, we conduct refinement experiments mainly on two types of objects. The first type is artificial 3D assets, with the primary model from Objavverse  filtered by ; The second type is objects generated by state-of-the-art 3D generation methods. For existing 3D assets, we pick 200 objects with diverse textures from Objavverse by human experts. For 3D generative models (InstantMesh , TripoSR , MVDream , Instant3D  and Fantasia3D ), we also generate 200 objects for each methods use prompts designed in GPTEval3D . We use Make-it-Real to refine the objects and compare the texture quality before and after refinement. We perform both GPT-4V  based evaluation and user study on the above objects (Detailed guidence and prompts for evaluation are available in Appendix B.5).

In addition to evaluation details, we also provide information about the rendering procedure in Appendix B.3 for reproducibility. This includes the rendering tools used in the experiments, hyper parameters related to back-projection, and the basic performance metrics of the model during the experiments, such as time, memory, and number of queries.

### Experiment results

Texture refinement for existing 3D assets.As shown in Figure 5, assets processed through Make-it-Real demonstrate the capability to accurately segment objects, assign various suitable materials, and synthesize high-fidelity, photorealistic textures. Some materials exhibit notable highlights, such as a marble bathtub and a globe made of gold. The interaction of these different materials with light varies significantly, leading to diverse reflective effects under the same environmental lighting, thus presenting a range of textures. Additionally, material properties vary across different regions; for instance, the globe's landmass and handle exhibit gold characteristics, while other parts are identified

Figure 7: **Qualitative comparisons** between Make-it-Real refining results and 3D objects generated by edge-cutting 3D content creation models. The upper row depicts image-to-3D models (InstantMesh and TripoSR), and the lower row shows results of text-to-3D models.

as plastic. Furthermore, the texture and appearance of materials also vary, such as the subtle wrinkles on the red box in the second column and the more pronounced color contrast at the base of the blue box, which enhances realism and reflects signs of use. Due to the albedo-referenced algorithm design in Section 3.2.3, regions with similar colors have similar BRDF values, avoiding abrupt changes in material properties, such as the continuous gold surface of the globe and the silver body of the kettle. Regions with significant color differences display consistent differential distributions with the key map, such as the embossed textures on the lower left corner of the water bottle in Figure 5 and the subtle particle variations on the surface of the red oil drum. Additionally, visualization of texture maps demonstrates reasonably consistent textures with albedo, shown in Figure 6. Quantitive results are shown in Table 1, and more qualitative results can be found in Appendix E.

**Texture refinement for generated 3D objects.** Figure 7 displays the qualitative results. By leveraging our model for enhancement, we observe that Make-it-Real successfully generates appropriate material maps for both image-to-3D and text-to-3D models.

Similarly, our Make-it-Real successfully paint these models with materials. As reported in Table 1, human experts consistently favor the objects post-refinement across all evaluated 3D content creation methods. This preference aligns with the evaluations performed by GPT-4V, indicating a general consensus on the enhancement in quality achieved through Make-it-Real refinement process.

    &  &  &  \\  & & Base object & +Make-it-Real & Base object & +Make-it-Real \\ 
3D assets & Objavverse  & 15.2\(\%\) & 84.8\(\%\) & 22.2\(\%\) & 77.8\(\%\) \\   & InstantMesh  & 28.2\(\%\) & 71.8\(\%\) & 31.1\(\%\) & 68.9\(\%\) \\  & TripoSR  & 36.4\(\%\) & 63.6\(\%\) & 33.0\(\%\) & 77.0\(\%\) \\   & Instant3D  & 38.5\(\%\) & 61.5\(\%\) & 35.4\(\%\) & 64.6\(\%\) \\  & MVDream  & 44.1\(\%\) & 55.9\(\%\) & 41.5\(\%\) & 58.5\(\%\) \\  & Fantasia3D  & 46.2\(\%\) & 53.8\(\%\) & 48.7\(\%\) & 51.3\(\%\) \\   

Table 1: **GPT evaluation and user preference**. GPT’s and user’s preference comparison on Make-it-Real refined objects sourced from existing 3D assets and state-of-the-art 3D generation methods.

Figure 8: **Ablation study of material segmentation refinement. Compared to direct usage of SemanticSAM , Our post-process tailored for material segmentation on 3D object can produce more consistent results.**

Figure 9: **Ablation study of missing part refinement. Our method on the bottom row produces consistent texture maps and avoids the missing parts of material texture.**

### Ablation Study

**Effects of mask refinement module.** In Section 3.2.1, we performed additional material post-processing on the segmentation outcomes, as depicted in Figure 8. The refined results in the last row indicate that our module achieves precise material segmentation for most standard objects, thereby enabling more accurate queries by GPT-4V. In Section 3.2.3, we addressed the completion of missing regions in the texture map within the UV space at the regional level, as illustrated in Figure 9. This method not only increases texture coverage but also enhances the visual quality and consistency of the refined 3D model. More ablation studies can be found in Appendix D.

**Effects of different texture maps.** We validate the impact of various texture maps generated by Make-it-Real on the appearance of 3D objects, as illustrated in Figure 16 of Appendix D.1. We provide a detailed qualitative analysis of how different maps enhance the visual texture of materials. For example, increasing metallic results in dampening of the base albedo and increasing in the shine on the surface, while reducing roughness gives the surface a smoother appearance and enhances highlights. Meanwhile, displacement and height maps contribute to the fine-grained bump details on the object's surface.

**Effects of different UV mappings.** Since UV mapping is a crucial step in the 2D-3D alignment technique of our method, we conduct experiments to assess how its quality impacts model performance. The results, shown in Appendix D.2 and in the second column of Figure 17, indicate that UV mappings with excessive fragmentation and color entanglement can cause issues. However, our method still performs well with artist-created UV mappings and Blender's built-in mapping techniques. This indicates our method still demonstrates good robustness with many mapping techniques.

## 5 Conclusion

In this paper, we present a novel framework leveraging MLLMs prior of the world to build a material library and proposing an automatic pipeline to refine and synthesize new PBR maps for initial 3D models, achieving highly photo-realistic PBR textures maps synthesis. Experimental results confirm that our approach can automatically refine both generated and CAD models to achieve photo-realism under dynamic lighting conditions. We believe Make-it-Real is a new and promising solution in the last few procedures of AI based 3D content creation pipeline with the development of MLLMs like GPT-4V  as well as the roaring field of deep learning based 3D generation from scratch.

## 6 Acknowledgement

This project is funded in part by Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of CPII under the InnoHK.