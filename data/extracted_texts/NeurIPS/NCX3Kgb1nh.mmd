# Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking

Gabriel Rioux

Center for Applied Mathematics

Cornell University

&Apoorva Nitsure

MIT-IBM Watson AI Lab

IBM Research

&Mattia Rigotti

MIT-IBM Watson AI Lab

IBM Research

&Kristjan Greenewald

MIT-IBM Watson AI Lab

IBM Research

&Youssef Mroueh

MIT-IBM Watson AI Lab

IBM Research

###### Abstract

Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models.

## 1 Introduction

In choice theory, economics, finance, and models benchmarking, agents are faced with stochastic prospects that are (eventually) multivariate random variables which they wish to order according to a utility or risk measure of interest. To formalize such a notion of ordering of stochastic quantities, the concept of stochastic dominance can be utilized.

In the univariate case, a standard notion of order is given by _First order Stochastic Dominance_ (FSD) which can be expressed in terms of the quantiles of the underlying random variables . To wit, a random variable \(X\) dominates another random variable \(Y\) in FSD, if it has larger quantiles than \(Y\) across all percentiles. A weaker notion of FSD, called almost stochastic dominance was introduced in del Barrio et al. (2018). Their approach is based on optimal transport (OT) and consists of measuring a ratio which quantifies how close \(X\) is to dominating \(Y\). del Barrio et al. (2018) further lay the groundwork for principled statistical analysis of almost FSD by establishing a central limit theorem for the empirical ratio as well as consistency of the bootstrap. Dror et al. (2018); Ulmer et al. (2022); Nitsure et al. (2023) used the almost FSD testing framework in benchmarking Large Language models to make statistically significant decisions regarding which model to select when these models are evaluated with a metric of interest on test data.

Our main motivation is to extend the testing framework for almost FSD of del Barrio et al. (2018) to the multivariate case as to enable applications with dependencies between metrics. For instance, the problem of multivariate portfolio selection in financial applications has been treated via a reduction to univariate orders (Kouaissah, 2021). Another application of interest is that of multi-metric benchmarking which is central nowadays in ranking and selecting Large Languages Models (Bommasani et al., 2023; Chang et al., 2023; Huang et al., 2023; MoasicLM, 2023; Wolf, 2023; Zhang and Hardt, 2024). Current approaches such as Nitsure et al. (2023) use aggregation techniques to reduce the ordering to the univariate case thereby ignoring dependencies between metrics.

Our starting point is the so-called _standard multivariate stochastic order_ (see Chapter 6 in Shaked and Shanthikumar, 2007). This order can be expressed in terms of couplings between random vectors. This insight allows us to follow the approach of del Barrio et al. (2018) and define an almost multivariate FSD via OT in Section 2. This notion of stochastic dominance can be defined using multivariate violation ratios that are expressed as optimal transport problems with smooth costs (Manole and Niles-Weed, 2024; Hundrieser et al., 2022; Groppe and Hundrieser, 2023). Given that empirical OT suffers from the curse of dimensionality, we resort in Section 3 to entropic regularization (Cuturi, 2013) to alleviate that issue and hence define Entropic Multivariate Violation Ratios. We establish in Section 3 convergence of these entropic violation ratios as the regularization parameter goes to zero, as well as a central limit theorem and bootstrapping consistency in Section 4 using the functional delta method (Romisch, 2006). We highlight that the delta method has seen general success for proving limit theorems with entropic OT (Hundrieser et al., 2024; Goldfeld et al., 2024, 2024). Armed with this theory, we propose a new framework for hypothesis testing of multivariate stochastic dominance and apply it to multi-metric benchmarking of LLMs. Multivariate FSD captures the dependencies between the metrics in this setting and leads to a more robust ordering.

**Notation** The indicator function of a set \(A^{d}\) is denoted \(_{A}(x)\) and takes the value \(1\) if \(x A\) and \(0\) otherwise. We also adopt the following shorthand notation, \(_{+}=[0,)\), the maximum of two numbers \(a,b\) is denoted by \(a b\), and for vectors \(x,y^{d}\), we write \(x y\) to indicate that \(x_{i} y_{i}\) for every \(i\{1,,d\}\).

Throughout, \((^{d})\) is the set of all probability measures on \(^{d}\). A measure \((^{d})\) is said to be sub-Gaussian with parameter \(^{2} 0\) with respect to the \(1\)-norm provided \(_{}[(}{{2}}^{2})] 2\).

Convergence in distribution of random variables is denoted by \(}{{}}\) (in the sense of Hoffmann-Jorgensen when necessary, see Chapter 1 in van der Vaart and Wellner, 1996).

## 2 Optimal transport and stochastic order

### FSD and Almost Stochastic Dominance in One Dimension

To properly motivate our results on multivariate FSD, we first review some theory for the univariate setting. For random variables \(X,Y\), it is said that \(X\) dominates \(Y\) in the stochastic order (denoted \(X}{}Y\)) if \((X t)(Y t)\) for every \(t\). Formally, this means that the inequality \(Y X\) generally holds for a given instantiation of these random variables. This condition can be cast, equivalently, as \(F_{Y}^{-1}(t) F_{X}^{-1}(t)\) for every \(t(0,1)\), where \(F_{X}^{-1}(t)\) and \(F_{Y}^{-1}(t)\) are the quantile functions for \(X\) and \(Y\) respectively. With this formulation in mind, del Barrio et al. (2018) propose the following index of almost stochastic dominance;

\[_{_{2}}(,)=^{1}(F_{Y}^{-1}(t)-F_{X}^ {-1}(t))_{+}^{2}dt}{_{2}^{2}(,)},_{2}^{2}(,)= _{0}^{1}(F_{X}^{-1}(t)-F_{Y}^{-1}(t))^{2}dt,\]

\(=(X),=(Y)\), and, for \(z\), \((z)_{+}^{2}=(0 z)^{2}\) denotes the squared hinge loss. Here, the numerator captures the degree to which \(X\) fails to dominate \(Y\) whereas the denominator serves as a normalizing constant so that \(_{_{2}(,)}\). Indeed, as \((x-y)_{+}^{2}+(y-x)_{+}^{2}=(x-y)^{2}\), we see that \(_{_{2}}(,)=0\) precisely when \(F_{Y}^{-1}(t) F_{X}^{-1}(t)\) for a.e. \(t(0,1)\) whereas \(_{_{2}}(,)=1\) when the opposite inequality holds. del Barrio et al. (2018) then propose to test the null hypothesis \(_{_{2}}(,)_{0}\) for some \(_{0}\) sufficiently close to \(0\), corresponding to the case where \(X\) almost dominates \(Y\) in the stochastic order, versus the alternative hypothesis \(_{_{2}}(,)>_{0}\). To this end, they provide a central limit theorem for the statistic \(_{_{2}}\) and propose to construct the rejection region via bootstrap estimation of the limiting variance. Similar results were obtained for a notion of almost second order stochastic dominance in Nitsure et al. (2023).

We highlight that the index \(_{_{2}}\) can be connected to OT. Indeed, by Theorem 2.9 in Santambrogio (2015), the numerator and the denominator in \(_{_{2}}(,)\) can be written, respectively, as \(_{(,)}(y-x)_{+}^{2}d(x,y)\), and \(_{(,)}(y-x)^{2}d(x,y)\), where \((,)\) denotes the set of all couplings of \((,)\). These problems are (univariate) instances of the well-studied OT problem

\[_{c}(,)_{(,)} cd,\] (1)

where \(c:^{d}^{d}_{+}\) is a given cost function. We highlight that the costs considered herein are such that an optimal coupling always exists (see Theorem 4.1 in Villani, 2009). This connection will serve as our basis for extending the notion of almost stochastic dominance to the multivariate setting.

### Multivariate FSD and its Relaxation via Optimal Transport with Compatible Costs

In the sequel, we provide a general framework for assessing multivariate almost FSD using a purely OT-based methodology. Following Chapter 6 and Theorem 6.B.1. in Shaked and Shanthikumar (2007), given the random vectors \(X,Y^{d}\), we say that \(X\) dominates \(Y\) in the usual stochastic order (denoted \(X}{}Y\)) provided that there exists a coupling \((,)\) of \((X,Y)\) satisfying \(()=1\) (i.e. for each \(i=1,,d\), \(_{i}_{i}\) with probability one). This condition can be cast as follows:

**Lemma 1**.: _Letting \(\) (resp. \(\)) denote the law of \(X\) (resp. \(Y\)), \(}{}Y\) if \(_{c}(,)=0\), where \(c:^{d}^{d}_{+}\) is the cost function \(c(x,y)=_{\{x y\}}(x,y)\)._

Evidently, the cost \(_{\{x y\}}(x,y)\) in Lemma 1 can be replaced by any nonnegative cost function \(c(x,y)\) satisfying \(c(x,y)=0\) if and only if \(x y\) and it still holds that \(X}{}Y\) if \(_{c}(,)=0\).

We denote the set of all such costs by \(_{}\).

**Definition 1** (OT Costs Compatible with Multivariate FSD).: _The set of all cost functions which are compatible with multivariate FSD in the sense that \(_{c}((X),(Y))=0\) implies that \(X}{}Y\) is given by \(_{}=\{c:^{d}^{d}_{+}\) such that \(c(x,y)=0\) if and only if \(x y\}\)._

A simple recipe for generating cost functions in \(_{}\) is to take any univariate function \(h:_{+}\) with the property that \(h^{-1}(\{0\})=(-,0]\) and define \(c(x,y)=_{i=1}^{d}h(y_{i}-x_{i})\). For notational simplicity, we write \(_{h}\) to denote the OT cost with this type of cost function even when the aforementioned property does not hold. The results presented in the following sections require some additional smoothness assumptions on the function \(h\) discussed above which we summarize presently.

**Definition 2** (Smooth Costs).: _The function \(h:_{+}\) satisfies the smoothness condition \((_{})\) if \(h\) is Lipschitz continuous with constant \(L 0\) (that is, \(|h(x)-h(y)| L|x-y|\) for every \(x,y\)) and, for \(k= d/2+1\), \(h\) is \(k\)-times continuously differentiable with derivatives of order \(s k\) satisfying \(|h^{(s)}(x)| C_{k}(1+|x|)^{p_{k}}\) for some \(C_{k}<\) and \(p_{k}>1\) which may depend on \(k\)._

We now discuss some examples of cost functions of interest.

**Example 1** (Examples of OT Costs).: 1) _The function \(h(z)=e^{-}{{z}}}\) for \(z(0,)\) and \(0\) otherwise is known to be smooth (see Example 1.3 in Tu, 2011) and satisfies \(h^{-1}(\{0\})=(-,0]\). It is easy to see that all derivatives of \(h\) are \(0\) on \((-,0]\), and decay to \(0\) at infinity (and hence are bounded on \(\)) so that \(h\) satisfies \((_{})\) for any \(d\), and induces a cost function in \(_{}\)._

2) _The squared hinge function \(h(z)=(z)_{+}^{2}\) considered in Section 2.1 has linear growth, but is non-smooth. Although this function can be smoothed using e.g. mollification as introduced in Friedrichs (1944), this will result in a cost \(c\) which is not an element of \(_{}\) and may be costly to implement due to the convolution operation used in mollification._

3) _The logistic function \(h(z)=(1+e^{ z})\) for \(>0\) has linear growth and derivative \(h^{}(z)=}{1+e^{ z}}=( z)\), where \((z)=}\) is the sigmoid function. As \(^{}(z)=(z)(1-(z))\), it is easy to see that all derivatives of \(h\) are bounded on \(\) so that the assumption \((_{})\) is satisfied for any \(d\). Although \(h\) does not induce a cost in \(_{}\), it is increasing and decays to \(0\) faster than \(e^{ z}\) as \(z-\). Moreover, if the induced cost satisfies \(c(x,y)_{0}\), then \(_{i=1}^{d}(y_{i}-x_{i})(e^{_{0}}-1)\). Thus, if \(_{c}(,)=_{\{c_{0}\}}cd^{}+_{\{c> _{0}\}}cd^{}=\) for an optimal plan \(^{}\) and some small \(>0\), one has that \(_{\{c>_{0}\}}cd^{}\) so that \(^{}(\{c>_{0}\})}{{_{0}}}\) i.e. \(^{}\) assigns at least mass \(1-}{{_{0}}}\) to points \((x,y)\) for which \(_{i=1}^{d}(y_{i}-x_{i})(e^{_{0}}-1)\). Hence, for large \(\), \(c\) enforces similar properties to a cost in \(_{+}\). Note also that \(h\) can be viewed as a smooth surrogate for the \(0/1\) loss for large \(\)._

At this point, a multivariate analogue to the univariate almost stochastic domination could be defined by analogy with Section 2.1. However, we highlight two major impasses which make the entropically regularized index considered in the following sections a far more palatable option in dimension \(d>1\). First, it is well-known that the expected rate of convergence of empirical OT generally suffers from the curse of dimensionality in statistical estimation; scaling as \(n^{-1/d}\)(cf. e.g. Manole and Niles-Weed, 2024). Although Hundrieser et al. (2022) improve these rates as to depend on the minimum of the intrinsic dimensions of \(,\) in place of \(d\), entropic optimal transport exhibits a preferable parametric rate of convergence. Next, solving the OT problem numerically between two finitely discrete distributions supported on \(N\) points requires solving a linear program in \(N^{2}\) variables which can be prohibitive for even moderately sized problems.

## 3 Entropic Regularization of OT with Multivariate FSD Compatible Costs

Before defining the regularized index, we first provide some background on entropic optimal transport (EOT) with a cost \(c:^{d}^{d}_{+}\). EOT is defined by regularizing the OT problem (1) as

\[_{c,}(,)=_{(,)} cd+ _{}(||),\] (2)

where \( 0\) is a regularization parameter and \(_{}\) is the Kullback-Leibler divergence defined by \(_{}(,)=()d\) if \(\) is absolutely continuous with respect to \(\) and \(_{}(,)=+\) otherwise. When \(=0\), we recover the standard OT problem. If \(c L^{1}()\), (2) admits a unique solution and is paired in strong duality with the problem

\[_{ L^{1}(), L^{1}()} d+ d - e^{}d(x,y) +.\] (3)

Solutions to (3) are known to be almost surely unique up to additive constants (i.e. if \((,),(^{},^{})\) solve (3), \(=^{}+C\)\(\)-almost surely and \(=^{}-C\)\(\)-almost surely for some constant \(C\)) and are uniquely determined for \(\)-a.e. \(x\) and \(\)-a.e. \(y\) by the so-called Schrodinger system

\[e^{-}{{}}}= e^{ }d(y), e^{-}{{}}}= e^{}d(x),\] (4)

which implies that \( e^{}d(x,y)=1\). EOT potentials satisfying (4) on the whole space are known to exist and are unique up to additive constants (see Lemma 6). We refer the reader to Nutz (2021) for a comprehensive introduction on EOT including all stated results.

Entropic regularization of OT problems was introduced in the seminal work of Cuturi (2013) as a means to accelerate computation by utilizing Sinkhorn-Knopp's matrix scaling algorithm which can be efficiently implemented on GPUs. Interestingly, entropic regularization also alleviates the curse of dimensionality rates in statistical estimation inherent to standard OT; for instance Genevay et al. (2019) and Mena and Niles-Weed (2019) show that the plug-in estimator for the EOT cost with fixed \(>0\) and cost \(c(x,y)=\|x-y\|^{2}\) achieves a parametric expected rate of convergence with a dimension dependent constant (see also Groppe and Hundrieser, 2023; Stromme, 2023 for related results with the dimension replaced by the minimum intrinsic dimension of \(\) and \(\)); del Barrio et al. (2023) and Goldfeld et al. (2024) further establish a central limit theorem in this setting.

Given that EOT is meant to approximate OT, we derive the properties of \(_{h,}(,)\) and its solutions as \( 0\). First, we quantify the rate of convergence of \(_{h,}\) to \(_{h,0}\) under mild conditions, then show that solutions of \(_{h,}\) converge to solutions of \(_{h,0}\) as \( 0\) in a suitable sense.

**Theorem 1** (Stability as \( 0\)).: _Let \(,(^{d})\) have finite first moment, \(h\) be a function satisfying \((_{})\), and fix \(>0\). Then,_1. _for any_ \([0,1)\) _satisfying_ \(^{-d}(}{{}})^{d}\)_, we have that_ \[0_{h,}(,)-_{h,0}(,) d( }{{}})+(5C_{}+20d)\] _for_ \(C_{}=_{j=1}^{d}|x_{j}|^{1+}d_{0}(x)_{j=1}^{d} |x_{j}|^{1+}d_{1}(x)\)_._
2. _if_ \(_{}\) _is the unique solution to_ \(_{h,}(,)\) _for_ \(>0\)_, then there exists a subsequence of_ \((_{})_{ 0}\) _which converges weakly to a solution of_ \(_{h,0}(,)\)_._

We highlight that the implications of Theorem 1 require only the Lipschitz condition imposed in \((_{})\). The proof of the first result follows that of Theorem 3.3 in Eckstein and Nutz (2023) which controls the error of approximating the OT cost and OT plan using discretizations of the measures at play. The main novelty in our approach is to provide explicit constants and a simple argument showing that the rate at which a general measure on \(^{d}\) can be approximated by a finitely discrete measure on at most \(n\) points under the \(1\)-Wasserstein distance scales at worst as \(n^{-}{{d}}}\) for sufficiently large \(n\). The second statement is proved using the machinery of \(\)-convergence (see Maso, 1993). Complete proofs are provided in Appendix C.2.

## 4 Entropic Multivariate FSD Violation Ratio and Testing

By analogy with the univariate case described in Section 2.1, we consider a normalized index of stochastic order violation given by

\[_{h,}(,)=_{h,}(,)}{ _{,}(,)},\] (5)

we adopt the convention that \(_{h,}(,)=0\) whenever \(_{,}(,)=0\). Here, \(_{,}(,)\) is the EOT problem with cost \(c(x,y)=_{i=1}^{d}h(y_{i}-x_{i})+h(x_{i}-y_{i})\). This cost function is induced by the function \((z)=h(z)+h(-z)\) and hence satisfies \((_{})\) provided that \(h\) satisfies \((_{})\). Moreover, \(_{h,}(,)_{,}(,)\) by construction so that \(_{h,}(,)\) yielding a normalized index. The corresponding notion of entropic multivariate almost stochastic dominance can thus be defined.

**Definition 3** (Entropic Multivariate Almost FSD).: _We define \((h,,_{0})-\), the entropic multivariate almost FSD via the violation ratio as follows:_

\[)-}{} _{h,}(,)_{0}.\]

In light of Theorem 1, \(_{ 0}_{h,}(,)=_{h,0}( ,)}{_{,0}(,)}\) with the convention that this latter quantity is zero when \(_{,0}(,)=0\). In the case that \(h\) is the squared hinge function described in Example 1 and \(d=1\), we recover the univariate index from Section 2.1. As aforementioned, the squared hinge function is not sufficiently smooth to enable us to characterize the asymptotic fluctuations of the empirical index in arbitrary dimensions. See Example 1 which lists other examples of costs satisfying \((_{})\); this condition is sufficient for the following statistical developments. We underscore that the choice of cost influences the notion of stochastic dominance reflected by the violation ratio. In applications where a practitioner wishes to formulate a domain-specific notion of dominance, a data-driven approach can be employed by replacing the fixed cost \(h\) in the previous development by a collection of costs \((h_{i,_{i}})_{i=1}^{d}\) for each dimension where \(_{i}\) is the corresponding parameter (e.g. \(\) in the logistic function) and optimizing over these parameters. The results presented herein readily adapt to this setting, but, for simplicity, we restrict our attention to the case of a fixed \(h\) throughout.

### Statistical Properties

We now lay the groundwork for performing principled statistical inference with the empirical estimator of the entropic index \(_{h,}\). Namely, we establish the asymptotic properties of the plug-in estimator \(_{h,}(_{n},_{n})\), where \(_{n}=_{i=1}^{n}_{X_{i}},_{n}=_{j=1}^{n}_{Y_{j}}\) are the empirical distributions from \(n\) independent observations, \((X_{i})_{i=1}^{n}\) and \((Y_{j})_{j=1}^{n}\) of \(\) and \(\) respectively. Furthermore, we establish consistency of the bootstrap procedure. To this end, given sets of \(n\) iterations observations of \(\) and \(\), \((X_{i})_{i=1}^{n}\) and \((Y_{j})_{j=1}^{n}\) as above and sets \((X_{i}^{B})_{i=1}^{n}\) and \((Y_{j}^{B})_{j=1}^{n}\) of \(n\) independent samples from \(_{n}\) and \(_{n}\), \(_{n}^{B}:=_{i=1}^{n}_{X_{i}^{B}}\) and \(_{n}^{B}:=_{j=1}^{n}_{Y_{j}^{B}}\) are the corresponding bootstrap empirical distributions. \(^{B}\) denotes the conditional probability given the data.

**Theorem 2** (Limit distribution and bootstrapping).: _Assume that \(,(^{d})\) are sub-Gaussian with a shared parameter \(^{2}>0\) and that \(h\) satisfies \((})\). Let \((_{h},_{h})\) and \((_{},_{})\) be any pairs of optimal potentials for \(_{h,}(,)\) and \(_{,}(,)\) respectively satisfying the Schrodinger system (4) on \(^{d}^{d}\). Then, if \(_{,}(,)>0\),_

1. \((_{h,}(_{n},_{n})- _{h,}(,))N(0,^{2}),\) _a mean-zero Gaussian with variance_ \(^{2}=_{}(_{,}(, )}_{h}-_{h,}(,)}{_{, }(,)^{2}}_{})+_{}(_{,}(,)}_{h}-_{, }(,)}{_{,}(,)^{2}}_{h}).\)__
2. _If_ \(^{2}>0\)_,_ \(_{l}|^{B}((_{h, }(_{n}^{B},_{n}^{B})-_{h,}(_{n}, _{n})) t)-(N(0,^{2}) t)|}{}0.\)__

Observe that the limiting distribution in Theorem 2 is non-pivotal in the sense that the variance depends on the population distributions \((,)\) rendering direct estimation of the limiting variance highly non-trivial. The bootstrap consistency result in the second point enables us to establish confidence intervals for \(_{h,}(,)\). Explicitly, if \(_{}\) denotes the smallest value of \(t\) for which \(^{B}(_{h,}(_{n}^{B},_{n}^{ B}) t) 1-\) for any \((0,1)\), then \(_{h,}(,)[0,2_{h,}(_{n}, _{n})-_{}]\) with probability approaching \(1-\) for any \((0,1)\) due to Lemma 23.3 in Van der Vaart (2000).

The proof of Theorem 2 is based on the functional delta method (Romisch, 2006) which extends the standard delta method to functionals defined on normed vector spaces, following the framework of Goldfeld et al. (2024). Formally, this approach consists of showing that the functional mapping \(^{2}\)-sub-Gaussian distributions \((,)\) to \(_{h,}(,)\) is directionally differentiable at \((,)\) and Lipschitz continuous in a suitable sense and that the relevant potentials lie in a space of sufficiently smooth functions using the assumption \((})\). Smoothness of the potentials is crucial to ensure that the empirical processes \((_{n}-)\), \((_{n}-)\) converge when treated as functionals on the aforementioned space of smooth functions. Complete details are included in Appendix C.3, and a primer on the functional delta method can be found in Appendix D.

**Remark 1** (On Theorem 2).: 1) _We note that the condition that \(_{,}(,)>0\) in Theorem 2 is satisfied except in certain degenerate settings. Indeed, for a general nonnegative cost \(c\), \(_{c,}(,)=0\) if and only if \( cd=0\) as follows from the fact that \(}(\|) 0\) with equality if and only if \(=\). In particular, \(_{,}(,)=0\) if and only if \(h(x_{i}-y_{i})=h(y_{i}-x_{i})=0\) for every \(x()\) and \(y()\) and every \(i\{1,,d\}\). If \(h\) is chosen as to generate a cost function which is compatible with multivariate FSD (recall Definition 1), \(h^{-1}(\{0\})=(-,0]\) so that \(_{,}(,)=0\) if and only if \(\) and \(\) are point masses at some shared \(a^{d}\)._

2) _Theorem 2 is presented in the balanced case with empirical measures from \(n\) samples. In the case where \(_{n}\) and \(_{m}\) are empirical measures from \(n m\) samples with \( s(0,1)\), the implications of Theorem 2 are easily seen to hold with \(}\) in place of \(\) and \(_{s}^{2}=_{}(_{,}( ,)}_{h}-_{h,}(,)}{_{ ,}(,)^{2}}_{})+_{}( _{,}(,)}_{h}-_{ ,}(,)}{_{,}(,)^{2}}_{})\) in place of \(^{2}\)._

### Multivariate FSD Hypothesis Testing In ML Models Benchmarking

With the statistical properties of the violation ratio in hand, we now consider using the violation ratio in the context of statistical testing for \((h,,_{0})-\) FSD. Consider two \(d\)-dimensional distributions \(\), \(\). In our application, these will correspond to the distributions of performance of two language models' responses evaluated on \(d\) metrics. Given \(n,m\) samples from \(\), \(\) respectively, we can apply Theorem 2 to create statistically valid tests comparing \(\) and \(\). Similarly to Nitsure et al. (2023), we consider both absolute and relative testing (see Nitsure et al., 2023 for a complete discussion).

**Absolute testing** The most straightforward application of Theorem 2 is to specify a desired threshold \(_{0}\) and consider the following hypothesis test for \((h,,_{0})-\) FSD: \(H_{0}:)-}{}\) versus the alternative \(H_{1}:)-}{}\). Note that \(\) dominating \(\) would be tested separately. Given a desired confidence \(1-\), the central limit theorem and bootstrap results in Theorem 2 suggest rejecting \(H_{0}\) if

\[_{h,}(_{n},_{m})_{0}+}_{B}(_{n},_{m})^{-1}(),\] (6)

where \(_{B}^{2}\) is the bootstrapped variance (See Algorithm 1) and \(\) is the CDF of the standard normal distribution. By Theorem 2, this test will be asymptotically valid.

**Relative testing** A downside of absolute testing is that it requires specifying a threshold \(_{0}\). This threshold can be meaningful in pairwise comparisons, but when multiple distributions (e.g. multiple language models evaluations) are being compared for ranking purposes, it is difficult to determine a priori what threshold to use to ensure that the distributions can be separated. As in Nitsure et al. (2023), we therefore also present a _relative_ test that compares each of \(k\) random vectors with measures \(_{1},,_{k}\) using a one-versus-all violation ratio. First, consider all pairs of violations ratios between the \(k\) measures: \(_{i,j}^{(h,)}=_{h,}(_{i},_{j})\) for \(i,j\{1 k\},i j.\) Let \(M=(_{1},_{k})\), and define the one-versus-all violation ratio of the dominance of \(_{i}\) on all other variables \(_{j},j i\): \(_{i}^{(h,)}(M)=_{j i}_{ij}^ {(h,)}\). We can then define _relative stochastic dominance_\((h,)\)-R-FSD as \(_{i_{1}}}{}_{i_{2}}}{}_{i_{k}}_{i_{1}}^{(h,)}(M) _{i_{k}}^{(h,)}(M).\) Here the most dominating model is the one with the lowest one-versus-all violation ratio. Testing for relative dominance of \(_{i}\) on \(_{j}\) we can then compare their one-versus-all ratios via the following statistic: \(_{ij}^{()}(M)=_{i}^{()}(M)-_{j }^{()}(M).\)To test for \((h,)\)-R-FSD of \(_{i}\) versus \(_{j}\) then, we have the null hypothesis \(H_{0}:_{ij}(M) 0\) versus the alternative \(H_{1}:_{ij}(M)<0\). It is possible to extend the central limit theorem and bootstrapping results in Theorem 2 to this relative statistic under an independence assumption (omitted for brevity). Let \(_{n}=(_{1,n},_{k,n})\) be the empirical measures for \(n\) samples from each distribution. As in the absolute case, we then reject \(H_{0}\) with a confidence \(1-\) if: \(_{i_{1},i_{2}}(_{n})}_{B, }(i_{1},i_{2})^{-1}()\) where \(_{B,}^{2}(i_{1},i_{2})\) is the bootstrapped variance (see Algorithm 1 for the variance expression).

**Multitesting and Ranking** To apply the above multivariate FSD violation ratio hypothesis tests to ranking of multiple distributions, we follow the approach of Nitsure et al. (2023). We aggregate the set of all pairwise tests, ensure multitesting statistical validity via Family-Wise Error Rate (FWER) control, and, if needed, aggregate the pairwise test results into a numerical ranking. Our approach is described below and summarized in Algorithm 1. The overall complexity of performing one pairwise test is dominated by the cost of computing the EOT cost with \(h\) and \(\). To this end, the Sinkhorn algorithm is used (Cuturi, 2013), which computes EOT between distributions on \(N\) points with a complexity of \(O(N^{2}K(d)+N^{2})\), where \(K(d)\) denotes the cost of complexity of computing the cost \(c(x,y)\) between \(x,y^{d}\).

**Ranking multiple distributions** In our experiments below, we seek to use the pairwise tests above to obtain a statistically valid ranking of a set of \(k\) random vectors \(X^{(i)}\) with measures \(^{(i)}\), e.g. samples of \(X^{(i)}\) can be the per-sample evaluation metrics for a set of language models. To rank \(k\) models at a specified significance level \(\), we first test all \(k^{2}-k\) pairs \((^{(i)},^{(j)})\), \(i j\), employing a FWER correction (see next paragraph) to guarantee a valid control on the overall false rejection rate. This yields a set of trinary outcomes for each \((i,j)\) pair, with 1 if the null is rejected in the positive direction, -1 if the null is rejected in the negative direction, and 0 if the null is not rejected. These pairwise rankings are then combined into a single rank using a simple Borda count (de Borda, 1781) rank aggregation algorithm.

**Multitesting FWER control** When running a family of \(T\) tests each at a significance level \(1-\), the true probability that at least one test falsely rejects the null scales with \(T\). If the output of all \(T\) tests needs to be trusted simultaneously, instead it is desirable that the probability of _any_ test falsely rejects the null is less than or equal to the specified \(\). Achieving this requires adjusting, or "correcting" the significance level of each of the \(T\) tests, in a process called Family-Wise Error Rate (FWER) control. In the present work, we use the Bonferroni correction, which sets the significance level of the \(i\)th test to \(1-_{i}\) with \(_{i}=/T\). Note that while the Bonferroni correction is known to be pessimistic, we choose it as it sets uniform significance levels for all tests, as opposed to other strategies such as the Holm correction (Holm, 1979) which is tighter but yields highly nonuniform statistical power across the family of tests. Exploring sensible ways to employ nonuniform FWER control approaches in the context of performance ranking is an interesting avenue for future work.

Experiments

All experiments were run on NVIDIA A100 80GB GPUs using PyTorch (Ansel et al., 2024) (v.2.3.0, BSD-3 license) and the Python Optimal Transport package (Flamary et al., 2021)(v.0.9.3, MIT license) to compute optimal transport distances with and without regularization. Code for these experiments is available at https://github.com/IBM/stochastic-order-eval.

### Synthetic Data Experiment

In this section we analyze our method on a synthetic toy dataset that enables us to parametrically control the level of the multivariate stochastic dominance between two random variables. Given a dimension \(d\), a parameter \(p\), and mean and variance parameters \(\), \(^{2}\), our synthetic dataset is generated by sampling from the multivariate random variables \(X,Y^{d}\):

* \(X_{i}(,^{2})\) for \(i=1,,d\)
* \(Y_{i}=X_{i}+(2 B_{i}(p)-1)U_{i}\) with \(B_{i}(p)=(p)\{0,1\}\) and \(U_{i}=(0,1)\).

These variables \(X\) and \(Y\) are designed in such a way that \(p\) parametrizes the dominance of \(Y\) over \(X\). In particular, \(X}{}Y\) if \(p<0.5\), and \(Y}{}X\) if \(p>0.5\).

As a baseline for our synthetic experiments, we also compute the violation ratio for the standard FSD framework using unregularized OT (EMD), i.e. \(_{,0}\), as a function of \(p\) for fixed \(d=5\), \(=0\), \(^{2}=1.0\) and \(N=100\) samples from \(X\) and \(Y\). We then investigate how well this baseline is approximated by \(_{,>0}\), the entropically regularized ratio with a logistic cost as in Example 1. Fig. 1 shows that as the entropic regularization parameter \(\) decreases towards 0 and as the gain of the logistic cost \(\) increases, \(_{,>0}\) converges towards \(_{,0}\) across all values of \(p\). In all cases, multivariate FSD violation ratio predicts linearly \(p\), indicating that it is captures well the FSD violations. This experiment indicates that, to best approximate the standard FSD violation ratio, \(\) should be taken as small as possible (c.f. Theorem 1) and \(\) should be taken as large as possible (c.f. Example 1). There is, in practice, a tradeoff that must be made when computing the regularized index, as Sinkhorn's algorithm requires the matrix \(e^{-C/}\), where \(C\) is the matrix of pairwise costs. As such, if the ratio \(/\) is too large, numerical underflow will occur and the algorithm will fail. As a rule of thumb, it is recommended to set \(\) first and increase the value of \(\) if instability occurs.

We now assess the power of our proposed test with as a function of the number of samples and the dimension. We consider the same setup as the previous experiment and set \(p=0.65\) so that \(Y}{}X\). We then estimate the type I and type II error of the relative test statistic by averaging across 100 repetitions, these results are compiled in Fig. 2.

### LLM Benchmarking

To test our method on real world scenarios, we have chosen a current topic of significant interest to the community: LLM Benchmarking. We show through our experiments that our method can provide a more holistic ranking of LLMs evaluated on different metrics as opposed to present strategies which involve mean win rate. To demonstrate our method's application to LLM Benchmarking, we have conducted assessments on two different sets of data.

**Mixinstruct** For our first evaluation we use the dataset from Jiang et al. (2023) (MIT license) that consists of responses from 12 different instruction following LLMs, with each response evaluated on 9 metrics such as BLEU, ROUGE, BERTScore, BARTScore, etc. The data has a train (100K rows) and test (5k rows) split where each row consists of an instruction, input sentence, the expected output from users, as well as the responses of a set of different LLMs with their decoding parameters and evaluation scores on different metrics. However for the test set, Jiang et al. (2023) also did a

Figure 3: Mix Instruct Results: Comparison of Multivariate FSD to Reduction to univariate FSD with aggregation across the dimensions.

Figure 2: Type I and type II error of the relative test statistic as a function of the sample size \(n\) in dimension \(d\{10,20,50\}\). Here, \(=8\) and \(=0.01d\).

pairwise evaluation of the responses from the models by asking ChatGPT which response was better. We use this test set and generate a ranking of the LLMs using Entropic Multivariate FSD (Algorithm 1), where for each LLM we construct an empirical measure on \(^{9}\) using \(n\) samples varying from 100 to 5000. We then compute the pairwise ratios for these empirical distributions using the logistic loss with \(=0.2\), the regularization parameter \(=0.1\), and utilize the relative testing procedure from Section 4.2 to rank the 12 LLMS (see Fig. 4 for the ranking obtained with \(n=5000\) in appendix B). The confidence intervals are then generated using \(1000\) bootstrap repetitions. Finally, we compare the resulting ranking with the (univariate ranking) provided by ChatGPT scoring (which serves as a human proxy) as a function of the sample size \(n\) using Kendall Tau similarity. These results are presented in Fig. 3.

We then compare different methods that reduce multivariate ordering to univariate FSD via aggregation. The first method is a portfolio aggregation with Independent Copula(IC) (Nitsure et al., 2023), where a dimension is normalized with a global univariate CDF across all models and a geometric mean is performed across all dimensions. A univariate FSD is then applied on the resulting univariate random variables. The second method, referred to as portfolio aggregation with Empirical Copula P(EC) (Ruscendorf, 1976; Ulan et al., 2021), estimates a global multivariate CDF across all models, and then assigns to each evaluation vector the value of its CDF. Similarly, a univariate FSD is applied on this one dimensional data.

**Results** We see from Fig. 3 that the multivariate FSD, is sample efficient and has the highest Kendall tau rank similarity with GPT score. We hypothesize that this thanks to its ability to capture dependencies between the metrics. The independent copula P(IC), ignores the dependencies and hence lags a little behind but is still sample efficient. Whilst the empirical copula P(EC) captures the dependencies, it suffers from the curse of dimension and is not sample efficient.

## 6 Conclusion

In this paper, we proposed entropic multivariate FSD violation ratio as a statistic for assessing multivariate first order dominance. We addressed the convergence of these ratios as the entropic regularization goes to zero and established a central limit theorem and bootstrap consistency for this statistic. These statistical properties were leveraged in a framework for multivariate FSD testing which was applied to multi-metrics benchmarking machine learning models, showing its benefits in capturing the metric dependencies. Casting testing for stochastic order as an optimal transport problem with a smooth cost and devising an entropic regularization to ensure beneficial statistical and computational properties is an interesting framework that we envision to be useful and versatile for other stochastic orders. For instance the \(\)-first order dominance of Galichon and Henry (2012) uses optimal transport maps as multivariate quantiles (Carlier et al., 2014) and defines a \(\)-stochastic dominance; our entropic violation ratio framework can be extended to that case and, upon proving central limit theorems on the OT potentials, will lead to similar central limit theorems to the one presented in this work. Similarly, for the multivariate Lorenz order (Fan et al., 2024) that is of interest when the agent making the choice is risk averse. The Lorenz order can be expressed in terms of optimal transport maps and can be extended to our statistical testing framework using the tools introduced in this paper. We leave these developments for future work.