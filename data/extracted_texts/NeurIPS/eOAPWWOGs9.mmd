# AutoPSV: Automated Process-Supervised Verifier

Jianqiao Lu\({}^{1}\), Zhiyang Dou\({}^{1}\), Hongru Wang\({}^{2}\), Zeyu Cao\({}^{3}\),

Jianbo Dai\({}^{4}\), Yingjia Wan\({}^{3}\), Yunlong Feng\({}^{5}\), Zhijiang Guo\({}^{3}\)

\({}^{1}\)The University of Hong Kong

\({}^{2}\)The Chinese University of Hong Kong

\({}^{3}\)University of Cambridge

\({}^{4}\)University of Edinburgh

\({}^{5}\)Independent

jqlu@cs.hku.hk, zg283@cam.ac.uk

Corresponding Author.

###### Abstract

In this work, we propose a novel method named **Aut**omated **P**rocess-**S**upervised **V**erifier (**A**uto**PSV**) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. AutoPSV begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process, enabling error detection even in scenarios where ground truth answers are unavailable. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. We experimentally validate that the step-level confidence changes learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps. We demonstrate that the verification model, when trained on process annotations generated by AutoPSV, exhibits improved performance in selecting correct answers from multiple LLM-generated outputs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of AutoPSV is available at https://github.com/rookie-joe/AutoPSV.

## 1 Introduction

Large language models (LLMs) have shown impressive performance on various reasoning tasks . Prior efforts primarily focus on specific prompting techniques, such as few-shot prompting with intermediate steps and augmented demonstrations . While these methods have shown promise, their effectiveness is often task-specific, and designing prompts can be labor-intensive, leading to inconsistent results . Another approach to improve reasoning in LLMs is through instruction tuning or knowledge distillation . These methods typically involve fine-tuning LLMs and require a large set of examples annotated with chain-of-thoughts (CoT; ). However, these approaches can be resource-intensive and may not always produce reliable results.

To address these challenges, verification techniques have emerged as a promising solution . Verification models are trained to evaluate and potentially correct the reasoning process generated by LLMs. This approach aims to mitigate the risk of relying solely on the top-1 result, which may not always be reliable . By reranking candidate responses, verification models can ensure higher accuracy and consistency in LLM outputs, and provide valuable feedback for improving LLMs  further.

Verification models generally fall into two training paradigms: outcome supervision and process supervision. In outcome supervision, the training annotations rely on the correctness of the final answer [21; 22], while in process supervision, the annotations are based on evaluations of each reasoning step [23; 19]. However, process supervision is demanding in terms of annotations. Typically, it relies on either expensive and highly skilled human evaluators [23; 16] or model-induced process annotations [18; 17] to estimate the future correctness of the current reasoning step using Monte Carlo tree search [24; 25]. In contrast, outcome supervision only requires annotations for the output, making it more economical in terms of annotation effort but less effective. That being said when answers involve multiple reasoning paths, all aforementioned model-induced methods require numerous samples to ensure accurate estimations.

In this paper, we introduce **Aut**omated **P**rocess-**S**upervised **V**erfier (**A**utoPSV**), a novel approach that synergistically combines the strengths of both process supervision and output supervision. Our method begins by training an outcome-supervised verification model using outcome supervision annotations. This model then assigns confidence scores to each intermediate reasoning step, estimating their likelihood of contributing to a correct final answer. A distinguishing feature of AutoPSV is its ability to automatically generate process annotations through relative step-level confidence change analysis, significantly reducing annotation effort while maintaining supervision quality without requiring ground truth answers. These automatically generated process annotations subsequently serve as training data for developing an enhanced verification model that leverages both process-level and outcome-level supervision signals. The complete framework of AutoPSV is illustrated in Figure 1. We conduct extensive experiments across five datasets, including mathematical reasoning benchmarks and commonsense reasoning tasks. The results demonstrate that our method effectively improves the reasoning capability of the model with our highly efficient labeling scheme for process supervision. Our contribution is summarized as follows:

* We introduce AutoPSV to automate the labeling of process data to enhance LLMs' reasoning capabilities. By combining the strengths of output and process supervision, AutoPSV effectively identifies variations in model confidence to annotate the correctness of intermediate reasoning steps, enabling efficient automatic labeling for process supervision.
* Comprehensive experiments demonstrate that AutoPSV significantly improves the performance and scalability of verification models in mathematical and commonsense reasoning tasks. This approach greatly reduces the need for manual intervention and extensive computational resources, making it a valuable tool for enhancing LLM capabilities.
* AutoPSV's versatility is evident in its applicability to both labeled and unlabeled dataset settings after completing the training process. This flexibility and generalizability highlight the method's potential for widespread adoption in various LLM applications.

## 2 Related Works

Improving Reasoning Abilities of LLMsTo enhance the reasoning capabilities of LLMs, prior research primarily focuses on specific prompting techniques . Existing efforts include few-shot prompting with intermediate steps augmented demonstrations [5; 6; 7; 27] or zero-shot prompting with specific instructions [28; 29]. Although these methods have shown promising results, their effectiveness is often constrained by their task-specific nature and the labour-intensive process of designing prompts, leading to inconsistent outcomes across different tasks [9; 10]. Another strategy to facilitate reasoning involves instruction tuning or knowledge distillation, which elicits reasoning paths from LLMs without explicit prompting [11; 12; 13; 30]. These approaches typically involve resource-intensive fine-tuning over LLMs and require a large set of examples annotated with chain-of-thoughts (CoT). Unlike methods that directly modify parameters or prompts, AutoPSV focuses on training an additional verification model to select the desired output from the original model's output. This approach is further discussed in the context of process supervision in the following paragraph.

From Outcome to Process SupervisionRecent efforts have focused on enhancing the reasoning capabilities of LLMs through the use of verifiers to select the best answer from multiple candidates. There are two main types of verifiers: the Outcome-Supervised Verifier (OSV) and the Process-Supervised Verifier (PSV). The OSV is supervised with a signal based on the final answer [21; 22], while the PSV is with detailed feedback which requires evaluating individual reasoning steps [15;19, 16, 23]. Despite the time-consuming annotation cost, the PSV offers several advantages that make it preferable to the OSV. It can provide fine-grained feedback by pinpointing the location of errors, which is valuable for reinforcement learning and automatic correction . To alleviate the extensive human annotation, recent approaches  propose a machine annotation framework using Monte Carlo Tree Search . This process demands a lot of computing resources, potentially imposing a limitation on the usage. AutoPSV is more efficient, as it utilizes an outcome-supervised verification model to assign confidence to each reasoning step and calculate relative step-level confidence changes, eliminating the need for additional sampling or manual labeling.

## 3 AutoPSV

In this section, we introduce the main problem that this paper focuses on in Section 3.1. We then discuss the motivation behind why we believe it is necessary to train a verification model in Section 3.2. Finally, we describe how we accomplish the transition from outcome supervision to process supervision during the training of the verification model in Section 3.3.

### Problem Setting

ObjectiveOur research addresses the challenge of response selection from multiple candidates generated by a Large Language Model (LLM). Specifically, given an LLM acting as a _response generator_, we seek to develop an effective method for identifying the correct response among multiple generated solutions. Our primary goal is to maximize the probability of selecting an accurate solution from the available candidates.

NotationWe define \(q\) as the input question presented to the model, and \(S_{i}^{(1:t)}\) as the sequence of intermediate reasoning steps up to step \(t\) for the \(i\)-th solution to question \(q\). The binary correctness label for the \(i\)-th solution is denoted as \(y_{i}\), and \(\) represents our training question dataset. This formalization enables us to systematically approach the response selection problem while maintaining mathematical rigor in our methodology.

### Motivation

Response selection methods can be broadly divided into two categories: models specifically fine-tuned for the selection task and those that employ various prompting strategies.

In our exploration, we initially investigate whether existing open-source LLMs could serve as effective selection agents to evaluate model outputs and choose the correct response without fine-tuning. We

Figure 1: An overview of AutoPSV. It utilizes an outcome-supervised verifier to automatically generate process annotations for each reasoning step by detecting its own confidence variations, _without_ relying on ground truth annotations. AutoPSV efficiently produces annotations serving as process supervision during the LLM training, which sidesteps costly annotations.

choose Miktral-Instruct  as the _response generator_, and its response results are listed Table 1. Our objective focuses on identifying the correct response from **five** candidate solutions.

To establish robust conclusions, we evaluate _selector_ models ranging from 7B to over 70B parameters, applying various prompting strategies. The results, tested on the GSM8K test set , are presented in Table 2. Notably, even models exceeding 70 billion parameters demonstrate suboptimal selection performance when relying solely on prompting without fine-tuning.

Based on these findings, our research focuses on training dedicated verification models and enhancing their response selection capabilities.

### Training Methodology

AutoPSV integrates outcome supervision with process supervision to create an effective training methodology. Below, we detail each component of our approach.

Outcome-SupervisionWe begin by training an outcome-supervised verification (OSV) model, denoted as \(f_{}()\), where \(\) represents the optimized parameters. The training utilizes Mean Squared Error (MSE) loss for each solution step:

\[(S_{i}^{(1:t)},y_{i};q)=(f_{}(S_{i}^{(1:t)};q)-y_{ i})^{2}\] (1)

The complete objective function across all training questions \(\) is:

\[_{}()=|}_{q }_{i=1}^{n}_{t=1}^{m_{i}}(f_{}( S_{i}^{(1:t)};q)-y_{i})^{2}\] (2)

Where \(n\) represents solutions per question and \(m_{i}\) denotes steps in the \(i\)-th solution. The OSV output approximates the expected correctness probability, as formalized as follows:

**Theorem 1**: _For a model trained with outcome supervision, \(f_{}\), characterized by optimally tuned parameters \(\), the assigned score for the sequence \(S^{(1:t)}\) is an estimation of the likelihood of ultimately deriving a correct answer, denoted by \(\), based on the progression observed in \(S^{(1:t)}\) and the pertinent question \(q\). This is mathematically represented as:_

\[f_{}(S^{(1:t)};q) p(|S^{(1:t)},q)\]

The proof follows from optimizing the MSE loss in equation (2), with details in .

Process-SupervisionWe compute the relative confidence change between steps:

\[_{conf}^{t}=}(S^{(1:t+1)};q)-f_{}(S^{(1:t )};q)}{f_{}(S^{(1:t)};q)}\] (3)

  
**Response Generator** & **Model Size** (Parameters) & **Pass@1 (\%)** & **Pass@5 (\%)** & **Self-Consistency (\%)** \\  Miktral-Instruct  & 8 x 7B (MOE) & 62.55 & 82.31 & 69.06 \\   

Table 1: Performance of Miktral-Instruct on GSM8K. All results are reported in accuracy (%).

    &  &  \\   & & Pairwise & Classification & Classification + CoT & Scoring & Scoring + CoT \\   Mistral-Instruct  \\ Mistral-Instruct  \\ Llama2-chat  \\ Qwen  \\  } & 7B & 60.73 & 61.18 & 64.82 & 61.49 & 69.75 \\  & 8\(\)7B & 58.83 & 59.14 & 67.40 & 61.79 & 65.58 \\  & 70B & 59.28 & 62.70 & 66.79 & 59.74 & 62.93 \\  & 72B & 59.14 & 66.64 & 69.52 & 61.86 & 65.88 \\   

Table 2: Comparison of different selection methods across various model sizes for selecting a response from candidate responses generated by Miktral-Instruct. All results are reported in accuracy (%).

\(_{conf}^{t}\) represents the relative variation in the model's confidence score from step \(t\) to step \(t+1\). A negative value of \(_{conf}^{t}\) signifies a reduced confidence in achieving a correct answer after incorporating information from the \((t+1)\)-th step. We denote the process label for the \(t\)-th step as \(y_{i}^{t}\) and \(\) as the variation threshold.

For process labeling, we employ the "first error location" strategy  with threshold \(\):

* If \(_{conf}^{t}>\): \(y_{i}^{t}=1\)
* Otherwise: \(y_{i}^{t}=0\) and \( t^{}>t\), \(y_{i}^{t^{}}=0\)

The process-supervision loss function is:

\[_{proc}(S_{i}^{(1:t)},y_{i}^{t};q)=(f_{}(S_{i}^{(1 :t)};q)-y_{i}^{t})^{2}\] (4)

## 4 Preliminary Findings

In this section, we present our findings aiming to validate two key aspects: In Section 4.1, we present a comprehensive analysis of the OSV model, i.e., to validate that the initially trained OSV model is effective and robust. In Section 4.2, we further introduce a self-designed benchmark for process errors and calculate \(_{conf}^{t}\) to detect these errors, i.e., to demonstrate the effectiveness and reliability of relative step-level confidence change in the proposed method. The validation of these two components serves as a foundation for automatic process labeling via AutoPSV, as described in Section 3.3.

### Experiment on Outcome-Supervised Verifier Performance

In this section, we validate the effectiveness and scalability of the OSV model. Initially, we fine-tune a pretrained language model using ground truth data from the GSM8K dataset. Then, we use this fine-tuned model to generate multiple response samples for the GSM8K training prompts. We label these samples based on the correctness of their final answers. After this, we train an OSV model using the method described in Eq. (1).

To evaluate the OSV, we measure its ability to select a sample with the correct final answer from samples generated by various LLMs, denoted as the _Response Generator_. Specifically, our task involves selecting the correct candidate from **five** responses. We assess the effectiveness of outcome supervision on two models: Phi2 (**OSV (Phi)**)  and Mistral-7B (**OSV (Mistral)**) . To explore the scalability of this outcome-supervised verifier effect, we choose Response Generators of varying scales, ranging from 7B to 72B parameters, i.e., Mistral-7B-Instruct (**Mistral-Instruct**) , Mistral 8 \(\) 7B (**Mistral-Instruct**)  and Qwen-72B-Chat (**Qwen**) . This allows us to check the OSV's generalized ranking capability across different LLM scales.

The results demonstrate the effectiveness and scalability of the OSV model in selecting the correct response among multiple responses generated by different generators. Specifically, the OSV models, trained using either Mistral or Phi, consistently outperform the self-consistency (**SC**) baseline across all generator configurations. The results validate the effectiveness of the OSV model in enhancing model selection strategies, particularly when applied to larger and more accurate LLM generators.

We further analyze the performance discrepancy between the two OSV models:

Performance Analysis of Different OSVsThe performance disparity among the verifiers can be attributed primarily to variations in model sizes and the quality of their training data. It's important to note that the OSV model is **continuously** trained from the GSM8K fine-tuned model with the

  
**Response Generator** & **Pass@1** & **Pass@5** & **SC** & **OSV (Mistral)** & **OSV (Phi)** \\  Mistral-Instruct & 42.08 & 69.90 & 50.03 & 60.72 & 52.61 \\ Mistral-Instruct & 62.55 & 82.31 & 69.06 & 74.07 & 69.37 \\ Qwen & 77.03 & 91.13 & 81.27 & 85.00 & 84.19 \\   

Table 3: Performance of OSV models across different configurations.

addition of a value head. This means that the training data for each OSV model is generated from its corresponding fine-tuned base model. Specifically, the training data for OSV (**Mistral**) is generated from the fine-tuned **Mistral** model, while the training data for OSV (**Phi**) is generated from the fine-tuned **Phi** model.

Table 4 presents a consolidated view of the model sizes along with the precision metrics of their outcome supervision training data.

It is worth noting that while both models are trained on the same quantity of data per question (100 samples), the quality of this data differs due to the capabilities of their respective base models. The Mistral model, being larger and potentially more capable, generates higher quality training data for its OSV, which in turn leads to better performance. In our content, we select the OSV (**Mistral**) model as the OSV model among other experiment settings due to its superior performance, as demonstrated Table 3.

### Detecting Calculation Error During Math Reasoning

In this section, we verify the effectiveness and reliability of our method AutoPSV. Specifically, We calculate \(^{t}_{conf}\) to identify inaccuracies in the process, as outlined in Eq. (3).

In Section 4.2.1, we introduce the concept of math calculation error and establish a preliminary benchmark. In Section 4.2.2, we assess the performance of calculating \(^{t}_{conf}\) to detect calculation errors against our established benchmark.

#### 4.2.1 Math Calculation Error

We outline a method for identifying instances of math calculation error, which we define as occurrences where the numerical values on either side of an equals sign within a mathematical expression do not align. This misalignment indicates a breakdown in logical reasoning, categorizing the instance as a calculation error in the context of mathematical problem-solving. This process establishes a benchmark for math calculation error detection with more details in Appendix E.

Math Calculation Error DetectionTo identify calculation errors in mathematical reasoning, we monitor the relative step-level confidence changes between the intermediate steps as defined in Eq. (3). if \(^{t}_{conf}\), we view the step as "incorrect". We also provide a detection example in Figure 2 for better understanding.

#### 4.2.2 Quantitative Results

We introduce three metrics for a thorough evaluation of math calculation error detection: Precision (**Prec.**), which calculates the proportion of samples with correct final answers but exhibiting hallucinatory errors during the reasoning process; **Recall**, which determines the proportion of samples with math calculation errors that the OSV model successfully identifies through step-level confidence changes; **F1-score**, which gauges the verifier's overall efficacy. In Table 5, we explore how different threshold (\(\)) values affect the precision, recall, and F1-score for math calculation error detection.

The results in Table 5 demonstrate that our method using step-level confidence change effectively detects calculation errors across threshold values from - 0.5 to - 0.9. As the threshold becomes more negative (stricter for labeling errors), the precision increases, indicating higher precision in identifying true errors. However, the recall decreases, meaning fewer actual errors are caught. Importantly, the F1-score, balancing precision and recall, remains relatively stable across thresholds. This demonstrates that our method strikes a good balance between detecting real errors and minimizing incorrect

    &  &  \\   & & Quality (acc.\%) & Quantity (per question) \\  OSV (Mistral) & 7B & 0.9914 & 100 \\ OSV (Phi) & 2.7B & 0.9605 & 100 \\   

Table 4: Model sizes and training data accuracy for training OSVs.

flagging of valid calculations. Overall, our detection method is effective and robust, performing well over a range of thresholds without significantly compromising overall detection quality.

We note that setting \(\) = - 0.5 in our detection methods helps maintain a balance between precision and recall, which can ensure a balanced distribution of labeled "incorrect" and "correct" responses.

Validation and Foundation for AutoPSVOur empirical validation of the OSV model encompasses two key aspects: its efficacy in response selection (Section 4.1) and its capability in detecting calculation errors (Section 4.2). These experimental results provide a robust foundation for automating process annotations using AutoPSV. Furthermore, Theorem 1 establishes the theoretical framework for utilizing OSV to estimate the probability of reaching correct final answers from any given intermediate reasoning step. This convergence of theoretical guarantees and empirical evidence provides the methodological groundwork for applying AutoPSV to generate process-supervised training data in our subsequent experiments.

## 5 Experiment

In this section, we first introduce the experimental setup in a subsection, which includes the response generator LLMs and evaluation settings in Section 5.1. We then present the main result of our process supervision-enhanced verification model on both mathematical and commonsense reasoning benchmarks, as described in Section 5.2.

### Experimental Setup

Models:We selected three instruction-tuned LLMs of varying sizes, ranging from 7 billion to over 70 billion parameters, to serve as the _response generator_. Specifically, we used Mistral-Instruct-7B (**Mistral-Instruct**), Mistral-8x7B-Instruct-v0.1 (**Mistral-Instruct**), and Qwen-72B-Chat (**Qwen**).

Datasets:Our evaluation encompasses five benchmarks across two domains: For mathematical reasoning, we include GSM8K , containing math word problems requiring multi-step reasoning, and MATH , composed of high school-level competition problems covering a range of math subjects. For commonsense reasoning, we use HellaSwag , a dataset for physically situated commonsense reasoning, Winogrande , fill-in-the-blank problems requiring commonsense pronoun resolution and ANLI , a dataset for natural language understanding and reasoning.

Evaluation:For evaluation, we follow the methodology outlined in  to ensure consistency across benchmarks. Our protocol involves:

(i) Our object is to select the correct answer from five candidate responses.

(ii) For OSV models, we use the final value assigned to the whole solution for evaluation, while for PSV models, we utilize the product of step-level scores as the aggregation function.

(iii) To obtain more reliable pass@k results, we implement the estimation method described in . This involves generating n samples per task (where n > k) and assessing the number of correct samples that pass unit tests. We then calculate the unbiased estimator for pass@k. For self-consistency (Self-Cons.) and verifier results, we randomly select k out of n samples and perform separate calculations. All results are reported with an accuracy of \(\)0.1 at a 95% confidence level.

Additional details regarding generation hyperparameters and different aggregation functions are provided in Appendix F.2.

    &  \\   & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\ 
**Prec.** & 0.85 & 0.88 & 0.91 & 0.93 & 0.94 \\
**Recall** & 0.90 & 0.89 & 0.86 & 0.83 & 0.80 \\
**F1-Score** & 0.88 & 0.89 & 0.88 & 0.88 & 0.86 \\   

Table 5: Process Calculation Error Detection Performance with Varying Threshold (\(\)) Values.

### Enhanced LLMs Reasoning via Process Supervision

To evaluate the efficacy and scalability of our proposed approach (detailed in Section 5.2), we conducted comprehensive experiments across mathematics and commonsense reasoning tasks using five diverse datasets.

Our experimental framework employs an autonomous process-supervision data annotation method where we calculate \(_{conf}^{t}\) based on model confidence from OSV, using a threshold of \(\) = -0.5. This annotated data is then utilized to continuously fine-tune the OSV model, resulting in our enhanced OSV + PSV model.

We evaluate three distinct approaches: (i) Self-Consistency (**Self-Cons.**): Our baseline approach (ii) Outcome-supervised verifier (**OSV**): Our initial verification model (iii) Process-supervised enhanced verifier (**OSV + PSV**): Our proposed enhancement. For each approach, we assess Pass@5 performance, which represents the upper limit of achievable performance on these benchmarks.

Mathematics Reasoning:As shown in Table 6, the process-supervised enhanced verifier demonstrates superior performance over the outcome-supervised verifier and Self-Consistency models for all evaluated response generators on GSM8K. For the MATH benchmark, the process-supervised enhanced verifier outperforms the other two approaches for Mistral-Instruct and Mistral-Instruct, but it is slightly less effective than the Self-Consistency model when applied to Qwen-72b.

Commonsense Reasoning:According to Table 7, OSV + PSV again leads to the best results among the three methods for each response generator tested on HellaSwag. For Winogrande, Mistral-Instruct paired with OSV + PSV achieves the highest performance, whereas, for Mistral-Instruct and Qwen-72b, the original OSV without process supervision has a marginal advantage. When looking at the results of the ANLI benchmark, OSV + PSV is the highest-performing method for Mistral-Instruct and Mistral-Instruct. Despite this, for Qwen-72b, the OSV model alone falls slightly behind the integrated OSV + PSV.

Conclusion:Our experimental results indicate that the process-supervised enhanced verifier (**OSV + PSV**) consistently outperforms or matches the baseline models across most mathematical and commonsense reasoning tasks. By leveraging automatic process annotations, our approach enhances the model's capacity to verify reasoning processes, resulting in improved accuracy and robustness across a wide range of benchmarks and response generators.

## 6 Analysis

In Section 6.1, we compare our process annotation method, AutoPSV with two other model-induced annotation methods to showcase the effectiveness and efficiency of our proposed approach. In Section 6.2, we validate the data quality constructed via AutoPSV as described in Section 5.2.

    &  &  \\  & Pass@5 & Self-Cons. & OSV & OSV + PSV & Pass@5 & Self-Cons. & OSV & OSV + PSV \\  Mistral-Instruct & 69.90 & 50.03 & 61.18 & **61.41** & 7.7 & 1.64 & 5.10 & **5.30** \\ Mistral-Instruct & 82.30 & 69.06 & 74.91 & **76.04** & 22.80 & 10.66 & 15.2 & **16.92** \\ Qwen & 91.13 & 81.27 & 84.91 & **85.15** & 56.10 & **40.10** & 38.94 & 39.36 \\   

Table 6: Results on mathematics benchmarks.

    &  &  &  \\  & Pass@5 & Self-Cons. & OSV & OSV + PSV & Pass@5 & Self-Cons. & OSV & OSV + PSV & Pass@5 & Self-Cons. & OSV & OSV + PSV \\  Mistral-Instruct & 76.84 & 40.50 & 73.81 & **74.65** & 91.16 & 58.64 & 79.16 & **79.98** & 73.4 & 45.6 & 59.8 & **59.3** \\ Mistral-Instruct & 84.05 & 73.67 & 82.83 & **83.62** & 79.16 & 68.75 & 73.40 & **73.88** & 68.4 & 59.0 & 62.9 & **64.0** \\ Qwen-72b & 95.28 & 85.44 & 93.08 & **93.59** & 88.63 & 72.21 & **80.34** & 79.32 & 82.4 & 63.8 & 69.1 & **71.4** \\   

Table 7: Results on commonsense reasoning benchmarks.

### Advantages of AutoPSV in Labeled and Unlabeled Settings

Aside from the labeling method defined by Eq. (3) in AutoPSV, another labeling strategy is the Monte Carlo Tree sampling estimation (MCTS), as described in [17; 18]. To better demonstrate the effect of our method, we make a comparison with this approach and conduct experiments on mathematical benchmarks across both labeled and unlabeled settings (i.e., whether the ground-truth answers to the questions are provided).

Performance in Labeled SettingsWe first evaluate the performance of AutoPSV in labeled settings. We follow the experimental settings described in [17; 18] to ensure a fair comparison. More implementation details are provided in Appendix F.3. Table 8 presents a comparison of process labeling methods across different response generators on the GSM8K and MATH datasets.

The experimental results shown in Table 8 suggest that our proposed method for process labeling, which relies on detecting changes in model confidence, performs competitively with the MCTS method from [18; 17]. In some cases, our method even outperforms the MCTS method, especially on the more challenging MATH benchmark.

A key advantage of AutoPSV is its computational efficiency. As shown in Table 9, our method requires significantly fewer tokens for process labeling compared to MCTS-based methods.

This efficiency stems from AutoPSV's ability to generate process annotations without requiring multiple samples for each reasoning step, making it particularly suitable for large-scale applications or scenarios with limited computational resources.

Performance in Unlabeled SettingsTo further demonstrate the flexibility of AutoPSV, we evaluate its performance in unlabeled settings. We generated an additional dataset of 7,000 unlabeled math problems using the Evol-Instruct method from WizardLM . These problems, created by LLMs without accompanying gold solutions, represent a challenging scenario for traditional supervision methods. We then conducted an experiment incorporating these unlabeled questions alongside the GSM8K dataset. Table 10 compares various methods across different response generators. In this context, OSV+PSV (GSM8K) refers to the original AutoPSV setting, while OSV+PSV (GSM8K+WizardLM) includes process annotations sourced from both GSM8K and WizardLM unlabeled questions. Notably, both MCTS and OSV-only training cannot leverage these unlabeled data, highlighting another key advantage of AutoPSV.

   Response Generator & Pass@5 & Self-Cons. & OSV (GSM8K) & MCTS (GSM8K) & OSV+PSV (GSM8K) & OSV+PSV (GSM8K+WizardLM) \\   Mistral-Instruct & 69.90 & 50.03 & 61.18 & 60.82 & 61.41 & 63.11 \\ Mistral-Instruct & 82.30 & 69.06 & 74.91 & 75.10 & 76.04 & 78.15 \\ Quen & 91.13 & 81.27 & 84.91 & 84.85 & 85.15 & 86.77 \\   

Table 10: Performance enhancement of our proposed AutoPSV method in unlabeled settings, where both MCTS and OSV-only training are unable to utilize unlabeled data.

    &  &  &  &  \\  & & & Process (MCTS) & Process (MCTS) & Process (MCTS) & Process (MCTS) & Process (AvitOPSV) \\  Mistral-Instruct & 69.90 & 50.03 & 54.13 & 55.32 & 7.7 & 1.64 & 3.1 & 3.24 \\ Mistral-Instruct & 82.30 & 69.06 & 72.36 & 72.12 & 22.80 & 10.66 & 12.18 & 12.54 \\ Qwen.72b & 91.13 & 81.27 & 82.17 & 82.83 & 56.10 & 40.10 & 36.88 & 37.10 \\   

Table 8: Comparison of process labeling methodsâ€™ performance across different response generators on GSM8K and MATH datasets. The table evaluates the Pass@5, Self-Consistency (Self-Cons.), and response selection performance of models fine-tuned using process annotations labeled by MCTS and AutoPSV.

    &  &  &  \\  & & & & & & & & \\  & & & & & & & & \\  GSM8K & 7,473 & 4.47 & 334,358 & 126 & 9,379,258 & 2,808 & 127 \\ MATH & 7,498 & 16.00 & 1,200,177 & 272 & 1,621,515,984 & 21,626 & 273 \\   

Table 9: Comparison of annotation costs between MCTS and AutoPSV for process labeling on the GSM8K and MATH datasets. Annotation cost represents the processed tokens into a model when generating process annotations, encompassing both input and output tokens.

The results demonstrate that the addition of unlabeled data leads to noticeable improvements across all response generators. For instance, the performance of Mistral-Instruct improves from 61.18 (OSV) to 63.11 (OSV+PSV with GSM8K+WizardLM). These results further underscore the value of the AutoPSV approach, particularly its ability to effectively utilize unlabeled data for enhanced performance.

In summary, AutoPSV offers several key advantages over MCTS-based methods:

**Consistent Performance with Computational Efficiency:** AutoPSV demonstrates robust, consistent improvements across different response generators and datasets. Its ability to efficiently generate process annotations--without the need for extensive sampling like MCTS--makes it particularly well-suited for large-scale applications and environments with limited computational resources.

**Leveraging Unlabeled Data for Enhanced Performance:** Unlike MCTS, which relies on ground truth labels, AutoPSV can effectively utilize unlabeled data. This capability not only enhances model performance in real-world settings but also offers scalability in scenarios where labeled data is scarce. The flexibility to harness unlabeled data ensures that AutoPSV can drive significant improvements even in data-constrained situations.

### Outcome-Supervised Verification vs. Process-Supervised Verification

We apply the OSV to relabel the process-supervised training data as in Section 5.2. We then retrain a new model using this relabeled data. This experiment highlights the performance gap between outcome-supervised and process-supervised training.

The experimental results in Table 11 reveal that retraining the model with process supervision from AutoPSV still yields better performance than self-consistency across three different response generators. We also noticed a small performance gap between the PSV and OSV. It is worth noting that our PSV was trained using data from the OSV. The small performance gap between the PSV and OSV models demonstrates that the relabeled process-supervised training method successfully inherits information from the outcome-supervised model without requiring ground truth annotations. This ablation study further provides quality assurance for automatic process labeling via AutoPSV. Moreover, OSV training is limited to labeled datasets, while AutoPSV demonstrates superior performance by utilizing both labeled and unlabeled data as shown in Table 10. This comparison further highlights the versatility and effectiveness of AutoPSV in real-world scenarios where ground truth annotations may be scarce or unavailable.

## 7 Conclusion

In conclusion, we propose a novel method for automatic process labeling in LLMs by detecting relative changes in model confidence. Our experimental results demonstrate that AutoPSV significantly enhances the precision and scalability of the verifier models in various reasoning tasks, ranging from mathematical to commonsense reasoning. AutoPSV therefore has the potential to considerably enhance existing LLMs' performance while drastically reducing the need for intensive computation and manual intervention. For future work, we aim to utilize the automatically constructed PSV to supervise the generator using step-wise proximal policy optimization, to enhance the accuracy of the generator's output during greedy decoding without the need for subsequent reranking. This avenue of research could lead to even more advancements in the capabilities of LLMs and their application in reasoning tasks. The limitations and broader impact of the paper are discussed in Appendix A and B.

  
**Response Generator** & **Pass@1** & **Pass@5** & **SC** & **OSV** & **PSV** \\  Mistral-Instruct & 42.08 & 69.90 & 50.03 & 60.72 & 59.14 \\ Mistral-Instruct & 62.55 & 82.31 & 69.06 & 74.07 & 71.39 \\ Qwen-72b & 77.03 & 91.13 & 81.27 & 85.00 & 83.70 \\   

Table 11: Experimental results showing the performance of OSV models across different configurations tested on GSM8K test sets.