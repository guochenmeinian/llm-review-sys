# Dynamic Neural Regeneration: Enhancing Deep Learning Generalization on Small Datasets

Vijaya Raghavan T Ramkumar1, Elahe Arani1,2, & Bahram Zonooz1,*

Eindhoven University of Technology 2Wayve

raghavijay95@gmail.com, e.arani@tue.nl, b.zonooz@tue.nl

Equal contribution.Code is available at https://github.com/NeurAI-Lab/Dynamic-Neural-Regeneration

###### Abstract

The efficacy of deep learning techniques is contingent upon access to large volumes of data (labeled or unlabeled). However, in practical domains such as medical applications, data availability is often limited. This presents a significant challenge: How can we effectively train deep neural networks on relatively small datasets while improving generalization? Recent works have explored evolutionary or iterative training paradigms, which reinitialize a subset of parameters to enhance generalization performance for small datasets. However, these methods typically rely on randomly selected parameter subsets and maintain fixed masks throughout training, potentially leading to suboptimal outcomes. Inspired by neurogenesis in the brain, we propose a novel iterative training framework, Dynamic Neural Regeneration (DNR), that employs a data-aware dynamic masking scheme to eliminate redundant connections by estimating their significance. This approach increases the model's capacity for further learning through random weight reinitialization. Experimental results demonstrate that our approach outperforms existing methods in accuracy and robustness, highlighting its potential for real-world applications where data collection is challenging. 2

## 1 Introduction

Deep neural networks (DNNs) have become essential for solving complex problems in various fields, such as image and speech recognition, natural language processing, and robotics [(1)]. With the increasing availability of data, DNNs have achieved unprecedented performance, surpassing human-level performance in some applications [(2)]. However, the success of DNNs is limited when dealing with small datasets, where the model tends to overfit and fails to generalize to new data. For example, it is often difficult to obtain a large amount of data in medical diagnosis due to the complexity and high cost of the procedures involved. In such cases, lack of generalization can be dangerous, which can lead to incorrect diagnosis and treatment.

Recently, several studies based on weight reinitialization methods [(3; 4)] have been proposed in the literature to improve generalization by iteratively refining the learned solution through partial weight reinitialization. These methods select and retain a subset of parameters while randomly reinitializing the rest of the network during iterative/evolutionary training schemes. For example, a state-of-the-art method named Knowledge Evolution (KE) [(5)] improves generalization by randomly splitting the network into fit and reset subnetworks and constantly reinitializing the reset subnetwork after each iteration. However, the KE approach is limited by its reliance on a predetermined mask creation, where a random subset of parameters is selected and kept constant throughout the iterative training process. This constraint may impede the model's ability to learn effectively from small datasets, ultimately limiting its generalization capabilities. These limitations raise two important questions:1) Can we leverage an evolutionary training paradigm to evolve or adapt the mask over generations, instead of using a fixed mask, in order to enhance the generalization performance of deep learning models trained on small datasets? 2) Can we utilize the available data and the internal state of the model to dynamically determine the important parameters for each generation, rather than randomly presetting them?

In our quest to address these questions, we draw inspiration from the phenomenon of neurogenesis in the brain. Neurogenesis, the process of dynamically generating or eliminating neurons in response to environmental demands, has been found to play a crucial role in learning and memory consolidation [(6; 7; 8)]. This intricate process enables the brain to adapt to new experiences and stimuli, enhancing generalizability. Recent advances in neuroscience have shed light on the non-random integration of new neurons within the brain [(9)]. For instance, in rodents, neurogenesis-dependent refinement of synaptic connections has been observed in the hippocampus, where the integration of new neurons leads to the elimination of less active synaptic connections [(10; 11)]. Selective neurogenesis is critical to improving generalization ability by providing a diverse pool of neurons with distinct properties that can integrate into existing neural networks and contribute to adaptive learning [(12)]. Although the precise mechanisms that govern selective neurogenesis are not fully understood, these findings suggest that selective neurogenesis in the human brain enhances generalization capabilities through its dynamic and selective nature. Thus, by emulating the characteristics of selective neurogenesis, we unlock its potential to improve generalization in deep neural networks.

Therefore, we present a novel iterative training approach called Dynamic Neural Regeneration (DNR), which distinguishes itself from the conventional Knowledge Evolution (KE) method through its mask computation. Unlike a predetermined fixed mask, DNR utilizes a data-aware dynamic masking criterion that evolves and adapts the mask over generations. Through extensive experiments on multiple datasets, we demonstrate that our proposed training paradigm greatly improves the performance and generalization of the models. Furthermore, DNR effectively addresses overfitting on relatively small datasets, alleviating the need for extensive data collection. The main contributions of the paper are as follows.

* Dynamic Neural Regeneration (DNR) is an evolutionary training paradigm that incorporates data-aware dynamic masking to selectively transfer knowledge across generations.
* Our proposed training paradigm facilitates the learning of generalizable features and increases the overall performance of DNNs across small datasets.
* DNR exhibits robustness in solving more common challenges in real-world problems, including class imbalance, natural corruption, and adversarial attacks.

## 2 Related work

Iterative training and weight reinitialization for DNNs is a prominent area of research [(5; 4; 13; 14)] that focuses mainly on improving generalization performance by partially refining the learned solution or fully iterating the learned solution. Dense-Sparse-Dense (DSD) [(3)] propose a three-phase approach where weights with small magnitudes are pruned after initial training to induce sparsity and retrain the network by reinitializing the pruned weights to zero. Zaidi et al. [(15)] conducted an extensive investigation into the conditions under which reinitialization proves beneficial. BANs (Born Again Neural Networks) [(4)] is a knowledge-distillation-based method that follows a similar iterative training paradigm. However, the critical difference between our work and BANs is that it employs the class-logits distribution instead of the network weights to transfer knowledge between successive networks. Recently, Zhou et al. [(16)] (LLF) propose the forget and relearn hypothesis, which aims to harmonize various existing iterative algorithms by framing them through the lens of forgetting. This approach operates on the premise that initial layers capture generalized features, while subsequent layers tend to memorize specific details. Accordingly, they advocate for the repeated reinitialization and retraining of later layers, effectively erasing information related to challenging instances. Similarly, the LW [(17)] approach progressively reinitializes all layers. However, these weight reinitialization methods rely on architecture-specific assumptions that do not take the data into account. They are based on presumed properties inherent to the model and its learning process. Consequently, these methods lack prior knowledge of which features, layers, or parameters should be reinitialized in a general context. Moreover, research indicates that memorization in neural networks is not confined to final layers but involves neurons distributed throughout the model [(18)]. Knowledge Evolution (KE) [(5)] splits model weights into fit and reset parts randomly and iteratively reinitializes the reset part during training. The splitting method can be arbitrary (weight-level splitting (WELS)) or structured (Kernel-level convolutional-aware splitting (KELS)). This approach involves perturbing the reset hypothesis to evolve the knowledge within the fit hypothesis over multiple generations. Our framework (DNR) distinguishes itself from the conventional Knowledge Evolution (KE) method through its mask computation. DNR utilizes data-aware dynamic masking that adapts the mask over generations and transfers selective knowledge.

Moreover, we differentiate our study from current literature on neural architecture search (NAS) [(19)] and growing neural networks [(20)]. Our emphasis is on a consistent network architecture, maintaining fixed connections and parameter count throughout our analysis. Notably, our work sets itself apart from the dynamic sparse training literature [(21; 22)], as our objective is not to achieve sparsity but rather to enhance generalization on small datasets.

## 3 Methodology

### Evolutionary training paradigm

We first introduce the evolutionary/iterative training paradigm as envisioned in KE [(5)]. Evolutionary training paradigms allow neural networks to be trained for many generations, where each generation focuses on optimizing the model to converge towards a local minimum while progressively improving generalization. Each generation within the training process is denoted as \(g\), where \(g\) ranges from 1 to the total number of generations, \(N\).

We define a deep neural network \(f\) with \(L\) layers and is characterized by the set of parameters \(\). We assume a dataset \(D\) consisting of \(n\) input-output pairs, denoted \(\{(x_{i},y_{i})\}_{i=1}^{n}\). For a classification task, we define the cross entropy loss for training the network as:

\[_{ce}=-_{i=1}^{n}[y_{i}((f(x_{i} ;)))+(1-y_{i})(1-(f(x_{i};)))]\] (1)

where \(_{i}=f(x_{i})\) is the network's predicted output for input \(x_{i}\). We initialize the weights and biases of the network randomly.

KE starts by introducing a binary mask \(M\), which partitions the weights of the neural network into two hypotheses before starting training: the fit hypothesis \(H_{}\) and the reset hypothesis \(H_{}\). This partitioning is expressed as follows:

\[H_{}\,= H_{}\,= (-)\] (2)

Here, the element-wise multiplication operator \(\) is applied to the mask \(M\) and the parameter set \(\) to obtain the fit hypothesis \(H_{}\). Similarly, the reset hypothesis \(H_{}\) is obtained by element-wise

Figure 1: Schematics of proposed _Dynamic Neural Regeneration (DNR)_ framework. Our framework utilizes a data-aware dynamic masking scheme to remove redundant connections and increase the networkâ€™s capacity for further learning by incorporating random weight reinitialization. Thus, effectively improving the performance and generalization of deep neural networks on small datasets.

multiplying the complement of the mask \((1-M)\) with the parameter set \(\). These parameters are chosen at random before the start of the first generation. This binary mask M is kept constant throughout the evolutionary training; i.e., the parameters belonging to the fit and reset hypotheses remain in that category across all generations.

We use the stochastic gradient descent (SGD) algorithm to train the network with a learning rate \(\). We run SGD for \(e\) epochs on the dataset \(D\). The beginning of every new generation is characterized by introducing perturbations applied to the network weights to induce a high loss. This is done by reinitializing the parameters in the reset hypothesis while transferring or retaining the parameters belonging to the fit hypothesis. This dynamic process triggers a subsequent round of optimization, guiding the neural network toward the search for a new minimum in the parameter space. The initialization of the network \(f\) for the next generation \(f_{g}\) is as follows:

\[_{g}_{g-1}+(-) _{Reinit}\] (3)

where \(_{g-1}\) and \(_{g}\) are the parameters of the network \(f\) belonging to the previous generation and current generation, respectively. \(_{Reinit}\) corresponds to the randomly initialized tensor sampled from a uniform distribution. We then train the next generation of the network \(f_{g}\) using SGD with the same hyperparameters and epochs as the first generation.

### Dynamic Neural Regeneration (DNR) with Data-aware Dynamic Masking

Unlike KE, we propose a methodology that offers a distinct advantage regarding binary mask computation and parameter reinitialization. Motivated by the symbiotic link between generalization and selective neurogenesis in biological neural networks (9), we introduce a _Data-aware Dynamic Masking_ (DDM) that emulates the process of selective neurogenesis in evolutionary training. The benefits of DDM's way of reinitialization are two-fold. 1) It takes advantage of the evolutionary training paradigm and adapts the mask dynamically in each generation rather than using a predetermined mask. This introduces flexibility in the network and improves the generalization performance of deep learning models trained on small datasets. 2) Our masking scheme leverages a model's data and internal state to dynamically determine the important parameters for a given task, rather than relying on random pre-setting, to enhance the performance of deep learning models on small datasets. Our way of masking offers a priori knowledge of where and what parameters and layers should be reinitialized in the general case.

The mask \(M\) is calculated at the beginning of each generation in a data-dependent manner. We assess the importance or sensitivity of each connection in the network to the specific task by employing the SNIP method (23). SNIP decouples the connection weight from the loss function to identify relevant connections. We randomly sample a small subset of data (\(\)) from the current dataset to evaluate connection sensitivity. We define a connection sensitivity mask \(\{0,1\}^{||}\), where \(||\) denotes the number of parameters in the network. The mask is designed to maintain a sparsity constraint \(k\), which specifies the percentage of parameters to retain. The computation of connection sensitivity is performed as follows:

\[_{j}(;)=._{ 0}_{ce}(;)-_{ce}(( -_{j});)}{} |_{=1}\] (4)

where \(j\) corresponds to the parameter index and \(e_{j}\) is the mask vector of the index \(j\), where the magnitude of the derivatives is then used to calculate the saliency criteria (\(s_{j}\)):

\[s_{j}=_{j}(;)|}{_{k=1}^{m}|g_{k}( ;)|}.\] (5)

After calculating the saliency values, we apply the sparsity constraint \(k\) to the connection sensitivity mask, which ensures that only the top-k task-specific connections are retained. The sparsity constraint \(k\) is defined as follows:

\[_{j}=[s_{j}-_{} 0],  j\{1 m\},\] (6)

where \(_{k}\) is the \(k^{}\) largest element in the saliency vector \(s\), \([.]\) is the indicator function and m represents the total number of parameters in the neural network. Subsequently, using the saliency values obtained from the connection sensitivity analysis, we select and preserve the top-k important connections. The parameters associated with the connections deemed less important for the current generation are then reinitialized. This process effectively induces selective neurogenesis, allowing the network to adapt and free up its capacity for learning more generalized representations in subsequent generations. Finally, the network for subsequent generation is initialized as shown in Equation 3.

Intuitively, we incorporated selective neurogenesis as a replacement mechanism, reinitializing the input and output synaptic weights of specific subsets of network parameters dynamically during the evolutionary training process (24). Due to the challenges associated with where, how, and when to create neurons (20), we explore data-aware dynamic masking to drive neuron creation and removal, which could improve learning. We first select the crucial parameters based on the computed saliency mask. Ideally, we would like the mask to keep the knowledge learned from the previous generation as much as possible and to have enough learning capacity to accommodate the learning happening in the new generation. The additional learning capacity facilitates the fast adoption of generalized knowledge and reinforces the knowledge retained from the previous generation. In this way, selective neurogenesis is achieved that inherently adapts the network connectivity patterns in a data-dependent way to learn generalized representations without altering overall network size.

The network with the new initialization undergoes next-generation training with the same data for the \(e\) epochs, where \(e\) is kept the same for each generation. The network is trained with the loss function shown in Equation 1. Thus, we favor the preservation of the task-specific connections more precisely than the mask criteria used in KE that can guide the network towards those desirable traits that efficiently improve the performance and generalization of DNNs in small datasets.

## 4 Experimental Setup

Here, we provide the details on the experimental setup, implementation details, and datasets used in our empirical evaluation.

**Datasets:** We evaluate the proposed method using five datasets: Flower102 (25), CUB-200-2011 (26), MIT64 (27), Stanford Dogs (28), FGVC-Aircraft (29). The summaries of the statistics of the data set are mentioned in Appendix, Table 10.

**Implementation Details:** Since our framework is a direct extension of the KE, we follow the same experimental setup. The efficacy of our framework is demonstrated in two widely used architectures: ResNet18 and ResNet50 (30). We randomly initialize the networks and optimize them with stochastic gradient descent (SGD) with momentum 0.9 and weight decay \(1e-4\). We use the cosine learning rate decay with an initial learning rate lr = {0.1, 0.256} on specific datasets. The networks are trained iteratively for \(N\) generations (\(N\)=11) with a batch size \(b\)=32 for \(e\)=200 epochs without early stopping. The standard data augmentation techniques, such as flipping and random cropping, are used. We employ SNIP (23) with network sparsity \(k\) to find the critical subset of parameters at the end of each

    &  \\   & CUB & Aircraft & Dog & Flower & MIT & Mean \\  CE (\(f_{1}\)) & 53.57 \( 0.20\) & 51.28 \( 0.65\) & 63.83\( 0.12\) & 48.48\( 0.65\) & 55.28\( 0.19\) & 54.49 \\  DSD & 53.00\( 0.32\) & **57.24\( 0.21\)** & 63.58\( 0.14\) & 51.39\( 0.19\) & 53.21\( 0.37\) & 55.68 \\ BAN (\(f_{10}\)) & 53.71\( 0.35\) & 53.19\( 0.22\) & 64.16\( 0.13\) & 48.53\( 0.17\) & 55.65\( 0.28\) & 55.05 \\ KE (\(f_{10}\)) & 58.11\( 0.25\) & 53.21\( 0.43\) & 64.56\( 0.31\) & 56.15\( 0.19\) & 58.33\( 0.43\) & 58.07 \\ DNR (\(f_{10}\)) & **59.72\( 0.21\)** & 55.87\( 0.47\) & **65.76\( 0.13\)** & **58.10\( 0.24\)** & **61.78\( 0.36\)** & 60.25 \\  Smth (\(f_{1}\)) & 58.92 \( 0.24\) & 57.16 \( 0.91\) & 63.64 \( 0.16\) & 51.02 \( 0.09\) & 57.74\( 0.39\) & 57.70 \\  Smth + LW (\(f_{8}\)) & 70.50 \( 0.26\) & 67.10 \( 0.32\) & 65.76 \( 0.36\) & 66.92 \( 0.20\) & 61.67 \( 0.30\) & 66.39 \\ Smth + LLF (\(f_{8}\)) & **71.30 \( 0.14\)** & **68.87 \( 0.12\)** & 66.35 \( 0.22\) & 67.20 \( 0.24\) & 63.14 \( 0.18\) & 67.37 \\ Smth + DNR (\(f_{8}\)) & 70.95 \( 0.16\) & 66.10 \( 0.25\) & **66.56 \( 0.18\)** & **68.50 \( 0.27\)** & **63.94 \( 0.21\)** & 67.21 \\  Smth + LB (\(f_{10}\)) & 69.80\( 0.13\) & 65.29\( 0.51\) & 66.19\( 0.03\) & 66.89\( 0.23\) & 61.29\( 0.49\) & 65.89 \\ Smth + KE (\(f_{10}\)) & 66.51\( 0.070\) & 63.32\( 0.30\) & 63.86\( 0.21\) & 62.56\( 0.17\) & 59.58\( 0.62\) & 63.17 \\ Smth + DNR (\(f_{10}\)) & **71.37\( 0.22\)** & **66.63\( 0.37\)** & **66.81\( 0.20\)** & **68.36\( 0.14\)** & **64.10\( 0.58\)** & 67.45 \\   

Table 1: Compares the results of our method with the other weight reinitialization methods on ResNet18. \(g\) in \(f_{g}\) indicates the number of generations the model is trained.

generation. For the importance estimation, we use 20% of the whole dataset as a subset (\(\)). For all our experiments, we reinitialize a fixed 20% parameters of the network globally. All training settings (lr, \(b\), \(e\)) are constant throughout generations.

**Baselines:** To evaluate and benchmark the effectiveness of our proposed approach, we conduct a comprehensive evaluation by comparing it against several existing methods that involve iterative retraining and reinitialization. Specifically, we benchmark our method against the following techniques: 1) Dense-Sparse-Dense Networks (DSD) (3); 2) Born Again Networks (BANs) (4); and 3) Knowledge Evolution (KE) with its variant, KELS (5). We also compare our method against a non-iterative approach known as the Long Baseline (LB), which undergoes training for the same number of epochs as the corresponding iterative methods. Since our framework is built on top of KE, we follow the same procedure in all our experiments unless specified.

## 5 Results

In this section, we conduct comprehensive experimental evaluations of our method across multiple datasets and ablation studies. For an in-depth analysis, extended robustness experiments, including robustness to 15 types of natural corruptions at varying severity levels, adversarial attacks, and class imbalance, are provided in Appendix.

### Evaluation on Small Datasets

Table 1 presents the quantitative classification evaluation results using ResNet18. \(f_{g}\) denotes the result at the end of \(g^{th}\) generation. We compare DNR with two different configurations: (1) using naive cross-entropy loss (CE), and (2) incorporating label smoothing (Smith) regularizer with a hyperparameter \(=0.1\) (31).

The Dynamic Neural Regeneration (DNR) framework demonstrates flexibility and consistently improves performance over the considered baselines across datasets. Interestingly, KE underperforms in terms of performance compared to long baseline (LB) with equal computation cost. This discrepancy may be attributed to the use of fixed masking criteria throughout evolutionary training, limiting the model's adaptability. In contrast, DNR outperforms both longer baselines and KE, consistently improving generalization performance across all datasets.

Similarly, we compare the performance of our method with the label smoothing regularizer (Smith) applied to the baselines. Table 1 shows that our method consistently outperforms LB and KE on all datasets. DNR showcases slightly better performance compared to LW and LLF, with a key advantage being its independence from architecture-specific assumptions. Unlike LLF and LW, which lack prior knowledge of which features, layers, or parameters to reinitialize, DNR uses data-aware dynamic masking to selectively remove redundant connections. This reduces complexity and improves scalability as the model grows. Moreover, research indicates that memorization in neural networks is not confined to final layers but involves neurons distributed throughout the model (18), highlighting the limitations of architecture-specific assumptions and underscoring the effectiveness of our approach. Additionally, in real-world settings where data arrives in batches (32), DNR can dynamically iterate over each batch, optimizing performance. These results demonstrate the efficacy of the data-aware dynamic masking and selective reinitialization employed by DNR. By adapting task-specific parameters in each generation, DNR achieves superior performance and enhances the model's generalization.

### Evaluation on Large Datasets

Our work is a direct extension of KE (5), which focuses explicitly on improving generalization in the low data regime. However, we also thoroughly evaluate our method on large datasets such as Tiny-ImageNet (33), CIFAR10, and CIFAR100 (34) using ResNet50 to assess its scalability. Table 2 compares the effectiveness of our method (DNR) with Knowledge Evolution (KE) and longer baseline (LB) in larger data sets. For each model, we trained it on top of the baseline for a specific number of generations (\(f_{10}\)), where N indicates the number of generations. The proposed approach exhibits promising performance and generalization across various large-scale datasets, such as TinyImageNet, when compared to KE and longer baselines. Furthermore, while the performance of KE and longer baselines (LB) falls below the normal standard training (\(f_{1}\)), the DNR framework demonstratescomparable or slightly improved performance in this scenario. This suggests that a selective way of reinitializing benefits iterative training and can effectively handle the complexities and challenges associated with larger datasets and architectures.

### Robustness of Connection Selection across Training Steps

Unlike KE, which employs a randomly predetermined and fixed masking strategy, DNR provides a notable advantage through the utilization of Data-aware Dynamic Masking (DDM) for parameter reinitialization. Therefore, it is crucial to investigate whether DNR fully leverages the benefits of the evolutionary training paradigm by dynamically adapting the mask in each generation.

The proposed DNR framework employs SNIP [(23)] as a masking criterion to selectively regulate the parameters that have the least impact on performance in each generation of training. To examine this, we analyze the CUB200 dataset using the ResNet18 architecture. We save the mask generated by SNIP after the end of every generation. Visualizing the mask generated by the DNR framework can be challenging due to the large number of parameters in each layer of the backbone. To assess the consistency of connections across generations, we adopt a metric based on the percentage of overlap of retained parameters between the masks created in consecutive generations. This metric provides a quantitative analysis of the degree of flexibility induced by DNR in the evolutionary training process.

Figure 2 illustrates the layer-wise percentage overlap of retained parameters between consecutive generations in the DNR framework. The results reveal that the earlier layers consistently exhibit a high overlap percentage across all generations, indicating a consistent selection of connections.

The overlap percentage decreases in the later layers (specifically, layer 4 in ResNet) as the model learns class-specific information. This observation suggests that the mask adapts to capture task-specific features while maintaining stability in the earlier layers. Interestingly, we observe that the overlap percentage of the mask progressively increases as the evolutionary training progresses. Specifically, the overlap between the 9th and 10th generations is higher compared to the overlap between the 1st and 2nd generations. This observation suggests that the mask becomes more saturated and stable as the model state converges to a lower-loss landscape. This flexible nature of the DNR framework, allowing for the regulation of connections in both early and later layers, contributes to its effectiveness in improving generalization performance.

    &  \\   & CIFAR10 & CIFAR100 & TinyImageNet & Mean \\  Smth (\(f_{1}\)) & 94.32\(_{0.38}\) & 73.83\(_{0.23}\) & 54.15\(_{0.18}\) & 74.10 \\ Smth + LB (\(f_{10}\)) & 93.60\(_{0.29}\) & 74.21\(_{0.28}\) & 51.16\(_{0.21}\) & 72.99 \\ Smth + KE (\(f_{10}\)) & 93.50\(_{0.22}\) & 73.92\(_{0.31}\) & 52.56\(_{0.17}\) & 73.33 \\ Smth + DNR (\(f_{10}\)) & **94.61\(_{0.30}\)** & **75.05\(_{0.23}\)** & **54.50\(_{0.26}\)** & 74.72 \\   

Table 2: Compares the results of the DNR framework with the KE and longer baselines for ResNet50 on large datasets. \(g\) in \(f_{g}\) indicates the number of generations the model is trained.

Figure 2: Layer-wise percentage overlap of the retained parameters in consecutive generations.

### Comparison with Transfer Learning

Our approach using DNR, indeed differs widely from the domain of transfer learning. Unlike transfer learning, which primarily focuses on leveraging pre-trained models trained on large datasets from different domains to boost task performance on downstream tasks, DNR is intricately designed to tackle the intricate challenge of enhancing generalization in the presence of inherently limited or small datasets. A key issue with transfer learning arises when the pre-trained model's source domain vastly differs from the target domain of interest. This discrepancy between domains often leads to domain shifts, where the knowledge transferred from the pre-trained model fails to adapt well to the specificities of the target domain, thereby resulting in suboptimal performance.

In particular, in scenarios like medical applications, obtaining sufficient labeled data that closely aligns with the task at hand is exceptionally challenging. Though transfer learning is predominantly used in this field, the need for domain expertise, privacy concerns, and the uniqueness of each application domain make it exceedingly difficult to find a pre-trained model that seamlessly fits. Furthermore, the presence of domain shift between the source and target might lead to compromised performance, affecting the accuracy and generalization of the model on a specific task with limited data. DNR, on the other hand, offers a novel solution to these intricate challenges. By employing data-aware dynamic masking and selective reinitialization, DNR fosters the gradual evolution of the network, enabling it to adapt more effectively to the characteristics of the specific dataset. This process circumvents the problems of domain shifts that often plague transfer learning methods. Thus, while transfer learning remains valuable in contexts with abundant and well-aligned data, DNR stands out as a specialized approach to address the unique hurdles faced in scenarios of limited data availability, where the domain-shift problem can severely hinder model performance and generalization.

Furthermore, we have included a comparative analysis in Table 3 that involves an instance of transfer learning within the iterative training process, a process we refer to as vanilla fine-tuning. In this particular case, weights are directly transferred from one generation to the next without undergoing reinitialization.

This comparison serves to highlight the unique effectiveness of the DNR method. Our results distinctly demonstrate that DNR enhances the process of generalization, showcasing superior performance in comparison to the approach of directly transferring the complete network's weights across generations. This outcome further underscores the distinct advantage of DNR in evolving the network's capacity for better adaptation and learning in the evolutionary training paradigm.

### Analyzing Convergence Patterns: A Comparative Study between DNR and Transfer Learning

In Figure 3, we present the convergence behavior of our proposed DNR algorithm juxtaposed with vanilla fine-tuning. The x-axis delineates different generations during the training process, while the y-axis represents the performance at the end of each generation. We observe that the convergence of vanilla fine-tuning unfolds at a more gradual pace. In contrast, DNR demonstrates a faster

   Baselines & CUB & Aircraft & Dog & Flower & MIT \\  Smth + transfer learning (f3) & 65.63 \( 0.21\) & 61.02 \( 0.23\) & 63.84 \( 0.17\) & 57.62 \( 0.19\) & 58.04 \( 0.31\) \\ Smth + DNR (f3) & **68.56**\( 0.24\) & **64.37**\( 0.19\) & **65.72**\( 0.15\) & **62.13**\( 0.23\) & **62.62**\( 0.51\) \\   

Table 3: Comparative analysis of DNR and transfer learning across diverse datasets.

Figure 3: Convergence behavior: evaluating performance across generations in DNR and transfer learning with ResNet18 architecture trained on CUB dataset.

convergence rate. Across generations, DNR consistently surpasses vanilla fine-tuning, delivering a heightened performance level within a shorter training duration. The utilization of data-aware dynamic masking through SNIP in DNR amplifies this efficiency, enabling the model to concentrate on the most pertinent information for effective generalization.

### Effect of Importance Estimation Method

We conduct an investigation into the effectiveness of different methods to estimate the importance of parameters within our proposed training paradigm. Specifically, we explore Fisher Importance (FIM), weight magnitude, random selection, and SNIP (23) criteria. In Table 4, we present the performance and generalization results of the model trained with these various selection methods on the CUB200 dataset using the ResNet18 architecture.

Our findings demonstrate that the use of SNIP as data-aware dynamic masking yields superior performance compared to all other baseline methods. Surprisingly, the importance criteria based on weight magnitude exhibited inferior performance compared to random selection. However, the lottery ticket hypothesis (35) suggests the existence of sparse subnets within neural networks. Remarkably, when these subnets are trained in isolation, they can achieve a final performance accuracy comparable to that of the entire network in the same or even fewer training epochs. In particular, neurons within these winning subnets demonstrate higher rates of weight changes relative to other neurons. This observation raises the possibility of selectively reinitializing neurons that undergo minimal weight changes during training, as they contribute the least to loss function. Merely relying on the \(_{1}\) norm, which fails to capture the rate of weight changes, as described by the lottery ticket hypothesis, may not adequately capture the notion of importance. Therefore, our findings suggest that the utilization of SNIP for data-aware dynamic masking proves to be more effective, as it considers the rate of weight changes in determining the importance of parameters. This approach aligns better with the lottery ticket hypothesis and leads to improved performance and enhanced generalization capabilities in our experimental evaluations.

## 6 Conclusion

We presented Dynamic Neural Regeneration (DNR), an iterative/evolutionary training paradigm designed to improve the generalization of deep networks on small datasets. Our framework incorporates selective reinitialization at the end of each generation, employing a data-aware dynamic masking scheme to remove redundant connections based on their importance. This enables the model to increase its capacity for further learning, emphasizing the acquisition of generalizable features. Empirical results demonstrate that the proposed framework substantially enhances performance and generalization across small datasets compared to other reinitializing techniques. Moreover, DNR exhibits improved robustness in challenging real-world scenarios, including adversarial attacks and learning with class imbalances, while enhancing generalization on natural corruption data.

As a direction for future research, delving into the possibilities offered by growing networks (36) presents an intriguing path worth exploring. Our primary aim was to demonstrate the practical effectiveness of DNR, laying the foundation for theoretical exploration in this domain. We envision future studies to delve deeper into elucidating the theoretical underpinnings of DNR's success and exploring its application in diverse domains beyond those examined in this study. One possible limitation that emerged from our empirical evaluation is the sensitivity of our method to the choice of parameter selection. Addressing this limitation in future work could involve developing a robust parameter selection or importance estimation technique. This enhancement would improve our method's overall performance and deepen our understanding of overfitting dynamics by disentangling the contributions of different parameters. Finally, we believe the development of techniques like DNR holds the potential to advance the capabilities of deep learning models, paving the way for robust, adaptable, and generalizable artificial intelligence systems.

   Importance Criteria & CUB200 & Flower \\  LB & 69.80 \( 0.13\) & 66.89 \( 0.23\) \\ Random (KE) & 66.51 \( 0.07\) & 62.56 \( 0.17\) \\ FIM & 67.73 \( 0.28\) & 65.96 \( 0.20\) \\ Weight Magnitude & 64.18 \( 0.19\) & 66.90 \( 0.11\) \\ SNIP & **71.87**\( 0.22\) & **68.36**\( 0.14\) \\   

Table 4: Evaluating the performance of DNR with different importance estimation.