# Activation Map Compression through Tensor Decomposition for Deep Learning

Le-Trung Nguyen  Ael Quelennec  Enzo Tartaglione

Samuel Tardieu  Van-Tam Nguyen

LTCI, Telecom Paris, Institut Polytechnique de Paris

{name.surname}@telecom-paris.fr

###### Abstract

Internet of Things and Deep Learning are synergetically and exponentially growing industrial fields with a massive call for their unification into a common framework called Edge AI. While on-device inference is a well-explored topic in recent research, backpropagation remains an open challenge due to its prohibitive computational and memory costs compared to the extreme resource constraints of embedded devices. Drawing on tensor decomposition research, we tackle the main bottleneck of backpropagation, namely the memory footprint of activation map storage. We investigate and compare the effects of activation compression using Singular Value Decomposition and its tensor variant, High-Order Singular Value Decomposition. The application of low-order decomposition results in considerable memory savings while preserving the features essential for learning, and also offers theoretical guarantees to convergence. Experimental results obtained on main-stream architectures and tasks demonstrate Pareto-superiority over other state-of-the-art solutions, in terms of the trade-off between generalization and memory footprint.1

## 1 Introduction

Recent advances in Deep Learning have enabled Deep Neural networks to be used as an efficient solution for a wide variety of use cases, including computer vision , speech recognition  and natural language processing . Much of this performance improvement is linked to the exponential increase in the number of parameters in the neural architectures. According to Sevilla _et al._, the release of AlphaGo  in late 2015 marks the advent of a new era, which they call the "Large Scale Era", in reference to the computational cost of training doubling every 8 to 17 months . At the root of exponential growth in neural network size is the improvement in hardware capabilities, particularly those designed for large-scale parallel computing, such as GPUs and TPUs . While this trend demonstrates the strength of neural networks as a powerful generalization tool in many fields, it goes in the opposite direction when it comes to environmental concerns , making the deployment of newer architectures increasingly difficult. This is particularly true for edge devices such as mobile phones and embedded sys

Figure 1: We compress the activations that will be later employed for backpropagation.

tems, which cannot afford the high computing or memory costs. To address these challenges, three interconnected factors must be taken into account: power consumption, memory usage, and latency.

When considering larger neural networks with more layers and nodes, reducing their storage and computational cost becomes essential, especially for certain real-time applications such as edge computing. In addition, recent years have seen significant advances in the fields of intelligent edge and embedded intelligence, creating unprecedented opportunities for researchers to address the fundamental challenges of deploying deep learning systems on edge devices with limited resources (e.g. memory, CPU, power, bandwidth). Efficient deep learning methods can have a significant impact on distributed systems and embedded devices for artificial intelligence. Since training is supposed to take place in the cloud, most research on model compression and acceleration is specifically focused on inference . There is, however, an emerging area of research concerning on-device training, which represents a decisive advance in the field of artificial intelligence, with considerable implications for a variety of practical situations . Models trained offline on a dataset built at one point in time tend to fall victim to data drift when deployed "in the wild" . Its combination with online learning strategies has the potential to enable continuous model improvement after deployment , thus adapting the model predictions to observed evolutions in the data distribution. We can illustrate this with the example of sensors in autonomous vehicles, where deep learning models must correctly classify vehicles with new designs never seen in the training set. Other advantages of on-device learning include security and privacy. By processing data locally, sensitive information remains more secure and less susceptible to data breach, a major concern in applications such as healthcare.

The main challenge limiting the feasibility of on-device learning lies in the computational demands of the backward pass, as gradient computation and parameter updates are significantly more resource-intensive than the forward pass (Appendix A.1). On embedded devices, memory and computation limitations act as strict budgets that must not be exceeded. Some approaches address the memory constraints by exploring alternatives to traditional backpropagation, including unsupervised learning for image segmentation , the Forward-Forward algorithm , and PEPITA . While promising, these methods typically fall short of backpropagation-based techniques in terms of performance. A pioneering effort by Lin _et al._, demonstrated that fine-tuning a deep neural network within a 256 kB of memory is feasible by selectively updating a sub-network, achieving a good results . In a complementary approach, Yang _et al._ proposed reducing the number of unique elements in the gradient map through patch-based compression of the input and gradients of a given layer with respect to the output, thereby lowering memory costs and speeding up the learning process .

Inspired by tensor decomposition methods, we propose a method that compresses activation maps, reducing the memory demands for backpropagation while maintaining the deep neural network's generalization capability(Fig. 1). Our approach adaptively captures the majority of tensor variance, offering guaranteed accuracy in the gradient estimation. The key contributions of our work are as follows:

* We propose to exploit powerful low-rank approximation algorithms to compress activation maps, enabling efficient on-device learning with controlled information loss (Sec. 3.2 and Sec. 3.3).
* We provide a theoretical foundation for our method, along with an error analysis demonstrating that high compression ratios are achievable with limited performance degradation (Sec. 3.4).
* We extensively explore a diverse experimental landscape, demonstrating the generalization capacity of our proposed algorithm (Sec. 4).

## 2 Related Works

**Tensor Decomposition.** Model compression and acceleration is a well-developed and ongoing area of research in the deep learning community. It can be divided into two major areas, namely hardware and algorithm design. In the case of algorithmic compression, the five main components that stand out are efficient and compact model design, data quantization, network sparsification, knowledge distillation, and tensor decomposition [4; 7]. All these approaches aim to reduce the space occupied by network parameters. Also known as a low-rank approximation, _tensor decomposition_ has emerged as a robust solution due to its combination of grounded theoretical findings and its practically in terms of hardware implementation . Originating in the field of systems theory and signal processing , low-rank approximation has attracted growing interest in the deep learning community. Early examples were limited to the application of singular value decomposition (SVD) used to compress fully connected layers . More recent work includes the application of generalized Kronecker product decomposition (GKPD) to both linear and convolutional layers , the introduction of semi-tensor product (STP) to improve compression ratios with reduced loss  or even more recently the acceleration of inference and backpropagation in vision transformers . Evolutions in this domain can be summed up as improvements in compression ratios, latency, and power consumption, with limited performance loss and diversification of architectures considered.

**Activation Map Compression.** The term "activations" here refers to the outputs of each layer after the non-linearity has been applied, which are then fed to the next layer. While model compression generally aims at reducing the storage space occupied by parameters (which typically translates into network acceleration during inference), a key observation is that activations occupy much more space than parameters in memory during backward pass, as they are required to compute weight derivatives . This state of facts motivates a new line of research, whose main objective is to compress activation maps using methods inspired by the literature on weight compression. For example, we mention the use of quantization , sparsification , a combination of both with the addition of entropy encoding , or even the application of wavelet transform in combination with quantization . With the exception of Eliassen _et al._'s work which accelerates training runtime, most of these works focus on accelerating inference, in a similar way to traditional model compression.

**On-device Learning.** Today's typical pipeline for on-device AI consists of designing resource-efficient deep neural networks , training them offline on target data, compressing them, and deploying them for inference on the resource-constrained environment. Alongside the rapid commercial expansion of the IoT field and the ubiquitous presence of embedded devices in our daily lives, AI at the edge has naturally attracted a great deal of interest. However, recent research has shown that real-world data often deviated from training data, resulting in poor predictive performance . In such a context, continual learning proved to be a strong candidate for ensuring ongoing adaptation after deployment. Similar to the human learning process, it provides models with the ability to adapt to new incoming tasks, ideally without falling into the trap of catastrophic forgetting (i.e., degrading performance on older tasks in favor of new ones) [11; 33; 19]. In this respect, the very low memory and computing resources of extreme edge devices are an obstacle to the high cost of backpropagation. Lin _et al._ have shown that this challenge can be overcome by selectively fine-tuning a sub-graph of pre-trained networks on a general dataset . Their work started a line of research showing the possibility of improvement through careful selection of channels to be updated at training time [23; 35]. However, none of these works addresses the computational cost of training a neuron, which is related to both the cost of loading the necessary weights and activations into RAM.

**Gradient Filtering.** In their work, Yang _et al._ demonstrate that it is possible to significantly reduce the memory and computational cost of full layer learning using a method called gradient filtering, which reduces the number of unique elements in the gradient map . In such a frame and with the same specific objective, we propose to compress the activation maps by using tensor decomposition to minimize the memory cost. We perform this while providing a strong guarantee on the signal loss: unlike gradient filtering, we adaptively size the decomposed tensor to guarantee minimal information loss. In the next section, we will illustrate our proposed approach.

## 3 Method

In this section, we motivate our compression proposal by exposing the major memory bottleneck of backpropagation (Sec. 3.1). We then present our contribution which is the compression of activation maps through two well-known low-rank decomposition strategies, namely Singular Values Decomposition (SVD) and its tensor counterpart Higher Order SVD (HOSVD) (Sec. 3.2). In Sec. 3.3 we explore the induced changes to backpropagation and in Sec. 3.4 we study the resulting memory and computational complexity, alongside an evaluation of the error introduced. Our ultimate goal here is to reduce the memory footprint of backpropagation (BP).

### The Memory Bottleneck of Backpropagation

Following the formalism introduced in , we consider a simple convolutional neural network (CNN) represented as a sequence of \(n\) convolutional layers (excluding the bias for simplicity):

\[()=(_{_{n}}_{ _{n-1}}_{_{2}}_{_{1}})(),\] (1)

where \(_{i}^{C^{} C D D}\) represents the filter parameters of the \(i^{th}\) layer, with a kernel size of \(D D\). This layer receives a \(C\)-channel input and produces an output with \(C^{}\) channels. For this layer, we denote the input and output activation tensors as \(_{i}^{B C H W}\) and \(_{i+1}^{B C^{} H^{} W^{ }}\), respectively. Note that \(H\) and \(W\) denote the height and width of each element of the input, while \(H^{}\) and \(W^{}\) denote the height and width of each element of the output, with \(B\) as the minibatch size. The loss \(\) is computed at the output of the network and backpropagated until the \(i^{th}\) layer as \(}{_{i+1}}\). At this stage, the filter parameters are updated thanks to the computation of \(}{_{i}}\) and the loss is propagated to the previous layer as \(}{_{i}}\). The computation of these terms follows the chain rule:

\[}{_{i}} =}{_{i+1}}_{i+1}}{_{i}}=( _{i},}{_{i+1}}),\] (2) \[}{_{i}} =}{_{i+1}}_{i+1}}{_{i}}=_{ }[}{_{i+1}}, (_{i})],\] (3)

where \(()\) is the traditional convolution operation, convolving the kernel \(}{_{i+1}}\) with the input \(_{i}\); while \(_{}()\) is the convolutional operation that naturally maps the input \(}{_{i+1}}\) to an output with the same dimensions as \(_{i}\) by using the \(180^{}\) rotated kernel \(_{i}\).

From (2), it is clear that computing the weight derivatives requires to load input activation \(_{i}\) and (3) shows that the weights \(_{i}\) must be loaded into memory to calculate activation derivatives. We obtain the same conclusions for linear layers and provide the demonstration in Appendix A.2.

To save memory, two possibilities naturally emerge: either compressing the weights or compressing the activation. Weight compression is an extensively explored matter for network acceleration and we do not intend to further this area of research in this work. This leaves us with activation compression, which is still a new domain of exploration, but shows great potential for prospectively enabling on-device backpropagation. In such a regard, thanks to its strong theoretical grounding, tensor decomposition stands as a promising approach.

### Tensor Decomposition

We will present here first the general Singular Value Decomposition (SVD) approach, which is then extended to a multidimensional variant (HOSVD), instrumental for our purposes.

**SVD.** Given a matrix \(A^{M N}\), applying SVD to \(A\) consists in a factorization of the form:

\[A=U V^{T}, U^{M M},^{ M N}, V^{N N},\] (4)

where \(\) is a rectangular diagonal matrix composed of \(r\) singular values \(s_{i[1,r]}\) with \(r\) the rank of \(A\). From this, we can deduce the amount of overall variance \(_{i}^{2}\) explained by the \(i^{th}\) pair of SVD vectors as \(_{i}^{2}=s_{i}^{2}/_{j}s_{j}^{2}\).

Let us assume the singular values in \(\) are ordered in descending order, \(s_{i} s_{j}, i j\). Given a desired threshold of cumulated explained variance \(\), it becomes easy to find the "truncation threshold" which is the minimal \(K[1,r]\) such that \(_{i=1}^{K}_{i}^{2}\). We can then approximate \(A\) by only selecting the \(K\) first columns of \(U\) and \(V\) and the \(K\) first singular values from \(\), according to:

\[=U_{(K)}_{(K)}V_{(K)}^{T}, U_{(K)}^{M K },_{(K)}^{K K}, V_{(K)}^{N K}.\] (5)

Historically, SVD was the first example of tensor decomposition applied to neural network compression and acceleration  but applied to the model's parameters . We take the SVD decomposition to the activation maps as one possible baseline to compare with more complex low-rank compression solutions.

In the case of convolutional layer \(i\), since SVD is designed for matrix decomposition, we will simply reshape the activation \(_{i}\) as matrix \(A_{i}\) of dimensions \(B(CHW)\). Given a desired level of explained variance \(\), we obtain the decomposition described in (5). We then store in memory the terms\(U_{(K)}_{(K)}\) and \(V_{(K)}^{T}\), meaning that instead of storing \(_{}(BCHW)\) unique elements in memory, we are only storing \(_{}[K(B+CHW)]\) unique elements. Regarding linear layers, activations are \(M N\) matrices, and applying SVD is much more straightforward, leading to the storage cost of \(_{}[K(M+N)]\) instead of \(_{}(MN)\) elements. The larger the explained variance, the closer \(\) will be to \(A\), intuitively allowing for better estimation when performing backpropagation. However, this also means a larger \(K\) which results in a larger memory occupation. The goal is then to find an efficient trade-off between the desired explained variance and compression rates.

**HOSVD.** By construction, SVD is designed for matrix decomposition. It was demonstrated that the reshaping operation on tensors introduced structure information distortion, leading to sub-optimal performance . Other methods more suited for tensor decomposition such as Canonical-Polydic (CP) decomposition  or Tucker decomposition  were naturally introduced to tackle this issue. Given a \(n^{th}\)-order tensor \(^{M_{1} M_{2} M_{n}}\), its Tucker decomposition corresponds to:

\[=_{1}U^{(1)}_{2}U^{(2)}_{3} _{n}U^{(n)},\] (6)

where \(^{L_{1} L_{2} L_{n}}\) is the core tensor which can be viewed as a compressed version of \(\), \(U^{(j)}^{M_{j} L_{j}}\) are the factor matrices and their columns correspond to the principal components over the \(j^{th}\) mode. The \(i\)-mode product "\(_{i}\)" of a \(n^{th}\)-order tensor \(^{P_{1} P_{2} P_{n}}\) and a matrix \(B^{Q P_{i}}\) is a \(n^{th}\)-order tensor \(^{P_{1} P_{i-1} Q P_{i+ 1} P_{n}}\) which can be expressed as:

\[_{p_{1},,p_{i-1},q,p_{i+1},,p_{n}}=_{ i}Q=_{p_{i}=1}^{P_{i}}g_{p_{1},p_{2},,p_{n}}b_{q,p_{i}}.\] (7)

In such a setup, Higher-Order SVD is a specific case of Tucker decomposition where the factor matrices \(U^{(j)}\) are orthogonal . As a follow-up to SVD, we propose to compress activation tensors through HOSVD, with the intuition that each dimension encodes a different variance, potentially providing enhanced compression rates for equivalent performance and vice-versa. Similarly to traditional SVD, we can truncate \(\) and each \(U^{(j)}\) along each mode given a desired level of explained variance resulting in:

\[}=}_{1}U^{(1)}_{(K_{1})}_{2} _{n}U^{(n)}_{(K_{n})},\] (8)

where \(U^{(j)}_{(K_{j})}^{M_{j} K_{j}}\) corresponds to the \(K_{j}\) first columns of \(U^{(j)}\) and \(}^{K_{1} K_{n}}\) is the truncated version of \(\). In the following section, we focus on HOSVD decomposition, more precisely on the alterations induced to the backpropagation graph, the resulting compression and speedup ratio, and the error bound. An equivalent analysis can be made for SVD.

### Backpropagation with Compressed Activations

Prior works [13; 18] already demonstrated the possibility of performing backpropagation operations in the decomposed space without relying on the recomposition of compressed tensors. In their work, Kim _et al_. compress a \(4^{th}\)-order kernel tensor \(^{C^{} C D D}\) through Tucker decomposition limited to mode \(1\) and \(2\):

\[=^{}_{1}U^{(1)}_{2}U^{(2)}, ^{}^{L_{1} L_{2} D D}, U ^{(1)}^{C^{} L_{1}}, U^{(2)}^{C  L_{2}}.\] (9)

Then, it is shown that the output is calculated through

\[=_{1 1}\{U^{(2)},_{D D}[ ^{},_{1 1}(U^{(1)},)]\}.\] (10)

Our problem is similar although with some fundamental differences linked to the need to reconstruct _gradients_. A convolution operator is still required to compute the weight derivatives as introduced in (2): we apply a specific case of Tucker to one of the two components, namely the activation \(_{i}\). Following the same reasoning, we derive that the computation of \(}{_{i}}\) in (2) becomes:

\[}{_{i}}=_{1 1} \{_{*}[_{1 1}(_{1  1}(},^{(3)}_{(K_{3})}), ^{(4)}_{(K_{4})}),_{1 1}( }{_{i+1}},U^{(1)}_{(K_{1})} )],U^{(2)}_{(K_{2})}\},\] (11)

where \(_{*}\) is a 2D convolution with a specific kernel size as defined in (23), and \(^{(j)}_{(K_{j})}\) is the vertically padded version of \(U^{(j)}_{(K_{j})}\) as defined in (19). Demonstration details are available in Appendix A.3. This way, we can compute the approximated weight derivative without reconstructing the activation, through the successive computation of simpler convolutions.

Fig. 1 illustrates our method. During training, the forward pass proceeds as usual, with one key modification to memory management. Instead of keeping complete activation maps in memory, we store only their principal components, which are derived from the decomposition process. During the backward pass, the principal components are retrieved from memory and used for calculations as described in (11).

### Complexity and Error Analysis

**Computational Speedup and Space Complexity.** For simplicity we assume here that \(_{i+1}\) and \(_{i}\) have the same shape and that the truncation thresholds across all modes are equal for HOSVD, i.e., \(K_{1}=K_{2}=K_{3}=K_{4}\). Considering different thresholding values, we can predict the relative improvement of HOSVD to vanilla training in both space complexity (30) and computational speedup (29), for a backward pass. As illustrated in Fig. 1(a) and Fig. 1(b), our method is more effective for both space complexity and latency for the backward pass when the truncation threshold decreases and the activation size increases. More details are provided in Appendix A.4.

We hypothesize that the first components along each dimension are enough to encode most of the variance, implying that with relatively low values of \(K_{j}\), we can achieve good training performance. We later empirically confirm this assumption in a variety of experimental setups (Sec. 4.2).

**Signal to Noise Ratio.** For simplicity, we rewrite in this paragraph \(}{_{i}}\) as \(\), \(_{i}\) as \(\) and \(}{_{i+1}}\) as \(\). Regarding the error introduced by truncating \(\) in order to retain the components corresponding to a given level of explained variance \(\), we demonstrate that the energy contained in the resulting gradient is equal to the energy contained in \(}\). We note \(I, W\) and \( Y\) as the input activation, weight derivative, and output activation derivative in the frequency domain; \(I[u,v]\), the spectrum value at frequency \((u,v)\). Applying the discrete Fourier transformation to \(}\) gives us \(= I\), which we use to compute the signal to noise ratio \(SNR_{}\):

\[SNR_{}=I[u,v]^{2}}{(I[u,v]-  I[u,v])^{2}}=}.\] (12)

Similarly, in the frequency domain, as the convolutional operation becomes a regular multiplication, \(}\) becomes \(= Y= I Y\). As for (12), we obtain \(SNR_{}=(1-)^{-2}=SNR_{}\). A visual representation is provided in Fig. 1(c). Analytically, this confirms the idea that the closer \(\) is to \(1\), the larger the energy is transferred from the compressed input to the weight derivatives (for \(=0.8\) we have \(SNR_{}=25\)).

Additionally, in our setup since we are only compressing the activations and given (2) and (3), the introduced error only transfers to the weight derivative for each layer. The activation derivatives that are propagated from one layer to another are exact computations as they do not involve the

Figure 2: For a single convolutional layer with minibatch size \(B\), **(a)** and **(b)** illustrate the predicted changes in compression rate \(R_{C}\) and speedup ratios \(R_{S}\) as functions of \(K_{j}\), when comparing HOSVD with vanilla training, respectively. **(c)** shows the evolution of the SNR with retained variance \(\).

activations. This means that the error introduced when compressing the activations at each layer does not accumulate through the network.

## 4 Experiments

In this section, we describe the experiments conducted to support the claims presented in Sec. 3. First, we introduce the setups used for our experiments (Sec. 4.1); then, we analyze the energy distribution in the different dimensions of HOSVD, providing an overview of the typical values of \(K\) (Sec. 4.2); finally, we test our algorithm in different setups, state-of-the-art architectures and datasets to evaluate the tradeoff between accuracy and memory footprint (Sec. 4.3). Experiments were performed using a NVIDIA RTX 3090Ti and the source code uses PyTorch 1.13.1.

### Experimental setup

To validate our approach, we perform a variety of computer vision experiments split across two tasks, classification and segmentation.

**Classification.** We explore two types of fine-tuning policies:

* _Full fine-tuning_ (referred to as "setup A"): Following conventional fine-tuning trends, we load models pre-trained on ImageNet  and we fine-tune them on a variety of downstream datasets (CIFAR-10, CIFAR-100, CUB , Flowers  and Pets ).
* _Half fine-tuning_ (referred to as "setup B"): Following Yang _et al._ approach, each classification dataset (ImageNet, CIFAR-10/100) is split into two non-i.i.d. partitions of equal size using the FedAvg  method. The partitions are then split as follows: \(80\%\) for training and \(20\%\) for validation. The first partition is used for pretraining, and the second partition is used for finetuning.

**Semantic Segmentation.** Similarly to the half fine-tuning method, we reproduce Yang _et al._ segmentation setup: we fine-tune models pretrained on Cityscapes  by MMSegmentation . Here there is only one downstream dataset which is Pascal-VOC12 . Experimental details for both tasks (hyper-parameters, policy, etc.) are provided in Appendix B.2.

**Memory Logging.** For HOSVD and SVD, instead of focusing on compressing the tensor based on rank, we control compression through the desired amount of retained information. As a consequence, we cannot explicitly control the memory usage of the principal components. In the results presented below, we will always include two columns displaying peak and average memory, along with their standard deviations.

### Explained Variance Evolution

In this section, we conduct experiments to fine-tune the last four convolutional layers of MCUNet  following Setup A, and CIFAR-10 as the downstream dataset. Using HOSVD with \(\) set to \(0.8\), Fig. 3 shows the explained variance retained across dimension \(j\) as a function of \(K_{j}\), where \(j=\{1,2\}\) corresponds to the two largest dimensions of the activation map. We define \(K_{j}^{(=x)}\) as the number of principal components required to retain at least a fraction \(x\) of the explained variance.

Figure 3: Explained variance \(\) for the first two dimensions of the activation map in the \(4^{th}\) last layer when fine-tuning the last four layers of MCUNet using HOSVD on CIFAR-10, following setup A.

We observe that along the largest dimensions (batch size and number of channels), less than \(20\%\) of the components capture more than \(80\%\) of the explained variance, confirming the assumption proposed in Sec. 3.4. Additionally, the curves observed present a logarithmic behavior, hinting at the possibility of reaching high explained variance with little loss regarding the compression ratio. This is especially important as the \(SNR\) transmitted to the weight derivatives increases quadratically to the explained variance (Fig. 2c). The results for the other layers are presented in Appendix B.3.

Fig. 4 illustrates the results of performing HOSVD with different explained variance thresholds \(\). The results are averaged over three different random seeds. For \(\) smaller than \(0.8\), we observe that as it increases, we achieve significant gains in accuracy, along with impressive compression ratios. However, when \(\) exceeds \(0.8\), the accuracy growth starts slowing down. Above the \(0.9\) threshold, the exponential growth of peak memory results in a worsened tradeoff between accuracy and compression ratio. Therefore, in subsequent experiments presented in this paper, we will use \(\) values of \(0.8\) and \(0.9\).

### Main Results

**MCUNet on classification with setup A.** In this experiment, we fine-tune on CIFAR-10 an MCUNet pre-trained on ImageNet, with the number of finetuned layers increasing from \(1\) to \(42\) (all layers). We compare vanilla training, and gradient filtering with patch sizes of \(2\), \(4\), and \(7\), SVD, and HOSVD with an explained variance threshold of \(0.8\). For each method, we compare the effect of fine-tuning different model depths. Fig. 5 presents the performance curves for our experiments, with the X-axis representing activation memory in kiloBytes (kB) on a logarithmic scale and the Y-axis representing the highest validation accuracy. Each marker indicates the number of convolutional layers finetuned. For example, on the yellow curve representing the HOSVD method, the marker closest to the Y-axis shows the result when finetuning the last convolutional layer, the next marker represents finetuning the last two convolutional layers, and so on. The most effective method is the one whose performance curve trends towards the upper-left corner of the plot.

Figure 4: Behavior of top1 validation accuracy and peak memory when applying HOSVD with different explained variance thresholds \(\) when finetuning the last four convolutional layers of an MCUNet model using the CIFAR-10 dataset on setup A.

Figure 5: Performance curves of an MCUNet pre-trained on ImageNet and finetuned on CIFAR-10 with different activation compression strategies.

We observe that as the number of finetuned layers increases, the gradient filtering accuracy increases up to a certain point, and then deteriorates, whereas the accuracy for SVD, HOSVD, and vanilla training keeps improving with additional layers, along a similar trend. Intuitively, gradient filtering might propagate errors through the layers during training, while our HOSVD method keeps the error introduced on each individual layer confined to that specific layer as demonstrated in Sec. 3.4. Moreover, we observe that for identical depths, SVD accuracies consistently exceed HOSVD ones. We hypothesize that this is due to HOSVD performing SVD across all modes of the tensor: it potentially loses information in all modes, whereas SVD only loses information in one mode.

Compared to SVD, HOSVD significantly reduces memory, given equivalent accuracy levels (up to \(18.87\) times). Similarly, for equivalent memory usage, HOSVD yields greatly improved accuracy compared to SVD (up to \(19.11\%\)). When compared to methods such as gradient filtering and vanilla training, given an equivalent memory budget, HOSVD yields significantly higher accuracies. Notably, fine-tuning all layers with HOSVD requires much less memory than fine-tuning only the last layer with vanilla training, which is consistent with the theoretical compression ratio shown in Fig. 1(a). Additional experiments with setup A can be found in Appendix B.5.

**ImageNet Classification with setup B.** Table 1 presents the classification performance and memory consumption for MobileNetV2 , ResNet18, and ResNet34  models using various methods, including vanilla training, gradient filtering, HOSVD, and SVD on the ImageNet dataset. We observe in most cases that, for the same depth, SVD and HOSVD are competitive with the gradient filtering method in terms of performance while reaching much lower activation memory with HOSVD.

**Segmentation.** Table 2 reports the segmentation performance and memory consumption for a variety of architectures as presented in Yang _et al._. In general, we observe that increasing the level of explained variance from \(0.8\) to \(0.9\) substantially increases the performance with little trade-off on retained activation memory. This further confirms that most of the explained variance is contained in the first few components, allowing for efficient generalization with high compression rates.

## 5 Conclusion

In this work, we have addressed one of the main obstacles to on-device training. Inspired by traditional low-rank optimization approaches, we propose to compress activation maps by tensor decomposition, using HOSVD as a supporting example (Sec. 3.2). We demonstrate that the compression error introduced is bounded and confined to each individual layer considered, validating our approach

    &  &  &  \\    & \#Layers & Acc \(\) & Peak Mem (MB) \(\) & Mean Mem (MB) \(\) & \#Layers & Acc \(\) & Peak Mem (MB) \(\) & Mean Mem (MB) \(\) \\  Vanilla & \(\)I & 74.0 & 1651.84 & 1651.84 \(\) 0.00 &  & All & 72.8 & 52.88 & 532.88 \(\) 0.00 \\ training & 2 & 62.6 & 15.31 & 15.37 \(\) 0.00 &  & \(\) & 69.9 & 12.52 & 12.25 \(\) 0.00 \\  & 4 & 65.8 & 28.71 & 28.71 \(\) 0.00 & & & 71.5 & 30.63 & 30.63 \(\) 0.00 \\  Gradient & 2 & 62.6 & 5.00 & 5.00 \(\) 0.00 & Gradient & 2 & 68.7 & 4.00 & 4.00 \(\) 0.00 \\  Filter R2 & 4 & 65.2 & 9.38 & 9.38 \(\) 0.00 & Filter R2 & 4 & 69.3 & 7.00 & 7.00 \(\) 0.00 \\  SVD & 2 & 61.7 & 4.97 & 4.92 \(\) 0.08 & SVD & 2 & 69.5 & 7.88 & 7.71 \(\) 0.21 \\ \((e=0.8)\) & 4 & 65.2 & 14.76 & 14.61 \(\) 0.09 & \((e=0.8)\) & 4 & 71.1 & 19.98 & 19.72 \(\) 0.28 \\  SVD & 2 & 62.3 & 8.97 & 8.91 \(\) 0.08 & SVD & 2 & 69.7 & 9.86 & 9.77 \(\) 0.13 \\ \((e=0.9)\) & 4 & 65.5 & 20.35 & 20.20 \(\) 0.07 & \((e=0.9)\) & 4 & 71.3 & 24.81 & 24.66 \(\) 0.17 \\  HOSVD & 2 & 61.1 & 0.15 & 0.15 \(\) 0.00 & HOSVD & 2 & 69.2 & 0.97 & 0.91 \(\) 0.05 \\ \((e=0.8)\) & 4 & 63.9 & 0.73 & 0.68 \(\) 0.03 & \((e=0.8)\) & 4 & 70.5 & 2.89 & 2.74 \(\) 0.12 \\  HOSVD & 2 & 61.8 & 0.43 & 0.43 \(\) 0.01 & HOSVD & 2 & 69.5 & 2.73 & 2.63 \(\) 0.10 \\ \((e=0.9)\) & 4 & 64.8 & 1.92 & 1.76 \(\) 0.08 & \((e=0.9)\) & 4 & 71.1 & 7.96 & 7.66 \(\) 0.21 \\    &  \\    & \#Layers & Acc \(\) & Peak Mem (MB) \(\) & Mean Mem (MB) \(\) & \#Layers & Acc \(\) & Peak Mem (MB) \(\) & Mean Mem (MB) \(\) \\  Vanilla & All & 67.4 & 62.98 & 63.98 \(\) 0.00 & Vanilla & All & 75.6 & 89.04 \(\) 0.00 \\ training & 2 & 62.71 & 13.78 & 13.78 \(\) 0.00 &  & \(\) & 69.6 & 12.55 & 12.25 \(\) 0.00 \\    & 4 & 64.7 & 19.52 & 19.52 \(\) 0.00 & & 4 & 72.2 & 24.50 \(\) 0.00 \\  Gradient & 2 & 61.8 & 4.50 & 4.50 \(\) 0.000 & Gradient & 2 & 68.8 & 4.00 & 4.00 \(\) 0.00 \\ Filter R2 & 4 & 64.4 & 6.38 & 6.38 \(\) 0.00 & Filter R2 & 4 & 70.9 & 8.00 & 8.00 \(\) 0.00 \\  SVD & 2 & 62.0 & 7.62 & 7.51 \(\) 0.12 & SVD & 2 & 69.2 & 6.70 & 6.49 \(\) 0.29 \\ \((e=0.8)\) & 4 & 64.5 & 10.59 & 10.37 \(\) 0.20 & \((e=0.8)\) & 4 & 71.8 & 14.68 & 14.24 \(\) 0.50 \\  SVD & 2 & 62.1 & 10.32 & 10.26 \(\) 0.08 & SVD & 2 & 69.4 & 9.10 & 8.96 \(\) 0.23 \\ \((e=0.9)\) & 4 & 64.6 & 14.39 & 14.26 \(\) 0.13 & \((e=0.9)\) & 4 & 72.0 & 19.11 & 18.83 \(\) 0.37 \\  HOSVD & 2 & 61.7 & 0.48 & 0.43 \(\) 0.04 & HOSVD & 2 & 68.7 & 0.30 & 0.27 \(\) 0.02 \\ \((e=0.8)\) & 4 & 63.9 & 0.88 & 0.78 \(\) 0.07 & \((e=0.8)\) & 4 & 71.1 & 1.11 & 1.02 \(\) 0.07 \\  HOSVD & 2 & 62.0 & 13.2 & 12.7 \(\) 0.06 & HOSVD & 2 & 69.2 & 0.71 & 0.65 \(\) 0.05 \\ \((e=0.9)\) & 4 & 64.4 & 2.52 & \(2.36\) 0.15 & \((e=0.9)\) & 4 & 71.9 & 3.24 & 3.09 \(\) 0.13 \\   

Table 1: Experimental results on ImageNet-1k. “#Layers” refers to the number of fine-tuned convolutional layers (counted from the end of the model). Activation memory consumption is shown in MegaBytes (MB).

[MISSING_PAGE_FAIL:10]