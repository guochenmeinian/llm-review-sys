# Retrieval-Augmented Multiple Instance Learning

Yufei Cui\({}^{1}\)\({}^{2}\), Ziquan Liu\({}^{2}\), Yixin Chen\({}^{3}\), Yuchen Lu\({}^{4}\), Xinyue Yu\({}^{4}\),

**Xue Liu\({}^{1}\), Tei-Wei Kuo\({}^{5}\)\({}^{6}\), Miguel R. D. Rodrigues\({}^{2}\), Chun Jason Xue\({}^{3}\), Antoni B. Chan\({}^{3}\)**

\({}^{1}\)Mila, McGill University \({}^{2}\)University College London \({}^{3}\)City University of Hong Kong

\({}^{4}\)Mila, Universite de Montreal \({}^{5}\)National Taiwan University \({}^{6}\)MBZUAI

###### Abstract

Multiple Instance Learning (MIL) is a crucial weakly supervised learning method applied across various domains, e.g., medical diagnosis based on whole slide images (WSIs). Recent advancements in MIL algorithms have yielded exceptional performance when the training and test data originate from the same domain, such as WSIs obtained from the same hospital. However, this paper reveals a performance deterioration of MIL models when tested on an out-of-domain test set, exemplified by WSIs sourced from a novel hospital. To address this challenge, this paper introduces the Retrieval-AugMented MIL (RAM-MIL) framework, which integrates Optimal Transport (OT) as the distance metric for nearest neighbor retrieval. The development of RAM-MIL is driven by two key insights. First, a theoretical discovery indicates that reducing the input's intrinsic dimension can minimize the approximation error in attention-based MIL. Second, previous studies highlight a link between input intrinsic dimension and the feature merging process with the retrieved data. Empirical evaluations conducted on WSI classification demonstrate that the proposed RAM-MIL framework achieves state-of-the-art performance in both in-domain scenarios, where the training and retrieval data are in the same domain, and more crucially, in out-of-domain scenarios, where the (unlabeled) retrieval data originates from a different domain. Furthermore, the use of the transportation matrix derived from OT renders the retrieval results interpretable at the instance level, in contrast to the vanilla \(l_{2}\) distance, and allows for visualization for human experts.

## 1 Introduction

As the standard supervised learning paradigm, single instance learning has been the focus of machine learning research and its performance has been improved significantly since the advent of deep learning . Nonetheless, a notable drawback of single instance learning lies in its reliance on a substantial volume of labeled data , which poses practical challenges as a result of the high expense and time-consuming nature of fine-grained data labeling. Consequently, multiple instance learning (MIL)  has gained increasing popularity, given its ability to be trained with limited supervision. This trend has particularly gained momentum within the domain of histopathology, where the analysis of medical images at a gigabyte scale, such as whole slide images (WSI), has emerged as a focal point in recent studies .

Even though the performance of MIL on cancer diagnosis  is remarkable, existing algorithms are only evaluated on the in-domain test set and have the risk of performance degradation when confronted with distribution shifts between the training and test sets . The distribution shift problem is particularly important in the context of automated medical diagnosis models, since the models are deployed across diverse hospitals in distinct regions, each may employing varying imagingtechniques. The robustness of such models in the face of distribution shift is of utmost importance within this safety-critical application, as any failure to address this challenge may result in severe consequences.

This paper first unveils that the performance degradation indeed happens for state-of-the-art MIL models (Fig. 2) and proposes a retrieval-based MIL solution for improving both in-domain and out-of-domain performance. We prove a theorem that demonstrates the reduced input dimensions leads to improved MIL performance. Inspired by the connection between feature merging learning and intrinsic dimension , we propose the Retrieval-AugMented MIL (RAM-MIL) algorithm to learn a low intrinsic dimension feature space and enhance the model generalization, especially for out-of-domain data. In the retrieval process, we use the optimal transport (OT) as the distance metric between two bags because our main theorem is proved with the generalized OT measure. In our setting, the instances in a bag and their associated attention values in attention-based MIL are used to form a discrete distribution used to represent a bag, and OT is used to calculate the distance between the two discrete distributions of two bags. Following retrieval, the original bags are merged with their retrieved counterparts from a retrieval dataset, employing a merge function such as convex combination, to generate feature representations. Subsequently, a classifier is trained as the MIL model. The retrieval dataset can either be the original dataset in the absence of out-of-domain data, or an _unlabeled_ out-of-domain dataset in the out-of-domain adaptation setting. Our empirical study confirms that RAM-MIL effectively reduces the intrinsic dimensionality of the input feature space, surpassing existing MIL methods in both in-domain and out-of-domain settings. The advantage of OT distance over \(l_{2}\) and OT-variant distances is demonstrated by our ablation study. In addition, the OT-based retrieval result can be visualized using the transportation matrix, which makes the retrieval process interpretable. Our contributions are three-fold:

1. This work is among the first to investigate the out-of-domain performance for MIL, which is a vital issue for the application of MIL in automated medical diagnosis with WSIs. Our empirical study exposes the risk of recent MIL algorithms under distribution shifts.
2. Our theoretical result based on Wasserstein distance in the input space shows a negative correlation between MIL performance and the input data dimension, which motivates us

Figure 1: The proposed RAM-MIL method. **Left**: An attention-based MIL model is pre-trained on the training set to produce feature representations and attention weights. **Right**: With pre-trained features and attention weights, Optimal-Transport (OT) is employed to compute bag-to-bag distance, based on which the nearest neighbor bag from the retrieval set is selected. A training bagâ€™s feature is then merged with its retrieved bag, which is used as the input feature to a bag classifier.

Figure 2: The comparisons of accuracy of MIL methods under in-domain and out-of-domain (OOD) settings. The out-of-domain is tested on Camelyon17  with models trained on Camelyon16 . MIL fails to generalize well to OOD data, while the proposed RAM-MIL improves the OOD performance as well as the in-domain performance.

to propose the novel RAM-MIL framework based on the OT as the distance metric in the retrieval process.
3. Our experiment demonstrates that RAM-MIL outperforms state-of-the-art MIL methods in both in-domain and out-of-domain adaptation scenarios. Additionally, the transportation matrix of OT distance provides a tool to visualize the correspondence between original and retrieval instances, which makes our retrieval method interpretable.

## 2 Motivation of RAM-MIL

In this section, we show that why using OT could lead to a performance improvement for multiple instance learning.

We follow the standard formulation of Attention-based Multiple Instance Learning (MIL) [3; 4]. In MIL, the input is a bag of instances, \(=\{_{1},,_{K}\},_{k}^{D}\), \(\) and \(K\) is the number of instances, which varies for different bags. There is a bag-level label \(Y\), which is positive if at least one of the instances is positive, and negative if all instances are negative. We further assume the instances also have corresponding instance-level labels \(\{y_{1},,y_{K}\}\), which are _unknown_ during training. There are \(N\) such bag-label pairs constituting the dataset \(=\{_{n},Y_{n}\}_{n=1}^{N}\). The objective of MIL is to learn an optimal function for predicting the bag-level label with the bag of instances as input. To this end, the MIL model should be able to aggregate the information of instances \(\{_{k}\}_{k=1}^{K}\) to make the final decision. A well-adopted aggregation method is the embedding-based approach which maps \(\) to a bag-level representation \(^{d}\) and use \(\) to predict \(Y\).  extends the embedding-based aggregation approach by leveraging the attention mechanism, namely attention-based deep MIL (ABMIL). First, a transformation \(g()\) computes a low-dimensional embedding \(_{k}=g(_{k})^{d}\) for each instance \(_{k}\). The attention module aggregates the set of embeddings \(\{_{k}\}_{k=1}^{K}\) into a bag level embedding \(=_{k=1}^{K}a_{k}_{k}=_{k=1}^{K}a_{k}g(_{k})=()\), where the attention \(a_{k}\) for the \(k\)-th instance is computed via a softmax function,

\[a_{k}=^{T}((_{1}^{T}_{k})(_{2}^{T}_{k}))\}}{_{j=1}^{K}\{^{T}( (_{1}^{T}_{j})(_{2}^{T}_{j}) )\}}.\] (1)

where \(,_{1},_{2}\) are the network parameters. The bag embedding \(\) is then mapped to the logits \(\) with a feed forward layer with parameter \(\) for the bag-level classification, \(=^{T}\).

Assuming there is a ground-truth set scoring function \(S():\) that generates the label \(Y\), our task is to approximate the function with the attention-based model. We have the following theorem demonstrating that, when employing the Wasserstein distance in the input space, the approximation error of a Lipschitz continuous set scoring function is upper bounded by a term associated with the input dimension \(d\). The insight from the theorem offers a guidepost for the design of our methodology.

**Theorem 1**.: _Suppose \(S()\) is a Lipschitz continuous function with respect to Wasserstein distance \(_{p}\) (\(1 p<\)) and the Lipschitz constant is \(L\). The bag of instances \(\) is sampled from a probability distribution \(()\) with distribution dimension \(d_{}\) (intrinsic dimension). For any invertible map \(:^{d}\), \(\) function \(\) and \(\), such that for any \(\),_

\[|S()-(_{}\{(): \})| O(L K^{-}}).\] (2)

The theorem has two implications for the ABMIL problem. First, using the Wasserstein metric in the input space, we establish this dimension-dependent error bound for the approximation, indicating the good property of Wasserstein metric in the theoretical sense. Second, this dimension-dependent error bound motivates our RAM-MIL framework to reduce the intrinsic dimensionality of the input space for improving the performance of ABMIL, which is consistent with a previous work  that shows feature mixup is effective at reducing input feature dimension. This paper uses OT distance as the instantiation of the Wasserstein distance, since OT with Euclidean distance as the cost is equivalent to \(_{1}\). We use the entropy-regularized OT as the approximation of OT distance in our implementation since the regularization makes the optimization process more stable and efficient and is widely used in machine learning and vision research [17; 18]. The incorporation of OT into Multiple Instance Learning (MIL) provides a new framework for understanding and assessing the similarity between different instances. Intuitively, each bag in MIL can be considered as a probability distribution of instances, where the probabilities are the attention values \(\). Thus, OT offers a rigorous and meaningful metric to quantify the similarity between two distributions. It takes into account not only the individual differences of instances but also their global arrangement within a bag, hence,providing a holistic comparison. Note that although Theorem 1 is proved for the input space \(\), it is trivial to prove a similar result for the feature space \(\) (see the supplemental).

Building upon the benefits brought by OT, we propose a retrieval mechanisms to further strengthen the MIL framework. The key intuition is to leverage the rich information available across different bags. This allows for more robust and generalizable learning as each bag might not exist in isolation but is part of a broader set of data with shared and contrasting characteristics. The retrieval process, particularly when guided by OT, helps to identify and bring into context "neighbor" bags that are most relevant or similar to a given bag. This allows the model to learn more effectively by taking into account the broader context in which each bag exists. Empirically, we confirm the OT-guided retrieval could effectively reduce the intrinsic dimension of input feature space, which explains the performance of proposed methodology. Moreover, with the computed transport matrix, the correspondence between the original and selected bag can be visualized, which makes the retrieval and classification process more interpretable/explainable.

## 3 Methodology

This section presents the RAM-MIL algorithm based on OT distance and its application in the out-of-domain adaptation.

### Retrieval-Augmented Multiple Instance Learning based on Optimal Transport

We introduce the methodology that utilizes OT distance to address the retrieval problem for multiple instance learning (MIL). The idea of enhancing MIL using retrieval lies in augmenting a bag feature from the original dataset with its nearest neighbor bag feature from the retrieval dataset. Consider the original dataset of bags marked for model training and validation, denoted as \(_{o}=\{_{n},Y_{n}\}_{n=1}^{N_{o}}\), and the unlabeled retrieval set represented as \(_{r}=\{}_{m}\}_{m=1}^{N_{r}}\), where \(N_{o}\) and \(N_{r}\) signify the number of bags in the two respective sets. For each instance \(_{k}\) in a bag \(\), we extract an instance representation \(_{k}=g(_{k})\), where \(g()\) is a neural net for feature extraction, such as ResNet50 . Recall that an MIL model maps the bag of instance representations to the logits \(\) for loss computation, expressed as \(=^{T}\). The intermediate representation \(=()\), computed by averaging instance features and attention weights \(=()=_{k=1}^{K}a_{k}_{k}=_{k=1}^{K}a_{k}g (_{k})\), is regarded as the _bag representation_.

The objective of the retrieval scheme is to select the bag in the retrieval set that is nearest to the input bag. To perform bag-to-bag retrieval, we use the distributional distance between two bags, which considers their constituent instances. Once the retrieved bag is obtained, its bag representation \(}^{}\) is then merged with the original bag representation \(\). The merged feature, denoted as \(}=(,}^{})\), is used for bag classification, where \((,)\) is the merging function, e.g., a convex combination.

```
1:Pre-train an MIL model on \(_{o}=\{_{n},Y_{n}\}_{n=1}^{N_{o}}\).
2:Extract the sets of instance representations, i.e., \(}_{o}=\{_{n}\}_{n=1}^{N_{o}}\) and \(_{r}=\{}_{m}\}_{m=1}^{N_{r}}\), and compute their attentions, i.e., \(\{_{n}\}_{n=1}^{N_{o}}\) and \(\{}_{m}\}_{m=1}^{N_{r}}\)
3:Extract the sets of bag representations \(\{_{n}\}_{n=1}^{N_{o}}\) and \(\{}_{n}\}_{n=1}^{N_{r}}\) with pre-trained model.
4:Input:\(\{_{n},Y_{n},_{n},_{n},_{n}\}_{n=1}^{N_{o}},\{}_{m},}_{m},}_{m},}_{m},}_{m}\}_{m=1}^{N_{r}}\)
5:for\(n\) from \(1\) to \(N_{o}\)do
6:\(^{*}=_{}_{m}_{r}}d_{OT}(_{n},_{m})\), where \(d_{OT}(_{n},_{m})\) is solved by (4)
7: Store \(}_{n}=(_{n},_{m^{*}})\), where \(m^{*}=(^{*})\)
8:endfor
9:Train a logistic regression bag classifier on \(\{}_{n},Y_{n}\}_{n=1}^{N_{r}}\). ```

**Algorithm 1** Retrieval-Augmented Multiple Instance Learning (RAM-MIL) Algorithm

Retrieval with Optimal Transport.To leverage the probabilistic geometry between instances, instead of directly retrieving the nearest bags based on the bag representations \(\), our retrieval operates on the bags of instance representations \(_{r}=\{}_{m}\}_{m=1}^{N_{r}}=\{\{}_{mk}\}_{ k=1}^{K}\}_{m=1}^{N_{r}}\). Here \(K\) represents the number of instances, \(}_{m}\) indicates _the bag of instance representations_ and \(}_{mk}\) is an _instance representation_ derived via \(}_{mk}=g(}_{mk})\). For each bag in \(_{o}\), we similarly extract bags of instance representations as \(_{o}\).

For each bag \(_{o}\), we find its nearest neighbor in \(_{r}\) using the following OT problem to calculate distance between the two bags \(\) and \(}_{r}\). Let \(=_{i=1}^{K}a_{i}_{_{i}}\) and \(=_{j=1}^{K}_{j}_{}_{j}}\) be discrete distributions supported on \(=\{_{k}\}_{k=1}^{K}\) and \(}=\{}_{k}\}_{k=1}^{K}\), where \(_{}=(-)\) is the translated Dirac delta function. Here, \(a_{i}\) and \(_{j}\) are the attention scores from the MIL model for instance \(i\) in bag \(\) and instance \(j\) in bag \(}\), respectively, forming the probability vectors for the bags. The OT problem between these two distributions can be formulated as follows:

\[d_{}(,)=_{(,})} _{i=1}^{K}_{j=1}^{}c(_{i},}_{j})T_{ij}\] (3)

In this equation, \(\) denotes the transport plan matrix where each element \(T_{ij}\) specifies the amount of "mass" to be transported from \(_{i}\) to \(}_{j}\). The function \(c(_{i},}_{j})\) is a cost function that quantifies the cost of transporting a unit of mass from \(_{i}\) to \(}_{j}\). A common choice of \(c(_{i},}_{j})\) is the squared \(l_{2}\) distance between the instance features, i.e., \(c(_{i},}_{j})=\|_{i}-}_{j}\|_{2}^{2}\). \((,})\) represents the set of all matrices \(\) that satisfy the marginals \(^{}_{K}=\) and \(_{}=}\). Here, \(_{n}\) and \(_{m}\) are vectors of ones. We also impose an entropy regularization  term to reduce the sensitivity to outlier instances. Specifically, we solve the following OT problem with a Sinkhorn's algorithm :

\[d_{}(,)=_{(, {})}_{i=1}^{K}_{j=1}^{}c(_{i},}_{j} )T_{ij}+_{ij}T_{ij} T_{ij}\] (4) \[s.t.,^{}_{K}=,_{}= }, 0\]

In the context of the retrieval-based MIL problem, the nearest neighbor bag for a given bag \(\) would be the one \(}\) in the retrieval set that results in the smallest OT cost with \(\):

\[^{*}=*{arg\,min}_{}_{m}_{r}}d_{OT} (,_{m})\] (5)

In the retrieval process, we use an MIL model pre-trained on \(_{o}\) to extract the attention vectors \(\) and \(}\), which serve as mass probability distributions for calculating the OT, in addition to extracting bag representations \(\) and \(}\). This method effectively transforms the instance importance, as determined by the model, into a mass probability distribution. Instances with high attention scores are considered to possess larger "mass". In the context of the OT problem, such instances become more "expensive" to transport, thereby significantly influencing the computation of the transport cost. Consequently, the model is encouraged to pair similar instances carrying substantial mass, a strategy that aligns with the intuitive notion of matching similar instances together. Employing the pre-extracted attention to classify the weighted bag representation after retrieval ensures consistency with the retrieval process and enhances efficiency. The whole retrieval-enhanced multiple instance learning is shown in Algo. 1.

### Unsupervised Domain Adaptation

The proposed methodology can be effectively employed in the context of unsupervised domain adaptation (UDA) . UDA seeks to leverage labeled data from a source domain to train a model that can perform well on an unlabeled target domain.

Remarkably, our retrieval algorithm (as described in Algo. 1) can be seamlessly applied to the UDA task. To achieve this, we set \(_{o}\) to represent the source domain and \(_{r}\) to represent the target domain. Our approach retrieves the nearest neighbor bag representation from the target domain for each bag representation in the source domain. Subsequently, we train the final classifier using the merged representation and the source domain label.

The RAM-MIL based on OT allows leveraging the geometric structure of the instance space across domains. This promotes the alignment of the source and target domains at the instance level. By merging the bag features, the discriminative information embedded in the source domain is transferred to the target domain. As a result, the bag classifier is able to discriminate the target domain bags without needing access to their labels.

### Reduction of Intrinsic Dimension

In this section, we validate our hypothesis that retrieval can effectively reduce the intrinsic dimension of each bag's distribution. We use the two NN estimator  for computing the intrinsic dimension for the bag representations, either the original features for MIL or merged features for retrieval models. Figure 3 shows the intrinsic dimension of CLAM , Manifold Mixup , OT based retrieval and different variants of retrieval methods. The \(l_{2}\) distance based retrieval, approximate OT retrieval and Hausdorff distance based retrieval are variants of our retrieval method that will be elaborated in Section 5.

The figure shows that our proposed retrieval methods indeed lower the intrinsic dimensions of bag representations in both in-domain and out-of-domain settings. As shown in (2), the approximation performance of a ground-truth set scoring function is theoretically constrained by the intrinsic dimension of the distribution, when measured under the Wasserstein distance. RAM-MIL delivers the lowest intrinsic dimensions among the methods compared, thus validating its superior performance in classification and aligns with our theoretical analysis (2). We offer more detailed discussion of intrinsic dimension in Section 5.2.

## 4 Related works

**Attention-based multiple instance learning** ABMIL  first introduced the attention mechanism for MIL to an interpretation of importance for instances. Building on this, DSMIL  used contrastive learning for feature extraction and established global connections between instance attentions. TransMIL  took this a step further and developed a correlated MIL, employing multi-head self-attention and spatial information encoding for thorough global correlation. CLAM  extends ABMIL to the case of multiple classes and builds an integrated toolbox for visualizing the uncertainty. DTFD-MIL  proposes a two-stage feature distillation MIL framework for enhancing the performance. Bayes-MIL  studies the fundamental interpretability problem in the attention based MIL framework and proposed to address it with a probabilistic method. In our RAM-MIL, the attention weight is utilized as a measure of the probability density, indicating the amount of "mass" being moved from the source bag to the target bag. Greater attention weights in RAM-MIL indicate more significant relationships between instances in the bags being compared.

**Domain adaptation for medical imaging** There have been a large volume of research studying the domain adaptation in medical imaging [24; 25; 26]. However, most of the preceding studies on whole slide images domain generalization have primarily focused on instance-level classification [27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39], rather than tackling the more complex challenge of weakly-supervised bag-level classification (MIL). In contrast, our approach seeks to address this more complex problem of unsupervised domain adaptation for MIL. We review these methods in details and make extensive comparisons in the supplemental.

**Retrieval method for whole slide images** Recent works also study how to efficiently retrieve relative WSI from database. Yottixel  builds a search engine for indexing WSIs at scale. SISH  uses a tree structure for fast search of WSI followed by an uncertainty-based ranking algorithm for retrieval. In experiment, we integrate the open-sourced SISH as a search engine into our RAM-MIL pipeline for comparisons (See supplemental). HHOT  proposes to use optimal transport (OT) as a distance measure to compare different WSIs, or different WSI datasets. Our paper, focusing on classification tasks, studies the principle why OT is suitable for WSI classification and propose a retrieval-based classification process. We compare RAM-MIL and HHOT in our experiments.

## 5 Experiments

The proposed methodology is evaluated on whole slide image (WSI) datasets (Camelyon16 [43; 13], Camelyon17 , TCGA-NSCLC, CPTAC-UCEC and CPTAC-LSCC) and general MIL datasets (See results in supplemental). The evaluation is divided into in-domain and UDA settings. For the in-domain setting, the retrieval set \(_{r}\) is the union of training and validation set of \(_{o}\). For the UDA setting, the retrieval set is a held-out dataset with no labels involved in training or inference.

Figure 3: The intrinsic dimension of bag representations for different MIL methods and retrieval models. CLAM uses the original feature and the rest use merged feature from retrieval. RAM-MIL is the most effective method at reducing intrinsic dimension of feature space.

We compare our OT-based retrieval with Euclidean distance retrieval, Hausdorff distance retrieval and Manifold MixUp as baselines. The Euclidean (\(l_{2}\) norm) distance \(d_{l_{2}}(,)\) and Hausdorff distance based retrieval are implemented in the following ways:

* **Direct bag representation retrieval.** As a baseline method, we propose an approach to augment bag features by directly retrieving the nearest neighbor. The setup follows the initial steps of Algo. 1, from line \(1\) to line \(3\). Instead of computing the instance correlations, we directly compute the \(l_{2}\) distance between bag representations, formulated as \(d_{l_{2}}(_{n},}_{m})=||_{n}-}_{m}||_{2}^ {2}\). For the \(n\)-th bag representation, we select the nearest neighbor from the retrieval set: \(}_{m^{*}}=*{arg\,min}_{}_{n}_{r}}d_{l_{2}}(_{n},}_{m})\). The merged bag representation \(}_{n}=(_{n},_{m^{*}})\) is subsequently employed for bag classification, where \(m^{*}=*{getIndex}(}_{m^{*}})\).
* **Approximate OT retrieval.** Recall that prior to applying the Sinkhorn's algorithm to solve OT, it is necessary to compute the cost matrix \(C=[c(_{i},}_{j})]_{ij}=\|_{i}-}_{j}\|_{2} ^{2}\), where each element is the squared \(l_{2}\) distance between instance representations. To highlight the significance of solving the OT problem, we introduce three simple approximations of OT by directly employing the minimum, average and maximum values of the cost matrix. The \(d_{}(_{n},}_{m})=_{ij}[c(_{i}, {}_{j})]_{ij}\) focuses on the closest and most similar instances between bags. The \(d_{}(_{n},}_{m})=*{average}_{ij} [c(_{i},}_{j})]_{ij}\) offers a holistic measure of bag similarity but might lose some finer details. The \(d_{}(_{n},}_{m})=*{max}_{ij}[c( _{i},}_{j})]_{ij}\) focuses on the most dissimilar instances between the two bags. These three measures can be considered as OT approximations that use information from the squared \(l_{2}\) cost matrix.
* **Hausdorff distance retrieval.** The symmetric Hausdorff distance is employed for retrieval, computed as \(d_{}(_{n},}_{m})=\{d_{}(_{n},}_{m}),d_{}(}_{m},_{n})\}\). Here, \(d_{}(_{n},}_{m})=_{_{i}_{n}} _{}_{j}}_{m}}\|_{i}-}_{j} \|_{2}^{2}\) is the directed Hausdorff distance. The Hausdorff distance is an appropriate retrieval metric when assessing the maximum dissimilarity between two sets, emphasizing the instances that differ the most between the sets. In comparison, OT provides a more comprehensive and versatile choice for retrieval tasks, given its ability to consider all possible matchings between elements in the sets and finds an optimal one.

### Classification of Whole Slide Image

Experimental setup:In this section, we validate the methodology within both in-domain and out-of-domain contexts. In both contexts, the training set is from Camelyon16. In the in-domain setting, we evaluate our models using the Camelyon16 dataset, where the combination of training and validation sets is treated as \(_{r}\), which is denoted as \(_{}\). For the out-of-domain context, we use Camelyon16 as \(_{o}\). As for \(_{r}\), we consider two configurations: 1) retrieval from the training and validation sets of both Camelyon16 and Camelyon17, and 2) retrieval solely from the training

    &  &  \\  & AUC & Accuracy & AUC & Accuracy \\  ABMIL & 0.9010\(\)0.026 & 0.8750\(\)0.020 & 0.7287\(\)0.035 & 0.7190\(\)0.049 \\ DSMIL & 0.8944\(\)0.051 & 0.8682\(\)0.060 & - & - \\ CLAM & 0.9177\(\)0.044 & 0.8650\(\)0.060 & 0.7613\(\)0.054 & 0.7214\(\)0.044 \\ TransMIL & 0.9307\(\)0.024 & 0.8837\(\)0.041 & 0.5697\(\)0.118 & 0.6451\(\)0.100 \\ Bayes-MIL & 0.9432\(\)0.049 & 0.8875\(\)0.052 & 0.7839\(\)0.044 & 0.7435\(\)0.058 \\  HHOT-NN (k=1) & 0.7007\(\)0.035 & 0.7318\(\)0.051 & 0.6939\(\)0.076 & 0.7173\(\)0.061 \\ HHOT-NN (k=3) & 0.7618\(\)0.026 & 0.7544\(\)0.055 & 0.7263\(\)0.048 & 0.7523\(\)0.076 \\  Mixup Retr\({}_{}\) & 0.9260\(\)0.051 & 0.8825\(\)0.051 & - & - \\ Mixup Retr\({}_{O}\) & 0.9271\(\)0.048 & 0.8850\(\)0.046 & 0.7658\(\)0.052 & 0.7594\(\)0.044 \\ Mixup Retr\({}_{}\) & 0.9271\(\)0.045 & 0.8825\(\)0.048 & 0.7641\(\)0.056 & 0.7593\(\)0.058 \\  \(l_{2}\) Retr\({}_{}\) & 0.9281\(\)0.047 & 0.8800\(\)0.055 & - & - \\ \(l_{2}\) Retr\({}_{O}\) & 0.9241\(\)0.048 & 0.8950\(\)0.048 & 0.7627\(\)0.055 & 0.7353\(\)0.051 \\ \(l_{2}\) Retr\({}_{}\) & 0.9398\(\)0.045 & 0.8950\(\)0.055 & 0.7738\(\)0.050 & 0.7493\(\)0.050 \\  RAM-MIL Retr\({}_{}\) & **0.9451\(\)0.036** & 0.8925\(\)0.050 & - & - \\ RAM-MIL Retr\({}_{O}\) & 0.9365\(\)0.052 & **0.9200\(\)0.050** & **0.7974\(\)0.054** & 0.7433\(\)0.073 \\ RAM-MIL Retr\({}_{O}\) & 0.9419\(\)0.048 & 0.9175\(\)0.051 & 0.7681\(\)0.058 & **0.7795\(\)0.021** \\   

Table 1: Results on CAMELYON16 and CAMELYON17 for in-domain classification and unsupervised domain adaptation, for comparisons with related methods.

and validation sets of Camelyon17, referred to as \(Retr_{IO}\) and \(Retr_{O}\), respectively. In the Camelyon17 challenge, each slide is labeled as either isolated tumour cells (ITC), micro-met, macro-met, or negative. ITC are strictly defined as single tumor cells or clusters smaller than 0.2 mm or less than 200 cells . Following the principles of MIL, we treated ITC as the positive class. More results about treating ITC as the negative class are shown in the supplemental. To ensure robustness and avoid the influence of outliers, each experiment is executed 10 times on randomly partitioned train/validation/test sets.

The baseline model we use for comparison is CLAM, an extension of attention-based Multiple Instance Learning (MIL) that incorporates an instance clustering loss. In our case, the pre-trained model is a CLAM model for binary classification. We also make comparisons with other models such as ABMIL, DSMIL, TransMIL, Bayes-MIL, HHOT, and Mixup. For the ablation study, we add our proposed \(l_{2}\) retrieval method with direct bag representation retrieval. For both the Mixup and \(l_{2}\) retrieval models, we ensure that the source of the augmented data aligns with our in-domain and out-of-domain settings. For the OT-based pathology work, HHOT focuses on comparing whole slide images using uniform weight-based OT and employs kNN as a discriminative method. Note that Mixup and our proposed methods use CLAM as the MIL method. Furthermore, Mixup shares the same merging function with our retrieval method, which is a simple addition or a convex combination, in the reported experiments. Our method does not need the ground-truth label information for retrieval set while Mixup uses that for the in-domain experiments. More results about the merging function is shown in the supplemental.

When we compute the RAM-MIL, we often deal with very large WSIs that can contain over 100,000 instances. This makes the calculation of OT time-consuming. To speed things up, we first perform pre-selection of potentially important instances. Instead of using all the instances in a bag \(_{n}\), we pick the top 20% of instances based on their attention scores and normalized the attention scores. We provide more detailed comparisons in percentage of instances used, in the supplemental. In this way, we are focusing on the most important instances and making the whole process more efficient.

**In-domain comparisons:** Table 1 presents the in-domain results. Simply using a uniform weight-based Optimal Transport (OT) in HHOT-KNN fails to deliver satisfactory performance. Mixup, \(l_{2}\) retrieval, and RAM-MIL all enhance the classification performance for in-domain data beyond the CLAM baseline. The straightforward \(l_{2}\) retrieval-based MIL outpaces Mixup, demonstrating that selecting the nearest neighbor using a basic metric offers superior augmentation compared to random data selection. Using OT as a distance metric, the retrieval-based MIL method surpasses all the other compared methods for the in-domain dataset. Notably, the \(_{}\) configuration, which employs the most extensive data set for retrieval (CAMELYON16 and CAMELYON17), offers the most significant boost in in-domain classification accuracy. Table 2 shows RAM-MIL consistently presents higher AUC and accuracy on CPTAC-UCEC. We also explore the performance of subtypes classification in Table 3, showing that RAM-MIL outperforms existing methods in accuracy, while providing AUC higher than the CLAM baseline. Compared with the results on tumor stage classification, our method enhances the model performance as shown in Table 4.

**Out-of-domain comparisons:** The right-hand side of Table 1 showcases the results for the out-of-domain setting. Notably, all retrieval methods, including ours, enhance the classification performance

   Method & AUC & Accuracy \\  CLAM & 0.7803 & 0.60 \\ Bayes-MIL & 0.8070 & 0.64 \\  RAM-MIL & **0.8139** & **0.65** \\   

Table 4: Tumor stage classification results on CAMELYON17.

    &  &  \\  & AUC & Accuracy & AUC & Accuracy \\  CLAM & 0.9500 & 0.9285 & 0.4986 & 0.6521 \\ RAM-MIL & **0.9667** & **0.9382** & **0.6056** & **0.6526** \\   

Table 2: Results on CPTAC-UCEC in domain classification, domain adaptation from CPTAC-LSCC to CPTAC-UCEC.

   Method & AUC & Accuracy \\  CLAM & 0.9420 & 0.8640 \\ Scaling ViT & 0.9516 & 0.8821 \\ TransMIL & **0.9603** & 0.8835 \\ Bayes-MIL & 0.9451 & 0.8965 \\  RAM-MIL & 0.9457 & **0.8988** \\   

Table 3: Results on TCGA-NSCLC (subtyping).

for out-of-domain data, without accessing its labels. This suggests that retrieval benefits from the transfer of representative information from the source to the target domain via the merging of bag features. The \(l_{2}\) retrieval, while trailing Mixup in terms of accuracy, surpasses it in terms of AUC. Our OT-based retrieval achieves the highest performance across both AUC and accuracy metrics. Table 2 presents the results on domain adaptation from CPTAC-UCEC to CPTAC-LSCC. RAM-MIL demonstrates a notable improvement of 0.1065 in AUC compared to CLAM. The underlying reason for the good performance might be it could consider all possible matchings between elements in the sets and finds an optimal one.

**Ablation study** Table 5 presents the ablation study using different retrieval methods, which shows that even \(l_{2}\) retrieval, which operates without considering instance-level distances, surpasses the performance of the three OT approximation methods in both in-domain and out-of-domain settings. Among these, only the Approx-OT-min, when used for in-domain data retrieval, achieves a performance comparable to that of \(l_{2}\) retrieval. This underscores the fact that retlying on simple statistics of the cost matrix cannot provide a robust and consistent approximation of the optimal transport. This outcome reinforces the necessity of solving the complete optimal transport problem, using Sinkhorn's algorithm, to attain reliable results.

### Dimensionality Reduction

We next show that retrieval-based methods can effectively reduce the intrinsic dimension. Figure 3 shows the intrinsic dimension for various methods. The proposed RAM-MIL based on optimal transport delivers the lowest intrinsic dimension, among all methods, both retrieval and vanilla models. Interestingly, Mixup seems to increase the intrinsic dimension, suggesting that random selection and merging of bag representations do not necessarily lead to a more compact representation. This could explain why Mixup only offers marginal improvements.

Figure 3 shows that while the CLAM model has an intrinsic dimension between 5-7, RAM-MIL achieves a dimension between 2-4. Based on these observations, we choose dimension 5 to guide manual dimensionality reduction. The objectives of this exercise are two-fold: firstly, to validate that intrinsic dimension can accurately explain performance, and secondly, to illustrate that simple methods like SVD are not necessarily beneficial for dimensionality reduction of bag representations.

In our experiment, we employ SVD to reduce each bag representation of CLAM and each merged bag representation of RAM-MIL to a 5-dimensional vector. Then, we use these vectors and the bag labels to train a single logistic regression classifier. The results indicate that RAM-MIL significantly outperforms CLAM in classification accuracy. This experiment shows that a simple dimensionality reduction method like SVD does not significantly improve the performance of the CLAM baseline.

    &  &  \\  & AUC & Accuracy & AUC & Accuracy \\  Approx-OT-min Ret\({}_{}\) & 0.9301\(\)0.033 & 0.8800\(\)0.043 & - & - \\ Approx-OT-min Ret\({}_{}\) & 0.8601\(\)0.080 & 0.8000\(\)0.062 & 0.7640\(\)0.058 & 0.7353\(\)0.043 \\ Approx-OT-min Ret\({}_{}\) & 0.9196\(\)0.063 & 0.8750\(\)0.056 & 0.7680\(\)0.065 & 0.7274\(\)0.046 \\  Approx-OT-avg Ret\({}_{}\) & 0.9227\(\)0.038 & 0.8600\(\)0.036 & - & - \\ Approx-OT-avg Ret\({}_{}\) & 0.8865\(\)0.063 & 0.8425\(\)0.061 & 0.7625\(\)0.060 & 0.7174\(\)0.041 \\ Approx-OT-avg Ret\({}_{}\) & 0.9195\(\)0.063 & 0.8750\(\)0.056 & 0.7641\(\)0.054 & 0.7374\(\)0.061 \\  Approx-OT-max Ret\({}_{}\) & 0.9174\(\)0.049 & 0.8625\(\)0.044 & - & - \\ Approx-OT-max Ret\({}_{}\) & 0.9182\(\)0.047 & 0.8650\(\)0.048 & 0.7576\(\)0.060 & 0.7214\(\)0.062 \\ Approx-OT-max Ret\({}_{}\) & 0.9195\(\)0.063 & 0.8750\(\)0.056 & 0.7681\(\)0.065 & 0.7414\(\)0.038 \\  \(l_{2}\) Ret\({}_{}\) & 0.9281\(\)0.047 & 0.8800\(\)0.055 & - & - \\ \(l_{2}\) Ret\({}_{}\) & 0.9241\(\)0.048 & 0.8950\(\)0.048 & 0.7627\(\)0.055 & 0.7353\(\)0.051 \\ \(l_{2}\) Ret\({}_{}\) & 0.9398\(\)0.045 & 0.8950\(\)0.055 & 0.7738\(\)0.050 & 0.7493\(\)0.050 \\  Hausdorff Ret\({}_{}\) & 0.9273\(\)0.057 & 0.8850\(\)0.049 & - & - \\ Hausdorff Ret\({}_{}\) & 0.9226\(\)0.055 & 0.8725\(\)0.060 & 0.7651\(\)0.054 & 0.7434\(\)0.051 \\ Hausdorff Ret\({}_{}\) & 0.9322\(\)0.046 & 0.8950\(\)0.048 & 0.7695\(\)0.056 & 0.7513\(\)0.051 \\  RAM-MIL Ret\({}_{}\) & **0.9451\(\)0.036** & 0.8925\(\)0.050 & - & - \\ RAM-MIL Ret\({}_{}\) & 0.9365\(\)0.052 & **0.9200\(\)0.050** & **0.7974\(\)0.054** & 0.7433\(\)0.073 \\ RAM-MIL Ret\({}_{}\) & 0.9419\(\)0.048 & 0.9175\(\)0.051 & 0.7681\(\)0.058 & **0.7795\(\)0.021** \\   

Table 5: Ablation study evaluated on CAMELYON16 and CAMELYON17.

However, RAM-MIL, which organizes the latent space by merging nearest neighbor features and then applies SVD, can achieve a commendable performance.

### Interpretability with OT for MIL

We show that by using optimal transport retrieval, a novel method for highlighting the important instances could be derived. For a pair of source bag \(\) and target bag \(}\), solving the optimal transport problem generates the transport matrix \(=[T_{ij}]_{ij}\). Suppose that we are interested in the instance labels of the source bag, and suppose that the instance labels of the target bag are known. The positive instances in the source bag are simply the instances transported to the positive instances in the target bag, i.e., \(_{i}=_{j^{*}=_{j}T_{ij}}\). Here, \(\) is the predicted instance label for the source bag, and and \(\) is the true instance label in the target bag. Figure 4 shows this method could provide a good coverage of the positive area, without model training or accessing the bag label. In contrast to OT, retrieval-based methods utilizing other distance metrics do not possess this specific property.

## 6 Conclusion

In this work, we investigate the out-of-domain performance of multiple instance learning model, which is crucial for the ML-assisted medical diagnosis with the whole slide images. An optimal transport based retrieval method is proposed based on the understanding that there exists a negative correlation between MIL performance and input data dimension. The proposed RAM-MIL framework works for both in-domain and out-of-domain, meanwhile outperforms state-of-the-art MIL methods. We hypothesize that OT-based retrieval could reduce the intrinsic dimension therefore improves the MIL performance and validate the hypothesis by computing the intrinsic dimension of bag representations. The transportation matrix of OT distance provides a tool to visualize the correspondence between original and retrieval instances and makes our retrieval method interpretable. A limitation is the efficiency of optimal transport solver is constrained by the number of instances within a bag. Although we have mitigated this problem by selecting partial instances based on high attention values, we anticipate that a more principled method will be sought in future works.