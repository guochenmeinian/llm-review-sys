# Distributional Pareto-Optimal Multi-Objective Reinforcement Learning

Xin-Qiang Cai\({}^{1}\), Pushi Zhang\({}^{2}\), Li Zhao\({}^{2}\), Jiang Bian\({}^{2}\),

**Masashi Sugiyama\({}^{3,1}\) Ashley J. Llorens\({}^{2}\)**

\({}^{1}\) The University of Tokyo, Tokyo, Japan

\({}^{2}\) Microsoft Research Asia, Beijing, China

\({}^{3}\) RIKEN AIP, Tokyo, Japan

Equal contribution. Work done during an internship at Microsoft Research Asia.

###### Abstract

Multi-objective reinforcement learning (MORL) has been proposed to learn control policies over multiple competing objectives with each possible preference over returns. However, current MORL algorithms fail to account for distributional preferences over the multi-variate returns, which are particularly important in real-world scenarios such as autonomous driving. To address this issue, we extend the concept of Pareto-optimality in MORL into distributional Pareto-optimality, which captures the optimality of return distributions, rather than the expectations. Our proposed method, called Distributional Pareto-Optimal Multi-Objective Reinforcement Learning (DPMORL), is capable of learning distributional Pareto-optimal policies that balance multiple objectives while considering the return uncertainty. We evaluated our method on several benchmark problems and demonstrated its effectiveness in discovering distributional Pareto-optimal policies and satisfying diverse distributional preferences compared to existing MORL methods.

## 1 Introduction

Apart from most Reinforcement Learning (RL) works that consider the scalar reward of a task , Multi-Objective Reinforcement Learning (MORL) has recently received extensive attention due to its address at managing intricate decision-making issues with multiple conflicting objectives . In many multi-objective tasks, the relative preferences of users over different objectives are typically indeterminate a priori. Consequently, MORL's primary aim is to learn a variety of optimal policies under different preferences to approximate the Pareto frontier of optimal solutions. It has been demonstrated that MORL can significantly reduce the reliance on scalar reward design for objective combinations and dynamically adapt to the varying preferences of different users.

However, numerous real-world situations involve not only unknown relative preferences across multiple objectives, but also uncertain preferences in return distributions. These may include preference in risk , safety conditions , and non-linear user's utility . Consider, for instance, autonomous driving scenarios where agents need to strike a balance between safety and efficiency objectives. Different users might possess varying levels of risk tolerance. Some may demand high safety performance, tolerating lower expected efficiency, while others may seek a more balanced performance between safety and efficiency. Current MORL methods, by focusing exclusively on expected values with linear preferences, may not adequately capture multi-objective risk-sensitive preferences, hence being unable to deliver diverse policies catering to users with varied risk preferences.

In this work, we broaden the concept of Pareto-optimality in MORL to encompass distributional Pareto-optimality, which prioritizes the optimality of return distributions over mere expectations, asdepicted in Figure 1. From a theoretical perspective, we define Distributional Pareto-Optimal (DPO) policies, which capture the optimality of multivariate distributions through stochastic dominance [13; 14]. Distributional Pareto-Optimality delineates the set of policies with optimal return distributions, which we formally establish as an extension of Pareto-optimality in MORL.

On the practical side, we propose a novel method named _Distributional Pareto-Optimal Multi-Objective Reinforcement Learning_ (DPMORL)2, which aims to learn a set of DPO policies. We demonstrate that a policy achieving the highest expected utility for a given strictly increasing utility function is a DPO policy. To this end, we first learn diverse non-linear utility functions and then optimize policies under them . Experimental outcomes on several benchmark problems attest to DPMORL's effectiveness in optimizing policies that meet preferences on multivariate return distributions. We posit that our proposed method significantly addresses the challenge of managing multiple conflicting objectives with unknown preferences on multivariate return distributions in complex decision-making situations.

Our main contributions are listed as follows:

1. We introduce the concept of Distributional Pareto-Optimal (DPO) policies. This expands the notion of Pareto-optimality in MORL to include preferences over the entire distribution of returns, not just their expected values. This enables agents to express more nuanced preferences over policies and align their decisions more closely with their actual objectives.
2. We propose DPMORL, a new algorithm for learning DPO policies under non-linear utility functions. This algorithm accommodates a broad range of distributional preferences, thus offering a more flexible and expressive approach to MORL.
3. We execute extensive experiments on various MORL tasks, demonstrating the effectiveness of our approach in learning DPO policies. Our findings show that our algorithm consistently surpasses existing MORL methods in terms of optimizing policies for multivariate expected and distributional preferences, underscoring its practical benefits.

## 2 Related Work

**Multi-Objective Reinforcement Learning.** MORL has emerged as a vibrant research area in the reinforcement learning community owing to its adeptness in managing intricate decision-making scenarios involving multiple conflicting objectives [5; 6]. A plethora of MORL algorithms have been put forth in the literature. For instance,  proposed the utilization of Generalized Policy Improvement (GPI) to infer a set of policies, employing Optimistic Linear Support (OLS) for dynamic reward weight exploration . This GPI-based learning process was further enhanced by  through the incorporation of a world model to augment sample efficiency.  proposed an evolutionary approach to learn policies with diverse weights. However, our proposed approach diverges from these by leveraging utility functions to guide policy learning [18; 19]. The utility-based paradigm  accentuates the user's utility in decision-making problems, capitalizing on known information about the user's utility function and permissible policy types. Such methods scalarize the multi-dimensional objectives into a single reward function, enabling traditional RL algorithms to infer

Figure 1: The comparison of learning targets between the traditional MORL tasks and distributional MORL tasks, in which \(f_{1}\) and \(f_{2}\) are two (conflicting) objectives.

desirable policies, while also respecting axiomatic principles where necessary. Several studies have utilized non-linear utility functions to guide policy learning [20; 12], catering to more intricate preferences compared to linear ones but still in view of the expectation of the Pareto front. In contrast, our work extends the Pareto-optimality to distributional Pareto-optimality by introducing a set of non-linear utility functions with distributional attributes to guide policy learning. Some of the recent works [21; 22; 23] in MORL defines Distributional Undominated Set or ESR set which is both very close to Distributionally Pareto-Optimal policies in this paper. These works propose novel designs to enhance policy improvement in distributional reinforcement learning algorithms to find the Distributional Undominated Set. Compared to these works, we build novel theoretical results on the conditions of DPO policies and a novel algorithm (DPMORL) of learning DPO policies based on reward shaping that can be built on the top of any online RL algorithm.

**Distributional Reinforcement Learning.** Distributional RL extends traditional RL by modeling the entire distribution of returns, rather than just their expected values [24; 25]. This approach has been shown to improve both learning efficiency and policy quality in a variety of single-objective RL tasks [26; 27]. Key algorithms in this area include Categorical DQN (C51) , Quantile Regression DQN (QR-DQN) , and Distributional MPO . Despite the success of these distributional RL algorithms in single-objective settings, their direct extension to MORL has been limited due to the added complexity of handling multiple conflicting objectives and expressing preferences over the distribution of returns. On the other hand, the insights and techniques developed in distributional RL can provide a valuable foundation for incorporating distributional preferences into MORL, as demonstrated by our proposed approach.

**Risk-Sensitive and Safe Reinforcement Learning.** Risk-sensitive and safe RL are special cases of MORL that accentuate specific facets of reward distributions . Risk-sensitive RL primarily considers reward variance, striving to optimize policies that negotiate the trade-off between expected return and risk [28; 29; 30]. Conversely, safe RL prioritizes constraints on the agent's behavior, ensuring adherence to specified safety criteria throughout the learning process [31; 32; 33]. Some studies have proposed the integration of constraints into the state space, constructing new Markov Decision Processes . Others have explored the distributional aspects of rewards, investigating the implications of distributional RL on risk-sensitive and safety-oriented decision-making [26; 24]. However, our proposed setting is more general in terms of objectives, as it considers a broader range of user preferences and captures the entire distribution of rewards, rather than focusing solely on specific aspects such as risk, variance, or constraints. By extending MORL to incorporate distributional properties, our approach enables the learning of distributional Pareto-optimal policies that cater to diverse user preferences and offer better decision-making in a wide range of real-world applications.

## 3 Preliminaries: Multi-Objective Reinforcement Learning

Similar to the procedure of RL [34; 35], in MORL, the agent interacts with an environment modeled as a Multi-Objective Markov Decision Process (MOMDP) with multi-dimensional reward functions. A Multi-Objective MDP is defined by a tuple \((S,A,P,P_{0},,,T)\), where \(S\) is the state space, \(A\) is the action space, \(P\) is the state transition function, \(P_{0}(s)\) is the initial state distribution, \(:S A S^{K}\) is a vectored reward function and \(K\) is the number of objectives, \(\) is the discount factor, \(T\) is the total timesteps3. The goal of the agent is to learn a set of Pareto-optimal policies, which represent the best possible trade-offs among the different objectives.

One popular methodology for MORL problems is the utility-based method, which combines the multi-dimensional reward functions into a single scalar reward function using a weighted sum or another aggregation method . The intuition is to map the agent's preferences over different objectives to a scalar value for the agent training. Given a weight vector \(=(w_{1},w_{2},,w_{K})\), with \(w_{i}\) representing the importance of the \(i\)-th objective, the scalarized reward function is defined as \(r_{}(s,a)=_{i=1}^{K}w_{i}r_{i}(s,a)\). The agent then solves the scalarized MDP by optimizing its policy to maximize the expected scalar reward, using standard reinforcement learning algorithms like Q-learning or policy gradient methods. This approach can be straightforward to implement and has been shown to be effective in various MORL settings under expected preferences . However, such a linear combination of each dimension of the reward cannot deal with the preferences with distributional considerations.

[MISSING_PAGE_FAIL:4]

**Theorem 1**.: _If a policy \(\) has optimal expected utility under some non-decreasing utility function \(f\), where either utility function \(f\) is strictly increasing, or \(\) is the only optimal policy with greatest expected utility under utility function \(f\), then \(\) is a Distributionally Pareto-Optimal policy. Also, any Pareto-Optimal policy is a Distributionally Pareto-Optimal policy._

The proof of Theorem 1 is provided in Appendix A which utilizes a form of optimal transport theory for stochastic dominance. In practice, this theorem guarantees that the optimal policy of a strictly increasing non-linear utility function is a DPO policy, making it a suitable candidate for deployment in MORL problems. The strict increasing property of utility function \(f\) in Theorem 1 is crucial in the theoretical analysis for proving the distributional pareto-optimality. In the next section, we base on the result of this theorem to find optimal policies for a diverse set of utility functions, in order to learn the set of DPO policies. This result also shows that our definition of Distributional Pareto-Optimal policies is an extension of Pareto-optimal policies, allowing for a more diverse set of optimal policies to be captured.

We also formally prove that optimal policies for risk-sensitive and safe constraint objectives belong to the set of Distributional Pareto-Optimal policies. This proves that DPO policies can successfully cover policies with diverse distributional preferences. The detailed proof is provided by Theorem 3 in Appendix A.

### Distributionally Pareto-Optimal Multi-Objective Reinforcement Learning

We now present our proposed algorithm, Distributionally Pareto-Optimal Multi-Objective Reinforcement Learning (DPMORL). The main idea of DPMORL is to learn a set of non-linear utility functions that can guide the agent to discover distributional Pareto-optimal policies. The algorithm proceeds in two stages: (1) generating the utility functions and (2) training the policies.

#### 4.2.1 Utility Function Generation with Diversity-based Objective

The first component of our algorithm focuses on generating a diverse set of plausible utility functions, upon which we find the optimal policies to find a diverse set of optimal policies. This is essential to ensure our method can accommodate various distributional preferences and adapt to different problem settings. To achieve this, we propose (1) Non-decreasing Neural Network for parameterizing a diverse set of non-linear utility functions (2) an objective function of minimum distance which encourages generating a diverse set of utility functions.

Non-decreasing Neural Network.We employ non-decreasing neural network to parameterize the utility function. The use of a neural network allows us to represent complex, non-linear, and arbitrary continuous utility functions, while the non-decreasing constraint ensures that the utility function satisfies the desired properties for multi-objective problems. We ensure the non-decreasing property in neural networks by constraining the weight matrix to be non-negative and the activation function to be non-decreasing following existing work in convex neural networks  and QMIX , which can approximate any multivariate non-decreasing function with arbitrary small errors . The implementation details of the Non-decreasing Neural Network are provided in Appendix B.

Diversity-based Objective Function.We propose an objective function based on diversity for learning a diverse set of plausible utility functions. Specifically, we define \(f_{_{1}},f_{_{2}},,f_{_{M}}\) to be the set of candidate utility functions, parameterized by \(_{1},,_{M}\). For any given utility function \(f_{_{i}}\), the learning objective is defined as

\[J^{}(_{i}) =_{j i}_{(^{K})}[f_{ _{i}}()-f_{_{j}}()]^{2}\] (1) \[J^{}(_{i}) =_{j i}_{_{1},_{2}([ 0,1]^{K})}[}(_{2})-f_{_{i}}(_{1})}{ \|_{2}-_{1}\|}-}(_{2})-f_{_{j}}( _{1})}{\|_{2}-_{1}\|}]^{2}\] (2) \[J(_{i}) = J^{}(_{i})+(1-)J^{ }(_{i})\] (3)

Optimizing this objective function can encourage diversity among the generated utility functions within the normalized range \(^{K}\) in both value and derivative, leading to more comprehensive coverage of potential user preferences. We minimize the objective function by gradient descent on non-decreasing neural networks to generate a set of \(N\) utility functions. To make the learned utility function strictly increasing for satisfying the constraints in Theorem 1, we add an additional term to the learned utility function as \(_{_{i}}()=f_{_{i}}()+_{k =1}^{K}z_{k}\) in our experiment with \(=0.01\), and use \(_{_{i}}()\), \(i=1,,M\) as the strictly increasing utility function to learn DPO policies.

#### 4.2.2 Optimizing Policy with Utility-based Reinforcement Learning

Once we have generated a diverse set of utility functions, the second component of our algorithm focuses on optimizing policies to maximize the expected utility. This process, which we call utility-based RL, leverages the generated utility functions to guide the optimization of policies. By focusing on the expected utility, our method can efficiently balance the trade-offs between multiple objectives and distributions, ultimately generating policies that are more likely to align with user preferences.

We show that the following utility-based reinforcement learning algorithm can effectively optimize the policy with respect to a given utility function.

```
0: policy \(\), an environment \(=(S,A,P,P_{0},,,T)\), utility function \(f\)
0: new policy \(^{}\)
1: Augment state space with \(}=\), where \(\) is the space of cumulative multivariate returns.
2: Let transition \(_{0}()\) and \((|(s_{t},_{t}),a_{t})\) with \(s_{0} P(s_{0}),z_{0}=0\), and \(s_{t+1} P(|s_{t},a_{t})\), \(_{t+1}=_{t}+^{t}_{t}\).
3: Let scalar reward function \(R\) as \(R((s_{t},_{t}),a_{t},(s_{t+1},_{t+1}))=^{-t}[f(_{t+1}) -f(_{t})]\).
4: Optimize policy \(\) under environment \(}=(,A,,_{0},R,,T)\) under off-the-shelf RL algorithm (such as PPO or SAC) to \(^{}\). ```

**Algorithm 1** Utility-based Reinforcement Learning

Briefly, Algorithm 1 augments the state space with the cumulative multi-objective returns, and transforms the multi-dimensional rewards into a scalar reward by the difference in the utility function \(f\). The following result shows that the new scalar-reward environment \(\) generated by Algorithm 1 has the same optimal policy as the optimal policy under utility function \(f\):

**Theorem 2**.: _The optimal policy \(^{*}\) under environment \(}=(,A,,_{0},R,,T)\), with scalar reward function_

\[R((s_{t},_{t}),a_{t},(s_{t+1},_{t+1}))=^{-t}[f(_{t+1}) -f(_{t})]\]

_is the optimal policy under the utility function \(f\), i.e._

\[_{(^{*})}[f()]=_{} _{()}[f()].\]

An advantage of Algorithm 1 is that we can directly utilize off-the-shelf RL algorithms, such as PPO  and SAC  without any modification, which makes the algorithm easy to implement using widespread existing implementations of online RL algorithms.

It is also important to note that our algorithm simplifies to optimizing the weighted sum of rewards in MORL when the utility function is linear. This implies that our method is a generalization of the linear utility function MORL approaches by accommodating a wide range of non-linear utility functions. This flexibility makes our algorithm particularly suited for problems where the user's preferences may not be adequately captured by a linear utility function.

In the next experimental section, DPMORL only undergoes a single iteration: we initially generate a set of \(N\) utility functions as per the methodology detailed in Section 4.2.1. Subsequently, it optimizes a set of \(N\) policies using these generated utility functions, as outlined in Section 4.2.2.

## 5 Experiments

In this section, we conducted several experiments under the setting of MORL. Through the experiments, we want to investigate the following questions:

1. Can Utility-based RL effectively learn policies with diverse distributional preferences?
2. Can DPMORL generate a set of diverse non-linear utility functions?
3. Can DPMORL obtain promising performance compared with state-of-the-art MORL methods in view of expected and distributional preferences?

### Case Study of Utility Functions

To answer the first question, we train policies with a diverse set of utility functions on DiverseGoal, DeepSeaTreasure and Reacher. Here, we focus on showing the effectiveness of the Utility-based RL algorithm (Algorithm 1). We select several different non-linear functions \(f\), each in favor of distributions at one target. We train policy \(_{i}\) with utility function \(f_{i}\) with Algorithm 1 for each target, and show the policy's trajectory and return samples.

DiverseGoal is a MORL environment in Figure 3 with multiple goals that the agent can take multiple steps to reach, where each goal has its unique reward function. Upon reaching a particular goal, the agent secures a 2D reward. This reward is sampled from the specific normal distribution associated with that goal, as illustrated in Figure 2(b). Conversely, if the agent reaches the boundary of the map, it incurs a negative 2-dimensional reward. The environment requires the agent to navigate the trade-offs between various reward distributions, underscoring the complexity and nuance inherent to the task.

In this study, we investigate if the learned policy can reach the goal that yields the highest expected utility under the utility function. The results are illustrated in Figure 4. Under different types of utility functions and return distributions, the utility-based RL algorithm is able to find the optimal policy with the highest expected utility, which shows the effectiveness of the utility-based RL algorithm.

DeepSeaTreasure and Reacher are among the most widely used MORL environments. Under the DeepSeaTreasure environment and Reacher environment, we show the return distributions of the policies optimized with utility-based RL under different utility functions. In Figure 5 and Figure 6, we use utility functions generated by DPMORL in Section 4.2.1 and a set of manually constructed utility functions for different types of distributional preferences respectively. From the result in Figure 5 and Figure 6, we can see that our algorithm effectively finds policies that maximize the expected utility for different non-linear utility functions. Meanwhile, the DeepSeaTreasure environment aligns well

Figure 4: The case study results of DPMORL under DiverseGoal environment.

Figure 3: Illustrations of the DiverseGoal environment.

with safe RL settings, where the two returns are time penalty and task reward respectively. With our method, users can provide different utility functions derived from safety constraints, and optimize the expected utility via DPMORL for obtaining policies that satisfy the constraint.

### Main Experiment

In this section, we illustrate the performance of DPMORL on five environments based on MO-Gymnasium  to answer the latter two questions.

**Environments.** We conducted experiments across five environments based on MO-Gymnasium  to evaluate the performance of our proposed method, DPMORL. These environments represent a diverse range of tasks, from simple toy problems to more complex continuous control tasks, and cover various aspects of multi-objective reinforcement learning:

* DeepSeaTreasure: A classic MORL benchmark that requires exploration in a gridworld to find treasures with different values and depths.
* FruitTree: A multi-objective variant of the classic gridworld problem, where an agent has to collect different types of fruits with varying rewards and penalties.
* HalCheetah, Hopper, MountainCar: Three continuous control tasks that require controlling different agents for task solving and minimizing energy usage.

More details about the environments are gathered in Appendix C.1.

**Baselines.** We compare DPMORL with four state-of-the-art baselines in the context of distributional preferences of MORL: Optimistic Linear Support (**OLS**) [47; 16]; Prediction-Guided Multi-Objective Reinforcement Learning (**PGMORL**) ; Generalized Policy Improvement with Linear Support (**GPI-LS**) ; Generalized Policy Improvement with Prioritized Dyna (**GPI-PD**) .

**Training Details.** For all methods, including DPMORL and the baselines, each policy was trained for \(1 10^{7}\) steps. We learn a set of \(N=20\) policies for DPMORL and all of the baselines to ensure a fair comparison. Finally, we use the learned \(N=20\) policies in each method for evaluations.

Figure 5: The case study results of DPMORL under DeepSeaTreasure environment.

Figure 6: The case study results of DPMORL under Reacher environment.

**Implementation Details.** We use PPO algorithms implemented in Stable Baselines 3  in Algorithm 1. We use a normalization technique by linearly mapping the return into scale \(^{K}\) without modifying the optimal policies. Detailed implementations are provided in Appendix B.

**Evaluation Metrics.** To thoroughly evaluate the performance of DPMORL and compare it with the baseline methods, we employed four distinct metrics. These comprise two existing MORL metrics, HyperVolume and Expected Utility, in addition to two novel metrics developed specifically for assessing distributional preference, i.e., Constraint Satisfaction and Variance Objective. The latter two are designed to underscore the optimality of multivariate distributions associated with the learned policies. In terms of Constraint Satisfaction, we randomly generate \(M=100\) constraints for the policy set produced. The Constraint Satisfaction metric is then computed by considering the highest probability within the policy set that satisfies each individual constraint. The Variance Objective metric, on the other hand, involves generating \(M=100\) random linear weights. These weights are applied to both the expected returns and the standard deviations of returns in each dimension. This objective encourages attaining greater expected returns while simultaneously reducing variance, thereby catering to dynamic preferences. Further details about the implementation of these evaluation metrics are provided in Appendix C.2 and Appendix C.3.

### Results

**Generation of Utility Functions.** In accordance with the methodology detailed in Section 4.2.1, we employ non-decreasing neural networks in conjunction with diversity-focused objectives to generate a diverse assortment of utility functions. The generated functions are visually represented in Figure 7. The outcomes clearly indicate that optimizing diversity-based objective functions allows for generating a broad range of non-linear utility functions, thereby encompassing an expansive array of preferences with respect to returns. Subsequently, we utilize the first \(N=20\) utility functions depicted in Figure 7 to train an equivalent number of policies under DPMORL.

**Standard MORL Metrics.** The results under standard MORL metrics are shown in Table 1. Focusing on the Expected Utility, the performance of DPMORL is the highest in the Hopper, HalfCheetah, MountainCar, and DeepSeaTreasure environments (4/5), indicating that our method outperforms others in terms of expected utility. While for FruitTree environment, DPMORL also obtains consistent performance with the best contender, GPI-LS. In terms of the HyperVolume, DPMORL also performs the best in the Hopper, HalfCheetah, MountainCar, and FruitTree environments (4/5). The results show that DPMORL can yield better performance across both metrics and most environments (Hopper, HalfCheetah, MountainCar, and DeepSeaTreasure), meanwhile delivering robust and consistent results on the other one (FruitTree). Since PGMORL works on continuous action spaces, here we omit the results of PGMORL on environments with discrete action space, DeepSeaTreasure and FruitTree.

**Distributional MORL Metrics.** The results under distributional metrics are shown in Table 2. DPMORL outperforms other methods in most environments on both Constraint Satisfaction (5/5) and Variance Objective (3/5), indicating its strong ability to handle the distributional multi-objective reinforcement learning problem. For the rest environments, DPMORL also obtained comparable results compared with the best contender, GPI-LS. The results show that the policies learned by

Figure 7: Illustration of 2D utility functions learned by our methods in Section 4.2.1.

DPMORL have a higher probability of satisfying randomly generated constraints, and can better balance the trade-off between expectations and variances.

**Return Distributions of Learned Policies by DPMORL.** We provide visualizations of the multi-dimensional return distributions of each policy learned by DPMORL and four baseline methods (PGMORL, OLS, GPI-LS and GPI-PD) on different MORL environments, as shown in Appendix C.4. DPMORL's learned policy set demonstrates higher diversity with respect to return distributions compared to baseline methods.

**DPMORL on More Than 2-dimensional Return Space.** We run experiments with 3 objectives under Hopper and FruitTree environment, and show the learned return distribution and standard MORL evaluation metrics in Appendix C.5. The results demonstrate that DPMORL can effectively learn DPO policies under more than 3-dimensional reward functions, and achieves better performance in different MORL metrics compared to baseline methods under 3-dimensional reward functions.

**Ablation Analysis with Variable Number of Policies.** We provide the ablation studies of DPMORL with a varying amount of learned policies \(N\) in Appendix C.6. As the number of policies increases, both expected utility and hypervolume increase, which means DPMORL can indeed obtain better performance as the number of policies increases. On the other hand, DPMORL has obtained promising results when there exist only 5 policies, which verify the effectiveness of DPMORL in all environments.

## 6 Conclusion

In this work, we initialized the study of distributional MORL, specifically when preferences over different objectives and their return distributions are uncertain. We introduced the concept of Distributional Pareto-Optimal (DPO) policies with rigorous theoretical analysis, which extend the notion of Pareto-optimality in MORL to include preferences over the entire distribution of returns, not just their expected values. To obtain such desirable policies, we proposed a new algorithm, DPMORL, designed to learn DPO policies with non-linear utility functions. DPMORL allows for expressing a wide range of distributional preferences, providing a flexible and expressive approach to MORL. Experiment results showed that DPMORL consistently outperformed existing MORL methods in terms of optimizing policies for multivariate expected and distributional preferences.