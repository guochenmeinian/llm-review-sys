# Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution

Kai Liu\({}^{1,2,}\), Zhihang Fu\({}^{2}\), Sheng Jin\({}^{2}\), Chao Chen\({}^{2}\), Ze Chen\({}^{2}\),

**Rongxin Jiang\({}^{1}\)**, **Fan Zhou\({}^{1}\)**, **Yaowu Chen\({}^{1}\)**, **Jieping Ye\({}^{2}\)**

\({}^{1}\)Zhejiang University, \({}^{2}\)Alibaba Cloud

Work done during Kai Liu's research internship at Alibaba Cloud. Email: kail@zju.edu.cn.Corresponding authors. Email: rongxinj@zju.edu.cn, zhihang.fzh@alibaba-inc.com.

###### Abstract

Detecting and rejecting unknown out-of-distribution (OOD) samples is critical for deployed neural networks to void unreliable predictions. In real-world scenarios, however, the efficacy of existing OOD detection methods is often impeded by the inherent imbalance of in-distribution (ID) data, which causes significant performance decline. Through statistical observations, we have identified two common challenges faced by different OOD detectors: misidentifying tail class ID samples as OOD, while erroneously predicting OOD samples as head class from ID. To explain this phenomenon, we introduce a generalized statistical framework, termed ImOOD, to formulate the OOD detection problem on imbalanced data distribution. Consequently, the theoretical analysis reveals that there exists a class-aware _bias_ item between balanced and imbalanced OOD detection, which contributes to the performance gap. Building upon this finding, we present a unified training-time regularization technique to mitigate the bias and boost imbalanced OOD detectors across architecture designs. Our theoretically grounded method translates into consistent improvements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks against several state-of-the-art OOD detection approaches. Code is available at https://github.com/alibaba/imood.

## 1 Introduction

Identifying and rejecting unknown samples during models' deployments, aka OOD detection, has garnered significant attention and witnessed promising advancements in recent years . Nevertheless, most advanced OOD detection methods are designed and evaluated in ideal settings with category-balanced in-distribution (ID) data. However, in practical scenarios, long-tailed class distribution (a typical imbalance problem) not only limits classifiers' capability , but also causes a substantial performance decline for OOD detectors .

As Wang et al.  reveal, a naive combination of long-tailed image cognition  and general OOD detection  techniques cannot simply mitigate this issue, and several efforts have been applied to study the _joint_ imbalanced OOD detection problem . They mainly attribute the performance degradation to misidentifying samples from tail classes as OOD (due to the lack of training data), and concentrate on improving the discriminability for tail classes and out-of-distribution samples . Whereas, we argue that the confusion between tail class and OOD samples presents only one aspect of the imbalance problem arising from the long-tailed data distribution.

To comprehensively understand the imbalance issue, we investigate a wide range of representative OOD detection methods (_i.e._, OE , Energy , and PASCL ) on the CIFAR10-LT dataset . For each model, we statistic the distribution of wrongly detected ID samples and wrongly detectedOOD samples, respectively. The results in Fig. 0(a) reveal that different approaches encounter the same two challenges: (1) ID samples from tail classes are prone to be detected as OOD, and (2) OOD samples are prone to be predicted as ID from head classes. As illustrated in Fig. 0(b), we argue that the disparate ID decision spaces on head and tail classes _jointly_ result in the performance decline for OOD detection, which has also been confirmed by Miao et al. .

To mitigate this problem, Miao et al.  developed a heuristic outlier class learning approach (namely COCL) to respectively separate OOD samples from head and tail ID classes in the feature space. Different from COCL, this paper introduces a generalized statistical framework, termed ImOOD, to formulate and explain the fundamental issue of imbalanced OOD detection from a probabilistic perspective. We start by extending closed-set ID classification to open-set scenarios and derive a unified posterior probabilistic model for ID/OOD identification. Consequently, we find that between balanced and imbalanced ID data distributions exists a class-aware _bias_ item, which concurrently explains the inferior OOD detection performance on both head and tail classes.

Based on ImOOD, we derive a unified loss function to regularize the posterior ID/OOD probability during training, which simultaneously encourages the separability between tail ID classes and OOD samples, and prevents predicting OOD samples as head ID classes. Furthermore, ImOOD can readily generalize to various OOD detection methods, including OE , Energy , and BinDisc , Mahalanobis-distance , _etc_. Besides, our method can easily integrate with other feature-level optimization techniques, including PASCL  and COCL , to derive stronger OOD detectors. With the support of theoretical analysis, our statistical framework consistently translates into strong empirical performance on the CIFAR10-LT, CIFAR100-LT , and ImageNet-LT  benchmarks.

Our contribution can be summarized as follows:

* Through statistical observation and theoretical analysis, we reveal that OOD detection approaches collectively suffer from the disparate decision spaces between tail and head classes in the imbalanced data distribution.
* We establish a generalized statistical framework to formulate and explain the imbalanced OOD detection issue, and further provide a unified training regularization to alleviate the problem.
* We achieve superior OOD detection performance on three representative benchmarks, outperforming state-of-the-art methods by a large margin.

## 2 Related Work

**Out-of-distribution detection.** To reduce the overconfidence on unseen OOD samples , a surge of post-hoc scoring functions has been devised based on various information, including output

Figure 1: **Issues of OOD detection on imbalanced data. (a) Statistics of the class labels of ID samples that are _wrongly_ detected as OOD, and the class predictions of OOD samples that are _wrongly_ detected as ID. (b) Illustration of the OOD detection process in feature space. Head classes’ huge decision space and tail classes’ small decision space _jointly_ damage the OOD detection.**

confidence [19; 29; 33], free energy [32; 14; 26], Bayesian inference [36; 9], gradient information , model/data sparsity [46; 60; 13; 1], and visual distance [47; 49], _etc._ Vision-language models like CLIP  have been recently leveraged to explicitly collect potential OOD labels [17; 16] or conduct zero-shot OOD detections . Other researchers add open-set regularization in the training time [37; 20; 54; 53; 35; 30], making models produce lower confidence or higher energy on OOD data. Manually-collected [20; 52] or synthesized [14; 49] outliers are required for auxiliary constraints.

Works insofar have mostly focused on the ideal setting with balanced data distribution for optimization and evaluation. This paper aims at OOD detection on practically imbalanced data distribution.

**OOD detection on imbalanced data distribution.** In real-world scenarios, the deployed data frequently exhibits long-tailed distribution, and Liu et al.  start to study the open-set classification on class-imbalanced setup. Wang et al.  systematically investigate the performance degradation for OOD detection on imbalanced data distribution, and develop a partial and asymmetric contrastive learning (PASCL) technique to tackle this problem. Consequently, Wei et al.  and  extend the feature-space optimization by introducing abstention classes or outlier class learning and integrating with data augmentation or margin learning techniques, respectively. Sapkota and Yu  employ adaptive distributively robust optimization (DRO) to quantify the sample uncertainty from imbalanced distributions. Choi et al.  focus on the imbalance problem in OOD data, and develop an adaptive regularization for each OOD sample during optimization. In particular, Jiang et al.  also utilize class prior to boost imbalanced OOD detector, which is however constrained as a heuristic post-hoc normalization for pre-trained models and unable to translate into training a better detector.

Different from previous efforts, this paper establishes a generalized probabilistic framework to formulate and explain the imbalanced OOD detection issue, and provides a unified training-time regularization technique to alleviate this problem across different OOD detectors.

## 3 Rethinking Imbalanced OOD Detection

In this section, we start from revisiting the closed-set imbalanced image recognition (in-distribution classification), and extend to open-set out-of-distribution detection problem. Finally, we will reveal the class-aware bias between balanced and imbalanced OOD detectors, and derive a unified training-time regularization technique to alleviate the bias for different detectors.

### Preliminaries

**Imbalanced Image Recognition**. Let \(^{in}\) and \(^{in}=\{1,2,,K\}\) denote the ID feature space and label space with \(K\) categories in total. Let \(^{in}\) and \(y^{in}\) be the random variables with respect to \(^{in}\) and \(^{in}\). The posterior probability for predicting sample \(\) into class \(y\) is given by:

\[P(y|)=|y) P(y)}{P()} P(|y) P(y)\] (1)

Given a learned classifier \(f^{in}^{K}\) that estimates \(P(y|)\), in the class-imbalance setting where the label prior \(P(y)\) is highly skewed, \(f\) is evaluated with the balanced error (BER) [6; 38; 39]:

\[(f)=_{y}P_{|y}(y_{y^{}} f_{y^{}}())\] (2)

This can be seen as implicitly estimating a class-balanced posterior probability :

\[P^{}(y|) P(|y))}{P(y)}\] (3)

The ideal Bayesian-optimal classification becomes \(y^{*}=_{y[K]}P^{}(y|)\).

**Out-of-distribution Detection**. In the open world, input sample \(x\) may also come from the OOD space \(^{out}\). Let \(o\) be the random variable for an unknown label \(o^{in}\), and \(i\) be the union variable of all ID class labels (_i.e._, \(i= y\)). Given an input \(\) from the union space \(^{in}^{out}\), the posterior probability for identifying \(\) as in-distribution is formulated as:

\[P(i|)=_{y}P(y|)=1-P(o|) 1\] (4)

Correspondingly, \(P(o|)\) measures the probability that sample \(\) does not belong to any known ID class, aka OOD probability. Hence, the OOD detection task can be viewed as a binary classification problem . Given a learned OOD detector \(g^{in}^{out}^{1}\) that estimate the \(P(i|)\), samples with lower scores \(g()\) are detected as OOD and vice versa.

### Analysis of OOD Detection on Imbalanced Data Distribution

When ID classification meets OOD detection, slightly different from Eq. (1), the classifier \(f\) is actually estimating the posterior class probability for sample \(\) from ID space \(^{in}\) merely, that is, \(P(y|,i)\)[20; 51]. Considering a sample \(\) from the open space \(^{in}^{out}\), the classification posterior in Eq. (1) is re-formulated as \(P(y|)=P(y|,i) P(i|)\), the probability that sample \(\) comes from ID data multiply the probability that sample \(\) belongs to the specific \(y\)-th ID class. According to Eq. (3), we assume the proportion between balanced classification \(P^{}(y|)\) and vanilla \(P(y|)\) for each class \(y\) still holds.

**Lemma 3.1**.: _For each ID class \(y\) in open-set, there exists a non-negative variable \(_{y}()\), so that \(P^{}(y|)=_{y}())}{P(y)}\), where \(_{y}()=}(|y)}{P(|y)}( 0,)\), \(P(y|),P^{}(y|),P(y)\)._

In fact, \(_{y}()\) captures the likelihood difference for the same sample \(\) between balanced and imbalanced distributions, and also plays the role in constraining the multiplication results to \((0,1)\). The proof can be found in Appendix A.1. Using Lemma 3.1, we reveal that there exists a class-aware bias term \(()\) between the OOD detection on balanced and imbalanced data distribution.

**Theorem 3.2**.: _According to Lemma 3.1, there exists a bias term \(()=_{y}_{y}(),i)}{P(y)}\) between \(P^{}(i|)\) and \(P(i|)\), i.e., \(P^{}(i|)=() P(i|)\)._

Proof.: Since \(P(y|)=P(y|,i) P(i|)\), from Lemma 3.1, \(P^{}(y|)\) can be further expressed as \(P^{}(y|)=_{y}(),i)}{P(y)}\). \(P(i|)\). According to Eq. (4), it can be formulated as: \(P^{}(i|)=_{y}P^{}(y|)=_{y}_{y}( ),i)}{P(y)}P(i|)() P(i| )\), where \(()=_{y}_{y}(),i)}{P(y)}\). 

Based on Theorem 3.2, we conclude that the original OOD posterior \(P(i|)\) estimated by the detector \(g()\), with the bias term \(()\), causes the performance gap for OOD detection on balanced and imbalanced data distributions. To understand this further, we will first discuss the scenario under ideal conditions and then extend our analysis to real-world scenarios, attempting to analyze the intrinsic bias of the out-of-distribution problem in both cases.

**On ideal class-balanced distribution**, the data likelihood \(P(|y)=P^{}(|y)\), so that \(_{y}()\). From Theorem 3.2, \(()=_{y},i)}{P(y)}\). Meanwhile, the class prior \(P(y)\), and the summary of in-distribution classification probabilities equals 1 (_i.e._, \(_{y}P(y|,i)=1\)), making the bias item \(()=_{y}P(y|,i)=1\). Ultimately, Theorem 3.2 indicates \(P^{}(i|)=P(i|)\), where the detector \(g()\) exactly models the balanced OOD detection.

**On more challenging class-imbalanced distribution**, the class prior \(P(y)\) is a class-specific variable for ID categories, and \((x)=_{y}_{y}(),i)}{P(y)} 1\). In previous works[20; 51], the class posterior \(P(y|,i)\) is usually estimated with a softmax function by classifier \(f\), and the class prior \(P(y)\) adopts the sample frequency for each ID class. \(_{y}()\) is under-explored and simply treated as a constant. Under this circumstance, since \(_{y}P(y|,i)=1\), the bias item can be viewed as a weighted sum of the reciprocal prior \(\). Theorem 3.2 explains how \((x)\) causes the gap between balanced (ideal) ID/OOD probability \(P^{}(i|)\) and imbalanced (learned) \(P(i|)\):* Given a sample \(\) from an ID tail-class \(y_{t}\) with a small prior \(P(y_{t})\), when the classification probability \(P(y_{t}|,i)\) gets higher, the term \(()\) becomes larger. Compared to the original \(P(i|)\) (learned by \(g\)), the calibrated probability \(P^{}(i|)\)_i.e._, \(P(i|)()\) is more likely to identify the sample \(x\) as in-distribution, rather than OOD.
* Given a sample \(^{}\) from OOD data, as the classifier \(f\) tends to produce a higher head-class probability \(P(y_{h}|^{},i)\) and a lower tail-class \(P(y_{t}|^{},i)\), the term \((^{})\) becomes smaller. Compared to the original \(P(i|^{})\), the calibrated probability \(P^{}(i|^{})\)_i.e._, \(P(i|^{})(^{})\) is more likely to identify the sample \(x^{}\) as out-of-distribution, rather than ID.

The above analysis is consistent with the statistical behaviors (see Fig. 1) of a vanilla OOD detector \(g\). Compared to an ideal balanced detector \(g^{}\), \(g\) is prone to wrongly detect ID samples from tail class as OOD, and simultaneously wrongly detect OOD samples as head class from ID.

### Towards Balanced OOD Detector Learning

To address the identified bias in OOD detection due to class imbalance, we present a unified approach from a statistical perspective. Our goal is to push the learned detector \(g\) towards the balanced \(g^{}\). We outline the overall formula for estimating the OOD posterior below.

Specially, we use the common practices [39; 23] to estimate the probability distribution \(P^{}(i|)=_{y}P^{}(y|)=_{y}_{y}( ),i)}{P(y)}P(i|)\) in Theorem 3.2:

First, for the class prior \(P(y)\), we use the label frequency of the training dataset [8; 39], expressed as \(P(y)}{_{y^{}}n_{y^{}}}_{y}\), where \(n_{y}\) refers to the number of instances in class \(y\). For the class posterior \(P(y|,i)\), given a learned classifier \(f\), the classification probability can be estimated using a _softmax_ function [39; 23]: \(P(y|,i))}}{_{y^{}}e^{f^{y}( )}} p_{y|,i}\), where \(f_{y}()\) represents the logit for class \(y\).

For the OOD posterior \(P(i|)\), since OOD detection is a binary classification task , the posterior probability for an arbitrary OOD detector \(g\)[20; 32; 28] can be estimated using a _sigmoid_ function: \(P(i|))}}\), where \(g()\) is the ID/OOD logit. Finally, for the class-specific scaling factor \(_{y}()\), estimating \(_{y}()\) is a sophisticated problem in long-tailed image recognition [39; 25]. To focus on the OOD detection problem, we use a parametric mapping \(_{y;}:(0,)\), where \(\) are learnable parameters that are optimized through gradient back-propagation.

This unified approach allows us to systematically estimate and correct for the bias introduced by class imbalance, thereby improving the performance of OOD detection in real-world scenarios. According to Theorem 3.2, the balanced OOD detectors \(g^{}\) is modeled as:

\[(g^{}())=(_{y}_{y;}(),i}}{_{y}})(g())() (g())\] (5)

Substitute the sigmoid function \((z)=}\) into Eq. (5), we have:

\[g()=g^{}()-[(()-1) e^{g^{}()}+()]\] (6)

The derivation is displayed in Appendix A.3. In order to make the detector \(g()\) directly estimate the balanced OOD detection distribution, we can apply the binary cross-entropy loss on calibrated logits:

\[_{}=_{}(g()-( ()-1) e^{g()}+(),t) _{}(g()-(),t)\] (7)

where \(t=\{^{in}\}\) indicates whether the sample \(\) comes from ID or not. For a ID sample from tail classes with \(t=1\), discussed in Sec. 3.2, the bias \(\) is larger than samples from head classes. In this situation, the punishment \(\) correspondingly increases, which encourages the detector \(g\) to generate a higher score \(g()\) to reduce the loss. On the other hand, for a OOD sample that predicted as a head ID class by classifier \(f\), \(\) and \(\) become smaller than those predicted as tail ID classes, and \(g\) are further forced to reduce the score \(g()\) to decrease the loss, as the label \(t\) for OOD samples is 0. In practice, to alleviate the optimization difficulty, the \(()\) for ID samples (with \(t=1\)) is cut off to be non-negative value, and \((^{})\) for OOD samples (with \(t=0\)) is cut off to be non-positive values, ensuring the optimization direct dose not conflict with the vanilla BCE loss function.

Meanwhile, according to Lemma 3.1, we add an extra constraint on \(_{y;}\) to ensure the posterior estimate \(P^{}(y|)=_{y}(),i}}{_{y}} (g())\) and \(P^{}(i|)=_{y}P^{}(y|)\) will not exceed 1:

\[_{}=\{0,_{y}_{y;}(),i}}{_{y}}(g())-1\}=\{0,() (g())-1\}\] (8)

Note that for each class \(y\), the term \(,i}}{_{y}}(g())>0\) always holds, so that we only have to constrain the summary on all classes \(P^{}(i|)=_{y}P^{}(y|)\) within 1, as indicated in Eq. (8), and \(P^{}(y|)\) for each class \(y\) will be constrained as well.

Combining with \(_{}\) and \(_{}\) for optimization, the learned OOD detector \(g^{*}\) has already estimated the balanced \(P^{}(i|)\). We thus predict the ID/OOD probability as usual: \((i|)=()}}\). The other terms like \(\), \(\), and \(\) are no longer needed to compute, maintaining the inference efficiency and simplicity for OOD detection applications.

## 4 Experiments

In this section, we empirically validate the effectiveness of our ImOOD on several representative imbalanced OOD detection benchmarks. The experimental setup is described in Sec. 4.1, based on which extensive experiments and discussions are displayed in Sec. 4.2 and Sec. 4.3.

### Setup

**Datasets.** Following the literature [51; 23; 10; 40], we use the popular CIFAR10-LT, CIFAR100-LT , and ImageNet-LT  as imbalanced in-distribution datasets.

For CIFAR10/100-LT benchmarks, the imbalance ratio (_i.e._, \(=_{y}(n_{y})/_{y}(n_{y})\)) is set as 100 [8; 51]. The original CIAFR10/100 test sets are kept for evaluating the ID classification capability. For OOD detection, the TinyImages80M  is adopted as the auxiliary OOD training data, and the test set is semantically coherent out-of-distribution (SC-OOD) benchmark .

For the large-scale ImageNet-LT benchmark, training samples are sampled from the original ImageNet-1k  dataset, and the validation set is taken for evaluation. We follow the OOD detection setting as Wang et al.  to use ImageNet-Extra as auxiliary OOD training and ImageNet-1k-OOD for testing. Randomly sampled from ImageNet-22k , ImageNet-Extra contains 517,711 images belonging to 500 classes, and ImageNet-1k-OOD consists of 50,000 images from 1,000 classes. All the classes in ImageNet-LT, ImageNet-Extra, and ImageNet-1k-OOD are not overlapped.

**Evaluation Metrics.** For OOD detection, we report three metrics: (1) AUROC, the area under the receiver operating characteristic curve, (2) AUPR, the area under the precision-recall curve, and (3) FPR95, the false positive rate of OOD samples when the true positive rate of ID samples are 95%. For ID classification, we measure the macro accuracy of the classifier. We report the mean and standard deviation of performance (%) over six random runs for each method.

**Implementation Details.** For the ID classifier \(f\), following the settings of Wang et al. , we train ResNet18  models on the CIFAR10/100LT benchmarks, and leverage ResNet50 models for the ImageNet-LT benchmark. Logit adjustment loss  is adopted to alleviate the imbalanced ID classification. Detailed settings are displayed in Appendix B.1. For the OOD detector \(g\), as Bitterwolf et al.  suggest, we implement \(g\) as a binary discriminator (abbreviated as _BinDisc_) to perform ID/OOD identification. Detector \(g\) shares the same backbone (feature extractor) as classifier \(f\), and \(g\) only attaches an additional output node to the classification layer of \(f\). In addition, we also add a linear layer on top of the backbone to produce the \(\) factors to perform the training regularization with Eq. (7) and Eq. (8). To reduce the optimization difficulty, the gradient is stopped between Eq. (7) and Eq. (8) (but still shared in the backbone), where Eq. (7) aims at training \(g\) while Eq. (8) only optimize \(\). Furthermore, to verify the versatility of our method, we also implement several representative OOD detection methods (_e.g._, OE , Energy , _etc._) into binary discriminators, and equip them with our ImOOD framework. For more details please refer to Appendix B.2.

**Methods for comparison.** In the following sections, we mainly compare our method on three benchmarks with the typical OOD detectors including OE , Energy , and BinDisc , as well as some state-of-the-art detectors such as PASCL , ClassPrior , EAT , COCL , _etc._. Specifically, as the results on the ImageNet-LT benchmark reported by COCL share a large discrepancy against PASCL (especially the AUPR measure), we re-implement COCL based on their released code7 and report the aligned results in Tab. 2.

### Main Results

**ImOOD significantly outperforms previous SOTA methods on CIFAR10/100-LT benchmarks.** As shown in Tab. 1, our ImOOD achieves new SOTA performance on both of CIFAR10/100-LT benchmarks. Built on top of the strong baseline PASCL , our method leads to 1.9% increase of AUROC, 3.2% increase of AUPR, and 4.6% decrease of FPR95 on CIFAR10-LT, with 0.9% - 1.8% enhancements of respective evaluation metrics on CIFAR100-LT. By integrating with COCL , our ImOOD further pushes the imbalanced OOD detection on CIFAR10/100-LT benchmarks towards a higher performance, _e.g._, achieving 93.55%/78.50% of AUROC respectively. To further demonstrate the efficacy, we validate our method on the real-world large-scale ImageNet-LT  benchmark, and the results are displayed below.

    &  &  \\   & AUROC\(\) & AUPR\(\) & FPR95\(\) & ACC\(\) & AUROC\(\) & UPR\(\) & FPR95\(\) & ACC\(\) \\  MSP & 72.28 & 70.27 & 66.07 & 72.34 & 61.00 & 57.54 & 82.01 & 40.97 \\ OECC & 87.28 & 86.29 & 45.24 & 60.16 & 70.38 & 66.87 & 73.15 & 32.93 \\ EnergyOE & 89.31 & 88.92 & 40.88 & 74.68 & 71.10 & 67.23 & 71.78 & 39.05 \\ OE & 89.77 & 87.25 & 34.65 & 73.84 & 72.91 & 67.16 & 68.89 & 39.04 \\ PASCL & 90.99 & 89.24 & 33.36 & 77.08 & 73.32 & 67.18 & 67.44 & 43.10 \\ OpenSampling & 91.94 & 91.08 & 36.92 & 75.78 & 74.37 & 75.80 & 78.18 & 40.87 \\ ClassPrior & 92.08 & 91.17 & 34.42 & 74.33 & 76.03 & 77.31 & 76.43 & 40.77 \\ BalEnergy & 92.56 & 91.41 & 32.83 & 81.37 & 77.75 & 78.61 & 73.10 & 45.88 \\ EAT & 92.87 & 92.40 & 28.83 & 81.31 & 75.45 & 70.87 & **64.01** & 46.23 \\ COCL & 93.28 & 92.24 & 30.88 & 81.56 & 78.25 & 79.37 & 74.09 & 46.41 \\  PASCL + **Ours** & 92.93 & 92.51 & 28.73 & 78.96 & 74.23 & 68.63 & 65.65 & 44.60 \\ COCL + **Ours** & **93.55** & **92.83** & **28.52** & **81.83** & **78.50** & **79.96** & 71.65 & **46.80** \\   

Table 1: OOD detection evaluation on CIFAR10/100-LT benchmarks. The best results are marked in **bold**, and the secondary results are marked with _underlines_. The base model is ResNet18.

   Method & AUROC\(\) & AUPR\(\) & FPR95\(\) & ACC\(\) \\  MSP & 53.81 & 51.63 & 90.15 & 39.65 \\ OECC & 63.07 & 63.05 & 86.90 & 38.25 \\ EnergyOE & 64.76 & 64.77 & 87.72 & 38.50 \\ OE & 66.33 & 68.29 & 88.22 & 37.60 \\ PASCL & 68.00 & 70.15 & 87.53 & 45.49 \\ EAT & 69.84 & 69.25 & 87.63 & 46.79 \\ COCL & 73.87 & 72.63 & 76.35 & 51.00 \\  PASCL + **Ours** & 74.69 & 73.08 & 74.37 & 46.63 \\ COCL + **Ours** & **75.84** & **73.19** & **74.96** & **52.43** \\   

Table 2: OOD detection evaluation on the ImageNet-LT benchmark. The base model is ResNet50.

**ImOOD achieves superior performance on the ImageNet-LT benchmark.** As Tab. 2 implies, our ImOOD brings significant improvements against PASCL, _e.g._, 6.7% increase on AUROC and 13.2% decrease on FPR95, and further enhance the SOTA method COCL for a better OOD detection performance, _e.g._, 75.84 of AUROC. Since the performance enhancement is much greater than those on CIFAR10/100-LT benchmarks, we further statistic the class-aware error distribution on wrongly detected ID/OOD sample in Fig. A1. The results indicate our method builds a relatively better-balanced OOD detector on ImageNet-LT, which leads to higher performance improvements. Besides, as we follow the literature [51; 40] to employ ResNet18 on CIFAR10/100-LT while adopt ResNet50 on ImageNet-LT, the model capacity also seems to play a vital role in balancing the OOD detection on imbalanced data distribution, particularly in more challenging real-world scenarios.

**Additional comparison following ClassPrior's setting.** Since ClassPrior  uses a totally different setting against the literature [51; 40] on the ImageNet datasets, including different ID imbalance ratio and OOD test sets, we additionally compare with ClassPrior in Tab. 3. For a fair comparison, as ClassPrior does not leverage real OOD data for training, we eliminate the auxiliary ImageNet-Extra dataset and utilize the recent VOS  technique to generate OOD syntheses for our regularization. According to Tab. 3, our method consistently outperforms ClassPrior by a large margin on all subsets.

### Ablation Studies

In this section, we conduct in-depth ablation studies on the CIFAR10-LT benchmark to assess the validity and versatility of our proposed ImOOD framework, and the results are reported as follows.

**Lemma 3.1** (\(P^{}(y|)=_{y}())}{P(y)}\)) is consistent with empirical results.: To validate that the coefficient \(_{y}()\) depends on input sample \(\) and differs for each class \(y\), we perform a series of ablation studies in Tab. 4. We first build a baseline model with BinDisc  only, and no extra regularization is adopted to mitigate the imbalanced OOD detection. As shown in the first row from Tab. 4, the baseline (denoted as _none_ of \(_{y}\) estimates) presents a fair OOD detection performance (_e.g._, 90.06% of AUROC and 33.39% of FPR95). Then, we simply take \(_{y}\) as a constant for all classes (denoted as \(_{y} const\)) by assuming \(P^{}(|y)=P(|y)\) and \(_{y}\) (see Appendix A.1) to apply the training regularization in Sec. 3.3. According to Tab. 4, the OOD detection performance receives a slight decline of AUROC (from 90.06% to 89.75%), despite the better FPR95 result. Consequently, after treating \(_{y}\) as a learnable variable for each class \(y\) (denoted as \(_{y}_{y;}\)), the detector receives significant improvement on all the three measures of AUROC, AUPR, and FPR95. Finally, setting \(_{y}\) as an input-dependent and class-aware learnable variable (denoted as \(_{y}_{y;}()\)) brings further OOD detection enhancement.

   \(_{y}\) Estimates & AUROC\(\) & AUPR\(\) & FPR95\(\) & ID ACC\(\) \\  none & 90.06 & 88.72 & 33.39 & 78.22 \\ \(_{y} const\) & 89.75 & 86.28 & 32.67 & 78.50 \\ \(_{y}_{y,}\) & 92.04 & 91.32 & 31.24 & 79.16 \\ \(_{y}_{y,}()\) & 92.23 & 91.92 & 29.95 & 79.56 \\  + PASCL & 92.93 & 92.51 & 28.73 & 78.96 \\ + COCL & **93.55** & **92.83** & **28.52** & **81.83** \\   

Table 4: Ablation on the \(_{y}\) estimates and technique integration on the CIFAR10-LT benchmark.

    &  &  &  &  \\   & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) \\  ClassPrior & 82.51 & 66.06 & 80.08 & 69.12 & 74.33 & 79.41 & 69.58 & 78.07 \\
**Ours** & **86.15** & **59.13** & **81.29** & **65.88** & **77.57** & **76.26** & **72.82** & **72.73** \\   

Table 3: Comparison on ClassPrior’s ImageNet-LT-a8 benchmark. The base model is MobileNet.

In addition, we further statistics the practical distribution of \(\) in Fig. 2, where \(_{y}()\) is relatively higher for the ID sample from head classes. It is consistent with \(_{y}()=}(|y)}{P(|y)}\) (see Appendix A.1), as the data likelihood \(P(|y)\) is close to the balanced situation \((P^{}(|y))\) for head samples while over-estimated for tail classes, leading to a higher fraction of \(}(|y)}{P(|y)}\) for head classes and lower for tail classes. Detailed discussion is presented in Appendix A.2.

**The designed training-time regularization in Sec. 3.3 is effective.** As shown in Tab. 4, compared with the BinDisc baseline in the first row, adding Eq. (7) and Eq. (8) by setting \(_{y}_{y;}()\) leads to significant improvement on the OOD detection performance (_i.e._, 2.2% increase of AUROC and 3.4% decrease of FPR95). As Fig. 2 shows, the automatically learned adjustments (_e.g._, \(\) and \(\)) are consistent with our motivation, where we aim to punish the ID data from tail classes with a large positive \(\), as well as the OOD samples predicted as ID head classes with a large negative \(\) (Sec. 3.3). Moreover, as indicated by Tab. 4, integrating with PASCL  and COCL  techniques further boosts the imbalanced OOD detection, ultimately resulting in a new SOTA performance.

**ImOOD generalizes to various OOD detection methods.** To verify the versatility of our statistical framework, we implement the ImOOD framework with different OOD detectors beside BinDisc, including the OE , Energy , and the Mahalanobis distance . For those detectors, we add an extra linear layer to conduct logistic regression on top of their vanilla OOD scores (see Appendix B.2), and leverage our training-time regularization to optimize those detectors in a unified manner. According to the results presented in Tab. 5, our ImOOD consistently boosts the original OOD detectors by a large margin on all the AUROC, AUPR, and FPR95 measures. In real-world applications, one may choose a proper formulation of our ImOOD to meet specialized needs.

**ImOOD is robust to different OOD test sets.** In the preceding sections, we evaluated our method on the CIFAR10-LT benchmark, where the SCOOD test set  comprises 6 subsets covering different scenarios. As suggested by Fort et al. , the SVHN subset can be viewed as far OOD, and the

   OOD Detector & Method & AUROC\(\) & AUPR\(\) & FPR95\(\) \\   & OE & 89.91 & 87.32 & 34.06 \\  & **+Ours** & **91.52** & **89.03** & **30.64** \\   & Energy & 90.27 & 88.73 & 34.42 \\  & **+Ours** & **91.41** & **90.63** & **31.81** \\   & Maha & 88.26 & 87.94 & 42.74 \\  & **+Ours** & **89.30** & **88.68** & **39.54** \\   

Table 5: Generalization to OOD detectors.

   OOD Dataset & Method & AUROC\(\) & AUPR\(\) & FPR95\(\) \\   & PASCL & 89.91 & 87.32 & 34.06 \\  & **+Ours** & **91.52** & **89.03** & **30.64** \\   & Energy & 90.27 & 88.73 & 34.42 \\  & **+Ours** & **91.41** & **90.63** & **31.81** \\   & Maha & 88.26 & 87.94 & 42.74 \\  & **+Ours** & **89.30** & **88.68** & **39.54** \\   

Table 6: Robustness to OOD test sets.

Figure 2: **Statistics on \(\), \(\), and \(\) from the CIFAR10-LT benchmark. (1) Upper: distributions on ID samples from head to tail (left to right) class indices; (2) Lower: distributions on OOD samples predicted as head to tail (left to right) ID classes.**

CIFAR100 subset can be seen as near OOD (with CIFAR10-LT as ID). According to the detailed results in Tab. 6, our ImOOD brings consistent enhancement against the strong baseline PASCL regardless of the near or far OOD test set. Furthermore, we also report the spurious OOD detection evaluation in Tab. 6. Specifically, we follow Ming et al.  to take WaterBird as the imbalanced ID dataset, which also suffers from the imbalance problem (on water birds and land birds), and a subset of Places  as the spurious OOD test set (with spurious correlation to background). Results in Tab. 6 also demonstrate our method's robustness in handling spurious OOD problems, with a considerable improvement of 4.3% increase on AUROC and 5.96% decrease on FPR95. The robustness of ImOOD to various OOD testing scenarios is verified.

### ImOOD's Inference-time Application

Despite our main focus on training more balanced OOD detectors, we also make some attempts to apply our method during pre-trained models' inference. According to our Theorem 3.2, for an existing OOD detector \(P(i|x)\) (_e.g._, trained with BinDisc), we can calculate the bias term \((x)\) to regulate the vanilla scorer \(P(i|x)\) into balanced \(P^{bal}(i|x)=(x) P(i|x)\). However, as \((x)=_{y}_{y}(x)\), the estimation of \(_{y}(x)\) presents considerable difficulty without training, but we have also tried some trivial approaches in Tab. 7.

First, we simply treat \(_{y}(x)\) as a constant (_e.g._, \(_{y}(x)_{1}=1\)) for arbitrary input \(x\) and class \(y\) to calculate the bias term (denoted as \(_{1}(x)\)), and the results on CIFAR10-LT benchmark immediately witness a performance improvement (_e.g._, 0.28% increase on AUROC and 1.29% decrease on FPR95) compared to the baseline OOD detector. However, the improvement is relatively insignificant, and the phenomenon is consistent with our ablation studies in Tab. 4, which demonstrates the importance of learning a _class-dependent_ and _input-dependent_\(_{y}(x)\) during training.

Then, inspired by the statistical results in Fig. 2, we take a further step to use a polynomial (rank=2) to fit the curve between the predicted class \(y\) and \(_{y}(x)\) learned by another model, and apply the coefficients to estimate a _class-dependent_\(_{y}\) for the baseline model (denoted as \((x)P(i|x)\)). This operation receives further enhancement on OOD detection and gets close to our learned model (_e.g._, 30.80% _v.s._ 29.95% of FPR95).

In conclusion, our attempts illustrate the potential of applying our method to an existing model without post-training, and we will continue to extend \(_{y}\) to an _input-dependent_ version (say \(_{y}(x)\)) in our future work.

## 5 Conclusion and Discussion

This paper establishes a statistical framework ImOOD to formulate OOD detection on imbalanced data distribution. Through theoretical analysis, we find there exists a class-aware biased item between balanced and imbalanced OOD detection models. Based on it, our ImOOD provides a unified training-time regularization technique to alleviate the imbalance problem. On three popular imbalanced OOD detection benchmarks, extensive experiments and ablation studies to demonstrate the validity and versatility of our method. We hope our work can inspire new research in this community.

**Limitations.** Following the literature, ImOOD utilizes auxiliary OOD training samples to refine the the ID/OOD decision boundary. However, unforeseen OOD samples in real-world applications could potentially challenge this boundary. To mitigate this issue, integrating online-learning strategies for adaptive decision-making during testing is a promising avenue. We view this as our future work.

   Method & Detector & AUROC\(\) & AUPR\(\) & FPR95\(\) \\  BinDisc & \(P(i|x)\) & 90.06 & 88.72 & 33.39 \\
**+Ours** (infer) & \(_{1}(x)P(i|x)\) & 90.34 & 88.45 & 32.10 \\
**+Ours** (infer) & \((x)P(i|x)\) & 90.86 & 88.95 & 30.80 \\
**+Ours** (train) & \((x)P(i|x)\) & **92.23** & **91.92** & **29.95** \\   

Table 7: Attempts to apply our ImOOD into pre-trained models’ inference stages.