# Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding

Yunze Man\({}^{1}\)&Shuhong Zheng\({}^{1}\)&Zhipeng Bao\({}^{2}\)

Martial Hebert\({}^{2}\)&Liang-Yan Gui\({}^{1}\)&Yu-Xiong Wang\({}^{1}\)

\({}^{1}\) University of Illinois Urbana-Champaign \({}^{2}\) Carnegie Mellon University

https://yunzeman.github.io/Lexicon3D

###### Abstract

Complex 3D scene understanding has gained increasing attention, with scene encoding strategies built on top of visual foundation models playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present the first comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans _seven_ vision foundation encoders, including image, video, and 3D foundation models. We evaluate these models in _four_ tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluation yields _key intriguing findings_: Unsupervised image foundation models demonstrate superior overall performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, language-pretrained models show unexpected limitations in language-related tasks, and the mixture-of-vision-expert (MoVE) strategy leads to consistent performance improvement. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene understanding tasks.

## 1 Introduction

Recently, complex 3D scene understanding has emerged as a pivotal area in computer vision, encompassing tasks such as scene generation , reasoning , and interaction . Leveraging large-scale vision foundation models, many approaches  have achieved promising results in various downstream tasks, thereby enabling a wide range of real-world applications, from autonomous driving , robotics , to multimodal agents .

While numerous studies  have provided guidance on the use of vision foundation models for 2D image-based tasks, the strategies for 3D scenarios remain unclear. A systematic understanding of complex real-world scenarios involves not only semantic and depth awareness , which is possible to evaluate within the 2D domain, but also geometric awareness and the ability to align with multimodal information for reasoning and grounding tasks. To address this gap, our work evaluates the use of different types of visual foundation models for complex scene understanding and seeks to identify the strengths and limitations of each model in different scenarios. Ultimately, this study aims to contribute to the development of more effective and efficient scene understanding systems.

Concretely, we aim to address several key questions. First, given that most vision foundation models are trained on image or video data, we want to determine _whether 2D foundation models can effectively interpret 3D scenes_. Second, since video models inherently contain temporal information that captures aspects of the 3D structure as well, we investigate _whether they lead to better 3D feature representations compared to image models_. Finally, we seek to identify _the most suitable scenarios for different foundation models trained under various settings_.

To answer these questions, we design a _unified_ paradigm to systematically probe visual encoding models for complex 3D scene understanding from different perspectives. Our evaluation spans _seven_ vision foundation models in images, videos, and 3D-based models, as shown in Table 1. Our evaluation is conducted among _four_ diverse tasks: **Vision-Language Scene Reasoning** assesses the model's ability to reason about scenes based on textual descriptions, evaluating _scene-level_ representation; **Visual Grounding** tests the model's capacity to associate language with specific objects within a scene, reflecting _object-level_ representation; **Segmentation** evaluates the model's ability to assign semantic labels to each pixel, assessing _semantic_ understanding; **Registration** measures the performance of aligning different views of a scene, testing _geometric_ capacity. Through these tasks, our aim is to explore the strengths and weaknesses of different vision foundation models in 3D scene understanding, providing insights into their applicability in various scenarios. With the major results demonstrated in Figure 1, our key findings include:

* Image or video foundation models achieve promising results for 3D scene understanding. Among them, DINOv2  demonstrates the best overall performance, showing strong generalizability and flexibility, which is consistent with the observation in 2D scenarios . Our evaluation further verifies its capability in global and object-level 3D vision-language tasks. It can serve as a general backbone for 3D scene understanding.
* Video models, benefiting from temporally continuous input frames, excel in object-level and geometric understanding tasks by distinguishing instances of the same semantics in a scene.
* Visual encoders pretrained with language guidance (_e.g._, CLIP ) _do not_ necessarily perform well in other language-related evaluation tasks, challenging the common practice of using such models as default encoders for vision-language reasoning tasks.
* Generative pretrained models, beyond their well-known semantic capacity, also excel in geometrical understanding, offering new possibilities for scene understanding.
* The mixture-of-vision-expert (MoVE) strategies, including combining multi-layer features from the same visual model, and concatenating features from multiple visual models, both lead to a consistent boost of performance across different tasks.

Our work, **Lexicon3D**, provides a unified probing architecture and the first comprehensive evaluation of 3D scene understanding with visual foundation models. The key findings we have achieved above, in conjunction with other interesting observations, suggest exploring more flexible encoder selections in future vision-language tasks to optimize performance and generalization.

Figure 1: Evaluation settings and major results of different vision foundation models (VFMs) for complex 3D scene understanding. We assess the performance of VFMs on multimodal scene reasoning, grounding, segmentation, and registration tasks.

## 2 Related Work

Our work is closely related to methods that focus on extraction of features from images, videos, and 3D assets, as well as learning joint representation spaces for vision-language fusion. A large body of recent literature has explored the representation learning for multimodal visual inputs and their complementary performance in image understanding. In contrast, our study presents a comprehensive analysis of the use of pretrained visual encoders for _zero-shot_ 3D scene understanding. _To the best of our knowledge, we are the first to examine pretrained video encoders on 3D scene understanding tasks and to compare image, video, and 3D point encoding strategies in this context._

Image self-supervised learning.In recent years, learning robust and generalizable pretrained image representations has become a prevalent research direction in computer vision and multimodal research. One line of work focuses on learning task-agnostic image features using self-supervised learning (SSL) signals, which include pretext tasks such as colorization , inpainting , transformation prediction , and self-distillation [14; 18; 19; 30; 31]. The recent development of the patch-based image tokenizer, ViT , has also led to the emergence of mask autoencoder architectures (MAEs) for feature extraction [8; 32; 115]. Of particular interest, DINOV2 , combining a masked-image modeling loss and an invariance-based self-distillation loss, has become one of the most scalable and competitive self-supervised learning architectures that uses only image signals. Another line of work proposes learning image features with text guidance, _i.e._, using textual descriptions to guide the pretraining of the image encoders [39; 56]. Building upon the powerful image-text encoder CLIP , LSeg  and BLIP [47; 48] extend the image pretraining objective to more complex visual perception tasks by incorporating pixel-level semantic understanding and encouraging better alignment with large language models (LLMs) [13; 69; 107; 106], respectively.

Video and 3D representation learning.Self-supervised representation learning has also been explored in the context of videos and 3D point clouds. Extending the success of the CLIP architecture  from images to videos, a body of work proposes to pretrain a video encoder by aligning the feature space with textual guidance extracted from video captions [3; 88; 92; 101]. Other pretext tasks used in video representation learning include next frame prediction  and MAE [29; 83; 86]. Among them, V-JEPA  adapts the MAE-inspired joint embedding prediction architecture (JEPA) [4; 45] to the spatio-temporal domain, achieving state-of-the-art performance on a wide spectrum of video and image tasks. Despite extensive research on 2D visual foundation encoders, pretrained models for 3D point clouds are significantly fewer due to the lack of large-scale 3D datasets. Existing work has explored contrastive pretraining [38; 91; 109] and masked signal modeling [50; 62; 90; 95; 100; 105] for point representation learning. Recently, benefiting from the rapid advancement of 3D data rendering and large synthetic datasets [21; 113], Swin3D  and Uni3D  have outperformed other pretraining methods by a significant margin with large-scale pretraining for scene-level perception and object-level understanding, respectively.

Generation and mixture of experts (MoE) for feature extraction.With the success of diffusion-based generative models [73; 33; 79], a line of research has begun to explore their role in image perception tasks. These methods extract feature maps or attention maps of a given image from the U-Net architectures of diffusion models and perform various downstream tasks, including depth estimation [24; 74; 111], semantic segmentation [9; 54; 59; 89; 111], object detection , and panoptic segmentation . Another line of work [63; 102; 103] investigates the complementary

   Model & Input Modality & Architecture & Supervision & Dataset \\  DINOV2  & & ViT-L/14 & SSL & LVD-142M \\ LSeg  & Image & ViT-L/16 & VLM & LSeg-7Mix \\ CLIP  & & ViT-L/14 & VLM & WIT-400M \\ StableDiffusion  & & UNet & Generation & LAION \\  V-JEPA  & & ViT-L/16 & SSL & VideoMix2M \\ StableVideoDiffusion  & & UNet & Generation & LVD-F \\  Swin3D  & 3D Points & Swin3D-L & Segmentation & Structure3D \\   

Table 1: Details of the seven evaluated VFMs. In supervision signals, we use “SSL” to represent self-supervised learning, and use “VLM” to represent vision-language modality alignment. A more detailed explanation of the evaluated VFMs is provided in the supplementary material A.

nature of different embeddings extracted by multiple foundation backbones and their joint effect on downstream tasks [6; 70]. However, these investigations have been limited to the 2D domain, leaving the potential of leveraging pretrained encoders for perception and reasoning tasks in complex 3D scenes [5; 22; 35; 36; 41; 55; 58; 66; 118] largely unexplored.

## 3 Probing Visual Encoders for Scene Understanding

The objective of Lexicon3D is to evaluate different visual foundation models in complex scene understanding tasks. We first construct a unified architecture capable of probing different visual foundation models on a spectrum of downstream tasks. Then, we break down the 3D scene understanding task into four sub-tasks, including (1) vision-language reasoning, (2) visual grounding, (3) semantic understanding, and (4) geometric understanding, for a more detailed evaluation.

### A Unified Probing Framework

We design a unified framework, as shown in Figure 2, to extract features from different foundation models, construct a 3D feature embedding as scene embeddings, and evaluate them on multiple downstream tasks. For a complex indoor scene, existing work usually represents it with a combination of 2D and 3D modalities. For realistic scenarios [15; 20; 98], videos are usually first captured with handheld cameras and then 3D points are obtained from reconstruction algorithms such as COLMAP . For digital and synthetic scenarios [72; 113], 3D assets are designed and generated first, before images and/or videos are rendered within the created space. Given a complex scene represented in posed images, videos, and 3D point clouds, we extract their feature embeddings with a collection of vision foundation models. For image- and video-based models, we project their features into the 3D space for subsequent 3D scene evaluation tasks with a _multi-view 3D projection module_. Following [22; 35; 36; 66], for a point cloud \(\), this module produces features \(f_{}\) for each point \(\) given image features \(f\) and the pose and camera information \(,\). We first project all points

Figure 3: **Visualization of extracted scene features from different visual foundation models. We use principal component analysis (PCA) to compress the feature embeddings into three dimensions. The clear distinction between colors and patterns demonstrates the behaviors of different models.**

Figure 2: **Our unified probing framework to evaluate visual foundation models on various tasks.**

onto the image plane to obtain their corresponding pixel features. Concretely, for a point \(\), we obtain its projected pixel \(\) on the image \(i\) with

\[}=_{i}_{i}},},},,.\] (1)

In addition, we use an indicator function \((,i)\) to represent whether a point \(\) is visible in the image of the \(i\)-th frame. After finding corresponding pixels of the given point in all image frames, we use mean pooling as an aggregation function \(\) to fuse all pixel features to form the point feature \(f_{}\). Assuming there are \(\) images in total, the projection and aggregation process is represented as:

\[f_{}=_{i=1}^{}((,i) f_{i}( _{i}_{i}})).\] (2)

After projection, we obtain 3D feature fields represented as point cloud feature embeddings for each VFM, and use them as input to the shallow probing heads to evaluate various downstream tasks. To minimize the effect of the model finetuning process, we freeze the parameters for the encoding models to be evaluated, and only tune the linear or shallow probing heads for all tasks.

Models.In this work, we focus primarily on evaluating visual foundation models that are frequently leveraged by recent complex scene understanding and multimodal reasoning models. A complex scene can often be represented in posed 2D images and videos or in 3D point clouds. The image and video modalities sacrifice explicit geometry information, but they preserve rich and dense semantic and textural information of a scene. Conversely, the point cloud modality offers the opposite trade-offs. Additionally, the 2D modalities benefit from strong foundation models trained on vast amounts of data, while 3D point backbones only leverage much smaller datasets.

We categorize visual foundation models into three categories, with an overview of the evaluated models provided in Table 1. For image encoders, we evaluate DINOV2 , LSeg , CLIP , and StableDiffusion (SD) . For the video modality, we evaluate V-JEPA , the state-of-the-art video understanding model succeede VideoMAE [83; 86] for a wide spectrum of perception and reasoning tasks, as well as StableVideoDiffusion (SVD) , a video generative model. The lack of large-scale 3D scene-level datasets hinders the development of strong zero-shot generalizable 3D foundation models as opposed to their 2D counterparts. However, for comparison, we evaluate Swin3D , a 3D backbone that achieves leading performance in zero-shot perception tasks in multiple evaluation datasets compared to previous methods [38; 91; 109]. Swin3D is pretrained on Structured3D , a dataset 10 times larger than ScanNet . In addition, we also evaluate the SAM model , an open-world instance segmentation model pretrained on the SA-1B  dataset, and the Uni3D model , which is an object-centric 3D foundation model pretrained on a mixture of datasets proposed by OpenShape . The detailed results of the evaluation of these two models are provided in the supplementary material.

Feature visualization.Figure 3 visualizes the features of representative scenes extracted by the vision foundation models. To visualize a high-dimensional feature space with \(C\) channels, we apply principal component analysis (PCA) to reduce the feature dimensions to three, normalize them to the range \(\), and interpret them as RGB color channels. We demonstrate several representative foundation models' feature visualization, which reveals many intuitive findings. The image models, DINOV2 and LSeg, demonstrate strong semantic understanding, with LSeg exhibiting clearer discrimination due to its pixel-level language semantic guidance. The diffusion-based models, SD and SVD, in addition to their semantic modeling, excel at preserving the local geometry and texture of the scenes because of the generation-guided pretraining. The video models, SVD and V-JEPA, showcase a unique ability to identify different instances of the same semantic concepts, such as the two trees in the first scene and the chairs in both scenes. The 3D model, Swin3D, also exhibits strong semantic understanding. However, due to limited training data and domain shift, its quality is not on par with the image foundation models, despite being pretrained on perfect semantic annotations.

### Vision-Language Reasoning

The vision-language reasoning task requires a model to engage in dialogues or answer questions about global understanding and local concepts related to a given complex 3D indoor scene. Following existing methods [36; 112], we formulate this as a visual-question answering (VQA) task using large language models (LLMs) as the backbone - given a 3D scene from multi-view images and point clouds, and a user-prompt question, the LLMs are asked to generate the answer to the question in an auto-regressive way. This task encompasses universal language-guided reasoning of the complex indoor scene, ranging from global layout to local details.

Datasets and optimization.We evaluate the performance on two challenging indoor 3D VQA datasets: ScanQA  and SQA3D . Following the evaluation methodology of , we report the metrics BLEU , ROUGE , METEOR , and CIDEr . We finetune a Q-Former module  to align features from different encoders to the LLM input space. More dataset and optimization details are provided in the supplementary material.

Evaluation results.Table 2 and Figure 4 present the results of our evaluation. We observe that image and video encoders generally outperform the 3D point encoder, with DINOV2 achieving the best performance, followed closely by V-JEPA and SVD. Interestingly, we find that for LSeg and CLIP, which are pretrained by language guidance, _their advantage in language alignment does not translate into superior performance on the LLM-guided VQA task_. This finding challenges the common practice of using language-pretrained VFMs  as default encoders for LLM-based vision-language reasoning tasks. Instead, it suggests the importance of considering a wider range of encoders, such as DINOV2 and V-JEPA, to support such tasks.

### Visual Grounding

Visual grounding is the task of locating an object in a 3D scene based on a text description. Compared to the 3D VQA task, visual grounding places a greater emphasis on object-level reasoning and matching capabilities. The task can be broken down into two sub-tasks: object detection and target discrimination (matching the text description with the target object). Although some methods focus on learning models to tackle both tasks , others primarily focus on the discrimination problem  by assuming access to ground-truth bounding boxes. For simplicity and to prevent task entanglement, we adopt the latter setting in our evaluation. More specifically, given a 3D scene in the form of multi-view images and point clouds, a free-form language description of objects, and the ground-truth 3D bounding boxes of all objects in the scene, our model's objective is to find the correct objects in the scene that match the language description. We believe that the object detection task requires semantic information from the visual encoder, which is similar in nature to the semantic segmentation task and will be analyzed in Section 3.4.

For the target discrimination task, we first obtain the feature for each object in the scene by taking the average pooling of all points inside its ground truth bounding box. Following Multi3DRefer , we use a CLIP text encoder to tokenize the text description, and adopt the attention head in  to fuse the text and visual embeddings from the previous steps and output an object score.

   } &  \\ 
**Model** & BLEU-1 & BLEU-4 & METEOR & ROUGE & CIDEr & EM-1 & BLEU-1 & METEOR & ROUGE & CIDEr \\ 
3D-LLM  (_for ref_) & 39.3 & 12.0 & 14.5 & 35.7 & 69.4 & 48.1 & 47.3 & 35.2 & 48.6 & 124.5 \\  DINOV2 & 39.2 & 134 & 15.3 & 36.8 & 73.2 & 50.1 & 49.5 & 35.6 & 50.7 & 129.1 \\ LSeg & 36.8 & 11.5 & 14.6 & 36.0 & 71.0 & 47.4 & 46.5 & 33.2 & 47.8 & 122.5 \\ CLIP & 36.4 & 10.7 & 14.4 & 36.0 & 70.3 & 48.1 & 47.3 & 34.6 & 48.6 & 124.5 \\ StableDiffusion & 35.5 & 11.7 & 14.1 & 34.9 & 68.2 & 47.7 & 47.2 & 33.6 & 48.3 & 124.0 \\  V-JEPA & 37.4 & 12.1 & 14.7 & 36.7 & 71.4 & 48.4 & 48.1 & 34.8 & 50.0 & 125.7 \\ StableVideoDiffusion & 38.5 & 12.5 & 14.5 & 35.4 & 70.6 & 48.5 & 47.9 & 34.4 & 49.0 & 127.7 \\  Swin3D & 36.1 & 10.5 & 13.9 & 35.4 & 70.0 & 48.3 & 48.0 & 34.1 & 47.3 & 123.9 \\   

Table 2: Evaluation of vision-language reasoning on ScanQA  and SQA3D  datasets. The top-2 results for each metric are shown in red and green, respectively. The 3D-LLM results  are shown for reference, indicating the relative position of our evaluation results with respect to the leading models trained on this task.

Figure 4: Evaluation curves on the ScanQA benchmark. The \(x\)-axis demonstrates models trained for different epochs. DINOV2 exhibits clearly superior performance.

Dataset.We evaluate on the ScanRefer  dataset, which provides 51K text descriptions of 11K objects in 800 ScanNet scenes . We report accuracy for _unique_, _multiple_, and _overall_ categories, with _unique_ referring to instances that have a unique semantic class in a given scene (easier).

Optimization.The model is trained with a cross-entropy loss using the AdamW  optimizer following . We train our models for 30 epochs until convergence.

Evaluation results.Table 3 presents our results, which show that video encoding models demonstrate significant advantages over image and 3D encoders. The performance gap primarily lies in the _multiple_ category, indicating that these models excel at discriminating the correct object among multiple objects of the same semantic category. This capability largely stems from the temporally continuous input frames, which provide instance-aware multi-view consistent guidance. In comparison, the image encoder LSeg, with its language-guided pretraining features aligned with language semantics, can also achieve high accuracy in the _unique_ category. However, its performance drops significantly in the _multiple_ category.

Insights from vision-language tasks.Our evaluation of vision-language reasoning and visual grounding reveals several key findings: (1) The DINOV2 unsupervised image learning model demonstrates strong generalizability and flexibility in global and object-level vision-language tasks. (2) Video encoders benefit from temporally continuous input frames and learn to distinguish instances of the same semantics in a scene, which is highly valuable for object-level understanding tasks. (3) Visual encoders pretrained with language guidance do not necessarily lead to strong performance in other language-related evaluation tasks. These findings suggest exploring a more flexible encoder selection in future vision-language tasks to optimize performance and generalization.

### Semantic Segmentation

Semantic segmentation is the task of predicting semantic labels at each 3D position, which requires fine-grained semantic awareness of the scenes. As mentioned in Section 3.1, all types of features are unified in the form of point clouds; therefore, semantic labels are predicted for each point within the point cloud in our setting. More specifically, given a 3D scene in the form of multi-view images and point clouds, the objective in this task is to predict the semantic label for every point in the cloud.

Dataset.We conduct the experiments on the ScanNet  segmentation dataset which has 1,201 and 312 scenes for training and validation, respectively, with a total of 20 semantic classes for evaluation.

Optimization.To make the semantic prediction performance better reflect the fine-grained semantic understanding capability of different features, we use a single linear layer followed by a Sigmoid function to perform a linear probe to predict the probability distribution \(^{N C}\) for all the labels from the foundation model feature \(^{N d}\): \(=(())\), where \(N\) is the number of points in each point cloud, \(d\) is the feature dimension, and \(C\) is the number of classes for segmentation.

  
**Model** & Unique \(\) & Multiple \(\) & Overall \(\) \\  M3DRefer  (_for ref_) & 88.0 & 46.1 & 54.3 \\  DINOV2 & 87.0 & 43.4 & 52.0 \\ LSeg & 88.1 & 41.2 & 50.4 \\ CLIP & 86.5 & 41.6 & 50.4 \\ StableDiffusion & 86.4 & 41.9 & 50.6 \\  V-JEPA & 85.6 & 44.9 & 52.9 \\ StableVideoDiffusion & 88.0 & 46.5 & 54.7 \\  Swin3D & 85.7 & 43.2 & 51.6 \\   

Table 3: Evaluation of 3D object grounding on ScanRefer . Video models exhibit significant advantages.

Figure 5: Visualization of 3D semantic segmentation on ScanNet . Image encoders obtain better performance.

We adopt the standard Adam optimizer  with a learning rate of 1e-4 and use a cross-entropy loss to train the linear layer for 20 epochs.

Evaluation results.Table 4 and Figure 5 demonstrates that image encoders have better performance than video and 3D encoders on 3D semantic segmentation tasks. The reason is that image encoders like DINOV2 and LSeg gain their semantic awareness during training with contrastive objectives via either SSL or language-driven guidance. In comparison, video encoders have the risk of over-smoothing the multi-view information during multi-frame integration, which may harm the fine-grained semantic understanding capability. As for 3D encoders like Swin3D, the data scarcity in 3D compared to 2D for training the foundation models leads to inferior performance on semantic understanding.

### Registration: Geometric Correspondence

To evaluate the geometric information contained in the VFM features, we design the following new task, _partial scene registration_, based on the point cloud registration  task that performs homography estimation between two point clouds. From a complete point cloud representing the entire scene, we sample two point clouds \(P_{1}^{N_{1} 3}\) and \(P_{2}^{N_{2} 3}\) within the scene, corresponding to two sets of consecutive viewpoints which have a certain amount of overlapped region but are displaced with a homography transformation. Our goal is to find the homography matrix \(H\) that correctly transforms the points in \(P_{1}\) to register with \(P_{2}\). Compared to the semantic segmentation task evaluated in Section 3.4, the partial scene registration task requires the foundation model features to have the capability of finding _geometric correspondence_ for registration, which cannot be achieved simply by finding the correspondence according to semantic understanding. For example, in semantic correspondence, we may find two semantically similar points, one on the left side of the sofa in \(P_{1}\), while the other on the right side of the sofa in \(P_{2}\). As a result, if we register the two partial point clouds solely based on semantic correspondence, we will fail to find the correct homography to align one point cloud with the other. The VFMs need to be equipped with geometric understanding capability to achieve decent performance on our partial scene registration task.

Dataset.We build our partial scene registration benchmark based on ScanNet  dataset. For each scene in ScanNet, we choose views #0 \(\) #31 and views #32 \(\) #63 to render \(P_{1}\) and \(P_{2}\), respectively, so that they can have a certain level of overlap that allows the registration of two partial point clouds. Afterwards, \(P_{2}\) is transformed by a homography \(H\) that consists of a rotation \((3)\) and a translation \(^{3}\). \(\) is created by a randomly generated quaternion \(^{4}\) for each scene, while each component of \(\) is randomly sampled from the uniform distribution \([-1.0,1.0]\).

Optimization.We follow REGTR  to adopt a transformer cross-encoder module to enable cross-reasoning of the foundation model features from two point clouds, followed by a lightweight decoder to obtain the corresponding position of every point in the other point cloud for all the \(N_{1}+N_{2}\) points in both point clouds, forming altogether \(N_{1}+N_{2}\) pairs of correspondences, where \(N_{1}\) and \(N_{2}\) are the number of points in \(P_{1}\) and \(P_{2}\), respectively. Afterward, the rotation \(\) and the translation \(\) can be obtained in a closed-form solution solved by a weighted version of the Kabsch-Umeyama  algorithm. We use Adam  for optimization and train our model for 30 epochs, and follow REGTR

  
**Model** & Acc \(\) & mAcc \(\) & mIoU \(\) \\  GrowSP  (_for ref._) & 73.5 & 42.6 & 31.6 \\  DINOV2 & 82.5 & 75.4 & 62.8 \\ LSeg & 78.2 & 58.5 & 47.5 \\ CLIP & 39.7 & 7.2 & 3.4 \\ StableDiffusion & 77.2 & 55.5 & 42.6 \\  V-PEPA & 58.7 & 13.2 & 8.1 \\ StableVideoDiffusion & 71.5 & 40.5 & 30.4 \\  Swin3D & 78.0 & 44.8 & 35.2 \\   

Table 4: Evaluation of semantic segmentation on ScanNet  benchmark.

  
**Model** & RR@0.05m (\%) \(\) & RR@0.1m (\%) \(\) & RR@0.2m (\%) \(\) & RRE (\(@math@degree\)) \(\) & RTE (m) \(\) \\  DINOV2 & 82.1 & 93.9 & 96.8 & 1.72 & 0.14 \\ LSeg & 4.8 & 23.7 & 63.8 & 9.80 & 0.59 \\ CLIP & 18.6 & 51.3 & 78.2 & 7.96 & 0.44 \\ StableDiffusion & 91.7 & 96.8 & 98.4 & 1.15 & 0.09 \\  V-JETA & 90.4 & 96.5 & 99.4 & 1.37 & 0.10 \\ StableVideoDiffusion & 96.8 & 99.0 & 99.7 & 0.83 & 0.06 \\  Swin3D & 60.3 & 81.1 & 91.3 & 3.60 & 0.23 \\   

Table 5: Evaluation of partial scene registration on ScanNet . We employ Registration Recall (RR) at various RMSE thresholds, Relative Rotation Error (RRE), and Relative Translation Error (RTE) as evaluation metrics. A higher RR indicates better performance, while lower RRE and RTE values signify superior results.

to adopt Registration Recall (RR), Relative Rotation Error (RRE), and Relative Translation Error (RTE) as evaluation metrics.

Evaluation results.Table 5 demonstrates the results for the partial scene registration. We can observe that StableDiffusion and StableVideoDiffusion showcase superior geometric capability in our partial scene registration task. It demonstrates that the pretraining objective of _generation_ empowers the foundation models to have a decent capability of finding geometric correspondences in 3D scenes. Another observation is that video encoders generally perform better than image encoders. The reason is that video foundation models have a better understanding of object shapes and geometry within the scenes from the multi-view input frames.

## 4 Analysis

The purpose of this section is to provide additional exploration towards the optimal usage of visual foundation models. The selection of encoding methods requires consideration of the trade-off between memory usage, running time, and performance. We will dive into complexity analysis and the study of design choices for various and a combination of foundation models. More visualization, ablation experiments, and elaboration on the limitations, broader impact, and future direction are presented in the supplementary material.

### Complexity Analysis

We compare memory usage, computation time, and model performance (_vision-language reasoning on ScanQA_) in Table 6 and Figure 6. Our findings show that image encoders generally require less time to process a sample compared to video and 3D encoders. And diffusion-based models, when used for feature extraction, require significantly more memory than other discriminative models. However, the drawbacks in running time become evident for 2D backbones, especially image encoders, when attempting to obtain a scene embedding by aggregating multi-view image embeddings. To illustrate this, we consider a 300-frame video as an exemplar of posed 2D information for a complex scene (a 10-second video at 30 FPS). As the length of the video increases, 2D methods, which necessitate feature extraction for each image frame, rapidly consume a substantial amount of time to process a single scene. In contrast, a 3D point encoder requires significantly less time to process a scene. Nevertheless, 3D encoders exhibit relatively poor model performance, which can be attributed to the scarcity of training data. To fully demonstrate their potential in scene understanding tasks, efforts should be directed toward enhancing the generalizability of 3D foundation models. All analyses and computations are performed on an NVIDIA A100 GPU.

  
**Model** & Time (_sample_) & Time (_scene_) & Mem. \\  DINov2 & 25.0 _ms_ & 7.5 _sec_ & 1.19 G \\ LSeg & 291.2 _ms_ & 87.4 _sec_ & 2.51 G \\ CLIP & 34.5 _ms_ & 10.4 _sec_ & 1.19 G \\ StableDiffusion & 42.7 _ms_ & 12.8 _sec_ & 5.08 G \\  V-JPEA & 175.1 _ms_ & 3.3 _sec_ & 1.31 G \\ StableVideoDiffusion & 667.1 _ms_ & 12.5 _sec_ & 11.70 G \\  Swin3D & 937.4 _ms_ & 0.9 _sec_ & 1.34 G \\   

Table 6: Complexity analysis of visual foundation models.

Figure 6: Memory usage of different encoders. An ideal model should be a small circle and be positioned in the upper left.

Figure 7: Evaluation on different video downsampling strategies for V-JEPA on the segmentation task. _Keyframe Sampling_ samples every \(N\) frames to form a new video sequence, while _Clip Sampling_ directly samples consecutive video clips. The performance before downsampling is regarded as 100%. Keyframe sampling demonstrates less performance drop with the same level of downsampling.

### Ablation Study - Insights into Optimal Usage of Visual Foundation Models

**Video downsampling strategy.** Long and high frame-per-second videos take a lot of space to store and time to process. We explore two straightforward ways of conducting temporal downsampling to achieve more efficient processing without sacrificing too much performance. As shown in Figure 7, we explore the _keyframe sampling_ (blue) and _clip sampling_ (orange) strategies. We can observe that keyframe sampling is a better strategy than clip sampling in this setting, more wisely balancing the trade-off between video processing overhead and task performance.

**Combination of multiple encoders.** We explore whether a mixture of foundation models (experts) has the potential to strengthen the capability of 3D scene understanding. We experiment on the 3D semantic segmentation task with three feature sources: LSeg, StableDiffusion, and Swin3D. When combining different feature sources, we concatenate all features along the channel dimension for every point in the point cloud. The results are shown in Figure 8. After combining features from different sources, there exists a potential that the semantic understanding capability can be boosted in a _mixture of experts_ manner. However, it is not necessarily true that combining the best features will lead to the best performance. For example, LSeg **(1)** has stronger capability on semantic segmentation than StableDiffusion **(2)** and Swin3D **(3)** individually, but it is StableDiffusion + Swin3D **(2+3)** that reaches the best performance when combining two features together.

### Diffusion Noise Level and Feature Layer

In Table 7, we evaluate the effect of different noise level (_noise steps_) and different feature layers in the decoder module in leveraging StableDiffusion (SD)  for feature extraction. The results show that for SD, adding noise \(t<100\) steps in general leads to the best performance. When \(t\) increases beyond \(100\) steps, the performance starts to downgrade. As for decoder layers, the decoding portion of the UNet consists of 4 blocks. We skip the final layer closest to the output and consider layers 0, 1, and 2. The results demonstrate that the output features of the layer one decoder lead to the best performance. These observations are consistent with the study in [6; 103].

## 5 Conclusion

This paper presents the first comprehensive analysis of leveraging visual foundation models for complex 3D scene understanding. We explore the strengths and weaknesses of models designed for various modalities and trained with different objectives. Our study reveals the superior performance of DINOv2, the advantages of video models in object-level tasks, and the benefits of diffusion models in geometric registration tasks. Surprisingly, we find limitations of language-pretrained models in language-related tasks. The extensive analysis suggests that a more flexible encoder selection and fusion can play a crucial role in future scene understanding and multimodal reasoning tasks.

  
**Stable Diffusion** & BLEU-1\(\) & BLEU-4\(\) & METEOR\(\) & ROUGE\(\) & CIDEr\(\) \\   \\ \(t=1\)_step_ & 35.3 & 11.6 & 14.0 & 34.5 & 68.5 \\ \(t=25\)_steps_ & 35.6 & 11.5 & 14.0 & 34.2 & 68.3 \\ \(\) & 35.5 & 11.7 & 14.1 & 34.9 & 68.2 \\ \(t=200\)_steps_ & 34.3 & 10.9 & 13.9 & 33.9 & 66.6 \\   \\ \(l=0\) & 33.6 & 10.5 & 13.3 & 32.6 & 65.9 \\ \(\) & 35.5 & 11.7 & 14.1 & 34.9 & 68.2 \\ \(l=2\) & 34.9 & 11.4 & 14.0 & 34.5 & 68.0 \\   

Table 7: Evaluation of diffusion noise level and feature layers when using StableDiffusion  for feature extraction. The settings we choose are highlighted with the grey color.

Figure 8: Evaluation on the segmentation task with **(1)** LSeg, **(2)** SD, **(3)** Swin3D, and their combinations.