# DeTrack: In-model Latent Denoising Learning for Visual Object Tracking

Xinyu Zhou\({}^{1}\)  Jinglun Li\({}^{2}\)  Lingyi Hong\({}^{1}\)  Kaixun Jiang\({}^{2}\)  Pinxue Guo\({}^{2}\)

&Weifeng Ge\({}^{1}\)1  Wenqiang Zhang\({}^{1,2}\)1

\({}^{1}\)Shanghai Key Lab of Intelligent Information Processing,

School of Computer Science, Fudan University, Shanghai, China

\({}^{2}\)Shanghai Engineering Research Center of AI & Robotics,

Academy for Engineering and Technology, Fudan University, Shanghai, China

zhouxinyu20@fudan.edu.cn, jingli960423@gmail.com, lyhong22@m.fudan.edu.cn,

kxjiang22@m.fudan.edu.cn, pxguo21@m.fudan.edu.cn,

weifeng.ge.ic@gmail.com,wqzhang@fudan.edu.cn

###### Abstract

Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model's robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets. The proposed in-model latent denoising tracker achieve real-time speed, rendering denoising learning applicable in the visual object tracking community.

## 1 Introduction

Visual object tracking is a fundamental task in computer vision, which involves localizing and tracking a specific object in a video given its initial position. It finds broad applications in video understanding, surveillance, and robot navigation . State-of-the-art approaches can be broadly categorized into two classes. The first class [17; 51; 41; 8; 19; 55; 29; 56] directly predicts the bounding box of the tracked target based on image features. The second class [6; 43], employs the coordinate autoregression framework.

While the mainstream methods have achieved prominent success, there are still certain issues to be addressed. Methods based on the image-feature regression framework rely heavily on the matching results between the template and the search region, which cannot utilize positional prior. Meanwhile, based on the autoregressive approach, it is necessary to utilize the bounding box from the previous frame to train the model, which can only utilize the existing bounding boxes in the training set. Therefore, during the testing phase, it may exhibit suboptimal performance for some unseen data.

The Diffusion model has achieved significant success in image generation task, allowing the generation of many images not seen in the training set. Inspired by the Diffusion model, we add noise to bounding boxes during training stage. The noisy box can have arbitrary size and position, thereby enhancing the robustness of the model to unseen data during testing stage. As illustrated in Fig.1 (a) and (b), the diffusion model in image generation task requires multiple iterations of U-Net, while in object detection task, denoising is accomplished through multiple iterations of the decoder. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed.

Therefore, motivated by DAE, we propose a novel denoising learning paradigm (_DeTrack_) for visual object tracking that decomposes the denoising learning process in every denoising block with a tracking model. We use templates, search region, and noisy boxes as inputs. During the denoising process, we inject the template and search region as conditions to predict the noises in previous predictions. We repeatedly conduct the conditional denoising process and finally achieve accurate object location prediction. Specifically, as shown in Fig.1 (c), we propose a novel denoising ViT. We decompose a complete denoising process into several denoising blocks within ViT model and implement every denoising operation with a denoising block. Then the denosing learning process can be implemented in a single forward pass of the tracking model, which can reduce the computational cost drastically. To benefit from the in-context information[21; 16; 9; 28; 20; 22], we also put the previously predicted bounding boxes into a trajectory memory, and put the templates from previous frame into a visual memory. We use them as additional conditions to help locate objects more accurately.

Our contributions can be summarized as follows:

Figure 1: **Difference of denoising learning paradigm.****(a)** Diffusion model in image generation task. **(b)** Diffusion model in object detection task. **(c)** The proposed In-model latent denoising learning paradigm. The pink box indicates the denoising module. \( N\) indicates denoising for \(N\) times.

* We propose a novel in-model latent denoising learning paradigm for visual object tracking, which provides a new perspective for the research community. It decomposes the classical explicit denosing process into several denoising blocks and solves the problem with a tracking network in a single forward pass, which is valuable for real applications.
* We present a tracking model including a denoising ViT, comprised of multiple denoising blocks. The denoising process can be completed by progressively denoising through the denoising blocks within ViT. Furthermore, we construct a compound memory in the model that improve the tracking results using visual features and trajectory.
* Experimental results on several popular experiments, including AVisT, GOT-10k, LaSOT, and LaSOT\({}_{ext}\), demonstrate that the proposed method achieve competitive results.

## 2 Related Work

**Visual Object Tracking.** The existing visual object tracking methods can be broadly categorized into two main classes. The first class[1; 11; 48; 46; 8; 19; 41; 32; 31; 51; 10; 35; 44; 54] involves directly regressing the bounding box from image features, the second class [43; 6] treats the bounding box as four distinct tokens, employing an autoregressive model to sequentially predict these four tokens.

In the first class, deep neural networks are initially used to extract visual features, followed by the design of various prediction heads for regressing the bounding box. Since 2016, some prevalent methods have adopted a two-stream framework, employing siamese networks to separately extract visual features from the template and the search region. One type of prediction head [46; 32; 31; 53] uses a branch to predict the possible location of the target and other branches to predict the corresponding bounding box for that location. Another type of prediction head [10; 48; 17] consists of two branches that predict the coordinates of the top-left and bottom-right corners. Subsequently, OSTrack  introduces a one-stream tracking paradigm that combines feature extraction and feature fusion into a single step, achieving a new state-of-the-art performance. For the second class, SeqTrack  proposes transforming the bounding box into four tokens, predicting them sequentially in the order of \(x,y,w,\) and \(h\). When predicting the bounding box, each box requires four passes through the decoder. Another autoregressive method, ARTrack , is similar to SeqTrack but differs in that it incorporates trajectory information in the input to enhance the model's awareness of trajectories.

**Denoising Learning.** DDPM  introduces denoising diffusion learning, which enhances the quality and diversity of generated images by adding noise to and denoising images. Subsequently, denoising learning has experienced explosive growth, being applied in various domains and achieving significant success. In the Super-Resolution field, SR3  leverages DDPM for conditional image generation, employing a stochastic denoising process for super-resolution. Meanwhile, CDM  comprises a sequence of multiple diffusion models, each responsible for generating images with progressively higher resolutions. In video generation, the Flexible Diffusion Model (FDM)  utilizes a generative model designed for sampling arbitrary subsets of video frames, facilitated by a specialized architecture tailored for this purpose. The Residual Video Diffusion (RVD) model  employs an autoregressive, end-to-end optimized video diffusion model. In addition to generative tasks, denoising learning has also found extensive applications in discriminative task. DiffusionDet  applies the diffusion model to object detection, utilizing DDIM  for denoising. However, this approach still requires multiple passes through the decoder for denoising, impacting inference speed.

## 3 Method

In this section, we start by formulating the proposed tracking paradigm learned through denoising learning (Section 3.1). Next, we present our overall model architecture (Section 3.2), which includes a proposed denoising ViT, a box refining and mapping module, and a compound memory.

### How to Formulate the Denoising Learning Tracking Paradigm?

**Image and Box Inputs.** We utilize both visual memory and the search region as conditional inputs \(c\), while introducing noisy boxes \(_{I}\) to predict the true position of the target, where \(I\) represents the \(I\)-th state in the denoising process. The visual memory stores templates, which are cropped based on previous frames. The search region is cropped based on the current frame and encompasses the area where the target may be present. In training stage, inspired by DDPM, we obtain noisy boxes \(_{I}\) by adding Gaussian noise \(\) to the ground truth box \(_{0}\):

\[_{I}=}_{0}+},(0,),\] (1)

where \(=_{j=0}^{T}_{j}\) and \(_{j}=1-_{j}\). \(_{j}(0,1)\) is the variance schedule, \(T\) is the time step.

**Optimization for Denoising Learning.** We take the visual memory and search region as conditional inputs \(c\), and predict the true target position \(_{0}\) from the noisy box \(_{I}\), \(p_{}(_{0}|_{I})\), where \(\) represents the neural network parameters. We aim to maximize the probability \(p_{}\) that the neural network predicts \(_{0}\), enabling the model to predict the true target position:

\[(p_{}(_{0}|_{I},c)).\] (2)

To maximize \(p_{}\), we need to make the predicted \(_{0}^{{}^{}}\) by the network \(f_{}\) close to the ground truth \(_{0}\):

\[_{0}^{{}^{}}=f_{}(c,_{I}),\] (3) \[|_{0}^{{}^{}}-_{0}|.\]

**Decomposes the Denoising Process into Multiple Denoising Block within a Model.** According to the principle of Markov, we can expand Equation 2 into a Markov chain:

\[p_{}(_{0}|_{I},c)=p(_{I})_{i=1}^{I}p_ {}(_{i-1}|_{i},c)=p(_{I})_{i=}^{I}p_{}(_{i-}|_{i},c).\] (4)

In the traditional Diffusion model, each step \(p_{}(_{i-1}|_{i},c)\) is iteratively predicted using a neural network model \(f_{}\). However, our denoising paradigm decomposes the iterations of neural network into the iterations of denosing blocks within a neural network, \(f_{}=\{d_{1},d_{2},,d_{l}\}\), where each denoising block \(d_{l}\) is responsible for predicting a state \(p_{}(_{i-}|_{i},c)\), where \(l\) denotes the number of blocks. This allows our model to complete denoising with only a single forward pass of the tracking model.

**Discussion on the Differences from the Diffusion Model. _The proposed denoising learning tracking paradigm is not a diffusion model._** (1) In the reverse denoising process of diffusion model, sampling a noise from a standard Gaussian distribution introduces randomness, making it more suitable for generating diverse images in image generation tasks. However, bounding boxes for visual object tracking are deterministic. Therefore, our proposed DeTrack does not involve a sampling process in reverse denoising process, making it more suitable for visual object tracking. (2) Each step of diffusion model is predicted recursively using a neural network. The proposed DeTrack predicts states using denosing blocks within a network (3) The diffusion model requires iterative prediction of neural network, whereas our method only requires a single forward pass through the network. Please refer to the Appendix A.1 for detailed analysis.

### Model Architecture

**Inputs representation.** As show in Fig. 2, we use noisy bounding boxes as input and take visual memory and a search region as conditional inputs. Visual memory stores multiple templates. Specifically, gaussian noise is added to the ground truth bounding box to obtain a noisy bounding box \(\{x_{I}^{1*},y_{I}^{1*},x_{I}^{2*},y_{I}^{2*}\}^{4 1}\), where * denotes noise addition, while 1 and 2 respectively denote the upper left corner and lower right corner. Subsequently, the noisy box is mapped to a high-dimensional space by word embedding, resulting in noisy box embedding \(_{I}^{4 C}\). Additionally, we map templates and the search region to templates embedding \(z^{N_{z} C}\) and search embedding \(s^{N_{z} C}\) by a image embedding, where \(N_{z}=n}{16}}{16}\), \(N_{s}=}{16}}{16}\), \(n\) denotes the number of templates, H and W represent the height and width of the image, respectively. For details, please refer to Model implement details in Section 4.1.

**Denoising ViT (In-model Latent Denoising).**

_ViT Transformer Block._ The specific transformer block structure is the same as the ViT transformer block. Therefore, we only introduce integrating the features of templates and search region within the ViT block. Specifically, we perform attention on image embedding. We first obtain \(q_{s}\) (searchquery), \(q_{z}\)(templates query), \(k_{s}\)(search key), \(k_{z}\)(templates key), \(v_{s}\)(search value) and \(v_{z}\)(templates value) through linear layer. The image attention is employed to interact and fuse image embedding:

\[_{Image}(z,s)=(,q_{z}][k_{s},k_{z}]}{ }[v_{s},v_{z}]),\] (5)

where \([]\) denotes concatenation. \(d\) is the dimensionality of the key.

_Denoising Block._ As shown in Fig. 2, the input to the denoising block comprises the noisy box embedding and the search region embedding. These are passed through linear layers to obtain the \(q_{_{i}}\)(box query), \(k_{s}\)(search key), and \(v_{s}\)(search value). Subsequently, a denoising attention mechanism is employed for the **first time** of denoising:

\[_{Denoising}(s,_{i})=(_{i}}k_{s}}{}v_{s}).\] (6)

Then, we incorporate a Feedforward Neural Network (FFN) layer to enhance \(_{i}^{{}^{}}\):

\[_{i}^{{}^{}}=_{Denoising}(s,_{i})+ _{i}.\] (7)

\[_{i}^{{}^{}}=_{i}^{{}^{}}+( _{i}^{{}^{}}),\] (8)

Finally, we use two linear layers to predict noise for the **second time** of denoising. Subtracting the noise from the box embedding yields the result after denoising through a NoisePred module:

\[=(_{i}^{{}^{}}) =(((_{i}^{{}^{ }}))),\] (9) \[_{i-} =_{i}^{{}^{}}-.\]

Denoising is performed through \(l\) Denoising blocks. Ultimately, denoising is accomplished with a single forward pass of the denosing ViT, resulting in denoised box embedding \(_{0}\):

\[_{0}=_{I}-_{j=1}^{l}_{j}.\] (10)

**Box Refining and Mapping.** As shown in Fig.3(a), we start by applying self-attention to the trajectory and denoised box embedding. We maintain that the current box embedding can only attend to its preceding box embedding by an attention mask in the self-attention, introducing temporal information. Subsequently, the output of self-attention is used as a query for cross-attention with the

Figure 2: **The overview of model architecture.****(a)** The model architecture comprises the input representation, the proposed Denoising ViT, and Box Refining and Mapping. It also includes Visual Memory and Trajectory Memory. **(b)** The proposed Denoising Block within Denoising ViT.

image features. After undergoing six times of box refining, we compute the similarity between the refined box and word embedding, apply Softmax to obtain probabilities for different positions in the word embedding, and use the position with the highest probability as the bounding box, which is similar to ARTrack.

**Compound Memory.** We design a compound memory that includes both a visual memory and a trajectory memory. The visual memory enhances the model's ability to adapt to changes in the appearance of the target and the environment in the video. Besides, the trajectory memory enables the model to continue tracking the target even in the presence of occlusions or disappearances.

_Visual Memory._ As shown in Fig.3(b), our visual memory consists of dynamic templates and a fixed template. The first template of dynamic templates is discarded, and a new template is added. Directly updating the template can lead to cumulative errors. Therefore, we propose a collaborative updating mechanism. This involves inputting the search embedding extracted after Denoising ViT into IoUNet to obtain the corresponding IoU score \(s_{1}\). Additionally, the Softmax score from Box Refining serves as a confidence value \(s_{2}\). A collaborative decision on the quality of the new template frame is made based on two threshold values \(_{1}\) and \(_{2}\), determining whether updating.

_Trajectory Memory._ The proposed trajectory memory stores the boxes of the previous 7 frames, using a first-in-first-out (FIFO) approach when a new box needs to be stored. This results in a continuously updated trajectory box used for refining the denoised box. The trajectory memory can provide the model with prior positional information and target size, allowing accurate prediction of the bounding box even in cases of visual occlusion.

## 4 Experiments

### Implementation Details

**Model implement details.** We design two variants of DeTrack with different resolutions as shown in Tab.1.

Our denoising ViT adopts ViT-B  and utilizes MAE for weight initialization, with a total of \(l=12\) denoising blocks. The box refining includes 6 transformer layers for self-attention and cross-attention. Additionally, we trained two models, namely DeTrack\({}_{256}\) and DeTrack\({}_{384}\). The template is cropped based on twice the size of the bounding box, while the search region is cropped based on four times (DeTrack\({}_{256}\)) and five times (DeTrack\({}_{384}\)) the size of the bounding box. To map

   Model & Template Size & Search Region Size & Flops & Speed & Device \\  DeTrack\({}_{256}\) & \(128 128\) & \(256 256\) & 53.0G & 42FPS & RTX3090 \\ DeTrack\({}_{384}\) & \(192 192\) & \(384 384\) & 117.1G & 30FPS & RTX3090 \\   

Table 1: The Floating-Point Operations per Second(FLOPs), and speed of the model variants.

Figure 3: **Box refining and mapping and the updating of visual memory.****(a)** Box refining and mapping introduces the trajectory memory to improve tracking performance. **(b)** Visual memory updating based on collaboratively decision including \(s_{1}\) (IoU score) and \(s_{2}\) (Softmax score).

boxes into a high-dimensional space, we utilize word embedding, similar to Pix2Seq , with the number of bins being 800 and 1200 for DeTrack\({}_{256}\) and DeTrack\({}_{384}\) respectively.

**Training.** Our experiments are conducted on Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz with 252GB RAM and 8 NVIDIA GeForce RTX 3090 GPUs with 24GB memory. In the first stage, there is only visual memory, which randomly samples two frames from the video. The model is trained on full datasets (COCO, GOT-10k, TrackingNet, and LaSOT). A total of 240 epochs are trained, with the learning rate set to 8e-5 for the denoising ViT and 8e-6 for the box refining. The learning rate decreases by a factor of 10 at the 192-th epoch. In the second stage, trajectory memory is introduced to refine the box, and sequential training is adopted. Consecutive frames are sampled from the video, with each frame's prediction result stored in the trajectory memory and updated in a first-in-first-out manner. The training is conducted on three datasets excluding COCO. A total of 60 epochs are trained, with the learning rates decreasing to 4e-6 and 4e-7 for the denoising ViT and box refining, respectively. In the third stage, only IoUNet is trained while other parts are frozen. The learning rate is set to 1e-4, and a total of 40 epochs are trained, with a 10\(\) learning rate decay at the 30-th epoch. For GOT-10k, the learning rate remains consistent with training on the full dataests. In the first stage, we train for 120 epochs, with a 10\(\) decrease in learning rate at the 96-th epoch, followed by training for 25 epochs in the second stage. During the training on GOT-10k, IoUNet is not used. The loss functions is cross-entropy and SloU, which is the same as ARTrack.

**Inference.** During the testing phase, we use the search region and template as image inputs and initialize the box with the previous box (predicted bounding box of t-1 frame). Additionally, the update interval of the visual memory is set to 5 for t <= 100, doubled every 100 frames until t = 500, and then remains 160. While testing on the GOT-10k dataset, the visual memory is updated directly. For other datasets, the IoU score and confidence score is applied to filter templates. The trajectory memory stores seven bounding boxes, updating with a frequency of every frame. Inference is conducted on an NVIDIA GeForce RTX 3090.

    &  &  &  & _{ext}\)} \\   & AUC & OP5 & OP5 & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & AUC & P\({}_{Norm}\) & P & AUC & P\({}_{Norm}\) & P \\  SiamPRN++255  & 39.0 & 43.5 & 21.2 & 51.7 & 61.6 & 32.5 & 49.6 & 56.9 & 49.1 & 34.0 & 41.6 & 39.6 \\ DiMP288  & - & - & - & 61.1 & 71.7 & 49.2 & 56.9 & 65.0 & 56.7 & 39.2 & 47.6 & 45.1 \\ ATOM288  & 38.6 & 41.5 & 22.2 & - & - & - & 51.5 & 57.6 & 50.5 & 37.6 & 45.9 & 43.0 \\ PrDiMP288  & 43.3 & 48.0 & 28.7 & 63.4 & 73.8 & 54.3 & 59.8 & 68.8 & 60.8 & - & - & - \\ Ocean253  & 38.9 & 43.6 & 20.5 & 61.1 & 72.1 & 47.3 & 56.0 & 65.1 & 56.6 & - & - & - \\ Alpha-Refine288  & 49.6 & 55.7 & 38.2 & - & - & - & 65.3 & 73.2 & 68.0 & - & - & - \\ TransT256  & 49.0 & 56.4 & 37.2 & 67.1 & 76.8 & 60.9 & 64.9 & 73.8 & 69.0 & - & - & - \\ ToMP288  & 51.9 & 59.5 & 38.9 & - & - & 67.6 & 78.0 & 72.2 & 45.9 & - & - \\ DAT256  & - & - & - & 72.8 & 83.1 & 68.4 & 65.2 & 69.3 & 73.6 & - & - & - \\ TATrack256  & - & - & - & 73.0 & 83.3 & 68.5 & 68.1 & 77.2 & 72.2 & - & - & - \\ CTTrack256  & 56.3 & 66.1 & 44.8 & 71.3 & 80.7 & 70.3 & 67.8 & 77.8 & 74.0 & - & - & - \\ TAT352  & 48.1 & 55.3 & 33.8 & 67.1 & 77.7 & 58.3 & 63.9 & - & 61.4 & - & - & - \\ KeepTrack352  & 49.4 & 56.3 & 37.8 & - & - & 67.1 & 77.2 & 70.2 & 48.2 & - & - \\ STARK320  & 51.1 & 59.2 & 39.1 & 68.8 & 78.1 & 64.1 & 67.1 & 77.0 & - & - & - \\ AiTAK320  & - & - & 67.9 & 79.0 & 69.6 & 80.0 & 63.2 & 47.7 & 55.6 & 55.4 \\ Mixformer320  & 56.5 & 66.3 & 45.1 & 70.7 & 80.0 & 67.8 & 69.2 & 78.7 & 74.7 & - & - & - \\ OSTrack256  & 54.2 & 63.2 & 42.2 & 71.0 & 80.4 & 68.2 & 69.1 & 78.7 & 75.2 & 47.4 & 57.3 & 53.3 \\ OSTrack34  & 57.7 & 67.3 & 48.3 & 73.7 & 83.2 & 70.8 & 71.1 & 81.1 & 77.6 & 50.5 & 61.3 & 57.6 \\ SwinTrack243  & - & - & - & 71.3 & 81.9 & 64.5 & 67.2 & 70.8 & - & 47.6 & 53.9 & - \\ SwinTrack384  & - & - & - & 72.4 & 80.5 & 67.8 & 71.3 & 76.5 & - & 49.1 & 55.6 & - \\ ROMTrack256  & 57.8 & 67.6 & 48.6 & 72.9 & 82.9 & 70.2 & 69.3 & 78.8 & 75.6 & - & - & - \\ ROMTrack384  & 59.1 & 68.7 & 50.5 & 74.2 & 84.3 & 72.4 & 71.4 & 81.4 & 78.2 & - & - & - \\ F-BDMTrack256  & - & - & - & 72.7 & 82.0 & 69.9 & 69.9 & 79.4 & 75.8 & 47.9 & 57.9 & 54.0 \\ F-BDMTrack384  & - & - & - & 75.4 & 84.3 & 72.9 & 72.0 & 81.5 & 77.7 & 50.8 & 61.3 & 57.8 \\ GGM256  & 54.5 & 63.1 & 45.2 & 73.4 & 82.9 & 70.4 & 69.9 & 79.3 & 75.8 & - & - & - \\ GRM320  & 55.2 & 64.2 & 46.8 & 73.4 & 82.9 & 70.5 & 69.9 & 79.3 & 75.8 & - & - & - \\ SeqTrack256  & 56.8 & 66.8 & 45.6 & 74.7 & 84.7 & 71.8 & 69.9 & 79.7 & 76.3 & 49.5 & 60.8 & 56.3 \\ SeqTrack34  & 57.8 & 67.4 & 48.0 & 74.8 & 81.9 & 72.2 & 71.5 & 81.8 & 77.8 & 50.5 & 61.6 & 57.5 \\ ARTrack256  & - & - & - & 73.5 & 82.2 & 70.9 & 70.4 & 79.5 & 76.6 & 46.4 & 56.5 & 52.3 \\ ARTrack384  & - & - & - & 75.5 & 84.3 & 74.3 & 72.6 & 81.7 & **79.1** & 51.9 & 62.0 & 58.5 \\ 
**DeTrack256 (ours)** & 60.1 & **69.7** & **50.6** & 77.1 & 86.1 & 73.5 & 71.3 & 80.1 &

### State-of-the-Art Comparisons

**AVisT**. The AVisT dataset, as described in , covers a broad spectrum of diverse and demanding situations, encompassing harsh weather conditions like thick fog, intense rainfall, and sandstorms. Our tracker demonstrates outstanding performance on AVisT , a dataset with extreme weather conditions and harsh environments. It outperforms SeqTrack\({}_{384}\) by 2.4% in AUC, substantiating our tracker's excellence in extreme environmental conditions.

**GOT-10k**. GOT-10k comprises a training dataset consisting of 10,000 videos and a testing dataset with 180 videos. There is no overlap between the training and test sets, necessitating trackers to demonstrate robust generalization capabilities towards unseen data. As shown in Tab. 2, our method demonstrates superior performance on the GOT-10k . Our DeTrack\({}_{256}\) achieves a significant improvement in AUC compared to SeqTrack\({}_{256}\), with increases of 3.0% and 2.4%, respectively. Our DeTrack\({}_{384}\) outperforms the state-of-the-art method ARTrack\({}_{384}\) by 2.4%. This is attributed to the non-overlapping nature of the training and testing sets in the GOT-10k dataset, indicating our method's strong performance on unseen data. The denoising learning paradigm has learned powerful denoising capabilities while facing with arbitrary positions and sizes of boxes.

**LaSOT**. LaSOT is benchmark designed for long-term tracking, featuring a test collection consisting of 280 videos. Our DeTrack256 achieves an AUC of 71.3%, exhibiting performance improvement compared to other methods based on 256 resolution. Additionally, our DeTrack384 also demonstrates state-of-the-art performance, validating the strong competitiveness of our approach in long-term dataset. This is attributed to our compound memory design, which leverages historical trajectory and appearance information to enhance the model's generalization ability on long-term dataset.

**LaSOT\({}_{ext}\)**. LaSOT\({}_{ext}\)is an extension of the LaSOT dataset, also categorized as a long-term tracking dataset. It comprises 150 video sequences and encompasses 15 object classes. Our DeTrack384 shows significant improvements compared to other methods, with a 1.7% increase in AUC over SeqTrack384 and a 2.4% improvement in P\({}_{norm}\). This demonstrates the strong generalization capability of our approach even with extended data, particularly manifesting notable advantages in the accuracy of bounding box center point.

### Ablation study on Denoising Learning

**Influence of denoising steps.** We investigate the impact of the number of denoising iterations on the performance of the tracker. Our proposed In-model latent denoising consists of a total of 12 steps based on denosing blocks, requiring only a forward pass to complete denosing. As shown in Tab.3, the model's performance is nearly zero at the first and second denoising steps because the bounding boxes are still filled with noise. However, there is a significant qualitative improvement in model performance at the eighth denoising step, reaching its peak at the twelfth step. As shown in Fig.4, the results improve progressively step by step, consistent with Tab. 3.

**Analysis of denoising paradigm.** Although our method completes denoising with only a single forward pass through the tracking model, it can also be adapted to perform multiple forward passes, similar to traditional Diffusion model. Therefore, we further analyze and compare the multiple

   Denoising paradigm & Steps & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & FLOPS & Speed \\  Multiple forward passes & 96 & 75.7 & 84.8 & 72.9 & 424.0G & 8FPS \\ Multiple forward passes & 48 & 75.7 & 84.6 & 72.5 & 212.0G & 12FPS \\ Multiple forward passes & 24 & 75.9 & 84.9 & 72.4 & 106.0G & 29FPS \\ Single forward pass & 12 & **77.1** & **86.1** & **73.5** & **53.0G** & **42FPS** \\   

Table 4: Ablation study of denoising paradigm on GOT-10k. The best results are highlighted in bold.

    & step1 & step2 & step3 & step4 & step5 & step6 & step7 & step8 & step9 & step10 & step11 & step12 \\  AO & 1.1 & 1.6 & 4.8 & 7.5 & 12.5 & 21.4 & 33.1 & 52.3 & 65.7 & 70.2 & 74.8 & **77.1** \\ SR\({}_{0.5}\) & 0.1 & 0.2 & 1.2 & 2.8 & 8.0 & 17.9 & 34.1 & 57.6 & 74.7 & 78.7 & 83.7 & **86.1** \\ SR\({}_{0.75}\) & 0.0 & 0.0 & 0.2 & 0.8 & 2.9 & 8.0 & 17.8 & 39.1 & 56.9 & 64.6 & 70.5 & **73.5** \\   

Table 3: Ablation study of denoising steps on GOT-10k. The best results are highlighted in bold.

forward passes and single forward pass paradigms, as shown in Tab. 4. In DeTrack, the performance of multiple forward passes is not superior to that of single forward pass. Additionally, if denoising is performed similarly to traditional Diffusion models, the computational cost increases significantly. Single forward pass only requires 53.0G FLOPS and achieves a speed of 42 FPS, while multiple forward passes incurs exponentially higher computational costs with a linear decrease in speed.

**Analysis of the denoising block**. As shown in Tab.5, if there is no NoisePred module, AO will decrease by 2.0%, and SR\({}_{0.5}\) will decrease by 2.0%. This demonstrates that noise prediction and gradually subtracting noise are crucial for the model. Furthermore, removing denoising attention leads to further performance degradation, demonstrating that utilizing image features as conditional inputs can also assist in denoising. Moreover, the computational overhead of the denoising block increased by only 1.80G, owing to the fact that the box comprises merely 4 tokens. Thus, even with the addition of denoising attention and NoisePred, this remains a negligible computational burden.

### Ablation study on Compound Memory

Because the memory mechanism is designed to address the challenge of dynamic changes in video, and considering the greater variety of environmental and appearance changes in long video datasets, we chose the LaSOT (long-term tracking dataset, averaging 2448 frames per video) to validate the effectiveness of our memory mechanism.

**Exploration on the length of the visual memory and the trajectory memory**. We firstly explore the impact of different visual memory lengths. As shown in Fig.5 (a), when the length is only 1, the model's AUC is only 70.2. However, with an increase in memory length, performance gradually improves, reaching its peak at the 3-rd frame. Subsequently, performance declines. This is because when the memory is too short, the model cannot adapt to changes in the target and the environment.

Figure 4: **Visualization of the denoising step GOT-10k.** The first row is the video GOT-10k-Test-000040, the second row is the video GOT-10k-Test-000003, and the third row is the video GOT-10k-Test-000051.

   Denosing block & Denoising attention & NoisePred & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & FLOPS \\   & & & & & 74.0 & 84.7 & 72.5 & 51.2G \\ ✓ & & ✓ & & & 75.1 & 84.1 & 72.0 & 52.7G \\ ✓ & & ✓ & & ✓ & **77.1** & **86.1** & **73.5** & **53.0G** \\  Step1 & Step2 & Step3 & Step4 & Step5 & Step6 & Step7 & Step8 & Step9 & Step10 & Step11 & Step12 \\   

Table 5: Ablation study of denoising block on GOT-10k. The best results are highlighted in bold.

Figure 5: **Ablation study of memory on LaSOT.** (a) Different visual memory lengths; (b) Different trajectory memory lengths; (c) Different IoU thresholds are applied for template updates; (d) The influence of Softmax thresholds. (e) With or without compound memory.

Conversely, when the memory is too long, it stores incorrect information. Unlike visual memory, as shown in Fig. 5 (b), trajectory memory does not exhibit a trend of initially rising and then falling with an increase in stored boxes. The performance consistently improves as the number of boxes ranges from 1 to 7. As shown in Fig.5 (e), we also achieved a performance of 70.2 by removing all memories, which further confirms the effectiveness of our memory.

**Effects of IoU score and Softmax scorefor visual memory updating**. For the update of visual memory, we strive to avoid updating poor templates into our visual memory. This would lead to tracking drift. Therefore, as shown in Fig. 5(d) keeping IoU score fixed, we conduct an ablation study on different Softmax score values. The study found that an accuracy update can be achieved when the Softmax score is set to 0.9, obtaining 71.3% on AUC. As shown in Fig. 5(c) keeping Softmax score fixed, the best IoU score is 0.75. When the IoU score threshold is set to 0.85, it leads to a decrease in AUC. It is because the overly strict condition reduces the frequency of visual memory updates.

## 5 Limitation

Despite achieving real-time speed and competitive performance, our DeTrack still has certain limitations. Existing tracking methods struggle to recover the target when facing challenges such as object occlusion and out-of-view situations. Although our proposed trajectory memory can assist in target reacquisition after target loss in some cases, further improvements are needed to address challenges like object occlusion and out-of-view scenarios. We will investigate the challenges in these scenarios.

## 6 Conclusion

Traditional visual object tracking methods using image-feature regression or coordinate autoregression models faced limitations in handling positional priors and unseen data. Inspired by the diffusion model, we introduced denoising learning to enhance model robustness. Our approach, employing noisy bounding boxes for training, introduces a novel paradigm of denoising learning in object tracking. By decomposing the process into individual denoising blocks within our proposed denoising Vision Transformer (ViT), we achieved real-time performance while maintaining effectiveness. Experimental results demonstrate the efficacy of our method, showcasing competitive performance and rendering denoising learning applicable in the visual object tracking community.