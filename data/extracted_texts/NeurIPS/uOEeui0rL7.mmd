# Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback

Marcel Torne\({}^{1,2}\) Max Balsells\({}^{3}\) Zihan Wang\({}^{3}\) Samedh Desai\({}^{3}\)

**Tao Chen\({}^{1}\) Pulkit Agrawal\({}^{1}\) Abhishek Gupta\({}^{3}\)**

\({}^{1}\)Massachusetts Institute of Technology \({}^{2}\)Harvard University \({}^{3}\)University of Washington

{marcelto,taochen,pulkitag}@mit.edu

{balsells,avinwang,samedh,abhgupta}@cs.washington.edu

###### Abstract

Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy. HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification. The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies. This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses. HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors. Project website at https://human-guided-exploration.github.io/HuGE/.

## 1 Introduction

How should we teach agents a new task? A general method is to provide agents with a reward function and employ reinforcement learning. This may appear to be a straightforward plug-and-play

Figure 1: HuGE leverages noisy and asynchronous feedback from multiple non-expert humans to train robot control policies directly on the real world.

procedure, yet in reality, it usually means iterative human intervention in designing reward functions to overcome exploration obstacles and circumvent potential reward hacking. The cyclical process of a human meticulously crafting a reward function, assessing the resulting behavior of the learned policy, and then refining the rewards to enhance performance can be a painstaking and inefficient process.

If an agent is able to effectively and autonomously explore its environment, it can learn new tasks using an easily designed sparse reward function . This would make the training process less reliant on humans designing detailed reward functions for "dense" supervision. Several strategies incentivize state coverage in exploration by expanding the frontier of visited states and explicitly rewarding discovering novel states.. When purely optimizing for state coverage, the agent _overexplores_ parts of the state space that are not relevant to the task at hand . This quickly becomes a futile effort when exploring in environments with large state spaces, suggesting the need for a more directed exploration strategy.

A promising avenue for directed exploration, without requiring a prohibitive amount of reward design, is in developing techniques that leverage non-expert human feedback to adjust the agent's training . This work is a step towards the overarching objective of learning new skills in the real-world through meaningful human-robot interactions. A challenge with techniques that optimize policies over learned reward functions is that humans may provide noisy feedback, or may not even know the optimal solution for the task at hand , rendering their feedback unreliable and biased, a problem which is exacerbated with a large number of non-expert supervisors. When this noisy and incorrect feedback is used to learn reward functions for RL, it can lead to learning biased and incorrect policies. Requiring human supervisors to provide high-quality feedback synchronously and frequently during the training process becomes prohibitively expensive. If we want to reduce the burden on supervisors it is important to ensure that inaccuracies, biases in the reward function are not reflected directly in the learned policy.

We note that reward functions in reinforcement learning serve two distinct purposes. One, determining which goal or behavior is optimal, and two, determining how to explore the environment to achieve this goal. In this work, we make the observation that these two independent purposes of reward functions can be decoupled. Human feedback can be used to guide exploration, while policy learning techniques like self-supervised reinforcement learning can learn unbiased optimal behaviors for reaching various goals from the data collected via exploration. This provides the best of both worlds: a way to guide exploration using non-expert human supervisors, while avoiding inheriting the bias from providing asynchronous, low-quality feedback.

Human Guided Exploration (HuGE) uses noisy, infrequent, and asynchronous binary human comparative feedback to guide exploration towards subsets of the state space closer to goal, while learning optimal policies via self-supervision. Rather than having humans directly label which states to visit, HuGE solicits users for binary comparisons between visited states to determine which are closer to the goal. These comparisons help train a goal selector which biases exploration towards chosen states that are more proximal to the objective. The process of exploration involves returning to these selected states and subsequently expanding the set of visited states from there. From this exploration data, we learn optimal policies for goal-reaching in a self-supervised manner using ideas from "hindsight" relabelling , hence avoiding reliance on any explicit reward function.

Iterating between exploration guided by the goal selector and self-supervised policy learning, HuGE expands the frontier of visited states until the desired target goals can be reached reliably. Opposed to directly using human feedback as a reward , HuGE lets humans provide noisy, asynchronous, and infrequent feedback, as it is simply used to guide exploration, not directly as an objective for policy learning. In the worst case, when human feedback is incorrect or absent, the exploration is hampered but the policy learning scheme is still convergent to the optimal solution. This way, humans are able to essentially "drop breadcrumbs", incrementally guiding agents to reach distant goals, as shown in Fig 3. This helps us solve more complex tasks than if solely relying on self-supervision and is able to overcome the optimization bias that is often encountered when optimizing over a learned and noisy reward function . Overall, this work presents the following contributions:

**Guided Exploration from Noisy and Minimal Human Feedback**: We propose an algorithm for separating (noisy) human feedback from policy learning by utilizing comparative, binary human feedback to guide exploration while learning a policy through self-supervision. This ensures that we can derive optimal goal-reaching policies even from imperfect human feedback.

**Capability to learn from crowdsourced non-expert human feedback**: We show HuGE can learn from noisy, asynchronous and infrequent human feedback. We validate this by collecting crowdsourced pilot data from over a hundred arbitrarily selected, non-expert annotators who interact with the robotic learning through a remote web interface.

**Demonstration with real-world policy learning**: We show that HuGE can learn policies on a robotic learning system in the real world, demonstrating effective exploration via human feedback and compatibility with pretraining from non-expert trajectory demonstrations as seen in Figure 1.

## 2 Related Work

Our work builds on techniques for exploration methods, goal-conditioned, and human-in-the-loop RL, but presents a technique for learning from noisy, asynchronous and infrequent human feedback. We present the connections to each of these sub-fields below:

**Exploration in RL:** While exploration is a widely studied subfield in RL [10; 43; 5; 37; 9; 21], this is typically concerned with either balancing the exploration-exploitation tradeoff  or maximizing state coverage [5; 10; 37]. This contrasts with our goal of performing _targeted_ exploration, informed by human feedback. Our work is related to ideas from Go-Explore , which explores by maintaining an archive of visited states, returning to states with a low-visitation count, and performing random exploration from there. However, this results in redundant overexploration shown in Figure 2 under the novelty-based exploration paradigm. In contrast, HuGE is able to leverage human feedback to only explore in promising directions as directed by human supervisors, significantly improving sample efficiency.

For goal-reaching problems, alternatives for exploration include self-supervision techniques such as hindsight relabeling [2; 24; 35]. By learning goal-reaching behavior in _hindsight_ for reached states, these methods obtain dense supervision, albeit only for the states that were actually reached. This has been shown to empirically aid with exploration in certain cases, via policy generalization across goals. However, as shown in Figure 3, these methods suffer from _underexploration_, since the policy generalization across states can be challenging to control. In contrast, HuGE does not rely on arbitrary policy generalization, instead revisiting already _reached_ states at the frontier of visited states and performing further exploration from these states.

**Goal-conditioned reinforcement learning:** Goal-conditioned RL algorithms [34; 24; 35; 2; 33; 28; 37] are multi-task RL methods where various tasks are defined as reaching different goal states. A number of different approaches have been proposed for goal-conditioned RL - learning with hindsight relabeling [2; 19; 33; 28], learning latent spaces for reward assignment [42; 48], learning dynamical distances [27; 22] and goal conditioned imitation learning [25; 35; 24; 40]. While these algorithms are able to solve tasks with a simple component of exploration, they can struggle with tasks with complex sequential nature, requiring complex machinery such as hierarchical architectures [33; 39]. In contrast, our work shows the ability to solve sequential goal-reaching problems with standard techniques, just using occasional comparative feedback from human supervisors to guide exploration.

Perhaps most closely related to our work is the paradigm introduced in DDL  where a human supervisor occasionally selects which states to explore from. This state is used to define the reward function, which when combined with self-supervised Q-learning, can aid with exploration for goal-conditioned RL problems. However, exploration is limited to exactly the state selected by the human, whereas in HuGE, learning the parametric goal selector allows the exploration frontier to continue expanding and decreases the overall amount of human feedback needed (Section 5, Figure 8).

**Reinforcement Learning from Human Feedback (RLHF):** RLHF typically characterizes a class of methods that learn reward models from binary human comparative feedback for use in RL. These techniques have seen use in better aligning language models, and guiding learning of embodied agents in simulation with expert feedback [16; 7; 8]. In much of this work, humans are assumed to provide high-quality and frequent feedback to learn rewards. In HuGE, we take a complementary view and show that by considering human feedback in a self-supervised policy learning setting, much of the burden can be placed on self-supervision, rather than on receiving high-quality human feedback. More broadly, human in the loop RL is a well explored concept, with various interfaces ranging from preferences [16; 7; 8; 31] to scalar rewards , language based corrections , binary right/wrong signals  and even sketching out rewards . While we restrict the studyin this work to binary comparative feedback, exploring how the underlying principles in HuGE (of decoupling human feedback and self supervised policy learning) can be extended to other forms of feedback is an exciting avenue for future work.

## 3 Problem Setup and Preliminaries

This work solves goal reaching tasks by using goal-conditioned policy learning methods. The goal reaching problem is characterized by the tuple \(,,,(s_{0}),T,p(g)\), adopting the standard MDP notation with \(p(g)\) being the distribution over goal states \(g\) that the agent is tasked with reaching. We assume sampling access to \(p(g)\). We aim to find a stationary goal-conditioned policy \((|s,g)\): \(()\), where \(()\) is the probability simplex over the action space. We will say that a goal is achieved if the agent has reached the goal at the end of the episode (or is within \(\) distance of the goal). The learning problem can be characterized as that of learning a goal conditioned policy that maximizes the likelihood of reaching the goal \(_{}J()=_{g p(g)}[P_{_{g}} (s_{T}=g)].\)

**Goal-conditioned Policy Learning:** Goal-conditioned RL methods approach this problem using the reward function \(r_{g}(s)=(s=g)\), defining a sparse reward problem: \(_{}_{(|s,g),g p(g)} [_{t=1}^{H}^{t}r_{g}(s_{t})]\). This problem can be difficult to solve with typical RL algorithms [26; 52; 23] because the training process is largely devoid of learning signals.

### Why is exploration in goal-conditioned reinforcement learning challenging?

**Hindsight relabelling**: To circumvent the challenge of sparse rewards in goal-conditioned reinforcement learning, the structure of goal-reaching problems can be exploited using the hindsight relabeling technique [28; 1; 3]. Hindsight relabeling leverages the insight that transitions that may be suboptimal for reaching "command" goals \(g\), may be optimal in _hindsight_ had the actually reached states \(s_{T}\) been chosen as goals. This allows us to relabel a transition tuple \((s,a,s^{},g,r_{g}(s))\), with a hindsight tuple \((s,a,s^{},g^{},r_{g^{}}(s))\), where \(g^{}\) can be arbitrary goals chosen in hindsight. When \(g^{}\) is chosen to be states \(s\) actually visited along a trajectory, the reward function \(r_{s}(s)=1\) provides a dense reward signal for reaching different goals \(g=s\), which can be used to supervise an off-policy RL algorithm  or a supervised learning algorithm [1; 40; 47; 24].

The key to exploration in these methods is _generalization_ - even though the desired goal distribution \(p(g)\) is not accomplished, learning policies on the _accomplished_ goal distribution \(p(g^{})\) influences exploration towards \(p(g)\) via generalization between \(g^{}\) and \(g\). However, this form of exploration is unreliable across domains and goal representations. As shown in Figure 2 and in Appendix E, this often _underexplores_ the environment without accomplishing the target goal.

**Novelty-based exploration**: On the other hand, pure novelty-based exploration [21; 51; 5; 44] is also ineffective, since it performs task agnostic exploration of the entire state-space, thereby _overexploring_ the environment (Fig 2). This becomes intractable with large state and action spaces.

**Directed exploration (Ours)**: A practical solution would explicitly encourage exploration but directed towards the goal. In the following sections, we will argue that providing noisy, asynchronous and infrequent binary comparison feedback by a variety of human users can provide this directed exploration, without inheriting the noise and bias of the provided feedback.

Figure 2: Comparison of exploration types. **hindsight relabelling**: suffers from exploration _collapse_, **novelty-based exploration** suffers from _overexploration_, we propose directed exploration from human feedback _Ours_.

## 4 HuGE: Guiding Exploration in Goal-Conditioned RL with Human Feedback

The key idea behind HuGE is to leverage (noisy) human feedback for guiding exploration and data collection, but to decouple this from the process of learning goal-reaching policies from the collected data. We show how this can be done in a self-supervised way, allowing unbiased goal-reaching policies to be learned, while human feedback can still be useful in guiding exploration. We describe each component of our proposed algorithm below.

### Decoupling Human Feedback from Policy Learning for Goal Reaching Problems

While human feedback has been shown to be an effective tool to guide exploration [16; 8], policies obtained by maximizing reward functions learned from noisy and infrequent human feedback can be very suboptimal, often getting trapped in local optima. This places a significant burden on human supervision, making it challenging for non-expert, casual supervisors to guide robot learning. _How can we enable policy learning from infrequent and noisy human feedback?_

Our key insight is that human feedback and policy learning can be disentangled by decoupling the process of exploration from the policy learning, while only leveraging the human feedback on the exploration process. In this new paradigm, where the generation of exploration data does not necessitate optimality, we can harness guiding signals originating from noisy human feedback, yet still effectively learning optimal goal-conditioned policies.

In particular, for human guided exploration we build on the idea of frontier expansion [21; 37; 14], where exploration is performed by revisiting states at the frontier of visited states and then continuing exploration from these. This technique would maintain a frontier \(\) that consists of the set of all visited states. We can then select a previously visited state for the policy to revisit, which we will call _breadcrumb_ state \(g_{b}\), by querying a "goal selector" on \(\), that is learned from (noisy) human feedback and defined in detail in 4.2. In this way, human feedback can be used to "softly" guide exploration towards promising states \(g_{b}\). Frontier _expansion_ can then performed by executing random exploration from the reached breadcrumb state \(g_{b}\), and adding the collected data to the replay buffer for self-supervised policy learning.

Given data collected by frontier expansion directed using human feedback, goal-reaching policies can then be learned by leveraging self-supervised policy learning techniques [24; 2; 35] that are able to learn policies from collected data using "hindsight" relabeling and supervised learning. Importantly, this self-supervised policy learning procedure does _not_ depend on the human feedback at all and is purely self-supervised. The decoupling of exploration and policy learning implies that even if this exploration is biased and imperfect, the self-supervised policy learns _unbiased_ paths to reach all goals that were visited. In this way, cheap low-quality supervision from non-expert human feedback can

Figure 3: Overview of HuGE. We train a goal selector, from human feedback through state comparisons, to perform directed exploration in self-supervised learning.

be used to ensure guided frontier expansion towards relevant subsets of the state space, while still being able to learn unbiased goal-reaching policies. Below, we delve into each of these components - guided frontier expansion and self-supervised policy learning and describe the overall algorithm in Algorithm 1.

### Guiding Exploration from Human Comparative Feedback by Learning Goal Selectors

To direct exploration more effectively, we hypothesize that receiving small amounts of occasional guidance from non-expert human users can be beneficial, even if the feedback is biased and noisy. To leverage this feedback, we learn a parametric goal selection function directly from binary human comparison feedback. Learning a parametric goal selection function allows this feedback to be useful even when humans are providing feedback infrequently and asynchronously.

**Learning State-Goal Distances from Binary Comparisons**: We propose a simple interface between the human and the algorithm that simply relies on the human supervisor to provide binary comparisons of which of two states \(s_{1}\), \(s_{2}\) is closer (given a human's judgment) to a particular goal \(g\) (as demonstrated in Fig 3 and Appendix Fig B.4). These binary comparisons can be used to train an unnormalized estimate of distances \(f_{}(s,g)\) by leveraging the Bradley-Terry model of choice [16; 8; 7]:

\[_{}&(s_{1}>s _{2}|g)(s_{1},g)}{ f_{}(s_{1},g)+ f_{ }(s_{2},g)}+\\ &(1-(s_{1}>s_{2}|g))(s_{2},g )}{ f_{}(s_{1},g)+ f_{}(s_{2},g)}\] (1)

This objective encourages states \(s\) closer to particular goals \(g\) to have smaller \(f_{}(s,g)\). While this estimate may be imperfect and noisy, it can serve to bias exploration in promising directions.

**Using the Learned State-Goal Distances for Biasing Exploration:** To guide exploration, we can select which breadcrumb state \(g_{b}\) to command and start exploring from during frontier expansion, by sampling states inversely proportional to their distance to the goal measured as, \((-f_{}(s,g))\), where \(f\) is learned above. This encourages guiding exploration towards states that have a lower estimated distance to the goal since these are more promising directions to explore, as indicated by the human comparisons. \(g_{b} p(g_{b}|g);p(g_{b}|g)=(g_{b},g)}{_{g ^{}}- f_{}(g^{},g)}(2)\) where \(\) represents the set of reached states. Sampling through a _soft_ distribution instead of choosing the state with the minimum estimated distance, can overcome noise and errors in the learned distance function, albeit at the cost of slower (yet convergent) exploration. The choice of softmax temperature \(\) determines how much the exploration algorithm trusts the goal selector estimate (large \(\)), versus resorting back to uniform frontier expansion (small \(\)). The key to effectively using this state-goal distance function, is getting away from directly optimizing the reward function learned from human preferences, avoiding the aforementioned problems regarding the optimization of the reward function learned from human preferences and instead relying on self-supervision for policy training, Section 4.3.

### Self-Supervised Policy Learning: Hindsight Relabeled Learning for Goal-Conditioned Policies

Given exploratory data collected by guided frontier expansion (Algorithm 2, Section 4.1), we can leverage a simple self-supervised learning scheme building on [1; 47; 24] for goal-conditioned policy learning. Given trajectories \(=\{s_{0},a_{0},s_{1},s_{2},,s_{T},a_{T}\}_{i=1}^{N}\) we to construct a dataset of optimal tuples using: \(_{}=\{(s_{t},a_{t},g=s_{t+h},h):t,h>0,t+h T\}(3)\). This relabeled optimal dataset can then be used for supervised learning: \(J_{}()=_{_{g}[_{}( |g)]}[_{t=0}^{T}(a_{t}|s_{t},g)](4)\). This process can be repeated, iterating between collecting data, relabeling it, and performing supervised learning. As policy learning continues improving, the learned policy can be deployed to solve an expanding set of goals, eventually encompassing the desired goal distribution. The overall pseudocode of HuGE is shown in Algorithm 1. We note that this relabeled supervised learning scheme has been studied in past works [1; 47; 24], the unique contribution here being how low-quality human comparative feedback is used to guide exploration for self-supervised policy learning. While prior techniques such as [1; 47; 24] often struggle with exploration, the occasional human feedback can lead to significantly more directed exploration behavior for HuGE as compared to prior work.

Bootstrapping Learning from Trajectory Demonstrations:While the system thus far has described a strategy that is applicable for learning policies from scratch, training HuGE from scratch can be prohibitively slow in the real world, and instead we may look to finetune already pre-trained "foundation" models or existing goal-directed policies using HuGE, or to pretrain from human demonstrations as considered in a huge plethora of prior work [49; 32; 6; 41].

Given that the self-supervised policy learning method in HuGE is fundamentally based on iterated supervised learning, a natural way to ensure the practicality of the method is by simply initializing the policy \(\) using supervised imitation learning on trajectory demonstrations (or from other sources if available). While this technique is effective on policy pretraining, it fails to account for the goal selection model \(f_{}(s,g)\). A simple modification to the training process allows _also_ for effective pre-training the state-goal distances model \(f_{}(s,g)\) from demonstrations. Concretely, since demonstrations are typically monotonically decreasing in terms of effective distance to the goal, a single trajectory \(\) can be reinterpreted as a set of \(\) comparisons, where later states in the trajectory are preferred to earlier ones: \(\{s_{t_{1}}<s_{t_{2}}:t_{1}<t_{2} s_{t_{1}},s_{t_{2}}\}\). These comparisons can then be used to pre-train the goal-selector, as discussed in Equation 1. Our proposed technique is not a replacement for learning from demonstrations, but rather serves a complementary role. As we show experimentally, this combination of HuGE and pre-training from non-expert demonstrations actually enables learning in the real world under practical time constraints.

```
1:Input: Human \(\), goal distribution \(p(g)\)
2:Initialize policy \(\), goal selector \(f_{}\), data buffer \(\), goal selector buffer \(\)
3:while True do
4: Sample goal \(g p(g)\)
5:\(_{}(,,_{ },,)\)
6:\((_{ })\) (3)
7:\((,)\) (4)
8:\((, )\) (4.2)
9:\(f_{}(f_{},)\) (1) ```

**Algorithm 1** HuGE: Guided Exploration with Human Feedback

## 5 Experimental Evaluation

In this work, we show that HuGE learns to successfully accomplish long-horizon tasks, and tasks with large combinatorial exploration spaces through little human supervision. To demonstrate these experimentally, we test on several goal-reaching domains in simulation, shown in 4, in the MuJoCo  and PyBullet  simulators where we compare against state-of-the-art baselines. Furthermore, we show the benefits of our method by learning policies directly on a real-world LoCoBot robot. We additionally show the ability for HuGE to be used with an uncurated, large-scale crowdsourcing setup with non-expert human supervisors. With these experiments, we show **1)** HuGE outperforms prior work on solving long horizon goal-reaching tasks in simulation; **2)** HuGE is suitable for collecting crowdsourced, noisy and asynchronous feedback from humans all over the world with different backgrounds and education levels; **3)** HuGE is suited to learning in the real world with a robotic platform in practical time-scales, directly from visual inputs.

Figure 4: Six simulation benchmarks where we test HuGE and compare against baselines. **Bandu, Block Stacking, Kitchen**, and **Pusher**, are long-horizon manipulation tasks; **Four rooms** and **Maze** are 2D navigation tasks, see Appendix D

### Learning Goal-Conditioned Policies with Synthetic Human-in-the-Loop Feedback in Simulation

We consider goal-reaching problems in the six domains shown in Fig 4. These are domains with non-trivial exploration challenges --the agents must assemble a structure in a specific order without breaking it, navigate around walls, etc., and purely random exploration is unlikely to succeed. We evaluate HuGE compared to the baselines described in Appendix E on these domains. These baselines were chosen to compare HuGE with methods that perform different types of exploration, hindsight relabeling, and methods that use human preferences to learn a reward function instead, to highlight the benefits of decoupling exploration from human preferences and policy learning. We report the number of final goals reached successfully in Fig 5 as learning progresses. Only in these and the analysis experiments, the human feedback is synthetic (see Appendix D).

In Figure 5 and in Appendix E with further detail, we show HuGE learning a goal selector model to perform goal selection (_Ours_) matches, at convergence, the performance of using an _Oracle_ always choosing the best goal to explore. Guiding exploration (_Ours_) is _significantly_ better than techniques that perform indiscriminate exploration (_Go-Explore+GCSL_) [21; 37]. _Ours_ also beats methods that purely rely on policy generalization and stochasticity to perform exploration and do not explicitly expand the frontier (_Inverse Models_) [1; 47; 24] which fail in complex exploration domains. Goal-conditioned reinforcement learning methods with _PPO_ and _sparse_ rewards do not experience enough reward signals to actually learn directed behavior. In _PPO_ with _dense_ reward, we use the same reward as for generating the synthetic human labels and we observe, that in most cases the performance is lower. The reason is that we did not do any reward engineering for either PPO or HuGE, showing that _Ours_ is more robust to simpler underlying reward functions than _PPO_, we analyze these results further in Appendix E.2. Similarly, using human feedback to bias exploration _Ours_ beats using it to learn a reward function (_Human Preferences_). Finally,in Appendix C we show that pretraining HuGE with 5 noisy trajectories (_BC + Ours_) gives a significant decrease in the number of timesteps needed to succeed and beats plain Behavior Cloning (_BC_) and also compared to other baselines with demonstrations.

### Learning Goal-Conditioned Policies with large-scale crowdsourced data collection

In this section, we show HuGE works on the kitchen environment from a crowdsourced data collection of 109 non-experts annotators labeling asynchronously and from all over the world, as shown in Figure 6. We spanned across three continents, having annotators living in 13 different countries, with ages ranging from 18 to 65+ and a variety of academic backgrounds, and we collected 1600 labels, we refer the reader to B.1 and B.2 for more details about the demographics. Each annotator

Figure 5: Success curves of HuGE on the proposed benchmarks compared to the baselines. HuGE outperforms the rest of the baselines, some of which cannot solve the environment while converging to the oracle accuracy. For those curves that are not visible, it means they never succeeded and hence are all at 0 (see E.10 for distance curves). The curves are the average of 4 runs, and the shaded region corresponds to the standard deviation.

could provide labels at any time during the day, our recommendation was to provide 30 labels, which took on average less than 2 minutes of their time. We collected a total of 2678 labels for this crowdsourcing experiment beating _Ours_ when labels are synthetically generated, This indicates that the crowdsourced data might provide more information than the simplistic reward function, designed to provide the synthetic feedback. We repeated the experiment collecting just 1670 labels from 4 annotators and saw that the crowdsourcing experiment also yield better results, suggesting that a wider variety of feedback does not damage performance. We refer the reader to Appendix B for additional results of HuGE from real human feedback on other simulated benchmarks, details on the platform we developed for these experiments, and where we clarify that this study was approved by the Institutional Review Board.

### Learning Goal-Conditioned Policies in the real world

HuGE's qualities of being robust to **noisy** feedback and requiring **minimal** and **asynchronous** human supervision together with its self-supervised policy learning nature and the capability to be pretrained from trajectory demonstrations makes it suitable for learning in the real world. As shown in Fig 7, HuGE can learn policies for pick and place and drawing in the real world with a LoCoBot . Some adaptations were made to run HuGE on real hardware such as changing the state space to images, instead of point space. In Appendix A we provide more results on HuGE from image space in the real world and simulation. We also pretrained our policy with 5 trajectory demonstrations (Appendix C. For both experiments, the robots learned directly on the real hardware. In the pick and place experiment, we collected around 130 labels across 20 hours of real-world robot training, whereas in the drawing one, we collected 150 labels across 6 hours.

### Ablation Analysis

Lastly, we conducted a number of quantitative analysis experiments to better appreciate and convey the benefits of HuGE. In Figure 8, we show that learning a parametric goal selector (_Ours_) is more sample efficient than directly using the human-selected goals (_DDL_). More concretely, learning a parametric goal selector needs 50% less human annotations than DDL . The underlying

Figure 6: **left:** Crowdsourcing experiment learning curves for the kitchen, **middle**: human annotators spanned 3 continents and 13 countries, **right**: screenshot of the interface for data collection.

Figure 7: Accomplished goals at the end of 5 different evaluation episodes throughout training in the real world.

idea is that by deriving a goal selector from human feedback, we extract recurrent patterns and generalizations that can be redeployed in future rollouts. Conversely, without learning from this goal selector, we would overlook these patterns, leading to more frequent queries for human input.

One of the properties of HuGE is that it works from noisy human feedback. In Figure 8, we observe that despite adding large amounts of noise on the human annotations, HuGE still converges to the optimal solution. This supports the idea that when the human feedback is noisier, exploration will be less efficient and will take longer to converge nevertheless, still converging to an optimal policy.

Furthermore, in Figure 8 (_right_), we provide a visualization explaining why optimizing over the reward function (_Human Preferences_) can provide non-optimal solutions. We observe reward functions learned from noisy human feedback are suboptimal, with multiple local maxima, and hence optimizing over them will potentially lead to the policy getting stuck in local optima. On the other hand, HuGE, which samples goals from this noisy learned reward function will still converge to the optimal solution. The explanation is that the goal selector will steer exploration towards the three modes uniformly, making exploration suboptimal but still converging to an optimal goal-conditioned policy reaching each one of the modes, the goal included.

In Appendix F, we explain further analysis as the trade-off between the amount of human querying and sample complexity of HuGE, as well as showing that HuGE is also robust to incomplete feedback and further details of its robustness to noisy underlying reward functions compared to PPO.

## 6 Discussion

In this work, we build from the insight of decoupling human feedback from policy learning. We introduced HuGE, that guides exploration by leveraging small amounts of noisy and asynchronous human feedback, improving upon indiscriminate exploration or relying on hindsight generalization. Moreover, the policy training using self-supervised learning remains disentangled from the human feedback and makes it robust to asynchronous and noisy human feedback. We firmly believe this insight is key to learn policies from crowdsourced human feedback, where the noise in the labels due to non-expertise and variety of the annotators will be significant. As we show, when the annotations are noisy, optimizing over the learned noisy reward function using standard RL techniques will fail. However, once we disentangle the human feedback from the learned policy, and only use this to softly guide exploration, we can still learn optimal policies despite the noise. We rigurously demonstrate that HuGE is able to solve difficult exploration problems for various control tasks learning in both the real world and in simulation. Moreover, we run a crowdsourced data collection experiment with 109 human annotators living across three continents, with a wide variety of backgrounds, education level, ages, and we show HuGE succeeds in learning optimal policies. Finally, we provide ablations and analysis on the design decisions and better show the benefits of HuGE.

There are a number of directions for future work that are very exciting building on HuGE. On an immediate note, adapting HuGE to work in reset-free environments so that we can scale up the applications on learning on the real robot is the most promising. It is with excitement that we would like to see this system scaled to fine-tune foundation models in robotics. In the future, aiming to learn behaviors not just from bianry comparative feedback, but from richer forms of communication such as language or physical corrections or vector valued feedback would be intriguing as well.

Figure 8: **(left)**: Learning a goal selector (_Ours_) needs on average 50% fewer labels than not (_DDL_) **(middle)**: As the noise in the feedback increases, so will the number of timesteps to succeed, however HuGE still finds a solution. **(right)**: HuGE is robust to noisy goal selectors since trajectories going to each mode will be sampled while if we ran RL the policy would become biased and fail. See Appendix F for more details.

Acknowledgements

We thank all of the participants to our human studies that gave us some of their time to provide labels. We thank the members of the Improbable AI Lab and the WEIRD Lab for their helpful feedback and insightful discussions. We also thank the reviewers for their insightful comments and helping us make the paper stronger.

The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper. This research was supported by MIT-IBM Watson AI Lab.

## 8 Contributions

**Marcel Torne** and **Abhishek Gupta** jointly conceived the project. **Marcel** set up the simulation and training code, designed and conducted experiments in simulation, designed and implemented the interface for collecting human feedback, led the human experiments, conducted the ablations, and made the figures. **Marcel** led the manuscript writing together with **Abhishek**. **Max Balsells** designed and conducted the experiments in the real-world, integrated the vision models in the code, helped running some of the simulation experiments and ablations and helped writing the manuscript. **Zihan Wang** provided feedback on the manuscript. **Samedh Desai** helped **Max** setting up the real-world experiments. **Tao Chen** was involved in the initial research discussions and provided feedback on the paper. **Pulkit Agrawal** was involved in research discussions, contributed some of the main ideas behind the project and provided feedback on the writing and positioning of the work. **Abhishek Gupta** conceived the project jointly with **Marcel**, led the manuscript writing together with **Marcel**, and provided the main overall advising.