# Detecting Backdoors with Meta-Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

It is widely known that it is possible to implant backdoors into neural networks, by which an attacker can choose an input to produce a particular undesirable output (e.g. misclassify an image). We propose to use _meta-models_, neural networks that take another network's parameters as input, to detect backdoors directly from model weights. To this end we present a meta-model architecture and train it on a dataset of approx. 4000 clean and backdoored CNNs trained on CIFAR-10. Our approach is simple and scalable, and is able to detect the presence of a backdoor with \(>99\%\) accuracy when the test trigger pattern is i.i.d., with some success even on out-of-distribution backdoors.

## 1 Introduction

A line of work often referred to as _mechanistic interpretability_ studies the internal workings of trained neural networks (Olah et al., 2020; Olsson et al., 2022; K. Wang et al., 2022; Meng et al., 2023; McGrath et al., 2022; Elhage et al., 2022). The goal of mechanistic interpretability is to obtain a human-understandable description of the algorithm a neural network has learned. Despite the supposed black-box nature of neural networks, the field has had some noteworthy successes, fully understanding the exact algorithm implemented by a network (Nanda et al., 2023). However, current work in interpretability is reliant on human labor and thus not scalable even in principle, since even a large team of humans cannot reverse-engineer a network consisting of billions of neurons by hand. In order to scale to large models, it is likely that we need to automate interpretability methods.

There have been a number of proposed approaches to automated interpretability, including using LLMs to annotate neurons based on dataset examples (Bills et al., 2023; Foote et al., 2023), automated circuit ablation (Conmy et al., 2023), and verification of circuit behavior (Chan et al., 2022). In this work, we propose to train a neural network to take the parameters of other neural networks as input in order to perform interpretability tasks.1 We refer to such models as **meta-models** and the networks they are trained on as **base models**. This simple approach permits us to train arbitrary tasks end-to-end, so long as it is possible to build a suitable training dataset.

**Main contributions.**

* We propose a meta-model architecture that can operate on arbitrary base model architectures and train it on datasets comprising base models of size ranging between approximately \(10^{3}-10^{7}\) parameters.
* We demonstrate that meta-models can be useful for understanding network internals on two distinct tasks. First, we translate synthetic (compiled, not trained) neural network weights into equivalent human-interpretable code (Figure 5, Section 3.3). Second, we detect the presence of backdoors in normally-trained convolutional networks (Figure 2, Section 3.1).

* We compare against previous work on meta-models and find that our approach outperforms a previous method on predicting base model hyperparameters from weights (Figure 4, Section 3.2).

## 2 Related Work

Meta-models.While to our knowledge we are the first to use the term meta-models in a paper, the idea of using neural networks to operate on neural network parameters is not new. A line of work focuses on _hyperrepresentations_ achieved by training an autoencoder on a dataset of neural network weights (Schurholt, Kostadinov, et al.2021; Schurholt, Knyazev, et al.2022). The trained encoder can be used as a feature extractor to predict model characteristics (such as hyperparameters), and the decoder can be used to sample new weights, functioning as an improved initialization scheme. In earlier work, Eilertsen et al. (2020) train a meta-model to predict base model hyperparameters such as learning rate and batch size. While a fully rigorous comparison is out of scope, our meta-model architecture is simpler and outperforms prior work on the comparison tasks we tested (Section 3.2). In a different line of work, Weiss et al. (2018) algorithmically extract a representation of an RNN as a finite state automaton. This is similar to our work because we are also interested in extracting a full description of the computation performed by a transformer (Section 3.3).

Interpretability.The field of interpretability studies the internal workings of neural networks, with the goal of making the outputs and behaviour of neural networks more understandable to humans (Doshi-Velez and Kim, 2017; Lipton, 2018). While there is no universally agreed-upon definition of interpretability, in the context of this work we will focus on the sub-problem of **mechanistic interpretability**, which aims to understand the learned mechanisms implemented by a neural network. Recent work on mechanistic interpretability includes the full reverse engineering of a transformer trained on a modular addition task (Nanda et al., 2023), tracking chess knowledge in AlphaZero (McGrath et al., 2022), locating a circuit responsible for a specific grammatical task in GPT-2 (K. Wang et al., 2022), and the study of superposition in transformers (Elhage et al., 2022). These tasks are impressive, especially as they allow humans to understand neural networks in purely conceptual terms.

Data poisoning and backdoors.Data poisoning is the act of tampering with the training data to be fed to a model, in such a way that a model trained on this data exhibits undesired or malicious behaviour. Some data poisoning attacks attempt to install a _backdoor_ in the model--a way in which an attacker can choose an input to produce a particular, undesirable output. Many basic backdoor attacks modify a small fraction of the training inputs (\(1\%\) or less) with a trigger pattern (Gu et al., 2017; Chen et al., 2017), and change the corresponding labels to the target class. At test time, the attacker

Figure 1: Our meta-model architecture. The inputs are the weights of a base model (in our experiments either a CNN or transformer). The weights are flattened, then divided into chunks of size 8-1024 depending on the size of the base model. Each chunk is passed through a linear embedding layer and then a transformer decoder. The output of the transformer depends on the task, and is either a single array of logits for classification or a tensor of logits for next-token prediction as in the inverting Tracr task (see Section 3.3 and Figure 5).

can modify any input to the model with the trigger pattern, causing the model to misclassify the image. Casper et al. (2023) propose backdoor detection as a benchmark for interpretability methods. Similarly, we use backdoor detection to benchmark our meta-model (Section 3.1). Backdoor detection with meta-models depends on recognizing the subset of weights responsible for a backdoor in a set of trained model weights and thus is a promising choice for a benchmark.

Backdoor defenses.A variety of backdoor defense methods have been developed to defend against attacks. Common methods prune neurons from a given network (B. Wang et al., 2019), remove backdoor examples and retrain the base model (B. Chen et al., 2018), or even introduce custom training procedures to produce a cleaned model (Li et al., 2021). However, meta-models can only operate on a model-by-model scale, and few methods are directly comparable. In terms of coarsely detecting whether a model is backdoored or not, two prior works exist that are directly comparable to meta-models. Universal litmus patterns (Kolouri et al., 2020) and meta neural analysis (Xu et al., 2020) are similar methods--they train a spread of base models, then, using gradient descent, jointly train dummy inputs and a classifier, such that when the dummy inputs are fed through the base model to produce output logits, the classifier predicts the likelihood that a base model is poisoned. We compare against their results, using a meta-model to directly take the weights as inputs and produce a classification.

## 3 Experiments

In this section we present empirical results on three main meta-modeling tasks: predicting data properties, mapping transformer parameters to equivalent programs written in human-readable code, and detecting and removing backdoors. All code and datasets are available under an open-source license.2 Throughout this section, we briefly describe the architectures and training methods used; more detail is available in the Appendix.

### Detecting Backdoors

Base model dataset.We train base models on CIFAR-10 (Krizhevsky et al., 2009), using a simple CNN architecture with 70,000 parameters. We train a set of clean models and a set of poisoned models for every poison type. Depending on poison type, the number of base models we train ranges from \(2,000-3,000\). The exact model architecture is described in the Appendix. We open-source this dataset for future work.3

Figure 2: **Left: ROC curves for our meta-model on the backdoor detection task, by poison trigger type. Triggers marked OOD mean our meta-model is trained on a different distribution than the trigger type. Right: Area under the ROC curve. A: BadNets Border; B: BadNets Center; C: BadNets Corner (OOD); D: Random Noise (OOD); E: ULP Test (OOD). Notably, we match Kolouri et al. (2020) on their custom trigger patterns, despite only training a randomly positioned BadNets trigger pattern (which is different in size).**

Data poisoning.We poison the training data by adding a trigger pattern to \(1\%\) of the images and setting all associated labels to an _attack target_ class determined randomly at the start of training. We use a suite of basic attacks in this work: the 4-pixel patch and single pixel attacks from Gu et al. (2017), a random noise blending attack from X. Chen et al. (2017), a strided checkerboard blending attack from Liao et al. (2018). We set \(=0.1\) for all blending attacks, and always use a poisoning fraction of 1% of the overall training dataset.

Meta-model training.We train a meta-model to detect backdoors by treating the problem as a classification task, between clean models trained on ordinary data, and poisoned models trained on poisoned data as described above. To use the base model weights as input to our meta-model, we first flatten the weights, then divide them into chunks of size 1024. Each chunk is passed through a linear embedding layer and then a transformer decoder as in Figure 1. We augment every training batch by permuting neurons in every layer except the last, as the function parametrized by a neural network is invariant under some permutations (Navon et al., 2023). Augmentations substantially improve validation accuracy.

Results.In the iid setting that is typically considered (that is, we test on attacks similar to the one we train the meta-model on), we achieve >99% accuracy on all attacks. Additionally, we compare against other model-scale detection methods: Meta Neural Analysis (Xu et al., 2020), and Universal Litmus Patterns (Kolouri et al., 2020) (Figure 2).

Xu et al. (2020) evaluate their method on base models poisoned with the 4-pixel patch and the random-blended backdoor attacks. The Random Noise and BadNets Corner settings are our direct comparison to Xu et al. (ibid.)'s results. We train base networks on their training distribution, then evaluate on nets poisoned with the the 4-pixel patch and random noise blending. As we see, the meta-model demonstrates substantially better performance on these tasks than their method, which is indicative that the weights of the network alone hold substantial information when it comes to detecting backdoors. Kolouri et al. (2020) evaluate on base models poisoned with a custom set of backdoor patches, and we match their evaluation regime. In this setting, we only train on the 4-pixel patches. While Kolouri et al. (ibid.) introduce their own new set of attack patterns, our trained meta-model generalizes near-perfectly to their (OOD) attacks without adjustment and matches their performance (Figure 2).

### Comparison with prior meta-model work

To sanity check our choice of meta-model architecture and implementation, we compare against (Eilertsen et al., 2020), who train a meta-model to predict hyperparameters used to train base models: the dataset, batch size, augmentation method, optimizer, activation function, and initialization scheme.

They have two settings: one where the architecture (and thus the size) of the base models are fixed, and another where they are allowed to have variable size. We focus on the second, more general setting. We replicated their dataset generation procedure, training CNNs with random variance in the hyperparameters listed above. Full details on the replication of Eilertsen et al. (ibid.)'s training procedure is deferred to the Appendix.

Eilertsen et al. (ibid.) use a 1-dimensional CNN on a 5,000-long randomly chosen segment of the flattened weights, training on 10,000 networks from the dataset as described. We instead use the meta-model described above, taking each of the 40,000 nets we generated following their procedure,

Figure 3: Left to right: 4-pixel patch and 1-pixel patch attacks from Gu et al. (2017), random noise blending from X. Chen et al. (2017), checkerboard blending from Liao et al. (2018), hand-crafted patch from Kolouri et al. (2020).

truncating flattened weights past 800,000 (or zero-padding to that length if the base network has fewer parameters), and training a meta-model with one of the variable hyperparameters as a target.

The results are visible in Figure 4. We outperform their method in every category, sometimes substantially. While these problems are not clearly valuable from an interpretability standpoint, they are a promising indicator that our meta-models method is useful, in that it readily solves extant tasks.

RASP and Tracr.In analogy to how finite state machines provide a computational model for RNNs (Weiss et al., 2018), in recent work Weiss et al. (2021) develop RASP, a computational model for a transformer encoder. RASP is a domain-specific programming language designed to describe the computations that a transformer is able to perform. Each line in a RASP program maps to a single attention head and/or two-layer MLP. The RASP language is implemented in Tracr (Lindner, Kramar, Rahtz, et al., 2023), a compiler that (deterministically) translates RASP programs into corresponding transformer weights. See more about RASP in appendix Section B, with an example of Tracr compilation in Section C.

Base model dataset.We generate a dataset of 8 million RASP programs and use Tracr to compile every program to a set of transformer weights, resulting in a dataset consisting of tuples \((P,r)\), where \(P\) is a dictionary containing the parameters of the compiled transformer and \(r\) is the corresponding RASP program. We then deduplicate the generated programs, resulting in a dataset of 6 million parameter-program pairs. We constrain RASP programs to contain between 5 and 15 instructions, each of which may handle up to 3 arguments, other variables, predicates or lambdas (Figure 17, Table 2).

Transformer Parameter and Program Tokenization.We convert RASP programs into computational graphs, ordering the instructions in every program based on their computational depth, argument type (Lambda > Predicates > Variables), and alphanumeric order, providing a unique representation for a every program (Figure 19). We flatten the base model parameters into 512 chunks (using padding for smaller models). For every block we add a layer-encoding by concatenating an array to describe the layer type.

Meta-model training.We train a transformer decoder on a next-token prediction loss to map base model parameters to the corresponding RASP programs (Figures 5 and 15). Inputs are divided into three segments: transformer parameters, padding, and a start token at timestep \(T-15\), followed by the tokenized RASP program. Targets consist of offset labels starting from timestep \(T-15\). At test time, we generate an entire RASP program autoregressively: we condition the trained model on a set of base model parameters and perform 15 consecutive model calls to generate the RASP program.

Figure 4: Comparison with a CNN meta-model from Eilertsen et al. (2020). The task is to predict training hyperparameters from model weights on a large distribution of base models with diverse architectures and training datasets. Despite not specializing our method to the task at all, we find that we can readily exceed their performance on the same data distribution.

### Inverting Tracr

## 4 Limitations

The tasks we train on are simple compared to the full problem of reverse-engineering a large neural network. While we are able to automatically reverse-engineer most RASP instructions from model weights, the models involved are relatively small (less than \(50,000\) parameters, on average \(3,000\)), and the Tracr-compiled model weights are dissimilar from the distribution of weights obtained via SGD-training.

More generally, we have chosen tasks for which we are able to train thousands of base models, and for which a loss function is easily evaluated. It may be hard to generate training data for real-world interpretability tasks. In addition, our meta-models tend to be larger than the base models they are trained on by about a factor of 10-1000, which would be prohibitive for very large base models.

We also only show how meta-models might be used to _propose_ mechanistic interpretations of a base model, but we do not address the problem of _verifying_ a mechanistic interpretation of a model is accurate. Without a means of verification, this approach can only provide limited assurance. While there might be ways to apply meta-models for verifying interpretations (or other properties) of a base model, this is beyond the scope of our work.

## 5 Conclusion

Interpretability is currently bottlenecked on _scaling_, which is challenging given the current state of the art which requires substantial direct human labor by researchers to understand a model. We propose to use transformers, which are famously scalable, as meta-models that can be trained to perform interpretability tasks. The method is general: we apply it to diverse tasks such detecting hyperparameters, generating human-readable code, and detecting backdoors. Despite its generality, it performs well, beating prior work on both backdoor detection and hyperparameter prediction and successfully recovering the majority of RASP instructions from Tracr-compiled transformer weights. To our knowledge, this is the first work that recovers a program from a transformer neural network.

We believe this demonstrates the potentially broad applicability of meta-models in the circumstances where it is possible to construct an appropriate dataset. We hope that future work extends meta-models to more complex and more immediately useful tasks, in the hopes of developing methods to readily interpret arbitrary black-box neural networks.

Figure 5: **Left: We train a transformer meta-model to predict the next instruction in a RASP program (red), conditioning on the flattened and chunked array of parameters from the corresponding compiled transformer (blue). We tokenize RASP programs and define a unique ordering of instructions for every program. Right: Next-token accuracy (blue) and fraction of programs where more than X% of instructions are recovered (accuracyX, yellow, green, and red). Notably, the meta-model is able to perfectly recover around 6% of RASP programs, and mostly recover (90%) programs 32.0% of the time. GT: accuracy obtained via conditioning on previous ground truth RASP instructions. AR: accuracy obtained via autoregressive generation, conditioning only on base model parameters.**

## Reproducibility Statement

We open source our datasets and our code (currently redacted for anonymity).