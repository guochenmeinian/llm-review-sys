# Agent Planning with World Knowledge Model

Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen\({}^{}\)Zhejiang University

National University of Singapore, NUS-NCS Joint Lab

Alibaba Group

Zhejiang Key Laboratory of Big Data Intelligent Computing

{shuofei,zhangingyu}@zju.edu.cn

###### Abstract

Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the "real" physical world. Imitating humans' _mental_ world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce _parametric_**W**orld **K**nowledge **M**odel (**WKM**) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior _task knowledge_ to guide the global planning and dynamic _state knowledge_ to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development3.

## 1 Introduction

The remarkable advances in Large Language Models (LLMs) have witnessed a rapid development of various natural language processing tasks . Recently, multiple attempts that

Figure 1: Traditional agent planning vs. Agent planning with world knowledge model.

directly exploit LLMs as agent models to address physical world planning tasks have demonstrated promising achievements [54; 57; 56; 34; 38; 64; 44]. However, as most state-of-the-art LLMs are autoregressive models trained with next-token prediction, they lack the ability to essentially understand the real world, leading to generating hallucinatory actions and performing brainless trial-and-error in the environment as shown in Figure 1(a).

In contrast to LLMs, humans possess a mental knowledge model about the physical world [1; 18; 17; 30]. When facing a specific task, they will first briefly rehearse the entire process in mind using their rich prior knowledge before performing mindless actions. We call this kind of knowledge global _task knowledge_ (a.k.a. environment/task commonsense). In addition, during the task procedure, the mental world knowledge model will constantly maintain a kind of local _state knowledge_, representing humans' cognition of the current world state. For example, imagine you are in a room and your task is to put a clean egg in microwave. The _task knowledge_ may refer to The egg is most likely in the fridge... The workflows are: 1) locate and take the egg; 2) clean the egg using sinkbasin... The _state knowledge_ possibly refers to My task is to... I have found and taked the egg... Next I should... The absence of world knowledge can lead to blind trial-and-error in the early planning stages when environmental information is limited. Conversely, in later stages when information is redundant, it can easily result in a confused cognition of the current world state and generate hallucinatory actions.

The process by which humans handle planning tasks reminds us to develop a parametric **W**orld **K**nowledge **M**odel (**WKM**) to facilitate agent planning. As humans typically acquire knowledge from expertise and practical experience, we build WKM based on knowledge learned from both expert and explored trajectories. Specifically, we first steer the agent model to synthesize task knowledge from the comparison between expert and sampled trajectories. Then we prompt it to summarize state knowledge for each planning step from expert trajectories and combine the previous and next actions to build a state knowledge base. Lastly, we integrate the generated knowledge into expert trajectories and train a WKM. The agent model needs to be retrained to adapt to the task knowledge. Note our agent and knowledge model are both trained with LoRA  sharing the same backbone.

During the planning phase, we use the WKM to provide global prior task knowledge and maintain local dynamic state knowledge for the agent model as shown in Figure 1(b). The task knowledge will be concatenated in natural language form following the specific task to guide the agent model's trial-and-error. At each planning step, to prevent the occurrence of hallucinatory actions, we utilize the generated state knowledge as the query to conduct \(k\)NN retrieval from the pre-built state knowledge base. We then use the constraints from the previous action, the probabilities of the retrieved next actions, and the probabilities from the agent model to make a weighted prediction for the next action.

We evaluate our method on three real-world simulated planning tasks: ALFWorld , WebShop , and ScienceWorld  with three state-of-the-art open-source LLMs: Mistral-7B , German-7B , and Llama-3-8B . Empirical results demonstrate that our method achieves superior performance compared to various strong baselines on both seen and unseen tasks. Moreover, further analytical results show that 1) our WKM can effectively reduce blind trial-and-error and hallucinatory actions, 2) our model-generated instance-level knowledge can generalize better to unseen tasks, 3) weak-guide-strong is feasible, 4) multi-task unified WKM possesses strong potential, and 5) explicit state knowledge will hurt the performance of agent planning.

## 2 Preliminaries

We mainly focus on interactive tasks with partial observations from environments. Following the task formulation in , the problem can be viewed as a Partially Observable Markov Decision Process (POMDP): \((,,,,)\). The instruction space \(\) defines the task and its corresponding regulations. \(\) is the state space, \(\) is the observation space, and \(\) is the action space. \(:\) defines the transition function, which we assume to be given by the environments. It is noticed that \(\), \(\), and \(\) are subspaces of the natural language space in the language agent scenarios.

Based on the above, the historical trajectory \(h_{t}\) that consists of a list of actions and observations at time \(t\) can be represented as:

\[h_{t}=(u,a_{0},o_{0},a_{1},o_{1},,a_{t},o_{t}), \]

where \(u\) is the task instruction and \(a\), \(o\) are the action and the observation. Given a task, the language agent with parameter \(\) serves as the policy model \(_{}\) responsible for generating the action \(a_{t+1}\) based on \(h_{t}\) at each time step \(t+1\):

\[a_{t+1}_{}(|h_{t}). \]

Specifically, \(a_{0}_{}(|u)\) is generated according to the task instruction \(u\). The whole trajectory \(\) concludes when the task is completed or exceeds the maximum time steps. Then the production of the entire trajectory with time length \(n\) can be modeled as:

\[_{}(|u)=_{t=0}^{n}_{}(a_{t+1}|h_{t})_{}(a_{0 }|u). \]

Ultimately, the final reward \(r(u,)\) representing the task completion rate is calculated. Note that we follow a ReAct-style  trajectory that includes rationales before each action. We use \(a\) to represent the action with rationales for convenience.

World Knowledge Model._World knowledge model_ serves as humans' mental cognition of the physical environment, more intricate than the _word knowledge model_ which LLM-powered agent models are trained to be [61; 10; 52; 13]. Our "world" here refers to the simulated environment of the task. Based on the static environment of the task and the dynamic changes during interaction with the agent, we define world knowledge as a combination of prior global knowledge and dynamic local knowledge, corresponding to the blind trial-and-error problem in global planning and the hallucinatory action issue in local planning in traditional agent models, respectively. To attain precise and efficient agent planning, we develop a _parametric_ WKM to simulate the _mental_ WKM of humans.

## 3 Method

As shown in Figure 2, we steer the agent model to self-synthesize the _task knowledge_ from the comparison of expert and sampled trajectories (SS3.1). Then we prompt the agent model to self-summarize the _state knowledge_ based on historical behavior and construct a state knowledge base (SS3.2). The generated knowledge will be integrated into the expert trajectories for training the WKM. After the training process (SS3.3), we augment the agent model with the world knowledge model to achieve effective and accurate planning (SS3.4).

### Task Knowledge Synthesis

The _task knowledge_ serves as the prior knowledge to guide the agent model's global planning and prevent it from dropping into blind trial-and-error.

Figure 2: **Overview of our WKM**. We train a world knowledge model on the knowledge synthesized by the agent model itself from both expert and explored trajectories, providing prior task knowledge to guide global planning and dynamic state knowledge to assist local planning.

Experienced Agent Exploration.We primarily acquire task knowledge through the comparison of preference trajectories (_chosen_ vs. _rejected_). In order to improve the quality of rejected trajectories and obtain more targeted task knowledge, we employ an experienced agent for exploration. Firstly, we train a vanilla language model with expert trajectories4 from the training set to obtain an experienced agent. Subsequently, the experienced agent explores the training set tasks again to generate rejected trajectories. Our purpose is to extract superior task knowledge that cannot be acquired solely through supervised fine-tuning on chosen trajectories, thus further effectively boosting the agent's capabilities.

Self Knowledge Synthesis.With the expert trajectories as the chosen ones and the trajectories sampled from the experienced agent as the rejected ones, we prompt the agent model itself to synthesize the task knowledge. Supposing \(\) is the task knowledge space:

\[_{}(|_{},u,_{w},_{l}), \]

where \(\) is the task knowledge, \(_{}\) stands for the prompt to instruct the task knowledge extraction, and \(_{w}\), \(_{l}\) are the chosen and rejected trajectories respectively. Note that given the same task \(u\), \(_{w}\) and \(_{l}\) always satisfy \(r(u,_{w})=1 r(u,_{l})\). Even when \(r(u,_{w})=r(u,_{l})\), we still consider trajectories sampled from the experienced agent as rejected ones. This is because expert trajectories often have shorter step lengths, enabling the agent to learn more knowledge of efficient planning. For detailed prompts of task knowledge synthesis, please refer to Appendix I.1.

### State Knowledge Summarization

The _state knowledge_ serves as the dynamic knowledge to constrain the agent model's local planning and prevent it from generating hallucinatory actions. We prompt the agent model to self-summarize state knowledge at each planning step based on the expert trajectories to guarantee quality. For detailed prompts of state knowledge summarization, please refer to Appendix I.2. Supposing the prompt used to summarize state knowledge is \(_{}\) and the state knowledge \(s\) is a part of the state space \(\), the generation of state knowledge at time \(t\) can be represented as:

\[s_{t}_{}(|_{},h_{t}). \]

State Knowledge Base Construction.To avoid confusion caused by excessive additional information, instead of explicitly concatenating the state knowledge to the context, we construct a state knowledge base for retrieval (we analyze in SS4.3 how explicit state knowledge may affect the performance of agent model). We combine the state knowledge \(s_{t}\) with the previous action \(a_{t}\) and next action \(a_{t+1}\) from the expert trajectory to form a action-state-action triplet \((a_{t},s_{t},a_{t+1})\). After iterating through all expert trajectories, we obtain a State Knowledge Base \(=\{(s,a_{},a_{})^{(i)}\}_{i=1}^{||}\), where \(a_{}=a_{t}\), \(a_{}=a_{t+1}\), and \(||\) is the size of the state knowledge base.

### Model Training

We integrate the generated world knowledge into expert trajectories and train a world knowledge model. The agent model needs to be re-trained to adapt to the incorporation of task knowledge. Note that our agent model and knowledge model are both trained with LoRA **sharing the same backbone**. We list the examples of training data for both the agent model and WKM in Appendix E.

Agent Model Training.Given the expert trajectories dataset \(=\{(u,,_{w})^{(i)}\}_{i=1}^{||}\) with **task knowledge**\(\) generated in SS3.1, we train the agent model to follow the task knowledge to generate actions. Under an auto-regressive manner, the loss of the agent model can be formulated as:

\[_{}(_{})=-_{_{w} }[_{}(_{w}|u,)] \]

Suppose \(=(x_{1},x_{2},,x_{||})\) is the token sequence of the trajectory \(_{w}\), we have:

\[_{}(_{w}|u,)=-_{j=1}^{||}((x_ {j})_{}(x_{j}|u,,x_{<j})). \]

Here \((x_{j})\) is the indicator function to mask tokens unrelated to actions. Please note that \(_{w}\) here **does not include** the state knowledge mentioned in SS3.2.

World Knowledge Model Training.The main difference in the training data between the agent and knowledge model is **the added state knowledge**. Given the expert trajectories dataset with both task and state knowledge \(^{}=\{(u,,^{}_{w})^{(i)}\}_{i=1}^{ }\) where \(^{}_{w}=(a_{0},o_{0},s_{0},,a_{n},o_{n},s_{n})\), the loss of the knowledge model \(_{}\) can be formulated as:

\[_{}(_{})=-_{,^{}_{w} ^{}}[_{}(|u)_{}(^{}_{w}|u, )] \]

Suppose \(^{}=(x^{}_{1},x^{}_{2},,x^{}_{ ^{}})\) is the token sequence of the expert trajectory with state knowledge \(^{}_{w}\) and \(=(y_{1},y_{2},,y_{})\) represents the token sequence of the task knowledge \(\), we have:

\[_{}(|u) =-_{i=1}^{}_{}(y_{i}|u,y_{< i}) \] \[_{}(^{}_{w}|u,) =-_{j=1}^{^{}}( (x^{}_{j})_{}(x^{}_{j}|u,,x^ {}_{<j})), \]

where \((x_{j})\) is the indicator function to mask tokens unrelated to state knowledge.

### Agent Planning with World Knowledge Model

At inference time, the agent model plans on the evaluation tasks with the aid of the world knowledge model. We redefine the historical trajectory \(h_{t}=(u,,a_{0},o_{0},a_{1},o_{1},,a_{t},o_{t})\). Given a specific task instruction \(u\), the knowledge model first generates the task knowledge \(_{}(|u)\), then the agent model starts planning. Assuming the available action set \(_{u}\) for the task \(u\) is \((^{(1)}_{u},^{(2)}_{u},,^{(|_{u}|)}_{u})\), at any time \(t 0\), instead of directly generating a next action \(a_{t+1}_{u}\) based on \(h_{t}\), we first employ the world knowledge model to generate the current state knowledge \(s_{t}_{}(|h_{t})\) and leverage \(s_{t}\) to query the state knowledge base \(=\{(s,a_{},a_{})^{(i)}\}_{i=1}^{ }\). With the state knowledge as the key, we retrieve \(\) nearest triplets **from where \(a_{}=a_{t}\)** based on semantic similarity and collect the corresponding next actions \(a_{}\). We count the probability of each action \(p_{}(^{(i)}_{u})=}{}\), where \(_{i}\) is the occurrence number of action \(^{(i)}_{u}\) in all the collected \(a_{}\). Therefore, we get the probability acquired from the state knowledge base:

\[P_{}(_{u})=(p_{}(^{(1)}_{u}),p_{ }(^{(2)}_{u}),,p_{}(^{(|_{u}|)}_{u})),_{i=1}^{_{u}}p_{}( ^{(i)}_{u})=1. \]

Afterward, we sample the probability distribution of the first token for each action \(^{(i)}_{u},1 i_{u}\) from the last layer of the agent model and apply a softmax function to normalize the probability distribution. We define the probability acquired from the agent model as:

\[P_{}(_{u})=(p_{}(^{(1)}_{u}),p_{ }(^{(2)}_{u}),,p_{}(^{(| _{u}|)}_{u})),_{i=1}^{_{u}}p_{ }(^{(i)}_{u})=1. \]

Finally, we determine the next action by combining the above two probabilities:

\[a_{t+1}=*{arg\,max}_{^{(i)}_{u}_{u},1 i _{u}}( p_{}(^{(i)}_ {u})+(1-) p_{}(^{(i)}_{u})), \]

where \(\) is the hyperparameter that controls the proportion of \(P_{}(_{u})\). Based on the above, we enhance the agent planning by global guidance from task knowledge and local constraints from state knowledge generated by our WKM. Due to the WKM and retrieval, the inference stage incurs additional time overhead compared to the pure agent model. The approximate ratio is around 2.5:1.

## 4 Experiments

### Experimental Settings

Datasets and Metrics.We evaluate our method on three real-world simulated planning datasets: **ALFWorld**, **WebShop**, and **ScienceWorld**. AlFWorld and ScienceWorld include unseen tasks to evaluate the agent's generalization ability. The reward of ALFWorld is binary 0 or 1, indicating whether the agent has completed the task or not. WebShop and ScienceWorld provide dense rewards from 0 to 1 to measure the completion level of the task. For all the datasets, we apply **average reward** as the final metrics. Please refer to Appendix B for detailed dataset information.

Models and Baselines.We evaluate on three state-of-the-art open-source models: 1) **Mistral-7B**, the Mistral-7B-Instruct-v0.2 version. 2) **Gemma-7B**, the Gemma-1.1-7B-it version. 3) **Llama-3-8B**, the Meta-Llama-3-8B-Instruct version. We compare our method with two prompt-based baselines: **React** and **Reflexion**. Besides, we adopt two strong baselines that introduce rejected trajectories into the training process to learn from experience: **NAT**, learn from rejected trajectories through SFT, and **ETO**, learn from rejected trajectories through DPO . Moreover, we compare with a knowledge-augmented planning method **KnowAgent**. We also include **ChatGPT** (gpt-3.5-turbo-0125)  and **GPT-4** (gpt-4-32K-0613)  for comparison. All the prompt-based baselines are tested under one-shot and all the fine-tuning-based baselines are trained with LoRA . Please refer to Appendix C for baselines and re-producing details.

Training and Inference Setups.We fine-tune the proposed approach with LoRA  using the LlamaFactory  framework. During training, the model is tuned after finishing the entire trajectory rather than each step of action. The learning rate is 1e-4 and the sequence length is 2048 for all the models. The training epoch is 3 and the batch size is 32. We adopt the AdamW optimizer  with a cosine learning scheduler. During inference, we apply the embedding layer of WKM as the encoder and use the **cosine similarity** between sentences for retrieval. The number of retrieved action-state-action triplets \(\) is set to 3000 and the \(P_{}(_{u})\) weight \(\) is set to {0.4, 0.5, 0.7}. All the training and inference experiments are conducted on 8 NVIDIA V100 32G GPUs within 12 hours. Please refer to Appendix D for detailed hyperparameters used in our paper.

### Results

Main Results.As shown in Table 1, **for prompt-based baselines** on open-source models, both ReAct and Reflexion exhibit poor performance, far behind our method and fine-tuning-based baselines on various datasets. GPT-3.5-Turbo performs ordinarily on two datasets other than WebShop, and it even falls behind Mistral-7B and Llama-3-8B's ReAct performance on ScienceWorld. However, GPT-4 exhibits strong performance across various datasets. Nevertheless, our approach, through

    &  &  &  &  \\   & & & &  &  &  &  &  \\    } &   } & 8.57 & 5.97 & 44.37 & 15.41 & 13.99 \\  & & 44.29 & 38.05 & 62.76 & 67.32 & 65.09 \\   &   } & REact & 7.86 & 5.22 & 14.63 & 20.72 & 17.65 \\  & & Reflexion & 11.56 & 6.00 & 16.64 & 21.07 & 18.11 \\  & & NAT & 64.43 & 68.96 & 61.01 & 57.12 & 50.79 \\  & & ETO & 66.84 & 71.43 & 64.09 & 58.17 & 51.85 \\  & & KnowAgent & 70.44 & 70.72 & 61.28 & 59.32 & 47.24 \\   & & **WKM** & **73.57 \(\)**3.13 & **76.87 \(\)**5.44 & **65.48 \(\)**1.39 & **62.12 \(\)**2.80 & **53.62 \(\)**1.77 \\   &   } & REact & 6.43 & 2.24 & 5.93 & 3.58 & 3.51 \\  & & Reflexion & 7.14 & 2.99 & 7.71 & 4.94 & 3.93 \\  & & NAT & 67.86 & 65.88 & 55.82 & 47.63 & 44.98 \\  & & ETO & 66.43 & 68.66 & 62.67 & 50.44 & 47.84 \\  & & KnowAgent & 69.29 & 67.60 & 58.80 & 48.55 & 45.28 \\   & & **WKM** & **70.71 \(\)**1.42 & **70.40 \(\)**1.74 & **63.75 \(\)**1.08 & **53.68 \(\)**3.24 & **49.24 \(\)**1.40 \\   &   } & REact & 2.86 & 3.73 & 19.32 & 24.76 & 22.66 \\  & & Reflexion & 4.29 & 4.48 & 22.73 & 27.23 & 25.41 \\  & & NAT & 60.71 & 59.70 & 61.60 & 55.24 & 48.76 \\  & & ETO & 64.29 & 64.18 & 64.57 & 57.90 & 52.33 \\  & & KnowAgent & 66.71 & 62.69 & 64.40 & 58.67 & 49.18 \\   & & **WKM** & **68.57 \(\)**1.86 & **65.93 \(\)**1.75 & **66.64 \(\)**2.07 & **60.12 \(\)**1.55 & **54.75 \(\)**2.42 \\   

Table 1: **Main Results. The best results are marked in bold and the second-best results are marked with underline. All the prompt-based baselines (●) are evaluated under one-shot prompting and all the fine-tuning-based baselines (●) are trained through LoRA. Red represents the changes of WKM relative to the optimal results in the baselines. WKM and agent model are different LoRAs sharing the same backbone.**LoRA training alone, surpasses GPT-4 on ALFWorld (44.29\(\)73.57 on seen, 38.05\(\)76.87 on unseen) and WebShop (62.76\(\)66.64). **For fine-tuning-based baselines**, both NAT and ETO fall behind our method, implying that just integrating world knowledge for agent models is worth more than further fussy SFT or DPO on negative examples. Our method also performs better than KnowAgent which brings human-designed fixed action knowledge and long action paths into trajectories. This suggests the effectiveness of our WKM which is responsible for generating instance-level task knowledge and maintaining implicit action constraints. Furthermore, KnowAgent's performance on unseen tasks is not as impressive as on seen tasks, while WKM can keep its advantage. This phenomenon also demonstrates the generalization ability of WKM.

Approach Ablations.As shown in Figure 3, taking Mistral-7B as an example, we decompose the key components of WKM to examine the roles of the task and state knowledge separately. In a macro view, removing each module results in a clear drop in the agent's performance, which validates the power of our world knowledge. Furthermore, the improvement through task knowledge (_w/task_) is more pronounced than that through state knowledge (_w/ state_), suggesting the necessity of global prior knowledge for agent planning. A more micro observation reveals that the impact of state knowledge is more significant on seen tasks compared to unseen tasks, while the influence of task knowledge is sustainable across seen and unseen tasks. This may be attributed that although our real-time state knowledge is generated by WKM, the state knowledge base is built on the training set, which may weaken generalization to some extent. Additionally, to validate our motivation of allowing the agent to learn task knowledge from both expert and generated trajectories, we exclude the rejected trajectories during the synthesis of task knowledge, instructing the agent model to synthesize knowledge solely based on the chosen trajectories. The results (_w/o rejected_) demonstrate that learning from the contrast between chosen and rejected trajectories is more effective than learning from chosen examples alone. This procedure is a little similar to DPO, but we achieve it through knowledge augmentation rather than directly converting it into a loss calculation between chosen and rejected trajectories. Additional results can further evident that training a WKM separately performs better than training one single model together with the agent model as well as using few-shot prompts to replace WKM for providing knowledge.

### Analysis

World knowledge can mitigate blind trial-and-error and reduce hallucinatory actions.We compare the number of planning steps for each dataset between three strong baselines and WKM and calculate the average steps of each method. As depicted in Figure 9 (in Appendix F), WKM

    &  &  &  \\   & Seen & Unseen & & **Seen** & Unseen \\  NAT & 23.27 & 23.42 & 4.08 & 20.18 & 21.21 \\ ETO & 19.82 & 22.29 & 3.99 & 24.13 & 26.35 \\ KnowAgent & 18.51 & 24.56 & 4.01 & 21.06 & 24.74 \\ 
**WKM** & **17.66** & **17.92** & **3.97** & **18.74** & **19.59** \\   

Table 2: **Average Steps. The maximum number of steps in ALFWorld and WebShop is 40 and 10. In ScienceWorld, the number of steps ranges from 10 to 120 depending on the task type, with an average of around 40.**

Figure 3: **Ablation Study on Mistral-7B. w/o all means the vanilla experienced agent model training with pure expert trajectories. w/ state is testing agent model with only state knowledge base constraints. w/ task stands for guiding agent model with only task knowledge. w/ task&state is our WKM with both task knowledge guidance and state knowledge constraints. w/o rejected means synthesizing task knowledge solely through expert trajectories. merge stands for training WKM and the agent model together with one single model. prompt means using few-shot prompts to replace the WKM for providing knowledge.**

demonstrates the ability to complete a significant proportion of tasks using the shortest trajectory, indicating that guidance from world knowledge can effectively reduce the agent's blind trial-and-error in the environment. Taking a further perspective from an average standpoint in Table 2, it can be observed that WKM exhibits lower average planning steps compared to other baselines. As ALFWorld can respond to invalid actions, in Table 3, we count the percentage of hallucinatory actions that occurred in trajectories from ALFWorld for each method. The results confirm the effectiveness of our world knowledge model to decrease hallucinatory actions. Furthermore, it is worth noting that most baselines show a prominent increase in the average number of steps and percentage of invalid actions when transitioning from seen tasks to unseen tasks, but WKM can still maintain a relatively low level. This reflects laterally that our world knowledge can still effectively guide the agent model on unseen tasks, highlighting the knowledge generalization brought by the world knowledge model. To see how our world knowledge works, please refer to our case study in Appendix H.

Our instance-level knowledge can generalize better to unseen tasks.To further explore the benefit of using a knowledge model to generate instance-level task knowledge, we carefully survey the task knowledge generated by our WKM and abstract it into dataset-level knowledge for each dataset. Then we retrain the agent model to adapt to new dataset-level knowledge5. As illustrated in Figure 4, we compare the performance of dataset-level knowledge with our instance-level task knowledge (WKM _w/o state_) on ALFWorld and ScienceWorld. It can be observed that our model-generated instance-level knowledge not only surpasses human-designed knowledge on seen tasks but also exhibits even more remarkable performance on unseen tasks, with the improvement in performance on unseen tasks significantly greater than that on seen tasks. This phenomenon straightly reflects the strong generalization ability of our knowledge model compared to rigidly designed knowledge by humans.

Weak knowledge model guides strong agent model planning.In our main experiments, the knowledge model and agent model are based on the same backbone. Here, we explore on ALFWorld what will happen if we use a weak knowledge model to guide a strong agent model. We choose Mistral-7B as the backbone of the knowledge model and ChatGPT and GPT-4 as the agent model. Since we cannot get the token distribution from OpenAI API, we also exhibited in Table 4, the results of both ChatGPT and GPT-4 show distinct advances after being guided by the Mistral-7B world knowledge model, indicating the weak world knowledge model also contains knowledge that the strong model may lack. In the era of LLMs, this inspires us with a new agent learning paradigm: **weak-guide-strong**. Due to its lightweight nature, the weak knowledge model can flexibly adjust its parameters based on the needs of the agent model, which can address the difficulty of large agent models in adapting to new environments through fine-tuning.

Unified World Knowledge Model Training.

We mix the world knowledge collected from all three datasets and jointly train one single world knowledge model to investigate the effect of multi-task world knowledge learning. Figure 5 illustrates the relative performance comparison between multi-task WKM and various baselines, from which we can observe that multi-task WKM not only does not lead to performance degradation but also exhibits visible improvements compared to single-task WKM, especially on WebShop and ScienceWorld. Similar to  which endeavor to train a unified agent model and achieve strong generalization ability to held-out tasks, this observation inspires us with the potential of training a unified world knowledge model that can be applied to help

    &  &  \\   & & Seen & Unseen \\   & REACT & 8.57 & 5.97 \\  & **WKM** w/o state & **12.86** & **8.96** \\   & REACT & 44.29 & 38.05 \\  & **WKM** w/o state & **50.71** & **47.01** \\   

Table 4: **Weak-guide-strong**. The knowledge model here is based on Mistral-7B.

Figure 4: Performance of human-designed dataset-level knowledge compared to WKM generated instance-level knowledge.

Figure 5: Relative performance of multi-task WKM compared to various baselines.

various held-in agent models and also generalize to guide held-out agent models. A more daring idea is whether a unified agent model combined with a unified world knowledge model is the key to Artificial General Intelligence (AGI).

**Explicit state knowledge will hurt the planning performance.** To demonstrate the rationality of our choice to construct a state knowledge base, we explore the effect of directly incorporating state knowledge into the context of the agent model (we retrain the agent model to follow both the task and state knowledge), as shown in Figure 6. The performance of explicit state knowledge is far inferior to our approach of retrieving from a state knowledge base and utilizing probabilistic constraints. It even performs worse than when we remove state knowledge and only include task knowledge. This clearly indicates that blindly extending prompts with a large amount of explicit natural language feedback is lose-more-than-gain for agent planning, and implicit knowledge constraints may be sometimes more prudent.

Case Study.In Figure 10 (Appendix H), we list the trajectories of ETO and our WKM within the same task in ALFWorld to illustrate how world knowledge functions. **The rationales before each action have been omitted to guarantee a clear illustration.** The task is to clean some soapbar and put it in cabinet. Initially, ETO blindly searches for the soapbar in the countertop and cabinet, introducing a lot of irrelevant information and unnecessary context. In the later stages of planning, ETO experiences the hallucination and executes the put action after close the cabinet, causing the environment to become unrecognizable and resulting in a collapse. On the contrary, guided by task knowledge, WKM directly identified the possible locations of the soapbar and successfully found it in the first attempt. Subsequently, WKM efficiently completed the task with precision, adhering to the constraints of state knowledge.

## 5 Related Work

LLM Agents.LLMs have emerged as a promising avenue towards unlocking the potential of Artificial General Intelligence, offering robust support for the development of agent systems [48; 51; 8; 63]. Existing works in this field mainly focuses on agent planning [14; 21; 54; 42], external tools harnessing [39; 23; 43; 29; 32; 35; 46], code generation [45; 21; 31; 11], etc. Recently, there has been an increasing focus on endowing open-source LLMs with agent functionalities through fine-tuning [2; 57; 56; 38; 44; 49]. However, these approaches rely on blindly fitting the probabilities of tokens to learn planning, without having an intimate cognition of the environment. The lack of knowledge can lead to the agent blindly attempting trial-and-error and generating hallucinatory actions.

Knowledge Augmented Agent Planning.Planning  is a crucial capability for intelligent agents to accomplish real-world tasks, often requiring agents to possess rich knowledge and environmental commonsense. Few works have explored the field of knowledge-augmented agent planning. [14; 61; 5] utilize the rich parametric knowledge stored in pre-trained language models to assist agent planners. [7; 20; 59; 64] design structured or natural language knowledge to regulate the actions. However, the above studies require the manual design of fixed prompt templates or task procedures, making it challenging to transfer across different task environments. [63; 55; 6] propose the automation of knowledge generation using language models. However, their knowledge either consists of only global workflow or only local action principles. In contrast, we train our world knowledge model both on global task knowledge and local state knowledge to assist agent planning, and these knowledge sources are derived from the model's self-summary rather than hand-curated.

LLM-based World Model.World model and agent model often co-occur in the domain of reinforcement learning and robotics [13; 9; 19; 37; 26; 4]. With LLMs commonly deemed as the most powerful intelligent machines constructed by humans thus far, the LLM-backed world models have been proposed [61; 10; 13]. In our paper, we attempt to self-synthesize world knowledge and train to obtain a world knowledge model. However, we consider our model to be a world **knowledge model** rather than a **world model** based on the reason that our model is temporarily unable to utilize search algorithms (e.g. MCTS) in conjunction with the agent model to make predictions about the world and we leave this for our future work.

Figure 6: Performance of explicit state knowledge.

Conclusion and Future Work

In this paper, we strive to develop a parametric world knowledge model (WKM) to augment language agent model planning. Our WKM can generate prior task knowledge to guide global planning as well as dynamic state knowledge to regulate local planning. Our extensive results show that our world knowledge can work on both GPT-4 and state-of-the-art open-source models and achieve superior performance compared to various strong baselines. Analytical experiments validate that our WKM can 1) reduce brainless trial-and-error and invalid actions, 2) generalize better to unseen tasks, 3) achieve weak-guide-strong, and 4) be effectively extended to unified world knowledge training. Potential future directions include: 1) building a unified world knowledge model, 2) learning to predict the world like a world model, 3) applying to multi-modal agent planning, etc.

## Limitations

Despite our best efforts, this paper still has some limitations: 1) Our primary intention behind designing the WKM is to compensate for the lack of world knowledge in the agent model. However, determining what a language model knows and doesn't know has been an ongoing challenge that remains unresolved. 2) It is widely acknowledged that world knowledge extends beyond textual representations. While our world knowledge is currently limited to textual information, exploring multi-modal world knowledge models is indeed one of our important future tasks. 3) Our world knowledge model cannot dynamically update with the changes of the world and feedback from the agent. 4) Generating world knowledge can introduce additional inference overhead.