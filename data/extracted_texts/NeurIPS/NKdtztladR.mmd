# Latent Diffusion for Language Generation

Justin Lovelace

Correspondence to <jl3353@cornell.edu>.

Varsha Kishore Chao Wan Eliot Shekhtman Kilian Q. Weinberger

Cornell University, Ithaca, NY

Correspondence to <jl3353@cornell.edu>.

###### Abstract

Diffusion models have achieved great success in modeling continuous data modalities such as images, audio, and video, but have seen limited use in discrete domains such as language. Recent attempts to adapt diffusion to language have presented diffusion as an alternative to existing pretrained language models. We view diffusion and existing language models as complementary. We demonstrate that encoder-decoder language models can be utilized to efficiently learn high-quality language autoencoders. We then demonstrate that continuous diffusion models can be learned in the latent space of the language autoencoder, enabling us to sample continuous latent representations that can be decoded into natural language with the pretrained decoder. We validate the effectiveness of our approach for unconditional, class-conditional, and sequence-to-sequence language generation. We demonstrate across multiple diverse data sets that our latent language diffusion models are significantly more effective than previous diffusion language models. Our code is available at https://github.com/justinlovelace/latent-diffusion-for-language.

## 1 Introduction

Although originally introduced by Sohl-Dickstein et al.  in 2015, diffusion models did not see widespread use until Ho et al.  demonstrated their viability for high-quality image generation in 2020. Since then, research has driven rapid improvements and they have recently surpassed generative adversarial networks on image generation benchmarks  and autoregressive models on density estimation benchmarks , outclassing generative modeling paradigms that have dominated those areas for the better part of a decade. Diffusion models are now, arguably, the most widely used class of generative models for continuous data modalities such as images, audio, and video .

The widespread success of diffusion models across a variety of domains and applications makes them appealing for language generation. However, they have seen less use in discrete domains, where the gradual transition of discrete states to Gaussian noise (and vice versa) is not as natural as in continuous domains. Prior work proposes to learn continuous diffusion models in the space of learnable word embeddings and decodes the continuous generations with a rounding step . However, combining representation learning with the diffusion objective requires careful regularization to avoid collapse.

One breakthrough in image generation was the introduction of latent diffusion , where diffusion models are trained to produce samples from the latent distribution of a pretrained autoencoder. This offloads the task of generating high-frequency details to the autoencoder and enables the diffusion process to focus on the high-level semantics of images. In this paper, we explore the viability of latent diffusion for text generation. We claim that this approach is particularly well-suited for discrete modalities because it offloads the challenge of modeling a discrete distribution to the autoencoder and simplifies the diffusion process by restricting it to the continuous, latent feature space.

We introduce Latent Diffusion for Language Generation (LD4LG), a method that leverages the latent space of a pretrained encoder-decoder network (e.g. BART , T5 ) to learn a high-qualitydiffusion model for text. The latent representations from such models are high-dimensional and input-length dependent -- complicating the use of diffusion models [51; 66]. To address both issues, we learn an additional compression module that maps the high-dimensional encoder representations to a lower-dimensional fixed-length representation. We also learn a corresponding reconstruction network to map these fixed-length features back to high dimensional features that guide the language decoder (via cross-attention) to reconstruct the original language.

The low-dimensional representation is ideally suited for diffusion. For language generation, we use a diffusion model to generate a low-dimensional (fixed-length) latent, which is mapped into a higher dimensional space with the reconstruction network. This high dimensional representation then guides the pre-trained decoder to generate natural language. Our approach naturally combines the continuous, fixed-length diffusion process with discrete, variable length text generation.

We demonstrate that LD4LG is effective for unconditional, class-conditional, and sequence-to-sequence language generation across a variety of datasets. Our approach significantly outperforms recent diffusion language models while using fewer sampling steps. For instance, we achieve a MAUVE score  of.716 for the ROCStories dataset with 250 sampling steps while Diffusion-LM  achieves a MAUVE score of.043 using 2000 sampling timesteps. For the challenging XSum summarization benchmark, we achieve a ROUGE-L of 31.9 with 250 timesteps while the recently proposed DiffuSeq  achieves a ROUGE-L of 14.1 with 2000 timesteps. We also find that the diffusion models offer some benefits over a strong autoregressive baseline. In particular, we observe that our latent language diffusion is less susceptible to memorization and more effective for class-conditional generation.

## 2 Background

Diffusion models [61; 22; 63] are a class of latent variable models that learn to iteratively transform random Gaussian noise, which can be sampled analytically, to a sample from an unknown data distribution specified by a collection of samples. This mapping is defined through a forward diffusion process that iteratively adds Gaussian noise to samples, and a generative process that iteratively "denoises" samples from the Gaussian distribution to obtain samples from the data distribution. We provide a formal description of diffusion models in the appendix.

The diffusion model consists of a denoising network \(}_{}\) trained with a regression objective

\[()=_{t,,}[_{t}]\|}_{}(}+},t )-\|_{2}^{2}]\]

where \(\) is the training data, \(t(,)\) is the timestep, \((,)\) is Gaussian noise, \(_{t}\) defines the noise schedule, and \(_{t}\) is a time-dependent weighting term. The denoising network is therefore trained to denoise a noisy latent, \(_{t}=}+}\), to the clean data, \(\), with a regression objective that emphasizes certain times \(t\). Sampling algorithms start from pure Gaussian noise, \(_{1}(,)\), and utilize the denoising network to iteratively generate latents \(_{t_{1}},_{t_{2}},...,_{t_{T}}\) where \(1=t_{1}>t_{2}>...>t_{T}=0\), with decreasing levels of noise until \(_{0}\) is drawn approximately from the data distribution.

## 3 Latent Diffusion For Language

Figure 1 presents an overview of Latent Diffusion for Language Generation. Our method consists of two main parts. We augment a pretrained encoder-decoder language model with two learnable networks to develop a high-quality language autoencoder with a compact latent space. We then introduce continuous diffusion models that learn to generate samples from the latent distribution of our language autoencoders. These continuous samples can, by design, be decoded into natural language.

### Language Autoencoder

We base our architecture on pretrained encoder-decoder language models (depicted in blue), such as BART  and T5  (we present results with both). By default, we freeze the pre-trained models and learn only the autoencoding modules to accelerate training. The _Language Encoder_, \(E()\), maps variable-length language, represented as a sequence of tokens, \(^{L}\), to a latent representation of the same length, \(E()^{L d_{}}\).

Compression Network.The learnable _Compression Network_ maps the encoder features to a compact latent space that is well-suited for diffusion. We adopt the Perceiver Resampler  architecture, originally developed to compress image features for a vision-language model, which is depicted in Figure 2. The Perceiver Resampler, like the transformer, consists of a stack of alternating multi-head attention (MHA) blocks and feedforward (FF) layers. We refer the reader to Vaswani et al.  for a detailed description of these components. We learn \(\) latent queries \(Z^{ d_{}}\) that iteratively cross-attend to the language encoder features \(E()^{L d_{}}\) to extract information. We follow Alayrac et al.  and allow the latent queries to simultaneously attend to themselves and the frozen encoder representations. We can write the attention layer as

\[Z=Z+(=Z,=[Z;E()])\]

where \(()\) is the multi-head attention operation with queries, q, and keys/values, kv. This design compresses the encoder representations to the fixed sequence length, \(\), of the latents. After each multi-head attention layer, a feedforward layer is applied to the latent query representations.

After the Compression Network maps the input to a fixed sequence length, we reduce the dimensionality of the output to dimension \(d_{}\) with a learnable linear projection. The compression network therefore maps the variable length output of the frozen encoder to a compact latent space

\[=f_{}(E())^{ d_{}}\]

of fixed length \(<L\) and dimensionality \(d_{}<d_{}\) where we will learn our diffusion model.

To ensure that the latent space is appropriately scaled for diffusion, we can optionally constrain the norm of the latent space. Since \(_{(,)}[\|\|_{2}^ {2}]=d_{}\), we can normalize the latent vectors along the feature dimension so that \(\|_{i}\|_{2}^{2}=d_{}\) similar to prior work on text diffusion .

Reconstruction Network.The _Reconstruction Network_ maps the compressed latent space to the feature space expected by the _Language Decoder_. To achieve this, we project \(=f_{}(E())^{ d_{}}\) back up to dimension \(d_{}\), add learnable absolute position embeddings, and pass it through a standard transformer model to obtain features \(g_{}()^{ d_{}}\).

The _Language Decoder_, \(D()\), cross-attends to to these features and generates text autoregressively. We train the compression and reconstruction networks to produce features that guide the decoder to reconstruct the input text

\[ D(g_{}())=D(g_{}(f_{}(E()))\]

with the cross-entropy loss. This gives us a continuous, semantic latent space that can be decoded to natural language.

Figure 1: Overview of our proposed latent language diffusion framework.

Figure 2: Architecture of our Compression Network.

Implementation Details.We utilize BART-base and FLAN-T5-base  as the encoder-decoder language models throughout this work and learn language autoencoders for each dataset. During autoencoder training, we freeze the pre-trained language models and only learn the autoencoding modules. The autoencoder training could likely be amortized across datasets by training a general-purpose language autoencoder on a large corpus of text, but we leave such explorations to future work. We train the autoencoder to reconstruct the input language with the cross-entropy loss. For the diffusion latent space, we set \(=32,d_{}=64\) and utilize 3 layers in both autoencoding modules across all monolingual datasets.

For our machine translation experiments, we utilize MT5-base  to develop our autoencoder. We found it beneficial to jointly fine-tune the language model and the autoencoding modules, likely because the dataset is an order of magnitude larger than our other datasets and therefore benefits from the additional capacity. We use the same latent dimensionality, but only use a single layer for the autoencoding modules. We report full hyperparameter settings in the appendix. We constrain the norm of the latent space across models and datasets except when using FLAN-T5 because it led to a minor degradation in autoencoding performance and downstream generation quality.

### Latent Language Diffusion

Figure 1 outlines our latent language diffusion framework. Given some dataset of natural language, \(\), we can now sample continuous data as \(=f_{}(E())^{ d_{}}\) where \(\). We then train a continuous denoising network, \(}_{}()\), to recover \(\) with the standard regression objective

\[()=_{t,,}[_{t} \|}_{}(}+ },t)-\|_{2}^{2}]\]

with some time-dependent weighting \(_{t}\). In practice, the denoising network is often parameterized as an \(\)-prediction network  or a \(\)-prediction network  where the velocity, \(\), is defined as \(=}-}\). These parameterizations can be interpreted as different weighting functions, \(_{t}\), for the regression objective above (see Salimans and Ho . We adopt the \(\)-parameterization in this work because it has been shown to be effective for latent image diffusion .

For generation, we sample a latent variable, \(_{1}^{ d_{}}( ,)\), that is iteratively denoised to produce a sample, \(=_{0}\), from the distribution of the language autoencoder's latent space. We then generate natural language with the pretrained reconstruction network and language decoder \(=D(g_{}())\). We train our diffusion models with the cosine noise schedule \(_{t}=(0.5 t)^{2}\)[45; 57; 55] by default. For our machine translation experiments, we employ a scaled cosine noise schedule (see subsection E.2 in the appendix for full details) [7; 27]. For generation, we use the DDPM sampler with 250 sampling timesteps. For text generation with the pretrained decoder, we utilize beam search with 4 beams. We train all of our diffusion models with a single Nvidia A6000 GPU except for the machine translation models which are trained with 4 Nvidia A6000 GPUs.

Denoising Network Architecture.Our denoising network, \(}_{}(_{t},t)\), is a pre-LayerNorm transformer [69; 70] with 12 layers and a dimension of 768. We utilize learnable absolute positional encodings and GeGLU activations . Bao et al.  adapted transformers to image diffusion and found that dense connections  between early and late layers are beneficial due to the dense nature of the denoising objective. We adopt this modification to improve the suitability of the transformer for diffusion. The autoencoder latent is projected to the transformer dimension, processed by the transformer, and then projected back to dimensionality of the autoencoder latent to obtain the final prediction. Following prior work [6; 56; 7], we utilize \(\)-conditioning to condition the model on the level of noise. We map \(_{t}\) to a sinusoidal positional embedding  and pass it through an MLP with a single hidden layer to obtain a time embedding. We add this time embedding to the input sequence and apply adaptive layer normalization  conditioned on the time embedding to the output of every feedfoward layer.

Self-ConditioningWe utilize the self-conditioning technique introduced by Chen et al.  which has been shown to improve the quality of diffusion models [8; 67]. The denoising network is typically conditioned on the latent variable and the current timestep as \(}_{t}=}_{}(_{t},t)\). Self-conditioning proposes to condition the network on its estimate of the data from the previous timestep, \(s>t\), to improve the prediction at the current timestep \(}_{t}=}_{}(_{t},t,}_{s})\). During inference, the sampling procedure is inherently iterative and at time \(t\), we have already computed the output of the denoising network for the previous step. Therefore, it does not require any additional applications of the network. We must, however, modify the training procedure so that the denoising network learns to utilize the estimate of the data, and we must define the inference behavior for the first timestep.

For each training step, we sample some time \(t()\) as before. With probability \(p\), we do not provide any estimate of the data for self-conditioning, denoted \(}_{t,}=}_{}(_{t},t,)\). With probability \(1-p\), however, we mimic the inference behavior by first computing \(}_{t,}=}_{}(_{t},t, )\) and then computing an additional estimate \(}_{t}=}_{}(_{t},t, (}_{t,}))\) where \(()\) is the stop-gradient operation. This second estimate is then used to compute the loss. We follow Chen et al.  and set \(p=0.5\).

This training procedure also maintains the capacity for inference without self-conditioning which is utilized to generate the first estimate during sampling. We condition on the previous estimate by concatenating it with the noisy latent along the feature dimension. When the previous estimate is not provided, we concatenate a learnable embedding with the noisy latent.

Class-Conditional Diffusion.For class-conditional diffusion, we have some dataset where each natural language utterance is associated with one of \(C\) class labels representing, for example, the topic of the text. We condition the denoising network on the class label, \(y\), during training, \(}_{t}=}_{}(_{t},t,y)\). We replace the ground truth class label, \(y_{i}\), with a null label, \(y_{}\), with probability \(p=0.1\) to maintain the capacity for unconditional generation. At inference time, we can choose some class \(y\) to guide the sampling process to generate text from the specified class. We condition on class labels by introducing learnable embeddings for all labels, including the null label, and add it to the time embedding.

Sequence-to-Sequence Diffusion.Given some seq2seq dataset consisting of source-target language pairs \((_{},_{})\), we condition our denoising network on the source sequence and generate the target latent \(_{}=f_{}(E(_{}))\). For news summarization, for instance, we generate a latent representation of the summary by conditioning the network on the article text. To achieve this, we introduce a cross-attention layer after every self-attention layer in the denoising network that attends to features from a frozen language encoder.

In general, we can incorporate any language encoder, \(E_{}()\), to extract features from the source text. By default, we use the same pretrained encoder used for our language autoencoder. For our machine translation experiments, we condition our latent diffusion models on representations from a frozen MT5-XL encoder, which we found to be more effective than MT5-base representations. Therefore, given a sample from our seq2seq dataset, \((_{},_{})\), we can compute \(_{}=f_{}(E(_{}))\) and use a modified seq2seq diffusion objective

\[()=_{t,(_{},_{ {rg}}),e}[_{t}\|}_{}(}_{ }+},t,E_{}(_{}))-_{}\|_{2}^{2}].\]

We also utilize classifier-free guidance  to improve sample quality. We jointly learn an unconditional network, \(}_{}(_{t},t)\), and a conditional network, \(}_{}(_{t},t,E(_{}))\), by dropping the conditioning information with probability \(p=0.1\) during training. When we drop the conditioning information, we cross-attend to a learnable embedding instead of the embedded source text. During sampling, we use guidance weight \(w\) and compute the prediction as

\[}_{t}=w}_{}(_{t},t,E(_{}))+(1-w)}_{}(_{t},t).\]

Setting \(w=1.0\) corresponds to the conditional diffusion model while setting \(w>1.0\) strengthens the influence of the conditioning information. We use \(w=2.0\) for the seq2seq tasks and ablate this choice in section 5.

We can also generate multiple outputs \(\) for each input by sampling different latents \(_{1}(,)\). We then select the most promising candidate with Minimum Bayes Risk (MBR) Decoding [15; 34]. In MBR decoding, we define a loss function \(\), such as the negative Rouge, and use it to select a candidate \(_{}=_{}|}_{^{}}(, ^{})\). In our experiments, we use \(||=5\) and denote the results from using MBR decoding as MBR-5. We also report results using the ground truth to select the best candidate \(_{}=_{} (,_{})\) to provide an upper bound on the performance of our method given optimal sample selection. Because this requires knowledge of the ground-truth target text, we refer to this as Oracle sampling.

## 4 Datasets

We evaluate LD4LG on a variety of natural language datasets. **ROCStories** is a corpus of 98k five-sentence commonsense stories, that capture casual and temporal relations. The **AG News Topic

**Classification** dataset consists of news articles across four topics: World, Sports, Business, Sci/Tech with article titles and descriptions from 120k training instances. We focus on generating the article descriptions in this work. The **XSum** dataset consists of BBC articles from 2010 to 2017 covering a wide range of topics (e.g., News, Politics, Sports, etc.). The training split has 204k instances and each example contains a document and a summary. The **QQP** dataset consists of 400k question pairs, where each example is two similar questions and a binary value indicating whether the two questions have the same meaning. The **WMT 2014 English-German** dataset is a widely used machine translation dataset consisting of roughly 4.5 million sentence pairs. We present detailed dataset statistics in the appendix.

### Evaluation Metrics.

We use **MAUVE Score** and **Perplexity** (Ppl) to evaluate the quality of our generated text. MAUVE Score is a metric for open-ended text generation that compares the distribution of generated text with that of reference text using divergence frontiers. We follow Pillutla et al.  and use the GPT-2-Large model  to embed the text. Perplexity measures how likely the generated samples are according to an autoregressive language model; we use GPT-2-Large to compute perplexity.

We also want to quantify the **Diversity** (Div) of generations. We define diversity as \(=_{n=2}^{4}(\{_{i}\})|}{| (\{_{i}\})|}\) where \(\{_{i}\}\) is a set of generated samples . The metrics discussed so far can be optimized by generating samples from the training set. We measure the proportion of generated 4-grams that are found in the training set to quantify the degree of **Memorization** (Mem).

To evaluate the performance for monolingual seq2seq language generation tasks, we utilize **Rouge** and **BERTScore**. Rouge-1/2 measures the number of unigrams/bigrams in the reference that appear in the generated text and Rouge-L measures the longest common sequence between the texts. BERTScore uses contextual embeddings from a pretrained language model to measure the similarity between texts. We follow prior work and use the microsoft/deberta-xlarge-mnli model  to extract contextual embeddings. For our machine translation experiments, we report **SacreBLEU** scores  to ensure fair comparison with prior work.

For our unconditional and class-conditional language generation experiments, we sample 1000 instances from the diffusion model. For the MAUVE reference text, we sample 1000 instances from the test set. We repeat this 5 times and report the mean and standard deviation as \(_{}\). We also compute reference values for our metrics with natural samples from the test set. The reference MAUVE, for instance, is computed between 1000 train and 1000 test samples. Qualitative samples from our models are in the supplemental materials.

## 5 Experiments

### Language Autoencoder

We evaluate the effectiveness of our proposed language autoencoder using heldout examples from our datasets. As a point of comparison, we also evaluate the default behavior of the language models that we use to develop the language autoencoders. A consequence of BART's particular denoising objective is that the pretrained model already generates a copy of the input language, although this is not true of other models such as T5 or FLAN-T5.

We present results for our two most complex datasets, ROCStories and AG News, in Table 1 and present the results for XSum, QQP, and WMT14-En-De, which show similar trends, in the appendix. We observe that our BART-base autoencoder is able to compress the feature space by a factor of \(24\) while improving the fidelity of the reconstructions. Our autoencoding modules are also effective at converting the pretrained FLAN-T5 into a language autoencoder, even though that is different from the model's default behavior. Across both models and all datasets, our language autoencoders are able to achieve near-perfect reconstruction with a low-dimensional latent space.

### Unconditional Language Generation

Baselines.We evaluate our approach's capacity for unconditional language generation with the ROCStories and AG News datasets. We compare against the recently proposed Diffusion-LM model. We also fine-tune the pretrained GPT-2-Medium model, which is roughly \(1.6\) larger than our denoising network, as a strong autoregressive baseline . For sampling from GPT-2, we prompt it with a BOS token and utilize nucleus sampling \((p=0.95)\). We explore different sampling configurations in the appendix and find that they lead to similar conclusions.

Results.We present this comparison in Table 2. We observe that our approach is significantly more effective than Diffusion-LM at modeling language distributions, as demonstrated by the higher MAUVE scores, while requiring fewer sampling steps. Diffusion-LM is unable to model diverse language distributions and exhibits poor diversity. Utilizing high quality latent spaces from pretrained language models improves the effectiveness of our diffusion model. We observe that both language models are highly effective for the AG News dataset, but using BART-base leads to a stronger MAUVE score for the ROCStories dataset. Across both datasets, FLAN-T5-base produces more diverse generations and exhibits less memorization.

While GPT-2 generally achieves strong language generation metrics, it is more susceptible to memorization than LD4LG. For the AG News dataset, GPT-2 exhibits significant memorization and a lower MAUVE score. We do find that GPT-2 samples have lower perplexity. However, measuring perplexity with a pretrained GPT-2 model likely biases the metric towards the fine-tuned GPT-2 model. Moreover, MAUVE scores have a stronger correlation with human judgments of quality .

Benefits of Compression.Because the pretrained BART model already copies the input text, we can ablate the impact of learning a compact latent space by learning a diffusion model directly in the encoder feature space. One complication of this setting is that the sequence length of the BART features vary. During training, the sequence length is simply determined by the sample. During generation, however, we must specify the length. To determine the sequence length for generation, we opt to sample a length from the empirical distribution of lengths in the training set. We refer to this baseline as BART-Diffusion and outline full implementation details in the appendix.

We compare BART-Diffusion with our proposed approach in Table 3. We quantify the speedup by measuring how long it takes each approach to match the peak validation MAUVE of BART-Diffusion. We observe that learning a compact latent space is beneficial both in terms of absolute performance and wall-clock time, reaching the peak MAUVE of BART-diffusion in a quarter of the time. Compressing the latent space along the sequence dimension significantly reduces the overhead per iteration due to the quadratic cost of self-attention, and we also observe faster convergence.

Self-conditioning.We ablate the impact of self-conditioning in Table 4. We find that it significantly improves the MAUVE score and the perplexity of the generated text, but sacrifices some diversity.

    &  &  &  &  \\   & & & Rouge-1/2/L & BLEU & Rouge-1/2/L & BLEU \\  BART-Base & \(L 768\) & \(\) 49.152 & 98.9/98.2/98.8 & 97.5 & 99.6/99.4/99.6 & 98.6 \\ BART-Base Autoencoder & \(32 64\) & 2048 & 99.2/98.5/99.2 & 97.6 & 99.7/99.4/99.7 & 98.8 \\  FLAN-T5-Base & \(L 768\) & \(\) 49.152 & 21.5/11.8/19.4 & 0.7 & 63.6/53.0/59.6 & 42.3 \\ FLAN-T5-Base Autoencoder & \(32 64\) & 2048 & 98.4/96.9/98.4 & 95.8 & 99.1/98.3/99.1 & 96.8 \\   

Table 1: Effectiveness of Language Autoencoder

    & &  &  \\   & Timesteps & MAUVE \(\) & Ppl \(\) & Div \(\) & Mem \(\) & MAUVE \(\) & Ppl \(\) & Div \(\) & Mem \(\) \\  Reference & - & \(.951_{.007}\) & \(21.1_{.3}\) & \(.414_{.003}\) & \(.362_{.003}\) & \(.951_{.014}\) & \(43.6_{.12}\) & \(.658_{.002}\) & \(.385_{.003}\) \\  Diffusion-LM  & 2000 & \(.043_{.006}\) & \(47.3_{.6}\) & \(.128_{.002}\) & \(.434_{.002}\) & \(.012_{.001}\) & \(67.1_{.12}\) & \(.043_{.002}\) & \(.086_{.005}\) \\ LD4LG (BART-Base) & 250 & \(.716_{.019}\) & \(30.6_{.5}\) & \(.331_{.005}\) & \(.461_{.004}\) & \(.866_{.006}\) & \(106.2_{.9}\) & \(.540_{.006}\) & \(.293_{.001}\) \\ LD4LG (FLAN-T5-base) & 250 & \(.481_{.007}\) & \(37.5_{.4}\) & \(.389_{.002}\) & \(.387_{.002}\) & \(.859_{.00}\) & \(.122_{.9}\) & \(.624_{.008}\) & \(.221_{.003}\) \\  GPT-2-Medium & - & \(.788_{.025}\) & \(20.0_{.2}\) & \(.372_{.002}\) & \(.688_{.006}\) & \(.820_{.012}\) & \(.37.3_{.1}\) & \(.532_{.007}\) & \(.829_{.005}\) \\   

Table 2: Unconditional Language Generation Evaluation. The fine-tuned language model is presented in gray.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Despite achieving good performance, LD4LG has important limitations. Sampling from diffusion models is slow due to the iterative generative process. LD4LG improves upon some prior continuous text diffusion models (that use 2000 steps) and only uses 250 sampling steps. However, speeding up the inference process of diffusion models is an active area of research and techniques developed for image diffusion can likely be adapted for LD4LG [65; 57]. Song et al. , for instance, distilled a trained image diffusion model to produce high-quality samples in a single step. We leave the extension of such techniques to language generation as future work. In subsection 5.4, we observe that diffusion models have excellent coverage but MBR decoding fails to identify the best candidate; developing improved sampling procedures or candidate re-ranking methods would likely improve performance for tasks such as summarization and machine translation.

## 7 Related Work

Diffusion models.Diffusion models [61; 22] are a class of generative models that have led to impressive results in image synthesis, recently surpassing Generative Adversarial Networks [17; 12]. These models typically operate directly in pixel-space, learning a distribution over images. Rombach et al.  introduced latent diffusion for image synthesis and demonstrated that they can be learned in the latent space of a pretrained autoencoder. Latent diffusion has since been successful in other domains such as audio synthesis , symbolic music generation , and molecule generation .

Diffusion for Language.Prior work has focused on directly modeling discrete data by designing diffusion processes for discrete state spaces [25; 3; 26]. Li et al.  train a continuous diffusion model in the space of token embeddings that are learned jointly with the denoising objective and decode generations with a rounding step. Strudel et al.  scaled up this approach and instead learn the diffusion model in the space of pretrained word embeddings and find that low-dimensional embeddings are better suited for diffusion. Gong et al.  extend Diffusion-LM  to sequence-to-sequence tasks by concatenating the source and target sequence and only performing diffusion for the target sequence. Chen et al.  map words to arbitrary binary strings, represented as a sequence of real numbers. They then train a continuous diffusion model and round the generated sequences to produce binary strings. The authors also introduce self-conditioning, which we adopt for our method.

## 8 Conclusion

In this work, we demonstrate that latent diffusion is an effective paradigm for language generation. To achieve this, we introduce a method for compressing the high-dimensional, variable-length language representations from pre-trained language models into a compact, fixed-size latent representation that can be decoded into natural language. This compact latent representation is, by design, well-suited for learning continuous latent diffusion models. Our latent language diffusion models are effective for unconditional, class-conditional, and sequence-to-sequence language generation. They offer some benefits over fine-tuned auto-regressive language models and significantly outperform recent diffusion language models across a variety of datasets.