# Break It Down: Evidence for Structural Compositionality in Neural Networks

Michael A. Lepori\({}^{1}\)1  Thomas Serre\({}^{2}\)  Ellie Pavlick\({}^{1}\)

\({}^{1}\)Department of Computer Science \({}^{2}\)Carney Institute for Brain Science

Brown University

###### Abstract

Though modern neural networks have achieved impressive performance in both vision and language tasks, we know little about the functions that they implement. One possibility is that neural networks implicitly break down complex tasks into subroutines, implement modular solutions to these subroutines, and compose them into an overall solution to a task -- a property we term _structural compositionality_. Another possibility is that they may simply learn to match new inputs to learned templates, eliding task decomposition entirely. Here, we leverage model pruning techniques to investigate this question in both vision and language across a variety of architectures, tasks, and pretraining regimens. Our results demonstrate that models often implement solutions to subroutines via modular subnetworks, which can be ablated while maintaining the functionality of other subnetworks. This suggests that neural networks may be able to learn compositionality, obviating the need for specialized symbolic mechanisms.

## 1 Introduction

Though neural networks have come to dominate most subfields of AI, much remains unknown about the functions that they learn to implement. In particular, there is debate over the role of _compositionality_. Compositionality has long been touted as a key property of human cognition, enabling humans to exhibit flexible and abstract language processing and visual processing, among other cognitive processes (Marcus, 2003; Piantadosi et al., 2016; Lake et al., 2017; Smolensky et al., 2022). According to common definitions (Quilty-Dunn et al., 2022; Fodor and Lepore, 2002), a representational system is compositional if it implements a set of discrete constituent functions that exhibit some degree of modularity. That is, _blue circle_ is represented compositionally if a system is able to entertain the concept _blue_ independently of _circle_, and vice-versa.

It is an open question whether neural networks require explicit symbolic mechanisms to implement compositional solutions, or whether they implicitly learn to implement compositional solutions during training. Historically, artificial neural networks have been considered non-compositional systems, instead solving tasks by matching new inputs to learned templates (Marcus, 2003; Quilty-Dunn et al., 2022). Neural networks' apparent lack of compositionality has served as a key point in favor of integrating explicit symbolic mechanisms into contemporary artificial intelligence systems (Andreas et al., 2016; Koh et al., 2020; Ellis et al., 2023; Lake et al., 2017). However, modern neural networks, with no explicit inductive bias towards compositionality, have demonstrated successes on increasingly complex tasks. This raises the question: are these models succeeding by implementing compositional solutions under the hood (Mandelbaum et al., 2022)?

#### Contributions and Novelty:

1. We introduce the concept of _structural compositionality_, which characterizes the extent to which neural networks decompose compositional tasks into subroutines and implement them modularly. We test for structural compositionality in several different models across both language and vision2. 2. We discover that, surprisingly, there is substantial evidence that many models implement subroutines in modular subnetworks, though most do not exhibit perfect task decomposition.
3. We characterize the effect of unsupervised pretraining on structural compositionality in fine-tuned networks and find that pretraining leads to a more consistently compositional structure in language models.

This study contributes to the emerging body of work on "mechanistic interpretability" (Olah, 2022; Cammarata et al., 2020; Ganguli et al., 2021; Henighan et al., 2023) which seeks to explain the algorithms that neural networks implicitly implement within their weights. We make use of techniques from model pruning in order to gain insight into these algorithms. While earlier versions of these techniques have been applied to study modularity in a multitask setting (Csordas et al., 2021), our work is novel in that it applies the method to more complex language and vision models, studies more complex compositional tasks, and connects the results to a broader discussion about defining and measuring compositionality within neural networks.

## 2 Structural Compositionality

Most prior work on compositionality in neural networks has focused on whether they _generalize_ in accordance with the compositional properties of data (Ettinger et al., 2018; Kim and Linzen, 2020; Hupkes et al., 2020). Such work has mostly yielded negative results - i.e., evidence that neural networks fail to generalize compositionally. This work is important for understanding how current models will behave in practice. However, generalization studies alone permit only limited conclusions about how models work.

As discussed above, leading definitions of compositionality are defined in terms of a system's representations, not its behavior. That is, definitions contrast compositional systems (which implement modular constituents) with noncompositional systems (which might, e.g., rely on learned templates). Poor performance on generalization studies does not differentiate these two types of systems, since even a definitionally compositional system might fail at these generalization tasks. For example, a Bayesian network that explicitly represents and composes distinct shape and color properties might nonetheless classify a _blue circle_ as a _red circle_ if it has a low prior for predicting the color blue and a high prior for predicting the color red.

Thus, in this work, we focus on evaluating the extent to which a model's representations are _structured_ compositionally. Consider the task described in Figure 1. In this task, a network learns to select the "odd-one-out" among four images. Three of them follow a compositional rule (they all contain two shapes, one of which is **inside** and **in contact** with the other). One of them breaks this rule. There are at least two ways that a network might learn to solve this type of compositional task. (1) A network might compare new inputs to prototypes or iconic representations of previously-seen inputs, avoiding any decomposition of these prototypes into constituent parts (i.e., it might implement a _non-compositional solution_). (2) A network might implicitly break the task down into subroutines, implement solutions to each, and compose these results into a solution (i.e., it might implement a _compositional solution_). In this case, the subroutines consist of a **(+/- Inside)** detector and a **(+/- Contact)** detector.

If a model trained on this task exhibits _structural compositionality_, then we would expect to find a subnetwork that implements each subroutine within the parameters of that model. This subnetwork should compute one subroutine, and not the other (Figure 1, Bottom Right; "Subnetwork"), and it should be _modular_ with respect to the rest of the network -- it should be possible to ablate this subnetwork, harming the model's ability to compute one subroutine while leaving the other subroutine largely intact (Figure 1, Bottom Right; "Ablation"). However, if a model does not exhibit structural compositionality, then it has only learned the _conjunction_ of the subroutines rather thantheir _composition_. It should not be possible to find a subnetwork that implements one subroutine and not the other, and ablating one subnetwork should hurt accuracy on both subroutines equally (Figure 1, Bottom Center). This definition is related to prior work on modularity in neural networks (Csordas et al., 2021; Hod et al., 2022), but here we specifically focus on modular representations of compositional tasks.

## 3 Experimental Design

### Preliminaries

Here we define terms used in the rest of the paper. **Subroutine:** A binary rule. The \(i^{th}\) subroutine is denoted \(SR_{i}\). **Compositional Rule:** A binary rule that maps input to output according to \(C=SR_{1}\&SR_{2}\), where \(SR_{i}\) is a subroutine. Compositional rules are denoted \(C\). **Base Model:** A model that is trained to solve a task defined by a compositional rule. Denoted \(M_{C}\). **Subnetwork:** A subset of the parameters of a base model, which implements one subroutine. The subnetwork that implements \(SR_{i}\) is denoted \(Sub_{i}\). This is implemented as a binary mask, \(m_{i}\), over the parameters of the base model, \(\), such that \(Sub_{i}=M_{C; m_{i}}\), where \(\) refers to elementwise multiplication.

Figure 1: **(Left)** An illustration of the tasks used to study structural compositionality. Stimuli are generated via the composition of two subroutines: **(+/- Inside)** and **(+/- Contact)**. These stimuli are used to construct **odd-one-out** tasks, where the model is tasked with identifying the image that does not follow a rule from a set of four samples. Here, two objects must be in contact, and one must be inside the other. Rule following images correspond to the upper right quadrant. A model may solve this task in two ways. **(Middle)** It may implement a non-compositional solution, e.g., storing learned template that encodes only the conjunction of the two subroutines. In this case, one should not be able to find a subnetwork that implements one subroutine and does not implement the other. Concretely, there should be no difference in the subnetwork’s performance on examples that depend on computing one subroutine vs. another. Ablating this subnetwork should harm the computation of both subroutines equally. In other words, there should be no difference in accuracy between examples that depend on different subroutines. **(Right)** A model may implement a compositional solution, which computes each subroutine in modular subnetworks and combines them. In this case, one should find a subnetwork that implements, say, **(+/- Inside)**, and this subnetwork should achieve high accuracy on examples that require computing **(+/- Contact)**. In other words, the difference in accuracies between examples that require computing **(+/- Contact)** examples should be positive. Likewise, one should be able to ablate this subnetwork and maintain performance on **(+/- Contact)** while compromising performance on **(+/- Inside)**, and so the difference in performance should be negative. Hypothetical results are represented as differences in performance between both types of examples.

**Ablated Model:** The complement set of parameters of a particular subnetwork. After ablating \(Sub_{i}\), we denote the ablated model \(M_{ablate_{i}}\).

### Experimental Logic

Consider a compositional rule, \(C\), such as the "Inside-Contact" rule described in Figures 1 and 2. The rule is composed of two subroutines, \(SR_{1}\) **(+/- Inside)** and \(SR_{2}\) **(+/- Contact)**. We define an odd-one-out task on \(C\), as described in Section 2. See Figure 1 for three demonstrative examples using the "Inside-Contact" compositional rule. For a given architecture and compositional rule, \(C\), we train a base model, \(M_{C}\), such that \(M_{C}\) solves the odd-one-out task to greater than 90% accuracy3 (Figure 2, Panel A). We wish to characterize the extent to which \(M_{C}\) exhibits structural compositionality. Does \(M_{C}\) learn only the conjunction (effectively entangling the two subroutines), or does \(M_{C}\) implement \(SR_{1}\) and \(SR_{2}\) in modular subnetworks?

Figure 2: Illustration of the experimental design. For brevity, we denote “subroutine” as **SR** in the diagram. **(A)** First, we train a neural network on a compositional task **(Inside-Contact)**, ensuring that it can achieve high accuracy on the task. **(B)** We then optimize a binary mask over weights, such that the resulting subnetwork can compute one subroutine **(+/- Inside)** while ignoring the other **(+/- Contact)**. We evaluate this subnetwork on datasets that require computing the target subroutine **(+/- Inside)**. We also evaluate this subnetwork on datasets that require computing the other subroutine **(+/- Contact)**. We expect success on the first evaluation and failure on the second if the model exhibits structural compositionality. **(C)** We invert the binary mask learned in **(B)**, ablating the subnetwork. We evaluate this on the same two datasets, expecting performance to be harmed on the target subroutine and performance to be high for the other subroutine.

To investigate this question, we will learn a binary mask \(m_{i}\) over the weights \(\) of \(M_{C}\) for each \(SR_{i}\), resulting in a subnetwork \(Sub_{i}\). Without loss of generality, assume \(Sub_{1}\) computes **(+/- Inside)** and \(Sub_{2}\) computes **(+/- Contact)**, and consider investigating \(Sub_{1}\). We can evaluate this subnetwork on two partitions of the training set: (1) **Test Target Subroutine** - Cases where a model must compute the target subroutine to determine the odd-one-out (e.g., cases where an image exhibits **(- Inside, + Contact)**) and (2) **Test Other Subroutine** - Cases where a model must compute the other subroutine to determine the odd-one-out (e.g., cases where the odd-one-out exhibits **(+ Inside, - Contact)**).

Following prior work (Csordas et al., 2021), we assess structural compositionality based on the subnetwork's performance on these datasets, as well as the base model's performance after _ablating_ the subnetwork4. If \(M_{C}\) exhibits structural compositionality, then \(Sub_{1}\) should only be able to compute the target subroutine **(+/- Inside)**, and thus it should perform better on **Test Target Subroutine** than on **Test Other Subroutine**. If \(M_{C}\) entangles the subroutines, then \(Sub_{1}\) will implement both subroutines and will perform equally on both partitions. See Figure 2, Panel B.

To determine modularity, we ablate the \(Sub_{1}\) from the base model and observe the behavior of the resulting model, \(M_{ablate_{1}}\). If \(M_{C}\) exhibits structural compositionality, we expect the two subroutines to be modular, such that ablating \(Sub_{1}\) has more impact on \(M_{ablate_{1}}\)'s ability to compute **(+/- Inside)** than **(+/- Contact)**. Thus, we would expect \(M_{ablate_{1}}\) to perform better on **Test Other Subroutine** than **Test Target Subroutine**. See Figure 2, Panel C. However, if \(M_{C}\) implemented a non-compositional solution, then ablating \(Sub_{1}\) should hurt performance on both partitions equally, as the two subroutines are entangled. Thus, performance on both partitions would be approximately equal.

Expected Results:For each model and task, our main results are the differences in performance between **Test Target Subroutine** and **Test Other Subroutine** for each subnetwork and ablated model. If a model exhibits structural compositionality, we expect the subnetwork to produce a positive difference in performance (**Test Target Subroutine \(>\) Test Other Subroutine**), and the corresponding ablated model to produce a negative difference in performance. Otherwise, we expect no differences in performance. See Figure 1 for hypothetical results.

## 4 Discovering Subnetworks

Consider a frozen model \(M_{C}(;w)\) trained on an odd-one-out task defined using the compositional rule \(C\). Within the weights of this model, we wish to discover a subnetwork that implements \(SR_{i}\)5. We further require that the discovered subnetwork should be as small as possible, such that if the model exhibits structural compositionality, it can be ablated with little damage to the remainder of the network. Thus, we wish to learn a binary mask over the weights of a trained neural network while employing \(L_{0}\) regularization

Most prior work that relies on learning binary masks over network parameters (Cao et al., 2021; Csordas et al., 2021; Zhang et al., 2021; Guo et al., 2021; De Cao et al., 2020, 2022) relies on stochastic approaches, introduced in Louizos et al. (2018). Savarese et al. (2020) introduced _continuous sparsification_ as a deterministic alternative to these stochastic approaches and demonstrated that it achieves superior pruning performance, both in terms of sparsity and subnetwork performance. Thus, we use continuous sparsification to discover subnetworks within our models. See Appendix B for details.

## 5 Vision Experiments

Tasks:We extend the collection of datasets introduced in Zerroug et al. (2022), generating several tightly controlled datasets that implement compositions of the following subroutines: **contact, inside**, and **number**. From these three basic subroutines, we define three compositional rules: **Inside-Contact**, **Number-Contact**, and **Inside-Number**. We will describe the **Inside-Contact** tasks in detail, as the same principles apply to the other two compositional rules (See Appendix E). Thistask contains four types of images, each containing two shapes. In these images, one shape is either inside and in contact with the other **(+ Inside, + Contact)**, not inside of but in contact with the other **(+ Inside, - Contact)**, or neither **(- Inside, - Contact)**. An example is defined as a collection of four images, three of which follow a rule and one of which does not. We train our base model to predict the odd-one-out on a task defined by a compositional rule over contact and inside: images of the type **(+ Inside, + Contact)** follow the rule, and any other image type is considered the odd-one-out. See Figure 3 (Left).

In order to discover a subnetwork that implements each subroutine, we define one odd-one-out task per subroutine. To discover the **+/- Inside** Subroutine, we define **(+ Inside)** to be rule-following (irrespective of contact) and **(- Inside)** to be the odd-one-out. Similarly for the **+/- Contact** Subroutine. See Figure 3 (Middle and Right, respectively). The base model has only seen data where **(+ Inside, + Contact)** images are rule-following. In order to align our evaluations with the base model's training data, we create two more datasets that probe each subroutine. For both, all rule-following images are **(+ Inside, + Contact)**. To probe for **(+/- Inside)**, the odd-one-out for one dataset is always a **(- Inside, + Contact)** image. This dataset is **Test Target Subroutine**, with respect to the subnetwork that implements **(+/- Inside)**. Similarly, to probe for **(+/- Contact)**, the odd-one-out is always a **(+ Inside, - Contact)** image. This dataset is **Test Other Subroutine**, with respect to the subnetwork that implements **(+/- Inside)**.

Methods:Our models consist of a backbone followed by a 2-layer MLP6, which produces embeddings of each of the four images in an example. Following Zerroug et al. (2022) we compute the dot product between each of the four embeddings to produce a pairwise similarity metric. The least similar embedding is predicted to be the "odd-one-out". We use cross-entropy loss over the four images. During mask training, we use \(L_{0}\) regularization to encourage sparsity. We investigate 3 backbone architectures: Resnet50 7(He et al., 2016), Wide Resnet50 (Zagoruyko and Komodakis, 2016), and ViT (Dosovitskiy et al., 2020). We perform a hyperparameter search over batch size and learning rate to find settings that allow each model to achieve near-perfect performance. We then train 3 models with different random seeds in order to probe for structural compositionality.8

After training our base models, \(M_{C}\), we perform a hyperparameter search over continuous sparsification parameters for each subroutine (See Appendix C). One hyperparameter to note is the mask configuration: the layer of the network in which to start masking. After finding the best continuous sparsification parameters, we run the algorithm three times per model, per subroutine, and evaluate on

Figure 3: Three **Inside-Contact** stimuli. The odd-one-out is always the bottom-right image in these examples. **(Right)** An example from the task used to train the base model. **(Middle)** An example from the task used to discover the **+/- Inside** Subroutine. **(Left)** An example from the task used to discover the **+/- Contact** Subroutine.

Test Target Subroutine and **Test Other Subroutine**. Finally, for each subnetwork, \(Sub_{i}\), we create \(M_{ablate_{i}}=M_{C}-Sub_{i}\) and evaluate it on **Test Target Subroutine** and **Test Other Subroutine9**.

## 6 Language Experiments

Tasks:We use a subset of the data introduced in Marvin and Linzen (2019) to construct odd-one-out tasks for language data. Analogous to the vision domain, odd-one-out tasks consist of four sentences, three of which follow a rule and one of which does not. We construct rules based on two forms of syntactic agreement: Subject-Verb Agreement and Reflexive Anaphora agreement10. In both cases, the agreement takes the form of long-distance coordination of the syntactic number of two words in a sentence. First, consider the subject-verb agreement, the phenomenon that renders _the house near the fields is on fire_ grammatical, and _the house near the fields are on fire_ not grammatical.

Accordingly, we define the following sentence types for Subject-Verb agreement: **((Singular/Plural) Subject, {Singular/Plural} Verb)**. Because both **(Singular Subject, Singular Verb)** and **(Plural Subject, Plural Verb)** result in a grammatical sentence, we partition the Subject-Verb Agreement dataset into two subsets, one that targets singular sentences and one that targets plural sentences11. For the Singular Subject-Verb Agreement dataset, base models are trained on a compositional rule that defines **(Singular Subject, Singular Verb)** sentences to be rule-following, and **(Plural Subject, Singular Verb)** and **(Singular Subject, Plural Verb)** sentences to be the odd-one-out. Thus, an odd-one-out example might look like: _the picture by the ministers interest people_. All other tasks are constructed analogously to those used in the vision experiments (See Section 5). The Reflexive Anaphora dataset is constructed simlarly (See Appendix F).

Methods:The language experiments proceed analogously to the vision experiments. The only difference in the procedure is that we take the representation of the [CLS] token to be the embedding of the sentence and omit the MLP. We study one architecture, BERT-Small (Bhargava et al., 2021; Turc et al., 2019), which is a BERT architecture with 4 hidden layers (Devlin et al., 2018).

## 7 Results

Most base models perform near perfectly, with the exception of ViT, which failed to achieve \(>\)90% performance on any of the tasks with any configuration of hyperparameters12. Thus, we exclude ViT from all subsequent analyses. See Appendix D for these results. If the base models exhibit structural compositionality, we expect subnetworks to achieve greater accuracy13 on **Test Target Subroutine** than on **Test Other Subroutine** (difference in accuracies \(>0\)). After ablating subnetworks, we expect the ablated model to achieve greater accuracy on **Test Other** than **Test Target** (difference in accuracies \(<0\)). Across the board, we see the expected pattern. Subnetwork and ablated accuracy differences for Resnet50 and BERT are visualized in Figure 4 (Subnetwork in Blue, Ablated Models in Red). See Appendix A for Wide Resnet50 results, which largely reproduce the results using Resnet50.

For some architecture/task combinations, the pattern of ablated model results is statistically significantly in favor of structural compositionality. See Figure 4 (C, D), where all base models seem to implement both subroutines in a modular fashion. We analyze the layerwise overlap between subnetworks found within one of these models in Appendix K. This analysis shows that there is relatively high overlap between subnetworks for the same subroutine, and low overlap between subroutines. Other results are mixed, such as those found in Resnet50 models trained on **Number-Contact**. Here,we see strong evidence of structural compositionality in Figure 4 (E), but little evidence for it in Figure 4 (F). In this case, it appears that the network is implementing the **(+/- Contact)** subroutine in a small, modular subnetwork, whereas the **(+/- Number)** subroutine is implemented more diffusely. We perform control experiments using randomly initialized models in Appendix I, which show that the pattern of results in (A), (B) and (F) are _not_ significantly different from a random model, while all other results _are_ significantly different.

## 8 Effect of Pretraining on Structural Compositionality

We compare structural compositionality in models trained from scratch to those that were initialized with pretrained weights. For Resnet50, we pretrain a model on our data using SimCLR (See Appendix G for details).

For BERT-Small, we use the pretrained weights provided by Turc et al. (2019). We rerun the same procedure described in Sections 5 and 6. See Appendix A for each base model's performance. Figure 5 contains the results of the language experiments. Across all language tasks, the ablation results indicate that models initialized with pretrained weights more reliably produce modular subnetworks than randomly initialized models. Results on vision tasks are found in Appendix A and do not suggest any benefit of pretraining.

Figure 4: Results from Subnetwork and Ablation studies. For each compositional task, we learn binary masks that result in subnetworks for each subroutine. Resnet50 results in the top row, BERT-Small results in the bottom row. Gray markers indicate that the corresponding base model did not achieve \(>\) 90% accuracy on the compositional task. **(Blue)** The difference between subnetwork performance on **Test Target Subroutine** and **Test Other Subroutine**. If a model exhibits structural compositionality, we expect that a subnetwork will achieve greater performance on the subroutine that it was trained to implement, resulting in values \(>\) 0. **(Red)** After ablating the subnetwork, we evaluate on the same datasets and plot the difference again. We expect that the ablated model will achieve lower performance on the subroutine that the (ablated) subnetwork was trained to implement and higher performance on the other subroutine dataset, resulting in values \(<\) 0. Across the board, we find that our results are largely significantly different from 0, despite the small number of samples. ** indicates significance at p =.01, *** indicates significance at p =.001 See Appendix A for details of this statistical analysis.

## 9 Related Work

This work casts a new lens on the study of compositionality in neural networks. Most prior work has focused on compositional generalization of standard neural models (Yu and Ettinger, 2020; Kim and Linzen, 2020; Kim et al., 2022; Dankers et al., 2022), though some has attempted to induce an inductive bias toward compositional generalization from data (Lake, 2019; Qiu et al., 2021; Zhu et al., 2021). Recent efforts have attempted to attribute causality to specific components of neural networks' internal representations (Ravfogel et al., 2020; Bau et al., 2019; Wu et al., 2022; Tucker et al., 2021; Lovering and Pavlick, 2022; Elazar et al., 2021; Cao et al., 2021). In contrast to these earlier studies, our method does not require any assumptions about where in the network the subroutine is implemented and does not rely on auxiliary classifiers, which can confound the causal interpretation. Finally, Dziri et al. (2023) performs extensive behavioral studies characterizing the ability of autoregressive language models to solve compositional tasks, and finds them lacking. In contrast, our work studies the structure of internal representations and sets aside problems that might be specific to autoregressive training objectives.

More directly related to the present study is the burgeoning field of _mechanistic interpretability_, which aims to reverse engineer neural networks in order to better understand how they function (Olah, 2022; Cammarata et al., 2020; Black et al., 2022; Henighan et al., 2023; Ganguli et al., 2021; Merrill et al., 2023). Notably, Chughtai et al. (2023) recovers universal mechanisms for performing group-theoretic compositions. Though group-theoretic composition is different from the compositionality discussed in the present article, this work sheds light on generic strategies that models may use to solve tasks that require symbolically combining multiple input features.

Some recent work has attempted to characterize modularity within particular neural networks (Hod et al., 2022). Csordas et al. (2021) also analyzes modularity within neural networks using learned binary masks. Their study finds evidence of modular subnetworks within a multitask network: Within a network trained to perform both addition and multiplication, different subnetworks arise for each operation. Csordas et al. (2021) also investigates whether the subnetworks are reused in a variety of contexts, and find that they are not. In particular, they demonstrate that subnetworks that solve particular partitions of compositional datasets (SCAN (Lake and Baroni, 2018) and the Mathematics

Figure 5: Performance differences between **Test Target Subroutine** and **Test Other Subroutine** for both models trained from scratch and pretrained models. Across the board, we see that pretraining produces more modular subnetworks (i.e., reveal a greater disparity in performance between datasets). Pretraining also appears to make our subnetwork-discovery algorithm more robust to random seeds.

Dataset (Saxton et al., 2018)), oftentimes do not generalize to other partitions. From this, they conclude that neural networks do not flexibly combine subroutines in a manner that would enable full compositional generalization. However, their work did not attempt to uncover subnetworks that implement specific compositional subroutines within these compositional tasks. For example, they did not attempt to find a subnetwork that implements a general "repeat" operation for SCAN, transforming "jump twice" into "JUMP JUMP". Our work finds such compositional subroutines in language and vision tasks, and localizes them into modular subnetworks. This finding extends Csordas et al. (2021)'s result on a simple multitask setting to more complex compositional vision and language settings, and probes for subroutines that represent intermediate subroutines in a compositional task (i.e. "inside" is a subroutine when computing "Inside-Contact").

## 10 Discussion

Across a variety of architectures, tasks, and training regimens, we demonstrated that models often exhibit structural compositionality. Without any explicit encouragement to do so, neural networks appear to decompose tasks into subroutines and implement solutions to (at least some of) these subroutines in modular subnetworks. Furthermore, we demonstrate that self-supervised pretraining can lead to more consistent structural compositionality, at least in the domain of language. These results bear on the longstanding debate over the need for explicit symbolic mechanisms in AI systems. Much work is focusing on integrating symbolic and neural systems (Ellis et al., 2023; Nye et al., 2020). However, our results suggest that some simple pseudo-symbolic computations might be learned directly from data using standard gradient-based optimization techniques.

We view our approach as a tool for understanding when and how compositionality arises in neural networks, and plan to further investigate the conditions that encourage structural compositionality. One promising direction would be to investigate the relationship between structural compositionality and recent theoretical work on compositionality and sparse neural networks (Mhaskar and Poggio, 2016; Poggio, 2022). Specifically, this theoretical work suggests that neural networks optimized to solve compositional tasks naturally implement sparse solutions. This may serve as a starting point for developing a formal theory of structural compositionality in neural networks. Another direction might be to investigate the structural compositionality of networks trained using iterated learning procedures (Ren et al., 2019; Vani et al., 2020). Iterated learning simulates the cultural evolution of language by jointly training two communicating agents (Kirby et al., 2008). Prior work has demonstrated that iterated learning paradigms give rise to simple compositional languages. Quantifying the relationship between structural compositionality within the agents and the compositionality of the language that they produce would be an exciting avenue for understanding the relationship between representation and behavior.

One limit of our technical approach is that one must specify which subroutines to look for in advance. Future work might address this by discovering functional subnetworks using unsupervised methods. Additionally, our approach requires us to use causal ablations and control models to properly interpret our results. Future work might try to uncover subnetworks that are necessarily causally implicated in model behavior. Finally, future work must clarify the relationship between structural compositionality and compositional generalization.