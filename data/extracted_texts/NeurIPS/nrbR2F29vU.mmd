# A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models

Alexander G. Reisach

CNRS, MAP5

Universite Paris Cite

F-75006 Paris, France

&Myriam Tami

CentraleSupelec

Universite Paris-Saclay

F-91190 Gif-sur-Yvette, France

&Christof Seiler

Department of Advanced

Computing Sciences, Maastricht

University, The Netherlands

Antoine Chambaz

CNRS, MAP5

Universite Paris Cite

F-75006 Paris, France

&Sebastian Weichwald

Department of Mathematical Sciences

and Pioneer Centre for AI,

University of Copenhagen, Denmark

Correspondence to alexander.reisach@math.cnrs.fr

###### Abstract

Additive Noise Models (ANMs) are a common model class for causal discovery from observational data. Due to a lack of real-world data for which an underlying ANM is known, ANMs with randomly sampled parameters are commonly used to simulate data for the evaluation of causal discovery algorithms. While some parameters may be fixed by explicit assumptions, fully specifying an ANM requires choosing all parameters. Reisach et al. (2021) show that, for many ANM parameter choices, sorting the variables by increasing variance yields an ordering close to a causal order and introduce 'var-sortability' to quantify this alignment. Since increasing variances may be unrealistic and cannot be exploited when data scales are arbitrary, ANM data are often rescaled to unit variance in causal discovery benchmarking.

We show that synthetic ANM data are characterized by another pattern that is scale-invariant and thus persists even after standardization: the explainable fraction of a variable's variance, as captured by the coefficient of determination \(R^{2}\), tends to increase along the causal order. The result is high '\(R^{2}\)-sortability', meaning that sorting the variables by increasing \(R^{2}\) yields an ordering close to a causal order. We propose a computationally efficient baseline algorithm termed '\(R^{2}\)-SornRegress' that exploits high \(R^{2}\)-sortability and that can match and exceed the performance of established causal discovery algorithms. We show analytically that sufficiently high edge weights lead to a relative decrease of the noise contributions along causal chains, resulting in increasingly deterministic relationships and high \(R^{2}\). We characterize \(R^{2}\)-sortability on synthetic data with different simulation parameters and find high values in common settings. Our findings reveal high \(R^{2}\)-sortability as an assumption about the data generating process relevant to causal discovery and implicit in many ANM sampling schemes. It should be made explicit, as its prevalence in real-world data is an open question. For causal discovery benchmarking, we provide implementations of \(R^{2}\)-sortability, the \(R^{2}\)-SornRegress algorithm, and ANM simulation procedures in our library CausalDisco.

Introduction

Causal Discovery in Additive Noise Models.Understanding the causal relationships between variables is a common goal across the sciences (Imbens and Rubin, 2015) and may facilitate reliable prediction, the identification of goal-directed action, and counterfactual reasoning (Spirtes, Glymour, and Scheines, 1993; Pearl, 2009; Peters, Janzing, et al., 2017). Causal reasoning using Structural Causal Models (SCMs) consists of discovering a graph encoding the causal structure between variables, learning the corresponding functions of the SCM, and using them to define and estimate causal quantities of interest. Discovering causal structure requires interventional data or assumptions on the functions and distributions to restrict the class of possible SCMs that describe a given data-generating process (see Glymour et al., 2019). Since collecting interventional data may be costly, infeasible, or unethical, we are interested in suitable assumptions to learn causal structure from observational data. Additive Noise Models (ANMs) encode a popular functional assumption that yields identifiability of the causal structure under various additional assumptions on the noise distributions, given the causal Markov (Kiiveri et al., 1984) and faithfulness (Spirtes, Glymour, and Scheines, 1993) assumptions. In ANMs, the causal graph can be identified by testing the independence of regression residuals (see Hoyer et al., 2008; Mooij et al., 2009; Peters, Mooij, et al., 2011). Another approach is to use assumptions on the relative magnitudes of noise variances for identifiability (Buhlmann et al., 2014; Loh and Buhlmann, 2014; Ghoshal and Honorio, 2018; Chen et al., 2019; Park, 2020). A range of causal discovery algorithms use such assumptions to find candidate causal structures (for an overview see Heinze-Deml et al., 2018; Kitson et al., 2023). Well-known structure learning algorithms include the constraint-based _PC_ algorithm (Spirtes and Glymour, 1991) and the score-based fast greedy equivalence search (_FGES_) algorithm (Meek, 1997; Chickering, 2002). Using a characterization of directed acyclic graphs as level set of a differentiable function over adjacency matrices paved the way for causal discovery employing continuous optimization (Zheng et al., 2018), which has inspired numerous variants (Vowels et al., 2022).

Implicit Assumptions in Synthetic ANM Data Generation.The use of synthetic data from simulated ANMs is common for the evaluation of causal discovery algorithms because there are few real-world datasets for which an underlying ANM is known with high confidence. For such benchmark results to be indicative of how causal discovery algorithms (and estimation algorithms, see Curth et al., 2021) may perform in practice, simulated data need to plausibly mimic observations of real-world data generating processes. Reisach et al. (2021) demonstrate that data in ANM benchmarks exhibit high var-sortability, a measure quantifying the alignment between the causal order and an ordering of the variables by their variances (see Section 3 for the definition). High var-sortability implies a tendency for variances to increase along the causal order. As a consequence, sorting variables by increasing variance to obtain a causal order achieves state-of-the-art results on those benchmarks, on par with or better than those of continuous structure learning algorithms, the _PC_ algorithm, or _FGES_. Var-sortability also explains why the magnitude of regression coefficients may contain more information about causal links than the corresponding p-values (Weichwald et al., 2020). However, an increase in variances along the causal order may be unrealistic because it would lead to downstream variables taking values outside plausible ranges. On top of that, var-sortability cannot be exploited on real-world data with arbitrary data scales because variances depend on the data scale and measurement unit. For these reasons, following the findings of Reisach et al. (2021), Kaiser and Sipos (2022), and Seng et al. (2022), synthetic ANM data are often standardized to control the increase of variances (see, for example, Rios et al., 2021; Rolland et al., 2022; Mogensen et al., 2022; Lorch et al., 2022; Xu et al., 2022; Li et al., 2023; Montagna et al., 2023), which deteriorates the performance of algorithms that rely on information in the variances. As first demonstrated for var-sortability (Reisach et al., 2021), the parameter choices necessary for sampling different ANMs constitute implicit assumptions about the resulting data-generating processes. These assumptions may result in unintended patterns in simulated data, which can strongly affect causal discovery results. Post-hoc standardization addresses the problem only superficially and breaks other scale-dependent assumptions on the data generating process, such as assumptions on the magnitude of noise variances. Moreover, standardization does not alter any scale-independent patterns that may inadvertently arise in ANM simulations. This motivates our search for a scale-invariant criterion that quantifies such scale-independent patterns in synthetic ANM data. Assessing and controlling their impact on causal discovery helps make ANM simulations more realistic and causal discovery benchmark performances more informative.

Contribution.We show that in data from ANMs with parameters randomly sampled following common practice, not only variance, but also the fraction of variance explainable by the other variables, tends to increase along the causal order. We introduce \(R^{2}\)-sortability to quantify the alignment between a causal order and the order of increasing coefficients of determination \(R^{2}\). In \(R^{2}\)-SorntRegress, we provide a baseline algorithm exploiting \(R^{2}\)-sortability that performs well compared to established methods on common simulation-based causal discovery benchmarks. In \(R^{2}\), we propose a sorting criterion for causal discovery that is applicable even when the data scale is arbitrary, as is often the case in practice. Unlike var-sortability, one cannot simply rescale the obtained variables to ensure a desired \(R^{2}\)-sortability in the simulated ANMs. Since real-world \(R^{2}\)-sortabilities are yet unknown, characterizing \(R^{2}\)-sortability and uncovering its role as a driver of causal discovery performance enables one to distinguish between data with different levels of \(R^{2}\)-sortability when benchmarking or applying causal discovery algorithms to real-world data. To understand when high \(R^{2}\)-sortability may be expected, we analyze the emergence of \(R^{2}\)-sortability in ANMs for different parameter sampling schemes. We identify a criterion on the edge weight distribution that determines the convergence of \(R^{2}\) to \(1\) along causal chains. An empirical analysis of random graphs confirms that the weight distribution strongly affects \(R^{2}\)-sortability, as does the average in-degree. The ANM literature tends to focus on assumptions on the additive noise distributions (see for example Shimizu et al.2006; Peters and Buhlmann2014; Park2020), leading to arguably arbitrary choices for the other ANM parameters not previously considered central to the model class (for example, drawing edge weights iid from some distribution). Our analysis contributes to closing the gap between theoretical results on the identifiability of ANMs and the need to make decisions on all ANM parameters in simulations. Narrowing this gap is necessary to ensure that structure learning results on synthetic data are meaningful and relevant in practice.

## 2 Additive Noise Models

We follow standard practice in defining the model class of linear ANMs. Additionally, we detail the sampling process to generate synthetic data.

Let \(X=[X_{1},...,X_{d}]^{}^{d}\) be a vector of \(d\) random variables. The causal structure between the components of \(X\) can be represented by a graph over nodes \(\{X_{1},...,X_{d}\}\) and a set of directed edges between nodes. We encode the set of directed edges by a binary adjacency matrix \(B\{0,1\}^{d d}\) where \(B_{s,t}=1\) if \(X_{s} X_{t}\), that is, if \(X_{s}\) is a direct cause of \(X_{t}\), and \(B_{s,t}=0\) otherwise. Throughout, we assume every graph to be a directed acyclic graph (DAG). Based on a causal DAG, we define an ANM. Let \(=[_{1},,_{d}]^{}\) be a vector of positive iid random variables. Let \(_{N}()\) be a distribution with parameter \(\) controlling the standard deviation. Given a draw \(\) of noise standard deviations, let \(N=[N_{1},...,N_{d}]^{}\) be the vector of independent noise variables with \(N_{t}_{N}(_{t})\). For each \(t\), let \(f_{t}\) be a measurable function such that \(X_{t}=f_{t}((X_{t}))+N_{t}\), where \((X_{t})\) is the set of parents of \(X_{t}\) in the causal graph. We assume that \(f_{1},...,f_{d}\) and \(_{N}()\) are chosen such that all \(X_{1},...,X_{d}\) have finite second moments and zero mean. Since we can always subtract empirical means, the assumption of zero means does not come with any loss of generality; in our implementations we subtract empirical means or use regression models with an intercept.

In this work, we consider linear ANMs and assume that \(f_{1},,f_{d}\) are linear functions. For linear ANMs, we define a weight matrix \(W^{d d}\) where \(W_{s,t}\) is the weight of the causal link from \(X_{s}\) to \(X_{t}\) and \(W_{s,t}=0\) if \(X_{s}\) is not a parent of \(X_{t}\). The structural causal model can be written as

\[X=W^{}X+N.\] (1)

### Synthetic Data Generation

ANMs with randomly sampled parameters are commonly used for generating synthetic data and benchmarking causal discovery algorithms (see for example Scutari2010; Ramsey et al.2018; Kalianathan et al.2020). We examine a pattern in data generated from such ANMs (\(R^{2}\)-sortability, see Section 3.1) and design an algorithm to exploit it (\(R^{2}\)-_SorntRegress_, see Section 3.2). Here, we describe the steps to sample ANMs for synthetic data generation using the following parameters:

**1. Generate the Graph Structure.** We generate the causal graph by sampling a DAG adjacency matrix \(B\{0,1\}^{d d}\) for a given number of nodes \(d\). In any DAG, the variables can be permuted such that its adjacency matrix is upper triangular. We use this fact to obtain a random DAG adjacency matrix from undirected random graph models by first deleting all but the upper triangle of the adjacency matrix and then shuffling the variable order (Zheng et al., 2018). In our simulations, we use the Erdos-Renyi (ER) (Erdos et al., 1960) and scale-free (SF) (Barabasi and Albert, 1999) random graph models for \(P_{}\) with an average node in-degree \(\). We denote the distribution of Erdos-Renyi random graphs with \(d\) nodes and \( d\) edges as \(_{}(d, d)\), and the distribution of scale-free graphs with the same parameters as \(_{}(d, d)\).

2. Define Noise Distributions.In linear ANMs, each variable \(X_{t}\) is a linear function of its parents plus an additive noise variable \(N_{t}\). We choose a distributional family for \(_{N}()\) (for example, zero-mean Gaussian or Uniform) and independently sample standard deviations \(_{1},...,_{d}\) from \(P_{}\). We then set \(N_{t}\) to have distribution \(_{N}(_{t})\) with standard deviation \(_{t}\).

3. Draw Weight Parameters.We sample \(_{s,t}\) for \(s,t=1,...,d\) independently from \(P_{W}\) and define the weight matrix \(W=B[_{s,t}]_{s,t=1,...,d}\), where \(\) denotes the element-wise product.

4. Sample From the ANM.To sample observations from the ANM with given graph (step 1), edge weights (step 2), and noise distributions (step 3), we use that \(-W^{}\) is invertible if \(W\) is the adjacency matrix of a DAG to re-arrange Equation (1) and obtain the data-generating equation:

\[X=(-W^{})^{-1}N.\]

We denote a dataset of \(n\) observations of \(X\) as \(=[^{(1)},...,^{(n)}]^{}^{n d}\). The \(i\)-th observation of variable \(t\) in \(\) is \(_{t}^{(i)}\), and \(^{(i)}^{d}\) is the \(i\)-th observation vector.

## 3 Defining and Exploiting \(}\)-Sortability

We describe a pattern in the fractions of explainable variance along the causal order and propose to measure it as '\(R^{2}\)-sortability'. \(R^{2}\)-sortability is closely related to var-sortability (Reisach et al., 2021), which measures a pattern in the variances of variables in a causal graph that can be exploited for causal discovery. Similarly to var-sortability, \(R^{2}\)-sortability can also be exploited to learn the causal structure of ANMs by use of a simple algorithm. In contrast to var-sortability, \(R^{2}\)-sortability is invariant to rescaling of the variables and the presented algorithm, \(R^{2}\)-_SortRegress_, recovers the causal structure equally well from raw, standardized, or rescaled data.

### From Var-Sortability to \(}\)-Sortability

Var-sortability measures the agreement between an ordering by variance and a causal ordering. In many common ANM simulation schemes, the variances of variables tend to increase along the causal order, leading to high var-sortability. This accumulation of noise along the causal order motivates our investigation of a related pattern. The variance of a variable \(X_{t}\) is given as

\[(X_{t})=({W_{:,t}}^{}X)+(N_{t})=({ W_{:,t}}^{}X)+_{t}^{2}.\]

If the variance \((X_{t})\) increases for different \(X_{t}\) along the causal order, but each noise variance \(_{t}^{2}\) is sampled iid, then the fraction of cause-explained variance

\[({W_{:,t}}^{}X)}{({W_{:,t}}^{}X)+_{t}^ {2}}\]is also likely to increase for different \(X_{t}\) along the causal order. Unlike the variance, we cannot calculate (nor obtain an unbiased estimate of) a variable's fraction of cause-explained variance without knowing its parents in the graph. Instead, we propose to use an upper bound as a proxy for the fraction of cause-explained variance by computing the fraction of explainable variance of a variable \(X_{t}\) given all remaining variables, known as the coefficient of determination

\[R^{2}=1-(X_{t}-[X_{t} X_{\{1,,d \}\{t\}}])}{(X_{t})}\]

(Glantz et al., 2001). In practice, we need to choose a regression model and regress the variable onto all other variables to estimate this quantity. Here, we choose linear models \(M_{t,S}^{}^{|S|},X_{S},X_{S}\) for the regression of \(X_{t}\) onto \(X_{S}\) with \(S\{1,...,d\}\{t\}\) and \(^{|S|}\). We denote the least-squares fit by \(M_{t,S}^{^{*}}\) and estimate the \(R^{2}\) coefficient of determination of \(X_{t}\) given \(X_{S}\) via

\[R^{2}(M_{t,S}^{^{*}},X)=1-(X_{t}-M_{t,S}^{^{ *}}(X_{S}))}{(X_{t})},\ \ \ \ S=\{1,,d\}\{t\}.\]

Per definition, \(R^{2}\) is scale-invariant: it does not change when changing the scale of components of \(X\).

To find a common definition for sortability by different ordering criteria, we introduce a family of \(\)-sortability criteria \(_{}\) that, for different functions \(\), assign a scalar in \(\) to the variables \(X\) and graph \(G\) (with adjacency matrix \(B_{}\)) of an ANM as follows:

\[_{}(X,)=^{d}_{(s t) B_{ }^{i}}((X,s),(X,t))}{_{i=1}^{d}_{(s t ) B_{}^{i}}1}\ \ (a,b)=\{1&a<b\\ &a=b\\ 0&a>b.\] (2)

and \(B_{}^{i}\) is the \(i\)-th matrix power of the adjacency matrix \(B_{}\) of graph \(\), and \((s t) B_{}^{i}\) if and only if at least one directed path from \(X_{s}\) to \(X_{t}\) of length \(i\) exists in \(\). In effect, \(_{}(X,)\) is the fraction of directed paths of unique length between any two nodes in \(\) satisfying the sortability condition in the numerator. Note that other definitions of sortability that count paths differently are possible, see Appendix D for a comparison to two alternative definitions. We obtain the original var-sortability for \((X,t)=(X_{t})\) and denote it as \(_{}\). We obtain \(R^{2}\)-sortability for \((X,t)=R^{2}(M_{t,\{1,,d\}\{t\}}^{^{*}},X)\) and denote it as \(_{R^{2}}\).

If \(_{R^{2}}\) is 1, then the causal order is identified by the variable order of increasing \(R^{2}\) (decreasing if \(_{R^{2}}=0\)). A value of \(_{R^{2}}=0.5\) means that ordering the variables by \(R^{2}\) amounts to a random guess of the causal ordering. \(R^{2}\)-sortability is related but not equal to var-sortability, and an ANM that may be identifiable under one criterion may not be so under the other. We present a quantitative comparison across simulation settings in Appendix C, showing that when \(R^{2}\)-sortability is high, it tends to be close to var-sortability (as well as to a sortability by cause-explained variances). The correspondence may be arbitrarily distorted by re-scaling of the variables since var-sortability is scale sensitive, whereas \(R^{2}\)-sortability is scale-invariant. In the following subsection we show that high \(R^{2}\)-sortability can be exploited similarly to high var-sortability.

### Exploiting \(R^{2}\)-sortability

We introduce '\(R^{2}\)-_SortnRegress_', an ordering-based search algorithm (Teyssier and Koller, 2005) that exploits high \(R^{2}\)-sortability. This procedure follows _Var-SortnRegress_2, a baseline algorithm that obtains state-of-the-art performance in causal discovery benchmarks with high var-sortability (Reisach et al., 2021). _Var-SortnRegress_ is computationally similar to the _Bottom-Up_ and _Top-Down_ algorithms by Ghoshal and Honorio (2018) and Chen et al. (2019), which rely on an equal noise variance assumption. However, unlike the assumptions underlying these algorithms, the \(R^{2}\) criterion is scale-invariant such that \(R^{2}\)-_SortnRegress_ is unaffected by changes in data scale.

In \(R^{2}\)-_SortnRegress_ (Algorithm 1), the coefficient of determination is estimated for every variable to obtain a candidate causal order. Then, a regression of each node onto its predecessors in that order is performed to obtain the candidate causal graph. To encourage sparsity, one can use a penalty function \(()\). In our implementation, we use an L1 penalty with the regularization parameter chosen via the Bayesian Information Criterion (Schwarz, 1978).

As before, we denote by \(M_{t,S}^{}\) a parameterized model of \(X_{t}\) with covariates \(X_{S}\) and parameters \(\). For an example, consider the linear model \(M_{t,S}^{}_{S},_{S}\) with \(_{S}=^{|S|}\). To simplify notation, we define the mean squared error (MSE) of a model as \((_{t},M_{t,S}^{})=n^{-1}_{i=1}^{n}(_{t}^ {(i)}-M_{t,S}^{}(_{S}^{(i)}))^{2}\).

``` Data:\(^{n d}\) Input:\(\) /* Penalty function, for example\(()=\|\|_{1}\) Result: Weight matrix estimate \(^{d d}\) /* Candidate causal order \(=^{d}\) /* Estimate \(R^{2}\)-s using \(d\) regressions (with intercept; cf. Section 2) for\(t=1,,d\)do \(^{*}=_{}(_{t},M_{t,\{1,,d \}\{t\}}^{})\) \(_{t}=R^{2}(M_{t,\{1,,d\}\{t\}}^{^{*}},)\) Find a permutation \(\) that sorts \(\) ascending /* Estimate weights using \(d-1\) regressions (with intercept) \(=^{d d}\) for\(i=2,,d\)do \(t=\{j(j)=i\}\) \(S=\{j(j)<i\}\) \(_{s,t}=_{}(_{t},M_{t,S}^{ })+()\) return\(\) ```

**Algorithm 1**\(R^{2}\)-SortnRegress

## 4 Utilizing \(R^{2}\)-Sortability for Causal Discovery

We compare \(R^{2}\)-_SortnRegress_ to a var-sortability baseline and established structure learning algorithms in the literature across simulated data with varying \(R^{2}\)-sortabilities, and evaluate the algorithms on the real-world protein signalling dataset by Sachs et al. (2005).

### Experiments on Simulated Data

We evaluate and compare \(R^{2}\)-_SortnRegress_ on observations obtained from ANMs simulated with the following parameters:

\[P_{} \{_{}(20,40),_{}(20,40 )\}\] (Erdos-Renyi and scale-free graphs, \[d=20,=2\] ) \[_{N}() =(0,^{2})\] (Gaussian noise distributions) \[P_{} =(0.5,2)\] (noise standard deviations) \[P_{W} =((-1,-0.5)(0.5,1))\] (edge weights)

In this Gaussian setting, the graph is not identifiable unless additional specific assumptions on the choice of edge and noise parameters are employed. We sample \(500\) ANMs (graph, noise standard deviations, and edge weights) using ER graphs, and another \(500\) using SF graphs. From each ANM, we generate \(1000\) observations. Data are standardized to mimic real-world settings with unknown data scales. We compare \(R^{2}\)-_SortnRegress_ to the _PC_ algorithm (Spirtes and Glymour, 1991) and _FGES_(Meek, 1997; Chickering, 2002). Representative for scale-sensitive algorithms, we include _Var-SortnRegress_, introduced by Reisach et al. (2021) to exploit var-sortability, as well as their _RandomRegress_ baseline (we refer to their article for comparisons to further algorithms). For details on the implementations used, see Appendix A. The completed partially directed acyclic graphs (CPDAGs) returned by _PC_ and _FGES_ are favorably transformed into DAGs by assuming the correct direction for any undirected edge if there is a corresponding directed edge in the true graph, and breaking any remaining cycles. We evaluate the structural intervention distance (SID) (Peters and Buhlmann, 2015) between the recovered and the ground-truth DAGs. The SID counts the number of incorrectly estimated interventional distributions between node pairs when using parent-adjustmentin the recovered graph compared to the true graph (lower values are better). It can be understood as a measure of disagreement between a causal order induced by the recovered graph and a causal order induced by the true graph.

The results are shown in Figure 1. To analyze trends, we use window averaging with overlapping windows of a width of \(0.1\) centered around \(0.05,0.1,,0.95\). The lines indicate the change in mean SID from window to window, and the shaded areas indicate the 95% confidence interval of the mean. The means of non-empty windows start at \(0.25\) for ER graphs and at \(0.65\) for SF graphs. For both graph types, there are many instances with a \(R^{2}\)-sortability well above \(0.5\). This is notable since we simulate a standard Gaussian non-equal variance setting with parameters chosen in line with common practice in the literature (see for example the default settings of the simulation procedures provided by Squires 2018; Kalainathan et al. 2020; Zhang et al. 2021). We present an analogous simulation with lower absolute edge weights in \((-0.5,0.1)(0.1,0.5)\) in Appendix A.1, show results in terms of the structural Hamming distance (SHD) in Appendix A.2, and provide a visualization of the distribution of \(R^{2}\)-sortability for higher and lower edge weights in Appendix A.3.

\(R^{2}\)_-SortnRegress_ successfully exploits high \(R^{2}\)-sortability for causal discovery. On ER graphs (Figure 0(a)), \(R^{2}\)_-SortnRegress_ outperforms the _RandomRegress_ baseline when \(R^{2}\)-sortability is greater than \(0.5\). For \(R^{2}\)-sortabilities close to \(1\), \(R^{2}\)_-SortnRegress_ outperforms _PC_ and matches or exceeds the performance of _FGES_. On SF graphs (Figure 0(b)), \(R^{2}\)-sortnabilities are generally higher, and \(R^{2}\)_-SortnRegress_ outperforms _PC_ and _FGES_ for all but the lowest values, even coming close to the performance of _Var-SortnRegress_ on raw data for very high \(R^{2}\)-sortabilities. On raw synthetic data, _Var-SortnRegress_ successfully exploits the high var-sortability typical for many ANM simulation settings. Its performance improves for high \(R^{2}\)-sortability, showing the link between \(R^{2}\)-sortability and var-sortability. However, this performance relies on knowledge of the true data scale to exploit var-sortability. The same is true for other scale-sensitive scores such as the one used by _NOTEARS_(Zheng et al. 2018) and many other algorithms that build upon it. Without information in the variances (as is the case after standardization), _Var-SortnRegress_ is reduced to _RandomRegress_, which consistently performs poorly across all settings. _FGES_ outperforms _PC_, which may be explained by _FGES_ benefitting from optimizing a likelihood specific to the setting, thus using information that is not available to _PC_. Further causal discovery results for settings with different noise and noise standard deviation distributions are shown in Appendix A.4.

### Real-World Data

We analyze the observational part of the Sachs et al. (2005) protein signalling dataset to provide an example of what \(R^{2}\)-sortabilities and corresponding causal discovery performances may be expected in the real world. The dataset consists of \(853\) observations of \(11\) variables, connected by \(17\) edges in the ground-truth consensus graph. We find the \(R^{2}\)-sortability to be \(0.82\), which is above the neutral value of \(0.5\), but not as high as the values for which \(R^{2}\)_-SortnRegress_ outperforms the other methods

Figure 1: Performance comparison on standardized synthetic data in terms of SID (lower is better) using moving window averages. \(R^{2}\)_-SortnRegress_ (blue) performs well if \(R^{2}\)-sortability is high, achieving results competitive with established methods. For reference, we show the performance achieved by _Var-SortnRegress_ on raw data (gray dashed line), which worsens to that of _RandomRegress_ (gray) when the raw data scale is unavailable, as is the case here after standardization.

in the simulation shown in Section 4.1. We run the same algorithms as before on this data and evaluate them in terms of SID and SHD to the ground-truth consensus graph. To gain insight into the robustness of the results, we resample the data with replacement and obtain \(30\) datasets with a mean \(R^{2}\)-sortability of \(0.79\) and mean var-sortability of \(0.65\). The results (mean, [min, max]) are shown in the table below. We find that none of the algorithms perform particularly well (for reference: the empty graph has an SID of \(53\) and an SHD of \(17\)). Although the other algorithms perform better than _RandomRegress_ on average with _PC_ performing best, the differences between them are less pronounced than in our simulation (Section 4.1).

These results showcase the difficulty of translating simulation performances to real-world settings. For benchmarks to be representative of what to expect in real-world data, benchmarking studies should differentiate between different settings known to affect causal discovery performance. This includes \(R^{2}\)-sortability, and we therefore recommend that benchmarks differentiate between data with different levels of \(R^{2}\)-sortability and report the performance of \(R^{2}\)-_SortnRegress_ as a baseline.

## 5 Emergence and Prevalence of \(R^{2}\)-Sortability in Simulations

The \(R^{2}\)-sortability of an ANM depends on the relative magnitudes of the \(R^{2}\) coefficients of determination of each variable given all others, and the \(R^{2}\) coefficients in turn depend on the graph structure, noise variances, and edge weights. Although one can make the additional assumption of an equal fraction of cause-explained variance for all non-root nodes (see, for example, Sections 5.1 of Squires et al., 2022; Agrawal et al., 2023), this does not guarantee a \(R^{2}\)-sortability close to \(0.5\), and the \(R^{2}\) may still carry information about the causal order as we show in Appendix E. Furthermore, one cannot isolate the effect of individual parameter choices on \(R^{2}\)-sortability, nor easily obtain the expected \(R^{2}\)-sortability when sampling ANMs given some parameter distributions, because the \(R^{2}\) values are determined by a complex interplay between the different parameters. By analyzing the simpler case of causal chains, we show that the weight distribution plays a crucial role in the emergence of \(R^{2}\)-sortability. We also empirically assess \(R^{2}\)-sortability for different ANM simulations to elucidate the connection between \(R^{2}\)-sortability and ANM parameter choices.

### The Edge Weight Distribution as a Driver of \(R^{2}\)-Sortability in Causal Chains

Our goal is to describe the emergence of high \(R^{2}\) along a causal chain. We begin by analyzing the node variances. In a chain from \(X_{0}\) to \(X_{p}\) of a given length \(p>0\), the structure of \(\) is fixed. Thus, sampling parameters consists of drawing edge weights \(W_{0,1},...,W_{p-1,p} P_{W}\) and noise standard deviations \(_{0},...,_{p} P_{}\) (cf. Section 2.1). Importantly, these parameters are commonly drawn iid, and we adopt this assumption here. As we show in Appendix B.1, the variance of the terminal node \(X_{p}\) in the resulting chain ANM is lower bounded by the following function of the random parameters

\[_{0}^{2}_{j=0}^{p-1}|W_{j,j+1}|.\] (3)

We assume that the distribution of noise standard deviations \(P_{}\) has bounded positive support, as is commonly the case when sampling ANM parameters (see Squires, 2018; Kalainathan et al., 2020; Zhang et al., 2021, as well as our simulations). We introduce the following sufficient condition for a weight distribution \(P_{W}\) to result in diverging node variances along causal chains of increasing length

\[0<[|V|]<+,V P_{W}.\] (4)

If Equation (4) holds, the lower bound on the node variances given in Equation (3) diverges almost surely by the strong law of large numbers

\[_{0}^{2}_{j=0}^{p-1}|W_{j,j+1}|[p]{}+,\]since the \(W_{j,j+1}\) are sampled iid from \(P_{W}\). This implies that the variance of a terminal node in a chain of length \(p\) also diverges almost surely to infinity as \(p\) goes to infinity. Under the same condition, the \(R^{2}\) of the terminal node given all other nodes converges almost surely to \(1\) as \(p\) goes to infinity. We can show this using the fraction of cause-explained variance as a lower bound on the \(R^{2}\) of a variable \(X_{p}\) and the divergence of the variances to infinity:

\[R^{2} =1-(X_{p}-[X_{p} X_{\{0, ,d\}\{p\}}])}{(X_{p})} 1- (X_{p}-[X_{p} X_{p-1}])}{ (X_{p})}\] \[=1-^{2}}{(X_{p})}[p ]{}1.\]

For a \(P_{W}\) with finite \([|V|]\) with \(V P_{W}\), it is thus sufficient that \([|V|]>0\) for the terminal node's \(R^{2}\) in a \(p\)-chain to converge to \(1\) almost surely as \(p+\), under iid sampling of edge weights and noise standard deviations. The fact that Equation (4) only depends on \(P_{W}\) highlights the importance of the weight distribution as a driver of \(R^{2}\).

Increasingly Deterministic Relationships Along the Causal Order.Our argument for the almost sure convergence of \(R^{2}\) along causal chains relies on a) the divergence of the total variance of the terminal node, given Equation (4) holds, and b) independently drawn noise variances \(^{2}\) from a bounded distribution. Together, these two observations imply that the relative contributions of the noise variances to the total variances tend to decrease along a causal chain. If data are standardized to avoid increasing variances (and high var-sortability), the noise variances tend to vanish3 along the causal chain, resulting in increasingly deterministic relationships that effectively no longer follow an ANM.

### Impact of the Weight Distribution on \(}\)-Sortability in Random Graphs

Understanding the drivers of \(R^{2}\)-sortability in random graphs is necessary to control the level of \(R^{2}\)-sortability in data simulated from ANMs with randomly sampled parameters. Given the importance of the weight distribution in our analysis of \(R^{2}\)-sortability in causal chains, we empirically evaluate the impact of the weight distribution on \(R^{2}\)-sortability in random graphs. We provide additional experiments showing how the weight distribution and average in-degree impact \(R^{2}\)-sortability in Appendix B.2.

We analyze \(R^{2}\)-sortability for different graph sizes. We use ER and SF graphs, that is \(P_{}\{_{}(d,2d),_{}(d,2d)\}\), with \(d\) nodes and \(=2\), and simulate graphs with \(d 5\) such that \(2d\) does not exceed the maximum number of possible edges. Let \(V\) be a random variable with \(V P_{W}\). We test different edge weight distributions \(P_{W}=((-,-0.1)(0.1,))\) with \(>0.1\) chosen such that the \([|V|]\) are approximately evenly spaced in \([-1,2.5]\). We otherwise use the same settings as in Section 4.1. For each graph model, weight distribution, and number of nodes, we independently simulate \(50\) ANMs (graph, noise standard deviations, and edge weights).

The results are shown in Figure 2. We observe that \(R^{2}\)-sortability is consistently higher for edge weight distributions \(P_{W}\) with higher \([|V|]\). For each weight distribution, we observe a slowing

Figure 2: \(R^{2}\)-sortability at different graph sizes for a range of weight distributions with different \([|V|]\) with \(V P_{W}\).

increase in \(R^{2}\)-sortability as graph size increases. Compared to ER graphs (Figure 1(a)), \(R^{2}\)-sortability in SF graphs tends to increase faster in graph size and is consistently higher on average (Figure 1(b)). In all cases, mean \(R^{2}\)-sortability exceeds \(0.5\), and weight distributions with \([|V|]>0\) reach levels of \(R^{2}\)-sortability for which \(R^{2}\)-_SorthRegress_ achieves competitive causal discovery performance in our simulation shown in Section 4.1. Settings with \([||]<0\) may result in lower \(R^{2}\)-sortabilities, but also make detecting statistical dependencies between variables connected by long paths difficult as the weight product goes to \(0\). In summary, we find that the weight distribution strongly affects \(R^{2}\)-sortability and can be used to steer \(R^{2}\)-sortability in simulations.

How the weight distribution affects \(R^{2}\)-sortability also depends on the density and topology of the graph, giving rise to systematic differences between ER and SF graphs. In Appendix B.2 we show the \(R^{2}\)-sortability for graphs of \(50\) nodes with different \([|V|]\) and average in-degree \(\). In Erdos-Renyi graphs, we find that \(R^{2}\)-sortabilities are highest when neither of the parameters take extreme values, as is often the case in simulations. In scale-free graphs we observe \(R^{2}\)-sortabilities close to 1 across all parameter settings. These results show that the graph connectivity interacts with the weight distribution and both can affect \(R^{2}\)-sortability and thereby the difficulty of the causal discovery task.

## 6 Discussion and Conclusion

We introduce \(R^{2}\) as a simple scale-invariant sorting criterion that helps find the causal order in linear ANMs. Exploiting \(R^{2}\)-sortability can give competitive structure learning results if \(R^{2}\)-sortability is sufficiently high, which we find to be the case for many ANMs with iid sampled parameters. This shows that the parameter choices necessary for ANM simulations strongly affect causal discovery beyond the well-understood distinction between identifiable and non-identifiable settings due to properties of the noise distribution (such as equal noise variance or non-Gaussianity). High \(R^{2}\)-sortability is particularly problematic because high \(R^{2}\) are closely related to high variances. When data are standardized to remove information in the variances, the relative noise contributions can become very small, resulting in a nearly deterministic model. Alternative ANM sampling procedures and the \(R^{2}\)-sortability of nonlinear ANMs and structural equation models without additive noise are thus of interest for future research. Additionally, determining what \(R^{2}\)-sortabilities may be expected in real-world data is an open problem. A complete theoretical characterization of the conditions sufficient and/or necessary for extreme \(R^{2}\)-sortability could help decide when to assume and exploit it in practice. Exploiting \(R^{2}\)-sortability could provide a way of using domain knowledge about parameters such as the weight distribution and the connectivity of the graph for causal discovery. On synthetic data, the prevalence of high \(R^{2}\)-sortability raises the question of how well other causal discovery algorithms perform relative to the \(R^{2}\)-sortability of the benchmark, and whether they may also be affected by \(R^{2}\)-sortability. For example, the same causal discovery performance may be less impressive if the \(R^{2}\)-sortability on a dataset is \(0.95\) rather than \(0.5\). In this context, the \(R^{2}\)-_SorthRegress_ algorithm offers a strong baseline that is indicative of the difficulty of the causal discovery task. Our empirical results on the impact of the weight distribution and graph connectivity provide insight on how to steer \(R^{2}\)-sortability within existing linear ANM simulations. Simulating data with different \(R^{2}\)-sortabilities can help provide context to structure learning performance and clarifies the impact of simulation choices, which is necessary to evaluate the real-world applicability of causal discovery algorithms.