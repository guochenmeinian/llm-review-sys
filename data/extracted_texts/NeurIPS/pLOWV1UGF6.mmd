# Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization

Quanqi Hu

Department of Computer Science

Texas A&M University

College Station, TX 77843

quanqi-hu@tamu.edu &Dixian Zhu

Department of Genetics

Stanford University

Stanford, CA 94305

dixian-zhu@stanford.edu &Tianbao Yang

Department of Computer Science

Texas A&M University

College Station, TX 77843

tianbao-yang@tamu.edu

###### Abstract

This paper investigates new families of compositional optimization problems, called non-smooth weakly-convex finite-sum coupled compositional optimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an \(\)-stationary point of the Moreau envelop of the objective function. Additionally, we also extend the algorithm to solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, which feature a nested arrangement of three functions. Lastly, we explore the applications of our algorithms in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization, using empirical studies to showcase the effectiveness of the proposed algorithms.

## 1 Introduction

In this paper, we consider two classes of non-convex compositional optimization problems. The first class is formulated as following:

\[_{^{d}}F():=_{i }f_{i}(_{_{i}}[g_{i}(; )]),\] (1)

where \(\) denotes a finite set of \(n\) items and \(_{i}\) denotes a distribution that could depend on \(i\). The second class is given by:

\[_{^{d}}F():=}_ {i_{1}}f_{i}(}_{j _{2}}g_{i}(_{_{i,j}}[h_{i,j}(;)]) ),\] (2)

where \(_{1}\) denotes a finite set of \(n_{1}\) items and \(_{2}\) denotes a finite set of \(n_{2}\) items and \(_{ij}\) denotes a distribution that could depend on \((i,j)\). For simplicity of discussion, we denote by\(_{_{i}}[g_{i}(;)]:^{d} ^{d_{i}}\) and by \(h_{i,j}()=_{_{i,j}}[h_{i,j}(; )]:^{d}^{d_{2}}\). For both classes of problems, we focus our attention on **non-convex \(F\) with non-smooth non-convex functions \(f_{i}\) and \(g_{i}\)**, which, to the best of our knowledge, has not been studied in any prior works.

The first problem (1) with smooth functions \(f_{i}\) and \(g_{i}\) has been explored in previous works , which is known as finite-sum coupled compositional optimization (FCCO). It is subtly different from standard stochastic compositional optimization (SCO)  and conditional stochastic optimization (CSO) . FCCO has been successfully applied to optimizing a wide range of X-risks  with convergence guarantee, including smooth surrogate losses of areas under the curves  and ranking measures , listwise losses , and contrastive losses . The second problem (2) is a novel class and is referred to as tri-level finite-sum coupled compositional optimization (TCCO). Both problems differ from traditional two-level or multi-level compositional optimization due to the coupling of variables \(i,\) in (1) or the coupling of variables \(i,j,\) in (2) at the inner most level.

One limitation of prior works about non-convex FCCO is that their convergence analysis heavily rely on the smoothness conditions of \(f_{i}\) and \(g_{i}\). This raises a concern about whether existing techniques can be leveraged for solving _non-smooth non-convex FCCO problems_ with non-asymptotic convergence guarantee. Non-smooth non-convex FCCO and TCCO problems have important applications in ML and AI, e.g., group distributionally robust optimization  and two-way partial AUC maximization for deep learning . We defer discussions and formulations of these problems to Section 5. The difficulty for solving smooth FCCO lies at high costs of computing a stochastic gradient \( g_{i}() f_{i}(g_{i}())\) for a randomly sampled \(i\) and the overall gradient \( F()\). To approximate the stochastic gradient, a variance-reduced estimator of \(g_{i}(_{t})\) denoted by \(u_{i,t}\) is usually maintained and updated for sampled data in the mini-batch \(i_{t}\). As a result, the stochastic gradient can be approximated by \( g_{i}(_{t};_{t}) f_{i}(u_{i,t})\), where \(_{t}_{i}\) is a random sample. The overall gradient can be estimated by averaging the stochastic gradient estimator over the mini-batch or using variance-reduction techniques. A key insight of the convergence analysis for smooth FCCO is to bound the following error using the \(L\)-smoothness of \(f_{i}\), which reduces to bounding the error of \(u_{i,t}\) for estimating \(g_{i}(_{t})\):

\[\| g_{i}(_{t};_{t}) f_{i}(u_{i,t})- g_{i}( _{t};_{t}) f_{i}(g_{i}(_{t}))\|^{2}\| g _{i}(_{t};_{t})\|^{2}L\|u_{i,t}-g_{i}(_{t})\|^{2}.\]

A central question to be addressed in this paper is _"Can these gradient estimators be used in stochastic optimization for solving non-smooth non-convex FCCO with provable convergence guarantee"?_ To address this question we focus our attention on a specific class of FCCO/TCCO called **non-smooth weakly-convex (NSWC) FCCO/TCCO**. This approach aligns with many established works on NSWC optimization . Nevertheless, NSWC FCCO/TCCO is more complex than a standard weakly-convex optimization problem because an unbiased stochastic subgradient is not readily accessible. In addition, the convergence measure in terms of the gradient norm of smooth non-convex objectives is not applicable to weakly convex optimization, which will complicate the analysis involving the biased stochastic gradient estimator \( g_{i}(_{t};_{t}) f_{i}(u_{i}^{t})\)1.

**Contributions.** A major contribution of this paper is to present _novel convergence analysis_ of single-loop stochastic algorithms for solving NSWC FCCO/TCCO problems, respectively. In particular,

* For non-smooth FCCO, we analyze the following single-loop updates: \[_{t+1}=_{t}-_{i_{t}} g_{i}(_{t};_{t}) f_{i}(u_{i,t}),\] (3) where \(_{t}\) is a random mini-batch of \(B\) items, and \(u_{i,t}\) is an appropriate variance-reduced estimator of \(g_{i}(_{t})\) that is updated only for \(i_{t}\) at the \(t\)-th iteration. To overcome the non-smoothness, we adopt the tool of Moreau envelop of the objective as in previous works . The key difference of our convergence analysis from previous ones for smooth FCCO is that we bound the inner product \(_{i} g_{i}() f_{i}(u_{i,t}), {}_{t}-_{t}\), where \(}_{t}\) is the solution of the proximal mapping of the objective at \(_{t}\). To this end, specific conditions of \(f_{i},g_{i}\) are imposed, i.e., \(f_{i}\) is weakly convex and non-decreasing and \(g_{i}()\) is weakly convex, under which we establish an iteration complexity of \(T=(^{-6})\) for finding an \(\)-stationary point of the Moreau envelope of \(F()\).
* For non-smooth TCCO, we analyze the following single-loop updates: \[_{t+1}=_{t}-}_{i_{1}^{*}}[}_{j_{2}^{*}}  h_{i,j}(_{t};_{t}) g_{i}(v_{i,j,t})] f _{i}(u_{i,t}),\] (4)* We extend the above algorithms to solving (multi-instance) two-way partial AUC maximization for deep learning, and conduct extensive experiments to verify the effectiveness of the both algorithms.

## 2 Related work

**Smooth SCO.** There are many studies about two-level smooth SCO [27; 38; 10; 19; 3; 28] and multi-level smooth SCO [32; 32; 1; 39]. The complexities of finding an \(\)-stationary point for two-level smooth SCO have been improved from \(O(^{-5})\) to \(O(^{-3})\), and that for multi-level smooth SCO have been improved from a level-dependent complexity of \(O(^{-(7+K)/2})\) to a level-independent complexity of \(O(^{-3})\), where \(K\) is the number of levels. The improvements mostly come from using advanced variance reduction techniques for estimating each level function or its Jacobian and for estimating the overall gradient. Two stochastic algorithms have been developed in  for CSO but suffer a limitation of requiring large batch sizes.

**Smooth FCCO.** FCCO was first introduced in  for optimizing average precision. Its algorithm and convergence analysis was improved in  and . The former work  proposed an algorithm named SOX by using moving average (MA) to estimate the inner function values and the overall gradient. In the smooth non-convex setting, SOX is proved to achieve an iteration complexity of \((^{-4})\). The latter work  proposed a novel multi-block-single-probe variance reduced (MSVR) estimator for estimating the inner function values, which helps achieve a lower iteration complexity \((^{-3})\). Recently,  proposed an extrapolation based estimator for the inner function, which yields a method with a complexity that matches MSVR when \(n^{2/3}\). These techniques have been employed for optimizing various X-risks, including contrastive losses , ranking measures and listwise losses , and other objectives [26; 15]. However, all of these prior works assume the smoothness of \(f_{i}\) and \(g_{i}\). Hence, their analysis is not applicable to NSWC FCCO problems. Our novel analysis of a simple algorithm for NSWC FCCO problems yields an iteration complexity of \(O(^{-6})\) for using the MSVR estimators of the inner functions. The comparison with [26; 15] is shown in Table 1.

**Non-smooth Weakly Convex Optimization.** Analysis of weakly convex optimization with unbiased stochastic subgradients was pioneered by [6; 7]. Optimization of compositional functions that are weakly convex have been tackled in earlier works [8; 9], where the inner function is deterministic or does not involve coupling between two random variables. A closely related work to our NSWC FCCO is weakly-convex concave minimax optimization . Assuming \(f_{i}\) is convex, (1) can be written as: \(_{}_{^{n}}_{i} _{i},g_{i}()-f_{i}^{*}(_{i})\), where \(f_{i}^{*}()\) is the convex conjugate of \(f_{i}\). It can be solved using existing methods [22; 31; 41; 43; 17] but with several limitations: (i) the algorithms in [22; 31; 41; 43] have a comparable complexity of \(O(1/^{6})\) but have unnecessary double loops which require setting the number of iterations for the inner loop; (ii) the algorithm in  is single loop but has a worse complexity of \((1/^{8})\); (iii) these existing algorithms and analysis does not account for complexity of updating all coordinates of \(\), which could be prohibitive in

   Method & Objective & Smoothness & Weak Convexity & Monotonicity & Complexity \\  SOX  & (1) & \(f_{i},g_{i}\) & none & none & \((^{-4})\) \\ MSVR  & (1) & \(f_{i},g_{i}\) & none & none & \((^{-3})\) \\  SONX (Ours) & (1) & none & \(f_{i},g_{i}\) & \(f_{i}\) & \((^{-6})\) \\ SONT (Ours) & (2) & none & \(f_{i},g_{i},h_{i,j}\) & \(f_{i},g_{i}\) & \((^{-6})\) \\ SONT (Ours) & (2) & \(h_{i,j}\) & \(f_{i},g_{i}\) & \(f_{i},g_{i}\) & \((^{-6})\) \\    
   _{t}^{1}\) and \(_{t}^{2}\) are random mini-batches of \(B_{1}\) and \(B_{2}\) items, respectively, and \(u_{i,t}\) is an appropriate variance-reduced estimator of \(}_{j_{2}}g_{i}(h_{ij}(_{t}))\) that is updated only for \(i_{t}^{1}\), and \(v_{i,j,t}\) is an appropriate variance-reduced estimator of \(h_{i,j}(_{t})\) that is updated only for \(i_{t}^{1}\), \(_{t}^{1},j_{t}^{2}\). To prove the convergence, we impose conditions of \(f_{i},g_{i},h_{i,j}\), i.e., \(f_{i}\) is weakly convex and non-decreasing and \(g_{i}()\) is weakly convex and non-decreasing (or monotonic), \(h_{ij}\) is weakly convex (or smooth), and establish an iteration complexity of \(T=(^{-6})\) for finding an \(\)-stationary point of the Moreau envelope of \(F()\).

Table 1: Comparison with prior works for solving (1) and (2). In the monotonicity column, notation \(\) means the given function is required to be non-decreasing. If not specified, the given function is only required to be monotone.

many applications; iv) these approaches are not applicable to NSWC FCCO/TCCO with weakly convex \(f_{i}\). In fact, the double loop algorithm has been leveraged and extended to solving the two-way partial AUC maximization problem, a special case of NSWC FCCO , by sampling and updating a batch of coordinates of \(\) at each iteration. However, it is less practical thus not implemented and its analysis did not explicitly show the convergence rate dependency on \(n_{+},n_{-}\) and the block batch size.

A special case of NSWC SCO problem was considered in , which is given by

\[_{x}f(x,g(x)),f(x,u)=_{}[u+ (0,g(x;)-u)], g(x)=_{}[g(x;)].\]

They proposed two methods, SCS for smooth \(g(x)\) and SCS with SPIDER for non-smooth \(g(x)\). For both proposed methods, they proved a sample complexity of \((1/^{6})\) for achieving an \(\)-stationary point of the objective's Moreau envelope 2. We would like to remark that the above problem with a non-smooth \(g(x)\) is a special case of NSWC FCCO with only a convex outer function, one block and no coupled structure. Nevertheless, their algorithm for non-smooth \(g()\) suffers a limitation of requiring a large batch size in the order of \(O(1/^{2})\) for achieving the same convergence.

Finally, we would like to mention that non-smooth convex or strongly convex SCO problems have been considered in [27; 42; 26], which, however, are out of scope of the present work.

## 3 Preliminaries

Let \(\|\|\) be the Euclidean norm of a vector and spectral norm of a matrix. We use \(_{C}[]\) to denote the Euclidean projection onto \(\{v^{m}:\|v\| C\}\). For vectors, inequality notations including \(,,>,<\) are used to denote element-wise inequality. For an expectation function \(f()=_{}[f(;)]\), let \(f(;)=|}_{}f(;)\) be its stochastic unbiased estimator evaluated on a sample batch \(\). A stochastic unbiased estimator is said to have bounded variance \(^{2}\) if \(_{}[\|f()-f(;)\|^{2}]^{2}\). The Jacobian matrix of function \(f:^{m_{1}}^{m_{2}}\) is in dimension \(^{m_{1} m_{2}}\). We recall the definition of general subgradient and subdifferential following [6; 24].

**Definition 3.1** (subgradient and subdifferential).: Consider a function \(f:^{n}\{\}\) and a point with \(f(x)\) finite. A vector \(v^{n}\) is a general subgradient of \(f\) at \(x\), if

\[f(y) f(x)+ v,y-x+o(\|y-x\|),y x.\]

The subdifferential \( f(x)\) is the set of subgradients of \(f\) at point \(x\).

For simplicity, we abuse the notation and also use \( f(x)\) to denote one subgradient from the corresponding subgradient set when no confusion could be caused. We use \( f(x;)\) to represent a stochastic unbiased estimator of the subgradient \( f(x)\) that is evaluated on a sample batch \(\). A function is called \(C^{1}\)-smooth if it is continuously differentiable. A function \(f:^{n}\{\}\) and a point with respect to each element of the input. Note that if a Lipschitz continuous function \(f:O^{m_{2}}\) is assumed to be non-increasing (resp. non-decreasing), where the domain \(O^{m_{1}}\) is open, then all subgradients of \(f\) are element-wise non-positive (resp. non-negative). We refer the details to Appendix D.1.

A function \(f\) is _\(C\)-Lipschitz continuous_ if \(\|f(x)-f(y)\| C\|x-y\|\). A differentiable function \(f\) is _\(L\)-smooth_ if \(\| f(x)- f(y)\| L\|x-y\|\). A function \(f:^{d}\{\}\) is _\(\)-weakly-convex_ if the function \(f()+\|\|^{2}\) is convex. A vector-valued function \(f:^{d}\{\{\}\}^{m}\) is called \(\)-weakly-convex if it is \(\)-weakly-convex for each output. It is difficult sometimes impossible to find an \(\)-stationary point of a non-smooth weakly-convex function \(F\), i.e., \((0, F())\). For example, an \(\)-stationary point of function \(f(x)=|x|\) does not exist for \(0<1\) unless it is the optimal solution. To tackle this issue,  proposed to use the stationarity of the problem's Moreau envelope as the convergence metric, which has become a standard metric for solving weakly-convex problems [7; 22; 31; 41; 43; 17]. Given a weakly-convex function \(:^{m}\), its Moreau envelope and proximal map with \(>0\) are constructed as

\[_{}(x):=_{y}\{(y)+\|y-x\|^{2}\}, _{}(x):=*{arg\,min}_{y}\{(y) +\|y-x\|^{2}\}.\]

The Moreau envelope is an implicit smoothing of the original problem. Thus it attains a continuous differentiation. As a formal statement, the following lemma follows from standard results [6; 18].

**Lemma 3.2**.: _Given a \(\)-weakly-convex function \(\) and \(<^{-1}\), the envelope \(_{}\) is \(C^{1}\)-smooth with gradient given by \(_{}(x)=^{-1}(x-_{}(x))\)._```
1:Initialization: \(_{0}\), \(\{u_{i,0}:i\}\).
2:for\(t=0,,T-1\)do
3: Draw sample batches \(_{1}^{t}\), and \(_{2,i}^{t}_{i}\) for each \(i_{1}^{t}\).
4:\(u_{i,t+1}=(1-)u_{i,t}+ g_{i}(_{t};_{2,i}^{t})+(g_{i}(_{t};_{2,i}^{t})-g_{i}(_{t -1};_{2,i}^{t})), i_{1}^{t}\\ u_{i,t}, i_{1}^{t}\)
5: Compute \(G_{t}=}_{i_{1}^{t}} g_{i}(_{t };_{2,i}^{t}) f_{i}(u_{i,t})\)
6: Update \(_{t+1}=_{t}- G_{t}\)
7:endfor ```

**Algorithm 1** Stochastic Optimization algorithm for Non-smooth FCCO (SONX)

Moreover, for any point \(x^{m}\), the proximal point \(:=_{}(x)\) satisfies 

\[\|-x\|=\|_{}(x)\|,() (x),(0,())\|_{ }(x)\|.\]

Thus if \(\|_{}(x)\|\), we can say \(x\) is close to a point \(\) that is \(\)-stationary, which is called nearly \(\)-stationary solution of \((x)\).

## 4 Algorithms and Convergence

### Non-Smooth Weakly-Convex FCCO

In this section, we assume the following conditions hold for the FCCO problem (1).

**Assumption 4.1**.: For all \(i\), we assume that

* \(f_{i}\) is \(_{f}\)-weakly-convex, \(C_{f}\)-Lipschitz continuous and non-decreasing;
* \(g_{i}()\) is \(_{g}\)-weakly-convex and \(g_{i}(;)\) is \(C_{g}\)-Lipschitz continuous;
* Stochastic gradient estimators \(g_{i}(;)\) and \( g_{i}(;)\) have bounded variance \(^{2}\).

**Proposition 4.2**.: _Under Assumption 4.1, \(F()\) in (1) is \(_{F}\) weakly convex with \(_{F}=}_{g}C_{f}+_{f}C_{g}^{2}\)._

One challenge in solving FCCO is the lack of access to unbiased estimation of the subgradients \(_{i} g_{i}() f_{i}(g_ {i}())\) due to the expectation form of \(g_{i}()\) inside a non-linear function \(f_{i}\). A common solution in existing works for solving smooth FCCO is to maintain function value estimators \(\{u_{i}:i\}\) for \(\{g_{i}():i\}\), and approximate the true gradient by a stochastic version \(}_{i_{1}} g_{i}(;_{2}) f_{i}(u_{i})\)[26; 15], where \(_{1}\), \(_{2}\) are sampled mini-batches. Simply using a mini-batch estimator of \(g_{i}\) inside \(f_{i}\) does not ensure convergence if mini-batch size is small.

Inspired by existing algorithms of smooth FCCO, a simple method for solving non-smooth FCCO is presented in Algorithm 1 referred to as SONX. A key step is the step 4, which uses the multi-block-single-probe variance reduced (MSVR) estimator proposed in  to update \(\{u_{i}:i\}\) in a block-wise manner. It is an advanced variance reduced update strategy for multi-block variable inspired by STORM . In the update of MSVR estimator, for each sampled \(i_{1}^{t}\), \(u_{i,t}\) is updated following a STORM-like rule with a specialized parameter \(=}{B_{1}(1-)}+(1-)\) for the error correction term. For the unsampled \(i_{1}^{t}\), no update for \(u_{i,t}\) is needed. When \(=0\), the estimator becomes the moving average estimator analyzed in  for smooth FCCO, which is also analyzed in the Appendix. With the function values of \(\{g_{i}(_{t}):i\}\) well-estimated, the gradient can be approximated by \(G_{t}\) in step 5. Next, we directly update \(_{t}\) by subgradient descent using the stochastic gradient estimator \(G_{t}\). Note that unlike existing works on smooth FCCO that often maintain a moving average estimator  or a STORM estimator  for the overall gradient to attain better rates, this is not possible in the non-smooth case as those variance reduction techniques for the overall gradient critically rely on the Lipschitz continuity of \( F\), i.e., the smoothness of \(F\).

### Non-Smooth Weakly-Convex TCCO

In this section, we consider non-smooth TCCO problem and aim to extend Algorithm 1 to solve it. First of all, for convergence analysis and to ensure the weak convexity of \(F()\) in (2), we make the following assumptions.

**Assumption 4.3**.: For all \((i,j)_{1}_{2}\), we assume that ```
1: Initialization: \(_{0}\), \(\{u_{i,0}:i_{1}\}\), \(v_{i,j,0}=h_{i,j}(_{0};^{0}_{3,i,j})\) for all \((i,j)_{1}_{2}\).
2:for\(t=0,,T-1\)do
3: Sample batches \(^{t}_{1}_{1}\), \(^{t}_{2}_{2}\), and \(^{t}_{3,i,j}_{i,j}\) for \(i^{t}_{1}\) and \(j^{t}_{2}\).
4:\(v_{i,j,t+1}=_{_{h}}[(1-_{1})v_{i,j,t}+_{1}h_{ i,j}(_{t};^{t}_{3,i,j})+_{1}(h_{i,j}(_{t}; ^{t}_{3,i,j})-h_{i,j}(_{t-1};^{t}_{3,i,j}))],\\ v_{i,j,t},(i,j)^{t}_{1}^{t}_{2} (i,j)^{t}_{1}^{t}_{2}\]
5:\(u_{i,t+1}=(1-_{2})u_{i,t}+}_{j ^{t}_{2}}[_{2}g_{i}(v_{i,j,t})+_{2}(g_{i}(v_{i,j,t})-g_{i}(v_{i,j,t-1 })], i^{t}_{1}\\ u_{i,t}, i^{t}_{1}\)
6:\(G_{t}=}_{i^{t}_{1}}[(} _{i^{t}_{2}} h_{i,j}(_{t};^{t}_{3,i,j}) g_{i}(v_{i,j,t})) f_{i}(u_{i,t})]\)
7: Update \(_{t+1}=_{t}- G_{t}\)
8:endfor ```

**Algorithm 2** Stochastic Optimization algorithm for Non-smooth TCCO (SONT)

```
1: Initialization: \(_{0}\), \(\{u_{i,0}:i_{1}\}\), \(v_{i,j,0}=h_{i,j}(_{0};^{0}_{3,i,j})\) for all \((i,j)_{1}_{2}\).
2:for\(t=0,,T-1\)do
3: Sample batches \(^{t}_{1}_{1}\), \(^{t}_{2}_{2}\), and \(^{t}_{3,i,j}_{i,j}\) for \(i^{t}_{1}\) and \(j^{t}_{2}\).
4:\(v_{i,j,t+1}=_{_{h}}[(1-_{1})v_{i,j,t}+_{1}h_ {i,j}(_{t};^{t}_{3,i,j})+_{1}(h_{i,j}(_{ t};^{t}_{3,i,j})-h_{i,j}(_{t-1};^{t}_{3,i,j}))],\\ v_{i,j,t},(i,j)^{t}_{1}^{t}_{2}\)
5:\(u_{i,t+1}=(1-_{2})u_{i,t}+}_{j^{t}_{2}}[_{2}g_{i}(v_{i,j,t})+_{2}(g_{i}(v_{i,j,t})-g_{i}(v_{i,j,t- 1})], i^{t}_{1}\\ u_{i,t}, i^{t}_{1}\)
6:\(G_{t}=}_{i^{t}_{1}}[(} _{i^{t}_{2}} h_{i,j}(_{t};^{t}_{3,i,j}) g_{i}(v_{i,j,t})) f_{i}(u_{i,t})]\)
7: Update \(_{t+1}=_{t}- G_{t}\)
8:endfor ```

**Algorithm 3** Stochastic Optimization algorithm for Non-smooth TCCO (SONT)

The weak convexity of \(F()\) in (2) is guaranteed by the following Proposition.

**Proposition 4.4**.: _Under Assumption 4.3, \(F()\) in (2) is \(_{F}\)-weakly-convex with \(_{F}=}(}L_{h}C_{g}+_{g}C_{h}^{2})C_{f}+_{f}C_ {g}^{2}C_{h}^{2}\)._

We extend SONX to Algorithm 2 for (2), which is referred to as SONT. For dealing with the extra layer of compositional problem, we maintain another multi-block variable to track the extra layer of function value estimation. To understand this, we first write down the true subgradient:

\[ F()=}_{i_{1}}[ (}_{j_{2}} h_{i,j}( ) g_{i}(h_{i,j}())) f_{i}( }_{j_{2}}g_{i}(h_{i,j}()) )].\]

To approximate this subgradient, we need the estimations of \(}_{j_{2}}g_{i}(h_{i,j}())\) and \(h_{i,j}()\), which can be tracked by using MSVR estimators denoted by \(\{u_{i,t}:i_{1}\}\) and \(\{v_{i,j,t}:(i,j)_{1}_{2}\}\), respectively. As a result, a stochastic estimation of \( F(_{t})\) is computed in step 6 of Algorithm 2, and the model parameter is updated similarly as before.

### Convergence Analysis

In this section, we present the proof sketch of the convergence guarantee for Algorithm 1. The analysis for Algorithm 2 follows in a similar manner. The detailed proofs can be found in Appendix A (please refer to the supplement). Before starting the proof, we define a constant \(M^{2} C_{f}^{2}C_{g}^{2}\) so that under Assumption 4.1 we have \(_{t}[\|G_{t}\|^{2}] M^{2}\). Then we start by giving the error bound of the MSVR estimator in Algorithm 1. The following norm bound of the estimation error follows from the squared-norm error bound in Lemma 1 from , whose proof is given in Appendix D.3.

**Lemma 4.5**.: _Consider the update for \(\{u_{i,t}:i\}\) in Algorithm 1. Assume \(g_{i}\) is \(C_{g}\)-Lipshitz for all \(i\). With \(=}{B_{1}(1-)}+(1-)\), \(\), we have_

\[_{i}\|u_{i,t+1}-g_{i}(_{t+1})\|(1-}{2n})^{t+1}_{i }\|u_{i,0}-g_{i}(_{0})\|+}{B_{2} ^{1/2}}+M}{B_{1}^{1/2}}.\]

For simplicity, denote by \(}_{t}:=_{F/}(_{t})\). Then using the definition of Moreau envelope and the update rule of \(_{t}\), we can obtain a bound for the change in the Moreau envelope,

\[_{t}[F_{1/}(_{t+1})] F_{1/}(_{t})+(}_{t}-_{t},_{t}[G_{t}])+M^{2}}{2}.\] (5)

where \(_{t}[G_{t}]=_{i_{1}} g_{i}( _{t}) f_{i}(u_{i,t})\) is the subgradient approximation based on the MSVR estimator \(u_{i,t}\) of the inner function value. This is a standard result in weakly-convex optimization To bound the inner product \(}_{t}-_{t},_{t}[G_{t}]\) on the right-hand-side of (5), we apply the assumptions that \(f_{i}\) is weakly-convex, Lipschitz continuous and non-decreasing, and \(g_{i}\) is weakly-convex. Its upper bound is given as follows.

\[(}_{t}-_{t})^{}_{t}[G_{t}] F(}_{t})-F(_{t})+_{i}[f_{i}(g_{ i}(_{t}))-f(u_{i,t})- f(u_{i,t})^{}(g_{i}(_{t})-u_{ i,t})\] \[+_{f}\|g_{i}(_{t})-u_{i,t} \|^{2}+(C_{f}}{2}+_{f}C_{g}^{2})\|}_{t}- _{t}\|^{2}].\] (6)

Due to the \(_{f}\)-weak convexity of \(F()\), we have \((-_{F})\)-strong convexity of \( F()+}{2}\|_{t}-\|^{2}\). Then it follows \(F(}_{t})-F(_{t})(}{2}-)\| _{t}-}_{t}\|^{2}\). Combining this with inequalities (5), (6), and setting \(\) sufficiently large we have

\[&_{t}[F_{1/}(_{t+1})]  F_{1/}(_{t})+M^{2}}{2}+ {}{n}_{i}[-}{2}\| _{t}-}_{t}\|^{2}\\ &+f_{i}(g_{i}(_{t}))-f(u_{i,t})- f_{i}(u_{ i,t})^{}(g_{i}(_{t})-u_{i,t})+_{f}\|g_{i}(_{t})-u_{ i,t}\|^{2}].\] (7)

Recall Lemma 3.2, we have \(\|_{t}-}_{t}\|^{2}=^{2}}\| F _{1/}(_{t})\|^{2}\). Moreover, the last three terms on the R.H.S of inequality (7) can be bounded using the Lipschitz continuity of \(f_{i}\) and the error bound given in Lemma 4.5. Then we can conclude the complexity of SONX with the following theorem.

**Theorem 4.6**.: _Under Assumption 4.1 with \(=}{B_{1}(1-)}+(1-)\), \(=(B_{2}^{4})\), \(=(B_{2}^{1/2}^{4}}{n})\), and \(=_{F}+_{g}C_{f}+2_{f}C_{g}^{2}\), Algorithm 1 converges to an \(\)-stationary point of the Moreau envelope \(F_{1/}\) in \(T=(B_{2}^{1/2}}^{-6})\) iterations._

**Remark.** Similar to the complexity for smooth FCCO problems , Theorem 4.6 guarantees that SONX for NSWC FCCO has a parallel speed-up in terms of the batch size \(B_{1}\) and linear dependency on \(n\). The dependency of the complexity on the batch size \(B_{2}\) is due to the use of MSVR estimator, which matches the results in . If the MSVR estimator in SONX is replaced by moving average estimator, the complexity becomes \((B_{2}}^{-8})\) (cf. Appendix B).

Following a similar proof strategy, the convergence guarantee of Algorithm 2 is given below.

**Theorem 4.7**.: _(Informal) Under Assumption 4.3, with appropriate values of \(_{1},_{2},_{1},_{2},\) and a proper constant \(\), Algorithm 2 converges to an \(\)-stationary point of the Moreau envelope \(F_{1/}\) in \(T=(\{^{1/2}},^{1/4}}{B_{1}^ {1/4}n_{2}^{1/4}},^{1/2}}{B_{1}^{1/2}n_{2}^{1/2}}\}n_{2}}{B_{1}B_{2}}^{-6})\) iterations._

**Remark.** In the worst case, the complexity has a worse dependency on \(n_{1}/B_{1}\), i.e., \((n_{1}^{3/2}/B_{1}^{3/2})\). This is caused by the two layers of block-sampling update for \(\{u_{i,t},i_{1}\}\) and \(\{v_{i,j,t}:(i,j)_{1}_{2}\}\). When \(n_{1}=B_{1}=1\) and \(B_{3}}\), the complexity of SONT becomes similar as SONX, which is understandable as the inner two levels in TCCO is the same as FCCO.

## 5 Applications

NSWC FCCO finds important applications in group distributionally robust optimization (group DRO) and two-way partial AUC (TPAUC) maximization.

Consider \(N\) groups with different distributions. Each group \(k\) has an averaged loss \(L_{k}(w)=}_{i=1}^{n_{k}}(f_{w}(x_{i}^{k}),y_{i}^{k})\), where \(w\) is the the model parameter and \((x_{i}^{k},y_{i}^{k})\) is a data point. It has been shown in previous study  that the group DRO problem can be formulated into

\[_{w}_{s}F(w,s)=_{k=1}^{N}[L_{k}(w)-s]_{+}+s.\]

This formulation can be mapped into non-smooth weakly-convex FCCO under certain assumptions. Due to space limitation, we defer the comprehensive discussion of group DRO to Appendix E. The rest of this section focuses on TPAUC maximization.

Let \(X\) denote an input example and \(h_{}(X)\) denote a prediction of a parameterized deep net on data \(X\). Denote by \(_{+}\) the set of \(n_{+}\) positive examples and by \(_{-}\) the set of \(n_{-}\) negative examples. TPAUC measures the area under ROC curve where the true positive rate (TPR) is higher than \(\) and the false positive rate (FPR) is lower than an upper bound \(\). A surrogate loss for optimizing TPAUCwith FPR\(\), TPR\(\) is given by :

\[_{}}}_{X_{i}_{+}^{}[1,k_{1}]}_{X_{j}_{-}^{ }[1,k_{2}]}(h_{}(X_{j})-h_{}(X_{i})),\] (8)

where \(()\) is a convex, monotonically non-decreasing surrogate loss of the indicator function \((h_{}(X_{j}) h_{}(X_{i}))\), \(_{+}^{}[1,k_{1}]\) is the set of positive examples with \(k_{1}= n_{+}\) smallest scores, and \(_{-}^{}[1,k_{2}]\) is the set of negative examples with \(k_{2}= n_{-}\) largest scores. To tackle the challenge of selecting examples from \(_{+}^{}[1,k_{1}]\) and \(_{-}^{}[1,k_{2}]\), the above problem is cast into the following :

\[_{,^{},}}_ {X_{i}_{+}}f_{i}(_{i}(,s_{i}),s^{}),\] (9)

where \(f_{i}(g,s^{})=s^{}+)_{+}}{}\), \(_{i}(,s_{i})=}_{X_{j} _{-}}s_{i}+}(X_{j})-h_{}(X_{i}))-s_{i})_{+} }{}\),

where \(=(s_{1},,s_{n_{+}})\). We will consider two scenarios, namely regular learning scenario where \(X_{i}^{d_{0}}\) is an instance, and multi-instance learning (MIL) scenario where \(X_{i}=\{_{1}^{1},,_{n}^{m_{i}}^{d_{0}}\}\) contains multiple instances (e.g., one patient has hundreds of high-resolution CT images). A challenge in MIL is that the number of instances \(m_{i}\) for each data might be large such that it is difficult to load all instances into the memory for mini-batch training. It becomes more nuanced especially because MIL involves a pooling operation that aggregates the predicted information of individual instances into a single prediction, which can be usually written as a compositional function with the inner function being an average over instances from \(X\). For simplicity of exposition, below we consider the mean pooling \(h_{}(X)=_{ X}e(_{e };)^{}_{c}\), where \(e(_{e},)\) is the encoded feature representation of instance \(\) with a parameter \(_{c}\), and \(_{c}\) is the parameter of the classifier. We will map the regular learning problem as NSWC FCCO and the MIL problem as NSWC TCCO.

The problem (9) is slightly more complicated than (1) or (2) due to the presence of \(s^{},\). In order to understand the applicability of our analysis and results to (9), we ignore \(s^{},\) for a moment. In the regular learning setting when \(h_{}(X)=e(_{e},X)^{}_{c}\) can be directly computed, we can map the problem into NSWC FCCO, where \(f_{i}(g,s^{})\) is non-smooth, convex, and non-decreasing in terms of \(g\), and \(g_{i}(,s_{i})=_{i}(,s_{i})\) is non-smooth, and is proved to be weakly when \(()\) is convex and \(h_{}(X)\) is smooth in terms of \(\). In the MIL setting with mean pooling, we can map the problem into NSWC TCCO by defining \(h_{i}()=|}_{ X_{i}}e( _{e};)^{}_{c}\), \(h_{ij}()=h_{j}()-h_{i}()\) and \(g_{i}(h_{i,j}(),s_{i})=s_{i}+())-s_{i})_ {+}}{}\), and \(f_{i}(g_{i},s^{})=s^{}+-s^{})_{+}}{}\), where \(f_{i}\) is non-smooth, convex, and non-decreasing in terms of \(g_{i}\), and \(g_{i}(h_{ij}(),s_{i})\) is non-smooth, convex, monotonic in terms of \(h_{ij}()\) when \(()\) is convex and monotonically non-decreasing, and \(g_{i}(h_{ij}(),s_{i})\) is weakly convex in terms of \(\) when \(h_{ij}()\) is smooth and Lipchitz continuous in terms of \(\). Hence, the problem (9) satisfies the conditions in Assumption 4.1 for the regular learning setting and that in Assumption 4.3 for the MIL with mean pooling under mild regularity conditions of the neural network. We present full details in Appendix C.1 for interested readers.

To compute the gradient estimator w.r.t \(\), \(u_{i,t}\) will be maintained for tracking \(g_{i}(,s_{i})\) in the regular setting or \(}_{X_{j}_{-}}g_{i}(h_{i,j}(),s_{i})\) in the MIL setting, \(v_{i,t}\) will be maintained for tracking \(h_{i}()\) in the MIL setting, which are updated similar to that in SONX and SONT. One difference from SONT is that \(v_{i,j,t}\) is decoupled into \(v_{i,t}\) and \(v_{j,t}\) due to that \(h_{i,j}\) can be decoupled. In terms of the extra variable \(s^{},\), the objective function is convex w.r.t both \(s^{}\) and \(\), which allows us to simply update \(s^{}\) by SGD using the stochastic gradient estimator \(}_{i_{1}^{d}}_{s^{}}f_{i} (u_{i,t},s^{}_{t})\) and we update \(s_{i}\) by SGD using the stochastic gradient estimator \([}_{j_{1}^{d}}_{s_{i}}g_{i }(v_{j,t}-v_{i,t},s_{i,t})]_{u}f_{i}(u_{i,t},s^{}_{t})\). Detailed updates are presented in Algorithm 5 and Algorithm 6 in Appendix C.2. We can extend the convergence analysis of SONX and SONT to the two learning settings of TPAUC maximization, which is included in Appendix C.4. Finally, it is worth mentioning that we can also extend the results to other pooling operations, including smoothed max pooling and attention-based pooling . Due to limit of space, we include discussions in Appendix C.3 as well.

## 6 Experimental Results

We justify the effectiveness of the proposed SONX and SONT algorithms for TPAUC Maximization in the regular learning setting and MIL setting .

**Baselines.** For _regular TPAUC maximization_, we compare SONN with the following competitive methods: 1) Cross Entropy (CE) loss minimization; 2) AUC maximization with squared hinge loss (AUC-SH); 3) AUC maximization with min-max margin loss (AUC-M) ; 4) Mini-Batch based heuristic loss (MB) ; 5) Adhoc-Weighting based method with polynomial function (AW-poly) ; 5) a single-loop algorithm (SOTAs) for optimizing a smooth surrogate for TPAUC . For _MIL TPAUC maximization_, we consider the following baselines: 1) AUC-M with attention-based pooling (AUC-M [att]); 2) SOTAs with attention-based pooling, which is a natural combination between advanced TPAUC optimization and MIL pooling technique; 3) the recently proposed provable multi-instance deep AUC maximization methods with stochastic smoothed-max pooling and attention-based pooling (MIDAM [smx] and MIDAM [att]) . The first two baselines use naive mini-batch pooling for computing the loss function in AUC-M and SOTAs. We implement SONT for MIL TPAUC maximization with attention-based pooling, which is referred to as SONT (att).

**Datasets.** For regular TPAUC maximization, we use three molecule datasets as in , namely moltox21 (the No.0 target), molmuv (the No.1 target) and molpcba (the No.0 target) . For MIL TPAUC maximization, we use four MIL datasets, including two tabular datasets MUSK2 and Fox, and two medical image datasets Colon and Lung. MUSK2 and Fox are two tabular datasets that have been widely adopted for MIL benchmark study . Colon and Lung are two histopathology (medical image) datasets that have large image size (512\(\)512) but local interests for classification . For Colon dataset, the adenocarcinoma is regarded as positive label and benign is negative; for Lung dataset, we treat adenocarcinoma as positive and squamous cell carcinoma as negative 3. For both of the histopathology datasets, we uniformly randomly sample 100 positive and 1000 negative data for experiments. For all MIL datasets, we uniformly randomly split 10% as the testing and the remaining as the training and validation. The statistics for all used datasets are summarized in Table 3and Table 4 in Appendix F.

**Experiment Settings.** For regular TPAUC maximization, we use the same setting as in . The adopted backbone Graph Nueral Network (GNN) model is Graph Isomorphism Network (GIN), which has 5 mean-pooling layers with 64 number of hidden units and dropout rate 0.5 . We utilize the sigmoid function for the final output layer to generate the prediction score, and set the surrogate loss \(()\) as squared hinge loss with a margin parameter. We follow the setups for model training and tuning exactly the same as the prior work . Essentially, the model is trained by 60 epochs and the learning rate is decreased by 10-fold after every 20 epochs. The model is initialized as a pretrained model from CE loss on the training datasets. We fix the learning rate of SONX as 1e-2 and moving average parameter \(\) as 0.9; tune the parameter \(\) in {0, 1e-1,1e-2,1e-3}, the parameter \(,\) in {0.1,0.3,0.5} and fix the margin parameter of the surrogate loss \(\) as 1.0, which cost the same

    &  &  &  \\  Method & (0.6, 0.4) & (0.5, 0.5) & (0.6, 0.4) & (0.5, 0.5) & (0.6, 0.4) & (0.5, 0.5) \\  CE & 0.067 (0.001) & 0.208 (0.001) & 0.161 (0.034) & 0.469 (0.018) & 0.095 (0.001) & 0.264 (0.001) \\ AUC-SH & 0.064 (0.008) & 0.217 (0.014) & 0.260 (0.130) & 0.444 (0.128) & 0.140 (0.003) & 0.312 (0.003) \\ AUC-M & 0.066 (0.009) & 0.209 (0.01) & 0.114 (0.079) & 0.433 (0.053) & 0.142 (0.009) & 0.313 (0.003) \\ MB & 0.067 (0.015) & 0.215 (0.023) & 0.173 (0.153) & 0.426 (0.118) & 0.095 (0.002) & 0.262 (0.003) \\ AW-poly & 0.064 (0.01) & 0.206 (0.025) & 0.172 (0.144) & 0.393 (0.123) & 0.110 (0.001) & 0.281 (0.002) \\ SOTA-s & 0.068 (0.018) & 0.23 (0.021) & 0.327 (0.164) & 0.526 (0.122) & 0.143 (0.001) & 0.314 (0.002) \\ SONX & **0.07 (0.035)** & **0.252 (0.025)** & **0.347 (0.175)** & **0.575 (0.122)** & **0.158 (0.006)** & **0.335 (0.006)** \\    &  &  \\  Method & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) \\  AUC-M (att) & 0.675 (0.1) & 0.783 (0.067) & **0.86 (0.036)** & 0.032 (0.03) & 0.253 (0.098) & 0.444 (0.118) \\ MIDAM (smx) & 0.525 (0.2) & 0.667 (0.149) & 0.8 (0.097) & 0.048 (0.059) & 0.265 (0.119) & 0.449 (0.113) \\ MIDAM (att) & 0.6 (0.215) & 0.717 (0.135) & 0.819 (0.092) & 0.016 (0.032) & 0.249 (0.125) & 0.509 (0.065) \\ SOTAs (att) & 0.6 (0.267) & 0.683 (0.178) & 0.819 (0.097) & 0.024 (0.032) & 0.278 (0.059) & 0.477 (0.046) \\ SONT (att) & **0.7 (0.1)** & **0.8 (0.067)** & **0.867 (0.036)** & **0.12 (0.131)** & **0.343 (0.176)** & **0.578 (0.119)** \\    &  &  \\  Method & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) \\  AUC-M (att) & 0.576 (0.10) & 0.739 (0.061) & 0.803 (0.038) & 0.32 (0.181) & 0.609 (0.113) & 0.744 (0.082) \\ MIDAM (smx) & 0.646 (0.083) & 0.787 (0.04) & 0.863 (0.026) & 0.43 (0.195) & 0.618 (0.128) & 0.824 (0.055) \\ MIDAM (att) & 0.548 (0.253) & 0.738 (0.149) & 0.826 (0.102) & 0.544 (0.261) & 0.716 (0.189) & 0.815 (0.129) \\ SOTAs (att) & 0.772 (0.124) & 0.862 (0.073) & 0.911 (0.045) & 0.539 (0.153) & 0.745 (0.077) & 0.841 (0.049) \\ SONT (att) & **0.8 (0.166)** & **0.875 (0.099)** & **0.916 (0.065)** & **0.639 (0.137)** & **0.779 (0.041)** & **0.865 (0.28)** \\   

Table 2: Testing TPAUC on molecule datasets (top) and on MIL datasets (bottom). The two numbers in parentheses of the second line refers to the lower bound of TPR and the upper bound of FPR for evaluating TPAUC. The two numbers of each method refers to the mean TPAUC and its std.

tuning effort as the other baselines. The weight decay is set as the same value (2e-4) with the other baselines. For baselines, we directly use the results reported in  since we use the same setting.

For MIL TPAUC maximization, we train a simple Feed Forward Neural Network (FFNN) with one hidden layer (the number of neurons equals to data dimension) for the two tabular datasets and ResNet20 for the two medical image datasets. Sigmoid transformation is adopted for the output layer to generate prediction score. The training epoch number is fixed as 100 epochs for all methods; the bag batch size is fixed as 16 (resp. 8) and the number of sampled instances per bag is fixed as 4 (resp. 128) for tabular (resp. medical image) datasets; the learning rate is tuned in {1e-2, 1e-3, 1e-4} and decreased by 10 folds at the end of 50-th and 75-th epoch for all baselines. For SONT (att), we set moving average parameter \(_{1}=_{2}\) as 0.9; tune the parameter \(_{1}=_{2}=\) in {0, 1e-1,1e-2,1e-3} and fix the margin parameter of the surrogate loss \(\) as 0.5, and the parameter \(,\) in {0.1,0.5,0.9}. Similar parameters in baselines are set the same or tuned similarly. For all experiments, we utilize 5-fold-cross-validation to evaluate the testing performance based on the best validation performance with possible early stopping choice.

**Results.** The testing results for the regular and MIL TPUAC maximization with different TPAUC measures are summarized in the Table 2. From Table 2, we observe that our method SONX achieves the best performance for regular TPAUC maximization. It is better than the state-of-the-art method SOTAs for TPAUC maximization. We attribute the better performance of SONX to the fact that the objective of SONX is an exact estimator of TPAUC while the smoothed objective of SOTAs is an inexact estimator of TPAUC. We also observe that SONT (att) achieves the best performance in all cases, which is not surprising since it is the only one that directly optimizes the TPAUC surrogate. In contrast, other baselines either optimizes a different objective (MIDAM) or does not ensure convergence due to the use of mini-batch pooling (AUC-M, SOTAs).

**Ablation Study.** We conduct ablation studies to demonstrate the effect of the error correction term on the training convergence by varying the \(\) value for SONX and SONT, where \(_{1}=_{2}=\) is set as the same value in SONT. The training convergence results are presented in Figure 1. We can see that an appropriate value of \(>0\) can yield a faster convergence than \(=0\), which verifies the faster convergence of using MSVR estimators than using moving average estimators. However, we do observe a gap between theory and practice, as setting a large value of \(>1\) as in the theory might not yield convergence. This phenomenon is also observed in . We conjecture that the gap could be fixed by considering convex objectives , which is left as future work.

## 7 Conclusions

In this paper, we have considered non-smooth weakly-convex two-level and tri-level finite-sum coupled compositional optimization problems. We presented novel convergence analysis of two stochastic algorithms and established their complexity. Applications in deep learning for two-way partial AUC maximization was considered and great performance of proposed algorithms were demonstrated through experiments on multiple datasets. A future work is to prove the convergence of both algorithms for convex objectives.