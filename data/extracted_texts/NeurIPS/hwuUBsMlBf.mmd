# CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts

Jiachen Li\({}^{*}\) Xinyao Wang\({}^{2}\)\({}^{}\) Sijie Zhu\({}^{2}\) Chia-Wen Kuo\({}^{2}\) Lu Xu\({}^{2}\)

**Fan Chen\({}^{2}\) Jitesh Jain\({}^{1}\) Humphrey Shi\({}^{1}\) Longyin Wen\({}^{2}\)\({}^{1}\)SHI Labs \(@\) Georgia Tech & UIUC \({}^{2}\)ByteDance Inc., San Jose https://github.com/SHI-Labs/CuMo**

 Work done during an internship at ByteDance San Jose, CA.Work done at ByteDance.Work done at ByteDance.Corresponding authors.

###### Abstract

Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks. However, these scaling approaches are computationally expensive and overlook the significance of efficiently improving model capabilities from the vision side. Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose **CuMo**, which incorporates **Co**-**u**pcycled Top-K sparsely-gated **M**ixture-**f**-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with neglectable additional activated parameters during inference. CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage, with auxiliary losses to ensure a balanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks within each model size group, all while training exclusively on open-sourced datasets.

## 1 Introduction

The advent of GPT-4V  has sparked excitement within open-source communities to transform large language models (LLM) into multimodal LLMs. Recent multimodal LLMs [11; 46; 2] typically integrate pre-trained vision encoders with MLP connectors to LLMs, with visual instruction tuning data to fine-tune the pre-trained LLMs, enhancing their visual understanding capabilities. To further scale up multimodal LLMs, previous efforts [44; 45; 39; 51; 7; 42] primarily focus on training the model with a more extensive collection of text-image paired data and employing stronger LLMs, significantly increasing training efforts. On the vision side, recent work concentrates on leveraging multiple vision encoders [43; 18] to enrich visual content, employing larger vision encoders , and using advanced vision-language connectors  to improve performance on multimodal tasks. However, these techniques result in an increased number of additional parameters and generate extra visual tokens for LLMs to process, making it inefficient to scale.

In terms of efficiently scaling up models, Mixture-of-Experts (MoE) has become the de-facto framework in modern large-scale neural networks, particularly in natural language processing (NLP). Most large language models (LLM) are built upon the transformer  architecture, wherein sparseMoE is used to replace the dense MLP block with the Top-K sparsely-gated MoE block . Recent state-of-the-art open-sourced  and private  LLMs have predominantly adopted the sparse MoE architecture. These models are scaled up using the MoE design during training while maintaining relatively lower inference costs as only selected MLP experts are activated during the feed-forward process. Nevertheless, the development and optimization of MoE-based models have been largely tailored to LLMs, and the exploration of scaling multimodal LLMs with MoE, especially on the vision side, remains largely unexplored.

Motivated by these observations, we introduce CuMo, which integrates Top-K sparsely-gated MoE blocks into the vision encoder and the MLP connector of multimodal LLMs. We also explore the associated training recipe and methodology for CuMo. Firstly, we pre-train the MLP connector and perform pre-finetuning to warm up the whole model without introducing the MoE architecture, which stabilizes the following visual instruction tuning stage with newly incorporated sparse MoE blocks. Then, we replace each MLP block with the sparse MoE block in the MLP connector and the vision encoder through co-upcycling. Each expert within the sparse MoE block is initialized from the corresponding MLP block after the pre-training and the pre-finetuning stages, as shown in Figure 1. Additionally, each MoE block contains a Top-K router trained from scratch to select experts during the visual instruction tuning stage with auxiliary losses on the router to maintain a balanced loading of experts. We conduct further comparisons between co-upcycled LLMs and pre-trained MoE-based LLMs. The results show that the pre-trained MoE-based LLMs significantly outperform the co-upcycled LLMs. As a result, the co-upcycling of LLMs is not included in CuMo. Our models are trained fully on open-sourced datasets that are converted to visual instruction following formats. Experimental results demonstrate that CuMo outperforms other state-of-the-art multimodal LLMs on various VQA and multimodal instruction-following benchmarks within the same model size group, as illustrated in Figure 1.

Our contributions can be summarized as follows:

* We introduce CuMo, which integrates co-upcycled sparsely-gated MoE layers into both the MLP connector and the vision encoder, enhancing the multimodal LLM with slightly additional activated parameters from the vision side.
* We outline the training methodology for CuMo, including a three-stage training process with auxiliary losses to stabilize training and ensure a balanced loading of experts.
* We train CuMo exclusively on open-sourced datasets and pre-trained models. It outperforms state-of-the-art open-sourced and private multimodal LLMs across multiple competitive benchmarks within each model size group.

Figure 1: Left: Each MLP expert within the MoE block during the visual instruction tuning stage is initialized from the corresponding pre-trained MLP in CuMo. Right: CuMo outperforms strong open-sourced models such as Mini-Gemini and LLaVA-NeXT, as well as the private MM1 model.

Related Works

### Multimodal LLM

While the ultimate goal for multimodal models may be generative across various modalities [66; 3; 60], modern multimodal LLMs primarily focus on integrating additional modalities, such as vision, into LLMs. InstructBLIP  adopts Q-Former  to sample from visual tokens for LLM to feed-forward and follow the instructions. Flamingo  and IDEFICS [23; 32] use shared decoder for visual-language understanding. Qwen-VL  uses three-stage training to convert QwenLM to QwenVL. LLaVA series [46; 44; 45] adopt visual instruction tuning that uses instruction-following data to convert LLM into multimodal LLM. ShareGPT4V  collects detailed image caption data from GPT4V to augment the LLaVA models. HoneyBee  investigates different designs of the MLP connector for better alignment. VILA  unfreezes the LLM during pre-training with interleaved image-text data. MoE-LLaVA  adopts the MoE design in small LLMs and reaches comparable performance to LLaVA with large LLMs. VCOder  adopts various vision adapters to enhance visual perception abilities. SPHINX [43; 18] adopts multiple visual encoders to enrich the visual features with scaled data and models. Internal-Xcomposer [69; 12] is trained with interleaved text-image composition data and achieves state-of-the-art performance. InternVL  scales up the vision encoder to a 6B ViT model. MM1  summarizes the essential steps towards building a strong multimodal LLM from a pre-trained LLM. Mini-Gemini  further collects guided generation into the pipeline.

### Mixture-of-Experts

Mixture-of-Experts  is proposed to utilize a set of expert networks to address specific tasks by employing a gating network to determine the selection of these experts. Recently, it has gained popularity in the design of large language models . The mainstream practice  is to replace the dense MLP layers with Top-K sparsely-gated mixture-of-experts (MoE) layers in the transformer . **MoE in Language** Subsequent works [33; 16] have further scaled up MoE-based large language models with improved stability and load balancing of experts. The design of gating networks often involves selecting the top-k experts for each token [57; 33]. Various routing strategies have been explored, such as choosing top-k tokens by experts , one-to-one matching between experts and tokens . Besides routing strategies, maintaining the load balance of experts is crucial for training MoE models. ST-MoE  adopts loading balancing loss and router-z loss to ensure a balanced distribution of the experts. Upcycling  proposes training sparse experts from dense checkpoints to stabilize training and lower the cost. Recent large language models like Gemini-Pro  and DBRX  are also based on the MoE design. **MoE in Vision** The success of MoE extends to the vision community, particularly following the popularity of vision transformers [13; 4; 72; 21; 20; 25; 36]. V-MoE  reaches comparable performance to dense ViT while only requiring half of the compute. LIMoE  replaces dense MLP layers with MoE layers in CLIP and observes improvements in zero-shot image classification. Residual MoE  corporates residual design into MoE transformer and saves over 30% training cost. AdaMV-MoE  proposes an adaptive MoE framework for multi-task learning.

## 3 Method

In this section, we first review the sparse MoE block structure and the upcycling strategy utilized in previous studies. Subsequently, we describe how these sparsely-gated MoE blocks are integrated into each module of multimodal LLMs using co-upcycling strategies. Then, we introduce the three-stage training process and auxiliary loss functions employed to stabilize training and balance the loads of experts.

### Revisit Sparse MoE

**Sparse MoE Structure** Previous mainstream practice  is to replace the dense MLP blocks with sparsely-gated mixture-of-experts blocks. Given input \(^{N C_{in}}\) and a MLP block,

\[X_{out}=(X)^{N C_{out}}\] (1)To scale up the model with multiple MLP blocks in parallel, a sparse MoE block includes a router network to select Top-K experts out of \(S\) total experts. This router network has a linear layer to compute the normalized weight matrix based on the inputs \(\) for voting, resulting in

\[W=((X))^{N S}\] (2)

The Top-K experts are selected for each token based on \(\), and the re-normalized weights \(}^{N K}\) are computed using

\[W_{K}=((W))^{N K}\] (3)

Each selected expert is represented by an MLP block, and the final output is obtained through a re-weighted sum

\[X_{out}=_{i}^{K}W_{K}^{i}_{i}(X)^{N C_{ out}}\] (4)

the output \(_{out}\) maintains the same dimension as the output of a single dense MLP block.

**Sparse Upcycling** Training MoE-based designs from scratch can be unstable and costly. Sparse Upcycling  addresses this challenge by initializing the experts in each MoE block from the corresponding MLP block in pre-trained dense checkpoints. This initialization approach provides a better starting point for training MoE-based models and reduces training costs compared to training from scratch.

### CuMo Architecture

**Sparse MoE in MLP Connector** The MLP connector converts visual tokens into word embedding space, aligning dimensions between visual and text tokens. An effective architecture for the vision-language connector is an MLP block  that contains two linear layers. We start from a single MLP block and replace it with a Top-K sparse MoE block, incorporating a Top-K router and a set of experts for projecting visual tokens into word embedding space.

**Sparse MoE in Vision Encoder** Vision encoders extract image features as sequences of visual tokens for reasoning in LLMs. CLIP  is one the most popular pre-trained vision encoders for multimodal LLM since it is pre-trained on large-scale image-text pairs, which makes it suitable for processing images for multimodal usage. The visual encoding part of CLIP is a ViT  model, which has consecutive MLP blocks in the transformer encoder. We substitute each MLP block with a Top-K sparse MoE block, retaining skip connections alongside MoE block outputs.

Figure 2: **Architecture of CuMo.** CuMo incorporates sparse Top-K MoE blocks into the CLIP vision encoder and vision-language MLP connector, thereby improving the multimodal LLM capabilities from the vision side. Skip connections are omitted for simplicity. Further implementation details are provided in Section 3.2.

**Sparse MoE in LLM** In terms of using MoE in LLM, we compare the co-upcycled LLM with pre-trained MoE-based LLM. We start from Mistral-7B and the upcycled Mistral-7B-MoE slightly outperforms Mistral-7B on certain benchmarks. However, considering the constrained knowledge base of upcycled experts from Mistral-7B, we compare it with the pre-trained Mistral 8x7B with pre-trained experts of a diverse knowledge base. Experimental results reveal that pre-trained Mistral 8x7B significantly outperforms Mistral-7B-MoE. As a result, LLM is not co-upcycled with CLIP and MLP connectors since it brings marginal improvements with great additional parameters.

### Training Recipe

**Co-Upcycling MoE blocks** We start with training the added MoE blocks from scratch while the model is struggling to converge. Attempts to address this issue with lower learning rates perform worse compared to the baseline. As a result, we adopt a co-upcycling approach, initializing each module that integrates sparsely-gated MoE blocks with pre-trained MLPs to replace corresponding MLP blocks, as shown in Figure 1. This strategy consistently improves training stability and model performance.

**Three-Stage Training** To further enhance training stability, we adopt a three-stage training strategy for CuMo models, as illustrated in Figure 3. In the first stage, we only pre-train the MLP connector, given that the vision encoder and LLM have already undergone pre-training on large-scale data. During the second pre-finetuning stage, we train all parameters using high-quality caption data to warm up the entire model before introducing MoE blocks in the subsequent stage. The third stage involves visual instruction finetuning, where the multimodal LLM is scaled up with upcycled MoE blocks and trained on visual instruction tuning data.

**Loss Function** To maintain a load balance between experts in each MoE block, we adopt auxiliary losses based on the language modeling cross-entropy loss. The auxiliary losses comprise loading balance loss and router z-loss . Hence, the total loss is

\[L=L_{ce}+_{b}L_{b}+_{z}L_{z}\] (5)

Here, \(L_{ce}\) represents the language modeling loss, which computes the cross-entropy of next-token predictions. \(_{b}\) and \(_{z}\) denote coefficients for loading balance loss \(L_{b}\) and router z-loss \(L_{z}\), set to 0.1 and 0.01, respectively, across all experiments. These auxiliary losses, abbreviated as bzloss in Section 4, are individually applied to the MLP connector, vision encoder, and LLM for simplicity.

## 4 Experiments

We train the CuMo models on a mixture of open-sourced datasets, which are converted into the visual instruction tuning format. Then, we conduct comprehensive evaluations of the performance of CuMo models across various competitive VQA-based and instruction-following-based benchmarks. Additionally, we perform ablation studies on each module with upcycled MoE blocks with qualitative analysis of the results.

Figure 3: **Training Stages of CuMo. The first stage involves pre-training the MLP for better alignment. Subsequently, the pre-finetuning stage trains all parameters as a warm-up before the next stage. Finally, the MLP experts within each MoE block are initialized from the weights of the corresponding MLP block, followed by training all parameters in the visual instruction tuning stage.**

### Implementation Details

**Training Datasets** During pre-training, we only utilize LLaVA-558K  to train the MLP connector for better alignment. In the subsequent pre-finetuning stage, detailed image caption data from ALLAVA  is employed to warm up all parameters of the multimodal LLM. For the final visual instruction tuning stage, a mixture of datasets including LLaVA-665K , ShareGPT4V , LAION-GPT-V , DocVOQA , ChartQA , AI2D , InfoVQA , SynDog-EN , ALLAVA , and LIMA  is utilized to train the CuMo models with upcycleed MoE blocks. The total data size for visual instruction tuning is approximately 1.65 million, and all training data are publicly accessible. The detailed breakdown of the training dataset is listed in Appendix A.

**Evaluation Benchmarks** Evaluation of CuMo models primarily focuses on academic VQA-based datasets such as VQAv2 , GQA , Science-QA , and TextVQA , as well as instruction-following-based LMM benchmarks including POPE , MME , MMBench , SEEDBench , LLaVA-Wild , and MM-Vet . Additionally, the challenging MMMU  is evaluated to assess the visual reasoning abilities of the multimodal LLMs.

**Training Settings** We employ the pre-trained CLIP ViT-L  as the vision encoder, a two-layer MLP as the vision-language connector, and Mistral-7B  as the LLM to establish the baseline model following LLaVA v1.5 . We only use LLaVA-558K  as pre-training data and LLaVA-665K  as visual instruction tuning data to train the baseline model and make ablation studies for comparisons. The learning rate is set to 1e-3 for pre-training the MLP connector and reduced to 2e-5 for visual instruction tuning of both the MLP connector and CLIP. To further stabilize the visual instruction tuning process after scaling up with additional data, the learning rate is lowered to 2e-6 for all parameters of the CuMo models in the final results. More hyperparameters of the training process is listed in Appendix B.

**Evaluation Settings** During evaluation, we adhere to the settings outlined in the LLaVA series , employing a greedy decoding strategy for all benchmarks. The data and questions are converted into visual instructions to prompt the multimodal LLMs. For benchmarks that utilize GPT API for evaluation, we adopt gpt-4-0613 for LLaVA-Wild .

   & Act. &  & Text &  & MM &  & LLaVA & SEED & MMMU \\  & LLM & (8) & IMG & VOA & GOA & POPE & MME & EN & CN & Wet & v2 & Wild & IMG & val \\   \\  InstraBLIP  & Vicuma-7B & 7.9 & 60.5 & 50.1 & 49.2 & - & - & 36.0 & 23.7 & 26.2 & - & 60.9 & 60.5 & - \\ Open-V-Cht  & Open-7B & - & 68.2 & 61.5 & 57.5 & - & 1487.5 & 60.6 & 56.7 & - & 78.2 & - & 58.2 & 35.9 \\ LLaVA-v1.5  & Vicuma-7B & 7.1 & 66.8 & 82.2 & 60.8 & 85.9 & 1510.7 & 64.3 & 58.3 & 70.5 & 78.3 & 43.6 & 66.1 & - \\ VLA  & Vicuma-7B & 7.1 & 68.2 & 64.4 & 62.3 & 85.5 & 1533.0 & 68.9 & 61.7 & 39.4 & 79.9 & 69.7 & 61.1 & - \\ SharoGPT4V  & Vicuma-7B & 7.1 & 68.4 & - & - & - & - & - & - & - & - & - & - & - \\ LLLA-Mv1  & Vicuma-7B & - & 68.3 & - & 64.3 & 86.0 & 1524.1 & 65.1 & - & - & 79.3 & - & 59.9 & - \\ EMBNN-Intema  & Internal-M2B & - & 70.4 & 58.1 & 56.2 & 86.9 & 1206.4 & 57.9 & - & 36.5 & 75.5 & 57.6 & 68.8 & - \\ LLaVA-NEXT  & Mustral-7B & 7.6 & 72.8 & 65.7 & 64.8 & 86.7 & 1498 & 68.7 & 61.2 & 47.3 & 82.2 & 83.2 & 72.2 & 35.3 \\ LLaVA-NEXT  & Vicuma-7B & 7.1 & 70.1 & 64.9 & 64.2 & 86.5 & 1519.7 & 67.4 & 60.6 & 49.3 & 81.8 & 81.6 & 70.2 & 35.8 \\ Mini-Gemini  & Vicuma-7B & 7.3 & 65.2 & - & - & - & 1523 & 69.3 & - & - & - & - & - & 36.1 \\ MMI  & MMT-7B & - & 72.6 & 72.8 & - & - & 86.6 & 1529.3 & 79.0 & - & - & - & - & - & - \\  InstraBLIP  & Vicuma-13B & 142 & 63.1 & 50.7 & 49.5 & 79.9 & 1212.2 & - & - & 25.6 & - & 58.2 & 63.1 & - \\ LLaVA-v1.5  & Vicuma-13B & 13.4 & 71.6 & 61.3 & 63.3 & 85.9 & 1531.3 & 67.7 & 63.6 & 35.4 & 80.0 & 70.7 & 68.2 & 36.4 \\ VLa  & Vicuma-13B & 13.4 & 73.7 & 66.6 & 63.3 & 84.2 & 1570.1 & 70.3 & 64.3 & 38.8 & 80.8 & 73.0 & 62.8 & - \\ LinearVL-Cht  & Vicuma-13B & 19 & - & 61.5 & **66.6** & 87.6 & 1586.4 & - & - & 81.2 & - & - & - \\ LLaMA-V1  & Vicuma-13B & - & 70.0 & 65.0 & 86.0 & 1542.3 & 66.6 & - & - & 80.0 & - & 62.3 & - \\ SPHNIX-Pla  & LLaMA-2B & - & 74.2 & 65.7 & - & - & - & 194.7 & 71.0 & - & - & - & 71.7 & 74.8 & - \\ Mini-Gemini  & Vicuma-13B & 13.6 & 65.9 & - & - & - & 1505 & 68.5 & - & 46.0 & - & - & 38.1 \\ LLaVA-NeXT  & Vicuma-13B & 13.4 & 73.6 & 67.1 & 65.4 & 86.2 & 1755 & 70 & 64.4 & - & 84.8 & **82.8** & **87.3** & 71.9 & 36.2 \\  CAM6 & Mixtral-7B & 7.8 & 73.9 & 67.0 & **64.9** & 86.7 & 1548.6 & 73.0 & 66.6 & **81.0** & 82.2 & 85.7\({}^{}\) & 72.1 & 39.1 \\   \\  SPHNX-MoE  & Mustral-8v2B & - & 74.5 & 68.0 & 63.8 & **89.6** & 1485.3 & 71.3 & - & 40.9 & 81.1 & 70.2 & 73.0 & 31.1 \\ MMI  & MMT-BiM & - & 75.3 & **72.8** & - & 87.6 & 1629.0 & **79.7** & - & 47.0 & **83.4** & 82.0 & 70.4 & 40.9 \\ Mini-Gemini  & Mustral-8v2B & 13.5 & - & 69.2 & - & - & 16.9 & 75.6 & 45.8 & - & - & - & 41.8 \\  CAM6 & Mixtral-8v2B & 13.5 & **77.9** & 66.0 & 63.8 & 85.7 & **1639.5** & 75.3 & **68.0** & 48.7 & 81.8 & 84.7 & **73.2** & **45.0** \\  

Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and **bold** indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \({}^{}\) are averaged by three inference runs of querying GPT API.

### Main Results

**Comparison with SoTA Multimodal LLMs** In Table 1, we present a comparison of CuMo models with other state-of-the-art instruction-following-based multimodal LLMs. We categorize the models based on the size of the base LLMs, including 7B models, 13B models, and 7B MoE models. CuMo Mistral-7B outperforms other 7B-based state-of-the-art multimodal LLMs across multiple benchmarks. Moreover, the performance of the CuMo Mistral-7B model is comparable to many 13B-based multimodal LLMs. In the case of Mistral-8\(\)7B models, CuMo achieves results on par with SPHINX-MoE, MM1, and Mini-Gemini. LLaMA-based LLMs [10; 63] are not utilized in our experiments due to license constraints.

**Comparison under limited training data** To further evaluate the effectiveness of the co-upcycled MoE blocks, we train the vanilla CuMo mistral-7B under limited training data in Table 2. It shows that CuMo outperforms other 7B models and reaches comparable performance to LLaVA-v1.5 Vicuna-13B under the same training data.

### Ablation Study

**Upcycle MLP connector to MLP-MoE** We initiate the ablation study by replacing the MLP connector with upcycled MLP-MoE, as depicted in Table 3(a). We start with a Top 2-in-4 router and train the MoE blocks from scratch, which leads to a clear performance drop on all benchmarks. Then, we adopt the upcycling strategy to initialize the MLP experts. We observe marginal improvements over the baseline, considering each expert comprises only two linear layers. Subsequently, the incorporation of bzloss to ensure a balanced loading of experts in the MLP-MoE yields noticeable enhancements on MMVet. However, employing a Top 2-in-8 router with upcycling and bzloss results in a slight performance decline, possibly due to the limited visual instruction tuning data to train robust and well-balanced eight experts.

**Empower CLIP with CLIP-MoE** In Table 3(b), initially unfreezing CLIP based on MLP-MoE leads to noticeable improvements on TextVQA and MMVet benchmarks. However, training with the added Top2-in-4 MoE blocks in CLIP from scratch proves unsuccessful, as the model fails to converge even with largely reduced learning rates. Consequently, adopting upcycled MoE blocks during the visual instruction tuning stage yields further enhancements on the TextVQA, MMVet, and SEED benchmarks, as well as a more stable training process.

**Upcycle LLM vs Pre-trained LLM-MoE** Upon replacing all MLP blocks with sparsely-gated MoE blocks in the visual part, we further investigate the utilization of the MoE architecture in the LLM. Starting from the Mistral-7B model, we first lower the learning rate to 2e-6 to set the baseline and the following experiments since a learning rate of 2e-5 induces training instabilities. Then, we upcycle each MLP block with a sparsely-gated MoE block, initializing the weight of each expert from the pre-trained MLP block. As demonstrated in Table 3(c), the upcycled Mistral-4\(\)7B and 8\(\)7B outperform the Mistral-7B model slightly except for TextVQA. However, considering that the upcycled experts significantly increase parameters without introducing new knowledge, we replace the upcycled Mistral 8\(\)7B with Mistral 8\(\)7B . In Mistral 8\(\)7B, all expert layers are pre-trained on large-scale language data, providing superior initialization and similar training stability compared to upcycling. The results indicate that CuMo Mistral-8x7B outperforms its upcycled

   &  &  &  &  &  &  &  &  &  &  \\   & & & & &  &  &  &  &  &  &  &  &  &  &  &  &  \\  InstructHLIP  & Vicuna-7B & 129M & 1.2M & 60.5 & 50.1 & 49.2 & - & - & 36.0 & 23.7 & 26.2 & - & 60.9 & 60.5 \\ InstructHLIP  & Vicuna-13B & 129M & 1.2M & 63.1 & 50.7 & 49.5 & 78.9 & 1212.8 & - & 25.6 & - & 58.2 & 63.1 \\ IDEFCS-9B  & LLLA-PA-5B & 135M & 1.3M & - & 25.9 & 38.4 & - & 48.2 & - & 52.2 & - & 50.9 & - & - \\ IDEFCS-80B  & LLLA-MA-5B & 353M & 1.1 & - & 30.9 & 45.2 & - & - & 54.5 & 38.1 & - & 60.0 & - & - \\ Open-Vm  & Open-TB & 1.4B & 50M & 67.1 & **63.8** & 59.3 & - & - & 38.2 & 7.4 & - & 78.8 & - & 56.3 \\ Open-VL-Clat  & Open-TB & 1.4B & 50M & 68.2 & 61.5 & 57.5 & - & 1487.5 & 60.6 & 56.7 & - & 78.2 & - & 58.2 \\ LLaVA-v1.5  & Vicuna-7B & 558K & 665K & 66.8 & 58.2 & 62.0 & 85.9 & **15167**.4 & 64.3 & 58.3 & 30.5 & 78.5 & 63.4 & 66.1 \\ LLaVA-v1.5  & Mistral-7B & 558K & 665K & **72.8** & 57.6 & 60.0 & 86.3 & 1414.9 & 66.5 & 60.1 & 32.1 & 78.2 & **69.4** & 66.4 \\  CuMo & Mistral-7B & 558K & 665K & 71.7 & 59.3 & **63.2** & **87.1** & 1428.6 & **69.6** & **62.6** & **34.3** & **80.6** & 68.8 & **69.6** \\  

Table 2: Comparisons between CuMo Mistral-7B and other multimodal LMM models with limited training data. The best performance are highlighted in **bold**. LLaVA-v1.5\({}^{}\) with Mistral-7B is reproduced by us as a baseline model.

counterparts significantly and is employed in the final models with bzloss to maintain a balanced loading of experts.

**Multi-Resolution Visual Features** Incorporating multi-resolution inputs is crucial for enhancing the understanding of image content in multimodal LLMs. Following the approach outlined in \(S^{2}\), we introduce multi-resolution inputs to CLIP and concatenate the feature maps channel-wise to maintain the total number of visual tokens consistent with low-resolution inputs. As illustrated in Table 3(d), an empirical combination of 3\(\) and 1\(\) reaches the best performance and we adopt this configuration for the final CuMo models.

**Pre-FineTuning Stage** Previous ablation studies were conducted directly after the pre-training of the MLP connector, leading to observed training instabilities during visual instruction tuning. To address this, we introduce a pre-finetuning stage using high-quality image caption data, wherein all parameters are unfrozen. In Table 3(e), we leverage caption data from ALLaVA for this stage. Results indicate that ALLaVA data proves to be a superior option, providing fewer but higher-quality captions for training, ultimately leading to improved performance.

**Added Parameters** In Table 3(f), we keep track of the added activated parameters during inference of CuMo. It shows that adding MoE blocks in the vision side upon MLP connector and CLIP only brings 0.22B extra parameters compared to the baseline model on Mistral-7B. More details can be found in Appendix C.

### More Analysis

**Expert Distribution** As shown in Figure 4, we visualize the expert distributions in the MoE block from selected layers in CLIP during inference. The dataset used for analyzation is the test set of the MME benchmark. The distribution indicates that the selected experts during infer

Figure 4: **Expert distributions of MoE blocks in CLIP. We select layers from CLIP and summarize the activated experts during inference.**

Table 3: **Ablation Studies during building CuMo. Each row represents a different configuration, with changes or additions marked using \(\) and \(+\) symbols, respectively. Settings highlighted with a light blue background are those adapted for final model in Table 1. For (b): all MoE blocks in CLIP are initialized with upcycling.**ence are evenly spread across layers, providing further evidence of the effectiveness of the auxiliary losses in maintaining load balance.

**Dialogue Comparisons** Presented in Figure 5, we contrast the responses from CuMo-Mistral-7B, LLaVA-Yi-34B, and MiniGemini-Yi-34B under challenging content understanding cases. It demonstrates that CuMo-Mistral-7B can effectively follow instructions and provide mostly correct answers to challenging questions derived from complex scenes. However, CuMo also exhibits instances of hallucinations, such as responding with "2 characters standing on the table", highlighting the need for further investigation to mitigate hallucinations and improve reliability of CuMo.

**Limitations** The main limitation of CuMo is that, similarly to other large language models, it can generate hallucinated responses. This may constrain its potentials in real-world multimodal applications like used as a chatbot. Future works, such as Reinforcement Learning with Human Feedback (RLHF) and Retrieval Augmented Generation (RAG), can be undertaken to mitigate these hallucinations and improve the model's reliability.

Figure 5: **Dialogues between the user and multimodal LLMs on challenging images.** We highlight the correct answers and hallucinations from the responses of the multimodal LLMs.

Conclusion

In this study, we introduce the sparse mixture-of-experts design into multimodal LLMs from the vision side. Specifically, we replace each MLP block with a Top-K sparse MoE block in the MLP connector and the vision encoder. To enhance training stability, we employ a three-stage training approach, incorporating upcycled MoE blocks during the visual instruction tuning stage, along with auxiliary bzloss to maintain a balanced loading of experts. All CuMo models are trained and evaluated on fully open-sourced datasets and benchmarks. Through extensive experiments and ablation studies, we validate the effectiveness of the upcycled MoE blocks in each module. CuMo outperforms state-of-the-art models across multiple competitive benchmarks within the same group of model sizes.

AcknowledgmentsWe extend our gratitude to Chunyuan Li, Lei Chen, and Haibin Lin for their insightful and valuable discussions throughout this project. Li, Jain, Shi are in part supported by National Science Foundation CAREER Award #2427478, and by National Science Foundation and the Institute of Education Sciences, U.S. Department of Education under Award #2229873 - National AI Institute for Exceptional Education, Beckman Institute and ECE Department at UIUC, and Georgia Institute of Technology.