# Finding Counterfactually Optimal Action Sequences

in Continuous State Spaces

 Stratis Tsirtsis

Max Planck Institute for Software Systems

Kaiserslautern, Germany

stsirtsis@mpi-sws.org

&Manuel Gomez-Rodriguez

Max Planck Institute for Software Systems

Kaiserslautern, Germany

manuelgr@mpi-sws.org

###### Abstract

Whenever a clinician reflects on the efficacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient's health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to _retrospectively_ analyze sequential decision making processes, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A\({}^{*}\) algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks.

## 1 Introduction

Had the chess player moved the king one round later, would they have avoided losing the game? Had the physician administered antibiotics one day earlier, would the patient have recovered? The process of mentally simulating alternative worlds where events of the past play out differently than they did in reality is known as counterfactual reasoning . Thoughts of this type are a common by-product of human decisions and they are tightly connected to the way we attribute causality and responsibility to events and others' actions . The last decade has seen a rapid development of reinforcement learning agents, presenting (close to) human-level performance in a variety of sequential decision making tasks, such as gaming [3; 4], autonomous driving  and clinical decision support [6; 7]. In conjunction with the substantial progress made in the field of causal inference [8; 9], this has led to a growing interest in machine learning methods that employ elements of counterfactual reasoning to improve or to retrospectively analyze decisions in sequential settings [10; 11; 12; 13; 14; 15].

In the context of reinforcement learning, sequential decision making is typically modeled using Markov Decision Processes (MDPs) . Here, we consider MDPs with a finite horizon where each episode (_i.e._, each sequence of decisions) consists of a finite number of time steps. As an example, consider a clinician treating a patient in an intensive care unit (ICU). At each time step, the clinician observes the current state of the environment (_e.g._, the patient's vital signs) and they choose among a set of potential actions (_e.g._, standardized dosages of a drug). Consequently, the chosen action causes the environment to transition (stochastically) into a new state, and the clinician earns a reward (_e.g._,satisfaction inversely proportional to the patient's severity). The process repeats until the horizon is met and the goal of the clinician is to maximize the total reward.

In this work, our goal is to aid the retrospective analysis of individual episodes as the example above. For each episode, we aim to find an action sequence that differs slightly from the one taken in reality but, under the circumstances of that particular episode, would have led to a higher counterfactual reward. In our example above, assume that the patient's condition does not improve after a certain period of time. A counterfactually optimal action sequence could highlight to the clinician a small set of time steps in the treatment process where, had they administered different drug dosages, the patient's severity would have been lower. In turn, a manual inspection of those time steps could provide insights to the clinician about potential ways to improve their treatment policy.

To infer how a particular episode would have evolved under a different action sequence than the one taken in reality, one needs to represent the stochastic state transitions of the environment using a structural causal model (SCM) [8; 17]. This has been a key aspect of a line of work at the intersection of counterfactual reasoning and reinforcement learning, which has focused on methods to either design better policies using offline data [10; 12] or to retrospectively analyze individual episodes [11; 13]. Therein, the work most closely related to ours is by Tsirtsis et al. , which introduces a method to compute counterfactually optimal action sequences in MDPs with discrete states and actions using a Gumbel-Max SCM to model the environment dynamics . However, in many practical applications, such as in critical care, the state of the environment is inherently continuous in nature . In our work, we aim to fill this gap by designing a method to compute counterfactually optimal action sequences in MDPs with continuous states and discrete actions. Refer to Appendix A for a discussion of further related work and to Pearl  for an overview of the broad field of causality.

**Our contributions.** We start by formally characterizing sequential decision making processes with continuous states and discrete actions using finite horizon MDPs and a general class of _bijective_ SCMs . Notably, this class of SCMs includes multiple models introduced in the causal discovery literature [20; 21; 22; 23; 24; 25]. Building on this characterization, we make the following contributions:

1. We formalize the problem of finding a counterfactually optimal action sequence for a particular episode in environments with continuous states under the constraint that it differs from the observed action sequence in at most \(k\) actions.
2. We show that the above problem is NP-hard using a novel reduction from the classic partition problem . This is in contrast with the computational complexity of the problem in environments with discrete states, which allows for polynomial time algorithms .
3. We develop a search method based on the \(A^{*}\) algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem upon termination.

Finally, we evaluate the performance and the qualitative insights of our method by performing a series of experiments using real patient data from critical care.1

## 2 A causal model of sequential decision making processes

At each time step \(t[T-1]:=\{0,1,,T-1\}\), where \(T\) is a time horizon, the decision making process is characterized by a \(D\)-dimensional vector state \(_{t}=^{D}\), an action \(a_{t}\), where \(\) is a finite set of \(N\) actions, and a reward \(R(_{t},a_{t})\) associated with each pair of states and actions. Moreover, given an episode of the decision making process, \(=\{(_{t},a_{t})\}_{t=0}^{T-1}\), the process's outcome \(o()=_{t}R(_{t},a_{t})\) is given by the sum of the rewards. In the remainder, we will denote the elements of a vector \(_{t}\) as \(s_{t,1},,s_{t,D}\).2

Further, we characterize the dynamics of the decision making process using the framework of structural causal models (SCMs). In general, an SCM is consisted of four parts: (i) a set of endogenous variables (ii) a set of exogenous (noise) variables (iii) a set of structural equations assigning values to the endogenous variables, and (iv) a set of prior distributions characterizing the exogenous variables . In our setting, the endogenous variables of the SCM \(\) are the random variables representing the states \(_{0},,_{T-1}\) and the actions \(A_{0},,A_{T-1}\). The action \(A_{t}\) at time step \(t\) is chosen based on the observed state \(_{t}\) and is given by a structural (policy) equation

\[A_{t}:=g_{A}(_{t},_{t}),\] (1)

where \(_{t}\) is a vector-valued noise variable, to allow some level of stochasticity in the choice of the action, and its prior distribution \(P^{}(_{t})\) is characterized by a density function \(f^{}_{_{t}}\). Similarly, the state \(_{t+1}\) in the next time step is given by a structural (transition) equation

\[_{t+1}:=g_{S}(_{t},A_{t},_{t}),\] (2)

where \(_{t}\) is a vector-valued noise variable with its prior distribution \(P^{}(_{t})\) having a density function \(f^{}_{_{t}}\), and we refer to the function \(g_{S}\) as the _transition mechanism_. Note that, in Eq. 2, the noise variables \(\{_{t}\}_{t=0}^{T-1}\) are mutually independent and, keeping the sequence of actions fixed, they are the only source of stochasticity in the dynamics of the environment. In other words, a sampled sequence of noise values \(\{_{t}\}_{t=0}^{T-1}\) and a fixed sequence of actions \(\{a_{t}\}_{t=0}^{T-1}\) result into a single (deterministic) sequence of states \(\{_{t}\}_{t=0}^{T-1}\). This implicitly assumes that the state transitions are stationary and there are no unobserved confounders. Figure 4 in Appendix B depicts the causal graph \(G\) corresponding to the SCM \(\) defined above.

The above representation of sequential decision making using an SCM \(\) is a more general reformulation of a Markov decision process, where a (stochastic) policy \((a\,|\,)\) is entailed by Eq. 1, and the transition distribution (_i.e._, the conditional distribution of \(_{t+1}\,|\,_{t},A_{t}\)) is entailed by Eq. 2. Specifically, the conditional density function of \(_{t+1}\,|\,_{t},A_{t}\) is given by

\[p^{}(_{t+1}=\,|\,_{t}=_{t},A_ {t}=a_{t})=p^{\,;\,do(A_{t}=a_{t})}(_{t+1}=\,|\,_ {t}=_{t})\\ =_{}[=g_{S}(_{t},a_ {t},)] f^{}_{_{t}}()d,\] (3)

where \(do(A_{t}=a_{t})\) denotes a (hard) intervention on the variable \(A_{t}\), whose value is set to \(a_{t}\).3 Here, the first equality holds because \(_{t+1}\) and \(A_{t}\) are d-separated by \(_{t}\) in the sub-graph obtained from \(G\) after removing all outgoing edges of \(A_{t}\)4 and the second equality follows from Eq. 2.

Moreover, as argued elsewhere , by using an SCM to represent sequential decision making, instead of a classic MDP, we can answer counterfactual questions. More specifically, assume that, at time step \(t\), we observed the state \(_{t}=_{t}\), we took action \(A_{t}=a_{t}\) and the next state was \(_{t+1}=_{t+1}\). Retrospectively, we would like to know the probability that the state \(_{t+1}\) would have been \(^{}\) if, at time step \(t\), we had been in a state \(\), and we had taken an action \(a\), (generally) different from \(_{t},a_{t}\). Using the SCM \(\), we can characterize this by a counterfactual transition density function

\[p^{\,|\,_{t+1}=_{t+1},_{t}=_ {t},A_{t}=a_{t}\,;\,do(A_{t}=a)}(_{t+1}=^{}\,|\,_{t}= s)=\\ _{}[^{}=g_{S}(,a,)] f^{\,|\,_{t+1}=_{t+1},_{t}=_{t},A_{t}=a_{t}}()d,\] (4)

where \(f^{\,|\,_{t+1}=_{t+1},_{t}=_{t},A_{t}=a_{t}}\) is the posterior distribution of the noise variable \(_{t}\) with support such that \(_{t+1}=g_{S}(_{t},a_{t},)\).

In what follows, we will assume that the transition mechanism \(g_{S}\) is continuous with respect to its last argument and the SCM \(\) satisfies the following form of Lipschitz-continuity:

**Definition 1**.: _An SCM \(\) is Lipschitz-continuous iff the transition mechanism \(g_{S}\) and the reward \(R\) are Lipschitz-continuous with respect to their first argument, i.e., for each \(a\), \(\), there exists a Lipschitz constant \(K_{a,}_{+}\) such that, for any \(,^{}\), \(\|g_{S}(,a,)-g_{S}(^{},a,)\| K_{a,}\, \|-^{}\|\), and, for each \(a\), there exists a Lipschitz constant \(C_{a}_{+}\) such that, for any \(,^{}\), \(|R(,a)-R(^{},a)| C_{a}\,\|-^{}\|\). In both cases, \(\|\|\) denotes the Euclidean distance._

Note that, although they are not phrased in causal terms, similar Lipschitz continuity assumptions for the environment dynamics are common in prior work analyzing the theoretical guarantees of reinforcement learning algorithms . Moreover, for practical applications (_e.g._, in healthcare), this is a relatively mild assumption to make. Consider two patients whose vitals \(\) and \(^{}\) are similar at a certain point in time, they receive the same treatment \(a\), and every unobserved factor \(\) that may affect their health is also the same. Intuitively, Definition 1 implies that their vitals will also evolve similarly in the immediate future, _i.e._, the values \(g_{S}(,a,)\) and \(g_{S}(^{},a,)\) will not differ dramatically. In this context, it is worth mentioning that, when the transition mechanism \(g_{S}\) is modeled by a neural network, it is possible to control its Lipschitz constant during training, and penalizing high values can be seen as a regularization method .

Further, we will focus on bijective SCMs , a fairly broad class of SCMs, which subsumes multiple models studied in the causal discovery literature, such as additive noise models , post-nonlinear causal models , location-scale noise models  and more complex models with neural network components .

**Definition 2**.: _An SCM \(\) is bijective iff the transition mechanism \(g_{S}\) is bijective with respect to its last argument, i.e., there is a well-defined inverse function \(g_{S}^{-1}:\) such that, for every combination of \(_{t+1},_{t},a_{t},_{t}\) with \(_{t+1}=g_{S}(_{t},a_{t},_{t})\), it holds that \(_{t}=g_{S}^{-1}(_{t},a_{t},_{t+1})\)._

Importantly, bijective SCMs allow for a more concise characterization of the counterfactual transition density given in Eq. 4. More specifically, after observing an event \(_{t+1}=_{t+1},_{t}=_{t},A_{t}=a_{t}\), the value \(_{t}\) of the noise variable \(_{t}\) can only be such that \(_{t}=g_{S}^{-1}(_{t},a_{t},_{t+1})\), _i.e._, the posterior distribution of \(_{t}\) is a point mass and its density is given by

\[f_{_{t}}^{\,|\,_{t+1}=_{t+1},_{t}=_{ t},A_{t}=a_{t}}()=[=g_{S}^{-1}(_{t},a_{t},_{t+1})],\] (5)

where \([]\) denotes the indicator function. Then, for a given episode \(\) of the decision making process, we have that the (non-stationary) counterfactual transition density is given by

\[p_{,t}(_{t+1}=^{}\,|\,_{t}=,A_ {t}=a) :=p^{\,|\,_{t+1}=_{t+1},_{t}=_{ t},A_{t}=a_{t}\,;\,do(A_{t}=a)}(_{t+1}=^{}\,|\,_{t}= )\] \[=_{}[^{}=g_{S}( {s},a,)][=g_{S}^{-1}(_{t},a_{t},_{t+1}) ]d\] \[=[^{}=g_{S}(,a,g_{S}^{-1} (_{t},a_{t},_{t+1}))].\] (6)

Since this density is also a point mass, the resulting counterfactual dynamics are purely deterministic. That means, under a bijective SCM, the answer to the question "_What would have been the state at time \(t+1\), had we been at state \(\) and taken action \(a\) at time \(t\), given that, in reality, we were at \(_{t}\), we took \(a_{t}\) and the environment transitioned to \(_{t+1}\)?_" is just given by \(^{}=g_{S}(,a,g_{S}^{-1}(_{t},a_{t},_{ t+1}))\).

**On the counterfactual identifiability of bijective SCMs.** Very recently, Nasr-Esfahany and Kiciman  have shown that bijective SCMs are in general not counterfactually identifiable when the exogenous variable \(_{t}\) is multi-dimensional. In other words, even with access to an infinite amount of triplets \((_{t},a_{t},_{t+1})\) sampled from the true SCM \(\), it is always possible to find an SCM \(\) with transition mechanism \(h_{S}\) and distributions \(P^{}(_{t})\) that entails the same transition distributions as \(\) (_i.e._, it fits the observational data perfectly), but leads to different counterfactual predictions. Although our subsequent algorithmic results do not require the SCM \(\) to be counterfactually identifiable, the subclass of bijective SCMs we will use in our experiments in Section 5 is counterfactually identifiable. The defining attribute of this subclass, which we refer to as _element-wise bijective SCMs_, is that the transition mechanism \(g_{S}\) can be decoupled into \(D\) independent mechanisms \(g_{S,i}\) such that \(S_{t+1,i}=g_{S,i}(_{t},A_{t},U_{t,i})\) for \(i\{1,,D\}\). This implies \(S_{t+1,i}\!\!\! U_{t,j}\,|\,U_{t,i},_{t},A_{t}\) for \(j i\), however, \(U_{t,i}\), \(U_{t,j}\) do not need to be independent. Informally, we have the following identifiability result (refer to Appendix C for a formal version of the theorem along with its proof, which follows a similar reasoning to proofs found in related work ):

**Theorem 3** (Informal).: _Let \(\) and \(\) be two element-wise bijective SCMs such that their entailed transition distributions for \(_{t+1}\) given any value of \(_{t},A_{t}\) are always identical. Then, all their counterfactual predictions based on an observed transition \((_{t},a_{t},_{t+1})\) will also be identical._

**On the assumption of no unobserved confounding.** The assumption that there are no hidden confounders is a frequent assumption made by work at the intersection of counterfactual reasoning and reinforcement learning  and, more broadly, in the causal inference literature . That said, there is growing interest in developing off-policy methods for partially observable MPDs (POMDPs) that are robust to certain types of confounding , and in learning dynamic treatment regimes in sequential settings with non-Markovian structure . Moreover, there is a line of work focusing on the identification of counterfactual quantities in non-sequential confounded environments . In that context, we consider the computation of (approximately) optimal counterfactual action sequences under confounding as a very interesting direction for future work.

Problem statement

Let \(\) be an observed episode of a decision making process whose dynamics are characterized by a Lipschitz-continuous bijective SCM. To characterize the counterfactual outcome that any alternative action sequence would have achieved under the circumstances of the particular episode, we build upon the formulation of Section 2, and we define a non-stationary counterfactual MDP \(^{+}=(^{+},,F_{,t}^{+},R^{+},T)\) with deterministic transitions. Here, \(^{+}=[T-1]\) is an enhanced state space such that each \(^{+}^{+}\) is a pair \((,l)\) indicating that the counterfactual episode would have been at state \(\) with \(l\) action changes already performed. Accordingly, \(R^{+}\) is a reward function which takes the form \(R^{+}((,l),a)=R(,a)\) for all \((,l)^{+}\), \(a\), _i.e._, it does not change depending on the number of action changes already performed. Finally, the time-dependent transition function \(F_{,t}^{+}:^{+}^{+}\) is defined as

\[F_{,t}^{+}((,l),a)=(g_{S} (,a,g_{S}^{-1}(_{t},a_{t},_{t+1})),l+1 )&(a a_{t})\\ (g_{S}(,a_{t},g_{S}^{-1}(_{t},a_{t},_{t+1} )),l)&\] (7)

Intuitively, here we set the transition function according to the point mass of the counterfactual transition density given in Eq. 6, and we use the second coordinate to keep track of the changes that have been performed in comparison to the observed action sequence up to the time step \(t\).

Now, given the initial state \(_{0}\) of the episode \(\) and any counterfactual action sequence \(\{a_{t}^{}\}_{t=0}^{T-1}\), we can compute the corresponding counterfactual episode \(^{}=\{(_{t}^{},l_{t}),a_{t}^{}\}_{t=0}^{T-1}\). Its sequence of states is given recursively by

\[(_{1}^{},l_{1})=F_{,0}^{+}((_{0},0),a_{0 }^{})\ \ (_{t+1}^{},l_{t+1})=F_{,0}^{+}((_{t }^{},l_{t}),a_{t}^{})\ t\{1,,T-1\},\] (8)

and its counterfactual outcome is given by \(o^{+}(^{}):=_{t}R^{+}((_{t}^{},l_{t}),a_{t}^{})=_{t}R(_{t}^{},a_{t}^{})\).

Then, similarly as in Tsirtsis et al. , our ultimate goal is to find the counterfactual action sequence \(\{a_{t}^{}\}_{t=0}^{T-1}\) that, starting from the observed initial state \(_{0}\), maximizes the counterfactual outcome subject to a constraint on the number of counterfactual actions that can differ from the observed ones, _i.e._,

\[,,a_{T-1}}{}\ \ \ o^{+}(^{})\ \ _{0}^{}=_{0}\ \ _{t=0}^{T-1}[a_{t} a_{t}^{}] k,\] (9)

where \(a_{0},,a_{T-1}\) are the observed actions. Unfortunately, using a reduction from the classic partition problem , the following theorem shows that we cannot hope to find the optimal action sequence in polynomial time:5

**Theorem 4**.: _The problem defined by Eq. 9. is NP-Hard._

The proof of the theorem relies on a reduction from the partition problem , which is known to be NP-complete, to our problem, defined in Eq. 9. At a high-level, we map any instance of the partition problem to an instance of our problem, taking special care to construct a reward function and an observed action sequence, such that the optimal counterfactual outcome \(o^{+}(^{*})\) takes a specific value if and only if there exists a valid partition for the original instance. The hardness result of Theorem 4 motivates our subsequent focus on the design of a method that _always_ finds the optimal solution to our problem at the expense of a potentially higher runtime for some problem instances.

## 4 Finding the optimal counterfactual action sequence via A* search

To deal with the increased computational complexity of the problem, we develop an optimal search method based on the classic \(A^{*}\) algorithm , which we have found to be very efficient in practice. Our starting point is the observation that, the problem of Eq. 9 presents an optimal substructure, _i.e._, its optimal solution can be constructed by combining optimal solutions to smaller sub-problems. For an observed episode \(\), let \(V_{}(,l,t)\) be the maximum counterfactual reward that could have been achieved in a counterfactual episode where, at time \(t\), the process is at a (counterfactual) state \(\), and there are so far \(l\) actions that have been different in comparison with the observed action sequence. Formally,

\[V_{}(,l,t)=_{a_{t}^{},,a_{T-1}^{}}_{t^{ }=t}^{T-1}R(_{t^{}}^{},a_{t^{}}^{}) \ \ _{t}^{}=\ \ _{t^{}=t}^{T-1}[a_{t^{}} a_{t^{}}^{}]  k-l.\]Then, it is easy to see that the quantity \(V_{}(,l,t)\) can be given by the recursive function

\[V_{}(,l,t)=_{a}\{R(,a)+V_{}(_{a},l_{a},t+1)\},,\ l<kt<T-1,\] (10)

where \((_{a},l_{a})=F_{,t}^{+}((,l)\,,a)\). In the base case of \(l=k\) (_i.e._, all allowed action changes are already performed), we have \(V_{}(,k,t)=R(,a_{t})+V_{}(_{a_{t}},l_{a_{t}},t+1)\) for all \(\) and \(t<T-1\), and \(V_{}(,k,T-1)=R(,a_{T-1})\) for \(t=T-1\). Lastly, when \(t=T-1\) and \(l<k\), we have \(V_{}(,l,T-1)=_{a}R(,a)\) for all \(\).

Given the optimal substructure of the problem, one may be tempted to employ a typical dynamic programming approach to compute the values \(V_{}(,l,t)\) in a bottom-up fashion. However, the complexity of the problem lies in the fact that, the states \(\) are real-valued vectors whose exact values depend on the entire action sequence that led to them. Hence, to enumerate all the possible values that \(\) might take, one has to enumerate all possible action sequences in the search space, which is equivalent to solving our problem with a brute force search. In what follows, we present our proposed method to find optimal solutions using the \(A^{*}\) algorithm, with the caveat that its runtime varies depending on the problem instance, and it can be equal to that of a brute force search in the worst case.

**Casting the problem as graph search.** We represent the solution space of our problem as a graph, where each node \(v\) corresponds to a tuple \((,l,t)\) with \(\), \(l[k]\) and \(t[T]\). Every node \(v=(,l,t)\) with \(l<k\) and \(t<T-1\) has \(||\) outgoing edges, each one associated with an action \(a\), carrying a reward \(R(,a)\), and leading to a node \(v_{a}=(_{a},l_{a},t+1)\) such that \((_{a},l_{a})=F_{,t}^{+}((,l)\,,a)\). In the case of \(l=k\), the node \(v\) has exactly one edge corresponding to the observed action \(a_{t}\) at time \(t\). Lastly, when \(t=T-1\), the outgoing edge(s) lead(s) to a common node \(v_{T}=(_{},k,T)\) which we call the _goal node_, and it has zero outgoing edges itself. Note that, the exact value of \(_{}\) is irrelevant, and we only include it for notational completeness.

Let \(_{0}\) be the initial state of the observed episode. Then, it is easy to notice that, starting from the root node \(v_{0}=(_{0},0,0)\), the first elements of each node \(v_{i}\) on a path \(v_{0},,v_{i},,v_{T}\) form a sequence of counterfactual states, and the edges that connect those nodes are such that the corresponding counterfactual action sequence differs from the observed one in at most \(k\) actions. That said, the counterfactual outcome \(o^{+}()=_{t=0}^{T-1}R(_{t}^{},a_{t}^{})\) is expressed as the sum of the rewards associated with each edge in the path, and the problem defined by Eq. 9 is equivalent to finding the path of maximum total reward that starts from \(v_{0}\) and ends in \(v_{T}\). Figure 0(a) illustrates the search graph for a simple instance of our problem. Unfortunately, since the states \(\) are vectors of real values, even enumerating all the graph's nodes requires time exponential in the number of actions \(||\), which makes classic algorithms that search over the entire graph non-practical.

To address this challenge, we resort to the \(A^{*}\) algorithm, which performs a more efficient search over the graph by preferentially exploring only parts of it where we have prior information that they are

Figure 1: Main components of our search method based on the A* algorithm. Panel (a) shows the search graph for a problem instance with \(||=2\). Here, each box represents a node \(v=(,l,t)\) of the graph, and each edge represents a counterfactual transition. Next to each edge, we include the action \(a\) causing the transition and the associated reward. Panel (b) shows the heuristic function computation, where the two axes represent a (continuous) state space \(=^{2}\) and the two levels on the z-axis correspond to differences in the (integer) values \((l,t)\) and \((l_{a},t+1)\). Here, the blue squares correspond to the finite states in the anchor set \(_{}\) and \((_{a},l_{a})=F_{,t}^{+}((,l)\,,a)\).

more likely to lead to paths of higher total reward. Concretely, the algorithm proceeds iteratively and maintains a queue of nodes to visit, initialized to contain only the root node \(v_{0}\). Then, at each step, it selects one node from the queue, and it retrieves all its children nodes in the graph which are subsequently added to the queue. It terminates when the node being visited is the goal node \(v_{T}\). Refer to Algorithm 2 in Appendix E for a pseudocode implementation of the \(A^{*}\) algorithm.

The key element of the \(A^{*}\) algorithm is the criterion based on which it selects which node from the queue to visit next. Let \(v_{i}=(_{i},l_{i},t)\) be a candidate node in the queue and \(r_{v_{i}}\) be the total reward of the path that the algorithm has followed so far to reach from \(v_{0}\) to \(v_{i}\). Then, the \(A^{*}\) algorithm visits next the node \(v_{i}\) that maximizes the sum \(r_{v_{i}}+_{}(_{i},l_{i},t)\), where \(_{}\) is a _heuristic function_ that aims to estimate the maximum reward that can be achieved via any path starting from \(v_{i}=(_{i},l_{i},t)\) and ending in the goal node \(v_{T}\), _i.e._, it gives an estimate for the quantity \(V_{}(_{i},l_{i},t)\). Intuitively, the heuristic function can be thought of as an "eye into the future" of the graph search, that guides the algorithm towards nodes that are more likely to lead to the optimal solution and the algorithm's performance depends on the quality of the approximation of \(V_{}(_{i},l_{i},t)\) by \(_{}(_{i},l_{i},t)\). Next, we will look for a heuristic function that satisfies _consistency_6 to guarantee that the \(A^{*}\) algorithm as described above returns the optimal solution upon termination .

**Computing a consistent heuristic function.** We first propose an algorithm that computes the function's values \(_{}(,l,t)\) for a finite set of points such that \(l[k]\), \(t[T-1]\), \(_{}\), where \(_{}\) is a pre-defined _finite_ set of states--an _anchor set_--whose construction we discuss later. Then, based on the Lipschitz-continuity of the SCM \(\), we show that these computed values of \(_{}\) are valid upper bounds of the corresponding values \(V_{}(,l,t)\) and we expand the definition of the heuristic function \(_{}\) over all \(\) by expressing it in terms of those upper bounds. Finally, we prove that the function resulting from the aforementioned procedure is consistent.

To compute the upper bounds \(_{}\), we exploit the observation that the values \(V_{}(,l,t)\) satisfy a form of Lipschitz-continuity, as stated in the following Lemma.

**Lemma 5**.: _Let \(_{t}=g_{S}^{-1}(_{t},a_{t},_{t+1})\), \(K_{_{t}}=_{a}K_{a,_{t}}\), \(C=_{a}C_{a}\) and the sequence \(L_{0},,L_{T-1}_{+}\) be such that \(L_{T-1}=C\) and \(L_{t}=C+L_{t+1}K_{_{t}}\) for \(t[T-2]\). Then, it holds that \(|V_{}(,l,t)-V_{}(^{},l,t)| L_{t}\|- ^{}\|\), for all \(t[T-1]\), \(l[k]\) and \(,^{}\)._

Based on this observation, our algorithm proceeds in a bottom-up fashion and computes valid upper bounds of the values \(V_{}(,l,t)\) for all \(l[k]\), \(t[T-1]\) and \(\) in the anchor set \(_{}\). To get the tinuition, assume that, for a given \(t\), the values \(_{}(,l,t+1)\) are already computed for all \(_{}\), \(l[k]\), and they are indeed valid upper bounds of the corresponding \(V_{}(,l,t+1)\). Then, let \((_{a},l_{a})=F^{+}_{,t}((,l),a)\) for some \(_{}\) and \(l[k]\). Since \(_{a}\) itself may not belong to the finite anchor set \(_{}\), the algorithm uses the values \(_{}(_{},l_{a},t+1)\) of all anchors \(_{}_{}\) in combination with their distance to \(_{a}\), and it sets the value of \(_{}(,l,t)\) in way that it is also guaranteed to be a (maximally tight) upper bound of \(V_{}(,l,t)\). Figure 0(b) illustrates the above operation. Algorithm 1 summarizes the overall procedure, which is guaranteed to return upper bounds, as shown by the following proposition:

**Proposition 6**.: _For all \(_{}\), \(l[k],t[T-1]\), it holds that \(_{}(,l,t) V_{}(,l,t)\), where \(_{}(,l,t)\) are the values of the heuristic function computed by Algorithm 1._

Next, we use the values \(_{}(,l,t)\) computed by Algorithm 1 to expand the definition of \(_{}\) over the entire domain as follows. For some \(\), \(a\), let \((_{a},l_{a})=F^{+}_{,t}((,l),a)\), then, we have that

\[_{}(,l,t)=0&t=T\\ _{a^{}}R(,a)&t=T-1\\ _{a^{}}\{R(,a)+_{_ {i}_{}}\{_{}(_{},l_{a},t+1) +L_{t+1}\|_{}-_{a}\|\}\}&,\] (11)

where \(^{}=\{a_{t}\}\) for \(l=k\) and \(^{}=\) for \(l<k\). Finally, the following theorem shows that the resulting heuristic function \(_{}\) is consistent:

**Theorem 7**.: _For any nodes \(v=(,l,t),v_{a}=(_{a},l_{a},t+1)\) with \(t<T-1\) connected with an edge associated with action \(a\), it holds that \(_{}(,l,t) R(,a)+_{}(_{a},l_{a},t+1)\). Moreover, for any node \(v=(,l,T-1)\) and edge connecting it to the goal node \(v_{T}=(_{},k,T)\), it holds that \(_{}(,l,T-1) R(,a)+_{}(_{},k,T)\)._

**Kick-starting the heuristic function computation with Monte Carlo anchor sets.** For any \(_{}\), whenever we compute \(_{}(,l,t)\) using Eq. 11, the resulting value is set based on the value \(_{}(_{},l_{a},t+1)\) of some anchor \(_{}\), increased by a _penalty_ term \(L_{t+1}\|_{}-_{a}\|\). Intuitively, this allows us to think of the heuristic function \(_{}\) as an upper bound of the function \(V_{}\) whose looseness depends on the magnitude of the penalty terms encountered during the execution of Algorithm 1 and each subsequent evaluation of Eq. 11. To speed up the \(A^{*}\) algorithm, note that, ideally, one would want all penalty terms to be zero, _i.e._, an anchor set that includes all the states \(\) of the nodes \(v=(,l,t)\) that are going to appear in the search graph. However, as discussed in the beginning of Sec. 4, an enumeration of those states requires a runtime exponential in the number of actions.

To address this issue, we introduce a Monte Carlo simulation technique that adds to the anchor set the observed states \(\{_{0},,_{T-1}\}\) and all unique states \(\{^{}_{0},,^{}_{T-1}\}\) resulting by \(M\) randomly sampled counterfactual action sequences \(a^{}_{0},,a^{}_{T-1}\). Specifically, for each action sequence, we first sample a number \(k^{}\) of actions to be changed and what those actions are going to be, both uniformly at random from \(\{1,,k\}\) and \(^{k^{}}\), respectively. Then, we sample from \(\{0,,T-1\}\) the \(k^{}\) time steps where the changes take place, with each time step \(t\) having a probability \(L_{t}/_{t^{}}L_{t^{}}\) to be selected. This biases the sampling towards earlier time steps, where the penalty terms are larger due to the higher Lipschitz constants. As we will see in the next section, this approach works well in practice, and it allows us to control the runtime of the \(A^{*}\) algorithm by appropriately adjusting the number of samples \(M\). We experiment with additional anchor set selection strategies in Appendix F.

## 5 Experiments using clinical sepsis management data

**Experimental setup.** To evaluate our method, we use real patient data from MIMIC-III , a freely accessible critical care dataset commonly used in reinforcement learning for healthcare . We follow the preprocessing steps of Komorowski et al.  to identify a cohort of \(20{,}926\) patients treated for sepsis . Each patient record contains vital signs and administered treatment information in time steps of \(4\)-hour intervals. As an additional preprocessing step, we discard patient records whose associated time horizon \(T\) is shorter than \(10\), resulting in a final dataset of \(15{,}992\) patients with horizons between \(10\) and \(20\).

To form our state space \(=^{D}\), we use \(D=13\) features. Four of these features are demographic or contextual and thus we always set their counterfactual values to the observed ones. The remaining \(=9\) features are time-varying and include the SOFA score --a standardized score of organ failure rate--along with eight vital signs that are required for its calculation. Since SOFA scores positively correlate with patient mortality , we assume that each \(\) gives a reward \(R()\) equal to the negation of its SOFA value. Here, it is easy to see that this reward function is just a projection of \(\), therefore, it is Lipschitz continuous with constant \(C_{a}=1\) for all \(a\). Following related work , we consider an action space \(\) that consists of \(25\) actions, which correspond to \(5 5\) levels of administered vasopressors and intravenous fluids. Refer to Appendix G for additional details on the features and actions.

To model the transition dynamics of the time-varying features, we consider an SCM \(\) whose transition mechanism takes a location-scale form \(g_{S}(_{t},A_{t},_{t})=h(_{t},A_{t})+(_{t},A_{t} )_{t}\), where \(h,:^{}\), and \(\) denotes the element-wise multiplication . Notably, this model is element-wise bijective and hence it is counterfactually identifiable, as shown in Section 2. Moreover, we use neural networks to model the location and scale functions \(h\) and \(\) and enforce their Lipschitz constants to be \(L_{h}\) and \(L_{}\), respectively. This results in a Lipschitz continuous SCM \(\) with \(K_{a,}=L_{h}+L_{}_{i}|u_{i}|\). Further, we assume that the noise variable \(_{t}\) follows a multivariate Gaussian distribution with zero mean and allow its covariance matrix to be a (trainable) parameter.

We jointly train the weights of the networks \(h\) and \(\) and the covariance matrix of the noise prior on the observed patient transitions using stochastic gradient descent with the negative log-likelihood of each transition as a loss. In our experiments, if not specified otherwise, we use an SCM with Lipschitz constants \(L_{h}=1.0\), \(L_{}=0.1\) that achieves a log-likelihood only \(6\%\) lower to that of the best model trained without any Lipschitz constraint. Refer to Appendix G for additional details on the network architectures, the training procedure and the way we enforce Lipschitz continuity.7

**Results.** We start by evaluating the computational efficiency of our method against (i) the Lipschitz constant of the location network \(L_{h}\), (ii) the number of Monte Carlo samples \(M\) used to generate the anchor set \(_{t}\), and (iii) the number of actions \(k\) that can differ from the observed ones. We measure efficiency using running time and the effective branching factor (EBF) . The EBF is defined as a real number \(b 1\) such that the number of nodes expanded by \(A^{*}\) is equal to \(1+b+b^{2}++b^{T}\), where \(T\) is the horizon, and values close to \(1\) indicate that the heuristic function is the most efficient in guiding the search. Figure 2 summarizes the results, which show that our method maintains overall a fairly low running time that decreases with the number of Monte Carlo samples \(M\) used for the generation of the anchor set and increases with the Lipschitz constant \(L_{h}\) and the number of action changes \(k\). That may not come as a surprise since, as \(L_{h}\) increases, the heuristic function becomes more loose, and as \(k\) increases, the size of the search space increases exponentially. To put things in perspective, for a problem instance with \(L_{h}=1.0\), \(k=3\) and horizon \(T=12\), the \(A^{*}\) search led by our heuristic function is effectively equivalent to an exhaustive search over a full tree with

Figure 2: Computational efficiency of our method under different configurations, as measured by the effective branching factor (pink-left axis) and the runtime of the \(A^{*}\) algorithm (green-right axis). In Panel (a), we set \(M=2000\) and \(k=3\). In Panel (b), we set \(L_{h}=1.0\) and \(k=3\). In Panel (c), we set \(L_{h}=1.0\) and \(M=2000\). In all panels, we set \(L_{}=0.1\) and error bars indicate \(95\%\) confidence intervals over \(200\) executions of the \(A^{*}\) algorithm for \(200\) patients with horizon \(T=12\).

\(2.1^{12} 7,355\) leaves while the corresponding search space of our problem consists of more than \(3\) million action sequences--more than \(3\) million paths to reach from the root node to the goal node.

Next, we investigate to what extent the counterfactual action sequences generated by our method would have led the patients in our dataset to better outcomes. For each patient, we measure their counterfactual improvement--the relative decrease in cumulative SOFA score between the counterfactual and the observed episode. Figures 2(a) and 2(b) summarize the results, which show that: (i) the average counterfactual improvement shows a diminishing increase as \(k\) increases; (ii) the median counterfactual improvement is only \(5\%\), indicating that, the treatment choices made by the clinicians for most of the patients were close to optimal, even with the benefit of hindsight; and (iii) there are \(176\) patients for whom our method suggests that a different sequence of actions would have led to an outcome that is at least \(15\%\) better. That said, we view patients at the tail of the distribution as "interesting cases" that should be deferred to domain experts for closer inspection, and we present one such example in Fig. 2(c). In this example, our method suggests that, had the patient received an early higher dosage of intravenous fluids while some of the later administered fluids where replaced by vasopressors, their SOFA score would have been lower across time. Although we present this case as purely anecdotal, the counterfactual episode is plausible, since there are indications of decreased mortality when intravenous fluids are administered at the early stages of a septic shock .

## 6 Conclusions

In this paper, we have introduced the problem of finding counterfactually optimal action sequences in sequential decision making processes with continuous state dynamics. We showed that the problem is NP-hard and, to tackle it, we introduced a search method based on the \(A^{*}\) algorithm that is guaranteed to find the optimal solution, with the caveat that its runtime can vary depending on the problem instance. Lastly, using real clinical data, we have found that our method is very efficient in practice, and it has the potential to offer interesting insights to domain experts by highlighting episodes and time-steps of interest for further inspection.

Our work opens up many interesting avenues for future work. For example, it would be interesting to develop algorithms with approximation guarantees that run in polynomial time, at the expense of not achieving strict counterfactual optimality. Moreover, since the practicality of methods like ours relies on the assumption that the SCM describing the environment is accurate, it would be interesting to develop methods to learn SCMs that align with human domain knowledge. Finally, it would be interesting to validate our method using real datasets from other applications and carry out user studies in which the counterfactual action sequences found by our method are systematically evaluated by the human experts (_e.g._, clinicians) who took the observed actions.

**Acknowledgements.** Tsirtsis and Gomez-Rodriguez acknowledge support from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 945719).

Figure 3: Retrospective analysis of patientsâ€™ episodes. Panel (a) shows the average counterfactual improvement as a function of \(k\) for a set of \(200\) patients with horizon \(T=12\), where error bars indicate \(95\%\) confidence intervals. Panel (b) shows the distribution of counterfactual improvement across all patients for \(k=3\), where the dashed vertical line indicates the median. Panel (c) shows the observed (solid) and counterfactual (dashed) SOFA score across time for a patient who presents a \(19.9\%\) counterfactual improvement when \(k=3\). Upward (downward) arrows indicate action changes that suggest a higher (lower) dosage of vasopressors (V) and fluids (F). In all panels, we set \(M=2000\).