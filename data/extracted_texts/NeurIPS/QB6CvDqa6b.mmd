# An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning

Qian Lin\({}^{1}\), Zongkai Liu\({}^{1}\), Danying Mo\({}^{1}\), Chao Yu\({}^{1,2,3}\)

\({}^{1}\)Sun Yat-sen University, Guangzhou, China

\({}^{2}\)Pengcheng Laboratory, Shenzhen, China

\({}^{3}\)MoE Key Laboratory of Information Technology, Guangzhou, China

{linq67,liuzk,mody3}@mail2.sysu.edu.cn, yuchao3@mail.sysu.edu.cn

Corresponding author

###### Abstract

In recent years, significant progress has been made in multi-objective reinforcement learning (RL) research, which aims to balance multiple objectives by incorporating preferences for each objective. In most existing studies, specific preferences must be provided during deployment to indicate the desired policies explicitly. However, designing these preferences depends heavily on human prior knowledge, which is typically obtained through extensive observation of high-performing demonstrations with expected behaviors. In this work, we propose a simple yet effective offline adaptation framework for multi-objective RL problems without assuming handcrafted target preferences, but only given several demonstrations to implicitly indicate the preferences of expected policies. Additionally, we demonstrate that our framework can naturally be extended to meet constraints on safety-critical objectives by utilizing safe demonstrations, even when the safety thresholds are unknown. Empirical results on offline multi-objective and safe tasks demonstrate the capability of our framework to infer policies that align with real preferences while meeting the constraints implied by the provided demonstrations.

## 1 Introduction

In the standard reinforcement learning (RL) setting, the primary goal is to obtain a policy that maximizes a cumulative scalar reward (Sutton and Barto, 2018). However, in many real-world applications involving multiple objectives, a desired policy must not only strike a balance among potentially conflicting objectives but also consider the constraints imposed on specific safety-critical objectives. Such requirements motivate the research of Multi-objective RL (MORL) (Liu et al., 2014; Mossalam et al., 2016) and safe RL (Gu et al., 2022; Achiam et al., 2017). In addition to reward maximization, the former aims to enable policies that cater to a target preference indicating the trade-off between competing objectives, while the latter focuses on reducing the cost measures of learned policies within a given safety threshold.

Despite the development of meticulous and effective mechanisms to achieve these goals, most existing MORL and safe RL algorithms rely on predefined target preferences or safety thresholds. These elements need to be carefully designed by human experts with prior knowledge, generalized from a large number of demonstrations through observation. Taking autonomous driving as an example, defining aggressive, stable, and conservative strategies through different preferences between minimizing energy consumption and increasing driving speed may require extensive human participation, which means that researchers need to categorize driving data into different styles based on human experience and observe the energy consumption-speed ratio to estimate appropriate preference weights andsafety thresholds for different strategies. Moreover, it is uncertain whether policies based on manually designed preferences will exhibit expected behaviors or whether feasible policies exist under given safety constraints. The challenge of designing appropriate preferences or safety thresholds becomes more pronounced as the number of objectives increases.

Compared to manually designing target preferences or safety thresholds based on human knowledge, it is more natural to infer expected behaviors through a few demonstrations that implicitly indicate the trade-off between multiple objectives and the constraints of safety. For instance, selecting demonstrations with conservative behaviors from the driving dataset can be easier than inferring preferences that lead to conservative policies. Therefore, in this work, we formulate a novel offline adaptation problem for constrained MORL, with the goal to leverage a few demonstrations with expected behaviors, rather than relying on handcrafted target preferences and safety thresholds, to generate a target policy that achieves desired trade-offs across various objectives and meets the constraints on safety-critical objectives under offline settings.

To achieve this, we first focus on unconstrained MORL scenarios and propose a simple yet effective offline adaptation framework **Preference Distribution Offline Adaptation (PDOA)**, which includes: 1) learning a set of policies that respond to various preferences during training, and 2) adapting a distribution of target preferences based on a few given demonstrations during deployment. Specifically, we initialize the first part with existing state-of-the-art offline MORL algorithms, and then in the second part, we propose to align the adapted policies with expected behaviors by modeling the posterior preference distribution regarding demonstrations. Moreover, we show that our framework can be extended to constrained MORL settings by converting a constrained RL problem into an unconstrained MORL counterpart, and incorporating a conservative estimate of preference weights on constrained objectives to mitigate the potential constraint violations. Lastly, we conduct several empirical experiments on classical MORL, safe RL tasks and a novel constrained MORL environment under offline settings, demonstrating the capability of our framework in generating policies that align with the real preferences and meet the constraints implied in demonstrations.

## 2 Related Work

Multi-objective RLMany existing MORL methods explicitly maintain a set of policies tailored to various given preferences to approximate the Pareto front of optimal policies. These works either apply a single-policy algorithm individually for each candidate preference (Roijers et al., 2014; Mossalam et al., 2016), employ evolutionary algorithms to generate a population of diverse policies (Handa, 2009; Xu et al., 2020) or simultaneously learn a set of policies represented by a single network (Abels et al., 2019; Basaklar et al., 2022). Furthermore, due to the potential costs and risks associated with extensive online exploration, several studies (Zhu et al., 2023; Lin et al., 2024) have been proposed to leverage only offline datasets for MORL by extending offline return-conditioned methods (Chen et al., 2021; Emmons et al., 2021) or offline policy-regularized methods (Fujimoto and Gu, 2021; Wang et al., 2022) to MORL settings. Despite the ability to obtain a set of well-performing policies for various preferences, most existing MORL methods overlook the process of acquiring target preferences for identifying appropriate policies during practical deployment. In this work, we assume no online interactions and no target preferences but several demonstrations generated with expected behaviors, which are easier to access than meticulously designed target preferences.

Safe RLWhile maximizing the expected reward, classical safe RL methods restrict the cumulative cost to stay within a predefined safety threshold through Lagrangian primal-dual methods (Stooke et al., 2020; Chow et al., 2017) or primal approaches (Xu et al., 2021; Sootla et al., 2022). Recently, several studies have focused on learning a set of policies that respond to various safety thresholds in both online settings (Yao et al., 2024) and offline settings (Liu et al., 2023; Lin et al., 2023). Similar to MORL, these works also assume access to well-designed safety thresholds that ensure safe behaviors. Unlike prior research, our framework relies solely on safe demonstrations to indicate the implicit constraints and offers a mechanism to mitigate constraint violations through conservatism.

RL with Offline AdaptationSeveral studies have applied meta-learning techniques to address MORL (Chen et al., 2019) or safe RL problems (Guan et al., 2024), but they require online interactions for task adaptation and suffer from low sample efficiency. Additionally, other research endeavors explore offline adaptation methods to mitigate environmental uncertainty by modelling a posterior distribution over all possible Markov Decision Processes (MDPs) (Ghosh et al., 2022) or considering varying confidence levels in conservative value estimates (Hong et al., 2022). (Mitchell et al., 2021; Xu et al., 2022a) focus on offline adaptation for multi-task problems, aiming to achieve fast adaptation to new downstream tasks after offline training on multi-task experience. Among these methods, offline meta RL (Mitchell et al., 2021) addresses a problem similar to ours, which aims to train a meta policy that can adapt to a new task with limited data. Prompt-DT (Xu et al., 2022a) achieves quick adaptation to new tasks by incorporating a few demonstrations as prompts into the decision transformer (Chen et al., 2021) framework. We present further discussions about the difference between multi-task RL and our setting in Appendix A.1.

## 3 Preliminaries

### Constrained Multi-Objective MDP (CMO-MDP)

Both multiple-objective RL and safe RL can be discussed based on a uniform formulation: constrained multi-objective MDP (CMO-MDP) proposed by LP3 (Huang et al., 2022). A CMO-MDP is defined as a tuple \((,,,,,,)\) with state space \(\), action space \(\), transition distribution \((s^{}|s,a)\), vector reward functions \( R^{N}\) for \(N\) unconstrained objectives, vector cost functions \( R^{K}_{+}\), safety thresholds \( R^{K}_{+}\) for \(K\) constrained objectives and discount factor \(\). The goal is to maximize the rewards on unconstrained objectives while ensuring the costs on constrained objectives remain within the safety threshold \(\). Since it is typically infeasible to maximize all task objectives simultaneously, preferences \(\) and preference functions \(f_{}()\) which map the reward \(\) to a scalar utility under a specific preference \(\), are introduced to control the trade-off between unconstrained objectives. Given preferences \(\) and safety thresholds \(\), the goal can be formulated as follows:

\[,}}{}_{_{, {}}}R_{},,\] (1)

where \(R_{}=_{t}f_{}(_{t})\) and \(=_{t}_{t}\) represent cumulative utility and vector cost over time \(t\), respectively. We denote \(_{,}\) as a policy conditioned on \(,\). In this paper, we consider the linear preference setting (i.e., \(f_{}()=^{}\) where \( R^{N}\) and \(\|\|_{1}=1\)), which is widely studied and applied (Mossalam et al., 2016; Abels et al., 2019) and also serves as a bridge between unconstrained and constrained MORL, as shown in Section 4.3. A CMO-MDP problem can degenerate to a standard safe RL problem when \(N=1\) and to a standard multi-objective problem when \(K=0\).

### Offline MORL

Under offline MORL settings, an offline dataset \(=\{(s,a,s^{},,,)\}\) is the only data available for training, which is generated by a set of behavior policies \(_{b}(|)\) with diverse behavioral preferences \(\). One straightforward yet effective approach to learn a set of policies for various preferences is to adapt offline single-objective RL methods for MORL settings. An example of this approach is the **multi-objective version of Diffusion-QL (MODF)**(Lin et al., 2024), which incorporates a preference-conditioned policy (i.e., \((a|s,)\)) and a multi-dimensional value function for \(N\) objectives (i.e., \((s,a,)=Q_{1}(s,a,),...,Q_{N}(s,a,)\)) into Diffusion-QL (Wang et al., 2022):

\[L_{} =-_{(s,a,)} _{_{(|s,)}}^{} (s,a^{},)-\] (2) \[_{i t,(, )}[\|-_{}(_{i}a+_{i}},s,i)\|^{2}],\] \[L_{} =_{(s,a,,s^{},)} [(+_{a^{}(|s,)}(s ^{},a^{},)-(s,a,))^{2}],\]

where \(i\) is the diffusion timestep, \(\) is the regularization weight, \(_{i}\) are pre-defined parameters of diffusion model and \(_{}()\) is a denoiser model. The diffusion policy generates the actions by iteratively using \(_{}()\) to recover actions from noise. The second term in the actor loss of Eq. (2) is a diffusion reconstruction loss, which serves as a regularization term to align the actions of diffusion policy with the behavioral actions in the dataset.

**Pareto-Efficient Decision Agents (PEDA)**(Zhu et al., 2023) is another MORL method based on supervised RL, which trains a policy conditioned on both target preferences and vector returns through a supervised paradigm:

\[L_{}=-_{(_{t:T},)}[(a_{ t}|s_{t},_{t},)],\] (3)where \(_{t:T}=\{(s_{t_{i}},a_{t},_{t}),...,(s_{T},a_{T},_{T})\}\) is a trajectory segment, \(_{t}=_{t^{}=t}^{T}_{t^{}}\) is the target vector return (a.k.a., return-to-go) and \(\) is the behavioral preference of \(_{t:T}\). It is worth noting that despite the significant performance of the above two methods in offline MORL problems, both require human-provided target preferences during deployment to achieve the desired behavior, and therefore cannot be directly applied to the setting in this paper.

## 4 An Offline Adaptation Framework for Constrained Multi-Objective RL

### Problem Formulation of Offline Adaptation for CMO-MDP

In this paper, we focus on a novel offline adaptation problem for constrained MORL, with the goal to leverage only a few demonstrations to generate the policies that exhibit expected behaviors. During training, an offline dataset \(=\{(s,a,s^{},,,)\}\) is provided for policy training. During deployment, we have access to a demonstration set corresponding to a target \(G\), i.e.,

\[_{G}=\{x_{i}_{G}^{*}\}_{i=1}^{M},\] (4)

where \(x_{i}\) is defined as a tuple \((s_{i},a_{i},s^{}_{i},_{i},_{i})\) and \(M\) is the total number of transition demonstrations. The target \(G\) can be preferences in MORL problems (\(G=_{g}\)), safety thresholds in safe problems (\(G=_{g}\)) or a combination of both (\(G=(_{g},_{g})\)), and \(_{G}^{*}\) is the expert policy that achieves the best utility and meet the constraints under the preference \(_{g}\) and safety threshold \(_{g}\) of the target \(G\). In our setting, the real target \(G\) is inaccessible, and the goal is to obtain an adapted policy for the target \(G\) that ensures high utility \(f_{_{g}}()\) and meets the constraints with safety threshold \(_{g}\) by leveraging demonstration set \(_{G}\) during the deployment phase.

### Offline Adaptation for the Unconstrained Case

First, we set aside the constraints in CMO-MDP and focus on the unconstrained version. Our proposed framework, Preference Distribution Offline Adaptation (PDOA), solves the offline adaptation problem under unconstrained settings in two steps: **1)** learning a set of policies that respond to various preferences during training; and then **2)** adapting a distribution of target preferences based on given demonstrations during deployment.

In the first part, we directly apply existing offline MORL algorithms on the dataset \(\) to obtain a set of policies \(_{}\) that respond to varying preferences \(\). In the second phase, we propose to model the distribution of target preferences and then utilize this distribution to obtain a reliable estimation of target preference. Specifically, we consider the posterior probability of the target preference \(_{g}\) with regard to demonstration set \(_{_{g}}\), i.e., \(P(_{g}|_{_{g}},)=_{_{g}}|_{g},)(_{g}|)}{P( _{_{g}}|)}\). Here \(P(_{_{g}}|_{g},)\) represents the probability that the optimal \(_{_{g}}^{*}\) generates samples \(_{_{g}}\) in the real environment \((s^{},|s,a)\). Due to the inaccessibility of \(\) and \(_{}^{*}\) under offline settings, we replace them with the empirical dynamics \(}_{}(s^{}|s,a)\) and its corresponding optimal policy \(_{}^{*}\), which can be obtained using offline data \(\) during training. Therefore, the preference posterior distribution can be approximated by

\[ P(|_{_{g}}, )&=_{_{g}}|, )P(|)}{P(_{_{g}}| )}_{_{g}}|}_{ },_{}^{*},)P(|)}{P(_{_{g}}|)}\\ & P(|)_{i=1}^{M}P_{_{ {}}^{*}}(s_{i})_{}^{*}(a_{i}|s_{i})}_ {}(s^{}_{i},_{i}|s_{i},a_{i}),\] (5)

where \((s_{i},a_{i},r_{i},s^{}_{i})_{_{g}}\). One challenge in Eq. (5) is the requirement of explicitly modeling the state distribution \(P_{_{}^{*}}(s_{i})\) and the transition probability \(}_{}(s^{}_{i},r_{i}|s_{i},a_{i})\). Another concern is that demonstrations \(_{_{g}}\) with the target preference \(_{g}\) can be out-of-distribution samples for the estimation of \(_{}^{*}\) and \(}_{}\), leading to considerable discrepancy between estimated \(P_{_{}^{*}}(s_{i})\), \(}_{}(s^{}_{i},_{i}|s_{i},a_{i})\) and their real-world counterparts. This challenge is pronounced in multi-objective settings due to significant differences in the trajectory distributions of the optimal policy under various preferences. Therefore, following previous offline adaptation works [Ghosh et al., 2022, Hong et al.,2022], we opt to approximate \( P_{^{*}_{}}(s_{i})}_{}(s^{ }_{i},_{i}|s_{i},a_{i})\) with a surrogate defined by TD error of value models of \(^{*}_{}\) in \(}_{}\):

\[r^{}_{}(s,a,,s^{})=-\|(s,a,)-(+(s^{},))\|_{2}^{2},\] (6)

where \(\) is a hyperparameter. This approximation makes sense because \(r^{}_{}(s,a,,s^{})\) not only measures how likely the sample \((s,a,,s^{})\) occurred during training for preference \(\) but also aligns with the estimated dynamics \(}_{}\). In other words, \(r^{}_{}(s,a,,s^{})\) has a high value if \((s,a,,s^{})\) is an in-distribution sample relative to the training datasets under preference and occurs with a high probability in \(}_{}\).

Then, we fit the posterior \(P(|_{_{g}},)\) with a Gaussian distribution \((,)\) with parameters \( R^{K}\) and \( R^{K}_{+}\) by minimizing their Kullback-Leibler divergence, leading to the following adaptation loss:

\[_{}(,)=& _{}(p_{}()||P(| _{_{g}},))/M+(\|\|_{1}-1)^{2}\\ =&-_{(,)}_{i=1}^{M}r^{}_{ }(s_{i},a_{i},_{i},s^{}_{i})+^{*}_{}(a_{i}|s_{t},)+\\ &|)}{M}-)}{M}+(\|\|_{1}-1)^{2},\] (7)

where the term \((\|\|_{1}-1)^{2}\) with hyperparameter \(\) regularizes \(\|\|_{1}\) to be close to \(1\). We approximate the prior distribution \(P(|)\) with a Gaussian distribution fit to the behavioral preferences of training data \(\). Eq. (7) indicates that \(\) is likely to be the target preference \(_{g}\) if it corresponds to high \(r^{}_{}(s_{i},a_{i},_{i},s^{}_{i})\), \(^{*}_{}(a_{i}|s_{t},)\) and \(P(|)\), which means: 1) the demonstration set \(_{_{g}}\) appears with high probability in both the training set and \(}_{}\); 2) \(^{*}_{}(a_{i}|s_{t},)\) is likely to generate the actions in \(_{_{g}}\); and 3) \(\) stays within the prior preference distribution with a high probability. We justify the effectiveness of this approach with the empirical results in Appendix A.9, where \(r^{}_{}(s_{i},a_{i},_{i},s^{}_{i})\) (called TD reward) and \(^{*}_{}(a_{t}|s_{t},)\) (called action likelihood reward) shows a strong correlation with the real target preference.

ImplementationThe first part of our framework PDOA is instantiated with state-of-the-art MORL algorithms: MODF and the best algorithm MORvS in PEDA mentioned in Section 3.2, to learn a set of policies that responds to various preferences. We refer to these two instances as **PDOA [MODF]** and **PDOA [MORvS]**. The well-trained policies and their corresponding value functions obtained by MORL algorithms during training are utilized to calculate the action likelihood reward \(^{*}_{}()\) and TD reward \(r^{}_{}()\) in Eq. (7). During deployment, the preference distribution \(p_{}()\) is updated through the adaptation loss (7) on the demonstration set \(_{_{g}}\). Finally, we obtain an adapted target preference \(_{a}=/\|\|_{1}\) and policy \(^{*}_{}(|_{a})\) that aligns with the real target preference \(_{g}\) implied in demonstrations \(_{_{g}}\). More details about implementation can be found in Appendix A.5.

### Extension to Constrained Settings

Then, we consider a natural extension of our MORL adaptation framework to constrained settings. Under the linear preference setting, the constrained MORL problem in Eq. (1) can be converted to its dual form with zero duality gap [Paternain et al., 2019]:

\[_{ R^{K}_{+}}_{}_{}_{t}_{g}^{}_{t}-^{}(_{t}_{t}- _{g}).\] (8)

Denoting \(^{*}\) as the solution of this problem, the dual problem 8 can be rewritten as:

\[_{}_{}_{t}[_{g}^{},^{}][_{t}^{},-_{t}^{}]^{ }.\] (9)

This formulation means that the constrained MORL problem under safety threshold \(\) is uniquely equivalent to an unconstrained problem of finding the optimal policy under an extended preference \(}_{g}=[_{g}^{},^{}]^{ }/\|[_{g}^{},^{}]^{}\|_{1}\) among extended \(N+K\) objectives \(}=[_{t}^{},-_{t}^{}]^{}\). However, solving for the extended preference is challenging and requires the real safety thresholds, which are inaccessible in our setting. Nevertheless, Section 4.2 provides an approach to infer the target preferences from the demonstration set, helping us circumvent this challenge. Therefore, we can convert a constrained MORL problem to an unconstrained MORL problem, where the vector reward is defined as \(}=[_{t},-_{t}]\) and dataset \(\) is augmented to \(}\). Here, \(}_{1:N}=\) corresponds to \(N\) unconstrained objectives, while \(}_{N+1:N+K}=-\) associated with \(K\) constrained objectives. One issue with this approach is how to set the behavior preference for augmented dataset \(}\). We present an effective scheme for approximating behavioral preferences in Appendix A.5.

However, one concern about this approach is the estimation error between the adapted preference \(}_{a}\) obtained through Eq. (7) and the real preference \(}_{g}\) associated with the problem 9 due to the insufficiency of demonstrations. This discrepancy can lead to constraint violations when the preference weight of the adapted preference \(}_{a}\) on a constrained objective is less than the one of \(}_{g}\) (i.e., \([}_{a}]_{i}<[}_{g}]_{i}\) for any \(i\{N+1,...,N+K\}\)). Therefore, we propose to apply a conservative estimate of preference weights on constrained objectives by neglecting the minimum value with probability \(1-\) in the adaptation distribution \(}(,)\). Specifically, the preference weights on the \(i^{}\) objective is estimated as follows:

\[_{i}=_{}([}]_{i})= _{1-}^{1}_{u}([}]_{i})u=_{i}+_{i}(1-))}{},N<i N +K,\] (10)

and \(_{i}=_{i}\) for \(1 i N\). Here, VaR is the value of risk and CVaR is the conditional one, and \((x)=}(-}{2})\) is the standard normal p.d.f., and \((x)\) is the standard normal c.d.f.. The parameter \(\) in Eq. (10) controls the conservatism of the estimation of preference weights. For all constrained objectives, \(<1\) results in a conservative estimate \(_{i}>_{i}\), reducing the risk of constraint violation. Additionally, as the number of demonstrations increases, the standard deviation \(_{i}\) decreases, and thus \(_{i}\) will move towards \(_{i}\), resulting in less conservatism. In the end, we obtain the adapted target preference \(}_{a}=/\|\|_{1}\) and the adapted policy \((s|a,}_{a})\).

## 5 Experiment

In this section, we conduct several experiments on classical MORL environments in Section 5.1 and safe RL tasks in Section 5.2 to evaluate our framework in achieving preference alignment and approaching constraint satisfaction, respectively. Then, we test our method on a set of new constrained MORL (denoted as CMORL) tasks in Section 5.3. Finally, we discuss the effectiveness of the components of our method in Section 5.4.

Environments and DatasetsIn Section 5.1, we utilize the D4MORL datasets (Zhu et al., 2023) for MORL experiments, which involve two conflict objectives and a variety of behaviors based on preferences in multi-objective MuJoCo tasks. In Section 5.2, all algorithms are trained on the datasets from DSRL benchmark (Liu et al., 2023b), which involve a constrained objective and an unconstrained objective in BulletSafeGym tasks (Gronauer, 2022). In Section 5.3, we develop a set of CMORL tasks by incorporating an additional velocity constraint to the original multi-objective MuJoCo environments and collect datasets from these tasks. Thus, these environments contain two unconstrained objectives and one constrained objective. More details about these environments and datasets can be found in Appendix A.2. We construct training sets and demonstration sets based on the above datasets and present the details on Appendix A.3.

Evaluation ProtocolsFor evaluation, we define a target set \(=\{G\}\), with each target \(G\) corresponding to a demonstration set \(_{G}\) that meets the target \(G\). For MORL tasks in Section 5.1, \(G\) is a target preference \(_{g}\). All target preferences are selected from the dataset's preference support at a fixed interval \(0.01\) (i.e., \(_{g}=[0.01k,1-0.01k],}_{g}}\) where \(k=0,..,100\) and \(}\) is the behavioral preference of the dataset). For safe RL tasks in Section 5.2, \(G\) is a safety threshold \(_{g}\). All safety thresholds are set to \(6\) equidistant points within the range of possible thresholds. The target of CMORL tasks in Section 5.3 is \((_{g},_{g})\), the Cartesian product of the target preference spaced at \(0.1\) interval and \(6\) equidistant safety thresholds.

We utilize \(_{G}\) to generate an adapted policy for each target \(G\) and gather \(5\) trajectories from environments to assess its expected vector return. For tasks without constraints, we present the average utility and Hypervolume metrics to demonstrate the overall performance across all targets and the diversity of adapted policies. For tasks with constraints, we group the adapted policies by safety thresholds and report the maximum cost return on constrained objectives and average utility and Hypervolume on unconstrained objectives for each group. We perform \(3\) runs with various seeds and report average performance along with \(1\)-sigma error bar. More details about metrics can be found in Appendix A.4.

Comparative AlgorithmsThe algorithms for comparison are divided into two categories. The first category is preference/threshold-agnostic baselines that have no access to the real target preferences or safety thresholds implied in the demonstrations, including: 1) **BC-Finetune**, where the policy is learned on training dataset via behavior cloning and is fine-tuned on demonstration sets and 2) **Prompt-MODT**, a multi-objective version of Prompt-DT  that transforms a multi-objective scenario into a multi-task problem via preference-based division and achieves offline adaptation for various tasks by taking demonstrations as prompts of the transformer. The second category is preference/threshold-informed baselines that have access to the implied preferences and thresholds and thus serve as the oracle benchmark, including: 1) original **MODF** and **MORvS** for MORL tasks, 2) **CDT** for safe RL tasks, which makes decisions based on given safety thresholds to ensure constraint satisfaction for various thresholds. For CMORL tasks, instead of adapting the preference through demonstrations, we enumerate all possible augmented preferences of MODF with a small interval and report the best performance under these preferences as the oracle. More details about comparative algorithms are placed in Appendix A.4.

### Preference Alignment for MORL Tasks

The average utility and Hypervolume of all algorithms are shown in Figure 1. Our method demonstrates superior overall performance and matches the oracle performance of preference-informed methods. Moreover, we present the Pareto fronts of different algorithms in Figure 2. Our method produces a broader and expanding Pareto front compared to BC-Finetune and Prompt-MODT, which indicates that the adapted policies of our method exhibit higher diversity and better performance. Meanwhile, we observe that the adapted policies obtained by BC-Finetune cluster around the behavior-cloned policy, and thus BC-Finetune obtains the narrow Pareto fronts and low Hypervolume, which indicates the difficulty of changing the policy's preference through simple fine-tune with limited samples. Besides, Prompt-MODT also yields a restricted Pareto front, which can be attributed to the difficulty of partitioning tasks based on preferences. Fine-grained divisions result in data insufficiency for each task, while coarse-grained divisions lead to multiple preference behaviors being grouped into

Figure 1: Results on D4MORL Amateur datasets. Higher average utility and Hypervolume are preferable. The dashed lines represent the best performance between the original MODF and MORvS.

Figure 2: Pareto fronts of different algorithms on D4MORL Amateur datasets. Each point represents an adapted policy for a specific unknown target preference.

a single task, both of which can hurt the policy performance and diversity. In contrast to BC-Finetune and Prompt-MODT, our method explicitly learns a set of policies with various preferences through MORL, thereby ensuring policy diversity. Once the target preferences are accurately identified, we can generate policies with diverse behaviors to meet various target preferences. To verify the capability of our method in preference alignment, we show the differences between the adapted preferences obtained by our method and the real target preferences in Figure 3, where we can observe a strong consistency between the adapted preferences and the real ones, especially on PDOA [MODF]. We additionally present the performance, Pareto fronts and preference comparison on the D4MORL expert datasets in Appendix A.6.1, which are consistent with the results in this section.

### Constraint Satisfaction for Safe RL Tasks

Figure 11 in Appendix A.7 presents the Pareto fronts of MODF and MORvS, i.e., the expected costs and expected rewards under various preferences, for safe RL tasks, which demonstrates that MORL algorithms can learn a variety of policies that meet various safety thresholds. Then, the reward and cost of all algorithms are shown in Figure 4. We also present the performance and Pareto fronts on other tasks in Appendix A.7. These results show that, even though the safety thresholds are inaccessible, PDOA [MODF] achieves relatively safe performance under various safety thresholds compared to other baselines, closely matching the oracle performance of the threshold-informed baseline CDT. Even with very tight safety thresholds, PDOA [MODF] can achieve constraint satisfaction or experience few constraint violations. Meanwhile, its reward performance is comparable to or even exceeds that of CDT. However, PDOA [MORvS] performs poorly on the DSRL datasets because MORvS requires accurate predictions of the target return for each preference, which is challenging due to the abundance of suboptimal trajectories in the DSRL datasets. Additionally, BC-Finetune and Prompt-MODT, which align behaviors without considering the constraints on constrained objectives, exhibit insufficient policy diversity and constraint violations in most environments.

### Evaluation for Constrained MORL Tasks

The results on CMO datasets are shown in Figure 5. The CMO tasks involve more objectives than previous MORL and safe RL tasks, posing a challenge in identifying and aligning behaviors.

Figure 4: The adapted policies’ cost and utility of each algorithm under various safety thresholds. Here, the utility is the normalized reward, since there is only one unconstrained objective in DSRL tasks. The points above the black dashed line represent the policies that violate the constraints.

Figure 3: The comparison between the real target preferences and the adapted preferences.

This is because behaviors with various preferences increase exponentially as the number of objectives increases. We can observe that in most environments, the performance of BC-Finetune and Prompt-MODT shows low Hypervolume and a weak correlation with the safety threshold, indicating that they cannot align with the desired behaviors and exhibit high policy diversity. Nevertheless, PDOA [MODF] approaches the oracle performance with high average utility, Hypervolume, and few constraint violations, which is consistent with the results presented in Sections 5.1 and 5.2.

### Ablation Study

In this section, we aim to figure out: 1) the impact of the quantity of demonstrations on adaptation performance, and 2) the influence of the conservatism parameter \(\) of Eq. (10) on safety performance. We conduct a set of ablation experiments for PDOA [MODF] on several MORL and safe RL tasks and present the results in Figure 6. In Figure 6 (a)(b), as the demonstration size (DS) increases, our method achieves better average utility, Hypervolume for MORL tasks and higher reward for safe RL tasks. Nevertheless, even with a very small number of demonstrations (DS = \(16\)), the adapted policies generated by our method still exhibit sufficient diversity and safety. Figure 6 (c)(d) demonstrate that by increasing the conservatism parameters, we can further reduce constraint violations and ultimately achieve constraint satisfaction, which validates the effectiveness of our conservatism mechanism in enhancing safety.

Figure 5: The maximum cost, the average utility and Hypervolume over all targets with a specific safety threshold.

Figure 6: The performance across different demonstration sizes (abbr. DS) in Figure (a)(b) and the performance under various conservatism parameters (abbr. CVaR) in Figure (c)(d).

Conclusion

In this paper, we present an offline adaptation framework Preference Distribution Offline Adaptation (PDOA) for constrained MORL problems where we assume no access to real target preferences or safety thresholds and only a few demonstrations with expected behaviors are available in our framework. For unconstrained MORL scenarios, we propose to 1) employ MORL methods to train a set of preference-varying policies, and then 2) align the preferences of adapted policies with expected behaviors. Furthermore, we expand our framework to accommodate constraints on specific objectives by transforming constrained problems into unconstrained counterparts. Additionally, we introduce a conservatism mechanism for preference estimation on constrained objectives to mitigate potential constraint violations. Empirical results on MORL and safe RL tasks illustrate the capability of our framework in generating diverse policies aligning with expected behaviors and approach constraint satisfaction through conservative preference estimation.

**Limitation** The PDOA framework involves multiple steps of preference sampling and gradient updates, which can lead to additional computational burden and latency during deployment. Besides, although the manual design of preferences and safety thresholds is not necessary, the process of constructing demonstrations may still involve human expertise.