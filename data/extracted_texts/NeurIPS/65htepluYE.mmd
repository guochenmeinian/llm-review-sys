# Locating What You Need: Towards Adapting Diffusion Models to OOD Concepts In-the-Wild

Jianan Yang\({}^{1,2}\)  Chenchao Gao\({}^{4,2}\)  Zhiqing Xiao\({}^{1,2}\)  Junbo Zhao\({}^{1,2}\)  Sai Wu\({}^{1,2}\)

**Gang Chen\({}^{1,2}\)**, **Haobo Wang\({}^{3,2}\)***

\({}^{1}\)College of Computer Science and Technology, Zhejiang University

\({}^{2}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security

\({}^{3}\)School of Software Technology, Zhejiang University

\({}^{4}\)International School of Information Science and Engineering, Dalian University of Technology

{jianan0115,zhiqing.xiao,j.zhao,wusai,cg,wanghaobo}@zju.edu.cn

gccgyllyp@gmail.com

Corresponding author.

###### Abstract

The recent large-scale text-to-image generative models have attained unprecedented performance, while people established _adaptor_ modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens. However, we empirically find that this workflow often fails to accurately depict the _out-of-distribution_ concepts. This failure is highly related to the low quality of training data. To resolve this, we present a framework called **C**ontrollable **A**daptor **T**owards **O**ut-of-**D**istribution Concepts (CATOD). Our framework follows the active learning paradigm which includes high-quality data accumulation and adaptor training, enabling a finer-grained enhancement of generative results. The _aesthetics_ score and _concept-matching_ score are two major factors that impact the quality of synthetic results. One key component of CATOD is the weighted scoring system that automatically balances between these two scores and we also offer comprehensive theoretical analysis for this point. Then, it determines how to select data and schedule the adaptor training based on this scoring system. The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a 33.08% decrease on the CMMD metric.

## 1 Introduction

The generative modeling for text-to-image has attained unprecedented performance most recently [37; 45; 41; 49]. Notably, by training over billions of text-image data pairs [52; 51], the family of diffusion models has allowed high-fidelity image synthesis directed by the _prompt_ provided in production. Despite their massive successes, these models still fail to generate images with decent quality and matching semantics, when encountering prompts that contain unseen or _out-of-distribution_ concept tokens [60; 12; 21]. Simply put, the reason causing this failure is due to that the training set is **not** unbounded with limited variations. On the side of production, this limitation could significantly impact the practicality of this technique in real-world applications.

To deal with such concepts, recent works have resorted to _adaptors_ such as Textual Inversion , DreamBooth [47; 58; 48], and LoRA [25; 66; 76], which tunes only a small part of the text-to-image model or insert extra modules. These adapttors largely reduce the training costs, and more importantly, preserve the visual aesthetic information originally learned by the underlying model.

However, in this paper, we have found that recent works still struggle to accurately depict the visual details of _out-of-distribution_ concepts (with a CMMD score above 3.5), as illustrated in Figure 1. Adapters like LoRA are able to accurately represent the shape and color of OOD concepts compared to the generative results before adaptor training, but they fall short when it comes to finer details such as texture, contours, and patterns. This issue arises from the fact that current studies predominantly focus on variations of in-distribution (ID) concepts (e.g., humans, dogs, and cats) while ignoring the out-of-distribution (OOD) ones. This failure motivates us to think about what makes the problem of distorted visual details happen when training adaptors.

In Figure 2, we observed that how an adaptor depicts OOD concepts can be significantly influenced by the quality of the training data. If the model is trained on samples containing disruptive objects, the resulting generative outputs are likely to reflect these disruptive elements. When the training data contains images with vague or very small instances of the OOD concept, the generative results may appear low-quality. In contrast, the high-quality data for adapting OOD concepts usually contains a single and clear object corresponding to the given concept, which is highly distinguishable from the background and other types of objects and helps produce accurate results with high-fidelity. However, manually picking such high-quality data requires much human labor and expertise, which may crucially limit the versatility of text-to-image models. Therefore, an effective paradigm to locate high-quality samples of OOD concepts is important for the practical shipping of this field.

To this end, we developed a framework called **C**ontrollable **A**daptor **T**owards **O**ut-of-**D**istribution Concepts (referred to as CATOD) which aims to identify high-quality samples to guide the adaptor training. This framework follows the Active Learning (AL) paradigm , involving iteratively accumulating training data and updating the adaptor. The profound motivation of this approach is to comprehensively model the interaction between training data and the underlying text-to-image model. Specifically, CATOD includes two interconnected scores: the _aesthetic_ score and the _concept-matching_ score, following the observation that object clarity and uniqueness largely impact generative results, as illustrated in Figure 2. Based on this, we devised a weighted scoring system that adapts itself according to the adaptor to select high-quality data while also properly balancing the two scores. With the carefully selected high-quality data, we schedule the adaptor training based on the quality of generative results evaluated through this scoring system.

In summary, our contributions are as follows: (i)-We have identified the challenge of adapting text-to-image models to out-of-distribution (OOD) concepts, where recent studies often struggle to accurately depict them; (ii)-We have introduced a framework called CATOD that iteratively updates training data and the adaptor to generate OOD concepts precisely; (iii)-Our extensive experiments verified that CATOD achieves significant performance gain with up to 11.10 on the CLIP score and 33.08% on the CMMD metric; (iv)-We have also offered theoretical insights into the key factors: _aesthetics_ and _concept-matching_, which contribute to the effectiveness of our method.

Figure 1: **Comparison of images generated before/after training adaptors over concepts with different CMD scores. One observation is that concepts with higher CMMD scores are notably more challenging for the underlying model to generate (the second row). Additionally, we also notice that a higher CMMD value leads to a more notable loss of visual details when training adaptors (the third row).**

## 2 OOD Problem in Latent Diffusion Models

**Revisiting Latent Diffusion Models (LDMs).** Latent Diffusion Models (LDMs)  comprise two components: a diffusion process operating the latent space and an auto-encoder which contains an encoder \(\) mapping an image into the latent space and a decoder \(\) that reconstruct images from latent codes. Furthermore, the diffusion process can be conditioned on the output of text embedding models, enabling the auto-encoder to integrate the information derived from texts. Let \(x\) be the image, the CLIP textual encoder \(c_{}\) that maps the corresponding text \(y\) into the latent space, the LDM loss is:

\[L_{LDM}(x,y)_{z(x),( 0,1),t}[\|-_{}(z_{t},t,c_{}(y) )\|_{2}^{2}],\] (1)

where \(t\) denotes the time step, \(z_{t}\) denotes the latent code noised at time step \(t\), when \(,_{}\) represents the noised samples and the denoising U-Net , respectively. Through this noising-denoising procedure applied to the latent codes, LDM enables the underlying model to integrate information derived from texts into the visual results, while also allowing more flexibility to produce images.

**The OOD Concepts for LDMs.** Intuitively, out-of-distribution (OOD) concepts refer to the category of data whose distribution deviates significantly from what the model has learned. This degree of drifting can be quantified by an MMD score , which evaluates the discrepancy between ground-truth images and the generative results in the image latent space. Formally, for two probability distributions \(P\) and \(Q\), the MMD distance with respect to a positive definite kernel \(k\) is:

\[_{MMD}^{2}(P,Q)_{x_{p} P,x_{p}^{} P }[k(x,x^{})]+_{x_{q} Q,x_{q}^{} Q} [k(x_{q},x_{q}^{})]-2_{x_{p} P,x_{q} Q} [k(x_{p},x_{q})],\] (2)

where \(x_{p},x_{p}^{}\) independently follow the distribution \(P\) while \(x_{q},x_{q}^{}\) independently follow the distribution \(Q\), with \(k\) the Gaussian RBF kernel . In our implementation, we sample two sets of vectors from the distribution \(P\) and \(Q\), then use CLIP embeddings  to calculate this score, which is also named as CMMD . We set a concept with a high CMMD score (above 3.5) as an OOD concept.

**OOD Concepts are Hard to Adapt.** Recent text-to-image LDMs [45; 39] have achieved unprecedented performance on a wide range of concept tokens. However, we have found that there still exist many concepts that make LDMs fail after the adapter is fully trained. As we show in Figure 1, there are several discoveries: (i)-The concepts with higher CMMD scores are much more challenging for the underlying model to generate or adapt. The concepts with a CMMD score above 3.0 show explicit wrong visual details. For Axololt and Frilled Lizards with CMMD above 4.0, the LDMs even generate the wrong species; (ii)-A higher CMMD score indicates a more severe loss of visual details when training adaptors. The concepts with CMMD scores above 3.5 in Axololts and Emperor Penguin Chicks, show explicit distorted visual details, like color, texture, and delicate details. For Axololt, the generative results show a wrong number and wrong positions of amateurs. For Emperor Penguin Chicks, the generative results show the wrong fur color of their heads and wings. To summarize, the higher the CMMD score of a concept, the more difficult it is for LDMs to adapt.

**The High-Quality Matters.** We further observe that the generative results are quite sensitive to training data when training adaptors meet OOD concepts. As shown in Figure 2, when the training images contain disruptive elements, the visual features of these disruptive elements will be easily introduced into the generative results. For example, when an adult emperor penguin appears in training data, then the black fur on their back can easily appear when generating their chicks, despite that their chicks have white fluffs as shown in the left part of Figure 2. If the object of the desired

Figure 2: **Comparison of synthetic results on data with different quality. Generated images are significantly influenced by the quality of training data. If the training data includes disruptive objects, the generative images may include disruptive visual details (_Left_). When an object within the image is too small, the results may not accurately represent the intended concepts (_Middle_). In contrast, if the image contains a high-fidelity object without disruptive elements (_Right_), the model is more likely to generate the desired result accurately.**concept appears to be too small within the image, then the generative results tend to be bad-looking, since the necessary visual details are not fully identified. In the middle part of Figure 2, we can see that distorted visual details like texture and shape in axolotl and emperor penguin chicks largely harm the aesthetics of the generated image. To generate images correctly and good-looking, we require enough amount of images with objects of high fidelity and do not contain disruptive elements, namely High-Quality samples as shown in the right part of Figure 2. Therefore, we aim to devise an effective data selection strategy for locating those high-quality images for these OOD concepts.

## 3 Method

### Overall Architecture

We aim to adapt OOD concepts to LDMs correctly by an iterative data selection criterion that locates high-quality data and training adaptors accordingly as shown in Figure 3. Consequently, our method would alleviate issues introduced by disruptive elements, _e.g._, irrelevant objects, and blurry images, while maintaining high fidelity of generative results. The core to CATOD is a scoring system, which consists of an aesthetic scorer and a concept-matching scorer, aiming to resolve the problems of incorrectly introduced visual details and distorted objects we have observed in Section 2. By properly trading off between these two scores, CATOD locates the most valuable samples for adapting underlying LDMs to OOD concepts and making the desired OOD concept depicted correctly.

### The Scoring System

As described above, we need to estimate the potential impact of real-world samples over LDMs, according to which we select the most high-quality samples for training. In Section 2, we have observed two major factors that significantly impact generated image quality: object clarity and disruptive elements. Diving into the loss term \(L_{LDM}(x,y)\) in Eq. (1), we may also observe that the training set \(_{T}\) should be optimized towards both the underlying model (including the embedded adaptor) and the conditional text \(y\), which indicates two important factors: object clarity and concept-matching achieved by preserving aesthetic information originally learned by LDMs and accurate image-text matching, respectively. Therefore, we attribute the quality of images to _aesthetic_ and _concept-matching_ and design two decoupled scorers accordingly.

**The Aesthetic Scorer.** Aesthetic evaluation is a long-standing field, which comprehensively considers whether the lighting, contrast, texture, and other photographic factors of an image are consistent with

Figure 3: The overall pipeline of CATOD. In brief, CATOD alternatively performs data selection and scheduled OOD concept adaption. In each training cycle, we first generate OOD concepts according to the current adaptor and calculate the weights for the _aesthetic_ score and _concept-matching_ score. Then, we calculate the weighted score for each sample within the data pool \(_{pool}\), select the top images accordingly, and add them to the training pool. At last, CATOD fine-tunes the scoring system and training adaptors according to the updated data pool, and proceed to the next cycle. The above three steps alternatively proceed until convergence.

human aesthetics. The general aesthetic scorer can be simply described as \(p=S_{aes}(x)\), where \(S_{aes}\) indicates the aesthetic scoring model, \(p\) denotes the predicted score, and \(x\) represents the input image. Following the work of PA-IAA , we fine-tune the generic aesthetic model and make it adapt to OOD concepts with personalized preference scores. In general, we assign a high score to the samples within the training set \(_{T}\), assign a low score to samples from irrelevant categories and samples generated by underlying LDMs without an adaptor, and use them to fine-tune the aesthetic scorer. More details of this personalization are given in the Appendix B.3.

**The Concept-Matching Scorer.** Intuitively, concept-matching describes whether the OOD concepts get perfectly reflected in the generated results. A similar task is image retrieval, which is designed to retrieve images containing objects describing the desired concepts. Since image retrieval also relies on feature maps and some sort of matching score to retrieve images, we adopt matching score from VLAD-related works [28; 64; 55] as follows:

\[S_{con}(x)=_{T}|}_{k=1}^{|_{T}|}(r_ {x},r_{k})(\|r_{x}-r_{k}\|),\] (3)

where \((r_{x},r_{k})=1\) if \(r_{k}\) is the closet representation for \(r_{x}\) and is set to 0 otherwise. Simply put, VLAD regards the extracted representations for \(_{T}\) as a codebook and maps each image to its nearest code. Note that samples within \(X_{T}\) mostly consist of clean and clear samples, this score is sufficient to quantify whether the object if exists in the given image distinguishes itself from other photographic components and matches the given concept.

### Active Data Acquisition

**Optimization with Active Learning (AL).** Aiming to mitigate the problems caused by low-quality training samples, we proactively integrate the training data \(_{T}\) into our objective as follows:

\[A^{*},_{T}^{*}=_{A,_{T}}_{x_{T}}L_{LDM}(x,A,y).\] (4)

Since the optimal set is initially unknown, a one-step optimization can easily lead to convergence to local optima. Therefore, we use an iterative paradigm to optimize adaptor \(A\) and training data \(_{T}\), respectively. In our implementation, we adopt the paradigm of AL [44; 54] to perform the optimization of \(_{T}^{(t)}\) by data accumulation before training adaptors, with \(t\) denotes the time step:

\[^{(t)}=*{arg\,min}_{_{ pool}-_{T}^{(t-1)},||=b}_{x_{T}^{(t-1)}  b}L_{LDM}(x,A,y),\] (5)

where \(b\) is the number of samples added to the training pool at each cycle. The main reason for using AL is its preferred sample efficiency, with better controllable data bias management [15; 50]. Instead of repeatedly selecting data from the whole real-world data pool, AL provides a more efficient procedure to optimize training data by using data selection to accumulate high-quality training data. To this stage, the learning procedure of CATOD is relatively clear: the sample pool is progressively accumulated -by Eq. (5) -and the optimization of the adaptor \(A\) is straightforward (Fig. 3).

Remember that AL involves iteratively updating the adaptor and the training data, it is important to design a training schedule for adaptors and determine how to acquire high quality based on the two scorers mentioned above. The primary objective of this design is to achieve a dynamic trade-off between the two scores. The specific details of these designs are described below.

**The Active Schedule for Training Adaptors.** The training schedule has been found crucial for successful adaption [47; 74; 65]. Since our training data continuously expands as the learning cycle of AL proceeds, the training schedule will be even more important. To arrange this schedule, we first calculate the aesthetics score \(_{aes}(A)=(_{T})|}_{x_{g} g_{A}( _{T})}S_{aes}(x_{g})\) and concept-matching score \(_{con}(A)=(_{T})|}_{x_{g} g_{A}( _{T})}S_{con}(x_{g})\) of the adaptor \(A\) based on its generative results \(g_{A}(_{T})\), in which both the aesthetic score \(_{aes}(A)\) and \(_{con}(A)\) range from 0 to 10. Then, we use a trigonometric indicator that comprehensively measures its performance:

\[(A)=10(_{aes}(A))(_{con}(A)).\] (6)

Notably, \((A)\) peaks when the two scores get close or around the common value of 5.0 for most samples. Meanwhile, it bottoms when the scoring exhibits a stronger signal of being biased (to either side). The properties emphasize that adapters balancing the two factors are of better quality. In training CATOD, we use \((A)\) as a signal to reduce the learning rate and stop training in time.

**Trading-off the Two Scores in Data Acquisition.** After getting the adaptor \(A\), we continue to select the most suitable samples for the next-cycle training. Notice that whether newly selected images enhance the adaptor depends on both aesthetics and concept-matching, we ought to adjust our preferences according to the adaptor. In more detail, when the adaptor has high aesthetics but does not accurately depict the OOD concept, samples with high concept-matching scores better enhance the adaptor; when the adaptor fails to exhibit photographic attributes consistent with humans, samples with high aesthetics are preferred. To implement this preference, our acquisition score is:

\[S(x)=(1-(_{acs}(A)))S_{acs}(x)+ (1-(_{con}(A)))S_{con}(x).\] (7)

This formulation offers some meritable advantages. On one hand, the balancing terms are not pre-fixed or manually tuned, but dynamically dependent on the scores marginalized over the current generations. Further, when the score of either side gets larger, the corresponding balancing coefficient, in turn, decreases, thus attaining a proper trade-off. As a result, this mechanism encourages an alternation of sample selection towards both scoring metrics by selecting Top-\(K\) samples to add to the training data \(_{T}\) accordingly, which effectively guarantees the sample set diversity.

## 4 Theoretical Insights

This section presents our theoretical analyses of why aesthetic/concept-matching scores work in OOD Adaption. Specifically, we derive the distance between real-world data distributions and synthetic data distributions and then induce the important factors that affect this distance. We first introduce the _minimum mean square error_ (MMSE) [22; 11; 6] to measure the discrepancy between distributions:

**Definition 4.1**.: The _minimum mean square error (MMSE)_ of estimating an input random vector \(}^{n}\) from an observation/output \(^{k}\) is defined as

\[(}|)=_{f(^{n})}[\|}-f()\|^{2} ],\] (8)

in which \((^{n})\) denotes the space consisting of all measurable functions on \(^{n}\).

Notice that we are trying to produce the best generation results which are initially unknown, our adaption task can be also regarded as estimating the optimal distribution with carefully selected data. By denoting the ideal generative results and the ideal adaptor with random variables \(}_{G}\) and \(A^{*}\), respectively, we conclude that the LDM loss \(L_{LDM}\) is consistent with this MMSE term:

**Theorem 4.2**.: _Let \(L_{LDM}\) be the LDM loss following Eq. 1, and the image space lies within \(^{n}\). Then there always exists an ideal random vector \(}_{G}^{n}\) and an adaptor \(A^{*}\) for LDM, such that_

\[*{arg\,min}_{_{T}^{n}}( }_{G}|_{T})=*{arg\,min}_{_{T}^{n}}_{x_{T}}[L_{LDM}(x, A^{*})].\] (9)

Based on the preceding deduction, we have confirmed that the change in MMSE can also indicate the change in LDMs. To dive deep into the MMSE term, we further decompose it as follows:

**Theorem 4.3**.: _(Pythagorean Theorem for MMSE .) Following Theorem 4.2, by setting \(f\) to the generative model \(g_{A}\), the MMSE term in Eq. (8) can be decomposed into two terms as follows:_

\[[\|}_{G}-g_{A}(_{T})\| ]=[\|}_{G}-g_{A^{*}}(_ {T})\|]+[\|g_{A^{*}}(_{T})-g_{A}( _{T})\|],\] (10)

in which \(A^{*}\) denotes a potential ideal adaptor containing information for text \(y\). Notice that the generative results \(_{G}\) relies on the training set \(_{T}\), \(g_{A}(_{T})\) can also be written as \(_{G}|_{T}\).

Upon revisiting the two terms, we can gain a deeper understanding of the relationship between MMSE and aesthetic/concept-matching score: (i)-The first term is focused on estimating the difference between the generative distribution \(_{G}|_{T}\) and the ideal distribution of \(}_{G}\). This assessment helpsus understand how the introduced samples lose visual information within the underlying LDM, thus we can connect this term to aesthetic preservation, i.e. aesthetic score; (ii)-The second term illustrates the degree to which the training set \(_{T}\) distorts the OOD concept information within the ideal adaptor. Hence, a concept-matching score accurately portrays how newly given samples impact this term. At this stage, we have completed the theoretical support showing that both aesthetics and concept-matching are major factors that influence the performance of adaptors.

## 5 Experiments

In this section, we present the main experimental results both qualitatively and quantitatively. To evaluate our proposed CATOD, we combine it with several works for adaption, i.e. DreamBooth , Textual Inversion , and LoRA . More experimental results can be found in the Appendix. _The source code is attached in the Supplementary._

**Datasets.** We test our method on datasets with 25 OOD concepts that can hardly be generated through prompt engineering on the text-to-image model. This dataset consists of 5 categories: insect, lizard, penguin, seafish, and snake, and each category contains data from 5 OOD concepts. Each concept has 1,000 examples in total with 100 samples left out for validation. The dataset is collected from publicly available datasets including ImageNet, iNaturalist 2018 , IP102 .

**Implementation Details.** We conduct the active generation experiments on our proposed CATOD and three representative adaptors, i.e. DreamBooth , Textual Inversion  (termed as TI in the paper), and LoRA . Since there are currently no available studies that focus on locating "high-quality" samples for training, we apply random sampling (RAND), and CLIP-score-based sampling (CLIP) for each baseline in our active learning setting. Each experiment starts with 20 randomly sampled instances, and we conducted 5 cycles of data accumulation in which we selected 20 "good" samples to add to the training pool. We train 20 epochs for all combinations of adaption techniques and sampling strategies in each active learning cycle, with a batch size of 1. Furthermore, we generate 100 images for each concept for evaluation. We use the commonly adopted Stable Diffusion 2.0 pre-trained on LAION-5B  following Rombach's work .

**Evaluation Metrics.** We evaluate the quality of our generated images with the widely used CLIP score  and the recently proposed CMMD score , which quantify the model performance in two aspects. Specifically, the CLIP score measures how generated images match the given text, which is expected to be as high as possible. Meanwhile, the CMMD score evaluates the discrepancy between generated images with the real ones, indicating better generative results with lower values.

### Single-concept Generation Results

To evaluate the performance of CATOD on OOD concepts, we test it on all 25 target concepts one by one separately and report the average performance of 5 concepts within each category in Table 1. We show the superior results of our CATOD with respect to both qualitative and quantitative comparisons.

**Qualitative Comparisons.** We qualitatively compare CATOD with other sampling strategies according to their generated images based on LoRA . With random sampling (RAND), we observe that the generative results only partially learned some photographic attributes like color and texture,

    & ^{}\))} & ^{}\))} \\  & insect & lizard & penguin & seafish & snake & Avg. & Imp. & insect & lizard & penguin & seafish & snake & Avg. & Imp. \\  DreamBooth + RAND & 65.35 & 67.89 & 68.07 & 66.59 & 72.07 & 67.79 & 67.58 & 1.35 & 1.39 & 1.67 & 1.59 & 1.25 & 1.45 & 00.50 \\ DreamBooth + CLIP & 70.56 & 72.19 & 72.09 & 71.71 & 74.59 & 72.12 & 13.25 & 1.04 & 1.24 & 1.05 & 1.38 & 1.16 & 1.17 & 01.02 \\ DreamBooth + CATOD & **72.18** & **75.30** & **75.16** & **74.00** & **76.14** & **75.37** & **0.92** & **1.08** & **0.80** & **1.21** & **0.74** & **0.95** & - \\  TI + RAND & 58.24 & 59.34 & 63.45 & 61.35 & 62.34 & 60.94 & \(\)70.4 & 2.45 & 2.15 & 2.09 & 1.95 & 1.65 & 2.06 & 0.63 \\ TI + CLIP & 61.65 & 67.24 & 66.95 & 62.27 & 64.23 & 64.47 & 43.51 & 2.11 & 1.86 & 1.44 & 1.87 & 1.37 & 1.73 & 10.30 \\ TI + CATOD & **69.21** & **70.14** & **68.23** & **64.99** & **67.41** & **67.98** & - & **1.57** & **1.73** & **1.15** & **1.43** & **1.25** & **1.43** & - \\  LoRA + RAND & 64.39 & 65.02 & 67.49 & 68.87 & 71.09 & 67.37 & \(\)10.60 & 1.52 & 1.47 & 1.56 & 1.61 & 1.18 & 1.47 & 00.63 \\ LoRA + CLIP & 70.27 & 74.13 & 72.08 & 73.19 & 75.64 & 30.66 & \(\)49.1 & 1.29 & 1.33 & 1.04 & 1.35 & 0.89 & 1.18 & 0.34 \\ LoRA + CATOD & **72.60** & **77.00** & **74.11** & **84.29** & **81.86** & **79.77** & - & **0.94** & **0.89** & **0.71** & **0.88** & **0.77** & **0.84** & - \\   

Table 1: A Comparison over the performance of CATOD, in terms of the CLIP score and CMMD score with 100 images sampled at last. This table shows the average result of 5 sub-classes within each category. The overall improvement of our proposed CATOD is provided by “Imp.”. Methods with the best performance are bold-folded.

but failed to make the objects have the correct appearance and shape. CLIP-based sampling (CLIP) somehow produces the corrected shape of the concept but still fails to capture the necessary details for describing the object. In comparison, our proposed CATOD successfully matches all the photographic attributes, while also guaranteeing the image aesthetics.

To further look at how CATOD learns the photographic attributes that precisely match the concept, we show samples generated from different cycles in Fig. 7. We can see that some attributes like color, and texture are already learned in cycle 1, but the shape of the object does not match the ground-truth ones. From cycle 1 to 3, CATOD shows a clear shape modification, making the objects more like the real ones. From cycles 3 to 5, an iterative refinement on more photographic details like light, contrast, and other minor modifications (like the beak for penguins and antenna for axolots) is shown in generative results, making them hard to distinguish from the ground-truth ones even with a careful look. At cycle 5 and later cycles, the image quality stabilizes and we can hardly see enhancement apart from image diversity. To conclude, we can see an explicit attribute process from easy ones to the finer ones within CATOD, showing the importance and effectiveness of iterative training.

**Quantitative Comparisons.** Table 1 reports CLIP scores  and CMMD scores  from each strategy with models trained on 25 different OOD concepts and evaluated through the generative results. Specifically, we evaluate the performance of CATOD on each concept and average the results within each category in Table 1. We have the following conclusions: (1) the average performance of our strategies outperforms all compared methods by a 0.56\(\)11.10 CLIP score, justifying the effectiveness of locating aesthetic and clean samples over text-image matching. (2) CATOD also brings a 0.12\(\)0.54 CMMD decrease on various frameworks and concepts, indicating better image alignment with proper sampling guided by CATOD. To summarize, both image-matching and text-matching scores exhibit better results compared to the baselines, suggesting that our proposed CATOD significantly outperforms other strategies, which is consistent with the qualitative results in Fig. 4.

    &  &  \\  & insect & lizard & penguin & seatfish & snake & Avg. Imp. & insect & lizard & penguin & seatfish & snake & Avg. Imp. \\  DreamBock(47) + RAND & 63.29 & 63.79 & 65.72 & 66.59 & 64.36 & 64.55 & 68.20 & 1.74 & 2.14 & 2.16 & 2.02 & 1.85 & 1.98 & 0.75 \\ DreamBock(47) + CLIP & 67.57 & 70.35 & 71.10 & 69.34 & 70.38 & 69.73 & 53.00 & 1.53 & 1.76 & 1.80 & 1.79 & 1.59 & 1.69 & 0.40 \\ DreamBock(47) + CLIP & **70.83** & **72.88** & **74.31** & **79.00** & **75.45** & **72.75** & **1.39** & **1.25** & **1.34** & **1.45** & **0.73** & **1.23** & - \\  IT + RAND & 59.23 & 56.97 & 57.90 & 60.83 & 62.65 & 59.52 & 58.88 & 2.84 & 2.56 & 2.27 & 2.39 & 2.41 & 2.49 & 0.83 \\ IT + CLIP & 61.74 & 60.72 & 63.79 & 62.71 & 65.71 & 62.93 & 21.47 & 2.26 & 2.08 & 1.94 & 1.97 & 2.23 & 2.10 & 0.44 \\ IT + CATOD & **64.48** & **63.33** & **65.93** & **65.44** & **67.24** & **65.40** & **-1.76** & **1.53** & **1.65** & **1.64** & **1.73** & **1.66** & - \\  LaR + RAND & 63.85 & 65.27 & 66.66 & 68.25 & 69.43 & 66.65 & 67.64 & 1.79 & 2.05 & 1.91 & 1.85 & 1.55 & 1.83 & 10.59 \\ LaR + CLIP & 69.25 & 70.84 & 71.39 & 71.91 & 72.78 & 71.23 & 63.06 & 1.40 & 1.69 & 1.74 & 1.59 & 1.28 & 1.54 & 0.30 \\ LaR + CATOD & **71.19** & **74.69** & **73.68** & **75.60** & **76.90** & **74.29** & - & **1.13** & **1.37** & **1.49** & **1.26** & **0.95** & **1.24** & - \\   

Table 2: A Comparison over the performance of CATOD when training with images from multiple concepts, in terms of the CLIP score and CMMD score with 100 images sampled at last. In each experiment, we sample images from all the sub-classes within each category and check whether the fine-tuned model can generate all 5 concepts. The overall improvement of our proposed CATOD is provided by “Imp.”. “Methods with the best performance are bold-folded.

Figure 4: **A comparison of different sampling strategies with LoRA.** Specifically, we compare three lines of works: (1) RAND, in which the model is trained with 100 randomly selected samples; (2) with samples of the highest CLIP scores (100 samples); (3) 100 samples with CATOD. The model trained with randomly sampled data fails to capture the features of out-of-distribution (OOD) concepts, while the ones trained with top CLIP scores contain necessary details but also include disruptive elements.

### Multi-concept adaption Results.

To further investigate whether CATOD could adapt multiple concepts simultaneously, we group the 25 concepts by category and train the adaptation model on each category. To be specific, we compare our baselines using a generated set consisting of 500 images (100 images per concept) and exhibit our results in Table 2. Following the setting of single-concept adaption, we conduct 10 cycles of data accumulation and get 200 samples at last, since multi-concept adaption requires more data. We still observe a notable performance gain with a 1.56\(\)5.07 CLIP score increase and a 0.12\(\)0.86 CMMD score decrease compared to baselines. These results verify that our CATOD still achieves better performance on multi-concept adaption.

### Ablation Studies

We verify the efficacy of all components in our proposed CATOD in Table 3 and 4, including the aesthetical scoring module and the concept-matching mechanism within the weighted scoring system.

**W/O Aesthetic Score.** First, we validate the effectiveness of CATOD by removing the aesthetic scores. A significant decrease in performance (up to 8.55) on the CLIP score can be observed in Table 3. We attribute this decline to the fact that matching-based metrics prioritize image representations over the given concept, leading to a deterioration in image-text matching.

**The Type of Aesthetic Scores.** To further investigate the impact of aesthetic scores, we have also applied different types of aesthetics with CATOD in Table 4. We observed that recent state-of-the-art aesthetic evaluations did not improve OOD adaption and even led to minor performance loss. This could be attributed to the fact that these models were originally designed for general aesthetic assessments, whereas our aesthetic scorer is highly personalized towards specific OOD concepts.

**W/O Concept-Matching Score.** After removing the concept-matching scorer, we observed that the adaptor tends to perform better compared to just removing the aesthetic scorers. This might be due to the aesthetic scorer being designed based on CLIP backbones, showing some consistency with the CLIP score. However, it still demonstrates limitations based on the image-matching score CMMD. While these aesthetic qualities partly describe the clarity and accuracy of the object, they do not adequately focus on the image representation space.

   Type of &  &  \\ Aesthetic Score & lizard & penguin & lizard & penguin \\  ReLIC  & 71.35 & 72.39 & 1.29 & 1.59 \\ TANet  & 72.05 & 72.67 & 1.32 & 1.35 \\ BAD  & 73.08 & 72.79 & 1.18 & 1.37 \\ Ours & **73.19** & **72.85** & **1.14** & **1.28** \\   

Table 4: **Results of CATOD with different types of aesthetic scorers.** We show the average results conducted on the categories “penguin” and “lizard” with LoRA.

Figure 5: **Generative results as cycle proceeds. Samples are generated with CATOD on cycles from 1 to 7. To better observe how generated images change as the cycle proceeds, we conduct another 2 cycles here. In each cycle, we select and add 20 high-quality samples. Generative samples start to converge and contain the right details within the original concept after cycle 4 or 5. We can also see that those generative results contain diverse contents within the background based on the few images given.**

    & Modules &  &  \\  Aesthetic & Concept & Weighted & lizard & penguin & lizard & penguin \\  ✓ & & & 73.19 & 72.85 & 1.14 & 1.28 \\  & ✓ & & 68.45 & 70.16 & 1.10 & 1.32 \\ ✓ & ✓ & & 75.35 & 73.24 & 0.94 & 0.93 \\ ✓ & ✓ & ✓ & **77.00** & **74.11** & **0.89** & **0.71** \\   

Table 3: **Results of Ablating Aesthetic, Concept-Matching Scorer and Weighted Scoring on CATOD.** We show the average results conducted on the categories “penguin” and “lizard” with LoRA.

The Weighted Score.We have confirmed the effectiveness of balancing two scores by simply adding them together to guide CATOD (Table 3, Line 5). We observed that this resulted in consistent performance loss for both the CLIP score (up to 1.65) and the CMMD score (up to 0.22). This suggests that both text-image matching and image-to-image matching are affected by the trade-off between aesthetics and concept matching, further emphasizing the importance of these two scores.

## 6 Related work

Personalized Text-to-image Synthesis with Adaptors.The task of text-to-image generation involves creating specific images based on text descriptions [3; 73; 79; 4], and has achieved impressive performance with state-of-the-art diffusion models [41; 45; 39]. Therefore, _adapting_ large-scale text-to-image models to a specific concept while also preserving this amazing performance, i.e. _personalization_, has become another recent research interest. But this is often difficult since re-training a model with an expanded dataset for each new concept is prohibitively expensive while fine-tuning the whole model [12; 32] or transformation modules [83; 20; 60] on few examples typically leads to some rate of forgetting . Therefore, _adaptors_ such as Textual Inversion , DreamBooth [47; 48; 5; 58], LoRA [25; 66; 86; 76], along with some other works [70; 19] derived from them, have become more commonly adopted. Typically, they focus on a small but crucial part of the model or extra networks inserted into underlying models, thus more computationally-efficient, while also preserving the efficacy of the underlying models with lower computational costs. For example, _textual inversion_ (TI)  represents the newly-given concept with pseudo word  and remapping it to another carefully trained embedding in the text-encoding space, guided by few images. Despite their computational efficiency, these approaches are still facing difficulties dealing with out-of-distribution concepts as we observe in Section 2.

Active Learning and Selection.Active Learning is a machine learning paradigm that involves actively selecting the most suitable data for training models from external data sources [44; 63]. The most crucial part of active learning is the strategy to locate the optimal data batch. Current studies can be roughly categorized as follows: (a) Score-based methods that prefer the samples with the highest information scores [36; 69; 10]; (b) Representation-based methods searching for the samples that are the most representative of the underlying data distribution [53; 1; 62]. The Active Selection paradigm within Active Learning serves as an efficient and powerful dataset curation tool (i.e. _adaption_), leading to numerous studies adopting this paradigm from a wide range of subjects [68; 24; 8]. Meanwhile, due to the effectiveness of Active Learning in selecting the most suitable training samples, recent studies have utilized similar sampling strategies to address challenges associated with long-tailed distributions [56; 57] and noisy data . Given that Out-Of-Distribution (OOD) concepts in Latent Diffusion Models (LDMs) often involve unseen or long-tailed concept tokens , this motivates us to leverage Active Learning for selecting samples that are well-suited for training adaptors.

In this work, we focus on scored-based strategies in two-fold: _aesthetics_ and _concept-matching_. Image Aesthetics Assessment (IAA) aims at evaluating image aesthetics computationally and automatically , while automatically assessing image aesthetics is useful for many applications [35; 29; 14]. To take a step further, personalized image aesthetics assessment (PIAA) [43; 85; 75] was proposed to capture unique aesthetic preferences, consistent with our goal to adjust our paradigms accordingly different concepts. At the same time, "Concept-matching" is adopted from the field of image retrieval, in which we search for relevant images in an image gallery by analyzing the visual content (e.g., objects, colors, textures, shapes _etc._), given a query image [61; 31]. To design a paradigm that automatically meets the harsh requirements for adaptor training, we consider both two factors.

## 7 Conclusion

We propose CATOD, an enhanced, data-efficient, and practically useful version of the OOD concept adaptation for AIGC. This method is encapsulated in an active-learned paradigm with carefully designed acquisitional scoring mechanisms. CATOD significantly outperforms the prior approaches in many (if not most) aspects including generation quality, concept matches, technological robustness, data efficiency, etc. In the future, we hope to ship CATOD to the open-source community so as to absorb more OOD concepts that were originally uncovered.