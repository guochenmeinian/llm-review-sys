# Tree of Attributes Prompt Learning for

Vision-Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the textual category name. To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a "concept - attribute - description" structure for each associated category name, and then learn the hierarchy with vision and text prompt tokens. Unlike existing methods that merely augment category names with a set of unstructured descriptions, our approach essentially distills structured knowledge graphs associated with class names from LLMs. Furthermore, our approach introduces text and vision prompts designed to explicitly learn the corresponding visual attributes, effectively serving as domain experts. Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images. To address this misalignment, we further introduce a vision-conditional pooling module to extract instance-specific text features. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods on the zero-shot base-to-novel generalization as well as few-shot classification across 11 diverse datasets.

## 1 Introduction

Recent advancements in vision-language models (VLMs) like CLIP  and ALIGN  merge the capabilities of visual perception with linguistic understanding, which have revolutionized the landscape with their zero-shot learning abilities. They proficiently handle tasks on unseen data, bypassing the conventional requirement for task-specific training. This feature has enabled a plethora of applications, ranging from content-based image retrieval to complex visual question answering, setting new benchmarks in the domain. A crucial development in this domain is the concept of prompt learning, which has significantly influenced both natural language processing (NLP)  and vision-only models . This approach leverages learnable prompts to guide model understanding, tailoring responses to specific tasks or datasets.

Prompt learning, particularly in vision-language models, has garnered considerable interest due to its parameter efficiency and rapid convergence . Techniques like CoOp  optimize learnable continuous prompts for few-shot image recognition, enhancing model performance significantly. Recent efforts have expanded to multimodal prompt learning, optimizing prompts in both visual and language domains . Despite their success, these models rely on simplistic text prompts, typically formatted as "a photo of a {class}", illustrated in Fig. 1 (a). While functional, this approach lacks depth, failing to encapsulate the intricacies and finer details inherent invisual data. Such limitations hinder the model's ability to fully leverage the rich, descriptive potential offered by more detailed and contextually relevant textual information.

In parallel, another stream of research has been exploring the utilization of large language models (LLMs) to generate more elaborate and descriptive text prompts for enhancing zero-shot learning capabilities [26; 32; 35; 17; 30; 48; 49; 36; 52; 40]. These LLM-generated descriptions offer a wealth of detail and context, potentially enriching the model's interpretative capabilities. However, current methodologies in integrating these descriptions often do not exploit the full potential of this richness. As shown in Fig. 1 (b), most of these approaches lack a structured framework to organize and utilize these descriptions effectively, leading to a scattergun approach where not all generated descriptions are contextually relevant or optimally aligned with the visual content. In addition, as noted in , descriptions generated by such paradigms are usually diverse, which covers most possibilities of the class, but include descriptions that are either likely not co-occurring, e.g. "steamed" and "fried", or absent in the input image, e.g. "long tail" for a cat shot from the front, necessitating the need for a selective pooling mechanism for clearer image-text alignments.

In response to these challenges, our work introduces "Tree of Attribute Prompt learning (TAP)," a method that redefines the integration and utilization of detailed descriptions within VLMs. As indicated in Fig. 1 (c), unlike existing methods that merely augment category names with a set of unstructured descriptions, our approach essentially distills structured knowledge graphs associated with class names from LLMs. Specifically, we adopt a hierarchical, tree-like structure to systematically generate and integrate descriptions, ensuring a layered and comprehensive understanding of visual content. Each branch of this tree represents a specific attribute, with finer details fleshed out in the subsequent leaves, ensuring that every aspect of the visual content is captured and represented. Furthermore, we reimagine the learnable prompt tokens as "domain experts", each specializing in different aspects of the image, supplemented by the CLS token's global perspective. In addition, we introduce vision-conditional layers for each expert-attribute pair, which pool the most applicable descriptions from each of the attribute sets with condition on the input image content, ensuring optimal image-text alignment. This setup not only provides a detailed, attribute-focused analysis but also harmonizes these insights with the overall context.

Extensive experiments in both base-to-novel generalization and few-shot classification across 11 diverse datasets demonstrate the effectiveness of our method. On base-to-novel generalization, TAP achieves average performance gains of \(1.07\%\) in harmonic mean over the state-of-the-art methods, and \(9.34\%\) over the vanilla CLIP. Competitive results are also observed in few-shot classification.

## 2 Related Work

**Prompt Learning for Vision-Language Models.** Prompt learning bridges linguistic understanding and visual perception by guiding VLMs with text prompts, a concept originated in NLP [20; 21; 22] and adapted to vision-only [14; 43; 44; 51] and multimodal contexts[54; 53; 15; 16; 38; 19; 40; 34; 36; 52; 55; 4; 23]. In the textual domain, CoOp  optimizes learnable continuous prompts in CLIP's language branch for few-shot image recognition, while CoCoOp  addresses CoOp's

Figure 1: Illustration of the methods for CLIP text prompts formation. (a) Manually created prompt with the single “a photo of a {class}” template; (b) A unstructured set of detailed descriptions generated by LLMs; (c) The proposed Tree of Attribute that organizes the descriptions in a “concept - attribute - descriptions” structure, essentially distilling knowledge graphs from LLMs; (d) An example Tree of Attribute for “dumplings”.

overfitting issues by conditioning prompts on visual features. In the visual domain, Visual Prompt Tuning (VPT)  and Dual-modality Prompt Tuning (DPT)  enhance CLIP's vision encoder by learning visual prompts in pixel space and dynamically generating prompts through cross-attention, respectively. TransHP  leverages category hierarchy for prompt learning to improve classification performance. LoGoPrompt  enhances classification by incorporating synthetic images with class name text as auxiliary visual prompts. MaPLe  explores multimodal prompt learning, jointly optimizing prompts in both vision and language branches. Other recent works have focused on regularizing prompt learning to leverage the knowledge from base VLMs effectively, demonstrating enhanced generalization in varied downstream visual tasks [16; 4; 36]. PromptSRC, for instance, introduced a self-regulating method that restricts both the vision and text prompt, demonstrating improved generalization. Distinct from these approaches, PLOT  and ALIGN  leverage Optimal Transport to align multiple prompts with local visual features, either from the multi-head self-attention layer or at a token level. Our work diverges from these methods by introducing a hierarchical "Tree of Attribute" framework derived from LLMs to structure textual descriptions and guide the learning of specialized "domain expert" tokens for attribute-level understanding.

**Image classification by descriptions.** There's a growing emphasis on using visual descriptions for zero-shot recognition, moving beyond generic prompts [54; 53]. These descriptions, like the "fur pattern" or "tail shape" of a cat, provide fine-grained and distinctive characteristics. The use of LLMs like GPT-3 , allows for efficient generation of a broad spectrum of class-specific descriptions, offering an advantage over manually crafted templates. While this approach has been extensively researched in zero-shot contexts [17; 26; 30; 35; 48; 49; 10; 32; 28], its application in conjunction with prompt learning for few-shot tasks remains relatively unexplored[25; 19; 40; 52; 50]. Previous methodologies, however, have largely utilized unstructured descriptions, lacking an organized framework for effective utilization. Our approach diverges by structuring these descriptions into a "Tree of Attribute" model, coupled with learnable visual prompts as domain experts. Additionally, LLM-generated descriptions often cover a wide range of potential class descriptions, of which not all may be pertinent to a given image, pointing to the need for a selective pooling mechanism to ensure optimal image-text alignment. We further introduce a vision-conditional pooling layer for refined image-text alignment. This structured approach not only enhances the interpretability of the model's learning process but also significantly improves alignment accuracy between image content and descriptive text.

## 3 Methodology

### Preliminary

**CLIP.** Our approach is built on the pre-trained vision-language model, CLIP . Formally, let \((x,c)\) denote the dataset, where \(x\) is an image and \(c\{1,,C\}\) are the class labels. For an image \(x\), the vision encoder \(h_{I}()\) transforms it into a feature vector \(^{v}_{x}=h_{I}(x)\). Simultaneously, each class label \(c\) is mapped to a text prompt \(t_{c}=}\), and converted into textual feature vectors \(^{t}_{c}=h_{T}(t_{c})\). The predicted class \(\) is given by:

\[=*{argmax}_{c}(^{v}_{x},^{t}_{c}) \]

where \(()\) denotes cosine similarity.

**Image classification with class descriptions.** To improve the model's understanding of the categories in the transfer datasets, previous works [26; 35] use more detailed descriptions from Large Language Models (LLMs) instead of the simple "a photo of a {c}" to prompt the CLIP text encoder. Under this approach, a convoluted set of descriptions is generated for a class \(c\) as \(_{c}:\) {"c, which is/has/etc description." }, e.g. c="television" and description="black or grey". This classification is reformulated as

\[=*{argmax}_{c}_{c}|}_{d _{c}}(_{}(x),_{}(d)) \]

### Overall Framework

We rethink the descriptions by LLM \(_{c}\) as nodes in knowledge graphs. While previous methods generate an unstructured set of descriptions, we distill structured knowledge graphs for each class from LLM, in which the root node is the class name \(c\), capturing the highest level semantics, and the leaf nodes are the detailed descriptions capturing fine-grained details. In this framework, previous paradigms only generate the leaf nodes of the graph, with the edges and graph structure missing, where the rich and inherent structure from the descriptions is overlooked. To address this limitation, we formulate our approach as a Tree of Attribute, which follows the "concept - attribute - description" structures, as illustrated in Fig. 1 (c).

Besides weighting the descriptions equally, previous works typically align descriptions that describe images from different aspects and at different granularities with a singular CLS token from the image encoder. However, while the use of a single CLS token is effective in certain contexts, we note that the CLS token is designed to capture the global information of an input image \(x\). As a result, even though this helps to further inform global understanding, it may fail to effectively capture the nuances and variances at the attribute level. This leads to suboptimal use of the rich descriptions. We address this by introducing a set of learnable prompt tokens that serve as domain experts in the vision branch, each of which aligns with a specific attribute-level textual embedding.

Additionally, close inspection of the LLM-generated descriptions indicates limited contextual relevance and a high degree of diversity. Previous works  reflect the issue of descriptions that are likely not co-occurring e.g. "steam" and "fried". We further identify cases where the descriptions are technically correct but irrelevant to certain images, such as describing "long tail" in frontal images of cats, underscoring the need for a selective pooling mechanism. Thus, we introduce a vision-conditional pooling layer to extract instance-specific text features for each attribute for selecting the most applicable descriptions.

Overall, our approach utilizes fine-grained descriptions and organizes them in a Tree of Attribute following the "concept - attributes -descriptions" structure. Learnable vision expert tokens are appended to the input image embedding to learn from specific fine-grained attributes such as color and shape. A vision-conditional pooling layer is further added for each attribute to ensure optimal image-text alignment. Inspired by CoOP , we also incorporate textual contextual tokens in the text encoder. The overall framework is presented in Fig. 2.

### Tree of Attribute generation by LLMs

We redefine the process of integrating LLM-generated descriptions by introducing a knowledge graph \(_{c}=\{_{c},_{c}\}\) for each class \(c\), where \(_{c}\) denotes the set of nodes, and \(_{c}\) denotes the edges that capture the semantic relationship between nodes. In previous works, \(_{c}\) is the set of descriptions \(_{c}\), while \(_{c}\) is missing. We argue that such methods overlook the inherent structure among the descriptions and thus do not exploit the richness of these descriptions effectively. To better leverage knowledge from LLMs, we introduce an attribute layer to link the root node class name, and the leaf node descriptions. The attribute nodes include visual attributes generated by LLMs, such as color and shape, for systematically guiding description generation as illustrated in Fig. 1 (c). Each branch of this "tree" represents a specific attribute, with the subsequent "leaves" fleshing out the descriptions

Figure 2: Overview of the proposed TAP method. TAP utilizes fine-grained descriptions from LLMs and organizes them in a Tree of Attribute. Vision expert tokens are added to the vision encoder to learn from specific attributes such as color and shape. A vision-conditional pooling layer is introduced to ensure optimal image-text alignment. Textual context tokens are also incorporated to the textual branch, shared across descriptions.

with finer details. In this framework, \(_{c}\) includes the class name which is the root node, the set of attributes such as color and shape being the intermediate layer, and lastly the set of descriptions under each attribute node. \(_{c}\) includes the edges that build up the hierarchy. This structure allows for a nuanced representation of class information, spanning from general concepts down to specific attributes and detailed descriptions.

To this end, we introduce the Tree of Attribute (ToA), where we use a tree structure to model the relationship and structure of the descriptions. Let \(_{c}\) denote the set of attributes, and for each attribute \(a_{c}_{c}\), we denote its leaf nodes as \(_{c}^{a}\). Each set \(_{c}^{a}\) contains descriptions that specifically pertain to attribute \(a\) for class \(c\), which is denoted as

\[_{c}^{a}=\{d_{c}^{a,1},d_{c}^{a,2},,d_{c}^{a,n}\}, \]

where \(d_{c}^{a,i}\) represents the \(i\)-th description for attribute \(a\) of class \(c\) and \(n\) is the number of descriptions per attribute.

The process of generating a Tree of Attribute (ToA) unfolds in three steps: 1) **Attribute Generation:** We first query LLMs with the dataset information and ask it to generate a set of attributes \(\) which are considered relevant and characteristic of the dataset. 2) **Example Generation:** We then ask LLMs to generate descriptions for a randomly sampled class in the dataset, using the attributes \(\) identified in the previous step. Each description takes the format of "class, which {is/has/etc} {description}". Human review is performed to ensure the quality of the example. 3) **Description Generation for All Classes:** Building upon the Q&A template from the previous step, the LLM is then tasked with generating descriptions for all classes in the dataset.

Additionally, we incorporate a "global context" attribute which is aligned with the CLS token in the vision encoder. The descriptions are the 7 standard templates provided in .

### Learning TAP with Learnable Expert Tokens

To fully exploit the structured Tree of Attribute, we introduce learnable visual expert tokens \(_{a}^{v}\) in the vision branch to learn from each of the attribute nodes \(a\). Unlike traditional methods that rely on a single CLS token for alignment, these expert tokens enable focused learning on specific image attributes, such as color or shape, enhancing the model's performance and interpretability.

We denote the set of introduced visual expert tokens as \(^{v}=\{_{a}^{v}|a\}\). Akin to the idea of visual prompt tuning (VPT) , we insert \(^{v}\) into the input sequence of the vision encoder, forming the prompted input sequences \(_{p}}=\{_{},^{v},_{ }\}\), where \(_{}\) is the input CLS token, and \(_{}\) denotes the embedded patch tokens. To further boost the model's capacity for nuanced attribute representation, we employ deep prompting by introducing a zero-initialized layer residual for each prompt token across transformer layers, which provides more explicit attribute guidance across transformer layers. In parallel, we adopt a set of \(m\) learnable context tokens \(^{t}=\{_{j}^{t}|j\{1,2,...,m\}\}\) for the text encoder shared across all descriptions, similar to .

### Vision-Conditional Pooling

To mitigate issues of misalignment and potential misleading information from the broad spectrum of LLM-generated descriptions, we proposed an adaptive vision-conditional pooling layer, applicable to each set of attribute descriptions \(_{a}\) shared across all classes to dynamically pool the most applicable descriptions based on the visual content of the image \(x\) using its corresponding visual expert token denoted as \(_{a,x}^{v}\). For ease of expression, we will proceed without explicitly mentioning \(x\), though it's important to note that both the expert token and the resulting attribute-level embeddings are dependent on the visual information. Intuitively, VCP uses attention to calculate the similarity between \(_{a}^{v}\) and all embedded descriptions in attribute \(_{a}\), which are then used as weights for a weighted sum of the original description embeddings. Formally, for each attribute \(a\) and its associated expert token \(_{a}^{v}\), the pooled attribute-level embedding \(_{c}^{a}\) for class \(c\) and attribute \(a\) is:

\[ =W_{q}_{a}^{v}, \] \[ =W_{k}(_{c}^{a}),\] \[ =(^{T}),\] \[_{c}^{a} =(D_{c}^{a}),\]where \(W_{q}\) and \(W_{k}\) are learnable weights \(^{d d}\), \(()\) denotes the embedding function, and \(()\) is the Softmax function. This layer mirrors cross-attention but omits \(W_{v}\) to maintain the output within the CLIP V-L space.

### Training and Inference

**Training objective.** During training, each visual expert token \(_{a}^{v}\) is aligned with its associated attribute-level embedding \(_{c}^{a}\), trained with the following contrastive objective:

\[L_{con}(_{a}^{v},_{c}^{a})=-_{i=1}^{N} _{a}^{v},_{y}^{a})/)}{_{c=1}^{C} ((_{a}^{v},_{c}^{a})/)}, \]

where \(N\) represents the number of training samples, and \(\) is the learned temprature of CLIP. The total classification loss \(L_{}\) is the average of the contrastive loss from each expert token as well as the CLS token, defined as:

\[L_{class}=|}_{a}L_{con}( _{a}^{v},_{c}^{a})), \]

Similar to  and , we regularize the vision CLS token, text feature, and the prediction logits from each attribute using the vanilla CLIP model. We denote the regularization loss as \(L_{reg}\), where the details can be found in Appendix. The overall training objective is \(L_{}=L_{}+L_{}\).

**Prediction fusion.** During inference, we integrate the prediction by each attribute expert pair by a weighted sum, formulated as follows:

\[=*{argmax}_{c}(_{CLS}^{v}, _{c}^{CLS})+|-1}_{a \{CLS\}}(_{a}^{v},_{c}^{a}) \]

where \(\) is a hyperparameter that signifies the weight assigned to the global context provided by the CLS token, balancing its contribution with that of the attribute-specific expert prompts.

## 4 Experiments

We extensively evaluate our method in two settings: 1) Base-to-novel class generalization, where the datasets are equally split into base and novel classes. We train the model on the base classes only and evaluate on both base and novel classes; and 2) Few-shot classification with 16 shots per class.

**Datasets and baselines.** For both base to novel class generalization and few-shot setting, we follow previous works [54; 53], using 11 image recognition datasets. The datasets span a range of recognition tasks: ImageNet  and Caltech101  for generic object recognition; OxfordPets , StanfordCars , Flowers102 , Food101 , and FGVCAircraft  for fine-grained classification; SUN397  for scene recognition; UCF101  for action recognition; DTD  for texture classification; and EuroSAT  for satellite image analysis. We benchmark against several leading methods, including CLIP , CoOp , Co-CoOP , ProGrad , RPO , LoGoPrompt , and the state-of-the-art PromptSRC .

**Implementation details.** A pre-trained CLIP model with a ViT-B/16 vision backbone is used in all of our experiments and results are averaged over 3 runs. We use GPT-3.5-turbo  for attribute and description generation. We initialize the text context tokens with the word embedding of a photo of a. For both settings, we iteratively train the vision and text encoders with 5 epochs for vision and 1 epoch for text schedule. We set \(=0.4\), \(_{1}=10\), and \(_{2}=2.5\) for all datasets. We train the vision encoder for 50 and 100 epochs, and text encoder for 10 and 20 epochs for base-to-novel generalization and few-shot experiments, respectively. For DTD, Oxford Flowers, Stanford Cars, UCF101, and Caltech101 datasets, we use a learning rate of 0.002 for the text encoder and 0.006 for the vision encoder, with \(_{3}=3\). For the remaining 6 datasets, the learning rates for both text and vision encoders are set as 0.004, with \(_{3}=1.5\). We also use a Gaussian Prompt Weighting (GPA) following , with a mean of 45, std of 10 for base-to-novel generalization, and 80, 20 for few-shot experiments. Refer to the Appendix for additional implementation details.

### Base-to-Novel Generalization

In base-to-novel generalization, we equally split the classes into base and novel classes. Initial training and evaluations are conducted on the seen base classes, followed by evaluation on the unseen novel classes in a zero-shot manner. TAP surpasses prior state-of-the-art models in terms of the base and novel class accuracy, as well as their harmonic mean across most of the 11 datasets, with an average increase of 1.53% in the zero-shot novel class prediction, and a 1.07% increase in the overall harmonic mean in average, as detailed inTable 1. Notably, our method improves unseen class prediction without compromising base class performance, exhibiting an average performance boost of 0.49%. In the challenging fine-grained tasks such as DTD, EuroSAT, and UCF101, TAP achieves significant improvements in novel class prediction by 5.03%, 8.27%, and 3.63% respectively. These results underscore the robust generalizability and efficacy of our method across diverse scenarios.

### Few-Shot Classification

In few-shot classification, TAP also outperforms existing methods in 9 out of the 11 datasets. Detailed in Table 2, we achieve an average accuracy of \(83.37\) across the 11 datasets, surpassing the previous state-of-the-art methods by \(0.5\%\), further demonstrating the effectiveness of our method.

Table 1: **Comparison of TAP in base-to-novel generalization. HM: harmonic mean .**

Table 2: Few shot classification results with 16 shots.

### Ablation Study

**Effects of Tree of Attribute.** A core inquiry is whether structuring descriptions into a Tree of Attribute (ToA) offers advantages over an unstructured aggregation of LLM-generated descriptions. To evaluate, we revert to aligning a mixed, unstructured set of descriptions with the CLS token - a common practice in prior studies [25; 19; 40; 52], while keeping the same number of visual prompt tokens. According to Table 3, substituting the ToA with an unstructured set results in significant performance decreases of 1.86%, 2.31%, and 2.11% across the average base, novel, and their harmonic mean performances, respectively. This stark contrast underscores the ToA's critical role in enhancing model efficacy.

**Effects of Learning through Domain Experts.** Further, we examine the impact of substituting the CLS token with visual expert tokens for learning fine-grained attributes, commonly adopted in in previous works [25; 19; 40; 52]. Our findings (Table 4) reveal improvements of 0.89%, 0.78%, and 0.82% in the average base, novel, and harmonic mean accuracies, respectively, upon integrating visual expert tokens. These results support the notion that domain-specific, learnable tokens enhance the model's ability to grasp fine-grained details by focusing on distinct aspects of the image, as opposed to the CLS token's global focus.

**Effects of Number of Attributes.** In our framework, the selection of attributes is dynamically determined by LLMs, leading to variability across different datasets. This adaptability stands in contrast to a static approach where the number of attributes is uniformly set across all datasets. To understand the impact of this variability, we explore how altering the number of attributes from 1 to 8 influences model performance. Our findings, detailed in Table 5, reveal a performance improvement trend as the number of attributes increases, with an optimal peak at 7 attributes before a slight decline at 8. However, crucially, across all fixed-attribute scenarios, none matched the performance achieved through our method's dynamic attribute determination. These results underscore the importance of an adaptive approach to attribute selection, as opposed to a one-size-fits-all strategy.

**Design choice of the vision-conditional pooling layer.** Lastly, we ablate the design of the pooling layer, starting from the naive training-free average pooling, to the attention-based pooling mechanism with condition on the input image. Compared to average pooling, VCP demonstrates a performance gain of 1.08% in the average harmonic mean. Furthermore, when compared with attention-based max pooling, which selects a single description per attribute according to the attention score in Eq. (4),

    Des. Org. \\  &  Unstructured \\  & 
 Ours \\  \\  Base & 82.89 & **84.75** \\ Novel & 75.32 & **77.63** \\ HM & 78.93 & **81.04** \\   

Table 3: Effects of the Tree of Attributes.

   
 Attrs. Num. \\  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & Ours \\  Base Acc. & 83.20 & 83.97 & 84.1 & 84.41 & 84.45 & 84.62 & 84.66 & 84.74 & **84.75** \\ Novel Acc. & 74.90 & 76.20 & 76.35 & 77.06 & 77.13 & 77.17 & 77.35 & 76.67 & **77.63** \\ HM & 78.83 & 79.

VCP maintains a superior advantage of 1.55% in average harmonic mean. These outcomes attest to the VCP layer's integral role in finetuning attribute relevance to the visual context, substantiating its design and implementation within our model.

### Visualization

**Expert tokens focus on attribute-related regions.** We further investigate the effects of vision domain experts by visualizing their class activation maps from three illustrative examples using GradCAM , as shown inFig. 3. These visualizations underscore the precision with which each expert token concentrates on the image regions pertinent to its designated attribute. Take the first cat image as an example. The "fur pattern" expert distinctly highlights the animal's fur texture, whereas the "ear" and "eye" experts focus precisely on the respective anatomical features. This pattern of attribute-specific attention is consistent across the evaluated examples, reinforcing the conceptualization of expert tokens as dedicated "domain experts" within the visual field.

**VCP layer pools the most applicable descriptions.** The inherently interpretable nature of the VCP layer, thanks to its attention mechanism, allows for insightful visualizations of its operational process. Through the examination of attention weights assigned by the VCP layer to different attributes in a given image, we elucidate the layer's capability to discern and prioritize the most applicable descriptions. As illustrated in Fig. 4 with a "dumplings" image, the VCP layer adeptly allocates higher attention weights to descriptions accurately reflecting the observed instance (e.g., assigning weights of 0.92 to "round with a pleated edge" under the "Shape" attribute and 0.95 to "soft and chewy texture" under the Texture"). In contrast, less relevant descriptions for the specific image context (e.g., "crescent-shaped" for Shape and "crispy texture from pan-frying" for Texture) receive significantly lower weights. This discernment is crucial, given the class dumplings" encompasses a broad variety of appearances based on cooking methods, yet not all descriptions are fitting for every instance. These visualizations compellingly demonstrate the VCP layer's effectiveness in refining description relevance, thereby enhancing the model's interpretative alignment with the visual content.

## 5 Conclusion

This paper introduces Tree of Attribute Prompt learning (TAP), a novel method that integrates detailed, LLM-generated descriptions within VLMs, achieving state-of-the-art performance in both base-to-novel generalization and few-shot image classification tasks across 11 diverse datasets. TAP leverages a hierarchical "Tree of Attribute" framework, distilling structured knowledge graphs from LLMs for nuanced representation of visual concepts, and employs learnable "domain expert" tokens and a vision-conditional pooling module for optimal image-text alignment. While promising, we note that the reliance on LLMs presents challenges in fine-grained datasets where similar classes require nuanced differentiation, in which cases LLMs generate identical descriptions for distinct classes, impacting novel class prediction performance. It highlights the current limitations of LLMs in discerning highly fine-grained distinctions. Addressing this challenge through enhanced LLM capabilities or alternative strategies will be a key focus of future research.

   Pooling Method & Base Acc. & Novel Acc. & HM \\  Attn. Max Pooling & 82.90 & 76.36 & 79.49 \\ Average Pooling & 83.18 & 76.98 & 79.96 \\  VCP (Ours) & **84.75** & **77.63** & **81.04** \\   

Table 6: Design choice of the pooling layer.

Figure 4: Visualization of the attention weights in the VCP layer for an example “dumplings” image.