# High dimensional, tabular deep learning with an auxiliary knowledge graph

Camilo Ruiz\({}^{1,2,*}\), Hongyu Ren\({}^{1,*}\), Kexin Huang\({}^{1}\), Jure Leskovec\({}^{1}\)

\({}^{1}\)Department of Computer Science, Stanford University

\({}^{2}\)Department of Bioengineering, Stanford University

\({}^{*}\)Equal contribution

{caruiz, hyren, kexinh, jure}@cs.stanford.edu

###### Abstract

Machine learning models exhibit strong performance on datasets with abundant labeled samples. However, for tabular datasets with extremely high \(d\)-dimensional features but limited \(n\) samples (i.e. \(d n\)), machine learning models struggle to achieve strong performance due to the risk of overfitting. Here, our key insight is that there is often abundant, auxiliary domain information describing input features which can be structured as a heterogeneous knowledge graph (KG). We propose Plato, a method that achieves strong performance on tabular data with \(d n\) by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). In Plato, each input feature corresponds to a node in the auxiliary KG. In the MLP's first layer, each input feature also corresponds to a weight vector. Plato is based on the inductive bias that two input features corresponding to similar nodes in the auxiliary KG should have similar weight vectors in the MLP's first layer. Plato captures this inductive bias by inferring the weight vector for each input feature from its corresponding node in the KG via a trainable message-passing function. Across \(6\)\(d n\) datasets, Plato outperforms 13 state-of-the-art baselines by up to 10.19%.

## 1 Introduction

Machine learning models have reached state-of-the-art performance in domains with abundant labeled data like computer vision [76; 10] and natural language processing [70; 12; 52]. However, for tabular datasets in which the number \(d\) of features vastly exceeds the number \(n\) of samples, machine learning models struggle to achieve strong performance [24; 41]. Crucially, many tabular datasets from scientific domains [21; 30; 79; 17; 16; 33] have high-dimensional features but limited labeled samples due to the high time and labor costs of experiments. For these and other tabular datasets with \(d n\), the performance of machine learning models is currently limited.

The key challenge for machine learning models when \(d n\) is the risk of overfitting. Indeed, deep models can have a large number of trainable weights, yet training is limited by the comparatively small number of labeled samples. As a result, tabular deep learning approaches so far have focused on data-rich regimes with far more samples than features (\(n d\)) [19; 18; 59]. In the low-data regime with far more features than samples (\(d n\)), the dominant approaches for single tabular datasets are still statistical methods . These statistical methods reduce the dimensionality of the input space [1; 41; 66; 67], select features [64; 8; 14; 46], impose regularization penalties on parameter magnitudes , or use ensembles of weak tree-based models [15; 7; 34; 42; 51].

Here, we present a novel problem setting and framework that enables tabular deep learning when \(d n\) (Figure 1). Our key insight is that there is often abundant, auxiliary domain information

[MISSING_PAGE_FAIL:2]

We exhibit Plato's performance on 6 \(d n\) tabular datasets with 13 state-of-the-art baselines spanning dimensionality reduction, feature selection, statistical models, graph regularization, weight-inference, and tabular deep learning. Following a rigorous evaluation protocol from the tabular deep learning literature [19; 18], Plato outperforms the prior state-of-the-art on all 6 datasets by up to 10.19%. Ablation studies demonstrate the importance of Plato's trainable message-passing, the importance of non-feature nodes in the KG, and Plato's robustness to missing edges in the KG. Ultimately, Plato enables deep learning for tabular data with \(d n\) by using an auxiliary KG describing the input features.

## 2 Related Work

**Tabular deep learning methods.** In contrast to Plato's setting, tabular deep learning methods have primarily been developed for settings with far more samples than features (_i.e._\(n d\)). Indeed, recent tabular deep learning benchmarks ignore datasets with a large number of features and a small number of samples [19; 18; 59]. In the \(n d\) setting, various categories of deep tabular models exist. First, decision tree models like NODE  make decision trees differentiable to enable gradient-based optimization [25; 36; 80]. Second, multilayer perceptrons (MLPs) apply sequential, non-linear transformations to input features [32; 31]. Third, tabular transformer architectures use an attention mechanism to select and learn interactions among features. Examples include TabNet , TabTransformer , FT-Transformer , TabPFN , SAINT , Non-Parametric Transformers , and AutoInt . Finally, although Plato focuses on single tabular datasets, transfer learning architectures can learn across multiple tabular datasets [39; 84; 73]. Ultimately, we compare Plato to several benchmarked, state-of-the-art models for single, tabular datasets [19; 18; 59].

\(d n\) **methods.** For Plato's setting in which \(d n\), various tabular machine learning approaches exist . First, dimensionality reduction techniques like PCA  aim to reduce the dimensionality of the input data while preserving as much of the the variance in the data as possible [41; 66; 67]. Second, feature selection approaches select a parsimonious set of features, leading to a smaller feature space. Feature selection approaches include LASSO  and its variants [8; 14; 46]. For feature selection with deep learning, Stochastic Gates  are among the best performing of many variants [4; 43]. Finally, tree-based models like XGBoost learn ensembles of weak decision trees models to make an overall prediction [15; 7; 34; 51].

**Weight inference.** Using one network to infer the weights of another has been studied extensively [11; 58; 6]. For example,  infers the weights in all layers of a sequential model (_i.e._ RNN, LSTM) by using information about the weights' structure. Diet Networks  infer weights by hand-crafting prior information about the input features or using random projections. By contrast, Plato infers the weights in a MLP from prior information describing the input features in an auxiliary KG. Plato's weight inference uniquely captures the inductive bias that two input features corresponding to similar nodes in a KG should have similar corresponding weight vectors in the first layer of a MLP (Figure 1).

**Graph regularization.** Graph regularization approaches regularize the weights of a linear model based on a simple graph between input features. The graph is typically constructed from the tabular data based on covariance relationships. Approaches then add a regularization penalty to the loss function which forces the weights of the linear model to vary smoothly over the corresponding feature nodes in the graph. State-of-the-art methods include GraphNet  and Network-Constrained LASSO  which are based on a Laplacian regularization [60; 2] as well as Network LASSO  which generalizes the Group LASSO  to a network setting. Plato differs from graph regularization approaches in two key ways. First, Plato's KG includes both feature and non-feature nodes and multiple edge types, thereby modeling diverse, prior domain information that is missing in graph regularization approaches. Second, Plato infers the weights of a deep non-linear model (_i.e._ a MLP) rather than adding a regularization penalty to a loss, representing a distinct regularization mechanism.

**Knowledge graph methods.** Existing KG approaches are designed for tasks directly on the graph like link prediction or node classification [71; 65; 72; 78; 13]. By contrast, Plato does not make any predictions on the KG. Instead, Plato makes predictions on a separate, tabular dataset by using the KG as a prior. Graph classification methods also do not apply (Appendix B).

Plato

Plato is a machine learning method for tabular datasets with \(d n\) and an auxiliary knowledge graph (KG) with input features as nodes (Section 3.1). Plato's key insight is that there often exists abundant domain information describing input features which can be structured as an auxiliary KG \(G\) (Figure 0(a)). Plato uses the auxiliary KG to regularize a multilayer perceptron (MLP) and achieve strong performance on tabular data when \(d n\).

### Problem setting

Consider a tabular dataset \(^{n d}\) with labels \(^{n}\) and far more \(d\) features than \(n\) samples such that \(d n\). The goal is to train a model \(\) to predict labels \(}\) from the input \(\). Plato assumes the existence of an auxiliary knowledge graph \(G=(V,E)\) with \(|V|\) nodes and \(|E|\) edges such that each input feature \(j\) corresponds to a node in \(G\). Formally, \( j\{1,,d\}, v V\) s.t. \(j v\), as shown in Figure 0(a). \(G\) also contains additional nodes which represent broader knowledge describing the domain. The edges in \(G\) are (head node, relation type, tail node) triplets.

### Plato's inductive bias

In Plato, each input feature \(j\) corresponds to a node in the auxiliary KG (Figure 0(a)). In the first layer of a MLP with \(h\) hidden units, each input feature \(j\) also corresponds to a weight vector \(_{j}^{}^{h}\) such that the weight vectors of all features collectively compose the weight matrix \(^{}^{d h}\) (Figure 0(b)). Plato is based on the inductive bias that two input features \(j\) and \(k\) which correspond to similar nodes in the KG should have similar weight vectors \(_{j}^{}\) and \(_{k}^{}\) in the first layer of the MLP. Plato captures this inductive bias by inferring the weight vector for a feature from its corresponding node in the auxiliary KG with a trainable message-passing function (Figure 0(c),d).

### Plato overview

Plato has four key steps. First, Plato uses a self-supervised objective on the auxiliary KG to pretrain an embedding for each input feature (Section 3.4). Second, Plato updates each feature embedding with a trainable message-passing function that is trained on the supervised loss objective for the tabular data (Section 3.5, Figure 0(c)). Third, Plato infers the weights in the first layer of the MLP from the feature embeddings with a small neural network that is shared across input features (Section 3.6, Figure 0(d)). Finally, the MLP predicts the label for the input sample.

### Pretraining feature embeddings with self-supervision on the knowledge graph

First, Plato learns general prior information about each input feature \(j\) from the auxiliary KG \(G\). Plato represents the general prior information about each input feature \(j\) as a low-dimensional embedding \(_{j}^{c}\). Since each input feature \(j\) corresponds to a node in \(G\), Plato can learn \(_{j}\) by learning an embedding for the corresponding feature node in \(G\). Any self-supervised node embedding method on \(G\) can be used within Plato's framework.

**Formal notation.** Formally, Plato uses self-supervision on \(G\) to pretrain an embedding for each input feature according to

\[=(G). \]

\(^{d c}\) is the matrix of all feature embeddings. \(\) is a self-supervised node embedding method. We refer to Eq. (1) as pretraining since only the auxiliary KG \(G\) is used but the tabular data \(\), \(\) is ignored. After pretraining, the feature embeddings \(\) are fixed.

For \(\), we choose ComplEx as it is a prominent and highly scalable KG node embedding method . ComplEx uses a self-supervised objective which learns an embedding for each node in \(G\) by classifying whether a proposed edge exists in \(G\). ComplEx's proposed edges include both feature nodes and other nodes in \(G\), thereby integrating prior information about the input features and the broader domain. We also test KG embedding methods DistMult  and TransE  in Appendix C.

### Updating feature embeddings with a message-passing function trained on tabular data

Plato next updates each feature embedding with a trainable message-passing function that is trained on the supervised loss for the tabular data (Figure 0(c)). During message-passing, Plato updates the embedding of each input feature to be a weighted aggregation of it's neighbors' embeddings.

**Formal notation.** Formally, Plato uses a message-passing function \(\) on the KG to update each pre-trained feature embedding \(_{j}^{c}\) to feature embedding \(_{j}^{c}\) according to

\[=(,G,_{i};). \]

As input, the message-passing function considers the pre-trained feature embeddings \(\), the knowledge graph \(G\), and the sample value \(_{i}\). \(Q\) uses an attention mechanism which considers the sample value \(_{i}\). The only trainable weights in \(\) are in the attention mechanism and are \(\).

**The message passing network \(\).** Let \(_{j}^{[r]}\) be the embedding of input feature \(j\) after round \(r\{1,...,R\}\) of message passing. For each input feature \(j\), \(\) first initializes the updated feature embedding to the pretrained feature embedding.

\[_{j}^{}=_{j}. \] \[\] then conducts \[R\] rounds of message passing. In each round of message passing, the feature embedding \[_{j}^{[r]}\] is updated from the feature embedding of each neighbor \[k\] in the prior round \[_{k}^{[r-1]}\] and its own feature embedding in the prior round \[_{j}^{[r-1]}\]. The "message" being passed is the embedding of each feature from the prior round. \[_{j}^{[r]}=} _{ijk}_{k}^{[r-1]})}^{}+_{j}^{[r-1]}}_{}. \]

\(\) is an optional nonlinearity. \(N_{j}\) are the neighbors of feature node \(j\) in \(G\).

During message-passing, \(\) uses two scalar values \(\) and \(_{ijk}\) to control the weights of messages. First, \(\) uses hyperparameter \(\) to control the weight of the messages aggregated from the feature node's neighbors vs. from the feature node itself. Second, \(\) calculates an attention coefficient \(_{ijk}\) to allow distinct nodes in the same neighborhood to have distinct weights. The coefficient \(_{ijk}\) specifies the weight of the message between feature \(j\) and neighbor \(k\) for sample \(i\).

After \(R\) rounds of message-passing, the updated feature embeddings \(_{j}\) are set.

\[_{j}=_{j}^{[R]}. \]

**The attention coefficient.**Plato's attention coefficient \(_{ijk}\) is inspired by  in which node attributes are used to calculate the weight of a message between neighboring nodes. For a sample \(i\) in Plato, the node attributes for features \(j\) and \(k\) are their sample values \(_{ij}\) and \(_{ik}\). Plato thus uses the sample values \(_{ij}\) and \(_{ik}\) to calculate the attention coefficient. The attention coefficient \(e_{ijk}\) indicates the importance of node \(j\) to node \(k\) for sample \(i\).

\[e_{ijk}=(_{ij},_{ik};). \]

\(\) is a shallow neural network parameterized by \(\) that is shared across samples and features. The number of trainable weights in \(\) is small since the input of \(\) is \(^{2}\) and the output of \(\) is a scalar \(\).

To make the attention coefficients comparable across different nodes, Plato normalizes the attention coefficients with a softmax function across the neighbors \(N_{j}\) of node \(j\).

\[_{ijk}=_{k}(e_{ijk})=)}}{_{t N _{j}})}}. \]

### Inferring the first layer of weights in \(\) from the updated feature embeddings

Finally, Plato infers the weights in the first layer of a MLP \(\) from the updated feature embeddings (Figure 0(d)). In the first layer of a MLP with \(h\) hidden units, each input feature \(j\) corresponds to a weight vector \(_{j}^{}^{h}\) (Figure 0(b)). The weight matrix in the first layer of the MLP, \(^{}^{d h}\), is simply the concatenation of \(d\) weight vectors, one corresponding to each input feature. For each input feature \(j\), Plato infers the weight vector \(}_{j}^{}^{h}\) from the feature embedding \(_{j}^{c}\) by using a shallow neural network shared across input features. Input features with similar feature embeddings will produce similar weight vectors. Thus, Plato captures the inductive bias that input features corresponding to similar nodes in the KG should have similar corresponding weight vectors in the MLP's first layer.

```
Input: A data sample \(_{i}^{d}\), a knowledge graph \(G\) containing each input feature in \(\) as a node, a matrix of input feature embeddings \(^{d c}\) pre-trained over \(G\). Output: A predicted label \(}_{i}\).
```

**Algorithm 1**The Plato Algorithm.

**Formal notation.**Plato infers the weight vector associated with each input feature \(j\) in the first layer of \(\) with

\[}_{j}^{}=(_{j}|_{i}; ). \]

\(\) is a shallow neural network with trainable weights \(\). \(_{j}\) is the updated feature embedding of \(j\) which is conditioned on the specific input sample \(_{i}\) since the input sample is used as an input in its calculation (Section 3.5, Equation 2). \(\) are the weights of \(\). \(\) and its weights \(\) are shared across each feature \(j\{1,,d\}\).

Plato drastically reduces the number of trainable weights compared to a standard MLP. The sharing of \(\) and \(\) across all input features drastically reduces the number of trainable weights compared to a standard MLP. For a high-dimensional tabular dataset (_i.e._\(d n\)), a standard MLP \(\) with \(h\) hidden units has a large number of trainable weights in the first layer since \(^{}^{d h}\). A standard MLP \(\) must learn all \(dh\) of these trainable weights by backpropagation. By contrast, \(\) uses a shared set of trainable weights \(\) to infer \(}_{j}\) from \(_{j}\) for every \(j\{1,,d\}\). The number of trainable weights in \(\) is small compared to \(dh\) since \(\) need only transform every \(_{j}^{c}\) to \(}^{}^{h}\). Thus, \(||=ch\) (assuming \(\) is a single layer neural network). \(c\), the dimensionality of the feature embedding, is much less than \(d\) the number of input features. As a result, \(||=ch dh\) and Plato drastically reduces the number of trainable weights in the first layer of a MLP.

### The Plato algorithm

Plato is outlined in Algorithm 1.

## 4 Experiments

We evaluate Plato against 13 baselines on 10 tabular datasets (6 with \(d n\), 4 with \(d n\)).

**Datasets.** We use 6 tabular \(d n\) datasets, 4 \(d n\) datasets [16; 17; 30; 79], and a KG from prior studies [44; 35; 38; 56; 63; 74; 75] (Appendix G, H). The KG contains 108,447 nodes, 3,066,156 edges, and 99 relation types. All datasets include features which map to a subset of knowledge graph nodes. Code, data, and the KG are available at [https://github.com/snap-stanford/plato](https://github.com/snap-stanford/plato).

**Baselines.** We compare Plato to 13 state-of-the art statistical and deep baselines. We consider regularization with Ridge Regression , dimensionality reduction with PCA  followed by linear regression, feature selection with LASSO , deep feature selection with Stochastic Gates , and gradient boosted decision trees with XGBoost . We consider tabular deep learning with a standard MLP, self-attention-based methods with TabTransformer  and TabNet , differentiable decision trees with NODE , and weight inference with Diet Networks . We also attempted FT-Transformer , but it experienced out of memory issues on all datasets due to the large number of features. Finally, we consider graph regularization methods which also have access to the knowledge graph including GraphNet , NC LASSO , and Network LASSO  (Appendix E).

**Fair Comparison of Plato with Baselines.** To ensure a fair comparison with baselines, we follow evaluation protocols in recent tabular benchmarks [19; 18]. We conduct a random search with 500 configurations of every model (including Plato) on every dataset across a broad range of hyperparameters (Appendix A). We split data with a 60/20/20 training, validation, test split. All results are computed across 3 data splits and 3 runs of each model in each data split. We report the mean and standard deviation of the Pearson correlation (PearsonR) between \(\) and \(}\) across runs and splits on the test set. Each model is run on a GeForce RTX 2080 TI GPU.

   Dataset & MNSCLC & CM & PDAC & BRCA & CRC & CH \\   & 15,390 & 13,183 & 12,932 & 12,693 & 18,206 & 19,902 \\   & 295 & 286 & 321 & 476 & 562 & 924 \\   & 52.2 & 46.1 & 40.3 & 28.2 & 22.6 & 19.7 \\  Classic Stat ML & Ridge & 0.153\(\)0.000 & 0.390\(\)0.000 & 0.344\(\)0.000 & 0.538\(\)0.000 & 0.376\(\)0.000 & 0.546\(\)0.000 \\  Dim. Reduct. & PCA & 0.156\(\)0.113 & 0.070\(\)0.000 & 0.232\(\)0.121 & 0.452\(\)0.000 & 0.193\(\)0.163 & 0.237\(\)0.232 \\   & LASSO & 0.168\(\)0.000 & 0.431\(\)0.000 & 0.346\(\)0.000 & 0.470\(\)0.000 & 0.400\(\)0.000 & 0.547\(\)0.000 \\  & STG & 0.132\(\)0.130 & 0.366\(\)0.003 & 0.258\(\)0.005 & 0.485\(\)0.037 & 0.301\(\)0.000 & 0.262\(\)0.076 \\  Decision Tree & XGBoost & -0.02\(\)0.000 & 0.225\(\)0.000 & 0.363\(\)0.000 & 0.347\(\)0.000 & 0.354\(\)0.000 & 0.728\(\)0.000 \\   & GraphNet & 0.169\(\)0.000 & 0.277\(\)0.009 & 0.249\(\)0.015 & 0.350\(\)0.009 & 0.125\(\)0.061 & 0.646\(\)0.051 \\  & NC LASSO & 0.210\(\)0.104 & 0.339\(\)0.004 & 0.327\(\)0.053 & 0.458\(\)0.083 & 0.220\(\)0.000 & 0.415\(\)0.083 \\  & Network LASSO & 0.212\(\)0.046 & 0.243\(\)0.058 & 0.136\(\)0.027 & 0.348\(\)0.033 & 0.171\(\)0.040 & 0.212\(\)0.091 \\  Param. Infer. & Diet & -0.04\(\)0.205 & 0.054\(\)0.149 & 0.309\(\)0.006 & 0.213\(\)0.036 & 0.087\(\)0.112 & 0.148\(\)0.008 \\   & MLP & 0.128\(\)0.126 & 0.322\(\)0.043 & 0.289\(\)0.047 & 0.240\(\)0.067 & 0.355\(\)0.022 & 0.044\(\)0.039 \\  & NODE & 0.003\(\)0.000 & 0.150\(\)0.000 & 0.190\(\)0.000 & 0.512\(\)0.000 & 0.344\(\)0.000 & 0.181\(\)0.000 \\  & TabTransformer & 0.265\(\)0.000 & 0.072\(\)0.000 & 0.029\(\)0.000 & 0.202\(\)0.000 & 0.238\(\)0.000 & 0.020\(\)0.000 \\  & TabNet & 0.085\(\)0.028 & 0.010\(\)0.086 & 0.088\(\)0.037 & 0.055\(\)0.037 & 0.018\(\)0.016 & 0.039\(\)0.025 \\  Ours & Plato & **0.272\(\)0.130** & **0.435\(\)0.022** & **0.400\(\)0.021** & **0.583\(\)0.019** & **0.401\(\)0.019** & **0.770\(\)0.003** \\   

Table 1: **Plato outperforms statistical and deep baselines when \(d n\). For every dataset, the best overall model is in bold and the second best model is underlined.**

### Results

**Plato outperforms statistical and deep baselines when \(d n\).**Plato outperforms all baselines across all 6 datasets with \(d n\) (Table 1). Plato achieves the largest improvement on the PDAC dataset, improving by 10.19% vs. XGBoost, the best baseline for PDAC (0.400 vs. 0.363). While Plato achieves the strongest performance across all 6 datasets, the best performing baseline varies across datasets. Ridge Regression is the strongest baseline for BRCA, LASSO for CM and CRC, XGBoost for PDAC and CH, and TabTransformer for MNSCLC. The remaining baselines are not the strongest baseline for any dataset. We also find that the performance of a specific baseline depends largely on the dataset. TabTransformer, for example, is the best baseline for the MNSCLC dataset but the worst baseline for the CH dataset. The rank order of all models on all datasets is Appendix D.

**Plato's performance depends on updating feature embeddings with a trainable message-passing function.**Plato infers the weights \(}^{[]}\) in the first layer of a MLP \(\) by using feature embeddings which contain prior information about the input features. Plato first pretrains general feature embeddings \(^{d c}\). Plato then updates the feature embeddings to \(^{d c}\) with a trainable message-passing function. We test whether updating the feature embeddings based on the trainable message-passing function is necessary by evaluating Plato's performance on the BRCA dataset in three configurations (Table 2). The default configuration uses the updated feature embeddings \(\) generated by the message-passing function to infer \(}^{[]}\) according to \(}^{[]}_{j}=(_{j}|_{i})\). The second configuration uses the general feature embeddings \(\) instead of \(\) to infer \(}^{[]}\) according to \(}^{[]}_{j}=(_{j})\). The third configuration does not use feature embeddings and thus ablates to a standard MLP. Using general feature embeddings \(\) improves over not using feature embeddings at all (0.522 vs. 0.240). Using feature embeddings \(\) that are generated by the trainable message-passing function further improves performance (0.583 vs. 0.522). Thus, updating the feature embeddings to \(\) based on the trainable message-passing function is key to Plato's performance.

   Dataset & ME & BC & SCLC & NSCLC \\   & 19,902 & 18,261 & 18,437 & 18,308 \\   & 10,064 & 10,101 & 10,712 & 16,730 \\   & 2.0 & 1.8 & 1.7 & 1.1 \\  Classic Stat ML & Ridge & 0.566\(\)0.008 & 0.483\(\)0.008 & 0.604\(\)0.057 & 0.679\(\)0.008 \\  Dim. Reduct. & PCA & 0.239\(\)0.310 & 0.233\(\)0.294 & 0.284\(\)0.274 & 0.645\(\)0.000 \\   & LASSO & 0.667\(\)0.000 & 0.633\(\)0.000 & 0.669\(\)0.000 & 0.637\(\)0.000 \\  & STG & 0.676\(\)0.000 & 0.643\(\)0.000 & 0.668\(\)0.000 & 0.646\(\)0.000 \\  Decision Tree & XGBoost & **0.875\(\)0.000** & 0.826\(\)0.000 & 0.878\(\)0.000 & **0.843\(\)0.000** \\   & GraphNet & 0.675\(\)0.047 & 0.723\(\)0.026 & 0.742\(\)0.039 & 0.627\(\)0.042 \\  & NC LASSO & 0.733\(\)0.016 & 0.730\(\)0.027 & 0.793\(\)0.009 & 0.746\(\)0.023 \\  & Network LASSO & 0.401\(\)0.034 & 0.451\(\)0.022 & 0.417\(\)0.074 & 0.465\(\)0.034 \\  Param. Infer. & Diet & 0.105\(\)0.000 & 0.037\(\)0.000 & -0.050\(\)0.000 & 0.002\(\)0.000 \\   & MLP & 0.487\(\)0.131 & 0.508\(\)0.061 & 0.537\(\)0.061 & 0.573\(\)0.005 \\  & NODE & 0.870\(\)0.000 & 0.420\(\)0.169 & 0.801\(\)0.102 & 0.487\(\)0.197 \\  & TabTransformer & 0.305\(\)0.028 & 0.010\(\)0.000 & 0.288\(\)0.203 & 0.503\(\)0.187 \\  & TabNet & 0.667\(\)0.002 & 0.624\(\)0.001 & 0.657\(\)0.004 & 0.647\(\)0.000 \\  Ours & Plato & **0.875\(\)0.004** & **0.844\(\)0.003** & **0.883\(\)0.002** & 0.839\(\)0.000 \\   

Table 6: **Plato’s performance is competitive with baselines when \(d n\). For every dataset, the best overall model is in bold and the second best model is underlined.**

**Plato's performance depends on both feature nodes and broader knowledge nodes in the auxiliary KG.**Plato relies on an auxiliary KG \(G\) which contains information describing input features and the broader domain. Information describing input features is represented as feature nodes while information describing the broader domain is represented as other nodes in \(G\) (Methods 3.1). To test the relative importance of the feature information in \(G\) vs. the broader domain information, we measured the performance of Plato on the BRCA dataset in two KG configurations: Plato with the full KG (_i.e._ both the feature nodes and the broader domain nodes) and Plato with a "feature-only KG" (_i.e._ an induced subgraph on only the feature nodes) (Table 3). We also compare to a "No KG" configuration in which Plato does not have access to the KG. Without auxiliary information describing the input features or the broader domain, Plato is ablated to a standard MLP.

We find that both the feature nodes and the broader knowledge nodes are important for Plato's performance. Using the "feature-only KG" configuration of Plato improves performance vs the "no KG" configuration (0.539 vs 0.240). Using the "full KG" configuration further improves performance vs the "feature-only KG" configuration (0.583 vs 0.539). Plato's performance thus relies on both the feature information and the broader domain information in the KG.

**Plato's performance with an incomplete knowledge graph.** All KGs are incomplete since there is undiscovered knowledge. Plato thus uses low-dimensional embeddings from KG embedding approaches  which are designed to account for missing information, enabling predictive performance even with missing edges. We conduct an ablation study to assess Plato's robustness to missing edges in the KG. We randomly remove edges from the KG and measure Plato's performance on the BRCA dataset. We observe that with only 50% of the KG's edges, Plato still has 71% of the performance as Plato with 100% of the KG's edges (\(0.412\) vs. \(0.583\)) (Table 4).

**The importance of MLP layers \(2,,L\), the layers with trainable weights, for Plato.**Plato is a MLP in which the weights in the first layer are inferred from the knowledge graph (KG) but the weights in the remaining layers \(2,,L\) are trained normally. We conduct an ablation study to determine whether MLP layers \(2,...,L\) are necessary for Plato's performance or whether the first layer of inferred weights are sufficient. Note that a single layer of inferred weights in Plato is equivalent to a linear regression in which the weights are inferred from the KG. We thus compare Plato to Plato-LR, a linear regression in which the weights are inferred from the KG (Table 5). Plato's standard configuration outperforms Plato-LR on the BRCA dataset (\(0.583\) vs. \(0.550\)). Therefore, layers \(2,,L\) of the MLP are important for Plato's performance.

**For datasets with \(d n\), Plato is competitive with baselines.** Finally, we test Plato's performance for datasets with \(d n\). We test 4 datasets with \(d n\) ranging from \(=1.1\) to \(2.0\) (Table 6). We find that on 4 datasets with \(d n\), Plato is competitive with the best baseline, XGBoost, but does not improve performance substantially. Plato's stronger performance for datasets with \(d n\) than for datasets with \(d n\) is justified. Plato's key idea is to include auxiliary information describing the input features. Auxiliary information is likely to help performance the most in settings with the least labeled data (_i.e._\(d n\)). When \(d n\), auxiliary information is less helpful since the tabular dataset may already have enough information to train a strong predictive model. We further find that XGBoost is the strongest baseline for all datasets with \(d n\), in contrast to XGBoost's varied performance on the datasets with \(d n\) (Table 1).

## 5 Discussion

Plato achieves strong performance on tabular data when \(d n\) by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). Across 6 datasets, Plato outperforms 13 state-of-the-art baselines by up to \(10.19\%\). Ablations demonstrate the importance of Plato's trainable message-passing function, of including nodes in the KG that don't represent input features but instead represent domain information, and of the layers in the MLP whose weights are trained directly rather than inferred. We also test Plato's robustness to missing information in the KG. Plato has several limitations. First, Plato matches but does not improve the performance of baselines for high-dimensional datasets with more samples (_i.e._\(d n\)). Second, Plato depends on the existence of an auxiliary KG of domain information. Overall, Plato enables tabular deep learning when \(d n\) by using an auxiliary KG of domain information describing input features.