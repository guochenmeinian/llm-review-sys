# VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset

Sihan Chen12, Handong Li12, Qunbo Wang2,

**Zijia Zhao21**, Mingzhen Sun21, Xinxin Zhu2, Jing Liu12\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{2}\) Institute of Automation, Chinese Academy of Science

{sihan.chen, xinxin.zhu, jliu}@nlpr.ia.ac.cn,

{lihandong2023, qunbo.wang, zhaozijia2021, sunmingzhen2020}@ia.ac.cn

Equal Contribution.Corresponding author.

###### Abstract

Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including **V**ision, **A**udio, and **S**ubtitle, and **T**ext by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks. Code, model and dataset will be released at https://github.com/TXH-mercury/VAST.

## 1 Introduction

A vast number of videos are recorded and uploaded to social media platforms every day, making the understanding of video content become a critical area of research in artificial intelligence. Simultaneously, the text serves as the most direct and efficient means of information propagation in our daily lives. Consequently, video-text cross-modality learning is crucial for AI systems to interact with humans, assisting them in understanding video content (video captioning), searching for desired videos (text-to-video retrieval), or answering questions about videos (video QA). With the advent of unsupervised pre-training techniques [1; 2] and large-scale video-text corpora [3; 4], video-text pre-training models have made rapid advancements and achieved relatively impressive performances on the aforementioned tasks. However, we contend that current pre-training models are far from perfect, as most of them are restricted to establishing connections solely between the text and visual content of videos, without incorporating other modality tracks such as audio and subtitles. Specifically, environmental audio can provide additional contexts overlapping with the visual information to reduce ambiguity and increase the prediction confidence of models, orcomplement the visual information to supply more multi-modal cues. Human speech or transcriptions (subtitles) also contain valuable information about conversations, news, or instructional procedures, which can be jointly modeled to enhance the comprehensive understanding of videos. Given that both audio and subtitles are informative for video comprehension, it is necessary to integrate them into a more advanced omni-modality foundational model, as depicted in Figure 1.

The primary challenge lies in the absence of an appropriate training corpus. To train such a foundational model, we require an omni-modality video caption corpus in which captions exhibit correspondence with vision, audio, and subtitles simultaneously. Unfortunately, as shown in Table 2, current video-text corpora either utilize raw subtitles as captions [4; 5; 6], contain only vision captions (alt-texts) , or possess audiovisual captions but limited in scale . On the other hand, manual annotation of an omni-modality corpus is not feasible due to the exorbitant cost associated with the high demand for descriptions.

To address this issue, we propose a two-stage automatic pipeline to generate omni-modality captions for video clips from a large-scale open-domain video corpus (HD_VILA_100M ). Specifically, as depicted in Figure 2, in the first stage, we train separate vision and audio captioners on publicly available vision and audio caption corpora, which are then utilized to generate high-quality single-modality objective descriptions. In the second stage, we employ Vicuna-13b , an off-the-shelf Large Language Model (LLM), as a zero-shot omni-modality captioner. We feed the LLM with the generated single-modality captions, subtitles, and instructional prompts to encourage LLM to integrate different modality information and summarize them into a single long sentence, forming an omni-modality caption. Through this process, we create VAST-27M, a dataset that consists of 27 million video clips, each paired with 11 captions (including 5 vision, 5 audio, and 1 omni-modality caption). Given its comprehensive caption types, VAST-27M can contribute to various research communities, including vision-text, audio-text, and omni-modality pretraining. Extensive experiments have demonstrated that VAST-27M surpasses current video-text, audio-text, and audiovisual-text corpora by a significant margin.

Based on VAST-27M, we train a Vision-Audio-Subtitle-Text omni-modality foundational model named VAST. As shown in Figure 2, VAST consists of three single modality encoders while text encoder can fulfill cross-modality fusion through cross-attention layers. VAST is trained with three objectives including OM-VCC, OM-VCM, and OM-VCG (detailed in section 4.2) to enhance omni-modality understanding and generation capabilities. As shown in Table 1, compared to existing cross-modality pre-training models, VAST is trained on omni-modality video caption corpus and is capable of perceiving and processing all four modalities and supports a wide range of downstream tasks, with various modalities (vision-text, audio-text, and multi-modal video-text), and multiple types (retrieval, captioning, and QA).

Figure 1: Illustration of the difference between conventional cross-modality pretraining and the proposed omni-modality pretraining. Thanks to the proposed omni-modality video caption corpus VAST-27M, the VAST foundation model can perceive videos from multiple information sources, including vision, audio, and subtitles, and enhance the connections between omni-modalities videos (OMV) and omni-modality captions (OMC) through large-scale pretraining. A, V, S, and T represent audio, vision, subtitle, and text, respectively. AC, VC, and AVC are abbreviations for audio, vision, and audiovisual captions.

Our contributions are summarized as follows: (_i_) We introduce VAST-27M, the first large-scale omni-modality video caption dataset automatically generated by trained single modality captioners and Large Language Model; (_ii_) We train the first vision-audio-subtitle-text foundation model VAST which fulfills omni-modality perception and understanding; (_iii_) Extensive experiments verify the effectiveness of VAST and it outperforms state-of-the-art in a diverse range of cross-modality benchmarks.

## 2 Related Work

### Cross-Modality Pretraining Corpus

**Video-Text Pretraining Corpus.** In the early stages, most video-text models were trained on the large-scale HowTo100M dataset . This dataset consists of 136 million video clips from 1.22 million instructional YouTube videos, and employs automatically generated subtitles from ASR transcription tools as corresponding clip captions. Later, following similar schemes for collecting narrative videos-subtitles corpora, Zellers et al. proposed the YT-Temporal-180M corpus , which contains 180 million clips from 6 million YouTube videos. To overcome the constraint of the instructional domain, Xue et al. assembled the open-domain, high-resolution HD_VILA_100M corpus , consisting of 100 million clips from 3.3 million YouTube videos. However, while labeling subtitles is more cost-effective than annotating captions, subtitles often lack direct relevance to visual content, resulting in weak video-text correlations in model learning. Inspired by the collection schemes of the Conceptual Captions datasets (CC) [9; 10], Bain et al. compiled the WebVid2.5M and WebVid10M datasets , utilizing alt-texts as video captions, surpassing the aforementioned datasets that use subtitles as captions. However, alt-texts, although more related to video content than subtitles, differ in style from standard captions and still contain noise. Additionally, alt-texts typically describe visual content only, without incorporating other modalities. Taking a step further beyond vision, Chen et al. created the VALOR-1M dataset , which contains 1 million video clips from AudioSet  paired with annotated audiovisual captions. However, subtitle contents are not reflected in these captions, and the dataset's scale is limited and challenging to expand due to the expensive cost of manual annotation. In comparison to the aforementioned training corpora, our VAST-27M is the first high-quality, large-scale omni-modality video caption dataset whose captions encompass all modalities of video including vision, audio, and subtitles. In addition, its expansion is more cost-effective due to the automated generation pipeline.

**Audio-Text Pretraining Corpus.** In contrast to vision-text pretraining, audio-text pretraining has progressed relatively slowly due to limitations in training corpora, both in terms of quantity and scale. Human-labeled datasets such as Clotho , AudioCaps , MCAS , and AudioCaption  all contain fewer than 50,000 audio clips, which fall far short of the requirements for large-scale pretraining. Wu et al. compiled LAION-Audio-630K  by crawling audios and corresponding alt-texts from multiple sources, with a majority of audios sourced from Freesound . Mei et al. introduced WavCaps , which comprises 403,000 audios and descriptions collected from various sources. They utilized ChatGPT to filter and rewrite the raw descriptions into caption-like sentences. However, the quality of the captions is heavily influenced by the quality of the raw descriptions. In contrast, we generates audio captions through a trained high-quality caption model, and VAST-27M is nearly two orders of magnitude larger than LAION-Audio-630K or WavCaps.

    &  &  \\   & Retrieval & Discriminative & Generative & V-T & A-T & VA-T & VS-T & VAS-T \\  GIT  & & ✓ & ✓ & ✓ & & & \\ VALUE  & ✓ & ✓ & ✓ & ✓ & & ✓ & \\ VALOR  & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & \\ VAST & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparisons between current cross-modality pretraining models and the proposed VAST. VAST is the first foundation model capable of perceiving four modalities and generalizing to various types of downstream tasks. V, A, S, and T represent vision, audio, subtitle, and text, respectively.

### Multi-Modality Learning

There have been early methods exploring multi-modality tracks for video-text understanding. For instance, MMT  leverages seven experts to enhance text-to-video retrieval, while SMPFF  introduces audio for video captioning. In the context of video-text pretraining, there are works that jointly model subtitles or audio together with vision and text. Notable examples of incorporating subtitles include UniVL , CoMVT , and VALUE . However, the subtitle-text correlations established in these models are implicit and weak, achieved through masked prediction  or next utterance prediction  that uses raw subtitles as prediction targets, rather than abstract text. This approach introduces inconsistency between the pretraining and fine-tuning stages. In contrast, VAST fully harnesses the generalization capabilities of Large Language Models to extract the most crucial information from subtitles into language descriptions. On the other hand, notable examples of incorporating audio include AVLNet , MERLOT Reserve , i-Code , and VALOR . However, AVLNet, MERLOT Reserve, and i-Code focus on learning audio-subtitle relations rather than audio-text, limiting their generalization to tasks such as text-to-audio retrieval and audio captioning. While VALOR jointly models audio, vision, and text, it primarily targets perceiving environmental sounds and pays less attention to human speech. In comparison to the aforementioned methods, VAST is the first omni-modality foundation model capable of perceiving four modalities, connecting vision, audio, and subtitle to caption.

## 3 Dataset

### Data Collection of VAST-27M

**Vision Captioner Training.** To establish the general correspondence between objects and visual concepts, we draw inspiration from BLIP  and commence by training a vision caption model on large-scale image-text corpora, including CC4M, CC12M, and a randomly selected set of 100M image-text pairs from LAION-400M . Subsequently, we fine-tune the model on a combination of manually labeled image and video caption datasets, such as MSCOCO , VATEX , MSRVTT , and MSVD . Through this process, we achieve a high-quality captioner capable of perceiving both static objects and dynamic actions, thus generating smooth and accurate captions.

**Audio Captioner Training.** For the audio captioning task, we train a dedicated audio caption model using a combination of large-scale audio-text corpora, namely VALOR-1M and WavCaps datasets. Unlike its vision counterpart, we do not employ a second-stage fine-tuning due to the limited scale of downstream audio caption datasets, which could lead to overfitting to a narrow range of audio concepts. Additionally, VALOR-1M is a human-annotated corpus that provides a certain level of assurance regarding the accuracy of the generated captions.

**Clip Selection.** Due to the consideration about time and money cost of downloading, storage and computation, we select a 27M video clips from the HD_VILA_100M dataset instead of using all of them. The selection rules are introduced as follows: 1) Clips whose length are shorter than 5 seconds or longer than 30 seconds are dropped. 2) Clips which have missing modalities of vision, audio or subtitle are dropped. 3) Clips are evenly chosen from the original 3.3M long-form videos in HD_VILA_100M.

   Dataset & Domain & \#Clips & \#Captions & Text Source & S & VC & AC & AVC & OMC \\  HowTo100M  & Instructional & 136M & 136M & ASR transcription & ✓ & & & & \\ YT\_Temporal\_180M  & Instructional & 180M & 180M & ASR transcription & ✓ & & & & \\ HD\_VILA\_100M  & Open & 103M & 103M & ASR transcription & ✓ & & & & \\ WebV4-2.5M  & Open & 2.5M & 2.5M & All-texts & & ✓ & & & \\ WebVid-10M  & Open & 10M & 10M & Alt-texts & & ✓ & & & \\ VALOR-1M  & Open & 1M & 1M & Manual & & & ✓ & & \\ LAION-Audio-630K  & Open & 634K & 634K & Alt-texts & & & ✓ & & \\ WavCaps  & Open & 403K & 403K & Generated & & & ✓ & & \\ VAST-27M (Ours) & Open & 27M & 297M & Generated & ✓ & ✓ & ✓ & & ✓ \\   

Table 2: Comparisons between current video-text and audio-text pretraining corpora and the proposed VAST-27M. VAST-27M is the first large-scale omni-modality video-caption corpus containing various types of captions generated automatically. S, VC, AC, AVC, and OMC stand for subtitle, vision, audio, audiovisual, and omni-modality captions, respectively.

**Caption Generation.** For each video clip of VAST-27M, we employ the trained vision and audio captioners to generate 5 captions each, using a Top-K sampling approach with K=10. Subsequently, we utilize the off-the-shelf Vicuna-13b  model as the omni-modality captioner. Vicuna-13b is an open-source auto-regressive Large Language Model (LLM) based on the Transformer architecture, trained through fine-tuning of LLaMA  on user-shared conversations collected from ShareGPT. For each video clip, we randomly select 3 vision captions and 3 audio captions, and feed them, along with the raw subtitle and designed instructional prompts, into the LLM. As depicted in Figure 2, the LLM generates omni-modality captions that effectively describe the visual, audio, and subtitle contents, while adhering to a natural human captioning style. More examples of VAST-27M dataset can be found in Appendix.

### Statistics of VAST-27M

VAST-27M consists of a total of 27M video clips sampled from the large-scale HD_VILA_100M corpus. The dataset covers 15+ categories, including music, gaming, education, entertainment, animals, and more, as opposed to being limited to instructional domains like HowTo100M. Compared to other commonly used cross-modality training corpora, as shown in Table 2, VAST-27M contains a larger number of captions (in terms of both modalities and quantity). The average lengths of vision, audio, and omni-modality captions in VAST-27M are 12.5, 7.2, and 32.4, respectively.

## 4 Approach

### Basic Framework

As shown in Figure 2, VAST employs a fully end-to-end Transformer architecture, comprising a vision encoder (ViT ), an audio encoder (BEATs ), and a text encoder (BERT ). This framework can accommodate multi-modal inputs such as images, videos, audios, subtitles, and captions. The text encoder is responsible for encoding single-modality captions or subtitles, as well as performing multi-modal encoding/decoding through cross-attention layers. The omni-modality captions and subtitles are tokenized using the WordPiece tokenizer  and then fed into the text encoder to obtain the output features \(f_{omc}\) and \(f_{s}\), respectively. The vision encoder takes raw images

Figure 2: Illustration of the caption generation process of VAST-27M (top) and training framework of VAST (bottom). The vision and audio captioners generate captions based on the input video clip, and the Omni-Modality Captioner (Vicuna-13b) integrates them along with the raw subtitle and instructional prompts, to generate the omni-modality caption. The VAST model consists of three encoders and is trained under three objectives including OM-VCC, OM-VCM, and OM-VCG.

or sparsely sampled video frames as input and produces the output feature \(f_{v}\). For audio clips, they are first divided into multiple 10-second-long clips, padded with zeros, converted to 64-dimensional log Mel filterbank spectrograms using a 25ms Hamming window, and then fed into the audio encoder to obtain the output feature \(f_{a}\). The global representations ( [CLS] token feature) of these features are denoted as \(f_{}}\), \(f_{}}\), \(f_{}}\), and \(f_{}}\), respectively.

### Pretraining Objectives

Building upon conventional vision-text pretraining objectives, VAST employs the following three pretraining objectives:

_Onni-Modality Video-Caption Contrastive Loss (OM-VCC)._ The global omni-modality representations of video clips, denoted as \(f_{}g}\), are obtained by concatenating \(f_{}}\), \(f_{}}\), and \(f_{}}\). Subsequently, \(f_{}g}\) and \(f_{}}\) are projected into the same semantic space using two linear layers and are then normalized. The contrastive loss is employed to regularize the feature distance between omni-modality video (OMV) and caption (OMC). The contrastive loss is defined as follows, where \(sim()\) represents the dot product of \(f_{}}\) and \(f_{}g}\), and \(B\) and \(\) denote the batch size and a learnable parameter, respectively.

\[_{}}=-_{i=1}^{B},c_{i}))}{_{j=1}^{B}( sim(v_{i},c_{j})))}- _{i=1}^{B},c_{i})))}{_{j= 1}^{B}( sim(v_{j},c_{i})))}\] (1)

_Onni-Modality Video-Caption Matching Loss (OM-VCM)._ This loss encourages the model to infer whether a pair of OMV and OMC is matched or not. Specifically, caption tokens are fed into the text encoder again, this time with cross-attention layers activated to attend to the condition features \(f_{}}\) which is obtained by concatenating those unpooled features \(f_{s}\), \(f_{v}\), and \(f_{a}\) along the sequential dimension. Before concatenation, three independent linear layers are applied to adjust their hidden size to the same value. The output feature of the text encoder is then fed into a two-layer MLP to make binary predictions. To create informative negative pairs, we adopt a hard negative mining strategy following . The loss function is formulated as follows, where \(y=1\) if OMV and OMC are matched, and 0 otherwise.

\[_{}}=_{(v_{i},c_{i})(, )}[y p_{}cm}+(1-y)(1-p_{}cm})]\] (2)

_Onni-Modality Video Caption Generation Loss (OM-VCG)._ This loss employs conditional causal masked language modeling to enhance the model's ability to generate omni-modality captions. Specifically, 60% of the tokens in OMC are masked at the input of the text encoder. Cross-attention layers are activated and \(f_{}}\) is used as condition features as in OM-VCM. The self-attention layers in the text encoder utilize a single-directional causal attention mask to prevent information leakage, and the masked tokens are reconstructed at the output of the text encoder using BERT's vanilla prediction layer. The loss is defined as follows, where \(c_{m}\) and \(c_{<m}\) denote the masked tokens and the tokens before them, respectively.

\[_{}}=-_{(v_{i},c_{i})(, )} P(c_{m} c_{<m},v)\] (3)

The overall loss \(_{}}\) is the sum of the three losses, with equal weights assigned for simplicity.

\[_{}}=_{}}+ _{}}+_{}}\] (4)

### Modality Grouping

While VAST establishes omni-modality video-caption correspondence during pretraining, it is important to address the potential absence of modalities in downstream benchmarks and practical applications, due to that inconsistencies between the modalities used in pretraining and adaptation can have negative effects. Inspired by the modality grouping strategy proposed by VALOR , we uniformly model the relations of V-T, A-T, VA-T, VS-T, and VAS-T (previously introduced as \(_{}}\)). Specifically, vision and audio captions are used in V-T and A-T modeling, respectively, while omni-modality captions are employed in VA-T, VS-T, and VAS-T modeling. The final loss is formulated as follows:

\[=_{}}+_{}+ _{}+_{}+_{}\] (5)

## 5 Experiments

### Implementation Details

VAST is trained using the PyTorch framework on 64 Tesla V100 cards. The vision, audio, and text encoders are initialized from EVAClip-ViT-G , BEATs, and BERT-B, respectively, resulting in a total parameter size of 1.3B. The training is conducted on a combination corpus consisting of VAST-27M, VALOR-1M, WavCaps, CC14M, and 110M randomly sampled pairs from LAION-400M, for a total of 200K training steps. At each training step, one corpus is sampled for training. In addition, the raw captions of CC14M and LAION are replaced with captions generated by the trained vision captioner. The initial learning rate is set to 1e-4, and a linear decay schedule is used. The batch size is set to 1024. During the pertaining stage, one video frame and two 10s long audio clips are randomly sampled for each video clip. For ablation studies, CLIP-ViT-B  is used as the vision encoder, with its parameters frozen to increase efficiency. Further details such as the mixture ratio of the pretraining dataset and the downstream finetuning configurations can be found in the Appendix.

Regarding the adaptation to downstream tasks, for retrieval tasks, all candidates are ranked using VCC, and then the Top-50 candidates are reranked using VCM. For captioning tasks, beam search with a beam size of 3 is employed. For QA tasks, they are formulated as open-ended generative problems, where questions serve as prefixes, and answers are predicted without any constraints. For SOTA comparison and ablation studies in the main paper, Recall@1, CIDEr, and Acc are used as evaluation metrics for retrieval, captioning, and QA tasks, respectively.

### Comparison to State-of-the-Art Models

We present a comparison of the VAST foundation model with state-of-the-art models across various vision-text, audio-text, and multi-modal video-text benchmarks. The corresponding results are summarized in Table 3. Although the primary focus of VAST lies in enhancing omni-modality understanding and generation capabilities, it also demonstrates remarkable performance in image-text,

  
**Vision-Text** &  & **Captioning** &  \\ 
**benchmark** & COCO & Flickr & Flickr(ZS) & COCO & TGIF & MSVD & VQAv2 \\  SOTA & **68.3** & 90.3 & 89.7 & **154.9***** & 78.7 & 60.2 & **84.3** \\ VAST & 68.0 & **91.0**(+0.7) & **90.4**(+0.7) & 149.0* & **79.1**(+0.4) & **60.2** & 80.2 \\  
**Audio-Text** &  &  \\ 
**benchmark** & Clothov1 & Clothov2 & AudioCaps & Clothov1 & Clothov2 & AudioCaps \\  SOTA & 17.5 & 21.5 & 42.2 & 42.3 & 48.8 & **78.7** \\ VAST & **25.1**(+7.6) & **26.9**(+5.4) & **52.0**(+9.8) & **50.7**(+8.4) & **51.9**(+3.1) & 78.2 \\  
**MM Video-Text** &  \\ 
**benchmark** & MSRVTT & YouCook2 & VALOR-32K & VATEX & DiDeMo & ANET \\  SOTA\_Vis & 58.8 & 33.7 & 43.4 & 71.1 & 70.4 & 66.8 \\ SOTA\_MM & 54.4 & 31.3 & 73.2 & 76.9 & 57.6 & 63.4 \\ VAST & **63.9**(+5.1) & **50.4**(+16.7) & **80.0**(+6.8) & **83.0**(+6.1) & **72.0**(+1.6) & **70.5**(+3.7) \\  
**MM Video-Text** &  &  \\ 
**benchmark** & MSRVTT & YouCook2 & VALOR-32K & VATEX & TVC & MSRVTT & MUSIC & ANET \\  SOTA\_Vis & 75.9 & 13.1 & 27.3 & 94.5 & 66.1 & 9.5 & - & 47.9 \\ SOTA\_MM & 74.0 & 190.(55) & 61. & 95.8 & 66.6 & 49.2 & 78.9 & 48.6 \\ VAST & **78.0**(+2.1) & **198.8**(+8.8) & **62.0**(+0.5) & **99.5***(+3.7) & **74.1**(+8.0) & **50.1**(+0.6) & **80.7**(+1.8) & **50.4**(+1.8) \\   

Table 3: Performance comparison between VAST and state-of-the-art methods. VAST has achieved **22 new SOTA results**. Recall@1, CIDEr, and Acc are used as evaluation metrics for retrieval, captioning, and QA tasks, respectively. For captioning tasks, results marked with ‘*’ means that SCST finetuning  is employed. ‘MM benchmark’ means that either audio or subtitle tracks are available in those benchmarks, and SOTA_Vis and SOTA_MM are the best methods with vision track used only or employing multi-modal tracks. **More detailed benchmark introductions, model references and comparison results can be found in Appendix**.

vision-only video-text, and audio-text benchmarks. Specifically, VAST surpasses BEiT-3  and BLIP-2  on the text-to-image retrieval benchmark of Flickr30K under finetuning and zero-shot settings, respectively, establishing new state-of-the-art results in TGIF-QA and MSVD-QA benchmarks. Furthermore, VAST exhibits exceptional performance in audio-text benchmarks, achieving five new state-of-the-art results with significant improvements, due to the abundant generated captions in the VAST-27M corpus.

In the context of multi-modal video-text benchmarks, they can be classified into three categories: subtitle-oriented (YouCook2, TVC) benchmarks that require a comprehensive understanding of subtitles for reasoning, audio-oriented (VALOR-32K) benchmarks that necessitates careful attention to audio cues, and even-oriented benchmarks (others) where both audio and subtitles provide supportive roles to vision. Our results demonstrate that VAST excels across all types of benchmarks, outperforming previous both vision-only and multi-modal state-of-the-art models. Notably, we surpass the GIT2  foundation model, specialized in captioning tasks, by 2.1, 67.6, 5.0, and 8.0 CIDEr points on MSRVTT, YouCook2, VATEX, and TVC captioning benchmarks, respectively, while utilizing only 22.5% of its parameters and 3.4% of its training data size. Furthermore, we achieve significant margins over the vision-audio-text foundation model VALOR  on 11 benchmarks. More detailed comparison results can be found in Appendix.

### Comparison to Open-Source Cross-Modality Training Corpus

In this subsection, we quantitatively compare the quality of the proposed VAST-27M to current open-source video, audio, and audiovisual pretraining corpora. We achieve this by training models on these corpora and fine-tuning them on different types of downstream tasks, including video retrieval (RET), captioning (CAP), and question answering (QA).

**V-T Quality.** We train multiple V-T models on different corpora and fine-tune them on the MSVD and MSRVTT datasets. It is important to note that only the visual contents in videos are utilized for both pretraining and fine-tuning, and all models are trained for 50K steps. As shown in Table 4, the model trained with vision captions in VAST-27M (h) achieves the best results on all six benchmarks, outperforming the baselines and models trained with other corpora by a significant margin. In contrast, model (f), which treats subtitles as captions and can be viewed as raw HD_VILA_100M dataset, performs the worst due to the weak vision-text relations. Model trained on VAST-27M (g) outperforms other corpora but is still inferior to model (h), as the multi-modal captions introduce noise when only vision contents are fed into the model.

    &  &  &  \\   & & RET & CAP & QA & RET & CAP & QA \\  (a) & - & 23.7 & 101.2 & 46.8 & 35.4 & 58.9 & 43.6 \\ (b) & WebVid2.5M & 43.1 & 139.2 & 53.0 & 42.4 & 63.7 & 45.5 \\ (c) & CC14M & 46.1 & 139.2 & 53.9 & 45.2 & 66.4 & 46.3 \\ (d) & LAION-150M & 42.4 & 139.3 & 53.2 & 44.9 & 66.9 & 45.4 \\ (e) & VALOR-1M & 42.3 & 136.7 & 52.6 & 42.8 & 65.3 & 45.5 \\ (f) & VAST-27M (subtitle) & 37.3 & 124.5 & 40.3 & 40.3 & 61.9 & 45.1 \\ (g) & VAST-27M (omc) & 46.8 & 143.2 & 54.3 & 47.5 & 67.4 & 46.7 \\ (h) & VAST-27M (vision caption) & **47.7** & **149.6** & **55.3** & **49.1** & **68.9** & **46.8** \\   

Table 4: Comparisons of V-T quality in VAST-27M and open-sourced vision-text corpora.

    &  &  &  \\   & & RET & CAP & RET & CAP \\  (a) & - & 17.2 & 42.1 & 41.1 & 70.4 \\ (b) & VALOR-1M & 21.0 & 47.3 & 47.4 & 74.8 \\ (c) & WavCaps & 22.0 & 47.2 & 43.2 & 73.0 \\ (d) & VAST-27M (audio caption) & **23.1** & **48.9** & **47.4** & **76.9** \\   

Table 5: Comparisons of A-T quality between VAST-27M and open-sourced audio-text corpora.

**A-T Quality.** We train multiple A-T models on different corpora and fine-tune them on audio-text benchmarks, including Clothov2 and AudioCaps. We observe that VALOR-1M and WavCaps are prone to overfitting, so we adjust the training iterations to obtain the best results for models trained on each corpus instead of training for the same number of iterations. As shown in Table 5, model (d) surpasses the baseline and models trained on both VALOR-1M and WavCaps on all four benchmarks.

**OMV-OMC Quality.** We further investigate the correspondence quality between OMV and OMC in the VAST-27M dataset and compare it primarily to the VALOR-1M dataset. All models undergo 50K training steps with \(_{}\) as the objective, utilizing all modalities in both pretraining and finetuning. The results are presented in Table 6. From the table, we observe that omni-modality pretraining on VAST-27M significantly improves performance across all 7 benchmarks compared to the baseline. Additionally, VAST-27M outperforms VALOR-1M on 5 benchmarks in the MSRVTT and YouCook2 datasets, but lags behind on the VALOR-32K benchmarks due to the consistent video distribution and caption style between VALOR-1M and VALOR-32K. As an additional experiment, we train model (c) on VAST-27M but replace the omni-modality captions generated by LLM with a simple concatenation of vision, audio captions, and subtitles. Model (d) outperforms model (c) on all benchmarks, showcasing the necessity and effectiveness of leveraging the powerful capabilities of LLM to integrate single-modality captions into omni-modality ones.

### Ablation Study

We conduct comprehensive ablation studies on pretraining and finetuning models using different modalities for video representations to demonstrate the strength and necessity of omni-modality foundation model pretraining. Models are evaluated on the MSRVTT (open-domain), YouCook2 (subtitle-important), and VALOR-32K (audio-important) benchmarks. The results are presented in Table 7, when no pretraining is applied, models trained with audio and subtitles incorporated (d) can improve the vision-only baseline on all benchmarks. When omni-modality pretraining is applied, the improvement becomes more evidently on most benchmarks, which can be reflected by the comparison between the green values in model (f) and model (d), demonstrating the effectiveness of proposed method. In addition, model (i) outperforms model (f) on all benchmarks, indicating that, similar to the V-T capability, the OMV-OMC capability also benefits from large-scale pretraining. It is worth noting that model (h) and (f) exhibit inferior results on VALOR-32K benchmarks compared to model (g) and even the vision baseline (e). This degradation can be attributed to the inconsistency between pretraining and finetuning, where all clips have subtitles in VAST-27M while most videos in VALOR-32K lack subtitles. When the modality grouping strategy is applied, model (j) demonstrates

    &  &  &  &  \\   & & & RET & CAP & QA & RET & CAP & RET & CAP \\  (a) & - & 37.6 & 60.3 & 44.6 & 32.6 & 120.9 & 37.8 & 40.8 \\ (b) & VALOR-1M & 45.4 & 67.8 & 46.4 & 29.3 & 120.1 & **70.8** & **53.9** \\ (c) & VAST-27M (omc w/o LLM) & 44.4 & 71.4 & 47.7 & 36.9 & 159.0 & 49.9 & 48.1 \\ (d) & VAST-27M (omc) & **53.0** & **71.7** & **47.9** & **44.0** & **187.5** & 58.8 & 48.5 \\   

Table 6: Comparisons of OMV-OMC quality between VAST-27M and VALOR-1M.

    &  &  &  &  &  \\   & & & RET & CAP & QA & RET & CAP & RET & CAP \\  (a) & - & V & 35.4 & 58.7 & 43.7 & 6.2 & 77.0 & 29.1 & 36.6 \\ (b) & - & VA+A & 37.3(+1.9) & 61.1(+2.4) & 44.6(+0.9) & 7.1(+0.9) & 71.4(+2.9) & 34.8(+3.9) & 20.2(+3.6) \\ (c) & - & VA+S & 35.8(+0.4) & 59.1(+0.4) & 43.6(+0.1) & 31.2(+5.0) & 119.1(+2.1) & 29.6(+0.5) & 38.1(+1.5) \\ (d) & - & VA+A+S & 37.6(+2.2) & 60.3(+1.6) & 44.6(+0.9) & 32.6(+2.6) & 120.9(+3.3) & 37.8(+8.7) & 40.8(+4.2) \\  (e) & V & V & 47.5 & 67.4 & 46.7 & 14.0 & 93.6 & 56.5 & 45.0 \\ (f) & V & VA+A+S & 49.2(+1.7) & 70.6(+3.2) & 47.3(+3.0) & 30.4(+1.6) & 130.1(+36.5) & 58.5(+2.0) & 44.9(+0.1) \\ (g) & VA+A & VA & 53.8(+6.3) & 70.9(+3.5) & 47.4(+0.7) & 30.3(+16.3) & 130.7(+37.1) & 62.6(+6.1) & 46.3(+1.3) \\ (h) & VA+S & V+S & 49.5(+2.0) & 68.9(+1.5) & 47.4(+0.7) & 24.6(+2.8) & 186.5(+2.9) & 24.8(+3.8) & 43.6(+1.4) \\ (i) & VA+A+S & VA+S & 53.0(+5.7) & 57.7(+4.3) & 43.7(+2.1) & 44.0(+2.8) & 187.5(+93.9) & 58.2(+2.3) & 48.5(+3.5) \\ (j) & VA+A+S & VA+A+S & VA+A+S & VA+A+S & 55.3(+7.8) & 71.7(+4.3) & 48.2(+1.5) & 45.9(+3.9) & 188.0(+94.4) & 62.2(+5.7) & 48.4(+3.4) \\   

Table 7: Downstream performances of models trained on VAST-27M with different modalities used during training and finetuning.

good generalization across all types of tasks, as the correlations between text and every modality group have been explored during pretraining.

## 6 Conclusion, Broader Impact and Limitation

In this paper, we introduce the VAST-27M corpus, a large-scale omni-modality video caption dataset, aimed at advancing research in multi-modality pretraining. Each video clip in the dataset is accompanied by automatically generated vision and audio captions, as well as an omni-modality caption that integrates information from vision, audio, and subtitles using a pre-existing Large Language Model (LLM). We also train a unified foundation model called VAST, capable of understanding and connecting the various modalities in videos and captions. Through extensive evaluations, VAST demonstrates its effectiveness in a wide range of vision-text, audio-text, and multi-modal video-text tasks, including retrieval, captioning, and question answering, surpassing existing state-of-the-art methods on public cross-modality benchmarks.

The advancements in multi-modal understanding and generation of video content can have a significant impact on various domains such as entertainment, education, security, transportation, and healthcare, among others. The development of omni-modality foundation models, like VAST-27M and VAST, can contribute to the progress and practical applications in these areas. However, it is important to acknowledge the limitations of our method. For instance, there is a need for more diverse and larger-scale omni-modality corpora beyond VAST-27M. Furthermore, although VAST already supports a wide range of downstream tasks, the integration of LLM is necessary to further enhance its generalization capabilities. Additionally, due to the reason that the data collection process of VAST-27M have used LLM, and the training process of video and audio captioners have utilized some open-sourced cross-modality corpus, so VAST-27M dataset and VAST model may suffer the same bias of those datasets and models.