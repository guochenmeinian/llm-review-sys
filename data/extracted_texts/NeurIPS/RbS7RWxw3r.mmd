# Iteratively Refined Behavior Regularization for Offline Reinforcement Learning

Yi Ma\({}^{1,2}\), Jianye Hao\({}^{3,4}\), Xiaohan Hu\({}^{3}\), Yan Zheng\({}^{3}\) Chenjun Xiao\({}^{5}\)

\({}^{1}\)School of Computer and Information Technology, Shanxi University, mayi@sxu.edu.cn

\({}^{2}\)Key Laboratory of Computational Intelligence

and Chinese Information Processing of Ministry of Education

\({}^{3}\)College of Intelligence and Computing, Tianjin University

{jianye.hao, huxiaohan, yanzheng}@tju.edu.cn

\({}^{4}\)Noah's Ark Lab, Huawei

\({}^{5}\)The Chinese University of Hongkong, Shenzhen, chenjunx@cuhk.edu.cn

Corresponding authors.

###### Abstract

One of the fundamental challenges for offline reinforcement learning (RL) is ensuring robustness to data distribution. Whether the data originates from a near-optimal policy or not, we anticipate that an algorithm should demonstrate its ability to learn an effective control policy that seamlessly aligns with the inherent distribution of offline data. Unfortunately, _behavior regularization_, a simple yet effective offline RL algorithm, tends to struggle in this regard. In this paper, we propose a new algorithm that substantially enhances behavior-regularization based on _conservative policy iteration_. Our key observation is that by iteratively refining the reference policy used for behavior regularization, conservative policy update guarantees gradually improvement, while also implicitly avoiding querying out-of-sample actions to prevent catastrophic learning failures. We prove that in the tabular setting this algorithm is capable of learning the optimal policy covered by the offline dataset, commonly referred to as the _in-sample optimal_ policy. We then explore several implementation details of the algorithm when function approximations are applied. The resulting algorithm is easy to implement, requiring only a few lines of code modification to existing methods. Experimental results on the D4RL benchmark indicate that our method outperforms previous state-of-the-art baselines in most tasks, clearly demonstrate its superiority over behavior regularization.

## 1 Introduction

Reinforcement learning (RL) has achieved considerable success in various decision-making problems, including games , software automation testing , recommendation and advertising , logistics optimization  and robotics . A critical obstacle that hinders a broader application of RL is its trial-and-error learning paradigm. For applications such as education, autonomous driving and healthcare, active data collection could be either impractical or dangerous . Instead of learning actively, a more favorable approach is to employ scalable data-driven learning methods that can utilize existing data and progressively improve as more training data becomes available. This motivates _offline RL_, of which the primary objective is to learn a control policy solely from previously collected data without online interactions with environment.

Offline datasets frequently offer limited coverage of the state-action space. Directly utilizing standard RL algorithms to such datasets can result in extrapolation errors when bootstrapping from out-of-distribution (OOD) state-actions, consequently causing significant overestimations in value functions.

To address this, previous works impose various types of constraints to promote pessimism towards accessing OOD state-actions. One simple yet effective approach is _behavior regularization_, which penalizes significant deviations from the behavior policy that collects the offline dataset [7; 8; 9; 10; 11].

A fundamental challenge in behavior regularization lies in its performance being closely tied to the underlying data distribution. Previous works suggest that it often yields subpar results when applied to datasets originating from suboptimal policies [12; 13]. To better understand this phenomenon, we investigate how the quality of a behavior policy influences the performance of behavior regularization through utilizing _percentile behavior cloning_. Given an offline dataset, we create three sub-datasets by filtering trajectories representing the top 5%, median 5%, and bottom 5% performance. We then leverage these sub-datasets to train three policies--referred to as _top_, _median_, and _bottom_--using behavior cloning. We then develop _TD3 with percentile behavior cloning (TD3+%BC)_, which optimizes the policy by \(_{}_{s}[v^{}(s)-((s)-_{\%}(s))]\) where \(_{\%}\{,,\}\). This variant of TD3+BC simply replaces the behavior policy with the percentile cloning policy. We refer to \(_{\%}\) as the _reference policy_. Our main hypothesis is that a better reference policy (e.g. top) can dramatically improve the efficiency of behavior regularization compared to a worse reference policy (e.g. bottom). Figure 1 provides the results on _hopper-medium-replay_, _hopper-medium-expert_, _walker-medium-replay_ and _walker-medium-expert_ from the D4RL datasets . A few key observations. _First_, the efficacy of behavior regularization is strongly contingent on the reference policy. For example, TD3+top%BC dominates TD3+bottom%BC across all benchmarks. This confirms our hypothesis. _Second_, behavior regularization guarantees improvement. No matter using top, median or bottom, TD3+%BC produces a better or similar policy compared to the reference policy, clearly demonstrating the advantage of behavior regularization over behavior cloning.

Inspired by these findings, we ask: _is it possible to automatically discover good reference policies from the data to increase the efficiency of behavior regularization?_ This paper attempts to give an affirmative answer to this question. Our key observation is that Conservative Policy Iteration, an algorithm that has been widely used for online RL [16; 17; 18; 19], can also be extended to the offline setting with minimal changes. The core concept behind this approach hinges on the iterative refinement of the reference policy utilized for behavior regularization. This iterative process enables the algorithm to implicitly avoid resorting to out-of-sample actions, all while ensuring continuous policy enhancement. We provide both theoretical and empirical evidence to establish that, for the tabular setting, our approach is capable of learning the optimal policy covered by the offline dataset, commonly referred to as the in-sample optimal policy [12; 13]. We then discuss several implementation details of the algorithm when function approximations are applied. A previous work STR  shares the similar idea with CPI. However, the theoretical foundation and the resulting algorithms are quite distinct, which is discussed in appendix. Our method can be seamlessly implemented with just a few lines of code modifications built upon TD3+BC . We evaluate our method on the D4RL benchmark . Experiment results show that it outperform previous state-of-the-art methods on the majority of tasks, with both rapid training speed and reduced computational overhead.

Figure 1: The dashed lines indicate the performance of reference policies trained on filtered sub-datasets consisting of top 5%, median 5%, and bottom 5% trajectories. The solid lines indicate the performance of TD3 with different reference policies. It can be concluded that behavior regularization has advantage over behavior cloning and the efficacy of behavior regularization is strongly contingent on the reference policy.

## 2 Preliminaries

### Markov Decision Process

We consider Markov Decision Process (MDP) determined by \(M=\{,,P,r,\}\), where \(\) and \(\) represent the state and action spaces. The discount factor is given by \([0,1)\), \(r:\) denotes the reward function, \(P:()\) defines the transition dynamics 2. Given a policy \(:()\), we use \(^{}\) to denote the expectation under the distribution induced by the interconnection of \(\) and the environment. The _value function_ specifies the future discounted total reward obtained by following policy \(\),

\[V^{}(s)=^{}[_{t=0}^{}^{t}r(s_{t},a_{t}) s_{0}=s]\,,\] (1)

The _state-action value function_ is defined as

\[Q^{}(s,a)=r(s,a)+_{s^{} P(|s,a)}[V^{}(s^ {})]\,.\] (2)

There exists an _optimal policy_\(^{*}\) that maximizes values for all states \(s\). The optimal value functions, \(V^{*}\) and \(Q^{*}\), satisfy the _Bellman optimality equation_,

\[V^{*}(s) =_{a}r(s,a)+_{s^{}}[V^{*}(s^{})]\,,\] (3) \[Q^{*}(s,a) =r(s,a)+_{s^{}}[_{a^{}}Q^{*} (s^{},a^{})]\,.\]

### Offline Reinforcement Learning

In this work, we consider learning an optimal decision making policy from previously collected offline dataset, denoted as \(=\{s_{i},a_{i},r_{i},s^{}_{i}\}_{i=0}^{n-1}\). The dataset is generated following this procedure: \(s_{i} p,a_{i}_{},s^{}_{i} P(|s_{i},a_{i}),r_{i}=r(s_{i},a_{i})\), where \(\) represents an unknown probability distribution over states, and \(_{}\) is an _unknown behavior policy_. In offline RL, the learning algorithm can only take samples from \(\) without collecting new data through interactions with the environment.

Behavior regularization is a simple yet efficient technique for offline RL [7; 8]. It imposes a constraint on the learned policy to emulate \(_{}\) according to some distance measure. A popular choice is to use the KL-divergence [11; 10]3,

\[_{}_{a}[Q(s,a)]- D_{}( (s)||_{}(s))\,\,\,,\] (4)

where \(>0\) is a hyper-parameter. Here, \(Q\) is some value function. Typical choices include the value of \(_{}\), or the value of the learned policy \(\)[8; 11]. As shown by Figure 1, the main limitation of behavior regularization is that it relies on the dataset being generated by an expert or near-optimal \(_{}\). When used on datasets derived from more suboptimal policies--typical of those prevalent in real-world applications--these methods do not yield satisfactory results.

## 3 Iteratively Refined Behavior Regularization

In this paper, we introduce a new offline RL algorithm by exploring idea of iteratively improving the reference policy used for behavior regularization. Our primary goal is to improve the robustness of existing behavior regularization methods while making minimal changes to existing implementation. We first introduce _conservative policy optimization (CPO)_, a commonly used technique in online RL, then describe how it can also shed some light on developing offline RL algorithms.

Let \(>0\), the CPO update the policy for any \(s\) with the following policy optimization problem

\[_{}_{a}[Q^{}(s,a)]- D_{ }((s)||(s))\,\,\,,\] (5)which generalizes behavior regularization (4) using an arbitrary _reference policy_\(\). The idea is to optimize a policy without moving too far away from the reference policy to increase learning stability. As shown in the following proposition, this conservative policy update rule enjoys two intriguing properties: _first_, it guarantees policy improvement over the reference policy \(\); and _second_, the updated policy still stays on the support of the reference policy. These properties also echo our empirical findings presented in Figure 1: as a special case of CPO with \(=_{}\), behavior regularization is stable in offline learning and always produces a better policy than \(_{}\).

**Proposition 1**.: _Let \(^{*}\) be the optimal policy of (5). For any \(s\), we have that \(V^{^{*}}(s) V^{}(s)\) ; and \(^{*}(a|s)=0\) given \((a|s)=0\)._

Proof.: The proof of this result, as well as other results, are provided in the Appendix. 

In summary, the conservative policy update (5) _implicitly guarantees policy improvement constrained on the support of the reference policy \(\)_. By extending this key observation in an iteratively manner, we obtain the following _Conservative Policy Iteration (CPI)_ algorithm for offline RL. It starts with the behavior policy \(_{0}=_{}\). Then in each iteration \(t=0,1,2,\), the following computations are done:

* _Policy evaluation_: compute \(Q^{_{t}}\) and;
* _Policy improvement_: \( s\), \(_{t+1}=*{arg\,max}_{}_{a}[Q^{_{t}} (s,a)]- D_{}(||_{t})\).

In this approach, the algorithm commences with the behavior policy and proceeds to iteratively refine the reference policy used for behavior regularization. Thanks to the conservative policy update, CPI ensures policy improvement while mitigating the risk of querying any OOD actions that could potentially introduce instability to the learning process.

We note that CPO is a special case of _mirror descent_ in the online learning literature . Extending this technique to sequential decision making has been previously investigated in online RL [16; 17; 18; 19]. In particular, the _Politex_ algorithm considers exactly the same update as (5) for online RL . This algorithm can be viewed as a softened or averaged version of policy iteration. Such averaging reduces noise of the value estimator and increases the robustness of policy update. Perhaps surprisingly, our key contribution is to show this simple yet powerful policy update rule also facilitates offline learning, as it guarantees policy improvement while implicitly avoid querying OOD actions.

### Theoretical Analysis

We now analyze the convergence properties of CPI. In particular, we consider the tabular setting with finite state and action space. Our analysis reveals that in this setting, CPI converges to the optimal policy that are well-covered by the dataset, commonly referred to as the in-sample optimal policy. Consider the _in-sample Bellman optimality equation_

\[V^{*}_{_{}}(s)=_{a:_{}(a|s)>0}\{r(s,a)+ _{s^{} P(|s,a)}[V^{*}_{_{}} (s^{})]\,\}.\] (6)

This equation explicitly avoids bootstrapping from OOD actions while still guaranteeing optimality for transitions that are well-supported by the data. A fundamental challenge lies in the development of scalable algorithms for its resolution [23; 12; 13]. Our next result shows that CPI provides a simple yet effective solution.

**Theorem 1**.: _We consider tabular MDPs with finite \(\) and \(\). Let \(_{t}\) be the produced policy of CPI at iteration \(t\). There exists a parameter \(>0\) such that for any \(s\)_

\[V^{*}_{_{}}(s)-V^{_{t}}(s)}|}{t}}\;.\] (7)

To ensure robustness to data distribution, an offline RL algorithm must possess the _stitching_ capability, which involves seamlessly integrating suboptimal trajectories from the dataset. In-sample optimality provides a formal definition to characterize this ability. As discussed in previous works and confirmed by our experiments, existing behavior regularization approaches, such as TD3+BC , fall short in this regard. In contrast, Theorem 1 suggests that iteratively refining the reference policy for behavior regularization can enable the algorithm to acquire the stitching ability, marking a significant improvement over existing approaches. Our empirical studies in the experiments section further support this claim.

## 4 Practical Implementations

In this section we discuss how to implement CPI properly when function approximation is applied. Throughout this section we develop algorithms for continuous actions. Extension to discrete action setting is straightforward. We build CPI as an actor-critic algorithm. We learn an actor \(_{}\) with parameters \(\), and critic \(Q_{}\) with parameters \(\). The policy \(_{}\) is parameterized using a Gaussian policy with learnable mean . We also normalize features of every states in the offline dataset as discussed in .

CPI consists of two steps at each iteration. The first step involves policy evaluation, which is carried out using standard TD learning: we learn the critic by

\[_{}_{s,a,r,s^{},a^{}_{ }(s^{})}[(r+ Q_{}(s^{},a^{})-Q_{}(s,a))^{2}]\,,\] (8)

where \(Q_{}\) is a target network. We also apply the double-Q trick to stabilize training . The policy improvement step requires more careful algorithmic design. The straightforward implementation is to use gradient descent on the following

\[_{^{}}_{s}_{a _{^{}}}[Q_{}(s,a)]- D_{}(_{}(s) ||(s))\,\,.\] (9)

Here, the reference policy \(\) is a copy of the current parameters \(\) and kept frozen during optimization. This leads to the so-called _iterative actor-critic_ or _multi-step_ algorithm . In their study,  observe that this iterative algorithm frequently encounters practical challenges, mainly attributed to the substantial variance associated with off-policy evaluation. Similar observations have also been made in our experiments (See Figure 4). We conjecture that while Proposition 1 establishes that the exact solution of (9) remains within the data support when the actor is initialized as \(_{}=_{}\), practical implementations often rely on a limited number of gradient descent steps for optimizing (9), thus could suffer from our-of-support samples. This leads to policy optimization errors which are further exacerbated iteratively. An option to approximate the in-support learning is to utilizes importance sampling on the dataset to mimic sampling from the current policy. However, the high variance of importance sampling could cause the training unstable . We instead find it is useful to add the original behavior regularization,

\[_{^{}}_{s} _{a_{^{}}}[Q_{}(s,a)]-  D_{}(_{}(s)||(s))\] \[-(1-)D_{}(_{}(s)||_{ }(s))\,\,,\] (10)

which further constrains the policy on the support of data to enhance learning stability. The parameter \(\) balances between one-step policy improvement and behavior regularization. Note that the term \(D_{}(_{}(s)||(s))\) in CPI is the only difference compared to TD3+BC . In practice this can be done with one line code modification based on TD3+BC implementations.

```
1:Initialize actors \(_{1}\), \(_{2}\) and critic networks \(q_{1}\), \(q_{2}\) with random parameters \(^{1}\), \(^{2}\), \(^{1}\), \(^{2}\); target networks \(^{1}^{1}\), \(^{2}^{2}\), \(^{1}^{1}\), \(^{2}^{2}\).
2:for\(t=0,1,2...,T\)do
3: Sample a mini-batch of transitions from dataset
4: Update the parameters of critic \(^{i}\) using equation 8
5:if\(t\) mod \(2\)then
6: // For CPI
7: Copy the historical snapshot of \(^{1}\) to \(^{2}\) and update \(^{1}\) using equation 10
8: // For CPI-RE
9: Choose the current best policy between \(^{1}\) and \(^{2}\) as the reference policy according to O-value
10: Update \(^{1}\) and \(^{2}\) using equation 10 respectively
11: Update target networks
12:endif
13:endfor ```

**Algorithm 1** CPI & CPI-REEnsembles of Reference PolicyOne limitation of (10) lies in the potential for a negligible difference between the learning policy and the reference policy, due to the limited gradient steps when optimizing. This minor discrepancy may restrict the policy improvement over the reference policy. To improve the efficiency of policy improvement, we explore the idea of using an ensembles of references policies. In particular, we apply two policies with independently initialized parameters \(^{1}\) and \(^{2}\). Let \(Q_{^{1}}\) and \(Q_{^{2}}\) be the value functions of these two policies respectively. When updating the parameters \(^{i}\) for \(i\{1,2\}\), we choose the current best policy as the reference policy, where the superiority is decided according to the current value estimate. In other words, we only enable a superior reference policy to elevate the performance of the learning policy, preventing the learning policy from being dragged down by an inferior reference policy. We call this algorithm _Conservative Policy iteration with Reference Ensembles (CPI-RE)_. We give the pseudocode of both CPI and CPI-RE in Algorithm 1. For CPI, the training process is exactly the same with that of TD3+BC except for the policy updating (marked in pink). The difference between CPI-RE and CPI is also reflected in the policy updating (marked in teal).

## 5 Experiment

We subject our algorithm to a series of rigorous experimental evaluations. We first present an empirical study to exemplify CPI's optimality in the tabular setting. Then, we compare the practical implementations of CPI, utilizing function approximation, against prior state-of-the-art algorithms in the D4RL benchmark tasks , to highlight its superior performance. In addition, we also present the resource consumption associated with different algorithms. Finally, comprehensive analysis of various designs to scrutinize their impact on the algorithm's performance are provided.

### Optimality in the tabular setting

We first conduct an evaluation of CPI within two GridWorld environments. The first environment's map comprises a \(7*7\) grid layout and the second environment's map is the FourRoom . In both environments, the agent is tasked with navigating from the bottom-left to the goal positioned in the upper-right in as few steps as possible. The agent has access to four actions: _[up, down, right, left]_. The reward is set to -1 for each movement, with a substantial reward of 100 upon reaching the goal; this incentivizes the agent to minimize the number of steps taken. Each episode is terminated after 30 steps, and \(\) is set to 0.9. We use an _inferior behavior policy_ to collect 10k transitions, of which the action probability is _[up:0.1, down:0.4, right:0.1, left:0.4]_ at every state in the \(7*7\) grid environment. For the FourRoom environment, we use three types of behavior policy to collect data: (1) Expert dataset: collect 10k transitions with the optimal policy; (2) Random dataset: collect 10k transitions

Figure 2: Training curves of BR, InAC and CPI on 7*7-GridWorld and FourRoom. CPI converges to the oracle across various \(\) and environments settings, similar to the in-sample optimal policy InAC.

[MISSING_PAGE_FAIL:7]

final 10 evaluations for Mujoco and Adroit, and the final 100 evaluations for Antmaze, are reported. More details of the experiments are provided in the Appendix.

Our experiment results, summarized in Table 1, clearly show CPI outperforms baselines in overall. In most Mujoco tasks, CPI surpasses the extant, widely-utilized algorithms, and it only slightly trails behind the state-of-the-art EDAC method. For Antmaze and Adroit, CPI's performance is on par with the top-performing methods such as POR and Diffusion-QL. We also evaluate the resource consumption of different algorithms from two aspects: (1) runtime per training epoch (1000 gradient steps); (2) GPU memory consumption. The results in Table 1 show that CPI requires fewer resources which could be beneficial for practitioners.

### Ablation Studies

#### 5.3.1 Effect of Reference Ensemble

We provide learning curves of CPI and CPI-RE on Antmaze in Figure 3 to further show the efficacy of using ensemble of reference policies. CPI-RE exhibits a more stable performance compared to vanilla CPI, also outperforming IQL. Learning curves on other domains are provided in the Appendix.

#### 5.3.2 Effect of using Behavior Regularization

A direct implementation based on the theoretical results observed in the tabular setting could be derived. Specifically, we initialize \(_{}=_{}\) and execute the CPI without incorporating behavior regularization. For each gradient step, we perform _one-step_ or _multi-step_ updates for both policy evaluation and policy improvement. The empirical outcomes presented in Fig.4 underscore the poor performance of this straightforward approach. Similar trends are also observed in . Such phenomenon arises primarily because, in the presence of function approximation, the deviation emerges when policy optimization does not necessarily remain within the defined support without the behavior regularization.

#### 5.3.3 Effect of using different KL strategies

In our implementation of the CPI method, we employ the reverse KL divergence. This is distinct from the forward KL divergence approach adopted by . A comprehensive ablation of incorporating different KL divergence strategies in CPI is presented in Fig.5. As evidenced from the

Figure 4: Effect of using Behavior Regularization. Without it, both one-step and multi-step updating methods at each gradient step suffer from deviation derived from out-of-support optimization.

Figure 3: The learning curves of CPI, CPI-RE, and IQL on Antmaze. With reference ensemble, CPI-RE exhibits a more stable performance compared to vanilla CPI, also outperforming IQL.

results, our reverse KL-based CPI exhibits superior performance compared to the forward KL-based implementations. Implementations details of CPI with forward KL are provided in Appendix.

#### 5.3.4 Effect of Hyper Parameters

Figure 6 illustrates the effects of using different hyperparameters \(\) and \(\), offering valuable insights for algorithm tuning. The weighting coefficient \(\) regulates the extent of behavior policy integration into the training and affects the training process. As shown in Figure 5(a), when \(=0.1\), the early-stage performance excels, as the behavior policy assists in locating appropriate actions in the dataset. However, this results in suboptimal final convergence performance, attributable to the excessive behavior policy constraint on performance improvement. For larger values, such as 0.9, the marginal weight of the behavior policy leads to performance increase during training. Unfortunately, the final performance might be poor. This is due to that the policy does not have sufficient behavior cloning guidance, leading to a potential distribution shift during the training process. Consequently, we predominantly select a \(\) value of 0.5 or 0.7 to strike a balance between the reference policy regularization and behavior regularization. The regularization parameter \(\) plays a crucial role in determining the weightage of the joint regularization relative to the Q-value component. We find that (Figure 5(b)) \(\) assigned to dataset of higher quality and lower diversity (e.g., expert dataset) ought to be larger than those associated with datasets of lower quality and higher diversity (e.g., medium dataset).

### Online Fine-tuning

The distinctive feature of CPI lies in its policy iteration training, which makes it particularly well-suited for the online fine-tuning process. Fine adjustments to CPI enable its seamless application in online fine-tuning. In an online setting, as the agent can interact with the environment, there is a need to progressively enhance the level of exploration during the training process. To achieve this, the \(\) parameter in formula 10 is modified. Specifically, in the online training process, the weight of \(D_{}(_{}(s)||(s))\) remains constant, while the weight of \(D_{}(_{}(s)||_{}(s))\) decreases exponentially, eventually reducing to 0.1. This progressive adaptive exploration mechanism allows the algorithm to adopt a conservative exploration strategy in its initial stages and then gradually expand its exploration scope, thereby enhancing the model's adaptability in the online learning phase.

Figure 5: Effect of using different KL strategies. Reverse KL-based CPI exhibits superior performance compared to the forward KL-based CPI. See Appendix for details of forward KL.

Figure 6: Hyperparameters ablation studies. The tuning experience drawn from this include: (1) \(\) could mostly be set to 0.5 or 0.7; (2) The higher the dataset quality is, the higher \(\) should be set.

Empirical evaluations were conducted on nine Mujoco datasets, including halfcheetah-medium-v2, halfcheetah-medium-replay-v2, halfcheetah-medium-replay-v2, hopper-medium-v2, hopper-medium-replay-v2, hopper-medium-expert-v2, walker2d-medium-replay-v2, and walker2d-medium-replay-v2, and walker2d-medium-reper-v2. We compare our algorithms with several baselines including TD3+BC, IQL, Cal-QL  and PEX . As shown in Figure 7, the CPI algorithm achieves exceptional overall performance compared to the state-of-the-art offline-to-online algorithms Cal-QL and PEX. Detailed results for each Mujoco dataset are provided in the Appendix.

## 6 Conclusion

In this paper, we propose an innovative offline RL algorithm termed _Conservative Policy iteration (CPI)_. By iteratively refining the policy used for regularization, CPI progressively improves itself within the behavior policy's support and provably converges to the in-sample optimal policy in the tabular setting. We then propose practical implementations of CPI for solving continuous control tasks. Experimental results on the D4RL benchmark show that CPI surpass previous cutting-edge methods in a majority of tasks of various domains, offering both expedited training speed and diminished computational overhead.

Nonetheless, our study is not devoid of limitations. For instance, our method's performance with function approximation is contingent upon the selection of two hyperparameters, which may necessitate tuning for optimal results.