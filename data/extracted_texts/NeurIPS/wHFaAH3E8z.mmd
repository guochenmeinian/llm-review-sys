# FasMe: Fast and Sample-efficient Meta Estimator

for Precision Matrix Learning in Small Sample Settings

Xiao Tan\({}^{1,3}\) Yiqin Wang\({}^{1}\) Yangyang Shen\({}^{1}\) Dian Shen\({}^{1}\) Meng Wang\({}^{2}\) Peibo Duan\({}^{3}\) Beilun Wang\({}^{1,*}\)

\({}^{1}\)School of Computer Science and Engineering, Southeast University, Nanjing, China

\({}^{2}\)College of Design and Innovation, Tongji University, Shanghai, China

\({}^{3}\)Department of Data Science & AI, Monash University, Melbourne, Australia

{xtan, 213200449, seu_syy, dshen, beilun}@seu.edu.cn

mengwangtj@tongji.edu.cn peibo.duan@monash.edu

###### Abstract

Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques. To this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to \(O( p/K)\) per meta-training task and \(O(||)\) for the meta-testing task. Moreover, the hereby proposed model only needs \(O(p^{-1})\) time and \(O(p)\) memory for converging to an \(\)-accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings.

+
Footnote â€ : * Corresponding Author.

## 1 Introduction

Precision matrix estimation aims to exploit conditional dependency relationships between random variables, which plays a vital role in high-dimensional statistical learning with a wide range of applications in areas such as genetics , neuroscience , and social networks. For instance, researchers are examining the gene network derived from the gene expression data of patients to investigate a rare disease. However, in cases involving rare diseases, the analysis is hindered by the high dimensionality of genetic datasets, where the number of features significantly exceeds the number of samples. For example, the Cholangocarcinoma dataset in TCGA contains only 51 samples but thousands of genes. In this context, we define a "small sample setting" as a scenario in which the number of samples is less than one-tenth of the number of dimensions. Although inferring such relationship graphs in small sample settings is challenging, it can provide valuable insights into the genetic characteristics underlying this disease and its etiology.

Precision matrix estimator inherently has the capability to handle datasets where the number of dimensions \(p\) is equal to or greater than the number of samples \(n\). However, this capability has itslimits, and maintaining effectiveness requires keeping the sample size at a certain level.  provides the lower bound of the sample size as \((d^{2} p)\) for well-performed graph recovery under the sub-Gaussian assumption, where \(d\) is the maximum node degree of the graph. Consequently, when the sample size is reduced to the aforementioned "small sample" levels, the sample size falls short of this theoretical lower bound with a relatively high probability due to the uncertainty of \(d\). This indicates insufficient data support for most methods, like GLasso  (detailed in Background), to recover the graph structure effectively, as shown in Figure 1. To address this issue, some methods use multi-task learning, integrating heterogeneous datasets to reduce the sample size bound. For example, the regularized methods  and  show that each task only needs \(O( K+ p)\) and \(O(K+ p)\) samples, respectively, for proper estimation, and \(K\) denotes the number of tasks. Although these methods are more capable of handling small sample problems, they concurrently introduce significant computational burdens. Specifically, integrating a new task necessitates repeated joint retraining, thereby resulting in substantial computational inefficiencies.

To address these challenges, meta-learning emerges as a promising paradigm, as it equips models to efficiently learn a new task using minimal data through the application of meta-knowledge. Such methods are particularly attractive for precision matrix estimation in small sample settings, as they diminish the reliance on extensive data and reduce computational demands. For example, state-of-the-art approaches from the class of Model-Agnostic Meta-Learning (MAML)  have achieved favorable performance in many meta-learning tasks. The strength of MAML lies in its ability to quickly adapt to new tasks (meta-testing dataset) with fewer samples by optimizing initial parameters using data from related tasks (meta-training dataset).

However, the design of MAML often emphasizes sample efficiency and fast adaptation, typically without a comprehensive and robust theoretical guarantee for its performance. While empirical improvements are sufficient for many neural network applications, they may lead to incorrect predictions when applied to precision matrix estimation, where precise and reliable results are crucial. Such empirical-only strategies typically rely on extensive training data. However, in precision matrix estimation, the available data is often significantly limited. For example, the dropout technique , which lacks a strict theoretical foundation, is frequently used in neural network training with millions of samples. In contrast, in precision matrix estimation, the volume of available data often falls short of the number of feature dimensions. Furthermore, MAML-based methods adapt to new tasks by performing gradient descent from meta-learned parameters, but as noted by , the computation and memory demands escalate dramatically as the number of features \(p\) increases, presenting significant challenges.

Recent studies [11; 12] have made initial attempts to successfully integrate meta-learning with precision matrix estimation. Despite achieving sample-efficiency through meta-learning, their methods exhibit several limitations: Restrictive Data Assumptions, and Ineffective Adaption to New Tasks. Firstly, they assume that the edges of a new task must be subsets of existing edges (where existing edges can be seen as optimized initial parameters in MAML), thereby constraining the discovery of novel connections in real-world applications. Besides, these works state that they use the state-of-the-art algorithm BigQuic, which speeds up the gradient calculation step with a block-coordinate descent Newton method, to solve a new task. The algorithm uses between \(O(p)\) and \(O(p^{3})\) time and \(O(p^{2})\) and \(O(p)\) memory per iteration. When the number of samples is extremely small, it takes a large number of iterations to converge to an accurate solution, leading to poor generalization performance.

Herein, we propose a novel approach for Fast and Sample-efficient Meta Precision Matrix Learning, FasMe. Our approach first extracts meta-knowledge as the shared pattern across auxiliary tasks through a multi-task learning diagram. We then adopt a maximum determinant matrix completion algorithm with meta-knowledge constraints to achieve fast adaptation in high-dimensional settings. In detail, our contributions can be summarized as follows:

Figure 1: Frobenius norm error (\(\)) and Matthews Correlation Coefficient (MCC, \(\)) V.S. sample size using GLasso. We fix \(p=200\) and vary \(n\) from \(10\) to \(400\) with step \(10\). The model performance drops sharply around \(n=20\), indicating its inadequacy for small sample.

* A novel meta-learning-based model: We propose a multi-task precision matrix estimator that extracts the meta-knowledge from the auxiliary tasks. We then utilize the maximum determinant matrix completion algorithm to learn a new precision matrix in the meta-testing.
* Efficient adaptation: We design a recursive closed-form algorithm to speed up the adaptation in the novel task. Our model converges to an \(\)-accurate solution in \(O(p^{-1})\) time and \(O(p)\) memory. It significantly outperforms the state-of-the-art ones.
* Theoretical guarantee: We also theoretically prove that for \(p\)-dimensional multivariate sub-Gaussian random vectors and \(K\) auxiliary tasks with meta-knowledge \(\), the sample complexity of our method is \(O( p/K)\) per auxiliary task for meta-knowledge and \(O(||)\) for the new task. It relaxes sample size requirements with the application of meta-learning, equipping FasMe with the ability to handle small sample settings.
* Experimental evaluation: On multiple synthetic datasets, FasMe outperforms the other baselines. Specifically, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy. In the real-world experiment, FasMe obtains the minimum log-determinant Bregman divergence and performs better than the other baselines.

## 2 Background

### Meta-Learning

The last years have witnessed a tremendous interest in methods for meta-learning, especially in a wide range of data-limited applications including medical image analysis [13; 14], language modeling [15; 16], and object detection [17; 18; 19]. Meta-learning is a machine learning technique that allows a model to learn from prior knowledge, or meta-knowledge, to improve its performance on new tasks and speed up the learning process [20; 21]. The process typically involves two stages: 1) Meta-training: In this stage, the model is trained on a set of related tasks, also known as auxiliary tasks . The goal of this step is to extract meta-knowledge from these tasks, which can be used to improve the performance of the model on new tasks. This step helps the model to learn how to quickly and effectively adapt to new tasks by identifying the shared patterns or features across the auxiliary tasks . 2) Meta-testing: In this stage, the model is tested on a new task, also known as the target task . The goal of this step is to apply the meta-knowledge learned during the meta-training step to improve the model's performance on the target task. Meta-knowledge is used to quickly and effectively adapt the model to the new task, without the need for additional training data. Overall, meta-learning allows a model to better generalize to new tasks by leveraging prior knowledge, thereby making it more sample-efficient and fast.

### Precision Matrix Estimation

Precision matrix estimation is the problem of finding the inverse of the covariance matrix of a multivariate random variable \(=^{-1}\). It is particularly useful in scenarios where the number of samples is equal to or smaller than the number of variables since the covariance matrix is not invertible. The graphical lasso, namely GLasso , is a typical penalized maximum likelihood estimator for precision matrix \(\) inference under the Gaussian assumption on the dataset \(^{n p}\). The popular estimator can be written as:

\[=*{arg\,min}_{ 0}\ -()+< ,>+\|\|_{1},\] (1)

where \( 0\), \(=_{i=1}^{n}(_{i}-})(_ {i}-})^{}\), \(}=_{i=1}^{n}_{i}\), and \( 0\) denotes that \(\) is positive definite and symmetric. \(\|\|_{1}\) is the \(_{1}\)-norm of matrix \(\). However,  proved that this estimator is sensible even for non-Gaussian \(\), since it corresponds to minimizing an \(_{1}\)-regularized log-determinant Bregman divergence. To solve the problem Eq. (1), we take the derivative of it \(^{-1}-=\|\|_{1}/\). Inspired by this derivative,  proposed the constrained \(_{1}\) minimization method for inverse matrix estimation (CLIME) estimator. This estimator can be used to estimate the precision matrix \(\) via an \(_{1}\) constrained optimization:

\[*{arg\,min}_{ 0}\|\|_{1},\ \| -I\|_{}.\] (2)

CLIME can be solved column-by-column. Compared with GLasso, CLIME has presented more favorable theoretical properties and can be solved by column-wise linear programming.

Method

The section elaborates on how the proposed meta-learning framework works. We begin by providing a formal problem setting in Section 3.1, which covers the task and data assumptions we consider. Next, we formulate the two stages of our meta-learning framework in Section 3.2.

### Problem Setting

Notations\(\{^{(k)}\}=\{^{(1)},^{(2)},,^{ (K)}\}\) denotes \(K\) datasets generated by \(K\) auxiliary tasks. We use \(^{(k)}^{n_{k} p}\) to represent the \(k\)-th dataset with \(n_{k}\) samples and \(p\) features. \(^{(k)}^{p p}\) represents the \(k\)-th precision matrix corresponding to \(^{(k)}\). \(\{^{(k)}\}=\{^{(1)},,^{(K)}\}\) is the set of the precision matrices corresponding to the auxiliary data \(\{^{(k)}\}\). \(_{}\) denotes the precision matrix of a new task \(_{}^{n p}\) to be estimated. We use \(\) to represent the meta-knowledge (i.e., the common substructure of \(\{^{(k)}\}\)) and \(_{r}^{(k)}\) to represent the rest of \(^{(k)}\) respectively.

AssumptionsWe consider a more general class of distributions than most previous studies, i.e., sub-Gaussian distributions, which cover Gaussian variables, bounded random variables, and so on. This relaxed data assumption significantly improves the flexibility and robustness of our method. We assume that the auxiliary tasks share some meta-knowledge \(\) with the target task \(_{}\). Specifically, we assume that \(()(_{})\). Here the meta-knowledge \(\) takes the form of a common substructure among the tasks. The assumption has been widely adopted [26; 27; 28] and proved to be feasible and applicable in the biological and genetic domains .

The paper aims to estimate the target precision matrix \(_{}\) of sub-Gaussian distribution with sparse structure and relatively limited samples given sufficient samples from auxiliary tasks \(\{^{(k)}\}\). The auxiliary datasets follow a family of multivariate sub-Gaussian distributions (see Definition 2 in Appendix F). To address the problem, we propose a novel meta-learning framework that leverages the meta-knowledge learned from auxiliary tasks to efficiently estimate \(_{}\).

### A Meta-learning Framework for Small Sample Precision Matrix Estimator

This part gives more details regarding the meta-learning framework we established. Figure 2 illustrates the pipeline of the framework in an intuitive way.

Figure 2: **The pipeline of the established framework.** The entire pipeline can be divided into two stages: meta-knowledge extraction (meta-training) and efficient adaptation (meta-testing). In Stage 1, our meta-teacher extracts the meta-knowledge, namely the shared structure, from the related auxiliary tasks with sufficient samples. In Stage 2, we obtain a prior sparsity pattern from the meta-knowledge and target dataset with a few samples. Then our meta-student aims to recover the edge structure by solving a matrix completion problem rapidly.

#### 3.2.1 Meta-teacher

We extract the common substructure across the tasks as the meta-knowledge. The goal of pre-training is to extract meta-knowledge \(\) from auxiliary tasks \(\{^{(k)}\}\).

To obtain the meta-knowledge, every precision matrix is modeled as

\[^{(k)}=+_{r}^{(k)}\] (3)

where \(^{p p}\) is the common substructure among all graphs and \(_{r}^{(k)}^{p p}\) represents the rest for \(k\)-th graph.

We adopt \(_{1}\)-penalization for \(\) and \(_{r}^{(k)}\) since they are expected to be sparse for better interpretability. Therefore, we write Eq. (3) into \(\|\|_{1}+\|_{r}^{(k)}\|_{1}\), where \(>0\). The value of the hyper-parameter \(\) depends on the properties of the graphs. This hyper-parameter controls the difference of sparsity level between \(\) and \(_{r}^{(k)}\). Concretely, with a smaller \(\), the shared part gets denser and the rest gets sparser.

Then we apply the formulation to Eq. (2), thus obtaining the single-task recovering method:

\[(,_{r}^{(k)})=*{arg\,min}_{, _{r}}\|\|_{1}+\|_{r}^{(k)}\|_{1},\|^{(k)}-(I_{p}-_{r}^{(k)}^{(k)})\|_{} .\] (4)

Here \(^{(k)}\) represents the covariance matrix corresponding to \(^{(k)}\). This single-task method, however, ignores the inner relationship between the auxiliary tasks. To this end, we sum up all the single-task estimators and devise the optimization problem in the following form:

\[(,\{_{r}^{(k)}\})=*{arg\,min }_{,\{_{r}^{(k)}\}}K\|\|_{1}+_{k=1}^{K}\|_{r}^ {(k)}\|_{1},\ \|^{(k)}-(I_{p}-_{r}^{(k)} ^{(k)})\|_{},\] (5)

where \(k=1,2,,K\).

#### 3.2.2 Meta-student

To speed up the estimation of \(_{}\), we propose our efficient estimator, which is favorable even with a few samples of the new task available. Additionally, we obtain a sparsity pattern from the meta-knowledge \(\) and target dataset with a few samples.

Different from most \(_{1}\)-penalized methods, our estimator aims to estimate the precision matrix by solving a Maximum Determinant Matrix Completion (MDMC) problem. MDMC is an effective technique that aims to pick the unique maximum determinant completion (when it exists) for a partially observed matrix from all the possible matrices .

Consider a maximum determinant matrix completion problem for a covariance matrix with some partially observed entries:

\[=*{arg\,max}_{ 0}\  \ _{ij}=M_{ij},\ (i,j),\] (6)

where \(M\) represents the partially observed matrix, and the set \(\) contains all the observed entries. Thus, the Lagrangian dual of this problem can be derived as:

\[=*{arg\,min}_{ 0}\ -+ ,M+p\ _{}^{p},\] (7)

with first-order optimality condition \(|(M)|^{-1}=M\), where the operator \(()\) computes the sign of the matrix and \(\) denotes the Hadamard product. Herein, the set \(_{}^{p}\) refers to the \(p p\) symmetric matrices with sparsity pattern \(\). Strong duality indicates a straightforward relation back to the primal \(=^{-1}\). Note that while \(\) is (in general) a dense matrix, \(\) is always sparse. Instead of solving the primal problem for a dense matrix, we choose to solve the dual problem for a sparse matrix, which also satisfies the optimality condition.

To solve the precision matrix \(_{}\), the first step is to find a reliable sparsity pattern. Previous works  obtain the pattern by thresholding the covariance matrix. However,  also proved the invalidity of such a method in the case of a relatively small sample size, as the covariance matrixcannot be accurately estimated. To meet the challenge, our estimator combines the support set of the thresholded covariance matrix and meta-knowledge to obtain the sparsity pattern \(\).

\[=()(S_{}(_ {})).\] (8)

Here, \(S_{}\) denotes the element-wise soft-thresholding operator with parameter \(\). Note that the problem (7) has a recursive closed-form solution whenever the graph is chordal, the embedding operation is conducted to satisfy the chordal property. To be specific, the chordal embedding of \(\) is represented as \(}\) and (7) is rewritten into the following formulation,:

\[_{}=*{arg\,min}_{>0}- +,_{}(_{}) \,,\,_{ij}=0,\;(i,j)}.\] (9)

where \(_{}\) is the projection operator from \(^{p}\) onto \(^{p}_{E}\), i.e., by setting \(_{}(A_{ij})=0\) if \((i,j) E\).

### Optimization Algorithms

The learning steps of meta-knowledge extraction (5) is solved efficiently through a formulation of multiple independent sub-problems of linear programming, which can be accelerated in a parallel form. With the assistance of meta-knowledge, we solve the optimization problem (9) using Newton Conjugate Gradient method. The key idea is to construct an inner conjugate gradients loop as a solution to the Newton subproblem of an outer Newton's method. The detailed description, complexity analysis, and pseudo-code (Algorithm 1) of the full algorithm are shown in Appendix C.

## 4 Theoretical Analysis

To present the analysis more concisely, we first define \(_{}:=(^{(1)},,^ {(K)})=(_{ij})_{Kp Kp}\), \(:=(,,)\), \(_{r}:=(_{r}^{(1)},,_{r}^{(K)})\), \(_{}:=(^{(1)},,^ {(K)})=+_{r}\), \(_{}:=(^{(1)},, ^{(K)})\). We assume that the precision matrix \(\) belongs to the uniformity class of matrices,

\[:=(q,s(Kp))=: 0, \|\|_{1},\|\|_{},_{1 i Kp}_{j= 1}^{Kp}|_{ij}|^{q} s(Kp)}.\] (10)

Here \(q,,\) are some constants, \(0 q<1\) and \(:=(w_{ij})_{Kp Kp}\). \(s(Kp)\) represents the sparsity level of \(\) in the uniformity class. Note that the sparsity level is related to \(p\) without an analytic form of relationships between them.

Then some important conditions and definitions are stated as the following.

Exponential Tail ConditionSuppose there exists a constant \(0\), so that \(}\) and

\[[(t(X_{i}-_{i})^{2})] C,|t|,  i\{1,,p\},\] (11)

where \(C\) is a constant.

Irrepresentable Condition There exists some \((0,1]\) such that

\[_{^{c}}^{*}(_{}^{*})^{-1}_{1} 1-,\] (12)

where \(^{*}:={^{*}}^{-1}}^{-1}\) (\(\) represents the Kronecker matrix product) and \(^{c}=\{1,,p\}^{2}\). We define \(_{^{*}}=(_{}^{*})^{-1} _{}\). Note that \(_{}^{*}\) is a \((s+p)(s+p)\) matrix indexed by vertex pairs, where \(s=||\). We define \(_{}^{}=_{(i,j)(_ {}^{*})}|_{,ij}^{*}|\) and \(_{}^{}=_{(i,j)(_ {}^{*})}|_{,ij}^{*}|\).

**Definition 1**.: _[_32_]_ _Given a matrix \(M^{p}\), define \(_{M}=\{(i,j):M_{ij} 0\}\) as its sparsity pattern. Then \(M\) is called **inverse-consistent** if there exists a matrix \(N^{p}\) such that_

\[M+N 0,(i,j)_{M} N_{ij}=0,(M+N)^{-1} ^{p}_{_{M}}\] (13)

_The matrix \(N\) is called an inverse-consistent complement of \(M\) and is denoted by \(M^{(c)}\). Furthermore, \(M\) is called **sign-consistent** if for every \((i,j)_{M}\), the \((i,j)\)-th elements of \(M\) and \((M+M^{(c)})^{-1}\) have opposite signs._Then we define the \((,)\) function with respect to the sparsity pattern \(\) and scalar \(>0\)

\[(,)=_{M>0}\|M^{(c)}\|_{} \ M_{}^{n}\ \ \|M\|_{},M_{i,i}=1,M\ .\]

Here \(i=1,2,,n\), \(\|\|_{}\) denotes the \(_{}\)-norm, e.g., \(\|A\|_{}=_{i j}|A_{ij}|\).

The Appendix F provides all the detailed proofs of the lemmas, theorems, and corollaries.

### Main Theorems

In this section, we mainly study the theoretical properties of the meta-knowledge \(\) in (5) and the precision matrix \(_{}\) in (7). The first theorem specifies a probability lower bound of recovering the true meta-knowledge by our estimator in (5) for multiple random multivariate sub-Gaussian distributions.

**Theorem 1**.: _Suppose that \(_{}^{*}\) and \(N=_{k=1}^{K}n_{k}\). When the variables are sub-Gaussian, the tail condition holds. Let \(=C_{0}}\), where the constant \(C_{0}=2^{-2}(2+_{0}+^{-1}e^{2}C^{2})^{2}\) and constant \(_{0}>0\). If \(_{}^{}>4_{n}\), then we have_

1. \(\|-^{*}\|_{} 8C_{0}^{2} }+2\)_;_
2. \(()=(^{*})\)_;_

_with a probability greater than \(1-4p^{-_{0}}\). Here \(\) is a threshold estimator with \(_{ij}=_{ij}I(|_{ij}|_{n})\), where \(_{n} 2\) is a tuning parameter._

According to Theorem 1, if \(n_{1}=n_{2}==n_{k}\), a sample complexity \(O()\) per task is sufficient for the recovery of the meta-knowledge. In most cases, \(O() O()\) since \(p>>K\).

The following theorem specifies a probability lower bound of recovering a correct precision matrix \(_{}\) by our estimator in (7) for a multivariate sub-Gaussian distribution.

**Theorem 2**.: _Suppose we have recovered the true sparsity pattern \(\) of a family of \(p\)-dimensional random multivariate sub-Gaussian distribution. Then for a new task of multivariate sub-Gaussian distribution with the precision matrix \(_{}^{*}\) such that \(()(_{}^{*})\) and satisfying irrepresentable condition, consider the estimator \(_{}\) with \(=C_{3}|}{n}}\). Then we have_

\[\|_{}-_{}^{*}\|_{} 2C_{3} ^{2}_{^{*}}|}{n}},\] (14)

_with a probability \(1-p^{-_{1}}\), where \(_{1}>0\)._

This theorem shows that \(n O(||)\) is sufficient for estimating a correct precision matrix of the new task with our estimation. Then, efforts should be made to prove that the max-det matrix completion method guarantees the consistency of the support set of \(_{}\) with \(\).

**Theorem 3**.: \(\) _coincides with the sparsity pattern of the optimal solution \(_{}^{*}\) if the normalized matrix \(_{}=D^{-1/2}_{}(_{} )D^{-1/2}\) where \(D=(_{}(_{}))\) satisfies the following conditions:_

1. \(_{}\) _is positive definite and sign-consistent,_
2. _We have_ \[(,\|_{}\|_{}) _{(k,l)}}(_{ }))_{kk}|}{}(_{ }))_{lk}}(_{}(_{} ))_{ll}}.\] (15)

We set the diagonal elements of \(_{}\) to 1 and its off-diagonal elements between -1 and 1. The projection operation \(_{}()\) leads to many zero elements in \(_{}\), resulting in \(_{}\) being positive definite or even diagonally dominant in most cases. As shown by , when \(\) induces an acyclic structure, condition 2 is automatically satisfied by condition 1. More generally,  shows that \(_{}\) is sign-consistent if \((_{}+_{}^{(c)})^{-1}\) is close to its first order Taylor expansion. This assumption holds in practice due to the fact that the magnitude of the off-diagonal elements of \(_{}+_{}^{(c)}\) is small. Moreover,  shows that the left side of (50) is upper bounded by \(a\|_{}\|_{}^{2}\) for some \(a>0\). That implies that when \(\|_{}\|_{}\) is small, or equivalently \(\) is large, condition 3 is automatically satisfied. If the conditions in Theorem 3 are satisfied, the support set of \(_{}\) is consistent with \(\) according to (7).

## 5 Related Work

There have been several methods proposed to estimate precision matrices, among which, three state-of-the-art methods, QUIC , Neighborhood Selection , Meta-IE  and gRankLasso  are chosen as baselines in the experiments (See reasons in Appendix E). These approaches all consider high-dimensional settings which are similar to our work. **QUIC:** This approach utilizes a quadratic approximation of the negative log-likelihood function to estimate sparse inverse covariance matrices to accelerate computation speed, which reduces the computational cost from \(O(p^{2})\) to \(O(p)\) at each iteration. **Neighborhood Selection:** This method is a type of regularized maximum likelihood estimation method for estimating precision matrices. Given some i.i.d. observations, it estimates the conditional independence restrictions separately for each node in the graph. The complexity of the neighborhood selection for one node with the Lasso is \(O(np\{n,p\})\) using LARS algorithm . **Meta-IE (BigQuic):** first proposes a meta-learning-based method that is composed of two steps. First, Meta-IE takes the support union of all the precision matrices of the auxiliary tasks as prior knowledge. Then the method states that the testing step can be solved by an interior point method  in polynomial time \(O(p^{3.5}^{-1})\) or BigQuic  with a better time complexity around \(O(p^{2}^{-1})\) (See analysis in Appendix). **gRankLasso:** gRankLasso  is a method for sparse precision matrix estimation that realizes automatic parameter tuning, making it completely tuning-free. It achieves robustness and accuracy by integrating a group penalty with Lasso regularization, effectively estimating sparse structures in high-dimensional data.

## 6 Experiment

### Synthetic Experiment

**Baselines:** We compare our method with the following baselines mentioned in Section 5: 1) The QUIC baseline, 2) the Neighborhood Selection (NS) baseline, 3) Meta-IE (BigQuic) baseline that is accelerated with \(32\) threads and 4) gRankLasso baseline.

**Metric:** We use the Frobenius-norm \(\|_{}-_{}^{*}\|_{F}\) and Mathews correlation coefficient (MCC) as metrics to evaluate the performance. MCC is widely used in machine learning as a measure of binary classifiers, defined as \(=(-)/+)(+)(+)(+)}\). Here the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values indicate the number of true non-zero entries, true-zero entries, false non-zero entries, and false zero entries, respectively. It produces a high score if the classifier generates desirable estimations.

**Simulated Datasets:** Each graph model contains a new task and \(K=4\) prior learning tasks. The sample matrices follow multivariate normal distributions where the corresponding precision matrices are generated using two sparse models: (1) Random graph and (2) Tree graph. More details about the data simulation are moved into Appendix B.

**Time Comparison** The time-complexity experiment on simulated datasets is conducted to observe the changes of all the estimators in computation time with the varying \(p\) and \(n\). As shown in Fig. 3 and Table 1 &2, FasMe performs better than all the baselines. Using our method, we solve a sparse inverse covariance estimation problem containing 2500 variables, with time cost around 1 second. By comparing all the subfigures in Fig. 3, we find that all the methods except the neighborhood selection approach are insensitive to the change of \(n\). This is because the neighborhood selection deals with the sample matrix \(_{}^{n p}\) directly instead of the covariance matrix \(_{}^{p p}\). However, it remains computationally costly compared to FasMe when the sample size is much smaller than the number of features. Interestingly, despite Meta-IE (BigQuic) method can speed up by parallelizing multiple threads, it is less time-efficient because of the time assumption of the I/O reads and writes. Moreover, BigQuic cannot converge within the maximum iterations (\(=100\)), especially when only a few samples are provided (\(n=p/20,p/10\)). Regarding gRankLasso, its tuning mechanism requires a substantial amount of time, leading to significant time costs in high-dimensional scenarios. In contrast, our method converges significantly faster within \(20\) iterations, regardless of small sample setting.

Accuracy ComparisonWe conduct several experiments on the simulated datasets to compare the prediction power of all methods with four different small sample settings \(n<p/10\). Table 1&2 indicates that FasMe obtains lower Frobenius-norm and higher MCC under all the conditions. It suggests that FasMe outperforms the other baselines in high-dimensional settings. The comparison of the two tables shows that FasMe can effectively recover the special structure from the data. In addition, the MCC of Meta-IE becomes small as the feature dimension increases. This is because the predicted edges must be a subset of the support union. As \(p\) increases, the estimation of the support union is not relatively accurate, thus significantly reducing the prediction accuracy of Meta-IE. The poor performance of gRankLasso can be attributed to the inefficacy of its automatic tuning mechanism in small sample settings, which degrades the model performance. We draw one subnetwork for the experimental result of every method and compare them with the ground truth in Fig. 4 (a)(b). They show that our method bears the highest similarity with the ground truth. The results are consistent with the MCC value of every method shown in Table 1 &2.

### Real-world Experiment: Application to Gene and fMRI Data

We further evaluate our method for estimating precision matrices on two real-world datasets: ChIP-Seq dataset (ENCODE project ) and fMRI dataset (OpenfMRI project , accession number: ds000002). Regarding the ChIP-Seq dataset, we aim to exploit the gene network of transcription

    &  &  &  &  \\  Methods & MCC(T) & F-norm(J) & Time(J) & MCC(T) & F-norm(J) & Time(J) & MCC(T) & F-norm(J) & Time(J) & MCC(T) & F-norm(J) & Time(J) \\  QUIC & 0.043 & 23.02 & 3.028 & 0.065 & 22.46 & 3.676 & 0.039 & 33.87 & 64.002 & 0.065 & 31.65 & 22.925 \\ INS & 0.287 & 66.08 & 8.724 & 0.436 & 54.21 & 10.829 & 0.388 & 88.98 & 43.03 & 6.024 & 67.02 & 87.193 \\ Meta-IE(BigQuic) & 0.043 & 29.10 & 27.370 & 0.400 & 22.42 & 6.655 & 0.281 & 41.92 & 96.850 & 0.310 & 32.10 & 87.140 \\ gRankLasso & 0.043 & 198.56 & * & 0.012 & 176.99 & * & 0.005 & 218.12 & * & 0.008 & 212.43 & * \\ Ours & **0.659** & **22.63** & **0.061** & **0.708** & **19.37** & **0.063** & **0.661** & **26.01** & **0.964** & **0.716** & **25.13** & **0.965** \\   

Table 1: Comparison of estimation error (in terms of MCC and F-norm\(=\|}_{}-_{}^{*}\|_{F}\)) and running time (seconds) on synthetic dataset. \(^{*}\) is generated from a Random graph. \(*\) means that the time exceeds 30 minutes.

    &  &  &  &  \\  Methods & MCC(T) & F-norm(J) & Time(J) & MCC(T) & F-norm(J) & Time(J) & MCC(T) & F-norm(J) & Time(J) & MCC(T) & F-norm(J) & Time(J) \\  QUIC & 0.038 & 22.98 & 3.212 & 0.071 & 20.69 & 3.541 & 0.036 & 35.07 & 58.922 & 0.070 & 31.48 & 31.625 \\ NSE & 0.306 & 66.02 & 8.701 & 0.455 & 56.51 & 11.147 & 0.396 & 57.87 & 50.601 & 0.655 & 66.08 & 89.237 \\ Meta-IE(BigQuic) & 0.451 & 28.64 & 28.32 & 0.342 & 0.38 & 17.632 & 0.395 & 39.979 & 0.340 & 31.72 & 85.176 \\ gRankLasso & 0.007 & -201.89 & - & 0.013 & 185.60 & - & 0.005 & 223.45 & - & 0.008 & 205.77 & - \\ Ours & **0.668** & **22.05** & **0.058** & **0.712** & **15.98** & **0.064** & **0.681** & **24.24** & **1.002** & **0.745** & **24.02** & **0.996** \\   

Table 2: Comparison of estimation error (in terms of MCC and F-norm\(=\|}_{}-_{}^{*}\|_{F}\)) and running time (seconds) on synthetic dataset. \(^{*}\) is generated from Tree graph.

Figure 3: Time cost of FasMe vs. baselines on simulated datasets with the feature dimension \(p\) varying in \(\{500,1000,1500,2000,2500\}\). Subfigure (a)(b)(c)(d) records the time required for each method to be implemented on a series of datasets with different sample sizes \(n=p/20,p/10,p/5,p\), respectively. Note that the missing points of the baselines mean the time cost is out of range.

factors (TFs). We randomly selected 300 gene features. 27 samples of H1-hESC (embryonic stem cells) and GM12878 (Blymphocyte) are chosen as two auxiliary tasks for meta-knowledge and 20 samples of K562 (leukemia) are chosen as a novel task. Regarding the fMRI dataset, we aim to exploit brain connectivity among different brain regions. We randomly selected 200 regions of interest (ROI) as features. 34 samples of PCL and DCL datasets are chosen as the auxiliary tasks for meta-knowledge and 20 samples of Mixed-Event dataset are chosen as a novel task. Additional experimental details including configuration choices and extended experiments in the economic domain are provided in Appendix B.

In Subfigure (c)(d) of Fig. 4, we visualize the experimental results on the two real-world datasets respectively. The predicted gene network exhibits scale-free and clustering behaviors, which is consistent with its biological properties. In another experiment, the majority of brain connections are found in the left ROIs. Besides, the left brain has positive connections, while the cross-hemisphere ones are negative. In this case, the subjects are asked to solve a classification problem, which mainly relies on their left brain's analysis function. The results align with our expectations.

## 7 Conclusion

In this work, we introduced FasMe, a Fast and Sample-efficient Meta Estimator designed to address the challenges of precision matrix learning in small sample settings. Under a novel meta-learning-based framework, our approach first leverages a multi-task precision matrix estimator to extract shared meta-knowledge from auxiliary tasks, enabling efficient adaptation to new tasks with minimal data. FasMe then incorporates a maximum determinant matrix completion strategy to enhance precision matrix estimation, ensuring both computational efficiency and theoretical robustness. Our experiments on synthetic and real-world datasets demonstrate that FasMe significantly outperforms state-of-the-art baselines in terms of accuracy and speed. These results highlight the potential of FasMe to address real-world challenges in high-dimensional settings, such as genetics and neuroscience, where data is scarce. Moving forward, we plan to extend our work by relaxing the sub-Gaussian data assumption and further reducing the sample size requirements for training meta-knowledge. This will extend the framework to other domains and improve scalability for even larger datasets.