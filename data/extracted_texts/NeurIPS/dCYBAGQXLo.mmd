# Supervised Pretraining Can Learn

In-Context Reinforcement Learning

 Jonathan N. Lee\({}^{*1}\)  Annie Xie\({}^{*1}\)  Aldo Pacchiano\({}^{2}\)\({}^{3}\)  Yash Chandak\({}^{1}\)

**Chelsea Finn\({}^{1}\)  Ofir Nachum  Emma Brunskill\({}^{1}\)**

\({}^{1}\)Stanford University \({}^{2}\)Broad Institute of MIT and Harvard \({}^{3}\)Boston University

Equal contribution. Correspondence to jnl@stanford.edu and anniexie@stanford.edu. Code is available at https://github.com/jon-lee/decision-pretrained-transformer

###### Abstract

Large transformer models trained on diverse datasets have shown a remarkable ability to _learn in-context_, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study the _Decision-Pretrained Transformer (DPT)_, a supervised pretraining method where a transformer predicts an optimal action given a query state and an in-context dataset of interactions from a diverse set of tasks. While simple, this procedure produces a model with several surprising capabilities. We find that the trained transformer can solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.

## 1 Introduction

For supervised learning, transformer-based models trained at scale have shown impressive abilities to perform tasks given an input context, often referred to as few-shot prompting or in-context learning . In this setting, a pretrained model is presented with a small number of supervised input-output examples in its context, and is then asked to predict the most likely completion (i.e. output) of an unpaired input, without parameter updates. Over the last few years, in-context learning has been applied to solve a range of tasks  and a growing number works are beginning to understand and analyze in-context learning for supervised learning . In this work, our focus is to study and understand in-context learning applied to sequential decision-making, specifically in the context of reinforcement learning (RL) settings. Decision-making (e.g. RL) is considerably more dynamic and complex than supervised learning. Understanding and leveraging in-context learning here could potentially unlock significant improvements in an agent's ability to adapt and make few-shot decisions in response to observations from the world. Such capabilities are instrumental for practical applications ranging from robotics to recommendation systems.

For in-context decision-making [7; 8; 9], rather than input-output tuples, the context takes the form of state-action-reward tuples representing a dataset of interactions with an unknown environment. The agent must leverage these interactions to understand the dynamics of the world and what actions lead to good outcomes. A hallmark of good decision-making in online RL algorithms is a judicious balance of selecting exploratory actions to gather information and selecting increasingly optimal actions by exploiting that information . In contrast, an RL agent with access to only a suboptimal offline dataset should produce a policy that conservatively selects actions . An ideal in-context decision-maker should exhibit similar behaviors.

To study in-context decision-making formally, we propose a new simple supervised pretraining objective, namely, to train (via supervised learning) a transformer to predict an optimal action label2 given a query state and an in-context dataset of interactions, across a diverse set of tasks. We refer to the pretrained model as a Decision-Pretrained Transformer (DPT). Once trained, DPT can be deployed as either an online or offline RL algorithm in a new task by passing it an in-context dataset of interactions and querying it for predictions of the optimal action in different states. For example, online, the in-context dataset is initially empty and DPT's predictions are uncertain because the new task is unknown, but it fills the dataset with its interactions as it learns and becomes more confident about the optimal action. We show empirically and theoretically that DPT yields a surprisingly effective in-context decision-maker with regret guarantees. As it turns out, DPT effectively performs posterior sampling -- a provably sample-efficient Bayesian RL algorithm that has historically been limited by its computational burden [12; 13]. We summarize our main findings below.

* **Predicting optimal actions alone gives rise to near-optimal decision-making algorithms.** The DPT objective is solely based on predicting optimal actions from in-context interactions. At the outset, it is not immediately apparent that these predictions at test-time would yield good decision-making behavior when the task is unknown and behaviors such as online exploration are necessary to solve it. Intriguingly, DPT as an algorithm is capable of dealing with this uncertainty in-context. For example, despite not being explicitly trained to explore, DPT exhibits an exploration strategy on par with hand-designed algorithms, as a means to discover the optimal actions.
* **DPT generalizes to new decision-making problems, offline and online.** We show DPT can handle reward distributions unseen in its pretraining data on bandit problems as well as unseen goals, dynamics, and datasets in simple MDPs. This suggests that the in-context strategies learned during pretraining are robust and generalizable without any parameter updates at test time.
* **DPT improves over the data used to pretrain it by exploiting latent structure.** As an example, in parametric bandit problems, specialized algorithms can leverage structure (such as linear rewards) and offer provably better regret. However, a representation must be known in advance. Perhaps surprisingly, we find that pretraining on linear bandit problems, even with unknown representations, leads DPT to select actions and explore in a way that matches an efficient linear bandit algorithm. This holds even when the source pretraining data comes from a suboptimal algorithm (i.e., one that does not take advantage of any latent structure), demonstrating the ability to learn improved in-context strategies beyond what it was trained on.
* **Posterior sampling can be implemented via in-context learning.** Posterior sampling (PS), a generalization of Thompson Sampling, can provably sample-efficiently solve online RL problems [12; 13], but a common criticism is the lack of computationally efficient ways to update and sample from a posterior distribution. DPT can be viewed as learning a posterior distribution over optimal actions, shortcutting the PS procedure. Under some conditions, we show theoretically that DPT in-context is equivalent to PS. Furthermore, DPT's prior and posterior updates are grounded in data rather than needing to be specified _a priori_. This suggests that in-context learning could be a possible path towards scalable and practical RL via posterior sampling.

## 2 Related Work

Meta-learning.Algorithmically, in-context learning falls under the meta-learning framework [14; 15]. At a high-level, these methods attempt to learn some underlying shared structure of the training distribution of tasks to accelerate learning of new tasks. For decision-making and RL, there is often a choice in what shared'structure' is specifically learned such as the dynamics of the task [16; 17; 18; 19], a task context identifier [20; 21; 22; 23], temporally extended skills and options [24; 25; 26], or initialization of a neural network policy [27; 28]). In-context learning can be viewed as taking a more agnostic approach by learning the learning algorithm itself, more similar to [29; 30; 31; 32; 8]. Algorithm Distillation (AD) [7; 33] falls under this category, using autoregressive supervised learning to distill traces of a single-task RL algorithm into a task-agnostic model. While DPT also leverages autoregressive SL, it does not distill an existing RL algorithm in order to imitate how to learn. Instead, we train DPT to predict task-specific optimal actions, yielding potentially emergent strategies at test time that automatically leverage the task structure to behave similarly to posterior sampling.

Autoregressive transformers for decision-making.In decision-making fields such as RL and imitation learning, transformer models trained using autoregressive supervised action prediction have proliferated , inspired by the successes of these techniques for large language models [35; 36; 1]. For example, Decision Transformer (DT) [37; 38; 8] uses a transformer to autoregressively model sequences of actions from offline experience data, conditioned on the achieved return. During inference, one can then query the model conditioned on a desired return value. This approach has been shown to scale favorably to large models and multi-task settings , at times exceeding the performance of large-scale multi-task imitation learning with transformers [40; 41; 42]. However, DT is known to be provably (and unboundedly) sub-optimal in common scenarios [43; 44]. A common criticism of DT, and supervised learned transformers in general, is their inability to improve upon the dataset. For example, there is little reason for DT to output meaningful behavior if conditioned on return higher than any observed in training, without strong extrapolation assumptions . In contrast, a major contribution of our work is theoretical and empirical evidence for the ability of DPT to improve over behaviors seen in the dataset in terms of regret.

Value and policy-based offline RL.Offline RL algorithms offer the opportunity to learn from existing datasets. To address distributional shift, many prior algorithms incorporate the principle of value pessimism [45; 46; 47; 48], or policy regularization [49; 50; 51; 52; 53]. To reduce the amount of offline data required in a new task, methods for offline meta-RL can reuse interactions collected in a set of related tasks [54; 55; 56]. However, they still must address distribution shift, requiring solutions such as policy regularization  or additional online interactions . DPT follows the success of autoregressive models like DT and AD, avoiding these issues. With our pretraining objective, DPT also leverages offline datasets for new tasks more effectively than AD.

## 3 In-Context Learning Model

Basic decision models.The basic decision model of our study is the finite-horizon Markov decision process (MDP). An MDP is specified by the tuple \(=,,T,R,H,\) to be solved, where \(\) is the state space, \(\) is the action space, \(T:()\) is the transition function, \(R:()\) is the reward function, \(H\) is the horizon, and \(()\) is the initial state distribution. A learner interacts with the environment through the following protocol: (1) an initial state \(s_{1}\) is sampled from \(\); (2) at time step \(h\), the learner chooses an action \(a_{h}\) and transitions to state \(s_{h+1} T(|s_{h},a_{h})\), and receives a reward \(r_{h} R(|s_{h},a_{h})\). The episode ends after \(H\) steps. A policy \(\) maps states to distributions over actions and can be used to interact with the MDP. We denote the optimal policy as \(^{}\), which maximizes the value function \(V(^{})=_{}V():=_{}_{}_{h}r_{h}\). When necessary, we use the subscript \(\) to distinguish \(V_{}\) and \(_{}^{}\) for the specific MDP \(\). We assume the state space is partitioned by \(h[H]\) so that \(^{}\) is notationally independent of \(h\). Note this framework encompasses multi-armed bandit settings where the state space is a single point, e.g. \(=\{1\}\), \(H=1\), and the optimal policy is \(a^{}*{argmax}_{a}[r_{1}|a_{1 }=a]\).

Pretraining.We give pseudocode in Algorithm 1 and a visualization of the transformer in Figure 1. Details on the precise structure of the transformer can be found in Section A.1. Let \(_{}\) be a distribution over tasks at the time of pretraining. A task \(_{}\) can be viewed as a specification of an MDP, \(=,,T,R,H,\). The distribution \(_{}\) can span different reward and transition functions and even different state and action spaces. We then sample a context (or a prompt) which consists of a dataset \(D_{}(;)\) of interactions between the learner and the MDP specified by \(\). \(D=\{s_{j},a_{j},s^{}_{r},r_{j}\}_{j[n]}\) is a collection of transition tuples taken in \(\). We refer to \(D\) as the _in-context dataset_ because it provides the contextual information about \(\). \(D\) could be generated through variety of means, such as: (1) random interactions within \(\), (2) demonstrations from an expert, and (3) rollouts of an algorithm. Additionally, we independently sample a query state \(s_{}\) from the distribution \(_{}\) over states \(\) and a label \(a^{}\) is sampled from the optimal policy \(_{}^{}(|s_{})\)for task \(\) (see Section 5.3 for how to implement this in common practical scenarios). We denote the joint pretraining distribution over tasks, in-context datasets, query states, and action labels as \(P_{pre}\):

\[P_{pre}(,D,s_{},a^{})=_{}() _{}(D;)_{}(s_{}) ^{}_{}(a^{}|s_{}).\] (1)

Given the in-context dataset \(D\) and a query state \(s_{}\), we can train a model to predict the optimal action \(a^{}\) in response simply via supervised learning. Let \(D_{j}=\{(s_{1},a_{1},s^{}_{1},r_{1}),,(s_{j},a_{j},s^{}_{j}, r_{j})\}\) denote the partial dataset up to \(j\) samples. Formally, we aim to train a causal GPT-2 transformer model \(M\) parameterized by \(\), which outputs a distribution over actions \(\), to minimize the expected loss over samples from the pretraining distribution:

\[_{}_{P_{pre}}_{j[n]}(M_{}(  s_{},D_{j}),a^{}).\] (2)

Throughout, we set the loss to be the negative log-likelihood with \((M_{}( s_{},D_{j}),a^{}):=- M_{} (a^{} s_{},D_{j})\). This framework can work for both discrete and continuous \(\). For our experiments with discrete \(\), we use a softmax parameterization for the distribution of \(M_{}\), essentially treating this as a classification problem. The resulting output model \(M_{}\) can be viewed as an algorithm that takes in a dataset of interactions \(D\) and can be queried with a forward pass for predictions of the optimal action via inputting a query state \(s_{}\). We refer to the trained model \(M_{}\) as a Decision-Pretrained Transformer (DPT).

Figure 1: A transformer model \(M_{}\) is pretrained to predict an optimal action \(a^{}_{}\) from a state \(s_{}\) in a task, given a dataset of interactions from that task. The resulting Decision-Pretrained Transformer (DPT) learns a distribution over the optimal action conditioned on an in-context dataset. \(M_{}\) can be deployed in _new_ tasks online by collecting data on the fly or offline by immediately conditioning on a static dataset.

Testing.After pretraining, a new task (MDP) \(\) is sampled from a test-task distribution \(_{}\). If the DPT is to be tested _offline_, then a dataset (prompt) is a sampled \(D_{}(\,\,;)\) and the policy that the model in-context learns is given conditionally as \(M_{}(,D)\). Namely, we evaluate the policy by selecting action \(a_{h}*{argmax}_{a}M_{}(a|s_{h},D)\) when the learner visits state \(s_{h}\). If the model is to be tested _online_ through multiple episodes of interaction, then the dataset is initialized as empty \(D=\{\}\). At each episode, \(M_{}(,D)\) is deployed where the model samples \(a_{h} M_{}(|s_{h},D)\) upon observing state \(s_{h}\). Throughout a full episode, it collects interactions \(\{s_{1},a_{1},r_{1},,s_{H},a_{H},r_{H}\}\) which are subsequently appended to \(D\). The model then repeats the process with another episode, and so on until a specified number of episodes has been reached. A key distinction of the testing phase is that there are no updates to the parameters of \(M_{}\). This is in contrast to hand-designed RL algorithms that would perform parameter updates or maintain statistics using \(D\) to learn from scratch. Instead, the model \(M_{}\) performs a computation through its forward pass to generate a distribution over actions conditioned on the in-context \(D\) and query state \(s_{h}\).

## 4 Learning in Bandits

We begin with an empirical investigation of DPT in a multi-armed bandit, a well-studied special case of the MDP where the state space \(\) is a singleton and the horizon \(H=1\) is a single step. We will examine the performance of DPT both when aiming to select a good action from offline historical data and for online learning where the goal is to maximize cumulative reward from scratch. Offline, it is critical to account for uncertainty due to noise as certain actions may not be sampled well enough. Online, it is critical to judiciously balance exploration and exploitation to minimize overall regret. For detailed descriptions of the experiment setups, see Appendix A.

Pretraining distribution.For the pretraining task distribution \(_{}\), we sample \(5\)-armed bandits (\(||=5\)). The reward function for arm \(a\) is a normal distribution \(R(|s,a)=(_{a},^{2})\) where \(_{a}\) independently and \(=0.3\). To generate in-context datasets \(_{}\), we randomly generate action frequencies by sampling probabilities from a Dirichlet distribution and mixing them with a point-mass distribution on one random arm (see details in Appendix A.3). Then we sample the actions accordingly from this distribution. This encourages diversity of the in-context datasets. The optimal policy \(^{*}_{}\) for bandit \(\) is \(*{argmax}_{a}_{a}\), which we can easily compute during pretraining. We pretrain the model \(M_{}\) to predict \(a^{*}\) from \(D\) as described in Section 3 for datasets up to size \(n=500\).

Comparisons.We compare to several well-known algorithms for bandits3. All of the algorithms are designed to reason in a particular way about uncertainty based on their observations.

* Empirical mean algorithm (Emp) selects the action with the highest empirical mean reward naively.
* Upper Confidence Bound (UCB) selects the action with the highest upper confidence bound.
* Lower Confidence Bound (LCB) selects the action with the highest lower confidence bound.
* Thompson Sampling (TS) selects the action with the highest sampled mean from a posterior distribution over reward models. The prior and likelihood functions are Gaussian.

Emp and TS [58; 59] can both be used for offline or online learning; UCB  is an online algorithm that achieves exploration via optimism under uncertainty; and LCB [61; 62] is used to minimize suboptimality given an offline dataset by selecting actions pessimistically. We evaluate algorithms with standard bandit metrics. Offline, we use the suboptimality \(_{a^{*}}-_{}\) where \(\) is the chosen action. Online, we use cumulative regret: \(_{k}_{a^{*}}-_{_{k}}\) where \(_{k}\) is the \(k\)th action chosen.

DPT learns to reason through uncertainty.As shown in Figure 1(a), in the offline setting, DPT significantly exceeds the performance of Emp and LCB while matching the performance of TS, when the in-context datasets are sampled from the same distribution as during pretraining. The results suggest that the transformer is capable of reasoning through uncertainty caused by the noisy rewards in the dataset. Unlike Emp which can be fooled by noisy, undersampled actions, the transformer has learned to _hedge_ to a degree. However, it also suggests that this hedging is fundamentally different from what LCB does, at least on this specific distribution4.

Interestingly, the same transformer produces an extremely effective online bandit algorithm when sampling actions instead of taking an argmax. As shown in Figure 1(b), DPT matches the performance of classical optimal algorithms, UCB and TS, which are specifically designed for exploration. This is notable because DPT was not explicitly trained to explore, but its emergent strategy is on par with some of the best. In Figure 1(c), we show this property is robust to noise in the rewards not seen during pretraining by varying the standard deviation. In Appendix B, we show this generalization happens offline too and even with unseen Bernoulli rewards.

Leveraging latent structure from suboptimal data.We now investigate whether DPT can learn to leverage the inherent structure of a problem class, even without prior knowledge of this structure and even when learning from in-context datasets that do not explicitly utilize it. More precisely, we consider \(_{}\) to be a distribution over _linear_ bandits, where the reward function is given by \([r a,]=_{},(a)\) and \(_{}^{d}\) is a task-specific parameter vector and \(:^{d}\) is fixed feature vector that is the same for all tasks. Given the feature representation \(\), LinUCB , a UCB-style algorithm that leverages \(\), should achieve regret \(}(d)\) over \(K\) steps, a substantial gain over UCB and TS when \(d||\). We do not assume \(\) is known to DPT, besides whatever can be gleaned from the pretraining data. To evaluate the ability of DPT to uncover latent structure, we collect pretraining data with in-context datasets gathered by standard TS, which does not leverage the linear structure. Figures 2(a) and 2(b) show that DPT can exploit the unknown linear structure, implicitly learning a surrogate for \(\), allowing to do more informed exploration online and decision-making offline. It is nearly on par with LinUCB (which is given \(\)). We also see that DPT significantly outperforms the source of its pretraining data, TS which did not know or use the latent structure to gather the in-context datasets. These results present evidence that (1) DPT can automatically leverage structure, and (2) supervised learning-based approaches to RL _can_ potentially learn novel exploration strategies that transcend the quality of their pretraining data.

Adapting to expert-biased datasets.A common assumption in offline RL is that datasets tend to be a mixture between optimal data (e.g. expert demonstrations) and suboptimal data (e.g. random interactions) . Hence, LCB is generally effective in practice and the pretraining and testing distributions should be biased towards this setting. Motivated by this, we pretrain another DPT model where \(_{}\) is generated by mixing the in-context datasets with varying fractions of expert data, biasing \(_{}\) towards datasets that contain more examples of the optimal action. We denote this model by DPT-Exp. In Figure 2(c), we plot the test-time performance of both pretrained models when evaluated on new offline datasets with varying percentages of expert data5. Our results suggest that when the pretraining distribution is also biased towards expert-suboptimal data, DPT-Exp behaves similarly to LCB, while DPT continues to resemble TS. This is quite interesting as, for other methods such as TS, it is less clear how to automatically incorporate the right amount of expert bias to yield the same effect, but DPT can leverage this from pretraining.

## 5 Learning in Markov Decision Processes

We next study how DPT can tackle Markov decision processes by testing its ability to perform exploration and credit assignment. In the following experiments, the DPT demonstrates generalization to new tasks, scalability to image-based observations, and capability to stitch in-context behaviors (Section 5.2). This section also examines whether DPT can be pretrained with datasets and action labels generated by a different RL algorithm, rather than the exact optimal policy (Section 5.3).

Figure 2: (a) Offline performance on in-distribution bandits, given random in-context datasets. (b) Online cumulative regret on bandits. (c) Final (after 500 steps) cumulative regret on out-of-distribution bandits with different Gaussian noise standard deviations. The mean and standard error are computed over \(200\) test tasks.

### Experimental Setup

Environments.We consider environments that require targeted exploration to solve the task. The first is Dark Room [22; 7], a 2D discrete environment where the agent must locate the unknown goal location in a \(10 10\) room, and only receives a reward of \(1\) when at the goal. We hold out a set of goals for generalization evaluation. The second is Miniworld , a 3D visual navigation problem to test the scalability of DPT to image observations. The agent is in a room with four boxes of different colors, and must find the target box. The target color is unknown initially. It receives a reward of \(1\) only when near the correct box. Environments and pretraining dataset details are in App. A.4 and A.5.

Comparisons.Our experiments aim to understand the effectiveness of DPT by comparing it to other context-based meta-RL algorithms based on supervised and RL objectives.

* Proximal Policy Optimization (PPO) : We compare to this single-task RL algorithm, which trains from scratch without any pretraining data, to contextualize the performance of DPT and other meta-RL algorithms.
* Algorithm Distillation (AD) : AD first generates a dataset of learning histories by running an RL algorithm in each training task. Then, given a sampled subsequence \(h_{j}=(s_{j},a_{j},r_{j},,s_{j+c})\) from a learning history, a transformer is trained to predict the next action \(a_{j+c}\) from the history.
* RL\({}^{2}\): This online meta-RL comparison uses a recurrent neural network to adapt the agent's policy from the given context. Unlike AD and DPT, which are trained with a supervised objective, the RL\({}^{2}\) agent is trained to maximize the expected return with PPO.

PPO and RL\({}^{2}\) are online algorithms, while AD is capable of learning both offline and online. Details on the implementation of these algorithms can be found in Appendix A.2.

### Main Results

Generalizing to new offline datasets and tasks.To study the generalization capabilities of DPT, we evaluate the model in Dark Room on a set of \(20\) held-out goals not in the pretraining dataset. When given an expert dataset, DPT achieves near-optimal performance. Even when given a random dataset, which has an average total reward of \(1.1\), DPT obtains a much higher average return of

Figure 4: (a) Offline performance on held-out Dark Room goals, given random and expert datasets. (b) Online performance on held-out Dark Room goals. (c) Offline performance on Miniworld (images), given random and expert datasets. (d) Online performance on Miniworld (images) after \(40\) episodes. We report the average and standard error of the mean over \(100\) different offline datasets in (a) and (c) and \(20\) online trials in (b) and (d).

Figure 3: (a) Offline performance of DPT trained on linear bandits from TS source data. LinReg does linear regression and outputs the greedy action. (b) Online cumulative regret of the same model. The mean and standard error are computed over \(200\) test tasks. (c) Offline performance on expert-biased datasets. DPT pretrained on a different prior continues to match TS, but DPT-Exp trained from a more representative prior excels.

\(61.5\) (see Fig. 3(a)). Qualitatively, we observe that when the in-context dataset contains a transition to the goal, DPT immediately exploits this and takes a direct path to the goal. In contrast, while AD demonstrates strong offline performance with expert data, it performs worse in-context learning with random data compared to DPT. The difference arises because AD is trained to infer a better policy than the in-context data, but not necessarily the optimal one.

We next evaluate DPT, AD, RL\({}^{2}\), and PPO online without any prior data from the \(20\) test-time Dark Room tasks, shown in Fig. 3(b). After \(40\) episodes, PPO does not make significant progress towards the goal, highlighting the difficulty of learning from such few interactions alone. RL\({}^{2}\) is trained to perform adaptation within four episodes each of length \(100\), and we report the performance after the four adaptation episodes. Notably, DPT on average solves each task faster than AD and reaches a higher final return than RL\({}^{2}\), demonstrating its capability to explore effectively online even in MDPs. In Appendix B, we also present results on generalization to new dynamics.

Learning from image-based observations.In Miniworld, the agent receives RGB image observations of \(25 25\) pixels. As shown in Fig. 3(d), DPT can solve this high-dimensional task offline from both random and expert datasets. Compared to AD and RL\({}^{2}\), DPT also learns online more efficiently.

Stitching novel trajectories from in-context subsequences.A desirable property of some offline RL algorithms is the ability to stitch suboptimal subsequences from the offline dataset into new trajectories with higher return. To test whether DPT exhibits stitching, we design the _Dark Room (Three Tasks)_ environment in which there are three possible tasks. The pretraining data consists only of expert demonstrations of two of them. At test-time DPT is evaluated on third unseen task, but its offline dataset is only expert demonstrations of the original two. Despite this, it leverages the data to infer a path solving the third task (see Fig. 4(a)).

### Learning from Algorithm-Generated Policies and Rollouts

So far, we have only considered action labels provided by an optimal policy. However, in some tasks, an optimal policy is not readily available even in pretraining. In this experiment, we use actions labeled by a policy learned via PPO and in-context datasets sampled from PPO replay buffers. We train PPO agents in each of the \(80\) train tasks for 1K episodes to generate \(80\)K total rollouts, from which we sample the in-context datasets. This variant, DPT (PPO, PPO), performs on par with DPT and still better than AD, as shown in Figures 4(b) and 4(c). DPT (PPO, PPO) can be viewed as a direct comparison between our pretraining objective and that of AD, given the same pretraining data but just used differently. We also evaluated a variant, DPT (Rand, PPO), which pretrains on random in-context datasets (like DPT), but still using PPO action labels. The performance is worse than other DPT variants in some settings, but only marginally so. In Appendix B, we analyze the sensitivity of DPT to other hyperparameters, such as context size and amount of pretraining data. These results give further evidence that DPT can learn an algorithm that transcends its pretraining data.

## 6 Theory

We now shed light on the observations of the previous empirical results through a theoretical analysis. Our main result shows that DPT (under a slight modification to pretraining) essentially is capable of executing posterior sampling (PS) reinforcement learning in-context. PS is a generalization of

Figure 5: (a) In _Dark Room (Three Tasks)_, DPT stitches a new, optimal trajectory to the goal (blue) given two in-context demonstrations of other tasks (pink and orange). (b) Offline Dark Room performance of DPT trained on PPO data. (c) Online Dark Room performance of DPT trained on PPO data.

Thompson Sampling for RL in MDPs . It maintains and samples from a posterior over tasks \(\) given historical data \(D\) and executes optimal policies \(_{}^{*}\) (see Appendix C for a formal outline). It is provably sample-efficient with online Bayesian regret guarantees , but maintaining posteriors is generally computationally intractable. The ability for DPT to perform PS in-context suggests a path towards computation- and provably sample-efficient RL with priors learned from the data.

### History-Dependent Pretraining and Assumptions

We start with a modification to the pretraining of DPT. Rather than conditioning only on \(s_{}\) and \(D\) to predict \(a^{}_{}^{*}(|s_{})\), we propose also conditioning on a sequence \(_{h}=(s_{1:h},a_{1:h}^{})\) where \(s_{1:h}_{h}(^{h})\) is a distribution over sets of states, independent of \(\), and \(a_{h^{}}^{}_{}^{*}(|s_{h^{}})\) for \(h^{}[h]\). Thus, we use \(_{}^{*}\) to label both the query state (which is the prediction label) and the sequence of states sampled from \(_{h}\). Note that this does not require any environment interactions and hence no sampling from either \(T_{}\) or \(R_{}\). At test-time at step \(h\), this will allow us to condition on the history \(_{h-1}\) of states that \(M_{}\) visits and the actions that it takes in those states. Formally, the learned \(M_{}\) is deployed as follows, given \(D\). (1) At \(h=0\), initialize \(_{0}=()\) to be empty. (2) At step \(h\), visit \(s_{h}\) and find \(a_{h}\) by sampling from \(M_{}(|s_{},D,_{h-1})\). (3) Append \((s_{h},a_{h})\) to \(_{h-1}\) to get \(_{h}\). Note for bandits and contextual bandits (\(H=1\)), there is no difference between this and the original pretraining procedure of prior sections because \(_{0}\) is empty. For MDPs, the original DPT can be viewed as a practically convenient approximation.6

We now make several assumptions to simplify the analysis. First, assume \(_{}\), \(_{}\), and \(\) have sufficient support such that all conditional probabilities of \(P_{pre}\) are well defined. Similar to other studies of in-context learning , we assume \(M_{}\) fits the pretraining distribution exactly with enough coverage and data, so that the focus of the analysis is just the in-context learning abilities.

**Assumption 1**.: _(Learned model is consistent). Let \(M_{}\) denote the pretrained model. For all \((s_{},D,_{h})\), we have \(P_{pre}(a|s_{},D,_{h})=M_{}(a|s_{},D,_{h})\) for all \(a\)._

To provide some cursory justification, if \(M_{}\) is the global minimizer of (2), then we might expect that \(_{P_{pre}}\|P_{pre}(|s_{},D,_{h})-M_{}( |s_{},D,_{h})\|_{1}^{2} 0\) as the number of pretraining samples \(N\) with high probability for transformer model classes of bounded complexity (see Proposition C.1). Approximate versions of the above assumptions are easily possible but obfuscate the key elements of the analysis. We also assume that the in-context dataset \(D_{}\) is compliant , meaning that the actions from \(D\) can depend only on the observed history and not additional confounders. Note that this still allows \(_{}\) to be very general -- it could be generated randomly or from adaptive algorithms like PPO or TS. Without compliance \(_{}\) can influence \(M_{}\). In Proposition 6.4, we show that all compliant \(_{}\) form a sort of equivalence class that generate the same \(M_{}\). For the remainder, we assume all \(_{}\) are compliant.

**Definition 6.1** (Compliance).: _The in-context dataset distribution \(_{}(;)\) is compliant if, for all \(i[n]\), the \(i\)th action of the dataset, \(a_{i}\), is conditionally independent of \(\) given the \(i\)th state \(s_{i}\) and partial dataset, \(D_{i-1}\), so far. In other words, the distribution \(_{}(a_{i}|s_{i},D_{i-1};)\) is invariant to \(\)._

### Main Results

Equivalence of DPT and PS.We now state our main result which shows that the trajectories generated by a pretrained \(M_{}\) will follow the same distribution as those from a well-specified PS algorithm. In particular, let PS use the well-specified prior \(_{}\). Let \(_{c}\) be an arbitrary task. Let \(P_{ps}( D,_{c})\) and \(P_{M_{}}( D,_{c})\) denote the distributions over trajectories \(_{H}()^{H}\) generated from running PS and \(M_{}(|,D,)\), respectively, in task \(_{c}\) given historical data \(D\).

**Theorem 1** (\(\)).: _Let the above assumptions hold. Then, \(P_{ps}(_{H} D,_{c})=P_{M_{}}(_{H} D,_{c})\) for all trajectories \(_{H}\)._

Regret implications.With almost no extra work besides verifying assumptions, we can see this result in action. Let us specialize to the finite MDP setting . Suppose we pretrain \(M_{}\) on a distribution \(_{}\) over MDPs with \(S:=||\) and \(A:=||\). Let \(_{}\) be constructed by uniform sampling \((s_{i},a_{i})\) and observing \((r_{i},s_{i}^{})\) for \(i[KH]\). Let \([r_{h}|s_{h},a_{h}]\). And let \(_{}\) and \(_{h}\) be uniform over \(\) and \(^{h}\) (for all \(h\)) respectively. Finally, let \(_{}\) be the distribution over test tasks with the same cardinalities. For a task \(\), define the online cumulative regret of DPT over \(K\) episodes as \(_{}(M_{}):=_{k[K]}V_{}(^{*}_{})-V_{}( _{k})\) where \(_{k}(|s_{h})=M_{}(|s_{h},D_{(k-1)},_{h-1})\) and \(D_{(k)}\) contains the first \(k\) episodes collected from \(_{1:k}\).

**Corollary 6.2** (Finite MDPs).: _Suppose that \(_{}_{}()/_{}() \) for some \(>0\). For the above MDP setting, the pretrained model \(M_{}\) satisfies \(_{_{}}[_{}(M_{}) ]}(H^{3/2}S)\)._

A similar analysis due to  allows us to see theoretically why pretraining on (lattently) linear bandits can lead to an in-context algorithm competitive with linear bandit algorithms, even when the in-context datasets are generated by algorithms unaware of this structure. We observed this empirically in Section 4. Consider a similar setup as there where \(\) is a singleton, \(\) is finite but large, \(_{}^{d}\) is sampled as \(_{}(0,I/d)\), \(:^{d}\) is a fixed feature map with \(_{a}\|(a)\|_{2} 1\), and the reward of \(a\) in task \(\) is distributed as \(((_{},(a)),1)\). This time, we let \(_{}(;)\) be given by running Thompson Sampling with Gaussian priors and likelihood functions on \(\).

**Corollary 6.3** (Latent representation learning in linear bandits).: _For \(_{}=_{}\) in the above linear bandit setting, \(M_{}\) satisfies \(_{_{}}[_{}(M_{}) ]}(d)\)._

This significantly improves over the \(}(|K})\) upper regret bound for TS that does not leverage the linear structure. This highlights how DPT can have provably tighter upper bounds on future bandit problems than the algorithms used to generate its (pretraining) data. Note that if there is additional structure in the tasks which yields a tighter regret bound (for example if there are only a small finite number of MDPs in the possible distribution), that may further improve performance, such as by removing the dependence on the state, action, or dimension in these cases.

**Invariance of \(M_{}\) to compliant \(_{}\).** Our final result sheds light on how \(_{}\) impacts the final DPT behavior \(M_{}\). Combined with Assumption 1, \(M_{}\) is invariant to \(_{}\) satisfying Definition 6.1.

**Proposition 6.4**.: _Let \(P^{1}_{pre}\) and \(P^{2}_{pre}\) be pretraining distributions that differ only by their in-context dataset distributions, denoted by \(^{1}_{}\) and \(^{2}_{}\). If \(^{1}_{}\) and \(^{2}_{}\) are compliant with the same support, then \(P^{1}_{pre}(a^{*}|s_{},D,_{h})=P^{2}_{pre}(a^{*}|s_{ },D,_{h})\) for all \(a^{*},s_{},D,_{h}\)._

That is, if we generate in-context datasets \(D\) by running various algorithms that depend only on the observed data in the current task, we will end up with the same \(M_{}\). For example, TS could be used for \(^{1}_{}\) and PPO for \(^{2}_{}\). Expert-biased datasets discussed in Section 4 violate Definition 6.1, since privileged knowledge of \(\) is being used. This helps explain our empirical results that pretraining on expert-biased datasets leads to a qualitatively different learned model at test-time (Fig. 3c).

## 7 Discussion

We studied the problem of in-context RL. We introduced a new pretraining method and transformer model, DPT, which is trained via supervised learning to predict optimal actions given an in-context dataset of interactions. Through in-depth evaluations in classic decision problems in bandits and MDPs, we showed that this simple objective naturally gives rise to an in-context RL algorithm that is capable of online exploration and offline decision-making in a way that leverages structure in the pretraining data. In some cases, the RL algorithm learned can transcend the quality of the pretraining data. Our empirical and theoretical results provide first steps towards understanding these capabilities and what factors are important for success. Via the simplicity of pretraining, we can sidestep the complexities of hand-designing exploration or conservatism in RL algorithms while simultaneously allowing the transformer to derive novel few-shot strategies. These findings underscore the potential of supervised pretraining to equip transformer models with in-context RL abilities.

Limitations and future work.One limitation of DPT is the requirement of optimal actions at pretraining. Empirically, we find that this requirement can be relaxed by using actions generated by another RL-trained agent during pretraining, which only leads to a slight loss in performance. However, fully understanding this problem and how best to leverage multi-task decision-making datasets remains a key open problem. We also discussed that the practical implementation for MDPs differs from true posterior sampling. It would be interesting to further understand and bridge this empirical-theoretical gap in the future. Finally, further investigation is required to understand the implications of these findings for existing transformer-based large language models that are increasingly being deployed in decision-making settings .