# Is Value Learning Really

the Main Bottleneck in Offline RL?

Seohong Park\({}^{1}\) Kevin Frans\({}^{1}\) Sergey Levine\({}^{1}\) Aviral Kumar\({}^{2}\)

\({}^{1}\)University of California, Berkeley \({}^{2}\)Carnegie Mellon University

seohong@berkeley.edu

###### Abstract

While imitation learning requires access to high-quality data, offline reinforcement learning (RL) should, in principle, perform similarly or better with substantially lower data quality by using a value function. However, current results indicate that offline RL often performs worse than imitation learning, and it is often unclear what holds back the performance of offline RL. Motivated by this observation, we aim to understand the bottlenecks in current offline RL algorithms. While poor performance of offline RL is typically attributed to an imperfect value function, we ask: _is the main bottleneck of offline RL indeed in learning the value function, or something else?_ To answer this question, we perform a systematic empirical study of (1) value learning, (2) policy extraction, and (3) policy generalization in offline RL problems, analyzing how these components affect performance. We make two surprising observations. **First**, we find that the choice of a policy extraction algorithm significantly affects the performance and scalability of offline RL, often more so than the value learning objective. For instance, we show that common value-weighted behavioral cloning objectives (_e.g._, AWR) do not fully leverage the learned value function, and switching to behavior-constrained policy gradient objectives (_e.g._, DDPG+BC) often leads to substantial improvements in performance and scalability. **Second**, we find that a big barrier to improving offline RL performance is often imperfect policy generalization on test-time states out of the support of the training data, rather than policy learning on in-distribution states. We then show that the use of suboptimal but high-coverage data or test-time policy training techniques can address this generalization issue in practice. Specifically, we propose two simple test-time policy improvement methods and show that these methods lead to better performance.

## 1 Introduction

Data-driven approaches that convert offline datasets of past experience into policies are a predominant approach for solving control problems in several domains . Primarily, there are two paradigms for learning policies from offline data: imitation learning and offline reinforcement learning (RL). While imitation requires access to high-quality demonstration data, offline RL loosens this requirement and can learn effective policies even from suboptimal data, which makes offline RL preferable to imitation learning in theory. However, recent results show that tuning imitation learning by collecting more expert data often outperforms offline RL even when provided with sufficient data in practice , and it is often unclear what holds back the performance of offline RL.

The primary difference between offline RL and imitation learning is the use of a _value function_, which is absent in imitation learning. The value function drives the learning progress of offline RL methods, enabling them to learn from suboptimal data. Value functions are typically trained via temporal-difference (TD) learning, which presents convergence  and representational  pathologies. This has led to the conventional wisdom that the gap between offline RL and imitation is a direct consequence of poor value learning . Following up on this conventional wisdom, recent research in the community has been devoted towards improving the value function quality of offline RL algorithms . While improving value functions will definitely help improve performance, we question whether this is indeed the best way to maximally improvethe performance of offline RL, or if there is still headroom to get offline RL to perform better even with current value learning techniques. More concretely, given an offline RL problem, we ask: _is the bottleneck in learning the value function, the policy, or something else? What is the best way to improve performance given the bottleneck?_

We answer these questions via an extensive empirical study. There are three potential factors that could bottleneck an offline RL algorithm: (B1) imperfect **value** function estimation, (B2) imperfect **policy** extraction guided by the learned value function, and (B3) imperfect policy **generalization** to states that it will visit during evaluation. While all of these contribute in some way to the performance of offline RL, we wish to identify how each of these factors interact in a given scenario and develop ways to improve them. To understand the effect of these factors, we use data size, quality, and coverage as levers for systematically controlling their impacts, and study the "data-scaling" properties, _i.e._, how data quality, coverage, and quantity affect these three aspects of the offline RL algorithm, for three value learning methods and three policy extraction methods on diverse types of environments. These data-scaling properties reveal how the performance of offline RL is bottlenecked in each scenario, hinting at the most effective way to improve the performance.

Through our analysis, we make two surprising observations, which naturally provide actionable advice for both domain-specific practitioners and future algorithm development in offline RL. **First, we find that the choice of a _policy extraction_ algorithm often has a larger impact on performance than value learning algorithms**, despite the policy being subordinate to the value function in theory. This contrasts with the common practice where policy extraction often tends to be an afterthought in the design of value-based offline RL algorithms. Among policy extraction algorithms, we find that behavior-regularized policy gradient (_e.g._, DDPG+BC ) almost always leads to much better performance and favorable data scaling than other widely used methods like value-weighted regression (_e.g._, AWR [46; 47; 58]). We then analyze why constrained policy gradient leads to better performance than weighted behavioral cloning via extensive qualitative and quantitative analyses.

**Second, we find that the performance of offline RL is often heavily bottlenecked by how well the policy _generalizes_ to _test-time_ states, rather than its performance on training states_. Namely, our analysis suggests that existing offline algorithms are often already great at learning an optimal policy from suboptimal data on _in-distribution_ states, to the degree that it is saturated, and the performance is often simply bottlenecked by the policy accuracy on novel states that the agent encounters at test time. This provides a new perspective on _generalization_ in offline RL, which differs from the previous focus on pessimism and behavioral regularization. Based on this observation, we provide two practical solutions to improve the generalization bottleneck: the use of high-coverage datasets and test-time policy extraction techniques. In particular, we propose new on-the-fly policy improvement techniques that further distill the information in the value function into the policy on test-time states _during evaluation rollouts_, and show that these methods lead to better performance.

Our main contribution is an analysis of the bottlenecks in offline RL as evaluated via data-scaling properties of various algorithmic choices. Contrary to the conventional belief that value learning is the bottleneck of offline RL algorithms, we find that the performance is often limited by the choice of a policy extraction objective and the degree to which the policy generalizes at test time. This suggests that, with an appropriate policy extraction procedure (_e.g._, gradient-based policy extraction) and an appropriate recipe for handling generalization (_e.g._, test-time training with the value function), collecting more high-coverage data to train a value function is a universally better recipe for improving offline RL performance, whenever the practitioner has access to collecting some new data for learning. These results also imply that more research should be pursued in developing policy learning and generalization recipes to translate value learning advances into performant policies.

## 2 Related work

Offline reinforcement learning [31; 33] aims to learn a policy solely from previously collected data. The central challenge in offline RL is to deal with the distributional shift in the state-action distributions of the dataset and the learned policy. This shift could lead to catastrophic value overestimation if not adequately handled . To prevent such a failure mode, prior works in offline RL have proposed diverse techniques to estimate more suitable value functions solely from offline data via conservatism [8; 26], out-of-distribution penalization [14; 53; 59], in-sample maximization [17; 25; 61], uncertainty minimization [1; 19; 60], convex duality [32; 41; 50], or contrastive learning . Then, these methods train policies to maximize the learned value function with behavior-regularized policy gradient (_e.g._, DDPG+BC) [14; 34], weighted behavioral cloning (_e.g._, AWR) [46; 47], or sampling-based action selection (_e.g._, SfBC) [7; 15; 21]. Depending on the algorithm, these value learning and policy extraction stages can either be interleaved [14; 26; 42] or decoupled [5; 11; 17; 25]. Despite the presence of a substantial number of offline RL algorithms, relatively few works have aimed to analyze and understand the practical challenges in offline RL. Instead of proposing a new algorithm, we mainly aim to understand the current bottlenecks in offline RL via a comprehensive analysis of existing techniques so that we can inform future methodological development.

Several prior works have analyzed individual components of offline RL or imitation learning algorithms: value bootstrapping [14; 15], representation learning [27; 29; 62], data quality , differences between RL and behavioral cloning (BC) , and empirical performance [10; 23; 35; 36; 54]. Our analysis is distinct from these lines of work: we analyze challenges appearing due to the interaction between these individual components of value function learning, policy extraction, and generalization, which allows us to understand the bottlenecks in offline RL from a _holistic_ perspective. This can inform how a practitioner could extract the most by improving one or more of these components, depending upon their problem. Perhaps the closest study to ours is Fu et al. , which study whether representations, value accuracy, or policy accuracy can explain the performance of offline RL. While this study makes insightful recommendations about which algorithms to use and reveals the potential relationships between some metrics and performance, the conclusions are only drawn from D4RL locomotion tasks , which are known to be relatively simple and saturated [48; 53], and the data-scaling properties of algorithms are not considered. In addition, this prior study does not identify policy _generalization_, which we find to be one of the most substantial yet overlooked bottlenecks in offline RL. In contrast, we conduct a large-scale analysis on diverse environments (_e.g._, pixel-based, goal-conditioned, and manipulation tasks) and analyze the bottlenecks in offline RL with the aim of providing actionable takeaways that can enhance the performance and scalability of offline RL.

## 3 Main hypothesis

Our primary goal is to understand when and how the performance of offline RL can be bottlenecked in practice. As discussed earlier, there exist three potential factors that could bottleneck an offline RL algorithm: (B1) imperfect **value** function estimation from data, (B2) imperfect **policy** extraction from the learned value function, and (B3) imperfect **generalization** on the test-time states that the policy visits in evaluation rollouts. We note that the bottleneck of an offline RL algorithm under a certain dataset can always be attributed to one or some of these factors, since the policy will attain optimal performance if both value learning and policy extraction are perfect, and perfect generalization to test-time states is possible.

**Our main hypothesis** in this work is that, somewhat contrary to the prior belief that the accuracy of the value function is the primary factor limiting performance of offline RL methods, **policy learning is often the main bottleneck of offline RL**. In other words, while value function accuracy is certainly important, how the policy is extracted from the value function (B2) and how well the agent generalizes on states that it visits at the deployment time (B3) are often the main factors that significantly affect both the performance and scalability of offline RL. To verify this hypothesis, we conduct two main analyses in this paper. In Section 4, we compare the effects of value learning and policy extraction on performance under various types of environments, datasets, and algorithms (B1 and B2). In Section 5, we analyze the degree to which the policy generalizes on test-time states affects performance (B3).

## 4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)

We first perform controlled experiments to identify whether imperfect value functions (B1) or imperfect policy extraction (B2) contribute more to holding back the performance of offline RL in practice. To systematically compare value learning and policy extraction, we run different algorithms while varying the _the amounts of data_ for value function training and policy extraction, and draw **data-scaling matrices** to visualize the aggregated results. Increasing the amount of data provides a convenient lever to control the effect of each component, enabling us to draw conclusions about whether the value or the policy serves as a bigger bottleneck in different regimes when different amounts of training data are available (or can be collected by a practitioner for a given problem), and to understand the differences between various algorithms.

To clearly dissect value learning from policy learning, we focus on offline RL methods with _decoupled_ value and policy training phases (_e.g._, One-step RL , IQL , CRL , etc.), where policy learning does not affect value learning. In other words, we focus on methods that first train a value function without involving policies, and then extract a policy from the learned value function with a separate objective. While this might sound a bit restrictive, we surprisingly find that policy learning is often the main bottleneck _even in these decoupled methods_, which attempt to solve a simple, single-step optimization problem for extracting a policy given a static and stationary value function.

### Analysis setup

We now introduce the value learning objectives, policy extraction objectives, and environments that we study in our analysis (see Appendix B for preliminaries).

**Value learning objectives.** We consider three decoupled value learning objectives that fit value functions without involving policy learning: **(1) implicit Q-learning (IQL)**, **(2) SARSA**, and **(3) contrastive RL (CRL)**. IQL fits an optimal Q function (\(Q^{*}\)) by approximating the Bellman optimality operator with an expectile loss. SARSA fits a behavioral Q function (\(Q^{}\)) using the Bellman evaluation operator. In goal-conditioned tasks, we employ CRL instead of SARSA, which similarly fits a behavioral Q function, but with a different contrastive learning-based objective that leads to better performance. We refer to Appendix D.1 for detailed descriptions of these value learning methods.

**Policy extraction objectives.** Prior works in offline RL typically use one of the following objectives to extract a policy from the value function. All of them are built upon the same principle: maximizing values while being close to the behavioral policy, to avoid the exploitation of erroneous critic values.

* **(1) Weighted behavioral cloning (_e.g._, AWR).** Weighted behavioral cloning is one of the most widely used offline policy extraction objectives for its simplicity . Among weighted behavioral cloning methods, we consider advantage-weighted regression (AWR ) in this work, which maximizes the following objective: \[_{}\ _{}()=_{s,a}[ e^{(Q(s,a)-V(s))}(a s)],\] (1) where \(\) is an (inverse) temperature hyperparameter. Intuitively, AWR assigns larger weights to higher-advantage transitions when cloning behaviors, which makes the policy selectively copy only good actions from the dataset.
* **(2) Behavior-constrained policy gradient (_e.g._, DDPG+BC).** Another popular policy extraction objective is behavior-constrained policy gradient, which directly maximizes Q values while not deviating far away from the behavioral policy . In this work, we consider the objective that combines deep deterministic policy gradient and behavioral cloning (DDPG+BC ): \[_{}\ _{}()=_{s,a }[Q(s,^{}(s))+(a s)],\] (2) where \(^{}(s)=_{a( s)}[a]\) and \(\) is a hyperparameter that controls the strength of the BC regularizer.
* **(3) Sampling-based action selection (_e.g._, SfBC).** Instead of learning an explicit policy, some previous methods implicitly define a policy as the action with the highest value among action samples from the behavioral policy . In this work, we consider the following objective that selects the \(\) action from behavioral candidates (SfBC ): \[(s)=*{arg\,max}_{a\{a_{1},,a_{N}\}}[Q(s,a)],\] (3) where \(a_{1},,a_{N}\) are sampled from the learned BC policy \(^{}( s)\).

**Environments and datasets.** To understand how different value learning and policy extraction objectives affect performance and data scalability, we consider eight environments (Figure 10) across state- and pixel-based, robotic locomotion and manipulation, and goal-conditioned and single-task settings with varying levels of data suboptimality: **(1)** gc-antmaze-large, **(2)** antmaze-large, **(3)** ddrl-hopper, **(4)** d4rl-walker2d, **(5)** exorl-walker, **(6)** exorl-cheetah, **(7)** kitchen, and **(8)** gc-roboverse. We highlight some features of these tasks: exorl-{walker, cheetah} are tasks with highly suboptimal, diverse datasets collected by exploratory policies, gc-antmaze-large and gc-roboverse are goal-conditioned ('gc-') tasks, and gc-roboverse is a _pixel-based_ robotic manipulation task with a \(48 48 3\)-dimensional observation space. For some tasks (_e.g._, gc-antmaze-large and kitchen), we additionally collect data to enhance dataset sizes to depict scaling properties clearly. We refer to Appendix D.2 for the complete task descriptions.

### Results: Policy extraction mechanisms substantially affect data-scaling trends

Figure 1 shows the data-scaling matrices of three policy extraction algorithms (AWR, DDPG+BC, and SfBC) and three value learning algorithms (IQL and {SARSA or CRL}) on eight environments, aggregated from a total of \(15{,}488\) runs (\(8\) seeds for each cell, numbers after "\(\)" denote standard

deviations). In each matrix, we individually tune the hyperparameter for policy extraction (\(\) or \(N\)) for each entry. These matrices show how performance varies with different amounts of data for the value and the policy. In our analysis, we specifically focus on the _color gradients_ of these matrices, which reveal the main limiting factor behind the performance of offline RL in each setting. Note that the color gradients are mostly either vertical, horizontal, or diagonal. Vertical (\(\)) color gradients indicate that the performance is most strongly affected by the amount of _policy_ data, horizontal (\(\)) gradients indicate it is mostly affected by _value_ data, and diagonal (\(\)) gradients indicate both.

Side-by-side comparisons of the data-scaling matrices from different policy extraction methods in Figure 1 suggest that, perhaps surprisingly, **different policy extraction algorithms often lead to significantly different performance and data-scaling behaviors, even though they extract policies from the _same value function_ (recall that the use of decoupled algorithms allows us to train a single value function, but use it for policy extraction in different ways). For example, on exorl-walker and exorl-cheetah, AWR performs remarkably poorly compared to DDPG+BC or SfBC on both value learning algorithms. Such a performance gap between policy extraction algorithms exists even when the value function is far from perfect, as can be seen in the low-data regimes in gc-antmaze-large and kitchen. In general, we find that the choice of a policy extraction procedure affects performance

 
**Task (Value Algorithm)** & **AWR** & **DDPG+BC** & **SfBC** \\  gc-antmaze-large (IdQL) & 51\(\) & **58\(\)** & **58\(\)** \\ gc-antmaze-large (CRL) & 37\(\) & **58\(\)** & 51\(\)** \\ amaneze-large (IQL) & 12\(\) & 17\(\) & **24\(\)** \\ amaneze-large (SARSA) & **0** & **0** & **0** & **0** \\ kitchen (IQL) & 80\(\) & **86\(\)** & 17\(\)** \\ kitchen (SASA) & **79\(\)** & **83\(\)** & 73\(\)** \\ exorl-walker (IQL) & 99\(\) & **191\(\)** & 140\(\)** \\ exorl-walker (SARSA) & 94\(\)0 & **193\(\)** & 125\(\)** \\  

Table 1: **DDPG+BC is often the best policy extraction method.** We aggregate the performances over the entire data-scaling matrix and then over \(8\) random seeds in each setting. Scores at or above \(95\%\) of the best score are highlighted in bold. The table shows that DDPG+BC is better than or as good as AWR in **15 out of 16 settings**. We note that policy extraction hyperparameters are individually tuned for each setting (Figure 11).

Figure 1: **Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and [SARSA or CRL]).** To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\(\), \(\), \(\)) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.

often more than the choice of a value learning objective except antmaze-large, where the value function must be learned from sparse-reward, suboptimal datasets with long-horizon trajectories.

Among policy extraction algorithms, we find that **DDPG+BC almost always achieves the best performance and scaling behaviors across the board**, followed by SfBC, and the performance of AWR falls significantly behind the other two extraction algorithms in many cases (Table 1). Notably, the data-scaling matrices of AWR always have vertical (\(\)) or diagonal (\(\)) color gradients, implying that it does not fully utilize the value function (see Section 4.3 for clearer evidence). In other words, a non-careful choice of the policy extraction algorithm (_e.g._, weighted behavioral cloning) hinders the use of learned value functions, imposing an unnecessary bottleneck on the performance of offline RL.

### Deep dive 1: How different are the scaling properties of AWR and DDPG+BC?

To gain further insights into the difference between value-weighted behavioral cloning (_e.g._, AWR) and behavior-regularized policy gradient (_e.g._, DDPG+BC), we draw data-scaling matrices with different values of \(\) (in Equations (1) and (2)), a hyperparameter that interpolates between RL and BC. Note that \(=0\) corresponds to BC in AWR and \(=\) corresponds to BC in DDPG+BC. We recall that the previous results (Figure 1) use the best temperature for each matrix entry (_i.e._, aggregated by the maximum over temperatures), but here we show the full results with individual hyperparameters.

Figure 2 highlights the results on gc-antmaze-large and exorl-walker (see Appendix E for the full results). The results on gc-antmaze-large show a clear difference in scaling matrices between AWR and DDPG+BC. That is, AWR is _always_ policy-bounded regardless of the BC strength \(\) (_i.e._, vertical (\(\)) color gradients), whereas DDPG+BC has two "modes": it is policy-bounded (\(\)) when \(\) is large, and value-bounded (\(\)) and when \(\) is small. Intriguingly, an in-between value of \(=1.0\) in DDPG+BC enables having the best of both worlds, significantly boosting performances across the entire matrix (note that it achieves very strong performance even with a \(0.1\)M-sized dataset)! This difference in scaling behaviors suggests that the use of the learned value function in weighted behavioral cloning is limited. This becomes more evident in exorl-walker (Figure 2), where AWR fails to achieve strong performance even with a very high temperature value (\(=100\)).

### Deep dive 2: _Why_ is DDPG+BC better than AWR?

We have so far seen several empirical results that suggest behavior-regularized policy gradient (_e.g._, DDPG+BC) should be preferred to weighted behavioral cloning (_e.g._, AWR) in any case. What makes DDPG+BC so much better than AWR? There are three potential reasons.

First, AWR only has a _mode-covering_ weighted behavioral cloning term, while DDPG+BC has both _mode-seeking_ first-order value maximization and _mode-covering_ behavioral cloning terms. As a result, actions learned by AWR always lie within the convex hull of dataset actions, whereas DDPG+BC can "hillclimb" the learned value function, even allowing extrapolation to some degree while not deviating too far away from the mode. This not only enables a better use of the value function but produces a wider range of actions. To illustrate this, we plot test-time action sampled from policies learned by AWR and DDPG+BC on exorl-walker. Figure 3 shows that AWR actions are relatively centered around the origin, while DDPG+BC actions are more spread out, which can sometimes help achieve an even higher degree of optimality.

Figure 3: **AWR vs. DDPG actions.**

Figure 2: **Data-scaling matrices of AWR and DDPG+BC with different BC strengths (\(\)).** In gc-antmaze-large, AWR is _always_ policy-bounded (\(\)), but DDPG+BC has _both_ policy-bounded (\(\)) and value-bounded (\(\)) modes, depending on the value of \(\). Notably, an in-between value of \(=1.0\) in DDPG+BC leads to the best of both worlds (see the bottom left corner of gc-antmaze-large with \(0.1\)M datasets)!

Second, value-weighted behavioral cloning uses a much smaller number of _effective_ samples than behavior-regularized policy gradient methods, especially when the temperature (\(\)) is large. This is because a small number of high-advantage transitions can potentially dominate learning signals for AWR (_e.g._, a single transition with a weight of \(e^{10}\) can dominate other transitions with smaller weights like \(e^{2}\)). As a result, AWR effectively uses only a fraction of datapoints for policy learning, being susceptible to overfitting. On the other hand, DDPG+BC is based on first-order maximization of the value function without any weighting, and thus is free from such an issue. Figure 4 illustrates this, where we compare the training and validation policy losses of AWR and DDPG+BC on gc-antmaze-large with the smallest \(0.1\)M dataset (\(8\) seeds). The results show that AWR with a large temperature (\(=3.0\)) causes severe overfitting. Indeed, Figure 1 shows DDPG+BC often achieves significantly better performance than AWR in low-data regimes.

Third, AWR has a theoretical pathology in the regime with limited samples: since the coefficient multiplying \((a s)\) in the AWR objective (Equation (1)) is always positive, AWR can increase the likelihood of _all_ dataset actions, regardless of how optimal they are. If the training dataset covers all possible actions, then the condition for normalization of the probability density function of \((a s)\) would alleviate this issue, but this coverage assumption is rarely achieved in practice. Under limited data coverage, and especially when the policy network is highly expressive and dataset states are unique (_e.g._, continuous control problems), AWR can in theory _memorize_ all state-action pairs in the dataset, potentially reverting to _unweighted_ behavioral cloning.

## 5 Empirical analysis 2: Policy generalization (B3)

We now turn our focus to the third hypothesis, that the degree to which the agent **generalizes** to states that it visits at the evaluation time has a significant impact on performance. This is a unique bottleneck to the _offline_ RL problem setting, where the agent encounters new, potentially out-of-distribution states at test time.

### Analysis setup

To understand this bottleneck concretely, we first define three key metrics quantifying a notion of _accuracy_ of a given policy in terms of distances against the optimal policy. Specifically, we use the following mean squared error (MSE) metrics to quantify policy accuracy:

\[() =_{s_{}}[((s)-^{*}(s) )^{2}],\] (4) \[() =_{s_{}}\ \ [((s)-^{*}(s))^{2}],\] (5) \[() =_{s p^{*}()}\ [((s)-^{*}(s))^{2}],\] (6)

where \(_{}\) and \(_{}\) respectively denote the training and validation datasets, \(^{*}\) denotes an optimal policy, which we assume access to for evaluation and visualization purposes only. Validation MSE

Figure 4: **AWR overfits.**

Figure 5: **Three distributions for the MSE metrics.**

measures the policy accuracy on states sampled from the _same_ dataset distribution as the training distribution (_i.e._, in-distribution MSE, Figure 5), while evaluation MSE measures the policy accuracy on states the agent visits at test time, which can potentially be very different from the dataset distribution (_i.e._, out-of-distribution MSE, Figure 5). We note that, while these metrics might not always be perfectly indicative of the performance of a policy (see Appendix A), they serve as convenient proxies to estimate policy accuracy in many continuous-control domains in practice.

One way to measure the degree to which test-time generalization affects performance is to evaluate how much room there is for various policy MSE metrics to improve when further training on additional policy rollouts is allowed. The distribution of states induced by rolling out the policy is an ideal distribution to improve performance, as the policy receives direct feedback on its own actions at the states it would visit. Hence, by tracking the extent to which various MSEs improve and how their predictive power towards performance evolves over online interaction, we will be able to understand which is a bigger bottleneck: in-distribution generalization (_i.e._, improvements towards validation MSE under the offline dataset distribution) or out-of-distribution generalization (_i.e._, improvements in evaluation MSE under the on-policy state distribution). To this end, we measure these three types of MSEs over the course of online interaction, when learning from a policy trained on offline data only (_i.e._, the _offline-to-online_ RL setting). Specifically, we train offline-to-online IQL agents on six D4RL  tasks (antmaze-{medium, large}, kitchen, and adroit-{pen, hammer, door}), and measure the MSEs with pre-trained expert policies that approximate \(^{*}\) (see Appendix D.4).

### Results: Test-time generalization is often the main bottleneck in offline RL

Figure 6 shows the results (8 seeds with 95% confidence intervals), where we denote online training steps in red. The results show that, perhaps surprisingly, in many environments continued training with online interaction _only_ improves evaluation MSEs, while training and validation MSEs often _remain completely flat_ during online training. Also, we can see that the evaluation MSE is the most predictive of the performance of offline RL among the three metrics. In other words, the results show that, despite the fact that on-policy data provides for an oracle distribution to improve policy accuracy, performance improvement is often only reflected in the evaluation MSEs computed under the policy's own state distribution.

What does this tell us? This indicates that, current offline RL methods may already be sufficiently great at learning the best possible policy _within the distribution of states covered by the offline dataset_, and **the agent's performance is often mainly determined by how well it _generalizes_ under its own state distribution at test time**, as suggested by the fact that evaluation MSE is most predictive of performance. This finding somewhat contradicts prior beliefs: while algorithmic techniques in offline RL largely attempt to improve policy optimality on _in-distribution states_ (by addressing the issue with out-of-distribution _actions_), our results suggest that modern offline RL algorithms may

Figure 6: **How do offline RL policies improve with additional interaction data?** In many environments, offline-to-online RL _only_ improves evaluation MSEs, while validation MSEs and training MSEs often _remain completely flat_ (see Section 5 for the definitions of these metrics). This suggests that current offline RL algorithms may already be great at learning an effective policy on _in-distribution_ states, and the performance of offline RL is often mainly determined by how well the policy _generalizes_ on its own state distribution at test time.

already saturate on this axis. Further performance differences may simply be due to the effects of a given offline RL objective on _novel states_, which very few methods explicitly control!

That said, controlling test-time generalization might also appear impossible: while offline RL methods could hillclimb on validation accuracy via a combination of techniques that address statistical errors such as regularization (_e.g._, Dropout , LayerNorm , etc.), improving _test-time_ policy accuracy requires generalization to a potentially very different _distribution_ (Figure 5), which is theoretically impossible to guarantee without additional coverage or structural assumptions, as the test-time state distribution can be arbitrarily adversarial in the worst case. However, we claim that if we actively utilize the information available at test time or have the freedom to design offline datasets, it is possible to improve test-time policy accuracy in practice, and we discuss such solutions below (see Appendix C for further discussions).

### Solution 1: Improve offline data coverage

If we have the freedom to control the data collection process, perhaps the most straightforward way to improve test-time policy accuracy is to use a dataset that has as _high coverage_ as possible so that test-time states can be covered by the dataset distribution. However, at the same time, high-coverage datasets often involve exploratory actions, which may compromise the quality (optimality) of the dataset. This makes us wonder in practice: _which is more important, high coverage or high optimality?_

To answer this question, we revert back to our analysis tool of data-scaling matrices from Section 4 and empirically compare the data-scaling matrices on datasets collected by expert policies with different levels of action noises (\(_{}\)). Figure 7 shows the results of IQL agents on gc-antmaze-large and adroit-pen (\(8\) seeds each). The results suggest that the performance of offline RL generally improves as the dataset has better state coverage, despite the increase in suboptimality. This is aligned with our findings in Figure 6, which indicate that the main challenge of offline RL is often _not_ learning an effective policy from suboptimal data, but rather learning a policy that generalizes well to test-time states. In addition, we note that it is crucial to use a value gradient-based policy extraction method (DDPG+BC; see Section 4) in this case as well, where we train a policy from high-coverage data. For instance, in low-data regimes in gc-antmaze-large in Figure 7, AWR fails to fully leverage the value function, whereas DDPG+BC still allows the algorithm to improve performance with better value functions. Based on our findings, we suggest practitioners prioritize _high coverage_ (particularly around the states that the optimal policy will likely visit) over high optimally when collecting datasets.

### Solution 2: Test-time policy improvement

If we do not wish to modify offline data collection, another way to improve test-time policy accuracy is to _on-the-fly_ train or steer the policy guided by the learned value function on _test-time states_. Especially given that imperfect policy extraction from the value function is often a significant bottleneck in offline RL (Section 4), we propose two simple techniques to further distill the information in the value function into the policy on test-time states.

**(1) On-the-fly policy extraction (OPEX).** Our first idea is to simply adjust policy actions in the direction of the value gradient at evaluation time. Specifically, after sampling an action from the policy \(a( s)\) at test time, we further adjust the action based on the _frozen_ learned \(Q\) function during evaluation rollouts with the following formula:

\[a a+_{a}Q(s,a),\] (7)

Figure 7: **Should we use high-coverage or high-optimality datasets? The data-scaling matrices above show that _high-coverage_ datasets can be much more effective than high-optimality datasets. This is because high-coverage datasets can improve _test-time policy accuracy_, one of the main bottlenecks of offline RL.**

where \(\) is a hyperparameter that corresponds to the test-time "learning rate". Intuitively, Equation (7) adjusts the action in the direction that maximally increases the learned Q function. We call this technique **on-the-fly policy extraction (OPEX)**. Note that OPEX requires only _a single line of additional code_ at evaluation and does not change the training procedure at all.

**(2) Test-time training (TTT).** We also propose another variant that further updates the parameters of the policy by continuously extracting the policy from the fixed value function on test-time states, as more rollouts are performed. Specifically, we update the policy \(\) with the following objective:

\[_{}\;_{}()=_{s,a p ^{}()}[Q(s,^{}(s))- D_{}(^{} )],\] (8)

where \(^{}\) denotes the fixed, learned offline RL policy, \( p^{}()\) denotes the mixture of the dataset and evaluation state distributions, and \(\) denotes a hyperparameter that controls the strength of the regularizer. Intuitively, Equation (8) is a "parameter-updating" version of OPEX, where we further update the parameters of the policy \(\) to maximize the learned value function, while not deviating too far away from the learned offline RL policy. We call this scheme **test-time training (TTT)**. Note that TTT only trains \(\) based on test-time interaction data, while \(Q\) and \(^{}\) remain fixed.

Figure 8 compares the performances of vanilla IQL, SfBC (Equation (3), another test-time policy extraction method that does not involve gradients), and our two gradient-based test-time policy improvement strategies on eight tasks (\(8\) seeds each, error bars denote \(95\%\) confidence intervals). The results show that OPEX and TTT improve performance over vanilla IQL and SfBC in many tasks, often by significant margins, by mitigating the test-time policy generalization bottleneck.

## 6 Conclusion: What does our analysis tell us?

In this work, we empirically demonstrated that, contrary to the prior belief that improving the quality of the value function is the primary bottleneck of offline RL, current offline RL methods are often heavily limited by how faithfully the policy is _extracted_ from the value function and how well this policy _generalizes_ on test-time states. **For practitioners**, our analysis suggests a clear empirical recipe for effective offline RL: train a value function on as _diverse_ data as possible, and allow the policy to maximally utilize the value function, with the best policy extraction objective (_e.g._, DDPG+BC) and/or potential test-time policy improvement strategies. **For future algorithms research**, our analysis emphasizes two important open questions in offline RL: (1) What is the best way to _extract_ a policy from the learned value function? (2) How can we train a policy in a way that it _generalizes_ well on test-time states? The second question is particularly notable, because it suggests a diametrically opposed viewpoint to the prevailing theme of pessimism in offline RL, where only a few works have explicitly aimed to address this generalization aspect of offline RL [37; 38; 63]. We believe finding effective answers to these questions would lead to significant performance gains in offline RL, substantially enhancing its applicability and scalability, and would encourage the community to incorporate a holistic picture of offline RL alongside the current prominent research on value function learning.

Figure 8: **Test-time policy improvement strategies (OPEX and TTT). Our two on-the-fly policy improvement techniques (OPEX and TTT) lead to substantial performance improvements on diverse tasks, by mitigating the test-time policy generalization bottleneck.**