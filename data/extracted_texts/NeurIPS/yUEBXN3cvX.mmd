# On the Effects of Data Scale on

UI Control Agents

Wei Li

Google DeepMind

William Bishop

Google DeepMind

Alice Li

Google DeepMind

Chris Rawles

Google DeepMind

Folawiyo Campbell-Ajala

Google

Divya Tyamagundlu

Google

Oriana Riva

Google DeepMind

###### Abstract

Autonomous agents that control user interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world UI control agents. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse UI control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.

## 1 Introduction

Recent work has studied how large language models (LLMs) can be leveraged to build UI control agents  that accomplish human tasks by interacting with a digital device environment. These agents perceive the state of the device by observing its screen (from screenshots or application UI trees), and generate actions (click, type, scroll, etc.) that are executed through the device's user interface. Tasks, specified in natural language, can range from configuring device settings and sending emails, to navigating shopping websites and planning a trip.

While progress is rapidly advancing, absolute performance of UI control agents that leverage pre-trained LLMs without fine-tuning on task demonstrations is still relatively low. When tested in real-world environments, where agents control everyday applications and websites, recently-reported task success rates range from 12% on desktop applications  to 46% on mobile applications . In contrast, agents that leverage models fine-tuned for task execution , achieve even 80%  success rate, when tested on websites and tasks similar to what they are trained on.

While the pattern of collecting new datasets and fine-tuning shows promise, there are at least two important unanswered questions. First, to the best of our knowledge no prior work has examined the question of scaling: how much data must be collected in order to obtain a given performance level with fine-tuned models. This question is particularly important because human demonstrationsof UI interactions for fine-tuning are time consuming and expensive to collect. Understanding how performance scales, both in domain and out of the domain of the collected demonstrations (unseen tasks and unseen applications), is important for determining whether fine-tuning alone is a viable path towards deploying UI control agents in the real world. Therefore, one of the main goals of this work is to rigorously quantify how performance of fine-tuned agents scales, both in and out of domain, as the amount of data used for fine-tuning is increased.

Second, it is not clear the level of task complexity fine-tuning might be fruitfully applied to. Conceptually, UI control agents must both decompose a high-level goal into a set of small atomic actions and execute ("ground") those actions in a device screen. While the high-level reasoning with LLMs, required for determining how to accomplish high-level goals, is still an open problem in artificial intelligence , the set of low-level actions (clicking, typing, etc...) required to execute tasks are more constrained, and general agents capable of robust grounding across domains might be approachable via fine-tuning. Therefore, a second goal of this work is to quantify the scaling of fine-tuning for agents performing both high-level and low-level tasks.

Rigorously quantifying scaling in these ways requires a carefully constructed dataset. To this end, we introduce AndroidControl, a large-scale dataset of 15,283 demonstrations of tasks performed by humans in Android apps. Figure 1 shows an example data sample. Compared to existing datasets , for every task AndroidControl provides both the high- and low-level human-generated instructions describing it. This is essential to investigate the level of task complexity a model can handle and also provides richer supervision during training. AndroidControl is also the most diverse UI control dataset that exists today, including 14,548 unique tasks over 833 different Android apps, thus allowing us to generate multiple test splits for measuring performance in and out of domain. As a resource to the community, we make AndroidControl publicly available.1

Overall, we make the following contributions: _(i)_ we collect and release AndroidControl, a new UI control dataset whose size, structure and diversity advances previous datasets, _(ii)_ we use AndroidControl to quantify how fine-tuning with demonstrations scales when applied to both low- and high-level tasks and to tasks in and out of the domain of the training data, and _(iii)_ we compare fine-tuning to various zero-shot and few-shot baselines, finding that fine-tuning scales favorably in domain, but out of domain, it requires one or two orders of magnitude more data to obtain robust performance on high-level tasks, suggesting that additional approaches may be beneficial for obtaining agents which robustly perform out-of-domain high-level tasks.

Figure 1: An example task demonstration contained in AndroidControl. Green circles/rectangles highlight the action on the screen. Red numbers are added only for illustration purposes.

## 2 Related work

UI control datasetsTable 1 compares AndroidControl to existing UI control datasets. The structure of these datasets is similar. They consist of a natural language task description and a human-recorded demonstration, in the form of a sequence of UI actions (click, type, swipe, etc.) and associated UI states. What differentiates these datasets is mainly whether they are single-step (as in grounding referring expressions datasets like UIBert ), whether the task description is expressed as a high-level goal or as a sequence of low-level step instructions, and how the UI state is represented (screenshot vs. UI tree). Three features make AndroidControl unique. First, for every task, it contains both low-level and high-level instructions generated by human annotators. While a few other datasets contain both these types of annotation, their low-level instructions are either synthetically generated (as in MoTIF ) or are limited to one action type (only click actions [22; 33]). In addition to bringing richer language supervision during training, the availability of human-generated low-level instructions allows us to test UI control agents on different levels of task complexity. Second, if we consider the number of unique task instructions and the number of human demonstrations, AndroidControl is the second-largest UI control dataset to date, second only to AitW .2 The diversity of task scenarios present in AndroidControl is its third differentiating feature: AndroidControl includes tasks from 833 different Android apps, 6 times more than popular datasets like Mind2Web  and almost 5 times more than AitW. This diversity makes AndroidControl optimal for realistic, out-of-domain analysis. Note that Mind2Web also provides out-of-domain splits but given its smaller size (2,350 tasks over 137 websites, with a train split of 1k demonstrations) is not suitable for a scaling analysis.

In addition to the datasets listed in Table 1, recent work proposes interactive testing environments for UI control agents [31; 46; 18; 38; 28; 4] where the environment provides the agents with reward signals. These environments are designed for online testing and are limited to no more than 20 applications or websites. The only exception is MiniWob  for which task demonstrations have been collected, but the environment consists of much simplified, synthetic websites.

UI control agentsEarly UI control agents were trained from scratch using behavioural cloning [15; 21; 22] or reinforcement learning [23; 11]. Current UI agents use pre-trained LLMs and multimodal models. One line of work prompts LLMs in a zero-shot or few-shot regime [39; 27; 12; 18; 17; 44]. Another line of work relies on fine-tuning which is applied end to end  or to build specific model capabilities, such as identifying the interactable UI elements in a webpage [43; 10; 7]. To name a few, SeeAct , which we use in our evaluation, is a web agent that leverages large multimodal models to understand text and visual elements on webpages. The best-performing SeeAct agent relies on a fine-tuned cross-encoder model to select candidates web elements for interaction. WebGPT 

   Dataset & Platform &  \# Human \\ demos \\  &  \# Unique \\ instr. \\  &  \# Xips or \\ websites \\  &  \# Task \\ steps \\  &  UI tree? \\  & Screen? &  High-level \\ instr. \\  & 
 Low-level \\ instr. \\  \\  MiniWob++  & Web (synthetic) & 17,971 & 100 & 114 & 2.3 & ✓ & ✗ & ✗ & ✓ \\ WebShop  & Web & 1,566 & 1,566 & 1 & 11.3 & ✗ & ✓ & ✓ & ✗ \\ UIBert  & Android & 16,660 & - & - & 1.0 & ✓ & ✓ & ✗ & ✓ \\ PixelHeip  & Android & 187 & 187 & 4 & 4.2 & ✓ & ✗ & ✓ & ✓ \\ UGIF  & Android & 523 & 523 & 12 & 5.3 & ✓ & ✓ & ✓ & ✓ \\ MoTIF  & Android & 4,707 & 276 & 125 & 4.5 & ✓ & ✓ & ✓ & ✓ \\ Mind2Web  & Web & 2,350 & 2,350 & 137 & 7.3 & ✓ & ✓ & ✓ & ✗ \\ AiW  & Android & 715,142 & 30,378 & 357 & 6.5 & ✗ & ✓ & ✓ & ✗ \\ WebVoyager  & Web & 643 & 643 & 15 & - & ✓ & ✓ & ✓ & ✗ \\ WebLINX  & Web & 2,337 & 2,377 & 155 & 43.0 & ✓ & ✓ & ✓ & ✗ \\   

Table 1: Comparison of AndroidControl to existing UI control datasets. We consider device platform, size (as number of task demonstrations), diversity (as number of unique task instructions and apps/websites), average number of steps in a task, how the UI state is captured (UI tree vs. screenshot), and whether tasks are described through high-level instructions or sequences of low-level commands.

fine-tunes GPT-3 to learn to use a web browser. WebAgent  pre-trains a T5 model to extract HTML snippets and leverages Flan-U-PaLM to generate Python code to control a web environment. Synapse  introduces a trajectory-as-exemplar prompting method where memory of previous interactions allows the agent to perform complex, multi-step tasks.

Domain generalizationAs evidenced by various LLM studies [5; 16; 13], scaling model and data size for the training leads to steady improvements in domain generalization. On the other hand, when transferring a pre-trained model to a downstream task through fine-tuning, while in-distribution performance improves, a reduction in the robustness to distribution shifts is observed [20; 37; 1]. In this work, we empirically study how scaling data size in fine-tuning affects in-domain and out-of-domain performance of UI control agents. While prior work has tested UI agents using out-of-domain test splits [7; 6], to the best of our knowledge a data scale analysis has not been conducted. To minimize training cost and to maintain the out-of-domain generalization, we evaluate also the option of not fine-tuning, by evaluating multiple zero-shot and few-shot baselines.

## 3 The AndroidControl dataset

The collection of the AndroidControl dataset is motivated by our dual goal of studying (i) how scaling data size for fine-tuning UI control models affects in-domain and out-of-domain performance, and (ii) the level of task complexity these fine-tuned models can be effective for.

### Data collection

We collect AndroidControl using crowdsourcing over the course of a year. The data collection starts by giving crowdworkers generic feature descriptions for apps from 40 different categories (Figure 2). These descriptions are generated using LLMs (e.g., "in a note taking app you can create a new note with details"). Then, we ask crowdworkers to instantiate each feature description into one or multiple tasks involving apps of their choice.

By allowing annotators to use any app of their choice we succeed in collecting a largely-variated dataset encompassing 833 Android apps, including Google apps (Settings, Gmail, Google Maps, etc.), high-trend apps (e.g., Amazon, Booking.com, Kayak, Spotify, etc.) as well as less-popular or regional apps. This is important because high-popularity apps tend to include well-annotated accessibility trees and have more user-friendly interfaces, thus possibly facilitating the agent's task. We confirm this assumption by analyzing the performance of some of our tested agents on Google apps and non-Google apps (see results in Section E.5 in the Appendix).

During collection of a demonstration, annotators first provide a high-level description of a task in natural language (e.g., "Add an alarm to wake me up on Saturday mornings at 6am"). We ask annotators to make the descriptions detailed enough to be interpretable without any ambiguity. We also instruct them to always include the name of the target app in the task description, unless obvious (e.g., Google first-party apps such as Clock or Settings). By doing so, the collected data can enable us to test memory-less, single-turn agent interactions.

In order to collect interaction traces, each annotator is provided with a setup that includes a physical Android phone (Google Pixel with Android 8.0 or higher) installed with a companion Android app that in turn connects to a web app running on a desktop Chrome browser. Annotators control the phone through the web app, using the WebUSB protocol and Android Debug Bridge (ADB). The web app provides annotators with controls to perform actions on the phone and observe their outcome. An annotator can select from the following set of UI actions to perform on the phone: click, long_press, input_text, scroll, navigate_home, navigate_back, open_app and wait (see Table 2). For each action, applicable metadata such as touch coordinates, target elements, entered text, and timing information

Figure 2: Distribution of the app categories that compose AndroidControl.

are automatically appended to the interaction trace (see Appendix B for more details). Annotators are instructed to avoid performing actions that are unnecessary or unrelated to the task. After an action is executed, a real-time screenshot of the phone's display is shown to the annotator and added to the interaction trace. This enables the annotator to completely operate their phone through the web app. Before executing each action, the annotator is asked to type in a short natural language description of the action they are about to take ("add a new alarm", "set the hours to 6", etc.), as if they were instructing someone to execute that action. These are also incorporated into the interaction trace and make up the low-level instructions in the dataset. If annotators realize the task is not feasible because of an unsupported functionality in the app or because of an error they tag the trace as infeasible or failed, respectively. Otherwise, it is tagged as successful.

As both high-level and low-level instructions are generated by annotators, they may contain typo and grammatically errors (for example, the high-level instruction in Figure 1). We left such errors in the dataset because humans often produce errors of this type.

Overall, this data collection involved 20 annotators. Each annotator went through a training process of several weeks. To maximize the diversity of the task demonstrations, in the last 4 months of the data collection we asked annotators to impersonate 20 different personas. Persona profiles are generated using an LLM prompted with detailed ranging from name, address, occupation, hobbies to upcoming week-end plans, family relationships, and typical day schedule.

### Dataset statistics

Data statistics about AndroidControl are summarized in Table 1. In addition, Figures 3 and 4 report distributions of UI actions, task lengths, and lengths of high and low level instructions. The task length distribution (Figure 3(a)), measured as number of steps required to complete the task, shows that tasks are of moderate length (between 1 and 13 steps for the 5th to 95th percentile, respectively). Lengths of high-level (HL) instructions fall between 8 and 34 words for the 5th and 95th percentile, and low-level (LL) instructions are between 3 and 14 words for the 5th and 95th percentile.

### Dataset splits

We create a train, a validation and 4 test splits whose number of task demonstrations (episodes) and characteristics are detailed in Table 3. In order to measure how performance scales in domain and out of the domain of the collected data, we create the following test sub-splits: _1) in domain data (IDD)_: randomly pulled episodes from the same distribution as the training data; _2) app-unseen_: a test split using apps not present in the train split; _3) task-unseen_: a test split with tasks not present in the train

   click-elem & Click the center of the specified element. \\ long\_presselem & Long press the center of the specified element. \\ input\_textless{}detro & Type the specified text. \\ second cliffraction & Scroll in the specified direction. \\  navigate\_home & Go to the home screen. \\ navigate\_back & Go back to the previous app screen. \\ open\_appapp & Launch the specified app. \\ wait & Wait until the next observation is received. \\   

Table 2: Actions captured in AndroidControl. Figure 3: Action distribution split; and _4) category-unseen_: a test split with apps from categories not present in the train split. Note that the test splits may contain overlapping episodes. For example, episodes in the unseen-category split will also be in the unseen-app and unseen-tasks splits. It is also the case that the average number of elements per screen can vary significantly between training and testing. Appendix B.4 explains why that is the case.

## 4 Experiments and results

In order to test the impact of data scale and task complexity on transfer performance in domain and out of domain, we conduct experiments in which we train on different amounts of the data in the AndroidControl's training set. We also test zero-shot and few-shot methods.

### Agent implementation

We implement a UI control agent for Android. The agent receives task instructions expressed in natural language. It observes the environment (the device) by deriving textual representations of the screen directly from the Android accessibility tree. The screen representation lists the on-screen UI elements. Each element is described according to the following attributes: type, text, content description, bounding boxes and various state tags (e.g., clickable, scrollable, focused, checked). As mobile UI screens may contain hundreds of UI elements (200 on average in AndroidControl, Table 3), we pre-process the screen representations to include only UI elements that have a non-empty text description or UI elements of critical types (switch and edit). This process facilitates the agent's task and reduces the input's size. Note that our agent implementation does not directly leverage the page screenshot. While recent work explores how to infer screen representations from raw screens , best performance is still reported when using accessibility trees or HTML [38; 43; 28]. We expect the general trends we observe will hold true for multimodal language models.

During execution, the agent maintains a history over the previous steps. To avoid excessively large inputs, in the agent's input we include the screen description of only the current screen but append a history of the previously executed actions. In contrast to the action prediction output that locates UI elements by absolute coordinates, an action in the history is described in a self-contained manner, using its textual description and without any external reference.

The agent predicts an action among a set of candidate actions. The set of available actions matches the actions defined by AndroidControl (Table 2) with two main changes. We add a terminate action that the agent predicts when it deems the task complete or infeasible. As this action is not originally provided in the dataset, for training purposes, we artificially insert it at the end of every episode (see Appendix D.1). For efficiency reasons, as in prior work , the input_text action is modified to include also the preceding click action necessary to set focus on the target element. The agent predicts the action type and any required arguments for the action, specifically the target UI element for a click action, the text to be typed and the target element for a typing action, the name of an app to open, the direction of a scroll, etc. For an example of screen representation, a summary of the agent's action space, and more details on the agent implementation please refer to Appendix C.

### Experimental setup

The LLMs we experiment with include PaLM-2L , PaLM-2S , Gemini 1.5 Pro , GPT-4 and GPT-4 Turbo . We set the temperature to zero for all models to obtain more deterministic

   Split & Sub-splits & \# Episodes & \# Step actions & \# Apps & \# Categories & Avg. \# elements per screen \\  Train & - & 13,604 & 74,722 & 769 & 39 & 222.2 \\ Val & - & 137 & 690 & 99 & 29 & 214.4 \\   & IDD & 721 & 3,897 & 296 & 35 & 221.5 \\  & App-unseen & 631 & 3,475 & 64 & 12 & 185.4 \\   & Task-unseen & 803 & 4,464 & 90 & 12 & 181.6 \\   & Category-unseen & 700 & 3,891 & 68 & 4 & 184.7 \\   

Table 3: Details on AndroidControl train, validation and test splits. For every split, we report number of episodes, step actions, unique apps, app categories, and UI elements per screen.

responses. To limit compute, we perform fine-tuning only with PaLM-2S, and adopt the parameter efficient tuning approach of LoRA . We set the LoRA rank to 4 when fine-tuning with small amounts of data (\(<1\)k episodes), while switch to a rank of 64 when using more episodes. For few-shot experiments we use Gemini 1.5 Pro which provides a context window of 1M tokens.

We create SeqIO  tasks to extract data points from AndroidControl and to generate prompts and target outputs (more details in Appendix D.1). We setup two SeqIO tasks: (i) SeqIO HL (high-level) where only a high-level instruction is included in the prompt, and (ii) SeqIO LL (low-level) where both a low-level instruction and its corresponding high-level instruction are included. This second task emulates the use case where an LLM is used for decomposing high-level instructions into a sequence of low-level commands and another LLM is used for grounding; the assumption is that this grounding LLM may improve performance by having access to the context of the high-level command. In addition to the natural language instruction(s), each data point contains the textual description of the start screen, the history of performed actions, and the ground-truth action. Through these two SeqIO tasks, we investigate how a model performs on simpler (LL) or harder (HL) task instructions.

To reduce LLM costs, some zero-shot and all few-shot evaluations are done on a subset of the test split of AndroidControl, _Random-500_, that contains 500 random step actions from the full test split and has a similar sub-split distribution. We verified through experiments that results on Random-500 are a good approximation of the results on the full test split (Appendix E.3).

Zero-shotWe test four zero-shot methods. (i) We use the AitW  prompt, specifically designed for Android and the PaLM model, without any modifications. (ii) We adapt the best-performing SeeAct  variant ("choice") which grounds actions via textual choices. SeeAct was originally designed for GPT-4V for web navigation tasks. At each step, SeeAct queries the LLM twice. In the first query, it analyzes the state and performs reasoning. Then, in the second query, it asks the LLM to select an action from multiple choices. We use the SeeAct prompt by Rawles et al.  adapted to work on mobile and to take textual representations of Android screens as input. (iii) We evaluate the text-only version of M3A  that combines ReAct-style  and Reflexion-style  prompting. (iv) Finally, we test a zero-shot prompt (implementation in Appendix D.2.2) of the same form we use with our agent described above. This allows us to measure performance of a base model for our agent without any fine-tuning. This prompt emphasizes the use of a screen description composed of UI elements, hence the name ER (Element Representations). Note that with the exception of ER, which we ran with all 4 base models, to limit prompt sensitivity , we ran the other prompts with the model family they were originally designed for.

Few-shot and LoRA-tuned modelsWhen evaluating few-shot on HL instructions, samples drawn from the HL SeqIO task are used in the prompt. When testing on LL instructions, samples from the LL SeqIO task are included. For convenience, LoRA-tuned models are trained on a mixture of both the LL and HL SeqIO tasks as we found training on the two SeqIO tasks separately or in a mixture to achieve similar accuracy (see Appendix E.2). Best model checkpoints are selected using the validation split. We use the simple ER prompt for few-shot and LoRA-tuned models.

Scaling analysisTo conduct the scaling analysis, we vary the number of samples included in the prompt of few-shot (FS) techniques or in the training set of LoRA-tuned (LT) models. We randomly sample episodes from the SeqIO tasks using the following sample sizes: 5, 10, 100, 1k, 10k, and all (13,604) episodes. For few-shot only, to make the prompts more varied, we sample an equivalent number of step-examples from different episodes.

MetricsAs in prior work , as our evaluation metric we adopt step-wise accuracy, which measures the success of each task step. A step is successful if the predicted action and arguments (target element and text, if present) are correct. We adopt a relaxed metric that considers equivalent actions in additions to exact matches as successful (see Appendix D.3 for details).

### In-domain performance

We start by evaluating zero-shot, few-shot and LoRA-tuned methods in domain. Table 4 reports the step-wise accuracy performance on the IDD sub-split of Random-500. In-domain, LoRA-tuned models, despite using the smaller PaLM 2S model, when trained with sufficient amounts of data, largely outperform the zero-shot and few-shot methods. For low-level instructions, even LT-5surpasses all non-fine-tuned models, while for high-level instructions, it requires more training data (\(1\)k episodes). The best fine-tuned model reaches 71.5% on high-level instructions and 86.6% on low-level instructions.

The best zero-shot performance on low-level instructions is obtained with AitW using PaLM 2L (56.7%) and on high-level instructions with M3A using GPT-4 (42.1%). This performance likely reflects the design of the prompts, the strength of the different base models used and the benefits of incorporating some high-level reasoning (included in M3A) for handling high-level instructions. Interestingly, the few-shot performance is for the most part inferior to that of zero-shot methods.

### Effect of scale on in-domain transfer

Fine-tuning obtains good performance in domain, but how much data is needed for acceptable performance? A failure at a single step may prevent task completion, a phenomenon we refer to as the "weakest link effect." Making the simplifying assumption of i.i.d. success across steps, completing, for example, a 5-step task correctly 95% of the time requires a 99% step-wise accuracy. We perform an analysis to extrapolate the amount of training data required to achieve such performance.

Figure 4(a) visualizes the number of training episodes drawn from AndroidControl and the step accuracy achieved on the full IDD test sub-split. Both high and low-level curves exhibit linear trends (\(R^{2}\) coefficients are greater than 0.95) with the log of training data. We extrapolate that it would take 500K and 1M episodes to reach \(95\%\) step-accuracy for low and high-level instructions, respectively. However, while low-level tasks can be accomplished in one step, high-level tasks require multiple steps. Taking 5 steps as the rough length for high-level tasks, we predict 2M episodes would be required to reach the 99% step-wise accuracy to achieve 95% episode completion for high-level tasks (see Appendix E.4 for an empirical evaluation of performance as episode length is varied). While this is conservative by assuming no possible mistake recovery, we still feel this analysis provides a helpful rough quantification.

### Effect of scale on out-of-domain transfer

We now use AndroidControl's out-of-domain test splits (Table 3) to quantify how fine tuning with more demonstrations affects out-of-domain performance. This is important for assessing the robustness of agents used in the real world on tasks not foreseen in the data used to train an agent.

   Regime & Method & Model &  \\  & & & high-level instr. & low-level instr. \\   & AitW & PaLM 2L & 19.5 & **56.7** \\  & SeeAct & GPT-4-Turbo & 33.9 & 54.3 \\  & M3A & GPT-4-Turbo & **42.1** & 55.0 \\  & ER & PaLM 2S & 19.5 & 45.5 \\  & ER & PaLM 2L & 33.0 & 45.9 \\  & ER & GPT-4 & 32.1 & 51.7 \\  & ER & Gemini 1.5 Pro & 24.4 & 50.2 \\   & FS-5 & Gemini 1.5 Pro & **41.8** & 50.2 \\  & FS-10 & Gemini 1.5 Pro & 40.2 & 50.8 \\  & FS-100 & Gemini 1.5 Pro & 39.5 & **53.3** \\   & LT-5 & PaLM 2S & 30.3 & 57.1 \\  & LT-10 & PaLM 2S & 28.5 & 58.9 \\  & LT-100 & PaLM 2S & 39.8 & 62.8 \\   & LT-1k & PaLM 2S & 52.5 & 71.4 \\   & LT-10k & PaLM 2S & 62.0 & 85.7 \\   & LT-all & PaLM 2S & 65.6 & 81.8 \\   & LT-1k-r64 & PaLM 2S & 54.8 & 76.6 \\   & LT-10k-r64 & PaLM 2S & 69.6 & 81.9 \\   & LT-all-r64 & PaLM 2S & **71.5** & **86.6** \\   

Table 4: Performance on the IDD sub-split of Random-500. For few-shot (FS) and LoRA-tuned (LT) methods -\(X\) (\(\)\(all\)) indicates \(X\) (\(all\)) episodes are used in the prompt or in training. Unless noted with a “r64” for models fine-tuned with a LoRA rank of 64, LoRA-tuned models use rank=4.

As the number of fine-tuning samples increases, performance improves and so does the gap between IDD and OOD performance (Table 5). With 10k or more episodes, the IDD accuracy is noticeably higher than on the three ODD splits. For example, with LT-10k (r=4), the gap is 7.7-8.7 pp for high-level instructions and 2.0-3.2 pp for low-level instructions. With r=64 the gap increases. In general, more out-of-domain transfer occurs for low-level tasks, which is expected as low-level tasks share more similarity across tasks and apps than high-level tasks.

As for in-domain, we extrapolate how much training data would be necessary to achieve a reasonable accuracy out of domain (Figure 4(b)). OOD step-accuracy grows more slowly than in domain, and is estimated to reach 95% at 10M and 60M episodes for low-level and high-level instructions, respectively. Similar to above, the number of episodes we predict would be required to reach 99% step accuracy to therefore achieve 95% episode completion rate on 5-step high-level tasks is 150M. Based on these projections, it seems expensive but feasible to obtain good general LL performance with fine-tuning, while the predicted higher order of magnitude of the number of required demonstrations suggests fine-tuning alone may not be sufficient to achieve robust OOD performance on HL tasks.

Figure 5: Relationship between step-wise accuracy and number of fine-tuning samples. With 5–100 episodes we use LoRA rank=4; with \(\)1k episodes we use LoRA rank=64. IDD performance is based on the full IDD test split. OOD performance is based on the average across the three OOD splits.

    & & IDD & app-unseen & task-unseen & categ-unseen \\   & HL & 26.9 & 25.7 [-1.2] & 26.4 [-0.5] & 25.1 [-1.8] \\  & LL & 55.7 & 56.9 [-1.2] & 56.6 [+0.9] & 56.4 [+0.7] \\  & HL & 30.6 & 29.9 [-0.7] & 31.1 [+0.5] & 30.2 [-0.4] \\  & LL & 56.4 & 58.3 [+1.9] & 58.2 [+1.8] & 58.2 [+1.8] \\  & HL & 43.3 & 42.4 [-0.9] & 42.5 [-0.8] & 42.1 [-1.2] \\  & LL & 58.6 & 62.7 [+4.1] & 61.7 [+3.1] & 61.8 [+3.2] \\   & HL & 53.2 & 49.0 [-4.2] & 49.3 [-3.9] & 48.1 [-5.1] \\  & LL & 68.0 & 68.0 [0.0] & 67.3 [-0.7] & 67.4 [-0.6] \\   & HL & 63.9 & 55.2 [-8.7] & 55.6 [-8.3] & 54.2 [-7.7] \\  & LL & 78.7 & 76.7 [-2.0] & 75.6 [-3.1] & 75.5 [-3.2] \\   & HL & 65.5 & 58.7 [-6.8] & 59.7 [-5.8] & 58.2 [-7.3] \\  & LL & 80.7 & 78.6 [-2.1] & 77.9 [-2.8] & 77.8 [-2.9] \\   & HL & 57.6 & 51.1 [-6.5] & 51.7 [-5.9] & 50.2 [-7.4] \\  & LL & 72.3 & 71.0 [-1.3] & 70.4 [-1.9] & 70.1 [-2.2] \\   & HL & 69.6 & 57.7 [-11.9] & 56.9 [-12.7] & 58.9 [-10.7] \\  & LL & 81.9 & 76.3 [-5.6] & 75.8 [-6.1] & 75.2 [-6.7] \\   & HL & 70.8 & 58.5 [-12.3] & 59.6 [-11.2] & 57.4 [-13.4] \\  & LL & 83.2 & 78.5 [-4.7] & 77.3 [-5.9] & 76.8 [-6.4] \\   

Table 5: OOD step accuracy. In square brackets [X] we report the percentage point increase (+) or drop (-) from the IDD accuracy obtained on the full IDD test split.

More experiments.To complete our evaluation analysis in Appendix E we run more experiments studying the impact of other factors, including episode length, action types, and app types.

## 5 Limitations

There are multiple potential limitations in this work. First, we only fine-tuned one model, PaLM-2S; however, while the absolute performance values would change, we expect our relative findings to be consistent across model families. Additionally, using offline evaluation for agent performance has the known issue of not rewarding alternative routes to complete a task and the ability to take corrective actions . Further, while selected to encompass important use cases, the set of app categories in AndroidControl is still an incomplete representation of all tasks users may ask agents to perform. Finally, studying inference costs is not a focus of the paper, although it is obvious that it is much cheaper to predict on a fine-tuned PaLM-2S model than on any larger models, such as PaLM-2L or GPT-4.

## 6 Conclusion

We have introduced AndroidControl, a large and diverse dataset structured for studying the performance of models in and out of domain on low and high-level tasks, as training data is scaled. Using this dataset, we evaluate scaling of LoRA fine-tuned models. We predict that to achieve 95% accuracy for in-domain low-level tasks, 1M episodes would be required, while 2M episodes would be required to obtain 95% episode completion rates for 5-step high-level tasks. While these results are for only one model, they suggest that fine-tuning may be a viable, though possibly expensive, route for obtaining high in-domain performance for low and high level tasks. Out of domain, 10M and 150M episodes would be required, respectively. This one to two orders of magnitude increase suggests fine-tuning may not scale well out of domain, and may not be sufficient to obtain good out-of-domain performance on HL tasks.