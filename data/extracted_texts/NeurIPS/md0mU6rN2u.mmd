# Sam-Clip : Merging Vision Foundation Models

towards Semantic and Spatial Understanding

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The landscape of publicly available vision foundation models (VFMs), such as CLIP and SAM, is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pretraining objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe based on multi-task distillation to efficiently _merge_ VFMs into a unified model that assimilates their expertise. By applying our method to SAM and CLIP, we derive SAM-CLIP : a unified model that amalgamates the strengths of SAM and CLIP into a _single backbone_, making it apt for edge device applications. We show that SAM-CLIP learns _richer visual representations_, equipped with both localization and semantic features, suitable for a broad range of vision tasks. We further show that SAM-CLIP not only retains the foundational strengths of its precursor models but also introduces _synergistic functionalities_, most notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.

## 1 Introduction

Vision Foundation Models (VFM) such as CLIP , SAM , MAE , and DINOv2  provide strong backbones that can be utilized for a wide range of vision tasks after finetuning. Additionally, some of these models exhibit notable zero-shot capabilities, such as classification from text prompts  and segmentation from geometric prompts (points and bounding boxes) . Depending on their pretraining objectives, VFMs can act as feature extractors suitable for diverse downstream tasks. For instance, models that employ contrastive losses during training , utilize low-frequency signals, and generate features that can linearly separate samples based on their semantics . Conversely, the pretraining objectives for MAE and SAM involve denoising masked images and instance mask segmentation, respectively, leading to the acquisition of features utilizing high-frequency signals with localization knowledge but limited semantic understanding (Figure3).

Deploying separate models for different downstream tasks is inefficient (high memory footprint and runtime, especially on edge devices) and lacks opportunity for cross-model learning . _Multitask learning_ is a paradigm capable of addressing this issue. However, it often requires costly training and simultaneous access to all tasks . Training foundation models often relies on an unsupervised or semi-supervised approach, requiring substantial computational resources. For example, state-of-the-art CLIP models are trained on extensive datasets, such as LAION  and DataComp , consuming massive amount of computational power. Similarly, SAM's pretraining on 1.1 billion masks is computationally demanding. A multi-objective pretraining method requires comparable or more data and compute as single objective VFM training. This is in addition to other multi-tasklearning challenges such as interfering gradients, training instabilities , and access to pretraining datasets that are often proprietary , which limit the scalability and feasibility of this approach.

To overcome these challenges, model merging has emerged as a rapidly growing area of research . The majority of merging techniques focus on combining multiple task-specific models into a single model without requiring additional training. For instance, this can be achieved through techniques such as model weights interpolation , parameter importance analysis , or leveraging invariances in the models . These techniques, however, put too much stress on not using data or not performing additional training/finetuning resulting in decreased performance or lack of generalization to diverse set of tasks . Our goal is to merge VFMs that are trained with fundamentally different objectives, have distinct capabilities, and possibly interact with other modalities. In this setup, naive merging approaches results in significant forgetting  (Appendix B).

We aim to fill the gap between training-free model merging and multitask training by drawing techniques from continual learning  and knowledge distillation . We treat model merging as a continual learning problem, where, given a pretrained base VFM, the knowledge of a second auxilary VFM is merged without forgetting of the initial knowledge. On one side, in contrast to weight averaging techniques, we allow access to _small part of_ pretraining data or its surrogates during the merging process. We leverage multi-task distillation on the replay data to avoid forgetting the original knowledge of base VFM during the merging process. On the other side, our merging process is significantly more efficient than traditional multitask training by requiring less than 10% of the data and compute compared to their original pretraining (Section).

We instantiate our proposed merging approach by combining SAM and CLIP into a _single multi-task model_, called SAM-CLIP, suitable for edge device deployment. This merged model inherits prompt-based zero-shot capabilities from both CLIP and SAM with minimal forgetting: specifically, zero-shot classification and image-text retrieval from CLIP, and zero-shot instance segmentation from SAM (see Figure). Further, we illustrate that SAM-CLIP learns richer visual representations compared to SAM and CLIP, endowed with both spatial and semantic features, resulting in improved head-probing performance on new tasks (see Figure). Finally, SAM-CLIP shows an emerging capability of zero-shot transfer to a new task: _zero-shot semantic segmentation_ thanks to combined skills inherited from SAM and CLIP. This task involves generating a segmentation mask based on a free-form text prompt. It requires both semantic understanding from text and segmentation capabilities, skills SAM-CLIP learns from CLIP and SAM, respectively. We demonstrate that SAM-CLIP achieves state-of-the-art performance on zero-shot semantic segmentation (Figure).

## 2 Proposed Approach

We constrain our discussion to the specific case where SAM serves as the base VFM, while a CLIP model serves as the auxiliary VFM. This pair presents an intriguing combination, as both models have

Figure 1: SAM-CLIP inherits zero-shot capabilities of SAM (instance segmentation) and CLIP (classification) using a single shared backbone (**left**). Further, SAM-CLIP is capable of a new task, zero-shot semantic segmentation, and obtains state-of-the-art results on several benchmarks (**right**).

been successfully deployed in diverse tasks and exhibit complementary capabilities. SAM excels in localization and high-resolution image segmentation but has limitations in semantic understanding. Conversely, CLIP offers a powerful image backbone for semantic understanding. We demonstrate it by several probing experiments (see Figure 3). We assume access to limited subsets of datasets (or their proxies) used to train the base and auxiliary VFMs, which function as memory replay in our CL setup. These are denoted as \(_{}\) and \(_{}\).

We employ a multi-head architecture, illustrated in Figure 2 Our base VFM, SAM, has an image encoder (\(_{}\)), a prompt encoder (\(_{}\)), and a light mask decoder (\(_{}\)). The auxiliary VFM, CLIP, has an image encoder (\(_{}\)) and a text encoder (\(_{}\)). Our goal is to merge both image encoders to a single backbone called \(_{}\) which is initialized by \(_{}\). Further, we consider lightweight heads corresponding to each VFM, namely, \(_{}\) and \(_{}\). \(_{}\) is initialized with \(_{}\) and \(_{}\) is initialized with random weights (since CLIP does not come with a head that we can deploy). We deploy other modality encoders (i.e., \(_{}\) and \(_{}\)) with no change (frozen).

As a baseline merging approach, we perform KD on \(_{}\) utilizing a cosine distillation loss :

\[_{}\ =_{_{ }}\ [1-^{}(_{}(_{}( )))^{T}_{}( )],\] (1)

where \(^{}\) is a pooling operator converting patch-level features from \(_{}\) to a normalized image-level embedding. In this setup, parameters of both \(_{}\) and \(_{}\) are learnable, while the CLIP encoder, \(_{}\), is frozen and used as a teacher. While this infuses SAM with CLIP's semantic abilities, it incurs at the cost of catastrophic forgetting of SAM's original capabilities even after deploying mitigative methods such as Wise-FT  (see supplementary materials).

To address these challenges, we propose a rehearsal-based multi-task distillation. This serves two primary goals: 1) facilitate the efficient transfer of knowledge from the auxiliary VFM to the base model, and 2) preserve the original capabilities of the base model. Inspired by , we consider a two-stage training: head-probing and multi-task distillation.

**I. Head probing:** In this stage, we first freeze the image backbone, \(_{}\), and only train \(_{}\) with the loss in Equation 1. Intuitively, with this approach we first learn some reasonable values for parameters of \(_{}\) (which is initialized randomly) before allowing any change in \(_{}\) that is prone to forgetting.

**II. Multi-task distillation:** In this stage, we allow all heads as well as our image encoder to be learnable. We perform a multi-task training on \(_{}\ +_{}\), with:

\[_{}\ =_{(,) _{}}\ _{}(_{}( _{}(), _{}()), ),\] (2)

where, \(\) is raw image, \(\) is a geometric prompt, \(=_{}(_{ }())\) is segmentation mask score produced by frozen SAM teacher, and \(_{}\) refers to a linear combination of Focal  and Dice  used in the original SAM training adapted for distillation. We train on \(_{}\ _{}\) with total loss of \(_{}\ +_{}\). During training, each batch has some samples from \(_{}\) and some form \(_{}\), which contribute to \(_{}\) and \(_{}\), respectively. To encourage less forgetting we use an order of magnitude smaller learning rate for parameters of \(_{}\) and \(_{}\) compared to \(_{}\) at this stage.

Figure 2: Multi-head architecture of SAM-CLIP for training (**left**) and inference (**right**).

## 3 Experiments

Experimentation details are presented in the supplementary materials.

**Zero-Shot Image Classification.** To examine the CLIP-related capabilities of SAM-CLIP, we perform zero-shot image classification on ImageNet , ImageNet-v2  and Places365 . Results shown in Figure  validate the efficacy of our approach in inheriting CLIP's capabilities.

**Zero-Shot Instance Segmentation.** For the SAM component of SAM-CLIP, we evaluate its performance in instance segmentation, a task at which the original SAM model excels , with COCO  and LVIS  datasets. Results (Figure  show that SAM-CLIP is close to the original SAM ViT-B on the two benchmarks, not suffering from catastrophic forgetting.

**Zero-Shot Transfer to Semantic Segmentation.** We extend our evaluation to (text-prompted) zero-shot semantic segmentation over 5 datasets, Pascal VOC , Pascal Context , ADE20k , COCO-Stuff  and COCO-Panoptic . SAM-CLIP establishes new state-of-the-art performance on all 5 datasets as shown in Figure  (right).

**Composing Both CLIP and SAM Heads for Better Segmentation.** Given that SAM-CLIP is a multi-task model with SAM and CLIP heads, one would naturally ask if the two heads can work together towards better performance on some tasks. Here, we showcase that a simple composition of SAM-CLIP's CLIP and SAM heads (low-resolution mask from CLIP head followed by high-resolution refinement by SAM head) can lead to even better zero-shot semantic segmentation. Example of this pipeline is shown at Figure  For fair comparison, when we compare with previous works in Figure  we report SAM-CLIP zero-shot segmentation performance with 448px resolution using \(_{}\) only. Using our high-resolution pipeline we obtain further gain: for example **mIoU on Pascal-VOC increases from 60.6% to 66.0%**.

**Head-Probing Evaluations on Learned Representations.** By merging the SAM and CLIP models, we anticipate that the resultant model will inherit advantages at the representation level from both parent models. Specifically, SAM excels at capturing low-level spatial visual details pertinent to segmentation tasks, while CLIP specializes in high-level semantic visual information encompassing the entire image. We hypothesize that the merged model combines these strengths, thereby enhancing its utility in broad range of downstream vision tasks. To investigate this hypothesis, we conduct head-probing (i.e., learn a task specific head with a frozen image backbone) evaluations on SAM, CLIP, and SAM-CLIP, utilizing different segmentation head structures (linear head, DeepLab-v3  and PSPNet ) across two semantic segmentation datasets, Pascal-VOC and ADE20k, and linear probing for image classification task on ImageNet and Places365 datasets. Results are presented in Figure  demonstrating SAM-CLIP superior visual feature representation capabilities.

Figure 4: Passing an input image through the image encoder (a), \(_{}\) can predict a semantic segmentation mask (c), and \(_{}\) can refine it to a more fine-grained mask with auto-generated geometric prompts (d) matching ground-truth (b).

Figure 3: Head-probing evaluation of each vision backbone for classification and semantic segmentation tasks demonstrating enriched visual features of SAM-CLIP.