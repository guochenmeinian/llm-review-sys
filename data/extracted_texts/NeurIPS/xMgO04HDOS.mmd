# Hierarchical Multi-Agent Skill Discovery

Mingyu Yang\({}^{1}\), Yaodong Yang\({}^{2}\)\({}^{}\), Zhenbo Lu\({}^{3}\)\({}^{}\), Wengang Zhou\({}^{1,3}\), Houqiang Li\({}^{1,3}\)

\({}^{1}\)University of Science and Technology of China, \({}^{2}\)Institute for AI, Peking University

\({}^{3}\)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

ymy@mail.ustc.edu.cn,yaodong.yang@pku.edu.cn

luzhenbo@iai.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn

Corresponding authors: Yaodong Yang and Zhenbo Lu

###### Abstract

Skill discovery has shown significant progress in unsupervised reinforcement learning. This approach enables the discovery of a wide range of skills without any extrinsic reward, which can be effectively combined to tackle complex tasks. However, such unsupervised skill learning has not been well applied to multi-agent reinforcement learning (MARL) due to two primary challenges. One is how to learn skills not only for the individual agents but also for the entire team, and the other is how to coordinate the skills of different agents to accomplish multi-agent tasks. To address these challenges, we present Hierarchical Multi-Agent Skill Discovery (HMASD), a two-level hierarchical algorithm for discovering both team and individual skills in MARL. The high-level policy employs a transformer structure to realize sequential skill assignment, while the low-level policy learns to discover valuable team and individual skills. We evaluate HMASD on sparse reward multi-agent benchmarks, and the results show that HMASD achieves significant performance improvements compared to strong MARL baselines.

## 1 Introduction

Multi-agent reinforcement learning (MARL) has recently demonstrated remarkable potential in solving various real-world problems, such as unmanned aerial vehicles , autonomous driving  and traffic light control . Despite its broad applications, current MARL algorithms  typically require well-crafted team or individual rewards to guide the agents to learn policies for efficient coordination. This limitation hinders the generalization of MARL to the sparse reward multi-agent tasks, where agents receive a non-zero reward only when they coordinate to achieve a challenging goal. Compared to single-agent tasks, the sparse reward problem in multi-agent tasks poses more challenges. On the one hand, the joint state and action spaces of multi-agent tasks increase exponentially with the number of agents, which exacerbates the difficulty for agents to explore those valuable but rare states. On the other hand, we need to distribute the received sparse reward not only to different timesteps but also to different agents .

To solve sparse reward multi-agent problems, a promising approach is to discover underlying skills within the multi-agent task and effectively combine these skills to achieve the final goal. For example, in a football game, there are various skills involved such as dribbling, passing, receiving and shooting. A common training strategy for a football team is to let each player learn these fundamental skills first and then train players together to cooperatively use these skills for scoring the goal, which is more efficient than directly training players without any skills together. Similarly, in a sparse reward multi-agent task, it's difficult for agents to cooperatively achieve a challenging goal from scratch. We can first try to discover the underlying useful skills within the task, which is much easier than solving the entire task under the sparse reward setting . Although these discovered skills maynot independently achieve the goal or induce non-zero rewards, we can learn to effectively combine them to accomplish the final task. In short, it is possible to decompose a sparse reward multi-agent task into a combination of different skills, which greatly simplifies the task complexity.

Previous works mainly follow two paradigms to discover skills in MARL. One is to let all agents learn a shared team skill [9; 10], which promotes team cooperation behaviors but suffers from high complexity. The other does the opposite and learns skills only from the perspective of individual agents [11; 12], which is more efficient since individual skills are usually easier to learn than team skills. However, merely learning individual skills may be insufficient to achieve team objectives.

Taking football as an example again, individual skills can refer to the technical abilities of individual players, such as dribbling and passing, while team skills refer to the ability of players to work together as a whole, _i.e._, the overall team tactics, such as wing-play and tiki-taka . As shown in Fig. 1, the goal of a football team is to let all players coordinate to master useful team skills from a global perspective. Directly learning the team skill is usually too complex. A better way is to decompose the team skill into different individual skills for players from an individual perspective, and ensure that the joint behavior of all players can form the team tactic. Both team skills and individual skills are important to a successful team. Therefore, a primary challenge for multi-agent skill discovery is how to simultaneously learn the individual skill for each individual agent and the team skill for the whole team, and the second challenge is how to combine these skills to accomplish multi-agent tasks.

To this end, we present a new paradigm for discovering both team and individual skills in MARL. Inspired by probabilistic inference in RL , we embed the multi-agent skill discovery problem into a probabilistic graphical model (PGM). With the PGM, we can formulate multi-agent skill discovery as an inference problem, allowing us to take advantage of various approximate inference tools. Then, we derive a variational lower bound as our optimization objective by following the structured variational inference , and propose Hierarchical Multi-Agent Skill Discovery (HMASD), a practical two-level hierarchical MARL algorithm for optimizing the derived lower bound. Specifically, the high-level policy employs a transformer  structure to assign skills to agents, with inputs consisting of the sequence of global state and all agents' observations, and outputs consisting of the sequence of team skill and agents' individual skills. In this way, an agent's individual skill depends on the team skill and all previous agents' individual skills, allowing agents to choose complementary skills and achieve better skill combination. The low-level policy chooses primitive actions for each agent to interact with the environment conditioned on the assigned skills. Moreover, we introduce two skill discriminators to generate intrinsic rewards for agents to learn diverse and distinguishable skills. Finally, we show that HMASD achieves superior performance on sparse reward multi-agent benchmarks compared to strong MARL baselines. To our best knowledge, our work is the first attempt to model both team skills and individual skills with the probabilistic graphical model in MARL.

## 2 Preliminaries

In this section, we first introduce the problem formulation and notations for cooperative MARL. Then, we describe the concept of skill and a mutual information based objective for skill discovery in unsupervised RL. We finally present the framework of representing RL as a probabilistic graphical model. More related works are discussed in Sec. 5.

### Problem Formulation

In this work, we consider a fully cooperative MARL problem, which is usually described as a decentralized partially observable markov decision process (Dec-POMDP) . A Dec-POMDP is

Figure 1: Illustration of team skill and individual skill in football.

defined by a tuple \(G=,,,P,r,,O,\), where \(\{g^{1},g^{2},,g^{n}\}\) is the set of \(n\) agents and \(\) is the global state space of the environment. All agents share the same action space \(\) and the joint action space is \(^{n}\). At timestep \(t\), each agent \(g^{i}\) chooses an action \(a_{t}^{i}\), where \(i\{1,2,,n\}\) is the agent identity. The actions of all agents form a joint action \(_{t}^{n}\). By executing \(_{t}\), the environment transitions to the next global state \(s_{t+1} P(s_{t+1}|s_{t},_{t})\), and all agents receive a shared team reward \(r(s_{t},_{t})\). Each agent \(g^{i}\) can only observe a partial observation \(o_{t}^{i}\) according to the observation probability function \(O(s_{t},g^{i})\). The joint observation of all agents is denoted as \(_{t}\). \([0,1)\) is the discount factor. The objective is to learn a joint policy \(\) that maximizes the expected global return \([_{t=0}^{}^{t}r_{t}|\,]\). In particular, this paper focuses on the sparse reward setting, where the team reward \(r_{t}\) is zero for most timesteps.

### Mutual Information based Skill Discovery

Skill discovery is a popular paradigm in unsupervised RL, which enables agents to discover diverse skills without a reward function. A _skill_ is represented by a latent variable \(z\), which is used as an additional input to the policy and results in a latent-conditioned policy \((a|s,z)\). The intention behind skill discovery is to expect the skill to control which states the agent visits and different skills to visit different states. This can be achieved by a simple objective of maximizing the mutual information between state \(s\) and skill \(z\), _i.e._, \((s;z)=(z)-(z|s)\). It's challenging to directly optimize the mutual information. Therefore, a majority of skill discovery methods [17; 18; 19; 20] derive a variational lower bound for the mutual information as follows:

\[(s;z)=_{s,z p(s,z)}[ p(z|s)- p(z)] _{s,z p(s,z)}[ q_{}(z|s)- p(z)],\] (1)

where \(q_{}(z|s)\) is a learned model parameterized by \(\) to approximate the intractable posterior \(p(z|s)\). Then, we can treat \( q_{}(z|s)- p(z)\) as the intrinsic reward to optimize the latent-conditioned policy \((a|s,z)\), where different \(z\) correspond to different skills.

### Probabilistic Graphical Model for RL

The probabilistic graphical model (PGM) is a powerful tool for modeling complex and uncertain systems. It provides a graphical representation of the relationship between variables in a system, where nodes correspond to random variables and edges represent conditional dependencies. In recent years, PGM has been widely used in RL to model complex decision-making tasks [21; 22; 23; 14; 15]. In this work, we follow the basic PGM for RL , which embeds the control problem into a graphical model and formulates it as an inference problem. As shown in Fig. 2(a), it first models the relationship among states, actions, and next states based on the dynamics \(P(s_{t+1}|s_{t},_{t})\). To incorporate the reward function, it introduces a binary random variable \(_{t}\) called _optimality variable_ into the model, where \(_{t}=1\) denotes timestep \(t\) is optimal, and \(_{t}=0\) indicates timestep \(t\) is not optimal. The probability distribution over \(_{t}\) is \(p(_{t}=1|s_{t},_{t})=(r(s_{t},_{t}))\). Refer to , we then perform structured variational inference to derive the final objective, which is to optimize a variational lower bound (also called evidence lower bound). The evidence is that \(_{t}=1\) for all \(t\{0,,T\}\). We will use \(_{t}\) to denote \(_{t}=1\) for conciseness in the remainder of this paper.

Figure 2: (a) RL can be cast as an inference problem by considering a basic probabilistic graphical model consisting of states \(s_{t}\), actions \(_{t}\) and optimality variables \(_{t}\). (b) Similarly, we formulate multi-agent skill discovery as an inference problem by augmenting the basic probabilistic graphical model with team skills \(Z\), individual skills \(z^{i}\) and observations \(o_{t}^{i}\).

The variational lower bound is given by:

\[ p(_{0:T})_{s_{0:T},_{0:T} q(s_{0:T},_{0:T})}[_{t=0}^{T}r(s_{t},_{t})- q(_{t}|s_{t})],\] (2)

where \(q(_{t}|s_{t})\) is the learned policy. Optimizing this lower bound corresponds to maximizing the cumulative reward and the policy entropy at the visited states, which differs from the standard RL objective that only maximizes reward. The entropy term can promote exploration and prevent the policy from becoming too deterministic. This type of RL objective is sometimes known as maximum entropy RL [25; 26].

## 3 Method

In this section, we present our solution for learning both team and individual skills in MARL. We first model the multi-agent skill discovery problem with a PGM and derive a tractable variational lower bound as the optimization objective. We then propose a practical MARL algorithm to optimize the derived lower bound.

### Multi-Agent Skill Discovery as an Inference Problem

In this work, we study the skill discovery problem in multi-agent tasks. One way for multi-agent skill discovery is to treat all agents as one big virtual agent and directly learn skills from the perspective of the whole team, which can improve teamwork but suffer from high complexity. Another way is to learn skills from the perspective of each individual agent, which reduces the complexity but lacks collaboration. To combine the advantages of two ways, we propose to learn skills not only from the individual perspective but also from the team viewpoint.

Specifically, we utilize a latent variable denoted by \(Z\) to represent the skill of the entire team, referred to as team skill. And we use a latent variable denoted by \(z^{i}\) to represent the skill of agent \(g^{i}\), referred to as individual skill. The individual skills of all agents are represented by \(z^{1:n}^{n}\). In this work, both the team skill space \(\) and the individual skill space \(\) are defined as discrete spaces consisting of finite latent variables with one-hot encoding, where the number of team skills and individual skills are denoted as \(n_{Z}\) and \(n_{z}\), respectively. The team skill \(Z\) is acquired from the global view and is expected to control the global states that the whole team visits, while the individual skill \(z^{i}\) is developed through an individual perspective and is intended to control the partial observations accessed by agent \(g^{i}\). Besides, since individual behaviors are typically based on the overall team strategy, the individual skill \(z^{i}\) should depend on the team skill \(Z\). According to these intuitions, we employ a PGM illustrated in Fig. 2(b) to model the multi-agent skill discovery problem, where the team skill \(Z\) is conditioned on the global state \(s_{t}\) and the individual skill \(z^{i}\) is conditioned on both the team skill \(Z\) and agent \(g^{i}\)'s partial observation \(o^{i}_{t}\). With the PGM, we formulate multi-agent skill discovery as an inference problem. Then, we perform structured variational inference to derive our objective, which is to optimize a variational lower bound as follows:

\[ p(_{0:T})_{ q()} _{t=0}^{T}r(s_{t},_{t})+)+ _{i=1}^{n} p(z^{i}|o^{i}_{t},Z)}_{}\] (3) \[)-_{i=1}^ {n} q(z^{i}|o^{i}_{t},Z)}_{}-^ {n} q(a^{i}_{t}|o^{i}_{t},z^{i})}_{} ,\]

where \(=s_{0:T},_{0:T},_{0:T},Z,z^{1:n}\) is the joint trajectory containing states, observations, actions and skills. The detailed derivation of Eq. 3 is shown in Appendix B.

Optimizing this lower bound is to maximize the team reward \(r(s_{t},_{t})\) and three terms. The diversity term shows the probability of skills on their corresponding states and observations, which can be maximized to encourage different skills to visit different states and observations, and is a crucial element for learning diverse skills. The skill entropy term and the action entropy term reflect the entropy of skills and actions at the visited states and observations, respectively. Optimizing the two entropy terms can enhance the exploration during skill learning.

### Hierarchical Multi-Agent Skill Discovery

In this subsection, we present Hierarchical Multi-Agent Skill Discovery (HMASD), a practical two-level hierarchical MARL algorithm designed to optimize the derived lower bound in Eq. 3. To estimate the probability distributions in the lower bound, we utilize four approximate functions. Specifically, we first employ a team skill discriminator \(q_{D}(Z|s_{t})\) to approximate \(p(Z|s_{t})\). Then, we use an individual skill discriminator \(q_{d}(z^{i}|o_{t}^{i},Z)\) to approximate \(p(z^{i}|o_{t}^{i},Z)\). Moreover, a skill coordinator \(_{h}(Z,z^{1:n}|s_{t},_{t})\) is utilized to approximate both \(q(Z|s_{t})\) and \(q(z^{i}|o_{t}^{i},Z)\), while a skill discoverer \(_{l}(a_{t}^{i}|o_{t}^{i},z^{i})\) is applied to approximate \(q(a_{t}^{i}|o_{t}^{i},z^{i})\). As shown in Fig. 3, these approximate functions are integrated into a two-level hierarchical structure for multi-agent skill discovery. At the high level, the skill coordinator assigns team skill \(Z\) and individual skills \(z^{1:n}\) to agents every \(k\) timesteps, where \(k^{+}\) is the number of timesteps between two consecutive skill assignments and is called _skill interval_. At the low level, the skill discoverer employs a latent-conditioned policy for each agent to explore the assigned skills using intrinsic rewards generated by the skill discriminators. Below, we describe how to learn these approximate functions for optimizing the derived lower bound.

Skill CoordinatorTransformer  has recently shown great potentials in MARL [27; 28; 29; 30; 31]. In this work, we take inspiration from MAT , which applies a transformer structure to map the agents' observation sequence into the agents' action sequence. Similar to MAT, we employ a transformer structure for the skill coordinator in our method as shown in Fig. 3. Specifically, we take the sequence of state and observations \((s,o^{1},o^{2},,o^{n})\) as inputs and embed them into vectors with the same dimension. Then, the embedded sequence is passed through a state encoder, which contains several encoding blocks. Each encoding block consists of a self-attention mechanism, a multi-layer perceptron (MLP) and residual connections. The state encoder outputs a sequence denoted as \((,^{1},^{2},,^{n})\), which encodes \((s,o^{1},o^{2},,o^{n})\) into informative representations and is used to approximate the high-level value function \(V_{h}\). After encoding the state and observations, we assign skills in an auto-regressive way. We start with an arbitrary symbol \(Z^{0}\), which is embedded and then fed into a skill decoder containing several decoding blocks. Each decoding block consists of two masked self-attention mechanisms, an MLP and residual connections, where the query of the second masked self-attention mechanism is the output of state encoder. The output of skill decoder is then fed to an MLP to generate the high-level policy \(_{h}(Z|,^{1:n},Z^{0})\). We sample a team skill \(Z\) from the policy and insert it back into the decoder to generate \(z^{1}\). By analogy, after \(n+1\) decoding rounds, we get team skill \(Z\) and all agents' individual skills \(z^{1:n}\). In this way, agent \(g^{i}\)'s individual skill

Figure 3: The overall framework of HMASD. At the high level, the skill coordinator adopts a transformer structure to assign team skill \(Z\) and individual skills \(z^{1:n}\) to agents. At the low level, the skill discoverer chooses primitive action \(a^{i}\) for agent \(g^{i}\) conditioned on the assigned skill \(z^{i}\) and forms a joint action \(\) to interact with the environment. The environment returns global state \(s\), joint observation \(\) and team reward \(r\). To make the skills diverse and distinguishable, two skill discriminators are employed to generate intrinsic rewards \(r_{in}\) for the skill discoverer.

depends on team skill \(Z\) and all previous agents' individual skills \(z^{1:i-1}\), which can prevent skill duplication and allow agents to choose complementary individual skills based on the team skill.

Skill DiscovererAfter assigning skills sequentially, we learn a shared skill discoverer to explore the assigned skills for agents. The skill discoverer consists of a decentralized actor and a centralized critic as shown in Fig. 3. The decentralized actor takes the partial observation \(o^{i}\) and individual skill \(z^{i}\) as inputs, and outputs the low-level policy \(_{l}(a^{i}|o^{i},z^{i})\) that chooses action \(a^{i}\) for each agent \(g^{i}\). All agents share a centralized critic to approximate the low-level value function \(V_{l}(s,Z)\) based on the global state \(s\) and team skill \(Z\). In our implementation, both the actor and the critic consist of an MLP, a GRU  and another MLP. The actor aims to learn the assigned individual skill \(z^{i}\) for each agent \(g^{i}\), while the critic takes a global view and expects to guide the joint behavior of all agents to discover the assigned team skill \(Z\).

Skill DiscriminatorTo make the learned skills diverse and distinguishable, we learn two skill discriminators, a team discriminator and an individual discriminator. The team discriminator inputs the global state \(s\) and outputs the probability of each team skill \(Z\), denoted as \(q_{D}(Z|s)\). In addition, we employ an individual discriminator \(q_{d}(z^{i}|o^{i},Z)\), which takes the partial observation \(o^{i}\) and team skill \(Z\) as inputs and then outputs the probability of each individual skill \(z^{i}\). Both discriminators are composed of an MLP. The team discriminator aims to discriminate the team skill based on the global state, while the individual discriminator is designed to discriminate the individual skill given the observation and team skill. The two skill discriminators are utilized to generate intrinsic rewards \( q_{D}(Z|s)\) and \( q_{d}(z^{i}|o^{i},Z)\), respectively. \( q_{D}(Z|s)\) is to reward all agents to jointly explore global states that are easy to discriminate, while \( q_{d}(z^{i}|o^{i},Z)\) rewards each agent \(g^{i}\) for visiting those easily distinguishable observations given the team skill \(Z\). In other words, it encourages different skills to explore different areas of the state-observation space. Since if two skills explore the same state, this state will be hard to discriminate and then lead to low intrinsic rewards. Therefore, the intrinsic rewards can guide agents to learn diverse and distinguishable skills.

Overall Training and ExecutionAccording to Eq. 3, we need to optimize the team reward, the diversity term, the skill entropy term and the action entropy term. At each episode, the high-level skill coordinator assigns skills to agents every \(k\) timesteps, then the low-level skill discoverer learns to explore the assigned skills for agents during these \(k\) timesteps. We define the sum of team rewards over these \(k\) timesteps as the single-step reward for the high-level policy, _i.e._, the high-level reward can be written as \(r_{t}^{h}=_{p=0}^{k-1}r_{t+p}\). For the low-level policy, we use a combination of extrinsic team reward and intrinsic rewards, _i.e._, the low-level reward for agent \(g^{i}\) is:

\[r_{t}^{i}=_{e}r_{t}+_{D} q_{D}(Z|s_{t+1})+_{d} q_ {d}(z^{i}|o^{i}_{t+1},Z),\] (4)

where \(_{e}\), \(_{D}\) and \(_{d}\) are three positive coefficients. The team reward ensures the learned skills are useful for the team performance, while the intrinsic rewards are used to optimize the diversity term in Eq. 3. We adopt the popular PPO  objective to optimize both the high-level skill coordinator and the low-level skill discoverer. Given a policy \((a|x)\) with parameters \(\), a value function \(V(y)\) with parameters \(\) and a reward function \(r\), we write a generic template for PPO objective as:

\[_{}\{(a|x),V(y),r\}=()+ _{c}(),\] (5) \[()=-_{t}[( |x_{t})}{_{old}(a_{t}|x_{t})}_{t},(|x _{t})}{_{old}(a_{t}|x_{t})},1-,1+)_{t}) ],\] \[()=_{t}[(V(y_{t}) -_{t})^{2},(V_{old}(y_{t})+(V(y_{t})-V_{old }(y_{t}),-,)-_{t})^{2}}],\]

where \(_{t}\) is the advantage computed using GAE , \(_{t}=_{t}+V_{old}(y_{t})\) and \(_{c}\) is the coefficient of value loss. Then, we write the overall objective for the high-level skill coordinator as:

\[_{h}(_{h},_{h})= _{}\{_{h}(Z|,^{1:n},Z^{0}),V_ {h}(),r_{t}^{h}\}+_{i=1}^{n}_{}\{_{h}(z^{i}| ,^{1:n},Z^{0},Z,z^{1:i-1}),V_{h}(^{i}),r_{t}^{h}\}\] \[-_{h}[(_{h}(Z|, ^{1:n},Z^{0}))]+_{i=1}^{n}[(_{h} (z^{i}|,^{1:n},Z^{0},Z,z^{1:i-1}))],\] (6)

where \((_{h}())\) is the entropy of the high-level policy that aims to optimize the skill entropy term in Eq. 3, and \(_{h}\) is the high-level entropy coefficient. \(_{h}\) and \(_{h}\) denote the parameters of the policy and value function in the skill coordinator, respectively. Similarly, the overall objective for the low-level skill discoverer is:

\[_{t}(_{t},_{t})=_{i=1}^{n}_{}\{ _{t}(a^{i}|o^{i},z^{i}),V_{t}(s,Z),r_{t}^{i}\}-_{t}_{i=1}^{n} [(_{t}(a^{i}|o^{i},z^{i}))],\] (7)

where \((_{t}())\) denotes low-level policy entropy and optimizes the action entropy term in Eq. 3, and \(_{l}\) is the low-level entropy coefficient. \(_{l}\) and \(_{l}\) represent the parameters of the low-level policy and value function, respectively. The skill discriminator is trained in a supervised manner with the categorical cross-entropy loss:

\[_{d}(_{D},_{d})=-_{(s,Z)}[  q_{D}(Z|s)]-_{i=1}^{n}_{(o^{i},Z,z^{i})}[ q_{d}(z^{i}|o^{i},Z)],\] (8)

where \(=\{(s,Z,,z^{1:n})\}\) is a dataset storing the state-skill pairs during training. \(_{D}\) and \(_{d}\) are the parameters of team discriminator and individual discriminator, respectively. The pseudo code of our method is shown in Appendix A.

During the execution phase, we only use the centralized high-level policy \(_{h}(Z,z^{1:n}|s,)\) and the decentralized low-level policy \(_{l}(a^{i}|o^{i},z^{i})\). For every \(k\) timesteps, the high-level policy \(_{h}(Z,z^{1:n}|s,)\) first chooses a team skill \(Z\) (_i.e._, team strategy) from a global perspective and then sequentially assigns complementary individual skills \(z^{1:n}\) to agents based on the team strategy \(Z\). With the assigned individual skill \(z^{i}\), each agent \(g^{i}\) selects an action \(a^{i}\) according to the low-level policy \(_{l}(a^{i}|o^{i},z^{i})\) at every timestep for execution. Therefore, our method performs one timestep of centralized execution and \(k-1\) timesteps of decentralized execution in every \(k\) timesteps. Such periodic and spaced centralized execution can coordinate agents more efficiently from a global view compared to the fully decentralized execution, and a small amount of centralized execution is acceptable in many multi-agent tasks. For example, during a basketball game, the coach can call a timeout and gather all players to adjust the team strategy and each player's individual strategy. In short, our method achieves a balance between fully centralized execution [31; 35] and fully decentralized execution [36; 37; 38; 4; 39].

## 4 Experiments

In this section, we evaluate the effectiveness of our method. We first conduct a case study to show how HMASD effectively learns diverse useful skills and combines them to complete the task. Then, we compare HMASD with strong MARL baselines on two challenging sparse reward multi-agent benchmarks, _i.e._, SMAC  with 0-1 reward and Overcooked . We further perform ablation studies for HMASD to confirm the benefits of components in our method. We select MAPPO , MAT  and MASER  as our baselines. MAPPO and MAT are two strong policy-based MARL algorithms that achieve state-of-art performance on various multi-agent tasks [43; 44; 40; 45]. MASER

Figure 4: (a) The Alice_and_Bob game. The objective is to collect both diamonds, where each diamond is allowed to be collected only when an agent is standing on the button with the same color. (b) At timestep \(t=0\), HMASD selects the team skill of collecting the blue diamond \((Z=0)\) and two individual skills for reaching the blue button \((z^{1}=3)\) and blue diamond \((z^{2}=2)\). (c) After collecting the blue diamond, the team skill transitions to collecting the red diamond \((Z=1)\) with two individual skills of reaching the red button \((z^{1}=1)\) and red diamond \((z^{2}=0)\) at timestep \(t=k\), where \(k\) is the skill interval.

presents a goal-conditioned method for solving sparse reward MARL. For all methods, we show the mean and variance of the performance across five different random seeds. The hyperparameter setting can be found in Appendix E.

### Case Study

In this subsection, we design a toy game Alice_and_Bob to demonstrate how our method works. As shown in Fig. 4(a), the Alice_and_Bob game is an \(8 8\) grid world environment surrounded by walls. There are two agents Alice and Bob with random initial positions, two buttons at the top and two diamonds at the bottom. The goal of the game is to collect both diamonds, where each diamond is allowed to be collected only when an agent is standing on the button with the same color.

Alice and Bob can receive a non-zero team reward only after they cooperatively collect both diamonds. We set the number of team skills \(n_{Z}=2\) and the number of individual skills \(n_{z}=4\) for HMASD. Fig. 4(b) and (c) show the learned skills of our method. We can observe that the two team skills \(Z=0,1\) correspond to collecting the blue diamond and red diamond for the whole team, respectively, and the four individual skills \(z^{i}=0,1,2,3\) guide the individual agent to reach the red diamond, red button, blue diamond and blue button, respectively. We observe similar skills for agents with different initial positions in Appendix C. By learning these diverse and useful skills, our method achieves a higher success rate of task completion than baselines as shown in Fig. 5. These results demonstrate that HMASD could discover significant team and individual skills, and effectively combine them to accomplish the task.

### Performance on Sparse Reward Multi-Agent Benchmarks

In this subsection, we first test our method on a widely-used MARL benchmark, SMAC , where we learn to control a team of ally units against a team of enemy units controlled by a built-in strategy. The ally team wins the game only if all enemy units are killed within the time limit of an episode. Our objective is to maximize the win rate for the ally team. The default reward setting of SMAC contains many dense rewards, such as the unit's health and damage. These dense rewards enable many MARL algorithms like MAT  and MAPPO  to attain almost 100% win rate on all scenarios. However, designing useful dense rewards is usually expensive and adjusting the weights between rewards is

Figure 5: Performance comparison between HMASD and baselines on Alice_and_Bob.

Figure 6: Performance comparison between HMASD and baselines on SMAC with 0-1 reward.

Figure 7: Performance comparison between HMASD and baselines on Overcooked.

time-consuming. In this work, we consider a more general reward setting, _i.e._, SMAC with 0-1 reward, which returns a non-zero team reward of 1 only at the last timestep if the ally team wins. As shown in Fig. 6, HMASD significantly outperforms baselines on SMAC with 0-1 reward, where MAT and MAPPO don't work at all. MASER only works on the scenario 3m and fails on the other two scenarios 2s_vs_lsc and 2m_vs_lz. This demonstrates the effectiveness of HMASD and the high efficiency of skill discovery for solving sparse reward multi-agent tasks.

Next, we evaluate HMASD on another sparse reward multi-agent task called Overcooked, a popular cooperative cooking simulation game. We follow a simplified version of the game proposed in , the objective of which is to deliver the soup as fast as possible. Each soup requires agents to place 3 onions in a pot, cook them for 20 timesteps, put the cooked soup in a dish, and deliver it to the service desk. All agents will receive a team reward of 20 after delivering a soup. In Fig. 7, we compare HMASD with baselines on three Overcooked scenarios. It can be observed that HMASD could also achieve a superior performance over baselines on Overcooked, which further confirms that HMASD can discover useful skills to effectively complete the task.

Moreover, we visualize the learned skills and find that HMASD uses only a few skills to complete the complex task after training. This is because HMASD will encourage different skills to explore different state-observation spaces, but only a small part of the state-observation space can result in a non-zero team reward in the complex sparse reward multi-agent task. For example, on the SMAC scenario, there is usually a big map, we observe that most of the learned skills explore areas of the map without enemies and thus don't contribute to the team reward. These results indicate that when the state-observation space is large, HMASD can discover diverse skills but maybe only some of them are useful for the team reward. More fine-grained results can be found in Appendix F.

Additionally, we compare HMASD with an exploration bonus MARL baseline  in Appendix H. The results show that the performance improvements of HMASD mainly come from diverse skill discovery and effective skill combination, rather than implicit exploration.

### Ablation Studies

In this subsection, we conduct ablation studies to investigate the impact of three main components in HMASD: (1) discovering both team and individual skills, (2) using a combination of extrinsic team reward and intrinsic rewards for the low-level reward and (3) employing the high-level skill coordinator to assign skills to agents. To test component (1), we introduce two variants of HMASD, denoted as HMASD_NoTeam and HMASD_NoIndi, respectively. HMASD_NoTeam only learns individual skills for agents, while HMASD_NoIndi only lets all agents learn a shared team skill. To evaluate component (2), we consider HMASD_NoExRew and HMASD_NoInRew, which set \(_{e}=0\) and \(_{D}=_{d}=0\) in Eq. 4, respectively. As for component (3), we adopt HMASD_NoHigh, which removes the high-level policy and randomly assigns skills to agents at the start of each episode. As shown in Fig. 8, HMASD performs better than both HMASD_NoTeam and HMASD_NoIndi, which emphasizes the importance of discovering both team and individual skills in multi-agent tasks. After removing the extrinsic team reward or intrinsic rewards in the low-level reward, the performance of HMASD has a large drop. Especially without intrinsic rewards, HMASD can't work on most scenarios. This highlights the considerable contributions of optimizing the diversity term in Eq. 3. Besides, the performance comparison between HMASD and HMASD_NoHigh reveals the necessity of the high-level policy. More ablations for HMASD are shown in Appendix D.

Figure 8: Ablation studies regarding components of HMASD.

Related Work

Skill Discovery in MARLSkill discovery is a promising approach to solving complex tasks for its ability to discover diverse skills even without any extrinsic reward. Recently, this approach has been extended to MARL. MASD  learns coordinating skills adversarially by setting an information bottleneck. VMAPD  constructs a diverse Dec-POMDP to learn diverse skills as different solutions for multi-agent tasks. However, these two methods focus on learning a shared team skill for all agents, and use only one skill throughout an episode. To combine different skills to tackle multi-agent tasks, HSD  employs a hierarchical structure, where the high-level policy selects low-level skills for agents based on local observations. HSL  follows a similar hierarchical structure and introduces a skill representation mechanism to enhance skill learning. Nevertheless, these two methods only learn individual skills from each agent's individual perspective, without considering team skill learning. Besides, in these two methods, each agent selects skills only based on its local observation, leading to potential skill duplication between agents with similar observations. ODIS  applies skill discovery to offline MARL, which discovers generalizable individual skills across different tasks from offline multi-task data, and trains a coordination policy to assign skills to agents with the centralized training and decentralized execution paradigm . However, this decentralized skill assignment may also lead to skill duplication. Chen et al.  learn joint options (_i.e._, skills) by approximating the joint state space as the Kronecker product of the state spaces of individual agents. In this work, we propose to discover both team skills and individual skills. Moreover, we assign skills to agents from a global perspective to enable complementary skill selection and prevent skill duplication effectively.

Sparse Reward MARLThe issue of sparse reward poses a significant obstacle when applying RL to solve real-world problems, which will be exacerbated in multi-agent tasks. Several approaches have been proposed to address the sparse reward problem in MARL. SEAC  leverages experience sharing among agents to enable efficient exploration. CMAE  promotes cooperative exploration by selecting a shared goal for agents from state spaces. VACL  tackles the sparse reward problem in MARL by utilizing an automatic curriculum learning algorithm that incrementally expands the training tasks from easy to hard. More recently, MASER  introduces a goal-conditioned method that generates subgoals for agents from the experience replay buffer. In this work, we present a hierarchical MARL algorithm that discovers underlying skills and effectively combines these skills to solve sparse reward multi-agent tasks.

## 6 Conclusion

It is an efficient way to learn a set of skills and combine them properly to tackle complex tasks. In this study, we take advantage of skill discovery to address MARL problems. We propose to discover latent team and individual skills by embedding them into a probabilistic graphical model. In this way, we formulate multi-agent skill discovery as an inference problem, and derive a variational lower bound as the optimization objective. We then design a practical MARL method called HMASD to optimize the lower bound, where different team skills explore different global state spaces and different individual skills explore different local observation spaces. The empirical results show that HMASD significantly improves performance on challenging sparse reward multi-agent tasks by learning diverse team and individual skills with efficient skill combination.