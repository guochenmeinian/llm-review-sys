# Zero-Shot Event-Intensity Asymmetric Stereo via

Visual Prompting from Image Domain

Hanyue Lou\({}^{\#1,2}\)

Jinxiu Liang\({}^{\#1,2}\)

Minggui Teng\({}^{1,2}\)

Bin Fan\({}^{3}\)

Yong Xu\({}^{4}\)

Boxin Shi\({}^{*1,2}\)

\({}^{1}\) State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University

\({}^{2}\) National Engineering Research Center of Visual Technology, School of Computer Science, Peking University

\({}^{3}\) National Key Laboratory of General AI, School of Intelligence Science and Technology, Peking University

\({}^{4}\) School of Computer Science and Engineering, South China University of Technology

{hylz,cssherryliang,minggui_teng,binfan,shiboxin}@pku.edu.cn yxu@scut.edu.cn

###### Abstract

Event-intensity asymmetric stereo systems have emerged as a promising approach for robust 3D perception in dynamic and challenging environments by integrating event cameras with frame-based sensors in different views. However, existing methods often suffer from overfitting and poor generalization due to limited dataset sizes and lack of scene diversity in the event domain. To address these issues, we propose a zero-shot framework that utilizes monocular depth estimation and stereo matching models pretrained on diverse image datasets. Our approach introduces a visual prompting technique to align the representations of frames and events, allowing the use of off-the-shelf stereo models without additional training. Furthermore, we introduce a monocular cue-guided disparity refinement module to improve robustness across static and dynamic regions by incorporating monocular depth information from foundation models. Extensive experiments on real-world datasets demonstrate the superior zero-shot evaluation performance and enhanced generalization ability of our method compared to existing approaches.

## 1 Introduction

Stereo matching has witnessed significant advancements in recent years, driven by deep learning techniques and the availability of extensive training datasets in the image domain . These advancements have enabled widespread applications in various fields, including mapping , navigation , 3D reconstruction , motion estimation , and image restoration . Additionally, the abundance of unlabeled data on the internet has recently fueled the progress of monocular depth estimation .

Event cameras report per-pixel relative intensity _changes_ asynchronously at high temporal resolutions within a wide dynamic range , providing complementary sensory information alongside conventional frame-based cameras that capture _absolute_ intensity values synchronously. Event-intensity asymmetric stereo matching has emerged as a promising approach to achieve robust performance in challenging conditions such as ultra-wide dynamic range and fast-moving scenes that cannot be faithfully captured by conventional frame-based cameras alone, by leveraging the complementary strengths of event and frame cameras in different views . Despite the potential benefits, existing event-intensity asymmetric stereo approaches often rely on supervised learning or fine-tuning, requiring large amounts of labeled training data. Unfortunately, the shorter history of event-based sensors in commercial markets poses a scarcity of large-scale datasets essential foreffective training and generalization. The scarcity of large-scale datasets in the event domain has resulted in overfitting and poor generalization to new environments or unseen disparity ranges .

Stereo models estimate disparity by establishing feature similarities between views, assuming that the two inputs are aligned in feature representation space. As events and frames capture relative differences and absolute values of intensity, respectively, they inherently possess a strong physical connection. This connection can be leveraged to convert them into intermediate representations with comparable appearance patterns. In the context of event-intensity asymmetric stereo, where training data are significantly limited compared to images, it is crucial and beneficial to develop a zero-shot approach that does not necessitate training data for modifying the underlying architecture or weights of the models. Considering the recent progress in image-based stereo matching [18; 16], where models trained on extensive datasets have exhibited effective zero-shot generalization, as well as the emerging techniques of "visual prompting" [32; 36; 1], which aims to adapt off-the-shelf models to new domains or modalities without modifying the model architecture or weights, we are motivated to utilize off-the-shelf models from the image domain with only modified inputs, rather than altering the weights, which requires substantial data.

Yet, several challenges impede the introduction of off-the-shelf models from the image domain to event in a zero-shot manner: 1) Significant modality gaps exist between events and frames (the red boxes in Figure 1), where events are triggered by temporal differences between frames exceeding predefined thresholds, compounded by sensor imperfections and stochastic electric noise. 2) In static regions where events cannot be triggered (the green boxes in Figure 1), no correspondences can be established, necessitating hallucination from the monocular model processing frames. However, these models typically provide relative disparities, whose distances from the actual metric are mostly calculated up to a global scale and bias.

In this paper, we propose a **Z**ero-shot **E**vent-intensity asymmetric **ST**ereo (ZEST) framework that leverages both monocular depth estimation and stereo matching models from the image domain, which is shown in Figure 1. To address the appearance gap between frames and events, we introduce a representation alignment module that considers the physical formulation from frames to events. The disparity map is then estimated from frames and events in different views using an off-the-shelf stereo model in the image domain. We further propose a monocular cue-guided disparity refinement module that re-renders these disparities by rescaling the relative depths predicted by a monocular depth estimation foundation model, enhancing robustness in regions with few events or textures. Our framework demonstrates superior performance among training-free methods for intensity-event asymmetric stereo matching and enhanced generalization across diverse real-world scenes. The

Figure 1: The proposed **Z**ero-shot **E**vent-intensity asymmetric **ST**ereo (ZEST) framework estimates disparity by finding correspondences between RGB frames and event data. (a) Our method conducts stereo matching by utilizing off-the-shelf stereo matching and monocular depth estimation models with frozen weights, and feeding them visual prompts tailored to the physical formulation of frames and events (temporal difference of frames and temporal integral of events, respectively). (b) In contrast, existing methods (_e.g._, ) that rely on training data with known ground truth disparities often suffer from limited annotated data availability, thus leading to unsatisfactory results.

flexibility of our approach allows for seamless upgrades of the stereo and monocular models alongside advances in the related fields. Our main contributions are as follows:

* We present the first zero-shot event-intensity asymmetric stereo matching method that leverages off-the-shelf depth estimation models from the image domain.
* We introduce a visual prompting method for representation alignment between events and frames, enabling the utilization of off-the-shelf stereo models without modification.
* We propose a monocular cue-guided disparity refinement method for robustness in regions with few events or textures, inspired by recent advancements in monocular depth estimation.

## 2 Related Work

Intensity-based stereo and monocular depth estimation.With the development of deep learning technology, significant progress has been made in stereo matching, with methods categorized based on their cost construction and aggregation approaches. Correlation-based methods [27; 35; 39; 8] and those using 3D convolutions [3; 5; 33] have achieved impressive performance. Recently, iterative optimization-based networks [20; 34; 41] have demonstrated superior accuracy and robustness. In monocular depth estimation, models like Depth Anything  and MiDaS  leverage extensive unlabeled data to estimate relative depth, enabling generalization across domains at the cost of unknown scale and shift.

Event-based symmetric stereo.Event cameras capture pixel-level brightness changes asynchronously, offering advantages over conventional frame-based cameras. Event-based stereo depth estimation has emerged rapidly. Representative works include utilizing camera velocity  or estimating depth without explicit event matching . Deep learning solutions have considered novel sequence embedding  and fusion of frame and event data [22; 23] for improved depth estimates in challenging scenarios. Recent efforts explore integrating off-the-shelf models from the image domain  to improve stereo matching performance by leveraging the inherent connection between frame and event data.

Event-intensity asymmetric stereo.Event-frame asymmetric stereo matching leverages the complementary strengths of event and frame cameras. Traditional methods focused on aligning and fusing asynchronous event data with synchronous frame data using hand-crafted features  and traditional stereo matching algorithms . Deep learning approaches [46; 40; 4] have been employed to learn complex mappings for event-frame fusion and dense depth estimation. However, these methods often suffer from overfitting and poor generalization due to limited dataset sizes and scene diversity in the event domain. Our work proposes a zero-shot approach that leverages disparity estimation models from the image domain by visual prompting, eliminating the need for additional training and improving generalization.

Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence.

Method

**Overview** The proposed method aims to estimate depth from a frame-based camera and an event camera in different views, separated by a baseline distance. Without loss of generality, we assume that the frames are in the left view and the events are in the right view. Given consecutive rectified event-intensity pairs \((I^{}(_{i}),E^{}(_{i})\) and \((I^{}(_{i+1}),E^{}(_{i+1}))\), our goal is to infer the corresponding disparity map \(D(_{i})\) at timestamp \(_{i}\).

The overall framework of the proposed ZEST for event-intensity asymmetric stereo is shown in Figure 2, consisting of two components: the representation alignment module for aligning the frames in the left view and events in the right view into an intermediate representation space (Sec. 3.1), and the disparity refinement module for improving stereo matching results under the guidance of monocular model predictions (Sec. 3.2).

### Event-intensity representation alignment for stereo matching

Stereo matching estimates depth by triangulation using pixel space representations, where stereo correspondence is established by finding similar patterns on a pixel-wise basis. With the advances in deep learning, modern stereo matching models are trained on massive data to estimate disparity. Due to the amount of training data and the diversity of real-world scenes, off-the-shelf models with frozen weights maintain robustness to different representations ranging from absolute values to relative changes in intensity, as shown in Figure 3. However, directly using these representations may not be optimal for event-intensity asymmetric stereo. This is because the event and frame data have fundamentally different characteristics, and a carefully designed intermediate representation can better bridge the appearance gap between them.

Inspired by this, we design an intermediate representation as a "visual prompt" to align the modalities in two views, enabling off-the-shelf stereo matching models to work for event-intensity asymmetric stereo. We will detail the formulation of the proposed intermediate representation in the following.

An event \(e=(t,,)\) at the pixel \(=(_{x},_{y})^{}\) and time \(t\) is triggered whenever the logarithmic change of irradiance \(I\) exceeds a pre-defined threshold \(c\) (> 0), _i.e._,

\[\| I_{}(t)- I_{}(t- t)\| c,\] (1)

where \(I(t)\) denotes the instantaneous intensity at time \(t\), and the polarity \(\{-1,+1\}\) indicates {negative, positive} brightness changes. We define \(e_{}(t)\) as a function of continuous time \(t\) such that,

\[e_{}(t)=_{}(t),\] (2)

whenever there is an event \(e=(,,)\). Here, \(_{}(t)\) is an impulse function, with unit integral, at time \(\), and the sequence of events is turned into a continuous-time signal, consisting of a sequence of impulses. There is such a function \(e_{}(t)\) for each position \(\) in the image. Since each pixel can be treated separately, we omit the subscripts \(\). Given a reference timestamp \(_{i}\), assuming that there are latent sharp image sequences \(I()\) with infinitesimal exposure time, their relationship between the corresponding events can be expressed as

\[I(_{i+1})=I(_{i})}^{_{i+1}}e(t)dt)}.\] (3)

Define the logarithmic brightness increment \( L_{i}(t)\) from two consecutive frames as

\[ L_{i}(t)= I(_{i+1})- I(_{i}).\] (4)

Figure 3: Visual comparisons of the disparity predicted by a stereo model  fed with inputs in the first two rows, which are aligned in the space of raw data, intensity (via ), events (via ), and intermediate (via the proposed method), respectively.

It can be approximated by events triggered during these frames as

\[_{i}(t)=c_{_{i}}^{_{i+1}}e(t)dt.\] (5)

In Eq. (5), the left-hand side represents the _temporal difference of frames_, while the right-hand side denotes the _temporal integral of events_. This formulation establishes an explicit intermediate representation that bridges the gap between frames and events with similar appearance, enabling correspondences to be found for stereo matching.

Now we turn to the frames captured in the real world, which have a non-negligible exposure time \(2T\). A frame \(I_{,T}(t)\) with exposure time \([-T,+T]\) can be represented as the average of the latent image \(I(t)\) over the exposure duration given a latent frame with a timestamp \(_{0}\) as reference , which can be formulated as

\[I_{,T}(t)=I(_{0})_{-T}^{+T}}^{t}e(s)ds)}dt.\] (6)

Then, the difference between two consecutive logarithmic frames \(L_{_{i}},L_{_{i+1}}\) with exposure time \(2T\) with the middle latent frame \(I_{_{0}}\) as reference can be formulated as

\[_{i}(t)=-T}^{_{i}+T}}^{t}e(s)ds)}dt)}--T}^{ _{i+1}+T}}^{t}e(s)ds)}dt)}.\] (7)

We use the temporal difference map \( L(t)\) defined by consecutive frames in Eq. (4) and its approximation version defined from the temporal integral of events in Eq. (7) as explicit intermediate representations, respectively. The event trigger threshold \(c\) is often unknown in real scenarios. However, Eq. (7) still holds after we normalize both sides of the equation for eliminating the unknown \(c\), where percentile normalization is used for robustness. In practice, the calculations are done in discrete form, whose details can be found in the appendix.

Specifically, the disparity \(D^{}\) at timestamp \(t\) is estimated by stereo matching model \(^{}\) as

\[D^{}(t)=^{}( L^{}(t), ^{}(t)).\] (8)

As shown in Figure 3, the proposed event-intensity alignment method successfully finds appropriate visual prompts for the stereo models from the image domain, which helps to establish correspondences between the frames and events.

### Monocular cue guided disparity refinement

In the context of event-intensity asymmetric stereo, stereo matching often faces challenges in establishing reliable correspondences, particularly in textureless regions of left images and static regions with sparse events in the right view. In contrast, monocular depth estimation directly infers depth maps from single images by leveraging monocular cues such as texture variations, gradients, occlusion, known object sizes, haze, and defocus. Off-the-shelf monocular depth estimation models, such as Depth Anything  and MiDaS , have demonstrated impressive "zero-shot cross-dataset transfer" capabilities, thanks to the relaxed requirements for training data in unsupervised learning.

Inspired by this, we propose a monocular cue-guided disparity refinement approach. However, there may be unknown scale and shift discrepancies between the predictions of the stereo and monocular models, which may vary spatially due to the absence of physically measurable information during monocular depth estimation. To address these factors, we model the desired refined disparity map as a locally linear transformation of the estimation from the monocular cue. Let \(D^{}\) represent the disparity map predicted by a monocular depth estimation model \(^{}\) from frame \(I\), _i.e._,

\[D^{}=^{}(I),\] (9)

whose relationship with the binocular estimation \(D^{}\) is assumed a linear transform as

\[D^{} W(D^{}+B),\] (10)

where \(\) denotes the element-wise multiplication operation, and \(W\) and \(B\) denote the scale map and the shift map, respectively.

To estimate the scale map \(W\) and shift map \(B\), we minimize the following loss function:

\[W^{*},B^{*}=*{argmin}_{W,B}_{}+ _{},\] (11)

where the loss function involves several priors about the desired disparity map, and \(\) is a regularization parameter to balance between them. We optimize this function using gradient descent with the Adam optimizer in PyTorch, running 500 iterations per image. Note that \(D^{}\) is predicted by establishing correspondence between frames and events, which is more reliable where there are more events. Therefore, the temporal difference map \( L^{}(t)\) of frames is utilized to construct a confidence map \(C\) to identify the density of events. Firstly, the estimated scale map \(W\) and shift map \(B\) should be consistent with the model defined in Eq. (10), which can be constrained by

\[_{}=_{}C_{}(W_{}(D^{}_{}+B_{})-D^{}_{}),\] (12)

where the \(_{1}\) distance is utilized for its robustness to outliers. Secondly, the scales and biases for neighboring pixels should be similar, which can be derived by an edge-ware smoothness as

\[_{}=_{}(|_{x}W_{}|+| _{x}B_{}|)e^{-(_{x}D^{}_{}) ^{2}}+(|_{y}W_{}|+|_{y}B_{}|)e^{-(_{ y}D^{}_{})^{2}}.\] (13)

This regularizer encourages local smoothness in the scale and shift maps. To ensure stability in the optimization steps for only one sample, a good initialization is necessary. While the shift map \(B\) is simply initialized as all ones matrix \(B^{(0)}=\), the scale map \(W\) is initialized as

\[W^{(0)}=_{}}C_{}D^{}_ {}/(D^{}_{}+B^{(0)}_{})}{_{ _{}}C_{}},\] (14)

where \(_{}\) is a window centered at position \(\). This loss term ensures the consistency modeled in Eq. (10) in regions with more events measured by \( L\) in the beginning of the optimization. At the end of the optimization, the refined disparity map \(\) can be obtained by

\[=W^{*}(D^{}+B^{*}).\] (15)

The proposed method effectively combines the strengths of both stereo matching and monocular depth estimation, leveraging the accurate but sparse disparity estimates from stereo matching to guide the refinement of the dense but relative depth estimates from monocular depth estimation.

Figure 4: From left to right, our model exhibits impressive generalization abilities across a broad spectrum of varied scenes, encompassing sparse event scenes, richly textured environments, dimly lit settings, close-range captures, and high dynamic range situations.

## 4 Experiments

**Dataset.** We evaluate the proposed ZEST framework on the widely-used benchmark dataset for event-intensity stereo matching, the DSEC dataset , a large-scale high-quality driving dataset with challenging scenes. It consists of synchronized event and frame streams captured from a stereo setup in a wider range of challenging scenarios, including fast motion, high dynamic range, and low light conditions. Specifically, it provides high-resolution (\(640 480\)) stereo event streams captured in outdoor driving scenes using Prophese Gen 3.1 event cameras across 53 outdoor driving scenarios under diverse lighting. Without specification, all 41 sequences (5 Intertaken sequences, 1 Thun sequence, and 35 Zurich City sequences) in the training set are adopted for evaluation, as the official "test" split lacks ground truth disparity. To address boundary issues with certain methods, such as E2VID's inability to reconstruct the initial frames without prior events, we exclude the first and last 10 frames of each sequence from metric calculations. To assess generalization, we also evaluate on the MVSEC  and M3ED  datasets. MVSEC , the pioneering stereo event dataset, includes ground truth depth maps in diverse scenarios with DAVIS346 event cameras (\(346 260\) resolution). We use three subsets for MVSEC evaluation: indoor_flying1 (500-1500), indoor_flying2 (500-2000),

    &  &  &  &  \\   & Zu & In & Th & All & Zu & In & Th & All & Zu & In & Th & All & Zu & In & Th & All \\  SHEF  & 10.43 & 11.93 & 14.61 & 10.66 & 18.05 & 18.22 & 24.42 & 18.10 & 51.07 & 74.54 & 55.98 & 54.37 & 60.21 & 80.12 & 65.93 & 63.01 \\ HSM  & 8.65 & 8.34 & 8.42 & 8.60 & 19.11 & 17.96 & 19.16 & 18.95 & 32.55 & 36.40 & 30.87 & 33.08 & 42.10 & 45.77 & 38.15 & 42.60 \\ DAE  & 12.43 & 12.09 & 13.89 & 12.39 & 15.66 & 15.44 & 17.12 & 15.63 & 8.71 & 80.62 & 89.97 & 86.96 & 91.48 & 90.74 & 93.58 & 91.39 \\  DAEI  & - & - & **1.93** & - & - & - & **2.94** & - & - & - & - & - & **16.82** & - & - & - \\   \\  PSMNet-ETNet & 29.58 & 30.27 & 19.68 & 29.64 & 44.80 & 44.67 & 34.23 & 44.74 & 80.09 & 58.98 & 75.93 & 80.90 & 89.33 & 91.95 & 87.14 & 89.69 \\ CR-ETNet & 27.99 & 19.20 & 5.25 & 26.67 & 34.31 & 26.93 & 12.31 & 33.19 & 31.90 & 25.44 & 12.75 & 30.92 & 40.46 & 35.64 & 20.42 & 39.71 \\ DS-ETNet & 20.84 & 24.04 & 2.93 & 21.22 & 29.32 & 20.45 & 5.67 & 30.78 & 34.19 & 36.18 & 23.00 & 34.42 & 44.36 & 47.43 & 33.10 & 43.98 \\ PSMNet E2VID & 29.50 & 26.96 & 25.07 & 25.09 & 29.04 & 34.84 & 37.34 & 39.49 & 38.41 & 38.44 & 82.31 & 81.86 & 90.15 & 91.93 & 90.53 & 90.32 \\ CR-E2VID & 24.65 & 7.70 & 3.67 & 22.20 & 30.60 & 12.17 & 8.15 & 27.94 & 27.78 & 12.75 & 0.51 & 25.60 & 35.06 & 21.23 & 15.18 & 33.05 \\ DS-E2VID & 13.30 & 17.40 & 2.37 & 13.83 & 20.02 & 30.72 & 4.28 & 21.46 & 28.20 & 28.83 & 20.23 & 28.25 & 36.38 & 38.70 & 29.09 & 36.68 \\   \\  CFF-v2e & 9.86 & 12.07 & 7.81 & 10.17 & 14.69 & 16.55 & 11.36 & 14.93 & 60.34 & 71.95 & 62.77 & 61.97 & 68.55 & 79.73 & 73.03 & 70.14 \\   \\  Ours-CR-MiDs & 3.64 & 8.79 & 2.21 & 4.35 & 4.60 & 9.68 & 3.23 & 5.30 & 28.68 & 33.07 & 21.61 & 29.26 & 48.02 & 51.52 & 41.12 & 48.48 \\ Ours-DS-MiDs & 22.47 & 7.56 & 1.68 & 3.00 & 3.46 & 12.07 & 2.82 & 4.66 & 14.48 & 15.77 & 12.46 & 14.91 & 26.31 & 25.85 & 22.39 & 26.61 \\ Ours-CR-DA & 3.18 & 9.00 & 31.31 & 3.99 & 4.27 & 9.93 & 24.00 & 5.05 & **9.75** & **10.48** & **2.26** & 9.84 & **18.76** & **18.05** & **12.43** & **18.64** \\ Ours-DS-DA & 2.24 & 7.66 & 1.71 & 2.99 & 3.44 & 12.05 & 2.86 & 4.64 & 14.67 & 17.44 & 13.11 & 15.05 & 26.14 & 28.21 & 22.83 & 26.41 \\   

Table 1: Quantitative comparisons of disparity estimation results with state-of-the-art methods from both event and image domains. The end-point-error (EPE), root mean square error (RMSE), 3-pixel error (3PE, %), and 2-pixel error (2PE, %) are adopted for evaluation. Zu, In, and Th denote the Zurich City, Intertaken, and Thun sequences on the DSEC  dataset, respectively. Red and orange highlights indicate the first and second best performing technique for each metric. \(\) (\(\)) indicates that higher (lower) values are better. The method with a gray background is the only one that does not adhere to the cross-dataset evaluation protocol.

Figure 5: Visual quality comparison of disparity estimation results among state-of-the-art methods (HSM , SHEF , DAEI  trained on MVSEC  and DSEC , respectively) and the proposed ZEST with various stereo matching models (CR and DS) and monocular depth estimation models (Mi and DA). The baseline method with the best EPE and RMSE metrics, _i.e._, DS-E2VID, is also included for comparison.

indoor_flying3 (500-2500), denoted as S1, S2, and S3, respectively. Given the differing frame and depth rates, we select the depth map closest to each image frame timestamp. The M3ED  dataset captures unique urban and forest scenes with Prophesee EVK4 HD event cameras (\(1280 720\) resolution). We use the car_urban_day_horse (300-700) sequence for evaluation.

**Metrics** We use the standard evaluation metrics for stereo matching, including the mean absolute error (MAE), root mean squared error (RMSE), and the percentage of pixels with errors larger than a threshold (_e.g._, 1, 2, or 3 pixels).

**Compared methods.** We compare the performance of the proposed ZEST framework with state-of-the-art event-intensity stereo matching methods, including both traditional and deep-learning-based approaches. For traditional methods, we consider SHEF  and HSM . For deep-learning-based methods, we compare against a state-of-the-art method DAEI , originally trained on the MVSEC  dataset (S2 and S3 splits), which has limited generalizability to DSEC. We also test a variant of DAEI (denoted DAEI\({}^{}\)), finetuned on the Zurich and Thun sequences in DSEC for 34 epochs and evaluated on both DSEC and M3ED.

**Baselines.** We also include several baseline methods that directly apply the off-the-shelf stereo models to the event and frame images without extra representation alignment or disparity refinement. To align the different modalities between the left and right views, we consider two cases, event-to-intensity and intensity-to-event, respectively. In the case of event-to-intensity, events in the right view are reconstructed into a gray image using E2VID  and ETNet  and paired with frames in the left view. The off-the-shelf image-based stereo models used include PSMNet (checkpoint trained on KITTI2015) , CREStereo (CR, checkpoint trained on ETH3D) , and DynamicStereo (DS, checkpoint trained on DynamicReplica and SceneFlow) . In the case of intensity-to-event, consecutive frames in the left view are converted by v2e , which are then fed to the off-the-shelf event-based stereo models CFF  together with the events in the right view. As for the proposed ZEST framework, we adopt CR and DS for the stereo models, and Depth Anything (DA, checkpoint Depth-Anything-Large, 335.3M parameters)  and MiDaS (Mi, checkpoint BEiT-L-512, 345M parameters)  for monocular depth estimation. Throughout this paper, we use abbreviations to denote specific combinations of modality alignment, stereo models, and monocular models. For example, the combination of the proposed technique, CREStereo, and Depth Anything is referred to as "Ours-CR-DA". All results for comparison are produced from their official codes and models with recommended hyperparameters provided on public available sources or provided by the authors.

**Compute environment setup.** All models are tested on an Intel i7-13700K CPU and a single NVIDIA RTX 4090 GPU. While representation alignment and disparity refinement modules can run on either CPU or GPU, stereo and monocular depth estimation models require GPU acceleration.

### Comparisons with prior arts

Quantitative results on the benchmark dataset DSEC  are reported in Table 1, demonstrating the proposed method's superior performance. The quantitative analysis revealed that our framework consistently outperformed almost all compared methods and baselines across every metric, except for DAEI\({}^{}\) and the baseline E2VID-CR. While all other methods are evaluated in a cross-dataset manner, DAEI\({}^{}\) is the only method that is evaluated in an in-dataset manner, which is trained and tested on the DSEC . Therefore, it is not surprising that

    &  &  &  &  \\   & S1 & S2 & S3 & All & S1 & S2 & S3 & All & S1 & S2 & S3 & All & S1 & S2 & S3 & All \\  HSM  & 9.64 & 12.98 & **9.09** & **10.57** & 11.43 & 15.81 & **10.87** & **12.70** & 82.22 & **83.75** & **79.48** & **81.82** & 85.96 & **89.03** & **83.68** & **86.22** \\
**DAEI ** & **10.08** & - & - & - & - & 1.55 & - & - & - & - & - & - & - & - & - \\ CR-E2VID & 16.33 & 23.62 & 18.01 & 19.32 & 16.94 & 24.64 & 18.55 & 20.04 & 79.18 & 82.43 & 83.39 & 83.64 & 82.12 & 92.05 & 88.42 & 87.53 \\ Ours-CR-DA & 3.34 & 7.19 & 5.13 & **5.22** & 3.83 & 7.83 & **5.60** & 5.76 & 19.06 & **41.72** & **28.66** & **29.82** & 33.22 & **61.38** & **43.97** & **46.19** \\   

Table 2: Quantitative results of the proposed zero-shot disparity estimation method on the MVSEC  dataset.

   Method & EPE\(\) & RMSE\(\) & 3PE\(\) & 2PE\(\) \\  HSM  & 12.39 & 14.27 & 90.87 & 92.58 \\ DAEI \({}^{}\) & 20.07 & 22.19 & 93.12 & 95.47 \\ CR-E2VID & 2.10 & 4.02 & 17.69 & 25.45 \\ Ours-CR-DA & 2.06 & 3.39 & 19.04 & 29.02 \\   

Table 3: Quantitative results of the proposed zero-shot disparity estimation method on the M3ED  dataset.

they achieve almost the best performance. Surprisingly, most of the variants of the proposed ZEST framework outperform this method in terms of 3PE metric, which demonstrates the effectiveness of the proposed method. The performance of the baseline CR-E2VID achieves good performance in terms of the 3PE and 2PE metrics in some sequences, although worse than the proposed method in all sequences. Quantitative results on MVSEC  and M3ED  datasets are shown in Tables 2 and 3, respectively, which demonstrate ZEST's robust generalization across diverse scenarios.

The visual results across varied scenes shown in Figure 4 demonstrate the generalizability of our method. Visual comparisons on DSEC are shown in Figure 5. For the baselines, we include DS-E2VID , which achieved the best performance in terms of EPE and RMSE metrics. They highlight the superior quality of our framework, generating depth maps with significantly enhanced sharpness, intricate details, and improved dynamic accuracy compared to the compared methods.

### Ablation study

To validate the effectiveness of each component in the proposed ZEST framework and analyze their contributions to the overall performance, we conduct a series of ablation studies to evaluate the impact of the representation alignment module and the monocular cue-guided disparity refinement module.

**Impact of the representation alignment module.** To assess the importance of the representation alignment module, we compare the performance of ZEST with and without this module. Quantitative results are shown in Table 4. In the absence of the proposed representation alignment, we feed the off-the-shelf stereo matching model DS with: 1) original frames and frames generated from events in the right view via E2VID (Figure 6); 2) events generated from frames in the left view via v2e and events in the right view; 3) the spatial gradient of frames in the left view and the spatial integral of events in the right view using ; and 7) the proposed representation alignment module. Among these settings, the proposed module achieves the best performance. This highlights the effectiveness of our approach in bridging the modality gap between events and frames, enabling the successful

   Setting & Rep. alignment & Monocular cue & EPE\(\) & RMSE\(\) & 3PE\(\) & 2PE\(\) & 1PE\(\) \\ 
1) DS w/ E2VID-right & ✗ & ✗ & 43.02 & 55.79 & 87.63 & 91.57 & 95.98 \\
2) DS w/ v2e-left & ✗ & ✗ & 13.90 & 20.26 & 72.68 & 81.44 & 90.65 \\
3) DS w/ spatial gradients & ✗ & ✗ & 19.01 & 23.76 & 78.88 & 86.24 & 93.05 \\
4) DS w/ spatial gradients + DA\_Large & ✗ & ✓ & 19.09 & 23.57 & 81.38 & 88.20 & 94.49 \\
5) DA\_Large & ✗ & ✓ & 35.85 & 41.34 & 99.05 & 99.35 & 99.65 \\
6) DA\_Large w/ GT scale & ✗ & ✓ & 2.40 & 3.16 & 28.35 & 47.45 & 72.07 \\
7) Ours-DS w/o DA & ✓ & ✗ & 1.49 & 2.85 & 7.77 & 16.84 & 47.84 \\
8) Ours-DS-DA\_Large & ✓ & ✓ & 1.41 & 2.62 & 7.22 & 16.00 & 46.37 \\
9) Ours-DS-DA\_Base & ✓ & ✓ & 1.39 & 2.59 & 6.96 & 15.60 & 46.17 \\
10) Ours-DS-DA\_Small & ✓ & ✓ & 1.42 & 2.64 & 7.30 & 15.94 & 46.95 \\   

Table 4: Quantitative results of ablation studies on the intertaken_00_c sequence of the DSEC  dataset. Compared to Table 1, 1-pixel error (1PE, %) is also utilized for evaluation.

Figure 6: Visual comparison of the disparity results of a stereo matching method DS using different representations and the proposed approach. From left to right: inputs, spatial gradients of frames and spatial integral of events (via ), their corresponding disparity result, the proposed representation, _i.e._, the temporal difference of the frame and the temporal integral of the events, and their corresponding disparity result.

application of off-the-shelf stereo matching models. The corresponding qualitative results are shown in Figures 3 and 6, respectively.

**Impact of the disparity refinement module.** To validate the effectiveness of each component, we compare the proposed method with its five variants: 3) stereo matching model fed with the spatial gradient of frames in the left view and the spatial integral of events in the right view; 4) the results of 3) refined by a monocular depth estimation; 5) only the monocular depth estimation model DA; 6) the results of (5) rescaled by a global scale calculated from the ground truth disparity; 7) the proposed method without disparity refinement; and 8) the proposed method with DA for disparity refinement. The effectiveness of the introduction of monocular depth estimation can be shown by comparing 7) and 8) and the corresponding qualitative results are shown in Figure 7, whose results demonstrate more natural edges with DA. However, the disparity refinement module fails when the stereo matching results are totally not reliable, as shown in the comparison between 3) and 4). As shown in Figures 6 and 7, the disparity refinement module improves sharp depth boundaries for objects, such as cars, in challenging scenarios with sparse events or low-texture regions.

**Impact of monocular depth estimation model size for refinement.** Our framework's modular design allows for deployment with lighter-weight models, ideal for resource-limited environments. While we use the DA_Large (335.3M parameters) for results in Table 1, we also evaluated compact alternatives, DA_Base (97.5M) and DA_Small (24.8M), as shown in 9) and 10) in Table 4. These alternatives provide substantial speed gains with acceptable accuracy trade-offs.

## 5 Conclusion

We introduce ZEST, a novel zero-shot event-intensity stereo matching framework that utilizes cutting-edge image domain models for accurate disparity estimation without training data. ZEST addresses the modality gap and labeled data scarcity in the event domain through representation alignment and monocular cue-guided disparity refinement. Experiments on DSEC show ZEST outperforms state-of-the-art methods in cross-dataset evaluation. Ablation studies validate the effectiveness of each component, highlighting the importance of representation alignment, model integration versatility, and monocular cue-guided refinement benefits.

**Acknowledgments.** This work was supported by National Natural Science Foundation of China (Grant No. 62136001, 62302019, 62088102, 62472179), Beijing Natural Science Foundation (Grant No. L233024), Beijing Municipal Science & Technology Commission, Administrative Commission of Zhongguancun Science Park (Grant No. Z241100003524012), National Key Research and Development Program of China (Grant No. 2024YFE0105400). Bin Fan was also supported by China Postdoctoral Science Foundation (Grant No. 2024M750101) and China National Postdoctoral Program for Innovative Talents (Grant No. BX20230013).

Figure 7: Visual comparison of the effectiveness of the monocular cue-guided disparity refinement module. From left to right: input frames, input events, scale map results, disparity results from the monocular model DA alone, results from the proposed method without DA, and results with DA incorporated.