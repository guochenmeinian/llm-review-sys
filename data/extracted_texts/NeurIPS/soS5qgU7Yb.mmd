# Analog Bayesian neural networks are insensitive to the shape of the weight distribution

Ravi G. Patel T. Patrick Xiao Sapan Agarwal Christopher Bennett Sandia National Laboratories

{rgpatel,txiao,sagarwa,cbennet}@sandia.gov

###### Abstract

Recent work has demonstrated that Bayesian neural networks (BNN's) trained with mean field variational inference (MFVI) can be implemented in analog hardware, promising orders of magnitude energy savings compared to the standard digital implementations. However, while Gaussians are typically used as the variational distribution in MFVI, it is difficult to precisely control the shape of the noise distributions produced by sampling analog devices. This paper introduces a method for MFVI training using real device noise as the variational distribution. Furthermore, we demonstrate empirically that the predictive distributions from BNN's with the same weight means and variances converge to the same distribution, regardless of the shape of the variational distribution. This result suggests that analog device designers do not need to consider the shape of the device noise distribution when hardware-implementing BNNs performing MFVI.

## 1 Introduction

Uncertainty quantification is an essential capability for machine learning (ML) algorithms, particularly where these models are in the critical path of high-consequence or safety-critical decisions . Without being able to accurately assess uncertainty, ML models cannot be trusted and therefore cannot be deployed in such systems. The Bayesian neural network (BNN) possesses the generalization capability of deep neural networks (DNNs), while providing rigorous estimates of predictive uncertainty by encoding its weights as trainable probability distributions rather than as fixed parameters [2; 3]. BNNs can capture uncertainty arising from ambiguous data, as well as uncertainty that comes from making inferences on data that lies well outside the distribution represented by the training data. Training of BNNs can be made computationally tractable by using variational inference [4; 5], a technique which constrains BNN weights to parameters with only a few tunable values, rather than arbitrarily complex distributions. Nonetheless, large BNN networks are difficult to implement at scale due to their complexity and sampling requirements; for every prediction from a BNN, a large number of random numbers must be sampled - at least one for every weight - and these random number generations can be prohibitively slow and power-intensive for these applications . Unfortunately, there are many target systems where uncertainty quantification is critical to enable safe autonomy, yet where ML algorithms must be processed in real time and within a small power envelope (typically less than 1W). Given this constraint, these methods can scarcely be deployed in modern edge or mobile devices.

To address this challenge with BNN inference on edge systems, a promising approach is to map BNNs onto energy-efficient analog in-memory computing (AIMC) hardware. This computing paradigm encodes weights as individual conductances within a larger crossbar array of programmable memory devices, and uses analog circuit laws to physically compute matrix-vector multiplications (MVMs) . Though AIMC has so far been applied only to conventional deterministic DNNs, early proposals have suggested that designers could exploit the noise inherent to emergingmemory devices to implement probabilistic algorithms . In particular, the conductances of memory devices have some level of intrinsic analog noise, which provides the opportunity to sample many analog random numbers and perform analog matrix computations on these random numbers in parallel . This approach is restricted to mean field variational inference

Critically, implementing BNNs on stochastic AIMC hardware requires the distribution of conductance noise in each individual memory device to be tunable (writable) to reflect the full range of probability distributions obtained from the software BNN training. Yet the noise distributions of real memory devices are often non-ideal; they may be minimally tunable, having only one or two degrees of freedom, or sampling them may yield an exotic distribution that isn't like the Gaussian distribution commonly used in VI. Prior work has nonetheless trained BNNs with Gaussian distributions and approximated them during inference time using non-ideal memory device device noise distributions [9; 10; 11; 12; 13]. Although this induces a significant amount of error at the level of a single probability distribution, the UQ end-application metrics from BNNs built from these non-ideal sampling devices (_e.g._, expected calibration error) has been observed to be remarkably resilient to these errors.

In this work, we present a method to train BNNs using variational inference that accounts for the non-standard probability distributions of actual noisy analog memory devices. This method effectively eliminates the approximation error involved in transferring the trained probability distributions in a BNN to the available conductance noise distributions in analog hardware. Nonetheless, we use this method to show numerically that the approximation error does not have a significant impact on the predictive uncertainty of deep BNNs, due to the effect of the central limit theorem (CLT) within each layer. This result provides a theoretical basis for the conclusion that large arrays of stochastic memory devices that do not have a high degree of tunability or precision in their encoded probability distributions can nonetheless be used to efficiently process large-scale BNNs.

## 2 Predictive distributions using device weights

A crossbar of memory devices with tunable mean conductances (\(\)) and tunable conductance noise (\(\)) enables the efficient computation of a stochastic MVM. In this case, the multiplication of a vector occurs with a matrix sampling from known, unique probability distributions. Often, the origin of randomness in the analog system is driven by thermal fluctuation in the conductance value of a given memory device; _e.g._, noise can can appear as complex \(1/f\) noise fingerprints in filamentary resistive random access memory (ReRAM) . Using a novel bit-cell design for stochastic MVMs introduced in , the mean conductance and the conductance noise do not need to be independently controllable within a single device. Stochastic MVMs have been demonstrated or simulated using magnetic random access memory (MRAM) , resistive random access memory (ReRAM) [13; 15; 10], charge-trap memory , and electrochemical random access memory (ECRAM) devices  that were engineered to enable operation over a range of programmable noise values (\(\)). Notably, among all these proposed devices, the shape of the noise probability distribution is not tunable in general, but rather a function of the underlying device physics of that memory component. Followinga number of individual sampling operations, the analog result of a stochastic MVM is converted from raw current and current noise into a vector of digital output signals using companion analog-to-digital converters (ADCs) at the bottom of each column in the array.

For the remaining results in this paper, we use the noise probability distribution of the Bayes-magnetic tunnel junction (Bayes-MTJ) device from . This device exploits the fact that the magnetization of a circular in-plane MTJ has two easy axes and no energetically preferred orientation. Therefore, the magnetization can rotate randomly due to thermal fluctuations, resulting in random changes in the tunnel magnetoresistance (conductance) of the MTJ. This noise has a distribution that is not perfectly described by a standard Gaussian distribution, as in Fig. 1. By modulating the built-in voltage inside the Bayes-MTJ, the width of the distribution could be made large or small without affecting the distribution's shape. Crossbar arrays of Bayes-MTJs, combined with crossbar arrays of less noisy multi-state MRAM devices such as domain-wall MTJs, can be used together to implement stochastic MVMs and perform BNN inference.

In addition to the real device noise, we also examine inference with a weight distribution much further from a Gaussian than device distribution, a mixture of Gaussians. We refer to this distribution as the bimodal distribution. Therefore as in Figure 1 bottom pane, we contrast these two designer distributions against a standard Gaussian distribution throughout the remainder of this work.

## 3 Mean field variational inference with device distributions

Our goal is to compare inference using Gaussians as the variational distribution to realistic device noise as the variational distribution. In VI, one minimizes the Kullback-Leibler (KL) divergence , \(_{}KL(p_{V}(;)||p_{post}(|X))\), between a variational distribution, \(p_{V}\), and the posterior, \(p_{post}(|X) p_{}(X|)p_{prior}()\) where \(X\) is data, \(\) is a vector of model parameters, \(\) is the variational parameters, \(p_{}\) is a likelihood, and \(p_{prior}\) is a prior, by maximizing the evidence lower bound (ELBO),

\[_{}()=E_{p_{V}}[ p_{}(X|)]-D_{KL}(p_ {V}||p_{prior}),\] (1)

The expectation of first term in (1) is high dimensional but can be estimated by Monte Carlo (MC), \(E_{p_{V}}[ p_{}(X|)]_{i}^{N} p_{}( X|_{i})\), sampling the parameters from the variational distribution, \(_{i} p_{V}\). We will work with the mean field assumption, i.e. assume the parameters are all independently distributed. Furthermore, we will assume each parameter is distributed by a scaled and shifted version of the same base distribution, \(q_{V}\). This allows us to use the reparameterization trick to compute the MC estimate. We sample, \(z_{ij} q_{V}\), per parameter, per Monte Carlo sample, and transform each vector \(z_{i}\) as \(_{i}= z_{i}+\) where and \(\) and \(\) are vectors of shift and scale variational parameters. Section 3.1.3 discusses our approach to sampling \(q_{V}\) for the device distribution.

The mean field assumption and reparameterization trick also allow us to simplify the KL divergence between the variational distribution and the prior in (1). We also choose an i.i.d. prior, \(p_{prior}()=_{i}(_{i})\). Due to the mean field approximation, the integrals in the second term of (1) are one dimensional. There are closed form expressions for Gaussian variational distributions and Gaussian priors, but we must use quadrature for the device distribution. See Section 3.1.2 for further details. The variational inference optimization problem becomes,

\[_{_{j},_{j}}_{i}^{N} p_{}(X|_{j}z_{ ij}+_{j})-_{j}(_{j}+ q(z)_{prior}( _{j}z+_{j})dz)\] (2)

where \(z_{ij} q\). We have drop terms constant in \(\) and \(\) and simplified.

### Numerical Approximations

This section details various numerical approximations used throughout this work. The device distribution needs careful numerical analysis to generate samples from and compute expectations with.

#### 3.1.1 Maximum likelihood fit to device noise

We compare the predictive distributions using three different neural network weight distributions, Gaussian, device, and a mixture of two Gaussians (bimodal). We model the device noise distribution as,

\[q_{D}(x)=A()-A()+C(1-x^ {2})-1 x 1\] (3)

\(q_{D}(x)=0\)\(\)

and find the maximum likelihood estimates of \(A,B,\) and \(C\) using the Adam optimizer  under the constraint that \(_{-1}^{1}q_{D}(x)dx=1\). Throughout this work, we compare results using this device distribution to a Gaussian distribution, \(q_{G}\), and a mixture of Gaussians, \(q_{M}\), with the same mean and standard deviations. Figure 1 demonstrates the device distribution fit and compares the three variational distributions.

#### 3.1.2 Quadrature

In general, there is no closed form expression for expectations with respect to the device distribution,

\[E_{q}[f(x)]=_{-1}^{1}f(x)q(x)dx\] (4)

so we need to use approximations. Of particular importance is approximating variance, \(E[(x-E[x])^{2}]\), and the KL divergence from variational distributions to other distributions, \(D_{KL}(q||p)=E_{q}[ q- p]\).

We can efficiently and accurately integrate expectations of polynomials and smooth functions using the device distribution as the weighting function in Gaussian quadrature. In fact, cross entropies with respect to the Gaussian distribution can be exactly integrated with only two quadrature points. In other cases, we design custom quadrature rules based on standard quadratures. Figure 2 demonstrates the efficacy of these quadratures. More details on this formulation and justification for this choice are given in Appendix B.

#### 3.1.3 Inverse sampling

We generate additional device noise samples using inverse transform sampling. After fitting to device noise, we have access to the CDF, \(Q_{D}(x)=_{-1}^{x}q_{D}(z)dz\). We can generate new samples by applying the inverse CDF, \(G=Q_{D}^{-1}\), to samples from the uniform distribution,

\[u U x=G(u) x q_{D}\] (5)

We cannot find a closed-form expression for the inverse CDF and must use numerical approximation. Because \(q_{D}\) is symmetric across \(x=0\), \(Q_{D}\) and \(G\) are symmetric about 180 degree rotations around \((x,u)=(0,)\). Therefore, we can approximate a restriction of the inverse CDF, \(:[-1,0][0,]\), and reuse it for the other halves of the full domain and codomain.

\(\) is smooth and amenable to polynomial approximation in the domain \([,]\) for \(0<\). However, near \(u=0\) the function is not differentiable. By differentiating \(Q_{D}((u))=u\), we find,

\[_{u 0^{+}}Q_{D}((u))^{}=_{u 0^{+}}Q _{D}^{}|_{(u)}\ ^{}_{u}=1_{u 0^{+}} ^{}_{u}=_{x-1}(x)}=_{x -1})}\] (6)

This limit diverges due to the quadratic term. Instead of approximating \(\) directly, we use polynomials to approximate a function with the non-differentiability subtracted and then add the non-differentiability back in.

The CDF has the 2nd order Taylor expansion near \(x=-1\), \(Q_{2}(-1+x)=Q_{D}(-1)+.Q_{D}^{}|_{-1}x+.Q_{D} ^{}|_{-1}x^{2}\), which can be inverted for \(x[-1,0]\) to give, \(_{2}=Q_{2}^{-1}\). See Appendix A for the formal expressions of \(Q_{2}\) and \(_{2}\). We fit a Legendre polynomial, shifted and scaled to the restricted domain, to the difference, \(=-_{2}_{i}c_{i}_{i}\), at the Gauss-Lobatto points. We employ Brent's root-finding method  to compute \(\) at these points. The restriction of the inverse CDF is then approximated as the sum, \(=_{i}c_{i}_{i}+_{2}\). To approximate the restriction of \(G\) to \(:[,1]\), we rotate \(\) 180 degrees around \((x,u)=(0,)\). The full approximation of \(G\) is,

\[G(u)=_{i}c_{i}_{i}(u)+_{2}(u)&u<\\ -_{i}c_{i}_{i}(1-u)-_{2}(1-u)&u>\\ 0&u=\] (7)The left subplot in Figure 3 show that the corrected fit performs better than the direct fit. We also compare them by evaluating Monte Carlo estimates of the KL divergence from the device distribution to a Gaussian of the same mean and variance. While Monte Carlo estimates using the polynomial fit to the inverse CDF appears to converge to a biased estimate for the KL divergence while the correct fit continues to converge to the quadrature approximation of the KL divergence.

## 4 Assessing the impact of diverse variational distributions on neural network performance

In this section we compare the predictive distributions from probabilistic neural networks using the three variational distributions. See Appendix C for details on training and the architectures.

### Energy distance minimization

Firstly, we trained a dense neural network function from reals to reals, \(f_{nn}:x; y\), with Gaussian weights, \(^{G}\), to match the normal distribution at \(x=0\). This set-up allows us to control the form of the predictive distribution and evaluate inference when we switch between various variational distributions. We minimize the energy distance with respect to the variational parameters,

\[_{,}2E_{^{G}_{i} p_{G}\\ y_{j} p_{T}}[|f_{nn}(0;^{G}_{i})-y_{j}|^{} ]-E_{^{G}_{i},^{G}_{j} p_{G}}[|f_{nn}(0;^{G}_ {i})-f_{nn}(0;^{G}_{j})|^{}]\] (8)

with \(=1.5\). We use MC to estimate expectations over the neural network outputs and quadrature with the target distribution as the weighting function for expectations over the target distribution. After training, we compare predictive distributions at \(x=0\) for the same network and variational parameters, replacing the base distribution from Gaussian to either device or bimodal. In Figure 4, we vary the neural network depth and width. We observe that the predictive distributions for the three distributions converge to the target distribution as the width increases.

### Scalar Regression

Next, we performed Bayesian 1D regression using the variational inference framework described above. We choose as our true model,

\[y=(2 x)+ N(0,0.1(1+(0,x-1)))\] (9)

We generate training data, \(x_{train} u[-1,1]\), and compute \(y_{train}\) from the above model. Likewise, we compute test data using, \(x_{test} u[-1.5,1.5]\) and \(y_{test}\) from the same model. We next fit the data with MFVI training using Gaussians as our variational distribution and compare the posterior

Figure 4: _(Left)_ Comparison of three predictive distributions for energy distance problem at different widths and depths. _(Right)_ KL divergence from predictive distributions for \(y(x=0)\) using Device and Bimodal weights to predictive distributions from Gaussian weights.

Figure 3: Comparison between corrected fit and polynomial fits to inverse CDF. _(Left)_ The approximations to inverse CDF over the full domain. _(Center)_ The approximations near \(u=0\). _(Right)_ Absolute difference between Monte Carlo and quadrature estimates for the KL divergence between the device distribution and a centered Gaussian with the same variance. The corrected fit performs better than the polynomial fit.

predictive distributions obtained by replace the Gaussians with device and bimodal distributions of the same means and variances.

We choose as our predictive model a feed-forward, Bayesian neural network. We model the error as heteroscedastic noise predicted by a feed-forward, deterministic neural network,

\[ p_{V} y_{pred}-y N(0,_{a}) y_{pred}=f_{nn}(x; )_{a}=f^{}_{nn}(x,^{})\] (10)

The model is similar to , except that the aleatoric predictions are deterministic. After training, in Figure 5 we compare inference using the three variational distributions by fixing the variational parameters, but replacing the form of the Gaussian base distribution to the device distribution and the bimodal distribution. At inference, the predictive distributions, with this change is nearly identical to the predictive distribution produced by the Gaussians.

### UTKFACE

Lastly, we performed Bayesian inference on the UTKFACE dataset . We seek a mapping from images of faces to ages and use the same model as (10), replacing the feedforward neural networks with convolutional neural networks. As in the last example, we train with the Gaussian distribution, but replace the variational distribution with the device and bimodal distributions. Figure6 compares the calibration curves obtained from inference on test data. They are obtained as discussed in . We find that the predictions obtained by any of the variational distributions used during inference are identical, regardless of the distribution used during training.

## 5 Discussion and Conclusions

We empirically demonstrate that the shape of the variational distribution has little impact on the posterior predictive distributions in mean field variational inference; only the mean and variance is important. As it is difficult to tune the shape of the variational distribution in hardware, these results suggest that device engineering efforts do not need to focus on producing specific sampling curves for downstream variational distributions. Furthermore, one may use the standard Gaussian variational distribution to train the networks in software and reuse the scale and shift parameters in hardware, regardless of the shape of the hardware noise distribution. Our work enables this by establishing methods for transforming predictive distributions. While, as was shown in Fig 4., sufficiently wide neural network layers are still necessary to reduce the difference between predictive distributions, the required width of networks to do so is minimal due to the CLT. Future work will examine whether any loss exists in narrow, deep networks on tasks of interest. Additionally, our claims could be verified in hardware within small probabilistic arrays.